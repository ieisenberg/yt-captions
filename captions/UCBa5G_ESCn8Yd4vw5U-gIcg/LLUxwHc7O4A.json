[
  {
    "text": "Let's talk about neighborhood sampling.",
    "start": "4130",
    "end": "7500"
  },
  {
    "text": "And this is really,",
    "start": "7500",
    "end": "8970"
  },
  {
    "text": "the key idea of the GraphSAGE,",
    "start": "8970",
    "end": "11730"
  },
  {
    "text": "uh, paper or the GraphSAGE architecture.",
    "start": "11730",
    "end": "14219"
  },
  {
    "text": "So what the GraphSAGE, uh,",
    "start": "14219",
    "end": "16410"
  },
  {
    "text": "what brought to the field of graph neural networks,",
    "start": "16410",
    "end": "19215"
  },
  {
    "text": "is a way to think about mini-batch implementations.",
    "start": "19215",
    "end": "22650"
  },
  {
    "text": "Before all the implementations were full batch and",
    "start": "22650",
    "end": "25920"
  },
  {
    "text": "people would then run- run graph neural networks on you know, 3,",
    "start": "25920",
    "end": "30030"
  },
  {
    "text": "4, 5,000 node graphs because that's what they could fit into GPU memory.",
    "start": "30030",
    "end": "34700"
  },
  {
    "text": "And what GraphSAGE , uh,",
    "start": "34700",
    "end": "36825"
  },
  {
    "text": "changed is they, it changed the way we think of graph neural networks,",
    "start": "36825",
    "end": "41480"
  },
  {
    "text": "the way we can think of creating minibatches,",
    "start": "41480",
    "end": "44285"
  },
  {
    "text": "and this also means that we can scale them up to graphs of,",
    "start": "44285",
    "end": "47739"
  },
  {
    "text": "uh, tens of billions of nodes and edges.",
    "start": "47740",
    "end": "49850"
  },
  {
    "text": "So, uh, let me now tell you about what do I mean by,",
    "start": "49850",
    "end": "53180"
  },
  {
    "text": "um, a- a neighborhood sampling.",
    "start": "53180",
    "end": "56565"
  },
  {
    "text": "So, uh, let's recall,",
    "start": "56565",
    "end": "59250"
  },
  {
    "text": "let- let's remember how, uh,",
    "start": "59250",
    "end": "61270"
  },
  {
    "text": "graph neural networks work and they operate through this notion of a computational graph.",
    "start": "61270",
    "end": "65600"
  },
  {
    "text": "Where GNN generate node embeddings via neighborhood aggregation,",
    "start": "65600",
    "end": "69920"
  },
  {
    "text": "meaning they aggregate features information from the neighbors,",
    "start": "69920",
    "end": "73400"
  },
  {
    "text": "and then they create a new message and pass it on.",
    "start": "73400",
    "end": "76385"
  },
  {
    "text": "So the way you can think of it,",
    "start": "76385",
    "end": "77600"
  },
  {
    "text": "if you say I want to create an embedding for this node, uh,",
    "start": "77600",
    "end": "80685"
  },
  {
    "text": "0, I will take neighbors and neighbors of neighbors.",
    "start": "80685",
    "end": "84079"
  },
  {
    "text": "Here is now my computation graph, um,",
    "start": "84080",
    "end": "87230"
  },
  {
    "text": "where nodes start with their individual feature vectors at the level 0.",
    "start": "87230",
    "end": "91550"
  },
  {
    "text": "Uh, these feature vectors get aggregated,",
    "start": "91550",
    "end": "94474"
  },
  {
    "text": "transformed, passed to the next level, um.",
    "start": "94474",
    "end": "97760"
  },
  {
    "text": "And then again, the same thing gets aggregated",
    "start": "97760",
    "end": "99800"
  },
  {
    "text": "transformed all the way to node- node of interest,",
    "start": "99800",
    "end": "102680"
  },
  {
    "text": "node 0, that makes the prediction.",
    "start": "102680",
    "end": "104675"
  },
  {
    "text": "Notice here that these nodes, you know, er,",
    "start": "104675",
    "end": "107330"
  },
  {
    "text": "the node 0 here,",
    "start": "107330",
    "end": "108935"
  },
  {
    "text": "means this is the feature vector of the node, uh,",
    "start": "108935",
    "end": "111814"
  },
  {
    "text": "0, um, er, for example,",
    "start": "111815",
    "end": "114760"
  },
  {
    "text": "here this means that this will be now a layer 1,",
    "start": "114760",
    "end": "117410"
  },
  {
    "text": "er, representation of nodes,",
    "start": "117410",
    "end": "119750"
  },
  {
    "text": "er, 1, 2, 3.",
    "start": "119750",
    "end": "121430"
  },
  {
    "text": "And here I have now a layer 2 representation of node ze- of node 0.",
    "start": "121430",
    "end": "125840"
  },
  {
    "text": "So what- what do I mean by this is that,",
    "start": "125840",
    "end": "127759"
  },
  {
    "text": "actually nodes have multiple representations,",
    "start": "127760",
    "end": "131820"
  },
  {
    "text": "uh, one for each, uh, layer.",
    "start": "131820",
    "end": "134370"
  },
  {
    "text": "And that's the- that's why when you do full batch,",
    "start": "134370",
    "end": "137030"
  },
  {
    "text": "you want to take level 0 embeddings to create level",
    "start": "137030",
    "end": "140480"
  },
  {
    "text": "1 embeddings of everyone, to create level 2 embeddings from everyone.",
    "start": "140480",
    "end": "144245"
  },
  {
    "text": "But here, because we have this local view,",
    "start": "144245",
    "end": "146900"
  },
  {
    "text": "we will only need to create level 1 embeddings for nodes 1, 2,",
    "start": "146900",
    "end": "151370"
  },
  {
    "text": "and 3, because we need those to create a level 2 embedding,",
    "start": "151370",
    "end": "155000"
  },
  {
    "text": "uh, for node uh, 0.",
    "start": "155000",
    "end": "156995"
  },
  {
    "text": "So that's, uh, the idea, right?",
    "start": "156995",
    "end": "159560"
  },
  {
    "text": "So the important observation is that a two layer GNN generates embedding for this node 0,",
    "start": "159560",
    "end": "165770"
  },
  {
    "text": "using got two-hop neighborhood structure and features around, uh, this graph.",
    "start": "165770",
    "end": "170020"
  },
  {
    "text": "So if I wanted to compute the layer, uh,",
    "start": "170020",
    "end": "172490"
  },
  {
    "text": "the embedding of node 0,",
    "start": "172490",
    "end": "174785"
  },
  {
    "text": "what I need is the graph structure,",
    "start": "174785",
    "end": "177124"
  },
  {
    "text": "plus features of this two-hop neighborhood around the- the node,",
    "start": "177124",
    "end": "182329"
  },
  {
    "text": "and I can ignore the rest of the graph.",
    "start": "182330",
    "end": "184715"
  },
  {
    "text": "I don't need to know the rest of the graph, right?",
    "start": "184715",
    "end": "187370"
  },
  {
    "text": "The embedding of node 0 will be independent of how",
    "start": "187370",
    "end": "190790"
  },
  {
    "text": "the graph structure is beyond the two-hop, uh, neighborhood.",
    "start": "190790",
    "end": "194599"
  },
  {
    "text": "And that's an important insight.",
    "start": "194600",
    "end": "196525"
  },
  {
    "text": "Because this means if I want to have a K layer GNN to emb- to generate",
    "start": "196525",
    "end": "201650"
  },
  {
    "text": "an embedding of a node using these K-hop neighborhood structure and features,",
    "start": "201650",
    "end": "206405"
  },
  {
    "text": "this means I only need to, um,",
    "start": "206405",
    "end": "208850"
  },
  {
    "text": "know at the time and I'm generating that the embedding that K-hop neighborhood.",
    "start": "208850",
    "end": "213050"
  },
  {
    "text": "And I can ignore the rest of the network.",
    "start": "213050",
    "end": "215560"
  },
  {
    "text": "And now, if the level k is, uh,",
    "start": "215560",
    "end": "218364"
  },
  {
    "text": "a relatively small or the neighborhood size is not too big,",
    "start": "218365",
    "end": "222185"
  },
  {
    "text": "which means that it is let say constant size,",
    "start": "222185",
    "end": "224780"
  },
  {
    "text": "but the entire graph can be much, much bigger,",
    "start": "224780",
    "end": "227015"
  },
  {
    "text": "then it means I need relatively little information,",
    "start": "227015",
    "end": "229730"
  },
  {
    "text": "or relatively little memory to use to generate or create the embedding,",
    "start": "229730",
    "end": "234800"
  },
  {
    "text": "uh, of node, uh, 0.",
    "start": "234800",
    "end": "236875"
  },
  {
    "text": "And, uh, this is the main insight.",
    "start": "236875",
    "end": "239120"
  },
  {
    "text": "The main insight is that we wanna- that to compute the embedding of a single node,",
    "start": "239120",
    "end": "244489"
  },
  {
    "text": "all we need is the K-hop neighborhood structure, uh,",
    "start": "244490",
    "end": "247580"
  },
  {
    "text": "around that node, and we can ignore the rest of the network.",
    "start": "247580",
    "end": "251060"
  },
  {
    "text": "The rest of the network does not affect the embedding of this node of interest,",
    "start": "251060",
    "end": "256875"
  },
  {
    "text": "um, all that affects it is the K-hop neighborhood around that node.",
    "start": "256875",
    "end": "261305"
  },
  {
    "text": "So what this means is that now we can generate minibatches in such a way that we say,",
    "start": "261305",
    "end": "266720"
  },
  {
    "text": "let's sample M different nodes in a mini-batch,",
    "start": "266720",
    "end": "270020"
  },
  {
    "text": "but we won't only put,",
    "start": "270020",
    "end": "271835"
  },
  {
    "text": "we won't put nodes into the mini-batch,",
    "start": "271835",
    "end": "274370"
  },
  {
    "text": "but we are going to put entire computation graphs into the mini-batch.",
    "start": "274370",
    "end": "278389"
  },
  {
    "text": "So it means that I would pick a first node and I'll",
    "start": "278390",
    "end": "280940"
  },
  {
    "text": "take its K-hop neighborhood computation graph,",
    "start": "280940",
    "end": "283660"
  },
  {
    "text": "and this is one element in the batch.",
    "start": "283660",
    "end": "286275"
  },
  {
    "text": "And then I'm going to sample the second node,",
    "start": "286275",
    "end": "288440"
  },
  {
    "text": "create its computation graph and put this into my mini-batch.",
    "start": "288440",
    "end": "292055"
  },
  {
    "text": "And so on and so forth.",
    "start": "292055",
    "end": "293690"
  },
  {
    "text": "So now this means perhaps my batches will be smaller",
    "start": "293690",
    "end": "296540"
  },
  {
    "text": "because batches are not now composed of individual nodes,",
    "start": "296540",
    "end": "299780"
  },
  {
    "text": "but batches are composed of network neighborhoods,",
    "start": "299780",
    "end": "302870"
  },
  {
    "text": "batches are composed from computation graphs.",
    "start": "302870",
    "end": "306665"
  },
  {
    "text": "So we are going to sample M computation graphs, um,",
    "start": "306665",
    "end": "310250"
  },
  {
    "text": "and then put these M computation graphs into the GPU memory,",
    "start": "310250",
    "end": "313550"
  },
  {
    "text": "so that we can compute the loss over this mini batch of M computation graphs.",
    "start": "313550",
    "end": "318895"
  },
  {
    "text": "So to emphasize again,",
    "start": "318895",
    "end": "321259"
  },
  {
    "text": "you know, what is the key idea?",
    "start": "321260",
    "end": "323015"
  },
  {
    "text": "The key idea is the following.",
    "start": "323015",
    "end": "324755"
  },
  {
    "text": "It starts with the insight that in order to compute the embedding of a given node,",
    "start": "324755",
    "end": "329810"
  },
  {
    "text": "all that we need to know is the K-hop neighborhood of that node.",
    "start": "329810",
    "end": "334300"
  },
  {
    "text": "So this means if I create a mini",
    "start": "334300",
    "end": "337370"
  },
  {
    "text": "batching not based on the nodes but based on the K-hop neighborhoods,",
    "start": "337370",
    "end": "341810"
  },
  {
    "text": "then I will be able to compute- um,",
    "start": "341810",
    "end": "344389"
  },
  {
    "text": "I will be able to compute the gradient in a reliable way.",
    "start": "344390",
    "end": "347320"
  },
  {
    "text": "So basically, rather than putting nodes into mini-batches,",
    "start": "347320",
    "end": "350680"
  },
  {
    "text": "we are putting, uh,",
    "start": "350680",
    "end": "351919"
  },
  {
    "text": "computational graphs, or in other words,",
    "start": "351920",
    "end": "354695"
  },
  {
    "text": "K-hop neighborhoods into the-,",
    "start": "354695",
    "end": "356540"
  },
  {
    "text": "uh, into the mini-batches.",
    "start": "356540",
    "end": "359455"
  },
  {
    "text": "Now, um, because we have put, um,",
    "start": "359455",
    "end": "363539"
  },
  {
    "text": "uh, we have put, uh,",
    "start": "363540",
    "end": "365310"
  },
  {
    "text": "uh, entire computation graphs,",
    "start": "365310",
    "end": "367550"
  },
  {
    "text": "entire network neighborhoods of K-hops into the, into the mini-batch,",
    "start": "367550",
    "end": "372275"
  },
  {
    "text": "we can, uh, consider the following, uh,",
    "start": "372275",
    "end": "374540"
  },
  {
    "text": "stochastic gradient descent strategy to train the model parameters, right?",
    "start": "374540",
    "end": "378860"
  },
  {
    "text": "We are going to sample,",
    "start": "378860",
    "end": "380650"
  },
  {
    "text": "um, let say M nodes.",
    "start": "380650",
    "end": "383540"
  },
  {
    "text": "For each node, we are going now to sample",
    "start": "383540",
    "end": "385730"
  },
  {
    "text": "the entire K-hop neighborhood to construct the computation graph.",
    "start": "385730",
    "end": "390290"
  },
  {
    "text": "And the we are, uh,",
    "start": "390290",
    "end": "391850"
  },
  {
    "text": "assuming that we have enough memory, um,",
    "start": "391850",
    "end": "394490"
  },
  {
    "text": "that we can fit into the mini-batch",
    "start": "394490",
    "end": "396470"
  },
  {
    "text": "both the nodes as well as their entire computation graphs.",
    "start": "396470",
    "end": "399965"
  },
  {
    "text": "Now, we have the complete set of information we need",
    "start": "399965",
    "end": "403625"
  },
  {
    "text": "to compute an embedding of every- every node in the mini-batch,",
    "start": "403625",
    "end": "407074"
  },
  {
    "text": "so we can then compute the loss over the- this mini-batch,",
    "start": "407074",
    "end": "410945"
  },
  {
    "text": "and we can then perform stochastic gradient descent to",
    "start": "410945",
    "end": "414110"
  },
  {
    "text": "basically update the model parameter with respect to the gradient,",
    "start": "414110",
    "end": "417879"
  },
  {
    "text": "um, er, to that,",
    "start": "417880",
    "end": "419250"
  },
  {
    "text": "uh, mini-batch, uh, loss.",
    "start": "419250",
    "end": "420890"
  },
  {
    "text": "And this is stochastic gradient because,",
    "start": "420890",
    "end": "423185"
  },
  {
    "text": "um, the batches are randomly created, so uh,",
    "start": "423185",
    "end": "425810"
  },
  {
    "text": "the- the gradient will have a bit of,",
    "start": "425810",
    "end": "428930"
  },
  {
    "text": "uh, randomness in it, but that's all fine.",
    "start": "428930",
    "end": "430955"
  },
  {
    "text": "It just means we can do, uh, great,",
    "start": "430955",
    "end": "432800"
  },
  {
    "text": "um, updates very, very fast.",
    "start": "432800",
    "end": "434735"
  },
  {
    "text": "So that's the, that's the idea.",
    "start": "434735",
    "end": "437270"
  },
  {
    "text": "So, um, if we, uh,",
    "start": "437270",
    "end": "440345"
  },
  {
    "text": "do it the way I explained it,",
    "start": "440345",
    "end": "442175"
  },
  {
    "text": "then we have, uh, still an issue with this notion of, uh,",
    "start": "442175",
    "end": "445460"
  },
  {
    "text": "mini-batches and stochastic training because for each node,",
    "start": "445460",
    "end": "449044"
  },
  {
    "text": "we need to get the entire K-hop neighborhood and pass it through",
    "start": "449045",
    "end": "453020"
  },
  {
    "text": "the computation graph and load it into the GPU memory.",
    "start": "453020",
    "end": "457490"
  },
  {
    "text": "So this means we need to aggregate a lot of information just to compute one- one node,",
    "start": "457490",
    "end": "463250"
  },
  {
    "text": "uh, e- e- embedding for a single node.",
    "start": "463250",
    "end": "465650"
  },
  {
    "text": "So computation will be expensive.",
    "start": "465650",
    "end": "467974"
  },
  {
    "text": "So let me tell you why it will be expensive.",
    "start": "467974",
    "end": "470375"
  },
  {
    "text": "First, it will be expensive because,",
    "start": "470375",
    "end": "472340"
  },
  {
    "text": "um, um, uh, the deeper I go,",
    "start": "472340",
    "end": "474710"
  },
  {
    "text": "the bigger these computation graphs and",
    "start": "474710",
    "end": "476389"
  },
  {
    "text": "these computation graphs are going to increase- um,",
    "start": "476390",
    "end": "479150"
  },
  {
    "text": "their size is going to increase exponentially, uh,",
    "start": "479150",
    "end": "481860"
  },
  {
    "text": "with the- with the depth of",
    "start": "481860",
    "end": "484250"
  },
  {
    "text": "the computation graph because even if every node has just three children,",
    "start": "484250",
    "end": "488480"
  },
  {
    "text": "it's going to increase, uh,",
    "start": "488480",
    "end": "490640"
  },
  {
    "text": "exponentially with the number of layers.",
    "start": "490640",
    "end": "493070"
  },
  {
    "text": "So that's one issue.",
    "start": "493070",
    "end": "495260"
  },
  {
    "text": "So the computation graphs are going to get very big if they get",
    "start": "495260",
    "end": "499370"
  },
  {
    "text": "very deep and then the second thing is that in natural graphs,",
    "start": "499370",
    "end": "503165"
  },
  {
    "text": "think of the lecture when we talked about",
    "start": "503165",
    "end": "505565"
  },
  {
    "text": "Microsoft Instant Messenger network when we talked about degree distribution,",
    "start": "505565",
    "end": "509149"
  },
  {
    "text": "we have these celebrity nodes,",
    "start": "509149",
    "end": "511189"
  },
  {
    "text": "these high degree nodes, uh,",
    "start": "511190",
    "end": "513110"
  },
  {
    "text": "that a lot of other people connect to or a lot of other nodes collect- connect to.",
    "start": "513110",
    "end": "517610"
  },
  {
    "text": "We have such nodes even in knowledge graphs.",
    "start": "517610",
    "end": "519740"
  },
  {
    "text": "If you think about, you know,",
    "start": "519740",
    "end": "521255"
  },
  {
    "text": "a node corresponding to a large country,",
    "start": "521255",
    "end": "523310"
  },
  {
    "text": "let's say like, uh, USA, um,",
    "start": "523310",
    "end": "525560"
  },
  {
    "text": "it will have a huge degree because there is",
    "start": "525560",
    "end": "527360"
  },
  {
    "text": "so many other entities related to it and of course,",
    "start": "527360",
    "end": "529925"
  },
  {
    "text": "a small country will have a much smaller degree",
    "start": "529925",
    "end": "532265"
  },
  {
    "text": "because there will be a smaller number of entities related to it.",
    "start": "532265",
    "end": "535115"
  },
  {
    "text": "So the point is, we'll have these hub nodes.",
    "start": "535115",
    "end": "537440"
  },
  {
    "text": "And now if a hub node has degree 1 million, uh,",
    "start": "537440",
    "end": "540530"
  },
  {
    "text": "which is nothing out of ordinary,",
    "start": "540530",
    "end": "542135"
  },
  {
    "text": "then you'll have to aggregate here information from 1 million nodes.",
    "start": "542135",
    "end": "545435"
  },
  {
    "text": "So this computation graph will get huge very quickly and you are",
    "start": "545435",
    "end": "549290"
  },
  {
    "text": "most likely going to hit these hub nodes, uh, very often.",
    "start": "549290",
    "end": "553430"
  },
  {
    "text": "So the point is,",
    "start": "553430",
    "end": "554810"
  },
  {
    "text": "you cannot take the entire K-hop neighborhood in most of the cases. So what do you do?",
    "start": "554810",
    "end": "560540"
  },
  {
    "text": "What do you do, is to do- is to",
    "start": "560540",
    "end": "563555"
  },
  {
    "text": "ap- apply on a project that is called neighborhood sampling.",
    "start": "563555",
    "end": "566810"
  },
  {
    "text": "Where the key idea is to cra- construct",
    "start": "566810",
    "end": "569120"
  },
  {
    "text": "a computational graph by sampling at most H neighbors,",
    "start": "569120",
    "end": "573080"
  },
  {
    "text": "uh, of every node.",
    "start": "573080",
    "end": "574655"
  },
  {
    "text": "So it means that every- every node- every node in the tree- in",
    "start": "574655",
    "end": "578750"
  },
  {
    "text": "the computation graph is going to have at most- is going to aggregate from at most,",
    "start": "578750",
    "end": "583205"
  },
  {
    "text": "uh, H other nodes.",
    "start": "583205",
    "end": "585740"
  },
  {
    "text": "So in our case, just to give you an example,",
    "start": "585740",
    "end": "588154"
  },
  {
    "text": "if I say, let's say H equals 3,",
    "start": "588155",
    "end": "590480"
  },
  {
    "text": "then my original computation graph is now going to be pruned in such a way that",
    "start": "590480",
    "end": "594785"
  },
  {
    "text": "every- every aggregation is going to aggregate from at most two other nodes.",
    "start": "594785",
    "end": "600019"
  },
  {
    "text": "So here, this entire branch is going to be cut out and you know,",
    "start": "600020",
    "end": "604970"
  },
  {
    "text": "some other nodes are going to be cut out as well.",
    "start": "604970",
    "end": "608644"
  },
  {
    "text": "And what this means is that now our, um,",
    "start": "608645",
    "end": "611570"
  },
  {
    "text": "computation graph will be much more manageable because we have just,",
    "start": "611570",
    "end": "616070"
  },
  {
    "text": "um, even if we hit the high degree hub node,",
    "start": "616070",
    "end": "618440"
  },
  {
    "text": "we are only to take,",
    "start": "618440",
    "end": "619835"
  },
  {
    "text": "uh, a fixed number of its neighbors,",
    "start": "619835",
    "end": "622460"
  },
  {
    "text": "uh, in the aggregation.",
    "start": "622460",
    "end": "624634"
  },
  {
    "text": "So the point is that you can use these print",
    "start": "624634",
    "end": "627500"
  },
  {
    "text": "computational graphs to more efficiently compute, uh, node embedding.",
    "start": "627500",
    "end": "631280"
  },
  {
    "text": "Um, so now, uh,",
    "start": "631280",
    "end": "632750"
  },
  {
    "text": "how do you do this computational graph sampling, right?",
    "start": "632750",
    "end": "636110"
  },
  {
    "text": "Basically the idea is for every node, for every layer, uh,",
    "start": "636110",
    "end": "639154"
  },
  {
    "text": "for every internal, um,",
    "start": "639155",
    "end": "641240"
  },
  {
    "text": "node of the- of the computation graph,",
    "start": "641240",
    "end": "643580"
  },
  {
    "text": "uh, we are going to, uh,",
    "start": "643580",
    "end": "645920"
  },
  {
    "text": "first basically compute the K-hop neighborhood, uh,",
    "start": "645920",
    "end": "648545"
  },
  {
    "text": "from the starting node and then for every, uh,",
    "start": "648545",
    "end": "651200"
  },
  {
    "text": "node in the computation graph,",
    "start": "651200",
    "end": "652490"
  },
  {
    "text": "we are going to pick at most, uh,",
    "start": "652490",
    "end": "655325"
  },
  {
    "text": "K, uh, H random, uh, neighbors.",
    "start": "655325",
    "end": "658580"
  },
  {
    "text": "Um, and this means that the K-layer GNN will, uh, involve, uh,",
    "start": "658580",
    "end": "663860"
  },
  {
    "text": "at most, uh, um, uh,",
    "start": "663860",
    "end": "666589"
  },
  {
    "text": "the product of the- of, uh,",
    "start": "666590",
    "end": "668645"
  },
  {
    "text": "H leaf nodes in the computation graph.",
    "start": "668645",
    "end": "670850"
  },
  {
    "text": "So our computation graphs are still gro- gro- going to grow, uh,",
    "start": "670850",
    "end": "674074"
  },
  {
    "text": "exponentially but the, uh,",
    "start": "674075",
    "end": "676760"
  },
  {
    "text": "but the point will be that,",
    "start": "676760",
    "end": "678785"
  },
  {
    "text": "uh, their fan out,",
    "start": "678785",
    "end": "680180"
  },
  {
    "text": "will be- will be, uh,",
    "start": "680180",
    "end": "681740"
  },
  {
    "text": "upper bounded by H. So the- the growth won't be uh,",
    "start": "681740",
    "end": "685490"
  },
  {
    "text": "that bad or that fast and we'll still be able to go,",
    "start": "685490",
    "end": "688279"
  },
  {
    "text": "uh, quite, uh, deep.",
    "start": "688280",
    "end": "690560"
  },
  {
    "text": "Now, let me make a few,",
    "start": "690560",
    "end": "692840"
  },
  {
    "text": "uh, remarks about these.",
    "start": "692840",
    "end": "694040"
  },
  {
    "text": "Uh, first remark is that there is, uh, the trade-off,",
    "start": "694040",
    "end": "696829"
  },
  {
    "text": "uh, in- in how many neighbors do we sample, right?",
    "start": "696830",
    "end": "699860"
  },
  {
    "text": "The smaller, uh, H leads to more efficient neighborhood aggregation",
    "start": "699860",
    "end": "703880"
  },
  {
    "text": "because computation graphs would be smaller but results in more unstable,",
    "start": "703880",
    "end": "708065"
  },
  {
    "text": "uh, training because we are ignoring",
    "start": "708065",
    "end": "710420"
  },
  {
    "text": "entire subparts of the network when we are doing message aggregation.",
    "start": "710420",
    "end": "714380"
  },
  {
    "text": "So our, uh, um, uh,",
    "start": "714380",
    "end": "717110"
  },
  {
    "text": "gradient estimates will be more, uh,",
    "start": "717110",
    "end": "719120"
  },
  {
    "text": "noisy, they will have higher, uh, variance.",
    "start": "719120",
    "end": "721790"
  },
  {
    "text": "Uh, another thing is that in terms of computation time,",
    "start": "721790",
    "end": "725255"
  },
  {
    "text": "even the neighborhood sampling,",
    "start": "725255",
    "end": "727085"
  },
  {
    "text": "the size of the computational graph,",
    "start": "727085",
    "end": "729035"
  },
  {
    "text": "as I said is still exponential with respect to the number of layers but",
    "start": "729035",
    "end": "733220"
  },
  {
    "text": "if H is not that large and we don't go too deep in terms of K,",
    "start": "733220",
    "end": "737704"
  },
  {
    "text": "uh, then it is still man-, uh,",
    "start": "737705",
    "end": "739230"
  },
  {
    "text": "uh, uh, manageable, right?",
    "start": "739230",
    "end": "742235"
  },
  {
    "text": "Um, so, you know, adding one more layer to the, uh,",
    "start": "742235",
    "end": "745910"
  },
  {
    "text": "to the GNN makes the computation H times,",
    "start": "745910",
    "end": "749389"
  },
  {
    "text": "uh, more expensive and now,",
    "start": "749390",
    "end": "750935"
  },
  {
    "text": "you know, if, uh,",
    "start": "750935",
    "end": "752150"
  },
  {
    "text": "H is maybe an order of 5-10,",
    "start": "752150",
    "end": "755120"
  },
  {
    "text": "and K is also, I don't know, on order of, you know,",
    "start": "755120",
    "end": "757820"
  },
  {
    "text": "5 plus minus, then you can still,",
    "start": "757820",
    "end": "760325"
  },
  {
    "text": "uh, keep, uh, doing this.",
    "start": "760325",
    "end": "762425"
  },
  {
    "text": "And then, uh, the last, uh,",
    "start": "762425",
    "end": "764855"
  },
  {
    "text": "the last important thing I want to mention is remark number 3 which is",
    "start": "764855",
    "end": "768500"
  },
  {
    "text": "this- this approach gives you a lot of freedom how you select the nodes,",
    "start": "768500",
    "end": "772820"
  },
  {
    "text": "uh, to sample and so far I don't- I call it a random sampling, right?",
    "start": "772820",
    "end": "777680"
  },
  {
    "text": "Just uniformly at random pick e- at H neighbors,",
    "start": "777680",
    "end": "781339"
  },
  {
    "text": "uh, of a given node.",
    "start": "781340",
    "end": "783350"
  },
  {
    "text": "Uh, but the- the issue is with this approach is that,",
    "start": "783350",
    "end": "787175"
  },
  {
    "text": "uh, real-world networks have these highly skewed degree distributions.",
    "start": "787175",
    "end": "790580"
  },
  {
    "text": "So there is a lot of kind of single- single nodes or low degree nodes in the network.",
    "start": "790580",
    "end": "796250"
  },
  {
    "text": "And if you are taking an, uh,",
    "start": "796250",
    "end": "798935"
  },
  {
    "text": "a given node of interest,",
    "start": "798935",
    "end": "800090"
  },
  {
    "text": "and sample H of its neighbors,",
    "start": "800090",
    "end": "801980"
  },
  {
    "text": "you are most likely going to sample this like degree 1, uh,",
    "start": "801980",
    "end": "805279"
  },
  {
    "text": "nodes that are not perhaps the most important nodes in",
    "start": "805280",
    "end": "807680"
  },
  {
    "text": "the network and perhaps very noisy, not the most informative.",
    "start": "807680",
    "end": "811055"
  },
  {
    "text": "This could be users that are not very engaged,",
    "start": "811055",
    "end": "813605"
  },
  {
    "text": "this could be pieces of content that are",
    "start": "813605",
    "end": "815540"
  },
  {
    "text": "not very important and you don't have a good signal on.",
    "start": "815540",
    "end": "818644"
  },
  {
    "text": "So what you can do,",
    "start": "818645",
    "end": "820130"
  },
  {
    "text": "and this works much better in practice,",
    "start": "820130",
    "end": "821840"
  },
  {
    "text": "is that you'll do a random walk with restart from the node of interest,",
    "start": "821840",
    "end": "826940"
  },
  {
    "text": "here, the green node.",
    "start": "826940",
    "end": "828320"
  },
  {
    "text": "And then your sampling strategy is to take, uh, uh,",
    "start": "828320",
    "end": "831935"
  },
  {
    "text": "H neighbors but not at- let's say not at random,",
    "start": "831935",
    "end": "836000"
  },
  {
    "text": "but based on, uh,",
    "start": "836000",
    "end": "837365"
  },
  {
    "text": "their random walk with restart scores.",
    "start": "837365",
    "end": "839945"
  },
  {
    "text": "So it means that at every layer you are going to take H most important neighbors,",
    "start": "839945",
    "end": "845359"
  },
  {
    "text": "uh, of a given node.",
    "start": "845359",
    "end": "846920"
  },
  {
    "text": "And, uh this means that the graph",
    "start": "846920",
    "end": "850399"
  },
  {
    "text": "you are going to select will be kind of much more representative,",
    "start": "850400",
    "end": "853820"
  },
  {
    "text": "much better connected, um,",
    "start": "853820",
    "end": "855830"
  },
  {
    "text": "and it will have- it will be based on",
    "start": "855830",
    "end": "857810"
  },
  {
    "text": "these more important nodes which have better and more reliable feature information,",
    "start": "857810",
    "end": "862279"
  },
  {
    "text": "these are more active users,",
    "start": "862279",
    "end": "863735"
  },
  {
    "text": "so they kind of provide you more information for predictions.",
    "start": "863735",
    "end": "866779"
  },
  {
    "text": "So in practice, this strategy of sampling the computation graph,",
    "start": "866780",
    "end": "870980"
  },
  {
    "text": "um, works much better,",
    "start": "870980",
    "end": "873454"
  },
  {
    "text": "um, and, you know, there is- I think here to say,",
    "start": "873455",
    "end": "876020"
  },
  {
    "text": "there is room to do a proper investigation about",
    "start": "876020",
    "end": "879545"
  },
  {
    "text": "how would you define what are different strategies to sample,",
    "start": "879545",
    "end": "883910"
  },
  {
    "text": "to define the computation graphs,",
    "start": "883910",
    "end": "885589"
  },
  {
    "text": "to sample the computation graphs and ha-",
    "start": "885590",
    "end": "888125"
  },
  {
    "text": "what are their strategies in which kind of cases?",
    "start": "888125",
    "end": "891050"
  },
  {
    "text": "I think this still hasn't been, uh,",
    "start": "891050",
    "end": "892700"
  },
  {
    "text": "systematically investigated but such a study would be very important,",
    "start": "892700",
    "end": "896990"
  },
  {
    "text": "uh, for the field of,",
    "start": "896990",
    "end": "898459"
  },
  {
    "text": "uh, graph machine learning.",
    "start": "898460",
    "end": "900860"
  },
  {
    "text": "So, uh, to summarize the pro- the neighborhood sampling approach,",
    "start": "900860",
    "end": "906079"
  },
  {
    "text": "the idea is that the computational graph is constructed for each node, um,",
    "start": "906080",
    "end": "910340"
  },
  {
    "text": "and the computational graphs are put into the mini- mini-batch because",
    "start": "910340",
    "end": "914540"
  },
  {
    "text": "computational graphs can become very big very",
    "start": "914540",
    "end": "916970"
  },
  {
    "text": "quickly by hitting a high degree hub node, um,",
    "start": "916970",
    "end": "920180"
  },
  {
    "text": "neighborhoods- we then proposed neighborhood sampling",
    "start": "920180",
    "end": "922910"
  },
  {
    "text": "which is where the computational graph is created",
    "start": "922910",
    "end": "925730"
  },
  {
    "text": "stochastically or is pruned sub-sampled to increase computational efficiency.",
    "start": "925730",
    "end": "931750"
  },
  {
    "text": "It also increases the model robustness because now the GNN architecture is,",
    "start": "931750",
    "end": "936620"
  },
  {
    "text": "uh, stochastic by it's self,",
    "start": "936620",
    "end": "938270"
  },
  {
    "text": "so it's almost like a form of dropout if you want to think of it that way, uh,",
    "start": "938270",
    "end": "942020"
  },
  {
    "text": "and the computa- pruned computational graph is used to generate node embeddings.",
    "start": "942020",
    "end": "947315"
  },
  {
    "text": "Um, here, uh, caveat is that if- if your network,",
    "start": "947315",
    "end": "951860"
  },
  {
    "text": "uh, GNN- number of GNN layers is very deep,",
    "start": "951860",
    "end": "955040"
  },
  {
    "text": "these computation graphs may still become large which means",
    "start": "955040",
    "end": "958040"
  },
  {
    "text": "your batch sizes will have to be smaller, um,",
    "start": "958040",
    "end": "961009"
  },
  {
    "text": "which means, uh, your gradient will be, um,",
    "start": "961010",
    "end": "963890"
  },
  {
    "text": "uh, uh, kind of more, uh, less reliable.",
    "start": "963890",
    "end": "967445"
  },
  {
    "text": "So if the batch size is small,",
    "start": "967445",
    "end": "970375"
  },
  {
    "text": "the, uh, the gradient is less reliable,",
    "start": "970375",
    "end": "974065"
  },
  {
    "text": "and if the pruning is too much,",
    "start": "974065",
    "end": "976660"
  },
  {
    "text": "then again the, uh,",
    "start": "976660",
    "end": "979879"
  },
  {
    "text": "gradient, uh, gradients are not too reliable.",
    "start": "979880",
    "end": "982460"
  },
  {
    "text": "So it's important to find a good balance between the batch size and, uh,",
    "start": "982460",
    "end": "985870"
  },
  {
    "text": "the pruning factor or sampling factor for the computation graphs.",
    "start": "985870",
    "end": "990505"
  },
  {
    "text": "But that's essentially the idea and this is really, I would say,",
    "start": "990505",
    "end": "993295"
  },
  {
    "text": "what most of the large-scale industrial implementations of graph neural networks use,",
    "start": "993295",
    "end": "997850"
  },
  {
    "text": "uh, to achieve, uh,",
    "start": "997850",
    "end": "999334"
  },
  {
    "text": "scaling gap to industrial size graphs.",
    "start": "999335",
    "end": "1001660"
  },
  {
    "text": "For example, this is what is used at Pinterest,",
    "start": "1001660",
    "end": "1003480"
  },
  {
    "text": "at Alibaba and so on.",
    "start": "1003480",
    "end": "1005920"
  }
]