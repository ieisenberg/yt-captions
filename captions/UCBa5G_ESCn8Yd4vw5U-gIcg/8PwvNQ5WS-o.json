[
  {
    "start": "0",
    "end": "5400"
  },
  {
    "text": "Hi, everybody. Welcome back. We're going to be talking more\nabout policy gradient methods today. And we're going to start off\nwith a quick Refresh Your",
    "start": "5400",
    "end": "12360"
  },
  {
    "text": "Understanding. ",
    "start": "12360",
    "end": "62090"
  },
  {
    "text": "All right. Let's go ahead and\ngo through these. So everybody said the last thing\nwas false, which is correct.",
    "start": "62090",
    "end": "67770"
  },
  {
    "text": "It is not guaranteed\nto converge. They're not guaranteed to\nconverge to a global optima. They're just guaranteed to\nconverge to a local optima",
    "start": "67770",
    "end": "74368"
  },
  {
    "text": "of the policy gradient space. The first one is true. ",
    "start": "74368",
    "end": "82100"
  },
  {
    "text": "There are different\nways to write this down, but in general,\nwhat we're doing is we're going to be trying to\nfind, take steps in the policy",
    "start": "82100",
    "end": "89240"
  },
  {
    "text": "parameterization space. We're parameterizing\nour policies by theta, so that we're going\nto be trying to move",
    "start": "89240",
    "end": "96620"
  },
  {
    "text": "in the direction of the log\nof the policy parameters times their value, the return\nyou get from them.",
    "start": "96620",
    "end": "105560"
  },
  {
    "text": "The second one is false. There's a bit of\ndisagreement over this.",
    "start": "105560",
    "end": "110870"
  },
  {
    "text": "So because you can see\nfrom this first derivative, we are going to look at the\ndirection of the derivative",
    "start": "110870",
    "end": "118670"
  },
  {
    "text": "with respect to theta of the\nlog of the policy parameters. But it's weighted by the return,\nor weighted by the Q-function.",
    "start": "118670",
    "end": "125520"
  },
  {
    "text": "So whether we push it up or\nnot will depend whether or not we're getting high rewards\nwhen we go in that direction.",
    "start": "125520",
    "end": "132890"
  },
  {
    "text": "So this one is false. And this one is also true.",
    "start": "132890",
    "end": "139280"
  },
  {
    "text": "But in general, what\nwe're trying to do is we're trying to find parts\nof the policy space, such",
    "start": "139280",
    "end": "144720"
  },
  {
    "text": "that when we follow\nthat policy, we visit states and actions,\nwhich have higher estimated",
    "start": "144720",
    "end": "151260"
  },
  {
    "text": "Q-function, or higher\nestimated rewards. Do you have a question?",
    "start": "151260",
    "end": "157150"
  },
  {
    "text": "OK.  Great.",
    "start": "157150",
    "end": "162950"
  },
  {
    "text": "All right.  So last time, we started talking\nabout policy search, which",
    "start": "162950",
    "end": "170555"
  },
  {
    "text": "was this idea of saying we're\ngoing to be directly trying to search in the policy\nparameterized by some theta.",
    "start": "170555",
    "end": "176930"
  },
  {
    "text": "This could be a\nGaussian policy class. This could be softmax, or this\ncould be, as it will often be,",
    "start": "176930",
    "end": "182450"
  },
  {
    "text": "a deep neural network. And what we're going\nto talk about today is we're going to get\nto finish off that part,",
    "start": "182450",
    "end": "188720"
  },
  {
    "text": "and then talk about more\nadvanced policy gradient methods. And in particular,\ntoday we're going to cover at least\nthe majority of PPO.",
    "start": "188720",
    "end": "195780"
  },
  {
    "text": "So this should be\nenough for you to be making significant\nprogress on Homework 2. ",
    "start": "195780",
    "end": "203200"
  },
  {
    "text": "So in particular, what we're\ngoing to be covering is, we've talked last time a lot\nabout likelihood ratio or score",
    "start": "203200",
    "end": "208270"
  },
  {
    "text": "function policy gradients. We're going to talk more\nabout the notion of a baseline and why introducing\nthat is not going",
    "start": "208270",
    "end": "214930"
  },
  {
    "text": "to incur any bias in the\nestimate of our gradient. We'll talk about\nalternative targets,",
    "start": "214930",
    "end": "220299"
  },
  {
    "text": "and then we're going\nto talk about PPO. And again, just to\nremind ourselves, PPO is what they used in\nChatGPT and a huge number",
    "start": "220300",
    "end": "227000"
  },
  {
    "text": "of other application\nareas, as well. So it's a really,\nreally useful technique. All right.",
    "start": "227000",
    "end": "232380"
  },
  {
    "text": "So let's just\nremind ourselves, we talked about how we can take\na derivative with respect to the value of a\nparticular policy.",
    "start": "232380",
    "end": "239470"
  },
  {
    "text": "So this was the\npolicy parameters. And we showed that it\ncould look like this.",
    "start": "239470",
    "end": "249019"
  },
  {
    "text": "And this was an unbiased\nestimate of the gradient, but it could be very noisy, in\npart because it looks something",
    "start": "249020",
    "end": "256338"
  },
  {
    "text": "like our Monte Carlo estimates\nthat we saw before because we're looking at these returns.",
    "start": "256339",
    "end": "261980"
  },
  {
    "text": "And so we talked about a\ncouple of different fixes, or we started to talk about\nfixes to make it tractable. So one was to leverage\nthe temporal structure,",
    "start": "261980",
    "end": "269100"
  },
  {
    "text": "meaning that your reward\non time step three can't depend on your\nactions after time step 3.",
    "start": "269100",
    "end": "274820"
  },
  {
    "text": "And so we could\nuse that to reduce the variance of our estimator. And now, the next thing\nwe're going to talk about--",
    "start": "274820",
    "end": "280470"
  },
  {
    "text": "we start talking about this\nlast time-- is baseline. So as we talk\nabout this, I think",
    "start": "280470",
    "end": "286229"
  },
  {
    "text": "it's just useful to keep\nin mind throughout this that we're always trying to\nconverge as quickly as possible.",
    "start": "286230",
    "end": "291390"
  },
  {
    "text": "So we want these\nestimates of the gradient to be as low\nvariance as possible so we can try to be taking\nbetter steps in our policy",
    "start": "291390",
    "end": "298745"
  },
  {
    "text": "space.  All right. So let's look at the baseline.",
    "start": "298745",
    "end": "304849"
  },
  {
    "text": "We started talking\nabout this before and we said, well,\nwhen we are thinking about how to move our policy\ninside of the policy space,",
    "start": "304850",
    "end": "312810"
  },
  {
    "text": "we want to think about, not just\nhow much reward we're getting, but really, maybe\nhow much reward we're getting relative to\nother things we could be doing.",
    "start": "312810",
    "end": "319818"
  },
  {
    "text": "So we want to know how\nmuch better this policy is compared to other stuff. And I said, you could introduce\nthis baseline, b of st,",
    "start": "319818",
    "end": "329000"
  },
  {
    "text": "which was only a\nfunction of the state. So note, not a function\nof a, of theta or a.",
    "start": "329000",
    "end": "342150"
  },
  {
    "text": "Now, there's been other\nwork, including from my lab, thinking about whether\nwe can introduce baselines that may be a function\nof something beyond the state.",
    "start": "342150",
    "end": "348850"
  },
  {
    "text": "But for today, we're just\ngoing to assume that it's only a function of the state. And what we're\ngoing to prove now is that, for any\nchoice of the baseline,",
    "start": "348850",
    "end": "355900"
  },
  {
    "text": "as long as it's only a\nfunction of the state, this gradient\nestimator is unbiased, which means that we could\nintroduce this here,",
    "start": "355900",
    "end": "362620"
  },
  {
    "text": "and we're not changing, on\naverage, what the gradient estimator is.",
    "start": "362620",
    "end": "368490"
  },
  {
    "text": "And a near optimal choice is\ngoing to be the expected return. So now, we're going\nto, again, just",
    "start": "368490",
    "end": "374820"
  },
  {
    "text": "be trying to think about\nhow much better is taking the actions under this\ncurrent policy compared to other things we could do.",
    "start": "374820",
    "end": "382620"
  },
  {
    "text": "So let's see. We're going to step through\nwhy adding a baseline does not incur any bias in\nour estimate of the gradient.",
    "start": "382620",
    "end": "391259"
  },
  {
    "text": "So what we're going\nto do this is we're going to think about how\nour gradient comes together. ",
    "start": "391260",
    "end": "399513"
  },
  {
    "text": "OK. So remember, tau here\nis our trajectories, and then this is our gradient.",
    "start": "399513",
    "end": "406900"
  },
  {
    "text": "So the goal is to show\nthis is equal to 0.",
    "start": "406900",
    "end": "414820"
  },
  {
    "text": "Why is that? Because if we think\nabout what this term was, this first term was an\nestimate, unbiased estimate",
    "start": "414820",
    "end": "421870"
  },
  {
    "text": "of the gradient. And now we've subtracted off\nthis term times this term, and we want to show that, in\nexpectation, subtracting off",
    "start": "421870",
    "end": "429610"
  },
  {
    "text": "that term is 0, which means that\nwe didn't introduce any bias. OK, so let's just step\nthrough how that works.",
    "start": "429610",
    "end": "436190"
  },
  {
    "text": "So the goal is to\nshow this as 0, and we're just going\nto step through this. So our expectation is over tau. Tau is our trajectory, so\nlet's just write it out.",
    "start": "436190",
    "end": "443660"
  },
  {
    "text": "You can write it\nout as the states we've seen up to a time\nstep t plus the states",
    "start": "443660",
    "end": "450380"
  },
  {
    "text": "that we see from that\ntime step onwards because that's like we're just\nwriting out our full trajectory.",
    "start": "450380",
    "end": "455820"
  },
  {
    "text": "So we can just think\nof our trajectory here as being s0 to t\nand a0 to t minus 1.",
    "start": "455820",
    "end": "465080"
  },
  {
    "text": "And that's just the\nfull trajectory. So that's tau. And so we're just going to\ndecompose this expectation.",
    "start": "465080",
    "end": "470630"
  },
  {
    "text": "So we break this up. And after we've\ndone that, we can notice that we can pull\none of the terms out.",
    "start": "470630",
    "end": "482815"
  },
  {
    "text": " I'm going to pull this out\nbecause this is not a function",
    "start": "482815",
    "end": "490009"
  },
  {
    "text": "of this future expectation. ",
    "start": "490010",
    "end": "501610"
  },
  {
    "text": "And I could pull that out\nthere because this is just a function of the\ncurrent state, and it doesn't depend on the future\nstate, so the future actions",
    "start": "501610",
    "end": "508270"
  },
  {
    "text": "that I take. All right. So that's what I did. And then next, I'm\ngoing to notice that, well, this\nterm here is only",
    "start": "508270",
    "end": "514029"
  },
  {
    "text": "a function of the\nstate and the action. It's not, again, a function\nof all the future actions and future states.",
    "start": "514030",
    "end": "519539"
  },
  {
    "text": "So we can just rewrite that as\nexpectation over the action t.",
    "start": "519539",
    "end": "528380"
  },
  {
    "start": "528380",
    "end": "543200"
  },
  {
    "text": "All right. So what are we going to do next? The next thing we're\ngoing to do is, I'm just going to write\nthis out more fully.",
    "start": "543200",
    "end": "550310"
  },
  {
    "text": "So I'll repeat this. ",
    "start": "550310",
    "end": "555890"
  },
  {
    "text": "So we've got our\nbaseline here, st, and we're going to write out\nwhat this expectation is. This is an expectation\nover the actions.",
    "start": "555890",
    "end": "562830"
  },
  {
    "text": "What actions are we taking? We're exactly taking the\nactions according to our policy. ",
    "start": "562830",
    "end": "577710"
  },
  {
    "text": "So I've just rewritten\nwhat that expectation is. The expectation we're\ntaking over actions is exactly the probability\nwe take each action according",
    "start": "577710",
    "end": "585050"
  },
  {
    "text": "to our current policy. OK, once we have that, we can\nplay the likelihood ratio,",
    "start": "585050",
    "end": "591170"
  },
  {
    "text": "or we can think of what\nis this derivative. This derivative is\nequal to bst sum over a.",
    "start": "591170",
    "end": "601682"
  },
  {
    "text": "The derivative of\nlog is just going to be the derivative of\nthe things inside divide it",
    "start": "601682",
    "end": "611490"
  },
  {
    "text": "by this one I should have added.",
    "start": "611490",
    "end": "617529"
  },
  {
    "text": "Let me put a theta in here to\nmake it clear that all of this is a function of\nmy current theta.",
    "start": "617530",
    "end": "623589"
  },
  {
    "text": "So I just took the derivative\nwith respect to the log. But when we see that, we\nrealize we can cross those out.",
    "start": "623590",
    "end": "629500"
  },
  {
    "text": "So we can cross off this. We can cross off this\nbecause that's the same. So now what do we have?",
    "start": "629500",
    "end": "635020"
  },
  {
    "text": "We have the expectation\nover states. ",
    "start": "635020",
    "end": "640300"
  },
  {
    "text": "b of st, sum over a, derivative. Remember, we're taking the\nderivative with respect",
    "start": "640300",
    "end": "647080"
  },
  {
    "text": "to theta pi of at st theta.",
    "start": "647080",
    "end": "653110"
  },
  {
    "text": "So this is what this\nlooks like so far. And now what I'm\ngoing to do is I'm going to switch the\nderivative and the sum.",
    "start": "653110",
    "end": "659589"
  },
  {
    "text": "OK? So if I'm going to say this is\nb of st, derivative of theta,",
    "start": "659590",
    "end": "665440"
  },
  {
    "text": "sum over a. ",
    "start": "665440",
    "end": "671709"
  },
  {
    "text": "OK, why was that important? Because now, and let me just-- ",
    "start": "671710",
    "end": "679740"
  },
  {
    "text": "but we know here that the sum\nover all actions we could take at this time step\nhas to sum to 1,",
    "start": "679740",
    "end": "686120"
  },
  {
    "text": "because that's true\nfor any policy. So this must equal\n1, which means",
    "start": "686120",
    "end": "695210"
  },
  {
    "text": "we have b of st of the\nderivative of theta of 1, which is equal to 0,\nbecause that's a constant.",
    "start": "695210",
    "end": "701635"
  },
  {
    "text": " Let me just show it here\nbecause it's more neat.",
    "start": "701635",
    "end": "709855"
  },
  {
    "text": "So there's two main insights\nfor how we did this proof. The first was we thought\nabout our expectation",
    "start": "709856",
    "end": "716240"
  },
  {
    "text": "over all trajectories,\nand we broke it up to the part of the\ntrajectory that happened before, the state of interest,\nthe st, and the part that",
    "start": "716240",
    "end": "723620"
  },
  {
    "text": "happened afterwards. After we did that,\nwe showed that we could rewrite that expectation\njust in terms of at,",
    "start": "723620",
    "end": "730727"
  },
  {
    "text": "because we didn't care\nabout all the future stuff. This only depends on at and st.\nAnd then we take the derivative,",
    "start": "730727",
    "end": "737660"
  },
  {
    "text": "and then we can see that\nwe can switch these, and then that just becomes 1. And it's a constant that\ndoesn't depend on theta.",
    "start": "737660",
    "end": "744260"
  },
  {
    "text": "So the derivative with\nrespect to theta is 0. And so that is why introducing\na baseline that only depends",
    "start": "744260",
    "end": "752360"
  },
  {
    "text": "on the state does not introduce\nany bias because an expectation, its value is 0.",
    "start": "752360",
    "end": "758089"
  },
  {
    "text": "So that allows us to\nderive what we often call a sort of a vanilla\npolicy gradient method, which incorporates both the temporal\nstructure and a baseline.",
    "start": "758090",
    "end": "766680"
  },
  {
    "text": "And so the idea in\nthis case is that we're going to collect\na set of-- we're going to take our\ncurrent policy, which is parameterized by theta.",
    "start": "766680",
    "end": "772150"
  },
  {
    "text": "We're going to roll it\nout a number of times, and then we go for each time\nstep t in each trajectory.",
    "start": "772150",
    "end": "779168"
  },
  {
    "text": "We're going to\ncompute the return, just like our Monte\nCarlo estimate. And then we can compute\nthe advantage estimate,",
    "start": "779168",
    "end": "784870"
  },
  {
    "text": "which is we take that current\nreturn from that state till the end of the\nepisode minus our baseline.",
    "start": "784870",
    "end": "792390"
  },
  {
    "text": "And someone asked me\nabout this last time. Generally, we're going\nto change refitting the baseline each time.",
    "start": "792390",
    "end": "798040"
  },
  {
    "text": "So we can re estimate\nthe baseline. Again, it doesn't matter what\nwe pick for the baseline.",
    "start": "798040",
    "end": "804190"
  },
  {
    "text": "It will always be\nunbiased, but there will be better or worse choices. So you can imagine,\nif the baseline is 0,",
    "start": "804190",
    "end": "809830"
  },
  {
    "text": "it will never make\nany difference. The goal is to,\nhopefully, have a baseline that's pretty informative\nand has a value close",
    "start": "809830",
    "end": "815970"
  },
  {
    "text": "to the value of your policy. And so then we'll update the\npolicy using our policy gradient",
    "start": "815970",
    "end": "821550"
  },
  {
    "text": "estimate, which is a\nsum of these terms. So we're going to use all\nof these terms, where we've",
    "start": "821550",
    "end": "830430"
  },
  {
    "text": "got that derivative with\nrespect to the log of the policy parameters times our\nadvantage, and then repeat.",
    "start": "830430",
    "end": "838260"
  },
  {
    "text": "So this is a vanilla\npolicy gradient algorithm. Yeah.",
    "start": "838260",
    "end": "843639"
  },
  {
    "text": "There is no discount\nfactor in the return. Is that intentional, or what? Yeah, good question.",
    "start": "843640",
    "end": "849860"
  },
  {
    "text": "So right now, the question is,\nthere's no discount factor. There's no discount\nfactor right now because we're assuming we're\nin the fully episodic case,",
    "start": "849860",
    "end": "856533"
  },
  {
    "text": "so we don't have to\nhave a discount factor. You could certainly\ninclude one if you want to. Yeah, so for right now here, we\ndon't have a discount factor.",
    "start": "856533",
    "end": "864400"
  },
  {
    "text": "Now, one thing that\nyou might think about when you're\nstarting to look at this is to say, well, a lot of this\nfeels like the Monte Carlo",
    "start": "864400",
    "end": "869740"
  },
  {
    "text": "estimation that we did\nearlier in the class. We've been using these G\nestimators, this return,",
    "start": "869740",
    "end": "878230"
  },
  {
    "text": "to estimate what\nis the performance of the policy from this time\nstep to the end of the episode. But as you might\nimagine in this case,",
    "start": "878230",
    "end": "885680"
  },
  {
    "text": "that generally is a\npretty noisy estimate. So then the question is going\nto be, could we maybe do better.",
    "start": "885680",
    "end": "891915"
  },
  {
    "text": " So there's two places\nwe could imagine.",
    "start": "891915",
    "end": "898262"
  },
  {
    "text": "I guess there's two things\nhere that we could imagine plugging in other choices for. There is what is the\nreturn of the policy",
    "start": "898262",
    "end": "905930"
  },
  {
    "text": "from a particular action\ntill the end of the episode, and what is my general\nestimate of the performance",
    "start": "905930",
    "end": "911480"
  },
  {
    "text": "in that state.  So one thing you\ncan imagine here is, if we think back to Q\nfunctions and value functions,",
    "start": "911480",
    "end": "919230"
  },
  {
    "text": "maybe we could plug those in,\ninstead of using the return and using a generic baseline. So you could plug in--\ninstead of saying,",
    "start": "919230",
    "end": "926533"
  },
  {
    "text": "what is the return\nfrom this state and action to the\nend of the episode, you could imagine\nplugging in the Q value for the current policy\nfrom the state and action",
    "start": "926533",
    "end": "934460"
  },
  {
    "text": "to the end of the episode. And we can either make\ngamma equal to 1 or not. We're going to generally\nassume no for now.",
    "start": "934460",
    "end": "942920"
  },
  {
    "text": "Assume episodic. So you can set gamma equal to 1.",
    "start": "942920",
    "end": "953560"
  },
  {
    "text": "And the state value function\ncould be a good baseline. ",
    "start": "953560",
    "end": "960330"
  },
  {
    "text": "And just to remember\nhere, on this slide, you can think of G as kind\nof being like a Q function, and V as being a value function.",
    "start": "960330",
    "end": "966329"
  },
  {
    "text": "So this would be an\nalternative we could do. OK, so let's think about how we\ngenerally could reduce variance.",
    "start": "966330",
    "end": "974922"
  },
  {
    "text": "So what we've seen so\nfar is we're mostly using Monte Carlo like returns. Now, let's see if we\ncan do something better.",
    "start": "974922",
    "end": "980155"
  },
  {
    "text": " So one thing we\ncan do now is we're going to try to plug in and use\nthings like state action values.",
    "start": "980155",
    "end": "988730"
  },
  {
    "text": "And this is where the idea\nof actor critic methods come in, which are also really\npopular in reinforcement learning.",
    "start": "988730",
    "end": "995760"
  },
  {
    "text": "So the idea here\nis that we could reduce the variance of\nthis estimate of the value",
    "start": "995760",
    "end": "1001490"
  },
  {
    "text": "function at a single st\nfrom a single rollout by bootstrapping or doing\nfunction approximation",
    "start": "1001490",
    "end": "1008390"
  },
  {
    "text": "at that point. So you could think back to\ndeep Q-learning or something like that as a way for us to\napproximate what the value might",
    "start": "1008390",
    "end": "1015110"
  },
  {
    "text": "be, or just general sort of deep\nlearning for the value function.",
    "start": "1015110",
    "end": "1021450"
  },
  {
    "text": "So when we do this,\nwe end up with what is called actor critic methods. The idea is that the\nactor is the policy.",
    "start": "1021450",
    "end": "1028890"
  },
  {
    "text": "So the actor is the policy\noften parameterized by theta.",
    "start": "1028890",
    "end": "1037750"
  },
  {
    "text": "And the value function, or the\nstate action value function, is the critic.",
    "start": "1037750",
    "end": "1043630"
  },
  {
    "text": "And it's representing\na V or a Q function. So that's why they're\ncalled actor critic.",
    "start": "1043630",
    "end": "1048790"
  },
  {
    "text": "Actor is our policy\nparameterization. Critic is our\nstate action value. And the great thing\nis that we can",
    "start": "1048790",
    "end": "1055360"
  },
  {
    "text": "use both of those inside of\na policy gradient algorithm. So you are constantly updating\nan estimate of the state action",
    "start": "1055360",
    "end": "1063100"
  },
  {
    "text": "value, as well as having\nan explicit policy parameterization. And you use them together to,\nhopefully, increase the rate",
    "start": "1063100",
    "end": "1069909"
  },
  {
    "text": "at which we learn to\nget a good policy. Now, in this case, normally\nwhat we're doing here is we're gathering\ndata using the policy.",
    "start": "1069910",
    "end": "1076850"
  },
  {
    "text": "And then we're using that\ndata to fit a critic. And the reason we\ncall it a critic",
    "start": "1076850",
    "end": "1083080"
  },
  {
    "text": "is because the critic\nis sort of trying to estimate the performance\nof an explicit representation",
    "start": "1083080",
    "end": "1089440"
  },
  {
    "text": "of the performance\nof the policy. So the actor makes\ndecisions, and the critic says that's how good it was.",
    "start": "1089440",
    "end": "1095370"
  },
  {
    "text": "So that's why it's\ncalled actor critic. A3C is a pretty popular\nactor critic method.",
    "start": "1095370",
    "end": "1100570"
  },
  {
    "text": "There's quite a lot of others. So many of the reinforcement\nlearning algorithms will end up being essentially\nactor critic algorithms.",
    "start": "1100570",
    "end": "1106470"
  },
  {
    "text": "And so it'll be useful to\nhave both representations. So if you think\nof it, you'd have a sort of a deep neural network\nto represent your policy,",
    "start": "1106470",
    "end": "1112490"
  },
  {
    "text": "and you'd have a deep neural\nnetwork, a separate one-- you could share parameters,\nbut you don't have to-- to represent\nyour value function.",
    "start": "1112490",
    "end": "1120230"
  },
  {
    "text": "All right. Once we do that, we can think\nof rewriting our policy gradient",
    "start": "1120230",
    "end": "1125520"
  },
  {
    "text": "formulas. So this was what we had before. We could approximate\nthis now as saying, well,",
    "start": "1125520",
    "end": "1132340"
  },
  {
    "text": "what if we just plugged in\ninstead of that return g, which is that sum over the rewards,\nwe plugged in a Q function,",
    "start": "1132340",
    "end": "1138720"
  },
  {
    "text": "and we plugged in a\nparameterized Q function with a parameter w. So these were our weights.",
    "start": "1138720",
    "end": "1145020"
  },
  {
    "text": "So now, just to\nhighlight here, we're going to have these two sets\nof parameters w and theta,",
    "start": "1145020",
    "end": "1150150"
  },
  {
    "text": "theta for the policy, w\nfor the value function. And if we let the baseline\nbe an estimate of the V,",
    "start": "1150150",
    "end": "1157779"
  },
  {
    "text": "then we can just directly write\ndown a state action advantage function, where we\nlook at the difference",
    "start": "1157780",
    "end": "1165090"
  },
  {
    "text": "between the Q and the V. So now\nV is serving as our baseline. And I'll just highlight here\nthat using the advantage",
    "start": "1165090",
    "end": "1172110"
  },
  {
    "text": "function was one of\nthe first things that-- I got best paper maybe in 2016.",
    "start": "1172110",
    "end": "1178419"
  },
  {
    "text": "Right after deep Q-learning\nhad started coming out when people thought about\nthese different adaptations,",
    "start": "1178420",
    "end": "1183652"
  },
  {
    "text": "one of the things\nthat was proposed is to think about\ntrying to maximize with respect to advantages.",
    "start": "1183652",
    "end": "1189700"
  },
  {
    "text": "But here, we're going to be\nusing this within a policy gradient approach.",
    "start": "1189700",
    "end": "1195247"
  },
  {
    "text": "Now, one of the things\nyou might wonder here is, OK, well, we've\ngot these extremes. On the one hand, you could\nhave this Monte Carlo return",
    "start": "1195247",
    "end": "1201730"
  },
  {
    "text": "of what is the value\nof the state in action that you get from starting\nthat state in action and rolling out to the\nend of the episode.",
    "start": "1201730",
    "end": "1207799"
  },
  {
    "text": "And the other is you could\nplug in a Q function. Now, there might be some sort\nof blending between these two.",
    "start": "1207800",
    "end": "1215140"
  },
  {
    "text": "So these are known\nas n step estimators. So a critic, in\ngeneral, doesn't have",
    "start": "1215140",
    "end": "1220360"
  },
  {
    "text": "to pick sort of a\ntemporal difference. Temporal difference in the\nway that we've seen it so far is normally-- so I'll\njust write down here.",
    "start": "1220360",
    "end": "1226840"
  },
  {
    "text": "We've seen it as\nTD0, which means we have the immediate reward\nplus gamma times V of s prime.",
    "start": "1226840",
    "end": "1235840"
  },
  {
    "text": "So in TD0, you take you think\nof your immediate reward, and then you plug\nin or bootstrap",
    "start": "1235840",
    "end": "1240850"
  },
  {
    "text": "immediately when you say\nns on the next state. And I'm going to plug-in my\nvalue for that next state.",
    "start": "1240850",
    "end": "1246330"
  },
  {
    "text": "So if you think back to\nour tree representation, it's like you see one\nactual observed return, and then you plug-in\nyour estimate.",
    "start": "1246330",
    "end": "1252940"
  },
  {
    "text": "But in general, you could trade\noff between taking one step or taking two steps,\nor three steps,",
    "start": "1252940",
    "end": "1258130"
  },
  {
    "text": "and then plugging in your\nestimate of the return. So in particular,\nhere's a number",
    "start": "1258130",
    "end": "1265450"
  },
  {
    "text": "of different types of\nestimators you could look at. So let's call this\nr hat 1, which",
    "start": "1265450",
    "end": "1273280"
  },
  {
    "text": "is get your immediate reward. This is what we've seen\nbefore, plus gamma. Then you bootstrap.",
    "start": "1273280",
    "end": "1278410"
  },
  {
    "text": "A second one would be\nyou take your next 2. Again, your gamma can be 1 or\nnot, and then you plug in it.",
    "start": "1278410",
    "end": "1285429"
  },
  {
    "text": "And sort of r hat infinity\nwould be your normal Monte Carlo return, which is you don't\ndo any bootstrapping.",
    "start": "1285430",
    "end": "1291620"
  },
  {
    "text": "You just sum up\nall your rewards. And you can think\nof each of these as being estimates\nof your Q function.",
    "start": "1291620",
    "end": "1298040"
  },
  {
    "text": "And then you could just\nget advantage estimators where you subtract off the V\nof your current state in each",
    "start": "1298040",
    "end": "1303970"
  },
  {
    "text": "of those settings. So those are all\nthings you could do. They're called n\nstep estimators,",
    "start": "1303970",
    "end": "1309650"
  },
  {
    "text": "where n is the number of time\nsteps until you bootstrap. And one of the important\nthings to think about",
    "start": "1309650",
    "end": "1315040"
  },
  {
    "text": "is where you might want to\ntrade off between these ones. So we'll do just a Check\nYour Understanding now.",
    "start": "1315040",
    "end": "1320900"
  },
  {
    "text": "If we think about\nintroducing these type of blended estimators, how does\nbias and variance trade off?",
    "start": "1320900",
    "end": "1329900"
  },
  {
    "text": "So why don't we go\nahead and do that now. ",
    "start": "1329900",
    "end": "1393323"
  },
  {
    "text": "Can you select\nlike more than one? You should be able to. Does that work? ",
    "start": "1393323",
    "end": "1411725"
  },
  {
    "text": "OK, I'll give you one more\nminute to think about it and put it in your answer. And then there's a\nlot of variability",
    "start": "1411725",
    "end": "1418559"
  },
  {
    "text": "in what people are saying. And so why don't you talk to\nyour neighbor in a second? ",
    "start": "1418560",
    "end": "1431240"
  },
  {
    "text": "All right. Turn to a neighbor. Compare what you got. ",
    "start": "1431240",
    "end": "1440720"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1440720",
    "end": "1464845"
  },
  {
    "text": "What do you think about it? [INTERPOSING VOICES] ",
    "start": "1464845",
    "end": "1470844"
  },
  {
    "text": "Yeah?  you are subtracting\nthe V function.",
    "start": "1470844",
    "end": "1477280"
  },
  {
    "text": "But if you think back to\nMonte Carlo and TD methods, did they have the same bias? [INTERPOSING VOICES]",
    "start": "1477280",
    "end": "1483960"
  },
  {
    "start": "1483960",
    "end": "1492320"
  },
  {
    "text": "Well, this is the question. Do these both have\nthe same bias? So if you ignore subtracting\na hundred [INAUDIBLE]",
    "start": "1492320",
    "end": "1500090"
  },
  {
    "text": "the first part of\nthe estimate, do you think the first one has\nhigher bias or the second one?",
    "start": "1500090",
    "end": "1506490"
  },
  {
    "text": "Yeah, I don't know\nwhy it has high bias. If you think back,\nthe first one should be like the temporal\ndifference method,",
    "start": "1506490",
    "end": "1513192"
  },
  {
    "text": "and the second one\nshould be Monte Carlo.  And remember, I guess,\nit should be clear",
    "start": "1513192",
    "end": "1519140"
  },
  {
    "text": "that these Vs are all estimates,\nso they're not converged. So then the top one\nhas higher bias.",
    "start": "1519140",
    "end": "1528240"
  },
  {
    "text": "Exactly. The second one has lower bias\nbecause of the actual value.",
    "start": "1528240",
    "end": "1534150"
  },
  {
    "text": "Exactly, yeah. And then what do you think\nthat means for the variance? I guess higher variance also for\nthe top one because, I guess,",
    "start": "1534150",
    "end": "1545090"
  },
  {
    "text": "because it's an estimate. Close. Yeah, so for the\nfirst one, you're totally right that it's got\nhigher bias because you're",
    "start": "1545090",
    "end": "1551690"
  },
  {
    "text": "immediately bootstrapping. But in general, it will\nhave lower variance. OK. Is that just\ngenerally a trade off?",
    "start": "1551690",
    "end": "1557660"
  },
  {
    "text": "Yeah, yeah. Normally, the Monte\nCarlo methods, because you're summing\nthem up, they're normally totally unbiased.",
    "start": "1557660",
    "end": "1562780"
  },
  {
    "text": "But they have lots of terms. So you could think of there's\nlots of stochasticity. OK. Yeah, and then the other\nway is normally aggressive.",
    "start": "1562780",
    "end": "1568860"
  },
  {
    "text": "I see. OK, that makes sense. Cool. [INTERPOSING VOICES]",
    "start": "1568860",
    "end": "1574779"
  },
  {
    "start": "1574780",
    "end": "1581830"
  },
  {
    "text": "All right. I'm going to ask if people\nall turn this-- yeah, I'm going to ask, did\nit may change their mind after talking to someone?",
    "start": "1581830",
    "end": "1588580"
  },
  {
    "text": "OK, at least a few. Yep, all right. So I think one of\nthe things, too, I just wanted to clarify here\nis that I find it easiest to--",
    "start": "1588580",
    "end": "1596683"
  },
  {
    "text": "I think it's often easiest\nto think about this without subtracting\nthe V, because the V is the same in both of these.",
    "start": "1596683",
    "end": "1602168"
  },
  {
    "text": "So you can just focus to\nunderstand which of them has high variance or high bias. You can just focus\non the first things.",
    "start": "1602168",
    "end": "1608840"
  },
  {
    "text": "And when you look at\njust the first things, it should remind\nyou of Monte Carlo methods versus temporal\ndifferent methods.",
    "start": "1608840",
    "end": "1614740"
  },
  {
    "text": "So the first one has low\nvariance and high bias. Does somebody want to say why?",
    "start": "1614740",
    "end": "1622160"
  },
  {
    "text": "So the first one, which looks\nkind of like a TD0 update, so this one has low variance.",
    "start": "1622160",
    "end": "1633110"
  },
  {
    "text": "So a1, low variance, high bias.",
    "start": "1633110",
    "end": "1639047"
  },
  {
    "text": "Does anyone want to\nshare why that is? ",
    "start": "1639047",
    "end": "1645040"
  },
  {
    "text": "Which is it? Sorry, it wasn't on top. It wasn't, no. ",
    "start": "1645040",
    "end": "1651407"
  },
  {
    "text": "Just to make sure, I'm going to\nexplain what each of them has. But yes. Do you want to say somewhat? Yeah. And remind me your name.",
    "start": "1651407",
    "end": "1658270"
  },
  {
    "text": "I'm not entirely\nsure, but I think that intuition, at least\nwhen I was thinking about it, was that using the actual values\nof, for example, rt plus 1",
    "start": "1658270",
    "end": "1667960"
  },
  {
    "text": "is more accurate, versus if\nyou bootstrap very early, it's more of an estimate.",
    "start": "1667960",
    "end": "1673430"
  },
  {
    "text": "It's not as accurate. Yeah, that's exactly right. So it's the same as in\ntemporal difference methods. In general, it's a\nlittle misleading",
    "start": "1673430",
    "end": "1680485"
  },
  {
    "text": "sometimes to look at these. Maybe I should put hats\nover all these next year. But all of these Vs\nare just estimates.",
    "start": "1680485",
    "end": "1685789"
  },
  {
    "text": "They're given finite amounts of\ndata and however many backups we've done, et cetera. V is an approximation.",
    "start": "1685790",
    "end": "1691280"
  },
  {
    "text": "So this is an estimate. So this isn't true. And if it's not true,\nit's probably biased.",
    "start": "1691280",
    "end": "1698070"
  },
  {
    "text": "So in general, V will not\nbe an unbiased estimator. And so this means that we're\nonly using 1.r from the true",
    "start": "1698070",
    "end": "1707730"
  },
  {
    "text": "policy, and then we're\nimmediately bootstrapping. So in general, this is going\nto be high bias or higher bias,",
    "start": "1707730",
    "end": "1713415"
  },
  {
    "text": "but it's generally going\nto be pretty low variance. And the way to think about this\nis that each of the rewards,",
    "start": "1713415",
    "end": "1719250"
  },
  {
    "text": "in general, are going to be\nfrom a stochastic process because you're taking\na series of steps.",
    "start": "1719250",
    "end": "1724613"
  },
  {
    "text": "And then at each\none, you're going to sample a reward from there. So you only have one\nreally random thing here,",
    "start": "1724613",
    "end": "1731140"
  },
  {
    "text": "and then one thing\nthat is fixed. It might be wrong,\nbut it's fixed.",
    "start": "1731140",
    "end": "1736290"
  },
  {
    "text": "In contrast, this one, which\nlooks like a Monte Carlo estimate, is going to have\nhigh variance because it's",
    "start": "1736290",
    "end": "1744240"
  },
  {
    "text": "got all of these\ndifferent rewards that are all being sampled\nfrom a stochastic process.",
    "start": "1744240",
    "end": "1750039"
  },
  {
    "text": "So if you think of\nit this way, imagine it's something where your robot\ncan walk anywhere over the room.",
    "start": "1750040",
    "end": "1755830"
  },
  {
    "text": "And under your\npolicy, your policy can go in all of these\ndifferent directions.",
    "start": "1755830",
    "end": "1762980"
  },
  {
    "text": "But when you actually just\nexecute one trajectory, you're just going\nto get one of those.",
    "start": "1762980",
    "end": "1768200"
  },
  {
    "text": "And so its variance\ngenerally is enormous. And this might be true\neven if, on average,",
    "start": "1768200",
    "end": "1774679"
  },
  {
    "text": "you kind of have a trajectory\nlike this or something. So in general, this one is going\nto be really high variance,",
    "start": "1774680",
    "end": "1780710"
  },
  {
    "text": "but it's generally going\nto be low or 0 bias. Why is it low or 0 bias?",
    "start": "1780710",
    "end": "1786960"
  },
  {
    "text": "Because this actually is\na return from the policy that you're executing.",
    "start": "1786960",
    "end": "1792140"
  },
  {
    "text": "So in expectation,\nthis really is equal to the value\nof that policy.",
    "start": "1792140",
    "end": "1797620"
  },
  {
    "text": "So it's generally\nlow or 0 bias, but it can be really high variance. And so that should maybe\ngive some intuition over--",
    "start": "1797620",
    "end": "1805140"
  },
  {
    "text": "and we were just\ndiscussing this. In general, it's going\nto be, unfortunately, be a trade off between\nlow variance and low bias.",
    "start": "1805140",
    "end": "1811350"
  },
  {
    "text": "And so often you're going to\nwant things in terms of n. So if that's the n\nstep, and you want",
    "start": "1811350",
    "end": "1817910"
  },
  {
    "text": "to minimize your\nmean squared error, often you're going\nto end up wanting to do something where\nit's a couple steps,",
    "start": "1817910",
    "end": "1823350"
  },
  {
    "text": "and then bootstrap to\nget a nice trade off between bias and variance. And I think we'll probably\ntalk a little bit more",
    "start": "1823350",
    "end": "1829800"
  },
  {
    "text": "about that next week. So I'll just highlight this\nhere that this has low bias",
    "start": "1829800",
    "end": "1838340"
  },
  {
    "text": "and high variance on this one. So this one has low\nvariance and high bias.",
    "start": "1838340",
    "end": "1844650"
  },
  {
    "text": "The other one is the opposite. All right, cool.",
    "start": "1844650",
    "end": "1849840"
  },
  {
    "text": "So just to think\nabout this, so when we are thinking\nabout these targets, we can go between\nthese different ones.",
    "start": "1849840",
    "end": "1856750"
  },
  {
    "text": "And then, oops, sorry. Somehow, these got copied. OK, so these are all things\nthat you can plug in.",
    "start": "1856750",
    "end": "1863030"
  },
  {
    "text": "You can make different\nchoices over whether you plug in these n step\nmethods or others.",
    "start": "1863030",
    "end": "1868862"
  },
  {
    "text": "And then you can use this all as\npart of your actor critic policy gradient method. So now what we're going to do,\nand I'll just make it a slide",
    "start": "1868862",
    "end": "1875410"
  },
  {
    "text": "to delete those slides. So now what we're\ngoing to go into is more advanced policy\ngradient methods.",
    "start": "1875410",
    "end": "1880567"
  },
  {
    "text": "OK, so those are the\nbasic ones, and they're kind of the backbone between\nall of the algorithms that we do now.",
    "start": "1880567",
    "end": "1885730"
  },
  {
    "text": "But there's been\na lot of interest in these types of methods\nand how do we scale them up, and how do we make them better.",
    "start": "1885730",
    "end": "1891910"
  },
  {
    "text": "And we'll talk about what\nwe mean by better here. So actually, we'll probably talk\nabout some of this next week",
    "start": "1891910",
    "end": "1900130"
  },
  {
    "text": "because I wanted to\nmake sure that we got through the algorithm\ntoday so that you guys can have",
    "start": "1900130",
    "end": "1905481"
  },
  {
    "text": "all the knowledge you\nneed to be starting to do the implementation. And then we'll do more\non the theory next week. ",
    "start": "1905482",
    "end": "1913010"
  },
  {
    "text": "So policy gradients,\nso far, we've been talking about\nthem being great, and we know that they're\nused in some really important application areas.",
    "start": "1913010",
    "end": "1918890"
  },
  {
    "text": "Why do we have to go beyond\nthe methods that we just saw? Well, there's a couple of\ndifferent limitations to them.",
    "start": "1918890",
    "end": "1926480"
  },
  {
    "text": "One is that the sample\nefficiency is generally poor.",
    "start": "1926480",
    "end": "1931940"
  },
  {
    "text": "And so in general, you\nhave to do many rollouts",
    "start": "1931940",
    "end": "1937429"
  },
  {
    "text": "with the same policy in\norder to get a good estimate of the gradient. Because you want a good\nestimate of the gradient,",
    "start": "1937430",
    "end": "1943320"
  },
  {
    "text": "because otherwise,\nwhen you take a step, you might get somewhere\nthat's not as good or, you don't get to the\nplace you want to as quickly.",
    "start": "1943320",
    "end": "1949700"
  },
  {
    "text": "And the other, and we're going\nto see an example of this shortly, is that the distance\nin the parameter space",
    "start": "1949700",
    "end": "1955940"
  },
  {
    "text": "doesn't necessarily equal the\ndistance in the policy space. So this is a little\nbit weird of an idea.",
    "start": "1955940",
    "end": "1961380"
  },
  {
    "text": "But the idea is that, if you\nhave a policy parameterization, whether it's like a deep\nneural network or others,",
    "start": "1961380",
    "end": "1966600"
  },
  {
    "text": "there's some\nparameters in there. And when you change them,\nwhen you change your theta, you're going to get a\ndifferent policy out.",
    "start": "1966600",
    "end": "1972809"
  },
  {
    "text": "But whether that is really\nsmooth, if I change theta, let's say theta is a scalar.",
    "start": "1972810",
    "end": "1977820"
  },
  {
    "text": "So maybe theta is like 0.7. If I change it to\n0.75, does that smoothly change how much more\nI take a particular action,",
    "start": "1977820",
    "end": "1984890"
  },
  {
    "text": "or might it be\nreally discontinuous? Might it suddenly say you\nwere taking this action with 20% probability, and\nI changed it a little bit,",
    "start": "1984890",
    "end": "1992330"
  },
  {
    "text": "and now I'm like taking that\naction with 90% probability. The main idea\nhere, and we'll see",
    "start": "1992330",
    "end": "1997850"
  },
  {
    "text": "an example of this in a second,\nis this part may not be smooth, which means that even really\nsmall changes in your theta,",
    "start": "1997850",
    "end": "2005720"
  },
  {
    "text": "in your policy\nparameterization, might actually lead to really big differences\nin how you're making decisions.",
    "start": "2005720",
    "end": "2012197"
  },
  {
    "text": "So you could imagine your\nrobot picks up things one way, and then you change\nyour theta a little bit, and suddenly, it\ndrives off a cliff.",
    "start": "2012197",
    "end": "2017980"
  },
  {
    "text": "OK, not quite that\nextreme, but it's not clear that, as we smoothly\ntake gradient steps in theta,",
    "start": "2017980",
    "end": "2024903"
  },
  {
    "text": "if that's going\nto smoothly change our policy parameterization\nor policy decisions. So let's look at those both.",
    "start": "2024903",
    "end": "2032050"
  },
  {
    "text": "All right. Sample efficiency. So what we've been\nseeing so far is the idea is that\nwe take our policy,",
    "start": "2032050",
    "end": "2037070"
  },
  {
    "text": "we roll it out\none or more times, and then we take\na gradient step, and we take one gradient step,\nand then we roll out our policy",
    "start": "2037070",
    "end": "2046330"
  },
  {
    "text": "again. And in general,\nit would be really nice to be able to take\nmultiple gradient steps.",
    "start": "2046330",
    "end": "2052899"
  },
  {
    "text": "But so far, we\nhave not seen that. And it's called on\npolicy expectation. So this is similar to\nSARSA and other methods",
    "start": "2052900",
    "end": "2062152"
  },
  {
    "text": "we've seen before,\nwhere you're learning about a policy and its value\nby actually executing it. So the problem is\nwhen we think about--",
    "start": "2062153",
    "end": "2070929"
  },
  {
    "text": "let me just go back to here. When we think about\ndoing these gradients,",
    "start": "2070929",
    "end": "2076669"
  },
  {
    "text": "we've assumed that we've\ngotten trajectories from the policy and the theta\nthat we're at right now.",
    "start": "2076670",
    "end": "2081949"
  },
  {
    "text": "And then we use that\ndata to take a step. So now we're at some point. Here's our theta.",
    "start": "2081949",
    "end": "2086980"
  },
  {
    "text": "We're at some point, and\nwe estimate the gradient from that point. So we estimate it\nfrom trajectories",
    "start": "2086980",
    "end": "2094780"
  },
  {
    "text": "that are generated under\ntheta, and then we take a step. Now, the problem is,\nnow we might be here.",
    "start": "2094780",
    "end": "2104300"
  },
  {
    "text": "And what we would like to\ndo is to take another step before getting more data. But the problem\nis, we don't have--",
    "start": "2104300",
    "end": "2110570"
  },
  {
    "text": "let's call this theta prime. What we have is we\nhave data from theta.",
    "start": "2110570",
    "end": "2116069"
  },
  {
    "text": "We don't have any\ndata from theta prime. So a priori, it\nshouldn't be clear that we could take\nmore than one step.",
    "start": "2116070",
    "end": "2122758"
  },
  {
    "text": "You can take one step because\nwe have data about theta. We estimate the\ngradient at that point. Now, we would like to\njust be able to continue",
    "start": "2122758",
    "end": "2129661"
  },
  {
    "text": "to take gradient steps\nbefore we actually go out in the real world\nand gather more data. But it shouldn't be\nobvious how to do that yet.",
    "start": "2129662",
    "end": "2136230"
  },
  {
    "text": "And so when we talk about\npolicy gradient right now, we've been talking\nabout on policy methods, where we just try to estimate\nthe gradient for the policy we",
    "start": "2136230",
    "end": "2143819"
  },
  {
    "text": "just executed. So similar to SARSA.",
    "start": "2143820",
    "end": "2149170"
  },
  {
    "text": "Now, so what we would\nlike to be able to do is-- so that's what we've\nbeen doing so far.",
    "start": "2149170",
    "end": "2154570"
  },
  {
    "text": "We collect sample\ntrajectories from the policy, then we form a sample estimate. It's pretty stable. We get our gradient, we take\na step, we rinse and repeat.",
    "start": "2154570",
    "end": "2161590"
  },
  {
    "text": "Another thing we could do,\nthinking about Q-learning or others is, well,\nwhat if we could use that old data to\nestimate the gradient",
    "start": "2161590",
    "end": "2168430"
  },
  {
    "text": "at some other theta,\nsome other policy. Could we do that? This is known as off\npolicy estimates.",
    "start": "2168430",
    "end": "2174670"
  },
  {
    "text": "This generally can start\nto be pretty unstable. We're going to think\nabout different ways we could even do that.",
    "start": "2174670",
    "end": "2179920"
  },
  {
    "text": "But we really would like that. We would like to be able\nto use our old data, take multiple gradient\nsteps before we actually",
    "start": "2179920",
    "end": "2186250"
  },
  {
    "text": "have to gather data. And you can imagine\nthat might end up allowing us to be much\nmore data efficient so",
    "start": "2186250",
    "end": "2191603"
  },
  {
    "text": "that the total amount of times\nwe have to gather more data is much less. So we're going to think\nabout a way today,",
    "start": "2191603",
    "end": "2198563"
  },
  {
    "text": "and we'll talk more\nabout this certainly over the next few\nweeks, as well, of how do we use\nour old data to,",
    "start": "2198563",
    "end": "2203770"
  },
  {
    "text": "essentially, move faster\nin our parameter space. Here's the second big challenge.",
    "start": "2203770",
    "end": "2210460"
  },
  {
    "text": "So in general, we're going to\nbe doing stochastic gradient ascent with some step size.",
    "start": "2210460",
    "end": "2215948"
  },
  {
    "text": "We've repeatedly thought\nof there being some sort of learning rate or step size. One of the challenges--\nand this was",
    "start": "2215948",
    "end": "2222250"
  },
  {
    "text": "important for deep Q learning. We thought about it even\nfor TD learning and stuff. What was the step size?",
    "start": "2222250",
    "end": "2227690"
  },
  {
    "text": "How much do we update\nour estimate every time we get new data? Turns out it's much harder here.",
    "start": "2227690",
    "end": "2233040"
  },
  {
    "text": "Now, we saw before that,\nunder some pretty loose, loose requirements\non the learning rate,",
    "start": "2233040",
    "end": "2238700"
  },
  {
    "text": "we could, at least\nin tabular cases, guarantee to\nconverge, et cetera.",
    "start": "2238700",
    "end": "2244180"
  },
  {
    "text": "Policy gradient methods\nare a little bit different. Here, the step size\nreally matters.",
    "start": "2244180",
    "end": "2251769"
  },
  {
    "text": "And if we take a step size\nthat is really quite bad, we can collapse in\nour performance.",
    "start": "2251770",
    "end": "2259510"
  },
  {
    "text": "Does anybody have\nan idea of why? Why does that happen?",
    "start": "2259510",
    "end": "2264870"
  },
  {
    "text": "We why can we suddenly collapse? ",
    "start": "2264870",
    "end": "2271490"
  },
  {
    "text": "So I [INAUDIBLE] might\nmix the optimal target,",
    "start": "2271490",
    "end": "2276635"
  },
  {
    "text": "and then go for the win rate. Yeah, that is great. Great. So let's look at an example.",
    "start": "2276635",
    "end": "2283190"
  },
  {
    "text": "So remember, the way that\nwe're getting our data is from our policy. ",
    "start": "2283190",
    "end": "2289920"
  },
  {
    "text": "So let's say we're trying\nto get to this point. You have a big learning rate. So we took big steps.",
    "start": "2289920",
    "end": "2295500"
  },
  {
    "text": "You might now get to part of\nthe space, which is really bad, really, really bad policies.",
    "start": "2295500",
    "end": "2300910"
  },
  {
    "text": "By really bad policies, I mean\nthey have really bad value functions. If you have really bad value\nfunctions and trajectories,",
    "start": "2300910",
    "end": "2308280"
  },
  {
    "text": "which are visiting\nstates and actions, which all have really bad\nreward, it's really hard to estimate a good\ngradient of where to go.",
    "start": "2308280",
    "end": "2315210"
  },
  {
    "text": "In general, you might be in\na really long plateau place. So this could be a really long.",
    "start": "2315210",
    "end": "2320550"
  },
  {
    "text": "And so the gradient\nhere might be really hard to estimate\nof how do I get back to that local optima.",
    "start": "2320550",
    "end": "2326020"
  },
  {
    "text": "In general, it's not\ngoing to be impossible, unless it's completely flat. But it might be really\nclose to completely flat.",
    "start": "2326020",
    "end": "2334780"
  },
  {
    "text": "And so that's a big problem,\nis that you don't necessarily know how large your\nstep size should be.",
    "start": "2334780",
    "end": "2340405"
  },
  {
    "text": "On the other hand, if you use a\nreally small step sizes, that's bad, too, because each\ntime, your step size",
    "start": "2340405",
    "end": "2346427"
  },
  {
    "text": "is basically determining how\nmuch you change your policy before you get new data. And so you would like to take\nas big a step sizes as possible",
    "start": "2346427",
    "end": "2353230"
  },
  {
    "text": "that don't overstep and that\nallow you to quickly get to the optimal local optima.",
    "start": "2353230",
    "end": "2358970"
  },
  {
    "text": "Now, things like of\natom style optimizers, et cetera, help, but it won't\nnecessarily solve the problem.",
    "start": "2358970",
    "end": "2365380"
  },
  {
    "text": "And one of the\nchallenges here is, we're only getting\ninformation about the states",
    "start": "2365380",
    "end": "2370480"
  },
  {
    "text": "and actions that are\nvisited under our policy. And you just might\nget to regions where there's very\nlittle information",
    "start": "2370480",
    "end": "2376240"
  },
  {
    "text": "to estimate those gradients. Here's another challenge. And this relates to that we're\ntaking steps in the theta space,",
    "start": "2376240",
    "end": "2386080"
  },
  {
    "text": "not directly in\nterms of the actions we take when we\nupdate our policy. So let's think about a\nparameterization, which",
    "start": "2386080",
    "end": "2394050"
  },
  {
    "text": "is a pretty simple\nparameterization. So this is a logistic function. ",
    "start": "2394050",
    "end": "2403950"
  },
  {
    "text": "And this is 1 over 1\nplus e to the minus w. ",
    "start": "2403950",
    "end": "2410456"
  },
  {
    "text": "So in this case, this\nis the parameterization. It's kind of like a softmax.",
    "start": "2410456",
    "end": "2415660"
  },
  {
    "text": "You just have some probability\nof going to one action, and the rest of the\nprobability goes to the other, and you've parameterized\nit with theta.",
    "start": "2415660",
    "end": "2422820"
  },
  {
    "text": "So if theta is equal\nto 0, it's 50/50. You'll either take a1 or a2.",
    "start": "2422820",
    "end": "2428549"
  },
  {
    "text": "If theta is equal to 2, you\nsuddenly take a2 is much less. And if theta is equal to 4,\nyou basically never take a2.",
    "start": "2428550",
    "end": "2437970"
  },
  {
    "text": "And that's just because of how\nour relationship goes from theta",
    "start": "2437970",
    "end": "2443130"
  },
  {
    "text": "to pi of a.  In this parameterization,\nit's pretty extreme.",
    "start": "2443130",
    "end": "2450240"
  },
  {
    "text": "So as we make sort of small,\nrelatively small changes to theta, I didn't make\ntheta a million or anything.",
    "start": "2450240",
    "end": "2456420"
  },
  {
    "text": "I've basically shifted,\neven with 0, to 2, to 4, I've radically shifted how\nmuch of my probability mass",
    "start": "2456420",
    "end": "2462800"
  },
  {
    "text": "goes on to a1. And that's just to illustrate\nthis issue of smoothness",
    "start": "2462800",
    "end": "2468595"
  },
  {
    "text": "that, as I make what might\nbe considered relatively small changes in my\ntheta space, that might make my policy\nnear deterministic.",
    "start": "2468595",
    "end": "2475780"
  },
  {
    "text": "And we know that if our\npolicy is deterministic, we can't learn anything\nabout other actions.",
    "start": "2475780",
    "end": "2481852"
  },
  {
    "text": "So let me just make\nthis a little smaller so you can see the question. So the challenge in this case\nis that step size can matter",
    "start": "2481852",
    "end": "2488450"
  },
  {
    "text": "a lot in terms of efficiency. We don't necessarily know\nwhat the right step size is, and it may be hard\nfor us to know",
    "start": "2488450",
    "end": "2494480"
  },
  {
    "text": "how small changes in\nour parameter space relate to changes in\nthe action distributions",
    "start": "2494480",
    "end": "2499700"
  },
  {
    "text": "we actually follow. And so what we'd\nreally like to do here is to actually come up\nwith an update rule that",
    "start": "2499700",
    "end": "2505099"
  },
  {
    "text": "doesn't over-change\nthe policy too quickly, but still allows us to\nmake rapid progress.",
    "start": "2505100",
    "end": "2510920"
  },
  {
    "text": "So we'd like to move as\nfar as we can in a way that we think is,\nreally, ideally, is going to just\ndirectly increase",
    "start": "2510920",
    "end": "2517170"
  },
  {
    "text": "the value of our policy.  And I guess I'll say, well,\nwe'll see a bit more of that.",
    "start": "2517170",
    "end": "2523040"
  },
  {
    "text": "And also, ideally, we would\nlike this all to be monotonic. We would like it so that if\nwe think back to the policy",
    "start": "2523040",
    "end": "2528440"
  },
  {
    "text": "improvement\nalgorithms that we've seen before, policy improvement\nalgorithms for the tabular case where we knew\nhow the world worked",
    "start": "2528440",
    "end": "2535500"
  },
  {
    "text": "had this great property\nthat every time we updated our policy, we got a\nbetter policy or we were done.",
    "start": "2535500",
    "end": "2541526"
  },
  {
    "text": "Now, the world is much\nmore complicated now. We've got these sort of\ncontinuous parameterizations.",
    "start": "2541527",
    "end": "2546910"
  },
  {
    "text": "We're not guaranteed to\nget to the optimal policy, but it would be really\ncool if we could still guarantee that we're\ngoing to just get",
    "start": "2546910",
    "end": "2552953"
  },
  {
    "text": "sort of monotonic improvement,\nunless we get to a local optima. And the things that\nI've shown you so far",
    "start": "2552953",
    "end": "2560513"
  },
  {
    "text": "don't necessarily have that\nproperty because they'll still converge to a local\noptima, but you might overstep, like we see here.",
    "start": "2560513",
    "end": "2566770"
  },
  {
    "text": "So you might go over. You might be having monotonic\nimprovement and then crash,",
    "start": "2566770",
    "end": "2572140"
  },
  {
    "text": "and then you have to\ngo back and forth. So we have not guaranteed\nmonotonic improvement so far. But that would be\nreally nice and that",
    "start": "2572140",
    "end": "2578470"
  },
  {
    "text": "could be important in a\nlot of real world domains, like you'd imagine. If you were using this for\nhealth care applications,",
    "start": "2578470",
    "end": "2584373"
  },
  {
    "text": "you would really like to\nhave monotonic improvement, and not suddenly\nperformance crash.",
    "start": "2584373",
    "end": "2589850"
  },
  {
    "text": "All right. So let's think about how we\nmight be able to get here. And you can think of a lot\nof this lecture is motivating",
    "start": "2589850",
    "end": "2596330"
  },
  {
    "text": "the things that you're going\nto be doing in Homework 2, including the theory.",
    "start": "2596330",
    "end": "2601400"
  },
  {
    "text": "So in general, what\nwe'd like to have is we'd like to have an update step\nthat uses all the data that we just got as efficiently\nas possible,",
    "start": "2601400",
    "end": "2608370"
  },
  {
    "text": "and that takes steps\nthat sort of respect this distance in the policy,\nlike the decision space, as",
    "start": "2608370",
    "end": "2614720"
  },
  {
    "text": "opposed to just smoothness\nin the parameter space. And in order to do that,\nwe need to understand",
    "start": "2614720",
    "end": "2621260"
  },
  {
    "text": "how does the performance\nof two policies relate. So we have data from one\npolicy, and we're considering",
    "start": "2621260",
    "end": "2627710"
  },
  {
    "text": "trying to move to a new policy. And we'd really like to\nknow, OK, given the data that I have from\npolicy one, what",
    "start": "2627710",
    "end": "2633260"
  },
  {
    "text": "does it tell me about how\ngood policy two might be? Because ideally,\npolicy one's data",
    "start": "2633260",
    "end": "2638930"
  },
  {
    "text": "would allow us to tell us\nwhich policy two I should move to next. So this is what you're\nproving in Homework 2.",
    "start": "2638930",
    "end": "2645960"
  },
  {
    "text": "You're going to prove the\nperformance difference lemma. And the performance\ndifference lemma, it allows us to relate the\nperformance of one policy",
    "start": "2645960",
    "end": "2657050"
  },
  {
    "text": "to the performance of\nanother policy, given data from one of the policies.",
    "start": "2657050",
    "end": "2662530"
  },
  {
    "text": "Let me just state this out. So what does this say? This is the value of one policy,\npolicy one, policy pi prime.",
    "start": "2662530",
    "end": "2673070"
  },
  {
    "text": "I'm just using j\nhere, but this is value V. You can think\nof this as just V.",
    "start": "2673070",
    "end": "2679190"
  },
  {
    "text": "So what is that equal to? That is equal to the expectation\nover trajectories that",
    "start": "2679190",
    "end": "2685670"
  },
  {
    "text": "are sampled using pi prime. And again, if this is finite,\ncan use gamma equal to 1.",
    "start": "2685670",
    "end": "2698930"
  },
  {
    "text": "We sum over the\ndistribution of trajectories you could get if you followed\npolicy pi 1 times the advantage",
    "start": "2698930",
    "end": "2708530"
  },
  {
    "text": "under policy pi. So this part here is just equal\nto the difference between if you",
    "start": "2708530",
    "end": "2719000"
  },
  {
    "text": "took an action minus.",
    "start": "2719000",
    "end": "2724700"
  },
  {
    "start": "2724700",
    "end": "2730829"
  },
  {
    "text": "But note here, because\nthis expectation here is over pi prime, the way\nwe're selecting these actions,",
    "start": "2730830",
    "end": "2737737"
  },
  {
    "text": "I'll just write it\nout a little bit more.  Imagine we have\ndeterministic policies.",
    "start": "2737737",
    "end": "2746090"
  },
  {
    "text": "So it's like we're\nthinking about the Q value, if we first take an action\naccording to policy pi prime",
    "start": "2746090",
    "end": "2751880"
  },
  {
    "text": "and then follow policy\npi for the rest of time, versus what we would have gotten\nif we just followed policy",
    "start": "2751880",
    "end": "2757070"
  },
  {
    "text": "pi the whole time. So you can think of it\nas sort of breaking down the difference in the\nvalue between two policies",
    "start": "2757070",
    "end": "2762440"
  },
  {
    "text": "into a series of\nsmall differences of, well, how much gain would\nI have gotten at this state if I had taken pi prime's action\ninstead of the one I actually",
    "start": "2762440",
    "end": "2770510"
  },
  {
    "text": "took. OK, what about here? And then you of want to\nsum up all those additions. Every day, I'm\nhappier because I went",
    "start": "2770510",
    "end": "2776930"
  },
  {
    "text": "to Stanford instead of Harvard. And I just add up all of\nthose, and that tells me over the course of my whole\ncareer how much happier.",
    "start": "2776930",
    "end": "2782440"
  },
  {
    "text": "I will have been,\nhypothetically. OK, so this here is\nover trajectories.",
    "start": "2782440",
    "end": "2789183"
  },
  {
    "text": "Now we're going to\nmake a transformation and move it into state\naction distributions.",
    "start": "2789183",
    "end": "2794300"
  },
  {
    "text": "Because what this is\ngoing to be here-- so now, this was over trajectories.",
    "start": "2794300",
    "end": "2800339"
  },
  {
    "text": "We're going to rewrite this\njust in terms of state action distributions.",
    "start": "2800340",
    "end": "2805727"
  },
  {
    "text": "What we're going to\nsay is, all right, as we think about adding up\nall these advantages, what I'm going to do is I'm\ngoing to cluster together",
    "start": "2805727",
    "end": "2811980"
  },
  {
    "text": "all the advantages that have\nto do with the same state. So I think of there as\nbeing a distribution",
    "start": "2811980",
    "end": "2817350"
  },
  {
    "text": "over states I might\nreach and actions I might take under policy pi prime.",
    "start": "2817350",
    "end": "2822570"
  },
  {
    "text": "So if I just follow this policy,\nI'm going to visit some states, and I'm just going\nto think about what",
    "start": "2822570",
    "end": "2827609"
  },
  {
    "text": "is the advantage in each\nof those states weighed by how frequently I visit them.",
    "start": "2827610",
    "end": "2833230"
  },
  {
    "text": "So we're sort of\ntransforming things from thinking of it as being\ntrajectories and thinking about weighting over\ntime steps, to weighting",
    "start": "2833230",
    "end": "2839800"
  },
  {
    "text": "over a finite set or a\nspace of states and actions. And so we'll have\na distribution.",
    "start": "2839800",
    "end": "2846050"
  },
  {
    "text": "It might be like, I visit\nstate 1 half the time. I visit state 7 only\n1 in 10,000 times.",
    "start": "2846050",
    "end": "2852760"
  },
  {
    "text": "So this allows us to\nreweight the advantages. What does this\ndistribution look like?",
    "start": "2852760",
    "end": "2859840"
  },
  {
    "text": "This looks like the following. Essentially, you just\nthink of what is my-- so here, we're allowing us\nto have discount factors",
    "start": "2859840",
    "end": "2865690"
  },
  {
    "text": "because we're looking\nfor the infinite case. But you can adjust this. What this is just\nsaying is that, well,",
    "start": "2865690",
    "end": "2871099"
  },
  {
    "text": "my weighted\ndistribution for state s is equal to, well, how\nlikely was I to be in state",
    "start": "2871100",
    "end": "2876744"
  },
  {
    "text": "s on time step 1\nunder that policy. Well, how likely was I to\nbe in state s in time step",
    "start": "2876745",
    "end": "2883420"
  },
  {
    "text": "2 under that policy? What if I was in time step 3? And so you just sum\nup all of those.",
    "start": "2883420",
    "end": "2888940"
  },
  {
    "text": "And as you could imagine, in\nthe infinite horizon case, those could easily go to\ninfinity, particularly",
    "start": "2888940",
    "end": "2894710"
  },
  {
    "text": "if you have states\nyou can visit a lot. And so the discount\nfactor here makes sure this becomes\na distribution.",
    "start": "2894710",
    "end": "2900359"
  },
  {
    "text": "So we want this still\nto be normalized. And then similarly, this is\nalso with respect to taking",
    "start": "2900360",
    "end": "2906620"
  },
  {
    "text": "the actions under pi prime. So you'll be proving\nthis in the homework,",
    "start": "2906620",
    "end": "2911790"
  },
  {
    "text": "but we'll see how\nthis can be helpful. So why are we\nmaking you do this?",
    "start": "2911790",
    "end": "2918410"
  },
  {
    "text": "So the nice thing is\nit's going to define the performance of pi prime in\nterms of advantages from pi.",
    "start": "2918410",
    "end": "2925660"
  },
  {
    "text": "OK, so that seems good\nbecause we're like, well, we have an existing policy.",
    "start": "2925660",
    "end": "2931880"
  },
  {
    "text": "But the problem is it\nstill requires trajectories sampled from pi prime. So you could think\nof pi as potentially",
    "start": "2931880",
    "end": "2937235"
  },
  {
    "text": "being the policy\nwe have right now, and maybe we can estimate\nthe Q function for it. We can estimate its advantages.",
    "start": "2937235",
    "end": "2942740"
  },
  {
    "text": "And now, we want to figure\nout how good would be this new policy we might take. But the problem is we\ndon't have any trajectories",
    "start": "2942740",
    "end": "2949400"
  },
  {
    "text": "for the new policy. We only have data\nfrom the old policy. So we really want\nto get to something where we can estimate how good\nis pi prime using data only",
    "start": "2949400",
    "end": "2957950"
  },
  {
    "text": "from pi. That's our goal. So our goal, estimate j pi\nprime only from data from pi.",
    "start": "2957950",
    "end": "2977460"
  },
  {
    "text": "So you can think of pi\nprime as the new policy. So what we really want\nto do is take a step so that our new\npolicy is the best",
    "start": "2977460",
    "end": "2984200"
  },
  {
    "text": "one we could get to in\nterms of its improvement over the previous policy. And we want to be able\nto do that by only using",
    "start": "2984200",
    "end": "2990680"
  },
  {
    "text": "an estimate from our old data. And it shouldn't be clear\nyet how we could do that. This is looking promising, but\nwe still seem like we need data.",
    "start": "2990680",
    "end": "2998220"
  },
  {
    "text": "So this is still data\nfrom the new policy.",
    "start": "2998220",
    "end": "3003440"
  },
  {
    "text": " So let's look at it\nfrom a different angle.",
    "start": "3003440",
    "end": "3008660"
  },
  {
    "text": "So this thing d, d pi of s is\na distribution over states. It's a discounted future\nstate distribution.",
    "start": "3008660",
    "end": "3017089"
  },
  {
    "text": "And we're going to use that to\nrewrite the relative performance of policy performance identity.",
    "start": "3017090",
    "end": "3023360"
  },
  {
    "text": "So why is this relative? It's because it's with\nrespect to the performance of our current policy.",
    "start": "3023360",
    "end": "3028370"
  },
  {
    "text": "So that's why we have\na subtraction there. We're going to\nrewrite that there.",
    "start": "3028370",
    "end": "3033585"
  },
  {
    "text": " I want to see if I have it. OK, yeah. So I'm going to\nstep through this.",
    "start": "3033585",
    "end": "3039460"
  },
  {
    "text": "So what we can do at this point\nis we can rewrite it as follows.",
    "start": "3039460",
    "end": "3044530"
  },
  {
    "text": "So right now, remember,\nthis is in the time or the trajectory notation. ",
    "start": "3044530",
    "end": "3052960"
  },
  {
    "text": "So I'm going to,\nagain, rewrite this so that I'm going to move\nit into the state action representation.",
    "start": "3052960",
    "end": "3058625"
  },
  {
    "text": "So instead of thinking\nabout as trajectories, I'm going to think\nabout it as what's the distribution over states\nand actions that I'm visiting.",
    "start": "3058625",
    "end": "3064960"
  },
  {
    "text": "So I'm going to\nwrite it as follows. 1 over 1 minus gamma sub over--",
    "start": "3064960",
    "end": "3073471"
  },
  {
    "text": "I should write this\nas the expectation.  Expectation over s prime\naccording to pi [INAUDIBLE]",
    "start": "3073471",
    "end": "3090410"
  },
  {
    "text": "and then under-- I should write it this way. ",
    "start": "3090410",
    "end": "3100520"
  },
  {
    "text": "OK, what have I done there? So this should look pretty\nsimilar to the previous slide. What I've said is, OK, this\nis the discounted future state",
    "start": "3100520",
    "end": "3107810"
  },
  {
    "text": "distribution. I saw on the previous slide that\nI can rewrite this expression",
    "start": "3107810",
    "end": "3116750"
  },
  {
    "text": "as 1 over 1 minus gamma, e\ntimes s times this d times a.",
    "start": "3116750",
    "end": "3122270"
  },
  {
    "text": "So that's what I'm doing now.  So I've rewritten it in terms of\nthis state action distribution.",
    "start": "3122270",
    "end": "3129954"
  },
  {
    "text": "And this is where I'm\nin this problematic case that I've got the wrong-- let me make sure I\nput a quote there.",
    "start": "3129955",
    "end": "3136289"
  },
  {
    "text": "So this is just to\nbe really clear. This is over pi prime.",
    "start": "3136290",
    "end": "3141630"
  },
  {
    "text": "So this is all with\nrespect to the new policy. So now I'm going to\nnote the following.",
    "start": "3141630",
    "end": "3147160"
  },
  {
    "text": "OK, so what does this look like? This looks like it's\n1 over 1 minus gamma, the expectation over s prime\nsampled according to d pi prime.",
    "start": "3147160",
    "end": "3157880"
  },
  {
    "text": "And then what is\nthis expectation? So this is going to be sum over\na pi of a, given s pi prime.",
    "start": "3157880",
    "end": "3169309"
  },
  {
    "text": "That's horrible notation. Let's try this again. ",
    "start": "3169310",
    "end": "3176090"
  },
  {
    "text": "That's what this means. I'm taking an expectation\nfor each of the states. I'm taking an expectation\nwith respect to here.",
    "start": "3176090",
    "end": "3184770"
  },
  {
    "text": "Let me just try\nto make this neat. There we go.",
    "start": "3184770",
    "end": "3189950"
  },
  {
    "text": "OK, so I'm saying, imagine\nI sample states from my d pi prime distribution.",
    "start": "3189950",
    "end": "3196330"
  },
  {
    "text": "How do I do this expectation\nover a sampled from p prime? Well, I just sum\nover all the actions, look at the probability\nof me taking",
    "start": "3196330",
    "end": "3202830"
  },
  {
    "text": "that action for that state under\np prime times my advantage. So now, the key thing\nwe're going to do",
    "start": "3202830",
    "end": "3209030"
  },
  {
    "text": "is we're going to try to change\nthis so that we are using more-- we're going to try\nto get to a point",
    "start": "3209030",
    "end": "3215150"
  },
  {
    "text": "where we only need data from pi. So the first thing\nwe're going to do here is we're just going to rewrite\nthis, and I'm going to multiply",
    "start": "3215150",
    "end": "3222200"
  },
  {
    "text": "and divide by the same thing.  So I'm going to say this\nis pi prime of a given s,",
    "start": "3222200",
    "end": "3229610"
  },
  {
    "text": "divided by pi of a given\ns, times pi of a given s.",
    "start": "3229610",
    "end": "3234837"
  },
  {
    "text": "OK, I have not done anything,\nexcept for I've multiplied and divided by the same thing. Why did I do that?",
    "start": "3234837",
    "end": "3240090"
  },
  {
    "text": "Well, the good thing is I know\nhow to get samples from this. I have samples from this.",
    "start": "3240090",
    "end": "3245438"
  },
  {
    "text": "This is from my old data. This is from the actual\npolicy that I took before. OK, so what this says I can\nwrite this as, I've got 1 over 1",
    "start": "3245438",
    "end": "3252900"
  },
  {
    "text": "minus gamma e of s\naccording to d pi prime. And then I have this\nexpectation over a sampled",
    "start": "3252900",
    "end": "3261360"
  },
  {
    "text": "according to pi, not pi prime,\nof my reweighted advantages.",
    "start": "3261360",
    "end": "3267566"
  },
  {
    "text": " And what do I reweigh them by? I reweigh them exactly\nby the probability",
    "start": "3267566",
    "end": "3274640"
  },
  {
    "text": "I take that action under the new\npolicy versus the old policy. And that's OK because I'm going\nto assume that I have access",
    "start": "3274640",
    "end": "3280970"
  },
  {
    "text": "to the policy parameterization\nof the new policy. I'm just trying to figure\nout how good it is. I don't have samples from\nit, but if you tell me,",
    "start": "3280970",
    "end": "3288622"
  },
  {
    "text": "hey, this is the action\nyou took in that state, I can say, OK, well, that's how\nlikely I would have taken that under my new policy.",
    "start": "3288622",
    "end": "3294550"
  },
  {
    "text": "So I can do that reweighting. And this is an\ninstance of something called importance sampling. And we're going to see a\nlot more about that soon.",
    "start": "3294550",
    "end": "3301280"
  },
  {
    "text": "So this is the\nfirst step I can do. So this is great. Right? Because now, I don't need\nto have samples from actions",
    "start": "3301280",
    "end": "3307730"
  },
  {
    "text": "taken by my new policy. I can just reweight the\ndata I already have. So that's super cool.",
    "start": "3307730",
    "end": "3313280"
  },
  {
    "text": "But there's a problem. This is still pi prime. So I still don't have any data\nover states from my pi prime.",
    "start": "3313280",
    "end": "3323540"
  },
  {
    "text": "All right. So this is that just\nwritten out more neatly. We'll see a lot more\non that in the future.",
    "start": "3323540",
    "end": "3329569"
  },
  {
    "text": "But we still have\nthis big problem because we don't have\nany states from pi prime.",
    "start": "3329570",
    "end": "3335580"
  },
  {
    "text": "We have data from pi. So what are we going\nto do about that?",
    "start": "3335580",
    "end": "3341610"
  },
  {
    "text": "Well, we're just\ngoing to ignore it. Always an option.",
    "start": "3341610",
    "end": "3347950"
  },
  {
    "text": "So we're just going to\nignore that and proceed.",
    "start": "3347950",
    "end": "3353363"
  },
  {
    "text": "And this is what this\nis going to happen in the rest of the class for\nthe rest of this lecture.",
    "start": "3353363",
    "end": "3358660"
  },
  {
    "text": "We're just going to pretend\nthat those states are the same. Now, as you might imagine, that\nis going to slightly induce",
    "start": "3358660",
    "end": "3367369"
  },
  {
    "text": "some error in my estimate. When might that be bad? Well, it might be really bad if\nthe two policies would actually",
    "start": "3367370",
    "end": "3373819"
  },
  {
    "text": "visit totally different\nparts of the state space. But if they visit things\nthat are really close,",
    "start": "3373820",
    "end": "3380460"
  },
  {
    "text": "maybe it's not going\nto be that bad. OK? So let's imagine that\nyou have policy 1, and it goes, since\nthis is your robot,",
    "start": "3380460",
    "end": "3386420"
  },
  {
    "text": "and it goes to most of\nthis part of the space, and then you change your policy. And maybe it also goes to\nthat part of the space,",
    "start": "3386420",
    "end": "3392210"
  },
  {
    "text": "and it goes a little\nover here, too. But there's quite\na bit of overlap. The places where\nit would be bad is",
    "start": "3392210",
    "end": "3398330"
  },
  {
    "text": "something like if your policy\ngoes like this, your new policy. And then there's no overlap\nin your state space.",
    "start": "3398330",
    "end": "3405590"
  },
  {
    "text": "So it's going to turn out that\nif pi and pi prime are close, and we're going to define\nwhat we mean by close,",
    "start": "3405590",
    "end": "3411080"
  },
  {
    "text": "then this is actually\nnot a bad approximation. It's not perfect. It's not too bad.",
    "start": "3411080",
    "end": "3416720"
  },
  {
    "text": "So in the paper, they\nprove that we can bound",
    "start": "3416720",
    "end": "3422180"
  },
  {
    "text": "how bad this approximation is. So in general, we're\ngoing to define",
    "start": "3422180",
    "end": "3428180"
  },
  {
    "text": "this to be L, math script L,\nof pi with respect to pi prime.",
    "start": "3428180",
    "end": "3433339"
  },
  {
    "text": "If this was perfect,\nthis thing would be 0 because this would exactly\nequal this difference.",
    "start": "3433340",
    "end": "3440859"
  },
  {
    "text": "So this minus this would be-- these two things\nwould exactly be 0.",
    "start": "3440860",
    "end": "3446220"
  },
  {
    "text": "But what it turns out is that\nhow far off this approximation",
    "start": "3446220",
    "end": "3452700"
  },
  {
    "text": "is depends-- bless you-- on the KL\ndivergence in the policies.",
    "start": "3452700",
    "end": "3458010"
  },
  {
    "text": "And I'll define what KL\ndivergence is in a second. So in particular, it depends on\nthe KL divergence with respect",
    "start": "3458010",
    "end": "3463790"
  },
  {
    "text": "to-- let me just undo that so\nI can just do this part. With respect to states\nif you were sampling them",
    "start": "3463790",
    "end": "3470510"
  },
  {
    "text": "according to d pi. Now, d pi is good because d pi\nis the actual discounted states",
    "start": "3470510",
    "end": "3476450"
  },
  {
    "text": "that we visit under\nour current policy. That means we actually\nhave data about it. So I always like to work\nwith my-- in my lab,",
    "start": "3476450",
    "end": "3483358"
  },
  {
    "text": "we always try to instantiate\nour theoretical bounds because I feel like it's super\ninformative to be like, is this 10 to the\n10, or is this 0.5.",
    "start": "3483358",
    "end": "3491329"
  },
  {
    "text": "And one of the big\nquestions often that comes up when\nwe try to do this is that sometimes you can't\ninstantiate your bounds at all",
    "start": "3491330",
    "end": "3497835"
  },
  {
    "text": "because it will depend on\nconstants you don't know. So it's a beautiful\ntheorem, but you can't even check how big it is.",
    "start": "3497835",
    "end": "3502950"
  },
  {
    "text": "The nice thing about\nthis is it's checkable. At least this part\nis because you",
    "start": "3502950",
    "end": "3508309"
  },
  {
    "text": "can look at your\nactual trajectories from your current policy. And if you have a\nnew policy, pi prime, you can see and evaluate what\nyour KL divergence will be.",
    "start": "3508310",
    "end": "3516119"
  },
  {
    "text": "So this is actually evaluatable. So we'll see. What is C?",
    "start": "3516120",
    "end": "3522000"
  },
  {
    "text": "So C is a constant. Any information about its value? No.",
    "start": "3522000",
    "end": "3527850"
  },
  {
    "text": "We're not going to\nneed it for now. But in the paper, you can read\nabout exactly what that is.",
    "start": "3527850",
    "end": "3533819"
  },
  {
    "text": "Yes, good question, though. OK, let's see what\nKL divergence is. So what this says,\njust at a high level,",
    "start": "3533820",
    "end": "3539220"
  },
  {
    "text": "is that this approximation\nis not totally insane if the policies are close.",
    "start": "3539220",
    "end": "3545580"
  },
  {
    "text": "In fact, this is going to be\na pretty good approximation. So as we're going to\nsee, this is tight",
    "start": "3545580",
    "end": "3551533"
  },
  {
    "text": "if the policies are identical,\nwhich is exactly what you'd expect this to be tight. So if your two\npolicies are identical,",
    "start": "3551533",
    "end": "3557200"
  },
  {
    "text": "their difference should be 0. And this bound would\ntell you it's 0. All right. What is KL divergence?",
    "start": "3557200",
    "end": "3562440"
  },
  {
    "text": "Some of you guys might\nhave seen this before, but in case some\npeople might be new. So what KL divergence\nallows us to do",
    "start": "3562440",
    "end": "3567900"
  },
  {
    "text": "is to compare two\nprobability distributions. So in our case, what this is\ngoing to be is over actions",
    "start": "3567900",
    "end": "3576180"
  },
  {
    "text": "we will take. So pi of a given s,\nversus pi of a given s.",
    "start": "3576180",
    "end": "3581460"
  },
  {
    "text": "So both of these are\nprobability distributions that sum to 1 for\na particular state.",
    "start": "3581460",
    "end": "3586950"
  },
  {
    "text": "And so in our case, what we\nwould be summing over here, x would be a. So we'd be summing\nover all of these.",
    "start": "3586950",
    "end": "3593730"
  },
  {
    "text": "If you have the same\nprobability distribution, the KL divergence is 0.",
    "start": "3593730",
    "end": "3599050"
  },
  {
    "text": "Otherwise, it's\nstrictly positive. It's good to know\nit's not symmetric because we've made a choice\nhere in the ordering.",
    "start": "3599050",
    "end": "3607412"
  },
  {
    "text": "These are good\nproperties to know about. It comes up all the time\nin reinforcement learning.",
    "start": "3607413",
    "end": "3613150"
  },
  {
    "text": "So in our case, we can look\nat, for a particular state s, what is the KL divergence\nand what the policies would",
    "start": "3613150",
    "end": "3618849"
  },
  {
    "text": "do for that particular state. So that's what we've got there. So it says, essentially,\nhow different",
    "start": "3618850",
    "end": "3624170"
  },
  {
    "text": "are the actions you would take. Now, why is this good? Well, we've been spending\nsome time saying, hey,",
    "start": "3624170",
    "end": "3629250"
  },
  {
    "text": "we really don't\nwant to think just about how close we\nare in theta space. We really want to get to\nthinking about how different are",
    "start": "3629250",
    "end": "3635585"
  },
  {
    "text": "our actual policies,\nhow different are the actual decisions we\nmake in particular states. And the nice thing\nis, what this bound",
    "start": "3635585",
    "end": "3642530"
  },
  {
    "text": "says is that the difference\nbetween two policies",
    "start": "3642530",
    "end": "3647630"
  },
  {
    "text": "is not just about how close you\nare in some parameter space. It's really about\nhow different are",
    "start": "3647630",
    "end": "3653089"
  },
  {
    "text": "the decisions you make\nin all the states you'd reach under your current\nstate distribution.",
    "start": "3653090",
    "end": "3658970"
  },
  {
    "text": "And so if you'd make\nall the same decisions in the states you're\nalready reaching, your policy value is going\nto be really similar.",
    "start": "3658970",
    "end": "3665220"
  },
  {
    "text": "If you would make very\ndifferent decisions, then your policy value\nmight be really different, because you'd go off and\nexplore really different parts",
    "start": "3665220",
    "end": "3671869"
  },
  {
    "text": "of the state space. So this is really elegant.",
    "start": "3671870",
    "end": "3677700"
  },
  {
    "text": "And now, we have something where\nwe can just use our old data. So we have our old data.",
    "start": "3677700",
    "end": "3684000"
  },
  {
    "text": "We can use it to estimate\nwhat the performance improvement would be if we\ntry to get to a new policy.",
    "start": "3684000",
    "end": "3691349"
  },
  {
    "text": "And so what you might\nimagine in this case is you could use it to\nsearch over or decide which new policy to try.",
    "start": "3691350",
    "end": "3697859"
  },
  {
    "text": "This allows you to compute it\nfor lots of different pi prime. It doesn't just have to be the\npi prime for one gradient step.",
    "start": "3697860",
    "end": "3704369"
  },
  {
    "text": "It says, in general, even\noutside of policy gradient methods, you can evaluate the\nvalue of changing your policy",
    "start": "3704370",
    "end": "3712530"
  },
  {
    "text": "to pi prime with respect\nto your current performance by this expression. And this will be\nmore or less tight",
    "start": "3712530",
    "end": "3718380"
  },
  {
    "text": "depending on how\ndifferent your policy is at making decisions in states\nthat you would currently reach.",
    "start": "3718380",
    "end": "3724920"
  },
  {
    "text": "And we'll talk more about this. We haven't talked about this\nyet, so we will talk about--",
    "start": "3724920",
    "end": "3731850"
  },
  {
    "start": "3731850",
    "end": "3738182"
  },
  {
    "text": "This also relates to some really\nnice literature from the last 20 years of thinking about how\ndo we do monotonic policy",
    "start": "3738182",
    "end": "3745530"
  },
  {
    "text": "improvement in policy gradient\nmethods and policy search methods. It also relates to the notion\nof a trust region, which",
    "start": "3745530",
    "end": "3752310"
  },
  {
    "text": "is this idea of, when\nyou're changing your policy, how far can you\ngo and still sort",
    "start": "3752310",
    "end": "3757860"
  },
  {
    "text": "of trust the performance\nof it and trust you can get improvement. So there's a bunch of different\nnice papers related to this.",
    "start": "3757860",
    "end": "3763330"
  },
  {
    "text": "OK, let's talk\nabout the algorithm. Proximal policy\noptimization is going to be inspired by all the things\nthat we just talked about.",
    "start": "3763330",
    "end": "3770670"
  },
  {
    "text": "So what we want\nto do is, it wants to be able to take\nmultiple gradient steps, and it wants to be able\nto do this in a way",
    "start": "3770670",
    "end": "3776960"
  },
  {
    "text": "so that we don't overstep. We try to focus on policy\nparameterization in terms",
    "start": "3776960",
    "end": "3783290"
  },
  {
    "text": "of the actual\ndecisions that we make. So there are two\ndifferent variants. One is it solves an\nunconstrained optimization",
    "start": "3783290",
    "end": "3790849"
  },
  {
    "text": "problem where it uses\nthis approximation. So that's the approximation\nwe had on the previous slides.",
    "start": "3790850",
    "end": "3797760"
  },
  {
    "text": "I'll just write down, from\nprior slides, where we use data",
    "start": "3797760",
    "end": "3803420"
  },
  {
    "text": "from the current policy, and we\nadd up these weighted advantage functions. So what it says is, well,\nthe thing you want to do",
    "start": "3803420",
    "end": "3810290"
  },
  {
    "text": "is you want to pick the policy\nthat maximizes our estimated difference, subject to a\nconstraint on the KL divergence.",
    "start": "3810290",
    "end": "3817560"
  },
  {
    "text": "Because it's realizing\nthat L approximation is going to get worse and\nworse as the KL divergence",
    "start": "3817560",
    "end": "3823490"
  },
  {
    "text": "gets really large. So it's directly\nincorporating this bound.",
    "start": "3823490",
    "end": "3834520"
  },
  {
    "text": "So it's thinking, OK, I want\nto think about what this is, but then I also have\nto consider the fact that my estimate might\nbe off by as much",
    "start": "3834520",
    "end": "3840910"
  },
  {
    "text": "as this square root of KL. So you really want to improve\nwith respect to something",
    "start": "3840910",
    "end": "3847490"
  },
  {
    "text": "that considers both of those. So this is one\nversion of policy.",
    "start": "3847490",
    "end": "3852510"
  },
  {
    "text": "This is not the way\nmost people do PPO. We'll see the other\nreally common one. But it's a nice\nbaseline to know about.",
    "start": "3852510",
    "end": "3859160"
  },
  {
    "text": "And here, when we\nthink about what that KL is, as you\nmight have noticed,",
    "start": "3859160",
    "end": "3864330"
  },
  {
    "text": "KL is defined for\na single state. So for a single\nstate, we can say what is the distribution\nover actions I take",
    "start": "3864330",
    "end": "3870875"
  },
  {
    "text": "in one policy versus another. But of course, we\nhave lots of states. And so what we can\ndo here is we can",
    "start": "3870875",
    "end": "3876260"
  },
  {
    "text": "take an expectation over the KL,\nover all the states we visit. And that was part of the\ntheoretical bound, too.",
    "start": "3876260",
    "end": "3883940"
  },
  {
    "text": "Another really important\nthing you can see here is this waiting between\ntrying to optimize this policy",
    "start": "3883940",
    "end": "3892210"
  },
  {
    "text": "improvement, while\nrespecting this KL penalty. And you can change this\nover each iteration",
    "start": "3892210",
    "end": "3899890"
  },
  {
    "text": "to approximately satisfy the\nKL divergence constraint. So this does not\nguarantee that you will.",
    "start": "3899890",
    "end": "3906670"
  },
  {
    "text": "This does not\nguarantee that you're going to get\nmonotonic improvement, but it's trying to\nget towards that.",
    "start": "3906670",
    "end": "3912893"
  },
  {
    "text": "So let's see how that works. So this is the algorithm. What it does is, you can\ncompute the advantages",
    "start": "3912893",
    "end": "3917950"
  },
  {
    "text": "using any advantage\nestimation algorithm. You compute your\npolicy update, and you can do K steps with that.",
    "start": "3917950",
    "end": "3924330"
  },
  {
    "text": "So the nice thing is that you\ncan use your old data here, and you can take\nmultiple gradient steps.",
    "start": "3924330",
    "end": "3933010"
  },
  {
    "text": "After you do this,\nyou can also check if your KL divergence for your\nnew resulting policy is large.",
    "start": "3933010",
    "end": "3939589"
  },
  {
    "text": "If it is, then you may\nincrease the penalty. If it's small, you can\ndecrease the penalty.",
    "start": "3939590",
    "end": "3946030"
  },
  {
    "text": "And that just allows\nus to trade off between how much we pay\nattention to this KL divergence constraint versus not.",
    "start": "3946030",
    "end": "3954109"
  },
  {
    "text": "And as I noted here, you might\nviolate the KL constraints, but most of them, they\ndon't, empirically.",
    "start": "3954110",
    "end": "3961099"
  },
  {
    "text": "This is one reasonable thing\nto do based on everything we've seen. Now we're going to see\nsomething else, which",
    "start": "3961100",
    "end": "3966203"
  },
  {
    "text": "is inspired by that is a\nmuch more common thing to do, which is, well, let me\njust highlight here.",
    "start": "3966203",
    "end": "3971400"
  },
  {
    "text": "Multiple gradient\nsteps is really good. So one of the benefits\nis that we're not just",
    "start": "3971400",
    "end": "3977240"
  },
  {
    "text": "taking a single gradient step. We're taking multiple. So just to really\nhighlight that. All right.",
    "start": "3977240",
    "end": "3982290"
  },
  {
    "text": "What is the other thing\nwe want to do here? We haven't talked about\nnatural gradients.",
    "start": "3982290",
    "end": "3987590"
  },
  {
    "text": "But for any of you that\nare familiar with these, they're another\nway to try to think about taking gradient steps.",
    "start": "3987590",
    "end": "3993630"
  },
  {
    "text": "We're not going to talk\nabout that for now. So the other thing we could\ndo is equipped objective.",
    "start": "3993630",
    "end": "3999029"
  },
  {
    "text": "So what we're going to\nlook at in this case is, remember how we talked about\nwe had this kind of ratio",
    "start": "3999030",
    "end": "4004810"
  },
  {
    "text": "between this is what we're\nusing to weight our advantage function, was the difference\nbetween how likely",
    "start": "4004810",
    "end": "4010480"
  },
  {
    "text": "you were to take that\naction under our old policy versus our new. And we're using it to weight\nour advantage function.",
    "start": "4010480",
    "end": "4017720"
  },
  {
    "text": "What the clipping\ndoes is it says, well, I don't want this to\nget too high or too low.",
    "start": "4017720",
    "end": "4024780"
  },
  {
    "text": "OK? This could become really\nhigh or really low when my policies are\nreally different.",
    "start": "4024780",
    "end": "4030490"
  },
  {
    "text": "If my policy is really-- you can imagine that,\nif my policy puts",
    "start": "4030490",
    "end": "4036700"
  },
  {
    "text": "really low probability\non something that the current policy\nputs high probability on, this ratio here is going\nto go towards-- this r is",
    "start": "4036700",
    "end": "4044680"
  },
  {
    "text": "going to go to about 0. And if this puts very\nhigh, let's say this is 1,",
    "start": "4044680",
    "end": "4051080"
  },
  {
    "text": "and this puts very low\nprobability on that, this could be extremely large. This could be like 10 to the 6.",
    "start": "4051080",
    "end": "4057579"
  },
  {
    "text": "It could be very, very large. And both of those are being\nused to weight the advantage",
    "start": "4057580",
    "end": "4062655"
  },
  {
    "text": "function. Right? So your advantage function could\nbe getting shrunk towards 0, or it could be getting blown up\nby a factor of, say, 10 to the 6",
    "start": "4062655",
    "end": "4069330"
  },
  {
    "text": "if you have a big difference\nin the actions you would take under one\npolicy versus another. And in general, we\ndon't like things",
    "start": "4069330",
    "end": "4075620"
  },
  {
    "text": "where we're thinking of\npolicy gradients, where we might have terms that\nare exploding or vanishing.",
    "start": "4075620",
    "end": "4082010"
  },
  {
    "text": "And that's part of the point\nof the KL divergence constraint is to say, you want your\npolicies to stay close.",
    "start": "4082010",
    "end": "4087280"
  },
  {
    "text": "So the clipping is sort of\ninspired by this general idea, but says, well, maybe\nsomething similar we can is we're just going to clip.",
    "start": "4087280",
    "end": "4093290"
  },
  {
    "text": "We're just going to say you\ncan't have weighted advantage terms that are going towards\ninfinity, or minus infinity,",
    "start": "4093290",
    "end": "4100670"
  },
  {
    "text": "or 0. And so if this ratio\nis too extreme, I'm just going to clip it.",
    "start": "4100670",
    "end": "4106229"
  },
  {
    "text": "I'm going to not allow it to\nbe less than 1 minus epsilon, or greater than 1 plus epsilon.",
    "start": "4106229",
    "end": "4113250"
  },
  {
    "text": "And epsilon is just\na hyperparameter. And essentially,\nit's sort of meaning that your policy might\nchange further than that,",
    "start": "4113250",
    "end": "4120790"
  },
  {
    "text": "but that's not going to\nbenefit your loss function. So this, again, is going to\nconstrain your policy class",
    "start": "4120790",
    "end": "4127318"
  },
  {
    "text": "to stay within this\nregion, for which it's making similar decisions.",
    "start": "4127319",
    "end": "4133028"
  },
  {
    "text": "So we're still really\nfocusing on what actions are we actually taking. Are we taking similar\nactions in these states,",
    "start": "4133029",
    "end": "4138389"
  },
  {
    "text": "as we would be normally,\nregardless of how much my theta is changing?",
    "start": "4138390",
    "end": "4143509"
  },
  {
    "text": "And then you just do\nyour policy update by taking an argmax over this. So this is your clipped\nobjective function.",
    "start": "4143510",
    "end": "4151068"
  },
  {
    "text": "All right. So let's see how this works. Let's think about\nwhat it's doing. So we'll do a quick\nCheck Your Understanding.",
    "start": "4151069",
    "end": "4159630"
  },
  {
    "text": "So this shows you\nwhat L clip does. This is L clip, as well.",
    "start": "4159630",
    "end": "4165568"
  },
  {
    "text": "And what I'd like\nyou to think about is, what does this look like,\ndepending on the advantage",
    "start": "4165569",
    "end": "4171089"
  },
  {
    "text": "function. So L. Let me just write it down. L clip.",
    "start": "4171090",
    "end": "4177969"
  },
  {
    "text": "OK, so this is r. So on the x-axis is r. And then on the\ny-axis is L clip.",
    "start": "4177970",
    "end": "4184907"
  },
  {
    "text": "And what this is asking\nyou to think about, this is from their paper, is to\nthink about what does clipping",
    "start": "4184908",
    "end": "4191500"
  },
  {
    "text": "do in terms of your loss. And so I'd like\nyou to think about,",
    "start": "4191500",
    "end": "4197590"
  },
  {
    "text": "in this case, which of\nthese two, if either, match within the advantage\nfunction as positive",
    "start": "4197590",
    "end": "4204850"
  },
  {
    "text": "or the advantage\nfunction as negative. So a here is the advantage. Let me just make that clear.",
    "start": "4204850",
    "end": "4211380"
  },
  {
    "text": "A is equal to the advantage. So just think of this\nfor a single term. Consider for one term.",
    "start": "4211380",
    "end": "4221343"
  },
  {
    "text": "So just this part. So just for one single rta,\nwhat is happening here?",
    "start": "4221343",
    "end": "4231136"
  },
  {
    "text": "And just to be clear here,\nwhat we're doing in this case is we're taking the minimum\nbetween the normal thing we do, which is this\nreweighted advantage",
    "start": "4231137",
    "end": "4237417"
  },
  {
    "text": "function times a clip of the r\ntimes the advantage function. ",
    "start": "4237417",
    "end": "4264490"
  },
  {
    "text": "And feel free to\nflip back and forth or play with\nnumbers, just to get some intuition of what is this\ndoing to our loss function.",
    "start": "4264490",
    "end": "4275100"
  },
  {
    "text": " Or I shouldn't say loss.",
    "start": "4275100",
    "end": "4281710"
  },
  {
    "text": "Our objective\nfunction, in this case, because we're trying to\ntake the argmax of it. So we're thinking\nof this as sort of an approximation of how much\nis our policy going to improve",
    "start": "4281710",
    "end": "4289330"
  },
  {
    "text": "when we change our theta. So we're going to want to take\na max of this over with respect to a new policy theta.",
    "start": "4289330",
    "end": "4295120"
  },
  {
    "text": "And we want to\nthink about, this is sort of bounding what that new\nperformance benefit could be,",
    "start": "4295120",
    "end": "4302500"
  },
  {
    "text": "and how does that vary with\nrespect to the advantage. ",
    "start": "4302500",
    "end": "4348989"
  },
  {
    "text": "Nobody thinks it depends on the\nvalue of e, which is correct. So this does not depend\non the value of e.",
    "start": "4348990",
    "end": "4356472"
  },
  {
    "text": "Why don't you turn\nto your neighbor and see if you got\nthe same thing? ",
    "start": "4356472",
    "end": "4364538"
  },
  {
    "text": "[INTERPOSING VOICES]",
    "start": "4364538",
    "end": "4470190"
  },
  {
    "text": "Cool. It looks like talking converged\nmost people, which is great. So the first one is correct.",
    "start": "4470190",
    "end": "4479370"
  },
  {
    "text": "So this is a greater than 0. This is a less than 0. Does someone want\nto explain why?",
    "start": "4479370",
    "end": "4485660"
  },
  {
    "text": " Most of you that\nvoted got it right.",
    "start": "4485660",
    "end": "4491830"
  },
  {
    "start": "4491830",
    "end": "4497620"
  },
  {
    "text": "Well, it is quite simple\nbecause the simply let a coefficient to the value.",
    "start": "4497620",
    "end": "4505670"
  },
  {
    "text": "So it has a positive\nslope, and it is positive and negative slope. Then it's negative.",
    "start": "4505670",
    "end": "4511210"
  },
  {
    "text": "Yeah, so if we just\nfocus on this for a being equal to\ngreater than 0, what will happen is, as that ratio\nr gets higher, and higher,",
    "start": "4511210",
    "end": "4520150"
  },
  {
    "text": "and higher, you'll\njust linearly go up because it's just\nsomething between 0 and 1",
    "start": "4520150",
    "end": "4525880"
  },
  {
    "text": "that's getting\nlarger and larger. r can never be negative. So it's just useful\nto see in this case.",
    "start": "4525880",
    "end": "4532040"
  },
  {
    "text": "So as it's getting\nlarger and larger, it's just going to\nincrease your L clip value. But at some point, you're going\nto run up against this part.",
    "start": "4532040",
    "end": "4540940"
  },
  {
    "text": "And so at this part,\nyou're going to clip it, and you can't get\nhigher anymore.",
    "start": "4540940",
    "end": "4547023"
  },
  {
    "text": "Remember, in\ngeneral, we're always trying to maximize L clip. In this case, when the\nadvantage is negative,",
    "start": "4547023",
    "end": "4552440"
  },
  {
    "text": "you're trying to reduce the\namount of probability mass you put on that action,\nbecause you don't",
    "start": "4552440",
    "end": "4558500"
  },
  {
    "text": "want to take that anymore. I got a negative advantage,\nso I need to stop doing that. So essentially, you\nwant to be sweeping,",
    "start": "4558500",
    "end": "4564660"
  },
  {
    "text": "changing your policy in\nthe opposite direction. You'd really like to be\nable to push r all away to 0 and say, I never want\nto do that action again.",
    "start": "4564660",
    "end": "4570860"
  },
  {
    "text": "It gave me a negative advantage. But you can't do that because\nthat might change, radically change your policy.",
    "start": "4570860",
    "end": "4577530"
  },
  {
    "text": "And so once you get to 1\nminus epsilon, you cap it, and you can't further\nshrink it to 0.",
    "start": "4577530",
    "end": "4584060"
  },
  {
    "text": "Great. OK. ",
    "start": "4584060",
    "end": "4589720"
  },
  {
    "text": "And another way to see\nthis as from the paper is, you can think about these\ndifferent types of constraints and different clipping.",
    "start": "4589720",
    "end": "4595070"
  },
  {
    "text": "And essentially,\nagain, it's sort of making the objective\npessimistic as you get really far from theta.",
    "start": "4595070",
    "end": "4600633"
  },
  {
    "text": "Now, just in the last\ncouple of minutes, I want to make sure\nto show some plots. So this is just\nthe same algorithm,",
    "start": "4600633",
    "end": "4607360"
  },
  {
    "text": "but we're doing clipping. I will just note here that\nnext week, so next time,",
    "start": "4607360",
    "end": "4615250"
  },
  {
    "text": "we'll discuss-- next time, we're going to\ndiscuss the choice of a further. Just like what we\nsaw earlier today,",
    "start": "4615250",
    "end": "4621170"
  },
  {
    "text": "you can do n step\nestimators, et cetera. And you can do what's\ncalled generalized advantage estimation.",
    "start": "4621170",
    "end": "4626530"
  },
  {
    "text": "You don't need to\nknow that for this. We won't cover that\nfor today, but we'll cover it more next week. So just to note, there's\nsome additional choices here.",
    "start": "4626530",
    "end": "4633770"
  },
  {
    "text": "But let's just look at\nwhat the performance is. So at this point, TRPO and some\nother algorithms were out there.",
    "start": "4633770",
    "end": "4641830"
  },
  {
    "text": "They have the PPO\nclipping in purple. This is a number of\ndifferent MuJoCo domains,",
    "start": "4641830",
    "end": "4646989"
  },
  {
    "text": "similar to MuJoCo domains\nyou're going to be working with. And what you can see\nhere is that, in general,",
    "start": "4646990",
    "end": "4652530"
  },
  {
    "text": "so TRPO has this\ntrust region idea, and it's similar in some ways\nto what they're doing in PPO.",
    "start": "4652530",
    "end": "4659730"
  },
  {
    "text": "But trust region\npolicy optimization is quite a bit more complicated. And what you can see here is\nthat this light purple, which",
    "start": "4659730",
    "end": "4665900"
  },
  {
    "text": "is PPO, this is the\nnumber of steps, is generally doing just much\nbetter than these other ones.",
    "start": "4665900",
    "end": "4673320"
  },
  {
    "text": "So they're not claiming they're\ngoing to get to a better optima. They're just saying,\nwe're going to be",
    "start": "4673320",
    "end": "4678710"
  },
  {
    "text": "able to get there much\nfaster with much less data. And in some of these\ncases, this is really an enormous\nperformance improvement",
    "start": "4678710",
    "end": "4685370"
  },
  {
    "text": "for the same amount of data. So this is one of the reasons\nwhy it became extremely popular,",
    "start": "4685370",
    "end": "4692660"
  },
  {
    "text": "is it is a pretty simple\nalgorithm to implement, and it has extremely good\nperformance in many cases.",
    "start": "4692660",
    "end": "4699170"
  },
  {
    "text": "Now, so you can go to the\noriginal paper, proximal policy optimization algorithms, or\ngo to the blog post from 2017.",
    "start": "4699170",
    "end": "4708110"
  },
  {
    "text": "I do think one thing\nthat's good to know, and really, throughout\nmuch of our recent history,",
    "start": "4708110",
    "end": "4713690"
  },
  {
    "text": "is to also understand what are\nthe implementation details. So this is optional. You don't have to\nread it, but there was a nice paper\nthat followed up",
    "start": "4713690",
    "end": "4719900"
  },
  {
    "text": "from this work\nbecause, again, PPO has been hugely influential\nfrom some colleagues at MIT that",
    "start": "4719900",
    "end": "4726230"
  },
  {
    "text": "said, well, really, which of\nthese things are most important. Because in general,\nin these algorithms, there will be these\nthings, but then there's",
    "start": "4726230",
    "end": "4732903"
  },
  {
    "text": "also some hyperparameters or\narchitecture choices, et cetera. And so knowing how these\nchoices are made often",
    "start": "4732903",
    "end": "4738889"
  },
  {
    "text": "do make a big\ndifference in reality. And so that's\nalways good to know. It's whether or not is it\nan algorithmic improvement,",
    "start": "4738890",
    "end": "4744830"
  },
  {
    "text": "or are there additional\nthings that we're not treating as part\nof the algorithm but actually are\nreally important for practical performance.",
    "start": "4744830",
    "end": "4750505"
  },
  {
    "text": "So you should know\neverything now that you need to for making\ngood progress on Homework 2. And we'll continue to\ndiscuss this next week.",
    "start": "4750505",
    "end": "4757160"
  },
  {
    "text": "Thanks ",
    "start": "4757160",
    "end": "4763000"
  }
]