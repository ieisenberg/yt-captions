[
  {
    "text": " SPEAKER 1: Hi, everyone.",
    "start": "0",
    "end": "6550"
  },
  {
    "text": "Welcome to the 224N Hugging\nFace Transformers tutorial.",
    "start": "6550",
    "end": "12280"
  },
  {
    "text": "So this tutorial is just going\nto be about using the Hugging Face library.",
    "start": "12280",
    "end": "17460"
  },
  {
    "text": "It's really useful and\nit's a super effective way of being able to use some\noff-the-shelf NLP models,",
    "start": "17460",
    "end": "24510"
  },
  {
    "text": "specifically, models that are\nkind of transformer-based. And being able to use those\nfor either your final project,",
    "start": "24510",
    "end": "33010"
  },
  {
    "text": "your custom final project,\nor something like that. Just using it in the future. So these are-- it's a really\nhelpful package to learn.",
    "start": "33010",
    "end": "41039"
  },
  {
    "text": "And it interfaces really well\nwith PyTorch in particular too. OK, so first things\nfirst is in case",
    "start": "41040",
    "end": "49350"
  },
  {
    "text": "there's anything else\nthat you are missing from this kind of tutorial,\nthe Hugging Face documentation",
    "start": "49350",
    "end": "55260"
  },
  {
    "text": "is really good. They also have lots of\ntutorials and walkthroughs as well as other kind\nof like notebooks",
    "start": "55260",
    "end": "61590"
  },
  {
    "text": "that you can play\naround with as well. So if you're ever wondering\nabout something else, that's a really\ngood place to look.",
    "start": "61590",
    "end": "68650"
  },
  {
    "text": "OK, so in the Colab,\nthe first thing we're going to do that I already\ndid, but can maybe run again, is just installing the\nTransformers Python package",
    "start": "68650",
    "end": "77200"
  },
  {
    "text": "and then the data\nsets Python package. So this corresponds to the\nHugging Face Transformers",
    "start": "77200",
    "end": "84189"
  },
  {
    "text": "and data sets. And so those are really helpful. The Transformers\nis where we'll get",
    "start": "84190",
    "end": "89230"
  },
  {
    "text": "a lot of these kind of\npre-trained models from. And the data sets will\ngive us some helpful data sets that we can potentially\nuse for various tasks.",
    "start": "89230",
    "end": "96860"
  },
  {
    "text": "So, in this case,\nsentiment analysis.  OK, and so we'll use a bit of a\nhelper function for helping us",
    "start": "96860",
    "end": "105370"
  },
  {
    "text": "understand what encoding is-- what encodings are\nactually happening as well.",
    "start": "105370",
    "end": "110750"
  },
  {
    "text": "So we'll run this just\nto kind of kick things off and import a\nfew more things.",
    "start": "110750",
    "end": "116950"
  },
  {
    "text": "OK, so first, what\nwe'll do is this is generally kind of like the\nstep-by-step for how to use",
    "start": "116950",
    "end": "123909"
  },
  {
    "text": "something off of Hugging Face. So first what we'll do\nis we'll find some model",
    "start": "123910",
    "end": "129490"
  },
  {
    "text": "from the Hugging Face Hub here. And note that there's a\nton of different models",
    "start": "129490",
    "end": "135519"
  },
  {
    "text": "that you're able to use. There's BERT, there's\nGPT-2, there's t5-small,",
    "start": "135520",
    "end": "140570"
  },
  {
    "text": "which is another language\nmodel from Google. So there are a bunch of\nthese different models that",
    "start": "140570",
    "end": "146963"
  },
  {
    "text": "are pre-trained and\nall of these weights are up here in Hugging Face\nthat are freely available for you guys to download.",
    "start": "146963",
    "end": "153400"
  },
  {
    "text": "So if there's a particular\nmodel you're interested in, you can probably find\na version of it here.",
    "start": "153400",
    "end": "158530"
  },
  {
    "text": "You can also see different\ntypes of models on the side as well that-- for a specific task.",
    "start": "158530",
    "end": "164909"
  },
  {
    "text": "So if we wanted to do something\nlike zero shot classification,",
    "start": "164910",
    "end": "169940"
  },
  {
    "text": "there are a couple models that\nare specifically good at doing that particular task.",
    "start": "169940",
    "end": "175310"
  },
  {
    "text": "So based off of what\ntask you're looking for, there's probably a\nHugging Face model for it that's available online\nfor you to download.",
    "start": "175310",
    "end": "182570"
  },
  {
    "text": "OK, so that's what\nwe'll do first is we'll go ahead and find a\nmodel in the Hugging Face Hub.",
    "start": "182570",
    "end": "188870"
  },
  {
    "text": "And then, whatever you\nwant to do, in this case, we'll do sentiment analysis.",
    "start": "188870",
    "end": "193940"
  },
  {
    "text": "And then, there are two\nthings that we need next. The first is a tokenizer for\nactually splitting your input",
    "start": "193940",
    "end": "201140"
  },
  {
    "text": "text into tokens that\nyour model can use and the actual model itself.",
    "start": "201140",
    "end": "207360"
  },
  {
    "text": "And so the tokenizer,\nagain, converts this to some vocabulary IDs.",
    "start": "207360",
    "end": "212629"
  },
  {
    "text": "These discrete IDs that your\nmodel can actually take in. And the model will produce some\nprediction based off of that.",
    "start": "212630",
    "end": "219590"
  },
  {
    "text": "OK, so first, what\nwe can do is, again, import this auto tokenizer\nand this AutoModel from--",
    "start": "219590",
    "end": "229910"
  },
  {
    "text": "for sequence classification. So what this will\ndo, initially, is download some of the key\nthings that we need so that we",
    "start": "229910",
    "end": "237680"
  },
  {
    "text": "can actually initialize these. So what do each of these do? So first, the tokenizer,\nthis auto tokenizer,",
    "start": "237680",
    "end": "244860"
  },
  {
    "text": "is from some\npre-trained tokenizer that has already been used.",
    "start": "244860",
    "end": "250060"
  },
  {
    "text": "So in general, there's a\ncorresponding tokenizer for every model that\nyou want to try and use. In this case, it's like\nSiEBERT, so like something",
    "start": "250060",
    "end": "258450"
  },
  {
    "text": "around Sentiment in RoBERTa. And then, the second is\nyou can import this model",
    "start": "258450",
    "end": "263790"
  },
  {
    "text": "for sequence classification as\nwell from something pre-trained on the model hub again.",
    "start": "263790",
    "end": "269430"
  },
  {
    "text": "So again, this corresponds\nto Sentiment, RoBERTa, Large English. And if we want, we can\neven find this over here.",
    "start": "269430",
    "end": "277050"
  },
  {
    "text": "We can find it as\nI think English.",
    "start": "277050",
    "end": "285110"
  },
  {
    "text": "Yeah, large English. So again, this is something\nwe can easily find. You just copy this\nstring up here",
    "start": "285110",
    "end": "290270"
  },
  {
    "text": "and then you can import that. OK, we've downloaded all\nthe things that we need,",
    "start": "290270",
    "end": "296850"
  },
  {
    "text": "some kind of binary\nfiles as well. And then, now, we can\ngo ahead and actually",
    "start": "296850",
    "end": "302070"
  },
  {
    "text": "use some of these inputs, right? So this gives you some\nset of an input, right? This input string, I'm excited\nto learn about Hugging Face",
    "start": "302070",
    "end": "309509"
  },
  {
    "text": "Transformers. We'll get some\ntokenized inputs here",
    "start": "309510",
    "end": "314910"
  },
  {
    "text": "from the actual tokenized\nthings here after we pass it through the tokenizer.",
    "start": "314910",
    "end": "320430"
  },
  {
    "text": "And then, lastly, we'll get\nsome notion of the model output that we get, right?",
    "start": "320430",
    "end": "325680"
  },
  {
    "text": "So this is kind of some\nlogits here over whatever classification that we have. So in this case, good or bad.",
    "start": "325680",
    "end": "332350"
  },
  {
    "text": "And then, some\ncorresponding prediction. And we'll walk through\nwhat this kind of looks like in just a second as\nwell in a little more depth.",
    "start": "332350",
    "end": "339660"
  },
  {
    "text": "But this is broadly how we can\nactually use these together. We'll tokenize some input.",
    "start": "339660",
    "end": "345389"
  },
  {
    "text": "And then, we'll pass these\ninputs through the model. So we'll talk about\ntokenizers first. So tokenizers are used\nfor, basically, just",
    "start": "345390",
    "end": "354900"
  },
  {
    "text": "pre-processing the inputs\nthat you get for any model. And it takes some\nraw string to like--",
    "start": "354900",
    "end": "360030"
  },
  {
    "text": "essentially a mapping\nto some number or ID that the model can take\nin and actually understand.",
    "start": "360030",
    "end": "367920"
  },
  {
    "text": "So tokenizers are\neither kind of like-- are specific to the model\nthat you want to use,",
    "start": "367920",
    "end": "374400"
  },
  {
    "text": "or you can use the\nauto tokenizer that will kind of conveniently\nimport whatever",
    "start": "374400",
    "end": "379830"
  },
  {
    "text": "corresponding tokenizer you\nneed for that model type. So that's kind of like the\nhelpfulness of the auto",
    "start": "379830",
    "end": "386910"
  },
  {
    "text": "tokenizer. It'll kind of make\nthat selection for you and make sure that you get the\ncorrect tokenizer for whatever",
    "start": "386910",
    "end": "394140"
  },
  {
    "text": "model you're using. So the question is,\ndoes it, make sure that everything is mapped\nto the correct index",
    "start": "394140",
    "end": "399420"
  },
  {
    "text": "that the model is trained on? The answer is yes. So that's why the auto\ntokenizer is helpful. So there are two\ntypes of tokenizers.",
    "start": "399420",
    "end": "407940"
  },
  {
    "text": "There's the Python tokenizer. And there's also\na tokenizer fast.",
    "start": "407940",
    "end": "415330"
  },
  {
    "text": "The tokenizer fast\nis written in Rust. In general, if you do\nthe auto tokenizer, it will just default\nto the fast one.",
    "start": "415330",
    "end": "422410"
  },
  {
    "text": "There's not really a\nhuge difference here. It's just about\nthe inference time for getting the model outputs.",
    "start": "422410",
    "end": "428860"
  },
  {
    "text": "Yeah, so the question\nis the tokenizer creates dictionaries\nof the model inputs.",
    "start": "428860",
    "end": "435070"
  },
  {
    "text": "So it's more like-- I think the way to\nthink about a tokenizer",
    "start": "435070",
    "end": "440080"
  },
  {
    "text": "is that dictionary\nalmost, right? So you want to kind of translate\nalmost or have this mapping",
    "start": "440080",
    "end": "448780"
  },
  {
    "text": "from the tokens that you\ncan get from this string. And then, map that\ninto some inputs",
    "start": "448780",
    "end": "455590"
  },
  {
    "text": "that the model\nwill actually use. So we'll see an example\nof that in just a second. So for example, we can kind of\ncall the tokenizer in any way",
    "start": "455590",
    "end": "464080"
  },
  {
    "text": "that we would for a\ntypical PyTorch model. But we're just going\nto call it on a string. So here, we have\nour input string",
    "start": "464080",
    "end": "471070"
  },
  {
    "text": "is Hugging Face\nTransformers is great. We pass that into the\ntokenizer, almost like it's",
    "start": "471070",
    "end": "476380"
  },
  {
    "text": "like a function, right? And then, we'll get\nout some tokenization. So this gives us a\nset of input IDs.",
    "start": "476380",
    "end": "483310"
  },
  {
    "text": "So to answer the\nearlier question, these are basically the numbers\nthat each of these tokens",
    "start": "483310",
    "end": "488780"
  },
  {
    "text": "represent, right? So that the model can\nactually use them. And then, a\ncorresponding attention",
    "start": "488780",
    "end": "495140"
  },
  {
    "text": "mask for the particular\ntransformer, OK?",
    "start": "495140",
    "end": "501210"
  },
  {
    "text": "So there are a couple\nways of accessing the actual tokenized input IDs.",
    "start": "501210",
    "end": "507600"
  },
  {
    "text": "You can treat it\nlike a dictionary. So hence, kind of thinking about\nit almost as that dictionary form.",
    "start": "507600",
    "end": "512610"
  },
  {
    "text": "It's also just like a property\nof the output that you get. So there are two\nways of accessing this in a pretty Pythonic way.",
    "start": "512610",
    "end": "519510"
  },
  {
    "text": " OK, so what we\ncan see as well is",
    "start": "519510",
    "end": "526560"
  },
  {
    "text": "that we can look at the\nparticular-- the actual kind of tokenization process almost.",
    "start": "526560",
    "end": "532060"
  },
  {
    "text": "And so this can maybe give\nsome insight into what happens at each step, right? So our initial input\nstring is going",
    "start": "532060",
    "end": "538980"
  },
  {
    "text": "to be Hugging Face\nTransformers is great. OK, the next step\nis that we actually",
    "start": "538980",
    "end": "544060"
  },
  {
    "text": "want to tokenize these\nindividual words that",
    "start": "544060",
    "end": "549850"
  },
  {
    "text": "are passed in. So here, this is the kind of\noutput of this tokenization step, right?",
    "start": "549850",
    "end": "555970"
  },
  {
    "text": "We get these individual\nsplit tokens. We'll convert them to IDs here.",
    "start": "555970",
    "end": "563230"
  },
  {
    "text": "And then, we'll add\nany special tokens that our model might need for\nactually performing inference",
    "start": "563230",
    "end": "569455"
  },
  {
    "text": "on this.  So there's a couple steps that\nhappen kind of underneath when",
    "start": "569455",
    "end": "577240"
  },
  {
    "text": "you use an actual-- when you\nuse a tokenizer that happens at a few things at a time.",
    "start": "577240",
    "end": "584680"
  },
  {
    "text": "One thing to note is that\nfor fast tokenizers as well, there is another option\nthat you're able to get to.",
    "start": "584680",
    "end": "592529"
  },
  {
    "text": "So you have, essentially,\nyou have this input string. You have the number of\ntokens that you get.",
    "start": "592530",
    "end": "599490"
  },
  {
    "text": "And you might have some notion\nof the special token mask as well.",
    "start": "599490",
    "end": "604810"
  },
  {
    "text": "So using char2word\nis going to give you the word piece of a particular\ncharacter in the input.",
    "start": "604810",
    "end": "611819"
  },
  {
    "text": "So here, this is just giving\nyou additional options that you can use for\nthe fast tokenizer as well for understanding\nhow the tokens are being",
    "start": "611820",
    "end": "618740"
  },
  {
    "text": "used from the input string. ",
    "start": "618740",
    "end": "626420"
  },
  {
    "text": "OK, so there are different\nways of using the outputs of these tokenizers too.",
    "start": "626420",
    "end": "631759"
  },
  {
    "text": "So one is that you\ncan pass this in. And if you indicate that you\nwant it to return a tensor,",
    "start": "631760",
    "end": "639620"
  },
  {
    "text": "you can also return\na PyTorch tensor. So that's great in case you\nneed a PyTorch tensor, which",
    "start": "639620",
    "end": "647450"
  },
  {
    "text": "you probably generally want. You can also add multiple\ntokens into the tokenizer,",
    "start": "647450",
    "end": "653420"
  },
  {
    "text": "and then, pad them\nas however you need. So for here, for example,\nwe can use the pad token",
    "start": "653420",
    "end": "661790"
  },
  {
    "text": "as being this kind of\nlike pad bracket almost. And giving the token ID is\ngoing to correspond to 0.",
    "start": "661790",
    "end": "669260"
  },
  {
    "text": "So this is just going to add\npadding to whatever input that you give. So if you need\nyour outputs to be",
    "start": "669260",
    "end": "676370"
  },
  {
    "text": "the same length for a\nparticular type of model, right, this will add\nthose padding tokens. And then,\ncorrespondingly, gives you",
    "start": "676370",
    "end": "682790"
  },
  {
    "text": "the zeros and the attention\nmask where you actually need it.",
    "start": "682790",
    "end": "688300"
  },
  {
    "text": "OK, and so the way to do\nthat here is you basically set padding to be true.",
    "start": "688300",
    "end": "694880"
  },
  {
    "text": "You can also set truncation\nto be true as well. And so if you ever have more-- ",
    "start": "694880",
    "end": "702459"
  },
  {
    "text": "any other kind of features\nof the tokenizer that you're interested in, again, you\ncan check the Hugging Face",
    "start": "702460",
    "end": "708009"
  },
  {
    "text": "documentation, which is\npretty thorough for what each of these things do. Yeah, so the question is looking\nat the hash hash, at least,",
    "start": "708010",
    "end": "718630"
  },
  {
    "text": "and whether that\nmeans that we should have a space before or not.",
    "start": "718630",
    "end": "724120"
  },
  {
    "text": "So here, in this case-- yeah, so in this\ncase, we probably",
    "start": "724120",
    "end": "729430"
  },
  {
    "text": "don't want like the\nspace before, right? Just because we have the\n\"Hugging--\" I don't know,",
    "start": "729430",
    "end": "736569"
  },
  {
    "text": "\"Hugging\" is all one\nword in this case. Generally, for the\ntokenizers, generally,",
    "start": "736570",
    "end": "745600"
  },
  {
    "text": "the output that they give\nis still pretty consistent though in terms of how the\ntokenization process works.",
    "start": "745600",
    "end": "752480"
  },
  {
    "text": "So there might be\nthese instances where it might be contrary\nto what you might expect",
    "start": "752480",
    "end": "758019"
  },
  {
    "text": "for how something is tokenized. In general, the tokenization\ngenerally works fine.",
    "start": "758020",
    "end": "765010"
  },
  {
    "text": "So in most cases,\nthe direct output that you get from the Hugging\nFace tokenizer is sufficient.",
    "start": "765010",
    "end": "772190"
  },
  {
    "start": "772190",
    "end": "777940"
  },
  {
    "text": "OK, awesome. So one last thing past the\nadding additional padding",
    "start": "777940",
    "end": "783460"
  },
  {
    "text": "is that you can also kind of\ndecode an entire batch at one",
    "start": "783460",
    "end": "789310"
  },
  {
    "text": "given time. So if we look again,\nwe have our tokenizer.",
    "start": "789310",
    "end": "795700"
  },
  {
    "text": "We'll additionally have this\nmethod called a batch decode. So if we have the model\ninputs that we get up here,",
    "start": "795700",
    "end": "802970"
  },
  {
    "text": "this is the output of\npassing these sentences or these strings\ninto the tokenizer.",
    "start": "802970",
    "end": "809560"
  },
  {
    "text": "We can go ahead and just pass\nthese input IDs that correspond",
    "start": "809560",
    "end": "814810"
  },
  {
    "text": "to that into the\nbatch decode and it'll give us this decoding\nthat corresponds",
    "start": "814810",
    "end": "821140"
  },
  {
    "text": "to all the padding we added in. Each of the particular\nkind of words and strings.",
    "start": "821140",
    "end": "828520"
  },
  {
    "text": "And if you want to ignore all\nof the presence of these padding tokens or anything\nlike that, you",
    "start": "828520",
    "end": "835570"
  },
  {
    "text": "can also pass that in as\nskipping the special tokens. Gotcha.",
    "start": "835570",
    "end": "840660"
  },
  {
    "text": "So this gives like-- this\nis a pretty high level overview of the-- how you\nwould want to use tokenizers,",
    "start": "840660",
    "end": "846690"
  },
  {
    "text": "I guess, in using Hugging Face. So now we can talk about maybe\nhow to use the Hugging Face",
    "start": "846690",
    "end": "855110"
  },
  {
    "text": "models themselves. So again, this is\npretty similar to what we're seeing for something\nlike initializing a tokenizer.",
    "start": "855110",
    "end": "863930"
  },
  {
    "text": "You just choose the specific\nmodel type for your model. And then, and you can use\nthat or the specific kind",
    "start": "863930",
    "end": "872000"
  },
  {
    "text": "of AutoModel class. Where, again, this AutoModel\nkind of takes almost",
    "start": "872000",
    "end": "877129"
  },
  {
    "text": "the initialization process. It takes care of it for you in\na pretty easy way without really",
    "start": "877130",
    "end": "884510"
  },
  {
    "text": "any too much overhead. So additionally, so for the\npre-trained Transformers",
    "start": "884510",
    "end": "891649"
  },
  {
    "text": "that we have, they generally\nhave the same underlying architecture. But you'll have different\nkind of heads associated",
    "start": "891650",
    "end": "899450"
  },
  {
    "text": "with each Transformer. So attention heads\nso you might have to train if you're doing some\nsequence classification or just",
    "start": "899450",
    "end": "905390"
  },
  {
    "text": "some other task. So Hugging Face will\ndo this for you. And so, for this, we'll\nwalk through an example",
    "start": "905390",
    "end": "913519"
  },
  {
    "text": "of how to do this for\nsentiment analysis. So if there's a specific context\nlike sequence classification",
    "start": "913520",
    "end": "920850"
  },
  {
    "text": "we want to use, we\ncan use like this-- the very specific kind of\nlike class Hugging Face",
    "start": "920850",
    "end": "927560"
  },
  {
    "text": "provides, so DistilBERT for\nsequence classification. Alternatively, if we were\ndoing it using DistilBERT",
    "start": "927560",
    "end": "934630"
  },
  {
    "text": "in a masked language\nmodel setting, we use DistilBERT for masked LM.",
    "start": "934630",
    "end": "939820"
  },
  {
    "text": "And then, lastly,\nif we're just doing it purely for the\nrepresentations that we get out\nof DistilBERT, we",
    "start": "939820",
    "end": "945310"
  },
  {
    "text": "can just use the baseline model. So the key thing\nhere, or key takeaway, is that there are some task\nspecific classes that we",
    "start": "945310",
    "end": "952930"
  },
  {
    "text": "can use from Hugging\nFace to initialize. So AutoModel, again, is\nsimilar to the auto tokenizer.",
    "start": "952930",
    "end": "961300"
  },
  {
    "text": "So for this, it's just\ngoing to kind of load by default that specific model.",
    "start": "961300",
    "end": "968150"
  },
  {
    "text": "And so, in this case, it's going\nto be just the basic weights that you need for that.",
    "start": "968150",
    "end": "973810"
  },
  {
    "text": " OK, so here, we'll have\nbasically three different types",
    "start": "973810",
    "end": "982940"
  },
  {
    "text": "of models that we can look at. One is like an encoder\ntype model, which is BERT. A decoder type model,\nlike GPT-2 that's",
    "start": "982940",
    "end": "992540"
  },
  {
    "text": "performing these-- generating\nsome text potentially. And encoder decoder, models\nso BART or T5, in this case.",
    "start": "992540",
    "end": "1000709"
  },
  {
    "text": "So again, if you go back to\nkind of the Hugging Face Hub, there's a whole sort of\ndifferent types of models",
    "start": "1000710",
    "end": "1009100"
  },
  {
    "text": "that you could potentially use. And if we look in the\ndocumentation as well, so here,",
    "start": "1009100",
    "end": "1016060"
  },
  {
    "text": "we can understand some notion of\nthe different types of classes that we might want to use.",
    "start": "1016060",
    "end": "1022240"
  },
  {
    "text": "So there's some notion of the\nauto tokenizer, different auto models for different\ntypes of tasks.",
    "start": "1022240",
    "end": "1029689"
  },
  {
    "text": "So here, again, if you have\nany kind of specific use cases that you're looking\nfor, then you",
    "start": "1029690",
    "end": "1035260"
  },
  {
    "text": "can check the documentation. Here, again, if you use an\nAutoModel from pre-trained,",
    "start": "1035260",
    "end": "1041035"
  },
  {
    "text": "you'll just create\na model that's an instance of that BERT model. In this case, BERT model\nfor the BERT-base case.",
    "start": "1041035",
    "end": "1048800"
  },
  {
    "start": "1048800",
    "end": "1053840"
  },
  {
    "text": "We can go ahead and start. One last thing to note is that,\nagain, the particular choice",
    "start": "1053840",
    "end": "1059540"
  },
  {
    "text": "of your model\nmatches up with kind of the type of architecture\nthat you have to use, right?",
    "start": "1059540",
    "end": "1066169"
  },
  {
    "text": "These different types of models\ncan perform specific tasks. So you're not going to\nbe able to load or use",
    "start": "1066170",
    "end": "1074390"
  },
  {
    "text": "BERT, for instance, or\nDistilBERT as a sequence to sequence model, for instance,\nwhich requires the encoder",
    "start": "1074390",
    "end": "1080720"
  },
  {
    "text": "and decoder because DistilBERT\nonly consists of an encoder.",
    "start": "1080720",
    "end": "1086330"
  },
  {
    "text": "So there's a bit of a limitation\non how you can exactly use these, but it's,\nbasically, based on the model",
    "start": "1086330",
    "end": "1092570"
  },
  {
    "text": "architecture itself.  OK, awesome.",
    "start": "1092570",
    "end": "1097960"
  },
  {
    "text": "So let's go ahead\nand get started here. So similarly here, we\ncan import to AutoModel",
    "start": "1097960",
    "end": "1106390"
  },
  {
    "text": "for sequence classification. So again, this is-- we're going\nto perform some classification task and we'll\nimport this AutoModel",
    "start": "1106390",
    "end": "1113530"
  },
  {
    "text": "here so that we don't\nhave to reference, again, something like DistilBERT\nfor sequence classification.",
    "start": "1113530",
    "end": "1119627"
  },
  {
    "text": "We'll be able to\nload it automatically and it'll be all set. Alternatively, we can do\nDistilBERT for sequence",
    "start": "1119627",
    "end": "1126460"
  },
  {
    "text": "classification here. And that specifically\nwill require DistilBERT to be the input there.",
    "start": "1126460",
    "end": "1133090"
  },
  {
    "text": "OK, so these are two different\nways of basically getting the same model here, one using\nthe AutoModel, one using just",
    "start": "1133090",
    "end": "1139960"
  },
  {
    "text": "explicitly DistilBERT. Cool. And here, because\nit's classification,",
    "start": "1139960",
    "end": "1145460"
  },
  {
    "text": "we need to specify the number of\nlabels or the number of classes that we're actually going to\nclassify for each of the input",
    "start": "1145460",
    "end": "1151190"
  },
  {
    "text": "sentences. OK, so here, we'll get some--",
    "start": "1151190",
    "end": "1157090"
  },
  {
    "text": "like a Warning here if\nyou are following along and you print this\nout because some",
    "start": "1157090",
    "end": "1162220"
  },
  {
    "text": "of the sequence classification\nparameters aren't trained yet. And so we'll go ahead\nand take care of that.",
    "start": "1162220",
    "end": "1170010"
  },
  {
    "text": "So here, similarly, we'll walk\nthrough how to actually train",
    "start": "1170010",
    "end": "1177440"
  },
  {
    "text": "some of these models. So the first is, how do you\nactually pass any of the inputs that you get from a\ntokenizer into the model, OK?",
    "start": "1177440",
    "end": "1185419"
  },
  {
    "text": "Well, if we get some model\ninputs from the tokenizer up here and we pass\nthis into the model",
    "start": "1185420",
    "end": "1193250"
  },
  {
    "text": "by specifying that the\ninput IDs are the input IDs from the model inputs.",
    "start": "1193250",
    "end": "1199940"
  },
  {
    "text": "And likewise, we\nwant to emphasize or we can show here\nand specifically",
    "start": "1199940",
    "end": "1205490"
  },
  {
    "text": "pass in that the\nattention mask is going to correspond\nto the attention mask that we gave from these\noutputs of the tokenizer, OK?",
    "start": "1205490",
    "end": "1214620"
  },
  {
    "text": "So this is option one where\nyou can specifically identify which property goes to what.",
    "start": "1214620",
    "end": "1220950"
  },
  {
    "text": "The second option is using\nkind of a Pythonic hack",
    "start": "1220950",
    "end": "1227010"
  },
  {
    "text": "almost, which is\nwhere you can directly pass in the model inputs.",
    "start": "1227010",
    "end": "1232120"
  },
  {
    "text": "And so this will, basically,\nunpack almost the keys of the model inputs here.",
    "start": "1232120",
    "end": "1238870"
  },
  {
    "text": "So the model input keys, so the\ninput IDs, correspond to this. The attention mask corresponds\nto the attention mask argument.",
    "start": "1238870",
    "end": "1247210"
  },
  {
    "text": "So when we use this star\nstar kind of syntax, this will go ahead and\nunpack our dictionary",
    "start": "1247210",
    "end": "1253020"
  },
  {
    "text": "and, basically,\nmap the arguments to something of the same keys. So this is an alternative way\nof passing it into the model.",
    "start": "1253020",
    "end": "1260429"
  },
  {
    "text": "Both are going to be the same. OK, so now, what we can do is\nwe can actually print out what",
    "start": "1260430",
    "end": "1268600"
  },
  {
    "text": "the model outputs look like. So again, these are the inputs,\nthe token IDs and the attention",
    "start": "1268600",
    "end": "1274330"
  },
  {
    "text": "mask. And then, second, we'll get\nthe actual model outputs.",
    "start": "1274330",
    "end": "1279360"
  },
  {
    "text": "So here, notice that the outputs\nare given by these logits here.",
    "start": "1279360",
    "end": "1285020"
  },
  {
    "text": "There's two of them we\npassed in one example, and there's two\npotential classes that we're trying\nto classify, OK?",
    "start": "1285020",
    "end": "1291500"
  },
  {
    "text": "And then, lastly,\nwe have a course-- the corresponding distribution\nover the labels here, right?",
    "start": "1291500",
    "end": "1297799"
  },
  {
    "text": "Since this is going to\nbe binary classification. Yes, it's a little\nbit weird that you're",
    "start": "1297800",
    "end": "1302990"
  },
  {
    "text": "going to have the two classes\nfor the binary classification task. And you could,\nbasically, just choose",
    "start": "1302990",
    "end": "1308540"
  },
  {
    "text": "to classify one class or not. But we do this just,\nbasically, because of how",
    "start": "1308540",
    "end": "1314299"
  },
  {
    "text": "Hugging Face models are set up. And so, additionally,\nthese are-- the models",
    "start": "1314300",
    "end": "1322549"
  },
  {
    "text": "that we load in from Hugging\nFace are basically just PyTorch modules.",
    "start": "1322550",
    "end": "1327690"
  },
  {
    "text": "So these are the actual models. And we can use them\nin the same way that we've been\nusing models before.",
    "start": "1327690",
    "end": "1333360"
  },
  {
    "text": "So that means things like\nloss.backward or something like that actually will do\nthis back propagation step",
    "start": "1333360",
    "end": "1341059"
  },
  {
    "text": "corresponding to the loss of\nyour inputs that you pass in. So it's really easy to train\nthese guys as long as you",
    "start": "1341060",
    "end": "1348920"
  },
  {
    "text": "have a label for your data. You can calculate your loss\nusing the PyTorch cross entropy",
    "start": "1348920",
    "end": "1356330"
  },
  {
    "text": "function. You get some loss back. And then, you can go ahead\nand back propagate it.",
    "start": "1356330",
    "end": "1362290"
  },
  {
    "text": "You can actually even get the\nparameters as well in the model",
    "start": "1362290",
    "end": "1367390"
  },
  {
    "text": "that you're-- would probably get\nupdated from this. So this is just some big tensor\nof the actual embedding weights",
    "start": "1367390",
    "end": "1374429"
  },
  {
    "text": "that you have. OK, we also have\na pretty easy way",
    "start": "1374430",
    "end": "1380640"
  },
  {
    "text": "for Hugging Face itself to\nbe able to calculate the loss that we get.",
    "start": "1380640",
    "end": "1385720"
  },
  {
    "text": "So again, if we tokenize\nsome input string, we get our model inputs. We have two labels,\npositive and negative.",
    "start": "1385720",
    "end": "1393330"
  },
  {
    "text": "And then, give some kind\nof corresponding label that we assign to the model\ninputs and we pass this in.",
    "start": "1393330",
    "end": "1400500"
  },
  {
    "text": "We can see here that\nthe actual model outputs that are\ngiven by Hugging Face",
    "start": "1400500",
    "end": "1405840"
  },
  {
    "text": "includes this loss here, right? So it'll include the\nloss corresponding",
    "start": "1405840",
    "end": "1411150"
  },
  {
    "text": "to that input anyways. So it's a really easy way of\nactually calculating the loss",
    "start": "1411150",
    "end": "1417420"
  },
  {
    "text": "just natively in Hugging\nFace without having to call any additional things\nfrom a PyTorch library.",
    "start": "1417420",
    "end": "1424040"
  },
  {
    "text": "And then, lastly, we\ncan actually even use-- if we have kind of\nlike these two labels",
    "start": "1424040",
    "end": "1429380"
  },
  {
    "text": "here, again, for positive\nor negative, what we can do is just take the model\noutputs, look at the logits,",
    "start": "1429380",
    "end": "1437269"
  },
  {
    "text": "and see which one is\nlike the biggest again. We'll pass that and\ntake the argmax.",
    "start": "1437270",
    "end": "1443330"
  },
  {
    "text": "So that will give the\nindex that's largest. And then, that's\nthe output label that the model is\nactually predicting.",
    "start": "1443330",
    "end": "1449380"
  },
  {
    "text": "So again, it gives\na really easy way of being able to do this\nsort of classification, getting the loss, getting what\nthe actual labels are just from",
    "start": "1449380",
    "end": "1457477"
  },
  {
    "text": "within Hugging Face. ",
    "start": "1457477",
    "end": "1462480"
  },
  {
    "text": "OK, awesome. So well, last thing as\nwell is that we can also",
    "start": "1462480",
    "end": "1468149"
  },
  {
    "text": "look inside the model\nin a pretty cool way",
    "start": "1468150",
    "end": "1473190"
  },
  {
    "text": "and also seeing what\nthe attention weights the model actually puts-- the attention weights\nthe model actually has.",
    "start": "1473190",
    "end": "1480840"
  },
  {
    "text": "So this is helpful\nif you're trying to understand what's going\non inside of some NLP model.",
    "start": "1480840",
    "end": "1488380"
  },
  {
    "text": "And so, for here,\nwe can do, again, where we're importing our model\nfrom some pre-trained model",
    "start": "1488380",
    "end": "1498120"
  },
  {
    "text": "weights in the Hugging Face Hub. We want to output attention. Set output attentions to\ntrue and output hidden states",
    "start": "1498120",
    "end": "1506400"
  },
  {
    "text": "to true. So these are going to\nbe the key arguments that we can use for\nactually investigating",
    "start": "1506400",
    "end": "1512160"
  },
  {
    "text": "what's going on inside the\nmodel at each point in time. Again, we'll set the\nmodel to be in eval mode.",
    "start": "1512160",
    "end": "1520230"
  },
  {
    "text": "And lastly, we'll go ahead\nand tokenize our input string again.",
    "start": "1520230",
    "end": "1526159"
  },
  {
    "text": "We don't really care about\nany of the gradients here. Again, so we don't actually want\nto backpropagate anything here.",
    "start": "1526160",
    "end": "1533230"
  },
  {
    "text": "And finally, pass\nin the model inputs. So now, what we're\nable to do is when",
    "start": "1533230",
    "end": "1538960"
  },
  {
    "text": "we print out the\nmodel hidden states, so now this is a new kind\nof property in the output",
    "start": "1538960",
    "end": "1544780"
  },
  {
    "text": "dictionary that we get. We can look at what these\nactually look like here. And so this is a massive output.",
    "start": "1544780",
    "end": "1551365"
  },
  {
    "text": " So you can actually look at the\nhidden state size per layer,",
    "start": "1551365",
    "end": "1558060"
  },
  {
    "text": "right? And so this kind\nof gives a notion of what we're going to be\nlooking like-- looking at, what the shape of this is, at\neach given layer in our model,",
    "start": "1558060",
    "end": "1567110"
  },
  {
    "text": "as well as the attention\nhead size per layer. So this gives you\nthe kind of shape",
    "start": "1567110",
    "end": "1572299"
  },
  {
    "text": "of what you're looking at. And then, if we actually look\nat the model output itself,",
    "start": "1572300",
    "end": "1577890"
  },
  {
    "text": "we'll get all of these different\nhidden states basically, right?",
    "start": "1577890",
    "end": "1583760"
  },
  {
    "text": "So we have tons and tons of\nthese different hidden states. We'll have the last\nhidden state here.",
    "start": "1583760",
    "end": "1590970"
  },
  {
    "text": "So the model output is\npretty robust for showing you what the hidden state\nlooks like as well",
    "start": "1590970",
    "end": "1596630"
  },
  {
    "text": "as what attention weights\nactually look like here. So in case you're trying to\nanalyze a particular model,",
    "start": "1596630",
    "end": "1603280"
  },
  {
    "text": "this is a really helpful\nway of doing that. So what model.eval does is it--",
    "start": "1603280",
    "end": "1608860"
  },
  {
    "text": "sorry, question is,\nwhat does the .eval do? What it does is it\nbasically sets your--",
    "start": "1608860",
    "end": "1615330"
  },
  {
    "text": "and this is true for any\nPyTorch module or model-- is it sets it into quote\nunquote \"eval mode\".",
    "start": "1615330",
    "end": "1621780"
  },
  {
    "text": "So again, for this,\nwe're not really trying to calculate any of\nthe gradients or anything",
    "start": "1621780",
    "end": "1627539"
  },
  {
    "text": "like that that might correspond\nto correspond to some data",
    "start": "1627540",
    "end": "1633390"
  },
  {
    "text": "that we pass in or try and\nupdate our model in any way. We just care about evaluating it\non that particular data point.",
    "start": "1633390",
    "end": "1641680"
  },
  {
    "text": "So for that, then, it's\nhelpful to set the model into eval mode,\nessentially, to help",
    "start": "1641680",
    "end": "1648090"
  },
  {
    "text": "make sure that kind of\ndisables some of that stuff that you'd use\nduring training time.",
    "start": "1648090",
    "end": "1654880"
  },
  {
    "text": "So it just makes it a\nlittle more efficient. Yeah, the question is,\nit's already pre-trained, so can you go ahead\nand evaluate it?",
    "start": "1654880",
    "end": "1661220"
  },
  {
    "text": "Yeah, you can. So yeah, this is just\nthe raw pre-trained model with no fine tuning.",
    "start": "1661220",
    "end": "1667110"
  },
  {
    "text": "So the question is,\nhow do you interpret these shapes, basically,\nfor the attention head size",
    "start": "1667110",
    "end": "1674490"
  },
  {
    "text": "and then the hidden state size? Yeah, the key thing\nhere is you'll probably",
    "start": "1674490",
    "end": "1680790"
  },
  {
    "text": "want to look at the\nshape given on the side. It'll correspond to the\nlayer that you're actually",
    "start": "1680790",
    "end": "1686370"
  },
  {
    "text": "kind of like looking at. So here, when we call-- we\nlooked at the shape here.",
    "start": "1686370",
    "end": "1692640"
  },
  {
    "text": "We're specifically looking at\nthe first one in this list, right? So this will give us\nthe first hidden layer.",
    "start": "1692640",
    "end": "1700480"
  },
  {
    "text": "The second gives us\na notion of the batch that we're looking at.",
    "start": "1700480",
    "end": "1705760"
  },
  {
    "text": "And then, the last is like--\nso this is like some tensor, right? 768 dimensional, I don't\nknow, representation",
    "start": "1705760",
    "end": "1713159"
  },
  {
    "text": "that corresponds there. And then, for the\nattention head size, it corresponds to the actual\nquery word and the keyword",
    "start": "1713160",
    "end": "1721170"
  },
  {
    "text": "for these last two here. ",
    "start": "1721170",
    "end": "1729090"
  },
  {
    "text": "Yeah, so but for this,\nwe would expect this kind of initial index here, the one\nto be bigger if we printed out",
    "start": "1729090",
    "end": "1737370"
  },
  {
    "text": "all of the layers. But we're just looking\nat the first one here. So we can also do\nthis for actually",
    "start": "1737370",
    "end": "1748090"
  },
  {
    "text": "being able to get some notion\nof how these different-- how this actually looks and\nplot out these axes as well.",
    "start": "1748090",
    "end": "1756320"
  },
  {
    "text": "So again, if we take this same\nkind of model input, which again, is this Hugging\nFace Transformers is great,",
    "start": "1756320",
    "end": "1762670"
  },
  {
    "text": "we're actually\ntrying to see what do these representations\nlook on a per layer basis.",
    "start": "1762670",
    "end": "1768860"
  },
  {
    "text": "So what we can do here\nis, basically, we're looking at-- for each layer\nthat we have in our model,",
    "start": "1768860",
    "end": "1775660"
  },
  {
    "text": "and again, this is purely\nfrom the model output attentions, or the actual\noutputs of the model.",
    "start": "1775660",
    "end": "1782210"
  },
  {
    "text": "So what we can do is, for each\nlayer, and then, for each head, we can analyze, essentially,\nwhat these representations look",
    "start": "1782210",
    "end": "1790270"
  },
  {
    "text": "like, and in particular,\nwhat the attention weights are across each of the\ntokens that we have.",
    "start": "1790270",
    "end": "1795470"
  },
  {
    "text": "So this is a good way\nof, again, understanding what your model is actually\nattending to within each layer.",
    "start": "1795470",
    "end": "1801880"
  },
  {
    "text": "So on the side, if we look\nhere, maybe zoom in a bit, we can see that this\nis going to be--",
    "start": "1801880",
    "end": "1808190"
  },
  {
    "text": "corresponds to the\ndifferent layers. And the top will\ncorrespond to-- these are across the different\nattention heads, OK?",
    "start": "1808190",
    "end": "1816692"
  },
  {
    "text": "This will just give\nyou some notion of what the weights are here. So again, just to clarify.",
    "start": "1816692",
    "end": "1823240"
  },
  {
    "text": "So again, if we maybe\nlook at the labels-- so it's a little cut\noff and like zoomed out.",
    "start": "1823240",
    "end": "1828760"
  },
  {
    "text": "So this y-axis here,\nthese different rows, correspond to the different\nlayers within the model.",
    "start": "1828760",
    "end": "1836050"
  },
  {
    "text": "Oops. On the x-axis here, we have\nthe different attention heads",
    "start": "1836050",
    "end": "1845200"
  },
  {
    "text": "that are present in\nthe model as well. And so for each\nhead, we're able to for-- at each layer\nto basically get",
    "start": "1845200",
    "end": "1853300"
  },
  {
    "text": "a sense of how the attention\ndistribution is actually being distributed, what's being\nattended to, corresponding",
    "start": "1853300",
    "end": "1861370"
  },
  {
    "text": "to each of the tokens that\nyou actually get here. So if we look up, again,\nhere as well, right?",
    "start": "1861370",
    "end": "1870120"
  },
  {
    "text": "We're just trying to\nlook at, basically, the model attentions that we get\nfor each corresponding layer.",
    "start": "1870120",
    "end": "1877690"
  },
  {
    "text": "The question is,\nwhat's the color key? Yellow is higher magnitude\nand higher value.",
    "start": "1877690",
    "end": "1884679"
  },
  {
    "text": "And then, darker is\nlike closer to 0. So probably very Navy is zero.",
    "start": "1884680",
    "end": "1889840"
  },
  {
    "text": " So what we can do is now maybe\nwalk through what a fine tuning",
    "start": "1889840",
    "end": "1896880"
  },
  {
    "text": "task looks like here. And so, first, in\na project, you're",
    "start": "1896880",
    "end": "1902130"
  },
  {
    "text": "probably going to want\nto fine tune a model. That's fine. And we'll go ahead and\nwalk through an example",
    "start": "1902130",
    "end": "1908100"
  },
  {
    "text": "of what that looks like here. ",
    "start": "1908100",
    "end": "1913150"
  },
  {
    "text": "OK, so what we can do as well is\nuse some of the data sets that",
    "start": "1913150",
    "end": "1925010"
  },
  {
    "text": "we can get from\nHugging Face as well-- so it doesn't just have models,\nit has really nice data sets--",
    "start": "1925010",
    "end": "1931309"
  },
  {
    "text": "and be able to load\nthat in as well. So here what we're\ngoing to be looking at is looking at the IMDb data set.",
    "start": "1931310",
    "end": "1941100"
  },
  {
    "text": "And so, here, again, is\nfor sentiment analysis. We'll just look at only\nthe first 50 tokens or so.",
    "start": "1941100",
    "end": "1949190"
  },
  {
    "text": "And generally, so this\nis a helper function",
    "start": "1949190",
    "end": "1954980"
  },
  {
    "text": "that we'll use for truncating\nthe output that we get. And then, lastly, for\nactually making this data set,",
    "start": "1954980",
    "end": "1962840"
  },
  {
    "text": "we can use the DatasetDict\nclass for Hugging Face again.",
    "start": "1962840",
    "end": "1967970"
  },
  {
    "text": "That will basically give\nus this smaller data set that we can get\nfor the train data set",
    "start": "1967970",
    "end": "1975500"
  },
  {
    "text": "as well as specifying what we\nwant for validation as well. So here, what we're going\nto do for our mini data set",
    "start": "1975500",
    "end": "1981920"
  },
  {
    "text": "for the purpose of\nthis demonstration is we'll use make train and\nval both from the IMDb trained",
    "start": "1981920",
    "end": "1989450"
  },
  {
    "text": "data set. We'll shuffle it a bit. And then, we're just going\nto select here 128 examples",
    "start": "1989450",
    "end": "1996770"
  },
  {
    "text": "and then 32 for validation. So it'll shuffle it around. It'll take the first one 28\nand it'll take the next 32.",
    "start": "1996770",
    "end": "2006500"
  },
  {
    "text": "And then, we'll kind of\ntruncate those particular inputs that we get. Again, just to make\nsure we're efficient",
    "start": "2006500",
    "end": "2013900"
  },
  {
    "text": "and we can actually\nrun this on a CPU.  OK, so next what\nwe can do is just",
    "start": "2013900",
    "end": "2022280"
  },
  {
    "text": "see, what does this look like? It'll just, again, this is\nkind of just like a dictionary. It's a wrapper class almost of\ngiving your trained data set",
    "start": "2022280",
    "end": "2030440"
  },
  {
    "text": "and then your\nvalidation data set. And in particular,\nwe can even look at what the first 10\nof these looks like.",
    "start": "2030440",
    "end": "2038820"
  },
  {
    "text": "So first, the output. So we specify train. We want to look at\nthe first 10 entries",
    "start": "2038820",
    "end": "2044600"
  },
  {
    "text": "in our trained data set. And the output of this is\ngoing to be a dictionary",
    "start": "2044600",
    "end": "2051199"
  },
  {
    "text": "as well, which is pretty cool. So we have some--\nthe first 10 text",
    "start": "2051199",
    "end": "2056600"
  },
  {
    "text": "examples that give the\nactual movie reviews here. So this is given in a list.",
    "start": "2056600",
    "end": "2064370"
  },
  {
    "text": "And then, the second\nkey that you get are the labels corresponding\nto each of these,",
    "start": "2064370",
    "end": "2069449"
  },
  {
    "text": "so whether it's\npositive or negative. So here, 1 is going to be a\npositive review, 0 is negative.",
    "start": "2069449",
    "end": "2076310"
  },
  {
    "text": "So it makes it really easy\nto use this for something like sentiment analysis.",
    "start": "2076310",
    "end": "2083580"
  },
  {
    "text": "OK, so what we can do is go\nahead and prepare the data set",
    "start": "2083580",
    "end": "2089550"
  },
  {
    "text": "and put it into\nbatches of 16, OK? So what does this look like? What we can do is we can\ncall the map function",
    "start": "2089550",
    "end": "2097260"
  },
  {
    "text": "that this small data\nset dictionary has. So you can call map.",
    "start": "2097260",
    "end": "2103890"
  },
  {
    "text": "And pass in a lambda function\nof what we want to actually do. So here, the lambda\nfunction is for each example",
    "start": "2103890",
    "end": "2110940"
  },
  {
    "text": "that we want to tokenize\nthe text, basically. So this is basically saying how\ndo we want to preprocess this.",
    "start": "2110940",
    "end": "2121540"
  },
  {
    "text": "And so, here, we're extracting\nthe tokens, the input IDs that we'll pass\ninto the model, we are adding padding\nand truncation as well.",
    "start": "2121540",
    "end": "2129250"
  },
  {
    "text": "We're going to do\nthis in a batch, and then, the batch\nsize will be 16. OK, hopefully this makes sense.",
    "start": "2129250",
    "end": "2136630"
  },
  {
    "text": "OK, so next, we're\nbasically just going to do a little more modification\non what the data set actually",
    "start": "2136630",
    "end": "2147040"
  },
  {
    "text": "looks like. So we're going to\nremove the column that corresponds to text.",
    "start": "2147040",
    "end": "2152170"
  },
  {
    "text": "And then, we're going to rename\nthe column label to labels. So again, if we see this,\nthis was called label.",
    "start": "2152170",
    "end": "2158950"
  },
  {
    "text": "We're just going\nto call it labels. And we're going to remove\nthe text column because we don't really need it anymore.",
    "start": "2158950",
    "end": "2164450"
  },
  {
    "text": "We just have gone ahead\nand pre-processed our data into the input IDs that we need.",
    "start": "2164450",
    "end": "2169480"
  },
  {
    "text": "OK, and lastly, we're going to\nset it-- the format to Torch so we can go ahead\nand just pass this in to our model or\nour PyTorch model.",
    "start": "2169480",
    "end": "2178940"
  },
  {
    "text": "The question is, what is labels? So label here\ncorresponds to, again,",
    "start": "2178940",
    "end": "2185720"
  },
  {
    "text": "in the context of sentiment\nanalysis, it's just-- yeah, positive or negative. And so here we're just\nrenaming the column.",
    "start": "2185720",
    "end": "2193710"
  },
  {
    "text": "OK, so now we'll just go ahead\nand see what this looks like. Again, we're going to look\nat the train set and only",
    "start": "2193710",
    "end": "2199170"
  },
  {
    "text": "these first two things. And so here now, we\nhave the two labels",
    "start": "2199170",
    "end": "2205620"
  },
  {
    "text": "that correspond to each of\nthe reviews and the input IDs that we get corresponding for\neach of the reviews as well.",
    "start": "2205620",
    "end": "2213420"
  },
  {
    "text": "Lastly, we also get\nthe attention mask. So it's basically\njust taking what you get out from the\ntokenizer and it's just adding",
    "start": "2213420",
    "end": "2220400"
  },
  {
    "text": "this back into the data set. So it's really easy to pass in. The question is, we truncated,\nwhich makes things easy.",
    "start": "2220400",
    "end": "2228140"
  },
  {
    "text": "But how do you want to\napply padding evenly.",
    "start": "2228140",
    "end": "2233470"
  },
  {
    "text": "So here, if we do pass in-- so first it's like you\ncould either manually",
    "start": "2233470",
    "end": "2238960"
  },
  {
    "text": "set some high truncation\nlimit, like we did. The second is that you can\njust go ahead and set padding",
    "start": "2238960",
    "end": "2247060"
  },
  {
    "text": "to be true. And then, basically,\nthe padding is basically",
    "start": "2247060",
    "end": "2253030"
  },
  {
    "text": "added based off of the longest\nsequence that you have.",
    "start": "2253030",
    "end": "2259840"
  },
  {
    "text": "Yeah, so question is, I guess,\ndoing it for all of them-- all the texts lists evenly.",
    "start": "2259840",
    "end": "2266080"
  },
  {
    "text": "So again, it just depends\non the size of the data set you're loading in, right?",
    "start": "2266080",
    "end": "2271570"
  },
  {
    "text": "So if you're looking at\nparticular batches at a time, you can just pad within that\nparticular batch versus--",
    "start": "2271570",
    "end": "2278265"
  },
  {
    "text": "yeah, you don't\nneed to like load all of the data set\ninto memory, pad the entire data set\nin the same way.",
    "start": "2278265",
    "end": "2285200"
  },
  {
    "text": "So it's fine to do it\nwithin just batches. Yeah, the question was, how\nare the input IDs added?",
    "start": "2285200",
    "end": "2292810"
  },
  {
    "text": "And yeah, the\nanswer is, yes, it's basically done automatically. So we had to manually\nremove the text column here",
    "start": "2292810",
    "end": "2301599"
  },
  {
    "text": "and that kind of,\nthis first line here. But if you recall, the\noutputs of tokenizer",
    "start": "2301600",
    "end": "2309580"
  },
  {
    "text": "is basically just the input\nIDs and the attention mask. So it just is smart enough\nto basically aggregate those",
    "start": "2309580",
    "end": "2315964"
  },
  {
    "text": "together.  OK, the last thing\nwe're going to do",
    "start": "2315965",
    "end": "2322370"
  },
  {
    "text": "is, basically, just split these. So we have this data set now.",
    "start": "2322370",
    "end": "2327650"
  },
  {
    "text": "It looks great. We're just going to import a\nPyTorch Data Loader, typical,",
    "start": "2327650",
    "end": "2333230"
  },
  {
    "text": "normal Data Loader. And then, go ahead and\nload each of these data sets that we just had.",
    "start": "2333230",
    "end": "2338240"
  },
  {
    "text": "I'm specifying the\nbatch size to be 16. OK, so that's fine and great.",
    "start": "2338240",
    "end": "2347360"
  },
  {
    "text": "And so, now, for\ntraining the model, it's basically exactly\nthe same as what",
    "start": "2347360",
    "end": "2352910"
  },
  {
    "text": "we would do in typical PyTorch. So again, it's like you still\nwant to compute the loss.",
    "start": "2352910",
    "end": "2359540"
  },
  {
    "text": "You can back propagate\nthe loss and everything. Yeah, so it's really\nup to your own design",
    "start": "2359540",
    "end": "2366190"
  },
  {
    "text": "how you do the training. So here, there's only a\nfew asterisks, I guess.",
    "start": "2366190",
    "end": "2374380"
  },
  {
    "text": "One is that you can import\nspecific optimizer types from the Transformers package.",
    "start": "2374380",
    "end": "2382090"
  },
  {
    "text": "So you can do add in\nwith weight decay. You can get a linear schedule\nfor like the learning rate,",
    "start": "2382090",
    "end": "2388190"
  },
  {
    "text": "which will kind of decrease\nthe learning rate over time for each training set.",
    "start": "2388190",
    "end": "2393970"
  },
  {
    "text": "So again, it's basically\nup to your choice. But if you look at the\nstructure of this code,",
    "start": "2393970",
    "end": "2399190"
  },
  {
    "text": "we load the model\nfor classification. We set a number of epochs. And then, however many training\nsteps we actually want to do.",
    "start": "2399190",
    "end": "2406480"
  },
  {
    "text": "We initialize our optimizer\nand get some learning rate schedule.",
    "start": "2406480",
    "end": "2411610"
  },
  {
    "text": "And then, from\nthere, it's basically the same thing as what we would\ndo for a typical PyTorch model,",
    "start": "2411610",
    "end": "2418030"
  },
  {
    "text": "right? We set the model to train mode. We go ahead and pass in all\nof these batches from the Data",
    "start": "2418030",
    "end": "2426109"
  },
  {
    "text": "Loader. And then, backpropagate, step\nthe optimizer, and everything like that.",
    "start": "2426110",
    "end": "2431740"
  },
  {
    "text": "So it's pretty similar from what\nwe're kind of used to seeing,",
    "start": "2431740",
    "end": "2436860"
  },
  {
    "text": "essentially. ",
    "start": "2436860",
    "end": "2443130"
  },
  {
    "text": "Awesome. So that'll go do its\nthing at some point. OK, and so, that's\none potential option",
    "start": "2443130",
    "end": "2451340"
  },
  {
    "text": "is if you really like\nPyTorch, you can just go ahead and do that and\nit's really nice and easy.",
    "start": "2451340",
    "end": "2456890"
  },
  {
    "text": "The second thing is that\nHugging Face actually has some sort of a\ntrainer class that you're",
    "start": "2456890",
    "end": "2464750"
  },
  {
    "text": "able to use that can handle\nmost of these things. So again, if we do\nthe same thing here,",
    "start": "2464750",
    "end": "2472400"
  },
  {
    "text": "this will actually run once\nour model is done training. We can create our data set\nin the same way as before.",
    "start": "2472400",
    "end": "2483050"
  },
  {
    "text": "Now, what we need to use is this\nimport of a training arguments",
    "start": "2483050",
    "end": "2488270"
  },
  {
    "text": "class. So this is going\nto be, basically, a dictionary of all\nthe things that we want to use when we're going\nto actually train our model.",
    "start": "2488270",
    "end": "2495950"
  },
  {
    "text": "And then, this kind of like\nadditional trainer class, which will handle the\ntraining magically for us",
    "start": "2495950",
    "end": "2503370"
  },
  {
    "text": "and wrap around in that way. ",
    "start": "2503370",
    "end": "2509276"
  },
  {
    "text": "OK, I think we're\nmissing a directory. But I think it's pretty\nstraightforward for how",
    "start": "2509277",
    "end": "2514460"
  },
  {
    "text": "you want to train here. So for here at\nleast, again, there",
    "start": "2514460",
    "end": "2520700"
  },
  {
    "text": "are the two key arguments. The first is training arguments. So this will specify how\na number of specifications",
    "start": "2520700",
    "end": "2527150"
  },
  {
    "text": "that you can actually\npass through to it. It's where you want to log\nthings for each device.",
    "start": "2527150",
    "end": "2533780"
  },
  {
    "text": "In this case, we're\njust using one GPU. But potentially, if you're\nusing multiple GPUs,",
    "start": "2533780",
    "end": "2539119"
  },
  {
    "text": "what the batch size\nis during training or the batch size is\nduring evaluation time,",
    "start": "2539120",
    "end": "2544820"
  },
  {
    "text": "how long you want\nto train it for, how you want to evaluate it. So this is kind of evaluating\non an epoch level what",
    "start": "2544820",
    "end": "2554090"
  },
  {
    "text": "the learning rate is\nand so on and so on. So again, if you want to\ncheck the documentation,",
    "start": "2554090",
    "end": "2559890"
  },
  {
    "text": "you can see that here. There's a bunch of different\narguments that you can give. There's warm up steps, warm\nup ratio, like weight decay,",
    "start": "2559890",
    "end": "2567890"
  },
  {
    "text": "there's so many things. So again, it's basically\nlike a dictionary.",
    "start": "2567890",
    "end": "2573500"
  },
  {
    "text": "Feel free to look at\nthese different arguments you can pass in. But there's a couple\nkey ones here.",
    "start": "2573500",
    "end": "2579119"
  },
  {
    "text": "And this is, basically--\nthis basically mimics the same arguments\nthat we used before in our explicit PyTorch\nmethod here for Hugging Face.",
    "start": "2579120",
    "end": "2587825"
  },
  {
    "text": " OK, similarly, what\nwe do is we can just",
    "start": "2587825",
    "end": "2593910"
  },
  {
    "text": "pass this into the trainer. And that will take care of,\nbasically, everything for us. So that whole training\nloop that we did before",
    "start": "2593910",
    "end": "2601170"
  },
  {
    "text": "is condensed into this one class\nfunction for, actually, just doing the training.",
    "start": "2601170",
    "end": "2606450"
  },
  {
    "text": "So we pass in the model, the\narguments, the train data set, eval data set, what\ntokenizer we want to use,",
    "start": "2606450",
    "end": "2613260"
  },
  {
    "text": "and then, some function\nfor computing metrics. So for here, we pass\nin this function, eval,",
    "start": "2613260",
    "end": "2621240"
  },
  {
    "text": "and it takes eval\npredictions as input. Basically, what this\ndoes is these predictions",
    "start": "2621240",
    "end": "2626280"
  },
  {
    "text": "are given from the trainer--\npassed into this function. And we just can split it\ninto the actual logits",
    "start": "2626280",
    "end": "2632340"
  },
  {
    "text": "and the labels\nthat are predicted. Or sorry, the ground\ntruth labels that we have. And then, from here,\nwe can just calculate",
    "start": "2632340",
    "end": "2638610"
  },
  {
    "text": "any sort of\nadditional metrics we want like accuracy, F1 score,\nrecall or whatever you want.",
    "start": "2638610",
    "end": "2647340"
  },
  {
    "text": "OK, so this is an\nalternative way of formulating\nthat training loop.",
    "start": "2647340",
    "end": "2653830"
  },
  {
    "text": "OK, the last thing\nhere as well is that we can have\nsome sort of callback",
    "start": "2653830",
    "end": "2659650"
  },
  {
    "text": "as well if you want to do things\nduring the training process. So after every epoch\nor something like that,",
    "start": "2659650",
    "end": "2665890"
  },
  {
    "text": "you want to evaluate your\nmodel on the validation set or something like that. Or just go ahead and\ndump some sort of output.",
    "start": "2665890",
    "end": "2674780"
  },
  {
    "text": "That's what you can\nuse a callback for. And so, here, this is\njust a logging callback.",
    "start": "2674780",
    "end": "2680859"
  },
  {
    "text": "It's just going to log\nkind of the information about the process itself.",
    "start": "2680860",
    "end": "2688150"
  },
  {
    "text": "Again, not super important. But in case that you're\nlooking to try and do",
    "start": "2688150",
    "end": "2693370"
  },
  {
    "text": "any sort of callback\nduring training, it's an easy way to add it in. The second is if you want to\ndo early stopping as well.",
    "start": "2693370",
    "end": "2700820"
  },
  {
    "text": "So early stopping will,\nbasically, stop your model early, as it sounds if\nit's not learning anything",
    "start": "2700820",
    "end": "2708820"
  },
  {
    "text": "and a bunch of\nepochs are going by. And so you can set that so that\nyou don't waste compute time",
    "start": "2708820",
    "end": "2714500"
  },
  {
    "text": "or you can see the\nresults more easily. The question is, is there a good\nchoice for the patient's value?",
    "start": "2714500",
    "end": "2720930"
  },
  {
    "text": "I think it just depends\non the model architecture. Not really, I guess, it's--",
    "start": "2720930",
    "end": "2726510"
  },
  {
    "text": "yeah, it's pretty up\nto your discretion. ",
    "start": "2726510",
    "end": "2731720"
  },
  {
    "text": "OK, awesome. And so the last thing that we\ndo is just do calltrainer.train.",
    "start": "2731720",
    "end": "2737360"
  },
  {
    "text": "So if you recall, this\nis just the instantiation of this trainer class\ncalltrainer.train.",
    "start": "2737360",
    "end": "2743510"
  },
  {
    "text": "And it'll just kind of go. So now it's training. It's great.",
    "start": "2743510",
    "end": "2749260"
  },
  {
    "text": "It gives us a nice kind\nof estimate of how long things are taking,\nwhat's going on, what arguments do\nwe actually pass in.",
    "start": "2749260",
    "end": "2758079"
  },
  {
    "text": "So that's just going to run. And then, likewise, hopefully\nit'll train relatively quickly.",
    "start": "2758080",
    "end": "2765690"
  },
  {
    "text": "OK, it'll take two minutes. We can also evaluate the\nmodel pretty easily as well.",
    "start": "2765690",
    "end": "2772090"
  },
  {
    "text": "So we just call it\ntrainer.predict on whatever data set that we're\ninterested in. So here, it's the tokenized\ndata set corresponding",
    "start": "2772090",
    "end": "2779109"
  },
  {
    "text": "to the validation data set. OK, hopefully we can\npop that out soon.",
    "start": "2779110",
    "end": "2786440"
  },
  {
    "text": "And lastly, so if\nwe saved anything to our model checkpoints,\nso hopefully,",
    "start": "2786440",
    "end": "2792500"
  },
  {
    "text": "this is saving stuff right now. ",
    "start": "2792500",
    "end": "2799710"
  },
  {
    "text": "Yeah, so this is\ngoing to be-- is continuing to save stuff to\nthe folder that we specified.",
    "start": "2799710",
    "end": "2805690"
  },
  {
    "text": "And so here, in\ncase we ever want to load our model\nagain from the weights",
    "start": "2805690",
    "end": "2811350"
  },
  {
    "text": "that we've actually\nsaved, we just pass in the name of the\ncheckpoint, the relative path",
    "start": "2811350",
    "end": "2816420"
  },
  {
    "text": "here to our checkpoint. So that is how we have some\ncheckpoint 8 here, right?",
    "start": "2816420",
    "end": "2822340"
  },
  {
    "text": "We just pass in the\npath to that folder. We load it back in. We tokenize and it's the\nsame thing as we did before.",
    "start": "2822340",
    "end": "2831500"
  },
  {
    "text": "There are a few\nadditional appendices for how to do different\ntasks as well.",
    "start": "2831500",
    "end": "2838620"
  },
  {
    "text": "So there's an appendix\non generation. How to define a custom\ndata set as well.",
    "start": "2838620",
    "end": "2844470"
  },
  {
    "text": "How to pipeline different\nkind of tasks together.",
    "start": "2844470",
    "end": "2849570"
  },
  {
    "text": " So this is kind of\nlike using some--",
    "start": "2849570",
    "end": "2855869"
  },
  {
    "text": "a pre-trained model that you can\njust use through the pipeline interface really\neasily there is--",
    "start": "2855870",
    "end": "2864020"
  },
  {
    "text": "in different types of tasks,\nlike math language modeling. But feel free to look through\nthose at your own time.",
    "start": "2864020",
    "end": "2869720"
  },
  {
    "text": "And yeah, thanks a bunch. ",
    "start": "2869720",
    "end": "2877000"
  }
]