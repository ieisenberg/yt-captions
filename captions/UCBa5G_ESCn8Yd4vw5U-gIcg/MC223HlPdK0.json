[
  {
    "start": "0",
    "end": "10790"
  },
  {
    "text": "Welcome to E380, this is\nfor June the 7th, 2023.",
    "start": "10790",
    "end": "19040"
  },
  {
    "text": "In any case, the other\nday as I was planning what we were going to do\ntoday, it occurred to me",
    "start": "19040",
    "end": "26330"
  },
  {
    "text": "that we'd been\nspending a lot of time on the current software issues\nof the day, the generative AI,",
    "start": "26330",
    "end": "34100"
  },
  {
    "text": "and machine learning. And we really had\nbeen neglecting the thing that makes it really\nwork, that is, the hard work.",
    "start": "34100",
    "end": "44240"
  },
  {
    "text": "And not only that, just\nabout the same time that I",
    "start": "44240",
    "end": "50390"
  },
  {
    "text": "was beginning to do this,\nNVIDIA, trust a magical marker,",
    "start": "50390",
    "end": "57390"
  },
  {
    "text": "it became the first\ntrillion-dollar company in the computer industry. And that was quite exciting.",
    "start": "57390",
    "end": "66300"
  },
  {
    "text": "And it suggested\nthat the best thing to do was to get\nour class together,",
    "start": "66300",
    "end": "80740"
  },
  {
    "text": "and that's what has happened. In any case, our speakers\nfrom NVIDIA, and he",
    "start": "80740",
    "end": "86980"
  },
  {
    "text": "can introduce himself. Jack, you're on. So, hello everybody.",
    "start": "86980",
    "end": "92860"
  },
  {
    "text": "I guess I've already\nbeen introduced. Jack Choquette, and I'm\ngoing to be talking--",
    "start": "92860",
    "end": "98020"
  },
  {
    "text": "I've been asked to talk to\nyou about NVIDIA's Hopper GPU and the design did.",
    "start": "98020",
    "end": "103830"
  },
  {
    "text": "And how we went\nabout designing it. Let's see.",
    "start": "103830",
    "end": "109159"
  },
  {
    "text": "So today I'm going to cover just\nan overview of the Hopper 100 GPU and talk a little bit about\nthe hierarchy and asynchrony,",
    "start": "109160",
    "end": "116260"
  },
  {
    "text": "or key to its performance. And now Hopper took\nadvantage of these components",
    "start": "116260",
    "end": "121780"
  },
  {
    "text": "to accelerate them, to describe\nhow the Hopper accelerates the deep learning,\nand then I'm going",
    "start": "121780",
    "end": "128110"
  },
  {
    "text": "to finish up with a\nbrief description of how the H100 GPU is used in systems\nto scale its performance.",
    "start": "128110",
    "end": "137000"
  },
  {
    "text": "Feel free to interrupt and ask\nquestions as we're going along, and I'll see if I can\nanswer them as best I can.",
    "start": "137000",
    "end": "145400"
  },
  {
    "text": "So let's start\nwith the overview. Hopper is built on TSMC\ncustom four-nanometer process,",
    "start": "145400",
    "end": "153110"
  },
  {
    "text": "contains over 80 billion\ntransistors, which makes it the world's most\nadvanced monolithic chip.",
    "start": "153110",
    "end": "159350"
  },
  {
    "text": "Contains 132 SMs each\ndelivering twice the performance of our previous\ngeneration A100 SMS,",
    "start": "159350",
    "end": "166370"
  },
  {
    "text": "and includes multiple\nnew features. Includes a new memory system\nfeaturing HBM3, larger L2 cache,",
    "start": "166370",
    "end": "175550"
  },
  {
    "text": "to deliver much higher\nmemory bandwidth than a previous\ngeneration, and also supports multi-GPU superPOD\nand cloud designs, a variety",
    "start": "175550",
    "end": "183860"
  },
  {
    "text": "and added a variety of new\nsystem architecture features, including the fourth generation\nNVLink and new NVLink",
    "start": "183860",
    "end": "189320"
  },
  {
    "text": "network, second-generation\nmulti-instance GPU technology, as well as new accelerated\nconfidential computing",
    "start": "189320",
    "end": "195650"
  },
  {
    "text": "support for secured\naccelerated computing. ",
    "start": "195650",
    "end": "201940"
  },
  {
    "text": "So diving into the SM\nprocessor core itself, Hopper's made many\nimprovements to the core.",
    "start": "201940",
    "end": "208599"
  },
  {
    "text": "It features a twice a 2x\nclock for clock improvement on traditional FP32\nand FP64 throughput",
    "start": "208600",
    "end": "215020"
  },
  {
    "text": "it supports a 256KB of unified\nL1 and shared memory storage, which is 33% more than our\nprevious generation A100.",
    "start": "215020",
    "end": "223150"
  },
  {
    "text": "Contains the new\nfourth-generation tensor core that is 2x faster and\nsignificantly more efficient.",
    "start": "223150",
    "end": "231100"
  },
  {
    "text": "Hopper also introduced a\nnew dynamic instruction set for dynamic\nprogramming called DPX,",
    "start": "231100",
    "end": "236620"
  },
  {
    "text": "devise advanced operand\nfusion for inner loop and many dynamic\nprogramming algorithms. It adds tensor\nmemory accelerator",
    "start": "236620",
    "end": "243160"
  },
  {
    "text": "for efficient asynchronous\ndata movements in multidimensional tensor data. And finally, we\nadded a new level",
    "start": "243160",
    "end": "250120"
  },
  {
    "text": "of hierarchy between\nCUDA and thread through the CUDA\nhierarchy of thread blocks and grids called the\nthread block clusters.",
    "start": "250120",
    "end": "257350"
  },
  {
    "text": "And thread block clusters\nwill enable us-- applications to take advantage of\nlocality, additional locality",
    "start": "257350",
    "end": "263139"
  },
  {
    "text": "that the GPU provides,\nto dramatically improve the efficiency of\nmany algorithms. ",
    "start": "263140",
    "end": "271910"
  },
  {
    "text": "Hopper has five three sites with\na total memory capacity of 80GB.",
    "start": "271910",
    "end": "278180"
  },
  {
    "text": "The memory system\nhas been designed to maximize not only\nthe peak bandwidth but delivered bandwidth.",
    "start": "278180",
    "end": "283220"
  },
  {
    "text": "To achieve this, it features\nimprovements to both the memory as well as GPUs\nmemory controllers,",
    "start": "283220",
    "end": "288500"
  },
  {
    "text": "which are optimized to run at\ndramatically increased DRAM frequencies and redesigned to\ninclude twice as many memory",
    "start": "288500",
    "end": "295100"
  },
  {
    "text": "channels per HBM\nsite to maintain the same high efficiency despite\nthe increase in frequency.",
    "start": "295100",
    "end": "301070"
  },
  {
    "text": "As a result, we can\nachieve three terabytes per second of memory\nbandwidth, 2x the throughput",
    "start": "301070",
    "end": "307430"
  },
  {
    "text": "of our previous generation A100.",
    "start": "307430",
    "end": "313740"
  },
  {
    "text": "And hopper made\nseveral improvements to our MIG technology. It provides three times\nmore compute capacity",
    "start": "313740",
    "end": "321660"
  },
  {
    "text": "per MIG than A100 and\nroughly twice more memory bandwidth per MIG.",
    "start": "321660",
    "end": "326970"
  },
  {
    "text": "We added dedicated image\nand video decoder engines for GPU instance. These are very important\nfor inferencing",
    "start": "326970",
    "end": "334230"
  },
  {
    "text": "customers do a lot of\nimage and video processing. It's not just about\nthe AI but a lot about the image processing,\nvideo processing",
    "start": "334230",
    "end": "340590"
  },
  {
    "text": "you need to do in\norder to do the AI. We combined our MIG technology\nwith confidential computing",
    "start": "340590",
    "end": "347490"
  },
  {
    "text": "technology for secure execution. Here, the trusted environment\nencapsulates one confidential VM",
    "start": "347490",
    "end": "353370"
  },
  {
    "text": "running on the CPU with a MIG\ninstance running on the GPU. We took advantage of the\nphysical isolation of the units",
    "start": "353370",
    "end": "360240"
  },
  {
    "text": "that the MIG already provide. The hardware MIG design, plus\nwe added a hardware firewall",
    "start": "360240",
    "end": "365490"
  },
  {
    "text": "to prevent any unauthorized\naccess to memory from outside the trusted\nexecution environment. Including from other MIG\ninstances on the same GPU, PCIe",
    "start": "365490",
    "end": "374669"
  },
  {
    "text": "interface, we have SR-IOV,\nwhich stands for Single Root Io Virtualization. It virtualizes all the registers\nneeded to control and program",
    "start": "374670",
    "end": "382409"
  },
  {
    "text": "each GPU instance. All transfers on\nPCIe are encrypted. Each GPU has its\nown set of keys,",
    "start": "382410",
    "end": "389340"
  },
  {
    "text": "so there's no data sharing\nbetween VMs or between GPU instances. And by combining the MIG\nand confidential computing,",
    "start": "389340",
    "end": "396150"
  },
  {
    "text": "H100 has the world's first\nmulti-tenant native confidential computing platform\nusing a single GPU.",
    "start": "396150",
    "end": "403275"
  },
  {
    "text": " And it's just kind of a\ndiagram, a table of where",
    "start": "403275",
    "end": "411580"
  },
  {
    "text": "you can see the performance\nfor key mainstream HPC models, including many vision and neural\nnetworks or small language",
    "start": "411580",
    "end": "418510"
  },
  {
    "text": "models. And you can see that we\ncan deliver anywhere from 2 to 3x performance or\nour previous generation",
    "start": "418510",
    "end": "423879"
  },
  {
    "text": "where we both use InfiniBand\nto connect to the GPUs.",
    "start": "423880",
    "end": "429130"
  },
  {
    "text": "If you start looking\nat adding NVLink to it, not using InfiniBand but using\nthe NVLink network interconnect.",
    "start": "429130",
    "end": "437460"
  },
  {
    "text": "It actually enables\nmore efficient scaling across multiple GPUs,\naccelerating the nodes and providing a major\nboost in performance",
    "start": "437460",
    "end": "443590"
  },
  {
    "text": "across HPC and AI applications. With this, with using NVLink\ninstead of InfiniBand,",
    "start": "443590",
    "end": "451150"
  },
  {
    "text": "provide an additional 2 to\n3x performance improvement for HPC and AI training. ",
    "start": "451150",
    "end": "460800"
  },
  {
    "text": "Now I'm going to dive\ninto the details of some of the new features for Hopper. But first, I'm\ngoing to talk over",
    "start": "460800",
    "end": "467360"
  },
  {
    "text": "some key principles\nfor performance and describe the improvements\nand features that Hopper has made to take advantage\nof those principles",
    "start": "467360",
    "end": "474170"
  },
  {
    "text": "to improve efficiency\nand performance. So here are two key principles\nfor achieving high performance",
    "start": "474170",
    "end": "480990"
  },
  {
    "text": "for parallel programs. One key principle\nis data locality. By moving and keeping\nthe data program data",
    "start": "480990",
    "end": "487740"
  },
  {
    "text": "as close as possible\nto execution units, a programmer can\nexploit the performance that comes from having lower\nlatency and higher bandwidth",
    "start": "487740",
    "end": "494340"
  },
  {
    "text": "access to local data. Another key principle is\nasynchronous execution. This means finding and allowing\nindependent tasks and data",
    "start": "494340",
    "end": "501990"
  },
  {
    "text": "movement to overlap\nas much as possible. The goal, keeping all the units\nfully utilized and avoiding",
    "start": "501990",
    "end": "507270"
  },
  {
    "text": "serialization. So let's look at\ncloser data locality",
    "start": "507270",
    "end": "512860"
  },
  {
    "text": "and cooperative execution. So locality can occur either\nspatially or temporally.",
    "start": "512860",
    "end": "520039"
  },
  {
    "text": "With spatial locality,\ndata and parallel execution of a spatial\nrelationship which can be thought of compute and\ndata being co-located.",
    "start": "520039",
    "end": "527150"
  },
  {
    "text": "One example of a computation\nusing spatial data",
    "start": "527150",
    "end": "533690"
  },
  {
    "text": "locality is spatial halo\noverlap as shown in the figure.",
    "start": "533690",
    "end": "539330"
  },
  {
    "text": "A common computation\noperates not only on the data that's in\nits rectilinear tile",
    "start": "539330",
    "end": "545000"
  },
  {
    "text": "but also shares data with\nthe computations operating on an adjacent tiles. Another example is\nwhere a competition",
    "start": "545000",
    "end": "551500"
  },
  {
    "text": "has a spatial sharing,\nwhere one dimension has to be combined with data in\na different dimension, which",
    "start": "551500",
    "end": "556900"
  },
  {
    "text": "is not shown in the figure\nbut is also a common pattern. With temporal locality,\ndata and parallel execution",
    "start": "556900",
    "end": "563250"
  },
  {
    "text": "have a temporal\nrelationship, which can be more thought of as\ncompute passing over the data. The data stays at one\nlocation and compute",
    "start": "563250",
    "end": "569670"
  },
  {
    "text": "kind of passes over it. In the example\nshown in the figure, first one kernel processes\nthe data in memory",
    "start": "569670",
    "end": "576090"
  },
  {
    "text": "and then another person kernel\nprocesses data in memory. ",
    "start": "576090",
    "end": "581940"
  },
  {
    "text": "So let's review first\nthe spatial locality that current NVIDIA GPUs\nbefore H100 supported in CUDA.",
    "start": "581940",
    "end": "590670"
  },
  {
    "text": "Locality can be expressed\nas a grid of execution. And a grid is mapped onto\nthe GPU hardware hierarchy.",
    "start": "590670",
    "end": "597180"
  },
  {
    "text": "Computation on the GPU\ntakes advantage of stores local to that GPU. In case of Hopper,\nthat's 50MB of L2 cache",
    "start": "597180",
    "end": "604440"
  },
  {
    "text": "and 80GB of HBM3 memory. The next level of locality\nthat can be expressed",
    "start": "604440",
    "end": "609449"
  },
  {
    "text": "is a thread block, which is\nmapped onto the SM hardware hierarchy. Here, a computation\ncan take advantage",
    "start": "609450",
    "end": "615480"
  },
  {
    "text": "of 256KB of SM-localized\nstorage and the L1 data cache plus shared memory.",
    "start": "615480",
    "end": "621779"
  },
  {
    "text": "The smallest level of locality\nyou can express in CUDA is threads.",
    "start": "621780",
    "end": "627150"
  },
  {
    "text": "These are mapped to\nhardware thread lanes, and here the computation\ntakes advantage of thread-local register\nfile storage, which",
    "start": "627150",
    "end": "634710"
  },
  {
    "text": "is a 1 kilobyte per thread. And this translates\nto 64KB per SM",
    "start": "634710",
    "end": "640949"
  },
  {
    "text": "partition and 256KB per SM of\nthread register file storage.",
    "start": "640950",
    "end": "647115"
  },
  {
    "start": "647115",
    "end": "652529"
  },
  {
    "text": "The way that work is\nmapped onto the hardware is making the grid-level\nwork and breaking into fragments of work and\nassign them to a thread block.",
    "start": "652530",
    "end": "660045"
  },
  {
    "text": "The figure shown is broken up\nwith a two-dimensional grid but CUDA hardware and\nthe CUDA and the hardware supports three-dimensional\nmappings as well.",
    "start": "660045",
    "end": "667279"
  },
  {
    "text": "Programmers use whatever the\nmapping works best for them. They don't. Hardware doesn't dictate\nwhat the mapping needs to be.",
    "start": "667280",
    "end": "673670"
  },
  {
    "text": "Independent blocks each work on\ntheir own fragment the problem. Within the blocks, there\nare multiple threads",
    "start": "673670",
    "end": "679279"
  },
  {
    "text": "that cooperatively execute\non the fragment of work they assigned to that block. ",
    "start": "679280",
    "end": "687770"
  },
  {
    "text": "So this three levels\nof GPU hierarchy is this inscrutable first\nintroduced, and has enabled",
    "start": "687770",
    "end": "692810"
  },
  {
    "text": "parallel programs take\nadvantage of locality that exists in their algorithms and\ntake advantage of the hardware",
    "start": "692810",
    "end": "698330"
  },
  {
    "text": "locality that existed,\nthat mapped onto. The challenges that GPUs\nhave scaled since then, since",
    "start": "698330",
    "end": "705800"
  },
  {
    "text": "was introduced by\norder of magnitude. Now here, I'm showing\nthe NVIDIA Kepler GPU, which was introduced\n10 years ago and had 15 SMs.",
    "start": "705800",
    "end": "714889"
  },
  {
    "text": "Here's the NVIDIA\nHopper GPU today. It has 132 SMs with an order of\nmagnitude increase in 10 years.",
    "start": "714890",
    "end": "723200"
  },
  {
    "text": "If you look at it,\nthe entire Kepler GPU could fit in the corner\nof one Hopper GPU, Right,",
    "start": "723200",
    "end": "729410"
  },
  {
    "text": "just a small corner of what\nthe hopper needs to be. The Kepler GPU is roughly the\nsame size of Hopper GPC level",
    "start": "729410",
    "end": "735949"
  },
  {
    "text": "of hierarchy that we\nhave in the hardware. ",
    "start": "735950",
    "end": "741450"
  },
  {
    "text": "To take advantage of\nthis additional locality, H100 adds an additional\nlevel of hierarchy to CUDA.",
    "start": "741450",
    "end": "747360"
  },
  {
    "text": "The thread block cluster. Here, the computation take\nadvantage of the GPC hardware",
    "start": "747360",
    "end": "752460"
  },
  {
    "text": "hierarchy and the 256KB of\nlocalized storage in the SM.",
    "start": "752460",
    "end": "758340"
  },
  {
    "text": "SMs within a cluster\ncommunicate directly with each other through\nan SM-to-SM network. ",
    "start": "758340",
    "end": "765095"
  },
  {
    "text": "In CUDA, thread block\ncluster is just a big block full of thread blocks.",
    "start": "765095",
    "end": "771180"
  },
  {
    "text": "You can think of it as\nthread blocks on steroids. Thread blocks and\ncluster no longer need to be mapped to be\nexecuting independently",
    "start": "771180",
    "end": "778320"
  },
  {
    "text": "like threads in a block. Blocks in a cluster\ncan cooperatively execute on their\nfragment of the problem.",
    "start": "778320",
    "end": "784620"
  },
  {
    "text": "They can exchange data. They can work. They can synchronize themselves. They don't have to\noperate independently.",
    "start": "784620",
    "end": "789900"
  },
  {
    "text": "This enabled us to target\nlookalike subset of the grid, enabling more opportunities\nfor programmability",
    "start": "789900",
    "end": "795330"
  },
  {
    "text": "and for performance. ",
    "start": "795330",
    "end": "800560"
  },
  {
    "text": "And CUDA cluster to\ncollect up to 16 blocks. Each block is guaranteed\nto be on separate SM and is guaranteed to be\nrunning at the same time.",
    "start": "800560",
    "end": "808360"
  },
  {
    "text": "By running a guarantee\nrunning at the same time, then they can guarantee\nto be cooperating in exchanging data,\nand working together",
    "start": "808360",
    "end": "814750"
  },
  {
    "text": "on whatever the problem\nsets that they need to be. They do not need to be\nrunning independently.",
    "start": "814750",
    "end": "820130"
  },
  {
    "text": "You can have a lot more\nthreads and resources, all cooperative working\nessentially on the same problem.",
    "start": "820130",
    "end": "825855"
  },
  {
    "text": "Your CUDA code, you\nannotate the kernel with a cluster\nsize and dimension. And just like blocks,\nclusters dimensions",
    "start": "825855",
    "end": "831680"
  },
  {
    "text": "can be one, two, or\nthree-dimensional. Programmers use whatever\nbit fits their algorithm to what's being mapped.",
    "start": "831680",
    "end": "837260"
  },
  {
    "text": " Or the Hopper\nitself, architecture",
    "start": "837260",
    "end": "842920"
  },
  {
    "text": "provides clusters with a\ndirect communication network. Using this network,\none thread block",
    "start": "842920",
    "end": "848650"
  },
  {
    "text": "can directly access the shared\nmemory of another thread block. Accesses are done through\na distributed shared",
    "start": "848650",
    "end": "853750"
  },
  {
    "text": "memory models laid out as a\npartitioned global address space. All memory operations\nsupported include load, stores,",
    "start": "853750",
    "end": "860319"
  },
  {
    "text": "atomic synchronization\noperations. The communication network also\naccelerates synchronization",
    "start": "860320",
    "end": "865870"
  },
  {
    "text": "of data exchange threads, and\nthread blocks on a cluster can directly synchronize\neach other through barriers",
    "start": "865870",
    "end": "871150"
  },
  {
    "text": "and distributed shared memory. It also supports\nasynchronous DMA operations between the thread\nblocks themselves.",
    "start": "871150",
    "end": "877110"
  },
  {
    "text": " Now let's move away\nfrom spatial locality",
    "start": "877110",
    "end": "883400"
  },
  {
    "text": "and talk about\ntemporal locality. One form of temporal\nlocality that is supported in\nexisting NVIDIA GPUs",
    "start": "883400",
    "end": "890390"
  },
  {
    "text": "is dependent grid launch,\nexecuting on data in L2 in the HBM3 memory.",
    "start": "890390",
    "end": "896449"
  },
  {
    "text": "In the figure, you see an\nexample of a fast Fourier transform workflow. It is the transporter transform.",
    "start": "896450",
    "end": "902498"
  },
  {
    "text": "There are multiple competition\nsteps are needed to form the transform, each with\na different ratings.",
    "start": "902498",
    "end": "908259"
  },
  {
    "text": "Here the data is moved\ninto the memory locally. From there, each\nstep is performed by reading the data from local\nmemory, doing the computation,",
    "start": "908260",
    "end": "915310"
  },
  {
    "text": "and writing the results\nback to local memory so it's available\nto the next step. Limitations of this\nis that each step",
    "start": "915310",
    "end": "921040"
  },
  {
    "text": "must be a different kernel\nin the CUDA program model and launched as the\nradix algorithm.",
    "start": "921040",
    "end": "927070"
  },
  {
    "text": "Each step requires a different\nconfiguration of the SM. That means that between each\nstep, any data stored in the SM",
    "start": "927070",
    "end": "934300"
  },
  {
    "text": "must be first flushed\nback to memory and then read back\nin for the next step. ",
    "start": "934300",
    "end": "941980"
  },
  {
    "text": "To prove the efficiency\nof temporal locality, we added a new capability.",
    "start": "941980",
    "end": "946990"
  },
  {
    "text": "Thread block reconfiguration. Going back to the example,\nyou can see how this works. There's a single grid slot\nfor the entire transform.",
    "start": "946990",
    "end": "954580"
  },
  {
    "text": "At each radix step, the threads\nand register file resources can be reconfigured to match\nwhat's needed for the radix done",
    "start": "954580",
    "end": "960790"
  },
  {
    "text": "at that step. At beginning of\nthe transform, data is loaded from memory\ninto shared memory",
    "start": "960790",
    "end": "965920"
  },
  {
    "text": "and distributed shared\nmemory for things in the concept of clusters,\nand each radix steps",
    "start": "965920",
    "end": "971560"
  },
  {
    "text": "perform without data\nleaving that memory. So instead of the\ntemporal locality",
    "start": "971560",
    "end": "977210"
  },
  {
    "text": "of execution passing over\ndata and L2 and memory, we now have execution passing\nover much closer and much more",
    "start": "977210",
    "end": "984260"
  },
  {
    "text": "efficient shared memory. ",
    "start": "984260",
    "end": "991050"
  },
  {
    "text": "Now let's look closer at another\ncore principle performance that we looked at. Asynchronous execution\nand data transfer.",
    "start": "991050",
    "end": "999570"
  },
  {
    "text": "On first review, how things\nwork in a synchronous machine design approach.",
    "start": "999570",
    "end": "1004698"
  },
  {
    "text": "Working on parallel\ncomputation, multiple threads will work together to\nperform that computation. One way to do this is that\nthreads can work cooperatively.",
    "start": "1004698",
    "end": "1012980"
  },
  {
    "text": "This is where each thread\nis doing some computation but are working on\ndifferent parts of the data, seeing the same\ncomputation but working",
    "start": "1012980",
    "end": "1019640"
  },
  {
    "text": "on different parts of the data. At each phase of\ncomputation, the threads will synchronize to ensure that\nthe data they are working on",
    "start": "1019640",
    "end": "1025650"
  },
  {
    "text": "is ready for the next step of\nthe computation, next phase of computation on that data.",
    "start": "1025650",
    "end": "1030849"
  },
  {
    "text": "Another model is where threads\ncan have more producer-consumer model. This is where data is produced\nby one thread is consumed",
    "start": "1030849",
    "end": "1037641"
  },
  {
    "text": "by another thread\nand they're doing essentially different\ncomputations in different parts of a pipeline computation. Here the consumer needs to\nsynchronize with the producer",
    "start": "1037641",
    "end": "1044949"
  },
  {
    "text": "to ensure the data\nhas been produced and is ready to be consumed. In a synchronous\nmachine approach,",
    "start": "1044950",
    "end": "1050950"
  },
  {
    "text": "all threads must arrive\nand wait at the barrier until all other threads have\nalso arrived at the barrier. They effectively running\nkind of a lock step manner.",
    "start": "1050950",
    "end": "1058450"
  },
  {
    "text": "This leads to inefficiency where\nthreads and execution resources are idle, waiting for\nthe barrier to clear.",
    "start": "1058450",
    "end": "1065940"
  },
  {
    "text": "Let's see how things work with\nan asynchronous machine design approach here. Synchronization split.",
    "start": "1065940",
    "end": "1071130"
  },
  {
    "text": "There is an arrive. There's Robert Barret, and then\nthere's the wait at that bear.",
    "start": "1071130",
    "end": "1076150"
  },
  {
    "text": "When threads are\ncooperatively executing, all threads continue to arrive\nand wait at the barrier. However, between the\narrive and the wait,",
    "start": "1076150",
    "end": "1082570"
  },
  {
    "text": "each thread can schedule\nthread-independent work. So instead of being idle waiting\nfor other threads to arrive, each thread can be\nbusy doing useful work,",
    "start": "1082570",
    "end": "1089410"
  },
  {
    "text": "and you can hide the\ncost of synchronization. In the producer-consumer\nmodel, the threads",
    "start": "1089410",
    "end": "1095290"
  },
  {
    "text": "operate in a decoupled manner. After the producer first\nproduces the first batch of data, it arrives\nand immediately",
    "start": "1095290",
    "end": "1100320"
  },
  {
    "text": "starts producing the\nsecond batch of data. It doesn't have to\nwait for the consumer. The data it needs is available.",
    "start": "1100320",
    "end": "1105549"
  },
  {
    "text": "It immediately starts\nto consume the data without waiting or any\nunnecessary synchronization. As you can see that an\nasynchronous design approach",
    "start": "1105550",
    "end": "1112950"
  },
  {
    "text": "can lead to much more\nefficient parallel execution. ",
    "start": "1112950",
    "end": "1124002"
  },
  {
    "text": "Before I dive into the\ndetails of hopper improvement, I'm going to start with a view\nof the asynchronous bears that were available on A100\nbecause we already",
    "start": "1124002",
    "end": "1129898"
  },
  {
    "text": "had the concept and\nthe ability to do asynchronous bears in A100,\nour previous generation.",
    "start": "1129898",
    "end": "1135497"
  },
  {
    "text": "Consider again, the example\nwhere a set of threads are executing a set of\ndata as shown on the right. A100 is asynchronous, barrier\nsplit the synchronization",
    "start": "1135497",
    "end": "1143159"
  },
  {
    "text": "into two steps. First, the thread\nsignals arrive where they're done producing a\nproportion of the shared data, and this arrives as\nnon-blocking, so the threads",
    "start": "1143160",
    "end": "1150120"
  },
  {
    "text": "are free to execute\nother independent work. Eventually, the\nthread needs to-- needs the data produced\nby all other threads.",
    "start": "1150120",
    "end": "1156810"
  },
  {
    "text": "At this point, it\ndoes a wait, which blocks until every thread has\nactually signaled their arrive.",
    "start": "1156810",
    "end": "1162113"
  },
  {
    "text": "And as shown before,\nthe advantage of asynchronous barriers that\nallow the threads to arrive to execute on independent\nwork instead of just waiting",
    "start": "1162113",
    "end": "1168630"
  },
  {
    "text": "and spinning, waiting for\nthe threads to happen. New for H1 is ability\nfor the waiting threads",
    "start": "1168630",
    "end": "1174340"
  },
  {
    "text": "to sleep while threads arrive. On previous chips,\nthe waiting threads have to spin to go\nthrough a spin loop",
    "start": "1174340",
    "end": "1179620"
  },
  {
    "text": "on the barrier object\nand share memory, taking up execution\nresourcing and adding latency. Now we've accelerated\nthat sort of process",
    "start": "1179620",
    "end": "1186550"
  },
  {
    "text": "where they can go\nto sleep, and they don't take extra resources\njust spinning on things.",
    "start": "1186550",
    "end": "1192490"
  },
  {
    "text": "They just wait until\nthe barrier resolves, and they immediately\nstart executing again. ",
    "start": "1192490",
    "end": "1200580"
  },
  {
    "text": "It's also new for\nHopper is a new form of a barrier we call the\nasynchronous transaction barrier.",
    "start": "1200580",
    "end": "1206309"
  },
  {
    "text": "The asynchronous transaction\nbarrier is also slip barrier, but instead of just\ncounting thread arrivals, it also counts memory\ntransactions themselves.",
    "start": "1206310",
    "end": "1214260"
  },
  {
    "text": "There are new\ncommands for writing memory that passes both data\nto be written plus the barrier",
    "start": "1214260",
    "end": "1219330"
  },
  {
    "text": "update in effect. The barrier can be triggered\nto track not only threads but also asynchronous\nmemory transactions as well.",
    "start": "1219330",
    "end": "1227360"
  },
  {
    "text": "The transaction barrier\nwill block threads at the wait command until all\nproducer threads have arrived and all asynchronous memory\ntransactions have completed.",
    "start": "1227360",
    "end": "1235929"
  },
  {
    "text": "It is very powerful\nprimitive and it works great for asynchronous\nmemory copies or data exchanges.",
    "start": "1235930",
    "end": "1243279"
  },
  {
    "text": "So let's look how the new\nbarrier support, combined with thread block clusters,\nimprove the block-to-block data",
    "start": "1243280",
    "end": "1250900"
  },
  {
    "text": "exchange and let you see how\nblocked the block data exchange is done on older NVIDIA GPUs.",
    "start": "1250900",
    "end": "1258800"
  },
  {
    "text": "Data exchange through global\nmemory using a barrier stored in the two thread blocks. One wanted to communicate\nto as needed to communicate",
    "start": "1258800",
    "end": "1266821"
  },
  {
    "text": "with each other, they had\nto go through global memory and synchronize\nthe global memory. The sequence for the producer\nis they write the data,",
    "start": "1266822",
    "end": "1273590"
  },
  {
    "text": "they do a memory barrier,\nthey write the barrier flag. The sequence of consumer is\nthey pull the flag to terminate,",
    "start": "1273590",
    "end": "1278690"
  },
  {
    "text": "the barrier is cleared, and\nthen they read the data. To perform a data exchange\nrequires three to four round",
    "start": "1278690",
    "end": "1284970"
  },
  {
    "text": "trips through\nglobal memory, which is fairly and very inefficient\nfrom a latency perspective.",
    "start": "1284970",
    "end": "1290033"
  },
  {
    "text": "All right, you see how\nthings are done hopper. You see the producer\nstores data directly into consumer's shared\nmemory while updating",
    "start": "1290033",
    "end": "1297260"
  },
  {
    "text": "the barrier that's also in\nthe consumer shared memory. This enables a minimum latency\ndata exchange, a one-way trip",
    "start": "1297260",
    "end": "1304320"
  },
  {
    "text": "from the producer\nto the consumer, and results in a 7x\nlatency reduction. ",
    "start": "1304320",
    "end": "1312790"
  },
  {
    "text": "Hopper also accelerates memory\ncopies using a new TMA unit. The TMA unit is able to\nasynchronously copy new memory",
    "start": "1312790",
    "end": "1320453"
  },
  {
    "text": "copies between global\nand shared memory, as well as between shared memory\nand shared memory between blocks in a cluster.",
    "start": "1320453",
    "end": "1326840"
  },
  {
    "text": "Using the numeric\ntransaction, it can do copies with\nminimal latency. The TMA is fully asynchronous\nwith thread execution.",
    "start": "1326840",
    "end": "1334552"
  },
  {
    "text": "There is no address\ngeneration or data movement or synchronization management\noverhead requirement on the thread.",
    "start": "1334552",
    "end": "1339780"
  },
  {
    "text": "From the threads perspective,\nit's simply a fire and forget. It says, do this copy and\nthen forgets about it.",
    "start": "1339780",
    "end": "1345750"
  },
  {
    "text": "Doesn't have to worry\nabout it at all. This makes for a very simplified\nand efficient programming model. ",
    "start": "1345750",
    "end": "1353409"
  },
  {
    "text": "Here is an example\nof how the halo data exchange is performed on Hopper\nwith its new capabilities.",
    "start": "1353410",
    "end": "1358420"
  },
  {
    "text": "Here we have a cluster\nof four thread blocks, each working on different\nparts of the data. When one thread block needs to\nget halo data to another thread",
    "start": "1358420",
    "end": "1366610"
  },
  {
    "text": "block, the other\nthread block writes it directly into its own\nlocal shared memory, updating a local barrier\nindicating the data is ready.",
    "start": "1366610",
    "end": "1374350"
  },
  {
    "text": "So here the data exchange\nis done asynchronously and with minimal latency. ",
    "start": "1374350",
    "end": "1382990"
  },
  {
    "text": "Then to summarize, the\ngoal of async execution is to keep all units\nbusy on the GPU, fully utilize and allow more\noverlap of computation and data",
    "start": "1382990",
    "end": "1390540"
  },
  {
    "text": "movement. Hopper introduces an\nasynchronous transaction barrier for atomic data synchronization\nand makes waiting",
    "start": "1390540",
    "end": "1396659"
  },
  {
    "text": "on barriers more efficient. Hopper also introduces an\nonshore accelerator for memory copies called the TMA unit.",
    "start": "1396660",
    "end": "1403170"
  },
  {
    "text": "It frees the\ngeneral-purpose threads from doing memory operations\nand adds calculations, allowing the threads\nto instead to focus",
    "start": "1403170",
    "end": "1409169"
  },
  {
    "text": "on independent\nprocessing of tasks and not have to worry about\ndata movement themselves. ",
    "start": "1409170",
    "end": "1416080"
  },
  {
    "text": "To combine Hoppers\nand rooms for locality with the asynchronous\nexecution data movement, you can get dramatic\nperformance improvements.",
    "start": "1416080",
    "end": "1423659"
  },
  {
    "text": "Here you can see in this\nwhere the performance room is a Hopper with these\nnew capability used, taking advantage of\nlocality and asynchrony.",
    "start": "1423660",
    "end": "1431100"
  },
  {
    "text": "Asynchrony provides 2 to\n3x performance benefits on these key algorithms.",
    "start": "1431100",
    "end": "1436230"
  },
  {
    "text": " Now let's look at some of\nthe improvements Hopper",
    "start": "1436230",
    "end": "1441280"
  },
  {
    "text": "made to accelerate\ndeep learning in AI. At the heart of the Hopper is\na new fourth-generation tensor",
    "start": "1441280",
    "end": "1449100"
  },
  {
    "text": "core. It has doubled throughput per\nSM of all the data formats. Hopper tensor core is\nalso add a new eight-bit",
    "start": "1449100",
    "end": "1456240"
  },
  {
    "text": "floating point format. It provides twice the throughput\nof FP16 and BFloat16 formats,",
    "start": "1456240",
    "end": "1462210"
  },
  {
    "text": "and matching the throughput\nof eight-bit integer. In order to keep the test score\nstraight and keep the GPU power",
    "start": "1462210",
    "end": "1468090"
  },
  {
    "text": "in check, hopper\ntensor cores have also improved opera and\ndeliverance by 30%.",
    "start": "1468090",
    "end": "1474520"
  },
  {
    "text": "H100, like A100, supports\nsparse tensor arithmetic, enabling 2x throughput when\nthe data is also sparse.",
    "start": "1474520",
    "end": "1481765"
  },
  {
    "text": " This chart shows the\ncumulative compute improvements",
    "start": "1481765",
    "end": "1488770"
  },
  {
    "text": "for going from A100 to H100. The increase in number of\nassets provides a 22% increase.",
    "start": "1488770",
    "end": "1494590"
  },
  {
    "text": "The fourth-generation\ntensor core provides an additional\n2x improvement. Switching from\n16-bit floating point",
    "start": "1494590",
    "end": "1500290"
  },
  {
    "text": "to the new eight-bit\nfloating point format provides another\nadditional 2x improvement.",
    "start": "1500290",
    "end": "1505690"
  },
  {
    "text": "And finally, the clock\nfrequency improvements provide another 30% increase. All combined provides\n66x throughput increase",
    "start": "1505690",
    "end": "1513490"
  },
  {
    "text": "over our previous generation.  Let's kind of dive\ninto the new FP8 format",
    "start": "1513490",
    "end": "1521110"
  },
  {
    "text": "support a little more detail. Unless we see the exponent\nrange of this precision divided by each tensor\ncore floating point format.",
    "start": "1521110",
    "end": "1529640"
  },
  {
    "text": "You'll note that the tensor\ncore supports two floating point 8, eight-bit floating\npoint formats.",
    "start": "1529640",
    "end": "1535130"
  },
  {
    "text": "One provides an extra\nbit of exponent range that matches the\nexponent range of FP16.",
    "start": "1535130",
    "end": "1540740"
  },
  {
    "text": "It can be thought of a truncated\nmantissa version of FP16, much like how BFloat16\nis a truncated mantissa",
    "start": "1540740",
    "end": "1547580"
  },
  {
    "text": "version of FP32. The other eight\nformat trades off",
    "start": "1547580",
    "end": "1552600"
  },
  {
    "text": "one bit of range for an\nadditional one bit of precision. On the right, you can\nsee how the tensor",
    "start": "1552600",
    "end": "1557900"
  },
  {
    "text": "core processes\nFP8 floating point eight-bit floating point data.",
    "start": "1557900",
    "end": "1563090"
  },
  {
    "text": "The multiplication of\neight-bit floating point data can be accumulated into\neither FP32 or FP16.",
    "start": "1563090",
    "end": "1570000"
  },
  {
    "text": "Once the matrix\nmultiply is done, various common network\nneural functions like adding bias and applying\nan activation function",
    "start": "1570000",
    "end": "1576419"
  },
  {
    "text": "can be performed in the\nhigher FP32 or FP16 precision. The final result\nis then converted",
    "start": "1576420",
    "end": "1582570"
  },
  {
    "text": "to the desired output format\nbefore being stored back to memory. ",
    "start": "1582570",
    "end": "1589433"
  },
  {
    "text": "The graph on the left shows\na numerical distribution for a variety of network layers\nand a single language model. You can see that\nthe distribution",
    "start": "1589433",
    "end": "1595890"
  },
  {
    "text": "can vary significantly. Some have a wider exponent range\nthat are best mapped to the E5M2",
    "start": "1595890",
    "end": "1602130"
  },
  {
    "text": "FP8 format others have\na narrow exponent range require more precision to\ndifferentiate the value.",
    "start": "1602130",
    "end": "1608100"
  },
  {
    "text": "These are best mapped\nto an E4M3 FP8 format.",
    "start": "1608100",
    "end": "1613140"
  },
  {
    "text": "On the right, you can see\nthat during the linear math computation, the\nrange of the results can be outside the\nrepresentable range.",
    "start": "1613140",
    "end": "1621340"
  },
  {
    "text": "Performing the matrix multiply\naccumulation in the FP16 or FP32 formats, we are able to\nsafely perform the computation",
    "start": "1621340",
    "end": "1628080"
  },
  {
    "text": "without any loss. But when we convert back\ndown to the FP8 format,",
    "start": "1628080",
    "end": "1633360"
  },
  {
    "text": "we scalarize the results into\nthe FP8 representable range. ",
    "start": "1633360",
    "end": "1643670"
  },
  {
    "text": "So the goal of this position\nis to intelligently manage the precision and\nmaintain accuracy while still gain the\nperformance of smaller,",
    "start": "1643670",
    "end": "1650390"
  },
  {
    "text": "faster numerical formats. Statistics of the output values\nof each layer of a network",
    "start": "1650390",
    "end": "1655420"
  },
  {
    "text": "can be analyzed to determine\nwhich FP8 format is optimal and what scaling\nfactor is needed when converting to\nthe final competition",
    "start": "1655420",
    "end": "1662440"
  },
  {
    "text": "into that FP8 format. The figure at right shows\ntraining of GPT-3 model",
    "start": "1662440",
    "end": "1668150"
  },
  {
    "text": "and how the training\nerror changes over time for various layers. It compares the training\nerror using FP8 format",
    "start": "1668150",
    "end": "1674929"
  },
  {
    "text": "using versus the\nnative BFloat16 format. You can see that you\nprobably can't tell",
    "start": "1674930",
    "end": "1681890"
  },
  {
    "text": "but because the lines are right\non top of each other, they basically--",
    "start": "1681890",
    "end": "1687920"
  },
  {
    "text": "using training, using FP8\nand scaling techniques, you can almost precisely\nmatch the error over time",
    "start": "1687920",
    "end": "1694490"
  },
  {
    "text": "of the native BFloat16 format. On the left, you see a\ntable of the final accuracy",
    "start": "1694490",
    "end": "1700090"
  },
  {
    "text": "for a variety of networks using\nFP8 versus native 16-bit format. As you can see, the accuracy\nare nearly identical.",
    "start": "1700090",
    "end": "1707830"
  },
  {
    "text": " Let's dive a bit more\ninto the new unit.",
    "start": "1707830",
    "end": "1713862"
  },
  {
    "text": "I've already talked about\nhow the two main unit efficiently performs\nasynchronous copies, but it was designed specifically\nto handle complexity",
    "start": "1713862",
    "end": "1721130"
  },
  {
    "text": "of copying and managing\nmultidimensional deep learning tensors and memory. It will automatically impute\nthe addresses, strides, bounds",
    "start": "1721130",
    "end": "1728500"
  },
  {
    "text": "checking needed for\ntensors up to rank 5. It will even automatically\npad out of bounds values",
    "start": "1728500",
    "end": "1734049"
  },
  {
    "text": "so you don't get junk when\ncopies go out of bounds and you need to pad.",
    "start": "1734050",
    "end": "1739860"
  },
  {
    "text": "All this is done with\na fire and forget model from a single thread. Single thread fires it\noff, it does the copy,",
    "start": "1739860",
    "end": "1745380"
  },
  {
    "text": "and the thread can\ncontinue executing, and once the copy is finished,\nthe asynchronous transaction",
    "start": "1745380",
    "end": "1753540"
  },
  {
    "text": "barrier will\nactually get updated, and then computation can be done\non that on that copied data.",
    "start": "1753540",
    "end": "1759615"
  },
  {
    "start": "1759615",
    "end": "1764640"
  },
  {
    "text": "So you can see, taking all that\nin, how much end-to-end speedup. We get on a model like GPT-3\nwith sequence like 2048.",
    "start": "1764640",
    "end": "1775170"
  },
  {
    "text": "And you can see that we\nget approximately almost a3x application speedup\nusing FP8 versus on H100",
    "start": "1775170",
    "end": "1782940"
  },
  {
    "text": "versus using BFloat and A100. And all experiments were\ndone running on a 64 GPUs. ",
    "start": "1782940",
    "end": "1792182"
  },
  {
    "text": "The Harbor is designed with\nthe rest of the system in mind. It's not just the chip itself. It needs to make sure it can\nintegrate to a larger system",
    "start": "1792182",
    "end": "1799050"
  },
  {
    "text": "and run very well. So it has direct support\nfor tight integration with other components and the\nability to scale up and out.",
    "start": "1799050",
    "end": "1805550"
  },
  {
    "text": "Give you a brief overview of\nsome of the integration scaling capabilities of the Hopper GPU\nand how the systems are built.",
    "start": "1805550",
    "end": "1815310"
  },
  {
    "text": "GPU is the H1 GPU is packaged\nin a module with 94GB of memory",
    "start": "1815310",
    "end": "1821010"
  },
  {
    "text": "with 3.5TB per\nsecond of bandwidth.",
    "start": "1821010",
    "end": "1826400"
  },
  {
    "text": "Eight of these GPUs are fully\nconnected on a single board using the NVLink switches,\nproviding 900GB per second",
    "start": "1826400",
    "end": "1833300"
  },
  {
    "text": "of bandwidth per GPU.  Here's what the board\nlooked like when",
    "start": "1833300",
    "end": "1838870"
  },
  {
    "text": "you add all the heat\nsinks, IO interfaces, and other components. Now there's a kind of block\ndiagram of that board,",
    "start": "1838870",
    "end": "1846400"
  },
  {
    "text": "and showing the main\ncomponents of the node and how they're connected. There are two CPUs that drive\nthe system along with some six,",
    "start": "1846400",
    "end": "1853210"
  },
  {
    "text": "seven, NICs and PCIe switches. All the A100 GPUs are fully\nconnected with switches.",
    "start": "1853210",
    "end": "1859600"
  },
  {
    "text": "These switches are also\nconnected to a high bandwidth NVLink IO ports leading\nout of the node.",
    "start": "1859600",
    "end": "1864985"
  },
  {
    "text": " Here's all patched\ntogether in the node.",
    "start": "1864985",
    "end": "1870490"
  },
  {
    "text": "It's what the box\nactually looks like. This node is plugged into a\nQuantum two InfiniBand network",
    "start": "1870490",
    "end": "1877450"
  },
  {
    "text": "with 400GBs per second\nof bandwidth per ports. ",
    "start": "1877450",
    "end": "1883559"
  },
  {
    "text": "And this system can\nscale to hundred or thousands of DGX nodes. ",
    "start": "1883560",
    "end": "1891770"
  },
  {
    "text": "Now this system redesign\nactually provides a very efficient scaling. Here you see the end-to-end\nperformance for weak scaling",
    "start": "1891770",
    "end": "1898130"
  },
  {
    "text": "and is almost linear, going\nfrom 32 GPUs to over 3000 GPUs.",
    "start": "1898130",
    "end": "1907740"
  },
  {
    "text": "Also, strong scales\nvery well, not just weak scale but strong very well. It provides almost\nlinear speedup",
    "start": "1907740",
    "end": "1913350"
  },
  {
    "text": "for Megatron going from\n64 GPUs to 2,048 GPUs.",
    "start": "1913350",
    "end": "1920160"
  },
  {
    "text": "We expect to push that\neven beyond 2,048 as well.",
    "start": "1920160",
    "end": "1928080"
  },
  {
    "text": "Right now, for questions that's\nit, that's it in a nutshell. You find more information in the\nHopper architecture whitepaper.",
    "start": "1928080",
    "end": "1935780"
  },
  {
    "text": "Any questions?  Amazing piece of hardware, Jack.",
    "start": "1935780",
    "end": "1941030"
  },
  {
    "text": " As the slide says, now this is,\nmany engineers worked on this.",
    "start": "1941030",
    "end": "1949063"
  },
  {
    "text": "And designed it and\nbuilt these systems. So it's a product of a lot\nof work with a lot of people. Yeah.",
    "start": "1949063",
    "end": "1954530"
  },
  {
    "text": "I can't take credit for it. OK. The one thing that--",
    "start": "1954530",
    "end": "1959840"
  },
  {
    "text": "of course is always a problem\nfor this class of machine is some of the advantage\ncomes from judicious",
    "start": "1959840",
    "end": "1967670"
  },
  {
    "text": "choices in layout of data\nand structure of the program",
    "start": "1967670",
    "end": "1973070"
  },
  {
    "text": "that uses the machine. And is that now entirely\nin the hands of CUDA",
    "start": "1973070",
    "end": "1979250"
  },
  {
    "text": "or is it something\nthat a programmer needs to actually do the\nwork to do the partitioning?",
    "start": "1979250",
    "end": "1987070"
  },
  {
    "text": "Well, we provide multiple\nsort of abstractions.",
    "start": "1987070",
    "end": "1994240"
  },
  {
    "text": "Right now, there\nare people, if you want, you can program\nto the bare metal and handle everything\nyourself, right?",
    "start": "1994240",
    "end": "2000810"
  },
  {
    "text": "And CUDA has that\nlevel of abstraction, and you can program\nin the CUDA level. When it comes to a lot of\nthese networks, most of this",
    "start": "2000810",
    "end": "2008700"
  },
  {
    "text": "is actually done\nunder our libraries and the abstraction that we\nwould do is we provide like 2GNN",
    "start": "2008700",
    "end": "2013770"
  },
  {
    "text": "and other libraries, other\nsort of compilers and stuff that will actually map things\nefficiently so people who",
    "start": "2013770",
    "end": "2019200"
  },
  {
    "text": "are doing DL networks and\ntrying to run on our system, they are not\nprogramming in CUDA,",
    "start": "2019200",
    "end": "2025740"
  },
  {
    "text": "they're programming\nmore the Python layer using regular sort\nof thing and all",
    "start": "2025740",
    "end": "2033210"
  },
  {
    "text": "the software and abstractions\nand compiler and libraries that need that, do the\nmapping for you.",
    "start": "2033210",
    "end": "2039840"
  },
  {
    "text": "If I want one for\nmy home computer, How much it's going to cost me?",
    "start": "2039840",
    "end": "2045440"
  },
  {
    "text": "Which one, just\nthe DGX box, or you want the whole\nsystem here, right?",
    "start": "2045440",
    "end": "2052489"
  },
  {
    "text": "You want this guy. Yeah, like the big one,\nbut the small one is easier",
    "start": "2052489",
    "end": "2058069"
  },
  {
    "text": "to carry around. No, I'm just looking\nfor scale of cost here.",
    "start": "2058070",
    "end": "2064869"
  },
  {
    "text": "To tell you the truth, I'm\nnot in sales and marketing, so I don't really know the\nexact cost for these things.",
    "start": "2064870",
    "end": "2073110"
  },
  {
    "text": "I do know that the\ntotal cost of ownership and the performance per\ntotal cost of ownership,",
    "start": "2073110",
    "end": "2080530"
  },
  {
    "text": "as well as execution-- sorry, not just\npurchase but to run it--",
    "start": "2080530",
    "end": "2088270"
  },
  {
    "text": "is actually very good\ncompared to other systems. And that's actually\none thing that we",
    "start": "2088270",
    "end": "2094060"
  },
  {
    "text": "do when we start talking\nabout how it filters down to me doing the architecture\nand stuff like that is that you",
    "start": "2094060",
    "end": "2105010"
  },
  {
    "text": "think like also for\nmillimeter-sized stuff, well, there are the cost\nis not just in the chip.",
    "start": "2105010",
    "end": "2114220"
  },
  {
    "text": "The cost is in the\nsystem, and you need to make sure that you build\na chip that even if it's not",
    "start": "2114220",
    "end": "2119710"
  },
  {
    "text": "the great performer like\nhaving all the NVLink IO and all other support says,\nwell, that makes the chip less",
    "start": "2119710",
    "end": "2125620"
  },
  {
    "text": "per millimeter but it makes\nthe system much more efficient and much more cost-effective.",
    "start": "2125620",
    "end": "2131170"
  },
  {
    "text": "So you need to think about that\nalso from power perspective and how you manage things\nbecause the cost of ownership,",
    "start": "2131170",
    "end": "2137319"
  },
  {
    "text": "power cost, is a significant\npart of the cost of ownership, and you make sure\nthat you do work well",
    "start": "2137320",
    "end": "2142570"
  },
  {
    "text": "on that even if it helps or\nhurts your per millimeter. You got to think about the\noverall power of the system",
    "start": "2142570",
    "end": "2148360"
  },
  {
    "text": "and how you actually make\nthe overall the system more",
    "start": "2148360",
    "end": "2154120"
  },
  {
    "text": "cost-effective to the\npeople who are buying them",
    "start": "2154120",
    "end": "2159390"
  },
  {
    "text": "Sounds good. All right, questions? Everybody? OK. I have one person.",
    "start": "2159390",
    "end": "2165560"
  },
  {
    "text": "Yeah, hey I have a question. I'll just read the question\nwhich somebody put in chat. OK. So 11kWh PBP, I guess\nthat's for-- must",
    "start": "2165560",
    "end": "2172790"
  },
  {
    "text": "be for a constellation of\nthem, not for a single H100. Can you tell us a bit\nabout power and cooling?",
    "start": "2172790",
    "end": "2180210"
  },
  {
    "text": "Of the system, no, I'm\nnot that familiar with it. You probably know more about\nit by looking it up than I do.",
    "start": "2180210",
    "end": "2186420"
  },
  {
    "text": " I don't do the system design. I do the GPU and so\nalthough I'm fairly",
    "start": "2186420",
    "end": "2195150"
  },
  {
    "text": "familiar with some of the trade\noffs and the concerns they have, I'm not the right person\nto quote the right specs.",
    "start": "2195150",
    "end": "2201773"
  },
  {
    "text": "Like I said, you probably\nlooked up more information than what I know off the top. Well, maybe could you just talk\nabout from a GPU standpoint?",
    "start": "2201773",
    "end": "2207580"
  },
  {
    "text": "Like, I'm certainly thermals are\nlike a very big concern or yeah, like what are aspects of\nthe design which really were",
    "start": "2207580",
    "end": "2214230"
  },
  {
    "text": "influenced by power, et cetera? Well, so the essence of\nthe design, you look at it,",
    "start": "2214230",
    "end": "2223710"
  },
  {
    "text": "these are the GPUs, and\nyou add the heatsinks. That's that. There's a lot of power\nto get out of there,",
    "start": "2223710",
    "end": "2229079"
  },
  {
    "text": "and part of what you see is\nthat from the board-- the GPU,",
    "start": "2229080",
    "end": "2234450"
  },
  {
    "text": "you'd like to have all the\npower in the world and he says, well, I can get more\nperformance by more power, but the challenge is that\nit has to fit into the form",
    "start": "2234450",
    "end": "2242010"
  },
  {
    "text": "factor and the system, and we're\none component of the system.",
    "start": "2242010",
    "end": "2247590"
  },
  {
    "text": "And like I said, to do the\nthermals and the whole thing, but there's a lot of\nfeedback of how we make sure",
    "start": "2247590",
    "end": "2255570"
  },
  {
    "text": "that we're power-efficient\nin order to get the most performance out of a\nparticular power budget,",
    "start": "2255570",
    "end": "2262770"
  },
  {
    "text": "and that actually influences\nfairly substantially. ",
    "start": "2262770",
    "end": "2268880"
  },
  {
    "text": "And I do know that there are\ndifferent sort of systems. And again, you probably\nknow better than I",
    "start": "2268880",
    "end": "2275000"
  },
  {
    "text": "because you're asking\nthese questions and probably looked into\nit a lot more than I have. But there are sort of things\nabout air-cooled versus",
    "start": "2275000",
    "end": "2283970"
  },
  {
    "text": "water-cooled and what different\ndata centers can support.",
    "start": "2283970",
    "end": "2290840"
  },
  {
    "text": "They're not all\nidentical, and you can't plug them\nall the same way, and so when you start looking\nat the performance-power sort",
    "start": "2290840",
    "end": "2298130"
  },
  {
    "text": "of curves, you can actually\nget a fair amount of efficiency",
    "start": "2298130",
    "end": "2303680"
  },
  {
    "text": "by going up the\nperformance power curve, assuming the system\ncan handle it.",
    "start": "2303680",
    "end": "2309920"
  },
  {
    "text": "So a lot of data centers\nare moving towards something where the individual\nnodes can actually",
    "start": "2309920",
    "end": "2315830"
  },
  {
    "text": "take a little more\npower and getting actual lower cost of\nownership by riding",
    "start": "2315830",
    "end": "2323570"
  },
  {
    "text": "that curve a bit better.  OK. ",
    "start": "2323570",
    "end": "2332150"
  },
  {
    "text": "So what do you think is\ngoing to happen next? ",
    "start": "2332150",
    "end": "2342520"
  },
  {
    "text": "Now we're working on our\nnext generation stuff and you'll see that eventually.",
    "start": "2342520",
    "end": "2348880"
  },
  {
    "text": " There's a lot of\ninteresting challenges",
    "start": "2348880",
    "end": "2356440"
  },
  {
    "text": "particularly with large language\nmodels and the direction that AI is going to.",
    "start": "2356440",
    "end": "2361869"
  },
  {
    "text": " As far as specifics,\nI don't think",
    "start": "2361870",
    "end": "2367090"
  },
  {
    "text": "I can talk about\nspecifics, about what's the main problems we're tackling\nand how we're tackling them.",
    "start": "2367090",
    "end": "2375020"
  },
  {
    "text": "But this is not the\npinnacle of where we can be.",
    "start": "2375020",
    "end": "2380830"
  },
  {
    "text": "No, it's just-- --we know that. It's a stepping\nstone in between. Yeah. And that's an impressive\npiece of hardware.",
    "start": "2380830",
    "end": "2391730"
  },
  {
    "text": "Yeah. But when you look\nat it, you know, sort of from 100,000-foot range,\nit's basically just another",
    "start": "2391730",
    "end": "2400570"
  },
  {
    "text": "collection of processors and\nmemory put together in a clever way.",
    "start": "2400570",
    "end": "2405850"
  },
  {
    "text": "And it's years of effort to\ntry to do that right and quite",
    "start": "2405850",
    "end": "2411820"
  },
  {
    "text": "amazing. Are there more questions? I don't see-- I have. This is-- sorry, there's\nanother question.",
    "start": "2411820",
    "end": "2419720"
  },
  {
    "text": "If you could just talk a little\nbit about the memory interfaces and issues in memory like\nHBM3, like the signaling rates,",
    "start": "2419720",
    "end": "2428900"
  },
  {
    "text": "bandwidth versus\nlatency, because there is like a bandwidth-latency\ntrade-off, right? HBM is higher latency\nover regular DDR.",
    "start": "2428900",
    "end": "2435740"
  },
  {
    "text": "Can you maybe talk a little bit\nabout what the memory surface looks like from your perspective\nand what are the challenges?",
    "start": "2435740",
    "end": "2443420"
  },
  {
    "text": "When it comes to the workloads\nthat we're dealing with and how we architect\nour GPUs and our systems",
    "start": "2443420",
    "end": "2454880"
  },
  {
    "text": "and how they're programmed and\nhow we deal with parallelism,",
    "start": "2454880",
    "end": "2459920"
  },
  {
    "text": "we're not like a regular CPU\nor memory latency is king. Latency is not king.",
    "start": "2459920",
    "end": "2466520"
  },
  {
    "text": "It's a much lower concern. We don't waste it,\nbut our main concern",
    "start": "2466520",
    "end": "2473470"
  },
  {
    "text": "is efficient bandwidth\nand efficiency from just a utilization\nperspective. That we make sure that we can\nfully utilize whatever bandwidth",
    "start": "2473470",
    "end": "2481120"
  },
  {
    "text": "is given to us and also\nfrom a power perspective. So one thing is it\ndoes both of those.",
    "start": "2481120",
    "end": "2488140"
  },
  {
    "text": "It has the total\nbandwidth that you want. And has the more power\nefficiency than you can",
    "start": "2488140",
    "end": "2494550"
  },
  {
    "text": "of regular DDR. So at least these types\nof system, it's a win-win. Now you can say that it's also\na lot more expensive, so hey,",
    "start": "2494550",
    "end": "2503242"
  },
  {
    "text": "you know what it\nis, but again, you have to look at the total cost\nof ownership, right, and all the parts, not just\nthe- is that if you",
    "start": "2503242",
    "end": "2509440"
  },
  {
    "text": "can get a lot more performance\nout of a more expensive memory subsystem.",
    "start": "2509440",
    "end": "2516220"
  },
  {
    "text": "It can actually be a\nwin for the other ones because it's making all\nyour other stuff that you're",
    "start": "2516220",
    "end": "2522190"
  },
  {
    "text": "doing support the whole\nsystem more efficient because your node is more\nefficient, more performant.",
    "start": "2522190",
    "end": "2527920"
  },
  {
    "text": "So there's kind of a funny\nlittle trade-off there. You're thinking about,\noh, I'm buying a graphics",
    "start": "2527920",
    "end": "2533590"
  },
  {
    "text": "card for my gaming\ncard for my desktop,",
    "start": "2533590",
    "end": "2538990"
  },
  {
    "text": "you know, HBM is too\nexpensive and everything's the cost of there,\nbut when it comes--",
    "start": "2538990",
    "end": "2544730"
  },
  {
    "text": "and the cost of the card\nitself is pretty high. When it comes to the cost\nof the system itself,",
    "start": "2544730",
    "end": "2553220"
  },
  {
    "text": "it is not a big a factor and\nperformance is a bigger factor,",
    "start": "2553220",
    "end": "2558890"
  },
  {
    "text": "power is a bigger factor. ",
    "start": "2558890",
    "end": "2564849"
  },
  {
    "text": "Yeah. That makes a lot of sense. I guess in the one sort\nof follow-up to that,",
    "start": "2564850",
    "end": "2571840"
  },
  {
    "text": "which touches on this issue is-- so you showed some really\nimpressive numbers for LMS",
    "start": "2571840",
    "end": "2579130"
  },
  {
    "text": "and I'm wondering what\nkind of flop utilization",
    "start": "2579130",
    "end": "2586539"
  },
  {
    "text": "do you see right on those LMS\nlike given the HBM bandwidth?",
    "start": "2586540",
    "end": "2592380"
  },
  {
    "text": "Do you have, so you\ngot five banks, right? So that's a lot? Yeah. And so, but like, can\nthose things feed the--",
    "start": "2592380",
    "end": "2599049"
  },
  {
    "text": "can they feed the\nGPU fast enough? Yes, yes.",
    "start": "2599050",
    "end": "2605359"
  },
  {
    "text": "And I can't give you the\nactual specific numbers, but there are multiple\nlayers and there are",
    "start": "2605360",
    "end": "2612500"
  },
  {
    "text": "different levels of efficiency. So it's not like all 100%. Some things are running\nhigh, some things are running more mid-range,\nand certain efficiency.",
    "start": "2612500",
    "end": "2619730"
  },
  {
    "text": "Overall, the utilization\nis pretty good. And I think from\na system, I think",
    "start": "2619730",
    "end": "2628369"
  },
  {
    "text": "you may be asking a system\nbalance perspective. The flops and the HBM,\nis the balance good?",
    "start": "2628370",
    "end": "2637309"
  },
  {
    "text": "It's a pretty good balance\nand part of the balance is that when we went from\none generation to the next,",
    "start": "2637310",
    "end": "2643520"
  },
  {
    "text": "we didn't just scale\neverything equally, right? We actually made the system\nthat's more efficient,",
    "start": "2643520",
    "end": "2651710"
  },
  {
    "text": "and one obvious example of\nthat is going from FP16 to FP8,",
    "start": "2651710",
    "end": "2656720"
  },
  {
    "text": "right? That having a\nsmaller data format makes the memory\nsystem more efficient,",
    "start": "2656720",
    "end": "2663109"
  },
  {
    "text": "and you can get\ntwice as much data in the same amount of bandwidth. That's one obvious example.",
    "start": "2663110",
    "end": "2669240"
  },
  {
    "text": "But just making sure that\nwe schedule and manage our data movement and\nconsumption in a way that",
    "start": "2669240",
    "end": "2679260"
  },
  {
    "text": "can efficiently\nuse and not waste, or when you don't need\nto use the HBM memory,",
    "start": "2679260",
    "end": "2687119"
  },
  {
    "text": "you don't have to.  Cool thing. This is a wonderful talk.",
    "start": "2687120",
    "end": "2692510"
  },
  {
    "text": "Lots of fantastic details. Thank you very much. You're welcome.",
    "start": "2692510",
    "end": "2697680"
  },
  {
    "text": "OK. I hope you can\nhear me well, so I wanted to ask about\nAI ecosystem support.",
    "start": "2697680",
    "end": "2703890"
  },
  {
    "text": "So about NVIDIA's\ncollaborations, partnerships, or initiatives to\nsupport the AI community,",
    "start": "2703890",
    "end": "2711060"
  },
  {
    "text": "like developer programs,\nAI research grants, do you have anything that also\nwould benefit the future GPU",
    "start": "2711060",
    "end": "2718650"
  },
  {
    "text": "developments? Because I see that it also\ngets focused on AI program.",
    "start": "2718650",
    "end": "2723730"
  },
  {
    "text": "So I would like to\nknow more about this. Yeah, I'm probably not\nthe right person to ask.",
    "start": "2723730",
    "end": "2729810"
  },
  {
    "text": "I know we do have a\nlot of initiatives. And I see in my email that\nwe have sort of summaries",
    "start": "2729810",
    "end": "2736772"
  },
  {
    "text": "of things that are happening. I see new initiatives\nevery day, so I'm not on the software side\nor the software stack",
    "start": "2736772",
    "end": "2743200"
  },
  {
    "text": "side or the support side. So I'm probably not the right\nperson to ask that question.",
    "start": "2743200",
    "end": "2751900"
  },
  {
    "text": "But I do know that we do have a\npretty large support and a lot",
    "start": "2751900",
    "end": "2758529"
  },
  {
    "text": "of initiatives to help\npeople use and develop on our GPUs and our systems.",
    "start": "2758530",
    "end": "2766840"
  },
  {
    "text": "Thank you. Sorry, I can't help\nyou with more detail. It's all right, thank you. ",
    "start": "2766840",
    "end": "2776600"
  },
  {
    "text": "Anyone else?  OK, well Jack,\nthank you so much.",
    "start": "2776600",
    "end": "2782580"
  },
  {
    "text": "It was an outstanding\ntalk, exactly what I was hoping you would do. So again, I can't\nthank you enough",
    "start": "2782580",
    "end": "2791310"
  },
  {
    "text": "and appreciate taking the time\nto tell us about the H100.",
    "start": "2791310",
    "end": "2796320"
  },
  {
    "text": "And now I want to do is find out\nhow to get one for my own use.",
    "start": "2796320",
    "end": "2802260"
  },
  {
    "text": "It's a gorgeous\nmachine actually. And, well, I suspect difficult\nto use in a systems context",
    "start": "2802260",
    "end": "2815610"
  },
  {
    "text": "because it has so many\nknobs to turn and so much local optimization\nthat you can make",
    "start": "2815610",
    "end": "2822960"
  },
  {
    "text": "an enormous amount\nof improvement by doing things very carefully. Well, I think the access\nto, and again, I'm",
    "start": "2822960",
    "end": "2831220"
  },
  {
    "text": "not familiar with the\ndetails, but a lot of these are being deployed in the cloud.",
    "start": "2831220",
    "end": "2837750"
  },
  {
    "text": "And there should be\ncloud access through it, through various avenues.",
    "start": "2837750",
    "end": "2846390"
  },
  {
    "text": "So if we want to use it\nas a general deep learning",
    "start": "2846390",
    "end": "2851730"
  },
  {
    "text": "user or HPC user,\nprobably people are more interested\nabout AI deep learning,",
    "start": "2851730",
    "end": "2858390"
  },
  {
    "text": "they should be able\nto get access to that. As far as buying one of your\nown, I don't think that's--",
    "start": "2858390",
    "end": "2866760"
  },
  {
    "text": " I don't know your financial\nsituation, and what kind",
    "start": "2866760",
    "end": "2872940"
  },
  {
    "text": "of data center sit up you have\nin your basement, but you know, that's a bit more--",
    "start": "2872940",
    "end": "2879060"
  },
  {
    "text": "A little more. A little more. My guess is that\nit takes a village",
    "start": "2879060",
    "end": "2886570"
  },
  {
    "text": "to make enough money to\npay for one of these things as an individual\nstandalone unit,",
    "start": "2886570",
    "end": "2892420"
  },
  {
    "text": "and that's why we\nhave cloud computing. But it is-- it's a\ngorgeous piece of gear.",
    "start": "2892420",
    "end": "2898270"
  },
  {
    "text": "Now, I will say that a lot of\nour things that you run and can run and frameworks that you can\nrun on these big, large systems,",
    "start": "2898270",
    "end": "2909160"
  },
  {
    "text": "they also can run on the gaming\nGPUs that you buy at Fry's.",
    "start": "2909160",
    "end": "2915549"
  },
  {
    "text": "Well, Fry's is dead,\nbut the Best Buy. ",
    "start": "2915550",
    "end": "2922720"
  },
  {
    "text": "So if you want to experiment\nat a very small scale and play around with\na very small scale,",
    "start": "2922720",
    "end": "2928150"
  },
  {
    "text": "that is definitely\navailable to you. And well-- Maybe people will be\nsuppressing the C100 than as--",
    "start": "2928150",
    "end": "2937080"
  },
  {
    "text": "an A100 rather as a\nlower-cost home computer,",
    "start": "2937080",
    "end": "2944290"
  },
  {
    "text": "I don't know but this is\nbecause of the enormous factor",
    "start": "2944290",
    "end": "2949420"
  },
  {
    "text": "of improvement,\njust an amazing-- that's an amazing game, and\nit's almost all entirely",
    "start": "2949420",
    "end": "2958030"
  },
  {
    "text": "architectural, it seems. Well, you get some\ngain from the-- It's from the process, there's\nmore SMs and things like that,",
    "start": "2958030",
    "end": "2966280"
  },
  {
    "text": "but yeah, there's a lot of\narchitectural improvements as well. And I will say there's a lot\nof system-level improvements",
    "start": "2966280",
    "end": "2972250"
  },
  {
    "text": "that you can say as well--\nthe NVLink, the Mellanox switches, and stuff like that.",
    "start": "2972250",
    "end": "2978130"
  },
  {
    "text": "It's not just the GPU from\na system-level perspective, it's a system-level architecture\nthat we need in order",
    "start": "2978130",
    "end": "2983968"
  },
  {
    "text": "to get these gains. It has to be a\nsystem-level design. OK. Well, thank you very much.",
    "start": "2983968",
    "end": "2989953"
  },
  {
    "text": "Again, and I guess with that,\nwe'll close for the quarter. All right, thank you much.",
    "start": "2989953",
    "end": "2996880"
  },
  {
    "text": "All right, well, thanks. Bye bye. Thanks. ",
    "start": "2996880",
    "end": "3005000"
  }
]