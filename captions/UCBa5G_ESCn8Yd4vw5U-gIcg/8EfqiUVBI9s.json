[
  {
    "text": "Hello, welcome to section 3 of EE104. This is about predictors.",
    "start": "4700",
    "end": "11590"
  },
  {
    "text": "So one of the primary tasks in machine learning is data fitting.",
    "start": "20060",
    "end": "26520"
  },
  {
    "text": "We have a variable y and variable x, and we think they're related by some function,",
    "start": "26520",
    "end": "34830"
  },
  {
    "text": "say y- say y is equal to f of x. We think Y is approximately equal to f of x.",
    "start": "34830",
    "end": "42610"
  },
  {
    "text": "Here, we think about X as an independent variable and y is the outcome or the response.",
    "start": "42710",
    "end": "52405"
  },
  {
    "text": "We might call it the target, or the label, or the dependent variable. Uh, very often, we have y is in Rm and x is in Rd.",
    "start": "52405",
    "end": "62035"
  },
  {
    "text": "So these are both vectors. Um, very often, m is 1, and so the outcome is a scalar.",
    "start": "62035",
    "end": "69790"
  },
  {
    "text": "And we say, well, the target is related to the independent variable.",
    "start": "69790",
    "end": "75590"
  },
  {
    "text": "Approximately by y is f of x. And we're going to be told x and we'd like to be able to predict y.",
    "start": "75590",
    "end": "83274"
  },
  {
    "text": "And we don't know what this function f is. And of course, there may not be such a function f. It may",
    "start": "83275",
    "end": "91490"
  },
  {
    "text": "just be that y and x are a bunch of unrelated data, but they're related by some other variable that we don't know.",
    "start": "91490",
    "end": "99159"
  },
  {
    "text": "Or if you put an x in at one time, you'll get a y. You put an x in at a different time, you'll get a different y.",
    "start": "99160",
    "end": "105335"
  },
  {
    "text": "And, um, there's a lot of noise, there's a probabilistic relationship between",
    "start": "105335",
    "end": "111634"
  },
  {
    "text": "y and x rather than a purely deterministic one. Usually, these x's, these variables x- usually,",
    "start": "111635",
    "end": "120720"
  },
  {
    "text": "these variables x are often vectors of features. That is, they are",
    "start": "120720",
    "end": "126530"
  },
  {
    "text": "descriptors of the underlying data rather than the underlying data itself.",
    "start": "126530",
    "end": "131990"
  },
  {
    "text": "So for example, if we have documents, for each document, we might make a histogram of",
    "start": "131990",
    "end": "137660"
  },
  {
    "text": "the word counts and count the numbers of all of the different words there. And that would be x,",
    "start": "137660",
    "end": "144470"
  },
  {
    "text": "that vector of the- of word counts. Er, if we had patient data,",
    "start": "144470",
    "end": "150560"
  },
  {
    "text": "then x might be patient attributes, tests re- results, systems.",
    "start": "150560",
    "end": "155860"
  },
  {
    "text": "Uh, let's start again. Very often, x is a vector of features rather than the raw underlying data.",
    "start": "158510",
    "end": "168480"
  },
  {
    "text": "So for example, if we add a document, then the corresponding x might be",
    "start": "168480",
    "end": "174830"
  },
  {
    "text": "a account of all of the different words in the document, the word-count histogram.",
    "start": "174830",
    "end": "180350"
  },
  {
    "text": "If, uh, we had patient data, then x might be a list of different patient attributes,",
    "start": "180350",
    "end": "187610"
  },
  {
    "text": "a list of test results, perhaps a list of symptoms. If, uh, data consists of customers,",
    "start": "187610",
    "end": "194959"
  },
  {
    "text": "then for each customer, we'll have an x, which would be the purchase history- history of that customer.",
    "start": "194959",
    "end": "200970"
  },
  {
    "text": "We have a- a general framework to construct features from the raw input data.",
    "start": "205180",
    "end": "214715"
  },
  {
    "text": "So the raw input data might be- it might be a vector itself. It might be a word or a document.",
    "start": "214715",
    "end": "221330"
  },
  {
    "text": "It might be an image. It might be video, it might be audio, um, or it could be a list of such attributes.",
    "start": "221330",
    "end": "229709"
  },
  {
    "text": "Um, it could consist of multiple attributes. We're going to, uh, uh,",
    "start": "229710",
    "end": "236084"
  },
  {
    "text": "call that input data u. And we're going to map it under a function Phi to construct x,",
    "start": "236085",
    "end": "243875"
  },
  {
    "text": "which we will call the corresponding feature vector. This function Phi has a name,",
    "start": "243875",
    "end": "249720"
  },
  {
    "text": "it's called the embedding, or the feature function or the feature mapping.",
    "start": "249720",
    "end": "255025"
  },
  {
    "text": "And sometimes Phi is very simple, or sometimes it's extremely complicated.",
    "start": "255025",
    "end": "261055"
  },
  {
    "text": "Um, and, uh, often, we will, uh, take a particular property.",
    "start": "261055",
    "end": "269615"
  },
  {
    "text": "Um, we will make sure- since Phi is going to be a vector, we'll make sure that the first component of phi,",
    "start": "269615",
    "end": "275485"
  },
  {
    "text": "um, is always 1, and so that we would know by x_1 or Phi of u_1.",
    "start": "275485",
    "end": "283685"
  },
  {
    "text": "It's the constant feature, um, which, uh, we'll explain the reason for that in just a second.",
    "start": "283685",
    "end": "292580"
  },
  {
    "text": "Similarly, we embed or we construct",
    "start": "293630",
    "end": "298915"
  },
  {
    "text": "features for the output data V and we will call those y,",
    "start": "298915",
    "end": "305455"
  },
  {
    "text": "and we will have y is equal to Psi of v. So our data comes in pairs.",
    "start": "305455",
    "end": "312645"
  },
  {
    "text": "Ui's and vi's the ith data element is a pair ui, vi.",
    "start": "312645",
    "end": "319224"
  },
  {
    "text": "We map those to a pair xi, yi.",
    "start": "319225",
    "end": "324050"
  },
  {
    "text": "And we have n data points, x_1 to x_n and y_1 to y_n.",
    "start": "328340",
    "end": "334680"
  },
  {
    "text": "And once we've embedded them, once we've constructed features, then we no longer need to look directly at u and v. Instead,",
    "start": "334680",
    "end": "343900"
  },
  {
    "text": "we can focus our attention on x and y. So we will have nd dimensional vectors, x_1 through x_n,",
    "start": "343900",
    "end": "353160"
  },
  {
    "text": "and nm dimensional vectors y_1 through y_n- y_1 through YN-.",
    "start": "353160",
    "end": "361960"
  },
  {
    "text": "And so we'll refer to the pair xi, yi as the ith data-pair observation.",
    "start": "362210",
    "end": "369435"
  },
  {
    "text": "A particularly, uh, evocative term is to refer to as the ith example from which we wish to learn.",
    "start": "369435",
    "end": "377760"
  },
  {
    "text": "And collectively, we would call the entire set of x's and y's a dataset.",
    "start": "377760",
    "end": "383820"
  },
  {
    "text": "Um, we're also- so that's our, um,",
    "start": "384190",
    "end": "389195"
  },
  {
    "text": "fundamental data that we're going to use in order to construct",
    "start": "389195",
    "end": "394400"
  },
  {
    "text": "some fitted model for the relationship between x and y.",
    "start": "394400",
    "end": "401824"
  },
  {
    "text": "We also might have prior knowledge about what f might look like.",
    "start": "401825",
    "end": "406885"
  },
  {
    "text": "For example, if f is a- a function, we might say f is,",
    "start": "406885",
    "end": "412574"
  },
  {
    "text": "um, smooth or continuous, which means that if x and x Tilde are two vectors that are close to each other,",
    "start": "412575",
    "end": "420995"
  },
  {
    "text": "then f of x and f of x Tilde should also be close to each other.",
    "start": "420995",
    "end": "426479"
  },
  {
    "text": "Another thing we might no prior knowledge about f is that we might know that y is always non-negative.",
    "start": "427030",
    "end": "434750"
  },
  {
    "text": "And so we would like to ensure that property of f holds in our model.",
    "start": "434750",
    "end": "442740"
  },
  {
    "text": "We're going to learn from x's and y's. And we're going to see a whole bunch of y values.",
    "start": "442740",
    "end": "448355"
  },
  {
    "text": "And no matter what y- y values we- we see, we would like to ensure that the model that our learning algorithm",
    "start": "448355",
    "end": "455500"
  },
  {
    "text": "produces has the property that f of any x is always non-negative.",
    "start": "455500",
    "end": "461300"
  },
  {
    "text": "And there are many other such prior knowledge. Such- there- and there are many other examples of prior knowledge that one would have.",
    "start": "461390",
    "end": "470810"
  },
  {
    "text": "So the thing we're going to construct is called a predictor. It's, uh, a model that takes an x",
    "start": "474200",
    "end": "481930"
  },
  {
    "text": "and gives you a prediction for what y would be at that x. Uh, we denote that by G. It's a function that takes",
    "start": "481930",
    "end": "490030"
  },
  {
    "text": "vectors in R_d x's and gives us vectors in R_m y's.",
    "start": "490030",
    "end": "497420"
  },
  {
    "text": "The feature vector x, the prediction, we would denote by y-hat.",
    "start": "498890",
    "end": "505350"
  },
  {
    "text": "Y-hat is g of x. And the predictor G is chosen on the basis of two things.",
    "start": "505350",
    "end": "513919"
  },
  {
    "text": "The- the data that we've seen and the prior knowledge that we have. And that means that we can,",
    "start": "513920",
    "end": "520279"
  },
  {
    "text": "in terms of the raw data, construct a prediction using a- a new rule data record.",
    "start": "520280",
    "end": "528160"
  },
  {
    "text": "So if you give me a new u that I've never seen before, I can map it under Phi to embed it and give me an x,",
    "start": "528160",
    "end": "537650"
  },
  {
    "text": "construct a feature vector corresponding to that u. Then I can take that x and feed it into my predictor,",
    "start": "537650",
    "end": "544370"
  },
  {
    "text": "and to construct g of x. And that's an estimate for y, a prediction for y.",
    "start": "544370",
    "end": "550644"
  },
  {
    "text": "And then I can unembed that by applying the inverse",
    "start": "550645",
    "end": "555725"
  },
  {
    "text": "of the feature map Psi to that y-hat to give me an estimate of v,",
    "start": "555725",
    "end": "562730"
  },
  {
    "text": "which we'll call v-hat. Um, and that means that I can test my-",
    "start": "562730",
    "end": "570060"
  },
  {
    "text": "the performance of my predictor in the original- in terms of the original data,",
    "start": "570060",
    "end": "575390"
  },
  {
    "text": "rather than in terms of my x's and y's. And sometimes, Psi is not invertible and then there is",
    "start": "575390",
    "end": "582380"
  },
  {
    "text": "a slight variation of this formula that we will come to.",
    "start": "582380",
    "end": "587160"
  },
  {
    "text": "Of course, you can also evaluate how well does this predictor do on the data.",
    "start": "588670",
    "end": "594745"
  },
  {
    "text": "And we can take, uh, a data pair, the ith data pair, it's xi, yi.",
    "start": "594745",
    "end": "600705"
  },
  {
    "text": "You'll feed xi into g, we'll get a y-hat. We will call y-hat i.",
    "start": "600705",
    "end": "607515"
  },
  {
    "text": "And, uh, we would like y-hat i to be close to yi. And that means that the predictor does well on the data.",
    "start": "607515",
    "end": "615755"
  },
  {
    "text": "That's a very reasonable thing to acquire of the predictor. Of course, our real goal is not to have the predictor do well on the data,",
    "start": "615755",
    "end": "626255"
  },
  {
    "text": "but to have predictor- the predictor do well on potential data that we've not yet seen.",
    "start": "626255",
    "end": "632870"
  },
  {
    "text": "When somebody gives us some new x or some new u,",
    "start": "632870",
    "end": "638890"
  },
  {
    "text": "they could quite reasonably ask, well, what did you learn from all that earlier data that we gave you?",
    "start": "638890",
    "end": "644870"
  },
  {
    "text": "And the answer should be, well, from all that data, I learned that these kinds of x's give rise to these kinds of y's,",
    "start": "644870",
    "end": "652430"
  },
  {
    "text": "and let me predict a specific y that would be generated by the x that you just gave me.",
    "start": "652430",
    "end": "659640"
  },
  {
    "text": "And when we're working with predictors, we typically don't just say, well, here's the predictor. We typically have a parameterized form for the predictor.",
    "start": "664720",
    "end": "673010"
  },
  {
    "text": "So the predictor is a function of two things, is a function of x, and it's a function of some parameters Theta.",
    "start": "673010",
    "end": "680500"
  },
  {
    "text": "So we'd have y hat is g of x and Theta. Sometimes we write this as a subscript.",
    "start": "680500",
    "end": "688110"
  },
  {
    "text": "So we'll write y hat is g subscript Theta of x, and that just makes it easy to refer to g-",
    "start": "688110",
    "end": "693540"
  },
  {
    "text": "g Theta as a function which takes an x and gives us a y.",
    "start": "693540",
    "end": "699464"
  },
  {
    "text": "And what this does for us is it specifies a form, a structure allowed predictors or predictors that we like.",
    "start": "699465",
    "end": "708930"
  },
  {
    "text": "And we're only going to pick predictors that correspond to a particular Theta.",
    "start": "708930",
    "end": "714180"
  },
  {
    "text": "And our job becomes instead of choosing an arbitrary function that maps x to y,",
    "start": "714180",
    "end": "720135"
  },
  {
    "text": "choose a Theta, and then evaluate g of x Theta.",
    "start": "720135",
    "end": "726550"
  },
  {
    "text": "So Theta is a parameter, it- it's usually a vector.",
    "start": "727040",
    "end": "734139"
  },
  {
    "text": "It's a parameter for the prediction model. Often Theta is in some Euclidean space.",
    "start": "735050",
    "end": "742364"
  },
  {
    "text": "Here we've written RP. Sometimes it's a matrix, we'll see that.",
    "start": "742364",
    "end": "747420"
  },
  {
    "text": "Um, and sometimes it's- it's a list of there's more than one parameter, more than one parameter vector,",
    "start": "747420",
    "end": "753449"
  },
  {
    "text": "and more than one parameter matrix. Um, choosing a particular Theta that's called tuning or training or fitting the model.",
    "start": "753450",
    "end": "765940"
  },
  {
    "text": "And the learning algorithm is a recipe for choosing Theta given data.",
    "start": "766160",
    "end": "771720"
  },
  {
    "text": "So for example, we might have a linear regression model that says that",
    "start": "771720",
    "end": "778230"
  },
  {
    "text": "y hat is Theta 1 x1 plus Theta 2 x2 all the way up to Theta d times xd.",
    "start": "778230",
    "end": "786644"
  },
  {
    "text": "Y hat is this linear combination of the Thetas,",
    "start": "786645",
    "end": "792615"
  },
  {
    "text": "or equivalently a linear combination of the Xs. And, uh, our job is to pick the Thetas so that,",
    "start": "792615",
    "end": "804075"
  },
  {
    "text": "uh, this is a good predictor. We've already seen in previous classes on linear algebra that you can",
    "start": "804075",
    "end": "812565"
  },
  {
    "text": "fit such a linear regression model using least squares. And that's picking Theta to minimize the mean square error.",
    "start": "812565",
    "end": "822074"
  },
  {
    "text": "Of course, there are lots of other methods for picking- for doing linear regression for picking the Thetas even in a linear regression model.",
    "start": "822075",
    "end": "829110"
  },
  {
    "text": "And we will talk about some of those in this class.",
    "start": "829110",
    "end": "832360"
  },
  {
    "text": "I wanna talk about, uh, a special class of predictors called nearest neighbor predictors.",
    "start": "835130",
    "end": "841720"
  },
  {
    "text": "They work as follows. We're given a dataset, x_1 through x_n,",
    "start": "843770",
    "end": "849720"
  },
  {
    "text": "y_1 through y_n and the predictor says the following.",
    "start": "849720",
    "end": "855464"
  },
  {
    "text": "You've got some new x and you'd like to predict the corresponding y hat. The way you do it is out of all of the data that you've gotten,",
    "start": "855465",
    "end": "863805"
  },
  {
    "text": "you find the- the x_i, that is the nearest to x.",
    "start": "863805",
    "end": "869170"
  },
  {
    "text": "And then your prediction for the corresponding y is simply y_i.",
    "start": "869180",
    "end": "875820"
  },
  {
    "text": "And that you define to be g of x at that x.",
    "start": "875820",
    "end": "883450"
  },
  {
    "text": "And so this is extremely intuitive. If you've got some new piece of data and you want to",
    "start": "884450",
    "end": "892410"
  },
  {
    "text": "make a prediction about what y is at that data point at that x. Well, why not- why not look for the closest example that matches it and say, well, uh,",
    "start": "892410",
    "end": "906690"
  },
  {
    "text": "that's probably the- it's closest example we've got so let's predict that,",
    "start": "906690",
    "end": "913395"
  },
  {
    "text": "uh, the y we get is the y we got in that previous example.",
    "start": "913395",
    "end": "918370"
  },
  {
    "text": "Um, it really is of course a parameterized, uh, predictor.",
    "start": "918890",
    "end": "926990"
  },
  {
    "text": "Here the parameter is a bit interesting because the parameter's a full dataset, you can think about Theta as x_1 through x_n and y_1 through y_n, the entire dataset.",
    "start": "926990",
    "end": "937555"
  },
  {
    "text": "And we don't have to choose the parameter. The parameter is given to us with the data, it is the data.",
    "start": "937555",
    "end": "943995"
  },
  {
    "text": "So training is easy. There isn't any training. There's no computation to be done.",
    "start": "943995",
    "end": "949380"
  },
  {
    "text": "All we have to do is keep track of all the data we've seen. And whenever we get a new query point, a new x,",
    "start": "949380",
    "end": "958110"
  },
  {
    "text": "we just look for the closest x in the dataset and return the corresponding y.",
    "start": "958110",
    "end": "963760"
  },
  {
    "text": "This means that g is going to be a piecewise constant function of x.",
    "start": "964300",
    "end": "970000"
  },
  {
    "text": "Because when x is closer to x_i than all the other Xs, well in g of x is just y_i.",
    "start": "970000",
    "end": "976380"
  },
  {
    "text": "We can see that on this plot right here. Uh, let me see if I can highlight some points on this.",
    "start": "976380",
    "end": "985090"
  },
  {
    "text": "So these are data points.",
    "start": "998720",
    "end": "1002129"
  },
  {
    "text": "And the orange line, which I will highlight here in black.",
    "start": "1004780",
    "end": "1012500"
  },
  {
    "text": "This, the way it goes about here is the predictor.",
    "start": "1012500",
    "end": "1020870"
  },
  {
    "text": "And it tells us that for example, if we receive a, uh,",
    "start": "1020870",
    "end": "1028550"
  },
  {
    "text": "value of x of 0.2, then we should predict the corresponding value, y of 0.47.",
    "start": "1028550",
    "end": "1041645"
  },
  {
    "text": "And so even though we've only got data points at specific points, we've fitted a curve,",
    "start": "1041645",
    "end": "1048020"
  },
  {
    "text": "not a curve really, a piecewise constant- piecewise constant function through those points.",
    "start": "1048020",
    "end": "1054934"
  },
  {
    "text": "Something to observe is that the function is- has discontinuities.",
    "start": "1054935",
    "end": "1062220"
  },
  {
    "text": "Let's look at some of those discontinuities. So here's one, here's another, here's another.",
    "start": "1062260",
    "end": "1072215"
  },
  {
    "text": "And these discontinuities are exactly halfway between two adjacent data points.",
    "start": "1072215",
    "end": "1078544"
  },
  {
    "text": "So as we vary x when we're here, we are, our closest data point is this red point here.",
    "start": "1078545",
    "end": "1087275"
  },
  {
    "text": "And then as we increase x, we suddenly switch to the closest data point being this data point x right here.",
    "start": "1087275",
    "end": "1095645"
  },
  {
    "text": "And as a result we- we switch our prediction also from predicting the y corresponding to this data point",
    "start": "1095645",
    "end": "1102245"
  },
  {
    "text": "to predicting the y corresponding to this data point.",
    "start": "1102245",
    "end": "1105690"
  },
  {
    "text": "You can do this in more than one dimension. It doesn't have to be, er,",
    "start": "1113360",
    "end": "1118800"
  },
  {
    "text": "a one dimensional x here. One can do this when x is D dimensional as well. Here's a two dimensional case.",
    "start": "1118800",
    "end": "1125730"
  },
  {
    "text": "The two dimensional case here we have, ah, data points which have,",
    "start": "1125730",
    "end": "1132044"
  },
  {
    "text": "er - which are right here. I'll - I'll highlight these blue points. And - um, and now associated with each data point,",
    "start": "1132045",
    "end": "1146730"
  },
  {
    "text": "associ- associate with each x there is a region.",
    "start": "1146730",
    "end": "1152025"
  },
  {
    "text": "So for example, for this data point right here, there's a region which is this region right here,",
    "start": "1152025",
    "end": "1159730"
  },
  {
    "text": "which is a polyhedron. It's got straight boundaries. And this is the set of data points for which the corresponding - this is the set of",
    "start": "1161660",
    "end": "1170655"
  },
  {
    "text": "x's for which the corresponding closest data point is this one.",
    "start": "1170655",
    "end": "1177040"
  },
  {
    "text": "If I'm here, my closest data point is this one.",
    "start": "1177590",
    "end": "1183225"
  },
  {
    "text": "Um, If I move across this boundary, suddenly I switch to the case where my closest data point becomes that one over there.",
    "start": "1183225",
    "end": "1194135"
  },
  {
    "text": "So each data point has a corresponding region. And those regions are called the Voronoi regions associated with the data.",
    "start": "1194135",
    "end": "1207240"
  },
  {
    "text": "So the function, the predictor is piecewise constant. It's constant on Voronoi regions.",
    "start": "1207800",
    "end": "1214934"
  },
  {
    "text": "And here's a three dimensional plot of it. This is x1, this is x2,",
    "start": "1214935",
    "end": "1220695"
  },
  {
    "text": "and this is the prediction y or y hat. And, um, here we have,",
    "start": "1220695",
    "end": "1228960"
  },
  {
    "text": "er, again, we have data points here.",
    "start": "1228960",
    "end": "1233860"
  },
  {
    "text": "And then this is the function values according on each data point.",
    "start": "1236000",
    "end": "1241590"
  },
  {
    "text": "So right here on this Voronoi region, right here, that Voronoi region right there.",
    "start": "1241590",
    "end": "1250410"
  },
  {
    "text": "That's the Voronoi region corresponding to this data point. The value at that data point is y is 1,",
    "start": "1250410",
    "end": "1256755"
  },
  {
    "text": "y i is minus 1, I'm sorry. And so if we are given a new x,",
    "start": "1256755",
    "end": "1263880"
  },
  {
    "text": "which is anywhere in that region, then we will predict that y hat is also minus 1.",
    "start": "1263880",
    "end": "1271830"
  },
  {
    "text": "So now we've seen the nearest neighbor predictor.",
    "start": "1272690",
    "end": "1277724"
  },
  {
    "text": "We can extend that idea and talk about the k nearest neighbor predictor. So the idea here is that you pick a number, k, an integer.",
    "start": "1277724",
    "end": "1287460"
  },
  {
    "text": "And then you say, well, you give me an x and instead of just looking at the nearest neighbor to x,",
    "start": "1287460",
    "end": "1293985"
  },
  {
    "text": "I'm going to look at the k nearest neighbors, x i1 through x i k,",
    "start": "1293985",
    "end": "1300315"
  },
  {
    "text": "amongst the given data. And what we're going to predict is we're going to predict y hat to- as follows.",
    "start": "1300315",
    "end": "1308880"
  },
  {
    "text": "We're going to take those k nearest neighbors. The nearest x i's.",
    "start": "1308880",
    "end": "1313920"
  },
  {
    "text": "Each one of them has a corresponding y i, until we wrote k corresponding y i's.",
    "start": "1313920",
    "end": "1319140"
  },
  {
    "text": "And we're going to construct the average of those k y i's. And that's going to be y hat.",
    "start": "1319140",
    "end": "1325029"
  },
  {
    "text": "So this is a generalization of the nearest neighbor predictor.",
    "start": "1326420",
    "end": "1331920"
  },
  {
    "text": "Um, and, ah, it's certainly very useful,",
    "start": "1331920",
    "end": "1338460"
  },
  {
    "text": "it's very well used. It - you might see that, er, for example,",
    "start": "1338460",
    "end": "1343830"
  },
  {
    "text": "makes you less sensitive to, ah, noise in the data.",
    "start": "1343830",
    "end": "1349545"
  },
  {
    "text": "If the data is very variable then the k nearest neighbor predictor is going to perform some averaging.",
    "start": "1349545",
    "end": "1356565"
  },
  {
    "text": "It's going to smooth out the data. It's going to remove some of the noise. And there are many ways you might extend this idea.",
    "start": "1356565",
    "end": "1365970"
  },
  {
    "text": "You might use a weighted average to form y hat or you might pre-process the data.",
    "start": "1365970",
    "end": "1372284"
  },
  {
    "text": "You might say, well, I'm only going to look at, ah, the data instead of looking at it as a collection of data points,",
    "start": "1372285",
    "end": "1381455"
  },
  {
    "text": "I'm just going to look at it as a collection of clustered data points, collection of clusters, of data points.",
    "start": "1381455",
    "end": "1389045"
  },
  {
    "text": "And then instead of picking the nearest neighbor, I might pick the newest cluster and a bunch of different things you could do.",
    "start": "1389045",
    "end": "1396540"
  },
  {
    "text": "Here's how you compute the, ah, k nearest neighbor predictor. This is Julia.",
    "start": "1397880",
    "end": "1404070"
  },
  {
    "text": "Um, so we have a function. It takes an X, a Y, and little x,",
    "start": "1404070",
    "end": "1410700"
  },
  {
    "text": "and a k. So here this matrix x,",
    "start": "1410700",
    "end": "1418014"
  },
  {
    "text": "it is an n by d matrix. The ith row of that is the ith x point,",
    "start": "1418015",
    "end": "1428970"
  },
  {
    "text": "it's x var -variable corresponding to the ith data record.",
    "start": "1428970",
    "end": "1434975"
  },
  {
    "text": "Y here is, ah, also, ah, an n by m matrix.",
    "start": "1434975",
    "end": "1446250"
  },
  {
    "text": "It's, ah the corresponding targets. The ith row of Y is the target variable corresponding to the ith row of x.",
    "start": "1446250",
    "end": "1457059"
  },
  {
    "text": "Here little x is, um, the query point.",
    "start": "1457310",
    "end": "1463815"
  },
  {
    "text": "We're given a new x at which we want to predict a y. That's little x.",
    "start": "1463815",
    "end": "1469545"
  },
  {
    "text": "And k here is the k nearest neighbors, it's how many neighbors we're going to consider.",
    "start": "1469545",
    "end": "1475750"
  },
  {
    "text": "Er, so this first line of the code here just gives us the n, the number of data points.",
    "start": "1477200",
    "end": "1483899"
  },
  {
    "text": "It looks at the number that returns the number of rows of the matrix x.",
    "start": "1483900",
    "end": "1489640"
  },
  {
    "text": "Here, we look at each row and subtract it from the query point.",
    "start": "1489950",
    "end": "1500534"
  },
  {
    "text": "Look at the sum of the squares of the entries of that vector.",
    "start": "1500535",
    "end": "1506595"
  },
  {
    "text": "And fair, we make a list of all the sums of the squares for each of the different data points.",
    "start": "1506595",
    "end": "1513930"
  },
  {
    "text": "Um, for i's 1 to n. And that's in dists_sq. That's a list of all of those different distances.",
    "start": "1513930",
    "end": "1522345"
  },
  {
    "text": "The ith element in that list is the distance from x i to x.",
    "start": "1522345",
    "end": "1528970"
  },
  {
    "text": "And then this function, sortperm, it gives us the indexes - the indices that point to the sorted,",
    "start": "1529250",
    "end": "1539220"
  },
  {
    "text": "ah, entries of dists_sq. So the first entry of nearest neighbor idx's, is an integer.",
    "start": "1539220",
    "end": "1549840"
  },
  {
    "text": "And if we look at the, um, ah, so - so if the nearest neighbor index is,",
    "start": "1549840",
    "end": "1558240"
  },
  {
    "text": "ah - if the first entry of nearest neighbor index is a 7,",
    "start": "1558240",
    "end": "1563610"
  },
  {
    "text": "then dists_sq 7 will be the smallest entry in dists_sq.",
    "start": "1563610",
    "end": "1570255"
  },
  {
    "text": "And similarly, in second entry in",
    "start": "1570255",
    "end": "1575310"
  },
  {
    "text": "nearest neighbor indexes points to the second smallest entry in dists_sq.",
    "start": "1575310",
    "end": "1582885"
  },
  {
    "text": "So if we look at the first k by picking out the first k entries as returned by sortperm,",
    "start": "1582885",
    "end": "1590355"
  },
  {
    "text": "that gives us k numbers that point to the corresponding closest data records to x.",
    "start": "1590355",
    "end": "1601590"
  },
  {
    "text": "And then what we do is we construct, the corresponding - take the corresponding y's and we",
    "start": "1601590",
    "end": "1607710"
  },
  {
    "text": "take their average and we return that as y hat.",
    "start": "1607710",
    "end": "1612730"
  },
  {
    "text": "Um, this is, of course, if you were to write this in Python or MATLAB,",
    "start": "1613370",
    "end": "1618809"
  },
  {
    "text": "it would look very similar. The nice thing about this code is that really,",
    "start": "1618810",
    "end": "1624524"
  },
  {
    "text": "it looks very much like the mathematics that one might write to describe this algorithm.",
    "start": "1624525",
    "end": "1632190"
  },
  {
    "text": "And that's a nice feature that, er, it's worth aiming at throughout this class,",
    "start": "1632190",
    "end": "1638895"
  },
  {
    "text": "is that if you've got a short piece of mathematics that describes something you are trying to code,",
    "start": "1638895",
    "end": "1645285"
  },
  {
    "text": "then the code should kind of correspond line by line to the mathematics. And that way it's easy to debug and, ah, easy to read.",
    "start": "1645285",
    "end": "1657090"
  },
  {
    "text": "And so this is the case where k is 2 on the same data that we had before.",
    "start": "1658190",
    "end": "1664200"
  },
  {
    "text": "And now this is interesting in that regions. Er, let me mark a region.",
    "start": "1664200",
    "end": "1670105"
  },
  {
    "text": "So for example, this region right here.",
    "start": "1670105",
    "end": "1673830"
  },
  {
    "text": "Regions are still - still have polyhedral, still have straight line boundaries. They still polyhedra, but they now,",
    "start": "1676210",
    "end": "1683460"
  },
  {
    "text": "rather than corresponding to a single data point, they correspond to k data points.",
    "start": "1683460",
    "end": "1689160"
  },
  {
    "text": "In this case, k is 2 . So every point in this region right here,",
    "start": "1689160",
    "end": "1696960"
  },
  {
    "text": "its two closest data points,",
    "start": "1696960",
    "end": "1702615"
  },
  {
    "text": "this one and this one. And you can see that if I'm in this region and I cross over into this region,",
    "start": "1702615",
    "end": "1713085"
  },
  {
    "text": "then I'm going to switch from having this one and this one, that's",
    "start": "1713085",
    "end": "1719460"
  },
  {
    "text": "my closest data points to having instead this one and this one, that's my closest two data points.",
    "start": "1719460",
    "end": "1728130"
  },
  {
    "text": "So now regions are associated with pairs of data points. There are more regions because there are more pairs of",
    "start": "1728130",
    "end": "1735210"
  },
  {
    "text": "data points than there are individual data points, although not every pair of data points has a non empty region.",
    "start": "1735210",
    "end": "1743409"
  },
  {
    "text": "And we can plot the corresponding k nearest neighbors estimate.",
    "start": "1743510",
    "end": "1749400"
  },
  {
    "text": "Here it is on the right hand side. Um, there are more regions, but also the - the function itself is somewhat flatter, has less variability.",
    "start": "1749400",
    "end": "1760685"
  },
  {
    "text": "Um, and that's something we expect because our k nearest neighbors, our predictor does some averaging.",
    "start": "1760685",
    "end": "1772725"
  },
  {
    "text": "And so instead it's going to smooth out the function. It's still a of course piecewise constant.",
    "start": "1772725",
    "end": "1781749"
  },
  {
    "text": "And instead what we can do is another variant of the k nearest neighbors. This is the soft nearest neighbor predictor.",
    "start": "1786670",
    "end": "1793835"
  },
  {
    "text": "And what we do here is we take, uh, a weighted average of the measured y,",
    "start": "1793835",
    "end": "1800284"
  },
  {
    "text": "but the weights that we use aren't fixed, they depend on x.",
    "start": "1800285",
    "end": "1805295"
  },
  {
    "text": "And so here's a very common choice of weights, uh, right here.",
    "start": "1805295",
    "end": "1810830"
  },
  {
    "text": "Uh, these weights are interesting. Uh, one thing you might notice is that, uh, uh,",
    "start": "1810830",
    "end": "1818419"
  },
  {
    "text": "if I sum thereof from i is 1  up to n, then w_i is equal to the sum of i of w_i is equal to 1.",
    "start": "1818420",
    "end": "1827015"
  },
  {
    "text": "Let's write that down, uh, perhaps you know.",
    "start": "1827015",
    "end": "1831720"
  },
  {
    "text": "So-",
    "start": "1835750",
    "end": "1845945"
  },
  {
    "text": "so another thing to notice is that there's a parameter here. The parameter is Rho.",
    "start": "1845945",
    "end": "1852260"
  },
  {
    "text": "Uh, it's- it's- it has the, um, uh, uh, dimensions, or the scaling like a length.",
    "start": "1852260",
    "end": "1859325"
  },
  {
    "text": "So in particular, uh, if you look here in the numerator,",
    "start": "1859325",
    "end": "1864760"
  },
  {
    "text": "I put the norm of x minus x_i squared divided by Rho squared. Uh, so when Rho is very small,",
    "start": "1864760",
    "end": "1874350"
  },
  {
    "text": "well, 1 over Rho squared is very large. And, uh, so e to the minus something divided by Rho",
    "start": "1874350",
    "end": "1883550"
  },
  {
    "text": "squared is going to be extremely small.",
    "start": "1883550",
    "end": "1889310"
  },
  {
    "text": "Unless where- unless there's something in the numerator, unless the x minus x_i norm squared is also very small,",
    "start": "1889310",
    "end": "1896000"
  },
  {
    "text": "and so the similar size to- to Rho squared. And so what that means is that,",
    "start": "1896000",
    "end": "1902540"
  },
  {
    "text": "um, when Rho is very small, this quantity is going to be 1 when x is close to x_i,",
    "start": "1902540",
    "end": "1911180"
  },
  {
    "text": "and 0 almost everywhere else.",
    "start": "1911180",
    "end": "1916200"
  },
  {
    "text": "Um, and that means that this weighted sum that I'm going to get right here is going to be the nearest,",
    "start": "1916810",
    "end": "1925804"
  },
  {
    "text": "the value of y at the nearest point x_i,",
    "start": "1925804",
    "end": "1931640"
  },
  {
    "text": "the nearest i, the- the- the, uh, the nearest data point.",
    "start": "1931640",
    "end": "1937370"
  },
  {
    "text": "Um, so this reverts to the nearest neighbor predictor when Rho is small. When Rho is large, it becomes a little different.",
    "start": "1937370",
    "end": "1945600"
  },
  {
    "text": "Let's look at what it becomes. So I won't go through this code, but this code is- because it's very similar to the previous code.",
    "start": "1946390",
    "end": "1954485"
  },
  {
    "text": "Um, it, uh, uh just explicitly follows the mathematics again.",
    "start": "1954485",
    "end": "1960000"
  },
  {
    "text": "So here the- here is the, uh, uh, this- the, uh, soft nearest neighbors predictor.",
    "start": "1965110",
    "end": "1971720"
  },
  {
    "text": "Uh, this is the case where Rho is 2 on the left here. And you can see it's really rather smooth.",
    "start": "1971720",
    "end": "1978770"
  },
  {
    "text": "Uh, when Rho is 1, it's becoming, uh, uh,",
    "start": "1978770",
    "end": "1984889"
  },
  {
    "text": "a little bit more similar to the, uh, uh, the nearest neighbor predictor.",
    "start": "1984890",
    "end": "1993270"
  },
  {
    "text": "Here, we can see when Rho is 0.5. Over here, we- we can see something that's really quite",
    "start": "1993460",
    "end": "2000549"
  },
  {
    "text": "close to the- the nearest neighbor predictor. And, uh, and the nearest neighbor predictor is here on the left,",
    "start": "2000550",
    "end": "2008680"
  },
  {
    "text": "and we can really see that there's a correspondence between those two. So this is a nice way of smoothing out the,",
    "start": "2008680",
    "end": "2017304"
  },
  {
    "text": "uh, the nearest neighbor predictor. And it could- instead of having a discontinuous piece-wise constant function,",
    "start": "2017305",
    "end": "2026440"
  },
  {
    "text": "one has, uh, a smooth function. And we can make it and as smooth as we'd like.",
    "start": "2026440",
    "end": "2034160"
  },
  {
    "text": "Okay. So let's turn now to another class of predictors, the linear predictor. Um, here g has the form g of x. Theta is Theta transpose multiplied by x.",
    "start": "2038430",
    "end": "2050655"
  },
  {
    "text": "So when m is 1, that means y is a scalar. The parameter Theta is a vector in R_d,",
    "start": "2050655",
    "end": "2057105"
  },
  {
    "text": "and Theta transpose x just returns for us a scalar. When m is greater than 1, uh,",
    "start": "2057105",
    "end": "2063444"
  },
  {
    "text": "Theta here is a matrix which is d by m. And so Theta transpose times x is gonna return for us an m-dimensional vector,",
    "start": "2063445",
    "end": "2072655"
  },
  {
    "text": "the same size as y. Um, this is also called a linear regression model.",
    "start": "2072655",
    "end": "2079210"
  },
  {
    "text": "And, uh, y hat, which is, uh,",
    "start": "2079210",
    "end": "2086214"
  },
  {
    "text": "our g of x is Theta_1 times x_1 plus Theta_2 times x_2,",
    "start": "2086215",
    "end": "2091409"
  },
  {
    "text": "all the way up to Theta_d times x_d, which is a linear combination of the Theta_i's with coefficients x_1 through x_d.",
    "start": "2091410",
    "end": "2099590"
  },
  {
    "text": "Here, if, uh, m is 1, then Theta_i is just the ith entry of Theta.",
    "start": "2099590",
    "end": "2106974"
  },
  {
    "text": "If m is greater than one, then Theta_i transpose is the ith Rho of Theta.",
    "start": "2106975",
    "end": "2114670"
  },
  {
    "text": "And, uh, so each of the Theta_i's has, uh, uh, uh, a dimension same as y.",
    "start": "2114670",
    "end": "2124370"
  },
  {
    "text": "So we can, uh, yeah, interpret this. Um, uh, suppose y is scalar just for the,",
    "start": "2129720",
    "end": "2137440"
  },
  {
    "text": "uh, to make the, uh, explanation simple. You can, of course, extend all of these ideas when y is a vector.",
    "start": "2137440",
    "end": "2146140"
  },
  {
    "text": "Well, then the- the linear predictor has the form y hat, which is g of x, is Theta_1 times x_1 plus Theta_2 times x_2,",
    "start": "2146140",
    "end": "2153775"
  },
  {
    "text": "all the way up to to Theta_d times x_d. And that means that, uh, we can interpret the Thetas very directly.",
    "start": "2153775",
    "end": "2161050"
  },
  {
    "text": "So, uh, Theta, um,",
    "start": "2161050",
    "end": "2166330"
  },
  {
    "text": "Theta_3 is the amount that their prediction y hat increases when x_3 increases by 1.",
    "start": "2166330",
    "end": "2173890"
  },
  {
    "text": "Uh, if you happen to be in the case where your x_3 is a Boolean,",
    "start": "2173890",
    "end": "2181119"
  },
  {
    "text": "so then it takes value 0 or 1, then, uh, Theta_3 is going to tell you how much y hat is affected when x_3 turns on or turns off.",
    "start": "2181120",
    "end": "2194420"
  },
  {
    "text": "Uh, if Theta_7 is 0, well, that tells us something also, it tells us that t he prediction does not depend on x_7.",
    "start": "2195420",
    "end": "2203990"
  },
  {
    "text": "Uh, and in general, if Theta is small as a vector, it means that the prediction is insensitive to changes in x.",
    "start": "2205710",
    "end": "2213670"
  },
  {
    "text": "And there's a very little, uh, nice little one line computation here that illustrates that.",
    "start": "2213670",
    "end": "2220330"
  },
  {
    "text": "Um, here, we're looking at the- the absolute value of the difference between g of x and g of x tilde.",
    "start": "2220330",
    "end": "2229585"
  },
  {
    "text": "We've got two different x's, and, um, well,",
    "start": "2229585",
    "end": "2234685"
  },
  {
    "text": "g of x, Theta transpose x, and, uh, g of x tilde is Theta transpose x tilde.",
    "start": "2234685",
    "end": "2241630"
  },
  {
    "text": "And, um, so gathering together the x's,",
    "start": "2241630",
    "end": "2246970"
  },
  {
    "text": "we get that- the- that's equal to Theta transpose x minus x tilde. And now we can use the Cauchy-Schwarz inequality.",
    "start": "2246970",
    "end": "2255130"
  },
  {
    "text": "Remember what that is? Let me just write it down.",
    "start": "2255130",
    "end": "2258530"
  },
  {
    "text": "That says that for two vectors, say p and q, that the absolute value of p transpose q is less than or equal",
    "start": "2266070",
    "end": "2276520"
  },
  {
    "text": "to the- the norm of the vector p multiplied by the norm of the vector q.",
    "start": "2276520",
    "end": "2283675"
  },
  {
    "text": "And, uh, of course, we know that p transpose q is the norm of p",
    "start": "2283675",
    "end": "2289600"
  },
  {
    "text": "multiplied by the norm of q multiplied by the cosine of the angle between p and q, and cosine of t minus 1 and 1.",
    "start": "2289600",
    "end": "2297175"
  },
  {
    "text": "So that means that this quantity, the absolute value of this scalar product,",
    "start": "2297175",
    "end": "2302560"
  },
  {
    "text": "is less than or equal to the norm of Theta times the norm of x minus x tilde. And so if Theta is very small,",
    "start": "2302560",
    "end": "2310660"
  },
  {
    "text": "so if the norm of Theta is small, then the difference between g of x and g of x tilde can't be very big.",
    "start": "2310660",
    "end": "2320380"
  },
  {
    "text": "There's- it's bounded by that small quantity multiplied by the norm of x minus x tilde.",
    "start": "2320380",
    "end": "2326454"
  },
  {
    "text": "So by making Theta small, you make your predictor insensitive to the data.",
    "start": "2326455",
    "end": "2334330"
  },
  {
    "text": "And it doesn't sound like a good idea, but as we will  see in this class, that turns out to be an extremely important idea that we will- we visit again and again.",
    "start": "2334330",
    "end": "2345440"
  },
  {
    "text": "Uh, in many cases, where the first feature is constant, we made x_1 as 1 and that's a choice.",
    "start": "2352920",
    "end": "2358015"
  },
  {
    "text": "And when we get to choose how we are constructing the features, what our maps Phi and Psi are.",
    "start": "2358015",
    "end": "2363670"
  },
  {
    "text": "And so we often choose x_1 to be 1, the constant feature. And the reason we do that is that then, the,",
    "start": "2363670",
    "end": "2371859"
  },
  {
    "text": "uh- uh, the linear predictor, so g of x, is Theta transpose x,",
    "start": "2371860",
    "end": "2377664"
  },
  {
    "text": "which has this term at the beginning of it, Theta 1, a constant term which doesn't depend on x,",
    "start": "2377665",
    "end": "2384940"
  },
  {
    "text": "plus a term that is linear in x. So it's linear plus constant.",
    "start": "2384940",
    "end": "2392875"
  },
  {
    "text": "And Theta 1 is called the offset or the constant term. Some people will call it the bias in the predictor.",
    "start": "2392875",
    "end": "2400270"
  },
  {
    "text": "Um, it's the prediction when all the features, except for the constant, of course, are 0.",
    "start": "2400270",
    "end": "2407180"
  },
  {
    "text": "Some people will call that an affine predictor, linear plus constant predictor.",
    "start": "2409200",
    "end": "2414415"
  },
  {
    "text": "And if it was a linear predictor, then when x was 0, we would have to have y-hat be also 0.",
    "start": "2414415",
    "end": "2421515"
  },
  {
    "text": "Here, 0 is somewhere down here is the origin.",
    "start": "2421515",
    "end": "2427059"
  },
  {
    "text": "And, uh, we can see that our prediction doesn't pass anywhere near the origin.",
    "start": "2429570",
    "end": "2435770"
  },
  {
    "text": "Another very common predictor, um, is, uh, uh, the polynomial predictor.",
    "start": "2441600",
    "end": "2448750"
  },
  {
    "text": "So here, what we're doing is we're taking an appropriate embedding, we're choosing features.",
    "start": "2448750",
    "end": "2455140"
  },
  {
    "text": "Um, and by choosing features in this way, we can get a non-linear function of u,",
    "start": "2455140",
    "end": "2461350"
  },
  {
    "text": "even though we're using a linear predictor based on x. But you do it, is you say,",
    "start": "2461350",
    "end": "2466975"
  },
  {
    "text": "I'm going to construct this feature map Phi to be a vector.",
    "start": "2466975",
    "end": "2473215"
  },
  {
    "text": "So you give it a u, say u is a scalar. And we're gonna construct a vector feature embed- uh,",
    "start": "2473215",
    "end": "2481540"
  },
  {
    "text": "feature map, which returns 1u, u squared u cubed, all the way up to the d minus 1th power of u.",
    "start": "2481540",
    "end": "2491470"
  },
  {
    "text": "Now, people would call Phi, uh, polynomial power embedding.",
    "start": "2491470",
    "end": "2496779"
  },
  {
    "text": "And it's interesting, it's taking a- a- a one-dimensional,",
    "start": "2496780",
    "end": "2502015"
  },
  {
    "text": "uh, independent variable u and making a bigger regression problem.",
    "start": "2502015",
    "end": "2507565"
  },
  {
    "text": "It's embedding it in a d-dimensional regrading- uh, uh, regression problem.",
    "start": "2507565",
    "end": "2514329"
  },
  {
    "text": "And that means when we construct a linear predictor, we're gonna have y-hat is Theta transpose x.",
    "start": "2514330",
    "end": "2521020"
  },
  {
    "text": "But actually, Theta transpose x is Theta 1 plus Theta",
    "start": "2521020",
    "end": "2526150"
  },
  {
    "text": "2u plus Theta 3u squared all the way up to Theta d u, d minus 1.",
    "start": "2526150",
    "end": "2532089"
  },
  {
    "text": "So it's a linear function of x, but a polynomial function of u. [NOISE] Here's an example.",
    "start": "2532090",
    "end": "2545095"
  },
  {
    "text": "So I've got, uh, a bunch of data points here and fitting a quadratic here or",
    "start": "2545095",
    "end": "2555280"
  },
  {
    "text": "a cubic right there. Uh, there are other nonlinear predictors.",
    "start": "2555280",
    "end": "2567580"
  },
  {
    "text": "Uh, here's one. This is a tree based predictor. Uh, and the idea is- is that, uh,",
    "start": "2567580",
    "end": "2574674"
  },
  {
    "text": "we have a predictor which is represented by a partially developed Boolean tree.",
    "start": "2574675",
    "end": "2582370"
  },
  {
    "text": "Um, and, uh, at each, uh, vertex, which is not a leaf,",
    "start": "2582370",
    "end": "2591189"
  },
  {
    "text": "such as this one or this one, we have a variable,",
    "start": "2591189",
    "end": "2599335"
  },
  {
    "text": "an index into one of the components of x. And associated with that index or that component is a threshold.",
    "start": "2599335",
    "end": "2610420"
  },
  {
    "text": "And that's the threshold which is the decision part at that node in the tree.",
    "start": "2610420",
    "end": "2615579"
  },
  {
    "text": "So we start off coming in with x. We say, is x second component x_2 less than or equal to 0.8?",
    "start": "2615580",
    "end": "2624234"
  },
  {
    "text": "Or is it greater than 0.8? If it's greater than 0.8, we go down here and we return a prediction y-hat is 0.6.",
    "start": "2624234",
    "end": "2633520"
  },
  {
    "text": "If it's less than 0.8, less than or equal to 0.8, we go down here.",
    "start": "2633520",
    "end": "2638694"
  },
  {
    "text": "And then we end up with another- at another, uh, non-leaf node.",
    "start": "2638695",
    "end": "2644455"
  },
  {
    "text": "And that's associated with another decision. And that decision is x. Is x_1 less than or equal to 0.2?",
    "start": "2644455",
    "end": "2650380"
  },
  {
    "text": "Or is x_1 greater than 0.2? If x_1 is less than or equal to 0.2, then we go down here and we retur n y-hat is 0.25.",
    "start": "2650380",
    "end": "2659320"
  },
  {
    "text": "If x_1 is greater than 0.2, then we go down here and we returned y-hat is 0.5.",
    "start": "2659320",
    "end": "2666740"
  },
  {
    "text": "So the leaves are associated with the values. These are the leaves and the non-leaf nodes are associated with conditions.",
    "start": "2667290",
    "end": "2681380"
  },
  {
    "text": "And the predictor is a piecewise constant function of x. And we could plot it as a piecewise constant function of x,",
    "start": "2682050",
    "end": "2691045"
  },
  {
    "text": "um, at least when the tree is small enough. Here's a one-dimensional x.",
    "start": "2691045",
    "end": "2696549"
  },
  {
    "text": "Here, what's the tree for this? Well, we can work it out. Let me switch to a smaller pen.",
    "start": "2696550",
    "end": "2707150"
  },
  {
    "text": "We've got x. First decision. We have a choice, we can- there's not- there's",
    "start": "2707550",
    "end": "2713350"
  },
  {
    "text": "more than one way to write any given decision tree. Let's do- let's look at this point right here first.",
    "start": "2713350",
    "end": "2719815"
  },
  {
    "text": "And we'll say, if x is less than 0.2,",
    "start": "2719815",
    "end": "2729040"
  },
  {
    "text": "then we're going to have y-hat is 0.25.",
    "start": "2729040",
    "end": "2734270"
  },
  {
    "text": "Here, we wrote another decision to make and this one is going to be here.",
    "start": "2735720",
    "end": "2744410"
  },
  {
    "text": "And the decision is, s o here, x is greater than 0.2.",
    "start": "2745020",
    "end": "2751975"
  },
  {
    "text": "And here, where the decision is- is x, uh, less than 0.8 or is x greater than 0.8.",
    "start": "2751975",
    "end": "2762625"
  },
  {
    "text": "And if there's less than 0.8, then we're going to have y-hat equal to 0.5.",
    "start": "2762625",
    "end": "2769960"
  },
  {
    "text": "And if it's greater than 0.8, we're gonna have y-hat equal to 0.6.",
    "start": "2769960",
    "end": "2775495"
  },
  {
    "text": "So there's our decision tree corresponding to this, uh, uh, predictor right here.",
    "start": "2775495",
    "end": "2785030"
  },
  {
    "text": "Um, and of course, if we were in higher dimensions, then we're slicing up the vector space corresponding",
    "start": "2785310",
    "end": "2794140"
  },
  {
    "text": "to rd into, uh, rectangular regions.",
    "start": "2794140",
    "end": "2800349"
  },
  {
    "text": "And, uh, at each vertex- at each non-leaf vertex of the tree,",
    "start": "2800350",
    "end": "2806230"
  },
  {
    "text": "we're making another cut.",
    "start": "2806230",
    "end": "2808550"
  },
  {
    "text": "Let us look at another class of predictors. And these are the neural network predictors, very important, very powerful class of predictors, extremely widely used.",
    "start": "2815520",
    "end": "2826850"
  },
  {
    "text": "So what is a neural network? A- a neural network, this is a feed-forward neural network,",
    "start": "2828360",
    "end": "2834790"
  },
  {
    "text": "which is perhaps the most common type of neural network. There are others as well that we will discuss, but this is the most common kind.",
    "start": "2834790",
    "end": "2841585"
  },
  {
    "text": "It's a- a composition of functions. So here I've got, uh,",
    "start": "2841585",
    "end": "2846985"
  },
  {
    "text": "three functions, g_1, g_2, and g_3. And I take x, and I apply it to g_1.",
    "start": "2846985",
    "end": "2853780"
  },
  {
    "text": "And then I take the result and apply it- it to g_2, and then I take the result and apply it to g_3.",
    "start": "2853780",
    "end": "2859300"
  },
  {
    "text": "And that gives me y-hat. And you can have any number of, uh, functions here.",
    "start": "2859300",
    "end": "2865240"
  },
  {
    "text": "Here, we've just got three for the sake of example. Now, we write that using this notation,",
    "start": "2865240",
    "end": "2871389"
  },
  {
    "text": "that the predictor g is g_3 composed with g_2, composed with g_1.",
    "start": "2871389",
    "end": "2877704"
  },
  {
    "text": "And this symbol with a little circle, uh, means function composition.",
    "start": "2877705",
    "end": "2884020"
  },
  {
    "text": "Uh, these g_i's are called layers of the neural network, and here we have three of them.",
    "start": "2884020",
    "end": "2889570"
  },
  {
    "text": "Uh, we can split up this function composition into three independent things.",
    "start": "2889570",
    "end": "2896545"
  },
  {
    "text": "First of all, we apply x. We take g_1 and apply it to x and get z1.",
    "start": "2896545",
    "end": "2904230"
  },
  {
    "text": "Then we take g_2, apply it to z_1 and get a z_2. And then we take z_2- uh,",
    "start": "2904230",
    "end": "2910559"
  },
  {
    "text": "we take g_3 and apply it to z_2 to get our output y-hat.",
    "start": "2910560",
    "end": "2915670"
  },
  {
    "text": "And these intermediate variables which we've introduced here, z_1 and z_2, are called activations.",
    "start": "2915670",
    "end": "2925165"
  },
  {
    "text": "They're the outputs of the ith layer. And to make it all consistent,",
    "start": "2925165",
    "end": "2932170"
  },
  {
    "text": "we might make x here, g- z_0, and we might make,",
    "start": "2932170",
    "end": "2938339"
  },
  {
    "text": "uh, y-hat here z_3. And then that's a, uh- a very reasonable notational convenience that we'll make useful.",
    "start": "2938340",
    "end": "2948124"
  },
  {
    "text": "Um, and, uh- and here,",
    "start": "2948124",
    "end": "2955710"
  },
  {
    "text": "it says here, d_0 and d_3. And let me tell you what those are, because one of the things that's going on here is that",
    "start": "2955710",
    "end": "2963704"
  },
  {
    "text": "each one of these z's can have a different dimension. And we will have z_i is going to be in R to the di.",
    "start": "2963705",
    "end": "2976160"
  },
  {
    "text": "And that way, this notation d_0 is d and d_3 is m,",
    "start": "2979450",
    "end": "2984845"
  },
  {
    "text": "is simply expressing our usual notational convention for the dimension of x being d and",
    "start": "2984845",
    "end": "2991820"
  },
  {
    "text": "the dimension of y being m. Now you can visualize,",
    "start": "2991820",
    "end": "2998810"
  },
  {
    "text": "this is a nice graph. X comes in, goes through g_1, gives us z_1,",
    "start": "2998810",
    "end": "3006895"
  },
  {
    "text": "goes through g_2 gives us z_2, goes through g_3, gives us y hat,",
    "start": "3006895",
    "end": "3012865"
  },
  {
    "text": "which we might call a flow graph for the data. Uh, these layer functions,",
    "start": "3012865",
    "end": "3022480"
  },
  {
    "text": "uh, have a particular form in neural networks. Here's what they look like. They're the composition of a function h with an affine function.",
    "start": "3022480",
    "end": "3033520"
  },
  {
    "text": "So here, Theta_i transpose 1, z_i minus 1.",
    "start": "3033520",
    "end": "3039715"
  },
  {
    "text": "What this parentheses notation means, let me tell you what this means. If I write x,",
    "start": "3039715",
    "end": "3045500"
  },
  {
    "text": "y, z in parentheses, that's exactly the same as writing a column vector x,",
    "start": "3045500",
    "end": "3054105"
  },
  {
    "text": "y, z with them stacked up on top of each other.",
    "start": "3054105",
    "end": "3059590"
  },
  {
    "text": "When I write Theta_i transpose one comma z, what I'm really saying is that I've got some vector Theta_i transpose 1, z.",
    "start": "3060660",
    "end": "3074960"
  },
  {
    "text": "And that's, of course, I could expand that as saying,",
    "start": "3075810",
    "end": "3081550"
  },
  {
    "text": "that's Theta_i 1 times 1 plus Theta_i,",
    "start": "3081550",
    "end": "3087130"
  },
  {
    "text": "2 times z_1 plus all the way up to Theta_i,",
    "start": "3087130",
    "end": "3094089"
  },
  {
    "text": "whatever the dimension is, d plus 1 times z_d.",
    "start": "3094090",
    "end": "3099925"
  },
  {
    "text": "It's a linear combination of the z_i's. And, of course, I don't need to write that one in there because it's just 1.",
    "start": "3099925",
    "end": "3108460"
  },
  {
    "text": "It's just Theta_i 1. So I take a linear combination of Zs with a constant term and offset,",
    "start": "3108460",
    "end": "3117580"
  },
  {
    "text": "and then I apply a function h to it. Now remember that, uh,",
    "start": "3117580",
    "end": "3125329"
  },
  {
    "text": "uh, the Zs, uh, vectors and the output can also be vectors.",
    "start": "3126090",
    "end": "3131980"
  },
  {
    "text": "And that would mean that Theta_i would be a matrix, and its dimension would be R d_ i minus 1 plus 1 by d_i,",
    "start": "3131980",
    "end": "3144285"
  },
  {
    "text": "because its input is a vector z_i minus 1,",
    "start": "3144285",
    "end": "3152214"
  },
  {
    "text": "which has dimension R d_i minus 1, and its output is going to have dimension R d_i.",
    "start": "3152215",
    "end": "3159830"
  },
  {
    "text": "And this is the parameter associated with the- with layer i. These are also called the weights.",
    "start": "3161730",
    "end": "3168220"
  },
  {
    "text": "Now what I'm doing with that vector, which is, uh, produced by this affine function,",
    "start": "3168220",
    "end": "3177025"
  },
  {
    "text": "is I am applying a scalar function h to it. And that scalar function h is called the activation function,",
    "start": "3177025",
    "end": "3183970"
  },
  {
    "text": "and it acts element-wise. So it's applied to each entry of the vector separately.",
    "start": "3183970",
    "end": "3190520"
  },
  {
    "text": "Um, so what that means if the- if I've got a vector z and it's equal to h of w?",
    "start": "3192000",
    "end": "3200515"
  },
  {
    "text": "What that means is that z_1 is h of w_1,",
    "start": "3200515",
    "end": "3206035"
  },
  {
    "text": "z_2 is h of w_2, and so on. It's really just a shorthand for applying the function separately to each component.",
    "start": "3206035",
    "end": "3220780"
  },
  {
    "text": "Now the scalar function h is called the activation function. Uh, there are many different activation functions that are used in neural networks.",
    "start": "3220780",
    "end": "3231175"
  },
  {
    "text": "The most common one probably by far, is this one called ReLU. Uh, let's, uh, take a look at that.",
    "start": "3231175",
    "end": "3239605"
  },
  {
    "text": "So the ReLU is this thing. It's, uh, usually denoted by x parentheses with a subscript plus,",
    "start": "3239605",
    "end": "3248650"
  },
  {
    "text": "uh, also written max of x, 0. Uh, what it is is that we can just draw a little graph of it here.",
    "start": "3248650",
    "end": "3255640"
  },
  {
    "text": "[NOISE] Uh, this, uh, is um, is x,",
    "start": "3255640",
    "end": "3264385"
  },
  {
    "text": "and this is x plus, and it's graph, 0 for negative x and just x reported of x.",
    "start": "3264385",
    "end": "3276070"
  },
  {
    "text": "So if x is negative, x plus is just 0. If x is positive, then, uh, x plus is just x.",
    "start": "3276070",
    "end": "3285260"
  },
  {
    "text": "Let's call it a ReLU, ReLU is short for rectified linear unit.",
    "start": "3286580",
    "end": "3292380"
  },
  {
    "text": "Uh, another one that's quite common is the sigmoid function,",
    "start": "3292380",
    "end": "3297704"
  },
  {
    "text": "e_x divided by 1 plus e_x. Um, and we will see a few others as well that are commonly used for activation functions.",
    "start": "3297705",
    "end": "3308599"
  },
  {
    "text": "Um, so once one knows the matrices Theta 1 to Theta n. Once one knows the activation functions,",
    "start": "3308600",
    "end": "3319615"
  },
  {
    "text": "one can construct the functions g_i. We can compose the functions g_i,",
    "start": "3319615",
    "end": "3327160"
  },
  {
    "text": "to have predictor, and, uh, that's a neural network predictor.",
    "start": "3327160",
    "end": "3333770"
  },
  {
    "text": "Uh, it's often drawn as, uh, a network. Uh, so here is a three layer neural network.",
    "start": "3337650",
    "end": "3346225"
  },
  {
    "text": "Remember what the functions are. Our first function, uh, g_1,",
    "start": "3346225",
    "end": "3352885"
  },
  {
    "text": "takes these two variables, x_1 and x_2, and constructs these four variables.",
    "start": "3352885",
    "end": "3362110"
  },
  {
    "text": "So here, the dimension here is d is 2,",
    "start": "3362110",
    "end": "3368800"
  },
  {
    "text": "d_1 is 4, that's the dimension of the variable at the first layer,",
    "start": "3368800",
    "end": "3376780"
  },
  {
    "text": "it's the first activation. And we have, uh, d_2 is 2.",
    "start": "3376780",
    "end": "3389680"
  },
  {
    "text": "And then we have M is 1, which is just d_3 is 1. And the function that maps this this way, is g_1.",
    "start": "3389680",
    "end": "3397600"
  },
  {
    "text": "The function that maps this, this way is g_2, and the function that maps this,",
    "start": "3397600",
    "end": "3404260"
  },
  {
    "text": "this way is g_3. But it's quite convenient to draw this as",
    "start": "3404260",
    "end": "3410905"
  },
  {
    "text": "a network like this where we've expanded out each of the variables. Because then one can see several things.",
    "start": "3410905",
    "end": "3417924"
  },
  {
    "text": "One can see that for example, the first component of z_1 depends on x_1 and x_2,",
    "start": "3417925",
    "end": "3424345"
  },
  {
    "text": "as does the second, as does the third and as does the fourth, and the coefficients that enter into this dependency are the entries of the Thetas.",
    "start": "3424345",
    "end": "3435205"
  },
  {
    "text": "So for example, z_1- the z_2 component 1 is a linear combination of the z_1,",
    "start": "3435205",
    "end": "3443469"
  },
  {
    "text": "z_1_1, z_1_2, z_1_3, z _1_4, plus a constant term passed through the activation function.",
    "start": "3443469",
    "end": "3451135"
  },
  {
    "text": "So it's h of a linear combination of these plus a constant term.",
    "start": "3451135",
    "end": "3456220"
  },
  {
    "text": "The coefficient of z_1_1 is going to be Theta_2_2_1.",
    "start": "3456220",
    "end": "3461630"
  },
  {
    "text": "Here we can also see that the z_1_4 it's- its coefficient is a- is",
    "start": "3462060",
    "end": "3469090"
  },
  {
    "text": "a the activation function composed with a linear combination of x_1 and x_2 plus a constant.",
    "start": "3469090",
    "end": "3476035"
  },
  {
    "text": "The coefficient in that linear combination is Theta_1 2_4.",
    "start": "3476035",
    "end": "3481460"
  },
  {
    "text": "So these kinds of network diagrams are, um, often drawn for neural networks.",
    "start": "3481620",
    "end": "3487615"
  },
  {
    "text": "Um, very often, the Theta_i's are sparse matrices. So we have lots of zeros.",
    "start": "3487615",
    "end": "3494920"
  },
  {
    "text": "And that would mean that many of these arrows are- have zero coefficients associated with them.",
    "start": "3494920",
    "end": "3500664"
  },
  {
    "text": "In which case, we typically wouldn't draw arrows that have zero coefficients.",
    "start": "3500665",
    "end": "3506120"
  },
  {
    "text": "Um, and so each of the vertices in this graph is a component of an activation.",
    "start": "3506130",
    "end": "3511825"
  },
  {
    "text": "And the edges, our entries of the weight matrices. Now we would call them individual weights or scalar parameters of the neural network.",
    "start": "3511825",
    "end": "3521785"
  },
  {
    "text": "And so here, we have three layers, and we have specific dimensions of this particular neural network.",
    "start": "3521785",
    "end": "3529190"
  },
  {
    "text": "Now by choosing the parameters, what are we going to choose? We've got to choose Theta_1, and Theta_2, and Theta_3.",
    "start": "3530510",
    "end": "3537075"
  },
  {
    "text": "This is the dimension of Theta_1. Here, it's, uh, 3 by 4. See the dimension of Theta_ 2,",
    "start": "3537075",
    "end": "3543525"
  },
  {
    "text": "it's 5 by 2. And this is a dimension that Theta_3, it's, uh, 3 by 1.",
    "start": "3543525",
    "end": "3549044"
  },
  {
    "text": "So there's quite a few parameters that enter into the Thetas. Um, and of course,",
    "start": "3549044",
    "end": "3554849"
  },
  {
    "text": "the activation function is a choice as well. Well, that's not a parameter. And we're going to pick",
    "start": "3554850",
    "end": "3561065"
  },
  {
    "text": "the predictor from this family of predictors by choosing the Thetas.",
    "start": "3561065",
    "end": "3566440"
  },
  {
    "text": "The Hs are part of the structure or the form of the predictor. By choosing these Thetas,",
    "start": "3566440",
    "end": "3572529"
  },
  {
    "text": "one can end up with a huge number of different functions which all look like crumpled pieces of paper.",
    "start": "3572530",
    "end": "3578080"
  },
  {
    "text": "Here's one for this particular choice of Theta. Um, you can see right here. Uh, you can see that it's, uh,",
    "start": "3578080",
    "end": "3585099"
  },
  {
    "text": "uh, piece-wise linear function. Uh, but it's got complicated,",
    "start": "3585100",
    "end": "3591490"
  },
  {
    "text": "uh, joins in the function. There's a complicated join there and there.",
    "start": "3591490",
    "end": "3597055"
  },
  {
    "text": "And, um, uh, now one can end up with all sorts of very strange functions,",
    "start": "3597055",
    "end": "3604270"
  },
  {
    "text": "uh, by picking the Thetas. And this is simply with a- a three layer neural network",
    "start": "3604270",
    "end": "3610000"
  },
  {
    "text": "with quite a small number of variables. Uh, neural networks can have millions of parameters.",
    "start": "3610000",
    "end": "3617485"
  },
  {
    "text": "And- and then, it becomes impossible to visualize or understand what those parameters mean or how they work.",
    "start": "3617485",
    "end": "3627655"
  },
  {
    "text": "Um, so in summary,",
    "start": "3627655",
    "end": "3636880"
  },
  {
    "text": "a predictor is a function. It takes Xs which live in R_d and gives us back Ys, which live in R_m.",
    "start": "3636880",
    "end": "3644545"
  },
  {
    "text": "And it's meant to predict the outcome y given a particular feature vector x.",
    "start": "3644545",
    "end": "3649585"
  },
  {
    "text": "There are many different types of predictors, nearest neighbors, trees, linear predictors, neural networks, many others.",
    "start": "3649585",
    "end": "3660560"
  },
  {
    "text": "Um, and most of them are parameterized. We'll write them g of Theta of x. Um,",
    "start": "3660600",
    "end": "3667435"
  },
  {
    "text": "g fixes the form, and Theta are parameters that we choose.",
    "start": "3667435",
    "end": "3673165"
  },
  {
    "text": "And we're going to choose them so that we fit the data as well as possible. And that's called training predictor.",
    "start": "3673165",
    "end": "3679945"
  },
  {
    "text": "And we're going to see later how training is done.",
    "start": "3679945",
    "end": "3684410"
  }
]