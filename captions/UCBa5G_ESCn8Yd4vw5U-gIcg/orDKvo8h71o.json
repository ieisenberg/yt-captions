[
  {
    "start": "0",
    "end": "125000"
  },
  {
    "text": "now we have Hyung Won give a talk. So he's \ncurrently a research scientist at on the OpenAI  ",
    "start": "5040",
    "end": "11520"
  },
  {
    "text": "ChatGPT team. He has worked on various aspects of \nlarge language models. Things like pre-training,  ",
    "start": "11520",
    "end": "18200"
  },
  {
    "text": "instruction fine tuning, reinforcement learning \nwith human feedback, reasoning, and so forth. ",
    "start": "18200",
    "end": "24320"
  },
  {
    "text": "And some of his notable works include the scaling \nflan papers such as FLAN-T5, as well as Flan-PALM,  ",
    "start": "24320",
    "end": "30400"
  },
  {
    "text": "and T5X, the training framework used to train \nthe PALM language model. And before OpenAI,  ",
    "start": "30400",
    "end": "36480"
  },
  {
    "text": "he was at Google Brain, and he received his \nPhD from MIT. So give a hand for Hyung Won. ",
    "start": "36480",
    "end": "55456"
  },
  {
    "text": "Do we switch the slides?\nAll right. My name   is Hyung Won, and I'm really happy to be here \ntoday. And this week I was thinking about--  ",
    "start": "55456",
    "end": "59920"
  },
  {
    "text": "by the way, is mic working fine? Yeah. So this \nweek I thought about-- OK, I'm giving a lecture  ",
    "start": "61280",
    "end": "68320"
  },
  {
    "text": "on transformers at Stanford. What should I \ntalk about? And I thought, OK, some of you in  ",
    "start": "68320",
    "end": "74120"
  },
  {
    "text": "this room and in Zoom will actually go shape the \nfuture of AI, so maybe I should talk about that. ",
    "start": "74120",
    "end": "79800"
  },
  {
    "text": "It's a really important goal and ambitious, and \nwe really have to get it right. So that could  ",
    "start": "79800",
    "end": "84880"
  },
  {
    "text": "be a good topic to think about. And when \nwe talk about something into the future,  ",
    "start": "84880",
    "end": "89960"
  },
  {
    "text": "the best place to get an advice is to \nlook into the history. And in particular,  ",
    "start": "89960",
    "end": "95040"
  },
  {
    "text": "we'll look at the early history of transformer, \nand try to learn many lessons from there. ",
    "start": "95040",
    "end": "101120"
  },
  {
    "text": "And the goal will be to develop a unified \nperspective in which we can look into many  ",
    "start": "101120",
    "end": "108000"
  },
  {
    "text": "seemingly disjoint events. And from \nthat we can probably hope to project  ",
    "start": "108000",
    "end": "114400"
  },
  {
    "text": "into the future what might be coming. And \nso that will be the goal of this lecture,  ",
    "start": "114400",
    "end": "120920"
  },
  {
    "text": "and we'll look at some of the architectures \nof the transformers, so let's get started. ",
    "start": "120920",
    "end": "126360"
  },
  {
    "start": "125000",
    "end": "918000"
  },
  {
    "text": "Everyone I see is saying, AI is so advancing so \nfast that it's so hard to keep up. And it doesn't  ",
    "start": "126360",
    "end": "133440"
  },
  {
    "text": "matter if you have years of experience, there \nare so many things are coming out every week   that it's just hard to keep up. And I do see \nmany people spend a lot of time and energy  ",
    "start": "133440",
    "end": "143280"
  },
  {
    "text": "catching up with this latest developments, the \ncutting edge and the newest thing, and then not  ",
    "start": "143280",
    "end": "149400"
  },
  {
    "text": "enough attention goes into old things because \nthey become deprecated and no longer relevant. ",
    "start": "149400",
    "end": "157079"
  },
  {
    "text": "But I think it's important, actually, to look into \nthat because we really need to-- when things are  ",
    "start": "157080",
    "end": "162320"
  },
  {
    "text": "moving so fast beyond our ability to catch up \nwhat we need to do is study the change itself.  ",
    "start": "162320",
    "end": "167800"
  },
  {
    "text": "And that means we can look back at the previous \nthings, and then look at the current thing,  ",
    "start": "167800",
    "end": "172920"
  },
  {
    "text": "and try to map how we got here, and from which we \ncan look into what where we are heading towards. ",
    "start": "172920",
    "end": "179440"
  },
  {
    "text": "So what does it mean to study the change itself? \nFirst, we need to identify the dominant driving  ",
    "start": "179440",
    "end": "188080"
  },
  {
    "text": "forces behind the change. So here dominant \nis an important word because typically,  ",
    "start": "188080",
    "end": "193680"
  },
  {
    "text": "a change has many, many driving forces, and we \nonly care about the dominant one because we're   not trying to get really accurate, we just \nwant to have the sense of directionality. ",
    "start": "193680",
    "end": "202560"
  },
  {
    "text": "Second, we need to understand the driving force \nreally well, and then after that we can predict   the future trajectory by rolling out the driving \nforce and so on. And you heard that right,  ",
    "start": "202560",
    "end": "212920"
  },
  {
    "text": "I mentioned about predicting the future, this is \na computer science class not like an astrology or   something. But we do-- I think it's actually \nnot that impossible to predict some future  ",
    "start": "212920",
    "end": "223800"
  },
  {
    "text": "trajectory of a very narrow scientific domain.\nAnd that endeavor is really useful to do because  ",
    "start": "223800",
    "end": "232240"
  },
  {
    "text": "let's say you do all these and then make your \nprediction accuracy to from 1% to 10%, and then  ",
    "start": "232240",
    "end": "238120"
  },
  {
    "text": "you make say 100 predictions, 10 of them will be \ncorrect, say one of them will be really, really  ",
    "start": "238120",
    "end": "243640"
  },
  {
    "text": "correct. Meaning it will have an outside impact \nthat outweighs everything. And I think that is  ",
    "start": "243640",
    "end": "249960"
  },
  {
    "text": "kind of how many-- I've seen many general thing in \nlife that you really have to be right a few times. ",
    "start": "249960",
    "end": "257280"
  },
  {
    "text": "So why-- so if we think about why predicting \nthe future is difficult or maybe even think  ",
    "start": "257280",
    "end": "265520"
  },
  {
    "text": "about the extreme case where we can all \ndo the prediction with perfect accuracy,   almost perfect accuracy. So here, I'm going to \ndo a very simple experiment of dropping this pen,  ",
    "start": "265520",
    "end": "276480"
  },
  {
    "text": "and follow this same three-step process. So we're \ngoing to identify the dominant driving force. ",
    "start": "276480",
    "end": "283600"
  },
  {
    "text": "First of all, what are the driving forces \nacting on this pen? Gravity downwards,   and is that all? We also have say air friction, \nright? If I drop it, and that will cause what's  ",
    "start": "283600",
    "end": "295000"
  },
  {
    "text": "called a drag force acting upwards. And \nactually, depending on how I drop this,  ",
    "start": "295000",
    "end": "300560"
  },
  {
    "text": "the orientation, the aerodynamic interaction \nwill be so complicated that we don't currently  ",
    "start": "300560",
    "end": "306440"
  },
  {
    "text": "have any analytical way of modeling that.\nWe can do it with the CFD, the computational   fluid dynamics, but it will be non-trivial. So \nwe can neglect that. This is heavy enough that  ",
    "start": "306440",
    "end": "316080"
  },
  {
    "text": "gravity is probably the only dominant force, so \nwe simplify the problem. Second, do we understand  ",
    "start": "316080",
    "end": "321879"
  },
  {
    "text": "this dominant driving force, which is gravity? And \nwe do because we have this Newtonian mechanics,   which provides a reasonably good model.\nAnd then with that, we can predict the future  ",
    "start": "321880",
    "end": "330919"
  },
  {
    "text": "trajectory of this pen. And if we remember \nfrom this dynamics class, if we have this  ",
    "start": "330920",
    "end": "337240"
  },
  {
    "text": "initial velocity is 0, I'm not going to put any \nvelocity. And then let's say position is 0 here,  ",
    "start": "337240",
    "end": "342520"
  },
  {
    "text": "and then 1/2 GT squared will give a precise \ntrajectory of this pen as I drop this. So  ",
    "start": "342520",
    "end": "350319"
  },
  {
    "text": "if there is a driving force, single driving \nforce that we really understand, it's actually   possible to predict what's going to happen.\nSo then, why do we really fear about predicting  ",
    "start": "350320",
    "end": "362320"
  },
  {
    "text": "the future in the most general sense? And I argue \nthat, among many reasons, the number of driving  ",
    "start": "362320",
    "end": "368280"
  },
  {
    "text": "force, the sheer number of dominant driving forces \nacting on the general prediction is so complicated  ",
    "start": "368280",
    "end": "374920"
  },
  {
    "text": "and their interaction creates a complexity \nthat we cannot predict the most general sense.  So here is my cartoon way of thinking about the \nprediction of future. x-axis, we have a number  ",
    "start": "374920",
    "end": "384920"
  },
  {
    "text": "of dominant driving forces, y-axis we have \na prediction difficulty. So on the left hand   side, we have a dropping of pen it's a very \nsimple case. It's a difficulty it's very small,  ",
    "start": "384920",
    "end": "394040"
  },
  {
    "text": "you just need to learn physics. And then, as \nyou add more stuff, it just becomes impossible. ",
    "start": "394040",
    "end": "401560"
  },
  {
    "text": "So how does this fit into the AI research? And \nyou might think, OK, I see all the time things  ",
    "start": "401560",
    "end": "408520"
  },
  {
    "text": "are coming in, we are bombarded by new things, \nand some people will come up with the new agent,  ",
    "start": "408520",
    "end": "413680"
  },
  {
    "text": "new modality, new MMLU score or whatever. We just \nsee so many things it just I'm not even able to  ",
    "start": "413680",
    "end": "420320"
  },
  {
    "text": "catch up with the latest thing, how can I even \nhope to predict the future of the AI research? ",
    "start": "420320",
    "end": "425600"
  },
  {
    "text": "But I argue that it's actually simpler \nbecause there is a dominant driving force  ",
    "start": "425600",
    "end": "431320"
  },
  {
    "text": "that is governing a lot if any if not all \nof the AI research. And because of that,  ",
    "start": "431320",
    "end": "437200"
  },
  {
    "text": "I would like to point out that, it's actually \ncloser to the left than to the right than we  ",
    "start": "437200",
    "end": "443120"
  },
  {
    "text": "actually may perceive. So what is the driving \nforce? Maybe before that, I would like to caveat  ",
    "start": "443120",
    "end": "451120"
  },
  {
    "text": "that when I do this kind of talk, I would like \nto not focus too much on the technical stuff  ",
    "start": "451120",
    "end": "458000"
  },
  {
    "text": "which you can probably do better in your own \ntime, but rather I want to share how I think. ",
    "start": "458000",
    "end": "464160"
  },
  {
    "text": "And for that, I want to share how my opinion as \nis, and so it will be very strongly opinionated.  ",
    "start": "464160",
    "end": "470920"
  },
  {
    "text": "And by no means I'm saying this is correct or \nnot, I just wanted to share my perspective. So  ",
    "start": "470920",
    "end": "476720"
  },
  {
    "text": "coming back to this driving force for AI, what is \nthat dominant driving force? And here is a plot  ",
    "start": "476720",
    "end": "482240"
  },
  {
    "text": "from Rich Sutton. And on the y-axis, we have the \ncalculations plot if you pay $100, and how much  ",
    "start": "482240",
    "end": "490960"
  },
  {
    "text": "computing power do you get? And it's in log scale.\nAnd then x-axis, we have a time of more than 100  ",
    "start": "490960",
    "end": "497880"
  },
  {
    "text": "years. So this is actually more than exponential. \nAnd I don't know any trend that is as strong and  ",
    "start": "497880",
    "end": "505640"
  },
  {
    "text": "as long lasting as this one. So whenever \nI see this kind of thing, I should say,  ",
    "start": "505640",
    "end": "511880"
  },
  {
    "text": "OK, I should not compete with this. And better, I \nshould try to leverage as much as possible. And so  ",
    "start": "511880",
    "end": "520200"
  },
  {
    "text": "what this means is, you get 10x more compute every \nfive years if you spend the same amount of dollar. ",
    "start": "520200",
    "end": "526800"
  },
  {
    "text": "And so in other words, you get the cost of \ncompute is going down exponentially. And  ",
    "start": "526800",
    "end": "532839"
  },
  {
    "text": "this associated scaling is really dominating the \nAI research, and that is somewhat hard to take,  ",
    "start": "532840",
    "end": "540560"
  },
  {
    "text": "but that is, I think, really important to think \nabout. So coming back to this AI research,  ",
    "start": "540560",
    "end": "546080"
  },
  {
    "text": "how is this exponentially cheaper compute \ndrive the AI research? Let's think about  ",
    "start": "546080",
    "end": "552160"
  },
  {
    "text": "the job of the AI researchers. It is to teach \nmachines how to think in a very general sense. ",
    "start": "552160",
    "end": "557680"
  },
  {
    "text": "And one somewhat unfortunately common approach \nis, we think about how we teach machine how we  ",
    "start": "557680",
    "end": "565279"
  },
  {
    "text": "think we think. So meaning, we model how \nwe think and then try to incorporate that  ",
    "start": "565280",
    "end": "572760"
  },
  {
    "text": "into some kind of mathematical model and teach \nthat. And now, the question is, do we understand  ",
    "start": "572760",
    "end": "577920"
  },
  {
    "text": "how we think at the very low level? I don't \nthink we do. I have no idea what's going on. ",
    "start": "577920",
    "end": "583160"
  },
  {
    "text": "So it's fundamentally flawed in the sense that \nwe try to model something that we have no idea   about. And what happens if we go with this kind \nof approach is that, it poses a structure that  ",
    "start": "583160",
    "end": "592839"
  },
  {
    "text": "serves as a shortcut in the short term, and so \nyou can maybe get a paper or something. But then  ",
    "start": "592840",
    "end": "598080"
  },
  {
    "text": "it becomes a bottleneck when-- because we don't \nknow how this will limit further scaling up. ",
    "start": "598080",
    "end": "604760"
  },
  {
    "text": "More fundamentally what this is doing is, \nwe are limiting the degree of freedom we   are giving to the machines, and that will \nbackfire at some point. And this has been  ",
    "start": "605400",
    "end": "615320"
  },
  {
    "text": "going on for decades. And bitter lesson \nis, I think the single most important  ",
    "start": "615840",
    "end": "623000"
  },
  {
    "text": "piece of writing in AI. And it says, this is my \nwording by the way. Say past 70 years of entire  ",
    "start": "623000",
    "end": "629520"
  },
  {
    "text": "AI research can be summarized into developing \nprogressively more general method with weaker  ",
    "start": "629520",
    "end": "636160"
  },
  {
    "text": "modeling assumptions or inductive biases, and add \nmore data and compute, in other words scale up.  And that has been the recipe of entire AI \nresearch, not fancy things. And if you think  ",
    "start": "636160",
    "end": "647279"
  },
  {
    "text": "about this, the models of 2000 is a lot more \ndifficult than what we use now. And so it's much  ",
    "start": "647280",
    "end": "654840"
  },
  {
    "text": "easier to get into AI nowadays from technical \nperspective. So this is, I think, the really  ",
    "start": "654840",
    "end": "662560"
  },
  {
    "text": "the key information. We have this compute cost is \ngoing down exponentially, and it's getting cheaper  ",
    "start": "662560",
    "end": "668600"
  },
  {
    "text": "faster than we are becoming a better researcher.\nSo don't compete with that and just try to   leverage that as much as possible, and that is \nthe driving force that I wanted to identify.  ",
    "start": "668600",
    "end": "679880"
  },
  {
    "text": "And I'm not saying this is the only driving \nforce, but this is the dominant driving force,   so we can probably neglect the other ones. So \nhere's a graphical version of that. x-axis we have  ",
    "start": "679880",
    "end": "689080"
  },
  {
    "text": "a compute, y-axis we have a performance of some \nkind. Let's think about some general intelligence. ",
    "start": "689080",
    "end": "694200"
  },
  {
    "text": "And let's look at two different methods. One, \nwith more structure, more modeling assumptions,  ",
    "start": "694200",
    "end": "699560"
  },
  {
    "text": "fancier math, whatever. And then the other one \nis a less structure. What you see is typically,  ",
    "start": "699560",
    "end": "704960"
  },
  {
    "text": "you start with a better performance when you have \na low compute regime, and then but it plateaus  ",
    "start": "704960",
    "end": "710320"
  },
  {
    "text": "because of some kind of structure backfiring.\nAnd then with the less structure because we give   a lot more freedom to the model, it doesn't work \nin the beginning. But then as we add more compute,  ",
    "start": "710320",
    "end": "719079"
  },
  {
    "text": "it starts working, and then it gets better, we \ncall this more scalable method. So does that mean  ",
    "start": "719080",
    "end": "726440"
  },
  {
    "text": "we should just go with the least structure, \nmost freedom to the model possible way from  ",
    "start": "726440",
    "end": "731960"
  },
  {
    "text": "the get go? And the answer is, obviously, no.\nLet's think about even less structured case. This   red one here is, it will pick up a lot later and \nrequires a lot more compute. So it really depends  ",
    "start": "731960",
    "end": "744959"
  },
  {
    "text": "on where we are. We cannot indefinitely wait \nfor the most general case. And so let's think  ",
    "start": "744960",
    "end": "750560"
  },
  {
    "text": "about the case where our compute situation is \nat this dotted line. If we're here, we should  ",
    "start": "750560",
    "end": "756279"
  },
  {
    "text": "choose this last structure one as opposed to \nthis even less structure one because the other   one doesn't really work and the other one works.\nBut crucially, we need to remember that we are  ",
    "start": "756280",
    "end": "765760"
  },
  {
    "text": "adding some structure because we don't have \ncompute, so we need to remove that later. And  ",
    "start": "765760",
    "end": "770840"
  },
  {
    "text": "so the difference between these two methods is \nthat additional inductive biases or structure we  ",
    "start": "770840",
    "end": "776200"
  },
  {
    "text": "impose, somewhat impose that typically don't get \nremoved. So adding this. What that means is that,  ",
    "start": "776200",
    "end": "783800"
  },
  {
    "text": "at the given level of compute data algorithmic \ndevelopment and architecture that we have,  ",
    "start": "783800",
    "end": "789760"
  },
  {
    "text": "there's like an optimal inductive bias or \nstructure that we can add to the problem   to make the progress. And that has been \nreally how we have made so much progress. ",
    "start": "789760",
    "end": "799399"
  },
  {
    "text": "But these are like shortcuts that hinder further \nscaling later on, so we have to remove them   later on when we have more compute, better \nalgorithm, or whatever. And as a community,  ",
    "start": "799400",
    "end": "809280"
  },
  {
    "text": "we do adding structure very well and because \nthere's an incentive structure with like papers.  ",
    "start": "809280",
    "end": "815560"
  },
  {
    "text": "You add a nice one then you get a paper. But \nremoving that doesn't really get you much,  ",
    "start": "815560",
    "end": "820640"
  },
  {
    "text": "so that we don't really do that. And I \nthink we should do a lot more of those.  So maybe another implication of this bitter lesson \nis that, because of this what is better in the  ",
    "start": "820640",
    "end": "831560"
  },
  {
    "text": "long-term almost necessarily looks worse now. And \nthis is quite unique to AI research because the  ",
    "start": "831560",
    "end": "839200"
  },
  {
    "text": "AI research of current paradigm is learning based \nmethod. Meaning that we are giving models freedom,  ",
    "start": "839200",
    "end": "846920"
  },
  {
    "text": "the machines choose how they learn.\nSo because we need to give more freedom, it  ",
    "start": "846920",
    "end": "852399"
  },
  {
    "text": "gives-- it's a more chaotic at the beginning, so \nit doesn't work. But then when it started working,  ",
    "start": "852400",
    "end": "858280"
  },
  {
    "text": "we can put in more compute and then it can be \nbetter, so it's really important to have this   in mind. So to summarize, we have identified this \ndominant driving force behind the AI research,  ",
    "start": "858280",
    "end": "870560"
  },
  {
    "text": "and that is this exponentially cheaper \ncompute, and associated scaling up. ",
    "start": "870560",
    "end": "876279"
  },
  {
    "text": "Now, that we have identified if you remember \nback from my initial slides, we-- the next   step is to understand this driving force better. \nAnd so that we're going to spend most of the time  ",
    "start": "876280",
    "end": "887560"
  },
  {
    "text": "doing that. And for that, we need to go back \nto some history of transformer because this is  ",
    "start": "887560",
    "end": "893160"
  },
  {
    "text": "a transformers class and analyze key structures \nand decisions that were made by the researchers  ",
    "start": "893160",
    "end": "899399"
  },
  {
    "text": "at the time, and why they did that. Whether \nthat was an optimal structure that could have   been added at the time, and why they might \nbe irrelevant now, and should we remove that? ",
    "start": "899400",
    "end": "909920"
  },
  {
    "text": "And we'll go through some of the \npractices of this. And hopefully,   this will give you some flavor of what \nscaling research looks like. So now,  ",
    "start": "909920",
    "end": "919760"
  },
  {
    "start": "918000",
    "end": "1409000"
  },
  {
    "text": "we'll go into a little bit of the technical \nstuff, tech transformer architecture,   there are some variants. I'll talk about \nthree of them. First, is the encoder-decoder,  ",
    "start": "919760",
    "end": "928560"
  },
  {
    "text": "which is the original transformer, \nwhich has a little bit more structure.  Second one is the encoder-only, which is \npopularized by Bert. And then third one  ",
    "start": "928560",
    "end": "937240"
  },
  {
    "text": "is decoder-only, which you can think of it as a \ncurrent like GPT-3 or other language models, this  ",
    "start": "937240",
    "end": "943880"
  },
  {
    "text": "has a lot less structure than the encoder-decoder. \nSo these are the three types we'll go into detail. ",
    "start": "943880",
    "end": "949240"
  },
  {
    "text": "Second, the encoder-only is actually not that \nuseful in the most general sense. It still has  ",
    "start": "949240",
    "end": "954520"
  },
  {
    "text": "some place, but we will just briefly go over \nthat, and then spend most of the time comparing  ",
    "start": "954520",
    "end": "960440"
  },
  {
    "text": "1 and 3. So one has more structure, what's the \nimplication of that and so on? So first of all,  ",
    "start": "960440",
    "end": "967320"
  },
  {
    "text": "let's think about what a transformer is, just \nat a very high level. Or first principles,  ",
    "start": "967320",
    "end": "973120"
  },
  {
    "text": "what is a transformer? Is a sequence model.\nAnd sequence model is has an input of a   sequence. So sequence can be-- a sequence \nof elements can be words, or images,  ",
    "start": "973120",
    "end": "983520"
  },
  {
    "text": "or whatever. It's a very general concept. In this \nparticular example I show you with the words,   sentence is a sequence of words. And then \nwe the first step is to tokenize it because  ",
    "start": "983520",
    "end": "992480"
  },
  {
    "text": "we have to represent this word in computers \nwhich requires some kind of encoding scheme,  ",
    "start": "992480",
    "end": "1000600"
  },
  {
    "text": "so we just do it with a fixed number of integers, \nand then we have now a sequence of integers. ",
    "start": "1000600",
    "end": "1006279"
  },
  {
    "text": "And then the dominant paradigm nowadays is to \nrepresent each sequence element as a vector,  ",
    "start": "1006280",
    "end": "1012000"
  },
  {
    "text": "dense vector, because we know how to multiply them \nwell. And then so we have a sequence of vectors.  ",
    "start": "1012000",
    "end": "1017160"
  },
  {
    "text": "And finally, this sequence model will do the \nfollowing. We just want to model the interaction  ",
    "start": "1017800",
    "end": "1024679"
  },
  {
    "text": "between sequence elements. And we do that by let \nthem take the dot product of each other. And if  ",
    "start": "1024680",
    "end": "1030640"
  },
  {
    "text": "the dot product is high, we can say semantically \nthey are more related than the dot products that   is low, and that's kind of the sequence model.\nAnd the transformer is a particular type of  ",
    "start": "1030640",
    "end": "1040400"
  },
  {
    "text": "sequence model that uses what's called attention \nto model this interaction . So let's get into the  ",
    "start": "1040400",
    "end": "1049040"
  },
  {
    "text": "details of this encoder-decoder, which was \nthe original transformer. It's quite many,   many pieces. So let's go into a little bit a piece \nat a time. So starting with the encoder. So here  ",
    "start": "1049040",
    "end": "1058720"
  },
  {
    "text": "I'm going to show you an example of machine \ntranslation, which used to be very cool thing. ",
    "start": "1058720",
    "end": "1064480"
  },
  {
    "text": "And so you are-- you have an English sentence that \nis good, and then we're going to translate it into   German. So first thing is to encode this into a \ndense vector. So here I'm representing it with  ",
    "start": "1064480",
    "end": "1075400"
  },
  {
    "text": "this a vector of size three or something. \nAnd then we have to let them take the dot  ",
    "start": "1076200",
    "end": "1081240"
  },
  {
    "text": "product. So this lines represent which element \ncan talk to which element, other elements. ",
    "start": "1081240",
    "end": "1088600"
  },
  {
    "text": "And here because it's an input, we take what is \ncalled a bidirectional attention, so any token   can talk to any other token. And then we have \nthis MLP or feedforward layer which is per token,  ",
    "start": "1088600",
    "end": "1099120"
  },
  {
    "text": "it doesn't have any interaction, we just do \nsome multiplication just because we can do  ",
    "start": "1099120",
    "end": "1104960"
  },
  {
    "text": "it. And then that's one layer, and we repeat that \nn times, and that's just the transformer encoder. ",
    "start": "1104960",
    "end": "1111840"
  },
  {
    "text": "And at the end, what you get is the sequence of \nvectors each representing the sequence element  ",
    "start": "1111840",
    "end": "1118919"
  },
  {
    "text": "in this case a word. So that's the output of this \nencoder. Now, let's look at the decoder, which is  ",
    "start": "1118920",
    "end": "1124800"
  },
  {
    "text": "similarly shaped stack of layers. So here we put \nin as an input what the answer should be. So here,  ",
    "start": "1124800",
    "end": "1134000"
  },
  {
    "text": "BOS, is the beginning of sequence, and then das \nist gut, I don't know how to pronounce it, but   that's the German translation of, that is good.\nAnd so we kind of go through the similar process.  ",
    "start": "1134000",
    "end": "1143200"
  },
  {
    "text": "Here we have a causal self-attention, \nmeaning that the tokens of time step t can  ",
    "start": "1143200",
    "end": "1148240"
  },
  {
    "text": "only attend to t and before because when we start \ngenerating it, we don't have the future tokens,  ",
    "start": "1148240",
    "end": "1154280"
  },
  {
    "text": "so we cannot-- when we train it, we should limit \nthat, and that way, this is done by like masking,  ",
    "start": "1154280",
    "end": "1160480"
  },
  {
    "text": "but this is different from the encoder.\nSo after this, you can get-- after again,  ",
    "start": "1160480",
    "end": "1167520"
  },
  {
    "text": "n layers, you get this sequence output, and you \nhave this-- the output of sequence, to sequence,  ",
    "start": "1167520",
    "end": "1174200"
  },
  {
    "text": "to sequence mapping, this is a general \nencoder-decoder architecture. And when you   get this end of sequence, you stop generating \nit. So this is the overall picture. Now,  ",
    "start": "1174200",
    "end": "1183639"
  },
  {
    "text": "I'll point out some important attention patterns.\nSo we are translating into German what is into  ",
    "start": "1183640",
    "end": "1191160"
  },
  {
    "text": "input to the encoder. So there has to be \nsome connection between the decoder and   the encoder that is done by this cross \nattention mechanism shown in this red,  ",
    "start": "1191160",
    "end": "1199039"
  },
  {
    "text": "which is just that each vector representation of \neach sequence in the output decoder should attend  ",
    "start": "1199040",
    "end": "1205080"
  },
  {
    "text": "to some of them in the encoder, and that is done.\nIn particular the design feature which is  ",
    "start": "1205080",
    "end": "1210640"
  },
  {
    "text": "interesting is that all the layers in the \ndecoder attend to the final layer output of the  ",
    "start": "1210640",
    "end": "1216280"
  },
  {
    "text": "encoder, and we'll come back to the implication \nof this design. So yeah, that's that. And now,  ",
    "start": "1216280",
    "end": "1223680"
  },
  {
    "text": "move on to the second type of architecture, which \nis encoder-only. We'll spend a little bit of time   here. So again, we have to-- we have the same \ninput, and we go through a similar structure. ",
    "start": "1223680",
    "end": "1235680"
  },
  {
    "text": "And then in this case, the final output is \na single vector. Regardless of the length   of the sequence, we just get a single vector. \nAnd that is that represent the input sequence,  ",
    "start": "1235680",
    "end": "1247120"
  },
  {
    "text": "that's a dense vector representation. And then \nlet's say we do some kind of a sentiment analysis,  ",
    "start": "1247120",
    "end": "1252400"
  },
  {
    "text": "we run through a task specific linear \nlayer to map it to classification   label positive or negative probabilities here.\nAnd that's required for all these task specific  ",
    "start": "1252400",
    "end": "1263920"
  },
  {
    "text": "cases. And this is kind of popularized \nby Bert. And what this means is that,  ",
    "start": "1263920",
    "end": "1269320"
  },
  {
    "text": "here at the time, 2018, when Bert came out \nwe had the benchmark called glue, which was  ",
    "start": "1269320",
    "end": "1275159"
  },
  {
    "text": "a language understanding task. You have a sequence \nin classification labels out for most cases. This   was how the field really advanced at the time.\nSo when we care about such tasks, then there's an  ",
    "start": "1275160",
    "end": "1286400"
  },
  {
    "text": "incentive to think about simplifying the problem, \nadding the structure to the problem so that we can   make a progress. So this, the additional structure \nthat was put into this particular architecture is  ",
    "start": "1286400",
    "end": "1295200"
  },
  {
    "text": "that we're going to give up on the generation. \nIf we do that, it becomes a lot simpler problem.  ",
    "start": "1295200",
    "end": "1301840"
  },
  {
    "text": "Instead of sequence to sequence, we're talking \nabout sequence to classification labels,   and that's just so much easier.\nAnd so at some point, 2018, 2019,  ",
    "start": "1301840",
    "end": "1311720"
  },
  {
    "text": "a lot of the papers are just research was like, \nwe sometimes call it Bert engineers, a little   bit change of something get like 0.5% better on \nglue, and you get a paper, and things like that.  ",
    "start": "1311720",
    "end": "1322160"
  },
  {
    "text": "It was very chaotic error, and-- but if we look at \nfrom this perspective, we are putting the sequence  ",
    "start": "1322160",
    "end": "1330000"
  },
  {
    "text": "structure of not generating the sequence that puts \na lot of performance win, but in the long term,  ",
    "start": "1330000",
    "end": "1336120"
  },
  {
    "text": "it's not really useful. So we're not going to look \nat this encoder-only architecture going forward. ",
    "start": "1336120",
    "end": "1341320"
  },
  {
    "text": "Third architecture, decoder-only. This one is \nmy favorite personally. And it looks kind of  ",
    "start": "1341320",
    "end": "1347880"
  },
  {
    "text": "daunting because of this attention pattern, \nbut it actually is very simple. So here,  ",
    "start": "1347880",
    "end": "1354240"
  },
  {
    "text": "we only have a single stack, and it can actually \ngenerate stuff. And so there's misconception that  ",
    "start": "1354240",
    "end": "1360880"
  },
  {
    "text": "some people think this decoder-only architecture \nis used for language modeling next to prediction,   so it cannot be used for supervised \nlearning. But here, we can actually do it. ",
    "start": "1360880",
    "end": "1368520"
  },
  {
    "text": "The trick is to have this input that is good, \nconcatenate it with the target. And if you do  ",
    "start": "1368520",
    "end": "1374040"
  },
  {
    "text": "that, then it just becomes simple to sequence in \nsequence out. So what we do is, the self-attention  ",
    "start": "1374040",
    "end": "1380080"
  },
  {
    "text": "mechanism here is actually handling both the \ncross attention between target and the input,  ",
    "start": "1380080",
    "end": "1385760"
  },
  {
    "text": "and self-attention sequence learning within \neach. So that's the causal attention. ",
    "start": "1385760",
    "end": "1391040"
  },
  {
    "text": "And then as I mentioned, the output is a \nsequence, and then the key design features  ",
    "start": "1391040",
    "end": "1396720"
  },
  {
    "text": "are self-attention is serving both roles, and \nwe are in some sense sharing the parameters  ",
    "start": "1396720",
    "end": "1402760"
  },
  {
    "text": "between input and target. So same set of \nparameters are applied to both input and   the target sequences. So this is the decoder \nonly, now, we will go into the comparison. ",
    "start": "1402760",
    "end": "1412520"
  },
  {
    "start": "1409000",
    "end": "2191000"
  },
  {
    "text": "So I think there are many-- like they look \nvery different, at least, on the schematics.  ",
    "start": "1412520",
    "end": "1417760"
  },
  {
    "text": "So how different are they actually? And I argue \nthat they're actually quite similar. And so to  ",
    "start": "1417760",
    "end": "1424960"
  },
  {
    "text": "illustrate that we're going to transform \nstarting from this encoder-decoder which   has more structures built in, and then \ninto the decoder-only architecture and  ",
    "start": "1424960",
    "end": "1433880"
  },
  {
    "text": "see what are some of the differences. And then \ninterpret those differences, those additional   structures, are they relevant nowadays now that \nwe have more compute better algorithm and so on? ",
    "start": "1433880",
    "end": "1444560"
  },
  {
    "text": "So let's have this table, four differences we'll \nsee each of them. And then as we go through,  ",
    "start": "1444560",
    "end": "1450200"
  },
  {
    "text": "we will populate this table. So let's first \nlook at this additional cross-attention. What  ",
    "start": "1450200",
    "end": "1456080"
  },
  {
    "text": "that means is that, this on the left is an \nencoder-decoder, which has this additional   red block, the cross-attention compared \nto the simpler one that doesn't have that. ",
    "start": "1456080",
    "end": "1464320"
  },
  {
    "text": "So we want to make it, make the left closer to the \nright, so that means we need to either get rid of  ",
    "start": "1464320",
    "end": "1470639"
  },
  {
    "text": "it or something. And attention mechanism \nhas kind of the four projection matrices,  ",
    "start": "1470640",
    "end": "1476000"
  },
  {
    "text": "and so self-attention and cross attention \nactually have the same number of parameters,   same shape, so we can just share them. So \nthat's the first step, share both of these,  ",
    "start": "1476000",
    "end": "1483600"
  },
  {
    "text": "and then it becomes mostly the same mechanism.\nAnd then so that's the first difference separate  ",
    "start": "1483600",
    "end": "1488880"
  },
  {
    "text": "cross-attention or self-attention serving both \nroles. Second difference is the parameter sharing.  ",
    "start": "1488880",
    "end": "1494320"
  },
  {
    "text": "So what that means is that, between the input and \nthe target, encoder-decoder architecture uses a  ",
    "start": "1494320",
    "end": "1500279"
  },
  {
    "text": "separate parameters. And decoder-only only has \na single stack so it uses the shared parameter. ",
    "start": "1500280",
    "end": "1505800"
  },
  {
    "text": "So if we want to make the left close to right, we \nwant to share the encoder parameters, so let's do  ",
    "start": "1505800",
    "end": "1511280"
  },
  {
    "text": "that. I just color this, so now, they share the \nparameters. Third difference is the target to  ",
    "start": "1511280",
    "end": "1516600"
  },
  {
    "text": "input attention pattern. So we need to connect the \ntarget to the input. And how does that-- how is it  ",
    "start": "1516600",
    "end": "1521799"
  },
  {
    "text": "done? In the encoder-decoder case, we had this \ncross attention. And then in the decoder-only,  ",
    "start": "1521800",
    "end": "1527400"
  },
  {
    "text": "it's a per the self-attention doing everything.\nWhat the difference is that we have this every  ",
    "start": "1527400",
    "end": "1535520"
  },
  {
    "text": "layer of the decoder attending to the final layer \noutput of the encoder, whereas if you think about  ",
    "start": "1535520",
    "end": "1540800"
  },
  {
    "text": "this decoder is actually per layer, within \nlayer when we are decoding the say word thus,  ",
    "start": "1540800",
    "end": "1547760"
  },
  {
    "text": "we are looking at the same layer representation \nof the encoder, and that's within layer,  ",
    "start": "1547760",
    "end": "1553560"
  },
  {
    "text": "and I think this is the design feature. So if \nwe want to make this close to that, we have   to bring back this attention to each layer.\nSo now, layer 1 will be attending to layer  ",
    "start": "1553560",
    "end": "1563480"
  },
  {
    "text": "1 of this. And that's then finally, the \nlast difference is the input attention.  ",
    "start": "1563480",
    "end": "1569640"
  },
  {
    "text": "I mentioned about this bidirectional attention. \nAnd because we have this decoder only typically,  ",
    "start": "1569640",
    "end": "1575160"
  },
  {
    "text": "with the unidirectional attention, we need to make \nthem matching, so that's the we can just get rid  ",
    "start": "1575160",
    "end": "1581040"
  },
  {
    "text": "of it. I just got rid of the some of the arrows.\nSo then at this point, these two architectures  ",
    "start": "1581040",
    "end": "1587520"
  },
  {
    "text": "are almost identical. A little bit \nof difference in the cross attention,   but same number of parameters. And if you have \nin deep learning, if you just train these, these  ",
    "start": "1587520",
    "end": "1596600"
  },
  {
    "text": "two architecture in the same task, same data, I \nthink, you will get pretty much within the noise,   probably closer than if you train the same \nthing twice. So I would say they are identical. ",
    "start": "1596600",
    "end": "1606240"
  },
  {
    "text": "And so these are the main differences. Now, \nwe'll look at what are the additional structures,  ",
    "start": "1606240",
    "end": "1611800"
  },
  {
    "text": "what they mean, it means. So yeah. \nThat's the populated table now,  ",
    "start": "1611800",
    "end": "1617520"
  },
  {
    "text": "and then so we can say that encoder-decoder \ncompared to the decoder-only architecture has  ",
    "start": "1617520",
    "end": "1623120"
  },
  {
    "text": "these additional structures inductive biases \nbuilt in, so let's go into each of them. ",
    "start": "1623120",
    "end": "1629360"
  },
  {
    "text": "The first one is what encoder-decoder tries added \nas a structure is that, input and the target  ",
    "start": "1629360",
    "end": "1635799"
  },
  {
    "text": "sequences are sufficiently different that we-- \nit will be useful to use a separate parameters,  ",
    "start": "1635800",
    "end": "1641160"
  },
  {
    "text": "that's the assumption. And so why is that a \nuseful? When can that assumption be useful?  ",
    "start": "1641160",
    "end": "1647600"
  },
  {
    "text": "And one example is machine translation. Back \nwhen the transformer was introduced in 2017,  ",
    "start": "1647600",
    "end": "1652960"
  },
  {
    "text": "translation was really popular task, and \nit was difficult, considered difficult.  And because it's a sequence-to-sequence and \nyou can actually have a blue score which is  ",
    "start": "1652960",
    "end": "1662160"
  },
  {
    "text": "heuristic based method that can give you a single \nnumber and then people can optimize that. So what  ",
    "start": "1662160",
    "end": "1668080"
  },
  {
    "text": "that in that task, we have this input and \ntarget in completely different languages. So   if the goal is to learn translation only, \nthen it kind of makes sense to have, OK,  ",
    "start": "1668080",
    "end": "1677400"
  },
  {
    "text": "this parameter in the encoder will take care of \nthe English, and this parameter in the decoder   will take care of the German, that seems natural.\nAnd what about now? Modern language models is  ",
    "start": "1677400",
    "end": "1688400"
  },
  {
    "text": "about learning knowledge. And it's not just \nabout translation or not even about language,   languages comes up as a byproduct of doing \nthis next token prediction and translation as  ",
    "start": "1688400",
    "end": "1698680"
  },
  {
    "text": "well. So does it make sense to have a separate \nparameter for this kind of situation now? We  ",
    "start": "1698680",
    "end": "1705440"
  },
  {
    "text": "have some knowledge in German, some knowledge in \nEnglish, and we-- if anything we want to combine  ",
    "start": "1705440",
    "end": "1711200"
  },
  {
    "text": "them. And if we represent them in a separate \nparameters, I don't think that's natural.  So I would say with this much more general, \nlarger models that can do a lot of things,  ",
    "start": "1711200",
    "end": "1722600"
  },
  {
    "text": "this assumption seems very unnatural to me. Second \nexample is a little bit more modern. Two years ago  ",
    "start": "1722600",
    "end": "1728720"
  },
  {
    "text": "when I was at Google and with Jason, we did this \ninstruction fine tuning work. And what this is,  ",
    "start": "1728720",
    "end": "1735840"
  },
  {
    "text": "is you take the pre-trained model, and then just \nfine tune on academic data set, and so that it  ",
    "start": "1735840",
    "end": "1741520"
  },
  {
    "text": "can understand the natural language instruction.\nSo the detail doesn't matter, but here let's think   about the performance gain by doing this fine \ntuning on two different architectures we tried. So  ",
    "start": "1741520",
    "end": "1753159"
  },
  {
    "text": "first five is the Flan-T5, which is T5 base which \nis the encoder-decoder architecture. Last one,  ",
    "start": "1753160",
    "end": "1759080"
  },
  {
    "text": "the latter five decoder-only architecture based \non PaLM. So we spent 99% of the time on PaLM,  ",
    "start": "1759080",
    "end": "1767039"
  },
  {
    "text": "optimizing a lot of these, and then at the \nend we just spent like three days on T5.  But the performance gain was a lot higher on \nthis. And I was really confused about this,  ",
    "start": "1767040",
    "end": "1776640"
  },
  {
    "text": "and in a very good way. And after the paper \nwas published, I wanted to take a little bit   deeper into why this might be the case. So my \nhypothesis is that, it's about the length. So  ",
    "start": "1776640",
    "end": "1789080"
  },
  {
    "text": "academic data sets we use, we use like 1,832 \ntasks. And here they have this very distinctive  ",
    "start": "1789080",
    "end": "1796039"
  },
  {
    "text": "characteristic where we have a long input, \nlong in order to make the task more difficult.  But then we cannot make the target long \nbecause if we do, there's no way to create it,  ",
    "start": "1796040",
    "end": "1806000"
  },
  {
    "text": "so there's fundamental challenge of that. So \nwhat happens is, you have a long text of input   and then short text of the target. And so this \nis kind of the length distribution of what it  ",
    "start": "1806000",
    "end": "1816120"
  },
  {
    "text": "went into the Flan fine tuning. So then you \nsee this, you have a very different sequence  ",
    "start": "1816120",
    "end": "1823520"
  },
  {
    "text": "going into the encoder as an input, and the very \ndifferent type of sequence going into the target.  So now, this encoder-decoder architecture has \nan assumption that they will be very different.  ",
    "start": "1823520",
    "end": "1833400"
  },
  {
    "text": "That structure really shines because of this. \nIt was a kind of an accident, but that was,  ",
    "start": "1833400",
    "end": "1839560"
  },
  {
    "text": "I think, why this really architecture was just \nsuitable for fine tuning with the academic data  ",
    "start": "1839560",
    "end": "1845760"
  },
  {
    "text": "sets. What about now? Do we care about this \nkind of assumption? And if you think about the  ",
    "start": "1845760",
    "end": "1852160"
  },
  {
    "text": "general use cases of language models nowadays, \nif anything the more interesting cases involve  ",
    "start": "1852160",
    "end": "1858000"
  },
  {
    "text": "longer generation, longer target.\nJust because we cannot grade them,  ",
    "start": "1858000",
    "end": "1863240"
  },
  {
    "text": "doesn't mean that we are not interested in them. \nActually, if anything we are more interested   in that. So now we have this longer target \nsituation, so this separate sequence length  ",
    "start": "1863240",
    "end": "1872720"
  },
  {
    "text": "parameter doesn't seem to make much sense. And \nmoreover, we think about this chat application  ",
    "start": "1872720",
    "end": "1878679"
  },
  {
    "text": "like ChatGPT, we do multi-turn conversation.\nAnd then so what is a target of this turn  ",
    "start": "1878680",
    "end": "1884440"
  },
  {
    "text": "becomes the input of the next turn. And then \nmy question is, does that make sense to even  ",
    "start": "1884440",
    "end": "1889519"
  },
  {
    "text": "think about a different parameters if next turn \nis going to be the same thing? So that was the  ",
    "start": "1889520",
    "end": "1897480"
  },
  {
    "text": "first inductive bias we just mentioned. And then \nthe second structure is that, target element can  ",
    "start": "1897480",
    "end": "1903240"
  },
  {
    "text": "only attend to the fully encoded ones, the \nfinal output of the encoder. Let's look at  ",
    "start": "1903240",
    "end": "1908640"
  },
  {
    "text": "this additional structure, what that means.\nSo as I mentioned, we have this very top  ",
    "start": "1908640",
    "end": "1914400"
  },
  {
    "text": "layer attending to it. And so in deep neural nets, \ntypically, we see that the bottom layers and the  ",
    "start": "1914400",
    "end": "1920880"
  },
  {
    "text": "top layers encode information at a very different \nlevel. Meaning that, for example, in computer  ",
    "start": "1920880",
    "end": "1926360"
  },
  {
    "text": "vision, lower layer-- the bottom layers encode \nsomething like edges, top layers, higher levels  ",
    "start": "1926360",
    "end": "1931920"
  },
  {
    "text": "combining the features, something like cat face.\nAnd so we call this deep learning a hierarchical  ",
    "start": "1931920",
    "end": "1937480"
  },
  {
    "text": "representation learning method. And so now the \nquestion is, if decoder layer one attends to  ",
    "start": "1937480",
    "end": "1945120"
  },
  {
    "text": "encoder final layer, which probably has \na very different level of information,   is that some kind of an information bottleneck \nwhich actually motivated the original attention  ",
    "start": "1945120",
    "end": "1954960"
  },
  {
    "text": "mechanism? And in practice I would say in my \nexperience doesn't really make any difference,  ",
    "start": "1954960",
    "end": "1960559"
  },
  {
    "text": "and that's because my experience was limited \nto say 25, 24 layers of encoder of T5. ",
    "start": "1960560",
    "end": "1966400"
  },
  {
    "text": "So layer one attended to 24, probably fine. But \nwhat if we have 10x or 1,000x more layers, would  ",
    "start": "1966400",
    "end": "1972200"
  },
  {
    "text": "that be problematic? I'm not really comfortable \nwith that. So I think this is also unnecessary  ",
    "start": "1972200",
    "end": "1977840"
  },
  {
    "text": "design that maybe we need to revisit. Final \nstructure we're going to talk about is the,  ",
    "start": "1979280",
    "end": "1985520"
  },
  {
    "text": "when we do this, there's a bi-directional thing \nin the encoder-decoder. Let's think about that. ",
    "start": "1985520",
    "end": "1990640"
  },
  {
    "text": "So yeah. Bi-directional input attention, is \nthat really necessary? So when we had this Bert,  ",
    "start": "1990640",
    "end": "1998400"
  },
  {
    "text": "B in Bert stands for bi-directional. 2018 when \nwe were solving like question answering squad,  ",
    "start": "1998400",
    "end": "2004400"
  },
  {
    "text": "actually it was very difficult task. So if you \nhave any additional trick, it can make a huge   difference. Bidirectionality was really useful, \nI think, maybe boosting up the squad score by  ",
    "start": "2004400",
    "end": "2015200"
  },
  {
    "text": "like 20, so it was really huge thing. But at \nscale, I don't think this matters that much. ",
    "start": "2015200",
    "end": "2020480"
  },
  {
    "text": "This is my highly anecdotal experience, so we \ndid in Flan-2, we tried both bi-directional and  ",
    "start": "2020480",
    "end": "2027240"
  },
  {
    "text": "uni-directional fine tuning, didn't really make \nmuch difference. So but I want to point out this  ",
    "start": "2027240",
    "end": "2033360"
  },
  {
    "text": "bidirectionality, actually bring in an engineering \nchallenge for modern multi-turn chat application.  ",
    "start": "2033360",
    "end": "2039559"
  },
  {
    "text": "So at every turn, the new input has to be encoded.\nAgain, and for uni-directional attention it's  ",
    "start": "2039560",
    "end": "2045280"
  },
  {
    "text": "much, much better, so here is what I mean by \nthat. So let's think about this more modern   conversation between user and assistant, \nhow are you? Bad, and why? And so here,  ",
    "start": "2045280",
    "end": "2055440"
  },
  {
    "text": "if we think about the bi-directional case, we \nwill-- and when we generate bad, we need to encode  ",
    "start": "2055440",
    "end": "2061280"
  },
  {
    "text": "this input with the bi-directional thing, which \nis fine. And then after the bad is generated,  ",
    "start": "2061280",
    "end": "2067360"
  },
  {
    "text": "when we're trying to generate why, we'll need to \nencode how, again, because how can attend to bad,  ",
    "start": "2067360",
    "end": "2074000"
  },
  {
    "text": "so we need to do everything from scratch again.\nIn contrast, if we do uni-directional one,  ",
    "start": "2074000",
    "end": "2079720"
  },
  {
    "text": "we can do much, much better because now when we \nare trying to generate why, we don't have to redo  ",
    "start": "2079720",
    "end": "2085800"
  },
  {
    "text": "how because we cannot attend to the future tokens, \nso we don't have to do anything. So if you see the  ",
    "start": "2085800",
    "end": "2092159"
  },
  {
    "text": "difference, this part can be cached and then this \npart is the only thing that can has to be encoded  ",
    "start": "2092160",
    "end": "2098000"
  },
  {
    "text": "again. So this kind of makes a big difference \nwhen we think about multiple turns going on. ",
    "start": "2098000",
    "end": "2103600"
  },
  {
    "text": "So I would say, bi-directional attention, that \nwell, in 2018, which is mostly sold by scale,  ",
    "start": "2103600",
    "end": "2109720"
  },
  {
    "text": "and now because of this engineering challenge, \nwe don't really need that. So to conclude,  ",
    "start": "2109720",
    "end": "2115680"
  },
  {
    "text": "we have looked into this driving force, \ndominant driving force covering this research,  ",
    "start": "2115680",
    "end": "2120760"
  },
  {
    "text": "and that was this exponentially cheaper \ncompute and associated scaling effort. ",
    "start": "2120760",
    "end": "2125880"
  },
  {
    "text": "And so to understand this driving force, we \nanalyzed some of the additional structures   added to the encoder-decoder compared to the \ndecoder only, and then thought about how--  ",
    "start": "2125880",
    "end": "2135280"
  },
  {
    "text": "what that means as a from the perspective of \nscaling. And I wanted to just conclude with  ",
    "start": "2135280",
    "end": "2141880"
  },
  {
    "text": "this remark. So we have looked at these kind \nof analysis, which are all-- one can say, this   is just historical artifacts and doesn't matter.\nBut if you do many of these, now you look at the  ",
    "start": "2141880",
    "end": "2153000"
  },
  {
    "text": "current events. You can hopefully think about \nthose in a more unified manner and then see,  ",
    "start": "2153000",
    "end": "2159000"
  },
  {
    "text": "OK, what assumptions in my problem that I need \nto revisit? And are they relevant? And if not,  ",
    "start": "2159000",
    "end": "2164680"
  },
  {
    "text": "why? And you have an answer to it. Is can we \ndo it with a more general thing and scale up?   And so I hope you can go back and really \nthink about these problems. And together  ",
    "start": "2164680",
    "end": "2174119"
  },
  {
    "text": "we can really shape the future of AI in \na really nice way, so that's it. Thanks.",
    "start": "2174120",
    "end": "2187560"
  }
]