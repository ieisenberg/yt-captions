[
  {
    "start": "0",
    "end": "5180"
  },
  {
    "text": "OK, so today, we're going\nto continue our discussion of energy-efficient computing.",
    "start": "5180",
    "end": "11537"
  },
  {
    "text": "At the end of the last lecture,\nwe talked about heterogeneity. And we said that the\nmotivation for heterogeneity",
    "start": "11537",
    "end": "16910"
  },
  {
    "text": "is because you had different\nkinds of program characteristics that could be exploited by\nmore specialized architectures.",
    "start": "16910",
    "end": "26550"
  },
  {
    "text": "And the key one that we've\nbeen focusing on so far is the idea of data\nparallel computation",
    "start": "26550",
    "end": "34520"
  },
  {
    "text": "that can be exploited by\narchitectures like GPUs. And the key capability you get\nis high performance and much",
    "start": "34520",
    "end": "44780"
  },
  {
    "text": "more importantly,\nenergy efficiency. And we're going to dig into\nenergy efficiency in a little",
    "start": "44780",
    "end": "51079"
  },
  {
    "text": "more detail today,\nand talk about why this is such a pressing\nconcern in modern computing",
    "start": "51080",
    "end": "57530"
  },
  {
    "text": "environments. So specialization\nand algorithms,",
    "start": "57530",
    "end": "63360"
  },
  {
    "text": "specific programming, which\nwould be the next step beyond a heterogeneous\ncompute environment,",
    "start": "63360",
    "end": "71050"
  },
  {
    "text": "would be one that's\nvery specialized for a particular application. So energy-efficient computing.",
    "start": "71050",
    "end": "77680"
  },
  {
    "text": "So we're constrained\nby energy today because of the state of the\nunderlying semiconductor",
    "start": "77680",
    "end": "85320"
  },
  {
    "text": "technology. So there was a time\nwhen as you got new generations of\nprocessing technology,",
    "start": "85320",
    "end": "94090"
  },
  {
    "text": "every generation gave\nyou more transistors. But those transistors\ndissipated less power.",
    "start": "94090",
    "end": "100540"
  },
  {
    "text": "And so you could\nget more performance for the same amount of power. This was called Dennard scaling.",
    "start": "100540",
    "end": "107020"
  },
  {
    "text": "So that ended\nabout 10 years ago. So, now, every time you increase\nthe number of transistors,",
    "start": "107020",
    "end": "112690"
  },
  {
    "text": "you dissipate more power. And now, you're constrained. So we're in this power,\nenergy-constrained environment,",
    "start": "112690",
    "end": "119520"
  },
  {
    "text": "and to understand\nhow this works. So energy is power times time.",
    "start": "119520",
    "end": "127170"
  },
  {
    "text": "So the amount of power\nwe have is fixed. I don't know why this\nis going on its own.",
    "start": "127170",
    "end": "132600"
  },
  {
    "text": "So we're at this point where in\norder to increase performance,",
    "start": "132600",
    "end": "139530"
  },
  {
    "text": "we have to decrease the amount\nof energy per operation.",
    "start": "139530",
    "end": "145300"
  },
  {
    "text": "So this is fundamentally\nwhere we are. Where if given a fixed amount\nof power, we can dissipate.",
    "start": "145300",
    "end": "152290"
  },
  {
    "text": "If we want more\nperformance, then we have to improve our\nenergy efficiency. And the key mechanism for\nincreasing energy efficiency",
    "start": "152290",
    "end": "160500"
  },
  {
    "text": "is to become more specialized. To get rid of the\nexcess power, we",
    "start": "160500",
    "end": "165569"
  },
  {
    "text": "dissipate by doing things\nthat don't focus on moving the computation forward.",
    "start": "165570",
    "end": "172530"
  },
  {
    "text": "All right. So why are we\nenergy-constrained?",
    "start": "172530",
    "end": "177900"
  },
  {
    "text": "Across the computing landscape,\nwe've got energy constraints. So if you're thinking\nabout supercomputers where",
    "start": "177900",
    "end": "183390"
  },
  {
    "text": "you've got thousands,\nor hundreds of thousands of cores in the data center,\nand you have to supply power",
    "start": "183390",
    "end": "193400"
  },
  {
    "text": "and cooling to keep the\nall the systems running.",
    "start": "193400",
    "end": "199920"
  },
  {
    "text": "And this, of course,\nhas a huge energy cost. So you're constrained in large\nsupercomputer environments.",
    "start": "199920",
    "end": "207330"
  },
  {
    "text": "You're also constrained\nin data centers behind the large websites\nlike Google and Facebook.",
    "start": "207330",
    "end": "217320"
  },
  {
    "text": "There, again, the cost\nof supplying the energy for powering and cooling\nthe computing possibly",
    "start": "217320",
    "end": "228050"
  },
  {
    "text": "over time, over say\nthe three-year lifetime of the computing resources is\nmore than the cost of actually",
    "start": "228050",
    "end": "236210"
  },
  {
    "text": "acquiring the computation. When we talk about mobile\ndevices, your energy constrained because, of course, you have\nno fan in your mobile device",
    "start": "236210",
    "end": "243830"
  },
  {
    "text": "because a fan would\nbe inconvenient. And so the heat dissipation\nhas to happen passively.",
    "start": "243830",
    "end": "251200"
  },
  {
    "text": "And then, of course,\nyou have a battery that has to provide the power\nor the energy to the compute.",
    "start": "251200",
    "end": "258430"
  },
  {
    "text": "And so, there, you're\nalso energy constraint. So across the\ncomputing landscape, your energy constraint.",
    "start": "258430",
    "end": "263760"
  },
  {
    "text": "So this is the equation\nwe were looking at. So energy, we said\nis power times time.",
    "start": "263760",
    "end": "270449"
  },
  {
    "text": "And we said power was fixed\nfor semiconductor processing",
    "start": "270450",
    "end": "277410"
  },
  {
    "text": "constraints. And so if we wanted to\nimprove performance, then we had to become\nmore energy efficient.",
    "start": "277410",
    "end": "284620"
  },
  {
    "text": "And the way to do that is to do\nspecialized functionality that reduces the overhead.",
    "start": "284620",
    "end": "289840"
  },
  {
    "text": "And the question is, what is the\nmagnitude of this improvement that you can get\nfrom specialization",
    "start": "289840",
    "end": "295650"
  },
  {
    "text": "over a general-purpose\nprocessing environment composed of CPUs? So let's dig into that.",
    "start": "295650",
    "end": "302130"
  },
  {
    "text": "And so we've already\nlooked at specializing for data-parallel applications\nusing GPUs at large scale.",
    "start": "302130",
    "end": "313090"
  },
  {
    "text": "And, of course, within\nthe architecture of a GPU,",
    "start": "313090",
    "end": "318560"
  },
  {
    "text": "we see SIMD processing,\nwhich is also exploiting data parallelism. And so the rules of\nthumb that you can",
    "start": "318560",
    "end": "327160"
  },
  {
    "text": "get a tremendous improvement. We'll come back to this about\nwhat improvement we can get.",
    "start": "327160",
    "end": "333879"
  },
  {
    "text": "But the question is,\nwe spent a lot of time talking in this class about how\nto get the most performance you",
    "start": "333880",
    "end": "341620"
  },
  {
    "text": "can for a particular algorithms\nfrom a modern CPU and GPUs.",
    "start": "341620",
    "end": "348760"
  },
  {
    "text": "So the question is, why are CPUs\nso fundamentally inefficient?",
    "start": "348760",
    "end": "354380"
  },
  {
    "text": "So let's look at this. So if you look at\nthe energy dissipated",
    "start": "354380",
    "end": "361240"
  },
  {
    "text": "in executing an instruction,\nsay a multiply-add,",
    "start": "361240",
    "end": "367460"
  },
  {
    "text": "you'll see that\nmost of the energy actually does not\ngo into performing",
    "start": "367460",
    "end": "373580"
  },
  {
    "text": "the actual computation. So, in this case, it's 6%. And the rest of\nthe energy is spent",
    "start": "373580",
    "end": "382520"
  },
  {
    "text": "dealing with the instruction\nand figuring out what the instruction is going to do.",
    "start": "382520",
    "end": "388129"
  },
  {
    "text": "Fetching and dealing with\nthe data, moving the data. And the overhead of\ncontrolling the circuitry",
    "start": "388130",
    "end": "397850"
  },
  {
    "text": "and distributing the clock,\nwhich, of course, keeps everything synchronous.",
    "start": "397850",
    "end": "403940"
  },
  {
    "text": "So if you look at\nall of the things that one has to do to\nexecute instruction,",
    "start": "403940",
    "end": "410172"
  },
  {
    "text": "you've got to read\nthe instruction. You've got to figure out what\nthe instruction is going to do. You've got to check to\nsee how the instruction is",
    "start": "410172",
    "end": "416450"
  },
  {
    "text": "dependent on other instructions\nthat are being executed. You've got to figure out\nwhether the resource that you want to use to execute this\ninstruction is available.",
    "start": "416450",
    "end": "424850"
  },
  {
    "text": "You've got to figure out\nwhere the operands are. You've got to fetch them\nfrom the register file.",
    "start": "424850",
    "end": "430633"
  },
  {
    "text": "If this happens to\nbe a load or a store, you may have to move\ndata from caches. And then way down here is the\nactual perform the arithmetic",
    "start": "430633",
    "end": "441080"
  },
  {
    "text": "operation. And then you still have\nto move the results. And so at the end\nof the day, you",
    "start": "441080",
    "end": "446819"
  },
  {
    "text": "end up spending very\nlittle of the energy for a particular instruction. And so the question is, how can\nyou make this situation better?",
    "start": "446820",
    "end": "455169"
  },
  {
    "text": "So how does SIMD make this\npie chart look better?",
    "start": "455170",
    "end": "461040"
  },
  {
    "text": "You have a better range of\ninstructions to arithmetic because you're only pushing\none instruction [INAUDIBLE].",
    "start": "461040",
    "end": "467820"
  },
  {
    "text": "Right. So you amortize all of\nthe parts that are not green over more green stuff.",
    "start": "467820",
    "end": "475139"
  },
  {
    "text": "So you are executing\nacross more data elements. And the width of\nyour SIMD is going",
    "start": "475140",
    "end": "481920"
  },
  {
    "text": "to tell you how efficient\npotentially you can be.",
    "start": "481920",
    "end": "487110"
  },
  {
    "text": "So if I can do eight data\noperations in one instruction,",
    "start": "487110",
    "end": "493360"
  },
  {
    "text": "why don't I do 16, or 32, or 64?",
    "start": "493360",
    "end": "499449"
  },
  {
    "text": "Yeah. It's hard to utilize wide SIMD. Right. Exactly. Right. The why do you make it.",
    "start": "499450",
    "end": "505820"
  },
  {
    "text": "Your peak is great,\nbut your average is going to be a lot\nworse because you may not",
    "start": "505820",
    "end": "511449"
  },
  {
    "text": "be able to fill up all\nof those SIMD slots. And so this is the question,\nhey, you can do better,",
    "start": "511450",
    "end": "520210"
  },
  {
    "text": "and you might go to extremes. But, ultimately,\nyou're not going to see the data parallelism\nor the SIMD data parallelism",
    "start": "520210",
    "end": "528070"
  },
  {
    "text": "that you need to keep\nall the SIMD units busy. All right. So the question is, does\nSIMD make improve things?",
    "start": "528070",
    "end": "534829"
  },
  {
    "text": "Well, it does. So this is a study\nfrom a few years back that was done at Stanford that\nlooked at how much energy gets",
    "start": "534830",
    "end": "547240"
  },
  {
    "text": "consumed in a SIMD-enabled CPU\nfor 264 video encoding, which",
    "start": "547240",
    "end": "558580"
  },
  {
    "text": "is a pretty data-parallel\nSIMD-friendly application. And you see that the\ncomponents of SIMD energy shown",
    "start": "558580",
    "end": "569660"
  },
  {
    "text": "by the red boxes\nis not that high. ",
    "start": "569660",
    "end": "578440"
  },
  {
    "text": "So the question is, if\nyou want to do better, then you need to think\nabout actually implementing",
    "start": "578440",
    "end": "585339"
  },
  {
    "text": "more specialized components. And so what we\nwant to look at is",
    "start": "585340",
    "end": "593420"
  },
  {
    "text": "to look at some other\ntypes of architecture. You see that the\nbest, in this case,",
    "start": "593420",
    "end": "600470"
  },
  {
    "text": "for doing Fast Fourier\nTransform, which is the core of many signal\nprocessing applications.",
    "start": "600470",
    "end": "608370"
  },
  {
    "text": "And it's a really\nwell-studied algorithm.",
    "start": "608370",
    "end": "613440"
  },
  {
    "text": "Some people call it the most\nimportant algorithm ever. And so you can think\nabout implementing",
    "start": "613440",
    "end": "620240"
  },
  {
    "text": "specialized hardware for that. And you can get a\ntremendous improvement in terms of the use of your\nsilicon area and the energy",
    "start": "620240",
    "end": "629720"
  },
  {
    "text": "efficiency. So, in this case, this is a\nfairly old study, 40 nanometers, which is ancient.",
    "start": "629720",
    "end": "635970"
  },
  {
    "text": "But what you see is that, the\nASIC, which is this diamond,",
    "start": "635970",
    "end": "645350"
  },
  {
    "text": "you can see these diamonds then\nrepresent in terms compared",
    "start": "645350",
    "end": "653240"
  },
  {
    "text": "to a CPU, which is the Core i7.",
    "start": "653240",
    "end": "658399"
  },
  {
    "text": "So the CPU is the\nCore i7, which is the lowest in terms of gigaFLOPS\nper millimeter squared.",
    "start": "658400",
    "end": "665810"
  },
  {
    "text": "And the star ASIC\nis the highest. So I reverse what I was\nsaying a moment ago.",
    "start": "665810",
    "end": "674580"
  },
  {
    "text": "And so the diamonds,\nwhich is the CPU gives you the lowest energy\nefficiency and the lowest use",
    "start": "674580",
    "end": "681050"
  },
  {
    "text": "of the chip area. And, basically, it's a factor\nof 1,000 in terms of the use",
    "start": "681050",
    "end": "689750"
  },
  {
    "text": "of the chip area and a factor\nof 100th in terms of energy efficiency that you get with a\nCPU versus something that's very",
    "start": "689750",
    "end": "697850"
  },
  {
    "text": "specialized for a\nparticular algorithm. So what's the downside\nof the ASIC approach?",
    "start": "697850",
    "end": "704480"
  },
  {
    "text": "Yeah. You can use it for that whole-- You can only use it\nfor that one algorithm.",
    "start": "704480",
    "end": "709860"
  },
  {
    "text": "And you have to design it. And so if you want to get\nyour application going, and you've got a\nnew idea, are you",
    "start": "709860",
    "end": "716310"
  },
  {
    "text": "going to wait 18 months\nto go design an ASIC? And then well, it better be\na really important algorithm",
    "start": "716310",
    "end": "723930"
  },
  {
    "text": "like FFT so in order to\njustify ASIC implementations.",
    "start": "723930",
    "end": "731279"
  },
  {
    "text": "But there are other ways\nof getting more efficiency than CPUs. One of the ideas that\ngets used extensively",
    "start": "731280",
    "end": "738480"
  },
  {
    "text": "is this idea of digital\nsignal processors, and that's the\nidea that, hey, you want to do a lot of\nprocessing of signals",
    "start": "738480",
    "end": "745260"
  },
  {
    "text": "using DSP algorithms like FFT\nand filtering IR filtering.",
    "start": "745260",
    "end": "751000"
  },
  {
    "text": "And it turns out\nthat the instructions and the addressing modes in\ngeneral-purpose computers",
    "start": "751000",
    "end": "757290"
  },
  {
    "text": "can be improved upon. So that's what DSPs do. They're very\ncomplex instructions",
    "start": "757290",
    "end": "763380"
  },
  {
    "text": "that do just what you want\nfor specific algorithms. They have very complex\naddressing modes",
    "start": "763380",
    "end": "770050"
  },
  {
    "text": "that allow you to do the\nbit reversed addressing that need for FFT, for example.",
    "start": "770050",
    "end": "776860"
  },
  {
    "text": "Now, the question\nis, if I gave you this complex instruction\nset, could you write a compiler for it?",
    "start": "776860",
    "end": "782630"
  },
  {
    "text": "And the answer is probably no. So along with the very\ncomplex instructions",
    "start": "782630",
    "end": "789430"
  },
  {
    "text": "that these DSPs have come\nwith low-level programming,",
    "start": "789430",
    "end": "795970"
  },
  {
    "text": "that has to be done\nfor implementing all these algorithms. But it's a hell of a lot\neasier than developing an ASIC.",
    "start": "795970",
    "end": "803090"
  },
  {
    "text": "But it's also much more\ndifficult than programming a general-purpose CPU.",
    "start": "803090",
    "end": "808310"
  },
  {
    "text": "So there are these\ntrade-offs now between efficiency and\nprogrammability that you get.",
    "start": "808310",
    "end": "815140"
  },
  {
    "text": "Another example of a\nspecialized compute unit is one that was\ndeveloped by DE Shaw.",
    "start": "815140",
    "end": "821290"
  },
  {
    "text": "So DE Shaw made a bunch of\nmoney in the financial area and decided that he was going to\nspend some of that money doing",
    "start": "821290",
    "end": "828699"
  },
  {
    "text": "things for humanity. And one of the things\nhe decided to do was develop a\nspecialized accelerator",
    "start": "828700",
    "end": "837700"
  },
  {
    "text": "for molecular dynamics. So if you want to understand\nhow proteins fold, it comes down to figuring\nout the interaction",
    "start": "837700",
    "end": "846820"
  },
  {
    "text": "between molecules. And so molecular dynamics is\nan important area in chemistry.",
    "start": "846820",
    "end": "856270"
  },
  {
    "text": "People have won Nobel\nPrizes for it, and so on. And so developed this\naccelerator called Anton.",
    "start": "856270",
    "end": "863600"
  },
  {
    "text": "And by carefully designing the\nalgorithm with the hardware,",
    "start": "863600",
    "end": "870110"
  },
  {
    "text": "they got tremendous performance\nimprovements over CPUs and GPUs. So you want to do\nan n-body simulation",
    "start": "870110",
    "end": "877750"
  },
  {
    "text": "given embodies figure out\nthe interaction between them. And they've got specialized\nhardware for doing that.",
    "start": "877750",
    "end": "886070"
  },
  {
    "text": "And I think they've got\nthree generations of Anton at this point. And each one is better.",
    "start": "886070",
    "end": "893570"
  },
  {
    "text": "And so the aside\nis, of course, there are ways of doing this\nwith accelerators.",
    "start": "893570",
    "end": "898662"
  },
  {
    "text": "But there are also ways\nof solving the problem statistically. And so there was this\ntension between the group of people who were\ndoing accelerators",
    "start": "898663",
    "end": "905290"
  },
  {
    "text": "and the group of\npeople who were doing broad-scale\nstatistical approaches.",
    "start": "905290",
    "end": "911980"
  },
  {
    "text": "These days, machine\nlearning is all the rage,",
    "start": "911980",
    "end": "917149"
  },
  {
    "text": "as you may have noticed. So, in fact, your last\nprogramming assignment was around machine learning.",
    "start": "917150",
    "end": "924530"
  },
  {
    "text": "One of the accelerators that\nkicked off a lot of the interest",
    "start": "924530",
    "end": "929650"
  },
  {
    "text": "in developing new architectures\nfor machine learning was the accelerator from Google\ncalled the tensor processing",
    "start": "929650",
    "end": "936370"
  },
  {
    "text": "unit. And the way to think about\nthe tensor processing unit is that it made dense matrix\nmultiply go very fast.",
    "start": "936370",
    "end": "944140"
  },
  {
    "text": "But dense matrix multiplies\nthat are large like 128.",
    "start": "944140",
    "end": "949270"
  },
  {
    "text": "Well, they start out at 256 by\n256 integer matrix multiplies.",
    "start": "949270",
    "end": "954850"
  },
  {
    "text": "And then future TPU versions,\nthey went to 128 by 128.",
    "start": "954850",
    "end": "962079"
  },
  {
    "text": "But then you were doing 16-bit\nfloating point multiplies.",
    "start": "962080",
    "end": "968296"
  },
  {
    "text": " This is a few years old\nin terms of the citations.",
    "start": "968296",
    "end": "977860"
  },
  {
    "text": "But lots of work in\nthe architecture area to try and understand\nhow to develop",
    "start": "977860",
    "end": "984880"
  },
  {
    "text": "new specific architectures for\nthe machine learning domain. So most of these\narchitectures are, in fact,",
    "start": "984880",
    "end": "993190"
  },
  {
    "text": "somewhat programmable because\nyou need to adapt to the changes",
    "start": "993190",
    "end": "998230"
  },
  {
    "text": "in the machine\nlearning algorithms. But fundamentally,\nthey are focused",
    "start": "998230",
    "end": "1004380"
  },
  {
    "text": "on doing the core, compute\nkernel in these ML algorithms,",
    "start": "1004380",
    "end": "1012170"
  },
  {
    "text": "which is matrix multiply. It could be dense. Most of the cases, it's dense. But there are sparse versions\nthat are interesting to.",
    "start": "1012170",
    "end": "1023260"
  },
  {
    "text": "All right. So there's this issue of\ndoing hardware that is fixed",
    "start": "1023260",
    "end": "1033660"
  },
  {
    "text": "for a particular algorithm. And the question is, is\nthere a middle ground that will allow you\nto develop hardware",
    "start": "1033660",
    "end": "1040290"
  },
  {
    "text": "that is somewhat programmable? So this is the whole\nreason and motivation",
    "start": "1040290",
    "end": "1047790"
  },
  {
    "text": "for hardware architectures\nthat are called",
    "start": "1047790",
    "end": "1053010"
  },
  {
    "text": "field programmable gate arrays. And some of you,\nof course, may have played with this technology\nin a digital design class.",
    "start": "1053010",
    "end": "1063360"
  },
  {
    "text": "And the key idea\nis that you've got a bunch of what are called\nconfigurable logic blocks, which",
    "start": "1063360",
    "end": "1069360"
  },
  {
    "text": "are basically lookup\ntables for Boolean Algebra.",
    "start": "1069360",
    "end": "1075580"
  },
  {
    "text": "So we'll give you some function\nof some number of inputs.",
    "start": "1075580",
    "end": "1081820"
  },
  {
    "text": "In this case, a four-input\nBoolean function can be computed using\nthe lookup table.",
    "start": "1081820",
    "end": "1088980"
  },
  {
    "text": "And then combined with\na combinational block is a register, which\ngives you storage.",
    "start": "1088980",
    "end": "1097370"
  },
  {
    "text": "And so then you put these kinds\nof configurable logic blocks in an array and you\nconnect them together.",
    "start": "1097370",
    "end": "1104750"
  },
  {
    "text": "And then you can connect them\ninto more complex logic blocks.",
    "start": "1104750",
    "end": "1112420"
  },
  {
    "text": "For instance, if you had a\n6-input lookup table and you wanted to generate\na 40-input AND gate,",
    "start": "1112420",
    "end": "1121130"
  },
  {
    "text": "you could cascade these 6-input\nlogic blocks together to create",
    "start": "1121130",
    "end": "1126880"
  },
  {
    "text": "a more complex function. So a lookup table basically\njust maps a binary number",
    "start": "1126880",
    "end": "1134980"
  },
  {
    "text": "to an output, and allows\nyou to compute functions of a different variety.",
    "start": "1134980",
    "end": "1141050"
  },
  {
    "text": "So modern FPGAs combine the\nconfigurable logic blocks",
    "start": "1141050",
    "end": "1148780"
  },
  {
    "text": "with more dedicated functions\nsuch as dense memory,",
    "start": "1148780",
    "end": "1156060"
  },
  {
    "text": "and also multiplies what\nare called DSP blocks. So the problem with\nconstructing everything out",
    "start": "1156060",
    "end": "1164700"
  },
  {
    "text": "of configurable logic\nblocks is it gives you the most flexibility. But it turns out there's\na lot of overhead.",
    "start": "1164700",
    "end": "1171309"
  },
  {
    "text": "The overhead in connecting\nthese blocks together. And there's the overhead\nof actually implementing",
    "start": "1171310",
    "end": "1176970"
  },
  {
    "text": "the compute elements using\nthis CLB sorts of technology.",
    "start": "1176970",
    "end": "1182740"
  },
  {
    "text": "And so if you want to have a\nmore dense, more efficient use of the silicon area, then you\nwant these hard macroblocks",
    "start": "1182740",
    "end": "1193679"
  },
  {
    "text": "for memory and for\nmultiplication.",
    "start": "1193680",
    "end": "1199750"
  },
  {
    "text": "And so you can combine\nthese together. And then if you actually\nwant to use them,",
    "start": "1199750",
    "end": "1207730"
  },
  {
    "text": "you could come visit my lab. I could show you\nhow to access them.",
    "start": "1207730",
    "end": "1215090"
  },
  {
    "text": "Or you could go to Amazon EC2. And they also provide\nFPGA resources.",
    "start": "1215090",
    "end": "1222760"
  },
  {
    "text": "And so they've got some quite\nadvanced FPGA capabilities that you can access\nusing cloud services.",
    "start": "1222760",
    "end": "1231740"
  },
  {
    "text": "And then these have both, of\ncourse, links to memory DDR-4.",
    "start": "1231740",
    "end": "1239320"
  },
  {
    "text": "We haven't said a\nlot about memory. But maybe if we have time\nat the end of this lecture,",
    "start": "1239320",
    "end": "1244820"
  },
  {
    "text": "we can talk about the different\nkinds of memory technologies interfaces to the CPUs, through\nPCIe, and links to other FPGAs.",
    "start": "1244820",
    "end": "1254559"
  },
  {
    "text": "And then they have\na whole environment that allows you to do the\nsoftware development in order",
    "start": "1254560",
    "end": "1263080"
  },
  {
    "text": "to actually program these FPGAs. So looking across the spectrum\nhere from easiest to program,",
    "start": "1263080",
    "end": "1273940"
  },
  {
    "text": "general purpose CPU to ASIC. We see this trade-off\nbetween energy efficiency",
    "start": "1273940",
    "end": "1282399"
  },
  {
    "text": "and programmability across the\nspace of computing technologies",
    "start": "1282400",
    "end": "1288310"
  },
  {
    "text": "that you could apply. And as a system\ndesigner, you need to pick the right one, which is\nwhat the constraints that you",
    "start": "1288310",
    "end": "1298120"
  },
  {
    "text": "have in terms of the\nenergy efficiency you need, and how quickly you need\nto get your application,",
    "start": "1298120",
    "end": "1303549"
  },
  {
    "text": "or how much effort you're\nwilling to spend to get your application working. So you can imagine that if\nyou can get the performance",
    "start": "1303550",
    "end": "1312460"
  },
  {
    "text": "you need with a CPU,\nhey, just go do it and program your application\nusing a high-level language.",
    "start": "1312460",
    "end": "1322090"
  },
  {
    "text": "If you need more\nperformance, then you keep going to the right GPUs.",
    "start": "1322090",
    "end": "1327710"
  },
  {
    "text": "You may have to write some CUDA. DSP. You might have to do assembly\nlanguage programming.",
    "start": "1327710",
    "end": "1333960"
  },
  {
    "text": "Domain specific compute. Well, this might work quite well\nif your domain is something that",
    "start": "1333960",
    "end": "1340139"
  },
  {
    "text": "might like machine learning. And you can program this\naccelerator using a framework like PyTorch or TensorFlow.",
    "start": "1340140",
    "end": "1347679"
  },
  {
    "text": "That might work. If you have to do an\nFPGA, then you basically",
    "start": "1347680",
    "end": "1353070"
  },
  {
    "text": "have to become a\nhardware designer. And an ASIC is definitely\nyour hardware designer.",
    "start": "1353070",
    "end": "1358660"
  },
  {
    "text": "So as you move to the right,\nyou get more energy efficiency dramatically more energy\nefficient if you do an ASIC.",
    "start": "1358660",
    "end": "1366870"
  },
  {
    "text": "But then you have\nto work much harder. And you've got to spend\na lot more money in order",
    "start": "1366870",
    "end": "1372060"
  },
  {
    "text": "to move to the right,\nespecially if you move all the way to the right.",
    "start": "1372060",
    "end": "1379080"
  },
  {
    "text": "Any questions so\nfar on the space of trade-offs between energy\nefficiency and programmability?",
    "start": "1379080",
    "end": "1389130"
  },
  {
    "text": "The different\npoints in the space. ",
    "start": "1389130",
    "end": "1396560"
  },
  {
    "text": "Yeah. I mean, obviously, if we're\ntalking about these things, it's like 1,000\n[INAUDIBLE] is much better.",
    "start": "1396560",
    "end": "1402690"
  },
  {
    "text": "But in the middle there, I think\nabout, OK, you can get the TPU.",
    "start": "1402690",
    "end": "1408090"
  },
  {
    "text": "But why not just get\ntwo GPUs in one monitor?",
    "start": "1408090",
    "end": "1413630"
  },
  {
    "text": "It's probably way less\nexpensive to get a couple. Well, I mean, I think the\njury is out between whether--",
    "start": "1413630",
    "end": "1422210"
  },
  {
    "text": "I mean, clearly, the GPU is\ntaking pages out of the TPU.",
    "start": "1422210",
    "end": "1427799"
  },
  {
    "text": "Said, OK, it was this\ngeneral-purpose thread thing. But let's put these\ntensor core units in",
    "start": "1427800",
    "end": "1434390"
  },
  {
    "text": "and make it more specialized. But now, you as\na CUDA programmer try to program those\ntensor core units.",
    "start": "1434390",
    "end": "1440383"
  },
  {
    "text": "I mean, we didn't do\nthat in this class. But it's actually\npretty challenging. So the space is\ninflux, and it's driven",
    "start": "1440383",
    "end": "1448790"
  },
  {
    "text": "by this really high-value\napplication called machine learning. So everybody's tilting their\narchitecture to exploit that.",
    "start": "1448790",
    "end": "1457679"
  },
  {
    "text": "Yeah. Why is the variable\nDSP higher than GPUs?",
    "start": "1457680",
    "end": "1463059"
  },
  {
    "text": "Because the DSP can use GPU\nmore efficiently, on average. Well, I mean, it tends to\nbe more focused on DSP.",
    "start": "1463060",
    "end": "1472320"
  },
  {
    "text": "And it's got a lot of\nspecialized addressing mechanisms and compute for that.",
    "start": "1472320",
    "end": "1478900"
  },
  {
    "text": "So given that you're trying to\ndo digital signal processing, it's going to be more efficient. But if you're trying to do\nmachine learning, probably not.",
    "start": "1478900",
    "end": "1486490"
  },
  {
    "text": "Yeah.  All right.",
    "start": "1486490",
    "end": "1492200"
  },
  {
    "text": "So, now, let's look at what it\nwould take to move to the right",
    "start": "1492200",
    "end": "1498289"
  },
  {
    "text": "a little bit more. So we spent a lot of time\nin this class thinking about how we program the fixed\nset of resources that we provide",
    "start": "1498290",
    "end": "1506390"
  },
  {
    "text": "you in existing architecture,\nsuch as a general-purpose",
    "start": "1506390",
    "end": "1511580"
  },
  {
    "text": "processor, or a GPU. But, now, let's\nthink a little bit",
    "start": "1511580",
    "end": "1517820"
  },
  {
    "text": "about what it would take to\neither specify or program",
    "start": "1517820",
    "end": "1524809"
  },
  {
    "text": "accelerator where you get\nto specify a bunch of things",
    "start": "1524810",
    "end": "1530060"
  },
  {
    "text": "that you don't get\nto control if you're thinking about a\ngeneral-purpose environment.",
    "start": "1530060",
    "end": "1535400"
  },
  {
    "text": "In particular, you get to have\nsome custom memory system.",
    "start": "1535400",
    "end": "1540470"
  },
  {
    "text": "A lot of the\nperformance improvement you can get in any\nparticular piece of hardware",
    "start": "1540470",
    "end": "1546350"
  },
  {
    "text": "has to do with how you\norganize the memory to exploit the particular\ncharacteristics of locality",
    "start": "1546350",
    "end": "1552980"
  },
  {
    "text": "and access behavior that\nyou see in your application. And you get a\nspecialized compute",
    "start": "1552980",
    "end": "1561110"
  },
  {
    "text": "that matches what you\nneed in your application. So the question is,\nhow do we think about",
    "start": "1561110",
    "end": "1566870"
  },
  {
    "text": "or how do we program specialized\nprocesses or accelerators?",
    "start": "1566870",
    "end": "1573450"
  },
  {
    "text": "So, traditionally, you had\nto become a hardware designer and think about things\nthat at the level of what's",
    "start": "1573450",
    "end": "1580970"
  },
  {
    "text": "called the register\ntransfer level, or the hardware\ndescription level. So you had to write in\nlanguages like VHDL or Verilog.",
    "start": "1580970",
    "end": "1589650"
  },
  {
    "text": "How many people have\nwritten Verilog?  Good fair number of you.",
    "start": "1589650",
    "end": "1594930"
  },
  {
    "text": "So you understand the pain\ninvolved in programming at the Verilog level. Now, recently, there\nhas been this idea",
    "start": "1594930",
    "end": "1603350"
  },
  {
    "text": "called high-level synthesis. And the approach is, hey, I'm\ngoing to write a C program.",
    "start": "1603350",
    "end": "1608789"
  },
  {
    "text": "And then I'm going to\nhave some smart compiler to convert that into hardware.",
    "start": "1608790",
    "end": "1615120"
  },
  {
    "text": "There are two things\nwrong with this idea. One is that C programs\nwere not intended to be descriptions of hardware.",
    "start": "1615120",
    "end": "1622360"
  },
  {
    "text": "So you have to make\nall kinds of inferences about what the hardware\nshould be doing",
    "start": "1622360",
    "end": "1627370"
  },
  {
    "text": "because the C\nprogram was designed for a general-purpose processor. It was not designed\nfor hardware.",
    "start": "1627370",
    "end": "1632890"
  },
  {
    "text": "So that's the first problem. The second problem\nis that in order to get around the deficiencies\nof C, they put in these pragmas.",
    "start": "1632890",
    "end": "1642210"
  },
  {
    "text": "So you get to direct the\ncompiler to do certain things. Well, the problem is\nputting all these pragmas,",
    "start": "1642210",
    "end": "1648730"
  },
  {
    "text": "you essentially have to\nknow a lot about hardware to put in the pragmas\nin the right way. And so you've defeated the\nwhole purpose of rising up",
    "start": "1648730",
    "end": "1659640"
  },
  {
    "text": "to the level of C. And\nthen, in fact, in order to get anything that's\nworthwhile and performs well,",
    "start": "1659640",
    "end": "1667690"
  },
  {
    "text": "you've got to descend down\nto the level of hardware by using these pragmas. So, today, instead of looking\nat high-level synthesis, what",
    "start": "1667690",
    "end": "1677059"
  },
  {
    "text": "we want to look at as a language\nthat we call spatial, which is a high-level language\nfor designing hardware",
    "start": "1677060",
    "end": "1683750"
  },
  {
    "text": "accelerators that's\ndesigned to enable performance-oriented\nprogrammers to specify hardware.",
    "start": "1683750",
    "end": "1692160"
  },
  {
    "text": "So everybody in this\nclass at this point is a performance-oriented\nprogrammer. That's what you've\nbeen doing all quarter.",
    "start": "1692160",
    "end": "1698639"
  },
  {
    "text": "And so you guys qualify. And so the key thing that\nperformance-oriented programmers",
    "start": "1698640",
    "end": "1704930"
  },
  {
    "text": "like to think about\nis parallelism. And locality is what\nwe've been dealing",
    "start": "1704930",
    "end": "1711020"
  },
  {
    "text": "with in the whole quarter. And in terms of\nlocality, we want",
    "start": "1711020",
    "end": "1716570"
  },
  {
    "text": "to think maybe about\nsome specialized memories and how you do\nthe data movement. So here's spatial.",
    "start": "1716570",
    "end": "1722550"
  },
  {
    "text": "So the quick one-slide\ndescription of spatial",
    "start": "1722550",
    "end": "1727910"
  },
  {
    "text": "is it's designed for a\ndomain-specific language",
    "start": "1727910",
    "end": "1735200"
  },
  {
    "text": "for accelerator design. And it has contracts to express\nparallel patterns, which you're",
    "start": "1735200",
    "end": "1741680"
  },
  {
    "text": "also pretty familiar with. So data parallel patterns-- sorry, data parallel\npatterns over collections.",
    "start": "1741680",
    "end": "1749850"
  },
  {
    "text": "So map, zip, reduce. These are all concepts\nthat you're familiar with.",
    "start": "1749850",
    "end": "1755424"
  },
  {
    "text": "And what we want\nto do is we want to think about how to execute\nthese parallel patterns using",
    "start": "1755425",
    "end": "1761030"
  },
  {
    "text": "two types of parallelism, one\nthat you're very familiar with, which is independent\nparallelism.",
    "start": "1761030",
    "end": "1766889"
  },
  {
    "text": "So thinking about taking\na map and running the map with independent\ncomputation units.",
    "start": "1766890",
    "end": "1774230"
  },
  {
    "text": "And the other is\ndependent parallelism, which I think some of you\nare quite familiar with, too.",
    "start": "1774230",
    "end": "1780830"
  },
  {
    "text": "Dependent parallelism is where\nyou've got parallel units that are dependent on each other.",
    "start": "1780830",
    "end": "1786090"
  },
  {
    "text": "So how would you execute\ndependent parallel units-- ",
    "start": "1786090",
    "end": "1794420"
  },
  {
    "text": "those of you who-- ",
    "start": "1794420",
    "end": "1799980"
  },
  {
    "text": "how do you execute a\nset of a computation",
    "start": "1799980",
    "end": "1809120"
  },
  {
    "text": "in which the components of\nthe computation are, in fact, dependent?",
    "start": "1809120",
    "end": "1814590"
  },
  {
    "text": "Yeah. You do some of\ndynamic scheduling, but maybe a direct pool of--",
    "start": "1814590",
    "end": "1820930"
  },
  {
    "text": "Maybe. It's a concept we haven't\ntalked about explicitly here. So it's not something that you--\nunless you've heard about it",
    "start": "1820930",
    "end": "1827639"
  },
  {
    "text": "in some other contexts. Maybe a hardware design context. Yeah. Would you do what you do with\nthe CPU instruction pipeline?",
    "start": "1827640",
    "end": "1834900"
  },
  {
    "text": "Exactly. Right. So the different components\nof an instruction execution pipeline are all dependent.",
    "start": "1834900",
    "end": "1840700"
  },
  {
    "text": "But you've got a bunch of\nindependent instructions. And you execute\nthem the same way you would if you were\ndoing in a factory",
    "start": "1840700",
    "end": "1848130"
  },
  {
    "text": "and you were working on a car. You create an assembly line. And each of the stations\ndo things independently.",
    "start": "1848130",
    "end": "1856360"
  },
  {
    "text": "And then you get parallelism\nacross the different sections of the pipeline. So pipelining is the other\nway of doing parallelism",
    "start": "1856360",
    "end": "1865650"
  },
  {
    "text": "where you've got dependencies. So we want to look at how\nto do pipeline parallelism.",
    "start": "1865650",
    "end": "1873240"
  },
  {
    "text": "Parallel patterns\ncan be nested so you can get hierarchical control. We said that one of the key\nmechanisms that a hardware",
    "start": "1873240",
    "end": "1882909"
  },
  {
    "text": "designer or somebody who\nwants to control the locality",
    "start": "1882910",
    "end": "1891190"
  },
  {
    "text": "or exploit locality\nin the application is to be able to explicitly\nspecify the memory hierarchy",
    "start": "1891190",
    "end": "1898000"
  },
  {
    "text": "and how that gets used. There's also this\nnotion of being able to look at the whole\ndesign space using parameters.",
    "start": "1898000",
    "end": "1905410"
  },
  {
    "text": "And you want to expose\nthese to the compiler and allow the compiler\nto potentially explore",
    "start": "1905410",
    "end": "1912580"
  },
  {
    "text": "the design space for you. So the key here is that\nlet's focus on what",
    "start": "1912580",
    "end": "1919870"
  },
  {
    "text": "is interesting and\nimportant in terms of getting high performance,\nwhich as we said repeatedly,",
    "start": "1919870",
    "end": "1926090"
  },
  {
    "text": "is about how you exploit\nparallelism both independent parallelism and\ndependent parallelism,",
    "start": "1926090",
    "end": "1931970"
  },
  {
    "text": "and how you manage and\nfigure out the locality.",
    "start": "1931970",
    "end": "1937580"
  },
  {
    "text": "And I would claim that it's\nmore intuitive than thinking about things from a thread level\nfor these kinds of applications",
    "start": "1937580",
    "end": "1946090"
  },
  {
    "text": "that you might see in a\nmachine learning context.",
    "start": "1946090",
    "end": "1951610"
  },
  {
    "text": "All right. So let's talk about\nthe spatial language.",
    "start": "1951610",
    "end": "1956680"
  },
  {
    "text": "And let's start with\nthe memory templates. So, as I said, you have this\nexplicit memory hierarchy.",
    "start": "1956680",
    "end": "1963230"
  },
  {
    "text": "So you get to specify\nwhat memory is on chip.",
    "start": "1963230",
    "end": "1968890"
  },
  {
    "text": " My pens back. And what memory is off chips.",
    "start": "1968890",
    "end": "1976010"
  },
  {
    "text": "So you might have SRAM on chip,\nand you might have a data type.",
    "start": "1976010",
    "end": "1982550"
  },
  {
    "text": "In this case, unsigned in 8. ",
    "start": "1982550",
    "end": "1988930"
  },
  {
    "text": "In this case, it's an array. So you might specify\nhow many elements.",
    "start": "1988930",
    "end": "1995680"
  },
  {
    "text": "And then you can\nalso specify DRAM.",
    "start": "1995680",
    "end": "2001610"
  },
  {
    "text": "All right. In this case, again,\nit's 8-bit value.",
    "start": "2001610",
    "end": "2007500"
  },
  {
    "text": "And this is a\ntwo-dimensional array. So you have, in this\ncase, image and buffer.",
    "start": "2007500",
    "end": "2013950"
  },
  {
    "text": "And then, of course,\nyou've got radishes. You've got variety of\ndifferent kinds of radishes.",
    "start": "2013950",
    "end": "2019320"
  },
  {
    "text": "You've got accumulators. You've got FIFOs,\nwhich are just queues.",
    "start": "2019320",
    "end": "2024640"
  },
  {
    "text": "So we'll say a lot\nabout using FIFOs. You might, if you're\ndoing image processing,",
    "start": "2024640",
    "end": "2030310"
  },
  {
    "text": "have the idea of a\nline buffer, which is this two-dimensional array\nthat can be shifted by lines.",
    "start": "2030310",
    "end": "2037270"
  },
  {
    "text": "And then you might\nhave a shift register, which is similar in\nspirit to the line buffer.",
    "start": "2037270",
    "end": "2044790"
  },
  {
    "text": "All right. So when we're dealing with CPU,\nonly the main address space,",
    "start": "2044790",
    "end": "2054460"
  },
  {
    "text": "the address space of memory that\nis visible to the programmer. And then you as the\nprogrammer can write",
    "start": "2054460",
    "end": "2062250"
  },
  {
    "text": "code that is cache-friendly. But you don't control how data\nmoves between the main memory",
    "start": "2062250",
    "end": "2068190"
  },
  {
    "text": "and the cache. That's handled automatically\nby the underlying hardware controller.",
    "start": "2068190",
    "end": "2073750"
  },
  {
    "text": "In spatial, that's not the case. You, the programmer,\nhave to explicitly move data back and forth\nbetween the different levels",
    "start": "2073750",
    "end": "2080610"
  },
  {
    "text": "of the memory hierarchy. And in this case, you're\nmoving data from the DRAM,",
    "start": "2080610",
    "end": "2086919"
  },
  {
    "text": "from the image to the buffer\nwith a load operation.",
    "start": "2086920",
    "end": "2093119"
  },
  {
    "text": "So this is a dense\ndata movement. The next is a gather.",
    "start": "2093120",
    "end": "2099610"
  },
  {
    "text": "So we've talked about gather. So can someone tell me\nhow gather works here?",
    "start": "2099610",
    "end": "2104825"
  },
  {
    "text": " It was taking data\nfrom [INAUDIBLE].",
    "start": "2104825",
    "end": "2111662"
  },
  {
    "text": "So why didn't it point\ntogether with this? So you're saying that you're\ngoing to get it from image.",
    "start": "2111663",
    "end": "2118559"
  },
  {
    "text": "And, in this case, 10 elements. And the addresses\nor the locations",
    "start": "2118560",
    "end": "2124160"
  },
  {
    "text": "are going to be specified\nin some array A. So, essentially, you\nare taking sparse data",
    "start": "2124160",
    "end": "2132740"
  },
  {
    "text": "and making it dense in buffer. So you can imagine\nthere's load, and gather,",
    "start": "2132740",
    "end": "2140120"
  },
  {
    "text": "and then there's\nstore, and scatter. ",
    "start": "2140120",
    "end": "2146510"
  },
  {
    "text": "And then you can\nalso create streams. And you can stream\ndata in and out.",
    "start": "2146510",
    "end": "2153599"
  },
  {
    "text": "And streaming will be a key\ncomponent of getting efficiency.",
    "start": "2153600",
    "end": "2158860"
  },
  {
    "text": "All right. What about control templates? Well, the idea is that-- by the way, the spatial\nlanguage is embedded",
    "start": "2158860",
    "end": "2167450"
  },
  {
    "text": "in a language called Scala. For historical reasons,\nwe won't get into it.",
    "start": "2167450",
    "end": "2172920"
  },
  {
    "text": "But Scala is actually\na very nice language to embed DSLs into it\nbecause it's very flexible.",
    "start": "2172920",
    "end": "2179600"
  },
  {
    "text": "We've actually seen Scala when\nwe were talking about Spark. So you have, in\nfact, seen it before.",
    "start": "2179600",
    "end": "2186960"
  },
  {
    "text": "And so it was very\npopular at one time as embedding languages go.",
    "start": "2186960",
    "end": "2192330"
  },
  {
    "text": "But it has certain deficiencies\naround the use of the JVM, which limited its\nwidespread use.",
    "start": "2192330",
    "end": "2203240"
  },
  {
    "text": "So you've got\nthese Accel blocks, which are going to divide\nyour program into parts",
    "start": "2203240",
    "end": "2210240"
  },
  {
    "text": "that are accelerated and parts\nthat just run on the CPU. And the question is whether\nyou run the Accel block once",
    "start": "2210240",
    "end": "2217020"
  },
  {
    "text": "or whether you run\nit continuously, which is the Accel star syntax.",
    "start": "2217020",
    "end": "2222460"
  },
  {
    "text": "And then there's this idea\nof finite state machines, which we are not going\nto focus on at all.",
    "start": "2222460",
    "end": "2228820"
  },
  {
    "text": "What we will focus on\nis the key mechanisms for doing parallel\npatterns, which is for each,",
    "start": "2228820",
    "end": "2235450"
  },
  {
    "text": "which is essentially a map. And then reduce,\nwhich is a reduce. And that says, for\nall the elements in C,",
    "start": "2235450",
    "end": "2245010"
  },
  {
    "text": "and you're going to\nstep through it by 1, do the following code, which\nis the block of for loop.",
    "start": "2245010",
    "end": "2259960"
  },
  {
    "text": "The core for loop as\nspecified within the braces.",
    "start": "2259960",
    "end": "2265744"
  },
  {
    "text": " So there are a bunch of design\nparameters that you can specify.",
    "start": "2265745",
    "end": "2277400"
  },
  {
    "text": "You can specify how\nmuch particular for each",
    "start": "2277400",
    "end": "2285529"
  },
  {
    "text": "and reduces a paralyzed. You can specify how\nthey get scheduled.",
    "start": "2285530",
    "end": "2290580"
  },
  {
    "text": "You can say that you\nwant this to be pipelined or you want it to be streamed. And we'll say a little bit about\neach of these in just a moment.",
    "start": "2290580",
    "end": "2298440"
  },
  {
    "text": "You can specify\nparameters such as want the size of the buffer to be\narranged the default to be 64.",
    "start": "2298440",
    "end": "2308640"
  },
  {
    "text": "But you might want the\nrange to be 64 to 1,024.",
    "start": "2308640",
    "end": "2313910"
  },
  {
    "text": "And that can be explored then\npotentially by the compiler.",
    "start": "2313910",
    "end": "2321510"
  },
  {
    "text": "All right. So if you specify\nthings that require",
    "start": "2321510",
    "end": "2327770"
  },
  {
    "text": "the use of memory banking, the\ncompiler will handle it for you. So if you paralyze something,\nand the parallelization implies",
    "start": "2327770",
    "end": "2335450"
  },
  {
    "text": "that you have multiple accesses\nto a particular memory unit,",
    "start": "2335450",
    "end": "2341040"
  },
  {
    "text": "then it's the responsibility\nof the compiler to make sure that\nyou can actually achieve that parallelization\nfactor by duplicating",
    "start": "2341040",
    "end": "2349160"
  },
  {
    "text": "memories or figuring out how to\nbank the memories appropriately. But that's a detail that\nyou don't have to consider.",
    "start": "2349160",
    "end": "2356609"
  },
  {
    "text": "So that's something that\nthe compiler deals for you. All right. So let's look at an example\nto bring these concepts home.",
    "start": "2356610",
    "end": "2364730"
  },
  {
    "text": "So we're going to\ndo inner product, your favorite little kernel.",
    "start": "2364730",
    "end": "2370160"
  },
  {
    "text": "And we want to build an\naccelerator in spatial. And so we can have these three.",
    "start": "2370160",
    "end": "2375530"
  },
  {
    "text": "We can have the code here. We have the sketch of the\ngenerated hardware below.",
    "start": "2375530",
    "end": "2380809"
  },
  {
    "text": "And let's see what happens. So let's start with the C code\njust to make sure everybody's",
    "start": "2380810",
    "end": "2388020"
  },
  {
    "text": "clear. So we're going to malloc\ntwo vectors, vec1 and vec2. And then we are going to\ncompute the inner product using",
    "start": "2388020",
    "end": "2399870"
  },
  {
    "text": "the simple for loop. We multiply each\nof the elements. And we add them all together. OK, that's clear.",
    "start": "2399870",
    "end": "2406240"
  },
  {
    "text": "So that's what you would do if\nyou're writing for a C code. So what are you going to\ndo if you want to build",
    "start": "2406240",
    "end": "2412470"
  },
  {
    "text": "an accelerator for this? Well, so, remember, you now\nhave to control all the memory.",
    "start": "2412470",
    "end": "2419320"
  },
  {
    "text": "So let's assume that vec1 and\nvec2 are integer arrays in DRAM. And so it's clear here by\nthe specification of DRAM",
    "start": "2419320",
    "end": "2428820"
  },
  {
    "text": "that these two arrays are\ngoing to live in DRAM. And now, we haven't said\nexactly how DRAM works.",
    "start": "2428820",
    "end": "2437020"
  },
  {
    "text": "But let's assume\nthat we have a way of moving data between the\nDRAM and the accelerator using",
    "start": "2437020",
    "end": "2444390"
  },
  {
    "text": "direct memory access, which is\nan efficient way of moving data between the main memory\nand the accelerator.",
    "start": "2444390",
    "end": "2452530"
  },
  {
    "text": "And so we have an\nAccel block, which is where we are going\nto do the acceleration.",
    "start": "2452530",
    "end": "2460420"
  },
  {
    "start": "2460420",
    "end": "2467440"
  },
  {
    "text": "All right. So the first thing\nwe're going to do is we need to move data from\nthe DRAM into the accelerator.",
    "start": "2467440",
    "end": "2479579"
  },
  {
    "text": "And we need a place\nfor that data to land. So we have to define some data\nstructure within the accelerator",
    "start": "2479580",
    "end": "2488690"
  },
  {
    "text": "for that. And we're going to create two\nSRAM blocks for this purpose.",
    "start": "2488690",
    "end": "2494540"
  },
  {
    "text": "Tile1 and tile2. And these are going to\nhave size, tile size.",
    "start": "2494540",
    "end": "2500930"
  },
  {
    "text": "And they are going\nto be in SRAM.",
    "start": "2500930",
    "end": "2506540"
  },
  {
    "text": "And there's a question of\nhow large they should be. But let's talk about\nthat in just a moment.",
    "start": "2506540",
    "end": "2513859"
  },
  {
    "text": "So then the first\nthing we need to do is we need to think\nabout, because we are going to do these\nthings by tiling,",
    "start": "2513860",
    "end": "2521400"
  },
  {
    "text": "we need a doubly nested loop. Single nest won't\nwork because we're",
    "start": "2521400",
    "end": "2527299"
  },
  {
    "text": "going to compute our inner\nproduct using by tiles. So why would we want\nto fetch a tile of data",
    "start": "2527300",
    "end": "2539030"
  },
  {
    "text": "from DRAM instead\nof single elements? Yeah.",
    "start": "2539030",
    "end": "2544450"
  },
  {
    "text": "You're better with caching\nand you can [INAUDIBLE] it. You're going to\nget much better use",
    "start": "2544450",
    "end": "2549600"
  },
  {
    "text": "of the interface between the\nDRAM and the accelerator. It's like going to\nthe grocery store.",
    "start": "2549600",
    "end": "2554800"
  },
  {
    "text": "You never just pick up one thing\nthat would be really wasteful. You take the effort to go all\nthe way to the grocery store.",
    "start": "2554800",
    "end": "2560830"
  },
  {
    "text": "You get a whole bunch of\nthings, and bring them, and put them in your\nfridge, or your pantry",
    "start": "2560830",
    "end": "2566550"
  },
  {
    "text": "so that you don't have to\ngo back to the grocery store every time you want\nto eat something.",
    "start": "2566550",
    "end": "2573250"
  },
  {
    "text": "So same idea here. It's costly to go to DRAM. You want to get\nmore than one thing.",
    "start": "2573250",
    "end": "2580240"
  },
  {
    "text": "You might want to get a whole\ntile-sized amount of data.",
    "start": "2580240",
    "end": "2586060"
  },
  {
    "text": "And so you need to have a place\nin your memory to hold that.",
    "start": "2586060",
    "end": "2592540"
  },
  {
    "text": "So, of course, if\nthis was a CPU, this might just be\ngeneral purpose cache.",
    "start": "2592540",
    "end": "2598240"
  },
  {
    "text": "And the movement of\ndata would be controlled by the caching algorithms. Here, you get to explicitly move\nthe data and program the data",
    "start": "2598240",
    "end": "2606330"
  },
  {
    "text": "movement. All right. So the first thing\nnow we do is we're",
    "start": "2606330",
    "end": "2611520"
  },
  {
    "text": "going to have a\nreduction over tile size. Because at the\nend of the day, we",
    "start": "2611520",
    "end": "2618240"
  },
  {
    "text": "need to reduce the elements\nusing addition in order",
    "start": "2618240",
    "end": "2624450"
  },
  {
    "text": "to generate the output. And so, first of all, we're\ngoing to load the two vectors.",
    "start": "2624450",
    "end": "2638560"
  },
  {
    "text": "Load a tile-sized element\nof data from vec into tile1 and a tile-sized element\nof vector 2 into tile 2.",
    "start": "2638560",
    "end": "2648830"
  },
  {
    "text": "Then we are going to reduce\nwithin the tile in step 2,",
    "start": "2648830",
    "end": "2661210"
  },
  {
    "text": "and then reduce across\nthe tiles in step 3. So, now, we've got\nthis three-step process",
    "start": "2661210",
    "end": "2672050"
  },
  {
    "text": "where we load a tile, we do\nthe intra tile accumulate, and then we do the\naccumulate across the tiles.",
    "start": "2672050",
    "end": "2679620"
  },
  {
    "text": "OK, so now the question\nis I want to improve the performance of my hardware.",
    "start": "2679620",
    "end": "2684710"
  },
  {
    "text": "And for that, I'm going to\nneed to exploit parallelism.",
    "start": "2684710",
    "end": "2689940"
  },
  {
    "text": "So where is the parallelism\nin this algorithm? ",
    "start": "2689940",
    "end": "2695830"
  },
  {
    "text": "Yeah. It's like you can\ndo multiple-- you can do steps 1, 2, and 3 at the\nsame time for different parts",
    "start": "2695830",
    "end": "2702370"
  },
  {
    "text": "of the data. OK, so that would be an\nexample of pipelining. So you could pipeline that.",
    "start": "2702370",
    "end": "2707830"
  },
  {
    "text": "So that that's one place that\nwe could exploit parallelism in a product representation.",
    "start": "2707830",
    "end": "2716609"
  },
  {
    "text": "Where else? Yeah. You can reduce across\nmultiple tiles at once. So within each tile,\nyou'd reduce [INAUDIBLE].",
    "start": "2716610",
    "end": "2725790"
  },
  {
    "text": "OK, so. So you're saying, so within\neach tile, we can do what?",
    "start": "2725790",
    "end": "2732480"
  },
  {
    "text": "So this innermost reduce can\noccur across multiple tiles at once.",
    "start": "2732480",
    "end": "2737990"
  },
  {
    "text": "Wait. The innermost reduce\nis only within a tile. Yes.",
    "start": "2737990",
    "end": "2743750"
  },
  {
    "text": "I'm sorry. You can do that\nseveral times at all. Yeah, right. So you want to\nparallelize step 2.",
    "start": "2743750",
    "end": "2750059"
  },
  {
    "text": "Yes. OK. So we can parallelize step 2. What might be the best\nway to parallelize",
    "start": "2750060",
    "end": "2756500"
  },
  {
    "text": "step 2 given what you know? What would be the most efficient\nway of parallelizing step 2?",
    "start": "2756500",
    "end": "2763779"
  },
  {
    "start": "2763780",
    "end": "2770950"
  },
  {
    "text": "Pull something\nout of your pocket that you are very familiar with. ",
    "start": "2770950",
    "end": "2780070"
  },
  {
    "text": "Yeah. It seems like you're always\ndoing the same instruction. So SIMD. SIMD.",
    "start": "2780070",
    "end": "2785210"
  },
  {
    "text": "SIMD would be a good way of\nparallelizing step 2 since we're doing the same operation to all\nof the elements in the tile.",
    "start": "2785210",
    "end": "2794570"
  },
  {
    "start": "2794570",
    "end": "2800000"
  },
  {
    "text": "So we are doing this\nmultiplication and reduction.",
    "start": "2800000",
    "end": "2805640"
  },
  {
    "text": "So if you want to think\nabout this reduce, if you want to paralyze\nthis reduce, what do you do?",
    "start": "2805640",
    "end": "2812900"
  },
  {
    "text": "You're going to need some\nparallel multiply followed by a reduction tree.",
    "start": "2812900",
    "end": "2818525"
  },
  {
    "start": "2818525",
    "end": "2824510"
  },
  {
    "text": "So spatial allows\nyou to do that. It's got this\nnotion of reduction.",
    "start": "2824510",
    "end": "2830660"
  },
  {
    "text": "It's got this notion\nof reduction trees. And so you can paralyze by 2. In this case, there's\nnot much of a tree.",
    "start": "2830660",
    "end": "2837390"
  },
  {
    "text": "But you're doing two multiplies. But you could go wider. Go as wide as you want.",
    "start": "2837390",
    "end": "2842580"
  },
  {
    "text": "And what would be the\ndownside of going wider? The reduction tree is larger.",
    "start": "2842580",
    "end": "2849590"
  },
  {
    "text": "Reduction tree is larger? What else is larger? Hardware. What? Hardware.",
    "start": "2849590",
    "end": "2854894"
  },
  {
    "text": "The hardware. You use more resources. You get to control.",
    "start": "2854894",
    "end": "2860300"
  },
  {
    "text": "You say, hey, I want\nmore parallelism. There's no free lunch here.",
    "start": "2860300",
    "end": "2865550"
  },
  {
    "text": "You're going to\nuse more resources. But you get to control\nhow many resources you",
    "start": "2865550",
    "end": "2871250"
  },
  {
    "text": "use based on how much\nperformance improvement want. So let's hold this idea of\npipelining for just a second.",
    "start": "2871250",
    "end": "2881490"
  },
  {
    "text": "But you could also\ncontrol the tile size such that you could\ndecide how much data",
    "start": "2881490",
    "end": "2888470"
  },
  {
    "text": "you want to fetch every time you\ngo to memory and optimize that.",
    "start": "2888470",
    "end": "2893820"
  },
  {
    "text": "So you could specify what\ntile size you want to use.",
    "start": "2893820",
    "end": "2898910"
  },
  {
    "text": "And lastly, this\nidea of you say, hey, instead of running the outer\nreduce one step at a time,",
    "start": "2898910",
    "end": "2909839"
  },
  {
    "text": "let's overlap them by\nspecifying a pipeline schedule. So pipelining here would\nsay, now, I want to overlap.",
    "start": "2909840",
    "end": "2918640"
  },
  {
    "text": "Now, what is the key thing that\nyou need for pipelining to work?",
    "start": "2918640",
    "end": "2923769"
  },
  {
    "text": "And it's shown in\nthis picture here. And maybe you could read. ",
    "start": "2923770",
    "end": "2931522"
  },
  {
    "text": "[INAUDIBLE] use like\nthe [INAUDIBLE] hazards.",
    "start": "2931522",
    "end": "2937170"
  },
  {
    "text": "Yeah. But we know there\nare no hazards. But what extra\nresources do we need?",
    "start": "2937170",
    "end": "2944160"
  },
  {
    "text": "Yeah. We need [INAUDIBLE]\nis just [INAUDIBLE].",
    "start": "2944160",
    "end": "2950820"
  },
  {
    "text": "We need some double buffering. So while stage one is working on\nsome filling data from memory,",
    "start": "2950820",
    "end": "2960540"
  },
  {
    "text": "stage 2 has to be\nable to work on data that has been generated from the\nprevious execution of stage 1.",
    "start": "2960540",
    "end": "2968980"
  },
  {
    "text": "So you essentially need\nsome double buffering. And so that's extra.",
    "start": "2968980",
    "end": "2974710"
  },
  {
    "text": "So pipelining is as\nclose to a free lunch as you might get in hardware. But it's not completely\nfree because you add memory.",
    "start": "2974710",
    "end": "2983849"
  },
  {
    "text": "So you decide the pipeline,\nand you get, in this case-- best case would be\npipeline to a depth of 3.",
    "start": "2983850",
    "end": "2991770"
  },
  {
    "text": "And you get a 3x\nimprovement in performance. That's, of course,\nnot always true.",
    "start": "2991770",
    "end": "2997690"
  },
  {
    "text": "But the overhead to doing\nthat would be the extra tile",
    "start": "2997690",
    "end": "3004190"
  },
  {
    "text": "memory that you\nneed at every stage interface in order to\nmake sure that data",
    "start": "3004190",
    "end": "3012290"
  },
  {
    "text": "doesn't get overwritten while\nyou're doing the pipelining. Does everybody follow that?",
    "start": "3012290",
    "end": "3019250"
  },
  {
    "text": "Good. So we saw three types\nof optimization,",
    "start": "3019250",
    "end": "3025010"
  },
  {
    "text": "parallelization, how we\ndeal with the data locality, and pipelining.",
    "start": "3025010",
    "end": "3032210"
  },
  {
    "text": "So just to make sure that\nyou all understand then. So spatial programmers\nresponsibility.",
    "start": "3032210",
    "end": "3038780"
  },
  {
    "text": "What, as a spatial programmer,\nare you responsible for? ",
    "start": "3038780",
    "end": "3051280"
  },
  {
    "text": "Yeah. What you're actually doing. Yeah. Can you be a little\nmore specific about what you're\nactually doing?",
    "start": "3051280",
    "end": "3056970"
  },
  {
    "text": "Basically, how do you want the--\nif you have some [INAUDIBLE], how you want to compose that\ninto the different types",
    "start": "3056970",
    "end": "3062170"
  },
  {
    "text": "of hardware, like ways that-- So you have got to be able to\nexpress your algorithm in using",
    "start": "3062170",
    "end": "3071859"
  },
  {
    "text": "for each and reduce constructs. What else are you\nresponsible for?",
    "start": "3071860",
    "end": "3077900"
  },
  {
    "text": "Yeah. Handling memory. Handling memory. Explicit memory\nhierarchy, figuring out where the data should live\nin the different memories",
    "start": "3077900",
    "end": "3087609"
  },
  {
    "text": "that you define. What else? Specifying parallelism. Specifying parallelism.",
    "start": "3087610",
    "end": "3093420"
  },
  {
    "text": "How much and where? ",
    "start": "3093420",
    "end": "3099960"
  },
  {
    "text": "I think that's about it. Specifying the algorithm,\nspecifying the memory hierarchy,",
    "start": "3099960",
    "end": "3105839"
  },
  {
    "text": "doing the explicit\ndata movement, and then picking the\ntiling factors parallelism, and scheduling.",
    "start": "3105840",
    "end": "3112120"
  },
  {
    "text": "And then the compiler's\nresponsibility is this banking and\nbuffering of memories",
    "start": "3112120",
    "end": "3118309"
  },
  {
    "text": "to maximize the performance,\nand minimize resources, and some lower level things\nabout generating configurations",
    "start": "3118310",
    "end": "3127760"
  },
  {
    "text": "for explicit targets. And, of course, if you\nwant to improve performance",
    "start": "3127760",
    "end": "3134105"
  },
  {
    "text": "and understand performance,\nyou need some way of getting feedback\nabout the performance that your particular\ncode can achieve",
    "start": "3134105",
    "end": "3142910"
  },
  {
    "text": "on any one of these\ntargets, and then what resources they might use.",
    "start": "3142910",
    "end": "3148609"
  },
  {
    "text": "So spatial is being\nused to convert TensorFlow\nrepresentations of machine",
    "start": "3148610",
    "end": "3157370"
  },
  {
    "text": "learning algorithms\ninto hardware.",
    "start": "3157370",
    "end": "3162510"
  },
  {
    "text": "I think more interesting\nmight be something that you're very\nfamiliar with, which",
    "start": "3162510",
    "end": "3168210"
  },
  {
    "text": "is how to optimize an\nalgorithm like attention.",
    "start": "3168210",
    "end": "3173750"
  },
  {
    "text": "So this is something that\nyou've just been thinking about. So we talked about\nfused attention.",
    "start": "3173750",
    "end": "3180420"
  },
  {
    "text": "And so what was the big\nbenefit of fused attention?",
    "start": "3180420",
    "end": "3185460"
  },
  {
    "text": "Yeah. You don't have to materialize\nthe attention matrix. You don't have to materialize\nthe field attention matrix.",
    "start": "3185460",
    "end": "3191970"
  },
  {
    "text": "You tile things into blocks. And then you compute\na block at a time.",
    "start": "3191970",
    "end": "3201580"
  },
  {
    "text": "And then you also get\nthis idea of fusing the different components\nof the attention algorithm",
    "start": "3201580",
    "end": "3210900"
  },
  {
    "text": "together and minimizing memory\nbandwidth by doing that.",
    "start": "3210900",
    "end": "3217900"
  },
  {
    "text": "And then also, you minimize\nthe memory bandwidth.",
    "start": "3217900",
    "end": "3225000"
  },
  {
    "text": "And that gives you the benefit. And so you get this performance\nand memory size benefits.",
    "start": "3225000",
    "end": "3240100"
  },
  {
    "text": "So it turns out that if\nyou write things using this spatial programming\nstreaming, programming model,",
    "start": "3240100",
    "end": "3252560"
  },
  {
    "text": "you can get a lot\nof these benefits with a simpler programming\nmodel, a model where",
    "start": "3252560",
    "end": "3258700"
  },
  {
    "text": "you don't have to write\nan explicit fused kernel.",
    "start": "3258700",
    "end": "3264079"
  },
  {
    "text": "So let's see how that works. So let's go back to the\ntime before FlashAttention.",
    "start": "3264080",
    "end": "3273130"
  },
  {
    "text": "And before FlashAttention, you\nhad this kernel-based execution",
    "start": "3273130",
    "end": "3279549"
  },
  {
    "text": "model. And the FlashAttention,\nas we said, prevents the materialization\nof the full matrix.",
    "start": "3279550",
    "end": "3286750"
  },
  {
    "text": "With the streaming\nexecution model, you also get these benefits. But you didn't have to write\nthe FlashAttention kernel.",
    "start": "3286750",
    "end": "3296750"
  },
  {
    "text": "And, in particular, you didn't\nhave to pay extra computation. So it turns out that to\ndeal with the softmax,",
    "start": "3296750",
    "end": "3310820"
  },
  {
    "text": "you had to do extra computation\nin order to deal with the fact",
    "start": "3310820",
    "end": "3316550"
  },
  {
    "text": "that you have this\nrow computation-- that you need the whole row in\norder to compute the softmax.",
    "start": "3316550",
    "end": "3325290"
  },
  {
    "text": "And it turns out that\nwith streaming, you can get away from doing this at\nthe cost of having potentially",
    "start": "3325290",
    "end": "3333619"
  },
  {
    "text": "a little bit more memory. All right. So let's see how that works.",
    "start": "3333620",
    "end": "3338820"
  },
  {
    "text": "So if we think about\nsoftmax, as you know, it's actually a\nthree-step procedure.",
    "start": "3338820",
    "end": "3346130"
  },
  {
    "text": "So, first of all, you've got\nto compute the exponential for the particular value of Sij.",
    "start": "3346130",
    "end": "3358030"
  },
  {
    "text": "And then you have to do\nthe row-wise reduction.",
    "start": "3358030",
    "end": "3365890"
  },
  {
    "text": "And then you have to do the\ndivision of the exponential",
    "start": "3365890",
    "end": "3371019"
  },
  {
    "text": "by the row-wise information. And so this three-step process\nis shown here pictorially.",
    "start": "3371020",
    "end": "3382160"
  },
  {
    "text": "So first the exponential,\nthen the reduction which is row-wise and\nthen the division.",
    "start": "3382160",
    "end": "3389740"
  },
  {
    "text": "So if you do this without the\noptimization of FlashAttention,",
    "start": "3389740",
    "end": "3400090"
  },
  {
    "text": "then you have this\nmaterialization of the whole matrix. ",
    "start": "3400090",
    "end": "3407000"
  },
  {
    "text": "And, of course, that increases\nyour memory footprint and increases your\nmemory bandwidth.",
    "start": "3407000",
    "end": "3413340"
  },
  {
    "start": "3413340",
    "end": "3419320"
  },
  {
    "text": "And so this shows\nyou the overview, and it shows all the data that\nhas to be both materialized",
    "start": "3419320",
    "end": "3426640"
  },
  {
    "text": "and moved between the\naccelerator, which you can think is happening up top, and the\nGPU memory, which is happening,",
    "start": "3426640",
    "end": "3435890"
  },
  {
    "text": "which is below. And so all the data that\ncrosses this line then",
    "start": "3435890",
    "end": "3443590"
  },
  {
    "text": "is memory bandwidth that\nhas to be used in order",
    "start": "3443590",
    "end": "3449230"
  },
  {
    "text": "to compute the attention.",
    "start": "3449230",
    "end": "3455740"
  },
  {
    "text": "So when the streaming\nexecution model, you can avoid the\nmaterialization of the matrix.",
    "start": "3455740",
    "end": "3462950"
  },
  {
    "text": "So let's show you how that\nworks, how the streaming works. So, essentially,\nin this example,",
    "start": "3462950",
    "end": "3470150"
  },
  {
    "text": "we're going to compute\nthe exponential. And then we are going to compute\nthe row sum by reducing the row.",
    "start": "3470150",
    "end": "3481130"
  },
  {
    "text": "And so the way that you\nwould write this in spatial is that you're going to\nhave the first for each,",
    "start": "3481130",
    "end": "3489050"
  },
  {
    "text": "which is a map do the\ncomputation of the exponential.",
    "start": "3489050",
    "end": "3495970"
  },
  {
    "text": "But then instead of putting\nthe output into another matrix,",
    "start": "3495970",
    "end": "3504080"
  },
  {
    "text": "we're just going to enqueue\nthe output in a FIFO. ",
    "start": "3504080",
    "end": "3510190"
  },
  {
    "text": "And so the semantics of spatial\nare this for each and this",
    "start": "3510190",
    "end": "3516920"
  },
  {
    "text": "for each are executing\nat the same time. So, now, you can think\nof the first for each",
    "start": "3516920",
    "end": "3527120"
  },
  {
    "text": "as being the producer. And the second for\neach is consuming",
    "start": "3527120",
    "end": "3532490"
  },
  {
    "text": "the output of the producer. And so, essentially,\nthe reduction",
    "start": "3532490",
    "end": "3540860"
  },
  {
    "text": "then happens by dequeuing an\nelement from the first for each",
    "start": "3540860",
    "end": "3549920"
  },
  {
    "text": "and having this continuous sum.",
    "start": "3549920",
    "end": "3555869"
  },
  {
    "text": "And then, finally,\nwhen you're done, you generate the\noutput also in a FIFO.",
    "start": "3555870",
    "end": "3565880"
  },
  {
    "text": "So this is a single element. So does everybody\nfollow how this works? You essentially have\nthese two Foreaches",
    "start": "3565880",
    "end": "3572230"
  },
  {
    "text": "and they're operating\nin a pipeline. And between the\npipeline are a FIFO.",
    "start": "3572230",
    "end": "3580880"
  },
  {
    "text": "So, initially, what happens is\nwe define on chip memory for S.",
    "start": "3580880",
    "end": "3590890"
  },
  {
    "text": "And we define the two FIFOs. ",
    "start": "3590890",
    "end": "3597270"
  },
  {
    "text": "We do the end queue. We compute the\nexponential element here.",
    "start": "3597270",
    "end": "3609690"
  },
  {
    "text": "And then we do end queue\nthe data on the FIFO.",
    "start": "3609690",
    "end": "3618900"
  },
  {
    "text": "We de queue the FIFO\nin the second for each.",
    "start": "3618900",
    "end": "3627640"
  },
  {
    "text": "And so this just shows how\nthings work with streaming.",
    "start": "3627640",
    "end": "3634400"
  },
  {
    "text": "So before streaming, you would\nmaterialize the whole of the N",
    "start": "3634400",
    "end": "3639910"
  },
  {
    "text": "by N matrix with streaming. Data just moves through. In this case, the\ntwo element FIFO.",
    "start": "3639910",
    "end": "3646310"
  },
  {
    "text": "So this is the equivalent\nof the double buffering that we showed in\nthe first example.",
    "start": "3646310",
    "end": "3651440"
  },
  {
    "text": "But here, you've got\nan explicit FIFO. And so because you have figured\nout that that's all you need,",
    "start": "3651440",
    "end": "3658315"
  },
  {
    "text": "you get a tremendous reduction\nin the amount of memory you need. But in order for this to\nwork, your programming model",
    "start": "3658315",
    "end": "3665200"
  },
  {
    "text": "has to think about\nthese two kernels operating at the same\ntime being connected",
    "start": "3665200",
    "end": "3671140"
  },
  {
    "text": "with a FIFO, which is a\nvery natural hardware way to think about things.",
    "start": "3671140",
    "end": "3676490"
  },
  {
    "text": "But it's not a natural software\nway to think about things. So spatial allows you to think\nabout this idea of pipeline",
    "start": "3676490",
    "end": "3687040"
  },
  {
    "text": "parallelism in ways that\nmatch what you would want",
    "start": "3687040",
    "end": "3692900"
  },
  {
    "text": "to do in an efficient\nhardware implementation, which don't match what you typically\nwould do with software.",
    "start": "3692900",
    "end": "3700880"
  },
  {
    "text": "So back to the original\nkernel by kernel scheme",
    "start": "3700880",
    "end": "3708349"
  },
  {
    "text": "in which you are\nmaterializing all the memory, you've got all this memory\nthat gets materialized, you've got all this data\nmovement that's happening,",
    "start": "3708350",
    "end": "3715680"
  },
  {
    "text": "which we showed was unnecessary. And that if you do\nthings with streaming,",
    "start": "3715680",
    "end": "3721280"
  },
  {
    "text": "then the data can just\nmove through FIFOs between the different kernels.",
    "start": "3721280",
    "end": "3728000"
  },
  {
    "text": "Because one kernel\nputs data in a FIFO, the next kernel picks it\nup, and does the compute.",
    "start": "3728000",
    "end": "3734990"
  },
  {
    "text": "And you never have to\nmaterialize the whole matrix. And in cases where you\nneed data, for instance,",
    "start": "3734990",
    "end": "3742893"
  },
  {
    "text": "for doing the row\noperation, then you've got to be able to accumulate a\nwhole row of data in your FIFO.",
    "start": "3742893",
    "end": "3748890"
  },
  {
    "text": "And so the limit\nis that you need to be able to have\nthis FIFO be the length",
    "start": "3748890",
    "end": "3754490"
  },
  {
    "text": "of a row of the matrix. Now, that could\npotentially become a limit.",
    "start": "3754490",
    "end": "3761040"
  },
  {
    "text": "You can go through the details. And the details will be clear.",
    "start": "3761040",
    "end": "3767970"
  },
  {
    "text": "And so the question\nis, so you need-- this is just showing that\nyou need a row of data",
    "start": "3767970",
    "end": "3779390"
  },
  {
    "text": "in order to compute the P matrix\nor an element of the P matrix.",
    "start": "3779390",
    "end": "3786744"
  },
  {
    "text": " And you can look\nat the [INAUDIBLE].",
    "start": "3786745",
    "end": "3792530"
  },
  {
    "text": "And so the question is, could we\ndo better with FlashAttention? And the answer is yes.",
    "start": "3792530",
    "end": "3798470"
  },
  {
    "text": "So there's still room\nfor optimizations like FlashAttention. Because, maybe at some\npoint, if your matrix size,",
    "start": "3798470",
    "end": "3806900"
  },
  {
    "text": "your sequence length gets really\nlong, then even a row of data is too much.",
    "start": "3806900",
    "end": "3812410"
  },
  {
    "text": "And so if you want to limit\nthe size of your FIFO,",
    "start": "3812410",
    "end": "3818260"
  },
  {
    "text": "you can apply FlashAttention to\na streaming-based optimization.",
    "start": "3818260",
    "end": "3832230"
  },
  {
    "text": "FlashAttention reorders\nthe operations, and uses a running sum, and\nrescaling instead of naive",
    "start": "3832230",
    "end": "3837750"
  },
  {
    "text": "reduction in order to\ndo this computation. And now, we can\ndramatically reduce the need",
    "start": "3837750",
    "end": "3845640"
  },
  {
    "text": "for this row amount\nof data in our FIFO.",
    "start": "3845640",
    "end": "3852740"
  },
  {
    "text": "All right. So if you compare streaming\nversus kernel by kernel,",
    "start": "3852740",
    "end": "3863609"
  },
  {
    "text": "so what you get with a\nstreaming implementation",
    "start": "3863610",
    "end": "3871270"
  },
  {
    "text": "is you get this idea that you\ncan exploit more parallelism",
    "start": "3871270",
    "end": "3877090"
  },
  {
    "text": "because you have this\nability to overlap the computation of the\nkernels between each other.",
    "start": "3877090",
    "end": "3884500"
  },
  {
    "text": "Using this pipeline\ntype of execution, you get the ability to\nexploit more parallelism.",
    "start": "3884500",
    "end": "3892059"
  },
  {
    "text": "And you can spatially\nmap each computation",
    "start": "3892060",
    "end": "3897310"
  },
  {
    "text": "with pipeline communication. And so this is what you get with\na streaming execution model.",
    "start": "3897310",
    "end": "3903980"
  },
  {
    "text": "And then you can\noverlap and pipeline the computation for\ndifferent output tiles.",
    "start": "3903980",
    "end": "3910460"
  },
  {
    "text": "So you get that extra dimension\nof parallelism and performance",
    "start": "3910460",
    "end": "3915880"
  },
  {
    "text": "with a streaming implementation. The other benefit\nthat you potentially get with a streaming\nimplementation",
    "start": "3915880",
    "end": "3923260"
  },
  {
    "text": "is that you don't have\nto explicitly create a fused kernel.",
    "start": "3923260",
    "end": "3929730"
  },
  {
    "text": "So you can imagine\nthat these kernels are implemented individually. And you can either\nuse the capability",
    "start": "3929730",
    "end": "3937730"
  },
  {
    "text": "of the compiler to do this\ndouble buffering technique. But then if you want even\nfurther optimization,",
    "start": "3937730",
    "end": "3944280"
  },
  {
    "text": "then you can replace\nthe double buffer with a FIFO like\nprogramming expression,",
    "start": "3944280",
    "end": "3952530"
  },
  {
    "text": "as I just showed\nyou in this example, and get even more efficiency. And that's easier than creating\nthis explicitly fused kernel",
    "start": "3952530",
    "end": "3963020"
  },
  {
    "text": "as you would have to do with a\ntraditional programming model.",
    "start": "3963020",
    "end": "3968880"
  },
  {
    "text": "So the streaming\nexecution model gives you this extra degree of freedom. Operations get\nfused automatically",
    "start": "3968880",
    "end": "3975680"
  },
  {
    "text": "if you write things using FIFOs. Or even if you write\nthem using buffers,",
    "start": "3975680",
    "end": "3981600"
  },
  {
    "text": "the double buffering\ntechnique can be employed. And then the compiler\ncan automatically",
    "start": "3981600",
    "end": "3987859"
  },
  {
    "text": "generate the fused execution. So any questions here? Yeah. So just to confirm, the\nstreaming execution model",
    "start": "3987860",
    "end": "3995110"
  },
  {
    "text": "is independent from the\nnotion of accelerated design? You can write a\nstreaming program, but it can run on pre-existing.",
    "start": "3995110",
    "end": "4002220"
  },
  {
    "text": "Yeah, it can. It can. I mean, you can\nimagine it running on existing architecture.",
    "start": "4002220",
    "end": "4009940"
  },
  {
    "text": "And the question is whether-- so it's very difficult in CUDA\nto write a streaming program.",
    "start": "4009940",
    "end": "4017010"
  },
  {
    "text": "I mean, you fundamentally\ndon't have the ability to have different parts of the\nkernel operating independently",
    "start": "4017010",
    "end": "4023760"
  },
  {
    "text": "and in ways. And so what you\ntypically would write is you would write\nsome of fused kernel.",
    "start": "4023760",
    "end": "4030280"
  },
  {
    "text": "And so you can imagine that-- I think if you talk to the\npeople developing CUDA,",
    "start": "4030280",
    "end": "4037450"
  },
  {
    "text": "they're trying to enable\nthis kind of execution. But I don't know\nhow to do it yet.",
    "start": "4037450",
    "end": "4042670"
  },
  {
    "text": "I'm not saying it will\nnever be possible, but it's not possible today.",
    "start": "4042670",
    "end": "4048760"
  },
  {
    "text": "Yeah. In execution model,\nwhat is the FIFO depth? It's getting too large, then\nis there an alternative to it",
    "start": "4048760",
    "end": "4056589"
  },
  {
    "text": "screening without a FIFO? ",
    "start": "4056590",
    "end": "4061994"
  },
  {
    "text": "Like, I could get it to break\nfor the resistors to accumulate. Yeah. So then you could use buffering.",
    "start": "4061994",
    "end": "4069339"
  },
  {
    "text": "You could just use SRAM\ninstead of registers. ",
    "start": "4069340",
    "end": "4075200"
  },
  {
    "text": "And you might have to write\nyour application slightly differently. Or you would try to think\nabout using techniques",
    "start": "4075200",
    "end": "4082250"
  },
  {
    "text": "that fundamentally reduce the\nFIFO size like FlashAttention.",
    "start": "4082250",
    "end": "4087660"
  },
  {
    "text": "So you can get it in a brain\ndead way just by using a FIFO.",
    "start": "4087660",
    "end": "4095030"
  },
  {
    "text": "And you don't have\nto work too hard. But then, eventually, maybe\nthe FIFO gets too big, and then you have to\nwork harder, and do",
    "start": "4095030",
    "end": "4101689"
  },
  {
    "text": "something that actually\ntransforms the application. Yeah. So just to clarify, the examples\nyou gave were using spatial?",
    "start": "4101689",
    "end": "4111250"
  },
  {
    "text": "But spatial is going to\ngive you [INAUDIBLE].",
    "start": "4111250",
    "end": "4117490"
  },
  {
    "text": "No, you could target\nall sorts of things. You could target? Yeah. ",
    "start": "4117490",
    "end": "4126830"
  },
  {
    "text": "It was intended as\na hardware model. It works very well for a\nnew style of architecture",
    "start": "4126830",
    "end": "4136270"
  },
  {
    "text": "that we've defined called\nreconfigurable data flow. It doesn't really match GPUs.",
    "start": "4136270",
    "end": "4142818"
  },
  {
    "text": "Of course, make it run on CPUs. ",
    "start": "4142819",
    "end": "4149880"
  },
  {
    "text": "Other questions? But the whole point\nI want you to get is this idea of the\nstreaming execution",
    "start": "4149880",
    "end": "4156028"
  },
  {
    "text": "model, this idea of\nhaving kernels that operate in parallel at the same\ntime in a pipeline execution",
    "start": "4156029",
    "end": "4163680"
  },
  {
    "text": "mode, and the notion that you\ncan get the benefit of fusing and tiling without\nit being explicit.",
    "start": "4163680",
    "end": "4170165"
  },
  {
    "start": "4170165",
    "end": "4176660"
  },
  {
    "text": "So the accelerated summary is\na significant energy efficiency improvements from acceleration.",
    "start": "4176660",
    "end": "4183469"
  },
  {
    "text": "So you can get 100\nto 1000x improvement.",
    "start": "4183470",
    "end": "4188620"
  },
  {
    "text": "Designing accelerators\nis all about understanding your\napplication, which is something",
    "start": "4188620",
    "end": "4193778"
  },
  {
    "text": "that we've been focusing on\nthe whole of this course, and then figuring out how to\nexploit the specific parallelism",
    "start": "4193779",
    "end": "4202840"
  },
  {
    "text": "and locality that is\nexhibited by your application.",
    "start": "4202840",
    "end": "4207860"
  },
  {
    "text": "And we have seen that. But now in the context\nof accelerator design,",
    "start": "4207860",
    "end": "4214550"
  },
  {
    "text": "you get to define\nexplicitly what resources are going to be used\nto exploit parallelism",
    "start": "4214550",
    "end": "4222490"
  },
  {
    "text": "and then how the\nmemory hierarchy is going to be designed\nto make sure that you can get maximum locality.",
    "start": "4222490",
    "end": "4230020"
  },
  {
    "text": "And so you get\ndefined the sizes. You get to define\nwhen the data moves from one part of\nthe memory hierarchy",
    "start": "4230020",
    "end": "4236300"
  },
  {
    "text": "to another part of\nthe memory hierarchy. So you need insight\ninto the application.",
    "start": "4236300",
    "end": "4242460"
  },
  {
    "text": "Whereas the bottleneck,\nis it memory bound, is it compute bound, how\ndoes that change as I change my algorithm, as I\nchange my implementation?",
    "start": "4242460",
    "end": "4249780"
  },
  {
    "text": "And then spatial is a\nway of programming model and a way of thinking about how\nto explore the design space when",
    "start": "4249780",
    "end": "4259160"
  },
  {
    "text": "you want to make\nthese trade-offs. So it's small step beyond\nwhat you've been doing here.",
    "start": "4259160",
    "end": "4268280"
  },
  {
    "text": "Now, understand the application. You understand parallelism, you\nunderstand the different types of parallelism. Well, now, what would\nyou do if you actually",
    "start": "4268280",
    "end": "4275750"
  },
  {
    "text": "had control over the hardware\nthat you get to define? And then you think\nabout what that means?",
    "start": "4275750",
    "end": "4283340"
  },
  {
    "text": "So we don't have time to talk\nabout the design of memory.",
    "start": "4283340",
    "end": "4289770"
  },
  {
    "text": "Maybe Kayvon will talk\nabout it on Thursday. Maybe not. It's up to him. But as far as acceleration\nand heterogeneous compute,",
    "start": "4289770",
    "end": "4299150"
  },
  {
    "text": "we've done with that. All right. Thanks. ",
    "start": "4299150",
    "end": "4308000"
  }
]