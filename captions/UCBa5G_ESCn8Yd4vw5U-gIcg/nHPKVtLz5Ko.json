[
  {
    "start": "0",
    "end": "5350"
  },
  {
    "text": "Any questions about\nlogistics first? ",
    "start": "5350",
    "end": "11806"
  },
  {
    "text": "So I mean, I think there's some\nbroad themes of the course. So one broad themes\nof the course is kind of lecture\n1, 2, 3, do you",
    "start": "11806",
    "end": "19300"
  },
  {
    "text": "understand multi-core\narchitecture? And you better\nbelieve that we're going to ask you something\nabout do you understand that?",
    "start": "19300",
    "end": "24740"
  },
  {
    "text": "So that's a topic we\ncan go over today. Another major theme is\njust general tactics",
    "start": "24740",
    "end": "31150"
  },
  {
    "text": "for optimizing\nparallel programs. Now, most of that stuff is stuff\nthat you get in your programming",
    "start": "31150",
    "end": "37629"
  },
  {
    "text": "assignment, and it just\nyou kind figure it out. So in general, when students\ncome into my office hours,",
    "start": "37630",
    "end": "44620"
  },
  {
    "text": "we don't spend as much time\non that before the exam. I'm like, if I\ngive you a program and it has workload imbalance,\ntry and identify it.",
    "start": "44620",
    "end": "51700"
  },
  {
    "text": "The other set of\noptimizations typically are, you have a program that might\nhave very bad cache performance.",
    "start": "51700",
    "end": "58340"
  },
  {
    "text": "Can you improve the order\nof some loops or something like that to improve\nthe cache performance?",
    "start": "58340",
    "end": "63970"
  },
  {
    "text": "Then we talked a little\nbit about-- what else do we talk about? So we talked about\nGPU architecture,",
    "start": "63970",
    "end": "71810"
  },
  {
    "text": "which first of all,\nyou should think of is just like everything from\nthe first three lectures, just had it again, and it's\nno surprise that GPUs",
    "start": "71810",
    "end": "78940"
  },
  {
    "text": "worked the same way. I'd like you to understand the\nbasics of the CUDA programming",
    "start": "78940",
    "end": "85060"
  },
  {
    "text": "model in that there's an\norganization in the thread blocks. There are threads in\nthose thread blocks,",
    "start": "85060",
    "end": "90590"
  },
  {
    "text": "but it's something you're\npresumably well acquainted with after the third\nprogramming assignment.",
    "start": "90590",
    "end": "96220"
  },
  {
    "text": "We had a lecture on\ndeep neural networks. Now, the specifics of\nany deep neural network",
    "start": "96220",
    "end": "101619"
  },
  {
    "text": "are not anything that\nyou need to care about. But what was like maybe the\ntwo or three main concepts",
    "start": "101620",
    "end": "107409"
  },
  {
    "text": "from the deep neural\nnetworks lecture? One was cache locality\nreally matters,",
    "start": "107410",
    "end": "114650"
  },
  {
    "text": "so we talked about blocking,\nand the other stuff",
    "start": "114650",
    "end": "119980"
  },
  {
    "text": "was kind of all specialized\nhardware and things like that. We'll spend more time\non that in the course,",
    "start": "119980",
    "end": "126320"
  },
  {
    "text": "so I think that's more of a\nfinals topic than anything else. We talked about data\nparallel thinking,",
    "start": "126320",
    "end": "131520"
  },
  {
    "text": "so there's the\npossibility-- it's fair game for me to ask\nyou to go through a data parallel thinking exercise\nthat reinforces were you",
    "start": "131520",
    "end": "139430"
  },
  {
    "text": "the one on your programming team\nthat actually did find repeats, or did you delegate that to\nyour partner, and now it shows?",
    "start": "139430",
    "end": "147064"
  },
  {
    "text": "And then we talked about\nin the last lecture I gave was fine grained\nlocking, and we talked about",
    "start": "147065",
    "end": "154400"
  },
  {
    "text": "compare and swap, and\nimplementing some locks. We talked about the\nrelationship of that to how cache coherence works,\nsome interesting relationship.",
    "start": "154400",
    "end": "164130"
  },
  {
    "text": "And we talked about\nusing locks, and a fine grained locking setting,\nand I've given you some practice problems on that.",
    "start": "164130",
    "end": "170879"
  },
  {
    "text": "I can tell you that's\nat the midterm level, the fine grained locking\npractice problems will be simpler.",
    "start": "170880",
    "end": "177110"
  },
  {
    "text": "Probably nothing\nmore sophisticated if I give you one on\nmaybe a linked list or something like that.",
    "start": "177110",
    "end": "183240"
  },
  {
    "text": "But you see some practice\nproblems on hash tables, and graphs, and trees,\nand other stuff.",
    "start": "183240",
    "end": "188370"
  },
  {
    "text": "I'd like you to have a little\nbit more time to wrestle with that before I think\nit's reasonable for me",
    "start": "188370",
    "end": "193430"
  },
  {
    "text": "to ask you a tough question. So in light of that-- and then, of course,\nConely definitely",
    "start": "193430",
    "end": "199849"
  },
  {
    "text": "talked about the MSI protocol,\nand what cache coherence means, and the main\nideas behind that.",
    "start": "199850",
    "end": "206360"
  },
  {
    "text": "And we talked about the idea\nof relaxed memory consistency, which is even though a program,\na thread might write X and then",
    "start": "206360",
    "end": "213200"
  },
  {
    "text": "read y, other processors might\nsee those writes or reads in different orders, and there's\nsome interesting implications",
    "start": "213200",
    "end": "219890"
  },
  {
    "text": "of that. Cool. ",
    "start": "219890",
    "end": "225980"
  },
  {
    "text": "So in light of that, I'm\nhappy to go over anything.",
    "start": "225980",
    "end": "231290"
  },
  {
    "text": "And so I think those\nthat raise their hand and vote with\ntheir vote quickly, will get more topics\nof their coverage.",
    "start": "231290",
    "end": "236820"
  },
  {
    "text": "Otherwise I can start\nmaking some topics up, but that may not be well. So sir, you were the\nfirst to raise your hand.",
    "start": "236820",
    "end": "242205"
  },
  {
    "text": "I think [INAUDIBLE]\nbriefly touched on block-free programming. I was wondering if\nyou just go over,",
    "start": "242205",
    "end": "248120"
  },
  {
    "text": "I think, some of the\n[INAUDIBLE] get to. Sure. Yeah. I mean, obviously anything that\nwe did not stress in class,",
    "start": "248120",
    "end": "253350"
  },
  {
    "text": "I'm not going to bang you with. OK. Yeah. I think that's cool. OK. So we talked about this\nidea compare and swap.",
    "start": "253350",
    "end": "261540"
  },
  {
    "text": "This is something I\nwould like you to know. So up until this\npoint in the class,",
    "start": "261540",
    "end": "268850"
  },
  {
    "text": "we basically did all of our\nsynchronization in two ways.",
    "start": "268850",
    "end": "274460"
  },
  {
    "text": "We said that there were locks,\nand what does a lock ensure?",
    "start": "274460",
    "end": "279930"
  },
  {
    "text": "Or in other words, what\ndoes mutual exclusion mean? Question for the class.",
    "start": "279930",
    "end": "285420"
  },
  {
    "text": "Yeah. One part of the code is only\nrun by one thread at a time. And this is a really\nimportant detail.",
    "start": "285420",
    "end": "291360"
  },
  {
    "text": "Mutual exclusion means,\nas was just said, that only one thread, one\nactor, one worker or whatever,",
    "start": "291360",
    "end": "298729"
  },
  {
    "text": "is allowed in a\nparticular section of code at one time, which means that\nif I'm in that section of code,",
    "start": "298730",
    "end": "305880"
  },
  {
    "text": "all of you are prevented\nfrom running it. And an atomic operation is\nbasically of that mentality",
    "start": "305880",
    "end": "313570"
  },
  {
    "text": "because an atomic operation\nsays, is usually used on a read, modify, write.",
    "start": "313570",
    "end": "319060"
  },
  {
    "text": "And it says that if I am\nmodifying, reading and modifying this variable, nobody else\nis reading and modifying",
    "start": "319060",
    "end": "326400"
  },
  {
    "text": "this variable. And compare and swap is a one\nform of an atomic operation.",
    "start": "326400",
    "end": "332730"
  },
  {
    "text": "It says, I'm going to-- it's a read, modify, or\nreally read, compare, write.",
    "start": "332730",
    "end": "340420"
  },
  {
    "text": "It says I'm going to read\nthe value of this variable, and if it has a\nparticular value,",
    "start": "340420",
    "end": "345730"
  },
  {
    "text": "if it has, in this\ncase, the value given by the\nargument compare, I'm going to write this new\nvalue to that position.",
    "start": "345730",
    "end": "353042"
  },
  {
    "text": "Otherwise, I'm\nessentially going to write the old value to the position\nand leave it unchanged. That's an atomic operation,\nit's all the heart",
    "start": "353042",
    "end": "360340"
  },
  {
    "text": "whatever hardware platform\nyou are running on if you call,\ncompare, and swap, it will guarantee that,\nthat is run atomically.",
    "start": "360340",
    "end": "366940"
  },
  {
    "text": "That's what you can\ntake as a given. And then I asked, we\ntalked about given atomic",
    "start": "366940",
    "end": "375960"
  },
  {
    "text": "compare and swap. How can we implement an\natomic min operation?",
    "start": "375960",
    "end": "381870"
  },
  {
    "text": "So one way we could think\nabout the atomic min operation as the hardware just gives me\na new primitive atomic min,",
    "start": "381870",
    "end": "390480"
  },
  {
    "text": "and it ensures mutual\nexclusion because it's atomic. It's like if I'm doing\na min on this variable,",
    "start": "390480",
    "end": "395800"
  },
  {
    "text": "it's going to make sure that\nno other threads or cores can read or write to it. But look how I implement.",
    "start": "395800",
    "end": "401278"
  },
  {
    "text": "That's a little bit-- that's\nnot how we design systems. We don't build a bunch\nof new primitives for every single thing\nthat we want to do.",
    "start": "401278",
    "end": "407800"
  },
  {
    "text": "We like to have one\nprimitive that's rock solid, and maybe fast, and use it\nin a variety of contexts.",
    "start": "407800",
    "end": "414819"
  },
  {
    "text": "So look at this\natomic min operation, and let's go over the\nphilosophy of what happens.",
    "start": "414820",
    "end": "420460"
  },
  {
    "text": "So what does min require? Min requires me reading\nthe value from memory, checking if the\nvalue from memory",
    "start": "420460",
    "end": "427290"
  },
  {
    "text": "is less than the new value\nthat I'm trying to min with it. And if my value is\nsmaller, we need",
    "start": "427290",
    "end": "433810"
  },
  {
    "text": "to update the value in memory\ninto something smaller, and that all has to be atomic. Now, first of all,\nI want you to make",
    "start": "433810",
    "end": "440710"
  },
  {
    "text": "sure you can confirm to yourself\nthat if you could do this easily",
    "start": "440710",
    "end": "447250"
  },
  {
    "text": "without compare and swap,\nif I gave you a lock, you would take the lock,\nyou would read the value,",
    "start": "447250",
    "end": "453380"
  },
  {
    "text": "you would check to\nsee if it was smaller, you would potentially\nupdate the value, and then you would\nunlock, and that",
    "start": "453380",
    "end": "458650"
  },
  {
    "text": "would be absolutely correct. And it would ensure\nmutual exclusion, only one thread is trying to\ndo the atomic min at once.",
    "start": "458650",
    "end": "466300"
  },
  {
    "text": "So look, can someone describe\nto me what this code is doing? It's not ensuring\nmutual exclusion",
    "start": "466300",
    "end": "472720"
  },
  {
    "text": "on the entire min operation. It's definitely not true.",
    "start": "472720",
    "end": "477880"
  },
  {
    "text": "Different threads can have\nread that value from memory and be checking if their\nvalue is less than it",
    "start": "477880",
    "end": "483980"
  },
  {
    "text": "at the same time. I think if we look\nat the predicate",
    "start": "483980",
    "end": "489190"
  },
  {
    "text": "upon which the min\nconclusion was reached, it's ensuring that, that\npredicate is still valid when it writes that conclusion.",
    "start": "489190",
    "end": "495560"
  },
  {
    "text": "So the idea is saying, if I\nread the value from memory, then I start doing\nwhatever work I need to do.",
    "start": "495560",
    "end": "501860"
  },
  {
    "text": "In this case, it's computing\nthe min of something, but maybe it was like atomic\ntest primality or something,",
    "start": "501860",
    "end": "508780"
  },
  {
    "text": "or I probably wouldn't work\nbecause I'd return a bool, but anyways. But let's say it's a\nmore expensive operation",
    "start": "508780",
    "end": "514299"
  },
  {
    "text": "than I have to atomically\nperform some math and then I write\nthe value back to--",
    "start": "514299",
    "end": "519909"
  },
  {
    "text": "I do my work. In this case, I\ndo the work, which is to take the min of the old\nvalue and the value that I have.",
    "start": "519909",
    "end": "528010"
  },
  {
    "text": "And then I go back\nand I say, all right, if the value in memory\nis still the value",
    "start": "528010",
    "end": "534490"
  },
  {
    "text": "that it was when I got started,\nI can be assured that no one",
    "start": "534490",
    "end": "540040"
  },
  {
    "text": "else has come in, or\nif they've come in, they at least haven't\nupdated the min value. And that's what the\natomic cast does.",
    "start": "540040",
    "end": "547070"
  },
  {
    "text": "It says if the value\nin this address is still the same as old,\nplease update it to new.",
    "start": "547070",
    "end": "553269"
  },
  {
    "text": "And I know if that is true\nbecause atomic cast returns old,",
    "start": "553270",
    "end": "558490"
  },
  {
    "text": "and so I check to see if\nthe old-- if it's still-- sorry, it returns the value\nin memory, and if that's old,",
    "start": "558490",
    "end": "564950"
  },
  {
    "text": "that's the check. So let's think about this. Will that atomic\ncompare and swap",
    "start": "564950",
    "end": "571930"
  },
  {
    "text": "succeed if while I was\ntrying to do my atomic min, you as another thread\ncame in, read the value,",
    "start": "571930",
    "end": "580190"
  },
  {
    "text": "checked it, and\ndecided that you didn't need to update it because\nyour value is too large. My atomic min succeeds.",
    "start": "580190",
    "end": "587420"
  },
  {
    "text": "So what happened here is\nthere's no mutual exclusion. We're both running\nat the same time in so-called this important\nsynchronized section.",
    "start": "587420",
    "end": "595750"
  },
  {
    "text": "But unless our\nboth of our actions create a situation where\nsynchronization is needed,",
    "start": "595750",
    "end": "602380"
  },
  {
    "text": "we can proceed. In other words, we\nare speculating. We're hoping that we're\nnot going to conflict,",
    "start": "602380",
    "end": "610150"
  },
  {
    "text": "and we're just\ngoing to go ahead. And then at the last second,\nbefore I do my write,",
    "start": "610150",
    "end": "615650"
  },
  {
    "text": "I'm going to go check to see\nif that assumption was true. And if it's true, all the work\nI did is great, and I move on.",
    "start": "615650",
    "end": "623470"
  },
  {
    "text": "If it's false, what's\nthe cost of this? You have to do it again. I have to do it again,\nand all the work",
    "start": "623470",
    "end": "630700"
  },
  {
    "text": "that I did on that old\nvalue is potentially wasted. So what if this function\nwas not min, but was",
    "start": "630700",
    "end": "638140"
  },
  {
    "text": "something really expensive? Like compute some\nexponent on that value, and then store the\nexponent value backward.",
    "start": "638140",
    "end": "644140"
  },
  {
    "text": "Then I've done\nwork, and I just got to throw it out, essentially. I essentially every time I\ngo through this while loop,",
    "start": "644140",
    "end": "651470"
  },
  {
    "text": "I throw out my min work that\nI did, and I do it again. So this lock free thinking\nis more like, hey,",
    "start": "651470",
    "end": "659089"
  },
  {
    "text": "I need to make sure that I\ngive the appearance that I get the same results as\nmutual exclusion,",
    "start": "659090",
    "end": "665360"
  },
  {
    "text": "but I'm actually not\nenforcing mutual exclusion, and that's the big\nmental difference.",
    "start": "665360",
    "end": "670880"
  },
  {
    "text": "And so what you're going to see\nafter Thanksgiving is actually Conely is going to give a bunch\nof lectures, and two of them",
    "start": "670880",
    "end": "676120"
  },
  {
    "text": "are actually going\nto be about the idea of transactional memory,\nwhich kind of extends this idea to an\narbitrary function,",
    "start": "676120",
    "end": "683270"
  },
  {
    "text": "writing to many variables. So you just write your\nfunction, it may read and write a bunch of variables,\nand the system",
    "start": "683270",
    "end": "689870"
  },
  {
    "text": "figures out if any of\nthose reads and writes conflict with another\nprocessors' reads and writes,",
    "start": "689870",
    "end": "695570"
  },
  {
    "text": "and will actually abort and\nback one of the processors out. OK, that's the level that\nI'd like you to know lock",
    "start": "695570",
    "end": "702290"
  },
  {
    "text": "free for tomorrow's assessment. And later in the lecture,\nif you're interested,",
    "start": "702290",
    "end": "709570"
  },
  {
    "text": "I show you how to implement\nlock-free stacks and stuff like that. Like a lock-free linked list is\ngiven in some parallel computing",
    "start": "709570",
    "end": "717150"
  },
  {
    "text": "classes, it would be\ncomplicated enough to get right that it would\ntake me the entire lecture. It would only be insert and\ndelete on a lock-free linked",
    "start": "717150",
    "end": "724580"
  },
  {
    "text": "list. So I would not ask you how\nto do a lock-free insertion and deletion on an exam, for\nexample, because it's actually",
    "start": "724580",
    "end": "731550"
  },
  {
    "text": "incredibly tricky to get right. I'm not sure if I\ncould do it off the top of my head on an exam.",
    "start": "731550",
    "end": "737550"
  },
  {
    "text": "Yeah. Could you say that lock-free\nprogramming in weeds to maybe a higher\nrisk of starvation?",
    "start": "737550",
    "end": "745170"
  },
  {
    "text": "Well, I mean, there's nothing\nin this implementation. Good. So first of all, what's\nthe idea of starvation?",
    "start": "745170",
    "end": "751650"
  },
  {
    "text": "Starvation is that the\nsystem is making progress. Some threads are\nmaking progress,",
    "start": "751650",
    "end": "757060"
  },
  {
    "text": "but for whatever reason,\none thread does not. One thread never succeeds here.",
    "start": "757060",
    "end": "763792"
  },
  {
    "text": "And there's nothing\nin this provision, in this implementation to ensure\nfairness amongst the threads.",
    "start": "763792",
    "end": "772040"
  },
  {
    "text": "But if you just had an atomic\nlock and unlock implemented like we did last time with the\nexception of that ticket lock",
    "start": "772040",
    "end": "779410"
  },
  {
    "text": "example, there's also no\nprovision of fairness, so lock-free versus\nmutual exclusion",
    "start": "779410",
    "end": "785560"
  },
  {
    "text": "is not really-- is\northogonal from fairness. I could do something\nhere which says,",
    "start": "785560",
    "end": "790779"
  },
  {
    "text": "after I've iterated through\nthis loop, if I fail, maybe I should sleep\nor something like that.",
    "start": "790780",
    "end": "798320"
  },
  {
    "text": "I could do an\nexponential backoff here, just like I could do an\nexponential backoff and a lock.",
    "start": "798320",
    "end": "804360"
  },
  {
    "text": "OK. Actually, did I miss anybody? Yeah, go ahead. Is there any extra\noverhead that's",
    "start": "804360",
    "end": "811500"
  },
  {
    "text": "caused when you\ncall from CAS with-- regardless of whether-- Well, let's talk about it.",
    "start": "811500",
    "end": "817530"
  },
  {
    "text": "What is the overhead\nof atomic CAS? Is that just from\nthe inside State,",
    "start": "817530",
    "end": "822660"
  },
  {
    "text": "like when you're\ntrying to do long run? So, first of all,\nlet's think about it in terms of a cache\ncoherence protocol. So first of all,\nactually, how would you",
    "start": "822660",
    "end": "829140"
  },
  {
    "text": "implement atomic CAS if you\ndidn't have cache coherence? You would have to be\nfrom memory each time.",
    "start": "829140",
    "end": "836680"
  },
  {
    "text": "But it's atomic, how\ndo you even implement-- it's not just a read,\nit's a read and write.",
    "start": "836680",
    "end": "844420"
  },
  {
    "text": "You'd have to have some special\nprovision in your system that kind of could say this\nis a current atomic operation",
    "start": "844420",
    "end": "852670"
  },
  {
    "text": "on this address, and so no other\ncore can access this address.",
    "start": "852670",
    "end": "857680"
  },
  {
    "text": "You basically build\na simple version of cache coherence for a single\naddress or something like that.",
    "start": "857680",
    "end": "863509"
  },
  {
    "text": "But if you did have\ncache coherence, what is the cost of a CAS?",
    "start": "863510",
    "end": "870630"
  },
  {
    "text": " It's a BusRd X.",
    "start": "870630",
    "end": "875750"
  },
  {
    "text": "I mean, it's certainly a\nBusRd X, so it's a right. Even if it fails, it's got to be\na right, and that's expensive.",
    "start": "875750",
    "end": "883610"
  },
  {
    "text": "And it's also a-- yeah. Basically, it would be--",
    "start": "883610",
    "end": "889210"
  },
  {
    "text": "and it's not just a BusRd X,\nit's a BusRd X that you read",
    "start": "889210",
    "end": "896390"
  },
  {
    "text": "the value, and then the cache\ncoherence protocol cannot let that line be ripped away by\nanother processor until this",
    "start": "896390",
    "end": "907220"
  },
  {
    "text": "comparison and potential\nright has happened. In the same way that the cache\ncoherence protocol, if you call",
    "start": "907220",
    "end": "913640"
  },
  {
    "text": "BusRd X, BusRd X puts the\ndata in your cache line, and it's also kind of assumed\nthat processor is actually",
    "start": "913640",
    "end": "920329"
  },
  {
    "text": "going to execute its write\nbefore it gets ripped away because if you didn't\nguarantee that, you might get into live lock\nsituation of two processors",
    "start": "920330",
    "end": "927560"
  },
  {
    "text": "or just constantly BusRd X,\nBusRd X, and then the line gets ripped away before\nthey actually do the right,",
    "start": "927560",
    "end": "934080"
  },
  {
    "text": "and so they're both sitting\nthere on the same instruction just continuously. So you should think\nabout it as once I",
    "start": "934080",
    "end": "939810"
  },
  {
    "text": "do-- once I get the line\nin the writeable state, here there needs to be a\ncomparison with another value",
    "start": "939810",
    "end": "945690"
  },
  {
    "text": "and a write, so the cache logic\nhas to-- the processor just has to be a little bit\nmore complex for that.",
    "start": "945690",
    "end": "953300"
  },
  {
    "text": "Cool. OK. New topic. Any other questions?",
    "start": "953300",
    "end": "959010"
  },
  {
    "text": "Well, here. Let's go here and then here. Yeah. Sorry for going back\ninto the couple weeks.",
    "start": "959010",
    "end": "964880"
  },
  {
    "text": "But I wonder if we could go over\nMapReduce from distributed data parallel? Sure. Yeah, absolutely.",
    "start": "964880",
    "end": "970949"
  },
  {
    "text": "Let's see. Let me cut this off, and what\nspecific is the question?",
    "start": "970950",
    "end": "977010"
  },
  {
    "text": "All right. I was looking at the run\nMapReduce implementation. The job with a bunch\nof diagrams and you",
    "start": "977010",
    "end": "983090"
  },
  {
    "text": "get the key values\nhow they're queued. And then you're talking about\nMapReduce itself, right?",
    "start": "983090",
    "end": "988110"
  },
  {
    "text": "Correct. Yeah. So I looked a little\nbit like this. Yeah.",
    "start": "988110",
    "end": "993440"
  },
  {
    "text": "I just throw over there. So in all of these\nprogramming systems,",
    "start": "993440",
    "end": "999810"
  },
  {
    "text": "the first thing to think\nabout is what is my API, and what is the meaning\nof the API functions?",
    "start": "999810",
    "end": "1009490"
  },
  {
    "text": "And in MapReduce, there\nactually is only two. So the history of this and\nI won't spend too much time on the history is MapReduce\nwas a very simple paper that",
    "start": "1009490",
    "end": "1017552"
  },
  {
    "text": "came out of Google, and I\ncan't remember what it was. It was probably like\nthe early 2000s. This was before Spark,\nbefore a lot of--",
    "start": "1017552",
    "end": "1023740"
  },
  {
    "text": "and the Google folks\nwere saying that there's a lot of jobs that have to\nrun at Google on an insane",
    "start": "1023740",
    "end": "1029170"
  },
  {
    "text": "amount of data. And there's well, to handle\nan insane amount of data like our entire\nindex of the web,",
    "start": "1029170",
    "end": "1035900"
  },
  {
    "text": "we've actually built Bigtable. We've actually built\nthe shared file system.",
    "start": "1035900",
    "end": "1040907"
  },
  {
    "text": "So you basically-- all\nthe data in the world is in this file system, and\nthe file system is distributed across a ton of computers.",
    "start": "1040907",
    "end": "1046859"
  },
  {
    "text": "And they basically had\na bunch of applications like you can just imagine if\nyou're like DevOps or something,",
    "start": "1046859",
    "end": "1052740"
  },
  {
    "text": "you have a bunch of\napplications that are kind of data mining\nstuff on this big database.",
    "start": "1052740",
    "end": "1059340"
  },
  {
    "text": "So the canonical\nexample here would be like mining through a\nbunch of web logs or something",
    "start": "1059340",
    "end": "1065990"
  },
  {
    "text": "like that. So imagine you just had a\nhuge file, terabytes of text, and every line in the file\nwas like a log from Apache.",
    "start": "1065990",
    "end": "1074420"
  },
  {
    "text": "Excuse me. OK, so there's two\nfunction calls. The first function\ncall was called map,",
    "start": "1074420",
    "end": "1081440"
  },
  {
    "text": "and it was called map because\nit was inspired by data parallel map, and the second\nfunction call",
    "start": "1081440",
    "end": "1086840"
  },
  {
    "text": "was called reduce, which\nwas inspired by reduce, but they're very\ndifferent than--",
    "start": "1086840",
    "end": "1092210"
  },
  {
    "text": "a little bit different\nthan the definitions that I gave you in the data\nparallel thinking lecture. Map is the following.",
    "start": "1092210",
    "end": "1098580"
  },
  {
    "text": "Let's say we're going to map. In this case, it was-- right. Yeah.",
    "start": "1098580",
    "end": "1103760"
  },
  {
    "text": "It was we're going to iterate\nover all lines of all files. Imagine you have this just\ngigantic terabyte file",
    "start": "1103760",
    "end": "1110750"
  },
  {
    "text": "with a ton of lines in it, and\ndifferent pieces of that file were just spread across\ndifferent machines.",
    "start": "1110750",
    "end": "1116180"
  },
  {
    "text": "And so before, if you were\na programmer at Google, you had to somehow get\nthe data from the machine,",
    "start": "1116180",
    "end": "1121380"
  },
  {
    "text": "bring it in and do\na bunch of stuff. It was just complicated because\nyour files were spread out all over the machine.",
    "start": "1121380",
    "end": "1126440"
  },
  {
    "text": "So map just says, for every\nline in the file in this case, or for every thing, I want you\nto run this function f just",
    "start": "1126440",
    "end": "1134990"
  },
  {
    "text": "like map. Now, in MapReduce specifically,\nthe output of map,",
    "start": "1134990",
    "end": "1141769"
  },
  {
    "text": "if the input is like\na line of the file, the output is not just\nsome arbitrary type T,",
    "start": "1141770",
    "end": "1149130"
  },
  {
    "text": "but is actually\na key value pair. So in this case, if I remember\nthe example correctly,",
    "start": "1149130",
    "end": "1155635"
  },
  {
    "text": "the input was a string,\na line in the file, and then the f,\nthe function that was mapped onto all\nthe lines of the file,",
    "start": "1155635",
    "end": "1162620"
  },
  {
    "text": "for every line in every file\nbasically produced a key value",
    "start": "1162620",
    "end": "1168080"
  },
  {
    "text": "pair. And in this case, the key\nwas like the user agent, the browser, and the\nvalue in this case",
    "start": "1168080",
    "end": "1173720"
  },
  {
    "text": "was actually just the number\n1, but you could actually imagine the key value pairs\nwere like browser error",
    "start": "1173720",
    "end": "1179120"
  },
  {
    "text": "message or something like that,\nso you produce keys and values. And then there's some\nmagic that happens,",
    "start": "1179120",
    "end": "1184560"
  },
  {
    "text": "and this is where\nMapReduce was really clever in that so\nmuch simplicity was",
    "start": "1184560",
    "end": "1189770"
  },
  {
    "text": "able to work for a lot of\nthings, or really janky, and that this is very hard coded\nis that the system then takes",
    "start": "1189770",
    "end": "1196850"
  },
  {
    "text": "all of those key value pairs. Now, keep in mind\nthat map function can run across all the machines.",
    "start": "1196850",
    "end": "1202650"
  },
  {
    "text": "If your file is distributed\nacross 1,000 machines, well, it'll just run that map function\non the lines of the file",
    "start": "1202650",
    "end": "1209240"
  },
  {
    "text": "on the machine that\nalready holds the file, so the data doesn't move,\nand now comes the big data",
    "start": "1209240",
    "end": "1214610"
  },
  {
    "text": "communication part. Every little-- every invocation\nof f produces a key and a value.",
    "start": "1214610",
    "end": "1222640"
  },
  {
    "text": "And then what MapReduce says is\nhard coded by design in the API, we're going to organize\nall of the tuples that",
    "start": "1222640",
    "end": "1229960"
  },
  {
    "text": "have the same key into a file. So I have different files now\nfor all of the different keys,",
    "start": "1229960",
    "end": "1237159"
  },
  {
    "text": "and that file is just a\nlist of all the values that match that key, so\nthat's where the data just",
    "start": "1237160",
    "end": "1242860"
  },
  {
    "text": "moved everywhere. It's like a big resort. In Spark, this would be called\ngroup by key or shuffle.",
    "start": "1242860",
    "end": "1250880"
  },
  {
    "text": "Yeah. So map produces keys\nand values, and they get shuffled all together,\nand they're reorganized.",
    "start": "1250880",
    "end": "1257980"
  },
  {
    "text": "Here's all the\ndata for this key. Here's all the\ndata for that key. Here's all the\ndata for that key. And because they had\na shared file system,",
    "start": "1257980",
    "end": "1264919"
  },
  {
    "text": "they already had it built, they\nactually stored this information in files. And so your parallel\nfile system was exactly",
    "start": "1264920",
    "end": "1272799"
  },
  {
    "text": "the transport mechanism to get\nall this data shuffling around. OK.",
    "start": "1272800",
    "end": "1278950"
  },
  {
    "text": "Then you have reduce, and\nreduce takes as arguments, the key and a list of values\nand produces a result,",
    "start": "1278950",
    "end": "1287750"
  },
  {
    "text": "so the reduce function\nis actually mapped onto all of the unique keys.",
    "start": "1287750",
    "end": "1295610"
  },
  {
    "text": "So the reduce function gets\nas input the key that it's working on, plus the list of\nvalues that were associated",
    "start": "1295610",
    "end": "1302970"
  },
  {
    "text": "with that key, essentially\nthe contents of a file, and now at this point it\ndoes whatever the heck it wants, and produces\na result. And presumably",
    "start": "1302970",
    "end": "1310940"
  },
  {
    "text": "it was called reduce\nbecause in this case, it takes all of those\nvalues with the same key, and in this case just\naggregated the number of lines",
    "start": "1310940",
    "end": "1321170"
  },
  {
    "text": "with that key and\nreturn the result. So the name reduce is\nactually not parallel reduce",
    "start": "1321170",
    "end": "1326930"
  },
  {
    "text": "or anything like that, it's\nactually map group by key, map from a map where every\nelement of the map gets a list",
    "start": "1326930",
    "end": "1336890"
  },
  {
    "text": "and returns it to a value. So in modern parallel\nprogramming, that's what it is.",
    "start": "1336890",
    "end": "1342368"
  },
  {
    "text": "And so if you're like,\nthat's pretty weird. I had all these\ncomputers, my data was distributed amongst\nall the computers,",
    "start": "1342368",
    "end": "1348080"
  },
  {
    "text": "really our a way\nof communicating would be to write\nto the file system, and then read it back\nin from another node.",
    "start": "1348080",
    "end": "1355940"
  },
  {
    "text": "That's what Spark\nsaid, that's stupid. And said, oh, we should\njust do all this in memory,",
    "start": "1355940",
    "end": "1363440"
  },
  {
    "text": "and that was the whole\nidea behind Spark, and do it with a proper set of\ndata parallel operators, not",
    "start": "1363440",
    "end": "1369470"
  },
  {
    "text": "just map, reduce,\nand group by key. Yeah. Can you touched on\nthat a little bit more",
    "start": "1369470",
    "end": "1374580"
  },
  {
    "text": "like why MapReduce required\nthis gap sets, or like-- This was how it was implemented.",
    "start": "1374580",
    "end": "1379830"
  },
  {
    "text": "The only thing you\ncan write in MapReduce is map, to produce\na bunch key values, then the system does a\ngroup by key for you.",
    "start": "1379830",
    "end": "1387610"
  },
  {
    "text": "You have no question\nabout that, and then you run another map, which is\nactually called the reducer",
    "start": "1387610",
    "end": "1392820"
  },
  {
    "text": "function, which\nis map the reducer function onto all unique keys\nto produce one output per key.",
    "start": "1392820",
    "end": "1400440"
  },
  {
    "text": "So the reducer\nfunction for a given key takes all of the\nvalues with that key and reduces it to something,\nbut that's mapped over all keys.",
    "start": "1400440",
    "end": "1410460"
  },
  {
    "text": "Yeah. So I'm not sure\nabout this, but I think one reason\nwhy they were using",
    "start": "1410460",
    "end": "1416130"
  },
  {
    "text": "these concepts for\nthe original MapReduce is because like\ncommercial hardware,",
    "start": "1416130",
    "end": "1421289"
  },
  {
    "text": "this was really\ncheap at the time, and it's like at the\nbeginning they don't really care about the latency\nbecause the amount of data",
    "start": "1421290",
    "end": "1428130"
  },
  {
    "text": "is not that huge\ncompared to right now. For example, there is no Google\nFile System implementation,",
    "start": "1428130",
    "end": "1434530"
  },
  {
    "text": "there is no replication\nfor the master server. It's just like, it's a\nsingle point of failure.",
    "start": "1434530",
    "end": "1440159"
  },
  {
    "text": "At the beginning it's just-- So the comment\nhere was about some of the motivations of\nMapReduce, and I wasn't there,",
    "start": "1440160",
    "end": "1446230"
  },
  {
    "text": "so I can't speak\nauthoritatively at all about it, but there was a comments about\nthe disk latency wasn't so bad.",
    "start": "1446230",
    "end": "1454570"
  },
  {
    "text": "But first of all, that gives\nme a teaching opportunity. Let's first think\nabout this as, am I",
    "start": "1454570",
    "end": "1460140"
  },
  {
    "text": "worried about going to\ndisk because of latency? ",
    "start": "1460140",
    "end": "1465330"
  },
  {
    "text": "I'm not worried about\ndisk latency at all. Why am I not worried\nabout disk latency? ",
    "start": "1465330",
    "end": "1473300"
  },
  {
    "text": "Why am I a heck of a lot more\nworried about disk bandwidth?",
    "start": "1473300",
    "end": "1478620"
  },
  {
    "text": "Because you care\nabout throughput. I care about throughput,\nbut latency can impact throughput, right?",
    "start": "1478620",
    "end": "1484080"
  },
  {
    "text": "If I do one operation and\nthen wait for a long time, and then do the next operation,\nlatency can definitely",
    "start": "1484080",
    "end": "1489240"
  },
  {
    "text": "impact throughput. In this case, I'm only worried\nabout throughput though. I guess you have a ton\nof nodes and they're all",
    "start": "1489240",
    "end": "1495899"
  },
  {
    "text": "trying to write back to disk. It's like, that's\na huge bottleneck. ",
    "start": "1495900",
    "end": "1503280"
  },
  {
    "text": "Right. But first of all,\nthat's why I definitely I care about\nbandwidth, but I guess one thing I want to\nstress here is why",
    "start": "1503280",
    "end": "1509790"
  },
  {
    "text": "do I not care about latency? Can you just hide all-- This is massively\nparallel, right?",
    "start": "1509790",
    "end": "1516670"
  },
  {
    "text": "There's billions\nof lines in a file. If I have to write a key value\npair out, I send the write out",
    "start": "1516670",
    "end": "1523680"
  },
  {
    "text": "and I go start working\non something else. Or in the reduce phase,\nif I'm reducing over",
    "start": "1523680",
    "end": "1529169"
  },
  {
    "text": "all the different keys,\nI start loading the data for the next key now while\nI'm working on this key.",
    "start": "1529170",
    "end": "1534640"
  },
  {
    "text": "So all of our ideas of\nmultithreading, whenever you have massive parallelism,\nlatency can be hidden,",
    "start": "1534640",
    "end": "1541360"
  },
  {
    "text": "you care about bandwidth\nand throughput. So the real problem is that\nin between these phases",
    "start": "1541360",
    "end": "1546480"
  },
  {
    "text": "of computation, the entire\ndata set, a terabytes of data, is read from disk, and put back\non disk, and then read back in.",
    "start": "1546480",
    "end": "1555530"
  },
  {
    "text": "And that's true even\nif the group by key",
    "start": "1555530",
    "end": "1561120"
  },
  {
    "text": "doesn't really do much\nat all, like it's all narrow dependencies. And so people at the\ntime were like, wow,",
    "start": "1561120",
    "end": "1568780"
  },
  {
    "text": "I can write code in 10 lines of\ncode that use 1,000 machines.",
    "start": "1568780",
    "end": "1574200"
  },
  {
    "text": "There's no, I get\nfault tolerance, I get this parallelism,\nit's all handled in those two abstractions. And some of the folks that\ndid Spark, and there's",
    "start": "1574200",
    "end": "1583110"
  },
  {
    "text": "this paper that I have in\nmy lecture slides about-- or in Conely's lecture\nslides now, I guess,",
    "start": "1583110",
    "end": "1588490"
  },
  {
    "text": "about this paper that somebody\nwrote that said, yeah, but the applications that\nyou're-- people were like, oh,",
    "start": "1588490",
    "end": "1594030"
  },
  {
    "text": "we can do everything\nin Spark and MapReduce because it's so parallel and so\nawesome, but so low bandwidth.",
    "start": "1594030",
    "end": "1602000"
  },
  {
    "text": "And then some folks popped\nopen a laptop and said,",
    "start": "1602000",
    "end": "1607550"
  },
  {
    "text": "you're using 5,000 nodes in\na Spark or MapReduce cluster to process a data set that's\n500 gigabytes or something like",
    "start": "1607550",
    "end": "1614890"
  },
  {
    "text": "that. 500 gigabytes fits in\nmemory in my laptop. And they showed that one core\nof a laptop could outperform",
    "start": "1614890",
    "end": "1621880"
  },
  {
    "text": "a 5,000 node Spark cluster just\nbecause people were getting enamored with parallelism\nand forgetting about locality",
    "start": "1621880",
    "end": "1628960"
  },
  {
    "text": "and bandwidth. What kind of laptop\nfits 500 gigabytes?",
    "start": "1628960",
    "end": "1634660"
  },
  {
    "text": "OK, fine. [LAUGHTER] I think the actual\nexperiment was they--",
    "start": "1634660",
    "end": "1643510"
  },
  {
    "text": "it was a laptop, actually. I could go bring it up. I'll send it is\nthey streamed it. So they streamed it\noff disk, and showed",
    "start": "1643510",
    "end": "1650890"
  },
  {
    "text": "that instead of going back\nout to disk every time, just keep a chunk in memory,\nand then do five iterations,",
    "start": "1650890",
    "end": "1658180"
  },
  {
    "text": "and then you can keep up\nwith the Spark cluster. Another version of it\nwould be these days,",
    "start": "1658180",
    "end": "1663250"
  },
  {
    "text": "in 2023, it would\nbe for $3 an hour, I can go get a machine on AWS\nwith 512 gigabytes of memory.",
    "start": "1663250",
    "end": "1671570"
  },
  {
    "text": "And most of us most\nof the time are not working on problems\nthat are exceeding what you can buy for pennies.",
    "start": "1671570",
    "end": "1678220"
  },
  {
    "text": "And so it makes a lot more\nsense to think about locality, embrace that, and maybe scale\nto 4 terabyte sized machines",
    "start": "1678220",
    "end": "1687730"
  },
  {
    "text": "instead of trying to make your\ncode robust to fault tolerance and failure on 10,000\nnodes in the cluster.",
    "start": "1687730",
    "end": "1695320"
  },
  {
    "text": "There's a great article\nby Amazon Web Streaming, the video folks like\nPrime Video, where--",
    "start": "1695320",
    "end": "1703480"
  },
  {
    "text": "so some of you might have\nheard of Amazon lambdas. So Amazon lambdas is like\nyou just give them a function",
    "start": "1703480",
    "end": "1708605"
  },
  {
    "text": "and say, hey, when\neverybody-- whenever I need to run this function,\nI'll let and run it for me. I don't want to think about\nservers and stuff like that.",
    "start": "1708605",
    "end": "1715063"
  },
  {
    "text": "So the idea is to do a\ncomputation, like in maybe a second or so or less. It's great because\nif I have a website,",
    "start": "1715063",
    "end": "1721450"
  },
  {
    "text": "and I want it to always be\nrunning, and nobody ever comes, I don't pay a dime\nif nobody's coming. And if people burst,\nit's really great",
    "start": "1721450",
    "end": "1729070"
  },
  {
    "text": "because they'll just execute\nall of those parallel functions for all these\nrequests, and I don't have to worry\nabout scaling, even",
    "start": "1729070",
    "end": "1735010"
  },
  {
    "text": "though the cost\nof those functions is actually quite high. OK, so Prime Video built\ntheir whole system out",
    "start": "1735010",
    "end": "1741940"
  },
  {
    "text": "of these microservices and\nlambdas like it was like, here's a chunk of-- a\nlittle chunk of video, go do it with a lambda\nand stuff like that.",
    "start": "1741940",
    "end": "1749505"
  },
  {
    "text": "And this is Amazon who's trying\nto sell the lambda service, and their Prime Video team has\nthis great blog post from a year",
    "start": "1749505",
    "end": "1755530"
  },
  {
    "text": "and a half ago, which says,\nwe built our whole system on Amazon lambda\nscale out massively,",
    "start": "1755530",
    "end": "1760930"
  },
  {
    "text": "and now we've returned it back\nto a few small servers because of locality and other things. And so they said there are\n10 times more cost efficient",
    "start": "1760930",
    "end": "1768280"
  },
  {
    "text": "with a small number\nof big servers than they are with\nthousands of tiny servers because communication just bites\nyou over, and over, and over",
    "start": "1768280",
    "end": "1776230"
  },
  {
    "text": "in any system. Yeah. Do they still get the\nsame level of performance? Is that like-- Yeah.",
    "start": "1776230",
    "end": "1781380"
  },
  {
    "text": "ISO performance, 10x lower\ncost, and that's similar to some of these observations\nwith Spark.",
    "start": "1781380",
    "end": "1786820"
  },
  {
    "text": "So these systems\nthat are scale out, there's one reason why you\nalways want to-- you to think",
    "start": "1786820",
    "end": "1792510"
  },
  {
    "text": "about scaling out if you\nbelieve that the data sets you're going to\nwork on are going",
    "start": "1792510",
    "end": "1797670"
  },
  {
    "text": "to be so big that you have no\nchance of keeping them in memory anytime soon. That's the reason why these\nsystems are super valuable,",
    "start": "1797670",
    "end": "1805630"
  },
  {
    "text": "right? If I have a database of\nterabytes and terabytes of data, I have no hope of ever\nkeeping that in one node,",
    "start": "1805630",
    "end": "1811870"
  },
  {
    "text": "so I should build\nfundamentally from day one in terms of the data can't\nfit anywhere, let's scale out.",
    "start": "1811870",
    "end": "1818280"
  },
  {
    "text": "But I do want to get\nthis back on topic, so let's move to\nanother question.",
    "start": "1818280",
    "end": "1824250"
  },
  {
    "text": "Yeah. Just a quick question on the\ndefault MSI without like--",
    "start": "1824250",
    "end": "1829481"
  },
  {
    "text": "Let's actually do a\ncache coherence one, I think, that would be a good-- I actually did it in my\nlock-free lecture recently,",
    "start": "1829481",
    "end": "1836290"
  },
  {
    "text": "so let's just pull\nup those slides. So we started the\nlock-free lecture",
    "start": "1836290",
    "end": "1841690"
  },
  {
    "text": "in terms of this MSI protocol. Let me go ahead and pop it up.",
    "start": "1841690",
    "end": "1846730"
  },
  {
    "text": "Now, on the exam-- well, first of all,\nit's open notes. And second of all, if we've\never asked you to do anything",
    "start": "1846730",
    "end": "1853390"
  },
  {
    "text": "with the MSI protocol,\nI've given you this diagram on the exam, so\nmemorizing the diagram probably",
    "start": "1853390",
    "end": "1859270"
  },
  {
    "text": "not necessary. But in general,\nit's hard for you to answer a question without\nthat diagram fully in your head,",
    "start": "1859270",
    "end": "1866330"
  },
  {
    "text": "so they're kind of the same. If you're learning the\nstate transitions-- so should we just\ngo over it, or?",
    "start": "1866330",
    "end": "1873080"
  },
  {
    "text": "There's a question especially\nspecific for atomic CAS. We were saying how it\nwas like a BusRd X,",
    "start": "1873080",
    "end": "1879030"
  },
  {
    "text": "but I thought like if\nyou were [INAUDIBLE] just the default MSI\nhas to do the BusRd",
    "start": "1879030",
    "end": "1884620"
  },
  {
    "text": "and then the BusRd\nX. Is that incorrect? That is incorrect. So let's just say I have\na-- well, let me make sure.",
    "start": "1884620",
    "end": "1890559"
  },
  {
    "text": "I believe it's\nincorrect, so let's just say we have an address\nthat is not in my cache.",
    "start": "1890560",
    "end": "1896570"
  },
  {
    "text": "So if it's not in my\ncache, that address is conceptually in what state? Invalid.",
    "start": "1896570",
    "end": "1902020"
  },
  {
    "text": "It's invalid. Now, keep in mind\nI have a cache, and every line in the cache says\nthis is the current address.",
    "start": "1902020",
    "end": "1907790"
  },
  {
    "text": "So it's not like the cache\nhas a bit for invalid on all possible addresses,\nit's just if it's not",
    "start": "1907790",
    "end": "1913150"
  },
  {
    "text": "in the cache it's invalid. So then I do a read,\nand if I do a read,",
    "start": "1913150",
    "end": "1919120"
  },
  {
    "text": "I will read it into\nthe shared state. Correct? And then let's say I\nwant to write to it.",
    "start": "1919120",
    "end": "1926330"
  },
  {
    "text": "If nobody else has\nwritten or read it, it can be in the\nmodified state now.",
    "start": "1926330",
    "end": "1932680"
  },
  {
    "text": "OK, so what is the question? The question was like, that\nseems like a two-stage process.",
    "start": "1932680",
    "end": "1939380"
  },
  {
    "text": "That is a two-stage process. So for atomic CAS since we're\nreading and then writing,",
    "start": "1939380",
    "end": "1944480"
  },
  {
    "text": "is it that two-stage-- Absolutely not, that's\nwhy I'm saying it's a write bus transaction, right? Because think of\nwhat can happen here.",
    "start": "1944480",
    "end": "1950870"
  },
  {
    "text": "Let's say we don't\nimplement CAS atomically, the processor will elevate\nto the shared state.",
    "start": "1950870",
    "end": "1958520"
  },
  {
    "text": "If it's not atomic some\nother processor can now do whatever it wants, which\nit might generate bus traffic,",
    "start": "1958520",
    "end": "1965120"
  },
  {
    "text": "it might write to the value,\nit might read from the value, and it will do its thing. And then when I do the\nwrite, perhaps it's",
    "start": "1965120",
    "end": "1971680"
  },
  {
    "text": "still in shared or not, and\nit will then get elevated. So that's why the nature of\nCAS, or test-and-set, or any",
    "start": "1971680",
    "end": "1980800"
  },
  {
    "text": "of these things\nbeing atomic means you need to think about\nthem from the performance protocol as a\nwrite, irregardless",
    "start": "1980800",
    "end": "1987940"
  },
  {
    "text": "of whether the\nvariable is actually updated by the transaction. So if I wanted to\nCAS a variable,",
    "start": "1987940",
    "end": "1994549"
  },
  {
    "text": "it's going to go from i all\nthe way to m write at once. ",
    "start": "1994550",
    "end": "2000820"
  },
  {
    "text": "Otherwise, I would need\nspecial other logic to ensure the atomicity\nof that operation.",
    "start": "2000820",
    "end": "2007600"
  },
  {
    "text": "In light of MSI, this is\nsomething we did last time, would you like to go\nthrough a sequence? No, or are there any\nhigher priority items?",
    "start": "2007600",
    "end": "2015110"
  },
  {
    "text": "I see some yeses. Yes. OK. Seems to be general. So this is the same\nsequence as last time.",
    "start": "2015110",
    "end": "2021470"
  },
  {
    "text": "I mean, I think,\nit's we didn't even do the whole thing last time. Let's go ahead and just do it. And I think this\nactually makes sense",
    "start": "2021470",
    "end": "2029170"
  },
  {
    "text": "if we have just\nphysically feeling",
    "start": "2029170",
    "end": "2034390"
  },
  {
    "text": "it is if someone wants to come\nup here and be the other-- if we have two caches,\nit can be very helpful,",
    "start": "2034390",
    "end": "2039669"
  },
  {
    "text": "so let's just do it. I'll have somebody\nelse come here.",
    "start": "2039670",
    "end": "2045220"
  },
  {
    "text": "I need two folks. If you're sleeping and\nyou need some energy.",
    "start": "2045220",
    "end": "2050395"
  },
  {
    "text": "It's like the\nfirst day of class, so you're going to be cache one. Nice. And I need cache-- I can be cache two, or\nsomebody else can be cache two.",
    "start": "2050395",
    "end": "2058060"
  },
  {
    "text": "We got cache two, why not? OK, so here we go. And this is what I want to--",
    "start": "2058060",
    "end": "2066260"
  },
  {
    "text": "nice. You got to be above 6 feet tall\nto be a cache in this class.",
    "start": "2066260",
    "end": "2071325"
  },
  {
    "text": "That's why I'm not volunteering. OK. So here's the sequence,\nwhich you can look at it. So let's say that just to keep\nyou guys in the same seat.",
    "start": "2071326",
    "end": "2078149"
  },
  {
    "text": "So let's have you be 0 and you\nbe 1, I'm forgetting names, sorry.",
    "start": "2078150",
    "end": "2083310"
  },
  {
    "text": "So we have cache 0 and cache\n1, and let's get started here. And what I want you to\ncommunicate like the protocol,",
    "start": "2083310",
    "end": "2090820"
  },
  {
    "text": "and then you can write on\nthe board like the state that your line is in. So you can write on\nthe board something",
    "start": "2090820",
    "end": "2095940"
  },
  {
    "text": "like I have x in\nthe shared state.",
    "start": "2095940",
    "end": "2101355"
  },
  {
    "text": "So at the moment you get nobody\nhas anything in their cache, so you have nothing.",
    "start": "2101355",
    "end": "2106517"
  },
  {
    "text": "And then so let's just say\nthe first thing happens is you say you want to load. OK, I have an initiative state.",
    "start": "2106517",
    "end": "2113635"
  },
  {
    "text": "Well, not yet. Not yet. You actually have to\nsay, I want to load. I want to load. OK.",
    "start": "2113635",
    "end": "2118965"
  },
  {
    "text": "And now you have to\nacknowledge, and once you get the acknowledgment,\nyou're like, that's cool.",
    "start": "2118965",
    "end": "2125589"
  },
  {
    "text": "I know that nobody's\nwriting to it, so now I can put it in the\nshared state, so let's go ahead and write x and shared there.",
    "start": "2125590",
    "end": "2132870"
  },
  {
    "text": "OK. All right. So we got that acknowledgment,\nand then, again,",
    "start": "2132870",
    "end": "2138570"
  },
  {
    "text": "you load x, so what do you do?",
    "start": "2138570",
    "end": "2143830"
  },
  {
    "text": "Nothing. You don't have to\ndo anything, right? And that's an important detail. There's not even a message\nthat has to be sent.",
    "start": "2143830",
    "end": "2149920"
  },
  {
    "text": "Why does a message\nnot need to be sent? We have [INAUDIBLE] By the nature of cash zero\nhaving it in a shared state,",
    "start": "2149920",
    "end": "2157640"
  },
  {
    "text": "we are guaranteed that everybody\nelse might have it in the shared",
    "start": "2157640",
    "end": "2163976"
  },
  {
    "text": "state, but they certainly don't\nhave it in the modified state, and it's OK to continue to\nread, so nothing changes",
    "start": "2163977",
    "end": "2170280"
  },
  {
    "text": "and we're good. OK, so now you need to write.",
    "start": "2170280",
    "end": "2175510"
  },
  {
    "text": "Yeah, I want to write. OK, I see that. I guess, I still\ndo nothing, right?",
    "start": "2175510",
    "end": "2182380"
  },
  {
    "text": "Yeah. So you're going to do nothing,\nand why do you shrug this off? That's not even my thing. You don't even\nhave in the cache.",
    "start": "2182380",
    "end": "2187800"
  },
  {
    "text": "You can be like, look, do\nwhatever the heck you want. I'm not doing anything. So what are you going\nto do in your cache now?",
    "start": "2187800",
    "end": "2193770"
  },
  {
    "text": "We're going to move from\nthe S to the M state. And that M state\nmeans, now that cache 0",
    "start": "2193770",
    "end": "2200849"
  },
  {
    "text": "is guaranteed that no\nother cache has the value. So you can proceed\nwith your write.",
    "start": "2200850",
    "end": "2206859"
  },
  {
    "text": "And in this case,\nactually, let's go ahead and write the value. You just updated the\nvalue, so it's X and M",
    "start": "2206860",
    "end": "2212670"
  },
  {
    "text": "and has the value 1. ",
    "start": "2212670",
    "end": "2218230"
  },
  {
    "text": "And so if I'm like the\nthird party, the under 6 foot memory system,\nthere's a value",
    "start": "2218230",
    "end": "2224950"
  },
  {
    "text": "of x and y out here in memory. And what's the value of x?",
    "start": "2224950",
    "end": "2230600"
  },
  {
    "text": "0. It's actually still 0. It's still. OK. Now, let's keep going.",
    "start": "2230600",
    "end": "2236497"
  },
  {
    "text": "So processor 0. OK, so you're being greedy. You're going to write\nagain, and you're",
    "start": "2236497",
    "end": "2241779"
  },
  {
    "text": "going to update the value to\n2, which, again, no update--",
    "start": "2241780",
    "end": "2247100"
  },
  {
    "text": "no coherence. traffic needs to\nhappen because we're guaranteed that cache 1 does\nnot have the data,",
    "start": "2247100",
    "end": "2253017"
  },
  {
    "text": "and there's no business\ntelling somebody something that they don't need to know. Value is 2, right?",
    "start": "2253017",
    "end": "2258410"
  },
  {
    "text": "Cool. OK. Now, processor 1. Now you're going to read it.",
    "start": "2258410",
    "end": "2266050"
  },
  {
    "text": "Did I get that right? No, processor 1 is me. Sorry. Yeah. You're storing. OK, so now some interesting\nstuff that has to happen.",
    "start": "2266050",
    "end": "2272258"
  },
  {
    "text": " OK. I need that because I need\nto run through [INAUDIBLE]",
    "start": "2272258",
    "end": "2279760"
  },
  {
    "text": "Yeah. So hold on. Let's sequence this\nvery precisely. You correctly said,\nhey, I need to read.",
    "start": "2279760",
    "end": "2286930"
  },
  {
    "text": "How do you react? I say, I've written to it. Oh, you get the flash memory.",
    "start": "2286930",
    "end": "2292150"
  },
  {
    "text": "Yeah, so you've got to\nflush, so in some sense, you send the value back\nto me, and I update this",
    "start": "2292150",
    "end": "2299230"
  },
  {
    "text": "with the value 2. You can-- and then what do\nyou do about your cache line?",
    "start": "2299230",
    "end": "2305650"
  },
  {
    "text": "And then, yeah. OK. And it's invalid, and yeah. And now you have the value\nof this cache line, which",
    "start": "2305650",
    "end": "2313660"
  },
  {
    "text": "you read from memory,\nand it has the value 2, and then you update\nits value to 3.",
    "start": "2313660",
    "end": "2320800"
  },
  {
    "text": "You see that whole sequence? Now, if those of you who\nare wondering why did this have to go back to memory\nand then all the way back,",
    "start": "2320800",
    "end": "2326750"
  },
  {
    "text": "why not just transfer\nthe cache line from one cache to the other? Any modern system has this.",
    "start": "2326750",
    "end": "2333200"
  },
  {
    "text": "So you studied MSI,\nyou studied MESI, and let's talk about\nthat for a second.",
    "start": "2333200",
    "end": "2339240"
  },
  {
    "text": "Intel AMD modern cache\ncoherence systems are actually five-stage systems, and that\nfifth stage is a special shared",
    "start": "2339240",
    "end": "2346400"
  },
  {
    "text": "state, which basically says\nif someone else needs the line and I have it, I will\nprovide it to them",
    "start": "2346400",
    "end": "2354140"
  },
  {
    "text": "without them having to\ngo get it from memory. So it's just an\nobvious optimization that you're all wearing, but\nthat optimization is not an MSI.",
    "start": "2354140",
    "end": "2360890"
  },
  {
    "text": "OK, where are we now? I just read it, so nothing. Yeah.",
    "start": "2360890",
    "end": "2365970"
  },
  {
    "text": "So your next one\nis you just read, and that's like we're good. Yeah. And now, processor\n0 wants to load.",
    "start": "2365970",
    "end": "2374720"
  },
  {
    "text": "So I got to tell you to write-- flush it to memory. Yeah, so first of\nall, you go, I got it.",
    "start": "2374720",
    "end": "2380027"
  },
  {
    "text": "Hold on. We got to do a flush. We flush to 3, and now--",
    "start": "2380027",
    "end": "2385950"
  },
  {
    "text": "Then I set this to invalid\nand I give you the marker. ",
    "start": "2385950",
    "end": "2396790"
  },
  {
    "text": "OK, so this is an interesting\ndetail, so you went to invalid. Now, first of all,\nhere's a question.",
    "start": "2396790",
    "end": "2403940"
  },
  {
    "text": "This is not the MSI\nprotocol what you executed, but is it correct?",
    "start": "2403940",
    "end": "2409390"
  },
  {
    "text": "As in is it correct\ncache coherence? It's absolutely correct\ncache coherence. [INAUDIBLE] was sharing that.",
    "start": "2409390",
    "end": "2415510"
  },
  {
    "text": "Well, I want to make\nit clear that what you did is correct in that cache\ncoherence is still working,",
    "start": "2415510",
    "end": "2421819"
  },
  {
    "text": "there's just an inefficiency,\nand the inefficiency is you could have kept\nit in your shared state.",
    "start": "2421820",
    "end": "2427670"
  },
  {
    "text": "Keep in mind that the fact\nthat processor 1 is invalid, this being in the\nshared state is",
    "start": "2427670",
    "end": "2433060"
  },
  {
    "text": "OK because all this guarantees\nis that cache 0 can't write to the value, so it was correct,\njust not a correct application",
    "start": "2433060",
    "end": "2441640"
  },
  {
    "text": "of the MSI protocol. So yes, if you were following\nMSI, you would have said, I'm going to keep this\naround in shared state",
    "start": "2441640",
    "end": "2447340"
  },
  {
    "text": "because there's no\nreason for me to drop it, and I might read from it again,\nand if I read from it again, it would be great not to have\nto go get it from memory,",
    "start": "2447340",
    "end": "2455010"
  },
  {
    "text": "so now we're in a\nnice shared state. And now, it might get\na little repetitive, but let's go ahead\nand do it anyways.",
    "start": "2455010",
    "end": "2461112"
  },
  {
    "text": "So we have store,\nso you are storing.",
    "start": "2461112",
    "end": "2466500"
  },
  {
    "text": "I have exclusive access. OK. I think you've had it, but yeah. Did you have it?",
    "start": "2466500",
    "end": "2472280"
  },
  {
    "text": "Do you have it? Let's see. Let's look at your caches. And actually, remember,\nyou're doing all of this",
    "start": "2472280",
    "end": "2477890"
  },
  {
    "text": "without ever seeing the\nother caches data structures. I'm just sitting the bus. Well, how do you know that\nanother cache may have it?",
    "start": "2477890",
    "end": "2487970"
  },
  {
    "text": "With only your own\ndata structures, and I'm blocking\neverything, right. Yes. Do I just-- Look here.",
    "start": "2487970",
    "end": "2493380"
  },
  {
    "text": "Yeah. It's in shared. It's in shared, so you have no\nguarantees on perhaps somebody has it, right?",
    "start": "2493380",
    "end": "2498720"
  },
  {
    "text": "So that's how you know\nwhat you can and cannot do. But in this case\nthere's a store,",
    "start": "2498720",
    "end": "2503790"
  },
  {
    "text": "so you have to hold on,\nnotify everybody first.",
    "start": "2503790",
    "end": "2509740"
  },
  {
    "text": "Yeah. I do. Because now keep in\nmind, notice what happens if cache 1\nupdated that value",
    "start": "2509740",
    "end": "2517670"
  },
  {
    "text": "and changed its state\nbefore notifying cache 1?",
    "start": "2517670",
    "end": "2523505"
  },
  {
    "text": "It may have proceeded with the\nright, processor 1 if it doesn't know that it needs to tear down\nand invalidate might continue",
    "start": "2523505",
    "end": "2530450"
  },
  {
    "text": "with reads, and that read may\nbe potentially out of date. So before you update\nthat variable,",
    "start": "2530450",
    "end": "2537299"
  },
  {
    "text": "you have to get\nexclusive access. To get exclusive\naccess, you have to say, I intend to right\neverybody else playing--",
    "start": "2537300",
    "end": "2543547"
  },
  {
    "text": "all the other caches\nplaying the game go, oh, OK, I got to\nget rid of this thing. Go ahead, and now you\ncan perform your write.",
    "start": "2543548",
    "end": "2550190"
  },
  {
    "text": "Good. And then would he write\nthat to memory now? Or I guess--",
    "start": "2550190",
    "end": "2557180"
  },
  {
    "text": "Well, let's talk about it. So there's a difference between\nwhat the MSI protocol is",
    "start": "2557180",
    "end": "2562250"
  },
  {
    "text": "and what would be correct to\ndo, but potentially inefficient. So we didn't write to memory\nafter the write last time,",
    "start": "2562250",
    "end": "2569540"
  },
  {
    "text": "and we didn't do that\nbecause why are you guaranteed to get the\nupdated value whenever",
    "start": "2569540",
    "end": "2574760"
  },
  {
    "text": "you read in the future? Let's think about it,\nso what's the process for reading in the future?",
    "start": "2574760",
    "end": "2580015"
  },
  {
    "text": "And in fact, actually the\nnext line is you reading, so let's just go ahead. The next line is\nprocessor 1 loads x.",
    "start": "2580015",
    "end": "2586750"
  },
  {
    "text": "So you're reading memory\nis out of date with three, you have the invalid cache line.",
    "start": "2586750",
    "end": "2592618"
  },
  {
    "text": "So then that would\ntrigger the flush? Correct, so you state\nyour intent to read-- OK, I need to read. You go, oop.",
    "start": "2592618",
    "end": "2598220"
  },
  {
    "text": " And the memory-- you're\ngoing to flush to me.",
    "start": "2598220",
    "end": "2603595"
  },
  {
    "text": "The other thing we're\nnot writing down is whether or not it's dirty. But I guess if it's in the M\nstate, it's dirty, so was it 4?",
    "start": "2603595",
    "end": "2609380"
  },
  {
    "text": "Yeah. OK. Yeah. 4, now you can get 4. Now I can get it. Yep. ",
    "start": "2609380",
    "end": "2619160"
  },
  {
    "text": "And we're good. Now, there's one\nmore detail here. This is the first time that\nit's a load of y and not x,",
    "start": "2619160",
    "end": "2626970"
  },
  {
    "text": "so both of you have\nx in the S state, and now there's a load of y.",
    "start": "2626970",
    "end": "2632890"
  },
  {
    "text": " Yeah. I can just load y.",
    "start": "2632890",
    "end": "2638500"
  },
  {
    "text": "Well, you have to say\nyou have to load y. I'm going to load y. OK. And you shrug it off. Yeah.",
    "start": "2638500",
    "end": "2643900"
  },
  {
    "text": "And you shrug it off\nbecause you don't have y. So just keep in mind that\ncoherence is actually about that diagram\nor that MSI diagram",
    "start": "2643900",
    "end": "2651460"
  },
  {
    "text": "is about keeping coherence\nbetween the same address, right? There's in some sense,\nthere's another copy",
    "start": "2651460",
    "end": "2657760"
  },
  {
    "text": "of this diagram, another\nstate machine rolling around for any other address\nbecause there might",
    "start": "2657760",
    "end": "2662859"
  },
  {
    "text": "be multiple values in cache. In this case, you're keeping\nY and S, and X and S,",
    "start": "2662860",
    "end": "2668330"
  },
  {
    "text": "and then finally, when\nprocessor 1 goes to write Y.",
    "start": "2668330",
    "end": "2674950"
  },
  {
    "text": "I need to modify Y. Modify Y. You shrug it off\nbecause you don't care. And then if you ever\nwrite Y, now you're",
    "start": "2674950",
    "end": "2682853"
  },
  {
    "text": "going to have to request it. There'll be a tear down. There'll be a flush of one,\nand then so on, and so on.",
    "start": "2682853",
    "end": "2688580"
  },
  {
    "text": "Exactly. OK. This is the whole-- so it's very\nimportant to think about of any",
    "start": "2688580",
    "end": "2694510"
  },
  {
    "text": "change in state to my local\ncache will result in shouting",
    "start": "2694510",
    "end": "2700580"
  },
  {
    "text": "to all the other caches because\nthe data structures of the other caches have state in them that\nallows those caches to make",
    "start": "2700580",
    "end": "2708950"
  },
  {
    "text": "decisions about what\nthey can and cannot do, and if my state changes, I\ngot to tell everybody else.",
    "start": "2708950",
    "end": "2715970"
  },
  {
    "text": "How we tell everybody\nelse is not something we talk about in this class. We talk about it as shouting\nas if it's a broadcast,",
    "start": "2715970",
    "end": "2721890"
  },
  {
    "text": "but typically under\nthe hood it'll be point to point messages\nand stuff like that. And that's a different\ndetail for another class.",
    "start": "2721890",
    "end": "2728290"
  },
  {
    "text": "All right. Thank you very much\nfor your demonstration, but any other questions\non cache coherence? Yes.",
    "start": "2728290",
    "end": "2735020"
  },
  {
    "text": "One cache has data\nin modified state. OK. One cache has the data\nin the modified state. OK.",
    "start": "2735020",
    "end": "2741000"
  },
  {
    "text": "Other guys wants to\nread or write to it, do we have to send the\ndata back to memory if we can't see cache\nmemory which has",
    "start": "2741000",
    "end": "2746820"
  },
  {
    "text": "the data in the modified state? So let's say that cache 0 has\nthe data in the modified state.",
    "start": "2746820",
    "end": "2753110"
  },
  {
    "text": "Cache 1 wants to write to\nit, and what's the question? Why does the data need\nto come from the memory?",
    "start": "2753110",
    "end": "2759880"
  },
  {
    "text": "Why does the first\nprocessor need to flush the data back to\nmemory, and then processor 1",
    "start": "2759880",
    "end": "2766119"
  },
  {
    "text": "gets it from memory\nrather than processor 0? Is your question why\ndoes processor 0 not",
    "start": "2766120",
    "end": "2771760"
  },
  {
    "text": "give the data to processor 1? Well, that's what I\nwas saying earlier. In a more complicated\ncache coherence protocol,",
    "start": "2771760",
    "end": "2777940"
  },
  {
    "text": "five state protocol it's\ncalled like MSF or MOSI, there's a fifth\nstate which says,",
    "start": "2777940",
    "end": "2784000"
  },
  {
    "text": "if you're trying to\nget data from memory, and another cache has a up\nto date copy of the data,",
    "start": "2784000",
    "end": "2792610"
  },
  {
    "text": "that cache will\nprovide the data. But I do want to be\nclear about one thing.",
    "start": "2792610",
    "end": "2799540"
  },
  {
    "text": "Remember, we're talking\nabout cache lines here, even though we\nassumed in that exercise",
    "start": "2799540",
    "end": "2804670"
  },
  {
    "text": "that the cache line\nsize was one value. So when we say load X, we really\nmean the cache line containing",
    "start": "2804670",
    "end": "2813579"
  },
  {
    "text": "X. So let's say X is here, and\nI write 4 to X. Keep in mind",
    "start": "2813580",
    "end": "2821210"
  },
  {
    "text": "that there's data here in\nthe rest of the cache line that is untouched.",
    "start": "2821210",
    "end": "2828530"
  },
  {
    "text": "So the reason why I\nwrite it back out, the cache line all the\nway back out to memory",
    "start": "2828530",
    "end": "2834050"
  },
  {
    "text": "is that I need to update\nthat whole cache line. I can't just send\n4 because I don't",
    "start": "2834050",
    "end": "2840859"
  },
  {
    "text": "know what's dirty in\nthis cache line at all. So just make sure\neven though that demo was in terms of single\nvalues, the information that's",
    "start": "2840860",
    "end": "2848870"
  },
  {
    "text": "being flushed or communicated\nin a more advanced protocol, our entire cache lines.",
    "start": "2848870",
    "end": "2855060"
  },
  {
    "text": "You would have to have a\nmore advanced cash that was going to track dirtiness at\nthe bit or at the byte level.",
    "start": "2855060",
    "end": "2861630"
  },
  {
    "text": "Yeah. So you just mentioned\nlike Internal CPUs",
    "start": "2861630",
    "end": "2866810"
  },
  {
    "text": "that there is actually\nthe future state, which can allow chache to transfer\nfrom one form to another.",
    "start": "2866810",
    "end": "2874400"
  },
  {
    "text": "Does that take care\nof the situation? You mean, what that does\nis avoids the flush.",
    "start": "2874400",
    "end": "2881869"
  },
  {
    "text": "It avoids the flush. So if I'm a cash, and I have\nthe line, in the shared state",
    "start": "2881870",
    "end": "2887630"
  },
  {
    "text": "that you need, if you read it,\nI'll just give you the line. And now we both have it shared.",
    "start": "2887630",
    "end": "2892820"
  },
  {
    "text": "If I have a line in the modified\nstate, and you need it to read,",
    "start": "2892820",
    "end": "2899060"
  },
  {
    "text": "I will give it to you,\nand flush it to memory, and we'll both be shared.",
    "start": "2899060",
    "end": "2905404"
  },
  {
    "text": "If I have it in\nthe modified state and you want it in\nthe modified state, I can actually just give\nit to you without flushing,",
    "start": "2905404",
    "end": "2912840"
  },
  {
    "text": "and you have it in\nthe modified state, and now you're responsible\nfor flushing it. So that's [INAUDIBLE]",
    "start": "2912840",
    "end": "2920000"
  },
  {
    "text": "Yeah. So that's more advanced\ndetails that we don't. So what about this messy,\nthis four-stage protocol?",
    "start": "2920000",
    "end": "2926983"
  },
  {
    "text": "This might be a good\ncheck your understanding. Do we have-- I don't know if I have Conely's\nslides, so hold on one second.",
    "start": "2926983",
    "end": "2935580"
  },
  {
    "text": "So in MESI you still\nhave the same modified, shared, and invalid\nstates as before,",
    "start": "2935580",
    "end": "2943090"
  },
  {
    "text": "but there's this\nlittle exclusive state. ",
    "start": "2943090",
    "end": "2948360"
  },
  {
    "text": "Did we talk about what that-- yeah, we did. And what's the advantage\nof this fourth state?",
    "start": "2948360",
    "end": "2953680"
  },
  {
    "text": "And let's think about\na particular use case where you read a value and\nthen at some point later",
    "start": "2953680",
    "end": "2959490"
  },
  {
    "text": "you write to it. Very common case in\ncomputer science. Yeah. [INAUDIBLE]",
    "start": "2959490",
    "end": "2966420"
  },
  {
    "text": "Correct. So let's think about\nMSI in the case where a thread reads\nand then writes. If a thread reads, I bring\nit in as a shared state.",
    "start": "2966420",
    "end": "2975150"
  },
  {
    "text": "All I know at this point\nis that there may be other processors with the data.",
    "start": "2975150",
    "end": "2980700"
  },
  {
    "text": "So if I go to write to it,\nI also have to speak up and say, hey, I'm\ngoing to write now.",
    "start": "2980700",
    "end": "2985950"
  },
  {
    "text": "OK. MESI says, when you\nload data, when I say,",
    "start": "2985950",
    "end": "2991930"
  },
  {
    "text": "hey, I want to\nload the data, you look at what the other\nperson says, and you say,",
    "start": "2991930",
    "end": "2997510"
  },
  {
    "text": "if the other person\nsays, oh, yeah, I've got that in the\nshared state, you go, OK, I should have it\nin the shared state",
    "start": "2997510",
    "end": "3002627"
  },
  {
    "text": "too because I know that it's\npossible for other people to have the data. If nobody says I have it in the\nshared state, if everybody goes,",
    "start": "3002627",
    "end": "3010130"
  },
  {
    "text": "I don't care, why\nare you telling me? I can load it in the state\nwhich says that it's not dirty.",
    "start": "3010130",
    "end": "3016440"
  },
  {
    "text": "I haven't written to\nit, but I'm guaranteed that nobody else has it. And so if I ever want to write\nto it without telling anybody,",
    "start": "3016440",
    "end": "3023789"
  },
  {
    "text": "I can just flip from E to M\nbecause nobody else has it, so nobody cares about what I\nhave to say about that line.",
    "start": "3023790",
    "end": "3030420"
  },
  {
    "text": "Yeah. What if someone else\nleaves it at the same time? Should you have something\nthat's just set for me?",
    "start": "3030420",
    "end": "3035640"
  },
  {
    "text": "So if I have it in\nthe exclusive state-- and well, I haven't\nfinished the diagram, sorry.",
    "start": "3035640",
    "end": "3044130"
  },
  {
    "text": "If I have it in\nthe exclusive state and I hear someone else's\nread, what do I do?",
    "start": "3044130",
    "end": "3050320"
  },
  {
    "text": "I dropped a shared, and I hope\nthat should be on the diagram now. I still haven't\nfinished the diagram. Sorry. OK, so if I'm in the exclusive\nstate, and I see a bus read,",
    "start": "3050320",
    "end": "3060299"
  },
  {
    "text": "I don't do anything\nand I drop to S. Yeah.",
    "start": "3060300",
    "end": "3066610"
  },
  {
    "text": "After the cache\n[INAUDIBLE] value, can you ever go back\nto, specifically, like a [INAUDIBLE]\nvalue or share, then--",
    "start": "3066610",
    "end": "3074027"
  },
  {
    "text": "Do you ever go back\nto the [INAUDIBLE] after another\npositive [INAUDIBLE]? And you can see it\nfrom this diagram",
    "start": "3074027",
    "end": "3080440"
  },
  {
    "text": "that there is no\ntransition, right? What it would take to do that. Let's talk about let's\ndesign a cache coherence",
    "start": "3080440",
    "end": "3087039"
  },
  {
    "text": "protocol that's better. What would it take to be\nable to elevate from S to E?",
    "start": "3087040",
    "end": "3095910"
  },
  {
    "text": "You have to tell all the\nprocessors to make it invalid. You'd have to tell all the\nother processors to go invalid.",
    "start": "3095910",
    "end": "3103589"
  },
  {
    "text": "So you would have it\nin your shared state, and then every time\nyou wrote to it, you would have to tell all\nthe other processors, hey,",
    "start": "3103590",
    "end": "3110098"
  },
  {
    "text": "I'm writing again. You have it still? So that kind of almost defeats\nthe value of the shared state.",
    "start": "3110098",
    "end": "3117070"
  },
  {
    "text": "But what we could do is that\nwhenever any processor went to invalid, normally,\nwhen you go to invalid,",
    "start": "3117070",
    "end": "3124930"
  },
  {
    "text": "it's in reaction\nto other things. Let's say you go to\ninvalid because you",
    "start": "3124930",
    "end": "3130200"
  },
  {
    "text": "got evicted from your cache,\nnot based on anybody else. You could tell everybody\nelse I'm going to invalid,",
    "start": "3130200",
    "end": "3136320"
  },
  {
    "text": "and that could be a trigger\nfor all the caches to report do I have it in the\nshared state or not? And if only one cache had\nit in the shared state,",
    "start": "3136320",
    "end": "3143700"
  },
  {
    "text": "then you would know\nyou could elevate to E, just be a more advanced\ncache coherence protocol.",
    "start": "3143700",
    "end": "3148740"
  },
  {
    "text": "Yeah. A question over here. When we loaded in pi at the\nend, we knew that X was not",
    "start": "3148740",
    "end": "3155770"
  },
  {
    "text": "in modified state, but\nif when we load in Y, X was already in\na modified state,",
    "start": "3155770",
    "end": "3161150"
  },
  {
    "text": "would we be required\nto first flush out? This is a great question. I want to make sure\neverybody understands it.",
    "start": "3161150",
    "end": "3166435"
  },
  {
    "text": "So the question was, we\nhave X in some state, let's say it's the\nmodified state, the cache line X. If the cache\nline Y is loaded into the cache,",
    "start": "3166435",
    "end": "3176440"
  },
  {
    "text": "does that change the\nbehavior of X in any way? No.",
    "start": "3176440",
    "end": "3181809"
  },
  {
    "text": "Right? Because they're just,\nthey are different. Coherence is about\na state machine for every address in the\nsystem, and different addresses",
    "start": "3181810",
    "end": "3189549"
  },
  {
    "text": "do not interact. The only way they interact is\nnothing to do with parallelism is maybe bringing cache\nline Y into the cache,",
    "start": "3189550",
    "end": "3197420"
  },
  {
    "text": "causing an eviction\nof cache line X, and then that would be something\nthat caused X to invalidate,",
    "start": "3197420",
    "end": "3204010"
  },
  {
    "text": "and then it would have\nto flush, but that's unrelated to the discussion\nof cache coherence, that's just a normal cache behavior.",
    "start": "3204010",
    "end": "3210369"
  },
  {
    "text": "OK. All right. If we have an example\nwhere our cache is full,",
    "start": "3210370",
    "end": "3216140"
  },
  {
    "text": "and then we're going\nto do some operation, is there like a\n[INAUDIBLE] write back",
    "start": "3216140",
    "end": "3222339"
  },
  {
    "text": "before we even do\nlike a processor read or something like that? Absolutely. Let's say, and this is we could\ntalk about normal cache behavior",
    "start": "3222340",
    "end": "3229900"
  },
  {
    "text": "unrelated to coherence. So imagine that\nthe cache you're-- and by the way, the\ncache is always full,",
    "start": "3229900",
    "end": "3236680"
  },
  {
    "text": "but you read a new\nvalue, which means you're going to have to\nkick out an existing value. You decide which one to kick\nout using whatever policy.",
    "start": "3236680",
    "end": "3243950"
  },
  {
    "text": "And if it is dirty,\nthe process is write--",
    "start": "3243950",
    "end": "3249010"
  },
  {
    "text": "or OK. So if it's dirty, it's\nalready in the modified state. So the process will\nbe flush the data",
    "start": "3249010",
    "end": "3254920"
  },
  {
    "text": "and go to invalid, and then\nBusRd X on the new thing, or sorry, BusRd\non the new thing.",
    "start": "3254920",
    "end": "3260050"
  },
  {
    "text": "And will that like [INAUDIBLE]\nthe model here with-- No, because the other line\nis some other address.",
    "start": "3260050",
    "end": "3266750"
  },
  {
    "text": "And again, this is how\nwe take this one address. So from the perspective of\nthis cache coherence diagram,",
    "start": "3266750",
    "end": "3273950"
  },
  {
    "text": "you just would not initiate\nthe bus read X on address X until all the appropriate\nroom in the cache",
    "start": "3273950",
    "end": "3281350"
  },
  {
    "text": "has been made to\nput X in your cache. So there would be some\nother diagram for state Y",
    "start": "3281350",
    "end": "3288190"
  },
  {
    "text": "that would now be moving from\nmodified or shared to invalid. Yes.",
    "start": "3288190",
    "end": "3293320"
  },
  {
    "text": "OK. So you mentioned\nthat this is state is maintaining her address.",
    "start": "3293320",
    "end": "3299480"
  },
  {
    "text": "Correct. Per cache line. Per cache line. Yeah. OK, should we move on to a\ndifferent topic or still--",
    "start": "3299480",
    "end": "3306530"
  },
  {
    "text": "yeah, let's move on, unless\nyou had one last follow up. Just a different topic. OK.",
    "start": "3306530",
    "end": "3311540"
  },
  {
    "text": "So maybe shout out\nsome different topics because I think if we\ngo an extended session, we can either get two more in or\none more in, so let's aggregate.",
    "start": "3311540",
    "end": "3318369"
  },
  {
    "text": "What are some topics? I want to choose segmented scan. Segmented scan. OK.",
    "start": "3318370",
    "end": "3323960"
  },
  {
    "text": "Data parallel thinking. Anything else? CUDA. CUDA in general, you might need\nto be more specific than that.",
    "start": "3323960",
    "end": "3329210"
  },
  {
    "text": "Yeah. Just like how do you warps? How do you say\nthey're organised?",
    "start": "3329210",
    "end": "3334690"
  },
  {
    "text": "How are warps organized\non a machine, and then? Memory consistency and\nhow to solve the problem--",
    "start": "3334690",
    "end": "3342130"
  },
  {
    "text": "No secret sauce,\njust think about all. I mean. OK, so one-- OK, let me answer\nthat one first because I",
    "start": "3342130",
    "end": "3347380"
  },
  {
    "text": "don't think doing a\nproblem will be the best. I think you're just\ndoing it on your own will be best is, one of\nthe things in this class",
    "start": "3347380",
    "end": "3354880"
  },
  {
    "text": "is very much a\nskill that we want you to have is think\nthrough what operation",
    "start": "3354880",
    "end": "3360250"
  },
  {
    "text": "is running on a machine, at what\ntime, like what's the schedule. So you're like, oh, this thread\nis running this instruction",
    "start": "3360250",
    "end": "3367120"
  },
  {
    "text": "on this core, when\nthis thread is running this instruction on this core. OK, so there was a\npractice problem,",
    "start": "3367120",
    "end": "3373247"
  },
  {
    "text": "if I remember correctly this\nweek, which was instruction interleavings, right? So we said, instruction-- let's\nsay I just have a thread T0 that",
    "start": "3373247",
    "end": "3386240"
  },
  {
    "text": "does instruction 1, 2, and 3. And I have another thread\nT1 that does 4, 5, and 6.",
    "start": "3386240",
    "end": "3397080"
  },
  {
    "text": "OK, so the only\nthing that's required is that T1 better do 1 before-- better observe 1, then\n2, then 3, and T1 may",
    "start": "3397080",
    "end": "3405648"
  },
  {
    "text": "be on a different processor\nor a different thread on the same core has\nto do 4, then 5, and 6.",
    "start": "3405648",
    "end": "3411290"
  },
  {
    "text": "If I put these into\na timeline, there's nothing preventing 1 coming\nbefore 4, 4 coming after 2,",
    "start": "3411290",
    "end": "3419400"
  },
  {
    "text": "3 happening in here,\nand so on, and so on. We want you to be able to think\nthrough all the possibilities",
    "start": "3419400",
    "end": "3425240"
  },
  {
    "text": "of what could happen and what\nthe observed results would be. OK, so what I just did\nthere kind of assumes",
    "start": "3425240",
    "end": "3433010"
  },
  {
    "text": "sequential consistency, right? There's some total\norder that exists. When we go to relax\nconsistency, what can happen",
    "start": "3433010",
    "end": "3441380"
  },
  {
    "text": "is there is not a possible way\nto put all six operations just",
    "start": "3441380",
    "end": "3446730"
  },
  {
    "text": "on a timeline when you say\nthe effects of the program are consistent with\nthis being the order.",
    "start": "3446730",
    "end": "3452670"
  },
  {
    "text": "That's what relaxed consistency\nis, is like if this is a right and this is a right, well,\nthis processor clearly",
    "start": "3452670",
    "end": "3459990"
  },
  {
    "text": "has to do 1 before 2\nbecause that's the program. But the values, those updates\nmight appear in different orders",
    "start": "3459990",
    "end": "3469020"
  },
  {
    "text": "to somebody else. So that's the whole concept. There's not a lot\nof magic about it,",
    "start": "3469020",
    "end": "3474070"
  },
  {
    "text": "and it's different\nfrom coherence because here we're talking about\nthe order of the effects of 1",
    "start": "3474070",
    "end": "3482520"
  },
  {
    "text": "and the effects of 2 operations\nto different addresses. Coherence is all about\noperations on the same address,",
    "start": "3482520",
    "end": "3490720"
  },
  {
    "text": "so they are orthogonal\nconcepts completely. So segmented scan, and a little\nbit of just CUDA mapping.",
    "start": "3490720",
    "end": "3497800"
  },
  {
    "text": "Yes. Just to add on to [INAUDIBLE]\nyou mentioned-- even for relaxed\nconsistency, we followed",
    "start": "3497800",
    "end": "3503340"
  },
  {
    "text": "the order of instructions. That's not always true. You can reorder reads\nand write, for example?",
    "start": "3503340",
    "end": "3508400"
  },
  {
    "text": "Right? Let me be very\nprecise about this. If we have one thread of\ncontrol, your compiler,",
    "start": "3508400",
    "end": "3517960"
  },
  {
    "text": "your hardware is broken\nif you get results that are in the opposite\norder or that are not",
    "start": "3517960",
    "end": "3525450"
  },
  {
    "text": "consistent with program\norder, so nothing about relaxed consistency\ncan be visible",
    "start": "3525450",
    "end": "3531720"
  },
  {
    "text": "if you only have one thread. In the same way that\nsuperscalar execution is broken, if a processor\ndecides to execute instructions",
    "start": "3531720",
    "end": "3540690"
  },
  {
    "text": "out of order or in parallel\nand you get different results. So you're saying commits\nare in order either way in a single thread?",
    "start": "3540690",
    "end": "3546720"
  },
  {
    "text": "In a single thread. Relaxed consistency is\nsaying, if T0 believes",
    "start": "3546720",
    "end": "3552360"
  },
  {
    "text": "it sees right to X,\nand then right to Y.",
    "start": "3552360",
    "end": "3559770"
  },
  {
    "text": "If you read a value\nin here, you would see the X done and\nthe Y not done,",
    "start": "3559770",
    "end": "3564839"
  },
  {
    "text": "and if you read a value here,\nyou would see both of them done. What relaxed consistency says is\nthat if T1 ever reads X and Y,",
    "start": "3564840",
    "end": "3574340"
  },
  {
    "text": "it can observe if you are\nrelaxing right ordering, for example, T1\nmight see the value",
    "start": "3574340",
    "end": "3581390"
  },
  {
    "text": "of y being updated\nbefore it sees the value of X being updated.",
    "start": "3581390",
    "end": "3586910"
  },
  {
    "text": "And I want to be really clear\nthat this is only something that when we're talking\nabout the observation",
    "start": "3586910",
    "end": "3593630"
  },
  {
    "text": "of another processors writes\nbecause if we're talking about my own writes, and I\ncompile a freaking program,",
    "start": "3593630",
    "end": "3600980"
  },
  {
    "text": "and it gives me an answer that's\nnot consistent with C semantics, I throw out the computer\nand the compiler and say,",
    "start": "3600980",
    "end": "3606300"
  },
  {
    "text": "this is all broken. Exactly, so I want to be\nvery clear about that. And keep in mind\nthat within a thread,",
    "start": "3606300",
    "end": "3613070"
  },
  {
    "text": "on the first day of class I\ntold you that instructions were reordered. Superscalar does that.",
    "start": "3613070",
    "end": "3618510"
  },
  {
    "text": "A bunch of reasons done that,\nbut you never ever observe that they are ordered,\nso it's still completely",
    "start": "3618510",
    "end": "3626420"
  },
  {
    "text": "in program order.  So just going on\ntop of his question,",
    "start": "3626420",
    "end": "3634380"
  },
  {
    "text": "I would like to see\nif we can go over just all the possible\nparallel [INAUDIBLE]",
    "start": "3634380",
    "end": "3642482"
  },
  {
    "text": "that we've covered\nin the course so far. [LAUGHS] You just increased\nscope in the last 12 minutes here, considerably.",
    "start": "3642482",
    "end": "3649337"
  },
  {
    "text": "[INAUDIBLE] So-- what? What are you asking? For example, what's the\nright way [INAUDIBLE]?",
    "start": "3649337",
    "end": "3659585"
  },
  {
    "text": "I'll put it this way. It's extremely\nhard to even create",
    "start": "3659585",
    "end": "3664619"
  },
  {
    "text": "questions that do much more\nthan right-right reordering.",
    "start": "3664620",
    "end": "3670450"
  },
  {
    "text": "So they're extremely hard. So I think right-right\nreordering is a big one.",
    "start": "3670450",
    "end": "3679307"
  },
  {
    "text": "Not even right-right reordering. Just think about--\nif you have a right, think about the implications\nof moving reads or other rights",
    "start": "3679307",
    "end": "3687450"
  },
  {
    "text": "above or below it. I think that's a good guidance.",
    "start": "3687450",
    "end": "3693040"
  },
  {
    "text": "If we tell you-- or it can be\neasy to just say everything is relaxed, just no\nguarantees on anything",
    "start": "3693040",
    "end": "3698740"
  },
  {
    "text": "other than program order, yeah. But the easiest\nquestions to write are definitely\nright-right reordering.",
    "start": "3698740",
    "end": "3704640"
  },
  {
    "text": " OK, so-- I see.",
    "start": "3704640",
    "end": "3712932"
  },
  {
    "text": "This is the Data\nParallel lecture. So there was a quick\ncomment on segmented scan.",
    "start": "3712933",
    "end": "3718330"
  },
  {
    "text": "OK. All I want you to\non segmented scan-- and you have it on the\nslide-- is the definition.",
    "start": "3718330",
    "end": "3724809"
  },
  {
    "text": "So segmented scan\nis just a scan. Imagine that we sequentially--",
    "start": "3724810",
    "end": "3731290"
  },
  {
    "text": "we had a list of lists, or if\nI wanted to be more precise, a sequence of sequences. So here's a sequence\nof two sequences.",
    "start": "3731290",
    "end": "3738560"
  },
  {
    "text": "The first sequence\nis three elements, and the second sequence\nis five elements, right?",
    "start": "3738560",
    "end": "3746290"
  },
  {
    "text": "Imagine we did a scan\non the first sequence. What would we get?",
    "start": "3746290",
    "end": "3751770"
  },
  {
    "text": " Let's say it's a sum. Let's say an exclusive sum.",
    "start": "3751770",
    "end": "3758120"
  },
  {
    "text": "So if it's an exclusive sum,\nwe would get 0, 1 plus 2 is 3. 3 plus 3 is 6.",
    "start": "3758120",
    "end": "3765710"
  },
  {
    "text": "So we know what a scan is. Now imagine that now I just\ndo a scan on the next thing.",
    "start": "3765710",
    "end": "3771800"
  },
  {
    "text": "And I get again 0,\n4, 8, 15, 22, 30.",
    "start": "3771800",
    "end": "3780290"
  },
  {
    "text": "So it's just two scans. So if you know scan, you\nknow what the right answer should be for segmented scan.",
    "start": "3780290",
    "end": "3788480"
  },
  {
    "text": "Now the segmented\nand scan algorithm that I gave you on\nthe page is a way",
    "start": "3788480",
    "end": "3793670"
  },
  {
    "text": "to compute those two scans\nwith O of n parallelism, where n is the total length\nof all the subsequences.",
    "start": "3793670",
    "end": "3801470"
  },
  {
    "text": "And that algorithm looks\na little bit like this. I visually described\nit like this. I put this in the\nlecture because I",
    "start": "3801470",
    "end": "3806930"
  },
  {
    "text": "thought you might find it cool. What I want you\nto be able to do, however, is, yes, if I gave you\nthe definition, if I gave you",
    "start": "3806930",
    "end": "3814880"
  },
  {
    "text": "the bottom half of\nthis slide, and I asked you to use segmented scan in\nsome higher-level algorithm",
    "start": "3814880",
    "end": "3820550"
  },
  {
    "text": "development, that is fair game. But I won't be like, hey, you\nneed to implement data parallel scan.",
    "start": "3820550",
    "end": "3826820"
  },
  {
    "text": "Or if I ask you to\nimplement it, it's going to be some very\nguided thing where",
    "start": "3826820",
    "end": "3832579"
  },
  {
    "text": "the real principle is something\nelse and not the segmented scan. OK, so that was\nthat and then CUDA.",
    "start": "3832580",
    "end": "3838110"
  },
  {
    "text": "So let's try and let's see\nwhat we can do with CUDA. So I hope you all remember\nfrom Assignment Three,",
    "start": "3838110",
    "end": "3845630"
  },
  {
    "text": "that when you-- and really from\nyour Assignment Two. So in Assignment Two, we\nhad these bulk tasks, right?",
    "start": "3845630",
    "end": "3853299"
  },
  {
    "text": "So in some sense, you\nhad a task system, and the application\nsent you a command.",
    "start": "3853300",
    "end": "3859020"
  },
  {
    "text": "And the command was run\nthis task n times, right?",
    "start": "3859020",
    "end": "3865240"
  },
  {
    "text": "And then it can send you\nmultiple commands which said, OK, here's a new task that\nyou need to run n times, but you can't do any\nof these until you're",
    "start": "3865240",
    "end": "3871920"
  },
  {
    "text": "done with all these\nprevious book launches. So CUDA is a bulk launch system,\nmuch like your tasking API.",
    "start": "3871920",
    "end": "3880200"
  },
  {
    "text": "Instead of the number\nn tasks, it just has two numbers in there. It says, I want you to\nrun n thread blocks,",
    "start": "3880200",
    "end": "3887280"
  },
  {
    "text": "and each thread block\nhas T threads in it. So it's the same\nbulk launch concept",
    "start": "3887280",
    "end": "3893640"
  },
  {
    "text": "where you say, here's the\nnumber of thread blocks-- and I think thread block\nand your task or an ISP task",
    "start": "3893640",
    "end": "3900330"
  },
  {
    "text": "have a bit of correspondence. And inside that\ntask, you're going",
    "start": "3900330",
    "end": "3905460"
  },
  {
    "text": "to do parallel work\nusing T threads. So the fact that you can do this\nin 2D versus 3D versus 1D is not",
    "start": "3905460",
    "end": "3912980"
  },
  {
    "text": "significant, right? It's just, create this\nmany thread blocks, and every thread block\nneeds this many resources.",
    "start": "3912980",
    "end": "3919950"
  },
  {
    "text": "It needs this much\nshared memory, and it needs this\nmany threads, OK?",
    "start": "3919950",
    "end": "3925380"
  },
  {
    "text": "So in this setup, the details of\nthe program are not important, but we wrote a\nCUDA program that,",
    "start": "3925380",
    "end": "3932630"
  },
  {
    "text": "if I remember correctly, yeah,\nhad 128 threads per block and needed--",
    "start": "3932630",
    "end": "3937813"
  },
  {
    "text": " I can't remember. It needed a little bit over--",
    "start": "3937813",
    "end": "3944630"
  },
  {
    "text": "I think it needed five-- it needed a little bit\nover 512 bytes of storage.",
    "start": "3944630",
    "end": "3950400"
  },
  {
    "text": "So in other words, if we have\non every core of the GPU, every SM of the GPU--",
    "start": "3950400",
    "end": "3956310"
  },
  {
    "text": "if we have execution\ncontexts for 384 threads, how many thread\nblocks can we fit?",
    "start": "3956310",
    "end": "3962335"
  },
  {
    "start": "3962335",
    "end": "3969101"
  },
  {
    "text": "What? 148. Just do the math. We fit two or three. I can't remember. We can fit-- we can\nfit three, yeah.",
    "start": "3969101",
    "end": "3976900"
  },
  {
    "text": "We can fit three thread\nblocks worth of threads. And if those thread\nblocks needed a little bit over one third of\nthis shared memory,",
    "start": "3976900",
    "end": "3984050"
  },
  {
    "text": "how many thread blocks can I\nfit from a storage perspective? Just two, right? So in this case, the\nnumber of thread blocks",
    "start": "3984050",
    "end": "3991180"
  },
  {
    "text": "that we can get onto this\nprocessor at one time is actually limited\nby our shared memory. But for example, if it uses zero\nshared memory, I can fit three.",
    "start": "3991180",
    "end": "3998175"
  },
  {
    "text": " So I say that there's an\nexecution context for 384 CUDA",
    "start": "3998175",
    "end": "4006180"
  },
  {
    "text": "threads here. And so the NVIDIA scheduler,\ntheir version of your Assignment Two is going to say, all right,\nhere's some here's some work.",
    "start": "4006180",
    "end": "4014610"
  },
  {
    "text": "I want you to run 1,000 thread\nblocks worth of this program. And at this point, it\njust starts handing it",
    "start": "4014610",
    "end": "4020970"
  },
  {
    "text": "to its worker cores, right? Or really, its worker\nexecution context on its cores.",
    "start": "4020970",
    "end": "4028000"
  },
  {
    "text": "And so step one is\nto say, well, look, there's nothing\nallocated right now. Let's get the first thread\nblock running on core zero,",
    "start": "4028000",
    "end": "4035020"
  },
  {
    "text": "and we're going to give it\n128 threads worth of execution contexts, and we're going\nto give it a little bit",
    "start": "4035020",
    "end": "4042430"
  },
  {
    "text": "over a third, 520\nbytes of storage. And there's still a lot\nmore worker capability",
    "start": "4042430",
    "end": "4048160"
  },
  {
    "text": "in this machine, so we're\ngoing to assign the next thread block over here on core one. Now a question, a good\nexam question, would be,",
    "start": "4048160",
    "end": "4056590"
  },
  {
    "text": "why did this scheduler, which\nwas an implementation detail-- but why is it a\nfairly sane decision",
    "start": "4056590",
    "end": "4062619"
  },
  {
    "text": "to put block zero on core zero\nand block one on core one? ",
    "start": "4062620",
    "end": "4070170"
  },
  {
    "text": "[INAUDIBLE] instead of-- If I would have put block\nzero and block one both here,",
    "start": "4070170",
    "end": "4076329"
  },
  {
    "text": "what is the performance\nimplication of that? So remember, these\nare 128 threads. These are 128 threads.",
    "start": "4076330",
    "end": "4082420"
  },
  {
    "text": "We have one core, and\nthat core, in this case, had a 32-wide capability.",
    "start": "4082420",
    "end": "4089160"
  },
  {
    "text": "What's the problem with loading\nthis up with a bunch of threads and leaving the other core idle?",
    "start": "4089160",
    "end": "4094170"
  },
  {
    "text": "Yeah? It does make it inherently\nsequential within that one core, instead of utilizing-- Because remember,\nthese threads are",
    "start": "4094170",
    "end": "4100649"
  },
  {
    "text": "interleaved onto these\nprocessing resources. So I'm having everybody\nthat I'm creating telling to do work share\nthe same resources, while I",
    "start": "4100649",
    "end": "4107759"
  },
  {
    "text": "have resources over there that\nare idle, exactly, exactly. OK? So I have more room.",
    "start": "4107760",
    "end": "4113469"
  },
  {
    "text": "I can fill up this chip. And this chip can simultaneously\nsupport four concurrent thread blocks.",
    "start": "4113470",
    "end": "4121024"
  },
  {
    "text": "OK, and I cannot put further\nthread blocks on here because I'm out of resources. So when a thread\ncompletes-- or sorry,",
    "start": "4121024",
    "end": "4128469"
  },
  {
    "text": "when a thread block completes,\njust like in your assignment, when a task completes,\nthat opens up",
    "start": "4128470",
    "end": "4133689"
  },
  {
    "text": "the ability to put another\ntask on the machine. So let's say we pull,\nlet's say, the first block",
    "start": "4133689",
    "end": "4139000"
  },
  {
    "text": "completed first,\nwhich makes sense, and the scheduler\ncan fill that gap.",
    "start": "4139000",
    "end": "4144100"
  },
  {
    "text": "Notice that the fact\nthat all thread blocks have the same\nnumber of resources and the same allocations is\nuseful for filling this gap,",
    "start": "4144100",
    "end": "4150799"
  },
  {
    "text": "otherwise you have an\nallocation problem, OK? And we just continue with that,\njust putting stuff on, putting",
    "start": "4150800",
    "end": "4156160"
  },
  {
    "text": "stuff on until we're done, OK? So that's the thread block\nscheduling part of CUDA.",
    "start": "4156160",
    "end": "4161899"
  },
  {
    "text": "Is there any question about the\nwarp scheduling part of CUDA? Or are you good there?",
    "start": "4161899",
    "end": "4168489"
  },
  {
    "text": "So my original question was,\nwhy is it on a single core--",
    "start": "4168490",
    "end": "4173950"
  },
  {
    "text": "at a single clock, how many\nwarps can we actually [? put? ?] OK, so I'm going to--",
    "start": "4173950",
    "end": "4182059"
  },
  {
    "text": "I want to preface this\nbecause this is recorded, and I don't want to\nbe doxed because I'm",
    "start": "4182060",
    "end": "4190489"
  },
  {
    "text": "saying the wrong thing\nabout modern NVIDIA GPUs. But for most NVIDIA\nGPUs, it's perfectly fine",
    "start": "4190490",
    "end": "4196940"
  },
  {
    "text": "to think about it in\nthe following way, that the chip has\na bank of ALUs. Now, this is not an NVIDIA GPU.",
    "start": "4196940",
    "end": "4203310"
  },
  {
    "text": "I made this up. My proxy NVIDIA GPU\nhere has 3270 ALUs.",
    "start": "4203310",
    "end": "4209750"
  },
  {
    "text": "And the way it runs\nCUDA threads is it takes 32 CUDA threads,\nwhich is called one warp,",
    "start": "4209750",
    "end": "4218030"
  },
  {
    "text": "and it runs one instruction for\neach of those 32 CUDA threads",
    "start": "4218030",
    "end": "4223550"
  },
  {
    "text": "on this ALU and makes\nprogress on the warp. And on the next clock, it\nmight choose a different warp.",
    "start": "4223550",
    "end": "4229100"
  },
  {
    "text": "So you can think about a warp's\nworth of execution as 32 CUDA",
    "start": "4229100",
    "end": "4234440"
  },
  {
    "text": "threads all doing the same\nthing at the same time and concurrently\nrunning on the chip, OK?",
    "start": "4234440",
    "end": "4240470"
  },
  {
    "text": "Does it always\nchoose the threads from the warp in the\nsame thread block?",
    "start": "4240470",
    "end": "4247247"
  },
  {
    "text": "I'm going to answer. The question was, does\nthe NVIDIA GPU always choose threads from the warp\nin the same thread block?",
    "start": "4247247",
    "end": "4254950"
  },
  {
    "text": "Officially, NVIDIA says nothing\nabout their implementation, so I cannot definitively tell\nyou do they do that or not.",
    "start": "4254950",
    "end": "4261950"
  },
  {
    "text": "Historically, the answer\nabout their implementation has been absolutely yes. The threads that are used\nto run at the same time",
    "start": "4261950",
    "end": "4270309"
  },
  {
    "text": "are thread IDs 0, 1, 2,\n3, all the way to thread ID mod 32, zero through 31.",
    "start": "4270310",
    "end": "4278469"
  },
  {
    "text": "Now, this might be an elite\nway to finish up here. But in the last minute, let's\nthink about it this way.",
    "start": "4278470",
    "end": "4285190"
  },
  {
    "text": "NVIDIA has full\nflexibility by writing-- since you wrote\nthings as threads,",
    "start": "4285190",
    "end": "4291590"
  },
  {
    "text": "if they want to change the SIMD\nwidth and cut the number of ALUs by half, you would never\nknow because you just",
    "start": "4291590",
    "end": "4298690"
  },
  {
    "text": "created 128 threads. You didn't think about warps. You didn't think about\nany of that stuff. So when you compile\nyour thread block code,",
    "start": "4298690",
    "end": "4305730"
  },
  {
    "text": "it's just a regular\nsequential scalar thread, and it happens to run as many\nas you ask for concurrently.",
    "start": "4305730",
    "end": "4313099"
  },
  {
    "text": "That's different from\nthe implementation detail of SIMD\ninstructions on a machine.",
    "start": "4313100",
    "end": "4319100"
  },
  {
    "text": "If we went next year, and\nIntel produced a processor with 64-wide SIMD you\nwould go back to ICPC,",
    "start": "4319100",
    "end": "4326659"
  },
  {
    "text": "you would change\nthe gang size to 64, you would recompile your code,\nand it would produce one thread",
    "start": "4326660",
    "end": "4332780"
  },
  {
    "text": "with 64-wide vector\ninstructions. That's the one difference\nbetween GPU SIMD and CPU SIMD.",
    "start": "4332780",
    "end": "4339440"
  },
  {
    "text": "In CPU SIMD, it's the\nresponsibility of the compiler to generate instructions that\nare exposed by the machine",
    "start": "4339440",
    "end": "4345620"
  },
  {
    "text": "architecture. In GPU assembly, the compiler\ndoesn't do nearly as much. It just generates\nsequential things",
    "start": "4345620",
    "end": "4352190"
  },
  {
    "text": "and says, like in your\nprogramming assignment, we're going to do\nthousands of them at once in group and block\nsizes of one, in this case, 128.",
    "start": "4352190",
    "end": "4360210"
  },
  {
    "text": "It's up to NVIDIA to decide\nhow it wants to run that. Historically, it's taken all\nyour threads in a thread block,",
    "start": "4360210",
    "end": "4365900"
  },
  {
    "text": "chopped them into groups of\nconsecutive 32 addresses, and run those in parallel.",
    "start": "4365900",
    "end": "4370980"
  },
  {
    "text": "These days, they actually\ndo have the ability to rearrange things\nfor you in order",
    "start": "4370980",
    "end": "4376070"
  },
  {
    "text": "to try and discover better SIMD\ncoherence if you have divergence and If statements. The extent to which they\ncan successfully do that",
    "start": "4376070",
    "end": "4383720"
  },
  {
    "text": "is complete trade\nsecret and not something that's well documented.",
    "start": "4383720",
    "end": "4388840"
  },
  {
    "start": "4388840",
    "end": "4392000"
  }
]