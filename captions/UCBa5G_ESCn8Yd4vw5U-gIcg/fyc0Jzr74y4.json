[
  {
    "start": "0",
    "end": "5940"
  },
  {
    "text": "OK, let me get\nstarted for today. So for today, first\nof all, I'm going",
    "start": "5940",
    "end": "13349"
  },
  {
    "text": "to spend a few minutes\ntalking about a couple more neural net\nconcepts, including",
    "start": "13350",
    "end": "19290"
  },
  {
    "text": "actually a couple\nof the concepts that turn up in assignment two.",
    "start": "19290",
    "end": "25230"
  },
  {
    "text": "Then the bulk of\ntoday is then going to be moving on to introducing\nwhat are language models.",
    "start": "25230",
    "end": "34080"
  },
  {
    "text": "And then after introducing\nlanguage models, we're going to introduce a new\nkind of neural network, which",
    "start": "34080",
    "end": "42300"
  },
  {
    "text": "is one way to build\nlanguage models, which is recurrent neural networks.",
    "start": "42300",
    "end": "47550"
  },
  {
    "text": "They're important\nthing to know about, and we use them in\nassignment three, but they're certainly\nnot the only way",
    "start": "47550",
    "end": "54510"
  },
  {
    "text": "to build language models. In fact, probably a\nlot of you already know that there's this other\nkind of neural network called",
    "start": "54510",
    "end": "60420"
  },
  {
    "text": "transformers, and\nwe'll get on to those after we've done\nrecurrent neural nets.",
    "start": "60420",
    "end": "66520"
  },
  {
    "text": "Talk a bit about problems\nwith recurrent neural networks and well, if I have time,\nI'll get on to the recap.",
    "start": "66520",
    "end": "73690"
  },
  {
    "text": "Before getting into the\ncontent of the class, I thought I could just\nspend a minute on giving you the stats of who is in CS224N.",
    "start": "73690",
    "end": "83020"
  },
  {
    "text": "Who's in CS224N\nlooks like the pie charts they show in CS106A these\ndays, except more grad students,",
    "start": "83020",
    "end": "92119"
  },
  {
    "text": "I guess. So the four big groups are the\ncomputer science undergrads,",
    "start": "92120",
    "end": "97340"
  },
  {
    "text": "the computer science grads,\nthe undeclared undergraduates and the NDO grads.",
    "start": "97340",
    "end": "104600"
  },
  {
    "text": "So this is a large portion\nof the SCPD students, though. Some of them are in the\ncomputer science grads.",
    "start": "104600",
    "end": "110810"
  },
  {
    "text": "So that makes up about\n60% of the audience. And if you're not in one\nof those four big groups,",
    "start": "110810",
    "end": "118250"
  },
  {
    "text": "you're in the other 40%\nand everybody is somewhere. So there are lots of other\ninteresting groups down here.",
    "start": "118250",
    "end": "124760"
  },
  {
    "text": "So the bright orange\ndown here, that's where the math and physics\nPhDs are, Up here, I",
    "start": "124760",
    "end": "133140"
  },
  {
    "text": "mean, interestingly, we\nnow have more statistics grad students than there\nare census undergrads.",
    "start": "133140",
    "end": "139420"
  },
  {
    "text": "It didn't used to be that\nway around in NLP classes. And one of my favorite groups,\nthe little magenta group",
    "start": "139420",
    "end": "148990"
  },
  {
    "text": "down here, these are the\nhumanities undergrads. Yeah, humanities undergrads.",
    "start": "148990",
    "end": "156610"
  },
  {
    "text": "In terms of years, it\nbreaks down like this. First year grad students\nare the biggest group.",
    "start": "156610",
    "end": "162980"
  },
  {
    "text": "Tons of juniors and seniors\nand a couple of brave frosh. Any brave frosh here today?",
    "start": "162980",
    "end": "169705"
  },
  {
    "text": "Yeah.  OK.",
    "start": "169705",
    "end": "175390"
  },
  {
    "text": "Welcome. Yeah. So modern neural networks,\nespecially language models,",
    "start": "175390",
    "end": "183830"
  },
  {
    "text": "are enormous. This chart's sort of out of date\nbecause it only goes up to 2022,",
    "start": "183830",
    "end": "191690"
  },
  {
    "text": "but it's sort of actually\nhard to make an accurate chart for 2024 because in\nthe last couple of years,",
    "start": "191690",
    "end": "198530"
  },
  {
    "text": "the biggest language\nmodel makers have in general stopped saying\nhow large their language models are in terms of parameters.",
    "start": "198530",
    "end": "204710"
  },
  {
    "text": "But at any rate, they're\nclearly huge models, which have over 100\nbillion parameters.",
    "start": "204710",
    "end": "214239"
  },
  {
    "text": "And so large and then deep\nin terms of very many layers",
    "start": "214240",
    "end": "219860"
  },
  {
    "text": "neural nets are a cornerstone\nof modern NLP systems. We're going to be pretty\nquickly working our way up",
    "start": "219860",
    "end": "228400"
  },
  {
    "text": "to look at those\nkind of deep models. But for starting off\nwith something simpler,",
    "start": "228400",
    "end": "235780"
  },
  {
    "text": "I did just want to key\nyou in for a few minutes into a little bit of history.",
    "start": "235780",
    "end": "243939"
  },
  {
    "text": "So the last time neural nets\nwere popular was in the 80s",
    "start": "243940",
    "end": "249250"
  },
  {
    "text": "and 90s, and that was\nwhen people worked out the backpropagation algorithm. Jeff Hinton and colleagues\nmade famous the backpropagation",
    "start": "249250",
    "end": "257648"
  },
  {
    "text": "algorithm that we've looked at. And that allowed the training of\nneural nets with hidden layers.",
    "start": "257649",
    "end": "265150"
  },
  {
    "text": "And so, but in those days,\npretty much all the neural nets with hidden layers\nthat were trained",
    "start": "265150",
    "end": "271810"
  },
  {
    "text": "were trained with\none hidden layer. You had the input, the\nhidden layer and the output,",
    "start": "271810",
    "end": "277130"
  },
  {
    "text": "and that's all that there was. And the reason for that was\nfor a very, very long time,",
    "start": "277130",
    "end": "285979"
  },
  {
    "text": "people couldn't really\nget things to work with more hidden layers.",
    "start": "285980",
    "end": "291020"
  },
  {
    "text": "So that only started to change\nin the resurgence of what's often got called deep learning,\nbut anyway, back to neural nets,",
    "start": "291020",
    "end": "300010"
  },
  {
    "text": "started around 2006. And this was one of the\ninfluential papers at the time.",
    "start": "300010",
    "end": "307250"
  },
  {
    "text": "Greedy Layer-Wise Training\nof Deep Neural Networks by Yoshua, Bengio\nand colleagues. And so right at the\nbeginning of that paper,",
    "start": "307250",
    "end": "314580"
  },
  {
    "text": "they observe the problem. \"However, until\nrecently, it was believed",
    "start": "314580",
    "end": "320180"
  },
  {
    "text": "too difficult to train deep\nmulti-layer neural networks. Empirically, deep networks\nwere generally found to be not",
    "start": "320180",
    "end": "327260"
  },
  {
    "text": "better, and often worse, than\nneural networks with one or two hidden layers.\"",
    "start": "327260",
    "end": "332690"
  },
  {
    "text": "Jerry Tesauro was\nactually a faculty member who worked very\nearly on autonomous driving with neural networks.",
    "start": "332690",
    "end": "339860"
  },
  {
    "text": "\"As this is a negative result,\nit has not been much reported in the machine\nlearning literature.\"",
    "start": "339860",
    "end": "346730"
  },
  {
    "text": "So although people had neural\nnetworks and back propagation",
    "start": "346730",
    "end": "352460"
  },
  {
    "text": "and recurrent neural networks--\nwe're going to talk about today-- that for a very\nlong period of time,",
    "start": "352460",
    "end": "360259"
  },
  {
    "text": "15 years or so, things\nseemed completely stuck, in that you couldn't--\nalthough in theory,",
    "start": "360260",
    "end": "367290"
  },
  {
    "text": "it seemed like deep neural\nnetworks should be promising. In practice, they didn't work.",
    "start": "367290",
    "end": "373720"
  },
  {
    "text": "And so it really then\ntook some new developments that happened in the late 2000's\ndecade and then more profoundly,",
    "start": "373720",
    "end": "381610"
  },
  {
    "text": "in the 2010's decade\nto actually figure out how we could have deep neural\nnetworks that actually worked,",
    "start": "381610",
    "end": "390580"
  },
  {
    "text": "working far better than\nthe shallow neural networks and leading into the\nnetworks that we have today.",
    "start": "390580",
    "end": "397449"
  },
  {
    "text": "And, we're going to be\nstarting to talk about some of those things in this class\nand coming up with classes.",
    "start": "397450",
    "end": "407289"
  },
  {
    "text": "And I mean, I think\nthe tendency when you see the things that got\nneural networks to work much",
    "start": "407290",
    "end": "414990"
  },
  {
    "text": "better, the natural reaction\nis to shrug and be underwhelmed",
    "start": "414990",
    "end": "424020"
  },
  {
    "text": "and think is this\nall there is to it? This doesn't exactly seem\nlike difficult science.",
    "start": "424020",
    "end": "430380"
  },
  {
    "text": "And in some sense that's true. They're fairly\nlittle introductions",
    "start": "430380",
    "end": "436710"
  },
  {
    "text": "of new ideas and\ntweaks of things. But nevertheless, a handful\nof little ideas and tweaks",
    "start": "436710",
    "end": "444390"
  },
  {
    "text": "of things turn things\naround from a field that was stuck for 15\nyears going nowhere",
    "start": "444390",
    "end": "452039"
  },
  {
    "text": "and which nearly everyone\nhad abandoned because of that to suddenly turning around\nand there being the ability",
    "start": "452040",
    "end": "459450"
  },
  {
    "text": "to train these deeper neural\nnetworks, which then behaved amazingly better as\nmachine learning systems",
    "start": "459450",
    "end": "466860"
  },
  {
    "text": "than other things\nthat had preceded them and dominated for\nthe intervening time.",
    "start": "466860",
    "end": "472930"
  },
  {
    "text": "So that took a lot of time. So what are these things? One of them, which you\ncan greet with a bit",
    "start": "472930",
    "end": "480660"
  },
  {
    "text": "of a yawn in some sense, is\ndoing better regularization of neural nets.",
    "start": "480660",
    "end": "487030"
  },
  {
    "text": "So regularization is the\nidea that beyond just having",
    "start": "487030",
    "end": "493380"
  },
  {
    "text": "a loss that we want to minimize\nin terms of describing the data,",
    "start": "493380",
    "end": "498690"
  },
  {
    "text": "we want to, in some other\nways, manipulate what parameters we learn so that\nour models work better.",
    "start": "498690",
    "end": "506740"
  },
  {
    "text": "And so normally we have some\nmore complex loss function",
    "start": "506740",
    "end": "512820"
  },
  {
    "text": "that does some regularization. The most common\nway of doing this is what's called L2 loss,\nwhere you add on this parameter",
    "start": "512820",
    "end": "522149"
  },
  {
    "text": "squared term at the end. And this regularization\nsays, it would",
    "start": "522150",
    "end": "529019"
  },
  {
    "text": "be good to find a model with\nsmall parameter weights.",
    "start": "529020",
    "end": "534790"
  },
  {
    "text": "So you should be finding the\nsmallest parameter weights that will explain your data well.",
    "start": "534790",
    "end": "540220"
  },
  {
    "text": "And there's a lot you can\nsay about regularization,",
    "start": "540220",
    "end": "545529"
  },
  {
    "text": "these losses. They get talked about a\nlot more in other classes",
    "start": "545530",
    "end": "550650"
  },
  {
    "text": "like CS229, machine learning. And so I'm not going to\nsay very much about it.",
    "start": "550650",
    "end": "556720"
  },
  {
    "text": "This is a machine\nlearning theory class. But I do just want to\nput in one note that's",
    "start": "556720",
    "end": "564870"
  },
  {
    "text": "very relevant to what's happened\nin recent neural networks work.",
    "start": "564870",
    "end": "573029"
  },
  {
    "text": "So the classic view\nof regularization was we needed this\nkind of regularization",
    "start": "573030",
    "end": "578699"
  },
  {
    "text": "to prevent our networks\nfrom overfitting, meaning that they would do\na very good job at modeling",
    "start": "578700",
    "end": "586950"
  },
  {
    "text": "the training data, but then\nthey would generalize badly to new data that was shown.",
    "start": "586950",
    "end": "593830"
  },
  {
    "text": "And so the picture\nthat you got shown was this, that as you train\non some training data,",
    "start": "593830",
    "end": "600820"
  },
  {
    "text": "your error\nnecessarily goes down. However, after some\npoint, you start",
    "start": "600820",
    "end": "608440"
  },
  {
    "text": "learning specific\nproperties of things that happen to turn up in\nthose training examples",
    "start": "608440",
    "end": "614800"
  },
  {
    "text": "and that you're learning\nthings that are only good for the training examples. And so they won't generalize\nwell to different pieces of data",
    "start": "614800",
    "end": "623520"
  },
  {
    "text": "you see at test time. So if you have a separate\nvalidation set or a final test",
    "start": "623520",
    "end": "629500"
  },
  {
    "text": "set and you traced out the\nerror or loss on that validation",
    "start": "629500",
    "end": "636790"
  },
  {
    "text": "or test set, that\nafter some point it would start to go up again. This is a quirk in\nmy bad PowerPoint.",
    "start": "636790",
    "end": "644270"
  },
  {
    "text": "It's just meant to go up. And the fact that\nit goes up is then you have overfit to\nyour training data.",
    "start": "644270",
    "end": "651670"
  },
  {
    "text": "And making the parameters\nnumerically small is meant to lessen the\nextent to which you",
    "start": "651670",
    "end": "657370"
  },
  {
    "text": "overfit on your training data. This is not a picture\nthat modern neural network",
    "start": "657370",
    "end": "665570"
  },
  {
    "text": "people believe at all. Instead, the picture\nis changed like this.",
    "start": "665570",
    "end": "672260"
  },
  {
    "text": "We don't believe that\noverfitting exists anymore.",
    "start": "672260",
    "end": "677310"
  },
  {
    "text": "But what we are\nconcerned about is models that will generalize\nwell to different data",
    "start": "677310",
    "end": "685939"
  },
  {
    "text": "so that when we train-- so in classical\nstatistics, the idea",
    "start": "685940",
    "end": "692690"
  },
  {
    "text": "that you could train\nbillions of parameters like large neural nets now have\nwould be seen as ridiculous",
    "start": "692690",
    "end": "700910"
  },
  {
    "text": "because you could not possibly\nestimate those parameters well. And so you just have\nall of this noisy mess.",
    "start": "700910",
    "end": "708920"
  },
  {
    "text": "But what's actually been\nfound is that yeah, it's true. You can't estimate the\nnumbers well, but what you get",
    "start": "708920",
    "end": "716090"
  },
  {
    "text": "is an interesting\naveraging function from all these myriad numbers.",
    "start": "716090",
    "end": "721230"
  },
  {
    "text": "And if you do it\nright, what happens is as you go on training\nthat for a while,",
    "start": "721230",
    "end": "729930"
  },
  {
    "text": "it might look like you're\nstarting to overfit. But if you keep on training in\na huge network, not only will",
    "start": "729930",
    "end": "736730"
  },
  {
    "text": "your training loss continue to\ngo down very infinitesimally, but your validation loss\nwill go down as well.",
    "start": "736730",
    "end": "745050"
  },
  {
    "text": "And so on huge\nnetworks these days,",
    "start": "745050",
    "end": "750620"
  },
  {
    "text": "we train our models so that they\noverfit to the training data",
    "start": "750620",
    "end": "756050"
  },
  {
    "text": "almost completely. So that if you train a huge\nnetwork now on a training set,",
    "start": "756050",
    "end": "762060"
  },
  {
    "text": "you can essentially\ntrain them to get 0 loss. Maybe it's 0.000007\nloss or something,",
    "start": "762060",
    "end": "770680"
  },
  {
    "text": "but you can train them to get\n0 loss because you've got such rich models. You can perfectly fit, memorize\nthe entire training set.",
    "start": "770680",
    "end": "779160"
  },
  {
    "text": "Now, classically, that would\nhave been seen as a disaster because you've overfit\nthe training data. With modern, large\nneural networks,",
    "start": "779160",
    "end": "785700"
  },
  {
    "text": "it's not seen as a\ndisaster because providing you've done regularization\nwell, that your model will also",
    "start": "785700",
    "end": "793100"
  },
  {
    "text": "generalize well\nto different data. However, the flip side\nof that is normally",
    "start": "793100",
    "end": "800120"
  },
  {
    "text": "this kind of L2\nregularization or similar ones like L1 regularization aren't\nstrong enough regularization",
    "start": "800120",
    "end": "807410"
  },
  {
    "text": "to achieve that effect. And so neural\nnetwork people have turned to other methods of\nregularization, of which",
    "start": "807410",
    "end": "814820"
  },
  {
    "text": "everyone's favorite is dropout. So this is one of the things\nthat's on the assignment.",
    "start": "814820",
    "end": "820050"
  },
  {
    "text": "And at this point, I should. ",
    "start": "820050",
    "end": "826110"
  },
  {
    "text": "apologize or something because\nthe way drop out is done,",
    "start": "826110",
    "end": "832620"
  },
  {
    "text": "the way drop out\nis presented here is the original formulation. The way you drop\nout is presented",
    "start": "832620",
    "end": "837780"
  },
  {
    "text": "on the assignment is the\nway it's now normally done in deep learning packages. So there are a couple of\ndetails that vary a bit.",
    "start": "837780",
    "end": "846280"
  },
  {
    "text": "And let me just present the main\nidea here and not worry too much",
    "start": "846280",
    "end": "851610"
  },
  {
    "text": "about the details of the math. So the idea of dropout\nis at training time,",
    "start": "851610",
    "end": "857610"
  },
  {
    "text": "every time you are doing\na piece of training with an example, what\nyou're going to do",
    "start": "857610",
    "end": "864630"
  },
  {
    "text": "is inside the middle layers\nof the neural network, you're just going to throw\naway some of the inputs.",
    "start": "864630",
    "end": "871810"
  },
  {
    "text": "And so technically,\nthe way you do this is have a random mask\nthat you sample",
    "start": "871810",
    "end": "877830"
  },
  {
    "text": "each time of zeros and ones. You do a Hadamard product\nof that with the data.",
    "start": "877830",
    "end": "883060"
  },
  {
    "text": "So some of the\ndata items go to 0 and you have different\nmasks each time.",
    "start": "883060",
    "end": "889580"
  },
  {
    "text": "So for the next thing, I've\nnow masked out something",
    "start": "889580",
    "end": "895120"
  },
  {
    "text": "different this time. And so you're just sort\nof randomly throwing away",
    "start": "895120",
    "end": "900220"
  },
  {
    "text": "the inputs. And the effect of this is that\nyou're training the model,",
    "start": "900220",
    "end": "906050"
  },
  {
    "text": "that it has to be robust\nand work well and make as much use of every\ninput as it can.",
    "start": "906050",
    "end": "913850"
  },
  {
    "text": "It can't decide that it can be\nextremely reliant on components",
    "start": "913850",
    "end": "918940"
  },
  {
    "text": "17 of the vector because\nsometimes it's just going to randomly disappear. So if there are other features\nthat you could use instead",
    "start": "918940",
    "end": "926410"
  },
  {
    "text": "that would let you work\nout what to do next, you should also know how to\nmake use of those features.",
    "start": "926410",
    "end": "932570"
  },
  {
    "text": "So at training time you\nrandomly delete things. At test time for efficiency,\nbut also quality of the answer,",
    "start": "932570",
    "end": "941200"
  },
  {
    "text": "You don't delete anything. You keep all of your\nweights, but you just rescale things to\nmake up for the fact",
    "start": "941200",
    "end": "947630"
  },
  {
    "text": "that you used to\nbe dropping things. OK, so there are\nseveral ways that you",
    "start": "947630",
    "end": "953810"
  },
  {
    "text": "can think of explaining this. One motivation\nthat's often given is that this prevents\nfeature co-adaptation.",
    "start": "953810",
    "end": "962690"
  },
  {
    "text": "So rather than a model\nbeing able to learn complex functions of\nfeature seven, eight and 11",
    "start": "962690",
    "end": "970010"
  },
  {
    "text": "can help me predict this, it\nknows that some of the features might be missing. So it has to make use of\nthings in a more flexible way.",
    "start": "970010",
    "end": "979550"
  },
  {
    "text": "Another way of thinking\nof it is that there's been a lot of work\non model ensembles",
    "start": "979550",
    "end": "984800"
  },
  {
    "text": "where you can mix\ntogether different models and improve your results.",
    "start": "984800",
    "end": "990029"
  },
  {
    "text": "If you're training\nwith dropout, it's kind of like you're training\nwith a huge model ensemble",
    "start": "990030",
    "end": "995720"
  },
  {
    "text": "because you're training with\nthe ensemble of the power set, the exponential number\nof every possible drop out",
    "start": "995720",
    "end": "1002949"
  },
  {
    "text": "of features all at once. And that gives you\na very good model.",
    "start": "1002950",
    "end": "1008490"
  },
  {
    "text": "So there are different\nways of thinking about it. I mean, if you've seen Naive\nBayes and logistic regression",
    "start": "1008490",
    "end": "1014700"
  },
  {
    "text": "models before, I think a\nnice way to think of it is that it gives a middle\nground between the two",
    "start": "1014700",
    "end": "1022440"
  },
  {
    "text": "because for a\nNaive Bayes models, you're weighting each\nfeature independently just based on the data statistics.",
    "start": "1022440",
    "end": "1028569"
  },
  {
    "text": "It doesn't matter what\nother features are there. In a logistic\nregression, weights are set in the context of\nall the other features.",
    "start": "1028569",
    "end": "1035470"
  },
  {
    "text": "And with dropout, you're\nsomewhere in between. You're setting the weights\nand the context of some",
    "start": "1035470",
    "end": "1041160"
  },
  {
    "text": "of the other features,\nbut different ones will disappear at\ndifferent times. But following work that\nwas done at Stanford",
    "start": "1041160",
    "end": "1049830"
  },
  {
    "text": "by Stefan Varga and others,\ngenerally these days people regard dropout\nas a form of feature",
    "start": "1049830",
    "end": "1057540"
  },
  {
    "text": "dependent regularization. And he shows some\ntheoretical results as to why to think\nof it that way.",
    "start": "1057540",
    "end": "1064160"
  },
  {
    "text": "OK. I think we've implicitly\nseen this one, but vectorization is the\nidea no for loops always use",
    "start": "1064160",
    "end": "1075080"
  },
  {
    "text": "vectors, matrices and tensors. The entire success and\nspeed of deep learning",
    "start": "1075080",
    "end": "1082250"
  },
  {
    "text": "works from the fact that we can\ndo things with vectors, matrices and tensors.",
    "start": "1082250",
    "end": "1088200"
  },
  {
    "text": "So if you're writing for\nloops in any language, but especially in Python,\nthings run really slowly.",
    "start": "1088200",
    "end": "1095490"
  },
  {
    "text": "If you can do things with\nvectors and matrices, even on CPU, things run at least\nan order of magnitude faster.",
    "start": "1095490",
    "end": "1103860"
  },
  {
    "text": "And, well, what\neveryone really wants to move to doing in deep\nlearning is running things",
    "start": "1103860",
    "end": "1109430"
  },
  {
    "text": "on GPUs or sometimes now\nneural processing units and then you're getting two,\nthree orders of magnitude",
    "start": "1109430",
    "end": "1116150"
  },
  {
    "text": "of speed up. So do always think about,\nI should be doing things",
    "start": "1116150",
    "end": "1121220"
  },
  {
    "text": "with vectors and matrices if I'm\nwriting a for loop for anything that isn't some very superficial\nbit of input processing,",
    "start": "1121220",
    "end": "1129570"
  },
  {
    "text": "I've almost certainly\nmade a mistake and I should be working\nout how to do things",
    "start": "1129570",
    "end": "1134960"
  },
  {
    "text": "with vectors and matrices. And that's things like dropout. You don't want to\nwrite a for loop that",
    "start": "1134960",
    "end": "1142220"
  },
  {
    "text": "goes through all the positions\nand sets some of them to 0. You want to be using a vector\noperation with your mask.",
    "start": "1142220",
    "end": "1151370"
  },
  {
    "text": "Two more, I think. Parameter initialization. I mean, this one\nmight not be obvious,",
    "start": "1151370",
    "end": "1158070"
  },
  {
    "text": "but when we start training\nour neural networks",
    "start": "1158070",
    "end": "1163529"
  },
  {
    "text": "in almost all cases\nit's vital that we",
    "start": "1163530",
    "end": "1168890"
  },
  {
    "text": "initialize the parameters of our\nmatrices to some random numbers.",
    "start": "1168890",
    "end": "1175830"
  },
  {
    "text": "And the reason for this is if\nwe just start with our matrices",
    "start": "1175830",
    "end": "1183539"
  },
  {
    "text": "all 0 or some other\nconstant, normally the case is that\nwe have symmetries.",
    "start": "1183540",
    "end": "1192060"
  },
  {
    "text": "So it's like in\nthis picture when you're starting on\nthis saddle point",
    "start": "1192060",
    "end": "1197659"
  },
  {
    "text": "that it's symmetric to the\nleft and the right or whatever,",
    "start": "1197660",
    "end": "1202800"
  },
  {
    "text": "forward and backwards\nand left and right. And so you don't\nknow where to go",
    "start": "1202800",
    "end": "1208400"
  },
  {
    "text": "and you might be stuck\nand stay in the one place. I mean, normally a way to\nthink about is the operations",
    "start": "1208400",
    "end": "1216000"
  },
  {
    "text": "that you're doing to all\nthe elements in the matrix are the same. So rather than having a\nwhole vector of features,",
    "start": "1216000",
    "end": "1224920"
  },
  {
    "text": "if all of them have the\nsame value initially, often it's only have one\nfeature and you've just",
    "start": "1224920",
    "end": "1230190"
  },
  {
    "text": "got a lot of copies of it. So to initialize learning\nand have things work well,",
    "start": "1230190",
    "end": "1236380"
  },
  {
    "text": "we almost always want\nto set all the weights to very small random numbers.",
    "start": "1236380",
    "end": "1243390"
  },
  {
    "text": "And so at that point,\nwhen I say very small,",
    "start": "1243390",
    "end": "1250270"
  },
  {
    "text": "we want to make them in a range\nso that they don't disappear to 0 if we make\nthem a bit smaller",
    "start": "1250270",
    "end": "1255870"
  },
  {
    "text": "and they don't start\nblowing up into huge numbers when we multiply them by things.",
    "start": "1255870",
    "end": "1262480"
  },
  {
    "text": "And doing this initialization\nat the right scale used to be seen as\nsomething pretty important.",
    "start": "1262480",
    "end": "1269320"
  },
  {
    "text": "And there are\nparticular methods that had a basis of thinking\nof what happens once you do matrix\nmultiplies that people",
    "start": "1269320",
    "end": "1275970"
  },
  {
    "text": "had worked out and often used. One of these was this\nXavier initialization,",
    "start": "1275970",
    "end": "1281320"
  },
  {
    "text": "which was working out what\nvariants of your distribution",
    "start": "1281320",
    "end": "1289679"
  },
  {
    "text": "to be using based on the number\nof inputs and outputs of a layer",
    "start": "1289680",
    "end": "1294960"
  },
  {
    "text": "and things like that. The specifics of\nthat, I think we still use to initialize things\nin assignment two,",
    "start": "1294960",
    "end": "1303310"
  },
  {
    "text": "but we'll see later that they go\naway because people have come up with clever methods,\nin particular doing",
    "start": "1303310",
    "end": "1309510"
  },
  {
    "text": "layer normalization, which\nobviates the need to be careful on the initialization.",
    "start": "1309510",
    "end": "1314920"
  },
  {
    "text": "But you still need to\ninitialize things to something. OK, then the final\none, which is also",
    "start": "1314920",
    "end": "1324020"
  },
  {
    "text": "something that appears\nin the second assignment that I just want to say a\nword about was optimizers.",
    "start": "1324020",
    "end": "1329850"
  },
  {
    "text": "So we talked about in class\nstochastic gradient descent",
    "start": "1329850",
    "end": "1335000"
  },
  {
    "text": "and did the basic equations for\nstochastic gradient descent. And to a first\napproximation, there's",
    "start": "1335000",
    "end": "1342919"
  },
  {
    "text": "nothing wrong with\nstochastic gradient descent. And if you fiddle\naround enough, you can usually get stochastic\ngradient descent actually",
    "start": "1342920",
    "end": "1349730"
  },
  {
    "text": "to work well for\nalmost any problem. But getting it to\nwork well is very",
    "start": "1349730",
    "end": "1356570"
  },
  {
    "text": "dependent on getting the\nscales of things right, of having the right step size. And often you have to\nhave a learning rate",
    "start": "1356570",
    "end": "1363080"
  },
  {
    "text": "schedule with\ndecreasing step sizes and various other complications. So people have come up with\nmore sophisticated optimizers",
    "start": "1363080",
    "end": "1373430"
  },
  {
    "text": "for neural networks. And for complex\nnets, sometimes these",
    "start": "1373430",
    "end": "1378470"
  },
  {
    "text": "seem necessary to get\nthem to learn well. And at any rate, they give\nyou lots of margins of safety",
    "start": "1378470",
    "end": "1386250"
  },
  {
    "text": "since they're much less\ndependent on you setting different hyperparameters. And the idea of, well, all\nthe methods I mentioned",
    "start": "1386250",
    "end": "1395400"
  },
  {
    "text": "and the most\ncommonly used methods is that for each\nparameter, they're",
    "start": "1395400",
    "end": "1400980"
  },
  {
    "text": "accumulating a measure\nof what the gradient has been in the past.",
    "start": "1400980",
    "end": "1406440"
  },
  {
    "text": "And they've got some\nidea of the scale of the gradient, the slope,\nfor a particular parameter.",
    "start": "1406440",
    "end": "1412750"
  },
  {
    "text": "And then they're\nusing that to decide how much you move the learning\nrate at each time step.",
    "start": "1412750",
    "end": "1418690"
  },
  {
    "text": "So the simplest method\nthat was come up was this one called Adagrad. If you know John\nDuchi, and he was one",
    "start": "1418690",
    "end": "1425160"
  },
  {
    "text": "of the co-inventors of this. It's simple and nice enough,\nbut it tends to stall early.",
    "start": "1425160",
    "end": "1431620"
  },
  {
    "text": "Then people came up\nwith different methods. Adam is the one that's\non assignment two. It's a really good,\nsafe place to start.",
    "start": "1431620",
    "end": "1439679"
  },
  {
    "text": "But in a way, our word vectors\nhave a special property",
    "start": "1439680",
    "end": "1444710"
  },
  {
    "text": "because of their\nsparseness that you're very sparsely updating them\nbecause particular words only",
    "start": "1444710",
    "end": "1450770"
  },
  {
    "text": "turn up occasionally. So people have actually come up\nwith particular optimizers that",
    "start": "1450770",
    "end": "1457550"
  },
  {
    "text": "sort of have special properties\nfor things like word vectors. And so these ones\nwith the W at the end",
    "start": "1457550",
    "end": "1463010"
  },
  {
    "text": "can sometimes be good to try. And then, again, there's a\nwhole family of extra ideas",
    "start": "1463010",
    "end": "1469550"
  },
  {
    "text": "that people have used\nto improve optimizers. And if you want to\nlearn about that, you can go off and do\nan optimization class",
    "start": "1469550",
    "end": "1475910"
  },
  {
    "text": "like convex optimization. But there are\nideas like momentum and Nesterov acceleration\nand things like that,",
    "start": "1475910",
    "end": "1482340"
  },
  {
    "text": "and all of those things people\nalso variously try to use. But Adam is a good\nname to remember,",
    "start": "1482340",
    "end": "1488120"
  },
  {
    "text": "if you remember nothing else. OK, that took\nlonger than I hoped,",
    "start": "1488120",
    "end": "1493440"
  },
  {
    "text": "but I'll get on now\nto language models. OK, language models. So in some sense, language\nmodel is just two English words.",
    "start": "1493440",
    "end": "1505440"
  },
  {
    "text": "But when in NLP we\nsay language models, we mean it as a technical term\nthat has a particular meaning.",
    "start": "1505440",
    "end": "1513140"
  },
  {
    "text": "So the idea of a\nlanguage model is something that\ncan predict, well,",
    "start": "1513140",
    "end": "1520880"
  },
  {
    "text": "what word is going to come\nnext or more precisely, it's going to put a\nprobability distribution",
    "start": "1520880",
    "end": "1527120"
  },
  {
    "text": "over what words come next. So the students opened their-- what words are\nlikely to come next?",
    "start": "1527120",
    "end": "1532765"
  },
  {
    "text": " Bags. Bags. Laptops.",
    "start": "1532765",
    "end": "1538870"
  },
  {
    "text": "Laptops. Notebooks. Notebooks, yes. I have some of those at least.",
    "start": "1538870",
    "end": "1545530"
  },
  {
    "text": "OK. Yeah.  So these are likely words.",
    "start": "1545530",
    "end": "1553430"
  },
  {
    "text": "And if on top of those we put\na probability on each one,",
    "start": "1553430",
    "end": "1558530"
  },
  {
    "text": "then we have a language model. So formally we've got a\ncontext of preceding items.",
    "start": "1558530",
    "end": "1565940"
  },
  {
    "text": "We're putting a\nprobability distribution over the next item, which means\nthat the sum of the estimates",
    "start": "1565940",
    "end": "1572740"
  },
  {
    "text": "of this for all the items in\nthe vocabulary will sum to 1. And if we've defined a P like\nthis that predicts probabilities",
    "start": "1572740",
    "end": "1581560"
  },
  {
    "text": "of next words, that is called a\nlanguage model, as it says here.",
    "start": "1581560",
    "end": "1588620"
  },
  {
    "text": "An alternative way that you\ncan think of a language model is that a language\nmodel is a system that",
    "start": "1588620",
    "end": "1596360"
  },
  {
    "text": "assigns a probability\nto a piece of text. And so we can say\nthat a language",
    "start": "1596360",
    "end": "1604040"
  },
  {
    "text": "model can take any piece of\ntext and give it a probability. And the reason we can do that\nis we can use the chain rule.",
    "start": "1604040",
    "end": "1611880"
  },
  {
    "text": "So if I want to know the\nprobability of any stretch of text, I say, given\nmy previous definition",
    "start": "1611880",
    "end": "1618799"
  },
  {
    "text": "of language model, easy. I can do that. Probability of x1 with\na null preceding context",
    "start": "1618800",
    "end": "1626000"
  },
  {
    "text": "times the probability of x2,\ngiven x1, et cetera, along. I can do this chain\nrule decomposition",
    "start": "1626000",
    "end": "1633470"
  },
  {
    "text": "and then the terms\nof that decomposition are precisely what the\nlanguage model, as I defined it",
    "start": "1633470",
    "end": "1639860"
  },
  {
    "text": "previously, provides. OK, so language models are this\nessential technology for NLP.",
    "start": "1639860",
    "end": "1647970"
  },
  {
    "text": "Just about everything\nfrom the simplest places forward where people do\nthings with human language",
    "start": "1647970",
    "end": "1655679"
  },
  {
    "text": "and computers, people\nuse language models. In particular, they\nweren't something",
    "start": "1655680",
    "end": "1661409"
  },
  {
    "text": "that got invented in\n2022 with ChatGPT. Language models have\nbeen central to NLP,",
    "start": "1661410",
    "end": "1669300"
  },
  {
    "text": "at least since the 80s. The idea of them goes\nback to at least the 50s.",
    "start": "1669300",
    "end": "1674500"
  },
  {
    "text": "So any time you're\ntyping on your phone and it's making\nsuggestions of next words,",
    "start": "1674500",
    "end": "1680799"
  },
  {
    "text": "regardless of whether you\nlike those suggestions or not, those suggestions\nare being generated",
    "start": "1680800",
    "end": "1686820"
  },
  {
    "text": "by a language\nmodel, traditionally a compact, not very\ngood language model",
    "start": "1686820",
    "end": "1693520"
  },
  {
    "text": "so it can run quickly and very\nlittle memory in your keyboard application.",
    "start": "1693520",
    "end": "1699750"
  },
  {
    "text": "If you go on Google and\nyou start typing some stuff and it's telling you stuff that\ncould come after it to complete",
    "start": "1699750",
    "end": "1707640"
  },
  {
    "text": "your query, well,\nagain, that's being generated by a language model.",
    "start": "1707640",
    "end": "1712799"
  },
  {
    "text": "So how can you build\na language model? So before getting into\nneural language models,",
    "start": "1712800",
    "end": "1718900"
  },
  {
    "text": "I've got just a few\nslides to tell you about the old days\nof language modeling.",
    "start": "1718900",
    "end": "1724030"
  },
  {
    "text": "So this is how\nlanguage models were built from 1975 until\neffectively around about 2012.",
    "start": "1724030",
    "end": "1736740"
  },
  {
    "text": "So we want to put probabilities\non these sequences.",
    "start": "1736740",
    "end": "1743010"
  },
  {
    "text": "And the way we're\ngoing to do it is we're going to build what's called\nan n-gram language model.",
    "start": "1743010",
    "end": "1750780"
  },
  {
    "text": "And so this is\nmeaning we're going to look at short\nword subsequences and use them to predict.",
    "start": "1750780",
    "end": "1757240"
  },
  {
    "text": "So n is a variable\ndescribing how short are the word\nsequences that we're",
    "start": "1757240",
    "end": "1762660"
  },
  {
    "text": "going to use to predict. So if we just look\nat the probabilities of individual words, we have\na unigram language model.",
    "start": "1762660",
    "end": "1771549"
  },
  {
    "text": "If we look at probabilities of\npairs of words, bigram language model, probabilities of three\nwords, trigram language models,",
    "start": "1771550",
    "end": "1780850"
  },
  {
    "text": "probabilities of more\nthan three words, they get called four-gram\nlanguage models, five-gram language models,\nsix-gram language models.",
    "start": "1780850",
    "end": "1789330"
  },
  {
    "text": "So for people with a\nclassics education, this is horrific of course. In particular, not\neven these ones",
    "start": "1789330",
    "end": "1796049"
  },
  {
    "text": "are correct because\ngram is a Greek root,",
    "start": "1796050",
    "end": "1801170"
  },
  {
    "text": "so it should really have\nGreek numbers in front here. So you should have\nmonograms and diagrams.",
    "start": "1801170",
    "end": "1807460"
  },
  {
    "text": "And actually so the\nfirst person who introduced the idea\nof n gram models was actually Claude Shannon\nwhen he was working out",
    "start": "1807460",
    "end": "1814390"
  },
  {
    "text": "information theory, the same\nguy that did cross entropy and all of that. And if you look at his 1951\npaper, he uses diagrams,",
    "start": "1814390",
    "end": "1823250"
  },
  {
    "text": "but the idea died about\nthere and everyone else-- this is what people\nsay in practice.",
    "start": "1823250",
    "end": "1830230"
  },
  {
    "text": "It's quite cute. I like it and I see\na practical notation. So to build these models,\nthe idea is, look,",
    "start": "1830230",
    "end": "1839450"
  },
  {
    "text": "we're just going to count how\noften different n-grams appear in text and use those to build\nour probability estimates.",
    "start": "1839450",
    "end": "1849519"
  },
  {
    "text": "And in particular,\nour trick is that we make a Markov assumption.",
    "start": "1849520",
    "end": "1854990"
  },
  {
    "text": "So that if we're predicting\nthe next word based on a long context, we\nsay, tell you what?",
    "start": "1854990",
    "end": "1862100"
  },
  {
    "text": "We're not going\nto use all of it. We're only going to use the\nmost recent n minus 1 words.",
    "start": "1862100",
    "end": "1869300"
  },
  {
    "text": "So we have this big context\nand we throw most of it away. And so if we're predicting\nword xt plus 1 based on simply",
    "start": "1869300",
    "end": "1878620"
  },
  {
    "text": "the preceding n minus 1\nwords, well then we can make the prediction using n-grams--",
    "start": "1878620",
    "end": "1886850"
  },
  {
    "text": "the fly, whatever it is. If we use n is 3, we'd have\na trigram here and normalize",
    "start": "1886850",
    "end": "1897220"
  },
  {
    "text": "by a bigram down here. And that would give us\nrelative frequencies",
    "start": "1897220",
    "end": "1903700"
  },
  {
    "text": "of the different terms. So we can do that\nsimply by counting",
    "start": "1903700",
    "end": "1910659"
  },
  {
    "text": "how often n-grams occur\nin a large amount of text",
    "start": "1910660",
    "end": "1916420"
  },
  {
    "text": "and simply dividing\nthrough by the counts and that gives us a\nrelative frequency",
    "start": "1916420",
    "end": "1922000"
  },
  {
    "text": "estimate of the probability\nof different continuations. Does that make sense?",
    "start": "1922000",
    "end": "1928250"
  },
  {
    "text": "Yeah, that's a way to do it. OK, so suppose we're learning\na four-gram language model",
    "start": "1928250",
    "end": "1936940"
  },
  {
    "text": "and we've got a piece of\ntext, as the Proctor started the clock, the\nstudents open their.",
    "start": "1936940",
    "end": "1942400"
  },
  {
    "text": "So well, to estimate\nthings, we are going to throw away all but\nthe preceding three words.",
    "start": "1942400",
    "end": "1950570"
  },
  {
    "text": "So we're going to estimate\nbased on students open there. And so we're going to\nwork out the probabilities",
    "start": "1950570",
    "end": "1956890"
  },
  {
    "text": "by looking for\ncounts of students open their W and counts\nof students open their.",
    "start": "1956890",
    "end": "1964300"
  },
  {
    "text": "So we might have in a\ncorpus that students open their occurred 1,000 times,\nstudents open their books",
    "start": "1964300",
    "end": "1971049"
  },
  {
    "text": "occurred 400 times. And so we'd say the\nprobability estimate is simply 0.4 for books.",
    "start": "1971050",
    "end": "1978500"
  },
  {
    "text": "If exams occurred 100 times,\nthe probability estimate is 0.1 for exams.",
    "start": "1978500",
    "end": "1985733"
  },
  {
    "text": " And well, you can\nsee that this is bad.",
    "start": "1985733",
    "end": "1992480"
  },
  {
    "text": "It's not terrible because if\nyou are going to try and predict the next word in a\nsimple way, looking",
    "start": "1992480",
    "end": "1998180"
  },
  {
    "text": "at the immediately prior words\nare the most helpful words to look at. But it's clearly primitive\nbecause if you've",
    "start": "1998180",
    "end": "2007300"
  },
  {
    "text": "known that the prior text\nwas as the Proctor started the clock, that makes it sound\nlikely that the words should",
    "start": "2007300",
    "end": "2013029"
  },
  {
    "text": "have been exams, where since\nyou're estimating just based on students open\ntheirs, well, you'd",
    "start": "2013030",
    "end": "2018790"
  },
  {
    "text": "be more likely to choose books\nbecause it's more common. So a crude estimate, but it's\na decent enough place to start.",
    "start": "2018790",
    "end": "2029950"
  },
  {
    "text": "It's a crude estimate that could\nbe problematic in other ways. I mean, why else might we\nget into troubles by using",
    "start": "2029950",
    "end": "2039400"
  },
  {
    "text": "this probability estimate? Yeah?",
    "start": "2039400",
    "end": "2046070"
  },
  {
    "text": "So there are a lot of n-grams. Yeah, so there\nare a lot of words and therefore there\nare a lot of n-grams.",
    "start": "2046070",
    "end": "2053320"
  },
  {
    "text": "Yeah. So that's a problem. We'll come to it later. Anything else? Maybe up the back.",
    "start": "2053320",
    "end": "2058879"
  },
  {
    "text": "The word W might not even\nshow up in the training data so you might just have\na count 0 for that. Yeah.",
    "start": "2058880",
    "end": "2064219"
  },
  {
    "text": "Yeah. So if we're counting over\nany reasonable sized corpus,",
    "start": "2064219",
    "end": "2071070"
  },
  {
    "text": "there are lots of words\nthat we just are not going to have seen\nthat they never",
    "start": "2071070",
    "end": "2078859"
  },
  {
    "text": "happen to occur in the\ntext that we counted over. So if you start thinking\nstudents open their,",
    "start": "2078860",
    "end": "2085654"
  },
  {
    "text": "there are lots of things\nthat you could put there. Students open their\naccounts or if the students",
    "start": "2085654",
    "end": "2092480"
  },
  {
    "text": "are doing dissections\nin a biology class, maybe students open their frogs. I don't know.",
    "start": "2092480",
    "end": "2099660"
  },
  {
    "text": "There are lots of words\nthat in some context would actually be\npossible and lots of them",
    "start": "2099660",
    "end": "2106080"
  },
  {
    "text": "that we won't have seen. And so we'd give them a\nprobability estimate of 0.",
    "start": "2106080",
    "end": "2111250"
  },
  {
    "text": "And that tends to be\nan especially bad thing to do with probabilities,\nbecause once we have a probability\nestimate of 0,",
    "start": "2111250",
    "end": "2117310"
  },
  {
    "text": "any computations that\nwe do that involve that will instantly go to 0. So we have to deal with\nsome of these problems.",
    "start": "2117310",
    "end": "2124660"
  },
  {
    "text": "So for that sparsity\nproblem, yeah, that we could have the word\nnever occurred in the numerator",
    "start": "2124660",
    "end": "2134080"
  },
  {
    "text": "and so simply done, we get\na probability estimate of 0.",
    "start": "2134080",
    "end": "2139150"
  },
  {
    "text": "The way that was dealt\nwith was that people just hacked the counts a little\nto make it non-zero.",
    "start": "2139150",
    "end": "2144940"
  },
  {
    "text": "So there are lots of\nways that are explored, but the easiest way is you just\nadded a little delta like 0.252",
    "start": "2144940",
    "end": "2153858"
  },
  {
    "text": "to counts. So things that you never saw\ngot a count of 0.25 in total,",
    "start": "2153858",
    "end": "2159760"
  },
  {
    "text": "and things you saw once\ngot a count of 1.25 and then there are\nno zeros anymore.",
    "start": "2159760",
    "end": "2164980"
  },
  {
    "text": "Everything is possible. You could think then there's\na second problem that, wait,",
    "start": "2164980",
    "end": "2170730"
  },
  {
    "text": "you might never have seen\nstudents open their before. And so that means your\ndenominator is just undefined",
    "start": "2170730",
    "end": "2180710"
  },
  {
    "text": "and you don't have any counts\nin the numerator either. So you need to do\nsomething different there.",
    "start": "2180710",
    "end": "2186150"
  },
  {
    "text": "And the standard trick\nthat was used then was that you did back off.",
    "start": "2186150",
    "end": "2191700"
  },
  {
    "text": "So if I couldn't estimate words\ncoming after students open their, you'd just worked\nout the estimates for words",
    "start": "2191700",
    "end": "2199940"
  },
  {
    "text": "coming after open their. And if I couldn't\nestimate that, you just use the estimate of\nwords coming after their.",
    "start": "2199940",
    "end": "2207140"
  },
  {
    "text": "So you use less and less context\nuntil you could get an estimate that you could use.",
    "start": "2207140",
    "end": "2213230"
  },
  {
    "text": "But something to\nnote is that we've got these conflicting pressures\nnow so that on the one hand,",
    "start": "2213230",
    "end": "2220850"
  },
  {
    "text": "if you want to come up\nwith a better estimate that you would like to\nuse more context, i.e.",
    "start": "2220850",
    "end": "2227849"
  },
  {
    "text": "to have a larger n-gram. But on the other hand, as you\nuse more conditioning words,",
    "start": "2227850",
    "end": "2238030"
  },
  {
    "text": "well, the storage size\nproblem someone mentioned gets worse and worse because the\nnumber of n-grams that you have",
    "start": "2238030",
    "end": "2245880"
  },
  {
    "text": "to know about is\ngoing up exponentially with the size of the\ncontext, but also",
    "start": "2245880",
    "end": "2251220"
  },
  {
    "text": "your sparseness problems\nare getting way, way worse and you're\nalmost necessarily going to be ending up seeing zeros.",
    "start": "2251220",
    "end": "2258090"
  },
  {
    "text": "And so because of that,\nin practice where things tended to max out was five.",
    "start": "2258090",
    "end": "2266890"
  },
  {
    "text": "And occasionally people use\nsix-grams and seven-grams. But most of the time,\nbetween the sparseness",
    "start": "2266890",
    "end": "2274350"
  },
  {
    "text": "and the cost of\nstorage, five-grams was the largest thing\npeople dealt with.",
    "start": "2274350",
    "end": "2279360"
  },
  {
    "text": "So a famous resource\nfrom back in the 2000's",
    "start": "2279360",
    "end": "2285020"
  },
  {
    "text": "decade that Google released\nwas Google n-grams, which was built on a trillion word web\ncorpus and had counts of n-grams",
    "start": "2285020",
    "end": "2295100"
  },
  {
    "text": "and it gave counts of\nn-grams up to n equals 5. And that is where they stopped.",
    "start": "2295100",
    "end": "2301630"
  },
  {
    "text": "OK. Well, we've said\nthe storage problem. The storage problem\nis, well, to do this,",
    "start": "2301630",
    "end": "2306700"
  },
  {
    "text": "you need to store these counts. The number of counts is\ngoing up exponentially",
    "start": "2306700",
    "end": "2311740"
  },
  {
    "text": "in the amount of context size. OK.",
    "start": "2311740",
    "end": "2316750"
  },
  {
    "text": "But what's good about\nn-gram language models, they're really easy to build.",
    "start": "2316750",
    "end": "2322250"
  },
  {
    "text": "You can build one\nyourself in a few minutes when you want to have a\nbit of fun on the weekend.",
    "start": "2322250",
    "end": "2328960"
  },
  {
    "text": "All you have to do is start\nstoring these counts for n-grams",
    "start": "2328960",
    "end": "2335320"
  },
  {
    "text": "and you can use them\nto predict things. So at least if you do it over\na small corpus like a couple",
    "start": "2335320",
    "end": "2341680"
  },
  {
    "text": "of million words of text you can\nbuild an n-gram language model",
    "start": "2341680",
    "end": "2347260"
  },
  {
    "text": "in seconds on your laptop or\nyou have to write the software. OK, a few minutes to\nwrite the software.",
    "start": "2347260",
    "end": "2353030"
  },
  {
    "text": "But building the\nmodel takes seconds because there's no\ntraining a neural network. All you do is count how\noften n-grams occur.",
    "start": "2353030",
    "end": "2361829"
  },
  {
    "text": "And so once you've\ndone that, you can then run an n-gram language\nmodel to generate text.",
    "start": "2361830",
    "end": "2368490"
  },
  {
    "text": "We could do text\ngeneration before ChatGPT. So if I have a trigram\nlanguage model,",
    "start": "2368490",
    "end": "2375360"
  },
  {
    "text": "I can start off with\nsome words, today the, and I could look at\nmy stored n-grams",
    "start": "2375360",
    "end": "2383060"
  },
  {
    "text": "and get a probability\ndistribution over next words. And here they are.",
    "start": "2383060",
    "end": "2388350"
  },
  {
    "text": "Note the strong patterning of\nthese probabilities because,",
    "start": "2388350",
    "end": "2396380"
  },
  {
    "text": "remember, they're all\nderived from counts that are being normalized. So really, these are\nwords that occurred once.",
    "start": "2396380",
    "end": "2403680"
  },
  {
    "text": "These are words\nthat occurred twice. These are words that occurred\nfour times in this context. So they're, in some\nsense, crude when you",
    "start": "2403680",
    "end": "2411650"
  },
  {
    "text": "look at them more carefully. So what we could do\nis then at this point, we roll a die and get a\nrandom number from 0 to 1.",
    "start": "2411650",
    "end": "2422010"
  },
  {
    "text": "And we can use that to sample\nfrom this distribution.",
    "start": "2422010",
    "end": "2427210"
  },
  {
    "text": "Sorry. Yeah. So we sample from\nthis distribution.",
    "start": "2427210",
    "end": "2432890"
  },
  {
    "text": "And so that if we generate\nas our random number",
    "start": "2432890",
    "end": "2439369"
  },
  {
    "text": "something like 0.35, if we go\ndown from the top, we'd say,",
    "start": "2439370",
    "end": "2446060"
  },
  {
    "text": "OK, we've sampled the word\nprice, today, the price. And then we repeat over.",
    "start": "2446060",
    "end": "2451270"
  },
  {
    "text": "We condition on that,\nprobability distribution of the next word. We generate a random\nnumber and use it to sample",
    "start": "2451270",
    "end": "2458620"
  },
  {
    "text": "from the distribution. We say generate 0.2. And so we choose of.",
    "start": "2458620",
    "end": "2465070"
  },
  {
    "text": "We now condition on that. We get a probability\ndistribution. We generate a random number,\nwhich is 0.5 or something.",
    "start": "2465070",
    "end": "2473420"
  },
  {
    "text": "And so we get gold coming out. And we can say, today\nthe price of gold. And we can keep on doing\nthis and generate some text.",
    "start": "2473420",
    "end": "2482059"
  },
  {
    "text": "And so here's some\ntext generated from 2 million words\nof training data",
    "start": "2482060",
    "end": "2488660"
  },
  {
    "text": "using a trigram language model. Today, the price of gold per\nton, while production of shoe",
    "start": "2488660",
    "end": "2495650"
  },
  {
    "text": "lasts and shoe industry. The bank intervened just after\nit considered and rejected an IMF demand to\nrebuild depleted",
    "start": "2495650",
    "end": "2503150"
  },
  {
    "text": "European stocks, September\n3 and primary $0.76 a share.",
    "start": "2503150",
    "end": "2511190"
  },
  {
    "text": "OK, that text isn't\ngreat, but I actually want people to be in\na positive mood today.",
    "start": "2511190",
    "end": "2519259"
  },
  {
    "text": "And actually, it's not so bad.",
    "start": "2519260",
    "end": "2524370"
  },
  {
    "text": "It's surprisingly grammatical. I mean, in particular,\nI lowercased everything.",
    "start": "2524370",
    "end": "2530819"
  },
  {
    "text": "So this is the\nIMF that should be capitalized of the\nInternational Monetary Fund. There are big pieces of\nthis that even make sense.",
    "start": "2530820",
    "end": "2539750"
  },
  {
    "text": "The bank intervened\njust after it considered and rejected an IMF demand.",
    "start": "2539750",
    "end": "2545099"
  },
  {
    "text": "That's pretty much making\nsense as a piece of text.",
    "start": "2545100",
    "end": "2550250"
  },
  {
    "text": "So it's mostly grammatical. It looks like English text.",
    "start": "2550250",
    "end": "2556590"
  },
  {
    "text": "I mean, it makes no sense. It's really incoherent. So there's work to do.",
    "start": "2556590",
    "end": "2562440"
  },
  {
    "text": "But what was already--\nyou could see that even these simple n-gram\nmodels, from a very low level,",
    "start": "2562440",
    "end": "2572640"
  },
  {
    "text": "you could approach what\ntext and human language worked like in from below.",
    "start": "2572640",
    "end": "2579780"
  },
  {
    "text": "And I could easily\nmake this better even with an n-gram language\nmodel because rather than 2 million\nwords of text, if I",
    "start": "2579780",
    "end": "2585830"
  },
  {
    "text": "trained on 10 million words\nof text, it'd be better. If I then rather\nthan a trigram model, could go to a four-gram\nmodel and get better.",
    "start": "2585830",
    "end": "2592880"
  },
  {
    "text": "And you'd start getting better\nand better approximations",
    "start": "2592880",
    "end": "2598400"
  },
  {
    "text": "of text. And so this is essentially what\npeople did until about 2012.",
    "start": "2598400",
    "end": "2607080"
  },
  {
    "text": "And really, the same story\nthat people tell today",
    "start": "2607080",
    "end": "2613430"
  },
  {
    "text": "that scale will solve everything\nis exactly the same story that people used to\ntell in the early 2010's",
    "start": "2613430",
    "end": "2621319"
  },
  {
    "text": "with these n-gram\nlanguage models. If you weren't getting\na good enough results with your 10 million words of\ntext and a trigram language",
    "start": "2621320",
    "end": "2628820"
  },
  {
    "text": "model, the answer\nwas that if you had 100 million words of text\nand a four-gram language model,",
    "start": "2628820",
    "end": "2635040"
  },
  {
    "text": "you'd do better. And then if you had\na trillion words of text and a five-gram\nlanguage model, you'd do better.",
    "start": "2635040",
    "end": "2641040"
  },
  {
    "text": "And wouldn't it be good if we\ncould collect 10 trillion words of text so we could train an\neven better n-gram language",
    "start": "2641040",
    "end": "2647660"
  },
  {
    "text": "model, same strategy? But it turns out\nthat sometimes you can do better with better\nmodels as well as simply scale.",
    "start": "2647660",
    "end": "2656600"
  },
  {
    "text": "And so things got reinvented\nand started again with building",
    "start": "2656600",
    "end": "2661610"
  },
  {
    "text": "neural language models. And so how can we build\na neural language model?",
    "start": "2661610",
    "end": "2668750"
  },
  {
    "text": "So we've got the same task\nof having a sequence of words",
    "start": "2668750",
    "end": "2673760"
  },
  {
    "text": "and we want to put a\nprobability estimate over what word comes next.",
    "start": "2673760",
    "end": "2679160"
  },
  {
    "text": "And so the simplest\nway you could do that, which you'll\nhopefully of all have thought of because it connects\nwhat we did in earlier classes.",
    "start": "2679160",
    "end": "2688320"
  },
  {
    "text": "Look, we already had this idea\nthat we could have represent a context by the concatenation\nof some word vectors,",
    "start": "2688320",
    "end": "2697140"
  },
  {
    "text": "and we could put that\ninto a neural network and we could use that\nto predict something.",
    "start": "2697140",
    "end": "2704610"
  },
  {
    "text": "And in the example I did in the\nlast couple of classes, what we used it to predict was,\nis the center word a location",
    "start": "2704610",
    "end": "2712940"
  },
  {
    "text": "or not a location,\njust a binary choice. But that's not the only\nthing we could predict.",
    "start": "2712940",
    "end": "2718473"
  },
  {
    "text": "We could have predicted lots of\nthings with this neural network. We could have predicted\nwhether the piece of text was positive or negative.",
    "start": "2718473",
    "end": "2725437"
  },
  {
    "text": "We could have predicted\nwhether it was written in English or Japanese. We could predict lots of things.",
    "start": "2725437",
    "end": "2731289"
  },
  {
    "text": "So one thing we could\nchoose to predict is we could choose to predict\nwhat word is going to come next",
    "start": "2731290",
    "end": "2737290"
  },
  {
    "text": "after this window of text. We'd have a model just like\nthis one, apart from up the top.",
    "start": "2737290",
    "end": "2742630"
  },
  {
    "text": "Instead of doing this\nbinary classification, we'd do a many-many\nway classification",
    "start": "2742630",
    "end": "2748770"
  },
  {
    "text": "over what is the next\nword that is going to appear in the piece of text,\nand that would then give us",
    "start": "2748770",
    "end": "2755490"
  },
  {
    "text": "a neural language model. In particular, it gives us a\nfixed window neural language",
    "start": "2755490",
    "end": "2761580"
  },
  {
    "text": "model so that we do the\nsame Markov assumption trick of throwing away\nthe further back context.",
    "start": "2761580",
    "end": "2770080"
  },
  {
    "text": "And so for the\nfixed window, we'll",
    "start": "2770080",
    "end": "2775530"
  },
  {
    "text": "use word embeddings,\nwhich you can concatenate. We'll put it through\na hidden layer",
    "start": "2775530",
    "end": "2781660"
  },
  {
    "text": "and then we'll take the\noutput of that hidden layer, multiply it by another\nlayer, say, and then",
    "start": "2781660",
    "end": "2789350"
  },
  {
    "text": "put that through a softmax and\nget an output distribution. And so this gives us a fixed\nwindow neural language model.",
    "start": "2789350",
    "end": "2798960"
  },
  {
    "text": "And, apart from the\nfact that we're now doing a classification\nover many-many classes,",
    "start": "2798960",
    "end": "2806250"
  },
  {
    "text": "this is exactly like\nwhat we did last week. So it should look familiar.",
    "start": "2806250",
    "end": "2812490"
  },
  {
    "text": "It's also like what you're\ndoing for assignment two. And so this is essentially\nthe first neural language",
    "start": "2812490",
    "end": "2820820"
  },
  {
    "text": "model that was proposed. So in particular, Yoshua Bengio,\nreally right at the beginning",
    "start": "2820820",
    "end": "2829740"
  },
  {
    "text": "of the 21st Century, suggested\nthat you could do this, that rather than using\nan n-gram language model,",
    "start": "2829740",
    "end": "2836170"
  },
  {
    "text": "you could use a fixed window\nneural language model. And even at that point,\nhe and colleagues",
    "start": "2836170",
    "end": "2844560"
  },
  {
    "text": "were able to get some positive\nresults from this model. But at the time, it\nwasn't widely noticed.",
    "start": "2844560",
    "end": "2852609"
  },
  {
    "text": "It didn't really take\noff that much and it was for a combination of reasons. When there was only\na fixed window,",
    "start": "2852610",
    "end": "2860140"
  },
  {
    "text": "it was not that different\nto n-grams in some sense. And although the\nneural network could",
    "start": "2860140",
    "end": "2865830"
  },
  {
    "text": "give better generalization,\nit could be argued rather than using counts.",
    "start": "2865830",
    "end": "2871090"
  },
  {
    "text": "I mean, in practice,\nneural nets were still hard to run without GPUs.",
    "start": "2871090",
    "end": "2878020"
  },
  {
    "text": "And people felt-- and I think\nin general this was the case-- that you could get more oomph\nby doing the scale story",
    "start": "2878020",
    "end": "2886870"
  },
  {
    "text": "and collecting your n-gram\ncounts on hundreds of billions of words of text rather\nthan trying to make",
    "start": "2886870",
    "end": "2894880"
  },
  {
    "text": "a neural network out of it. And so it didn't really\nespecially take off at that time.",
    "start": "2894880",
    "end": "2900800"
  },
  {
    "text": "But in principle, it\nseemed a nice thing. It got rid of the\nsparsity problem.",
    "start": "2900800",
    "end": "2906310"
  },
  {
    "text": "It got rid of the storage costs. You no longer have to\nstore all observed n-grams.",
    "start": "2906310",
    "end": "2911990"
  },
  {
    "text": "You just have to\nstore the parameters of your neural network. But it didn't solve\nall the problems",
    "start": "2911990",
    "end": "2917830"
  },
  {
    "text": "that we'd like to solve. So in particular, we still\nhave this problem of the Markov",
    "start": "2917830",
    "end": "2923350"
  },
  {
    "text": "assumption that we're just using\na small fixed context beforehand to predict from.",
    "start": "2923350",
    "end": "2930890"
  },
  {
    "text": "And there are some disadvantages\nto enlarging that window and there's no fixed window\nthat's ever big enough.",
    "start": "2930890",
    "end": "2941170"
  },
  {
    "text": "There's another thing that\nif you look technically at this model that might make\nyou suspicious of it, which",
    "start": "2941170",
    "end": "2949960"
  },
  {
    "text": "is when we have words\nin different positions,",
    "start": "2949960",
    "end": "2955130"
  },
  {
    "text": "that those words in\ndifferent positions will be treated by\ncompletely different subparts",
    "start": "2955130",
    "end": "2962440"
  },
  {
    "text": "of this matrix W. So you might\nthink that, OK, for predicting",
    "start": "2962440",
    "end": "2969430"
  },
  {
    "text": "that books comes next, the\nfact that this is a student",
    "start": "2969430",
    "end": "2974470"
  },
  {
    "text": "is important, but it doesn't\nmatter so much exactly where the word student occurs.",
    "start": "2974470",
    "end": "2981850"
  },
  {
    "text": "The context could have been\nthe students slowly open their",
    "start": "2981850",
    "end": "2987400"
  },
  {
    "text": "and it's still\nthe same students. We've just got a bit\ndifferent linguistic structure",
    "start": "2987400",
    "end": "2992560"
  },
  {
    "text": "where this W matrix would\nbe using completely separate parameters to be learning\nstuff about student",
    "start": "2992560",
    "end": "2998680"
  },
  {
    "text": "here versus student\nin this position. So that seems\ninefficient and wrong.",
    "start": "2998680",
    "end": "3005340"
  },
  {
    "text": "And so that suggested\nthat we need a different kind of\nneural architecture",
    "start": "3005340",
    "end": "3011120"
  },
  {
    "text": "that can process\nany length of input and can use the same\nparameters to say,",
    "start": "3011120",
    "end": "3019590"
  },
  {
    "text": "hey, I saw the word student. That's evidence that things\nlike books, exams, homework will be turning up regardless\nof where it occurs.",
    "start": "3019590",
    "end": "3027599"
  },
  {
    "text": "And so that then\nled to exploration of this different neural\nnetwork architecture",
    "start": "3027600",
    "end": "3034190"
  },
  {
    "text": "called recurrent\nneural networks, which is what I'll go on to next. But before I do, is everyone\nbasically OK with what",
    "start": "3034190",
    "end": "3041780"
  },
  {
    "text": "a language model is? Yeah. No questions? ",
    "start": "3041780",
    "end": "3048450"
  },
  {
    "text": "OK. Recurrent neural networks. ",
    "start": "3048450",
    "end": "3055470"
  },
  {
    "text": "So recurrent neural networks\nis a different family",
    "start": "3055470",
    "end": "3061260"
  },
  {
    "text": "of neural networks. So effectively in this class,\nwe see several neural network",
    "start": "3061260",
    "end": "3066840"
  },
  {
    "text": "architectures. So in some sense, the\nfirst architecture we saw",
    "start": "3066840",
    "end": "3072690"
  },
  {
    "text": "was word2vec. It's a very simple encoder\ndecoder architecture.",
    "start": "3072690",
    "end": "3080880"
  },
  {
    "text": "The second family we\nsaw was feedforward networks or fully connected\nlayer classic neural networks.",
    "start": "3080880",
    "end": "3090640"
  },
  {
    "text": "And the third family\nwe're going to see is recurrent neural networks,\nwhich have different kinds. And then we'll go on and go\non to transformer models.",
    "start": "3090640",
    "end": "3099420"
  },
  {
    "text": "So the idea of a\nrecurrent neural network is that you've got\none set of weights",
    "start": "3099420",
    "end": "3107550"
  },
  {
    "text": "that are going to be applied\nthrough successive moments in time, i.e.",
    "start": "3107550",
    "end": "3113930"
  },
  {
    "text": "sets of positions in the text. And as you do that, you're\ngoing to update the parameters",
    "start": "3113930",
    "end": "3119910"
  },
  {
    "text": "as you go. We'll go through this in\nquite a bit of detail. But here's the idea of it.",
    "start": "3119910",
    "end": "3126850"
  },
  {
    "text": "So we've got the\nstudents open their and we want to predict\nwith that and the way",
    "start": "3126850",
    "end": "3132240"
  },
  {
    "text": "that we're going to do it-- OK. I've still got four\nwords in my example.",
    "start": "3132240",
    "end": "3138450"
  },
  {
    "text": "So I can put stuff down\nthe left side of the slide. But there could\nhave been 24 words",
    "start": "3138450",
    "end": "3143690"
  },
  {
    "text": "with recurrent neural\nnetworks because they can deal with any length of context. OK, so as before,\nour words start off",
    "start": "3143690",
    "end": "3152329"
  },
  {
    "text": "as just words or one hot vectors\nand we can look up their word embeddings just like before.",
    "start": "3152330",
    "end": "3159470"
  },
  {
    "text": "OK, but now to compute\nprobabilities for the next word,",
    "start": "3159470",
    "end": "3164970"
  },
  {
    "text": "we're going to do\nsomething different. So our hidden layer is\ngoing to be recurrent.",
    "start": "3164970",
    "end": "3171150"
  },
  {
    "text": "And by recurrent,\nit means we're going to change a hidden\nstate at each time step",
    "start": "3171150",
    "end": "3178670"
  },
  {
    "text": "as we proceed through the\ntext from left to right. So we're going to start\noff with an h0, which",
    "start": "3178670",
    "end": "3185390"
  },
  {
    "text": "is the initial hidden state,\nwhich can actually just be all zeros.",
    "start": "3185390",
    "end": "3190430"
  },
  {
    "text": "And then at each time step,\nwhat we're going to do is we're going to multiply\nthe previous hidden state",
    "start": "3190430",
    "end": "3199140"
  },
  {
    "text": "by a weight matrix. We're going to take\na word embedding and multiply it by\na weight matrix,",
    "start": "3199140",
    "end": "3206980"
  },
  {
    "text": "and then we're going to sum the\nresults of those two things, and that's going to give\nus a new hidden state.",
    "start": "3206980",
    "end": "3212980"
  },
  {
    "text": "So that hidden state will then\nstore a memory of everything",
    "start": "3212980",
    "end": "3218400"
  },
  {
    "text": "that's been seen so far. So we'll do that and then\nwe'll continue along.",
    "start": "3218400",
    "end": "3224230"
  },
  {
    "text": "So we'll multiply\nthe next word vector by the same weight matrix, we.",
    "start": "3224230",
    "end": "3230580"
  },
  {
    "text": "We multiply the\nprevious hidden state by the same weight matrix,\nWh and we add them together",
    "start": "3230580",
    "end": "3238980"
  },
  {
    "text": "and get a new representation. I've only said this bit,\nso I've left out a bit.",
    "start": "3238980",
    "end": "3245760"
  },
  {
    "text": "Commonly, there are two\nother things you're doing. You're adding on a bias\nterm because we usually",
    "start": "3245760",
    "end": "3250769"
  },
  {
    "text": "separate out a bias term. And you're putting things\nthrough a non-linearity. So I should make\nsure I mention that.",
    "start": "3250770",
    "end": "3257440"
  },
  {
    "text": "And for recurrent\nneural networks, most commonly this\nnon-linearity is actually",
    "start": "3257440",
    "end": "3263190"
  },
  {
    "text": "been the tanh function. So it's balanced on the\npositive and negative side. And so keep on doing\nthat through each step.",
    "start": "3263190",
    "end": "3271570"
  },
  {
    "text": "And so the idea is once\nwe've gone to here, this h4's hidden state\nis a hidden state,",
    "start": "3271570",
    "end": "3278490"
  },
  {
    "text": "in some sense has read\nthe text up until now. It's seen all of the\nstudents open their",
    "start": "3278490",
    "end": "3284230"
  },
  {
    "text": "and if the word\nstudents occurred in any of these\npositions, it will have been multiplied by the\nsame we matrix and added",
    "start": "3284230",
    "end": "3293490"
  },
  {
    "text": "into the hidden state. So it's got a\ncleaner low parameter way of incorporating in the\ninformation that's seen.",
    "start": "3293490",
    "end": "3301480"
  },
  {
    "text": "So now I want to\npredict the next word. And to predict the next\nword I'm then going to do,",
    "start": "3301480",
    "end": "3308470"
  },
  {
    "text": "based on the final hidden\nstate, the same thing I did thing I did before.",
    "start": "3308470",
    "end": "3314090"
  },
  {
    "text": "So I'm going to multiply\nthat hidden state by a matrix and add another bias and\nstick that through a softmax",
    "start": "3314090",
    "end": "3321520"
  },
  {
    "text": "and use that to sample\nfrom that softmax. Well, the softmax will\ngive me a language model,",
    "start": "3321520",
    "end": "3328640"
  },
  {
    "text": "a probability over\nall next words and I can sample from it\nto generate the next word.",
    "start": "3328640",
    "end": "3333990"
  },
  {
    "text": " That makes sense?",
    "start": "3333990",
    "end": "3339599"
  },
  {
    "text": "OK, recurrent neural networks. ",
    "start": "3339600",
    "end": "3349160"
  },
  {
    "text": "OK. So for recurrent\nneural networks, we can now process any\nlength of preceding context",
    "start": "3349160",
    "end": "3356690"
  },
  {
    "text": "and we'll just put more and\nmore stuff in our hidden state. ",
    "start": "3356690",
    "end": "3362990"
  },
  {
    "text": "So our computation is using\ninformation from many steps back.",
    "start": "3362990",
    "end": "3369890"
  },
  {
    "text": "Our model size doesn't increase\nfor having a long context.",
    "start": "3369890",
    "end": "3375230"
  },
  {
    "text": "We have to do more computation\nfor a long context, but our representation\nof that long context",
    "start": "3375230",
    "end": "3381590"
  },
  {
    "text": "just remains this fixed\nsize hidden vector, h of whatever dimension it is.",
    "start": "3381590",
    "end": "3387210"
  },
  {
    "text": "So there's no exponential\nblow out anymore. There's the same weights\napply in every time step.",
    "start": "3387210",
    "end": "3393210"
  },
  {
    "text": "So there's a symmetry in\nhow inputs are processed. There are some catches.",
    "start": "3393210",
    "end": "3399500"
  },
  {
    "text": "The biggest catch in practice\nis that recurrent computation is slow.",
    "start": "3399500",
    "end": "3405170"
  },
  {
    "text": "So for the feedforward layer,\nwe just had an input vector.",
    "start": "3405170",
    "end": "3410559"
  },
  {
    "text": "We multiply it by matrix. We multiplied it by a\nmatrix however many times, and then at the end we're done.",
    "start": "3410560",
    "end": "3416440"
  },
  {
    "text": "Whereas here, we've stuck\nwith this sequentiality that you have to be doing\none hidden vector at a time.",
    "start": "3416440",
    "end": "3424630"
  },
  {
    "text": "In fact, this is\ngoing against what I said at the beginning of class\nbecause essentially here you're",
    "start": "3424630",
    "end": "3429660"
  },
  {
    "text": "doing a for loop. You're going through\nfor time equals 1 to t and then you're generating\na term each hidden vector.",
    "start": "3429660",
    "end": "3438160"
  },
  {
    "text": "And that's one of the\nbig problems with RNNs that have led them\nto fall out of favor.",
    "start": "3438160",
    "end": "3444830"
  },
  {
    "text": "There's another\nproblem that we'll look at more is that in\ntheory, this is perfect.",
    "start": "3444830",
    "end": "3452970"
  },
  {
    "text": "You're just incorporating\nall of the past context in your hidden vector.",
    "start": "3452970",
    "end": "3457980"
  },
  {
    "text": "In practice, it tends\nnot to work perfectly because although stuff you\nsaw back here is in some sense",
    "start": "3457980",
    "end": "3467510"
  },
  {
    "text": "still alive in the hidden\nvector as you come across here, that your memory of it\ngets more and more distant.",
    "start": "3467510",
    "end": "3475200"
  },
  {
    "text": "And it's the words\nthat you saw recently that dominate the hidden state. Now, in some sense,\nthat's right,",
    "start": "3475200",
    "end": "3481470"
  },
  {
    "text": "because the recent stuff is\nthe most important stuff that's freshest in your mind. It's the same with human beings.",
    "start": "3481470",
    "end": "3488060"
  },
  {
    "text": "They tend to forget stuff\nfrom further back as well. But RNNs, especially in the\nsimple form that I've just",
    "start": "3488060",
    "end": "3494690"
  },
  {
    "text": "explained, forget stuff from\nfurther back rather too quickly",
    "start": "3494690",
    "end": "3500240"
  },
  {
    "text": "and we'll come back to that\nagain in Thursday's class.",
    "start": "3500240",
    "end": "3507290"
  },
  {
    "text": "OK. So for training an\nRNN language model, the starting off point is we\nget a big corpus of text again",
    "start": "3507290",
    "end": "3516680"
  },
  {
    "text": "and then we're going to\ncompute for each time step",
    "start": "3516680",
    "end": "3523309"
  },
  {
    "text": "a prediction of the\nprobability of next words. And then there's going to\nbe an actual next word.",
    "start": "3523310",
    "end": "3531300"
  },
  {
    "text": "And we're going to use that\nas the basis of our loss.",
    "start": "3531300",
    "end": "3536630"
  },
  {
    "text": "So our loss function\nis the cross-entropy between the\npredicted probability",
    "start": "3536630",
    "end": "3542340"
  },
  {
    "text": "and what the actual next word\nthat we saw is, which again, as in the example\nI showed before,",
    "start": "3542340",
    "end": "3548400"
  },
  {
    "text": "is just the negative\nlog likelihood of the actual next word.",
    "start": "3548400",
    "end": "3553619"
  },
  {
    "text": "Ideally, you'd like to\npredict the actual next word with probability 1, which means\nthe negative log of 1 would be 0",
    "start": "3553620",
    "end": "3563120"
  },
  {
    "text": "and there'd be no loss. But in practice, if you\ngive it an estimate of 0.5, there's only a little\nbit of loss and so on.",
    "start": "3563120",
    "end": "3570940"
  },
  {
    "text": "And so to get our overall\nobjective function,",
    "start": "3570940",
    "end": "3575950"
  },
  {
    "text": "we work out the average loss,\nthe average negative log likelihood of predicting\neach word in turn.",
    "start": "3575950",
    "end": "3583650"
  },
  {
    "text": "So showing that as pictures,\nif our corpus is the students opened their exams,\nwe're first of all,",
    "start": "3583650",
    "end": "3590770"
  },
  {
    "text": "going to be trying to predict\nwhat comes after that.",
    "start": "3590770",
    "end": "3596980"
  },
  {
    "text": "And we will predict some word\nwith different probabilities",
    "start": "3596980",
    "end": "3603240"
  },
  {
    "text": "and then we'll say the\nactual next word is students. OK, you gave that a\nprobability of 0.05, say,",
    "start": "3603240",
    "end": "3610470"
  },
  {
    "text": "because all we know was that\nthe first word was there. OK, there's a loss for\nthat, the negative log",
    "start": "3610470",
    "end": "3616710"
  },
  {
    "text": "prob given to students. We then go on and generate\nthe probability estimate",
    "start": "3616710",
    "end": "3623130"
  },
  {
    "text": "over the next words and then\nwe say, well, the actual word is opened.",
    "start": "3623130",
    "end": "3628269"
  },
  {
    "text": "What probability estimate\ndid you give to that? We get a negative\nprobability loss.",
    "start": "3628270",
    "end": "3633580"
  },
  {
    "text": "Keep on running this along and\nthen we sum all of those losses",
    "start": "3633580",
    "end": "3639480"
  },
  {
    "text": "and we average them per word. And that's our\naverage per word loss.",
    "start": "3639480",
    "end": "3645279"
  },
  {
    "text": "And we want to make that\nas small as possible. And so that's our\ntraining mechanism.",
    "start": "3645280",
    "end": "3652230"
  },
  {
    "text": "And it's important to notice\nthat for generating this loss,",
    "start": "3652230",
    "end": "3660010"
  },
  {
    "text": "we're not just doing\nfree generation. We're not just\nsaying to the model, go off and generate a sentence.",
    "start": "3660010",
    "end": "3665760"
  },
  {
    "text": "What we're actually\ndoing is at each step, we're effectively\nsaying, OK, the prefix",
    "start": "3665760",
    "end": "3671820"
  },
  {
    "text": "is the students opened. What probability distribution\ndo you put on next words",
    "start": "3671820",
    "end": "3676830"
  },
  {
    "text": "after that? Generate it with our\nrecurrent neural network and then say ask for\nthe actual next word.",
    "start": "3676830",
    "end": "3684730"
  },
  {
    "text": "What probability estimate\ndid you give to their? And that's our loss. But then what we\ndo is stick there",
    "start": "3684730",
    "end": "3692099"
  },
  {
    "text": "into our recurrent neural\nnetwork the right answer. So we always go back\nto the right answer,",
    "start": "3692100",
    "end": "3697990"
  },
  {
    "text": "generate probability\ndistribution for next words, and then ask, OK,\nwhat probability",
    "start": "3697990",
    "end": "3704430"
  },
  {
    "text": "did you give to the\nactual next word exams? And then again, we use\nthe actual next word.",
    "start": "3704430",
    "end": "3710349"
  },
  {
    "text": "So we do one step\nof generation, then we pull it back to what\nwas actually generated,",
    "start": "3710350",
    "end": "3717070"
  },
  {
    "text": "what was actually in the text. And then we ask it for\nguesses over the next word",
    "start": "3717070",
    "end": "3722560"
  },
  {
    "text": "and repeat forever. And so the fact that we\ndon't do free generation, but we pull it back to the\nactual piece of text each time",
    "start": "3722560",
    "end": "3731559"
  },
  {
    "text": "makes things simple because we\nknow what an actual author used",
    "start": "3731560",
    "end": "3737890"
  },
  {
    "text": "for the next word. And that process is\ncalled teacher forcing. And so the most common way\nto train language models",
    "start": "3737890",
    "end": "3747340"
  },
  {
    "text": "is using this teacher\nforcing method. I mean, it's not\nperfect in all respects because we're not\nactually exploring",
    "start": "3747340",
    "end": "3755380"
  },
  {
    "text": "different things the model might\nwant to generate on its own and seeing what\ncomes after them. We're only doing the\ntell me the next word",
    "start": "3755380",
    "end": "3763030"
  },
  {
    "text": "from some human\ngenerated piece of text. ",
    "start": "3763030",
    "end": "3771900"
  },
  {
    "text": "OK. So that's how we get losses. And then after that,\nwe want to, as before,",
    "start": "3771900",
    "end": "3781420"
  },
  {
    "text": "use these losses to update the\nparameters of a neural network.",
    "start": "3781420",
    "end": "3787359"
  },
  {
    "text": "OK. And how do we do that? Well, in principle,\nwe just have all",
    "start": "3787360",
    "end": "3794950"
  },
  {
    "text": "of the texts that\nwe've collected, which you could think of as\njust a real long sequence of,",
    "start": "3794950",
    "end": "3800619"
  },
  {
    "text": "OK, we've got a\nbillion words of text. Here it is. So in theory, you could just run\nyour recurrent neural network",
    "start": "3800620",
    "end": "3809440"
  },
  {
    "text": "over your billion words of text,\nupdating the context as you go.",
    "start": "3809440",
    "end": "3814690"
  },
  {
    "text": "But that would make\nit very difficult to train a model because\nyou'd be accumulating",
    "start": "3814690",
    "end": "3822730"
  },
  {
    "text": "these losses for a billion steps\nand you'd have to store them and then you'd have\nto store hidden states",
    "start": "3822730",
    "end": "3831100"
  },
  {
    "text": "so you could update parameters\nand it just wouldn't work. So what we actually do is we cut\nour training data into segments",
    "start": "3831100",
    "end": "3840099"
  },
  {
    "text": "of a reasonable\nlength, and then we're going to run our\nrecurrent neural network",
    "start": "3840100",
    "end": "3847070"
  },
  {
    "text": "on those segments. And then we're going to compute\na loss for each segment,",
    "start": "3847070",
    "end": "3852330"
  },
  {
    "text": "and then we're going to\nupdate the parameters of the recurrent neural\nnetwork based on the losses",
    "start": "3852330",
    "end": "3859550"
  },
  {
    "text": "that we found for that segment. I describe it here\nas the segments",
    "start": "3859550",
    "end": "3865490"
  },
  {
    "text": "being sentences or\ndocuments, which seems a linguistically nice thing.",
    "start": "3865490",
    "end": "3870859"
  },
  {
    "text": "It turns out that in recent\npractice, when you're wanting to scale most\nefficiently on GPUs,",
    "start": "3870860",
    "end": "3879000"
  },
  {
    "text": "people don't bother with\nthose linguistic niceties. They just say a\nsegment is 100 words.",
    "start": "3879000",
    "end": "3885359"
  },
  {
    "text": "Just cut every 100 words. And the reason why\nthat's really convenient is you can then create a batch\nof segments, all of which",
    "start": "3885360",
    "end": "3893660"
  },
  {
    "text": "are hundreds words long\nand stick those in a matrix and do vectorized\ntraining more efficiently",
    "start": "3893660",
    "end": "3902600"
  },
  {
    "text": "and things go great for you. OK. But there's still\na few more things that we need to know to get\nthings to work great for you",
    "start": "3902600",
    "end": "3910360"
  },
  {
    "text": "and I'll try and get a bit more\nthrough this before today ends. So we need to know\nabout how to work out",
    "start": "3910360",
    "end": "3919339"
  },
  {
    "text": "the derivative of\nour loss with respect to the parameters of our\nrecurrent neural network.",
    "start": "3919340",
    "end": "3930540"
  },
  {
    "text": "And the interesting\ncase here is these Wh parameters are being\nused everywhere",
    "start": "3930540",
    "end": "3938660"
  },
  {
    "text": "through the neural network at\neach stage, as are the We ones. So they appear at many\nplaces in the network.",
    "start": "3938660",
    "end": "3946770"
  },
  {
    "text": "So how do we work out\nthe partial derivatives of the loss with respect to\nthe repeated weight matrices?",
    "start": "3946770",
    "end": "3955350"
  },
  {
    "text": "And the answer to that\nis it's really simple. You can just pretend that\nthose Wh's in each position",
    "start": "3955350",
    "end": "3966230"
  },
  {
    "text": "are different and work out the\npartials with respect to them at one position.",
    "start": "3966230",
    "end": "3972620"
  },
  {
    "text": "And then to get the\npartials with respect to Wh, you just sum whatever you found\nin the different positions.",
    "start": "3972620",
    "end": "3981320"
  },
  {
    "text": "And so that is-- OK, the gradient with\nrespect to repeated weight",
    "start": "3981320",
    "end": "3988490"
  },
  {
    "text": "is the sum of the gradient with\nrespect to each time it appears. And the reason why\nthat is, it follows",
    "start": "3988490",
    "end": "3996440"
  },
  {
    "text": "what I talked about in\nlecture three that we talked--",
    "start": "3996440",
    "end": "4001960"
  },
  {
    "text": "or you can also think\nabout it in terms of what you might remember\nfrom multivariable chain rules.",
    "start": "4001960",
    "end": "4011090"
  },
  {
    "text": "But the way I introduced\nin lecture three is the gradient sum\nat outward branches.",
    "start": "4011090",
    "end": "4017700"
  },
  {
    "text": "And so what you can think\nabout in a case like this is that you've got\na Wh matrix which",
    "start": "4017700",
    "end": "4026529"
  },
  {
    "text": "has been copied by identity\nto Wh1, Wh2, Wh3, Wh4, et",
    "start": "4026530",
    "end": "4033250"
  },
  {
    "text": "cetera, at each time step. And so since those\nare identity copies,",
    "start": "4033250",
    "end": "4039320"
  },
  {
    "text": "they have a partial\nderivative with respect to each other of one.",
    "start": "4039320",
    "end": "4046130"
  },
  {
    "text": "And so then we apply the\nmultivariable chain rule to these copies.",
    "start": "4046130",
    "end": "4053500"
  },
  {
    "text": "And so we've then got an\noutward branching node and you're just\nsumming the gradients",
    "start": "4053500",
    "end": "4060220"
  },
  {
    "text": "to get the total gradient\nof each time for the matrix. ",
    "start": "4060220",
    "end": "4069414"
  },
  {
    "text": "OK.  Yeah. I mean, there's one other trick\nthat's perhaps worth knowing.",
    "start": "4069415",
    "end": "4077819"
  },
  {
    "text": "I mean, if you've got segments\nthat are hundreds long, a common speed up\nis to say, maybe we",
    "start": "4077820",
    "end": "4085200"
  },
  {
    "text": "don't actually have to run back\npropagation for 100 time steps. Maybe we could just run it\nfor 20 time steps and stop,",
    "start": "4085200",
    "end": "4092910"
  },
  {
    "text": "which is referred to as\ntruncated back propagation through time. I mean, in practice, that\ntends to be sufficient.",
    "start": "4092910",
    "end": "4100059"
  },
  {
    "text": "Note in particular, you're\nstill on the forward path updating your hidden state\nusing your full context.",
    "start": "4100060",
    "end": "4107799"
  },
  {
    "text": "But in the back\npropagation, you're just cutting it short\nto speed up training.",
    "start": "4107800",
    "end": "4115370"
  },
  {
    "text": "OK. So just as I did before with\nan n-gram language model,",
    "start": "4115370",
    "end": "4120750"
  },
  {
    "text": "we can use RNN language\nmodel to generate text.",
    "start": "4120750",
    "end": "4125910"
  },
  {
    "text": "And it's pretty\nmuch the same idea, except now rather than just\nusing counts of n-grams,",
    "start": "4125910",
    "end": "4134040"
  },
  {
    "text": "we're using the hidden\nstate of our neural network to give us the input to a\nprobability distribution",
    "start": "4134040",
    "end": "4141710"
  },
  {
    "text": "that we can then sample from. So I can start with the\ninitial hidden state.",
    "start": "4141710",
    "end": "4147500"
  },
  {
    "text": "I can use the start\nof sentence symbol. I mean, the example I had\nbefore, I started immediately",
    "start": "4147500",
    "end": "4154009"
  },
  {
    "text": "with the, hoping that was\nless confusing the first time. But what you should have\nasked is, wait a minute,",
    "start": "4154010",
    "end": "4160799"
  },
  {
    "text": "where did the the come from? So normally what\nwe actually do is use a special start\nof sequence symbol",
    "start": "4160800",
    "end": "4169818"
  },
  {
    "text": "like this angle bracketed\nS. And so we sort of feed it in as a pseudo word, which\nhas a word embedding.",
    "start": "4169819",
    "end": "4178028"
  },
  {
    "text": "And then based on this,\nwe'll be generating first words of the text.",
    "start": "4178029",
    "end": "4183479"
  },
  {
    "text": "So we end up with some\nrepresentation from which we can",
    "start": "4183479",
    "end": "4188700"
  },
  {
    "text": "sample and get the first word. So now we don't have\nany actual text.",
    "start": "4188700",
    "end": "4194290"
  },
  {
    "text": "So what we're going to do\nis take that generated word that we generated and copy\nit down as the next input,",
    "start": "4194290",
    "end": "4202780"
  },
  {
    "text": "and then we're going to run a\nnext stage of neural network, sample from the\nprobability distribution",
    "start": "4202780",
    "end": "4210159"
  },
  {
    "text": "and x word favorite,\ncopy it down as the next word of the\ninput and keep on generating.",
    "start": "4210160",
    "end": "4217360"
  },
  {
    "text": "And so this is referred\nto as a rollout that you're continuing\nto roll the dice",
    "start": "4217360",
    "end": "4222960"
  },
  {
    "text": "and generate forward and\ngenerate a piece of text. ",
    "start": "4222960",
    "end": "4228830"
  },
  {
    "text": "And normally you want\nto stop at some point. And the way we can stop at some\npoint is we can have a second",
    "start": "4228830",
    "end": "4235460"
  },
  {
    "text": "special symbol, the\nangle brackets /S, which says end of your sequence.",
    "start": "4235460",
    "end": "4244050"
  },
  {
    "text": "So we can generate an\nend of sequence symbol and then we can stop.",
    "start": "4244050",
    "end": "4249270"
  },
  {
    "text": "And so using this, we can\ngenerate pieces of text. And essentially, this\nis exactly what's",
    "start": "4249270",
    "end": "4256580"
  },
  {
    "text": "happening if you use something\nlike ChatGPT, that the model is a more complicated model that\nwe've haven't yet gotten to,",
    "start": "4256580",
    "end": "4264350"
  },
  {
    "text": "but it's generating\nthe response to you by doing this kind of process of\ngenerating a word at the time,",
    "start": "4264350",
    "end": "4271500"
  },
  {
    "text": "treating it as an input and\ngenerating the next word and generating this\nsort of rollout.",
    "start": "4271500",
    "end": "4277620"
  },
  {
    "text": "And that's why and it's\ndone probabilistically. So if you do it multiple times,\nyou can get different answers.",
    "start": "4277620",
    "end": "4285260"
  },
  {
    "text": "We haven't yet\ngotten to ChatGPT, but we can have a\nlittle bit of fun. So you can take this simple\nrecurrent neural network",
    "start": "4285260",
    "end": "4293219"
  },
  {
    "text": "that we've just\nbuilt here and you can train it on\nany piece of text and get it to generate stuff.",
    "start": "4293220",
    "end": "4300320"
  },
  {
    "text": "So, for example, I can train\nit on Barack Obama's speeches. So that's a small corpus.",
    "start": "4300320",
    "end": "4306800"
  },
  {
    "text": "He didn't talk that much. I've only got a few\n100,000 words of text. It's not a huge corpus.",
    "start": "4306800",
    "end": "4313260"
  },
  {
    "text": "I'll just show you this and\nthen I can answer the question. But I can generate from\nit and I get something",
    "start": "4313260",
    "end": "4318950"
  },
  {
    "text": "like, the United\nStates will step up to the cost of a new challenges\nof the American people that",
    "start": "4318950",
    "end": "4324590"
  },
  {
    "text": "will share the fact that\nwe created the problem. They were attacked. And so that they have to\nsay that all the tasks",
    "start": "4324590",
    "end": "4330650"
  },
  {
    "text": "of the final days of war, that\nI will not be able to get this done. Yeah, well, maybe\nthat's slightly better",
    "start": "4330650",
    "end": "4338000"
  },
  {
    "text": "than my n-gram language model. It's still not perfect, you\nmight say, but somewhat better, maybe.",
    "start": "4338000",
    "end": "4343760"
  },
  {
    "text": "Did you have a question? Yeah. So it's like creating a\nmodel like a truncated set",
    "start": "4343760",
    "end": "4350540"
  },
  {
    "text": "of the corpus. Does that impose some\nkind of limitation on how much you can\nproduce and still",
    "start": "4350540",
    "end": "4357080"
  },
  {
    "text": "have some coherency and the\nmeaning of the sentence we're creating?",
    "start": "4357080",
    "end": "4363840"
  },
  {
    "text": "So yeah. So I suggest that we're going\nto chunk the text into 100 word",
    "start": "4363840",
    "end": "4369570"
  },
  {
    "text": "units. So that's the limit of the\namount of prior context that we're going to use.",
    "start": "4369570",
    "end": "4374980"
  },
  {
    "text": "So I mean, that's a\nfair amount, 100 words. That's typically\nseveral sentences.",
    "start": "4374980",
    "end": "4380110"
  },
  {
    "text": "But to the extent\nthat you wanted to even more about the\nfurther back context,",
    "start": "4380110",
    "end": "4385810"
  },
  {
    "text": "you wouldn't be able to. And certainly that's\none of the ways in which modern large\nlanguage models are using",
    "start": "4385810",
    "end": "4393599"
  },
  {
    "text": "far bigger contexts than that. They're now using thousands\nof words of prior context. Yeah, absolutely.",
    "start": "4393600",
    "end": "4400239"
  },
  {
    "text": "It's a limit on how\nmuch far back context. So in some sense, actually,\neven though in theory",
    "start": "4400240",
    "end": "4407370"
  },
  {
    "text": "a recurrent neural network can\nfeed in an arbitrary length context, as soon as\nI say practically,",
    "start": "4407370",
    "end": "4413740"
  },
  {
    "text": "we cut it into\nsegments actually that means we are making a\nMarkov assumption again",
    "start": "4413740",
    "end": "4419110"
  },
  {
    "text": "and we're saying the further\nback context doesn't matter. Yeah?",
    "start": "4419110",
    "end": "4424380"
  },
  {
    "text": "OK. A couple more examples. So instead of\nBarack Obama, I can",
    "start": "4424380",
    "end": "4430380"
  },
  {
    "text": "feed in Harry Potter, which is\na somewhat bigger corpus of text actually, and\ngenerate from that.",
    "start": "4430380",
    "end": "4436659"
  },
  {
    "text": "And so I can get, sorry,\nHarry shouted, panicking. I'll leave those brooms\nin London, are they?",
    "start": "4436660",
    "end": "4442960"
  },
  {
    "text": "No idea, said nearly\nheadless Nick, casting low close\nby Cedric carrying the last bit of treacle\ncharms from Harry's shoulder.",
    "start": "4442960",
    "end": "4450510"
  },
  {
    "text": "And to answer him the\ncommon room perched upon it. Four arms held a\nshining knob from when",
    "start": "4450510",
    "end": "4456420"
  },
  {
    "text": "the spider hadn't felt. It\nseemed he reached the teams too. Well, there you are.",
    "start": "4456420",
    "end": "4462570"
  },
  {
    "text": "You can do other things as well,\nso you can train it on recipes",
    "start": "4462570",
    "end": "4468719"
  },
  {
    "text": "and generate a recipe. This one's a recipe I don't\nsuggest you try and cook,",
    "start": "4468720",
    "end": "4475590"
  },
  {
    "text": "but it looks like a recipe\nif you don't look very hard. Chocolate ranch barbecue.",
    "start": "4475590",
    "end": "4483130"
  },
  {
    "text": "Categories: game,\ncasseroles, cookies, cookies. Yields six servings.",
    "start": "4483130",
    "end": "4489610"
  },
  {
    "text": "2 tablespoons of\nParmesan cheese chopped, 1 cup of coconut milk\nand three eggs beaten.",
    "start": "4489610",
    "end": "4497830"
  },
  {
    "text": "Place each pasta. Over layers of lumps. Shape mixture into the moderate\noven and simmer until firm.",
    "start": "4497830",
    "end": "4506980"
  },
  {
    "text": "Serve hot in bodied fresh\nmustard, orange and cheese.",
    "start": "4506980",
    "end": "4512360"
  },
  {
    "text": "Combine the cheese\nand salt together, the dough in a large\nskillet and the ingredients",
    "start": "4512360",
    "end": "4519130"
  },
  {
    "text": "and stir in the\nchocolate and pepper. Yeah, it's not exactly\na very consistent recipe",
    "start": "4519130",
    "end": "4525970"
  },
  {
    "text": "when it comes down to it. It has a language of a\nrecipe, but it's absolute--",
    "start": "4525970",
    "end": "4532360"
  },
  {
    "text": "maybe if I had scaled it\nmore and had a bigger corpus, it would have done a bit better. But it's definitely not using\nthe ingredients there are.",
    "start": "4532360",
    "end": "4540910"
  },
  {
    "text": "Let's see. It's almost time today, so\nmaybe about all I can do is do--",
    "start": "4540910",
    "end": "4549940"
  },
  {
    "text": "I can do one more fun\nexample and then after that-- Yeah, I probably should do that\nbit at the start next time.",
    "start": "4549940",
    "end": "4556810"
  },
  {
    "text": "So as a variant of building\nRNN language models, I mean, so far we've been\nbuilding them over words",
    "start": "4556810",
    "end": "4566210"
  },
  {
    "text": "so our token timesteps over,\nwhich you build it as words.",
    "start": "4566210",
    "end": "4572930"
  },
  {
    "text": "I mean, actually\nyou can use the idea of recurrent neural networks\nover any other size unit",
    "start": "4572930",
    "end": "4578410"
  },
  {
    "text": "and people have used\nthem for other things. So people have used\nthem in bioinformatics for things like DNA, for having\ngene sequencing or protein",
    "start": "4578410",
    "end": "4588940"
  },
  {
    "text": "sequencing or\nanything like that. But even staying with language,\ninstead of building them",
    "start": "4588940",
    "end": "4594580"
  },
  {
    "text": "over words, you can build\nthem over characters so that I'm generating\na letter at a time",
    "start": "4594580",
    "end": "4603130"
  },
  {
    "text": "rather than a word at a time. And so that can\nsometimes be useful",
    "start": "4603130",
    "end": "4608650"
  },
  {
    "text": "because it allows us to generate\nthings that look like words",
    "start": "4608650",
    "end": "4615010"
  },
  {
    "text": "and perhaps have the\nstructure of English words. And so similarly, there are\nother things that you can do.",
    "start": "4615010",
    "end": "4622600"
  },
  {
    "text": "So before I initialized\nthe hidden state,",
    "start": "4622600",
    "end": "4627620"
  },
  {
    "text": "I said, you just have\nan initial hidden state. You can make it\nzeros if you want. Well, sometimes we're going\nto build a contextual RNN",
    "start": "4627620",
    "end": "4637090"
  },
  {
    "text": "where we can initialize the\nhidden state with something else. So in particular, I can\ninitialize the hidden state",
    "start": "4637090",
    "end": "4644980"
  },
  {
    "text": "with the RGB values of a color. And so I can have initialized\nthe hidden state with the color",
    "start": "4644980",
    "end": "4652240"
  },
  {
    "text": "and generate character at a\ntime the name of paint colors.",
    "start": "4652240",
    "end": "4657530"
  },
  {
    "text": "And I can train a model based\non paint companies' catalog",
    "start": "4657530",
    "end": "4663460"
  },
  {
    "text": "of names of colors and\ntheir RGB of their colors. And then I can give it\ndifferent paint colors",
    "start": "4663460",
    "end": "4672010"
  },
  {
    "text": "and it'll come up\nwith names for them. And it actually does\nan excellent job. This one worked really well.",
    "start": "4672010",
    "end": "4677900"
  },
  {
    "text": "Look at this. This one here is ghastly pink,\npower gray, navel tan, bock",
    "start": "4677900",
    "end": "4686290"
  },
  {
    "text": "coe white, horrible\ngray, homestar brown.",
    "start": "4686290",
    "end": "4691590"
  },
  {
    "text": "Now, couldn't you just\nimagine finding all of these in a paint catalog? I mean, some of them are--",
    "start": "4691590",
    "end": "4697480"
  },
  {
    "text": "there's some really good ones\nover here in the bottom right. This color here is dope and then\nthis stoner blue, burble simp,",
    "start": "4697480",
    "end": "4709120"
  },
  {
    "text": "stinky bean and turdly. Now, I think I've got a real\nbusiness opportunity here",
    "start": "4709120",
    "end": "4717040"
  },
  {
    "text": "in the paint company market for\nmy recurrent neural network. OK, I'll stop there\nfor today and do",
    "start": "4717040",
    "end": "4723580"
  },
  {
    "text": "more of the science of\nneural networks next time. ",
    "start": "4723580",
    "end": "4732000"
  }
]