[
  {
    "text": " So today we're\ngoing to be talking",
    "start": "0",
    "end": "6399"
  },
  {
    "text": "about domain adaptation. And we'll get into\nwhat that means, as well as a few different\nalgorithms for doing that.",
    "start": "6400",
    "end": "12440"
  },
  {
    "text": "And the goal for the\nend of the lecture is to understand different\ndomain adaptation methods and when you might use one\nmethod versus another method.",
    "start": "12440",
    "end": "22220"
  },
  {
    "text": "Now, what is domain adaptation? So we've covered a few\ndifferent problem settings",
    "start": "22220",
    "end": "27670"
  },
  {
    "text": "in this course, starting with\nmulti-task learning, where our goal was to\nsolve multiple tasks,",
    "start": "27670",
    "end": "32816"
  },
  {
    "text": "then looking at transfer\nlearning, where we wanted to solve one task after having\npreviously solved some source",
    "start": "32817",
    "end": "39190"
  },
  {
    "text": "task. And then we also looked at\nthe meta-learning problem statement, where our goal\nwas to solve a new task more",
    "start": "39190",
    "end": "44920"
  },
  {
    "text": "quickly or more\nproficiently after solving a set of previous tasks. And today and on\nWednesday, we're",
    "start": "44920",
    "end": "51539"
  },
  {
    "text": "going to be talking\nabout domain adaptation and domain generalization. And these are two\nproblems that end up",
    "start": "51540",
    "end": "57910"
  },
  {
    "text": "looking a lot like transfer\nlearning and meta-learning but are somewhat of a special\ncase of them in some sense.",
    "start": "57910",
    "end": "65510"
  },
  {
    "text": "And so in particular, the\ngoal of domain adaptation is we want to be able to perform\nwell on some target domain",
    "start": "65510",
    "end": "74290"
  },
  {
    "text": "after training on\ndata from a source domain or possibly\nmultiple source domains.",
    "start": "74290",
    "end": "80000"
  },
  {
    "text": "Although in this\nlecture, we'll mostly consider one source domain. Now, this ends up looking almost\nexactly the same as transfer",
    "start": "80000",
    "end": "89630"
  },
  {
    "text": "learning, except a really\nkey assumption that we're going to make is\nthat we're going to assume that we can access\nsome data from the target",
    "start": "89630",
    "end": "97610"
  },
  {
    "text": "domain during training. And so this is referred to\nas transductive learning,",
    "start": "97610",
    "end": "104270"
  },
  {
    "text": "where we have access to\nbasically some test data. And there's a few different\nforms of domain adaptation",
    "start": "104270",
    "end": "111810"
  },
  {
    "text": "which basically correspond\nto different kinds of access to this target domain data.",
    "start": "111810",
    "end": "117200"
  },
  {
    "text": "In the first case,\nwe assume access to unlabeled data\nfrom the target domain or from the target distribution.",
    "start": "117200",
    "end": "125329"
  },
  {
    "text": "In the second case,\nwe would assume access to unlabeled and labeled\ntarget domain data.",
    "start": "125330",
    "end": "130371"
  },
  {
    "text": "Typically, the labeled\ntarget domain data would be much smaller\nthan the unlabeled data that we have access to.",
    "start": "130372",
    "end": "135750"
  },
  {
    "text": "And in the last case,\nwe would assume access to a small amount of\nlabeled target domain data.",
    "start": "135750",
    "end": "142310"
  },
  {
    "text": "Now, we're going to focus the\nmost on unsupervised domain",
    "start": "142310",
    "end": "147739"
  },
  {
    "text": "adaptation. Problems like supervised\ndomain adaptation, you could actually do\npretty well on just",
    "start": "147740",
    "end": "152930"
  },
  {
    "text": "by fine-tuning and with the\ntransfer learning techniques that we talked about before. Whereas unsupervised\ndomain adaptation,",
    "start": "152930",
    "end": "160040"
  },
  {
    "text": "fine-tuning isn't\napplicable to them because you only have unlabeled\ndata from your target domain.",
    "start": "160040",
    "end": "168490"
  },
  {
    "text": "Now, there's a couple\nof different assumptions that we're going to make\nthat are fairly common.",
    "start": "168490",
    "end": "174040"
  },
  {
    "text": "And this is where\nwe're going to see some differences from the\ntransfer learning setup.",
    "start": "174040",
    "end": "179515"
  },
  {
    "text": "The first assumption\nthat we're going to make is that the source and target\ndomain differ only in p of x",
    "start": "179515",
    "end": "187510"
  },
  {
    "text": "or differ only in the\ndomain of the function. So this is one way\nthat you can think",
    "start": "187510",
    "end": "192940"
  },
  {
    "text": "about what a domain means. And as a result of\nthis, this means that the conditional\ndistribution of y given x",
    "start": "192940",
    "end": "199630"
  },
  {
    "text": "is going to be the same for the\nsource domain and the target domain. ",
    "start": "199630",
    "end": "205530"
  },
  {
    "text": "And another\nassumption that we'll consider that's very related\nto this first assumption",
    "start": "205530",
    "end": "211660"
  },
  {
    "text": "is that we're going to\nassume that there exists a single hypothesis or\na single function that can achieve low error\non both the source",
    "start": "211660",
    "end": "217960"
  },
  {
    "text": "domain and the target domain. And so in many\nways, you can think",
    "start": "217960",
    "end": "225480"
  },
  {
    "text": "of a domain as a\nspecial case of a task where when we introduce\nthis notion of a task, we thought of it as basically\ncorresponding to a data",
    "start": "225480",
    "end": "234180"
  },
  {
    "text": "generating distribution\nover x, over y given x, and over the loss,\nand separately a loss",
    "start": "234180",
    "end": "239400"
  },
  {
    "text": "function for that task. And O domain is\ngoing to be something where only p of x differs\nbetween the different domains,",
    "start": "239400",
    "end": "247440"
  },
  {
    "text": "and y given x and\nthe loss function are going to be the\nsame across the domain.",
    "start": "247440",
    "end": "253770"
  },
  {
    "text": "So essentially, a domain is\na special case of a task. And yeah, basically,\nit's going to correspond",
    "start": "253770",
    "end": "259709"
  },
  {
    "text": "to different\ndistributions over x. And by making this\nassumption that it only",
    "start": "259709",
    "end": "267500"
  },
  {
    "text": "differs in x and by making the\nassumption that we could access some unlabeled target\ndomain during a training,",
    "start": "267500",
    "end": "273590"
  },
  {
    "text": "we're going to be able to do\nbetter than approaches that are designed for\nmulti-task learning",
    "start": "273590",
    "end": "280280"
  },
  {
    "text": "or transfer learning because\nwe can assume that p of y given x is staying fixed. ",
    "start": "280280",
    "end": "288180"
  },
  {
    "text": "Now let's look at\na few examples, and this might make this\nnotion of domain adaptation",
    "start": "288180",
    "end": "293220"
  },
  {
    "text": "a little bit more concrete. So one example is maybe our goal\nis to detect or classify tumors",
    "start": "293220",
    "end": "300000"
  },
  {
    "text": "from slides of tissue cells. And we trained a\nclassifier to be",
    "start": "300000",
    "end": "306960"
  },
  {
    "text": "able to detect tumors from\nthese images from one hospital. And now we also want to\ndeploy that same model",
    "start": "306960",
    "end": "314040"
  },
  {
    "text": "in a different hospital. But the other hospital might\nhave different techniques",
    "start": "314040",
    "end": "319199"
  },
  {
    "text": "for collecting these\nimages, or maybe they have different\ndemographics of patients.",
    "start": "319200",
    "end": "324370"
  },
  {
    "text": "And as a result,\nthe images might look a little bit different\nbetween the source hospital",
    "start": "324370",
    "end": "330300"
  },
  {
    "text": "and the target hospital. Now there still exists-- doctors can still look at these\nimages and figure out whether--",
    "start": "330300",
    "end": "339300"
  },
  {
    "text": "figure out whether there's\na tumor and so forth. And so there still\nexist a single function that can predict whether or not\nthere's a tumor from the image.",
    "start": "339300",
    "end": "348120"
  },
  {
    "text": "But there's a distribution\nshift between these two different domains. And in some ways,\nyou can-- well,",
    "start": "348120",
    "end": "355050"
  },
  {
    "text": "you can sort of think of\nthese as different tasks because you're basically\nstill doing the same task. It's only p of x that's changed.",
    "start": "355050",
    "end": "361050"
  },
  {
    "text": "We'll refer to these\nas different domains. As another example,\nmaybe we want",
    "start": "361050",
    "end": "367110"
  },
  {
    "text": "to be able to classify\nhow land is being used. And you trained a really\ngood model that works well",
    "start": "367110",
    "end": "373830"
  },
  {
    "text": "in North America,\nand you want to be able to deploy that model on\nanother continent such as South",
    "start": "373830",
    "end": "379170"
  },
  {
    "text": "America. The appearances of\nbuildings, of plants in that region,\nweather conditions,",
    "start": "379170",
    "end": "384900"
  },
  {
    "text": "pollution may be different\nbetween the source region and the target region. And so again, this is\nreally the same task,",
    "start": "384900",
    "end": "390660"
  },
  {
    "text": "but you have a different\ndistribution over the images. And so p of x is changing. ",
    "start": "390660",
    "end": "397500"
  },
  {
    "text": "And then there's one\nnonimage example. Maybe we want to classify\nor generate text.",
    "start": "397500",
    "end": "402930"
  },
  {
    "text": "And we trained a\nmodel on Wikipedia, and then we want to deploy\nour model on papers on arXiv",
    "start": "402930",
    "end": "409140"
  },
  {
    "text": "or on PubMed. Because of the\ndiffering vocabulary use and different\nsentence structure,",
    "start": "409140",
    "end": "416820"
  },
  {
    "text": "a model trained on\nthe source domain may not translate\nwell or transfer well to the target domain.",
    "start": "416820",
    "end": "422790"
  },
  {
    "text": "And our goal is to be able to\nbasically use unlabeled data from the target domain in\norder to improve performance",
    "start": "422790",
    "end": "431190"
  },
  {
    "text": "on that target domain.  And so here are\nbasically three examples,",
    "start": "431190",
    "end": "438000"
  },
  {
    "text": "but domains could also\nbe different people or different users. It could correspond to\ndifferent points in time",
    "start": "438000",
    "end": "445220"
  },
  {
    "text": "or different\ninstitutions like data from different schools,\ndifferent companies, or different universities.",
    "start": "445220",
    "end": "450430"
  },
  {
    "text": "Yeah? [INAUDIBLE] does\nit mean that there exists a model that does well\nin both the source and target.",
    "start": "450430",
    "end": "457100"
  },
  {
    "text": "Yeah. So there exists some\nfunction, f of x, that achieves a low error\non both the source data set",
    "start": "457100",
    "end": "464010"
  },
  {
    "text": "and the target data set. Yeah? [INAUDIBLE]",
    "start": "464010",
    "end": "471710"
  },
  {
    "text": "Yeah. So the key difference-- or one\nkey difference between domain adaptation and a typical\ntransfer learning problem",
    "start": "471710",
    "end": "476740"
  },
  {
    "text": "is that we have access to target\ndomain data during training. And if we kind of revisit these\nassumptions, this means that--",
    "start": "476740",
    "end": "484730"
  },
  {
    "text": "for example, if we want to\ntranslate our text classifier to arXiv, we might have access--",
    "start": "484730",
    "end": "492259"
  },
  {
    "text": "we have access to unlabeled\ndata on arXiv already. And so this is going\nto assume that we",
    "start": "492260",
    "end": "498550"
  },
  {
    "text": "don't have to kind of directly\ndeploy our model immediately. We can take that\nunlabeled data from arXiv, incorporate that with\nthe training data",
    "start": "498550",
    "end": "504910"
  },
  {
    "text": "from Wikipedia that\nmight be labeled, and kind of train\na model that we",
    "start": "504910",
    "end": "510010"
  },
  {
    "text": "think should be better\nat arXiv in comparison to if we had only\ntrained on Wikipedia and deployed it directly.",
    "start": "510010",
    "end": "516979"
  },
  {
    "text": "And so there are going to\nbe some scenarios where it's this assumption is\nunrealistic, but there's also",
    "start": "516980",
    "end": "522520"
  },
  {
    "text": "going to be a number of\nscenarios where you do already have access to your unlabeled\ndata, such as on arXiv.",
    "start": "522520",
    "end": "529030"
  },
  {
    "text": "Or maybe when you are trying to\ntranslate it to a new hospital, you do already have\ndata from that hospital, and you don't need to\ndeploy it immediately",
    "start": "529030",
    "end": "535840"
  },
  {
    "text": "without any additional training. Yeah? Assume we can access\nand label the data",
    "start": "535840",
    "end": "543840"
  },
  {
    "text": "on the target [INAUDIBLE]\non the target domain?",
    "start": "543840",
    "end": "550150"
  },
  {
    "text": "Yeah. So if you have access to a\nlot of labelled data from the target domain, then you\nprobably don't need things like",
    "start": "550150",
    "end": "557010"
  },
  {
    "text": "domain adaptation. You don't necessarily need\nto use any of the source data because you can just train\ndirectly on your target data.",
    "start": "557010",
    "end": "563760"
  },
  {
    "text": "Whereas if you only have a\nsmall amount of label data or you have a small\namount of label data,",
    "start": "563760",
    "end": "568950"
  },
  {
    "text": "a lot of unlabeled data, or if\nyou only have unlabeled data, that's where things like domain\nadaptation can be very helpful.",
    "start": "568950",
    "end": "575250"
  },
  {
    "text": " And then again, if\nwe kind of revisit",
    "start": "575250",
    "end": "581540"
  },
  {
    "text": "this hypothesis of\ntheir existing-- or this assumption of there\nexisting a single hypothesis, I think that this\nsort of assumption",
    "start": "581540",
    "end": "587900"
  },
  {
    "text": "is very realistic in all\nof these scenarios, where",
    "start": "587900",
    "end": "593270"
  },
  {
    "text": "you can probably kind\nof just look at an image and figure out the\nland use if you're kind of an expert on what\ndifferent buildings look like",
    "start": "593270",
    "end": "602570"
  },
  {
    "text": "and likewise be able to predict\ncertain things from text",
    "start": "602570",
    "end": "609030"
  },
  {
    "text": "regardless of where\nit's coming from. ",
    "start": "609030",
    "end": "615102"
  },
  {
    "text": "OK. Any questions on the domain\nadaptation problem statement before we move on?",
    "start": "615102",
    "end": "622190"
  },
  {
    "text": "Yeah? [INAUDIBLE] And that\nfunction [INAUDIBLE]",
    "start": "622190",
    "end": "634470"
  },
  {
    "text": "Does it have any\nspecial meaning? Because you can already\ncombine multiple xs into one [INAUDIBLE].",
    "start": "634470",
    "end": "640690"
  },
  {
    "text": "Yeah. So maybe one thing that\nyou're pointing out here is that you can\ntrivially say that there exists a single\nhypothesis, given",
    "start": "640690",
    "end": "651450"
  },
  {
    "text": "the identity of the domain.  So yeah.",
    "start": "651450",
    "end": "656840"
  },
  {
    "text": "So if you can identify\nthe domain x from-- or the domain d\nfrom x and assuming",
    "start": "656840",
    "end": "662930"
  },
  {
    "text": "that you can get low error on\neach of the individual domains, then that assumption\ntrivially holds.",
    "start": "662930",
    "end": "668840"
  },
  {
    "text": "There are often\nscenarios where it is difficult to identify\nthe domain from the input.",
    "start": "668840",
    "end": "674600"
  },
  {
    "text": "Certainly, images are something\nwhere it may not be that hard. Although if you take an image\nfrom anywhere on the globe,",
    "start": "674600",
    "end": "682933"
  },
  {
    "text": "I'm not sure if a\nclassifier would be able to tell exactly\nwhat continent it's from. So there's certainly\nscenarios where",
    "start": "682933",
    "end": "688610"
  },
  {
    "text": "you can't predict d from x. And the stronger\nassumption beyond this one",
    "start": "688610",
    "end": "696493"
  },
  {
    "text": "is that basically\nthe distribution over y given x for\nyour source data",
    "start": "696494",
    "end": "703009"
  },
  {
    "text": "is equal to the\ndistribution over y given x for your target data. This is a stronger assumption.",
    "start": "703010",
    "end": "709430"
  },
  {
    "text": "And most of the approaches\nthat we'll be looking at today are also going to\nmake this assumption.",
    "start": "709430",
    "end": "714438"
  },
  {
    "start": "714438",
    "end": "720180"
  },
  {
    "text": "Cool. So we're going to look at\nthree different algorithms for unsupervised\ndomain adaptation.",
    "start": "720180",
    "end": "728230"
  },
  {
    "text": "And I guess maybe before\nI touch on the algorithms, I guess one other thing that\nmay be worth touching on is--",
    "start": "728230",
    "end": "737190"
  },
  {
    "text": "on Wednesday, we're\ngoing to also consider a variant of this problem where\nwe assume that we have data",
    "start": "737190",
    "end": "742890"
  },
  {
    "text": "from multiple different\ndomains during training. And our goal is to generalize\nzero-shot to a new domain.",
    "start": "742890",
    "end": "749550"
  },
  {
    "text": "That's going to--\nand basically, a lot of the algorithms that\nwe're going to build up today are also going to be\napplicable in that domain",
    "start": "749550",
    "end": "756720"
  },
  {
    "text": "generalization problem\nsetting as well. ",
    "start": "756720",
    "end": "762160"
  },
  {
    "text": "Cool. So now let's first start\nby considering a toy domain",
    "start": "762160",
    "end": "767640"
  },
  {
    "text": "adaptation problem. And in particular,\nour source domain",
    "start": "767640",
    "end": "773550"
  },
  {
    "text": "will be this blue distribution,\nand our target domain will be the green distribution.",
    "start": "773550",
    "end": "779460"
  },
  {
    "text": "And it's just going to be a\nbinary classification problem. This could correspond\nto something",
    "start": "779460",
    "end": "785250"
  },
  {
    "text": "like sample\nselection bias, where when you collected a data\nset, it wasn't actually",
    "start": "785250",
    "end": "790410"
  },
  {
    "text": "representative of\nthe true population.",
    "start": "790410",
    "end": "795670"
  },
  {
    "text": "And if we have a binary\nclassification problem, maybe it looks\nsomething like this, where these are examples drawn\nfrom our source distribution.",
    "start": "795670",
    "end": "805950"
  },
  {
    "text": "And the bulk of the samples\nare kind of in the region where we have high\nprobability under p of s.",
    "start": "805950",
    "end": "815960"
  },
  {
    "text": "And unfortunately, one thing\nthat might come up here is if you have a--\nif a lot of your data",
    "start": "815960",
    "end": "821000"
  },
  {
    "text": "is coming from the\nhigh probability regions of that space,\nthen the classifier trained",
    "start": "821000",
    "end": "826550"
  },
  {
    "text": "on that source data set may\npay very little attention to examples that have low\nprobability under the source",
    "start": "826550",
    "end": "832670"
  },
  {
    "text": "data set and high probability\nunder the target data set. So these data points right here.",
    "start": "832670",
    "end": "838230"
  },
  {
    "text": "And if it pays very little\nattention to those two data points, then it may just\nlearn a very simple classifier",
    "start": "838230",
    "end": "845690"
  },
  {
    "text": "that just learns like basically\na single linear decision boundary that's kind of able to\naccurately classify almost all",
    "start": "845690",
    "end": "853460"
  },
  {
    "text": "of the data points. And unfortunately,\nif it does this, then that classifier,\nif it was actually",
    "start": "853460",
    "end": "859700"
  },
  {
    "text": "evaluated on data from\nthe target distribution, would actually\nperform quite poorly",
    "start": "859700",
    "end": "865430"
  },
  {
    "text": "when evaluated on that\nnew target distribution. ",
    "start": "865430",
    "end": "871110"
  },
  {
    "text": "And so there's a\nquestion of, well, OK, if we have labeled data\nfrom the source data",
    "start": "871110",
    "end": "876330"
  },
  {
    "text": "set and unlabeled data\nfrom the target data set, is there anything that\nwe can do to learn a classifier that does well on\nthe green target distribution?",
    "start": "876330",
    "end": "886830"
  },
  {
    "text": "Does anyone have any\nthoughts on what we might do in this particular example? ",
    "start": "886830",
    "end": "897720"
  },
  {
    "text": "Yeah? [INAUDIBLE] three examples\nfrom the source distribution",
    "start": "897720",
    "end": "903149"
  },
  {
    "text": "[INAUDIBLE] the targeted\ndistribution examples?",
    "start": "903150",
    "end": "910610"
  },
  {
    "text": "So you're suggesting\nthat we shift the source examples in some way?",
    "start": "910610",
    "end": "916350"
  },
  {
    "text": "So how exactly\nwould you do that? So you've got like your target\nexamples which are unlabeled",
    "start": "916350",
    "end": "923890"
  },
  {
    "text": "but no [INAUDIBLE]. And you've also got\nyour source examples.",
    "start": "923890",
    "end": "929894"
  },
  {
    "text": " Well, I guess [INAUDIBLE]",
    "start": "929895",
    "end": "942000"
  },
  {
    "text": "Estimate the mean of the-- so we know where the\ntarget examples are. We know that they're kind\nof on the left-hand side.",
    "start": "942000",
    "end": "948459"
  },
  {
    "text": "But we don't know\nwhat their labels are. And we have access to labeled\nsource examples as well. ",
    "start": "948460",
    "end": "956280"
  },
  {
    "text": "Yeah?  [INAUDIBLE] what you could\ndo is shift the weights",
    "start": "956280",
    "end": "963020"
  },
  {
    "text": "of the different examples. So you can do\nimportance sampling or importance free weighting. Yeah. So maybe this is--",
    "start": "963020",
    "end": "968630"
  },
  {
    "text": "maybe this is what\nyou're describing. I'm not sure. You can basically kind of change\nthe weight of the examples",
    "start": "968630",
    "end": "975680"
  },
  {
    "text": "such that you kind of\nupweight the examples that have high probability under\nthe target distribution,",
    "start": "975680",
    "end": "980810"
  },
  {
    "text": "especially if they have low\nprobability under the source distribution. And so kind of\nthe intuition here",
    "start": "980810",
    "end": "987530"
  },
  {
    "text": "is if we upweight\nthese data points and downweight some of\nthe other data points, then we should learn\na classifier that",
    "start": "987530",
    "end": "994550"
  },
  {
    "text": "can more accurately get a\ndecision boundary that's more accurate for the\ntarget distribution.",
    "start": "994550",
    "end": "1004199"
  },
  {
    "text": "Cool. So now there's a\nquestion of this is kind of-- this\nintuitively, I think,",
    "start": "1004200",
    "end": "1010020"
  },
  {
    "text": "makes sense kind of visually. You can see that if you\nupweight those examples, you'll probably learn\na classifier that's",
    "start": "1010020",
    "end": "1015330"
  },
  {
    "text": "more accurate on the\ntarget distribution. But there's also a question\nof why this makes sense",
    "start": "1015330",
    "end": "1020430"
  },
  {
    "text": "mathematically. And so our goal is to kind\nof minimize how well we",
    "start": "1020430",
    "end": "1034500"
  },
  {
    "text": "do on the target distribution. This should be x. So our goal is like\nto minimize f of x--",
    "start": "1034500",
    "end": "1044720"
  },
  {
    "text": "the loss function\nfor our function on the target\ndistribution with respect",
    "start": "1044720",
    "end": "1052130"
  },
  {
    "text": "to our model parameters. Now, of course, we can't\nsample from this distribution directly.",
    "start": "1052130",
    "end": "1058140"
  },
  {
    "text": "But we can sample from\nour source distribution. And so if we just did standard\nempirical risk minimization,",
    "start": "1058140",
    "end": "1065909"
  },
  {
    "text": "that would correspond to\nminimizing with respect to our source distribution\nof the loss function.",
    "start": "1065910",
    "end": "1076590"
  },
  {
    "text": "Now, we know that we can sample\nfrom the source distribution, but our goal is to minimize\nthis term right here.",
    "start": "1076590",
    "end": "1085460"
  },
  {
    "text": "And what we can do is\nif we kind of expand out this expectation as an\nintegral over the target",
    "start": "1085460",
    "end": "1095290"
  },
  {
    "text": "distribution of\nour loss function,",
    "start": "1095290",
    "end": "1102880"
  },
  {
    "text": "then we know that the-- we want to be able to\nsample from p of s.",
    "start": "1102880",
    "end": "1109200"
  },
  {
    "text": "And so we can do a\nsomewhat similar trick to what we did in the\nvariational inference lecture and basically multiply\nthis by p source of x,",
    "start": "1109200",
    "end": "1122040"
  },
  {
    "text": "y divided by p source of x, y. This is just equal to 1.",
    "start": "1122040",
    "end": "1127350"
  },
  {
    "text": "And so we're just\nmultiplying everything by 1. And then we can basically\nincorporate this",
    "start": "1127350",
    "end": "1135470"
  },
  {
    "text": "into the expectation rather\nthan this in the expectation. And so if we do this,\nwe see that this is equal to the expectation\nof under the source",
    "start": "1135470",
    "end": "1145309"
  },
  {
    "text": "distribution, which is\nwhat we have access to",
    "start": "1145310",
    "end": "1153560"
  },
  {
    "text": "of p target of x, y divided by\np source of x, y and our loss",
    "start": "1153560",
    "end": "1162240"
  },
  {
    "text": "function.  And so this is pretty nice\nbecause this means that now we",
    "start": "1162240",
    "end": "1168370"
  },
  {
    "text": "can sample from our\nsource distribution, evaluate our loss on\nthose data points.",
    "start": "1168370",
    "end": "1175169"
  },
  {
    "text": "But we're basically going\nto be weighting our loss by this ratio right here.",
    "start": "1175170",
    "end": "1180820"
  },
  {
    "text": "And so the data points\nthat have a high target value and a low\nsource likelihood",
    "start": "1180820",
    "end": "1188475"
  },
  {
    "text": "are going to be upweighted. And if they have\na high likelihood under the source and a low\nlikelihood under the target,",
    "start": "1188475",
    "end": "1194970"
  },
  {
    "text": "then they're going\nto be downweighted. There's one more\nstep here, which is that if we assume\nthis equality, the kind",
    "start": "1194970",
    "end": "1204863"
  },
  {
    "text": "of-- the assumption\nthat y given x is the same for the\ntwo distributions, then we can now replace\nthis with just p of x.",
    "start": "1204863",
    "end": "1214420"
  },
  {
    "text": "And the reason why we can do\nthat is we know that p of x, y equals p of x\ntimes p of y given x.",
    "start": "1214420",
    "end": "1222310"
  },
  {
    "text": "And if this is the\nsame, then this is going to cancel\nout between the two, and we'll be left with p target\nof x divided by p source of x.",
    "start": "1222310",
    "end": "1231010"
  },
  {
    "start": "1231010",
    "end": "1236710"
  },
  {
    "text": "Cool. And so this is just\nwritten out on the slides. Our goal is to basically be\nable to minimize the risk",
    "start": "1236710",
    "end": "1244960"
  },
  {
    "text": "on the target distribution. We can write out that\nequation as an integral",
    "start": "1244960",
    "end": "1251710"
  },
  {
    "text": "and then multiply\nthe inside by 1, which is the source divided\nby the source likelihood,",
    "start": "1251710",
    "end": "1257530"
  },
  {
    "text": "and then evaluate that. And then that ends up\nbeing the expectation under the source data set of\nthis kind of importance weight",
    "start": "1257530",
    "end": "1266380"
  },
  {
    "text": "here. Yeah. What's the intuition behind\nthis very strong assumption?",
    "start": "1266380",
    "end": "1273400"
  },
  {
    "text": "Oh, the intuition behind this. ",
    "start": "1273400",
    "end": "1280190"
  },
  {
    "text": "I guess there-- I don't think\nthat there's any particular--",
    "start": "1280190",
    "end": "1288490"
  },
  {
    "text": "I guess there are some\nproblems that where this is going to hold, and\nthere are some problems where this is not going to hold.",
    "start": "1288490",
    "end": "1293530"
  },
  {
    "text": "[INAUDIBLE] would\nfirst [INAUDIBLE]",
    "start": "1293530",
    "end": "1301122"
  },
  {
    "text": "probability be the same step?  So it really depends\non the problem.",
    "start": "1301122",
    "end": "1307540"
  },
  {
    "text": " If for example, you are--",
    "start": "1307540",
    "end": "1317409"
  },
  {
    "text": "I don't know-- polling\npeople about their political preferences and you kind of\ndon't sample very uniformly--",
    "start": "1317410",
    "end": "1326230"
  },
  {
    "text": "you kind of get\na biased sample-- then typically, if you have\ngood features of a person like",
    "start": "1326230",
    "end": "1332860"
  },
  {
    "text": "they're-- regardless of the\nperson that you sampled,",
    "start": "1332860",
    "end": "1338350"
  },
  {
    "text": "this should stay fixed. But it may be that you\njust kind of sampled.",
    "start": "1338350",
    "end": "1344260"
  },
  {
    "text": "You didn't kind of uniformly\nsample from the population you wanted to sample from.",
    "start": "1344260",
    "end": "1349279"
  },
  {
    "text": "And so I guess in the problem\nthat we're looking at here, this is really-- we'll talk about the kind of\nthe limitations of this approach",
    "start": "1349280",
    "end": "1356080"
  },
  {
    "text": "in a minute, but this is\nreally focusing on sort of sample selection bias. But there's also lots\nof other examples",
    "start": "1356080",
    "end": "1361240"
  },
  {
    "text": "where this will also hold like\nkind of in the medical imaging scenario, where you should\nbe able to recognize",
    "start": "1361240",
    "end": "1370809"
  },
  {
    "text": "a tumor from the image. And you won't ever\nhave a scenario where an image from one\nhospital has a tumor",
    "start": "1370810",
    "end": "1380030"
  },
  {
    "text": "and that same exact\nimage would be generated but doesn't have a tumor\nin another hospital.",
    "start": "1380030",
    "end": "1385300"
  },
  {
    "text": "And so there are\na lot of scenarios where this can hold if\nyou're kind of collecting data in certain scenarios.",
    "start": "1385300",
    "end": "1390940"
  },
  {
    "text": "And then there are also\nmulti-task problems where it won't hold as well. And you want to-- these\nsorts of approaches",
    "start": "1390940",
    "end": "1396910"
  },
  {
    "text": "are only applicable or primarily\napplicable when that does hold. ",
    "start": "1396910",
    "end": "1404770"
  },
  {
    "text": "Cool. So this equation agrees\nwith the intuition",
    "start": "1404770",
    "end": "1410450"
  },
  {
    "text": "that we saw on the\nprevious slide, where we're going to be upweighting\nexamples with a high likelihood",
    "start": "1410450",
    "end": "1416450"
  },
  {
    "text": "under the target distribution\nand a low likelihood under the source distribution.",
    "start": "1416450",
    "end": "1422380"
  },
  {
    "text": "Now a key question that comes\nup is, how do we actually compute this weight right here?",
    "start": "1422380",
    "end": "1428019"
  },
  {
    "text": "How do we compute the\nimportance weight? One thing that we\ncould do is just estimate these likelihoods,\nlike fit a generative model",
    "start": "1428020",
    "end": "1437679"
  },
  {
    "text": "to estimate the\nlikelihood of example under the target\ndistribution and separately fit a generative model to\nour source distribution.",
    "start": "1437680",
    "end": "1447070"
  },
  {
    "text": "But unfortunately,\nit can be fairly difficult to estimate these\nlikelihoods accurately",
    "start": "1447070",
    "end": "1452140"
  },
  {
    "text": "and in a way that is calibrated\nand consistent with-- kind of consistent across the\ntarget and source distribution.",
    "start": "1452140",
    "end": "1460390"
  },
  {
    "text": "And so we'd like to be able\nto estimate these weights without training a density\nmodel on our target distribution",
    "start": "1460390",
    "end": "1468280"
  },
  {
    "text": "and our source distribution.  And so it turns out\nthere's actually",
    "start": "1468280",
    "end": "1473770"
  },
  {
    "text": "a way that you can do that. And really the key reason\nwhy we can get away",
    "start": "1473770",
    "end": "1480440"
  },
  {
    "text": "with doing that is\nour goal isn't just to estimate the target,\nthe kind of the density. But our goal is specifically\nto estimate this ratio.",
    "start": "1480440",
    "end": "1489090"
  },
  {
    "text": "And the reason why we can-- if we kind of--",
    "start": "1489090",
    "end": "1494885"
  },
  {
    "text": "there's a way that we can\nbasically manipulate this ratio to get something that looks\nmore like a discriminative,",
    "start": "1494885",
    "end": "1500465"
  },
  {
    "text": "something that we can estimate\nwith discriminative models rather than with\ngenerative models. ",
    "start": "1500465",
    "end": "1506809"
  },
  {
    "text": "And so specifically,\nfirst, we can write out that the likelihood of x under\nthe target distribution, this",
    "start": "1506810",
    "end": "1519710"
  },
  {
    "text": "is equal to the likelihood\nof x given that the domain is",
    "start": "1519710",
    "end": "1526309"
  },
  {
    "text": "equal to the target domain. And writing it out\nthis way is going to just make things a\nlittle bit more clear.",
    "start": "1526310",
    "end": "1533360"
  },
  {
    "text": "And we can use Bayes' rule to\nrewrite this as the probability",
    "start": "1533360",
    "end": "1540950"
  },
  {
    "text": "that the domain is equal to\nthe target given x times p",
    "start": "1540950",
    "end": "1547789"
  },
  {
    "text": "of x divided by the probability\nthat the domain is the target",
    "start": "1547790",
    "end": "1553040"
  },
  {
    "text": "domain. And we can likewise kind of\nwrite out the same exact thing",
    "start": "1553040",
    "end": "1560070"
  },
  {
    "text": "if the domain is\nthe source domain. And now our goal\nis we want to be",
    "start": "1560070",
    "end": "1565960"
  },
  {
    "text": "able to estimate the probability\nthat an example is p of x given",
    "start": "1565960",
    "end": "1573730"
  },
  {
    "text": "the domain is target divided\nby the probability of x given",
    "start": "1573730",
    "end": "1580590"
  },
  {
    "text": "that the domain is equal\nto the source domain. This is our importance weight.",
    "start": "1580590",
    "end": "1585670"
  },
  {
    "text": "And so if we want to do\nthis, we can basically just take this\nequation for the target and divide it by this\nequation for the source.",
    "start": "1585670",
    "end": "1593190"
  },
  {
    "text": "And if we do that-- first, we'll divide\nthe first term. So this is going to be\np of d equals target",
    "start": "1593190",
    "end": "1601320"
  },
  {
    "text": "given x divided by p of\nd equals source given x.",
    "start": "1601320",
    "end": "1607287"
  },
  {
    "text": "Then we'll divide the\nnext term, which would be p of x divided by p of x. And then we'll divide\nthe last term, which",
    "start": "1607287",
    "end": "1615260"
  },
  {
    "text": "will give us p of domain\nequals source and p of domain",
    "start": "1615260",
    "end": "1622280"
  },
  {
    "text": "equals target.  And this cancels out, of course.",
    "start": "1622280",
    "end": "1629779"
  },
  {
    "text": "This is just a constant. And so multiplying our\nloss function by a constant",
    "start": "1629780",
    "end": "1635860"
  },
  {
    "text": "doesn't really change anything. It doesn't depend\non theta in any way.",
    "start": "1635860",
    "end": "1641190"
  },
  {
    "text": "And this term right here, we've\nnow kind of basically flipped",
    "start": "1641190",
    "end": "1647149"
  },
  {
    "text": "x given target to now be target\ngiven x and source given x. And this is something\nthat we can estimate",
    "start": "1647150",
    "end": "1653600"
  },
  {
    "text": "with the discriminative model. And in particular, we can\njust train a classifier",
    "start": "1653600",
    "end": "1658700"
  },
  {
    "text": "to be able to predict if an\nexample came from the target domain or the source domain.",
    "start": "1658700",
    "end": "1664830"
  },
  {
    "text": "So we'll basically train a\nclassifier to take as input x. And then it could be whatever\nsort of function you want.",
    "start": "1664830",
    "end": "1672832"
  },
  {
    "text": "And it's going to tell\nyou whether or not it thinks it came from the\ntarget domain or the source domain.",
    "start": "1672832",
    "end": "1679460"
  },
  {
    "text": "And this will basically give\nyou an estimate of p of target",
    "start": "1679460",
    "end": "1685820"
  },
  {
    "text": "given x, which is going\nto be equal to 1 minus p of source given x.",
    "start": "1685820",
    "end": "1691639"
  },
  {
    "text": "And so then you can\nuse this classifier to estimate the importance\nweight right here.",
    "start": "1691640",
    "end": "1697190"
  },
  {
    "text": " And so specifically,\nwhat this looks like is",
    "start": "1697190",
    "end": "1704120"
  },
  {
    "text": "we first used Bayes' rule to\nkind of write out what p of x",
    "start": "1704120",
    "end": "1710830"
  },
  {
    "text": "given the domain is equal to. And then once we divide\nour importance weight out,",
    "start": "1710830",
    "end": "1718020"
  },
  {
    "text": "we get this-- we get the product of a\nconstant and this term",
    "start": "1718020",
    "end": "1723830"
  },
  {
    "text": "right here which we can estimate\nwith a binary classifier. Oh, so then specifically\nif you kind of walk",
    "start": "1723830",
    "end": "1730305"
  },
  {
    "text": "through the\nalgorithm and what it looks like, the first thing\nthat we'll do is we'll train this binary classifier.",
    "start": "1730305",
    "end": "1737140"
  },
  {
    "text": "And its goal will be to\nestimate if an example comes from the target distribution\nor the source distribution.",
    "start": "1737140",
    "end": "1744380"
  },
  {
    "text": "And this is going to operate\nonly on kind of the input. It's not going to\nlook at the label. ",
    "start": "1744380",
    "end": "1752000"
  },
  {
    "text": "Then we will resample\nor reweight the data according to the kind\nof importance weight",
    "start": "1752000",
    "end": "1760310"
  },
  {
    "text": "that we derive right\nhere, which is just-- you can think of it as kind\nof-- if your classifier is",
    "start": "1760310",
    "end": "1768140"
  },
  {
    "text": "estimating the\nprobability of a source that the x came from\nthe source distribution, then it'll just be 1 minus\nthat classifier value divided",
    "start": "1768140",
    "end": "1775010"
  },
  {
    "text": "by the classifier probability. And then once we either\nreweight or resample our data,",
    "start": "1775010",
    "end": "1782570"
  },
  {
    "text": "then we'll optimize\nour loss function on the reweighted\nor resampled data. ",
    "start": "1782570",
    "end": "1794190"
  },
  {
    "text": "Yeah? It makes sense when there's a\nwell-defined source and target",
    "start": "1794190",
    "end": "1800580"
  },
  {
    "text": "distribution. But maybe North\nand South America don't turn out to be\nthe best two categories.",
    "start": "1800580",
    "end": "1807900"
  },
  {
    "text": "Maybe there's some--\nmaybe it's nice to think about it in terms of just\nweighted variables that apply.",
    "start": "1807900",
    "end": "1813760"
  },
  {
    "text": "And I'm wondering if you're-- are you going to get to\ngeneralizing to that scenario? Yeah. So the question is--",
    "start": "1813760",
    "end": "1820230"
  },
  {
    "text": "we've been talking about source\nand target distributions, and it may be that we don't\nhave these kind of too clear-cut",
    "start": "1820230",
    "end": "1828990"
  },
  {
    "text": "things, like two continents-- it may be that there are some\ncountries in North and South America that are actually\nmore like each other than--",
    "start": "1828990",
    "end": "1836360"
  },
  {
    "text": "because they're actually very\nclose to one another than they are in different domains. And so perhaps we\ncould generalize this",
    "start": "1836360",
    "end": "1842310"
  },
  {
    "text": "by thinking about things\nlike continuously, variables and so forth. ",
    "start": "1842310",
    "end": "1848500"
  },
  {
    "text": "So it's a good question. We're not going to generalize\nit in this lecture.",
    "start": "1848500",
    "end": "1858170"
  },
  {
    "text": "One thing that I'll\nmention, though, is that one of the reasons\nwhy we are defining this",
    "start": "1858170",
    "end": "1863440"
  },
  {
    "text": "in a very clear-cut way\nis that we're really defining our source\ndistribution to be our training",
    "start": "1863440",
    "end": "1869230"
  },
  {
    "text": "data and our target\ndistribution to be the distribution\nin our test data. And so it could be that your\nsource distribution looks",
    "start": "1869230",
    "end": "1878440"
  },
  {
    "text": "something like this, for\nexample, and your target distribution--",
    "start": "1878440",
    "end": "1884860"
  },
  {
    "text": "maybe your target distribution\nlooks something like this, for example. And it seems a little bit\nweird to call these two",
    "start": "1884860",
    "end": "1890977"
  },
  {
    "text": "different domains because\nthey actually have a lot of overlap right here. But if you think instead\nof this as basically kind",
    "start": "1890977",
    "end": "1898660"
  },
  {
    "text": "of our training distribution and\nthis as our test distribution-- that's why we're\ngoing to basically",
    "start": "1898660",
    "end": "1904780"
  },
  {
    "text": "define it as so clear-cut. And the algorithms that\nwe're talking about here,",
    "start": "1904780",
    "end": "1910330"
  },
  {
    "text": "they can take into account\nthat there is this overlap. And they'll still work\nwell in that scenario. ",
    "start": "1910330",
    "end": "1922475"
  },
  {
    "text": "Cool.  I'm also thinking that they're--",
    "start": "1922475",
    "end": "1928490"
  },
  {
    "text": "I mean, there may\nalso be algorithms that you can derive that\nto try to kind of take into account how close\ntogether two data points are.",
    "start": "1928490",
    "end": "1936897"
  },
  {
    "text": "We've actually been thinking\nabout developing algorithms like that in some\nof our research, but there aren't really that\nmany mainstream algorithms,",
    "start": "1936897",
    "end": "1944840"
  },
  {
    "text": "I think, that take\nthat into account.  Yeah?",
    "start": "1944840",
    "end": "1951100"
  },
  {
    "text": "I want to see more of the\nwork if we could model the-- if we could go [INAUDIBLE]\nand model the--",
    "start": "1951100",
    "end": "1958285"
  },
  {
    "text": "[INAUDIBLE] data set [INAUDIBLE] So the question is,\nhow would this look",
    "start": "1958285",
    "end": "1963520"
  },
  {
    "text": "like if we could estimate\nthe data distribution?",
    "start": "1963520",
    "end": "1969026"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "1969026",
    "end": "1974410"
  },
  {
    "text": "For target, or for source? For both if we\ncould approximate. ",
    "start": "1974410",
    "end": "1983695"
  },
  {
    "text": "So typically, I\nthink that if you can estimate-- if you can\napproximate the densities, then you can just\nuse that directly",
    "start": "1983695",
    "end": "1989900"
  },
  {
    "text": "rather than training\na classifier. I don't know of any\nway to combine the two.",
    "start": "1989900",
    "end": "1995300"
  },
  {
    "text": "I mean, you could\ncombine the two. If you think that you have\na rough estimate of one and a rough estimate\nof the other,",
    "start": "1995300",
    "end": "2000430"
  },
  {
    "text": "then you could try to\naverage them or combine them in some other way. But I don't know of--",
    "start": "2000430",
    "end": "2006370"
  },
  {
    "text": " typically, you do\none or the other. Which one works better?",
    "start": "2006370",
    "end": "2013660"
  },
  {
    "text": "Typically, this one works. Using a classifier\ntypically works better because getting good\nlikelihood estimates is--",
    "start": "2013660",
    "end": "2020299"
  },
  {
    "text": "especially for high dimensional\ndata is very difficult. Yeah?",
    "start": "2020300",
    "end": "2025820"
  },
  {
    "text": "[INAUDIBLE] trying\nto minimize the loss on the target distribution.",
    "start": "2025820",
    "end": "2031142"
  },
  {
    "text": "But in general, you\nwant to minimize over just p of x, not p of x given\n[INAUDIBLE] on all the samples.",
    "start": "2031142",
    "end": "2039090"
  },
  {
    "text": "But I'm just thinking\nthat even this is OK because you are assuming\nthat p of y given x is the same for both\n[INAUDIBLE] So isn't this a bit",
    "start": "2039090",
    "end": "2047670"
  },
  {
    "text": "of like a problem\nbecause in general, you're only targeting the\ntarget distribution right,",
    "start": "2047670",
    "end": "2054186"
  },
  {
    "text": "in this loss function. And you don't care about the\nperformance on [INAUDIBLE] at all in this formulation.",
    "start": "2054186",
    "end": "2059710"
  },
  {
    "text": "Yeah. So in this formulation,\nwe're really optimizing for how well we do\non the target data distribution.",
    "start": "2059710",
    "end": "2064919"
  },
  {
    "text": "And this is operating\nunder the assumption that the unlabeled\ndata that we have from the target distribution\nis representative",
    "start": "2064920",
    "end": "2071190"
  },
  {
    "text": "of our test data. And if you optimize\nfor this and then evaluate it on the\nsource data distribution",
    "start": "2071190",
    "end": "2078760"
  },
  {
    "text": "and you kind of broke\nthat assumption, then this wouldn't\nwork as well as if you just trained on the source\ndistribution directly.",
    "start": "2078760",
    "end": "2085135"
  },
  {
    "text": "And so this is really\nassuming that you are getting an\naccurate estimate, basically that you really have\nunlabeled data from your target",
    "start": "2085135",
    "end": "2091260"
  },
  {
    "text": "distribution rather than\nsome other distribution. And so you can think\nof this as sort of like optimizing for a\nparticular test distribution",
    "start": "2091260",
    "end": "2100420"
  },
  {
    "text": "and really trying to specialize\nthe model for that test distribution rather than trying\nto learn a general purpose model that will work\nwell for any domain.",
    "start": "2100420",
    "end": "2107573"
  },
  {
    "text": "And some of the things that\nwe'll cover on Wednesday will be actually optimizing\nfor doing well on basically",
    "start": "2107573",
    "end": "2113910"
  },
  {
    "text": "all the domains that\nyou've seen so far, not just the test domain.",
    "start": "2113910",
    "end": "2119520"
  },
  {
    "text": " Cool.",
    "start": "2119520",
    "end": "2124550"
  },
  {
    "text": " Now, one thing I'm\nactually somewhat surprised that hasn't come\nup yet is that this",
    "start": "2124550",
    "end": "2130520"
  },
  {
    "text": "is making a pretty important\nassumption when we optimize for this and generally when\nwe take this approach, which",
    "start": "2130520",
    "end": "2138140"
  },
  {
    "text": "is that if you optimize\nfor this quantity",
    "start": "2138140",
    "end": "2143569"
  },
  {
    "text": "and there's something\nthat has like zero likelihood under your\nsource distribution,",
    "start": "2143570",
    "end": "2150260"
  },
  {
    "text": "that's going to be a\nlittle bit of a problem. You won't be able to actually\noptimize this effectively.",
    "start": "2150260",
    "end": "2156714"
  },
  {
    "text": "And essentially, the\nassumption that this is making is that your source\ndistribution really needs to cover the target\ndistribution because if it",
    "start": "2156715",
    "end": "2163138"
  },
  {
    "text": "doesn't cover the target\ndistribution, especially the parts with high\nlikelihood, then you won't be able to upweight\nthe parts with high likelihood.",
    "start": "2163138",
    "end": "2171470"
  },
  {
    "text": "And so more formally, if the\nlikelihood under the target distribution is\nnonzero for an input,",
    "start": "2171470",
    "end": "2178670"
  },
  {
    "text": "then you need the likelihood\nunder the source distribution to also be nonzero.",
    "start": "2178670",
    "end": "2183859"
  },
  {
    "text": "You need to have kind of\ndata from that region. And if you can satisfy\nthis assumption,",
    "start": "2183860",
    "end": "2189180"
  },
  {
    "text": "then you can actually-- there's actually some kind\nof theoretical guarantees",
    "start": "2189180",
    "end": "2194920"
  },
  {
    "text": "that you can show for\nthis kind of method.  And so there are going to\nbe some scenarios where",
    "start": "2194920",
    "end": "2201860"
  },
  {
    "text": "this might hold and other\nscenarios where this may not hold. So if you, for example,\ntrain on Wikipedia",
    "start": "2201860",
    "end": "2207769"
  },
  {
    "text": "and your target distribution\nis arXiv or PubMed, that might be a scenario\nwhere you may have",
    "start": "2207770",
    "end": "2213050"
  },
  {
    "text": "enough coverage in\nyour source data set because Wikipedia\ndoes actually have some pretty\ntechnical content in it.",
    "start": "2213050",
    "end": "2220859"
  },
  {
    "text": "But on the other hand,\nif you have data only from one source\nhospital and you're trying to generalize to be\nable to make predictions",
    "start": "2220860",
    "end": "2227660"
  },
  {
    "text": "for a new hospital with pretty\ndifferent-looking images,",
    "start": "2227660",
    "end": "2232910"
  },
  {
    "text": "in scenarios like\nthat, the source probably wouldn't cover\nthe target distribution. And as a result, approaches\nlike this wouldn't perform well.",
    "start": "2232910",
    "end": "2241220"
  },
  {
    "text": " And so this gets into\nthe next two classes--",
    "start": "2241220",
    "end": "2248480"
  },
  {
    "text": "the next two algorithms which\nactually can handle scenarios where the source and target\ndistribution don't overlap.",
    "start": "2248480",
    "end": "2254645"
  },
  {
    "text": " Cool.",
    "start": "2254645",
    "end": "2260540"
  },
  {
    "text": "So let's look at\nanother toy example.",
    "start": "2260540",
    "end": "2269190"
  },
  {
    "text": "And in particular,\nagain, we're going to be trying to learn\na binary classifier. And our source distribution\nwill look something like this.",
    "start": "2269190",
    "end": "2277180"
  },
  {
    "text": "And let's say our\ntarget distribution looks something like this. And we're again going to\noperate in this setting",
    "start": "2277180",
    "end": "2284428"
  },
  {
    "text": "where we only have unlabeled\ndata from our target distribution. But some of the methods\nhere can actually also",
    "start": "2284428",
    "end": "2289740"
  },
  {
    "text": "be applied to the setting\nwhere you have labeled data from the target distribution.",
    "start": "2289740",
    "end": "2295070"
  },
  {
    "text": "Now, unfortunately, if you\njust train a classifier on your source\ndistribution, this wouldn't give you a\nvery accurate classifier",
    "start": "2295070",
    "end": "2301430"
  },
  {
    "text": "on the target domain. And as an example\nproblem, you could",
    "start": "2301430",
    "end": "2308200"
  },
  {
    "text": "imagine that maybe your\ntarget domain corresponds to the MNIST data\nset, where you want to be able to classify\ndifferent digits.",
    "start": "2308200",
    "end": "2314890"
  },
  {
    "text": "And your target\ndomain corresponds to a different digit\nclassification task.",
    "start": "2314890",
    "end": "2321820"
  },
  {
    "text": "In particular, these are\nimages taken from Street View of different house numbers.",
    "start": "2321820",
    "end": "2327161"
  },
  {
    "text": "And you will basically want to\ntake the classifier that you trained on MNIST maybe\nthrough a PyTorch tutorial",
    "start": "2327162",
    "end": "2332590"
  },
  {
    "text": "and also apply it to\nimages of house numbers. ",
    "start": "2332590",
    "end": "2340700"
  },
  {
    "text": "In order to do this-- this next class of approach is--",
    "start": "2340700",
    "end": "2345990"
  },
  {
    "text": "because there isn't kind of a\ndirect overlap in the support, we can't just\nupweight data points in MNIST that probably wouldn't\nwork very well on this problem.",
    "start": "2345990",
    "end": "2355075"
  },
  {
    "text": "But what we could\ntry to do is we could try to align the\nfeatures that it has learned from the two domains\nin order to encourage",
    "start": "2355075",
    "end": "2362599"
  },
  {
    "text": "it to have a similar\nrepresentation of the two distributions.",
    "start": "2362600",
    "end": "2369253"
  },
  {
    "text": "And in particular,\nwhat this might look like is\nsomething where we try to encourage the feature spaces\nto overlap as much as possible.",
    "start": "2369253",
    "end": "2376522"
  },
  {
    "text": "And then if they do\noverlap as much as possible and they have the\nsame distribution, then if we train a classifier\nonly on the source data,",
    "start": "2376522",
    "end": "2384030"
  },
  {
    "text": "then that classifier\nshould perform much better on the target distribution. ",
    "start": "2384030",
    "end": "2396015"
  },
  {
    "text": "Then there's the\nquestion of, how do we kind of go about trying\nto align these different feature spaces?",
    "start": "2396015",
    "end": "2401930"
  },
  {
    "text": "And I should also mention\nthat something like this should work the best when\nthere is kind of a clear--",
    "start": "2401930",
    "end": "2409340"
  },
  {
    "text": "when there is some sort of\nalignment between the two distributions. If you're two distributions are\nkind of both look like circles",
    "start": "2409340",
    "end": "2419570"
  },
  {
    "text": "and you have just\nsome kind of two--",
    "start": "2419570",
    "end": "2424880"
  },
  {
    "text": "some data points\nthat look like this, if you try to align\nthese two circles, you may not necessarily\nget a good classifier",
    "start": "2424880",
    "end": "2431030"
  },
  {
    "text": "by trying to align\nyour features. Whereas if your\ndistribution looks something",
    "start": "2431030",
    "end": "2436970"
  },
  {
    "text": "like this and your\ntarget distribution looks something\nlike this, there's",
    "start": "2436970",
    "end": "2443869"
  },
  {
    "text": "more of a clear alignment\nbetween these two distributions. And in that case, if you try to\nkind of align these two shapes,",
    "start": "2443870",
    "end": "2452090"
  },
  {
    "text": "you're more likely to\nhave something work out. And so you could imagine that\nsomething like MNIST digits",
    "start": "2452090",
    "end": "2457460"
  },
  {
    "text": "maybe has kind of the\nzeros are over here. The ones are over here. Maybe the nines are over here.",
    "start": "2457460",
    "end": "2463670"
  },
  {
    "text": "And you could imagine\nthat with something that has a little bit more\nof an interesting manifold to the data distribution.",
    "start": "2463670",
    "end": "2469410"
  },
  {
    "text": "It might align more\nreadily than if you just have features that are drawn\nfrom like a Gaussian space.",
    "start": "2469410",
    "end": "2475565"
  },
  {
    "start": "2475565",
    "end": "2480615"
  },
  {
    "text": "Cool.  So in terms of\naligning the features,",
    "start": "2480615",
    "end": "2486390"
  },
  {
    "text": "we're going to\nassume that we have some encoder that encodes our\ninputs into some feature space.",
    "start": "2486390",
    "end": "2493320"
  },
  {
    "text": "These encoders could be\njust the same function. Or they could be separate\nfunctions, one for the source",
    "start": "2493320",
    "end": "2499530"
  },
  {
    "text": "and one for the target. And really, our goal\nis to try to match",
    "start": "2499530",
    "end": "2505560"
  },
  {
    "text": "the features that's coming\nout of these two encoders. But we don't just want\nto match them in terms",
    "start": "2505560",
    "end": "2511380"
  },
  {
    "text": "of having them all be the same. We want to match them\nat the population level. We want basically\nthese to be mapping",
    "start": "2511380",
    "end": "2518297"
  },
  {
    "text": "rather than trying to just\ncollapse these features into the same spot. And so we can't just apply\nlike a kind of an L1 or L2 loss",
    "start": "2518297",
    "end": "2526140"
  },
  {
    "text": "on the individual features. Instead, we need to\ntry to figure out how to match the distributions.",
    "start": "2526140",
    "end": "2531870"
  },
  {
    "text": " And essentially, what we\nwant is we want samples",
    "start": "2531870",
    "end": "2538750"
  },
  {
    "text": "from this distribution to be\nindistinguishable from samples from the other\ndistribution of features.",
    "start": "2538750",
    "end": "2545470"
  },
  {
    "text": "And if those samples\nare indistinguishable, then the distribution\nshould be the same.",
    "start": "2545470",
    "end": "2551990"
  },
  {
    "text": "Yeah? [INAUDIBLE] require\nthat both p(s) and p(t) have the same kind\nof distribution,",
    "start": "2551990",
    "end": "2558430"
  },
  {
    "text": "like they're both\n[INAUDIBLE] distributed. I suppose if one were\nfor example gaussian",
    "start": "2558430",
    "end": "2564670"
  },
  {
    "text": "but the other one\nin target domain followed a different\nkind of distribution that would be kind of hard\nto make the alignment work?",
    "start": "2564670",
    "end": "2574069"
  },
  {
    "text": "So the question was, does it\nmean that p of s and p of t have to be the\nsame distribution, such as Gaussian distributions?",
    "start": "2574070",
    "end": "2580400"
  },
  {
    "text": "So they don't have to be\nidentical distributions. And I guess the--",
    "start": "2580400",
    "end": "2587859"
  },
  {
    "text": "but you do need them to be-- you do want to have them\nhave a similar shape. And I guess, in the MNIST\nexample, p of s corresponds",
    "start": "2587860",
    "end": "2596980"
  },
  {
    "text": "to the distribution\nover these MNIST digits. And p target corresponds\nto the distribution",
    "start": "2596980",
    "end": "2603400"
  },
  {
    "text": "of Street View house numbers. The one scenario in\nwhich-- one example where",
    "start": "2603400",
    "end": "2609580"
  },
  {
    "text": "that kind of agrees\nwith your intuition here is that if, for\nexample, in MNIST, you had like 90% of\nyour data set was zeros",
    "start": "2609580",
    "end": "2617590"
  },
  {
    "text": "and in Street View house\nnumbers, 90% of your data set was like fives or\nsomething like that.",
    "start": "2617590",
    "end": "2623230"
  },
  {
    "text": "If you have a mismatch in the\nlabel distribution like that, then something\nlike this probably wouldn't work well\nbecause it wouldn't",
    "start": "2623230",
    "end": "2629043"
  },
  {
    "text": "be able to kind of find an\nalignment between the two distributions. Whereas if the\ndistribution over digits,",
    "start": "2629043",
    "end": "2637010"
  },
  {
    "text": "for example, is much\nmore even, then you should be able to align\nit much more easily. And so roughly,\nyou could sort of",
    "start": "2637010",
    "end": "2644850"
  },
  {
    "text": "think of this as\nthe shape being-- the shape of the distribution\nbeing somewhat similar.",
    "start": "2644850",
    "end": "2650080"
  },
  {
    "text": "But it's OK if the\ndistribution is rotated or if it has a different\nmean or something like that.",
    "start": "2650080",
    "end": "2656130"
  },
  {
    "text": "Yeah, and I'm not sure\nif there's actually a way to formally describe this\nconstraint or this assumption.",
    "start": "2656130",
    "end": "2663220"
  },
  {
    "text": "But it is something\nthat is pretty important for these\napproaches to work. ",
    "start": "2663220",
    "end": "2671760"
  },
  {
    "text": "Cool. And so the key idea in\norder to basically try to encourage these samples to\nbe indistinguishable from each",
    "start": "2671760",
    "end": "2678240"
  },
  {
    "text": "other is to\nbasically-- we're again going to train a classifier that\ntries to predict whether or not",
    "start": "2678240",
    "end": "2684569"
  },
  {
    "text": "an example is from a source\ndomain or a target domain. But this time, the\nclassifier is going to operate on the features\nrather than on the inputs.",
    "start": "2684570",
    "end": "2692640"
  },
  {
    "text": "And our goal is\ngoing to be to try to learn features that fool\nthat classifier such that--",
    "start": "2692640",
    "end": "2699750"
  },
  {
    "text": "basically, if the classifier\ncannot accurately predict which domain the features came\nfrom, that means that the two",
    "start": "2699750",
    "end": "2707280"
  },
  {
    "text": "distributions over the\nencoded samples are identical.",
    "start": "2707280",
    "end": "2714700"
  },
  {
    "text": "Yeah? [INAUDIBLE] possible to rotate\nor align the two circles, I can't understand why it's not\npossible to find a rotation.",
    "start": "2714700",
    "end": "2719822"
  },
  {
    "text": " So the question was,\nin the circle case,",
    "start": "2719822",
    "end": "2725680"
  },
  {
    "text": "why is it not possible\nto kind of kind of rotate and align the two distributions?",
    "start": "2725680",
    "end": "2731290"
  },
  {
    "text": "In this scenario, I-- if you only have unlabeled\ndata from your target domain, it's ambiguous how\nyou should rotate it.",
    "start": "2731290",
    "end": "2738238"
  },
  {
    "text": "And so that was the\nmain point that I was trying to make there. Whereas if you have\na distribution that has more features\nto it, it will be",
    "start": "2738238",
    "end": "2744220"
  },
  {
    "text": "more clear how you're supposed\nto rotate and align the two.  And so in this example,\nwhat it would probably do",
    "start": "2744220",
    "end": "2751430"
  },
  {
    "text": "is it would just try to find\nthe easiest or simplest way to align the two.",
    "start": "2751430",
    "end": "2756528"
  },
  {
    "text": "That would be, for example,\nwithout any rotation. And if your positive\nexamples look like this, then that might end up\nleading to a poor classifier.",
    "start": "2756528",
    "end": "2766319"
  },
  {
    "text": "Yeah? [INAUDIBLE] population\nthat will be [INAUDIBLE]",
    "start": "2766320",
    "end": "2773040"
  },
  {
    "text": "What does population level mean? Yeah, it does\nsupport [INAUDIBLE]",
    "start": "2773040",
    "end": "2780190"
  },
  {
    "text": "Right. So by support, I'm\nreferring to the assumption that we made on the previous--",
    "start": "2780190",
    "end": "2788966"
  },
  {
    "text": "in the previous case, when we're\ndoing the importance weights, we assume that the support\nof the distribution--",
    "start": "2788967",
    "end": "2794370"
  },
  {
    "text": "of the source distribution\ncovered the target distribution. And so support means\nbasically the region",
    "start": "2794370",
    "end": "2801060"
  },
  {
    "text": "of the probability\ndistribution for which the density is nonzero. ",
    "start": "2801060",
    "end": "2809390"
  },
  {
    "text": "And then population\nlevel, I basically mean-- instead of looking\nat individual trying",
    "start": "2809390",
    "end": "2814700"
  },
  {
    "text": "to match individual\nexamples, we're trying to match kind of the\nentire population of examples",
    "start": "2814700",
    "end": "2821650"
  },
  {
    "text": "or population of\nfeatures, in this case. Yeah? What ensures that the\nencoder for the target domain",
    "start": "2821650",
    "end": "2829220"
  },
  {
    "text": "does something useful? It seems like you could\nsatisfy this objective by just generating random\nsamples that have nothing to do",
    "start": "2829220",
    "end": "2836360"
  },
  {
    "text": "with the numbers advantage. Yeah. So if your goal was only to\nfool the domain classifier, what",
    "start": "2836360",
    "end": "2843200"
  },
  {
    "text": "it could do is it\ncould just output random features or output all\nzero features or something like that.",
    "start": "2843200",
    "end": "2848360"
  },
  {
    "text": "And there's nothing encouraging\nit to actually give you good features.",
    "start": "2848360",
    "end": "2854180"
  },
  {
    "text": "And so what we're going\nto do is we're not only going to have this\nloss, but we're also going to try to be able\nto classify the source",
    "start": "2854180",
    "end": "2862490"
  },
  {
    "text": "examples using the features. [INAUDIBLE] You\ncould do it right",
    "start": "2862490",
    "end": "2867770"
  },
  {
    "text": "for the source examples and\nwrong for the target examples, right? ",
    "start": "2867770",
    "end": "2873799"
  },
  {
    "text": "So even if you try to\nclassify on the features, yeah, it could\nlearn a classifier",
    "start": "2873800",
    "end": "2879050"
  },
  {
    "text": "that works well for the\nsource but not necessarily for the target. ",
    "start": "2879050",
    "end": "2886359"
  },
  {
    "text": "I-- [INAUDIBLE] target encoder\ncould be a bad one. Right. So there is a\nquestion of whether",
    "start": "2886360",
    "end": "2894430"
  },
  {
    "text": "or not to actually learn-- have these encoders separate. And the target\nencoder could learn",
    "start": "2894430",
    "end": "2900070"
  },
  {
    "text": "something that is pretty\ndifferent from the source encoder. And so there is a little\nbit of a trade-off. You could learn\na single encoder.",
    "start": "2900070",
    "end": "2905980"
  },
  {
    "text": "You could also-- which would\nkind of prevent this issue.",
    "start": "2905980",
    "end": "2911192"
  },
  {
    "text": "And if you learn\na target encoder, you're basically hoping\nthat the solution that",
    "start": "2911193",
    "end": "2916868"
  },
  {
    "text": "is kind of the simplest\nis the one that maps them into a consistent space. And if you have a\nsimilar architecture",
    "start": "2916868",
    "end": "2922900"
  },
  {
    "text": "and randomly initialize\nthem in a similar way, my expectation is it would\ngive you something reasonable.",
    "start": "2922900",
    "end": "2928720"
  },
  {
    "text": "And there's kind of\nempirical evidence that supports that as well. But you may also--\nit may also be",
    "start": "2928720",
    "end": "2935050"
  },
  {
    "text": "that you want to\nactually share some of the weights between\nthese two encoders. ",
    "start": "2935050",
    "end": "2942650"
  },
  {
    "text": "Cool. So concretely, what\ndoes this look like? So we're going to be\ntraining a feature",
    "start": "2942650",
    "end": "2948330"
  },
  {
    "text": "encoder that takes as input, our\nexample, and gives us features. We're going to be also training\na classifier to predict labels",
    "start": "2948330",
    "end": "2956940"
  },
  {
    "text": "from these features\nand backpropagating the kind of cross-entropy\nloss with respect to our label",
    "start": "2956940",
    "end": "2964140"
  },
  {
    "text": "predictions into both the\nfeature encoder and the label classifier. This corresponds to the\nstandard supervised learning.",
    "start": "2964140",
    "end": "2972570"
  },
  {
    "text": "What's new is we're also\ngoing to be training a classifier that estimates\nthe domain that the input came",
    "start": "2972570",
    "end": "2979680"
  },
  {
    "text": "from from the features\nof that input. And what we can do is then try\nto train the features such that",
    "start": "2979680",
    "end": "2989190"
  },
  {
    "text": "we cannot predict the\ndomain accurately. And so what this means is\nthat the domain classifier,",
    "start": "2989190",
    "end": "2996270"
  },
  {
    "text": "its goal will be to-- its goal will be\nto kind of maximize",
    "start": "2996270",
    "end": "3003260"
  },
  {
    "text": "the accuracy of this classifier. Whereas the goal\nof the features are to minimize the accuracy\nof this classifier.",
    "start": "3003260",
    "end": "3011260"
  },
  {
    "text": "And so to do that,\nwhat we can do is we can just take\nthe gradient coming from the loss for the classifier\nand negate it and then pass",
    "start": "3011260",
    "end": "3021630"
  },
  {
    "text": "the negative gradients\nfrom the domain classifier into the feature encoder. ",
    "start": "3021630",
    "end": "3029310"
  },
  {
    "text": "This is called a kind of a\ngradient reversal layer, where you're basically just\ngoing to be reversing the gradient before\nbackpropagating",
    "start": "3029310",
    "end": "3035880"
  },
  {
    "text": "into the feature encoder.  And so here we're going to be\nminimizing label prediction",
    "start": "3035880",
    "end": "3043970"
  },
  {
    "text": "error and trying to\nmaximize domain confusion.",
    "start": "3043970",
    "end": "3049560"
  },
  {
    "text": "And if we write out\nthis algorithm kind of more completely, what\nthis looks like is first",
    "start": "3049560",
    "end": "3055980"
  },
  {
    "text": "we're going to update the\ndomain classifier C or C phi.",
    "start": "3055980",
    "end": "3067100"
  },
  {
    "text": "And this is going\nto be with respect to basically how accurate is--",
    "start": "3067100",
    "end": "3072230"
  },
  {
    "text": "how accurate that classifier\nis, which is just denoted by Lc.",
    "start": "3072230",
    "end": "3077810"
  },
  {
    "text": "And then we're going to update\nour features or the encoder,",
    "start": "3077810",
    "end": "3088200"
  },
  {
    "text": "f of x, as well as the\nlabel classifier, g.",
    "start": "3088200",
    "end": "3098710"
  },
  {
    "text": "And we're updating these\nwith respect to basically",
    "start": "3098710",
    "end": "3104349"
  },
  {
    "text": "how well we're predicting\ny minus the loss",
    "start": "3104350",
    "end": "3111770"
  },
  {
    "text": "function of the classifier. And we have a kind\nof a coefficient",
    "start": "3111770",
    "end": "3117119"
  },
  {
    "text": "here to control how much\nyou weight the classifier loss versus the label\nprediction loss.",
    "start": "3117120",
    "end": "3123030"
  },
  {
    "text": "Yeah? How exactly do you\nsend a negative example in this case for the\ndomain classifier since--",
    "start": "3123030",
    "end": "3130099"
  },
  {
    "text": "how do you get negative\nexamples for the-- Domain classifier--\nso we're assuming like before that we have\naccess to unlabeled data",
    "start": "3130100",
    "end": "3136110"
  },
  {
    "text": "from the target domain. [INAUDIBLE] So the label classifier,\nbasically, Ly,",
    "start": "3136110",
    "end": "3143880"
  },
  {
    "text": "is only evaluated on data\nfrom our source data set. And Lc is evaluated using\nboth the source data set",
    "start": "3143880",
    "end": "3153150"
  },
  {
    "text": "and the target data set. ",
    "start": "3153150",
    "end": "3159339"
  },
  {
    "text": "Yeah? [INAUDIBLE] space issue. So is it possible to get around\nlike this resolution thing",
    "start": "3159340",
    "end": "3166420"
  },
  {
    "text": "by projecting them\ninto common corners and then in that space,\nlike the shapes would align? So could we think\nabout it that way,",
    "start": "3166420",
    "end": "3171564"
  },
  {
    "text": "a space where the\nshapes should align? ",
    "start": "3171564",
    "end": "3177910"
  },
  {
    "text": "Is there a common space\nwhere the shape should align? I'm not sure how you\nwould get the projection",
    "start": "3177910",
    "end": "3184450"
  },
  {
    "text": "to project it into that space. [INAUDIBLE] I don't\nremember [INAUDIBLE]",
    "start": "3184450",
    "end": "3199390"
  },
  {
    "text": "be able to do that\nlike that, I think. Yeah, sounds good. And one thing that\nI'll mention here",
    "start": "3199390",
    "end": "3205210"
  },
  {
    "text": "is it's important to\ndo this iteratively. So if you first kind of just\ntrain your domain classifier",
    "start": "3205210",
    "end": "3210460"
  },
  {
    "text": "and then you just kind\nof fix it and then train your features to try\nto fool it, at some point,",
    "start": "3210460",
    "end": "3217630"
  },
  {
    "text": "it may-- basically, we'll just\nchange our features such that they're out of distribution\nfor your domain classifier",
    "start": "3217630",
    "end": "3223690"
  },
  {
    "text": "and kind of fool the domain\nclassifier without actually having features that\nare indistinguishable.",
    "start": "3223690",
    "end": "3229119"
  },
  {
    "text": "And so in practice, you\nneed to kind of iterate this process between updating\nyour domain classifier and updating your features\nin your label classifier.",
    "start": "3229120",
    "end": "3236109"
  },
  {
    "text": "And this will ensure that your\ndomain classifier is always kind of up-to-date on the\nlatest version of the features.",
    "start": "3236110",
    "end": "3241990"
  },
  {
    "text": "Cool. So this is written out here. We've randomly initialized\nour encoder, label classifier, and domain classifier.",
    "start": "3241990",
    "end": "3248170"
  },
  {
    "text": "We then update our\ndomain classifier, which basically corresponds\nto binary classification",
    "start": "3248170",
    "end": "3254950"
  },
  {
    "text": "between the source examples\nand the target examples. And so this is just writing\nout the cross-entropy loss.",
    "start": "3254950",
    "end": "3260740"
  },
  {
    "text": " And then we will update the\nparameters of our features",
    "start": "3260740",
    "end": "3267910"
  },
  {
    "text": "and label classifier\nwith respect to how well it's predicting\nthe labels, as well",
    "start": "3267910",
    "end": "3275980"
  },
  {
    "text": "as this auxiliary\nterm that corresponds to domain confusion.",
    "start": "3275980",
    "end": "3283480"
  },
  {
    "text": "Yeah? Is there any problem that\nif we feed the target",
    "start": "3283480",
    "end": "3289809"
  },
  {
    "text": "or label it out into\ngradient descent on LC? ",
    "start": "3289810",
    "end": "3301180"
  },
  {
    "text": "You're saying is\nthere any problem with passing in target data\ninto the domain classifier?",
    "start": "3301180",
    "end": "3310180"
  },
  {
    "text": "[INAUDIBLE] So the domain classifier will\nbe trained on both source data",
    "start": "3310180",
    "end": "3315790"
  },
  {
    "text": "and target data. And so you can see\nthat right here in step two, where it's trained\non both source data and target data.",
    "start": "3315790",
    "end": "3321230"
  },
  {
    "text": "And so the target examples-- they'll not be out of\ndistribution for the domain classifier. The domain classifier\nwill be trained on them.",
    "start": "3321230",
    "end": "3328333"
  },
  {
    "text": "And so that will give\nan accurate estimate for whether or not those\nexamples came from the source domain or the target domain.",
    "start": "3328333",
    "end": "3333550"
  },
  {
    "text": "And then you'll kind\nof reverse the gradient to encourage the features\nto produce features that the domain classifier\ncan't accurately predict.",
    "start": "3333550",
    "end": "3340330"
  },
  {
    "text": " But the first step is polling\nthe features of source data",
    "start": "3340330",
    "end": "3352430"
  },
  {
    "text": "close to target data\nbut why don't we also poll the features on target\ndata close to the source data?",
    "start": "3352430",
    "end": "3362720"
  },
  {
    "text": "So this loss will be evaluated\non both the source and target. And so it will be\nlike encouraging",
    "start": "3362720",
    "end": "3369700"
  },
  {
    "text": "it to bring the two together. And so it's not going\nto bring one to another. It will be bringing kind\nof the two together.",
    "start": "3369700",
    "end": "3375722"
  },
  {
    "text": "It would basically\njust try to find what features will make\nbasically the domain",
    "start": "3375722",
    "end": "3381460"
  },
  {
    "text": "classifier's job hard. The first term in\nthe third step, this just corresponds to\nhow accurately we're",
    "start": "3381460",
    "end": "3387730"
  },
  {
    "text": "able to predict the labels. And this is only\ndone on the source data set because we only have\nlabels for the source data set.",
    "start": "3387730",
    "end": "3393309"
  },
  {
    "start": "3393310",
    "end": "3398555"
  },
  {
    "text": "Cool.  Now in terms of a couple\nof design choices,",
    "start": "3398555",
    "end": "3404819"
  },
  {
    "text": "I mentioned that you can learn\na separate source and target encoder.",
    "start": "3404820",
    "end": "3410190"
  },
  {
    "text": "This can give the model a\nlittle bit more flexibility because if the source and target\nimages look very different,",
    "start": "3410190",
    "end": "3415930"
  },
  {
    "text": "you might need different\nfilters or different weights to process them. But it can also possibly\ngive the model too much",
    "start": "3415930",
    "end": "3424470"
  },
  {
    "text": "flexibility, which could\nlead to some of the issues that we discussed.",
    "start": "3424470",
    "end": "3429790"
  },
  {
    "text": "There's also a couple\nof different forms of the loss function for\ntrying to confuse the domain",
    "start": "3429790",
    "end": "3436740"
  },
  {
    "text": "classifier. And this is often referred to as\ndomain-adversarial training. .",
    "start": "3436740",
    "end": "3442440"
  },
  {
    "text": "One of them is what\nwe talked about with this gradient\nreversal layer. And this is the same as how\ngenerative adversarial networks",
    "start": "3442440",
    "end": "3448440"
  },
  {
    "text": "are implemented. But another option\nis instead of trying to maximize the loss of\nthe domain classifier,",
    "start": "3448440",
    "end": "3455070"
  },
  {
    "text": "you could also try to\noptimize for basically",
    "start": "3455070",
    "end": "3461220"
  },
  {
    "text": "for the classifier to\nbe outputting a 50/50 guess between the two domains.",
    "start": "3461220",
    "end": "3466819"
  },
  {
    "text": "And this will be something--\nthis will be kind of optimizing for kind of--",
    "start": "3466820",
    "end": "3472860"
  },
  {
    "text": "essentially, for\nit to be-- to have kind of basically no idea\nwhat the correct domain is.",
    "start": "3472860",
    "end": "3480539"
  },
  {
    "text": "And kind of the intuition\nbehind the second option is that if you maximize\nthe domain classifier loss,",
    "start": "3480540",
    "end": "3487300"
  },
  {
    "text": "then that corresponds to\npredicting source confidently when it's actually target\nand predicting target",
    "start": "3487300",
    "end": "3493440"
  },
  {
    "text": "confidently when\nit's the source. And if you can actually-- if you\ncan do that and get the worst",
    "start": "3493440",
    "end": "3498600"
  },
  {
    "text": "loss of the two, that\nactually would give you features that can\nstill distinguish between source and target.",
    "start": "3498600",
    "end": "3506070"
  },
  {
    "text": "And so the second option\nwill somewhat prevent that, although in practice\nwith option one,",
    "start": "3506070",
    "end": "3511260"
  },
  {
    "text": "if you're updating your\ndomain classifier enough, you should be able to\navoid that issue as well.",
    "start": "3511260",
    "end": "3516720"
  },
  {
    "start": "3516720",
    "end": "3521990"
  },
  {
    "text": "Cool. And this is a question of\nhow well does this work. So we'll look at two\ndifferent examples.",
    "start": "3521990",
    "end": "3528560"
  },
  {
    "text": "This is a toy example\nwhere the source domain data is shown as the red\nand green data points.",
    "start": "3528560",
    "end": "3535330"
  },
  {
    "text": "And the target domain is\nthe black data points. And if you train a neural\nnetwork in a standard way only",
    "start": "3535330",
    "end": "3541809"
  },
  {
    "text": "on the source data, you\nget a decision boundary in the black line. And we see it this different--\nat these few different points.",
    "start": "3541810",
    "end": "3548410"
  },
  {
    "text": "Like, here, for example,\nit's incorrectly classifying these data points as green. It's likewise incorrectly\nclassifying these data points",
    "start": "3548410",
    "end": "3555760"
  },
  {
    "text": "as red. In contrast, if you\nuse the approach that we just talked about,\nthis will shift the decision",
    "start": "3555760",
    "end": "3563869"
  },
  {
    "text": "boundary to something that looks\nlike this, where it is actually accurately classifying\nthese data points",
    "start": "3563870",
    "end": "3570050"
  },
  {
    "text": "and also has kind of shifted the\ndecision boundary here as well so that you're actually\nmuch more accurately making",
    "start": "3570050",
    "end": "3579110"
  },
  {
    "text": "predictions on the\ntarget domain data. They also evaluated\non the digit examples",
    "start": "3579110",
    "end": "3585958"
  },
  {
    "text": "that we looked at before. And so they looked at MNIST to\nthis kind of synthetic, colored MNIST version.",
    "start": "3585958",
    "end": "3592180"
  },
  {
    "text": "These synthetic numbers to\nStreet View house numbers, Street View house\nnumbers to MNIST and these signs to these\nGerman traffic signs.",
    "start": "3592180",
    "end": "3602710"
  },
  {
    "text": "And if you compare training\nonly on the source data set to this approach,\nyou see that you",
    "start": "3602710",
    "end": "3608920"
  },
  {
    "text": "can get a really substantial\nimprovement in performance often, kind of as much as\nalmost 20% in some cases.",
    "start": "3608920",
    "end": "3618640"
  },
  {
    "text": "And it doesn't do as well as if\nyou were to train on label data from the target\ndistribution, but it",
    "start": "3618640",
    "end": "3625270"
  },
  {
    "text": "is able to bridge the\ngap fairly significantly in a number of different cases.",
    "start": "3625270",
    "end": "3631210"
  },
  {
    "start": "3631210",
    "end": "3637000"
  },
  {
    "text": "Cool. So to summarize this part,\nthese sorts of methods are fairly simple to\nimplement, and they",
    "start": "3637000",
    "end": "3644190"
  },
  {
    "text": "can work pretty well like we\nsaw on the previous slide. They don't require\nthe source data",
    "start": "3644190",
    "end": "3650430"
  },
  {
    "text": "to cover the target\ndistribution, which is pretty nice. It does involve an adversarial\noptimization, which sometimes",
    "start": "3650430",
    "end": "3657555"
  },
  {
    "text": "can be a little bit tricky. And you really need to\ntune this weight right here",
    "start": "3657555",
    "end": "3662579"
  },
  {
    "text": "to kind of trade-off\nthe indistinguishability of the features and your\naccuracy on the source domain.",
    "start": "3662580",
    "end": "3672030"
  },
  {
    "text": "It also, like we\ndiscussed, requires some degree of clear\nalignment in the distributions of the data.",
    "start": "3672030",
    "end": "3677730"
  },
  {
    "text": "And if those distributions\nare very different, it may be difficult\nfor the algorithm to figure out how the two\ndomains align in practice.",
    "start": "3677730",
    "end": "3684180"
  },
  {
    "start": "3684180",
    "end": "3689380"
  },
  {
    "text": "Cool. And then the last class of\nmethods that we'll look at is it's also going\nto be somewhat trying",
    "start": "3689380",
    "end": "3697540"
  },
  {
    "text": "to find an alignment. But instead of trying to learn a\nfeature space that is perfectly aligned, it's\ninstead going to try",
    "start": "3697540",
    "end": "3704140"
  },
  {
    "text": "to learn mapping from one\ndomain to the other domain. And really, the key idea\nhere is if we could translate",
    "start": "3704140",
    "end": "3711950"
  },
  {
    "text": "from examples from one\ndomain to another domain, then we would be able to do\npretty well on the target data",
    "start": "3711950",
    "end": "3718360"
  },
  {
    "text": "set. So if you could translate source\nexamples to target examples, then you could just basically\ntranslate your labeled source",
    "start": "3718360",
    "end": "3724610"
  },
  {
    "text": "data set into your target\ndomain and train a predictor on the translated data set\nand then deploy your predictor",
    "start": "3724610",
    "end": "3731750"
  },
  {
    "text": "on the target example. Likewise, if you were able\nto translate from target",
    "start": "3731750",
    "end": "3737105"
  },
  {
    "text": "to source, then\nwhat you could do is train a predictor\non your source data set and then translate\nyour test example,",
    "start": "3737105",
    "end": "3743460"
  },
  {
    "text": "your target example\nto the source domain with your translator\nand then evaluate the predictor",
    "start": "3743460",
    "end": "3749970"
  },
  {
    "text": "on the translated examples. And one key difference\nbetween these approaches",
    "start": "3749970",
    "end": "3754980"
  },
  {
    "text": "and the previous approaches\nis we're actually going to be operating in the\noriginal input space x rather",
    "start": "3754980",
    "end": "3761280"
  },
  {
    "text": "than operating on features.  Then, of course, the\nquestion comes up",
    "start": "3761280",
    "end": "3767140"
  },
  {
    "text": "is that how do we\nactually go about learning to translate between\nthese different domains? ",
    "start": "3767140",
    "end": "3773400"
  },
  {
    "text": "So the first thing\nthat you could do is train your model F, just\ntranslating from source",
    "start": "3773400",
    "end": "3779500"
  },
  {
    "text": "to target to generate images\nfrom your target distribution and likewise, train a function\nG to be able to generate images",
    "start": "3779500",
    "end": "3787960"
  },
  {
    "text": "from your source distribution. And you can do this with a\ngenerative adversarial network,",
    "start": "3787960",
    "end": "3795670"
  },
  {
    "text": "where you'll be training\na classifier that'd be able to predict if\nsomething came from the source",
    "start": "3795670",
    "end": "3801820"
  },
  {
    "text": "domain or the target domain. And then your goal is\nfor your generative model to be able to fool\nthat classifier",
    "start": "3801820",
    "end": "3808480"
  },
  {
    "text": "and think that the data\nthat it's generating came from the domain that\nyou're trying to generate from.",
    "start": "3808480",
    "end": "3818250"
  },
  {
    "text": "And so kind of what\nthis looks like is if you have some kind\nof source distribution",
    "start": "3818250",
    "end": "3823880"
  },
  {
    "text": "that looks like this. And you have another\ntarget distribution",
    "start": "3823880",
    "end": "3828920"
  },
  {
    "text": "that looks like this. Essentially, what you're\ngoing to be trying to do is train something that takes\nan example from your source",
    "start": "3828920",
    "end": "3835310"
  },
  {
    "text": "distribution and translates it\ninto your target distribution. This is going to\nbe a function F.",
    "start": "3835310",
    "end": "3841910"
  },
  {
    "text": "And this function F will take\nas input the source example, and it will be\ntrained with again",
    "start": "3841910",
    "end": "3848390"
  },
  {
    "text": "to generate examples that look\nlike the target distribution. And likewise, you'll also\nbe training a function G",
    "start": "3848390",
    "end": "3857350"
  },
  {
    "text": "to take as input an\nexample and map it to an example from\nthe source data set.",
    "start": "3857350",
    "end": "3866462"
  },
  {
    "text": "And one of the nice things\nabout this objective is it doesn't require you\nto have any paired data. You don't need to know kind\nof what example specifically",
    "start": "3866462",
    "end": "3875260"
  },
  {
    "text": "corresponds to\nthe other example. We're just going to be\ntraining this generative model to generate samples that look\nlike they came from the target",
    "start": "3875260",
    "end": "3881660"
  },
  {
    "text": "set.  That said, if you only\ndo this objective,",
    "start": "3881660",
    "end": "3888140"
  },
  {
    "text": "you'll run into a bit\nof a problem, which is that it won't necessarily\nlearn to map in a way that's",
    "start": "3888140",
    "end": "3898040"
  },
  {
    "text": "somewhat consistent between\nthe different domains. In particular, if F\nmaps from here to here,",
    "start": "3898040",
    "end": "3905180"
  },
  {
    "text": "there's nothing that's stopping\nG from mapping from here to over here to mapping to\nsomething completely different.",
    "start": "3905180",
    "end": "3912660"
  },
  {
    "text": "And so there's one\nadditional objective that we can incorporate\ninto this approach that tries to actually optimize for\nthe consistency of these two",
    "start": "3912660",
    "end": "3922250"
  },
  {
    "text": "kind of domain translators. And in particular,\nwhat we're going to try to do is we can take\na data point from our source",
    "start": "3922250",
    "end": "3929750"
  },
  {
    "text": "data set, map it from F,\nand then map it back with G and basically try to\nencourage the distance",
    "start": "3929750",
    "end": "3938030"
  },
  {
    "text": "between the original data\npoint and the data point after going through this\ncycle to be very small.",
    "start": "3938030",
    "end": "3945180"
  },
  {
    "text": "And so we're basically going to\nkind of minimize this distance right here. ",
    "start": "3945180",
    "end": "3952160"
  },
  {
    "text": "And so this is trying to address\nthe fact that the mapping is under constraint, and it can be\nkind of an arbitrary mapping.",
    "start": "3952160",
    "end": "3960520"
  },
  {
    "text": "And we're basically going to be\nencouraging the models to learn this kind of consistent\nbijective mapping",
    "start": "3960520",
    "end": "3966760"
  },
  {
    "text": "by training them to be\ncyclically consistent such that if you map from\none domain and back, it",
    "start": "3966760",
    "end": "3973083"
  },
  {
    "text": "gives you a data\npoint that's very similar to the\noriginal data point. And likewise, if you go from\ntarget to source and back",
    "start": "3973083",
    "end": "3979870"
  },
  {
    "text": "to target, you want to get back\nthe same data point as before. ",
    "start": "3979870",
    "end": "3986060"
  },
  {
    "text": "And so the way that you can\nimplement this is basically just with a kind of a\nstandard L1 or L2 objective,",
    "start": "3986060",
    "end": "3993069"
  },
  {
    "text": "where you sample a data\npoint from your source data set, map it to target, and\nthen map it back to source,",
    "start": "3993070",
    "end": "3998830"
  },
  {
    "text": "and then compare that\nexample after the cycle to the original example\nand encourage them",
    "start": "3998830",
    "end": "4004800"
  },
  {
    "text": "to be similar to one another. And so that's how we get this\nloss function right here.",
    "start": "4004800",
    "end": "4010530"
  },
  {
    "start": "4010530",
    "end": "4015640"
  },
  {
    "text": "And so then the full\nobjective for this approach will be we're going\nto be training F",
    "start": "4015640",
    "end": "4021180"
  },
  {
    "text": "and G. F will be kind of\ntrained to generate examples",
    "start": "4021180",
    "end": "4026849"
  },
  {
    "text": "that look like target examples. G will be trained\nto generate examples that look like the\nsource, and then we'll",
    "start": "4026850",
    "end": "4033270"
  },
  {
    "text": "have this additional\nregularizer that encourages cycle\nconsistency, which says that when you\nkind of form a cycle,",
    "start": "4033270",
    "end": "4039968"
  },
  {
    "text": "you should get back to\nwhere you came from. ",
    "start": "4039968",
    "end": "4046320"
  },
  {
    "text": "Any questions on how this works? ",
    "start": "4046320",
    "end": "4062410"
  },
  {
    "text": "So if you take this approach\nand apply it to data sets from different domains--",
    "start": "4062410",
    "end": "4067750"
  },
  {
    "text": "and so for example,\nif you take a data set of Monet photos\nor Monet paintings",
    "start": "4067750",
    "end": "4073950"
  },
  {
    "text": "and a data set of\nphotos, you can get something that maps from-- basically can take a kind\nof a painting from Monet",
    "start": "4073950",
    "end": "4081599"
  },
  {
    "text": "and translate it into a photo\nand likewise take a photo and translate it into\nsomething that looks like a painting from Monet.",
    "start": "4081600",
    "end": "4089220"
  },
  {
    "text": "Likewise, you can take a data\nset of pictures from the summer and translate it to something\nthat looks more like winter",
    "start": "4089220",
    "end": "4097869"
  },
  {
    "text": "and the reverse of that. There's also something\nmore abstract like edges to shoes and\nshoes to edges or translate",
    "start": "4097869",
    "end": "4106199"
  },
  {
    "text": "between zebras and horses. One thing that's different\nabout this approach compared",
    "start": "4106200",
    "end": "4111720"
  },
  {
    "text": "to the other approach, the kind\nof domain-adversarial training is that here we\nactually don't even",
    "start": "4111720",
    "end": "4117509"
  },
  {
    "text": "need any labeled examples\nfrom our source data set in order to train\nfor these mappings. ",
    "start": "4117510",
    "end": "4124962"
  },
  {
    "text": "This is kind of a purely\nunsupervised approach for mapping between two domains. ",
    "start": "4124962",
    "end": "4131783"
  },
  {
    "text": "So the original paper\nfor this didn't actually use it for domain adaptation. They just used it to kind of\ngenerate pictures like this.",
    "start": "4131783",
    "end": "4139039"
  },
  {
    "text": "But you can actually use\nit for domain adaptation. So one place where\nit was used was",
    "start": "4139040",
    "end": "4145939"
  },
  {
    "text": "it was used to translate\nbetween simulated robots and real robots. And so the simulation is\nkind of shown on the left.",
    "start": "4145939",
    "end": "4153799"
  },
  {
    "text": "The real image is\nshown on the right and it's able to basically kind\nof generate real-looking images",
    "start": "4153800",
    "end": "4159680"
  },
  {
    "text": "from simulated images\nand vice versa. And it turns out that\nif you basically train",
    "start": "4159680",
    "end": "4167620"
  },
  {
    "text": "with reinforcement\nlearning in the simulator and actually evaluate that\npolicy on the real robot,",
    "start": "4167620",
    "end": "4173950"
  },
  {
    "text": "you can get a success rate-- a grasp success rate that's\nmuch higher than if you only",
    "start": "4173950",
    "end": "4179259"
  },
  {
    "text": "use some data and if you\nkind of used something",
    "start": "4179260",
    "end": "4184802"
  },
  {
    "text": "called domain\nrandomization, where you try to randomize the\nsimulator as much as possible but don't actually try to use\nreal data to kind of translate",
    "start": "4184803",
    "end": "4191770"
  },
  {
    "text": "between simulation and real. ",
    "start": "4191770",
    "end": "4197380"
  },
  {
    "text": "You can also use these\nto kind of translate between humans and robots. So this is the\napproach that wanted",
    "start": "4197380",
    "end": "4203740"
  },
  {
    "text": "to use data from humans in\norder to improve a robot policy.",
    "start": "4203740",
    "end": "4210550"
  },
  {
    "text": "And so the top row\nhere are real images, and the bottom row\nare images generated",
    "start": "4210550",
    "end": "4217240"
  },
  {
    "text": "by the generative model. And once you kind of generate\nit in the robot domain,",
    "start": "4217240",
    "end": "4223000"
  },
  {
    "text": "it's a lot easier to kind\nof use that data directly. Yeah,?",
    "start": "4223000",
    "end": "4228989"
  },
  {
    "text": "I still have question\nabout CycleGAN. How do you ensure that\nsomething from the source domain",
    "start": "4228990",
    "end": "4235140"
  },
  {
    "text": "doesn't map arbitrarily to\nsomething else in the target domain? So for example,\nyou're first domain",
    "start": "4235140",
    "end": "4241620"
  },
  {
    "text": "is, say, synthetic\ndogs and foxes. And the target domain\nis real dogs and foxes.",
    "start": "4241620",
    "end": "4247860"
  },
  {
    "text": "What's to stop the model\nfrom mapping synthetic dogs",
    "start": "4247860",
    "end": "4253050"
  },
  {
    "text": "with real foxes and vice versa? Yeah. So that's a great question. So one thing it could\ndo is it could map--",
    "start": "4253050",
    "end": "4261630"
  },
  {
    "text": "maybe it does kind of\nobey the cycle consistency because that's what\nwe trained it for. And so if, for example--",
    "start": "4261630",
    "end": "4268630"
  },
  {
    "text": "maybe it does actually\nmap from here to here. And it also maps like\nfrom here to here.",
    "start": "4268630",
    "end": "4275800"
  },
  {
    "text": "But maybe this is kind of real\ndogs, and this is real foxes,",
    "start": "4275800",
    "end": "4282540"
  },
  {
    "text": "I think, you said. And this is what?",
    "start": "4282540",
    "end": "4287592"
  },
  {
    "text": "Synthetic foxes. And this is synthetic dogs.",
    "start": "4287592",
    "end": "4295050"
  },
  {
    "text": "And so first, it is possible\nfor it to learn this.",
    "start": "4295050",
    "end": "4300210"
  },
  {
    "text": "There are two things that can\nencourage it not to learn this. The first is that\noftentimes, when",
    "start": "4300210",
    "end": "4307560"
  },
  {
    "text": "people design\narchitectures for this, they'll encourage\nthe architectures to only change the local fea--",
    "start": "4307560",
    "end": "4313650"
  },
  {
    "text": "[CLEARS THROAT] the local features of the image. And if you encourage it to\nonly change the local features,",
    "start": "4313650",
    "end": "4320910"
  },
  {
    "text": "then it's somewhat hard to\ncreate a dog out of a fox, like maybe the\nears look different",
    "start": "4320910",
    "end": "4326490"
  },
  {
    "text": "or something like that. And the second thing is that\nif you have a data set that has",
    "start": "4326490",
    "end": "4332700"
  },
  {
    "text": "maybe it's like 80% real\ndogs and 20% real foxes",
    "start": "4332700",
    "end": "4338340"
  },
  {
    "text": "and 80% synthetic dogs\nand 20% synthetic foxes,",
    "start": "4338340",
    "end": "4343590"
  },
  {
    "text": "then these sorts of data set\nstatistics will encourage it to actually get the\nright mapping because it",
    "start": "4343590",
    "end": "4352086"
  },
  {
    "text": "will be pretty difficult-- basically, if you need\nto generate things",
    "start": "4352086",
    "end": "4357180"
  },
  {
    "text": "that look like the\ntarget distribution and you are mapping 80%\nof your dogs to 80% foxes,",
    "start": "4357180",
    "end": "4362969"
  },
  {
    "text": "then that won't actually\nmatch the target distribution. So having these\nsorts of statistics",
    "start": "4362970",
    "end": "4368400"
  },
  {
    "text": "and the kind of frequency\nof objects in your data set be consistent between\nthe source and target can really help the mapping.",
    "start": "4368400",
    "end": "4376770"
  },
  {
    "text": "But if you do have 80% synthetic\nfoxes and 80% real dogs, then it may actually learn a\nmapping between foxes and dogs.",
    "start": "4376770",
    "end": "4383670"
  },
  {
    "text": "And so this is kind of\ngetting back to the assumption that we made with the\nprevious approach, which is that the two domains do need\nto have a similar distribution",
    "start": "4383670",
    "end": "4394199"
  },
  {
    "text": "in some sense. So then most data sets are\nchoices and design choices more than the mathematical--",
    "start": "4394200",
    "end": "4401520"
  },
  {
    "text": "Yeah, exactly. And so the-- when I worked with\nthese kinds of methods before,",
    "start": "4401520",
    "end": "4407789"
  },
  {
    "text": "typically, we actually\nare trying to-- when we tune the\nmethod, we actually tune the data set\nmore than the method. ",
    "start": "4407790",
    "end": "4417080"
  },
  {
    "text": "Cool. So the kind of pros\nof this approach is conceptually pretty cool,\nalthough maybe that's not",
    "start": "4417080",
    "end": "4423400"
  },
  {
    "text": "a reason to use it. And it can actually\nwork pretty well. It's also quite interpretable.",
    "start": "4423400",
    "end": "4430488"
  },
  {
    "text": "This means that it\ngives you cool pictures, but it also means\nthat it can be easier to debug because if you\nactually generate, like run F",
    "start": "4430488",
    "end": "4438880"
  },
  {
    "text": "with your model and you see\nit's mapping dogs to foxes, then you know\nwhat's going wrong. Whereas if you just have\nthis feature space that's",
    "start": "4438880",
    "end": "4445820"
  },
  {
    "text": "a little bit difficult\nto interpret because it's very high dimensional, then it\ncan be a little bit difficult to understand what's going\nwrong with your approach.",
    "start": "4445820",
    "end": "4452387"
  },
  {
    "text": " The downside is\nthat it does involve an adversarial optimization\njust like before,",
    "start": "4452387",
    "end": "4459920"
  },
  {
    "text": "and it also involves generative\nmodeling now as well. And those can require\nlarger models.",
    "start": "4459920",
    "end": "4465680"
  },
  {
    "text": "And like feature\nalignment, it does require this sort of clear\nalignment in the distributions in order to work well.",
    "start": "4465680",
    "end": "4471230"
  },
  {
    "text": " Cool. And then the last thing\nI'd like to mention",
    "start": "4471230",
    "end": "4477342"
  },
  {
    "text": "is you can actually\ncombine the two approaches that we just talked about,\nthe CycleGAN approach and domain-adversarial\nneural networks.",
    "start": "4477342",
    "end": "4484123"
  },
  {
    "text": "There's an approach\nthat basically incorporates both of these\ninto a single approach.",
    "start": "4484123",
    "end": "4490020"
  },
  {
    "text": "And on things like the character\nrecognition task we looked at, you're able to do much better.",
    "start": "4490020",
    "end": "4495660"
  },
  {
    "text": "So domain-adversarial\nneural networks alone get 73% accuracy,\nwhereas this approach",
    "start": "4495660",
    "end": "4501560"
  },
  {
    "text": "gets a 90% accuracy when\ntranslating from Street View house numbers to MNIST.",
    "start": "4501560",
    "end": "4506930"
  },
  {
    "text": "And it could also work\non more complex data. So this is something\nwhere they're trying to translate a classifier\ntrained on synthetic driving",
    "start": "4506930",
    "end": "4514610"
  },
  {
    "text": "data to real driving data\nfrom the Cityscapes data set. And they were able to much\nmore accurately segment objects",
    "start": "4514610",
    "end": "4521960"
  },
  {
    "text": "in the scene in comparison\nto prior domain adaptation approaches. So that's it on\ndomain adaptation.",
    "start": "4521960",
    "end": "4529310"
  },
  {
    "text": "Those are really the\nkind of three classes of domain adaptation\nmethods that have been most successful\nand most popular.",
    "start": "4529310",
    "end": "4536139"
  },
  {
    "start": "4536140",
    "end": "4541000"
  }
]