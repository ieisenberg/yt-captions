[
  {
    "start": "0",
    "end": "5920"
  },
  {
    "text": "OK, so today we're going\nto start talking about meta reinforcement learning.",
    "start": "5920",
    "end": "11230"
  },
  {
    "text": "We'll have three\nlectures on this topic, and this will be the first\none that covers the basics.",
    "start": "11230",
    "end": "16955"
  },
  {
    "text": "OK, so for today, we're going\nto start talking about meta reinforcement learning. We're going to cover\nthe problem statement.",
    "start": "16955",
    "end": "23501"
  },
  {
    "text": "We're going to talk about\nhow black-box methods and optimization-based\nmethods can be translated to a meta-RL setting.",
    "start": "23502",
    "end": "30610"
  },
  {
    "text": "I think that a lot of the\ncontent in this lecture is fairly straightforward.",
    "start": "30610",
    "end": "35750"
  },
  {
    "text": "It's mostly a matter of\nbasically taking the concepts that we were thinking about\nbefore in meta-learning, and trying to translate them\nto a reinforcement learning",
    "start": "35750",
    "end": "42723"
  },
  {
    "text": "setting. And so we'll cover also a lot\nof really kind of examples of what this looks like.",
    "start": "42723",
    "end": "49488"
  },
  {
    "text": "Black-box meta-RL methods are\ngoing to come up in Homework 4. You'll basically\nget an opportunity to play around\nwith these methods",
    "start": "49488",
    "end": "55899"
  },
  {
    "text": "and get a sense\nfor how they work. But they-- kind of the\nprimary part of Homework 4",
    "start": "55900",
    "end": "62560"
  },
  {
    "text": "is actually going to be\non learning to explore, which is what we're going to be\ncovering in lecture on Monday",
    "start": "62560",
    "end": "69490"
  },
  {
    "text": "next week. And the goals of this lecture\nis to provide an understanding",
    "start": "69490",
    "end": "75540"
  },
  {
    "text": "of the meta-RL problem\nstatement and a setup to help you\nunderstand the basics of black-box meta-RL\nmethods and the basics",
    "start": "75540",
    "end": "82225"
  },
  {
    "text": "and some of the\nchallenges that come up with optimization-based\nmeta-RL algorithms.",
    "start": "82225",
    "end": "88530"
  },
  {
    "text": "Cool. So let's get started with\nthe problem statement. And for this, we'll recap\nbriefly some of the things",
    "start": "88530",
    "end": "95300"
  },
  {
    "text": "that we covered in\nthe previous lectures to build up to the\nproblem statement.",
    "start": "95300",
    "end": "100970"
  },
  {
    "text": "So in the past few\nlectures, we've introduced model-free and\nmodel-based reinforcement learning methods and\nwe introduced them",
    "start": "100970",
    "end": "108460"
  },
  {
    "text": "in within this kind of general\nframework of a reinforcement learning algorithm that iterates\nbetween generating samples,",
    "start": "108460",
    "end": "115180"
  },
  {
    "text": "fitting some model to\nestimate the return, and using that to\nimprove the policy.",
    "start": "115180",
    "end": "121210"
  },
  {
    "text": "For this box right here,\nthere is a few different ways that you can\nestimate the return. One is to use Monte Carlo\nrollouts of the policy,",
    "start": "121210",
    "end": "130389"
  },
  {
    "text": "one is to fit a\nmodel that predicts the cumulative\nreturns, and a third",
    "start": "130389",
    "end": "137650"
  },
  {
    "text": "is to fit a model\nof the dynamics of the one-step dynamics\nwhich can also give you",
    "start": "137650",
    "end": "143830"
  },
  {
    "text": "an estimate of the return if you\naccumulate the rollouts using",
    "start": "143830",
    "end": "150070"
  },
  {
    "text": "this one-step model. And then once you have one of\nthese returns, you can either--",
    "start": "150070",
    "end": "156588"
  },
  {
    "text": "one of these estimates\nof the return, you can either use it to\ndirectly improve the policy, use it to compute the\npolicy with regard",
    "start": "156588",
    "end": "164400"
  },
  {
    "text": "to your current function\nin a Q-learning approach or optimize a policy\nby, for example,",
    "start": "164400",
    "end": "170790"
  },
  {
    "text": "back-propagating gradients\nfrom a model into the policy.",
    "start": "170790",
    "end": "176219"
  },
  {
    "text": "So these are kind of a\ngeneral class of reinforcement learning approaches.",
    "start": "176220",
    "end": "181500"
  },
  {
    "text": "Beyond reinforcement\nlearning and back in the supervised\nlearning setting, we also talked about these\ndifferent problems settings,",
    "start": "181500",
    "end": "186900"
  },
  {
    "text": "multi-task learning problem\nsetting, a transfer learning problem setting, and the\nmeta-learning problem.",
    "start": "186900",
    "end": "192870"
  },
  {
    "text": "One key aspect of\nall these settings is that we assume that the\ntasks must share some structure.",
    "start": "192870",
    "end": "199290"
  },
  {
    "text": "And the key idea of\nmeta-reinforcement learning of what we'll talk\nabout today is basically",
    "start": "199290",
    "end": "204930"
  },
  {
    "text": "taking this\nmeta-learning problem and instead of using\nsupervised learning tasks,",
    "start": "204930",
    "end": "210570"
  },
  {
    "text": "each of the tasks will be\nreinforcement learning tasks. So a reinforcement\nlearning task,",
    "start": "210570",
    "end": "216750"
  },
  {
    "text": "as we defined it\nbefore, basically corresponds to a Markov\ndecision process or tasks",
    "start": "216750",
    "end": "221910"
  },
  {
    "text": "may vary in terms of different\nelements of that Markov decision process, especially\nthe dynamics or the reward.",
    "start": "221910",
    "end": "227055"
  },
  {
    "text": " And so what we'll\ntalk about today is that meta reinforcement\nlearning will basically",
    "start": "227055",
    "end": "233310"
  },
  {
    "text": "just correspond to meta-learning\nwhere each of your tasks are these MDPs. So basically, your\ngoal will be, you're",
    "start": "233310",
    "end": "240659"
  },
  {
    "text": "given some set of training tasks\nor some set of training MDPs and then your goal will\nbe to quickly learn",
    "start": "240660",
    "end": "246750"
  },
  {
    "text": "to solve a new MDP, based\non your previous experience with those other MDPs.",
    "start": "246750",
    "end": "252153"
  },
  {
    "text": " OK, so that's kind of\nthe high level overview.",
    "start": "252154",
    "end": "260958"
  },
  {
    "text": "Another view that we gave\non the meta-learning problem was one of basically\nmapping from a training",
    "start": "260959",
    "end": "267620"
  },
  {
    "text": "dataset and a test input to\na corresponding prediction",
    "start": "267620",
    "end": "273470"
  },
  {
    "text": "for that test input. So some sort of kind of\nfunction or procedure",
    "start": "273470",
    "end": "278780"
  },
  {
    "text": "that takes in the training set,\ntakes as input the test input and makes a prediction\nbased on what it learned from the training set.",
    "start": "278780",
    "end": "286590"
  },
  {
    "text": "And the way that\nthis is learned is through a dataset of datasets.",
    "start": "286590",
    "end": "292479"
  },
  {
    "text": "With this view, we can\ntranslate this view towards the meta-reinforcement\nlearning problem statement",
    "start": "292480",
    "end": "298810"
  },
  {
    "text": "by basically\nreplacing supervised learning with\nreinforcement learning and then translating that to\nthe meta-learning setting.",
    "start": "298810",
    "end": "309080"
  },
  {
    "text": "So in particular, in\nreinforcement learning, our goal is not to\nmap from x to y, is to map from\nstates to actions.",
    "start": "309080",
    "end": "317139"
  },
  {
    "text": "And the way that we do this is\nnot through input output pairs, but experience that has\ncorresponding rewards that",
    "start": "317140",
    "end": "324040"
  },
  {
    "text": "indicate what kinds of\nactions and states are good, and what kinds of states\nand actions are bad.",
    "start": "324040",
    "end": "330820"
  },
  {
    "text": "Now to translate\nthis to the setting where we want to\nlearn many tasks and ultimately learning\na new task quickly,",
    "start": "330820",
    "end": "338320"
  },
  {
    "text": "we can kind of\nraise it up a level like we did with\nsupervised learning.",
    "start": "338320",
    "end": "344230"
  },
  {
    "text": "In this case, our goal will\nbe given a small amount of experience from a new\ntask, can we learn a policy",
    "start": "344230",
    "end": "352240"
  },
  {
    "text": "from that experience? So given a small\namount of experience and the current\nstate, can we predict an action that will do well\nin that particular task?",
    "start": "352240",
    "end": "363292"
  },
  {
    "text": "So you can view it\nas this mapping. Again, that takes as\ninput some experience and the current state\nand outputs the action.",
    "start": "363292",
    "end": "370570"
  },
  {
    "text": "And then the way that you\nlearn this is a setting where you have a set of datasets,\nand each dataset contains",
    "start": "370570",
    "end": "378010"
  },
  {
    "text": "experience collected\nfor each of the tasks. ",
    "start": "378010",
    "end": "385330"
  },
  {
    "text": "OK, so this is kind\nof one way that we can view the meta-reinforcement\nlearning problem.",
    "start": "385330",
    "end": "392200"
  },
  {
    "text": "And then a core part of\nthis problem, of course, is to design what this\nprocedure looks like",
    "start": "392200",
    "end": "397889"
  },
  {
    "text": "and how we're going to\noptimize this procedure. And then also something\nthat's different",
    "start": "397890",
    "end": "403919"
  },
  {
    "text": "than the standard meta\nsupervised learning problem is collecting appropriate data. How do we decide?",
    "start": "403920",
    "end": "410310"
  },
  {
    "text": "Now that we're in this\nreinforcement learning setting, we get to control the\ndata collection process. So we need to figure out how\nwe should collect this data,",
    "start": "410310",
    "end": "418229"
  },
  {
    "text": "how we should collect kind\nof rollouts from our policy, and also how we should collect\nkind of our meta training data",
    "start": "418230",
    "end": "426000"
  },
  {
    "text": "as well. This first part where we want to\nfigure out how to collect data,",
    "start": "426000",
    "end": "432535"
  },
  {
    "text": "this is kind of the\nproblem of learning how to explore for a task\nbecause collecting this data",
    "start": "432535",
    "end": "437820"
  },
  {
    "text": "is exploring for that\ntask in a way that allows you to solve that task.",
    "start": "437820",
    "end": "442870"
  },
  {
    "text": "And so this part is what\nwe're going to focus on in the next lecture on Monday. Today, we're just going to think\nabout designing these learning",
    "start": "442870",
    "end": "451110"
  },
  {
    "text": "procedures and optimizing them\nwith reinforcement learning. ",
    "start": "451110",
    "end": "459419"
  },
  {
    "text": "OK, so as a kind of\ninstantiating example for this,",
    "start": "459420",
    "end": "465640"
  },
  {
    "text": "in meta-reinforcement learning,\nlet's say the different tasks correspond to navigating\ndifferent mazes.",
    "start": "465640",
    "end": "472729"
  },
  {
    "text": "So ultimately, what\nwe'd like to be able to do is given a\nsmall amount of experience",
    "start": "472730",
    "end": "477890"
  },
  {
    "text": "in a new maze that we\nhaven't seen before, can we learn how to solve\nthat maze essentially,",
    "start": "477890",
    "end": "484910"
  },
  {
    "text": "and allow us to navigate to\nthe goal position in that maze? So in particular what\nthis is showing is",
    "start": "484910",
    "end": "490750"
  },
  {
    "text": "this is showing\nsome trajectories in the maze where\nyou're exploring the maze and so forth. And then given this\nexperience in the maze,",
    "start": "490750",
    "end": "497080"
  },
  {
    "text": "if you're then kind of\nre-instantiated at this point, you want your policy to be\nable to directly go to the goal",
    "start": "497080",
    "end": "502810"
  },
  {
    "text": "and directly navigate that maze.  So this is what you-- this is\nkind of the equivalent few-shot",
    "start": "502810",
    "end": "510310"
  },
  {
    "text": "learning problem in a\nreinforcement learning setting-- few-shot reinforcement learning.",
    "start": "510310",
    "end": "517065"
  },
  {
    "text": "This is what we want to\nbe able to accomplish at meta-test time. And the way that we can go\nabout accomplishing this",
    "start": "517065",
    "end": "523370"
  },
  {
    "text": "is by training on lots of other\nmazes or lots of other tasks.",
    "start": "523370",
    "end": "528450"
  },
  {
    "text": "So you can learn how to\nquickly explore and navigate a number of different mazes\nso that when we're then",
    "start": "528450",
    "end": "536020"
  },
  {
    "text": "given a new maze at meta-test\ntime, we can quickly solve it. ",
    "start": "536020",
    "end": "543390"
  },
  {
    "text": "Any questions on how this works? ",
    "start": "543390",
    "end": "551959"
  },
  {
    "text": "So the data for each task, there\nwould be multiple trajectories from that task or one episode?",
    "start": "551960",
    "end": "561154"
  },
  {
    "text": "Is it multiple episodes or\none episode from each task? Yeah, that's a good question. So there's two\ndifferent variants",
    "start": "561154",
    "end": "568740"
  },
  {
    "text": "of the meta-reinforcement\nlearning problem. One I'll refer to is\nthe episodic variant where you basically have\nk trajectories that you're",
    "start": "568740",
    "end": "575940"
  },
  {
    "text": "trying to learn\nfrom, and one I'll call the online variant, which\nyour data now just contains",
    "start": "575940",
    "end": "582120"
  },
  {
    "text": "k time steps within that task. So you can consider either\nof these or something",
    "start": "582120",
    "end": "591490"
  },
  {
    "text": "in between basically. But yeah, you're\neither trying to learn from a small amount\nof kind of time steps",
    "start": "591490",
    "end": "596520"
  },
  {
    "text": "or a small amount\nof trajectories. In both cases, it's a\nsmall amount of experience. Different problem\nformulations will",
    "start": "596520",
    "end": "602970"
  },
  {
    "text": "consider different\nvariants of the problem. OK, thanks. ",
    "start": "602970",
    "end": "613820"
  },
  {
    "text": "There's also a question\nin the chat that's asking when we fit\na model to measure the reward, what is the reward?",
    "start": "613820",
    "end": "620760"
  },
  {
    "text": "Is it the policy or the\nreward of the policy? So a Q function\nwill be estimating",
    "start": "620760",
    "end": "627960"
  },
  {
    "text": "the cumulative reward\nof a certain policy. So this is given by\nQ pi, for example.",
    "start": "627960",
    "end": "635149"
  },
  {
    "text": "Then something like a\nmodel-based approach will be learning to predict\nthe next state and also the reward for that current\nstate action and next state.",
    "start": "635150",
    "end": "644920"
  },
  {
    "text": "So it kind of depends\non the problems setting or on the approach. ",
    "start": "644920",
    "end": "655873"
  },
  {
    "text": "OK, so that's the\nmeta-RL problem.  Next, we'll talk about\nblack-box approaches,",
    "start": "655873",
    "end": "661847"
  },
  {
    "text": "if there are any more\nquestions on the problem. ",
    "start": "661847",
    "end": "667880"
  },
  {
    "text": "OK.  So for our black-box\nmeta-RL approaches,",
    "start": "667880",
    "end": "674290"
  },
  {
    "text": "we can basically\ntranslate all the ideas that we had for\nblack-box meta learning to this reinforcement\nlearning setting where we're",
    "start": "674290",
    "end": "680740"
  },
  {
    "text": "going to be training a\nneural network to take as input a data set as\nwell as your current state",
    "start": "680740",
    "end": "686500"
  },
  {
    "text": "and output the next action. Now, one of the things we\ntalked about in black-box meta",
    "start": "686500",
    "end": "691880"
  },
  {
    "text": "learning is that typically when\nyou have some data, D train,",
    "start": "691880",
    "end": "696950"
  },
  {
    "text": "there's no kind of natural\nordering of that data set. So there's kind of-- you may want to kind\nof process that data",
    "start": "696950",
    "end": "703550"
  },
  {
    "text": "in a permutation invariant way. But in reinforcement\nlearning, the data",
    "start": "703550",
    "end": "709550"
  },
  {
    "text": "does actually have\ntemporal structure. So one very common approach\nfor processing this data",
    "start": "709550",
    "end": "714589"
  },
  {
    "text": "is to use some sort\nof recurrent network that process the\ndata sequentially.",
    "start": "714590",
    "end": "720940"
  },
  {
    "text": "And specifically, what this\nlooks like is basically you rule out this\nrecurrent network.",
    "start": "720940",
    "end": "728080"
  },
  {
    "text": "Everything up until\nthe current timestep is basically the training\ndata for that task.",
    "start": "728080",
    "end": "736959"
  },
  {
    "text": "And this training data includes\nstates, actions, and rewards. And you basically pass on that\ndata as it becomes available.",
    "start": "736960",
    "end": "744740"
  },
  {
    "text": "So once you take action a1,\nfor example, from state s1, then you actually-- after\nyou take that action,",
    "start": "744740",
    "end": "751060"
  },
  {
    "text": "you observe the reward\nand also the next state. So you pass in that next\nstate and that reward",
    "start": "751060",
    "end": "756760"
  },
  {
    "text": "that you just received as\ninput to your recurrent policy and predict the next action.",
    "start": "756760",
    "end": "762400"
  },
  {
    "start": "762400",
    "end": "767420"
  },
  {
    "text": "OK, now, one\nquestion for you is,",
    "start": "767420",
    "end": "773110"
  },
  {
    "text": "how is this approach different\nfrom simply just running reinforcement learning\nusing a recurrent policy?",
    "start": "773110",
    "end": "782240"
  },
  {
    "text": "And feel free to answer in\nchat or by raising your hand. And specifically, I\nhave at least two things",
    "start": "782240",
    "end": "788600"
  },
  {
    "text": "in mind with regard to\nhow this general algorithm is a little bit different than\njust running reinforcement",
    "start": "788600",
    "end": "794210"
  },
  {
    "text": "learning with a\nrecurrent policy.  Any thoughts on how\nit is different?",
    "start": "794210",
    "end": "802029"
  },
  {
    "text": "So there's a few\nanswers in the chat. So we don't use the\npredicted actions",
    "start": "802030",
    "end": "810190"
  },
  {
    "text": "as the D train is fixed. Not quite sure what\nyou're suggesting.",
    "start": "810190",
    "end": "815330"
  },
  {
    "text": " Said each timestep\nis not dependent",
    "start": "815330",
    "end": "820740"
  },
  {
    "text": "on the previous state,\nbut all previous steps. So this is pretty close\nto one of the things",
    "start": "820740",
    "end": "828600"
  },
  {
    "text": "that I was going to\nsuggest that the reward can be different from test time.",
    "start": "828600",
    "end": "834040"
  },
  {
    "text": "So this is one of\nthe differences is that you're actually\ngoing to be training this recurrent policy across\nmultiple different MDPs",
    "start": "834040",
    "end": "840329"
  },
  {
    "text": "with different reward functions.  And this reward\nfunction is kind of",
    "start": "840330",
    "end": "846269"
  },
  {
    "text": "passed as input to\nthe policy, which is a bit different than\nhow you would typically use a recurrent policy.",
    "start": "846270",
    "end": "853110"
  },
  {
    "text": "And then I was\ngetting to what we're saying about each timestep\nnot being dependent",
    "start": "853110",
    "end": "858420"
  },
  {
    "text": "just on the previous step\nstate, but all previous steps. The other core thing that's\ndifferent about this policy",
    "start": "858420",
    "end": "865180"
  },
  {
    "text": "is that the hidden state of\nyour recurrent neural network is actually maintained across\nepisodes within a task.",
    "start": "865180",
    "end": "873700"
  },
  {
    "text": "So typically, if you were\nto train a recurrent policy, you would just kind of roll\nout that recurrent network",
    "start": "873700",
    "end": "879940"
  },
  {
    "text": "throughout that\nepisode, and then when you start a new episode,\nyou restart the hidden state.",
    "start": "879940",
    "end": "885520"
  },
  {
    "text": "But in this case,\nbecause we want to learn across\nmultiple episodes, we want to actually\nkind of maintain",
    "start": "885520",
    "end": "892043"
  },
  {
    "text": "what we've learned\nacross those episodes rather than completely\nresetting the hidden state.",
    "start": "892043",
    "end": "897870"
  },
  {
    "text": "So what this looks like is say\nthat we have kind of episodes",
    "start": "897870",
    "end": "905410"
  },
  {
    "text": "of length capital\nT. Then we're going to be ruling out this policy\nfor T timesteps right here.",
    "start": "905410",
    "end": "913990"
  },
  {
    "text": "Then keep on maintaining this\nhidden state across the episode boundary and we\ncontinue rolling out",
    "start": "913990",
    "end": "920860"
  },
  {
    "text": "that policy to build to learn\nacross multiple episodes. So this is kind of episode\n1, this is episode 2,",
    "start": "920860",
    "end": "926980"
  },
  {
    "text": "and so forth. And then if you then want to\nstart learning a new task,",
    "start": "926980",
    "end": "932779"
  },
  {
    "text": "then that's kind of\nthe only time when you'll reset the hidden state\nand start learning again",
    "start": "932780",
    "end": "939440"
  },
  {
    "text": "for that new task. ",
    "start": "939440",
    "end": "944460"
  },
  {
    "text": "So [MUTED] is asking, if you\nrestart the hidden state, how does the model learn? ",
    "start": "944460",
    "end": "950589"
  },
  {
    "text": "So the model is able to\nlearn like within a task. So you don't reset the\nhidden state within that task",
    "start": "950590",
    "end": "957247"
  },
  {
    "text": "because it needs to be\nable to learn how to solve that task across episodes. You only reset the\nhidden state when",
    "start": "957247",
    "end": "962579"
  },
  {
    "text": "you're trying to then\nlearn a new task. So this would be like,\nI don't know, task i,",
    "start": "962580",
    "end": "971130"
  },
  {
    "text": "this might be task j. ",
    "start": "971130",
    "end": "976781"
  },
  {
    "text": "[MUTED] is asking, do\nwe start with D train, or are we making a D train\nfrom the intermediate actions",
    "start": "976781",
    "end": "982035"
  },
  {
    "text": "outputted by each\nmodel and constantly generate new trajectories\nfrom the model?",
    "start": "982035",
    "end": "987930"
  },
  {
    "text": "So yeah, so the typical-- the kind of the way that\nyou can view this is we are constantly adding to D\ntrain and at each time step,",
    "start": "987930",
    "end": "998740"
  },
  {
    "text": "for example, when we were\nright at the beginning, when we were right\nhere, D train just corresponds to\nS1, 0, a1, and r1.",
    "start": "998740",
    "end": "1007589"
  },
  {
    "text": "And then later on, over\nhere, D train corresponds to all of your experience so\nfar throughout this trajectory.",
    "start": "1007590",
    "end": "1015600"
  },
  {
    "text": "There are situations. There are-- well, I guess\nwe'll get into this later, but there are situations where\nyou may just-- you may add to D",
    "start": "1015600",
    "end": "1021750"
  },
  {
    "text": "train kind of\nincrementally, you just add one trajectory then\nanother trajectory, rather than adding\nthem step by step.",
    "start": "1021750",
    "end": "1027929"
  },
  {
    "text": "But for these approaches,\nyou're basically adding D train at every single time step. ",
    "start": "1027930",
    "end": "1037819"
  },
  {
    "text": "OK, so concretely,\nthis is sort of like what the model looks like. What the algorithm looks\nlike is we sample a task.",
    "start": "1037819",
    "end": "1044069"
  },
  {
    "text": "We then roll out this\npolicy for N episodes,",
    "start": "1044069",
    "end": "1049400"
  },
  {
    "text": "which is N episodes\nis the budget that we have to learn the task,\nincrementally adding to D train",
    "start": "1049400",
    "end": "1056289"
  },
  {
    "text": "as we go. This would be under the dynamics\nand the reward for the task",
    "start": "1056290",
    "end": "1062010"
  },
  {
    "text": "that we sampled. Then we essentially\nstore that sequence",
    "start": "1062010",
    "end": "1067110"
  },
  {
    "text": "of episodes in a replay buffer\nfor this particular task. And then we update our policy to\nmaximize the discounted return",
    "start": "1067110",
    "end": "1075210"
  },
  {
    "text": "for all of the tasks\nin our replay buffer.  And then you can kind\nof repeat this process.",
    "start": "1075210",
    "end": "1083580"
  },
  {
    "text": "And note that when you're\nrolling out this policy, you're actually optimizing\nnot just for return",
    "start": "1083580",
    "end": "1091225"
  },
  {
    "text": "after a fixed data\nset size, but you're optimizing for the return\nacross a full learning process.",
    "start": "1091225",
    "end": "1097419"
  },
  {
    "text": "So in many ways, you're\noptimizing actually for regret. You're optimizing for kind\nof not just for return",
    "start": "1097420",
    "end": "1104670"
  },
  {
    "text": "after a fixed amount\nof data, but for return as you're exploring.",
    "start": "1104670",
    "end": "1109720"
  },
  {
    "text": "And this allows\nyou to optimize end to end both for a very\nefficient exploration and good exploitation\nof that data,",
    "start": "1109720",
    "end": "1118200"
  },
  {
    "text": "and you're optimizing for\nkind of a good trade off between exploration\nand exploitation.",
    "start": "1118200",
    "end": "1124440"
  },
  {
    "text": "OK, questions about how\nthis algorithm works? Good question. So someone is asking\nwhat does a rollout mean?",
    "start": "1124440",
    "end": "1131750"
  },
  {
    "text": "This is actually a term that\nI didn't quite understand when I was learning things. So by rollout, I just\nmean a trajectory.",
    "start": "1131750",
    "end": "1139890"
  },
  {
    "text": "So it's basically a\nway to say that you are kind of rolling\nout the policy or you're incrementally\nselecting actions",
    "start": "1139890",
    "end": "1148039"
  },
  {
    "text": "from the policy, executing\nthat action in the environment, and so forth. So basically, just\ncorresponds to a trajectory",
    "start": "1148040",
    "end": "1155840"
  },
  {
    "text": "that is acquired by\nexecuting a certain policy. ",
    "start": "1155840",
    "end": "1167975"
  },
  {
    "text": "OK, so you can\nalso think of this as like execute, for example. Execute the policy\nfor an episode.",
    "start": "1167975",
    "end": "1175320"
  },
  {
    "text": "You get some trajectory,\nand you store that trajectory of episodes\nin your replay buffer.",
    "start": "1175320",
    "end": "1182075"
  },
  {
    "text": "Yeah, and then\n[MUTED] is asking, what is it like\nfor meta testing? At meta testing, you're\ngiven a new task and you",
    "start": "1182075",
    "end": "1188760"
  },
  {
    "text": "can basically just-- it basically just corresponds\nto these first two steps here where you're given\na task and you just",
    "start": "1188760",
    "end": "1196230"
  },
  {
    "text": "rollout or execute the\npolicy that you have where you're incrementally\nadding to your training data.",
    "start": "1196230",
    "end": "1202230"
  },
  {
    "start": "1202230",
    "end": "1208455"
  },
  {
    "text": "This is asking, how\ndoes this algorithm encourage exploration? So I guess there's a few\ndifferent points here.",
    "start": "1208455",
    "end": "1216740"
  },
  {
    "text": "So there's exploration\nat the meta level and also exploration\nwithin a task.",
    "start": "1216740",
    "end": "1221990"
  },
  {
    "text": "One way that it will encourage\nexploration within a task is that say, for\nexample, maybe--",
    "start": "1221990",
    "end": "1229272"
  },
  {
    "text": "say for example, you're\nin the maze environment that we had before and you\nget a reward if you go here.",
    "start": "1229272",
    "end": "1234650"
  },
  {
    "text": "And you start right here and\nmaybe there are some walls and so forth that make\nit kind of non-trivial",
    "start": "1234650",
    "end": "1242600"
  },
  {
    "text": "to get to the reward. Actually should\nremove that last one. And so this is going to\nencourage exploration",
    "start": "1242600",
    "end": "1249140"
  },
  {
    "text": "because if, for example,\ndifferent tasks have rewards",
    "start": "1249140",
    "end": "1254420"
  },
  {
    "text": "in different positions, so\nmaybe this one looks like this. This maze looks\nsomething like this, then",
    "start": "1254420",
    "end": "1261700"
  },
  {
    "text": "in different tasks in order\nfor this policy to get rewards within a task, it needs\nto be able to execute",
    "start": "1261700",
    "end": "1268200"
  },
  {
    "text": "actions that allow it to\nidentify where the rewards are. And this policy at\nthe top can basically",
    "start": "1268200",
    "end": "1276870"
  },
  {
    "text": "encapsulate both the exploration\nprocess and the process of actually exploiting that.",
    "start": "1276870",
    "end": "1283200"
  },
  {
    "text": "And this recurrent\npolicy won't actually be successful at\ngetting good rewards",
    "start": "1283200",
    "end": "1288720"
  },
  {
    "text": "unless it has actually\nsufficiently explored its environment to find\nwhere the rewards are.",
    "start": "1288720",
    "end": "1294980"
  },
  {
    "text": "There's also exploration\nat the meta level as well, and you might actually\nbe able to kind of find",
    "start": "1294980",
    "end": "1303800"
  },
  {
    "text": "a good strategy for learning\nand find a good strategy for finding rewards. ",
    "start": "1303800",
    "end": "1310659"
  },
  {
    "text": "That sort of exploration\nat the meta level really comes down\nto the objective that you're using for\nupdating the policy.",
    "start": "1310660",
    "end": "1316820"
  },
  {
    "text": "And here kind of standard\napproaches for exploration can be used to\ntry to incentivize different kinds of approaches.",
    "start": "1316820",
    "end": "1323617"
  },
  {
    "text": " So [MUTED] is\nasking, why can't we",
    "start": "1323617",
    "end": "1329929"
  },
  {
    "text": "use this algorithm for\na standard RL as well? This algorithm written\nout here requires that you",
    "start": "1329930",
    "end": "1336530"
  },
  {
    "text": "have multiple different tasks. I guess in principle,\nyou can also apply this to a\nsingle task where",
    "start": "1336530",
    "end": "1343035"
  },
  {
    "text": "you're optimizing for a good\ntradeoff between exploration and exploitation for\nthat single task.",
    "start": "1343035",
    "end": "1348590"
  },
  {
    "text": "However, if you just do\nit for a single task, then through this\nprocess, you'll learn a good\nstrategy for the task",
    "start": "1348590",
    "end": "1354420"
  },
  {
    "text": "and you'll learn a good way to\nexploit and get lots of rewards for that task. And then if you're given kind\nof at meta test time, given",
    "start": "1354420",
    "end": "1362330"
  },
  {
    "text": "the same task, then\nit doesn't make sense to explore again\nfor that same task",
    "start": "1362330",
    "end": "1368583"
  },
  {
    "text": "because you've already learned\na good strategy for solving that task. ",
    "start": "1368583",
    "end": "1379610"
  },
  {
    "text": "OK, so what do-- one thing I want to touch\non is how do you actually instantiate this algorithm\nwith different architectures",
    "start": "1379610",
    "end": "1386929"
  },
  {
    "text": "and optimizers? I'll highlight a few\ncommon approaches. So one common approach\nis to use some sort",
    "start": "1386930",
    "end": "1393980"
  },
  {
    "text": "of RNN-like architecture like\nwe were just talking about. And this might include\nthings like an LSTM or a GRU.",
    "start": "1393980",
    "end": "1404570"
  },
  {
    "text": "And then again, you're rolling\nthis out across episodes. And then the way that you could\noptimize this recurrent policy",
    "start": "1404570",
    "end": "1412070"
  },
  {
    "text": "might be with something\nlike trust region policy optimization, which is a policy\ngradient method or A3C, which",
    "start": "1412070",
    "end": "1420110"
  },
  {
    "text": "is Asynchronous Advantage\nActor Critic I believe. The takeaway here is that\nboth these approaches are",
    "start": "1420110",
    "end": "1426830"
  },
  {
    "text": "on policy approaches and kind\nof are updating this policy",
    "start": "1426830",
    "end": "1433399"
  },
  {
    "text": "with the current-- the latest\nexperience that it collected. And these were summarized\nby these two works",
    "start": "1433400",
    "end": "1439970"
  },
  {
    "text": "right here, which were\ntwo of the works that used some of the\nfirst works that",
    "start": "1439970",
    "end": "1445940"
  },
  {
    "text": "showed with modern deep neural\nnetworks and deep reinforcement learning algorithms\nthat you can learn how to learn for\nthings like navigation",
    "start": "1445940",
    "end": "1454669"
  },
  {
    "text": "tasks and other tasks. Also the first author of\nthe second paper, Jane Wang",
    "start": "1454670",
    "end": "1461549"
  },
  {
    "text": "is going to be giving a guest\nlecture in a couple of weeks, and she'll be talking about a\ncognitive science perspective",
    "start": "1461550",
    "end": "1468120"
  },
  {
    "text": "on meta-reinforcement learning. So this is kind of a basic\napproach that you could use.",
    "start": "1468120",
    "end": "1475100"
  },
  {
    "text": "One paper followed up\non these, and instead of using an RNN\narchitecture, they used a architecture\nthat uses attention.",
    "start": "1475100",
    "end": "1483260"
  },
  {
    "text": "It actually interleaved\nattention and 1D convolutions. We actually-- I included\nthis as an example",
    "start": "1483260",
    "end": "1490220"
  },
  {
    "text": "of an architecture for the\nmeta-supervised learning problem as well. And this paper\nactually showed results",
    "start": "1490220",
    "end": "1495480"
  },
  {
    "text": "for both\nmeta-supervised learning and meta-reinforcement learning.",
    "start": "1495480",
    "end": "1500750"
  },
  {
    "text": "This is before things\nlike transformers existed, but you can also use\nthings like transformers and other attention-based\narchitectures",
    "start": "1500750",
    "end": "1506930"
  },
  {
    "text": "to integrate your\nprevious experience. And then the last\nexample that I'll mention",
    "start": "1506930",
    "end": "1513190"
  },
  {
    "text": "is this architecture right here. The way that this\narchitecture looks",
    "start": "1513190",
    "end": "1518860"
  },
  {
    "text": "is it is actually a feed\nforward architecture where it passes transitions\ninto a single feed",
    "start": "1518860",
    "end": "1526510"
  },
  {
    "text": "forward neural network and then\nintegrates those to produce",
    "start": "1526510",
    "end": "1532540"
  },
  {
    "text": "a single context z, our\nkind of signal embedding",
    "start": "1532540",
    "end": "1538060"
  },
  {
    "text": "z based on this context here.",
    "start": "1538060",
    "end": "1543130"
  },
  {
    "text": "And then this\nembedding of the task is passed to the Q\nfunction in the policy",
    "start": "1543130",
    "end": "1549280"
  },
  {
    "text": "and use a Q learning\nlike method, in particular a method\ncalled soft actor critic",
    "start": "1549280",
    "end": "1554679"
  },
  {
    "text": "to optimize for the tasks.  And this method is\noff-policy in the sense",
    "start": "1554680",
    "end": "1563269"
  },
  {
    "text": "that they're able to store\ndata in a replay buffer, and we call those\nexperiences to train",
    "start": "1563270",
    "end": "1569360"
  },
  {
    "text": "the policy in a Q function. ",
    "start": "1569360",
    "end": "1577420"
  },
  {
    "text": "OK, you might ask,\nwell, why would you use a feed-forward\narchitecture if there's all this kind of\ntemporal structure?",
    "start": "1577420",
    "end": "1582507"
  },
  {
    "text": "One thing that is nice about\nthis feed-forward architecture is that it is a lot more\nefficient because it's",
    "start": "1582507",
    "end": "1588039"
  },
  {
    "text": "really easy to paralyze\nfeed-forward calls to a neural network. Whereas with a recurrent\nneural network, you have to process the\ndata sequentially, which",
    "start": "1588040",
    "end": "1595149"
  },
  {
    "text": "ends up being\ncomputationally a lot slower. ",
    "start": "1595150",
    "end": "1604972"
  },
  {
    "text": "OK, so now that we've\nintroduced the algorithm in different kinds\nof architectures, I want to go through a\ncouple different examples",
    "start": "1604973",
    "end": "1610940"
  },
  {
    "text": "of how these algorithms\nhave been applied. So the first is the\nmaze navigation example",
    "start": "1610940",
    "end": "1618950"
  },
  {
    "text": "that I mentioned. And also, before I\ngo there, there's a kind of a quick clarification\non the previous one that's",
    "start": "1618950",
    "end": "1625520"
  },
  {
    "text": "asking the subscripts or\nepisodes in the bottom diagram of slide 19. ",
    "start": "1625520",
    "end": "1635890"
  },
  {
    "text": "So in this case,\nthese subscripts are actually corresponding\nto transitions.",
    "start": "1635890",
    "end": "1642030"
  },
  {
    "text": "So they sampled a number\nof different transitions. These transitions may come\nfrom different episodes",
    "start": "1642030",
    "end": "1648270"
  },
  {
    "text": "or some of them may come\nfrom the same episode. You can either use an online\nvariant or an episodic variant. ",
    "start": "1648270",
    "end": "1655799"
  },
  {
    "text": "OK, so for the maze\nnavigation setting that I introduced\npreviously, this",
    "start": "1655800",
    "end": "1662650"
  },
  {
    "text": "is basically exactly\nthe experiment that a couple of\npapers have looked at including this paper called\na Simple Neural Attentive",
    "start": "1662650",
    "end": "1669310"
  },
  {
    "text": "Meta-Learner. And what they did is\nthey did meta-training on 1,000 different\nsmall mazes or they",
    "start": "1669310",
    "end": "1676995"
  },
  {
    "text": "were trying to learn how\nto quickly navigate in all of these different mazes. And then they tested\non held-out small mazes",
    "start": "1676995",
    "end": "1684009"
  },
  {
    "text": "and held-out large\nmazes with the goal of seeing if they\ncan learn to navigate these new mazes with a very\nsmall amount of experience",
    "start": "1684010",
    "end": "1691450"
  },
  {
    "text": "in those mazes. And the result looked\nsomething like this. So this is after the\nmeta-training process.",
    "start": "1691450",
    "end": "1699560"
  },
  {
    "text": "So what it's doing here\nis it's exploring the maze to figure out where the goal is.",
    "start": "1699560",
    "end": "1709090"
  },
  {
    "text": "So this is the first\nepisode at meta test time. And it finds where the goal is.",
    "start": "1709090",
    "end": "1714529"
  },
  {
    "text": "And then in the second\nepisode of meta test time, it's able to go\ndirectly to the goal.",
    "start": "1714530",
    "end": "1721000"
  },
  {
    "text": "So it's kind of-- yeah, learned how to\nexplore in a single episode. And then after that episode,\nit can exploit that strategy",
    "start": "1721000",
    "end": "1727320"
  },
  {
    "text": "to go directly to the goal. Here's another example,\nin a larger maze.",
    "start": "1727320",
    "end": "1733010"
  },
  {
    "text": "And actually to\nclarify, the policy only gets this first person viewpoint\nas input, not the right.",
    "start": "1733010",
    "end": "1738470"
  },
  {
    "text": "But the right is useful\nfor visualization purposes. Here's the first episode\nof meta-training time",
    "start": "1738470",
    "end": "1745400"
  },
  {
    "text": "or at meta-test time\nwhere it's exploring. And then after that\nexploration episode,",
    "start": "1745400",
    "end": "1750560"
  },
  {
    "text": "it's able to go\ndirectly to the goal. ",
    "start": "1750560",
    "end": "1757230"
  },
  {
    "text": "Cool. And then they also had\nsome quantitative results where they compared\nthis architecture that uses attention and\nconvolutions, to an architecture",
    "start": "1757230",
    "end": "1765290"
  },
  {
    "text": "that uses an LSTM. And here they're\nlooking at the--",
    "start": "1765290",
    "end": "1771860"
  },
  {
    "text": "they're looking at\nthe average time to find the goal within\neach of the two episodes.",
    "start": "1771860",
    "end": "1777230"
  },
  {
    "text": " And then this up here is just\nshowing like a random policy.",
    "start": "1777230",
    "end": "1784830"
  },
  {
    "text": "And first you can see the time\nto find the goal decreases from episode 1 to\nepisode 2, which",
    "start": "1784830",
    "end": "1790080"
  },
  {
    "text": "suggests that it's able\nto learn from episode 1 to more quickly find the goal.",
    "start": "1790080",
    "end": "1795570"
  },
  {
    "text": "And second, they\nfind that this kind of attention-based\narchitecture is better able to learn\nfrom data than an LSTM.",
    "start": "1795570",
    "end": "1802950"
  },
  {
    "text": " There's also a question\nin the chat that's asking,",
    "start": "1802950",
    "end": "1809470"
  },
  {
    "text": "can we speed up the learning of\nthe goal with some heuristics?",
    "start": "1809470",
    "end": "1814570"
  },
  {
    "text": "In general, you could\nimagine having like adding some heuristics to the process.",
    "start": "1814570",
    "end": "1822309"
  },
  {
    "text": "However, it's a\nlittle bit tricky to do that in these visual\nnavigation settings.",
    "start": "1822310",
    "end": "1830680"
  },
  {
    "text": "It's easy to do that,\nfor example, you had like this low\ndimensional representation of your current state.",
    "start": "1830680",
    "end": "1836740"
  },
  {
    "text": "But if you only have\nthe visual observations, then it's a bit tricky to try\nto process those observations",
    "start": "1836740",
    "end": "1842890"
  },
  {
    "text": "and provide heuristics\nbecause you don't actually know you're kind of low\ndimensional to your position",
    "start": "1842890",
    "end": "1848020"
  },
  {
    "text": "in the maze. That said, you may be\nable to kind of shape the reward function in some way\nor something to make the meta",
    "start": "1848020",
    "end": "1856410"
  },
  {
    "text": "training process faster\nor something else.  [MUTED] is asking, what\nis the reward like--",
    "start": "1856410",
    "end": "1863210"
  },
  {
    "text": "the distance at each point\nin the time from the goal? So the reward in this\ncase corresponded to,",
    "start": "1863210",
    "end": "1868940"
  },
  {
    "text": "I believe it was kind\nof a sparse reward where you received a positive\nreward when you were at the--",
    "start": "1868940",
    "end": "1877040"
  },
  {
    "text": "when you're at the goal\nposition, and either 0 or a small negative\nreward if you're not at the goal position.",
    "start": "1877040",
    "end": "1883700"
  },
  {
    "text": "And this makes exploration\nreally important. If it's something more like the\nnegative distance to the goal,",
    "start": "1883700",
    "end": "1889310"
  },
  {
    "text": "then you don't\nnecessarily need to-- it doesn't require as\nsophisticated of an exploration",
    "start": "1889310",
    "end": "1896090"
  },
  {
    "text": "strategy because you can\nsomewhat greedily act with respect to that. However, of course even\ngreedier approaches may not",
    "start": "1896090",
    "end": "1903410"
  },
  {
    "text": "work because there's\nalso walls and distances, may not capture\nthings like walls. ",
    "start": "1903410",
    "end": "1916190"
  },
  {
    "text": "OK, cool. So this is the first example\nfor these maze navigation tasks.",
    "start": "1916190",
    "end": "1923070"
  },
  {
    "text": "A second example\nof tasks is based on these continuous control\nproblems that correspond",
    "start": "1923070",
    "end": "1929460"
  },
  {
    "text": "to these simulated robots. So there is this half cheetah\nrobot, this humanoid robot,",
    "start": "1929460",
    "end": "1936180"
  },
  {
    "text": "this quadruped,\nand this 2D walker that is basically a bipedal\nrobot that has two legs.",
    "start": "1936180",
    "end": "1944370"
  },
  {
    "text": "And different tasks\ncorrespond to moving in different directions,\nrunning at different velocities,",
    "start": "1944370",
    "end": "1950430"
  },
  {
    "text": "and also being able to handle\ndifferent physical dynamics like different\nfriction and so forth.",
    "start": "1950430",
    "end": "1955970"
  },
  {
    "text": " And one of the things that\nthis paper in particular",
    "start": "1955970",
    "end": "1962790"
  },
  {
    "text": "wanted to study is that\nmeta-reinforcement learning algorithms are extremely\nefficient at learning",
    "start": "1962790",
    "end": "1968210"
  },
  {
    "text": "new tasks, but the\nmeta-training process may not be that efficient.",
    "start": "1968210",
    "end": "1975023"
  },
  {
    "text": "Basically, the\nprocess of learning how to learn on all\nthe meta-training tasks may require a lot of data.",
    "start": "1975023",
    "end": "1981860"
  },
  {
    "text": "So kind of one\nquestion is, would you expect an off-policy\nmeta-RL algorithm",
    "start": "1981860",
    "end": "1989559"
  },
  {
    "text": "to be more or less efficient\nthan an on-policy meta-RL algorithm? And recall that an\noff-policy meta-RL algorithm",
    "start": "1989560",
    "end": "1997120"
  },
  {
    "text": "is able to store and reuse data,\nwhereas an on-policy meta-RL algorithm basically\njust uses only the data",
    "start": "1997120",
    "end": "2004800"
  },
  {
    "text": "from the current policy. ",
    "start": "2004800",
    "end": "2009970"
  },
  {
    "text": "So you can answer in chat or\nraise your hand if you want.",
    "start": "2009970",
    "end": "2015155"
  },
  {
    "text": "Do you think off-policy\nis more or less efficient than\non-policy meta-RL? ",
    "start": "2015155",
    "end": "2021028"
  },
  {
    "text": "Cool. So at least a few people\nare paying attention.",
    "start": "2021028",
    "end": "2029059"
  },
  {
    "text": "And people are saying that\nit'll be more efficient. To answer the question,\nI mean the number of--",
    "start": "2029060",
    "end": "2035419"
  },
  {
    "text": "the amount of data that's\nneeded for meta-learning. ",
    "start": "2035420",
    "end": "2042000"
  },
  {
    "text": "Yeah, and so the reason why\noff-policy meta-RL is going to be more efficient is because\nbasically off-policy algorithms",
    "start": "2042000",
    "end": "2047790"
  },
  {
    "text": "can store data in the replay\nbuffer and keep on leveraging that data, whereas\non-policy algorithms,",
    "start": "2047790",
    "end": "2053580"
  },
  {
    "text": "they need to collect data-- collect new data\nevery time your policy or your meta-policy changes.",
    "start": "2053580",
    "end": "2059658"
  },
  {
    "text": "And if you need to\ncollect data every time your meta-policy\nchanges, then you're not able to reuse any\nof your previous data",
    "start": "2059659",
    "end": "2065340"
  },
  {
    "text": "and you have to just keep\non collecting data each time you want to update\nyour meta-policy. ",
    "start": "2065340",
    "end": "2073820"
  },
  {
    "text": "OK, and so here are what some\nof the results look like. So to unpack this,\nthe x-axis is showing",
    "start": "2073820",
    "end": "2079820"
  },
  {
    "text": "the number of-- the amount\nof meta-training data, so the amount of data needed\nfor the meta-training process,",
    "start": "2079820",
    "end": "2085908"
  },
  {
    "text": "and the y-axis is showing\nthe average return",
    "start": "2085909",
    "end": "2091408"
  },
  {
    "text": "after adapting to one of the\ntasks in the task distribution.",
    "start": "2091409",
    "end": "2097420"
  },
  {
    "text": "And in particular, what these\ndifferent methods correspond to is PEARL, is an\noff-policy meta-RL algorithm",
    "start": "2097420",
    "end": "2107070"
  },
  {
    "text": "that uses kind of the\nfeed forward architecture that I showed before. RL squared is using TRPO.",
    "start": "2107070",
    "end": "2113970"
  },
  {
    "text": "And so its an\non-policy algorithm. And then these\noptimization-based methods are also on-policy algorithms.",
    "start": "2113970",
    "end": "2119670"
  },
  {
    "text": "And you see a pretty\nhuge gain by using an off-policy meta-RL method.",
    "start": "2119670",
    "end": "2125560"
  },
  {
    "text": "So you're able to learn\nmuch more efficiently. These dashed lines are\nshowing the final performance",
    "start": "2125560",
    "end": "2132600"
  },
  {
    "text": "of these three methods. So we also actually see a\nperformance gain in this case.",
    "start": "2132600",
    "end": "2137693"
  },
  {
    "text": "But kind of the\nmain takeaway here is that they're\nsignificantly more efficient than on-policy methods.",
    "start": "2137693",
    "end": "2144983"
  },
  {
    "text": "[MUTED] is asking, is\nhindsight on experience replay necessary to make off-policy\nmore efficient than on-policy?",
    "start": "2144983",
    "end": "2150089"
  },
  {
    "text": "So in this case, there\nactually isn't any relabeling or experience replay or\nhindsight experience replay.",
    "start": "2150090",
    "end": "2157670"
  },
  {
    "text": "There is-- you are using\na replay buffer, which uses experience replay.",
    "start": "2157670",
    "end": "2163320"
  },
  {
    "text": "And experience\nreplay is something that existed long before\nhindsight experience replay.",
    "start": "2163320",
    "end": "2170730"
  },
  {
    "text": "If you don't use a replay buffer\nand don't replay experiences, then off-policy methods won't be\nmore efficient than on-policy.",
    "start": "2170730",
    "end": "2178930"
  },
  {
    "text": "They'll both be operating in\nthis kind of on-policy setting. ",
    "start": "2178930",
    "end": "2187890"
  },
  {
    "text": "OK, so those are a\ncouple of examples for black-box meta-RL methods.",
    "start": "2187890",
    "end": "2193829"
  },
  {
    "text": "One quick digression kind of in\nrelation to multitask policies is that it's common to do\nlike this sort of multitask RL",
    "start": "2193830",
    "end": "2202650"
  },
  {
    "text": "where you have some\ninformation about the task that corresponds to stacking-- where to stack a Lego block\nor in what direction to walk.",
    "start": "2202650",
    "end": "2211920"
  },
  {
    "text": "And in this meta-RL\nsetting, in some ways you can kind of\nview this experience",
    "start": "2211920",
    "end": "2217020"
  },
  {
    "text": "as some context that\ntells you what the task is and then kind of the last\npart of it as your policy.",
    "start": "2217020",
    "end": "2229260"
  },
  {
    "text": "Essentially, this is kind\nof-- you can view data as another way to\nexpress information or identifiers about the task.",
    "start": "2229260",
    "end": "2235170"
  },
  {
    "text": " We also talked about\ngoal condition policies",
    "start": "2235170",
    "end": "2241720"
  },
  {
    "text": "before and goal-conditioned RL. And in that setting,\nthe task identifier corresponds to different goals.",
    "start": "2241720",
    "end": "2248950"
  },
  {
    "text": "And I guess, in this\nway, you can view meta-RL as a strict generalization\nof goal-conditioned RL",
    "start": "2248950",
    "end": "2255730"
  },
  {
    "text": "because rewards can express\nthings other than goal reaching tasks. ",
    "start": "2255730",
    "end": "2262581"
  },
  {
    "text": "Although, of course, you can\nalso apply it to goal reaching tests as well.",
    "start": "2262582",
    "end": "2267960"
  },
  {
    "text": "And then the other\nway to kind of view this difference between\nmeta-RL and goal-conditioned RL is that the meta-RL\nobjective, your goal",
    "start": "2267960",
    "end": "2274430"
  },
  {
    "text": "is to adapt to new tasks versus\ngeneralizing to new goals. So it's kind of the\ndifference k-shot learning",
    "start": "2274430",
    "end": "2282320"
  },
  {
    "text": "to new tasks and\n0-shot generalization to a new task given kind of\na low dimensional identifier",
    "start": "2282320",
    "end": "2289940"
  },
  {
    "text": "of that task. ",
    "start": "2289940",
    "end": "2299260"
  },
  {
    "text": "OK, so to summarize\nBlack-box meta-RL, these approaches are very\ngeneral and expressive.",
    "start": "2299260",
    "end": "2304780"
  },
  {
    "text": "There's a variety of design\nchoices for the architecture. The kind of downside from before\nis that these architectures",
    "start": "2304780",
    "end": "2313780"
  },
  {
    "text": "can be hard to optimize\nbecause they have to figure out how to process experience and\nuse that to learn the task.",
    "start": "2313780",
    "end": "2319059"
  },
  {
    "text": "So they have a pretty\nhefty task to do.  And then lastly, these methods\ninherit the sample complexity",
    "start": "2319060",
    "end": "2327510"
  },
  {
    "text": "of the outer RL optimizer. If you use an off-policy\nRL optimizer that",
    "start": "2327510",
    "end": "2333029"
  },
  {
    "text": "is able to reuse\ndata, then they tend to be a lot more efficient\nthan if you use an on-policy RL",
    "start": "2333030",
    "end": "2338430"
  },
  {
    "text": "optimizer. ",
    "start": "2338430",
    "end": "2348480"
  },
  {
    "text": "OK, cool. Any questions before we move on\nto optimization-based meta-RL methods?",
    "start": "2348480",
    "end": "2354090"
  },
  {
    "text": "I was wondering if there are\nany scaling problems related to how long you\nunroll the policy, how",
    "start": "2354090",
    "end": "2361480"
  },
  {
    "text": "long the trajectory is? Yeah, so if you're\ntrying to learn",
    "start": "2361480",
    "end": "2366580"
  },
  {
    "text": "from kind of a long trajectory,\nthen the learning problem gets harder certainly.",
    "start": "2366580",
    "end": "2374140"
  },
  {
    "text": "And especially, if you want to-- if your task involves exploring\nin very sophisticated ways,",
    "start": "2374140",
    "end": "2380320"
  },
  {
    "text": "then it also becomes\na lot harder. And we'll talk about\nthe exploration part in the next lecture.",
    "start": "2380320",
    "end": "2386380"
  },
  {
    "text": "One way to kind of\nmitigate long horizons for very long kind of\nsequences that you're",
    "start": "2386380",
    "end": "2397253"
  },
  {
    "text": "trying to learn from\nis to use something like an attention-based\narchitecture rather than an RNN.",
    "start": "2397253",
    "end": "2402730"
  },
  {
    "text": "Attention-based\narchitectures basically allow you to shortcut. ",
    "start": "2402730",
    "end": "2408448"
  },
  {
    "text": "I guess, basically,\nthese connections between the orange dots, they\nbasically kind of shortcut--",
    "start": "2408448",
    "end": "2414490"
  },
  {
    "text": "provide shortcut\ncuts across time that directly connect things\nin the far past to the present,",
    "start": "2414490",
    "end": "2421640"
  },
  {
    "text": "and that allows you\nto handle potentially some of the optimization\nchallenges associated with very long horizon learning.",
    "start": "2421640",
    "end": "2428700"
  },
  {
    "text": "It makes sense. Thank you.  [MUTED] is asking,\nwhy is off policy",
    "start": "2428700",
    "end": "2435030"
  },
  {
    "text": "more efficient if\nyou store more data? So kind of the key thing\nis that by storing data",
    "start": "2435030",
    "end": "2443039"
  },
  {
    "text": "and then kind of reusing that\ndata, your model gets to see-- it gets to take\nmultiple gradient steps",
    "start": "2443040",
    "end": "2448589"
  },
  {
    "text": "on a batch of data. Whereas in on-policy methods,\nyou're not storing any data,",
    "start": "2448590",
    "end": "2454170"
  },
  {
    "text": "you're not actually able to\nreuse data and learn from it. Basically, you can just\ntake one gradient step",
    "start": "2454170",
    "end": "2459810"
  },
  {
    "text": "on any batch of\ndata that you have. And you have to kind of keep on\nthrowing away data every time",
    "start": "2459810",
    "end": "2466080"
  },
  {
    "text": "you change your policy. And as you might imagine,\nthrowing away data leads to kind of pretty\ninefficient learning.",
    "start": "2466080",
    "end": "2474180"
  },
  {
    "start": "2474180",
    "end": "2485910"
  },
  {
    "text": "OK, so let's talk a bit about\noptimization-based meta-RL methods. So recall with\noptimization-based meta",
    "start": "2485910",
    "end": "2492240"
  },
  {
    "text": "learning, we're embedding this\nkind of optimization procedure or gradient descent into\nthe learning process.",
    "start": "2492240",
    "end": "2500240"
  },
  {
    "text": "Now, to turn this into a meta-RL\nmethod, the key thing that's going to change is\nthat we're going",
    "start": "2500240",
    "end": "2505730"
  },
  {
    "text": "to be running an optimization\non experience from our policy, and instead of\noutputting some label,",
    "start": "2505730",
    "end": "2512340"
  },
  {
    "text": "we're going to be outputting\nan action from a policy. ",
    "start": "2512340",
    "end": "2517905"
  },
  {
    "text": "One thing that's pretty\ndifferent about these optimization-based meta-RL\nmethods is they tend to--",
    "start": "2517905",
    "end": "2523052"
  },
  {
    "text": "well, I guess we'll\nsee examples of both, but they tend to lie more\nin the episodic variant where you first collect\na few trajectories,",
    "start": "2523052",
    "end": "2529410"
  },
  {
    "text": "if you use full trajectories,\nuse those full trajectories to update your policy and then\nroll out your executed-- roll",
    "start": "2529410",
    "end": "2538700"
  },
  {
    "text": "out your adapted policy.  Now, one question that\nI have for you again",
    "start": "2538700",
    "end": "2547330"
  },
  {
    "text": "is, what should we use for\nthis inner optimization?",
    "start": "2547330",
    "end": "2553640"
  },
  {
    "text": "And you can answer this in\nchat or by raising your hand. Basically, you\nhave three options.",
    "start": "2553640",
    "end": "2559658"
  },
  {
    "text": "We could use policy gradients,\nwe could use Q-learning, or we could use model-based RL. Does anyone have\nthoughts or suggestions",
    "start": "2559658",
    "end": "2566539"
  },
  {
    "text": "on which of these\nmight be a good idea and why we might\nconsider using that",
    "start": "2566540",
    "end": "2573470"
  },
  {
    "text": "in the inner loop of a\noptimization-based meta-RL method?",
    "start": "2573470",
    "end": "2578960"
  },
  {
    "text": "Also, there's no\nright answer here. So there's pros and\ncons to all of them.",
    "start": "2578960",
    "end": "2585080"
  },
  {
    "text": "[MUTED] is suggesting\nmaybe model-based. Do you have thoughts\non why model-based",
    "start": "2585080",
    "end": "2591140"
  },
  {
    "text": "might be a good idea? Because model-based\ncan allow you to have off-policy\nlearning and that,",
    "start": "2591140",
    "end": "2599660"
  },
  {
    "text": "like we discussed before\nthat off-policy can be efficient, [INAUDIBLE]. ",
    "start": "2599660",
    "end": "2607010"
  },
  {
    "text": "Yeah, so one kind of benefit\nof a model-based RL approach is that it allows you to use\noff-policy data, which allows",
    "start": "2607010",
    "end": "2613970"
  },
  {
    "text": "you to be more data efficient.  Other thoughts?",
    "start": "2613970",
    "end": "2621210"
  },
  {
    "text": "Suggesting policy\ngradients because then we only need a few\nsteps in the inner loop.",
    "start": "2621210",
    "end": "2627609"
  },
  {
    "text": "So this is a good\nsuggestion of maybe we",
    "start": "2627610",
    "end": "2633750"
  },
  {
    "text": "would need only a few steps\nand suggesting Q-learning could",
    "start": "2633750",
    "end": "2644580"
  },
  {
    "text": "work for large state spaces. ",
    "start": "2644580",
    "end": "2651250"
  },
  {
    "text": "[MUTED] is suggesting\nmodel-based because it takes into account\nchange in the transition probabilities.",
    "start": "2651250",
    "end": "2657040"
  },
  {
    "text": " Cool. So these are some good thoughts.",
    "start": "2657040",
    "end": "2662923"
  },
  {
    "text": " So I'll start with\npolicy gradients. So policy gradients are nice\nbecause it's gradient-based,",
    "start": "2662923",
    "end": "2673460"
  },
  {
    "text": "and we know that you can kind\nof basically with kind of--",
    "start": "2673460",
    "end": "2678710"
  },
  {
    "text": " kind of the policy\ngradient looks something",
    "start": "2678710",
    "end": "2685310"
  },
  {
    "text": "like the sum over\ntimesteps of log pi times",
    "start": "2685310",
    "end": "2691910"
  },
  {
    "text": "sum over timesteps of rt. And basically, with just like\none or a few gradient steps,",
    "start": "2691910",
    "end": "2701428"
  },
  {
    "text": "you actually get a\nlot of information about the entire\ntrajectory which is nice.",
    "start": "2701428",
    "end": "2707190"
  },
  {
    "text": "Now, one of the downsides\nof policy gradients is that they're on policy.",
    "start": "2707190",
    "end": "2712520"
  },
  {
    "text": "So they can be\npretty inefficient. Another downside is\nthat policy gradients",
    "start": "2712520",
    "end": "2718460"
  },
  {
    "text": "tend to be pretty noisy. And in particular, one\nthing you might notice here",
    "start": "2718460",
    "end": "2723710"
  },
  {
    "text": "is that with this kind of\nform of the policy gradient, if the reward function is\nsparse or not very informative,",
    "start": "2723710",
    "end": "2733288"
  },
  {
    "text": "then this policy\ngradient isn't going to tell you that much about\nthe dynamics or that much",
    "start": "2733288",
    "end": "2738460"
  },
  {
    "text": "about the task. So say, for example,\nyou're in the maze--",
    "start": "2738460",
    "end": "2743640"
  },
  {
    "text": "kind of in a maze setting\nand you start off right here. The goal is right here. And maybe you have some\ntrajectories that look--",
    "start": "2743640",
    "end": "2753292"
  },
  {
    "text": "one trajectory looks like\nthis and another trajectory looks like this, another\ntrajectory looks like this. From these three trajectories,\nyou should have a pretty good",
    "start": "2753292",
    "end": "2760390"
  },
  {
    "text": "idea of where the reward is\nnow because you've basically exhausted all of the\nkind of password isn't.",
    "start": "2760390",
    "end": "2769378"
  },
  {
    "text": "And so from these\nthree trajectories, you'd love to be\nable to kind of then figure out that the next thing\nyou should do is go over here.",
    "start": "2769378",
    "end": "2776630"
  },
  {
    "text": "However, if you have only\nthese three trajectories, the sum of your rewards is 0\nbecause you haven't experienced",
    "start": "2776630",
    "end": "2783760"
  },
  {
    "text": "where the reward is. And so this policy\ngradient term is going to be 0 also,\nand so basically just",
    "start": "2783760",
    "end": "2793265"
  },
  {
    "text": "because of this term\nbeing 0, being 0, right? ",
    "start": "2793265",
    "end": "2800829"
  },
  {
    "text": "And so if you're in the setting\nwhere you have very sparse rewards and you haven't yet\nexperienced where the task is,",
    "start": "2800830",
    "end": "2806920"
  },
  {
    "text": "you're not going to get any\nupdate to your policy even though you just had all this\nrich information about parts",
    "start": "2806920",
    "end": "2814200"
  },
  {
    "text": "of the environment\nwhere the reward isn't. So this is one of the challenges\nwith kind of policy gradients",
    "start": "2814200",
    "end": "2820830"
  },
  {
    "text": "is that they're noisy and\nthey're often uninformative. ",
    "start": "2820830",
    "end": "2830322"
  },
  {
    "text": "[MUTED] is asking,\ncan't that be somewhat alleviated by attaching\na small negative reward to every action?",
    "start": "2830322",
    "end": "2836080"
  },
  {
    "text": "Yes, it can be somewhat\nalleviated by that. If instead your\nreward function's instead of assigning like\na small negative value",
    "start": "2836080",
    "end": "2843190"
  },
  {
    "text": "to these things or if you have\nsome sort of baseline that suggests that you should be\nable to do better than 0,",
    "start": "2843190",
    "end": "2848980"
  },
  {
    "text": "then that will allow\nit to push your policy in the right direction.",
    "start": "2848980",
    "end": "2854150"
  },
  {
    "text": "This is kind of one\nextreme example too. Even if you have\nrewards that are",
    "start": "2854150",
    "end": "2860110"
  },
  {
    "text": "like small negative\nvalues for example, that still doesn't tell you-- this gradient is still fairly\nkind of deprived of information",
    "start": "2860110",
    "end": "2868809"
  },
  {
    "text": "about what the\nenvironment looks like and what the dynamics\nlook like and so forth. ",
    "start": "2868810",
    "end": "2875810"
  },
  {
    "text": "OK, so that's policy gradients. What about Q-learning? So one of the\nbenefits of Q-learning",
    "start": "2875810",
    "end": "2883010"
  },
  {
    "text": "is that it is\noff-policy and so it's very efficient at learning. The downside, which I think that\nit may have been alluding to",
    "start": "2883010",
    "end": "2891800"
  },
  {
    "text": "is that because Q-learning is\nrunning dynamic programming, it requires many gradient steps.",
    "start": "2891800",
    "end": "2898560"
  },
  {
    "text": "So if you look at the\nBellman equation for example, maybe I'll write it over\nhere, your Bellman equation",
    "start": "2898560",
    "end": "2905510"
  },
  {
    "text": "looks something like Q hat minus\nr plus gamma a max a prime of Q",
    "start": "2905510",
    "end": "2915260"
  },
  {
    "text": "of s prime, a prime. And this right here, this\nis kind of Q hat of s, a.",
    "start": "2915260",
    "end": "2925630"
  },
  {
    "text": "And basically,\neach gradient step that you take on this\nobjective, you're only propagating information\none timestep from s prime to s.",
    "start": "2925630",
    "end": "2935210"
  },
  {
    "text": "And so as a result, if you\nwant to propagate information throughout your entire\nhorizon, then you",
    "start": "2935210",
    "end": "2940370"
  },
  {
    "text": "need at least kind of\neach gradient steps",
    "start": "2940370",
    "end": "2945780"
  },
  {
    "text": "to really propagate\nthat information. And this is a pretty typical-- this is a big challenge\nfor optimization-based",
    "start": "2945780",
    "end": "2952410"
  },
  {
    "text": "meta-learning methods\nwhere we typically run only a few gradient\nsteps in the inner loop. ",
    "start": "2952410",
    "end": "2960299"
  },
  {
    "text": "So this is one of the\nchallenges with Q-learning. And then with\nmodel-based RL, it's",
    "start": "2960300",
    "end": "2968529"
  },
  {
    "text": "both off-policy\nlike was mentioned. It's also gradient-based. Model-learning just\ndirectly corresponds",
    "start": "2968530",
    "end": "2974360"
  },
  {
    "text": "to supervised learning. So this makes it\nalso pretty easy to combine with\noptimization-based",
    "start": "2974360",
    "end": "2980440"
  },
  {
    "text": "meta-learning. So both of these are quite nice. There are scenarios where\nmodel-based RL is difficult",
    "start": "2980440",
    "end": "2988060"
  },
  {
    "text": "because like learning\na model is hard. But in settings where\nmodel-based RL works well, we would also expect\noptimization-based meta-RL",
    "start": "2988060",
    "end": "2995560"
  },
  {
    "text": "to be a pretty good option. So here are some of\nthe general trade-offs",
    "start": "2995560",
    "end": "3000710"
  },
  {
    "text": "in terms of combining different\nkinds of optimizations with optimization-based\nmeta-learning.",
    "start": "3000710",
    "end": "3010270"
  },
  {
    "text": "Now, let's go into\na few examples. And in particular, we're going\nto look at an example of MAML",
    "start": "3010270",
    "end": "3017049"
  },
  {
    "text": "with policy gradients and\nMAML with model-based RL.",
    "start": "3017050",
    "end": "3022210"
  },
  {
    "text": "There actually aren't\nreally any papers that look at something\ndirectly like MAML",
    "start": "3022210",
    "end": "3028150"
  },
  {
    "text": "with Q-learning because of\nthis challenge right here. There are a couple\npapers that suggest",
    "start": "3028150",
    "end": "3033953"
  },
  {
    "text": "kind of different alternatives\nfor potentially alleviating this. But really they're\nchallenging, I guess.",
    "start": "3033953",
    "end": "3043000"
  },
  {
    "text": "If you're interested in kind\nof looking more into this, one paper that you could look\nat is by Eric Mitchell et al.",
    "start": "3043000",
    "end": "3054079"
  },
  {
    "text": " Well, the name of the\nmethod is called MACAW,",
    "start": "3054080",
    "end": "3060150"
  },
  {
    "text": "not quite a kind of\nmamall, but close. And it's using a kind of\nadvantage weighted actor critic",
    "start": "3060150",
    "end": "3066870"
  },
  {
    "text": "algorithm and combining\nthat with MAML. ",
    "start": "3066870",
    "end": "3073849"
  },
  {
    "text": "OK, so let's look at\na couple examples. So if we combine MAML\nwith policy gradients,",
    "start": "3073850",
    "end": "3080160"
  },
  {
    "text": "this will just be like\na really simple example, like I mentioned before,\ncombining MAML with policy gradients has some challenges.",
    "start": "3080160",
    "end": "3087798"
  },
  {
    "text": "And this is actually\njust the example that was in the\noriginal MAML paper that illustrates one of the things\nthat you can do with it.",
    "start": "3087798",
    "end": "3095400"
  },
  {
    "text": "And in particular, we'll have\njust two different tasks here. One task will be just\nkind of going left",
    "start": "3095400",
    "end": "3105420"
  },
  {
    "text": "and the other task will\ncorrespond to going right. And first we'll visualize what\nthe policy parameters look",
    "start": "3105420",
    "end": "3115000"
  },
  {
    "text": "like after the\nmeta-learning process, but before adopting\nto one of the tasks. ",
    "start": "3115000",
    "end": "3122860"
  },
  {
    "text": "So this looks like something\nlike this where essentially the policy is running in place.",
    "start": "3122860",
    "end": "3128950"
  },
  {
    "text": "It's kind of ready to run in\none of the two directions.",
    "start": "3128950",
    "end": "3135940"
  },
  {
    "text": "And then if we take one\ngradient step with respect to the task of\nrunning backward, we",
    "start": "3135940",
    "end": "3142540"
  },
  {
    "text": "get a policy that\nlooks like this. And if we take one\ngradient step with respect",
    "start": "3142540",
    "end": "3148530"
  },
  {
    "text": "to the task of\nrunning forward, we get a policy that\nlooks like this. So this is an illustration of\nhow with policy gradients--",
    "start": "3148530",
    "end": "3156091"
  },
  {
    "text": "I think these are with\n20 trajectories used to compute the\npolicy gradient, you can actually get a pretty\ndrastic change of performance",
    "start": "3156092",
    "end": "3163780"
  },
  {
    "text": "which is nice. These are both with\nstraightforward functions, which make it a\nlittle bit easier",
    "start": "3163780",
    "end": "3169485"
  },
  {
    "text": "to handle the noise in\nthe policy gradient. ",
    "start": "3169485",
    "end": "3176282"
  },
  {
    "text": "Ok, and then let's look at\none more example with MAML plus a model-based RL approach. And in this case,\nwe're actually going",
    "start": "3176282",
    "end": "3182510"
  },
  {
    "text": "to be looking at the\nonline variant of meta-RL where our data corresponds\nto k different time",
    "start": "3182510",
    "end": "3188570"
  },
  {
    "text": "steps from some policy. ",
    "start": "3188570",
    "end": "3194230"
  },
  {
    "text": "And so what this will look\nlike is at meta test time we're going to be adapting\nthe parameters of our model",
    "start": "3194230",
    "end": "3200380"
  },
  {
    "text": "to the last k time steps,\nand this will be with regard to the error of kind of f of\ns, a minus the true next state.",
    "start": "3200380",
    "end": "3212558"
  },
  {
    "text": "So we're obviously going to\nbe running gradient descent on this objective\nto update our model.",
    "start": "3212558",
    "end": "3218380"
  },
  {
    "text": "And then once we compute\nour updated model for that current\ntime step, we then",
    "start": "3218380",
    "end": "3225150"
  },
  {
    "text": "plan a sequence of actions\nusing our adapted model. ",
    "start": "3225150",
    "end": "3232220"
  },
  {
    "text": "And this will be kind of\nwith respect to some task forward, which may be\ndifferent for different tasks.",
    "start": "3232220",
    "end": "3240050"
  },
  {
    "text": " And then at\nmeta-training time, we're actually going to\nbe at a scenario",
    "start": "3240050",
    "end": "3246280"
  },
  {
    "text": "where the task may actually\nvary within an episode even.",
    "start": "3246280",
    "end": "3251350"
  },
  {
    "text": "But we're going to assume that\nthe dynamics, for example, are constant within kind\nof a short period of time,",
    "start": "3251350",
    "end": "3256990"
  },
  {
    "text": "but then it may change\nwithin a longer episode. And essentially,\ndifferent tasks are",
    "start": "3256990",
    "end": "3264190"
  },
  {
    "text": "going to correspond to\ndifferent windows in time where this kind of\nwindow right here",
    "start": "3264190",
    "end": "3270670"
  },
  {
    "text": "will correspond to what\nwe're wanting to adapt to, which is the last k timesteps. And then this window\nright here, which",
    "start": "3270670",
    "end": "3276520"
  },
  {
    "text": "is kind of the next h timesteps\nwill correspond to our test set.",
    "start": "3276520",
    "end": "3282140"
  },
  {
    "text": "And we can construct\nmany different tasks by essentially sampling\na point in time",
    "start": "3282140",
    "end": "3289130"
  },
  {
    "text": "and sampling these kind of\ntrain set and test set windows. ",
    "start": "3289130",
    "end": "3295940"
  },
  {
    "text": "And this is assuming\nthat the tasks are-- the tasks only change or the\ndynamics only change sparsely",
    "start": "3295940",
    "end": "3301820"
  },
  {
    "text": "in time, but it's\nstill fine if-- it's still reasonable.",
    "start": "3301820",
    "end": "3307490"
  },
  {
    "text": "If, for example, the\ndynamics change here, you're only going to\nhave a few tasks that are sampled with the task\nchanging in the middle of it.",
    "start": "3307490",
    "end": "3314480"
  },
  {
    "text": "And most of the windows will\nbe at different parts of--",
    "start": "3314480",
    "end": "3320150"
  },
  {
    "text": "at different points\nin time where you have a consistent task. Like maybe this\ncorresponds to running",
    "start": "3320150",
    "end": "3326990"
  },
  {
    "text": "on grass and this corresponds\nto running on asphalt.",
    "start": "3326990",
    "end": "3334850"
  },
  {
    "text": "And we're going to\nhave different dynamics for these two different\nparts of the environment.",
    "start": "3334850",
    "end": "3340000"
  },
  {
    "text": "And when we sample\ntasks that correspond right to this\nboundary, those tasks",
    "start": "3340000",
    "end": "3346500"
  },
  {
    "text": "are going to be pretty\nnoisy because we're really trying to adapt to asphalt when\nwe're given data from grass. But everywhere else, when we\nsample all of these windows,",
    "start": "3346500",
    "end": "3354210"
  },
  {
    "text": "we'll get kind of valid\nparts of a single task. ",
    "start": "3354210",
    "end": "3362795"
  },
  {
    "text": "OK, so in terms\nof the algorithm, we basically sample these\ntrain sets and test sets from different windows of time.",
    "start": "3362795",
    "end": "3369000"
  },
  {
    "text": "We use MAML to adapt our\ndynamics model on the train set, evaluate it on\nthe test set and then",
    "start": "3369000",
    "end": "3375820"
  },
  {
    "text": "update the initial\nparameters such that that adaptation\nwas successful. ",
    "start": "3375820",
    "end": "3386740"
  },
  {
    "text": "So how does this actually\nlook like in practice? So we can look at\nsome environments that are, as I mentioned before,\ndynamically changing in time.",
    "start": "3386740",
    "end": "3395140"
  },
  {
    "text": "And if we first start with\njust a single model-based RL approach or we're trying to\nfit a single model to varying",
    "start": "3395140",
    "end": "3402890"
  },
  {
    "text": "the dynamics that may actually\nbe varying across time, this isn't very successful.",
    "start": "3402890",
    "end": "3408522"
  },
  {
    "text": "So in this first\nexample, one of the legs becomes crippled\nand single model",
    "start": "3408522",
    "end": "3413570"
  },
  {
    "text": "isn't able to capture the\ndynamics of a normal leg and a crippled leg.",
    "start": "3413570",
    "end": "3419810"
  },
  {
    "text": "In the second example,\nthese different panels have different buoyancy, kind\nof like if you're on a dock,",
    "start": "3419810",
    "end": "3426800"
  },
  {
    "text": "and a single model struggles\nto capture the dynamics when you're walking on these\ndifferent parts of the dock.",
    "start": "3426800",
    "end": "3434040"
  },
  {
    "text": "And so the agent\nends up flipping over and is unable to kind\nof get to the goal which",
    "start": "3434040",
    "end": "3439980"
  },
  {
    "text": "is kind of over here basically. ",
    "start": "3439980",
    "end": "3445410"
  },
  {
    "text": "Now, instead if we use\nMAML with model-based RL like I mentioned before,\nwhat these can do",
    "start": "3445410",
    "end": "3452010"
  },
  {
    "text": "is they can actually adapt\nonline to the latest k timesteps and\ncontinuously replan",
    "start": "3452010",
    "end": "3458010"
  },
  {
    "text": "using the adaptive dynamics\nmodel to successfully make it over to the right hand\nside of the screen.",
    "start": "3458010",
    "end": "3463369"
  },
  {
    "text": " Basically, here\nat every timestep,",
    "start": "3463370",
    "end": "3469200"
  },
  {
    "text": "the model is being adapted\nwith a gradient step and then we're running\nplanning with regard to that adapted model.",
    "start": "3469200",
    "end": "3474240"
  },
  {
    "text": " Now, one of the cool things\nthat was mentioned before",
    "start": "3474240",
    "end": "3480420"
  },
  {
    "text": "is that model-based RL methods\nare really efficient, right, because they can use\nlots of off-policy data. And so if you compare these\napproaches shown in green,",
    "start": "3480420",
    "end": "3490570"
  },
  {
    "text": "these are both kind of\nmodel-based RL approaches where this dark green\napproach corresponds to kind",
    "start": "3490570",
    "end": "3496150"
  },
  {
    "text": "of MAML plus model-based RL. This approach here\ncorresponds to basically",
    "start": "3496150",
    "end": "3503200"
  },
  {
    "text": "an RNN-based meta-learner\nplus model-based RL. All of these are like\nmuch more efficient",
    "start": "3503200",
    "end": "3509339"
  },
  {
    "text": "than these model-free approaches\nshown in red and yellow. And that means that it's\nactually potentially practical",
    "start": "3509340",
    "end": "3514950"
  },
  {
    "text": "to run these on a real robot. So you can collect data\non different trains",
    "start": "3514950",
    "end": "3520050"
  },
  {
    "text": "with this little\nsix-legged robot. These robots are\nactually built by hand, and their dynamics\nvery significantly",
    "start": "3520050",
    "end": "3526470"
  },
  {
    "text": "across different trains and\nacross different conditions of the environment.",
    "start": "3526470",
    "end": "3532712"
  },
  {
    "text": "You can collect data like\nthis, meta-train your model on these different\nwindows of time, and then evaluate how well\nit can handle new situations.",
    "start": "3532712",
    "end": "3541530"
  },
  {
    "text": "So in this first\ncase, the goal is to walk up this narrow slope\nand the goal is to follow--",
    "start": "3541530",
    "end": "3549840"
  },
  {
    "text": "be able to like, walk\nalong a straight line. If you just train a single model\nacross all the different trains",
    "start": "3549840",
    "end": "3555839"
  },
  {
    "text": "and so forth, that\nmodel struggles to capture the different\nsituations including this test situation and the robot ends\nup veering off to the right.",
    "start": "3555840",
    "end": "3564300"
  },
  {
    "text": "Whereas if you kind\nof continuously adapt your model to the\ndifferent situations, is able to figure out a good\nmodel for this slope's terrain",
    "start": "3564300",
    "end": "3572640"
  },
  {
    "text": "and follow it in\na straight line. ",
    "start": "3572640",
    "end": "3578453"
  },
  {
    "text": "Then kind of one\nmore example, you can take off the front\nright leg of the robot. ",
    "start": "3578453",
    "end": "3584340"
  },
  {
    "text": "Again, the model\nthat was trained across all the different\ntrains struggles to handle this situation\nunderstandably.",
    "start": "3584340",
    "end": "3592079"
  },
  {
    "text": "But if you adapt\nyour model to kind of the previous\nwindows of time, you",
    "start": "3592080",
    "end": "3597480"
  },
  {
    "text": "can actually adapt\nto this new condition and walk in a straight line.",
    "start": "3597480",
    "end": "3602900"
  },
  {
    "text": " [MUTED] is asking,\nhow do we figure out",
    "start": "3602900",
    "end": "3608610"
  },
  {
    "text": "if the train has changed? Do we have a fixed horizon,\nre-plan and then re-plan again? So in all of these\nexperiments, it's",
    "start": "3608610",
    "end": "3616710"
  },
  {
    "text": "actually resetting to the\ninitial model parameters and adapting the model\nat every single timestep.",
    "start": "3616710",
    "end": "3623580"
  },
  {
    "text": "So it's just assuming that\nthe model might have changed at every single timestep and\nwith that, the adapted model",
    "start": "3623580",
    "end": "3629190"
  },
  {
    "text": "is also re-planning at\nevery single timestep. Because these models\nare kind of relatively--",
    "start": "3629190",
    "end": "3634230"
  },
  {
    "text": "because the state is\nrelatively low dimensional and the model is just a\ncouple layer neural network, it's pretty fast to\nrun a gradient step.",
    "start": "3634230",
    "end": "3641609"
  },
  {
    "text": "And therefore, it's\nfeasible to run this re-planning in real time. If you had like a\nmuch larger model,",
    "start": "3641610",
    "end": "3647940"
  },
  {
    "text": "then it may be a\nlot more expensive. [MUTED] is asking where is\nthe reward signal coming from at meta-test time?",
    "start": "3647940",
    "end": "3654590"
  },
  {
    "text": "So in the case of the\nsimulated experiments, the goal was just to move as\nfar to the right as possible.",
    "start": "3654590",
    "end": "3662000"
  },
  {
    "text": "So I think the reward\nfunction was something like negative or something\nlike actually forward velocity,",
    "start": "3662000",
    "end": "3667720"
  },
  {
    "text": "for example. In this case, there are\nactually motion capture markers",
    "start": "3667720",
    "end": "3673300"
  },
  {
    "text": "on the robot that allow the\nrobot to know where in the room it is, and then\nthe reward function corresponds to this\nlike a negative distance",
    "start": "3673300",
    "end": "3681160"
  },
  {
    "text": "to some waypoint, which may\nbe at the end of the line. [MUTED] is asking, are\nwe training at test time?",
    "start": "3681160",
    "end": "3687650"
  },
  {
    "text": "So in this case,\nthe model is kind of being adapted at test\ntime with one gradient step",
    "start": "3687650",
    "end": "3695110"
  },
  {
    "text": "to go from f theta to\nf phi t, and this kind",
    "start": "3695110",
    "end": "3700630"
  },
  {
    "text": "of happens with\na single gradient",
    "start": "3700630",
    "end": "3705730"
  },
  {
    "text": "step on kind of the model\nerror on kind of the last k",
    "start": "3705730",
    "end": "3712300"
  },
  {
    "text": "time steps. This is the only training\nthat we're doing though. ",
    "start": "3712300",
    "end": "3720660"
  },
  {
    "text": "OK, cool.  Yeah, and this should be D\ntrain because this is what we're",
    "start": "3720660",
    "end": "3726350"
  },
  {
    "text": "adapting to at meta-test time. So this is the train\nset at meta-test time",
    "start": "3726350",
    "end": "3733140"
  },
  {
    "text": "OK, so to summarize,\nblack-box methods are generally expressive.",
    "start": "3733140",
    "end": "3738610"
  },
  {
    "text": "There's a variety\nof design choices. They're hard to optimize\nin the meta-RL setting as well as, of course, in the\noriginal meta-learning setting.",
    "start": "3738610",
    "end": "3746550"
  },
  {
    "text": "In the meta-RL setting,\noptimization-based meta-RL methods have the inductive\nbias of optimization built in.",
    "start": "3746550",
    "end": "3752790"
  },
  {
    "text": "They're easy to combine\nwith policy gradients and model-based methods\nwith the caveats that I mentioned about policy\ngradients before of policy",
    "start": "3752790",
    "end": "3762360"
  },
  {
    "text": "gradients being very noisy. They are also hard to combine\nwith value-based RL methods",
    "start": "3762360",
    "end": "3767850"
  },
  {
    "text": "because you have to run\na lot of gradient steps in the inner loop to get\nsignal across your horizon.",
    "start": "3767850",
    "end": "3773360"
  },
  {
    "text": " So these are very\nsimilar pros and cons to before with kind of\nsome added challenges",
    "start": "3773360",
    "end": "3781230"
  },
  {
    "text": "if you want to apply things-- if you want to apply\noptimization-based meta-RL methods.",
    "start": "3781230",
    "end": "3787460"
  },
  {
    "text": "And then both of them\ninherit the sample efficiency of the outer RL optimizer. If you use a\nmodel-based approach,",
    "start": "3787460",
    "end": "3793220"
  },
  {
    "text": "it's extremely efficient. Using an off-line approach\nis a lot more efficient than a fully on-policy approach.",
    "start": "3793220",
    "end": "3800990"
  },
  {
    "text": "And the efficiency\nisn't necessarily-- it's kind of more\ndominated by this outer RL optimizer than the\nmeta-RL approach itself.",
    "start": "3800990",
    "end": "3808300"
  },
  {
    "text": " OK, so that's it.",
    "start": "3808300",
    "end": "3815610"
  },
  {
    "text": "So kind of we got through\nthe lecture goals, and hopefully that gave you\nkind of a decent understanding",
    "start": "3815610",
    "end": "3821730"
  },
  {
    "text": "of the meta-RL problem\nstatement and the basics of black-box and\noptimization-based meta-RL",
    "start": "3821730",
    "end": "3827400"
  },
  {
    "text": "algorithms.  As I mentioned before,\nnext time we'll",
    "start": "3827400",
    "end": "3832720"
  },
  {
    "text": "be talking about how we\nlearn how to explore. So in all of the\nexamples we gave",
    "start": "3832720",
    "end": "3837880"
  },
  {
    "text": "before, we had relatively\nshaped rewards, it was relatively easy to figure\nout how to adapt to the task",
    "start": "3837880",
    "end": "3845950"
  },
  {
    "text": "or we were able to learn\nexploration strategies end to end in the\nnavigation example.",
    "start": "3845950",
    "end": "3851710"
  },
  {
    "text": "And on Monday we'll\ntalk about situations where we need to explore in\nmuch more sophisticated ways",
    "start": "3851710",
    "end": "3857079"
  },
  {
    "text": "in order to solve the tasks. And then on next\nWednesday, Carol",
    "start": "3857080",
    "end": "3862177"
  },
  {
    "text": "will be talking about\na Bayesian perspective on meta-RL and how we can\nderive some of these algorithms from kind of first principles\nfrom probabilistic graphical",
    "start": "3862177",
    "end": "3870850"
  },
  {
    "text": "models. Cool. And then after that,\nwe'll be talking about more advanced topics\nlike hierarchical-RL, lifelong",
    "start": "3870850",
    "end": "3877780"
  },
  {
    "text": "learning, and we'll also\nhave a guest lecture from Jane Wang who\nwill be talking about a cognitive science\nperspective on meta-learning.",
    "start": "3877780",
    "end": "3884839"
  },
  {
    "text": "Hello. I have a question regarding the\noptimization based on meta-RL.",
    "start": "3884840",
    "end": "3889870"
  },
  {
    "text": "Can you go back two steps,\nI guess two slides, yes. - Yup.",
    "start": "3889870",
    "end": "3895420"
  },
  {
    "text": "So say if for the\norganization base,",
    "start": "3895420",
    "end": "3900799"
  },
  {
    "text": "if you are in organization,\nhorizon is pretty long. That means you have to perform\nmultiple, even many gradient",
    "start": "3900800",
    "end": "3908380"
  },
  {
    "text": "steps in your inner loop. And how does that affect\nyour gradient with respect",
    "start": "3908380",
    "end": "3913839"
  },
  {
    "text": "to the outer loop in terms of\nnoise and also computation? Yeah, so if you have\na very long horizon,",
    "start": "3913840",
    "end": "3923590"
  },
  {
    "text": "like a very long\nepisode, for example, if you have a\nvalue-based RL method,",
    "start": "3923590",
    "end": "3929050"
  },
  {
    "text": "you need to take\nmany gradient steps. But if you have a\npolicy-gradient method, you don't necessarily\nneed to take",
    "start": "3929050",
    "end": "3935230"
  },
  {
    "text": "a large number of gradient steps\nbecause the policy gradient is able to capture\nkind of information",
    "start": "3935230",
    "end": "3941800"
  },
  {
    "text": "about the entire trajectory. Because like we\nmentioned, it looks something grad log pi\ntimes the sum of rewards.",
    "start": "3941800",
    "end": "3951740"
  },
  {
    "text": "So this contains information\nabout the full trajectory. So in this case, it's\nactually fine to take",
    "start": "3951740",
    "end": "3957010"
  },
  {
    "text": "one or few gradient steps\neven for long episodes. But for value-based RL\nmethods, it's a big challenge",
    "start": "3957010",
    "end": "3963940"
  },
  {
    "text": "and very few methods have\nbeen able to kind of overcome this challenge.",
    "start": "3963940",
    "end": "3968950"
  },
  {
    "text": "OK, and one more\nclarification is, just for the\nnanostructure in general,",
    "start": "3968950",
    "end": "3974590"
  },
  {
    "text": "like regardless\nif it's RL or LL, it's generally not\na good idea to take",
    "start": "3974590",
    "end": "3980080"
  },
  {
    "text": "many steps in the inner\nloop, is that correct? I think the main\ndownside is that it",
    "start": "3980080",
    "end": "3985840"
  },
  {
    "text": "tends to be more computationally\nexpensive to take a large number of\ninner gradient steps because then you have to back\npropagate through that path.",
    "start": "3985840",
    "end": "3993910"
  },
  {
    "text": "If you have enough compute, then\nit's not too much of an issue.",
    "start": "3993910",
    "end": "4000559"
  },
  {
    "text": "And yeah, if you\nhave like plenty of compute that it's\nnot a huge issue,",
    "start": "4000560",
    "end": "4006230"
  },
  {
    "text": "you can also run into challenges\nthough where we're basically-- the same kind of\nchallenges when you train",
    "start": "4006230",
    "end": "4012770"
  },
  {
    "text": "RNNs where you get like\nexploding gradients or vanishing gradients. ",
    "start": "4012770",
    "end": "4018360"
  },
  {
    "text": "That can also be a\nchallenge, although usually the computational\nbottlenecks are more of a challenge\nthan the ingredients.",
    "start": "4018360",
    "end": "4025620"
  },
  {
    "text": "OK. Ingredients. I have a quick question. We have the [INAUDIBLE] I was\nwondering how that's meta-RL",
    "start": "4025620",
    "end": "4042640"
  },
  {
    "text": "and how it's different\nfrom conventional RL? Yeah, so it's basically like\nvery similar to the approach",
    "start": "4042640",
    "end": "4050920"
  },
  {
    "text": "that you implemented before. The key difference from\nkind of standard RL",
    "start": "4050920",
    "end": "4056760"
  },
  {
    "text": "is that you are training\nacross multiple MDPs. And the hidden\nstate is maintained",
    "start": "4056760",
    "end": "4065460"
  },
  {
    "text": "across episodes within a task. And so you're allowing kind of\nthe algorithm to kind of learn",
    "start": "4065460",
    "end": "4071610"
  },
  {
    "text": "across these episodes. Yeah, that's kind of\nthe main gist of it.",
    "start": "4071610",
    "end": "4078140"
  },
  {
    "text": "OK. And I have one more quick\nquestion, may be two. So how-- I'm telling you,\nI have zero RL background.",
    "start": "4078140",
    "end": "4084609"
  },
  {
    "text": "How is model-based RL\ndifferent from policy gradient? Yeah, so model-based\nRL algorithms are--",
    "start": "4084610",
    "end": "4092020"
  },
  {
    "text": " so policy gradient, you get\nbasically the way that you",
    "start": "4092020",
    "end": "4103259"
  },
  {
    "text": "update your policy is with\nsomething like grad log pi",
    "start": "4103260",
    "end": "4108568"
  },
  {
    "text": "times some of the rewards. And then with a\nmodel-based method.",
    "start": "4108569",
    "end": "4113877"
  },
  {
    "text": "So this is, you just like kind\nof directly update your policy. With a model-based method like\nwe talked about in the lecture",
    "start": "4113877",
    "end": "4120120"
  },
  {
    "text": "on Monday, you train a model-- train like a neural network\nto model your dynamics",
    "start": "4120120",
    "end": "4127410"
  },
  {
    "text": "and potentially your\nreward, and then you use this learned model to update\nyour policy or to plan actions.",
    "start": "4127410",
    "end": "4134660"
  },
  {
    "text": "And what is inductive\nbias in the [INAUDIBLE].. Yeah, I guess what\nI meant by that",
    "start": "4134660",
    "end": "4141000"
  },
  {
    "text": "is you're building the\nstructure of gradient descent inside the optimizer, rather\nthan just like completely",
    "start": "4141000",
    "end": "4149399"
  },
  {
    "text": "training an RNN from\nscratch to learn completely without any kind of structure\nof learning or optimization.",
    "start": "4149399",
    "end": "4157259"
  },
  {
    "text": "And one more thing,\nyou had something called zero-shot going back. I was wondering-- a\nk-shot versus zero-shot.",
    "start": "4157260",
    "end": "4164889"
  },
  {
    "text": "I was wondering if you\nknew what that was? Yeah, so what I meant by\nthat is that basically",
    "start": "4164889",
    "end": "4172479"
  },
  {
    "text": "when you're given-- when you're trying to\nlearn from experience, you're kind of trying to\nlearn from k data points.",
    "start": "4172479",
    "end": "4179089"
  },
  {
    "text": "And so you're doing\nk-shot learning where k is greater than zero.",
    "start": "4179090",
    "end": "4185080"
  },
  {
    "text": "And then in goal-conditioned\nRL when you're just trying to generalize\nto new things,",
    "start": "4185080",
    "end": "4190540"
  },
  {
    "text": "you're given a task identifier\nthat describes something about the task like the goal\nthat you're trying to reach",
    "start": "4190540",
    "end": "4196480"
  },
  {
    "text": "and you're not given any data. And so in that sense kind of you\ncan give k as being equal to 0,",
    "start": "4196480",
    "end": "4202420"
  },
  {
    "text": "and you're giving it\ninformation like the goal. And that's why it's referred\nto as zero-shot generalization",
    "start": "4202420",
    "end": "4208449"
  },
  {
    "text": "because you're not\ngiven any experience, but you're still trying to\ngeneralize to a new task. OK, thank you.",
    "start": "4208450",
    "end": "4214228"
  },
  {
    "text": " Yeah, so one\nquestion is now is it",
    "start": "4214228",
    "end": "4221079"
  },
  {
    "text": "usual to incorporate\nimportance sampling when you try to do meta-RL?",
    "start": "4221080",
    "end": "4226975"
  },
  {
    "text": " I guess there's a few different\nparts that you could imagine",
    "start": "4226975",
    "end": "4232230"
  },
  {
    "text": "using importance sampling. If you're using\npolicy gradients, that's kind of a\nreasonable approach",
    "start": "4232230",
    "end": "4237450"
  },
  {
    "text": "to try to allow it to use\noff-policy data to some degree.",
    "start": "4237450",
    "end": "4244400"
  },
  {
    "text": "So one paper that does\nsomething like this is called Guided Meta\nPolicy Search that use",
    "start": "4244400",
    "end": "4253823"
  },
  {
    "text": "kind of importance\nweights in the outer loop to allow kind of a\npolicy gradient method",
    "start": "4253823",
    "end": "4259025"
  },
  {
    "text": "to use a little bit\nof off-policy data.  I don't know of\nother approaches that",
    "start": "4259025",
    "end": "4265210"
  },
  {
    "text": "do it in the\nmeta-learning literature. So in the efficient\noff-policy meta-RL paper,",
    "start": "4265210",
    "end": "4275710"
  },
  {
    "text": "during meta test time,\nyou don't update a policy, but only infer the--",
    "start": "4275710",
    "end": "4281880"
  },
  {
    "text": "I guess the context, which is\nlike a task identifier right?",
    "start": "4281880",
    "end": "4287420"
  },
  {
    "text": "Yep.  So what's the reason behind\nnot updating the policy",
    "start": "4287420",
    "end": "4294280"
  },
  {
    "text": "during meta-test time? Yeah, so it can be a few, just\nlike another black-box meta-RL",
    "start": "4294280",
    "end": "4300430"
  },
  {
    "text": "method, where you're basically\njust kind of taking data as input. So all of this data here is\nreferred to as c in the paper.",
    "start": "4300430",
    "end": "4310030"
  },
  {
    "text": "But you could view\nthis as D train. And we're taking\nD train as input and producing some kind of\nlow dimensional embedding of D",
    "start": "4310030",
    "end": "4318130"
  },
  {
    "text": "train that corresponds to z. And this z is passed into the\npolicy and the Q function.",
    "start": "4318130",
    "end": "4324440"
  },
  {
    "text": "And so you can\nview the z as kind of allowing it to take in\ndata and adapt to that data.",
    "start": "4324440",
    "end": "4330370"
  },
  {
    "text": "OK, if that's the\ncase, wouldn't it be more helpful to separate the\ninference of the environment",
    "start": "4330370",
    "end": "4338080"
  },
  {
    "text": "and the task goal? ",
    "start": "4338080",
    "end": "4343250"
  },
  {
    "text": "For example, if you have a\ndifferent state space for each task, then you'll have to-- if\nyou want to have them to share",
    "start": "4343250",
    "end": "4352240"
  },
  {
    "text": "some common structure, then you\nhave to use a part of the goal",
    "start": "4352240",
    "end": "4358260"
  },
  {
    "text": "space as like a [INAUDIBLE]\nfoundation of the goal state--",
    "start": "4358260",
    "end": "4363519"
  },
  {
    "text": "I mean, the state base to\nincorporate information",
    "start": "4363520",
    "end": "4368830"
  },
  {
    "text": "from different tasks and use\nan additional environment identifier.",
    "start": "4368830",
    "end": "4373953"
  },
  {
    "text": "Would that be helpful? Yeah, so, if your MDPs are\nvaried in different ways, than you could\nimagine potentially",
    "start": "4373953",
    "end": "4379090"
  },
  {
    "text": "having one encoder that tries\nto summarize information",
    "start": "4379090",
    "end": "4385580"
  },
  {
    "text": "about the dynamics\nand then one thing",
    "start": "4385580",
    "end": "4392000"
  },
  {
    "text": "that tries to summarize\ninformation about the reward. And then you could pass\nboth of those into these.",
    "start": "4392000",
    "end": "4399110"
  },
  {
    "text": "OK. The PEARL paper didn't\ndo anything like this. And I don't know of any\npapers that does this,",
    "start": "4399110",
    "end": "4404780"
  },
  {
    "text": "but it does seem like something\nlike this could be helpful, especially if you know that some\ntasks share a reward function",
    "start": "4404780",
    "end": "4410515"
  },
  {
    "text": "but just have\ndifferent dynamics. And other tasks\nonly share dynamic-- other tasks share the reward\nbut have different dynamics.",
    "start": "4410515",
    "end": "4418112"
  },
  {
    "text": "Yeah, cool. So just one last\nquestion, and it's about the memorization paper.",
    "start": "4418112",
    "end": "4425730"
  },
  {
    "text": "So in the loop of-- I mean, when you calculate\nthe generalization error bound",
    "start": "4425730",
    "end": "4432440"
  },
  {
    "text": "within each task,\nthat one is upper bounded by the\ngeneralized staging",
    "start": "4432440",
    "end": "4438560"
  },
  {
    "text": "area of the meta-parameter.",
    "start": "4438560",
    "end": "4444650"
  },
  {
    "text": "Mm-hmm. And would it make sense to\nactually use the, for example,",
    "start": "4444650",
    "end": "4453140"
  },
  {
    "text": "count the elements between the\nupdated task-specific parameter",
    "start": "4453140",
    "end": "4459030"
  },
  {
    "text": "and then try to\ndetermine the learning rate and that kind of stuff? ",
    "start": "4459030",
    "end": "4465420"
  },
  {
    "text": "Right, so let's see.",
    "start": "4465420",
    "end": "4473179"
  },
  {
    "text": "So I guess to write things out,\nmaybe just pick a random slide.",
    "start": "4473180",
    "end": "4479438"
  },
  {
    "text": "What that bound is\nlooking at is it's looking at basically the\ngeneralization error.",
    "start": "4479438",
    "end": "4485020"
  },
  {
    "text": "It's saying that\nthis generalization error is bounded by-- ",
    "start": "4485020",
    "end": "4494310"
  },
  {
    "text": "yeah. I'm referring-- yeah. Plus-- I'm referring to the\ntail bound, yeah.",
    "start": "4494310",
    "end": "4499739"
  },
  {
    "text": "Yes, but it's like\nsomething like this. This is from memory, but\nsomething like Q of theta",
    "start": "4499740",
    "end": "4507873"
  },
  {
    "text": "by p of theta. And there's some\nother stuff over here.",
    "start": "4507873",
    "end": "4514500"
  },
  {
    "text": "And you're basically saying\nthat if this tail is-- No, there's two\npart of the equation",
    "start": "4514500",
    "end": "4522139"
  },
  {
    "text": "there, like your bound\ngeneralization error to the expected task error.",
    "start": "4522140",
    "end": "4529400"
  },
  {
    "text": "And the expected\ntask error is also bounded by the empirical error. And I'm saying if you can\nseparate those two bounds",
    "start": "4529400",
    "end": "4538880"
  },
  {
    "text": "and use the tail you've\nwritten on the slide to bound the meta-test--",
    "start": "4538880",
    "end": "4544730"
  },
  {
    "text": "I mean, the meta-parameter. But the [INAUDIBLE] about\nthe updating parameter",
    "start": "4544730",
    "end": "4552680"
  },
  {
    "text": "like in the way you updated\nin the MAML, for example.",
    "start": "4552680",
    "end": "4561210"
  },
  {
    "text": "Yeah, potentially. If you could post the\nquestion on Piazza,",
    "start": "4561210",
    "end": "4566310"
  },
  {
    "text": "I think I may be able to\ngive a better response. Sure. Thank you. Yeah.",
    "start": "4566310",
    "end": "4573190"
  },
  {
    "text": "Yeah, so I'm still\na little confused about black-box meta-reinforced\nlearning methods.",
    "start": "4573190",
    "end": "4581270"
  },
  {
    "text": "My understanding is-- this is\njust as how you explained it to the other classmate.",
    "start": "4581270",
    "end": "4588760"
  },
  {
    "text": "So we give trajectories to this\nblack box recurrence network,",
    "start": "4588760",
    "end": "4595349"
  },
  {
    "text": "and it outputs some identifiers\nor like to some later variables",
    "start": "4595350",
    "end": "4605650"
  },
  {
    "text": "that will be fed into\na policy gradient or whatever RL\nmethods for training.",
    "start": "4605650",
    "end": "4615540"
  },
  {
    "text": "Is that what it is? ",
    "start": "4615540",
    "end": "4620890"
  },
  {
    "text": "So we still need to collect\ndata when we train it with the RL methods.",
    "start": "4620890",
    "end": "4626030"
  },
  {
    "text": "And its own policy will\ncollect the data, right? Yeah, exactly. So when I said roll-out policy\nhere, kind of what I meant",
    "start": "4626030",
    "end": "4632860"
  },
  {
    "text": "is you collect data with\nthe policy that you have. OK, got you.",
    "start": "4632860",
    "end": "4638382"
  },
  {
    "text": "That makes sense. All right, thank you. Uh-huh. ",
    "start": "4638382",
    "end": "4648299"
  }
]