[
  {
    "text": "Hello, everybody. Hi. My name is Masha.",
    "start": "5160",
    "end": "10590"
  },
  {
    "text": "Some of you may have met me\nalready as part of office hours and seen me post on Ed\nand things like that.",
    "start": "10590",
    "end": "17189"
  },
  {
    "text": "I'm really excited to be\ngiving the lecture today. It's going to be in a\nslightly different format than",
    "start": "17189",
    "end": "22710"
  },
  {
    "text": "Tengyu's or Chris's. So feel free to give me feedback\non that afterwards on Ed,",
    "start": "22710",
    "end": "28349"
  },
  {
    "text": "or by email, whatever you like. But the topic today is\nactually kind of fun.",
    "start": "28349",
    "end": "33480"
  },
  {
    "text": "So we're going to start\nour foray a little bit into deep learning,\nso neural networks.",
    "start": "33480",
    "end": "41390"
  },
  {
    "text": "I'm assuming everyone here\nhas heard of neural networks before. Anyone who hasn't?",
    "start": "41390",
    "end": "46878"
  },
  {
    "text": "Yeah, that's what I\nthought pretty much. So it'll be fun to see\nhow the ideas of what's",
    "start": "46879",
    "end": "53379"
  },
  {
    "text": "happening in the\nstate of the art actually come back to all\nthe things we've been talking about so far, so ideas\nin linear regression,",
    "start": "53379",
    "end": "60989"
  },
  {
    "text": "ideas in logistic regression. All of this will connect to\nhow neural networks work.",
    "start": "60990",
    "end": "67230"
  },
  {
    "text": "So before I get into the mathy\nnotationy type of material,",
    "start": "67230",
    "end": "73549"
  },
  {
    "text": "I wanted to start\nwith some motivation. Who here has heard of GPT 3?",
    "start": "73549",
    "end": "80890"
  },
  {
    "text": "Yeah, a lot of you. So it made big waves\na couple of years ago. It's a huge, huge model.",
    "start": "80890",
    "end": "87220"
  },
  {
    "text": "But it was able to do\nreally impressive things. Like in this case here, it\ncame up with a poem on its own,",
    "start": "87220",
    "end": "96640"
  },
  {
    "text": "right? And the hope is that\nthese deep learning models learn to be so expressive that\nthey can do creative things,",
    "start": "96640",
    "end": "105290"
  },
  {
    "text": "like creative writing. More recently, this\nwas very, very recent,",
    "start": "105290",
    "end": "111720"
  },
  {
    "text": "has anyone heard of DALL-E 2? A few of you, yeah. So this is very, very recent.",
    "start": "111720",
    "end": "118090"
  },
  {
    "text": "And so DALL-E 2 is able\nto generate images. And the prompt here is an\nastronaut riding a horse",
    "start": "118090",
    "end": "126280"
  },
  {
    "text": "as a pencil drawing. And you can see the main\none that I picked out and then a bunch of different\nversions of what else",
    "start": "126280",
    "end": "132879"
  },
  {
    "text": "it can come up with. So deep learning is very, very\npowerful, for better or worse.",
    "start": "132879",
    "end": "139650"
  },
  {
    "text": "But it also has\nso much potential to do such amazing\nthings, right?",
    "start": "139650",
    "end": "145060"
  },
  {
    "text": "Obviously, this is cool. But there's also applications\nin medicine, applications",
    "start": "145060",
    "end": "150110"
  },
  {
    "text": "in education, applications in\nthings like autonomous driving,",
    "start": "150110",
    "end": "155349"
  },
  {
    "text": "which could hopefully\nmake our roads safer. So there's lots\nof potential here.",
    "start": "155349",
    "end": "161430"
  },
  {
    "text": "And that's sort of the backdrop\nto where we're going to start. We're going to talk\nabout the origin",
    "start": "161430",
    "end": "167569"
  },
  {
    "text": "story of all these things. So we're going to start\nwith supervised learning",
    "start": "167569",
    "end": "172630"
  },
  {
    "text": "with non-linear models. So far most of what we talked\nabout has been linear models.",
    "start": "172630",
    "end": "179290"
  },
  {
    "text": "And now we're getting into\nthe non-linear territory. And I'll talk a little\nbit about what that means.",
    "start": "179290",
    "end": "184390"
  },
  {
    "text": "And then we'll get started\nwith neural networks. We'll figure out what\nthe heck they are,",
    "start": "184390",
    "end": "190159"
  },
  {
    "text": "how we can define them. And next lecture, we'll\nactually talk about how",
    "start": "190159",
    "end": "195350"
  },
  {
    "text": "you can optimize them. And I believe Tengyu will\nbe giving that lecture. Any questions before\nI get started?",
    "start": "195350",
    "end": "202510"
  },
  {
    "text": "Cool. All right, so first, let's\nthink about linear regression.",
    "start": "202510",
    "end": "210519"
  },
  {
    "text": "So we've seen this a bunch. And so we can get started with\na data set that we might have,",
    "start": "210519",
    "end": "216110"
  },
  {
    "text": "right? So we might have some xi's\nand yi's, so some inputs",
    "start": "216110",
    "end": "222840"
  },
  {
    "text": "and outputs in our data set. And say our data set\nis of size n, right?",
    "start": "222840",
    "end": "231620"
  },
  {
    "text": "And we know for\nlinear regression that we have a prediction\nthat we can make according",
    "start": "231620",
    "end": "238080"
  },
  {
    "text": "to something like this.",
    "start": "238080",
    "end": "248129"
  },
  {
    "text": "So we have a linear function\nthat we're using to predict. And this function\ndepends on our inputs x.",
    "start": "248129",
    "end": "256370"
  },
  {
    "text": "Can anyone help me out\nwith what our cost function might look like in terms of y's\nand h's for linear regression?",
    "start": "256370",
    "end": "269200"
  },
  {
    "text": "[INAUDIBLE] OK, h minus y. [INAUDIBLE]",
    "start": "269200",
    "end": "278039"
  },
  {
    "text": "OK. [INAUDIBLE]",
    "start": "278039",
    "end": "287080"
  },
  {
    "text": "I heard somewhere. The sum over i. Sum over i, yeah.",
    "start": "287080",
    "end": "292639"
  },
  {
    "text": "This looks good to me. This look good to\neveryone, familiar? Hopefully we've seen\nthis all before.",
    "start": "292639",
    "end": "299880"
  },
  {
    "text": "Awesome. OK, so we have a\nprediction here.",
    "start": "299880",
    "end": "305919"
  },
  {
    "text": "And we have our label.",
    "start": "305919",
    "end": "313669"
  },
  {
    "text": "And what we can do is we\ncan also write this directly in terms of our parameters.",
    "start": "313669",
    "end": "321199"
  },
  {
    "text": "So our parameters\nhere are theta and b. Cool.",
    "start": "321200",
    "end": "328440"
  },
  {
    "text": "I just plugged it in.",
    "start": "328440",
    "end": "341039"
  },
  {
    "text": "Makes sense so far, right? And what we can do is run\nthings like gradient descent",
    "start": "341039",
    "end": "346370"
  },
  {
    "text": "or stochastic gradient descent\nin order to optimize this.",
    "start": "346370",
    "end": "357240"
  },
  {
    "text": "Cool?",
    "start": "357240",
    "end": "364800"
  },
  {
    "text": "So last lecture, you talked\nabout a slightly different set of models. You talked about kernel models.",
    "start": "364800",
    "end": "371698"
  },
  {
    "text": "And so with kernel models we\nstill have a similar setup. So we have our xi's, our yi's.",
    "start": "371699",
    "end": "387138"
  },
  {
    "text": "And then what does our\nh theta of x look like? So what does our prediction\nlook like with kernel models?",
    "start": "387139",
    "end": "394000"
  },
  {
    "text": "Does anyone remember\nfrom last class? Hint, it's very similar\nto what we had before.",
    "start": "394000",
    "end": "406970"
  },
  {
    "text": "[INAUDIBLE] Yeah. And what's phi of x?",
    "start": "406970",
    "end": "413550"
  },
  {
    "text": "[INAUDIBLE] Sorry? The feature map. The feature map, exactly.",
    "start": "413550",
    "end": "419500"
  },
  {
    "text": "So what's interesting\nabout this setup is we're still linear\nin parameters, right?",
    "start": "419500",
    "end": "430010"
  },
  {
    "text": "But we're non-linear\nin the inputs.",
    "start": "430010",
    "end": "439130"
  },
  {
    "text": "Because phi of x can\nbe any non-linear function that you\ndiscussed last time.",
    "start": "439130",
    "end": "444830"
  },
  {
    "text": "So what if we want\nto be non-linear in both the parameters\nand in the inputs?",
    "start": "444830",
    "end": "453570"
  },
  {
    "text": "So generally\nspeaking, what if we want to do something like this?",
    "start": "453570",
    "end": "470800"
  },
  {
    "text": "Say our h theta of x\nis anything non-linear. So let's say theta 1\ncubed x2 plus maybe",
    "start": "470800",
    "end": "484539"
  },
  {
    "text": "a square root in there theta",
    "start": "484539",
    "end": "491610"
  },
  {
    "text": "the whole thing. So this type of model could\nbe a lot more expressive",
    "start": "491610",
    "end": "498009"
  },
  {
    "text": "potentially. But we also want to think\nabout how can we make",
    "start": "498009",
    "end": "503150"
  },
  {
    "text": "this computationally tractable? How can we make this useful?",
    "start": "503150",
    "end": "508600"
  },
  {
    "text": "OK, so we have some\nnon-linear model.",
    "start": "508600",
    "end": "516459"
  },
  {
    "text": "And by the way, all these notes\nwill be up online afterwards. So if you don't finish\nwriting something, please don't worry about it.",
    "start": "516459",
    "end": "522839"
  },
  {
    "text": "It will be up. And if you want\nto follow along, I should have mentioned\nthis earlier, but there is a\ntemplate up as well.",
    "start": "522839",
    "end": "530959"
  },
  {
    "text": "So it's going to be\nwhat I'm writing on, so the blank version. Cool.",
    "start": "530959",
    "end": "537440"
  },
  {
    "text": "So let's go back to our\nnon-linear model now.",
    "start": "537440",
    "end": "551550"
  },
  {
    "text": "We can assume that our xi's\nare in rd, so some vector",
    "start": "551550",
    "end": "560240"
  },
  {
    "text": "of features, or inputs. And our yi is some\nscalar, so just an r.",
    "start": "560240",
    "end": "570329"
  },
  {
    "text": "This is pretty standard. We've been looking at this type\nof formulation a whole bunch.",
    "start": "570329",
    "end": "575510"
  },
  {
    "text": "And then our h\ntheta is a mapping from our d, which\nis our inputs, to r,",
    "start": "575510",
    "end": "585320"
  },
  {
    "text": "which is the dimensionality\nof our outputs. So the cost function we're going\nto think about here for our now",
    "start": "585320",
    "end": "594860"
  },
  {
    "text": "non-linear model is going\nto look very familiar. So for one example,\ni, we're going",
    "start": "594860",
    "end": "606990"
  },
  {
    "text": "to have ji of theta\nis equal to the square",
    "start": "606990",
    "end": "615990"
  },
  {
    "text": "of the difference between the\nclass label and the prediction. So yi minus h theta of xi\nand all of that squared.",
    "start": "615990",
    "end": "631459"
  },
  {
    "text": "So that's the cost\nfor one example. If we want to get the cost\nfor the whole entire data set,",
    "start": "631460",
    "end": "636980"
  },
  {
    "text": "we're going to average. So what this is\ngoing to look like is we're going to have j\nof theta 1 over n.",
    "start": "636980",
    "end": "644769"
  },
  {
    "text": "n is the size of our data\nset from i equals 1 to n. And then here we\nhave ji of theta.",
    "start": "644769",
    "end": "655130"
  },
  {
    "text": "So this is for entire data set.",
    "start": "655130",
    "end": "661709"
  },
  {
    "text": "OK. So this constant is a little\ndifferent from before.",
    "start": "661709",
    "end": "670310"
  },
  {
    "text": "This is a common convention\nin deep learning. This is called the\nmean squared error.",
    "start": "670310",
    "end": "676019"
  },
  {
    "text": "So it's usually the average. The constant really\ndoesn't matter. Your optimizer is going\nto be the same regardless",
    "start": "676020",
    "end": "682680"
  },
  {
    "text": "of what constant you're going\nto have out front of that sum. Does that make sense?",
    "start": "682680",
    "end": "691940"
  },
  {
    "text": "Cool. All right, so-- now we're\ngoing to talk about how do we--",
    "start": "691940",
    "end": "713690"
  },
  {
    "text": "oh yeah, question? If the constant doesn't\nmatter, why is it there? Like we're typically\nnot supposed",
    "start": "713690",
    "end": "719959"
  },
  {
    "text": "to make n equal to 1, right? So n is the size\nof your data set.",
    "start": "719960",
    "end": "725510"
  },
  {
    "text": "So this is the\nmean squared error. So you're averaging over\nall the squared errors",
    "start": "725510",
    "end": "732180"
  },
  {
    "text": "in your data set. So the reason the\nconstant doesn't matter is when you take the\ngradient, your x that's",
    "start": "732180",
    "end": "740040"
  },
  {
    "text": "going to result\nin your minimum is going to be the same regardless\nof whether it's 1 over n",
    "start": "740040",
    "end": "745870"
  },
  {
    "text": "or 1 over 2. You asked why do\nwe use it at all? Sometimes it's\nhelpful for scaling.",
    "start": "745870",
    "end": "752269"
  },
  {
    "text": "But other than that, there's\nno real magic behind this. It's just a convenient\nthing to do.",
    "start": "752269",
    "end": "758980"
  },
  {
    "text": "It makes sense to\naverage over your errors. Yeah, it's a nice\nway to scale things.",
    "start": "758980",
    "end": "764640"
  },
  {
    "text": "Thank you. No problem. Cool. So what we want to do once\nwe have this cost function is",
    "start": "764640",
    "end": "772779"
  },
  {
    "text": "we want to minimize it, right? One way to do that, we\ncan use gradient descent.",
    "start": "772779",
    "end": "779459"
  },
  {
    "text": "And so this notation\nhere, we're assigning this kind of like coding\nnotation you can think of,",
    "start": "779459",
    "end": "788199"
  },
  {
    "text": "we're assigning the right\nside to the left side.",
    "start": "788199",
    "end": "803089"
  },
  {
    "text": "Can anyone tell me why this\nis gradient descent, what is written here, and not\nstochastic gradient descent?",
    "start": "803090",
    "end": "811050"
  },
  {
    "text": "What makes it gradient descent? Yeah. Reason the whole data set.",
    "start": "811050",
    "end": "818490"
  },
  {
    "text": "Yeah, exactly. So you're considering\nthe whole data set here. And the reason for\nthat is if we write out",
    "start": "818490",
    "end": "825120"
  },
  {
    "text": "what's actually going on\nhere, our j of theta is-- so that 1 over n is here.",
    "start": "825120",
    "end": "831250"
  },
  {
    "text": "And we have i equals That's theta, right?",
    "start": "831250",
    "end": "837060"
  },
  {
    "text": "So we're reasoning over\nthe whole data set.",
    "start": "837060",
    "end": "846519"
  },
  {
    "text": "So each update here\nconsiders the entire data set that we have.",
    "start": "846519",
    "end": "851829"
  },
  {
    "text": "Cool? All right, so here's\nstochastic gradient descent.",
    "start": "851829",
    "end": "859360"
  },
  {
    "text": "So the idea here is\nwe're considering now",
    "start": "859360",
    "end": "864509"
  },
  {
    "text": "only one single example\nevery time we update theta.",
    "start": "864509",
    "end": "869690"
  },
  {
    "text": "So we have some\nhyperparameter alpha, alpha same as for gradient\ndescent some number",
    "start": "869690",
    "end": "877490"
  },
  {
    "text": "greater than 0. And we're basically initializing\nour parameters theta randomly",
    "start": "877490",
    "end": "886110"
  },
  {
    "text": "at the start. And then we go through\nsome number of iterations.",
    "start": "886110",
    "end": "891810"
  },
  {
    "text": "We sample some example\nfrom our data set.",
    "start": "891810",
    "end": "897100"
  },
  {
    "text": "And we do this with replacement.",
    "start": "897100",
    "end": "905880"
  },
  {
    "text": "And we continue on until\neither we converge to something we're happy with or we\nreach our end maximum",
    "start": "905880",
    "end": "914670"
  },
  {
    "text": "number of iterations. So this is width replacement. I'm going to briefly\nsketch out what SGD usually",
    "start": "914670",
    "end": "923310"
  },
  {
    "text": "looks like more commonly\nin deep learning settings, just so you guys have an idea. So this is one variant.",
    "start": "923310",
    "end": "930250"
  },
  {
    "text": "Here is going to be another one.",
    "start": "930250",
    "end": "937630"
  },
  {
    "text": "And so the variant that's\nin algorithm one, that's going to be in your notes. This other one is not.",
    "start": "937630",
    "end": "942860"
  },
  {
    "text": "But I just think it's helpful\nto know some of the terminology when you go into the fray\nof deep learning as well.",
    "start": "942860",
    "end": "950259"
  },
  {
    "text": "OK, so we go through a\nfor loop, where let's say,",
    "start": "950259",
    "end": "965600"
  },
  {
    "text": "we're indexing at k. We have 1 to n epoch.",
    "start": "965600",
    "end": "973579"
  },
  {
    "text": "So epoch is basically\na term to mean",
    "start": "973580",
    "end": "978759"
  },
  {
    "text": "you've gone through\nyour entire data set. So in deep learning\nyou'll often see,",
    "start": "978759",
    "end": "984319"
  },
  {
    "text": "or if you read the deep\nlearning papers, you'll see oh, we trained for blah\nnumber of epochs.",
    "start": "984319",
    "end": "989829"
  },
  {
    "text": "That's what that means. That's how many times the\noptimization has basically",
    "start": "989830",
    "end": "994980"
  },
  {
    "text": "gone through the data set. So for k equals 1 to n epoch\nwe can shuffle the data.",
    "start": "994980",
    "end": "1010490"
  },
  {
    "text": "And then for j from",
    "start": "1010490",
    "end": "1017040"
  },
  {
    "text": "we might not have\nenough time or desire to go through the\nentire data set.",
    "start": "1017040",
    "end": "1022490"
  },
  {
    "text": "So maybe we decide\nthat we want to go through 500 examples\nout of the data set and call that an epoch.",
    "start": "1022490",
    "end": "1027699"
  },
  {
    "text": "That's also fine. So we have 4j\nequals 1 to n iter.",
    "start": "1027699",
    "end": "1034089"
  },
  {
    "text": "We're doing the\nsame type of update.",
    "start": "1034089",
    "end": "1049400"
  },
  {
    "text": "And here we have no\nreplacement in this inner for loop with the\nj index because we",
    "start": "1049400",
    "end": "1056809"
  },
  {
    "text": "don't want to look at the same\nexample twice in the data set. Does this make sense?",
    "start": "1056809",
    "end": "1062059"
  },
  {
    "text": "Yep. Sorry, reall quick, [INAUDIBLE] Yeah, pretty much, pretty\nmuch, and slightly different",
    "start": "1062059",
    "end": "1069929"
  },
  {
    "text": "terminology. Cool? Yeah. [INAUDIBLE] So you basically\nlike having this number,",
    "start": "1069929",
    "end": "1081809"
  },
  {
    "text": "by n epoch you mean n times 1\nepoch or one epoch in itself?",
    "start": "1081809",
    "end": "1089860"
  },
  {
    "text": "If the data set has n samples,\nare you going to go to 1 to n?",
    "start": "1089860",
    "end": "1097150"
  },
  {
    "text": "Or are you going from theta The latter. So the question is,\nwhat does an epoch mean?",
    "start": "1097150",
    "end": "1105130"
  },
  {
    "text": "And basically, it's if you go\nthrough the entire data set, it's how many times you go\nthrough the entire data set.",
    "start": "1105130",
    "end": "1113330"
  },
  {
    "text": "Any other questions here? Cool. I'll talk about the last version\nof gradient descent for today.",
    "start": "1113330",
    "end": "1121049"
  },
  {
    "text": "And this is mini batch\ngradient descent. So with mini batch\ngradient descent,",
    "start": "1121049",
    "end": "1126549"
  },
  {
    "text": "the main difference is you're\nconsidering b, or a batch number of gradients, at a time.",
    "start": "1126549",
    "end": "1134200"
  },
  {
    "text": "The reason we want to\ndo this is with things like GPUs and parallelism, this\nactually speeds up computation.",
    "start": "1134200",
    "end": "1142799"
  },
  {
    "text": "So we can compute b\ngradients at the same time",
    "start": "1142799",
    "end": "1148610"
  },
  {
    "text": "or simultaneously, as opposed\nto doing them sequentially. And that can be\nquite a bit faster.",
    "start": "1148610",
    "end": "1154880"
  },
  {
    "text": "So let me write\nout some of that. So we're computing b gradients.",
    "start": "1154880",
    "end": "1167720"
  },
  {
    "text": "And they look like\nmaybe grad j j1 of theta",
    "start": "1167720",
    "end": "1176190"
  },
  {
    "text": "all the way to grad jb of theta.",
    "start": "1176190",
    "end": "1186419"
  },
  {
    "text": "And we do this simultaneously.",
    "start": "1186419",
    "end": "1212200"
  },
  {
    "text": "So one question you\nmight be thinking about, how do you choose b? And very often, you\nchoose b empirically.",
    "start": "1212200",
    "end": "1220600"
  },
  {
    "text": "So you test things out. You look at your validation\nset, things like that.",
    "start": "1220600",
    "end": "1226100"
  },
  {
    "text": "And you'll talk about\nevaluation a bit later as well. But one way to choose\nb is choose the maximum",
    "start": "1226100",
    "end": "1234679"
  },
  {
    "text": "b that your GPU\nmemory can handle. And that's sort of a good way\nto speed up what you're doing.",
    "start": "1234679",
    "end": "1243720"
  },
  {
    "text": "But the trade off is usually-- and this might come up in\nyour homework as well--",
    "start": "1243720",
    "end": "1252169"
  },
  {
    "text": "usually the lower\nthe b, the better the performance\nof the algorithm.",
    "start": "1252169",
    "end": "1260000"
  },
  {
    "text": "And I'm not going\nto talk too much about why this is the case. This is also active\nresearch, but just",
    "start": "1260000",
    "end": "1266520"
  },
  {
    "text": "to give you some\nidea of how folks go about choosing these numbers.",
    "start": "1266520",
    "end": "1279090"
  },
  {
    "text": "Any questions about mini\nbatch gradient descent? Yeah? My guess is exactly the\nsame as model gradient",
    "start": "1279090",
    "end": "1292730"
  },
  {
    "text": "descent, except that\nyou're just doing this entire thing over a batch\nas opposed to the whole data",
    "start": "1292730",
    "end": "1299680"
  },
  {
    "text": "set, right? Yes, yes, precisely. Any other questions?",
    "start": "1299680",
    "end": "1306150"
  },
  {
    "text": "Yeah. I have a question\nabout [INAUDIBLE].. Uh-huh. In this slide.",
    "start": "1306150",
    "end": "1311350"
  },
  {
    "text": "Yeah. What does that\nexactly [INAUDIBLE].. Yeah, so with replacement means\nsay we picked example two.",
    "start": "1311350",
    "end": "1320440"
  },
  {
    "text": "We can pick example two\nagain in the future.",
    "start": "1320440",
    "end": "1325908"
  },
  {
    "text": "Whereas without replacement\nin the corner case, it means if we picked example\ntwo within that j for loop,",
    "start": "1325909",
    "end": "1336389"
  },
  {
    "text": "we will not pick\nexample two again until we get to the next epoch. So an example one is\ngoing to have [INAUDIBLE]",
    "start": "1336390",
    "end": "1344860"
  },
  {
    "text": "You're just randomly\nchoosing one example. It can happen to be\nthe same example. It cannot.",
    "start": "1344860",
    "end": "1350440"
  },
  {
    "text": "Does that make sense? [INAUDIBLE] the entire data set? Or do you just randomly-- In the middle one, you\nrandomly pick examples",
    "start": "1350440",
    "end": "1358159"
  },
  {
    "text": "until you reach an iter. Whereas in the left\none, you're more",
    "start": "1358159",
    "end": "1363409"
  },
  {
    "text": "trying to go through\nyour entire data set and then do that again.",
    "start": "1363410",
    "end": "1373408"
  },
  {
    "text": "Any other questions on anything\ngradient descent related?",
    "start": "1373409",
    "end": "1380250"
  },
  {
    "text": "Yeah. So on the mini batch\ncase, we actually do not--",
    "start": "1380250",
    "end": "1389900"
  },
  {
    "text": "there is no variant\nwas replaced.",
    "start": "1389900",
    "end": "1396820"
  },
  {
    "text": "So with the mini batch, no,\nbecause you're considering, say, your batch size is",
    "start": "1396820",
    "end": "1405690"
  },
  {
    "text": "You don't want multiple\nexamples in that to be the same. You want all the\nexamples to be different.",
    "start": "1405690",
    "end": "1412480"
  },
  {
    "text": "But-- Mm-hmm. Between batches do we\nreplace these elements back",
    "start": "1412480",
    "end": "1418990"
  },
  {
    "text": "to the stack? Between batches you can. You can, yeah.",
    "start": "1418990",
    "end": "1425440"
  },
  {
    "text": "Sometimes you don't. Sometimes you do. It's sort of like\na design choice. But you can. In this case, when it's\nsaying without replacement,",
    "start": "1425440",
    "end": "1432620"
  },
  {
    "text": "it means that within one\nbatch you don't have doubles. Thank you. Yeah, no worries.",
    "start": "1432620",
    "end": "1439540"
  },
  {
    "text": "Other questions? OK. All right.",
    "start": "1439540",
    "end": "1445039"
  },
  {
    "text": "So next we're actually going\nto define some neural networks. So we talked a little bit about\nhow we optimize these things.",
    "start": "1445039",
    "end": "1453030"
  },
  {
    "text": "But we didn't really get\ninto, for example, well, how do we define the\nneural network yet.",
    "start": "1453030",
    "end": "1459270"
  },
  {
    "text": "Or how do we compute\nthe gradients? Like when we talk about\nmultiple gradients,",
    "start": "1459270",
    "end": "1465440"
  },
  {
    "text": "how do we actually\ncompute these things? And we need to be able to\ndo this in a way that's computationally efficient.",
    "start": "1465440",
    "end": "1471250"
  },
  {
    "text": "And progress in\nmachine learning really took off in the last 10, 20\nyears because of advancements",
    "start": "1471250",
    "end": "1482200"
  },
  {
    "text": "in hardware because we're\nable to parallelize on GPUs and make these things\nso much faster,",
    "start": "1482200",
    "end": "1489140"
  },
  {
    "text": "and also, algorithmic\ndevelopments as well, of course. Cool.",
    "start": "1489140",
    "end": "1494679"
  },
  {
    "text": "So we're going to talk\nabout neural networks today and how we define them. The back propagation,\nwhich is how do we",
    "start": "1494679",
    "end": "1500929"
  },
  {
    "text": "compute these gradients, that\nwill be covered next lecture.",
    "start": "1500929",
    "end": "1507190"
  },
  {
    "text": "So this example came\nup at the very start, I think maybe first lecture\nor something like that.",
    "start": "1507190",
    "end": "1513820"
  },
  {
    "text": "But the example\nhere, we're looking to predict housing prices.",
    "start": "1513820",
    "end": "1520279"
  },
  {
    "text": "And we looked at how we can do\nthis with linear regression, with linear models.",
    "start": "1520279",
    "end": "1527000"
  },
  {
    "text": "And say with our linear model,\ngiven the size of a house, we have some data, where we\nhave some size and prices.",
    "start": "1527000",
    "end": "1535908"
  },
  {
    "text": "And we can plot\nthem on this plot. And our model maybe\nlooks something like this with something\nlike linear regression.",
    "start": "1535909",
    "end": "1546389"
  },
  {
    "text": "What's a problem with this? Why might this not work so well?",
    "start": "1546390",
    "end": "1563648"
  },
  {
    "text": "Any thoughts? Yeah. Something non-linear\npredicts it better.",
    "start": "1563649",
    "end": "1571380"
  },
  {
    "text": "Yep, yep, yep. That's a good first one.",
    "start": "1571380",
    "end": "1578659"
  },
  {
    "text": "So prediction might have\na non-linear relationship with the input, right?",
    "start": "1578660",
    "end": "1583679"
  },
  {
    "text": "And we only have a\nlinear [INAUDIBLE].. So maybe that model doesn't\ncapture the relationship that well.",
    "start": "1583679",
    "end": "1592830"
  },
  {
    "text": "What's another reason\nthis is not great? [INAUDIBLE] Sorry, repeat that.",
    "start": "1592830",
    "end": "1599580"
  },
  {
    "text": "Prices can't be negative. Yes. [INAUDIBLE]",
    "start": "1599580",
    "end": "1605170"
  },
  {
    "text": "Yes, exactly. The second issue with this is\nthat prices can't be negative.",
    "start": "1605170",
    "end": "1611960"
  },
  {
    "text": "And when we have a linear model\nlike we do, they can, right? Nothing's preventing\nthem from being negative.",
    "start": "1611960",
    "end": "1618059"
  },
  {
    "text": "Can anyone think of like\nthe best simplest thing you can do to fix this problem?",
    "start": "1618059",
    "end": "1623090"
  },
  {
    "text": "Yeah. You can fix the\nintercept to be 0. Sorry, say that louder.",
    "start": "1623090",
    "end": "1628600"
  },
  {
    "text": "You can fix the\nintercept to be 0. Yes, you can fix the\nintercept to be 0.",
    "start": "1628600",
    "end": "1634660"
  },
  {
    "text": "So what does that mean\nfor negative numbers?",
    "start": "1634660",
    "end": "1643100"
  },
  {
    "text": "So fixing the intercept to\nbe 0 would be this, right? Yeah?",
    "start": "1643100",
    "end": "1648210"
  },
  {
    "text": "You have whatever\nhouse was negative price will have negative size,\nwhich also doesn't make sense.",
    "start": "1648210",
    "end": "1660390"
  },
  {
    "text": "Yeah, so one thing we can do-- let me write out\nthese issues first.",
    "start": "1660390",
    "end": "1665790"
  },
  {
    "text": "But I will show you a\nsolution in just a second. Does anyone a function\nthat can fix this for us?",
    "start": "1665790",
    "end": "1671780"
  },
  {
    "text": "[INAUDIBLE] Sorry, say that louder. [INAUDIBLE]",
    "start": "1671780",
    "end": "1677240"
  },
  {
    "text": "OK, any other ones? [INAUDIBLE] Yeah, ReLUs.",
    "start": "1677240",
    "end": "1683380"
  },
  {
    "text": "Has anyone heard of ReLUs? One person, a few.",
    "start": "1683380",
    "end": "1690268"
  },
  {
    "text": "OK, so we're going to talk\nabout ReLUs in just a second. So the issues that\nwe talked about here",
    "start": "1690269",
    "end": "1696440"
  },
  {
    "text": "are the prediction might have\na non-linear relationship",
    "start": "1696440",
    "end": "1703600"
  },
  {
    "text": "with the input. And the second issue is we\ncan have negative prices.",
    "start": "1703600",
    "end": "1716820"
  },
  {
    "text": "OK, so what we want\nour ReLU to look like,",
    "start": "1716820",
    "end": "1728440"
  },
  {
    "text": "or what we want this\nfunction to look like is something like this. So basically, for all things\nthat are in the negative side,",
    "start": "1728440",
    "end": "1737880"
  },
  {
    "text": "we map to 0. So that's what our\nReLU is going to be.",
    "start": "1737880",
    "end": "1744549"
  },
  {
    "text": "And what that looks\nlike in math terms, we have our regular prediction.",
    "start": "1744549",
    "end": "1751059"
  },
  {
    "text": "But we want the maximum between\nwhat linear regression would",
    "start": "1751059",
    "end": "1757970"
  },
  {
    "text": "output as a prediction and 0. And then our parameters here\nare going to be w and b.",
    "start": "1757970",
    "end": "1768230"
  },
  {
    "text": "So this is ReLU. This is how it's\nusually written.",
    "start": "1768230",
    "end": "1774040"
  },
  {
    "text": "And this is the notation\nthat we will use. So you can say that ReLU\nis a function of t here.",
    "start": "1774040",
    "end": "1785760"
  },
  {
    "text": "And then really we can\njust write our prediction as this ReLU function,\nwhich is wx plus b.",
    "start": "1785760",
    "end": "1798789"
  },
  {
    "text": "And does anyone know\nwhat ReLU is, what it's called in deep learning terms?",
    "start": "1798789",
    "end": "1805440"
  },
  {
    "text": "Yes. What's a [INAUDIBLE]",
    "start": "1805440",
    "end": "1810470"
  },
  {
    "text": "Yes. So what category of functions\nis it in deep learning? Yeah. Activation function.",
    "start": "1810470",
    "end": "1815740"
  },
  {
    "text": "Exactly, activation function. So our ReLU is an\nactivation function.",
    "start": "1815740",
    "end": "1825929"
  },
  {
    "text": "Can anyone tell\nme is ReLU linear? [INAUDIBLE] OK, raise hands if\nyou think it's linear.",
    "start": "1825929",
    "end": "1835090"
  },
  {
    "text": "OK. Non-linear? Yeah, yeah. It is definitely non-linear.",
    "start": "1835090",
    "end": "1841220"
  },
  {
    "text": "The maximum creates\nour nonlinearity there. OK, so this is our nonlinearity\nin deep learning, or often is.",
    "start": "1841220",
    "end": "1858809"
  },
  {
    "text": "It's also sometimes\ncalled one neuron.",
    "start": "1858809",
    "end": "1865268"
  },
  {
    "text": "OK. So going back to our housing\nprice prediction set up--",
    "start": "1865269",
    "end": "1873600"
  },
  {
    "text": "yeah, question? I have a question about\nour activation function. Are they all supposed\nto be non-linear?",
    "start": "1873600",
    "end": "1879559"
  },
  {
    "text": "Yes, activation functions\nare by definition non-linear. We'll talk at the end\na little bit about what",
    "start": "1879559",
    "end": "1886419"
  },
  {
    "text": "happens if they are linear. Other questions? Cool.",
    "start": "1886419",
    "end": "1891950"
  },
  {
    "text": "All right, so let's set up our\nhigh-dimensional input example.",
    "start": "1891950",
    "end": "1905100"
  },
  {
    "text": "So far especially\nin this plot, we have been looking at one\ninput, one output, so scalar to scalar.",
    "start": "1905100",
    "end": "1910870"
  },
  {
    "text": "So what if we have\nmore features, so high-dimensional input?",
    "start": "1910870",
    "end": "1917700"
  },
  {
    "text": "And this is the case when\nwe have x be in rd and y",
    "start": "1917700",
    "end": "1924299"
  },
  {
    "text": "be still scalar. So we're still predicting\nhousing prices, for example.",
    "start": "1924299",
    "end": "1929870"
  },
  {
    "text": "So our new terminology\nfor our prediction",
    "start": "1929870",
    "end": "1936289"
  },
  {
    "text": "is ReLU of wtx plus b.",
    "start": "1936289",
    "end": "1945419"
  },
  {
    "text": "And so our x is just going to\nbe stacked features or inputs. So we have x1 all the way to xd.",
    "start": "1945419",
    "end": "1957370"
  },
  {
    "text": "And this is in rd. And then our weights,\nour weight vector,",
    "start": "1957370",
    "end": "1968970"
  },
  {
    "text": "w is going to be\nin what dimension? Can anyone tell me, based\noff of how I've written it?",
    "start": "1968970",
    "end": "1975080"
  },
  {
    "text": "[INAUDIBLE] d, that's right because we're\nmaking a dot product with x.",
    "start": "1975080",
    "end": "1981340"
  },
  {
    "text": "And then our b is\ncalled our bias.",
    "start": "1981340",
    "end": "1987380"
  },
  {
    "text": "And it is going to be scalar. What we want to do\nin deep learning",
    "start": "1987380",
    "end": "1993360"
  },
  {
    "text": "is we want to stack\nthese neurons.",
    "start": "1993360",
    "end": "2004470"
  },
  {
    "text": "So output of\nactivation functions",
    "start": "2004470",
    "end": "2013460"
  },
  {
    "text": "is going to be the\ninput to the next one.",
    "start": "2013460",
    "end": "2022210"
  },
  {
    "text": "And this is what creates that\nexpressivity of deep learning models. Basically they have a bunch of\nnonlinearities that are stacked",
    "start": "2022210",
    "end": "2030250"
  },
  {
    "text": "one on top of the other. And this becomes a\nsuper flexible framework",
    "start": "2030250",
    "end": "2035590"
  },
  {
    "text": "that can represent a lot\nof different domains.",
    "start": "2035590",
    "end": "2041919"
  },
  {
    "text": "So now let's make the housing\nprice prediction problem a little bit more concrete.",
    "start": "2041919",
    "end": "2048540"
  },
  {
    "text": "So let's say our x\nis going to be in R4.",
    "start": "2048540",
    "end": "2054480"
  },
  {
    "text": "And say besides size, or\nsquare footage, which is x1,",
    "start": "2054480",
    "end": "2060388"
  },
  {
    "text": "we're also going to\nconsider things like number of bedrooms in the house.",
    "start": "2060389",
    "end": "2071980"
  },
  {
    "text": "What's another thing? Maybe we can consider the zip\ncode that the house is in.",
    "start": "2071980",
    "end": "2077700"
  },
  {
    "text": "Maybe it's close to a\nsubway or something, right?",
    "start": "2077700",
    "end": "2083908"
  },
  {
    "text": "And the last thing\nwe'll consider is maybe something like\nwealth of the neighborhood.",
    "start": "2083909",
    "end": "2090829"
  },
  {
    "text": "So these are our\nfeatures or inputs. And what we might\nwant to do is compute",
    "start": "2090829",
    "end": "2097609"
  },
  {
    "text": "some intermediate variables.",
    "start": "2097609",
    "end": "2103799"
  },
  {
    "text": "So what I mean\nhere is maybe there are some things that\ncombine some of these ideas",
    "start": "2103800",
    "end": "2112980"
  },
  {
    "text": "and help us make a prediction\nfor what the price of the house might be. So one example is maybe\nthe maximum family size",
    "start": "2112980",
    "end": "2122349"
  },
  {
    "text": "that a particular\nhouse can accommodate. So what would the\nmaximum family size",
    "start": "2122349",
    "end": "2133390"
  },
  {
    "text": "potentially depend on, out of\nthe four inputs that we have?",
    "start": "2133390",
    "end": "2147039"
  },
  {
    "text": "Size and number of bedrooms. Size the number of bedrooms. Yeah, I would agree with that.",
    "start": "2147040",
    "end": "2152050"
  },
  {
    "text": "So we can also think\nof other variables, like maybe how walkable\nthe neighborhood is.",
    "start": "2152050",
    "end": "2160510"
  },
  {
    "text": "And that might depend on\nthe zip code, for example. And the last one is maybe school\nquality in the neighborhood",
    "start": "2160510",
    "end": "2169970"
  },
  {
    "text": "that this house is in. And this will be our a3. So a1 through a3 are some\nintermediate variables",
    "start": "2169970",
    "end": "2177119"
  },
  {
    "text": "that we think might be\nhelpful to make predictions about housing prices.",
    "start": "2177119",
    "end": "2182250"
  },
  {
    "text": "Well, so how could we\nmaybe write these out",
    "start": "2182250",
    "end": "2187770"
  },
  {
    "text": "in terms of math notation? Well, let's use our ReLUs\nthat we just found out about",
    "start": "2187770",
    "end": "2195859"
  },
  {
    "text": "and do ReLU of some linear\ncombination of the features",
    "start": "2195859",
    "end": "2202140"
  },
  {
    "text": "that we think might make\nsense in this context. So maximum family size, maybe we\nhave combo, like someone said,",
    "start": "2202140",
    "end": "2212800"
  },
  {
    "text": "of the size, which is x1. And then maybe we add the\nnumber of bedrooms, which is x2.",
    "start": "2212800",
    "end": "2220289"
  },
  {
    "text": "And we have some\nbias term, which is going to be theta 3 here. So theta 1, theta 2, theta",
    "start": "2220290",
    "end": "2228730"
  },
  {
    "text": "And then we can\ndo the same thing for the rest of these\nintermediate variables.",
    "start": "2228730",
    "end": "2233869"
  },
  {
    "text": "So we can say theta 4. So walkability depends\non x3, which is code.",
    "start": "2233869",
    "end": "2240500"
  },
  {
    "text": "And then we have\nanother bias term. And then finally a3.",
    "start": "2240500",
    "end": "2246130"
  },
  {
    "text": "So we have ReLU of theta 6.",
    "start": "2246130",
    "end": "2251630"
  },
  {
    "text": "So a3 is walkability. Maybe that depends on-- or sorry, a3 is school quality.",
    "start": "2251630",
    "end": "2258670"
  },
  {
    "text": "So that depends\nmaybe on zip code, which is x3, and maybe on wealth\nof the neighborhood, which",
    "start": "2258670",
    "end": "2266090"
  },
  {
    "text": "is x4. And then we have some\nbias here as well. So these are the\nintermediate variables",
    "start": "2266090",
    "end": "2272630"
  },
  {
    "text": "that we think might\nbe helpful here, OK?",
    "start": "2272630",
    "end": "2277809"
  },
  {
    "text": "And the last thing that we\nmight want to do here is--",
    "start": "2277810",
    "end": "2284490"
  },
  {
    "text": "sorry, let me just\nchange this notation so it's not confusing\nfor the notes later.",
    "start": "2284490",
    "end": "2291630"
  },
  {
    "text": "These are all w's. And the biases are\ngoing to be b's. These are b2's, b1's.",
    "start": "2291630",
    "end": "2312310"
  },
  {
    "text": "Oh, sorry.",
    "start": "2312310",
    "end": "2324390"
  },
  {
    "text": "No, I actually had it\nright the first time.",
    "start": "2324390",
    "end": "2338640"
  },
  {
    "text": "Ignore me. So these were all\ntheta parameters. At the end of the day,\nit doesn't matter. I just want to make sure that it\nmatches your notes so it's not",
    "start": "2338640",
    "end": "2346530"
  },
  {
    "text": "confusing later. So 1, 2, 3, theta 4, theta 5.",
    "start": "2346530",
    "end": "2366319"
  },
  {
    "text": "So this is actually one\nlayer that we defined here. OK?",
    "start": "2366320",
    "end": "2372570"
  },
  {
    "text": "And finally, once we have\nthese intermediate variables, we're actually going\nto make the output. We're going to\nconstruct the output.",
    "start": "2372570",
    "end": "2380350"
  },
  {
    "text": "And our output is\nour h theta of x, which is going to be, if we\nfollow this construction,",
    "start": "2380350",
    "end": "2388890"
  },
  {
    "text": "we're going to write ReLU of. And here we're going\nto make combinations,",
    "start": "2388890",
    "end": "2394500"
  },
  {
    "text": "a linear combination, of these\nintermediate variables a. So we're going to have\ntheta 9 a1 plus theta 10 a2",
    "start": "2394500",
    "end": "2406480"
  },
  {
    "text": "plus theta 11 a3 plus theta 12.",
    "start": "2406480",
    "end": "2413190"
  },
  {
    "text": "OK, one thing is this is going\nto be our end goal or end",
    "start": "2413190",
    "end": "2418328"
  },
  {
    "text": "prediction here. One thing that usually\nhappens in deep learning",
    "start": "2418329",
    "end": "2424000"
  },
  {
    "text": "is we actually for the\noutput we don't use a ReLU. So it's sort of like convention.",
    "start": "2424000",
    "end": "2431240"
  },
  {
    "text": "Nothing is necessarily\nstopping you from doing that. It's just by convention,\nusually we just have a linear layer at the end.",
    "start": "2431240",
    "end": "2438849"
  },
  {
    "text": "Cool. So now that we look\nat this diagram that's",
    "start": "2438849",
    "end": "2444890"
  },
  {
    "text": "here on your right,\nwe have all the things",
    "start": "2444890",
    "end": "2449910"
  },
  {
    "text": "that I talked about, right? We have the size, the\nnumber of bedrooms, the zip code, the wealth. And it's going into these\nintermediate variables.",
    "start": "2449910",
    "end": "2458680"
  },
  {
    "text": "So this is a1, a2, a3. OK?",
    "start": "2458680",
    "end": "2463818"
  },
  {
    "text": "And the weights that\nwe're considering that are relating sort of taking\nfrom the first set of inputs",
    "start": "2463819",
    "end": "2471480"
  },
  {
    "text": "x to a. So for example, here\nwe have theta 1.",
    "start": "2471480",
    "end": "2476568"
  },
  {
    "text": "Here we're going to\nhave theta 2, and so on. Does this make sense?",
    "start": "2476569",
    "end": "2483390"
  },
  {
    "text": "And the structure that we--\nyeah, sorry, a question? I had a question about the\ndimensionality of the weights.",
    "start": "2483390",
    "end": "2489200"
  },
  {
    "text": "Right now they're all scalar. Everything we're talking\nabout right now is scalar.",
    "start": "2489200",
    "end": "2496010"
  },
  {
    "text": "So whenever we have the\nsubscript, it's usually scalar. OK. Yep?",
    "start": "2496010",
    "end": "2501100"
  },
  {
    "text": "Here are you having a few\nintermediate towards the end. Are you just using those\nintermediate [INAUDIBLE]",
    "start": "2501100",
    "end": "2506430"
  },
  {
    "text": "Yes. It's possible that some of\nthose original variables might still be useful.",
    "start": "2506430",
    "end": "2512150"
  },
  {
    "text": "Yes. So is there any way you\ncan transport them over towards the end or something?",
    "start": "2512150",
    "end": "2519040"
  },
  {
    "text": "Yeah, I mean, if you look at a2,\nfor example, if you set the--",
    "start": "2519040",
    "end": "2525110"
  },
  {
    "text": "say x3 is positive. And you set theta 4\nto 1 and theta 5 to 0,",
    "start": "2525110",
    "end": "2533080"
  },
  {
    "text": "then you transfer over all\nthe information from x3. But this structure, we came up\nwith the structure based off",
    "start": "2533080",
    "end": "2543609"
  },
  {
    "text": "of our knowledge, right? This was not something that was\ndetermined by some algorithm. We just came up with it because\nit seemed to make sense.",
    "start": "2543609",
    "end": "2550779"
  },
  {
    "text": "So this is called\nprior knowledge and infusing your model\nwith it basically.",
    "start": "2550780",
    "end": "2559160"
  },
  {
    "text": "But what if we want to\nbe a little more general? What if we don't want or\ndon't have the prior knowledge",
    "start": "2559160",
    "end": "2567640"
  },
  {
    "text": "to do this in a way that\nresults in good performance? So this is getting\ninto something called",
    "start": "2567640",
    "end": "2574560"
  },
  {
    "text": "fully-connected neural network. And what this means\nis we no longer",
    "start": "2574560",
    "end": "2580569"
  },
  {
    "text": "think about these\nideas of family size, walkability, school quality.",
    "start": "2580569",
    "end": "2586450"
  },
  {
    "text": "We don't know what those\nintermediate variables might be. But maybe they depend\non all of the inputs.",
    "start": "2586450",
    "end": "2593589"
  },
  {
    "text": "So each intermediate\nvariable will depend on every single input. So this is going to get messy.",
    "start": "2593589",
    "end": "2600730"
  },
  {
    "text": "But I'll try to use\ndifferent colors. So every single variable here\nwill depend on all the ones",
    "start": "2600730",
    "end": "2609140"
  },
  {
    "text": "that came previously.",
    "start": "2609140",
    "end": "2615160"
  },
  {
    "text": "And this is a much\nmore general way of thinking about this, r ight?",
    "start": "2615160",
    "end": "2620450"
  },
  {
    "text": "We don't have to infuse\nthis prior knowledge into the system. We can let the neural\nnetwork figure it out.",
    "start": "2620450",
    "end": "2631070"
  },
  {
    "text": "So what this looks like\nmathematically is maybe we would have a1 b equal ReLU of\nsome weight x1 plus some weight",
    "start": "2631070",
    "end": "2644920"
  },
  {
    "text": "x2 plus some weight x3\nplus another weight x4",
    "start": "2644920",
    "end": "2650040"
  },
  {
    "text": "and plus the bias term. And we can do this for a2\nand so on and so forth.",
    "start": "2650040",
    "end": "2655390"
  },
  {
    "text": "Does that make sense? Cool.",
    "start": "2655390",
    "end": "2660720"
  },
  {
    "text": "So what this looks\nlike if we start looking at vector\nnotation now more,",
    "start": "2660720",
    "end": "2668730"
  },
  {
    "text": "this might look like this. So we have a1 is equal\nto the ReLU of w1.",
    "start": "2668730",
    "end": "2681609"
  },
  {
    "text": "This higher index notation\nin square brackets",
    "start": "2681609",
    "end": "2687440"
  },
  {
    "text": "is going to refer to layers. So this would be\nthe first layer.",
    "start": "2687440",
    "end": "2693039"
  },
  {
    "text": "And this would be the second\nlayer in this network.",
    "start": "2693040",
    "end": "2698609"
  },
  {
    "text": "So we have w1 layer 1 transposed\nwith all the inputs x.",
    "start": "2698609",
    "end": "2704000"
  },
  {
    "text": "Now this is a vector. So now we're doing\na dot product. And we add some bias term.",
    "start": "2704000",
    "end": "2713240"
  },
  {
    "text": "Can anyone tell me\nwhat dimensionality w1",
    "start": "2713240",
    "end": "2719619"
  },
  {
    "text": "in the first layer is? Yeah. Four. Yes, that's right.",
    "start": "2719619",
    "end": "2728540"
  },
  {
    "text": "And that's because our\nx is dimensionality 4 and our bias is still scalar.",
    "start": "2728540",
    "end": "2736500"
  },
  {
    "text": "And then we can\ndo the same thing. We can say this\nstill first layer--",
    "start": "2736500",
    "end": "2753220"
  },
  {
    "text": "and the last one same thing.",
    "start": "2753220",
    "end": "2766420"
  },
  {
    "text": "And then finally, we\nhave our prediction.",
    "start": "2766420",
    "end": "2772220"
  },
  {
    "text": "And the prediction now\nis going to use weights from the second layer.",
    "start": "2772220",
    "end": "2779240"
  },
  {
    "text": "And it's going to operate on\nthose intermediate variables a.",
    "start": "2779240",
    "end": "2785829"
  },
  {
    "text": "So we're going to have w2. And here we have b2.",
    "start": "2785829",
    "end": "2791869"
  },
  {
    "text": "And now w2 is going to\nbe of what dimension?",
    "start": "2791870",
    "end": "2798940"
  },
  {
    "text": "Yeah, I heard it somewhere. Yes, because a is\nof dimension 3 here.",
    "start": "2798940",
    "end": "2813980"
  },
  {
    "text": "And the bias still scalar.",
    "start": "2813980",
    "end": "2819510"
  },
  {
    "text": "So this is a two-layer\nneural network.",
    "start": "2819510",
    "end": "2827140"
  },
  {
    "text": "And this is the\nsame thing as saying we have one hidden layer.",
    "start": "2827140",
    "end": "2832750"
  },
  {
    "text": "So intermediate\nvariables are referred to in deep learning\nas hidden units.",
    "start": "2832750",
    "end": "2841720"
  },
  {
    "text": "And the associated\nlayers are hidden layers. Any questions on that?",
    "start": "2841720",
    "end": "2848020"
  },
  {
    "text": "Yeah. Yes. [INAUDIBLE] Great question.",
    "start": "2848020",
    "end": "2855089"
  },
  {
    "text": "Great question. So you're going to be\nlimited by compute.",
    "start": "2855089",
    "end": "2860589"
  },
  {
    "text": "But aside from that, it's\na lot of experimentation. So for example, GPT 3\nhas a lot of layers.",
    "start": "2860590",
    "end": "2868579"
  },
  {
    "text": "But it's also dealing\nwith a lot of data. If you have just a little bit\nof data and just a few examples,",
    "start": "2868580",
    "end": "2875440"
  },
  {
    "text": "you probably don't want\nto use a very big model. And I think you'll talk a\nlittle bit about why shortly.",
    "start": "2875440",
    "end": "2884838"
  },
  {
    "text": "Any other questions? Yeah. What's the difference between\nall the different hidden units if they're all relying\non the same input?",
    "start": "2884839",
    "end": "2892280"
  },
  {
    "text": "So the hope is that\nthe network learns different representations. But technically\nnothing is really",
    "start": "2892280",
    "end": "2897838"
  },
  {
    "text": "stopping it from\nexactly what you said, just replicating ideas. But it often doesn't.",
    "start": "2897839",
    "end": "2905510"
  },
  {
    "text": "And it's trying to learn\nthese a's in the best way that",
    "start": "2905510",
    "end": "2910920"
  },
  {
    "text": "can help the neural\nnetwork make a prediction h theta of x, right?",
    "start": "2910920",
    "end": "2916450"
  },
  {
    "text": "So this ends up working\nactually quite well.",
    "start": "2916450",
    "end": "2923490"
  },
  {
    "text": "Yeah. In the prior example,\nthe [INAUDIBLE] example,",
    "start": "2923490",
    "end": "2932420"
  },
  {
    "text": "you had some [INAUDIBLE].",
    "start": "2932420",
    "end": "2938020"
  },
  {
    "text": "Yeah, so that's actually an\nactive research area as well. It's called interpretability\nin deep learning.",
    "start": "2938020",
    "end": "2945360"
  },
  {
    "text": "And either figuring\nout if there is any or figuring out\nhow we can induce",
    "start": "2945360",
    "end": "2951328"
  },
  {
    "text": "there to be some level\nof prior knowledge, so some interpretability. I think there's a\nquestion at the back.",
    "start": "2951329",
    "end": "2961880"
  },
  {
    "text": "Yeah. So to double check,\nw is the weight",
    "start": "2961880",
    "end": "2969039"
  },
  {
    "text": "you're going to be\napplying [INAUDIBLE].. That you're going to\nbe applying to what?",
    "start": "2969040",
    "end": "2974160"
  },
  {
    "text": "The x. Yes, so w are the weights that\nyou're going to be applying to the x's. So those are w1 with the\nsquare bracket at the top.",
    "start": "2974160",
    "end": "2982180"
  },
  {
    "text": "And then w2 with a square\nbracket at the top is going to be the weights you're\napplying to the a's.",
    "start": "2982180",
    "end": "2987470"
  },
  {
    "text": "OK, thanks. Yep. All right, OK. So this is some notation.",
    "start": "2987470",
    "end": "2995280"
  },
  {
    "text": "Yes. Yes, question? I have a question which\nis more generally, I kind of get the sense that\nif we get rid of the ReLUs",
    "start": "2995280",
    "end": "3006070"
  },
  {
    "text": "everything is just linear. So what's the point of the\nneural network versus just a regular linear regression?",
    "start": "3006070",
    "end": "3012230"
  },
  {
    "text": "Yeah, so that's\na great question. You're completely right. If you get rid of\nthe ReLUs things are just going to be linear.",
    "start": "3012230",
    "end": "3018359"
  },
  {
    "text": "The ReLUs are what make\nthe neural network be more expressive.",
    "start": "3018359",
    "end": "3023730"
  },
  {
    "text": "So those nonlinearities,\nthose activation functions, are really the heart\nof the neural network.",
    "start": "3023730",
    "end": "3030730"
  },
  {
    "text": "I'm going to move on in\nthe interest of time. So we have a two-layer\nneural network.",
    "start": "3030730",
    "end": "3037960"
  },
  {
    "text": "And we just talked\nabout all these things. The only difference\nhere is that we're",
    "start": "3037960",
    "end": "3044680"
  },
  {
    "text": "changing notation a little bit. So we're introducing\nthe z, which is just a linear combination\nof x's with our weights",
    "start": "3044680",
    "end": "3053330"
  },
  {
    "text": "and our biases. And then we still have our a's. The number of a's here is m.",
    "start": "3053330",
    "end": "3059790"
  },
  {
    "text": "So this is the number of hidden\nunits that we're considering.",
    "start": "3059790",
    "end": "3065980"
  },
  {
    "text": "So we had three in\nthe last example. More generally we'll have m. Otherwise, this is exactly\nwhat we just wrote out before.",
    "start": "3065980",
    "end": "3077000"
  },
  {
    "text": "Does this make\nsense to everyone? Any questions on this?",
    "start": "3077000",
    "end": "3082270"
  },
  {
    "text": "OK, so we're going to\ntalk about vectorization. We're going to make this\neven more vectorized.",
    "start": "3082270",
    "end": "3089630"
  },
  {
    "text": "So we had a lot of notation\nand a lot of indices here. We want to get rid of as\nmuch of that as possible.",
    "start": "3089630",
    "end": "3095720"
  },
  {
    "text": "One, to make things cleaner\nand not have to write all these indices everywhere.",
    "start": "3095720",
    "end": "3101790"
  },
  {
    "text": "And two, which is the more\nimportant reason, vectorization actually makes things faster.",
    "start": "3101790",
    "end": "3107880"
  },
  {
    "text": "It makes things better\nable to be parallelized on, for example, GPUs.",
    "start": "3107880",
    "end": "3115300"
  },
  {
    "text": "So what we can do first is\nthink about those weights in our first layer, right, those\nw's with the square bracket 1.",
    "start": "3115300",
    "end": "3125190"
  },
  {
    "text": "And what we can do is we can\ntranspose them so they are rows and stack them in a matrix.",
    "start": "3125190",
    "end": "3131990"
  },
  {
    "text": "And so what we're looking\nat here is m by d. So m is our hidden\nunit dimension.",
    "start": "3131990",
    "end": "3145290"
  },
  {
    "text": "And d is our input dimension. So these are all the\nweights in our first layer.",
    "start": "3145290",
    "end": "3158240"
  },
  {
    "text": "And then what we can\ndo is we can actually write out those equations from\nbefore in this vectorized form,",
    "start": "3158240",
    "end": "3166260"
  },
  {
    "text": "where, for example,\nwe can write a z1. If we take the\nfirst row of this,",
    "start": "3166260",
    "end": "3172520"
  },
  {
    "text": "z1 will be w1 of 1\ntranspose x1 plus b1 1,",
    "start": "3172520",
    "end": "3181460"
  },
  {
    "text": "which is that first\nrow that we had before.",
    "start": "3181460",
    "end": "3186690"
  },
  {
    "text": "But this looks way, way nicer. And this is our\nvectorized notation.",
    "start": "3186690",
    "end": "3193089"
  },
  {
    "text": "Any questions on this? So it's the exact same thing,\njust more compactly written.",
    "start": "3193089",
    "end": "3214789"
  },
  {
    "text": "So now that we have these\nz's, this is actually called pre-activation.",
    "start": "3214790",
    "end": "3221099"
  },
  {
    "text": "So this is before we\napplied our ReLUs.",
    "start": "3221099",
    "end": "3228270"
  },
  {
    "text": "And we have these z's. This is capital W. I'll do\nit with those little bars",
    "start": "3228270",
    "end": "3233470"
  },
  {
    "text": "at the top. And it's all our weights\nfrom our first layer.",
    "start": "3233470",
    "end": "3241589"
  },
  {
    "text": "We take the x. And we add our biases, which\nare also stacked together.",
    "start": "3241590",
    "end": "3247710"
  },
  {
    "text": "And all of this is going\nto be of dimension m, which is the same dimension\nas our hidden units.",
    "start": "3247710",
    "end": "3255500"
  },
  {
    "text": "Now, we want to get\na's out of this. And to do so we need to apply\nthe ReLU to every element",
    "start": "3255500",
    "end": "3262319"
  },
  {
    "text": "in the z vector. So our a's are a1 through am.",
    "start": "3262319",
    "end": "3272130"
  },
  {
    "text": "And we want to apply ReLU\nto each one of these z's.",
    "start": "3272130",
    "end": "3279480"
  },
  {
    "text": "We're actually going to abuse\nnotation here a little bit. And we're just\ngoing to write this",
    "start": "3279480",
    "end": "3284950"
  },
  {
    "text": "by definition to be ReLU of z. So it's an\nelement-wise operation.",
    "start": "3284950",
    "end": "3294270"
  },
  {
    "text": "And we're going to\nrefer to it the same way as we would for a scalar. OK?",
    "start": "3294270",
    "end": "3300290"
  },
  {
    "text": "And then what we can do is\nwe can write our second layer weights.",
    "start": "3300290",
    "end": "3306090"
  },
  {
    "text": "In the second layer we only have\na scalar output that we want. So we only have\none weight vector",
    "start": "3306090",
    "end": "3317600"
  },
  {
    "text": "to include here that we're\ngoing to transpose to be a row.",
    "start": "3317600",
    "end": "3323099"
  },
  {
    "text": "And this is going to\nbe in dimension 1 by m.",
    "start": "3323099",
    "end": "3330580"
  },
  {
    "text": "Any questions on that part? OK.",
    "start": "3330580",
    "end": "3339530"
  },
  {
    "text": "And we're still going to have\nour bias in the second layer, which is still scalar.",
    "start": "3339530",
    "end": "3345970"
  },
  {
    "text": "And so our final\noutput here is going",
    "start": "3345970",
    "end": "3351190"
  },
  {
    "text": "to be this w2 matrix times a\ndot product with a and the bias",
    "start": "3351190",
    "end": "3361058"
  },
  {
    "text": "term. And what I said before is\nthat vectorization helps",
    "start": "3361059",
    "end": "3366338"
  },
  {
    "text": "us paralyze things on GPUs.",
    "start": "3366339",
    "end": "3376980"
  },
  {
    "text": "OK.",
    "start": "3376980",
    "end": "3385640"
  },
  {
    "text": "So we talked about\ntwo-layer neural networks. What if we have more layers?",
    "start": "3385640",
    "end": "3390680"
  },
  {
    "text": "So notation just follows, right? So before we stopped with\none hidden layer, which",
    "start": "3390680",
    "end": "3398300"
  },
  {
    "text": "is a two-layer neural network. And now we're going to have r\nminus 1 hidden layers, which",
    "start": "3398300",
    "end": "3407420"
  },
  {
    "text": "is an r-layered neural network.",
    "start": "3407420",
    "end": "3413470"
  },
  {
    "text": "And like I mentioned\nbefore, all the hidden units will have ReLUs.",
    "start": "3413470",
    "end": "3419270"
  },
  {
    "text": "Whereas the hidden layers,\nwhereas the prediction just by convention will not.",
    "start": "3419270",
    "end": "3429580"
  },
  {
    "text": "And we'll refer to these\nbig W's as weight matrices.",
    "start": "3429580",
    "end": "3440079"
  },
  {
    "text": "And these bias terms\nwill just be called bias. Nothing really changed.",
    "start": "3440079",
    "end": "3447289"
  },
  {
    "text": "And these a's will\nbe hidden units.",
    "start": "3447289",
    "end": "3454410"
  },
  {
    "text": "One thing to note is the\ndimensionality of these a's.",
    "start": "3454410",
    "end": "3459760"
  },
  {
    "text": "So what is the dimensionality\nof ak, so a layer k.",
    "start": "3459760",
    "end": "3471549"
  },
  {
    "text": "We're going to\nrefer to that as mk. And so can anyone tell me\nwhat the dimensionality",
    "start": "3471549",
    "end": "3478740"
  },
  {
    "text": "is of w layer one? [INAUDIBLE]",
    "start": "3478740",
    "end": "3485120"
  },
  {
    "text": "Sorry, say that louder. d cross k?",
    "start": "3485120",
    "end": "3491420"
  },
  {
    "text": "d cross k? What do folks think? I think it's the other one.",
    "start": "3491420",
    "end": "3498260"
  },
  {
    "text": "So it's-- We're going to divide by x--",
    "start": "3498260",
    "end": "3503410"
  },
  {
    "text": "k cross d? Is that looking-- yeah. I think it's mk.",
    "start": "3503410",
    "end": "3509460"
  },
  {
    "text": "Yes. m1. Yes, yes. There we go. So it's m1 cross d.",
    "start": "3509460",
    "end": "3517349"
  },
  {
    "text": "So d is our input. So we want this matrix to\ndo matrix multiplication",
    "start": "3517349",
    "end": "3524480"
  },
  {
    "text": "with x. x is of dimension d. So we want that last\ndimension to be d.",
    "start": "3524480",
    "end": "3529599"
  },
  {
    "text": "The first dimension\nis the dimension we want as the output. The output will be a1.",
    "start": "3529599",
    "end": "3535250"
  },
  {
    "text": "a1 is of dimension m1. Does that make sense?",
    "start": "3535250",
    "end": "3541450"
  },
  {
    "text": "So just for practice,\nwhat would w2 be?",
    "start": "3541450",
    "end": "3547310"
  },
  {
    "text": "I have a question. Yeah. Would this be r layer network\nor r minus 1 layer network?",
    "start": "3547310",
    "end": "3556869"
  },
  {
    "text": "This would be an r layer network\nwith r minus 1 hidden units.",
    "start": "3556869",
    "end": "3564010"
  },
  {
    "text": "What is the\ndimensionality of w2? Can anyone help me?",
    "start": "3564010",
    "end": "3570038"
  },
  {
    "text": "Yeah. m2 cross m1. Yes, m2 cross m1 because\nthe input will be a1.",
    "start": "3570039",
    "end": "3578200"
  },
  {
    "text": "That's what's being\nmultiplied with w2. And the output will be a2.",
    "start": "3578200",
    "end": "3584789"
  },
  {
    "text": "So we want m2 output. OK, cool.",
    "start": "3584789",
    "end": "3590559"
  },
  {
    "text": "And so more generally we\ncan write this for wk. We're going to have our\nmk cross mk minus 1.",
    "start": "3590560",
    "end": "3603280"
  },
  {
    "text": "And bk is just going\nto be mk dimension.",
    "start": "3603280",
    "end": "3612160"
  },
  {
    "text": "Any questions here? No?",
    "start": "3612160",
    "end": "3619579"
  },
  {
    "text": "OK, awesome.",
    "start": "3619579",
    "end": "3626170"
  },
  {
    "text": "So we got this question earlier. Why do we need an\nactivation function ReLU?",
    "start": "3626170",
    "end": "3633280"
  },
  {
    "text": "Can anyone remind me\nwhy do we need it? [INAUDIBLE] Yeah, so it's our nonlinearity.",
    "start": "3633280",
    "end": "3640050"
  },
  {
    "text": "It's what makes what we're\ndoing here non-linear. But for fun, let's\nsee what happens",
    "start": "3640050",
    "end": "3646450"
  },
  {
    "text": "if we don't have the ReLU at all\nor we have it be just identity.",
    "start": "3646450",
    "end": "3652838"
  },
  {
    "text": "So this is a 1, sorry.",
    "start": "3652839",
    "end": "3658359"
  },
  {
    "text": "So we have our hidden layer a1.",
    "start": "3658359",
    "end": "3663569"
  },
  {
    "text": "And then say we only\nhave one hidden layer. So our output here is going\nto be w2 of a plus b2.",
    "start": "3663570",
    "end": "3680140"
  },
  {
    "text": "And then if we actually\nsubstitute in for a1, what",
    "start": "3680140",
    "end": "3687538"
  },
  {
    "text": "are we going to get? So we're going to have\nw2 of w1x plus b--",
    "start": "3687539",
    "end": "3702119"
  },
  {
    "text": "oh, sorry, that was a-- plus b2.",
    "start": "3702119",
    "end": "3711539"
  },
  {
    "text": "And then we can\nactually expand it out. And we're going to get--",
    "start": "3711540",
    "end": "3718309"
  },
  {
    "text": "watch my math so I\ndon't mess this up-- going to get w2w1x\nplus w2b1 plus b2.",
    "start": "3718310",
    "end": "3734370"
  },
  {
    "text": "So what does this\nreally look like? Well, we can define these\nto be-- say this as w tilde.",
    "start": "3734370",
    "end": "3741808"
  },
  {
    "text": "And say this is b tilde, right? The second term doesn't\ndepend on x at all.",
    "start": "3741809",
    "end": "3749619"
  },
  {
    "text": "And the first term does. So this is just\na linear function of x as we would have\nin linear regression.",
    "start": "3749619",
    "end": "3758299"
  },
  {
    "text": "Essentially everything\nwould collapse. And we're here linear\nin these parameters.",
    "start": "3758299",
    "end": "3774609"
  },
  {
    "text": "Any questions about this? So if we lose the ReLU, we\nlose the nonlinear expressivity",
    "start": "3774609",
    "end": "3780289"
  },
  {
    "text": "of the neural network. Yeah.",
    "start": "3780289",
    "end": "3786630"
  },
  {
    "text": "[INAUDIBLE] Yeah, you can definitely\nhave other ones.",
    "start": "3786630",
    "end": "3793619"
  },
  {
    "text": "So in the notes I\nthink we mentioned the sigmoid activation function\nand the tan h activation",
    "start": "3793619",
    "end": "3800210"
  },
  {
    "text": "function. I would probably say\nReLU is the most common. But it really depends\non your application,",
    "start": "3800210",
    "end": "3805470"
  },
  {
    "text": "the kinds of outputs you\nmight want, things like that. There are definitely other ones. And there's also besides\njust the kinds of outputs",
    "start": "3805470",
    "end": "3816670"
  },
  {
    "text": "that you might want,\nthese activation functions have certain\nproperties when you try",
    "start": "3816670",
    "end": "3824369"
  },
  {
    "text": "to compute gradients with\nthem and things like that. And that's a little bit beyond\nthe scope of this lecture.",
    "start": "3824369",
    "end": "3830380"
  },
  {
    "text": "But if you have questions\nabout this later, come by. And I'll be happy to chat.",
    "start": "3830380",
    "end": "3837818"
  },
  {
    "text": "Any other questions on this? Yeah. On that note, on the-- if you look at ReLUs,\nperhaps the closest",
    "start": "3837819",
    "end": "3843940"
  },
  {
    "text": "to what we can call as a\nlinear activation function. Yes.",
    "start": "3843940",
    "end": "3849690"
  },
  {
    "text": "Why is it so commonly\nused despite it being quite close to it.",
    "start": "3849690",
    "end": "3857828"
  },
  {
    "text": "Quite close to it. It works well. Yeah.",
    "start": "3857829",
    "end": "3863140"
  },
  {
    "text": "[INAUDIBLE] in the\nparameters when it's not [INAUDIBLE] Why is\nit not linear [INAUDIBLE]",
    "start": "3863140",
    "end": "3870079"
  },
  {
    "text": "Oh, yeah, good point. Yes, you're right. You're right.",
    "start": "3870079",
    "end": "3875180"
  },
  {
    "text": "I meant it in the sense\nthat the parameters",
    "start": "3875180",
    "end": "3880490"
  },
  {
    "text": "are going to be linear here. Yeah.",
    "start": "3880490",
    "end": "3887140"
  },
  {
    "text": "Other questions?",
    "start": "3887140",
    "end": "3892980"
  },
  {
    "text": "One last new-ish idea\nI want to talk about.",
    "start": "3892980",
    "end": "3901390"
  },
  {
    "text": "And that is connection between\nneural networks and kernel methods. So kernel methods, we talked\nabout this very briefly",
    "start": "3901390",
    "end": "3910089"
  },
  {
    "text": "at the beginning\nof this lecture. So the kernel method\noutput or prediction",
    "start": "3910089",
    "end": "3916359"
  },
  {
    "text": "is going to look like h theta\nof x beta transpose phi of x.",
    "start": "3916359",
    "end": "3924440"
  },
  {
    "text": "So we're linear in parameters\nhere, but not in x.",
    "start": "3924440",
    "end": "3934859"
  },
  {
    "text": "Yeah? Everyone agrees?",
    "start": "3934859",
    "end": "3945130"
  },
  {
    "text": "So looking at that penultimate\nlayer ar minus 1, what if we",
    "start": "3945130",
    "end": "3951210"
  },
  {
    "text": "write it as phi b of x, where--",
    "start": "3951210",
    "end": "3956818"
  },
  {
    "text": "phi beta-- where our beta is\ngoing to be all our parameters. So our parameter is w1 all the\nway to the penultimate layer",
    "start": "3956819",
    "end": "3968040"
  },
  {
    "text": "wr minus 1. And also, our biases, which are",
    "start": "3968040",
    "end": "3980538"
  },
  {
    "text": "And then we can\nwrite the prediction from our neural\nnetwork as w of r,",
    "start": "3980539",
    "end": "3989890"
  },
  {
    "text": "So our last layer, matrix\nmultiplied with phi beta",
    "start": "3989890",
    "end": "3996190"
  },
  {
    "text": "of x, if we fix our parameters.",
    "start": "3996190",
    "end": "4001410"
  },
  {
    "text": "So we're fixing beta here. And we can add our bias term.",
    "start": "4001410",
    "end": "4009440"
  },
  {
    "text": "So really this looks pretty\nmuch the same, right? The only difference is\nwithin the neural network,",
    "start": "4009440",
    "end": "4019480"
  },
  {
    "text": "phi beta is actually learned.",
    "start": "4019480",
    "end": "4024730"
  },
  {
    "text": "So the algorithm is looking\nfor the best possible features",
    "start": "4024730",
    "end": "4032000"
  },
  {
    "text": "for this data. Whereas in the kernel\nmethods we choose the kernel.",
    "start": "4032000",
    "end": "4037710"
  },
  {
    "text": "So there's more of that prior\nknowledge and prior structure that we're infusing\ninto the algorithm.",
    "start": "4037710",
    "end": "4042920"
  },
  {
    "text": "Whereas here,\nthere's flexibility in that these phi beta\nparameters can actually be",
    "start": "4042920",
    "end": "4048130"
  },
  {
    "text": "learned to best fit the data.",
    "start": "4048130",
    "end": "4063500"
  },
  {
    "text": "And because of this sort\nof structural similarity, the penultimate layer\noutput ar minus 1",
    "start": "4063500",
    "end": "4072380"
  },
  {
    "text": "is sometimes called the\nfeatures or the representation",
    "start": "4072380",
    "end": "4082778"
  },
  {
    "text": "within a neural network. And so if you ever hear terms\nlike representation learning",
    "start": "4082779",
    "end": "4090328"
  },
  {
    "text": "or something like\nthat, it's talking about those hidden layers\nwithin the neural network",
    "start": "4090329",
    "end": "4095359"
  },
  {
    "text": "and what we're learning there. Any questions here?",
    "start": "4095359",
    "end": "4101960"
  },
  {
    "text": "Yeah, at the back. Just to check,\nthat line where you",
    "start": "4101960",
    "end": "4109730"
  },
  {
    "text": "say h theta of x\nequals wr phi beta, that's all supposed\nto be one line, right?",
    "start": "4109730",
    "end": "4118940"
  },
  {
    "text": "Yes. So phi, whatever that\nGreek letter is called,",
    "start": "4118940",
    "end": "4124658"
  },
  {
    "text": "is the only subscript? So it's wr matrix\nmultiplied with phi",
    "start": "4124659",
    "end": "4132798"
  },
  {
    "text": "underscore beta of x plus all\nof that plus the bias term br.",
    "start": "4132799",
    "end": "4139270"
  },
  {
    "text": "Is the plus the bias term\n[INAUDIBLE] part of the--",
    "start": "4139270",
    "end": "4145568"
  },
  {
    "text": "it's not a subscript at all? No. OK, thank you.",
    "start": "4145569",
    "end": "4154230"
  },
  {
    "text": "Yeah, sorry. Other questions?",
    "start": "4154230",
    "end": "4160338"
  },
  {
    "text": "No? All right, so today we talked\nabout two different things.",
    "start": "4160339",
    "end": "4168179"
  },
  {
    "text": "We talked about supervised\nlearning with nonlinear models and what that might\nlook like, what",
    "start": "4168179",
    "end": "4173630"
  },
  {
    "text": "the cost function looks like. And the second thing we talked\nabout are neural networks.",
    "start": "4173630",
    "end": "4179278"
  },
  {
    "text": "How do we construct them? We first started with\ntwo-layer neural networks and then expanded that notation\nto be r-layer neural networks.",
    "start": "4179279",
    "end": "4187718"
  },
  {
    "text": "And next time, Tengyu is going\nto talk about back propagation. So how do we actually optimize?",
    "start": "4187719",
    "end": "4194020"
  },
  {
    "text": "How do we perform\nstochastic gradient descent or any kind of gradient\ndescent in this framework?",
    "start": "4194020",
    "end": "4199030"
  },
  {
    "text": "So how do we compute the\ngradients for the cost function",
    "start": "4199030",
    "end": "4204079"
  },
  {
    "text": "and for the neural network? Any last questions? OK.",
    "start": "4204080",
    "end": "4210160"
  }
]