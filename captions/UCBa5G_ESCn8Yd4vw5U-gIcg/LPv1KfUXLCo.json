[
  {
    "text": "so we'll get started today we're going to cover a mixture of experts last year this was kind of a fun bonus lecture",
    "start": "5040",
    "end": "11360"
  },
  {
    "text": "that i threw together um but this year thanks to you know lots of people doing",
    "start": "11360",
    "end": "16400"
  },
  {
    "text": "this has become a much more critical lecture so i've added a lot of um the recent developments and at the end we'll",
    "start": "16400",
    "end": "21840"
  },
  {
    "text": "try to walk through um deepseek v3 and try to understand like what are all the sort of components that make up a",
    "start": "21840",
    "end": "28720"
  },
  {
    "text": "state-of-the-art open source system or at least on the architecture side what that looks like so mixture of experts is how a lot",
    "start": "28720",
    "end": "37280"
  },
  {
    "text": "of you know the most modern high performance systems today are are built and deployed um so there was the funny",
    "start": "37280",
    "end": "45200"
  },
  {
    "text": "nvidia leak of uh gpt4 actually being potentially revealed as",
    "start": "45200",
    "end": "51000"
  },
  {
    "text": "gptoe1 bt um but more you know broadly others like grock um and deepseek and",
    "start": "51000",
    "end": "59280"
  },
  {
    "text": "llama 4 now um have all adopted a mixture of experts uh architecture and",
    "start": "59280",
    "end": "64478"
  },
  {
    "text": "it seems like at this point in 2025 that the advantage of mixtures of experts",
    "start": "64479",
    "end": "69600"
  },
  {
    "text": "over dense architectures is very much clear right almost um all compute scales",
    "start": "69600",
    "end": "75600"
  },
  {
    "text": "training a mixture of experts model if you do it well um is going to give you benefits over a dense model and so",
    "start": "75600",
    "end": "82000"
  },
  {
    "text": "everyone seems seems to be doing it in both the east and the west and so this will be an important um thing to",
    "start": "82000",
    "end": "87119"
  },
  {
    "text": "understand if you're trying to build sort of the the best model that you can um for the flops that you",
    "start": "87119",
    "end": "94439"
  },
  {
    "text": "have so mixture of experts is very simple um it's a very terribly named",
    "start": "94439",
    "end": "100320"
  },
  {
    "text": "concept i think you hear mixture of experts and you think oh there must be experts specialized for different domains and they're like doing different",
    "start": "100320",
    "end": "107040"
  },
  {
    "text": "things like there's a coding expert and like an english expert and a other languages expert um it is very far from",
    "start": "107040",
    "end": "114399"
  },
  {
    "text": "that mental model a mixture of experts is a type of fancy architecture that has",
    "start": "114399",
    "end": "119600"
  },
  {
    "text": "several subcomponents called experts that are activated sparsely um and in particular when you think about mixture",
    "start": "119600",
    "end": "126000"
  },
  {
    "text": "of experts you should be thinking about the mlps this is where all the action is right so ae architecture and a nonoe",
    "start": "126000",
    "end": "133280"
  },
  {
    "text": "architecture are going to be similar in almost all of its components um except for one and that is you know if you look",
    "start": "133280",
    "end": "140720"
  },
  {
    "text": "at this this slide over here you know this is the components um of a standard transformer you got your self attention",
    "start": "140720",
    "end": "147280"
  },
  {
    "text": "you got your ffn if you zoom in you know in a dense model the the feed forward component just sort of you know is there",
    "start": "147280",
    "end": "153599"
  },
  {
    "text": "it's one big block in a sparse model what you would do is that you would take this ffn and you would split it up or",
    "start": "153599",
    "end": "159840"
  },
  {
    "text": "you would copy it depending on how you're going to be setting up your multiple copies let's say of your ffn",
    "start": "159840",
    "end": "166000"
  },
  {
    "text": "your fully connected networks and you're going to have a router that picks some smaller number of those you know in each",
    "start": "166000",
    "end": "172000"
  },
  {
    "text": "forward pass or at each inference right so this is the basic idea behind and we're going to replace this one big feed",
    "start": "172000",
    "end": "178000"
  },
  {
    "text": "forward on the left side with a selector layer and many smaller ones um and what's the advantage of this thing well",
    "start": "178000",
    "end": "184640"
  },
  {
    "text": "if it's sparssely activated that is let's say only picks one expert and an expert is the same size as your dense",
    "start": "184640",
    "end": "190879"
  },
  {
    "text": "ffn then the flops between the left side and the right side the dense model and the model they have the same flops right",
    "start": "190879",
    "end": "197920"
  },
  {
    "text": "they're doing the same matrix multiplies as you do your forward pass um so you",
    "start": "197920",
    "end": "203040"
  },
  {
    "text": "have more parameters without affecting your flops and if you're a believer that what matters is having more parameters",
    "start": "203040",
    "end": "208879"
  },
  {
    "text": "to for example memorize facts about the world well you know this is a great architecture so you can kind of see the",
    "start": "208879",
    "end": "215040"
  },
  {
    "text": "the intuition behind um hopefully that's all very clear um and you might wonder",
    "start": "215040",
    "end": "221360"
  },
  {
    "text": "okay so it makes sense that you can get you know more parameters per flops but does that translate to actually better",
    "start": "221360",
    "end": "227760"
  },
  {
    "text": "performance for the models that you're training um and there's been i think at this point many many many papers showing",
    "start": "227760",
    "end": "234400"
  },
  {
    "text": "that at the same flop count at the same training amount of flops you get better performance out of a mixture of experts",
    "start": "234400",
    "end": "241360"
  },
  {
    "text": "um than out of a dense model so um this is a nice paper to so today i'm going to",
    "start": "241360",
    "end": "246560"
  },
  {
    "text": "go over a couple of the the classic google papers that you know put this field together um and this is one of",
    "start": "246560",
    "end": "252560"
  },
  {
    "text": "them by fetis at all 2022 um where they show that you know if you flops match",
    "start": "252560",
    "end": "257759"
  },
  {
    "text": "your your you know training flops so that's the same amount of compute used for training and as you increase the",
    "start": "257759",
    "end": "262880"
  },
  {
    "text": "number of experts the training loss of your language model just keeps going down and down and down and down and down right so you know more experts better of",
    "start": "262880",
    "end": "270160"
  },
  {
    "text": "course the experts aren't free you need to store the memory for these experts and when you do parallelism you're going",
    "start": "270160",
    "end": "275199"
  },
  {
    "text": "to have to think about you know routing your data into 256 separate experts so",
    "start": "275199",
    "end": "280320"
  },
  {
    "text": "that's there's going to be systems complexities but if you're only thinking about flops this is a great chart to see",
    "start": "280320",
    "end": "286400"
  },
  {
    "text": "because you have the same flops but you've gotten free you know test loss um here and you see the same thing",
    "start": "286400",
    "end": "292240"
  },
  {
    "text": "reflected on the right side you know as you train for longer and longer the model the switch base with 128 experts",
    "start": "292240",
    "end": "298320"
  },
  {
    "text": "right the model with more experts you know gets better perplexity um faster",
    "start": "298320",
    "end": "303440"
  },
  {
    "text": "right so hopefully that that is quite clear um you might say well this is a 2022 paper um is this true sort of on",
    "start": "303440",
    "end": "310000"
  },
  {
    "text": "modern architectures on modern scales um it you know continues to very much be true um ai2 had a very nice paper um olo",
    "start": "310000",
    "end": "319440"
  },
  {
    "text": "which did a whole bunch of ablations and carefully controlled comparisons into dense versus um and other architectures",
    "start": "319440",
    "end": "326880"
  },
  {
    "text": "and they sort of see exactly the same thing so here on the left side this is still from fetus at all you see the 7x",
    "start": "326880",
    "end": "332000"
  },
  {
    "text": "speed up from having many experts on the right side this is the omo comparison you see um the the pink one is thee and",
    "start": "332000",
    "end": "339360"
  },
  {
    "text": "the the teal one is dense and the training loss for the dense model goes down much more slowly than thee right so",
    "start": "339360",
    "end": "346400"
  },
  {
    "text": "hopefully you know i have in some sense sold you on the value of and for for learning this kind of new slightly new",
    "start": "346400",
    "end": "353199"
  },
  {
    "text": "architecture right so we're going to pay a a price for all of this but at least at the flops level this looks very",
    "start": "353199",
    "end": "358720"
  },
  {
    "text": "compelling right so yes question the last lecture you mentioned uh like yeah",
    "start": "358720",
    "end": "365680"
  },
  {
    "text": "the bias turning part because although the it's a pretty cheap",
    "start": "365680",
    "end": "371960"
  },
  {
    "text": "computation affects our actual process pretty badly we're you know loading in",
    "start": "371960",
    "end": "377680"
  },
  {
    "text": "and out is there right so the question was in last",
    "start": "377680",
    "end": "383840"
  },
  {
    "text": "lecture you know i was saying um even small non- flops you know negligible flops can be really big in wall clock is",
    "start": "383840",
    "end": "391360"
  },
  {
    "text": "anything in thee world going to look like that and so i think one of the drawbacks of why you know that's not the",
    "start": "391360",
    "end": "397120"
  },
  {
    "text": "standard thing that's being taught you know let's say at 224n is because there's significant systems complexities",
    "start": "397120",
    "end": "402560"
  },
  {
    "text": "to making this thing efficient so i'll you know get to that it's possible to make these things very efficient",
    "start": "402560",
    "end": "407600"
  },
  {
    "text": "especially if each expert lives on a separate device so that you know you're routing thing data to different places",
    "start": "407600",
    "end": "413520"
  },
  {
    "text": "you can be very efficient when you do that but it's not easy right so there's a lot of infrastructural concerns s and",
    "start": "413520",
    "end": "419120"
  },
  {
    "text": "you're going to see a lot of complexities to get this thing to work but when it does work you know you're you're putting all of your flops to",
    "start": "419120",
    "end": "426638"
  },
  {
    "text": "use okay and then uh the last one uh that i wanted to show is you know a lot",
    "start": "427560",
    "end": "433199"
  },
  {
    "text": "of the companies really love because you get to present plots that look very compelling like this right this was from",
    "start": "433199",
    "end": "439120"
  },
  {
    "text": "the deepseek v2 paper um you know on the x-axis this is this is a little bit of slight of hand this is only activated",
    "start": "439120",
    "end": "445360"
  },
  {
    "text": "parameters right so this is only the parameters that are used you know for computation so you ignore all the",
    "start": "445360",
    "end": "450720"
  },
  {
    "text": "deactivated experts and the y- axis is mmlu performance right and we see deepseek v2 wow look very few activated",
    "start": "450720",
    "end": "458319"
  },
  {
    "text": "parameters really good mmlu performance right and so if you're only interested in both training and inference flops you",
    "start": "458319",
    "end": "464800"
  },
  {
    "text": "know activated parameters is the name of the game you get really good performance here and this is you know not just an ablation this is a real system that",
    "start": "464800",
    "end": "471520"
  },
  {
    "text": "someone you know spent a lot of money to train um and deployed out in the wild right and we'll see this sort of pattern",
    "start": "471520",
    "end": "476720"
  },
  {
    "text": "uh recur in other uh examples as well oh was there a question oh no all right and",
    "start": "476720",
    "end": "484319"
  },
  {
    "text": "so the systems thing that is also um a benefit is that allow us to have another",
    "start": "484319",
    "end": "491039"
  },
  {
    "text": "axis of parallelism so i'm going to get into parallelism in much much more detail sort of in the systems lectures",
    "start": "491039",
    "end": "496800"
  },
  {
    "text": "when i'm going to talk about how you're going to take your model and you're going to cut it up into many small pieces and lay them out across many",
    "start": "496800",
    "end": "502960"
  },
  {
    "text": "different devices but i'm going to talk at a very high level but when you have experts there's a very natural way to",
    "start": "502960",
    "end": "510400"
  },
  {
    "text": "parallelize at the expert level right so you have multiple different feed forward blocks you can take each of these",
    "start": "510400",
    "end": "516800"
  },
  {
    "text": "experts and you can put them on a different device right and because experts are sparsely activated all you",
    "start": "516800",
    "end": "522719"
  },
  {
    "text": "have to do is take your token and route it to the appropriate device and the computation will happen on that device right so it's a natural sort of cutting",
    "start": "522719",
    "end": "529279"
  },
  {
    "text": "point to be able to shard your model into different devices and so this is called expert parallelism um and this is",
    "start": "529279",
    "end": "535120"
  },
  {
    "text": "another reason why are very popular right if you really want to paralyze really big models this is a thing that",
    "start": "535120",
    "end": "541519"
  },
  {
    "text": "you're going to have to do and kind of interestingly enough i think you know",
    "start": "541519",
    "end": "547200"
  },
  {
    "text": "developed at google um and many of the the frontier labs the closed labs were",
    "start": "547200",
    "end": "552399"
  },
  {
    "text": "doing it but i think the open results actually came from china um very frequently um quen and deepseek were",
    "start": "552399",
    "end": "559600"
  },
  {
    "text": "doing you know a lot of work um last year and it's only really recently that i think western open source groups have",
    "start": "559600",
    "end": "566240"
  },
  {
    "text": "started to do more uh work so mixstrol grock um i guess grock's not open um and",
    "start": "566240",
    "end": "572080"
  },
  {
    "text": "then now llama is now ane architecture right and so here you know llama 4 just got released right latest and greatest",
    "start": "572080",
    "end": "579360"
  },
  {
    "text": "um this is also a sparsee and i'll talk about llama 4 as well um as i go through the",
    "start": "579360",
    "end": "585240"
  },
  {
    "text": "lecture um as i said before so you know one of the kind of starting points for",
    "start": "585240",
    "end": "590959"
  },
  {
    "text": "this is um some of the chinese groups quen and deepseek have actually done some really nice work benchmarking and",
    "start": "590959",
    "end": "598080"
  },
  {
    "text": "understanding and evaluating um some of these results so these so quen 1.5 was",
    "start": "598080",
    "end": "605040"
  },
  {
    "text": "one of the first uh models that i knew of to have like this largecale well-",
    "start": "605040",
    "end": "610640"
  },
  {
    "text": "tested well doumented um and what they did was they took a quen 1.5 dense model",
    "start": "610640",
    "end": "616800"
  },
  {
    "text": "and they had a nice trick to upcycle it into a mixture of experts that's a",
    "start": "616800",
    "end": "622160"
  },
  {
    "text": "clever kind of trick to take a dense model and then turn it into ane and they showed sort of significant gains at",
    "start": "622160",
    "end": "627360"
  },
  {
    "text": "least in terms of compute efficiency um while sort of m decreasing the total number of parameters relative to their",
    "start": "627360",
    "end": "633680"
  },
  {
    "text": "sort of 7b uh model deepseek which is now famous um but originally when these",
    "start": "633680",
    "end": "640000"
  },
  {
    "text": "papers were coming out were were not quite as famous um did some of the i think really foundational work in the",
    "start": "640000",
    "end": "647360"
  },
  {
    "text": "open-source world um a big part of this lecture is actually going to be tracing the trajectory of the mo uh deepseek moe",
    "start": "647360",
    "end": "654160"
  },
  {
    "text": "architecture but if you look at their original deepseek paper you'll see very nice papers sorry very nice sort of",
    "start": "654160",
    "end": "661440"
  },
  {
    "text": "comparison showing things like what happens when you train a dense model with a particular amount of flops what happens when you train a a really naivee",
    "start": "661440",
    "end": "668399"
  },
  {
    "text": "that doesn't do very smart routing what happens and then if you use a smarter routing called the switch sort of uh",
    "start": "668399",
    "end": "674320"
  },
  {
    "text": "what happens and so you'll see all of these very carefully controlled comparisons and you see you know as you go from dense to sparse right so that's",
    "start": "674320",
    "end": "680800"
  },
  {
    "text": "the leftmost column to the rightmost column you see all of these sort of benchmark metrics very consistently improve for a fixed amount of flops",
    "start": "680800",
    "end": "688320"
  },
  {
    "text": "right so this is um very consistent and kind of um one thing that i think almost",
    "start": "688320",
    "end": "694399"
  },
  {
    "text": "everyone at this point has probably heard of right is deepseek v3 and that's in some sense you know a culmination of",
    "start": "694399",
    "end": "700640"
  },
  {
    "text": "all this line of work but if you had been following and you were excited about kind of this branch of of um",
    "start": "700640",
    "end": "706640"
  },
  {
    "text": "neural networks and and language modeling you would have actually known about you know deepseek long before v3",
    "start": "706640",
    "end": "712079"
  },
  {
    "text": "got popular um and we'll see at the very end of this lecture actually deepseek v3 is not very different from the very",
    "start": "712079",
    "end": "718959"
  },
  {
    "text": "earliest deepseces architecturally you know they had kind of nailed it way back when they were training these sort of",
    "start": "718959",
    "end": "725360"
  },
  {
    "text": "much smaller two billion parameter models they really just kind of got the engineering right to get something that",
    "start": "725360",
    "end": "731040"
  },
  {
    "text": "is actually really quite remarkably good uh which is their v3 model",
    "start": "731040",
    "end": "737800"
  },
  {
    "text": "okay so now i think you know i have spent quite a few minutes trying to really hype you up ones and they really",
    "start": "737800",
    "end": "744560"
  },
  {
    "text": "are i think worth hyping up they're very good um but i think there's a question of why haven't they been more popular",
    "start": "744560",
    "end": "750959"
  },
  {
    "text": "right why isn't it the standard thing we teach in you know nlp and and language modeling classes um it's just that",
    "start": "750959",
    "end": "757519"
  },
  {
    "text": "they're very complex and they're very messy and i'm hoping that they'll get simplified over the next few years but",
    "start": "757519",
    "end": "762880"
  },
  {
    "text": "they still remain pretty nasty um so one of the things is you know the",
    "start": "762880",
    "end": "768000"
  },
  {
    "text": "infrastructure is very complex and the biggest advantages of really happen when",
    "start": "768000",
    "end": "773040"
  },
  {
    "text": "you're doing multi-node training like when you have to split up your models anyway then it starts to make sense to",
    "start": "773040",
    "end": "778240"
  },
  {
    "text": "shard experts across different models that's a very natural thing to do but until you get to that point maybe are",
    "start": "778240",
    "end": "784800"
  },
  {
    "text": "not quite as good right so um some of the earlier google papers really talk about this trade-off where they say",
    "start": "784800",
    "end": "790480"
  },
  {
    "text": "actually when you get these really big models that you have to split up then exports become uniquely good um there's",
    "start": "790480",
    "end": "796800"
  },
  {
    "text": "also other things that are are really tricky um if you think about it carefully right this decision of which",
    "start": "796800",
    "end": "802800"
  },
  {
    "text": "expert you route tokens to is a very difficult thing to learn right in deep learning we really like differentiable",
    "start": "802800",
    "end": "807920"
  },
  {
    "text": "objectives right very smooth things that we can take gradients of routing decisions are not differentiable because we have to pick and commit to a",
    "start": "807920",
    "end": "814560"
  },
  {
    "text": "particular expert so if we're doing that you know we're going to have a very tricky optimization problem and the",
    "start": "814560",
    "end": "819680"
  },
  {
    "text": "training objectives to make that work is either heristic and or unstable right",
    "start": "819680",
    "end": "825680"
  },
  {
    "text": "and so we're going to have to really carefully engineer those guys to get them to work right so those are two",
    "start": "825680",
    "end": "830959"
  },
  {
    "text": "reasons why you don't really um want to to maybe do this normally um so what do look like as i",
    "start": "830959",
    "end": "839279"
  },
  {
    "text": "started this lecture with you know the classices that you should think of is you take you know the densely connected",
    "start": "839279",
    "end": "846160"
  },
  {
    "text": "layers the the ffns and you split them up or you you know copy them um and you have sparse routing decisions among them",
    "start": "846160",
    "end": "853120"
  },
  {
    "text": "of course you could do the same kind of idea you could have a sparsely routed attention layer and some people have",
    "start": "853120",
    "end": "858720"
  },
  {
    "text": "done this there's been a couple papers and a couple releases that have taken this approach um but it is actually",
    "start": "858720",
    "end": "864720"
  },
  {
    "text": "quite rare uh to see this in the major model releases um i think i've seen",
    "start": "864720",
    "end": "869920"
  },
  {
    "text": "people talking on the internet saying like this approach is actually really much even more unstable and very",
    "start": "869920",
    "end": "875279"
  },
  {
    "text": "difficult to really train consistently um it's sort of i haven't really seen the ablations to to back that out but",
    "start": "875279",
    "end": "881760"
  },
  {
    "text": "certainly there haven't really been many people training those kinds of models with uh",
    "start": "881760",
    "end": "887480"
  },
  {
    "text": "attentions so now you know i've told you about the basic architecture right it's",
    "start": "887480",
    "end": "892560"
  },
  {
    "text": "really simple it's just you have a router of some kind and you route and then you have different mlps so what are",
    "start": "892560",
    "end": "898959"
  },
  {
    "text": "the the things that might vary across different choices um you might ask how",
    "start": "898959",
    "end": "904320"
  },
  {
    "text": "do we route right the routing function is an obviously important choice how many experts and how big should the",
    "start": "904320",
    "end": "910240"
  },
  {
    "text": "experts be that's another choice and then the final one is how would we train this router right this non-ifferiable",
    "start": "910240",
    "end": "916079"
  },
  {
    "text": "objective that seems very difficult to train so those are very important design questions and we're going to go through",
    "start": "916079",
    "end": "921120"
  },
  {
    "text": "each one um hopefully covering the design space um of all these things okay",
    "start": "921120",
    "end": "926800"
  },
  {
    "text": "any questions before i get into to each one of these different subcomponents here good okay",
    "start": "926800",
    "end": "935959"
  },
  {
    "text": "so if you're interested in in just kind of understanding a broad overview of at least circa 2022 um there's a really",
    "start": "937120",
    "end": "944320"
  },
  {
    "text": "nice sort of survey or a review paper by fetus at all in 2022 that covers a lot",
    "start": "944320",
    "end": "949680"
  },
  {
    "text": "of these and and many of my figures you know are credited to that paper if we're thinking about how we're going to route",
    "start": "949680",
    "end": "955839"
  },
  {
    "text": "or essentially match tokens to experts right this is the core component of",
    "start": "955839",
    "end": "962240"
  },
  {
    "text": "because whate does is you know tokens are going to be coming in right you have your sequence that you're processing and",
    "start": "962240",
    "end": "968079"
  },
  {
    "text": "those sequences are going to be assigned to experts right not all experts will process every token that's the whole point of a sparsely routed and so you",
    "start": "968079",
    "end": "976160"
  },
  {
    "text": "can ask how are these routing decisions made so you can sort of have three different kinds of choices you can have",
    "start": "976160",
    "end": "983120"
  },
  {
    "text": "token choice where each token is going to have a sort of routing sort of preference for different experts and i",
    "start": "983120",
    "end": "989040"
  },
  {
    "text": "will choose the top k experts for each token or i can have expert choice where",
    "start": "989040",
    "end": "994959"
  },
  {
    "text": "each expert is going to sort of have a rank preference over tokens and then i'm going to choose the top k tokens for",
    "start": "994959",
    "end": "1000399"
  },
  {
    "text": "each expert this has a really nice benefit of being balanced over experts um and then the last one is sort of you",
    "start": "1000399",
    "end": "1006800"
  },
  {
    "text": "could solve some sort of complicated optimization problem to make sure that the mapping between experts and tokens",
    "start": "1006800",
    "end": "1012399"
  },
  {
    "text": "is somehow balanced right this is global assignment um and just to to you know give you a",
    "start": "1012399",
    "end": "1020079"
  },
  {
    "text": "bit of a a teaser here almost all the do token choice top k um in the early days",
    "start": "1020079",
    "end": "1027400"
  },
  {
    "text": "of people tried many many different things sort of spanning this whole spectrum of design space of token",
    "start": "1027400",
    "end": "1034319"
  },
  {
    "text": "routers um if you look at the big releases they have all converged to basically one class of routing",
    "start": "1034319",
    "end": "1040798"
  },
  {
    "text": "mechanisms which is token choice top k so each token is going to rank order um experts by affinity and then there's",
    "start": "1040799",
    "end": "1047199"
  },
  {
    "text": "going to be kind of a top k choice for each one of this um and mo which i'll keep referring to uh throughout this",
    "start": "1047199",
    "end": "1053039"
  },
  {
    "text": "lecture because they have a really nice series of ablations so it's really nice to teach off of um have exactly this",
    "start": "1053039",
    "end": "1058240"
  },
  {
    "text": "oblation they compare a token choice routing versus an expert choice routing and they show if you look at validation",
    "start": "1058240",
    "end": "1063760"
  },
  {
    "text": "loss token choice is much much nicer behaved um much faster um in loss decay",
    "start": "1063760",
    "end": "1069200"
  },
  {
    "text": "yes is this function a function of the token itself for its position uh it's a",
    "start": "1069200",
    "end": "1074720"
  },
  {
    "text": "function of the sort of the hidden state right so the token is going to get processed with all the position",
    "start": "1074720",
    "end": "1080080"
  },
  {
    "text": "embeddings and so on and then the hidden state will come in and then it will be processed by the mlp and so for the",
    "start": "1080080",
    "end": "1085840"
  },
  {
    "text": "other um for the other two like for the experts choosing the token and also like the next one when you say it's like more",
    "start": "1085840",
    "end": "1092480"
  },
  {
    "text": "balanced across the experts are they like it's still for the current",
    "start": "1092480",
    "end": "1098960"
  },
  {
    "text": "like token sequence but it's like it's forcing them to be more distributed like",
    "start": "1098960",
    "end": "1104960"
  },
  {
    "text": "it's still going to be the same set of tokens but really it's about kind of the ranking selector function right in token",
    "start": "1104960",
    "end": "1110240"
  },
  {
    "text": "choice i'm just going to take the top k amongst the columns like maybe the fours are even identical right i'm just going",
    "start": "1110240",
    "end": "1116080"
  },
  {
    "text": "to take the top k amongst the columns in expert choice i'm going to take top k amongst the rows right um and top k",
    "start": "1116080",
    "end": "1123120"
  },
  {
    "text": "amongst the columns is kind of nice because you might be able to say oh i can define a scoring function such that",
    "start": "1123120",
    "end": "1128960"
  },
  {
    "text": "the score is how well each token gets processed by each expert and token choice will will route me to the best expert right for that token so that",
    "start": "1128960",
    "end": "1135440"
  },
  {
    "text": "makes sense from processing but expert choice has the benefit that each expert gets exactly the same number of tokens",
    "start": "1135440",
    "end": "1141200"
  },
  {
    "text": "and so now you might like if you're putting different experts on different devices you've got balanced utilization",
    "start": "1141200",
    "end": "1146240"
  },
  {
    "text": "so there's different trade-offs at play as you think about routing yes um how does a token know which expert is the",
    "start": "1146240",
    "end": "1153600"
  },
  {
    "text": "best good yes so the question was how how does each token know which expert is good that is exactly the role of the",
    "start": "1153600",
    "end": "1159679"
  },
  {
    "text": "router and i'll give you the router equation but to give you a bit of a not really a spoiler but you know the",
    "start": "1159679",
    "end": "1165679"
  },
  {
    "text": "routers are much more lightweight than you think so you know your token let's say is represented by vector x that's",
    "start": "1165679",
    "end": "1170880"
  },
  {
    "text": "your like hidden you know residual stream coming in so now x is going to get multiplied by you know w a matrix",
    "start": "1170880",
    "end": "1177039"
  },
  {
    "text": "and then you'll just take you know a sigmoid or something and that's the score so it's really just a a vector",
    "start": "1177039",
    "end": "1182880"
  },
  {
    "text": "vector inner product almost like an attention operation in a way yes",
    "start": "1182880",
    "end": "1189960"
  },
  {
    "text": "right so the choice of so the question was is k1 here um so k is actually a hyperparameter and different will choose",
    "start": "1192000",
    "end": "1199120"
  },
  {
    "text": "different things um i will talk about this again but to give you the high level intuition the initial argument",
    "start": "1199120",
    "end": "1205120"
  },
  {
    "text": "that the the earliest moe papers made was that k should be greater than two because that way you get some",
    "start": "1205120",
    "end": "1210640"
  },
  {
    "text": "exploration right if you're doing k equals 1 maybe you're just always exploiting the best arm and you'll never",
    "start": "1210640",
    "end": "1215919"
  },
  {
    "text": "know about the potential other things you could do but if k is two then maybe that second arm can tell you a little bit of exploration information so um you",
    "start": "1215919",
    "end": "1223679"
  },
  {
    "text": "know k equals 2 was the canonical choice um and k equals 2 actually continues to be very popular that would be like",
    "start": "1223679",
    "end": "1229840"
  },
  {
    "text": "double the flops like that's right that's right so so that would double the flops and so when people talk about they",
    "start": "1229840",
    "end": "1236159"
  },
  {
    "text": "usually say things like x number of activated parameters and that would account for the fact that you're you know put putting in two mlps yes so when",
    "start": "1236159",
    "end": "1244000"
  },
  {
    "text": "k is um greater than one like even time do we combine the outputs of the",
    "start": "1244000",
    "end": "1250000"
  },
  {
    "text": "different experts into yes the question was when k is one do the outputs get combined that's right like if you if you",
    "start": "1250000",
    "end": "1256799"
  },
  {
    "text": "look at i guess like look at the attention diagram over there you know um you got the router it's routed to two",
    "start": "1256799",
    "end": "1262000"
  },
  {
    "text": "mlps up top and then they get combined together right after right so that's exactly right",
    "start": "1262000",
    "end": "1268559"
  },
  {
    "text": "so in that case you can just like a simple average so the question was how does the the",
    "start": "1268559",
    "end": "1274480"
  },
  {
    "text": "aggregation happen it's just the sum right so um i'm going to go over the",
    "start": "1274480",
    "end": "1280480"
  },
  {
    "text": "variance very common variance that people do um and really in some ways all",
    "start": "1280480",
    "end": "1286240"
  },
  {
    "text": "you need to know is top k in order to actually implement a uh high performance but i'll give you the other variants",
    "start": "1286240",
    "end": "1292480"
  },
  {
    "text": "because they're natural things you might think of um top k routing is what is used in most token choice top routing uh",
    "start": "1292480",
    "end": "1298880"
  },
  {
    "text": "topk routing so how that works is you know you have your residual stream inputs x um that will go into a router",
    "start": "1298880",
    "end": "1304720"
  },
  {
    "text": "and as i said a router is really kind of like the attention operation there's like a linear inner product and then a softmax and then you pick the top k most",
    "start": "1304720",
    "end": "1313120"
  },
  {
    "text": "highly activated um experts and then those outputs are um gated depending on",
    "start": "1313120",
    "end": "1318960"
  },
  {
    "text": "the implementation you might weight the outputs based on um this router weight or you might not um and then you will",
    "start": "1318960",
    "end": "1325600"
  },
  {
    "text": "just output the weighted average or just a straight sum depending on how your implementation works um and so a lot of",
    "start": "1325600",
    "end": "1332799"
  },
  {
    "text": "thee papers and and methods use top k switch transformer gshard grock mixtro clan uh all the deepseeek um variants",
    "start": "1332799",
    "end": "1341039"
  },
  {
    "text": "use uh different top k variants um maybe a very surprising fact",
    "start": "1341039",
    "end": "1347760"
  },
  {
    "text": "and this should really make you think about what's going on with um there are a lot of results that show that actually",
    "start": "1347760",
    "end": "1354159"
  },
  {
    "text": "you don't even need a smart router at all you can actually just use a hashing function at the very bottom to map these",
    "start": "1354159",
    "end": "1359919"
  },
  {
    "text": "x's onto your experts and even if you're doing hashing so no semantic information at all you will still get gains from a",
    "start": "1359919",
    "end": "1366799"
  },
  {
    "text": "hashing based which is pretty wild um some of the earliest work ones i",
    "start": "1366799",
    "end": "1375039"
  },
  {
    "text": "think had the very smart idea and in many ways the right idea if you're thinking about this top down of using rl",
    "start": "1375039",
    "end": "1381440"
  },
  {
    "text": "to learn the routing behavior right of course you know the choice of where to route to is a discrete decision and rl",
    "start": "1381440",
    "end": "1388320"
  },
  {
    "text": "is great for learning discrete decisions why don't you use rl to learn routing it was used in some of the earliest work on",
    "start": "1388320",
    "end": "1394080"
  },
  {
    "text": "mixture of experts um as far as i know basically no one does this now the compute cost to do this is too",
    "start": "1394080",
    "end": "1399919"
  },
  {
    "text": "prohibitive and you already have stability issues you might not want to do that um there have been a couple of",
    "start": "1399919",
    "end": "1406400"
  },
  {
    "text": "papers that have explored things like solving linear assignment problems or optimal transport style problems um",
    "start": "1406400",
    "end": "1412480"
  },
  {
    "text": "they're very elegant but once again the cost of doing this is much higher than the benefits that it gives you i think",
    "start": "1412480",
    "end": "1418400"
  },
  {
    "text": "in practice and it hasn't really uh been adopted but there's a lot of really interesting things that people are doing like this um to try to improve uh the",
    "start": "1418400",
    "end": "1427720"
  },
  {
    "text": "routing so now i can you know point at this slide and really talk through how routing um works in detail um so this is",
    "start": "1427720",
    "end": "1435679"
  },
  {
    "text": "the kind of top k routing that almost everyone has converged to um now um this",
    "start": "1435679",
    "end": "1441919"
  },
  {
    "text": "is the the router that's used in deepseek v1 to2 quen and grock do almost exactly this um there's a instead of",
    "start": "1441919",
    "end": "1450000"
  },
  {
    "text": "having a softmax directly at the bottom here um they do a soft uh deepsseek v3 mixrol dbrx um don't have a softmax at",
    "start": "1450000",
    "end": "1457919"
  },
  {
    "text": "the bottom but they'll softmax the g of its but it's a very minor difference so let's walk through what's going on here",
    "start": "1457919",
    "end": "1464799"
  },
  {
    "text": "um and try to to reason about you know the behavior of this so what's happening",
    "start": "1464799",
    "end": "1470240"
  },
  {
    "text": "here is at the very bottom um we've got our inputs this is our our ufl uh input",
    "start": "1470240",
    "end": "1477440"
  },
  {
    "text": "and i would like to take this sort of residual stream input and process it",
    "start": "1477440",
    "end": "1482640"
  },
  {
    "text": "through you know my first thing i'm going to do is i have to figure out which experts are going to",
    "start": "1482640",
    "end": "1488080"
  },
  {
    "text": "be activated now how am i going to do that well how i'm going to do that is very similar to attention i'm gonna take",
    "start": "1488080",
    "end": "1493760"
  },
  {
    "text": "my u which is my residual stream input and i'm gonna take the inner products with the e of i's these are kind of",
    "start": "1493760",
    "end": "1500080"
  },
  {
    "text": "learned vectors that are for each expert that tells the expert i'm an expert you",
    "start": "1500080",
    "end": "1505760"
  },
  {
    "text": "know that points in this direction right and so i'm computing in this inner product here expert and input affinity",
    "start": "1505760",
    "end": "1512000"
  },
  {
    "text": "and i'm computing a softmax to determine for each you know uh token what are you",
    "start": "1512000",
    "end": "1517279"
  },
  {
    "text": "know the best experts right so i normalize this is s of t now i take the s of i of t and i go through a top k",
    "start": "1517279",
    "end": "1524480"
  },
  {
    "text": "function i only select the k best um weights and then i use this as my gate",
    "start": "1524480",
    "end": "1531120"
  },
  {
    "text": "so i zero out everything else and i take the weighted average of each of the",
    "start": "1531120",
    "end": "1536320"
  },
  {
    "text": "experts outputs um and then i add that to my original residual stream and then",
    "start": "1536320",
    "end": "1542159"
  },
  {
    "text": "i return that right so this is hopefully very familiar to kind of um what you're",
    "start": "1542159",
    "end": "1547679"
  },
  {
    "text": "all very familiar with in terms of how you know uh transformer works with only the difference of this top k routing",
    "start": "1547679",
    "end": "1554480"
  },
  {
    "text": "piece is that clear kind of to everyone how this thing works good excellent so in some sense",
    "start": "1554480",
    "end": "1562240"
  },
  {
    "text": "the the mechanics of the the forward process of the routing is very simple um what is kind of mystifying is that fact",
    "start": "1562240",
    "end": "1569760"
  },
  {
    "text": "that you can learn this very well right this is in some sense a fairly complicated set of things to have to learn to do well by a model yes",
    "start": "1569760",
    "end": "1578279"
  },
  {
    "text": "so we're using soft max here uh previously one of the benefits of soft",
    "start": "1578279",
    "end": "1585200"
  },
  {
    "text": "max is that it's going to push you pretty extremely to choosing a singular",
    "start": "1585200",
    "end": "1590400"
  },
  {
    "text": "max it's not a hard max but it was so i'm having trouble thinking of the",
    "start": "1590400",
    "end": "1595679"
  },
  {
    "text": "intuition of putting the soft max basically on top of like combining it",
    "start": "1595679",
    "end": "1600960"
  },
  {
    "text": "with the top k where you're getting multiple and then you're using something that's going to push you towards choosing just one thing yeah i mean i",
    "start": "1600960",
    "end": "1609520"
  },
  {
    "text": "think maybe one way of thinking about the soft max is you know it the whole",
    "start": "1609520",
    "end": "1615760"
  },
  {
    "text": "purpose of this is just to make it so that when i average my experts later it kind of sums to one don't think of the",
    "start": "1615760",
    "end": "1621520"
  },
  {
    "text": "softmax as like a soft max operation even though that's literally the name um really the softmax operation is a",
    "start": "1621520",
    "end": "1627679"
  },
  {
    "text": "normalized to one operation and the normalize to one operation is going to make that a weighted average up top um",
    "start": "1627679",
    "end": "1634400"
  },
  {
    "text": "the other thing that's very important is you know you might think why can't i just get rid of the top k why don't i",
    "start": "1634400",
    "end": "1639600"
  },
  {
    "text": "just use the softmax here and just you know gate all the experts well then you immediately lose the the systems",
    "start": "1639600",
    "end": "1645360"
  },
  {
    "text": "efficiency aspect of this right you have to have top k during training otherwise you pay the training cost of all capital",
    "start": "1645360",
    "end": "1652640"
  },
  {
    "text": "n of your experts right this is the key thing about like we have to do all this",
    "start": "1652640",
    "end": "1657799"
  },
  {
    "text": "gymnastics to make sure that both at training time and inference time we have a sparse number of activated ated",
    "start": "1657799",
    "end": "1663679"
  },
  {
    "text": "experts that's why we go through the top k right okay yes from the back yeah so",
    "start": "1663679",
    "end": "1669520"
  },
  {
    "text": "because you're doing soft max first and then the top k get the weights you no longer have to guarantee",
    "start": "1669520",
    "end": "1677240"
  },
  {
    "text": "so the question was yeah so the question was um if you soft max first you no longer sum to one and yes uh that's",
    "start": "1678080",
    "end": "1684240"
  },
  {
    "text": "absolutely right you no longer sum to one and in some ways like there's no requirement that you have to sum to one",
    "start": "1684240",
    "end": "1690159"
  },
  {
    "text": "cuz you know the next layer can magnify it back up you know there's layer norms everywhere it's not as if it has to sum",
    "start": "1690159",
    "end": "1695440"
  },
  {
    "text": "to one but i think that is the reason why some of the other architectures basically move the location of the softmax there's a kind of aesthetic",
    "start": "1695440",
    "end": "1701440"
  },
  {
    "text": "choice about whether you really want that you know weight to be normalized to one or not",
    "start": "1701440",
    "end": "1706640"
  },
  {
    "text": "yes yeah so i was wondering like how does the e vector here relates to the weight of the feed forward okay so the",
    "start": "1706640",
    "end": "1714399"
  },
  {
    "text": "question was whether the whether and how the e vectors relate to the feed forward um they're not really tied in any way the e vectors are just learned vectors",
    "start": "1714399",
    "end": "1721039"
  },
  {
    "text": "for the just think of the e as parameters for the router right they're just separate objects from the ffm",
    "start": "1721039",
    "end": "1728360"
  },
  {
    "text": "i was just wondering how does compared to sampling from the soft",
    "start": "1728640",
    "end": "1734799"
  },
  {
    "text": "great uh the question was about how does it compare to sampling from the softmax um you can sample from the softmax and",
    "start": "1734799",
    "end": "1740720"
  },
  {
    "text": "and some um uh methods actually do a kind of soft sampling from the softmax",
    "start": "1740720",
    "end": "1746559"
  },
  {
    "text": "specifically um one of the google papers has a procedure where they take the top element of the softmax and then they",
    "start": "1746559",
    "end": "1752880"
  },
  {
    "text": "randomly sample the second element proportional to the remainder of the softmax um and that gives you more",
    "start": "1752880",
    "end": "1758559"
  },
  {
    "text": "exploration which is good but the drawback of that is that if you don't sample at test time now you've got a train test mismatch",
    "start": "1758559",
    "end": "1766919"
  },
  {
    "text": "okay yes why not just reormalize after the top k uh why not just reormalize",
    "start": "1766960",
    "end": "1772399"
  },
  {
    "text": "after k was the question is that right um in some some models do that some models do bring normalize off to the top",
    "start": "1772399",
    "end": "1777600"
  },
  {
    "text": "k but that's a kind of a choice like some architectures don't do that some architectures do it doesn't actually",
    "start": "1777600",
    "end": "1783120"
  },
  {
    "text": "matter because the scale can be basically adjusted post hop right so there's no reason why it has to sum to",
    "start": "1783120",
    "end": "1788720"
  },
  {
    "text": "one after the g operation cool oh sorry yes the",
    "start": "1788720",
    "end": "1797320"
  },
  {
    "text": "bias term is u there up there yeah so the first term of the sum if g is",
    "start": "1797320",
    "end": "1806960"
  },
  {
    "text": "approximating probability vector could be seen as an expectation of the function fn right plus you so so ff",
    "start": "1806960",
    "end": "1816640"
  },
  {
    "text": "actually uh this is not an expectation of ffn because each ffn is a different fn so this is not actually an",
    "start": "1816640",
    "end": "1823200"
  },
  {
    "text": "expectation and the gates are sparse so this is like a weighted selection operation over k different or actually",
    "start": "1823200",
    "end": "1830080"
  },
  {
    "text": "capital n different ffns and then the utl at the very end there you know if you remember the transformer that's the",
    "start": "1830080",
    "end": "1835919"
  },
  {
    "text": "residual stream right so i'm adding back the inputs because i want sort of a identity connection throughout",
    "start": "1835919",
    "end": "1842480"
  },
  {
    "text": "okay oh there's another question uh why does the router have such a basic parameterization like what happens if",
    "start": "1842480",
    "end": "1848880"
  },
  {
    "text": "you put more weights into um your router function right the the question was why",
    "start": "1848880",
    "end": "1854480"
  },
  {
    "text": "is the router so basic seems like if you're going to have experts it seems important to route to the right experts",
    "start": "1854480",
    "end": "1859600"
  },
  {
    "text": "um so why don't you do that um i think you know there have been some oblations",
    "start": "1859600",
    "end": "1864720"
  },
  {
    "text": "in some of the the earlier google papers on having like mlp uh routers and like more sophisticated things um i think the",
    "start": "1864720",
    "end": "1873200"
  },
  {
    "text": "the sort of complex answer here is that the systems concerns sort of weigh heavily if you're using a lot of flops",
    "start": "1873200",
    "end": "1878720"
  },
  {
    "text": "to make routing decisions you know you have to pay for those flops and so you have to get performance improvements in",
    "start": "1878720",
    "end": "1883760"
  },
  {
    "text": "just the routing you know and i think the one other thing to appreciate here is that there are really big limits to",
    "start": "1883760",
    "end": "1889840"
  },
  {
    "text": "how well you can route because the learning process for this routing thing is actually pretty dicey right because",
    "start": "1889840",
    "end": "1895840"
  },
  {
    "text": "how are you going to get gradients for which routers are good or bad well the only thing you have is if you have top",
    "start": "1895840",
    "end": "1901039"
  },
  {
    "text": "two then you can compare the two things that you have and you can push the gradients into s of t because your your",
    "start": "1901039",
    "end": "1906320"
  },
  {
    "text": "g is a weight and then the s of t might inform your your inner products but that's a very indirect way to be",
    "start": "1906320",
    "end": "1912080"
  },
  {
    "text": "learning your affinity so even if you make it complex there's no guarantee that you're going to really learn the optimal router",
    "start": "1912080",
    "end": "1917880"
  },
  {
    "text": "right great okay so um i think the one of the great",
    "start": "1917880",
    "end": "1924240"
  },
  {
    "text": "innovations of the deepseek and uh which was very quickly adopted by all the other sort of chinese",
    "start": "1924240",
    "end": "1931880"
  },
  {
    "text": "uhe releases is this idea of both a shared expert and a fine grained expert",
    "start": "1931880",
    "end": "1939360"
  },
  {
    "text": "um and so the basic structure that was sort of originally proposed is to take",
    "start": "1939360",
    "end": "1945279"
  },
  {
    "text": "your dense architecture and kind of copy the experts over right so in this case you know you're going to have let's say",
    "start": "1945279",
    "end": "1951200"
  },
  {
    "text": "if you have two if you have top two routing you know you're going to have twice the activated parameters of your",
    "start": "1951200",
    "end": "1956399"
  },
  {
    "text": "original dense model right so you take your and you copy it over and you activate k equals 2 so this is kind of",
    "start": "1956399",
    "end": "1962720"
  },
  {
    "text": "what you might think of as like the vanilla or like the basic that you might start with um people realize fairly",
    "start": "1962720",
    "end": "1969760"
  },
  {
    "text": "quickly um that having lots of experts is good and the logical sort of next",
    "start": "1969760",
    "end": "1975039"
  },
  {
    "text": "step beyond having lots of experts is good is i want lots of experts but i don't want to pay the parameter cost for",
    "start": "1975039",
    "end": "1980960"
  },
  {
    "text": "having lots of experts and so um deepssee basically argued that the right thing to do then was to cut the expert",
    "start": "1980960",
    "end": "1987679"
  },
  {
    "text": "up into smaller pieces right so remember last lecture i was you know telling you about oh the the kind of golden rule in",
    "start": "1987679",
    "end": "1994159"
  },
  {
    "text": "some sense is to have you know your your hidden layer and then you multiply that by four and that will give you kind of your projection layer right so now what",
    "start": "1994159",
    "end": "2000559"
  },
  {
    "text": "you would do is you would instead of multiplying by let's say four you might multiply by two right so now you have smaller matrices you have more fine",
    "start": "2000559",
    "end": "2006880"
  },
  {
    "text": "grained experts you can have twice as many of them right um and you can kind of take that logic much more to the extreme you can like you know quadruple",
    "start": "2006880",
    "end": "2013279"
  },
  {
    "text": "or multiply by eight and you can keep decreasing the size of your sort of projection uh dimension there that's",
    "start": "2013279",
    "end": "2019519"
  },
  {
    "text": "fine grained experts and there's you know drawbacks i'll i'll talk about later it's not it doesn't come for free so you have to be very careful about how",
    "start": "2019519",
    "end": "2025840"
  },
  {
    "text": "you you structure these things um and then the the other thing that you know",
    "start": "2025840",
    "end": "2031039"
  },
  {
    "text": "uh has been sort of studied and noted is maybe it's helpful to have at least some",
    "start": "2031039",
    "end": "2036480"
  },
  {
    "text": "mlp that can capture shared structure right like maybe there's just like processing that always needs to happen",
    "start": "2036480",
    "end": "2042240"
  },
  {
    "text": "no matter which token you're processing in that case it seems like kind of a waste to do all this routing work and to",
    "start": "2042240",
    "end": "2048638"
  },
  {
    "text": "have all these like you know parameters spread out everywhere when we can just have one shared or one or a few shared",
    "start": "2048639",
    "end": "2053919"
  },
  {
    "text": "experts you know whose job it is to handle all of this like shared processing that's needed and so they're",
    "start": "2053919",
    "end": "2059599"
  },
  {
    "text": "shared experts um and so this setup of using fine grained experts plus shared",
    "start": "2059599",
    "end": "2064878"
  },
  {
    "text": "experts um originally came out um in deepseek um although i think the",
    "start": "2064879",
    "end": "2070638"
  },
  {
    "text": "original inspiration came from deep speed um and quen and others um so",
    "start": "2070639",
    "end": "2076638"
  },
  {
    "text": "almost all of the uh open uh releases since deepseek have adopted some sets of",
    "start": "2076639",
    "end": "2083280"
  },
  {
    "text": "these innovations because it's it's quite clear that especially fine grained experts is just really really useful",
    "start": "2083280",
    "end": "2089200"
  },
  {
    "text": "that's a kind of no-brainer um at this point to do um one of the things i",
    "start": "2089200",
    "end": "2095200"
  },
  {
    "text": "really like about uh reading deepseek papers is that they do ablations you know it's not like a whatever sales tech",
    "start": "2095200",
    "end": "2102480"
  },
  {
    "text": "report you know they actually care about whether or not their methods work um and so they have this lovely ablation in the",
    "start": "2102480",
    "end": "2108760"
  },
  {
    "text": "deepseek paper where they show you know the the blue bar over here this is g-shard this is a very basic vanilla",
    "start": "2108760",
    "end": "2116000"
  },
  {
    "text": "implementation of you know you can have uh one shared expert that's the orange bar and that gives you a big boost on",
    "start": "2116000",
    "end": "2122160"
  },
  {
    "text": "some tasks and no boosts on others um you can have fine grained experts that's the green and orange you know bars and",
    "start": "2122160",
    "end": "2128560"
  },
  {
    "text": "you get further boosts from that and if you compare the blue to the orange you know composing all these differences give you quite the big boost um over",
    "start": "2128560",
    "end": "2136240"
  },
  {
    "text": "others um and so we can see that you know more experts and shared experts generally um seem to help okay yes",
    "start": "2136240",
    "end": "2143119"
  },
  {
    "text": "question like when it says seven out of something does that mean it's doing like top seven",
    "start": "2143119",
    "end": "2149359"
  },
  {
    "text": "yes sorry i should have i should have explained that that's right um so x out of y means x activated out of y total",
    "start": "2149359",
    "end": "2156400"
  },
  {
    "text": "routed experts that's right yeah and so you can kind of see the pattern here as well of as you increase the number of",
    "start": "2156400",
    "end": "2161839"
  },
  {
    "text": "experts you also often increase the number of activated experts um especially if you're doing fine grained",
    "start": "2161839",
    "end": "2167200"
  },
  {
    "text": "experts it flops wise it's free right because you know each expert is now",
    "start": "2167200",
    "end": "2172599"
  },
  {
    "text": "smaller good okay um so has you know basically",
    "start": "2172599",
    "end": "2179440"
  },
  {
    "text": "corroborating evidence that shows really nicely uh that these things work so uh",
    "start": "2179440",
    "end": "2184880"
  },
  {
    "text": "the bottom one i think i'll start with because it's more decisive um shows you know fine- grained experts going from 8",
    "start": "2184880",
    "end": "2190480"
  },
  {
    "text": "to 32 to 64 fine grained experts mirroring in some sense the deepseek ablations um and you see very clear",
    "start": "2190480",
    "end": "2196720"
  },
  {
    "text": "trends and losses and other kinds of um uh metrics that you see improvements going from 8 to 32 to 64 right fine",
    "start": "2196720",
    "end": "2203520"
  },
  {
    "text": "grain experts is great um shared experts which is uh purple versus teal at the",
    "start": "2203520",
    "end": "2208960"
  },
  {
    "text": "very top um you actually don't see really any gains at least in the mo setup um so they actually end up going",
    "start": "2208960",
    "end": "2215119"
  },
  {
    "text": "with no shared experts um even though the deep seek paper seemed to show more gain so that one actually is maybe more",
    "start": "2215119",
    "end": "2220320"
  },
  {
    "text": "mixed given this sort of follow-up or this you know third party uh replication of these kinds of",
    "start": "2220320",
    "end": "2227160"
  },
  {
    "text": "ideas so at this point you might be wondering you know what are common configurations i think i'm going to you",
    "start": "2227160",
    "end": "2233280"
  },
  {
    "text": "know take the page out of you know last lectures playbook of looking at a lot of",
    "start": "2233280",
    "end": "2238560"
  },
  {
    "text": "the recent releases you know looking at what people do and trying to talk a little bit about the patterns that have",
    "start": "2238560",
    "end": "2244240"
  },
  {
    "text": "have arisen um so some of the early um uh google papers so ghart switch",
    "start": "2244240",
    "end": "2250079"
  },
  {
    "text": "transformer stmoe um some of them had really large numbers of routed experts",
    "start": "2250079",
    "end": "2255520"
  },
  {
    "text": "um and there was a lot of like really interesting stuff going on in those papers i'd encourage you to read them um some of them happened in lstms and other",
    "start": "2255520",
    "end": "2262079"
  },
  {
    "text": "kinds of architectures um regardless you know very quickly i think there was like kind of a period of like 8 to 16 experts",
    "start": "2262079",
    "end": "2269520"
  },
  {
    "text": "like mixtrol dbrx grock with two active um experts those worked reasonably well",
    "start": "2269520",
    "end": "2275440"
  },
  {
    "text": "but then kind of deepsee or deepseek you know v1 comes out um that has kind of",
    "start": "2275440",
    "end": "2281119"
  },
  {
    "text": "the the prototypical configuration i told you about fine grained experts 64 of them six actively routed two shared",
    "start": "2281119",
    "end": "2287680"
  },
  {
    "text": "experts um and each sort of expert is sort of one/4 the size of a of a normally sized expert um take that last",
    "start": "2287680",
    "end": "2294000"
  },
  {
    "text": "column with a grain of salt because i had to sort of back them out from like config files and things like that so i'm not 100% sure about the exact ratios",
    "start": "2294000",
    "end": "2300560"
  },
  {
    "text": "here um so we've then got essentially quen 1.5 deepseek v3 um minax these are",
    "start": "2300560",
    "end": "2308079"
  },
  {
    "text": "you know chinese they follow essentially in the same footsteps as deepseek v1 the specific numbers are different but in",
    "start": "2308079",
    "end": "2314480"
  },
  {
    "text": "the in the sense that they use you know um fine grained experts and they often have shared experts they're very similar",
    "start": "2314480",
    "end": "2321680"
  },
  {
    "text": "to kind of this um original deepsee configuration um omo minimax and llama",
    "start": "2321680",
    "end": "2327520"
  },
  {
    "text": "are very recent they definitely do all this like fine grained expert stuff um",
    "start": "2327520",
    "end": "2333119"
  },
  {
    "text": "and llama 4 also uses um a shared expert and you kind of see um sort of variations in configuration but you see",
    "start": "2333119",
    "end": "2339760"
  },
  {
    "text": "what's basically shared which is this this fine grained experts id idea and especially for the big models like llama 4 and deepseeek very very large numbers",
    "start": "2339760",
    "end": "2346400"
  },
  {
    "text": "of routed experts or sorry not not routed like total total experts yes so can you explain what the ratios are the",
    "start": "2346400",
    "end": "2354400"
  },
  {
    "text": "the ratio is expend uh is representing roughly like how much each export is sliced relative to having just the",
    "start": "2354400",
    "end": "2361359"
  },
  {
    "text": "standard dense configuration so in terms of hyperparameters you know that if you're following the rule of thumb your",
    "start": "2361359",
    "end": "2366960"
  },
  {
    "text": "hidden dimension and sort of your projection from in your mlp should be about 1 to four or one to 2.6 if you're",
    "start": "2366960",
    "end": "2373040"
  },
  {
    "text": "doing a gated network right and so by looking at the hidden layers of these architectures you can kind of see how",
    "start": "2373040",
    "end": "2378079"
  },
  {
    "text": "many times they sliced up that that original uh uh feed forward size so if",
    "start": "2378079",
    "end": "2383680"
  },
  {
    "text": "like for those experts does that mean that",
    "start": "2383680",
    "end": "2390000"
  },
  {
    "text": "like still increasing their group like the factor that's right yeah so so you",
    "start": "2390000",
    "end": "2395599"
  },
  {
    "text": "know you can think of this as roughly you know they have you know 16 normally sized experts oh okay and so they you",
    "start": "2395599",
    "end": "2401520"
  },
  {
    "text": "know they're they're of course having more parameters than the dens equivalent they have six routed um so they have",
    "start": "2401520",
    "end": "2408160"
  },
  {
    "text": "eight total active experts at any time each that are quarter sized and so you should think of them as like roughly",
    "start": "2408160",
    "end": "2413280"
  },
  {
    "text": "double the flops right of a of a dense equivalent so some arithmetic but hopefully uh the math is clear and",
    "start": "2413280",
    "end": "2419359"
  },
  {
    "text": "consistent hopefully yes like the ratios like one are like",
    "start": "2419359",
    "end": "2427240"
  },
  {
    "text": "so for some of the exotic ratios i'm not quite sure why they're that way but they are very precisely whole numbers when",
    "start": "2430160",
    "end": "2436000"
  },
  {
    "text": "you when you take the ratios between the the ffns and the implied hyperparameters and so i think those are exactly the the",
    "start": "2436000",
    "end": "2442240"
  },
  {
    "text": "split counts of like how much they were sliced but i'm not sure why they they have one over 14 i mean like like does",
    "start": "2442240",
    "end": "2448240"
  },
  {
    "text": "it do you ever like project to like smaller dimension because like that ratio is so small in the mlp so yeah so",
    "start": "2448240",
    "end": "2454800"
  },
  {
    "text": "yeah oh that's why you're asking like do they do they down project yeah that's right in some of them they are actually smaller i don't remember which models in",
    "start": "2454800",
    "end": "2460880"
  },
  {
    "text": "particular but in some of them i do remember they were actually down project yes what is the intuition for wanting",
    "start": "2460880",
    "end": "2467520"
  },
  {
    "text": "more than one shared expert yeah i mean i it does kind of seem like there was a",
    "start": "2467520",
    "end": "2473440"
  },
  {
    "text": "period where where some of the chinese lm companies tried many shared experts and then you know people have come back",
    "start": "2473440",
    "end": "2478640"
  },
  {
    "text": "to zero or one and if you look at um the om ablations it's not quite clear that even one shared expert is decisively",
    "start": "2478640",
    "end": "2485200"
  },
  {
    "text": "useful um i think the original motivation was that then you have equally sized you know experts like",
    "start": "2485200",
    "end": "2492000"
  },
  {
    "text": "these are both one quarter sized experts and now you have eight active experts total and so you can keep the sizes",
    "start": "2492000",
    "end": "2498560"
  },
  {
    "text": "consistent otherwise i don't really see a particular justification for why it should be two smaller one versus one",
    "start": "2498560",
    "end": "2504560"
  },
  {
    "text": "larger one okay cool",
    "start": "2504560",
    "end": "2511119"
  },
  {
    "text": "so then hopefully you know you get a sense of how the the routing works um for a lot of these and how it's all set",
    "start": "2511119",
    "end": "2517200"
  },
  {
    "text": "up the forward pass hopefully you fully understand um now we need to think about training and training is pretty pretty",
    "start": "2517200",
    "end": "2523359"
  },
  {
    "text": "gnarly right um and the major challenge i foreshadowed earlier right when we train we cannot turn on all the experts",
    "start": "2523359",
    "end": "2530720"
  },
  {
    "text": "because if we do that then we pay the full flops cost of all the experts right",
    "start": "2530720",
    "end": "2536079"
  },
  {
    "text": "having a model that's like i don't know 256 times more expensive to train is a total no-go right so we need train times",
    "start": "2536079",
    "end": "2542800"
  },
  {
    "text": "sparity but sparse gaining decisions are are obviously not differentiable we now have a kind of annoying rlish problem",
    "start": "2542800",
    "end": "2550480"
  },
  {
    "text": "and so we could do any of these things like rl to optimize gating policies we could do you know bandit inspired things",
    "start": "2550480",
    "end": "2556640"
  },
  {
    "text": "of doing randomization to to do exploration um or you know we can just have some heruristics that try to",
    "start": "2556640",
    "end": "2563200"
  },
  {
    "text": "balance things out right like put some loss terms in there and hope things work out um you know having gone through deep",
    "start": "2563200",
    "end": "2569440"
  },
  {
    "text": "learning classes of many kinds you can kind of guess internally which one people use in practice um and i'll talk",
    "start": "2569440",
    "end": "2575280"
  },
  {
    "text": "about each one of these three in turn okay so rl i think is one of the the",
    "start": "2575280",
    "end": "2582880"
  },
  {
    "text": "earliest things that people tried it's probably the most principle thing that you can do in this space right you have",
    "start": "2582880",
    "end": "2588240"
  },
  {
    "text": "a you know non-ifferiable routing decision well think of that as a policy",
    "start": "2588240",
    "end": "2593359"
  },
  {
    "text": "throw rl at it and then solve the problem um unfortunately it's not better",
    "start": "2593359",
    "end": "2599760"
  },
  {
    "text": "than a lot of the other things that you can do um there is a paper by clark at",
    "start": "2599760",
    "end": "2605040"
  },
  {
    "text": "all in 2020 who were exploring various like scaling related questions in",
    "start": "2605040",
    "end": "2610599"
  },
  {
    "text": "uh and they do have an rl baseline that you know i was able to dig up um but",
    "start": "2610599",
    "end": "2616000"
  },
  {
    "text": "unfortunately it's not really that much better than say using hashing for decisions and they were you know they",
    "start": "2616000",
    "end": "2622400"
  },
  {
    "text": "were really interested in benchmarking this thing on the left called sbs which is like a linear assignment kind of a method and that thing you know handily",
    "start": "2622400",
    "end": "2628480"
  },
  {
    "text": "beats you know doing rl and i think in practice the the gradient variances and",
    "start": "2628480",
    "end": "2634319"
  },
  {
    "text": "complexity means that it's pretty finicky to use and no one you know at at scale has really used um an rl based",
    "start": "2634319",
    "end": "2641599"
  },
  {
    "text": "approach to optimize these gating decisions as far as i know um a thing",
    "start": "2641599",
    "end": "2647280"
  },
  {
    "text": "that has been done much more at scale um is stochastic approximations of various",
    "start": "2647280",
    "end": "2652760"
  },
  {
    "text": "kinds um so what they might do is they might add a bit of you know",
    "start": "2652760",
    "end": "2658760"
  },
  {
    "text": "perturbations um so here is an example of one um from shazir in 2017 um this is",
    "start": "2658760",
    "end": "2665040"
  },
  {
    "text": "one of the early uh papers where they're still going to do kind of top k routing",
    "start": "2665040",
    "end": "2670720"
  },
  {
    "text": "so they're going to keep the top k elements of this h of x operation and they're going to softmax that to get the",
    "start": "2670720",
    "end": "2676680"
  },
  {
    "text": "gate but what we're going to do to get this you know h of x operation is kind",
    "start": "2676680",
    "end": "2681760"
  },
  {
    "text": "of the following so what we're going to do is we're going to have our original sort of linear you know affinity this is",
    "start": "2681760",
    "end": "2688000"
  },
  {
    "text": "identical to what we were doing before um we were basically just computing you know our inputs x and you know a sort of",
    "start": "2688000",
    "end": "2694079"
  },
  {
    "text": "learned weight for each gate and so this part's the same but i'm actually now going to jitter it a little bit i'm",
    "start": "2694079",
    "end": "2699599"
  },
  {
    "text": "going to add a normal and then i'm going to pick sort of a w noise scale um",
    "start": "2699599",
    "end": "2705520"
  },
  {
    "text": "that's learned and this thing is going to control how much noise to inject into this process and you can kind of think",
    "start": "2705520",
    "end": "2711359"
  },
  {
    "text": "of this as a stochcastic exploration policy and by manipulating w noise in particular ways like sort of a kneeling",
    "start": "2711359",
    "end": "2717599"
  },
  {
    "text": "it down or doing various things i can control the exploration exploitation trade-offs um that this is going to have",
    "start": "2717599",
    "end": "2724720"
  },
  {
    "text": "right and so this is going to give you one solution to the to the explore exploit dilemma um and especially if",
    "start": "2724720",
    "end": "2731520"
  },
  {
    "text": "you're noising things up each expert might randomly get you know some other tokens that it wasn't expecting to get",
    "start": "2731520",
    "end": "2737359"
  },
  {
    "text": "so it'll lead to experts that are less specialized but maybe a little bit more robust um and so that that seems",
    "start": "2737359",
    "end": "2743599"
  },
  {
    "text": "generally quite nice um of course the stochasticity also means that you don't get as much specialization and that",
    "start": "2743599",
    "end": "2750480"
  },
  {
    "text": "leads to loss of efficiency um and you know there's",
    "start": "2750480",
    "end": "2755599"
  },
  {
    "text": "another approach that people have done where they sort of multiply the router loits um or sorry they they uh add yeah",
    "start": "2755599",
    "end": "2762480"
  },
  {
    "text": "have a multiplicative perturbation to the to the router logets um with the",
    "start": "2762480",
    "end": "2767520"
  },
  {
    "text": "goal of getting less brittle experts um but this sort of jitter process was kind of removed in some of the later papers",
    "start": "2767520",
    "end": "2774079"
  },
  {
    "text": "because they found it just didn't work as well as some of the heristic loss based approaches and so this was an",
    "start": "2774079",
    "end": "2779760"
  },
  {
    "text": "approach that was tried in a couple this kind of stochastic routing tricks were were tried in a couple of the the early",
    "start": "2779760",
    "end": "2785040"
  },
  {
    "text": "google papers um but i think that has generally been abandoned by a lot of the",
    "start": "2785040",
    "end": "2790079"
  },
  {
    "text": "the people training these okay so yes um for the stochcastic like",
    "start": "2790079",
    "end": "2797359"
  },
  {
    "text": "what problem does that solve because we're still taking the top k so we still can't differentiate backwards right well",
    "start": "2797359",
    "end": "2803200"
  },
  {
    "text": "if you think of this so the question was um we still can't differentiate because we're taking the top k but if you kind",
    "start": "2803200",
    "end": "2808480"
  },
  {
    "text": "of change the your interpretation of the problem a little bit um if you think about a bandit problem right it has the",
    "start": "2808480",
    "end": "2814720"
  },
  {
    "text": "same structure as this where you know you pull a bandit arm and you don't see any of the other arms so you can't you know really allocate your resources",
    "start": "2814720",
    "end": "2821040"
  },
  {
    "text": "efficiently if you pull some of the other ones at random now you've got enough data to be able to do some",
    "start": "2821040",
    "end": "2826240"
  },
  {
    "text": "optimization and so this jittering is very similar to in spirit to this kind of like epsilon greedy style exploration",
    "start": "2826240",
    "end": "2832880"
  },
  {
    "text": "thing where you're randomly pulling some of the other arms with some probability where the probability itself depends on",
    "start": "2832880",
    "end": "2838800"
  },
  {
    "text": "how confident you are about this routing decision so that's kind of the intuition and then of course you know um that's",
    "start": "2838800",
    "end": "2846160"
  },
  {
    "text": "going to give you some way of of getting some signal back",
    "start": "2846160",
    "end": "2851319"
  },
  {
    "text": "okay so um the thing that in practice um people have ended up with is you know we",
    "start": "2851319",
    "end": "2859280"
  },
  {
    "text": "don't do any of that we don't do you know rl we don't do stochastic exploration um but we rely on really",
    "start": "2859280",
    "end": "2865280"
  },
  {
    "text": "another mechanism to sort of keep things reasonable so if we're doing top two routing right technically speaking we do",
    "start": "2865280",
    "end": "2872079"
  },
  {
    "text": "get some signal in the gradient descent process because we can compare the top two you know uh experts that we did",
    "start": "2872079",
    "end": "2877280"
  },
  {
    "text": "evaluate um and so it's possible to do you know some optimization but when we do you know ignore if we if we drop all",
    "start": "2877280",
    "end": "2884560"
  },
  {
    "text": "the other constraints um the big issue that arises is you just end up sort of picking one expert all the time and that",
    "start": "2884560",
    "end": "2891200"
  },
  {
    "text": "expert is good at everything and all the other experts are terrible right you end up in this local minimum where you've routed all of your tokens to one experts",
    "start": "2891200",
    "end": "2898000"
  },
  {
    "text": "all the time so really the key game becomes then how do we get out of that local minimum and loss balancing or like",
    "start": "2898000",
    "end": "2905839"
  },
  {
    "text": "balancing losses is really the key trick to get out of this and this is this is kind of important to understand because",
    "start": "2905839",
    "end": "2911760"
  },
  {
    "text": "this is the the loss that mostly everyone actually uses to train the right so if you were zoning out earlier",
    "start": "2911760",
    "end": "2918559"
  },
  {
    "text": "you know you probably should make sure to pay attention to these this particular set of equations here um so",
    "start": "2918559",
    "end": "2924240"
  },
  {
    "text": "this is originally from the the switch transformer um from fetus at all in 2022",
    "start": "2924240",
    "end": "2929920"
  },
  {
    "text": "and they add this particular loss where what they're going to do is they're going to you know loop all over each of",
    "start": "2929920",
    "end": "2935599"
  },
  {
    "text": "the experts and they're going to take you know the you could think of this as an inner product between the vector f",
    "start": "2935599",
    "end": "2940800"
  },
  {
    "text": "and the the vector p and so what are these vectors well f is for each of the",
    "start": "2940800",
    "end": "2946200"
  },
  {
    "text": "experts this is the fraction of the tokens that were allocated to expert i so you can think of this as kind of a",
    "start": "2946200",
    "end": "2952240"
  },
  {
    "text": "probability vector that's telling me you know what fraction of my tokens in my batch or in my you know whatever the",
    "start": "2952240",
    "end": "2958079"
  },
  {
    "text": "unit is here um did i route to expert per i now p of i is the fraction of the",
    "start": "2958079",
    "end": "2966400"
  },
  {
    "text": "router probability that was allocated to expert i so the router probability is",
    "start": "2966400",
    "end": "2971520"
  },
  {
    "text": "kind of the um the the original sort of softmaxed routing decision that i was",
    "start": "2971520",
    "end": "2976960"
  },
  {
    "text": "sort of intending to send right so this is kind of measuring p of i is what was sort of the the intended probability",
    "start": "2976960",
    "end": "2983119"
  },
  {
    "text": "from the router and then um f of i was what was the actual sort of like you know uh what was the actual routing",
    "start": "2983119",
    "end": "2989680"
  },
  {
    "text": "decision made by the top k method and one thing that's kind of interesting to",
    "start": "2989680",
    "end": "2995359"
  },
  {
    "text": "to look at here is let's say we take the derivative of that loss with respect to to p of i so you know this is a a linear",
    "start": "2995359",
    "end": "3003520"
  },
  {
    "text": "function with respect to p of i and you'll see that the strongest downweing",
    "start": "3003520",
    "end": "3008640"
  },
  {
    "text": "action happens on the sort of biggest experts with the biggest allocations right so the it's actually in fact",
    "start": "3008640",
    "end": "3014880"
  },
  {
    "text": "proportional to the amount of tokens that you get so you're going to be pushed downwards um sort of more",
    "start": "3014880",
    "end": "3022160"
  },
  {
    "text": "strongly if you got more tokens and so this is kind of the basic behavior of this loss and you know almost everybody",
    "start": "3022160",
    "end": "3029760"
  },
  {
    "text": "uses this kind of f.p kind of a of a trick to try to balance tokens across",
    "start": "3029760",
    "end": "3035359"
  },
  {
    "text": "different units so the basic unit that you might want to balance over um initially is batches you might want each",
    "start": "3035359",
    "end": "3040720"
  },
  {
    "text": "batch to get allocated evenly to experts but you might actually have other kinds um of uh balancing that you might want",
    "start": "3040720",
    "end": "3048000"
  },
  {
    "text": "to do um and deepseek does uh exactly this kind of thing i'll talk about all the variants that they've thrown in but",
    "start": "3048000",
    "end": "3053599"
  },
  {
    "text": "you know the first thing is per expert balancing per batch so each batch they want to make sure experts get an even",
    "start": "3053599",
    "end": "3058960"
  },
  {
    "text": "number of tokens and you know this is from the deepseek paper and hopefully this looks you know very familiar to you",
    "start": "3058960",
    "end": "3064400"
  },
  {
    "text": "this is exactly the same you know f.p p inner product structure as you saw before you know p of i is defined a",
    "start": "3064400",
    "end": "3070720"
  },
  {
    "text": "little bit differently that's s of i of t you know but that should be familiar from earlier as well that's the softmax",
    "start": "3070720",
    "end": "3076160"
  },
  {
    "text": "pre-top k right so hopefully this looks all pretty good to you um the other",
    "start": "3076160",
    "end": "3081839"
  },
  {
    "text": "thing you might want though is you know you might want to balance across experts that's all well and good but you might",
    "start": "3081839",
    "end": "3086960"
  },
  {
    "text": "also want to think about the systems concerns right because you're going to shard your experts onto different",
    "start": "3086960",
    "end": "3092000"
  },
  {
    "text": "devices and you might want to balance per device right and so you might have another loss that's essentially the same",
    "start": "3092000",
    "end": "3098720"
  },
  {
    "text": "structure but instead of summing you know which tokens go to which experts you might measure which tokens go to",
    "start": "3098720",
    "end": "3105359"
  },
  {
    "text": "which devices right and that's going to be a different f that's measured over the device groups rather than over each",
    "start": "3105359",
    "end": "3111760"
  },
  {
    "text": "expert and so now you can set up a different loss to balance over devices you optimize this you're naturally going",
    "start": "3111760",
    "end": "3118559"
  },
  {
    "text": "to try to learn routing functions that make sure each gpu or each tpu what have",
    "start": "3118559",
    "end": "3123680"
  },
  {
    "text": "you um have an even number of tokens leading to even utilization right and that would be great from a systems",
    "start": "3123680",
    "end": "3129880"
  },
  {
    "text": "perspective so basically everyone does you know kind of this kind of a thing um",
    "start": "3129880",
    "end": "3136480"
  },
  {
    "text": "and so deep seek v3 um actually kind of innovates a little bit this is this is",
    "start": "3136480",
    "end": "3141599"
  },
  {
    "text": "kind of cool and i don't think i've seen this before it's one of the uh first things in thee world that doesn't",
    "start": "3141599",
    "end": "3147040"
  },
  {
    "text": "actually come from google really um which is that they have gotten rid of",
    "start": "3147040",
    "end": "3152400"
  },
  {
    "text": "this per expert balancing term they've gotten rid of this entirely and instead",
    "start": "3152400",
    "end": "3157599"
  },
  {
    "text": "what they now do is they basically take their soft max scores and they add a",
    "start": "3157599",
    "end": "3162640"
  },
  {
    "text": "little fudge factor b of i where b of i is a little fudge factor score for each expert right so expert i you know might",
    "start": "3162640",
    "end": "3169920"
  },
  {
    "text": "get upweed or downweed so if if an expert isn't getting enough tokens you know it's going to uh be given a higher",
    "start": "3169920",
    "end": "3177200"
  },
  {
    "text": "b of and then that's going to allow it to grab more tokens um and the way that this works is um uh the way that this",
    "start": "3177200",
    "end": "3185599"
  },
  {
    "text": "works is that they're going to learn bofi through a really simple online gradient scheme online learning and so",
    "start": "3185599",
    "end": "3191839"
  },
  {
    "text": "they're going to measure at each batch you know what are each of the experts getting like are they getting an even",
    "start": "3191839",
    "end": "3196880"
  },
  {
    "text": "number of tokens and if they're not getting enough tokens they add sort of gamma some learning rate to b of i sort",
    "start": "3196880",
    "end": "3203200"
  },
  {
    "text": "of making it higher if they're if they're getting too many tokens they're going to subtract gamma making that",
    "start": "3203200",
    "end": "3208240"
  },
  {
    "text": "expert slightly less attractive right so they're just learning little you know offsets for each of the s of i and",
    "start": "3208240",
    "end": "3213599"
  },
  {
    "text": "notice here you know you're only using the b of i to make the routing decisions you're not actually sending it over as",
    "start": "3213599",
    "end": "3219040"
  },
  {
    "text": "part of your gating weights right that's a that's a sort of somewhat important thing to do so they call this auxiliary",
    "start": "3219040",
    "end": "3224480"
  },
  {
    "text": "loss free balancing if you go and read the deepseek v3 paper which all of you should because it's a it's a really nice",
    "start": "3224480",
    "end": "3229920"
  },
  {
    "text": "paper um they'll make a big deal about how this makes training so stable so great um so wonderful um and then of",
    "start": "3229920",
    "end": "3237599"
  },
  {
    "text": "course you like keep reading the section and they're like actually but we decided that you know for each sequence maybe we",
    "start": "3237599",
    "end": "3242880"
  },
  {
    "text": "still want to be balanced and this doesn't work well enough so we've added the the you know the heristic loss back so they do have um something called a",
    "start": "3242880",
    "end": "3249280"
  },
  {
    "text": "complimentary sequence-wise auxiliary loss that you know is basically exactly the auxiliary loss um that they decided",
    "start": "3249280",
    "end": "3255839"
  },
  {
    "text": "they needed because what they wanted to do was to balance um uh load balance the",
    "start": "3255839",
    "end": "3261200"
  },
  {
    "text": "experts at a per sequence level rather than a per batch level um i'm not sure why they do this particular thing rather",
    "start": "3261200",
    "end": "3267760"
  },
  {
    "text": "than any other sort of you know uh b of style trick but that's just kind of what",
    "start": "3267760",
    "end": "3272880"
  },
  {
    "text": "they do um in deepseek v3 so it's not fully auxiliary lossfree as they'd like you to believe okay oh yes question this",
    "start": "3272880",
    "end": "3282079"
  },
  {
    "text": "is a bit of an unfair question but if we did not have to worry about systems optimizations do you think the",
    "start": "3282079",
    "end": "3287920"
  },
  {
    "text": "performance of this model would be a lot better or would it stay roughly the same if we did not think about systems",
    "start": "3287920",
    "end": "3293920"
  },
  {
    "text": "optimization would the performance of this model be better or stay the same when you say this model what do you mean",
    "start": "3293920",
    "end": "3299200"
  },
  {
    "text": "deep seek v3 or like just in general like this model never so are you saying like if we ignore the systems concerns",
    "start": "3299200",
    "end": "3306960"
  },
  {
    "text": "um do we think are still good is that kind of one way of asking that question",
    "start": "3306960",
    "end": "3312240"
  },
  {
    "text": "like would the performance on downstream pass for example be better than what we have right now yeah so i think um i",
    "start": "3312240",
    "end": "3319760"
  },
  {
    "text": "didn't have to balance this like i must set roughly equal number of tokens for every expert or yeah yeah that's right",
    "start": "3319760",
    "end": "3326720"
  },
  {
    "text": "that's right well i think actually per expert balancing this term right this is not a systems concern so you still want",
    "start": "3326720",
    "end": "3333520"
  },
  {
    "text": "to do this because if you don't do this what you'll find um and actually there's you know i'm going to keep referring to",
    "start": "3333520",
    "end": "3338800"
  },
  {
    "text": "the old mode paper because they have so many ablations they have a a really nice ablation where they get rid of exactly this um and what they find is um",
    "start": "3338800",
    "end": "3346720"
  },
  {
    "text": "basically early on in training the model just picks like one or two experts and all the other experts are dead like the",
    "start": "3346720",
    "end": "3352640"
  },
  {
    "text": "router never sends anything to them so you're just wasting memory at that point right so now you've just lost",
    "start": "3352640",
    "end": "3357920"
  },
  {
    "text": "performance for free you've effectively gotten a smaller model and so even if you ignore all the other like device",
    "start": "3357920",
    "end": "3363280"
  },
  {
    "text": "balancing parallelism concerns you've just gotten a worse model because you didn't properly allocate your your",
    "start": "3363280",
    "end": "3368960"
  },
  {
    "text": "experts right it's the same way as like you want to use all your parameters right you would like to effectively use your parameters you want to do expert",
    "start": "3368960",
    "end": "3375359"
  },
  {
    "text": "debalancing sorry say uh device what does device",
    "start": "3375359",
    "end": "3381040"
  },
  {
    "text": "refer to yeah actually um so normally this would refer to like gpu or tpu there is a subtlety i i'll talk about",
    "start": "3381040",
    "end": "3387440"
  },
  {
    "text": "this maybe in the very last or second to last slide um there are more sophisticated and cool versions of this",
    "start": "3387440",
    "end": "3392480"
  },
  {
    "text": "where you try to balance things to minimize communication costs as well and so there's you know broader notions of",
    "start": "3392480",
    "end": "3398079"
  },
  {
    "text": "device like you know one rack or whatever else but here it usually refers to like gpu",
    "start": "3398079",
    "end": "3403760"
  },
  {
    "text": "yes going back to the fact that like hashing as a routing algorithm seems to",
    "start": "3403760",
    "end": "3410240"
  },
  {
    "text": "improve performance like is there intuition for that because that's effectively just like randomly choosing",
    "start": "3410240",
    "end": "3416880"
  },
  {
    "text": "a um like one of the few forward members to send it through right so like why",
    "start": "3416880",
    "end": "3422240"
  },
  {
    "text": "does having multiple copies of that i guess each of which get less data why does that make performance better yes",
    "start": "3422240",
    "end": "3428799"
  },
  {
    "text": "the question was um why does hashing do anything at all um i don't have the you",
    "start": "3428799",
    "end": "3434319"
  },
  {
    "text": "really precise intuition for this but you can make arguments either two ways one is you know even if you're hashing",
    "start": "3434319",
    "end": "3439520"
  },
  {
    "text": "the same tokens are going to go to the same you know or the same kinds of you know sequences are going to go to the",
    "start": "3439520",
    "end": "3444720"
  },
  {
    "text": "same expert every time right and so each expert will still get some deterministic",
    "start": "3444720",
    "end": "3450559"
  },
  {
    "text": "subset of the inputs and so there's some specialization that can still occur it's just non-semantic or you know non-learned um and if you're a",
    "start": "3450559",
    "end": "3457599"
  },
  {
    "text": "distribution zipian like the word 'the' might dominate one expert you know and so you might still get actually semantic",
    "start": "3457599",
    "end": "3463680"
  },
  {
    "text": "specialization where like one expert is effectively dominated by like very frequent things but like a random uh",
    "start": "3463680",
    "end": "3469599"
  },
  {
    "text": "routing function probably wouldn't be a day like a pure random thing that's not dependent on input i would bet that that",
    "start": "3469599",
    "end": "3475839"
  },
  {
    "text": "would be really terrible yes i have never run or seen that but yes i think that would be that would be horrible",
    "start": "3475839",
    "end": "3481520"
  },
  {
    "text": "good yes yeah so for like during lm like you have many layers right many transform i think in the lecture you",
    "start": "3481520",
    "end": "3489119"
  },
  {
    "text": "mentioned that each expert okay so like you do like ad like 32 layers like 64",
    "start": "3489119",
    "end": "3496319"
  },
  {
    "text": "experts that's like a lot of gpus or i wonder if like experts are bundled together on like a single gpu is that",
    "start": "3496319",
    "end": "3504400"
  },
  {
    "text": "the question was like won't you need lots of gpus if you have lots of layers and lots of experts yeah if you if you",
    "start": "3504400",
    "end": "3509440"
  },
  {
    "text": "exclusively give a gpu to a single expert yes that would be that would be kind of crazy um but you would kind of",
    "start": "3509440",
    "end": "3515520"
  },
  {
    "text": "shard things so that each gp would hold you know enough of these units um to you know effectively use memory right the",
    "start": "3515520",
    "end": "3521839"
  },
  {
    "text": "name of the game in parallelism is you always want to use up all of your memory because that's one of your resources right you don't want to paralyze more",
    "start": "3521839",
    "end": "3527760"
  },
  {
    "text": "than you have to cool okay excellent oh okay i did put",
    "start": "3527760",
    "end": "3533599"
  },
  {
    "text": "the ablation in here yeah so this is exactly what happens to the question of what happens if you don't do you know um",
    "start": "3533599",
    "end": "3539680"
  },
  {
    "text": "uh expert balancing loss i think the the great picture to see is this bottom left one if you don't do load balancing you",
    "start": "3539680",
    "end": "3545520"
  },
  {
    "text": "know what are the tokens assigned to which expert you see the pink and the yellow expert they just like kind of",
    "start": "3545520",
    "end": "3550960"
  },
  {
    "text": "take over they take up you know about 50% of the tokens all the other experts are dead they do nothing right and so",
    "start": "3550960",
    "end": "3557280"
  },
  {
    "text": "you've wasted you know the majority of your experts at this point you know uh six out of eight of your experts um and",
    "start": "3557280",
    "end": "3562880"
  },
  {
    "text": "you've created a two experte unintentionally and you know that gives you uh you know worse losses up seen up",
    "start": "3562880",
    "end": "3570559"
  },
  {
    "text": "on the top right the teal lines um of course maybe that's still better than the dense model because at least you've",
    "start": "3570559",
    "end": "3575680"
  },
  {
    "text": "got two experts going um but you could have done better right counterfactually",
    "start": "3575680",
    "end": "3581720"
  },
  {
    "text": "speaking okay so um i won't go quite as deep as i could into the system side",
    "start": "3581720",
    "end": "3587680"
  },
  {
    "text": "because i haven't really started to cover you know the core systems concepts necessary for you to deeply appreciate a",
    "start": "3587680",
    "end": "3594480"
  },
  {
    "text": "lot of the parallelism concerns like you know basically the hierarchy of communication speeds in a data center",
    "start": "3594480",
    "end": "3599680"
  },
  {
    "text": "and so on um but really as i said before you know one thing to keep in mind is just how nicely can fit into devices you",
    "start": "3599680",
    "end": "3608240"
  },
  {
    "text": "you know the the thing that people say is expert parallel you know that involves sending or putting one or a few",
    "start": "3608240",
    "end": "3615599"
  },
  {
    "text": "um you know uh experts onto each device and what happens when you you know are",
    "start": "3615599",
    "end": "3620640"
  },
  {
    "text": "basically processing a token well you would hit the router and after the router you now have picked few experts",
    "start": "3620640",
    "end": "3628000"
  },
  {
    "text": "and so now you would have a collective communication call like all to all communication dispatch that would send",
    "start": "3628000",
    "end": "3633359"
  },
  {
    "text": "the tokens to the relevant devices you know the feed fors would compute um you know their outputs and then you would",
    "start": "3633359",
    "end": "3640240"
  },
  {
    "text": "return the tokens um to sort of where they belong or you would you know combine i guess multiple experts and so",
    "start": "3640240",
    "end": "3646319"
  },
  {
    "text": "you would need another sort of collective communication call and so if your your feed forward computations are",
    "start": "3646319",
    "end": "3651440"
  },
  {
    "text": "sort of big and beefy enough you can kind of pay for the cost of basically doing this expert parallelism um and the",
    "start": "3651440",
    "end": "3658720"
  },
  {
    "text": "one of the the thing that's nice about this is that it's another form of uh parallelism in your in your toolkit so",
    "start": "3658720",
    "end": "3664720"
  },
  {
    "text": "you've got on the right side you know you know data parallelism model parallelism of you know two or three",
    "start": "3664720",
    "end": "3670559"
  },
  {
    "text": "different kinds and then you've got expert parallelism and you can combine all of them to come up with sort of ways",
    "start": "3670559",
    "end": "3676160"
  },
  {
    "text": "of trading off all the resources you have so the communication speed the amount of data that you have your batch",
    "start": "3676160",
    "end": "3681280"
  },
  {
    "text": "size um and your your um number of experts and your memory so um i'm not",
    "start": "3681280",
    "end": "3686480"
  },
  {
    "text": "going to go into too much detail about how specifically this is going to help but keep in mind that this gives you",
    "start": "3686480",
    "end": "3691839"
  },
  {
    "text": "another sort of tool in your expert toolkit another thing um that is also you know",
    "start": "3691839",
    "end": "3699599"
  },
  {
    "text": "useful is let's say you have multiple experts um on a single device you know",
    "start": "3699599",
    "end": "3704799"
  },
  {
    "text": "you might hope that because the computations are sparse like let's say you know token one this first token you",
    "start": "3704799",
    "end": "3711920"
  },
  {
    "text": "know gets multiplied to export zero the second one is expert one and this third one's expert two so this is really three",
    "start": "3711920",
    "end": "3717440"
  },
  {
    "text": "matrix multiplies that are small and sparse and you might hope that modern gpus can sort of take advantage of these",
    "start": "3717440",
    "end": "3724880"
  },
  {
    "text": "kinds of you know complex uh these kinds of sparse matrix multiplications um and",
    "start": "3724880",
    "end": "3730720"
  },
  {
    "text": "that's exactly right so if you you know lay out your your sort of experts correctly and the weights are sort of",
    "start": "3730720",
    "end": "3736160"
  },
  {
    "text": "fused in the right way then modern sort of sparse matrix multiply sort of engines can sort of effectively make",
    "start": "3736160",
    "end": "3743040"
  },
  {
    "text": "sure that you're not wasting any flops in doing this one big matrix multiply so so modern libraries like meta mega",
    "start": "3743040",
    "end": "3749040"
  },
  {
    "text": "blocks can basically take advantage of this you know device level sort of sparity support to do multiple expert",
    "start": "3749040",
    "end": "3755440"
  },
  {
    "text": "computations sort of all at once so this is yet another advantage that you get um",
    "start": "3755440",
    "end": "3761640"
  },
  {
    "text": "with um so one fun side thing um which",
    "start": "3761640",
    "end": "3767280"
  },
  {
    "text": "maybe isn't mysterious to you all anymore because you've sort of grown up in the era of gpt4 um but when the gpt4",
    "start": "3767280",
    "end": "3774480"
  },
  {
    "text": "api first came out it was kind of mysterious to me because when you set the temperature to zero you know you",
    "start": "3774480",
    "end": "3780640"
  },
  {
    "text": "kind of got different responses even though it was supposed to be deterministic um and lots of people",
    "start": "3780640",
    "end": "3786000"
  },
  {
    "text": "speculated about why would that be um i'm not saying this is the the answer to that reason um but there is actually a",
    "start": "3786000",
    "end": "3793119"
  },
  {
    "text": "interesting source of randomness in right so in movies think about you know what happens you're going to route your",
    "start": "3793119",
    "end": "3798799"
  },
  {
    "text": "tokens to experts right and experts live in different devices um it could be that",
    "start": "3798799",
    "end": "3804000"
  },
  {
    "text": "you know you have a lot of examples you're going to batch of course batch your queries when you're processing them and so if you've batched your queries",
    "start": "3804000",
    "end": "3810480"
  },
  {
    "text": "these tokens are going to get routed into different experts so imagine you've got you know this this uh this batch to",
    "start": "3810480",
    "end": "3816079"
  },
  {
    "text": "process and you've got a bunch of experts but for whatever reason this batch really loves expert number three",
    "start": "3816079",
    "end": "3822559"
  },
  {
    "text": "like all the tokens go to expert number three so now what happens well the device for expert number three doesn't",
    "start": "3822559",
    "end": "3828400"
  },
  {
    "text": "have enough memory to load all of those tokens um and then what happens is what",
    "start": "3828400",
    "end": "3833839"
  },
  {
    "text": "people call token dropping and this happens at training time as well you often have what's called a load factor",
    "start": "3833839",
    "end": "3839680"
  },
  {
    "text": "where you're sort of controlling the maximum number of allowed tokens and if the router just allocates too many",
    "start": "3839680",
    "end": "3845039"
  },
  {
    "text": "tokens to an expert you just drop those tokens off either for systems reasons or because you're just worried that that",
    "start": "3845039",
    "end": "3851440"
  },
  {
    "text": "expert is going to take over at least in the training time and so now this token has gotten dropped and it's not going to",
    "start": "3851440",
    "end": "3857520"
  },
  {
    "text": "get anything at all like the mlp is just going to do a zero computation and the residual connection is just going to pass things straight forward um and then",
    "start": "3857520",
    "end": "3864240"
  },
  {
    "text": "you're going to return an output and so if your token got dropped you're going to get a different result than if your token didn't get dropped and so based on",
    "start": "3864240",
    "end": "3871039"
  },
  {
    "text": "you know who else is in your batch can induce stochasticity both at training time and inference time which is like",
    "start": "3871039",
    "end": "3877280"
  },
  {
    "text": "kind of an interesting thing that you don't normally think about because you almost never think about like cross batch effects um when uh doing",
    "start": "3877280",
    "end": "3885000"
  },
  {
    "text": "inference okay so that's kind of the the main bits of of you know the the main",
    "start": "3885000",
    "end": "3891119"
  },
  {
    "text": "basic components of of building the fun side thing if you were to actually go out tomorrow and trying to train ane um",
    "start": "3891119",
    "end": "3898720"
  },
  {
    "text": "i think the system side will make you a little bit sad but the other thing that would make you sad is probably the stability side of things um so kind of",
    "start": "3898720",
    "end": "3907520"
  },
  {
    "text": "have this property that sometimes they'll just kind of blow up on you if you try to fine-tune them they're very",
    "start": "3907520",
    "end": "3912799"
  },
  {
    "text": "difficult to fine-tune and they'll sometimes blow up on you um and so you know um barrett zoff and others um",
    "start": "3912799",
    "end": "3920480"
  },
  {
    "text": "really studied they had a whole paper on basically trying to make more stable and there's a paper which is the one i'm i'm",
    "start": "3920480",
    "end": "3927039"
  },
  {
    "text": "referencing here um whose entire purpose is to stabilize training and there's a",
    "start": "3927039",
    "end": "3932799"
  },
  {
    "text": "couple tricks that i'll i'll mention um that i think are relevant and that people do um the first one is you know",
    "start": "3932799",
    "end": "3939760"
  },
  {
    "text": "if if you're doing the router softmax so this goes back to last lecture about stability right like what did i say",
    "start": "3939760",
    "end": "3944799"
  },
  {
    "text": "about stability well the thing to be afraid of is the soft maxes right the softmax is always where you want to be",
    "start": "3944799",
    "end": "3950319"
  },
  {
    "text": "afraid and so um so for the um they do all the computations in float 32 for the",
    "start": "3950319",
    "end": "3957280"
  },
  {
    "text": "router computations just to be safe right um and sometimes they also add the",
    "start": "3957280",
    "end": "3963039"
  },
  {
    "text": "you know an auxiliary zlos so hopefully you remember that it was just last lecture you know you do log of the sum",
    "start": "3963039",
    "end": "3968640"
  },
  {
    "text": "of the the exponentiated you know values in the softmax and you square that and you add that as an extra loss right so",
    "start": "3968640",
    "end": "3973839"
  },
  {
    "text": "this is going to keep the the normalizer values near one which is nice for stability um so this is actually one of",
    "start": "3973839",
    "end": "3979280"
  },
  {
    "text": "the places where zlos was used earlier before it got sort of more popular for training um models you can kind of see",
    "start": "3979280",
    "end": "3986000"
  },
  {
    "text": "the effects here um if you look at the losses i think the the center the second plot here is maybe a great one you know",
    "start": "3986000",
    "end": "3992799"
  },
  {
    "text": "if you remove the zloss from your router uh routing function you see these like giant loss spikes um in your validation",
    "start": "3992799",
    "end": "4000079"
  },
  {
    "text": "loss where you know the model just kind of goes a little bit crazy um for a couple iterations and then gets kind of",
    "start": "4000079",
    "end": "4005599"
  },
  {
    "text": "pulled back of course it like still trains okay but you are better off having the z loss than not having a",
    "start": "4005599",
    "end": "4010640"
  },
  {
    "text": "z-loss there is a pretty noticeable gap in the validation loss by the end here",
    "start": "4010640",
    "end": "4016680"
  },
  {
    "text": "right um other things that can happen um people you know of course you want to",
    "start": "4016680",
    "end": "4022160"
  },
  {
    "text": "fine-tune your like also rlhf your if you're going to you know ship and release it um but this turns out to be",
    "start": "4022160",
    "end": "4029359"
  },
  {
    "text": "uh kind of problematic some of the earlier work you know when people were starting to do this was back in kind of",
    "start": "4029359",
    "end": "4035440"
  },
  {
    "text": "the bert and p5 era so there was a lot of fine-tuning going on um and you know",
    "start": "4035440",
    "end": "4040799"
  },
  {
    "text": "one of the things that people saw was you know actually there's a lot of overfitting that happens if you were",
    "start": "4040799",
    "end": "4046160"
  },
  {
    "text": "kind of doing uh sparse models you see this big gap between train and val right this blue and orange line um whereas the",
    "start": "4046160",
    "end": "4052960"
  },
  {
    "text": "dense model this green and red line has a smaller train test gap um and so there was a lot of worries about overfitting",
    "start": "4052960",
    "end": "4059440"
  },
  {
    "text": "because you have these like gigantic parameter models that you're fine-tuning on small data um one of the solutions",
    "start": "4059440",
    "end": "4065119"
  },
  {
    "text": "that was proposed at the time i don't think this is very popular um as far as i understand is to architect yours such",
    "start": "4065119",
    "end": "4072319"
  },
  {
    "text": "that not every layer is layer but you like let's say alternate dense layers and layers then you can just fine-tune",
    "start": "4072319",
    "end": "4078720"
  },
  {
    "text": "the dense layers and then that will that will still be fine right that behaves just like a dense model um so that was",
    "start": "4078720",
    "end": "4084319"
  },
  {
    "text": "fine another solution um the one that we saw in the the deepseek moe paper is just kind of use a lot of data like if",
    "start": "4084319",
    "end": "4090559"
  },
  {
    "text": "overfitting is a problem you know we have access to lots and lots of sft data just shovel all of those guys in so in",
    "start": "4090559",
    "end": "4096318"
  },
  {
    "text": "the case of uh deepseek they use 1.4 four million training examples um then",
    "start": "4096319",
    "end": "4101600"
  },
  {
    "text": "maybe you're you're not quite as worried about these overfitting concerns um the",
    "start": "4101600",
    "end": "4107120"
  },
  {
    "text": "last thing i'll end with which is a is a trick in the toolkit that i that people have done and seen um is upycling and so",
    "start": "4107120",
    "end": "4115440"
  },
  {
    "text": "this idea is to take a dense model like the one over here um and then you take",
    "start": "4115440",
    "end": "4121040"
  },
  {
    "text": "your mlp and you make a bunch of copies of it um and then you maybe perturb it",
    "start": "4121040",
    "end": "4126159"
  },
  {
    "text": "and then you have your router that's initialized from scratch and then you just pretend this is and then you train",
    "start": "4126159",
    "end": "4132080"
  },
  {
    "text": "it from that point on right you just initialize thee from a dense model and this is a trick that's kind of called",
    "start": "4132080",
    "end": "4137600"
  },
  {
    "text": "upycling um and you know people have shown that if you can get it to work um",
    "start": "4137600",
    "end": "4143120"
  },
  {
    "text": "it is a very very very cost-effective way of getting right and thee is great for inference because not every mlp is",
    "start": "4143120",
    "end": "4150719"
  },
  {
    "text": "going to be active or uh at inference time right so so you're going to you might effectively get a much larger",
    "start": "4150719",
    "end": "4156000"
  },
  {
    "text": "parameter model without doing the training of a much larger parameter model um and several people have",
    "start": "4156000",
    "end": "4161600"
  },
  {
    "text": "succeeded at this um mini cpm which um i'll mention again in the scaling wall",
    "start": "4161600",
    "end": "4167040"
  },
  {
    "text": "lecture but this is a chinese open llm that basically tried to build really",
    "start": "4167040",
    "end": "4172238"
  },
  {
    "text": "good small uh language models and they succeeded at taking a dense model and upycling it into that you can see that",
    "start": "4172239",
    "end": "4178880"
  },
  {
    "text": "their numbers get significantly better in the last two rows right so the dense models to thee they get a a pretty non-trivial bump in performance um quen",
    "start": "4178880",
    "end": "4187278"
  },
  {
    "text": "uh i mentioned at the start of this lecture one of their earliest attempts ate was taking one of their dense models",
    "start": "4187279",
    "end": "4193199"
  },
  {
    "text": "and then building upcycled um and they got you know fairly significant uh",
    "start": "4193199",
    "end": "4198640"
  },
  {
    "text": "performance gains relative to sort of smaller models at the time like they got models on par with their 7b models with",
    "start": "4198640",
    "end": "4205199"
  },
  {
    "text": "a 2.7 billion parameter active model um so uh to wrap up um i want to sort of",
    "start": "4205199",
    "end": "4214880"
  },
  {
    "text": "walk through the deepsseek architecture at the very end here um and hopefully",
    "start": "4214880",
    "end": "4220159"
  },
  {
    "text": "this will give you a sense of you know the first thing i want to do is i want you to understand the deepseek v3",
    "start": "4220159",
    "end": "4225520"
  },
  {
    "text": "architecture setup and all the changes that they did um because that's an example of a modern high performance",
    "start": "4225520",
    "end": "4231760"
  },
  {
    "text": "open source system i also want you to maybe appreciate that architectures don't change that much deepseek v1 or",
    "start": "4231760",
    "end": "4237920"
  },
  {
    "text": "you know deepseek v1 um is uh you know it's not that new it's like maybe a year",
    "start": "4237920",
    "end": "4244560"
  },
  {
    "text": "and a half or something maybe two years old um and they basically nailed the",
    "start": "4244560",
    "end": "4249600"
  },
  {
    "text": "architecture at that point right so i want to see i want you to see what they changed from that very ear the earliest attempt um to their big training run so",
    "start": "4249600",
    "end": "4256800"
  },
  {
    "text": "this is the the very first starting point this is um deepseek i'm calling it v1 but actually you know probably the",
    "start": "4256800",
    "end": "4262560"
  },
  {
    "text": "right way to refer to it is deepseek it's a 16 billion parameter model with 2.8 of those parameters active um and",
    "start": "4262560",
    "end": "4269600"
  },
  {
    "text": "you've seen already this diagram over here this is the um shared two shared plus 64 fine grained um experts um of",
    "start": "4269600",
    "end": "4278239"
  },
  {
    "text": "which uh four of them are active at a time or maybe about six of them are active at a time sorry um and the",
    "start": "4278239",
    "end": "4284640"
  },
  {
    "text": "routing you know you've already seen this i presented this in the in the middle of the lecture here this is the very standard top k routing uh where the",
    "start": "4284640",
    "end": "4291120"
  },
  {
    "text": "softmax is at the bottom before the the top k selection um and for balancing right at training time all they do is to",
    "start": "4291120",
    "end": "4297679"
  },
  {
    "text": "add this auxiliary loss balancing term right both the expert and device level balancing terms right so hopefully you",
    "start": "4297679",
    "end": "4303679"
  },
  {
    "text": "know you remember those from earlier so that's the the v1 um and then um they",
    "start": "4303679",
    "end": "4309040"
  },
  {
    "text": "saw how sort of effective their model was so i guess to add some more context right deepseeek originally had a dense",
    "start": "4309040",
    "end": "4316480"
  },
  {
    "text": "model and then they had ae model and thee model was remarkably good and so when they went to v2 um they went",
    "start": "4316480",
    "end": "4323600"
  },
  {
    "text": "straight to thee and now this is a 236 billion parameter model of which 21 of",
    "start": "4323600",
    "end": "4329679"
  },
  {
    "text": "those billion parameters are active right so you need a lot of memory but your your flops consumption for for",
    "start": "4329679",
    "end": "4335440"
  },
  {
    "text": "inferencing this model is not so bad now um the architecture is identical i copied literally the the same figure",
    "start": "4335440",
    "end": "4341920"
  },
  {
    "text": "because the architecture is literally the same minus changes to the number of you know experts that are active um and",
    "start": "4341920",
    "end": "4350880"
  },
  {
    "text": "uh we've got now sort of some new things happening but not too many new things so",
    "start": "4350880",
    "end": "4356320"
  },
  {
    "text": "the top selector is the same so the equation from before this previous equation this is identical this this is",
    "start": "4356320",
    "end": "4362080"
  },
  {
    "text": "still how they do things um but they have this very clever trick that they add on and this is um you know i was",
    "start": "4362080",
    "end": "4368800"
  },
  {
    "text": "going to say at the very beginning you know what's the drawback of having fine grained experts why can't i have i don't know uh 1024 fine- grained experts or",
    "start": "4368800",
    "end": "4375920"
  },
  {
    "text": "2046 fine grained experts well the problem is when you shard your experts very finely and you have a lot of active",
    "start": "4375920",
    "end": "4382239"
  },
  {
    "text": "experts right you're going to have to route to those experts right so your communication costs potentially grow and",
    "start": "4382239",
    "end": "4387760"
  },
  {
    "text": "if you're very fragmented you might have to send a lot of tokens to a lot of devices right and so the clever thing",
    "start": "4387760",
    "end": "4394320"
  },
  {
    "text": "they come up with is to say i'm not just going to you know at for each batch route to the top k exports naively which",
    "start": "4394320",
    "end": "4401520"
  },
  {
    "text": "might force me to send my tokens to lots of devices what i'm going to do is i'm going to first pick top m devices right",
    "start": "4401520",
    "end": "4408800"
  },
  {
    "text": "so i'm going to do my you know normal scoring calculation but i'm first going to sort of subset the set of allowed",
    "start": "4408800",
    "end": "4414480"
  },
  {
    "text": "devices to top m right and once i've picked my devices then i'm going to pick top k for each token within each device",
    "start": "4414480",
    "end": "4420880"
  },
  {
    "text": "right so so now i've restricted the devices this really controls the communication cost and now this gives",
    "start": "4420880",
    "end": "4426560"
  },
  {
    "text": "you more efficient training when you're scaling up to these gigantic sizes right you need to start really engaging with",
    "start": "4426560",
    "end": "4431679"
  },
  {
    "text": "the systems aspect of things when you're training a 236 billion parameter model the other thing which reflects the",
    "start": "4431679",
    "end": "4438320"
  },
  {
    "text": "systems concerns that are necessary at this scale is that they add a communication balancing loss um one way",
    "start": "4438320",
    "end": "4444800"
  },
  {
    "text": "of thinking about things is you know for an expert there's kind of inputs and outputs right the inputs are you know",
    "start": "4444800",
    "end": "4449840"
  },
  {
    "text": "the the token comes in and you route to your expert and the outputs are you know you have to kind of bring the tokens",
    "start": "4449840",
    "end": "4454960"
  },
  {
    "text": "back where they belong right so if a batch belongs on this device it has to go back where the original device was so",
    "start": "4454960",
    "end": "4460480"
  },
  {
    "text": "we have to think about both the input communication cost and the output communication cost and so they add a balancing loss to try to balance out the",
    "start": "4460480",
    "end": "4467360"
  },
  {
    "text": "output communication cost as well not just the sort of input side um so that's a minor note but you can kind of see",
    "start": "4467360",
    "end": "4472800"
  },
  {
    "text": "their attention to detail on trying to make sure all the different sort of systems aspects are properly um taken",
    "start": "4472800",
    "end": "4479360"
  },
  {
    "text": "care of and then finally we kind of get to to the you know the big deepseek uh",
    "start": "4479360",
    "end": "4484719"
  },
  {
    "text": "v3 sorry that should say v3 not v2 up there 671 billion parameters of which uh",
    "start": "4484719",
    "end": "4490239"
  },
  {
    "text": "37 are active you know once again um you know exactly the same figure because thee architecture itself doesn't change",
    "start": "4490239",
    "end": "4496960"
  },
  {
    "text": "that's stayed the same since deepseek moe right like if it works don't change it um they do change a couple things um",
    "start": "4496960",
    "end": "4502960"
  },
  {
    "text": "maybe they were you know hearing you all say why don't you normalize to one and so you know they've normalized the gate",
    "start": "4502960",
    "end": "4508239"
  },
  {
    "text": "to one they've moved kind of the softmax normalizer operation up there um but they're not actually exponentiating um",
    "start": "4508239",
    "end": "4514400"
  },
  {
    "text": "sort of the the sort of gating decisions they're actually taking sigmoids um which is a sort of softer sort of more",
    "start": "4514400",
    "end": "4520159"
  },
  {
    "text": "nicely behaved operation um you know than the soft max so they they've got some changes here but conceptually this",
    "start": "4520159",
    "end": "4526960"
  },
  {
    "text": "is still the same as the top k routing decision right you hopefully see very very similar things happening and then",
    "start": "4526960",
    "end": "4532960"
  },
  {
    "text": "in terms of the losses um they've gone to this auxiliary loss-free trick of",
    "start": "4532960",
    "end": "4538080"
  },
  {
    "text": "this b of i being incremented or decremented based on the expert load and then they have a sequence-wise auxiliary",
    "start": "4538080",
    "end": "4543920"
  },
  {
    "text": "loss um and just to you know add some context why would you want to balance different um uh experts on a single",
    "start": "4543920",
    "end": "4551199"
  },
  {
    "text": "sequence well the thing that they're very concerned about is at training time you know it's fine to to not have a",
    "start": "4551199",
    "end": "4557520"
  },
  {
    "text": "sequence-wise balancing loss but at inference time it might be the case that someone sends you very out of",
    "start": "4557520",
    "end": "4562719"
  },
  {
    "text": "distribution sequences and that might overwhelm certain experts right so at inference time you can't control which",
    "start": "4562719",
    "end": "4568080"
  },
  {
    "text": "sequences you get so you might want sort of stronger balancing that operates at a single sequence level rather than",
    "start": "4568080",
    "end": "4574560"
  },
  {
    "text": "overall batch level okay and then in the oh sorry yes",
    "start": "4574560",
    "end": "4580080"
  },
  {
    "text": "um does 3 still do like the top m devices like does it keep the b2",
    "start": "4580080",
    "end": "4587679"
  },
  {
    "text": "improvement yeah they keep the top m improvement they do not keep uh for example the communication loss so so",
    "start": "4587679",
    "end": "4593760"
  },
  {
    "text": "they've they've jettisoned some things but top m is a i mean it seems like a pretty clever idea they keep it",
    "start": "4593760",
    "end": "4600800"
  },
  {
    "text": "yeah yeah but it's not like they they always add things they have removed some of the things um and so in the last two",
    "start": "4601000",
    "end": "4608800"
  },
  {
    "text": "or so minutes uh of the class i'm going to go over the none parts of deepseek v3",
    "start": "4608800",
    "end": "4614239"
  },
  {
    "text": "because i think you know we're already at the point where i've explained most of deepseek v3 i might as well go through the the steps of explaining the",
    "start": "4614239",
    "end": "4620159"
  },
  {
    "text": "rest of deepseek v3 at this point so you all know kind of how that works so um they have a clever sort of optimization",
    "start": "4620159",
    "end": "4627679"
  },
  {
    "text": "for the attention piece called mla or multi head latent attention and um you all actually already know all the",
    "start": "4627679",
    "end": "4634320"
  },
  {
    "text": "ingredients that you need to understand this because at the end of last lecture you know i talked about like gqa and mha",
    "start": "4634320",
    "end": "4639760"
  },
  {
    "text": "right so those are all inference optimizations that you need in order to optimize the size of the kv catch so the",
    "start": "4639760",
    "end": "4646080"
  },
  {
    "text": "deepse folks take a different tac or different approach at optimizing this instead of reducing the number of heads",
    "start": "4646080",
    "end": "4652560"
  },
  {
    "text": "um they're actually going to sort of project the heads into a lower dimensional space so you have your inputs h of t and instead of sort of",
    "start": "4652560",
    "end": "4659280"
  },
  {
    "text": "generating the k's and v's directly from these h of t's what i'm going to do is i'm going to first generate a",
    "start": "4659280",
    "end": "4664880"
  },
  {
    "text": "lowdimensional c this you can think of this as like a you know compressed version of h and this c is going to be",
    "start": "4664880",
    "end": "4671920"
  },
  {
    "text": "smaller and easier to cache and i'm just going to cach these c's and whenever i need you know these k's and v's well i",
    "start": "4671920",
    "end": "4678239"
  },
  {
    "text": "can sort of up project from this kv sort of conceptually speaking and then you know i can take the inner products with",
    "start": "4678239",
    "end": "4684159"
  },
  {
    "text": "the q's right so you can kind of see how this would be a kv cache savings if i only have to save the c instead of the",
    "start": "4684159",
    "end": "4690880"
  },
  {
    "text": "higher dimensional h of t um and that's exactly the idea so you take your h of t",
    "start": "4690880",
    "end": "4697040"
  },
  {
    "text": "you project it into a lower dimensional c and then you up project this back into the k's and v's right and if the c's are",
    "start": "4697040",
    "end": "4704320"
  },
  {
    "text": "small well that's you've compressed the kv cache that's good um and then you know in terms of the computation right",
    "start": "4704320",
    "end": "4710960"
  },
  {
    "text": "if you're thinking about flops well you might think well this is not good because i have to multiply an extra",
    "start": "4710960",
    "end": "4716000"
  },
  {
    "text": "matrix w u k right i didn't have this matrix before that's an extra matrix multiply that i have to pay for but kind",
    "start": "4716000",
    "end": "4723040"
  },
  {
    "text": "of the clever thing here is remember that on the other side of k right i'm going to take k q right there that q.k",
    "start": "4723040",
    "end": "4730640"
  },
  {
    "text": "is going to be an inner product in the attention operation right and q itself has a projection matrix q and so the",
    "start": "4730640",
    "end": "4737360"
  },
  {
    "text": "trick here is you can merge this w u k and this q matrix together into one matrix so i haven't gotten any extra",
    "start": "4737360",
    "end": "4744080"
  },
  {
    "text": "matrix multiplies i've just merged this new matrix multiply into my other one right this is you know just associivity",
    "start": "4744080",
    "end": "4750239"
  },
  {
    "text": "um i can just merge the two um they also compress the queries for memory savings during training but really that one is",
    "start": "4750239",
    "end": "4756400"
  },
  {
    "text": "is not quite as necessary because it doesn't interact at all with the kv cache um i'm only going to mention this",
    "start": "4756400",
    "end": "4762960"
  },
  {
    "text": "last one in passing um because it is a subtlety but it's kind of a clever subtlety that you realize which is that",
    "start": "4762960",
    "end": "4769520"
  },
  {
    "text": "this original trick this sort of thing that i just described at the top is not compatible with rope right and the",
    "start": "4769520",
    "end": "4775920"
  },
  {
    "text": "reason is because you know the rope matrices you know basically you have the q's and the k's and you rotate each of",
    "start": "4775920",
    "end": "4782400"
  },
  {
    "text": "those q's and the k's by multiplying with a rotation matrix rq and rk but if you do that then these rqs and rks are",
    "start": "4782400",
    "end": "4789520"
  },
  {
    "text": "in between the query projection and this up uh latent vector up projection matrix",
    "start": "4789520",
    "end": "4794719"
  },
  {
    "text": "and since i can't reorder these matrix multiplies you know rope kind of gets in the way and they still have a a solution",
    "start": "4794719",
    "end": "4800800"
  },
  {
    "text": "of basically um doing rope on non-compressed dimensions that's kind of a side point i think it's not quite as",
    "start": "4800800",
    "end": "4807040"
  },
  {
    "text": "important you can kind of look at the paper if you're super interested the other thing that they do and this is the last thing i promise is that they have a",
    "start": "4807040",
    "end": "4813600"
  },
  {
    "text": "a minor change in their loss function called mtp where they predict multiple",
    "start": "4813600",
    "end": "4818719"
  },
  {
    "text": "tokens um in parallel and so what they can do is normally right you have your inputs you shift them to the to the left",
    "start": "4818719",
    "end": "4825679"
  },
  {
    "text": "by one so you're predicting one token in the future and then your transformer is going to predict all those tokens right that's your normal transformer loss but",
    "start": "4825679",
    "end": "4833600"
  },
  {
    "text": "then what you can do is right before you make those predictions you can take you know the hidden state you can pass it to",
    "start": "4833600",
    "end": "4840080"
  },
  {
    "text": "a very lightweight one layer transformer and that model can predict you know one",
    "start": "4840080",
    "end": "4845199"
  },
  {
    "text": "token in the future right so now the model is not just predicting the next token it's predicting the two tokens",
    "start": "4845199",
    "end": "4850560"
  },
  {
    "text": "into the future right so that hopefully all makes sense um and this is just a small lightweight model that can do that",
    "start": "4850560",
    "end": "4856719"
  },
  {
    "text": "um you can sort of see the architecture right here um the one thing that is is kind of disappointing that i learned as",
    "start": "4856719",
    "end": "4862480"
  },
  {
    "text": "i was sort researching for this lecture is actually they only do mtp with one token ahead so even though they have this very complicated diagram of how",
    "start": "4862480",
    "end": "4869120"
  },
  {
    "text": "they could do it for many tokens um turns out it's only done for for one token okay so now i'm all done um are",
    "start": "4869120",
    "end": "4877760"
  },
  {
    "text": "kind of now at the core of how you would build and deploy you know a really high performance large scale system and they",
    "start": "4877760",
    "end": "4884239"
  },
  {
    "text": "take advantage of of kind of the sparsity idea that you don't need all of the uh parameters all the time and",
    "start": "4884239",
    "end": "4891120"
  },
  {
    "text": "discrete routing is the real big challenge and this is i think one of the big reasons why didn't immediately catch",
    "start": "4891120",
    "end": "4896159"
  },
  {
    "text": "on it's very scary to have to try to optimize this top k routing decisions um but heristics somehow seem to work right",
    "start": "4896159",
    "end": "4902640"
  },
  {
    "text": "like they they just do and so there's a lot of empirical evidence now that at least for for uh flop constraint",
    "start": "4902640",
    "end": "4909120"
  },
  {
    "text": "settings is just a good idea it's cost effective um you should do it so definitely worth learning um thanks a",
    "start": "4909120",
    "end": "4915679"
  },
  {
    "text": "lot uh for listening",
    "start": "4915679",
    "end": "4920199"
  }
]