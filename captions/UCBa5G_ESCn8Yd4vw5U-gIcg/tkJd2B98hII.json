[
  {
    "start": "0",
    "end": "5510"
  },
  {
    "text": "So I guess, yeah, sorry\nfor the delay a little bit. I couldn't find water somehow. ",
    "start": "5510",
    "end": "12950"
  },
  {
    "text": "Anyway, so but OK,\nlet's get started. So last time we talked about\nconcentration equality,",
    "start": "12950",
    "end": "21680"
  },
  {
    "text": "which was for some preparations\nfor what we need today or maybe",
    "start": "21680",
    "end": "27710"
  },
  {
    "text": "the next lecture. And today, we are\ngoing to go back to the uniform convergence. So recall that our goal was\nto do a uniform convergence.",
    "start": "27710",
    "end": "37910"
  },
  {
    "text": " And we have proved some results. For example, we have\nproved that, I guess,",
    "start": "37910",
    "end": "46760"
  },
  {
    "text": "the first thing\nwe had is that we say the excess risk is bounded\nby this uniform convergence.",
    "start": "46760",
    "end": "52290"
  },
  {
    "text": "So we basically\ncare about something like the sup of the differences.",
    "start": "52290",
    "end": "58400"
  },
  {
    "start": "58400",
    "end": "65349"
  },
  {
    "text": "So guys, we have shown\nthat the excess risk-- ",
    "start": "65349",
    "end": "73100"
  },
  {
    "text": "I don't know why I'm-- something wrong with my pen? Excess risk, this is\nbounded by, for example,",
    "start": "73100",
    "end": "80820"
  },
  {
    "text": "something like L h star minus L\nhat h star plus sup L h minus L",
    "start": "80820",
    "end": "90820"
  },
  {
    "text": "hat h, where h is\ncapital H. And we have used this to get a certain\nkind of uniform convergence",
    "start": "90820",
    "end": "100100"
  },
  {
    "text": "result. For example, we have shown that\nfor finite hypothesis class,",
    "start": "100100",
    "end": "108400"
  },
  {
    "text": "we've got L h hat\nminus L of h star.",
    "start": "108400",
    "end": "116160"
  },
  {
    "text": "This is bounded by-- I guess we have shown\nthis technically. But this can be turned\ninto excess risk bound.",
    "start": "116160",
    "end": "123130"
  },
  {
    "text": "We have shown this\nis less than--  sorry.",
    "start": "123130",
    "end": "130340"
  },
  {
    "text": "This is less than\nsquare root ln h,",
    "start": "130340",
    "end": "136080"
  },
  {
    "text": "roughly speaking over n, if\nyou ignore other log factors",
    "start": "136080",
    "end": "141570"
  },
  {
    "text": "and you can take sup\nover h and capital H. And also, for hypothesis\nclass parameterized by class",
    "start": "141570",
    "end": "152860"
  },
  {
    "text": "with p parameters, we\nalso have got something",
    "start": "152860",
    "end": "158810"
  },
  {
    "text": "like L theta hat minus L hat. Sorry, I think there's\nsomething wrong with my note.",
    "start": "158810",
    "end": "167420"
  },
  {
    "text": "Let me take a quick note here.",
    "start": "167420",
    "end": "173760"
  },
  {
    "text": "So we can get L sup theta\nin capital theta, L theta",
    "start": "173760",
    "end": "179810"
  },
  {
    "text": "minus L hat theta. This is bounded by something\nlike O tilde square p over n.",
    "start": "179810",
    "end": "185750"
  },
  {
    "text": "This is what we did\ntwo lectures ago. So and you can think\nof this, I guess",
    "start": "185750",
    "end": "192590"
  },
  {
    "text": "we have discussed this briefly. So this quantity,\nand this quantity, these are in some sense\ncomplex dimensions",
    "start": "192590",
    "end": "203090"
  },
  {
    "text": "of the hypothesis class. And this is generally\nthe type of results",
    "start": "203090",
    "end": "208157"
  },
  {
    "text": "that we're going to get. So we're going to have\nsomething that decreases as n goes to infinity.",
    "start": "208158",
    "end": "213770"
  },
  {
    "text": "And also, there is\nanother constant, which is, there's another factor,\nwhich is the hypothesis class,",
    "start": "213770",
    "end": "220100"
  },
  {
    "text": "right? So basically,\neventually you can say that if n is bigger than\nthe hypothesis class, then you can get\nnon-trivial error bound.",
    "start": "220100",
    "end": "228930"
  },
  {
    "text": "So the problem with these\ntwo bound is the following. So the limitation\nI think you can",
    "start": "228930",
    "end": "238680"
  },
  {
    "text": "talk about different limitations\nfrom different perspectives. But I think the basic limitation\nof this p parameter bound here",
    "start": "238680",
    "end": "246989"
  },
  {
    "text": "is that it requires n\nto be much bigger than p so that this bound is small.",
    "start": "246990",
    "end": "255209"
  },
  {
    "text": "So and this is not necessarily\nfeasible in many cases. And also, this is not really\nwhat happens in reality, right?",
    "start": "255210",
    "end": "262620"
  },
  {
    "text": "So in reality, in many\ncases, n is smaller than p is quite often.",
    "start": "262620",
    "end": "269305"
  },
  {
    "text": " It's not always the case,\nbut it's pretty often. It's more often in\na modern situation",
    "start": "269305",
    "end": "278320"
  },
  {
    "text": "where you have a so-called\noverparameterized neural network. By overparameterized I\nguess I would define that",
    "start": "278320",
    "end": "283750"
  },
  {
    "text": "more carefully in the later\ncourse, in the later lectures. But basically, in\nthe modern setting",
    "start": "283750",
    "end": "290230"
  },
  {
    "text": "when you have a\ndeep neural network, your ImageNet has\na million examples. But your parameters could be\nsomething like 10 million,",
    "start": "290230",
    "end": "297820"
  },
  {
    "text": "or maybe 100 million,\nsometimes could be billions. So of course, this is not\nnecessarily always the case.",
    "start": "297820",
    "end": "304030"
  },
  {
    "text": "Where sometimes you still\nhave n is bigger than p depending on the situation. But generally, people\nfound that it's",
    "start": "304030",
    "end": "310180"
  },
  {
    "text": "useful to make your\nnetwork, your p very large. So definitely it's not\nthe case that you want n",
    "start": "310180",
    "end": "315415"
  },
  {
    "text": "to be much, much bigger than p. That's definitely not true. So and the reason is sometimes\nwhy this is not capturing",
    "start": "315415",
    "end": "323270"
  },
  {
    "text": "what happens in reality is that\nthis is not precise enough, right? So not precise\nenough in the sense",
    "start": "323270",
    "end": "331139"
  },
  {
    "text": "that your complexity measure is,\nin some sense, to worst case. Your complex measure is\nmeasuring the complexity",
    "start": "331140",
    "end": "337050"
  },
  {
    "text": "of all possible parameters\nwith all possible models with p parameters. But you are not\nspecializing enough",
    "start": "337050",
    "end": "343919"
  },
  {
    "text": "to some special kind of\nmodels among all the models with p parameters.",
    "start": "343920",
    "end": "349530"
  },
  {
    "text": "For example, you cannot\nachieve your case, especially in the more\nclassical language,",
    "start": "349530",
    "end": "355020"
  },
  {
    "text": "you cannot distinguish a\nsparse parameter from a dense",
    "start": "355020",
    "end": "360460"
  },
  {
    "text": "parameter. You cannot distinguish,\nfor example, you have a parameter class where\ntheta has not one norm bound",
    "start": "360460",
    "end": "368319"
  },
  {
    "text": "versus some hypothesis class\nwhere theta has some two norm bound, right? ",
    "start": "368320",
    "end": "374840"
  },
  {
    "text": "So in either of these cases,\nyou get the parameter p, the p will be showing\nup in your bound.",
    "start": "374840",
    "end": "381360"
  },
  {
    "text": "So and but not\nnecessarily the b, the control of the\nnorm of the parameters.",
    "start": "381360",
    "end": "387930"
  },
  {
    "text": "So that's why we are\nlooking for something that can be more precise\nthat can not depend on p",
    "start": "387930",
    "end": "397160"
  },
  {
    "text": "but depend on some more\naccurate characterization of the complexity.",
    "start": "397160",
    "end": "403849"
  },
  {
    "text": "So today and next lecture,\nand next few lectures",
    "start": "403850",
    "end": "409010"
  },
  {
    "text": "in some sense, so what\nwe are going to say,",
    "start": "409010",
    "end": "416220"
  },
  {
    "text": "our goal is to prove something\nlike L theta hat minus L hat theta hat is less than something\nlike some complexity of theta",
    "start": "416220",
    "end": "428810"
  },
  {
    "text": "and over n. And but this complexity\nmeasure could be more fine-grained than\njust a single number p.",
    "start": "428810",
    "end": "436580"
  },
  {
    "text": "And this complex measure\ncould possibly also depend on distribution. So this complexity\nmay depend even",
    "start": "436580",
    "end": "445960"
  },
  {
    "text": "on distribution, distribution p. p is the distribution\nof your data.",
    "start": "445960",
    "end": "452699"
  },
  {
    "text": "So maybe for some distribution\np your complexity is smaller. For some other distribution\np your complexity is higher.",
    "start": "452700",
    "end": "460670"
  },
  {
    "text": "And we are trying to capture\nthe intrinsic kind of difficulty of the learning problem.",
    "start": "460670",
    "end": "466290"
  },
  {
    "text": "But of course, this is\nsomewhat subjective. Because this is\ndepends a little bit",
    "start": "466290",
    "end": "472760"
  },
  {
    "text": "on what you believe that is\nhappening in the real life, right? So if you believe that the\nreal parameter is sparse,",
    "start": "472760",
    "end": "480770"
  },
  {
    "text": "then you probably should have\na complex measure that captures the L1 norm of the parameters.",
    "start": "480770",
    "end": "487790"
  },
  {
    "text": "If you believe that the\nground truth parameters have other properties,\nthen you probably",
    "start": "487790",
    "end": "493550"
  },
  {
    "text": "should use a different\ncomplexity measure. So but this is the general goal.",
    "start": "493550",
    "end": "499310"
  },
  {
    "text": "And also, the practical\nway of thinking about this is that you can think\nof the right-hand side as something that motivates\nyour regularization.",
    "start": "499310",
    "end": "508970"
  },
  {
    "text": "So the practical\nimpact, I guess, maybe,",
    "start": "508970",
    "end": "514070"
  },
  {
    "text": "or the practical\nimplication is that you can use this complexity of\ntheta as a regularization.",
    "start": "514070",
    "end": "521450"
  },
  {
    "start": "521450",
    "end": "526560"
  },
  {
    "text": "Because if you just\noptimize your model where you're going to\nfind some parameter theta, especially if you don't\nhave enough theta,",
    "start": "526560",
    "end": "533050"
  },
  {
    "text": "you may have multiple global\nminimum among the search space. So but if you know that\ncertain complexity measure",
    "start": "533050",
    "end": "540270"
  },
  {
    "text": "will make the bound better, then\nyou can actively find models with small complexity.",
    "start": "540270",
    "end": "546610"
  },
  {
    "text": "So you can use this complex\nmeasure on the right-hand side. And you add this complexity\nmeasure multiplied by lambda",
    "start": "546610",
    "end": "554160"
  },
  {
    "text": "to your tuning loss. So you get a regular loss. So that you are more likely\nto find a small complexity",
    "start": "554160",
    "end": "560550"
  },
  {
    "text": "one which generalizes better. So I guess that's\nthe basic idea.",
    "start": "560550",
    "end": "569560"
  },
  {
    "text": "And so, what we're\ngoing to do is that--",
    "start": "569560",
    "end": "577930"
  },
  {
    "text": "so we're going to-- today, the first part is\nwe're going to talk about--",
    "start": "577930",
    "end": "583139"
  },
  {
    "text": "a week ago, before we were\ntalking about the sup-- the uniform convergence. So this is our\ntool where you want",
    "start": "583140",
    "end": "589410"
  },
  {
    "text": "to prove that L h minus L hat\nh is small for all possible h.",
    "start": "589410",
    "end": "598100"
  },
  {
    "text": " And in the first\npart of the lecture,",
    "start": "598100",
    "end": "603649"
  },
  {
    "text": "we're going to bound\nthe expectation of this as some kind of a weaker goal. And in the second part of\nthe lecture, if we have time,",
    "start": "603650",
    "end": "611450"
  },
  {
    "text": "we're going to bound it\nwith high probability without expectation\nin front of it. And what's the\nexpectation comes from,",
    "start": "611450",
    "end": "617170"
  },
  {
    "text": "this randomness come from the\ndata and the training data. ",
    "start": "617170",
    "end": "623690"
  },
  {
    "text": "Because L hat depends\non the training data. And the training data\nare randomly being drawn. So that's why what's\ninside the expectation",
    "start": "623690",
    "end": "631579"
  },
  {
    "text": "is a random variable\nthat depends on the randomness of\nthe training data. And you would take expectations\nof these random variables.",
    "start": "631580",
    "end": "637910"
  },
  {
    "text": "And that's the goal. So we're going to\nbound, upper bound this with some other\nquantities that we think",
    "start": "637910",
    "end": "643460"
  },
  {
    "text": "are more intrinsic and\nconvenient for us to use. So I guess I need to start\nwith some definitions.",
    "start": "643460",
    "end": "652530"
  },
  {
    "text": " So this is a definition called\nRademacher complexity, which",
    "start": "652530",
    "end": "662210"
  },
  {
    "text": "is the main object we're going\nto focus on in this lecture. So definition is,\nlet f be a family",
    "start": "662210",
    "end": "675380"
  },
  {
    "text": "of real valued functions.",
    "start": "675380",
    "end": "681950"
  },
  {
    "text": "So far, in this definition\nf is just an abstract family of functions. And we're going to\ndefine a complexity",
    "start": "681950",
    "end": "687550"
  },
  {
    "text": "for this family of functions f. And then, we're going to\nsay what functions of f we are going to care about.",
    "start": "687550",
    "end": "692980"
  },
  {
    "text": "We care about actually the\nfunctions of the losses, the family of the losses. But for now, f is just the\nabstract family of functions.",
    "start": "692980",
    "end": "701107"
  },
  {
    "text": "And you're going to\ndefine a complex measure for this abstract\nfamily of functions. So let's say this\nfamily of functions",
    "start": "701107",
    "end": "707020"
  },
  {
    "text": "that maps some input space,\nlet's call it Z to real number.",
    "start": "707020",
    "end": "714960"
  },
  {
    "text": "And let P be a distribution\nover this input space Z.",
    "start": "714960",
    "end": "727790"
  },
  {
    "text": "Then the average,\nthe so-called-- often you don't really\nnecessarily have",
    "start": "727790",
    "end": "733360"
  },
  {
    "text": "to specify average\nRademacher complexity. But technically, it's the\naverage Rademacher complexity",
    "start": "733360",
    "end": "743290"
  },
  {
    "text": "of f is defined\nas the following.",
    "start": "743290",
    "end": "750959"
  },
  {
    "text": "So this is R and\nthen sub f, where n indicates how many\nexamples you have,",
    "start": "750960",
    "end": "757340"
  },
  {
    "text": "how many training examples,\nhow many empirical examples you have. Rnf is defined to be you\nfirst draw some examples.",
    "start": "757340",
    "end": "768620"
  },
  {
    "text": "You can think of this as\ntraining examples, iid from the distribution p.",
    "start": "768620",
    "end": "774070"
  },
  {
    "text": "And then you draw some so-called\nRademacher random variables. Recall that Rademacher\nrandom variables are just",
    "start": "774070",
    "end": "780580"
  },
  {
    "text": "binary plus 1 minus 1 uniform. You draw sigma 1 up to sigma n,\niid uniformly from minus 1, 1.",
    "start": "780580",
    "end": "792329"
  },
  {
    "text": "And then you look\nat this quantity. You look at the sup over this\nfunction class capital F.",
    "start": "792330",
    "end": "803570"
  },
  {
    "text": "And you look at the quantity\nthe average of sigma i f Zi",
    "start": "803570",
    "end": "811310"
  },
  {
    "text": "and from 1 to n. So this sounds like kind of a\npretty complicated definition.",
    "start": "811310",
    "end": "817550"
  },
  {
    "text": "But let me try to\ninterpret a little bit. So in some sense, maybe\nfirst of all, only",
    "start": "817550",
    "end": "825240"
  },
  {
    "text": "think about this\nquantity right here. Just think about\nwhat's inside this sup. So this is the correlation.",
    "start": "825240",
    "end": "831390"
  },
  {
    "start": "831390",
    "end": "836895"
  },
  {
    "text": "1 over n is just\na normalization, which is not important. This is the correlation\nbetween the outputs of f.",
    "start": "836895",
    "end": "852580"
  },
  {
    "text": "So the output of f\nis fZ1 up to fZn,",
    "start": "852580",
    "end": "858640"
  },
  {
    "text": "and some random variable\nsigma 1 and sigma n. Of course, if you\njust look at this,",
    "start": "858640",
    "end": "865649"
  },
  {
    "text": "a correlation should be\ntypically very close to 0 because you shouldn't correlate\nwith random variables.",
    "start": "865650",
    "end": "871150"
  },
  {
    "text": "But there is a sup, right? So you are first joining\nthe sigma 1 up to sigma n, and then you take a sup over f.",
    "start": "871150",
    "end": "878110"
  },
  {
    "text": "So basically, you are saying\nthat what's the maximal-- so basically, this whole thing\nis the maximal correlation",
    "start": "878110",
    "end": "886220"
  },
  {
    "text": "between f, the output of f,\nand sigma 1 up to sigma m",
    "start": "886220",
    "end": "891689"
  },
  {
    "text": "after you draw sigma i. So you first draw sigma i, and\nthen you try to find something that correlates with sigma i's.",
    "start": "891690",
    "end": "897930"
  },
  {
    "text": "Or you try to find\nf such that it can output something that looks\nlike the random things they have drawn.",
    "start": "897930",
    "end": "905740"
  },
  {
    "text": "So in some sense, if you\nhave a high complexity,",
    "start": "905740",
    "end": "912700"
  },
  {
    "text": "means that for most\nor for almost all-- for most the binary patterns,\nbinary patterns just",
    "start": "912700",
    "end": "920530"
  },
  {
    "text": "means that you have the\nsigma 1 up to sigma n, there exist f in this hypothesis\nclass such that the output",
    "start": "920530",
    "end": "930150"
  },
  {
    "text": "on this family is similar\nto or similar or correlate",
    "start": "930150",
    "end": "942230"
  },
  {
    "text": "with a random pattern. So for any random\npattern, if you draw it,",
    "start": "942230",
    "end": "948640"
  },
  {
    "text": "then you can find post hoc a\nfunction f in this family class",
    "start": "948640",
    "end": "953830"
  },
  {
    "text": "such that the output\non Z1 up to Zn",
    "start": "953830",
    "end": "959770"
  },
  {
    "text": "looks like the random\npatterns you have drawn. So in some sense, this is saying\nthat how diverse the outputs",
    "start": "959770",
    "end": "968830"
  },
  {
    "text": "you can have from this\nfamily of functions f. So if this family of\nfunctions f can map your Z1 up",
    "start": "968830",
    "end": "975820"
  },
  {
    "text": "to Zn into any\npossible patterns, then this Rademacher\ncomplexity will be the largest.",
    "start": "975820",
    "end": "981730"
  },
  {
    "text": "So for example, suppose every\nbinary pattern can be somewhat output by this family\nof functions f on Z,",
    "start": "981730",
    "end": "989470"
  },
  {
    "text": "then you get the maximum\nRademacher complexity intuitively.",
    "start": "989470",
    "end": "995131"
  },
  {
    "text": " Any questions so far? Is this necessarily a\nnon-increasing function of n?",
    "start": "995131",
    "end": "1004170"
  },
  {
    "text": "Is this necessarily a\nnon-increasing function of n?  The question is,\nis this necessarily",
    "start": "1004170",
    "end": "1010230"
  },
  {
    "text": "a non-increasing function of n?",
    "start": "1010230",
    "end": "1015839"
  },
  {
    "text": "I think it should be. But it shouldn't be-- but I don't think\nit's trivial to see why it is non-increasing.",
    "start": "1015840",
    "end": "1027170"
  },
  {
    "text": "At least off the\ntop of my head, I don't see a super\nsimple argument. ",
    "start": "1027170",
    "end": "1034880"
  },
  {
    "text": "But I think you can prove\nit without too much effort. I think roughly speaking, how do\nyou prove it is that you can--",
    "start": "1034880",
    "end": "1043730"
  },
  {
    "text": "because you take the sup,\nright, so you somehow can--",
    "start": "1043730",
    "end": "1049000"
  },
  {
    "text": "I think you can prove\nit by switching the sup with the expectation for one--",
    "start": "1049000",
    "end": "1054880"
  },
  {
    "text": "for example, the last example. And then you've got\nthe roughly speaking the definition of the n minus\n1 version of the Rademacher",
    "start": "1054880",
    "end": "1061840"
  },
  {
    "text": "complexity. But maybe I may not\ndo it on the fly, just in case I missed\nsomething, I got stuck.",
    "start": "1061840",
    "end": "1068810"
  },
  {
    "text": "So but roughly speaking,\nI think that should work. Any other questions? ",
    "start": "1068810",
    "end": "1076110"
  },
  {
    "text": "By the way, I never got\nany questions from Zoom. So you should feel\nfree to speak up. Just unmute yourself\nand ask questions.",
    "start": "1076110",
    "end": "1083370"
  },
  {
    "text": "Sometimes I'm not even sure\nwhether the Zoom is working. Should f be mapped\nfrom minus 1 to 1?",
    "start": "1083370",
    "end": "1090890"
  },
  {
    "text": "Otherwise it would\nhave [INAUDIBLE]?? That's a great question. So f is not required to be\nmapped to plus 1 minus 1.",
    "start": "1090890",
    "end": "1100020"
  },
  {
    "text": "And it's true that this\ncan be unbounded, right? So this is actually\nsensitive to the scale of f.",
    "start": "1100020",
    "end": "1105630"
  },
  {
    "text": "If you scale f by a\nfactor of 2, then you're going to have 2 times the\nRademacher complexity.",
    "start": "1105630",
    "end": "1112870"
  },
  {
    "text": "And this is actually somewhat\nuseful in certain cases, which",
    "start": "1112870",
    "end": "1118043"
  },
  {
    "text": "we probably will\ntalk about later.  There is a question?",
    "start": "1118043",
    "end": "1123270"
  },
  {
    "text": " Cool. ",
    "start": "1123270",
    "end": "1130029"
  },
  {
    "text": "So now, let's see why\nwe all care about this. So why do we care about\nthis Rademacher complexity?",
    "start": "1130030",
    "end": "1135730"
  },
  {
    "text": "The reason is that you\nknow the following. ",
    "start": "1135730",
    "end": "1144180"
  },
  {
    "text": "So you know that-- let me write down\nwhat is the theorem.",
    "start": "1144180",
    "end": "1151565"
  },
  {
    "text": "Suppose you did this\nhypothetical experiment. You draw n examples\nfrom distribution P.",
    "start": "1151565",
    "end": "1157460"
  },
  {
    "text": "And then you look at\nthis quantity, the error",
    "start": "1157460",
    "end": "1162950"
  },
  {
    "text": "rate of f of Zi, i from 1 to\nn, minus the expectation of fZ.",
    "start": "1162950",
    "end": "1169279"
  },
  {
    "text": "This is the quantity\nwe have dealt with in the last lecture, the\nconcentration how much you",
    "start": "1169280",
    "end": "1174530"
  },
  {
    "text": "deviate from a min. But you take a sup here\nbecause you sometimes",
    "start": "1174530",
    "end": "1181429"
  },
  {
    "text": "care about the maximum\npossible deviation post-hoc after you draw the examples.",
    "start": "1181430",
    "end": "1187200"
  },
  {
    "text": "If you look at this\nquantity, then this quantity is bounded by 2 times the\nRademacher complexity of i.",
    "start": "1187200",
    "end": "1194220"
  },
  {
    "text": " I guess, to appreciate what\nthe theorem is really doing,",
    "start": "1194220",
    "end": "1200350"
  },
  {
    "text": "I guess, it's probably time to\nsay what exactly what kind of f we care about. So f so far is abstract thing.",
    "start": "1200350",
    "end": "1206690"
  },
  {
    "text": "But now let's try\nto instantiate. So suppose you take\nf, the capital F",
    "start": "1206690",
    "end": "1212230"
  },
  {
    "text": "to be the family of\nfunctions that maps Z, which is taken to be a pair of\nx and y, the input and output.",
    "start": "1212230",
    "end": "1222900"
  },
  {
    "text": "And you map it to\nthe loss function, the loss of x, y on the\nhypothesis h for any h",
    "start": "1222900",
    "end": "1232080"
  },
  {
    "text": "in the hypothesis class. So basically, this is\nthe family of losses. ",
    "start": "1232080",
    "end": "1240690"
  },
  {
    "text": "For every model, every\nmodel is a function, right? And given a model, and\nyou get a loss function",
    "start": "1240690",
    "end": "1248370"
  },
  {
    "text": "defined by the model h. So basically, this\nis the composition of the model function\nwith the loss function--",
    "start": "1248370",
    "end": "1259230"
  },
  {
    "text": "the two-dimensional loss\nfunction, the little l. So together, basically\nyou get the--",
    "start": "1259230",
    "end": "1264255"
  },
  {
    "text": "this is mapped\nfrom the data point to the loss of the data point. But you can vary what functions,\nwhat models you care about.",
    "start": "1264255",
    "end": "1272340"
  },
  {
    "text": "So you get a family of losses. So in some sense, it's\njust a slight extension",
    "start": "1272340",
    "end": "1278790"
  },
  {
    "text": "of the family of\nmodels in some sense. But here it's about the losses. And suppose you care about this.",
    "start": "1278790",
    "end": "1284730"
  },
  {
    "text": "You take f to be this. And you can see that the\nleft-hand side is exactly what we was trying to bound, right?",
    "start": "1284730",
    "end": "1293309"
  },
  {
    "text": "Just because fZi is\nloss of xi, guess",
    "start": "1293310",
    "end": "1299790"
  },
  {
    "text": "we write xi, yi like\nthis, xi, yi at h, right?",
    "start": "1299790",
    "end": "1306780"
  },
  {
    "text": "So then, the sum of-- the empirical sum is just\nthe empirical loss, right?",
    "start": "1306780",
    "end": "1313065"
  },
  {
    "text": " So 1 over n, sum of fZi,\nthis is just 1 over n times",
    "start": "1313065",
    "end": "1323790"
  },
  {
    "text": "the sum of L xi, yi h.",
    "start": "1323790",
    "end": "1329430"
  },
  {
    "text": "This is just the loss-- the empirical loss of\nthe hypothesis class-- the hypothesis h, right?",
    "start": "1329430",
    "end": "1335460"
  },
  {
    "text": "And the expectation fZi, fZ is\nthe expectation of the loss.",
    "start": "1335460",
    "end": "1341654"
  },
  {
    "start": "1341655",
    "end": "1346880"
  },
  {
    "text": "And where x and y are drawn\nfrom the distribution p. And this becomes\nthe population loss.",
    "start": "1346880",
    "end": "1355190"
  },
  {
    "text": "So that's why the left-hand\nside of this theorem is really just the sup\nover h, L hat h minus Lh.",
    "start": "1355190",
    "end": "1365912"
  },
  {
    "text": " Something like this. A quick question?",
    "start": "1365912",
    "end": "1372300"
  },
  {
    "text": "And you take expectation of\nthe randomness of the data. So that's the weaker version\nof uniform convergence",
    "start": "1372300",
    "end": "1380010"
  },
  {
    "text": "that we outline in the\nbeginning of the lecture. So and you can bound this\nby the Rademacher complexity",
    "start": "1380010",
    "end": "1388559"
  },
  {
    "text": "of this function class f,\nthe Rademacher complexity of this family of losses.",
    "start": "1388560",
    "end": "1394920"
  },
  {
    "text": " So basically, the\ntheorem is saying",
    "start": "1394920",
    "end": "1401450"
  },
  {
    "text": "that the generalization error\nis less than the Rademacher",
    "start": "1401450",
    "end": "1410120"
  },
  {
    "text": "complexity of f. I think technical expectation\nof the generalization.",
    "start": "1410120",
    "end": "1416226"
  },
  {
    "text": " And there was a question here?",
    "start": "1416227",
    "end": "1422074"
  },
  {
    "text": "Is there a sup of the absolute\nvalue of the difference? No, there is no\nabsolute value here.",
    "start": "1422074",
    "end": "1427270"
  },
  {
    "text": "Okay. Yes. There is no absolute value? There is no absolute value. That's a great question. So there is no absolute value. And it becomes a\nlittle bit trickier",
    "start": "1427270",
    "end": "1433270"
  },
  {
    "text": "if you add your absolute value. I think, if you add absolute\nvalue, first of all, you need a different proof-- a slightly different proof.",
    "start": "1433270",
    "end": "1439000"
  },
  {
    "text": "And second, you're going to\nhave a different constant. Instead of 2, you\ncan get probably 4. And the cleanest way to\ndo it is that you don't do",
    "start": "1439000",
    "end": "1446500"
  },
  {
    "text": "absolute value in this theorem. You do the absolute\nvalue in the outer layer.",
    "start": "1446500",
    "end": "1453502"
  },
  {
    "text": "Actually, you don't even\nneed absolute value actually technically. Because eventually,\nyou only cover one side of the bound when you\ndo the generalization error.",
    "start": "1453502",
    "end": "1461150"
  },
  {
    "text": "So technically, you don't even\nneed absolute value anywhere. OK? ",
    "start": "1461150",
    "end": "1471350"
  },
  {
    "text": "So and if you really\nthink about this R and f",
    "start": "1471350",
    "end": "1479110"
  },
  {
    "text": "in this context, right,\nso for this particular f,",
    "start": "1479110",
    "end": "1486720"
  },
  {
    "text": "what does it mean? It really means that how\nwell the family of losses, so",
    "start": "1486720",
    "end": "1493150"
  },
  {
    "text": "the losses of data, n data can\ncorrelate with random pattern.",
    "start": "1493150",
    "end": "1502300"
  },
  {
    "start": "1502300",
    "end": "1522970"
  },
  {
    "text": "So this is still sounds a\nlittle bit not super intuitive. We can further--\nfor simplified case,",
    "start": "1522970",
    "end": "1530030"
  },
  {
    "text": "we can further simplify\nthis a little bit. So suppose you have a\nbinary classification.",
    "start": "1530030",
    "end": "1537070"
  },
  {
    "start": "1537070",
    "end": "1549440"
  },
  {
    "text": "So suppose, let's say, y is\nbetween plus 1 and minus 1.",
    "start": "1549440",
    "end": "1554850"
  },
  {
    "text": "An L is 0, 1 loss. So L of xyh is equals\nto the indicator",
    "start": "1554850",
    "end": "1566320"
  },
  {
    "text": "of h of x is not equal to y. If they are not equal,\nyou have loss 1.",
    "start": "1566320",
    "end": "1571420"
  },
  {
    "text": "Otherwise you have loss 0. And in this case, we can further\ninterpret this a little bit",
    "start": "1571420",
    "end": "1577519"
  },
  {
    "text": "more. So what you can do\nis the following. First of all, you will write\nthis indicator into this form.",
    "start": "1577520",
    "end": "1584240"
  },
  {
    "text": "We write it as 1/2 times\ny minus 1 times h of x.",
    "start": "1584240",
    "end": "1591929"
  },
  {
    "text": "This is assuming--\nhere I'm assuming h of x is also n plus 1 minus y.",
    "start": "1591930",
    "end": "1599080"
  },
  {
    "text": "So by the way,\nwhat I'm doing here is to try to instantiate\nthis into a special case",
    "start": "1599080",
    "end": "1604289"
  },
  {
    "text": "so that you can interpret the\nRademacher complexity in a more",
    "start": "1604290",
    "end": "1609750"
  },
  {
    "text": "intuitive way. And also, this whole thing is\nalso useful by itself as well.",
    "start": "1609750",
    "end": "1616500"
  },
  {
    "text": "So when h of x is plus 1 minus\n1, then y is plus 1 minus, and also y is plus 1 minus\n1, then the indicator",
    "start": "1616500",
    "end": "1623309"
  },
  {
    "text": "that they are different,\nyou can write this as this. Because if y and\nh are different,",
    "start": "1623310",
    "end": "1628509"
  },
  {
    "text": "then you get yhx is minus 1. And then the whole\nthing will be 1. And if y and hx are the\nsame, then y times x is 1.",
    "start": "1628510",
    "end": "1636570"
  },
  {
    "text": "And then this quantity is 0. So you can just verify it. So the reason we do\nthis is we somehow",
    "start": "1636570",
    "end": "1642810"
  },
  {
    "text": "make it more linear in y and hx. And then, you can look at\nthe Rademacher complexity.",
    "start": "1642810",
    "end": "1650270"
  },
  {
    "text": "So the Rnf is this\nexpectation of sup sigma I. So",
    "start": "1650270",
    "end": "1667850"
  },
  {
    "text": "and let's plug in the loss. ",
    "start": "1667850",
    "end": "1673971"
  },
  {
    "text": "So here are the expectations. So in the definition, I\nhave two expectations. So but now, I put two of\nthe expectations into one,",
    "start": "1673972",
    "end": "1681640"
  },
  {
    "text": "just you merge them. So that the randomness\ncome from both the data and the Rademacher pattern.",
    "start": "1681640",
    "end": "1686910"
  },
  {
    "text": "And you get sup over h and h.  So you plug in this formula\n1/2 times 1 minus yi hxi times",
    "start": "1686910",
    "end": "1701890"
  },
  {
    "text": "sigma i.  And now, let's do\nsome rearrangements.",
    "start": "1701890",
    "end": "1709660"
  },
  {
    "text": "It's a very simple\nrearrangement. ",
    "start": "1709660",
    "end": "1720600"
  },
  {
    "text": "Plus 1 over n times 1/2 sigma i.",
    "start": "1720600",
    "end": "1726380"
  },
  {
    "text": " So here, this quantity\nit's inside the sup.",
    "start": "1726380",
    "end": "1736149"
  },
  {
    "text": "But actually, it's a constant\nthat doesn't depend on h. So you can put it\noutside of the sup.",
    "start": "1736150",
    "end": "1742090"
  },
  {
    "text": "So you can technically\nwrite this, just because this sum of\nsigma i is a constant.",
    "start": "1742090",
    "end": "1748180"
  },
  {
    "text": "And then, because now you\ncan switch the expectation",
    "start": "1748180",
    "end": "1753670"
  },
  {
    "text": "with the sum and get expectation\nsup of the first term",
    "start": "1753670",
    "end": "1768420"
  },
  {
    "text": "and plus the expectation of\nthis 1 over 2n sum of sigma i.",
    "start": "1768420",
    "end": "1774510"
  },
  {
    "text": "And this term becomes 0.  Oops. ",
    "start": "1774510",
    "end": "1782390"
  },
  {
    "text": "And this thing becomes 0\nbecause the expectation of the Rademacher variable is 0.",
    "start": "1782390",
    "end": "1788770"
  },
  {
    "text": "And so, then we're only left\nwith the first quantity. And if you look at\nthe first quantity,",
    "start": "1788770",
    "end": "1795250"
  },
  {
    "text": "then you realize that-- so sigma i is a random variable. So h yi sigma i has the same\ndistribution, sigma i, right?",
    "start": "1795250",
    "end": "1815035"
  },
  {
    "text": "No matter what\nvalue you express. So for even for y is\n1 or for y is minus 1, they will have the\nexact same distribution.",
    "start": "1815035",
    "end": "1821640"
  },
  {
    "text": "So that's why you can\nreplace yi sigma-- actually, you can also have minus here. That's still true.",
    "start": "1821640",
    "end": "1827720"
  },
  {
    "text": "Because you-- and then we\nrandomly flip the sign. So basically, that means you\ncan replace yi sigma-- minus y",
    "start": "1827720",
    "end": "1834650"
  },
  {
    "text": "sigma i by sigma itself. And still you don't\nchange the expectation.",
    "start": "1834650",
    "end": "1840140"
  },
  {
    "text": "So you can replace this by-- maybe let's define this--",
    "start": "1840140",
    "end": "1845780"
  },
  {
    "text": "I guess you can\nsay technically-- the easiest way to check this-- I saw some confusion. The easiest way to check this,\nyou just define sigma i prime",
    "start": "1845780",
    "end": "1852350"
  },
  {
    "text": "to be minus y sigma\nI. Then you get sup 1 over n sum of\nhxi sigma i prime.",
    "start": "1852350",
    "end": "1862170"
  },
  {
    "text": "But still, sigma\nprime distribution is still plus 1 minus 1\nuniform and independent, right?",
    "start": "1862170",
    "end": "1867630"
  },
  {
    "text": "So sigma i prime has the\nsame distribution as sigma i. So you can just write this\nsame way as this sigma i.",
    "start": "1867630",
    "end": "1877355"
  },
  {
    "text": "OK?  So what we have achieved here?",
    "start": "1877355",
    "end": "1883240"
  },
  {
    "text": "What we have achieved\nhere is that this seems to be a strictly\nsimpler quantity than before.",
    "start": "1883240",
    "end": "1889100"
  },
  {
    "text": "Why? This is basically the\nRademacher complexity of the hypothesis class h,\nbut not the family of losses,",
    "start": "1889100",
    "end": "1895705"
  },
  {
    "text": "right? Before we were talking\nabout a hypothesis class of the family of losses. And now you're\ntalking about exactly",
    "start": "1895705",
    "end": "1902210"
  },
  {
    "text": "the Rademacher complexity\nof the hypothesis class h. So basically, this is\nsaying that for binary--",
    "start": "1902210",
    "end": "1911130"
  },
  {
    "text": "I think I'm missing something. I'm missing 1/2 here. Where did the 1/2 go? Yeah, I think I lost the 1/2.",
    "start": "1911130",
    "end": "1917150"
  },
  {
    "text": "Sorry. ",
    "start": "1917150",
    "end": "1925440"
  },
  {
    "text": "I think I lost-- oh, I\nhave the 1/2 in the notes. It's just I forgot to copy it. So 1/2.",
    "start": "1925440",
    "end": "1932512"
  },
  {
    "text": "So basically, it's the 1/2\ntimes the Rademacher complexity. So what we achieved is that\nthe Rademacher complexity",
    "start": "1932512",
    "end": "1938110"
  },
  {
    "text": "of f in this special case\nof binary classification and 0, 1 loss is equal to 1/2\ntimes the Rademacher complexity",
    "start": "1938110",
    "end": "1946510"
  },
  {
    "text": "of the hypothesis class. So that's a slightly simpler\nway of thinking about this.",
    "start": "1946510",
    "end": "1953530"
  },
  {
    "text": "Because what's this? This is basically\nsaying that how well h",
    "start": "1953530",
    "end": "1959980"
  },
  {
    "text": "can memorize the random label.",
    "start": "1959980",
    "end": "1969559"
  },
  {
    "text": "You can think of sigma 1 up\nto sigma n as random label. And R and h is big\nwhen you can-- there",
    "start": "1969560",
    "end": "1977720"
  },
  {
    "text": "exist an h in the capital\nH such that h of xi",
    "start": "1977720",
    "end": "1985320"
  },
  {
    "text": "is equals to sigma i. This is the best\nsituation, right? This has the\nstrongest correlation. So basically, if you can\nmemorize all the random label",
    "start": "1985320",
    "end": "1992460"
  },
  {
    "text": "with some hypothesis,\nhypothesis class, that means your Rademacher\ncomplexity is the biggest.",
    "start": "1992460",
    "end": "1997710"
  },
  {
    "text": "And that gives you the\nworst generalization bound. And vice versa, if\nyou cannot memorize, then you get better\ngeneralization bound.",
    "start": "1997710",
    "end": "2003570"
  },
  {
    "text": "Right. OK. ",
    "start": "2003570",
    "end": "2011646"
  },
  {
    "text": "I have a question [INAUDIBLE]. ",
    "start": "2011646",
    "end": "2018820"
  },
  {
    "text": "So [INAUDIBLE] yi [INAUDIBLE].",
    "start": "2018820",
    "end": "2025625"
  },
  {
    "text": "But [INAUDIBLE]? ",
    "start": "2025625",
    "end": "2037432"
  },
  {
    "text": "I see. That's a good question. Let me repeat the question. So the question is, sigma\nprime is equal to y times--",
    "start": "2037432",
    "end": "2044740"
  },
  {
    "text": "actually, there's a minus there. So but it doesn't matter. So sigma prime is\nminus y times sigma i.",
    "start": "2044740",
    "end": "2050980"
  },
  {
    "text": "But yi itself is\nrandom variable. So can we still claim that sigma\nprime has the same distribution",
    "start": "2050980",
    "end": "2057158"
  },
  {
    "text": "as sigma i? So that's, indeed,\nthat's a good question. So technically, I think what\nyou should do is the following.",
    "start": "2057159",
    "end": "2064730"
  },
  {
    "text": "So if you are really\ncareful about this. So there are two\nrandomness, right?",
    "start": "2064730",
    "end": "2069835"
  },
  {
    "text": "So one is from the x\nand one is from a sigma. So you first condition the\nrandomness of xi and say that--",
    "start": "2069835",
    "end": "2078330"
  },
  {
    "text": "so in the first\nexpectation, so basically-- how do I say this? So you can write this\nas the following.",
    "start": "2078330",
    "end": "2085270"
  },
  {
    "text": "So the conditional xi, yi.",
    "start": "2085270",
    "end": "2090290"
  },
  {
    "text": "And then you look at the\nrandomness of sigma i, right? So now, after you\ncondition xi, then this",
    "start": "2090290",
    "end": "2097760"
  },
  {
    "text": "is absolutely clear, right? So for any choice of yi,\nsigma i and sigma prime",
    "start": "2097760",
    "end": "2104360"
  },
  {
    "text": "has the same distribution,\nconditioned on any choice of deterministic choice y. So then, so you do it inside.",
    "start": "2104360",
    "end": "2111650"
  },
  {
    "text": "And then y is gone\nin your formula. So then you don't have to\ncare about the outside.",
    "start": "2111650",
    "end": "2118205"
  },
  {
    "text": "Make sense? Cool. So sounds good.",
    "start": "2118205",
    "end": "2123640"
  },
  {
    "text": " And let me-- so the\ntake-home-away here",
    "start": "2123640",
    "end": "2131620"
  },
  {
    "text": "is that the Rademacher\ncomplexity of f is similar to the Rademacher\ncomplexity of the model. And the Rademacher\ncomplexity of the model",
    "start": "2131620",
    "end": "2137920"
  },
  {
    "text": "is basically saying\nhow well we can memorize random label, right? But there is a\nsmall caveat here,",
    "start": "2137920",
    "end": "2144380"
  },
  {
    "text": "which is that this relationship\nis not always true. This relationship is\ntrue, exactly true",
    "start": "2144380",
    "end": "2150280"
  },
  {
    "text": "for binary\nclassifications 0, 1 loss. But it's not even true for,\nfor example, some other loss",
    "start": "2150280",
    "end": "2155440"
  },
  {
    "text": "function. So I think the intuition\nlargely is still correct. But you cannot take this\nliterally or rigorously,",
    "start": "2155440",
    "end": "2165650"
  },
  {
    "text": "like religiously\nfor every situation. And in some cases, actually,\nthere could be a confusion.",
    "start": "2165650",
    "end": "2170710"
  },
  {
    "text": "Because there could be cases\nwhere these two are mismatched,",
    "start": "2170710",
    "end": "2177970"
  },
  {
    "text": "especially if your loss function\ncan do something different. For example, loss function\ncould change the binary number",
    "start": "2177970",
    "end": "2184540"
  },
  {
    "text": "to real number, or\nthe loss function has other kind of properties. So for example, the loss\nfunction is often nonlinear,",
    "start": "2184540",
    "end": "2191470"
  },
  {
    "text": "so suppose you take\nexponential loss. So and actually, they are\nin some sense, in the past,",
    "start": "2191470",
    "end": "2198100"
  },
  {
    "text": "they were this--  in some extreme\ncases, some papers",
    "start": "2198100",
    "end": "2206360"
  },
  {
    "text": "actually misinterpreted\nthis, in some sense. ",
    "start": "2206360",
    "end": "2213080"
  },
  {
    "text": "I guess I'm just giving you\na warning in some sense. Don't always apply\nthis every time without even thinking about it.",
    "start": "2213080",
    "end": "2220110"
  },
  {
    "text": "The intuition is roughly\ntrue, but it's not exactly true at all time.",
    "start": "2220110",
    "end": "2225380"
  },
  {
    "text": "I guess, there will\nbe a place where I'm going to mention this\nagain later in the lecture,",
    "start": "2225380",
    "end": "2231500"
  },
  {
    "text": "in some of the later lectures. Can I ask one question?",
    "start": "2231500",
    "end": "2238250"
  },
  {
    "text": "And by the way, just to--\nand what we will do next is that we are going\nto prove the theorem. ",
    "start": "2238250",
    "end": "2248240"
  },
  {
    "text": "And just a small overview for\nwhat we will do next lecture. So in this lecture,\nwe are going to deal",
    "start": "2248240",
    "end": "2254180"
  },
  {
    "text": "with this abstract measure, the\nRademacher complexity, right? And you may wonder--\nprobably some of you are wondering why\nRademacher complexity is",
    "start": "2254180",
    "end": "2262160"
  },
  {
    "text": "something measurable,\nsomething that is useful. So we don't answer that today.",
    "start": "2262160",
    "end": "2268280"
  },
  {
    "text": "We are going to answer that\nin the next few lectures. So today we are just introducing\nthis Rademacher complexity and say, this is bounding\ndoing it from convergence.",
    "start": "2268280",
    "end": "2276110"
  },
  {
    "text": "And this Rademacher complexity\nis something intuitive, I hope you find that, right? So it's talking about how\nwell you can memorize labels.",
    "start": "2276110",
    "end": "2283885"
  },
  {
    "text": "So it's something at\nleast makes sense. And in the next few\nlectures, we are going to instantiate this to\nmore concrete models where",
    "start": "2283885",
    "end": "2291769"
  },
  {
    "text": "you can bound the Rademacher\ncomplexity by something more concrete in the\nnext two lectures.",
    "start": "2291770",
    "end": "2298290"
  },
  {
    "text": "I got some-- oh. ",
    "start": "2298290",
    "end": "2305883"
  },
  {
    "text": "Did somebody ask a question? I didn't hear.  Yeah, I think a couple\nof people chimed in.",
    "start": "2305883",
    "end": "2314022"
  },
  {
    "text": "You answered my question\nin the meantime. But someone else might\nhave a question still. Yeah. Sorry.",
    "start": "2314022",
    "end": "2319450"
  },
  {
    "text": "I forgot to open my-- have volume on. Yeah. Please ask questions\nif you-- yeah.",
    "start": "2319450",
    "end": "2326710"
  },
  {
    "text": "Now it's working. All right. Thank you. ",
    "start": "2326710",
    "end": "2332240"
  },
  {
    "text": "Oh, actually, there\nis a question. What is the connection between\nthe Rademacher complexity",
    "start": "2332240",
    "end": "2337400"
  },
  {
    "text": "and a degree of freedom? I think, I assume by\nthe degree of freedom",
    "start": "2337400",
    "end": "2343730"
  },
  {
    "text": "you mean the number\nof parameters, right? So I guess that's\nkind of like what",
    "start": "2343730",
    "end": "2349250"
  },
  {
    "text": "we motivated in the beginning. So using this\nRademacher complexity, we will be able to\nprove more precise",
    "start": "2349250",
    "end": "2356330"
  },
  {
    "text": "bound than on the\nnumber of parameters. So probably so far,\nyou haven't seen that.",
    "start": "2356330",
    "end": "2364820"
  },
  {
    "text": "I don't expect you to see that. But in the next\nlecture, we're going to see you can prove\nbetter bounds that",
    "start": "2364820",
    "end": "2370760"
  },
  {
    "text": "depends on something more\nfine-grained than the number of parameters.",
    "start": "2370760",
    "end": "2377250"
  },
  {
    "text": "I hope that answers\nthe question. But please feel free to\nunmute yourself and ask any follow-ups.",
    "start": "2377250",
    "end": "2383940"
  },
  {
    "text": "I have a, I guess,\nconceptual question. How do you generally think\nabout the distinction between the hypothesis,\nthe family of hypotheses,",
    "start": "2383940",
    "end": "2394110"
  },
  {
    "text": "versus the family\nof losses over them? To me-- because they have\nthe same cardinality, right?",
    "start": "2394110",
    "end": "2399330"
  },
  {
    "text": "They seem like a direct\nmapping between 1/2. How do you distinguish, I guess,\nin your mind between those two?",
    "start": "2399330",
    "end": "2405450"
  },
  {
    "text": "How do you think about them? Yeah, that's a great question. So in my mind, they\nare very similar,",
    "start": "2405450",
    "end": "2412780"
  },
  {
    "text": "except that except that I think\nthis will be a little more",
    "start": "2412780",
    "end": "2421560"
  },
  {
    "text": "explicit in the next lecture\nor maybe two lectures later. So except that when you\ntalk about the models,",
    "start": "2421560",
    "end": "2426720"
  },
  {
    "text": "the models oftentimes\noutput the real number. So for example, if you think\nabout logistic regression,",
    "start": "2426720",
    "end": "2432450"
  },
  {
    "text": "the models output the logits,\nwhich could be anywhere on the real line. And they would turn\nthat into a probability",
    "start": "2432450",
    "end": "2439350"
  },
  {
    "text": "and then use that probability\nto compute the loss. And the loss becomes something,\nfirst of all, non-active.",
    "start": "2439350",
    "end": "2445230"
  },
  {
    "text": "And often, sometimes\nthe loss is reasonably-- it's between 0 and 1. For logistic loss it's\nnot between 0 and 1.",
    "start": "2445230",
    "end": "2451345"
  },
  {
    "text": "But I think the most\ninteresting regime is that it's somewhat small. So it's between 0 and 1. And if you care about\nclassification loss,",
    "start": "2451345",
    "end": "2458730"
  },
  {
    "text": "then there's literally\nbetween 0 and 1. So the loss function sometimes\nhas a scale in some sense.",
    "start": "2458730",
    "end": "2466080"
  },
  {
    "text": "It's something on\nthe order of 1. But your model could\nbe sometimes outputting",
    "start": "2466080",
    "end": "2472170"
  },
  {
    "text": "some bigger numbers.  So there is a\nconversion there, which",
    "start": "2472170",
    "end": "2478350"
  },
  {
    "text": "will be more explicit\nin a future lectures. But besides-- beyond\nthat, typically I",
    "start": "2478350",
    "end": "2485610"
  },
  {
    "text": "don't distinguish\nthem very much. Gotcha. Yeah. I found it interesting that\nin the example, at least",
    "start": "2485610",
    "end": "2492780"
  },
  {
    "text": "for binary classification, the\ncomplexity of the loss family was less than or half of the\ncomplexity of the model family.",
    "start": "2492780",
    "end": "2504180"
  },
  {
    "text": "Is it common that your\ncomplexity goes down when you compose it\nwith the loss function? ",
    "start": "2504180",
    "end": "2511530"
  },
  {
    "text": "I think it's common\nthat they are related. We will see that in many\ncases, they can be related,",
    "start": "2511530",
    "end": "2517660"
  },
  {
    "text": "but I think I wouldn't read too\nmuch from that constant of 1/2. Because this 1/2 does depend on\nhow you define on your labels.",
    "start": "2517660",
    "end": "2527730"
  },
  {
    "text": "For example, if\nyour label is 0, 1, I think you wouldn't\nsee the 1/2. So there are some\nsmall artifacts there",
    "start": "2527730",
    "end": "2534260"
  },
  {
    "text": "so the constant. So it doesn't really\nmatter that much. Sure. ",
    "start": "2534260",
    "end": "2541635"
  },
  {
    "text": "OK. Cool. OK. Let's continue. So we're going to prove this.",
    "start": "2541635",
    "end": "2547440"
  },
  {
    "text": "And this is called\nsymmetrization technique, the proof. And this is a technique that\ncan be used in many other cases.",
    "start": "2547440",
    "end": "2556020"
  },
  {
    "text": "Not necessary in this\ncourse, but in other areas",
    "start": "2556020",
    "end": "2561270"
  },
  {
    "text": "of probability, let's say. So summarization technique,\nI think it's probably",
    "start": "2561270",
    "end": "2566309"
  },
  {
    "text": "comes from those kind of\nrisk probability anyways, in the first place.",
    "start": "2566310",
    "end": "2573060"
  },
  {
    "text": "So the technique is that-- let's write down\nwhat we care about.",
    "start": "2573060",
    "end": "2579040"
  },
  {
    "text": "So what we care\nabout is the sup--",
    "start": "2579040",
    "end": "2588200"
  },
  {
    "text": "I mean, I'll take\nexpectation for now, just so that it's a\nlittle bit cleaner. We will take\nexpectation in a bit.",
    "start": "2588200",
    "end": "2596470"
  },
  {
    "text": "So this is not\nsymmetric in some sense. Because you have this\nsubtraction here.",
    "start": "2596470",
    "end": "2606220"
  },
  {
    "text": "And so, these two terms\ndon't look the same. That's what I mean\nby not symmetric.",
    "start": "2606220",
    "end": "2611700"
  },
  {
    "text": "So there's a way to make\nthem somehow more symmetric. So what you do is\nthat if you fix--",
    "start": "2611700",
    "end": "2619630"
  },
  {
    "text": "for now, let's say we\nfixed x1, Z1 up to Z1.",
    "start": "2619630",
    "end": "2625470"
  },
  {
    "text": "And then, we let Z1\nprime and Zealand prime to be a different\ndraw, another draw",
    "start": "2625470",
    "end": "2632550"
  },
  {
    "text": "from the distribution\np, and iid. So you draw a sequence of\ncopies of Z1 up to Zn iid.",
    "start": "2632550",
    "end": "2642990"
  },
  {
    "text": "And then, what you can\ndo is that you can say, convert this second, the\nexpectation, this quantity,",
    "start": "2642990",
    "end": "2652500"
  },
  {
    "text": "using the Zi prime. Just because by\ndefinition, all the Zi's",
    "start": "2652500",
    "end": "2658369"
  },
  {
    "text": "have the same\ndistribution from P. So expectation of f\nis really the same",
    "start": "2658370",
    "end": "2666430"
  },
  {
    "text": "as you look at expectation\nof sum of f of Zi prime. Because each of these term has\nexpectations the same as e f.",
    "start": "2666430",
    "end": "2675130"
  },
  {
    "text": "And you average them. So you got e of f. ",
    "start": "2675130",
    "end": "2683780"
  },
  {
    "text": "So and you see that this already\nmakes it a little bit more symmetric for whatever,\njust on the surface",
    "start": "2683780",
    "end": "2691875"
  },
  {
    "text": "looks more symmetric. Because this is a sum of things,\nand this is sum of things. Of course, it's still\na little bit different",
    "start": "2691875",
    "end": "2697673"
  },
  {
    "text": "because the expectation\nis in front of this thing. But there is no expectation\nin front of the first.",
    "start": "2697673",
    "end": "2703170"
  },
  {
    "text": "So what you can do is you can,\nof course, one thing you can do",
    "start": "2703170",
    "end": "2710180"
  },
  {
    "text": "is you can put the expectation\nin front of the poster, which",
    "start": "2710180",
    "end": "2715369"
  },
  {
    "text": "is not doing really anything. Because for now, Zi is constant\nand Zi prime is random.",
    "start": "2715370",
    "end": "2722497"
  },
  {
    "text": "So in some sense,\nyou are just putting some constant\ninside expectation. ",
    "start": "2722497",
    "end": "2730560"
  },
  {
    "text": "And now, what happens is that\nyou can switch the expectation with the sup.",
    "start": "2730560",
    "end": "2736970"
  },
  {
    "text": "Maybe ask the question first. If you [INAUDIBLE] the\nsum between [INAUDIBLE].. Oh, sure. Yes, sorry. ",
    "start": "2736970",
    "end": "2744747"
  },
  {
    "text": "It's this one? Yeah. Cool. Thanks. Yeah. So now we'll make\nit more symmetric. So we'll switch the\nexpectation with the sup.",
    "start": "2744747",
    "end": "2753760"
  },
  {
    "text": "So I'm claiming that\nif you switch them you get an inequality. ",
    "start": "2753760",
    "end": "2762170"
  },
  {
    "text": "Zn prime sup. ",
    "start": "2762170",
    "end": "2782810"
  },
  {
    "text": "So why this is true? And this is just a\nvery generic inequality",
    "start": "2782810",
    "end": "2788090"
  },
  {
    "text": "which claims that you can\nswitch sup and expectation get inequality.",
    "start": "2788090",
    "end": "2793770"
  },
  {
    "text": "So generically, the\nclaim is that suppose you have a function g that\ntakes in two variable.",
    "start": "2793770",
    "end": "2801170"
  },
  {
    "text": "And suppose you are\ntaking expectation first over the randomness of the first\nvariable-- the second variable.",
    "start": "2801170",
    "end": "2808440"
  },
  {
    "text": "And then you take sup\nover the first variable.",
    "start": "2808440",
    "end": "2815550"
  },
  {
    "text": "Suppose this is a\nquantity you have. Then you can replace this by\neventually by first taking",
    "start": "2815550",
    "end": "2821819"
  },
  {
    "text": "expectation and then-- actually, by first\ntaking the sup and then take the expectation.",
    "start": "2821820",
    "end": "2827359"
  },
  {
    "text": "Because when we do the\nmath, you are doing it from the right-hand side\nto the left-hand side. And why this is true,\nthis is because you",
    "start": "2827360",
    "end": "2835070"
  },
  {
    "text": "can have an intermediate step,\nwhich is you take sup over u,",
    "start": "2835070",
    "end": "2840210"
  },
  {
    "text": "take expectation over v. And\nyou bound this guv by sup over u",
    "start": "2840210",
    "end": "2846839"
  },
  {
    "text": "guv. Maybe let's call it u prime. ",
    "start": "2846840",
    "end": "2853740"
  },
  {
    "text": "So this inequality\nis very simple. It's just because this term is\nterm-wise bounded by the sup.",
    "start": "2853740",
    "end": "2862019"
  },
  {
    "text": "And once you do\nthe sup, then you see that this whole thing\ndoesn't depend on i anymore,",
    "start": "2862020",
    "end": "2870545"
  },
  {
    "text": "right? So maybe I should\nhave another step. So actually, I'm claiming that\nthis is just equal to this.",
    "start": "2870545",
    "end": "2878580"
  },
  {
    "text": "Because this term it\ndoesn't depend on u. You already got rid of u, right?",
    "start": "2878580",
    "end": "2885760"
  },
  {
    "text": "So the sup over u\njust can be gone. And then, the green term\nis equal to the term below.",
    "start": "2885760",
    "end": "2892410"
  },
  {
    "text": "Just because you change the\nvariable name u to u prime, that's nothing. So that's why it's equality.",
    "start": "2892410",
    "end": "2900460"
  },
  {
    "text": "So in general,\nit's just probably useful to know this\nas a fact so you can switch the sup with\nexpectation and get inequality.",
    "start": "2900460",
    "end": "2910805"
  },
  {
    "text": "Yes, sometimes I do it. I don't remember which\ndirection of the inequality is. So that's why you want\nto probably somewhat know",
    "start": "2910805",
    "end": "2916970"
  },
  {
    "text": "how to prove it,\nso that in case you got confused which direction\nit is you can still recover it.",
    "start": "2916970",
    "end": "2923440"
  },
  {
    "text": "OK. Cool. OK. So that's how this works.",
    "start": "2923440",
    "end": "2929990"
  },
  {
    "text": "And now, if you take\nexpectation over Z again, we're already\nconditioned on Z. But now suppose let's take the\nexpectation over Z.",
    "start": "2929990",
    "end": "2936480"
  },
  {
    "text": "Then you see that this\nis very symmetric. So what we got is that you take\nexpectation over Z1 up to Zn.",
    "start": "2936480",
    "end": "2943360"
  },
  {
    "start": "2943360",
    "end": "2954690"
  },
  {
    "text": "And it's bounded by-- now you have two\nexpectations here. One is over Zi's, and the\nother is over Zi primes.",
    "start": "2954690",
    "end": "2963330"
  },
  {
    "text": "And then you have sup. ",
    "start": "2963330",
    "end": "2973359"
  },
  {
    "text": "Let's put it into a\nsingle sum, by the way-- ",
    "start": "2973360",
    "end": "2984650"
  },
  {
    "text": "minus f Zi prime. OK. ",
    "start": "2984650",
    "end": "2994299"
  },
  {
    "text": "So now it becomes a\nlittle more symmetric. And I'll do some--\none more thing to make it even more symmetric.",
    "start": "2994300",
    "end": "3000730"
  },
  {
    "text": "So this one, it's symmetric\nin a sense that actually",
    "start": "3000730",
    "end": "3008830"
  },
  {
    "text": "is mean 0 random variable. It's not even mean zero.",
    "start": "3008830",
    "end": "3014240"
  },
  {
    "text": "But actually, in\nterms of distribution, it's the distribution\nis symmetric in the following sense.",
    "start": "3014240",
    "end": "3019440"
  },
  {
    "text": "So fZi minus fZi prime\nhas the same distribution",
    "start": "3019440",
    "end": "3030030"
  },
  {
    "text": "as fZi prime minus fZi. Because these two things\nare just the renaming",
    "start": "3030030",
    "end": "3037140"
  },
  {
    "text": "of each other in some sense. So they have the\nsame distribution. Or in other words, this\nhas the same distribution",
    "start": "3037140",
    "end": "3042630"
  },
  {
    "text": "as sigma i, fZi minus fZi prime\nfor any sigma i that is binary.",
    "start": "3042630",
    "end": "3055039"
  },
  {
    "text": "If it's minus 1, if it's\nplus 1 it's the same thing. If it's minus one, it's\njust to flip the order. ",
    "start": "3055040",
    "end": "3062500"
  },
  {
    "text": "So could that means\nthat you can for free introduce this random\nvariable sigma i",
    "start": "3062500",
    "end": "3068109"
  },
  {
    "text": "and not change anything? So that means that if you\nintroduce this Rademacher",
    "start": "3068110",
    "end": "3073130"
  },
  {
    "text": "random variable, and you take\nexpectation over sigma i's,",
    "start": "3073130",
    "end": "3089369"
  },
  {
    "text": "you multiply the sigma i. This fZi is fZi prime.",
    "start": "3089370",
    "end": "3095170"
  },
  {
    "text": "So this is still inequality. Actually, here even you choose\nany sigma i, this is equality.",
    "start": "3095170",
    "end": "3101860"
  },
  {
    "text": "Of course, if you choose random\nsigma i's and average them, it's still equality. OK? ",
    "start": "3101860",
    "end": "3110470"
  },
  {
    "text": "So I think, technically, you\nclaim that for any sigma i, this is equality.",
    "start": "3110470",
    "end": "3117053"
  },
  {
    "text": "Technically the\nfirst step is this is equality for every\nsigma i, for any sigma i.",
    "start": "3117053",
    "end": "3122990"
  },
  {
    "text": "And then you say that even\nyou take another expectation over sigma i's,\nthis is still true.",
    "start": "3122990",
    "end": "3130060"
  },
  {
    "text": " This is still true.",
    "start": "3130060",
    "end": "3135320"
  },
  {
    "text": "And then you can switch the\nexpectation whatever you want. And I'm going to switch\nit, just because it's",
    "start": "3135320",
    "end": "3140920"
  },
  {
    "text": "a little bit convenient\nfor me to do that. OK. ",
    "start": "3140920",
    "end": "3159680"
  },
  {
    "text": "So now, what I'm going\nto do is that I'm going",
    "start": "3159680",
    "end": "3166260"
  },
  {
    "text": "to break this into two sums.",
    "start": "3166260",
    "end": "3173730"
  },
  {
    "text": "So I'm going to\nhave expectations. Here you have all\nthe randomness.",
    "start": "3173730",
    "end": "3179930"
  },
  {
    "text": "This is just a\nsimplification of notation. So I'm claiming that this is\nless than sup of the first term",
    "start": "3179930",
    "end": "3190130"
  },
  {
    "text": "plus the sup of the second term. ",
    "start": "3190130",
    "end": "3205520"
  },
  {
    "text": "And here, what we are\ndoing is essentially exactly the same\nthing as the switch of the expectation-- the swap\nof the expectation and the sup.",
    "start": "3205520",
    "end": "3214280"
  },
  {
    "text": "But here, we only\nhave two terms. So it's a swap of sum and sup. So here, we are doing something\nlike sup of two terms.",
    "start": "3214280",
    "end": "3222500"
  },
  {
    "text": "So something like a function\nof f plus another function of f over f.",
    "start": "3222500",
    "end": "3228020"
  },
  {
    "text": "And then, you can say\nthat this is less than sup of f, u of f, sup u of f.",
    "start": "3228020",
    "end": "3238264"
  },
  {
    "text": "Yeah. I guess you can prove this\nalmost the same way as we have done with the expectations.",
    "start": "3238264",
    "end": "3244300"
  },
  {
    "text": "But just you need one\nstep in the middle. I will use this as\nan exercise for you.",
    "start": "3244300",
    "end": "3250310"
  },
  {
    "text": "So and now, probably\nhave seen that we are getting closer and\ncloser to the definition",
    "start": "3250310",
    "end": "3256720"
  },
  {
    "text": "of the Rademacher complexity. The only thing is that\nwe have two terms, and the Rademacher complexity\nhave just the only of this.",
    "start": "3256720",
    "end": "3263450"
  },
  {
    "text": "So now, we can change this to-- we can swap the\nexpectation with the sum.",
    "start": "3263450",
    "end": "3269970"
  },
  {
    "text": "So you get sup Zi.",
    "start": "3269970",
    "end": "3275830"
  },
  {
    "text": "And here, and plus expectation\nsup minus sigma i, fZi prime.",
    "start": "3275830",
    "end": "3285710"
  },
  {
    "text": "And here the randomness\nis Z1 up to Zn. And sigma went up to sigma n.",
    "start": "3285710",
    "end": "3291082"
  },
  {
    "text": "So this is exactly--\nthe first time it's exactly\nRademacher complexity. And I'm going to claim that\nthe second term is also",
    "start": "3291082",
    "end": "3296530"
  },
  {
    "text": "exactly Rademacher complexity. Because here, my randomness\nis Zi prime up to Zn prime,",
    "start": "3296530",
    "end": "3304340"
  },
  {
    "text": "and sigma 1 up to sigma n. But again, because\nminus sigma i fZi",
    "start": "3304340",
    "end": "3310550"
  },
  {
    "text": "prime has the same\ndistribution as sigma i fZi,",
    "start": "3310550",
    "end": "3320220"
  },
  {
    "text": "because minus sigma i has the\nsame distribution as sigma i, and Zi prime has the\nsame distribution as Zi.",
    "start": "3320220",
    "end": "3326460"
  },
  {
    "text": "So then the second term is still\nthis is equal to the first one.",
    "start": "3326460",
    "end": "3331750"
  },
  {
    "text": "So basically this is just\nequal to 2 times this. ",
    "start": "3331750",
    "end": "3341900"
  },
  {
    "text": "Plus 2 times the\nRademacher complexity of f. ",
    "start": "3341900",
    "end": "3355740"
  },
  {
    "text": "Any questions? ",
    "start": "3355740",
    "end": "3363040"
  },
  {
    "text": "So it kind of feels\nlike we just did algebra for a bunch of lines? Is that [INAUDIBLE]?",
    "start": "3363040",
    "end": "3369354"
  },
  {
    "start": "3369354",
    "end": "3374890"
  },
  {
    "text": "That's a great question. And that's exactly what\nI'm going to remark on. So the question\nwas, if I phrased it",
    "start": "3374890",
    "end": "3381820"
  },
  {
    "text": "slightly differently. So what we have\nreally done here, did we do anything\npowerful, or did we",
    "start": "3381820",
    "end": "3387190"
  },
  {
    "text": "do something-- because the\nleft-hand side has a sup. The right-hand side\nstill has a sup.",
    "start": "3387190",
    "end": "3393370"
  },
  {
    "text": "So we do something really\nuseful or did we just do a bunch of algebra, right?",
    "start": "3393370",
    "end": "3399339"
  },
  {
    "text": "So I'm going to claim that\nwe did do something useful. And the reason is that the\nleft-hand side is something",
    "start": "3399340",
    "end": "3407710"
  },
  {
    "text": "like a sup is what we\ncare about, the difference between the empirical mean\nand the population mean.",
    "start": "3407710",
    "end": "3416900"
  },
  {
    "text": " So on the right-hand\nside, roughly speaking,",
    "start": "3416900",
    "end": "3424930"
  },
  {
    "text": "the most important\nthing is this sigma. ",
    "start": "3424930",
    "end": "3432470"
  },
  {
    "text": "So what we have achieved here? So one, we have achieved\nthat we remove the Ef, right?",
    "start": "3432470",
    "end": "3449010"
  },
  {
    "text": "So we get rid of the Ef. And it's probably\nnot super clear",
    "start": "3449010",
    "end": "3455790"
  },
  {
    "text": "why we should appreciate this\nfact that we got rid of the ef at the first sight. But I can say that\nthis ef is somewhat",
    "start": "3455790",
    "end": "3463980"
  },
  {
    "text": "annoying because you don't have\na good control on it, right? So when you look\nat this, in some",
    "start": "3463980",
    "end": "3469410"
  },
  {
    "text": "sense this quality doesn't\ndepend on the relative-- for example, this quantity\ndoesn't depend on Ef.",
    "start": "3469410",
    "end": "3476283"
  },
  {
    "text": "So if you shift it a little\nbit, it wouldn't change. Actually, we are going to claim\nthat this on the right hand side is translation environment.",
    "start": "3476283",
    "end": "3482960"
  },
  {
    "text": "So in some sense, you move\nthe-- remove the translation invariant part. So maybe let me just claim the\nright-hand side is translation",
    "start": "3482960",
    "end": "3494950"
  },
  {
    "text": "environment. Or maybe, the\nRademacher complexity.",
    "start": "3494950",
    "end": "3504619"
  },
  {
    "text": "I'm going to claim that--\nprove this in a moment. Let me see whether I plan to\ndo this in today's lecture.",
    "start": "3504620",
    "end": "3511430"
  },
  {
    "start": "3511430",
    "end": "3518161"
  },
  {
    "text": "I think I didn't plan to\ndo it today's lecture. But this is a claim. So in some sense, you remove\nthe translation invariant part.",
    "start": "3518162",
    "end": "3524350"
  },
  {
    "text": "You remove the Ef, right, so\nwhich is useful in many cases.",
    "start": "3524350",
    "end": "3529420"
  },
  {
    "text": "And second, you sometimes\nintroduce use more randomness,",
    "start": "3529420",
    "end": "3541200"
  },
  {
    "text": "sigma 1 up to sigma n. So why introducing this\nrandomness is useful?",
    "start": "3541200",
    "end": "3546539"
  },
  {
    "text": "It's probably still\nunclear right now. But eventually, what we can do\nis that we are going to have--",
    "start": "3546540",
    "end": "3553090"
  },
  {
    "text": "so currently what\nwe really have is we have expectations\nof this, right? You also have the\nexpectation of this.",
    "start": "3553090",
    "end": "3558670"
  },
  {
    "text": "And here, the randomness\nis Z1 up to Zn and sigma 1 up to sigma n, right?",
    "start": "3558670",
    "end": "3564730"
  },
  {
    "text": "So we will use\nadditional randomness. And this allows us--\nthis will allows us to drop the randomness\nfrom Z1 up to Zn.",
    "start": "3564730",
    "end": "3582050"
  },
  {
    "text": "This will be\nsomething we'll see, I guess, probably\nin the next lecture.",
    "start": "3582050",
    "end": "3587940"
  },
  {
    "text": "So eventually, you don't\nhave to care about the-- you don't have to take\nexpectation over Zi up to Zn.",
    "start": "3587940",
    "end": "3593779"
  },
  {
    "text": "You can claim with\nhigh probability. So the right-hand side\nwouldn't have to run-- Zi up to Zn, you don't need\nto take expectation over Zi",
    "start": "3593780",
    "end": "3601850"
  },
  {
    "text": "up to Zn. The only randomness\ncome from the sigma i's, which I guess probably you\ndon't see exactly what I mean.",
    "start": "3601850",
    "end": "3610630"
  },
  {
    "text": "But if eventually, you only care\nabout the randomness of sigma 1 up to sigma i, and\nsigma 1 up to sigma n,",
    "start": "3610630",
    "end": "3616160"
  },
  {
    "text": "that seems to be a benefit. Because this is very\nmuch simpler randomness.",
    "start": "3616160",
    "end": "3623589"
  },
  {
    "text": "So sigma up to sigma n has\na very simple distribution. They are just Rademacher\nrandom variables.",
    "start": "3623590",
    "end": "3630080"
  },
  {
    "text": "So they are much\nless complicated than the distribution\nof Z1 up to Zn, which is something you don't know.",
    "start": "3630080",
    "end": "3636010"
  },
  {
    "text": "You just assume there's\na distribution P. But you didn't really know\nany other properties about it.",
    "start": "3636010",
    "end": "3641140"
  },
  {
    "text": "So I think that's\nthe second benefit.",
    "start": "3641140",
    "end": "3646760"
  },
  {
    "text": "But of course, the\nlimitation is that we still have the sup, which\nis still a problem.",
    "start": "3646760",
    "end": "3651823"
  },
  {
    "text": "But I think you probably\nwouldn't-- shouldn't expect that you can remove\nthe sup on this level. When you have\nabstract family class,",
    "start": "3651823",
    "end": "3658765"
  },
  {
    "text": "you probably\nshouldn't expect you can remove the sup completely. So it should be the\nnext level where you remove the sup when you have\na concrete hypothesis class.",
    "start": "3658765",
    "end": "3666680"
  },
  {
    "start": "3666680",
    "end": "3671920"
  },
  {
    "text": "Cool. ",
    "start": "3671920",
    "end": "3681742"
  },
  {
    "text": "Let me just drink something. ",
    "start": "3681742",
    "end": "3687210"
  },
  {
    "text": "So the next part is another\nuseful property or useful thing",
    "start": "3687210",
    "end": "3693800"
  },
  {
    "text": "to know about Rademacher\ncomplexity, which is that this Rademacher\ncomplexity can depend on the distribution p.",
    "start": "3693800",
    "end": "3704569"
  },
  {
    "text": "It still can depend\non distribution p, even though our goal\nis to try to use the new randomness, deal with\nthe simpler randomness, right?",
    "start": "3704570",
    "end": "3713130"
  },
  {
    "text": "Why this is the case? This is just because in this\ndefinition of Rademacher",
    "start": "3713130",
    "end": "3719579"
  },
  {
    "text": "complexity, you do\nhave to draw some z1 up to Zn from the\ndistribution p, right?",
    "start": "3719580",
    "end": "3724780"
  },
  {
    "text": "So this is extreme example\nwhere you can see that where",
    "start": "3724780",
    "end": "3731800"
  },
  {
    "text": "p is a point max. ",
    "start": "3731800",
    "end": "3737080"
  },
  {
    "text": "So let's say Z is always equal\nto Z0 almost [INAUDIBLE].. So whatever how you\ndraw it, you always",
    "start": "3737080",
    "end": "3743470"
  },
  {
    "text": "just draw a single point. And in this case,\nactually, you can have a good\nRademacher complexity",
    "start": "3743470",
    "end": "3748640"
  },
  {
    "text": "bound for any bounded function. So suppose, let's say minus 1.",
    "start": "3748640",
    "end": "3756900"
  },
  {
    "text": "Suppose you care\nabout [AUDIO OUT] f. And this is the only\nconstraint on the family f.",
    "start": "3756900",
    "end": "3763110"
  },
  {
    "text": "So basically, you care about f-- or maybe more-- let's say f\nis the family of functions f such that fZ0 is bounded by 1.",
    "start": "3763110",
    "end": "3773160"
  },
  {
    "text": "So we just have a bounded\nfamily of functions. You don't even have\nany parametric form. Still you can prove that\nthe Rademacher complexity",
    "start": "3773160",
    "end": "3781020"
  },
  {
    "text": "of this family is small. So you can say that look at sup.",
    "start": "3781020",
    "end": "3787945"
  },
  {
    "start": "3787945",
    "end": "3796360"
  },
  {
    "text": "So this is what? Because fZi is always the same. So this is literally\nequals to 1 over n times",
    "start": "3796360",
    "end": "3805105"
  },
  {
    "text": "fZ0 times sum of sigma i. ",
    "start": "3805105",
    "end": "3811780"
  },
  {
    "text": "And because fZ0 is\njust a constant, it doesn't depend on what f is,\nbecause Z0 is-- wait, sorry.",
    "start": "3811780",
    "end": "3819339"
  },
  {
    "text": "My bad. I'm wrong with that. Let's see. So fZ0 still depends\non Z0, right?",
    "start": "3819340",
    "end": "3826780"
  },
  {
    "text": "But fZ0 is bounded between 0\nand 1, or between minus 1 and 1.",
    "start": "3826780",
    "end": "3832110"
  },
  {
    "text": "So that means that this is less\nthan or equal to expectation if you just bound\nthis fZ0 by 1, you",
    "start": "3832110",
    "end": "3838950"
  },
  {
    "text": "got 1 over n times sum of\nsigma i absolute value. ",
    "start": "3838950",
    "end": "3845900"
  },
  {
    "text": "So and this is actually--",
    "start": "3845900",
    "end": "3850980"
  },
  {
    "text": "then use Cauchy-Schwarz\nor use the-- I think this is\ncalled Cauchy-Schwarz. So the expectation of\nthis random variable",
    "start": "3850980",
    "end": "3858480"
  },
  {
    "text": "is smaller than the\nexpectation of the square",
    "start": "3858480",
    "end": "3864635"
  },
  {
    "text": "of the random variable\nto the power 1/2. And then you can--",
    "start": "3864635",
    "end": "3871323"
  },
  {
    "text": "actually, these\nderivations, we're going to see these kind of\nderivation several times. So you get a 1/2 expectation\nsum of sigma i sigma j.",
    "start": "3871323",
    "end": "3885180"
  },
  {
    "text": "i is not equal to j, plus sum\nof sigma i squared, times 1",
    "start": "3885180",
    "end": "3891220"
  },
  {
    "text": "over n square. I'm just expanding it. So you get 1 over n square.",
    "start": "3891220",
    "end": "3898359"
  },
  {
    "text": "Sigma sigma j just means 0. So you get sum over i from\n1 to n expectation sigma i",
    "start": "3898360",
    "end": "3905029"
  },
  {
    "text": "squared 1/2. So this is-- each of this is 1. You take the sum, you get n.",
    "start": "3905030",
    "end": "3911750"
  },
  {
    "text": "So you get 1 over n to the\npower 1/2 is 1 over square root. ",
    "start": "3911750",
    "end": "3919099"
  },
  {
    "text": "So in some sense, this is\nkind of interesting, right? But for very, very large\nfamily of functions,",
    "start": "3919100",
    "end": "3924175"
  },
  {
    "text": "without even\nparametric form, you can still have a good\nRademacher complexity. And the reason is that the\ndistribution is so simple.",
    "start": "3924175",
    "end": "3931430"
  },
  {
    "text": "So in some sense,\nthis is an indicator that the Rademacher complexity\ncan capture something",
    "start": "3931430",
    "end": "3937940"
  },
  {
    "text": "about the distribution. If a distribution\nis extremely simple, then Rademacher complexity\ncan capture that and tell you",
    "start": "3937940",
    "end": "3944600"
  },
  {
    "text": "that it's very\neasy to generalize. So basically, any f on a\nvery simple distribution",
    "start": "3944600",
    "end": "3953990"
  },
  {
    "text": "should be considered\nas very simple. Even though this-- in some\nsense, this family of f",
    "start": "3953990",
    "end": "3959420"
  },
  {
    "text": "is just-- you have basically no\nassumptions on f in some sense. There is no\nparametric form, it's a very large family\nof functions.",
    "start": "3959420",
    "end": "3965960"
  },
  {
    "text": "But with respect to\nthe distribution, a simple distribution, it\nshould be considered as simple.",
    "start": "3965960",
    "end": "3970980"
  },
  {
    "text": "And this is what Rademacher\ncomplexity can tell you. So that's saying that\nRademacher complexity can take into account\nthe distribution P.",
    "start": "3970980",
    "end": "3980570"
  },
  {
    "text": "But how much it can take into\naccount of distribution P, that's a question mark. So in many of the\nother analysis,",
    "start": "3980570",
    "end": "3989310"
  },
  {
    "text": "you don't have this property. You don't really use too\nmuch about the solution",
    "start": "3989310",
    "end": "3994640"
  },
  {
    "text": "P in many of the concrete bounds\nfor Rademacher complexity. But in principle, it can\ncapture something about P.",
    "start": "3994640",
    "end": "4008240"
  },
  {
    "text": "I have 15 minutes. I think there is time for\nme to do this next part.",
    "start": "4008240",
    "end": "4013309"
  },
  {
    "start": "4013310",
    "end": "4018830"
  },
  {
    "text": "Let me see.  Here, I think I have\ntime to do this.",
    "start": "4018830",
    "end": "4025790"
  },
  {
    "text": "OK. So the next part, if\nthere's any-- no questions-- [INAUDIBLE] ",
    "start": "4025790",
    "end": "4033661"
  },
  {
    "text": "What if you know\nthat [INAUDIBLE]?? ",
    "start": "4033662",
    "end": "4059230"
  },
  {
    "text": "So your question is\nwhether, for example, when the features,\nthe x, the coordinates",
    "start": "4059230",
    "end": "4066640"
  },
  {
    "text": "of x have correlations, or\nmaybe have independence? Yeah.",
    "start": "4066640",
    "end": "4072020"
  },
  {
    "text": "Independence is probably more\nlike a simplistic thing, right? So can you get a better bound\nfrom Rademacher complexity?",
    "start": "4072020",
    "end": "4079810"
  },
  {
    "text": " I think this-- to\nanswer this question,",
    "start": "4079810",
    "end": "4086910"
  },
  {
    "text": "we need to zoom in\nto concrete settings. For linear models,\nI guess you will",
    "start": "4086910",
    "end": "4095119"
  },
  {
    "text": "see that you would get bound-- at least if you compare to\nextreme cases, in one case",
    "start": "4095120",
    "end": "4100189"
  },
  {
    "text": "all the coordinates\nare correlated. And the other case is that--",
    "start": "4100189",
    "end": "4106299"
  },
  {
    "text": "actually, it's unclear. So because if all\nthe coordinates are correlated,\nactually you probably",
    "start": "4106300",
    "end": "4111939"
  },
  {
    "text": "should have a better bound. Because if, for example,\nin the very extreme case, where all the coordinates are\nthe same, then you effective",
    "start": "4111939",
    "end": "4121479"
  },
  {
    "text": "have a one-dimensional problem. So you should have\na better bound. So it does depend on the\nparticular situation, I think.",
    "start": "4121479",
    "end": "4128710"
  },
  {
    "text": " So yeah. So it's interesting. It's not clear that independence\nmeans really simpler.",
    "start": "4128710",
    "end": "4137250"
  },
  {
    "text": "Independence could mean\nthat it's more complicated. Just because we have\nindependent input distribution.",
    "start": "4137250",
    "end": "4143068"
  },
  {
    "text": "So you have a diverse\nset of distributions, diverse set of data. It might be even more--",
    "start": "4143069",
    "end": "4149170"
  },
  {
    "text": "it's harder to\ngeneralize in some cases. So for example, here\nin this one point mass case, where you have a\nvery narrow family of data.",
    "start": "4149170",
    "end": "4157790"
  },
  {
    "text": "It means you can generalize\neasier because you can just memorize that Z is 0.",
    "start": "4157790",
    "end": "4163899"
  },
  {
    "text": "So independence\nmight make it harder. ",
    "start": "4163899",
    "end": "4170568"
  },
  {
    "text": "So the next 15 to 10-- 10 to 15 minutes,\nlet me try to define",
    "start": "4170569",
    "end": "4177140"
  },
  {
    "text": "this so-called empirical\nRademacher complexity. And the goal here is to\nremove that expectation",
    "start": "4177140",
    "end": "4184699"
  },
  {
    "text": "in front of the sup. So currently, the average\nversion has two expectations.",
    "start": "4184700",
    "end": "4191799"
  },
  {
    "text": "One is over the\nrandomness of Z1 up to Zn. And there's another\nexpectation of the randomness",
    "start": "4191800",
    "end": "4198390"
  },
  {
    "text": "that we created over\nsigma 1 up to sigma n. And you have this sup.",
    "start": "4198390",
    "end": "4203489"
  },
  {
    "start": "4203490",
    "end": "4213970"
  },
  {
    "text": "And we are going\nto claim that this is basically similar to\nthis without expectation",
    "start": "4213970",
    "end": "4222930"
  },
  {
    "text": "with high probability. ",
    "start": "4222930",
    "end": "4227950"
  },
  {
    "text": "So with high probability. And the probability is over\nthe ruggedness of Z1 up to Zn.",
    "start": "4227950",
    "end": "4233970"
  },
  {
    "text": "So you still have\nto draw Z1 up to Zn. But for most of the\nchoice of Z1 up to Zn,",
    "start": "4233970",
    "end": "4240750"
  },
  {
    "text": "there's two things are similar. ",
    "start": "4240750",
    "end": "4246640"
  },
  {
    "text": "So this is the random variable\nthat depends on Z1 up to Zn. This is a random-- this\nis just a constant.",
    "start": "4246640",
    "end": "4252270"
  },
  {
    "text": "This number is not\na random number. It's-- probably shouldn't\ncall it constant. This is a deterministic\nnumber, right?",
    "start": "4252270",
    "end": "4259230"
  },
  {
    "text": "So this number depends\non Z1 up to Zn. And I'm claiming that the\nsecond one, the right-hand side,",
    "start": "4259230",
    "end": "4265460"
  },
  {
    "text": "is concentrating\naround the first one with high probability.",
    "start": "4265460",
    "end": "4272920"
  },
  {
    "text": "So and if we can do\nthis, then that's what we I alluded to before. So now if this is defined\nto be empirical Rademacher",
    "start": "4272920",
    "end": "4281430"
  },
  {
    "text": "complexity.  I guess, let me have\na notation for that.",
    "start": "4281430",
    "end": "4288480"
  },
  {
    "start": "4288480",
    "end": "4297760"
  },
  {
    "text": "This is called, I\nthink let me just--",
    "start": "4297760",
    "end": "4303467"
  },
  {
    "text": "I think in the notes there\nis a formal definition. But here, I mean, just\nfor the sake of time, let's just define this\nto be R s of f, where",
    "start": "4303467",
    "end": "4311820"
  },
  {
    "text": "s is the set of Z1 up to Zn. And this is called empirical\nRademacher complexity.",
    "start": "4311820",
    "end": "4321910"
  },
  {
    "text": "And you can see that the\noriginal Rademacher complexity, the average\nRademacher complexity is the expectation of\nthe empirical Rademacher",
    "start": "4321910",
    "end": "4330160"
  },
  {
    "text": "complexity, where you take\nexpectation over the set s, right?",
    "start": "4330160",
    "end": "4336190"
  },
  {
    "text": "So just because\nthese two things only differ by a single expectation.",
    "start": "4336190",
    "end": "4342010"
  },
  {
    "text": "And so, if you can\ndo this, then you",
    "start": "4342010",
    "end": "4347550"
  },
  {
    "text": "have a high probability bound. You don't have to\naverage over Z1 up to Zn. And also, you can\ndo the same thing for the left-hand side for\nthe uniform convergence thing.",
    "start": "4347550",
    "end": "4355210"
  },
  {
    "text": "So recall that\nbefore we only prove that the expectation of Z1\nup to Zn sup, this minus ef,",
    "start": "4355210",
    "end": "4372363"
  },
  {
    "text": "we want to prove\nthat this is less than Rademacher complexity. So this one, we will\nalso show that this",
    "start": "4372363",
    "end": "4379180"
  },
  {
    "text": "is approximately equals to\nthis with high probability.",
    "start": "4379180",
    "end": "4385040"
  },
  {
    "text": "I guess, I should say,\nthe later one, this one, later one is a random variable\nthat depends on Z1 up to Zn",
    "start": "4385040",
    "end": "4394360"
  },
  {
    "text": "is approximately equal\nto the expectation with higher probability.",
    "start": "4394360",
    "end": "4399860"
  },
  {
    "text": "So if you have\nboth of those, then you basically remove the\nexpectation from your equation and you get a high\nprobability bound.",
    "start": "4399860",
    "end": "4406050"
  },
  {
    "start": "4406050",
    "end": "4411579"
  },
  {
    "text": "Does that make sense? Is there any questions? So basically, eventually\nwe're going to prove this.",
    "start": "4411580",
    "end": "4418090"
  },
  {
    "text": "So let me state\nthe formal theorem. ",
    "start": "4418090",
    "end": "4423699"
  },
  {
    "text": "We can prove that you seem\nall the f is are bounded, then",
    "start": "4423700",
    "end": "4442030"
  },
  {
    "text": "with probability, at least 1\nminus delta, we have the sup--",
    "start": "4442030",
    "end": "4450849"
  },
  {
    "text": "so here, I don't\nhave expectation. This is the-- there's a\nran-- over the randomness.",
    "start": "4450850",
    "end": "4456100"
  },
  {
    "start": "4456100",
    "end": "4462700"
  },
  {
    "text": "of Z1 up to Zn the sup of\nthis is less than 2 times",
    "start": "4462700",
    "end": "4477590"
  },
  {
    "text": "the empirical\nRademacher complexity, the average\nRademacher complexity,",
    "start": "4477590",
    "end": "4482790"
  },
  {
    "text": "plus additional cost,\nadditional term, which is the log ln of\n2 over delta over 2n.",
    "start": "4482790",
    "end": "4492170"
  },
  {
    "text": "So you pay additional small\nterm, which is on order of 1 over square root of n\ntimes something logarithmic",
    "start": "4492170",
    "end": "4499460"
  },
  {
    "text": "and a probability delta. So and but basically\nby paying this,",
    "start": "4499460",
    "end": "4505550"
  },
  {
    "text": "you get a high probability bound\ninstead of an average version. ",
    "start": "4505550",
    "end": "4524070"
  },
  {
    "text": "I think the proof here-- the proof is actually\nrelatively straightforward.",
    "start": "4524070",
    "end": "4530380"
  },
  {
    "text": "It's basically just applying\n[INAUDIBLE] inequality.",
    "start": "4530380",
    "end": "4536469"
  },
  {
    "text": "But maybe let me do that\nin the next lecture. I think it takes\nprobably 10 minutes. ",
    "start": "4536470",
    "end": "4544080"
  },
  {
    "text": "Maybe let me start\nwith the remark. So I guess, typically,\nthis ln 2 delta over n,",
    "start": "4544080",
    "end": "4555780"
  },
  {
    "text": "this is typically much smaller\nthan either the Rademacher complexity or the--",
    "start": "4555780",
    "end": "4562320"
  },
  {
    "text": "either the empirical one\nor the population one. And the reason is\nthat these two things",
    "start": "4562320",
    "end": "4568860"
  },
  {
    "text": "will be something like\nsquare root something over n. And this something depends\non the complexity of f.",
    "start": "4568860",
    "end": "4576389"
  },
  {
    "text": "It's something that\nis not negligible. But here, you have square root--",
    "start": "4576390",
    "end": "4581550"
  },
  {
    "text": "a logarithmic, some\nlogarithmic term over n. So that's pretty\nmuch the smallest thing you can think of, right?",
    "start": "4581550",
    "end": "4586590"
  },
  {
    "text": "So logarithmic, it's\nkind of like a constant. So your complexity\nof f wouldn't be",
    "start": "4586590",
    "end": "4593100"
  },
  {
    "text": "on the logarithmic of anything. It should be something\nbigger than that. So that's why typically this\nadditional term is negligible.",
    "start": "4593100",
    "end": "4601450"
  },
  {
    "text": "So that's why basically\nyou can think of this, you didn't lose anything by\ndoing the empirical version.",
    "start": "4601450",
    "end": "4608970"
  },
  {
    "text": "And it's interesting\nthat what you lose here at least on this level-- what you lose here doesn't\ndepend on complexity of f.",
    "start": "4608970",
    "end": "4615840"
  },
  {
    "text": "So basically, if you-- so this term depends\non complexity of f. But what you lose here\nbetween the expected empirical",
    "start": "4615840",
    "end": "4622650"
  },
  {
    "text": "and population don't\ndepend on complex of f. ",
    "start": "4622650",
    "end": "4635060"
  },
  {
    "text": "And maybe, I think this is\na perfect time for doing the second remark, remark two.",
    "start": "4635060",
    "end": "4641700"
  },
  {
    "text": "So and I guess, Rnf or Rsf, they\nare both translation invariant.",
    "start": "4641700",
    "end": "4649890"
  },
  {
    "start": "4649890",
    "end": "4656930"
  },
  {
    "text": "So what does that mean? That means that suppose\nyou have f prime, which",
    "start": "4656930",
    "end": "4665240"
  },
  {
    "text": "is equal to a translation of\nf, which means that this is",
    "start": "4665240",
    "end": "4670790"
  },
  {
    "text": "a family of function f prime\nZ, which is equal to fZ",
    "start": "4670790",
    "end": "4677230"
  },
  {
    "text": "plus the universal constant Z0. ",
    "start": "4677230",
    "end": "4683250"
  },
  {
    "text": "So for every function\nin capital F, you have got a function\ncapital F prime, which is just the translation.",
    "start": "4683250",
    "end": "4690239"
  },
  {
    "text": "You just add some Z0 to it. Then they have the same\nempirical Rademacher",
    "start": "4690240",
    "end": "4701630"
  },
  {
    "text": "complexity. ",
    "start": "4701630",
    "end": "4709850"
  },
  {
    "text": "And in some sense, we have\nseen this derivation somewhere in this involved\nderivations before.",
    "start": "4709850",
    "end": "4716480"
  },
  {
    "text": "But let me just make\nit more explicit. So the Rademacher\ncomplexity of this",
    "start": "4716480",
    "end": "4722489"
  },
  {
    "text": "is, you look at the\nexpectation of sigma. And you take the sup of sum of\nsigma i f i prime, f prime Zi.",
    "start": "4722490",
    "end": "4738449"
  },
  {
    "start": "4738450",
    "end": "4744160"
  },
  {
    "text": "And you plug in a\ndefinition plus the Z0.",
    "start": "4744160",
    "end": "4755235"
  },
  {
    "start": "4755235",
    "end": "4762790"
  },
  {
    "text": "So now, you can put\nthe part about Z0. I think we have seen the\nsame technique before.",
    "start": "4762790",
    "end": "4769690"
  },
  {
    "text": "Because Z0 is not a\nfunction of little f. So you can put it out. So you get plus 1 over n\ntimes sum of sigma i times Z0.",
    "start": "4769690",
    "end": "4788250"
  },
  {
    "text": "And then you can swap\nexpectation with the sum. So you get expectation sigma\nis sup plus expectation of 1",
    "start": "4788250",
    "end": "4798000"
  },
  {
    "text": "over n times sum of sigma i Z0. And this becomes 0 because\nsigma i is a binary--",
    "start": "4798000",
    "end": "4806380"
  },
  {
    "text": "or is a Rademacher\nrandom variable. So then this is Rs of f. ",
    "start": "4806380",
    "end": "4813770"
  },
  {
    "text": "So in some sense,\nthis is a property of the Rademacher complexity,\nwhich is somewhat interesting.",
    "start": "4813770",
    "end": "4823170"
  },
  {
    "text": "You don't care about\nthe translation. But you do care about the scale. So if you scale\neverything by 1/2 or by 2,",
    "start": "4823170",
    "end": "4828850"
  },
  {
    "text": "then you would change the\nRademacher complexity. But it wouldn't change\nwhen you shift things. So it's about the\nrelative differences",
    "start": "4828850",
    "end": "4834960"
  },
  {
    "text": "between functions in f. It's not about\nabsolute size of f.",
    "start": "4834960",
    "end": "4840739"
  },
  {
    "text": "Or so, for example,\nif the function of f always takes values between 1\n[INAUDIBLE] and 1 [INAUDIBLE]",
    "start": "4840740",
    "end": "4846390"
  },
  {
    "text": "1, that's not very\ndifferent from taking values between 0 and 1.",
    "start": "4846390",
    "end": "4851530"
  },
  {
    "text": "OK. I think this is a natural\nstopping point for today. Any questions?",
    "start": "4851530",
    "end": "4858788"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4858789",
    "end": "4870030"
  },
  {
    "text": "First of all, it's not\nalways the case that-- right, that's a good question. So I claimed this vaguely\nwithout any justification.",
    "start": "4870030",
    "end": "4877920"
  },
  {
    "text": "So why the Rademacher\ncomplexity should be like this, like 1 over square root of n. So I think it's not even--",
    "start": "4877920",
    "end": "4886360"
  },
  {
    "text": "so I should say, it's not\nactually exactly true. For most of the cases--\nactually for all the cases we are going to see\nin the lectures,",
    "start": "4886360",
    "end": "4894909"
  },
  {
    "text": "it's 1 over square root of n. But in some cases, it could be-- the dependency alone could\nbe a little bit different.",
    "start": "4894910",
    "end": "4902240"
  },
  {
    "text": "So yeah. So sorry. I was not quite clear. And I think-- I'm not sure\nwhether that question is still in the homework. I think in the homework\nquestion there used",
    "start": "4902240",
    "end": "4908732"
  },
  {
    "text": "to be a question where you\nhave other dependency on. I think I probably removed\nthat question for this year.",
    "start": "4908732",
    "end": "4916420"
  },
  {
    "text": "I remember I remove it\njust because it's not that relevant to the overall goal. But there could be\nother dependencies.",
    "start": "4916420",
    "end": "4924160"
  },
  {
    "text": "For some reason, it's always\nlike, it's mostly the cases 1 over square root of n,\nI think the reason is",
    "start": "4924160",
    "end": "4929530"
  },
  {
    "text": "that even you look\nat a single example,",
    "start": "4929530",
    "end": "4938150"
  },
  {
    "text": "you don't take the sup. We just look at it-- you do the wrong thing.",
    "start": "4938150",
    "end": "4944110"
  },
  {
    "text": "You say, I fixed my function\nand then I draw my data. I look at how different\nthe empirical one",
    "start": "4944110",
    "end": "4950400"
  },
  {
    "text": "is different from\nthe population one. That's always 1 over square\nroot n without any doubt.",
    "start": "4950400",
    "end": "4956790"
  },
  {
    "text": "So that's why you can\nnever go better than 1 over square root n. But you can be worse than\n1 over square root n. I'm not sure whether\nthat makes sense, right?",
    "start": "4956790",
    "end": "4963480"
  },
  {
    "text": "So even you look at a single-- the concentration of a single-- at a single f, right,\nyou fixed the function h,",
    "start": "4963480",
    "end": "4969920"
  },
  {
    "text": "you draw the random\nvariable Z1 up to Zn, and you still have\nsome fluctuation on all the 1 over square root n.",
    "start": "4969920",
    "end": "4975480"
  },
  {
    "text": "And so, you cannot repeat that. But it could be worse than that. [INAUDIBLE]",
    "start": "4975480",
    "end": "4982090"
  },
  {
    "text": " From definition of\nthe Rademacher--",
    "start": "4982090",
    "end": "4988550"
  },
  {
    "text": "I think can still see\nthat to some extent. Because if you look at the sum\nof sigma ifi, maybe that's just",
    "start": "4988550",
    "end": "4997699"
  },
  {
    "text": "be here. So this is still\na sum of n terms. And so, even you\ndon't take the sup,",
    "start": "4997700",
    "end": "5005620"
  },
  {
    "text": "this term would be\nsomething on order of 1 over square root n just\nbecause of the concentration.",
    "start": "5005620",
    "end": "5011740"
  },
  {
    "text": "You have sum of n terms, each\nof them is on the order of 1. And so, the sum of the n terms\nis on order of square root n.",
    "start": "5011740",
    "end": "5018260"
  },
  {
    "text": "And then you divide by n,\nyou get 1 over square root n. ",
    "start": "5018260",
    "end": "5026100"
  },
  {
    "text": "We can talk more\noffline maybe. yeah. Sounds good. I guess I will see you\non Monday-- on Wednesday.",
    "start": "5026100",
    "end": "5033460"
  },
  {
    "start": "5033460",
    "end": "5038000"
  }
]