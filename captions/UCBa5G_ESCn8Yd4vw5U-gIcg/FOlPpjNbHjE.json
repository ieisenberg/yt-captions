[
  {
    "text": " Hey, everybody, we're going\nto go ahead and get started.",
    "start": "0",
    "end": "7210"
  },
  {
    "text": "And we'll start with a\nrefresher understanding, thinking back to DPO and RLHF. ",
    "start": "7210",
    "end": "50178"
  },
  {
    "text": "All right, why don't you\nturn to someone near you and see if you got the\nsame answer, particularly for the third and fourth one?",
    "start": "50178",
    "end": "57899"
  },
  {
    "text": "But-- ",
    "start": "57900",
    "end": "104845"
  },
  {
    "text": "All right, so let's\ncome back together. The first one is true. The DPO model does\nassume that we",
    "start": "104845",
    "end": "110560"
  },
  {
    "text": "have a particular\nmodel of how people are responding to preferences,\nin particular the Bradley-Terry model.",
    "start": "110560",
    "end": "115640"
  },
  {
    "text": "The second one is also true. Even though we've been thinking\na lot about when we actually have preferences, we can\nalso use this in cases",
    "start": "115640",
    "end": "124420"
  },
  {
    "text": "where someone just directly\nprovided you reward labels. And so RLHF is a paradigm.",
    "start": "124420",
    "end": "130278"
  },
  {
    "text": "It's totally compatible with\nthe idea of just getting rewards from some way. But normally, when we think\nabout that human feedback,",
    "start": "130278",
    "end": "137240"
  },
  {
    "text": "it's normally from preferences. The third one is\nan interesting one. Does somebody want\nto argue why they",
    "start": "137240",
    "end": "143140"
  },
  {
    "text": "think that is not a good way\nto learn about the reward model for board games?",
    "start": "143140",
    "end": "148530"
  },
  {
    "text": "With multiple optimal\npoints, you know what is-- Yeah, you feel there's\nmultiple options.",
    "start": "148530",
    "end": "153540"
  },
  {
    "text": "Yeah, I think that could be one. I was thinking of\nsomething even simpler. Does anybody else want to\nadd why you might not want",
    "start": "153540",
    "end": "158987"
  },
  {
    "text": "to do this for board games? It might be really hard\nto compare this in a game",
    "start": "158987",
    "end": "165599"
  },
  {
    "text": "like chess, where the\nreward is at the end. And self-play\nmight just be that.",
    "start": "165600",
    "end": "170910"
  },
  {
    "text": "Yeah. So it might as well-- and\nalso because we normally know what the reward model\nactually is in games. And so if we know\nthat at the very end,",
    "start": "170910",
    "end": "177610"
  },
  {
    "text": "we can say this is a plus\n1 or this is a minus 1, there is no reason,\nnecessarily, to assume",
    "start": "177610",
    "end": "182640"
  },
  {
    "text": "we want to look\nat two-game states and ask a human to try to judge\nwhich of those two is better. We know the ground truth reward.",
    "start": "182640",
    "end": "188410"
  },
  {
    "text": "And so it's probably better\njust to directly use those. And it may be that\nthose pairwise rankings",
    "start": "188410",
    "end": "194190"
  },
  {
    "text": "for intermediate game states\nmight not be very reliable, too. And then the last\none is also true.",
    "start": "194190",
    "end": "199570"
  },
  {
    "text": "DPO and RLHF can both be\nused in extremely large-- with extremely large\npolicy networks.",
    "start": "199570",
    "end": "206190"
  },
  {
    "text": "All right, so where are we? Last time, we talked a lot\nabout Monte Carlo tree search, and we talked about AlphaZero.",
    "start": "206190",
    "end": "211750"
  },
  {
    "text": "And what we'll do today is\nwe'll talk briefly to finish up that part, just quite briefly.",
    "start": "211750",
    "end": "217775"
  },
  {
    "text": "I want to clarify\na couple things that I mentioned last time. So the rest of the\nday, we're going to have a guest lecture,\nalmost the rest of the day.",
    "start": "217775",
    "end": "223450"
  },
  {
    "text": "We're going to have a guest\nlecture by Dan Webber, which is a way to introduce the\nlast part of our course, which",
    "start": "223450",
    "end": "229620"
  },
  {
    "text": "is to think about where the\nrewards come from in terms of how we make judgments\nabout which rewards",
    "start": "229620",
    "end": "234750"
  },
  {
    "text": "we might want to prefer or not. And we'll talk about that today. And we'll talk about that\nafter the quiz as well.",
    "start": "234750",
    "end": "240850"
  },
  {
    "text": "But before we do that, I just\nwant to do a little bit more on Monte Carlo tree search. So let's see if we can get\ntake 2 for the video first.",
    "start": "240850",
    "end": "250220"
  },
  {
    "start": "250220",
    "end": "256489"
  },
  {
    "text": "So I think that sort of\ncaptures-- it's a nice-- we don't normally\nget documentaries made about the work that happens\nin artificial intelligence,",
    "start": "256490",
    "end": "263040"
  },
  {
    "text": "at least not yet. But I think that it's a pretty\npowerful examine of why people were so excited about this\nresult and the implications it",
    "start": "263040",
    "end": "271100"
  },
  {
    "text": "had when you can exceed the\nbest performance in the world at something by computers.",
    "start": "271100",
    "end": "277460"
  },
  {
    "text": "And we've seen examples\nof this in the past. For those of you\nwho heard about it, there was an IBM Watson for\nJeopardy case a number of years",
    "start": "277460",
    "end": "285880"
  },
  {
    "text": "ago. And I remember being\nin the audience when that was happening. Many, many people watched it\nin different watching parties",
    "start": "285880",
    "end": "292449"
  },
  {
    "text": "at the time, and I was in one\nof those watching parties. And it was a similar\nmoment in AI, thinking about what are\nthe levels that we're",
    "start": "292450",
    "end": "299270"
  },
  {
    "text": "going to be able\nto achieve in AI and what are the implications\nof that for of human expertise and human excellence.",
    "start": "299270",
    "end": "304880"
  },
  {
    "text": "So Monte Carlo tree\nsearch, of course-- so, of course, they\ndid win the game. Deepmind did win\nagainst Lee Sedol.",
    "start": "304880",
    "end": "312290"
  },
  {
    "text": "Let's now just go\nback and think a bit about what Monte Carlo tree\nsearch and AlphaZero are doing.",
    "start": "312290",
    "end": "318460"
  },
  {
    "text": "So this is another\nrefresher understanding. And I'm also doing two\nof these today, just to give you an example. These are the types of questions\nyou also might see on the quiz.",
    "start": "318460",
    "end": "325570"
  },
  {
    "text": "So we'll do another\none of these. And then I'm going to clarify\na couple points about AlphaZero",
    "start": "325570",
    "end": "330930"
  },
  {
    "text": "from last time. ",
    "start": "330930",
    "end": "405937"
  },
  {
    "text": "Why don't you find somebody near\nyou and compare your answers? ",
    "start": "405937",
    "end": "523669"
  },
  {
    "text": "OK, good. I'm hearing a lot of discussion\nabout this, which is good. So the first one is true.",
    "start": "523669",
    "end": "529130"
  },
  {
    "text": "The first one is it does\napproximates a forward search tree.",
    "start": "529130",
    "end": "534270"
  },
  {
    "text": "The second one is\nfalse, and I know this is a little bit subtle. So Monte Carlo tree search\ntries to approximate",
    "start": "534270",
    "end": "541579"
  },
  {
    "text": "the forward search tree. But as you might remember,\nthe forward search tree can scale exponentially\nwith the number of states",
    "start": "541580",
    "end": "547627"
  },
  {
    "text": "and the number of actions\nbecause you're expanding by both of those at each level. And so what Monte Carlo\ntree search does is it",
    "start": "547627",
    "end": "554420"
  },
  {
    "text": "uses its dynamics model\nto sample a next state. So you don't have to enumerate\nall the possible next states.",
    "start": "554420",
    "end": "560120"
  },
  {
    "text": "So it uses sampling to help\nwith the state branching factor, but it doesn't tell you what\nto do about the action factor.",
    "start": "560120",
    "end": "568263"
  },
  {
    "text": "One thing you can do is you\ncan do upper confidence trees. And then that tells you how\nto use a bandit to figure out",
    "start": "568263",
    "end": "574070"
  },
  {
    "text": "which action to select next. In AlphaZero, we see\nthat even that is likely not to be sufficient\nwhen you have",
    "start": "574070",
    "end": "579649"
  },
  {
    "text": "an enormous branching factor. And so you may need some\nsort of additional weight, like the probability,\nlike a policy",
    "start": "579650",
    "end": "585570"
  },
  {
    "text": "to select among those actions. The second one is-- the\nthird one is also false.",
    "start": "585570",
    "end": "591240"
  },
  {
    "text": "So this was true in\nthe original AlphaGo. And I think in the Lee Sedol 1,\n2, that you saw in the video,",
    "start": "591240",
    "end": "597610"
  },
  {
    "text": "they did have two networks. But in AlphaZero, they\njust have a single network, and it outputs both\na policy and a value.",
    "start": "597610",
    "end": "604520"
  },
  {
    "text": "So it just has two output heads. The third thing is true. So amazingly, even\nif you spend 40 days",
    "start": "604520",
    "end": "612810"
  },
  {
    "text": "and you've got many\nTPUs, et cetera to learn a policy output and\na value output in the network,",
    "start": "612810",
    "end": "619900"
  },
  {
    "text": "they still do it at test time. Like if you're\nplaying Lee Sedol, they still do additional\nguided Monte Carlo tree",
    "start": "619900",
    "end": "626040"
  },
  {
    "text": "search at that point, and\nit makes a big difference. So I think it was something like\ngoing from an Elo score of 3,000",
    "start": "626040",
    "end": "632740"
  },
  {
    "text": "to 4,500 or 5,000. I'm going to get\nthe numbers wrong, but it was a huge gain by\ndoing a little bit more",
    "start": "632740",
    "end": "639810"
  },
  {
    "text": "of extra local computation. And the third thing-- the\nfourth thing is also--",
    "start": "639810",
    "end": "645029"
  },
  {
    "text": "or I guess the next\nthing is also true, which is self-play does form-- provides a form of implicit\ncurriculum learning,",
    "start": "645030",
    "end": "651210"
  },
  {
    "text": "because the agent is\nalways essentially working with an opponent that's\nvery similar to its level.",
    "start": "651210",
    "end": "657010"
  },
  {
    "text": "In fact, itself. So it's exactly at its level. And that means that the\ndensity of reward it gets",
    "start": "657010",
    "end": "662293"
  },
  {
    "text": "is going to be much\nhigher than it would get if it was playing\nan opponent that was much higher or much lower.",
    "start": "662293",
    "end": "668940"
  },
  {
    "text": "The other thing that\nI wanted to clarify is-- so when we talked before,\nwe talked about selecting",
    "start": "668940",
    "end": "675060"
  },
  {
    "text": "a move in a single game. And we talked about how\nit both maintains this Q-- ",
    "start": "675060",
    "end": "681900"
  },
  {
    "text": "maintains a Q s, a for\na certain node, as well as this upper bound\nhere, which is",
    "start": "681900",
    "end": "687570"
  },
  {
    "text": "going to be proportional to\nthis policy function that gets from the neural\nnetwork divided by 1",
    "start": "687570",
    "end": "695880"
  },
  {
    "text": "plus the number of samples. So I mentioned in class-- I just wanted to make\nsure to clarify this.",
    "start": "695880",
    "end": "701582"
  },
  {
    "text": "I mentioned in class that I\nthought that all of the S's here are just the nodes. But S is a little bit\nweird of a notation",
    "start": "701582",
    "end": "708392"
  },
  {
    "text": "because you could\nimagine it could either be the state space or the node,\nlike that part of the tree search.",
    "start": "708392",
    "end": "713950"
  },
  {
    "text": "I looked back on it. This is actually the node. So I was correcting\nwhat I said last time, but I just wanted to\nmake sure that was clear.",
    "start": "713950",
    "end": "719737"
  },
  {
    "text": "So they're thinking of\neach of these points as being like a particular s, a. But in theory-- and again,\nI'm not a Go expert.",
    "start": "719737",
    "end": "726760"
  },
  {
    "text": "So I'm not sure how\noften this happens. You could end up at\nthe same game state lower down in the tree, and you\nwould maintain totally different",
    "start": "726760",
    "end": "734200"
  },
  {
    "text": "statistics for down there. So you're not\nsharing across those. And there's just\nsimplicity to do",
    "start": "734200",
    "end": "739660"
  },
  {
    "text": "that in terms of--\nthere's simplicity-- that can help in terms\nof the architectures that you need to derive\nand just simplify",
    "start": "739660",
    "end": "747010"
  },
  {
    "text": "some of the storage for this. So it is-- these are per node.",
    "start": "747010",
    "end": "758640"
  },
  {
    "text": "And then the other thing that\nI just wanted to clarify also is that when you get\nto the root later",
    "start": "758640",
    "end": "765720"
  },
  {
    "text": "and you're making a decision\nover which of these actions to take, I mentioned that\nwhat they do at the final end",
    "start": "765720",
    "end": "773238"
  },
  {
    "text": "is that they're\ngoing to do something proportional to an s, a at the\nroot divided by 1 over tau.",
    "start": "773238",
    "end": "780310"
  },
  {
    "text": "So this is going to be\nthe policy at the root.  I just wanted to make sure to\nbe clear about what that does.",
    "start": "780310",
    "end": "788199"
  },
  {
    "text": "So this is sort of\nprioritizing parts of actions that you've taken more,\nthat you've explored more",
    "start": "788200",
    "end": "793290"
  },
  {
    "text": "in your neural network. So let's just see a little bit\nabout what this would look like. So if tau is equal to\n1, what that would mean",
    "start": "793290",
    "end": "799290"
  },
  {
    "text": "is that your probability of\ntaking action A from the root would be equal to\nthe number of times",
    "start": "799290",
    "end": "806280"
  },
  {
    "text": "you've taken action\nA in the root divided by the sum of the\ntimes you've taken.",
    "start": "806280",
    "end": "811503"
  },
  {
    "text": "Well, just really the\ntotal number of rollouts you've done from the root. So that would be\nstrictly proportional.",
    "start": "811503",
    "end": "817810"
  },
  {
    "text": "In that case, if you\nhave a tau less than 1, that means that you are going\nto upway some of these things.",
    "start": "817810",
    "end": "826509"
  },
  {
    "text": "So then you would have-- so let's say if tau\nis equal to 0.5, then you would have an A root\nsquared divided by the sum of N",
    "start": "826510",
    "end": "837700"
  },
  {
    "text": "a, root squared. I'll put a prime there. So what that would mean\nis that if you make tau",
    "start": "837700",
    "end": "845170"
  },
  {
    "text": "closer and closer to 0,\nthen this is basically going to do a\nwinners-takes-all approach, and you'll basically select\nwhichever action you took most",
    "start": "845170",
    "end": "852670"
  },
  {
    "text": "from the root. And then as you go\nto more towards 1, it's sort of equally spread\nacross all the times you've",
    "start": "852670",
    "end": "858370"
  },
  {
    "text": "taken each of the actions. And as you might\nimagine, that's going to have different implications\nfor how exploratory you are.",
    "start": "858370",
    "end": "864982"
  },
  {
    "text": "Now, note that none\nof these things are doing it based on what\nthe value is at the root. All of these are just based on\nessentially how much of the time",
    "start": "864982",
    "end": "871300"
  },
  {
    "text": "you've explored different\nparts of the tree. So I just wanted to make sure\nto clarify those in those cases.",
    "start": "871300",
    "end": "877420"
  },
  {
    "text": "So I may have any other\nquestions about alpha 0. And I do just want\nto say that I-- as I mentioned before,\nthere's a number",
    "start": "877420",
    "end": "883990"
  },
  {
    "text": "of different other derivatives\nthat have happened since this. So there's mu 0, which\ndoesn't even need to know the rules of the game.",
    "start": "883990",
    "end": "890050"
  },
  {
    "text": "And there are also a lot\nof sophisticated approaches which have to do with hidden\ninformation, games like poker.",
    "start": "890050",
    "end": "896150"
  },
  {
    "text": "So in this case, there's\nfull information. You know exactly where\nall the white stones are. You know exactly where\nall the black stones are.",
    "start": "896150",
    "end": "902750"
  },
  {
    "text": "There's no hidden information\nthat either player has, and there's only two players. But there's been a lot of\nwork on thinking about cases,",
    "start": "902750",
    "end": "909279"
  },
  {
    "text": "like poker and\nothers, where there's some cards that one agent\ndoesn't see from the other. And so then, how do you play\noptimally in those games",
    "start": "909280",
    "end": "916210"
  },
  {
    "text": "as well? So do you have any\nquestions before we move on to our guest lecture?",
    "start": "916210",
    "end": "921339"
  },
  {
    "text": "Yeah. With these other models,\nspecifically user that doesn't make\nthe rules the game,",
    "start": "921340",
    "end": "926830"
  },
  {
    "text": "do we just observe that\neven though it doesn't know the rules, it\nlearns just as well, or does it do better\nwithout knowing the rules?",
    "start": "926830",
    "end": "933379"
  },
  {
    "text": "What's the consequences\nof doing that? Yeah, that's a great question. I'd have to go back to the paper\nand remember the exact results.",
    "start": "933380",
    "end": "938389"
  },
  {
    "text": "It certainly can\ndo just as well. So it can quickly-- you still\nhave to give it some feedback.",
    "start": "938390",
    "end": "943670"
  },
  {
    "text": "It has to know\nwhether or not it won, but it doesn't have to know\nall the individual rules. And so you can--",
    "start": "943670",
    "end": "950860"
  },
  {
    "text": "I just don't remember\nhow much additional data you needed in that case. As you guys might\nremember from last time,",
    "start": "950860",
    "end": "957080"
  },
  {
    "text": "we saw that there is a\nreally substantial impact of architecture. So depending on the\narchitectures you're using",
    "start": "957080",
    "end": "962699"
  },
  {
    "text": "and we're using a\nconvolutional neural net or some other different\ntypes of networks, those also make a\nmassive difference",
    "start": "962700",
    "end": "967900"
  },
  {
    "text": "to the amount of data you need\nand the quality of the result. So I think that's\nsomething to keep in mind when we think of\nremoving information,",
    "start": "967900",
    "end": "974470"
  },
  {
    "text": "like what the rules are. You could imagine that if\nyou do that but then you have some other innovations in\nterms of the architecture,",
    "start": "974470",
    "end": "980880"
  },
  {
    "text": "it might be that you need only\nthe same amount of data or even less than what\nwe're needed here. So generally, they\ndon't do full ablations",
    "start": "980880",
    "end": "987350"
  },
  {
    "text": "over all the combinatorics of\nthe ways these systems just specified. It's a good question. But that work\ncertainly suggested--",
    "start": "987350",
    "end": "993780"
  },
  {
    "text": "and they've also extended\nthis to other games and things like chess and\nothers just to show that you can use very\nsimilar techniques to conquer",
    "start": "993780",
    "end": "1000727"
  },
  {
    "text": "those games as well.  All right. With that, let's\nswitch over to Dan.",
    "start": "1000727",
    "end": "1006980"
  },
  {
    "text": "So I'm really delighted\nto have Dan talking today. He is-- I guess I'll keep\nthis here until he comes up.",
    "start": "1006980",
    "end": "1012620"
  },
  {
    "text": "He is a postdoc fellow\nhere at Stanford. He'll introduce his own\nbackground a little bit more,",
    "start": "1012620",
    "end": "1017940"
  },
  {
    "text": "but he has a lot of\nexpertise and thinking about different frameworks for\nhow do we think about rewards",
    "start": "1017940",
    "end": "1023513"
  },
  {
    "text": "and what are the implications\nof the different ways. We're going to define those in\nterms of the subsequent type of systems we might develop.",
    "start": "1023513",
    "end": "1029425"
  },
  {
    "start": "1029425",
    "end": "1041204"
  },
  {
    "text": "Please, please\nhold your applause until you see how\nit actually goes. Oh, yeah. ",
    "start": "1041204",
    "end": "1048010"
  },
  {
    "text": "All right, I'm going to need\na sec to get this hooked up. While I do that,\nI should note, I",
    "start": "1048010",
    "end": "1053130"
  },
  {
    "text": "am going to ask you,\nat various points maybe, to talk to some\nof the folks next to you.",
    "start": "1053130",
    "end": "1058550"
  },
  {
    "text": "So if you're not in a\ngood position to do that, that might be a time to move to\nget yourself in such a position.",
    "start": "1058550",
    "end": "1067990"
  },
  {
    "text": "OK, is that good? Is that too loud? Just right? Love it. ",
    "start": "1067990",
    "end": "1077340"
  },
  {
    "text": "OK. And we go to 250. Yes? 250. Yep, perfect or-- or\nwe can at any rate.",
    "start": "1077340",
    "end": "1087000"
  },
  {
    "text": " Great.",
    "start": "1087000",
    "end": "1093559"
  },
  {
    "text": "OK. So yeah, I'm Dan.",
    "start": "1093560",
    "end": "1098639"
  },
  {
    "text": "Dan Webber. Here today to talk to you\nabout value alignment. But before we do that,\nmaybe it is just worth",
    "start": "1098640",
    "end": "1105650"
  },
  {
    "text": "saying a little bit\nabout, who is this guy? Why should we listen\nto him or care at all",
    "start": "1105650",
    "end": "1111560"
  },
  {
    "text": "about what he has to say? He's not the professor. So as I mentioned,\nI am a postdoc here",
    "start": "1111560",
    "end": "1119360"
  },
  {
    "text": "at Stanford in HAI and EIS. That's the Institute for Human\nCentered Artificial Intelligence",
    "start": "1119360",
    "end": "1126919"
  },
  {
    "text": "and the Center for\nEthics and Society. If you've taken a lot of\nCS classes at Stanford,",
    "start": "1126920",
    "end": "1132938"
  },
  {
    "text": "you've probably seen\nsomebody who has my job at some point or other.",
    "start": "1132938",
    "end": "1138140"
  },
  {
    "text": "Yeah, big part of my\njob is embedding ethics into computer science\ncourses like this one.",
    "start": "1138140",
    "end": "1144410"
  },
  {
    "text": "Before I came here to Stanford,\nI got my PhD in Philosophy at the University of Pittsburgh,\nwhere I wrote my dissertation",
    "start": "1144410",
    "end": "1152640"
  },
  {
    "text": "on moral theory, which\nbasically means just trying",
    "start": "1152640",
    "end": "1158400"
  },
  {
    "text": "really hard, maybe too hard\nto think systematically about value, which is what\nbrings me to you today.",
    "start": "1158400",
    "end": "1166720"
  },
  {
    "text": "So before that, even\nI got my bachelor's in Computer Science\nat Amherst, I",
    "start": "1166720",
    "end": "1172625"
  },
  {
    "text": "did a couple of years in\nsoftware development after that. So I'm not completely new to CS.",
    "start": "1172625",
    "end": "1179860"
  },
  {
    "text": "I know this world a little bit. I did take an\nintroductory course on AI. That was 10 years ago.",
    "start": "1179860",
    "end": "1187799"
  },
  {
    "text": "I think the field has\nchanged immensely since then. I don't even think we covered\nreinforcement learning at all.",
    "start": "1187800",
    "end": "1195220"
  },
  {
    "text": "So you all are going to know\nthe reinforcement learning way better than I do. I'm not here to be\nan expert about that.",
    "start": "1195220",
    "end": "1203610"
  },
  {
    "text": "What I am hoping to do is\ngive you a bit of a window into how to think\nabout value and how",
    "start": "1203610",
    "end": "1210120"
  },
  {
    "text": "it might be more\ncomplicated than you think. So we're not going to solve\nany deep problems about value",
    "start": "1210120",
    "end": "1218490"
  },
  {
    "text": "in the next hour. We're not going to be\nable to go very in depth on a lot of this stuff.",
    "start": "1218490",
    "end": "1224068"
  },
  {
    "text": "If you're interested in\nthat, I recommend courses in the philosophy department. But just try to\ngive you a quick,",
    "start": "1224068",
    "end": "1231150"
  },
  {
    "text": "maybe, lay of the land, sense\nof the range of possibilities when we're talking about\nvalue and value alignment.",
    "start": "1231150",
    "end": "1240810"
  },
  {
    "text": "So, OK, it might help to\nstart with an example of value alignment or maybe, more\naccurately, an example of value",
    "start": "1240810",
    "end": "1249090"
  },
  {
    "text": "misalignment. One of the classic examples in\nthis literature is paperclip AI.",
    "start": "1249090",
    "end": "1257820"
  },
  {
    "text": "But this example from\nNick Bostrom in 2016,",
    "start": "1257820",
    "end": "1263350"
  },
  {
    "text": "maybe you're all used to this\nin reinforcement learning, but it tells you something about\nthe state of this literature",
    "start": "1263350",
    "end": "1268500"
  },
  {
    "text": "that a classic example\ncould be from 2016. So Bostrom describes an AI,\ndesigned to manage production",
    "start": "1268500",
    "end": "1276059"
  },
  {
    "text": "in a factory, which is given\nthe final goal of maximizing the manufacture of paperclips.",
    "start": "1276060",
    "end": "1283980"
  },
  {
    "text": "Does anyone have an idea maybe\nof how this example continues? Maybe you've seen it before.",
    "start": "1283980",
    "end": "1289710"
  },
  {
    "text": "Anyone knows this one? No? OK. Well, in Bostrom's\nexample, at least",
    "start": "1289710",
    "end": "1296400"
  },
  {
    "text": "this AI proceeds by first\nconverting the Earth and then increasingly large chunks\nof the observable universe",
    "start": "1296400",
    "end": "1302430"
  },
  {
    "text": "into paperclips. OK. Now, Bostrom is\nthinking in particular",
    "start": "1302430",
    "end": "1308040"
  },
  {
    "text": "about superintelligent AI. That's what his book is about. So he's got the destruction of\nthe entire universe in view.",
    "start": "1308040",
    "end": "1316620"
  },
  {
    "text": "But even a less\npowerful AI system might pursue a simple goal\nlike this in surprising ways.",
    "start": "1316620",
    "end": "1325429"
  },
  {
    "text": "Does anybody maybe have\na more mundane example of what could go wrong if\nan AI system were, say,",
    "start": "1325430",
    "end": "1334250"
  },
  {
    "text": "in charge of a\npaperclip factory, given no further\ninstruction than to maximize",
    "start": "1334250",
    "end": "1339880"
  },
  {
    "text": "the production of paperclips? Yeah? [INAUDIBLE] people for a lot of\nshifts, like through the night",
    "start": "1339880",
    "end": "1346540"
  },
  {
    "text": "and then fired workers who\ncomplain and hire new ones. Yeah. Good, good, right.",
    "start": "1346540",
    "end": "1351580"
  },
  {
    "text": "Yeah, we could\nmaximize production if only we trapped people\ninside the building and made them work\naround the clock, right?",
    "start": "1351580",
    "end": "1357836"
  },
  {
    "text": "Excellent. Any other-- yeah? [INAUDIBLE] about the quality\nof the paperclips, right?",
    "start": "1357836",
    "end": "1363050"
  },
  {
    "text": "So they could all be really bad. Good. Yes, exactly. It might be that the\neasiest way to maximize",
    "start": "1363050",
    "end": "1369920"
  },
  {
    "text": "the number of paperclips\nI produce is to produce really terrible paperclips. That's not really what I\nwas looking for probably.",
    "start": "1369920",
    "end": "1377080"
  },
  {
    "text": "Great, thank you. Anyone else?  Yeah?",
    "start": "1377080",
    "end": "1382779"
  },
  {
    "text": "I mean, if the price\nof electricity changes at different times\nof day, it could be like trying to make\npaperclips but just",
    "start": "1382780",
    "end": "1389169"
  },
  {
    "text": "economically and efficiently. Yeah. Good, right. So it's maximized the\nnumber of paperclips, but there's no\nsense of other goals",
    "start": "1389170",
    "end": "1398400"
  },
  {
    "text": "that you might also want to\npursue here, like efficiency or minimizing the\namount of electricity",
    "start": "1398400",
    "end": "1404580"
  },
  {
    "text": "you use or anything like that. Great, yeah. Or you could imagine-- I mean, it depends what\nlevers the eye has to pull,",
    "start": "1404580",
    "end": "1411570"
  },
  {
    "text": "but you could imagine it\nrecycling the factory's plumbing for raw materials or\nlocking out humans who",
    "start": "1411570",
    "end": "1418110"
  },
  {
    "text": "could interrupt its process. Something like that. So great.",
    "start": "1418110",
    "end": "1424390"
  },
  {
    "text": "So, in general, we might say\nthe problem of value alignment",
    "start": "1424390",
    "end": "1429630"
  },
  {
    "text": "is this problem of, how do we\ndesign AI agents that will do",
    "start": "1429630",
    "end": "1434850"
  },
  {
    "text": "what we really want them to do? ",
    "start": "1434850",
    "end": "1440000"
  },
  {
    "text": "What we really want is usually\na lot more nuanced than what we say we want, right?",
    "start": "1440000",
    "end": "1447590"
  },
  {
    "text": "Humans work with a lot of\nbackground assumptions, and these assumptions\ncan be hard to formalize,",
    "start": "1447590",
    "end": "1454280"
  },
  {
    "text": "easy to take for granted. If I told you as the\nmanager of the factory",
    "start": "1454280",
    "end": "1460430"
  },
  {
    "text": "to maximize the\nproduction of paperclips, you would realize\nthat you should do that consistent with\nexisting labor laws,",
    "start": "1460430",
    "end": "1468200"
  },
  {
    "text": "or that you should make\npaperclips that actually work, or that you should\nbe on the lookout",
    "start": "1468200",
    "end": "1474800"
  },
  {
    "text": "for keeping your costs\ndown, things like that. But because these can\nbe hard to formalize,",
    "start": "1474800",
    "end": "1483700"
  },
  {
    "text": "they're easy for\nus to forget about. It's hard to solve this\nproblem just by giving better",
    "start": "1483700",
    "end": "1489210"
  },
  {
    "text": "instructions to AI agents. And here, I mean, if anybody\nwants to give it a try,",
    "start": "1489210",
    "end": "1498490"
  },
  {
    "text": "what would be the better-- how would you solve\nthis problem maybe just by trying to give a\nbetter instruction to the AI?",
    "start": "1498490",
    "end": "1508240"
  },
  {
    "text": "Anybody have what\nthey think might be an improvement on just to\nmaximize paperclip production?",
    "start": "1508240",
    "end": "1514605"
  },
  {
    "text": "Yeah? [INAUDIBLE] Too much? Good, yeah. So yeah, specifying,\nA, that you want",
    "start": "1514605",
    "end": "1523210"
  },
  {
    "text": "paper clips of a certain quality\nand giving a sample of what that looks like. Good, that would help--",
    "start": "1523210",
    "end": "1529150"
  },
  {
    "text": "that could help address\nthis problem potentially of, can you maximize production\njust by making worse paperclips?",
    "start": "1529150",
    "end": "1535340"
  },
  {
    "text": "Right? It might not go far\nenough to, say-- and, by the way,\nyou shouldn't work",
    "start": "1535340",
    "end": "1542470"
  },
  {
    "text": "the factory workers around\nthe clock, but great start. Yeah? Maximize the long-run profits\nof the paperclip factory.",
    "start": "1542470",
    "end": "1552039"
  },
  {
    "text": "Good, good. So yeah, giving a broader goal-- right, I want to maximize\nthe production of paperclips,",
    "start": "1552040",
    "end": "1559690"
  },
  {
    "text": "but that's something\nI want probably because I want to\nmaximize the profit that the factory generates. Good.",
    "start": "1559690",
    "end": "1565960"
  },
  {
    "text": "Is that going to be enough\nto avoid all of the problems that we've seen come up?",
    "start": "1565960",
    "end": "1572054"
  },
  {
    "text": " I mean-- yeah? I mean, it's most\nof them, right?",
    "start": "1572055",
    "end": "1577500"
  },
  {
    "text": "We need high-quality paperclips. You can't turn the\nuniverse into paperclips. The profit will be zero.",
    "start": "1577500",
    "end": "1582760"
  },
  {
    "text": "You can't be using\ntoo much electricity or doing things that\nare economically inefficient because it\nwon't be profitable.",
    "start": "1582760",
    "end": "1588909"
  },
  {
    "text": "I mean, the labor\nlaws are probably-- I think that you'd be violating. Yeah, right. I mean, if there's enough people\nwilling to work in this factory,",
    "start": "1588910",
    "end": "1597400"
  },
  {
    "text": "maybe we're able to keep a lid\non how poorly we treat people. We could get away with\nmaximizing profit while still--",
    "start": "1597400",
    "end": "1604980"
  },
  {
    "text": "but good, OK. So that's getting us\nsome of the way there, but still there's a worry about\nessentially treating people",
    "start": "1604980",
    "end": "1615090"
  },
  {
    "text": "well. OK. I mean, we could keep\ndoing this all day.",
    "start": "1615090",
    "end": "1621434"
  },
  {
    "text": "But hopefully, this is a\nlittle bit of an illustration.  Even trying to think\nof better instructions,",
    "start": "1621434",
    "end": "1628260"
  },
  {
    "text": "you might just realize, oh,\nthere's another thing I forgot, there's another thing I forgot.",
    "start": "1628260",
    "end": "1633800"
  },
  {
    "text": "I mean, you can\ncompare this maybe to the difficulty in manually\nspecifying reward functions.",
    "start": "1633800",
    "end": "1639900"
  },
  {
    "text": "I mean, in some sense,\nthis is the same problem. OK, I think I know what the\nthing is that that I want.",
    "start": "1639900",
    "end": "1649127"
  },
  {
    "text": "OK, it turns out to be much\nmore complicated than that, much harder to\nspecify, especially",
    "start": "1649128",
    "end": "1655430"
  },
  {
    "text": "when you're thinking about\nmaking a system that's going to take instructions\nfrom users maybe,",
    "start": "1655430",
    "end": "1661100"
  },
  {
    "text": "who are not experts in\nreinforcement learning. Folks in this room are going to\nbe relatively good at foreseeing",
    "start": "1661100",
    "end": "1669860"
  },
  {
    "text": "these kinds of problems with\ngiving incomplete instructions. If you're designing\na system that's",
    "start": "1669860",
    "end": "1675140"
  },
  {
    "text": "supposed to take instructions\nfrom non-expert users, they might not be so good\nat foreseeing these issues.",
    "start": "1675140",
    "end": "1681170"
  },
  {
    "text": " OK. ",
    "start": "1681170",
    "end": "1687279"
  },
  {
    "text": "Maybe any-- I should-- I say, any questions now? And in general, going forward-- I mean, if anybody has\nany questions at any time,",
    "start": "1687280",
    "end": "1694360"
  },
  {
    "text": "don't hesitate to\nraise your hand.",
    "start": "1694360",
    "end": "1699480"
  },
  {
    "text": "OK. So we have this problem. How do we design AI agents that\nwill do what we really want?",
    "start": "1699480",
    "end": "1706495"
  },
  {
    "text": " But that's a little\nunderspecified, right?",
    "start": "1706495",
    "end": "1712100"
  },
  {
    "text": "I mean, there are lots of things\nthat we might mean by a phrase like \"what we really want.\"",
    "start": "1712100",
    "end": "1718929"
  },
  {
    "text": "So here's one of them. You might think\nvalue alignment is the problem of designing AI\nagents that do what we really",
    "start": "1718930",
    "end": "1725800"
  },
  {
    "text": "intend for them to do. The problem with\nPaperclip AI might",
    "start": "1725800",
    "end": "1731490"
  },
  {
    "text": "be that it failed to derive the\nuser's true intention, which is to, let's say,\nmaximize production",
    "start": "1731490",
    "end": "1738150"
  },
  {
    "text": "subject to certain constraints,\nmaximize production without overworking the workers\nwhile making sufficiently",
    "start": "1738150",
    "end": "1747240"
  },
  {
    "text": "good paperclips and while\nkeeping costs down and so on and so on and so on. Deriving that nuanced,\ncomplicated intention",
    "start": "1747240",
    "end": "1755639"
  },
  {
    "text": "from the underspecified\ninstruction maximize production.",
    "start": "1755640",
    "end": "1761570"
  },
  {
    "text": "If that's how we think\nabout value alignment, then, of course, the\nsolution is going to be",
    "start": "1761570",
    "end": "1767210"
  },
  {
    "text": "to design AI systems\nthat can successfully do this translation, take\nunder-specified instructions,",
    "start": "1767210",
    "end": "1774799"
  },
  {
    "text": "figure out what the user's\nactual intention is that they're trying to express, and\nthen act on that instead.",
    "start": "1774800",
    "end": "1782253"
  },
  {
    "text": " How is this from a\ntechnical perspective?",
    "start": "1782253",
    "end": "1789620"
  },
  {
    "text": "Here's Iason\nGabriel, a researcher in Philosophy and Ethics of AI.",
    "start": "1789620",
    "end": "1796203"
  },
  {
    "text": "So what he says\nabout it, he says, \"This is a significant\nchallenge.\" And he means from a\ntechnical perspective.",
    "start": "1796203",
    "end": "1801520"
  },
  {
    "text": "\"To really grasp the\nintention behind instructions, AI may require a complete\nmodel of human language",
    "start": "1801520",
    "end": "1807880"
  },
  {
    "text": "and interaction, including an\nunderstanding of the culture, institutions, and practices\nthat allow people to understand",
    "start": "1807880",
    "end": "1814840"
  },
  {
    "text": "the implied meaning of terms.\" That's what he said in 2020. How do folks in this room feel\nabout how this quote has aged",
    "start": "1814840",
    "end": "1825770"
  },
  {
    "text": "maybe in the last four years? Does this seem like a\nsignificant technical challenge?",
    "start": "1825770",
    "end": "1833420"
  },
  {
    "text": "Does it seem less\nsignificant maybe than it might have seemed four\nyears ago for any reasons? Seeing shaking your head.",
    "start": "1833420",
    "end": "1839720"
  },
  {
    "text": "Why not? Well, you're probably\ntrying to imply--",
    "start": "1839720",
    "end": "1845060"
  },
  {
    "text": "trying to allude to GPT. But I don't think that's\nenough, because GPT",
    "start": "1845060",
    "end": "1850130"
  },
  {
    "text": "might omit certain\naspects of the world model that might still cause\nloopholes like that.",
    "start": "1850130",
    "end": "1856620"
  },
  {
    "text": "So I don't think the problem\nhas really been solved. Good, yeah.",
    "start": "1856620",
    "end": "1862169"
  },
  {
    "text": "So, yes, I am not a subtle man. I was indeed thinking,\nyeah, require",
    "start": "1862170",
    "end": "1870230"
  },
  {
    "text": "a complete model of human\nlanguage and interaction. Hmm, that maybe sounds like\na model that a lot of folks",
    "start": "1870230",
    "end": "1876350"
  },
  {
    "text": "have been hard at\nwork developing. But, yes, I agree with you.",
    "start": "1876350",
    "end": "1883010"
  },
  {
    "text": "Yeah, so you might think-- yeah, could you use\nsomething like an LLM",
    "start": "1883010",
    "end": "1888550"
  },
  {
    "text": "to affect this translation\nas part of the system? But, yeah, how complete do we\nthink those models really are",
    "start": "1888550",
    "end": "1899070"
  },
  {
    "text": "if I give this\nunder-- if I say-- If I give the ChatGPT the user\nwants to maximize production",
    "start": "1899070",
    "end": "1907400"
  },
  {
    "text": "in the paperclip factory, what\ndo you think they really intend? Is it going to catch all of\nthe nuances that are typically",
    "start": "1907400",
    "end": "1913940"
  },
  {
    "text": "communicated when one human\nis talking to another?",
    "start": "1913940",
    "end": "1919159"
  },
  {
    "text": "Yeah, I agree. There's reason to doubt that,\nbut see what the future holds.",
    "start": "1919160",
    "end": "1925440"
  },
  {
    "text": "But that's the\ntechnical challenge. ",
    "start": "1925440",
    "end": "1931400"
  },
  {
    "text": "This is a philosophical\nchallenge here as well, which is that you might think\nour intentions don't always",
    "start": "1931400",
    "end": "1937160"
  },
  {
    "text": "track what it is\nthat we really want. So classic cases\nof this might be",
    "start": "1937160",
    "end": "1944450"
  },
  {
    "text": "cases of incomplete information\nor imperfect rationality. We've sort of already\nbroached this one.",
    "start": "1944450",
    "end": "1951830"
  },
  {
    "text": "I mean, suppose that\nI intend for the AI to maximize paperclip\nproduction, again,",
    "start": "1951830",
    "end": "1956880"
  },
  {
    "text": "subject to these\nconstraints, because what I want is to maximize return on\nmy investment in the factory.",
    "start": "1956880",
    "end": "1963892"
  },
  {
    "text": "If the AI knows that\nI would get a better return by producing something\nelse or by selling the factory,",
    "start": "1963892",
    "end": "1971419"
  },
  {
    "text": "has it given me what I really\nwant if it does what I intend, which is for it to maximize\npaperclip production?",
    "start": "1971420",
    "end": "1977700"
  },
  {
    "text": "Well, in one sense, yes. But in another sense, no. You might think that other\nsense is the more important one.",
    "start": "1977700",
    "end": "1984780"
  },
  {
    "text": "It's not giving me the\nthing that I really wanted because that\nthing is coming apart from my plan about\nhow to get it.",
    "start": "1984780",
    "end": "1992769"
  },
  {
    "text": " OK.",
    "start": "1992770",
    "end": "1997850"
  },
  {
    "text": "So you might think the\nsolution here is that-- what you really\nwant is an AI agent",
    "start": "1997850",
    "end": "2003790"
  },
  {
    "text": "that does what the user prefers,\nwhat they actually prefer, even if this isn't\nwhat they intend.",
    "start": "2003790",
    "end": "2010030"
  },
  {
    "text": "On this interpretation\nof the problem, Paperclip AI is misaligned\nbecause I prefer that it not",
    "start": "2010030",
    "end": "2015880"
  },
  {
    "text": "destroy the world, or I prefer\nthat it not lock all the users in the factory. Users, all the workers.",
    "start": "2015880",
    "end": "2021809"
  },
  {
    "text": " OK.",
    "start": "2021810",
    "end": "2027466"
  },
  {
    "text": "Now, the problem\nhere is that, if you want to align to what the\nuser actually prefers,",
    "start": "2027466",
    "end": "2033277"
  },
  {
    "text": "there's going to have to\nbe some way for the agent to know what the\nuser prefers when that differs from the intentions\nthat the user expresses.",
    "start": "2033277",
    "end": "2042260"
  },
  {
    "text": "How are you going to\ngo about doing that? ",
    "start": "2042260",
    "end": "2047560"
  },
  {
    "text": "Solution to this might be\nto work with the user's revealed preferences. Preferences that are learned\nfrom observing the user's",
    "start": "2047560",
    "end": "2054908"
  },
  {
    "text": "behavior or feedback. Obviously, you've\nlearned some techniques",
    "start": "2054909",
    "end": "2061540"
  },
  {
    "text": "for how to do this\nkind of thing, but not every technique\nis going to be like this. You're going to have\nto do something,",
    "start": "2061540",
    "end": "2067179"
  },
  {
    "text": "like inverse reinforcement\nlearning or reinforcement learning from human feedback\nthat allows the agent",
    "start": "2067179",
    "end": "2073388"
  },
  {
    "text": "to train on\nobservation of the user to try to determine what they\nprefer based on how they've",
    "start": "2073389",
    "end": "2080440"
  },
  {
    "text": "behaved or what they've\ntold it its preferences are. Of course, you're going\nto run into this problem",
    "start": "2080440",
    "end": "2088270"
  },
  {
    "text": "that from a finite number of\nobservations of the user's behavior or preferences,\nthere are, at least in theory,",
    "start": "2088270",
    "end": "2095239"
  },
  {
    "text": "infinitely many\npreference/functions that could represent inferring\nthat could be a challenge.",
    "start": "2095239",
    "end": "2102610"
  },
  {
    "text": "And it might be especially\nhard to infer preferences about unexpected situations,\nlike emergencies where",
    "start": "2102610",
    "end": "2109720"
  },
  {
    "text": "you don't have any direct-- you're unlikely to have directly\nobserved the user's preferences",
    "start": "2109720",
    "end": "2114910"
  },
  {
    "text": "about unusual\nemergency situations because they arise so rarely.",
    "start": "2114910",
    "end": "2121300"
  },
  {
    "text": "But you might think\nit's precisely an unusual or\nemergency situations, where it's so important\nfor an AI agent",
    "start": "2121300",
    "end": "2127630"
  },
  {
    "text": "to be aligned to our values. So those are some of the\ntechnical challenges.",
    "start": "2127630",
    "end": "2133650"
  },
  {
    "text": "But here, again, we have\na philosophical problem, which is that, just as\nmy intentions can diverge",
    "start": "2133650",
    "end": "2139440"
  },
  {
    "text": "from my preferences, it seems\nlike my preferences can diverge from what's actually\ngood for me,",
    "start": "2139440",
    "end": "2146370"
  },
  {
    "text": "or so some people might think. So, for instance, a lot\nof people prefer to smoke,",
    "start": "2146370",
    "end": "2153180"
  },
  {
    "text": "but you might think it's not\nreally good for them to do that. Or I might prefer to maximize\nprofit on my paperclip factory",
    "start": "2153180",
    "end": "2161670"
  },
  {
    "text": "at all costs, but maybe\nit would be better for me to be less\nfocused on money and spend more time\nwith my family, right?",
    "start": "2161670",
    "end": "2168658"
  },
  {
    "text": "So the thought here is that\nyour preferences might actually,",
    "start": "2168658",
    "end": "2176490"
  },
  {
    "text": "in some cases, come\napart from what's really in your best interests,\nobjectively speaking.",
    "start": "2176490",
    "end": "2183422"
  },
  {
    "text": "And that this is\nsomething that you might try to align an\nAI agent to instead.",
    "start": "2183422",
    "end": "2188430"
  },
  {
    "text": "We want to do what's actually\nin the user's interests, even when that's not what the\nuser themself prefers to do,",
    "start": "2188430",
    "end": "2196230"
  },
  {
    "text": "right? If you think this,\nyou're going to think paperclip AI is\nmisaligned because it's objectively bad for me, for\nthe world, to be destroyed.",
    "start": "2196230",
    "end": "2203240"
  },
  {
    "text": "Or objectively, bad for\nme, for these things, to--",
    "start": "2203240",
    "end": "2208640"
  },
  {
    "text": "the pipes in my factory to be\nripped out or what have you. ",
    "start": "2208640",
    "end": "2215450"
  },
  {
    "text": "Here is a sort of combined\ntechnical and philosophical problem, though, which is that--",
    "start": "2215450",
    "end": "2222970"
  },
  {
    "text": "unlike the intended\nmeaning of my instructions or my revealed\npreferences, what's",
    "start": "2222970",
    "end": "2229690"
  },
  {
    "text": "objectively good for me\nis not something that can be determined empirically.",
    "start": "2229690",
    "end": "2234990"
  },
  {
    "text": "This is a philosophical\nquestion, not a scientific one. So it's not just a\nmatter of building",
    "start": "2234990",
    "end": "2241329"
  },
  {
    "text": "the right model\nof human language or observing the user enough.",
    "start": "2241330",
    "end": "2246450"
  },
  {
    "text": "To figure out what's\nactually in my best interest is not entirely an\nempirical endeavor.",
    "start": "2246450",
    "end": "2252600"
  },
  {
    "text": "You've got to actually do some\nsubstantive moral philosophy to solve this.",
    "start": "2252600",
    "end": "2257849"
  },
  {
    "text": " Now, the bad news for\nsolving this problem",
    "start": "2257850",
    "end": "2264500"
  },
  {
    "text": "is that there's a lot of\ndisagreement about what is objectively good for a person.",
    "start": "2264500",
    "end": "2270410"
  },
  {
    "text": "I say philosophers\ndisagree about this, but I think\nnon-philosophers also disagree about this as well.",
    "start": "2270410",
    "end": "2276987"
  },
  {
    "text": "Is it just a person's own\npleasure or happiness that's",
    "start": "2276987",
    "end": "2282170"
  },
  {
    "text": "good for them, or is\nit the satisfaction of that person's\ndesires or preferences",
    "start": "2282170",
    "end": "2288710"
  },
  {
    "text": "that could be different\nfrom pleasure or happiness? I might have preferences that\nwill be satisfied only after I'm",
    "start": "2288710",
    "end": "2296570"
  },
  {
    "text": "dead or something. I'll never derive any pleasure\nfrom their satisfaction, although they could\nstill be satisfied.",
    "start": "2296570",
    "end": "2303470"
  },
  {
    "text": "Or do we want to say that\nthings like health or safety, knowledge, human\nrelationships, these things",
    "start": "2303470",
    "end": "2310700"
  },
  {
    "text": "are objectively good for us,\neven if we don't enjoy them, don't prefer them?",
    "start": "2310700",
    "end": "2316250"
  },
  {
    "text": "These are all sort of live\noptions in the theory of value. And depending on how you\nanswer this question,",
    "start": "2316250",
    "end": "2323109"
  },
  {
    "text": "you're going to be looking\nat a different kind of value, even if you already know\nthat what you want to do",
    "start": "2323110",
    "end": "2329711"
  },
  {
    "text": "is aligned to what's in\nthe user's best interest.  The good news, though, is\nthat behind this disagreement,",
    "start": "2329711",
    "end": "2337450"
  },
  {
    "text": "there is quite a lot of\nagreement, I would say.",
    "start": "2337450",
    "end": "2344440"
  },
  {
    "text": "These things like health,\nsafety, liberty, knowledge, dignity, happiness,\nalmost everyone",
    "start": "2344440",
    "end": "2351958"
  },
  {
    "text": "agrees that these\nthings are at least usually good for the\nperson who has them. Even if you think that really,\nultimately, all that matters,",
    "start": "2351958",
    "end": "2360460"
  },
  {
    "text": "all that's good for a person\nis their own happiness, well, these things\ntypically make",
    "start": "2360460",
    "end": "2366260"
  },
  {
    "text": "the person who has them happy. So you might think\nyou don't really",
    "start": "2366260",
    "end": "2371330"
  },
  {
    "text": "need to resolve this underlying\nphilosophical dispute to have",
    "start": "2371330",
    "end": "2376520"
  },
  {
    "text": "a good sense of what's in\nthe user's best interest. I mean, these are things\nthat, for the most part,",
    "start": "2376520",
    "end": "2382730"
  },
  {
    "text": "are in a person's best\ninterest, no matter what theory you endorse behind it.",
    "start": "2382730",
    "end": "2390160"
  },
  {
    "text": " OK, any questions about\nany of that so far?",
    "start": "2390160",
    "end": "2396963"
  },
  {
    "text": " OK, one complication about\naligning to the user's best",
    "start": "2396963",
    "end": "2406079"
  },
  {
    "text": "interest is that one\nthing that we normally",
    "start": "2406080",
    "end": "2411300"
  },
  {
    "text": "take to be good for\na person is autonomy, which is the ability to\nchoose for yourself how",
    "start": "2411300",
    "end": "2416730"
  },
  {
    "text": "to live your life. Even if you don't always\nmake the best choice, it might be good for you to\nhave this kind of control",
    "start": "2416730",
    "end": "2425400"
  },
  {
    "text": "over your own life. We want to avoid paternalism. We want to avoid choosing what\nwe think is best for someone,",
    "start": "2425400",
    "end": "2433140"
  },
  {
    "text": "rather than letting them\nchoose for themselves. So even in a case where\nyou're aligning to the user's",
    "start": "2433140",
    "end": "2439590"
  },
  {
    "text": "own best interest,\nyou might still need to take their intentions or\ntheir preferences into account.",
    "start": "2439590",
    "end": "2445730"
  },
  {
    "text": "It might be that\npart of what's best for them is to have\ntheir own intentions fulfilled, to have their\nown preferences honored.",
    "start": "2445730",
    "end": "2452383"
  },
  {
    "text": " OK. So this has all been\npretty abstract.",
    "start": "2452383",
    "end": "2461619"
  },
  {
    "text": "I want to move into slightly\nmore concrete case study. But first, maybe just to recap\nwhat we've covered so far,",
    "start": "2461620",
    "end": "2472329"
  },
  {
    "text": "value alignment is this problem\nof designing AI agents to do what we really want them to do.",
    "start": "2472330",
    "end": "2477760"
  },
  {
    "text": "But this can mean\na lot of things. It could mean doing what we\nreally intend them to do, what we really\nprefer that they do,",
    "start": "2477760",
    "end": "2483859"
  },
  {
    "text": "what it would be actually in our\nbest interest for them to do. And all of these\nthings can come apart.",
    "start": "2483860",
    "end": "2488930"
  },
  {
    "text": "They're not necessarily\nthe same thing, and they might impose certain\ntechnical or philosophical",
    "start": "2488930",
    "end": "2494799"
  },
  {
    "text": "constraints on your approach.  OK, let's talk about how this\nworks or what kind of difference",
    "start": "2494800",
    "end": "2502670"
  },
  {
    "text": "this could make in practice. Think a little bit\nabout LLM chatbots.",
    "start": "2502670",
    "end": "2509810"
  },
  {
    "text": "So everyone who talks to ChatGPT\nis talking to the same chatbot.",
    "start": "2509810",
    "end": "2515950"
  },
  {
    "text": "OK, there's different--\nthere's GPT 3.5. There's GPT 4. Ignore that.",
    "start": "2515950",
    "end": "2521079"
  },
  {
    "text": "I mean, fundamentally, it's\nthe same chatbot for everyone.",
    "start": "2521080",
    "end": "2526650"
  },
  {
    "text": "But plenty of chatbot\nproviders are now offering a wide range\nof different chatbots",
    "start": "2526650",
    "end": "2533700"
  },
  {
    "text": "with different personas. Some of these designed\nby users themselves.",
    "start": "2533700",
    "end": "2539130"
  },
  {
    "text": "So these examples are\nall from character.ai,",
    "start": "2539130",
    "end": "2545339"
  },
  {
    "text": "which promises personalized AI\nfor every moment of your day. So here, this comes out\nmaybe a little small.",
    "start": "2545340",
    "end": "2552730"
  },
  {
    "text": "But you can talk to the\ncreative writing helper. You can talk to the\nare-you-feeling-OK bot.",
    "start": "2552730",
    "end": "2559440"
  },
  {
    "text": "You can talk to\nthe dating coach. These are some of the\nrelatively normal ones.",
    "start": "2559440",
    "end": "2564869"
  },
  {
    "text": "You can talk to\ndepressed roommate. You can talk to Torybot.",
    "start": "2564870",
    "end": "2570280"
  },
  {
    "text": "I am Torybot. I believe in the free market. You can chat with AOC.",
    "start": "2570280",
    "end": "2575779"
  },
  {
    "text": "You can chat with Donald Trump. You can chat with Feminist Faye.",
    "start": "2575780",
    "end": "2581109"
  },
  {
    "text": "I am a feminist that\nhates Donald Trump. OK, lots of variety,\nlots of options here.",
    "start": "2581110",
    "end": "2587660"
  },
  {
    "text": "You could imagine yet\nstranger and stranger personas that you might\nbuild into a chatbot",
    "start": "2587660",
    "end": "2597040"
  },
  {
    "text": "or that your users might. None of this-- all of these\nare designed by users. None of these are\ncoming top down",
    "start": "2597040",
    "end": "2602710"
  },
  {
    "text": "from the provider of the LLMs. ",
    "start": "2602710",
    "end": "2609237"
  },
  {
    "text": "OK, so think about\nthis a little bit. Imagine you're building\nan LLM chatbot to serve",
    "start": "2609237",
    "end": "2614450"
  },
  {
    "text": "as a source of news for users. I mean, maybe this is going to\nstrike you already as crazy,",
    "start": "2614450",
    "end": "2619752"
  },
  {
    "text": "but I think there are a\nlot of people out there who already treat Google as\ntheir primary source of news,",
    "start": "2619752",
    "end": "2625859"
  },
  {
    "text": "a lot of people\nwho are replacing Google and other search\nengines with LLMs.",
    "start": "2625860",
    "end": "2631980"
  },
  {
    "text": "So I think there's\ndemand for this. Imagine you were\nwanting to fill it.",
    "start": "2631980",
    "end": "2639319"
  },
  {
    "text": "You can think a little\nbit about these questions. How would you make--\nin what ways would you",
    "start": "2639320",
    "end": "2645780"
  },
  {
    "text": "make the chatbot\npersonalizable if you were interested in aligning\nto the user's preferences?",
    "start": "2645780",
    "end": "2652950"
  },
  {
    "text": "In what ways might you\nmake it personalizable if you wanted to align to\nthe user's best interests?",
    "start": "2652950",
    "end": "2660560"
  },
  {
    "text": "And think a little bit about\nthe pros and cons of this. So I think take a minute to\nthink about this and then maybe",
    "start": "2660560",
    "end": "2668540"
  },
  {
    "text": "chat with somebody near\nyou, compare notes, see what you're thinking. And we'll come back\nin a couple of minutes",
    "start": "2668540",
    "end": "2675650"
  },
  {
    "text": "for a larger discussion. ",
    "start": "2675650",
    "end": "2716410"
  },
  {
    "text": "[SIDE CONVERSATION] ",
    "start": "2716410",
    "end": "2788930"
  },
  {
    "text": "All right, I've been hearing\na lot of good conversations that I'm not eager to cut\nshort, but maybe there",
    "start": "2788930",
    "end": "2796160"
  },
  {
    "text": "are conversations\nthat we can now bring back to the whole room. So anybody have any thoughts\nfrom their discussions",
    "start": "2796160",
    "end": "2805310"
  },
  {
    "text": "that maybe they want to share? I know some of you have\nthoughts because I was hearing",
    "start": "2805310",
    "end": "2811490"
  },
  {
    "text": "a lot of good ones out there. So don't be shy. You probably have better\nthoughts than I do.",
    "start": "2811490",
    "end": "2818302"
  },
  {
    "text": "If you don't say\nanything, then I'm just going to tell\nyou what I think. And then you're going\nto be stuck with that.",
    "start": "2818302",
    "end": "2823520"
  },
  {
    "text": "Yeah? I guess for the first\npoint, it would be-- I think it's pretty simple.",
    "start": "2823520",
    "end": "2829609"
  },
  {
    "text": "You'd probably be-- you'd\nuse a preference optimization approach. And you'd offer 10\ndifferent questions of, hey,",
    "start": "2829610",
    "end": "2838490"
  },
  {
    "text": "do you prefer this\nanswer or that answer? And then you would\noptimize the news",
    "start": "2838490",
    "end": "2844990"
  },
  {
    "text": "that's being fed to\nthat user accordingly. Yeah, good.",
    "start": "2844990",
    "end": "2851740"
  },
  {
    "text": "Yeah, like you\nsaid, fairly simple. If I want to align to\nthe user's preferences,",
    "start": "2851740",
    "end": "2857660"
  },
  {
    "text": "I'm going to figure out what\nit is that the user prefers. I'm going to give them news\nthat fits that profile, right?",
    "start": "2857660",
    "end": "2865180"
  },
  {
    "text": "Is that what\neverybody was thinking about this first question?",
    "start": "2865180",
    "end": "2870770"
  },
  {
    "text": "Anybody have something\nthey want to add to that? Yeah, great.",
    "start": "2870770",
    "end": "2875980"
  },
  {
    "text": "OK, I think-- yeah, I\nthink that's exactly right. OK, what about if\nyou were trying",
    "start": "2875980",
    "end": "2882599"
  },
  {
    "text": "to align to the user's best\ninterests, their own good,",
    "start": "2882600",
    "end": "2888970"
  },
  {
    "text": "objectively considered? Yeah?",
    "start": "2888970",
    "end": "2893980"
  },
  {
    "text": "I have a thought on that one,\nwhich is that you don't-- it's pretty hard to know what\nsomeone's best interest is,",
    "start": "2893980",
    "end": "2902690"
  },
  {
    "text": "as well as avoiding\nthe tenet that was on the previous slide\nof, don't be paternalistic.",
    "start": "2902690",
    "end": "2908200"
  },
  {
    "text": "So really, the\nonly way you could have any hope of\ndoing this would be optimizing for best interests\nof an entire population.",
    "start": "2908200",
    "end": "2917510"
  },
  {
    "text": "So if it doesn't apply, if\nthe policy of best interest",
    "start": "2917510",
    "end": "2923860"
  },
  {
    "text": "doesn't apply to\neveryone, then I would argue that\nyou can't actually",
    "start": "2923860",
    "end": "2929320"
  },
  {
    "text": "do it for an individual user. So that's the way you\nwould personalize it if it's [INAUDIBLE] the line\nto a user's best interest",
    "start": "2929320",
    "end": "2937030"
  },
  {
    "text": "is you wouldn't ask that\nquestion to begin with. You would just\nhave it set already for the entire population.",
    "start": "2937030",
    "end": "2944500"
  },
  {
    "text": "OK, yeah. I mean, good. I think-- well, without I\nneed to constantly resist",
    "start": "2944500",
    "end": "2953750"
  },
  {
    "text": "the temptation to just turn\nevery one of these lectures into a philosophy class.",
    "start": "2953750",
    "end": "2959160"
  },
  {
    "text": "So I love that answer. I'm curious about why it might\nbe less difficult to determine",
    "start": "2959160",
    "end": "2970250"
  },
  {
    "text": "what would be in the objective\nbest interest of a large group than maybe of one person.",
    "start": "2970250",
    "end": "2975510"
  },
  {
    "text": "But this is a question we'll\ncome back to maybe later. Anybody else have thoughts\nabout this second one thing?",
    "start": "2975510",
    "end": "2982168"
  },
  {
    "text": "Something different come\nout of your discussions? Yeah? I think just take the\nmovie Her as an example.",
    "start": "2982168",
    "end": "2988590"
  },
  {
    "text": "When you got to know\nthe person very well, the person opened it\nup, a lot of data, a lot of the\ninformation, then you",
    "start": "2988590",
    "end": "2996740"
  },
  {
    "text": "will be able to maybe prioritize\non how you make the suggestion.",
    "start": "2996740",
    "end": "3001930"
  },
  {
    "text": "And also depend on the\nperson using the tool. For example, some tools are\nbetter on delving into the news,",
    "start": "3001930",
    "end": "3008210"
  },
  {
    "text": "trying to understand\nthe sources. Some of them are better at-- you just want to take\nthe most important thing",
    "start": "3008210",
    "end": "3013450"
  },
  {
    "text": "and then just have to\nspend time on an email on random news and all that.",
    "start": "3013450",
    "end": "3018630"
  },
  {
    "text": "So I'm talking about\ntwo components at least. One is, you know the person\nbetter, the other person.",
    "start": "3018630",
    "end": "3025030"
  },
  {
    "text": "The other thing is,\nyou know the behavior and how they would\nuse the tools. And just referring--\nI mean, the tools",
    "start": "3025030",
    "end": "3031349"
  },
  {
    "text": "should be refrained\nfrom extending too much and just grabbing too much\nattention of the user.",
    "start": "3031350",
    "end": "3038520"
  },
  {
    "text": "Yeah, good. Thank you. Yeah, and I think that\nthere is something to this",
    "start": "3038520",
    "end": "3044010"
  },
  {
    "text": "that it might be that just\nfrom observing someone's preferences for long enough.",
    "start": "3044010",
    "end": "3049810"
  },
  {
    "text": "Getting that much\ndata about them, you might be able to get\na little bit of insight, maybe, into what's in\ntheir best interests,",
    "start": "3049810",
    "end": "3057340"
  },
  {
    "text": "even when that diverges from\nwhat they want in the moment. So yeah, great.",
    "start": "3057340",
    "end": "3063010"
  },
  {
    "text": "I see-- yeah? We have similar--\nwe similarly had an idea about maintaining\nsome sort of state",
    "start": "3063010",
    "end": "3069900"
  },
  {
    "text": "for the user's best interest. Maybe you could have\nsome sort of structure",
    "start": "3069900",
    "end": "3076839"
  },
  {
    "text": "that would represent different\naspects of the best interest and which could be\npersonalizable to the user.",
    "start": "3076840",
    "end": "3082369"
  },
  {
    "text": "And with every interaction,\nit would reprompt the LLM",
    "start": "3082370",
    "end": "3087880"
  },
  {
    "text": "and then change\nthis if appropriate. And every time you are trying\nto get an output for the user,",
    "start": "3087880",
    "end": "3097599"
  },
  {
    "text": "you could put this as\npart of the context and write a prompt\naccordingly alongside whatever",
    "start": "3097600",
    "end": "3103690"
  },
  {
    "text": "the user is asking in order\nto fit that goal better. Good.",
    "start": "3103690",
    "end": "3108849"
  },
  {
    "text": "Well, that sounds to\nme a little bit more like maybe aligning to\nthe user's preferences.",
    "start": "3108850",
    "end": "3116500"
  },
  {
    "text": "Maybe I misunderstood. This sounds trying to figure out\nwhat it is that the users want",
    "start": "3116500",
    "end": "3122200"
  },
  {
    "text": "to get, what they're looking\nto get out of the bot,",
    "start": "3122200",
    "end": "3127420"
  },
  {
    "text": "and then determining what\nto return based on that. Maybe I misunderstood. I don't think that's\nnecessarily true.",
    "start": "3127420",
    "end": "3132950"
  },
  {
    "text": "I think you could write\na prompt that would-- like the internal\nprompt for keeping up",
    "start": "3132950",
    "end": "3138430"
  },
  {
    "text": "the state of the user's best\ninterest could be written,",
    "start": "3138430",
    "end": "3143740"
  },
  {
    "text": "and the fields could be provided\nsuch that it would try to meta-- you could ask it to meta-reason\nabout what the user's interests",
    "start": "3143740",
    "end": "3152710"
  },
  {
    "text": "likely are. Oh, I see. I see. OK, good.",
    "start": "3152710",
    "end": "3157780"
  },
  {
    "text": "Yeah, great. ",
    "start": "3157780",
    "end": "3162880"
  },
  {
    "text": "Yeah, any anybody else? I mean, maybe there's,\nin some sense, a more basic question behind\nthis, which would be something",
    "start": "3162880",
    "end": "3171579"
  },
  {
    "text": "like, what is maybe in a\nnews-seeking agent's best",
    "start": "3171580",
    "end": "3177530"
  },
  {
    "text": "interest? What kind of news would it\nbe best to provide somebody?",
    "start": "3177530",
    "end": "3182730"
  },
  {
    "text": "Yeah? Probably news that shows\na variety of perspectives that you check that\nit's actually correct",
    "start": "3182730",
    "end": "3189880"
  },
  {
    "text": "as well, I think. It's in the user's\nbest interests that they're properly\ninformed as opposed",
    "start": "3189880",
    "end": "3195730"
  },
  {
    "text": "to maybe only seeing news\nthat puts them in a good mood or aligned with their\nexisting opinions.",
    "start": "3195730",
    "end": "3202550"
  },
  {
    "text": "Yeah. Good, right. Yeah, you might think-- yeah, in contrast to the\napproach we discussed earlier",
    "start": "3202550",
    "end": "3208840"
  },
  {
    "text": "of, we're going to query the\nuser about their preferences, every time that we give them\nnews, we're going to say,",
    "start": "3208840",
    "end": "3215960"
  },
  {
    "text": "did you like that? Was that what you\nwere looking for? Yes/no. We're going to\nadjust and give you the news you want based on that.",
    "start": "3215960",
    "end": "3222730"
  },
  {
    "text": "Yeah, you might think it's\nactually better for people to be exposed to high-quality\nnews, unbiased news,",
    "start": "3222730",
    "end": "3232540"
  },
  {
    "text": "to be exposed to a variety\nof opinions and arguments, rather than--",
    "start": "3232540",
    "end": "3237770"
  },
  {
    "text": "What's the worry about aligning\ntoo heavily to the user's preferences is that you\nmight be putting them",
    "start": "3237770",
    "end": "3242890"
  },
  {
    "text": "in a kind of echo\nchamber, where they're getting all of their news from\ntalking to Donald Trump bot",
    "start": "3242890",
    "end": "3251128"
  },
  {
    "text": "or talking to Feminist\nbot, and they're not getting other perspectives.",
    "start": "3251128",
    "end": "3256720"
  },
  {
    "text": "Yeah, good. Does anybody else have\na different answer to that question maybe?",
    "start": "3256720",
    "end": "3262030"
  },
  {
    "text": "What would be in the user's\nbest interest to receive as news or how you would approach that\nfrom a design perspective?",
    "start": "3262030",
    "end": "3272495"
  },
  {
    "text": " OK.",
    "start": "3272496",
    "end": "3279630"
  },
  {
    "text": "Well, good. I think that's definitely right. I mean, in terms\nof pros and cons. Does anybody have get into this?",
    "start": "3279630",
    "end": "3287910"
  },
  {
    "text": "If you were designing\nthe news chatbot, which of these approaches\nwould be better?",
    "start": "3287910",
    "end": "3294850"
  },
  {
    "text": "What would be the pros\nof one, cons of another?  Yeah?",
    "start": "3294850",
    "end": "3300310"
  },
  {
    "text": "For me, I think optimizing\nfor best interests is almost like paternalistic because\nyou are assuming that you",
    "start": "3300310",
    "end": "3305388"
  },
  {
    "text": "know the interests of the user. You have a good approximation. You might really\nnot know at all. So it's like that user had\nsome tragedy or whatever",
    "start": "3305388",
    "end": "3315339"
  },
  {
    "text": "in their life recently. And then some sort\nof recent news event has a lot of mass death.",
    "start": "3315340",
    "end": "3321165"
  },
  {
    "text": "Like, maybe they\ndon't want to be exposed to that, even\nthough maybe it's a very important event. You should know\nabout this, but you",
    "start": "3321165",
    "end": "3326500"
  },
  {
    "text": "don't have complete state of\nthe user's psychic state, how they actually feel. So maybe just using the\npreferences that are already--",
    "start": "3326500",
    "end": "3334390"
  },
  {
    "text": "that you actually have just\nfrom using the app, what did they click on, might be better. ",
    "start": "3334390",
    "end": "3339725"
  },
  {
    "text": "Good.  Yeah, I think that's great.",
    "start": "3339725",
    "end": "3344990"
  },
  {
    "text": "So there are-- even if\nwe can say things maybe at a very general\nlevel about what",
    "start": "3344990",
    "end": "3351640"
  },
  {
    "text": "is in a person's\ninterest, what is actually good for a person in general,\nthat leaves a lot of room",
    "start": "3351640",
    "end": "3357880"
  },
  {
    "text": "for variation from\nperson to person, especially if you think\nthat quite a lot of what's",
    "start": "3357880",
    "end": "3363790"
  },
  {
    "text": "good for a person is built\nout of subjective interests",
    "start": "3363790",
    "end": "3369700"
  },
  {
    "text": "of theirs or their desires or\nwhat makes them happy, what",
    "start": "3369700",
    "end": "3375099"
  },
  {
    "text": "makes them unhappy human. That's not a thing you\nmight have full access to.",
    "start": "3375100",
    "end": "3382565"
  },
  {
    "text": "So there is this\nproblem that if you're trying to align to what's really\ngood for the user, your only",
    "start": "3382565",
    "end": "3391390"
  },
  {
    "text": "real way to do\nthat is by aligning to what it is that you think\nis good for the user, right? And you might be good\nat figuring that out.",
    "start": "3391390",
    "end": "3398839"
  },
  {
    "text": "You might not be. And absolutely, that's where you\nrun this risk of paternalism.",
    "start": "3398840",
    "end": "3404359"
  },
  {
    "text": "So an advantage of just\naligning the user's preferences, giving them what they\nsaid that they want",
    "start": "3404360",
    "end": "3411520"
  },
  {
    "text": "means that you avoid that risk. You avoid trying to\nposition yourself as saying,",
    "start": "3411520",
    "end": "3417800"
  },
  {
    "text": "no, I know what's\nreally good for you when maybe you're not in a\nposition to determine that.",
    "start": "3417800",
    "end": "3424060"
  },
  {
    "text": "Yeah, anyone else on this point? ",
    "start": "3424060",
    "end": "3434130"
  },
  {
    "text": "OK. [INAUDIBLE]? Yeah. I just thought that\nthe counterargument",
    "start": "3434130",
    "end": "3439590"
  },
  {
    "text": "is that running the risk\nof being paternalistic, you actually give convenience. But then giving them\nlow-quality choices,",
    "start": "3439590",
    "end": "3447497"
  },
  {
    "text": "you actually waste\nthem a lot of time. So apps evolve. Yeah, I think that's right.",
    "start": "3447497",
    "end": "3455640"
  },
  {
    "text": "And right. I mean, to the\nearlier point, it's-- yeah, there might\nbe some aspects",
    "start": "3455640",
    "end": "3461549"
  },
  {
    "text": "of the user's best\ninterest that are easier to determine than others. We might be reasonably confident\nthat it would be in any",
    "start": "3461550",
    "end": "3468690"
  },
  {
    "text": "user's best interest to be given\nhigh-quality sources of news, to be exposed to a variety\nof opinions that might be--",
    "start": "3468690",
    "end": "3476895"
  },
  {
    "text": "you might want to align in part\nto that general human interest, while still allowing\nsome room to align",
    "start": "3476895",
    "end": "3483097"
  },
  {
    "text": "to the user's preferences. So these are not necessarily\nmutually exclusive goals",
    "start": "3483097",
    "end": "3489180"
  },
  {
    "text": "in alignment. I mean, it might be-- in some ways, it might\nbe worth focusing more",
    "start": "3489180",
    "end": "3496527"
  },
  {
    "text": "on the user's preferences. In some ways, in\nsome cases, contexts you might want to focus\nmore on what do we think",
    "start": "3496527",
    "end": "3502760"
  },
  {
    "text": "is actually good for the\nuser, because what they prefer might be junk\ninformation or convenient",
    "start": "3502760",
    "end": "3510440"
  },
  {
    "text": "bias-confirming information,\nthings like that. OK, great. ",
    "start": "3510440",
    "end": "3519360"
  },
  {
    "text": "Well, there's one thing I think\nthat has been not completely absent, maybe, from\nour discussion,",
    "start": "3519360",
    "end": "3525700"
  },
  {
    "text": "but I hope noticeably\nabsent from my lecture",
    "start": "3525700",
    "end": "3531540"
  },
  {
    "text": "and from my slides. So far, is there may be\na big piece of the puzzle that we're missing, something\nthat you would have thought?",
    "start": "3531540",
    "end": "3538800"
  },
  {
    "text": "This would be a-- this is\nwhat we're going to talk about with value alignment. And why haven't we\ngotten there yet?",
    "start": "3538800",
    "end": "3544475"
  },
  {
    "text": " Anybody at all? We've talked about aligning\nto the user's intentions.",
    "start": "3544475",
    "end": "3552400"
  },
  {
    "text": "We've talked about aligning\nto the user's preferences, to the user's best interests. Yeah?",
    "start": "3552400",
    "end": "3558180"
  },
  {
    "text": "A good way to measure alignment? A way to measure alignment. Yes, that has been absent.",
    "start": "3558180",
    "end": "3566210"
  },
  {
    "text": "I just think-- yeah? Maybe aligning to a\nsociety's overall interests, rather than just a person's?",
    "start": "3566210",
    "end": "3572840"
  },
  {
    "text": "Yeah. So you are correct. But I was thinking--\nyeah, I mean, that there",
    "start": "3572840",
    "end": "3578630"
  },
  {
    "text": "are people other than the user. Where's my text? There's my text.",
    "start": "3578630",
    "end": "3585830"
  },
  {
    "text": "Yeah, there are other people\nwhose interests are important,",
    "start": "3585830",
    "end": "3591320"
  },
  {
    "text": "maybe, to take into\naccount than just the person who is giving\ninstructions to the agent.",
    "start": "3591320",
    "end": "3599210"
  },
  {
    "text": "So you might think\nthere's really",
    "start": "3599210",
    "end": "3604415"
  },
  {
    "text": "another possible\ninterpretation of what we're after with value\nalignment, which is that an AI agent\nis value aligned,",
    "start": "3604415",
    "end": "3611870"
  },
  {
    "text": "if it does what's\nmorally right, right? I mean, the main problem\nwith paperclip AI",
    "start": "3611870",
    "end": "3619400"
  },
  {
    "text": "isn't that it does\nwhat's bad for me. It does what's bad for everyone\nif it destroys the world.",
    "start": "3619400",
    "end": "3625849"
  },
  {
    "text": "Or it does what's bad\nfor the factory workers if it makes them work around\nthe clock making paperclips",
    "start": "3625850",
    "end": "3632210"
  },
  {
    "text": "and so on. So earlier, we were\nfocusing on, what do",
    "start": "3632210",
    "end": "3637550"
  },
  {
    "text": "we mean by what we really want? What does really want mean? This would be to focus a\nlittle bit more on the we.",
    "start": "3637550",
    "end": "3645450"
  },
  {
    "text": "What is it that we really want? Because, of course, what\nthe user intends, prefers,",
    "start": "3645450",
    "end": "3650809"
  },
  {
    "text": "even what's in their\nindividual interests might be bad for others.",
    "start": "3650810",
    "end": "3655890"
  },
  {
    "text": "We probably don't want to say\nthat paperclip AI is value aligned if it\nmaximizes production",
    "start": "3655890",
    "end": "3662309"
  },
  {
    "text": "by exploiting the workers in the\nfactory, even if I, as the user, have no qualms about\nexploiting the workers, right?",
    "start": "3662310",
    "end": "3672110"
  },
  {
    "text": "OK. That said, it wasn't\na waste of time to start by focusing\non the user, right?",
    "start": "3672110",
    "end": "3678700"
  },
  {
    "text": "Even if we want to align to\nmorality or to the interests of more people than\nthe user, we also",
    "start": "3678700",
    "end": "3685859"
  },
  {
    "text": "do want to align to\nwhat the user wants when what the user wants\nis morally acceptable.",
    "start": "3685860",
    "end": "3692040"
  },
  {
    "text": "So it still matters\nhow we understand what it is that the\nuser really wants, even if we need to\nplace that in a larger",
    "start": "3692040",
    "end": "3698369"
  },
  {
    "text": "moral or societal context. ",
    "start": "3698370",
    "end": "3705620"
  },
  {
    "text": "But, of course, here, too, we\nhave a philosophical problem. I mean, which things are\nreally morally right?",
    "start": "3705620",
    "end": "3714170"
  },
  {
    "text": "There's a lot of\ndisagreement on this one, too, not unlike the\nquestion of what is objectively good for a person.",
    "start": "3714170",
    "end": "3721110"
  },
  {
    "text": "Is it right to lie to\nspare someone's feelings?",
    "start": "3721110",
    "end": "3726350"
  },
  {
    "text": "Is it right to pirate\ncopyrighted material? Is it right to buy\nluxuries when you could",
    "start": "3726350",
    "end": "3732080"
  },
  {
    "text": "donate to charity instead? Is it right to kill one person\nto save five or a thousand",
    "start": "3732080",
    "end": "3739820"
  },
  {
    "text": "or a million? These are at least some of them.",
    "start": "3739820",
    "end": "3747559"
  },
  {
    "text": "I hope you think\ndifficult moral questions. Certainly, they\nare moral questions",
    "start": "3747560",
    "end": "3752930"
  },
  {
    "text": "that people disagree about. Again, philosophers and\nnon-philosophers alike.",
    "start": "3752930",
    "end": "3757960"
  },
  {
    "text": " So how do we align\nto what's morally",
    "start": "3757960",
    "end": "3763730"
  },
  {
    "text": "right in the face of\nthis disagreement? This is you might think, where\nmy field of study comes in.",
    "start": "3763730",
    "end": "3772579"
  },
  {
    "text": "You might turn to moral\ntheory, which is basically just a systematic attempt to\nanswer questions like these.",
    "start": "3772580",
    "end": "3781245"
  },
  {
    "text": "So a moral theory, you\nmight have heard of, it's called consequentialism.",
    "start": "3781246",
    "end": "3786950"
  },
  {
    "text": "It says that an act is right if\nwhatever produces the greatest net good of any act available.",
    "start": "3786950",
    "end": "3792030"
  },
  {
    "text": "Or you might have heard\nof utilitarianism, which is a kind of consequentialism.",
    "start": "3792030",
    "end": "3798300"
  },
  {
    "text": "It says that you should produce\nthe greatest total happiness",
    "start": "3798300",
    "end": "3803330"
  },
  {
    "text": "that you can across all people. ",
    "start": "3803330",
    "end": "3809850"
  },
  {
    "text": "If you have a theory\nlike this, this can be used to answer some of\nthese difficult questions people disagree about.",
    "start": "3809850",
    "end": "3815692"
  },
  {
    "text": "Is it right to lie to\nspare someone's feelings? Well, if you're the\nconsequentialist, you'll say it might be.",
    "start": "3815692",
    "end": "3820910"
  },
  {
    "text": "If you can get away\nwith it, if no one discovers that it's a lie, and\nit makes somebody feel better,",
    "start": "3820910",
    "end": "3827010"
  },
  {
    "text": "that might produce more good\nthan not telling the lie.  So there's an idea\nhere, which is",
    "start": "3827010",
    "end": "3833390"
  },
  {
    "text": "that we could align AI\nto morality, to what's",
    "start": "3833390",
    "end": "3839539"
  },
  {
    "text": "morally right. If we align agents to the\ncorrect or best moral theory,",
    "start": "3839540",
    "end": "3848960"
  },
  {
    "text": "there's going to be a\nphilosophical problem with this. Does anybody think they\nknow what it's going to be?",
    "start": "3848960",
    "end": "3854570"
  },
  {
    "text": "Has a similar form to all of\nthe philosophical problems we've encountered so far. ",
    "start": "3854570",
    "end": "3862950"
  },
  {
    "text": "Well, there's a lot of\ndisagreement about what the correct moral theory is. So there's disagreement\nnot only at the order",
    "start": "3862950",
    "end": "3869215"
  },
  {
    "text": "of ground-level moral facts\nabout whether you should tell",
    "start": "3869215",
    "end": "3876183"
  },
  {
    "text": "a lie to spare\nsomeone's feelings, but also about the best\ntheory for systematizing this kind of stuff.",
    "start": "3876183",
    "end": "3883710"
  },
  {
    "text": "We already saw a\nconsequentialism. But there's a whole\nhost of others,",
    "start": "3883710",
    "end": "3889299"
  },
  {
    "text": "and just to put a few\nof these on the table, just to give you a sense of the\nrange that we're looking at.",
    "start": "3889300",
    "end": "3896160"
  },
  {
    "text": " You could be a prioritarian,\nwhere you would think that,",
    "start": "3896160",
    "end": "3902460"
  },
  {
    "text": "really, what you\nwant to do is not to maximize the total good\nbut to produce the greatest",
    "start": "3902460",
    "end": "3909150"
  },
  {
    "text": "weighted sum of good, where the\ninterests of those who are worse off is given more weight.",
    "start": "3909150",
    "end": "3915339"
  },
  {
    "text": "Or you could take this to\nextreme, a maximin, or minimax",
    "start": "3915340",
    "end": "3923320"
  },
  {
    "text": "view, where what's\nmorally right is to make things as good as\npossible for the person who's left the worst off\nby what you've done",
    "start": "3923320",
    "end": "3932000"
  },
  {
    "text": "or to minimize the negative\nconsequences for the person who suffers the most.",
    "start": "3932000",
    "end": "3940240"
  },
  {
    "text": "So in cases where you have\nto think about a quantifiable",
    "start": "3940240",
    "end": "3949119"
  },
  {
    "text": "good-- but if I have\nfour people who I can--",
    "start": "3949120",
    "end": "3957820"
  },
  {
    "text": "how would I do this-- ",
    "start": "3957820",
    "end": "3965780"
  },
  {
    "text": "assign goods to, I have\nthe option to say-- ",
    "start": "3965780",
    "end": "3982822"
  },
  {
    "text": "I have options to\ndistribute goods, say these ways among\ndifferent people. If I'm the\nconsequentialist, I'm going",
    "start": "3982822",
    "end": "3989109"
  },
  {
    "text": "to say, well, I\nwant the one that produces the most total good. That's this first option.",
    "start": "3989110",
    "end": "3995360"
  },
  {
    "text": "If I'm a prioritarian,\nwell, I'm going to need some kind of\nway of weighting this.",
    "start": "3995360",
    "end": "4000740"
  },
  {
    "text": "Say that the way to give more\nweight to the people who's worst off is that we weight\nthe good to you on a log",
    "start": "4000740",
    "end": "4010840"
  },
  {
    "text": "scale or something, right? Then in-- well, in\nbase 10, at least",
    "start": "4010840",
    "end": "4019779"
  },
  {
    "text": "this is going to be the\nprioritarian's choice. We want to-- by giving more\npriority to those who have it",
    "start": "4019780",
    "end": "4028180"
  },
  {
    "text": "worse with the-- sorry, I'm not explaining\nthis very well.",
    "start": "4028180",
    "end": "4034869"
  },
  {
    "text": "I'm trying to move too quickly. If I was taking the log of each\nof these as the prioritarian, I'd say here, we have--",
    "start": "4034870",
    "end": "4044580"
  },
  {
    "text": "this is coming out to 6. This is coming out 7. That's better. If I want to make things as good\nas possible for the person who",
    "start": "4044580",
    "end": "4050815"
  },
  {
    "text": "ends up worst off, I might\nchoose this last option, even though in\nthis option, we're",
    "start": "4050815",
    "end": "4056190"
  },
  {
    "text": "getting the least\ntotal good, right? The person who ends up\nworst off is doing better",
    "start": "4056190",
    "end": "4061200"
  },
  {
    "text": "than the person who ends up\nworst off in the other options. So all these options and\nmore are available to you",
    "start": "4061200",
    "end": "4069119"
  },
  {
    "text": "in moral theory. You might take a satisficing\nversion of any of these views, instead of trying to\nmaximize the total good.",
    "start": "4069120",
    "end": "4076490"
  },
  {
    "text": "You might think an act\nis right if it just produces a sufficiently great\nsum of good or weighted sum",
    "start": "4076490",
    "end": "4082650"
  },
  {
    "text": "of good. ",
    "start": "4082650",
    "end": "4088200"
  },
  {
    "text": "We haven't even yet touched\ndeontological views, which hold that, even acts\nwith the best consequences",
    "start": "4088200",
    "end": "4094350"
  },
  {
    "text": "can be wrong if they violate\ncertain moral rules or rights.",
    "start": "4094350",
    "end": "4099509"
  },
  {
    "text": "Often, these rules will be\nrules like, don't murder anyone, don't steal, don't lie,\nkeep your promises.",
    "start": "4099510",
    "end": "4109299"
  },
  {
    "text": "Right, you might think\nthat an act can't be right if it involves\nstealing from someone, even if it produces\na lot of good.",
    "start": "4109300",
    "end": "4116509"
  },
  {
    "text": "This is something that a\nview like consequentialism might not capture. ",
    "start": "4116510",
    "end": "4123268"
  },
  {
    "text": "Although you might think that\nthese rules or rights are themselves justified by\ntheir good consequences,",
    "start": "4123268",
    "end": "4128778"
  },
  {
    "text": "it would be best if we\naccepted rules like this and followed them. ",
    "start": "4128779",
    "end": "4138670"
  },
  {
    "text": "OK, returning to this\nproblem of paternalism that we encountered earlier,\nthere is another problem here.",
    "start": "4138670",
    "end": "4145489"
  },
  {
    "text": "So one is there's just, what\nis the best moral theory? Who knows that's-- I've been working\non that for a decade",
    "start": "4145490",
    "end": "4155370"
  },
  {
    "text": "and haven't gotten\nmuch closer to it. But even if we knew what\nthe best moral theory was, it might be bad to design AI\nagents to act on moral values",
    "start": "4155370",
    "end": "4163710"
  },
  {
    "text": "that their users don't share. This could be because we want\nto avoid a kind of paternalism",
    "start": "4163710",
    "end": "4169979"
  },
  {
    "text": "where we say, no, these are\nthe correct moral values. It could be for more\npractical reasons.",
    "start": "4169979",
    "end": "4175839"
  },
  {
    "text": "Just the users won't\ntrust AI agents if they disagree with\nthem about moral matters.",
    "start": "4175840",
    "end": "4183270"
  },
  {
    "text": "OK. So there's some\ndifficulty trying to align to the best or\nthe correct moral theory.",
    "start": "4183270",
    "end": "4190100"
  },
  {
    "text": " But also, like with\nthe objective good, where there's a lot\nof disagreement here,",
    "start": "4190100",
    "end": "4196538"
  },
  {
    "text": "there's also quite a lot\nof agreement about what is the morally right thing to do. ",
    "start": "4196538",
    "end": "4203980"
  },
  {
    "text": "In simple cases, we all agree\nyou shouldn't kill people.",
    "start": "4203980",
    "end": "4209177"
  },
  {
    "text": "You shouldn't lie to them. You shouldn't steal from them. So another idea for\naligning to morality",
    "start": "4209177",
    "end": "4214690"
  },
  {
    "text": "would just be aligning\nAI agents to what we might call common sense\nor consensus morality.",
    "start": "4214690",
    "end": "4220659"
  },
  {
    "text": "Common sense, moral ideas,\nthat most people agree on. Instead of trying to\nmake AI morally perfect,",
    "start": "4220660",
    "end": "4227990"
  },
  {
    "text": "we should just aim to have\nit make moral decisions like a normal person would. ",
    "start": "4227990",
    "end": "4235830"
  },
  {
    "text": "Right? This view probably\nends up being pretty deontological and\nsatisficing, right? Most of us think you\nfollow certain moral rules,",
    "start": "4235830",
    "end": "4242770"
  },
  {
    "text": "you respect other\npeople's rights, then you're not morally\nrequired to do the best you can.",
    "start": "4242770",
    "end": "4251099"
  },
  {
    "text": "It's fine to do less\nto prioritize yourself in some cases, things like that.",
    "start": "4251100",
    "end": "4256235"
  },
  {
    "text": " Now, one advantage of\naligning to something",
    "start": "4256235",
    "end": "4263385"
  },
  {
    "text": "like common-sense\nmorality, rather than to a particular moral theory,\nis that moral theories often have surprising implications.",
    "start": "4263385",
    "end": "4270920"
  },
  {
    "text": "I know we're just\nabout out of time, so I'll skip to\nthe chase on these.",
    "start": "4270920",
    "end": "4276350"
  },
  {
    "text": "I mean, you can think about the\nconsequentialist requirement to maximize net good. I mean, suppose you had an\nAI agent that was a surgeon.",
    "start": "4276350",
    "end": "4287750"
  },
  {
    "text": "His five patients dying, each\nof which needs a different organ transplant to save their life.",
    "start": "4287750",
    "end": "4293630"
  },
  {
    "text": "Well, if you're thinking\nabout just maximizing the net good subject\nto no constraints,",
    "start": "4293630",
    "end": "4299369"
  },
  {
    "text": "maybe what you think\nis, well, that nurse walking by in the hall has\nall of the organs that I need.",
    "start": "4299370",
    "end": "4306260"
  },
  {
    "text": "Maybe if I just harvest\nthe organs from the nurse, put them in the five\npeople, save five lives,",
    "start": "4306260",
    "end": "4313349"
  },
  {
    "text": "the cost of one five\nis greater than one. We just maximize the net good. That's probably not what you\nwanted your surgeon AI to do.",
    "start": "4313350",
    "end": "4322469"
  },
  {
    "text": "Think about cases\nwhere you might",
    "start": "4322470",
    "end": "4327990"
  },
  {
    "text": "want to break a deontological\nrule against lying as well. AI agents aligned to a\nparticular moral theory",
    "start": "4327990",
    "end": "4334409"
  },
  {
    "text": "might discover some of these\nsurprising implications before we do. And they might discover them\nin practice, rather than",
    "start": "4334410",
    "end": "4340349"
  },
  {
    "text": "in the philosophy seminar\nroom, which is where we prefer for them to come up.",
    "start": "4340350",
    "end": "4345600"
  },
  {
    "text": "So by contrast, aligning\nto common-sense morality, you might end up\nwith an agent that",
    "start": "4345600",
    "end": "4351660"
  },
  {
    "text": "behaves more predictably,\nmaking moral decisions like a regular human.",
    "start": "4351660",
    "end": "4357120"
  },
  {
    "text": "It might be unpredictable\nin some edge cases, where common sense arguably runs out.",
    "start": "4357120",
    "end": "4364409"
  },
  {
    "text": "Would an AI aligned to\ncommon-sense morality kill one person\nto save a million?",
    "start": "4364410",
    "end": "4371110"
  },
  {
    "text": "I don't know. That's what we-- We got into moral\ntheory to try to answer hard questions like this.",
    "start": "4371110",
    "end": "4377060"
  },
  {
    "text": "If we've just taught the agents\nto think about morality like we do, it might be as unsure\nas we are about what",
    "start": "4377060",
    "end": "4382780"
  },
  {
    "text": "to do in a case like this. I need to let you go. So I'll just leave you with the\nthought, how bad would that be?",
    "start": "4382780",
    "end": "4391210"
  },
  {
    "text": "How bad would it be if AI was as\nunsure about morally hard cases",
    "start": "4391210",
    "end": "4396280"
  },
  {
    "text": "as we are?  OK.",
    "start": "4396280",
    "end": "4402190"
  },
  {
    "text": "We've covered that. I will let you go to\nenjoy your Wednesdays.",
    "start": "4402190",
    "end": "4407560"
  },
  {
    "text": "If you are interested\nin talking more about any of this ethics in\ngeneral, feel free to reach out.",
    "start": "4407560",
    "end": "4412592"
  },
  {
    "text": "Set up a meeting. We can talk more. Any questions now\nbefore we depart?",
    "start": "4412592",
    "end": "4419060"
  },
  {
    "text": "Or I can stick around for\na few minutes, if folks want to talk to me offline.",
    "start": "4419060",
    "end": "4424719"
  },
  {
    "text": "OK, great. Well, take care. ",
    "start": "4424720",
    "end": "4434000"
  }
]