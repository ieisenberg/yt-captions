[
  {
    "start": "0",
    "end": "5170"
  },
  {
    "text": "OK, now, let's talk about math. So last time,\nwhere we ended was,",
    "start": "5170",
    "end": "11030"
  },
  {
    "text": "we were talking about\nuniform convergence. So we said that our goal for\nthe next few lectures will be",
    "start": "11030",
    "end": "17510"
  },
  {
    "text": "the so-called\nuniform convergence,",
    "start": "17510",
    "end": "24960"
  },
  {
    "text": "which means that you want to\nsomewhat prove that with high probability, if you\ntake sup on maximum--",
    "start": "24960",
    "end": "32290"
  },
  {
    "text": "like a sup really just means\nmaximum for this course. So if you take sup over\nthe hypothesis class",
    "start": "32290",
    "end": "38390"
  },
  {
    "text": "and you look at the difference\nbetween the empirical risk and the population\nrisk, you want",
    "start": "38390",
    "end": "45070"
  },
  {
    "text": "to show that this is small\nwith high probability.",
    "start": "45070",
    "end": "52540"
  },
  {
    "text": "So this is the general idea. And we said that this is\ndifferent from showing that--",
    "start": "52540",
    "end": "59379"
  },
  {
    "text": "so this is different from--",
    "start": "59380",
    "end": "67130"
  },
  {
    "text": "so for every fixed h with high\nprobability, L hat minus Lh,",
    "start": "67130",
    "end": "75530"
  },
  {
    "text": "is small. So these two are of different\nnature because one-- so the order of the quantifier\nin some sense is different.",
    "start": "75530",
    "end": "83210"
  },
  {
    "text": "One requires that\nif this event-- with higher\nprobability, the event",
    "start": "83210",
    "end": "88460"
  },
  {
    "text": "that the entire population risk\nis close to the empirical risk, right.",
    "start": "88460",
    "end": "93830"
  },
  {
    "text": "So the other one is saying\nthat you just only look at one single h, and\nyou look at what's the probability that\nthis population risk is",
    "start": "93830",
    "end": "101600"
  },
  {
    "text": "different from empirical risk. And you want to show\nthat this event happens with high probability.",
    "start": "101600",
    "end": "108087"
  },
  {
    "text": "So in some sense, the difference\nis kind of like a union bound in some sense,\nwhich I'm going to talk more next when we get\nto prove this kind of statement.",
    "start": "108087",
    "end": "117829"
  },
  {
    "text": " So I mean, this\nlecture, we are going",
    "start": "117830",
    "end": "123500"
  },
  {
    "text": "to talk about two\nways to do this. Actually, we are going to\ntalk about two cases about h.",
    "start": "123500",
    "end": "130399"
  },
  {
    "text": "So certainly, this\nstatement depends on h. You cannot hope to prove things\nlike this for every possible",
    "start": "130400",
    "end": "135680"
  },
  {
    "text": "capital H. It does depend on the\nfamily of hypotheses you think about. And the bound actually depends\non the family of hypotheses",
    "start": "135680",
    "end": "142070"
  },
  {
    "text": "you are talking about. So the first part is going to\nbe about finite hypothesis class",
    "start": "142070",
    "end": "150760"
  },
  {
    "text": "where h is assumed to be finite. ",
    "start": "150760",
    "end": "161760"
  },
  {
    "text": "And the next part is\ngoing to be infinite case, infinite hypothesis class. And for infinite\nhypothesis class,",
    "start": "161760",
    "end": "167860"
  },
  {
    "text": "there are many different ways\nto achieve this kind of bound. And today, we're going to talk\nabout a relatively brute force",
    "start": "167860",
    "end": "176440"
  },
  {
    "text": "way to do it. In some sense,\nyou do a reduction to the finite hypothesis class. Essentially, no\nmatter what you do,",
    "start": "176440",
    "end": "182495"
  },
  {
    "text": "you are doing a reduction to\nthe finite hypothesis class. But how do you reduce to\nthe finite case does matter.",
    "start": "182495",
    "end": "188709"
  },
  {
    "text": "So in today, we're going to\ntalk about the brute force reduction, which does show\nsome kind of intuition. OK.",
    "start": "188710",
    "end": "194650"
  },
  {
    "text": "And so that's a brief\noverview of what we're going to do in this lecture.",
    "start": "194650",
    "end": "201625"
  },
  {
    "start": "201625",
    "end": "208960"
  },
  {
    "text": "So I guess let me\njust start by-- let's talk about the\nfinite hypothesis.",
    "start": "208960",
    "end": "215530"
  },
  {
    "text": "And here is the theorem\nwe're going to prove. So there are some conditions.",
    "start": "215530",
    "end": "220790"
  },
  {
    "text": "So the condition is, as we did\nlast time, recorded last time, we assume the loss\nis between 0 and 1",
    "start": "220790",
    "end": "229879"
  },
  {
    "text": "for every xy and\nevery hypothesis. And this is true\nfor binary loss. It's 0-1 loss.",
    "start": "229880",
    "end": "235849"
  },
  {
    "text": "It's not true for\nevery possible losses, but if you have\nother losses, you have do a little bit\nsmall, kind of like,",
    "start": "235850",
    "end": "242000"
  },
  {
    "text": "fix to make these\nproofs still work. But this is not very essential. It's mostly for convenience.",
    "start": "242000",
    "end": "247580"
  },
  {
    "text": "And what we're going to prove\nis the following statement. So we say that for every\ndelta between 0 and one half,",
    "start": "247580",
    "end": "256579"
  },
  {
    "text": "this is not very\nimportant either. So delta is a small number. And you will say that with\nprobability at least 1",
    "start": "256579",
    "end": "269320"
  },
  {
    "text": "minus delta, we have that for\nevery h, L hat h minus Lh,",
    "start": "269320",
    "end": "277810"
  },
  {
    "text": "an absolute value is bounded by\nsquare root ln, the size of h,",
    "start": "277810",
    "end": "294900"
  },
  {
    "text": "plus ln 2 over delta over 2n.",
    "start": "294900",
    "end": "301690"
  },
  {
    "text": "And recall that\nthe reason why we care about this\nuniform convergence was that it's useful for us to\nbound the excess risk, right?",
    "start": "301690",
    "end": "308690"
  },
  {
    "text": "So we have shown\nthat if you have this kind of uniform\nconvergence, then you can prove that your\nexcess risk is bounded.",
    "start": "308690",
    "end": "314560"
  },
  {
    "text": "So using what we have discussed\nlast time as a corollary,",
    "start": "314560",
    "end": "320860"
  },
  {
    "text": "we also get L excess risk of\nLh hat-- h hat is the ERM--",
    "start": "320860",
    "end": "326680"
  },
  {
    "text": "minus Lh star. So this is the ERM solution. ",
    "start": "326680",
    "end": "333889"
  },
  {
    "text": "Lh hat minus Lh\nstar is less than-- you pay a factor 2\nin that derivation,",
    "start": "333890",
    "end": "342289"
  },
  {
    "text": "so you multiply the factor 2. So you get something\nlike 2 times ln size of h",
    "start": "342290",
    "end": "352080"
  },
  {
    "text": "plus ln 2 over delta over n. ",
    "start": "352080",
    "end": "363080"
  },
  {
    "text": "OK, cool. ",
    "start": "363080",
    "end": "371087"
  },
  {
    "text": "So this is the theorem\nwe're going to prove.  Before we prove the\ntheorem, you can",
    "start": "371087",
    "end": "376880"
  },
  {
    "text": "see that the bound-- the\nright hand side of the bound-- does depend on the size of\nthe hypothesis class, right?",
    "start": "376880",
    "end": "386480"
  },
  {
    "text": "If you have a bigger\nhypothesis class, then your bound would be worse. So it's harder to prove\nthis uniform convergence",
    "start": "386480",
    "end": "393980"
  },
  {
    "text": "when you have a larger\nhypothesis class. And if you try to interpret\nthis bound at the end--",
    "start": "393980",
    "end": "400310"
  },
  {
    "text": "so here, this is\nbounded excess risk. And we can see that\nyou need n to be bigger",
    "start": "400310",
    "end": "407090"
  },
  {
    "text": "than the log of the size\nof h so that the right hand side of the bound becomes\nmeaningful, right?",
    "start": "407090",
    "end": "413185"
  },
  {
    "text": "So you want the excess risk\nto be something smaller than 1, at least, at a minimum. So you need n to be at\nleast larger than log",
    "start": "413185",
    "end": "420560"
  },
  {
    "text": "of the size of the\nhypothesis class. So that's why you need\nenough samples, right,",
    "start": "420560",
    "end": "426889"
  },
  {
    "text": "to make these bounds meaningful. And, of course, as\nn goes to infinity, you have better\nand better bound.",
    "start": "426890",
    "end": "434930"
  },
  {
    "text": "I'm going to have\nmore discussion after we prove the theorem. OK? So now let's try to\nprove the theorem.",
    "start": "434930",
    "end": "441110"
  },
  {
    "text": " So I guess the\noutline of the proof",
    "start": "441110",
    "end": "449610"
  },
  {
    "text": "is that first, you\nwould do individual h.",
    "start": "449610",
    "end": "455479"
  },
  {
    "text": "You prove this for individuals. You prove the simple\nversion, basically like we discussed the last time.",
    "start": "455480",
    "end": "461510"
  },
  {
    "text": "And second, we take a\nunion bound over all h.",
    "start": "461510",
    "end": "467830"
  },
  {
    "text": "OK.  So let's do the first step.",
    "start": "467830",
    "end": "475110"
  },
  {
    "text": "So recall that last time,\nwe have done this already for fixed data.",
    "start": "475110",
    "end": "480317"
  },
  {
    "text": "So here, I'm just doing\nit a little more formally. So last time, we actually\nshowed this, right?",
    "start": "480317",
    "end": "485580"
  },
  {
    "text": "We used this\nHoeffding inequality to get something like L\nhat theta minus L theta.",
    "start": "485580",
    "end": "491520"
  },
  {
    "text": "This is something like an order\nof 1 over square root of n. That's what we did somewhat\ninformally last time",
    "start": "491520",
    "end": "499110"
  },
  {
    "text": "with the Hoeffding inequality. And today, I'm going to\nhave a little more kind of like careful\nderivations to get exactly",
    "start": "499110",
    "end": "506759"
  },
  {
    "text": "all the dependencies\nup to constant. And by the way, theta\nand hr, the same. h is just when you talk about\nfinite hypothesis class, right?",
    "start": "506760",
    "end": "514164"
  },
  {
    "text": "You don't necessarily\nhave a parameter. You may just list\nall the hypotheses. That's why it's called h.",
    "start": "514164",
    "end": "520080"
  },
  {
    "text": "And when you parameterize it,\nyou have the parameter theta. But for this purpose, they\nare not different at all.",
    "start": "520080",
    "end": "527460"
  },
  {
    "text": "So let's apply-- this\nis the last lecture. So let's apply Hoeffding\ninequality, so where ai is 0,",
    "start": "527460",
    "end": "545470"
  },
  {
    "text": "bi is 1. So the bound is 0 and 1, right?",
    "start": "545470",
    "end": "551020"
  },
  {
    "text": "So we gather that\nfor every h in h-- suppose this h is fixed and\nthen you draw your sample,",
    "start": "551020",
    "end": "557350"
  },
  {
    "text": "you look at the probability\nthat L hat h minus Lh",
    "start": "557350",
    "end": "563440"
  },
  {
    "text": "is less than epsilon. And here, what's random? The random is the data set. The randomness comes from a\ndata set which goes into L hat.",
    "start": "563440",
    "end": "573220"
  },
  {
    "text": "And if you use the\nHoeffding inequality, you get this is larger than 1\nminus 2 times exponential minus",
    "start": "573220",
    "end": "579959"
  },
  {
    "text": "2 n square, epsilon\nsquare, over sum of beta i, bi, minus ai square. ",
    "start": "579960",
    "end": "587709"
  },
  {
    "text": "And because bi is 1\nand ai is 0, so the sum of bi minus ai square is n.",
    "start": "587710",
    "end": "593260"
  },
  {
    "text": "So you get 1 minus 2 exponential\nminus 2n epsilon squared. ",
    "start": "593260",
    "end": "602904"
  },
  {
    "text": "And right, this is because\nbi is 1, and ai is 0. ",
    "start": "602905",
    "end": "609190"
  },
  {
    "text": "And so in other\nwords, if you look at the other side\nof the bound, you look at what's the chance\nthat they are different.",
    "start": "609190",
    "end": "618363"
  },
  {
    "text": "The chance that\nthey are different is less than 2 times exponential\n2n minus epsilon squared.",
    "start": "618363",
    "end": "624340"
  },
  {
    "text": "Actually in many cases,\nHoeffding inequality was stated in this\nway, instead of the way that I showed before.",
    "start": "624340",
    "end": "629837"
  },
  {
    "text": "They are exactly the same. It's just complementary\nof each other. But you have a lower\nbound for some events,",
    "start": "629837",
    "end": "636760"
  },
  {
    "text": "then you have the upper\nbound for the complementary of the event.",
    "start": "636760",
    "end": "641860"
  },
  {
    "text": "And now, this is for every\nh, you have this, right? So basically, for\nevery h, you have",
    "start": "641860",
    "end": "647410"
  },
  {
    "text": "some kind of failure\nevent, which is this event. And this event happens\nwith a small probability.",
    "start": "647410",
    "end": "653590"
  },
  {
    "text": "And now, let's recall that you\nhave the so-called union bound. And the union bound\nis saying that if you",
    "start": "653590",
    "end": "659750"
  },
  {
    "text": "look at the union of\na bunch of events, maybe let's say k\nevents, then they",
    "start": "659750",
    "end": "665700"
  },
  {
    "text": "are smaller than the probability\nof the sum of the probability",
    "start": "665700",
    "end": "671550"
  },
  {
    "text": "of each event. And here, suppose you say the\nEh corresponds to the event",
    "start": "671550",
    "end": "681290"
  },
  {
    "text": "that L hat h is different from\nLh from average by epsilon,",
    "start": "681290",
    "end": "686759"
  },
  {
    "text": "then you know that the\nprobability that the union of the Eis, which is\nbasically saying what's",
    "start": "686760",
    "end": "694079"
  },
  {
    "text": "the union of this failure event? Basically, it means\nthat there exists h such that this event happens, right?",
    "start": "694080",
    "end": "700312"
  },
  {
    "text": "So such that L hat h minus\nLh is larger than epsilon.",
    "start": "700312",
    "end": "707800"
  },
  {
    "text": "So this is the union\nof all of these events.",
    "start": "707800",
    "end": "713890"
  },
  {
    "text": "And it's less than the\nsum of each of the events. OK.",
    "start": "713890",
    "end": "719195"
  },
  {
    "start": "719195",
    "end": "728435"
  },
  {
    "text": "And now, you plug-in\nwhat you have prepared. Maybe let's say this equation. Let's call it 1.",
    "start": "728435",
    "end": "733940"
  },
  {
    "text": "So if you plug-in\n1, then you get-- this is sum over all the hs.",
    "start": "733940",
    "end": "741920"
  },
  {
    "text": "And you get 2 times exponential\nminus 2n epsilon square.",
    "start": "741920",
    "end": "747170"
  },
  {
    "text": "So each of these\nevents is small. And you multiply it by the total\nnumber of the possible events,",
    "start": "747170",
    "end": "753750"
  },
  {
    "text": "which is the size of h. So basically, we have 2 times\nh times exponential times 2n epsilon squared.",
    "start": "753750",
    "end": "758945"
  },
  {
    "text": "OK? ",
    "start": "758945",
    "end": "766797"
  },
  {
    "text": "And you can see that\nthis is basically what we wanted to have. Because now, we have\nthere exists h such",
    "start": "766797",
    "end": "776270"
  },
  {
    "text": "that they are different. So the complementary\nof this will be just that this is just the equals\nto 1 minus the probability",
    "start": "776270",
    "end": "784069"
  },
  {
    "text": "that for every h, the\nflipped event is true. ",
    "start": "784070",
    "end": "793339"
  },
  {
    "text": "By the way, I'm not\ndistinguishing the latter two equal in most of this course--",
    "start": "793340",
    "end": "801865"
  },
  {
    "text": "so technically,\nI probably should write this is less than epsilon,\nright, instead of less than",
    "start": "801865",
    "end": "807227"
  },
  {
    "text": "or equal to epsilon. But for this part,\nfor this course, I'm not super careful about\nthis, because they don't really",
    "start": "807227",
    "end": "814300"
  },
  {
    "text": "matter that much. And in many cases,\nactually, the probability that this is exactly equal\nto epsilon is actually 0.",
    "start": "814300",
    "end": "822070"
  },
  {
    "text": "So technically, they\nare even correct. But anyway, this is not super\nimportant for this course.",
    "start": "822070",
    "end": "827589"
  },
  {
    "text": " Because of this, you\ncan see that this",
    "start": "827590",
    "end": "834062"
  },
  {
    "text": "is what we care about, right? For every h, L hat\nh is close to Lh. And we are already kind\nof getting there, almost.",
    "start": "834062",
    "end": "840820"
  },
  {
    "text": "So the only thing we need is\nto know what this point is. We need to upper bound\nthis so that we can lower bound this probability.",
    "start": "840820",
    "end": "848040"
  },
  {
    "text": "OK? So now, let's choose epsilon\nso that you can get--",
    "start": "848040",
    "end": "854680"
  },
  {
    "text": "so basically, you want\nthis probability to happen. You want this thing to be\nbigger than 1 minus delta.",
    "start": "854680",
    "end": "864110"
  },
  {
    "text": "So that's why you want this\nto be less than delta, right?",
    "start": "864110",
    "end": "872589"
  },
  {
    "text": "So basically, we just\nneed to choose epsilon so that this probability\nbecomes delta. So choose epsilon such\nthat 2h times exponential",
    "start": "872590",
    "end": "884660"
  },
  {
    "text": "minus 2n epsilon squared\nis equals to delta. And this involves you\nsolve the equation,",
    "start": "884660",
    "end": "891660"
  },
  {
    "text": "which is not too hard. So if you solve it, you get\nepsilon is equals to, I guess,",
    "start": "891660",
    "end": "898730"
  },
  {
    "text": "exactly what I had before. So epsilon is equals to ln h\nplus ln 2 over delta over 2n.",
    "start": "898730",
    "end": "913085"
  },
  {
    "text": "So basically, if you\ntake epsilon to be this, then you know that the\nprobability that for every h,",
    "start": "913085",
    "end": "920750"
  },
  {
    "text": "L hat h minus-- maybe I start with\nthe existence.",
    "start": "920750",
    "end": "927459"
  },
  {
    "text": "Lh is bigger than epsilon, it's\nless than 1, less than delta,",
    "start": "927460",
    "end": "938040"
  },
  {
    "text": "right? And then if you flip the event,\nyou get the desired zero. ",
    "start": "938040",
    "end": "950530"
  },
  {
    "text": "Any questions so far? ",
    "start": "950530",
    "end": "968683"
  },
  {
    "text": "OK, so let me have a few\nremarks to kind of somewhat interpret what we have done\nand compare it with what we did in the first lecture.",
    "start": "968683",
    "end": "975440"
  },
  {
    "text": "So if you compare with\nthe asymptotic results,",
    "start": "975440",
    "end": "983267"
  },
  {
    "text": "you're going to see this, right? So for asymptotic\nresults, what you got",
    "start": "983267",
    "end": "989949"
  },
  {
    "text": "is that Lh hat minus Lh star-- the excess risk-- is bounded\nby, is something like C over n",
    "start": "989950",
    "end": "997690"
  },
  {
    "text": "plus o of 1 over n. And recall that\nthis C can depend",
    "start": "997690",
    "end": "1004590"
  },
  {
    "text": "on dimension of the problem. ",
    "start": "1004590",
    "end": "1014040"
  },
  {
    "text": "And here, you can hide any other\ndependencies on the problem.",
    "start": "1014040",
    "end": "1027933"
  },
  {
    "text": " And what we have\nnow is that Lh hat,",
    "start": "1027934",
    "end": "1036280"
  },
  {
    "text": "the excess risk, minus the-- sorry, Lh hat, the excess\nrisk, is smaller than--",
    "start": "1036280",
    "end": "1047319"
  },
  {
    "text": "so here, you don't\nhide anything. You hide some\nconstant, of course. You hide this as ln h over n\nover-- sorry, square root this.",
    "start": "1047319",
    "end": "1059717"
  },
  {
    "text": "And of course, you\nalso have something like O of square root\nln 1 over delta over n.",
    "start": "1059717",
    "end": "1069670"
  },
  {
    "text": "So this term is supposed\nto be relatively small because you can take the delta\nto be-- this is a logarithmic,",
    "start": "1069670",
    "end": "1074750"
  },
  {
    "text": "right? You can take delta to be\nsomething like, maybe, n to the 10, right? So you still get square root\nlog n over square root n, right?",
    "start": "1074750",
    "end": "1083600"
  },
  {
    "text": "So let's say take delta\nto be n to the minus 10 so that this one will\nbe square root log",
    "start": "1083600",
    "end": "1089657"
  },
  {
    "text": "n over square root of n, which\nis almost negligible compared",
    "start": "1089657",
    "end": "1095620"
  },
  {
    "text": "to the first term. So basically, let's say we\nignore this for the comparison.",
    "start": "1095620",
    "end": "1101750"
  },
  {
    "text": "So ignore this for\nthe comparison, then you can compare\nthis and this.",
    "start": "1101750",
    "end": "1107290"
  },
  {
    "text": "So you can see that-- the first thing is that we have\na worse dependency on n, right?",
    "start": "1107290",
    "end": "1120120"
  },
  {
    "text": "Before the dependency, at\nleast in terms of leading term, the dependency on\nn was 1 over n.",
    "start": "1120120",
    "end": "1126427"
  },
  {
    "text": "And now, it's 1 over\nsquare root of n. So it goes to 0 slower. ",
    "start": "1126427",
    "end": "1135270"
  },
  {
    "text": "So this could be improved. This can be improved in certain\ncases, which we probably",
    "start": "1135270",
    "end": "1143200"
  },
  {
    "text": "wouldn't do at all. I guess in one of the\nhomework question,",
    "start": "1143200",
    "end": "1148670"
  },
  {
    "text": "you're going to be asked to\nimprove this to some extent. In some cases, you\ncan improve this to 1",
    "start": "1148670",
    "end": "1154122"
  },
  {
    "text": "over n, depending on\nvarious situations. But generally, I guess you get\nrelatively worse dependency",
    "start": "1154122",
    "end": "1161600"
  },
  {
    "text": "on n in comparatry asymptotics. ",
    "start": "1161600",
    "end": "1170030"
  },
  {
    "text": "One of the reason why this\nis happening is that-- so partly because we didn't\nassume twice differentiability.",
    "start": "1170030",
    "end": "1180895"
  },
  {
    "start": "1180895",
    "end": "1193020"
  },
  {
    "text": "Of loss function. So here, the only assumption\nwe have on loss function",
    "start": "1193020",
    "end": "1199600"
  },
  {
    "text": "is between 0 and 1. So it even works for 0-1 loss--\nthe classification 0-1 loss.",
    "start": "1199600",
    "end": "1204880"
  },
  {
    "text": "But before, we did\nassume that the loss has to be continuous\nand differentiable.",
    "start": "1204880",
    "end": "1210070"
  },
  {
    "text": "And I think we also assume\nit's twice differentiable. So that does play a\nfundamental role here.",
    "start": "1210070",
    "end": "1216190"
  },
  {
    "text": "So when we don't have\ntwice differentiability",
    "start": "1216190",
    "end": "1222039"
  },
  {
    "text": "and we don't have\nother assumptions, it's kind of actually\nimpossible to get 1",
    "start": "1222040",
    "end": "1228320"
  },
  {
    "text": "over n rates in many cases.  But here, what I'm\ntalking here is",
    "start": "1228320",
    "end": "1236710"
  },
  {
    "text": "all about the downside\nof our new bound. So the pros, we actually\nalready kind of like mentioned.",
    "start": "1236710",
    "end": "1242050"
  },
  {
    "text": "The main pro is that now, we\ndon't have any dependencies about anything.",
    "start": "1242050",
    "end": "1247809"
  },
  {
    "text": "So before, we recall\nthat last time, we were motivated to have\nnon-asymptotic bounds, we are saying that this thing\ncould hide a lot of things.",
    "start": "1247810",
    "end": "1255009"
  },
  {
    "text": "This could hide, for example,\nsomething like dimension to the 50-- that's my extreme\nexample-- over n squared.",
    "start": "1255010",
    "end": "1260960"
  },
  {
    "text": "So p to the 50 over n squared\nwill be counted as little of 1 over n.",
    "start": "1260960",
    "end": "1266140"
  },
  {
    "text": "So that doesn't\nmake a lot of sense just because if a\ndimension is too high, then this requires n to\nbe very big to be small.",
    "start": "1266140",
    "end": "1274179"
  },
  {
    "text": "So this was the issue that\nwe mentioned last time",
    "start": "1274180",
    "end": "1279190"
  },
  {
    "text": "about asymptotis. And now, we fixed that issue. And that's the main\nbenefit we gain.",
    "start": "1279190",
    "end": "1284470"
  },
  {
    "text": "So we don't have anything\nabout the dependencies. And also, we expand\nto see how does",
    "start": "1284470",
    "end": "1292840"
  },
  {
    "text": "this depend on the\ncomplexity, in some sense, of the hypothesis class. You can think of\nthis as a complexity.",
    "start": "1292840",
    "end": "1300909"
  },
  {
    "text": "The ln h can be thinked\nof as a complexity",
    "start": "1300910",
    "end": "1307130"
  },
  {
    "text": "of the hypothesis class. ",
    "start": "1307130",
    "end": "1312780"
  },
  {
    "text": "Probably, if you have\nbeen through CS 229, we have talked\nabout if you-- you can overfit if you have too\ncomplex a function class,",
    "start": "1312780",
    "end": "1320340"
  },
  {
    "text": "but you don't have enough data. And this is, in some sense, a\nmathematical characterization of that.",
    "start": "1320340",
    "end": "1325560"
  },
  {
    "text": "So if your function class is\ntoo complex so that the log of H",
    "start": "1325560",
    "end": "1331110"
  },
  {
    "text": "is too big, and you don't\nhave enough data compared",
    "start": "1331110",
    "end": "1336150"
  },
  {
    "text": "to log of H, then you\nmay have a worse bound. And on the other hand,\nsuppose your log of H is small",
    "start": "1336150",
    "end": "1342720"
  },
  {
    "text": "and your n is bigger than log of\nH, then you have a better bound which could be meaningful.",
    "start": "1342720",
    "end": "1348785"
  },
  {
    "text": " Any questions so far?",
    "start": "1348785",
    "end": "1356750"
  },
  {
    "text": "How does the [INAUDIBLE]? ",
    "start": "1356750",
    "end": "1364960"
  },
  {
    "text": "We are doing-- yes-- or no. This is the differentiating\nof the loss function.",
    "start": "1364960",
    "end": "1371480"
  },
  {
    "text": "So the loss function\nis the function of-- ",
    "start": "1371480",
    "end": "1378150"
  },
  {
    "text": "depending on how\nyou think about it, but by the\ndifferentiability, I really mean this function that\ntakes in y hat and y",
    "start": "1378150",
    "end": "1384250"
  },
  {
    "text": "and outputs a scalar. So taking the prediction,\nand the real label, and the optical scalar.",
    "start": "1384250",
    "end": "1389740"
  },
  {
    "text": "So whether this function is\ndifferentiable with respect to y and y hat.",
    "start": "1389740",
    "end": "1395800"
  },
  {
    "text": "So we didn't assume that this\nfunction is differentiable here. But implicitly, you are assuming\nthat this loss function is",
    "start": "1395800",
    "end": "1402309"
  },
  {
    "text": "differentiable with\nrespect to y and y hat in the previous asymptotic\nanalysis, because there,",
    "start": "1402310",
    "end": "1408003"
  },
  {
    "text": "actually, we assume\nthe whole loss function, if you compose\nit with the model, has to be differentiable.",
    "start": "1408003",
    "end": "1413328"
  },
  {
    "text": " [INAUDIBLE]",
    "start": "1413328",
    "end": "1426460"
  },
  {
    "text": "So-- I didn't hear very-- [INAUDIBLE] practical\nimplementation of floating point numbers-- did you use the same bound?",
    "start": "1426460",
    "end": "1434210"
  },
  {
    "text": "For practical implementations\nwhere you have floating points? Yeah.",
    "start": "1434210",
    "end": "1439310"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "1439310",
    "end": "1452610"
  },
  {
    "text": "So I guess my interpretation\nof the question-- maybe let me rephrase\nthe question a little bit also for the Zoom people\non the Zoom meeting.",
    "start": "1452610",
    "end": "1458370"
  },
  {
    "text": "So I think the proposal is\nthat, for example, if you really have a practical model,\nand you have p parameters,",
    "start": "1458370",
    "end": "1466190"
  },
  {
    "text": "and they are-- when you really implement\nthis in computer,",
    "start": "1466190",
    "end": "1471740"
  },
  {
    "text": "it's not continuous. So you can think of each\nparameter is described by maybe 32 bits,\nand then you can",
    "start": "1471740",
    "end": "1478940"
  },
  {
    "text": "count how many total possible\nnumber of different models there are, and apply this bound.",
    "start": "1478940",
    "end": "1484880"
  },
  {
    "text": "So yes. And that's a good idea, and\nthat will give you what?",
    "start": "1484880",
    "end": "1490550"
  },
  {
    "text": "That will give you that--\nsuppose you have p parameters. ",
    "start": "1490550",
    "end": "1496180"
  },
  {
    "text": "Let's say you have 32 bits. So 1 bit-- so then\nwhat does that mean?",
    "start": "1496180",
    "end": "1501570"
  },
  {
    "text": "That means that\nthe total size of h would be that for\nevery p parameter, you have 2 to the 32 choices,\nand you multiply that to p--",
    "start": "1501570",
    "end": "1511160"
  },
  {
    "text": "or you take the risk power to p. And so that means the log of H\nis equal to something like 32--",
    "start": "1511160",
    "end": "1518947"
  },
  {
    "text": "like O of p. It's a constant times p. So basically, you\nonly get the bound",
    "start": "1518947",
    "end": "1524780"
  },
  {
    "text": "that depends on the\nnumber of parameters.  And this is reasonable\nin some cases.",
    "start": "1524780",
    "end": "1533000"
  },
  {
    "text": "This is not very reasonable\nin some other cases. But definitely, it's a pretty--\nit's a bound that makes sense.",
    "start": "1533000",
    "end": "1540080"
  },
  {
    "text": "So in some of the later\nparts of the lecture, we are going to see\nhow to get a bound that",
    "start": "1540080",
    "end": "1545793"
  },
  {
    "text": "doesn't depend on parameter. But if you are fine with\ngetting a bound that depends on a number\nof parameters,",
    "start": "1545793",
    "end": "1551130"
  },
  {
    "text": "then this is indeed\na good bound. And this is actual\nnatural question",
    "start": "1551130",
    "end": "1557300"
  },
  {
    "text": "that leads me to the second\npart of the lecture-- today's lecture. So this is a\nproposal where you--",
    "start": "1557300",
    "end": "1564410"
  },
  {
    "text": "the proposal to do\nthis has a small con, or small kind of\nproblem, which is you",
    "start": "1564410",
    "end": "1569990"
  },
  {
    "text": "basically have to say\nthat I have to resort to practical implementation.",
    "start": "1569990",
    "end": "1575210"
  },
  {
    "text": "In practice, I cannot really\nimplement real numbers. All the real numbers, I have\nto discretize in some way.",
    "start": "1575210",
    "end": "1582360"
  },
  {
    "text": "So that's-- and sometimes,\nyou put additional restriction on yourself, saying that if I\ncan only use floating points,",
    "start": "1582360",
    "end": "1592520"
  },
  {
    "text": "then what bound I can have? So what I'm going\nto discuss next is that you don't\neven need this.",
    "start": "1592520",
    "end": "1597830"
  },
  {
    "text": "You can even say\nthat, even for all the possible\ncontinuous models which",
    "start": "1597830",
    "end": "1603403"
  },
  {
    "text": "are supposed to\nhave p parameters, and each parameter is\nreally a real number. Like, you can-- for\nexample, suppose",
    "start": "1603403",
    "end": "1608540"
  },
  {
    "text": "you have almighty computer which\ncan have infinite precision.",
    "start": "1608540",
    "end": "1613790"
  },
  {
    "text": "Still, your bound would still\nlook something like this. You still have O of p bound.",
    "start": "1613790",
    "end": "1619490"
  },
  {
    "text": "So then you don't have to-- suppose we have that infinite\nhypothesis class proof.",
    "start": "1619490",
    "end": "1625399"
  },
  {
    "text": "Then, you don't need\nthis practical-- ",
    "start": "1625400",
    "end": "1631120"
  },
  {
    "text": "this way of proving it. You can have a more generalized,\nstronger way to prove it, and that's what\nwe're looking for.",
    "start": "1631120",
    "end": "1638320"
  },
  {
    "text": "Cool.  So maybe let's start to do that.",
    "start": "1638320",
    "end": "1644620"
  },
  {
    "text": "So let's talk about\ninfinite hypothesis class. ",
    "start": "1644620",
    "end": "1652720"
  },
  {
    "text": "And as I suggested a\nlittle bit before-- so we are going to\nhave a bound that looks",
    "start": "1652720",
    "end": "1658090"
  },
  {
    "text": "like P over n square root. P over n, and P is the\nnumber of parameters. So this is something\nwe're going to have.",
    "start": "1658090",
    "end": "1664600"
  },
  {
    "text": "Cool. And so today, we're going to\ndo this so-called brute force",
    "start": "1664600",
    "end": "1673419"
  },
  {
    "text": "discretization. This is a technique of-- at least, this is how I name\nthis technique, I guess.",
    "start": "1673420",
    "end": "1679810"
  },
  {
    "text": "Because this technique\nis to brute force, I guess there's no\nreal name for it.",
    "start": "1679810",
    "end": "1687669"
  },
  {
    "text": "And what you can do\nis the following. So maybe-- yeah, let\nme state the theorem",
    "start": "1687670",
    "end": "1695982"
  },
  {
    "text": "that I'm going to\nprove first, and then I can tell you what's the\nintuition and how to prove it.",
    "start": "1695982",
    "end": "1701590"
  },
  {
    "text": "So this is the theorem. So suppose H is-- OK, I guess I'm\nstill setting up.",
    "start": "1701590",
    "end": "1709019"
  },
  {
    "text": "Suppose H is parameterized\nby theta in P dimension.",
    "start": "1709020",
    "end": "1715890"
  },
  {
    "text": "So H is--\nmathematically, you write H is a family of H sub theta.",
    "start": "1715890",
    "end": "1721020"
  },
  {
    "text": "Each H sub theta is a\nparameterized model, where a theta is in some set of\ntheta, which is a subset of Rp.",
    "start": "1721020",
    "end": "1729750"
  },
  {
    "text": "So capital theta is\nthe set of parameters you are going to choose from.",
    "start": "1729750",
    "end": "1735520"
  },
  {
    "text": "And in some sense, this\nis for convenience. But I guess you\nprobably wouldn't",
    "start": "1735520",
    "end": "1741270"
  },
  {
    "text": "see why this is only\nfor convenience, but it doesn't really matter. So suppose you only select\nmodels from this set,",
    "start": "1741270",
    "end": "1749370"
  },
  {
    "text": "where your norm of the\nmodel is bounded by B.",
    "start": "1749370",
    "end": "1755550"
  },
  {
    "text": "Our dependency on will be very-- will be only logarithmic. So in some sense, this is not\nreally a real restriction.",
    "start": "1755550",
    "end": "1760920"
  },
  {
    "text": "You can choose B to\nbe pretty big just because your dependency\non B is very relaxed where",
    "start": "1760920",
    "end": "1766200"
  },
  {
    "text": "it's logarithmic in B. So this is our setup. And also let's recall\nthat we're sometimes",
    "start": "1766200",
    "end": "1776160"
  },
  {
    "text": "going to use this notation. This is really-- we use\nall of these notations",
    "start": "1776160",
    "end": "1781420"
  },
  {
    "text": "interchangeably. So either this is really\njust a loss of theta--",
    "start": "1781420",
    "end": "1788070"
  },
  {
    "text": "the model theta on the\ndata point x and y. So it's really just\ncompare H theta of x and y,",
    "start": "1788070",
    "end": "1794610"
  },
  {
    "text": "and you get the loss. These two are just\nthe same thing. We are abusing the\nnotation a little bit.",
    "start": "1794610",
    "end": "1800400"
  },
  {
    "text": "And also recall that we\nhave L theta, L hat theta. This is all as we\ndefined before.",
    "start": "1800400",
    "end": "1805810"
  },
  {
    "text": "And so here's the theorem. So we still have to assume that\nthe loss is between 0 and 1.",
    "start": "1805810",
    "end": "1813410"
  },
  {
    "text": " This is probably\nalways assumed in most",
    "start": "1813410",
    "end": "1819090"
  },
  {
    "text": "of these lectures-- most of\nthis course for every x, y, and theta.",
    "start": "1819090",
    "end": "1825570"
  },
  {
    "text": "And suppose this is\nadditional assumption, where",
    "start": "1825570",
    "end": "1833559"
  },
  {
    "text": "you assume that this loss\nfunction is K-Lipschitz",
    "start": "1833560",
    "end": "1842240"
  },
  {
    "text": "in theta. ",
    "start": "1842240",
    "end": "1848530"
  },
  {
    "text": "So for every x and y. So what does this really mean?",
    "start": "1848530",
    "end": "1854710"
  },
  {
    "text": "This really means that you\nare assuming l of x, y, theta,",
    "start": "1854710",
    "end": "1860270"
  },
  {
    "text": "minus l of x, y,\nand theta prime. If you change your\nmodel to theta prime,",
    "start": "1860270",
    "end": "1865880"
  },
  {
    "text": "then your loss would be\ndifferent by a constant times theta minus theta\nprime [INAUDIBLE]..",
    "start": "1865880",
    "end": "1874340"
  },
  {
    "text": "So maybe let's try to-- this is kappa. ",
    "start": "1874340",
    "end": "1880770"
  },
  {
    "text": "So again, this-- our\ndependency on kappa",
    "start": "1880770",
    "end": "1886640"
  },
  {
    "text": "will also be logarithmic. So in some sense,\nthis is also not assuming much, because if your\nloss is somewhat continuous,",
    "start": "1886640",
    "end": "1899480"
  },
  {
    "text": "then you can have a-- it's going to be\nLipschitz to some extent.",
    "start": "1899480",
    "end": "1904597"
  },
  {
    "text": "Probably, the Lipschitz\nconstant is not very good, but the Lipschitz constant\nwould be something reasonable. And if you take\nlogarithmic of it,",
    "start": "1904597",
    "end": "1910910"
  },
  {
    "text": "it's not very sensitive\nto the Lipschitz constant. And then with this, you get\nwith probability and the least 1",
    "start": "1910910",
    "end": "1922640"
  },
  {
    "text": "minus, I guess, O\nof e to the minus P. So actually, you\nhave even higher--",
    "start": "1922640",
    "end": "1930590"
  },
  {
    "text": "even lower failure probability. The failure probability\nis e to the minus P. So with such small\nfailure probability,",
    "start": "1930590",
    "end": "1938030"
  },
  {
    "text": "you get that for\nevery theta, you have the uniform convergence\nis less than some big O",
    "start": "1938030",
    "end": "1946475"
  },
  {
    "text": "of square root P over n times\nmax 1, and ln, kappa Bn.",
    "start": "1946475",
    "end": "1957000"
  },
  {
    "text": " So eventually, the dependency\non kappa and B are logarithmic.",
    "start": "1957000",
    "end": "1965809"
  },
  {
    "text": "That's what I promised. And the main thing\nis really P over n, so you get the\nparameter dependency",
    "start": "1965810",
    "end": "1972020"
  },
  {
    "text": "and you get the\ndependency of it. And you still have\nthe square root here, so this is still worse\nthan the asymptotic bound",
    "start": "1972020",
    "end": "1979129"
  },
  {
    "text": "if you compare with the leading\nterm of the asymptotic bound. But as we said, you don't\nhave the second order",
    "start": "1979130",
    "end": "1985070"
  },
  {
    "text": "term in an asymptotic bound.  So how do we--",
    "start": "1985070",
    "end": "1992904"
  },
  {
    "text": "how to prove this? So actually, the proof\nis very similar to what",
    "start": "1992905",
    "end": "1998070"
  },
  {
    "text": "was suggested in the question. So you are doing\nthis quantization, and then you deal with\nthe discretization error",
    "start": "1998070",
    "end": "2004429"
  },
  {
    "text": "separately. So what you do is the following. So let me start with a\nsketch in some sense.",
    "start": "2004430",
    "end": "2013646"
  },
  {
    "text": "So the kind of alternate\nsketch is the following. So you define E theta--",
    "start": "2013646",
    "end": "2021019"
  },
  {
    "text": "be the event that-- you have this failure event.",
    "start": "2021020",
    "end": "2026070"
  },
  {
    "text": "L hat theta minus L theta\nis larger than epsilon. And epsilon is going\nto be something TBD.",
    "start": "2026070",
    "end": "2033060"
  },
  {
    "text": "But epsilon would be very\nsimilar to this thing, because you care about whether\nthese two are different-- how--",
    "start": "2033060",
    "end": "2040440"
  },
  {
    "text": "this much different. But anyway, so\nabsolutely some number. This is kind of\nlike a placeholder.",
    "start": "2040440",
    "end": "2045780"
  },
  {
    "text": "So you care about\nthis kind of event. And we know that this will be\na small probability event as we",
    "start": "2045780",
    "end": "2052590"
  },
  {
    "text": "have shown for the final case. So this E theta is a\nsmall probability event. ",
    "start": "2052590",
    "end": "2061760"
  },
  {
    "text": "And before we called that-- what\nwe did is which we said that the union of this E theta is\nless than the sum of the E",
    "start": "2061760",
    "end": "2068960"
  },
  {
    "text": "theta-- the probability of E theta. But now, because you have\ninfinite number of theta,",
    "start": "2068960",
    "end": "2074780"
  },
  {
    "text": "this is infinite, because each\none has some small probability",
    "start": "2074780",
    "end": "2079919"
  },
  {
    "text": "event so to fail. And you take the sum over\nall possible events-- then you get an infinite\nnumber anything.",
    "start": "2079920",
    "end": "2087138"
  },
  {
    "text": "Like each of these\nwill be some epsilon. You take the sum of\ninfinite number of things-- you get infinite, so\nthat's why it doesn't work.",
    "start": "2087139",
    "end": "2094440"
  },
  {
    "text": "You cannot use exactly\nthe same thing as before. ",
    "start": "2094440",
    "end": "2099450"
  },
  {
    "text": "But the reason why\nthis can be fixed is because this union\nbound is very pessimistic.",
    "start": "2099450",
    "end": "2105620"
  },
  {
    "start": "2105620",
    "end": "2112310"
  },
  {
    "text": "So if you think\nabout union bound-- so union bound is\nreally just saying that, I guess I'm not sure--",
    "start": "2112310",
    "end": "2118620"
  },
  {
    "text": "and depending on\nhow you learn union bounds in previous lectures. Like what I learned about the\nunion bound is the following.",
    "start": "2118620",
    "end": "2124960"
  },
  {
    "text": "For example, you have-- this\nis the full probability space. And each event takes\nsome part of space.",
    "start": "2124960",
    "end": "2131260"
  },
  {
    "text": "Maybe this is E theta 1-- this is E1 and this is maybe E2.",
    "start": "2131260",
    "end": "2137430"
  },
  {
    "text": "And the optimal-- when the union\nbound would be tight is when all of these events--",
    "start": "2137430",
    "end": "2144168"
  },
  {
    "text": "I call it-- they call them\nfailure events-- all of them are destroyed. So suppose this is the case.",
    "start": "2144168",
    "end": "2149940"
  },
  {
    "text": "Then, the union of\nthese events will be the sum of the probability\nof each of the events. But here, it's not clear whether\nthese events are disjoint.",
    "start": "2149940",
    "end": "2158100"
  },
  {
    "text": "And actually, they may\nhave a lot of overlap. So you have 1 theta-- so E theta.",
    "start": "2158100",
    "end": "2163599"
  },
  {
    "text": "And if you change your\ntheta to your nearby theta, you probably have something like\nthis, which is E theta prime.",
    "start": "2163600",
    "end": "2169043"
  },
  {
    "text": "And they have all\nof this overlap. And then your union bound\nstarts to be very loose.",
    "start": "2169043",
    "end": "2174210"
  },
  {
    "text": "So that also kind of\nmotivates our way to fix it. The way that we fix\nit is the following.",
    "start": "2174210",
    "end": "2181150"
  },
  {
    "text": "So we fix it by\nfirst picking not--",
    "start": "2181150",
    "end": "2188299"
  },
  {
    "text": "we don't take union bound\nover all possible events. We select a subset of events,\nand with a union bound",
    "start": "2188300",
    "end": "2193684"
  },
  {
    "text": "over them. And then we say the\nother events will be close to some of this\nsubset of prototypical events.",
    "start": "2193685",
    "end": "2201530"
  },
  {
    "text": "So basically, the\nidea is that you-- the rough idea is\nthat you select some prototypical\ntypical events.",
    "start": "2201530",
    "end": "2209390"
  },
  {
    "text": " Sorry. Maybe I should just\nsay typical events.",
    "start": "2209390",
    "end": "2215767"
  },
  {
    "text": "Or you just take\nsome exemplar events. I don't know how-- I forgot how to spell that-- some exemplar events.",
    "start": "2215767",
    "end": "2222240"
  },
  {
    "text": "And this subset of events you-- is a smaller set of events than\nwhat you finally care about.",
    "start": "2222240",
    "end": "2228480"
  },
  {
    "text": "And then, you use union\nbound on the subset. ",
    "start": "2228480",
    "end": "2236640"
  },
  {
    "text": "And then you say\nthat other events are similar to the subset--",
    "start": "2236640",
    "end": "2245180"
  },
  {
    "text": "to the exemplars.  So then, you cover\nall the events,",
    "start": "2245180",
    "end": "2252900"
  },
  {
    "text": "so that's the rough idea. So let's see how\nwe exactly do this. So to exactly do this, we need\nto introduce something called--",
    "start": "2252900",
    "end": "2264360"
  },
  {
    "text": "any questions so far?  So to exactly do this, we need\nto introduce something called",
    "start": "2264360",
    "end": "2270990"
  },
  {
    "text": "discretized epsilon cover. This is actually also a useful\ntool for other cases as well.",
    "start": "2270990",
    "end": "2280386"
  },
  {
    "text": "So let me first define\nthis epsilon cover, and then say why it's--",
    "start": "2280386",
    "end": "2288150"
  },
  {
    "text": "it's kind of like a\nlanguage to describe what are called\nprototypical events,",
    "start": "2288150",
    "end": "2294210"
  },
  {
    "text": "or prototypical kind of\nparameters or models. So epsilon net-- sometimes,\nthose are called epsilon net--",
    "start": "2294210",
    "end": "2302190"
  },
  {
    "text": "sometimes, it's\ncalled epsilon cover-- of a set, S. And here, S\ncorresponds to the family",
    "start": "2302190",
    "end": "2309110"
  },
  {
    "text": "of all models you care about. And you care about\na subset of models. And if you-- this--",
    "start": "2309110",
    "end": "2315290"
  },
  {
    "text": "and with respect-- when\nyou really define it, you have to specify\na metric, rho.",
    "start": "2315290",
    "end": "2320359"
  },
  {
    "text": " Rho is a set, C, which is\nalso a subset of S. But I",
    "start": "2320360",
    "end": "2332698"
  },
  {
    "text": "think technically, we\ndon't have to require C to be a subset of S, but\nI think in almost all cases, it's a subset of S.",
    "start": "2332698",
    "end": "2339650"
  },
  {
    "text": "Such that for every x in S-- so there exists kind\nof a neighbor in C",
    "start": "2339650",
    "end": "2349730"
  },
  {
    "text": "which is close to x. ",
    "start": "2349730",
    "end": "2358040"
  },
  {
    "text": "So if you draw this, it's\nkind of like you're saying",
    "start": "2358040",
    "end": "2363780"
  },
  {
    "text": "that you have a set of models-- of parameters. This is called S.",
    "start": "2363780",
    "end": "2369390"
  },
  {
    "text": "And the epsilon cover\nis a subset of S. So as you select subpoints--\nand let's call these points--",
    "start": "2369390",
    "end": "2376530"
  },
  {
    "text": "these are all in C. And then\nyou say that the set of C",
    "start": "2376530",
    "end": "2381540"
  },
  {
    "text": "needs to satisfy the following\nto be an epsilon cover. So what it has to satisfy-- it\nhas to satisfy that for every",
    "start": "2381540",
    "end": "2387030"
  },
  {
    "text": "point you pick in x in S-- you have to pick\nthis point x in S-- there exists a\nneighbor-- somewhat kind",
    "start": "2387030",
    "end": "2392774"
  },
  {
    "text": "of a neighbor in C.\nLet's call this x prime.",
    "start": "2392774",
    "end": "2398849"
  },
  {
    "text": "I guess I cross and x\nseems to be the same. I'm not sure whether--",
    "start": "2398850",
    "end": "2404730"
  },
  {
    "text": "maybe I should-- anyway,\nyou see what I mean. The purple cross is\njust indicating a point.",
    "start": "2404730",
    "end": "2410400"
  },
  {
    "text": "So you have a point x\nhere, and you can always find some other point in\nC such that x prime is",
    "start": "2410400",
    "end": "2418440"
  },
  {
    "text": "kind of close to x. So that's basically is saying\nthat all of these points in C",
    "start": "2418440",
    "end": "2423840"
  },
  {
    "text": "are prototypical kind of\npoints, because every point in S can find a neighbor in\nC. Does that make sense?",
    "start": "2423840",
    "end": "2431370"
  },
  {
    "start": "2431370",
    "end": "2439210"
  },
  {
    "text": "And equivalently,\nyou can also write this in the following way. So equivalently-- this\nis in some sense more--",
    "start": "2439210",
    "end": "2450290"
  },
  {
    "text": "explaining why this is\ncalled epsilon cover. So equivalently,\nyou can write this as the S is covered by the union\nof the ball around all the x.",
    "start": "2450290",
    "end": "2464680"
  },
  {
    "text": "Let me write down and\nexplain what this is. So first of all, this is the-- so this thing is\nthe ball centered",
    "start": "2464680",
    "end": "2474984"
  },
  {
    "text": "at x with radius\nepsilon and distance",
    "start": "2474984",
    "end": "2483990"
  },
  {
    "text": "metric-- or metric rho. So basically, this is\nsaying the following.",
    "start": "2483990",
    "end": "2491190"
  },
  {
    "text": "So this is the equivalent\ndefinition of epsilon cover. So you are saying\nthat if you look at all the balls around all\nof these purple points--",
    "start": "2491190",
    "end": "2501195"
  },
  {
    "text": " so this is-- the\nradius is epsilon. So in some sense, you\ncan say that this ball--",
    "start": "2501195",
    "end": "2509540"
  },
  {
    "text": "so this point covers\nthe entire ball, because for every\npoint in this ball,",
    "start": "2509540",
    "end": "2515760"
  },
  {
    "text": "you can find-- you\ncan use this point-- the center as the neighbor. So basically, every point\ncovers some part of the space.",
    "start": "2515760",
    "end": "2524310"
  },
  {
    "text": "And so the requirement is that-- if you look at all\nthe points that can-- if you look at all the\nballs around all the centers,",
    "start": "2524310",
    "end": "2534470"
  },
  {
    "text": "then this would\ncover the entire S. That means that every point in\nS can be covered by some ball,",
    "start": "2534470",
    "end": "2542030"
  },
  {
    "text": "and that means every point\nin S has a neighbor in C.",
    "start": "2542030",
    "end": "2552345"
  },
  {
    "text": "Any questions? ",
    "start": "2552345",
    "end": "2564880"
  },
  {
    "text": "C might not be--",
    "start": "2564880",
    "end": "2570069"
  },
  {
    "text": "in some sense, we will\ninsist that C-- like, we will need to find a very\nsmall cover C, which is finite.",
    "start": "2570070",
    "end": "2575950"
  },
  {
    "text": "And also, hopefully, we want\nthe size of C to be small. But by definition,\nthere's nothing about whether it's finite or\nnot, but we will construct",
    "start": "2575950",
    "end": "2583652"
  },
  {
    "text": "epsilon cover that is finite. ",
    "start": "2583652",
    "end": "2589720"
  },
  {
    "text": "So this is-- so so far,\nthis is only a definition saying that C is\nepsilon cover of S.",
    "start": "2589720",
    "end": "2596560"
  },
  {
    "text": "And we will try to\nmake C be small. And this is actually exactly-- ",
    "start": "2596560",
    "end": "2611952"
  },
  {
    "text": "so this is exactly what\nwe're going to do next. So how do we construct a set, C,\nthat is finite and also covers",
    "start": "2611952",
    "end": "2617650"
  },
  {
    "text": "the entire set? So what is S? So for us, S is the\nset of all parameters.",
    "start": "2617650",
    "end": "2624170"
  },
  {
    "text": "It's the set of parameter theta\nwith this l2 norm less than B.",
    "start": "2624170",
    "end": "2629480"
  },
  {
    "text": "You only construct a\nsubset of parameters that can cover all the parameters.",
    "start": "2629480",
    "end": "2634890"
  },
  {
    "text": "So here is the lemma that\nsays that you can do this. And you can have finite\nC, and also, actually,",
    "start": "2634890",
    "end": "2640579"
  },
  {
    "text": "you can have a reasonable\nbound on how many C's-- how many points in C there are. ",
    "start": "2640580",
    "end": "2650220"
  },
  {
    "text": "So let's define this to be-- I guess, for this lemma,\nI call this theta.",
    "start": "2650220",
    "end": "2656160"
  },
  {
    "text": "So theta is defined as above. So for every epsilon in 0\nand B, for every radius,",
    "start": "2656160",
    "end": "2666690"
  },
  {
    "text": "there exists an epsilon cover\nof this theta, the l2 norm",
    "start": "2666690",
    "end": "2673819"
  },
  {
    "text": "with radius B such\nthat with at most 3B",
    "start": "2673820",
    "end": "2683660"
  },
  {
    "text": "over epsilon to the\npower P elements. So this is a cover, and\nthe size of this cover",
    "start": "2683660",
    "end": "2691840"
  },
  {
    "text": "is bounded by 3B over\nepsilon to the power of P.",
    "start": "2691840",
    "end": "2699910"
  },
  {
    "text": "So I think this is actually-- we're going to prove\na weaker version. The full-- we're going to\nhave a homework question which",
    "start": "2699910",
    "end": "2708280"
  },
  {
    "text": "guides you to prove\nexactly this version. So for now, in\nthe lecture, we're going to prove a weaker version\nwhich is somewhat easier.",
    "start": "2708280",
    "end": "2719420"
  },
  {
    "text": "So this weaker version-- and also actually\nsuffices for our purpose. So you don't really necessarily\nneed a stronger version",
    "start": "2719420",
    "end": "2725650"
  },
  {
    "text": "to prove the final theorem just\nbecause the weaker version is only weaker by a little bit.",
    "start": "2725650",
    "end": "2731420"
  },
  {
    "text": "So I guess the homework will\nguide you towards the stronger",
    "start": "2731420",
    "end": "2737932"
  },
  {
    "text": "version, which also introduces\nsome techniques which are useful. ",
    "start": "2737933",
    "end": "2743220"
  },
  {
    "text": "So here is the weaker version. The weaker version\nis pretty much like you discretize\nyour computer. You just do a trivial\ndiscretization using some grid.",
    "start": "2743220",
    "end": "2752040"
  },
  {
    "text": "So what you do is\nyou just take C-- be a trivial grid in some sense.",
    "start": "2752040",
    "end": "2762230"
  },
  {
    "text": "So what does that mean? It really means that\nyou have this ball-- I guess there--",
    "start": "2762230",
    "end": "2767950"
  },
  {
    "text": "I guess if you have\nthis ball, and you just say that you take that-- some\narbitrary coordinate system.",
    "start": "2767950",
    "end": "2775780"
  },
  {
    "text": "You just take the\nnatural coordinate system and you discretize\nyour space like this. ",
    "start": "2775780",
    "end": "2786070"
  },
  {
    "text": "And then you take all\nthese points as your C,",
    "start": "2786070",
    "end": "2791230"
  },
  {
    "text": "and that's it. And then the question is\njust a matter of counting and how fine-grained\nyour grid needs to be.",
    "start": "2791230",
    "end": "2798790"
  },
  {
    "text": "So formally-- so C is taken to\nbe all the points x in RP such",
    "start": "2798790",
    "end": "2806080"
  },
  {
    "text": "that xi, the coordinate,\nis a multiple--",
    "start": "2806080",
    "end": "2815090"
  },
  {
    "text": "I guess this is k. k times epsilon over\nsquare root of P.",
    "start": "2815090",
    "end": "2820369"
  },
  {
    "text": "So epsilon over square\nroot of P is my grid size, and k is the integer\nmultiplied with it.",
    "start": "2820370",
    "end": "2828609"
  },
  {
    "text": "For some integer on k where k is\nsmaller than B square root of P",
    "start": "2828610",
    "end": "2838990"
  },
  {
    "text": "over epsilon. Why I have this constraint on\nk is because at some point, you don't need more points\nbecause you already--",
    "start": "2838990",
    "end": "2846550"
  },
  {
    "text": "you don't have to do\nanything beyond this part, because if your k is too\nbig, you already offset x,",
    "start": "2846550",
    "end": "2853010"
  },
  {
    "text": "and there is no point. And if you do the calculation,\nthis is the right thing.",
    "start": "2853010",
    "end": "2858609"
  },
  {
    "text": "And so now, we have\nto do two things. One thing is we have\nto see how large C is,",
    "start": "2858610",
    "end": "2864563"
  },
  {
    "text": "and the second thing\nis we have to prove that this is epsilon cover. So let's do the first thing.",
    "start": "2864563",
    "end": "2870190"
  },
  {
    "text": "So why this is epsilon\ncover is because if you look at any point\nx in S, you just",
    "start": "2870190",
    "end": "2877270"
  },
  {
    "text": "round it to the nearest point. So when you run\nit, you run it to--",
    "start": "2877270",
    "end": "2884590"
  },
  {
    "text": "so you do some rounding-- ",
    "start": "2884590",
    "end": "2889859"
  },
  {
    "text": "let's see. I guess when you run it, you\nrun it to let's call it x prime.",
    "start": "2889860",
    "end": "2896710"
  },
  {
    "text": "Let me not write\nexactly what the long-- the long way just means you take\nany vertex in this grid and you",
    "start": "2896710",
    "end": "2905049"
  },
  {
    "text": "round it-- the nearest-- any reasonable\nnearest-- that's what I mean. You just do the trivial run.",
    "start": "2905050",
    "end": "2913110"
  },
  {
    "text": "Let's say we run to\na smaller number. It doesn't really\nmatter that much. So if you run it-- so what you\ngot is that xi minus xi prime",
    "start": "2913110",
    "end": "2924360"
  },
  {
    "text": "is less than epsilon\nover square root of P. Because for every dimension\nwhen you're wrong,",
    "start": "2924360",
    "end": "2930720"
  },
  {
    "text": "you increase--\nyou create epsilon over square root of p error. epsilon over square root\nof P is your grid size.",
    "start": "2930720",
    "end": "2936900"
  },
  {
    "text": "And that means that the\ndistance between x and x prime in the l2 sense--",
    "start": "2936900",
    "end": "2944700"
  },
  {
    "text": "this is, I guess-- so I think this is-- I should mention\nthat the rho l2 norm.",
    "start": "2944700",
    "end": "2954500"
  },
  {
    "text": "The rho-- this-- yeah, I should have\nmentioned this. So the magic we are\nusing is rho is l2 norm.",
    "start": "2954500",
    "end": "2962481"
  },
  {
    "start": "2962481",
    "end": "2972780"
  },
  {
    "text": "So then if you look at l2\nnumber of these two things-- so this is the sum of xi\nminus xi prime squared, i",
    "start": "2972780",
    "end": "2984050"
  },
  {
    "text": "from 1 to P. And then\nyou bound each corner. You get P times\nepsilon squared over P",
    "start": "2984050",
    "end": "2991330"
  },
  {
    "text": "squared root, which is epsilon. That's actually why\nI chose the grid size to be epsilon over\nsquare root of P",
    "start": "2991330",
    "end": "2997270"
  },
  {
    "text": "just because I want to make\nit epsilon right there. So this proves that it's\nepsilon cover, right?",
    "start": "2997270",
    "end": "3002655"
  },
  {
    "text": " And also we can\ncount how large C is.",
    "start": "3002655",
    "end": "3009540"
  },
  {
    "text": "So C is what? C is something to the power P,\nbecause for every coordinate, you have a bunch\nof choices for k.",
    "start": "3009540",
    "end": "3016619"
  },
  {
    "text": "And how many choices\nfor k there are-- basically, this was--\nhere is like k--",
    "start": "3016620",
    "end": "3022500"
  },
  {
    "text": "the absolute value of k is\nless than B square root of P over epsilon. So basically, you get B\nsquare root of P over epsilon.",
    "start": "3022500",
    "end": "3029960"
  },
  {
    "text": "And because it can be\npositive and active, that's why you multiply 2. And it can also be\n0, so you add 1.",
    "start": "3029960",
    "end": "3035520"
  },
  {
    "text": "So that's the total\nnumber of choices in C.",
    "start": "3035520",
    "end": "3042950"
  },
  {
    "text": "And one common is that-- eventually, only log C\nmatters as you'll see.",
    "start": "3042950",
    "end": "3048680"
  },
  {
    "text": "So log C will be P log 2B square\nroot of P over epsilon plus 1.",
    "start": "3048680",
    "end": "3054900"
  },
  {
    "text": "And that's why this\nweaker version is not super different from\nthe stronger version, because the difference--\nso the stronger version was",
    "start": "3054900",
    "end": "3065280"
  },
  {
    "text": "3B over epsilon\nto the power of P. And the log becomes P\nlog 3B over epsilon.",
    "start": "3065280",
    "end": "3073980"
  },
  {
    "text": "And if we compare the stronger\nversion with the weaker version, the only\nthing that's different is the square root\nof P and the log.",
    "start": "3073980",
    "end": "3079150"
  },
  {
    "text": "So that's why\neventually, it doesn't change the bounds too much. ",
    "start": "3079150",
    "end": "3084300"
  },
  {
    "text": "Cool. So this is our proof for the\nweaker version of the lemma.",
    "start": "3084300",
    "end": "3090690"
  },
  {
    "text": "And now, let's use this\nlemma and the epsilon cover to prove the final bound.",
    "start": "3090690",
    "end": "3098339"
  },
  {
    "text": "So as we planned, what\nwe do is that we first apply finite hypothesis case--",
    "start": "3098340",
    "end": "3113220"
  },
  {
    "start": "3113220",
    "end": "3120930"
  },
  {
    "text": "the finite hypothesis analysis\nto C. And then, you say that--",
    "start": "3120930",
    "end": "3132640"
  },
  {
    "text": "then you somewhat--\nso this may be-- let's say this is\nnumber one, and then you say that extend 1\nto the whole set, S.",
    "start": "3132640",
    "end": "3144130"
  },
  {
    "text": "So now, the first\nstep should be trivial because we already proved it.",
    "start": "3144130",
    "end": "3150520"
  },
  {
    "text": "So if you want to do i, then\nbasically you got that if you--",
    "start": "3150520",
    "end": "3157420"
  },
  {
    "text": "the first thing is that you do\nit for every fixed theta in C.",
    "start": "3157420",
    "end": "3164690"
  },
  {
    "text": "Then, you have\nprobability of this-- ",
    "start": "3164690",
    "end": "3170040"
  },
  {
    "text": "similar to epsilon, if you\nuse Hoeffding's inequality, you get this is-- I guess let's call\nit epsilon tilde,",
    "start": "3170040",
    "end": "3175110"
  },
  {
    "text": "because this epsilon tilde will\nbe tuned to be decided later to make the bounds fit.",
    "start": "3175110",
    "end": "3181920"
  },
  {
    "text": "So you got this is\n2 times exponential minus 2n epsilon square root.",
    "start": "3181920",
    "end": "3187110"
  },
  {
    "text": "This is by Hoeffding-- exactly the same thing\nas we have done before.",
    "start": "3187110",
    "end": "3192810"
  },
  {
    "text": "And then you take a union bound.  You got the probability\nthat for every theta--",
    "start": "3192810",
    "end": "3200760"
  },
  {
    "text": "I guess exists theta in C\nsuch that this is not right.",
    "start": "3200760",
    "end": "3209270"
  },
  {
    "text": " It's small.",
    "start": "3209270",
    "end": "3214500"
  },
  {
    "text": "And how small it is? You multiply C with\nthis exponential minus 2n epsilon theta squared.",
    "start": "3214500",
    "end": "3221190"
  },
  {
    "text": " So these two steps\nare exactly as we did.",
    "start": "3221190",
    "end": "3226690"
  },
  {
    "text": "And if you flip this,\nyou get 1 minus--",
    "start": "3226690",
    "end": "3232890"
  },
  {
    "text": "so probability\nof-- the good event",
    "start": "3232890",
    "end": "3237930"
  },
  {
    "text": "happens with high probability. ",
    "start": "3237930",
    "end": "3246539"
  },
  {
    "text": "I'm just flipping it. So now, we have to\ndo the second step.",
    "start": "3246540",
    "end": "3253230"
  },
  {
    "text": "How do we extend this\nfor everything in S? And so second-- and we are\nbasically using Lipschitzness.",
    "start": "3253230",
    "end": "3269050"
  },
  {
    "text": "And you can see that this is not\nreally anything super clever. It's kind of like a\nsubset brute force.",
    "start": "3269050",
    "end": "3274960"
  },
  {
    "text": " So just for some quick\npreparation-- so because l,",
    "start": "3274960",
    "end": "3282600"
  },
  {
    "text": "x theta is kappa Lipschitz--",
    "start": "3282600",
    "end": "3289410"
  },
  {
    "text": "this is kappa\nLipschitz in theta,",
    "start": "3289410",
    "end": "3296700"
  },
  {
    "text": "this implies that L\ntheta and L hat theta",
    "start": "3296700",
    "end": "3302160"
  },
  {
    "text": "are both kappa Lipschitz. ",
    "start": "3302160",
    "end": "3307910"
  },
  {
    "text": "Why? This is just because if you\naverage two kappa Lipschitz functions, they are\nstill kappa Lipschitzs.",
    "start": "3307910",
    "end": "3314300"
  },
  {
    "text": "So if f is kappa Lipschitz,\ng is kappa Lipschitz.",
    "start": "3314300",
    "end": "3321280"
  },
  {
    "text": "f plus g over 2 is\nalso a kappa Lipschitz. And you can prove this by a\nsimple triangle inequality.",
    "start": "3321280",
    "end": "3329260"
  },
  {
    "text": "And you can do this\nfor multiple functions, not only just two functions. You can do it for n functions.",
    "start": "3329260",
    "end": "3335420"
  },
  {
    "text": "So suppose we have this. ",
    "start": "3335420",
    "end": "3343430"
  },
  {
    "text": "Now-- so we also know that for--\nso suppose we know for every-- so we already know\nthat for every theta--",
    "start": "3343430",
    "end": "3351619"
  },
  {
    "text": "so L hat theta-- so it's supposed to be\nconditional on this event.",
    "start": "3351620",
    "end": "3356725"
  },
  {
    "text": "So with some chance-- with\na very high probability, this happens. And suppose this happens. This condition--\nthis event, we want",
    "start": "3356725",
    "end": "3362060"
  },
  {
    "text": "to prove that the same thing\nhappens when we replace C by theta-- by S. And so this means that if you\nhave-- for every theta in--",
    "start": "3362060",
    "end": "3371480"
  },
  {
    "text": "I guess I call it capital\ntheta, but not S. Capital theta, the ball, you\ncan find some theta 0",
    "start": "3371480",
    "end": "3378300"
  },
  {
    "text": "in C such that theta minus\ntheta 0 l2 is less than epsilon.",
    "start": "3378300",
    "end": "3384790"
  },
  {
    "text": "This is by the definition\nof epsilon cover. C is epsilon cover\nof capital theta. That's why you have this.",
    "start": "3384790",
    "end": "3391680"
  },
  {
    "text": "And then this implies that\nL theta minus L theta 0 is",
    "start": "3391680",
    "end": "3398710"
  },
  {
    "text": "less than kappa times epsilon. This is by Lipschitzness. ",
    "start": "3398710",
    "end": "3408809"
  },
  {
    "text": "And so in some\nsense, you just use theta 0 as a reference point. So what you finally care\nabout is L hat theta minus L--",
    "start": "3408810",
    "end": "3417069"
  },
  {
    "text": "so I guess you also know this. You don't know this. You also know L hat theta\nminus L theta hat theta",
    "start": "3417070",
    "end": "3424809"
  },
  {
    "text": "0 is less than\nkappa times epsilon. This is also by Lipschitz. ",
    "start": "3424810",
    "end": "3438840"
  },
  {
    "text": "So now, with this tool, you\ncan-- what eventually you want is you bound the difference\nbetween L hat theta and L",
    "start": "3438840",
    "end": "3445770"
  },
  {
    "text": "theta. And we have seen this kind of\ntriangle inequality-- this kind",
    "start": "3445770",
    "end": "3451590"
  },
  {
    "text": "of manipulation already. Because eventually, you care\nabout the difference between L hat and L, but you use the\ntheta 0-- some reference points",
    "start": "3451590",
    "end": "3459450"
  },
  {
    "text": "to kind of bridge them. So you do this decomposition.",
    "start": "3459450",
    "end": "3464460"
  },
  {
    "text": "You say that this is L hat theta\nminus L hat theta 0, plus L hat",
    "start": "3464460",
    "end": "3470220"
  },
  {
    "text": "theta 0, minus l theta 0,\nplus l theta 0, minus l theta.",
    "start": "3470220",
    "end": "3479890"
  },
  {
    "text": "And now, these two things\nare about differences between theta and theta 0. So this quantity is less\nthan kappa times epsilon.",
    "start": "3479890",
    "end": "3490650"
  },
  {
    "text": "And this quantity is also\nless than kappa times epsilon. And this quantity is\nless than epsilon.",
    "start": "3490650",
    "end": "3498470"
  },
  {
    "text": "This is because theta 0 is\nin C. So we have already proved that for\nevery theta in C,",
    "start": "3498470",
    "end": "3504680"
  },
  {
    "text": "L hat theta is equal\nto-- is close to L theta.",
    "start": "3504680",
    "end": "3509720"
  },
  {
    "text": "So that's why we got\nthis 3 inequality. So in total, if you\nlook at absolute value,",
    "start": "3509720",
    "end": "3516755"
  },
  {
    "text": "then you can do the\ntriangle inequality to get the absolute\nvalue of the sum of-- the sum of the absolute\nvalue of each of them.",
    "start": "3516755",
    "end": "3522520"
  },
  {
    "text": "You get 2 kappa\ntimes epsilon, plus-- ",
    "start": "3522520",
    "end": "3528140"
  },
  {
    "text": "oh, epsilon tilde. So sorry, this is epsilon\ntilde, because recall",
    "start": "3528140",
    "end": "3533180"
  },
  {
    "text": "that I used a different\nepsilon for the concentration just so that I can tune this\nepsilon tilde eventually.",
    "start": "3533180",
    "end": "3542080"
  },
  {
    "text": "And if I-- now is the\ntime to start epsilon",
    "start": "3542080",
    "end": "3547750"
  },
  {
    "text": "to be epsilon tilde over 2\nkappa, or maybe you can do it, and then we want\nlike epsilon tilde",
    "start": "3547750",
    "end": "3553060"
  },
  {
    "text": "to be epsilon times 2 kappa. Then you get-- so that you\nbalance these two error terms,",
    "start": "3553060",
    "end": "3559790"
  },
  {
    "text": "so you get this is less\nthan 2 epsilon tilde.",
    "start": "3559790",
    "end": "3566610"
  },
  {
    "start": "3566610",
    "end": "3575610"
  },
  {
    "text": "So now, let's look at the-- ",
    "start": "3575610",
    "end": "3580920"
  },
  {
    "text": "what's the-- let's\ngo back to here.",
    "start": "3580920",
    "end": "3587002"
  },
  {
    "text": "Because here, there is\nsomething about the cover size we have to deal with. We have to plug in\nthe right cover size.",
    "start": "3587002",
    "end": "3593740"
  },
  {
    "text": "And what is the cover size? So the cover size was-- ",
    "start": "3593740",
    "end": "3599220"
  },
  {
    "text": "so log cover size-- ",
    "start": "3599220",
    "end": "3605050"
  },
  {
    "text": "log C is equal to log 3B over\nepsilon to the power of P.",
    "start": "3605050",
    "end": "3612530"
  },
  {
    "text": "And I have already\nset epsilon to be epsilon tilde over\n2 kappa, so I need to plug in that so I get P--",
    "start": "3612530",
    "end": "3620070"
  },
  {
    "text": "oh, let's first to\nget this, and then let's plug in the\nchoice of epsilon tilde. So we get P log 3B\nkappa epsilon tilde.",
    "start": "3620070",
    "end": "3631040"
  },
  {
    "text": " And you can see that\nkappa is inside the log,",
    "start": "3631040",
    "end": "3637119"
  },
  {
    "text": "so that's why it's somewhat\nnot sensitive to the choice of kappa.",
    "start": "3637120",
    "end": "3642160"
  },
  {
    "text": "And also epsilon tilde is also\nin the log, which is also nice.",
    "start": "3642160",
    "end": "3647770"
  },
  {
    "text": "And now, we have to care about\nthis failure probability. So we basically want\nto say that this is equal to something like delta.",
    "start": "3647770",
    "end": "3655690"
  },
  {
    "text": "So we want to bound the failure\nprobability to C exponential,",
    "start": "3655690",
    "end": "3666839"
  },
  {
    "text": "minus 2n epsilon tilde squared. So this is something-- we'll\nshow that this is small. Actually, in this\ncase, I'm hoping",
    "start": "3666840",
    "end": "3674010"
  },
  {
    "text": "to show that this\nexponential minus P.",
    "start": "3674010",
    "end": "3686450"
  },
  {
    "text": "So how do we show this? And of course, it depends\non what epsilon tilde is. So you need to choose the\nright epsilon pseudo such",
    "start": "3686450",
    "end": "3692510"
  },
  {
    "text": "that this is true, and that's\nbasically your final bound. And just to get\nsomething-- so you're",
    "start": "3692510",
    "end": "3700160"
  },
  {
    "text": "going to see that the\nexact calculation of this is going to be a\nlittle bit complicated. But just to get some\nintuition here, so suppose--",
    "start": "3700160",
    "end": "3711710"
  },
  {
    "text": "so this is a heuristic, which\nis not even calculated correct,",
    "start": "3711710",
    "end": "3717530"
  },
  {
    "text": "but it's approximately correct. So suppose optimistically that\nlog C is equal to P instead",
    "start": "3717530",
    "end": "3725930"
  },
  {
    "text": "of P times-- instead of P\ntimes log 3B over epsilon--",
    "start": "3725930",
    "end": "3733059"
  },
  {
    "text": "3B kappa over epsilon tilde. So suppose you just have P.\nYou don't have the log term.",
    "start": "3733060",
    "end": "3738160"
  },
  {
    "text": "Actually, this becomes a\nvery simple calculation. So what you got\nis that you got--",
    "start": "3738160",
    "end": "3744954"
  },
  {
    "start": "3744955",
    "end": "3750910"
  },
  {
    "text": "so basically, if you take the\nlog of this bizarre inequality, you want that-- ",
    "start": "3750910",
    "end": "3759400"
  },
  {
    "text": "let me see. If you take the\nlog, you get log 2, which is not super important.",
    "start": "3759400",
    "end": "3765130"
  },
  {
    "text": "You get log 2 times log C,\nminus 2n epsilon tilde squared. And suppose log C\nis equal to P. Then",
    "start": "3765130",
    "end": "3771930"
  },
  {
    "text": "you've got P minus\n2n epsilon squared. And if you take epsilon--",
    "start": "3771930",
    "end": "3779790"
  },
  {
    "text": "tilde to be square\nroot P over n, then you get this is\nequal to P minus 2p.",
    "start": "3779790",
    "end": "3797060"
  },
  {
    "text": "It's equal to minus P. Which means that 2 C exponential\nminus 2n epsilon squared--",
    "start": "3797060",
    "end": "3805674"
  },
  {
    "text": "if you take the\nexponential back, you get this is less than\nexponential minus P. So this is",
    "start": "3805675",
    "end": "3815000"
  },
  {
    "text": "fundamentally how it works.  But we did make this\nincorrect assumption",
    "start": "3815000",
    "end": "3822770"
  },
  {
    "text": "that log C is equal to P. But\nthis is something not very far, so it's only off\nby a log factor.",
    "start": "3822770",
    "end": "3828150"
  },
  {
    "text": "So if you want to fix\nthis, technically, you need to deal with\nthe log factor. It wouldn't change much,\nbut it would introduce",
    "start": "3828150",
    "end": "3834680"
  },
  {
    "text": "a little bit of complication. So I did have the\ncalculation here.",
    "start": "3834680",
    "end": "3840170"
  },
  {
    "text": "I'm just going to\nbasically write it down, but I don't really expect\nthat you follow all of this.",
    "start": "3840170",
    "end": "3846230"
  },
  {
    "text": "It took me one hour\nto even figure out all the constants,\nso on and so forth. It's not super important.",
    "start": "3846230",
    "end": "3851592"
  },
  {
    "text": "I think the intuition\nis already there. But let me just\nquickly write this just to say what you do formally.",
    "start": "3851592",
    "end": "3858890"
  },
  {
    "text": "So if you suppose\nlog C is equal-- you only have this bound.",
    "start": "3858890",
    "end": "3864110"
  },
  {
    "text": "So this is what we only have.  This is 6B kappa\nover epsilon tilde.",
    "start": "3864110",
    "end": "3872900"
  },
  {
    "text": "And then let's\ntake epsilon tilde to be square root--\nsome constant times",
    "start": "3872900",
    "end": "3881080"
  },
  {
    "text": "P over n, times max. This epsilon tilde is\nactually the epsilon--",
    "start": "3881080",
    "end": "3886240"
  },
  {
    "text": "is the final bound,\nso that's why you're going to see kind of the\nsame thing in your final bound.",
    "start": "3886240",
    "end": "3893200"
  },
  {
    "text": "And C0 is a sufficiently\nlarge constant, which",
    "start": "3893200",
    "end": "3899170"
  },
  {
    "text": "we will choose a bit later. ",
    "start": "3899170",
    "end": "3909470"
  },
  {
    "text": "And you're plug in all\nof this, and you just-- again, you take the-- you look\nat the log of the inequalities",
    "start": "3909470",
    "end": "3915000"
  },
  {
    "text": "we care about-- the\nlog of it is this-- and you plug in\nthis choice of C.",
    "start": "3915000",
    "end": "3921180"
  },
  {
    "text": "You get P log 6B kappa\nover epsilon tilde,",
    "start": "3921180",
    "end": "3928010"
  },
  {
    "text": "minus 2n epsilon tilde squared. And you somehow know that if\nyou don't ignore this log, it's already work. It's just if you have the log,\nyou still have to deal with it.",
    "start": "3928010",
    "end": "3936579"
  },
  {
    "text": "So you get something\nlike P log-- I'm not even sure\nwhether I really",
    "start": "3936580",
    "end": "3942490"
  },
  {
    "text": "have to write down all of this,\nbut just in case some of you",
    "start": "3942490",
    "end": "3947560"
  },
  {
    "text": "want to have this\nhard calculation. ",
    "start": "3947560",
    "end": "3958790"
  },
  {
    "text": "So you get this. And then you-- somehow, I\nguess the first term becomes",
    "start": "3958790",
    "end": "3969570"
  },
  {
    "text": "log 6B kappa. You explain the first term.",
    "start": "3969570",
    "end": "3975690"
  },
  {
    "text": "Square root C0 P. I guess\nI decompose this into--",
    "start": "3975690",
    "end": "3981675"
  },
  {
    "start": "3981675",
    "end": "3991055"
  },
  {
    "text": "I guess I'm decomposing\nthe first term.  6 over C0 P, minus\nC0 P. Log kappa Bn.",
    "start": "3991055",
    "end": "4002920"
  },
  {
    "text": "I guess the way\nthat I always think about this is that when\nyou do the calculation, you always need to\ncheck with what happens",
    "start": "4002920",
    "end": "4009880"
  },
  {
    "text": "if you don't have the log. So what-- if you\ndon't have the log, then this term is large constant\ntimes P, and this term is P,",
    "start": "4009880",
    "end": "4015790"
  },
  {
    "text": "so that's why it's nice. So eventually, you\ncan-- if you take C0 to be something\nlike 32, 36, I",
    "start": "4015790",
    "end": "4022030"
  },
  {
    "text": "think you can show that this\nis bigger than this one. And this one I think-- I guess I-- and this one is\ninactive when P is large.",
    "start": "4022030",
    "end": "4032110"
  },
  {
    "text": "And then you've got this\nis less than minus P. I guess the exact\ncalculation-- there is some more detailed\ncalculation in the notes,",
    "start": "4032110",
    "end": "4038290"
  },
  {
    "text": "but it doesn't matter that much.  So that's what we do.",
    "start": "4038290",
    "end": "4043540"
  },
  {
    "text": "So then basically, this\nis saying that if you take an exponential-- so you get-- after this inequality, you get\n2 C exponential minus 2n epsilon",
    "start": "4043540",
    "end": "4053050"
  },
  {
    "text": "squared is less than 2\ntimes exponential minus P. So this is our\nfailure probability.",
    "start": "4053050",
    "end": "4060440"
  },
  {
    "text": "So basically, with\nthis probability-- so with probability larger than\n1 minus O, e to the minus P--",
    "start": "4060440",
    "end": "4067240"
  },
  {
    "text": "we'll have L hat\ntheta minus L theta is less than 2\nepsilon tilde, which",
    "start": "4067240",
    "end": "4073730"
  },
  {
    "text": "is this thing that we wanted.",
    "start": "4073730",
    "end": "4078740"
  },
  {
    "text": "Let me not just copy it again. ",
    "start": "4078740",
    "end": "4083790"
  },
  {
    "text": "Cool. So that's the proof. And this proof is\na little messy, and this is probably\none of the reasons why.",
    "start": "4083790",
    "end": "4090350"
  },
  {
    "text": "If you open up a classical\nmachine learning book, they typically don't\nshow you this proof.",
    "start": "4090350",
    "end": "4096390"
  },
  {
    "text": "So it's just because it's a\nlittle messy, but actually, it's--",
    "start": "4096390",
    "end": "4102009"
  },
  {
    "text": "the reason why I always\ntry to show this proof is that I feel like\nit's very intuitive,",
    "start": "4102010",
    "end": "4107509"
  },
  {
    "text": "and it demonstrates\nwhat's really going on. And also this kind\nof thing is actually useful for many recent\nnetworks, if you're",
    "start": "4107510",
    "end": "4116370"
  },
  {
    "text": "looking-- if you look at the\ntechnical low level details. So the fancy Radamacher\ncomplexity thing",
    "start": "4116370",
    "end": "4121740"
  },
  {
    "text": "that we are going to talk\nabout next, they are very nice, but sometimes, they\ndon't apply, and you have to really use this. You go back to the most brute\nforce way to think about it.",
    "start": "4121740",
    "end": "4129420"
  },
  {
    "text": " So maybe just a few quick\ncomments about this proof.",
    "start": "4129420",
    "end": "4141120"
  },
  {
    "text": "I guess if you really\nthink about this, this is really saying that you\nhave a generalization error.",
    "start": "4141120",
    "end": "4147000"
  },
  {
    "text": " So it's less than the log\nof this excess risk up",
    "start": "4147000",
    "end": "4157630"
  },
  {
    "text": "to constant factor, of\ncourse, plus epsilon to the k.",
    "start": "4157630",
    "end": "4162969"
  },
  {
    "text": "So this part is from the\nfinite hypothesis case, and this one comes from the--",
    "start": "4162970",
    "end": "4169630"
  },
  {
    "text": "so this is not k, this is kappa. ",
    "start": "4169630",
    "end": "4174799"
  },
  {
    "text": "So this is the\ndiscretization error,",
    "start": "4174800",
    "end": "4180068"
  },
  {
    "text": "and this is the finite\nhypothesis case. ",
    "start": "4180069",
    "end": "4187630"
  },
  {
    "text": "And in some sense, you are\njust trading off these two. And what that means by\ntrading off, these two--",
    "start": "4187630",
    "end": "4193500"
  },
  {
    "text": "it really means that-- what epsilon you choose. So this one depends on epsilon.",
    "start": "4193500",
    "end": "4198795"
  },
  {
    "start": "4198795",
    "end": "4205800"
  },
  {
    "text": "So this one depends\non epsilon, but it depends on epsilon\nin a very weak way because it depends\non epsilon in a--",
    "start": "4205800",
    "end": "4212250"
  },
  {
    "text": "by logarithmic. So that makes it very\neasy to trade off this, because you\ncan pick epsilon to be quite small so that\nthis term becomes small,",
    "start": "4212250",
    "end": "4219989"
  },
  {
    "text": "and this term-- this depends on--\nsorry, I think, technically, this should\ndepend on log 1 over epsilon.",
    "start": "4219990",
    "end": "4227360"
  },
  {
    "text": "So the smaller the epsilon is,\nthe better the second term, but the worse the first term. But the first term increases as\nepsilon goes to 0 very slowly.",
    "start": "4227360",
    "end": "4235930"
  },
  {
    "text": "So that's why you pretty much\ncan ignore the second term in some sense, just\nbecause you take epsilon to be very small so\nthat the second term becomes",
    "start": "4235930",
    "end": "4242560"
  },
  {
    "text": "negligible. And even for those\nsmall epsilon, the first term is still\nreasonably bounded.",
    "start": "4242560",
    "end": "4249160"
  },
  {
    "text": "And that's why you can make\nthis trade off really nice. But in some other cases,\nas we'll see later,",
    "start": "4249160",
    "end": "4257520"
  },
  {
    "text": "we will do the discretization--\nthe first term wouldn't be as nice as this. It wouldn't be log\n1 over epsilon. It would be something that\ngoes to infinity as epsilon",
    "start": "4257520",
    "end": "4265630"
  },
  {
    "text": "goes to 0 in a faster rate. So it probably--\nin the later case-- later-- sometimes,\nthis first term",
    "start": "4265630",
    "end": "4272680"
  },
  {
    "text": "will be log over\nepsilon squared. Then, the trade off will become\na little bit more tricky, and you have to be more\ncareful about the trade off.",
    "start": "4272680",
    "end": "4283739"
  },
  {
    "text": "And finally, just to-- for-- this is probably\nan overview for--",
    "start": "4283740",
    "end": "4290639"
  },
  {
    "text": " this is from a somewhat\nbird's eye view.",
    "start": "4290640",
    "end": "4296130"
  },
  {
    "text": "So log H of P in this case. ",
    "start": "4296130",
    "end": "4302320"
  },
  {
    "text": "Or the P, you can think of\nthis as complexity measures. I guess I've mentioned\nthis as well.",
    "start": "4302320",
    "end": "4308679"
  },
  {
    "text": "So these are complexity measures\nof the hypothesis class. ",
    "start": "4308680",
    "end": "4316540"
  },
  {
    "text": "And the general\nphenomenon-- it's always like you have a\nbigger H. It means",
    "start": "4316540",
    "end": "4323980"
  },
  {
    "text": "that you need more samples.  This is always-- you\nhave worse bound,",
    "start": "4323980",
    "end": "4330400"
  },
  {
    "text": "which means you need\nmore samples to learn. And in some sense, the\nnext one or two weeks,",
    "start": "4330400",
    "end": "4337480"
  },
  {
    "text": "we are talking about\na more accurate--",
    "start": "4337480",
    "end": "4343567"
  },
  {
    "text": "I guess accurate may\nnot be the right word-- a more fine-grained hypothesis--",
    "start": "4343567",
    "end": "4349750"
  },
  {
    "text": "more fine-grained\ncomplexity measure.",
    "start": "4349750",
    "end": "4355150"
  },
  {
    "text": "So what is the right\ncomplexity measure? There is no really super\ndecisive answer what's",
    "start": "4355150",
    "end": "4360932"
  },
  {
    "text": "the right complexity measure. In some sense, it's up\nto the theorem prover.",
    "start": "4360932",
    "end": "4368620"
  },
  {
    "text": "But we're going to have a\nmore fine-grained and more, in some sense,\nfundamental complexity",
    "start": "4368620",
    "end": "4373630"
  },
  {
    "text": "measure in the\nnext two lectures, which is called\nRadamacher complexity. And you can use that to derive\nmany of these bounds in more",
    "start": "4373630",
    "end": "4381640"
  },
  {
    "text": "principled ways. And in general, I think one of\nthe important questions for--",
    "start": "4381640",
    "end": "4387310"
  },
  {
    "text": "especially in somewhat classical\nstatistical machine learning-- is to find out what's the\nright complexity measure",
    "start": "4387310",
    "end": "4393010"
  },
  {
    "text": "for your hypothesis class. So we're going to discuss\nwhat does it really mean by right and wrong.",
    "start": "4393010",
    "end": "4398449"
  },
  {
    "text": "There's no unique answer,\nbut there are some kind of--",
    "start": "4398450",
    "end": "4403630"
  },
  {
    "text": "but this is the\ncentral question. So you need a\ncomplexity measure that really captures the fundamental\ncomplexity of this class.",
    "start": "4403630",
    "end": "4412750"
  },
  {
    "text": "For example, if you\nhave infinite class, you shouldn't use log H. Log H is not really the\nfundamental complexity measure",
    "start": "4412750",
    "end": "4419540"
  },
  {
    "text": "for infinite hypothesis class. You probably should\nuse dimensionality. Probably in the\nlater course, we are going to see if you can use\nthe norm of your parameters",
    "start": "4419540",
    "end": "4427960"
  },
  {
    "text": "as the complexity measure. And it does depend on\nthe specific cases.",
    "start": "4427960",
    "end": "4433960"
  },
  {
    "text": "And also sometimes,\nit depends on data. So this will be what we\ndiscuss in the next few weeks.",
    "start": "4433960",
    "end": "4441640"
  },
  {
    "text": "I think this is a\nnatural place to stop. Yeah. I think that's all for today.",
    "start": "4441640",
    "end": "4448949"
  },
  {
    "start": "4448950",
    "end": "4453000"
  }
]