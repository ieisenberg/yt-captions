[
  {
    "start": "0",
    "end": "4374"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4374",
    "end": "6220"
  },
  {
    "text": "This is part 9 in our series\non contextual representation.",
    "start": "6220",
    "end": "9450"
  },
  {
    "text": "For part 9, we're going to\nswitch gears a little bit",
    "start": "9450",
    "end": "12090"
  },
  {
    "text": "and talk about distillation.",
    "start": "12090",
    "end": "13710"
  },
  {
    "text": "The name of the game here\nis going to be efficiency.",
    "start": "13710",
    "end": "16379"
  },
  {
    "text": "We are seeking models that\nare smaller and, therefore,",
    "start": "16379",
    "end": "20310"
  },
  {
    "text": "more efficient to use,\nespecially at inference time,",
    "start": "20310",
    "end": "23220"
  },
  {
    "text": "but nonetheless very performant.",
    "start": "23220",
    "end": "25740"
  },
  {
    "text": "And distillation is a set of\ntechniques for achieving that.",
    "start": "25740",
    "end": "30060"
  },
  {
    "text": "On the first day\nof the course, I",
    "start": "30060",
    "end": "32159"
  },
  {
    "text": "had this slide that tracked\nmodel size over time",
    "start": "32159",
    "end": "35160"
  },
  {
    "text": "for our large language models.",
    "start": "35160",
    "end": "36900"
  },
  {
    "text": "And you saw it going up\nand up and up all the way",
    "start": "36900",
    "end": "39300"
  },
  {
    "text": "to Palm at like 540\nbillion parameters.",
    "start": "39300",
    "end": "43000"
  },
  {
    "text": "And then I offered a\nhopeful perspective",
    "start": "43000",
    "end": "44970"
  },
  {
    "text": "that models would\nstart getting smaller.",
    "start": "44970",
    "end": "47160"
  },
  {
    "text": "And one perspective on why\nmodels might get smaller",
    "start": "47160",
    "end": "50940"
  },
  {
    "text": "is that we can distill the\nessence of these really",
    "start": "50940",
    "end": "54090"
  },
  {
    "text": "large models down\ninto the small ones",
    "start": "54090",
    "end": "56382"
  },
  {
    "text": "and, therefore,\nget models that are",
    "start": "56382",
    "end": "57840"
  },
  {
    "text": "more efficient when deployed.",
    "start": "57840",
    "end": "61440"
  },
  {
    "text": "The name of the game\nfor distillation",
    "start": "61440",
    "end": "63840"
  },
  {
    "text": "is that we have a teacher\nmodel that is presumably",
    "start": "63840",
    "end": "67690"
  },
  {
    "text": "very good but also very large\nand, therefore, very expensive",
    "start": "67690",
    "end": "71830"
  },
  {
    "text": "to use.",
    "start": "71830",
    "end": "72460"
  },
  {
    "text": "And the goal is to train\na student model that",
    "start": "72460",
    "end": "75760"
  },
  {
    "text": "has similar input/output\nbehavior to the teacher",
    "start": "75760",
    "end": "80140"
  },
  {
    "text": "but is nonetheless much\nmore efficient to use.",
    "start": "80140",
    "end": "83150"
  },
  {
    "text": "And we can do that in\nvery lightweight ways",
    "start": "83150",
    "end": "85570"
  },
  {
    "text": "that simply depend on\nhaving the student mimic",
    "start": "85570",
    "end": "88030"
  },
  {
    "text": "the teacher in terms of its\nbasic input/output behavior.",
    "start": "88030",
    "end": "91960"
  },
  {
    "text": "But we can also think about\ngoing deeper and having",
    "start": "91960",
    "end": "95260"
  },
  {
    "text": "it be the case that we\ntrain the student to have",
    "start": "95260",
    "end": "97540"
  },
  {
    "text": "internal\nrepresentations that are",
    "start": "97540",
    "end": "99820"
  },
  {
    "text": "similar in some sense\nto those of the teacher,",
    "start": "99820",
    "end": "102159"
  },
  {
    "text": "to gain an even deeper\ndistillation of that teacher.",
    "start": "102160",
    "end": "107270"
  },
  {
    "text": "So in that context, let's review\nsome distillation objectives.",
    "start": "107270",
    "end": "110649"
  },
  {
    "text": "And what I've done here is\nlist them out from least",
    "start": "110650",
    "end": "113470"
  },
  {
    "text": "to most heavy duty.",
    "start": "113470",
    "end": "115060"
  },
  {
    "text": "And, of course,\nyou'll commonly see",
    "start": "115060",
    "end": "117159"
  },
  {
    "text": "that people take weighted\naverages of different elements",
    "start": "117160",
    "end": "119830"
  },
  {
    "text": "of this list.",
    "start": "119830",
    "end": "121570"
  },
  {
    "text": "For item 0 on the\nlist, I just mentioned",
    "start": "121570",
    "end": "124810"
  },
  {
    "text": "that you will probably\ndistill your student by,",
    "start": "124810",
    "end": "127470"
  },
  {
    "text": "in part, training it on\ngold data for the task",
    "start": "127470",
    "end": "130139"
  },
  {
    "text": "if you have it available\nand can make use of it.",
    "start": "130139",
    "end": "132340"
  },
  {
    "text": "And so we're talking\nessentially about supplementing",
    "start": "132340",
    "end": "135750"
  },
  {
    "text": "that core training with\nadditional components",
    "start": "135750",
    "end": "138630"
  },
  {
    "text": "of the objective.",
    "start": "138630",
    "end": "140980"
  },
  {
    "text": "So the first\ndistillation objective",
    "start": "140980",
    "end": "143069"
  },
  {
    "text": "and the most lightweight\none is that we simply",
    "start": "143070",
    "end": "145920"
  },
  {
    "text": "train the student to produce\nthe same output as the teacher.",
    "start": "145920",
    "end": "149910"
  },
  {
    "text": "And this is very lightweight.",
    "start": "149910",
    "end": "151200"
  },
  {
    "text": "Because at distillation\ntime, we actually",
    "start": "151200",
    "end": "153239"
  },
  {
    "text": "don't require any direct\naccess to the teacher.",
    "start": "153240",
    "end": "156060"
  },
  {
    "text": "We simply run the teacher on\nall our available training data.",
    "start": "156060",
    "end": "159540"
  },
  {
    "text": "It produces labels.",
    "start": "159540",
    "end": "160950"
  },
  {
    "text": "And then we train the\nstudent on those labels.",
    "start": "160950",
    "end": "164069"
  },
  {
    "text": "It's a bit mysterious\nwhy that is useful.",
    "start": "164070",
    "end": "167320"
  },
  {
    "text": "I think the guiding\ninsight here is",
    "start": "167320",
    "end": "169740"
  },
  {
    "text": "that there might be aspects\nof your training data",
    "start": "169740",
    "end": "172050"
  },
  {
    "text": "that are noisy or just\nvery difficult to learn.",
    "start": "172050",
    "end": "175540"
  },
  {
    "text": "So the teacher acts as\na kind of regularizer.",
    "start": "175540",
    "end": "178650"
  },
  {
    "text": "And the student benefits from\nseeing the teacher's output,",
    "start": "178650",
    "end": "181950"
  },
  {
    "text": "even if it contains some\nmistakes because that",
    "start": "181950",
    "end": "184890"
  },
  {
    "text": "ultimately helps\nwith generalization.",
    "start": "184890",
    "end": "188740"
  },
  {
    "text": "Going one layer deeper,\nwe could train the student",
    "start": "188740",
    "end": "191770"
  },
  {
    "text": "to have similar output\nbehavior as the teacher",
    "start": "191770",
    "end": "194500"
  },
  {
    "text": "at the level of the full\nvector of output scores.",
    "start": "194500",
    "end": "197663"
  },
  {
    "text": "This is, in fact, the\ncenterpiece of one",
    "start": "197663",
    "end": "199330"
  },
  {
    "text": "of the most famous distillation\npapers, Hinton et al, 2015.",
    "start": "199330",
    "end": "203500"
  },
  {
    "text": "It's a little bit\nmore heavy duty",
    "start": "203500",
    "end": "205270"
  },
  {
    "text": "than just the output labels\nbecause we do require",
    "start": "205270",
    "end": "208060"
  },
  {
    "text": "those entire score vectors.",
    "start": "208060",
    "end": "209830"
  },
  {
    "text": "But it's still a purely\nbehavioral distillation",
    "start": "209830",
    "end": "213220"
  },
  {
    "text": "objective.",
    "start": "213220",
    "end": "215210"
  },
  {
    "text": "Going one layer deeper\nin the famous DistilBERT",
    "start": "215210",
    "end": "217940"
  },
  {
    "text": "paper, Sanh et al,\n2019, in addition to",
    "start": "217940",
    "end": "221630"
  },
  {
    "text": "having components\nthat are like 1 and 2,",
    "start": "221630",
    "end": "224120"
  },
  {
    "text": "their distillation\nobjective also",
    "start": "224120",
    "end": "226370"
  },
  {
    "text": "has a cosine loss component.",
    "start": "226370",
    "end": "228470"
  },
  {
    "text": "Here, what we're trying to do\nis have the teacher and student",
    "start": "228470",
    "end": "231470"
  },
  {
    "text": "output states in a\ntransformer sense",
    "start": "231470",
    "end": "233870"
  },
  {
    "text": "be very similar to each other.",
    "start": "233870",
    "end": "235970"
  },
  {
    "text": "This requires much more access\nto the teacher at distillation",
    "start": "235970",
    "end": "240140"
  },
  {
    "text": "time because we\nneed to do forward",
    "start": "240140",
    "end": "242240"
  },
  {
    "text": "inference on the\nteacher for each example",
    "start": "242240",
    "end": "244310"
  },
  {
    "text": "that we train the student on\nto get those output states",
    "start": "244310",
    "end": "247760"
  },
  {
    "text": "and then apply the cosine\nloss and update the student.",
    "start": "247760",
    "end": "253340"
  },
  {
    "text": "You could also think about\ntying other teacher and student",
    "start": "253340",
    "end": "256220"
  },
  {
    "text": "states-- other hidden states\nand maybe, most prominently,",
    "start": "256220",
    "end": "259040"
  },
  {
    "text": "the embedding layers for the\nteacher and student model,",
    "start": "259040",
    "end": "261620"
  },
  {
    "text": "again, with an intuition that\nthe models will be more alike",
    "start": "261620",
    "end": "264889"
  },
  {
    "text": "and the student,\ntherefore, more powerful",
    "start": "264890",
    "end": "266600"
  },
  {
    "text": "if its internal representations\nmimic those of the teacher.",
    "start": "266600",
    "end": "270860"
  },
  {
    "text": "And then maybe even\nmore heavy duty--",
    "start": "270860",
    "end": "272930"
  },
  {
    "text": "this is work that I was\ninvolved with-- we now",
    "start": "272930",
    "end": "275210"
  },
  {
    "text": "train the student to mimic\nthe counterfactual behavior",
    "start": "275210",
    "end": "278300"
  },
  {
    "text": "of the teacher\nunder interventions.",
    "start": "278300",
    "end": "280729"
  },
  {
    "text": "That is, instances\nin which we actually",
    "start": "280730",
    "end": "282800"
  },
  {
    "text": "change the internal\nstate of the teacher",
    "start": "282800",
    "end": "285259"
  },
  {
    "text": "and do the same corresponding\nthing to the student",
    "start": "285260",
    "end": "288470"
  },
  {
    "text": "and ensure that the two have\nmatching input/output behavior.",
    "start": "288470",
    "end": "291770"
  },
  {
    "text": "And that's a more\nthorough exploration",
    "start": "291770",
    "end": "294110"
  },
  {
    "text": "of the input/output\nbehavior, putting the model",
    "start": "294110",
    "end": "296479"
  },
  {
    "text": "into counterfactual\nstates with the hope",
    "start": "296480",
    "end": "298850"
  },
  {
    "text": "that it will lead the models\nto have very similar causal",
    "start": "298850",
    "end": "301940"
  },
  {
    "text": "internal structure.",
    "start": "301940",
    "end": "303930"
  },
  {
    "text": "For 3, 4, and 5, this is\nvery heavy duty in the sense",
    "start": "303930",
    "end": "307710"
  },
  {
    "text": "that we do require full access\nto the teacher at distillation",
    "start": "307710",
    "end": "310940"
  },
  {
    "text": "time.",
    "start": "310940",
    "end": "311440"
  },
  {
    "text": "But in all of these cases,\nI'm presuming that the teacher",
    "start": "311440",
    "end": "314370"
  },
  {
    "text": "is a frozen artifact.",
    "start": "314370",
    "end": "315960"
  },
  {
    "text": "And all you have to do\nis forward inference.",
    "start": "315960",
    "end": "319699"
  },
  {
    "text": "There's another dimension to\nthese distillation objectives",
    "start": "319700",
    "end": "322910"
  },
  {
    "text": "that is worth thinking about.",
    "start": "322910",
    "end": "324140"
  },
  {
    "text": "And, again, these can be\ncombined with each other",
    "start": "324140",
    "end": "327050"
  },
  {
    "text": "and with the different\nmodes that I just described.",
    "start": "327050",
    "end": "330199"
  },
  {
    "text": "In standard\ndistillation, the teacher",
    "start": "330200",
    "end": "332930"
  },
  {
    "text": "is frozen, as I said before.",
    "start": "332930",
    "end": "334820"
  },
  {
    "text": "And only the student\nparameters are updated.",
    "start": "334820",
    "end": "338510"
  },
  {
    "text": "We could also think about\nmulti-teacher distillation.",
    "start": "338510",
    "end": "341240"
  },
  {
    "text": "In this case, we have\nmultiple teachers maybe",
    "start": "341240",
    "end": "343400"
  },
  {
    "text": "with different capabilities.",
    "start": "343400",
    "end": "344630"
  },
  {
    "text": "And we simultaneously try\nto distill them all down",
    "start": "344630",
    "end": "347780"
  },
  {
    "text": "into a single student\nthat can presumably",
    "start": "347780",
    "end": "350210"
  },
  {
    "text": "perform multiple tasks\ncoming from those teachers.",
    "start": "350210",
    "end": "354530"
  },
  {
    "text": "Co-distillation is really\ninterestingly different",
    "start": "354530",
    "end": "357170"
  },
  {
    "text": "to think about.",
    "start": "357170",
    "end": "357860"
  },
  {
    "text": "In this case, the\nstudent and the teacher",
    "start": "357860",
    "end": "360319"
  },
  {
    "text": "are trained jointly.",
    "start": "360320",
    "end": "361760"
  },
  {
    "text": "This is sometimes also\ncalled online distillation.",
    "start": "361760",
    "end": "364700"
  },
  {
    "text": "And so this is very\nheavy duty in the sense",
    "start": "364700",
    "end": "366710"
  },
  {
    "text": "that you're training both of\nthese artifacts simultaneously.",
    "start": "366710",
    "end": "370310"
  },
  {
    "text": "And it's hard to think about.",
    "start": "370310",
    "end": "372620"
  },
  {
    "text": "Self-distillation is even\nharder to think about.",
    "start": "372620",
    "end": "375199"
  },
  {
    "text": "In this case, the\ndistillation objective",
    "start": "375200",
    "end": "377270"
  },
  {
    "text": "includes terms that seek to\nmake some model components",
    "start": "377270",
    "end": "381569"
  },
  {
    "text": "align with others\nfrom the same model.",
    "start": "381570",
    "end": "384105"
  },
  {
    "start": "384105",
    "end": "386870"
  },
  {
    "text": "In terms of performance,\nthis is the name of the game.",
    "start": "386870",
    "end": "389810"
  },
  {
    "text": "As I said before, we\nare seeking artifacts",
    "start": "389810",
    "end": "392030"
  },
  {
    "text": "that are more efficient but\nnonetheless still performant.",
    "start": "392030",
    "end": "394940"
  },
  {
    "text": "And so I thought I would\nwrap up this short screencast",
    "start": "394940",
    "end": "397370"
  },
  {
    "text": "by just summarizing what we\nknow for the specific case",
    "start": "397370",
    "end": "401000"
  },
  {
    "text": "of natural language\nunderstanding focused on GLUE.",
    "start": "401000",
    "end": "404210"
  },
  {
    "text": "Based on the\nevidence, I think it's",
    "start": "404210",
    "end": "405860"
  },
  {
    "text": "fair to say that we can\ndistill BERT models down",
    "start": "405860",
    "end": "409490"
  },
  {
    "text": "into much smaller models that\nare still highly performant.",
    "start": "409490",
    "end": "412940"
  },
  {
    "text": "A lot of this research has\nused the GLUE benchmark",
    "start": "412940",
    "end": "415460"
  },
  {
    "text": "to track this.",
    "start": "415460",
    "end": "416960"
  },
  {
    "text": "And it's all converging\non the same insight.",
    "start": "416960",
    "end": "419300"
  },
  {
    "text": "In the famous DistilBERT\npaper, they took BERT base",
    "start": "419300",
    "end": "422659"
  },
  {
    "text": "and distilled it\ndown into six layers",
    "start": "422660",
    "end": "424910"
  },
  {
    "text": "with 97% of the GLUE\nperformance retained.",
    "start": "424910",
    "end": "428870"
  },
  {
    "text": "Sun et al did something similar.",
    "start": "428870",
    "end": "430940"
  },
  {
    "text": "They tried BERT-base done\ninto three layer and six layer",
    "start": "430940",
    "end": "434330"
  },
  {
    "text": "and also saw that\nthey could main",
    "start": "434330",
    "end": "436610"
  },
  {
    "text": "maintain outstanding\nperformance on GLUE.",
    "start": "436610",
    "end": "438919"
  },
  {
    "text": "Similarly, Jiao et al,\n2020, distilled BERT base",
    "start": "438920",
    "end": "442220"
  },
  {
    "text": "into four layers and, again,\nsaw very strong results on GLUE.",
    "start": "442220",
    "end": "446700"
  },
  {
    "text": "This set of results\nhere is noteworthy",
    "start": "446700",
    "end": "448980"
  },
  {
    "text": "because it's converging\non the same lesson.",
    "start": "448980",
    "end": "451170"
  },
  {
    "text": "We can make BERT much\nsmaller by distilling down",
    "start": "451170",
    "end": "454230"
  },
  {
    "text": "into a much smaller student\nthat still does well",
    "start": "454230",
    "end": "457410"
  },
  {
    "text": "on benchmarks like GLUE.",
    "start": "457410",
    "end": "459000"
  },
  {
    "text": "And so that should\nbe inspiring in terms",
    "start": "459000",
    "end": "460770"
  },
  {
    "text": "of thinking about distillation\nas a powerful tool in your tool",
    "start": "460770",
    "end": "464595"
  },
  {
    "text": "kit for taking very large\nand maybe expensive teachers",
    "start": "464595",
    "end": "468210"
  },
  {
    "text": "and turning them\ninto things that",
    "start": "468210",
    "end": "469770"
  },
  {
    "text": "might have more practical\nutility out in the world.",
    "start": "469770",
    "end": "474410"
  },
  {
    "start": "474410",
    "end": "479000"
  }
]