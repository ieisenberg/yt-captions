[
  {
    "text": "SPEAKER 1: Welcome\nback, everyone.",
    "start": "4653",
    "end": "6069"
  },
  {
    "text": "This is part 2 in our series\non information retrieval.",
    "start": "6070",
    "end": "8730"
  },
  {
    "text": "We're going to briefly review\nclassical IR approaches.",
    "start": "8730",
    "end": "11639"
  },
  {
    "text": "It will be a brief\noverview because our focus",
    "start": "11640",
    "end": "13860"
  },
  {
    "text": "in this course is on neural\ninformation retrieval,",
    "start": "13860",
    "end": "16049"
  },
  {
    "text": "but I did want to cover these\nclassical ideas because they're",
    "start": "16050",
    "end": "19199"
  },
  {
    "text": "very powerful.",
    "start": "19200",
    "end": "20250"
  },
  {
    "text": "And classical IR\nsystems could very well",
    "start": "20250",
    "end": "22740"
  },
  {
    "text": "be important components in\nmodels that you develop.",
    "start": "22740",
    "end": "26220"
  },
  {
    "text": "The standard starting\npoint for classical IR",
    "start": "26220",
    "end": "28800"
  },
  {
    "text": "is the term-document matrix.",
    "start": "28800",
    "end": "30509"
  },
  {
    "text": "I've got a fragment of\na real-term document",
    "start": "30510",
    "end": "32669"
  },
  {
    "text": "matrix on the slide here.",
    "start": "32670",
    "end": "34199"
  },
  {
    "text": "The terms are along\nthe rows, the documents",
    "start": "34200",
    "end": "37050"
  },
  {
    "text": "go along the columns,\nand the cells",
    "start": "37050",
    "end": "39270"
  },
  {
    "text": "record the number of times\nthat each word appeared",
    "start": "39270",
    "end": "41610"
  },
  {
    "text": "in each one of these documents.",
    "start": "41610",
    "end": "43320"
  },
  {
    "text": "These are standardly very\nlarge, very sparse matrices,",
    "start": "43320",
    "end": "47550"
  },
  {
    "text": "but they encode latently\na lot of information",
    "start": "47550",
    "end": "50460"
  },
  {
    "text": "about which documents are\nrelevant to which query terms.",
    "start": "50460",
    "end": "54960"
  },
  {
    "text": "TF-IDF approach.",
    "start": "54960",
    "end": "56820"
  },
  {
    "text": "TF-IDF is a common\napproach to massaging",
    "start": "56820",
    "end": "59400"
  },
  {
    "text": "those term document\nvalues to get",
    "start": "59400",
    "end": "61800"
  },
  {
    "text": "more information about\nrelevance from the matrix.",
    "start": "61800",
    "end": "64720"
  },
  {
    "text": "Here's how TF-IDF works.",
    "start": "64720",
    "end": "66190"
  },
  {
    "text": "We begin from a\ncorpus of documents",
    "start": "66190",
    "end": "68080"
  },
  {
    "text": "D. Term frequency is\nactually internal to each one",
    "start": "68080",
    "end": "71890"
  },
  {
    "text": "of these documents.",
    "start": "71890",
    "end": "73030"
  },
  {
    "text": "TF of a word given a document\nis simply the number of times",
    "start": "73030",
    "end": "76570"
  },
  {
    "text": "that word appears in\nthe document divided",
    "start": "76570",
    "end": "79210"
  },
  {
    "text": "by the total length\nof the document.",
    "start": "79210",
    "end": "81470"
  },
  {
    "text": "So a standard relative\nfrequency value.",
    "start": "81470",
    "end": "84550"
  },
  {
    "text": "Document frequency is a function\nof words in our entire corpus.",
    "start": "84550",
    "end": "89080"
  },
  {
    "text": "And we're simply\ncounting the number",
    "start": "89080",
    "end": "90820"
  },
  {
    "text": "of documents that contain\nthe target word regardless",
    "start": "90820",
    "end": "93312"
  },
  {
    "text": "of how frequent the word is in\neach one of those documents.",
    "start": "93312",
    "end": "95770"
  },
  {
    "text": "Simple occurrence.",
    "start": "95770",
    "end": "96640"
  },
  {
    "text": "The number of documents that\ncontains the target word.",
    "start": "96640",
    "end": "100060"
  },
  {
    "text": "And then, inverse\ndocument frequency",
    "start": "100060",
    "end": "102220"
  },
  {
    "text": "is just the log\nof the total size",
    "start": "102220",
    "end": "104440"
  },
  {
    "text": "of our corpus divided by\nthe document frequency value",
    "start": "104440",
    "end": "107890"
  },
  {
    "text": "that we calculated.",
    "start": "107890",
    "end": "109479"
  },
  {
    "text": "And then TF-IDF is\nsimply the product",
    "start": "109480",
    "end": "111850"
  },
  {
    "text": "of the TF and the IDF values.",
    "start": "111850",
    "end": "115189"
  },
  {
    "text": "Here's a little work example.",
    "start": "115190",
    "end": "117220"
  },
  {
    "text": "I have a term-document matrix\non the slide in the left here.",
    "start": "117220",
    "end": "121570"
  },
  {
    "text": "We calculate the IDF values.",
    "start": "121570",
    "end": "123430"
  },
  {
    "text": "Those are given on the right.",
    "start": "123430",
    "end": "124930"
  },
  {
    "text": "And then, the term\nfrequency values",
    "start": "124930",
    "end": "126640"
  },
  {
    "text": "are given at the\nbottom of the slide.",
    "start": "126640",
    "end": "128619"
  },
  {
    "text": "And then, we get the product\nof those for the TF-IDF",
    "start": "128620",
    "end": "131409"
  },
  {
    "text": "values down in the\nlower right here.",
    "start": "131410",
    "end": "133390"
  },
  {
    "text": "And I think you can start to\nsee some noteworthy patterns.",
    "start": "133390",
    "end": "136120"
  },
  {
    "text": "For example, the term C is\nin relatively few documents.",
    "start": "136120",
    "end": "141220"
  },
  {
    "text": "Just two of them.",
    "start": "141220",
    "end": "142240"
  },
  {
    "text": "And it's relatively frequent\nin both of those documents.",
    "start": "142240",
    "end": "145730"
  },
  {
    "text": "And as a result, it\nhas high TF-IDF values.",
    "start": "145730",
    "end": "149090"
  },
  {
    "text": "We could also look\nat term D here.",
    "start": "149090",
    "end": "151280"
  },
  {
    "text": "It occurs in only one document\nand is relatively infrequent",
    "start": "151280",
    "end": "155360"
  },
  {
    "text": "in that document.",
    "start": "155360",
    "end": "156590"
  },
  {
    "text": "As a result of occurring\nin only one document,",
    "start": "156590",
    "end": "159080"
  },
  {
    "text": "it ends up with a\npretty high TF-IDF value",
    "start": "159080",
    "end": "161630"
  },
  {
    "text": "because its IDF value is so\nhigh even though its term",
    "start": "161630",
    "end": "165230"
  },
  {
    "text": "frequency is low.",
    "start": "165230",
    "end": "166280"
  },
  {
    "text": "And correspondingly,\ndoc word a-- term",
    "start": "166280",
    "end": "169880"
  },
  {
    "text": "a here gets a TF-IDF value of 0.",
    "start": "169880",
    "end": "172880"
  },
  {
    "text": "It was highly frequent\nin document 4,",
    "start": "172880",
    "end": "176060"
  },
  {
    "text": "but it occurs in\nall of the documents",
    "start": "176060",
    "end": "178220"
  },
  {
    "text": "and therefore ends up\nwith an IDF value of 0.",
    "start": "178220",
    "end": "181250"
  },
  {
    "text": "And therefore TF-IDF value of 0.",
    "start": "181250",
    "end": "183680"
  },
  {
    "text": "So it gives you a sense for\nhow these values combine",
    "start": "183680",
    "end": "187310"
  },
  {
    "text": "to give us TF-IDF.",
    "start": "187310",
    "end": "189020"
  },
  {
    "text": "Let's actually break down the\nscoring in a little bit more",
    "start": "189020",
    "end": "191780"
  },
  {
    "text": "of a systematic way starting\nwith the IDF values.",
    "start": "191780",
    "end": "195020"
  },
  {
    "text": "For IDF, we do have a\nlittle bit of a problem.",
    "start": "195020",
    "end": "197820"
  },
  {
    "text": "If we have a word that\noccurs in no documents,",
    "start": "197820",
    "end": "200690"
  },
  {
    "text": "then the IDF value is undefined\nbecause we need to divide by 0.",
    "start": "200690",
    "end": "205220"
  },
  {
    "text": "So what I've done here is simply\nstipulate that that's a 0.",
    "start": "205220",
    "end": "209280"
  },
  {
    "text": "If a word appears in\njust one document,",
    "start": "209280",
    "end": "212160"
  },
  {
    "text": "it gets a maximal IDF value.",
    "start": "212160",
    "end": "215160"
  },
  {
    "text": "And the IDF values\ndrop off steadily",
    "start": "215160",
    "end": "217770"
  },
  {
    "text": "as the word appears in more and\nmore documents all the way up",
    "start": "217770",
    "end": "220980"
  },
  {
    "text": "to appearing in every document\ngiven the little corpus of 10",
    "start": "220980",
    "end": "224040"
  },
  {
    "text": "documents that were imagining.",
    "start": "224040",
    "end": "225599"
  },
  {
    "text": "And that, too, is\nan IDF value of 0,",
    "start": "225600",
    "end": "227940"
  },
  {
    "text": "as we saw on the previous slide.",
    "start": "227940",
    "end": "230190"
  },
  {
    "text": "And the idea here\nis that by the time",
    "start": "230190",
    "end": "231780"
  },
  {
    "text": "you have a word that\nappears in every document,",
    "start": "231780",
    "end": "234150"
  },
  {
    "text": "it's simply not\ninformative about which",
    "start": "234150",
    "end": "236430"
  },
  {
    "text": "documents are relevant,\nand so its IDF",
    "start": "236430",
    "end": "238439"
  },
  {
    "text": "value should be minimized.",
    "start": "238440",
    "end": "241530"
  },
  {
    "text": "And here's a slide showing\nselected TF-IDF values.",
    "start": "241530",
    "end": "244510"
  },
  {
    "text": "And I think the\npattern is very clear.",
    "start": "244510",
    "end": "246150"
  },
  {
    "text": "TF-IDF reaches\nits maximal values",
    "start": "246150",
    "end": "249000"
  },
  {
    "text": "for terms that occur\nvery frequently",
    "start": "249000",
    "end": "252210"
  },
  {
    "text": "in very few documents.",
    "start": "252210",
    "end": "253830"
  },
  {
    "text": "And correspondingly,\nTF-IDF values",
    "start": "253830",
    "end": "256109"
  },
  {
    "text": "are at their lowest\nfor words that",
    "start": "256110",
    "end": "258419"
  },
  {
    "text": "are very infrequent in\nvery many documents.",
    "start": "258420",
    "end": "261959"
  },
  {
    "text": "Those are the tiny\nbubbles up here.",
    "start": "261959",
    "end": "263610"
  },
  {
    "text": "That's the kind of core\nbehavior of TF-IDF.",
    "start": "263610",
    "end": "266699"
  },
  {
    "text": "What we're really\nlooking for is words",
    "start": "266700",
    "end": "269010"
  },
  {
    "text": "that are truly\ndistinguishing indicators",
    "start": "269010",
    "end": "272040"
  },
  {
    "text": "of particular documents.",
    "start": "272040",
    "end": "275400"
  },
  {
    "text": "To calculate relevant scores\nfor a given query which",
    "start": "275400",
    "end": "278310"
  },
  {
    "text": "might contain multiple\nterms, the standard approach",
    "start": "278310",
    "end": "281190"
  },
  {
    "text": "is simply to sum over\nwhatever weighting",
    "start": "281190",
    "end": "283620"
  },
  {
    "text": "we're using for the\nterm-document matrix.",
    "start": "283620",
    "end": "286720"
  },
  {
    "text": "So, for example, if\nweight here is TF-IDF,",
    "start": "286720",
    "end": "289170"
  },
  {
    "text": "we simply sum over\nthe TF-IDF values",
    "start": "289170",
    "end": "291600"
  },
  {
    "text": "for every word in our query, and\nthat gives us a relevant score",
    "start": "291600",
    "end": "295230"
  },
  {
    "text": "for the entire user query.",
    "start": "295230",
    "end": "297150"
  },
  {
    "text": "BM25 is arguably the most\nfamous classical IR approach.",
    "start": "300240",
    "end": "305490"
  },
  {
    "text": "BM25 stands for Best\nmatch attempt 25, which",
    "start": "305490",
    "end": "309360"
  },
  {
    "text": "suggests a lot of exploration\nof the different hyperparameters",
    "start": "309360",
    "end": "312240"
  },
  {
    "text": "of this model, looking for\na solution that was best.",
    "start": "312240",
    "end": "315150"
  },
  {
    "text": "And this has indeed\nturned out to be",
    "start": "315150",
    "end": "317220"
  },
  {
    "text": "an enduringly good solution.",
    "start": "317220",
    "end": "319350"
  },
  {
    "text": "With BM25, you're going to see\nthat this is a kind of enhanced",
    "start": "319350",
    "end": "323280"
  },
  {
    "text": "version of TF-IDF.",
    "start": "323280",
    "end": "325080"
  },
  {
    "text": "We begin from\nsmoothed IDF values.",
    "start": "325080",
    "end": "327750"
  },
  {
    "text": "These are essentially\nthe IDF values",
    "start": "327750",
    "end": "329820"
  },
  {
    "text": "that I just showed you with\na little bit of an adjustment",
    "start": "329820",
    "end": "332550"
  },
  {
    "text": "to handle the\nundefined in this case",
    "start": "332550",
    "end": "334530"
  },
  {
    "text": "that we briefly worried about.",
    "start": "334530",
    "end": "337600"
  },
  {
    "text": "The next component is scoring.",
    "start": "337600",
    "end": "339220"
  },
  {
    "text": "And this is kind of\nanalogous to term frequency.",
    "start": "339220",
    "end": "341800"
  },
  {
    "text": "And you can see\nin this definition",
    "start": "341800",
    "end": "343240"
  },
  {
    "text": "that term frequency is\nan important component.",
    "start": "343240",
    "end": "345919"
  },
  {
    "text": "We also have two hyper\nparameters, K and B,",
    "start": "345920",
    "end": "348520"
  },
  {
    "text": "which I'm going to\ntalk about in a second.",
    "start": "348520",
    "end": "351289"
  },
  {
    "text": "But just to round this\nout, the BM25 weight",
    "start": "351290",
    "end": "353920"
  },
  {
    "text": "is a combination of\nthose adjusted IDF",
    "start": "353920",
    "end": "356920"
  },
  {
    "text": "values and the scoring values,\nwhich are analogous somewhat",
    "start": "356920",
    "end": "360910"
  },
  {
    "text": "to term frequency and TF-IDF.",
    "start": "360910",
    "end": "364130"
  },
  {
    "text": "So the definitions\nare different,",
    "start": "364130",
    "end": "365510"
  },
  {
    "text": "and we're going\nto dive into them.",
    "start": "365510",
    "end": "366530"
  },
  {
    "text": "But at a high level can\nsee it's a product of two",
    "start": "366530",
    "end": "368810"
  },
  {
    "text": "very similar values to the\nones we had for TF-IDF.",
    "start": "368810",
    "end": "373110"
  },
  {
    "text": "Let's take a look at the\nindividual components",
    "start": "373110",
    "end": "375479"
  },
  {
    "text": "in a bit more detail,\nstarting with IDF.",
    "start": "375480",
    "end": "377880"
  },
  {
    "text": "What I have on the slide\nhere is the IDF plot",
    "start": "377880",
    "end": "380940"
  },
  {
    "text": "that I showed you previously\nfrom the TF-IDF definitions.",
    "start": "380940",
    "end": "384210"
  },
  {
    "text": "And then I have the BM25 variant\nof that at the bottom here.",
    "start": "384210",
    "end": "387610"
  },
  {
    "text": "And what I've done\nis just emphasized",
    "start": "387610",
    "end": "389580"
  },
  {
    "text": "that this s value here\nis standardly set at 0.5,",
    "start": "389580",
    "end": "393550"
  },
  {
    "text": "but we could, in\nprinciple, adjust it.",
    "start": "393550",
    "end": "396039"
  },
  {
    "text": "So here are a few\nvalues for that value s.",
    "start": "396040",
    "end": "399900"
  },
  {
    "text": "The standard value\nis the one in purple.",
    "start": "399900",
    "end": "402060"
  },
  {
    "text": "That's 0.5.",
    "start": "402060",
    "end": "403020"
  },
  {
    "text": "And you can see that the result\nis that we very closely match",
    "start": "403020",
    "end": "406500"
  },
  {
    "text": "the standard IDF values\nthroughout the entire space",
    "start": "406500",
    "end": "409560"
  },
  {
    "text": "of document frequency\nvalues with the exception",
    "start": "409560",
    "end": "412320"
  },
  {
    "text": "that we give a very high\nvalue incidentally to words",
    "start": "412320",
    "end": "415860"
  },
  {
    "text": "that appear in no documents that\nwon't turn out to be relevant.",
    "start": "415860",
    "end": "418919"
  },
  {
    "text": "And what really\nhappens as we adjust",
    "start": "418920",
    "end": "420810"
  },
  {
    "text": "s is we're adjusting things\nat that kind of really",
    "start": "420810",
    "end": "424230"
  },
  {
    "text": "degenerate part of this\noverall space for words",
    "start": "424230",
    "end": "427050"
  },
  {
    "text": "that appear in no documents.",
    "start": "427050",
    "end": "428759"
  },
  {
    "text": "Once we get into words\nappearing in documents,",
    "start": "428760",
    "end": "430920"
  },
  {
    "text": "the values track pretty closely\nwith maybe the exception",
    "start": "430920",
    "end": "434140"
  },
  {
    "text": "that if you set s very high,\nyou get real differences",
    "start": "434140",
    "end": "437740"
  },
  {
    "text": "in the lowest part\nof this spectrum.",
    "start": "437740",
    "end": "440840"
  },
  {
    "text": "But by and large,\nif we set it at 0.5,",
    "start": "440840",
    "end": "442850"
  },
  {
    "text": "we're just reproducing\nthe IDF values",
    "start": "442850",
    "end": "444890"
  },
  {
    "text": "that we had from the\nearlier definitions.",
    "start": "444890",
    "end": "447650"
  },
  {
    "text": "The scoring function is more\nnuanced as a result of having",
    "start": "447650",
    "end": "450530"
  },
  {
    "text": "lots of hyperparameters.",
    "start": "450530",
    "end": "451639"
  },
  {
    "text": "So let's break this\ndown a little bit.",
    "start": "451640",
    "end": "453470"
  },
  {
    "text": "See if we can get some analytic\ninsights into what's happening.",
    "start": "453470",
    "end": "457070"
  },
  {
    "text": "The scoring function is\nrepeated at the bottom here.",
    "start": "457070",
    "end": "459620"
  },
  {
    "text": "And I've highlighted\nin orange a term",
    "start": "459620",
    "end": "462080"
  },
  {
    "text": "that plays the role of\npenalizing long documents.",
    "start": "462080",
    "end": "465470"
  },
  {
    "text": "And then this plot\nshould help us see",
    "start": "465470",
    "end": "467060"
  },
  {
    "text": "precisely how that plays out.",
    "start": "467060",
    "end": "469080"
  },
  {
    "text": "So let's imagine that we're\nlooking at a document that",
    "start": "469080",
    "end": "471470"
  },
  {
    "text": "has length 10.",
    "start": "471470",
    "end": "472790"
  },
  {
    "text": "I have the term frequency\nvalues along the x-axis",
    "start": "472790",
    "end": "475970"
  },
  {
    "text": "and the BM25 scoring\nvalues along the y-axis.",
    "start": "475970",
    "end": "480240"
  },
  {
    "text": "If I am looking at a document\nthat has average-- sorry",
    "start": "480240",
    "end": "484460"
  },
  {
    "text": "if the corpus has average\ndocument length of 10,",
    "start": "484460",
    "end": "486860"
  },
  {
    "text": "that's the purple line here.",
    "start": "486860",
    "end": "488389"
  },
  {
    "text": "And that's the same length\nas our example document.",
    "start": "488390",
    "end": "491900"
  },
  {
    "text": "As our example\ndocument becomes long",
    "start": "491900",
    "end": "494509"
  },
  {
    "text": "relative to the average\nlength with 5 and 3,",
    "start": "494510",
    "end": "496850"
  },
  {
    "text": "you can see that the scoring\nvalues systematically go down.",
    "start": "496850",
    "end": "500570"
  },
  {
    "text": "So to summarize, as\nour target document is",
    "start": "500570",
    "end": "504320"
  },
  {
    "text": "long relative to the average,\nthe scores are diminished.",
    "start": "504320",
    "end": "507810"
  },
  {
    "text": "And the overall effect\nof this, as I said,",
    "start": "507810",
    "end": "510250"
  },
  {
    "text": "is to penalize long documents.",
    "start": "510250",
    "end": "511800"
  },
  {
    "text": "And the intuition there is\nthat long documents might just,",
    "start": "511800",
    "end": "515308"
  },
  {
    "text": "as a result of being\nlong, contain more terms,",
    "start": "515309",
    "end": "518490"
  },
  {
    "text": "and therefore on\naverage, we should",
    "start": "518490",
    "end": "520289"
  },
  {
    "text": "trust the terms they do\ncontain less as evidence",
    "start": "520289",
    "end": "523919"
  },
  {
    "text": "for our overall\nrelevance scoring.",
    "start": "523919",
    "end": "526650"
  },
  {
    "text": "So that's the penalty\nfor long documents.",
    "start": "526650",
    "end": "530260"
  },
  {
    "text": "Now let's dive\ninto the role of b.",
    "start": "530260",
    "end": "532930"
  },
  {
    "text": "The function of that\nhyperparameter b",
    "start": "532930",
    "end": "535089"
  },
  {
    "text": "is to control the\namount of the penalty",
    "start": "535090",
    "end": "538240"
  },
  {
    "text": "that we give to long documents.",
    "start": "538240",
    "end": "540560"
  },
  {
    "text": "So let's break that down\na little bit over here.",
    "start": "540560",
    "end": "542800"
  },
  {
    "text": "Again, we have a target\ndocument of length 10.",
    "start": "542800",
    "end": "545709"
  },
  {
    "text": "That's our example.",
    "start": "545710",
    "end": "547070"
  },
  {
    "text": "And we have an average\ndocument length of 5 over here.",
    "start": "547070",
    "end": "550570"
  },
  {
    "text": "And you can see that as we\nincrease b from 0.1 to 1,",
    "start": "550570",
    "end": "554350"
  },
  {
    "text": "the overall effect is\nto diminish the scores.",
    "start": "554350",
    "end": "557180"
  },
  {
    "text": "So higher values of b\nmean more of a penalty",
    "start": "557180",
    "end": "561220"
  },
  {
    "text": "given to long\ndocuments because that",
    "start": "561220",
    "end": "563740"
  },
  {
    "text": "reduces the score even more.",
    "start": "563740",
    "end": "565930"
  },
  {
    "text": "Over on the right here, if\nour target document is--",
    "start": "565930",
    "end": "569980"
  },
  {
    "text": "the example document\nhas length 10,",
    "start": "569980",
    "end": "572209"
  },
  {
    "text": "which is the same as the average\ndocument length for our corpus,",
    "start": "572210",
    "end": "575650"
  },
  {
    "text": "then the value of b\nmakes no difference",
    "start": "575650",
    "end": "577880"
  },
  {
    "text": "as a result of the fact\nthat there's no penalty",
    "start": "577880",
    "end": "580180"
  },
  {
    "text": "even to apply in these cases.",
    "start": "580180",
    "end": "582470"
  },
  {
    "text": "It's really just\nfor long documents",
    "start": "582470",
    "end": "584740"
  },
  {
    "text": "relative to the\naverage that b is",
    "start": "584740",
    "end": "586810"
  },
  {
    "text": "controlling the\namount of penalty",
    "start": "586810",
    "end": "588700"
  },
  {
    "text": "that we apply in those cases.",
    "start": "588700",
    "end": "591830"
  },
  {
    "text": "And then what about k?",
    "start": "591830",
    "end": "593120"
  },
  {
    "text": "What is the effect of k?",
    "start": "593120",
    "end": "594770"
  },
  {
    "text": "It appears here in\nthe scoring function.",
    "start": "594770",
    "end": "596870"
  },
  {
    "text": "The overall effect is to\nflatten out higher frequencies.",
    "start": "596870",
    "end": "600860"
  },
  {
    "text": "And I think one\nway to get a grip",
    "start": "600860",
    "end": "602600"
  },
  {
    "text": "on this is to think about the\nextreme situation in which you",
    "start": "602600",
    "end": "605690"
  },
  {
    "text": "have set k very, very low.",
    "start": "605690",
    "end": "607850"
  },
  {
    "text": "This would be a\nnon-standard value for k.",
    "start": "607850",
    "end": "610670"
  },
  {
    "text": "In the situation where\nyou set it very low,",
    "start": "610670",
    "end": "613380"
  },
  {
    "text": "what you essentially do is\nturn the scoring function",
    "start": "613380",
    "end": "616070"
  },
  {
    "text": "into an indicator function.",
    "start": "616070",
    "end": "617540"
  },
  {
    "text": "You can see that we get a\nregister of a scoring value",
    "start": "617540",
    "end": "620600"
  },
  {
    "text": "if the word is in the document.",
    "start": "620600",
    "end": "622759"
  },
  {
    "text": "And then, it simply flattens out\nover all the different values",
    "start": "622760",
    "end": "625940"
  },
  {
    "text": "for term frequency.",
    "start": "625940",
    "end": "627090"
  },
  {
    "text": "So it's like you\nappeared, and then",
    "start": "627090",
    "end": "629180"
  },
  {
    "text": "I don't care how many\ntimes you appeared,",
    "start": "629180",
    "end": "631190"
  },
  {
    "text": "I'm hardly going to\nadjust the scoring.",
    "start": "631190",
    "end": "634360"
  },
  {
    "text": "And then, as you\nmake k larger, you",
    "start": "634360",
    "end": "636730"
  },
  {
    "text": "get less and less of a\ndramatic effect like that.",
    "start": "636730",
    "end": "639199"
  },
  {
    "text": "And so you care more and more\nabout whether the word appears",
    "start": "639200",
    "end": "642010"
  },
  {
    "text": "frequently in the\ndocuments or not.",
    "start": "642010",
    "end": "643850"
  },
  {
    "text": "So this red line is\nreally an extreme case",
    "start": "643850",
    "end": "645730"
  },
  {
    "text": "where you've decided\nnot to care very",
    "start": "645730",
    "end": "647440"
  },
  {
    "text": "much about the different\nvalues of relative frequency.",
    "start": "647440",
    "end": "650860"
  },
  {
    "text": "A more standard\nvalue for k is 1.2,",
    "start": "650860",
    "end": "653589"
  },
  {
    "text": "and that's giving this kind\nof modest diminishing amount.",
    "start": "653590",
    "end": "656410"
  },
  {
    "text": "As you get terms that are\nreally frequent in documents,",
    "start": "656410",
    "end": "659410"
  },
  {
    "text": "you kind of flatten out\nthe scoring function.",
    "start": "659410",
    "end": "662750"
  },
  {
    "text": "So that's the overall effect.",
    "start": "662750",
    "end": "664120"
  },
  {
    "text": "Flattening out\nhigher frequencies",
    "start": "664120",
    "end": "665800"
  },
  {
    "text": "with the value of k.",
    "start": "665800",
    "end": "667180"
  },
  {
    "text": "Controlling how much\nflattening you decide to do.",
    "start": "667180",
    "end": "670089"
  },
  {
    "text": "With those components\nin place, we",
    "start": "672610",
    "end": "674260"
  },
  {
    "text": "can return to our\nclassic inverted index",
    "start": "674260",
    "end": "676900"
  },
  {
    "text": "from information retrieval.",
    "start": "676900",
    "end": "678178"
  },
  {
    "text": "That's an inverted\nindex in the sense",
    "start": "678178",
    "end": "679720"
  },
  {
    "text": "that we go from terms to\ndocuments rather than documents",
    "start": "679720",
    "end": "682720"
  },
  {
    "text": "to terms.",
    "start": "682720",
    "end": "683649"
  },
  {
    "text": "Have our query come in.",
    "start": "683650",
    "end": "684730"
  },
  {
    "text": "We do our term lookup.",
    "start": "684730",
    "end": "686050"
  },
  {
    "text": "And previously,\nI showed you this",
    "start": "686050",
    "end": "687730"
  },
  {
    "text": "as simply a list of\ndocuments, but now, of course,",
    "start": "687730",
    "end": "690079"
  },
  {
    "text": "with something like\nBM25 or TF-IDF,",
    "start": "690080",
    "end": "692380"
  },
  {
    "text": "we can augment this with\npre-computed scores or document",
    "start": "692380",
    "end": "696700"
  },
  {
    "text": "frequency values and with\npre-computed IDF values.",
    "start": "696700",
    "end": "700263"
  },
  {
    "text": "And so we have all\nthe ingredients",
    "start": "700263",
    "end": "701680"
  },
  {
    "text": "we need for a given query to do\nfull-on document scoring very",
    "start": "701680",
    "end": "705850"
  },
  {
    "text": "efficiently.",
    "start": "705850",
    "end": "706420"
  },
  {
    "text": "And that is one\nessential ingredient",
    "start": "706420",
    "end": "708670"
  },
  {
    "text": "for why these classical\napproaches are so",
    "start": "708670",
    "end": "711070"
  },
  {
    "text": "massively scalable.",
    "start": "711070",
    "end": "712195"
  },
  {
    "text": "That's kind of it\nfor what I wanted",
    "start": "714930",
    "end": "716640"
  },
  {
    "text": "to cover on\nclassical IR, but I'd",
    "start": "716640",
    "end": "718380"
  },
  {
    "text": "be remiss if I didn't mention\na few obvious topics that",
    "start": "718380",
    "end": "721170"
  },
  {
    "text": "are explored in detail\nin this literature.",
    "start": "721170",
    "end": "723480"
  },
  {
    "text": "We could, of course, do\nquery and document expansion.",
    "start": "723480",
    "end": "726329"
  },
  {
    "text": "We could augment what\nthe user gives us",
    "start": "726330",
    "end": "728040"
  },
  {
    "text": "and what's in our corpus\nwith additional information",
    "start": "728040",
    "end": "730350"
  },
  {
    "text": "and maybe metadata that would\nhelp us with relevance scoring.",
    "start": "730350",
    "end": "734130"
  },
  {
    "text": "We could move to\nphrase-level search.",
    "start": "734130",
    "end": "735990"
  },
  {
    "text": "I focused on unigrams.",
    "start": "735990",
    "end": "737459"
  },
  {
    "text": "But, of course, that's not\na necessary restriction.",
    "start": "737460",
    "end": "739710"
  },
  {
    "text": "We could think about n-grams\nand even more sophisticated",
    "start": "739710",
    "end": "742770"
  },
  {
    "text": "notions of linguistic units.",
    "start": "742770",
    "end": "745470"
  },
  {
    "text": "We haven't talked at all\nabout term dependence.",
    "start": "745470",
    "end": "747449"
  },
  {
    "text": "We've kind of assumed that\nall the terms in a document",
    "start": "747450",
    "end": "749700"
  },
  {
    "text": "are independent of each other.",
    "start": "749700",
    "end": "751000"
  },
  {
    "text": "But if you think about\nbigrams like New York,",
    "start": "751000",
    "end": "753960"
  },
  {
    "text": "that's obviously an\nunhappy approximation.",
    "start": "753960",
    "end": "756870"
  },
  {
    "text": "We should be thinking about\nhow all these terms have",
    "start": "756870",
    "end": "759510"
  },
  {
    "text": "their own internal\nstatistical dependencies",
    "start": "759510",
    "end": "762270"
  },
  {
    "text": "and bring that into the\nsearch functionality.",
    "start": "762270",
    "end": "764963"
  },
  {
    "text": "We could also--\nand this is really",
    "start": "764963",
    "end": "766380"
  },
  {
    "text": "important, think about\ndifferent document fields.",
    "start": "766380",
    "end": "768660"
  },
  {
    "text": "Documents are not\nhomogeneous, and words",
    "start": "768660",
    "end": "770759"
  },
  {
    "text": "that appear in the title might\nhave a different relevance",
    "start": "770760",
    "end": "773490"
  },
  {
    "text": "value inherently than\nwords that appear",
    "start": "773490",
    "end": "775740"
  },
  {
    "text": "in the body of a document.",
    "start": "775740",
    "end": "776940"
  },
  {
    "text": "And we would want our best\nclassical search technologies",
    "start": "776940",
    "end": "780305"
  },
  {
    "text": "and our best search\ntechnologies,",
    "start": "780305",
    "end": "781680"
  },
  {
    "text": "in general, to be sensitive\nto those distinctions.",
    "start": "781680",
    "end": "784920"
  },
  {
    "text": "And then, of course, a big\ngap in what I've showed so far",
    "start": "784920",
    "end": "787769"
  },
  {
    "text": "is link analysis.",
    "start": "787770",
    "end": "789120"
  },
  {
    "text": "We could think about how\nthe documents in our corpus",
    "start": "789120",
    "end": "792460"
  },
  {
    "text": "form an implicit\ngraph based on how",
    "start": "792460",
    "end": "794520"
  },
  {
    "text": "they hyperlink with each other.",
    "start": "794520",
    "end": "795990"
  },
  {
    "text": "And we know from\nmodern search that",
    "start": "795990",
    "end": "798510"
  },
  {
    "text": "is a crucial factor in shaping\nrelevance and having the best",
    "start": "798510",
    "end": "801930"
  },
  {
    "text": "documents come to the top.",
    "start": "801930",
    "end": "803190"
  },
  {
    "text": "I have left that out, but it's\nobviously incredibly important.",
    "start": "803190",
    "end": "808340"
  },
  {
    "text": "And then, of course,\nfinally learning to rank.",
    "start": "808340",
    "end": "811660"
  },
  {
    "text": "That is, learn\nfunctionality for what's",
    "start": "811660",
    "end": "813610"
  },
  {
    "text": "relevant given queries\nis an important feature",
    "start": "813610",
    "end": "816430"
  },
  {
    "text": "of the neural IR models\nthat we're going to discuss.",
    "start": "816430",
    "end": "819770"
  },
  {
    "text": "I have not introduced\nthat in the class--",
    "start": "819770",
    "end": "821560"
  },
  {
    "text": "in the context of\nclassical IR but we",
    "start": "821560",
    "end": "823450"
  },
  {
    "text": "could have learned\nranking functions that",
    "start": "823450",
    "end": "826030"
  },
  {
    "text": "would go beyond the simple\napriori calculations of things",
    "start": "826030",
    "end": "829750"
  },
  {
    "text": "like TF-IDF and BM25.",
    "start": "829750",
    "end": "831740"
  },
  {
    "text": "Finally, there are\nlots of tools out there",
    "start": "835020",
    "end": "837570"
  },
  {
    "text": "that would help you\nwith classical IR.",
    "start": "837570",
    "end": "839230"
  },
  {
    "text": "Elasticsearch is\nwidely deployed, very",
    "start": "839230",
    "end": "841680"
  },
  {
    "text": "robust mature search\ntechnology, highly scalable,",
    "start": "841680",
    "end": "845010"
  },
  {
    "text": "lots of features.",
    "start": "845010",
    "end": "846270"
  },
  {
    "text": "Pyserini and PrimeQA are\nresearch repositories",
    "start": "846270",
    "end": "849660"
  },
  {
    "text": "that could also be\nreally useful to you",
    "start": "849660",
    "end": "851639"
  },
  {
    "text": "if you want to think about\nsetting up classical IR",
    "start": "851640",
    "end": "854790"
  },
  {
    "text": "models as baselines.",
    "start": "854790",
    "end": "856199"
  },
  {
    "text": "Or as using them in components\nin larger systems that",
    "start": "856200",
    "end": "859590"
  },
  {
    "text": "might have a small role\nfor neural IR models",
    "start": "859590",
    "end": "863460"
  },
  {
    "text": "as kind of re-rankers\nof results that",
    "start": "863460",
    "end": "865380"
  },
  {
    "text": "come from a very fast, very\nrobust classical IR system.",
    "start": "865380",
    "end": "869340"
  },
  {
    "text": "That's a common mode to\noperate in that gives you",
    "start": "869340",
    "end": "871740"
  },
  {
    "text": "highly scalable solutions where\nthe neural models, that we'll",
    "start": "871740",
    "end": "874440"
  },
  {
    "text": "talk about later, play\nthe role of refining",
    "start": "874440",
    "end": "877350"
  },
  {
    "text": "the core results returned\nby the classical model.",
    "start": "877350",
    "end": "880940"
  }
]