[
  {
    "start": "0",
    "end": "6080"
  },
  {
    "text": "OK. Hello, everyone. Let's get started. So last time, what we did was\nthe NTK, the neural tangent",
    "start": "6080",
    "end": "13849"
  },
  {
    "text": "kernel approach. And so today, we're\ngoing to continue",
    "start": "13850",
    "end": "19880"
  },
  {
    "text": "with that to finish the last\npart of the neural tangent kernel approach. And then we talk about\nthe so-called implicit",
    "start": "19880",
    "end": "25790"
  },
  {
    "text": "regularization effect. So the last time, briefly,\nwe recall that last time we",
    "start": "25790",
    "end": "32180"
  },
  {
    "text": "have done the following. So we have claimed that\nthe are two steps--",
    "start": "32180",
    "end": "41239"
  },
  {
    "text": "three steps in this analysis\nusing an NTK approach.",
    "start": "41240",
    "end": "46850"
  },
  {
    "text": "So one step is that you say that\nf theta x is close to g theta",
    "start": "46850",
    "end": "56180"
  },
  {
    "text": "x in some neighborhood. [INAUDIBLE] Oh, wait.",
    "start": "56180",
    "end": "62600"
  },
  {
    "text": "Oh, sorry. ",
    "start": "62600",
    "end": "72735"
  },
  {
    "text": "Yeah, there are too\nmany steps in the setup. So I always forgot some step. ",
    "start": "72735",
    "end": "79650"
  },
  {
    "text": "The worst step I would forget\nis that I forgot to record all. ",
    "start": "79650",
    "end": "85503"
  },
  {
    "text": "This one, you can remind me. But if I forgot to record,\nthen nobody will remind me. So that's the thing\nI check every time.",
    "start": "85503",
    "end": "93030"
  },
  {
    "text": "OK, cool. And I think this is recording.",
    "start": "93030",
    "end": "98400"
  },
  {
    "text": "This is recording. And you can hear\nme on Zoom, right? Maybe-- [INAUDIBLE] it's\nrecording [INAUDIBLE]..",
    "start": "98400",
    "end": "104689"
  },
  {
    "text": "That would be great. ",
    "start": "104689",
    "end": "113320"
  },
  {
    "text": "Nobody seems to say anything. But it sounds like the-- it's recording the--\nit's receiving audio.",
    "start": "113320",
    "end": "121305"
  },
  {
    "text": "OK. ",
    "start": "121305",
    "end": "128110"
  },
  {
    "text": "So last time, we said\nthat in some neighborhood,",
    "start": "128110",
    "end": "133810"
  },
  {
    "text": "this theta around\nB theta 0, you can have accurate approximation.",
    "start": "133810",
    "end": "140680"
  },
  {
    "text": "And recall that the B\ntheta 0 was something like a neighborhood\nof size, something",
    "start": "140680",
    "end": "146140"
  },
  {
    "text": "that depends on a sigma. So B theta 0 was defined. We show that if you look\nat the neighborhood, where",
    "start": "146140",
    "end": "157230"
  },
  {
    "text": "you have some-- where a theta is close\nto theta 0 with distance, something like a square\nroot n over sigma,",
    "start": "157230",
    "end": "166870"
  },
  {
    "text": "then you indeed have something-- ",
    "start": "166870",
    "end": "172630"
  },
  {
    "text": "I guess you get how well\nthe approximation is. I think the approximation\nin this neighborhood-- the approximation error is\nsomething like beta square root",
    "start": "172630",
    "end": "185680"
  },
  {
    "text": "n-- beta n over sigma squared. ",
    "start": "185680",
    "end": "191706"
  },
  {
    "text": "All right, that's what we had. And also in this neighborhood--\ntwo, in this neighborhood,",
    "start": "191706",
    "end": "203120"
  },
  {
    "text": "so there exists a\nglobal minimum error 0.",
    "start": "203120",
    "end": "212409"
  },
  {
    "start": "212410",
    "end": "218690"
  },
  {
    "text": "Nobody seems to\nrespond to my request about confirming that\nthe audio is working.",
    "start": "218690",
    "end": "225170"
  },
  {
    "text": "But I don't know. There are only four people here. Oh, OK. Well-- OK, so you can hear me.",
    "start": "225170",
    "end": "233850"
  },
  {
    "text": "Thank you so much. Great, great. Thanks. It's just sometimes I\nget paranoid by this.",
    "start": "233850",
    "end": "243140"
  },
  {
    "text": "OK, thank you so much. OK, cool. So all right.",
    "start": "243140",
    "end": "248180"
  },
  {
    "text": "So this is what we\nproved last time. And we discussed here that\nthis quantity is the key thing, right? Beta over sigma squared\nis the very key thing.",
    "start": "248180",
    "end": "255120"
  },
  {
    "text": "And if it goes to 0,\nthen that's great. Because your error becomes\nsmaller and smaller. ",
    "start": "255120",
    "end": "262780"
  },
  {
    "text": "Oh, I see. I cannot hear you. That's the problem. I see. Probably now I can hear it.",
    "start": "262780",
    "end": "268830"
  },
  {
    "text": "I think my speaker is very-- the volume is very low. OK, cool.",
    "start": "268830",
    "end": "274590"
  },
  {
    "text": "Thanks so much. ",
    "start": "274590",
    "end": "281539"
  },
  {
    "text": "And now, the third step as we\ndiscussed last time, is that-- and we have discussed\nthe various cases, where",
    "start": "281540",
    "end": "287389"
  },
  {
    "text": "this Beta sigma squared\nis close to zero, we discussed two cases.",
    "start": "287390",
    "end": "293250"
  },
  {
    "text": "And today we are going to talk\nabout the third step, where we show that optimizing with f\ntheta within a neural network",
    "start": "293250",
    "end": "315610"
  },
  {
    "text": "is similar to\noptimizing with g theta.",
    "start": "315610",
    "end": "323379"
  },
  {
    "text": "And in some sense, the\nonly thing you care about is an analysis of the\noptimization for f theta.",
    "start": "323380",
    "end": "328930"
  },
  {
    "text": "But you want to do this\nkind of like relationship, so that you can make\nthe optimization easier.",
    "start": "328930",
    "end": "334870"
  },
  {
    "text": "And we also briefly\ndiscussed what we do with this optimization. I think there are two\nways or one way you--",
    "start": "334870",
    "end": "341319"
  },
  {
    "text": "there are two ways to\ndeal with the g theta. So I think there are two ways. One is something like\nusing strong convexity. ",
    "start": "341320",
    "end": "348819"
  },
  {
    "text": "And the other is using\nonly the smoothness. And today, we're going to\nfocus on this case, which",
    "start": "348820",
    "end": "355510"
  },
  {
    "text": "doesn't require too much of the\nbackground about optimization.",
    "start": "355510",
    "end": "360550"
  },
  {
    "text": "All right. So now, let's go\ninto the detail. ",
    "start": "360550",
    "end": "374363"
  },
  {
    "text": "By the way, I think\na small remark before I go into the detail--\nso why you care about the step three in some sense?",
    "start": "374363",
    "end": "380020"
  },
  {
    "text": "Right? So a priori there's no reason. OK, so there's one\nreason, which is",
    "start": "380020",
    "end": "385620"
  },
  {
    "text": "that you want to understand\nwhat happens when you optimize over neural networks, right? But suppose the solution--",
    "start": "385620",
    "end": "392400"
  },
  {
    "text": "at the understanding,\nsuppose at some point like we are at a moment, where\nwe want to prove this number 3",
    "start": "392400",
    "end": "398760"
  },
  {
    "text": "but we haven't succeeded. But you probably would\nquestion yourself, why I care about such an answer.",
    "start": "398760",
    "end": "404775"
  },
  {
    "text": "Even if I prove this answer,\nwhy it's interesting? And the answer is yes,\nit's not that interesting",
    "start": "404775",
    "end": "410490"
  },
  {
    "text": "because if you prove\nthat optimizing over a neural network is\nthe same as optimizing",
    "start": "410490",
    "end": "416100"
  },
  {
    "text": "some linear model\nlike a kernel method, then why not just use\na kernel method, right?",
    "start": "416100",
    "end": "423569"
  },
  {
    "text": "And it turns out that\nthat's indeed true. If you use kernel\nmethod, you are not-- it's not going to work.",
    "start": "423570",
    "end": "429060"
  },
  {
    "text": "If you optimize neural\nnetwork in this way, in this particular iteration,\nwith this particular learning ratio and so forth, it\nwouldn't work as well either.",
    "start": "429060",
    "end": "437670"
  },
  {
    "text": "So in some sense, this is-- so this whole thing, the\nvalue of this theorem",
    "start": "437670",
    "end": "442740"
  },
  {
    "text": "is only for showing that\nunder certain regime, optimizing of a neural network\nis the same as optimizing",
    "start": "442740",
    "end": "448560"
  },
  {
    "text": "of a linear model. But there is no any bigger\nimpact in some sense",
    "start": "448560",
    "end": "454598"
  },
  {
    "text": "just because you are optimizing\nthe neural artificial network in a weird regime, which is\nthe same as optimizing kernels.",
    "start": "454598",
    "end": "462210"
  },
  {
    "text": "And in this regime,\nnothing works very well. But still, for the\ntechnical reason,",
    "start": "462210",
    "end": "467660"
  },
  {
    "text": "I still try to go through this. It's not super complicated. And I think, in some\nsense, the techniques",
    "start": "467660",
    "end": "473240"
  },
  {
    "text": "also is kind of useful\npartly because I think it's somewhat surprising. Because at the first place,\nyou wouldn't probably",
    "start": "473240",
    "end": "479810"
  },
  {
    "text": "believe it, right? Why you believe that optimizing\nneural network in any case would be similar to\noptimizing kernels.",
    "start": "479810",
    "end": "485840"
  },
  {
    "text": "And this shows that that's\npossible in some cases, even though that case is not\nthat informative or that kind",
    "start": "485840",
    "end": "492380"
  },
  {
    "text": "of useful. OK. So let's analyze. So we start with the first step.",
    "start": "492380",
    "end": "499440"
  },
  {
    "text": "We start with analyzing the\noptimization with g theta. ",
    "start": "499440",
    "end": "512250"
  },
  {
    "text": "And this is really just like\nlinear regression, right? It's really understanding\noptimization",
    "start": "512250",
    "end": "519479"
  },
  {
    "text": "of linear regression. So the problem is really\nyou take the min of this y",
    "start": "519480",
    "end": "526217"
  },
  {
    "text": "back minus phi delta theta\nto a norm square with GD.",
    "start": "526217",
    "end": "533110"
  },
  {
    "text": " And just to briefly\nrecall the notation,",
    "start": "533110",
    "end": "538440"
  },
  {
    "text": "so phi is this feature matrix,\nwhich is of dimension n by p.",
    "start": "538440",
    "end": "544690"
  },
  {
    "text": "So this is n by p matrix. And each row is something\nlike right f theta and 0",
    "start": "544690",
    "end": "551530"
  },
  {
    "text": "x i-th transposed, I see.",
    "start": "551530",
    "end": "556980"
  },
  {
    "text": "You put all of these as rows. What exactly this\nphi matrix is, it doesn't matter so much anymore\nfor the rest of the discussion.",
    "start": "556980",
    "end": "565580"
  },
  {
    "text": "Because phi is just a matrix. And delta theta\nis the difference between a theta and theta 0.",
    "start": "565580",
    "end": "571010"
  },
  {
    "text": "And we are just optimizing\nover delta theta. And what we do is we just\ntake gradient descent.",
    "start": "571010",
    "end": "576050"
  },
  {
    "text": "And you'd say the\ngradient descent",
    "start": "576050",
    "end": "581200"
  },
  {
    "text": "as you take the gradient. And the gradient is\nphi transpose times y",
    "start": "581200",
    "end": "590155"
  },
  {
    "text": "vec minus phi delta theta.",
    "start": "590155",
    "end": "595210"
  },
  {
    "text": "All right. This is the gradient update. Well, kind of one of the\nfeatures of this analysis",
    "start": "595210",
    "end": "601940"
  },
  {
    "text": "is that you are looking at\nthe convergence, the changes",
    "start": "601940",
    "end": "610050"
  },
  {
    "text": "in the output space.  And in some sense,\nthis is kind of like--",
    "start": "610050",
    "end": "615890"
  },
  {
    "text": "to some extent,\nthis is the spirit of the kernel\nmethod, where you're looking at not the parameter.",
    "start": "615890",
    "end": "620907"
  },
  {
    "text": "You don't look at\nthe parameter space, but you look at output space. And when you look\nat the output space,",
    "start": "620907",
    "end": "625910"
  },
  {
    "text": "it's kind of how you're\nlooking at the function space to some extent. But what does that mean?",
    "start": "625910",
    "end": "631230"
  },
  {
    "text": "That means that you're\nlooking at the output y at time t plus 1.",
    "start": "631230",
    "end": "636530"
  },
  {
    "text": "So this is defined to\nbe just the phi times delta theta and t plus 1.",
    "start": "636530",
    "end": "642670"
  },
  {
    "text": "I guess by definition-- let's\ndefine that version with t. And you look at how you\nchange the residual over time.",
    "start": "642670",
    "end": "652760"
  },
  {
    "text": "All right. So you compare your\noutput, item t,",
    "start": "652760",
    "end": "658620"
  },
  {
    "text": "with the target output y vec.",
    "start": "658620",
    "end": "665730"
  },
  {
    "text": "And how does this\nchange over time? And this is just the\ndefinition and your plug-in--",
    "start": "665730",
    "end": "674360"
  },
  {
    "text": "the definition of theta t, which\nis delta theta t minus eta phi",
    "start": "674360",
    "end": "681019"
  },
  {
    "text": "transpose, y vec minus phi\ndelta theta, minus y vec.",
    "start": "681020",
    "end": "688190"
  },
  {
    "text": "And now this requires\nsome kind of rearrangement to make it look cleaner.",
    "start": "688190",
    "end": "694310"
  },
  {
    "text": "And how do I rearrange this? I guess I'm going to first\ngroup everything on about",
    "start": "694310",
    "end": "703900"
  },
  {
    "text": "the delta theta. So you group this term\nand the term related this.",
    "start": "703900",
    "end": "709120"
  },
  {
    "text": "This also delta theta t. So you get phi minus\neta phi transposed phi.",
    "start": "709120",
    "end": "717385"
  },
  {
    "start": "717385",
    "end": "723300"
  },
  {
    "text": "I think I'm missing\nsomething here. What's wrong with this?",
    "start": "723300",
    "end": "729649"
  },
  {
    "text": "One moment, sorry. Yeah, OK. I think I'm right.",
    "start": "729650",
    "end": "735529"
  },
  {
    "text": "So if you look at what's-- multiplied in front of this\nterm, there's this one. This one, right? So there are three things--",
    "start": "735530",
    "end": "742165"
  },
  {
    "text": "multiplier in front of theta t.  So this is the multiplier\nin front of theta t.",
    "start": "742166",
    "end": "749300"
  },
  {
    "text": "And then you look at the\nmultiplier in front of y vec, then you get minus eta phi phi\ntransposed plus identity y vec.",
    "start": "749300",
    "end": "767120"
  },
  {
    "text": "And interestingly, this, you\ncan write it as I minus eta phi",
    "start": "767120",
    "end": "774440"
  },
  {
    "text": "transposed-- sorry-- phi.",
    "start": "774440",
    "end": "780925"
  },
  {
    "text": " I'll write this as eta phi\nphi transpose times phi,",
    "start": "780925",
    "end": "790410"
  },
  {
    "text": "delta theta t minus I minus\neta phi phi transposed, y vec.",
    "start": "790410",
    "end": "798629"
  },
  {
    "text": "And then you get I minus eta\nphi phi transpose delta theta",
    "start": "798630",
    "end": "808600"
  },
  {
    "text": "t minus y vec. All of these are basically\njust the standard calculation.",
    "start": "808600",
    "end": "814750"
  },
  {
    "text": "I think some version\nof-- if you take some version of a linear\nregression course,",
    "start": "814750",
    "end": "820660"
  },
  {
    "text": "then probably you\nwould have seen this. You got this. So that's the update.",
    "start": "820660",
    "end": "828040"
  },
  {
    "text": "That's the recursion for\nthe residual of the output.",
    "start": "828040",
    "end": "834949"
  },
  {
    "text": "And you can see what\nhappens is basically the residual in the\nprevious round got multiplied by this matrix.",
    "start": "834950",
    "end": "840880"
  },
  {
    "text": "And what this matrix is? This matrix is a matrix that\nis smaller than an entity, because you have\nI minus something.",
    "start": "840880",
    "end": "847420"
  },
  {
    "text": "And the something\nis a PSD matrix. So you are shrinking\nyour residual in some way",
    "start": "847420",
    "end": "852970"
  },
  {
    "text": "every time. And you can quantify how\nfast you are shrinking.",
    "start": "852970",
    "end": "858430"
  },
  {
    "text": "So when eta is less than\nsay 2 over sigma max,",
    "start": "858430",
    "end": "864399"
  },
  {
    "text": "phi phi transposed. And let's define this quantity. Let's call it 1\nover 2 tau squared.",
    "start": "864400",
    "end": "871620"
  },
  {
    "text": "Then you can show that\nthis convergence--",
    "start": "871620",
    "end": "877339"
  },
  {
    "text": "then I minus eta\nphi phi transpose",
    "start": "877340",
    "end": "882650"
  },
  {
    "text": "can be shown to have\noperator norm less than 1",
    "start": "882650",
    "end": "891680"
  },
  {
    "text": "over eta tau eta.",
    "start": "891680",
    "end": "902020"
  },
  {
    "text": "So I think I have some\neta sigma squared. ",
    "start": "902020",
    "end": "911720"
  },
  {
    "text": "And then you have-- so this, also this is a moment. But suppose you have this,\nthen you know that y t--",
    "start": "911720",
    "end": "918440"
  },
  {
    "text": "y-hat t minus y back into norm\nis less than the operator norm",
    "start": "918440",
    "end": "924870"
  },
  {
    "text": "of this matrix times-- sorry, let's take this\nt plus 1, y vec 2 norm.",
    "start": "924870",
    "end": "935410"
  },
  {
    "text": "So this is less than 1\nminus eta sigma squared, y-hat t minus y, 2 norm.",
    "start": "935410",
    "end": "943583"
  },
  {
    "text": "And if you do a\nlot of recursions, you get 1 minus\neta sigma squared to the power t plus 1 times\ny vec0 minus y, 2 norm.",
    "start": "943583",
    "end": "954950"
  },
  {
    "text": "So you have an exponential\ndecay of error. ",
    "start": "954950",
    "end": "961412"
  },
  {
    "text": "[INAUDIBLE] Yes, that's a good point.",
    "start": "961412",
    "end": "966839"
  },
  {
    "text": "So sigma is the\nminimum value of phi.",
    "start": "966840",
    "end": "973650"
  },
  {
    "text": "And let's prove this right now. Let's say this is number one. But suppose you have\nnumber one, then",
    "start": "973650",
    "end": "979650"
  },
  {
    "text": "you have all of this\nexponential decay. So now, let's prove\nthe number one.",
    "start": "979650",
    "end": "985110"
  },
  {
    "text": "So how do we do that? ",
    "start": "985110",
    "end": "997569"
  },
  {
    "text": "So establishing\n1-- so intuitively,",
    "start": "997570",
    "end": "1005970"
  },
  {
    "text": "it's just really that\nthis is a matrix PST so that's why you subtract\nsomething by your operator",
    "start": "1005970",
    "end": "1011430"
  },
  {
    "text": "norm less than 1. All right. But we need to make sure we\nknow exactly how small it is. But it cannot be--",
    "start": "1011430",
    "end": "1017543"
  },
  {
    "text": "it has to be\nstrictly less than 1. That's why we needed\nthis equality 1. And to see this, I guess there\nare multiple ways to see it.",
    "start": "1017543",
    "end": "1025842"
  },
  {
    "text": "But I think the\nway I tend to think about it is that-- so\nfirst of all, sigma is sigma min of phi, which\nis also the sigma min of phi",
    "start": "1025842",
    "end": "1036059"
  },
  {
    "text": "phi transposed square root. This is just by the\nstandard property",
    "start": "1036060",
    "end": "1041720"
  },
  {
    "text": "of the singular values.  So I guess one way to think\nabout this is that if you",
    "start": "1041720",
    "end": "1049250"
  },
  {
    "text": "look at the eigen single-- either eigenvalue\nor singular value--",
    "start": "1049250",
    "end": "1054554"
  },
  {
    "text": " because this is a PSD matrix,\nso it doesn't matter--",
    "start": "1054555",
    "end": "1061330"
  },
  {
    "text": "value of phi phi\ntransposed, then let's say suppose the\neigenvalues are tau 1 squared",
    "start": "1061330",
    "end": "1068559"
  },
  {
    "text": "up to tau 1 squared,\nand then tau 1 squared is equal to tau squared. That's the definition\nof the sigma max, right?",
    "start": "1068560",
    "end": "1076580"
  },
  {
    "text": "And tau n squared is\nthe sigma squared. This is my definition\nof the sigma min.",
    "start": "1076580",
    "end": "1083980"
  },
  {
    "text": "And then the I minus-- we care about this matrix. So this one has singular\nvalue-- eigenvalue or singular",
    "start": "1083980",
    "end": "1093750"
  },
  {
    "text": "value 1 minus eta tau1 squared\nup to 1 minus eta tau-n",
    "start": "1093750",
    "end": "1103290"
  },
  {
    "text": "squared. Right? This is just because I\nhas singular value of 1,",
    "start": "1103290",
    "end": "1109540"
  },
  {
    "text": "so I'm not sure\nwhether this is-- do you think this\nthing requires a proof? This is just because,\nI guess, there",
    "start": "1109540",
    "end": "1115390"
  },
  {
    "text": "are many ways to see this. I guess the best way for\nme to see it is always take",
    "start": "1115390",
    "end": "1121877"
  },
  {
    "text": "eigen decomposition. You say, phi is u\nsigma v transpose where sigma is this matrix\nwith tau-1 up to tau end.",
    "start": "1121877",
    "end": "1130160"
  },
  {
    "text": "And then I minus\nphi phi transposed eta transpose is just I\nminus eta u sigma squared",
    "start": "1130160",
    "end": "1143170"
  },
  {
    "text": "u transposed, right? And I is equals\nto u u transpose.",
    "start": "1143170",
    "end": "1150580"
  },
  {
    "text": " Transpose, and then you get\nu times I minus eta sigma",
    "start": "1150580",
    "end": "1157809"
  },
  {
    "text": "squared u transpose. And this is the SVD for\nall the eigen decomposition",
    "start": "1157810",
    "end": "1164350"
  },
  {
    "text": "or SVD for I minus eta\nphi phi transposed.",
    "start": "1164350",
    "end": "1171760"
  },
  {
    "text": "And that's why what's inside is\nthe eigenvalues or the singular values for the resulting matrix.",
    "start": "1171760",
    "end": "1178210"
  },
  {
    "text": "All right. And then now you\nbound this, right? But if you care about\nthe operator norm,",
    "start": "1178210",
    "end": "1183460"
  },
  {
    "text": "you care about the\nlargest, the absolute value of the eigenvalue, right? So basically, that's I\nminus eta phi phi transpose",
    "start": "1183460",
    "end": "1193470"
  },
  {
    "text": "the operator norm is less than\nthe max over j, 1 minus eta tau",
    "start": "1193470",
    "end": "1199169"
  },
  {
    "text": "j squared-- absolute value. And I think your eta is\nguaranteed-- the choice of eta",
    "start": "1199170",
    "end": "1205529"
  },
  {
    "text": "is trying to guarantee that you\nnever get to the negative side. You try to make sure that eta is\nless than 1 over 2 tau squared.",
    "start": "1205530",
    "end": "1212490"
  },
  {
    "text": "And tau is the largest one. So that's why even\nthe largest one is-- even the largest tau\nwill not make the 1 minus eta",
    "start": "1212490",
    "end": "1219780"
  },
  {
    "text": "tau squared negative. So everything is positive. So then this is just equals to\n1 minus eta tau-n squared, which",
    "start": "1219780",
    "end": "1235220"
  },
  {
    "text": "is equals to 1 minus\neta sigma squared. OK? So it sounds good.",
    "start": "1235220",
    "end": "1243392"
  },
  {
    "text": "Yeah, in some sense\nthese are sometimes",
    "start": "1243392",
    "end": "1248554"
  },
  {
    "text": "the basics of optimization. But this course doesn't\nrequire optimization. That's why I'm providing\nsome basic tools here.",
    "start": "1248555",
    "end": "1255890"
  },
  {
    "text": "And all right. OK, so basically we are\ndone with the analysis",
    "start": "1255890",
    "end": "1263899"
  },
  {
    "text": "of this linear\nregression, right? After you have this,\nyou know that you are-- your error is decaying\nexponentially fast.",
    "start": "1263900",
    "end": "1271190"
  },
  {
    "text": "And then after sufficient\nnumber of iterations, your error is small. All right.",
    "start": "1271190",
    "end": "1276560"
  },
  {
    "text": "So basically, maybe\nlet's call this two. So from two, you have \"for\nT is at most something",
    "start": "1276560",
    "end": "1287110"
  },
  {
    "text": "like log 1 over epsilon, over\neta sigma squared, iteration.",
    "start": "1287110",
    "end": "1293750"
  },
  {
    "text": "So your error y-hat t minus y\nvec is less than epsilon times",
    "start": "1293750",
    "end": "1302530"
  },
  {
    "text": "the initial thing.",
    "start": "1302530",
    "end": "1307870"
  },
  {
    "text": "And you can take epsilon to\nbe small so that you can--",
    "start": "1307870",
    "end": "1313390"
  },
  {
    "text": "you can take epsilon to be\nvery small, because it's a logarithmic decay of errors.",
    "start": "1313390",
    "end": "1319470"
  },
  {
    "text": "OK.  So this is the analysis for g.",
    "start": "1319470",
    "end": "1324760"
  },
  {
    "text": "And now let's talk\nabout analysis for f. All right. So you will see that\nthe analysis for f",
    "start": "1324760",
    "end": "1334059"
  },
  {
    "text": "is very similar to this\nbut with some tweak. Maybe let me state the\ntheorem just so that we have",
    "start": "1334060",
    "end": "1340180"
  },
  {
    "text": "a formal statement somewhere. So there exists a\nconstant, say, c0",
    "start": "1340180",
    "end": "1346906"
  },
  {
    "text": "and 0, 1 such that\nwhen this key quantity",
    "start": "1346906",
    "end": "1355420"
  },
  {
    "text": "beta over sigma\nsquared is less than c0 for sufficiently small eta.",
    "start": "1355420",
    "end": "1365410"
  },
  {
    "text": "And eta could-- this\ncould depend on-- which could depend\non beta, sigma,",
    "start": "1365410",
    "end": "1375510"
  },
  {
    "text": "maybe the dimension\np, so and so forth. I think you can have\na concrete bound for how small eta has to be.",
    "start": "1375510",
    "end": "1381899"
  },
  {
    "text": "But I am just lazy. I want to, but I'm lazy. But I want to have\nthese details.",
    "start": "1381900",
    "end": "1387510"
  },
  {
    "text": "I'm also lazy, but I also\nwant to have these details. So that it's not too\ncomplicated for the explanation.",
    "start": "1387510",
    "end": "1395220"
  },
  {
    "text": "But when you have\nsufficient small eta, then in T is equals o of\nlog 1 over epsilon, over eta",
    "start": "1395220",
    "end": "1403860"
  },
  {
    "text": "sigma squared steps,\nthe empirical loss",
    "start": "1403860",
    "end": "1410700"
  },
  {
    "text": "for the f theta t is also less\nthan-- is less than epsilon. ",
    "start": "1410700",
    "end": "1418470"
  },
  {
    "text": "So the empirical loss\nfor the neural network is also having\nthis error epsilon.",
    "start": "1418470",
    "end": "1424000"
  },
  {
    "text": "So how do we do this? So I guess we have kind\nof discussed the intuition already.",
    "start": "1424000",
    "end": "1429390"
  },
  {
    "text": "So the intuition\nis that you always have to try to kind of\nrelate this to the g.",
    "start": "1429390",
    "end": "1434908"
  },
  {
    "text": "And here, by relating to\nthe g, basically, you're just trying to\nfollow-- what I mean is, you really try to follow\nthe proof you had before.",
    "start": "1434908",
    "end": "1442740"
  },
  {
    "text": "Just try to imitate\nas much as possible. And of course, there\nwill be some differences. And then you will deal\nwith the differences.",
    "start": "1442740",
    "end": "1450690"
  },
  {
    "text": "So I think one\ndifference is that-- by the way this\nis a proof sketch, because I'm going to omit some\nsmall technical jargon, which",
    "start": "1450690",
    "end": "1458940"
  },
  {
    "text": "is not super important. So the important\nthing is that you have",
    "start": "1458940",
    "end": "1464730"
  },
  {
    "text": "a changing phi in some sense. So this is what's the difference\nin neural networks, when",
    "start": "1464730",
    "end": "1472590"
  },
  {
    "text": "you have neural\nnetworks compared to the linear regression case.",
    "start": "1472590",
    "end": "1480700"
  },
  {
    "text": "So you'll see why\nthis is the case. So suppose you define this\nphi, phi superscript t",
    "start": "1480700",
    "end": "1487230"
  },
  {
    "text": "to be the kernel at\ntime T. All right.",
    "start": "1487230",
    "end": "1494480"
  },
  {
    "text": "So this is the-- ",
    "start": "1494480",
    "end": "1502520"
  },
  {
    "text": "this is the NTK kernel, when\nyou Taylor expand at time T. And if you try to expand at time\nT, then the gradient descent--",
    "start": "1502520",
    "end": "1512230"
  },
  {
    "text": "I think we have\ndiscussed this before, but now you can\nsee it explicitly. I think I have claimed that if\nyou Taylor expand at time T,",
    "start": "1512230",
    "end": "1518679"
  },
  {
    "text": "then the gradient in\nrespect to the approximation is the same as the\ngradient with respect",
    "start": "1518680",
    "end": "1524260"
  },
  {
    "text": "to the origin of the\nneural network, right?",
    "start": "1524260",
    "end": "1530210"
  },
  {
    "text": "So this is what I\nthink you wrote. At the very end here,\nthis is a remark. So if you Taylor\nexpand at time T,",
    "start": "1530210",
    "end": "1537400"
  },
  {
    "text": "then the gradient with\nrespect to neural network is the same as the gradient\nin respect to linear function.",
    "start": "1537400",
    "end": "1542527"
  },
  {
    "text": "This is just because\nthese two things agree.  These two things agree at\nthis point up to first order.",
    "start": "1542527",
    "end": "1549742"
  },
  {
    "text": "So that's why even you\ncompose with the loss function up to the first order,\nthey still agree. ",
    "start": "1549742",
    "end": "1557259"
  },
  {
    "text": "And here, you can even\nsee that explicitly. So suppose you write down the\ngradient of the loss function--",
    "start": "1557260",
    "end": "1565910"
  },
  {
    "text": "I'd say at t-- then what you've\ngot is that you have 1/n. ",
    "start": "1565910",
    "end": "1572887"
  },
  {
    "text": "You can do the chain rule. So you can get this is y i-th\nminus f theta t x-th i-th times",
    "start": "1572887",
    "end": "1582039"
  },
  {
    "text": "gradient f sub theta\nt times x i-th. You can verify this just even--\nwithout using the remark I had,",
    "start": "1582040",
    "end": "1589399"
  },
  {
    "text": "right? This is just the chain rule. And then this is equals to--",
    "start": "1589400",
    "end": "1594655"
  },
  {
    "text": " if you view this as\nthis is from the phi,",
    "start": "1594655",
    "end": "1600390"
  },
  {
    "text": "this is the ith row\nof the phi, right, so this corresponds\nto ith row phi.",
    "start": "1600390",
    "end": "1606780"
  },
  {
    "text": "And this corresponds\nto the difference of-- maybe let me write\nthis more explicitly.",
    "start": "1606780",
    "end": "1612480"
  },
  {
    "text": "This is y i-th minus y\ni-th t times the gradient.",
    "start": "1612480",
    "end": "1621535"
  },
  {
    "text": " And then if you write this as a\nvector for matrix modification",
    "start": "1621535",
    "end": "1628910"
  },
  {
    "text": "formula, you got phi t which\ncorresponds to this one. And then you have y\nvec minus y-hat t.",
    "start": "1628910",
    "end": "1640980"
  },
  {
    "text": "And there's a 1/n\nover n in front of it. Right.",
    "start": "1640980",
    "end": "1646890"
  },
  {
    "text": "So that's the gradient.  And that means that the\nupdate rule for theta t,",
    "start": "1646890",
    "end": "1657000"
  },
  {
    "text": "I think I somehow\nhave a theta here. Yeah, updated rule\nfor delta theta t--",
    "start": "1657000",
    "end": "1662130"
  },
  {
    "text": "I guess, I'm going to use theta\nt instead of theta theta t. They are the same, right? They are just only different\nup to a translation.",
    "start": "1662130",
    "end": "1668799"
  },
  {
    "text": "So theta t plus 1, is\nequals to theta t minus eta",
    "start": "1668800",
    "end": "1673830"
  },
  {
    "text": "times this gradient. ",
    "start": "1673830",
    "end": "1680870"
  },
  {
    "text": "And this is equals to a theta\nt minus 1/n times phi t y vec",
    "start": "1680870",
    "end": "1686585"
  },
  {
    "text": "y-hat t And-- ",
    "start": "1686585",
    "end": "1697140"
  },
  {
    "text": "OK. So now there is a little bit\nkind of like a small thing",
    "start": "1697140",
    "end": "1703510"
  },
  {
    "text": "here. So suppose you say, you\ngive a name called this b t. So then this is--",
    "start": "1703510",
    "end": "1710940"
  },
  {
    "text": "let's say-- so eta 1/n.",
    "start": "1710940",
    "end": "1717350"
  },
  {
    "text": "So this is theta t, eta b t. So I'm going to try to--",
    "start": "1717350",
    "end": "1724240"
  },
  {
    "text": "OK, so what's our goal? Our goal is to try to deal\nwith recursion for the y's. That's what we\ndid before, right?",
    "start": "1724240",
    "end": "1730170"
  },
  {
    "text": "A recursion for the y's. And how do I get a\nrecursion for the y's? I have to look at\nhow the y changes.",
    "start": "1730170",
    "end": "1736720"
  },
  {
    "text": "What is the y? This is one entry of the y-hat--",
    "start": "1736720",
    "end": "1741809"
  },
  {
    "text": "the output at time t plus 1. But I also write\nthis as of something",
    "start": "1741810",
    "end": "1747710"
  },
  {
    "text": "related to the time at the\nfunction output at time t.",
    "start": "1747710",
    "end": "1753149"
  },
  {
    "text": "So how do I do that? This is a nonlinear function. Before, we just do a\nlinear multiplication. Because before we just know\nthat if this were g, then",
    "start": "1753150",
    "end": "1762990"
  },
  {
    "text": "this is just equals to phi\ntimes delta theta, t plus 1, right, if this f was g.",
    "start": "1762990",
    "end": "1769740"
  },
  {
    "text": "But because this is nonlinear,\nwe have to do something. So what we do is we try to\nTaylor expand at time t.",
    "start": "1769740",
    "end": "1777179"
  },
  {
    "text": "So that you can have a\nrelationship between theta t and theta t plus 1.",
    "start": "1777180",
    "end": "1782345"
  },
  {
    "text": "So if you try to\nexpand, then you have to write the gradient\nof f say that t x i and times",
    "start": "1782345",
    "end": "1791549"
  },
  {
    "text": "the differences\nbetween the two iterate and plus something\nhigh order, right?",
    "start": "1791550",
    "end": "1801142"
  },
  {
    "text": "And if you look at\nwhat's the difference? The difference is\na function of eta. All right. The difference is\nexactly this eta b t.",
    "start": "1801142",
    "end": "1807430"
  },
  {
    "text": "So that's why we can write plus\nthe gradient of f-theta t x i",
    "start": "1807430",
    "end": "1815920"
  },
  {
    "text": "times-- minus eta b t and plus-- the second order term\nwill be a quadratic.",
    "start": "1815920",
    "end": "1822580"
  },
  {
    "text": "And this will be\nquadratic in this way if I'm going to write\nit somewhat informally.",
    "start": "1822580",
    "end": "1832000"
  },
  {
    "text": "And more formally, I can\nwrite this as something-- a function of eta\nsquared times something,",
    "start": "1832000",
    "end": "1841590"
  },
  {
    "text": "because the difference\nhas an eta in it. So that's why you square it. You'll get eta squared.",
    "start": "1841590",
    "end": "1846840"
  },
  {
    "text": "And sometimes this is a\nterm I want to ignore. I'm trying very hard\nhere just because I",
    "start": "1846840",
    "end": "1852500"
  },
  {
    "text": "want to ignore this term. And the reason why we ignore\nit is because it's eta squared, right?",
    "start": "1852500",
    "end": "1859740"
  },
  {
    "text": "So basically, what I'm going to\nsay is that Mt, the constant,",
    "start": "1859740",
    "end": "1864780"
  },
  {
    "text": "is not a function of eta. So if you fix\neverything else and just",
    "start": "1864780",
    "end": "1871429"
  },
  {
    "text": "take eta to be very small-- so if you take eta to go to\n0, then the second order term,",
    "start": "1871430",
    "end": "1877520"
  },
  {
    "text": "the eta squared Mt\nterm is negligible.",
    "start": "1877520",
    "end": "1883613"
  },
  {
    "text": "We can do this more\nformally, but I don't want to go into so much jargons. But there is a way for you to\nkind of bound Mt by something,",
    "start": "1883613",
    "end": "1891810"
  },
  {
    "text": "right? Whatever you bound it to, right,\nso you get the bond for Mt and then you just say that\nif eta is small enough,",
    "start": "1891810",
    "end": "1897080"
  },
  {
    "text": "so that eta squared of\nMt becomes negligible. That is basically how\nyou formally do it.",
    "start": "1897080",
    "end": "1903410"
  },
  {
    "text": "So if you ignore this\nsecond order term, then everything becomes\nso simple, right? So for now, let's ignore\nthe second order term.",
    "start": "1903410",
    "end": "1911240"
  },
  {
    "start": "1911240",
    "end": "1918960"
  },
  {
    "text": "Then what you have is\nthat y hat t plus 1.",
    "start": "1918960",
    "end": "1924299"
  },
  {
    "text": "This is equals to-- if you put this equation,\nlet's call this equation 3",
    "start": "1924300",
    "end": "1930540"
  },
  {
    "text": "in vector form. ",
    "start": "1930540",
    "end": "1936270"
  },
  {
    "text": "Now you've got y t hat-- y hat t plus 1 is\nequals to y hat t. Because this is y hat t.",
    "start": "1936270",
    "end": "1942990"
  },
  {
    "text": "And plus, this linear\nfunction of b t, right? So what is this? This is really eta--",
    "start": "1942990",
    "end": "1948510"
  },
  {
    "text": " minus eta times phi\ntransposed times b t.",
    "start": "1948510",
    "end": "1963429"
  },
  {
    "text": "I guess actually this is right. So yes, phi\ntransposed times b t. And then plus something like\neta squared times some constant.",
    "start": "1963430",
    "end": "1973570"
  },
  {
    "text": "I'm going to keep this just\nfor a little bit, so that-- but essentially, I\nwant to ignore it.",
    "start": "1973570",
    "end": "1979900"
  },
  {
    "text": "And then you can\nrewrite this as y hat t minus eta phi transposed.",
    "start": "1979900",
    "end": "1984940"
  },
  {
    "text": "And what is b t? b t is this difference between\ntheta t and theta t prime. This is something\nlike this, right?",
    "start": "1984940",
    "end": "1991583"
  },
  {
    "text": "Let's go back to that. So this phi-- wait,\nI'm not sure why",
    "start": "1991583",
    "end": "1999250"
  },
  {
    "text": "I'm missing like I\nhave a constant here.",
    "start": "1999250",
    "end": "2004820"
  },
  {
    "text": "Oh, I see. I see. So I think this 1/n-- ",
    "start": "2004820",
    "end": "2011310"
  },
  {
    "text": "I guess there's some\nmismatch in my notes. But it doesn't matter. So let's have the 1/n here.",
    "start": "2011310",
    "end": "2016710"
  },
  {
    "text": "Well, like for the\nlinear regression case, I didn't have the 1/n\nin the loss function. And now I have the 1/n. And that's why there's a\nlittle bit of mismatch.",
    "start": "2016710",
    "end": "2023510"
  },
  {
    "text": "But it's not a\nfundamental difference.",
    "start": "2023510",
    "end": "2029280"
  },
  {
    "text": "So let's have the 1/n. And then you have-- ",
    "start": "2029280",
    "end": "2035610"
  },
  {
    "text": "actually this is phi t. So this is phi t.",
    "start": "2035610",
    "end": "2041340"
  },
  {
    "text": "And then you have\neta phi T phi T",
    "start": "2041340",
    "end": "2049109"
  },
  {
    "text": "transposed times y vec\nminus y hat t times 1/n.",
    "start": "2049110",
    "end": "2055739"
  },
  {
    "text": " How do I do this?",
    "start": "2055739",
    "end": "2061770"
  },
  {
    "text": "So let me ignore\nthe 1/n forever. Because you can redefine a loss\nfunction whatever you want,",
    "start": "2061770",
    "end": "2068429"
  },
  {
    "text": "right? So let's say just we don't\nhave 1/n in the loss function. Sorry.",
    "start": "2068430",
    "end": "2074408"
  },
  {
    "text": "So then you have this. And then this becomes-- ",
    "start": "2074409",
    "end": "2081969"
  },
  {
    "text": "if you subtract y vec\nfrom both of these",
    "start": "2081969",
    "end": "2106920"
  },
  {
    "text": "and then you will\nreorganize this, so you're going to get this\nis equals to I minus eta phi t",
    "start": "2106920",
    "end": "2119372"
  },
  {
    "text": "phi t transposed, y\nhat t minus y vec.",
    "start": "2119373",
    "end": "2127845"
  },
  {
    "text": " I think there's-- somehow, there\nis a little bit problem with",
    "start": "2127845",
    "end": "2135065"
  },
  {
    "text": "the-- ",
    "start": "2135065",
    "end": "2140900"
  },
  {
    "text": "there's a little bit. I think this is a plus. This is a plus. Right. OK.",
    "start": "2140900",
    "end": "2146540"
  },
  {
    "text": "But the point is that basically\nif you compare this equation,",
    "start": "2146540",
    "end": "2154790"
  },
  {
    "text": "I guess, technically, you still\nhave some eta squared term, which we don't care.",
    "start": "2154790",
    "end": "2160640"
  },
  {
    "text": "If you compare this equation\nwith the recursion before-- the recursion before was\nthis, actually, maybe here--",
    "start": "2160640",
    "end": "2170910"
  },
  {
    "text": "the only difference is that\nthis matrix is different. But before you are multiplying\nwith a fixed matrix phi phi",
    "start": "2170910",
    "end": "2177780"
  },
  {
    "text": "transposed. And now you are using\nphi t phi t transpose.",
    "start": "2177780",
    "end": "2183290"
  },
  {
    "text": " But everything could be\nthe same if this phi t--",
    "start": "2183290",
    "end": "2190204"
  },
  {
    "text": "but you don't\nnecessarily need it to be the same, right,\nin the original proof. You only need this\nmatrix to be smaller.",
    "start": "2190205",
    "end": "2197100"
  },
  {
    "text": "You only need I minus eta phi\nphi transpose for this matrix to be smaller than an entity. Right?",
    "start": "2197100",
    "end": "2202470"
  },
  {
    "text": "So suppose we ignore the eta\nsquared M term, because it's",
    "start": "2202470",
    "end": "2208109"
  },
  {
    "text": "second order. And then suppose\ntheta t minus theta 0.",
    "start": "2208110",
    "end": "2215390"
  },
  {
    "text": "Suppose you are within\nsigma over 4 beta at time t.",
    "start": "2215390",
    "end": "2221510"
  },
  {
    "text": " So suppose you are not very\nfar away from the theta",
    "start": "2221510",
    "end": "2228460"
  },
  {
    "text": "0, OK, so then you know that\nphi t minus phi in 2 norm",
    "start": "2228460",
    "end": "2236099"
  },
  {
    "text": "is less than sigma over 4. This is by Lipschitzness of phi.",
    "start": "2236100",
    "end": "2244020"
  },
  {
    "text": "This is our assumption. And that means that\nsigma min of phi t",
    "start": "2244020",
    "end": "2250140"
  },
  {
    "text": "is not very different\nfrom the sigma min of phi,",
    "start": "2250140",
    "end": "2256661"
  },
  {
    "text": "let's say, minus sigma over 4. This is the largest,\n3/4 times sigma.",
    "start": "2256662",
    "end": "2263420"
  },
  {
    "text": "So sigma min of\nphi t is also good.  So you still have a lower\nbound for the eigenvalue.",
    "start": "2263420",
    "end": "2270280"
  },
  {
    "text": "It's just a little bit weaker\nup to a constant factor. And that means that I minus\neta phi t phi t transposed",
    "start": "2270280",
    "end": "2281280"
  },
  {
    "text": "operator norm is less than\n1 minus eta times 3/4 times",
    "start": "2281280",
    "end": "2287850"
  },
  {
    "text": "sigma. All right. So very similar to the before. ",
    "start": "2287850",
    "end": "2295079"
  },
  {
    "text": "So but there is an\nassumption here. So this sounds great, right? But there is an\nassumption, which is that theta t is not\nvery far away from theta 0.",
    "start": "2295080",
    "end": "2302250"
  },
  {
    "text": "This is something you\ncannot take for granted. You have to prove\nit is the case. So that's why we\nhave to inductively--",
    "start": "2302250",
    "end": "2309180"
  },
  {
    "text": "so basically, the\nonly thing left is that we need to\ninductively prove this.",
    "start": "2309180",
    "end": "2320380"
  },
  {
    "text": "Prove theta t minus\ntheta 0 is never too big. ",
    "start": "2320380",
    "end": "2331310"
  },
  {
    "text": "Basically, that's the thing. ",
    "start": "2331310",
    "end": "2337775"
  },
  {
    "text": "And in some sense,\nthis is expected. ",
    "start": "2337775",
    "end": "2342829"
  },
  {
    "text": "This is expected because-- in some sense this is expected\nbecause recall that delta theta",
    "start": "2342830",
    "end": "2350869"
  },
  {
    "text": "hat the 2 norm-- theta hat was the\nglobal minimizer.",
    "start": "2350870",
    "end": "2357260"
  },
  {
    "text": "This is the global min\nthat we constructed",
    "start": "2357260",
    "end": "2362540"
  },
  {
    "text": "in the last lecture. So we said that there is a\nglobal min of size squared",
    "start": "2362540",
    "end": "2369280"
  },
  {
    "text": "root n over sigma, right? And because there's a\nglobal min with this size,",
    "start": "2369280",
    "end": "2376570"
  },
  {
    "text": "and if this is much, much\nless than sigma over 4 beta--",
    "start": "2376570",
    "end": "2382090"
  },
  {
    "text": "so when beta over sigma squared\nis sufficiently small, right?",
    "start": "2382090",
    "end": "2389710"
  },
  {
    "text": "So I guess this-- when beta over sigma squared\nis sufficiently small,",
    "start": "2389710",
    "end": "2395000"
  },
  {
    "text": "then you have this inequality. And that means that there exists\na global min within this region",
    "start": "2395000",
    "end": "2400520"
  },
  {
    "text": "sigma over 4 beta. If there exist a global\nmin within this region, why you should leave this region?",
    "start": "2400520",
    "end": "2406220"
  },
  {
    "text": "Right? That's why you should somewhat\nexpect that it's always within this region.",
    "start": "2406220",
    "end": "2412910"
  },
  {
    "text": "And how do we formally do this? I think you just say formally,\nyou just do an induction,",
    "start": "2412910",
    "end": "2420424"
  },
  {
    "text": "right, because you know that-- I guess a square root.",
    "start": "2420425",
    "end": "2426119"
  },
  {
    "text": "We know that 1/n-- ",
    "start": "2426120",
    "end": "2432259"
  },
  {
    "text": "you see where I made\nthe mistake here. So 1 over square root n\ntimes y hat t minus y vec--",
    "start": "2432260",
    "end": "2440674"
  },
  {
    "start": "2440675",
    "end": "2446080"
  },
  {
    "text": "0. This is o of 1. Because every entry is\non the order of o of 1. So you have n entries.",
    "start": "2446080",
    "end": "2452470"
  },
  {
    "text": "And then in that way,\nyou can show that-- this implies that\nwe actually have",
    "start": "2452470",
    "end": "2460510"
  },
  {
    "text": "an exponential decay of error. But actually, even\nwe don't care about that, you still have for\nevery time t, you have this.",
    "start": "2460510",
    "end": "2466869"
  },
  {
    "text": "And if for every\ntime t you have this, then it means that 1 square root\nn phi sigma t minus sigma hat",
    "start": "2466870",
    "end": "2480670"
  },
  {
    "text": "2 norm is less than O of 1. Because theta hat is\nthe ground truth, right?",
    "start": "2480670",
    "end": "2488470"
  },
  {
    "text": "So this is because phi\ntheta hat is equal to y vec.",
    "start": "2488470",
    "end": "2495536"
  },
  {
    "text": "Theta hat is the\nconstruct-- the one that we constructed the last lecture. And then this means that\ntheta t minus theta hat 2 norm",
    "start": "2495537",
    "end": "2503640"
  },
  {
    "text": "is less than square root n over\nsigma, which is exactly right. You are saying that your\niterate is not very far away",
    "start": "2503640",
    "end": "2514890"
  },
  {
    "text": "from the target theta hat. And then you also know that your\ntarget theta hat is also not",
    "start": "2514890",
    "end": "2520290"
  },
  {
    "text": "very far away from-- so we also know that the\ntarget, this is also less than--",
    "start": "2520290",
    "end": "2530270"
  },
  {
    "text": "I guess there's a big O here. Big O of square\nroot n over sigma.",
    "start": "2530270",
    "end": "2535670"
  },
  {
    "text": "Because this is what\nwe did last time. And then by triangle inequality,\nwe got theta t minus theta 0",
    "start": "2535670",
    "end": "2549670"
  },
  {
    "text": "is less than theta\nt minus theta hat.",
    "start": "2549670",
    "end": "2556534"
  },
  {
    "text": "Right? So theta t minus theta hat\nand theta hat minus theta 0 2 norm which is less than\nO square root n over sigma.",
    "start": "2556534",
    "end": "2565220"
  },
  {
    "text": "And this is less than\nsigma over 4 beta if beta over sigma\nsquared is less than--",
    "start": "2565220",
    "end": "2571057"
  },
  {
    "text": "much, much less than what\n1 over square root of n.  So this is how you inductively\nmaintain the distance between,",
    "start": "2571057",
    "end": "2580310"
  },
  {
    "text": "let's say, how do\nyou inductively show that theta t is not\nvery far away from theta 0.",
    "start": "2580310",
    "end": "2587511"
  },
  {
    "text": " Yeah. The step sounds a\nlittle bit complicated.",
    "start": "2587511",
    "end": "2593730"
  },
  {
    "text": "But actually, the\nintuition is very simple. There are probably many\nways to prove this.",
    "start": "2593730",
    "end": "2598830"
  },
  {
    "text": "I just presented one way. So there's already a\nglobal minimum there. So there shouldn't be any\nways for you to leave.",
    "start": "2598830",
    "end": "2607170"
  },
  {
    "text": "And what you do is basically you\nsay that you have a theta hat here. You have theta 0. You know that these two are--",
    "start": "2607170",
    "end": "2613770"
  },
  {
    "text": "the distance between these two\nis of square root n over sigma.",
    "start": "2613770",
    "end": "2620210"
  },
  {
    "text": "And you are optimizing. And in some sense, theta\nhat is your target, right? Because theta hat\nhas the best fitting.",
    "start": "2620210",
    "end": "2626750"
  },
  {
    "text": "So you are somewhat moving\neven closer to theta hat. So why not you should have\neven bigger distance eventually",
    "start": "2626750",
    "end": "2632840"
  },
  {
    "text": "afterwards, right? So that's why this is working.",
    "start": "2632840",
    "end": "2639668"
  },
  {
    "text": "So if you look at the iterate,\nI think you are somewhat moving to theta hat. So OK.",
    "start": "2639668",
    "end": "2645470"
  },
  {
    "text": "So enough. Out of all of this, so\nthen we got this equality.",
    "start": "2645470",
    "end": "2653060"
  },
  {
    "text": "And with this equality,\nwe got that y vec t plus 1 minus y vec--",
    "start": "2653060",
    "end": "2660240"
  },
  {
    "text": "y hat t plus 1 minus\ny vec is less than-- t minus y vec, 2 norm, minus\neta times 3/4 sigma squared.",
    "start": "2660240",
    "end": "2671560"
  },
  {
    "text": "And then you can do\na recursion to get exponential decay of error. ",
    "start": "2671560",
    "end": "2683309"
  },
  {
    "text": "OK. Any questions? I think I made a small typo\nsomewhere in the assumption",
    "start": "2683310",
    "end": "2690230"
  },
  {
    "text": "of the theorem. I need to fix that. I think my assumption should\nbe that this is less than c0",
    "start": "2690230",
    "end": "2697940"
  },
  {
    "text": "over square root n. But it doesn't really\nmatter, because you can make beta I think-- if you\nchange as we see last time.",
    "start": "2697940",
    "end": "2705790"
  },
  {
    "text": "Or you can make either them\nwith bigger or the alpha bigger. You can make beta over sigma\nsquared arbitrarily small too.",
    "start": "2705790",
    "end": "2713087"
  },
  {
    "text": "So it doesn't really\nmatter very much.  Any questions?",
    "start": "2713087",
    "end": "2718845"
  },
  {
    "text": "[INAUDIBLE] Sure. [INAUDIBLE]",
    "start": "2718845",
    "end": "2730587"
  },
  {
    "text": "Yup. [INAUDIBLE]",
    "start": "2730587",
    "end": "2751660"
  },
  {
    "text": "I guess, I think,\nthere's one version too. Let me rephrase your\nquestion and let me know",
    "start": "2751660",
    "end": "2757750"
  },
  {
    "text": "if it's not what you asked. So I guess one\nquestion you could ask is that whether you really\nrely on the exponential decay",
    "start": "2757750",
    "end": "2766360"
  },
  {
    "text": "for the kernel case to\nhave this relationship between neural network\nand the kernel.",
    "start": "2766360",
    "end": "2771400"
  },
  {
    "text": "I think the answer\nto that is no. So the second type of\napproach that I somewhat",
    "start": "2771400",
    "end": "2776710"
  },
  {
    "text": "outlined last time but\ndidn't really go into detail, that approach didn't\nrequire that you have",
    "start": "2776710",
    "end": "2783340"
  },
  {
    "text": "exponential decay of error. So in that case, both the\nkernel and neural network, you can only show them to have\nsome polynomial speed of decay,",
    "start": "2783340",
    "end": "2792370"
  },
  {
    "text": "like the error is\npolynomial in t. So you can still make\nthis relationship.",
    "start": "2792370",
    "end": "2797799"
  },
  {
    "text": "So exponential decay\nis not that important. But I think this is\nactually something",
    "start": "2797800",
    "end": "2803890"
  },
  {
    "text": "that people realize after\nthe first few papers. At the very beginning,\nthe very first paper",
    "start": "2803890",
    "end": "2809589"
  },
  {
    "text": "using this exponential\nthing, and people thought that because you have\nexponential decay of errors so fast, that's why you don't\nleave this neighborhood.",
    "start": "2809590",
    "end": "2816910"
  },
  {
    "text": "But I think you\ncan do something so that even without\nexponential decay you still don't leave\nthe neighborhood.",
    "start": "2816910",
    "end": "2822340"
  },
  {
    "text": "Because whether you\nleave the neighborhood, it probably depends most on\nwhether in the neighborhood there is a global\nminimum, right?",
    "start": "2822340",
    "end": "2829658"
  },
  {
    "text": "If there is a global\nminimum in the neighborhood, but somehow you cannot converge\nto it exponentially fast,",
    "start": "2829658",
    "end": "2834887"
  },
  {
    "text": "that's still probably fine\nas long as you converge to it eventually. All right. So I'm not sure whether\nthat's what you asked.",
    "start": "2834887",
    "end": "2843710"
  },
  {
    "text": "[INAUDIBLE] OK. ",
    "start": "2843710",
    "end": "2859350"
  },
  {
    "text": "Right, right. You do want to say-- and also you want\nto characterize",
    "start": "2859350",
    "end": "2865075"
  },
  {
    "text": "the neural networks, right? So if they don't have\nthe same property, right, and you somehow can\noptimize-- analyze",
    "start": "2865075",
    "end": "2870802"
  },
  {
    "text": "the optimization of the\nneural networks, that's fine. ",
    "start": "2870802",
    "end": "2876400"
  },
  {
    "text": "But the relationship\nis something to help us to bridge the\ngap between what we knew",
    "start": "2876400",
    "end": "2882668"
  },
  {
    "text": "and which we don't know, right? So the neural network is\nsomething we didn't know. But the kernel one\nis something we knew.",
    "start": "2882668",
    "end": "2890428"
  },
  {
    "text": "And if they are\nsimilar, then you can hope to analyze the\nneural network, right?",
    "start": "2890428",
    "end": "2896000"
  },
  {
    "text": "Yeah. So I think that's\nwhy we show they are doing something similar. ",
    "start": "2896000",
    "end": "2904069"
  },
  {
    "text": "OK. All right.  I think I have a\nlittle bit more things",
    "start": "2904070",
    "end": "2911150"
  },
  {
    "text": "to add about the\nneural tangent kernel. I guess we've discussed\nthis many times.",
    "start": "2911150",
    "end": "2916670"
  },
  {
    "text": "The limitation of\nneural tangent kernel is that you only, at most, do\nas well as the kernel method.",
    "start": "2916670",
    "end": "2923250"
  },
  {
    "text": "Right? So they are kind of-- so basically the question is how\nwell a kernel method can work?",
    "start": "2923250",
    "end": "2930740"
  },
  {
    "start": "2930740",
    "end": "2935855"
  },
  {
    "text": "All right. So are we really characterizing\nthe power of deep learning? If deep learning is only doing\nas well as kernels, is it",
    "start": "2935855",
    "end": "2943010"
  },
  {
    "text": "really, is it good or bad? All right. And the answer, I think,\nis that at least I think most people believe in this--",
    "start": "2943010",
    "end": "2949430"
  },
  {
    "text": "the answer is that\nneural network can do much better\nthings than kernels.",
    "start": "2949430",
    "end": "2955700"
  },
  {
    "text": "And this characterization of\nthe neural network as kernels is not characterizing the\ntrue power of neural network.",
    "start": "2955700",
    "end": "2965360"
  },
  {
    "text": "And you can try to say\nthis in various ways.",
    "start": "2965360",
    "end": "2970460"
  },
  {
    "text": "So there are a lot of papers\nthat tries to do this-- so beyond the NTK approach. I guess, if you search beyond\nNTK or beyond lazy training,",
    "start": "2970460",
    "end": "2977780"
  },
  {
    "text": "you'll see a bunch of papers,\nincluding some of my papers. So we try to analyze deep\nlearning in different regimes.",
    "start": "2977780",
    "end": "2987800"
  },
  {
    "text": "But there is a simple\nseparation if you don't care about the\noptimization performance.",
    "start": "2987800",
    "end": "2993500"
  },
  {
    "text": "If you just care about the power\nof the regularization and the--",
    "start": "2993500",
    "end": "2999126"
  },
  {
    "text": " if you only care about\nthe statistical aspect, you can easily show\nthat neural network can",
    "start": "2999126",
    "end": "3006580"
  },
  {
    "text": "do better things than kernels. And this is an example.",
    "start": "3006580",
    "end": "3014312"
  },
  {
    "text": "So the example is\nsomething like this. ",
    "start": "3014312",
    "end": "3027330"
  },
  {
    "text": "So I guess this is an example,\nwhere NTK or any kernel method",
    "start": "3027330",
    "end": "3043450"
  },
  {
    "text": "is statistically limited. ",
    "start": "3043450",
    "end": "3054109"
  },
  {
    "text": "And in some sense,\nthe intuition is that the limitation comes from\nthat the kernel or the features",
    "start": "3054110",
    "end": "3063840"
  },
  {
    "text": "are fixed in the NTK approach. You don't have any\nadaptability to the data.",
    "start": "3063840",
    "end": "3069920"
  },
  {
    "text": "So your data probably\nwants to use some features. But you are using a fixed\nfeature for the data.",
    "start": "3069920",
    "end": "3077109"
  },
  {
    "text": "And this is a simple\ncase, where you have to have such\na concrete example. So suppose you\nconsider this case",
    "start": "3077110",
    "end": "3083410"
  },
  {
    "text": "where x is in let's say r d\nAnd y is in plus 1 minus 1.",
    "start": "3083410",
    "end": "3091240"
  },
  {
    "text": "And let's say each\nof the xi's are just a uniform like an iid Gaussian.",
    "start": "3091240",
    "end": "3098049"
  },
  {
    "text": "So xi is the i-th index.",
    "start": "3098050",
    "end": "3103710"
  },
  {
    "text": "The superscript is\nfor the examples. And the subscript is\nfor the dimension.",
    "start": "3103710",
    "end": "3110710"
  },
  {
    "text": "And let's say y\nis equal to x1 x2 So we have a very\nsimple function, which is just learning the\nproduct of the first two",
    "start": "3110710",
    "end": "3117420"
  },
  {
    "text": "dimension of the data. So if you draw this, suppose\nthis is x1, this is x2.",
    "start": "3117420",
    "end": "3124020"
  },
  {
    "text": "Then you just have four\ndifferent combinations. And this is a positive example.",
    "start": "3124020",
    "end": "3129320"
  },
  {
    "text": "This is an active\nexample-- positive example. And this is negative. And this is negative. So so this is not\nlinearly separable.",
    "start": "3129320",
    "end": "3135830"
  },
  {
    "text": "Because you have\nthese four points that are positioned like this. So you have to use a nonlinear\nmodel, or a linear model",
    "start": "3135830",
    "end": "3144260"
  },
  {
    "text": "on some feature space. All right. So if you use neural networks-- ",
    "start": "3144260",
    "end": "3153190"
  },
  {
    "text": "and suppose you regularize. Suppose you regularize\nthe l2 norm.",
    "start": "3153190",
    "end": "3158859"
  },
  {
    "text": " And this is equivalent\nto regularize this norm",
    "start": "3158860",
    "end": "3170090"
  },
  {
    "text": "that we discussed-- this norm c of theta, which is\nsomething like sum of ai wi.",
    "start": "3170090",
    "end": "3179220"
  },
  {
    "text": "I'm not sure whether\nyou still do remember this, when we have this\nneural network which is the y is equals to something\nlike a sum of ai sigma wi",
    "start": "3179220",
    "end": "3187750"
  },
  {
    "text": "transposed x. And then you can define\nthis complexity measure which is kind of the\npath norm, right?",
    "start": "3187750",
    "end": "3193599"
  },
  {
    "text": "And we have shown that\nregularizing l2 norm of the outer parameters is\nthe same as regularizing",
    "start": "3193600",
    "end": "3198849"
  },
  {
    "text": "this somewhat complex-- and which gives the\nactually the complexity,",
    "start": "3198850",
    "end": "3204355"
  },
  {
    "text": "the generalization\nguarantees right. So we have discussed\nthis in some sense.",
    "start": "3204355",
    "end": "3209410"
  },
  {
    "text": "And suppose you use the neural\nnetworks, then what you'll find is the best solution.",
    "start": "3209410",
    "end": "3215275"
  },
  {
    "text": " By the best solution, I\nmean the minimum norms--",
    "start": "3215275",
    "end": "3220340"
  },
  {
    "text": "so which in the\nminimum norm solutions",
    "start": "3220340",
    "end": "3228040"
  },
  {
    "text": "is a sparse one, right? It uses a sparse\ncombination of neurons.",
    "start": "3228040",
    "end": "3240778"
  },
  {
    "text": "So basically, the best solution\nactually you can in this case is you can exactly compute\nwhat's the best solution. I'm not going to prove it.",
    "start": "3240778",
    "end": "3246519"
  },
  {
    "text": "But I think it's something\nrelatively believable. So the best solution,\nfirst of all,",
    "start": "3246520",
    "end": "3253020"
  },
  {
    "text": "it doesn't really use\nany other dimensions. That seems to be\nbelievable, right? Why you want to use\nany other dimensions",
    "start": "3253020",
    "end": "3258450"
  },
  {
    "text": "if your function is only a\nfunction of the first two dimensions. And you only have\nto use something about the first two dimensions.",
    "start": "3258450",
    "end": "3264692"
  },
  {
    "text": "You only need the\nfollowing four neurons.",
    "start": "3264692",
    "end": "3270400"
  },
  {
    "text": "So one neuron\ncomputes x1 plus x2. And another neuron\ncompute minus x1 minus x2.",
    "start": "3270400",
    "end": "3280170"
  },
  {
    "text": "And we need another\nneuron compute-- that computes ReLU of x1\nminus x2 and another one",
    "start": "3280170",
    "end": "3285530"
  },
  {
    "text": "that computes ReLU\nof x2 minus x1. So I claim that this is\nactually equals to the function.",
    "start": "3285530",
    "end": "3293529"
  },
  {
    "text": "And if you want to verify\nthat, I can briefly do that. So the ReLU of x the ReLU\nof t plus ReLU minus t",
    "start": "3293530",
    "end": "3303720"
  },
  {
    "text": "is equals to\nabsolute value of t. So this is equal to x1\nplus x2 minus x1 minus x2.",
    "start": "3303720",
    "end": "3311474"
  },
  {
    "text": " And now, I claim that this is\nactually equals to x1 times x2,",
    "start": "3311475",
    "end": "3319522"
  },
  {
    "text": "where x1 x2 are both binary. And how do you see this? I guess, the only way I\ncan see this is just to try",
    "start": "3319522",
    "end": "3327460"
  },
  {
    "text": "all the four combinations. If x1 x2 have the\nsame sign, then this",
    "start": "3327460",
    "end": "3332470"
  },
  {
    "text": "will become 0 and\nthis will become 1-- if x1 or x2 are either\nboth 1 or both minus 1.",
    "start": "3332470",
    "end": "3339310"
  },
  {
    "text": "And that's is the case in x\nminus 2, the product is one. And if x1 x2 have different\nsign, then the first term is 0",
    "start": "3339310",
    "end": "3346300"
  },
  {
    "text": "and the second term becomes 1. Wait, why I'm\nhaving a half here?",
    "start": "3346300",
    "end": "3353650"
  },
  {
    "text": "Oh, yes. And the second term becomes 2. So you multiply 1/2\nand you get minus 1.",
    "start": "3353650",
    "end": "3358660"
  },
  {
    "text": "Right. So good. So basically if you use neural\nnetwork and you regularize,",
    "start": "3358660",
    "end": "3364410"
  },
  {
    "text": "you can show that this is\nthe solution finds, which is a very sparse combination\nof a small number of features.",
    "start": "3364410",
    "end": "3371279"
  },
  {
    "text": "In some sense, when\nyou use regularization, you find these four features\nand you do a linear combination",
    "start": "3371280",
    "end": "3376380"
  },
  {
    "text": "on them. So these four features are the\nright features for this task.",
    "start": "3376380",
    "end": "3382029"
  },
  {
    "text": "However, if you use NTK-- suppose you use NTK. ",
    "start": "3382030",
    "end": "3392470"
  },
  {
    "text": "Suppose you're using\nNTK, then what you do is you just don't\nlearn any features. You just try to do a--",
    "start": "3392470",
    "end": "3402020"
  },
  {
    "text": "you do a dense combination\nof your existing features. All right. So in some sense, what\nyou do is you say--",
    "start": "3402020",
    "end": "3411760"
  },
  {
    "text": "I guess, how do I say\nthis in the best way? ",
    "start": "3411760",
    "end": "3417860"
  },
  {
    "text": "So basically when you do\nNTK, what you will earn is something like-- ",
    "start": "3417860",
    "end": "3423990"
  },
  {
    "text": "oh sorry. Why am I not-- ",
    "start": "3423990",
    "end": "3431930"
  },
  {
    "text": "I guess I can still see them. But I think roughly\nthe intuition is that your y will be--",
    "start": "3431930",
    "end": "3437125"
  },
  {
    "text": "your prediction\nwill be something like a sum of ai sigma wi\ntranspose x or maybe something",
    "start": "3437125",
    "end": "3445640"
  },
  {
    "text": "like this-- sum of\nai times phi of x.",
    "start": "3445640",
    "end": "3453056"
  },
  {
    "text": "Right? And there are a\nbunch of features, and each feature is phi i.",
    "start": "3453056",
    "end": "3458089"
  },
  {
    "text": "And its features use\nall the dimensions.",
    "start": "3458090",
    "end": "3465850"
  },
  {
    "text": "And this depends. Of course, exactly\nwhat the features are, it depends on what\nkernels you are using. If you use NTK\nkernel, you're going",
    "start": "3465850",
    "end": "3472060"
  },
  {
    "text": "to get some feature vector. If you use a random kernel,\nyou get some other feature-- ",
    "start": "3472060",
    "end": "3479327"
  },
  {
    "text": "some other features. But whatever\nfeatures you do, this is always the function\nof all the theta.",
    "start": "3479327",
    "end": "3485055"
  },
  {
    "text": " You cannot specialize to a\nspecial subset of features.",
    "start": "3485055",
    "end": "3492580"
  },
  {
    "text": "And also because you are\ndoing a regularization on the minimum l2 norm solution\nfor the coefficient in front",
    "start": "3492580",
    "end": "3498789"
  },
  {
    "text": "of the feature, you don't\nprefer any sparse solution. All right. So recall that--",
    "start": "3498790",
    "end": "3504490"
  },
  {
    "start": "3504490",
    "end": "3510410"
  },
  {
    "text": "Yeah, sorry. I'm using the wrong\nversion of notes. So I guess I have to\nimprovise a little bit.",
    "start": "3510410",
    "end": "3517580"
  },
  {
    "text": "So if you look at\nNTK, what you do is you try to minimize\nthe l2 norm of this vector",
    "start": "3517580",
    "end": "3524300"
  },
  {
    "text": "a such that the data, sum of\nai phi i of x is equal to y.",
    "start": "3524300",
    "end": "3530720"
  },
  {
    "text": "Maybe you also have j. Right? And if you do the\nneural network,",
    "start": "3530720",
    "end": "3539180"
  },
  {
    "text": "I think we have claimed that\nneural network is the same as l1 SVM in a kernel space.",
    "start": "3539180",
    "end": "3544700"
  },
  {
    "text": "So then the\ncorresponding thing would be minimized to l1 norm of\na such that the sum of ai",
    "start": "3544700",
    "end": "3552500"
  },
  {
    "text": "phi i x t j t is equals to y jt. So in some sense, when\nyou do the neural networks",
    "start": "3552500",
    "end": "3558890"
  },
  {
    "text": "and you have a lot\nof features, you're choosing this\nsubset of features-- a sparse subset of features.",
    "start": "3558890",
    "end": "3564440"
  },
  {
    "text": "And when we do NTK, you\nare minimizing the l2 norm, and that never gives\nyou sparse combinations.",
    "start": "3564440",
    "end": "3570137"
  },
  {
    "text": "It's actually a preferred\ndense combination. It's the reverse direction. You want a smooth combinations\nof the existing features",
    "start": "3570137",
    "end": "3576349"
  },
  {
    "text": "as possible. So that's why you have\nto pay more samples too if you use the NTK,\nbecause you are using",
    "start": "3576350",
    "end": "3583430"
  },
  {
    "text": "kind of suboptimal features. And this can be\nproved in this case. Like you can prove that this is\na theorem, where you can prove",
    "start": "3583430",
    "end": "3594000"
  },
  {
    "text": "that kernel method\nwith NTK kernel",
    "start": "3594000",
    "end": "3602690"
  },
  {
    "text": "requires n to be omega\nof d squared samples",
    "start": "3602690",
    "end": "3610910"
  },
  {
    "text": "to learn a problem\nwith error less than 1.",
    "start": "3610910",
    "end": "3624670"
  },
  {
    "text": " And in contrast,\nregularized network",
    "start": "3624670",
    "end": "3639109"
  },
  {
    "text": "only need-- neural net only need\nn is equals to O of d samples. ",
    "start": "3639110",
    "end": "3649760"
  },
  {
    "text": "Any questions about this? I think this part is a little\nbit kind of like hand-wavy because I didn't want to\ngo into all the details.",
    "start": "3649760",
    "end": "3656312"
  },
  {
    "text": "And also, this\ndepends a little bit on what we discussed\nin the past, right? So what I want is that we have\nsome connections onto the--",
    "start": "3656312",
    "end": "3664702"
  },
  {
    "text": "a connection between\nl1 SVM networks. Any questions?",
    "start": "3664702",
    "end": "3670190"
  },
  {
    "start": "3670190",
    "end": "3679319"
  },
  {
    "text": "So I guess maybe just to\nwrap up this once again-- so basically, if you do neural\nnetwork with regularization,",
    "start": "3679320",
    "end": "3688690"
  },
  {
    "text": "then we have shown that\nthis is equivalent to l1",
    "start": "3688690",
    "end": "3694579"
  },
  {
    "text": "SVM in a feature space. We are trying to find\nthe sparsest combination of features that face our data.",
    "start": "3694580",
    "end": "3700400"
  },
  {
    "text": "Right? And in this particular\nexample, this can sound pretty intuitive that\nfinding a sparse combination",
    "start": "3700400",
    "end": "3706720"
  },
  {
    "text": "is useful, because not all the\nfeatures are equally useful. So these features we designed\nare much better features",
    "start": "3706720",
    "end": "3713710"
  },
  {
    "text": "than a random feature. And that's why neural\nnetwork with regularization",
    "start": "3713710",
    "end": "3720730"
  },
  {
    "text": "could have good\nsample complexity. And on the other\nhand, when we do NTK kernel on most\nof the other kernels,",
    "start": "3720730",
    "end": "3727780"
  },
  {
    "text": "so you are not trying to\nfind a sparse combination of the features. You are trying to find a dense\ncombination of the features,",
    "start": "3727780",
    "end": "3733958"
  },
  {
    "text": "because you are doing l2-- you are finding the\nminimal l2 norm solution.",
    "start": "3733958",
    "end": "3739697"
  },
  {
    "text": "And each of the\nfeatures is a function of all the data-- all the\ncoordinates of the data point.",
    "start": "3739697",
    "end": "3745390"
  },
  {
    "text": "So the features are not\nthat useful in some sense. There are a lot of\nnoise on features. You have to rely on\naveraging all the noise",
    "start": "3745390",
    "end": "3752770"
  },
  {
    "text": "over multiple features\nto learn something. You can still learn\nsomething, but it's going to be less efficient.",
    "start": "3752770",
    "end": "3757810"
  },
  {
    "text": " Right. I think that's the summary.",
    "start": "3757810",
    "end": "3763125"
  },
  {
    "start": "3763125",
    "end": "3778310"
  },
  {
    "text": "OK. So if there's no\nany other questions,",
    "start": "3778310",
    "end": "3788400"
  },
  {
    "text": "I'm going to move on to\nthe next topic, which is about implicit\nregularization effect.",
    "start": "3788400",
    "end": "3798165"
  },
  {
    "start": "3798165",
    "end": "3803170"
  },
  {
    "text": "I'm not sure whether\nyou still remember what we discussed in the mystery\nof the deep learning theory",
    "start": "3803170",
    "end": "3808349"
  },
  {
    "text": "section. So I'm going to briefly repeat\nkind of the high-level goal here. So the observation we had about\nthe empirical deep learning",
    "start": "3808350",
    "end": "3816849"
  },
  {
    "text": "is that we found that there\nare multiple global minimum",
    "start": "3816850",
    "end": "3826050"
  },
  {
    "text": "of training loss exist\nand the optimizers has",
    "start": "3826050",
    "end": "3835940"
  },
  {
    "text": "some preference-- have some implicit preferences.",
    "start": "3835940",
    "end": "3842450"
  },
  {
    "text": " And we have claimed that almost\nevery aspect of the optimizers",
    "start": "3842450",
    "end": "3849210"
  },
  {
    "text": "have some preferences. For example, if you use the\nparticular initialization",
    "start": "3849210",
    "end": "3854869"
  },
  {
    "text": "that enables NTK, then you\nhave the NTK preferences. You are learning\nthe NTK solution.",
    "start": "3854870",
    "end": "3860567"
  },
  {
    "text": "And if you use some\nother initialization you have some other preferences,\nand we have kind of concluded that if\nthe NTK solution is",
    "start": "3860567",
    "end": "3867530"
  },
  {
    "text": "the wrong preference-- so like you don't do much\nbeyond the kernel method, you actually do exactly\nthe same as kernel method--",
    "start": "3867530",
    "end": "3875000"
  },
  {
    "text": "so basically that\nmeans you are finding the wrong global minimum that\ndoesn't necessarily generalize as well as other global minimum.",
    "start": "3875000",
    "end": "3882170"
  },
  {
    "text": "So from now on,\nwe're going to try to look at other global\nminimum of this objective",
    "start": "3882170",
    "end": "3887510"
  },
  {
    "text": "and see what other\noptimizers prefer. So if you use\ndifferent optimizers, you may prefer a solution\nthat is different from the NTK",
    "start": "3887510",
    "end": "3894495"
  },
  {
    "text": "solution. [INAUDIBLE] NT-- oh yeah. What does it mean--",
    "start": "3894495",
    "end": "3899810"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "3899810",
    "end": "3906260"
  },
  {
    "text": "Right. So why I call it NTK\ninitialization, right? ",
    "start": "3906260",
    "end": "3913202"
  },
  {
    "text": "So the NTK\ninitialization basically mean the initialization\nunder which you can prove the NTK result.\nSo maybe specifically, I",
    "start": "3913202",
    "end": "3920480"
  },
  {
    "text": "think last time we have\ntwo examples, right? So one example is-- ",
    "start": "3920480",
    "end": "3928380"
  },
  {
    "text": "maybe this example is-- [INAUDIBLE]",
    "start": "3928380",
    "end": "3937780"
  },
  {
    "text": "Right, right, right. [INAUDIBLE] Right, right, right. So for example, I think, just\nwhen have to weight things.",
    "start": "3937780",
    "end": "3944220"
  },
  {
    "text": "So like where there is-- ",
    "start": "3944220",
    "end": "3950210"
  },
  {
    "text": "something like here,\nI think we have this overparameterized model. We have this width. And we initialize with\nai to be plus 1 minus 1.",
    "start": "3950210",
    "end": "3956360"
  },
  {
    "text": "And wi to be this\nspherical Gaussian. And you can, for example,\ninitialize something",
    "start": "3956360",
    "end": "3961820"
  },
  {
    "text": "with something much smaller. Right? And actually, you\nshould if you really do the experiments for this\nexact parameterization.",
    "start": "3961820",
    "end": "3967880"
  },
  {
    "text": "You should initialize\nboth either ai or wi's on some like a\nsquare root n factor smaller.",
    "start": "3967880",
    "end": "3976490"
  },
  {
    "text": "And then you're going\nto see much different empirical results. And actually, we have\ndone this in the-- it's",
    "start": "3976490",
    "end": "3982010"
  },
  {
    "text": "actually in the paper. Many people have done this. It's relatively\nsimple experiments.",
    "start": "3982010",
    "end": "3987170"
  },
  {
    "text": "So here, you can already\nsay that initialization is the culprit, right? So for the other\ncase, I think when",
    "start": "3987170",
    "end": "3992660"
  },
  {
    "text": "you change the parameterization\nto see the NTK regime, I think you can say that\nthe parameterization is",
    "start": "3992660",
    "end": "3997670"
  },
  {
    "text": "the culprit. And also even in this case-- even this case is supposed to\ninitialize the same as the NTK.",
    "start": "3997670",
    "end": "4003670"
  },
  {
    "text": "Suppose you do stochastic\nwithin this set. You have sufficiently\nlarge stochastic-- it doesn't have\nto be super large.",
    "start": "4003670",
    "end": "4010420"
  },
  {
    "text": "But a little bit\nlarger than zero, then you will leave\nthat initialization. And you're going to convert\nto some other places.",
    "start": "4010420",
    "end": "4017329"
  },
  {
    "text": "So that's another way\nto leave the NTK regime. All right. So we are going to\nsometimes discuss",
    "start": "4017330",
    "end": "4023890"
  },
  {
    "text": "this other kind of ways. Basically, what we\nwill discuss next are either using [INAUDIBLE] to\nleave NTK or use stochasticity.",
    "start": "4023890",
    "end": "4033550"
  },
  {
    "text": "And what else? You can also use\nthe learning rate.",
    "start": "4033550",
    "end": "4041543"
  },
  {
    "text": "Learning rate is kind of almost\nthe same as stochasticity, because if you have larger\nthan the rate, in some sense--",
    "start": "4041543",
    "end": "4046780"
  },
  {
    "text": "and you have SGD then your\nstochasticity is bigger. Right. ",
    "start": "4046780",
    "end": "4055230"
  },
  {
    "text": "Right. So the first thing\nI'm going to do is the effective from the\nimplicit regularization",
    "start": "4055230",
    "end": "4060760"
  },
  {
    "text": "effect from initialization. So first is this effect\nof initialization.",
    "start": "4060760",
    "end": "4067339"
  },
  {
    "text": " And you will see that in\ncertain cases, you can leave--",
    "start": "4067340",
    "end": "4074105"
  },
  {
    "text": " we don't necessarily really\ncare about leaving NTK, we really care about having\na better generalization rate.",
    "start": "4074105",
    "end": "4081760"
  },
  {
    "text": "So that means you\nhave to leave NTK, but you have to do\nprobably more than that to get better generalization.",
    "start": "4081760",
    "end": "4089740"
  },
  {
    "text": "So this is what we're\ngoing to do the next 15 minutes of this lecture,\nand the next lecture,",
    "start": "4089740",
    "end": "4095080"
  },
  {
    "text": "the effect of regularization. And I'm going to start\nwith a simple case",
    "start": "4095080",
    "end": "4101140"
  },
  {
    "text": "where you have overparameterized\nlinear regression case.",
    "start": "4101140",
    "end": "4112149"
  },
  {
    "text": "You need overparameterization\nbecause you need, especially if you consider\nlinear models, right?",
    "start": "4112149",
    "end": "4119640"
  },
  {
    "text": "One of the important\nthing is that you have to have multiple\nglobal minimum otherwise there's no so-called, implicit\nregularization effect, right?",
    "start": "4119640",
    "end": "4126000"
  },
  {
    "text": "Because optimizers have to\nconverge to a global minimum. You have to have\nmultiple global minimums so that the optimizers\ncan have a choice",
    "start": "4126000",
    "end": "4131997"
  },
  {
    "text": "to choose between them. So that's why we need\noverparameterized regression,",
    "start": "4131997",
    "end": "4137200"
  },
  {
    "text": "so that you have\nmultiple global minimum. Actually, there is an\ninfinite above global minimum where you have\noverparameterization.",
    "start": "4137200",
    "end": "4144060"
  },
  {
    "text": "And we will see\nthat in this case small initialization\nprefers a low-norm solution.",
    "start": "4144060",
    "end": "4155238"
  },
  {
    "text": " And this is also the case\nwhen we in the next lecture",
    "start": "4155239",
    "end": "4162026"
  },
  {
    "text": "we're going to go\nbeyond linear model. And the high level\nconclusion is the same. If you use small\ninitialization then",
    "start": "4162026",
    "end": "4167870"
  },
  {
    "text": "you prefer lower-norm solution. And today we're only going to in\nthe next 15 minutes we're only",
    "start": "4167870",
    "end": "4174380"
  },
  {
    "text": "going to do the linear models. And this is actually\nnot that hard. So let's set up first.",
    "start": "4174380",
    "end": "4182719"
  },
  {
    "text": "So this is a standard\nlinear regression case. ",
    "start": "4182720",
    "end": "4190899"
  },
  {
    "text": "For this lecture, I'm\nusing the lower subscript for the number of examples\njust because you should really",
    "start": "4190899",
    "end": "4198850"
  },
  {
    "text": "look up any linear\nregression book, then they use\nsubscript for examples.",
    "start": "4198850",
    "end": "4205210"
  },
  {
    "text": "And here we don't do this. So each of these\nxi's are examples.",
    "start": "4205210",
    "end": "4212620"
  },
  {
    "text": "Example i, and you put\nthem into a matrix. And let's assume x is full\nrank So it means rank n.",
    "start": "4212620",
    "end": "4222909"
  },
  {
    "text": "And let's also assume n is\nmuch smaller than d, OK?",
    "start": "4222910",
    "end": "4228760"
  },
  {
    "text": "And we have a parameter beta.",
    "start": "4228760",
    "end": "4234420"
  },
  {
    "text": "So you get a loss function y\nvec minus x beta 2 non square.",
    "start": "4234420",
    "end": "4244199"
  },
  {
    "text": "As you have 1/2 here\njust for convenience. This is my empirical loss, OK?",
    "start": "4244200",
    "end": "4251039"
  },
  {
    "text": "So this is standard\nlinear regression. And indeed L hat beta has\ninfinite number of global min.",
    "start": "4251040",
    "end": "4268280"
  },
  {
    "text": "And you actually can\ncharacterize exactly what that global mins are.",
    "start": "4268280",
    "end": "4273380"
  },
  {
    "text": "And all the global\nmins with loss 0. So what are the global mins?",
    "start": "4273380",
    "end": "4278659"
  },
  {
    "text": "So beta is equals to,\nsupposed to take beta to be x pseudoinverse times\ny vec plus some vector zeta",
    "start": "4278660",
    "end": "4290809"
  },
  {
    "text": "where zeta is any vector\nthat is orthonormal, also",
    "start": "4290810",
    "end": "4297350"
  },
  {
    "text": "orthogonal to x1 up to\nxn, is a global min. ",
    "start": "4297350",
    "end": "4304830"
  },
  {
    "text": "So as your beta has this\nform, then it's a global min. And these are actually\nall the global min.",
    "start": "4304830",
    "end": "4311820"
  },
  {
    "text": "And actually here, I\nthink last time someone asked about pseudoinverse. Maybe let me have quickly\nsome basic properties",
    "start": "4311820",
    "end": "4319410"
  },
  {
    "text": "of pseudoinverse. I guess my way of thinking\nabout it is probably slightly",
    "start": "4319410",
    "end": "4329040"
  },
  {
    "text": "different from Wikipedia. So the way I always think about\npseudoinverse is the following.",
    "start": "4329040",
    "end": "4334630"
  },
  {
    "text": "So I always think\nabout it when in SVD. Because with the SVD it\ncan verify everything",
    "start": "4334630",
    "end": "4342090"
  },
  {
    "text": "so that I don't have\nto remember them. So suppose you have a matrix\nx in dimension n by d.",
    "start": "4342090",
    "end": "4348870"
  },
  {
    "text": "And suppose x is of\nrank r, of course,",
    "start": "4348870",
    "end": "4354340"
  },
  {
    "text": "r has to be less than\neither n, both less than n and less than d. So the way how I remember every\nproperty of the pseudoinverse",
    "start": "4354340",
    "end": "4364022"
  },
  {
    "text": "is the following. So I consider SVD of x,\nwhich is u sigma v-transpose.",
    "start": "4364022",
    "end": "4370030"
  },
  {
    "text": "And sigma is of\ndimension r by r. Let's say you ignore all\nthe non-zero entries.",
    "start": "4370030",
    "end": "4376800"
  },
  {
    "text": "And u is of dimension n by r. And v is of dimension d by r.",
    "start": "4376800",
    "end": "4385600"
  },
  {
    "text": "So then you know that\nthe column-span of u",
    "start": "4385600",
    "end": "4391010"
  },
  {
    "text": "is the same as the\ncolumn-span of x.",
    "start": "4391010",
    "end": "4396070"
  },
  {
    "text": "And the column-span of v because\nthere's a transpose here,",
    "start": "4396070",
    "end": "4402230"
  },
  {
    "text": "right? So column-span of v\nis the row-span of x.",
    "start": "4402230",
    "end": "4410930"
  },
  {
    "text": "And also you know\nthat the pseudoinverse in this notation\nyou can think of as",
    "start": "4410930",
    "end": "4417320"
  },
  {
    "text": "defined to be v sigma\ninverse u-transpose.",
    "start": "4417320",
    "end": "4424099"
  },
  {
    "text": "So here sigma is a diagonal\nmatrix with entry sigma",
    "start": "4424100",
    "end": "4429200"
  },
  {
    "text": "1 up to sigma r. And sigma i's are all positive.",
    "start": "4429200",
    "end": "4435680"
  },
  {
    "text": "So then this inverse\nis well defined. And x pseudoinverse is\njust this, v sigma inverse",
    "start": "4435680",
    "end": "4440825"
  },
  {
    "text": "u-transpose. And now if you\nwant to understand what's the property\nof the pseudoinverse, you can verify it yourself.",
    "start": "4440825",
    "end": "4446820"
  },
  {
    "text": "So you know x pseudoinverse\nis going to be what?",
    "start": "4446820",
    "end": "4452000"
  },
  {
    "text": "It's going to be u sigma\nv-transpose v sigma inverse u-transpose.",
    "start": "4452000",
    "end": "4457760"
  },
  {
    "text": "v-transpose v is an entity. So u u-transpose, right?",
    "start": "4457760",
    "end": "4466150"
  },
  {
    "text": "Because v-transpose\nv is an entity. Sigma times sigma\ninverse is an entity. So what is? This is the projection\nto the column-span of x.",
    "start": "4466150",
    "end": "4477616"
  },
  {
    "text": "It's the projection to\nthe column-span of u. And the column-span for u is the\nsame as the column-span of x.",
    "start": "4477617",
    "end": "4483170"
  },
  {
    "text": "And x pseudoinverse\ntimes x, this is equal if you do the same\ncalculation, it's",
    "start": "4483170",
    "end": "4488480"
  },
  {
    "text": "going to be equal to v sigma\ninverse u transpose times u",
    "start": "4488480",
    "end": "4495080"
  },
  {
    "text": "sigma v transpose. Which is v v-transpose,\nwhich is the projection to the row-span of x.",
    "start": "4495080",
    "end": "4503583"
  },
  {
    "text": "And you can also\nsee the dimension matches because this is a\nmatrix of dimension d by d.",
    "start": "4503583",
    "end": "4512780"
  },
  {
    "text": "And the rows of v\nis in dimension d. Sorry, the rows of\nx is in dimension d.",
    "start": "4512780",
    "end": "4519920"
  },
  {
    "text": "The column of v\nis in dimension d. ",
    "start": "4519920",
    "end": "4526445"
  },
  {
    "text": "So in this case\nwhere x is in n by d,",
    "start": "4526445",
    "end": "4535190"
  },
  {
    "text": "and the rank is n then you\nknow that x pseudoinverse",
    "start": "4535190",
    "end": "4541090"
  },
  {
    "text": "is the projection to\nthe column-span of x.",
    "start": "4541090",
    "end": "4546630"
  },
  {
    "text": "And the column-span of\nx now is the full span of all the vectors. But column span is what?",
    "start": "4546630",
    "end": "4553500"
  },
  {
    "text": "The column-span is the\nspan of all the vectors.",
    "start": "4553500",
    "end": "4558550"
  },
  {
    "text": "So that's why this\nis just an entity. And x pseudoinverse x,\nthis is the projection",
    "start": "4558550",
    "end": "4566760"
  },
  {
    "text": "of row-span of x. So how many rows there are. There are n-th rows of x.",
    "start": "4566760",
    "end": "4573190"
  },
  {
    "text": "And they don't span everything\nbecause the dimension d is bigger. So you cannot span everything. So this is why this\nis not an entity.",
    "start": "4573190",
    "end": "4580150"
  },
  {
    "text": "This is really\njust the projection of the row-span of x. You cannot simplify more, right?",
    "start": "4580150",
    "end": "4590080"
  },
  {
    "text": "So it's a little bit too long\nas a building block, but OK.",
    "start": "4590080",
    "end": "4595690"
  },
  {
    "text": "I hope this helps. This is how I understand\npseudoinverse. I never remember what x\nx-pseudoinverse is equal to.",
    "start": "4595690",
    "end": "4603100"
  },
  {
    "text": "So this is how I remember. ",
    "start": "4603100",
    "end": "4608829"
  },
  {
    "text": "So now we have so many\nglobal minima, right? I think with this\nit's easy to verify these are global minimum.",
    "start": "4608830",
    "end": "4614860"
  },
  {
    "text": "Because you can verify\nthis beta is global minimum because you can say\ntake x beta, which",
    "start": "4614860",
    "end": "4620500"
  },
  {
    "text": "is equal to x x-pseudoinverse\ny work plus x zeta.",
    "start": "4620500",
    "end": "4626020"
  },
  {
    "text": "The zeta is orthogonal\nto the rows of x. So that's where x beta is 0. So you got x\nx-pseudoinverse y vec",
    "start": "4626020",
    "end": "4633610"
  },
  {
    "text": "and x x-pseudoinverse\nin this case is an entity, so you\nget y vec, right? I just claim that x\nx-pseudoinverse is an entity.",
    "start": "4633610",
    "end": "4639887"
  },
  {
    "text": "OK. So that's why x beta\nis equal to y vec. That's why it's\na global minimum.",
    "start": "4639887",
    "end": "4644993"
  },
  {
    "text": "And the question is\nwhich global minimum you're going to converge to. So the theorem is\nthat if you initialize",
    "start": "4644993",
    "end": "4656200"
  },
  {
    "text": "the grid in descent L hat beta\nwith initialization beta 0",
    "start": "4656200",
    "end": "4666170"
  },
  {
    "text": "is equal to 0 and\nsufficiently smaller in rate.",
    "start": "4666170",
    "end": "4671410"
  },
  {
    "start": "4671410",
    "end": "4676860"
  },
  {
    "text": "And the rate, actually you\nknow exactly how small it is. I just don't want to\ngive you too much jargon.",
    "start": "4676860",
    "end": "4685857"
  },
  {
    "text": "So if you have lineal\nis small enough, and the initialization\nis 0, then",
    "start": "4685857",
    "end": "4690890"
  },
  {
    "text": "this converges to the\nminimum-norm solution.",
    "start": "4690890",
    "end": "4696830"
  },
  {
    "text": " So the minimum-norm\nsolution beta hat",
    "start": "4696830",
    "end": "4703340"
  },
  {
    "text": "is defined to be among\nall global minimum",
    "start": "4703340",
    "end": "4710429"
  },
  {
    "text": "where the minimum-norm solution\namong all global minimum of the loss function.",
    "start": "4710430",
    "end": "4716050"
  },
  {
    "text": "So basically you get this\n2 norm for free, right? You are minimizing\nthis, you didn't",
    "start": "4716050",
    "end": "4721097"
  },
  {
    "text": "say that I want to have\nthe minimum-norm solution. You just say I want to\ndo gradient descent. But you get the minimum-norm\nsolution for free.",
    "start": "4721097",
    "end": "4728280"
  },
  {
    "text": "And the reason why you\ngot it is because you express your\nimplicit preferences",
    "start": "4728280",
    "end": "4733620"
  },
  {
    "text": "through the initialization.  OK, cool.",
    "start": "4733620",
    "end": "4740070"
  },
  {
    "text": "So yeah, I think I have\n5 minutes, which is perfect for the proof sketch.",
    "start": "4740070",
    "end": "4745110"
  },
  {
    "text": "So I guess this is\nactually really a proof.",
    "start": "4745110",
    "end": "4762170"
  },
  {
    "text": "But I think I ignored\nsome small details. That's why I call you a sketch.",
    "start": "4762170",
    "end": "4767210"
  },
  {
    "text": "So the first step is that if you\ndo standard convex organization",
    "start": "4767210",
    "end": "4774940"
  },
  {
    "text": "you know that this goes to\n0, as t goes to infinity.",
    "start": "4774940",
    "end": "4780193"
  },
  {
    "text": "You know that if you run\nfor a long time, then your loss will become 0.",
    "start": "4780193",
    "end": "4785320"
  },
  {
    "text": "I'm not going to show\nhow do you do this. But you can invoke any off the\nshelf optimization results.",
    "start": "4785320",
    "end": "4793699"
  },
  {
    "text": "And the second thing is that\nyou know that the speed ahead is actually equal to x\nx-pseudoinverse times",
    "start": "4793700",
    "end": "4802360"
  },
  {
    "text": "y vec, right? So we know that all of\nthese are global minimum.",
    "start": "4802360",
    "end": "4807550"
  },
  {
    "text": "But if you take zeta to\nbe 0 then that's the-- ",
    "start": "4807550",
    "end": "4821500"
  },
  {
    "text": "Yes, if you take zeta\nto be 0, then that's the minimum-norm solution. I think this is this. There is no x here.",
    "start": "4821500",
    "end": "4826870"
  },
  {
    "start": "4826870",
    "end": "4832690"
  },
  {
    "text": "And this can also\nbe simply verified. ",
    "start": "4832690",
    "end": "4838350"
  },
  {
    "text": "So for any zeta\northogonal to x1 up to xn,",
    "start": "4838350",
    "end": "4845900"
  },
  {
    "text": "then you look at x-pseudoinverse\ny work plus zeta, the 2 norm of this.",
    "start": "4845900",
    "end": "4851276"
  },
  {
    "text": "This is equal to\nx-pseudoinverse y vec 2 norm, plus zeta 2 norm, plus 2 times\nx-pseudoinverse y vec zeta.",
    "start": "4851276",
    "end": "4865370"
  },
  {
    "text": "And this is larger\nthan x-pseudoinverse y vec 2 norm squared plus\n0, because the norm",
    "start": "4865370",
    "end": "4872420"
  },
  {
    "text": "is less than 0. And this quantity\nis just equal to 0.",
    "start": "4872420",
    "end": "4879410"
  },
  {
    "text": " Why this is equal to 0?",
    "start": "4879410",
    "end": "4885430"
  },
  {
    "text": "This is equals to 0 because\nI guess this is maybe",
    "start": "4885430",
    "end": "4891450"
  },
  {
    "text": "let's say this is equal\nto 2 times y vec x.",
    "start": "4891450",
    "end": "4898700"
  },
  {
    "text": "So what's this? This is zeta transpose\nx-pseudoinverse y vec, right?",
    "start": "4898700",
    "end": "4906429"
  },
  {
    "text": "And I claim that this is equal\nto x-pseudoinverse y vec 2 norm because this is 0.",
    "start": "4906430",
    "end": "4911949"
  },
  {
    "text": "And why this is 0? I guess this is actually a good\nway to practice what I had.",
    "start": "4911950",
    "end": "4918250"
  },
  {
    "text": "So x-pseudoinverse is v\nsigma inverse u-transpose.",
    "start": "4918250",
    "end": "4924850"
  },
  {
    "text": "Sorry. U-pseudoinverse-inverse is\nv sigma inverse u-transpose.",
    "start": "4924850",
    "end": "4930490"
  },
  {
    "text": "So the column-span\nof x-pseudoinverse",
    "start": "4930490",
    "end": "4936190"
  },
  {
    "text": "is the same as\nthe row-span of x.",
    "start": "4936190",
    "end": "4942130"
  },
  {
    "text": "And zeta-- sorry, this\nis transpose, not this.",
    "start": "4942130",
    "end": "4948639"
  },
  {
    "text": "Zeta is orthogonal\nto the rows of x. So which means that zeta\nis orthogonal to the column",
    "start": "4948640",
    "end": "4956110"
  },
  {
    "text": "of x-pseudoinverse, right? So that's why zeta times this\nis the zeta times the columns",
    "start": "4956110",
    "end": "4962469"
  },
  {
    "text": "of the pseudoinverse. So that's why\neverything is 0, right? So this is 0.",
    "start": "4962470",
    "end": "4969750"
  },
  {
    "text": "So zeta is orthogonal to\ncolumn-span of x-pseudoinverse",
    "start": "4969750",
    "end": "4976750"
  },
  {
    "text": "which is equal to\nthe row-span of x. ",
    "start": "4976750",
    "end": "4986969"
  },
  {
    "text": "So basically you\nsee that in the norm is only decreasing if\nyou set a zeta to be 0.",
    "start": "4986970",
    "end": "4992670"
  },
  {
    "text": "That's why when zeta is 0 that's\nthe minimum-norm solution. Right, so 3, I guess-- ",
    "start": "4992670",
    "end": "5000140"
  },
  {
    "text": "1 and 2 are basic facts about\nthis linear regression thing. 3 is what's really\nabout the optimization",
    "start": "5000140",
    "end": "5006830"
  },
  {
    "text": "so you can prove that beta t\nis in a span of x1 up to xn.",
    "start": "5006830",
    "end": "5013100"
  },
  {
    "text": "You can prove this inductively. ",
    "start": "5013100",
    "end": "5020890"
  },
  {
    "text": "And why this is the case? This is a super simple\ninduction because beta t",
    "start": "5020890",
    "end": "5026110"
  },
  {
    "text": "is equals beta t plus 1 is\nequals to beta t minus eta times the gradient and beta t.",
    "start": "5026110",
    "end": "5032680"
  },
  {
    "text": "And what's the gradient? The gradient is x-transpose\ny vec minus x beta.",
    "start": "5032680",
    "end": "5040840"
  },
  {
    "text": "So this is in the\ncolumn-span of x-transpose.",
    "start": "5040840",
    "end": "5049659"
  },
  {
    "text": "And it is also in\nthe row span of x.",
    "start": "5049660",
    "end": "5056980"
  },
  {
    "text": "So basically your update is\nalways in the row-span of x. That's why you never\nleave this span, right?",
    "start": "5056980",
    "end": "5065080"
  },
  {
    "text": "Maybe I should start with\nthe beta 0 is in this span. Beta 0 is in this\nspan of x1 up to xn.",
    "start": "5065080",
    "end": "5073389"
  },
  {
    "text": "And each time you update,\nand update is in the span x1 up to xn. So by induction you get that\nyou're always in this span.",
    "start": "5073390",
    "end": "5080289"
  },
  {
    "text": " So basically then, so for-- Because you're\nalways in your span, right, the only solution to\nL hat beta 0 in this span",
    "start": "5080290",
    "end": "5098790"
  },
  {
    "text": "is this, right?",
    "start": "5098790",
    "end": "5103800"
  },
  {
    "text": "Because what are the\nsolutions with error 0? The solutions with\nerror 0 are these ones.",
    "start": "5103800",
    "end": "5109800"
  },
  {
    "text": " So this has solution 0.",
    "start": "5109800",
    "end": "5117610"
  },
  {
    "text": "And among this who are in\nthe row-span of x, right? So only this one\nis in the row-span",
    "start": "5117610",
    "end": "5125245"
  },
  {
    "text": "of x because all of these\nare not in the row-span of x. They are orthogonal\nto the row-span of x. So the only solution that\nis in the row-span of x",
    "start": "5125246",
    "end": "5133150"
  },
  {
    "text": "is just the first term. You just take the first term. And that happens to be\nthe minimum-norm solution.",
    "start": "5133150",
    "end": "5139489"
  },
  {
    "text": "And that's why you get\nthe minimum-norm solution. And sometimes basically all the\nmagic come from this, right?",
    "start": "5139490",
    "end": "5147429"
  },
  {
    "text": "So basically this is a\nregularization sometimes. This is a constraint\nimposed by the algorithm.",
    "start": "5147430",
    "end": "5154010"
  },
  {
    "text": "The algorithm say that\nyou cannot go everywhere. The algorithm say you can\nonly go to those places who",
    "start": "5154010",
    "end": "5160765"
  },
  {
    "text": "are in the span of the data. So that's why you have to\nstay in the span of the data.",
    "start": "5160765",
    "end": "5165790"
  },
  {
    "text": "And it happens that in\nthe span of the data there is only one solution. And that solution is the\nminimum-norm solution.",
    "start": "5165790",
    "end": "5171148"
  },
  {
    "start": "5171148",
    "end": "5178110"
  },
  {
    "text": "So I think in some sense\nthe if I draw a picture--",
    "start": "5178110",
    "end": "5183165"
  },
  {
    "text": "OK, I guess I'm running\nlate, but real quick. So if I draw a picture, I\nthink this is a very difficult",
    "start": "5183165",
    "end": "5189090"
  },
  {
    "text": "picture to draw. But I think you\ncan still try it. So you can have maybe I\nsay this is blue direction,",
    "start": "5189090",
    "end": "5201030"
  },
  {
    "text": "this is the span of the data. Let's suppose the\nspan has one data. So the direction of\nthe span of the data",
    "start": "5201030",
    "end": "5206400"
  },
  {
    "text": "is only one dimensional. And then you have a subspace of\nsolutions which are orthogonal.",
    "start": "5206400",
    "end": "5218780"
  },
  {
    "text": "So this is orthogonal\nhere to the span. ",
    "start": "5218780",
    "end": "5224210"
  },
  {
    "text": "This is the solution where\nyou have this, all right? So it's orthogonal to\nthe span of the data",
    "start": "5224210",
    "end": "5231360"
  },
  {
    "text": "and the intersection part\nis the target solution. So the intersection part is\nreally x-pseudoinverse y vec.",
    "start": "5231360",
    "end": "5244935"
  },
  {
    "text": " So you start with this.",
    "start": "5244935",
    "end": "5250860"
  },
  {
    "text": "You try to reach\nthis purple plane, because that's what the\noptimization wants to do.",
    "start": "5250860",
    "end": "5256290"
  },
  {
    "text": "The optimization wants to\nreach the purple plane. But optimization\nalso say you can only go in the blue direction.",
    "start": "5256290",
    "end": "5263440"
  },
  {
    "text": "And so that's why you\nmeet in the intersection. And the intersection is the\nclosest point to the origin. ",
    "start": "5263440",
    "end": "5271719"
  },
  {
    "text": "OK, I guess that's, yeah. ",
    "start": "5271720",
    "end": "5276760"
  },
  {
    "text": "[INAUDIBLE]  Yeah. So do you need a condition that\nyou have to span on a span,",
    "start": "5276760",
    "end": "5284390"
  },
  {
    "text": "right? So, yes, you do. Because if you don't\nspan, suppose for example, you're supposed to start here.",
    "start": "5284390",
    "end": "5290900"
  },
  {
    "text": "So what happens is that you can\nonly move in this direction. That's what the algorithm says.",
    "start": "5290900",
    "end": "5296582"
  },
  {
    "text": "But the algorithm says\nthe update is in a span. So all your changes\nis in the span.",
    "start": "5296582",
    "end": "5302570"
  },
  {
    "text": "So basically you go in\nthis way, and you hit here. So then this place is not the\nminimum-norm solution anymore.",
    "start": "5302570",
    "end": "5310324"
  },
  {
    "text": "This place is going\nto have some higher norm than the ideal point.",
    "start": "5310325",
    "end": "5316554"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "5316554",
    "end": "5322119"
  },
  {
    "text": "Yes. So you can say the implicit\nregularization effect always happens. But the effect is the\nminimum-norm solution",
    "start": "5322120",
    "end": "5328900"
  },
  {
    "text": "only if your\ninitialization is 0. You always have\nyour preferences.",
    "start": "5328900",
    "end": "5334340"
  },
  {
    "text": "So whatever you do\nwith initialization you have some preferences\nabout which global minima you want to converge to, right?",
    "start": "5334340",
    "end": "5340930"
  },
  {
    "text": "And if you want the\npreferences to be the minimum-norm\nsolution, then you really have to choose 0 as\nthe initialization.",
    "start": "5340930",
    "end": "5349974"
  },
  {
    "start": "5349974",
    "end": "5358380"
  },
  {
    "text": "Any the other questions?  [INAUDIBLE] ",
    "start": "5358380",
    "end": "5384605"
  },
  {
    "text": "So the question is whether\nthere's any hope that this can transfer to nonlinear cases. I think here we are using a lot\nof things about linear algebra.",
    "start": "5384605",
    "end": "5393910"
  },
  {
    "text": "So we know what is the\nminimum-norm solution. And so far we have the\northogonality, everything, right? So when you have nonlinearity\nyou don't have most of this.",
    "start": "5393910",
    "end": "5403300"
  },
  {
    "text": "So those parts that we\ndiscussed about the very heavily linear algebraic part, those\ndon't transfer at all probably.",
    "start": "5403300",
    "end": "5412270"
  },
  {
    "text": "But somehow at least we can\nfind one other situation where when you have\nnonlinear models",
    "start": "5412270",
    "end": "5420010"
  },
  {
    "text": "you still prefer the\nminimum-norm solution. And that's next lecture. But the mechanism is\nnot exactly the same.",
    "start": "5420010",
    "end": "5428620"
  },
  {
    "text": "So next lecture and this\nlecture, the only connection is that the final\nmessage is the same.",
    "start": "5428620",
    "end": "5439930"
  },
  {
    "text": "But the techniques\nare quite different. We still don't know how to\nunify them in the right way.",
    "start": "5439930",
    "end": "5447430"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "5447430",
    "end": "5468930"
  },
  {
    "text": "Right, right. [INAUDIBLE] ",
    "start": "5468930",
    "end": "5484750"
  },
  {
    "text": "Yeah, yeah. So you are absolutely right. So like the difficult case\ncome from the very, very small linear rate case.",
    "start": "5484750",
    "end": "5490630"
  },
  {
    "text": "I think infinitesimal\nsmall linear rate case. So even for infinitesimal\nsmall linear rate.",
    "start": "5490630",
    "end": "5496982"
  },
  {
    "text": "So basically you have a\ndifferential equation, right? So you just have a trajectory.",
    "start": "5496982",
    "end": "5502239"
  },
  {
    "text": "And you want to know where\nthe trajectory goes, right? ",
    "start": "5502240",
    "end": "5509565"
  },
  {
    "text": "I don't know too much about\ndifferential equations, but I think the problem is\nhow to solve that equation.",
    "start": "5509565",
    "end": "5515580"
  },
  {
    "text": "You know solution exists. You know there's a trajectory. But where this trajectory really\ngoes, that's the hard part.",
    "start": "5515580",
    "end": "5522430"
  },
  {
    "text": "I don't think we, at least\nI'm not aware of any papers that use tools from\ndifferential equations heavily.",
    "start": "5522430",
    "end": "5531670"
  },
  {
    "text": "So this is a useful\nlanguage, right?  The formulation of the language\nperspective, the differential",
    "start": "5531670",
    "end": "5539710"
  },
  {
    "text": "equations language\nare very useful. But typically the hard part\nis, how do you solve it?",
    "start": "5539710",
    "end": "5547150"
  },
  {
    "text": "[INAUDIBLE] In some cases, you can.",
    "start": "5547150",
    "end": "5552310"
  },
  {
    "text": "I know one paper where\nyou can solve it. It's using the structure\nof the problem. You have to literally solve\nit using some new math.",
    "start": "5552310",
    "end": "5560320"
  },
  {
    "text": "It's not like you can invoke\na theorem in the differential equation literature saying\nthese kind of questions can all be solved.",
    "start": "5560320",
    "end": "5566212"
  },
  {
    "text": "I don't think so.  OK, sounds great.",
    "start": "5566212",
    "end": "5572400"
  },
  {
    "text": "OK, cool. See you next week. ",
    "start": "5572400",
    "end": "5580000"
  }
]