[
  {
    "start": "0",
    "end": "2500"
  },
  {
    "text": "So we've talked about\nsubset selection methods,",
    "start": "2500",
    "end": "4621"
  },
  {
    "text": "ridge regression and\nlasso, and now we're",
    "start": "4622",
    "end": "6330"
  },
  {
    "text": "moving on to the\nlast class of method",
    "start": "6330",
    "end": "8172"
  },
  {
    "text": "that we're going to talk\nabout in this lecture, which",
    "start": "8172",
    "end": "10380"
  },
  {
    "text": "is dimension reduction.",
    "start": "10380",
    "end": "12060"
  },
  {
    "text": "And so if we remember in the\nsubset selection methods,",
    "start": "12060",
    "end": "15510"
  },
  {
    "text": "we were just taking a\nsubset of the predictors",
    "start": "15510",
    "end": "17580"
  },
  {
    "text": "and using least squares\nto fit the model.",
    "start": "17580",
    "end": "19600"
  },
  {
    "text": "And then in ridge\nregression and the lasso,",
    "start": "19600",
    "end": "21883"
  },
  {
    "text": "we were really doing\nsomething different",
    "start": "21883",
    "end": "23550"
  },
  {
    "text": "where we were taking\nall of the predictors,",
    "start": "23550",
    "end": "25560"
  },
  {
    "text": "but we weren't\nusing least squares,",
    "start": "25560",
    "end": "27250"
  },
  {
    "text": "we were using a shrinkage\napproach to fit the model.",
    "start": "27250",
    "end": "29850"
  },
  {
    "text": "And now we're going to do\nsomething different, which",
    "start": "29850",
    "end": "32369"
  },
  {
    "text": "is we're going to\nuse least squares,",
    "start": "32369",
    "end": "33883"
  },
  {
    "text": "but we're not going\nto use least squares",
    "start": "33883",
    "end": "35550"
  },
  {
    "text": "on the original\npredictors X1 through Xp.",
    "start": "35550",
    "end": "38610"
  },
  {
    "text": "Instead, we're going to come\nup with new predictors, which",
    "start": "38610",
    "end": "42060"
  },
  {
    "text": "are linear combinations\nof the original predictors",
    "start": "42060",
    "end": "45400"
  },
  {
    "text": "and we're going to use these new\npredictors to fit a linear model",
    "start": "45400",
    "end": "48810"
  },
  {
    "text": "using least squares.",
    "start": "48810",
    "end": "50580"
  },
  {
    "text": "So this is known as\ndimension reduction.",
    "start": "50580",
    "end": "52960"
  },
  {
    "text": "And the reason it's\ncalled dimension reduction",
    "start": "52960",
    "end": "55379"
  },
  {
    "text": "is because we're\ngoing to use those",
    "start": "55380",
    "end": "57150"
  },
  {
    "text": "p original predictors to fit\na model using m new predictors",
    "start": "57150",
    "end": "63250"
  },
  {
    "text": "where m is going\nto be less than p.",
    "start": "63250",
    "end": "65892"
  },
  {
    "text": "So we're going to shrink the\nproblem from 1 of p predictors",
    "start": "65892",
    "end": "68350"
  },
  {
    "text": "to 1 of m predictors.",
    "start": "68350",
    "end": "71340"
  },
  {
    "text": "So in a little bit\nof detail here,",
    "start": "71340",
    "end": "73259"
  },
  {
    "text": "we're going to define m linear\ncombinations Z1 through Zm,",
    "start": "73260",
    "end": "77950"
  },
  {
    "text": "where m is some\nnumber less than p,",
    "start": "77950",
    "end": "80039"
  },
  {
    "text": "and these are going to be linear\ncombinations of the original p",
    "start": "80040",
    "end": "82950"
  },
  {
    "text": "predictors.",
    "start": "82950",
    "end": "84570"
  },
  {
    "text": "So for instance, Zm is going to\nbe the sum of the p predictors",
    "start": "84570",
    "end": "89250"
  },
  {
    "text": "where each predictor is\nmultiplied by phi mj, where",
    "start": "89250",
    "end": "93420"
  },
  {
    "text": "phi mj is some constant.",
    "start": "93420",
    "end": "95280"
  },
  {
    "text": "And in a minute we'll talk about\nwhere this phi mj comes from.",
    "start": "95280",
    "end": "99240"
  },
  {
    "text": "But the point is, once we get\nour new predictor Z1 through Zm,",
    "start": "99240",
    "end": "103020"
  },
  {
    "text": "we're just going to fit a linear\nregression model using least",
    "start": "103020",
    "end": "107240"
  },
  {
    "text": "squares, but instead of using\nthe X's, we're going to use",
    "start": "107240",
    "end": "110030"
  },
  {
    "text": "the Z's.",
    "start": "110030",
    "end": "112140"
  },
  {
    "text": "So in this new\nleast squares model,",
    "start": "112140",
    "end": "114000"
  },
  {
    "text": "my predictors are going to be\nthe Z's and my coefficients",
    "start": "114000",
    "end": "117600"
  },
  {
    "text": "are going to be theta\nnaught through theta m.",
    "start": "117600",
    "end": "120799"
  },
  {
    "text": "And the idea is\nthat if I can just",
    "start": "120800",
    "end": "122750"
  },
  {
    "text": "be really clever in how I choose\nthese linear combinations,",
    "start": "122750",
    "end": "126140"
  },
  {
    "text": "in particular, if I'm clever\nabout how I choose these phi",
    "start": "126140",
    "end": "129919"
  },
  {
    "text": "mjs, then I can actually\nbeat least squares",
    "start": "129919",
    "end": "134360"
  },
  {
    "text": "that I would have\ngotten if I had just",
    "start": "134360",
    "end": "136130"
  },
  {
    "text": "used the raw predictors.",
    "start": "136130",
    "end": "139860"
  },
  {
    "text": "So one thing that\nwe should notice",
    "start": "139860",
    "end": "142430"
  },
  {
    "text": "is that on the\nprevious slide here,",
    "start": "142430",
    "end": "144629"
  },
  {
    "text": "we had this summation\nover theta mzim.",
    "start": "144630",
    "end": "149215"
  },
  {
    "text": "And if we look at that a\nlittle bit more carefully",
    "start": "149215",
    "end": "151680"
  },
  {
    "text": "and we plug in the definition\nof zim, which remember was just",
    "start": "151680",
    "end": "156150"
  },
  {
    "text": "a linear combination of the\noriginal X's, and we switched",
    "start": "156150",
    "end": "161189"
  },
  {
    "text": "the order of the sums and we\ndo a little bit of algebra,",
    "start": "161190",
    "end": "163980"
  },
  {
    "text": "we see that what we\nactually have here",
    "start": "163980",
    "end": "166319"
  },
  {
    "text": "is a sum over the p\npredictors times this quantity",
    "start": "166320",
    "end": "173330"
  },
  {
    "text": "times the fifth predictor.",
    "start": "173330",
    "end": "176010"
  },
  {
    "text": "So this is actually just\na linear combination",
    "start": "176010",
    "end": "178379"
  },
  {
    "text": "of the original X's, where the\nlinear combination involves",
    "start": "178380",
    "end": "184110"
  },
  {
    "text": "a beta j, that's\ndefined like this.",
    "start": "184110",
    "end": "188830"
  },
  {
    "text": "So the point is that when I\ndo this dimension reduction",
    "start": "188830",
    "end": "192610"
  },
  {
    "text": "approach and I define these new\nZ's that are linear combinations",
    "start": "192610",
    "end": "195820"
  },
  {
    "text": "of the X's, I'm actually\ngoing to fit a linear model",
    "start": "195820",
    "end": "199210"
  },
  {
    "text": "that's linear in\nthe original X's.",
    "start": "199210",
    "end": "202410"
  },
  {
    "text": "But the beta J's\nin my model need",
    "start": "202410",
    "end": "205470"
  },
  {
    "text": "to take a very,\nvery specific form.",
    "start": "205470",
    "end": "209515"
  },
  {
    "text": "So these dimension\nreduction approaches,",
    "start": "209515",
    "end": "211390"
  },
  {
    "text": "they're giving me models\nfit by least squares,",
    "start": "211390",
    "end": "213885"
  },
  {
    "text": "but I'm fitting the model not\non the original predictors,",
    "start": "213885",
    "end": "216430"
  },
  {
    "text": "it's on a new set of predictors.",
    "start": "216430",
    "end": "218439"
  },
  {
    "text": "And I can think of it\nactually as ultimately",
    "start": "218440",
    "end": "221410"
  },
  {
    "text": "a linear model on the\noriginal predictors,",
    "start": "221410",
    "end": "224980"
  },
  {
    "text": "but using different coefficients\nthat take this funny form here.",
    "start": "224980",
    "end": "228409"
  },
  {
    "text": "So in a way, it's\nsimilar ridge and lasso,",
    "start": "228410",
    "end": "231050"
  },
  {
    "text": "it's still a linear model\nin all the variables,",
    "start": "231050",
    "end": "233110"
  },
  {
    "text": "but there's a constraint\non the coefficients.",
    "start": "233110",
    "end": "235490"
  },
  {
    "text": "That's exactly right.",
    "start": "235490",
    "end": "236380"
  },
  {
    "text": "But we're getting a\nconstraint in a different way.",
    "start": "236380",
    "end": "237950"
  },
  {
    "text": "We're not getting a constraint\nlike in the ridge case",
    "start": "237950",
    "end": "240157"
  },
  {
    "text": "by saying, OK, my sum of\nsquared betas needs to be small.",
    "start": "240158",
    "end": "243530"
  },
  {
    "text": "Instead, we're\nsaying my betas need",
    "start": "243530",
    "end": "245260"
  },
  {
    "text": "to take this really funny\nform if you look at it.",
    "start": "245260",
    "end": "247930"
  },
  {
    "text": "But it's got a\nsimple interpretation",
    "start": "247930",
    "end": "249579"
  },
  {
    "text": "in terms of least squares\non a new set of features.",
    "start": "249580",
    "end": "253910"
  },
  {
    "text": "The idea here is really it\nboils down to the bias variance",
    "start": "253910",
    "end": "256489"
  },
  {
    "text": "trade off by saying\nthat my betas need",
    "start": "256490",
    "end": "258588"
  },
  {
    "text": "to take this particular\nform, I can win.",
    "start": "258589",
    "end": "261199"
  },
  {
    "text": "I can get a model with low\nbias and also low variance",
    "start": "261200",
    "end": "264110"
  },
  {
    "text": "relative to what I\nwould have gotten",
    "start": "264110",
    "end": "265610"
  },
  {
    "text": "if I had just done plain\nvanilla least squares",
    "start": "265610",
    "end": "267800"
  },
  {
    "text": "on the original features.",
    "start": "267800",
    "end": "270319"
  },
  {
    "text": "One thing that I\nshould mention is",
    "start": "270320",
    "end": "271940"
  },
  {
    "text": "that this is only going to work\nnicely if m is less than p.",
    "start": "271940",
    "end": "275300"
  },
  {
    "text": "And instead if my\nm equal p, then I",
    "start": "275300",
    "end": "279110"
  },
  {
    "text": "would just end up\nwith least squares",
    "start": "279110",
    "end": "280772"
  },
  {
    "text": "and this whole dimension\nreduction thing",
    "start": "280773",
    "end": "282440"
  },
  {
    "text": "would have just given me\nleast squares on the raw data.",
    "start": "282440",
    "end": "285500"
  },
  {
    "start": "285500",
    "end": "286000"
  }
]