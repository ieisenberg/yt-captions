[
  {
    "start": "0",
    "end": "5930"
  },
  {
    "text": "OK. Let's get started. Let's get started on\nthe topic for today. So today we're\ngoing to be talking",
    "start": "5930",
    "end": "13209"
  },
  {
    "text": "about non-parametric\nfew-shot learning. This will also be part of your\nsecond homework assignment.",
    "start": "13210",
    "end": "19510"
  },
  {
    "text": "We'll talk about a few\ndifferent methods for this and what non-parametric\nfew-shot learning even is.",
    "start": "19510",
    "end": "25390"
  },
  {
    "text": "And then we'll also\ngo over a case study of applying a form of\nnon-parametric few-shot learner",
    "start": "25390",
    "end": "31600"
  },
  {
    "text": "to a medical image\ndiagnosis problem. After that, we'll talk\nabout some properties",
    "start": "31600",
    "end": "37830"
  },
  {
    "text": "of meta-learning algorithms\nthat are desirable. And we'll compare the three\ndifferent classes of approaches",
    "start": "37830",
    "end": "43890"
  },
  {
    "text": "that we've talked about\nso far with respect to those desirable properties.",
    "start": "43890",
    "end": "49559"
  },
  {
    "text": "And then we'll also go over a\nfew meta-learning applications including imitation learning,\ndrug discovery, motion",
    "start": "49560",
    "end": "54920"
  },
  {
    "text": "prediction, and\nlanguage generation and cover some of\nthe ways that people have applied ideas\nin meta-learning",
    "start": "54920",
    "end": "60260"
  },
  {
    "text": "to various problems. And then by the end of\nthe lecture hopefully",
    "start": "60260",
    "end": "65540"
  },
  {
    "text": "you'll be able to understand\nnon-parametric few-shot learning techniques\nas well as how to implement them,\nwhich will be helpful",
    "start": "65540",
    "end": "72800"
  },
  {
    "text": "for your second homework. We'll go over kind\nof the trade-offs between different\nkinds of methods.",
    "start": "72800",
    "end": "79580"
  },
  {
    "text": "And then also I hope\nto hopefully give you some familiarity with\ndifferent applied formulations",
    "start": "79580",
    "end": "85248"
  },
  {
    "text": "of meta-learning as well.  OK, so that's the\nplan for today.",
    "start": "85248",
    "end": "91000"
  },
  {
    "text": "And before we kind of talk\nabout non-parametric few-shot learning, I want to\njust briefly recap the two classes of\nmeta-learning approaches",
    "start": "91000",
    "end": "97620"
  },
  {
    "text": "that we've talked about so far. So last week we first talked\nabout black-box meta-learning.",
    "start": "97620",
    "end": "104910"
  },
  {
    "text": "And the key idea with a\nblack-box meta-learner is that you have some set\nof training data points",
    "start": "104910",
    "end": "109950"
  },
  {
    "text": "that's going to be fed into\na black-box neural network such as a recurrent\nneural network f theta.",
    "start": "109950",
    "end": "116730"
  },
  {
    "text": "And you'll also additionally\npass it a new unlabeled test input and train\nthis entire learner",
    "start": "116730",
    "end": "124050"
  },
  {
    "text": "to be able to correctly predict\nthe corresponding label. And this is also the\nkind of meta-learner",
    "start": "124050",
    "end": "129326"
  },
  {
    "text": "that you've been\nimplementing in homework 1.  And the [INAUDIBLE] effects\nof this is very expressive.",
    "start": "129327",
    "end": "137513"
  },
  {
    "text": "The downside is it's a very\nchallenging optimization problem, which you may have\nencountered in the homework as well.",
    "start": "137513",
    "end": "143740"
  },
  {
    "text": "Next, on Wednesday\nof last week, we talked about\noptimization-based modeling, which embeds an optimization\ninside this meta-learning",
    "start": "143740",
    "end": "151290"
  },
  {
    "text": "process. And then you can\ndifferentiate and optimize things such as\nthe initialization",
    "start": "151290",
    "end": "156660"
  },
  {
    "text": "of that optimization\nprocess, the learning rate, and other free\nparameters of the optimization.",
    "start": "156660",
    "end": "162900"
  },
  {
    "text": "And this is-- kind of the main\nbenefit of this kind of method is that it builds in the\nstructure of optimization",
    "start": "162900",
    "end": "168900"
  },
  {
    "text": "into the meta-learning\nprocess, which can make it easier to\noptimize and also easier",
    "start": "168900",
    "end": "175739"
  },
  {
    "text": "to generalize to tasks outside\nof your task distribution. The downside is\nthat it typically",
    "start": "175740",
    "end": "181620"
  },
  {
    "text": "requires a second-order\noptimization, which may be computationally expensive.",
    "start": "181620",
    "end": "187940"
  },
  {
    "text": "OK. So these are the two\nmeta-learning algorithms that we've talked about so far. And what we want to\ntalk about-- what",
    "start": "187940",
    "end": "194200"
  },
  {
    "text": "we're going to\ntalk about today is if we can embed a learning\nprocedure into the meta-learner",
    "start": "194200",
    "end": "199450"
  },
  {
    "text": "without requiring a\nsecond-order optimization. And in particular,\nso far we've been",
    "start": "199450",
    "end": "206689"
  },
  {
    "text": "thinking about learning\nparametric models and kind of parametric\napproaches to learning,",
    "start": "206690",
    "end": "214460"
  },
  {
    "text": "but when you're in\na low data regime, non-parametric\nmethods are simple,",
    "start": "214460",
    "end": "219590"
  },
  {
    "text": "and they actually work quite\nwell in a low data regime. So for example, things\nlike nearest neighbors",
    "start": "219590",
    "end": "225409"
  },
  {
    "text": "is an example of a\nnon-parametric approach to learning, where\nyou have some data,",
    "start": "225410",
    "end": "233090"
  },
  {
    "text": "and you take whatever\nkind of training data point is closest to\nyour test data point.",
    "start": "233090",
    "end": "239500"
  },
  {
    "text": "You output the corresponding\nclassification. There are, of course, variants,\nsuch as k-nearest neighbors and so forth.",
    "start": "239500",
    "end": "246150"
  },
  {
    "text": "And one kind of\ncritical thing to note is that, in the kind of\nfew-shot meta-learning problem",
    "start": "246150",
    "end": "251180"
  },
  {
    "text": "that we've been talking\nabout, at meta test time, few-shot learning is exactly\nin the low data regime.",
    "start": "251180",
    "end": "258640"
  },
  {
    "text": "So kind of with this\nin mind, we want to think about how we might\nleverage non-parametric methods",
    "start": "258640",
    "end": "265120"
  },
  {
    "text": "in the inner loop. However, during\nmeta-training, we still want to be parametric\nbecause we want",
    "start": "265120",
    "end": "270142"
  },
  {
    "text": "to be able to leverage a large\namount of previous experience and data. So kind of the key\ntrade-offs between",
    "start": "270142",
    "end": "276490"
  },
  {
    "text": "parametric and\nnon-parametric methods is that non-parametric\nmethods are simple and work well in\na low data regime,",
    "start": "276490",
    "end": "281770"
  },
  {
    "text": "but when you have a ton of\ndata, they don't scale as well, because you have to kind of do\nlookups on your training data",
    "start": "281770",
    "end": "287320"
  },
  {
    "text": "set. And so that's why it's kind of-- when we're doing few-shot\nlearning at meta test time,",
    "start": "287320",
    "end": "292539"
  },
  {
    "text": "we're going to look at being\nin the non-parametric setting and, during\nmeta-training, we're going to try to be in the\nparametric setting.",
    "start": "292540",
    "end": "300267"
  },
  {
    "text": "And then in particular,\nkind of the key idea is whether we can use\nparametric meta-learners that produce effective\nnon-parametric learners.",
    "start": "300267",
    "end": "306797"
  },
  {
    "text": " So that's the motivation here. Do you have a question?",
    "start": "306797",
    "end": "313449"
  },
  {
    "text": "Yeah. Can you please recap as\nin what is a parametric and what is a\nnon-parametric learner?",
    "start": "313450",
    "end": "319990"
  },
  {
    "text": "Yeah. So a parametric\nlearner is a setting where you are learning a\nmodel, a parametric model that",
    "start": "319990",
    "end": "327300"
  },
  {
    "text": "has parameters hence the name\nparametric, that, for example, like a classifier that\ntakes as input an input",
    "start": "327300",
    "end": "334919"
  },
  {
    "text": "and produces an output. And in non-parametric methods\nlike nearest neighbors",
    "start": "334920",
    "end": "342060"
  },
  {
    "text": "and k-nearest neighbors, there\nis no parameters involved,",
    "start": "342060",
    "end": "347790"
  },
  {
    "text": "because-- and essentially, kind of,\nyour training data points are kind of like\nyour parameters. Your data is like\nyour parameters.",
    "start": "347790",
    "end": "353587"
  },
  {
    "text": "So in this diagram, which is\nshowing an example of nearest neighbors, each of these\nblue and red points",
    "start": "353587",
    "end": "361260"
  },
  {
    "text": "are the data points\nin your data set. And whenever you want to\nclassify a new data point, you basically just\nlook at the color",
    "start": "361260",
    "end": "369134"
  },
  {
    "text": "of that corresponding spot. So if your new data point\nis somewhere over here, then it would be blue, because\nthe nearest data point is blue.",
    "start": "369135",
    "end": "374940"
  },
  {
    "text": "If you're somewhere\nover here, then you would output the label red,\nbecause the nearest data points are red.",
    "start": "374940",
    "end": "381188"
  },
  {
    "text": "And in this sort of\napproach, there's kind of no parameters,\nwhich is kind of why they're",
    "start": "381188",
    "end": "386750"
  },
  {
    "text": "called non-parametric methods. ",
    "start": "386750",
    "end": "391850"
  },
  {
    "text": "OK, cool.  And I guess it's\nalso worth noting that some of the methods\nthat we'll be covering today",
    "start": "391850",
    "end": "398150"
  },
  {
    "text": "actually precede some of\nthe parametric approaches to meta-learning. ",
    "start": "398150",
    "end": "405250"
  },
  {
    "text": "OK, so how do you\nactually do this, and what does this actually\nlook like in a few-shot learning setting? So if you were to take\nour classic few-shot image",
    "start": "405250",
    "end": "413740"
  },
  {
    "text": "classification\nproblem and applied a non-parametric learning\ntechnique like nearest neighbors, what we would do\nis we would take our test data",
    "start": "413740",
    "end": "421030"
  },
  {
    "text": "point and compare it to\neach of the [INAUDIBLE]",
    "start": "421030",
    "end": "429250"
  },
  {
    "text": "and then find the nearest\nneighbor, and output the label corresponding to the\nnearest neighbor.",
    "start": "429250",
    "end": "434810"
  },
  {
    "text": "So if this image was\nthe nearest neighbor, then we would output the class\ncorresponding to this image. If this one was the\nnearest neighbor,",
    "start": "434810",
    "end": "440781"
  },
  {
    "text": "we'd output the other\ncorresponding class. ",
    "start": "440782",
    "end": "446313"
  },
  {
    "text": "Now, so the idea is that we're\ngoing to be comparing our test image with our training images.",
    "start": "446313",
    "end": "451390"
  },
  {
    "text": "Now an important question here\nis, what space do you compare? And with what distance\nmetric do you compare?",
    "start": "451390",
    "end": "459040"
  },
  {
    "text": " So one kind of very naive and\nsimple idea that we might do",
    "start": "459040",
    "end": "466770"
  },
  {
    "text": "is maybe do, like, l2\ndistance in pixel space to compare images. And that's what, like, a typical\nkind of naive nearest neighbors",
    "start": "466770",
    "end": "474900"
  },
  {
    "text": "approach would do,\nbut unfortunately this is a pretty bad idea. So for example,\nthis is an example",
    "start": "474900",
    "end": "482970"
  },
  {
    "text": "from this paper by\nRichard Zhang et al, where if you took this image and\ncompared it to these two",
    "start": "482970",
    "end": "488129"
  },
  {
    "text": "images, you would\nthink intuitively that it should be\ncloser to this image",
    "start": "488130",
    "end": "493400"
  },
  {
    "text": "just because that\nkind of perceptually looks more similar. But if you actually compute the\nl2 distance between these two",
    "start": "493400",
    "end": "499250"
  },
  {
    "text": "images, it turns out\nthat this one is actually closer in the l2\ndistance than this one.",
    "start": "499250",
    "end": "505980"
  },
  {
    "text": "So this suggests\nthat this kind of one anecdotal example of why l2\ndistance might be a bad idea.",
    "start": "505980",
    "end": "511870"
  },
  {
    "text": " OK. And so what might we\ndo instead of doing",
    "start": "511870",
    "end": "519799"
  },
  {
    "text": "pixel space or l2 distance? Does anyone have any\nideas or thoughts",
    "start": "519799",
    "end": "528980"
  },
  {
    "text": "on what we might do to get\na good distance metric? ",
    "start": "528980",
    "end": "534825"
  },
  {
    "text": "You can raise your hand\nor enter it in chat. ",
    "start": "534825",
    "end": "539990"
  },
  {
    "text": "Yeah. Maybe you can learn from\nsort of embeddings heuristic, and then, I guess,\nsort of like features",
    "start": "539990",
    "end": "546730"
  },
  {
    "text": "that are a big factor in\na CNN, and use that, like, embedding in the feature\nspace to compare?",
    "start": "546730",
    "end": "552970"
  },
  {
    "text": "Yeah. So one thing you could do\nis, instead of doing distance in the original space, you could\ndo it in some embedding space.",
    "start": "552970",
    "end": "559615"
  },
  {
    "text": " And this will likely produce\nkind of a much better distance",
    "start": "559615",
    "end": "567090"
  },
  {
    "text": "metric if that\nembedding reflects kind of more the\nsemantics of these images.",
    "start": "567090",
    "end": "572730"
  },
  {
    "text": "Any thoughts on\nwhere you might get that embedding either from-- or from someone else?",
    "start": "572730",
    "end": "577832"
  },
  {
    "text": "Professor? Yeah. So maybe I'm thinking we can\ntake the pre-trained model,",
    "start": "577832",
    "end": "584570"
  },
  {
    "text": "for example, like ImageNet\npre-trained on ResNet50, and take maybe the first layer\nor a second hidden layer,",
    "start": "584570",
    "end": "592190"
  },
  {
    "text": "and just feed through the\nimage that I want to compare, and take out the second or\nsome intermediate hidden layers",
    "start": "592190",
    "end": "599449"
  },
  {
    "text": "and use that as an embedding. Yeah. So you could basically\ntrain them all on ImageNet",
    "start": "599450",
    "end": "604580"
  },
  {
    "text": "and use the kind of embeddings\nproduced by ImageNet and do nearest neighbors\nand that space.",
    "start": "604580",
    "end": "609770"
  },
  {
    "text": "So that's kind of one way that\nyou could get such embeddings. Are there any ideas for\nhow you might kind of use",
    "start": "609770",
    "end": "617950"
  },
  {
    "text": "a meta-learning like approach to\nget an embedding or a distance",
    "start": "617950",
    "end": "623750"
  },
  {
    "text": "metric? ",
    "start": "623750",
    "end": "637150"
  },
  {
    "text": "Perhaps you can\nhave a model which reads over each of the images\nand produces an embedding.",
    "start": "637150",
    "end": "644710"
  },
  {
    "text": "You'd perform the distance\nmeasure over that embedding, and then the loss is used\nto train that models--",
    "start": "644710",
    "end": "652300"
  },
  {
    "text": "the embeddings that it produces. Yeah. So I think that you're kind\nof getting at the right idea.",
    "start": "652300",
    "end": "660240"
  },
  {
    "text": "Does anyone want to\nelaborate on that maybe? ",
    "start": "660240",
    "end": "669290"
  },
  {
    "text": "I'm thinking we can train\nmodels to directly predict whether two images\nbelong to the same class",
    "start": "669290",
    "end": "678130"
  },
  {
    "text": "for the output [INAUDIBLE]\ndirectly this distance metric. Yeah.",
    "start": "678130",
    "end": "683300"
  },
  {
    "text": "So that's actually the first\napproach that we'll talk about, which is that you can\nbasically train something",
    "start": "683300",
    "end": "688460"
  },
  {
    "text": "to compare images and tell you\nwhether or not those images are from the same class.",
    "start": "688460",
    "end": "693730"
  },
  {
    "text": "And so we can basically\nuse the meta-training data and learn this comparator\nthat tells us if something",
    "start": "693730",
    "end": "699380"
  },
  {
    "text": "is the same or not. So what this looks\nlike is, for example,",
    "start": "699380",
    "end": "704905"
  },
  {
    "text": "you could train\na Siamese network to predict whether two\nimages are of the same class. What this would look like\nis you take two images.",
    "start": "704905",
    "end": "711800"
  },
  {
    "text": "You pass that through\na network to produce kind of an embedding,\nand then you",
    "start": "711800",
    "end": "718070"
  },
  {
    "text": "have kind of an additional\nset of layers that output whether or not they are\nthe same class or not.",
    "start": "718070",
    "end": "724320"
  },
  {
    "text": "So for this pair, you\nwould train it to output 0 because they're a\ndifferent class.",
    "start": "724320",
    "end": "729950"
  },
  {
    "text": "For this pair of images, you\nwould tell it to output 1 because these both correspond\nto the class of bowls.",
    "start": "729950",
    "end": "735560"
  },
  {
    "text": "It looks like this slide\nisn't advancing on. Yeah, so in this case, you\nwould train it to output 1.",
    "start": "735560",
    "end": "742370"
  },
  {
    "text": "In this case, you would\ntrain it to output 0. In this case, you would\ntrain it to output 1. And so forth.",
    "start": "742370",
    "end": "748850"
  },
  {
    "text": "So essentially you're\nkind of training a network with your meta-training data\nto tell you whether or not two",
    "start": "748850",
    "end": "753970"
  },
  {
    "text": "images are from the same class. And then at meta-test time when\nyou want to predict the label,",
    "start": "753970",
    "end": "759880"
  },
  {
    "text": "what you can do is\nyou can compare each-- you compare your x test image\nto each of the images in D train",
    "start": "759880",
    "end": "767860"
  },
  {
    "text": "and predict the one that has--\npredict the label corresponding to the image that\nhas the highest",
    "start": "767860",
    "end": "773500"
  },
  {
    "text": "probability of corresponding\nto the same class as your test image. ",
    "start": "773500",
    "end": "781350"
  },
  {
    "text": "Cool. So this is kind of\nlike the first-- this was actually\nproposed in 2015",
    "start": "781350",
    "end": "787900"
  },
  {
    "text": "and was really one of\nthe first approaches to using neural networks in the\ncontext of few-shot learning.",
    "start": "787900",
    "end": "793380"
  },
  {
    "text": " Any questions on that?",
    "start": "793380",
    "end": "798470"
  },
  {
    "text": "I feel like I'm also\nbehind on the chat. ",
    "start": "798470",
    "end": "803972"
  },
  {
    "text": "I think the chat was mostly\nthinking about questions from-- or answers to the\nprevious question. So do you have a question?",
    "start": "803972",
    "end": "810610"
  },
  {
    "text": "Yes. So when we're thinking\nabout non-parametric, because a neural\nnetwork has parameters,",
    "start": "810610",
    "end": "816500"
  },
  {
    "text": "is that, like, not\ncounted in this case? Like, maybe if you can--",
    "start": "816500",
    "end": "823256"
  },
  {
    "text": "I was a bit confused. Yeah. So I guess I'm calling\nthis kind of a class of non-parametric methods\nor learners in the sense",
    "start": "823256",
    "end": "832520"
  },
  {
    "text": "that there are\nactually parameters, and these parameters are\nactually trained with the meta training set.",
    "start": "832520",
    "end": "837920"
  },
  {
    "text": " I like to call it kind of\na non-parametric method",
    "start": "837920",
    "end": "844990"
  },
  {
    "text": "in the sense that\nit really resembles a lot of the\nnon-parametric methods that we have in machine\nlearning like nearest",
    "start": "844990",
    "end": "850225"
  },
  {
    "text": "neighbors, k-nearest neighbors,\nand so forth, because we are kind of just\nlooking at the images",
    "start": "850225",
    "end": "856570"
  },
  {
    "text": "and literally, like in\nthis case literally doing nearest neighbors, but according\nto this meta-learned metric",
    "start": "856570",
    "end": "862210"
  },
  {
    "text": "of whether or not two\nimages are the same. So essentially you can view\nit as like a parametric meta-learner that's\nkind of telling you--",
    "start": "862210",
    "end": "869650"
  },
  {
    "text": "giving you a metric under\nwhich to compare images. And then once you\nhave this metric, you're using kind of a\nnon-parametric approach",
    "start": "869650",
    "end": "875797"
  },
  {
    "text": "to predict the label\nfor the test input. Oh, OK.",
    "start": "875797",
    "end": "881313"
  },
  {
    "text": "So the non-parametric\naspect of it is where we actually\npredicted from, like, the final level of testing.",
    "start": "881313",
    "end": "888740"
  },
  {
    "text": "Yeah. Yeah. So you can sort of view it\nas like the inner loop being non-parametric and the\nouter loop being parametric.",
    "start": "888740",
    "end": "894220"
  },
  {
    "text": "Of course, the model that\nthe outer loop produces is used in the inner\nloop, so it potentially",
    "start": "894220",
    "end": "900070"
  },
  {
    "text": "could be viewed as like a\nsemi-parametric approach. But I think you have the idea. OK. Cool. Thanks.",
    "start": "900070",
    "end": "905400"
  },
  {
    "text": " Yeah. And so Neal was asking,\ndoes the distance layer",
    "start": "905400",
    "end": "913050"
  },
  {
    "text": "use a specific metric,\nor does it learn metrics from the training data? So in this case,\nthis network here",
    "start": "913050",
    "end": "919990"
  },
  {
    "text": "which is essentially kind\nof learning a sort of metric although it may not kind of\nobey the triangle inequality,",
    "start": "919990",
    "end": "927089"
  },
  {
    "text": "but in any case, it's\nlearning something that makes comparisons\nbetween these two images.",
    "start": "927090",
    "end": "932399"
  },
  {
    "text": "And we train this network using\nall of the meta-training data, all of the image classes\nin our meta-training data,",
    "start": "932400",
    "end": "937500"
  },
  {
    "text": "and then when we're\ngiven new image classes, you can still expect this model\nto generalize, because it's simply telling you, for\na new pair of images,",
    "start": "937500",
    "end": "944649"
  },
  {
    "text": "whether or not they're\nthe same class. And you follow this\nkind of procedure down here to make a prediction\nfor x test and meta testing.",
    "start": "944650",
    "end": "952670"
  },
  {
    "text": " OK.",
    "start": "952670",
    "end": "958020"
  },
  {
    "text": "So this is kind of the key idea\nbehind this initial approach. Now in general, this\napproach works pretty well.",
    "start": "958020",
    "end": "967260"
  },
  {
    "text": "It's quite simple,\nwhich is really nice. But one downside\nof this approach is that, during\nmeta training time,",
    "start": "967260",
    "end": "973110"
  },
  {
    "text": "you're basically training it\nfor this binary classification problem of whether or not these\ntwo images are the same class.",
    "start": "973110",
    "end": "980280"
  },
  {
    "text": "But at meta test time, you're\ndoing this N-way classification where you're comparing\nthe test image to all",
    "start": "980280",
    "end": "986766"
  },
  {
    "text": "of the images in the\ntraining data set and outputting the corresponding\nlabel within the n classes.",
    "start": "986767",
    "end": "993000"
  },
  {
    "text": "And so we have this\ndiscrepancy between what we're meta-training for and\nwhat we're meta testing for.",
    "start": "993000",
    "end": "999930"
  },
  {
    "text": "So one thing that we would like\nto do to improve this approach is to try to match what's\nhappening at meta-training time",
    "start": "999930",
    "end": "1007150"
  },
  {
    "text": "and meta-testing time. So this is kind of the key\nidea behind this work called",
    "start": "1007150",
    "end": "1016430"
  },
  {
    "text": "matching networks. And essentially\nyou could view it",
    "start": "1016430",
    "end": "1021910"
  },
  {
    "text": "as learning in embedding space,\nand doing nearest neighbors in that embedding space,\nand then optimizing",
    "start": "1021910",
    "end": "1027400"
  },
  {
    "text": "for a model such that\nnearest neighbors outputs the correct answer.",
    "start": "1027400",
    "end": "1033099"
  },
  {
    "text": "So what this looks\nlike is something like this, where all\nthe images on the left are the training data\npoints for your task.",
    "start": "1033099",
    "end": "1040849"
  },
  {
    "text": "And this is the\ntest input, x test. You pass all of\nthese examples in.",
    "start": "1040849",
    "end": "1046929"
  },
  {
    "text": "You basically get embeddings for\neach of your training examples. You also get an embedding\nfor your test example.",
    "start": "1046930",
    "end": "1054220"
  },
  {
    "text": "And then this operation\ncomputes something",
    "start": "1054220",
    "end": "1060010"
  },
  {
    "text": "like the kind of negative\ndistance between your test embedding and each of\nyour training embeddings.",
    "start": "1060010",
    "end": "1067470"
  },
  {
    "text": "And then once you have\ncompared these and kind of have an estimate\nfor which of these",
    "start": "1067470",
    "end": "1075030"
  },
  {
    "text": "is the most similar to your test\nexample, you then use this to--",
    "start": "1075030",
    "end": "1080130"
  },
  {
    "text": "use this estimate\nand dot product it with the labels of each\nof your training examples to produce the\ncorresponding label.",
    "start": "1080130",
    "end": "1086580"
  },
  {
    "text": "So for example, in\nthis one when you compare the embedding\nof the dog here",
    "start": "1086580",
    "end": "1092437"
  },
  {
    "text": "and the embedding\nof the test example, it finds that kind of\nhas the highest weight, or is the most likely to match.",
    "start": "1092437",
    "end": "1100020"
  },
  {
    "text": "And then when you\ntake the dot product, it will output the corresponding\nlabel for that dog,",
    "start": "1100020",
    "end": "1105990"
  },
  {
    "text": "because that has the-- for the red label, because\nthat has the closest image.",
    "start": "1105990",
    "end": "1112300"
  },
  {
    "text": "So kind of\nmathematically what this looks like is you can view\neach of these black dots",
    "start": "1112300",
    "end": "1118930"
  },
  {
    "text": "as this function that tells you\nthe similarity between x test",
    "start": "1118930",
    "end": "1124540"
  },
  {
    "text": "and each of your\ntraining examples xk. These are each of the\nlabels yk for each",
    "start": "1124540",
    "end": "1129970"
  },
  {
    "text": "of your training examples. And then your\noutput y hat test is",
    "start": "1129970",
    "end": "1136240"
  },
  {
    "text": "basically equal\nto the kind of dot",
    "start": "1136240",
    "end": "1141340"
  },
  {
    "text": "product between your\nsimilarities and your labels.",
    "start": "1141340",
    "end": "1147583"
  },
  {
    "text": "So you can essentially\nview these similarities as probabilities over which\nexample it thinks it is.",
    "start": "1147583",
    "end": "1152720"
  },
  {
    "text": "And once you take these\nprobabilities and output it by the corresponding\nlabel, you then get a probability distribution\nover what you think",
    "start": "1152720",
    "end": "1159743"
  },
  {
    "text": "is the label for\nthis test example. ",
    "start": "1159743",
    "end": "1166659"
  },
  {
    "text": "Cool. And then, I guess, I don't want\nto go into the architecture too much-- but a couple\nkind of general things",
    "start": "1166660",
    "end": "1173470"
  },
  {
    "text": "about this architecture is that\nit uses a convolutional encoder to embed the images\ninto these vectors.",
    "start": "1173470",
    "end": "1179980"
  },
  {
    "text": "And for the embeddings\nof the training examples, it actually isn't just\na feedforward model. They actually use a\nbidirectional LSTM.",
    "start": "1179980",
    "end": "1186490"
  },
  {
    "text": "So the embedding of\neach example is actually also affected by the other\nimages in the training dataset.",
    "start": "1186490",
    "end": "1195150"
  },
  {
    "text": "And this whole architecture\nis trained with respect to the accuracy on\nyour test example.",
    "start": "1195150",
    "end": "1203450"
  },
  {
    "text": "OK. Do you have a question? Yes. So for the embedding\ndo you need--",
    "start": "1203450",
    "end": "1209280"
  },
  {
    "text": "do you use a pre-trained model,\nor this is trained end to end? It's all trained end to end.",
    "start": "1209280",
    "end": "1215470"
  },
  {
    "text": "You can kind of pre-train\nsome of these modules with, like, ImageNet\nor something. But all of it is\ntrained end to end",
    "start": "1215470",
    "end": "1223150"
  },
  {
    "text": "with respect to the few-shot\nclassification prediction here. It also looks like\nthere's a question",
    "start": "1223150",
    "end": "1229550"
  },
  {
    "text": "in the chat saying, how is\nh theta and g theta related? I think that they're\njust kind of denoting",
    "start": "1229550",
    "end": "1236060"
  },
  {
    "text": "these as basically theta\nover the parameters",
    "start": "1236060",
    "end": "1241265"
  },
  {
    "text": "of the entire meta-learner. And these are just kind of\ntwo completely separate models that use different\nparts of theta",
    "start": "1241265",
    "end": "1247490"
  },
  {
    "text": "to make their\ncorresponding prediction.  You have a question?",
    "start": "1247490",
    "end": "1253870"
  },
  {
    "text": "Yeah. Just on in the g of theta part,\nwhich network comes first? The LSTM comes first or the CNN?",
    "start": "1253870",
    "end": "1263519"
  },
  {
    "text": "I think it's a convoluted-- I think they first\nkind of use a CNN, and then they do a\nbidirectional LSTM",
    "start": "1263520",
    "end": "1269490"
  },
  {
    "text": "on top of what the convolution\nnet layers produce. You can check the\npaper for details.",
    "start": "1269490",
    "end": "1274530"
  },
  {
    "text": "Do you have any\nintuition behind this? Yeah. So LSTMs typically\noperate on kind",
    "start": "1274530",
    "end": "1282559"
  },
  {
    "text": "of low dimensional vectors. And so if you took, like, the\nstandard LSTM and applied it",
    "start": "1282560",
    "end": "1288313"
  },
  {
    "text": "on top of the\nimages, you actually need to flatten the\nimages before passing it into the LSTM, which wouldn't\nleverage the spatial structure",
    "start": "1288313",
    "end": "1294660"
  },
  {
    "text": "of the image. So that's kind of one\nreason why it may make sense to put the convolutions first.",
    "start": "1294660",
    "end": "1299970"
  },
  {
    "text": "That said, there\nis an architecture. There's kind of a version of\nan LSTM called a ConvLSTM that",
    "start": "1299970",
    "end": "1305490"
  },
  {
    "text": "basically replaces all\nthe matrix multiplies with convolution operations. And you could imagine\ndoing something like that",
    "start": "1305490",
    "end": "1311910"
  },
  {
    "text": "instead of doing\nconvolution and then LSTM. Thank you. ",
    "start": "1311910",
    "end": "1318170"
  },
  {
    "text": "OK.  Yes. This looks a lot like\nan attention mechanism",
    "start": "1318170",
    "end": "1325095"
  },
  {
    "text": "in a lot of ways, so I'm curious\nhow well in practice this really generalizes\nto unseen data and how much this\nis just learning",
    "start": "1325095",
    "end": "1331440"
  },
  {
    "text": "things, like for this example,\nfeatures endemic to dogs? ",
    "start": "1331440",
    "end": "1338025"
  },
  {
    "text": "Yeah.  Let's see. So it does actually draw a lot\nof similarities to attention.",
    "start": "1338025",
    "end": "1344530"
  },
  {
    "text": "This was back in\n2016, and I don't even know if attention had really\nbeen really thoroughly defined,",
    "start": "1344530",
    "end": "1351539"
  },
  {
    "text": "or wasn't very popular\nat least, at that point. And maybe if this paper\nwas introduced today,",
    "start": "1351540",
    "end": "1356730"
  },
  {
    "text": "they would have called it\nsomething more like attention. This operation right here\ncertainly, in particular, looks a lot like\nattention, because you're",
    "start": "1356730",
    "end": "1363250"
  },
  {
    "text": "kind of comparing\nthe embedding to each of the other embeddings. In general, it does\nactually generalize.",
    "start": "1363250",
    "end": "1370033"
  },
  {
    "text": "These kinds of\narchitectures do actually usually generalize pretty well. ",
    "start": "1370033",
    "end": "1376029"
  },
  {
    "text": "In practice, I think because\nof kind of the structure that's built in, we'll also introduce\none more approach that",
    "start": "1376030",
    "end": "1382600"
  },
  {
    "text": "builds upon this idea that\nis kind of one of the more, I think, de facto models in kind\nof the non-parametric few-shot",
    "start": "1382600",
    "end": "1389795"
  },
  {
    "text": "classification realm.  Also [AUDIO OUT] was asking,\nare h theta and g theta",
    "start": "1389795",
    "end": "1396840"
  },
  {
    "text": "the same network or different? These are kind of\ndifferent networks. In this in particular,\nit's just kind",
    "start": "1396840",
    "end": "1402420"
  },
  {
    "text": "of a feedforward\nconvolutional model. And this model is a\nbidirectional LSTM.",
    "start": "1402420",
    "end": "1408750"
  },
  {
    "text": "I think [AUDIO OUT]\nare some weights between this\nconvolutional encoder",
    "start": "1408750",
    "end": "1414840"
  },
  {
    "text": "and kind of the\nencoder of this one, but I can't remember exactly. You can check the\npaper for details.",
    "start": "1414840",
    "end": "1421990"
  },
  {
    "text": "OK.  Cool. And then [AUDIO OUT]\nis asking, what",
    "start": "1421990",
    "end": "1428250"
  },
  {
    "text": "are the drawbacks of letting\nthem share the same network? I think that there are\nbenefits to parameter sharing",
    "start": "1428250",
    "end": "1435019"
  },
  {
    "text": "in general because it may\nincrease the efficiency. It may also be that\nit's helpful to have",
    "start": "1435020",
    "end": "1440929"
  },
  {
    "text": "a different encoder for\nthe training examples and for the test examples. For things like edge detectors,\nthen parameter sharing",
    "start": "1440930",
    "end": "1447590"
  },
  {
    "text": "makes a lot of sense,\nbut maybe there's some other features\nthat may want to encode differently in the\ntraining examples in the test",
    "start": "1447590",
    "end": "1452840"
  },
  {
    "text": "example. I will take one more\nquestion from [AUDIO OUT]..",
    "start": "1452840",
    "end": "1459370"
  },
  {
    "text": "Hi. So is this still going\nto be trained in, like, a true process\nwhere you back",
    "start": "1459370",
    "end": "1466750"
  },
  {
    "text": "propagate the performance\non the meta test set or, like the third query part\nof each task in the [INAUDIBLE]??",
    "start": "1466750",
    "end": "1476429"
  },
  {
    "text": "Yeah. So basically you'll\nstill-- well, we'll go over the\nfull procedure. But you will back propagate\nand treat all of this",
    "start": "1476430",
    "end": "1482710"
  },
  {
    "text": "with respect to how\naccurate y test, y hat test is-- basically, how\naccurate it's able to classify",
    "start": "1482710",
    "end": "1489490"
  },
  {
    "text": "this test image. Cool, thanks.",
    "start": "1489490",
    "end": "1494620"
  },
  {
    "text": "Cool. And then I guess one\nthing to highlight here is, unlike\nSiamese networks, now meta-training and\nmeta testing match,",
    "start": "1494620",
    "end": "1500659"
  },
  {
    "text": "because you're actually\nmeta-training it for kind of n way\nclassification,",
    "start": "1500660",
    "end": "1506387"
  },
  {
    "text": "whereas Siamese networks\nis just training it for two-way classification.",
    "start": "1506387",
    "end": "1512740"
  },
  {
    "text": "What the algorithm\nlooks like is basically pretty similar to the\nkind of general framework",
    "start": "1512740",
    "end": "1519820"
  },
  {
    "text": "that we've gone through before. What's going to\nbe a bit different is, instead of computing\nthese parameters,",
    "start": "1519820",
    "end": "1526210"
  },
  {
    "text": "we're going to be computing the\nprediction for the test input.",
    "start": "1526210",
    "end": "1531340"
  },
  {
    "text": "This kind of illustrates\nhow it's somewhat of a non-parametric approach. We're not actually computing\nany parameters of any model.",
    "start": "1531340",
    "end": "1537700"
  },
  {
    "text": "You compute the prediction\non the test input. You can also view this as\nhaving the parameters sort",
    "start": "1537700",
    "end": "1542770"
  },
  {
    "text": "of integrated out. And then we'll update the\nkind of meta parameters",
    "start": "1542770",
    "end": "1549160"
  },
  {
    "text": "of this overall model using the\nprediction error on your test",
    "start": "1549160",
    "end": "1554420"
  },
  {
    "text": "inputs. ",
    "start": "1554420",
    "end": "1560000"
  },
  {
    "text": "OK, great. So what we just talked about\nis called matching networks.",
    "start": "1560000",
    "end": "1567380"
  },
  {
    "text": "It's a pretty popular approach\nfor few-shot learning. Now when we think about what\nkind of one more approach that",
    "start": "1567380",
    "end": "1574100"
  },
  {
    "text": "builds upon this, one important\nquestion to think about is, what if you have\nmore than one shot?",
    "start": "1574100",
    "end": "1580760"
  },
  {
    "text": "What if you have more than\none example per class? And in particular, if we go\nback to this architecture,",
    "start": "1580760",
    "end": "1590430"
  },
  {
    "text": "does anyone have\nany ideas for what you might do with this\narchitecture or kind of how",
    "start": "1590430",
    "end": "1596730"
  },
  {
    "text": "this architecture handles\nthe case where you have more than one example per class?",
    "start": "1596730",
    "end": "1602830"
  },
  {
    "text": "So maybe you have two dogs\nof this breed, for example. How do you think that this\nmodel handles that case?",
    "start": "1602830",
    "end": "1608370"
  },
  {
    "start": "1608370",
    "end": "1623840"
  },
  {
    "text": "Yeah. [AUDIO OUT] is asking, take\nthe sum of all examples. So one thing that you could do\nis, instead of-- so basically",
    "start": "1623840",
    "end": "1631570"
  },
  {
    "text": "when you have two examples\nof this dog, this embedding, for example, you can have\njust kind of one embedding for that kind of breed.",
    "start": "1631570",
    "end": "1638612"
  },
  {
    "text": "This is actually hinting\nat what we'll talk about in the next approach. ",
    "start": "1638612",
    "end": "1645260"
  },
  {
    "text": "In this matching\nnetworks paper though, the way that they\nhandled the setting where you had more\nthan one examples is they just treated them\ncompletely independently.",
    "start": "1645260",
    "end": "1652620"
  },
  {
    "text": "So it just took-- if you had two examples\nof this other dog, it would also pass kind of\nthe example of that dog breed,",
    "start": "1652620",
    "end": "1659780"
  },
  {
    "text": "create another embedding\nvector for that dog breed so it had two embeddings\nfor that label, and then it would add kind\nof an additional label here.",
    "start": "1659780",
    "end": "1667300"
  },
  {
    "text": "It would basically treat them,\nlike, independently almost as if they were different classes.",
    "start": "1667300",
    "end": "1674167"
  },
  {
    "text": "So this is kind of\nthe approach that I think it's mentioning where\nyou just basically feed all the images in\nand also feed all of the corresponding\nlabels, and you",
    "start": "1674167",
    "end": "1682380"
  },
  {
    "text": "don't do anything special\nto aggregate information across a class. ",
    "start": "1682380",
    "end": "1688799"
  },
  {
    "text": "And this is kind of\npotentially a bit of a downside because it doesn't really\naggregate class information",
    "start": "1688800",
    "end": "1696510"
  },
  {
    "text": "across images. And the aggregated information\nmay be more useful,",
    "start": "1696510",
    "end": "1701700"
  },
  {
    "text": "because maybe, kind of, the\ntest examples you'll see may have features\npresent in one example and some features present\nin another example.",
    "start": "1701700",
    "end": "1708160"
  },
  {
    "text": "And if you don't aggregate\nthat information, it may not be able to accurately\npredict the label for that test",
    "start": "1708160",
    "end": "1716140"
  },
  {
    "text": "example.  And so this is kind of\nthe key idea behind what's",
    "start": "1716140",
    "end": "1721730"
  },
  {
    "text": "called prototypical networks. And what it does is each of\nthese colored dots corresponds",
    "start": "1721730",
    "end": "1728390"
  },
  {
    "text": "to the different examples in\nd train embedded into kind",
    "start": "1728390",
    "end": "1735190"
  },
  {
    "text": "of a low dimensional space. And instead of treating\nthese examples completely independently, which is what\nmatching networks would do,",
    "start": "1735190",
    "end": "1742310"
  },
  {
    "text": "it takes the average of all\nthe examples of the same class. And then when it does\nnearest neighbors, instead",
    "start": "1742310",
    "end": "1749080"
  },
  {
    "text": "of doing nearest neighbors\nto kind of each of these, it does nearest neighbors to\nthe prototypical embedding of each of those classes.",
    "start": "1749080",
    "end": "1755720"
  },
  {
    "text": "So all these green\nexamples correspond to c1, all the blue examples\ncorrespond to c3, and so forth.",
    "start": "1755720",
    "end": "1760810"
  },
  {
    "text": "And you average the\nembeddings to create kind of a prototypical\nembedding for that class",
    "start": "1760810",
    "end": "1766512"
  },
  {
    "text": "and then do nearest\nneighbors with regard to the prototypical embedding.",
    "start": "1766512",
    "end": "1771945"
  },
  {
    "text": "So mathematically\nwhat this looks like is you compute each\nof these centroids cn",
    "start": "1771945",
    "end": "1778750"
  },
  {
    "text": "by averaging all of the\nembeddings for the data",
    "start": "1778750",
    "end": "1783940"
  },
  {
    "text": "points in that class-- basically for all the embeddings\nwith the label equal to n.",
    "start": "1783940",
    "end": "1791750"
  },
  {
    "text": "And then when you want\nto compute the label for a new test input\nx, you then look",
    "start": "1791750",
    "end": "1798920"
  },
  {
    "text": "at the distance between\nyour test example and each of these centroids,\nand then to get a probability,",
    "start": "1798920",
    "end": "1805740"
  },
  {
    "text": "you exponentiation the negative\ndistance and then normalize. ",
    "start": "1805740",
    "end": "1811580"
  },
  {
    "text": "OK. And then this distance\nfunction d, it could be-- in this paper, they looked\nat the Euclidean distance, or a cosine distance.",
    "start": "1811580",
    "end": "1817929"
  },
  {
    "text": "There are other kind of metrics\nthat you could use as well. Crucially, though,\nthis distance metric",
    "start": "1817930",
    "end": "1823210"
  },
  {
    "text": "is inside this embedding space. It's not in the original space. ",
    "start": "1823210",
    "end": "1829420"
  },
  {
    "text": "OK. Any questions on how this works? ",
    "start": "1829420",
    "end": "1838388"
  },
  {
    "text": "Do you have a question, or\nis your hand up from before? ",
    "start": "1838388",
    "end": "1847920"
  },
  {
    "text": "OK, sounds good. I'll take that as that\nwas clear hopefully. What's your question?",
    "start": "1847920",
    "end": "1853150"
  },
  {
    "text": " I'm just curious about\nhow this method compares",
    "start": "1853150",
    "end": "1859520"
  },
  {
    "text": "to previous methods-- other non-parametric methods. ",
    "start": "1859520",
    "end": "1866120"
  },
  {
    "text": "So this model was able\nto show improvements over matching networks\nparticularly in the case",
    "start": "1866120",
    "end": "1871520"
  },
  {
    "text": "where you have\nmore than one shot. If you only have one shot, then\nthey are basically the same",
    "start": "1871520",
    "end": "1877070"
  },
  {
    "text": "because the embedding will\nbe the same as kind of-- it'll just be the kind of\nthat one example's embedding.",
    "start": "1877070",
    "end": "1885980"
  },
  {
    "text": "It also shows kind of\nsignificant improvement. Both of these, both\nmatching networks and prototypical networks,\nshow significant improvements",
    "start": "1885980",
    "end": "1891559"
  },
  {
    "text": "over Siamese networks.  Yeah.",
    "start": "1891560",
    "end": "1896580"
  },
  {
    "text": "Just curious if people have\ncreated better strategies than averaging the\nembeddings from train set.",
    "start": "1896580",
    "end": "1905040"
  },
  {
    "text": "Yeah. So this kind of gets\nactually at my next point,",
    "start": "1905040",
    "end": "1911040"
  },
  {
    "text": "which is that maybe\naveraging isn't really the best way to look at things.",
    "start": "1911040",
    "end": "1916549"
  },
  {
    "text": "And what if you have kind of\nmore complex relationships between different data\npoints of the same class?",
    "start": "1916550",
    "end": "1921830"
  },
  {
    "text": "And there are a few different\napproaches that do this. So for example, I\nguess this isn't really",
    "start": "1921830",
    "end": "1929059"
  },
  {
    "text": "replacing the\naverage, but this is learning a distance in\nprototypical networks",
    "start": "1929060",
    "end": "1935060"
  },
  {
    "text": "rather than using Euclidean, or\ncosine distance, or something. It's actually learning\na neural network",
    "start": "1935060",
    "end": "1940220"
  },
  {
    "text": "to produce the distance.  In general, I think\nthat averaging",
    "start": "1940220",
    "end": "1945440"
  },
  {
    "text": "is a reasonable choice,\nbecause oftentimes the encoder of these\nembeddings is already",
    "start": "1945440",
    "end": "1951317"
  },
  {
    "text": "doing something more\nsophisticated than just a feedforward model. It may already--\nlike, for example,",
    "start": "1951317",
    "end": "1956870"
  },
  {
    "text": "if it's a bidirectional\nLSTM, it's already taking into account\nsome of the differences between these images when it\nproduces the embedding space.",
    "start": "1956870",
    "end": "1964410"
  },
  {
    "text": "So typically just\naveraging I think is pretty reasonable,\nalthough you may want to learn [AUDIO OUT] this.",
    "start": "1964410",
    "end": "1970880"
  },
  {
    "text": "One setting where\naveraging may not work is if you have kind of a\npretty diverse variation within a class.",
    "start": "1970880",
    "end": "1977040"
  },
  {
    "text": "So for example,\nif you're in the--",
    "start": "1977040",
    "end": "1982310"
  },
  {
    "text": "let's see, what's a\ngood example of this? Maybe if kind of\nyour class is like,",
    "start": "1982310",
    "end": "1988190"
  },
  {
    "text": "is this a person or an animal? Maybe kind of people\nlook very different, and so you may want to have\ndifferent representations",
    "start": "1988190",
    "end": "1994860"
  },
  {
    "text": "of things. Or maybe animals\nlook very different, and so you may want to have\nkind of different, multiple",
    "start": "1994860",
    "end": "2001580"
  },
  {
    "text": "prototypes for different\nkinds of a more coarse class.",
    "start": "2001580",
    "end": "2006890"
  },
  {
    "text": "And so this paper allows you to\nlearn a mixture of prototypes for each class rather than\njust having a single prototype.",
    "start": "2006890",
    "end": "2014400"
  },
  {
    "text": "So this is an alternative\nto just using averaging. ",
    "start": "2014400",
    "end": "2019730"
  },
  {
    "text": "And then a third\napproach is to-- if you have even more\ncomplex relationships",
    "start": "2019730",
    "end": "2026240"
  },
  {
    "text": "between the different\nclasses, there's also another approach that\nuses graph convolutions to essentially do message\npassing on the embeddings,",
    "start": "2026240",
    "end": "2033452"
  },
  {
    "text": "where you have a\nset of embeddings, and then you pass\nmessages kind of loosely",
    "start": "2033452",
    "end": "2038880"
  },
  {
    "text": "between those\ndifferent embeddings so you refine your\nrepresentation of each class.",
    "start": "2038880",
    "end": "2044190"
  },
  {
    "text": " What's your question? I feel like-- So\ndoes it make sense",
    "start": "2044190",
    "end": "2050530"
  },
  {
    "text": "to learn the distance embedding\nonce a input has, like, a bimodal or multimodal\ndistribution,",
    "start": "2050530",
    "end": "2056210"
  },
  {
    "text": "because [INAUDIBLE] model? Yeah. So I see view there\nbeing two design choices.",
    "start": "2056210",
    "end": "2063138"
  },
  {
    "text": "One is, like, when you\nhave a set of embeddings for all the examples\nin your class,",
    "start": "2063139",
    "end": "2068739"
  },
  {
    "text": "how do you choose\nthe prototypes? And one way might be to have\nkind of multiple prototypes using a mixture.",
    "start": "2068739",
    "end": "2074500"
  },
  {
    "text": "And then the second question\nis, how do you kind of derive your distance function?",
    "start": "2074500",
    "end": "2079629"
  },
  {
    "text": "And this method learns\nthe distance function in prototypical networks.",
    "start": "2079630",
    "end": "2084759"
  },
  {
    "text": "And you could also kind of\ncombine these two ideas. I can't remember\nexactly what distance",
    "start": "2084760",
    "end": "2090250"
  },
  {
    "text": "function they used here. It looks like it's just\nsomething like Euclidean, for example, with the, sort of,\neach of the prototype centers.",
    "start": "2090250",
    "end": "2097030"
  },
  {
    "text": " So I think that both of those\nare kind of design choices",
    "start": "2097030",
    "end": "2103029"
  },
  {
    "text": "that come up in these methods. And the correct way is more\nthan just a graph network. Right?",
    "start": "2103030",
    "end": "2108400"
  },
  {
    "text": "A graph convolution when they\nare learning this distance metric here? Yeah. ",
    "start": "2108400",
    "end": "2118350"
  },
  {
    "text": "And then [AUDIO OUT]\nasked in the chat, here it seems the embeddings\nfrom the same class cluster",
    "start": "2118350",
    "end": "2125280"
  },
  {
    "text": "together. Is that the general case? So yeah. So in the slide that I was\nshowing before, in general,",
    "start": "2125280",
    "end": "2131940"
  },
  {
    "text": "it will kind of cluster together\nin your embedding space, because that's kind of\nthe right thing to do. But in applications\nlike the ones",
    "start": "2131940",
    "end": "2137730"
  },
  {
    "text": "right here, they may\nnot cluster together. And in those kinds\nof situations if you have a more multimodal\ndistribution within a class,",
    "start": "2137730",
    "end": "2144420"
  },
  {
    "text": "it may make sense to\nhave multiple prototypes within that class. ",
    "start": "2144420",
    "end": "2151980"
  },
  {
    "text": "OK, cool. So now let's move on\nand talk about how",
    "start": "2151980",
    "end": "2158290"
  },
  {
    "text": "we might apply these methods\nto a practical problem.",
    "start": "2158290",
    "end": "2163360"
  },
  {
    "text": "And in particular, we'll\nlook at this paper which uses a variant on\nprototypical networks",
    "start": "2163360",
    "end": "2169440"
  },
  {
    "text": "for dermatological\nimage classification. And this paper appeared\nat the Machine Learning",
    "start": "2169440",
    "end": "2176310"
  },
  {
    "text": "For Health Care\nconference in 2019 and also the NeurIPS Machine\nLearning For Health Care workshop in 2018.",
    "start": "2176310",
    "end": "2182040"
  },
  {
    "text": "And there's also a\nlink to the paper if you're interested in\nlooking at it yourself. And the problem that\nthey wanted to look at",
    "start": "2182040",
    "end": "2188600"
  },
  {
    "text": "was few-shot learning\nfor diagnosing different\ndermatological diseases.",
    "start": "2188600",
    "end": "2195180"
  },
  {
    "text": "So different skin conditions. And this problem has a number\nof different challenges.",
    "start": "2195180",
    "end": "2200560"
  },
  {
    "text": "So first, it tends to be\npretty hard to get data, because the data\nis fairly personal.",
    "start": "2200560",
    "end": "2208670"
  },
  {
    "text": "And also the data is\nquite long-tailed, so there's a lot of\ndifferent conditions that are very common. And there are some skin\nconditions that are very rare.",
    "start": "2208670",
    "end": "2217300"
  },
  {
    "text": "And then lastly, there's\nalso significant intra-class variability. So depending on where\nthe skin condition",
    "start": "2217300",
    "end": "2223350"
  },
  {
    "text": "is occurring on the skin or\ndepending on the skin type",
    "start": "2223350",
    "end": "2230512"
  },
  {
    "text": "and so forth, you may\nhave a lot of variability within a single disease. And then the goal\nof this paper was",
    "start": "2230512",
    "end": "2236250"
  },
  {
    "text": "to try to acquire an accurate\nclassifier on all classes.",
    "start": "2236250",
    "end": "2241625"
  },
  {
    "text": "So essentially,\ndifferent classes are going to correspond\nto different diseases. And they want to be able\nto classify which disease",
    "start": "2241625",
    "end": "2247290"
  },
  {
    "text": "a condition corresponds to-- which disease a patch\nof skin corresponds to,",
    "start": "2247290",
    "end": "2254040"
  },
  {
    "text": "among all of the classes\nin their training data set. And in particular, they\nlooked at the DermNet data",
    "start": "2254040",
    "end": "2259320"
  },
  {
    "text": "set, which is visualized here. And in particular,\nwhat this is showing",
    "start": "2259320",
    "end": "2265650"
  },
  {
    "text": "is that this is showing that\nonly the top 200 classes",
    "start": "2265650",
    "end": "2271160"
  },
  {
    "text": "where you see this really\nlong tail distribution where, for some of the classes,\nlike eczema and acne,",
    "start": "2271160",
    "end": "2276290"
  },
  {
    "text": "there's a very large number\nof examples with maybe 1,000 or 500 examples.",
    "start": "2276290",
    "end": "2282440"
  },
  {
    "text": "And then there's\nthis hugely long tail where there's many, many\nconditions that have,",
    "start": "2282440",
    "end": "2288560"
  },
  {
    "text": "like, less than 50\nexamples, for example. And there are\nactually more classes. I think there are up to--\nthere may be up to 600 classes",
    "start": "2288560",
    "end": "2295760"
  },
  {
    "text": "total in this data set. And even when you only look\nat the first 200 classes, you still see this\npretty dramatic tail.",
    "start": "2295760",
    "end": "2302390"
  },
  {
    "text": " And beyond kind of\ndermatology, these sorts",
    "start": "2302390",
    "end": "2309077"
  },
  {
    "text": "of long-tailed distributions\ncome up all the time, so this is a\nproblem that's quite relevant in practical machine\nlearning applications.",
    "start": "2309077",
    "end": "2317599"
  },
  {
    "text": "OK, so their goal was to acquire\nbasically a 200-way classifier for these different diseases.",
    "start": "2317600",
    "end": "2324510"
  },
  {
    "text": "And the way that\nthey formulated this is that different image\nclasses correspond to different diseases. And they took 150 base\nclasses that they're",
    "start": "2324510",
    "end": "2332490"
  },
  {
    "text": "going to use for meta-training\nand 50 novel classes that they're going\nto hold out and try",
    "start": "2332490",
    "end": "2338460"
  },
  {
    "text": "to be able to generalize to just\nfrom a small amount of data. And in particular, they\ntook the base classes are",
    "start": "2338460",
    "end": "2345040"
  },
  {
    "text": "the ones with the most\ndata, and the novel classes are the ones where you only\nhave a small number of examples, which is kind of a natural\nway to split the data.",
    "start": "2345040",
    "end": "2353690"
  },
  {
    "text": "And then at meta-test\ntime, they wanted to get a classifier that can\nclassify among all 200 classes.",
    "start": "2353690",
    "end": "2360730"
  },
  {
    "text": "So unlike the Omniglot\nsetting that you're looking at in your homework,\nwhere you are doing, like, 10-way classification,\nfor example, here",
    "start": "2360730",
    "end": "2368240"
  },
  {
    "text": "they're actually going to be\nmeta-training with around, kind of, leveraging data\nfrom 150 base classes.",
    "start": "2368240",
    "end": "2375585"
  },
  {
    "text": "And if they actually\nwant a classifier, they can go beyond\n150 base classes.",
    "start": "2375585",
    "end": "2380839"
  },
  {
    "text": "And now one thing\nthat's worth noting here is that, kind of unlike\nthe approaches that we've seen so far with black-box\nand optimization-based",
    "start": "2380840",
    "end": "2387079"
  },
  {
    "text": "meta-learning non-parametric\nmethods can actually train for N-way\nclassification and test",
    "start": "2387080",
    "end": "2393800"
  },
  {
    "text": "for greater than\nN-way classification, because you can still\ncompute embeddings",
    "start": "2393800",
    "end": "2399290"
  },
  {
    "text": "for all of the examples. And you can still do this\nform of nearest neighbors, or kind of nearest neighbors to\nthe prototypes, for more than N",
    "start": "2399290",
    "end": "2409340"
  },
  {
    "text": "even if you've\nonly trained on N. So this is one of the\nkind of positive features",
    "start": "2409340",
    "end": "2415040"
  },
  {
    "text": "of these kinds of approaches. So that means that\nthey can actually get a 200-way\nclassifier after only",
    "start": "2415040",
    "end": "2421310"
  },
  {
    "text": "meta-training on a number of\nclasses that's less than that. ",
    "start": "2421310",
    "end": "2428119"
  },
  {
    "text": "So the approach\nthat they take is they take prototypical\nnetworks, and they expand on it in two different ways.",
    "start": "2428120",
    "end": "2433870"
  },
  {
    "text": "So first they learn multiple\nprototypes per class. So this is pretty similar\nto one of the papers",
    "start": "2433870",
    "end": "2439510"
  },
  {
    "text": "that I was mentioning\nbefore, where they learned a mixture of prototypes. And this allows them to handle\nintra-class variability.",
    "start": "2439510",
    "end": "2447910"
  },
  {
    "text": "And second, they also\nincorporate unlabeled support examples, where\nthey-- basically, support",
    "start": "2447910",
    "end": "2453880"
  },
  {
    "text": "examples where they\ndon't have labels for. For example, the query\nexamples by doing k-means",
    "start": "2453880",
    "end": "2460300"
  },
  {
    "text": "on top of the learned\nembedding space. And this allows them\nto kind of leverage these unlabeled examples\nto better predict",
    "start": "2460300",
    "end": "2469905"
  },
  {
    "text": "the query examples.  OK. And then one side\nnote is that, if you",
    "start": "2469905",
    "end": "2475890"
  },
  {
    "text": "do decide to kind\nof read this paper and look into some\nof the details, they actually flip the\nstandard notation of k and N",
    "start": "2475890",
    "end": "2481559"
  },
  {
    "text": "in the paper, which is a bit\nconfusing if you read it. But once you understand\nthat they're flipping this, it's hopefully a bit easier\nto kind of flip in your head",
    "start": "2481560",
    "end": "2489600"
  },
  {
    "text": "if you're used to N-way\nand K-shot classification. ",
    "start": "2489600",
    "end": "2494930"
  },
  {
    "text": "OK. And then in their\nevaluation, they compare it to standard\nprototypical networks, which is trained on 150 base classes.",
    "start": "2494930",
    "end": "2502280"
  },
  {
    "text": "It's also pre-trained\non ImageNet. With their method,\nthey also pre-trained",
    "start": "2502280",
    "end": "2507380"
  },
  {
    "text": "the prototypical clustering\nnetworks on ImageNet as well. They also compared to this just\nkind of fine-tuning approach,",
    "start": "2507380",
    "end": "2517000"
  },
  {
    "text": "where they took an\nImageNet-pretrained model. They fine-tuned it on N\nclasses, where N is varying.",
    "start": "2517000",
    "end": "2526569"
  },
  {
    "text": "N is either going\nto be 150 or 200. And then they ran one nearest\nneighbors or three nearest neighbors in the\nresulting embedding space.",
    "start": "2526570",
    "end": "2535369"
  },
  {
    "text": "And then lastly, they compared\nto an ImageNet pretrained model that's fine tuned actually\non all 200 classes",
    "start": "2535370",
    "end": "2542530"
  },
  {
    "text": "using balancing. And this last method is\na pretty strong baseline.",
    "start": "2542530",
    "end": "2547790"
  },
  {
    "text": "It accesses more\ninformation during training than both their approach as\nwell as these two baselines.",
    "start": "2547790",
    "end": "2554140"
  },
  {
    "text": "And also if you want to\nintroduce new classes into your data set,\nit requires completely retraining for\nthose new classes.",
    "start": "2554140",
    "end": "2561740"
  },
  {
    "text": "So in some ways, this\nmethod is a bit privileged. It has kind of--",
    "start": "2561740",
    "end": "2567495"
  },
  {
    "text": "it has kind of more information\nand requires more compute than the other methods.",
    "start": "2567495",
    "end": "2574470"
  },
  {
    "text": "And then they evaluated this\non mean class accuracy, which is just the average of each\nof the per class accuracies",
    "start": "2574470",
    "end": "2582180"
  },
  {
    "text": "across the 200 classes. And this treats\nall of the classes as equal even if\nsome of the classes",
    "start": "2582180",
    "end": "2588550"
  },
  {
    "text": "have more data\nthan other classes. And they kind of argued that\nthis is a reasonable thing",
    "start": "2588550",
    "end": "2594940"
  },
  {
    "text": "to do in this setting.  OK.",
    "start": "2594940",
    "end": "2600730"
  },
  {
    "text": "[AUDIO OUT] is asking, how\ndoes the embedding generalize on unseen classes? So basically, when you train\nthis encoder across 150 base",
    "start": "2600730",
    "end": "2611000"
  },
  {
    "text": "classes, you're\ntraining it to be able to produce\nembeddings that are distinguishable within\nthose 150 classes.",
    "start": "2611000",
    "end": "2618440"
  },
  {
    "text": "And then when you're\ngiven an unseen class, the hope is that\nencoder will also produce a distinguishable\nembedding for a new class.",
    "start": "2618440",
    "end": "2626569"
  },
  {
    "text": "And if you train\non enough classes, you can expect\nreasonable generalization",
    "start": "2626570",
    "end": "2632720"
  },
  {
    "text": "assuming that the new classes,\nthe new unseen classes, are not significantly far from\nthe distribution of images",
    "start": "2632720",
    "end": "2640550"
  },
  {
    "text": "that you saw previously.  OK.",
    "start": "2640550",
    "end": "2645800"
  },
  {
    "text": "So to finish up this\nkind of case study, these are what the\nresults look like.",
    "start": "2645800",
    "end": "2651180"
  },
  {
    "text": "So the first thing\nto note here is that their method that\nincorporates the unlabeled data",
    "start": "2651180",
    "end": "2657200"
  },
  {
    "text": "and also has multiple\nprototypes per class is able to do significantly\nbetter than just vanilla",
    "start": "2657200",
    "end": "2662720"
  },
  {
    "text": "prototypical networks,\nwhich is nice. It does better across both\nthe base classes as well as",
    "start": "2662720",
    "end": "2670790"
  },
  {
    "text": "the novel classes\nthat are held out. Second is that their method\nsignificantly outperforms--",
    "start": "2670790",
    "end": "2680850"
  },
  {
    "text": "this is at least on\nthe novel classes. So in general, on\nthe novel classes, PCN is able to\nsignificantly outperform",
    "start": "2680850",
    "end": "2688300"
  },
  {
    "text": "when you're fine tuning and\nthen doing nearest neighbors on the fine tuned embedding.",
    "start": "2688300",
    "end": "2693580"
  },
  {
    "text": "For the base classes, it does\nsimilarly and slightly worse.",
    "start": "2693580",
    "end": "2698930"
  },
  {
    "text": "Although in terms of the\noverall accuracy on both base and novel, it's able to do--",
    "start": "2698930",
    "end": "2705250"
  },
  {
    "text": "it's able to do better than\nthis kind of fine tuning nearest neighbors approach. And then lastly they\nfound that their kind",
    "start": "2705250",
    "end": "2712930"
  },
  {
    "text": "of prototypical\nclustering network is able to do about as well as\nthis kind of strong baseline",
    "start": "2712930",
    "end": "2719230"
  },
  {
    "text": "that's fine tuning\non all 200 classes even though PCN was only kind\nof trained on 150 classes.",
    "start": "2719230",
    "end": "2728560"
  },
  {
    "text": "So this is pretty\nnice in that they're able to do as well\nas this method without requiring retraining\nfor every new class",
    "start": "2728560",
    "end": "2734562"
  },
  {
    "text": "that you might want\nto add to this model. ",
    "start": "2734562",
    "end": "2740290"
  },
  {
    "text": "Great. And then they also have more\nvisualizations and analysis in the paper. ",
    "start": "2740290",
    "end": "2748090"
  },
  {
    "text": "Yeah. I'm just curious to\nknow does feed forward-- this fine tuning method that\nsees all the 200 classes.",
    "start": "2748090",
    "end": "2757000"
  },
  {
    "text": "Why that had, like, a\nvery low performance on the novel classes? Because in effect, the model has\nbeen trained on those classes",
    "start": "2757000",
    "end": "2763465"
  },
  {
    "text": "already, but the\n[AUDIO OUT] is very low compared to the base classes. Like, is there some explanation\nwhere it might be the case?",
    "start": "2763465",
    "end": "2773790"
  },
  {
    "text": "Yeah. So I think that my\nintuition for that is that basically,\nwhen you explicitly",
    "start": "2773790",
    "end": "2779480"
  },
  {
    "text": "train for a few-shot\ngeneralization, which is what meta-learning methods\ndo in general and also what prototypical\nnetworks and PCN does,",
    "start": "2779480",
    "end": "2788210"
  },
  {
    "text": "you're explicitly generalizing\nit to be able to produce an embedding that can\ngeneralize even when",
    "start": "2788210",
    "end": "2793400"
  },
  {
    "text": "that kind of prototype\nwas produced with only a few examples, whereas\nit's very likely, I think,",
    "start": "2793400",
    "end": "2800430"
  },
  {
    "text": "that this fine tuned model\nis probably overfitting when it only sees a few examples\nfor these novel classes.",
    "start": "2800430",
    "end": "2806529"
  },
  {
    "text": "Each of these novel\nclasses have, like, less than 50 examples total,\nso it's probably pretty easy",
    "start": "2806530",
    "end": "2812670"
  },
  {
    "text": "to overfit. And then also PCN introduces a\nway to leverage unlabeled data.",
    "start": "2812670",
    "end": "2819150"
  },
  {
    "text": "And that may be helping\nit to perform better than this method, which isn't\nable to leverage the unlabeled",
    "start": "2819150",
    "end": "2826460"
  },
  {
    "text": "data. Thank you. Also asking in the chat,\nwhy is the mean class",
    "start": "2826460",
    "end": "2832990"
  },
  {
    "text": "accuracy low, less than 50%? So first, this is kind of a\n200-way classification problem,",
    "start": "2832990",
    "end": "2840140"
  },
  {
    "text": "so random performance would\nbe at kind of 1 over 200,",
    "start": "2840140",
    "end": "2845200"
  },
  {
    "text": "so it is, like, substantially\nimproving over that at least. I also think that this task\nis quite a challenging task.",
    "start": "2845200",
    "end": "2852940"
  },
  {
    "text": "The amount of total data points\nper class is relatively low.",
    "start": "2852940",
    "end": "2858460"
  },
  {
    "text": "Even the most populous examples\nhad around 1,000 examples.",
    "start": "2858460",
    "end": "2864280"
  },
  {
    "text": "And once you get to\nthe novel examples, you have a very, very\nsmall amount of data that it has to learn from.",
    "start": "2864280",
    "end": "2869990"
  },
  {
    "text": "And so that would explain why\nthese numbers aren't, like, in the 90s, for example.",
    "start": "2869990",
    "end": "2875395"
  },
  {
    "text": " OK, great.",
    "start": "2875395",
    "end": "2882160"
  },
  {
    "text": "And then there is\none other question in the chat that was about\nnon-parametric few-shot",
    "start": "2882160",
    "end": "2888484"
  },
  {
    "text": "learners, which\nis that, can they be effective for\ntext classification? And yeah.",
    "start": "2888485",
    "end": "2893579"
  },
  {
    "text": "So the original\nmatching networks paper actually included experiments on\na text classification problem.",
    "start": "2893580",
    "end": "2900158"
  },
  {
    "text": "So I'd encourage you to look at\nthe experiments in that paper if you're interested\nin that, but they",
    "start": "2900158",
    "end": "2905339"
  },
  {
    "text": "can be effective for\ntext classification is the short answer.  OK.",
    "start": "2905340",
    "end": "2910820"
  },
  {
    "text": "So now let's talk about\nsome of the properties of meta-learning\nalgorithms and compare the three different\nclasses of approaches",
    "start": "2910820",
    "end": "2917030"
  },
  {
    "text": "that we've talked about so far. So we talked about\nblack-box methods,",
    "start": "2917030",
    "end": "2922230"
  },
  {
    "text": "optimization-based methods,\nand non-parametric methods. And the first way that we'll\nthink about comparing them",
    "start": "2922230",
    "end": "2927660"
  },
  {
    "text": "is from the standpoint\nof a computation graph. And we talked about\nthis perspective",
    "start": "2927660",
    "end": "2932980"
  },
  {
    "text": "before when we were\ncomparing Black-Box methods and optimization-based methods. And we talked about how a\nBlack-Box method is basically",
    "start": "2932980",
    "end": "2939550"
  },
  {
    "text": "directly producing a model that\ntakes as input of training data set and a test input. And an optimization\n[AUDIO OUT] could also",
    "start": "2939550",
    "end": "2946900"
  },
  {
    "text": "be viewed as this\ncomputation graph that takes as input a training\ndata set and a test example but one where it involves\nthis kind of inner loop",
    "start": "2946900",
    "end": "2955150"
  },
  {
    "text": "optimization inside\nthat computation graph, where essentially you\nrun gradient descent",
    "start": "2955150",
    "end": "2961180"
  },
  {
    "text": "on your training example\nand then use the resulting parameters to make predictions\non your test input.",
    "start": "2961180",
    "end": "2968620"
  },
  {
    "text": "But it's still this kind\nof general computation graph that takes as input a\ntraining data set and a test input.",
    "start": "2968620",
    "end": "2974960"
  },
  {
    "text": "Non-parametric\nmethods can also be cast inside this perspective\nof a computation graph, where",
    "start": "2974960",
    "end": "2982069"
  },
  {
    "text": "you have a train data\nset and test input. And the way that this\nprediction is produced",
    "start": "2982070",
    "end": "2988310"
  },
  {
    "text": "is basically comparing the\nembeddings of your test",
    "start": "2988310",
    "end": "2995600"
  },
  {
    "text": "example and your prototypes. And each of these prototype\nexamples is computed",
    "start": "2995600",
    "end": "3002360"
  },
  {
    "text": "using your training data set-- is computed using kind of the\naverage of all of the classes",
    "start": "3002360",
    "end": "3008210"
  },
  {
    "text": "in that-- the average of all\nthe embeddings in that class.",
    "start": "3008210",
    "end": "3014099"
  },
  {
    "text": "So all three of these\nmethods can basically just be viewed as different\nforms of this function",
    "start": "3014100",
    "end": "3019130"
  },
  {
    "text": "f, where you kind\nof pass the data set into the model in some\nway either by running",
    "start": "3019130",
    "end": "3025670"
  },
  {
    "text": "gradient descent or by\nlooking at nearest neighbors to a prototype, or it is passing\nit directly into the model.",
    "start": "3025670",
    "end": "3034580"
  },
  {
    "text": "And also all of them take as\ninput the test the example in some way either by passing\nit into the model with updated",
    "start": "3034580",
    "end": "3042770"
  },
  {
    "text": "parameters, by comparing\nit to the prototype, or by just passing it\ndirectly into the model.",
    "start": "3042770",
    "end": "3048290"
  },
  {
    "text": " Cool, so this is one\nperspective of thinking about the differences\nof these approaches",
    "start": "3048290",
    "end": "3054990"
  },
  {
    "text": "as basically just different\nways to ingest data and a test example.",
    "start": "3054990",
    "end": "3060010"
  },
  {
    "text": "And it's also worth noting\nagain that you can mix and match different components of the\ndifferent computation graphs",
    "start": "3060010",
    "end": "3066480"
  },
  {
    "text": "and get different kinds\nof methods as well. So for example,\nthere's one method",
    "start": "3066480",
    "end": "3072329"
  },
  {
    "text": "that both conditions\non the data in this way and runs gradient descent.",
    "start": "3072330",
    "end": "3077380"
  },
  {
    "text": "This is one valid approach. There's also an approach that\ndoes gradient descent on top of a relational\nnetwork embedding,",
    "start": "3077380",
    "end": "3083970"
  },
  {
    "text": "so it's somewhat similar to\na non-parametric approach and an optimization-based\napproach.",
    "start": "3083970",
    "end": "3089830"
  },
  {
    "text": "There's also a method that\nruns MAML but initializes the last layer as a ProtoNet\nduring the meta-training.",
    "start": "3089830",
    "end": "3095910"
  },
  {
    "text": "So there's kind of various ways\nto design this function that takes as input the training\ndata, and the test input.",
    "start": "3095910",
    "end": "3102580"
  },
  {
    "text": "And you can leverage\ndifferent elements of a non-parametric learner, of\na gradient-based optimization,",
    "start": "3102580",
    "end": "3108420"
  },
  {
    "text": "and so forth.  OK.",
    "start": "3108420",
    "end": "3114160"
  },
  {
    "text": "So that's the computation\ngraph perspective. And then [INAUDIBLE]\nI have to provide",
    "start": "3114160",
    "end": "3119620"
  },
  {
    "text": "to think about these\nclasses of methods is from the standpoint\nof various properties that we might want.",
    "start": "3119620",
    "end": "3125790"
  },
  {
    "text": "So one property that we\nmight want from these models is for them to be\nvery expressive to be",
    "start": "3125790",
    "end": "3131720"
  },
  {
    "text": "able to process the data\nin many different ways-- essentially, the\nability for this",
    "start": "3131720",
    "end": "3137150"
  },
  {
    "text": "function f to represent a\nrange of learning procedures. And this is helpful for\nscalability and applicability",
    "start": "3137150",
    "end": "3145380"
  },
  {
    "text": "to a wide range of\ndomains especially if we don't have good ways to\ndo kind of optimization learning",
    "start": "3145380",
    "end": "3152003"
  },
  {
    "text": "in that setting.  The second property that\nI think is desirable",
    "start": "3152003",
    "end": "3158160"
  },
  {
    "text": "is something that\nI'll call consistency. And the idea of\nconsistency is that we",
    "start": "3158160",
    "end": "3165650"
  },
  {
    "text": "want the learned\nlearning procedure to be able to improve with more data.",
    "start": "3165650",
    "end": "3172770"
  },
  {
    "text": "Essentially, at test\ntime, when you give it new data from a new\ntask, the more data",
    "start": "3172770",
    "end": "3178470"
  },
  {
    "text": "you give it the better\nyou want it to do, right? That seems like a pretty\nreasonable thing to ask.",
    "start": "3178470",
    "end": "3183540"
  },
  {
    "text": "And ideally, it would do\nthat regardless of the task that you're giving\nit and regardless of the meta-training tasks.",
    "start": "3183540",
    "end": "3190080"
  },
  {
    "text": "Even if this task is\nout of distribution, it would be nice if it\ncan actually improve",
    "start": "3190080",
    "end": "3195630"
  },
  {
    "text": "as you give it more data. And this will-- if you have a\nlearned procedure like this,",
    "start": "3195630",
    "end": "3202020"
  },
  {
    "text": "this will reduce the reliance\non the meta-training tasks. And it will also\npotentially give you",
    "start": "3202020",
    "end": "3207450"
  },
  {
    "text": "better out-of-distribution\ntask performance. And this is\nsomething that we can expect when we're actually\nrunning an optimization at test",
    "start": "3207450",
    "end": "3215070"
  },
  {
    "text": "time.  OK. And you can also recall, kind\nof, the example that we showed",
    "start": "3215070",
    "end": "3222540"
  },
  {
    "text": "before when we looked at\nout-of-distribution tasks and we saw that MAML\nwas a bit more robust.",
    "start": "3222540",
    "end": "3228283"
  },
  {
    "text": "I do think that these\ntwo properties are pretty important for\nmost applications. Now how do these\ndifferent approaches",
    "start": "3228283",
    "end": "3234510"
  },
  {
    "text": "fare on these two metrics? So black-box methods have\ncomplete expressive power",
    "start": "3234510",
    "end": "3242190"
  },
  {
    "text": "if you have a big enough\nrecurrent neural network, but they aren't consistent. So if you give it more data,\nand you pass in more data",
    "start": "3242190",
    "end": "3249570"
  },
  {
    "text": "into your RNN at test time, it\nmay not continue to improve. There's nothing to\nkind of guarantee",
    "start": "3249570",
    "end": "3256260"
  },
  {
    "text": "that it's actually kind of\ndescending on some objective per se.",
    "start": "3256260",
    "end": "3262500"
  },
  {
    "text": "Optimization-based methods--\nthey are consistent. Because you're running gradient\ndescent in expectation,",
    "start": "3262500",
    "end": "3268890"
  },
  {
    "text": "you can expect your loss\nto go down over time with gradient descent.",
    "start": "3268890",
    "end": "3276150"
  },
  {
    "text": "And they are expressive, like\nwe talked about on Wednesday, although this does require\nvery deep models in order",
    "start": "3276150",
    "end": "3283380"
  },
  {
    "text": "to get the kind of\nmaximal expressive power that you get with a\nBlack-Box learner. ",
    "start": "3283380",
    "end": "3291010"
  },
  {
    "text": "And this is for the\nsupervised learning setting. This actually isn't true in the\nreinforcement learning setting, but there are some\ndetails there.",
    "start": "3291010",
    "end": "3296512"
  },
  {
    "text": "And we'll talk about that\nwhen we start talking about reinforcement learning. And then lastly for\nnon-parametric methods,",
    "start": "3296512",
    "end": "3303210"
  },
  {
    "text": "they are quite expressive\nfor most architectures that you use.",
    "start": "3303210",
    "end": "3308650"
  },
  {
    "text": "The expressive power essentially\ndepends on the architecture. And they're also consistent\nunder certain conditions.",
    "start": "3308650",
    "end": "3314670"
  },
  {
    "text": "Basically, as you\nadd more examples,",
    "start": "3314670",
    "end": "3320290"
  },
  {
    "text": "if your metric\nproperly assigns, like, 0 distance to two examples that\nare the same or very similar,",
    "start": "3320290",
    "end": "3330400"
  },
  {
    "text": "then you can expect\ngiving it more training data will lead it to be\nmore likely to produce",
    "start": "3330400",
    "end": "3335910"
  },
  {
    "text": "an example that is very\nclose to your test example.",
    "start": "3335910",
    "end": "3342079"
  },
  {
    "text": " OK. So that's kind of an\nanalysis of these three",
    "start": "3342080",
    "end": "3347300"
  },
  {
    "text": "methods along those two axes. There are also a number of other\npros and cons of these methods.",
    "start": "3347300",
    "end": "3352440"
  },
  {
    "text": "So one thing with\nBlack-Box methods is that they're very easy to\ncombine with different learning",
    "start": "3352440",
    "end": "3358970"
  },
  {
    "text": "problems, but it also produces\na more challenging optimization.",
    "start": "3358970",
    "end": "3364490"
  },
  {
    "text": "And I think this is\ngetting at a question that Anu asked in\nthe chat, which is, what is the convergence\nrate of different approaches?",
    "start": "3364490",
    "end": "3372500"
  },
  {
    "text": "It's a bit difficult to\nkind of theoretically analyze the convergence\nrate of different conditions",
    "start": "3372500",
    "end": "3378780"
  },
  {
    "text": "especially when you're\nusing neural networks, because it relies on\na number of factors. A neural network\noptimization is, in general,",
    "start": "3378780",
    "end": "3385520"
  },
  {
    "text": "a complex topic to analyze. But we can at\nleast say, kind of, in practice and empirically,\nwe observe that--",
    "start": "3385520",
    "end": "3392755"
  },
  {
    "text": "at least for\nBlack-Box models, we observe that we have a more\nchallenging optimization and, likely, slower convergence.",
    "start": "3392755",
    "end": "3399050"
  },
  {
    "text": "And we can also say things\nabout optimization-based andnon-parametric methods,\nwhich we'll get to in a second.",
    "start": "3399050",
    "end": "3404990"
  },
  {
    "text": "And as a result,\nblack-box methods tend to be data inefficient\nin terms of the data that's needed for meta-training.",
    "start": "3404990",
    "end": "3410282"
  },
  {
    "text": " OK. For optimization-based methods,\nwe get this positive inductive",
    "start": "3410282",
    "end": "3416490"
  },
  {
    "text": "bias at the start\nof meta-learning because we are embedding an\noptimization into the meta",
    "start": "3416490",
    "end": "3423860"
  },
  {
    "text": "learner. It also handles\nvarying K and large K well, because you\ncan simply average",
    "start": "3423860",
    "end": "3430310"
  },
  {
    "text": "the gradient across varying\nnumbers of shots that you have. And it's model-agnostic\nin the sense",
    "start": "3430310",
    "end": "3436480"
  },
  {
    "text": "that you don't have to design\na particular architecture like you do in a\nblack-box method.",
    "start": "3436480",
    "end": "3441762"
  },
  {
    "text": "And then the downside\nis that it requires a second-order optimization,\nwhich can require more compute and memory.",
    "start": "3441762",
    "end": "3449480"
  },
  {
    "text": "And then lastly with\nnon-parametric methods, one really nice benefit is that\nthey're entirely feedforward.",
    "start": "3449480",
    "end": "3456230"
  },
  {
    "text": "This makes them in general\nkind of computationally fast, and easy to optimize, and just\neasy to work with in general.",
    "start": "3456230",
    "end": "3463869"
  },
  {
    "text": "The downside is that\nit's empirically been observed that they don't\ngeneralize to varying K well.",
    "start": "3463870",
    "end": "3470335"
  },
  {
    "text": "Although, we did actually\ntalk about recently how they can\ngeneralize to varying N well, so they can generalize\nto varying numbers of classes",
    "start": "3470335",
    "end": "3477490"
  },
  {
    "text": "but not necessarily varying\namounts of data per class. Of course, if you have\na very large amount",
    "start": "3477490",
    "end": "3483420"
  },
  {
    "text": "of data in the\ninner loop, they're going to be hard to\nscale because you have kind of large computation\ncosts for nearest neighbor",
    "start": "3483420",
    "end": "3489990"
  },
  {
    "text": "lookups. And so far, these methods have\nbeen limited to classification for the most part",
    "start": "3489990",
    "end": "3497570"
  },
  {
    "text": "Yeah. And actually, this\ngets at the question that just appeared in the chat\nis, can non-parametric methods",
    "start": "3497570",
    "end": "3502750"
  },
  {
    "text": "be used for regression? In general, they haven't been\nused for regression so far. ",
    "start": "3502750",
    "end": "3510130"
  },
  {
    "text": "OK. And then lastly, I'll mention\nthat generally well-tuned versions of these\nmethods generally",
    "start": "3510130",
    "end": "3515400"
  },
  {
    "text": "perform comparably on existing\nfew-shot learning benchmarks. And this may say more about\nthe benchmarks than about",
    "start": "3515400",
    "end": "3523792"
  },
  {
    "text": "the methods themselves. I think that kind of existing\nfew-shot image classification benchmarks don't really get\nthe heart at what's challenging",
    "start": "3523792",
    "end": "3529955"
  },
  {
    "text": "in meta-learning problems. And what method that you\nmight want to use really",
    "start": "3529955",
    "end": "3535320"
  },
  {
    "text": "depends on your use\ncase and how much you care about these\npositives and these negatives",
    "start": "3535320",
    "end": "3540510"
  },
  {
    "text": "in your particular use case. I guess one thing\nI would recommend is, if you are doing\nclassification,",
    "start": "3540510",
    "end": "3546820"
  },
  {
    "text": "and you want to kind of just\ntry out one of these approaches, I think that non-parametric\nmethods are probably",
    "start": "3546820",
    "end": "3553630"
  },
  {
    "text": "the easiest to get up\nand running in practice. ",
    "start": "3553630",
    "end": "3558819"
  },
  {
    "text": "But beyond that, I\nthink that there's kind of different trade-offs\nbetween different methods.",
    "start": "3558820",
    "end": "3564460"
  },
  {
    "text": "OK. So my question is, how does\nthe meta-learning methods like black-box and\noptimization-based",
    "start": "3564460",
    "end": "3571510"
  },
  {
    "text": "converge for regression tasks? Because we have\n[INAUDIBLE] function-- and let me see.",
    "start": "3571510",
    "end": "3576526"
  },
  {
    "text": "so inot possible to\nhave, like, few-shot, like five-shot on the ideas\nof what kind of example",
    "start": "3576526",
    "end": "3582452"
  },
  {
    "text": "they should start with\nfor the regression tasks kind of a problem with\nthe black-box method?",
    "start": "3582452",
    "end": "3588000"
  },
  {
    "text": "Yeah. So for optimization-based--\nwell, for both of these, it's very straightforward\nto apply them to regression.",
    "start": "3588000",
    "end": "3593680"
  },
  {
    "text": "You just change the\nouter loss function to be like mean squared error\nor some other regression loss",
    "start": "3593680",
    "end": "3600010"
  },
  {
    "text": "function. And for MAML or\noptimization-based meta-learners, you will\nalso change the inner loss",
    "start": "3600010",
    "end": "3605400"
  },
  {
    "text": "to be a regression-based\nloss as well. And beyond that, you're\nkind of basically done.",
    "start": "3605400",
    "end": "3613060"
  },
  {
    "text": "[INAUDIBLE] another\n[INAUDIBLE] like, we had to look at so many\nexamples to have for learning.",
    "start": "3613060",
    "end": "3621410"
  },
  {
    "text": "Like, we don't want to reflect\ntoo many examples [INAUDIBLE] number of training\nexamples when you're running a [INAUDIBLE] task.",
    "start": "3621410",
    "end": "3627400"
  },
  {
    "text": "Like, in classical, we\ncan-- in classification, we had five-shot learning\nand [INAUDIBLE] method.",
    "start": "3627400",
    "end": "3633859"
  },
  {
    "text": "That's not going to work in\nregression problems, I think. You can still do\nfew-shot learning in a regression problem.",
    "start": "3633860",
    "end": "3639000"
  },
  {
    "text": "It really depends on how broad\nyour task distribution is. But actually in the\noriginal MAML paper,",
    "start": "3639000",
    "end": "3645020"
  },
  {
    "text": "for example, we had\na regression task where different\ntasks corresponded to different sinusoid curves.",
    "start": "3645020",
    "end": "3650990"
  },
  {
    "text": "And we basically had\na family of sinusoids with varying phase\nand varying amplitude.",
    "start": "3650990",
    "end": "3656550"
  },
  {
    "text": "So there's basically two degrees\nof freedom within the task family. And it's able to learn\nwith, like, five examples.",
    "start": "3656550",
    "end": "3663720"
  },
  {
    "text": "It basically just has to\nidentify the sine curve from those five examples,\nand then it can actually",
    "start": "3663720",
    "end": "3668809"
  },
  {
    "text": "regress to the correct values of\nthat sine curve from the input. Because every task\nis just one function.",
    "start": "3668810",
    "end": "3674430"
  },
  {
    "text": "Right? That's, I think-- Yeah. ----[INAUDIBLE] from other\nclassification problems.",
    "start": "3674430",
    "end": "3680285"
  },
  {
    "text": "Yep. And second question, why\ndoes the non-parametric does not generalize [INAUDIBLE]\nbecause maybe each class might",
    "start": "3680285",
    "end": "3690088"
  },
  {
    "text": "depend more and that\nmight be the problem when we have a large number of K\nfor a non-parametric approach.",
    "start": "3690088",
    "end": "3696440"
  },
  {
    "text": "Yeah. So the reason why it doesn't\nscale to very large K is for the same\nreason that we don't use non-parametric methods\nwith really huge data sets.",
    "start": "3696440",
    "end": "3704980"
  },
  {
    "text": "To make a prediction with\na non-parametric method, with, like, nearest\nneighbors, for example, you have to search over\nyour entire data set.",
    "start": "3704980",
    "end": "3711010"
  },
  {
    "text": "And if you have a\nvery large data set, then you have to search\nover a large data set. So it's going to be basically\no of K. If K is small,",
    "start": "3711010",
    "end": "3718212"
  },
  {
    "text": "that's completely fine. If K is huge, that's\nmore of a challenge. And then [AUDIO OUT]\nwas asking, I think,",
    "start": "3718212",
    "end": "3724270"
  },
  {
    "text": "a related question is, isn't\nthe challenge with scaling prototypical\nnetworks in N and not K since we can compute the\ndistance from a created",
    "start": "3724270",
    "end": "3731140"
  },
  {
    "text": "vector to centroids\nand not the distance? OK. So yeah, this is mostly--",
    "start": "3731140",
    "end": "3737540"
  },
  {
    "text": "this comment is mostly\ncorresponding to things",
    "start": "3737540",
    "end": "3742702"
  },
  {
    "text": "like matching networks. If you use\nprototypical networks, then you're right in that it\nactually has to do more with N.",
    "start": "3742702",
    "end": "3750590"
  },
  {
    "text": "If N is very large,\nthen you'll have to do more comparisons\nto those prototypes.",
    "start": "3750590",
    "end": "3755762"
  },
  {
    "text": "But you can\nessentially pre-compute the prototypes, which will make\nit easier to scale to large K.",
    "start": "3755762",
    "end": "3763990"
  },
  {
    "text": "OK. One more question\nfrom [AUDIO OUT].. Yes. So I had a question about, like,\n[INAUDIBLE] setting, right?",
    "start": "3763990",
    "end": "3769980"
  },
  {
    "text": "So we were using this dataset\nwhich has around 1,600 classes. And each task, we sampled\naround five classes for that.",
    "start": "3769980",
    "end": "3776340"
  },
  {
    "text": "And then we're assuming at\ntest time in the real world when you're using-- we'll also supply, like,\na support set that has,",
    "start": "3776340",
    "end": "3782460"
  },
  {
    "text": "like, a few number of classes. And then we will use it\nto predict on a query set. But if you want to learn a\nmodel that can predict all",
    "start": "3782460",
    "end": "3790859"
  },
  {
    "text": "the classes, or,\nlike, 500 classes, or something like that,\nthen we do continue with the meta-learning in the\nfew-shot learning setting,",
    "start": "3790860",
    "end": "3797670"
  },
  {
    "text": "because there's still-- our classes still have\nvery few examples. Right? I believe, then move on to the\nsupervised learning setting",
    "start": "3797670",
    "end": "3805470"
  },
  {
    "text": "basically. Unless, how do you\ntrain a learner that can do many classes\nbut still having not too",
    "start": "3805470",
    "end": "3812555"
  },
  {
    "text": "many examples per class? Yeah, so that's basically\nexactly what this case study was looking at, where you wanted\nnot to just-- you basically",
    "start": "3812555",
    "end": "3819772"
  },
  {
    "text": "wanted to be able to produce.  To be able to essentially kind\nof test on all 200 classes",
    "start": "3819772",
    "end": "3826050"
  },
  {
    "text": "even if you had only trained\non up to 150 classes. And with these\nnon-parametric methods,",
    "start": "3826050",
    "end": "3831060"
  },
  {
    "text": "this is actually\nquite easy to do this, because for these base\nclasses, you've essentially formed kind of-- you've formed\nprototypes for these base",
    "start": "3831060",
    "end": "3839220"
  },
  {
    "text": "classes that work well with\nall of the kind of examples that you have. And then you could form\nadditional prototypes",
    "start": "3839220",
    "end": "3845819"
  },
  {
    "text": "with your novel classes by\npassing in those two examples and then averaging them\nto produce an embedding.",
    "start": "3845820",
    "end": "3853530"
  },
  {
    "text": "And then you just basically\nadd in prototypes. And then when you classify\namong the 200 classes, you consider all 200\nprototypes or kind of,",
    "start": "3853530",
    "end": "3861390"
  },
  {
    "text": "in this specific\ncase they're actually learning multiple\nprototypes per class, there's actually more\nthan 200 prototypes.",
    "start": "3861390",
    "end": "3866400"
  },
  {
    "text": "But it's easy to basically\nadd these prototypes, and kind of add them as\nyou get more classes,",
    "start": "3866400",
    "end": "3873900"
  },
  {
    "text": "and then do nearest neighbors\nto those prototypes.",
    "start": "3873900",
    "end": "3879470"
  },
  {
    "text": "Thanks.  Great. ",
    "start": "3879470",
    "end": "3887859"
  },
  {
    "text": "Cool. So now let's talk about\nsome applications, which is maybe\nthe most fun part.",
    "start": "3887860",
    "end": "3893800"
  },
  {
    "text": "Oh, actually, before we\ntalk about applications, there's one more property\nthat I want to mention-- ",
    "start": "3893800",
    "end": "3900779"
  },
  {
    "text": "is uncertainty awareness. So one other thing that's\nnice to have in a meta-learner",
    "start": "3900780",
    "end": "3905820"
  },
  {
    "text": "is to reason about\nambiguity during learning. If you only have a\nfew examples, then it",
    "start": "3905820",
    "end": "3911430"
  },
  {
    "text": "may not be completely clear\nwhat the correct function is even if you have a lot\nof previous experience.",
    "start": "3911430",
    "end": "3917609"
  },
  {
    "text": "It would be nice to know, like,\nif the few examples you have is enough for you to make\nan effective prediction,",
    "start": "3917610",
    "end": "3924480"
  },
  {
    "text": "or if the model thinks\nit needs more data to be able to make an\naccurate prediction.",
    "start": "3924480",
    "end": "3930380"
  },
  {
    "text": "This is especially useful in\nactive learning settings, where it's possible to get more-- to get more data in\nreinforcement learning settings",
    "start": "3930380",
    "end": "3937620"
  },
  {
    "text": "where you can explore\nin different ways to collect more data. And it's also useful for\ngetting calibrated uncertainty",
    "start": "3937620",
    "end": "3942779"
  },
  {
    "text": "estimates, which might\nbe relevant in safety critical settings\nwhere you want to know",
    "start": "3942780",
    "end": "3948690"
  },
  {
    "text": "whether you trust your model\nto make an accurate prediction. And then it also leads to\nkind of a more principled",
    "start": "3948690",
    "end": "3955840"
  },
  {
    "text": "interpretation to exactly\nwhat these methods are doing within the framework\nof Bayesian graph models.",
    "start": "3955840",
    "end": "3961140"
  },
  {
    "text": "OK. And then let's\nwrap up by talking about some applications,\nwhich I think is always pretty exciting.",
    "start": "3961140",
    "end": "3968985"
  },
  {
    "text": "And I'll just cover-- I'm just going to go over\nfour applications that illustrate some of the\nthings that you can do with meta-learning methods.",
    "start": "3968985",
    "end": "3975920"
  },
  {
    "text": "This is by no means kind\nof a complete survey of the applications that\npeople have looked at, but you can maybe hopefully get\na taste for what these methods",
    "start": "3975920",
    "end": "3984200"
  },
  {
    "text": "are capable of doing. So in this first\napplication the goal is to do imitation learning.",
    "start": "3984200",
    "end": "3991990"
  },
  {
    "text": "And imitation learning is\na really powerful framework for training robots and other\nsequential decision-making",
    "start": "3991990",
    "end": "4000600"
  },
  {
    "text": "systems by just telling\nthem to supervise-- basically doing supervised\nlearning with regard to some",
    "start": "4000600",
    "end": "4005610"
  },
  {
    "text": "demonstrations\nthat are provided. But collecting a lot of\ndemonstrations, for example on a robot, is very\nexpensive, because it",
    "start": "4005610",
    "end": "4012450"
  },
  {
    "text": "requires a human to teleoperate\nthe robot in a certain way. So in this case,\nthe different tasks",
    "start": "4012450",
    "end": "4018660"
  },
  {
    "text": "correspond to manipulating\ndifferent objects that you want the robot\nto be able to manipulate.",
    "start": "4018660",
    "end": "4025277"
  },
  {
    "text": "So for example,\ndifferent tasks might correspond to different objects\nthat the robot is holding and different containers that\nyou want the robot to place",
    "start": "4025278",
    "end": "4031950"
  },
  {
    "text": "the object into. In this case, D train\nis going to just",
    "start": "4031950",
    "end": "4037050"
  },
  {
    "text": "be a video of a\nhuman doing the task. So our goal is to\nhave the robot be able to do kind of\none-shot learning",
    "start": "4037050",
    "end": "4043350"
  },
  {
    "text": "from this video of a human. And D test will correspond to\na teleoperated demonstration.",
    "start": "4043350",
    "end": "4049350"
  },
  {
    "text": "And this basically tells it,\nafter you see this training data, these are the\nactions that you should take to complete the task.",
    "start": "4049350",
    "end": "4057108"
  },
  {
    "text": "And then the model\nthat's used in this work is an optimization-based model. It corresponds to MAML with a\nlearned inner loss function.",
    "start": "4057108",
    "end": "4064890"
  },
  {
    "text": "And the reason why we need to\nlearn this inner loss function is we don't actually have,\nlike, a set of labels",
    "start": "4064890",
    "end": "4069970"
  },
  {
    "text": "in this video of a human. And if you're kind of interested\nin learning more about that,",
    "start": "4069970",
    "end": "4075010"
  },
  {
    "text": "you can look at the paper. And here's an\nexample of what it's able to accomplish at test time,\nwhere a person, in this case,",
    "start": "4075010",
    "end": "4083329"
  },
  {
    "text": "places a peach into this\nred bowl then shuffles the objects around. And the robot\nexecutes the policy",
    "start": "4083330",
    "end": "4090350"
  },
  {
    "text": "that it learned in one shot. And it can figure\nout what it should do is place the peach\ninto the red bowl.",
    "start": "4090350",
    "end": "4097568"
  },
  {
    "text": " Cool. So this is one example of\none-shot learning applied",
    "start": "4097569",
    "end": "4104259"
  },
  {
    "text": "to imitation learning. The next application\nthat I'll talk about is low-resource molecular\nproperty prediction,",
    "start": "4104260",
    "end": "4112810"
  },
  {
    "text": "where you want to be able\nto predict the activities and properties of a certain\ninstance of a molecule",
    "start": "4112810",
    "end": "4119920"
  },
  {
    "text": "with only a few-- only\na small amount of data. And this might be quite useful\nfor low-resource drug discovery",
    "start": "4119920",
    "end": "4126850"
  },
  {
    "text": "problems, where basically each\ndata point requires running-- actually synthesizing\nthat molecule",
    "start": "4126850",
    "end": "4132790"
  },
  {
    "text": "and running experiments\nwith that molecule. And so different\ntasks correspond",
    "start": "4132790",
    "end": "4138210"
  },
  {
    "text": "to predicting the\nproperties and activations of different molecules. And D train and\nD test correspond",
    "start": "4138210",
    "end": "4144120"
  },
  {
    "text": "to different instances, I\nthink, of different molecules and potentially different\nkind of circumstances.",
    "start": "4144120",
    "end": "4152068"
  },
  {
    "text": "Although, I wasn't-- it was\ndifficult to find some details on this in the paper. But if you dig a bit\nfurther, it might",
    "start": "4152069",
    "end": "4158120"
  },
  {
    "text": "be a little bit more clear\nexactly what D train and D test correspond to. And then the model in this\ncase is optimization-based.",
    "start": "4158120",
    "end": "4164502"
  },
  {
    "text": "They tried a few\ndifferent variants of optimization-based\nmodels including MAML, first-order MAML,\nand almost no inner loop,",
    "start": "4164502",
    "end": "4171299"
  },
  {
    "text": "which basically\ncorresponds to MAML but where you only\nupdate the last layer of the model in the inner\nloop rather than updating",
    "start": "4171300",
    "end": "4178380"
  },
  {
    "text": "the entire network. And then the base\nmodel that they use with MAML was a gated\ngraph neural network,",
    "start": "4178380",
    "end": "4185380"
  },
  {
    "text": "so that they could\nprocess the molecules. And these are generally the\nresults that they found.",
    "start": "4185380",
    "end": "4191778"
  },
  {
    "text": "They found that,\nin general, MAML was able to do better than\nfirst-order MAML and ANIL in each of these cases and\nwas also able to significantly",
    "start": "4191779",
    "end": "4200510"
  },
  {
    "text": "outperform using\nk-nearest neighbors and fine tuning on a variety of\ndifferent held out molecules.",
    "start": "4200510",
    "end": "4207560"
  },
  {
    "text": " So this is pretty cool. ",
    "start": "4207560",
    "end": "4213930"
  },
  {
    "text": "The third application\nthat I'll mention is few-shot human\nmotion prediction, and the authors\nargued that this might",
    "start": "4213930",
    "end": "4220100"
  },
  {
    "text": "be useful for\nhuman-robot interaction. I would also suggest\nthat this could be quite useful for\nautonomous driving",
    "start": "4220100",
    "end": "4226040"
  },
  {
    "text": "if you want to know\nwhether a human will cross the road, for example, or\nmight continue standing at their particular location.",
    "start": "4226040",
    "end": "4233210"
  },
  {
    "text": "Different tasks correspond\nto different human users and different motions\nthat they're performing.",
    "start": "4233210",
    "end": "4239750"
  },
  {
    "text": "And the training\ndata for each task corresponds to the past\nk time steps of motion,",
    "start": "4239750",
    "end": "4245870"
  },
  {
    "text": "and the test data corresponds\nto the future seconds of motion. Essentially, you want to be\nable to predict the future based",
    "start": "4245870",
    "end": "4252350"
  },
  {
    "text": "on the last kind\nof snippet of time. ",
    "start": "4252350",
    "end": "4258650"
  },
  {
    "text": "In this case, they\nused kind of a hybrid of an optimization-based\nand a black-box method, where they used MAML where\nthey kind of first fine",
    "start": "4258650",
    "end": "4265219"
  },
  {
    "text": "tuned with gradient descent,\nand then additionally use a learned update rule on top\nof those fine tuned parameters,",
    "start": "4265220",
    "end": "4271369"
  },
  {
    "text": "and then trained\neverything end to end. And then their base model was\na recurrent neural network.",
    "start": "4271370",
    "end": "4278200"
  },
  {
    "text": "And these are kind\nof qualitatively what their results\nlook like, where PAML is kind of their\nproposed hybrid method.",
    "start": "4278200",
    "end": "4285140"
  },
  {
    "text": "The top is the ground truth. This is the data. This is their kind of D\ntrain data right here. And then this is the\nprediction forward.",
    "start": "4285140",
    "end": "4292545"
  },
  {
    "text": "And what you can\nsee is that it's able to kind of predict\nthat this person will keep on sitting down. And it's also able\nto predict the kind",
    "start": "4292545",
    "end": "4299470"
  },
  {
    "text": "of arm motions of the human\nwith relatively decent accuracy.",
    "start": "4299470",
    "end": "4304687"
  },
  {
    "text": " And they also looked at the\nkind of mean angle error",
    "start": "4304687",
    "end": "4311438"
  },
  {
    "text": "with respect to the\nprediction height horizon ranging from 80 time\nsteps to 1,000 time steps--",
    "start": "4311438",
    "end": "4317030"
  },
  {
    "text": "that are able to find that it\nkind of performed a lot better than various multitask learning\napproaches and various transfer",
    "start": "4317030",
    "end": "4322760"
  },
  {
    "text": "learning approaches.  OK.",
    "start": "4322760",
    "end": "4327931"
  },
  {
    "text": "And then the last\napplication that I'll mention is the GPT3 paper\nthat we didn't quite",
    "start": "4327932",
    "end": "4332950"
  },
  {
    "text": "have time to go through in\nthe lecture on Black-Box meta learning.",
    "start": "4332950",
    "end": "4338530"
  },
  {
    "text": "And in this case, their\ntasks are quite diverse. They correspond to things\nlike spelling correction,",
    "start": "4338530",
    "end": "4345130"
  },
  {
    "text": "simple math problems, and\ntranslating between languages, and a variety of other\ntasks, which I think",
    "start": "4345130",
    "end": "4351640"
  },
  {
    "text": "is actually really\ncool that they're able to train it on such a\nbroad distribution of tasks. And they represented\nall of these tasks",
    "start": "4351640",
    "end": "4358760"
  },
  {
    "text": "as language generation\nproblems, where kind of D train corresponds to\nlanguage that shows",
    "start": "4358760",
    "end": "4367909"
  },
  {
    "text": "kind of the correct answers\nfor some of those problems, and D test corresponds to kind\nof what the future language",
    "start": "4367910",
    "end": "4376760"
  },
  {
    "text": "should look like. And here's kind of an example\nof how you can write down simple math functions,\nspelling correction,",
    "start": "4376760",
    "end": "4383600"
  },
  {
    "text": "and translating between\nlanguages all as this language generation problem. ",
    "start": "4383600",
    "end": "4391187"
  },
  {
    "text": "The way that they\ntrained this is they took, like, a really just\nlarge amount of text data from the internet, from books\nand other language corpora.",
    "start": "4391187",
    "end": "4399490"
  },
  {
    "text": "And D train corresponded to\na sequence of characters. And D test corresponded\nto the following sequence",
    "start": "4399490",
    "end": "4404590"
  },
  {
    "text": "of characters. And the model is what\nwe can call, like,",
    "start": "4404590",
    "end": "4410159"
  },
  {
    "text": "a black-box meta-learner. It was basically just this\nreally huge transformer model, which essentially corresponds\nto different attention modules.",
    "start": "4410160",
    "end": "4419970"
  },
  {
    "text": "The details on that\ndon't matter too much though in terms of\nunderstanding what it's doing.",
    "start": "4419970",
    "end": "4426580"
  },
  {
    "text": "And here are some\nresults from the model. So they could do\none-shot learning from dictionary definitions.",
    "start": "4426580",
    "end": "4432310"
  },
  {
    "text": "So the gray text essentially\ncorresponds to D train, and the black text corresponds\nto D test for this task.",
    "start": "4432310",
    "end": "4440540"
  },
  {
    "text": "So the kind of\ntraining example is to screeg something is\nto swing a sword at it. An example of a sentence\nthat uses the word screeg is",
    "start": "4440540",
    "end": "4448610"
  },
  {
    "text": "and then what the\nmodel produces is, we screeghed each other\nfor several minutes and then went outside\nand ate ice cream.",
    "start": "4448610",
    "end": "4455980"
  },
  {
    "text": "There's also an example of\nfew-shot language editing, where you want to edit\nlanguage from poor English",
    "start": "4455980",
    "end": "4461980"
  },
  {
    "text": "to good English usage. And you can give it kind of\nall of this context of D train",
    "start": "4461980",
    "end": "4469000"
  },
  {
    "text": "that shows it kind\nof examples of what it looks like to convert from\npoor English to good English.",
    "start": "4469000",
    "end": "4475090"
  },
  {
    "text": "And you can find that\nit can kind of correct the sentence, which said, I'd\nbe more than happy to work",
    "start": "4475090",
    "end": "4480280"
  },
  {
    "text": "with you in another project, to\nI'd be more than happy to work with you on another project.",
    "start": "4480280",
    "end": "4486820"
  },
  {
    "text": "And also this\nsentence here which is, provide me with a\nshort brief of the design you're looking for\nand that would be nice",
    "start": "4486820",
    "end": "4493351"
  },
  {
    "text": "if you could share some\nexamples or project you did before to, please provide\nme with a brief description of the design\nyou're looking for.",
    "start": "4493352",
    "end": "4500199"
  },
  {
    "text": "And that would be nice if\nyou could share some examples or projects that you\nhave done before.",
    "start": "4500200",
    "end": "4506450"
  },
  {
    "text": "So it corrects kind\nof the language in a few different cases. And this model is also\ntrained on kind of tasks",
    "start": "4506450",
    "end": "4512702"
  },
  {
    "text": "that we wouldn't really\nconsider as few-shot learning tasks, which, for example,\njust correspond to giving it",
    "start": "4512702",
    "end": "4517880"
  },
  {
    "text": "a title and a subtitle. And it can generate an article\non the topic that you give it.",
    "start": "4517880",
    "end": "4523190"
  },
  {
    "text": " OK. So yeah.",
    "start": "4523190",
    "end": "4529430"
  },
  {
    "text": "Those are some examples of\nmeta-learning applications and ways to formulate\ndifferent problems",
    "start": "4529430",
    "end": "4535570"
  },
  {
    "text": "as meta-learning\nproblems ranging from imitation learning, drug\ndiscovery, motion prediction, and language generation.",
    "start": "4535570",
    "end": "4541937"
  },
  {
    "text": " And then that kind of covers\nthe goals for this lecture.",
    "start": "4541937",
    "end": "4553450"
  },
  {
    "text": "Hi, Professor Finn. So I have a question. So for the data set\nof Omniglot-- so",
    "start": "4553450",
    "end": "4559270"
  },
  {
    "text": "we have different languages\nand different characters. So we want to perform\nthe method learning.",
    "start": "4559270",
    "end": "4564639"
  },
  {
    "text": "So I'm wondering-- so\nwhat we are trying to do in the homework is to sample.",
    "start": "4564640",
    "end": "4571630"
  },
  {
    "text": "For example, if we wanted to\ndo five-way one-shot learning, we would sample five characters\nfrom the same language.",
    "start": "4571630",
    "end": "4579670"
  },
  {
    "text": "And I'm wondering if\nthere's any reason that we want the five characters\nto be in the same language.",
    "start": "4579670",
    "end": "4585070"
  },
  {
    "text": "What I think we can\nalso do is to shuffle all of these characters\nacross all the languages",
    "start": "4585070",
    "end": "4590650"
  },
  {
    "text": "into just five characters\nand do the same method. Yeah.",
    "start": "4590650",
    "end": "4595800"
  },
  {
    "text": "So in the literature,\npeople actually most frequently shuffle\ncharacters across languages just like what you mentioned.",
    "start": "4595800",
    "end": "4602520"
  },
  {
    "text": "And that's a kind of, yeah,\na completely reasonable thing to do.",
    "start": "4602520",
    "end": "4608460"
  },
  {
    "text": "One reason why it could\nmake sense to not shuffle is if, for example, it's\nharder to discern characters",
    "start": "4608460",
    "end": "4615510"
  },
  {
    "text": "from the same language\nthan to discern characters across languages, which\nwill kind of better",
    "start": "4615510",
    "end": "4621990"
  },
  {
    "text": "prepare you for test\ntime if you're only asked to compare\ncharacters across kind",
    "start": "4621990",
    "end": "4627510"
  },
  {
    "text": "of within a language. But in general, it\nmakes a lot of sense to shuffle across alphabets.",
    "start": "4627510",
    "end": "4633300"
  },
  {
    "text": "And that essentially gives you\nmore tasks for meta-training.",
    "start": "4633300",
    "end": "4638312"
  },
  {
    "text": "And so I also want to ask--\nso I think these two class distributions are different. Right? So whether you shuffle.",
    "start": "4638312",
    "end": "4645130"
  },
  {
    "text": "Yep. They are different. And so in practice, is there\nany reason, for example,",
    "start": "4645130",
    "end": "4652010"
  },
  {
    "text": "whether we should consider\nto shuffle or not shuffle? In general, I think\nit makes sense",
    "start": "4652010",
    "end": "4658050"
  },
  {
    "text": "to shuffle the\nmeta-training set, because it essentially\njust gives you more tasks in\naddition to the tasks",
    "start": "4658050",
    "end": "4664920"
  },
  {
    "text": "that you get within a language. So in general, I think\nthat in almost all cases",
    "start": "4664920",
    "end": "4670170"
  },
  {
    "text": "it makes sense, if you have a\nvery large number of alphabets, and you have plenty\nof data, then it may make sense to\nonly consider tasks",
    "start": "4670170",
    "end": "4676830"
  },
  {
    "text": "within a language, because\nthat will be kind of a harder",
    "start": "4676830",
    "end": "4682140"
  },
  {
    "text": "task that's reflective\nof a setting that you'll realistically\nsee at test time. Because, oftentimes, with\nthese systems, you won't be--",
    "start": "4682140",
    "end": "4689820"
  },
  {
    "text": "what you care about\nmost is, like, reading someone's handwriting. And you're not going\nto see people, like, writing characters in different\nlanguages most frequently.",
    "start": "4689820",
    "end": "4697930"
  },
  {
    "text": "But in general, in\nalmost all cases, I think it makes sense to\nshuffle all of the characters",
    "start": "4697930",
    "end": "4703290"
  },
  {
    "text": "across languages. So if we can actually\nshuffle all the languages--",
    "start": "4703290",
    "end": "4708960"
  },
  {
    "text": "and I'm wondering. So I think most of\nthe data sets can be formalized into the\nmeta-learning case where,",
    "start": "4708960",
    "end": "4717210"
  },
  {
    "text": "for example, we have\ndifferent classes. And what we can do\nis just to sample from these different classes,\nand to perform this task",
    "start": "4717210",
    "end": "4723750"
  },
  {
    "text": "distribution, and perform this\nkind of few-shot learning task.",
    "start": "4723750",
    "end": "4730360"
  },
  {
    "text": "Yeah, that makes sense. OK. Thank you so much. ",
    "start": "4730360",
    "end": "4736250"
  },
  {
    "text": "Yeah. I'm a little confused at the\ngermological disease diagnosis problem setup.",
    "start": "4736250",
    "end": "4742489"
  },
  {
    "text": "So there are 150 base\nclasses and 50 novel classes. Right? So are the 50 novel classes,\nthe held-out classes?",
    "start": "4742490",
    "end": "4749815"
  },
  {
    "text": " Yeah. So those 50 classes are\nheld out from the training,",
    "start": "4749815",
    "end": "4761120"
  },
  {
    "text": "for training the prototypical\nclustering network and the prototypical network. So this like a 200-way k-shot\nclassification problem.",
    "start": "4761120",
    "end": "4770310"
  },
  {
    "text": "Right? Yeah. So they held out these\n50 novel classes.",
    "start": "4770310",
    "end": "4776010"
  },
  {
    "text": "And then at test\ntime, they evaluated kind of on the full 200-way\nclassification problem.",
    "start": "4776010",
    "end": "4782300"
  },
  {
    "text": "And then these\nnumbers are reporting kind of the 200-way\nclassification prediction averaged across base\nand novel classes,",
    "start": "4782300",
    "end": "4789686"
  },
  {
    "text": "averaged across only\nthe base classes, and averaged across\nonly the novel classes. And during the training, what\nis being sent to the network",
    "start": "4789687",
    "end": "4796730"
  },
  {
    "text": "since you only have\n150 classes in the set? During the training, the\nimages from those 150 classes",
    "start": "4796730",
    "end": "4806570"
  },
  {
    "text": "are passed into the network,\nwhere you basically pass it",
    "start": "4806570",
    "end": "4811909"
  },
  {
    "text": "into the network, embed\nkind of a few examples",
    "start": "4811910",
    "end": "4817070"
  },
  {
    "text": "from, like, five examples\nfrom each of the classes",
    "start": "4817070",
    "end": "4822710"
  },
  {
    "text": "that you sample. You compute the embeddings. And then you take new examples\nfrom those image classes",
    "start": "4822710",
    "end": "4828020"
  },
  {
    "text": "and train it such\nthat these prototypes are accurate on those new\nexamples for the classes",
    "start": "4828020",
    "end": "4834410"
  },
  {
    "text": "that you sampled. And then at test time,\ngiven the new ones, you also additionally\nembed those to get additional prototypes in\nthis learned embedding space.",
    "start": "4834410",
    "end": "4845340"
  },
  {
    "text": "And then when you\ndo classification, you look at of kind of\nwhether it predicts it for some of the existing\nprototypes or some",
    "start": "4845340",
    "end": "4851347"
  },
  {
    "text": "of the new prototypes, like,\nbased on what prototype it's closest to. And then in training, you\nonly feed 150 classes.",
    "start": "4851347",
    "end": "4858290"
  },
  {
    "text": "Right? And during test, you\nfeed 200 classes? ",
    "start": "4858290",
    "end": "4864370"
  },
  {
    "text": "Yeah. And it may be that\nthey actually-- I couldn't actually find this\nhyperparameter in the paper, but it could be that they\nactually sub-sampled 150",
    "start": "4864370",
    "end": "4871650"
  },
  {
    "text": "during training. They made a sample of\n50, or something, or 20 when they do this-- when\nthey kind of predict this.",
    "start": "4871650",
    "end": "4881100"
  },
  {
    "text": "The downside is that\nmeta-training and meta-testing aren't exactly\nmatching, but it still can work reasonably well\njust like how the Siamese",
    "start": "4881100",
    "end": "4887490"
  },
  {
    "text": "networks worked\nwell when you had the kind of a one-class\nclassification during training and a N-way\nclassification during testing.",
    "start": "4887490",
    "end": "4895940"
  },
  {
    "text": "Thank you.  Yes. I have a quick question\nabout the GPT-3 example.",
    "start": "4895940",
    "end": "4905760"
  },
  {
    "text": "So you listed a few\nexamples at test time.",
    "start": "4905760",
    "end": "4910889"
  },
  {
    "text": "Yeah. Maybe the next slide? Yes, here.",
    "start": "4910890",
    "end": "4916760"
  },
  {
    "text": "This is how the model is\nrun at inference time. But during training time,\nthe meta-training examples",
    "start": "4916760",
    "end": "4927170"
  },
  {
    "text": "are just the-- they split the\nwhole sentence into two pieces. And the first one is\nthe meta-training input,",
    "start": "4927170",
    "end": "4934820"
  },
  {
    "text": "and the second one is\nthe meta-training test-- Mhm. --output.",
    "start": "4934820",
    "end": "4939860"
  },
  {
    "text": "And for the meta-test\nexample, did they mask the output, so that\na model can predict the output?",
    "start": "4939860",
    "end": "4949320"
  },
  {
    "text": "Yeah. So different tasks correspond\nto basically, like, language generation. And so D train might be\nthis, and then D test",
    "start": "4949320",
    "end": "4955730"
  },
  {
    "text": "is, like, generating this. So it isn't fed in. This is like-- you could\nconsider this as, like, y test.",
    "start": "4955730",
    "end": "4964340"
  },
  {
    "text": "Actually, sorry. You consider the black as like\ny test, and this is like--",
    "start": "4964340",
    "end": "4973460"
  },
  {
    "text": "this is like y train. And x is equal to nothing.",
    "start": "4973460",
    "end": "4979080"
  },
  {
    "start": "4979080",
    "end": "4985280"
  },
  {
    "text": "So the task is to basically\ngenerate y to-- ah. Yeah, I remember that.",
    "start": "4985280",
    "end": "4991950"
  },
  {
    "text": "Oh. Yeah. So the task is to basically\ngenerate y train where-- so they're given y train. And then the goal is to generate\ny test, which corresponds",
    "start": "4991950",
    "end": "5000640"
  },
  {
    "text": "to this black sentence. I see. So there is actually\nno input to the model?",
    "start": "5000640",
    "end": "5008830"
  },
  {
    "text": "There's no kind of x test input. This kind of-- the aggregate\nof x train and y train",
    "start": "5008830",
    "end": "5016840"
  },
  {
    "text": "is equal to D train, and D\ntrain is passed into the model, because it's a black-box model.",
    "start": "5016840",
    "end": "5023889"
  },
  {
    "text": "I see. So if they used an\noptimization-based meta-learning,\nwould the setup be",
    "start": "5023890",
    "end": "5031500"
  },
  {
    "text": "similar to the language model\npre-training and fine tuning approach? Mhm, yeah, where you would\nthen fine tune on all of this",
    "start": "5031500",
    "end": "5039990"
  },
  {
    "text": "if you use an\noptimization-based meta-learner I see. Cool, thank you. ",
    "start": "5039990",
    "end": "5047570"
  },
  {
    "text": "OK. Sounds good. I think that was\nthe last question. Have a good rest of your week.",
    "start": "5047570",
    "end": "5053370"
  },
  {
    "start": "5053370",
    "end": "5058000"
  }
]