[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "So given the insights so far, let's now go and design the most powerful graph neural network.",
    "start": "4070",
    "end": "12450"
  },
  {
    "text": "So let's go and design the most expressive, uh, graph neural network. And let's develop the theory that will allow us, uh, to do that.",
    "start": "12450",
    "end": "21855"
  },
  {
    "text": "So the key observation so far is that the expressive power of a graph neural network can be characterized by",
    "start": "21855",
    "end": "29730"
  },
  {
    "start": "22000",
    "end": "72000"
  },
  {
    "text": "the expressive power of the neighborhood aggregation function they use. Because the more expressive",
    "start": "29730",
    "end": "35340"
  },
  {
    "text": "the neighborhood aggregation leads to a more expressive graph neural network. And we saw that if the neighborhood aggregation is injective,",
    "start": "35340",
    "end": "44420"
  },
  {
    "text": "this leads to the most expressive, uh, GNN. A neighborhood aggregation being injective,",
    "start": "44420",
    "end": "50155"
  },
  {
    "text": "it means that whatever is the number and the features of the- of the children,",
    "start": "50155",
    "end": "55519"
  },
  {
    "text": "you- you map every different combination into a different output, so no information get- gets lost.",
    "start": "55520",
    "end": "63235"
  },
  {
    "text": "So let's do the following next, let's theoretically analyze the expressive power of different, uh, aggregation functions.",
    "start": "63235",
    "end": "72265"
  },
  {
    "start": "72000",
    "end": "199000"
  },
  {
    "text": "So the way you think of neighborhood aggregation, uh, basically taking the information from the children and aggregating is that",
    "start": "72265",
    "end": "80255"
  },
  {
    "text": "neighborhood aggregation can be abstracted as a function over a multi-set. A multi-set is simply a set with repeated elements, right?",
    "start": "80255",
    "end": "89660"
  },
  {
    "text": "So if you say, \"I'm a node here and I aggregated from two neighbors for two children,\" this is the same as saying,",
    "start": "89660",
    "end": "96005"
  },
  {
    "text": "I have a set of children, uh, two yellow guys, and I need to aggregate information from them, right?",
    "start": "96005",
    "end": "102845"
  },
  {
    "text": "And of course, in a multi-set, um, nodes can have different colors, node can have different features, so, you know,",
    "start": "102845",
    "end": "109325"
  },
  {
    "text": "could say, uh-huh, I have, uh, I have two children, one with yellow attribute and the other one with",
    "start": "109325",
    "end": "114590"
  },
  {
    "text": "blue attribute versus some other node has, um, three children, two of them with yellow attribute or yellow feature,",
    "start": "114590",
    "end": "122900"
  },
  {
    "text": "and one, uh, with the blue feature. And then we aggregate this information,",
    "start": "122900",
    "end": "128054"
  },
  {
    "text": "we want the- the new message, the aggregated information not to be lost. Somehow we wanna the,",
    "start": "128055",
    "end": "135020"
  },
  {
    "text": "er, to- in this aggregation, in this compression step, basically to retain all the information we know about the children, right?",
    "start": "135020",
    "end": "141739"
  },
  {
    "text": "So here we'd wanna say, two yellow and a blue, and here we'd wanna say one yellow and one blue so that",
    "start": "141740",
    "end": "147829"
  },
  {
    "text": "these two sets- multi-sets still remain, uh, distinguishable as- as we are aggregating them, uh, to their parent,",
    "start": "147830",
    "end": "155760"
  },
  {
    "text": "so that then we aggregate this parent further to the super parent, uh, no information, uh, gets lost.",
    "start": "155760",
    "end": "162680"
  },
  {
    "text": "So let's look at, uh, the neighborhood information- aggregation functions used by the two,",
    "start": "162680",
    "end": "170170"
  },
  {
    "text": "uh, models that we have discussed so far in the class. First, we'll talk about, uh, GCN, which uses mean pooling.",
    "start": "170170",
    "end": "177705"
  },
  {
    "text": "It uses element-wise mean pooling over neighborhood node features, and then let's talk about the max pooling variant of GraphSAGE that uses",
    "start": "177705",
    "end": "187490"
  },
  {
    "text": "element-wise maximum pooling over neighboring, uh, node features. And let's see what is the expressive power of mean",
    "start": "187490",
    "end": "195395"
  },
  {
    "text": "and what is the expressive power of max, uh, pooling. So, uh, let's first talk about, uh, GCN,",
    "start": "195395",
    "end": "203519"
  },
  {
    "start": "199000",
    "end": "273000"
  },
  {
    "text": "so the mean pooling when you average the messages coming from the ne- from the children. Uh, if we take the, er, element-wise mean,",
    "start": "203520",
    "end": "211415"
  },
  {
    "text": "then in a GCN, it's followed by a linear function, and a ReLU, a- activation function.",
    "start": "211415",
    "end": "218045"
  },
  {
    "text": "Um, and, er, what is, uh, what is the observation? The observation function is that GCN's aggregation function cannot",
    "start": "218045",
    "end": "226740"
  },
  {
    "text": "disting- distinguish multi- different multi-sets with the same, kind of, proportion of colors, right?",
    "start": "226740",
    "end": "233450"
  },
  {
    "text": "So for example, um, this is a failure case. When you average together messages,",
    "start": "233450",
    "end": "238899"
  },
  {
    "text": "it doesn't- it doesn't matter whether you average one yellow and one blue message or whether you average two yellow and two blue messages.",
    "start": "238899",
    "end": "247025"
  },
  {
    "text": "At the end, the average is the same, all right? And this is the failure case of the average.",
    "start": "247025",
    "end": "253835"
  },
  {
    "text": "It- it will combine these two multi-sets, it will aggregate them into the same,",
    "start": "253835",
    "end": "260019"
  },
  {
    "text": "uh, into the same message. So it means it will lose information in the case that here are,",
    "start": "260020",
    "end": "265610"
  },
  {
    "text": "um, one and one, and here is two and two, because the ratio is the same.",
    "start": "265610",
    "end": "271055"
  },
  {
    "text": "So let me be a bit more precise and give you a proper example. Let's for simplicity, assume that node colors are represented as one-hot encodings, right?",
    "start": "271055",
    "end": "280940"
  },
  {
    "start": "273000",
    "end": "298000"
  },
  {
    "text": "So now every node- every node, uh, has a feature vector that simply encodes what color is the color of the node, right?",
    "start": "280940",
    "end": "288670"
  },
  {
    "text": "That is its, uh, feature vector. And this is just, kind of, uh, a- a way to illustrate, uh,",
    "start": "288670",
    "end": "294380"
  },
  {
    "text": "this concept and what happens when we do, uh, aggregation, right?",
    "start": "294380",
    "end": "299700"
  },
  {
    "start": "298000",
    "end": "396000"
  },
  {
    "text": "So for example, when you do, uh, average of, uh, two vectors, uh, 1,0, and 0,1,",
    "start": "299700",
    "end": "306370"
  },
  {
    "text": "you- you- you get, uh, uh, half and half. So that's your aggregated message now of these two, uh, feature vectors.",
    "start": "306370",
    "end": "314625"
  },
  {
    "text": "Uh, in this case, when you have a multi-set, again, of two yellow and two blue, here are the corresponding feature representations.",
    "start": "314625",
    "end": "321720"
  },
  {
    "text": "If I take the e- the- the element-wise average of these, uh, four vectors, I also get half half.",
    "start": "321720",
    "end": "328699"
  },
  {
    "text": "So it means that even if I then apply some non-linear transformation, and activation, and so on, at the end,",
    "start": "328700",
    "end": "335770"
  },
  {
    "text": "I will get the same output because the aggregation of, uh, yellow and blue is the same as the aggregation of two yellows and two blues.",
    "start": "335770",
    "end": "344995"
  },
  {
    "text": "Even though I encode them in- with different feature vectors, so yellow and blue nodes are de- definitely distinguishable because, you know,",
    "start": "344995",
    "end": "353780"
  },
  {
    "text": "one has the first element set to 1, and the other one has the second, uh, element, uh, set to 1.",
    "start": "353780",
    "end": "359530"
  },
  {
    "text": "So you see how mean pooling can basically aggregate, uh,",
    "start": "359530",
    "end": "365030"
  },
  {
    "text": "multi-sets that have the same proportion of nodes of one type of feature versus the other type of feature,",
    "start": "365030",
    "end": "372889"
  },
  {
    "text": "um, into the same representation, regardless of what is the total number of nodes or what is the total size of the underlying, uh, multi-set.",
    "start": "372890",
    "end": "381560"
  },
  {
    "text": "So this is the issue with the mean pooling. This is a failure case of mean pooling because it won't be able to distinguish a multi-set",
    "start": "381560",
    "end": "389420"
  },
  {
    "text": "of size 2 versus size 4 if the proportion of features is the same in both.",
    "start": "389420",
    "end": "396140"
  },
  {
    "start": "396000",
    "end": "456000"
  },
  {
    "text": "And now, let's look at, uh, um, um, uh, GraphSAGE, uh, max-pooling, uh, variant of it.",
    "start": "396140",
    "end": "405540"
  },
  {
    "text": "So we- in the GraphSAGE, we apply our multi-layer perceptron transformation, and then take, uh, uh, element-wise maximum pooling.",
    "start": "405540",
    "end": "414250"
  },
  {
    "text": "Um, and what we learn here is that maximum pooling function cannot",
    "start": "414250",
    "end": "419930"
  },
  {
    "text": "distinguish different multi-sets with the same set of distinct colors, right?",
    "start": "419930",
    "end": "425550"
  },
  {
    "text": "So what does- what does this mean, is that all of these different multi-sets will be aggregated into the same representation.",
    "start": "425550",
    "end": "433920"
  },
  {
    "text": "Why is that the case is because as long as multi-sets have the same set of distinct colors,",
    "start": "433920",
    "end": "440330"
  },
  {
    "text": "then the me- whatever is the maximum, right? Maximum will be one of the colors, that maximum is the same regardless of how many different nodes,",
    "start": "440330",
    "end": "450180"
  },
  {
    "text": "uh, and what are the proportions of colors in the, um, uh, in the multi-set.",
    "start": "450180",
    "end": "456035"
  },
  {
    "start": "456000",
    "end": "545000"
  },
  {
    "text": "So to give you an example, imagine I have these three different multi-sets,",
    "start": "456035",
    "end": "461365"
  },
  {
    "text": "I, uh, I encode this colors using some encoding, then I apply some nonlinear transformation",
    "start": "461365",
    "end": "468110"
  },
  {
    "text": "like an MLP to it because this is what GraphSAGE does, and let's assume without loss of generality,",
    "start": "468110",
    "end": "473720"
  },
  {
    "text": "that basically now, you know, these colors get transformed to some new colors, and we encode these colors with one-hot encoding.",
    "start": "473720",
    "end": "480860"
  },
  {
    "text": "So I- so everything is disti- distinguishable at to this level. But the problem is that now if you take, uh,",
    "start": "480860",
    "end": "488000"
  },
  {
    "text": "element-wise, meaning coordinate-wise, maximum, in all these different cases,",
    "start": "488000",
    "end": "493310"
  },
  {
    "text": "you get the same aggregation, you get the same maximum value, you get 1 and 1.",
    "start": "493310",
    "end": "498900"
  },
  {
    "text": "So this means that regardless whether the node has, uh, two children, four children,",
    "start": "498900",
    "end": "505810"
  },
  {
    "text": "or three children, um, and whatever is the ratio between blue, and, uh, uh,",
    "start": "505810",
    "end": "511150"
  },
  {
    "text": "yellow, in all cases, the maximum pooling will give me the same representation.",
    "start": "511150",
    "end": "516474"
  },
  {
    "text": "So it means that all this information here gets lost and all these different multi-sets get mapped to the same representation.",
    "start": "516475",
    "end": "525355"
  },
  {
    "text": "So clearly, uh, maximum pooling is not an injective operator because it maps different inputs into the same,",
    "start": "525355",
    "end": "533995"
  },
  {
    "text": "uh, output, and that's the problem. You get these collisions and information, uh, gets lost,",
    "start": "533995",
    "end": "539740"
  },
  {
    "text": "and that decreases the expressive power, uh, of the graph neural network.",
    "start": "539740",
    "end": "545340"
  },
  {
    "start": "545000",
    "end": "605000"
  },
  {
    "text": "So, let's summarize what we have learned so far. We have analyzed the expressive power of graph neural networks,",
    "start": "545340",
    "end": "553399"
  },
  {
    "text": "and the main takeaways are the following; the expressive power of a graph neural network can be characterized by",
    "start": "553399",
    "end": "560390"
  },
  {
    "text": "the expressive power of its neighborhood aggregation function, right? So the message aggregation function.",
    "start": "560390",
    "end": "566700"
  },
  {
    "text": "Neighborhood aggregation is a function over multi-sets, basically sets with repeating elements.",
    "start": "566700",
    "end": "573160"
  },
  {
    "text": "Um, and GCN and GraphSAGE aggregation functions fail to distinguish some of the basic multi-sets.",
    "start": "573160",
    "end": "580394"
  },
  {
    "text": "Meaning these two aggregation functions, mean and maximum, are not injective,",
    "start": "580395",
    "end": "586190"
  },
  {
    "text": "which means different inputs get mapped into the same output, and this way, the information gets lost.",
    "start": "586190",
    "end": "592885"
  },
  {
    "text": "Therefore, GCN and GraphSAGE are not maximally powerful graph neural networks.",
    "start": "592885",
    "end": "600480"
  },
  {
    "text": "They're not maximally expressive, uh, graph neural networks. So let's now move on and say,",
    "start": "600480",
    "end": "609774"
  },
  {
    "start": "605000",
    "end": "654000"
  },
  {
    "text": "can we design the most expressive graph neural network, right? So our goal will be to design maximally powerful graph neural network,",
    "start": "609775",
    "end": "619240"
  },
  {
    "text": "uh, among all possible message-passing graph neural networks. And this will- the way we are going to do this is to-",
    "start": "619240",
    "end": "627160"
  },
  {
    "text": "to design an injective neighborhood aggregation function. So basically a neighborhood aggregation function that will never lose information",
    "start": "627160",
    "end": "635305"
  },
  {
    "text": "when it aggregates from the children to create a message, uh, for the parent. So the property will be an injectivity of the aggregation function, right?",
    "start": "635305",
    "end": "645535"
  },
  {
    "text": "Um, so the goal is design a neural network that can model this injective, uh, multi-set function because that's the aggregation operator.",
    "start": "645535",
    "end": "654399"
  },
  {
    "start": "654000",
    "end": "734000"
  },
  {
    "text": "So, uh, here is a very, uh, useful, uh, theorem. The theorem says that any injective multi-set function",
    "start": "654400",
    "end": "662860"
  },
  {
    "text": "can be expressed in the following way. So it can be- so if I have a set of elements, right,",
    "start": "662860",
    "end": "669730"
  },
  {
    "text": "I have my multi-set function S, uh, multi-set, uh, that has a set of elements,",
    "start": "669730",
    "end": "675490"
  },
  {
    "text": "then the way I can write an injective function over a multi-set is I can write it as, uh, I apply my function f to every element of the multi-set,",
    "start": "675490",
    "end": "684940"
  },
  {
    "text": "I sum up these, uh, uh, these, uh, outputs of f and then I apply another non-linear function, okay?",
    "start": "684940",
    "end": "695230"
  },
  {
    "text": "So the point is that if I want to have an injective set over, uh, over a multi-set,",
    "start": "695230",
    "end": "701515"
  },
  {
    "text": "then I can realize this injective, uh, function by having two functions, f and Phi,",
    "start": "701515",
    "end": "709135"
  },
  {
    "text": "where f I apply to every element of the multi-set, I sum up the outputs of f,",
    "start": "709135",
    "end": "715524"
  },
  {
    "text": "and then I apply another function, another transformation, uh, Phi to it.",
    "start": "715525",
    "end": "721195"
  },
  {
    "text": "And this means that, uh, this is how, um, a gr- um,",
    "start": "721195",
    "end": "727120"
  },
  {
    "text": "a multi-set function can be expressed, uh, and it will still be, uh, injective.",
    "start": "727120",
    "end": "733525"
  },
  {
    "text": "So the way you can think of the proof, what is the intuition? The intuition is that our f can, uh,",
    "start": "733525",
    "end": "741145"
  },
  {
    "start": "734000",
    "end": "802000"
  },
  {
    "text": "produce kind of one-hot encodings of colors, right? So f is injective for different node,",
    "start": "741145",
    "end": "747415"
  },
  {
    "text": "it produces a different output, um, and these outputs need to be different enough so that when you sum them up,",
    "start": "747415",
    "end": "754120"
  },
  {
    "text": "you don't lose any information. So in some sense, if- if f takes colors and produces their one-hot encodings,",
    "start": "754120",
    "end": "761065"
  },
  {
    "text": "this means that then you can basically by summing up, you are counting how many elements of each color you have.",
    "start": "761065",
    "end": "767515"
  },
  {
    "text": "And this way, you don't lose any information, right? You say, uh huh, I have one yellow node and I have, uh,",
    "start": "767515",
    "end": "773155"
  },
  {
    "text": "two blue nodes, and that is kind of the way you can think of f, right? f takes the colors and- and kind of encodes them as one-hot,",
    "start": "773155",
    "end": "782125"
  },
  {
    "text": "so that when you sum them up, you basically count how many different colors you have. Of course, f needs to be a function that does, that does this.",
    "start": "782125",
    "end": "790510"
  },
  {
    "text": "If f does not do this for you, um, this won't, uh, this won't work. So f has to be a very special, uh,",
    "start": "790510",
    "end": "797605"
  },
  {
    "text": "function, um, and then, uh, it will, uh, work out.",
    "start": "797605",
    "end": "802615"
  },
  {
    "start": "802000",
    "end": "988000"
  },
  {
    "text": "So now the question is, what kind of function f and Phi can I use?",
    "start": "802615",
    "end": "809665"
  },
  {
    "text": "How do I define them? And we're going to u- to use them to basically define them with a neural network.",
    "start": "809665",
    "end": "816144"
  },
  {
    "text": "We are going to define them using a multi-layer perceptron. Um, and why would we want to define it just using a perceptron?",
    "start": "816145",
    "end": "824755"
  },
  {
    "text": "There reason is that, uh, there is something called an universal approximation theorem,",
    "start": "824755",
    "end": "830905"
  },
  {
    "text": "and it goes as follows: So, uh, one hidden layer uh, multiple,",
    "start": "830905",
    "end": "837459"
  },
  {
    "text": "uh, layer perceptron with sufficiently large, uh, hidden layer dimensionality and appropriate non-linearity can",
    "start": "837460",
    "end": "845680"
  },
  {
    "text": "approximate any continuous function to an arbitrary accuracy, right?",
    "start": "845680",
    "end": "850930"
  },
  {
    "text": "So what is this saying? It says that I have this unknown special functions of Phi and f",
    "start": "850930",
    "end": "857980"
  },
  {
    "text": "that I need to define so that I- so that I can write my injective function in terms of f and Phi,",
    "start": "857980",
    "end": "865180"
  },
  {
    "text": "but f and Phi are not known ahead of time. So but- so I'm going to represent f and Phi with neural networks.",
    "start": "865180",
    "end": "874885"
  },
  {
    "text": "And then because multi-layer perceptron is able to learn any function to arbitrary accuracy,",
    "start": "874885",
    "end": "883020"
  },
  {
    "text": "this basically means I can use data to learn f and Phi that have the property to- to- to create these types of injective mappings.",
    "start": "883020",
    "end": "893204"
  },
  {
    "text": "So basically this means that, um, we have arrived to a neural network that can model any injective function, right?",
    "start": "893205",
    "end": "901810"
  },
  {
    "text": "If I take a multi-set with elements x, then if I apply a multi-layer perceptron to it,",
    "start": "901810",
    "end": "908214"
  },
  {
    "text": "sum it up and apply another multi-layer perceptron, then multi-layer perceptron can, um,",
    "start": "908215",
    "end": "914605"
  },
  {
    "text": "approximate any function, so it can approximate my function f and Phi as well.",
    "start": "914605",
    "end": "919915"
  },
  {
    "text": "So this means now I have a neural network that can do this injective multi-set, uh, mapping.",
    "start": "919915",
    "end": "926755"
  },
  {
    "text": "And, you know, in theory, this embedding dimensionality, the dimensionality of the MLP could be very large,",
    "start": "926755",
    "end": "934060"
  },
  {
    "text": "but in practice, it turns out that, you know, something between 100 and 500, um, is good enough and gives you a good performance.",
    "start": "934060",
    "end": "941275"
  },
  {
    "text": "So what magic has just happened is that we said any injective multi-set function can be written as,",
    "start": "941275",
    "end": "948310"
  },
  {
    "text": "uh, um, as, uh, uh, uh, as a- with two functions, f and Phi.",
    "start": "948310",
    "end": "953815"
  },
  {
    "text": "F is first applied to every, uh, element of the multi-set summed up and that",
    "start": "953815",
    "end": "959215"
  },
  {
    "text": "that is passed through the function Phi and this way, uh, the- the injectivity, uh, is preserved.",
    "start": "959215",
    "end": "966175"
  },
  {
    "text": "And because of the universal approximation theorem, we can model f and Phi with a multi-layer perceptron,",
    "start": "966175",
    "end": "973330"
  },
  {
    "text": "and now we have an injective, uh, aggregation function, because MLP can learn any possible function, and, uh,",
    "start": "973330",
    "end": "981730"
  },
  {
    "text": "um, and, uh, the other MLP also can learn any function, meaning, it can learn the function f as well.",
    "start": "981730",
    "end": "987985"
  },
  {
    "text": "So, uh, what is now the most expressive graph neural network there is?",
    "start": "987985",
    "end": "993190"
  },
  {
    "start": "988000",
    "end": "1063000"
  },
  {
    "text": "The most expressive graph neural network there- there is, is called Graph Isomorphism Neural Network or GIN for short.",
    "start": "993190",
    "end": "1001845"
  },
  {
    "text": "And the way its aggregation function looks like it says, let's take messages from the children,",
    "start": "1001845",
    "end": "1008805"
  },
  {
    "text": "let's transform them with a multi-layer perceptron, let's sum them up and apply another,",
    "start": "1008805",
    "end": "1014610"
  },
  {
    "text": "uh, multi-layer, uh, perceptron. And, you know, uh, given everything I explained, this is, uh,",
    "start": "1014610",
    "end": "1021720"
  },
  {
    "text": "injective multi-set aggregation function, so it means it has no failure cases.",
    "start": "1021720",
    "end": "1026954"
  },
  {
    "text": "It doesn't have any collisions, and this is the most expressive graph neural network",
    "start": "1026955",
    "end": "1032985"
  },
  {
    "text": "in the class of this message passing, uh, graph neural networks. So, uh, it is super cool that we were basically able",
    "start": "1032985",
    "end": "1041745"
  },
  {
    "text": "to define the most powerful graph neural network, um, out of, uh,",
    "start": "1041745",
    "end": "1047970"
  },
  {
    "text": "an entire class, uh, of graph neural networks. And we now theoretically understand that it is really all about the aggregation function,",
    "start": "1047970",
    "end": "1055725"
  },
  {
    "text": "and that the summation aggregation function is better than the average,",
    "start": "1055725",
    "end": "1061184"
  },
  {
    "text": "is better than the maximum. So, uh, let me, uh, summarize a bit, right?",
    "start": "1061185",
    "end": "1067200"
  },
  {
    "start": "1063000",
    "end": "1122000"
  },
  {
    "text": "We have described neighborhood aggregation fun- uh, uh, of, uh, function of a GIN, um, and,",
    "start": "1067200",
    "end": "1072900"
  },
  {
    "text": "uh, we see that basically the aggregation is a summation. We take messages, we transform them through an MLP and then sum them up.",
    "start": "1072900",
    "end": "1080430"
  },
  {
    "text": "And that has the injective property, which means it will be able to capture the structure of the entire computation graph.",
    "start": "1080430",
    "end": "1088425"
  },
  {
    "text": "Now, uh, that we have seen what GIN is, we are going to describe the full,",
    "start": "1088425",
    "end": "1095130"
  },
  {
    "text": "uh, model of the Graph Isomorphism Network, and we are actually going to relate it back to the, uh,",
    "start": "1095130",
    "end": "1100500"
  },
  {
    "text": "Weisfeiler-Lehman graph kernel, the WL graph kernel that we talked about,",
    "start": "1100500",
    "end": "1105900"
  },
  {
    "text": "I think in lecture number 2. And what we are going to provide is this very interesting prospect where we are going to see that,",
    "start": "1105900",
    "end": "1113370"
  },
  {
    "text": "uh, GIN is a neural network version of the WL ker- kernel.",
    "start": "1113370",
    "end": "1118485"
  },
  {
    "text": "So, um, let me explain this in more detail. So what is WL graph kernel?",
    "start": "1118485",
    "end": "1125655"
  },
  {
    "start": "1122000",
    "end": "1365000"
  },
  {
    "text": "Right? It is also called a color refinement algorithm where basically we are given a graph G and a set of nodes V,",
    "start": "1125655",
    "end": "1133394"
  },
  {
    "text": "we assign initial color, uh, c to each node v. Uh,",
    "start": "1133395",
    "end": "1138570"
  },
  {
    "text": "let's say we- the colors are based on degree of the node, and then we are iteratively aggregating,",
    "start": "1138570",
    "end": "1144510"
  },
  {
    "text": "hashing colors of neighbors to create a new color for the node, right? So we take the at- at, uh,",
    "start": "1144510",
    "end": "1151485"
  },
  {
    "text": "if you want to create the color at level k plus 1 for a given node, we take the colors of the nodes, uh,",
    "start": "1151485",
    "end": "1157875"
  },
  {
    "text": "u that are its neighbors from the previous iteration, we take, uh,",
    "start": "1157875",
    "end": "1163320"
  },
  {
    "text": "color of node v from the previous iteration, somehow hash these together into a new color.",
    "start": "1163320",
    "end": "1169965"
  },
  {
    "text": "All right, then hash- hash function. The idea is that it maps different inputs to different, uh, outputs.",
    "start": "1169965",
    "end": "1177045"
  },
  {
    "text": "Uh, so hash functions are as injective, uh, as possible. And the idea is that after k steps of this color refinement, the color, uh,",
    "start": "1177045",
    "end": "1187485"
  },
  {
    "text": "of every node will summarize the K-hop neighborhood structure, uh, around a given node.",
    "start": "1187485",
    "end": "1193845"
  },
  {
    "text": "So let me give you an example. Imagine I have two different graphs, um.",
    "start": "1193845",
    "end": "1200055"
  },
  {
    "text": "Here they are, they are different, they are, um, uh, uh, non-isomorphic.",
    "start": "1200055",
    "end": "1205695"
  },
  {
    "text": "So the way we do this is, uh, let's say we first simply initialize all the colors to value 1 and then we aggregate, right?",
    "start": "1205695",
    "end": "1212850"
  },
  {
    "text": "So the- for example, this node has color 1 and then has three neighbors, each one of color 1.",
    "start": "1212850",
    "end": "1217890"
  },
  {
    "text": "So this will be now one comma 1, 1, 1, and then, you know, every node does the same.",
    "start": "1217890",
    "end": "1223350"
  },
  {
    "text": "Now we are going to hash these descriptions, these, uh, colors into new colors.",
    "start": "1223350",
    "end": "1230760"
  },
  {
    "text": "And let's assume that our hash function is injective, meaning it has no, uh, collisions, then this would be a new set of,",
    "start": "1230760",
    "end": "1238770"
  },
  {
    "text": "uh, node colors now. Um, and for example, in this case, this node, uh,",
    "start": "1238770",
    "end": "1245490"
  },
  {
    "text": "and that node have the same color because their descriptions, uh, are the same, right? They have color 1 and they have three neighbors, each with color 1,",
    "start": "1245490",
    "end": "1253559"
  },
  {
    "text": "so this particular input got mapped to a new color, uh, number 4.",
    "start": "1253560",
    "end": "1258795"
  },
  {
    "text": "And now I can then repeat this process, uh, uh, one more time and so on.",
    "start": "1258795",
    "end": "1264885"
  },
  {
    "text": "What you should notice is that at every iteration of this WL kernel, what is happening is we are taking the colors, uh,",
    "start": "1264885",
    "end": "1272985"
  },
  {
    "text": "from the neighbors, uh, and putting them together, together with our own color. So if you go back and look here,",
    "start": "1272985",
    "end": "1279764"
  },
  {
    "text": "this is very similar to the, uh, graph neural network, where we take messages from the neighbors,",
    "start": "1279765",
    "end": "1286305"
  },
  {
    "text": "combine it with the v's own message, somehow transform, uh, all these into a new- and- and, uh,",
    "start": "1286305",
    "end": "1293550"
  },
  {
    "text": "transform this and call this- that this is the message for node V at the next level. So this is essentially like a hard coded graph neural network, right?",
    "start": "1293550",
    "end": "1303510"
  },
  {
    "text": "We take, uh, colors from neighbors, uh, aggregate them, take our own color,",
    "start": "1303510",
    "end": "1309690"
  },
  {
    "text": "aggregate it, and then call this to be the embedding of the node v at the next layer or at the next, uh, level.",
    "start": "1309690",
    "end": "1318165"
  },
  {
    "text": "So the point is that as- the more iterations we do this, the farther out information,",
    "start": "1318165",
    "end": "1324630"
  },
  {
    "text": "uh, is captured at a given node, right? The farther out kind of the network neighborhood gets,",
    "start": "1324630",
    "end": "1329985"
  },
  {
    "text": "more and more hops get added to it. And the idea of this color refinement is that the- if you are doing, let's say, um,",
    "start": "1329985",
    "end": "1337919"
  },
  {
    "text": "isomorphism testing, then the process continues al- uh, until the stable coloring is reached.",
    "start": "1337920",
    "end": "1344790"
  },
  {
    "text": "And two graphs are considered isomorphic if they have the same set of colors. In our case, the colors in these two graphs are different.",
    "start": "1344790",
    "end": "1354165"
  },
  {
    "text": "The distribution of them- the number of them is different, which means these two graphs are not isomorphic and you- if you look at them,",
    "start": "1354165",
    "end": "1361275"
  },
  {
    "text": "you really see that, uh, they are not, uh, isomorphic. So now, how does this relate to the GIN model?",
    "start": "1361275",
    "end": "1368789"
  },
  {
    "start": "1365000",
    "end": "1392000"
  },
  {
    "text": "All right, GIN uses neural network to model this injective hash function, right?",
    "start": "1368790",
    "end": "1374400"
  },
  {
    "text": "The way we can, uh, write out GIN is we can say, aha, it's some aggregation over the- the, uh, embeddings,",
    "start": "1374400",
    "end": "1381900"
  },
  {
    "text": "messages from the children, uh, from the neighbors of node v plus the color,",
    "start": "1381900",
    "end": "1387855"
  },
  {
    "text": "the message of the node V, uh, from the previous, uh, step. So the way we write this in terms of,",
    "start": "1387855",
    "end": "1397274"
  },
  {
    "start": "1392000",
    "end": "1445000"
  },
  {
    "text": "um, uh, GIN operator is to say, aha, we are taking the messages from the children,",
    "start": "1397275",
    "end": "1403395"
  },
  {
    "text": "we aggregate- we transform them using an MLP, this is our function f, and we summed them up.",
    "start": "1403395",
    "end": "1409770"
  },
  {
    "text": "Um, and then we also add 1 plus epsilon,",
    "start": "1409770",
    "end": "1415020"
  },
  {
    "text": "where epsilon is some small, uh, learnable scalar, our own message transformed by",
    "start": "1415020",
    "end": "1420990"
  },
  {
    "text": "f and then add the two together and pass through another function, uh, phi.",
    "start": "1420990",
    "end": "1426434"
  },
  {
    "text": "And this is exactly now a, um, uh, an injective operator that basically an in- in",
    "start": "1426435",
    "end": "1434190"
  },
  {
    "text": "an injective way maps the neighborhood information plus the, um, plus- plus the node's own information into",
    "start": "1434190",
    "end": "1442080"
  },
  {
    "text": "a unique embedding into a unique, uh, representation. So, um, if- assume that in- input, uh,",
    "start": "1442080",
    "end": "1450900"
  },
  {
    "text": "feature is c is represented, let's say as one-hot encoding, then basically just direct summation is injective, right?",
    "start": "1450900",
    "end": "1459630"
  },
  {
    "text": "So if I say, how do I, uh, have an injective function over a multiset where",
    "start": "1459630",
    "end": "1464820"
  },
  {
    "text": "elements of a multiset are encoded with one-hot? Then basically, all I have to do is sum up these vectors and I'll get",
    "start": "1464820",
    "end": "1473190"
  },
  {
    "text": "a unique representation because every coordinate will count how many nodes of a given color, uh, there are.",
    "start": "1473190",
    "end": "1479355"
  },
  {
    "text": "Now, if these colors are not presented so nicely, you need to transform them with the function f so that it kind of",
    "start": "1479355",
    "end": "1487845"
  },
  {
    "text": "approximates this intuitive one-hot encoding, right? So this means that,",
    "start": "1487845",
    "end": "1493934"
  },
  {
    "text": "uh, uh, GIN, uh, uh, aggregation GIN type of convolution is composed of two M- uh, uh,",
    "start": "1493935",
    "end": "1501720"
  },
  {
    "text": "two MLPs, one operated on the colors of neighbors, one, um, and then the aggregation is a summation plus some, uh,",
    "start": "1501720",
    "end": "1511620"
  },
  {
    "text": "final MLP that, again, kind of provides the next level one-hot encoding so that when we,",
    "start": "1511620",
    "end": "1517530"
  },
  {
    "text": "again, sum up information from the children at the next level, no information, uh, gets lost.",
    "start": "1517530",
    "end": "1523395"
  },
  {
    "text": "So you can think of these f's and, uh, f's and phi as some kind of transformation- transform- transformations that kind of softly do",
    "start": "1523395",
    "end": "1532215"
  },
  {
    "text": "uh, one-hot, uh, encoding. So let's now summarize and provide the entire, uh, GIN model.",
    "start": "1532215",
    "end": "1540090"
  },
  {
    "start": "1534000",
    "end": "1589000"
  },
  {
    "text": "Uh, GIN, uh, node-embedding update goes as follows. Given a graph with a set of nodes v,",
    "start": "1540090",
    "end": "1547050"
  },
  {
    "text": "we assign a scalar, uh, vector to each node v, and then iteratively, ap- uh, apply this, uh,",
    "start": "1547050",
    "end": "1554730"
  },
  {
    "text": "GINConv operator that basically takes the information from the neighbors, takes its own information,",
    "start": "1554730",
    "end": "1561045"
  },
  {
    "text": "applies these func- uh, functions f and phi that are modeled by MLPs and produces the next level embedding.",
    "start": "1561045",
    "end": "1568005"
  },
  {
    "text": "And if you look at this, this is now written exactly the same way as the WL, right?",
    "start": "1568005",
    "end": "1573210"
  },
  {
    "text": "Rather than hash here, we write out this GINConv. So this means that basically after K steps of GIN iteration,",
    "start": "1573210",
    "end": "1581145"
  },
  {
    "text": "CK summarizes the- the structure of the k-hop neighborhood around a given node, uh,",
    "start": "1581145",
    "end": "1588255"
  },
  {
    "text": "v. So to bring the two together, this means that GIN can be viewed- understood as",
    "start": "1588255",
    "end": "1594690"
  },
  {
    "start": "1589000",
    "end": "1781000"
  },
  {
    "text": "a differentiable neural network version of WL, uh, graph kernel, right,",
    "start": "1594690",
    "end": "1599910"
  },
  {
    "text": "wherein WL views node colors. Um, and, uh, let's say we can encode them as one-hot and use this, uh,",
    "start": "1599910",
    "end": "1607905"
  },
  {
    "text": "abstract deterministic hash function, while in GIN views node embeddings which are low-dimensional vectors.",
    "start": "1607905",
    "end": "1614760"
  },
  {
    "text": "And we use this GIN convolution with these two MLPs; the MLP phi and MLP, uh,",
    "start": "1614760",
    "end": "1621210"
  },
  {
    "text": "f that, uh, aggregate information. You know, what are the advantages of GIN over",
    "start": "1621210",
    "end": "1627390"
  },
  {
    "text": "the WL is that node embeddings are low-dimensional. Hence, they can capture the fine",
    "start": "1627390",
    "end": "1634920"
  },
  {
    "text": "grained similarity of different nodes and that parameters, uh, of the update function can be learned from the downstream task.",
    "start": "1634920",
    "end": "1642929"
  },
  {
    "text": "So we are going to actually be able to learn functions, uh, f and phi that,",
    "start": "1642930",
    "end": "1648225"
  },
  {
    "text": "uh, ensure, uh, injectivity. So, um, you know,",
    "start": "1648225",
    "end": "1653565"
  },
  {
    "text": "because the relationship between GIN and the WL kernel, their expressive power is exactly the same.",
    "start": "1653565",
    "end": "1661620"
  },
  {
    "text": "So this now means that if two graphs can be distinguished by GIN, they can be also distinguished by WL and vice versa.",
    "start": "1661620",
    "end": "1669705"
  },
  {
    "text": "So it means that, uh, graph neural networks are at most as powerful or as",
    "start": "1669705",
    "end": "1678029"
  },
  {
    "text": "expressive as the WL kernel or the WL graph isomorphism test.",
    "start": "1678030",
    "end": "1684930"
  },
  {
    "text": "Um, and, uh, this is great because now we have the upper bound.",
    "start": "1684930",
    "end": "1690495"
  },
  {
    "text": "We know that GIN attains this upper bound and we also know that WL kernel,",
    "start": "1690495",
    "end": "1696434"
  },
  {
    "text": "both theoretically and empirically, has shown to distinguish many or most of the real-world graphs, right?",
    "start": "1696435",
    "end": "1703875"
  },
  {
    "text": "So this means that GIN is the- powerful enough to distinguish most,",
    "start": "1703875",
    "end": "1709080"
  },
  {
    "text": "uh, real-world graphs, which is, uh, great- uh, which is great news.",
    "start": "1709080",
    "end": "1714549"
  },
  {
    "text": "So let me summarize. Uh, we design a neural network that can model",
    "start": "1714550",
    "end": "1720005"
  },
  {
    "text": "injective multi-set function by basically saying that any injective multi-set function can be written as a,",
    "start": "1720005",
    "end": "1726870"
  },
  {
    "text": "uh, app- application of a function f to the elements of the multiset plus a summation.",
    "start": "1726870",
    "end": "1732615"
  },
  {
    "text": "Um, in our case, we use a neural network for neg- neighborhood aggregation function, uh,",
    "start": "1732615",
    "end": "1738585"
  },
  {
    "text": "and rely on the, um, uh, Universal Approximation Theorem to basically say that MLP is able to learn any function.",
    "start": "1738585",
    "end": "1746265"
  },
  {
    "text": "So this means that GIN is able to capture the neighborhoods in an injective way,",
    "start": "1746265",
    "end": "1751935"
  },
  {
    "text": "which means it is the most powerful or the most expressive graph neural network there is.",
    "start": "1751935",
    "end": "1757155"
  },
  {
    "text": "Um, and the key is to use element-wise summation pooling instead of mean or max pooling.",
    "start": "1757155",
    "end": "1764295"
  },
  {
    "text": "So it means that sum pooling is more expressive than mean or max pooling.",
    "start": "1764295",
    "end": "1769725"
  },
  {
    "text": "We also saw that the GIN is closely related to the WL kernel and that both GIN and WL kernel can distinguish,",
    "start": "1769725",
    "end": "1777779"
  },
  {
    "text": "uh, most of the real-world, uh, graph structures. To summarize, the- the- the important point of",
    "start": "1777779",
    "end": "1785820"
  },
  {
    "start": "1781000",
    "end": "1837000"
  },
  {
    "text": "the lecture is that if you say about mean and max pooling, for example, mean and max pooling are not able to distinguish",
    "start": "1785820",
    "end": "1793020"
  },
  {
    "text": "these types of neighborhood structures where you have two or three neighbors, all the same features. Here is where maximum pooling fails because",
    "start": "1793020",
    "end": "1800250"
  },
  {
    "text": "the number of distinct- k- kind of the distinct colors are the same. So whatever is the maximum is the same in both cases and this is, again,",
    "start": "1800250",
    "end": "1807559"
  },
  {
    "text": "the case where both mean and max pooling fail because we have, uh, um, green and red and they are in the same proportion.",
    "start": "1807560",
    "end": "1815420"
  },
  {
    "text": "So if you rank, uh, different pooling operators by- by discriminative power, um,",
    "start": "1815420",
    "end": "1821615"
  },
  {
    "text": "sum pooling is the best, is most expressive, is more expressive than mean pooling,",
    "start": "1821615",
    "end": "1826955"
  },
  {
    "text": "is more expressive than, uh, maximum pooling. So in general, sum pooling, uh,",
    "start": "1826955",
    "end": "1832845"
  },
  {
    "text": "is the most expressive, uh, to use in graph neural networks. And last thing I want to mention is that you can",
    "start": "1832845",
    "end": "1841350"
  },
  {
    "start": "1837000",
    "end": "1911000"
  },
  {
    "text": "further improve the expressive power of graph neural networks. So the important characteristic of what we talked",
    "start": "1841350",
    "end": "1849195"
  },
  {
    "text": "today was that node features are indistinguishable, meaning that all nodes have the same node feature information.",
    "start": "1849195",
    "end": "1856325"
  },
  {
    "text": "So by adding rich features, nodes may become, um, distinguishable.",
    "start": "1856325",
    "end": "1861580"
  },
  {
    "text": "The other important thing that we talked about today is that because graph neural networks only aggregate features",
    "start": "1861580",
    "end": "1868290"
  },
  {
    "text": "and they use no reference point in the network. Nodes that have the same, um,",
    "start": "1868290",
    "end": "1873870"
  },
  {
    "text": "uh, computation graph structure are indistinguishable. And what we are going to talk about, uh,",
    "start": "1873870",
    "end": "1880830"
  },
  {
    "text": "later in the course is actually how do we improve the expressive power of,",
    "start": "1880830",
    "end": "1886380"
  },
  {
    "text": "uh, graph neural networks, um, to be more expressive than GIN, and to be more expressive that- than, uh, WL.",
    "start": "1886380",
    "end": "1894330"
  },
  {
    "text": "And of course in those cases, it will actually require more than just the message passing.",
    "start": "1894330",
    "end": "1900195"
  },
  {
    "text": "It will require more advanced operations, um, and we are going to talk about those, uh, in the future.",
    "start": "1900195",
    "end": "1906550"
  }
]