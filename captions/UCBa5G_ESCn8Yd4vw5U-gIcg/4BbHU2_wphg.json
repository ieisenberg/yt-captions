[
  {
    "start": "0",
    "end": "185000"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "0",
    "end": "6315"
  },
  {
    "text": "So today, we're going to start lecture number 15 and the topic for",
    "start": "6315",
    "end": "11320"
  },
  {
    "text": "today is basically what's left of reinforcement learning.",
    "start": "11320",
    "end": "16410"
  },
  {
    "text": "For the purposes of this course, we're going to wrap up our reinforcement learning today. So first we're going to discuss learned models.",
    "start": "16410",
    "end": "25845"
  },
  {
    "text": "We're going to do this first and then talk about extensions of the methods we've seen to continue state settings.",
    "start": "25845",
    "end": "35290"
  },
  {
    "text": "And we're going to cover two topics over there, discretization and fitted value function iteration.",
    "start": "35290",
    "end": "42520"
  },
  {
    "text": "And once we are done with these two, that basically marks the end of what's part of the syllabus for this course,",
    "start": "42520",
    "end": "49880"
  },
  {
    "text": "the parts that you'll be tested on. And for the rest of the lecture, once we're done with this, we will do a quick survey of",
    "start": "49880",
    "end": "56315"
  },
  {
    "text": "the larger reinforcement field- reinforcement learning field in general. Just talk about the different kinds of approaches",
    "start": "56315",
    "end": "63155"
  },
  {
    "text": "that are there in reinforcement learning that we're not covering and kind of give you an approximate mind map of how to kind",
    "start": "63155",
    "end": "69590"
  },
  {
    "text": "of relate all the different concepts in RL to each other, all right? So before we jump into today's topics,",
    "start": "69590",
    "end": "77180"
  },
  {
    "text": "let's do a quick review of what was covered on the previous class on Wednesday. So we started off with a formalism called Markov decision processes.",
    "start": "77180",
    "end": "86480"
  },
  {
    "text": "Where this formalism describes the setting in which there is an agent that is making sequential decisions over time.",
    "start": "86480",
    "end": "95039"
  },
  {
    "text": "And a MDP, or a Markov decision process, is a tuple of five entities.",
    "start": "95600",
    "end": "101384"
  },
  {
    "text": "So first is a set of state, set of all possible states that the agent can currently be in.",
    "start": "101385",
    "end": "107125"
  },
  {
    "text": "A, is the set of all actions that an agent can take. For simplicity, we'll assume that all actions can be taken in all states.",
    "start": "107125",
    "end": "116150"
  },
  {
    "text": "P(s, a) is what is called as the transition probabilities. So transition probabilities is- we have a set of",
    "start": "116150",
    "end": "123650"
  },
  {
    "text": "transition probabilities for the set of all states and the set of all actions.",
    "start": "123650",
    "end": "128759"
  },
  {
    "text": "What that specifically means is the transition probability is",
    "start": "128759",
    "end": "135040"
  },
  {
    "text": "each P(s, a) is a probability distribution over the set of states and it is different for every- for every combination of S and A.",
    "start": "135040",
    "end": "142795"
  },
  {
    "text": "So if we are in current state S and we take current action A, then the state that we will end up in for",
    "start": "142795",
    "end": "150010"
  },
  {
    "text": "the next time step is distributed according to P(s, a). This is only describing how the world works.",
    "start": "150010",
    "end": "158185"
  },
  {
    "text": "Think of this as like the laws of physics. You take an action and you end up in some states, this is how the world works, right?",
    "start": "158185",
    "end": "164600"
  },
  {
    "text": "And Gamma is what we call as a discount factor, it's a value between 0 and 1,",
    "start": "164600",
    "end": "170165"
  },
  {
    "text": "and R is a function. It can either be a function of state or a function of a state",
    "start": "170165",
    "end": "176719"
  },
  {
    "text": "and action pair and it is the immediate reward function. What is the immediate reward that we get for being in a particular state, right?",
    "start": "176720",
    "end": "184720"
  },
  {
    "text": "And there are these two central concepts in reinforcement learning that's beyond the MDP formalism called the policy and the value, right?",
    "start": "184720",
    "end": "193440"
  },
  {
    "start": "185000",
    "end": "900000"
  },
  {
    "text": "These two are kind of- you can in a loose sense- you can think of them as the duals of each other, right?",
    "start": "193440",
    "end": "199474"
  },
  {
    "text": "So the policy basically is the rule book that the agent follows, right?",
    "start": "199475",
    "end": "205255"
  },
  {
    "text": "It tells us when we are in a current state S, what is the action that you need to take? And the action that we take based on the policy will decide",
    "start": "205255",
    "end": "213395"
  },
  {
    "text": "which new- new state will end up in based on the transition probabilities. Alright, so the policy is- is just a rule book",
    "start": "213395",
    "end": "220190"
  },
  {
    "text": "of what action we need to take when we are in a particular state. And the value- value function is- is like",
    "start": "220190",
    "end": "227780"
  },
  {
    "text": "the long-term accumulated or the expected long-term accumulated reward,",
    "start": "227780",
    "end": "234125"
  },
  {
    "text": "along with the discount factor Gamma. So the V pi of S is the expected sum of all rewards.",
    "start": "234125",
    "end": "242545"
  },
  {
    "text": "That is the reward at current state plus a discount times the reward at the second state,",
    "start": "242545",
    "end": "248165"
  },
  {
    "text": "plus a discount- discount squared times the reward at the third state and so on and it is the expected sum of these future rewards, discounted future rewards,",
    "start": "248165",
    "end": "257465"
  },
  {
    "text": "assuming we are starting at a state S naught equal to the point where we're evaluating the value function and assuming we",
    "start": "257465",
    "end": "264500"
  },
  {
    "text": "are taking- we are following some particular policy pi, right? And this is also called the Bellman's equation and this-",
    "start": "264500",
    "end": "272830"
  },
  {
    "text": "this expected sum of future rewards can be expressed this way according to the Bellman's equation.",
    "start": "272830",
    "end": "278300"
  },
  {
    "text": "There was a question from some students about how- how we go from here to here and there is a short derivation that I posted on Piazza number 240.",
    "start": "278300",
    "end": "285080"
  },
  {
    "text": "If you want to see how we went from here to here. It's pretty straightforward. Now, there was also a question of Gamma yesterday.",
    "start": "285080",
    "end": "293590"
  },
  {
    "text": "Should it be the same value, should it be less than 1, and so on? The answer is, it should be less than 1,",
    "start": "293590",
    "end": "299900"
  },
  {
    "text": "strictly less than 1. It cannot be greater than 1 and it is this Gamma that is less than 1,",
    "start": "299900",
    "end": "306289"
  },
  {
    "text": "that makes the value function bounded, right? So it's- it's easy to see if supposing Gamma was equal to 1.",
    "start": "306290",
    "end": "313820"
  },
  {
    "text": "Then in general, we could have accumulated- we can keep accumulating rewards by an infinite amount and in general,",
    "start": "313820",
    "end": "320735"
  },
  {
    "text": "the expectation need not be bound, so we write the expected sum of all future rewards can explode to infinity.",
    "start": "320735",
    "end": "325820"
  },
  {
    "text": "But because we have this discount factor Gamma, which is mostly, mostly here for",
    "start": "325820",
    "end": "331400"
  },
  {
    "text": "mathematical reasons just to make the value function bounded. But it also so happens that it has nice interpretation such as",
    "start": "331400",
    "end": "336800"
  },
  {
    "text": "the interest rate and whatnot if you're working in- in- in a finance setting or you can think of it as the rate at which",
    "start": "336800",
    "end": "344870"
  },
  {
    "text": "your robot's fuel is decreasing or whatever. So you can have different kinds of interpretations for Gamma,",
    "start": "344870",
    "end": "351530"
  },
  {
    "text": "but it exists mostly for mathematical convenience to make the value function bounded. And the reason why it's bounded is- is pretty- pretty simple.",
    "start": "351530",
    "end": "359210"
  },
  {
    "text": "You can see that- you can- this expectation is going to be less than uh- it's going to be- you can show a quick proof here.",
    "start": "359210",
    "end": "369705"
  },
  {
    "text": "So this expected sum of rewards are of S1 plus Gamma times so on,",
    "start": "369705",
    "end": "377639"
  },
  {
    "text": "will be less than expectation of- I'm just going to call it R star",
    "start": "377640",
    "end": "384045"
  },
  {
    "text": "plus Gamma times R star plus Gamma squared times R star, etc.",
    "start": "384045",
    "end": "390585"
  },
  {
    "text": "Where R star is the maximum possible reward that you can get in any possible state.",
    "start": "390585",
    "end": "395705"
  },
  {
    "text": "And once you have this, you can take R star common and you get a 1 plus Gamma plus Gamma squared and so on, that is bounded.",
    "start": "395705",
    "end": "401514"
  },
  {
    "text": "So the expected sum of all future rewards will always be bounded if we use a Gamma of less than 1, right?",
    "start": "401515",
    "end": "410560"
  },
  {
    "text": "And then based on these- these- these two central concepts, policy and value, we can define something called as the optimal value functions.",
    "start": "412790",
    "end": "422260"
  },
  {
    "text": "So the optimal value function is the best possible long-term value we can have if we were to choose a suitable policy.",
    "start": "422260",
    "end": "430110"
  },
  {
    "text": "So if you were to optimize over all the possible policies that can exist, what is the best possible future long-term- long-term value that we can come up with, right?",
    "start": "430110",
    "end": "441125"
  },
  {
    "text": "And this is- similarly we can see that V star s is equal to- can also be written as R of S",
    "start": "441125",
    "end": "448125"
  },
  {
    "text": "plus max of A of this term and you can see that this is pretty intuitive.",
    "start": "448125",
    "end": "455720"
  },
  {
    "text": "What this means is, V star of s is- the best possible long-term value is equal to the sum of the current value that we got.",
    "start": "455720",
    "end": "464199"
  },
  {
    "text": "That's- that's immutable. The reward that we got at the current step is the reward that we got at the current step no matter what our policy",
    "start": "464200",
    "end": "470330"
  },
  {
    "text": "was and the- and choose.",
    "start": "470330",
    "end": "475444"
  },
  {
    "text": "This is a recursive definition. And choose an action such that we maximize the long-term reward at the next time step, right?",
    "start": "475445",
    "end": "483330"
  },
  {
    "text": "So depending on which action we choose, we get a different probability distribution for the future- different transition probability distribution,",
    "start": "484010",
    "end": "491810"
  },
  {
    "text": "and take the expectation of the next time step according to the probability distribution. So max is basically- the max operator",
    "start": "491810",
    "end": "500855"
  },
  {
    "text": "is choosing different transition probabilities essentially, right? Choose an action such that",
    "start": "500855",
    "end": "506195"
  },
  {
    "text": "the resulting transition probabilities will give you the highest expected reward, expected future value at the next time step.",
    "start": "506195",
    "end": "513710"
  },
  {
    "text": "It is this- this recursive definition which also happens to be called Bellman's equation confusingly.",
    "start": "513710",
    "end": "520550"
  },
  {
    "text": "It is also called the Bellman's equation.",
    "start": "520550",
    "end": "525269"
  },
  {
    "text": "And the corresponding to the value of the optimal value function there is this implicit optimal policy.",
    "start": "526090",
    "end": "534925"
  },
  {
    "text": "So the implicit optimal policy is any such policy that allows us to achieve the optimum value function, right?",
    "start": "534925",
    "end": "542425"
  },
  {
    "text": "In- in- in theory, it is possible that there exists multiple policies, so strictly speaking,",
    "start": "542425",
    "end": "550045"
  },
  {
    "text": "this should be element of the set of arg max because this arg max- probably this should not be element.",
    "start": "550045",
    "end": "560880"
  },
  {
    "text": "Yeah, so this should be an element because there could be multiple actions that you can take in any step that gives",
    "start": "560880",
    "end": "567560"
  },
  {
    "text": "you the optimal the- that maximizes the future value.",
    "start": "567560",
    "end": "574205"
  },
  {
    "text": "But for the purposes of this course, we'll assume that it is unique, right, and it is this optimal policy",
    "start": "574205",
    "end": "580175"
  },
  {
    "text": "that is kind of our- that's our end goal. That's what we strive to achieve.",
    "start": "580175",
    "end": "586535"
  },
  {
    "text": "And then we say we want to solve an MDP, it means we want to find the pi star, the optimal policy that in some way,",
    "start": "586535",
    "end": "594830"
  },
  {
    "text": "and that's called solving an MDP.",
    "start": "594830",
    "end": "597510"
  },
  {
    "text": "And then we- we saw the relation between policy and value, right?",
    "start": "600150",
    "end": "606610"
  },
  {
    "text": "The policy and value are kind of, they have this intimate relationship where given- given- given a policy Pi,",
    "start": "606610",
    "end": "614365"
  },
  {
    "text": "there is a corresponding value function that you achieve by just following the policy all the time, right?",
    "start": "614365",
    "end": "622089"
  },
  {
    "text": "And we saw- we- we, uh, saw this derivation last time, where given a policy Pi,",
    "start": "622090",
    "end": "628029"
  },
  {
    "text": "there is, uh, uh, an associated V Pi that can be derived in this way where this is the identity function and P Pi is the transition, uh,",
    "start": "628030",
    "end": "636160"
  },
  {
    "text": "transition matrix, where for each state we choose the corresponding P (s,a) according where the A is taken from Pi.",
    "start": "636160",
    "end": "643634"
  },
  {
    "text": "And- and so from- from-, uh, from Pi, we can recover a V Pi this way,",
    "start": "643635",
    "end": "650055"
  },
  {
    "text": "and this is called policy evaluation. I think that's one direction, the other direction where you're given a value function, right?",
    "start": "650055",
    "end": "657310"
  },
  {
    "text": "And this value function induces a policy, which is also called the greedy policy with respect to V, right?",
    "start": "657310",
    "end": "663265"
  },
  {
    "text": "And that policy is basically at any given state S, choose the action- the- the- action that is prescribed",
    "start": "663265",
    "end": "670209"
  },
  {
    "text": "by this policy is basically the action that maximizes the- the expected value",
    "start": "670210",
    "end": "676630"
  },
  {
    "text": "of the next time step where the value is defined by the given value function. All right? So we can- we can- we can go",
    "start": "676630",
    "end": "683700"
  },
  {
    "text": "between policy to value and value to policy using these two relations. Right? A subtle point to note is that given a policy,",
    "start": "683700",
    "end": "696564"
  },
  {
    "text": "we get- we get a corresponding value function that- that's kind of associated with this policy.",
    "start": "696564",
    "end": "702865"
  },
  {
    "text": "However, a subtle point which may not be obvious at- at first sight is if we were to",
    "start": "702865",
    "end": "708459"
  },
  {
    "text": "take this policy Pi that was induced in this greedy way from this value function, right?",
    "start": "708460",
    "end": "714430"
  },
  {
    "text": "If- if we were to follow this policy, the resulting value that we get is not this value function.",
    "start": "714430",
    "end": "719769"
  },
  {
    "text": "It may be a different value function, right? That's a subtle point. A value function induces a policy,",
    "start": "719770",
    "end": "725200"
  },
  {
    "text": "but if we were to follow this policy, it is not- we will not get the same value function. You need not get the same value function right?",
    "start": "725200",
    "end": "731360"
  },
  {
    "text": "And that a subtle point, and then that's something that we're going to see in- in policy iteration, right? And we saw one algorithm of solving",
    "start": "731360",
    "end": "740130"
  },
  {
    "text": "MDPs where the first algorithm we saw was value iteration. Now in value iteration,",
    "start": "740130",
    "end": "745825"
  },
  {
    "text": "it is an iterative algorithm, you know, which is called value iteration, it's an iterative algorithm where we start at some arbitrary value function.",
    "start": "745825",
    "end": "753265"
  },
  {
    "text": "It could be the value function is equal to, you can either set it to all zeros or you can initialize it to the reward function itself.",
    "start": "753265",
    "end": "759325"
  },
  {
    "text": "You start at some value function, right? And we iterated over and over where we take",
    "start": "759325",
    "end": "765735"
  },
  {
    "text": "the value function of the current iteration and run it through something what is called the Bellman Backup operator, right?",
    "start": "765735",
    "end": "771430"
  },
  {
    "text": "And we get the next value function and then we plug it in and get the next value function and so on. And the Bellman value operator is defined in this way.",
    "start": "771430",
    "end": "778375"
  },
  {
    "text": "So the Bellman Backup operator is, uh, the- the, uh, you know,",
    "start": "778375",
    "end": "783805"
  },
  {
    "text": "if- if this is V pri- V prime of s is equal to well V prime is Bellman operator on the",
    "start": "783805",
    "end": "791800"
  },
  {
    "text": "previous V. So the value function for the next time step is equal to R of s plus max of whatever expression here.",
    "start": "791800",
    "end": "800139"
  },
  {
    "text": "And this thing here is basically the right-hand side of the optimal value function. Right? And this is called the Bellman Backup operator.",
    "start": "800140",
    "end": "808645"
  },
  {
    "text": "And we run this algorithm in a loop and iteratively",
    "start": "808645",
    "end": "814165"
  },
  {
    "text": "until we converge on a V- a V function where the V function stops- stops- stops changing.",
    "start": "814165",
    "end": "820285"
  },
  {
    "text": "And once we get that V function, we can recover the corresponding policy using this expression.",
    "start": "820285",
    "end": "825790"
  },
  {
    "text": "So given a V that has converged through value iteration, recover a policy and that's our, you know, optimal policy,",
    "start": "825790",
    "end": "831805"
  },
  {
    "text": "that's our solution to an MDP. And the reason why this converges, we saw that, uh,",
    "start": "831805",
    "end": "836905"
  },
  {
    "text": "it is possible to show that this Bellman op, um, Bellman backup operator is what is called as a contraction mapping.",
    "start": "836905",
    "end": "843880"
  },
  {
    "text": "So if we imagine the space of function-a functional space where each point here defines a value function,",
    "start": "843880",
    "end": "851245"
  },
  {
    "text": "not- a point here represents an entire value function where the coordinate of that,",
    "start": "851245",
    "end": "857470"
  },
  {
    "text": "uh, where a coordinate along a particular axis is the value of the value function along that state, right?",
    "start": "857470",
    "end": "864490"
  },
  {
    "text": "And the Bellman Backup operator is a contraction mapping, which means if we take any two value functions,",
    "start": "864490",
    "end": "871270"
  },
  {
    "text": "run the two of them through the Bellman operator, right? The distance between the outputs will be",
    "start": "871270",
    "end": "876339"
  },
  {
    "text": "closer than the distance between the inputs, right? That's why it's called a contraction mapping because as you- as you apply",
    "start": "876340",
    "end": "881709"
  },
  {
    "text": "points through the Bellman Backup operator or through this contraction mapping, they get closer and closer, right?",
    "start": "881710",
    "end": "886960"
  },
  {
    "text": "And the point at which they converge is called the fixed point of that operator, and that fixed point is V star,",
    "start": "886960",
    "end": "893140"
  },
  {
    "text": "that V star is the optimal value.",
    "start": "893140",
    "end": "895970"
  },
  {
    "text": "And then we saw this other algorithm called policy iteration, right? So this- this was- this is one possible algorithm for solving an MDP.",
    "start": "898170",
    "end": "907675"
  },
  {
    "start": "900000",
    "end": "1020000"
  },
  {
    "text": "This is another algorithm for solving an MDP. And in this algorithm, we basically exploit the subtlety that I mentioned earlier,",
    "start": "907675",
    "end": "915355"
  },
  {
    "text": "that from a given Pi, we get a corresponding V, where this V is the value that you get by following,",
    "start": "915355",
    "end": "923035"
  },
  {
    "text": "uh, the policy Pi. However, there is a- a way from which, you know, the- the- others- other direction where we recover",
    "start": "923035",
    "end": "929920"
  },
  {
    "text": "a policy by using the guidance of some value function, right? We use this value function estimate to choose",
    "start": "929920",
    "end": "937270"
  },
  {
    "text": "a particular policy to do- where we strive to do better, where we strive to do our best given the current policy.",
    "start": "937270",
    "end": "944079"
  },
  {
    "text": "And using this policy, if we actually use this policy, the resulting value function may be different, right?",
    "start": "944080",
    "end": "949704"
  },
  {
    "text": "And that is the subtlety that we make- that we use over here. So start with some randomly initialized policy",
    "start": "949705",
    "end": "955480"
  },
  {
    "text": "so random init of Pi, right?",
    "start": "955480",
    "end": "963595"
  },
  {
    "text": "And from this random initialization of Pi we, you know, we do step one from- for a given Pi,",
    "start": "963595",
    "end": "969970"
  },
  {
    "text": "we use, uh, um, we- we calculate the corresponding value function, which is- which is just what we call as policy- a policy evaluation, right?",
    "start": "969970",
    "end": "977410"
  },
  {
    "text": "And once we get the policy, uh, the- the corresponding policy V Pi, we get, you know,",
    "start": "977410",
    "end": "982779"
  },
  {
    "text": "we- we- we try to recover- we try to calculate the optimal policy for this V pi.",
    "start": "982780",
    "end": "989515"
  },
  {
    "text": "Right? And we do this over and over until Pi converges. And this is called policy iteration,",
    "start": "989515",
    "end": "996640"
  },
  {
    "text": "and the- the converged Pi, will be Pi star.",
    "start": "996640",
    "end": "1004695"
  },
  {
    "text": "And- and once we get Pi star, we can again recover V star and the Pi star and V star that we",
    "start": "1004695",
    "end": "1012420"
  },
  {
    "text": "get from this algorithm and a Pi star and V star we get from this algorithms are essentially the same.",
    "start": "1012420",
    "end": "1017565"
  },
  {
    "text": "Right? So this is- this is how much we covered last- in the last class.",
    "start": "1017565",
    "end": "1023100"
  },
  {
    "start": "1020000",
    "end": "1390000"
  },
  {
    "text": "Any questions on this before we jump into today's topics? Right, cool. So today we're gonna start seeing extensions of these methods.",
    "start": "1023100",
    "end": "1033240"
  },
  {
    "text": "[NOISE]",
    "start": "1033240",
    "end": "1059675"
  },
  {
    "text": "So the first extension we're going to see is what happens when we- when we don't know Psa.",
    "start": "1059675",
    "end": "1065930"
  },
  {
    "text": "When- what happens when Psa is not given?",
    "start": "1065930",
    "end": "1070940"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "1070940",
    "end": "1076129"
  },
  {
    "text": "So this is our first, um, first extension [NOISE] and this corresponds to section 3 in the notes,",
    "start": "1076130",
    "end": "1084184"
  },
  {
    "text": "in learning a model for an MDP. So here, when we, um, in reinforcement learning, um, you, you,",
    "start": "1084185",
    "end": "1092365"
  },
  {
    "text": "you- when you, when you see the literature, when you read text books and, and papers and so on, you will see this terminology called model-based versus model-free.",
    "start": "1092365",
    "end": "1100540"
  },
  {
    "text": "[NOISE]",
    "start": "1100540",
    "end": "1109340"
  },
  {
    "text": "So over here, in, in, in this terminology, model does not mean the model that we are constructing, right?",
    "start": "1109340",
    "end": "1116044"
  },
  {
    "text": "So model over here refers to the model of the environmental model of the universe in which the agent is operating, right?",
    "start": "1116045",
    "end": "1123080"
  },
  {
    "text": "Now, it is the model that this is referring to, right? And this essentially means- here model means [NOISE] Psa.",
    "start": "1123080",
    "end": "1131745"
  },
  {
    "text": "Now, if you know Psa, if you know how- uh, what state you might end up in, depending on what action you took,",
    "start": "1131745",
    "end": "1138115"
  },
  {
    "text": "it means you know how the environment works, right? You know- you have a model of the environment. That's what this model means, right?",
    "start": "1138115",
    "end": "1144335"
  },
  {
    "text": "Now, Psa is also called the model and the, uh, the, the, the first topic that we're going to cover is learning a model.",
    "start": "1144335",
    "end": "1151460"
  },
  {
    "text": "[NOISE] Again, here, by learning a model, I mean learning Psa.",
    "start": "1151460",
    "end": "1156620"
  },
  {
    "text": "[NOISE] So supposing we don't know how the, um, how,",
    "start": "1156620",
    "end": "1164600"
  },
  {
    "text": "how, how and how, how the transitions work, we don't know what, what state we might end up with, and supposing this,",
    "start": "1164600",
    "end": "1170165"
  },
  {
    "text": "you know- ju- just assume Psa is not given, what do we do? And when Psa is not given,",
    "start": "1170165",
    "end": "1175520"
  },
  {
    "text": "the one- one thing that we can do is basically, um, assuming we have some kind of a nom- um,",
    "start": "1175520",
    "end": "1182540"
  },
  {
    "text": "a simulator where, you know, we can- we can, uh, run these experiments. Start, ru- run multiple trials [NOISE] okay.",
    "start": "1182540",
    "end": "1191600"
  },
  {
    "text": "Run multiple trials where in each step you start at some- some state S naught, take a random action,",
    "start": "1191600",
    "end": "1199855"
  },
  {
    "text": "you know, you're naught, you end up in S1. [NOISE] Take another random action,",
    "start": "1199855",
    "end": "1206389"
  },
  {
    "text": "[NOISE] S2, [NOISE] and so on,",
    "start": "1206390",
    "end": "1215365"
  },
  {
    "text": "and then repeat the trial S naught,  [NOISE]",
    "start": "1215365",
    "end": "1230990"
  },
  {
    "text": "and so on, right? So each horizontal line corresponds to one trial, right?",
    "start": "1230990",
    "end": "1237665"
  },
  {
    "text": "In, uh, in the homework that you will see that we'll be releasing on Monday so that, um, you can think of each trial.",
    "start": "1237665",
    "end": "1243980"
  },
  {
    "text": "So, so the homework problem is about, uh, uh, balancing the cartwheel or the inverted pendulum where the goal is, um,",
    "start": "1243980",
    "end": "1251090"
  },
  {
    "text": "you're given, um, a stick and you need- which is, uh, which has one degree of freedom, which can move around and it's balanced on, uh, on a cartwheel.",
    "start": "1251090",
    "end": "1259025"
  },
  {
    "text": "The cart will also has one degree of freedom and you need to keep moving the cartwheel to keep the stick balanced upright, right?",
    "start": "1259025",
    "end": "1266390"
  },
  {
    "text": "And, uh, a trial in such a case would be starting at some initial configuration and initial position and",
    "start": "1266390",
    "end": "1271700"
  },
  {
    "text": "initial angle of the stick and initial, um, uh, velocity of the cart, and the actions in that case would be to move the cart left or move,",
    "start": "1271700",
    "end": "1280070"
  },
  {
    "text": "move the cart right, right? So take some random actions starting from some initial state until, you know,",
    "start": "1280070",
    "end": "1285215"
  },
  {
    "text": "the, the, the, the stick falls down or some such terminating condition, [NOISE] right?",
    "start": "1285215",
    "end": "1291154"
  },
  {
    "text": "And then start over again with a different starting, um, um, state and, you know, take different actions.",
    "start": "1291155",
    "end": "1296600"
  },
  {
    "text": "You may be following some particular policy. You may be taking random actions. It does not matter. We are basically exploring the environment.",
    "start": "1296600",
    "end": "1303740"
  },
  {
    "text": "We are learning the environment, okay, and, uh, take, uh, some such actions like this and then construct",
    "start": "1303740",
    "end": "1311600"
  },
  {
    "text": "[NOISE] estimators for Psa from this.",
    "start": "1311600",
    "end": "1317360"
  },
  {
    "text": "[NOISE]",
    "start": "1317360",
    "end": "1325580"
  },
  {
    "text": "So P- I'm just going to call it P-hat, sa of S prime is equal to the number of times,",
    "start": "1325580",
    "end": "1338750"
  },
  {
    "text": "[NOISE] number of times, uh,",
    "start": "1338750",
    "end": "1347495"
  },
  {
    "text": "we took action A [NOISE] at state S and",
    "start": "1347495",
    "end": "1357960"
  },
  {
    "text": "got to state S prime over a number of",
    "start": "1362950",
    "end": "1373534"
  },
  {
    "text": "times we took action",
    "start": "1373535",
    "end": "1379520"
  },
  {
    "text": "A at state S, right?",
    "start": "1379520",
    "end": "1385535"
  },
  {
    "text": "So run such trials, lots and lots of them and once,",
    "start": "1385535",
    "end": "1392225"
  },
  {
    "start": "1390000",
    "end": "1810000"
  },
  {
    "text": "once you, um, run all these trials, we just count the number of times we were at state, some particular S,",
    "start": "1392225",
    "end": "1400220"
  },
  {
    "text": "and took a particular action A and that goes into the denominator, and the num- the number of times when we took that action in that state.",
    "start": "1400220",
    "end": "1410345"
  },
  {
    "text": "What- what fraction of times did we arrive at state S prime. Okay. So this is the simple maximum likelihood estimator of- of,",
    "start": "1410345",
    "end": "1417665"
  },
  {
    "text": "uh, the- the transition probabilities. All right. And it- it might so happen that sometimes both of these,",
    "start": "1417665",
    "end": "1427130"
  },
  {
    "text": "uh, could end up being 0 or 0, right? We saw this when in- in Naive Bayes when you're trying",
    "start": "1427130",
    "end": "1432860"
  },
  {
    "text": "to es- get maximum likelihood estimates based on, um, accounts such as this. It may so happen that for some of the cases,",
    "start": "1432860",
    "end": "1439775"
  },
  {
    "text": "we may, uh, we may have 0 over 0. In those cases, it is, um, common to, you know,",
    "start": "1439775",
    "end": "1446044"
  },
  {
    "text": "a common practice in such cases when we get 0 over 0 is to assume a uniform,",
    "start": "1446045",
    "end": "1455795"
  },
  {
    "text": "um, a uniform distribution for uh, uh, Psa, which means, uh,",
    "start": "1455795",
    "end": "1461285"
  },
  {
    "text": "if we have never reached the state S and taken an action A from state S,",
    "start": "1461285",
    "end": "1466400"
  },
  {
    "text": "just assume that we are, you know, um, that we are going to- we're going to, um, um, um, uniformly end up in any one of them.",
    "start": "1466400",
    "end": "1473120"
  },
  {
    "text": "Now, the moment we actually end up taking an action at state S,",
    "start": "1473120",
    "end": "1478235"
  },
  {
    "text": "we can then update our counts to reflect what the, the actual, um, the,",
    "start": "1478235",
    "end": "1484040"
  },
  {
    "text": "the actual dir- distribution was. The moment we reach, uh, uh, an- a state S and perform an action A,",
    "start": "1484040",
    "end": "1491105"
  },
  {
    "text": "the denominator is no longer 0, it will be 1, right? And then we- we- we won't have a 0 over 0.",
    "start": "1491105",
    "end": "1496220"
  },
  {
    "text": "Uh, until then, you know, it's common to, um, assume a uniform action, uh, er,",
    "start": "1496220",
    "end": "1501575"
  },
  {
    "text": "assume a uniform distribution over, uh, fu- future states. Okay. And using this,",
    "start": "1501575",
    "end": "1510634"
  },
  {
    "text": "this kind of an estimate, estimation of the, of the states o- of the, uh, transition probabilities, we can now construct an algorithm.",
    "start": "1510635",
    "end": "1519750"
  },
  {
    "text": "It looks like this. [NOISE] So this",
    "start": "1527140",
    "end": "1533300"
  },
  {
    "text": "is an algorithm with model learning.",
    "start": "1533300",
    "end": "1542075"
  },
  {
    "text": "Again, here, by model I mean Psa, the transient probabilities. [NOISE] Okay.",
    "start": "1542075",
    "end": "1550790"
  },
  {
    "text": "So [NOISE] initialize [NOISE] randomly,",
    "start": "1550790",
    "end": "1558140"
  },
  {
    "text": "[NOISE] Okay. No. [NOISE] Two, uh, [NOISE] repeat.",
    "start": "1558140",
    "end": "1569300"
  },
  {
    "text": "[NOISE]",
    "start": "1569300",
    "end": "1574655"
  },
  {
    "text": "Execute Pi for some time.",
    "start": "1574655",
    "end": "1581820"
  },
  {
    "text": "Right. Which means basically- no step one or 2a rather- 2a.",
    "start": "1582400",
    "end": "1590825"
  },
  {
    "text": "Right. So run a few trials where the actions you are choosing are based on the random initialization of Pi.",
    "start": "1590825",
    "end": "1598740"
  },
  {
    "text": "B,",
    "start": "1599260",
    "end": "1603180"
  },
  {
    "text": "using these trials",
    "start": "1606310",
    "end": "1611820"
  },
  {
    "text": "estimate [NOISE] P hat of sa [NOISE] and by estimate P-hat of sa.",
    "start": "1613360",
    "end": "1624035"
  },
  {
    "text": "Basically, this means estimate s times a number of",
    "start": "1624035",
    "end": "1629419"
  },
  {
    "text": "probability distribution where each distribution is over all the future states. Right. For each s, a.",
    "start": "1629420",
    "end": "1638990"
  },
  {
    "text": "Right. So there are s times a number of such probability distributions, so estimate all of them.",
    "start": "1638990",
    "end": "1645570"
  },
  {
    "text": "C, [NOISE] apply value iteration.",
    "start": "1647440",
    "end": "1655519"
  },
  {
    "text": "I'm just going to write it as VI. Value iteration using P-hat sa- using",
    "start": "1655520",
    "end": "1666740"
  },
  {
    "text": "P-hat sa to get",
    "start": "1666740",
    "end": "1672600"
  },
  {
    "text": "updated V- Let's call it V^t.",
    "start": "1674230",
    "end": "1680750"
  },
  {
    "text": "Right. And D, update Pi [NOISE]",
    "start": "1680750",
    "end": "1692830"
  },
  {
    "text": "to be greedy",
    "start": "1692830",
    "end": "1697940"
  },
  {
    "text": "with respect to V^t.",
    "start": "1697940",
    "end": "1704779"
  },
  {
    "text": "Right. So what's this algorithm telling us? Start at some- some random initialization of- of policy- some random policy, right.",
    "start": "1704780",
    "end": "1714890"
  },
  {
    "text": "And follow that policy for some time, and by following this policy for some time,",
    "start": "1714890",
    "end": "1720275"
  },
  {
    "text": "basically what we are trying to do is ah, not only learn um,",
    "start": "1720275",
    "end": "1725284"
  },
  {
    "text": "an estimate of our value function, but we're also learning Psa as well, right?",
    "start": "1725285",
    "end": "1730970"
  },
  {
    "text": "By following some- some um, policy, we are learning how the environment works. You take some action from a current state,",
    "start": "1730970",
    "end": "1736910"
  },
  {
    "text": "you end up in some new- new state and you keep observing that again and again. You tend to learn how the world works.",
    "start": "1736910",
    "end": "1743570"
  },
  {
    "text": "You tend to learn how the dynamics of the system works, right? That's like a separate task from estimating the value function, right?",
    "start": "1743570",
    "end": "1751280"
  },
  {
    "text": "Learning how the dynamics of the environment works as a separate task from learning the value function corresponding to that policy.",
    "start": "1751280",
    "end": "1759750"
  },
  {
    "text": "Using this experience, we can estimate P hat sa. It's important to note that learning",
    "start": "1759760",
    "end": "1767780"
  },
  {
    "text": "P-hat sa is independent of the policy that we were following, right?",
    "start": "1767780",
    "end": "1773225"
  },
  {
    "text": "And in fact, what we can do is to estimate P hat sa. In each iteration, we can actually use the accumulated counts from",
    "start": "1773225",
    "end": "1781790"
  },
  {
    "text": "all the previous iterations as well to get a better estimate of- of the P-hat sa.",
    "start": "1781790",
    "end": "1787370"
  },
  {
    "text": "Right. And using the estimated P-hat sa, we perform value iteration.",
    "start": "1787370",
    "end": "1792889"
  },
  {
    "text": "Now what was value iteration? In value iteration. [NOISE] To remind you this was value iteration.",
    "start": "1792890",
    "end": "1800360"
  },
  {
    "text": "[NOISE] In value iteration, we basically loop ah.",
    "start": "1800360",
    "end": "1806135"
  },
  {
    "text": "So this- this line over here, apply value iteration means perform this loop until convergence.",
    "start": "1806135",
    "end": "1814850"
  },
  {
    "text": "Right? So step 2c is actually a full loop of value iteration.",
    "start": "1814850",
    "end": "1820775"
  },
  {
    "text": "And in this value iteration, we see that we need to take an expectation as part of computing value iteration.",
    "start": "1820775",
    "end": "1828890"
  },
  {
    "text": "And it is for this expectation where we use the P-hat sa's instead of Psa's. We don't know Psa.",
    "start": "1828890",
    "end": "1835985"
  },
  {
    "text": "We are just- we are using data to construct an estimate P-hat sa.",
    "start": "1835985",
    "end": "1841700"
  },
  {
    "text": "It's this P-hat sa- estimating this P-hat sa that we call as learning a model, right?",
    "start": "1841700",
    "end": "1847325"
  },
  {
    "text": "We use the estimated dynamics of the environment to- to ah, to perform value iteration and read some V star.",
    "start": "1847325",
    "end": "1855940"
  },
  {
    "text": "So it is the V star of that particular iteration. And the V star will change from iteration to iteration of that algorithm because",
    "start": "1855940",
    "end": "1864610"
  },
  {
    "text": "we are using different dynamics in each iteration. Yes, question. [inaudible]",
    "start": "1864610",
    "end": "1887380"
  },
  {
    "text": "[NOISE]. Yeah, good question. So the question is, why not just perform a and b over and over until",
    "start": "1887380",
    "end": "1892570"
  },
  {
    "text": "we are satisfied and then do c and d. The reason why ah, you know, um, that might not work all the time is because unless you have a good policy,",
    "start": "1892570",
    "end": "1900830"
  },
  {
    "text": "you may not even reach certain states. But, you know, you have to- like to be able to understand again,",
    "start": "1900830",
    "end": "1908659"
  },
  {
    "text": "you have to exhaust [inaudible]. Okay? So- so the- so the question is, we nee- why not exhaustively search sa,",
    "start": "1908660",
    "end": "1916700"
  },
  {
    "text": "all the states and all the actions. All tuples. All tuples. And the reason that might not be possible in practice is because,",
    "start": "1916700",
    "end": "1923705"
  },
  {
    "text": "we may not be able to arbitrarily choose an s over our choice.",
    "start": "1923705",
    "end": "1928730"
  },
  {
    "text": "Right. We- we- we may be limited to start from certain starting initial states and explore by taking actions.",
    "start": "1928730",
    "end": "1937070"
  },
  {
    "text": "Maybe, it may not be possible to say, you know, I want to start at some particular ah,",
    "start": "1937070",
    "end": "1942200"
  },
  {
    "text": "um- some particular state at some time. And to- to exhaustively try",
    "start": "1942200",
    "end": "1947705"
  },
  {
    "text": "all possible s and a sequences can be exponentially, you know, complex. It may, you may not be able to do",
    "start": "1947705",
    "end": "1953690"
  },
  {
    "text": "a brute force and do a full sweep that could take, you know, that could be exponentially logically, it could take years, for example, right,",
    "start": "1953690",
    "end": "1959375"
  },
  {
    "text": "to do a full sweep of the entire state and action space. And that's why we- we- we learned,",
    "start": "1959375",
    "end": "1965630"
  },
  {
    "text": "we follow this iterative approach where we- where we try some- try some initial trials for some policy,",
    "start": "1965630",
    "end": "1974390"
  },
  {
    "text": "get an estimate and then perform value iteration and use that value iteration to drive the exploration in the next state.",
    "start": "1974390",
    "end": "1980090"
  },
  {
    "text": "Right. Yes, question.",
    "start": "1980090",
    "end": "1983760"
  },
  {
    "text": "[inaudible] So you say one",
    "start": "1987190",
    "end": "2007039"
  },
  {
    "text": "of the problem [inaudible] you can use part of two episodes as new.",
    "start": "2009750",
    "end": "2017970"
  },
  {
    "text": "So the question is, can we use parts of es- episodes as new episodes? And the answer is to- to- uh, to calculate this,",
    "start": "2017970",
    "end": "2026630"
  },
  {
    "text": "we're only calculating, you know, these links at a time, right?",
    "start": "2026630",
    "end": "2034580"
  },
  {
    "text": "We take S, a, and S prime and count them to- to, uh, estimate this.",
    "start": "2034580",
    "end": "2040565"
  },
  {
    "text": "So when we're- uh, whe- when we're estimating the probabilities, we take all the episodes that we have and only look at every consecutive S,",
    "start": "2040565",
    "end": "2047585"
  },
  {
    "text": "a, S prime, uh, triples. All right, so this is an algorithm that- that we get,",
    "start": "2047585",
    "end": "2055639"
  },
  {
    "text": "where we are simultaneously performing value iteration and also learning the dynamics of the system.",
    "start": "2055640",
    "end": "2062224"
  },
  {
    "text": "So at this step, we will achieve the optimal value function, assuming a gi- a certain given dynamics, right?",
    "start": "2062225",
    "end": "2070010"
  },
  {
    "text": "For this particular dynamics of P hat_sa that we've- that we've learned, the resulting V at the end of one loop of- of- uh,",
    "start": "2070010",
    "end": "2077960"
  },
  {
    "text": "of this algorithm will give us the V star corresponding to that P hat_sa, right? But that P hat_sa might not be the-",
    "start": "2077960",
    "end": "2085550"
  },
  {
    "text": "the correct dynamics of the system because it's just some kind of an estimate, right? And then we use this- the resulting, uh, uh,",
    "start": "2085550",
    "end": "2093500"
  },
  {
    "text": "pi star, to go back and perform so- get some more experience, right?",
    "start": "2093500",
    "end": "2099200"
  },
  {
    "text": "Run some more trials, improve our P hat_sa, and then re-estimate, um, uh, the value iteration.",
    "start": "2099200",
    "end": "2105815"
  },
  {
    "text": "Now, one obvious, um, optimization or computational optimization is",
    "start": "2105815",
    "end": "2112970"
  },
  {
    "text": "that when we start the value iteration in some- in some loop,",
    "start": "2112970",
    "end": "2118055"
  },
  {
    "text": "when- when we describe value iteration, we would always start, uh, the value function at zero.",
    "start": "2118055",
    "end": "2124250"
  },
  {
    "text": "So instead, what we can do is, for the next- in the next iteration, when we start value iteration,",
    "start": "2124250",
    "end": "2130235"
  },
  {
    "text": "we can initialize the value function at the same point where we had converged in the previous function,",
    "start": "2130235",
    "end": "2135845"
  },
  {
    "text": "instead of starting it at 0 all the time, right? And that will- that will, uh, uh, allow us to have, uh, a much more faster, uh, uh,",
    "start": "2135845",
    "end": "2142760"
  },
  {
    "text": "convergence in step c. Any questions on this?",
    "start": "2142760",
    "end": "2150485"
  },
  {
    "text": "Cool. [NOISE]",
    "start": "2150485",
    "end": "2156980"
  },
  {
    "text": "Question.",
    "start": "2156980",
    "end": "2158190"
  },
  {
    "text": "Yes, question? Can you explain what step d [inaudible]",
    "start": "2162490",
    "end": "2168140"
  },
  {
    "text": "Step d? Yes, wha- what [NOISE] So step d, um, you're saying, you know, update pi to be, uh, greedy with respect to V^t,",
    "start": "2168140",
    "end": "2174185"
  },
  {
    "text": "and that is, um, [NOISE] this answer, step 2. You have- you- you get some V and you get the pi corresponding to that V.",
    "start": "2174185",
    "end": "2183140"
  },
  {
    "text": "[NOISE]",
    "start": "2183140",
    "end": "2188930"
  },
  {
    "text": "So- so value iteration and policy iteration,",
    "start": "2188930",
    "end": "2194030"
  },
  {
    "text": "they assume that we know the dynamics, right? And when we don't know the dynamics, what- what the exact transition probabilities are, you know,",
    "start": "2194030",
    "end": "2201215"
  },
  {
    "text": "you can think of that as one relaxation of the MDP where we don't know what the exact transition probabilities is.",
    "start": "2201215",
    "end": "2207155"
  },
  {
    "text": "And- and that gives us this algorithm. This algorithm, where we are simultaneously learning the dynamics,",
    "start": "2207155",
    "end": "2214700"
  },
  {
    "text": "learning the model, and also performing value iteration according to the learned dynamics.",
    "start": "2214700",
    "end": "2220415"
  },
  {
    "start": "2220000",
    "end": "2290000"
  },
  {
    "text": "Next, we will, um, see another relaxation, which is- thus far we have limited our- our study to cases of- uh,",
    "start": "2220415",
    "end": "2232069"
  },
  {
    "text": "or the types of MDP where the states and the actions were discrete and finite, right?",
    "start": "2232070",
    "end": "2237635"
  },
  {
    "text": "We were limited to finite state and finite actions. Now, what happens when the states is continuous, right?",
    "start": "2237635",
    "end": "2243845"
  },
  {
    "text": "What happens if, uh, the set of all possible states is- is, uh, continuous?",
    "start": "2243845",
    "end": "2249530"
  },
  {
    "text": "For example, um, [NOISE] in the, uh, inverted pendulum problem that you'll be seeing in your next p set,",
    "start": "2249530",
    "end": "2256070"
  },
  {
    "text": "the- the- the position of the pendulum, which- which you're trying to maintain it upright.",
    "start": "2256070",
    "end": "2262205"
  },
  {
    "text": "The state of this- the pendulum can be represented by the angle it makes from- the angle it makes from the horizontal- uh,",
    "start": "2262205",
    "end": "2270770"
  },
  {
    "text": "uh, uh, uh, horizontal floor or the horizontal table, wherever it is placed, right? And that angle is continuous, right?",
    "start": "2270770",
    "end": "2277025"
  },
  {
    "text": "So in- in- uh, in- in, kind of, reality, the state of the pendulum or the state of your system or your agent is continuous.",
    "start": "2277025",
    "end": "2287150"
  },
  {
    "text": "And what do we do in that s- uh, in- in such situations? One obvious answer or one simplistic approach is what is called as discretization.",
    "start": "2287150",
    "end": "2296750"
  },
  {
    "start": "2290000",
    "end": "2520000"
  },
  {
    "text": "[NOISE] In discretization, the idea is pretty simple.",
    "start": "2296750",
    "end": "2306570"
  },
  {
    "text": "Supposing our state- so, um, assume that, uh,",
    "start": "2307240",
    "end": "2313970"
  },
  {
    "text": "he- this is the S space of,",
    "start": "2313970",
    "end": "2319505"
  },
  {
    "text": "say, the inverted pendulum problems. So as a reminder, in the inverted pendulum problem, uh, we have a cart, right?",
    "start": "2319505",
    "end": "2326890"
  },
  {
    "text": "That can move in on- along one direction. And on- on top of that, we have a stick that you're trying to balance.",
    "start": "2326890",
    "end": "2333204"
  },
  {
    "text": "And the stick could, you know, tilt this way or it could tilt this way according to the dynamics of the system, right?",
    "start": "2333205",
    "end": "2339950"
  },
  {
    "text": "And if it is tilting this way, then we want to have a policy where we- where we recognize that when, you know, um,",
    "start": "2339950",
    "end": "2345800"
  },
  {
    "text": "the- the angle is turned- is- is tilted to the left, then we want to move the cart to the left.",
    "start": "2345800",
    "end": "2351725"
  },
  {
    "text": "So the cart has actions: move left or move right, like, two actions. And the state can be, you know,",
    "start": "2351725",
    "end": "2356855"
  },
  {
    "text": "any possible angle and any possible velocity, any possible position of this- um,",
    "start": "2356855",
    "end": "2363079"
  },
  {
    "text": "uh, of this cart, right? So, uh, assume we have, uh, um,",
    "start": "2363080",
    "end": "2369035"
  },
  {
    "text": "angle and [NOISE] velocity, right?",
    "start": "2369035",
    "end": "2375545"
  },
  {
    "text": "So this- this space represents the set of all states that the system can be.",
    "start": "2375545",
    "end": "2383119"
  },
  {
    "text": "However, um, the algorithms that we've studied so far are all designed for finite state and finite actions, right?",
    "start": "2383120",
    "end": "2392690"
  },
  {
    "text": "So a common- a common technique, uh, in- in such cases is to discretize",
    "start": "2392690",
    "end": "2399859"
  },
  {
    "text": "the- [NOISE] your state space into discrete parts,",
    "start": "2399860",
    "end": "2408470"
  },
  {
    "text": "[NOISE] right?",
    "start": "2408470",
    "end": "2418070"
  },
  {
    "text": "So what used to be, um, um, a continuous problem, by discretization, we can turn it into a discrete problem or a discrete set.",
    "start": "2418070",
    "end": "2427295"
  },
  {
    "text": "And we generally have some kind of a bound, um, on- on th- on the- on the, uh, state space.",
    "start": "2427295",
    "end": "2434180"
  },
  {
    "text": "For example, angles are kind of limited between, you know, zero and 180 degrees.",
    "start": "2434180",
    "end": "2439520"
  },
  {
    "text": "So, you know, the- there is a natural boundary, uh, for- even though it is continuous, there's a natural boundary,",
    "start": "2439520",
    "end": "2445280"
  },
  {
    "text": "uh, on- on both sides. And similarly, the velocity can be bounded within zero and the maximum velocity the cart can go,",
    "start": "2445280",
    "end": "2450935"
  },
  {
    "text": "you know, whatever, 10 miles per hour, so some- some- some- some such thing, right? And then we can discretize this state space into",
    "start": "2450935",
    "end": "2458000"
  },
  {
    "text": "a discrete number of actual states or the discrete number of, um, you know, virtual states that we'll be working with. Yes, question?",
    "start": "2458000",
    "end": "2466985"
  },
  {
    "text": "So and, uh- and so you- you- in- in the- let's say in the piece, right? You will have to program it with,",
    "start": "2466985",
    "end": "2472820"
  },
  {
    "text": "like, let's say, some laws of physics, right? Like you would [inaudible] might even extend differential equations which",
    "start": "2472820",
    "end": "2479050"
  },
  {
    "text": "would [inaudible] And for the state space we can also exploit some kind of symmetry, right? Like, so we don't have to [OVERLAPPING]",
    "start": "2479050",
    "end": "2484730"
  },
  {
    "text": "Yeah. So the question is we could- we can limit this more using, you know, a domain knowledge like symmetry and whatnot.",
    "start": "2484730",
    "end": "2491704"
  },
  {
    "text": "But that's not the point here. So the- the main- the main concern with such approaches which are coming to next",
    "start": "2491705",
    "end": "2498710"
  },
  {
    "text": "is once we- once we come up with a discretized state,",
    "start": "2498710",
    "end": "2504980"
  },
  {
    "text": "S prime or S hat, which where S was continuous and this is discrete.",
    "start": "2504980",
    "end": "2514730"
  },
  {
    "text": "So this is the discretized version of- of S. [BACKGROUND] A common problem that we will see is,",
    "start": "2514730",
    "end": "2521855"
  },
  {
    "text": "you know, there are basically two problems. So the first problem is um, [NOISE] so the first problem",
    "start": "2521855",
    "end": "2534980"
  },
  {
    "text": "is, now what happens? Let's -let's kind of switch back to supervised learning and linear regression.",
    "start": "2534980",
    "end": "2545435"
  },
  {
    "text": "Right? Let's- let's step away from reinforcement learning for a brief moment. So if we had a data-set like this,t",
    "start": "2545435",
    "end": "2551270"
  },
  {
    "text": "[NOISE] What kind of a model would you try to fit?",
    "start": "2551270",
    "end": "2558950"
  },
  {
    "text": "Supposing this is X and this is Y, What kind of a model would we try to fit? Linear. Linear right? So this- this looks like a perfect um, you know,",
    "start": "2558950",
    "end": "2567575"
  },
  {
    "text": "perfect textbook case of a data-set where linear regression would work very well.",
    "start": "2567575",
    "end": "2573155"
  },
  {
    "text": "Right? Now in lin- in linear regression, we would just fit a straight line through this. However if we were to perform discretization and",
    "start": "2573155",
    "end": "2581690"
  },
  {
    "text": "learn the appropriate Y-value within each discrete unit,",
    "start": "2581690",
    "end": "2587569"
  },
  {
    "text": "we would get something like this. Right?",
    "start": "2587570",
    "end": "2590670"
  },
  {
    "text": "Which means now we are, if we discretize it into- into",
    "start": "2597760",
    "end": "2604070"
  },
  {
    "text": "bins and take the average in each bin to be the estimator for that bin, right? That's essentially what we're doing from- moving",
    "start": "2604070",
    "end": "2611240"
  },
  {
    "text": "from a continuous state to a discrete state by discretizing, we would get a hypothesis like this.",
    "start": "2611240",
    "end": "2617570"
  },
  {
    "text": "This is pretty bad because first, it is much more complex, right?",
    "start": "2617570",
    "end": "2623090"
  },
  {
    "text": "The number of parameters we need is the number of bins that we are breaking down- breaking this down into.",
    "start": "2623090",
    "end": "2629460"
  },
  {
    "text": "And further, we are not getting any kind of generalization. Now what happens if our dataset had some examples here and some examples here,",
    "start": "2629460",
    "end": "2637369"
  },
  {
    "text": "and we did not have any examples here. Discretization would not work because we have never- we have",
    "start": "2637370",
    "end": "2643010"
  },
  {
    "text": "never seen what's- what happens in the bin. So in these regions, you know,",
    "start": "2643010",
    "end": "2650135"
  },
  {
    "text": "we would have zero examples and discretization is- is a pretty bad thing to do in such cases, right?",
    "start": "2650135",
    "end": "2657380"
  },
  {
    "text": "Instead, we would rather have a simple straight line, right? Which uses a much smaller number of parameters.",
    "start": "2657380",
    "end": "2665585"
  },
  {
    "text": "Also it can interpolate well in regions where we don't have a lot of examples. Right. So this- this,",
    "start": "2665585",
    "end": "2672395"
  },
  {
    "text": "this idea of- of having some kind of a fitted function not only reduces the number of parameters that we need,",
    "start": "2672395",
    "end": "2682520"
  },
  {
    "text": "but we also get some amount of generalization into regions where we don't have a lot of- lot of examples.",
    "start": "2682520",
    "end": "2689030"
  },
  {
    "text": "[NOISE]  And that's the idea that we're going to [NOISE] look into next.",
    "start": "2689030",
    "end": "2695880"
  },
  {
    "start": "2690000",
    "end": "2750000"
  },
  {
    "text": "So as far as your homework problem is concerned,",
    "start": "2697060",
    "end": "2702245"
  },
  {
    "text": "we will actually be solving it with discretization. That's because it's a simple problem and that's, you know, it's,",
    "start": "2702245",
    "end": "2708965"
  },
  {
    "text": "the purpose of the homework is to- is to give you a framework to understand",
    "start": "2708965",
    "end": "2714095"
  },
  {
    "text": "value iteration and estimating- estimating the transition probabilities, etc.",
    "start": "2714095",
    "end": "2719765"
  },
  {
    "text": "However, in- in larger problems, in more complex problems, discretization usually is, it's not the first step that you can use.",
    "start": "2719765",
    "end": "2730040"
  },
  {
    "text": "In- in um, in some cases, you do use discretization,",
    "start": "2730040",
    "end": "2736130"
  },
  {
    "text": "but there you need to kind of use some pretty- you need to heavily use",
    "start": "2736130",
    "end": "2742535"
  },
  {
    "text": "the domain knowledge of the problem space to reduce the dimensionality. Okay. So with discretization,",
    "start": "2742535",
    "end": "2750605"
  },
  {
    "start": "2750000",
    "end": "2925000"
  },
  {
    "text": "one problem is this, you know, we need to break it into multiple bins and ah, you know,",
    "start": "2750605",
    "end": "2755975"
  },
  {
    "text": "it does not generalize well, the other problem with discretization is what's called as the curse of dimensionality.",
    "start": "2755975",
    "end": "2761980"
  },
  {
    "text": "[NOISE]",
    "start": "2761980",
    "end": "2771460"
  },
  {
    "text": "Which means, suppose we have- suppose we have a state-space of,",
    "start": "2771460",
    "end": "2777485"
  },
  {
    "text": "you know, let's call it six- six states. So let's say we have things like the,",
    "start": "2777485",
    "end": "2784790"
  },
  {
    "text": "[NOISE] yes supposing it's,",
    "start": "2784790",
    "end": "2792665"
  },
  {
    "text": "you know, it's ah, [NOISE] you're trying to learn- learn a helicopter,",
    "start": "2792665",
    "end": "2798125"
  },
  {
    "text": "then the number of states that you have are like the X position, Y position, Z position and you know,",
    "start": "2798125",
    "end": "2804215"
  },
  {
    "text": "the- the angle at which the helicopter is, is- is pointing to.",
    "start": "2804215",
    "end": "2810680"
  },
  {
    "text": "So it's like how upward or downward the helicopters pointing to or, you know, um.",
    "start": "2810680",
    "end": "2817460"
  },
  {
    "text": "So you have three different angles. The- the angle- the- the angle at which the nose of the helicopter's pointing, the angle at which it is turned left or right.",
    "start": "2817460",
    "end": "2825994"
  },
  {
    "text": "And there's one more angle, which I'm not familiar with flying helicopters, but- but anyway,",
    "start": "2825995",
    "end": "2831470"
  },
  {
    "text": "so you have, you know, different kinds of angles and different velocities.",
    "start": "2831470",
    "end": "2837649"
  },
  {
    "text": "X, Y dot, and Z dot. So what's the velocity along the X-axis?",
    "start": "2837649",
    "end": "2844340"
  },
  {
    "text": "What's the velocity on the Y-axis? What's the velocity in the Z-axis and all the angular velocity. So theta dot.",
    "start": "2844340",
    "end": "2850130"
  },
  {
    "text": "[NOISE] y dot, psi dot, and I think it's psi here.",
    "start": "2850130",
    "end": "2856460"
  },
  {
    "text": "Right. So one, two, three, four, five, six plus six.",
    "start": "2856460",
    "end": "2861500"
  },
  {
    "text": "So you have 12- 12 different coordinates in your state space, and they're all continuous, right?",
    "start": "2861500",
    "end": "2867350"
  },
  {
    "text": "And if you were to discretize them, right. Let's say you were to discretize them into where each thing into say, ten units, right?",
    "start": "2867350",
    "end": "2876185"
  },
  {
    "text": "You would- would essentially get 10 to the 12 number of discretized states, right?",
    "start": "2876185",
    "end": "2883250"
  },
  {
    "text": "So each- each, [NOISE] each coordinate of your state space is continuous and let's say you discretize them into ten- into ten units or ten bins,",
    "start": "2883250",
    "end": "2891590"
  },
  {
    "text": "then the total size of your state space is essentially 10 to the 12, right. That's pretty massive. It- and",
    "start": "2891590",
    "end": "2899463"
  },
  {
    "text": "it is exactly this kind of an explosion in states that is um, that you get due to discretization.",
    "start": "2899464",
    "end": "2906470"
  },
  {
    "text": "That is called the curse of dimensionality. Right. And it is, and that's",
    "start": "2906470",
    "end": "2912589"
  },
  {
    "text": "probably one of the biggest limitations of discretization because [NOISE] the number of states, as you add the number of components in your- in your state space,",
    "start": "2912590",
    "end": "2921245"
  },
  {
    "text": "it just- just- just explodes. Right. And that brings us to what is called as the value function approximation.",
    "start": "2921245",
    "end": "2930380"
  },
  {
    "start": "2925000",
    "end": "3090000"
  },
  {
    "text": "[NOISE]",
    "start": "2930380",
    "end": "2961089"
  },
  {
    "text": "So in value function approximation, we- we now want to- to work in a setting where S is",
    "start": "2961090",
    "end": "2970180"
  },
  {
    "text": "continuous- S is continuous.",
    "start": "2970180",
    "end": "2978505"
  },
  {
    "text": "And we don't discretize, right?",
    "start": "2978505",
    "end": "2989650"
  },
  {
    "text": "And in this case, the most common setting is to have something- to have some kind of a simulator, okay?",
    "start": "2989650",
    "end": "2996850"
  },
  {
    "text": "So you have a simulator.",
    "start": "2996850",
    "end": "2999380"
  },
  {
    "text": "And in this simulator accepts some state a_t and get you S^t plus 1.",
    "start": "3005000",
    "end": "3017040"
  },
  {
    "text": "Okay. So assume you have some kind of a simulator where it takes as input the current state and the action and outputs the next state. Right?",
    "start": "3017040",
    "end": "3025380"
  },
  {
    "text": "And the biggest difference between this and the previous model that we were working with was that in the previous model,",
    "start": "3025380",
    "end": "3035310"
  },
  {
    "text": "we had a finite number of states, right? And we had a multinomial kind of a distribution or the next state for every state and action pair.",
    "start": "3035310",
    "end": "3043080"
  },
  {
    "text": "But over here, the number of states is continuous and we have in place of a multinomial kind of a model -no,",
    "start": "3043080",
    "end": "3052230"
  },
  {
    "text": "I'm referring to the collection Psa as a collection of multinomials in place of",
    "start": "3052230",
    "end": "3058050"
  },
  {
    "text": "a multinomial which worked well in the discrete setting. We instead have, you know, assume we have some kind of a simulator where we feed",
    "start": "3058050",
    "end": "3065430"
  },
  {
    "text": "a real valued input and some action and you get output as some other real valued state.",
    "start": "3065430",
    "end": "3071490"
  },
  {
    "text": "All right? And these simulators can be- are- are generally,",
    "start": "3071490",
    "end": "3077610"
  },
  {
    "text": "you know, things like, you know, a physics simulators are commonly used if you're working in some kind of a game-playing scenario,",
    "start": "3077610",
    "end": "3084450"
  },
  {
    "text": "then the game engine can itself act as the simulator and so on, right? And now what we wanna do is basically try to do",
    "start": "3084450",
    "end": "3094650"
  },
  {
    "start": "3090000",
    "end": "3599000"
  },
  {
    "text": "the equivalent of this in a continuous space, right?",
    "start": "3094650",
    "end": "3100785"
  },
  {
    "text": "So you want to kind of transform this into a continuous setting.",
    "start": "3100785",
    "end": "3105369"
  },
  {
    "text": "This worked well because we were just counting. And it worked well because, you know,",
    "start": "3109700",
    "end": "3115365"
  },
  {
    "text": "we could count er, discrete things. However, now we are in a continuous state and counting doesn't work anymore, right?",
    "start": "3115365",
    "end": "3122730"
  },
  {
    "text": "And here we're going to use- and here we're",
    "start": "3122730",
    "end": "3135960"
  },
  {
    "text": "going to learn the simulator to the extent possible. And what do I mean by that?",
    "start": "3135960",
    "end": "3142410"
  },
  {
    "text": "What I mean by that is we are going to make an assumption that S^t plus 1 is equal",
    "start": "3142410",
    "end": "3154019"
  },
  {
    "text": "to some function of S^t, a^t.",
    "start": "3154020",
    "end": "3162120"
  },
  {
    "text": "All right. And this relation of a common thing, er for example,",
    "start": "3162120",
    "end": "3170325"
  },
  {
    "text": "what we could do is assume f of- assume a linear form for this relation,",
    "start": "3170325",
    "end": "3177075"
  },
  {
    "text": "which means f is linear. So AS^t plus Ba^t,",
    "start": "3177075",
    "end": "3186240"
  },
  {
    "text": "where A is a matrix and B is another matrix.",
    "start": "3186240",
    "end": "3193455"
  },
  {
    "text": "And you can think of supposing S is in, let's say R^d.",
    "start": "3193455",
    "end": "3202770"
  },
  {
    "text": "Now A is therefore in R^d by d. And similarly,",
    "start": "3202770",
    "end": "3212355"
  },
  {
    "text": "if A is in R,",
    "start": "3212355",
    "end": "3217605"
  },
  {
    "text": "let's call it R^p then B is in",
    "start": "3217605",
    "end": "3222900"
  },
  {
    "text": "R^b by p, right? And for those of you who have er,",
    "start": "3222900",
    "end": "3229470"
  },
  {
    "text": "you know, some background in- in electrical engineering, you might recognize this as a linear dynamic system, right?",
    "start": "3229470",
    "end": "3235725"
  },
  {
    "text": "There's- you have some particular state and you're taking some action. And the action and the state,",
    "start": "3235725",
    "end": "3241244"
  },
  {
    "text": "er have this linear relationship to decide the next state, right?",
    "start": "3241245",
    "end": "3247050"
  },
  {
    "text": "And assuming that you're- you're- that your environment has this kind of, um,",
    "start": "3247050",
    "end": "3254670"
  },
  {
    "text": "a linear dynamical relation, we can then do the equivalent thing of this,",
    "start": "3254670",
    "end": "3261420"
  },
  {
    "text": "which is- what should I erase? Probably this- this.",
    "start": "3261420",
    "end": "3266650"
  },
  {
    "text": "Which is um, follow some policy,",
    "start": "3276020",
    "end": "3284160"
  },
  {
    "text": "and run against, er, basically get some experience against this simulator S_0^1, a_0^1, S_1^1, a_1^1, S_2^1",
    "start": "3284160",
    "end": "3299350"
  },
  {
    "text": "and so on. And another trial S_0^2, a_0^2 and so on.",
    "start": "3305570",
    "end": "3320010"
  },
  {
    "text": "So what's happening here is these- these are trials that we are running according to some policy against the simulator, right?",
    "start": "3320010",
    "end": "3328750"
  },
  {
    "text": "And from this we collect, Ss and these S,",
    "start": "3329750",
    "end": "3338190"
  },
  {
    "text": "a, S prime tuples. So S_a got us to S prime.",
    "start": "3338190",
    "end": "3344579"
  },
  {
    "text": "This is one example. And S_a got us to the next S prime, there's another example, and so on.",
    "start": "3344580",
    "end": "3351210"
  },
  {
    "text": "And we can um, collect this data set of S, a,",
    "start": "3351210",
    "end": "3357150"
  },
  {
    "text": "S prime tuples and then define a loss function,",
    "start": "3357150",
    "end": "3364260"
  },
  {
    "text": "J of A, B is equal to sum over i equals 1 to n,",
    "start": "3364260",
    "end": "3372480"
  },
  {
    "text": "assuming we do n trials.",
    "start": "3372480",
    "end": "3377650"
  },
  {
    "text": "And just to make the math simple, assuming they are terminating all of them at S_t, right?",
    "start": "3377660",
    "end": "3385230"
  },
  {
    "text": "After you take t time steps, Let's assume we just terminate it, okay,",
    "start": "3385230",
    "end": "3390190"
  },
  {
    "text": "and t equals 0 to t minus 1.",
    "start": "3390530",
    "end": "3399190"
  },
  {
    "text": "What we want to- what we want to do is minimize the squared error between S_t plus",
    "start": "3399800",
    "end": "3409065"
  },
  {
    "text": "1^i minus AS_t^i plus Ba_t^i.",
    "start": "3409065",
    "end": "3422609"
  },
  {
    "text": "All right. So define a cost function where",
    "start": "3422610",
    "end": "3431805"
  },
  {
    "text": "the S_t plus 1 or the S prime is considered the true or the y label.",
    "start": "3431805",
    "end": "3442079"
  },
  {
    "text": "And this is your hypothesis. Okay. So this S_t plus 1,",
    "start": "3442080",
    "end": "3447720"
  },
  {
    "text": "you think of this as y and this you think of as Theta and x,",
    "start": "3447720",
    "end": "3454215"
  },
  {
    "text": "where Theta is A, B and x is s, a.",
    "start": "3454215",
    "end": "3459130"
  },
  {
    "text": "And now, once you have a loss function like this, you can solve it using gradient descent and recover A and B,",
    "start": "3459740",
    "end": "3468420"
  },
  {
    "text": "where A and B are the parameters of this- this cost function. Next question. [inaudible]",
    "start": "3468420",
    "end": "3484575"
  },
  {
    "text": "So the uh, the question is- is this- can we think of this as a policy because it takes s and a and gives us the next state.",
    "start": "3484575",
    "end": "3491175"
  },
  {
    "text": "So a policy takes as input to the current state, and returns as output the action that we need to take while",
    "start": "3491175",
    "end": "3498150"
  },
  {
    "text": "we're in this current state, right? So this, um, think of this as- as the dynamics of the system, right?",
    "start": "3498150",
    "end": "3505380"
  },
  {
    "text": "If you are in a current state and you take an action according to some policy, what is the new state that we're going to end up with?",
    "start": "3505380",
    "end": "3512025"
  },
  {
    "text": "Right. Whereas a policy will tell us if we are given, uh, the current state s, what should a be? Tha- that's the policy.",
    "start": "3512025",
    "end": "3520530"
  },
  {
    "text": "[inaudible]",
    "start": "3520530",
    "end": "3529500"
  },
  {
    "text": "So the ah, the- the question is, are we not, uh, uh, concerned about policy in- in a continuous state? The answer is yes.",
    "start": "3529500",
    "end": "3536400"
  },
  {
    "text": "Right now, we are still in the state where we are doing something that is equivalent to learning the model,",
    "start": "3536400",
    "end": "3541665"
  },
  {
    "text": "and once we learne the model, we will plug it into some algorithm like this, where we learn the model and estimate value and policy.",
    "start": "3541665",
    "end": "3548880"
  },
  {
    "text": "We are- we are yet to construct the full algorithm, we are only addressing, uh,",
    "start": "3548880",
    "end": "3555085"
  },
  {
    "text": "this part of- of- of the corresponding algorithm of the continuous state. Yes, question.",
    "start": "3555085",
    "end": "3560190"
  },
  {
    "text": "[inaudible]",
    "start": "3560190",
    "end": "3565920"
  },
  {
    "text": "That's correct. So first sum ah, is across trials. The second sum is within a trial. That's correct.",
    "start": "3565920",
    "end": "3571950"
  },
  {
    "text": "And would you explain [inaudible]",
    "start": "3571950",
    "end": "3580560"
  },
  {
    "text": "So the question is, why is this a linear ah, ah, a reasonable assumption? Ah, it may or may not be a reasonable assumption.",
    "start": "3580560",
    "end": "3586560"
  },
  {
    "text": "Ah, this is just an example of how if you model it as a and b as- as a linear dynamical system,",
    "start": "3586560",
    "end": "3592560"
  },
  {
    "text": "then you can have a cost function like this and learn ah, suitable, um, you know,",
    "start": "3592560",
    "end": "3598170"
  },
  {
    "text": "ah, ah, a suitable model. You can- you can absolutely use a more complex mo- model",
    "start": "3598170",
    "end": "3603390"
  },
  {
    "text": "over here and minimize you know, instead of AST plus BAT, you would have a more complex form here and you can minimize it.",
    "start": "3603390",
    "end": "3609300"
  },
  {
    "text": "That's totally reasonable too. [inaudible]. We will- we will - we will,",
    "start": "3609300",
    "end": "3616109"
  },
  {
    "text": "ah, at this point, let's just assume that our model can be represented as a- ah, ah linear system.",
    "start": "3616110",
    "end": "3622125"
  },
  {
    "text": "Ah, most, ah, a lot of- lot of, ah, a lot of examples can actually do pretty well in this.",
    "start": "3622125",
    "end": "3627855"
  },
  {
    "text": "And we'll see you know what some changes can be done to this to actually, you know, what can, a lot more cases shortly.",
    "start": "3627855",
    "end": "3634289"
  },
  {
    "text": "Right. Yes, question. Ah, so in this case, in the next step [inaudible]",
    "start": "3634290",
    "end": "3648690"
  },
  {
    "text": "So the- the- so the question is, in a- in a continuous setting, ah, should the action also be real valued for the policy?",
    "start": "3648690",
    "end": "3655349"
  },
  {
    "text": "Ah, it need not be. So you can have continuous states but still discrete actions.",
    "start": "3655350",
    "end": "3660570"
  },
  {
    "text": "For example, if you're trying to balance a cart pull, um, your state can be continuous, ah, where you know the states have angles and velocities.",
    "start": "3660570",
    "end": "3667740"
  },
  {
    "text": "But your action can still be discrete. It can be move left or move right? So you- you can- you can, um, um, yeah,",
    "start": "3667740",
    "end": "3675000"
  },
  {
    "text": "having continuous state does not mean your action also have to be ah, ah continuous. In- in cases when the action is discreet and status",
    "start": "3675000",
    "end": "3682620"
  },
  {
    "text": "continuous your policy will take continuous input and output discrete?",
    "start": "3682620",
    "end": "3687400"
  },
  {
    "text": "It will be a functional [inaudible] Yes. [inaudible] Yes. Now policy will- will, ah- will be some kind of ah,",
    "start": "3700490",
    "end": "3707190"
  },
  {
    "text": "ah, function. Yes. So policy. it ah, so- so one thing to notice, should a policy always be a function?",
    "start": "3707190",
    "end": "3715845"
  },
  {
    "text": "So if you see this relation over here, so what you do in um,",
    "start": "3715845",
    "end": "3723060"
  },
  {
    "text": "so the policy, if you think of it as something in this form, right?",
    "start": "3723060",
    "end": "3728295"
  },
  {
    "text": "The- the induced policy according to sum v, and it need not actually have any particular functional form.",
    "start": "3728295",
    "end": "3733920"
  },
  {
    "text": "Right? It can just evaluate the v function which takes a continuous input at,",
    "start": "3733920",
    "end": "3739500"
  },
  {
    "text": "um, um at- at- at different possible, ah, ah, according to different possible actions, right?",
    "start": "3739500",
    "end": "3745470"
  },
  {
    "text": "And you can get the- the next state according to the dynamical system that we- that we constructed, right?",
    "start": "3745470",
    "end": "3751710"
  },
  {
    "text": "And evaluate the value function at the new states. And you can just take the argmax, right?",
    "start": "3751710",
    "end": "3760080"
  },
  {
    "text": "So- so ah, the policy can be kind of implicit or induced according to some value function and some model.",
    "start": "3760080",
    "end": "3767325"
  },
  {
    "text": "It need not always be an explicit functional form.",
    "start": "3767325",
    "end": "3770829"
  },
  {
    "text": "All right. So then if we minimize this cost,",
    "start": "3774140",
    "end": "3779519"
  },
  {
    "text": "so A hat, B hat equals argmin of A, B.",
    "start": "3779520",
    "end": "3788625"
  },
  {
    "text": "Then A hat and B hat will be our learned model. By again, by model I mean the,",
    "start": "3788625",
    "end": "3795420"
  },
  {
    "text": "you know, what we have learned about the way the world works. This A hat and B hat is,",
    "start": "3795420",
    "end": "3800520"
  },
  {
    "text": "you can think of it as the equivalent of p-hat SA. Right? So p-hat SA gave us a way to relate state and action to the next state.",
    "start": "3800520",
    "end": "3809850"
  },
  {
    "text": "And here A hat B hat gives us a way to relate state and action to the next state. Right. Yes, question.",
    "start": "3809850",
    "end": "3822390"
  },
  {
    "text": "[inaudible]",
    "start": "3822390",
    "end": "3832680"
  },
  {
    "text": "We'll come to stochasticity next, yeah. Um, right.",
    "start": "3832680",
    "end": "3852840"
  },
  {
    "text": "So the model that we learned with um, the linear model that we learned is- was deterministic in some sense that",
    "start": "3852840",
    "end": "3864660"
  },
  {
    "text": "ST plus 1 is equal to A hat ST plus B hat AT, right?",
    "start": "3864660",
    "end": "3878670"
  },
  {
    "text": "So once we minimize A hat and B hat acc- according to this, um, according to this- this ah,",
    "start": "3878670",
    "end": "3886110"
  },
  {
    "text": "ah, cost function, we will recover some A hat and B hat. And the- the prediction that we're going to make for the next state is according to this.",
    "start": "3886110",
    "end": "3894570"
  },
  {
    "text": "Right. But in general- so- so if- if we just limit ourselves to this, we would call this a deterministic model.",
    "start": "3894570",
    "end": "3900970"
  },
  {
    "text": "Right. But in reality it's always a good idea to have- to um,",
    "start": "3905270",
    "end": "3910920"
  },
  {
    "text": "to have some kind of stochasticity in- in your model, which means we want to estimate,",
    "start": "3910920",
    "end": "3916440"
  },
  {
    "text": "we want to predict ST plus 1 will be equal to A hat ST plus",
    "start": "3916440",
    "end": "3925680"
  },
  {
    "text": "B hat AT plus some epsilon.",
    "start": "3925680",
    "end": "3934030"
  },
  {
    "text": "Right. Where epsilon comes from, you know, a common choice is a normal distribution that has mean 0 and some covariance Sigma.",
    "start": "3934640",
    "end": "3944925"
  },
  {
    "text": "And it's also possible to learn this Sigma from data.",
    "start": "3944925",
    "end": "3950025"
  },
  {
    "text": "Basically, ah, the way to think of this ah, covariance is to um, you can- you can estimate this covariance as one over n times one over p minus",
    "start": "3950025",
    "end": "3962310"
  },
  {
    "text": "1 ST- IT plus 1 minus AS_ti plus BA_ti, BA_it",
    "start": "3962310",
    "end": "3987520"
  },
  {
    "text": "transpose, right? So this is just the, if- if you know, you- you- you have done this with GDA,",
    "start": "4003040",
    "end": "4008750"
  },
  {
    "text": "you'll, you know, this is something you should be familiar with, which is the maximum likelihood estimate of a covariance will be just the ah,",
    "start": "4008750",
    "end": "4015710"
  },
  {
    "text": "residual, residual transpose ah, across all the examples. Yes, question.",
    "start": "4015710",
    "end": "4021590"
  },
  {
    "text": "[inaudible] Because this is, ah, we're summing across all N and T, right?",
    "start": "4021590",
    "end": "4030450"
  },
  {
    "text": "Right. So it is, um, um, in general, it is considered a good practice to build stochastic models.",
    "start": "4030670",
    "end": "4039720"
  },
  {
    "text": "Where the prediction that we make about the next state is- follows this format.",
    "start": "4041410",
    "end": "4048530"
  },
  {
    "text": "Where for each prediction, we sample an epsilon according to this distribution.",
    "start": "4048530",
    "end": "4055839"
  },
  {
    "text": "And this distribution is learned where the covariance is estimated this way. So you can think of Sigma hat as this. Yes, question.",
    "start": "4055840",
    "end": "4065920"
  },
  {
    "text": "Ah, yeah, so let's say [inaudible] like when do you know when to stop ah,",
    "start": "4065920",
    "end": "4073424"
  },
  {
    "text": "that step, like how much- like when do you know your estimates are [inaudible]",
    "start": "4073425",
    "end": "4089330"
  },
  {
    "text": "So the question is for the p-set, we- you know, um, you will not be doing any of this for the p-set.",
    "start": "4089330",
    "end": "4096109"
  },
  {
    "text": "Right. In general, the- the question is, um, you know, when, when do we know we have collected enough evidence ah, or- or experience?",
    "start": "4096110",
    "end": "4104029"
  },
  {
    "text": "When this- in case of the discrete ah, ah, setting, you can you know stop when the",
    "start": "4104030",
    "end": "4111424"
  },
  {
    "text": "v stars that you get at the end of each iteration starts converging. [inaudible]",
    "start": "4111425",
    "end": "4118609"
  },
  {
    "text": "So the question is, you know, you can- you can phrase the same question as when do you know you have ah, your estimate of p-hat SA is sufficient, you know,",
    "start": "4118610",
    "end": "4125259"
  },
  {
    "text": "which is the same as when- when is the estimate of A and B you know sufficiently correct? And in this case, ah, you can consider it as sufficient when this overall,",
    "start": "4125260",
    "end": "4134214"
  },
  {
    "text": "you know, this outer loop converges. [inaudible]",
    "start": "4134215",
    "end": "4151109"
  },
  {
    "text": "Yeah. So, uh, again, so for the p set that we have instructions given in the p set itself, you can follow those instructions.",
    "start": "4151110",
    "end": "4156824"
  },
  {
    "text": "And- and, uh, the idea in general is that you're- you're- you're, uh, your value function stops changing overtime,",
    "start": "4156824",
    "end": "4162569"
  },
  {
    "text": "right? That's- that's a good sign. [NOISE] Um, right? So, um,",
    "start": "4162570",
    "end": "4168165"
  },
  {
    "text": "instead of constructing a deterministic model, you can- it is always better to construct a stochastic model where the noise that you want to",
    "start": "4168165",
    "end": "4175650"
  },
  {
    "text": "induce into the stochastic system can also be estimated using- using MLE, right?",
    "start": "4175650",
    "end": "4180765"
  },
  {
    "text": "A and B were estimated using MLE. And then we can also estimate, um, uh, the noise that we want to induce,",
    "start": "4180765",
    "end": "4186375"
  },
  {
    "text": "which is kind of consistent with the- the, uh- uh, with the errors that we are making can also be learned this way.",
    "start": "4186375",
    "end": "4194325"
  },
  {
    "text": "And, ah, when you're making the next prediction, you know, just sample some noise according to that covariance and make your,",
    "start": "4194325",
    "end": "4200505"
  },
  {
    "text": "you know, a noisy prediction every time. So this is how we would learn a model,",
    "start": "4200505",
    "end": "4208094"
  },
  {
    "text": "so learn a model in continuous setting.",
    "start": "4208095",
    "end": "4218670"
  },
  {
    "text": "[NOISE] Right? Now, let's,",
    "start": "4218670",
    "end": "4226380"
  },
  {
    "text": "now- now let's see, uh, after having addressed, uh, modeling the- the,",
    "start": "4226380",
    "end": "4233235"
  },
  {
    "text": "uh, modeling the environment itself as continuous. Now what do we do about the value function itself?",
    "start": "4233235",
    "end": "4238335"
  },
  {
    "text": "Because if you remember- if you remember, for example,",
    "start": "4238335",
    "end": "4244710"
  },
  {
    "text": "in this example, s comes up in- in the model, in the- in the transition,",
    "start": "4244710",
    "end": "4250815"
  },
  {
    "text": "and s comes as an input to V, right? In the discrete setting, V was just an array where we had, uh,",
    "start": "4250815",
    "end": "4258045"
  },
  {
    "text": "as the length of the array equal to the number of states that we had,",
    "start": "4258045",
    "end": "4263145"
  },
  {
    "text": "and the transition function or the- or the environment was also a matrix where the number of states was continuous.",
    "start": "4263145",
    "end": "4270990"
  },
  {
    "text": "So it was just a matrix. But now when we move to continuous states, the transitions or the- or the environment or the model,",
    "start": "4270990",
    "end": "4278895"
  },
  {
    "text": "you know, can be made continuous. Next, we need to make the value function also continuous.",
    "start": "4278895",
    "end": "4285405"
  },
  {
    "text": "Right? And a common approach for, uh, for doing that is to first, uh,",
    "start": "4285405",
    "end": "4293625"
  },
  {
    "text": "think of our- think of our, ah, value function as, you know,",
    "start": "4293625",
    "end": "4301245"
  },
  {
    "text": "V of s is equal to R of s plus Gamma times-",
    "start": "4301245",
    "end": "4311140"
  },
  {
    "text": "Gamma times max a expectation of s",
    "start": "4315890",
    "end": "4327180"
  },
  {
    "text": "prime from Psa V s prime.",
    "start": "4327180",
    "end": "4339015"
  },
  {
    "text": "Now Psa, you can think of Psa from, you know, this, uh,",
    "start": "4339015",
    "end": "4346095"
  },
  {
    "text": "model as Psa is equal to some normal distribution where the mean",
    "start": "4346095",
    "end": "4353700"
  },
  {
    "text": "is As plus Ba and covariance is the,",
    "start": "4353700",
    "end": "4363825"
  },
  {
    "text": "uh- uh, covariance that we estimated here. Right? So you can think of this",
    "start": "4363825",
    "end": "4371580"
  },
  {
    "text": "as Psa where you sample the next state according to, um, this. And- and so this can now,",
    "start": "4371580",
    "end": "4380055"
  },
  {
    "text": "you can think of it as R of s plus Gamma times max",
    "start": "4380055",
    "end": "4387285"
  },
  {
    "text": "a integral p of- [NOISE] Psa",
    "start": "4387285",
    "end": "4403740"
  },
  {
    "text": "of s prime times V of s prime ds prime.",
    "start": "4403740",
    "end": "4412965"
  },
  {
    "text": "Where this is the Gaussian pdf.",
    "start": "4412965",
    "end": "4417489"
  },
  {
    "text": "Right? So you can think of the value function as this analog of,",
    "start": "4420380",
    "end": "4425460"
  },
  {
    "text": "uh, of- of the, uh, discrete version where the expectation is now",
    "start": "4425460",
    "end": "4431550"
  },
  {
    "text": "over a continuous random variable instead of a discrete random variable, right?",
    "start": "4431550",
    "end": "4436905"
  },
  {
    "text": "And the input to the value function is also a continuous, uh, continuous value, right?",
    "start": "4436905",
    "end": "4442695"
  },
  {
    "text": "And this identity is- is,",
    "start": "4442695",
    "end": "4447704"
  },
  {
    "text": "you know, is again, um, is also called the Bellman equation because it's- Bellman equation.",
    "start": "4447705",
    "end": "4457770"
  },
  {
    "text": "Uh, it's just the, uh, continuous version of the discrete setting. However, um, what we end up doing in practice is to use what is called as, uh- uh,",
    "start": "4457770",
    "end": "4469770"
  },
  {
    "text": "fitted value function or the value function approximation- value function approximation,",
    "start": "4469770",
    "end": "4481320"
  },
  {
    "text": "where we say V of s is equal to, you know, for example, ah, in this case,",
    "start": "4481320",
    "end": "4486930"
  },
  {
    "text": "let us assume of, uh- uh, a simple Theta transpose Phi of s, right?",
    "start": "4486930",
    "end": "4495255"
  },
  {
    "text": "Just like linear regression with some kind of a feature map, you construct a feature map of your state space using, you know,",
    "start": "4495255",
    "end": "4501360"
  },
  {
    "text": "whatever domain knowledge you have and assume you have some kind of, uh- uh, a linear relationship for, ah,",
    "start": "4501360",
    "end": "4508530"
  },
  {
    "text": "for the- for the value of a function based on the state that it is- that it's in. Right? And with this assumption,",
    "start": "4508530",
    "end": "4517420"
  },
  {
    "text": "what we- what we wanna do is try to come up with a value function,",
    "start": "4517910",
    "end": "4525045"
  },
  {
    "text": "a V function that has this form, but satisfies the Bellman equation, right?",
    "start": "4525045",
    "end": "4530280"
  },
  {
    "text": "With- which was basically what we did with- with, um, the value iteration in the discrete case. And we're gonna now do the same thing with the,",
    "start": "4530280",
    "end": "4537810"
  },
  {
    "text": "ah, continuous, um, uh, in the continuous case. right? So this gives us an algorithm",
    "start": "4537810",
    "end": "4543029"
  },
  {
    "text": "[NOISE] with this two assumptions and it looks like this. [NOISE]",
    "start": "4543029",
    "end": "4587370"
  },
  {
    "text": "So now this is an algorithm where we are using,",
    "start": "4587370",
    "end": "4593625"
  },
  {
    "text": "um, continuous state with,",
    "start": "4593625",
    "end": "4603525"
  },
  {
    "text": "you know, sample model where the model in this, you know, you can assume it to be a- a linear dynamical system with,",
    "start": "4603525",
    "end": "4611250"
  },
  {
    "text": "you know, where you have the A and B matrices. And you resu- you're doing value function approximation.",
    "start": "4611250",
    "end": "4622545"
  },
  {
    "text": "Right? So here this was s^t plus 1",
    "start": "4622545",
    "end": "4627795"
  },
  {
    "text": "equals As^t plus Ba^t.",
    "start": "4627795",
    "end": "4635735"
  },
  {
    "text": "The value function approximation where we have V of s is equal to Theta transpose",
    "start": "4635735",
    "end": "4642885"
  },
  {
    "text": "Phi of s. So this algorithm is basically an adaptation of this algorithm.",
    "start": "4642885",
    "end": "4651719"
  },
  {
    "text": "But to the setting of- of continuous states and value function approximation, right?",
    "start": "4651720",
    "end": "4658560"
  },
  {
    "text": "Over here, both the states and the value functions, um,",
    "start": "4658560",
    "end": "4664020"
  },
  {
    "text": "you can- you can think of them as non-parametric, which means we did not have a family of, um, uh,",
    "start": "4664020",
    "end": "4669825"
  },
  {
    "text": "some kind of a family of functions for V or Psa. However, these are parametric in the sense that",
    "start": "4669825",
    "end": "4677864"
  },
  {
    "text": "the transition or the model has A and B as the parameters. Or if you're having Epsilon,",
    "start": "4677865",
    "end": "4683520"
  },
  {
    "text": "it has Sigma as also a parameter, and the value function has Theta as the parameter.",
    "start": "4683520",
    "end": "4688665"
  },
  {
    "text": "Right? So it's basically an adaptation of that algorithm into the setting.",
    "start": "4688665",
    "end": "4693690"
  },
  {
    "text": "So Step 1, randomly sample n states.",
    "start": "4693690",
    "end": "4707474"
  },
  {
    "text": "Call them S^1, S^2,",
    "start": "4707474",
    "end": "4715920"
  },
  {
    "text": "S^n and S. Two,",
    "start": "4715920",
    "end": "4721915"
  },
  {
    "text": "initialize Theta to be 0.",
    "start": "4721915",
    "end": "4728265"
  },
  {
    "text": "And Step 3, repeat until convergence.",
    "start": "4728265",
    "end": "4735180"
  },
  {
    "text": "Now, there's an inner loop here for i equals 1 till n. The inner loop.",
    "start": "4735180",
    "end": "4745930"
  },
  {
    "text": "Is there a question? Sure, for i equals 1 through n,",
    "start": "4747620",
    "end": "4754330"
  },
  {
    "text": "also write this an inner loop within this as well. For each A",
    "start": "4756500",
    "end": "4766570"
  },
  {
    "text": "in the action space, sample, S_1 prime till",
    "start": "4768800",
    "end": "4780795"
  },
  {
    "text": "S_k prime from Ps^i a.",
    "start": "4780795",
    "end": "4790650"
  },
  {
    "text": "Right? So in this case it could be, for example, um, the normal distribution with As^i plus Ba covariance Sigma hat, for example.",
    "start": "4790650",
    "end": "4806349"
  },
  {
    "text": "And then set qa",
    "start": "4809540",
    "end": "4815740"
  },
  {
    "text": "is equal to average 1 over k, right?",
    "start": "4816080",
    "end": "4821355"
  },
  {
    "text": "J equals 1 through k R of s^i",
    "start": "4821355",
    "end": "4829400"
  },
  {
    "text": "plus Gamma of V of sj prime.",
    "start": "4829400",
    "end": "4839140"
  },
  {
    "text": "Again, where here V of s is equal to Theta transpose phi of s. Right?",
    "start": "4839240",
    "end": "4848980"
  },
  {
    "text": "So we do this for each action, and then continue it over here,",
    "start": "4852520",
    "end": "4861210"
  },
  {
    "text": "set Y I equals-",
    "start": "4863320",
    "end": "4867599"
  },
  {
    "text": "equals max A,",
    "start": "4872890",
    "end": "4878630"
  },
  {
    "text": "Q of A and that ends this.",
    "start": "4878630",
    "end": "4887855"
  },
  {
    "text": "So let's call this as a start A, start B and start C. So this is finish C,",
    "start": "4887855",
    "end": "4896780"
  },
  {
    "text": "this is finish B [NOISE] and",
    "start": "4896780",
    "end": "4904820"
  },
  {
    "text": "then site Theta hat",
    "start": "4904820",
    "end": "4911929"
  },
  {
    "text": "equals arg min Theta I equals 1 to n,",
    "start": "4911930",
    "end": "4923370"
  },
  {
    "text": "Theta transpose Phi of S",
    "start": "4923650",
    "end": "4929570"
  },
  {
    "text": "I minus Y I square.",
    "start": "4929570",
    "end": "4936545"
  },
  {
    "text": "This is finish, A, right? So start A, ends with finish A,",
    "start": "4936545",
    "end": "4942605"
  },
  {
    "text": "and start B ends finish B and start C ends with finished C. Right?",
    "start": "4942605",
    "end": "4950390"
  },
  {
    "text": "So what's happening here? First, we start by initializing the Theta of our value function,",
    "start": "4950390",
    "end": "4959270"
  },
  {
    "text": "ah, class to be some, ah, to- to some value in our Theta equals 0,",
    "start": "4959270",
    "end": "4964460"
  },
  {
    "text": "and then we repeat this iteration over and over, where first,",
    "start": "4964460",
    "end": "4970025"
  },
  {
    "text": "for each example, our goal is for each, um, for each [NOISE] I,",
    "start": "4970025",
    "end": "4981770"
  },
  {
    "text": "N- yeah, so for, ah,",
    "start": "4981770",
    "end": "4988200"
  },
  {
    "text": "for each of the states that we have sampled randomly, we want to,",
    "start": "4989190",
    "end": "4995110"
  },
  {
    "text": "ah, we want to come up with an estimate Y I for each S I, right?",
    "start": "4995110",
    "end": "5001659"
  },
  {
    "text": "That's the goal here of- of the outer loop. So for each of the ah, states that we've sampled,",
    "start": "5001660",
    "end": "5007585"
  },
  {
    "text": "we want to come up with ah, Y I, which is our- think of Y I as the ah,",
    "start": "5007585",
    "end": "5017080"
  },
  {
    "text": "label for V of S I.",
    "start": "5017080",
    "end": "5025540"
  },
  {
    "text": "Right? So we want to construct ah, an estimate of V of S I for each I,",
    "start": "5025540",
    "end": "5033085"
  },
  {
    "text": "and that's what ah, the ah, the start B to end B achieves.",
    "start": "5033085",
    "end": "5038455"
  },
  {
    "text": "And the way we go about doing that is for at each- ah, for each ah,",
    "start": "5038455",
    "end": "5045450"
  },
  {
    "text": "S I take all the possible actions starting from S I,",
    "start": "5045450",
    "end": "5052105"
  },
  {
    "text": "and for each action, we sample, K, next states.",
    "start": "5052105",
    "end": "5057114"
  },
  {
    "text": "So for each S I and action, we get K, number, of next states sampled according to the transition.",
    "start": "5057115",
    "end": "5065034"
  },
  {
    "text": "Right? And this transition we are going to use the learnt model, it could be a linear dynamical system, it could be,",
    "start": "5065035",
    "end": "5071035"
  },
  {
    "text": "you know, anything more complex. And then we're going to define this quantity Q of A,",
    "start": "5071035",
    "end": "5078864"
  },
  {
    "text": "as the expected value of taking that action from that state, right?",
    "start": "5078865",
    "end": "5087790"
  },
  {
    "text": "So Q of A is the average of R over S I plus Gamma of V of the next state and average across all the J's.",
    "start": "5087790",
    "end": "5098300"
  },
  {
    "text": "Right, does make sense? So we're going to take- you're going to start with some- some,",
    "start": "5099090",
    "end": "5104679"
  },
  {
    "text": "random state S, and from this S, we're going to take all the possible actions a, a1,",
    "start": "5104680",
    "end": "5113364"
  },
  {
    "text": "a2, A- in this case, capital A.",
    "start": "5113364",
    "end": "5119349"
  },
  {
    "text": "We are going to take all the possible actions from each state, and for each action that we take,",
    "start": "5119350",
    "end": "5125215"
  },
  {
    "text": "we are going to get K, number, of S prime, so S prime 1, until S prime K for each action, right?",
    "start": "5125215",
    "end": "5135625"
  },
  {
    "text": "And similarly, you get S prime 1 until S prime K for each action, for each state.",
    "start": "5135625",
    "end": "5143185"
  },
  {
    "text": "Right? And then from each of those final states,",
    "start": "5143185",
    "end": "5148605"
  },
  {
    "text": "you're going to evaluate the value according to the current estimate of the V function, right?",
    "start": "5148605",
    "end": "5155469"
  },
  {
    "text": "So the current esti- estimate of the V function uses the Theta value we have so far, right?",
    "start": "5155470",
    "end": "5160960"
  },
  {
    "text": "And the first iteration is going to be 0, after that, it's gonna be something else, right? According to the current estimate of the- of the V function,",
    "start": "5160960",
    "end": "5168415"
  },
  {
    "text": "we're going to estimate the- the ah, we're going to use the evaluation of these states using the current estimate of the value function,",
    "start": "5168415",
    "end": "5177220"
  },
  {
    "text": "and we're gonna es- construct this Q value as the- as the expected value of taking this action.",
    "start": "5177220",
    "end": "5192295"
  },
  {
    "text": "Yes, function- yes question. [inaudible]. Yes, Phi- so Phi of S is- is",
    "start": "5192295",
    "end": "5200710"
  },
  {
    "text": "just a feature map and you define Phi of S according to your problem setting, right? So in- in homework one,",
    "start": "5200710",
    "end": "5207040"
  },
  {
    "text": "we define feature maps as sinusoidal squared, polynomials, etc. You define, sum such feature map for your state space.",
    "start": "5207040",
    "end": "5213310"
  },
  {
    "text": "Question? Yes, question? According to that answer, can you explain that design to us probably,",
    "start": "5213310",
    "end": "5220730"
  },
  {
    "text": "like why do we say, we have [inaudible] I'll come to that ah, for- for now, assume V has this linear relationship, right?",
    "start": "5220730",
    "end": "5228175"
  },
  {
    "text": "So what we do is, for each S, we'll take all the possible actions, and from each possible action, you know,",
    "start": "5228175",
    "end": "5234400"
  },
  {
    "text": "sample new states and for each of those new states, calculate the value of the- each new state",
    "start": "5234400",
    "end": "5242665"
  },
  {
    "text": "according to the value function approximation that we have, right?",
    "start": "5242665",
    "end": "5248140"
  },
  {
    "text": "And then take the average and this ah, Q is now an approximation for, right?",
    "start": "5248140",
    "end": "5258070"
  },
  {
    "text": "So Q is, therefore, an approximation, Q of um, Q of A, is,",
    "start": "5258070",
    "end": "5267940"
  },
  {
    "text": "therefore, appro- approximately R of S I,",
    "start": "5267940",
    "end": "5273969"
  },
  {
    "text": "plus Gamma times expectation of S prime NQ, ah,",
    "start": "5273970",
    "end": "5281305"
  },
  {
    "text": "sorry, S prime of Psa, of V of S prime.",
    "start": "5281305",
    "end": "5289555"
  },
  {
    "text": "Right? So the Q value that we constructed is an exp- is-",
    "start": "5289555",
    "end": "5294640"
  },
  {
    "text": "is approximately this and the reason is because- so R is just a constant. So you can actually take out R from this averaging, right?",
    "start": "5294640",
    "end": "5302710"
  },
  {
    "text": "And what remains here, you know this- what we're seeing here is, therefore, a Monte Carlo estimate of this expectation, right?",
    "start": "5302710",
    "end": "5311094"
  },
  {
    "text": "It's a Monte Carlo expectation because we are approximating the transition with K items,",
    "start": "5311095",
    "end": "5319225"
  },
  {
    "text": "and it is also an approximation of V because we are only using the current estimate of- ah,",
    "start": "5319225",
    "end": "5325120"
  },
  {
    "text": "of the V- of the value function with this functional form. Yes.",
    "start": "5325120",
    "end": "5332350"
  },
  {
    "text": "So Q of A is the estimated, ah- it is the expected result will get [inaudible]",
    "start": "5332350",
    "end": "5337870"
  },
  {
    "text": "Expected long-term value for taking from- from this ah, ah, state for taking action A.",
    "start": "5337870",
    "end": "5343975"
  },
  {
    "text": "So shouldn't Q of S, A? Yeah, so you can think of it as Q of S, A. That's fine too.",
    "start": "5343975",
    "end": "5349795"
  },
  {
    "text": "So yeah Q of S prime. Right? That's fine. So right? So this Q is an approximation",
    "start": "5349795",
    "end": "5357625"
  },
  {
    "text": "of the- of the right-hand side of the- of the um,",
    "start": "5357625",
    "end": "5367840"
  },
  {
    "text": "ah of the right-hand side of the Bellman equation, right? So now we define Y I as the max over Q I.",
    "start": "5367840",
    "end": "5377155"
  },
  {
    "text": "So Y I now max A of Q",
    "start": "5377155",
    "end": "5384130"
  },
  {
    "text": "I of- of Q A and Q A is basically um,",
    "start": "5384130",
    "end": "5391739"
  },
  {
    "text": "R of S I plus max A of",
    "start": "5391740",
    "end": "5398830"
  },
  {
    "text": "Gamma times- Gamma times the expectation of",
    "start": "5398830",
    "end": "5406120"
  },
  {
    "text": "S prime from the Psa of V, S I.",
    "start": "5406120",
    "end": "5414100"
  },
  {
    "text": "Right? So Y I, which is max over QA, is max over, you know,",
    "start": "5414100",
    "end": "5419140"
  },
  {
    "text": "max with respect to A of this thing and therefore it is, you know, when we apply the max, R comes out of the max because there's no A term in it,",
    "start": "5419140",
    "end": "5427380"
  },
  {
    "text": "and its max overr A of this term. Right? And this now looks like the Bellman update operator, right?",
    "start": "5427380",
    "end": "5436300"
  },
  {
    "text": "Or the Bellman backup operator.",
    "start": "5436320",
    "end": "5439639"
  },
  {
    "text": "So according to the Bellman uh, backup operator, we should have want to set V of S to be equal to this, right?",
    "start": "5442410",
    "end": "5454179"
  },
  {
    "text": "But our V takes this parametric form, so we just cannot set V of S to be some value, right?",
    "start": "5454180",
    "end": "5462540"
  },
  {
    "text": "Because V is not an array and we're not setting the Sth element to be some value. What we instead want to do because we are in this value function setting,",
    "start": "5462540",
    "end": "5470860"
  },
  {
    "text": "is to minimize the distance between Y I and V of S I. And that's exactly what we're gonna do.",
    "start": "5470860",
    "end": "5476365"
  },
  {
    "text": "We're gonna minimize the distance between Y- Y I and V of S I. Yes, question.",
    "start": "5476365",
    "end": "5483920"
  },
  {
    "text": "Why- why is it V is not a vector of [inaudible] ? V is not a vector because S is continuous,",
    "start": "5484830",
    "end": "5490675"
  },
  {
    "text": "we are in a continuous setting and we are, you know, we are- we are approximating this- this continuous function with this linear form.",
    "start": "5490675",
    "end": "5498025"
  },
  {
    "text": "So if S is continuous what do you mean by V of S? V of S is a feature map for, you know, sum S.",
    "start": "5498025",
    "end": "5505480"
  },
  {
    "text": "For S okay. So, um, the way to- to- to think about",
    "start": "5505480",
    "end": "5511179"
  },
  {
    "text": "this is to see the similarity with the Bellman backup operator, right? In case of the bellman backup operator,",
    "start": "5511180",
    "end": "5516745"
  },
  {
    "text": "you would have V of S^i to be equal to R of S^i plus max of a over this expectation of,",
    "start": "5516745",
    "end": "5523015"
  },
  {
    "text": "um, V of s prime. Instead, because we are in thi- in this, you know, um,",
    "start": "5523015",
    "end": "5529225"
  },
  {
    "text": "fitted value- fitted function setting, we are going to approximate this expectation with the Monte Carlo expectation.",
    "start": "5529225",
    "end": "5536785"
  },
  {
    "text": "That is, you know, this is the Monte Carlo expectation.",
    "start": "5536785",
    "end": "5541100"
  },
  {
    "text": "This- this expectation is going to come in here. R of S^i is just R of S^i and this will evaluate to some y of S^i,",
    "start": "5542010",
    "end": "5551170"
  },
  {
    "text": "which in the discrete setting we would have just set it as the- as the Vth element or Sth element of the VR^a but instead we are in,",
    "start": "5551170",
    "end": "5560305"
  },
  {
    "text": "you know uh, uh, uh, uh fitted function setting, which means we want V of S to evaluate to y or as close to y as possible, right?",
    "start": "5560305",
    "end": "5569085"
  },
  {
    "text": "Which is why we minimize the squared error between Phi of- Phi- so think of this as V of S^i, right?",
    "start": "5569085",
    "end": "5577364"
  },
  {
    "text": "And this is the RHS of",
    "start": "5577365",
    "end": "5583360"
  },
  {
    "text": "the Bellman backup operator, all right?",
    "start": "5583360",
    "end": "5593875"
  },
  {
    "text": "So what's happening here, um, okay, is that first of all, um,",
    "start": "5593875",
    "end": "5601660"
  },
  {
    "text": "let's see the sum, um, um, a few things, you know, the way that's similar to this.",
    "start": "5601660",
    "end": "5606789"
  },
  {
    "text": "So first of all, here we are- in- in- in the discrete setting we were learning both the model and performing value iteration",
    "start": "5606790",
    "end": "5616945"
  },
  {
    "text": "kind of intertwined and we are trying to do something similar here",
    "start": "5616945",
    "end": "5622275"
  },
  {
    "text": "in the sense we are first- in this setting,",
    "start": "5622275",
    "end": "5628239"
  },
  {
    "text": "we are- we are- we are learning the A's and B's, the dynamics, which is equal to this,",
    "start": "5628240",
    "end": "5634025"
  },
  {
    "text": "first, we're not doing it, uh, um, intertwine, but we are first learning the dynamics and using those dynamics,",
    "start": "5634025",
    "end": "5642345"
  },
  {
    "text": "we are now performing value iteration using the Bellman backup operator,",
    "start": "5642345",
    "end": "5648675"
  },
  {
    "text": "but we're performing it in such a way that the- the- the, uh, Bellman op- backup operator is working with these fitted value functions, right?",
    "start": "5648675",
    "end": "5657579"
  },
  {
    "text": "Instead of the, uh, two value functions. And another way to kind of un- understand this better is,",
    "start": "5657580",
    "end": "5666230"
  },
  {
    "text": "okay, if you remember value iteration. So S1 through to Sd right?",
    "start": "5678630",
    "end": "5688975"
  },
  {
    "text": "In the- in the, uh, fitted- in the- in the discrete setting, we had, now supposing this was V-star, right?",
    "start": "5688975",
    "end": "5697105"
  },
  {
    "text": "we start with some, you know, random initialization V-node and apply the bellman backup operator to",
    "start": "5697105",
    "end": "5703614"
  },
  {
    "text": "get V1 and we would keep getting closer to- closer and closer, right?",
    "start": "5703615",
    "end": "5710155"
  },
  {
    "text": "Where each hop was one application of the Bellman backup operator and the Bellman backup operator",
    "start": "5710155",
    "end": "5717190"
  },
  {
    "text": "was a contraction mapping so no matter where it started it would converge at V-star. But now in this setting,",
    "start": "5717190",
    "end": "5724450"
  },
  {
    "text": "this- the space of value functions is somewhat limited. We are limiting ourselves to only those that can be",
    "start": "5724450",
    "end": "5731200"
  },
  {
    "text": "expressed as a linear combination of the feature maps of S, which means this whole algorithm is kind of limited to some set of, you know,",
    "start": "5731200",
    "end": "5744670"
  },
  {
    "text": "think of this as set of all V,",
    "start": "5744670",
    "end": "5751585"
  },
  {
    "text": "which can be, you know, such that V of S equals theta transpose V of S. Right?",
    "start": "5751585",
    "end": "5762280"
  },
  {
    "text": "This family of functions cannot represent all the possible functions in this functional space, right?",
    "start": "5762280",
    "end": "5768730"
  },
  {
    "text": "So we're kind of limited in that sense. And when we- if we start with some value function over here,",
    "start": "5768730",
    "end": "5776545"
  },
  {
    "text": "and we apply one iteration of this fitted value,",
    "start": "5776545",
    "end": "5782290"
  },
  {
    "text": "uh, algorithm- fitted value, uh, iteration. It will give us, uh,",
    "start": "5782290",
    "end": "5788860"
  },
  {
    "text": "so let me- so if we are over here,",
    "start": "5788860",
    "end": "5794125"
  },
  {
    "text": "let's call this V of t. The fitted value iteration might give us y^i such that y^i is over here, right?",
    "start": "5794125",
    "end": "5804315"
  },
  {
    "text": "It might be outside our um, um, a functional space, the set of y^i's, right?",
    "start": "5804315",
    "end": "5809640"
  },
  {
    "text": "So we ideally want V of S^i to be equal to y^i.",
    "start": "5809640",
    "end": "5815525"
  },
  {
    "text": "Right? We want V of S^i to be equal to i, we want this to be 0, right? But this y may be outside our- our,",
    "start": "5815525",
    "end": "5824830"
  },
  {
    "text": "uh, um, our functional space, and therefore when we perform this minimization,",
    "start": "5824830",
    "end": "5830155"
  },
  {
    "text": "we are going to get the nearest- the projection of y^i onto this subspace, right?",
    "start": "5830155",
    "end": "5837190"
  },
  {
    "text": "That's- that's exactly, you know, the- the- what happened in linear regression. We're going to get this function over here,",
    "start": "5837190",
    "end": "5843940"
  },
  {
    "text": "which is the projection of y onto this- onto this space. The nearest point in our family of functions that is nearest to y, right?",
    "start": "5843940",
    "end": "5852910"
  },
  {
    "text": "And then we're- we're probably going to keep hopping, you know, by- by keeping ourselves within the family of- of,",
    "start": "5852910",
    "end": "5860320"
  },
  {
    "text": "um, the V functions of this parametric space, uh, that we have defined and get as close as possible to V-star.",
    "start": "5860320",
    "end": "5868344"
  },
  {
    "text": "That's what this- this, uh, algorithm will do us. This algorithm does not have",
    "start": "5868345",
    "end": "5873535"
  },
  {
    "text": "the convergence gua- guarantees of the discrete space setting.",
    "start": "5873535",
    "end": "5878890"
  },
  {
    "text": "In the discrete space setting, the bellman backup operator would take us to the true V-star after certain number of, uh, uh, trials.",
    "start": "5878890",
    "end": "5886525"
  },
  {
    "text": "Whereas in the fitted value functions, we have limited ourselves to some space and V-star might be out of the space.",
    "start": "5886525",
    "end": "5892435"
  },
  {
    "text": "There's no guarantee and there is no guarantee that this algorithm will converge. But in practice, it seems, you know,",
    "start": "5892435",
    "end": "5898330"
  },
  {
    "text": "it generally tends to work well. Yes, question. Is it- is it same like RV that is that is held in",
    "start": "5898330",
    "end": "5905860"
  },
  {
    "text": "some low dimension subspace in the bigger space? So think of this as some kind of a functional space,",
    "start": "5905860",
    "end": "5911770"
  },
  {
    "text": "don't think of this as- as. you know, uh, um, yeah, just think of it as some functional space where the- the set of",
    "start": "5911770",
    "end": "5918820"
  },
  {
    "text": "all functions that can be defined in this form is- is limited to some kind of a set.",
    "start": "5918820",
    "end": "5924054"
  },
  {
    "text": "If that- can that problem be solved by coming up with [inaudible]. Yeah, I am coming to that next, yeah.",
    "start": "5924055",
    "end": "5930429"
  },
  {
    "text": "Yeah, so your V-star can be outside the set of all functions.",
    "start": "5930430",
    "end": "5935830"
  },
  {
    "text": "Yes, question. Is this assuming the actions are still discrete? Yes,",
    "start": "5935830",
    "end": "5942190"
  },
  {
    "text": "here we are still assuming actions are discrete because we are performing a max over the different q^a's we're- is still assuming actions are discrete, good question.",
    "start": "5942190",
    "end": "5950840"
  },
  {
    "text": "And the [inaudible]. Yeah. Right? The- the fact that we are minimizing this,",
    "start": "5954600",
    "end": "5960775"
  },
  {
    "text": "you know, think of this as the, um, the resulting, um, theta transpose S^i offer minimizing this, um, uh,",
    "start": "5960775",
    "end": "5968680"
  },
  {
    "text": "after minimizing this squared error, the result, you know, one- once you do this, so V hat of S is equal to theta hat of Phi",
    "start": "5968680",
    "end": "5979420"
  },
  {
    "text": "of S. So this is essentially the projection of y onto the nearest point in the function. Yeah.",
    "start": "5979420",
    "end": "5986500"
  },
  {
    "text": "Also why- why- why have you not physically described below to that space, because-",
    "start": "5986500",
    "end": "5992770"
  },
  {
    "text": "Yeah, so the question is, um, yeah, that's- that's- that's, uh, something I'm coming, uh, when I address next.",
    "start": "5992770",
    "end": "5998290"
  },
  {
    "text": "Right? So, uh, there are- there are- there are, um,",
    "start": "5998290",
    "end": "6007440"
  },
  {
    "text": "we- we've seen two kind of, uh, extensions to the standard policy iteration and value iteration so far,",
    "start": "6007440",
    "end": "6013815"
  },
  {
    "text": "one extension was when we don't know the model, when we don't know the- the transition, uh, probabilities we estimate it from data, right?",
    "start": "6013815",
    "end": "6021615"
  },
  {
    "text": "PSA, and the other extension that we saw was to move on to this continuous setting- from",
    "start": "6021615",
    "end": "6027389"
  },
  {
    "text": "discrete setting and once we move into continuous setting, uh, the number of parameters we have can potentially be infinite and",
    "start": "6027390",
    "end": "6034760"
  },
  {
    "text": "therefore we limit ourselves- limit our express ability to some kind of, uh, parametric family model, right?",
    "start": "6034760",
    "end": "6042285"
  },
  {
    "text": "And once we are in- limited to this- to this, uh, um, parametric family,",
    "start": "6042285",
    "end": "6048870"
  },
  {
    "text": "uh, of- of value functions, which we call as value function approximation, we then define an algorithm which is,",
    "start": "6048870",
    "end": "6056840"
  },
  {
    "text": "you know, kind of, uh, uh, similar in spirit to the value iteration. Right? It is similar in spirit because we are performing",
    "start": "6056840",
    "end": "6063990"
  },
  {
    "text": "the same kind of updates except when we are, uh, in- in- in the- in the, uh, value iteration,",
    "start": "6063990",
    "end": "6070880"
  },
  {
    "text": "we could just set the value- the value that V of S needed to evaluate to.",
    "start": "6070880",
    "end": "6076255"
  },
  {
    "text": "However, in the- because we are now limited to, uh, a parametric family, what we do is minimize the loss between, um,",
    "start": "6076255",
    "end": "6084120"
  },
  {
    "text": "what- what our, uh, hypothesis can predict and what the right answer should be. Right? Now that we're, um,",
    "start": "6084120",
    "end": "6091745"
  },
  {
    "text": "you know, based on these two extensions, we can, um, so this- this- this basically kind of,",
    "start": "6091745",
    "end": "6097815"
  },
  {
    "text": "uh, marks the end of what's kind of in syllabus. Let's see how we can, you know,",
    "start": "6097815",
    "end": "6103395"
  },
  {
    "text": "the- some of the other concepts in reinforcement learning and how you can think of the other concepts that you may come across as extension for what we have seen so far.",
    "start": "6103395",
    "end": "6113160"
  },
  {
    "text": "Right? So in reinforcement learning,",
    "start": "6113160",
    "end": "6118780"
  },
  {
    "text": "in the- in the broader space , all right? We can think of two approaches in general.",
    "start": "6118880",
    "end": "6128320"
  },
  {
    "text": "The two approaches are approaches based",
    "start": "6130580",
    "end": "6136050"
  },
  {
    "text": "on value functions and approaches based on policies.",
    "start": "6136050",
    "end": "6141639"
  },
  {
    "text": "So value function-based approaches",
    "start": "6141790",
    "end": "6149254"
  },
  {
    "text": "versus policy-based, all right?",
    "start": "6149254",
    "end": "6154739"
  },
  {
    "text": "So in the simplest setting, right? And when everything is discrete and finite,",
    "start": "6154740",
    "end": "6160480"
  },
  {
    "text": "it's basically the value iteration and the policy based-approach in this is basically the policy iteration.",
    "start": "6163370",
    "end": "6170650"
  },
  {
    "text": "Right? And as we move on to more, uh, as we move on in terms of the, uh, complexity hierarchy.",
    "start": "6171020",
    "end": "6180550"
  },
  {
    "text": "When we move on to, uh, infinite space- infinite state space,",
    "start": "6180830",
    "end": "6188290"
  },
  {
    "text": "we get the fitted value iteration.",
    "start": "6190160",
    "end": "6194380"
  },
  {
    "text": "So fitted value iteration is basically this algorithm that we saw. All right.",
    "start": "6195380",
    "end": "6202120"
  },
  {
    "text": "In both these algorithms, we call both these algorithms as model-based algorithms.",
    "start": "6206230",
    "end": "6212540"
  },
  {
    "text": "We call them model-based algorithms because we explicitly- we constructed explicit versions of the way we believe that the model works, right?",
    "start": "6212540",
    "end": "6221285"
  },
  {
    "text": "It was either in the form of the PSA transition matrix or it was in the form of this linear dynamical system,",
    "start": "6221285",
    "end": "6228320"
  },
  {
    "text": "where S, S_t plus 1 was distributed according to a normal equation.",
    "start": "6228320",
    "end": "6237155"
  },
  {
    "text": "Okay? Either we constructed a- a dynamical system approximation of the- of the model, or we used,",
    "start": "6237155",
    "end": "6243800"
  },
  {
    "text": "um, you know, directly the PSA, uh, transition matrix for the,",
    "start": "6243800",
    "end": "6248825"
  },
  {
    "text": "uh, a finite, uh, finite state case. Okay? However, we can move on to,",
    "start": "6248825",
    "end": "6255530"
  },
  {
    "text": "you know, model-free methods. In the model free methods, right,",
    "start": "6255530",
    "end": "6261545"
  },
  {
    "text": "we acknowledge that we don't know how the model, uh, how the real world works, or how the dynamics of the system works.",
    "start": "6261545",
    "end": "6267889"
  },
  {
    "text": "But we also say we don't care how it works. Whereas in model-based approaches we acknowledged that we didn't know,",
    "start": "6267890",
    "end": "6273035"
  },
  {
    "text": "and we tried to learn it, right? In model-free approaches, we say we don't know how the- how the- how the world works.",
    "start": "6273035",
    "end": "6279110"
  },
  {
    "text": "We don't know how the transitions happen, and we don't care. And the way we say we don't care is by coming up with what is called as a Q function, right?",
    "start": "6279110",
    "end": "6287460"
  },
  {
    "text": "And, uh, the Q function is basically, think of it as, you know, Q of s,",
    "start": "6288370",
    "end": "6295640"
  },
  {
    "text": "a to be the value- the expected value of being in state S and taking an action a, right?",
    "start": "6295640",
    "end": "6306020"
  },
  {
    "text": "And the reason why, you know, even though it seems like a simple addition of an action,",
    "start": "6306020",
    "end": "6311795"
  },
  {
    "text": "it is fundamentally very different. The reason why it's very different because supposing we learned, you know,",
    "start": "6311795",
    "end": "6317420"
  },
  {
    "text": "V star of s and we don't know how the dynamics of the system works, but we still somehow figured out the optimal value of being in each,",
    "start": "6317420",
    "end": "6325460"
  },
  {
    "text": "uh, uh, being in, uh, for some state. Now, how do we can- decide what the action is?",
    "start": "6325460",
    "end": "6332700"
  },
  {
    "text": "There's no way because we don't know which state we will end up in by taking some action because we don't know how the dynamics of the system works, right?",
    "start": "6332770",
    "end": "6341480"
  },
  {
    "text": "So knowing V star is insufficient if you don't know the dynamics of the system. And the way we go about, uh, uh,",
    "start": "6341480",
    "end": "6349100"
  },
  {
    "text": "go about that limitation is to instead learn these Q functions, which are- which is basically, uh,",
    "start": "6349100",
    "end": "6355684"
  },
  {
    "text": "the Q function is basically telling us what is the expected sum of all future rewards- discounted sum of",
    "start": "6355685",
    "end": "6361820"
  },
  {
    "text": "future rewards if we are at state S and take an action a. We don't know what state we're going to end up in,",
    "start": "6361820",
    "end": "6367570"
  },
  {
    "text": "but if you take this action from the state, you know, the Q of s, a tells us what's the expected sum of future rewards, right?",
    "start": "6367570",
    "end": "6372610"
  },
  {
    "text": "We just stay agnostic to which state we're going to end up in and directly assign a value to taking an action at a given state, right?",
    "start": "6372610",
    "end": "6380705"
  },
  {
    "text": "And these techniques are called Q learning, and we can apply Q learning, uh,",
    "start": "6380705",
    "end": "6388130"
  },
  {
    "text": "function approximation to Q learning as well. We saw function approximation applied to value functions and just- just in a very similar way,",
    "start": "6388130",
    "end": "6396080"
  },
  {
    "text": "you can- we can perform function approximation for Q values or- of- of a Q function, right?",
    "start": "6396080",
    "end": "6403715"
  },
  {
    "text": "And, um, we saw an example where the family of,",
    "start": "6403715",
    "end": "6409070"
  },
  {
    "text": "uh, of- of - of the function space with which we perform the approximation was linear.",
    "start": "6409070",
    "end": "6414349"
  },
  {
    "text": "But there's nothing stopping us from using more complex functions. All right? So this, uh, so this approximation of the value function using a linear function,",
    "start": "6414350",
    "end": "6427445"
  },
  {
    "text": "this could have been something much more complex. We could have used a neural network here where the input was",
    "start": "6427445",
    "end": "6432530"
  },
  {
    "text": "S and the output was the value at the last layer. And- and- and when we're minimizing this, you know,",
    "start": "6432530",
    "end": "6439430"
  },
  {
    "text": "use back propagation and minimize the loss with respect to, you know, the parameters of a neural network, right?",
    "start": "6439430",
    "end": "6444530"
  },
  {
    "text": "There was nothing that stopped us from doing it. And when we use neural networks as the function approximators for either the value function or",
    "start": "6444530",
    "end": "6452750"
  },
  {
    "text": "the Q function that basically gives us the field of deep RL or deep reinforcement learning.",
    "start": "6452750",
    "end": "6459155"
  },
  {
    "text": "So you can use deep reinforcement learning by using neural networks as approximators for your value function or the Q function.",
    "start": "6459155",
    "end": "6466204"
  },
  {
    "text": "There is another- there are- there are other approaches you or- or other kinds of classifications you can think of.",
    "start": "6466205",
    "end": "6475400"
  },
  {
    "text": "Uh, in the finite case, the way we evaluated the value function was using something called as exact method.",
    "start": "6475400",
    "end": "6482640"
  },
  {
    "text": "So the Bellman backup operator, when you apply it over and over again, you get the exact V star.",
    "start": "6482640",
    "end": "6487809"
  },
  {
    "text": "And there are other, uh, approaches where we- either we can use Markov chains or Monte Carlo approaches,",
    "start": "6487810",
    "end": "6493900"
  },
  {
    "text": "not Markov chain, Monte Carlo approaches or you can use something called as TD learning or temporal difference learning.",
    "start": "6493900",
    "end": "6500335"
  },
  {
    "text": "TD Learning, uh, so in Mon- in- in Monte Carlo approaches, what we do is we take some value- we- we take",
    "start": "6500335",
    "end": "6506770"
  },
  {
    "text": "some policy and just follow that policy until the end, until we reach a terminating state and then average- and then",
    "start": "6506770",
    "end": "6515920"
  },
  {
    "text": "calculate the discounted sum of rewards and use that as the- as the estimate of the value function.",
    "start": "6515920",
    "end": "6520960"
  },
  {
    "text": "That's actually try it out and measure how much total value you got, and average it across multiple runs.",
    "start": "6520960",
    "end": "6526224"
  },
  {
    "text": "That's- that's, uh, Monte Carlo based. However, the limitations with that is that you've got to follow the policy until things terminate, right?",
    "start": "6526225",
    "end": "6533764"
  },
  {
    "text": "Whereas there is, uh, something called TD learning or temporal difference learning that is something in between exact and,",
    "start": "6533765",
    "end": "6539570"
  },
  {
    "text": "uh, Monte Carlo and in fact, in practice TD learning is what's used most commonly.",
    "start": "6539570",
    "end": "6544850"
  },
  {
    "text": "Okay? If you're using, uh, uh, so TD learning is in fact something very similar to this.",
    "start": "6544850",
    "end": "6551239"
  },
  {
    "text": "What we did over here is something very similar to TD learning. Hence, TD learning means temporal difference learning.",
    "start": "6551240",
    "end": "6558450"
  },
  {
    "text": "Uh, that's probably all",
    "start": "6558730",
    "end": "6568610"
  },
  {
    "text": "that I wanted to. So there's- there's probably one more thing I want to say. So there is another- another kind of, uh,",
    "start": "6568610",
    "end": "6577370"
  },
  {
    "text": "line of- line of- of classification you can use for, uh, a reinforcement learning algorithm.",
    "start": "6577370",
    "end": "6583580"
  },
  {
    "text": "There's something called on policy versus off policy methods.",
    "start": "6583580",
    "end": "6593030"
  },
  {
    "text": "So in on policy methods, you're trying to learn the value function by staying on the given policy whereas with off policy, you know,",
    "start": "6593030",
    "end": "6599900"
  },
  {
    "text": "you're looking at data that was- that was where the- the- the, uh, trajectories were- were obtained from some other policy,",
    "start": "6599900",
    "end": "6607250"
  },
  {
    "text": "and you're- you trying to learn the value function of a new policy that you're trying to come up with. And this off policy method has, you know,",
    "start": "6607250",
    "end": "6614300"
  },
  {
    "text": "for those of you who have a more, uh, statistical background, off policy, uh, methods and reinforcement- reinforcement learning has a strong connection",
    "start": "6614300",
    "end": "6622250"
  },
  {
    "text": "to causal inference, right? Because in causal inference you're seeing some observational data,",
    "start": "6622250",
    "end": "6629765"
  },
  {
    "text": "and you're trying to- to work your way backwards to decide, you know, what would have happened if we had taken some other action instead, right?",
    "start": "6629765",
    "end": "6636810"
  },
  {
    "text": "And- and- and that has, uh, kind of, uh, pretty strong ties to causal inference where here we're, you know,",
    "start": "6636810",
    "end": "6643565"
  },
  {
    "text": "observing a sequence of actions that were taken according to some policy, and we are trying to answer what would have happened if you had tried",
    "start": "6643565",
    "end": "6649040"
  },
  {
    "text": "some other policy instead, right? So reinforcement learning is, uh, is- is- is a vast, uh,",
    "start": "6649040",
    "end": "6655430"
  },
  {
    "text": "area of machine learning and in fact, probably reinforcement learning has its roots, uh,",
    "start": "6655430",
    "end": "6661145"
  },
  {
    "text": "in- in- in- in mostly in- in the field of electrical engineering called, uh, optimal control.",
    "start": "6661145",
    "end": "6667925"
  },
  {
    "text": "And I think, you know, it's history is probably older than machine learning in general.",
    "start": "6667925",
    "end": "6673970"
  },
  {
    "text": "And so it's- it's a pretty vast field and there are lots of ways in which you can use machine learning. And the most common way of the most popular way where machine learning and",
    "start": "6673970",
    "end": "6681800"
  },
  {
    "text": "reinforcement learning are kind of coming together is using, uh, is- is using more expressive, uh, uh,",
    "start": "6681800",
    "end": "6689210"
  },
  {
    "text": "function approximation methods to approximate either the value function or- or the Q function using neural networks or deep learning techniques.",
    "start": "6689210",
    "end": "6695900"
  },
  {
    "text": "And also there is, uh, it's- you can also use neural networks for, you know, policy based methods for something or policy iteration",
    "start": "6695900",
    "end": "6702380"
  },
  {
    "text": "though we didn't cover any of that, uh, in this course. All right. So that brings us to an end to, uh, reinforcement learning,",
    "start": "6702380",
    "end": "6710795"
  },
  {
    "text": "and starting Monday we will start the next chapter which is unsupervised learning, right.",
    "start": "6710795",
    "end": "6715969"
  },
  {
    "text": "Have a good weekend. [NOISE]",
    "start": "6715970",
    "end": "6726999"
  }
]