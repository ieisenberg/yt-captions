[
  {
    "start": "0",
    "end": "5010"
  },
  {
    "text": "OK, all right. Shall we get started? Everything else going OK?",
    "start": "5010",
    "end": "11070"
  },
  {
    "text": "All right, week four. How about we talk about GPUs?",
    "start": "11070",
    "end": "16279"
  },
  {
    "text": "Let me talk about\nsome GPUs today. All right. So here's what I\nthought I'd do today. I wanted to start with just\na little bit of history",
    "start": "16280",
    "end": "25500"
  },
  {
    "text": "on how we got here with modern\nGPU computing, which basically is like how did these chips\nthat folks like NVIDIA and AMD",
    "start": "25500",
    "end": "35100"
  },
  {
    "text": "were producing that were\nproducing largely to play Quake, how did they get us to\na point where NVIDIA",
    "start": "35100",
    "end": "43170"
  },
  {
    "text": "is $1 trillion company\nthat's much, much bigger than Intel and\neverybody's fighting over how to buy these things?",
    "start": "43170",
    "end": "49559"
  },
  {
    "text": "So I'll give some\nhistory, and then we're going to do a\nconversation about how to program these GPUs\nusing a language that's",
    "start": "49560",
    "end": "57270"
  },
  {
    "text": "provided by NVIDIA. I mean, most people write\nGPU code in CUDA these days. CUDA, you're going to\nfind very similar to ISPC.",
    "start": "57270",
    "end": "64849"
  },
  {
    "text": "In fact, ISPC, if\nyou read that blog post about the history of ISPC\nactually is a reaction to CUDA.",
    "start": "64849",
    "end": "71360"
  },
  {
    "text": "They were like, people are\nprogramming GPUs with CUDA, why can't we program\nthe same way on CPUs.",
    "start": "71360",
    "end": "76380"
  },
  {
    "text": "And so they hacked\nup ISPC in response. So the programming model\nshould be pretty familiar.",
    "start": "76380",
    "end": "82272"
  },
  {
    "text": "And then we're going\nto talk a little bit about how modern GPU\narchitectures work",
    "start": "82272",
    "end": "88960"
  },
  {
    "text": "and how they run that CUDA code. So I want you to think back\nto basically the first week",
    "start": "88960",
    "end": "95590"
  },
  {
    "text": "of class or the\nsecond week of class. And we talked about-- and you've\ndone a written assignment now,",
    "start": "95590",
    "end": "103600"
  },
  {
    "text": "really working through\nthe details of ideas like multithreading and SIMD\nand multicore execution.",
    "start": "103600",
    "end": "109520"
  },
  {
    "text": "And you've had some experience\nrunning that code on CPUs. So the first thing\nI want to tell you is that there's not going\nto be anything new today.",
    "start": "109520",
    "end": "117730"
  },
  {
    "text": "It's the same ideas-- multicore, SIMD, multithreading,\nand stuff like that.",
    "start": "117730",
    "end": "123039"
  },
  {
    "text": "They're just going to be\ndeployed at a little bit bigger scale for a modern GPU. So in some sense, no\nnew concepts today,",
    "start": "123040",
    "end": "130099"
  },
  {
    "text": "just another instantiation\nof those ideas.  So how many of you have taken--",
    "start": "130100",
    "end": "137650"
  },
  {
    "text": "looking for familiar faces-- have taken 148 or sit around for\n148 after this or 248 with me",
    "start": "137650",
    "end": "144610"
  },
  {
    "text": "or something like that? So some of you know a\nlittle bit about graphics, you know a little bit about\nhow pictures are rendered.",
    "start": "144610",
    "end": "151090"
  },
  {
    "text": "For the rest of you, here is\nCS 248, in like five slides.",
    "start": "151090",
    "end": "157810"
  },
  {
    "text": "So the goal of an\noriginal GPU design was to solve one\nspecific problem.",
    "start": "157810",
    "end": "166060"
  },
  {
    "text": "And that one\nspecific problem was given some mathematical\ndescription of a scene--",
    "start": "166060",
    "end": "172900"
  },
  {
    "text": "and that mathematical\ndescription was the geometry of surfaces,\nthe location of lights,",
    "start": "172900",
    "end": "179299"
  },
  {
    "text": "the position of some\nfake virtual camera-- perform a simulation,\nsimulate the bouncing of light",
    "start": "179300",
    "end": "186940"
  },
  {
    "text": "in the scene, simulate\nall these materials, and give me a picture of what\nI would have seen if that was",
    "start": "186940",
    "end": "194230"
  },
  {
    "text": "a real camera at that location. That was the goal\nof a graphics chip.",
    "start": "194230",
    "end": "200110"
  },
  {
    "text": "And graphics chips\ndo that quite well. Here's an image that's actually\na pretty old image now.",
    "start": "200110",
    "end": "205540"
  },
  {
    "text": "It's 2015. This image, 60 frames per\nsecond easily on a high end",
    "start": "205540",
    "end": "212110"
  },
  {
    "text": "GPU in 2015. If we step forward a little\nbit more modern graphics, this is something\nthat we can render",
    "start": "212110",
    "end": "217900"
  },
  {
    "text": "in real time on\nunreasonable high end GPUs. So this is from the\nNanite demo from Epic.",
    "start": "217900",
    "end": "225430"
  },
  {
    "text": "And the workload, the\nactual computation, if I asked you to write\nsome software that ran,",
    "start": "225430",
    "end": "232810"
  },
  {
    "text": "that computed this stuff,\nit's pretty simple. And so here's CS 248\nin maybe 45 seconds.",
    "start": "232810",
    "end": "242950"
  },
  {
    "text": "So the name of the game is to\nrender a surface like this. And so the way you work\nwith a GPU in the old days,",
    "start": "242950",
    "end": "251510"
  },
  {
    "text": "or at least if you're\ndoing graphics, is you provide it some notion\nof what your surface is,",
    "start": "251510",
    "end": "256570"
  },
  {
    "text": "and the surface that we-- the\nrepresentation that we typically use in computer graphics\nare triangle meshes.",
    "start": "256570",
    "end": "263390"
  },
  {
    "text": "So you give the GPU a list of\npoints in 3D space where all",
    "start": "263390",
    "end": "268750"
  },
  {
    "text": "your triangle vertices are,\nand you basically ask the GPU,",
    "start": "268750",
    "end": "275480"
  },
  {
    "text": "given these points in 3D space\nand given a camera position or orientation, hey, GPU, you go\ndo a little bit of math that we",
    "start": "275480",
    "end": "284050"
  },
  {
    "text": "teach you in 248 to figure out\nwhere on screen all of these little vertices project onto.",
    "start": "284050",
    "end": "291219"
  },
  {
    "text": "So it's like, if I held\nup a camera right here, where is every vertex\ngoing to be on screen.",
    "start": "291220",
    "end": "296860"
  },
  {
    "text": "And for every pixel\nin the image that's covered by these triangles, once\nI know where they're on screen,",
    "start": "296860",
    "end": "302960"
  },
  {
    "text": "I need to compute the\ncolor of that triangle. And if I want to compute\nthe color of the triangle, if you look around this\nroom, the color of all",
    "start": "302960",
    "end": "310440"
  },
  {
    "text": "the various surfaces\nin the room, it comes from what material\nthey're made out of. And if we have to simulate\nhow light is bouncing off",
    "start": "310440",
    "end": "319350"
  },
  {
    "text": "these chairs or off\nthis bamboo texture or off the shiny metal of the\ndoor handles, you quickly go,",
    "start": "319350",
    "end": "328420"
  },
  {
    "text": "wow, there's a lot of different\nmaterials in the world. And early graphics said--",
    "start": "328420",
    "end": "335250"
  },
  {
    "text": "and here's an example\nof a bunch of materials. These all look a lot lookalike. But if we can get\npretty sophisticated",
    "start": "335250",
    "end": "340470"
  },
  {
    "text": "with our materials, if we look\nat the glints in the shoes, if we look at the fact that\nhuman skin light enters",
    "start": "340470",
    "end": "346439"
  },
  {
    "text": "the skin, bounces around a\nlot and comes out somewhere else, simulating how\nlight bounces off",
    "start": "346440",
    "end": "352740"
  },
  {
    "text": "these materials to just compute\nthe color of these triangles, people were like, the materials\nof the world are so different.",
    "start": "352740",
    "end": "360580"
  },
  {
    "text": "We just want to write a\nprogram that does this. It's not like we're going\nto say the color is red or the color is blue.",
    "start": "360580",
    "end": "366290"
  },
  {
    "text": "We're just going\nto write a program. And so in the mid the early\n2000s was the time when people",
    "start": "366290",
    "end": "374470"
  },
  {
    "text": "started developing these little\nprogramming languages that were run for every pixel\non screen which,",
    "start": "374470",
    "end": "382599"
  },
  {
    "text": "given some information\nabout the current material and the orientation\nof the surface-- so the details of the code\ndoesn't really matter,",
    "start": "382600",
    "end": "390800"
  },
  {
    "text": "but it's like\nthere's a coordinate, a 2D coordinate of where\nI am on the surface. There's the normal\nof the surface,",
    "start": "390800",
    "end": "397610"
  },
  {
    "text": "which is like the vector\npointing away from the surface. And there's details\nof texture maps",
    "start": "397610",
    "end": "404180"
  },
  {
    "text": "which, if we just do\na little bit of work, we have algorithms\nfor computing, given the orientation\nof the surface, given",
    "start": "404180",
    "end": "411879"
  },
  {
    "text": "that I want like this image\nto be wrapped over the chair surface, if I run this code for\nevery single pixel on screen,",
    "start": "411880",
    "end": "420650"
  },
  {
    "text": "I compute a color. If you look at the output,\nreturn vector is a color. This is like KD is a vec 3 RGB\nand that last component there",
    "start": "420650",
    "end": "429300"
  },
  {
    "text": "is alpha. Running this function for\nevery element or every pixel",
    "start": "429300",
    "end": "434490"
  },
  {
    "text": "gives me an image\nthat looks like this. So there's just basic\ndata, parallel computation",
    "start": "434490",
    "end": "440919"
  },
  {
    "text": "like we've talked about for\nevery pixel compute some inputs and then run this completely\nindependent function",
    "start": "440920",
    "end": "447729"
  },
  {
    "text": "on every pixel to\ncompute some colors. That's kind of CS\n248 in a nutshell.",
    "start": "447730",
    "end": "452830"
  },
  {
    "text": "And so the reason why GPUs\nstarted adding multicore and adding SIMD is\nbecause they were like,",
    "start": "452830",
    "end": "459200"
  },
  {
    "text": "I got a process of 4K\nimage, millions of pixels,",
    "start": "459200",
    "end": "465400"
  },
  {
    "text": "and I need to do it 60\ntimes a second or more. I've got to get through\nhundreds of millions of pixels to billions of\npixels per second.",
    "start": "465400",
    "end": "472250"
  },
  {
    "text": "So they just started\nadding more and more GPUs to compute color more--",
    "start": "472250",
    "end": "477700"
  },
  {
    "text": "they added more and more\ncores and more and more ALUs in order to compute the\ncolor much more fast, much",
    "start": "477700",
    "end": "483940"
  },
  {
    "text": "more quickly. And so if you go back about\nalmost exactly 20 years now,",
    "start": "483940",
    "end": "490480"
  },
  {
    "text": "and we go back to\nlecture one, remember we said that there were\nmore and more transistors. The green line was the number\nof transistors per chip.",
    "start": "490480",
    "end": "498530"
  },
  {
    "text": "But all these ways to\nturn these transistors into faster single\nthreaded processors",
    "start": "498530",
    "end": "504590"
  },
  {
    "text": "were kind of not\nworking anymore. We couldn't ramp clock\nspeed because power",
    "start": "504590",
    "end": "510169"
  },
  {
    "text": "was going through the roof\nwith 4 gigahertz processors. We couldn't do\nsuperscalar anymore",
    "start": "510170",
    "end": "515390"
  },
  {
    "text": "because there was no\nILP to find in threads. But these GPUs had already\ngone to parallelism.",
    "start": "515390",
    "end": "523781"
  },
  {
    "text": "They were just going to\nadd more and more cores as the green line went up. And so some folks\naround the country,",
    "start": "523782",
    "end": "530430"
  },
  {
    "text": "some folks here at\nStanford, some folks at the University of North\nCarolina and a few other places",
    "start": "530430",
    "end": "535730"
  },
  {
    "text": "said, well, Intel's processors\nare not getting any faster,",
    "start": "535730",
    "end": "541610"
  },
  {
    "text": "but these GPUs are adding\nmore cores every single time those transistor doubles. And they can run code.",
    "start": "541610",
    "end": "547050"
  },
  {
    "text": "They run these\nlittle programs that compute the color of things. So some folks were\njust starting to hack,",
    "start": "547050",
    "end": "554870"
  },
  {
    "text": "and they started to hack\nby doing the following. Let's say you were rendering\nan image to a screen that",
    "start": "554870",
    "end": "560720"
  },
  {
    "text": "was 512 by 512. They said, here's\nwhat we're going to do-- we're going to triangles\nso that the entire screen is",
    "start": "560720",
    "end": "568880"
  },
  {
    "text": "covered by those triangles. And then instead of\nrunning a program that",
    "start": "568880",
    "end": "575006"
  },
  {
    "text": "computes the color\nof something, we're just going to have that\nprogram do something interesting, like time step a\nparticle in a physics simulation",
    "start": "575007",
    "end": "581879"
  },
  {
    "text": "or do some protein folding\nor something like that. So there was this big\nhack where you actually",
    "start": "581880",
    "end": "587760"
  },
  {
    "text": "drew two triangles on the\nscreen basically to create 512 squared function calls.",
    "start": "587760",
    "end": "594327"
  },
  {
    "text": "And then inside this\nthing that was supposed to compute the\ncolor of a surface, you just said, hey, do\nsomething that I want it to do.",
    "start": "594327",
    "end": "601140"
  },
  {
    "text": "And so there are these hacks. If you look at the dates on\nthese about 20 years ago, people said, I can\ntreat that RGBA output",
    "start": "601140",
    "end": "608790"
  },
  {
    "text": "as XYZ position of\na fluid simulation or something like that. So people started hacking.",
    "start": "608790",
    "end": "614250"
  },
  {
    "text": "And they're like, these GPUs\nare really, really fast, we can actually run a lot faster\nthan what we would do if we ran",
    "start": "614250",
    "end": "620970"
  },
  {
    "text": "threaded code in C++\nor something like that. Now there was a research\nproject at Stanford in 2004",
    "start": "620970",
    "end": "629100"
  },
  {
    "text": "which kind of said, wait\na minute, this is a hack. This is pretty stupid. If we're going to use these\nthings as parallel processors,",
    "start": "629100",
    "end": "635710"
  },
  {
    "text": "and we all know about\ndata parallelism and all this other stuff, can we just\nhave a proper programming system",
    "start": "635710",
    "end": "641530"
  },
  {
    "text": "to just treat them as such? And so there was\nthis project where",
    "start": "641530",
    "end": "646660"
  },
  {
    "text": "folks use the data\nparallel programming model which we'll talk about\nin more detail on Thursday.",
    "start": "646660",
    "end": "656900"
  },
  {
    "text": "But the main idea is\nthere's some function here, like in this case scale,\nwhich takes as input an A",
    "start": "656900",
    "end": "662260"
  },
  {
    "text": "and a B and just\nscales A by amount. But scale is evaluated\nnot on scalar values,",
    "start": "662260",
    "end": "669230"
  },
  {
    "text": "but on collections. So I'm making up\nsome syntax here. Think about this as a\nvector of 1,000 numbers,",
    "start": "669230",
    "end": "674690"
  },
  {
    "text": "and you're applying scale\nto every element of that collection. So this is called like the\nstream programming model or data",
    "start": "674690",
    "end": "681520"
  },
  {
    "text": "parallel programming model. And this code,\nwhich was actually valid code in this\nlanguage called Brook,",
    "start": "681520",
    "end": "688840"
  },
  {
    "text": "actually got compiled\nsource to source to this graphics program\nthat drew two triangles and then used the GPU.",
    "start": "688840",
    "end": "694720"
  },
  {
    "text": "Again, less of a\nhack for the users, but a total hack from the\nimplementation standpoint.",
    "start": "694720",
    "end": "701560"
  },
  {
    "text": "So around this time,\nNVIDIA was making faster and faster processors. They were starting to\nthink about, well, maybe",
    "start": "701560",
    "end": "708430"
  },
  {
    "text": "these things can do more than\njust graphics because they run code. And so inspired by projects\nlike this and others,",
    "start": "708430",
    "end": "717459"
  },
  {
    "text": "NVIDIA says, you know\nwhat, we should actually just do it ourselves to allow\npeople to write general purpose",
    "start": "717460",
    "end": "724060"
  },
  {
    "text": "code for these things because\nthe chip could already do that. So if we remember a little\nbit about how CPUs work,",
    "start": "724060",
    "end": "732410"
  },
  {
    "text": "imagine you had a\nmulti-core CPU here that has two cores, two\ntotal execution contexts.",
    "start": "732410",
    "end": "738230"
  },
  {
    "text": "How do you run some\ncode on this processor? Or in other words, what does\nthe operating system do? ",
    "start": "738230",
    "end": "748089"
  },
  {
    "text": "Can actually think through it. Imagine you were taking-- let's hypothesize. Many of you probably have not. But imagine you're taking\nan operating systems class.",
    "start": "748090",
    "end": "755940"
  },
  {
    "text": "And as an offering\nsystem, you're supposed to-- let's say we want\nto run two different programs on this two core machine.",
    "start": "755940",
    "end": "761670"
  },
  {
    "text": "How do you get that\nprogram started? So if you have two\ndifferent programs,",
    "start": "761670",
    "end": "767290"
  },
  {
    "text": "you ideally have two\nstreams of instructions. So I have two programs. I compile my program.",
    "start": "767290",
    "end": "773700"
  },
  {
    "text": "And then as the\noperating system, if I want that program\nto run, what happens? What do I do? You have two streams\nof instructions.",
    "start": "773700",
    "end": "779760"
  },
  {
    "text": "I'm sure you could\nkind parse each stream and assign it to\neach of the cores.",
    "start": "779760",
    "end": "785010"
  },
  {
    "text": "Yeah, so the operating\nsystem is going to say, here's an instruction\nstream one. Hey, core, let me initialize\nthe value of your registers.",
    "start": "785010",
    "end": "792340"
  },
  {
    "text": "I'm going to set the next\ninstruction to execute to the top of that program. You just tell it go.",
    "start": "792340",
    "end": "797520"
  },
  {
    "text": "So it runs the thread here. And if you want to\nrun another thread, the operating system might\nsay, I've got this other thread from this other program.",
    "start": "797520",
    "end": "803637"
  },
  {
    "text": "I'm going to run it\nhere on core zero. So you say, here's\nyour program binary.",
    "start": "803637",
    "end": "809379"
  },
  {
    "text": "Initialize the state\nof the thread and go. And you do this one\nthread at a time.",
    "start": "809380",
    "end": "814910"
  },
  {
    "text": "And that's exactly what\nan operating system does if you took an\noperating system, of course.",
    "start": "814910",
    "end": "822040"
  },
  {
    "text": "That's how we run code on CPUs. But how we ran code on GPUs\nbefore NVIDIA said, hey,",
    "start": "822040",
    "end": "829130"
  },
  {
    "text": "we need to give you access,\nwas no run of thread. It was literally\nthe command you sent",
    "start": "829130",
    "end": "835940"
  },
  {
    "text": "the GPU was draw these\ntriangles and use this program to compute the\ncolor of the pixels",
    "start": "835940",
    "end": "841330"
  },
  {
    "text": "after you figured out what\npixels need to be colored in. And so that interface to\nthe GPU was this thing",
    "start": "841330",
    "end": "847060"
  },
  {
    "text": "we call the graphics\npipeline that has all the algorithms\nof CS 248 in it, but the interface to\nsoftware is literally",
    "start": "847060",
    "end": "854320"
  },
  {
    "text": "software gives the GPU a list\nof triangles and a program to compute the\ncolor of the surface",
    "start": "854320",
    "end": "860620"
  },
  {
    "text": "and a location of the camera\nand then the pipeline just runs. And it runs that\nprogram whenever",
    "start": "860620",
    "end": "866860"
  },
  {
    "text": "it needs to compute\nthe color of a pixel. Very, very different way of\nthinking about how to kick off",
    "start": "866860",
    "end": "872050"
  },
  {
    "text": "computation. So in 2007, NVIDIA said, we need\nsomething in between those two.",
    "start": "872050",
    "end": "880430"
  },
  {
    "text": "It seems stupid to draw\ntriangles to use our processor to do things that have\nnothing to do with graphics.",
    "start": "880430",
    "end": "885819"
  },
  {
    "text": "And at the same\ntime, we don't really want users to create all these\ndifferent threads because all",
    "start": "885820",
    "end": "894100"
  },
  {
    "text": "these different\nthreads aren't going to run very well on my\nGPU that has 32 wide SIMD and has terrible single threaded\nperformance and stuff like that.",
    "start": "894100",
    "end": "901810"
  },
  {
    "text": "So they created this new\nabstraction called compute mode, which exists today. And the interface to the\nhardware is the following now.",
    "start": "901810",
    "end": "910450"
  },
  {
    "text": "It says, you write\nyourself a little program, like some function foo. In this case I call it my\nkernel because often we",
    "start": "910450",
    "end": "917080"
  },
  {
    "text": "call, in a data parallel\nsense, the code you run is called the kernel. And then you say, I want to\ninvoke N copies of this kernel.",
    "start": "917080",
    "end": "929580"
  },
  {
    "text": "GPU chip, you figure\nout how to do that. But I just want this\nkernel run N times.",
    "start": "929580",
    "end": "936180"
  },
  {
    "text": "So what programming\nmodel is this? This is classic\nSPMD programming. I'm going to give\nyou some code, it's",
    "start": "936180",
    "end": "942357"
  },
  {
    "text": "going to have a thread\nID in here somewhere, and we're going to launch N\ncopies and run N copies of this.",
    "start": "942357",
    "end": "947800"
  },
  {
    "text": "So you can think about this\nas similar programming model to something like ISPC.",
    "start": "947800",
    "end": "953550"
  },
  {
    "text": "And that programming model is\nwhat's embodied in what's called the CUDA programming\nlanguage which,",
    "start": "953550",
    "end": "958709"
  },
  {
    "text": "in modern GPU programming,\nis basically C, C++, with the one exception of\nyou don't create threads.",
    "start": "958710",
    "end": "965920"
  },
  {
    "text": "You just say, here's my program,\nplease run N copies of it. And then the GPU says,\nOK, I'll go figure",
    "start": "965920",
    "end": "973500"
  },
  {
    "text": "out how to parallelize those\ncopies, however I wish. Yes. [INAUDIBLE] contrast this again\nwith what we were doing before.",
    "start": "973500",
    "end": "979949"
  },
  {
    "text": "Before it was just\ncomputing two triangles and it wasn't in\nan SPMD fashion?",
    "start": "979950",
    "end": "985780"
  },
  {
    "text": "Yeah. I mean, the difference from\nbefore, it was literally-- you would open up OpenGL and\nyou would say OpenGL, draw",
    "start": "985780",
    "end": "995200"
  },
  {
    "text": "these triangles and\nthese positions. And you would set\nup the positions very carefully so that every\npixel on screen was covered.",
    "start": "995200",
    "end": "1002009"
  },
  {
    "text": "It was a hack, yeah. So now they're just like, let's\nuse proper SPMD programming abstractions to essentially\ndo the same thing.",
    "start": "1002010",
    "end": "1010080"
  },
  {
    "text": "Yeah, exactly. So here's the plan. So that's just some\nhistory and that's how where we are\ntoday with CUDA.",
    "start": "1010080",
    "end": "1017610"
  },
  {
    "text": "So I'm going to talk a little\nbit about this programming model. And while you're\nthinking about it,",
    "start": "1017610",
    "end": "1023248"
  },
  {
    "text": "I want you to think\nabout is this-- should I be thinking about\nthis as data parallel, should I",
    "start": "1023248",
    "end": "1028530"
  },
  {
    "text": "be thinking about it as SPMD? Is this message passing\nor a shared address space?",
    "start": "1028530",
    "end": "1033910"
  },
  {
    "text": "If you had to organize\nthis in your head, how would you draw analogies\nto ISPC and some things",
    "start": "1033910",
    "end": "1039780"
  },
  {
    "text": "that you know? And I'm going to\ndescribe-- since we're",
    "start": "1039780",
    "end": "1046260"
  },
  {
    "text": "talking about CUDA today, I'm\ngoing to use CUDA terminology. So when I talked\nabout ISPC, remember",
    "start": "1046260",
    "end": "1053330"
  },
  {
    "text": "I said there was like a\ngang of program instances? And you thought about program\ninstances as a thread logically,",
    "start": "1053330",
    "end": "1060630"
  },
  {
    "text": "but you knew under the hood\nthe implementation was not like a hardware\nexecution context. It was a little vector lane.",
    "start": "1060630",
    "end": "1067160"
  },
  {
    "text": "In CUDA, a program instance in\nISPC is called a CUDA thread.",
    "start": "1067160",
    "end": "1075210"
  },
  {
    "text": "So when I say thread\nin this lecture, unless I'm being careful to\nsay hardware execution context,",
    "start": "1075210",
    "end": "1080750"
  },
  {
    "text": "I mean a CUDA thread. And a CUDA thread is whatever\nNVIDIA defines it to be.",
    "start": "1080750",
    "end": "1089180"
  },
  {
    "text": "So here we go. So just like in ISPC, when\nyou call an ISPC function,",
    "start": "1089180",
    "end": "1094590"
  },
  {
    "text": "you launched a gang\nof program instances and the number of\ninstances in the gang",
    "start": "1094590",
    "end": "1100190"
  },
  {
    "text": "was N. It was kind of set\nat compile time, usually 8 or something like that.",
    "start": "1100190",
    "end": "1105290"
  },
  {
    "text": "And when you called\nISPC tasks, you said that I'm going\nto run this function,",
    "start": "1105290",
    "end": "1111040"
  },
  {
    "text": "I'm going to run this code,\nI need to run it N times. So when you're\nprogramming CUDA, you're",
    "start": "1111040",
    "end": "1118080"
  },
  {
    "text": "going to basically think\nabout it the same way. So here's a call from\nregular C++ code which says,",
    "start": "1118080",
    "end": "1125259"
  },
  {
    "text": "I want you to run the\nCUDA function matrix add. And to be honest, and I want\nyou to run it given these arrays",
    "start": "1125260",
    "end": "1133038"
  },
  {
    "text": "as input arguments. A, B, and C are just\nnormal C style arrays. And the only thing\nthat's kind of funky here",
    "start": "1133038",
    "end": "1139620"
  },
  {
    "text": "in this weird bracket\nsyntax is instead of saying, I want you\nto run it N times,",
    "start": "1139620",
    "end": "1146730"
  },
  {
    "text": "the number of\ninstances you create is this multi-dimensional value. And the reason why\nit's multi-dimensional",
    "start": "1146730",
    "end": "1153299"
  },
  {
    "text": "is just because it's convenient\nfor graphics and tensor operations. Can be helpful to\nsay create N threads,",
    "start": "1153300",
    "end": "1160720"
  },
  {
    "text": "but organize them in a 4 by 3\ngrid or something like that. So in other words,\nthread IDs are",
    "start": "1160720",
    "end": "1167310"
  },
  {
    "text": "going to be multi-dimensional. And the one additional\ndetail is that the instances",
    "start": "1167310",
    "end": "1175289"
  },
  {
    "text": "are grouped into these blocks. So what this code does is I'm\ncreating thread blocks of size 4",
    "start": "1175290",
    "end": "1186690"
  },
  {
    "text": "by 3 threads, 12\nthreads per block, and I'm creating a total of--",
    "start": "1186690",
    "end": "1194940"
  },
  {
    "text": "the number of\nblocks I'm creating is 12 divided by 4\nand 6 divided by 3.",
    "start": "1194940",
    "end": "1200740"
  },
  {
    "text": "So in other words, imagine\nI'm going to element wise add 2 matrices that are\neach of size 12 by 6.",
    "start": "1200740",
    "end": "1210019"
  },
  {
    "text": "And I want every\nCUDA thread to take on the job of\nadding one element.",
    "start": "1210020",
    "end": "1215280"
  },
  {
    "text": "So I need to create 72 threads. Now here, I decided to just\nsay, instead of just creating",
    "start": "1215280",
    "end": "1224630"
  },
  {
    "text": "72 threads and saying,\ncreate 72 threads, I just said, create\nthese thread blocks where",
    "start": "1224630",
    "end": "1231710"
  },
  {
    "text": "each thread block is 4 by 3. And the reason why I'm\nillustrating that now is I just want you to know about the\nconcept of thread blocks",
    "start": "1231710",
    "end": "1238640"
  },
  {
    "text": "so it comes back later. If all I was trying to do in\npractice of adding 2 arrays,",
    "start": "1238640",
    "end": "1243690"
  },
  {
    "text": "I might have just chosen to say\nthat there are 1D thread blocks and I'm just going to\nhave one block of size 72.",
    "start": "1243690",
    "end": "1248695"
  },
  {
    "text": " So here are some\nbasic CUDA syntax.",
    "start": "1248695",
    "end": "1253995"
  },
  {
    "text": "The code I showed on\nthe previous slide is what's up here. That's like main.cpp, calls\nthe CUDA matrix add function.",
    "start": "1253995",
    "end": "1261409"
  },
  {
    "text": "It does so with a certain\nnumber of thread blocks. And then the CUDA matrix add\nfunction looks a whole lot",
    "start": "1261410",
    "end": "1269150"
  },
  {
    "text": "like an SPMD program that\nyou're used for with ISPC. The only difference is instead\nof program count and program",
    "start": "1269150",
    "end": "1277260"
  },
  {
    "text": "index, there are these built-in\nvariables, block index, block dimension,\nand thread index",
    "start": "1277260",
    "end": "1284250"
  },
  {
    "text": "that allow you to figure\nout your unique thread ID. So in this case, thread\nID is the ID of the thread",
    "start": "1284250",
    "end": "1294150"
  },
  {
    "text": "in the block like 00,\n10, or so on and so on. The block ID is the block.",
    "start": "1294150",
    "end": "1301860"
  },
  {
    "text": "And then to know\nyour unique position, you need to know the\nsize of the blocks.",
    "start": "1301860",
    "end": "1307240"
  },
  {
    "text": "So you can think about it as\nyour thread ID in the block is kind of like your\nprogram instance",
    "start": "1307240",
    "end": "1312330"
  },
  {
    "text": "ID, your program instance. The block ID is is kind of\nlike your task ID in ISPC.",
    "start": "1312330",
    "end": "1320520"
  },
  {
    "text": "So there's this\nlevel of hierarchy is that instead of ISPC\norganizes things into tasks",
    "start": "1320520",
    "end": "1326220"
  },
  {
    "text": "and gangs inside the task,\nCUDA organizes itself into thread blocks and\nthen can create many thread",
    "start": "1326220",
    "end": "1333210"
  },
  {
    "text": "blocks at the same time. Again, no new concepts. Just some different names\nand some small differences",
    "start": "1333210",
    "end": "1340230"
  },
  {
    "text": "in the details. And so if you look here,\nthis is a CUDA function",
    "start": "1340230",
    "end": "1345780"
  },
  {
    "text": "called matrix add. The first thing it\ndoes is every thread computes which\nelement of the input",
    "start": "1345780",
    "end": "1351720"
  },
  {
    "text": "arrays it is responsible\nfor based on its thread ID and its block ID,\nand then every thread",
    "start": "1351720",
    "end": "1357809"
  },
  {
    "text": "carries out the work that\nit's responsible for. And keep in mind\nthat in my program,",
    "start": "1357810",
    "end": "1363100"
  },
  {
    "text": "72 instances of this function\nare being executed concurrently. So the entire array\nis added together.",
    "start": "1363100",
    "end": "1370179"
  },
  {
    "text": "Each thread copies\nor adds one element, and I create one thread\nper element of the matrix.",
    "start": "1370180",
    "end": "1377377"
  },
  {
    "text": "So there's some questions. So let me get to them. Yeah. I still don't understand why\ndid we move to a 2D system of--",
    "start": "1377377",
    "end": "1385800"
  },
  {
    "text": "I'm showing you the 2D system in\nthese slides because it exists, and I want you to know about it.",
    "start": "1385800",
    "end": "1392250"
  },
  {
    "text": "I could have done all of these\nintro examples as thread ID and block ID being\none dimensional,",
    "start": "1392250",
    "end": "1399780"
  },
  {
    "text": "and that would have worked\njust fine for this example. It turns out that in a lot of\nthings that we do on GPUs--",
    "start": "1399780",
    "end": "1405570"
  },
  {
    "text": "image processing, tensor\nprocessing, graphics-- it actually can be\na little bit helpful",
    "start": "1405570",
    "end": "1411659"
  },
  {
    "text": "to have your thread ID be two\ndimensional because then you just go IJ, whereas if your\nthread ID was dimensional",
    "start": "1411660",
    "end": "1418770"
  },
  {
    "text": "and then you had to compute\nyour location in an ND tensor, there would be a lot of divides.",
    "start": "1418770",
    "end": "1425640"
  },
  {
    "text": "So actually, you can\nskip a lot of divides by giving you the\nmulti-dimensional location,",
    "start": "1425640",
    "end": "1432270"
  },
  {
    "text": "and so you can compute your\nmemory address more efficiently. So that's the history of that. Yeah. But it's not fundamental at all.",
    "start": "1432270",
    "end": "1439470"
  },
  {
    "text": "And every year, I consider\nchanging these as the initial intro to be 1D thread IDs, but\nthen I have to introduce 2D",
    "start": "1439470",
    "end": "1447320"
  },
  {
    "text": "later. And so I just decide to save\nsome time by doing that. So at this point, you should\nbe thinking about the kind",
    "start": "1447320",
    "end": "1454020"
  },
  {
    "text": "of two worlds to live in. There's the world that's running\nthe normal C++ code over here",
    "start": "1454020",
    "end": "1463260"
  },
  {
    "text": "that's like void main. And you should think\nabout that as running on a thread on the CPU.",
    "start": "1463260",
    "end": "1470400"
  },
  {
    "text": "And then there's\nthe world that runs all of these instances of\nmatrix add and an ISPC.",
    "start": "1470400",
    "end": "1476980"
  },
  {
    "text": "That's where we said, it's\na gang of program instances. In ISPC, we said\nthe implementation",
    "start": "1476980",
    "end": "1482730"
  },
  {
    "text": "with SIMD instructions\nin that same thread. Now, without getting into too\nmany implementation details,",
    "start": "1482730",
    "end": "1490780"
  },
  {
    "text": "the implementation of running\nall of these CUDA threads is actually going to be\nexecution on the GPU.",
    "start": "1490780",
    "end": "1497070"
  },
  {
    "text": "So that call of matrix\nadd double V here is the point at which in the\nunderlying implementation, if we",
    "start": "1497070",
    "end": "1507240"
  },
  {
    "text": "get into that, that's going to\nbe the communication to start running things on the GPU.",
    "start": "1507240",
    "end": "1513156"
  },
  {
    "text": "Can you, in the\nprevious diagram, point out what the\nblock had mentioned. Yeah, sure. So for reasons that don't seem\nnecessary in this example,",
    "start": "1513156",
    "end": "1525240"
  },
  {
    "text": "I have organized my code. Instead of just saying\nlaunch 72 threads,",
    "start": "1525240",
    "end": "1531330"
  },
  {
    "text": "I have said launch a certain\nnumber of thread blocks. And each block is a 4\nby 3 grid of threads.",
    "start": "1531330",
    "end": "1539950"
  },
  {
    "text": "So the thread ID\nis two dimensional. Those are these numbers. The block dims are\nnot on this slide,",
    "start": "1539950",
    "end": "1547300"
  },
  {
    "text": "but the block dim is 4 by 3. And then the block index is\nwhatever the current block is.",
    "start": "1547300",
    "end": "1553830"
  },
  {
    "text": "So it's just a multidimensional\naddressing there.",
    "start": "1553830",
    "end": "1558906"
  },
  {
    "text": "Make sense? So the CUDA memory\nmodel is I have",
    "start": "1558906",
    "end": "1564630"
  },
  {
    "text": "void main presumably running on\nthe CPU that has its own address space.",
    "start": "1564630",
    "end": "1570300"
  },
  {
    "text": "And in simplistic\nCUDA, then there's the device, the GPU\nside of the world that",
    "start": "1570300",
    "end": "1578460"
  },
  {
    "text": "has its own memory address space\nthat can be accessed by the CUDA threads.",
    "start": "1578460",
    "end": "1585211"
  },
  {
    "text": "So that means that if you\nallocate an array in C and try and dereference\nthat pointer in CUDA,",
    "start": "1585211",
    "end": "1594070"
  },
  {
    "text": "that won't work because these\nare in different address spaces. So here's an example\nof this code.",
    "start": "1594070",
    "end": "1599870"
  },
  {
    "text": "Here's my void main\nrunning on the CPU. I allocate an array,\na normal C allocation.",
    "start": "1599870",
    "end": "1606980"
  },
  {
    "text": "That's an allocation in\nyour CPU address space. Then I allocate, where is\nit, cudamalloc, and then",
    "start": "1606980",
    "end": "1614440"
  },
  {
    "text": "I allocate an array in the GPU\nside of the fence, the device address space. And then I use library\ncalls to move the values",
    "start": "1614440",
    "end": "1622450"
  },
  {
    "text": "from the CPU address space\nto the GPU address space. And then when I\ncall a CUDA kernel,",
    "start": "1622450",
    "end": "1629419"
  },
  {
    "text": "I pass at the\npointers to device A because the CUDA kernels\nrunning on the GPU need",
    "start": "1629420",
    "end": "1635830"
  },
  {
    "text": "to access the memory in the\naddress space of the GPU. This is kind of\nnuts and bolts CUDA.",
    "start": "1635830",
    "end": "1642160"
  },
  {
    "text": "Now these days, on\nmore modern systems, it's easy to actually just\ndirectly pass a C pointer",
    "start": "1642160",
    "end": "1649149"
  },
  {
    "text": "into a CUDA kernel,\nbut it actually means that your GPU\nmemory access is actually",
    "start": "1649150",
    "end": "1654820"
  },
  {
    "text": "going to go over PCIe\nand things like that. So I want to stick to\nstraightforward, separate",
    "start": "1654820",
    "end": "1661000"
  },
  {
    "text": "address spaces for now.  All right. Questions, yes.",
    "start": "1661000",
    "end": "1667179"
  },
  {
    "text": "So given that\n[INAUDIBLE], going back to the example of\n[INAUDIBLE], in this case,",
    "start": "1667180",
    "end": "1674740"
  },
  {
    "text": "are there any\nsituations where it's useful to prefetch\ndata from the CPU to the GPU knowing what\ncomputations [INAUDIBLE]?",
    "start": "1674740",
    "end": "1682039"
  },
  {
    "text": "Absolutely. So this CUDA memcpy right\nhere is a movement from data--",
    "start": "1682040",
    "end": "1687640"
  },
  {
    "text": "now we're talking\nabout implementation-- is a movement of\ndata from CPU memory.",
    "start": "1687640",
    "end": "1694602"
  },
  {
    "text": "And if we're thinking\nabout a system that has a GPU that's\na discrete card, like an RTX GPU or\nan A100, this memcpy",
    "start": "1694602",
    "end": "1703330"
  },
  {
    "text": "is actually moving\nbytes over the PCI bus and putting it in the GPU's\nDRAM, two different drams.",
    "start": "1703330",
    "end": "1710600"
  },
  {
    "text": "So that's actually a slow copy. And yes, there are ways\nto do that asynchronous",
    "start": "1710600",
    "end": "1715810"
  },
  {
    "text": "and stuff like that. So what does this feel\nlike actually in terms of-- last time, didn't we talk\nabout message passing?",
    "start": "1715810",
    "end": "1722530"
  },
  {
    "text": "Kind of in some\nsense, this memcpy is a message passed from one\naddress space to another.",
    "start": "1722530",
    "end": "1727730"
  },
  {
    "text": "And you very well might want\nto do it asynchronously to hide latency and stuff like that. Absolutely.",
    "start": "1727730",
    "end": "1734280"
  },
  {
    "text": "Yeah. So going onto the\nprevious example when we had matrix add, what we\nwant is we pass in A, B, and C",
    "start": "1734280",
    "end": "1743160"
  },
  {
    "text": "into this function. So we would want A, B, and C-- A, B, and C in this example\nare CUDA device allocations.",
    "start": "1743160",
    "end": "1751770"
  },
  {
    "text": "And actually, I'm glad you\nbrought me back to this example. Here's an example of-- I wanted to check everybody's\nunderstanding here.",
    "start": "1751770",
    "end": "1759159"
  },
  {
    "text": "So this is the same\nmatrix add example, but I changed the\nmatrix size to 11 by 5.",
    "start": "1759160",
    "end": "1766919"
  },
  {
    "text": "But I kept the thread\nblock size to 4 by 3. ",
    "start": "1766920",
    "end": "1772830"
  },
  {
    "text": "I did this just for\nexplanatory purposes. So I have 55 things to do.",
    "start": "1772830",
    "end": "1779279"
  },
  {
    "text": "But I have thread blocks. I create work in the granularity\nof groups of 12 threads.",
    "start": "1779280",
    "end": "1786140"
  },
  {
    "text": "So if I round up-- and if\nyou look at my little math there for num blocks,\nI'm rounding up,",
    "start": "1786140",
    "end": "1793669"
  },
  {
    "text": "which means I'm creating more\nthreads than just 55 threads. So when each of these\nthreads does this work",
    "start": "1793670",
    "end": "1800330"
  },
  {
    "text": "to figure out its\nI and J, notice that some of those\nthreads would actually be out of bounds in that array.",
    "start": "1800330",
    "end": "1807950"
  },
  {
    "text": "And I have an if\nstatement in the code to say, hey, this thread\ncheck to make sure that you're actually\nresponsible for valid work.",
    "start": "1807950",
    "end": "1814770"
  },
  {
    "text": "And if you're not,\ndon't do anything. So this really underscores--\nit should underscore that this",
    "start": "1814770",
    "end": "1821270"
  },
  {
    "text": "is an SPMD programming model. You're just creating\nthreads and the threads use the program to figure out\nwhat they're supposed to do.",
    "start": "1821270",
    "end": "1829100"
  },
  {
    "text": "You are not doing\nsomething which says, for every element in\nan array, launch a thread. That's not the\nprogramming model.",
    "start": "1829100",
    "end": "1835620"
  },
  {
    "text": "The programming\nmodel is you launch a thread for all your blocks and\nfor all your threads per block.",
    "start": "1835620",
    "end": "1841650"
  },
  {
    "text": "And inside in my\ncode, I might write some code that happens to\ndo one thing per thread",
    "start": "1841650",
    "end": "1846850"
  },
  {
    "text": "or I could do whatever I want. So that if statement\nthere, what would happen if I didn't\nhave that if statement?",
    "start": "1846850",
    "end": "1852970"
  },
  {
    "text": " I would basically read\nand write to memory that",
    "start": "1852970",
    "end": "1859530"
  },
  {
    "text": "was off the end of my array. So who knows? My program would crash. So you're saying\nthe abstraction is",
    "start": "1859530",
    "end": "1865769"
  },
  {
    "text": "that you're kind of\ncreating threads, but it's just the\nimplementation that's very different versus CPU.",
    "start": "1865770",
    "end": "1871750"
  },
  {
    "text": "Correct The abstraction,\nto be precise, is you're creating CUDA threads. But you're not creating\nthem one by one.",
    "start": "1871750",
    "end": "1878490"
  },
  {
    "text": "You're creating them with\na bulk thread launch. So imagine you had a\nC++ API which said,",
    "start": "1878490",
    "end": "1885300"
  },
  {
    "text": "standard thread create, but you\ngave it some parameters after that which was like,\nI want 150 of them.",
    "start": "1885300",
    "end": "1893760"
  },
  {
    "text": "But really, it's not only\nthat I want 150 of them, I want 150 blocks of threads\nand each block should be 4 by 3.",
    "start": "1893760",
    "end": "1901020"
  },
  {
    "text": "So there's this bulk\nlaunch of threads. And I haven't told you\nhow it's implemented yet other than the GPU is\ngoing to run those threads.",
    "start": "1901020",
    "end": "1910768"
  },
  {
    "text": "So can you go back a few\nslides ahead [INAUDIBLE]? ",
    "start": "1910768",
    "end": "1920330"
  },
  {
    "text": "Yeah, I know. There it is. Yeah, OK. Yeah, so out here, after you've\ndone this CUDA [INAUDIBLE]",
    "start": "1920330",
    "end": "1928530"
  },
  {
    "text": "function, I guess\nall the computations you will be performing\nwill be on device A, right?",
    "start": "1928530",
    "end": "1935610"
  },
  {
    "text": "All the CUDA computations I'll\nbe doing will be on device A, correct. And in this code right\nhere, in this C++ code,",
    "start": "1935610",
    "end": "1943380"
  },
  {
    "text": "if I said print\nF device A sub 0,",
    "start": "1943380",
    "end": "1949500"
  },
  {
    "text": "convince yourself that should\nsegfault because device A is actually a pointer into\nan address space that I",
    "start": "1949500",
    "end": "1956610"
  },
  {
    "text": "can't reference from here. This is low level C code. So you can do that kind\nof stuff [INAUDIBLE].",
    "start": "1956610",
    "end": "1962648"
  },
  {
    "text": " So the basics of\nthe memory model",
    "start": "1962648",
    "end": "1969070"
  },
  {
    "text": "is that you have an address\nspace for your CPU threads, you have an address space\nfor your GPU CUDA threads,",
    "start": "1969070",
    "end": "1976450"
  },
  {
    "text": "and it turns out that you\nactually have additional address spaces in CUDA. We have a block of threads.",
    "start": "1976450",
    "end": "1981720"
  },
  {
    "text": "And one reason why\nthey have this concept of a block of threads is they\nsay that every thread block has",
    "start": "1981720",
    "end": "1990210"
  },
  {
    "text": "an address space that only\nthose threads can access, and every thread has\nits own local address",
    "start": "1990210",
    "end": "1997710"
  },
  {
    "text": "space, its own local stack\nthat only it can access. So local variables\nto a thread can only",
    "start": "1997710",
    "end": "2005300"
  },
  {
    "text": "be accessed by the thread. That makes sense. That's like ISPC. But now there's per\nthread block variables,",
    "start": "2005300",
    "end": "2012169"
  },
  {
    "text": "which are kind of uniform\nvariables in ISPC actually. But there's a per\nthread block variables",
    "start": "2012170",
    "end": "2018140"
  },
  {
    "text": "and then there's overall\ndevice global memory that can be accessed by any\nthread with loads and stores.",
    "start": "2018140",
    "end": "2024470"
  },
  {
    "text": "So you just see this\norganization that's popping up. You're kind of\nsaying, here's a bunch of threads that are going to\nwork together on something.",
    "start": "2024470",
    "end": "2031973"
  },
  {
    "text": "Here's another group of\nthreads that are going to work together on something. And to do that work\ntogether, they're going to have some shared space.",
    "start": "2031973",
    "end": "2039450"
  },
  {
    "text": "And so, hopefully,\nyou're starting to think, we're giving hints\nto the GPU on how to co-locate all these\nthreads for locality purposes.",
    "start": "2039450",
    "end": "2047769"
  },
  {
    "text": "So let's do some CUDA practice\nwith one of the simplest things I can think of in CUDA, which is\nlet's not add to two matrices,",
    "start": "2047770",
    "end": "2055690"
  },
  {
    "text": "but let's perform a\nsimple 1D convolution.",
    "start": "2055690",
    "end": "2062260"
  },
  {
    "text": "So I'm going to\ntake the output is the average of the inputs\nin the same location.",
    "start": "2062260",
    "end": "2067719"
  },
  {
    "text": "So this is a basic operation\nin image processing or signal processing. It's actually the 1D equivalent\nof a convolutional layer",
    "start": "2067719",
    "end": "2074688"
  },
  {
    "text": "in a deep neural network,\nsomething that you might want to do. So look at the top of the slide,\nand I have illustrated the input",
    "start": "2074688",
    "end": "2081570"
  },
  {
    "text": "and output location, and I\nhave some red arrows there that suggest what input\nelements are needed to compute",
    "start": "2081570",
    "end": "2089580"
  },
  {
    "text": "every output element. So first of all,\nconfirm that you agree the input is two elements\nbigger than the output,",
    "start": "2089580",
    "end": "2095770"
  },
  {
    "text": "so I don't have to about\nany boundary conditions or crap like that. Just keep it. So here's the code for that.",
    "start": "2095770",
    "end": "2101980"
  },
  {
    "text": "So first of all, we call\nthe convolve CUDA kernel.",
    "start": "2101980",
    "end": "2107700"
  },
  {
    "text": "If the array is 1,024 in size-- let's say the array is a\nmillion elements, 1,024 in size.",
    "start": "2107700",
    "end": "2118079"
  },
  {
    "text": "If my block size is\nthreads per block, notice that I'm spawning n\nover threads per block blocks,",
    "start": "2118080",
    "end": "2125280"
  },
  {
    "text": "assuming that divides evenly. Let's keep my integer math. And then for every thread, we\njust treat it as a 1D array",
    "start": "2125280",
    "end": "2131400"
  },
  {
    "text": "index in this case,\nand I say, OK, I'm going to compute my\nindex that I'm responsible",
    "start": "2131400",
    "end": "2136680"
  },
  {
    "text": "for computing, I'm going to\nload three values from memory, multiply them together\nand output the result.",
    "start": "2136680",
    "end": "2144549"
  },
  {
    "text": "So first of all, confirm that\nthis is a correct implementation of a 1D convolution in CUDA\nbecause I will produce 1 million",
    "start": "2144550",
    "end": "2153099"
  },
  {
    "text": "or 1,024 squared\nout for elements. ",
    "start": "2153100",
    "end": "2159779"
  },
  {
    "text": "So far in this example, I\ndidn't use the idea of threads per block really at all.",
    "start": "2159780",
    "end": "2166168"
  },
  {
    "text": "I really would have\npreferred a programming model which just said,\nget rid of all this stuff. Just say launch N\nthreads and I'm good.",
    "start": "2166168",
    "end": "2173843"
  },
  {
    "text": "I don't have to deal\nwith any of this blocking and stuff like that. So if you look at\nthis code, this code,",
    "start": "2173843",
    "end": "2181069"
  },
  {
    "text": "every thread reads\nthree elements, and different threads kind\nof neighboring threads",
    "start": "2181070",
    "end": "2187310"
  },
  {
    "text": "actually overlap in the\nelements that they read. So in some sense, this thread\nis issuing load instructions",
    "start": "2187310",
    "end": "2194420"
  },
  {
    "text": "for data, and then\nthe other thread is issuing load instructions\nfor the same data.",
    "start": "2194420",
    "end": "2200490"
  },
  {
    "text": "Now maybe there could\nbe some cache hits and stuff like that\nto amplify that, but if we really wanted to\nwrite an efficient program,",
    "start": "2200490",
    "end": "2207698"
  },
  {
    "text": "let me show you something\nelse that I can-- So here's a much\nmore complex program, but it gives you a\nsense of what you",
    "start": "2207698",
    "end": "2213140"
  },
  {
    "text": "can do with this\nidea of a block that has memory local to the block.",
    "start": "2213140",
    "end": "2218940"
  },
  {
    "text": "So the same thing down here. I'm still launching the same\nnumber of thread blocks. My thread block size\nis, what, 128 here.",
    "start": "2218940",
    "end": "2225530"
  },
  {
    "text": "So to compute 128 outputs,\nhow many inputs do I need? ",
    "start": "2225530",
    "end": "2232260"
  },
  {
    "text": "128 plus 2, 130 inputs. So now look at what\nmy thread does.",
    "start": "2232260",
    "end": "2238870"
  },
  {
    "text": "My thread allocates this\nshared array of 130 elements.",
    "start": "2238870",
    "end": "2245200"
  },
  {
    "text": "And that prefix, that\ntype modifier shared is saying this is not a\nper thread allocation,",
    "start": "2245200",
    "end": "2252250"
  },
  {
    "text": "this is a per thread\nblock allocation. So this is like a\nuniform variable in ISPC.",
    "start": "2252250",
    "end": "2259470"
  },
  {
    "text": "And now look what\nthe threads do. This is kind of interesting. So every thread\nloads one value--",
    "start": "2259470",
    "end": "2270690"
  },
  {
    "text": "if I go right to that\nfirst line index equals-- so I compute the index.",
    "start": "2270690",
    "end": "2276450"
  },
  {
    "text": "And then the first line\nof code in the block is copying a value\nfrom the input array",
    "start": "2276450",
    "end": "2282690"
  },
  {
    "text": "into the shared allocation. So the first thread in the block\ncomputes something and then",
    "start": "2282690",
    "end": "2289710"
  },
  {
    "text": "copies that value into that\nshared variable at address 0, at index 0, and so on and so on.",
    "start": "2289710",
    "end": "2296830"
  },
  {
    "text": "And then I say, if the\nthread index-- and remember, that's a block local thread\nindex-- is less than 2--",
    "start": "2296830",
    "end": "2303049"
  },
  {
    "text": "so in other words, if your\nthread 0 or 1 in the block-- what do they do?",
    "start": "2303050",
    "end": "2308730"
  },
  {
    "text": " They move the last two-- They move the last two elements.",
    "start": "2308730",
    "end": "2315019"
  },
  {
    "text": "So I have 128\nthreads in the block. Every thread is\nresponsible for grabbing",
    "start": "2315020",
    "end": "2320470"
  },
  {
    "text": "one piece of data from\nmemory and putting it in this shared array. And then because I need 130\nthings and not 128 things,",
    "start": "2320470",
    "end": "2327470"
  },
  {
    "text": "I had to have two of these\nfolks do a little bit more work. So ugly. And then ignore\nthis for a second.",
    "start": "2327470",
    "end": "2334630"
  },
  {
    "text": "And then every thread just\ndoes what it's supposed to do. It computes the convolution. But instead of accessing\nmain global memory,",
    "start": "2334630",
    "end": "2341720"
  },
  {
    "text": "it accesses this local\nsupport, the shared variable. ",
    "start": "2341720",
    "end": "2348600"
  },
  {
    "text": "Now this is the last\nlittle piece of this. And that sync threads\nis the equivalent",
    "start": "2348600",
    "end": "2355150"
  },
  {
    "text": "of a CUDA barrier\nwithin a thread block. So remember, all these\nthreads, it's SPMD.",
    "start": "2355150",
    "end": "2362150"
  },
  {
    "text": "They might be\nrunning concurrently. All the threads\ncooperatively load some data",
    "start": "2362150",
    "end": "2367660"
  },
  {
    "text": "into this shared array. They barrier, which is waiting\nfor all the threads in the block",
    "start": "2367660",
    "end": "2373940"
  },
  {
    "text": "to do their share of the work. And then once they know\nthat all the data is there, they run completely\nagain in parallel,",
    "start": "2373940",
    "end": "2380940"
  },
  {
    "text": "independently computing\ntheir output element. So this is an interesting\nthing about why",
    "start": "2380940",
    "end": "2387740"
  },
  {
    "text": "did I spend all this time like\norchestrating this computation. Well, the reason is that\nthese shared variables",
    "start": "2387740",
    "end": "2395000"
  },
  {
    "text": "are backed by\nstorage that you can think of as a high\nperformance L1 cache.",
    "start": "2395000",
    "end": "2401559"
  },
  {
    "text": "So what CUDA knows-- and now I'm talking\nimplementation-- is that it's going to put all\nthe threads in the same thread",
    "start": "2401560",
    "end": "2408530"
  },
  {
    "text": "block, kind of on the same core. And those threads can now\naccess this fast local memory",
    "start": "2408530",
    "end": "2416450"
  },
  {
    "text": "whenever there is reuse. So this code is actually\nsaying that, look, no thread accesses\nthe same data twice.",
    "start": "2416450",
    "end": "2425030"
  },
  {
    "text": "No one thread ever reuses\ndata, but one thread uses data",
    "start": "2425030",
    "end": "2430640"
  },
  {
    "text": "that its neighbor also uses. So we cooperate to bring\nall that data in once, and then we do our\nwork running out",
    "start": "2430640",
    "end": "2437180"
  },
  {
    "text": "of really fast shared\nmemory, or local memory. Yes, sir. Yep.",
    "start": "2437180",
    "end": "2442610"
  },
  {
    "text": "Why do you need\nthe sync threads? Can you not [INAUDIBLE]? I've loaded this\nsmall grid and then",
    "start": "2442610",
    "end": "2448130"
  },
  {
    "text": "let's run the company\nfor this small-- This is a great question. I want us to think\nabout that as a class. If I remove that\nsync threads call,",
    "start": "2448130",
    "end": "2455120"
  },
  {
    "text": "why could my program\nbe incorrect? Yeah, the access thread--",
    "start": "2455120",
    "end": "2461599"
  },
  {
    "text": "the access [INAUDIBLE] can get\nloaded into the shared space. That's right.",
    "start": "2461600",
    "end": "2466650"
  },
  {
    "text": "So there was no\nguarantee in what order a GPU is going to run this code. It's just all these threads\nare potentially concurrent.",
    "start": "2466650",
    "end": "2473810"
  },
  {
    "text": "So one thread might load\nits data in and then immediately start\ndoing the computation. But we don't know if\nall the other threads",
    "start": "2473810",
    "end": "2479650"
  },
  {
    "text": "has brought its data in yet. So that syncthreads\nis the barrier which, when you\nproceed past it, you",
    "start": "2479650",
    "end": "2485240"
  },
  {
    "text": "are guaranteed that the\nshared local memory has been initialized properly.",
    "start": "2485240",
    "end": "2490940"
  },
  {
    "text": "Good question. Why do we need a separate\nthread [INAUDIBLE]? Why couldn't we have\na large block memory?",
    "start": "2490940",
    "end": "2497680"
  },
  {
    "text": " I see. So you're saying, why do I have\nlocal variables like result.",
    "start": "2497680",
    "end": "2506000"
  },
  {
    "text": "I wouldn't necessarily\nstress too much about it. I mean, I feel like, clearly,\nthe programming model",
    "start": "2506000",
    "end": "2512480"
  },
  {
    "text": "needs to allocate\nthread local variables.  So by default, everything\nis a thread local variable.",
    "start": "2512480",
    "end": "2519970"
  },
  {
    "text": "There's a copy per thread. And then there are\nalso the ability",
    "start": "2519970",
    "end": "2525150"
  },
  {
    "text": "to allocate once\nper block variables. Yeah, so where are these\nvariables get allocated--",
    "start": "2525150",
    "end": "2532440"
  },
  {
    "text": "the implementation\ncould allocate these in a piece\nof memory that's",
    "start": "2532440",
    "end": "2538023"
  },
  {
    "text": "a big block for all the\nthreads and stuff like that, but that's up to the compiler. But conceptually, it\nmakes sense that I",
    "start": "2538023",
    "end": "2543390"
  },
  {
    "text": "need private variables that\nare private to my own thread's execution, and I might need\nshared variables as well.",
    "start": "2543390",
    "end": "2550369"
  },
  {
    "text": "Yes, sir. So these threads run\nin SPMD fashion, right? CUDA code is SPMD, yes.",
    "start": "2550370",
    "end": "2556130"
  },
  {
    "text": "[INAUDIBLE]  SPMD, you're\nconfusing with SIMD.",
    "start": "2556130",
    "end": "2563430"
  },
  {
    "text": "SPMD is a programming\nmodel question independent of the implementation.",
    "start": "2563430",
    "end": "2568690"
  },
  {
    "text": "So like ISPC is an\nSPMD programming model, CUDA is a SPMD\nprogramming model.",
    "start": "2568690",
    "end": "2575440"
  },
  {
    "text": "What that means is\nI write one program, like this program right\nhere, and the system will run it many times\nwith different thread IDs.",
    "start": "2575440",
    "end": "2583170"
  },
  {
    "text": "Now how we execute all those\nthreads or all those instances efficiently is up to\nthe implementation.",
    "start": "2583170",
    "end": "2590640"
  },
  {
    "text": "And ISPC use SIMD\nthe instructions to execute things\nvery efficiently.",
    "start": "2590640",
    "end": "2597549"
  },
  {
    "text": "I have not yet told you how\nthe GPU implements things. And yes, there will\nbe SIMD involved, but it's a little bit different.",
    "start": "2597550",
    "end": "2603810"
  },
  {
    "text": "Yeah. Is it up to the\nprogrammer to [INAUDIBLE]? This is completely\nunder your control.",
    "start": "2603810",
    "end": "2610050"
  },
  {
    "text": "In the same way that it's up\nto you to figure out how many threads you want to\nlaunch in C++, here, it's up to you to figure out how\nmany threads you want to launch,",
    "start": "2610050",
    "end": "2617080"
  },
  {
    "text": "but it's also up to you to\ntell the system how you want to organize them into blocks. [INAUDIBLE] always be\nthe number of threads",
    "start": "2617080",
    "end": "2622650"
  },
  {
    "text": "that a single core can run? Well, it cannot be.",
    "start": "2622650",
    "end": "2630668"
  },
  {
    "text": "Let me tell you this way. You are not allowed for\nit to be more threads than a single core can run.",
    "start": "2630668",
    "end": "2636180"
  },
  {
    "text": "It can certainly be\nless because I can put multiple blocks on a core.",
    "start": "2636180",
    "end": "2641670"
  },
  {
    "text": "Yeah. In this case, we were able to\ndo this block data [INAUDIBLE]",
    "start": "2641670",
    "end": "2647490"
  },
  {
    "text": "because we were reading\ndata from the same part of the memory [INAUDIBLE]. But if we're reading, say,\nsomething like a matrix,",
    "start": "2647490",
    "end": "2654000"
  },
  {
    "text": "and you want to read\nthe column of a matrix, would it be more efficient to\ndo the same thing where you're",
    "start": "2654000",
    "end": "2662190"
  },
  {
    "text": "reading per block memory or\nwould you want to read it, say, by--",
    "start": "2662190",
    "end": "2668033"
  },
  {
    "text": "well, all I'm\nasking is like, does it matter if you're reading\ncontiguous in block or-- It will certainly\nmatter your performance.",
    "start": "2668033",
    "end": "2673960"
  },
  {
    "text": "Absolutely. The nice thing here is that\nall these threads are accessing consecutive memory\naddresses, and you",
    "start": "2673960",
    "end": "2679170"
  },
  {
    "text": "can imagine that's going to be\nquite friendly under the hood. But if they were accessing-- certainly it's possible they\ncould access arbitrary memory",
    "start": "2679170",
    "end": "2686730"
  },
  {
    "text": "addresses and bring them in. The program would still work,\nbut obviously the memory system",
    "start": "2686730",
    "end": "2692160"
  },
  {
    "text": "may provide lower performance\nbecause of cache locality and other things.",
    "start": "2692160",
    "end": "2697780"
  },
  {
    "text": "And so do we have control\nover how cudamalloc stores [INAUDIBLE]? Well, you have control over--",
    "start": "2697780",
    "end": "2704410"
  },
  {
    "text": "when you ask\ncudamalloc for data, it will allocate you a\ncontinuous part of the address space, just like\nmalloc does in C++.",
    "start": "2704410",
    "end": "2712480"
  },
  {
    "text": "How you lay out and place\nyour matrix elements in that block of memory\nis completely up to you as",
    "start": "2712480",
    "end": "2717790"
  },
  {
    "text": "how you wrote that program. Here, I'm assuming\ncontiguous row major.",
    "start": "2717790",
    "end": "2722990"
  },
  {
    "text": "But that was my\nchoice as the writer. So unlike ISPC,\nyou actually-- you",
    "start": "2722990",
    "end": "2729119"
  },
  {
    "text": "know how ISPC had those cross\nlane operations to communicate with some of the instances.",
    "start": "2729120",
    "end": "2734540"
  },
  {
    "text": "CUDA has a variety of\nsynchronization constructs. It has a per thread\nblock barrier, it has atomic operations,\nand then, of course,",
    "start": "2734540",
    "end": "2741680"
  },
  {
    "text": "there's actually\nsynchronization that you have to do between the\nhost and the device-- start these CUDA threads,\nwait for them to be done",
    "start": "2741680",
    "end": "2747820"
  },
  {
    "text": "and stuff like that. And you'll run into all\nof that on assignment 3. So the programming\nmodel for CUDA",
    "start": "2747820",
    "end": "2753369"
  },
  {
    "text": "is an SPMD programming\nmodel where write a single program that\ndefines what a thread does.",
    "start": "2753370",
    "end": "2759800"
  },
  {
    "text": "What a thread does is based on\nits thread ID, its block ID, and so on and so on.",
    "start": "2759800",
    "end": "2765040"
  },
  {
    "text": "Those threads can synchronize\nin a block with some barriers if they so wish. And when I launch\nthreads, I launch",
    "start": "2765040",
    "end": "2773349"
  },
  {
    "text": "threads in mass for a large\nnumber of threads all at once. ",
    "start": "2773350",
    "end": "2781470"
  },
  {
    "text": "So now let's start thinking\nabout implementation. So take a look at this program.",
    "start": "2781470",
    "end": "2787700"
  },
  {
    "text": "This is the same convolve\nprogram as before. Exact same thing as\nthe previous slide.",
    "start": "2787700",
    "end": "2792850"
  },
  {
    "text": "Thread block size is 128. But look down here\nin the host code. This is a C++ code.",
    "start": "2792850",
    "end": "2799059"
  },
  {
    "text": "And I decide to create\nabout a million threads.",
    "start": "2799060",
    "end": "2808690"
  },
  {
    "text": "And if you divide\na million by 128, it's about 8K thread blocks. So how do you think this\nis going to be implemented?",
    "start": "2808690",
    "end": "2816590"
  },
  {
    "text": "Do you think NVIDIA\nis going to launch? Semantically, this says, we need\nto run a million CUDA threads.",
    "start": "2816590",
    "end": "2826530"
  },
  {
    "text": "Do you think it's actually\ngoing to go try and find a million thread contexts\naround these widths? Why not?",
    "start": "2826530",
    "end": "2832810"
  },
  {
    "text": "It probably doesn't even\nhave that capability to have that many\nexecution contexts. So what does this big\nbulk launch remind you of?",
    "start": "2832810",
    "end": "2842914"
  },
  {
    "text": "[INAUDIBLE] Yeah, it feels a lot like task. It's like, I'm going to give\nyou a whole bunch of work.",
    "start": "2842914",
    "end": "2847990"
  },
  {
    "text": "Do all of these thread blocks. That's all the\nwork you got to do. And just like an\nISPC task breaks down",
    "start": "2847990",
    "end": "2854590"
  },
  {
    "text": "in the program instances,\nCUDA thread block breaks down into CUDA threads.",
    "start": "2854590",
    "end": "2860520"
  },
  {
    "text": "So we want to be able to run\nthis thing without the user. This code should work regardless\nof the number of cores you have",
    "start": "2860520",
    "end": "2867960"
  },
  {
    "text": "or the number of your SIMD\nwidth and so on and so on. We want this thing to work on\na big GPU with a bunch of cores",
    "start": "2867960",
    "end": "2875220"
  },
  {
    "text": "or a mid-range GPU with small\nnumber of cores or a Grace Hopper chip that cost $100,000\nor whatever it is these days.",
    "start": "2875220",
    "end": "2882690"
  },
  {
    "text": "We want it to run on\nany of these things. So now let's talk\nabout implementation.",
    "start": "2882690",
    "end": "2888190"
  },
  {
    "text": "So here's our program again. Host CPU code, device code. And when I compile\nthis device code now,",
    "start": "2888190",
    "end": "2894720"
  },
  {
    "text": "I not only get the\ninstruction stream, but I also have a\nlittle bit of metadata about what it requires to run.",
    "start": "2894720",
    "end": "2901320"
  },
  {
    "text": "I might have something\nlike-- it says, well, I need 128 threads per\nblock because that's",
    "start": "2901320",
    "end": "2907030"
  },
  {
    "text": "what the programmer\nasserted I needed. It also says that if it's\n128 threads per block,",
    "start": "2907030",
    "end": "2913370"
  },
  {
    "text": "it means that shared allocation\nis 130 elements wide.",
    "start": "2913370",
    "end": "2918800"
  },
  {
    "text": "So I know that I need 130 floats\nor I need 512 bytes of storage",
    "start": "2918800",
    "end": "2924670"
  },
  {
    "text": "in order to do that allocation. So I know I need 128\nthreads worth of resources",
    "start": "2924670",
    "end": "2931030"
  },
  {
    "text": "and I need 520 bytes\nof storage in order",
    "start": "2931030",
    "end": "2936760"
  },
  {
    "text": "to actually run this thing.  So what's going to happen is\nimagine we have a little GPU",
    "start": "2936760",
    "end": "2943900"
  },
  {
    "text": "here, has four cores. Let's not think\nabout the details. But I just created those 8,000\nthread blocks up at the top.",
    "start": "2943900",
    "end": "2953200"
  },
  {
    "text": "So you're pretty\ngood at this now, or at least for those\nof you that have been doing a little bit\nof thread pool stuff. How is this going to work?",
    "start": "2953200",
    "end": "2959153"
  },
  {
    "text": "You just got all these tasks,\nall these thread blocks, and you're just going\nto go woof, woof, woof to all your different workers.",
    "start": "2959153",
    "end": "2964688"
  },
  {
    "text": "And by the way, in\nCUDA, you could actually set up dependencies between\nsome of these thread blocks. But instead of what you're\ndoing in assignment 2",
    "start": "2964688",
    "end": "2973530"
  },
  {
    "text": "and writing some software that\ndecides what worker threads get what work, what you're\ndoing in assignment",
    "start": "2973530",
    "end": "2982230"
  },
  {
    "text": "2 is embedded in GPU\nhardware in Silicon. So the API that you're\nimplementing an assignment 2,",
    "start": "2982230",
    "end": "2989190"
  },
  {
    "text": "run this task this\nmany times, remember, that's the interface\nto the hardware now.",
    "start": "2989190",
    "end": "2995099"
  },
  {
    "text": "So the interface is not\nhere's a program, here's a PC, start running it on this thread.",
    "start": "2995100",
    "end": "3001109"
  },
  {
    "text": "The interface to the\nhardware is here's an instruction stream,\na program binary, please run all of these\nthread blocks of it.",
    "start": "3001110",
    "end": "3007940"
  },
  {
    "text": "And in the hardware, right at\nthe top of the GPU is something that says, I have to\nrun 8,000 thread blocks,",
    "start": "3007940",
    "end": "3013950"
  },
  {
    "text": "I've got four cores,\nlet's get to work. So this is another instance\nof this design pattern",
    "start": "3013950",
    "end": "3020420"
  },
  {
    "text": "of you declare independent work\nand you let a scheduler assign it to workers.",
    "start": "3020420",
    "end": "3026250"
  },
  {
    "text": "So let's talk a little bit\nabout those workers now. So I'm going to give you\ndiagrams for a Volta, V100.",
    "start": "3026250",
    "end": "3033930"
  },
  {
    "text": "I will, at some point\nlater in the week, once I get done with one of\nmy own personal deadlines",
    "start": "3033930",
    "end": "3039173"
  },
  {
    "text": "and once you get into the CUDA\nassignment, just for kicks, I'll go ahead and make a few\nslides on what a modern Grace",
    "start": "3039173",
    "end": "3045130"
  },
  {
    "text": "Hopper chip looks like. But it's pretty similar. Not much has changed since then.",
    "start": "3045130",
    "end": "3050170"
  },
  {
    "text": "So I want you to think about\nbasically a mini little GPU processor, a GPU core.",
    "start": "3050170",
    "end": "3056370"
  },
  {
    "text": "And it's got a fetch and\ndecode unit at the top like we're used to. And ignore all the\nrainbow colors here.",
    "start": "3056370",
    "end": "3064000"
  },
  {
    "text": "Off to the side here are 16\nALUs that have to run in a 7d fashion.",
    "start": "3064000",
    "end": "3069029"
  },
  {
    "text": "So that's, again,\nmy orange boxes. So that's the 16\nexecution units.",
    "start": "3069030",
    "end": "3074470"
  },
  {
    "text": "And then here, I have all of\nmy execution context storage.",
    "start": "3074470",
    "end": "3079750"
  },
  {
    "text": "Now I'm drawing it this\nway a little bit different for a reason is I want you\nto think about-- here's",
    "start": "3079750",
    "end": "3084960"
  },
  {
    "text": "the storage for thread zero. So r0, r1, r2, r3.",
    "start": "3084960",
    "end": "3090210"
  },
  {
    "text": "Here's the storage for thread 1,\nhere's the storage for thread 2, here's the storage\nfor thread 31,",
    "start": "3090210",
    "end": "3097230"
  },
  {
    "text": "and I'm going to keep\ngoing and going and going. So I have execution context\nfor, at least in this diagram,",
    "start": "3097230",
    "end": "3105059"
  },
  {
    "text": "128 threads. Yeah.",
    "start": "3105060",
    "end": "3110640"
  },
  {
    "text": "Heavily multithreaded chip. 128 threads now of\nexecution context.",
    "start": "3110640",
    "end": "3116250"
  },
  {
    "text": "I have no superscalar on\nwhat I've drawn so far, and I have execution units\nthat are 16 wide SIMD.",
    "start": "3116250",
    "end": "3126299"
  },
  {
    "text": "Makes sense? And the reason why I have\nother things-- this is",
    "start": "3126300",
    "end": "3131490"
  },
  {
    "text": "floating point execution SIMD. It could also do an integer\noperation SIMD it could also",
    "start": "3131490",
    "end": "3137100"
  },
  {
    "text": "do some weird, funky math\nlike an eight wide SIMD trig functions and stuff like that.",
    "start": "3137100",
    "end": "3142510"
  },
  {
    "text": "It can also do loads and stores. So just imagine\nthere's a whole bunch of different types\nof SIMD operations that the thing can run.",
    "start": "3142510",
    "end": "3150100"
  },
  {
    "text": "And this is actually\ncalled an SM sub core. That's their terminology.",
    "start": "3150100",
    "end": "3155540"
  },
  {
    "text": "So these are scalar\nregisters for one thread.",
    "start": "3155540",
    "end": "3162320"
  },
  {
    "text": "Now remember, when we\ntalked about SIMD on a CPU, we said vector registers. So these are not\nvector registers.",
    "start": "3162320",
    "end": "3168680"
  },
  {
    "text": "These are scalar registers. The execution context\nof an NVIDIA thread is a bunch of scalar registers.",
    "start": "3168680",
    "end": "3175420"
  },
  {
    "text": "Here are scalar\nregisters for thread 2. So two different\nexecution contexts.",
    "start": "3175420",
    "end": "3181300"
  },
  {
    "text": "Now here's the one\nmajor difference between how SIMD\nexecution works on CPUs",
    "start": "3181300",
    "end": "3188320"
  },
  {
    "text": "and how SIMD execution\npops up on GPUs. So on CPUs, if we had a\nthread and it ran SIMD,",
    "start": "3188320",
    "end": "3196539"
  },
  {
    "text": "the compiler generated\nSIMD instructions. On GPUs, because it's SPMD\ninterface to the hardware,",
    "start": "3196540",
    "end": "3204610"
  },
  {
    "text": "the hardware knows\nthat it's running N copies of this program.",
    "start": "3204610",
    "end": "3210290"
  },
  {
    "text": "So what it's going to do is\nthat every single thread here has its own program counter.",
    "start": "3210290",
    "end": "3215670"
  },
  {
    "text": "It's one of the registers. In the event that 32 consecutive\nthreads are on the same program",
    "start": "3215670",
    "end": "3223340"
  },
  {
    "text": "counter, that same\ninstruction is going to get executed\nin SIMD by all the GPUs.",
    "start": "3223340",
    "end": "3230569"
  },
  {
    "text": "So it's all the same idea. There's just a small\nimplementation detail difference",
    "start": "3230570",
    "end": "3235770"
  },
  {
    "text": "is that on CPUs, the compiler\ngenerates that SIMD instruction.",
    "start": "3235770",
    "end": "3241670"
  },
  {
    "text": "On GPUs, I have all\nof these CUDA threads that are all running\nthe same program. If they all happen to\nbe in the same place,",
    "start": "3241670",
    "end": "3249510"
  },
  {
    "text": "the GPU goes, let's run them\noff my SIMD unit all at once. So this is called implicit SIMD.",
    "start": "3249510",
    "end": "3256160"
  },
  {
    "text": "But at the end of the\nday, it's the same idea. And one way you\ncan think about it is you can think about it as--",
    "start": "3256160",
    "end": "3262670"
  },
  {
    "text": "up until very, very recently,\nespecially in this older GPU, the only way to run\nmultiple threads in SIMD",
    "start": "3262670",
    "end": "3270580"
  },
  {
    "text": "is if the threads\nwith consecutive IDs had the same operation.",
    "start": "3270580",
    "end": "3277338"
  },
  {
    "text": "NVIDIA would never take a\nthread from here and a thread from there and run these SIMD. So effectively, a group\nof 32 execution contexts",
    "start": "3277338",
    "end": "3286470"
  },
  {
    "text": "is kind of a CPU thread\nwith vector registers. You see how it's\nbasically the same thing.",
    "start": "3286470",
    "end": "3293079"
  },
  {
    "text": "And NVIDIA calls\nthis concept a warp. So a warp is a block of\nexecution contexts for 32 CUDA",
    "start": "3293080",
    "end": "3300720"
  },
  {
    "text": "threads that, when\nthose CUDA threads are doing the same thing, those\n32 CUDA threads actually",
    "start": "3300720",
    "end": "3307980"
  },
  {
    "text": "get SIMD-like execution\non this hardware. ",
    "start": "3307980",
    "end": "3316090"
  },
  {
    "text": "So threads in a warp are\nexecuted in a timely manner if they're at the same\npoint in the program.",
    "start": "3316090",
    "end": "3323650"
  },
  {
    "text": "And it's up to the hardware\nto identify that it's the same point in the program. They just basically compare PCs.",
    "start": "3323650",
    "end": "3329390"
  },
  {
    "text": "And if all the PCs are\nthe same across all of the threads in\nthe warp, boom, one clock SIMD instruction.",
    "start": "3329390",
    "end": "3336400"
  },
  {
    "text": "Yes. You said that [INAUDIBLE]. Yeah, OK. So I just said that\na warp is 32 threads,",
    "start": "3336400",
    "end": "3345100"
  },
  {
    "text": "and I've only got 16 SIMD ALUs. Something's weird here.",
    "start": "3345100",
    "end": "3352270"
  },
  {
    "text": "Boom, boom. So it actually will only run an\ninstruction every other clock. ",
    "start": "3352270",
    "end": "3358950"
  },
  {
    "text": "So it'll do that SIMD\ninstruction as 16 wide, 16 wide on consecutive clocks.",
    "start": "3358950",
    "end": "3365674"
  },
  {
    "text": "And you might be\nwondering, well, doesn't that mean that\ninstruction fetch and decode is just idle every other clock.",
    "start": "3365675",
    "end": "3372060"
  },
  {
    "text": "It's not because on\nthat extra clock, it actually starts\ndispatching instructions to the other things. ",
    "start": "3372060",
    "end": "3379836"
  },
  {
    "text": "So they say, we can actually\nget by with less instruction fetch and decode. We'll issue a 32 wide\noperation every clock,",
    "start": "3379836",
    "end": "3386502"
  },
  {
    "text": "but it means we don't have\nto fill that unit up again for another two clocks,\nwhich gives me some time to issue floating point now\nand integer in the next clock",
    "start": "3386502",
    "end": "3393480"
  },
  {
    "text": "and then back to floating\npoint and so on and so on. So the instructions actually\nlook a little bit like this.",
    "start": "3393480",
    "end": "3398559"
  },
  {
    "text": "This is time and clocks. This is like instruction zero,\ninstruction one, instruction two.",
    "start": "3398560",
    "end": "3404220"
  },
  {
    "text": "And so the actual\nschedule of the processor is let's dispatch\nthis warp instruction. It takes two cycles.",
    "start": "3404220",
    "end": "3410710"
  },
  {
    "text": "Let's dispatch the\nnext warp instruction. That might be I plus plus on\nall the threads that's integer.",
    "start": "3410710",
    "end": "3416050"
  },
  {
    "text": "And then two cycles later, we\ncan dispatch back to the F/P 32 unit and stuff like that. This is all really low level\nimplementation details.",
    "start": "3416050",
    "end": "3423667"
  },
  {
    "text": "It doesn't matter to the course. But I thought you\nmight find it cool. Are these schedulers intelligent\nenough so this [INAUDIBLE] is",
    "start": "3423667",
    "end": "3430359"
  },
  {
    "text": "doing instruction decode and\nfiguring out do you have N number of units and\nhaving an execution--",
    "start": "3430360",
    "end": "3437230"
  },
  {
    "text": "it has a buffer to send\nit to that particular-- Yeah, for sure. I mean, this is\nstandard, in some sense,",
    "start": "3437230",
    "end": "3442990"
  },
  {
    "text": "out of order pipeline processor. Yeah. [INAUDIBLE]  It's actually pretty simple.",
    "start": "3442990",
    "end": "3448610"
  },
  {
    "text": "It's not being that dynamic. It's just saying, when I issue\na floating point instruction, I know I can issue the\nnext one in two cycles.",
    "start": "3448610",
    "end": "3454930"
  },
  {
    "text": "It's not that hard. Yeah, that's not nearly\nCPU level sophistication. And so those different banks\nare it's floating point ALU,",
    "start": "3454930",
    "end": "3461450"
  },
  {
    "text": "it's 8-bit integer, it's 32-bit\ninteger, it's load store, it's transcendental math. So basically, that\nfetch and decode unit",
    "start": "3461450",
    "end": "3467560"
  },
  {
    "text": "is just going down the program,\nthe CUDA program, and going,",
    "start": "3467560",
    "end": "3473710"
  },
  {
    "text": "this block of 32 threads\nneeds to run a floating point, this block of 32 threads\nneeds to run integer.",
    "start": "3473710",
    "end": "3479933"
  },
  {
    "text": "So that's just how it works. But the details are\nnot super important. Yeah. [INAUDIBLE]",
    "start": "3479933",
    "end": "3485920"
  },
  {
    "text": " Every thread has its own PC.",
    "start": "3485920",
    "end": "3491620"
  },
  {
    "text": "Every thread has its own PC. But you're only\ngoing to get SIMD",
    "start": "3491620",
    "end": "3497200"
  },
  {
    "text": "if that PC is the same for\neverything in the warp. So it's effectively the same\nas if there were only four PCs,",
    "start": "3497200",
    "end": "3505089"
  },
  {
    "text": "effectively. But there are a few\nthings in NVIDIA could do with this abstraction.",
    "start": "3505090",
    "end": "3511160"
  },
  {
    "text": "They could reorganize\nthe threads to get better SIMD coherence\nfor you if they wanted to.",
    "start": "3511160",
    "end": "3517840"
  },
  {
    "text": "I think they did that recently. They only very recently,\nonly very recently, under some very\nlimited conditions,",
    "start": "3517840",
    "end": "3524870"
  },
  {
    "text": "they'll actually do that. So they're, in some\nsense, 99% of the time",
    "start": "3524870",
    "end": "3531190"
  },
  {
    "text": "behaves like a 32 wide SIMD\nmachine with four unique PCs. And you can think about\nas a warp, as basically",
    "start": "3531190",
    "end": "3538840"
  },
  {
    "text": "like a conventional thread\nwith vector instructions. But that's actually not true. There really are a different\nPC for every CUDA thread",
    "start": "3538840",
    "end": "3547150"
  },
  {
    "text": "and the chip is\ndynamically checking to make sure that the PCs are\nall the same before it actually uses the vector instruction.",
    "start": "3547150",
    "end": "3554079"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3554080",
    "end": "3559089"
  },
  {
    "text": "Sorry. In this diagram, I have\nexecution context for-- sorry, I expanded myself now to 60\nwarps worth of execution.",
    "start": "3559090",
    "end": "3570040"
  },
  {
    "text": "That's a lot of execution. But that's actually true. But let me just go\nback to this diagram.",
    "start": "3570040",
    "end": "3578060"
  },
  {
    "text": "This will be better. Hold on. Oh, shoot. There's a reason\nwhy I'm doing that. I'll tell you in one second.",
    "start": "3578060",
    "end": "3583480"
  },
  {
    "text": "But really, there's\n16 of these warps. So there's 16 times 32\nexecution context here,",
    "start": "3583480",
    "end": "3593480"
  },
  {
    "text": "and that's how\nmany PCs there are. But there can only be one\nof those uniquely executing",
    "start": "3593480",
    "end": "3600210"
  },
  {
    "text": "at once. Yeah. So a warp is like a\ncollection of threads. A warp is a hardware\nimplementation detail.",
    "start": "3600210",
    "end": "3607000"
  },
  {
    "text": "It's a collection of threads. So how is a warp different\nfrom a thread block",
    "start": "3607000",
    "end": "3612670"
  },
  {
    "text": "or is there any relationship-- Because a thread block is\npart of the programming model. A warp, you never see.",
    "start": "3612670",
    "end": "3618210"
  },
  {
    "text": "All you saw is you\ncreated 128 CUDA threads, and NVIDIA is going to use\nfour warps of execution context",
    "start": "3618210",
    "end": "3623910"
  },
  {
    "text": "to actually run it. Let me put it all together. Again, we're not going to do\nany of this stuff on the exam.",
    "start": "3623910",
    "end": "3629110"
  },
  {
    "text": "I just think it's kind of fun to\nnerd out on this a little bit. The actual core of\nthis chip is this thing",
    "start": "3629110",
    "end": "3635790"
  },
  {
    "text": "replicated four times. So now this is why\nI was numbering",
    "start": "3635790",
    "end": "3641220"
  },
  {
    "text": "my execution context this way. So there's actually six--\nthis core, this big core",
    "start": "3641220",
    "end": "3647579"
  },
  {
    "text": "actually has 64 warps\nworth of execution context. So that's 2000 CUDA\nthreads, 64 times 32.",
    "start": "3647580",
    "end": "3657280"
  },
  {
    "text": "Now it's got four different\nbanks of these vector ALUs",
    "start": "3657280",
    "end": "3662350"
  },
  {
    "text": "and it's got four different\ninstruction fetching decoders. So this is all one thing.",
    "start": "3662350",
    "end": "3668319"
  },
  {
    "text": "And it's got like shared\nmemory attached to all of this. So the way it\nactually works, you",
    "start": "3668320",
    "end": "3673680"
  },
  {
    "text": "could think about it as we\nhave superscalar four wide and every clock,\nevery fetch and decode",
    "start": "3673680",
    "end": "3681730"
  },
  {
    "text": "tries to find a warp\nto run and runs it. In practice, it's a\nlittle bit limited. That fetch and\ndecode is only going",
    "start": "3681730",
    "end": "3688299"
  },
  {
    "text": "to pick amongst these warps\nand this fetch and decode is only going to pick\namongst these warps. So you can think about this\nalmost as four separate cores,",
    "start": "3688300",
    "end": "3696380"
  },
  {
    "text": "but it's actually not true\nbecause all these execution contexts share the same\nshared memory storage.",
    "start": "3696380",
    "end": "3702910"
  },
  {
    "text": "So you could create\na thread block on this chip to your\nquestion earlier that has 2000 CUDA threads\nand has 128 of shared storage,",
    "start": "3702910",
    "end": "3712940"
  },
  {
    "text": "and that would\noccupy this thing. And then this core,\nwhich is called an SM, would actually do 2000 way multi\nthreading of those 2000 CUDA",
    "start": "3712940",
    "end": "3721880"
  },
  {
    "text": "threads. Every clock, it could\nactually make progress on up to four warps that it\nselects from any of the 64 warps",
    "start": "3721880",
    "end": "3729800"
  },
  {
    "text": "that are available to it. There's a ton of\nlatency hiding here. But notice, there's\nno new concept here.",
    "start": "3729800",
    "end": "3736400"
  },
  {
    "text": "It's multi-threading,\nSIMD, and actually simultaneous multithreading\nand superscalar",
    "start": "3736400",
    "end": "3744350"
  },
  {
    "text": "coming from the\ninstruction infections. Yeah. Sorry. Just to clarify,\nyou said [INAUDIBLE]",
    "start": "3744350",
    "end": "3750920"
  },
  {
    "text": "so it could be 16\nat a time, right? No, no, just think about\nonly what's on the slide.",
    "start": "3750920",
    "end": "3756830"
  },
  {
    "text": "Every fetch and decode is\ngoing to choose a warp to run. So I can only run\nfour ropes at a time.",
    "start": "3756830",
    "end": "3763859"
  },
  {
    "text": "Or any one clock, I'm\ndispatching four warps. Maybe one warp\nruns here, one warp runs here, one warp once\nhere, one warp runs here.",
    "start": "3763860",
    "end": "3771920"
  },
  {
    "text": "So I can run, I\ncan make progress on four warps at a time, but\nI've got 64 to choose from.",
    "start": "3771920",
    "end": "3779119"
  },
  {
    "text": "It almost be like a processor\nthat can run instructions like an Intel processor\ncould run vector instructions",
    "start": "3779120",
    "end": "3784730"
  },
  {
    "text": "from four threads at once\nand it had 64 hyper threads to choose from. So you're not running\nthe entire warp, right,",
    "start": "3784730",
    "end": "3791660"
  },
  {
    "text": "because there might be some\nthreads with the same program [INAUDIBLE]. Yeah, I'm selecting\na warp and I'm",
    "start": "3791660",
    "end": "3799310"
  },
  {
    "text": "finding all the threads on\nthe same PC and running it. Otherwise, I'm masking. Same SIMD mask stuff.",
    "start": "3799310",
    "end": "3805780"
  },
  {
    "text": "Yes, you're right. Yeah. So you keep saying PC. What is PC? Sorry. PC is just there's one register\nhere in any program, which",
    "start": "3805780",
    "end": "3814130"
  },
  {
    "text": "is the program counter. And that's like the pointer to\nthe next instruction to run. So the way to think about\nit is you look around",
    "start": "3814130",
    "end": "3820248"
  },
  {
    "text": "and you say, OK, this warp,\nall of my different threads are all at the same\npoint in the program,",
    "start": "3820248",
    "end": "3825590"
  },
  {
    "text": "and they're all runnable. Let's go dispatch this warp and\nuse the SIMD alias to do it. Yeah, exactly.",
    "start": "3825590",
    "end": "3832488"
  },
  {
    "text": "I want to move on because\nI don't want to get too in the weeds for too long here. I'm just a little confused.",
    "start": "3832488",
    "end": "3838010"
  },
  {
    "text": "What's the advantage\nof not having a SIMD and having swap one set? You can run in SIMD,\nbut not always.",
    "start": "3838010",
    "end": "3845810"
  },
  {
    "text": "Well, it behaves exactly\nlike the SIMD that you know. If all the threads in a warp\ndon't have the same program",
    "start": "3845810",
    "end": "3852500"
  },
  {
    "text": "counter, we're just going\nto run all the ones that do, and then we'll run\nanother instruction that runs the other program counter.",
    "start": "3852500",
    "end": "3858930"
  },
  {
    "text": "So it'll behave just like you're\nmasking from assignment 2. The only difference\nnow is the hardware",
    "start": "3858930",
    "end": "3864860"
  },
  {
    "text": "is figuring that out for you. You are not asserting it at\ncompile time what runs together.",
    "start": "3864860",
    "end": "3870510"
  },
  {
    "text": "So NVIDIA is leaving themselves\nthe option to, in a future chip,",
    "start": "3870510",
    "end": "3875690"
  },
  {
    "text": "change the SIMD width to 64. If Intel changes\ntheir SIMD width,",
    "start": "3875690",
    "end": "3880830"
  },
  {
    "text": "you have to recompile your\ncode because your binaries say, this is an AVX instruction\nwith SIMD width eight.",
    "start": "3880830",
    "end": "3887250"
  },
  {
    "text": "They want to allow\nthemselves to change that SIMD width in some\nfuture chip and your code will still run because it's\nall just scalar instructions.",
    "start": "3887250",
    "end": "3895680"
  },
  {
    "text": "They want the ability to reorder\nthe operations to improve",
    "start": "3895680",
    "end": "3900710"
  },
  {
    "text": "coherence for you under the\nhood if you have divergence. And they do a little\nbit of that sometimes.",
    "start": "3900710",
    "end": "3906240"
  },
  {
    "text": "But for most code,\nif you just think about it like straightforward\nSIMD that you know, you will have a great model of\nhow it works on a modern GPU.",
    "start": "3906240",
    "end": "3914569"
  },
  {
    "text": "Yeah. You were talking\nabout the user being able to use the block size. But let's say across\nNVIDIA architectures,",
    "start": "3914570",
    "end": "3922250"
  },
  {
    "text": "they make some changes. If the user thinks-- Correct. If you put something\nin your program,",
    "start": "3922250",
    "end": "3927300"
  },
  {
    "text": "you are asserting that the\nhardware has to handle it. So let's say on this\nparticular processor,",
    "start": "3927300",
    "end": "3933690"
  },
  {
    "text": "you could create a block\nsize of 2048 threads because I can fit\n2048 CUDA threads here",
    "start": "3933690",
    "end": "3939530"
  },
  {
    "text": "and they can all have\naccess to the same shared memory in the block. Earlier GPUs had a maximum\nnumber of execution contexts",
    "start": "3939530",
    "end": "3946640"
  },
  {
    "text": "per SM core of 256. So you could not have\ncompiled time fail.",
    "start": "3946640",
    "end": "3952920"
  },
  {
    "text": "You cannot run this program on\nthis GPU because your program needs 2000 threads per block,\nand this GPU can only support",
    "start": "3952920",
    "end": "3961020"
  },
  {
    "text": "256. [INAUDIBLE] the program,\nlet's say you wrote it",
    "start": "3961020",
    "end": "3966150"
  },
  {
    "text": "for the previous architecture,\nit's not going to scale the way you just-- Correct. So if I wrote a program that\nused 256 threads per block,",
    "start": "3966150",
    "end": "3976860"
  },
  {
    "text": "and then NVIDIA comes out with\na new GPU that supports 2000, I could change my program or\nI can just keep it the same",
    "start": "3976860",
    "end": "3986190"
  },
  {
    "text": "and NVIDIA might be able to\nrun four blocks per core. Yeah, exactly.",
    "start": "3986190",
    "end": "3991329"
  },
  {
    "text": "So yes, whenever you\nstart manually scheduling something yourself,\nyou're making assumptions",
    "start": "3991330",
    "end": "3997230"
  },
  {
    "text": "about what the hardware can do. And that's what\nNVIDIA is saying here is we prefer you not to\nmanually schedule the SIMD.",
    "start": "3997230",
    "end": "4004670"
  },
  {
    "text": "Let us do it because we\nmight change the block size. We might want to\nreorder things for you. And we'll do it better\nthan you will sometimes.",
    "start": "4004670",
    "end": "4012859"
  },
  {
    "text": "Yeah, cool. So to wrap this section up,\nlet's go back to this code.",
    "start": "4012860",
    "end": "4019510"
  },
  {
    "text": "Remember, this is something\nthat has 128 threads per block and needs 512 bytes\nof storage to run.",
    "start": "4019510",
    "end": "4025960"
  },
  {
    "text": "So when CUDA says, I need\nto run this thing on a core, it will take this thread\nblock, allocate 512 bytes out",
    "start": "4025960",
    "end": "4033810"
  },
  {
    "text": "of the shared memory\nhere, and allocate how many execution context or\nhow many execution contexts?",
    "start": "4033810",
    "end": "4041369"
  },
  {
    "text": "128 threads which\nactually, here, ends up being four warps\nworth of execution context.",
    "start": "4041370",
    "end": "4047760"
  },
  {
    "text": "And I intelligently allocated\nthem across the slices so that they can\nrun concurrently. Now this diagram here,\nwhat you're seeing",
    "start": "4047760",
    "end": "4055920"
  },
  {
    "text": "is what NVIDIA calls the\nstreaming multiprocessor, the SM.",
    "start": "4055920",
    "end": "4061900"
  },
  {
    "text": "And this is the block that's\nreplicated 80 times on the V100.",
    "start": "4061900",
    "end": "4070079"
  },
  {
    "text": "And so if you just do the\nmath, there's a 1.2 GHz clock, I think, at normal clock speed.",
    "start": "4070080",
    "end": "4075160"
  },
  {
    "text": "There's 80 of these\nthings per chip. Each of these 80 has\nthese four columns,",
    "start": "4075160",
    "end": "4080890"
  },
  {
    "text": "and those columns\nhave 16 wide vector ALUs for floating point math. So you multiply\nall those together,",
    "start": "4080890",
    "end": "4087840"
  },
  {
    "text": "that's 12.7 teraflops of\nperformance for floating point math. And if you do 80 times 64 warps\nper SM times 32 threads per",
    "start": "4087840",
    "end": "4100140"
  },
  {
    "text": "warp, that's 163,000 concurrent\nlive CUDA threads on the chip at once.",
    "start": "4100140",
    "end": "4105310"
  },
  {
    "text": "So your Intel processor has\nsupport for two hyperthreads. This thing has support\nfor 163,000 CUDA threads.",
    "start": "4105310",
    "end": "4113939"
  },
  {
    "text": "They can't all execute\nat the same time, but they're all there on\nthe chip at the same time. Yes.",
    "start": "4113939",
    "end": "4119939"
  },
  {
    "text": "If you were to say you mentioned\nthat each thread's memory will get allocated in that\nshared memory, right?",
    "start": "4119939",
    "end": "4125670"
  },
  {
    "text": "Those shared variables. So remember, this program had\nthat shared variable, shared",
    "start": "4125670",
    "end": "4130960"
  },
  {
    "text": "float support says\nthis thread block. So the shared\nmemory is the place where you start asserting\nactual scheduling",
    "start": "4130960",
    "end": "4138310"
  },
  {
    "text": "details as the programmer. So there's a compromise. Like CUDA said, you should just\ntell us how many blocks to run",
    "start": "4138310",
    "end": "4144640"
  },
  {
    "text": "and we'll run them\nhowever we wish. But they said, memory\nis super important. So we're going to allow\nyou to assert that I",
    "start": "4144640",
    "end": "4151899"
  },
  {
    "text": "need 512 bytes of storage. And what NVIDIA is going to\ndo, or we're getting to that,",
    "start": "4151899",
    "end": "4158560"
  },
  {
    "text": "is if we run this thing,\nto run a thread block, it's going to choose\nwarps execution context",
    "start": "4158560",
    "end": "4164710"
  },
  {
    "text": "on the same core processor. It's going to allocate that\nstorage so all those execution contexts can read\nand write from it.",
    "start": "4164710",
    "end": "4171589"
  },
  {
    "text": "And since all these threads are\nkind of there in the same core, things like a barrier between\nthem will be very fast.",
    "start": "4171590",
    "end": "4177890"
  },
  {
    "text": "Or as if I happen to\ntake some of the threads from this thread block and\nput them somewhere else",
    "start": "4177890",
    "end": "4183818"
  },
  {
    "text": "on this machine\nover here, imagine a barrier would be communication\nacross all of these things.",
    "start": "4183819",
    "end": "4189219"
  },
  {
    "text": "So the thread block\nis the granularity of synchronization\nand data locality when you're writing a program.",
    "start": "4189220",
    "end": "4194956"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4194956",
    "end": "4203920"
  },
  {
    "text": "And I'm giving you a\nsimplistic version of CUDA. If you go to the modern CUDA\ndocs, they're going to say, sure, can call malloc\non shared memory also.",
    "start": "4203920",
    "end": "4211820"
  },
  {
    "text": "But then there's\ngoing to be some-- it gets harder. So I want to give the\nsimple version first.",
    "start": "4211820",
    "end": "4219010"
  },
  {
    "text": "So let's talk about\nrunning this thing. We write a program. We have all of\nthese thread blocks.",
    "start": "4219010",
    "end": "4224780"
  },
  {
    "text": "We need 512 bytes of storage. And let's imagine a very simple\nGPU, not the thing I just showed you, but a\nvery simple GPU which",
    "start": "4224780",
    "end": "4231880"
  },
  {
    "text": "can run one\ninstruction per clock, actually has 32 wide SIMD--\nso I drew it out-- and has",
    "start": "4231880",
    "end": "4237340"
  },
  {
    "text": "execution context per SM\ncore for 384 CUDA threads,",
    "start": "4237340",
    "end": "4243050"
  },
  {
    "text": "which is 12 warps. So what do you think\nis going to happen? So we run this thing.",
    "start": "4243050",
    "end": "4248305"
  },
  {
    "text": "We say, OK, we need\n8,000 thread blocks. You're implementing thread\nqueues or thread pools.",
    "start": "4248305",
    "end": "4253350"
  },
  {
    "text": "You know how it works. You're going to say, I\nneed to execute this task, this convolve function,\nmy arguments are this,",
    "start": "4253350",
    "end": "4259200"
  },
  {
    "text": "and the number of\nblocks are 1,000. And my GPU work scheduler,\nprobably very much",
    "start": "4259200",
    "end": "4267510"
  },
  {
    "text": "like your implementation\nsays, well, first thing I'm going to do is I'm going\nto allocate 128 execution contexts over here, I'm going\nto allocate 512 bytes of shared",
    "start": "4267510",
    "end": "4274889"
  },
  {
    "text": "memory, and I'm going to\nincrement my next pointer to 1 because I've already dispatched\none of these thread blocks.",
    "start": "4274890",
    "end": "4282180"
  },
  {
    "text": "What happens next?  Schedule more work?",
    "start": "4282180",
    "end": "4287670"
  },
  {
    "text": "Yeah. Absolutely. So where do we put it? [INAUDIBLE] Probably should put\nit on the other core.",
    "start": "4287670",
    "end": "4292898"
  },
  {
    "text": "So the next thread block\ncomes out, we execute it, or we allocate\n128 thread blocks. We grab 512 bytes of storage.",
    "start": "4292898",
    "end": "4299530"
  },
  {
    "text": "We're good. Now, the next pointer of\nthe next thread block is 2.",
    "start": "4299530",
    "end": "4305820"
  },
  {
    "text": "Can we keep going? Yeah, we can. Let's schedule the next block. So now notice multiple\nblocks are being scheduled",
    "start": "4305820",
    "end": "4311580"
  },
  {
    "text": "on the same processor. I've got a ton of\nexecution context. So now I grabbed\nanother 128 execution,",
    "start": "4311580",
    "end": "4317290"
  },
  {
    "text": "I grabbed another 512 bytes. Let me go ahead and\nfill this thing up with the fourth thread block.",
    "start": "4317290",
    "end": "4322958"
  },
  {
    "text": "And now the fifth thread block\ncannot be scheduled right now. I do not have the resources.",
    "start": "4322958",
    "end": "4329350"
  },
  {
    "text": "Why do I not-- I actually have the\nexecution context, but I actually didn't\nhave the shared memory.",
    "start": "4329350",
    "end": "4335430"
  },
  {
    "text": "In this example, I only\nhad 1.5k of shared memory. So NVIDIA is going to\nnot schedule things",
    "start": "4335430",
    "end": "4341670"
  },
  {
    "text": "that it knows it can't fit. So we'll just wait. And at some point, one of\nthose thread blocks finishes.",
    "start": "4341670",
    "end": "4348850"
  },
  {
    "text": "When that thread block finishes,\nsome resources are available. And now let's schedule\nthread block four.",
    "start": "4348850",
    "end": "4355420"
  },
  {
    "text": "And now notice that\nsince this is SPMD and all of these\nthread blocks use the same amount of resources,\nit's pretty easy to just,",
    "start": "4355420",
    "end": "4362350"
  },
  {
    "text": "whenever someone finishes,\nthe exact amount of resources are free to start a new thing. It's very homogeneous\nresource requirements.",
    "start": "4362350",
    "end": "4369200"
  },
  {
    "text": "You don't have to solve\nan allocation problem or anything like that. So that's why it's nice to\nknow the resources up front.",
    "start": "4369200",
    "end": "4375400"
  },
  {
    "text": "So things will just keep going\nand going and going and going and going.",
    "start": "4375400",
    "end": "4380690"
  },
  {
    "text": "And that's what the\nNVIDIA GPU is going to do. So let's check our\nunderstanding just a little bit",
    "start": "4380690",
    "end": "4386890"
  },
  {
    "text": "in the last four or\nfive minutes of class. So if you understand\nthe next three slides, you are really\nthinking through step",
    "start": "4386890",
    "end": "4394449"
  },
  {
    "text": "by step-- you're in\nyour head going through, I can just play\nout in my head what the implementation is doing.",
    "start": "4394450",
    "end": "4400330"
  },
  {
    "text": "So here's an example. So now I made a core and\nthis is not the full thing.",
    "start": "4400330",
    "end": "4407480"
  },
  {
    "text": "Notice that let's just say it\nonly has 128 execution contexts. So four warps here.",
    "start": "4407480",
    "end": "4413030"
  },
  {
    "text": "I made a tiny little core. And I have my thread block\nwith 256 CUDA threads.",
    "start": "4413030",
    "end": "4420670"
  },
  {
    "text": "So I have a core that only\nhas execution context for 128. I have a program\nthat I wrote 256.",
    "start": "4420670",
    "end": "4433560"
  },
  {
    "text": "So NVIDIA is going to say,\nsorry, you can't run this. But why can't it? Why it just run the first 128\nthreads and the thread block,",
    "start": "4433560",
    "end": "4442420"
  },
  {
    "text": "wait till they finish, and\nthen run the next 128 threads? I think it's because\nthe programmer has",
    "start": "4442420",
    "end": "4448800"
  },
  {
    "text": "specified that the data\nlocality is [INAUDIBLE]. ",
    "start": "4448800",
    "end": "4457060"
  },
  {
    "text": "And that's true because that's\nthe rules of the programming model. But why are the rules of the\nprogramming model this way?",
    "start": "4457060",
    "end": "4463020"
  },
  {
    "text": "Why don't they just\nsay, you can ask for as many threads per block\nas you want, and we're just going to run them,\nand we're going",
    "start": "4463020",
    "end": "4469005"
  },
  {
    "text": "to run the first half of\nthe threads to completion, then we'll run the next half? I'm still going to give\nyou your shared memory.",
    "start": "4469005",
    "end": "4475620"
  },
  {
    "text": "Because it's based on what\nyou know about the programs that are running more\nefficiently like that.",
    "start": "4475620",
    "end": "4482310"
  },
  {
    "text": "That's true. I mean, what you're saying is\nthat if the programmer told you to do something, it's\ngood in a system to do it.",
    "start": "4482310",
    "end": "4487960"
  },
  {
    "text": "But there are a lot of times\nwhen the underlying system implementer says, I\nthink I know better. As long as I can do it\ncorrectly, I'm fine.",
    "start": "4487960",
    "end": "4494395"
  },
  {
    "text": " So look at this program. It has a barrier in it.",
    "start": "4494395",
    "end": "4500710"
  },
  {
    "text": "So that barrier says\nthat we will not proceed until all threads get here.",
    "start": "4500710",
    "end": "4505929"
  },
  {
    "text": "What if we ran 128 threads,\nthey stalled at that barrier, they're still having\nthose execution contexts?",
    "start": "4505930",
    "end": "4514840"
  },
  {
    "text": "And what happens? Nothing. They never yield them. And so the other threads\nnever get a chance to run.",
    "start": "4514840",
    "end": "4521600"
  },
  {
    "text": "So unless you start\nthinking about more advanced implementations where we're\ngoing to preempt these threads,",
    "start": "4521600",
    "end": "4528380"
  },
  {
    "text": "rip them off the processor,\nput the other ones on, which would be highly not\nperformant, this program",
    "start": "4528380",
    "end": "4534670"
  },
  {
    "text": "is going to deadlock if I try\nand run it on this machine. So that's why NVIDIA\nsays, look, we're",
    "start": "4534670",
    "end": "4539980"
  },
  {
    "text": "a high performance\nprogramming system. If the programmer\ndecided to do something, we need to respect their wishes.",
    "start": "4539980",
    "end": "4546200"
  },
  {
    "text": "We don't want to do\ncrazy things that might impact performance\nin a bad way just",
    "start": "4546200",
    "end": "4551349"
  },
  {
    "text": "to get the program\nto run correctly. So standard CUDA you\nshould think about it",
    "start": "4551350",
    "end": "4556840"
  },
  {
    "text": "as no preemption and things that\nwould be magical are not done.",
    "start": "4556840",
    "end": "4562580"
  },
  {
    "text": "And so this is why CUDA\nwill reject this program. The idea is that the\nthreads in a thread block are not just SPMD threads,\nbut running concurrently",
    "start": "4562580",
    "end": "4570130"
  },
  {
    "text": "at the same time, live at\nthe same time on the same SM because you may do\nthings like want",
    "start": "4570130",
    "end": "4576040"
  },
  {
    "text": "them to cooperate with barriers\nor atomics and stuff like that. So this thing is busted.",
    "start": "4576040",
    "end": "4582720"
  },
  {
    "text": "Two more things. So this is just a review slide. Let's think about this one. Let's think about a CUDA program\nthat creates a histogram.",
    "start": "4582720",
    "end": "4589700"
  },
  {
    "text": "I create a bunch of\nthreads and all of them, in whatever thread\nblocks they're in,",
    "start": "4589700",
    "end": "4595660"
  },
  {
    "text": "just compute some value\nand then increment a bin. And CUDA has the ability\nto have atomic operations",
    "start": "4595660",
    "end": "4603670"
  },
  {
    "text": "like atomic add. So imagine all of\nus are CUDA threads. We all grab a variable,\ncompute some function, say,",
    "start": "4603670",
    "end": "4609700"
  },
  {
    "text": "I need to increment bin 42. So you go to bin 42 and\ndo an atomic safe update.",
    "start": "4609700",
    "end": "4615264"
  },
  {
    "text": " Is this a valid CUDA program? ",
    "start": "4615265",
    "end": "4625880"
  },
  {
    "text": "I have threads in\ndifferent thread blocks touching the same memory.",
    "start": "4625880",
    "end": "4630980"
  },
  {
    "text": "[INAUDIBLE]  Well, imagine that\nthis atomic add",
    "start": "4630980",
    "end": "4636560"
  },
  {
    "text": "is atomic out to global memory.  This is OK.",
    "start": "4636560",
    "end": "4642780"
  },
  {
    "text": "I've not done nothing here\nthat makes any assumptions at all about how CUDA\nruns these thread blocks.",
    "start": "4642780",
    "end": "4648480"
  },
  {
    "text": "It's just going to\nrun them whenever it wants and they might contend\nfor an atomic, but so what?",
    "start": "4648480",
    "end": "4654870"
  },
  {
    "text": "So this is OK. This is OK. I'm treating the thread\nblock as I'm just going to launch as many\nas I need and execute them",
    "start": "4654870",
    "end": "4661110"
  },
  {
    "text": "in whatever order you want. But now think about this one. Imagine a piece of code where\nI launch two thread blocks",
    "start": "4661110",
    "end": "4668400"
  },
  {
    "text": "and one of them says, OK,\nupdate this variable in memory,",
    "start": "4668400",
    "end": "4674010"
  },
  {
    "text": "and on the other\nthread block says, wait until that\nvariable is updated.",
    "start": "4674010",
    "end": "4679560"
  },
  {
    "text": "So one thread block\nsays, I did something, the other thread\nblock waits for it. Imagine running\nthis code on a GPU",
    "start": "4679560",
    "end": "4687809"
  },
  {
    "text": "that only can run one\nthread block at a time. If it runs thread\nblock one first,",
    "start": "4687810",
    "end": "4693810"
  },
  {
    "text": "thread block 0 never gets\nto go and thread block 1 is waiting all the time. So you see how\nthere's a difference?",
    "start": "4693810",
    "end": "4699480"
  },
  {
    "text": "Things communication\nis possible, but you cannot make assumptions\nabout the order in which stuff",
    "start": "4699480",
    "end": "4704820"
  },
  {
    "text": "is done. So thread blocks need to be-- they can interact, but they\ncannot make assumptions about",
    "start": "4704820",
    "end": "4711520"
  },
  {
    "text": "order, whereas threads\ninside a thread block, you can assume that they're\nall running at the same time,",
    "start": "4711520",
    "end": "4716810"
  },
  {
    "text": "and you can use barriers\nand stuff like that. OK, I should stop here\nso you guys can go. But feel free to\nask any questions.",
    "start": "4716810",
    "end": "4722340"
  },
  {
    "start": "4722340",
    "end": "4727000"
  }
]