[
  {
    "start": "0",
    "end": "190000"
  },
  {
    "text": "my name is Alex I'm here with Edie who's in the back and will join us in the",
    "start": "10820",
    "end": "16080"
  },
  {
    "text": "middle of the talk we're both from Google brain the SAR team as you joint work with Tim Frasca who's a professor",
    "start": "16080",
    "end": "22050"
  },
  {
    "text": "at MIT Jeff Dean and alkies Poly's Odie's for both also at Google and this",
    "start": "22050",
    "end": "28230"
  },
  {
    "text": "work really is about database systems in particular but I think this has interesting implications for any system",
    "start": "28230",
    "end": "33570"
  },
  {
    "text": "that sort of is running on data doing operations over data and really for all",
    "start": "33570",
    "end": "38760"
  },
  {
    "text": "of these they're these sort of fundamental building blocks that we've all learned about and sort of rely upon",
    "start": "38760",
    "end": "44100"
  },
  {
    "text": "in any of these systems everything from sorting and joins is sort of key",
    "start": "44100",
    "end": "49649"
  },
  {
    "text": "operations it's a data structures like beech trees and bloom filters that are well studied for decades now one thing I",
    "start": "49649",
    "end": "59909"
  },
  {
    "text": "found interesting as a more ml data mining person coming into sort of the world of infrastructure is the degree to",
    "start": "59909",
    "end": "65850"
  },
  {
    "text": "which infrastructure is really built as a this is one size fits all we should have one PJ instructor that really",
    "start": "65850",
    "end": "71909"
  },
  {
    "text": "should work for a huge number of applications and this essentially is",
    "start": "71909",
    "end": "77369"
  },
  {
    "text": "interesting Lee in contrast I think to the ways in which much of machine learning focuses a lot of attention on",
    "start": "77369",
    "end": "83579"
  },
  {
    "text": "individual applications and to defect of",
    "start": "83579",
    "end": "94469"
  },
  {
    "text": "yeah we cited a DML side you know I was",
    "start": "94469",
    "end": "100649"
  },
  {
    "text": "not I came in an 89",
    "start": "100649",
    "end": "109039"
  },
  {
    "text": "sorry soapy trees obviously I've been well-studied to give some background of I'm guessing us when the room or do you",
    "start": "115230",
    "end": "120850"
  },
  {
    "text": "know this you say you have this huge array of data right there's a bunch of keys when some payload over that data",
    "start": "120850",
    "end": "126940"
  },
  {
    "text": "that we're trying to do lookups on and we build this tree with some branching factor that will help us efficiently go",
    "start": "126940",
    "end": "132400"
  },
  {
    "text": "from the root node to find that data and then you can do a range query data sorted so you find the starting point in",
    "start": "132400",
    "end": "138490"
  },
  {
    "text": "the range and you walk through this array now this is a good idea general",
    "start": "138490",
    "end": "143530"
  },
  {
    "text": "purpose it has nice properties in terms of cache efficiency tons of useful things what happens if your data though",
    "start": "143530",
    "end": "150280"
  },
  {
    "text": "is all integers from 0 to 1 million you",
    "start": "150280",
    "end": "155320"
  },
  {
    "text": "can see this here done on the slide at 0 to 480 right so in this case you really",
    "start": "155320",
    "end": "160840"
  },
  {
    "text": "need the tree structure I mean the tree will work just fine but it's kind of besides the point right you can just do a direct look up",
    "start": "160840",
    "end": "167530"
  },
  {
    "text": "with the key if it was from a hundred to a hundred million hundred as the range",
    "start": "167530",
    "end": "174400"
  },
  {
    "text": "you could do lookup key minus 100 right so for pretty simple patterns we can see",
    "start": "174400",
    "end": "179500"
  },
  {
    "text": "obviously that the B tree is overkill can do it in oh of one lookup time of",
    "start": "179500",
    "end": "184840"
  },
  {
    "text": "one memory in those cases these corner cases so this one-size-fits-all thing",
    "start": "184840",
    "end": "190150"
  },
  {
    "start": "190000",
    "end": "280000"
  },
  {
    "text": "while it works it's also overkill right it's not really the best fit on the flip",
    "start": "190150",
    "end": "196510"
  },
  {
    "text": "side we don't want to go and check every time we build a data set does it meet one of these sort of properties we",
    "start": "196510",
    "end": "203170"
  },
  {
    "text": "expect and therefore we can use as other data base and the cool thing is machine learning lets us think about this over",
    "start": "203170",
    "end": "210280"
  },
  {
    "text": "distributions more generally kind of abstract away that concept of consecutive integers and say even for",
    "start": "210280",
    "end": "215380"
  },
  {
    "text": "these other patterns that may not be quite as simple we can still aim to do",
    "start": "215380",
    "end": "220510"
  },
  {
    "text": "that same thing the kind of simplified lookup the pattern and this is the key",
    "start": "220510",
    "end": "226630"
  },
  {
    "text": "insight it's the switch from thinking about this as sort of the data as a black a black box that we're operating",
    "start": "226630",
    "end": "231640"
  },
  {
    "text": "over and we think about scaling in terms of just how many items are in that data",
    "start": "231640",
    "end": "237160"
  },
  {
    "text": "set to thinking about the data distribution right how noisy is the state of distribution or how complicated",
    "start": "237160",
    "end": "242709"
  },
  {
    "text": "the distribution and not just the size and this can actually escaped me a change the ways in which we design these",
    "start": "242709",
    "end": "249340"
  },
  {
    "text": "algorithms so I really think honestly",
    "start": "249340",
    "end": "255430"
  },
  {
    "text": "this has implications for tons of things and we since we we started working on it and we increasingly find new places",
    "start": "255430",
    "end": "261430"
  },
  {
    "text": "where this can affect the way we think about the design but we're gonna go through a couple cases and really be",
    "start": "261430",
    "end": "266470"
  },
  {
    "text": "trees in a ton of detail i'm hashmaps and blue filters in some detail and we can sort of opine on on where it has",
    "start": "266470",
    "end": "274030"
  },
  {
    "text": "effects for things like sorting and joins but start with boot b-trees as an",
    "start": "274030",
    "end": "279550"
  },
  {
    "text": "example right so as I mentioned this is this tree structure and the real reason this is a huge benefit over things like",
    "start": "279550",
    "end": "285910"
  },
  {
    "start": "280000",
    "end": "435000"
  },
  {
    "text": "binary search in practice because it matches well the actual systems that",
    "start": "285910",
    "end": "291490"
  },
  {
    "text": "these databases are run on right there is a cache hierarchy and B trees are well-suited for that and we can tune",
    "start": "291490",
    "end": "297190"
  },
  {
    "text": "things like the branching factor to match page sizes etc but sorry for that",
    "start": "297190",
    "end": "304830"
  },
  {
    "text": "looping thing but we see a beech tree really just this input/output property right they see the input the key and",
    "start": "304830",
    "end": "311050"
  },
  {
    "text": "outputs really a page right in practice these are usually store on disk so here's this page it lies on this page",
    "start": "311050",
    "end": "317800"
  },
  {
    "text": "and then usually we do some search within the page to find exactly where in that page the the key the record lies so",
    "start": "317800",
    "end": "325930"
  },
  {
    "text": "abstracting further we can say this is a general mapping right key to position and then within then we know that the",
    "start": "325930",
    "end": "333010"
  },
  {
    "text": "record with that key lies in the range from position to position Plus page size",
    "start": "333010",
    "end": "338130"
  },
  {
    "text": "if we abstract one more step we see this can be pretty loose we just need some",
    "start": "338130",
    "end": "344950"
  },
  {
    "text": "function to magically go from key to position and at least as long that positions pretty close and we kind of",
    "start": "344950",
    "end": "351850"
  },
  {
    "text": "know how close it is we can say at the worst that's overshooting by this much and the worse that's undershooting by",
    "start": "351850",
    "end": "357490"
  },
  {
    "text": "this much then we can search in that range and find the exact right record",
    "start": "357490",
    "end": "364228"
  },
  {
    "text": "and one cool thing that came out once you frame it this way is that you find that what the beech tree is really doing",
    "start": "365280",
    "end": "370600"
  },
  {
    "text": "its building a model of the cumulative distribution function right so if your data is a sample from a Gaussian as here",
    "start": "370600",
    "end": "376960"
  },
  {
    "text": "you find that most of your data will come in this or narrow range in the middle and as a result it will sort of allocate more",
    "start": "376960",
    "end": "382780"
  },
  {
    "text": "parts of the tree there to understand that but the cool thing is people in ml and statistics have been studying CDF's",
    "start": "382780",
    "end": "388270"
  },
  {
    "text": "for a long time we can leverage a lot of that understanding as interesting properties like a skew monotonic and",
    "start": "388270",
    "end": "393880"
  },
  {
    "text": "other constraints on the modeling but the point is this is a pretty general purpose model that really just at the",
    "start": "393880",
    "end": "399370"
  },
  {
    "text": "core describing what's the distribution of your data where is it coming from how does it look and so we had this idea and",
    "start": "399370",
    "end": "408370"
  },
  {
    "text": "we thought okay this seems like a cool new thing what do we do with this right and there's all these reasons we were",
    "start": "408370",
    "end": "413440"
  },
  {
    "text": "excited they should be much smaller models are tiny by comparison to be trees right maybe it'll be faster if you",
    "start": "413440",
    "end": "419380"
  },
  {
    "text": "can do folding point operations instead of if statements we can do parallelism we can do batches of operations at a",
    "start": "419380",
    "end": "425650"
  },
  {
    "text": "time obviously GPUs and TP use are on the rise right so you know all these hopes for what this would do right so this was",
    "start": "425650",
    "end": "432460"
  },
  {
    "text": "we were really excited about the work at the start and we said okay let's just do the ML thing we'll just throw tensorflow",
    "start": "432460",
    "end": "438610"
  },
  {
    "start": "435000",
    "end": "705000"
  },
  {
    "text": "at the problem and say it'll just figure it out right this is the ML magic so we have 200 million records",
    "start": "438610",
    "end": "444940"
  },
  {
    "text": "we'll say look I'll take a two layer neural network and just sort of typical fully connected thing trained it to",
    "start": "444940",
    "end": "451060"
  },
  {
    "text": "predict the position given the key these are integers it's pretty pretty normal beat reruns on the order of 300",
    "start": "451060",
    "end": "457930"
  },
  {
    "text": "nanoseconds this first version ran in",
    "start": "457930",
    "end": "462970"
  },
  {
    "text": "80,000 nanoseconds right so this was kind of a bit of a step back so two",
    "start": "462970",
    "end": "468130"
  },
  {
    "text": "steps forward one step back kind of situation right we thought okay yeah this huge breakthrough and insight we'll just do it and then it doesn't",
    "start": "468130",
    "end": "473890"
  },
  {
    "text": "work and the more we spend time with this we found there's tons of pieces to this problem that I think are actually",
    "start": "473890",
    "end": "479950"
  },
  {
    "text": "fascinating directions on the system side as well as on the ml side for one thing tensorflow really is not it's the",
    "start": "479950",
    "end": "495760"
  },
  {
    "text": "data set size is 200 million keys but we're measuring here the time to run the",
    "start": "495760",
    "end": "500980"
  },
  {
    "text": "index so in the Petrie case it's just walking through the tree and the model really this is just instantiating tensorflow",
    "start": "500980",
    "end": "506470"
  },
  {
    "text": "effectively it's I mean it's independent of the size of the data",
    "start": "506470",
    "end": "512310"
  },
  {
    "text": "right yeah the beautifully doesn't it's not even close yeah there's a size of",
    "start": "515980",
    "end": "521830"
  },
  {
    "text": "data that would have made this come close there is obviously but yeah I",
    "start": "521830",
    "end": "528130"
  },
  {
    "text": "think the thing it was is that tensorflow really is not designed for that size of operations or anything",
    "start": "528130",
    "end": "534640"
  },
  {
    "text": "we're moving in that direction there becomes increasingly sort of efforts to make it work for example on on varieties",
    "start": "534640",
    "end": "540670"
  },
  {
    "text": "of hardware and to have it handle different performance requirements but generally like you looking at can we",
    "start": "540670",
    "end": "547660"
  },
  {
    "text": "analyze searches and recommendation systems which is another area of others areas of research that I work on there",
    "start": "547660",
    "end": "553210"
  },
  {
    "text": "you see that it's just different orders of magnitude right so come into like",
    "start": "553210",
    "end": "567040"
  },
  {
    "text": "what actually is happening I don't think it's a good that is by far anyway not the best that we could do I think with",
    "start": "567040",
    "end": "573340"
  },
  {
    "text": "out-of-the-box tools now even from a year ago when we ran that but it was definitely not like the most thorough",
    "start": "573340",
    "end": "581020"
  },
  {
    "text": "study of that I think we just like if you do the standard thing it won't work obviously there's tons of work on GPUs that are trying to make that faster",
    "start": "581020",
    "end": "586840"
  },
  {
    "text": "those are generally throughput focused different different constraints for",
    "start": "586840",
    "end": "592180"
  },
  {
    "text": "different systems so there's pure speed",
    "start": "592180",
    "end": "597750"
  },
  {
    "text": "there's actually model structure right b-trees virtually really good at overfitting it turns out and that's",
    "start": "597750",
    "end": "603430"
  },
  {
    "text": "usually like a bad word in ml you don't want to over fit it turns out for databases that's that she's not so bad",
    "start": "603430",
    "end": "609100"
  },
  {
    "text": "in many cases right if you're a read only database I know the data I know exactly what I'm trying to - there's",
    "start": "609100",
    "end": "615580"
  },
  {
    "text": "nothing else outside to generalize over cache efficiency I don't think I've ever heard of a paper for ML thinking about a",
    "start": "615580",
    "end": "622480"
  },
  {
    "text": "cache efficient designed model there's a lot of work now going into matching the hardware for things like GPUs but this",
    "start": "622480",
    "end": "628930"
  },
  {
    "text": "is sort of a new problem and then search right at the I mentioned at the end there's this process of actually have to",
    "start": "628930",
    "end": "634390"
  },
  {
    "text": "to search over normally in the page but here could be much larger possibly or much more narrow right it can vary you",
    "start": "634390",
    "end": "640480"
  },
  {
    "text": "don't know how accurate the model is and so there's questions like what's the best search style give it an approximate",
    "start": "640480",
    "end": "645910"
  },
  {
    "text": "answer so I'll kind of go through a few of these things we've tried to tackle so",
    "start": "645910",
    "end": "651430"
  },
  {
    "text": "for one thing is speed right here essentially we wanted to go from using raw tensorflow",
    "start": "651430",
    "end": "657490"
  },
  {
    "text": "to just sort of compiled operations right so we can learn you stencil to",
    "start": "657490",
    "end": "663550"
  },
  {
    "text": "learn a model extract the weights and then have that sort of better organized",
    "start": "663550",
    "end": "668890"
  },
  {
    "text": "in memory in terms of the weights you'll see in a few slides how there's different pieces to them also organized as well as actually have those",
    "start": "668890",
    "end": "675610"
  },
  {
    "text": "operations being run in like raw igan which is with underneath log the tensorflow but but in simpler operations",
    "start": "675610",
    "end": "681840"
  },
  {
    "text": "there's obviously things like a B X s if we didn't actually or seemed DVI she didn't even make use of a lot of this",
    "start": "681840",
    "end": "688090"
  },
  {
    "text": "builds on work team had done previously on compiling ahead of time database operations and so I think there's a lot",
    "start": "688090",
    "end": "694150"
  },
  {
    "text": "of potential here generally for ml to sort of catch up for doing these really really low latency kind of things next",
    "start": "694150",
    "end": "705700"
  },
  {
    "start": "705000",
    "end": "1119000"
  },
  {
    "text": "wish became like what is accurate enough when is it even worthwhile to use a model in the first place and we'd spent",
    "start": "705700",
    "end": "711730"
  },
  {
    "text": "a long time debating does it matter to have the bottom of the tree be accurate or the top of the tree be accurate or",
    "start": "711730",
    "end": "717220"
  },
  {
    "text": "where do we think it actually is beneficial and here's some like rough math that you have you have a hundred million records and your typical let's",
    "start": "717220",
    "end": "723850"
  },
  {
    "text": "say page size or branch factor 64 and that gets you from essentially knowing it's in this 100 million records so it's",
    "start": "723850",
    "end": "731200"
  },
  {
    "text": "in this give or take 1.5 million records right and so pretty much if you can get a model that can be more accurate than",
    "start": "731200",
    "end": "738880"
  },
  {
    "text": "being the right 1.5 million range that's probably worthwhile and you can",
    "start": "738880",
    "end": "744040"
  },
  {
    "text": "see that at every level right and there's a horses challenge right they take 60 cycles for that for that thing to actually do this search within that",
    "start": "744040",
    "end": "750340"
  },
  {
    "text": "page within that branch and know which branch to take so we have to have fewer in 60 cycles of modeling time used to",
    "start": "750340",
    "end": "758350"
  },
  {
    "text": "actually figure out that 1.5 million record range we can do this in I've seen each level and they becomes like a how",
    "start": "758350",
    "end": "765190"
  },
  {
    "text": "big models we make all right so what's the right model which I do a super-wide",
    "start": "765190",
    "end": "770200"
  },
  {
    "text": "model so okay people know that these are supposedly universal function approximator is just make it super wide",
    "start": "770200",
    "end": "776140"
  },
  {
    "text": "and it should be able to model anything and this will then talk in terms of math operations two times",
    "start": "776140",
    "end": "782260"
  },
  {
    "text": "the width of this where we can say okay well deep is actually better for complicated patterns so it's in this",
    "start": "782260",
    "end": "789670"
  },
  {
    "text": "case with squared in terms of the the width of the layer and you can say for every intermediate layer you have to do",
    "start": "789670",
    "end": "794680"
  },
  {
    "text": "like it's that square matrix right so that's an additional matrix math we have to do now the problem with this is we",
    "start": "794680",
    "end": "802930"
  },
  {
    "text": "found is a doing a single model is good but not great we really want to be able do is make use of the memory right do",
    "start": "802930",
    "end": "810730"
  },
  {
    "text": "one of these models is tiny compared to a b-tree right that that's like the gap is huge but we want to develop move in",
    "start": "810730",
    "end": "816760"
  },
  {
    "text": "that in between those two things a bit more elegantly right we should all say I want to ramp up the model size and with",
    "start": "816760",
    "end": "823660"
  },
  {
    "text": "that not totally kill my speed and at the same time improve my accuracy",
    "start": "823660",
    "end": "829240"
  },
  {
    "text": "greatly and this became a sort of this design space challenge we had to figure out and so we used this idea called",
    "start": "829240",
    "end": "834790"
  },
  {
    "text": "hierarchy of experts it's there's a bunch of literature on modeling at multiple experts in ml this is a",
    "start": "834790",
    "end": "841180"
  },
  {
    "text": "structure being used in particular where we say I can have a very small model even a linear model to try to say given",
    "start": "841180",
    "end": "847120"
  },
  {
    "text": "a key predicted position and I train it sort of in the typical way predicted",
    "start": "847120",
    "end": "852760"
  },
  {
    "text": "position and then given that approximate position use that to select another model and that model that be trained on",
    "start": "852760",
    "end": "860350"
  },
  {
    "text": "the subset of data that was thought to be in that range right so you're slowly doing is you're whittling down similar",
    "start": "860350",
    "end": "866050"
  },
  {
    "text": "to a b-tree the amount the range of data that each model is expected to model",
    "start": "866050",
    "end": "871270"
  },
  {
    "text": "well and we can do this over and over again recursively until we sort of get",
    "start": "871270",
    "end": "877270"
  },
  {
    "text": "these models that are sort of approximating less and less data well you really do feel the linear models for",
    "start": "877270",
    "end": "882640"
  },
  {
    "text": "example you're getting these piecewise linear function but we don't to do this sort of search over which piece we're in",
    "start": "882640",
    "end": "888310"
  },
  {
    "text": "it's selecting it through this sequence of multiplications and the good part of",
    "start": "888310",
    "end": "894310"
  },
  {
    "text": "this is the decouples entirely the model size and its complexity in terms of what it can model effectively",
    "start": "894310",
    "end": "901710"
  },
  {
    "text": "you're treating that whole thing as one model we trained it stage wise so within",
    "start": "902630",
    "end": "908399"
  },
  {
    "text": "the top layer first that can be like a linear model when pass so yet independent agents linear each one each",
    "start": "908399",
    "end": "914610"
  },
  {
    "text": "stage is one pass over the data so you do one pass you find out then the next one which bucket it would fall into and",
    "start": "914610",
    "end": "920190"
  },
  {
    "text": "train that on one pass if it's non linear it's obviously a little more",
    "start": "920190",
    "end": "925320"
  },
  {
    "text": "complicated but still you do stage wise dari we've tried some things but actually training it end-to-end there's",
    "start": "925320",
    "end": "932100"
  },
  {
    "text": "some interesting challenges so the related work here is the mixture of experts work where they'll train a softmax over different models but that's",
    "start": "932100",
    "end": "939300"
  },
  {
    "text": "inefficient if you have large numbers we'll see we go up to a hundred thousand sub models in some cases so once you've a soft max youth actually",
    "start": "939300",
    "end": "945779"
  },
  {
    "text": "now search what's the max in that soft max that's slow we're here we're doing listen to your regression prediction",
    "start": "945779",
    "end": "951000"
  },
  {
    "text": "right we're saying this is the index of the model I want to look up and for that you definitely know add some variance in",
    "start": "951000",
    "end": "956130"
  },
  {
    "text": "order to do some searching over and it gets complicated so I think there's potential there but at this point this was like good enough just I have",
    "start": "956130",
    "end": "970680"
  },
  {
    "text": "expression when you go come down into hierarchy does it mean that the training set becomes smaller yeah each model",
    "start": "970680",
    "end": "979680"
  },
  {
    "text": "becomes now it's handling a much more narrow region it's a problem that you're",
    "start": "979680",
    "end": "986220"
  },
  {
    "text": "over fitting for our overfitting because only the data in the database we have to",
    "start": "986220",
    "end": "991560"
  },
  {
    "text": "index this is like an these cool things that we won over fit this is why the model looks quite different from what everyone does in traditional neural",
    "start": "991560",
    "end": "997170"
  },
  {
    "text": "networks is it guaranteed that you that",
    "start": "997170",
    "end": "1003350"
  },
  {
    "text": "prediction of the top level model will like the range that it predicts will include the actual here it's not",
    "start": "1003350",
    "end": "1010760"
  },
  {
    "text": "printing arrange its printing in numbers and so it could be pretty far off so we we can do because again it's a fixed",
    "start": "1010760",
    "end": "1016910"
  },
  {
    "text": "size data set we can look at essentially how far off were we so one pass train",
    "start": "1016910",
    "end": "1023420"
  },
  {
    "text": "the models I can pass look at how wrong where we possibly in that pass Oh in that trained model and",
    "start": "1023420",
    "end": "1028850"
  },
  {
    "text": "we can use that now to to narrow down",
    "start": "1028850",
    "end": "1032860"
  },
  {
    "text": "case we can say look I didn't really do it well enough right I see there's a big error and I can substitute out saying it for whatever reason there's some pattern",
    "start": "1036410",
    "end": "1042050"
  },
  {
    "text": "here that just I can't get right fall back I think a lot of the work will",
    "start": "1042050",
    "end": "1048079"
  },
  {
    "text": "come to is there's this hybrid pattern right where ml can provide a very good",
    "start": "1048080",
    "end": "1055040"
  },
  {
    "text": "approximation for a large number of the cases but B not every case and using this in combination with traditional",
    "start": "1055040",
    "end": "1060290"
  },
  {
    "text": "data structures makes it sort of give the guarantees you want while getting a lot of benefit so we have a bunch of",
    "start": "1060290",
    "end": "1079820"
  },
  {
    "text": "slides later but it's a open question right so we only tested at least empirically so far the the question of",
    "start": "1079820",
    "end": "1086500"
  },
  {
    "text": "if was read only it's not read only you could leave space and say okay the montane less accurate I'll just degrade",
    "start": "1086500",
    "end": "1091880"
  },
  {
    "text": "gracefully which would be not the end of the world your error estimates go away um we can deal with that in a few slides",
    "start": "1091880",
    "end": "1098680"
  },
  {
    "text": "you can imagine doing training again again that's gonna kill your error estimates and depending on how you do",
    "start": "1098680",
    "end": "1103940"
  },
  {
    "text": "that training it could affect how well it's doing so there's a huge design space there of what's possible I'll go",
    "start": "1103940",
    "end": "1111830"
  },
  {
    "text": "into more detail but I think there's a bunch of like interesting questions so one more question so the question is it",
    "start": "1111830",
    "end": "1120920"
  },
  {
    "start": "1119000",
    "end": "1338000"
  },
  {
    "text": "we have a pretty good prediction but we need to do this sort of last mile issue that should go from approximate answer",
    "start": "1120920",
    "end": "1127340"
  },
  {
    "text": "to getting you the right data you actually asked for so I can say there's some pretty wide error bars here and I",
    "start": "1127340",
    "end": "1133640"
  },
  {
    "text": "can do the typical binary search thing I know the min error and the max error and then I can slowly walk through from the",
    "start": "1133640",
    "end": "1141950"
  },
  {
    "text": "predicted position until I find the right point now the problem with this of course is that we were actually really",
    "start": "1141950",
    "end": "1147980"
  },
  {
    "text": "close and now we wasted a lot of time trying to do this full range search and",
    "start": "1147980",
    "end": "1153260"
  },
  {
    "text": "so a lot of us do with then trade-offs depending on how accurate your model is and what your air distribution looks",
    "start": "1153260",
    "end": "1158510"
  },
  {
    "text": "like to matching your search algorithm with that so we can duke ordinary search",
    "start": "1158510",
    "end": "1163550"
  },
  {
    "text": "this is a swing tune played with a bit where we look at a standard deviation and we add multiple predictions around",
    "start": "1163550",
    "end": "1169040"
  },
  {
    "text": "it and we can move all these simultaneously efficiently this is not the most cleared I'll show some how that",
    "start": "1169040",
    "end": "1175250"
  },
  {
    "text": "works but essentially you can slowly hone in and the last thing say if we don't know the errors so I think is",
    "start": "1175250",
    "end": "1181130"
  },
  {
    "text": "actually a useful case because in many cases things like inserts my dish drawer destroy our error estimates in this case",
    "start": "1181130",
    "end": "1187610"
  },
  {
    "text": "we can still do sort of exponential search right so I have this prediction and I just slowly move step by step away",
    "start": "1187610",
    "end": "1193790"
  },
  {
    "text": "and so I overshoot and go back right so you overshoot here and now I can slowly walk back to the right place so it's",
    "start": "1193790",
    "end": "1200660"
  },
  {
    "text": "pretty flexible in terms of like there's mobile search strategies with different properties they do we'll see a couple",
    "start": "1200660",
    "end": "1206090"
  },
  {
    "text": "experiment results that do show that they have some trade-off spanned the model but overall it wasn't like a",
    "start": "1206090",
    "end": "1211430"
  },
  {
    "text": "make-or-break part of what made it succeed so we tested on four different",
    "start": "1211430",
    "end": "1217490"
  },
  {
    "text": "data sets with real patterns so when it's a time stand data set from some server logs longitude from maps some",
    "start": "1217490",
    "end": "1225350"
  },
  {
    "text": "ones like the most linear so she gets the best results this is a synthetic data from a log normal distribution",
    "start": "1225350",
    "end": "1230890"
  },
  {
    "text": "right so in this case we know it's nonlinear and then string documents",
    "start": "1230890",
    "end": "1236540"
  },
  {
    "text": "strings have their own issues because kind of arbitrary in many ways and their document IDs so I'll give some structure",
    "start": "1236540",
    "end": "1243500"
  },
  {
    "text": "to them at least watch the choke collar steena but Berkeley and that's question",
    "start": "1243500",
    "end": "1249290"
  },
  {
    "text": "is basically a lot of progress was made simply because improvements in device physics and the",
    "start": "1249290",
    "end": "1256510"
  },
  {
    "text": "database people made a distinction between in court databases where everything was in memory versus off core",
    "start": "1256510",
    "end": "1264640"
  },
  {
    "text": "you know on secondary storage devices and totally recently and we had solid state disks to replace rotating media",
    "start": "1264640",
    "end": "1270880"
  },
  {
    "text": "sample sorry you can you get into databases with address that's larger than 32 bits so there's it's a good",
    "start": "1270880",
    "end": "1278770"
  },
  {
    "text": "question there's two points to that I would say one is the question of how did the model deal with large address spaces and the second becomes how does how do",
    "start": "1278770",
    "end": "1286630"
  },
  {
    "text": "we deal with actually not being able to assume that the end location is ordered",
    "start": "1286630",
    "end": "1293170"
  },
  {
    "text": "right so if you have pages that are on disk those might not actually be in order or they've been separate devices like that right in terms of the disk",
    "start": "1293170",
    "end": "1300670"
  },
  {
    "text": "based part we have some preliminary ideas that we can't sort of give",
    "start": "1300670",
    "end": "1306160"
  },
  {
    "text": "empirical numbers for yet but there is",
    "start": "1306160",
    "end": "1311820"
  },
  {
    "text": "in terms of the the there's also the issue of like most models aren't dealing with like really large numbers and you",
    "start": "1311820",
    "end": "1317020"
  },
  {
    "text": "guys obviously have to scale them like your operations to be able to handle that but I think even that also is not",
    "start": "1317020",
    "end": "1322270"
  },
  {
    "text": "quite as bad it's manageable it's an engineering issue sure yes these are",
    "start": "1322270",
    "end": "1332200"
  },
  {
    "text": "definitely not your all in memory tests we ran so the course of the question is",
    "start": "1332200",
    "end": "1339760"
  },
  {
    "start": "1338000",
    "end": "1670000"
  },
  {
    "text": "obviously doesn't work this is on the the Maps data I believe yep the master data and we see the b-tree this was the",
    "start": "1339760",
    "end": "1346330"
  },
  {
    "text": "best beach resize the page 128 for the particular device we were testing on and",
    "start": "1346330",
    "end": "1351370"
  },
  {
    "text": "we ran this with linear models in this case so each model in this trees a linear model but we see the second stage",
    "start": "1351370",
    "end": "1357430"
  },
  {
    "text": "size is a huge effect on to the performance right so there's one key root node of a linear and then followed",
    "start": "1357430",
    "end": "1363700"
  },
  {
    "text": "by many many small models the second node saint layer",
    "start": "1363700",
    "end": "1369840"
  },
  {
    "text": "it's how many models there are each model is linear so there's a bias and a slope okay so if you give me the bolded",
    "start": "1372890",
    "end": "1384750"
  },
  {
    "text": "ones 50,001 and how big are each of your models parameters it's a linear model so",
    "start": "1384750",
    "end": "1390510"
  },
  {
    "text": "it's a single floating-point number and a bias term so two floating point there's just two parameters for you",
    "start": "1390510",
    "end": "1395850"
  },
  {
    "text": "learn about yeah we did there's some other results that go through nonlinear",
    "start": "1395850",
    "end": "1401580"
  },
  {
    "text": "models that pays off especially in the string case other things but I think it",
    "start": "1401580",
    "end": "1406950"
  },
  {
    "text": "turns out the hierarchy is actually very powerful compared to like when you reduce a bigger neural network for the",
    "start": "1406950",
    "end": "1412380"
  },
  {
    "text": "usual one so having a mini a larger tree seems to be the right choice and we do see here",
    "start": "1412380",
    "end": "1420150"
  },
  {
    "text": "there's this trade-off essentially you make the number of models in second layer larger obviously makes the actual",
    "start": "1420150",
    "end": "1426860"
  },
  {
    "text": "thing itself larger we generally see that uses the time so you have this trade-off between memory and latency in",
    "start": "1426860",
    "end": "1434820"
  },
  {
    "text": "some ways I think this is nice I think this generally what we get with the model approach learning approach you actually have some nice ability to move",
    "start": "1434820",
    "end": "1441480"
  },
  {
    "text": "more cleanly in the space of let me use more memory decrease latency and change and also trade-off actual compute and I",
    "start": "1441480",
    "end": "1449070"
  },
  {
    "text": "can actually moving that's okay think much more flexibly depend on how I design my mouse structure itself what I'm willing to deal with and so we do",
    "start": "1449070",
    "end": "1455760"
  },
  {
    "text": "find in something scary we can get to be one one hundredth the size or multiple",
    "start": "1455760",
    "end": "1460830"
  },
  {
    "text": "times as fast which i think is a nice thing given this is like all from that earlier 80,000 nanosecond this",
    "start": "1460830",
    "end": "1467430"
  },
  {
    "text": "commensurate 308 are seconds we didn't",
    "start": "1467430",
    "end": "1475710"
  },
  {
    "text": "look closely any times for linear models is pretty fast it's again single pass so this would be I think like a four passes",
    "start": "1475710",
    "end": "1481950"
  },
  {
    "text": "at most including the error computation and that could be all done in the learn the LIF system actually does the",
    "start": "1481950",
    "end": "1488850"
  },
  {
    "text": "training there you although linear stuff was done sort of in process whereas a",
    "start": "1488850",
    "end": "1495570"
  },
  {
    "text": "nonlinear model you use tensor flow and offload it generally wasn't a huge concern and a lot of systems that like",
    "start": "1495570",
    "end": "1501450"
  },
  {
    "text": "we work with have a separate triggered model construction time when it's a read-only system",
    "start": "1501450",
    "end": "1508220"
  },
  {
    "text": "so is this like adverbs lookup time or is it the others are there six about how",
    "start": "1508610",
    "end": "1516210"
  },
  {
    "text": "it performs in the worst case we did look at it I can't remember the details honestly and there was a we did look at",
    "start": "1516210",
    "end": "1523140"
  },
  {
    "text": "like different percentile marks remember the details unfortunately we also didn't",
    "start": "1523140",
    "end": "1528780"
  },
  {
    "text": "really look at in anyway this is another assumption and it's worth mentioning this assumes the sort of uniform query distribution right so we're not also",
    "start": "1528780",
    "end": "1535820"
  },
  {
    "text": "optimizing for some preference for one part of date over the other it's only looking it's the same amount information",
    "start": "1535820",
    "end": "1542160"
  },
  {
    "text": "that a b-tree has over all we get sort",
    "start": "1542160",
    "end": "1548850"
  },
  {
    "text": "of these these levels improvements occur several data sets the the map stated I think does the best because it's the most linear and so it's an easiest to to",
    "start": "1548850",
    "end": "1555720"
  },
  {
    "text": "model and you do see I think this is interesting property right that don't know how complicated the data is yeah",
    "start": "1555720",
    "end": "1561360"
  },
  {
    "text": "are that affects how well we can do on it we under the current model class for",
    "start": "1561360",
    "end": "1567150"
  },
  {
    "text": "looking at strings are present the biggest challenge they can be a sort of arbitrary word length and the",
    "start": "1567150",
    "end": "1573270"
  },
  {
    "text": "computation needed to deal with it is not great there's much other details about how to actually model strings",
    "start": "1573270",
    "end": "1578310"
  },
  {
    "text": "right people model them typically for language purposes not for the sort of index lookup in ml and so this is an",
    "start": "1578310",
    "end": "1586080"
  },
  {
    "text": "example of that where we actually start to find that using things like the coronary search has the biggest has a",
    "start": "1586080",
    "end": "1591720"
  },
  {
    "text": "decent impact actually and some of these hybrid index to start to matter right so we do find that in these harder cases",
    "start": "1591720",
    "end": "1599610"
  },
  {
    "text": "these extra steps do end up paying off whereas in the earlier maps case sort of out of the box modeling works pretty",
    "start": "1599610",
    "end": "1606300"
  },
  {
    "text": "well I mean well obviously with the compilation everything else there's also",
    "start": "1606300",
    "end": "1615690"
  },
  {
    "text": "a bunch of discussion afterwards about like what's the right baseline and how would you actually take this insight using other places right so someone",
    "start": "1615690",
    "end": "1621690"
  },
  {
    "text": "suggested for example using a fixed size B tree and then you can do interpolation search afterwards right into these",
    "start": "1621690",
    "end": "1628379"
  },
  {
    "text": "different combinations of these ideas so we tested this on this log normal Dana synthetic data the payload size affects",
    "start": "1628379",
    "end": "1634110"
  },
  {
    "text": "how well does this affects cache efficiency but ultimately we end up seeing that that we can still always",
    "start": "1634110",
    "end": "1640110"
  },
  {
    "text": "find through a better balancing of both memory and latency this partially also makes use of using a multivariate model",
    "start": "1640110",
    "end": "1646230"
  },
  {
    "text": "right so we can look at things like the square of the key and that helps give some more flexibility beyond just a linear model both increasing the model",
    "start": "1646230",
    "end": "1653759"
  },
  {
    "text": "itself more complicate a significant way in complexity I think that again by no",
    "start": "1653759",
    "end": "1659100"
  },
  {
    "text": "means is this the end-all be-all of models I think there's just in a big open area to make better models for this",
    "start": "1659100",
    "end": "1664139"
  },
  {
    "text": "kind of system yeah and that's just a visualization of that and last thing I",
    "start": "1664139",
    "end": "1671850"
  },
  {
    "text": "think this is but yet by no means sort of a deep theoretical analysis but I think it's nice to think about under this ml perspective what would theory",
    "start": "1671850",
    "end": "1679409"
  },
  {
    "text": "look like right and so here I looked at saying if we have its empirical CDF it's",
    "start": "1679409",
    "end": "1684840"
  },
  {
    "text": "kind of flip from the way you normally think about CDF style analysis and statistics say I have the empirical CDF that is the ground truth right I need to",
    "start": "1684840",
    "end": "1691889"
  },
  {
    "text": "model that as well as possible but if you imagine that the model of it is the theoretical CDF that it was sampled from",
    "start": "1691889",
    "end": "1697590"
  },
  {
    "text": "and say well that they're closely up isn't any more complicated but we do say is that it has some accuracy there's",
    "start": "1697590",
    "end": "1703679"
  },
  {
    "text": "some noise inherent in a sampling process but it's a reasonably good accurate model it's not overfitting unfortunately but we can find",
    "start": "1703679",
    "end": "1710850"
  },
  {
    "text": "essentially as the data size grows that affects the accuracy and this is happens",
    "start": "1710850",
    "end": "1716190"
  },
  {
    "text": "to be a way that accuracy sort of works out this expected squared error between the empirical CDF and theoretical CDF",
    "start": "1716190",
    "end": "1721980"
  },
  {
    "text": "scales 1 over N but if we're now multiplying this by n we're right we",
    "start": "1721980",
    "end": "1728460"
  },
  {
    "text": "have to look up in an N size data set that means the the error in that look up it scales with oh of square root of n so",
    "start": "1728460",
    "end": "1735840"
  },
  {
    "text": "I don't think this is particularly the best way or even the most charitable way of analyzing what we're doing I think it",
    "start": "1735840",
    "end": "1741720"
  },
  {
    "text": "does provide a path forward thing about okay how do we think about the distribution and the difficulty of it",
    "start": "1741720",
    "end": "1746759"
  },
  {
    "text": "and how that affects the scalability of our algorithm and it sends two open questions inserts",
    "start": "1746759",
    "end": "1754220"
  },
  {
    "start": "1752000",
    "end": "1932000"
  },
  {
    "text": "is obviously one of the ones that you always bring up if yours potential right so a pens is a whole class of problem so you have time",
    "start": "1754220",
    "end": "1760700"
  },
  {
    "text": "stamps that are being inserted so sort of sequential data that's coming in chances are those pretty consistent",
    "start": "1760700",
    "end": "1765920"
  },
  {
    "text": "patterns where you can imagine they're sort of cyclical daily patterns if we can generalize an ml studies a ton about",
    "start": "1765920",
    "end": "1771320"
  },
  {
    "text": "how you generalize into the future it's possible we actually can do this without having to build out your index over time",
    "start": "1771320",
    "end": "1779860"
  },
  {
    "text": "and there's less yep so the idea here",
    "start": "1780190",
    "end": "1791300"
  },
  {
    "text": "would be to say if I have one model that can generalize into the future which is what ml mostly tries to do we don't",
    "start": "1791300",
    "end": "1799070"
  },
  {
    "text": "actually need to keep constructing the model right I don't have to keep building the b-tree as I insert data and",
    "start": "1799070",
    "end": "1805220"
  },
  {
    "text": "so I'm Lily's going to be adding data inserting it at the end of my the next page on disk so I still do the writing",
    "start": "1805220",
    "end": "1812540"
  },
  {
    "text": "to disk I still have to or adding it to the end of the array but the model should hopefully still be able to predict as n grows just predict",
    "start": "1812540",
    "end": "1820850"
  },
  {
    "text": "into the future it could still be at noon as peak time and at 6:00 a.m. is is quiet time and that's the cyclical",
    "start": "1820850",
    "end": "1826340"
  },
  {
    "text": "pattern I have to put it into the future and I'll have some error and I'll can do search to correct that at the end right",
    "start": "1826340",
    "end": "1842420"
  },
  {
    "text": "if you're indexing on timestamp [Music]",
    "start": "1842420",
    "end": "1849640"
  },
  {
    "text": "I think the key thing I'm saying is if it's a case where you want to be ordered",
    "start": "1856000",
    "end": "1861530"
  },
  {
    "text": "by timestamp which is a set of property like give me all the indexed thing is",
    "start": "1861530",
    "end": "1867590"
  },
  {
    "text": "coming in in order and I don't need to know as I say append-only right not",
    "start": "1867590",
    "end": "1873410"
  },
  {
    "text": "arbitrary inserts here",
    "start": "1873410",
    "end": "1876310"
  },
  {
    "text": "so arbitrary inserts you could say I can leave space in my sorted array b-trees can do this - you can leave space",
    "start": "1878790",
    "end": "1884040"
  },
  {
    "text": "obviously and then hope that space is well allocated and that if the space is well allocated such as the pages don't",
    "start": "1884040",
    "end": "1890340"
  },
  {
    "text": "fill up I can just sort of keep things in order in the page and the b-tree above it can stay the same but when I",
    "start": "1890340",
    "end": "1896280"
  },
  {
    "text": "spell over pages I have to now do some reshuffling of the internals of the tree and the hope here becomes well maybe I",
    "start": "1896280",
    "end": "1903120"
  },
  {
    "text": "can use being smarter in this way again in the modelling side it can be approximate ml has a lot of work again",
    "start": "1903120",
    "end": "1908670"
  },
  {
    "text": "on generalize in the future looking covariant shifts and saying not just where is there what says you should nap distribution now but where I think it's",
    "start": "1908670",
    "end": "1915450"
  },
  {
    "text": "gonna grow in the future and I'll leave space there and I think again this is not by any means that we've done this but there's the the language of ml or",
    "start": "1915450",
    "end": "1922800"
  },
  {
    "text": "they sort of head towards been done in the past could be adapted in useful ways to try to deal with this issue and so",
    "start": "1922800",
    "end": "1933450"
  },
  {
    "start": "1932000",
    "end": "2111000"
  },
  {
    "text": "now turn over to Ed moving on from b-trees to some other structures in the",
    "start": "1933450",
    "end": "1946200"
  },
  {
    "text": "beginning we started showing this picture right to give you a sense of all the various possible data structures",
    "start": "1946200",
    "end": "1952740"
  },
  {
    "text": "that we might be interested in affecting using this learn index idea and Alex",
    "start": "1952740",
    "end": "1959010"
  },
  {
    "text": "just spent all this time basically taking you through this particular use case of B trees but what about other",
    "start": "1959010",
    "end": "1967260"
  },
  {
    "text": "kinds of data structures for example hashmaps now it's actually kind of interesting",
    "start": "1967260",
    "end": "1972810"
  },
  {
    "text": "we're telling this story a little bit in reverse chronological order because we started with obviously something a",
    "start": "1972810",
    "end": "1978480"
  },
  {
    "text": "little simpler and we told you actually the most complicated case and hashmap was actually the second one we looked at",
    "start": "1978480",
    "end": "1984660"
  },
  {
    "text": "and we'll talk about bloom filters later but if you think about a hash map keys",
    "start": "1984660",
    "end": "1991140"
  },
  {
    "text": "basically our map to a position inside an array right so that's the prediction task that essentially that we care about",
    "start": "1991140",
    "end": "1999300"
  },
  {
    "text": "now if it turns out that spot is taken in that array what we do is we do a",
    "start": "1999300",
    "end": "2005030"
  },
  {
    "text": "linked list overflow right so collisions unfortunately in hash map",
    "start": "2005030",
    "end": "2011370"
  },
  {
    "text": "actually really expensive because you're basically doing pointer chasing and every pointer that you chase is actually",
    "start": "2011370",
    "end": "2018690"
  },
  {
    "text": "about 100 nanoseconds using the current architectures so you know we want to",
    "start": "2018690",
    "end": "2024330"
  },
  {
    "text": "avoid that collision as much as possible that's essentially the the job of what a hash map is trying to do right so if we",
    "start": "2024330",
    "end": "2033000"
  },
  {
    "text": "think about hashmaps essentially as a learned model now the same ideas that we were talking about in",
    "start": "2033000",
    "end": "2039780"
  },
  {
    "text": "the B tree case actually applies as well so basically a machine learning model in this case is kind of learning",
    "start": "2039780",
    "end": "2046080"
  },
  {
    "text": "essentially the the CDF the cumulative distribution function right and we can",
    "start": "2046080",
    "end": "2051090"
  },
  {
    "text": "directly actually use that learned function as our hash right so in that",
    "start": "2051090",
    "end": "2056638"
  },
  {
    "text": "case if you have perfect information about your data which in a way we do because we know every single key that we",
    "start": "2056639",
    "end": "2062700"
  },
  {
    "text": "care about in the hash map and then a perfect CDF model would give us zero",
    "start": "2062700",
    "end": "2069750"
  },
  {
    "text": "collisions right and and this goes back to the question I think you were asking",
    "start": "2069750",
    "end": "2075090"
  },
  {
    "text": "about a little bit before is that if we assume that we're only doing reading",
    "start": "2075090",
    "end": "2080460"
  },
  {
    "text": "then at that moment in time we have the perfect information and therefore there's no collision but of course with",
    "start": "2080460",
    "end": "2085860"
  },
  {
    "text": "inserts you end up with more and more collisions right now what's interesting about this is that this fact that if you",
    "start": "2085860",
    "end": "2093240"
  },
  {
    "text": "have the perfect CDF at that moment in time you can construct the perfect a",
    "start": "2093240",
    "end": "2098480"
  },
  {
    "text": "lookup table essentially right is independent of any kind of modification",
    "start": "2098480",
    "end": "2104700"
  },
  {
    "text": "that you want to do to the hash map right whatever kind of hash map type you might want to use so the question is is",
    "start": "2104700",
    "end": "2113820"
  },
  {
    "start": "2111000",
    "end": "2183000"
  },
  {
    "text": "whether this is really true so you know like like good researchers when we started thinking about okay hashmaps",
    "start": "2113820",
    "end": "2119850"
  },
  {
    "text": "we wanted to build a essentially the quickest bench benchmark that we can to",
    "start": "2119850",
    "end": "2126350"
  },
  {
    "text": "rapidly reduce whether we have a difficult problem here or not so we",
    "start": "2126350",
    "end": "2133470"
  },
  {
    "text": "tried this idea on three different data sets on the map data I think alex has",
    "start": "2133470",
    "end": "2139230"
  },
  {
    "text": "talked about a little bit before the web data I think will get actually get into",
    "start": "2139230",
    "end": "2144420"
  },
  {
    "text": "a little bit more later in the slides and then just some arbitrary lognormal distribution that we",
    "start": "2144420",
    "end": "2149769"
  },
  {
    "text": "created so you can see that in the traditional hash map you get collision",
    "start": "2149769",
    "end": "2155109"
  },
  {
    "text": "rates roughly in the thirty five percent range whereas this alert model you can",
    "start": "2155109",
    "end": "2161140"
  },
  {
    "text": "drop dramatically reduce the number of collisions in in the MA in in the",
    "start": "2161140",
    "end": "2167769"
  },
  {
    "text": "lookups so that reduction looked pretty pretty dramatic to us it was like oh",
    "start": "2167769",
    "end": "2173019"
  },
  {
    "text": "that's good news you know we should chase after this idea maybe this ideas wasn't so terrible after all so that was",
    "start": "2173019",
    "end": "2180190"
  },
  {
    "text": "very encouraging early on when we were starting to look at hash maps when we look deeper into this so the data I'm",
    "start": "2180190",
    "end": "2188259"
  },
  {
    "start": "2183000",
    "end": "2350000"
  },
  {
    "text": "showing here is basically you build a hash map and you either use a random",
    "start": "2188259",
    "end": "2194829"
  },
  {
    "text": "hash function or you used the learned model hash that we learned through",
    "start": "2194829",
    "end": "2201700"
  },
  {
    "text": "through the procedure that we described you can see that the amount of empty slots that we end up having to use has a",
    "start": "2201700",
    "end": "2210430"
  },
  {
    "text": "pretty dramatic reduction in the amount of space that we need for those empty slots and yes space is the model the",
    "start": "2210430",
    "end": "2220690"
  },
  {
    "text": "model itself cost oh that's a good question the model themselves are actually very very cheap right because",
    "start": "2220690",
    "end": "2227739"
  },
  {
    "text": "you would basically just need to know the CDF right like the CDF is what",
    "start": "2227739",
    "end": "2233650"
  },
  {
    "text": "you're you're learning yeah exactly so it's it's super cheap as a result",
    "start": "2233650",
    "end": "2240940"
  },
  {
    "text": "right the interesting thing here is that you can see that at least in our",
    "start": "2240940",
    "end": "2246999"
  },
  {
    "text": "implementation and compared to a random hash the the model actually you have",
    "start": "2246999",
    "end": "2252430"
  },
  {
    "text": "some little bit of computation to do and that that cost you a couple of nanoseconds and so it's actually not",
    "start": "2252430",
    "end": "2260289"
  },
  {
    "text": "true in this particular case where the render we can beat the random hash as a function yes if you're assuming that the",
    "start": "2260289",
    "end": "2270220"
  },
  {
    "text": "distribution in your data is pretty much fixed and you know the keys mm-hmm why",
    "start": "2270220",
    "end": "2275739"
  },
  {
    "text": "learn it why not just you know in a sensor of a tablet just",
    "start": "2275739",
    "end": "2283779"
  },
  {
    "text": "tablet it completely right yeah you know that's interesting because in a way it",
    "start": "2283779",
    "end": "2289450"
  },
  {
    "text": "connects to your question about inserts earlier in the particular case of hashmaps we as we are thinking about the",
    "start": "2289450",
    "end": "2297430"
  },
  {
    "text": "transaction where you're inserting these items right so if if it's truly",
    "start": "2297430",
    "end": "2302470"
  },
  {
    "text": "completely a static world where you know all the keys ahead of time",
    "start": "2302470",
    "end": "2308980"
  },
  {
    "text": "you you're right in a sense that you just tablet the the CDF and just use that function right the size is kind of",
    "start": "2308980",
    "end": "2318039"
  },
  {
    "text": "an issue a learn model my actually compress me you have a for example if",
    "start": "2318039",
    "end": "2323470"
  },
  {
    "text": "you had a linear function right that's literally two parameter as Alex was saying before that's much smaller and as",
    "start": "2323470",
    "end": "2329140"
  },
  {
    "text": "storing the entire CDF so you may have",
    "start": "2329140",
    "end": "2344829"
  },
  {
    "text": "heard about a little bit of the controversy around this paper and there were a little bit of conversation online",
    "start": "2344829",
    "end": "2351700"
  },
  {
    "start": "2350000",
    "end": "2557000"
  },
  {
    "text": "about whether we're comparing against the stadia our hash map implementations",
    "start": "2351700",
    "end": "2358359"
  },
  {
    "text": "and whatnots and so one the interesting thing about this space is that you could",
    "start": "2358359",
    "end": "2363999"
  },
  {
    "text": "use various different kinds of assumptions about what kind of instructions that you're using what kind",
    "start": "2363999",
    "end": "2369009"
  },
  {
    "text": "of hashing technique you're using etc so we ran the the experiments after some of",
    "start": "2369009",
    "end": "2378940"
  },
  {
    "text": "the the code were made available to us looking at for example the the Avs",
    "start": "2378940",
    "end": "2386559"
  },
  {
    "text": "instruction says how much payload you're actually using and things like that in our experiments we see that in for",
    "start": "2386559",
    "end": "2394480"
  },
  {
    "text": "particularly in the in place chain hashmaps with the assumption of using a 20-byte as the payload that you're looking up",
    "start": "2394480",
    "end": "2402150"
  },
  {
    "text": "the learned hash function actually achieves very very good performance of about 35 nanoseconds compared to some",
    "start": "2402150",
    "end": "2409150"
  },
  {
    "text": "other implementations so we we do think that it the soundest ideas seems",
    "start": "2409150",
    "end": "2414340"
  },
  {
    "text": "to hold up under some scrutiny and the space utilization and obviously also",
    "start": "2414340",
    "end": "2420400"
  },
  {
    "text": "very good because again you know the basic idea here is that since you have perfect information on CDF why wouldn't",
    "start": "2420400",
    "end": "2427210"
  },
  {
    "text": "you be able to do better so let me touch upon a little bit on",
    "start": "2427210",
    "end": "2434770"
  },
  {
    "text": "bloom filters yes yeah that's a toast oh",
    "start": "2434770",
    "end": "2448530"
  },
  {
    "text": "right you mean instead of running the four PI values I think that the reason",
    "start": "2449850",
    "end": "2458410"
  },
  {
    "text": "that we were really thinking about is that we think that the four PI case is not really all that realistic in the",
    "start": "2458410",
    "end": "2464080"
  },
  {
    "text": "real real world workload right I mean that's a very very small payload I think",
    "start": "2464080",
    "end": "2473470"
  },
  {
    "text": "I remember the first to fix the cache",
    "start": "2473470",
    "end": "2481270"
  },
  {
    "text": "essentially the options I mean the",
    "start": "2481270",
    "end": "2489430"
  },
  {
    "text": "larger record can only slow you down he wouldn't make you right so a four byte I",
    "start": "2489430",
    "end": "2495220"
  },
  {
    "text": "would imagine would be less should be less than 35 nanoseconds right yeah so",
    "start": "2495220",
    "end": "2510160"
  },
  {
    "text": "you could be still 35 days where you're saying yeah I mean that's reasonable assumption I think thinking about this",
    "start": "2510160",
    "end": "2518950"
  },
  {
    "text": "space one major challenges as you kind of are pointing out is is why the water",
    "start": "2518950",
    "end": "2525160"
  },
  {
    "text": "you're hitting the same cache line or not yeah that's right so you know that kind of thing sometimes",
    "start": "2525160",
    "end": "2531760"
  },
  {
    "text": "I even depend on your your workload assumptions and things like that right so let me actually tell the story around",
    "start": "2531760",
    "end": "2540490"
  },
  {
    "text": "bloom filters because to some extent this is actually the story that I love the most because",
    "start": "2540490",
    "end": "2545500"
  },
  {
    "text": "this was the place where we actually started when we started thinking about learning disease and it's a part that I",
    "start": "2545500",
    "end": "2553150"
  },
  {
    "text": "feel like I also have more of a hand in it so in just just as a quick summary of",
    "start": "2553150",
    "end": "2559420"
  },
  {
    "text": "what bloom filters are right so what bloom filters are basically is you give it a key and you ask it whether this key",
    "start": "2559420",
    "end": "2566830"
  },
  {
    "text": "exists in your index or not and so what bloom filters are nice is that they",
    "start": "2566830",
    "end": "2573250"
  },
  {
    "text": "guarantee that the false negatives is or is essentially false the unit rate is",
    "start": "2573250",
    "end": "2578350"
  },
  {
    "text": "zero and you can sort of tune your performance of your traditional bloom",
    "start": "2578350",
    "end": "2584950"
  },
  {
    "text": "filter by choosing as a false positive rate that you're willing to tolerate",
    "start": "2584950",
    "end": "2590320"
  },
  {
    "text": "basically when it says yes how often it is actually wrong about being being yes",
    "start": "2590320",
    "end": "2597390"
  },
  {
    "text": "so how a bloom filter works in in in a traditional way is in the following way",
    "start": "2597390",
    "end": "2604510"
  },
  {
    "text": "so you take a key and you have multiple hash functions that you apply to that key and then you have this bit array",
    "start": "2604510",
    "end": "2611110"
  },
  {
    "text": "that basically serves as a lookup right so doing insert when you're inserting your particular key you hash that key",
    "start": "2611110",
    "end": "2618550"
  },
  {
    "text": "and you have those bit positions and you turn those bits to one right and doing a",
    "start": "2618550",
    "end": "2625540"
  },
  {
    "text": "lookup or a probe what you do is you take that that key that you want to look up and you apply that those same set of",
    "start": "2625540",
    "end": "2633100"
  },
  {
    "text": "hash functions and you look in those bit positions if any one of them is a zero",
    "start": "2633100",
    "end": "2638440"
  },
  {
    "text": "you will return now right if they're all ones and then you will return yes so",
    "start": "2638440",
    "end": "2643900"
  },
  {
    "text": "that's how bloom filters work but if you think about what it is trying to do from",
    "start": "2643900",
    "end": "2650980"
  },
  {
    "text": "machine learner perspective which by the way is that's what I guess at this point I can call myself a machine learning",
    "start": "2650980",
    "end": "2656440"
  },
  {
    "text": "researcher I'm wearing a Google AI sure so I guess I am a machine learning guy",
    "start": "2656440",
    "end": "2661710"
  },
  {
    "start": "2661000",
    "end": "2821000"
  },
  {
    "text": "you know actually that just as a side note the last time I was in this room was Terry Winograd inviting me to give a",
    "start": "2661710",
    "end": "2668170"
  },
  {
    "text": "talk here and I was back then known as a web analytic and social computing guy so",
    "start": "2668170",
    "end": "2673420"
  },
  {
    "text": "you know like you can reinvent your career right but if you think about bloom filters as",
    "start": "2673420",
    "end": "2679810"
  },
  {
    "text": "a model and then all they are is really a blank binary classification task that's all right",
    "start": "2679810",
    "end": "2685930"
  },
  {
    "text": "you give it a key and he needs to predict whether is inside this set or outside of this set and that's it the",
    "start": "2685930",
    "end": "2693130"
  },
  {
    "text": "big problem with that view is that the false negatives are not guaranteed in a",
    "start": "2693130",
    "end": "2698349"
  },
  {
    "text": "typical binary classification machine learned model so how do you solve that problem and this is the part that I'm",
    "start": "2698349",
    "end": "2706450"
  },
  {
    "text": "pretty proud of because when when Tim was interning in my team he was doing",
    "start": "2706450",
    "end": "2713170"
  },
  {
    "text": "his sabbatical in my team we were discussing this problem and I suggested that we could use a spillover bloom",
    "start": "2713170",
    "end": "2719410"
  },
  {
    "text": "filter so the idea of course is that if it says no what you can do is you and",
    "start": "2719410",
    "end": "2724900"
  },
  {
    "text": "then double check it using a bloom filter but the beauty of it is that this",
    "start": "2724900",
    "end": "2730420"
  },
  {
    "text": "beautiful bloom filter can be much much smaller because you've taken care of all the other keys that you don't need to",
    "start": "2730420",
    "end": "2736450"
  },
  {
    "text": "know about a century there are the easy cases so the the bloom filter becomes this guarantee that deals with only the",
    "start": "2736450",
    "end": "2742930"
  },
  {
    "text": "HAR cases I think this design patterns really interesting is that it's sort of",
    "start": "2742930",
    "end": "2749099"
  },
  {
    "text": "unfortunately I know the somebody in in the seus department Chris Manning who's",
    "start": "2749099",
    "end": "2754890"
  },
  {
    "text": "actually kind of a acquaintance of mine you know sort of talked about the talked",
    "start": "2754890",
    "end": "2762400"
  },
  {
    "text": "up this paper a little bit and it some people start saying oh store away your database or a data structure textbooks",
    "start": "2762400",
    "end": "2770200"
  },
  {
    "text": "but actually we were we never sit that really in the paper in fact part of the paper was saying make use of the",
    "start": "2770200",
    "end": "2776740"
  },
  {
    "text": "traditional data structure along with learning decks right at the same time",
    "start": "2776740",
    "end": "2782230"
  },
  {
    "text": "they can actually work hand-in-hand the beauty of the traditional loan filter is that it gives you this guarantee",
    "start": "2782230",
    "end": "2788560"
  },
  {
    "text": "essentially of the false negatives being zero the false positive rate is zero so",
    "start": "2788560",
    "end": "2795160"
  },
  {
    "text": "this solution of having a spillover is is something that I think is a design",
    "start": "2795160",
    "end": "2802119"
  },
  {
    "text": "pattern in these kind of learning index situations that we can make use of in",
    "start": "2802119",
    "end": "2807640"
  },
  {
    "text": "other kinds of we just talked about all the other data structures that can conceivably use this idea on so",
    "start": "2807640",
    "end": "2815249"
  },
  {
    "text": "there may be these patterns in those other data structures as well so here's",
    "start": "2815249",
    "end": "2822599"
  },
  {
    "start": "2821000",
    "end": "2921000"
  },
  {
    "text": "some results again you know we wear every time we think about an idea like this we want to rapidly reduce to see",
    "start": "2822599",
    "end": "2829289"
  },
  {
    "text": "whether we're wrong right this is the idea holding water and so in this",
    "start": "2829289",
    "end": "2835529"
  },
  {
    "text": "particular case we're plotting on the y-axis the memory footprint the size of the model in this case we build",
    "start": "2835529",
    "end": "2842299"
  },
  {
    "text": "basically what the task that we're trying to do is you have URLs that are",
    "start": "2842299",
    "end": "2847859"
  },
  {
    "text": "held in a database and we want to find out whether those URLs are good or bad they're they're bad when they're a",
    "start": "2847859",
    "end": "2854970"
  },
  {
    "text": "fishing URL so and then we want to warm the user basically but you're good if",
    "start": "2854970",
    "end": "2860339"
  },
  {
    "text": "they're not in our database right and so the y-axis is applauding",
    "start": "2860339",
    "end": "2865619"
  },
  {
    "text": "the memory footprint the wire the x-axis is the false positive rate you can see a",
    "start": "2865619",
    "end": "2870989"
  },
  {
    "text": "traditional bloom filter in red here the top line and then the various versions",
    "start": "2870989",
    "end": "2876839"
  },
  {
    "text": "of our model which are built using RN ends and the W refers to how wide the",
    "start": "2876839",
    "end": "2883140"
  },
  {
    "text": "RNN model is how many neurons wide it is and then the e refers to the embedding",
    "start": "2883140",
    "end": "2889319"
  },
  {
    "text": "size that's used as the vocabulary underneath the RNN so you can see that",
    "start": "2889319",
    "end": "2894690"
  },
  {
    "text": "different size in terms of the machine learning model you get slightly different performance but they're",
    "start": "2894690",
    "end": "2900299"
  },
  {
    "text": "actually pretty close to each other the 120a is a little bit over capacity of",
    "start": "2900299",
    "end": "2905339"
  },
  {
    "text": "over capacity so you can see that it's memory footprint goes up a little bit but 32 or 16 is more than enough for the",
    "start": "2905339",
    "end": "2912420"
  },
  {
    "text": "tasks that we were looking at and that resulted in a 36 percent reduction in memory for this particular bloom filter",
    "start": "2912420",
    "end": "2921170"
  },
  {
    "start": "2921000",
    "end": "3015000"
  },
  {
    "text": "now we after we put the initial version",
    "start": "2921170",
    "end": "2926220"
  },
  {
    "text": "of this paper on archive as you can imagine lots of people started writing",
    "start": "2926220",
    "end": "2931249"
  },
  {
    "text": "emails to us one of them was Michael Mitchell mocker from Harvard University",
    "start": "2931249",
    "end": "2937160"
  },
  {
    "text": "who have been spending essentially a majority part of his career on bloom",
    "start": "2937160",
    "end": "2943890"
  },
  {
    "text": "filters so as you can imagine when he was alerted that we were interested in bloom filters he was very",
    "start": "2943890",
    "end": "2950700"
  },
  {
    "text": "curious about our results we ended up at having a very very long email thread",
    "start": "2950700",
    "end": "2955800"
  },
  {
    "text": "back and forth about whether to learn index learn bloom filters is a good idea",
    "start": "2955800",
    "end": "2961530"
  },
  {
    "text": "or not and eventually we're happy to say that that he came around to our point of",
    "start": "2961530",
    "end": "2967620"
  },
  {
    "text": "view and in fact not only da he had an additional build on our idea of this",
    "start": "2967620",
    "end": "2973490"
  },
  {
    "text": "spillover bloom filter he said why not also use another simple bloom filter in",
    "start": "2973490",
    "end": "2979680"
  },
  {
    "text": "front of the learned model and and create a sandwich structure so you run",
    "start": "2979680",
    "end": "2985650"
  },
  {
    "text": "through the initial bloom filter first again handling some really easy cases",
    "start": "2985650",
    "end": "2991560"
  },
  {
    "text": "and then have the learn model handled kind of the medium hard cases and the really hard models end up in us in an",
    "start": "2991560",
    "end": "2999270"
  },
  {
    "text": "additional spillover at the bottom and he was able to prove recently that it was an additional 12% saving in size and",
    "start": "2999270",
    "end": "3007400"
  },
  {
    "text": "sound workloads that he was looking at so I think that paper is now available",
    "start": "3007400",
    "end": "3012590"
  },
  {
    "text": "online right so so that kind of gives you a sense of three cases of the",
    "start": "3012590",
    "end": "3020840"
  },
  {
    "text": "various things that we're looking at so one thing that we spend a little bit of",
    "start": "3020840",
    "end": "3026870"
  },
  {
    "text": "time discussing already in this form is inserts and updates how do we deal with inserts and updates and trees you know",
    "start": "3026870",
    "end": "3034340"
  },
  {
    "text": "this is one of those annoying tricks that lecturers and professors like to do",
    "start": "3034340",
    "end": "3040100"
  },
  {
    "text": "is you know that exercises left to the reader right so inserts and updates I think there's some real opportunities to",
    "start": "3040100",
    "end": "3047360"
  },
  {
    "text": "do some research there but there are lots of other potential applications so",
    "start": "3047360",
    "end": "3052910"
  },
  {
    "text": "for example we haven't talked about at all about joints how do you do joins well and and in fact",
    "start": "3052910",
    "end": "3059690"
  },
  {
    "text": "one the most tricky problems in joints is the problem of estimating the",
    "start": "3059690",
    "end": "3065390"
  },
  {
    "text": "cardinality if you can do that using a learned approach you might be able to do",
    "start": "3065390",
    "end": "3070940"
  },
  {
    "text": "much more efficient joints sorting is pretty interesting that's obviously had",
    "start": "3070940",
    "end": "3077600"
  },
  {
    "text": "been a lot of work that has gone into beating the best",
    "start": "3077600",
    "end": "3082980"
  },
  {
    "text": "sorting algorithms out there we think that there might be an opportunity to use a learned approach for sorting the",
    "start": "3082980",
    "end": "3091319"
  },
  {
    "text": "CDF essentially gives you a proximate sort order so there may be an",
    "start": "3091319",
    "end": "3096420"
  },
  {
    "text": "opportunity to again you know make use of that information and maybe combine with some traditional sorting algorithms",
    "start": "3096420",
    "end": "3102810"
  },
  {
    "text": "in that sort of maybe sandwich or not sandwiched approach unit that combine hybrid approach to really kind of look",
    "start": "3102810",
    "end": "3110369"
  },
  {
    "text": "at sorting in some some some new lens using this the the formula that we given",
    "start": "3110369",
    "end": "3117660"
  },
  {
    "text": "in this lecture another really interesting thing that that we have started working on simply because it's",
    "start": "3117660",
    "end": "3125220"
  },
  {
    "text": "just so fascinating is that you can think of a lot of the work that we have described here as being wanting to",
    "start": "3125220",
    "end": "3132960"
  },
  {
    "text": "create a mapping function right and it's typically 1d or 2d mapped onto some",
    "start": "3132960",
    "end": "3142430"
  },
  {
    "text": "one-dimensional thing that we wanted to know so yes/no is real early just you know are you over a threshold or under",
    "start": "3142430",
    "end": "3148859"
  },
  {
    "text": "threshold and that's it right but what about multi-dimensional indices where we",
    "start": "3148859",
    "end": "3154980"
  },
  {
    "text": "have much more complicated 3d 4d 5d",
    "start": "3154980",
    "end": "3160130"
  },
  {
    "text": "lookup keys that you're trying to index and you want to do very efficient lookup",
    "start": "3160130",
    "end": "3165780"
  },
  {
    "text": "in in those situations so you know you can imagine since machine learning",
    "start": "3165780",
    "end": "3173250"
  },
  {
    "text": "models ml models are just simply creating these mapping functions that",
    "start": "3173250",
    "end": "3178770"
  },
  {
    "text": "they should apply for multi-dimensional indices as well and I think the saving",
    "start": "3178770",
    "end": "3184589"
  },
  {
    "text": "could be even more dramatic in those part use cases the other thing that I",
    "start": "3184589",
    "end": "3191700"
  },
  {
    "start": "3190000",
    "end": "3339000"
  },
  {
    "text": "think is worth thinking about in closing is that the fundamental ideas in from",
    "start": "3191700",
    "end": "3199470"
  },
  {
    "text": "this lecture that I think you might be able to take away from is that if you learn the patterns in your data you",
    "start": "3199470",
    "end": "3206400"
  },
  {
    "text": "should be able to make use of that in your data structure design right that's in some extent to some extent is not",
    "start": "3206400",
    "end": "3212880"
  },
  {
    "text": "really huge surprise and there's been other data structure work that has make made",
    "start": "3212880",
    "end": "3218580"
  },
  {
    "text": "advantage of data patterns as well but one thing that is really interesting here is actually the fact that much of",
    "start": "3218580",
    "end": "3226800"
  },
  {
    "text": "the old data structure design assumes essentially a kind of computing architecture that's based upon CPUs and",
    "start": "3226800",
    "end": "3233940"
  },
  {
    "text": "in CPUs you know the typical operation a lot of times is if them statements right",
    "start": "3233940",
    "end": "3240600"
  },
  {
    "text": "if this and then do this otherwise do this other thing and you know that makes",
    "start": "3240600",
    "end": "3245940"
  },
  {
    "text": "like branching a prediction etc are very very important in these kinds of",
    "start": "3245940",
    "end": "3251460"
  },
  {
    "text": "architectures but the thing is that increasingly we're moving to a world where we have vector processors where",
    "start": "3251460",
    "end": "3259850"
  },
  {
    "text": "vector matrix multiplies are really really cheap so to some extent one of",
    "start": "3259850",
    "end": "3265500"
  },
  {
    "text": "the things that I think is much it's really exciting about the work that we're doing here is the realization that",
    "start": "3265500",
    "end": "3271290"
  },
  {
    "text": "maybe we won't be even running these learning disease on CPUs we'd be running them on vector processors where",
    "start": "3271290",
    "end": "3278310"
  },
  {
    "text": "essentially you're replacing the if-then statements with multiplies vector",
    "start": "3278310",
    "end": "3284010"
  },
  {
    "text": "multiplies to be precise and so that means that if every server comes with a",
    "start": "3284010",
    "end": "3289340"
  },
  {
    "text": "vector processor I still I'm I'm unlike Alex I am Oh enough to remember the",
    "start": "3289340",
    "end": "3295050"
  },
  {
    "text": "coprocessors next to the 83 86 s right so there was already some at some point",
    "start": "3295050",
    "end": "3301710"
  },
  {
    "text": "in our past and in computing history where we thought that coupling a vector processor along with with CPUs could be",
    "start": "3301710",
    "end": "3311400"
  },
  {
    "text": "a real advantage in the design and we think that's kept the case in data centers as well so that's it I think",
    "start": "3311400",
    "end": "3319770"
  },
  {
    "text": "that was kind of the way to to summarize what we're thinking about in this space and I hope that was interesting to all",
    "start": "3319770",
    "end": "3326130"
  },
  {
    "text": "of you thank you very much you",
    "start": "3326130",
    "end": "3330900"
  },
  {
    "text": "you",
    "start": "3337290",
    "end": "3339350"
  }
]