[
  {
    "text": "Hi. Hi, everyone. Welcome back.",
    "start": "5480",
    "end": "7440"
  },
  {
    "text": "Let's get started. So lecture 3. Today is the last\nlecture where we",
    "start": "10630",
    "end": "17260"
  },
  {
    "text": "will talk about more theoretical\ntopics on lossless compression. And then next lecture onwards,\nit's mostly being algorithms.",
    "start": "17260",
    "end": "27570"
  },
  {
    "text": "So today, we will\ncover three topics. The first one is something\ncalled Kraft's inequality,",
    "start": "27570",
    "end": "35390"
  },
  {
    "text": "which you will see soon. We'll explain when\nwe come to it. The second two are\nreally the main result",
    "start": "35390",
    "end": "41840"
  },
  {
    "text": "of lossless compression,\nwhich tells you what is the best possible\nlossless compression you can do.",
    "start": "41840",
    "end": "46879"
  },
  {
    "text": "And so, that will\nbe the takeaway. And that's what we will\nthen try to achieve",
    "start": "46880",
    "end": "52190"
  },
  {
    "text": "for the next several lectures. OK, stop me anytime\nwith any questions.",
    "start": "52190",
    "end": "59789"
  },
  {
    "text": "So let me talk about SCL. So what is SCL? SCL is Stanford\nCompression Library.",
    "start": "59790",
    "end": "65729"
  },
  {
    "text": "We'll show you how it looks. Yep? [INAUDIBLE] my I ask how\nthis course [INAUDIBLE]??",
    "start": "65730",
    "end": "72350"
  },
  {
    "text": "For homework or a study\ngroup, sort of thing? Yeah, sure. Yeah.\nYeah. But you have to submit\nthe homework individually.",
    "start": "76710",
    "end": "83170"
  },
  {
    "text": "Yeah. For the projects, you\ncan do it in groups. Yeah.",
    "start": "83170",
    "end": "86380"
  },
  {
    "text": "Any other questions? [INAUDIBLE]",
    "start": "89070",
    "end": "101570"
  },
  {
    "text": "Yep. Yep.\nYeah. Yeah, so as I go\nthrough the SCL thing, basically, the aim\nof this library",
    "start": "101570",
    "end": "107770"
  },
  {
    "text": "is to make your life easy, to\nmake it easy to learn things. But sometimes, if you are\nnot as familiar with Python",
    "start": "107770",
    "end": "114880"
  },
  {
    "text": "or if you are not familiar\nwith this style of Python or something, it might\nbecome an obstacle. We don't want that.",
    "start": "114880",
    "end": "120400"
  },
  {
    "text": "So if you get stuck at\nany point, in anything, just reach out ASAP\nso that we can help.",
    "start": "120400",
    "end": "127510"
  },
  {
    "text": "Yeah. We have tried very hard\nto learn from last year.",
    "start": "127510",
    "end": "132580"
  },
  {
    "text": "So hopefully, this year, will\nbe a much smoother experience. OK, so SCL, what is SCL?",
    "start": "132580",
    "end": "138430"
  },
  {
    "text": "SCL, Stanford\nCompression Library, so this started last\nyear, mostly, initiative.",
    "start": "138430",
    "end": "144760"
  },
  {
    "text": "So what you see, if you want\nto learn about compression, you learn about the theory. You learn about the\nbasic algorithms.",
    "start": "144760",
    "end": "149962"
  },
  {
    "text": "But then you go to try find\nsome actual implementations. You want to use it\nin your research. And you suddenly\nrun across, like--",
    "start": "149962",
    "end": "157180"
  },
  {
    "text": "for some of the\nimplementations, you find-- for some of the algorithms,\nyou find Python implementations that are possible to read.",
    "start": "157180",
    "end": "163239"
  },
  {
    "text": "But most of the times,\nyou just see a C file, but very low level C.\nSometimes, even SIMD",
    "start": "163240",
    "end": "171250"
  },
  {
    "text": "and that very low level code. That's very hard to\nread for a beginner, so you don't\nunderstand anything.",
    "start": "171250",
    "end": "177100"
  },
  {
    "text": "And so you can't\nunderstand the algorithm. So how will you use\nit in the research?",
    "start": "177100",
    "end": "182180"
  },
  {
    "text": "So the point here\nwas like, we want-- many basic algorithms are\nhard to find and write.",
    "start": "182180",
    "end": "187909"
  },
  {
    "text": "Efficient implementations\nare hard for beginners to understand. And just understanding\nthe algorithms,",
    "start": "187910",
    "end": "193750"
  },
  {
    "text": "like we understand\nin class, that's not enough to\nactually implement it. So unless you\nactually implement it,",
    "start": "193750",
    "end": "199187"
  },
  {
    "text": "you will find that\nthere are many gaps in your understanding. So with that, we started writing\nsome common data compression",
    "start": "199187",
    "end": "207550"
  },
  {
    "text": "algorithms, research\nimplementations, right? Aim is not to be the most\nefficient implementation,",
    "start": "207550",
    "end": "212860"
  },
  {
    "text": "but something that you\ncan read and understand. Plus, the other aim was to have\na framework so that somebody",
    "start": "212860",
    "end": "219610"
  },
  {
    "text": "could modify these, make their\nown compressors on top of this, that sort of thing. And the last thing\nwas more selfish,",
    "start": "219610",
    "end": "225160"
  },
  {
    "text": "like we, ourselves, wanted\nto actually implement these at least. If you want to\ncall ourselves data compression researchers,\nbetter that we understand",
    "start": "225160",
    "end": "232810"
  },
  {
    "text": "some of these in more detail. Yeah. So with that in mind, let me\ngive you a very quick tour.",
    "start": "232810",
    "end": "240920"
  },
  {
    "text": "So this is the course website,\nwhich I hope you have seen.",
    "start": "240920",
    "end": "246240"
  },
  {
    "text": "You'll see Fall 23 here, right?",
    "start": "246240",
    "end": "251470"
  },
  {
    "text": "Don't go to the Fall\n22 website, right? Use the Fall 23\nwebsite, bookmark it so that-- because on\nGoogle, sometimes,",
    "start": "251470",
    "end": "258156"
  },
  {
    "text": "it might lead you to\nthe old website, right? The homework will be\non the assignment tab",
    "start": "258157",
    "end": "264690"
  },
  {
    "text": "here, homework 1. If you go to logistics, and\nthen you go to useful links,",
    "start": "264690",
    "end": "273390"
  },
  {
    "text": "the top useful link is\nStanford Compression Library. So let's go there.",
    "start": "273390",
    "end": "279090"
  },
  {
    "text": "Search on GitHub. Right.",
    "start": "279090",
    "end": "284245"
  },
  {
    "text": "I guess I will just briefly\nwalk through what's in it. So basically, it\nimplements a bunch",
    "start": "286880",
    "end": "292430"
  },
  {
    "text": "of different common algorithms. A lot of which you\nwill study in class. And it will be very useful for\nyour projects, for example.",
    "start": "292430",
    "end": "299300"
  },
  {
    "text": "Many of the students\nlast year did projects on extending a bunch of these. And then you can contribute\nback to the library.",
    "start": "299300",
    "end": "306560"
  },
  {
    "text": "So there are some\nprefix free algorithms. There are some other entropic\norders, more efficient ones.",
    "start": "306560",
    "end": "315790"
  },
  {
    "text": "And there are some external\nlibraries imported here. There is a getting started,\nif you want to install it,",
    "start": "315790",
    "end": "321590"
  },
  {
    "text": "and so on. This will all be\npart of the homework. So you will get to do this\nas part of the homework.",
    "start": "321590",
    "end": "326425"
  },
  {
    "text": "Let me just-- so that you get\na sense of what's in here. So if you go to the SCL,\nI hope you can see it.",
    "start": "330590",
    "end": "336600"
  },
  {
    "text": "Yeah. OK. Right. So there is a utils one which\nI will go through very quickly",
    "start": "336600",
    "end": "342560"
  },
  {
    "text": "first. So the first thing\nyou will encounter when you start implementing\na data compressor",
    "start": "342560",
    "end": "348230"
  },
  {
    "text": "is that machines\nwork with bytes. But oftentimes, many\nof these compression algorithms work with bits.",
    "start": "348230",
    "end": "353750"
  },
  {
    "text": "So you need various\ntypes of utilities to convert from your bit\narrays into actual bytes",
    "start": "353750",
    "end": "359030"
  },
  {
    "text": "that you will write\nto the file and so on. And you will run into\nthis in the homework. So this bit array util file\nhas bitwidth and integer",
    "start": "359030",
    "end": "368160"
  },
  {
    "text": "to bitarray, bitarray to\ninteger, random bitarray, just a bunch of\nuseful functions.",
    "start": "368160",
    "end": "373870"
  },
  {
    "text": "So in the homework, what\nyou will find is, like-- we have tried to provide\nyou hints on what functions",
    "start": "373870",
    "end": "379020"
  },
  {
    "text": "you should try to use, right? So just look at the homework\ncode that the starter code.",
    "start": "379020",
    "end": "384510"
  },
  {
    "text": "And hopefully, it helps you. There are a lot of algorithms\nwe saw last time, right?",
    "start": "384510",
    "end": "389845"
  },
  {
    "text": "They work with trees. So there are some tree-related\nutils, some miscellaneous stuff testing.",
    "start": "389845",
    "end": "394380"
  },
  {
    "text": "Right. Then many algorithms, as\nyou have already seen, work with probability\ndistributions, right?",
    "start": "396820",
    "end": "403009"
  },
  {
    "text": "So you want to sort a\nprobability distribution. Last time, we read\nabout Shannon codes,",
    "start": "403010",
    "end": "408660"
  },
  {
    "text": "where you needed\nsome sort of sorting. You want to normalize thing. You want to do\ncumulative distributions. So just this thing, a\nlot of utility stuff.",
    "start": "408660",
    "end": "416835"
  },
  {
    "text": "End of utility. Let me talk about one thing we\nactually-- very concrete thing we saw in class.",
    "start": "419370",
    "end": "426070"
  },
  {
    "text": "I think it's in compressors.",
    "start": "426070",
    "end": "428480"
  },
  {
    "text": "Yeah, so if you\nremember, last time, we talked about prefix-free codes. And we saw that to decode\nthem, you go through a process",
    "start": "435160",
    "end": "444010"
  },
  {
    "text": "where you keep on\nadding the symbol, and then you try to decode it.",
    "start": "444010",
    "end": "449830"
  },
  {
    "text": "So this is like--\nthis is the structure of many of the functions here. So it's a prefix-free\ndecoder, which",
    "start": "449830",
    "end": "457330"
  },
  {
    "text": "needs to have a decode block\nfunction and an encode block.",
    "start": "457330",
    "end": "462819"
  },
  {
    "text": "This is just a decoder, so\nit will have a decode block function. And to decode a block in a\nloop, the loop you just see",
    "start": "462820",
    "end": "469330"
  },
  {
    "text": "on the screen-- let me zoom in-- while number of bits\nconsumed is less than-- Basically, as long as\nyou have bits left,",
    "start": "469330",
    "end": "475720"
  },
  {
    "text": "you try to decode a symbol. And then in a loop,\nbasically, you decode one symbol at a time. That's prefix-free decoding.",
    "start": "475720",
    "end": "482650"
  },
  {
    "text": "And you might ask, how\ndo I decode a symbol?",
    "start": "482650",
    "end": "485259"
  },
  {
    "text": "Let's see. Yeah, so here is like\ndecoding a symbol.",
    "start": "488540",
    "end": "495139"
  },
  {
    "text": "So line 166, you set num\nbits consumed to zero.",
    "start": "495140",
    "end": "500320"
  },
  {
    "text": "Line 170, you set current\nnode to root node. So this might be bringing\nsome memories back",
    "start": "500320",
    "end": "506830"
  },
  {
    "text": "from last lecture, where\nwe saw that you start from the root of the tree. And then you keep going\ndown until you hit a leaf,",
    "start": "506830",
    "end": "513250"
  },
  {
    "text": "and you decode that\nleaf, and so on, right? So all of this is\nimplemented, and you will use it in your homework.",
    "start": "513250",
    "end": "518950"
  },
  {
    "text": "And you can read it if you\ndon't understand something in the lecture. Because, often, for many\nof us, like reading a code",
    "start": "518950",
    "end": "524620"
  },
  {
    "text": "helps us bring more clarity\nthan just a vaguely drawn diagram in the lecture.",
    "start": "524620",
    "end": "530035"
  },
  {
    "text": "So there will be\na tutorial on SCL, which will help you\nget started, which",
    "start": "532940",
    "end": "538220"
  },
  {
    "text": "will be posted later\nthis week sometime so that should be\nvery useful for you as you start your homework.",
    "start": "538220",
    "end": "545260"
  },
  {
    "text": "And again, if you are\nstruggling with anything that seems to you that it's\nour fault or it's SCL's fault,",
    "start": "545260",
    "end": "551650"
  },
  {
    "text": "talk to us. We will help you out. Any questions on this?",
    "start": "551650",
    "end": "557350"
  },
  {
    "text": "Yeah. I'm sure you have questions\nonce you get started. But for now, let's move on.",
    "start": "557350",
    "end": "564810"
  },
  {
    "text": "OK, so last time's quiz,\nwe will come back in a bit. Please be patient.",
    "start": "564810",
    "end": "570170"
  },
  {
    "text": "I want to cover one topic,\nwhich will be useful to talk about the course.",
    "start": "570170",
    "end": "575370"
  },
  {
    "text": "OK, so last time, we\ntalked about prefix codes. So prefix codes are codes,\nwhere no codeword is",
    "start": "575370",
    "end": "582779"
  },
  {
    "text": "a prefix of another codeword. And we saw that it really\nsimplifies the decoding. And one way of decoding\nwas this tree-based thing,",
    "start": "582780",
    "end": "589360"
  },
  {
    "text": "where you start from the root. You go down the tree\nand so on, right? I've drawn one of\nthose here, where if you see a 1, 0, you go this\nway, then you go this way,",
    "start": "589360",
    "end": "597090"
  },
  {
    "text": "and you decode a B. And then\nyou go back to the root, and then you see a 1,\n1, 1, so you go to D.",
    "start": "597090",
    "end": "602610"
  },
  {
    "text": "You decode D. You\ngo back to the root. I won't spend too much time.",
    "start": "602610",
    "end": "607710"
  },
  {
    "text": "I think you might have gotten\nsome practice in the homework-- in the quiz. Then, we defined a good code.",
    "start": "607710",
    "end": "615610"
  },
  {
    "text": "I just gave you a thumb\nrule, asked you to believe. We will show this\ntime why this held,",
    "start": "615610",
    "end": "620830"
  },
  {
    "text": "but we said that a good code\nhas the length of a symbol. The length of the\nencoding of a symbol",
    "start": "620830",
    "end": "625930"
  },
  {
    "text": "is roughly the log of 1 over Px. And then we actually constructed\na code called the Shannon code,",
    "start": "625930",
    "end": "632320"
  },
  {
    "text": "where the lengths\nwere actually very close to the promised good code.",
    "start": "632320",
    "end": "636175"
  },
  {
    "text": "For Shannon code, we\ndid the construction. Construction was very simple. We sorted the symbols\naccording to their length,",
    "start": "638980",
    "end": "645160"
  },
  {
    "text": "and then we just\nbasically pick a code word that doesn't violate the\nprefix free condition.",
    "start": "645160",
    "end": "651990"
  },
  {
    "text": "And I think you had some\nhomework questions on that. Then, we did the proof for\nwhy that construction works.",
    "start": "651990",
    "end": "657540"
  },
  {
    "text": "Even if it didn't fully follow\nthe proof, that's totally fine. We won't be using that.",
    "start": "657540",
    "end": "662295"
  },
  {
    "text": "You will see a proof\nwhich is very similar, so hopefully, that helps\nbetter understanding.",
    "start": "664598",
    "end": "671440"
  },
  {
    "text": "OK, so first topic today\nis Kraft's inequality. What is Kraft's inequality? So last time, we saw\nthat a prefix code",
    "start": "671440",
    "end": "678399"
  },
  {
    "text": "is equivalent to having\nyour codewords leaves on a binary tree, right?",
    "start": "678400",
    "end": "684040"
  },
  {
    "text": "So when we say\nprefix code, we mean no code word is a\nprefix of another leaves of a binary tree. These are very English\nrepresentations of the thing,",
    "start": "684040",
    "end": "693280"
  },
  {
    "text": "not as mathematical as\nsome of you might like. Today, we will see\na characterization of prefix codes, which\nis very mathematical.",
    "start": "693280",
    "end": "701020"
  },
  {
    "text": "You can say a prefix code\nis equivalent to satisfying a certain inequality,\nwhich we will just",
    "start": "701020",
    "end": "706570"
  },
  {
    "text": "see in the next slide. And this will be very useful\nfor the rest of the lecture",
    "start": "706570",
    "end": "711759"
  },
  {
    "text": "and beyond. So this is the inequality.",
    "start": "711760",
    "end": "717540"
  },
  {
    "text": "So the first part\nis the forward part, and then we have a converse. So given a prefix code with\ncertain codeword lengths--",
    "start": "717540",
    "end": "725370"
  },
  {
    "text": "oh, by the way, these will\nbe posted on the website. So, yeah, if you\nwant to take notes,",
    "start": "725370",
    "end": "732360"
  },
  {
    "text": "but if you don't,\nthen that's fine. OK, so for any prefix\ncode in the world, that",
    "start": "732360",
    "end": "738480"
  },
  {
    "text": "has certain codeword lengths l1\nto lk, the sum of this quantity summation I equal to 1\nto k, 2 to the minus li",
    "start": "738480",
    "end": "747210"
  },
  {
    "text": "is less than or equal to 1. Right. And we'll see this come\nup again and again.",
    "start": "747210",
    "end": "753480"
  },
  {
    "text": "So for now, OK,\nthis is something. And maybe the converse part\nmight interest some people more, which is that given\nany set of integers l1 to lk,",
    "start": "753480",
    "end": "763160"
  },
  {
    "text": "satisfying this inequality,\nthere exists a prefix code with lengths l1 to lk, right?",
    "start": "763160",
    "end": "770000"
  },
  {
    "text": "So basically,\nhaving a prefix code is equivalent to satisfying\nKraft's inequality on the lengths, so it's an\nif and only if situation.",
    "start": "770000",
    "end": "778579"
  },
  {
    "text": "Any questions before I move on? We'll see some\nexamples in a second.",
    "start": "778580",
    "end": "783865"
  },
  {
    "text": "OK. And don't worry if it\nlooks very abstract. Why are we doing this? As we move on, I think by\nthe end of this lecture,",
    "start": "786440",
    "end": "793370"
  },
  {
    "text": "you will have a very strong\nintuition about some of this. Letter ink ran out here.",
    "start": "793370",
    "end": "799060"
  },
  {
    "text": "OK. Proof of the converse part. So converse part says that if\nyou have a sequence of lengths",
    "start": "799060",
    "end": "805940"
  },
  {
    "text": "satisfying this inequality, then\nyou can create a prefix code. This I won't cover too much.",
    "start": "805940",
    "end": "811750"
  },
  {
    "text": "Basically, it's the same\nconstruction as Shannon codes. So you sort the lens. You pick lens-- you pick\na codeword according",
    "start": "811750",
    "end": "818590"
  },
  {
    "text": "to the prefix condition, and\nyou get a code at the end.",
    "start": "818590",
    "end": "824420"
  },
  {
    "text": "Right. So let me not do an example. I think it's the exact same\nconstruction as the Shannon",
    "start": "824420",
    "end": "832370"
  },
  {
    "text": "code's. Yeah, feel free to revisit.",
    "start": "832370",
    "end": "837779"
  },
  {
    "text": "OK. OK, the forward part, I will\ndo a proof sketch, which should",
    "start": "837780",
    "end": "843390"
  },
  {
    "text": "give you enough intuition. And maybe it will help\nyou understand the proof from last lecture\nbetter as well.",
    "start": "843390",
    "end": "850720"
  },
  {
    "text": "So what do we need to prove? We want to prove that\nif you have a prefix code with these lengths,\nwith lengths l1 to lk,",
    "start": "850720",
    "end": "861050"
  },
  {
    "text": "then this inequality\nholds this one. Summation li to k, 2 to\nthe minus li, less than",
    "start": "861050",
    "end": "868070"
  },
  {
    "text": "or equal to 1. So this is what you need\nto prove for any prefix code in the world. So how will we prove it?",
    "start": "868070",
    "end": "874130"
  },
  {
    "text": "We define something called lmax. lmax is the maximum--",
    "start": "874130",
    "end": "879350"
  },
  {
    "text": "lmax is the maximum length,\nmaximum codeword length,",
    "start": "882620",
    "end": "888310"
  },
  {
    "text": "the longest codeword. OK. So this time, I came\nprepared with a diagram.",
    "start": "888310",
    "end": "894160"
  },
  {
    "text": "So, OK. So let's see what we can. So I've drawn here the actual\ncode, all the code words",
    "start": "894160",
    "end": "899440"
  },
  {
    "text": "on a tree. And I've shown the depths\nof the different leaves. So l1, l2, l3 are\nat the same depth.",
    "start": "899440",
    "end": "907490"
  },
  {
    "text": "L4 is this the deepest\nnode in the tree, which is same as lmax, because\nthat's the deepest one, right?",
    "start": "907490",
    "end": "915260"
  },
  {
    "text": "Now this is something, I\nthink, we saw last lecture, that 2 power lmax minus li\nis the number of descendants",
    "start": "915260",
    "end": "926330"
  },
  {
    "text": "of the ith codeword\nat depth lmax, right? So we saw this last time.",
    "start": "926330",
    "end": "932320"
  },
  {
    "text": "I won't dwell on it too much. Basically, the idea is if you\nhave a binary tree of depth n, then the number of leaves\nis 2 to the n, right?",
    "start": "932320",
    "end": "939830"
  },
  {
    "text": "So if you are at\ndepth li already, and then you need to go to lmax,\nso the binary tree starting",
    "start": "939830",
    "end": "946430"
  },
  {
    "text": "at li up to lmax has number\nof leaves 2 to the power, like the difference\nin the heights.",
    "start": "946430",
    "end": "952760"
  },
  {
    "text": "So let me illustrate it here. Like, if you see at\nthis part, right? So this is l1.",
    "start": "952760",
    "end": "958190"
  },
  {
    "text": "This is the node l1. And this is the subtree\nthat you can create,",
    "start": "958190",
    "end": "964490"
  },
  {
    "text": "which is the\nchildren of this guy. And this has number\nof nodes, which",
    "start": "964490",
    "end": "970160"
  },
  {
    "text": "is 2 to the lmax minus\nl number of leaves, and so on for this guy, so\non for this guy, and so on.",
    "start": "970160",
    "end": "977430"
  },
  {
    "text": "OK, the other property we will\nuse is that if you have node i",
    "start": "983860",
    "end": "989390"
  },
  {
    "text": "and node j, their\ndescendants cannot overlap. Why can't they overlap? Because prefix-free\nproperty, right?",
    "start": "989390",
    "end": "996334"
  },
  {
    "text": "So you can see it, and\nsometimes, it's easier to see in the picture, right? Like, clearly, these\nguys can't overlap because they are just like\nseparate branches of the tree.",
    "start": "996335",
    "end": "1005000"
  },
  {
    "text": "So therefore, if you\nadd all of these up, you add all of these guys,\nwho are at depth lmax.",
    "start": "1005000",
    "end": "1010670"
  },
  {
    "text": "Some will be lower than the\ntotal size of the number of nodes at depth lmax.",
    "start": "1013740",
    "end": "1020520"
  },
  {
    "text": "So we're using a\nvery simple property, where if you have a bunch\nof disjoint subsets,",
    "start": "1020520",
    "end": "1025680"
  },
  {
    "text": "and you add up their sizes, that\ncannot be more than the total size of the entire set, right?",
    "start": "1025680",
    "end": "1031589"
  },
  {
    "text": "So summation of this thing is\nless than or equal l2 to lmax.",
    "start": "1031589",
    "end": "1034559"
  },
  {
    "text": "Yeah, let me give you a\nminute to absorb this. And if you have any\nquestions, please ask.",
    "start": "1037359",
    "end": "1043540"
  },
  {
    "text": "Yes? Are these the number of branches\nbefore you get to a leaf?",
    "start": "1057140",
    "end": "1064740"
  },
  {
    "text": "Yeah. Yeah. So it's-- yeah, it's the\ndepth, or it's the length of the codeword.",
    "start": "1064740",
    "end": "1069920"
  },
  {
    "text": "That's the way to think. Yeah. So l1 is 1, because\nthis is 1, right?",
    "start": "1069920",
    "end": "1076820"
  },
  {
    "text": "l2, in this example, is 3\nbecause there are three things.",
    "start": "1076820",
    "end": "1082220"
  },
  {
    "text": "Yeah. I think this is\nthe last time you will see this tree-style proof.",
    "start": "1085590",
    "end": "1091320"
  },
  {
    "text": "So if you don't like\nit, this is the end.",
    "start": "1091320",
    "end": "1093929"
  },
  {
    "text": "OK, let me move on. OK. We'll go to the quiz. Before that, let me just say\nthat last lecture, if you",
    "start": "1099760",
    "end": "1107620"
  },
  {
    "text": "remember, we talked\nabout prefix codes, but we also talked about a\nbigger category of codes, called uniquely\ndecodable codes which",
    "start": "1107620",
    "end": "1113529"
  },
  {
    "text": "were basically lossless codes. And here, as you see, we are\nproving Kraft's inequality",
    "start": "1113530",
    "end": "1120020"
  },
  {
    "text": "for prefix codes. In the homework, you will\nprove Kraft's inequality for uniquely\ndecodable codes, which",
    "start": "1120020",
    "end": "1125720"
  },
  {
    "text": "is just a more\npowerful statement, because prefix codes\nare a subset of uniquely decodable codes.",
    "start": "1125720",
    "end": "1131600"
  },
  {
    "text": "And that's a very\ndifferent proof style, so I hope you enjoy\nthat one as well.",
    "start": "1131600",
    "end": "1136460"
  },
  {
    "text": "OK, let's do the quiz questions. And here, we will do some\nexamples of Kraft's inequality as well.",
    "start": "1139980",
    "end": "1145000"
  },
  {
    "text": "So this will help you get\nfamiliar with the thing.",
    "start": "1145000",
    "end": "1147900"
  },
  {
    "text": "OK. I think, here, I will\nask for your help. So here, the question was,\nyou were given a distribution",
    "start": "1150740",
    "end": "1158299"
  },
  {
    "text": "over A, B, C, D, E. And you wanted to\nbuild a Shannon code for this distribution.",
    "start": "1158300",
    "end": "1163880"
  },
  {
    "text": "So you could just use that\nShannon code construction algorithm. So can somebody help me with--",
    "start": "1163880",
    "end": "1171010"
  },
  {
    "text": "I already computed the\nlengths of the codewords with this formula, right? So now, you only need\nto tell me the codewords",
    "start": "1171010",
    "end": "1177350"
  },
  {
    "text": "with these lengths. Please, anybody? A?",
    "start": "1177350",
    "end": "1181030"
  },
  {
    "text": "OK, someone says it's 00. Yeah, so the\nalgorithm is you just pick any available codeword\nat that length, right?",
    "start": "1183780",
    "end": "1190920"
  },
  {
    "text": "And available means that\nit shouldn't be a suffix or prefix of anybody else.",
    "start": "1190920",
    "end": "1197430"
  },
  {
    "text": "OK. B? OK, here, 01 and 10.",
    "start": "1197430",
    "end": "1203840"
  },
  {
    "text": "Let's do 01. Keep it simple this time. OK, C. OK, I see 1011.",
    "start": "1203840",
    "end": "1212120"
  },
  {
    "text": "Again, take 10, so easier,\nmore lexicographic. OK, D. OK, D, let's\ndo 110, right?",
    "start": "1212120",
    "end": "1222020"
  },
  {
    "text": "So we could-- for example,\nwe couldn't choose 100 because that would\nbe like C would be, then it would violate\nthe prefix-free property.",
    "start": "1222020",
    "end": "1229370"
  },
  {
    "text": "But 110 is totally fine. E?",
    "start": "1229370",
    "end": "1231860"
  },
  {
    "text": "Yeah, so you see, you could\nhave chosen 111 in a good code, but a Shannon code must\nhave these lengths.",
    "start": "1236220",
    "end": "1242460"
  },
  {
    "text": "So it's like\nsuboptimal in that way. So we have to choose a\ncodeword with length four.",
    "start": "1242460",
    "end": "1247770"
  },
  {
    "text": "So let's choose 1110, which\nwill satisfy all the prefix",
    "start": "1247770",
    "end": "1252870"
  },
  {
    "text": "Properties OK. And the expected length\nthat was question 2.",
    "start": "1252870",
    "end": "1260139"
  },
  {
    "text": "I hope this is very easy now. So you just do the math. You get the 2.37.",
    "start": "1260140",
    "end": "1270257"
  },
  {
    "text": "OK, this is something that\nshould have been on the quiz, but we couldn't add, because\nI didn't finish on time.",
    "start": "1270257",
    "end": "1274840"
  },
  {
    "text": "We just saw that Kraft\ninequality, 2 power minus li, the sum should be less\nthan or equal to 1. So if we compute that\nsum for these lengths,",
    "start": "1277840",
    "end": "1287240"
  },
  {
    "text": "you can do the math. You get 15/16, which\nis less than 1. So that's good.",
    "start": "1287240",
    "end": "1292400"
  },
  {
    "text": "You are satisfying the\nKraft's inequality. And there is a gap actually,\nlike it's not equal to 1 here.",
    "start": "1292400",
    "end": "1298760"
  },
  {
    "text": "And we'll come back to\nthat gap in a second.",
    "start": "1298760",
    "end": "1303770"
  },
  {
    "text": "Right. I hope this is very clear. So 2 power minus li,\nyou are summing over. So 2 power minus 2\nplus 2 power minus 2",
    "start": "1303770",
    "end": "1309169"
  },
  {
    "text": "plus two power minus\n2 plus 2 power minus 3 plus 2 power minus 4. That's the math here. And the total is 15/16,\nwhich is less than 1.",
    "start": "1309170",
    "end": "1317930"
  },
  {
    "text": "Any prefix code will always\nsatisfy the Kraft's inequality. OK.",
    "start": "1317930",
    "end": "1322160"
  },
  {
    "text": "Then we asked for a better code,\nand I already made it at home. So as some of you, I think,\nsuggested that the last length",
    "start": "1324730",
    "end": "1334019"
  },
  {
    "text": "need not be 4, right? This is clearly suboptimal. You could have\nchosen length 3 here.",
    "start": "1334020",
    "end": "1339840"
  },
  {
    "text": "Is that clear to everyone? That you could have chosen\nlength 3 here, right? Because 110 is also not a\nprefix or suffix of anything.",
    "start": "1339840",
    "end": "1347820"
  },
  {
    "text": "OK, now if you compute\nthe expected length, you actually get 2.25, which\nis strictly lower than the 2.37",
    "start": "1347820",
    "end": "1355830"
  },
  {
    "text": "we got for the\nShannon code, right? So Shannon code is a good code,\nbut it's not the optimal code. In this case, you\ncan [INAUDIBLE]",
    "start": "1355830",
    "end": "1362035"
  },
  {
    "text": "better than the Shannon code. And in fact, if you now compute\nthe Kraft sum, that 1 over 2",
    "start": "1362035",
    "end": "1369780"
  },
  {
    "text": "to the l, and you sum it over,\nyou actually get 1, right? So you will see,\nin the next lecture",
    "start": "1369780",
    "end": "1376970"
  },
  {
    "text": "that an optimal code\nsatisfies Kraft's inequality",
    "start": "1376970",
    "end": "1382020"
  },
  {
    "text": "with equality. For an optimal code, the\nsummation of 2 power minus li is actually equal to 1.",
    "start": "1382020",
    "end": "1388170"
  },
  {
    "text": "So actually, the gap\nbetween 1 and your Kraft sum tells you how far you\nare from the optimality.",
    "start": "1388170",
    "end": "1393870"
  },
  {
    "text": "OK, any questions on question 1? I think most of\nyou got it, right? From what I remember.",
    "start": "1396560",
    "end": "1404460"
  },
  {
    "text": "OK, good. Second was, I think, very easy. I hope let's quickly\nsolve it together.",
    "start": "1404460",
    "end": "1411900"
  },
  {
    "text": "We just asked, are\nthese codes prefix-free? Was 2.1 prefix-free?",
    "start": "1411900",
    "end": "1415770"
  },
  {
    "text": "2.1, some people\nare saying no, and I think it is not right because,\nsee, D is a prefix of--",
    "start": "1419370",
    "end": "1429809"
  },
  {
    "text": "D is a prefix of E,\nso clearly no, OK? I hope that's clear.",
    "start": "1429810",
    "end": "1436080"
  },
  {
    "text": "OK, let's compute the\nKraft sum for this guy.",
    "start": "1436080",
    "end": "1442179"
  },
  {
    "text": "So it's like, you\nhave four terms, which are like 1/2 square, because\nyou have four things which",
    "start": "1442180",
    "end": "1449399"
  },
  {
    "text": "have length 2. And then you have one term,\nwhich has length 3, right?",
    "start": "1449400",
    "end": "1456190"
  },
  {
    "text": "So this is just\n2 power minus li. I am summing over\nall the codewords. And if you do the math,\nthis will be more than--",
    "start": "1456190",
    "end": "1465140"
  },
  {
    "text": "it's like 1 plus 1/8. So it's greater than 1, right? So we saw that any\nprefix code has",
    "start": "1465140",
    "end": "1471740"
  },
  {
    "text": "to satisfy Kraft's inequality. So if your code doesn't\nsatisfy Kraft's inequality, it cannot be a\nprefix code, right?",
    "start": "1471740",
    "end": "1476750"
  },
  {
    "text": "So another way to check. OK, just by looking at\nthe lens, you don't even",
    "start": "1476750",
    "end": "1482250"
  },
  {
    "text": "need to see the codewords. Just by looking at\nthe lens, you know that it's not a prefix code. It cannot be a prefix code, OK?",
    "start": "1482250",
    "end": "1488190"
  },
  {
    "text": "Is that clear? That Kraft's inequality\nbasically gives you a way to identify prefix codes.",
    "start": "1488190",
    "end": "1494430"
  },
  {
    "text": "OK, 2.2. So you have these five. Was this a prefix code?",
    "start": "1494430",
    "end": "1500575"
  },
  {
    "text": "OK, people are nodding. Good. Yes, it was a very\neasy to check again. And if you compute the\nKraft's sum, which I will--",
    "start": "1504610",
    "end": "1513450"
  },
  {
    "text": "I think it's 5/8, so this\nis less than or equal to 1, so good. It satisfies the\nKraft's inequality.",
    "start": "1513450",
    "end": "1522090"
  },
  {
    "text": "How about 2.3? Is that a prefix code? Yeah, people are saying no.",
    "start": "1522090",
    "end": "1529060"
  },
  {
    "text": "And they are correct, because\nthese two have the same, so it's a very bad code. OK.",
    "start": "1529060",
    "end": "1535539"
  },
  {
    "text": "Now if you compute the\nKraft sum for this guy, it's like 3 divided by 4 plus 2\ndivided by 8, which is like 1.",
    "start": "1535540",
    "end": "1546289"
  },
  {
    "text": "So how is this possible, right? So this satisfies\nKraft's inequality. The sum is less\nthan or equal to 1.",
    "start": "1546290",
    "end": "1553970"
  },
  {
    "text": "But it's not a prefix code. Can you square that with\nthe results we just saw?",
    "start": "1553970",
    "end": "1558560"
  },
  {
    "text": "You could just\nswitch [INAUDIBLE]..",
    "start": "1561360",
    "end": "1563348"
  },
  {
    "text": "I think you are on\nthe right track. Kraft's inequality,\nwhat it says is that if your lengths satisfy\nthe Kraft's inequality,",
    "start": "1567260",
    "end": "1574490"
  },
  {
    "text": "then you can-- yeah, then a prefix\ncode exists, right? But your particular code\nneed not be a prefix code.",
    "start": "1574490",
    "end": "1580220"
  },
  {
    "text": "You could just make a code,\nwhere everything is zero. So clearly, it's\nnot a prefix code. But you could make a code out\nof just by slight modification",
    "start": "1580220",
    "end": "1589560"
  },
  {
    "text": "of this, just make 01, and\nthen it becomes a prefix code, right? So if Kraft's\ninequality is satisfied,",
    "start": "1589560",
    "end": "1595860"
  },
  {
    "text": "you will get a prefix code. But your particular code\nmay not be that prefix code. It could not-- it might\njust be not a prefix code.",
    "start": "1595860",
    "end": "1605215"
  },
  {
    "text": "OK. OK, good.",
    "start": "1605215",
    "end": "1611179"
  },
  {
    "text": "OK, let's move on\nthe exciting parts. So what will we do?",
    "start": "1611180",
    "end": "1616340"
  },
  {
    "text": "We will talk about some\ninformation theoretic quantities. If you do an information\ntheory course, you will see much more on this.",
    "start": "1616340",
    "end": "1623120"
  },
  {
    "text": "We will just do enough for\ncompression, basically.",
    "start": "1623120",
    "end": "1628309"
  },
  {
    "text": "We will very\nprecisely characterize what is the best prefix code. As we saw, Shannon code is\nclearly not the best prefix",
    "start": "1628310",
    "end": "1634880"
  },
  {
    "text": "code. But it's a good one. OK. And then, we will\njustify the thumb rule, which is that the lengths should\nbe roughly log 1/Px, right?",
    "start": "1634880",
    "end": "1644040"
  },
  {
    "text": "This log 1/Px will keep\ncoming again and again throughout the course. And if you have done\nmachine learning,",
    "start": "1644040",
    "end": "1649799"
  },
  {
    "text": "you will see that it\neven comes up there, where you have these log\nloss terms, and you have--",
    "start": "1649800",
    "end": "1655860"
  },
  {
    "text": "there was a recent paper\nby DeepMind, the Google subsidiary, where the paper is\njust called language modeling",
    "start": "1655860",
    "end": "1663360"
  },
  {
    "text": "compression. And we will see that in the\nlater homeworks, and so on. There is a very deep\nconnection between compression",
    "start": "1663360",
    "end": "1669630"
  },
  {
    "text": "and modeling or\nlearning in general. But, yeah, just this\nbasic thing will just",
    "start": "1669630",
    "end": "1677580"
  },
  {
    "text": "keep talking about it. OK, so first, quantity entropy.",
    "start": "1677580",
    "end": "1684110"
  },
  {
    "text": "How many of you have\nheard about entropy? How many of you have heard about\nentropy in a physics context?",
    "start": "1684110",
    "end": "1692640"
  },
  {
    "text": "Yeah, so many people are\nfamiliar about entropy from physics context.",
    "start": "1692640",
    "end": "1697679"
  },
  {
    "text": "I'm not done physics, so I don't\nknow what it exactly means. So I will tell you what\nit means here, right?",
    "start": "1697680",
    "end": "1703679"
  },
  {
    "text": "So I think it's very\nsimilar, conceptually. OK, so you have a\nalphabet 1 to k.",
    "start": "1703680",
    "end": "1710460"
  },
  {
    "text": "From now on, we will not\ntalk about A, B, C, D. It's just harder to keep track of. Our alphabets will typically\nbe just 1, 2, 3, 4, k.",
    "start": "1710460",
    "end": "1718580"
  },
  {
    "text": "You have a probability\ndistribution over that. And we will use this\nshorthand notation. Pi will be the probability\nthat x is equal to i,",
    "start": "1718580",
    "end": "1725450"
  },
  {
    "text": "just to simplify our notation. Then entropy is this quantity--",
    "start": "1725450",
    "end": "1731970"
  },
  {
    "text": "sum of Pi log 1 by Pi. The unit of entropy is bits. We will see why in a bit.",
    "start": "1731970",
    "end": "1739440"
  },
  {
    "text": "OK, we'll do some examples, and\nthen I will wait for questions.",
    "start": "1739440",
    "end": "1748840"
  },
  {
    "text": "So one thing you notice\nis that entropy only depends on the\nprobability distribution. So entropy is really\nnot a function of x.",
    "start": "1748840",
    "end": "1755408"
  },
  {
    "text": "It's not a function of\nthe random variable. It's a function of\nthe distribution. If you have two random variables\nwith the same distribution,",
    "start": "1755408",
    "end": "1760799"
  },
  {
    "text": "they have the same entropy. It's a function of\nthe distribution.",
    "start": "1760800",
    "end": "1763740"
  },
  {
    "text": "You can write the entropy\nas an expectation, right? Because you see here that--",
    "start": "1766760",
    "end": "1776240"
  },
  {
    "text": "you remember that\nthe expectation is expectation of some\ny is summation Pi y",
    "start": "1776240",
    "end": "1785052"
  },
  {
    "text": "Something like this.",
    "start": "1785052",
    "end": "1785885"
  },
  {
    "text": "OK. So here, like, it just Pi\ntimes log 1 over Px equal to i.",
    "start": "1792390",
    "end": "1800660"
  },
  {
    "text": "Just by the definition\nof expectation, so it's just an easier\nway to write it. And third thing, if you\nhave actual questions",
    "start": "1800660",
    "end": "1808790"
  },
  {
    "text": "on this, if you ever\nrun into a place where you want to\ncompute the entropy or if you want to\nwrite it in Python,",
    "start": "1808790",
    "end": "1815060"
  },
  {
    "text": "like a code to compute entropy,\nwhen p is 0, this term is 0.",
    "start": "1815060",
    "end": "1821390"
  },
  {
    "text": "You can either assume it, or\nyou can use your calculus, take the limit or\nwhatever you want to do,",
    "start": "1821390",
    "end": "1826820"
  },
  {
    "text": "but this is zero, right? Otherwise, you run\ninto not a numbers",
    "start": "1826820",
    "end": "1831860"
  },
  {
    "text": "and so on in your Python script. So that's just\nlike a definition. OK, any questions so far?",
    "start": "1831860",
    "end": "1838362"
  },
  {
    "text": "We'll do some examples. OK, let's do examples.",
    "start": "1838362",
    "end": "1843445"
  },
  {
    "text": "So you have a\nuniform distribution. So your x is uniform.",
    "start": "1846210",
    "end": "1850580"
  },
  {
    "text": "So the x is uniform over 1 to k. So what that means is like\nPx equal to i is just 1",
    "start": "1856130",
    "end": "1864110"
  },
  {
    "text": "over k for all i. Then your entropy of x is--",
    "start": "1864110",
    "end": "1870460"
  },
  {
    "text": "just use the formula\nfrom last slide, so P log 1 over P.\nThat's the formula.",
    "start": "1870460",
    "end": "1878309"
  },
  {
    "text": "And after you do the\nmath, you see log k bits.",
    "start": "1878310",
    "end": "1888350"
  },
  {
    "text": "OK. Is that clear?",
    "start": "1888350",
    "end": "1893120"
  },
  {
    "text": "Good. Have you seen log k\nin any other context for uniform distributions\nin the past?",
    "start": "1898630",
    "end": "1904690"
  },
  {
    "text": "In the last two\nlectures, basically. Let me just remind\nyou, you might remember that log k is actually\nthe length of the fixed length",
    "start": "1907300",
    "end": "1914530"
  },
  {
    "text": "code for-- or I guess the ceiling of log k. Right.",
    "start": "1914530",
    "end": "1919602"
  },
  {
    "text": "So you already see that there is\nsome connection between entropy and the length of a code, and\nwe will explore that shortly.",
    "start": "1919603",
    "end": "1927610"
  },
  {
    "text": "Right. So in particular, if x\nis like Bernoulli half--",
    "start": "1927610",
    "end": "1934650"
  },
  {
    "text": "so Bernoulli random\nvariables, this just means that uniform\nover 0 and 1--",
    "start": "1934650",
    "end": "1941400"
  },
  {
    "text": "then H of x is log2,\nwhich is 1 bit.",
    "start": "1941400",
    "end": "1948975"
  },
  {
    "text": "So if you have a\nrandom variable, which is either 0 or 1 with\nequal probability, then it has 1 bit of entropy.",
    "start": "1948975",
    "end": "1958760"
  },
  {
    "text": "Right. OK.",
    "start": "1958760",
    "end": "1960750"
  },
  {
    "text": "Let us compute it\nfor this one also. Thus, we are very comfortable\nwith the calculations.",
    "start": "1964670",
    "end": "1971550"
  },
  {
    "text": "So 1/2 log 1 over-- you\nwill have some questions in the homework-- in the\nquiz and the homework--",
    "start": "1971550",
    "end": "1978300"
  },
  {
    "text": "times 2. So this is 1/2 log1 plus\n1/4 log2 plus 1/8 times 5",
    "start": "1986150",
    "end": "1996409"
  },
  {
    "text": "plus 1/8 times 3. So 1/2 plus 1/2 plus 6/8, 7/4,\naccording to helpful audience",
    "start": "1996410",
    "end": "2015950"
  },
  {
    "text": "members. OK. So as an exercise,\nwhich I would encourage",
    "start": "2015950",
    "end": "2024780"
  },
  {
    "text": "you to do at home, if\nyou have this code--",
    "start": "2024780",
    "end": "2028835"
  },
  {
    "text": "if you create this code\nfor this distribution, then if you calculate\nthe expected",
    "start": "2039120",
    "end": "2045480"
  },
  {
    "text": "length of the codewords. Yeah, you just\ncompute it at home,",
    "start": "2045480",
    "end": "2050530"
  },
  {
    "text": "and you will find\nthat it's the entropy. So again, you see some\nconnection between the lengths,",
    "start": "2050530",
    "end": "2056349"
  },
  {
    "text": "average length of the\ncodewords, and the entropy.",
    "start": "2056350",
    "end": "2058405"
  },
  {
    "text": "OK, any questions about-- for now, you should just know\nthat given a distribution, you follow this exact formula,\nand you know the entropy.",
    "start": "2061630",
    "end": "2068320"
  },
  {
    "text": "That's it. And we'll see more shortly.",
    "start": "2068320",
    "end": "2071455"
  },
  {
    "text": "Properties of entropy. So for a random variable\nthat takes up to k values,",
    "start": "2074380",
    "end": "2083000"
  },
  {
    "text": "the entropy is\nbetween 0 and log k. We already saw when it's log k.",
    "start": "2083000",
    "end": "2088595"
  },
  {
    "text": "It's log k when it's a\nuniformly distributed. And it is zero when\nit is deterministic.",
    "start": "2088595",
    "end": "2096050"
  },
  {
    "text": "By deterministic, we mean\nthat x takes exactly one value with probability\n1, so it's not random.",
    "start": "2096050",
    "end": "2101420"
  },
  {
    "text": "It's deterministic. So again, for a\nrandom variable that takes values in an\nalphabet of size k,",
    "start": "2101420",
    "end": "2109820"
  },
  {
    "text": "entropy is between 0 and log k. I went to the proofs.",
    "start": "2109820",
    "end": "2115720"
  },
  {
    "text": "The zero proof is very easy. It just depends on the fact that\nprobabilities are less than 1. And the log k proof\nuses convexity",
    "start": "2115720",
    "end": "2123549"
  },
  {
    "text": "and Jensen's inequality. So for those of you who\nare familiar with that, it's a few lines of math.",
    "start": "2123550",
    "end": "2129175"
  },
  {
    "text": "OK, so let's just talk about\nwhat entropy means and so on. So entropy is like--",
    "start": "2133260",
    "end": "2138660"
  },
  {
    "text": "it's like a measure of\nuncertainty or randomness in your random variable.",
    "start": "2138660",
    "end": "2143790"
  },
  {
    "text": "So if your random\nvariable is deterministic, there is no randomness. Entropy is zero, right?",
    "start": "2143790",
    "end": "2149570"
  },
  {
    "text": "If a random variable is\nuniform over all possibilities, then it has the\nmaximum entropy, right?",
    "start": "2149570",
    "end": "2154760"
  },
  {
    "text": "So the more random your\nthing is, the more entropy it will have. Right. So that's just\nlike what it means.",
    "start": "2154760",
    "end": "2161450"
  },
  {
    "text": "Very similar meaning in physics. Another way to think\nabout it is the entropy is the amount of information\ncontained in a message.",
    "start": "2161450",
    "end": "2169790"
  },
  {
    "text": "So for example, let's\nsay you have a friend who sends you messages. And all the messages\nthey send you are hello.",
    "start": "2169790",
    "end": "2177110"
  },
  {
    "text": "They just send the same\nmessage every time. So when you receive the\nmessage, it gives you very little information, right?",
    "start": "2177110",
    "end": "2183260"
  },
  {
    "text": "Some other person-- like,\nsomebody, sometimes, they say hello. Sometimes, they say hi. So maybe you have some\ninformation, right?",
    "start": "2183260",
    "end": "2188720"
  },
  {
    "text": "So the more entropy you have,\nthe more variability you have, more information you can\ncapture in that message.",
    "start": "2188720",
    "end": "2197360"
  },
  {
    "text": "So entropy is the\namount of information that's contained in a random.",
    "start": "2197360",
    "end": "2202490"
  },
  {
    "text": "Deterministic random variable\nhas zero information. And since at the end\nof the day, compression",
    "start": "2202490",
    "end": "2210130"
  },
  {
    "text": "is about storing\nthe information, entropy is also how\nmuch you can compress.",
    "start": "2210130",
    "end": "2215160"
  },
  {
    "text": "And this, we'll see in\nthis class, that entropy is the limit of compression. For a given random\nvariable, the entropy",
    "start": "2215160",
    "end": "2222250"
  },
  {
    "text": "is the best compression\nyou can achieve for that random variable. And that's why the unit is bits.",
    "start": "2222250",
    "end": "2230069"
  },
  {
    "text": "And if you have ever seen these\npuzzles, like I have x coins, and then one of the coin is\nlighter than the other coins,",
    "start": "2230070",
    "end": "2237347"
  },
  {
    "text": "and one of the coins is heavier. And I have a\nweighing scale, and I want to-- how many minimum\nnumber of weighings",
    "start": "2237347",
    "end": "2243359"
  },
  {
    "text": "should I do to find out the\nheavier coins or the lighter coin?",
    "start": "2243360",
    "end": "2249680"
  },
  {
    "text": "Or you have a bunch of\ncontestants in a tournament, and then one of the\ncontestants is stronger.",
    "start": "2249680",
    "end": "2254960"
  },
  {
    "text": "Everybody else is equal. How many tournament\nrounds do I need? So there are a bunch\nof these puzzles.",
    "start": "2254960",
    "end": "2260360"
  },
  {
    "text": "And many times, you will see the\nanswer is related to entropy. And you will see some of\nthose in our homeworks.",
    "start": "2260360",
    "end": "2266390"
  },
  {
    "text": "Entropy is also important for\nsampling from distributions, so you have a-- you\nhave a fair coin,",
    "start": "2268950",
    "end": "2275210"
  },
  {
    "text": "but you want to\nsimulate a biased coin. Or you have a\nbiased coin, and you want to simulate a fair coin.",
    "start": "2275210",
    "end": "2280700"
  },
  {
    "text": "All these situations,\nentropy comes up as the limit of things.",
    "start": "2280700",
    "end": "2288019"
  },
  {
    "text": "I think we will see all\nof these to some extent in the homeworks, so I\nwon't spend class time.",
    "start": "2288020",
    "end": "2294550"
  },
  {
    "text": "OK, any questions so far?",
    "start": "2294550",
    "end": "2299590"
  },
  {
    "text": "So this is mostly\nstandard material. Let's keep going. What is this?",
    "start": "2299590",
    "end": "2305450"
  },
  {
    "text": "Joint entropy of independent\nrandom variables. So if you have two random\nvariables, x1 and x2,",
    "start": "2305450",
    "end": "2312140"
  },
  {
    "text": "which are independent\nof each other, then you can define\ntheir joint entropy.",
    "start": "2312140",
    "end": "2319020"
  },
  {
    "text": "So one thing that's\nuseful to think about, which, in math,\neven programming, everywhere, right,\nyou have two things.",
    "start": "2319020",
    "end": "2324970"
  },
  {
    "text": "But x1 and x2 are\nindividual random variables, but they're pair. The tuple itself is\nanother random variable.",
    "start": "2324970",
    "end": "2331240"
  },
  {
    "text": "So the entropy of the tuple is\njust the entropy of the tuple. You don't even need to worry\nabout the individual random",
    "start": "2331240",
    "end": "2336610"
  },
  {
    "text": "variables, right? You just think of\nthis pair as y. I can just call this whole\nthing as y, and then this is y.",
    "start": "2336610",
    "end": "2344190"
  },
  {
    "text": "So just in terms of\nthinking about, if you see a more complicated\nobject, just think of that whole big\nobject as your y.",
    "start": "2344190",
    "end": "2350184"
  },
  {
    "text": "OK, those who are\nable to follow, how did I get the second\nstep from the first step?",
    "start": "2354540",
    "end": "2360330"
  },
  {
    "text": "Because the two random\nvariables are independent, so then the joint probability\nis just the product.",
    "start": "2363610",
    "end": "2368780"
  },
  {
    "text": "Yep. Yep. Yep.\nYeah. So correct answer is that since\nx1 and x2 are independent,",
    "start": "2368780",
    "end": "2374089"
  },
  {
    "text": "the joint probability\nfactor factors out as the product of the\nindividual probabilities.",
    "start": "2374090",
    "end": "2379700"
  },
  {
    "text": "Great. So we know this, and then\nwe know the log properties.",
    "start": "2379700",
    "end": "2384890"
  },
  {
    "text": "If we have forgotten properties\nof log, please revisit them. They will be helpful.",
    "start": "2384890",
    "end": "2390380"
  },
  {
    "text": "Right. So at the end of\nthe day, you get that the entropy of\nthe two variables",
    "start": "2390380",
    "end": "2396750"
  },
  {
    "text": "is the same as the\nsum of the entropy. So what we are saying is you\nhave two random variables",
    "start": "2396750",
    "end": "2402300"
  },
  {
    "text": "are independent. So the total information\ncontained in them is the sum of their\nindividual informations",
    "start": "2402300",
    "end": "2407880"
  },
  {
    "text": "because they are independent. If they were not independent,\nthen something else will happen. And we'll talk about\nthat in a future lecture.",
    "start": "2407880",
    "end": "2414765"
  },
  {
    "text": "OK, so we already saw\nthe first statement, that if x1 and x2\nare independent, then the joint entropy is\nthe sum of the entropies.",
    "start": "2423220",
    "end": "2431210"
  },
  {
    "text": "How many of you have\nheard the term iid? OK, that's good.",
    "start": "2431210",
    "end": "2436440"
  },
  {
    "text": "Yeah. So iid is a very\ncommon term, which means independent and\nidentically distributed, right? So you have n variables.",
    "start": "2436440",
    "end": "2443490"
  },
  {
    "text": "They are all mutually\nindependent, so not only pairwise, but jointly. They're all mutually\nindependent of each other.",
    "start": "2443490",
    "end": "2449970"
  },
  {
    "text": "And each of them have\nthe same distribution. So we often use this notation. So what this is saying is xn\nis an n-tuple of x1, dot, dot,",
    "start": "2449970",
    "end": "2459059"
  },
  {
    "text": "dot, xn. Each of them have the\nsame distribution as x. x is a placeholder\nrandom variable,",
    "start": "2459060",
    "end": "2464850"
  },
  {
    "text": "which is just denotes\nthe distribution of all of these guys. So you are maybe tossing\na coin n times, right?",
    "start": "2464850",
    "end": "2471090"
  },
  {
    "text": "So each of the coin toss is\nan independent random variable with the same distribution,\nwith some Bernoulli",
    "start": "2471090",
    "end": "2476790"
  },
  {
    "text": "half or Bernoulli P. So\nthen using the same thing",
    "start": "2476790",
    "end": "2481890"
  },
  {
    "text": "inductively, you will get that\nthe entropy of the n-tuple is n times the entropy of\na single random variable.",
    "start": "2481890",
    "end": "2488160"
  },
  {
    "text": "So entropy just\nadds up, if you have independent random variables. If you don't have\nindependent, then we will talk about it in lecture 8.",
    "start": "2488160",
    "end": "2495460"
  },
  {
    "text": "It's a very important use case. But for now, let's\nkeep it simple. Any questions so far?",
    "start": "2495460",
    "end": "2501900"
  },
  {
    "text": "OK. Another quantity. Don't be scared.",
    "start": "2507680",
    "end": "2512980"
  },
  {
    "text": "This is called KL divergence.",
    "start": "2512980",
    "end": "2518859"
  },
  {
    "text": "So this is defined on two random\nvariables-- two probability distributions. You have p, and you have q.",
    "start": "2518860",
    "end": "2525160"
  },
  {
    "text": "Both are on the same alphabet,\nso both are on 1 to k. So p is like p1,\np2, p3, pk, and then",
    "start": "2525160",
    "end": "2532150"
  },
  {
    "text": "q is like q1, q2, qk on\nthe same distribution. So then this\ndivergence or distance",
    "start": "2532150",
    "end": "2539230"
  },
  {
    "text": "or relative entropy between p\nand q is defined as this thing.",
    "start": "2539230",
    "end": "2544850"
  },
  {
    "text": "I will not talk\nabout it for now. Just you will have\nsome examples in quiz.",
    "start": "2544850",
    "end": "2550040"
  },
  {
    "text": "Solve some questions, right? It's just something pi log\npi/qi, the sum of these things,",
    "start": "2550040",
    "end": "2556550"
  },
  {
    "text": "right? Doesn't tell you much,\nbut the properties will tell us much more.",
    "start": "2556550",
    "end": "2562630"
  },
  {
    "text": "OK. OK. There is only one property\nwe need that it is positive.",
    "start": "2562630",
    "end": "2568510"
  },
  {
    "text": "It's greater than or equal to\nzero with equality if and only if p is equal to q. So it is in a way a distance\nbetween two probability",
    "start": "2568510",
    "end": "2576785"
  },
  {
    "text": "distributions, right? If the probability distributions\nare similar, it's smaller. If they are not\nsimilar, it's bigger.",
    "start": "2576785",
    "end": "2582783"
  },
  {
    "text": "So it's a way to describe\nthe difference between two probability distributions. Proof is in lecture notes, uses\nconvexity Jensen's inequality",
    "start": "2582783",
    "end": "2589940"
  },
  {
    "text": "again. Yeah. So just remember this.",
    "start": "2589940",
    "end": "2595560"
  },
  {
    "text": "We will just need this\nproperty one time in our proof later so that's\nwhy I described it.",
    "start": "2595560",
    "end": "2600300"
  },
  {
    "text": "So, yeah, so KL divergence-- Kullback-Leibler,\nthe two people who came up with it--\nor relative entropy",
    "start": "2603460",
    "end": "2609370"
  },
  {
    "text": "is a measure of distance between\ntwo probability distributions. It is not symmetric.",
    "start": "2609370",
    "end": "2615070"
  },
  {
    "text": "So when you think of your\nloss, your functions,",
    "start": "2615070",
    "end": "2620430"
  },
  {
    "text": "typically, like\ndistance functions, like Euclidean distance or l1,\nl2, l0, something, most of them",
    "start": "2620430",
    "end": "2627540"
  },
  {
    "text": "are symmetric. So between x and y, and y\nand x, it doesn't matter. Here, it does matter. You will see it in your quiz.",
    "start": "2627540",
    "end": "2634140"
  },
  {
    "text": "So this is, in general,\nit's not equal to D qp.",
    "start": "2634140",
    "end": "2643079"
  },
  {
    "text": "It's fine. It's just a fact.",
    "start": "2645740",
    "end": "2648380"
  },
  {
    "text": "How many of you have\nseen it actually? Some of you might have\nseen it in your machine learning classes.",
    "start": "2651290",
    "end": "2656869"
  },
  {
    "text": "If you have done\ngenerative models, especially, this comes up a lot.",
    "start": "2656870",
    "end": "2662220"
  },
  {
    "text": "And why does it come up a lot? Because this is a distance\nbetween probability distributions, right? So all of these\ngenerative models,",
    "start": "2662220",
    "end": "2668090"
  },
  {
    "text": "what are they trying to do? They're trying to your LLMs\nand diffusion models and so on.",
    "start": "2668090",
    "end": "2673700"
  },
  {
    "text": "They are trying to\ngenerate samples, which are similar to the\nreal distribution, right?",
    "start": "2673700",
    "end": "2679890"
  },
  {
    "text": "They're trying to\ncreate human-like text. They are trying to create\nartistic-like images.",
    "start": "2679890",
    "end": "2686830"
  },
  {
    "text": "But to compute the\nloss, like, how far they are from the real thing? You need to have a\nloss function, which",
    "start": "2686830",
    "end": "2692800"
  },
  {
    "text": "determines how different two\nprobability distributions are. And for various reasons,\nincluding the fact",
    "start": "2692800",
    "end": "2698080"
  },
  {
    "text": "that it's differentiable\nand everything, KL divergence is\nthe loss function of choice in many, many cases.",
    "start": "2698080",
    "end": "2704470"
  },
  {
    "text": "So I was just mentioning it. You don't need it for now. But, yeah.",
    "start": "2704470",
    "end": "2710540"
  },
  {
    "text": "And it has some\ninterpretation in compression, which we might talk about. It's in the notes, definitely.",
    "start": "2710540",
    "end": "2718190"
  },
  {
    "text": "OK. We will move on from this. For now, just remember\nthis one thing that the KL divergence\nis greater than or equal",
    "start": "2718190",
    "end": "2724450"
  },
  {
    "text": "to zero with\nequality if and only if the two distributions\nare equal, right?",
    "start": "2724450",
    "end": "2730000"
  },
  {
    "text": "Rest of it is just\ngeneral knowledge. OK, so we are here.",
    "start": "2730000",
    "end": "2737039"
  },
  {
    "text": "So the main result of\nlossless compression, right? So information theory\nis one of the fields",
    "start": "2737040",
    "end": "2742700"
  },
  {
    "text": "where you will see results\nwhich are very, very precise. You know exactly\nwhat's happening.",
    "start": "2742700",
    "end": "2748010"
  },
  {
    "text": "For every prefix\ncode in the world, the expected length is greater\nthan or equal to the entropy.",
    "start": "2748010",
    "end": "2756050"
  },
  {
    "text": "There is no prefix code for\nany distribution, right? There is no prefix code\nthat can beat entropy.",
    "start": "2756050",
    "end": "2761780"
  },
  {
    "text": "You cannot go\nbetter than entropy. Entropy is the\nfundamental lower bound. So this result is\ncalled a lower bound,",
    "start": "2761780",
    "end": "2767870"
  },
  {
    "text": "or it's called a converse. And not only this. If I just give this and\nthen I didn't give you",
    "start": "2767870",
    "end": "2775220"
  },
  {
    "text": "what can you do, this would\nbe-- this is a negative result, anyway, right? This just tells you can't\ndo better than entropy.",
    "start": "2775220",
    "end": "2780832"
  },
  {
    "text": "But there is another result.\nYou can achieve entropy. You can get arbitrarily\nclose to entropy.",
    "start": "2780832",
    "end": "2788300"
  },
  {
    "text": "And we will talk about\nmaybe today or next lecture about what do you mean\nby arbitrarily close",
    "start": "2788300",
    "end": "2795190"
  },
  {
    "text": "with prefix codes. So this is the\nachievability part. I will stop here\nfor a minute and let",
    "start": "2795190",
    "end": "2802060"
  },
  {
    "text": "you digest OK, any questions?",
    "start": "2802060",
    "end": "2825160"
  },
  {
    "text": "Good. Right.",
    "start": "2828460",
    "end": "2833920"
  },
  {
    "text": "So what does mean? This means that entropy\nis the fundamental limit of lossless compression.",
    "start": "2833920",
    "end": "2839080"
  },
  {
    "text": "It's a very powerful result.\nIn the next several lectures, what we will be doing is trying\nto actually achieve entropy,",
    "start": "2839080",
    "end": "2846070"
  },
  {
    "text": "all of the work, basically,\nlossless compression. So Shannon, in his 1948\npaper, he showed this result.",
    "start": "2846070",
    "end": "2852110"
  },
  {
    "text": "He showed that you can't\ndo better than entropy. And next 50 years-- even as\nsoon as like 2014 or something,",
    "start": "2852110",
    "end": "2859480"
  },
  {
    "text": "people are still trying to\nfind more and more efficient ways to achieve entropy. All of your lossless\nand lossy compressors,",
    "start": "2859480",
    "end": "2869020"
  },
  {
    "text": "at their very root,\nhave something called an entropy coder. Entropy coder is\nsomething that is just",
    "start": "2869020",
    "end": "2874270"
  },
  {
    "text": "trying to achieve the entropy\nof whatever the input is. You have Huffman codes, and\nyou have arithmetic codes.",
    "start": "2874270",
    "end": "2880540"
  },
  {
    "text": "You have ANS. You have this and a\nbunch of different ideas. All of these, trying\nto just do this,",
    "start": "2880540",
    "end": "2886030"
  },
  {
    "text": "try to achieve entropy\nin an efficient way.",
    "start": "2886030",
    "end": "2888550"
  },
  {
    "text": "So after Shannon\nproved this result, I think it was almost\n30 years before somebody",
    "start": "2891510",
    "end": "2897127"
  },
  {
    "text": "was able to come up\nwith a method that was linear time, a linear time\nin computational complexity",
    "start": "2897127",
    "end": "2903660"
  },
  {
    "text": "way of achieving entropy. And then more recently\npeople have improved that.",
    "start": "2903660",
    "end": "2909299"
  },
  {
    "text": "It's still linear time, but\nbetter for modern hardware. OK, same result applies\nto the more general class",
    "start": "2909300",
    "end": "2918870"
  },
  {
    "text": "of uniquely decodable codes. So if you noticed\nin the last lecture, I said for every prefix code,\nyou cannot beat entropy.",
    "start": "2918870",
    "end": "2926010"
  },
  {
    "text": "But actually, for any lossless\ncode, any uniquely decodable code, you can't beat entropy. Entropy is the fundamental\nbound of lossless",
    "start": "2926010",
    "end": "2931893"
  },
  {
    "text": "compression, whether or not\nyou want to use prefix codes. This, you will prove in--",
    "start": "2931893",
    "end": "2936960"
  },
  {
    "text": "this part, you will\nprove in the homework.",
    "start": "2936960",
    "end": "2938865"
  },
  {
    "text": "And the last point is that, as\nI mentioned a little bit before, we assume here that your things\nare independent identically",
    "start": "2942890",
    "end": "2949610"
  },
  {
    "text": "distributed. Don't think that this\nis only useful in very",
    "start": "2949610",
    "end": "2955410"
  },
  {
    "text": "specific settings. No, this is useful\neverywhere, and we will see it again and again. However, more\nspecifically, we will",
    "start": "2955410",
    "end": "2961650"
  },
  {
    "text": "study more general distributions\nin lecture 8 to 10, so a bit later. For now, we'll just\nstick to the iid case.",
    "start": "2961650",
    "end": "2967859"
  },
  {
    "text": "OK, any questions so far? We'll now go to the\nproof of this result.",
    "start": "2970520",
    "end": "2978220"
  },
  {
    "text": "I think the proof is not as\nexciting I think really want. If you understand this\nresult, the enormity of this,",
    "start": "2978220",
    "end": "2984280"
  },
  {
    "text": "I think, that's the main\npoint, the proof-proof. We will do the proof.",
    "start": "2984280",
    "end": "2989487"
  },
  {
    "text": "Some of you will like it. Some of you will not. OK, let's do the proof.",
    "start": "2989488",
    "end": "2995720"
  },
  {
    "text": "Actually, proof will give\nyou something very useful, so let's do it. OK, so first, we will\nprove the converse.",
    "start": "2999010",
    "end": "3005970"
  },
  {
    "text": "Converse is that you cannot\ndo better than the entropy. Any prefix code in\nthe world, well,",
    "start": "3005970",
    "end": "3012950"
  },
  {
    "text": "cannot have expected code length\nthat is lower than the entropy. So we start with\nour usual regime.",
    "start": "3012950",
    "end": "3019840"
  },
  {
    "text": "We assume that x is distributed\naccording to some distribution p. And you have a prefix\ncode with certain lengths.",
    "start": "3019840",
    "end": "3027640"
  },
  {
    "text": "And what we will try\nto show is that-- so what are we trying to show? Let's write it down so\nthat it's very explicit.",
    "start": "3027640",
    "end": "3034390"
  },
  {
    "text": "We are trying to show that\nsummation i equal to 1 to k,",
    "start": "3034390",
    "end": "3043220"
  },
  {
    "text": "pi li is greater than or\nequal to the entropy of p.",
    "start": "3043220",
    "end": "3050035"
  },
  {
    "text": "OK.",
    "start": "3050035",
    "end": "3050535"
  },
  {
    "text": "OK, so now some of the\nproof will be very like, why are we doing this? But we will do it anyway.",
    "start": "3055120",
    "end": "3060380"
  },
  {
    "text": "So now let's define qi as\nanother distribution, where qi is defined as c\ntimes 2 power minus li,",
    "start": "3060380",
    "end": "3071320"
  },
  {
    "text": "where c is defined\nas this quantity. So here, you see I wrote that\nc is equal to 1 over summation",
    "start": "3071320",
    "end": "3081210"
  },
  {
    "text": "to power minus li is\ngreater than or equal to 1. Can somebody explain\nwhy this holds?",
    "start": "3081210",
    "end": "3086250"
  },
  {
    "text": "Isn't that just like from\nthe Kraft's inequality? This one over that? Yes.",
    "start": "3091640",
    "end": "3097010"
  },
  {
    "text": "Yes. So the answer is that this\nis Kraft's inequality, right? So we said that we will\nprove this for prefix codes.",
    "start": "3097010",
    "end": "3102890"
  },
  {
    "text": "But somehow, somewhere,\nwe need to use the fact that it's a prefix code, right? It's clearly not\ntrue for general.",
    "start": "3102890",
    "end": "3108859"
  },
  {
    "text": "If you didn't want\na lossless code, you could just have a code\nwhere every length is zero or every length is one.",
    "start": "3108860",
    "end": "3114080"
  },
  {
    "text": "So lossless codes can't\nbe better than entropy. You're like, non-lossless codes\ncan be better than entropy.",
    "start": "3114080",
    "end": "3122300"
  },
  {
    "text": "So this is where we will use the\nfact that it is a prefix code. A prefix code, we saw that\nit's like an if and only",
    "start": "3122300",
    "end": "3129387"
  },
  {
    "text": "if condition, right? Prefix codes if and only\nif Kraft's inequality. So we use this here.",
    "start": "3129387",
    "end": "3134930"
  },
  {
    "text": "OK. And then we do a\nquick check, right? The c is a normalization\nfactor, right?",
    "start": "3134930",
    "end": "3140560"
  },
  {
    "text": "Oftentimes, you have\nsomething, and then you want to convert it into a\nprobability distribution. So you divide it by the sum so\nthat it becomes a probability",
    "start": "3140560",
    "end": "3148838"
  },
  {
    "text": "distribution. So that's what we did here. So summation of q is just like\nthis thing, where you can--",
    "start": "3148838",
    "end": "3154450"
  },
  {
    "text": "after you substitute\nthe value of c, you will very easily check\nthat the sum is actually one because the terms\nwill cancel out.",
    "start": "3154450",
    "end": "3162320"
  },
  {
    "text": "OK, so with all\nthis, what we did was we already had a\nprobability distribution p.",
    "start": "3162320",
    "end": "3167809"
  },
  {
    "text": "And now, we created a new\nprobability distribution q, where q has all\nthese lengths inside it.",
    "start": "3167810",
    "end": "3174520"
  },
  {
    "text": "So now we have\ntwo distributions, and we just read about a way\nof using two distributions.",
    "start": "3174520",
    "end": "3180759"
  },
  {
    "text": "We will use the KL divergence. And we already saw the\nproperty that KL divergence is",
    "start": "3180760",
    "end": "3187690"
  },
  {
    "text": "greater than or equal to zero. So let's just use it. And again, I didn't prove this\nquantity, but I'm not cheating.",
    "start": "3187690",
    "end": "3196090"
  },
  {
    "text": "You can read it. It's like a four-line proof. You just need to know Jensen's\ninequality from convexity.",
    "start": "3196090",
    "end": "3201790"
  },
  {
    "text": "So, yeah, I hope you\ncan trust me with this. OK, so now, math, math,\njust moving around terms.",
    "start": "3201790",
    "end": "3211460"
  },
  {
    "text": "I won't do it. I think it-- read it\nat home to understand",
    "start": "3214350",
    "end": "3219510"
  },
  {
    "text": "some of the moving of\nvariables from here to there. Let me focus on the main parts. So here, you write it down.",
    "start": "3219510",
    "end": "3225359"
  },
  {
    "text": "Write down the KL divergence. It's greater than\nor equal to zero. Good. Then, next step,\nlet's write it here.",
    "start": "3225360",
    "end": "3234200"
  },
  {
    "text": "So you basically\nsplit for the log. Very simple operation, right?",
    "start": "3234200",
    "end": "3240250"
  },
  {
    "text": "You move out the-- you move out the qi term\nhere in the second term, OK?",
    "start": "3240250",
    "end": "3247960"
  },
  {
    "text": "Now this term, this\nleft term is actually",
    "start": "3247960",
    "end": "3256180"
  },
  {
    "text": "the negative of the entropy. If you remember the definition\nof entropy was Pi log 1 by Pi.",
    "start": "3256180",
    "end": "3261309"
  },
  {
    "text": "So here, you have\nnot the denominator. So it's the negative\nof the entropy.",
    "start": "3261310",
    "end": "3267355"
  },
  {
    "text": "Sorry. OK.",
    "start": "3267355",
    "end": "3272720"
  },
  {
    "text": "So from here to here, you move\nH(x) to right-hand side, right?",
    "start": "3272720",
    "end": "3286210"
  },
  {
    "text": "And I would encourage you to\ngo over it in piece at home.",
    "start": "3286210",
    "end": "3289464"
  },
  {
    "text": "Then we had this term with qi. We substitute qi here. So you get this c times\n2 to the minus li.",
    "start": "3292370",
    "end": "3300005"
  },
  {
    "text": "Then you have expectation.",
    "start": "3303730",
    "end": "3305290"
  },
  {
    "text": "You can basically open up\nthis log again, split the log, and let's not do this.",
    "start": "3309090",
    "end": "3316109"
  },
  {
    "text": "Let's just focus on the result. After you do all of this,\nwhat you will get is-- you will get that\nexpectation of the length",
    "start": "3316110",
    "end": "3323700"
  },
  {
    "text": "is equal to this, which is\ngreater than this term because of Kraft's inequality,\nwhich is greater",
    "start": "3323700",
    "end": "3330030"
  },
  {
    "text": "than or equal to entropy. So long story short,\nthis is what you get. This is the main result. That expected length is greater\nthan or equal to entropy,",
    "start": "3330030",
    "end": "3337750"
  },
  {
    "text": "right? Don't worry about\nthe specific stuff. Really, this is what\nwe care about, right?",
    "start": "3337750",
    "end": "3344680"
  },
  {
    "text": "We get the result. It's just\nmathematical manipulations plus Kraft's inequality\nplus KL divergence positive.",
    "start": "3344680",
    "end": "3351250"
  },
  {
    "text": "These are the three\nthings we used. Now oftentimes, like when\nyou prove an inequality,",
    "start": "3351250",
    "end": "3357720"
  },
  {
    "text": "you want to see when\ndoes equality hold. So let's do that.",
    "start": "3357720",
    "end": "3362849"
  },
  {
    "text": "If you follow all the\ncalculations carefully, you will see that there are\ntwo conditions you need. One is that c must\nbe equal to 1.",
    "start": "3362850",
    "end": "3370310"
  },
  {
    "text": "So now, let's go\nback to what c was. So this was c.",
    "start": "3370310",
    "end": "3376380"
  },
  {
    "text": "So what does see\nbeing equal to 1 mean? It means that Kraft's\ninequality must be satisfied with equality, right?",
    "start": "3376380",
    "end": "3384080"
  },
  {
    "text": "We saw before that-- we\nsaw two chords before. One was a suboptimal\nchord, and we saw Kraft's inequality was\nnot satisfied with equality.",
    "start": "3384080",
    "end": "3390693"
  },
  {
    "text": "And then we saw an optimal\nchord, where Kraft's inequality was satisfied with equality. So basically, for an optimal\nchord that achieves entropy,",
    "start": "3390693",
    "end": "3398690"
  },
  {
    "text": "c has to be 1. So you have to satisfy\nKraft's inequality. So let's write it down.",
    "start": "3398690",
    "end": "3404930"
  },
  {
    "text": "So Kraft's with equality.",
    "start": "3404930",
    "end": "3412309"
  },
  {
    "text": "OK. And the second thing,\nyou get out of the thing is that pi must be equal to\nqi, which basically gives you",
    "start": "3415360",
    "end": "3423819"
  },
  {
    "text": "the thumb rule.",
    "start": "3423820",
    "end": "3424465"
  },
  {
    "text": "So you can achieve\nentropy, if you can satisfy Kraft's inequality\nand you follow the thumb rule.",
    "start": "3429230",
    "end": "3435680"
  },
  {
    "text": "You follow that length\nis log 1 over pi. So that is the fact,\nbasically, yeah.",
    "start": "3435680",
    "end": "3443350"
  },
  {
    "text": "So you cannot do\nbetter than entropy, and you can do equal to\nentropy, if you satisfy Kraft's inequality and\nyour lengths are log 1/p.",
    "start": "3443350",
    "end": "3452350"
  },
  {
    "text": "So you satisfy the thumb\nrule we saw before. OK, let me stop here.",
    "start": "3452350",
    "end": "3458775"
  },
  {
    "text": "And yeah, read it at home. It's easy.",
    "start": "3462070",
    "end": "3467500"
  },
  {
    "text": "I don't read it. That's fine. Just remember the results.",
    "start": "3467500",
    "end": "3470559"
  },
  {
    "text": "I'm just curious. Those two things, like\nclass and equality being satisfied in\nthe equality and then",
    "start": "3474300",
    "end": "3481920"
  },
  {
    "text": "the probabilities being. Yeah. Like what are those two\nthings like almost the same.",
    "start": "3481920",
    "end": "3488440"
  },
  {
    "text": "Because I imagine if\nthe probabilities aren't exactly log2, then\nclass inequality",
    "start": "3488440",
    "end": "3493510"
  },
  {
    "text": "wouldn't be satisfied. Like, could you\nsatisfy class equality and still not have them be like\nperfect log2 probabilities?",
    "start": "3493510",
    "end": "3502000"
  },
  {
    "text": "Like, others do the same,\nor are they different? That's a very good question.",
    "start": "3502000",
    "end": "3506140"
  },
  {
    "text": "So one thing to notice is that\nKraft's inequality is purely a property about the lengths.",
    "start": "3509030",
    "end": "3515059"
  },
  {
    "text": "There is no probability\nin there, right? Kraft's inequality\nis just a property about the lengths\nof the prefix code.",
    "start": "3515060",
    "end": "3522210"
  },
  {
    "text": "So it is independent of\nany distribution, right? So if you have a code\nthat doesn't satisfy",
    "start": "3522210",
    "end": "3527339"
  },
  {
    "text": "Kraft's inequality\nwith equality, it cannot be the optimal\ncode for any distribution.",
    "start": "3527340",
    "end": "3533579"
  },
  {
    "text": "It's just a bad code, period. So that's one thing\nto think about.",
    "start": "3533580",
    "end": "3539069"
  },
  {
    "text": "However, the thing you said is\nthat for a given distribution, if you do not have\nli equal to log 1/pi,",
    "start": "3539070",
    "end": "3546000"
  },
  {
    "text": "if you have lis which are bigger\nthan that, then, yes, you will not satisfy Kraft's inequality.",
    "start": "3546000",
    "end": "3550320"
  },
  {
    "text": "No, I take it back. No, you can still satisfy\nKraft's inequality because-- OK, maybe it's worthwhile\ndoing a quick example here.",
    "start": "3552970",
    "end": "3560780"
  },
  {
    "text": "OK, so the question was, the\ntwo conditions for equality, are they the same\none and the same, or are they different, right?",
    "start": "3573170",
    "end": "3578990"
  },
  {
    "text": "So basically, so Kraft's\nhas no p, basically, right?",
    "start": "3578990",
    "end": "3587369"
  },
  {
    "text": "It's just a property\nabout l, right? So now, if I make a code,\nwhich is like this, yep.",
    "start": "3587370",
    "end": "3600550"
  },
  {
    "text": "OK.",
    "start": "3600550",
    "end": "3601050"
  },
  {
    "text": "So we already checked\nbefore that this satisfies Kraft's inequality, right? So this here, like\nsummation 2 power minus",
    "start": "3615110",
    "end": "3621710"
  },
  {
    "text": "l, you can easily\nverify that this is 1. OK. Now, if you take\nA, B, C, D, and you",
    "start": "3621710",
    "end": "3626968"
  },
  {
    "text": "take the probability\ndistribution, which is like, this is 1/8. This is 1/8. This is 1/4.",
    "start": "3626968",
    "end": "3632340"
  },
  {
    "text": "By This is 1/2. You see, you can see that this\nis not the right code because D",
    "start": "3632340",
    "end": "3641720"
  },
  {
    "text": "has the smallest probability,\nthe highest probability, but you are giving it a\nvery long codeword, right?",
    "start": "3641720",
    "end": "3646790"
  },
  {
    "text": "So here you see that li is not--",
    "start": "3646790",
    "end": "3649175"
  },
  {
    "text": "li is not equal to log 1/pi.",
    "start": "3654600",
    "end": "3657060"
  },
  {
    "text": "But it does satisfy\nthe Kraft's inequality. So you do need both properties.",
    "start": "3660590",
    "end": "3665810"
  },
  {
    "text": "Yeah, that makes a lot of sense. Yeah. Yeah. Yeah. But basically, Kraft's is\njust about the lengths,",
    "start": "3665810",
    "end": "3672640"
  },
  {
    "text": "and whether it's an optimal\ncode in that sense, right? However, I think there is\nthe opposite side, which I should mention--",
    "start": "3672640",
    "end": "3678325"
  },
  {
    "text": "then, we move on-- is that if li is indeed\nequal to log 1/pi, then",
    "start": "3683550",
    "end": "3692670"
  },
  {
    "text": "you can do it very easily, some\ncalculation, which is like you do 2 power minus li, right? And then you put it\nin log 1 over 1/pi.",
    "start": "3692670",
    "end": "3703710"
  },
  {
    "text": "And you do the 2\npower log thing. You get pi, and\nthis is equal to 1.",
    "start": "3703710",
    "end": "3714340"
  },
  {
    "text": "So this implies that\nKraft's with equality.",
    "start": "3714340",
    "end": "3720580"
  },
  {
    "text": "All right, so one of them\nimplies the other not. It's a situation. I think that's what I was\nlike thinking about one",
    "start": "3724510",
    "end": "3730705"
  },
  {
    "text": "would mean the other one, but it\ndoesn't go with the backwards. Yeah.\nYeah. Yeah. Yeah.",
    "start": "3730705",
    "end": "3737020"
  },
  {
    "text": "OK, any other questions? OK.",
    "start": "3737020",
    "end": "3743640"
  },
  {
    "text": "Now the other part. We said that not\nonly is entropy-- you can't do better\nthan entropy,",
    "start": "3743640",
    "end": "3749280"
  },
  {
    "text": "but you can do as\ngood as entropy. So this is the\ntopic of discussion",
    "start": "3749280",
    "end": "3755517"
  },
  {
    "text": "for next several lectures. Today, we will see the\nstart of the discussion.",
    "start": "3755517",
    "end": "3759360"
  },
  {
    "text": "We already saw\nShannon codes, which achieved this length, right? And we saw that if you achieve\nthese lengths, log 1/Px,",
    "start": "3761890",
    "end": "3768040"
  },
  {
    "text": "you are going to be\nvery close to entropy. Just in the last\nfew slides we saw. So now let's make\nit very formal.",
    "start": "3768040",
    "end": "3775510"
  },
  {
    "text": "So if you have lengths\nequal to log 1/px ceiling, then the expected length\nis this summation pi",
    "start": "3775510",
    "end": "3783369"
  },
  {
    "text": "times li, which is\nsummation pi times ceiling function of log 1/px, OK?",
    "start": "3783370",
    "end": "3788859"
  },
  {
    "text": "1/pi, yep. Now this is just a general\nthing about ceiling functions,",
    "start": "3794800",
    "end": "3802370"
  },
  {
    "text": "which you may or\nmay not have seen, which is that ceiling\nfunction of x is bigger than 1 or equal to x, and it's\nless than x plus 1.",
    "start": "3802370",
    "end": "3809119"
  },
  {
    "text": "So it's somewhere\nbetween these two things. So you can put the second part. We just need the second part,\nthe ceiling is less than x",
    "start": "3809120",
    "end": "3816230"
  },
  {
    "text": "plus 1. So you get that the expected\nlength is less than pi times log 1/pi plus 1.",
    "start": "3816230",
    "end": "3823520"
  },
  {
    "text": "So you get this\nplus a second term. What is the second term? Just what is this term?",
    "start": "3823520",
    "end": "3830170"
  },
  {
    "text": "Summation 1/pi. So just summation-- 1, right? Because sum of\nprobabilities, sum to 1.",
    "start": "3830170",
    "end": "3836740"
  },
  {
    "text": "So what we have shown is\nthat for Shannon codes, the expected length is\nless than H(x) plus 1.",
    "start": "3836740",
    "end": "3844690"
  },
  {
    "text": "And we already know for any\nprefix code expected length cannot be more than 1,\nmore than X So therefore,",
    "start": "3847720",
    "end": "3854380"
  },
  {
    "text": "we have found codes, which are\nin this very narrow window,",
    "start": "3854380",
    "end": "3860049"
  },
  {
    "text": "between Hx and Hx plus 1, right? So we have already gotten\nsuper close to the entropy.",
    "start": "3860050",
    "end": "3865060"
  },
  {
    "text": "We are like one bit\naway from entropy.",
    "start": "3865060",
    "end": "3867430"
  },
  {
    "text": "Is that OK? OK.",
    "start": "3870730",
    "end": "3875840"
  },
  {
    "text": "And there is something\ncalled a dyadic distribution, where all the probabilities\nare powers of two.",
    "start": "3880790",
    "end": "3887350"
  },
  {
    "text": "We have been seeing some\nof those in our examples. So when you have things\nthat are all powers of two, then your logs become\nperfect integers.",
    "start": "3887350",
    "end": "3894400"
  },
  {
    "text": "Your ceiling functions\nare equal to your x's. And for those very\nspecial cases,",
    "start": "3894400",
    "end": "3900369"
  },
  {
    "text": "you have the expected length\nactually equal to the entropy. So you can exactly\nachieve entropy",
    "start": "3900370",
    "end": "3905680"
  },
  {
    "text": "for these dyadic distributions. And actually, you can't\nachieve entropy exactly for any other distribution.",
    "start": "3905680",
    "end": "3911730"
  },
  {
    "text": "So dyadic distributions are\nreally the only distributions. So dyadic just means\nall probabilities are powers of two, like 1/2,\n1/4, that sort of thing.",
    "start": "3911730",
    "end": "3920312"
  },
  {
    "text": "But you can get very,\nvery close as we'll see in the next or\ntwo days later, OK. So Shannon code is\nwithin 1 bit of entropy.",
    "start": "3920312",
    "end": "3928410"
  },
  {
    "text": "That's what we showed. Now some of you\nwho might have been like observing\nvery closely, might",
    "start": "3928410",
    "end": "3934950"
  },
  {
    "text": "remember that I promised\nthat we will get arbitrarily close to the entropy, right?",
    "start": "3934950",
    "end": "3941059"
  },
  {
    "text": "This doesn't seem like that. We are 1 bit away from entropy. If your entropy is like\n0.00001, then I am like 1.00001.",
    "start": "3941060",
    "end": "3952250"
  },
  {
    "text": "I am like thousands of\ntimes more than entropy. I'm very, very far\naway from entropy.",
    "start": "3952250",
    "end": "3957260"
  },
  {
    "text": "So even though this looks like a\nvery powerful thing, it is not. There is still like a\nlong way to go, right?",
    "start": "3957260",
    "end": "3965030"
  },
  {
    "text": "So, yeah, it is not\nquite arbitrarily close. This is just not true.",
    "start": "3965030",
    "end": "3971670"
  },
  {
    "text": "OK, so now I will ask you\nfor help on an exercise. So you have a uniform\ndistribution over 1, 2, 3.",
    "start": "3971670",
    "end": "3977700"
  },
  {
    "text": "Just three things\nuniform distribution. What is the entropy of\nthis uniform distribution?",
    "start": "3977700",
    "end": "3983130"
  },
  {
    "text": "Why is it log 3? OK, because it's a uniform\ndistribution, right?",
    "start": "3992330",
    "end": "3998730"
  },
  {
    "text": "We saw before that for a uniform\ndistribution over k symbols. It's log k. So for three\nsymbols, it's log 3.",
    "start": "3998730",
    "end": "4004940"
  },
  {
    "text": "Can somebody compute\nlog 3 for me? Log 3, base 2.",
    "start": "4004940",
    "end": "4009170"
  },
  {
    "text": "Did you calculate,\nor do you remember? OK, nice. So I had 1.58 bits roughly.",
    "start": "4015100",
    "end": "4024552"
  },
  {
    "text": "I think that's correct. OK. Nice.",
    "start": "4024552",
    "end": "4027340"
  },
  {
    "text": "So let's make a Shannon code. We have time. Let's make a small--",
    "start": "4032710",
    "end": "4038430"
  },
  {
    "text": "so for Shannon code, the symbols\nwill have length this log 3",
    "start": "4038430",
    "end": "4045530"
  },
  {
    "text": "ceiling, so which is 2. So a Shannon code might look\nlike this, where you have A, B,",
    "start": "4045530",
    "end": "4053359"
  },
  {
    "text": "C is 00. B is 01. C is 10.",
    "start": "4053360",
    "end": "4061650"
  },
  {
    "text": "What is the expected code\nlength of this Shannon code?",
    "start": "4061650",
    "end": "4064515"
  },
  {
    "text": "So the expected length is 2\nbecause all of the codewords have length 2, right?",
    "start": "4071790",
    "end": "4076950"
  },
  {
    "text": "Here, the Shannon code is\njust the fixed length code we saw a long time ago\nin the first lecture.",
    "start": "4076950",
    "end": "4083310"
  },
  {
    "text": "OK, so you already see\nthis big overhead, right? 1.582 and the bound is 2.58.",
    "start": "4083310",
    "end": "4089700"
  },
  {
    "text": "It is like x plus 1. So clearly, the Shannon code\nis between entropy and entropy",
    "start": "4089700",
    "end": "4094769"
  },
  {
    "text": "plus 1. But it's worse than entropy. It's like roughly 25%\nworse than entropy.",
    "start": "4094770",
    "end": "4104200"
  },
  {
    "text": "There is a gap. There is a gap from\nentropy to Shannon codes.",
    "start": "4104200",
    "end": "4107804"
  },
  {
    "text": "Do you think this code is\noptimal for this distribution?",
    "start": "4110910",
    "end": "4113370"
  },
  {
    "text": "Let me make a new slide. Let me ask you. So yeah, let's do\nthis properly so",
    "start": "4118120",
    "end": "4125670"
  },
  {
    "text": "that we are on the same page. So we have this. So we have x. We have p of x.",
    "start": "4125670",
    "end": "4131189"
  },
  {
    "text": "We have log 1 over p of x. So 2, 2, 2.",
    "start": "4131189",
    "end": "4139170"
  },
  {
    "text": "And then we made this codeword,\ncx, which is like 00, 01, 10,",
    "start": "4139170",
    "end": "4146949"
  },
  {
    "text": "we can make a better\ncodeword, right? Better code.",
    "start": "4146950",
    "end": "4152470"
  },
  {
    "text": "And I guess for the\ncode what we saw, for this the expected\ncode length was 2.",
    "start": "4152470",
    "end": "4162589"
  },
  {
    "text": "Let's make a better code. So if we draw--",
    "start": "4162590",
    "end": "4168270"
  },
  {
    "text": "let me just make it. So it's 00, 01. We don't need to\nhave this 0 after 1.",
    "start": "4168270",
    "end": "4174689"
  },
  {
    "text": "I can just make it a 1. This is still prefix-free.",
    "start": "4174689",
    "end": "4179960"
  },
  {
    "text": "And we computed entropy\nwas 1.58 before. So what is the entropy?",
    "start": "4179960",
    "end": "4186120"
  },
  {
    "text": "What is the expected\ncode length of this guy? 00, 01, 1.",
    "start": "4186120",
    "end": "4189390"
  },
  {
    "text": "A few people say 1.67. Let me just check. So it's 2/3 plus 2/3 plus 1/3.",
    "start": "4195880",
    "end": "4204770"
  },
  {
    "text": "So yeah, it's 1.67. Nice.",
    "start": "4204770",
    "end": "4210770"
  },
  {
    "text": "See, we got close. We got closer. We were at 2 bits. Now, we are at 1.67.",
    "start": "4210770",
    "end": "4218270"
  },
  {
    "text": "So this code that you\nsee on the far right, it's a Huffman code\nfor this distribution,",
    "start": "4218270",
    "end": "4225020"
  },
  {
    "text": "and that's our next lecture. It is the optimum code\nfor this distribution. You cannot do better than this.",
    "start": "4225020",
    "end": "4230615"
  },
  {
    "text": "But it's not still at entropy\nyou might observe, right? Still, 1.67 versus 1.5. There is a gap.",
    "start": "4234130",
    "end": "4240280"
  },
  {
    "text": "Anybody has ideas how\nwe can remove this gap? Is it like for every\nepsilon that you--",
    "start": "4240280",
    "end": "4249020"
  },
  {
    "text": "for like every\ndegree of closeness, there exists a length\nof your alphabet,",
    "start": "4249020",
    "end": "4254630"
  },
  {
    "text": "so you would have to have\na much longer alphabet? But this is the\nalphabet you are given.",
    "start": "4254630",
    "end": "4260030"
  },
  {
    "text": "How can you create a bigger\nalphabet starting from this?",
    "start": "4260030",
    "end": "4263059"
  },
  {
    "text": "I see a few residents. Yes.",
    "start": "4266400",
    "end": "4271650"
  },
  {
    "text": "Yes. So, yeah. So both of you are\non the right track. Basically, block coding, right? You need a bigger alphabet\nin this case, right?",
    "start": "4271650",
    "end": "4277950"
  },
  {
    "text": "This alphabet, you\ncan't do better, right? There is only so much, like,\nwith one symbol you can do.",
    "start": "4277950",
    "end": "4283910"
  },
  {
    "text": "What you saw in homework\n1, if you remember, you had a uniform fixed\nlength code over nine symbols,",
    "start": "4283910",
    "end": "4289148"
  },
  {
    "text": "and then you made\nit over 81 symbols. And you got better, and then\nyou can keep increasing it. And the result will be\nsomething like that.",
    "start": "4289148",
    "end": "4294800"
  },
  {
    "text": "For every epsilon, there\nexists a block size n, such that you can get within\nepsilon gap of the entropy.",
    "start": "4294800",
    "end": "4301460"
  },
  {
    "text": "So of that's what we mean\nby arbitrarily close.",
    "start": "4301460",
    "end": "4304310"
  },
  {
    "text": "Right. So clearly, we are\nnot at achievability. We are not there.",
    "start": "4306960",
    "end": "4313590"
  },
  {
    "text": "So this thing, we will\nsolve with Huffman code. We saw that Shannon codes\nare sometimes just bad.",
    "start": "4313590",
    "end": "4320930"
  },
  {
    "text": "And you will see an\nexample in the quiz today, where they're\nlike absurdly bad.",
    "start": "4320930",
    "end": "4326000"
  },
  {
    "text": "So Huffman code is\nthe solution here. And you will see that\nin the next lecture.",
    "start": "4326000",
    "end": "4330185"
  },
  {
    "text": "Somebody suggested\nblock codes, right? And block codes are good, right? So they can achieve entropy.",
    "start": "4333490",
    "end": "4339430"
  },
  {
    "text": "You can show that\nthey achieve entropy. We'll probably do that\nearly in the next lecture.",
    "start": "4339430",
    "end": "4344950"
  },
  {
    "text": "But there are issues\nwith block codes. It just gets exponential. Bigger and bigger\nblocks, you have",
    "start": "4347530",
    "end": "4353920"
  },
  {
    "text": "to keep around\nthese huge tables. Its exponential complexity. It's not great. So we'll first look\nat block codes.",
    "start": "4353920",
    "end": "4363660"
  },
  {
    "text": "Then, we will look at something\ncalled arithmetic codes, which get the same result, but\nmuch more efficiently.",
    "start": "4363660",
    "end": "4370073"
  },
  {
    "text": "And then we will\nlook at something called ANS, which, again,\ngets the same result but even more efficiently.",
    "start": "4370073",
    "end": "4375390"
  },
  {
    "text": "So there is a sequence\nof codes, all of them used in a bunch of\nreal-life data compressors,",
    "start": "4375390",
    "end": "4381510"
  },
  {
    "text": "themselves data compressors. Yeah. So this will be like the\nplan for the next two weeks,",
    "start": "4381510",
    "end": "4389340"
  },
  {
    "text": "basically, to look at\nall these different codes and understand the there\nare various very cool ideas.",
    "start": "4389340",
    "end": "4395317"
  },
  {
    "text": "Like Huffman codes\nare based on trees. Arithmetic codes are\nbased on arithmetic, and S codes are based on some\nnumber systems and so on.",
    "start": "4395317",
    "end": "4402540"
  },
  {
    "text": "We have 4 minutes. Let me do something\non block codes, unless we have questions.",
    "start": "4405240",
    "end": "4410940"
  },
  {
    "text": "OK. OK. This will be very\nshort and simple. And then, we will do some\nexamples in the next lecture.",
    "start": "4417070",
    "end": "4424667"
  },
  {
    "text": "This is just so that you go home\nsatisfied that we have proved that you can achieve entropy. OK, so if you code in blocks\nof n symbols, what will happen?",
    "start": "4424667",
    "end": "4441700"
  },
  {
    "text": "So if you remember\nthat the entropy of xn",
    "start": "4441700",
    "end": "4451030"
  },
  {
    "text": "was-- if you have symbols, you\nhave independent identically distributed symbols. So the entropy of\nxn was n times H(x).",
    "start": "4451030",
    "end": "4458830"
  },
  {
    "text": "I hope everybody remembers. OK, now the Shannon\ncode on n symbols",
    "start": "4458830",
    "end": "4468010"
  },
  {
    "text": "will obviously be\nbigger than entropy. You cannot beat entropy. And it's less than\nthe entropy Xn plus 1.",
    "start": "4468010",
    "end": "4476350"
  },
  {
    "text": "Yep. Let me write down this thing.",
    "start": "4476350",
    "end": "4481770"
  },
  {
    "text": "I expanded the entropy term.",
    "start": "4485230",
    "end": "4490970"
  },
  {
    "text": "Let me divide by n, right? Because what we care about\nis per symbol, how much I'm paying, not per block. So per symbol is like\ndivided by n because I",
    "start": "4490970",
    "end": "4498290"
  },
  {
    "text": "have a block of n symbols.",
    "start": "4498290",
    "end": "4499595"
  },
  {
    "text": "Then, that's it basically--\nif you work in blocks of 10, you can get within\n1/n of entropy.",
    "start": "4509190",
    "end": "4513840"
  },
  {
    "text": "Yeah. Before we go, let's quickly\nlook at this example.",
    "start": "4518350",
    "end": "4524469"
  },
  {
    "text": "I think this might be\ninteresting, right? Actually, we didn't block--\nwe're doing blocks at all, OK?",
    "start": "4524470",
    "end": "4530210"
  },
  {
    "text": "So this is not good. Next lecture, we'll\ndo an example. But everybody able\nto follow this?",
    "start": "4530210",
    "end": "4538670"
  },
  {
    "text": "So we basically code\nin blocks of n symbols. So what you do is\nyou take n symbols,",
    "start": "4538670",
    "end": "4545119"
  },
  {
    "text": "like you did in the first quiz. And you code over that\njoint alphabet, right?",
    "start": "4545120",
    "end": "4550400"
  },
  {
    "text": "So if you had A, B, C before. Now, you have AA, AB,\nAC, dot, dot, dot, right?",
    "start": "4550400",
    "end": "4555890"
  },
  {
    "text": "You have nine symbols. You make a Shannon code you make\na big tree with nine symbols.",
    "start": "4555890",
    "end": "4561000"
  },
  {
    "text": "Then you code those\ntwo at a time, right? Two at a time.",
    "start": "4561000",
    "end": "4567670"
  },
  {
    "text": "And Shannon code, the property\nholds for any distribution, right? It will hold for the\njoint distribution also.",
    "start": "4567670",
    "end": "4573370"
  },
  {
    "text": "It will hold for\nthe n-tuple also. The same property, this\nproperty that we proved before, that you are between\nx and x plus 1, that",
    "start": "4573370",
    "end": "4581440"
  },
  {
    "text": "holds for arbitrary things. And then you divide by n, and\nyou get basically this result,",
    "start": "4581440",
    "end": "4586720"
  },
  {
    "text": "that you can get\nwithin one by n of. So within 1/n of entropy.",
    "start": "4586720",
    "end": "4599270"
  },
  {
    "text": "So this is what we call\narbitrarily close, right? So this is the proof of\nthe achievability part.",
    "start": "4599270",
    "end": "4607660"
  },
  {
    "text": "Yep. OK, happy to answer\nany questions.",
    "start": "4607660",
    "end": "4613610"
  },
  {
    "text": "It's my office hour,\nI believe, today.",
    "start": "4613610",
    "end": "4616389"
  }
]