[
  {
    "text": "excited to be here feel free to interruptive questions anytime so today maybe ambitiously maybe a",
    "start": "10960",
    "end": "18720"
  },
  {
    "text": "little lack of humility i want to explore the question of how can we trust machine learning",
    "start": "18720",
    "end": "25039"
  },
  {
    "text": "and as anthony mentioned i joined stanford about a year ago and in the",
    "start": "25039",
    "end": "30080"
  },
  {
    "text": "process i had to share all my teaching evaluations for all the classes i taught over the last",
    "start": "30080",
    "end": "35760"
  },
  {
    "text": "15 plus years and i strategically hid this one",
    "start": "35760",
    "end": "41760"
  },
  {
    "text": "it says professor gastrin i'm very disappointed in your class",
    "start": "41760",
    "end": "47679"
  },
  {
    "text": "i thought machine learning was going to be magic but it's just math",
    "start": "48800",
    "end": "57280"
  },
  {
    "text": "and i call this the math myth of machine learning circa 2008 when i got that",
    "start": "57280",
    "end": "63680"
  },
  {
    "text": "teaching evaluation and there we thought that you could start with some data you do some machine learning",
    "start": "63680",
    "end": "69760"
  },
  {
    "text": "magic or math and you end up with profit which at the time meant showing that my curve was",
    "start": "69760",
    "end": "75520"
  },
  {
    "text": "better than your curve and then writing a paper at top conference about it",
    "start": "75520",
    "end": "81520"
  },
  {
    "text": "now this seems all well and good but there must be some",
    "start": "81520",
    "end": "87840"
  },
  {
    "text": "magic in machine learning i mean we're making a huge difference in the world from",
    "start": "87920",
    "end": "93280"
  },
  {
    "text": "solving games that have never been solved before to self-driving cars and living a healthier day using devices",
    "start": "93280",
    "end": "100880"
  },
  {
    "text": "they use machine learning in ai now to me the magic in machine learning",
    "start": "100880",
    "end": "106799"
  },
  {
    "text": "is what happens in the spaces between the math that is",
    "start": "106799",
    "end": "112960"
  },
  {
    "text": "how you define the problem how you collect or clean or sell set up your data how you define or choose your model",
    "start": "112960",
    "end": "119200"
  },
  {
    "text": "how you integrate that into an overall system how you deploy the system and how you evaluate to measure it",
    "start": "119200",
    "end": "126000"
  },
  {
    "text": "along the way there's math in this but the typical math that we think about",
    "start": "126000",
    "end": "131920"
  },
  {
    "text": "is a small component of an overall system and each step of the system is filled",
    "start": "131920",
    "end": "138560"
  },
  {
    "text": "with complex human decisions that are getting harder and harder because this machine learning is",
    "start": "138560",
    "end": "144560"
  },
  {
    "text": "becoming more and more of a part of every decision that we make and the systems are getting more and",
    "start": "144560",
    "end": "149680"
  },
  {
    "text": "more complex it's important to step back and ask the question how can we even trust",
    "start": "149680",
    "end": "156560"
  },
  {
    "text": "machine learning and one way to think about this is to step back further and review the",
    "start": "156560",
    "end": "163440"
  },
  {
    "text": "question of how do we trust how do humans trust and i love the seminal work",
    "start": "163440",
    "end": "170959"
  },
  {
    "text": "by tom and all that looked at medicine and said that if",
    "start": "170959",
    "end": "177280"
  },
  {
    "text": "you have trust in your provider say your doctor you have increased adherence to the treatment",
    "start": "177280",
    "end": "183680"
  },
  {
    "text": "you continue with the provider longer but if you don't have trust you don't seek care you don't seek preventive care",
    "start": "183680",
    "end": "190480"
  },
  {
    "text": "you don't take surgical care and this leads to detriment in the long run",
    "start": "190480",
    "end": "197760"
  },
  {
    "text": "so they define three areas",
    "start": "197760",
    "end": "203519"
  },
  {
    "text": "of trust technical competency does my provider does my doctor know",
    "start": "203519",
    "end": "208879"
  },
  {
    "text": "what they're doing interpersonal competency do you communicate well with me about what they're doing",
    "start": "208879",
    "end": "215200"
  },
  {
    "text": "and the third one is agency do they have my best interests in mind",
    "start": "215200",
    "end": "220640"
  },
  {
    "text": "and so inspired by this and the work that we've been doing in machine learning i've been asking",
    "start": "220640",
    "end": "227519"
  },
  {
    "text": "myself what are the core principles of trust in machine learning or ai",
    "start": "227519",
    "end": "234959"
  },
  {
    "text": "and here i'm thinking about trust for whom so",
    "start": "234959",
    "end": "240560"
  },
  {
    "text": "can i as a developer trust the things that i'm building if you're using what i built for you can",
    "start": "240560",
    "end": "247120"
  },
  {
    "text": "you trust what i built and then as a society or as a community",
    "start": "247120",
    "end": "252159"
  },
  {
    "text": "can we trust over building are we doing the right thing for humanity in the long term these are big questions",
    "start": "252159",
    "end": "258320"
  },
  {
    "text": "and today i'll focus a little bit on the first question can i trust what i'm building",
    "start": "258320",
    "end": "264720"
  },
  {
    "text": "and i'm proposing three principles of trust inspired by all the research has been done in the",
    "start": "264720",
    "end": "272000"
  },
  {
    "text": "area more generally with humans first one is clarity second is competence and the third one is",
    "start": "272000",
    "end": "277520"
  },
  {
    "text": "alignment so clarity to start is really the property",
    "start": "277520",
    "end": "283360"
  },
  {
    "text": "of being well understood of communicating well with each other",
    "start": "283360",
    "end": "289199"
  },
  {
    "text": "so to explore this as a machine learning engineer things are pretty simple you give me a",
    "start": "289199",
    "end": "296400"
  },
  {
    "text": "function f i'll optimize it the best i can i'll stay up all night i'll forget to eat but",
    "start": "296400",
    "end": "303759"
  },
  {
    "text": "f is going to be as big as possible but the problem here",
    "start": "303759",
    "end": "309120"
  },
  {
    "text": "with trust is have i picked the right f have i figured out the right problem",
    "start": "309120",
    "end": "315360"
  },
  {
    "text": "that i'm trying to solve so let's do a little user study",
    "start": "315360",
    "end": "321039"
  },
  {
    "text": "uh based on a much bigger user study that we did some years ago and this was part of the university of",
    "start": "321039",
    "end": "327680"
  },
  {
    "text": "washington still and the mascot for the university is a husky",
    "start": "327680",
    "end": "333120"
  },
  {
    "text": "so every time there's a game we brought a husky to the stadium",
    "start": "333120",
    "end": "338240"
  },
  {
    "text": "the real problem is that if we by mistake brought a wolf",
    "start": "338240",
    "end": "343360"
  },
  {
    "text": "then thousands of people might die you might think this is a funny example",
    "start": "343360",
    "end": "350160"
  },
  {
    "text": "but it's real so we got together",
    "start": "350160",
    "end": "355840"
  },
  {
    "text": "with the senior leadership of the university and we decided the best way to solve this problem is to build a neural",
    "start": "355840",
    "end": "362639"
  },
  {
    "text": "network that would predict if it was a husky or a dot or a or a wolf that came to the stadium",
    "start": "362639",
    "end": "371199"
  },
  {
    "text": "and as a community we decided 95 accuracy was the best possible what was",
    "start": "371199",
    "end": "376960"
  },
  {
    "text": "that was the right metric for us in other words we decided if one in 20",
    "start": "376960",
    "end": "382560"
  },
  {
    "text": "games thousands of people died that's okay that's an acceptable risk",
    "start": "382560",
    "end": "388639"
  },
  {
    "text": "and so my question to you is i have a model it meets the requirements the safety",
    "start": "388639",
    "end": "394319"
  },
  {
    "text": "requirements they're defined for this problem do you trust this model",
    "start": "394319",
    "end": "401759"
  },
  {
    "text": "raise your hand if you trust the model meets the requirement",
    "start": "402319",
    "end": "407440"
  },
  {
    "text": "clark does not trust them all raise your hand if you don't trust the model",
    "start": "407440",
    "end": "413199"
  },
  {
    "text": "if you don't trust the model is it because you don't trust me raise your hand",
    "start": "413199",
    "end": "418800"
  },
  {
    "text": "raise your hand don't trust me thank you you don't trust my machine learning but",
    "start": "418800",
    "end": "425520"
  },
  {
    "text": "why you meet the requirements",
    "start": "425520",
    "end": "430880"
  },
  {
    "text": "i'm sure you learned in this class that you know you set up some safety levels you might have",
    "start": "430880",
    "end": "435919"
  },
  {
    "text": "multiple metrics you meet those metrics and you're ready to deploy",
    "start": "435919",
    "end": "441520"
  },
  {
    "text": "yeah i'm not sure if this is like sensitivity versus specificity i i",
    "start": "441840",
    "end": "448240"
  },
  {
    "text": "wouldn't summarize with just accuracy as my trick makes a good point",
    "start": "448240",
    "end": "454479"
  },
  {
    "text": "um this is not the right metric but the senior leadership of the",
    "start": "454479",
    "end": "459919"
  },
  {
    "text": "university decided that this is the right metric so",
    "start": "459919",
    "end": "465520"
  },
  {
    "text": "one of the questions is is this the right metric and we'll talk a little bit about it in the third section of the",
    "start": "465520",
    "end": "470560"
  },
  {
    "text": "talk but even before then if you pick the right metric even if you meet it",
    "start": "470560",
    "end": "476879"
  },
  {
    "text": "it might worry that you're not measuring the right thing",
    "start": "476879",
    "end": "481520"
  },
  {
    "text": "so in this case what uh my student marco hibedo did was",
    "start": "482479",
    "end": "488080"
  },
  {
    "text": "try to come up with a method to try to understand why a machine learning model makes a particular prediction",
    "start": "488080",
    "end": "494160"
  },
  {
    "text": "and so what you're seeing here in this next slide",
    "start": "494160",
    "end": "499840"
  },
  {
    "text": "is the prediction the image and the prediction made which was a husky and the parts of the image",
    "start": "499840",
    "end": "507120"
  },
  {
    "text": "that the model thought were important this approach thought were important to make this prediction",
    "start": "507120",
    "end": "512959"
  },
  {
    "text": "and i'm going to show you examples from predictions of wolf and",
    "start": "512959",
    "end": "519200"
  },
  {
    "text": "husky and what do you notice",
    "start": "519200",
    "end": "522880"
  },
  {
    "text": "yes well i read the line paper so i know it's you know the answer so you already know",
    "start": "524959",
    "end": "531120"
  },
  {
    "text": "um it's looking for snow and remember that at the end of the",
    "start": "531120",
    "end": "536480"
  },
  {
    "text": "quarter but it's correct um if you see here",
    "start": "536480",
    "end": "542880"
  },
  {
    "text": "i don't i haven't built a wolf detector i built a very deep and fancy snow",
    "start": "542880",
    "end": "550160"
  },
  {
    "text": "detector in other words there was this spurious correlation between",
    "start": "550160",
    "end": "556560"
  },
  {
    "text": "wolf and snow whenever there was a picture of wolf most typically was in snow that was giving us the high",
    "start": "556560",
    "end": "562240"
  },
  {
    "text": "accuracy and it doesn't snow very often in seattle so it's unlikely that when",
    "start": "562240",
    "end": "568880"
  },
  {
    "text": "the the animal mascot will come in there'll be snow to help us ground the predictions",
    "start": "568880",
    "end": "575200"
  },
  {
    "text": "of wolf so if socrates via plato said",
    "start": "575200",
    "end": "581360"
  },
  {
    "text": "an examined life is not worth living i say that marco would say an unexamined",
    "start": "581360",
    "end": "588000"
  },
  {
    "text": "model is not worth using so part of this",
    "start": "588000",
    "end": "593519"
  },
  {
    "text": "of clarity is about this property the quality of being",
    "start": "593519",
    "end": "598880"
  },
  {
    "text": "easily understood the quality of examining why you're making a particular decision",
    "start": "598880",
    "end": "604320"
  },
  {
    "text": "and whether you're doing this for the right reasons and as",
    "start": "604320",
    "end": "609519"
  },
  {
    "text": "we thought about this the first way we've approached this is through clarity v-interpretability",
    "start": "609519",
    "end": "615519"
  },
  {
    "text": "so interpretability is about giving humans a mental model of the behavior of",
    "start": "615519",
    "end": "620720"
  },
  {
    "text": "machine learning models so if you might use a simple model like a",
    "start": "620720",
    "end": "626240"
  },
  {
    "text": "shallow decision tree it's very easy to get a mental model of what's happening",
    "start": "626240",
    "end": "632000"
  },
  {
    "text": "but the problem is this model might not be particularly accurate so you can make decision tree bigger",
    "start": "632000",
    "end": "639360"
  },
  {
    "text": "now it's a little harder to interpret it a lot of if and then roles",
    "start": "639360",
    "end": "644399"
  },
  {
    "text": "and it's a little more accurate and there's a trade-off and there's been some field of study of",
    "start": "644399",
    "end": "650480"
  },
  {
    "text": "trying to find more accurate interpretable models but the problem is that real world use",
    "start": "650480",
    "end": "656959"
  },
  {
    "text": "cases and superhuman ai the focus on accuracy",
    "start": "656959",
    "end": "663279"
  },
  {
    "text": "tends to leave us with a complete lack of interpretability and the question we approach from that",
    "start": "663279",
    "end": "669760"
  },
  {
    "text": "original work which has led to thousands of works in this area",
    "start": "669760",
    "end": "676640"
  },
  {
    "text": "is to ask can we provide some transparency or interpretation to",
    "start": "676640",
    "end": "682160"
  },
  {
    "text": "even the most accurate state-of-the-art models",
    "start": "682160",
    "end": "686959"
  },
  {
    "text": "so our first endeavor said let's ignore the internal structure of the",
    "start": "687760",
    "end": "693760"
  },
  {
    "text": "model consider a black box so we can examine any state-of-the-art model",
    "start": "693760",
    "end": "699920"
  },
  {
    "text": "we can compare existing models and future models and models from different classes",
    "start": "699920",
    "end": "705920"
  },
  {
    "text": "and so in the first paper which you mentioned lime well the the",
    "start": "705920",
    "end": "712079"
  },
  {
    "text": "insight was if you take a prediction it might be very complicated in terms of",
    "start": "712079",
    "end": "717519"
  },
  {
    "text": "what the decision space looks like but if you zoom in around the prediction and if you zoom in some more",
    "start": "717519",
    "end": "724160"
  },
  {
    "text": "the space might look simpler or locally linear so can we fit a simple model",
    "start": "724160",
    "end": "732079"
  },
  {
    "text": "around each prediction that helps us explain what's happening",
    "start": "732079",
    "end": "737199"
  },
  {
    "text": "so if you take this confusing image and you give it to image classifier",
    "start": "738320",
    "end": "743600"
  },
  {
    "text": "now we can interpret its behavior can say it predicts a",
    "start": "743600",
    "end": "748959"
  },
  {
    "text": "electric guitar with probability 0.32 and then it's looking at this part of the image",
    "start": "748959",
    "end": "755279"
  },
  {
    "text": "it's predicting it's an acoustic guitar when it looks at this part of the image",
    "start": "755279",
    "end": "760480"
  },
  {
    "text": "and a labrador when it's looking at this part of the image",
    "start": "760480",
    "end": "765760"
  },
  {
    "text": "and thus gives us some sense of what grounding we have",
    "start": "765760",
    "end": "770800"
  },
  {
    "text": "for the predictions now if we go back to this user study",
    "start": "770800",
    "end": "776880"
  },
  {
    "text": "that we did together thank you for signing the release form when you came in by the way",
    "start": "776880",
    "end": "782079"
  },
  {
    "text": "and we we made it much more formal and we worked with 27 subjects",
    "start": "782079",
    "end": "789600"
  },
  {
    "text": "before seeing explanations about 60 percent of subject subjects didn't trust the model",
    "start": "789600",
    "end": "795600"
  },
  {
    "text": "because none of them trusted me and about uh",
    "start": "795600",
    "end": "801120"
  },
  {
    "text": "40 percent had the snow inside after seeing the explanations",
    "start": "801120",
    "end": "808240"
  },
  {
    "text": "26 out of 27 no longer trusted the model so 25 verticals have no longer trusted",
    "start": "808240",
    "end": "813680"
  },
  {
    "text": "model and 26 out of 27 had the snow inside",
    "start": "813680",
    "end": "820399"
  },
  {
    "text": "so the question i always ask myself is who is that person who had the snow inside and still",
    "start": "820720",
    "end": "827360"
  },
  {
    "text": "trusted the model but as you can see by providing",
    "start": "827360",
    "end": "834079"
  },
  {
    "text": "explanations and an ability to interrogate a model better you can discover for example spurious",
    "start": "834079",
    "end": "840639"
  },
  {
    "text": "correlations in the data they're impacting the behavior of the system",
    "start": "840639",
    "end": "846560"
  },
  {
    "text": "so really the first part here is to try to provide some transparency to give humans a",
    "start": "847920",
    "end": "854000"
  },
  {
    "text": "mental model of the behavior of a system and lyme was the first endeavor in this",
    "start": "854000",
    "end": "860480"
  },
  {
    "text": "space and assumed this locally linear behavior there's been a lot of",
    "start": "860480",
    "end": "866000"
  },
  {
    "text": "follow-on work with a lot of different assumptions in algorithms",
    "start": "866000",
    "end": "871600"
  },
  {
    "text": "one little example i'd like to give you just to show an alternative here is what we call",
    "start": "871600",
    "end": "877600"
  },
  {
    "text": "anchors this is about finding sufficient conditions for a prediction in other words if i were to move",
    "start": "877600",
    "end": "884560"
  },
  {
    "text": "the input anywhere in this box i will still make the same prediction so let me show you an example of that",
    "start": "884560",
    "end": "891760"
  },
  {
    "text": "for this prediction in image classification i always predict a beagle",
    "start": "891760",
    "end": "897199"
  },
  {
    "text": "if i fix this areas of the image no matter what happens to the rest",
    "start": "897199",
    "end": "903360"
  },
  {
    "text": "now take another task which is the visual question answering task it's really cool you give me this image and",
    "start": "905440",
    "end": "911519"
  },
  {
    "text": "you ask what's a mustache made of it says banana that's really impressive or how many bananas are there in the",
    "start": "911519",
    "end": "918240"
  },
  {
    "text": "picture and it says two which is also really impressive but if you look at the anchors the",
    "start": "918240",
    "end": "924399"
  },
  {
    "text": "sufficient conditions for prediction for the first question is the word what",
    "start": "924399",
    "end": "930560"
  },
  {
    "text": "and for the second question is the word many in other words any question with the word what gets",
    "start": "930560",
    "end": "937440"
  },
  {
    "text": "answered banana like what's the ground made of banana what was the head of the u.s banana and",
    "start": "937440",
    "end": "944240"
  },
  {
    "text": "so on any question of the word many gets the answer too like how many people in the picture",
    "start": "944240",
    "end": "949920"
  },
  {
    "text": "two how many is too many obviously too and so these notions",
    "start": "949920",
    "end": "956639"
  },
  {
    "text": "of anchors interpretability help us understand brittleness in the system",
    "start": "956639",
    "end": "963519"
  },
  {
    "text": "that you normally would not detect with standard test procedures",
    "start": "963519",
    "end": "969839"
  },
  {
    "text": "so clarity can help machine can help machine learning help humans",
    "start": "970959",
    "end": "976800"
  },
  {
    "text": "make more informed decisions too so here's a great example that i'm not i",
    "start": "976800",
    "end": "983199"
  },
  {
    "text": "was not involved in where they're using similar kinds of explanations in the operating room",
    "start": "983199",
    "end": "989199"
  },
  {
    "text": "to detect when the oxygen levels go too low so hypoxemia and not just providing a",
    "start": "989199",
    "end": "997120"
  },
  {
    "text": "prediction but providing predictions plus explanations helped doctors be more effective",
    "start": "997120",
    "end": "1003199"
  },
  {
    "text": "in their efforts so that's",
    "start": "1003199",
    "end": "1008959"
  },
  {
    "text": "the clarity area trust is competence a competence",
    "start": "1008959",
    "end": "1014079"
  },
  {
    "text": "is about having sufficient skill or knowledge",
    "start": "1014079",
    "end": "1020240"
  },
  {
    "text": "for a particular duty or in a particular respect so sufficient skill for a particular",
    "start": "1020240",
    "end": "1026798"
  },
  {
    "text": "duty in machine learning is about evaluation",
    "start": "1026799",
    "end": "1032079"
  },
  {
    "text": "does this model have the skills we're hoping for now in 2008 this was very simple",
    "start": "1032079",
    "end": "1039438"
  },
  {
    "text": "i showed that my curve was better than your curve on a benchmark data set and that was sufficient skill to get my",
    "start": "1039439",
    "end": "1044640"
  },
  {
    "text": "paper published but even this notion of a test set is not",
    "start": "1044640",
    "end": "1052559"
  },
  {
    "text": "even practice in practice data is changing all the time",
    "start": "1052559",
    "end": "1058320"
  },
  {
    "text": "you have sometimes little insight on how the system might behave in the real world based on the training that you do",
    "start": "1058320",
    "end": "1065039"
  },
  {
    "text": "offline so benchmark data sets and test data sets at best",
    "start": "1065039",
    "end": "1070720"
  },
  {
    "text": "a weak proxy on what's going to happen in the real world",
    "start": "1070720",
    "end": "1076640"
  },
  {
    "text": "and so to me we need to rethink how we evaluate these models in practical settings and learn from software",
    "start": "1077280",
    "end": "1083200"
  },
  {
    "text": "engineering where we do things like failure analysis and more systematic testing",
    "start": "1083200",
    "end": "1088480"
  },
  {
    "text": "of software so what does that mean let me just give you",
    "start": "1088480",
    "end": "1094559"
  },
  {
    "text": "a couple examples in terms of failure analysis one of the tasks that we looked at is can you perturb the input",
    "start": "1094559",
    "end": "1102559"
  },
  {
    "text": "adversarially in a way that will change the prediction so there's been some work in this area",
    "start": "1102559",
    "end": "1109200"
  },
  {
    "text": "for example you give me this image and the model predicts a panda you can add some imperceptible noise",
    "start": "1109200",
    "end": "1116000"
  },
  {
    "text": "and now it predicts a given of high confidence now this is",
    "start": "1116000",
    "end": "1121120"
  },
  {
    "text": "really interesting work but this is more for like an adversarial",
    "start": "1121120",
    "end": "1127120"
  },
  {
    "text": "attack setting maybe for security but it's not the kind of noise that you typically see in the real world",
    "start": "1127120",
    "end": "1134720"
  },
  {
    "text": "our question was what kinds of noises or perturbations are natural that really will break the",
    "start": "1134960",
    "end": "1140799"
  },
  {
    "text": "model so let me give you a couple examples of that so again visual question answering i give you this emergency",
    "start": "1140799",
    "end": "1146880"
  },
  {
    "text": "what type of roadside they show and it says a stop sign or i give you this paragraph and ask how long is the rhine",
    "start": "1146880",
    "end": "1153679"
  },
  {
    "text": "and it says 1 230 kilometers long now here are some small perturbations so",
    "start": "1153679",
    "end": "1159360"
  },
  {
    "text": "for the first question i changed it from what type of roadside is shown so which type",
    "start": "1159360",
    "end": "1166240"
  },
  {
    "text": "of roadside is shown and now it's do not enter and i change the second question from",
    "start": "1166240",
    "end": "1172799"
  },
  {
    "text": "how long is the rhine to how long is the rhine by adding an extra question mark and",
    "start": "1172799",
    "end": "1177919"
  },
  {
    "text": "it's now 1 million and 50 000 long and so this is the kind of natural",
    "start": "1177919",
    "end": "1184960"
  },
  {
    "text": "variability you see in language that is really breaking the model",
    "start": "1184960",
    "end": "1190799"
  },
  {
    "text": "so we asked ourselves and this is also marco's work uh how can we find",
    "start": "1190799",
    "end": "1195840"
  },
  {
    "text": "semantically equivalent adversarial examples and we approach this is by using",
    "start": "1195840",
    "end": "1201280"
  },
  {
    "text": "paraphrasing models to perturb the input but retain the semantics and then figure",
    "start": "1201280",
    "end": "1207520"
  },
  {
    "text": "out which ones would break the behavior of the model so let me show you an example that again visual question answering",
    "start": "1207520",
    "end": "1214799"
  },
  {
    "text": "give me this image i ask what color is the tray and the model says pink which is pretty",
    "start": "1214799",
    "end": "1220240"
  },
  {
    "text": "impressive but i start paraphrasing it for example i ask what color is the tray of a british accent and it's now green",
    "start": "1220240",
    "end": "1228480"
  },
  {
    "text": "or which color is a tray and is now green so these are natural variabilities that",
    "start": "1228480",
    "end": "1234640"
  },
  {
    "text": "are semantically uh very similar but they lead to changes in prediction",
    "start": "1234640",
    "end": "1241919"
  },
  {
    "text": "and this is a problem typically a problem in terms of coverage of your data and so detecting it lets you do",
    "start": "1243039",
    "end": "1249360"
  },
  {
    "text": "nice things so for example you could do data augmentation and if you do that",
    "start": "1249360",
    "end": "1254480"
  },
  {
    "text": "in very simple ways you can significantly increase the robust model once you discover",
    "start": "1254480",
    "end": "1261039"
  },
  {
    "text": "these issues",
    "start": "1261039",
    "end": "1263840"
  },
  {
    "text": "so machinery models are prone to bugs and really need to find semantically meaningful ways to discover these issues",
    "start": "1269520",
    "end": "1276320"
  },
  {
    "text": "test for them and improve the behavior of models but really how do we know",
    "start": "1276320",
    "end": "1282720"
  },
  {
    "text": "when we're ready ready to deploy this is a big question in general",
    "start": "1282720",
    "end": "1288960"
  },
  {
    "text": "and i think we need to test models more systematically like with that software",
    "start": "1288960",
    "end": "1294080"
  },
  {
    "text": "so inspired by software testing approaches we came up with this methodology we call",
    "start": "1294080",
    "end": "1300480"
  },
  {
    "text": "checklist which it's pretty simple so let me give you an example",
    "start": "1300480",
    "end": "1306880"
  },
  {
    "text": "so let's say you have an input sentence like easily the best airline in seattle",
    "start": "1306880",
    "end": "1312000"
  },
  {
    "text": "actually i should change it to is it the best airline in san francisco and i give it to a sentiment classifier",
    "start": "1312000",
    "end": "1318799"
  },
  {
    "text": "and it says it's a positive neutral or negative sentence very simple task",
    "start": "1318799",
    "end": "1325279"
  },
  {
    "text": "how do we test that if we're software engineers we start from testing small units of",
    "start": "1326640",
    "end": "1333520"
  },
  {
    "text": "behavior so for example in nlp you might have different vocabulary named entities",
    "start": "1333520",
    "end": "1340000"
  },
  {
    "text": "negations and so on so what checklist does is a methodology",
    "start": "1340000",
    "end": "1345679"
  },
  {
    "text": "for creating tests along different categories like this and",
    "start": "1345679",
    "end": "1351520"
  },
  {
    "text": "based on the skill set that you want the model to have you can get some checks and crosses",
    "start": "1351520",
    "end": "1357679"
  },
  {
    "text": "and decide when you're ready to deploy now let me show you a couple examples of tests and what happens",
    "start": "1357679",
    "end": "1364240"
  },
  {
    "text": "the first one is medium functionality test this is kind of no-brainer you expect everything to work just fine",
    "start": "1364240",
    "end": "1370880"
  },
  {
    "text": "so for example in sentiment analysis i give you a positive sentence",
    "start": "1370880",
    "end": "1376159"
  },
  {
    "text": "it should be positive like or a negative sentence like i despise that aircraft it should be negative",
    "start": "1376159",
    "end": "1383519"
  },
  {
    "text": "another one is negation so i gave you a",
    "start": "1384400",
    "end": "1390000"
  },
  {
    "text": "negated neutral sentence and it should be neutral so this is not an international",
    "start": "1390000",
    "end": "1395200"
  },
  {
    "text": "flight now if you generate simple tests like this",
    "start": "1395200",
    "end": "1400799"
  },
  {
    "text": "and you give it to public apis or state-of-the-art models",
    "start": "1400799",
    "end": "1407600"
  },
  {
    "text": "they fail at rates of up to 15 percent even though they're very simple tests",
    "start": "1407600",
    "end": "1414400"
  },
  {
    "text": "or with negation they can fill a rate set to 54",
    "start": "1414400",
    "end": "1419039"
  },
  {
    "text": "this second kinds of tests you can imagine are metamorphic or perturbation",
    "start": "1422159",
    "end": "1427279"
  },
  {
    "text": "like base tests so for example um if you have an input",
    "start": "1427279",
    "end": "1432960"
  },
  {
    "text": "that has a location like i'm taking a flight to chicago and now you change your flight to dallas maybe you have feelings about dallas versus chicago but",
    "start": "1432960",
    "end": "1440640"
  },
  {
    "text": "in general the sentiment should stay the same nonetheless state-of-the-art models and public apis",
    "start": "1440640",
    "end": "1448240"
  },
  {
    "text": "fail at rates of up to 20 percent when you do perturbations like this",
    "start": "1448240",
    "end": "1453440"
  },
  {
    "text": "or finally you can imagine changing sentences changing the direction so for",
    "start": "1453520",
    "end": "1458559"
  },
  {
    "text": "example if you have a positive sentence you add another positive sentence you shouldn't get more negative",
    "start": "1458559",
    "end": "1464880"
  },
  {
    "text": "but despite that simple perturbations like this",
    "start": "1464880",
    "end": "1470960"
  },
  {
    "text": "cause models to fail rates of up to 34 and these are models that people are paying money for in public in public",
    "start": "1470960",
    "end": "1476880"
  },
  {
    "text": "clouds so what it says to me is",
    "start": "1476880",
    "end": "1482000"
  },
  {
    "text": "we haven't had a systematic way to evaluate these models before we ship them and what checklist does is a methodology",
    "start": "1482000",
    "end": "1489520"
  },
  {
    "text": "and a tool to make it easy to detect and build tests like this",
    "start": "1489520",
    "end": "1494640"
  },
  {
    "text": "and we did a user study on with engineers that work on a real problem like this",
    "start": "1494640",
    "end": "1501360"
  },
  {
    "text": "and using the methodology and the tool they're able to generate more tests",
    "start": "1501360",
    "end": "1508159"
  },
  {
    "text": "more test cases per test and detect more mission critical bugs on",
    "start": "1508159",
    "end": "1513279"
  },
  {
    "text": "the system is already applied in a short period of time",
    "start": "1513279",
    "end": "1518559"
  },
  {
    "text": "so competence is this really property of having sufficient skill",
    "start": "1521120",
    "end": "1527039"
  },
  {
    "text": "for the task at hand and is the second anchor pillar",
    "start": "1527039",
    "end": "1532320"
  },
  {
    "text": "of the three principles of trust here the third one which is the most complex",
    "start": "1532320",
    "end": "1537919"
  },
  {
    "text": "one is alignment so alignment",
    "start": "1537919",
    "end": "1543760"
  },
  {
    "text": "is really about having a state of agreement of cooperation between peoples or parties for common",
    "start": "1543760",
    "end": "1550559"
  },
  {
    "text": "cause of viewpoint and the key here is a common cause or viewpoint does my ai does my system have",
    "start": "1550559",
    "end": "1557039"
  },
  {
    "text": "the same cause of viewpoint that i hope to have",
    "start": "1557039",
    "end": "1562080"
  },
  {
    "text": "so again 2008 very simple we had totally aligned principles we want to get a paper into",
    "start": "1562559",
    "end": "1569679"
  },
  {
    "text": "the conference but these issues can get much more significant",
    "start": "1569679",
    "end": "1575039"
  },
  {
    "text": "when you start building real world systems and netflix articulated this really well",
    "start": "1575039",
    "end": "1580400"
  },
  {
    "text": "so they're recommending content for you all the time and they had this challenge in 2006",
    "start": "1580400",
    "end": "1588240"
  },
  {
    "text": "which was really popular and really put a lot of effort into recommender systems and the",
    "start": "1588240",
    "end": "1594559"
  },
  {
    "text": "idea here is you're given a data set of movies and you had to create a model to",
    "start": "1594559",
    "end": "1599760"
  },
  {
    "text": "predict what rating a user would give to a particular movie this was extremely useful for them in a",
    "start": "1599760",
    "end": "1607120"
  },
  {
    "text": "sense there was a lot of work in recommender systems yet they didn't deploy this",
    "start": "1607120",
    "end": "1613039"
  },
  {
    "text": "in their actual systems and the reason is this is not the metric they care about in their business",
    "start": "1613039",
    "end": "1620080"
  },
  {
    "text": "the metric they really care about is churn what is the chance that next year you'll",
    "start": "1620080",
    "end": "1626960"
  },
  {
    "text": "cancel your netflix subscription and predicting your rating",
    "start": "1626960",
    "end": "1632320"
  },
  {
    "text": "doesn't help me too much with respect to children based on a variety of methodologies they",
    "start": "1632320",
    "end": "1639120"
  },
  {
    "text": "discovered a metric that didn't make sense and it's binge watching",
    "start": "1639120",
    "end": "1644399"
  },
  {
    "text": "so if you click on something and you binge watch it you're a lot less likely to turn next year",
    "start": "1644399",
    "end": "1650720"
  },
  {
    "text": "and so they decided to create a metric which is to maximize the probability that you'll",
    "start": "1650720",
    "end": "1656480"
  },
  {
    "text": "click on something and watch it to the end and optimizing that was closer",
    "start": "1656480",
    "end": "1664080"
  },
  {
    "text": "to the business metric they cared about now i call this the loss",
    "start": "1664080",
    "end": "1670720"
  },
  {
    "text": "metric gap the loss function is where we optimize machine learning the metric is what we",
    "start": "1670720",
    "end": "1676799"
  },
  {
    "text": "actually care about for the things we build now you could switch a loss function",
    "start": "1676799",
    "end": "1683039"
  },
  {
    "text": "like increase the probability of binge watching and get closer to the metric that you care about churn",
    "start": "1683039",
    "end": "1688880"
  },
  {
    "text": "or you could try to find some other smooth approximations of the metric you care about and try to get closer to the",
    "start": "1688880",
    "end": "1695760"
  },
  {
    "text": "loss function",
    "start": "1695760",
    "end": "1698480"
  },
  {
    "text": "but the problem is that there are typically multiple metrics that you care about they're complex and fundamentally as a",
    "start": "1700960",
    "end": "1708240"
  },
  {
    "text": "machine learning engineer you typically don't own those metrics it's often a product team that's",
    "start": "1708240",
    "end": "1714640"
  },
  {
    "text": "deciding how to evaluate the overall system and you don't see all of that or you",
    "start": "1714640",
    "end": "1720080"
  },
  {
    "text": "don't necessarily have access to all of it so it's a really complex process",
    "start": "1720080",
    "end": "1726320"
  },
  {
    "text": "so what we did uh when i was still at apple is come up with a simple methodology to solve this",
    "start": "1727919",
    "end": "1734480"
  },
  {
    "text": "problem to address this problem and the idea here was can we come up with a parametric",
    "start": "1734480",
    "end": "1739840"
  },
  {
    "text": "family of loss functions that you can evaluate against the metric that you really care about",
    "start": "1739840",
    "end": "1745760"
  },
  {
    "text": "during the training process as you're optimizing the model parameters every so often you go back and you re-evaluate",
    "start": "1745760",
    "end": "1751760"
  },
  {
    "text": "the metric you actually care about like churn and then you use that to update the",
    "start": "1751760",
    "end": "1756880"
  },
  {
    "text": "model parameters and get them the metric the loss parameters and get them a little closer to the metric you",
    "start": "1756880",
    "end": "1762480"
  },
  {
    "text": "care about the simple procedure was very effective",
    "start": "1762480",
    "end": "1769600"
  },
  {
    "text": "but this issue of loss metric gap is getting more and more complex so for example you",
    "start": "1769600",
    "end": "1776559"
  },
  {
    "text": "think about foundation models or large language models or the idea that we're training these really complex models and",
    "start": "1776559",
    "end": "1782960"
  },
  {
    "text": "reusing them in many ways now we have an even bigger gap because",
    "start": "1782960",
    "end": "1788000"
  },
  {
    "text": "we have a model that's been trained for example with a language model loss",
    "start": "1788000",
    "end": "1793600"
  },
  {
    "text": "which has some gap to the prompting loss that you use which has some graph to the metric that you",
    "start": "1793600",
    "end": "1799360"
  },
  {
    "text": "use so the bigger the more complex the system becomes the harder it is",
    "start": "1799360",
    "end": "1804720"
  },
  {
    "text": "to align it with the behaviors they care about",
    "start": "1804720",
    "end": "1809760"
  },
  {
    "text": "and doing this is hard but important",
    "start": "1809760",
    "end": "1815600"
  },
  {
    "text": "and in fact there isn't a single metric you care about like we saw in checklist there might be multiple metrics we care about and we have to somehow figure out",
    "start": "1815600",
    "end": "1822080"
  },
  {
    "text": "how to trade off between all of them and that's even more complicated",
    "start": "1822080",
    "end": "1828640"
  },
  {
    "text": "an alignment is about agreeing on how to balance the different concerns",
    "start": "1828640",
    "end": "1834559"
  },
  {
    "text": "and we we got an uh example earlier we talked about sensitivity versus specificity that's a a different set of",
    "start": "1834559",
    "end": "1841760"
  },
  {
    "text": "concerns and they can sometimes go against each other so let me show you a very simple example",
    "start": "1841760",
    "end": "1848720"
  },
  {
    "text": "of that so one of my teams at apple came up with",
    "start": "1848720",
    "end": "1854399"
  },
  {
    "text": "this feature for the watch where instead of having to say an activation word for the personal assistant you could just",
    "start": "1854399",
    "end": "1861279"
  },
  {
    "text": "use a gesture so you could say what's the weather today",
    "start": "1861279",
    "end": "1866640"
  },
  {
    "text": "i don't know if i have internet connection but uh it is detected and it's thinking",
    "start": "1869039",
    "end": "1875360"
  },
  {
    "text": "and just based on the gesture it was able to evoke the personal assistant and this is",
    "start": "1875360",
    "end": "1881279"
  },
  {
    "text": "really interesting and you have to think about what metrics you care about",
    "start": "1881279",
    "end": "1887120"
  },
  {
    "text": "so for example false reject is a misgesture so i raised my arm",
    "start": "1887120",
    "end": "1893360"
  },
  {
    "text": "and it did not detect me that's that's kind of a a little bit of a problem but a false",
    "start": "1893360",
    "end": "1900240"
  },
  {
    "text": "accept is a big problem this is when i'm giving a talk i'm gesticulating a lot and being",
    "start": "1900240",
    "end": "1906240"
  },
  {
    "text": "latino and then suddenly siri speaks up it's like i didn't hear what you said it's like",
    "start": "1906240",
    "end": "1912720"
  },
  {
    "text": "you know you're so stupid and the problem is i spend much more time",
    "start": "1912720",
    "end": "1918000"
  },
  {
    "text": "gesticulating and talking then i do evoking a personal assistant and so",
    "start": "1918000",
    "end": "1923840"
  },
  {
    "text": "false accepts are a much bigger concern",
    "start": "1923840",
    "end": "1929440"
  },
  {
    "text": "so when you're thinking about a product you have to think about the trade-offs between the different criteria that",
    "start": "1929440",
    "end": "1934960"
  },
  {
    "text": "might arise the way that we did that was we trained a bunch of models",
    "start": "1934960",
    "end": "1941360"
  },
  {
    "text": "and we came up with a set of product decisions so we're",
    "start": "1941360",
    "end": "1946960"
  },
  {
    "text": "gonna have less than one false accept per week so that's one time per week my",
    "start": "1946960",
    "end": "1953679"
  },
  {
    "text": "weird gestures evoke the personal system try to make the false rejects as small as possible",
    "start": "1953679",
    "end": "1960240"
  },
  {
    "text": "but even that simple trade-off was really complex and in reality",
    "start": "1960240",
    "end": "1966720"
  },
  {
    "text": "what we're trying to align is not that just simple trade-off is all sorts of values that you might care about",
    "start": "1966720",
    "end": "1972320"
  },
  {
    "text": "like the accuracy of the system like false positives was negative sensitivity specificity but but",
    "start": "1972320",
    "end": "1979039"
  },
  {
    "text": "more as we've seen machine learning can have a",
    "start": "1979039",
    "end": "1984320"
  },
  {
    "text": "disproportionate impact on different populations around the",
    "start": "1984320",
    "end": "1989519"
  },
  {
    "text": "world this has been discussed for example a lot in recidivism",
    "start": "1989519",
    "end": "1994799"
  },
  {
    "text": "uh this recidivism setting where black defendants were much more likely",
    "start": "1994799",
    "end": "2000559"
  },
  {
    "text": "to have predicted to be uh committing a crime again than white",
    "start": "2000559",
    "end": "2006000"
  },
  {
    "text": "defendants so there might be many criteria you care",
    "start": "2006000",
    "end": "2011679"
  },
  {
    "text": "like in fairness there are multiple criteria and there's a there was a for a while a formula for writing papers we",
    "start": "2011679",
    "end": "2017760"
  },
  {
    "text": "say here's three criteria they all make sense and here's an impossibility theorem that says you can't satisfy them",
    "start": "2017760",
    "end": "2023200"
  },
  {
    "text": "all at the same time now what that means is not that it's it's something you give up on",
    "start": "2023200",
    "end": "2029919"
  },
  {
    "text": "but it is that you have to make complex trade-offs that express the value system",
    "start": "2029919",
    "end": "2035760"
  },
  {
    "text": "that you care about and today there's no really good way to think about machine learning as a multi-criteria",
    "start": "2035760",
    "end": "2041600"
  },
  {
    "text": "optimization problem and how these trade-offs can be made",
    "start": "2041600",
    "end": "2047279"
  },
  {
    "text": "and every choice that you make as a developer every choice that you make when create",
    "start": "2047279",
    "end": "2052560"
  },
  {
    "text": "systems impact the world impact people and they have to be aligned",
    "start": "2052560",
    "end": "2058398"
  },
  {
    "text": "before you're hoping for so for example i often ask myself what am i teaching my models",
    "start": "2058399",
    "end": "2066320"
  },
  {
    "text": "maybe i should ask you know what am i teaching my kids so here's some time ago my kids are watching a show that they",
    "start": "2066320",
    "end": "2072560"
  },
  {
    "text": "loved and i hate called paw patrol",
    "start": "2072560",
    "end": "2078560"
  },
  {
    "text": "in this show there's five puppies uh six puppies they go on life-saving",
    "start": "2078560",
    "end": "2084320"
  },
  {
    "text": "missions there's only one girl puppy skye who is dressed in pink",
    "start": "2084320",
    "end": "2091040"
  },
  {
    "text": "and almost always in the sideline watching while the other puppy saved the day",
    "start": "2091040",
    "end": "2097359"
  },
  {
    "text": "this is not the kind of model that i want to share with my son or",
    "start": "2097359",
    "end": "2103520"
  },
  {
    "text": "daughter and as we're experiencing more and more of the world through devices",
    "start": "2103520",
    "end": "2109040"
  },
  {
    "text": "it's important to think about how our values are aligned",
    "start": "2109040",
    "end": "2115560"
  },
  {
    "text": "and again the data in the choice of data is one of those important decisions that defines the",
    "start": "2116400",
    "end": "2122560"
  },
  {
    "text": "behavioral machine learning model and this is not a new problem so",
    "start": "2122560",
    "end": "2128079"
  },
  {
    "text": "before digital photography we use film so there was this plastic",
    "start": "2128079",
    "end": "2133440"
  },
  {
    "text": "canisters with a little thing inside you might see this no and you have to send it to lab to",
    "start": "2133440",
    "end": "2138960"
  },
  {
    "text": "develop and get your prints and the labs around the world and kodak",
    "start": "2138960",
    "end": "2144560"
  },
  {
    "text": "that was the biggest producer of the time had to make sure that the chemicals in those labs they had different",
    "start": "2144560",
    "end": "2150320"
  },
  {
    "text": "temperatures and humidity and so on were calibrated to give good pictures the way they did that is they created",
    "start": "2150320",
    "end": "2156800"
  },
  {
    "text": "this scards in the 40s and 50s which they called shirley cards you can",
    "start": "2156800",
    "end": "2163280"
  },
  {
    "text": "see a bit of sexism there already too and they're always a light-skinned",
    "start": "2163280",
    "end": "2169200"
  },
  {
    "text": "female model they always used to calibrate the pictures and so what that meant is for",
    "start": "2169200",
    "end": "2176640"
  },
  {
    "text": "people with lighter skins the photos look great but if you had darker skins the photos",
    "start": "2176640",
    "end": "2183760"
  },
  {
    "text": "were overexposed and you can really see that when you have photos photographs with folks who",
    "start": "2183760",
    "end": "2191119"
  },
  {
    "text": "like their skin tones and darker skin tones in the same image and it wasn't until",
    "start": "2191119",
    "end": "2198560"
  },
  {
    "text": "the 80s and 90s they started using cards with folks with different skin tones in",
    "start": "2198560",
    "end": "2205440"
  },
  {
    "text": "order to calibrate chemicals so this is an example",
    "start": "2205440",
    "end": "2210880"
  },
  {
    "text": "of how uh diversity or representation the data",
    "start": "2210880",
    "end": "2216000"
  },
  {
    "text": "is really important but it's not just about the data that we collect and we use to train a model",
    "start": "2216000",
    "end": "2222880"
  },
  {
    "text": "every choice that we make in the development of a machine learning system has to be aligned with the values",
    "start": "2222880",
    "end": "2229920"
  },
  {
    "text": "that we're hoping to exhibit in the world let me show you a final example here",
    "start": "2229920",
    "end": "2237440"
  },
  {
    "text": "machine translation it's amazing with neural machine translation it can translate between a",
    "start": "2237599",
    "end": "2244400"
  },
  {
    "text": "large number of languages we can also be faced with very significant images so for example if you",
    "start": "2244400",
    "end": "2250480"
  },
  {
    "text": "go from a non-gerund language like english to a gender language like latin language",
    "start": "2250480",
    "end": "2255760"
  },
  {
    "text": "for example portuguese you can get this kind of behavior the doctor called",
    "start": "2255760",
    "end": "2261440"
  },
  {
    "text": "the nurse gets translated to the male doctor called the female nurse",
    "start": "2261440",
    "end": "2266720"
  },
  {
    "text": "which will perpetuate a number of stereotypes now the issue here is",
    "start": "2266720",
    "end": "2272640"
  },
  {
    "text": "in brazil today about 57 of the doctors identify as male",
    "start": "2272640",
    "end": "2278560"
  },
  {
    "text": "now the ratio of doctors they identify as female is increasing rapidly",
    "start": "2278560",
    "end": "2283839"
  },
  {
    "text": "but today if you do a hard translation like this anytime that more than 50",
    "start": "2283839",
    "end": "2289520"
  },
  {
    "text": "of the doctors are male then you're gonna get a male translation the male form",
    "start": "2289520",
    "end": "2296960"
  },
  {
    "text": "and so even if we had infinite data if we design a system this way we'll",
    "start": "2297119",
    "end": "2303520"
  },
  {
    "text": "always be perpetuating the stereotype",
    "start": "2303520",
    "end": "2308599"
  },
  {
    "text": "so even in this setting we have to think about alignment",
    "start": "2312400",
    "end": "2319520"
  },
  {
    "text": "as how we're going to represent our values in every decision that we make along the way",
    "start": "2319520",
    "end": "2327119"
  },
  {
    "text": "so stepping back i think we need to think carefully about where the principles of machine learning",
    "start": "2327119",
    "end": "2333680"
  },
  {
    "text": "they can help us address trust and making trustworthy systems and i'm glad you're here this course in discussing",
    "start": "2333680",
    "end": "2340480"
  },
  {
    "text": "this and my proposed framework is around clarity competence and alignment",
    "start": "2340480",
    "end": "2346160"
  },
  {
    "text": "and with that thank you anthony thanks for having me and i'm happy to take some questions and discussions",
    "start": "2346160",
    "end": "2352240"
  },
  {
    "text": "[Applause]",
    "start": "2352240",
    "end": "2358720"
  },
  {
    "text": "yes thank you for your talk i like your animations um i'm glad to serve yeah",
    "start": "2358720",
    "end": "2364960"
  },
  {
    "text": "could you talk about the interaction between these three different um categories like for example to what",
    "start": "2364960",
    "end": "2370240"
  },
  {
    "text": "degree do you think clarity helps with alignment that's a that's a really great point um",
    "start": "2370240",
    "end": "2375920"
  },
  {
    "text": "you know you'd think that you'd want to create orthogonal access but the axes are not necessarily orthogonal so for",
    "start": "2375920",
    "end": "2381599"
  },
  {
    "text": "example i think they build on each other so to understand alignment part of it is that",
    "start": "2381599",
    "end": "2387119"
  },
  {
    "text": "you want to evaluate competence along multiple metrics that you care about",
    "start": "2387119",
    "end": "2392960"
  },
  {
    "text": "and part of evaluating competence or understanding where the model is working is not working and building trust",
    "start": "2392960",
    "end": "2399920"
  },
  {
    "text": "is the notions of clarity so to to me these are more like layers that build on each other",
    "start": "2399920",
    "end": "2405680"
  },
  {
    "text": "um so for example you asked about the relationship between clarity and alignment",
    "start": "2405680",
    "end": "2411200"
  },
  {
    "text": "i would like to be able to understand who is being impacted by something or",
    "start": "2411200",
    "end": "2417280"
  },
  {
    "text": "if you're making trade-offs who is benefiting and who is not",
    "start": "2417280",
    "end": "2422319"
  },
  {
    "text": "when something happens and you can think about this uh from a fairness perspective in terms of group fairness so what groups or",
    "start": "2422319",
    "end": "2428560"
  },
  {
    "text": "populations let's say um blacks hispanics for example or typical",
    "start": "2428560",
    "end": "2433760"
  },
  {
    "text": "ones that are discussed in terms of race are impacted by the decision models in a disproportionate way compared to whites",
    "start": "2433760",
    "end": "2440640"
  },
  {
    "text": "but you also might want to ask about individuals so how is the treatment that you're getting",
    "start": "2440640",
    "end": "2446079"
  },
  {
    "text": "relate to the ones that everybody else is getting in this class is anthony being fair to you and can we",
    "start": "2446079",
    "end": "2452800"
  },
  {
    "text": "make that more clear or transparent and so part of that is about what values we want to",
    "start": "2452800",
    "end": "2458720"
  },
  {
    "text": "reflect in the system thank you for that question",
    "start": "2458720",
    "end": "2464560"
  },
  {
    "text": "others yes so you and your team did some of the",
    "start": "2464560",
    "end": "2470560"
  },
  {
    "text": "seminal work on explainability methods uh and i forget what year exactly that was but like",
    "start": "2470560",
    "end": "2476720"
  },
  {
    "text": "where have things gone since then what are you most excited about in terms of emerging algorithms and whatnot for",
    "start": "2476720",
    "end": "2482880"
  },
  {
    "text": "explainability yeah it's uh i can tell you what i'm excited about and i can tell you what i'm concerned",
    "start": "2482880",
    "end": "2488960"
  },
  {
    "text": "about um so what i'm excited about is that there's been a tremendous amount of focus on",
    "start": "2488960",
    "end": "2495040"
  },
  {
    "text": "this and a number of different methods or ways to think about explanation so for example in lime explanations are",
    "start": "2495040",
    "end": "2502880"
  },
  {
    "text": "about what features in the input this is called future importance or feature attribution so what parts of",
    "start": "2502880",
    "end": "2509680"
  },
  {
    "text": "the input are important there's others have asked can we use examples or data points",
    "start": "2509680",
    "end": "2515920"
  },
  {
    "text": "to help explain the prediction some folks ask can we use we're called concepts so these are not",
    "start": "2515920",
    "end": "2521440"
  },
  {
    "text": "neither features nor data points these are things like uh when i predict zebras am i using the",
    "start": "2521440",
    "end": "2527440"
  },
  {
    "text": "fact that zebras have stripes in order to make the prediction so there's been a lot of exploration in",
    "start": "2527440",
    "end": "2532480"
  },
  {
    "text": "terms of what kinds of modalities are important for explanations there's been",
    "start": "2532480",
    "end": "2539280"
  },
  {
    "text": "great exploration in terms of making these algorithms more robust and faster and more scalable",
    "start": "2539280",
    "end": "2547359"
  },
  {
    "text": "some are model agnostic like we talked about where you can see the model black box some of them have been",
    "start": "2547520",
    "end": "2554880"
  },
  {
    "text": "opening up the box of the model and using the inside to help make these things better",
    "start": "2554880",
    "end": "2560240"
  },
  {
    "text": "or faster so all that is great work and i'm very excited about it i think that the",
    "start": "2560240",
    "end": "2566720"
  },
  {
    "text": "biggest fault but the biggest concern i have and i think you know we made a mistake",
    "start": "2566720",
    "end": "2574160"
  },
  {
    "text": "six years ago that's been reproduced often which is",
    "start": "2574160",
    "end": "2580160"
  },
  {
    "text": "a lot of these methods go from the explanations forward so they say oh isn't it cool we can explain this way",
    "start": "2580160",
    "end": "2586480"
  },
  {
    "text": "isn't it cool i can have this other notion explanation isn't cool that i can make it faster but almost none of them",
    "start": "2586480",
    "end": "2593839"
  },
  {
    "text": "start from the decision backwards so what decision is a human making",
    "start": "2593839",
    "end": "2599359"
  },
  {
    "text": "like is a human deciding whether to intervene on a patient or not is the human deciding whether or not",
    "start": "2599359",
    "end": "2606400"
  },
  {
    "text": "to join netflix you know what is the decisions being made and how does explanations how do",
    "start": "2606400",
    "end": "2612560"
  },
  {
    "text": "explanations support that decision so one of the things i'm most interested now is to think about the decision-making",
    "start": "2612560",
    "end": "2619520"
  },
  {
    "text": "process so how do humans make decisions and how can we make more informed data-driven decisions",
    "start": "2619520",
    "end": "2626319"
  },
  {
    "text": "based on things like explanations and i think that that's kind of the open biggest open question today uh in",
    "start": "2626319",
    "end": "2634400"
  },
  {
    "text": "my opinion",
    "start": "2634400",
    "end": "2636960"
  },
  {
    "text": "yes um i guess this might remain beyond your talk but in alignment you mentioned",
    "start": "2639440",
    "end": "2645119"
  },
  {
    "text": "that you know you have to figure out trade house between values and i was wondering when such a resolution is not",
    "start": "2645119",
    "end": "2651359"
  },
  {
    "text": "possible um how do you reconciling so for example you gave the machine translation example",
    "start": "2651359",
    "end": "2656720"
  },
  {
    "text": "right um with the fair way of translating it be a female doctor and a male nurse um or you know male doctor",
    "start": "2656720",
    "end": "2663839"
  },
  {
    "text": "female nurse like i guess in my view there isn't truly a resolution by the way um and what happens when",
    "start": "2663839",
    "end": "2671040"
  },
  {
    "text": "your values might be indirect contrast so for example if netflix wants to make sure that users binge watch their",
    "start": "2671040",
    "end": "2677760"
  },
  {
    "text": "content but users interests are perhaps different um is there any way to kind of think about",
    "start": "2677760",
    "end": "2686160"
  },
  {
    "text": "resolving these things and i think it also comes up quite a bit these days in the tech world in the case of privacy where companies and trust would be",
    "start": "2686160",
    "end": "2692880"
  },
  {
    "text": "different from the user's interests um so yeah is there any way you could think about that i",
    "start": "2692880",
    "end": "2698640"
  },
  {
    "text": "really appreciate your question so thanks for bringing it up um and i teach a class",
    "start": "2698640",
    "end": "2704400"
  },
  {
    "text": "on the ethics of ai and i see a couple of the students here today so they're getting a double header of of me on",
    "start": "2704400",
    "end": "2710560"
  },
  {
    "text": "these topics but one of the things that we talk a lot in the class is that",
    "start": "2710560",
    "end": "2716000"
  },
  {
    "text": "there's no right answer there's a lot of nuances and one of the problems is",
    "start": "2716000",
    "end": "2722079"
  },
  {
    "text": "as engineers we're not trained to deal with nuances like every homework that you have has",
    "start": "2722079",
    "end": "2728560"
  },
  {
    "text": "one right answer there's a rubric this day is an automated system to evaluate your homework and you're done",
    "start": "2728560",
    "end": "2735760"
  },
  {
    "text": "and then when you leave here when you leave stanford and get an amazing job at whatever",
    "start": "2735760",
    "end": "2742000"
  },
  {
    "text": "job you want to take you're going to have to deal with the nuances of the real world and as an educator",
    "start": "2742000",
    "end": "2748880"
  },
  {
    "text": "i feel like i failed that we failed to help build up the muscle of trying to",
    "start": "2748880",
    "end": "2754160"
  },
  {
    "text": "lay out the trade-offs and the nuances in this algorithms and so",
    "start": "2754160",
    "end": "2759200"
  },
  {
    "text": "the first thing the the first step you can think about this is a mindfulness exercise the first step is",
    "start": "2759200",
    "end": "2766480"
  },
  {
    "text": "to do what you're talking about can we articulate the different perspectives what are the different values where who",
    "start": "2766480",
    "end": "2772800"
  },
  {
    "text": "are the stakeholders where the different interests they're being represented um can we understand what metrics",
    "start": "2772800",
    "end": "2779440"
  },
  {
    "text": "different people care about and so on and part of what we need to do is that exercise more",
    "start": "2779440",
    "end": "2786480"
  },
  {
    "text": "thoroughly as opposed to saying i'm gonna pick lambda one plus lambda two plus number three and be done",
    "start": "2786480",
    "end": "2793119"
  },
  {
    "text": "and so can we do that more thoroughly and that's what a anathesis would do for example one of the",
    "start": "2793119",
    "end": "2799359"
  },
  {
    "text": "folks who gave a lecture in the ethics of air class methodist um",
    "start": "2799359",
    "end": "2804640"
  },
  {
    "text": "rob rich who's a very well known stanford professor and",
    "start": "2804640",
    "end": "2810160"
  },
  {
    "text": "you know he jokes a lot that people come to him and say oh you're the ethicist come solve this ethical problem for me",
    "start": "2810160",
    "end": "2816800"
  },
  {
    "text": "like how do i deal with this machine translation give me the answer and he doesn't have the answer",
    "start": "2816800",
    "end": "2823040"
  },
  {
    "text": "like what he's very good at is what ethicists turned out to be very good at is articulating the perspectives",
    "start": "2823040",
    "end": "2831359"
  },
  {
    "text": "like helping you write down okay this is one perspective and this is what they care about it's another perspective",
    "start": "2831359",
    "end": "2837760"
  },
  {
    "text": "what they care about so there's that process but ultimately as an engineer you're gonna make one choice like you're",
    "start": "2837760",
    "end": "2843280"
  },
  {
    "text": "shipping a machine translation system and you're going to pick an operating point or a decision of how that's going",
    "start": "2843280",
    "end": "2848640"
  },
  {
    "text": "to go and so what what i'm interested in is can we help articulate the process of",
    "start": "2848640",
    "end": "2854960"
  },
  {
    "text": "trade-offs can we understand if you make a choice since you have to make one choice who is",
    "start": "2854960",
    "end": "2860400"
  },
  {
    "text": "impacted so who is going to benefit who's not going to benefit can we articulate that can we",
    "start": "2860400",
    "end": "2866559"
  },
  {
    "text": "make that transparent or more explainable in a way that you can understand that and that happens all",
    "start": "2866559",
    "end": "2872160"
  },
  {
    "text": "over you mentioned privacy is a great example of that and i've been involved in very complex",
    "start": "2872160",
    "end": "2877520"
  },
  {
    "text": "privacy decisions that um that again there are no right answers",
    "start": "2877520",
    "end": "2882559"
  },
  {
    "text": "and you have to figure out what choices to make so sorry i have not answered",
    "start": "2882559",
    "end": "2887599"
  },
  {
    "text": "your question just to say that there's lots and lots and we have to be more better at dealing with those",
    "start": "2887599",
    "end": "2894240"
  },
  {
    "text": "any other questions",
    "start": "2895200",
    "end": "2898680"
  },
  {
    "text": "yes uh in your in the section about alignment um seems like you talked about uh like aligning",
    "start": "2900800",
    "end": "2907359"
  },
  {
    "text": "loss and metrics that gap and um kind of these these other like current",
    "start": "2907359",
    "end": "2912720"
  },
  {
    "text": "uh differences in how we deploy our models how like we intend them to do you think like i guess what are your hopes",
    "start": "2912720",
    "end": "2918960"
  },
  {
    "text": "for your research um thinking like larger agi general intelligence stuff",
    "start": "2918960",
    "end": "2925359"
  },
  {
    "text": "do you think things like adaptive loss alignment or other algorithms like that also translate to aligning superhuman ai",
    "start": "2925359",
    "end": "2931760"
  },
  {
    "text": "to human values broadly well that's that's a big question so",
    "start": "2931760",
    "end": "2937119"
  },
  {
    "text": "um agi before we go to agi i'm a big believer in augmentation over",
    "start": "2937119",
    "end": "2944800"
  },
  {
    "text": "replacement so in other words how can a.i help us",
    "start": "2944800",
    "end": "2951119"
  },
  {
    "text": "um be super humans let's say or you know do things that we didn't think would be",
    "start": "2951119",
    "end": "2956240"
  },
  {
    "text": "able to do before and if you take that perspective of things so ai for",
    "start": "2956240",
    "end": "2961760"
  },
  {
    "text": "as a tool for augmentation um they can get better and better alignment is incredibly important right",
    "start": "2961760",
    "end": "2968319"
  },
  {
    "text": "so i think that that is uh that's something that you know adapter loves element i",
    "start": "2968319",
    "end": "2973359"
  },
  {
    "text": "think is a tiny step in this very big problem because in our heads we have a set of",
    "start": "2973359",
    "end": "2979359"
  },
  {
    "text": "values we care about we don't even know how to write them down and so how do we help translate what's",
    "start": "2979359",
    "end": "2986640"
  },
  {
    "text": "in our head to a behavior of a system an example i use in the ethics of ai class",
    "start": "2986640",
    "end": "2992800"
  },
  {
    "text": "is a self-driving car you're probably familiar with the question of you know the trolley problem",
    "start": "2992800",
    "end": "2999839"
  },
  {
    "text": "like do i veer myself off a cliff with a car or do i hit somebody in the",
    "start": "2999839",
    "end": "3005920"
  },
  {
    "text": "road but take a much simpler question um do i",
    "start": "3005920",
    "end": "3011599"
  },
  {
    "text": "drive on a windy road fast to get there faster or do i take more risks",
    "start": "3011599",
    "end": "3018319"
  },
  {
    "text": "or do i take a more conservative kind of safer but slower pace",
    "start": "3018319",
    "end": "3024000"
  },
  {
    "text": "that's a hard decision does self-driving car dog call make that",
    "start": "3024000",
    "end": "3029760"
  },
  {
    "text": "decision for you like you step into the car and say it decides to you know",
    "start": "3029760",
    "end": "3036000"
  },
  {
    "text": "drive slowly or drive fast or something does it ask you to enter your",
    "start": "3036000",
    "end": "3041200"
  },
  {
    "text": "preferences in this you know interface with lots of sliders can you understand that",
    "start": "3041200",
    "end": "3047440"
  },
  {
    "text": "is there something in between like what does that look like and and i",
    "start": "3047440",
    "end": "3053119"
  },
  {
    "text": "want to say that our or even for that very simple example our values are not aligned",
    "start": "3053119",
    "end": "3059359"
  },
  {
    "text": "like and it depends it's contextual if i have all my kids in my car",
    "start": "3059359",
    "end": "3066160"
  },
  {
    "text": "i might want to behave differently than you know if by myself my kids are going to college maybe i'll",
    "start": "3066160",
    "end": "3072160"
  },
  {
    "text": "behave different than if my kids are young and depend on me i don't know and so",
    "start": "3072160",
    "end": "3077680"
  },
  {
    "text": "uh how do you how do you make this complex context contextual decisions in",
    "start": "3077680",
    "end": "3083040"
  },
  {
    "text": "collaborative ai um human settings it's a it's a really hard",
    "start": "3083040",
    "end": "3088880"
  },
  {
    "text": "question and so i i'm very interested in that and when you get to agi i don't",
    "start": "3088880",
    "end": "3094000"
  },
  {
    "text": "know what you mean by agi do you mean agi is a human um you know an entity that",
    "start": "3094000",
    "end": "3100800"
  },
  {
    "text": "is itself sentient and uh is it has its own separate value",
    "start": "3100800",
    "end": "3105839"
  },
  {
    "text": "system or is it a reflection of our value system and that's a that's a very complex philosophical question too",
    "start": "3105839",
    "end": "3113040"
  },
  {
    "text": "hope that helps",
    "start": "3113119",
    "end": "3115838"
  },
  {
    "text": "all right let's thank carlos [Applause]",
    "start": "3119040",
    "end": "3125519"
  },
  {
    "text": "you",
    "start": "3127280",
    "end": "3129359"
  }
]