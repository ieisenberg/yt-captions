[
  {
    "text": "Hello, and welcome to the section on non-quadratic regularizers.",
    "start": "4220",
    "end": "10150"
  },
  {
    "text": "So remember the idea of regularization.",
    "start": "13070",
    "end": "18105"
  },
  {
    "text": "We want to choose our Theta which both minimizes the empirical risk and also makes the predictor be not too sensitive.",
    "start": "18105",
    "end": "29700"
  },
  {
    "text": "So I've got an x near an x tilde. Then we'd like g Theta of x to also be close to g Theta of x tilde.",
    "start": "29700",
    "end": "40585"
  },
  {
    "text": "And the reason for this deduction in sensitivity that we're going to put in there is that if you make the predictor too sensitive,",
    "start": "40585",
    "end": "48065"
  },
  {
    "text": "then what happens is you end up with something that doesn't generalize well. And so this is a way of forcing the predictor to be insensitive to the data.",
    "start": "48065",
    "end": "59525"
  },
  {
    "text": "And that makes the predictor generalize better, it's a way of preventing overfitting.",
    "start": "59525",
    "end": "65610"
  },
  {
    "text": "And so what we do in order to achieve this is we use a regularizer, our function R,",
    "start": "65650",
    "end": "71269"
  },
  {
    "text": "which is a function of the parameters Theta, and it's a real valued function of R, which measures the sensitivity of g Theta.",
    "start": "71269",
    "end": "79355"
  },
  {
    "text": "So that in particular when Theta is large, then our function- our regularizer function is also large.",
    "start": "79355",
    "end": "85670"
  },
  {
    "text": "And so by making the regularizer small, we'll make Theta small and thereby we'll make the sensitivity of g Theta small.",
    "start": "85670",
    "end": "93299"
  },
  {
    "text": "There's another way of thinking about this, and this is very common in the statistical literature,",
    "start": "93470",
    "end": "98915"
  },
  {
    "text": "in the statistical view point. And that is that the regularizer's encoding some prior information we have about Theta.",
    "start": "98915",
    "end": "106650"
  },
  {
    "text": "Specifically that the regularized r of Theta is actually small.",
    "start": "106650",
    "end": "111760"
  },
  {
    "text": "And so this is a way of saying, well, we believe that the Theta that generalizes, uh,",
    "start": "111760",
    "end": "118725"
  },
  {
    "text": "that could- that corresponds to the true predictor underlying the data,",
    "start": "118725",
    "end": "124085"
  },
  {
    "text": "the true model of the data has a small Theta.",
    "start": "124085",
    "end": "130365"
  },
  {
    "text": "And so we're going to enforce that. In our learning algorithm, we only look at Thetas that are actually small.",
    "start": "130365",
    "end": "138224"
  },
  {
    "text": "And that's a completely, uh, uh, different way of looking at, uh,",
    "start": "138225",
    "end": "143340"
  },
  {
    "text": "the purpose of the regularizer, but it's equally valid. In both cases, you want both",
    "start": "143340",
    "end": "150350"
  },
  {
    "text": "the empirical risk l of Theta and the regularized r of Theta to be small.",
    "start": "150350",
    "end": "156390"
  },
  {
    "text": "So a regularized empirical risk minimization, we choose Theta to minimize the empirical risk, l of Theta",
    "start": "160500",
    "end": "169210"
  },
  {
    "text": "plus some positive constant Lambda multiplied by r of Theta, the regularizer.",
    "start": "169210",
    "end": "175450"
  },
  {
    "text": "And remember Lambda here is called a regularization hyper-parameter, which we can trade off L of Theta against r of Theta.",
    "start": "175450",
    "end": "183040"
  },
  {
    "text": "And we choose it by validating against data in a separate set called a test set.",
    "start": "183040",
    "end": "190420"
  },
  {
    "text": "And the- the trick of all of this, of course, is that this actually works. It works in the sense that by enforcing regularization,",
    "start": "190420",
    "end": "199060"
  },
  {
    "text": "we end up with worse performance on the training set, but better performance on- on",
    "start": "199060",
    "end": "206300"
  },
  {
    "text": "the test set and it's test set performance that we actually care about.",
    "start": "206300",
    "end": "210810"
  },
  {
    "text": "Now, when you're constructing regularizers, we've seen so far the two-norm used as a regularizer in ridge regression.",
    "start": "212980",
    "end": "221644"
  },
  {
    "text": "And a very common format for regularizers is to have a penalty function q,",
    "start": "221645",
    "end": "229675"
  },
  {
    "text": "which is a function which maps the real numbers to the real numbers. And the regularizer with Theta is simply q of Theta_1 plus q of Theta_2,",
    "start": "229675",
    "end": "239465"
  },
  {
    "text": "all the way up to all of our parameters, Theta_p. And so we penalize each of the parameters,",
    "start": "239465",
    "end": "246635"
  },
  {
    "text": "each of the components of Theta separately and add up the corresponding penalizations.",
    "start": "246635",
    "end": "253965"
  },
  {
    "text": "And normally we choose these penalty functions q such that they are non-negative.",
    "start": "253965",
    "end": "259234"
  },
  {
    "text": "And so that they're only 0 when Theta_i is 0.",
    "start": "259235",
    "end": "265599"
  },
  {
    "text": "And q of Theta_ i is therefore expressing our displeasure in choosing the predictor coefficient Theta_ i.",
    "start": "265600",
    "end": "272764"
  },
  {
    "text": "And in particular, by defining it so that it increases with the value of Theta_i,",
    "start": "272765",
    "end": "278750"
  },
  {
    "text": "it expresses the fact that we prefer small Theta_i's of a large Theta_i's.",
    "start": "278750",
    "end": "285475"
  },
  {
    "text": "And so we've seen the case where we have q of Theta_y is just Theta_i squared.",
    "start": "285475",
    "end": "293300"
  },
  {
    "text": "That's the sum of square regularizer or ridge regression,",
    "start": "293300",
    "end": "298310"
  },
  {
    "text": "it's also called a quadratic regularizer, the Tikhonov regularizer, or the l_2 regularizer.",
    "start": "298310",
    "end": "305065"
  },
  {
    "text": "And it's the q square function, it's just q squared of a, to a squared.",
    "start": "305065",
    "end": "311294"
  },
  {
    "text": "And with that penalty function then the regularizer is just the norm of Theta squared.",
    "start": "311295",
    "end": "317515"
  },
  {
    "text": "And in particular, it's the 2-norm of Theta squared. Another very common regularizer is the sum absolute function, um,",
    "start": "317515",
    "end": "328310"
  },
  {
    "text": "where the penalty function that we're using is q_abs, the absolute value function.",
    "start": "328310",
    "end": "335865"
  },
  {
    "text": "And that way, the regularizer function r of Theta is the sum of the absolute values of the components of Theta,",
    "start": "335865",
    "end": "344600"
  },
  {
    "text": "that's called the 1-norm of the vector Theta. We'll call that sum absolute regularization or l_1",
    "start": "344600",
    "end": "353389"
  },
  {
    "text": "regularization, or lasso regularization. Ah, another one we might see is the non-negative regularizer where",
    "start": "353390",
    "end": "365660"
  },
  {
    "text": "the penalty function is 0 when a is greater than or equal to 0 and it's infinity when a is less than 0.",
    "start": "365660",
    "end": "374915"
  },
  {
    "text": "And that's, uh, a very hard penalty. It says that the only Thetas that we will accept",
    "start": "374915",
    "end": "383224"
  },
  {
    "text": "are Thetas for which all of the components of Theta are greater than or equal to 0.",
    "start": "383224",
    "end": "389900"
  },
  {
    "text": "So in other words, we are enforcing the fact that the predictor coefficients have to be non-negative.",
    "start": "389900",
    "end": "395614"
  },
  {
    "text": "And any minimizer, which minimizes L of Theta plus r of Theta,",
    "start": "395614",
    "end": "401690"
  },
  {
    "text": "where r has this form. The only way you can have a minimizer as if the resulting Theta is, uh, non-negative.",
    "start": "401690",
    "end": "412139"
  },
  {
    "text": "Now, let's, uh, uh, look at this in the context of sensitivity. And, uh, suppose we've got a- [NOISE] a- a linear predictor,",
    "start": "420800",
    "end": "431039"
  },
  {
    "text": "g_Theta of x, which is Theta transpose x. And this here is, uh,",
    "start": "431039",
    "end": "436950"
  },
  {
    "text": "uh, we're predicting a scalar y. And let's suppose that the feature vector x changes to x tilde,",
    "start": "436950",
    "end": "445505"
  },
  {
    "text": "which is x plus some Delta. And Delta here is the perturbation or change in x.",
    "start": "445505",
    "end": "452020"
  },
  {
    "text": "And we'll assume for now that any Delta, uh, is possible.",
    "start": "452020",
    "end": "457544"
  },
  {
    "text": "But we're gonna look at the set of, uh, perturbations, capital Delta.",
    "start": "457545",
    "end": "464310"
  },
  {
    "text": "And, uh, we're only going to allow perturbations, little Delta that live in this capital Delta set.",
    "start": "464310",
    "end": "471225"
  },
  {
    "text": "We'll call it the feature perturbation set. And, um, we can say, well,",
    "start": "471225",
    "end": "479324"
  },
  {
    "text": "suppose Theta is a predictor parameter and x now changes to x plus Delta,",
    "start": "479325",
    "end": "489105"
  },
  {
    "text": "what happens to our prediction? In that case, the prediction becomes Theta transpose multiplied by x plus Delta.",
    "start": "489105",
    "end": "499815"
  },
  {
    "text": "And so the change in the prediction is going to be Theta transpose x tilde minus Theta transpose x,",
    "start": "499815",
    "end": "508080"
  },
  {
    "text": "which is just gonna be Theta transpose Delta. So if we look at the absolute value of that quantity,",
    "start": "508080",
    "end": "514919"
  },
  {
    "text": "the absolute value of Theta transpose Delta, that's the magnitude of the change in prediction.",
    "start": "514920",
    "end": "520680"
  },
  {
    "text": "And we can ask ourselves, \"Well, how big can this be when we're allowing Delta to range in some set capital Delta.",
    "start": "520680",
    "end": "528795"
  },
  {
    "text": "And we can look at the worst-case sensitivity, the maximum over all Deltas",
    "start": "528795",
    "end": "534975"
  },
  {
    "text": "in capital Delta of the absolute value as Theta transpose Delta.",
    "start": "534975",
    "end": "540285"
  },
  {
    "text": "This is a measure of sensitivity, right? If we for some specific set capital Delta, then we can say,",
    "start": "540285",
    "end": "549090"
  },
  {
    "text": "well, this quantity tells us how much the prediction can change in the worst case.",
    "start": "549090",
    "end": "556460"
  },
  {
    "text": "[NOISE] So let's look",
    "start": "556460",
    "end": "565380"
  },
  {
    "text": "at the case where we're looking at l_2 perturbations. In other words, we're looking at the script,",
    "start": "565380",
    "end": "571829"
  },
  {
    "text": "the set capital Delta to be the set of all perturbations little Delta with 2-norm less than or equal to Epsilon.",
    "start": "571830",
    "end": "581910"
  },
  {
    "text": "Epsilon is some number. And this is a ball, this is a- a sphere in a Delta space.",
    "start": "581910",
    "end": "591689"
  },
  {
    "text": "So the set of Deltas which are in capital Delta is a sphere- capital Delta is a sphere. [NOISE] And this is called an l_2 ball.",
    "start": "591690",
    "end": "600779"
  },
  {
    "text": "And this means that the feature vector x can change to any x tilde within distance Epsilon.",
    "start": "600780",
    "end": "608279"
  },
  {
    "text": "And in particular, we're measuring distance with the 2-norm. Now, as we've seen before,",
    "start": "608280",
    "end": "614279"
  },
  {
    "text": "the Cauchy-Schwarz inequality tells us that the absolute value of Theta transpose Delta is less than or equal to",
    "start": "614280",
    "end": "620070"
  },
  {
    "text": "the 2-norm of Theta multiplied by the 2-norm of Delta. And the 2-norm of Delta we know has to be less than or",
    "start": "620070",
    "end": "627030"
  },
  {
    "text": "equal to Epsilon for all Deltas in our lab set. And that means that the absolute value of",
    "start": "627030",
    "end": "633149"
  },
  {
    "text": "Theta transpose Delta is less than or equal to Epsilon times the 2-norm of Theta.",
    "start": "633150",
    "end": "638440"
  },
  {
    "text": "So in particular, the worst-case sensitivity is therefore Epsilon times the 2-norm of Theta.",
    "start": "638840",
    "end": "647235"
  },
  {
    "text": "And maximizing the change in the prediction at a- all possible Deltas that we're allowed to choose,",
    "start": "647235",
    "end": "655020"
  },
  {
    "text": "the one that maximizes the change in the prediction is, um, Epsilon divided by the 2-norm of Theta times Theta.",
    "start": "655020",
    "end": "663825"
  },
  {
    "text": "In other words, we take Theta and we normalize that vector, and multiply by Epsilon.",
    "start": "663825",
    "end": "670810"
  },
  {
    "text": "And so this is a justification for the sum of square regularization.",
    "start": "671750",
    "end": "677370"
  },
  {
    "text": "If we are concerned about mi- minimizing",
    "start": "677370",
    "end": "683385"
  },
  {
    "text": "the worst case sensitivity to changes in x,",
    "start": "683385",
    "end": "691695"
  },
  {
    "text": "which are in a unit l_2 ball,",
    "start": "691695",
    "end": "697605"
  },
  {
    "text": "then the sum of square regularizer measures exactly that.",
    "start": "697605",
    "end": "704524"
  },
  {
    "text": "It measures how badly can much- can the prediction change when we know",
    "start": "704525",
    "end": "713660"
  },
  {
    "text": "that x can only change by an amount Epsilon in distance?",
    "start": "713660",
    "end": "723839"
  },
  {
    "text": "Now, that's a way of interpreting the purpose of l_2 regularization.",
    "start": "729500",
    "end": "735360"
  },
  {
    "text": "And it's, uh, it says what you're doing is you're minimizing the worst-case response to a particular class of perturbations.",
    "start": "735360",
    "end": "744405"
  },
  {
    "text": "Now, if we look at the L-infinity perturbation class instead,",
    "start": "744405",
    "end": "752415"
  },
  {
    "text": "well, then we'll get a different appropriate choice of regularizer. So the L-infinity class of perturbations,",
    "start": "752415",
    "end": "759435"
  },
  {
    "text": "that's this allowed set of Deltas, allowed set of changes to x,",
    "start": "759435",
    "end": "764745"
  },
  {
    "text": "which- for which each of the components Delta_i has absolute value less than equal to Epsilon.",
    "start": "764745",
    "end": "774180"
  },
  {
    "text": "So people call that an L-infinity ball, but of course really it's just a cube,",
    "start": "774180",
    "end": "779685"
  },
  {
    "text": "that you might call a hypercube in d dimensions. And so capital Delta here is a cube.",
    "start": "779685",
    "end": "786960"
  },
  {
    "text": "And we're saying that you can take an x and perturb it by a vector which is in this cube,",
    "start": "786960",
    "end": "793665"
  },
  {
    "text": "and the- the widths of the sides of this cube are 2 Epsilon. Uh, and the reason it's called an L-infinity ball is that",
    "start": "793665",
    "end": "803640"
  },
  {
    "text": "it has exactly the same form as the- the usual ball. It's the norm of Deltas as less than or equal to",
    "start": "803640",
    "end": "811290"
  },
  {
    "text": "Epsilon as the defining characteristic of this ball. But here the norm is a different norm,",
    "start": "811290",
    "end": "817905"
  },
  {
    "text": "instead of being the 2-norm, it's the infinity norm. And the infinity norm of a vector Delta",
    "start": "817905",
    "end": "825449"
  },
  {
    "text": "is the maximum of the absolute value of the components of Delta.",
    "start": "825449",
    "end": "830894"
  },
  {
    "text": "So this is a very complicated way of defining a cube in terms of a norm.",
    "start": "830895",
    "end": "837690"
  },
  {
    "text": "Um, so this means that any component of the feature vector can change by up to Epsilon.",
    "start": "837690",
    "end": "844100"
  },
  {
    "text": "So instead of saying, we're going to think about changes to x as being",
    "start": "844100",
    "end": "849320"
  },
  {
    "text": "changes to the vector x and they all live in the sphere. And if you're gonna have changes in the sphere,",
    "start": "849320",
    "end": "855920"
  },
  {
    "text": "it means if you change a lot in one direction, then you can't change very much in any other direction. Here we're saying you can change all of the components separately,",
    "start": "855920",
    "end": "864200"
  },
  {
    "text": "and you can change all of those components by up to Epsilon. And with that kind of model,",
    "start": "864200",
    "end": "870055"
  },
  {
    "text": "how big can the absolute value of Theta transpose Delta be? Um, now, there's a way of maximizing the change in",
    "start": "870055",
    "end": "880570"
  },
  {
    "text": "the absolute value of Theta transpose Delta when Delta is required to be in a cube.",
    "start": "880570",
    "end": "886455"
  },
  {
    "text": "And the idea is, is that you take the Thetas and you choose",
    "start": "886455",
    "end": "891700"
  },
  {
    "text": "your Deltas to be Epsilon times the sine of the Thetas.",
    "start": "891700",
    "end": "898085"
  },
  {
    "text": "So if Theta_i is positive, then you will choose Delta_i to be Epsilon.",
    "start": "898085",
    "end": "905800"
  },
  {
    "text": "And if Theta_i is negative, then you'll choose Delta_i to be minus Epsilon.",
    "start": "905800",
    "end": "911240"
  },
  {
    "text": "And if you do that, then what you get is that the change in the prediction Theta transpose times Delta,",
    "start": "911240",
    "end": "922920"
  },
  {
    "text": "well, tha becomes Theta transpose times sine of Theta, where sine of Theta is applied element-wise to the vector Theta,",
    "start": "922920",
    "end": "931170"
  },
  {
    "text": "all multiplied by Epsilon. And that becomes Epsilon times the sum of the absolute values of Theta,",
    "start": "931170",
    "end": "939845"
  },
  {
    "text": "which is Epsilon times the 1-norm of Theta. In other words, if x is allowed to change by",
    "start": "939845",
    "end": "950940"
  },
  {
    "text": "any- change any component by Epsilon possibly or the components by up to Epsilon,",
    "start": "950940",
    "end": "960115"
  },
  {
    "text": "the worst-case change you're going to have in Theta transpose x is Epsilon times the sum of the absolute values of the Theta_i's.",
    "start": "960115",
    "end": "972000"
  },
  {
    "text": "And if you're concerned about those kinds of perturbations happening to x,",
    "start": "972370",
    "end": "977795"
  },
  {
    "text": "then your regularizer that you should choose is the sum of the absolute regularizer.",
    "start": "977795",
    "end": "983524"
  },
  {
    "text": "So r of Theta should be the 1-norm of Theta, which is the sum of the absolute values of Theta_i.",
    "start": "983525",
    "end": "990480"
  },
  {
    "text": "Now, this is all well and good. This is a- an- an analysis of what it",
    "start": "991790",
    "end": "1000310"
  },
  {
    "text": "means to penalize this particular- these particular regularizers.",
    "start": "1000310",
    "end": "1007250"
  },
  {
    "text": "Um, er, it's important not to take this too far or too seriously, right?",
    "start": "1007250",
    "end": "1018190"
  },
  {
    "text": "This is, uh, certainly gives us a way of interpreting the meaning of choosing these particular regularizations.",
    "start": "1018190",
    "end": "1025905"
  },
  {
    "text": "But it doesn't give us a way of choosing which one we should apply in a particular machine learning problem.",
    "start": "1025905",
    "end": "1034509"
  },
  {
    "text": "The way we would do that is, well, there's two ways.",
    "start": "1034510",
    "end": "1040600"
  },
  {
    "text": "One is that we gain some experience with a wide range of machine learning problems.",
    "start": "1040600",
    "end": "1046704"
  },
  {
    "text": "And we see that for some types of machine learning problems, the 2-norm regularizer tends to do better.",
    "start": "1046704",
    "end": "1052450"
  },
  {
    "text": "And for other types of machine learning problems, the 1-norm regularizer tends to do better.",
    "start": "1052450",
    "end": "1058880"
  },
  {
    "text": "And we'll have something to say along those lines in this section. The other way is even more pragmatic;",
    "start": "1058880",
    "end": "1066370"
  },
  {
    "text": "and that is to say, validate. You want to know which one's better?",
    "start": "1066370",
    "end": "1071390"
  },
  {
    "text": "Try both of them, compute the performance, compute the empirical risk on your test set,",
    "start": "1071390",
    "end": "1079480"
  },
  {
    "text": "and see which one did better. [NOISE]",
    "start": "1079480",
    "end": "1089399"
  },
  {
    "text": "So we have now, uh, two types of regularization that we're going to focus on.",
    "start": "1089400",
    "end": "1098340"
  },
  {
    "text": "One is 2-norm regularization. And if, uh, we're using the square loss, remember,",
    "start": "1098340",
    "end": "1106530"
  },
  {
    "text": "which is l of y hat y, is the square of y-hat minus y. Again, when y is scalar.",
    "start": "1106530",
    "end": "1113385"
  },
  {
    "text": "Then if you choose Theta to minimize l of Theta plus Lambda times the two-norm of Theta squared,",
    "start": "1113385",
    "end": "1119280"
  },
  {
    "text": "that's called ridge regression. The other one we'll focus on is where we're using the one-norm of Theta as our regressor.",
    "start": "1119280",
    "end": "1128820"
  },
  {
    "text": "And then we're gonna be minimizing empirical risk plus Lambda times the 1-norm of Theta,",
    "start": "1128820",
    "end": "1134669"
  },
  {
    "text": "and that's called lasso regression. Uh, this was invented at Stanford by ‎Robert Tibshirani in 1994 and",
    "start": "1134670",
    "end": "1144765"
  },
  {
    "text": "lasso regularization- lasso regression is actually widely used in advanced machine learning.",
    "start": "1144765",
    "end": "1152655"
  },
  {
    "text": "Um, we saw for ridge regression that even though it looks like an extension of least squares,",
    "start": "1152655",
    "end": "1162225"
  },
  {
    "text": "the problem actually reduces to a least squares problem and we can apply the least squares formula to solve explicitly for the optimal Theta.",
    "start": "1162225",
    "end": "1172200"
  },
  {
    "text": "Um, lasso regression doesn't have a formula. There is no analytical expression,",
    "start": "1172200",
    "end": "1179100"
  },
  {
    "text": "there's no algebraic expression for the Theta that",
    "start": "1179100",
    "end": "1184350"
  },
  {
    "text": "minimizes the regularized empirical risk when the regularizer is the one-norm.",
    "start": "1184350",
    "end": "1191530"
  },
  {
    "text": "When the predictor is linear,",
    "start": "1194630",
    "end": "1200835"
  },
  {
    "text": "then if we're using square loss, both ridge regression and lasso regression can be efficiently computed.",
    "start": "1200835",
    "end": "1210510"
  },
  {
    "text": "For ridge regression, there's a formula. For lasso regression, we have to use numerical optimization.",
    "start": "1210510",
    "end": "1216645"
  },
  {
    "text": "But these numerical optimization methods are extremely efficient and always",
    "start": "1216645",
    "end": "1223260"
  },
  {
    "text": "converge to the global optimal solution because the problem is convex.",
    "start": "1223260",
    "end": "1230830"
  },
  {
    "text": "Now, when we have a constant feature, so we have x_1 is 1, and the predictor coefficient Theta_1 is the offset,",
    "start": "1234310",
    "end": "1243184"
  },
  {
    "text": "again, when the predictor is linear. So in that case, we have g-Theta of x is Theta_1 plus Theta_2, x_2.",
    "start": "1243185",
    "end": "1257700"
  },
  {
    "text": "And so x_1 doesn't change even when,",
    "start": "1260200",
    "end": "1268279"
  },
  {
    "text": "uh, x changes, uh,",
    "start": "1268280",
    "end": "1273775"
  },
  {
    "text": "we did not consider perturbations to x_1 because x_1 is the constant feature.",
    "start": "1273775",
    "end": "1280320"
  },
  {
    "text": "So Delta 1 is always considered to be 0. As a result, Theta_1 does not contribute to predict a sensitivity and as a result,",
    "start": "1280320",
    "end": "1290880"
  },
  {
    "text": "we do not regularize the associated coefficients in either one. And so we modify both regularizers to only include terms from Theta_2 to Theta_d.",
    "start": "1290880",
    "end": "1301455"
  },
  {
    "text": "We either take the 2-norm of Theta_2 to Theta_d or the 1-norm of Theta_2 to Theta_d.",
    "start": "1301455",
    "end": "1310090"
  },
  {
    "text": "Now, as a specific property of certain regularizers which is very useful and that's to do with sparseness.",
    "start": "1319520",
    "end": "1328965"
  },
  {
    "text": "So if I have a linear predictor, g Theta of x is Theta transpose x,",
    "start": "1328965",
    "end": "1335250"
  },
  {
    "text": "where Theta is sparse, sparse here means that many of the entries of Theta are 0.",
    "start": "1335250",
    "end": "1342554"
  },
  {
    "text": "So the number of non-zero components of Theta is small compared to the length of Theta.",
    "start": "1342555",
    "end": "1349965"
  },
  {
    "text": "Then, um, as a result, the prediction, Theta transpose x doesn't depend on some of the features x.",
    "start": "1349965",
    "end": "1358065"
  },
  {
    "text": "In particular, it doesn't depend on those features x_i for which Theta_i is 0.",
    "start": "1358065",
    "end": "1364149"
  },
  {
    "text": "Um, it means that, well, if I've got a predictor that only uses some features,",
    "start": "1364670",
    "end": "1370365"
  },
  {
    "text": "rather than thinking about it as g Theta of all of x, I can think about it as g Theta of just those particular components of x,",
    "start": "1370365",
    "end": "1378420"
  },
  {
    "text": "those with which Theta_i is non-zero. At least I can have practical benefits.",
    "start": "1378420",
    "end": "1386265"
  },
  {
    "text": "Um, uh, in particular, um, you can make the predictor simpler to interpret.",
    "start": "1386265",
    "end": "1394835"
  },
  {
    "text": "Instead of having a prediction as to somebody's illness based on 1,000 different measured properties and diagnostic tests,",
    "start": "1394835",
    "end": "1405555"
  },
  {
    "text": "we might have a prediction which is- actually only depends on a few and that can be much more useful.",
    "start": "1405555",
    "end": "1411404"
  },
  {
    "text": "It's also much cheaper if I want to- decide that I'm going to try and diagnose somebody's health using such an algorithm.",
    "start": "1411405",
    "end": "1418980"
  },
  {
    "text": "If I only have to run four or five blood tests rather than 50 or 60,",
    "start": "1418980",
    "end": "1425520"
  },
  {
    "text": "that's a huge savings. Um, so the predictor is simpler to interpret,",
    "start": "1425520",
    "end": "1431684"
  },
  {
    "text": "it's cheaper to actually execute, um, and you can actually get better performance.",
    "start": "1431685",
    "end": "1437565"
  },
  {
    "text": "And this happens when some of the components x_i,",
    "start": "1437565",
    "end": "1442995"
  },
  {
    "text": "some of the regressors, are actually irrelevant. So imagine we're in a situation where we've got y and we've got x.",
    "start": "1442995",
    "end": "1453195"
  },
  {
    "text": "And some of the components of x actually don't affect y at all. We might be trying to,",
    "start": "1453195",
    "end": "1459645"
  },
  {
    "text": "uh- uh, to fit house price. And we've got various components of x,",
    "start": "1459645",
    "end": "1464760"
  },
  {
    "text": "such as lot area, lot size, number of bedrooms. But we've also got components of x,",
    "start": "1464760",
    "end": "1470475"
  },
  {
    "text": "such as the current weather in Barcelona, which may be totally irrelevant to the house price in Mountain View.",
    "start": "1470475",
    "end": "1479775"
  },
  {
    "text": "Um, but even if it's irrelevant, our learning algorithm may assign that regressor, that component,",
    "start": "1479775",
    "end": "1490050"
  },
  {
    "text": "a non-zero Theta, because it achieves slightly be- better performance on the training set with a non-zero Theta than,",
    "start": "1490050",
    "end": "1502110"
  },
  {
    "text": "uh, it does with a 0Theta. And, uh, that's totally reasonable and that's exactly what will typically happen.",
    "start": "1502110",
    "end": "1510255"
  },
  {
    "text": "The learning algorithm can't tell the difference between",
    "start": "1510255",
    "end": "1515760"
  },
  {
    "text": "components of the variables x,",
    "start": "1515760",
    "end": "1521700"
  },
  {
    "text": "which are just noise and components which are actually meaningful. If choosing non-zero components makes",
    "start": "1521700",
    "end": "1529035"
  },
  {
    "text": "the empirical risks more low, that's what it will do. Um, but of course,",
    "start": "1529035",
    "end": "1535769"
  },
  {
    "text": "it's total nonsense from a practical perspective. We've got a- a predictor that we know is fitting the wrong thing.",
    "start": "1535770",
    "end": "1545399"
  },
  {
    "text": "Um, and so if we could somehow remove those irrelevant regressors,",
    "start": "1545400",
    "end": "1554250"
  },
  {
    "text": "then we might not achieve better training loss, which we won't do.",
    "start": "1554250",
    "end": "1559485"
  },
  {
    "text": "But we might achieve better test loss. And so by- if we can somehow induce sparsity in",
    "start": "1559485",
    "end": "1567975"
  },
  {
    "text": "our linear predictor- if we can induce sparsity in a linear predictor,",
    "start": "1567975",
    "end": "1573990"
  },
  {
    "text": "then it may enable us to remove those components of Theta,",
    "start": "1573990",
    "end": "1582075"
  },
  {
    "text": "which don't really matter, but just happen to be correlated the right way with the training data.",
    "start": "1582075",
    "end": "1589630"
  },
  {
    "text": "This idea that we're going to choose the sparsity pattern of Theta is called feature",
    "start": "1590480",
    "end": "1596549"
  },
  {
    "text": "selection and there are several different ways of carrying it out.",
    "start": "1596550",
    "end": "1602830"
  },
  {
    "text": "One, in particular, is call- is to use l_1 regularization.",
    "start": "1604310",
    "end": "1609750"
  },
  {
    "text": "And it is a- an important property of l_1 regularization that it leads to sparse coefficient vectors.",
    "start": "1609750",
    "end": "1619000"
  },
  {
    "text": "In other words, if we use R of Theta as the 1-norm of Theta and",
    "start": "1619010",
    "end": "1625410"
  },
  {
    "text": "we minimize empirical risk plus Lambda times the 1-norm of Theta,",
    "start": "1625410",
    "end": "1631215"
  },
  {
    "text": "as a consequence of that structure, that structure of that optimization problem,",
    "start": "1631215",
    "end": "1636765"
  },
  {
    "text": "we will tend to produce Thetas that are sparse.",
    "start": "1636765",
    "end": "1641680"
  },
  {
    "text": "So one explanation for this, when we're using a square penalty, well,",
    "start": "1641930",
    "end": "1649260"
  },
  {
    "text": "once you made Theta small, Theta squared is really very small.",
    "start": "1649260",
    "end": "1654510"
  },
  {
    "text": "And so the incentive for the sum of squares regularizer to make a coefficient smaller decreases once Theta has already become small.",
    "start": "1654510",
    "end": "1664290"
  },
  {
    "text": "For the absolute penalty, that doesn't happen. Once, u,, Theta becomes small, well,",
    "start": "1664290",
    "end": "1672020"
  },
  {
    "text": "it- it- Theta squared may become very small, but the absolute value of Theta_doesn't necessarily become very small,",
    "start": "1672020",
    "end": "1677810"
  },
  {
    "text": "it just has the same scale as Theta. And so the incentive to make Theta small continues until the Theta is actually 0.",
    "start": "1677810",
    "end": "1686460"
  },
  {
    "text": "Of course, that's a very rough explanation. Uh, one can do a more sophisticated mathematical analysis to show that in fact,",
    "start": "1686460",
    "end": "1695730"
  },
  {
    "text": "the 1-norm of Theta produces sparse Thetas when one uses the 1-norm as a regularization.",
    "start": "1695730",
    "end": "1704820"
  },
  {
    "text": "Uh, we're not gonna do that in this class, but we will see it in practice.",
    "start": "1704820",
    "end": "1711040"
  },
  {
    "text": "So here's an example. On the left-hand side, we have ridge regression,",
    "start": "1713570",
    "end": "1719115"
  },
  {
    "text": "and on the right-hand side, we have lasso regression. And so the left-hand two plots are the usual regularization path plots that we make.",
    "start": "1719115",
    "end": "1729420"
  },
  {
    "text": "Here, we have the test loss,",
    "start": "1729420",
    "end": "1735075"
  },
  {
    "text": "which increases as a function of Lambda, and we have the training loss.",
    "start": "1735075",
    "end": "1742380"
  },
  {
    "text": "And in this case, we see that the training loss also increases with Lambda. We might sometimes get a small dip in the test loss.",
    "start": "1742380",
    "end": "1752715"
  },
  {
    "text": "We also see in the bottom plot the usual shrinkage, we see the magnitude of Theta",
    "start": "1752715",
    "end": "1761459"
  },
  {
    "text": "in the size or the various components of Theta with a particular value of Lambda.",
    "start": "1761459",
    "end": "1766470"
  },
  {
    "text": "And we see that those numbers get smaller as we increase Theta- as we increase Lambda.",
    "start": "1766470",
    "end": "1774880"
  },
  {
    "text": "Now, on the right-hand side, we have the same plots, but for lasso when we're using l_1 regularization.",
    "start": "1775490",
    "end": "1786780"
  },
  {
    "text": "And here, we can see very similar training loss.",
    "start": "1786780",
    "end": "1796995"
  },
  {
    "text": "One noticeable feature is that there's a sharp corner there, and that's a characteristic feature of, um, lasso regression.",
    "start": "1796995",
    "end": "1806804"
  },
  {
    "text": "We also see that the test loss has",
    "start": "1806805",
    "end": "1812070"
  },
  {
    "text": "a much more marked dip with a significantly smaller test loss than in the l_2 regularized case.",
    "start": "1812070",
    "end": "1823919"
  },
  {
    "text": "Um, now, why is this? And the reason is, is that the lasso regression is eliminating the irrelevant features.",
    "start": "1823920",
    "end": "1837615"
  },
  {
    "text": "Let's look at the regularization path for Theta. What you can see is that as we increase Lambda,",
    "start": "1837615",
    "end": "1849375"
  },
  {
    "text": "the components of Theta tend to 0. And they tend to 0,",
    "start": "1849375",
    "end": "1856330"
  },
  {
    "text": "and hit 0 exactly, and then stay there.",
    "start": "1856330",
    "end": "1861210"
  },
  {
    "text": "And they hit 0 exactly at a particular value of Lambda.",
    "start": "1863090",
    "end": "1868559"
  },
  {
    "text": "And each one of them will hit, um, uh, Lambda one after the other, and, uh,",
    "start": "1868560",
    "end": "1876770"
  },
  {
    "text": "eventually would end up with a completely zero- exactly zero predictor, which will be, in this case,",
    "start": "1876770",
    "end": "1883145"
  },
  {
    "text": "for Lambda here which is a little bit larger than 1.",
    "start": "1883145",
    "end": "1892980"
  },
  {
    "text": "For Lambda a little bit smaller than 1, we have a predictor that consists of two non-zero components of Theta.",
    "start": "1892980",
    "end": "1904169"
  },
  {
    "text": "And back here, we have three, and once we're back to here, we have maybe 20 non-zero components of Theta.",
    "start": "1904170",
    "end": "1914580"
  },
  {
    "text": "And this sudden hitting of zero exactly is what",
    "start": "1914580",
    "end": "1919649"
  },
  {
    "text": "causes the sharp corner in the loss path as well.",
    "start": "1919650",
    "end": "1926560"
  },
  {
    "text": "And so the lasso regression is selecting features for us.",
    "start": "1927650",
    "end": "1935265"
  },
  {
    "text": "It's selecting those features which are relevant to the training problem.",
    "start": "1935265",
    "end": "1942960"
  },
  {
    "text": "And it's removing those features which are irrelevant, and it's removing them exactly.",
    "start": "1942960",
    "end": "1949065"
  },
  {
    "text": "Now, because they are, the features that I removed are irrelevant,",
    "start": "1949065",
    "end": "1954164"
  },
  {
    "text": "they are simply noise, um, fitting them to the training data may",
    "start": "1954165",
    "end": "1960630"
  },
  {
    "text": "improve training loss but it will have the downside, it will inherently make the test loss worse.",
    "start": "1960630",
    "end": "1969450"
  },
  {
    "text": "And that's why we see such a distinction between the lasso test loss and the ridge test loss.",
    "start": "1969450",
    "end": "1979450"
  },
  {
    "text": "Here are the results. So on the top, we have ridge regression with the square regularizer,",
    "start": "1983180",
    "end": "1991625"
  },
  {
    "text": "and on the bottom plot, we have lasso regularization with the absolute regularizer. Then on the top, we can see that this is- so this is",
    "start": "1991625",
    "end": "2000190"
  },
  {
    "text": "a plot of the components of Theta, uh, sorted, and- so first of all,",
    "start": "2000190",
    "end": "2009889"
  },
  {
    "text": "we take the components of Theta, we take the absolute value, and then we plot the sorted absolute values.",
    "start": "2009890",
    "end": "2015350"
  },
  {
    "text": "And you can see that for the Tikhonov regularization, that all 200 of the components of Theta are non-zero.",
    "start": "2015350",
    "end": "2023705"
  },
  {
    "text": "And as we go down the list, they decay, but rather slowly.",
    "start": "2023705",
    "end": "2031370"
  },
  {
    "text": "Every one of those x's is being used in some way or another to predict y.",
    "start": "2031370",
    "end": "2040050"
  },
  {
    "text": "For lasso at this particular optimal Lambda,",
    "start": "2040240",
    "end": "2045290"
  },
  {
    "text": "only the first 35 components of Theta are non-zero,",
    "start": "2045290",
    "end": "2052804"
  },
  {
    "text": "the other 165 components are exactly zero. And so we have a predictor which doesn't use in any way",
    "start": "2052805",
    "end": "2062629"
  },
  {
    "text": "165 components of x which are the irrelevant components of x.",
    "start": "2062630",
    "end": "2069799"
  },
  {
    "text": "Now, after we've trained with lasso, we can pick a particular Lambda where we've only got",
    "start": "2069800",
    "end": "2078905"
  },
  {
    "text": "a few components left which are non-zero so we've done feature selection to select the features that are most important.",
    "start": "2078905",
    "end": "2088550"
  },
  {
    "text": "So here, we've looked at the Lambda where we've reduced the predictor to only using seven features.",
    "start": "2088550",
    "end": "2096845"
  },
  {
    "text": "And if we look back at our plot, that is somewhere here,",
    "start": "2096845",
    "end": "2103759"
  },
  {
    "text": "[NOISE] there's some cut off Lambda value",
    "start": "2103759",
    "end": "2110870"
  },
  {
    "text": "at which there's only seven non-zero components of Theta left.",
    "start": "2110870",
    "end": "2115590"
  },
  {
    "text": "And then what we can do is we can be trained, we can take all of our x's,",
    "start": "2117970",
    "end": "2123635"
  },
  {
    "text": "cut off all of the components of x for which the corresponding Theta that we have has a zero entry.",
    "start": "2123635",
    "end": "2134540"
  },
  {
    "text": "And as a result, our x's will reduce from being 200 dimensional to being only 7-dimensional, in this case.",
    "start": "2134540",
    "end": "2141724"
  },
  {
    "text": "And then with a 7-dimensional x, we can retrain using either Tikhonov regression or lasso regression. And what do we see?",
    "start": "2141725",
    "end": "2152105"
  },
  {
    "text": "We see- on the left here, we see ridge or Tikhonov, on the right here, we see lasso.",
    "start": "2152105",
    "end": "2157685"
  },
  {
    "text": "Um, we see on the left for the plots of Theta that we're seeing the usual shrinkage as we increase Lambda.",
    "start": "2157685",
    "end": "2167660"
  },
  {
    "text": "And on the right, we're seeing lasso style shrinkage where the components are going down to touch zero exactly.",
    "start": "2167660",
    "end": "2179720"
  },
  {
    "text": "However, the key thing about these two plots is that if we look at the test loss. In both plots, were getting test loss which is similar,",
    "start": "2179720",
    "end": "2187925"
  },
  {
    "text": "and that's because feature selection has happened. And what we have now is, um,",
    "start": "2187925",
    "end": "2196849"
  },
  {
    "text": "only those features, the components of x,",
    "start": "2196850",
    "end": "2202265"
  },
  {
    "text": "which are relevant to the problem we're trying to solve which are relevant to y.",
    "start": "2202265",
    "end": "2209279"
  },
  {
    "text": "And as a result, lasso has no components to remove which won't affect the test loss,",
    "start": "2209320",
    "end": "2220130"
  },
  {
    "text": "uh, in a significant way. And, uh, conversely, ridge regression has",
    "start": "2220130",
    "end": "2227990"
  },
  {
    "text": "no extra components to use which it can come to - which it would normally use to overfit.",
    "start": "2227990",
    "end": "2235320"
  },
  {
    "text": "Nounw, this sparsification property of, um,",
    "start": "2240730",
    "end": "2245900"
  },
  {
    "text": "the 1-norm, uh, can be strengthened.",
    "start": "2245900",
    "end": "2252275"
  },
  {
    "text": "So remember what these regularizers look like. If we plot them,",
    "start": "2252275",
    "end": "2257585"
  },
  {
    "text": "then- so this will be a component of Theta and this will be the absolute value.",
    "start": "2257585",
    "end": "2268970"
  },
  {
    "text": "Um, or we can look at the component of Theta and look at the square.",
    "start": "2270090",
    "end": "2277440"
  },
  {
    "text": "Now, if we want to make this a sharper corner here, then we would have a stronger sparsifier.",
    "start": "2278170",
    "end": "2285755"
  },
  {
    "text": "One way we might make it a sharper corner is to replace the penalty function with a penalty function that does this.",
    "start": "2285755",
    "end": "2294480"
  },
  {
    "text": "And as a result, we've got an even greater incentive on",
    "start": "2294820",
    "end": "2300200"
  },
  {
    "text": "regularized empirical risk minimization to make the components of Theta exactly 0.",
    "start": "2300200",
    "end": "2307349"
  },
  {
    "text": "Some people would call this the l one-half regularizer. That is a little bit of a misnomer because the- the- the no-",
    "start": "2309160",
    "end": "2318440"
  },
  {
    "text": "the nomenclature l_p refers to the P norm.",
    "start": "2318440",
    "end": "2323780"
  },
  {
    "text": "And so, um, if I use p as a half, then that- that says I'm using the one-half norm.",
    "start": "2323780",
    "end": "2332494"
  },
  {
    "text": "And it turns out that that quantity, which should be defined as the sum of",
    "start": "2332495",
    "end": "2339770"
  },
  {
    "text": "i of Theta i absolute value to the one-half,",
    "start": "2339770",
    "end": "2345740"
  },
  {
    "text": "all squared, is not actually a norm, it doesn't satisfy the triangular inequality.",
    "start": "2345740",
    "end": "2352615"
  },
  {
    "text": "And so, um, we tend to avoid calling this the l one-half regularizer.",
    "start": "2352615",
    "end": "2359230"
  },
  {
    "text": "Uh, this is the strongest sparsifier. It, uh, it's not convex, however.",
    "start": "2359230",
    "end": "2365270"
  },
  {
    "text": "And as a result, uh, algorithms to compute this may not work as well as the 1-norm or 2-norm regularizers.",
    "start": "2365270",
    "end": "2376860"
  },
  {
    "text": "Here's an example. On the left, we have the plots for ridge regression,",
    "start": "2377710",
    "end": "2387755"
  },
  {
    "text": "in the middle, we have plots for lasso or l_1 regression,",
    "start": "2387755",
    "end": "2393095"
  },
  {
    "text": "and on the right, we have an example computed with the square root regularizer.",
    "start": "2393095",
    "end": "2398135"
  },
  {
    "text": "And we can see that we're getting some very similar behavior for the square root regularizer as we do for",
    "start": "2398135",
    "end": "2404540"
  },
  {
    "text": "the absolute value regularizer where we're getting exact sparsification at some- as we increase Lambda,",
    "start": "2404540",
    "end": "2412609"
  },
  {
    "text": "some of the components of Theta go to 0 exactly.",
    "start": "2412610",
    "end": "2419510"
  },
  {
    "text": "[NOISE]",
    "start": "2419510",
    "end": "2427540"
  },
  {
    "text": "Now let's have one more- look at one more regularizer. This is the non-negative regularizer.",
    "start": "2427540",
    "end": "2433825"
  },
  {
    "text": "Um, so sometimes we know or require that theta_i should be greater than or equal to zero.",
    "start": "2433825",
    "end": "2443185"
  },
  {
    "text": "So when x_i increases, so must our prediction.",
    "start": "2443185",
    "end": "2449630"
  },
  {
    "text": "Um- um, that means that we'd like to impose the constraint,",
    "start": "2450210",
    "end": "2455260"
  },
  {
    "text": "that theta_i is greater than or equal to 0, on the empirical risk minimization.",
    "start": "2455260",
    "end": "2460480"
  },
  {
    "text": "One way to do this is to have a regularizer which charges a cost to the- in the objective function,",
    "start": "2460480",
    "end": "2469390"
  },
  {
    "text": "which is infinite, when theta_i is negative, and is 0 when theta_i is non-negative.",
    "start": "2469390",
    "end": "2479030"
  },
  {
    "text": "So you might have this, for example, if your target variable is lifespan,",
    "start": "2479280",
    "end": "2485290"
  },
  {
    "text": "and x measures healthy behavior. Um, people would call this non-negative least-squares,",
    "start": "2485290",
    "end": "2494890"
  },
  {
    "text": "if you're doing quadratic loss. Um, so one- one way of solving these kinds of problems, uh, is, um,",
    "start": "2494890",
    "end": "2504220"
  },
  {
    "text": "to solve the least squares problem for our theta, and then say, \"Okay,",
    "start": "2504220",
    "end": "2509410"
  },
  {
    "text": "well I've got at least a theta which, uh, minimizes the square loss,",
    "start": "2509410",
    "end": "2515365"
  },
  {
    "text": "so it's empirical risk minimization. And I'm gonna take that theta, and any components of it which are negative I'm going to set to 0.\"",
    "start": "2515365",
    "end": "2523930"
  },
  {
    "text": "We will- we might write that as theta_ls plus.",
    "start": "2523930",
    "end": "2531710"
  },
  {
    "text": "It turns out that doesn't work very well. Because the minimization has been done without any knowledge",
    "start": "2532590",
    "end": "2540295"
  },
  {
    "text": "of the constraint that the components of theta have to be non-negative, and the negative components that we've set to 0 might actually be very important.",
    "start": "2540295",
    "end": "2551289"
  },
  {
    "text": "Um, it's much better to actually impose it as a regularization term on the obje- uh,",
    "start": "2551290",
    "end": "2559390"
  },
  {
    "text": "on the minimization problem and solve the minimization knowing that we would like theta to have non-negative components.",
    "start": "2559390",
    "end": "2568610"
  },
  {
    "text": "Here's a specific example. Here we have, uh,",
    "start": "2569520",
    "end": "2575035"
  },
  {
    "text": "a one-dimensional u and a one-dimensional v. We've set y equal to v,",
    "start": "2575035",
    "end": "2583030"
  },
  {
    "text": "and we've constructed features from u. The features are one, the constant feature,",
    "start": "2583030",
    "end": "2589119"
  },
  {
    "text": "u, and then u minus 0.2 plus.",
    "start": "2589120",
    "end": "2594450"
  },
  {
    "text": "Remember what that is, u plus, u minus 0.2 plus.",
    "start": "2594450",
    "end": "2600550"
  },
  {
    "text": "If I plot that,",
    "start": "2600740",
    "end": "2603790"
  },
  {
    "text": "this will be u, and this is going to be u minus a plus.",
    "start": "2606030",
    "end": "2613210"
  },
  {
    "text": "This is the point a, and this is the function.",
    "start": "2613210",
    "end": "2618410"
  },
  {
    "text": "And so it eng- it- it- the resulting predictor is going to be piecewise linear,",
    "start": "2622440",
    "end": "2631060"
  },
  {
    "text": "and it's going to have kinks in it at these points,",
    "start": "2631060",
    "end": "2638155"
  },
  {
    "text": "0.2, we've got 0.4, 0.6, 0.8, in this list here.",
    "start": "2638155",
    "end": "2644785"
  },
  {
    "text": "And so we're gonna have a piecewise linear predictor. Now if we want the- the- if we're going to impose",
    "start": "2644785",
    "end": "2653590"
  },
  {
    "text": "the constraint that theta_i be non-negative,",
    "start": "2653590",
    "end": "2658735"
  },
  {
    "text": "well, what does the predictor look like? The predictor looks like y hat is theta_1 plus theta_2 u,",
    "start": "2658735",
    "end": "2670915"
  },
  {
    "text": "plus theta_3, u minus 0.2 plus,",
    "start": "2670915",
    "end": "2676390"
  },
  {
    "text": "plus theta_4, u minus 0.4 plus, and so on.",
    "start": "2676390",
    "end": "2683450"
  },
  {
    "text": "So if the thetas are non-negative, what it means is that theta_1 is non-negative,",
    "start": "2684210",
    "end": "2689875"
  },
  {
    "text": "so at zero the function y hat has to be positive.",
    "start": "2689875",
    "end": "2695725"
  },
  {
    "text": "It means that theta_2 is non-negative, which means that when u is less than 0.2,",
    "start": "2695725",
    "end": "2705055"
  },
  {
    "text": "the function is just theta_1 plus theta_2 u, and so that has to have,",
    "start": "2705055",
    "end": "2710560"
  },
  {
    "text": "uh, a non-negative slope. So if we look at this predictor between the values of u is 0.2 and the val- and u is 0.4,",
    "start": "2710560",
    "end": "2721710"
  },
  {
    "text": "and we find that the slope in that region is theta_2 plus theta_3.",
    "start": "2721710",
    "end": "2729050"
  },
  {
    "text": "Because both theta_2 and theta_3 are non-negative, that slope is greater than or equal to the slope- at- between 0 and 0.2,",
    "start": "2729050",
    "end": "2738085"
  },
  {
    "text": "which is just theta_2. So all of the thetas being non-negative means that the function must be non-decreasing,",
    "start": "2738085",
    "end": "2748705"
  },
  {
    "text": "and it must be convex. As we increase u, the slope has to increase.",
    "start": "2748705",
    "end": "2754209"
  },
  {
    "text": "Um, so here's a dataset, and here's the predictor.",
    "start": "2754209",
    "end": "2760550"
  },
  {
    "text": "Here's the predictor. It's- this is the optimal non-negative least squares",
    "start": "2760980",
    "end": "2771789"
  },
  {
    "text": "fit where we've used a non-negative regularizer, and, uh,",
    "start": "2771790",
    "end": "2779170"
  },
  {
    "text": "um, and we can see that the function indeed is convex and non-decreasing.",
    "start": "2779170",
    "end": "2786744"
  },
  {
    "text": "This is the optimal least squares fit. So any piecewise linear has to be- means convex,",
    "start": "2786745",
    "end": "2796420"
  },
  {
    "text": "but it's not non-decreasing. Now if we use our heuristic and we say, \"Well, look,",
    "start": "2796420",
    "end": "2802840"
  },
  {
    "text": "what we're going to do is we're going to take that theta and",
    "start": "2802840",
    "end": "2807880"
  },
  {
    "text": "simply adjust it so that all of the components of theta are non- are non-negative.",
    "start": "2807880",
    "end": "2813819"
  },
  {
    "text": "In particular, here we see that theta_2 is less than 0,",
    "start": "2813820",
    "end": "2820540"
  },
  {
    "text": "and so we change this function to make- by making theta_2 equal to 0. Then we end up with a function that looks like this,",
    "start": "2820540",
    "end": "2827690"
  },
  {
    "text": "and that's, uh- it's an extremely poor fit to the data.",
    "start": "2830100",
    "end": "2838150"
  },
  {
    "text": "We can see here that the non-negative least squares loss is 0.59. The least squares loss of course does better because it doesn't have",
    "start": "2838150",
    "end": "2846940"
  },
  {
    "text": "the constraint that the coefficients be non-negative,",
    "start": "2846940",
    "end": "2852220"
  },
  {
    "text": "and so it does better at 0.3. But the heuristic loss is 15 because the da- the,",
    "start": "2852220",
    "end": "2858130"
  },
  {
    "text": "uh- this, uh, approach, uh, this heuristic, is not guaranteed to work.",
    "start": "2858130",
    "end": "2865120"
  },
  {
    "text": "So the message here is that simply taking the least squares predictor and truncating it can perform very badly indeed.",
    "start": "2865120",
    "end": "2874855"
  },
  {
    "text": "Much better to use a non-negative regularizer. Let's summarize.",
    "start": "2874855",
    "end": "2885790"
  },
  {
    "text": "You want to choose a regularizer, ultimately, the thing you have to do is use validation.",
    "start": "2885790",
    "end": "2892855"
  },
  {
    "text": "If you've got a choice between two different regularizers that you have in mind, say l_1 and l_2,",
    "start": "2892855",
    "end": "2899065"
  },
  {
    "text": "try them both and see one- see which one gives you the best test loss. Um, and the way we do that, well,",
    "start": "2899065",
    "end": "2909310"
  },
  {
    "text": "we know- is that- we choose a range of lambdas, we do RERM,",
    "start": "2909310",
    "end": "2915175"
  },
  {
    "text": "regularized empirical risk minimization, to each one, to compute a predictor, and then we take those predictors,",
    "start": "2915175",
    "end": "2921190"
  },
  {
    "text": "we evaluate them on the test set, and then we choose the lambda that gives the best test error.",
    "start": "2921190",
    "end": "2927680"
  },
  {
    "text": "That tells you the vari- the variation of the test performance as a function of lambda.",
    "start": "2928200",
    "end": "2935575"
  },
  {
    "text": "Now we're gonna do that with each of our different regularizers, and we'll use the regularizer that gives the best test error.",
    "start": "2935575",
    "end": "2942650"
  },
  {
    "text": "Uh, beyond that, we've talked a little bit about",
    "start": "2943770",
    "end": "2951480"
  },
  {
    "text": "some situations in which one might expect lasso to perform better than ridge regression,",
    "start": "2951480",
    "end": "2960840"
  },
  {
    "text": "and in particular, the situation that's most common is when there are irrelevant features in your data.",
    "start": "2960840",
    "end": "2970480"
  },
  {
    "text": "We've seen also that the importance of getting rid of those features can be very significant.",
    "start": "2972210",
    "end": "2979285"
  },
  {
    "text": "If it costs you a lot to collect data, then getting rid of some unwanted data can be a really useful thing to",
    "start": "2979285",
    "end": "2985650"
  },
  {
    "text": "do to save you having to collect the data for future tests.",
    "start": "2985650",
    "end": "2990969"
  },
  {
    "text": "Um, ultimately, though, it's important not to believe that one can a priori",
    "start": "2991320",
    "end": "2998860"
  },
  {
    "text": "determine which regularizer to use. You have to determine it based on validation.",
    "start": "2998860",
    "end": "3005590"
  }
]