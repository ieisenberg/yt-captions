[
  {
    "text": "so when when sean invited me today i did think about a few different topics that i could come and present um and i could",
    "start": "11280",
    "end": "18800"
  },
  {
    "text": "have presented also on readability which is a recent focus of mine um and i will mention it at the end of the",
    "start": "18800",
    "end": "24960"
  },
  {
    "text": "presentation because it's a topic in which i'm actively recruiting uh collaborators and interns for but i",
    "start": "24960",
    "end": "31920"
  },
  {
    "text": "decided to take the opportunity since this is a class to give a kind of",
    "start": "31920",
    "end": "37040"
  },
  {
    "text": "summary of the body of work focused around attention um or another way to put this is that i'm going to focus on",
    "start": "37040",
    "end": "44079"
  },
  {
    "text": "how human perception can be leveraged for computational applications um and i'll show you how that applies to image",
    "start": "44079",
    "end": "50800"
  },
  {
    "text": "processing graphic design processing and some visualization work as well and",
    "start": "50800",
    "end": "57120"
  },
  {
    "text": "um just as manish said i'm excited about it particularly because it's a very",
    "start": "57120",
    "end": "62399"
  },
  {
    "text": "interdisciplinary research area and i think that there's a lot of exciting prospects and a lot of unanswered",
    "start": "62399",
    "end": "68880"
  },
  {
    "text": "questions and a lot of possibility to jump in and do cool stuff in this space and and so i hope that's going to be my",
    "start": "68880",
    "end": "75040"
  },
  {
    "text": "call to action from this talk and um students i'll get students riled up to",
    "start": "75040",
    "end": "80479"
  },
  {
    "text": "work in this space um all right so why why is this so exciting uh well",
    "start": "80479",
    "end": "87439"
  },
  {
    "text": "if you know where people look you have these enticing sci-fi like applications",
    "start": "87439",
    "end": "93439"
  },
  {
    "text": "um you might be able to take guesses about what they're thinking or whether they're struggling or what they're going",
    "start": "93439",
    "end": "98880"
  },
  {
    "text": "to do next and of course marketers and commercial agencies are all about that they want to understand",
    "start": "98880",
    "end": "105600"
  },
  {
    "text": "um where someone looks so that they might predict what that person might be interested in um so this is an example i",
    "start": "105600",
    "end": "112000"
  },
  {
    "text": "pulled from an paper from 2018 um that's claimed that claims to predict",
    "start": "112000",
    "end": "118159"
  },
  {
    "text": "um whether viewers are looking for information or shopping around so in this case with this eye",
    "start": "118159",
    "end": "124799"
  },
  {
    "text": "movement trajectory the model would predict that they're just browsing for information but this",
    "start": "124799",
    "end": "130560"
  },
  {
    "text": "time movement trajectory would be more indicative of someone who's trying to shop around on the website and the",
    "start": "130560",
    "end": "136640"
  },
  {
    "text": "examples are my own but um the flavor is is from that paper",
    "start": "136640",
    "end": "141840"
  },
  {
    "text": "and another example is that um this this extends not just to the screen environment but to the physical and",
    "start": "141840",
    "end": "148400"
  },
  {
    "text": "virtual store environments where saliency is actually used to figure out how to",
    "start": "148400",
    "end": "153840"
  },
  {
    "text": "arrange products to make them more attention capturing on shelves and another customer of this kind of",
    "start": "153840",
    "end": "160879"
  },
  {
    "text": "technology are vr ar and gaming companies who want to prioritize um",
    "start": "160879",
    "end": "166879"
  },
  {
    "text": "which pixels they render on the screen at a time this is from my colleague",
    "start": "166879",
    "end": "172720"
  },
  {
    "text": "chief thumb and the idea is that you don't want to render all of the pixels in real time",
    "start": "172720",
    "end": "179120"
  },
  {
    "text": "for every frame that would be computationally uh too difficult but if you know where someone's looking or whether it's gonna",
    "start": "179120",
    "end": "185599"
  },
  {
    "text": "where they're gonna look next you can prioritize that content and the notion of prioritization using visual attention",
    "start": "185599",
    "end": "192319"
  },
  {
    "text": "is a theme you'll see again and again throughout this talk and if we venture further into mind",
    "start": "192319",
    "end": "198239"
  },
  {
    "text": "reading there's a whole line of work that looks at the relationship between eye movements and memory and one of my",
    "start": "198239",
    "end": "204959"
  },
  {
    "text": "earlier papers from my phd looked at",
    "start": "204959",
    "end": "210080"
  },
  {
    "text": "using the trajectory of eye movements on an image to predict whether someone would remember an image at a later point",
    "start": "210080",
    "end": "216000"
  },
  {
    "text": "in time so here's an eye movement trajectory of someone who remembered this image",
    "start": "216000",
    "end": "221120"
  },
  {
    "text": "uh later on and here's an eye movement trajectory of someone who forgot this image um",
    "start": "221120",
    "end": "227200"
  },
  {
    "text": "even after looking at it for a few seconds and just as an aside the key difference between these two",
    "start": "227200",
    "end": "232879"
  },
  {
    "text": "trajectories is that in the first case um the participant paid attention to the road signs which is the distinguishing",
    "start": "232879",
    "end": "239760"
  },
  {
    "text": "characteristic making this image different from other road images whereas if you just looked at the cars in the",
    "start": "239760",
    "end": "245120"
  },
  {
    "text": "road it would mesh in your brain with all the other road images you've seen um but with enough data a machine learning",
    "start": "245120",
    "end": "251040"
  },
  {
    "text": "model could learn some of these relationships and in a more applied domain uh eye movements can also be",
    "start": "251040",
    "end": "257359"
  },
  {
    "text": "related to how people explore and understand information visualizations and what they recall from them uh at a",
    "start": "257359",
    "end": "264240"
  },
  {
    "text": "later point in time so there's also a body of work um in the visualization community that's",
    "start": "264240",
    "end": "270160"
  },
  {
    "text": "very interested in looking at eye movements and how people explore uh visual data",
    "start": "270160",
    "end": "276320"
  },
  {
    "text": "but all of these applications that i've mentioned despite being quite different they have this uh one important thing in",
    "start": "276320",
    "end": "282479"
  },
  {
    "text": "common which is that their focus on the individual observer and what they're thinking or doing and these applications",
    "start": "282479",
    "end": "290000"
  },
  {
    "text": "are exciting because they give us the possibility to customize to the individual of observer",
    "start": "290000",
    "end": "296400"
  },
  {
    "text": "and to understand their theory of mind but there's another set of application that becomes possible when we aggregate",
    "start": "296400",
    "end": "303360"
  },
  {
    "text": "the attention patterns of many observers and in this case if we consider the population perspective",
    "start": "303360",
    "end": "309919"
  },
  {
    "text": "then we can infer general attention trends and use them to differentiate say",
    "start": "309919",
    "end": "314960"
  },
  {
    "text": "effective from ineffective designs or to suggest design guidelines or to help",
    "start": "314960",
    "end": "320160"
  },
  {
    "text": "prioritize visual content because where a population of participants looks tells us something about what's most",
    "start": "320160",
    "end": "326720"
  },
  {
    "text": "interesting or relevant in that image or visual and the rest of my talk will be more",
    "start": "326720",
    "end": "332240"
  },
  {
    "text": "focused on this population perspective though we're going to come back to the individual um in the very last few",
    "start": "332240",
    "end": "338479"
  },
  {
    "text": "slides on uh readability first i do want to give you just a little bit of background and terminology",
    "start": "338479",
    "end": "344960"
  },
  {
    "text": "so we're on the same page the traditional way that attention is usually collected is using",
    "start": "344960",
    "end": "351360"
  },
  {
    "text": "dedicated hardware in this case i'm showing you someone in front of an infrared eye",
    "start": "351360",
    "end": "357039"
  },
  {
    "text": "tracker which captures reflections from the eyeballs and interprets them as coordinates on the screen for where",
    "start": "357039",
    "end": "363199"
  },
  {
    "text": "someone is looking and the software that often accompanies",
    "start": "363199",
    "end": "368400"
  },
  {
    "text": "the eye tracking hardware uh parses that those continuous eye movements into discrete fixation points using a set of",
    "start": "368400",
    "end": "375440"
  },
  {
    "text": "preset thresholds and that's preset in the software and that's sometimes that's proprietary",
    "start": "375440",
    "end": "381440"
  },
  {
    "text": "the scientist has the fixation points in their hand and what they can do with them is frequently they post process them into",
    "start": "381440",
    "end": "388800"
  },
  {
    "text": "fixation maps they can analyze the fixation points on their own but it's often helpful to parse them into",
    "start": "388800",
    "end": "395759"
  },
  {
    "text": "these fixation maps you can aggregate the fixation points of many observers in order to do that one way you can do that",
    "start": "395759",
    "end": "402000"
  },
  {
    "text": "is just gaussian blurring them and that way you're representing them as a distribution that you can either sample",
    "start": "402000",
    "end": "408800"
  },
  {
    "text": "from or interpret as the high density regions being the ones that",
    "start": "408800",
    "end": "414000"
  },
  {
    "text": "most observers look at or look at for the longest periods of time so those are the regions that are most interesting",
    "start": "414000",
    "end": "421039"
  },
  {
    "text": "and um just a disclaimer throughout the rest of my presentation you're gonna see heat maps rendered in",
    "start": "421039",
    "end": "427840"
  },
  {
    "text": "every possible way i think i've probably tried every possible color map and and",
    "start": "427840",
    "end": "433120"
  },
  {
    "text": "that's just by nature of different uh trying out different visualization schemes for different publications but",
    "start": "433120",
    "end": "438800"
  },
  {
    "text": "the key idea is that anytime you see a heat map it's really representing the same thing",
    "start": "438800",
    "end": "444080"
  },
  {
    "text": "and the way to think about it is the high density regions are parts of the visual world that we're trying to",
    "start": "444080",
    "end": "449199"
  },
  {
    "text": "prioritize and that they're intended to be a proxy of what people would find most attention capturing uh or",
    "start": "449199",
    "end": "455680"
  },
  {
    "text": "interesting all right so um saliency models are",
    "start": "455680",
    "end": "461199"
  },
  {
    "text": "computational models that aim to predict where people look and so what you can see here is just a few images from a",
    "start": "461199",
    "end": "467440"
  },
  {
    "text": "saliency benchmark that i used to run uh back at mit and the ground smooth fixation or you can call them the ground",
    "start": "467440",
    "end": "474240"
  },
  {
    "text": "truth saliency maps of human observers collected with an eye tracker and then what you see along the other columns are",
    "start": "474240",
    "end": "480720"
  },
  {
    "text": "different computational models over time and the way to read this is actually from right to left because the earlier",
    "start": "480720",
    "end": "486400"
  },
  {
    "text": "models are on the right side of the screen uh and i'm just showing you like a small",
    "start": "486400",
    "end": "491840"
  },
  {
    "text": "slither of the models available because today there must be a couple hundred or even a few more hundred models saliency",
    "start": "491840",
    "end": "499280"
  },
  {
    "text": "models out there um either for the same task although some are specialized for different types of",
    "start": "499280",
    "end": "504560"
  },
  {
    "text": "images and one thing to observe is that over time they've converged more and more to look like human brown troops so",
    "start": "504560",
    "end": "511360"
  },
  {
    "text": "today we have really good models to predict where people look in natural images and that's why we're seeing more",
    "start": "511360",
    "end": "517919"
  },
  {
    "text": "and more the use of saliency models in in the industry and i'll show you some of those applications as well the",
    "start": "517919",
    "end": "524240"
  },
  {
    "text": "earlier models were tended to be more handcrafted features that would be combined together",
    "start": "524240",
    "end": "530800"
  },
  {
    "text": "and then the later models as you might or uh might expect already their neural",
    "start": "530800",
    "end": "535920"
  },
  {
    "text": "network models um of of human intention and the way these models are trained is uh by having",
    "start": "535920",
    "end": "543200"
  },
  {
    "text": "paired images most frequently um where you have an image and then a fixation",
    "start": "543200",
    "end": "548480"
  },
  {
    "text": "map that corresponds to it either collected using eye tracking or other methods as you will see soon um and then",
    "start": "548480",
    "end": "555600"
  },
  {
    "text": "a network is then trained on on those sets of images to learn the correlations between visual features and what's",
    "start": "555600",
    "end": "561760"
  },
  {
    "text": "interesting in an image or not and then given an image of test time these models can run in near real time",
    "start": "561760",
    "end": "569279"
  },
  {
    "text": "and output saliency maps that can be used for downstream image processing tasks",
    "start": "569279",
    "end": "576000"
  },
  {
    "text": "so because these neural network models are really data hungry um starting using",
    "start": "576000",
    "end": "582480"
  },
  {
    "text": "the traditional hardware based eye tracking methods is infeasible at that",
    "start": "582480",
    "end": "587600"
  },
  {
    "text": "scale and so this has led to proliferation of methods to crowdsource visual attention remotely um without an",
    "start": "587600",
    "end": "594320"
  },
  {
    "text": "eye tracker and i like to um put them into the two categories webcam webcam",
    "start": "594320",
    "end": "600800"
  },
  {
    "text": "based methods um which are based uh on the premise of capturing the face detecting it um finding the eyes and",
    "start": "600800",
    "end": "608000"
  },
  {
    "text": "then mapping um the eyes to coordinates on the screen and the earlier methods",
    "start": "608000",
    "end": "613920"
  },
  {
    "text": "such as the ones on the screen tended to be pretty coarse in their predictions but as data sets have increased and as",
    "start": "613920",
    "end": "622000"
  },
  {
    "text": "neural network models have improved these have also improved and are are used more and more today uh and this",
    "start": "622000",
    "end": "628399"
  },
  {
    "text": "could use a webcam but it can also use a cell phone camera and today's cameras that have depth sensors are doing a much",
    "start": "628399",
    "end": "634880"
  },
  {
    "text": "better job of this so they have more information um and then the other category of methods are the ones that",
    "start": "634880",
    "end": "640959"
  },
  {
    "text": "i'm going to focus the rest of my talk on today because that's my area of expertise which are i call them to your",
    "start": "640959",
    "end": "646720"
  },
  {
    "text": "clickbase methods but maybe the right name is more interaction based methods because it's not just clicks it's",
    "start": "646720",
    "end": "652959"
  },
  {
    "text": "various interactions that you might have with your mouse cursor keyboard or or even with your hands as you'll see um",
    "start": "652959",
    "end": "660480"
  },
  {
    "text": "and the difference of these methods from the webcam-based ones is that they don't capture the eyes at all instead they are",
    "start": "660480",
    "end": "666880"
  },
  {
    "text": "proxies of what someone might look at or choose to attend to or find interesting",
    "start": "666880",
    "end": "672079"
  },
  {
    "text": "without capturing uh the eyes and all these methods what they have in common",
    "start": "672079",
    "end": "677120"
  },
  {
    "text": "is that they are cheap and easy to collect at scale and this has led to the",
    "start": "677120",
    "end": "682320"
  },
  {
    "text": "development of a toolbox of related interfaces um for crowdsourcing attention that i worked on together with",
    "start": "682320",
    "end": "688800"
  },
  {
    "text": "a great group of mit students um the lead on this project was annelise newman who is now a stanford phd",
    "start": "688800",
    "end": "695839"
  },
  {
    "text": "and our goal was to bring together the different methodologies that could be used",
    "start": "695839",
    "end": "700880"
  },
  {
    "text": "to crowdsource attention uh without crowdsourcing uh eye movements uh",
    "start": "700880",
    "end": "705920"
  },
  {
    "text": "directly and uh i will go through these different methodologies and and what they uh allow us to do um but the code",
    "start": "705920",
    "end": "713519"
  },
  {
    "text": "for all of them the demos we um we make available to the research community for further development so feel free to use",
    "start": "713519",
    "end": "720639"
  },
  {
    "text": "it we'll be happy if you find new use cases for them uh in a kai 2020 paper we",
    "start": "720639",
    "end": "726800"
  },
  {
    "text": "showed that at a course level all these methodologies uh capture similar patterns as eye movements which was",
    "start": "726800",
    "end": "732800"
  },
  {
    "text": "really exciting to find and this happens when you average again when you take the population",
    "start": "732800",
    "end": "738000"
  },
  {
    "text": "perspective and when you average the attention pattern over a population of participants",
    "start": "738000",
    "end": "744079"
  },
  {
    "text": "there are subtle differences between them um as as you'll see and those are interesting as well because those point",
    "start": "744079",
    "end": "750639"
  },
  {
    "text": "to unique use cases or applications or image types for which some interfaces",
    "start": "750639",
    "end": "755760"
  },
  {
    "text": "might be preferable over others and we'll see that um in the next few",
    "start": "755760",
    "end": "760800"
  },
  {
    "text": "slides as well so one of the first interfaces that we use at scale to",
    "start": "760800",
    "end": "766880"
  },
  {
    "text": "collect data is bubble view and that's based on an earlier idea on using the",
    "start": "766880",
    "end": "772800"
  },
  {
    "text": "restricted focus viewer that's um an older idea that already was intended to",
    "start": "772800",
    "end": "778480"
  },
  {
    "text": "mimic eye movement patterns but this was really the first time where we systematically tested it against eye",
    "start": "778480",
    "end": "784320"
  },
  {
    "text": "tracking and looked at um which which parameters best model uh eye movements",
    "start": "784320",
    "end": "790560"
  },
  {
    "text": "and then use that to collect a large data set that we could train a model on so here as you can see",
    "start": "790560",
    "end": "797519"
  },
  {
    "text": "mouse moves around as someone explores an image and the blurriness is intended to model this",
    "start": "797519",
    "end": "804079"
  },
  {
    "text": "kind of peripheral viewing very coarsely and we give them a task in this case they're viewing the",
    "start": "804079",
    "end": "810000"
  },
  {
    "text": "visualization we give them the task to describe the visualization to keep them both to keep them on task but to add",
    "start": "810000",
    "end": "817200"
  },
  {
    "text": "interesting data that we can then link back to um the their patterns of attention where did they look and what",
    "start": "817200",
    "end": "823760"
  },
  {
    "text": "they described um nicely you average the these click patterns over a population of",
    "start": "823760",
    "end": "830720"
  },
  {
    "text": "participants 20 or 30 um you start to converge on fixation maps so the the",
    "start": "830720",
    "end": "837680"
  },
  {
    "text": "patterns that we purchased here very similar to eye movements that we would have collected in the lab using an eye",
    "start": "837680",
    "end": "844160"
  },
  {
    "text": "tracker in fact we can account for um around 90 of eye fixations using these",
    "start": "844160",
    "end": "851360"
  },
  {
    "text": "click maps and that's a good enough amount for a lot of the uh applications",
    "start": "851360",
    "end": "856959"
  },
  {
    "text": "that we might have in mind so this amount of data allows us of course",
    "start": "856959",
    "end": "862720"
  },
  {
    "text": "to train these data hungry neural network models of saliency that take as input in this",
    "start": "862720",
    "end": "869440"
  },
  {
    "text": "case they were trained on visualizations and graphic designs so they can take his input visualizations and graphic designs",
    "start": "869440",
    "end": "875279"
  },
  {
    "text": "and output heat maps of attention on them uh and uh in our west 2017 paper we just",
    "start": "875279",
    "end": "882959"
  },
  {
    "text": "gave some demo applications some toy applications where you could use these maps out of the box to already show some",
    "start": "882959",
    "end": "889839"
  },
  {
    "text": "interesting um thumbnailing or summarization uh results",
    "start": "889839",
    "end": "895199"
  },
  {
    "text": "so here we're just using the predicted attention map and then we're taking all",
    "start": "895199",
    "end": "900560"
  },
  {
    "text": "the most important content the most predicted important content seam carving around it to create",
    "start": "900560",
    "end": "906320"
  },
  {
    "text": "a thumbnail and you can see some of those other examples at the bottom here but one thing that stood out for us as",
    "start": "906320",
    "end": "913440"
  },
  {
    "text": "we played with this application is it was really hard to capture full design elements you see that some of the text",
    "start": "913440",
    "end": "919279"
  },
  {
    "text": "is getting cut off or some of the design elements because the data capture methodology was not uh built to",
    "start": "919279",
    "end": "927279"
  },
  {
    "text": "keep these design elements intact and together that's a segmentation problem",
    "start": "927279",
    "end": "932880"
  },
  {
    "text": "and that actually uh moved us to considering a different user",
    "start": "932880",
    "end": "938320"
  },
  {
    "text": "interface that would collect a proxy of attention together with segmentation all-in-one so let me show you what that",
    "start": "938320",
    "end": "945199"
  },
  {
    "text": "one looks like uh this is the important ops tool and this is based on earlier work by peter",
    "start": "945199",
    "end": "951440"
  },
  {
    "text": "o'donovan i believe from 2015 uh and as you'll see here um the task to",
    "start": "951440",
    "end": "957120"
  },
  {
    "text": "participants um to crowd workers is to annotate the most important regions on",
    "start": "957120",
    "end": "963120"
  },
  {
    "text": "these graphic designs and this seems like an instruction that's quite far from where people are looking and",
    "start": "963120",
    "end": "971040"
  },
  {
    "text": "might appear to be subjective but what's exciting about it is that if you average these",
    "start": "971040",
    "end": "977040"
  },
  {
    "text": "individual um important importance maps or 30 to 40 participants you again start to",
    "start": "977040",
    "end": "984079"
  },
  {
    "text": "converge on something that looks like an attention heat map and it has this nice added benefit of having the uh design",
    "start": "984079",
    "end": "991199"
  },
  {
    "text": "elements segmented because that's that was the annotation test and so they they come together and so if you want to use",
    "start": "991199",
    "end": "998320"
  },
  {
    "text": "this uh importance heat map to parse the graphic design you can use these annotations out of the box",
    "start": "998320",
    "end": "1005199"
  },
  {
    "text": "we use this tool for a later paper uh at west 2020 to collect the importance uh heat maps",
    "start": "1005199",
    "end": "1013199"
  },
  {
    "text": "for a thousand different designs so you can see some of the design examples on the left and the collected heat maps on",
    "start": "1013199",
    "end": "1019360"
  },
  {
    "text": "the right um and we did this for ads infographics mobile uis movie posters",
    "start": "1019360",
    "end": "1024720"
  },
  {
    "text": "and web pages and that data set is also available online",
    "start": "1024720",
    "end": "1030160"
  },
  {
    "text": "and we use this to train a model that would predict saliency and importance",
    "start": "1030160",
    "end": "1036240"
  },
  {
    "text": "you could call it either silence your importance uh for not just graphic designs but also",
    "start": "1036240",
    "end": "1042240"
  },
  {
    "text": "natural images so we train this model on natural images as well um so that there",
    "start": "1042240",
    "end": "1047360"
  },
  {
    "text": "would be a single one-stop model that would uh be able to perform this task on",
    "start": "1047360",
    "end": "1053360"
  },
  {
    "text": "all these different image types and the idea is that this kind of model that can transition seamlessly from predicting",
    "start": "1053360",
    "end": "1059919"
  },
  {
    "text": "saliency on natural images to predicting importance on graphic designs is better suited um to be used within real graphic",
    "start": "1059919",
    "end": "1068160"
  },
  {
    "text": "design applications in the loop with whatever the user is doing because a lot of um",
    "start": "1068160",
    "end": "1074400"
  },
  {
    "text": "posters or advertisements evolve out of natural images and other components that get brought together and so having a",
    "start": "1074400",
    "end": "1081280"
  },
  {
    "text": "model that can understand the natural image but also the final design is important",
    "start": "1081280",
    "end": "1086960"
  },
  {
    "text": "and i'm gonna just show you very briefly two prototype applications that we built on on top of this to showcase the real",
    "start": "1086960",
    "end": "1094240"
  },
  {
    "text": "time capabilities of this model um and what you're going to see is that",
    "start": "1094240",
    "end": "1099840"
  },
  {
    "text": "this is a design canvas and and elements can be dragged and manipulated on this canvas but in the right hand panel what",
    "start": "1099840",
    "end": "1107280"
  },
  {
    "text": "you should see is a real-time updated prediction of what's the most salient",
    "start": "1107280",
    "end": "1114240"
  },
  {
    "text": "element what are the most salient elements so you see our salience heat map projected there as well and what you see",
    "start": "1114240",
    "end": "1121600"
  },
  {
    "text": "at the bottom is a set of bar graphs that correspond to all the design elements currently on the canvas and the",
    "start": "1121600",
    "end": "1127200"
  },
  {
    "text": "user can interact with the importance values of each of those design elements",
    "start": "1127200",
    "end": "1132640"
  },
  {
    "text": "and ask the an optimization procedure behind the scenes to change the placement the sizes and other values of",
    "start": "1132640",
    "end": "1139520"
  },
  {
    "text": "those design elements such that some elements are more or less attention",
    "start": "1139520",
    "end": "1144640"
  },
  {
    "text": "capturing than others so this is kind of our first pass at integrating saliency with an",
    "start": "1144640",
    "end": "1150960"
  },
  {
    "text": "interactive design tool um here's another one where the idea is to retarget or find",
    "start": "1150960",
    "end": "1157760"
  },
  {
    "text": "alternate layouts for an existing design now this is a vector design and when you",
    "start": "1157760",
    "end": "1163039"
  },
  {
    "text": "see the alternate designs they're designed for a different aspect ratio and the designs that are generated",
    "start": "1163039",
    "end": "1169600"
  },
  {
    "text": "actually have saliency operating behind the scenes to pick the designs that most closely",
    "start": "1169600",
    "end": "1176080"
  },
  {
    "text": "match the original design in terms of its um attention heat maps so just to",
    "start": "1176080",
    "end": "1181679"
  },
  {
    "text": "kind of show you a side by side um whoops let me go to the next slide",
    "start": "1181679",
    "end": "1187520"
  },
  {
    "text": "yeah just to show you a side-by-side and explain how that works you see the original design",
    "start": "1187520",
    "end": "1194320"
  },
  {
    "text": "on the left sorry i have a bit of a lag you have the original design on the left",
    "start": "1194320",
    "end": "1200559"
  },
  {
    "text": "and then the new design on the right and the idea of our of our model was to pick this new",
    "start": "1200559",
    "end": "1207039"
  },
  {
    "text": "design such that the elements that were most attention capturing in the original design are still the most attention",
    "start": "1207039",
    "end": "1212960"
  },
  {
    "text": "capturing elements in the final redesign um because that's supposed to capture the intention of the designer and so",
    "start": "1212960",
    "end": "1220559"
  },
  {
    "text": "here's another application of how attention can be used in the loop within a design tool",
    "start": "1220559",
    "end": "1226080"
  },
  {
    "text": "all right but um as i presented to you right now both these tools bubble view",
    "start": "1226080",
    "end": "1231200"
  },
  {
    "text": "and important they work well for data collection but they depend on um a kind",
    "start": "1231200",
    "end": "1236559"
  },
  {
    "text": "of artificial image viewing setup where you're asking participants to",
    "start": "1236559",
    "end": "1241919"
  },
  {
    "text": "engage with the image or the visual in a way that they wouldn't naturally so that this would really be just for data",
    "start": "1241919",
    "end": "1247760"
  },
  {
    "text": "collection but then we sat and we thought about whether there might be a way to capture more natural in the wild",
    "start": "1247760",
    "end": "1254320"
  },
  {
    "text": "attention patterns from interactions and what we realized is that while bubble view is a restricted focus viewer right",
    "start": "1254320",
    "end": "1261280"
  },
  {
    "text": "so it's a restricted area of focus around where your mouse cursor is we all have a restricted",
    "start": "1261280",
    "end": "1267120"
  },
  {
    "text": "focus viewer in our pockets which is our cell phone through which we look at this broader world and so we could use it to",
    "start": "1267120",
    "end": "1274000"
  },
  {
    "text": "explore images and interpret the zoom patterns that's zoom maps interpret the zoom",
    "start": "1274000",
    "end": "1280960"
  },
  {
    "text": "patterns as regions of the image that have the highest importance to that",
    "start": "1280960",
    "end": "1286640"
  },
  {
    "text": "viewer so really what we do behind the scenes is we look at for each pixel the zoom level",
    "start": "1286640",
    "end": "1294080"
  },
  {
    "text": "that it lies in and for what amount of time and we interpret that as a value in",
    "start": "1294080",
    "end": "1299200"
  },
  {
    "text": "our density heat map later and so this is what it looks like um the zoom apps for three different viewers of",
    "start": "1299200",
    "end": "1306720"
  },
  {
    "text": "this large um multiples visualization you can see where the various uh viewers",
    "start": "1306720",
    "end": "1312880"
  },
  {
    "text": "zoomed in on and what they found most interesting and what's nice is you can relate it to some tasks you give them so you can ask",
    "start": "1312880",
    "end": "1319520"
  },
  {
    "text": "them some questions you can have them explore the visualizations and you can see you can get them to describe them",
    "start": "1319520",
    "end": "1325280"
  },
  {
    "text": "afterwards but you can relate what they choose to explore with what they describe",
    "start": "1325280",
    "end": "1330559"
  },
  {
    "text": "and here's an interesting example where we again take the population view but now we break up the population in two",
    "start": "1330559",
    "end": "1338000"
  },
  {
    "text": "and we uh aggregate the zoom patterns of the population of participants that have the highest rating from this for this",
    "start": "1338000",
    "end": "1344640"
  },
  {
    "text": "visualization and on the right hand side the population of participants that have the lowest rating for this visualization",
    "start": "1344640",
    "end": "1351120"
  },
  {
    "text": "and we can see that um on the left are the participants that um engaged much",
    "start": "1351120",
    "end": "1356960"
  },
  {
    "text": "more with the visualization they had similar amounts of time to view it but the ones on the left explored much more",
    "start": "1356960",
    "end": "1362480"
  },
  {
    "text": "of it and found it more interesting as a result and here's an example of zoom maps",
    "start": "1362480",
    "end": "1368640"
  },
  {
    "text": "applied to some natural images just to show you some of that variability now it's very much the same",
    "start": "1368640",
    "end": "1375280"
  },
  {
    "text": "story as we saw with importance in that on an individual by individual basis we're going to see some differences in",
    "start": "1375280",
    "end": "1381679"
  },
  {
    "text": "how people view images but once we aggregate the the data from multiple",
    "start": "1381679",
    "end": "1387120"
  },
  {
    "text": "participants in this case after around 20 participants we start to converge",
    "start": "1387120",
    "end": "1394080"
  },
  {
    "text": "we end up with a average heat map that looks very much like an eye tracking heat map",
    "start": "1394080",
    "end": "1400080"
  },
  {
    "text": "and you can see that average zoom app with the uh ground truth eye fixations overlaid on top of it",
    "start": "1400080",
    "end": "1406080"
  },
  {
    "text": "and of course we can use this as just another data capture modality for",
    "start": "1406080",
    "end": "1411440"
  },
  {
    "text": "um figuring out which content to prioritize um in that image if we have a thumbnailing application or a",
    "start": "1411440",
    "end": "1417679"
  },
  {
    "text": "summarization application so while we could compute individualized summaries or thumbnails",
    "start": "1417679",
    "end": "1424400"
  },
  {
    "text": "for each of the observers that viewed the image we can also aggregate them together and compute a single",
    "start": "1424400",
    "end": "1430640"
  },
  {
    "text": "uh population thumbnail a general thumbnail for that image that represents",
    "start": "1430640",
    "end": "1435760"
  },
  {
    "text": "the content in that image that's most interesting to a general population of observers",
    "start": "1435760",
    "end": "1441760"
  },
  {
    "text": "and i i mentioned to you earlier that saliency models have gotten really good lately um and the data sets have gotten",
    "start": "1441760",
    "end": "1449760"
  },
  {
    "text": "quite large for capturing attention patterns and so this is why we're starting to see them more and more in",
    "start": "1449760",
    "end": "1455520"
  },
  {
    "text": "industrial application whereas maybe 10 years ago it would be in every introduction and",
    "start": "1455520",
    "end": "1461440"
  },
  {
    "text": "motivation what people would find",
    "start": "1461440",
    "end": "1465679"
  },
  {
    "text": "what what people would find useful for their saliency model they would describe these thumbnailing or summarization or",
    "start": "1466480",
    "end": "1472960"
  },
  {
    "text": "retargeting applications uh now we see them out in the industry out in the world and twitter and adobe are two two",
    "start": "1472960",
    "end": "1479600"
  },
  {
    "text": "of the companies that um use these models these same models can be applied to",
    "start": "1479600",
    "end": "1484640"
  },
  {
    "text": "retargeting and reframing videos to new aspect ratios",
    "start": "1484640",
    "end": "1490080"
  },
  {
    "text": "and um in all these cases uh cropping has been based on the idea of",
    "start": "1490080",
    "end": "1495440"
  },
  {
    "text": "prioritizing image content where the saliency models are used to decide what's important to keep in the crop",
    "start": "1495440",
    "end": "1502480"
  },
  {
    "text": "but what's important varies on the context and my claim here is that depending on how long someone is",
    "start": "1502480",
    "end": "1509520"
  },
  {
    "text": "looking at something at an image in particular or even where an image is",
    "start": "1509520",
    "end": "1514799"
  },
  {
    "text": "viewed or where it's displayed on the screen is it in the banner is it off to the side is it the main image on the",
    "start": "1514799",
    "end": "1520480"
  },
  {
    "text": "screen that should affect um uh what the observer takes away from it they have a different amount of time to",
    "start": "1520480",
    "end": "1526720"
  },
  {
    "text": "interact with it they'll see a different portion of that image and so you might actually want and this",
    "start": "1526720",
    "end": "1533120"
  },
  {
    "text": "is my claim different crops depending on how long you think the end consumer will interact with that image and that was",
    "start": "1533120",
    "end": "1540320"
  },
  {
    "text": "the premise for uh another piece of work um done with um in this case the code",
    "start": "1540320",
    "end": "1546799"
  },
  {
    "text": "charts methodology and this methodology is uniquely well suited to capture",
    "start": "1546799",
    "end": "1552000"
  },
  {
    "text": "attention at different viewing durations as you'll see so the way that this methodology works and this is going to",
    "start": "1552000",
    "end": "1558799"
  },
  {
    "text": "loop so hopefully you'll you'll you'll get a feel for it you see an image for a fixed",
    "start": "1558799",
    "end": "1564559"
  },
  {
    "text": "duration of time when the image disappears you see a grid of three letter codes the grid of three letter",
    "start": "1564559",
    "end": "1570080"
  },
  {
    "text": "codes flashes so quickly that then you're asked to report um what is the",
    "start": "1570080",
    "end": "1575200"
  },
  {
    "text": "three letter code you were looking at and because it was so quick that's gonna be the last point where you looked at on",
    "start": "1575200",
    "end": "1581120"
  },
  {
    "text": "the image so you're really recording where your eyes landed last on the image before that grid of three letter codes",
    "start": "1581120",
    "end": "1588080"
  },
  {
    "text": "appeared um and so this is wonderfully fast to collect attention patterns but one one",
    "start": "1588080",
    "end": "1595200"
  },
  {
    "text": "thing if you're a keen listener you'll notice is that we get a single point where someone's looking for a single",
    "start": "1595200",
    "end": "1601520"
  },
  {
    "text": "observer at a single duration but because this is very cheap we can aggregate this over many observers who",
    "start": "1601520",
    "end": "1609120"
  },
  {
    "text": "look at the image for different amounts of time and this is what we do here in this paper um this is again with um",
    "start": "1609120",
    "end": "1615919"
  },
  {
    "text": "annelise newman who's a phd student here and here what you can see is that the",
    "start": "1615919",
    "end": "1622960"
  },
  {
    "text": "attention patterns at half a second of viewing after three seconds of viewing and after five seconds of viewing are",
    "start": "1622960",
    "end": "1628880"
  },
  {
    "text": "quite different from each other and we use that um we can use that for different applications and so our",
    "start": "1628880",
    "end": "1634960"
  },
  {
    "text": "proposal in this paper was this first saliency model that predicted",
    "start": "1634960",
    "end": "1640640"
  },
  {
    "text": "not a single saliency image but three saliency images in a single forward pass",
    "start": "1640640",
    "end": "1646320"
  },
  {
    "text": "through this neural network and the idea is that you can pass in an image and and figure",
    "start": "1646320",
    "end": "1653120"
  },
  {
    "text": "out what the saliency map should look like for those different durations in applying them to different applications",
    "start": "1653120",
    "end": "1658640"
  },
  {
    "text": "like cropping as you see on the left here uh or prioritizing image content",
    "start": "1658640",
    "end": "1664240"
  },
  {
    "text": "for and that's what you see a visualization um for on the right side of the screen where um we're seeing that",
    "start": "1664240",
    "end": "1671600"
  },
  {
    "text": "if if people have very little amount of time to look at something you might only want to render that portion of the image",
    "start": "1671600",
    "end": "1677679"
  },
  {
    "text": "and if you're using if you have vrar applications or you're transmitting that image across the network",
    "start": "1677679",
    "end": "1685519"
  },
  {
    "text": "so a lot of the focus um up until this point in the presentation has been on",
    "start": "1686240",
    "end": "1691440"
  },
  {
    "text": "using saliency for prioritizing visual content right for doing image processing tasks to crop retarget",
    "start": "1691440",
    "end": "1699200"
  },
  {
    "text": "and otherwise parse some of these images now i wanna show you one more application but this",
    "start": "1699200",
    "end": "1705520"
  },
  {
    "text": "time we're gonna look at saliency as a way to edit an image so saliency is used",
    "start": "1705520",
    "end": "1710799"
  },
  {
    "text": "to train a computational model to learn to edit an image to make certain parts",
    "start": "1710799",
    "end": "1716720"
  },
  {
    "text": "of that image stand out and this model this is gate shift net it takes as",
    "start": "1716720",
    "end": "1722880"
  },
  {
    "text": "input and image and it takes as input an object or a region of interest um",
    "start": "1722880",
    "end": "1728399"
  },
  {
    "text": "that the user supplies um or this could be replaced by a click that we then",
    "start": "1728399",
    "end": "1733679"
  },
  {
    "text": "interpret as an object and what the network does is it predicts a set of parameters to apply to the",
    "start": "1733679",
    "end": "1740399"
  },
  {
    "text": "foreground and the background regions of the image to make that target object stand out or recede",
    "start": "1740399",
    "end": "1746880"
  },
  {
    "text": "into the background in the case of distractor suppression so that's just a quick diagram of our um of the encoder",
    "start": "1746880",
    "end": "1754080"
  },
  {
    "text": "part of our network the only key i want to show you here is that it's a neural network but that",
    "start": "1754080",
    "end": "1759919"
  },
  {
    "text": "at the end we're just predicting two sets of parameters foreground parameters pf and background parameters and that's",
    "start": "1759919",
    "end": "1765679"
  },
  {
    "text": "all we're doing and that makes uh our model able to apply to images of",
    "start": "1765679",
    "end": "1771520"
  },
  {
    "text": "arbitrary sizes and to videos because we're not just outputting images but a set of parameters we can interact",
    "start": "1771520",
    "end": "1778159"
  },
  {
    "text": "with later as well so let's take a look at some visual examples let's look at this archival",
    "start": "1778159",
    "end": "1783279"
  },
  {
    "text": "photo and this um and our goal here is to take this bus on the right hand side",
    "start": "1783279",
    "end": "1788559"
  },
  {
    "text": "and make it pop even more make it more attention capturing and you can see um hopefully in this",
    "start": "1788559",
    "end": "1795279"
  },
  {
    "text": "image um that our approach successfully emphasizes the bus while maintaining",
    "start": "1795279",
    "end": "1800960"
  },
  {
    "text": "fidelity to the input photograph so our goal is to um",
    "start": "1800960",
    "end": "1806240"
  },
  {
    "text": "redirect attentions in a subtle way such that the photo does not look too edited",
    "start": "1806240",
    "end": "1811279"
  },
  {
    "text": "so there's these complementary objectives that are working together to achieve this um and we can see indeed um if we run a",
    "start": "1811279",
    "end": "1819520"
  },
  {
    "text": "saliency model over it um that we succeed in the edited result in",
    "start": "1819520",
    "end": "1826960"
  },
  {
    "text": "putting more attention on the bus and now we can use this within graphic design workflows we want to take this",
    "start": "1826960",
    "end": "1833200"
  },
  {
    "text": "image and we want to make this about the bus and we want to make a poster around it well we can do it using our tool",
    "start": "1833200",
    "end": "1839440"
  },
  {
    "text": "and the exact same model with uh somewhat different objective but the same architecture can be used to",
    "start": "1839440",
    "end": "1846159"
  },
  {
    "text": "decrease attention to an image region in this case to say suppress a distractor",
    "start": "1846159",
    "end": "1851279"
  },
  {
    "text": "um and move attention away from it and here's a visualization of the attention",
    "start": "1851279",
    "end": "1857919"
  },
  {
    "text": "um predicted saliency maps and you can see that we've indeed succeeded at moving",
    "start": "1857919",
    "end": "1863200"
  },
  {
    "text": "sailing seat from that back boat to more towards the front",
    "start": "1863200",
    "end": "1869039"
  },
  {
    "text": "and here's a stock photo a high-def photograph that that we can parse as well and we",
    "start": "1869039",
    "end": "1875279"
  },
  {
    "text": "might want to emphasize the shoes to create a product ad and very subtly manipulate attention towards them and",
    "start": "1875279",
    "end": "1882559"
  },
  {
    "text": "here's the result of our uh model um and here is a hiking picture take a",
    "start": "1882559",
    "end": "1889039"
  },
  {
    "text": "look at the blue backpack on the right it should pop more now",
    "start": "1889039",
    "end": "1895039"
  },
  {
    "text": "one last example with the beer glass we want to make the beer glass on the left more attention capturing",
    "start": "1895039",
    "end": "1901919"
  },
  {
    "text": "and there we go and we can i mentioned we can apply this to videos",
    "start": "1901919",
    "end": "1908080"
  },
  {
    "text": "so there we go um the video should",
    "start": "1908080",
    "end": "1913200"
  },
  {
    "text": "ah sorry my computer is lagging a bit behind let's try to make this work",
    "start": "1913200",
    "end": "1920240"
  },
  {
    "text": "okay so what you should see is two snowboarders um and what we do is we make the",
    "start": "1920240",
    "end": "1927039"
  },
  {
    "text": "snowboard stand out um that didn't play but you should see now",
    "start": "1927039",
    "end": "1932720"
  },
  {
    "text": "that the backpack more attention capturing in this video as well but we can apply the same methodology frame to",
    "start": "1932720",
    "end": "1939120"
  },
  {
    "text": "frame but i mentioned because this is parametric so we're not predicting a",
    "start": "1939120",
    "end": "1946000"
  },
  {
    "text": "final image we're predicting a set of parameters we can hand control back to the user and allow them to interact with",
    "start": "1946000",
    "end": "1952480"
  },
  {
    "text": "it and tailor the the image to their liking so they can dial up or down uh",
    "start": "1952480",
    "end": "1957840"
  },
  {
    "text": "how salient that that object is intended to be and that's where they're trading off realism for attention capture or",
    "start": "1957840",
    "end": "1965360"
  },
  {
    "text": "they can dig into some of the parameters that we've predicted um so sharpness exposure contrast and",
    "start": "1965360",
    "end": "1973519"
  },
  {
    "text": "adjust them themselves to get the exact look that they want to go after",
    "start": "1973519",
    "end": "1979360"
  },
  {
    "text": "okay and um now i i do have a few final slides and",
    "start": "1979360",
    "end": "1985840"
  },
  {
    "text": "i do want to wrap up kind of this uh big set of projects that i've uh talked",
    "start": "1985840",
    "end": "1991679"
  },
  {
    "text": "to you about um in in using attention and what i want you to notice here is this this common",
    "start": "1991679",
    "end": "1997600"
  },
  {
    "text": "thread the common thread for all these projects whether we're parsing images or graphic designs or visualizations or in",
    "start": "1997600",
    "end": "2004640"
  },
  {
    "text": "the last case that i just showed you videos this was all done in with the use of hci",
    "start": "2004640",
    "end": "2010720"
  },
  {
    "text": "tools um that um were measuring and understanding human",
    "start": "2010720",
    "end": "2016080"
  },
  {
    "text": "perception that were then used to train machine learning models on that collected perceptual data and",
    "start": "2016080",
    "end": "2023519"
  },
  {
    "text": "then we went full circle and we fed those models back into interfaces that would make designer image processing",
    "start": "2023519",
    "end": "2030240"
  },
  {
    "text": "applications easier or more effective and this makes um i think this body of",
    "start": "2030240",
    "end": "2037440"
  },
  {
    "text": "work a very interdisciplinary space and i'm going to reiterate this again this is very ripe for continued innovation so",
    "start": "2037440",
    "end": "2043840"
  },
  {
    "text": "i'd be excited to see more students in this space merging insights from",
    "start": "2043840",
    "end": "2049280"
  },
  {
    "text": "human perception with computer vision computer graphics and machine learning applications and",
    "start": "2049280",
    "end": "2055599"
  },
  {
    "text": "that's what drew me to these topics during my phd and i hope that um i drove your interest to these topics in this",
    "start": "2055599",
    "end": "2061679"
  },
  {
    "text": "presentation too now i want to spend i'm gonna kind of switch gears a little bit and i'm just",
    "start": "2061679",
    "end": "2067679"
  },
  {
    "text": "gonna spend five minutes on um a different area that's been occupying me",
    "start": "2067679",
    "end": "2072720"
  },
  {
    "text": "um over the last couple of years at adobe and this is unreadability",
    "start": "2072720",
    "end": "2079280"
  },
  {
    "text": "and it seems like a different topic but actually much like all the work that",
    "start": "2079280",
    "end": "2085118"
  },
  {
    "text": "i've presented to you today i'm drawn to this topic because it's an interdisciplinary space that leverages",
    "start": "2085119",
    "end": "2091599"
  },
  {
    "text": "our understanding of human perception and tools to study human perception into",
    "start": "2091599",
    "end": "2097440"
  },
  {
    "text": "the design of better tools and in this case we're focused on reading and education and literacy",
    "start": "2097440",
    "end": "2104720"
  },
  {
    "text": "so the premise of this body of work is that if we tune various typographic",
    "start": "2104720",
    "end": "2110320"
  },
  {
    "text": "characteristics various features like the ones i have on the screen here whether it's font family and font size",
    "start": "2110320",
    "end": "2116320"
  },
  {
    "text": "and character and line spacing or or we get um to the full extreme of variable",
    "start": "2116320",
    "end": "2121599"
  },
  {
    "text": "fonts and we start to tune the parameters there if you haven't heard of variable fonts look them up they're",
    "start": "2121599",
    "end": "2127359"
  },
  {
    "text": "really cool well it turns out if we tune all these parameters to the individual reader we can make everyone a better",
    "start": "2127359",
    "end": "2133839"
  },
  {
    "text": "reader and this holds based on our research for",
    "start": "2133839",
    "end": "2139200"
  },
  {
    "text": "young readers for older readers struggling readers and proficient readers and what i'm showing you here is",
    "start": "2139200",
    "end": "2145520"
  },
  {
    "text": "um every line on these visualizations as an individual so you see some grade",
    "start": "2145520",
    "end": "2150720"
  },
  {
    "text": "three students on the left here and you see kind of a sampling of crowdsourced adults on the right here of varying ages",
    "start": "2150720",
    "end": "2158480"
  },
  {
    "text": "and the length of the line is proportional to their reading gain when a reading",
    "start": "2158480",
    "end": "2164400"
  },
  {
    "text": "format was tuned to them so in the in the case of the adults we just changed what font they read in and",
    "start": "2164400",
    "end": "2172079"
  },
  {
    "text": "in some cases we saw uh over 200 words per minute difference and so",
    "start": "2172079",
    "end": "2178160"
  },
  {
    "text": "what we're seeing is that you could translate this to say four or five extra pages of reading you could do",
    "start": "2178160",
    "end": "2184800"
  },
  {
    "text": "an hour if you just knew what font was right for you and that's what leads us to kind of this",
    "start": "2184800",
    "end": "2190720"
  },
  {
    "text": "this really interesting challenge how do we find the format that works for everyone",
    "start": "2190720",
    "end": "2195839"
  },
  {
    "text": "because one thing we do see is that it's so different across individuals there's no one set or even small set of formats",
    "start": "2195839",
    "end": "2203359"
  },
  {
    "text": "that work for everyone everyone could benefit from their unique personalized font uh format which which",
    "start": "2203359",
    "end": "2210240"
  },
  {
    "text": "isn't just font it's all these other characteristics i talked about and we talk about this as your digital",
    "start": "2210240",
    "end": "2216079"
  },
  {
    "text": "prescription so what's the equivalent of the quick optometrist like test um and",
    "start": "2216079",
    "end": "2221760"
  },
  {
    "text": "we're after this question as well so we um we have this virtual readability lab",
    "start": "2221760",
    "end": "2227040"
  },
  {
    "text": "and you can try it out today but we're continuing to populate it with a set of uh short tests they're intended to help",
    "start": "2227040",
    "end": "2234160"
  },
  {
    "text": "an individual narrow in on some features of their best uh format right now we",
    "start": "2234160",
    "end": "2239280"
  },
  {
    "text": "just have fonts and spacings but we plan to expand this and in parallel we're",
    "start": "2239280",
    "end": "2244480"
  },
  {
    "text": "also looking at um machine learning models of what personal characteristics",
    "start": "2244480",
    "end": "2250320"
  },
  {
    "text": "help us figure out how to tailor a format to that individual",
    "start": "2250320",
    "end": "2255760"
  },
  {
    "text": "example test that one would be given on our vrl and that the kinds of tests",
    "start": "2255760",
    "end": "2261119"
  },
  {
    "text": "that we give our participants is we have them read chunks of text in a particular format we track their reading",
    "start": "2261119",
    "end": "2268000"
  },
  {
    "text": "speed we ask them comprehension questions and we repeat this again and again while varying the format",
    "start": "2268000",
    "end": "2274240"
  },
  {
    "text": "underneath uh and we do that in order to be able to recommend a format to them at the end",
    "start": "2274240",
    "end": "2280320"
  },
  {
    "text": "based on the reading speeds and comprehension and as we evolve the suite of tests we're also looking at",
    "start": "2280320",
    "end": "2286720"
  },
  {
    "text": "additional metrics of comprehension comfort fluency um endurance and um and",
    "start": "2286720",
    "end": "2294640"
  },
  {
    "text": "i promised i'd bring alexandra pakutsuki back initially",
    "start": "2294640",
    "end": "2300560"
  },
  {
    "text": "so she's a collaborator with us looking at eye tracking um and she was working",
    "start": "2300560",
    "end": "2305760"
  },
  {
    "text": "on web gazer one of the web uh based eye tracking methodologies and we're looking at",
    "start": "2305760",
    "end": "2311680"
  },
  {
    "text": "what the signatures of the eyes can tell us about how people read and how that",
    "start": "2311680",
    "end": "2317119"
  },
  {
    "text": "could lead to a better format for them so can we use their eyes to figure out",
    "start": "2317119",
    "end": "2322400"
  },
  {
    "text": "what they're more comfortable reading and we're doing the same with voice as well so there's a lot more exciting",
    "start": "2322400",
    "end": "2327920"
  },
  {
    "text": "directions in which this is going so uh as you can see that brings us back",
    "start": "2327920",
    "end": "2334160"
  },
  {
    "text": "full circle and here um in this readability project as in the other projects that i've described to you",
    "start": "2334160",
    "end": "2340320"
  },
  {
    "text": "today again the focus is leveraging human perception for better tool design except that now",
    "start": "2340320",
    "end": "2347119"
  },
  {
    "text": "the findings can be taken one step further so rather than just optimizing the visual",
    "start": "2347119",
    "end": "2353440"
  },
  {
    "text": "in this case we're optimizing the visual rendering of the textual and we're optimizing it to help",
    "start": "2353440",
    "end": "2360640"
  },
  {
    "text": "individuals ingest information more easily and i think this is the holy grail because it's the potential to",
    "start": "2360640",
    "end": "2366640"
  },
  {
    "text": "contribute to closing the learning gap and making an impact on education and literacy and um that makes me very",
    "start": "2366640",
    "end": "2374000"
  },
  {
    "text": "excited to to work on these topics so with that i'm gonna close um the main",
    "start": "2374000",
    "end": "2379440"
  },
  {
    "text": "portion of the talk i'm gonna open up the zoom floor to questions and i'm happy to answer them based on any of the",
    "start": "2379440",
    "end": "2385440"
  },
  {
    "text": "um topics that i've presented today thank you very much i left lots of time for discussion",
    "start": "2385440",
    "end": "2392640"
  },
  {
    "text": "yes this is great um and uh i think we already have some questions so thanks for the talk there",
    "start": "2392640",
    "end": "2398240"
  },
  {
    "text": "are so many interesting points in this and i have so many questions but i'll try and limit myself to one unless maybe",
    "start": "2398240",
    "end": "2403280"
  },
  {
    "text": "there's some more time at the end but i can ask another um one of the questions i had was about the um salient suit prediction that you were",
    "start": "2403280",
    "end": "2409520"
  },
  {
    "text": "talking about um correct me if i'm wrong it seemed like a lot of the content that was being uh",
    "start": "2409520",
    "end": "2415920"
  },
  {
    "text": "demonstrated in the talk was more like one-way uh content in the center in the sense of",
    "start": "2415920",
    "end": "2421520"
  },
  {
    "text": "like a producer and they want you to sort of get to know something like maybe there's an infographic or just a poster",
    "start": "2421520",
    "end": "2427040"
  },
  {
    "text": "um i'm wondering if at all the sales prediction can act as a proxy for like actual",
    "start": "2427040",
    "end": "2432400"
  },
  {
    "text": "engagement but when i think of a lot of advertisement on the internet there's a talk there's a lot of talk",
    "start": "2432400",
    "end": "2438079"
  },
  {
    "text": "about like click through and just like trying to get you to actually engage um with the content and so i'm wondering",
    "start": "2438079",
    "end": "2444240"
  },
  {
    "text": "at all if like the failure to prediction is a proxy um to that or if there's any sort of like",
    "start": "2444240",
    "end": "2450640"
  },
  {
    "text": "further to that it definitely yeah it definitely can be so um in the case of websites um companies might be",
    "start": "2450640",
    "end": "2457920"
  },
  {
    "text": "monitoring not just click through rates but right where the cursor is going in some cases that's proxy for where your",
    "start": "2457920",
    "end": "2463760"
  },
  {
    "text": "eyes are moving um in some cases not because your eyes can move somewhere while your cursor isn't but in that case you can take say",
    "start": "2463760",
    "end": "2470800"
  },
  {
    "text": "beta users or beta testers and have them look through those websites or user interfaces and see where their eyes move",
    "start": "2470800",
    "end": "2476800"
  },
  {
    "text": "and if they even notice your content right so in what i was showing you was like let's look at the saliency map of a",
    "start": "2476800",
    "end": "2483680"
  },
  {
    "text": "particular piece of content but you can embed that content in other content and just treat the bigger content as what",
    "start": "2483680",
    "end": "2491200"
  },
  {
    "text": "you compute the sale and see over what you measure eye movement so you can see you can then compare how the attention",
    "start": "2491200",
    "end": "2498240"
  },
  {
    "text": "um moves between those various pieces and do these pieces attract attention more",
    "start": "2498240",
    "end": "2504000"
  },
  {
    "text": "than those pieces so definitely and that's done in the industry and marketers are interested in that too and",
    "start": "2504000",
    "end": "2510160"
  },
  {
    "text": "i think another example was like the storefront right because every individual object has a graphic design",
    "start": "2510160",
    "end": "2517200"
  },
  {
    "text": "on it that that's optimized to lead your eyes in a particular way maybe if it's well designed but then in",
    "start": "2517200",
    "end": "2523839"
  },
  {
    "text": "in the context of all the other ones all the other products um the the research might be like what's",
    "start": "2523839",
    "end": "2530319"
  },
  {
    "text": "popping out on the shelf when you look at it all at once right and and then there's strategic placement where you",
    "start": "2530319",
    "end": "2536960"
  },
  {
    "text": "might want to place things so that they stand out from their surroundings so you can always take the bigger context into",
    "start": "2536960",
    "end": "2542960"
  },
  {
    "text": "account and compute the sale and see over that right or run some eye tracking studies there",
    "start": "2542960",
    "end": "2548960"
  },
  {
    "text": "right on awesome thank you yeah great thank you um",
    "start": "2548960",
    "end": "2554800"
  },
  {
    "text": "i was wondering uh thank you for this talk by the way super interesting work um i was wondering that uh readability",
    "start": "2554800",
    "end": "2562960"
  },
  {
    "text": "work are you collecting any data on potential learning differences or um",
    "start": "2562960",
    "end": "2569680"
  },
  {
    "text": "yeah differences in learning style definitely so we're not um",
    "start": "2569680",
    "end": "2575520"
  },
  {
    "text": "um we're not indexing on the differences in learning style but we're definitely capturing different types of say reading",
    "start": "2575520",
    "end": "2583440"
  },
  {
    "text": "and learning proficiency um and when when we run this in a crowdsourced way",
    "start": "2583440",
    "end": "2588480"
  },
  {
    "text": "um we have these very thorough surveys where we ask every possible question of",
    "start": "2588480",
    "end": "2593839"
  },
  {
    "text": "um you know how how proficient do you self rate yourself to",
    "start": "2593839",
    "end": "2599200"
  },
  {
    "text": "how proficient do you rate yourself to be you know whether you've been diagnosed with reading learning",
    "start": "2599200",
    "end": "2604240"
  },
  {
    "text": "disabilities whether you read frequently and how frequently and what kind of reading do you do do you do reading as",
    "start": "2604240",
    "end": "2610000"
  },
  {
    "text": "part of your work all of those when taken together they paint a picture for how proficient a reader someone is",
    "start": "2610000",
    "end": "2617200"
  },
  {
    "text": "and the way that we're talking about this work is that it's it's not that there's like a dyslexic population a",
    "start": "2617200",
    "end": "2623040"
  },
  {
    "text": "non-dyslexic population because that's the question that comes up right are we what are",
    "start": "2623040",
    "end": "2628240"
  },
  {
    "text": "what who are we designing for the average reader the proficient reader the dyslexic creator well we're all",
    "start": "2628240",
    "end": "2633680"
  },
  {
    "text": "somewhere on the spectrum from um not very proficient to proficient reader and",
    "start": "2633680",
    "end": "2639920"
  },
  {
    "text": "what our research is showing is that indeed everyone can can kind of jump up in that scale and improve their reading",
    "start": "2639920",
    "end": "2646800"
  },
  {
    "text": "um and and so we we are looking at different types of reading and in some",
    "start": "2646800",
    "end": "2652400"
  },
  {
    "text": "targeted ways we're also recruiting participants with different proficiency levels in reading so we're looking at",
    "start": "2652400",
    "end": "2658160"
  },
  {
    "text": "second language learners we're looking at adult learners who who are",
    "start": "2658160",
    "end": "2663520"
  },
  {
    "text": "um struggling to read and and attending classes in order to do that we're",
    "start": "2663520",
    "end": "2668800"
  },
  {
    "text": "looking at um children uh in in classrooms both um",
    "start": "2668800",
    "end": "2674000"
  },
  {
    "text": "diagnosed dyslexic and cannot but the the right way to think about this is that everyone can um",
    "start": "2674000",
    "end": "2680480"
  },
  {
    "text": "gain some improvements and that we we have to design for the individual",
    "start": "2680480",
    "end": "2686000"
  },
  {
    "text": "yeah thank you just a quick follow-up to that um i was wondering for example as a starting point um",
    "start": "2686000",
    "end": "2692319"
  },
  {
    "text": "when you want to customize for for individuals if you do have some of those standardized um rating scales that are",
    "start": "2692319",
    "end": "2698880"
  },
  {
    "text": "used would you be able to make predictions without having um that like",
    "start": "2698880",
    "end": "2704800"
  },
  {
    "text": "initial data on each other individuals well so we we have some",
    "start": "2704800",
    "end": "2711520"
  },
  {
    "text": "some data from the different users like um even their age could put them in a",
    "start": "2711520",
    "end": "2717440"
  },
  {
    "text": "bucket that would help us recommend something for them the more we know about them the more we can recommend to",
    "start": "2717440",
    "end": "2723280"
  },
  {
    "text": "them so we haven't um used maybe some of the standardized learning skills that you're",
    "start": "2723280",
    "end": "2728560"
  },
  {
    "text": "talking about um but uh we we are trying to capture as much data as possible in",
    "start": "2728560",
    "end": "2734160"
  },
  {
    "text": "the studies that we're doing to understand what uh features of an individual uh map them to what features",
    "start": "2734160",
    "end": "2740640"
  },
  {
    "text": "of the reading format uh but in practice if it deployed depending on where it's deployed or who it's deployed by and",
    "start": "2740640",
    "end": "2747440"
  },
  {
    "text": "what tools you you know you could use whatever data is available or none at all to try to",
    "start": "2747440",
    "end": "2753760"
  },
  {
    "text": "offer something so if you know nothing you you could still offer some formats",
    "start": "2753760",
    "end": "2758800"
  },
  {
    "text": "that do generally well but the more and more you know you can tailor that prediction",
    "start": "2758800",
    "end": "2763920"
  },
  {
    "text": "better and better and the idea is that as you tailor it more and more the gains increase",
    "start": "2763920",
    "end": "2769760"
  },
  {
    "text": "thank you thank you so much for the talk uh in the beginning i think you're",
    "start": "2769760",
    "end": "2775200"
  },
  {
    "text": "you're saying that you looked at click-based methods and webcam based methods because they're more scalable to",
    "start": "2775200",
    "end": "2781520"
  },
  {
    "text": "collect data i was i was just wondering like what about like ar headsets do you think like as people",
    "start": "2781520",
    "end": "2790000"
  },
  {
    "text": "wear them or or buy more of them that would also be like more scalable or like yeah what are the pros and cons",
    "start": "2790000",
    "end": "2797359"
  },
  {
    "text": "yeah definitely um we can see more of that we i think we haven't seen",
    "start": "2797359",
    "end": "2803440"
  },
  {
    "text": "the proliferation of ar and vr headsets in the consumer market as much as maybe we expected um but maybe that will",
    "start": "2803440",
    "end": "2810960"
  },
  {
    "text": "change and definitely those those headsets offer us a way to capture",
    "start": "2810960",
    "end": "2816160"
  },
  {
    "text": "attention in very natural scenarios right especially if any of those glasses um actually take off",
    "start": "2816160",
    "end": "2822720"
  },
  {
    "text": "then having someone's eye movements as they uh walk through the world is the most natural signal we can capture and",
    "start": "2822720",
    "end": "2829760"
  },
  {
    "text": "we'll answer a whole bunch of different questions right um because the context varies so much and and the key is really",
    "start": "2829760",
    "end": "2836160"
  },
  {
    "text": "to get the data from individuals all over the world to understand how attention patterns vary so",
    "start": "2836160",
    "end": "2843280"
  },
  {
    "text": "another thing to consider is that vr and ar headsets may end up in some hands and not others and um we might be pulling",
    "start": "2843280",
    "end": "2851119"
  },
  {
    "text": "somewhat biased data from that from those sources as well so there's any any tool we use to capture attention",
    "start": "2851119",
    "end": "2858079"
  },
  {
    "text": "can teach us something about how people look and in all cases it's a trade-off between",
    "start": "2858079",
    "end": "2864240"
  },
  {
    "text": "cost diversity of the population and accuracy of the signal and we can",
    "start": "2864240",
    "end": "2870400"
  },
  {
    "text": "make all those trade-offs and like even with the cell phone cameras today there's much better tracking available",
    "start": "2870400",
    "end": "2876400"
  },
  {
    "text": "from the cell phone camera um that then was available even a few years ago and cell phones are something that many more",
    "start": "2876400",
    "end": "2882400"
  },
  {
    "text": "individuals have so i think that um there's more promise from um cell phone",
    "start": "2882400",
    "end": "2887839"
  },
  {
    "text": "collection of of eye movement data but then then it's going to be mostly limited to the cell phone screen um but",
    "start": "2887839",
    "end": "2893440"
  },
  {
    "text": "yes future ar glasses will will give us a broader view",
    "start": "2893440",
    "end": "2898720"
  },
  {
    "text": "awesome thank you thank you hi zoya um i actually have i think a nice follow-up question which is um",
    "start": "2898720",
    "end": "2905280"
  },
  {
    "text": "we've been seeing a lot of excitement around vr and ar and uh these platforms are really",
    "start": "2905280",
    "end": "2910800"
  },
  {
    "text": "interesting because they basically let you create an entire virtual world or a virtual",
    "start": "2910800",
    "end": "2916400"
  },
  {
    "text": "overlay for the world and you've done a bunch of research looking at",
    "start": "2916400",
    "end": "2921520"
  },
  {
    "text": "how human perception works and how we can manipulate visual content to change how",
    "start": "2921520",
    "end": "2926640"
  },
  {
    "text": "people perceive it and i'm i'm wondering what you think the interesting research directions are in this space when we are",
    "start": "2926640",
    "end": "2933920"
  },
  {
    "text": "able to control not just like a single image that somebody is looking at but they're basically entire surroundings",
    "start": "2933920",
    "end": "2940160"
  },
  {
    "text": "that they perceive yeah and and it's kind of scary right that kind of a feature it has a black mirror like",
    "start": "2940160",
    "end": "2947040"
  },
  {
    "text": "flavor to it of uh what do you block out from your visual field of view and um i",
    "start": "2947040",
    "end": "2952240"
  },
  {
    "text": "i think that uh it's it would be easier if it's controlled by",
    "start": "2952240",
    "end": "2958160"
  },
  {
    "text": "the individual user right rather than it's controlled by some third party um where you might have preferences for",
    "start": "2958160",
    "end": "2964640"
  },
  {
    "text": "what you want to look at or what you don't and and you why very much those same preferences when um you know you",
    "start": "2964640",
    "end": "2971680"
  },
  {
    "text": "use your computer and your browser and there's certain things you block and and certain things you ignore um so",
    "start": "2971680",
    "end": "2978400"
  },
  {
    "text": "if you could apply those same filters to the real world i mean in some cases people talked about some of these",
    "start": "2978400",
    "end": "2983920"
  },
  {
    "text": "applications of blocking out advertisements right like um like the blocker you have in chrome",
    "start": "2983920",
    "end": "2989280"
  },
  {
    "text": "just apply to the real world because i don't want to see that stuff so um i could imagine those kinds of",
    "start": "2989280",
    "end": "2994640"
  },
  {
    "text": "applications i mean another like our video screens and zoom is is a kind of",
    "start": "2994640",
    "end": "3001119"
  },
  {
    "text": "you know first stage proxy of what ar looks like and and we have these um virtual backgrounds we have",
    "start": "3001119",
    "end": "3008160"
  },
  {
    "text": "people we can change how they um look to us and they could change how they look to us so we're already doing that um and",
    "start": "3008160",
    "end": "3015920"
  },
  {
    "text": "actually there's a very relevant paper that that google um just put on archive i think this this past year um showing",
    "start": "3015920",
    "end": "3023040"
  },
  {
    "text": "that they could do distractor removal and zoom so this is very similar to the work that i showed you where um",
    "start": "3023040",
    "end": "3029680"
  },
  {
    "text": "they they can take a real background and then just suppress right change like",
    "start": "3029680",
    "end": "3035440"
  },
  {
    "text": "like make this green tree blend into the background so it's less distracting right and um really manipulate um so it",
    "start": "3035440",
    "end": "3043839"
  },
  {
    "text": "then it's a transition between a completely virtual background in the real world but we're kind of already",
    "start": "3043839",
    "end": "3049599"
  },
  {
    "text": "living it um so we'll see we'll see um as it expands to beyond the computer",
    "start": "3049599",
    "end": "3056000"
  },
  {
    "text": "screen that we can walk away from and see everything unfiltered what happens when you start to filter content",
    "start": "3056000",
    "end": "3062079"
  },
  {
    "text": "advertisers will obviously use it too so if the individuals have some control",
    "start": "3062079",
    "end": "3067440"
  },
  {
    "text": "uh left right to to turn some of those things on or off that'll be important",
    "start": "3067440",
    "end": "3073520"
  },
  {
    "text": "yeah thank you i think it's really interesting to think about how you would give people ability to",
    "start": "3073520",
    "end": "3079119"
  },
  {
    "text": "gain back more control over what they're what they're perceiving and you might have thoughts too annalise",
    "start": "3079119",
    "end": "3084720"
  },
  {
    "text": "because you were featured on a lot of these slides and",
    "start": "3084720",
    "end": "3089279"
  },
  {
    "text": "yeah um yeah no i mean i think i i think i sort of share your intuition",
    "start": "3090240",
    "end": "3096559"
  },
  {
    "text": "that applying human perception sort of too aggressively to fully virtual spaces",
    "start": "3096559",
    "end": "3105280"
  },
  {
    "text": "could like there are a lot of people who have who are really motivated to try to capture people's attention um and",
    "start": "3105280",
    "end": "3112800"
  },
  {
    "text": "putting sort of tools out in the world that allow like advertisers or social media",
    "start": "3112800",
    "end": "3119359"
  },
  {
    "text": "companies for example to have even more control over people's visual perception um is a little bit alarming to me so i i",
    "start": "3119359",
    "end": "3127040"
  },
  {
    "text": "think there is um sort of a an onus on researchers to try to figure out like how do we how do we",
    "start": "3127040",
    "end": "3133359"
  },
  {
    "text": "give more control back to the perceiver and also how can we use these tools for",
    "start": "3133359",
    "end": "3138720"
  },
  {
    "text": "um for more beneficial purposes so you know when we were doing some of our memorability work we were thinking about",
    "start": "3138720",
    "end": "3145040"
  },
  {
    "text": "um you know applications potentially in education for how to highlight content",
    "start": "3145040",
    "end": "3150559"
  },
  {
    "text": "that people would be more likely to retain your readability work has a lot of connections to education as well um",
    "start": "3150559",
    "end": "3158000"
  },
  {
    "text": "but yeah i think those are important points too is that one right is releasing the tools",
    "start": "3158000",
    "end": "3164400"
  },
  {
    "text": "back to the community to the research community to the education community to leverage their needs right as as opposed",
    "start": "3164400",
    "end": "3171760"
  },
  {
    "text": "to just close it leaving it behind closed doors right and the second is giving that to the individuals giving",
    "start": "3171760",
    "end": "3177680"
  },
  {
    "text": "some of the designing tools are with the individual and mind around the individual right so that they can",
    "start": "3177680",
    "end": "3183040"
  },
  {
    "text": "personalize what they see yeah",
    "start": "3183040",
    "end": "3188240"
  },
  {
    "text": "great um we have uh maybe one last question i sort of want to follow up on that um",
    "start": "3188240",
    "end": "3193920"
  },
  {
    "text": "when henry was just saying uh how do you weigh uh releasing uh a tool like the one that",
    "start": "3193920",
    "end": "3200640"
  },
  {
    "text": "your your team did an open source tool when like it has the potential to just you know sort of be exploited by",
    "start": "3200640",
    "end": "3206480"
  },
  {
    "text": "advertisers and whatever to um you know to control a little bit more",
    "start": "3206480",
    "end": "3212240"
  },
  {
    "text": "perception right like like what is that what is the like the judgement call there",
    "start": "3212240",
    "end": "3217440"
  },
  {
    "text": "for which for any of these tools for the laughter yeah yeah yeah like how do you weigh like uh benefiting the research",
    "start": "3217440",
    "end": "3223200"
  },
  {
    "text": "community versus um like potential misuse by members of industry well i",
    "start": "3223200",
    "end": "3229520"
  },
  {
    "text": "mean that's a tough question for anyone in computer science and artificial intelligence because pretty much any",
    "start": "3229520",
    "end": "3234800"
  },
  {
    "text": "technology you put out can be misused and i i the approach that that i take is",
    "start": "3234800",
    "end": "3240480"
  },
  {
    "text": "to try to release it openly as much as possible because that way it doesn't",
    "start": "3240480",
    "end": "3246400"
  },
  {
    "text": "it it can end up in all hands kind of equally right and um by by talking to the right",
    "start": "3246400",
    "end": "3252880"
  },
  {
    "text": "individuals or or to get it in front of the right individuals you can make sure that it um there's good use cases for it",
    "start": "3252880",
    "end": "3259760"
  },
  {
    "text": "in education right um and in you know we were talking about social media but",
    "start": "3259760",
    "end": "3265119"
  },
  {
    "text": "how we can also uh educate people better about the social media so we can take their",
    "start": "3265119",
    "end": "3271280"
  },
  {
    "text": "attention and put it on the visuals not just on the text and make sure that they engage more with the um infographic or",
    "start": "3271280",
    "end": "3278880"
  },
  {
    "text": "the visualizations that they they're more motivated to explore the data themselves um there's",
    "start": "3278880",
    "end": "3284480"
  },
  {
    "text": "even with the same kind of agencies there's ways to use that content um in all sorts of ways right you can",
    "start": "3284480",
    "end": "3290799"
  },
  {
    "text": "also um use it to bring people's attention to the warning on labels on on",
    "start": "3290799",
    "end": "3296640"
  },
  {
    "text": "product boxes right just as much as you can on them buying that box in the first place",
    "start": "3296640",
    "end": "3302160"
  },
  {
    "text": "um i you know as researchers like i chase interesting questions that tell me something about human perception the",
    "start": "3302160",
    "end": "3309119"
  },
  {
    "text": "more you know the more can hurt but um but also the more you know the more",
    "start": "3309119",
    "end": "3315359"
  },
  {
    "text": "you can find um ways of of kind of hacking um tools to to improve",
    "start": "3315359",
    "end": "3322079"
  },
  {
    "text": "perception right and to bridge some gaps right in the case of readability as well we're trying to close some of the",
    "start": "3322079",
    "end": "3327760"
  },
  {
    "text": "education gaps and if you make something more visible more salient for people who",
    "start": "3327760",
    "end": "3333200"
  },
  {
    "text": "might have missed that you're bridging those gaps as well um and while creating others so",
    "start": "3333200",
    "end": "3338640"
  },
  {
    "text": "yeah this it's not an answer it's a it's an ongoing philosophical discussion with with all of us in computer science and",
    "start": "3338640",
    "end": "3345280"
  },
  {
    "text": "ai and that's our role to also figure that out especially as as beyond the phd",
    "start": "3345280",
    "end": "3350880"
  },
  {
    "text": "and we go out into the real world we can have real impact so um i think that's important to think about",
    "start": "3350880",
    "end": "3357760"
  },
  {
    "text": "appreciate it thank you thank you great well thank you so much zoya this was a",
    "start": "3357760",
    "end": "3364799"
  },
  {
    "text": "fantastic talk and it's great to see uh really the interdisciplinary core of",
    "start": "3364799",
    "end": "3370319"
  },
  {
    "text": "the work that you're doing",
    "start": "3370319",
    "end": "3374040"
  },
  {
    "text": "you",
    "start": "3376000",
    "end": "3378079"
  }
]