[
  {
    "start": "0",
    "end": "5360"
  },
  {
    "text": "Hello, everyone. Welcome back to CS224N. And today, I'm delighted to\nintroduce our final guest",
    "start": "5360",
    "end": "12559"
  },
  {
    "text": "speaker, Yulia Tsvetkov. So Yulia is\ncurrently a professor at Carnegie Mellon University.",
    "start": "12560",
    "end": "19100"
  },
  {
    "text": "But actually, starting\nfrom next year she's going to be a professor\nat the University of Washington,",
    "start": "19100",
    "end": "24440"
  },
  {
    "text": "that you can already see\nupdated in her email address. Yulia's research focuses on\nextending the capabilities",
    "start": "24440",
    "end": "31609"
  },
  {
    "text": "of human language technology\nbeyond individual cultures and across language\nboundaries, so",
    "start": "31610",
    "end": "37219"
  },
  {
    "text": "lots of work that\nconsiders the roles of human beings in different\nmultilingual situations.",
    "start": "37220",
    "end": "43739"
  },
  {
    "text": "And today, she's going\nto be giving a talk to us on \"Social & Ethical\nConsiderations in NLP Systems.\"",
    "start": "43740",
    "end": "50810"
  },
  {
    "text": "Just one more note on the\nway things are going to run-- so Yulia has some\ninteractive exercises.",
    "start": "50810",
    "end": "57300"
  },
  {
    "text": "So what we're going to do is\nfor the interactive exercises, you'll be asked to put something\ninto the Zoom comments--",
    "start": "57300",
    "end": "66955"
  },
  {
    "text": "sorry, the Zoom chats. That means really\nusing the chat. And you might want to set\nwho to the chat is to,",
    "start": "66955",
    "end": "73880"
  },
  {
    "text": "I think it's by default\npanelists, which is OK, or to Yulia. So if it goes to the\npanelists, it's good.",
    "start": "73880",
    "end": "79190"
  },
  {
    "text": "But probably not all\nattendees, because that would be a bit overwhelming. And then if you have\nquestions, put them in the Q&A",
    "start": "79190",
    "end": "87260"
  },
  {
    "text": "as usual, because that'll\nkeep the two streams separate. And as for our other\ninvited lecturers,",
    "start": "87260",
    "end": "94880"
  },
  {
    "text": "if you've got some\nquestions that you'd like to ask Yulia at the\nend, stay on the line",
    "start": "94880",
    "end": "100430"
  },
  {
    "text": "and raise your hand. And we can promote people\nto be panelists and have a chat with Yulia.",
    "start": "100430",
    "end": "105590"
  },
  {
    "text": "OK, so without further ado, I'm\ndelighted to have you, Yulia. Thank you very much, Chris.",
    "start": "105590",
    "end": "111140"
  },
  {
    "text": "I'm very excited to speak\nto you all today despite-- unfortunately, I cannot see you.",
    "start": "111140",
    "end": "116690"
  },
  {
    "text": "But I'm excited.  So this lecture is\nstructured as follows.",
    "start": "116690",
    "end": "124680"
  },
  {
    "text": "We'll have three parts. The first part will be\nprimarily a discussion",
    "start": "124680",
    "end": "129889"
  },
  {
    "text": "in which I will ask questions. It's supposed to be interactive. But I realize we are\nvery limited in ways",
    "start": "129889",
    "end": "136730"
  },
  {
    "text": "we can interact now. So this is when please\nput the responses, if you",
    "start": "136730",
    "end": "142130"
  },
  {
    "text": "want, in the chat window. And I will answer\nmy own questions following also your\nresponses, and maybe",
    "start": "142130",
    "end": "148460"
  },
  {
    "text": "read some of your responses. So this will be the first part. And the goal of this\npart is to provide you",
    "start": "148460",
    "end": "153770"
  },
  {
    "text": "some practical tools. When you have a new\nproblem to work on in the AI in your\nfield, how would you",
    "start": "153770",
    "end": "161720"
  },
  {
    "text": "assess this problem in terms of\nhow ethical it is to solve it? What kind of biases does\nit incorporate, and so on?",
    "start": "161720",
    "end": "169850"
  },
  {
    "text": "So in the second\npart, I will try to generalize, to\ngive a review of what are overall topics in the\nintersection of ethics and NLP,",
    "start": "169850",
    "end": "179980"
  },
  {
    "text": "because it's actually\na very big field. And what I will talk about today\nis just a motivational lecture.",
    "start": "179980",
    "end": "186190"
  },
  {
    "text": "But there is a lot of\ninteresting technical content and a lot of subfields\nof this field.",
    "start": "186190",
    "end": "192610"
  },
  {
    "text": "And I will dive a little deeper\nin one topic in this field, specifically focusing\non algorithmic bias.",
    "start": "192610",
    "end": "200530"
  },
  {
    "text": "And if time is left,\nwhich I am not sure about, I will talk about\none or two projects",
    "start": "200530",
    "end": "206560"
  },
  {
    "text": "in my lab, so specific\nresearch projects. But if we don't have\ntime to cover it,",
    "start": "206560",
    "end": "211900"
  },
  {
    "text": "then you can always\nread the paper. So the first two parts are\nmore important for the purpose",
    "start": "211900",
    "end": "217660"
  },
  {
    "text": "of this lecture. So let's start.",
    "start": "217660",
    "end": "222870"
  },
  {
    "text": "As far as I understand,\nthis is a course on deep learning and\nnatural language processing. So you've probably covered\nvarious deep learning",
    "start": "222870",
    "end": "230640"
  },
  {
    "text": "architectures and their\napplications to various NLP tasks, like machine\ntranslation, dialogue systems,",
    "start": "230640",
    "end": "236340"
  },
  {
    "text": "question answering. And there is an\nobvious question. What does it all has\nto do with ethics?",
    "start": "236340",
    "end": "243780"
  },
  {
    "text": "What does syntactic parsing\nor part of speech tagging has to do with ethics?",
    "start": "243780",
    "end": "248890"
  },
  {
    "text": "And the answer which\nI want to suggest is this quote, that--\nit's a simple answer,",
    "start": "248890",
    "end": "258049"
  },
  {
    "text": "that the common misconception\nis that language has to do with words.",
    "start": "258050",
    "end": "263540"
  },
  {
    "text": "But it doesn't. It has to do with people. So every word, every\nsentence that we produce,",
    "start": "263540",
    "end": "271169"
  },
  {
    "text": "language is produced by people. It is directed\ntowards other people. And everything that\nis related to language",
    "start": "271170",
    "end": "277889"
  },
  {
    "text": "necessarily involves people. And it has social meaning and\nincorporates human biases.",
    "start": "277890",
    "end": "284610"
  },
  {
    "text": "And this is why,\nalso, models that we build which will be\nused by other people",
    "start": "284610",
    "end": "291660"
  },
  {
    "text": "may incorporate social biases. So this is why decisions that\nwe make about our data, what",
    "start": "291660",
    "end": "299530"
  },
  {
    "text": "kinds of considerations that\nwe incorporate into our model, may have direct impact on\npeople and maybe societies.",
    "start": "299530",
    "end": "310470"
  },
  {
    "text": "And to start this\nlecture, we need to start with understanding\nwhat is ethics.",
    "start": "310470",
    "end": "317790"
  },
  {
    "text": "So what is ethics? Here is a definition from\na textbook on ethics. \"Ethics is a study of\nwhat are good and bad",
    "start": "317790",
    "end": "324210"
  },
  {
    "text": "ends to pursue in life and\nwhat is right and wrong to do in the conduct of life.\"",
    "start": "324210",
    "end": "330599"
  },
  {
    "text": "So it is a practical discipline. And the primary goal is to\ndetermine how one ought to live",
    "start": "330600",
    "end": "337470"
  },
  {
    "text": "and what actions one ought to\ndo in the conduct of one's life.",
    "start": "337470",
    "end": "342920"
  },
  {
    "text": "So to summarize, it\nis very practical. And it's simple. It's just doing the good things\nand doing the right things.",
    "start": "342920",
    "end": "352620"
  },
  {
    "text": "Then my question to\nyou is, how simple it is to define what is\ngood and what is right?",
    "start": "352620",
    "end": "360300"
  },
  {
    "text": "So let's start discussion by\ndiving into various problems.",
    "start": "360300",
    "end": "365460"
  },
  {
    "text": "And we start with a\nboring theoretical problem which everybody knows about,\nwhich is the trolley dilemma.",
    "start": "365460",
    "end": "372270"
  },
  {
    "text": "And we won't spend\ntoo much time on it. I am sure all of about it.",
    "start": "372270",
    "end": "378419"
  },
  {
    "text": "So it's a classical problem\nin ethics in which--",
    "start": "378420",
    "end": "384270"
  },
  {
    "text": "so this is you,\nstanding near the lever. And here is a trolley coming.",
    "start": "384270",
    "end": "389340"
  },
  {
    "text": "And there are several people. So the trolley cannot\nsee the people. And the people cannot\nsee the trolley.",
    "start": "389340",
    "end": "394410"
  },
  {
    "text": "And you are the only one\nin control, in charge. You can save people.",
    "start": "394410",
    "end": "401040"
  },
  {
    "text": "And maybe you need to make\ndecisions about people's life. And you ask yourself, why me?",
    "start": "401040",
    "end": "407640"
  },
  {
    "text": "But the point here\nis that imagine that there are five\npeople on one side",
    "start": "407640",
    "end": "416400"
  },
  {
    "text": "and then no one\non the other side. And then I would\nask you, would you pull the lever to save five\npeople if the trolley is",
    "start": "416400",
    "end": "423210"
  },
  {
    "text": "supposed to go straight? And if I would ask\nyou interactively,",
    "start": "423210",
    "end": "433490"
  },
  {
    "text": "everybody would say yes,\nI will pull the lever. And then I will follow up\nwith the next question.",
    "start": "433490",
    "end": "439470"
  },
  {
    "text": "OK, what about if five people\non one side and only one person on the other side? So would you pull the lever to\nminimize the number of lives",
    "start": "439470",
    "end": "448640"
  },
  {
    "text": "that will be sacrificed? And some people will not answer. Some people will say yes.",
    "start": "448640",
    "end": "454610"
  },
  {
    "text": "Some people will say no. And those who will say\nyes, I will ask them, what if this one person is your\nbrother and on the other side",
    "start": "454610",
    "end": "462110"
  },
  {
    "text": "just five random people? What would be your answer? And I can go on and\non and on to make",
    "start": "462110",
    "end": "468650"
  },
  {
    "text": "this problem harder and harder. And as you can\nimagine, there are--",
    "start": "468650",
    "end": "475820"
  },
  {
    "text": "the answers are\ndifficult. And also, we don't know what the answer will\nbe in the actual situation.",
    "start": "475820",
    "end": "483850"
  },
  {
    "text": "And while this problem\nis theoretical, it is in part\nbecoming relevant now",
    "start": "483850",
    "end": "489300"
  },
  {
    "text": "when we talk about\nself-driving cars. So I am now moving\ncloser to the topics",
    "start": "489300",
    "end": "496850"
  },
  {
    "text": "that we will discuss today. And I want to introduce\na new problem which I call the chicken dilemma.",
    "start": "496850",
    "end": "504349"
  },
  {
    "text": "So in this dilemma,\nlet's train a classifier.",
    "start": "504350",
    "end": "510030"
  },
  {
    "text": "And this will be a\nsimple CNN classifier. And the input to the\nclassifier is an egg.",
    "start": "510030",
    "end": "519839"
  },
  {
    "text": "And the classifier needs\nto define the gender of the chicken, of the chick.",
    "start": "519840",
    "end": "525990"
  },
  {
    "text": "And that decides if it's a hen,\nit will go to egg laying farm.",
    "start": "525990",
    "end": "531750"
  },
  {
    "text": "And if it's a rooster, it\nwill go to a meat farm.",
    "start": "531750",
    "end": "537810"
  },
  {
    "text": "So first of all,\ndo you think you can build such a classifier? I'm sure every\nstudent in this course",
    "start": "537810",
    "end": "545310"
  },
  {
    "text": "will easily build\nthis classifier. And I'm sure it will have\na quite good accuracy.",
    "start": "545310",
    "end": "551325"
  },
  {
    "text": "And then the question to you\nis, do you think it is ethical? ",
    "start": "551325",
    "end": "556410"
  },
  {
    "text": "And I invite you to type\nyour responses in the chat. ",
    "start": "556410",
    "end": "567870"
  },
  {
    "text": "Yes, no. I mean, you can\njustify a little bit. ",
    "start": "567870",
    "end": "574900"
  },
  {
    "text": "Thank you. Thank you for participating. So could you repeat\nthe question? So the question is,\nthere is an egg.",
    "start": "574900",
    "end": "582470"
  },
  {
    "text": "You need to determine\nthe gender of the chick. And if it's a rooster, it\nwill go to a meat farm.",
    "start": "582470",
    "end": "588149"
  },
  {
    "text": "And if it's a hen, it will\ngo to an egg laying farm. And the question\nis, is this ethical?",
    "start": "588150",
    "end": "596970"
  },
  {
    "text": "So there are all\nkinds of responses. Let's see. So yes and no-- but you can use the\nexact same thing",
    "start": "596970",
    "end": "605070"
  },
  {
    "text": "to target ethnic groups\ninstead of chickens. So, yes. Thank you.",
    "start": "605070",
    "end": "610350"
  },
  {
    "text": "And I see there are many\ninteresting responses here. Just the amount of responses,\nI cannot even have time to read",
    "start": "610350",
    "end": "619200"
  },
  {
    "text": "them because-- so anyway, based\non this question,",
    "start": "619200",
    "end": "626230"
  },
  {
    "text": "I can tell you what\nmy thoughts are. So as a vegetarian, I\nmaybe think it's unethical.",
    "start": "626230",
    "end": "633490"
  },
  {
    "text": "But as a mother, I actually\nwant my kids to eat meat. And whether I think\nit's ethical or not,",
    "start": "633490",
    "end": "639850"
  },
  {
    "text": "we are doing this anyway today. And there are all kinds of\nconsiderations, pro and cons.",
    "start": "639850",
    "end": "646569"
  },
  {
    "text": "For example, this is what\nalready is done today. And then maybe such\nclassifier will minimize",
    "start": "646570",
    "end": "651940"
  },
  {
    "text": "the suffering of the animal. But on the other hand, we hope\nthat in the future society,",
    "start": "651940",
    "end": "660170"
  },
  {
    "text": "the life of a chicken\nwill be as valuable as the life of a person. And I can continue on and on.",
    "start": "660170",
    "end": "666740"
  },
  {
    "text": "But from this example-- I also don't want to\nstay on it too long-- you can see that the\nquestions of ethics",
    "start": "666740",
    "end": "674149"
  },
  {
    "text": "are difficult, that whether-- you don't know too\nmuch about this field.",
    "start": "674150",
    "end": "682160"
  },
  {
    "text": "But you can feel what\nis the right answer. So ethics is inner-guiding.",
    "start": "682160",
    "end": "688700"
  },
  {
    "text": "It's moral principles. And there are often\nno easy answers.",
    "start": "688700",
    "end": "694980"
  },
  {
    "text": "So there are many gray areas. And importantly, ethics\nchanges over time",
    "start": "694980",
    "end": "701060"
  },
  {
    "text": "with the values and\nbeliefs of people. So whatever we discuss\ntoday, we can think it's ethical or not ethical.",
    "start": "701060",
    "end": "707750"
  },
  {
    "text": "But it may change\nin a hundred years. And maybe hundred\nyears ago, this",
    "start": "707750",
    "end": "712760"
  },
  {
    "text": "would not even be a question\nwhy this would be unethical. And another important\npoint is that this",
    "start": "712760",
    "end": "721670"
  },
  {
    "text": "is what we are doing today. So what is ethical\nis what is legal, is not necessarily aligned.",
    "start": "721670",
    "end": "727670"
  },
  {
    "text": "We can do legal things that\nwill still be unethical. And now having\nthis pimer, I want",
    "start": "727670",
    "end": "734030"
  },
  {
    "text": "to move to the actual\nproblems, the actual problems that we can kind of be asked\nto build and decide whether we",
    "start": "734030",
    "end": "741950"
  },
  {
    "text": "want to build them or not. And the way I will\nguide this discussion is I will ask you\nspecific questions.",
    "start": "741950",
    "end": "748580"
  },
  {
    "text": "And I will ask you\nfor your answers. And I realize it's\nvery difficult to read the chat, the specific answers.",
    "start": "748580",
    "end": "754400"
  },
  {
    "text": "But the point is that\nthe types of questions that I will ask you, these\nwould be the questions",
    "start": "754400",
    "end": "759800"
  },
  {
    "text": "that you could ask\nyourself when you need to build the technology. And maybe the question whether\nsomething is ethical or not",
    "start": "759800",
    "end": "766670"
  },
  {
    "text": "is a difficult question. But let's try to\nbreak down an analysis of a specific application\nof a specific model",
    "start": "766670",
    "end": "774950"
  },
  {
    "text": "to kind of derive\nan answer which will give us some tools to\nderive an answer in an easier",
    "start": "774950",
    "end": "782720"
  },
  {
    "text": "way.  So here is the next classifier\nthat we want to build.",
    "start": "782720",
    "end": "790740"
  },
  {
    "text": "We want to build\nan IQ classifier. So we will be talking about\npredictive technology.",
    "start": "790740",
    "end": "797400"
  },
  {
    "text": "So based on people's\npersonal data-- for example, facial images.",
    "start": "797400",
    "end": "802589"
  },
  {
    "text": "And maybe we can collect\nthe texts of these people on social media.",
    "start": "802590",
    "end": "809010"
  },
  {
    "text": "Let's predict the\nIQ of the person. So if you don't\nknow what IQ is, IQ",
    "start": "809010",
    "end": "817529"
  },
  {
    "text": "is a general capacity\nof an individual",
    "start": "817530",
    "end": "823630"
  },
  {
    "text": "to consciously adjust their\nthinking to new requirements. So it's basically how\nintelligent a person is.",
    "start": "823630",
    "end": "832420"
  },
  {
    "text": "So this is already not\na hypothetical problem. You can collect\nindividuals' data.",
    "start": "832420",
    "end": "838040"
  },
  {
    "text": "You can collect\ntheir texts online. And you collect it. And you can collect training\ndata to predict people's IQ.",
    "start": "838040",
    "end": "845920"
  },
  {
    "text": "But when I will ask you, is this\nan ethical question or not, it might be a difficult question\nto answer immediately.",
    "start": "845920",
    "end": "853311"
  },
  {
    "text": " Thank you very much\nfor participating. I really appreciate it.",
    "start": "853311",
    "end": "858788"
  },
  {
    "text": "I hope I can save this chat\nlater to read your answers. ",
    "start": "858788",
    "end": "871070"
  },
  {
    "text": "OK, so let's start with\nthe first question. ",
    "start": "871070",
    "end": "876490"
  },
  {
    "text": "We need to predict people's\nIQ from their photos and text. And then the first\nquestion that if I",
    "start": "876490",
    "end": "883210"
  },
  {
    "text": "ask you, is it ethical\nor not, I don't know. And then you can\nask yourself, first,",
    "start": "883210",
    "end": "889000"
  },
  {
    "text": "who would benefit from\nsuch a technology? So can you think who would\nbenefit from a technology that",
    "start": "889000",
    "end": "895720"
  },
  {
    "text": "predicts an IQ of a person? ",
    "start": "895720",
    "end": "902960"
  },
  {
    "text": "Hiring.  Right. Employers, schools,\nuniversities--",
    "start": "902960",
    "end": "910170"
  },
  {
    "text": "so it's your answers. Right. So overall, it can be a\nuseful technology, right?",
    "start": "910170",
    "end": "917520"
  },
  {
    "text": "Immigration Services can\nbenefit from it, right, and invite only smart people to\nimmigrate to a country, right?",
    "start": "917520",
    "end": "927450"
  },
  {
    "text": "Even individuals with high IQ\ncan benefit from this, right, because they would maybe\nnot need to do GRE and SAT.",
    "start": "927450",
    "end": "936420"
  },
  {
    "text": "They would not need\nto write essays. They would just need\nto show their IQ.",
    "start": "936420",
    "end": "942800"
  },
  {
    "text": "OK, so this technology\ncan potentially be useful. And then the next\nquestion is, let's assume",
    "start": "942800",
    "end": "952100"
  },
  {
    "text": "we can build such a technology. I will show you later\nthat we actually cannot. But even if we can\nbuild such a technology,",
    "start": "952100",
    "end": "959750"
  },
  {
    "text": "let's think about corner\ncases and understand who can be harmed by this technology.",
    "start": "959750",
    "end": "966420"
  },
  {
    "text": "So basically, what is the\npotential for dual use? How this technology\ncan be misused?",
    "start": "966420",
    "end": "972680"
  },
  {
    "text": "So assume that the classifier is\n100% accurate now for a second. And please think about it.",
    "start": "972680",
    "end": "979540"
  },
  {
    "text": "And type who do you think can\nbe harmed from such a classifier and how this classifier\ncan be misused.",
    "start": "979540",
    "end": "986470"
  },
  {
    "start": "986470",
    "end": "998920"
  },
  {
    "text": "Right. So I can see your answers. And I wish we can\nhave this interactive.",
    "start": "998920",
    "end": "1006750"
  },
  {
    "text": "But I can try to summarize\nwhat I have read so far. So first of all, one of\nyou wrote that IQ is--",
    "start": "1006750",
    "end": "1018170"
  },
  {
    "text": " let me just answer my\nquestion because it's difficult to summarize the chat.",
    "start": "1018170",
    "end": "1025170"
  },
  {
    "text": "The interactive\nfeature is difficult. So I would think\nabout it in this way.",
    "start": "1025170",
    "end": "1034270"
  },
  {
    "text": "First of all, why would we want\nto build such a classifier? So to build a classifier to\npredict an IQ, companies,",
    "start": "1034270",
    "end": "1044209"
  },
  {
    "text": "universities, they don't\nreally need to know your IQ. What they're trying to predict\nis your future success,",
    "start": "1044210",
    "end": "1053710"
  },
  {
    "text": "the way you will succeed\nin a job or the way you will study at school. And then the question is,\nis IQ is the right proxy",
    "start": "1053710",
    "end": "1063180"
  },
  {
    "text": "for future success? And the answer is, no.",
    "start": "1063180",
    "end": "1068400"
  },
  {
    "text": "IQ correlates with\nfuture success. But it's not necessarily the\nright proxy for future success.",
    "start": "1068400",
    "end": "1075190"
  },
  {
    "text": "And then, who are people\nwho could be harmed? For example, people who have\nlower IQ but very hard-working",
    "start": "1075190",
    "end": "1082770"
  },
  {
    "text": "people, people who have lower\nIQ but have good soft skills--",
    "start": "1082770",
    "end": "1089850"
  },
  {
    "text": "so assuming that,\nfirst of all, the IQ",
    "start": "1089850",
    "end": "1095850"
  },
  {
    "text": "is a proxy of future success\nis a incorrect, biased proxy.",
    "start": "1095850",
    "end": "1101220"
  },
  {
    "text": "And this kind of problem of\nusing a proxy to the actual label-- because we cannot\nhave the actual label,",
    "start": "1101220",
    "end": "1108690"
  },
  {
    "text": "the future success. We cannot have this label. It's actually very\ncommon in other types",
    "start": "1108690",
    "end": "1114390"
  },
  {
    "text": "of predictive technology. If you think about\nparole decisions, technology that decides\non parole decisions, what",
    "start": "1114390",
    "end": "1122400"
  },
  {
    "text": "they want to predict is\nwhether the individual will recommit the crime. But this is a label that\nis very hard to obtain.",
    "start": "1122400",
    "end": "1129630"
  },
  {
    "text": "And this is why they might\nresort to another label, whether this individual will\nbe convicted of a crime again,",
    "start": "1129630",
    "end": "1141040"
  },
  {
    "text": "and build this technology to\npredict future conviction. But conviction of a\ncrime is a biased proxy",
    "start": "1141040",
    "end": "1147730"
  },
  {
    "text": "of the actual objective\nthat we want to have, the likelihood to\nmake the crime.",
    "start": "1147730",
    "end": "1154930"
  },
  {
    "text": "And this is one example for\nbiased proxy, which does not [INAUDIBLE] us to build the\nright application for the goals",
    "start": "1154930",
    "end": "1162730"
  },
  {
    "text": "that we have. So this is one problem. The second problem\nis IQ test itself.",
    "start": "1162730",
    "end": "1169980"
  },
  {
    "text": "It is a biased test. So actually, we cannot\nbuild an accurate classifier",
    "start": "1169980",
    "end": "1178230"
  },
  {
    "text": "for the right IQ.",
    "start": "1178230",
    "end": "1184190"
  },
  {
    "text": "And also, if we look at the\ndata that we use, this data--",
    "start": "1184190",
    "end": "1190250"
  },
  {
    "text": "picture, photos\non social media-- this data is biased itself.",
    "start": "1190250",
    "end": "1195840"
  },
  {
    "text": "So there are all\nkinds of biases, because of which we cannot\nactually build the right model.",
    "start": "1195840",
    "end": "1201210"
  },
  {
    "text": "And this is why this classifier\nwill not be 100% accurate. But there will be many\nindividuals who can be harmed.",
    "start": "1201210",
    "end": "1209110"
  },
  {
    "text": "And then there\nwill be questions. For example, assume our\nclassifier that we built",
    "start": "1209110",
    "end": "1218670"
  },
  {
    "text": "is not actually accurate. But it has high accuracy--\nfor example, 90% or 95%.",
    "start": "1218670",
    "end": "1227400"
  },
  {
    "text": "And then I would ask you, is\n95% a good accuracy, or 99%",
    "start": "1227400",
    "end": "1233860"
  },
  {
    "text": "a good accuracy? And then the questions to\nthink about is whether--",
    "start": "1233860",
    "end": "1242620"
  },
  {
    "text": "what would happen with\nmisclassification? What would be an impact\non individual lives",
    "start": "1242620",
    "end": "1249310"
  },
  {
    "text": "if the classifier\nmakes mistakes? And in this case,\nthe important point is that the cost of\nmisclassification is very high.",
    "start": "1249310",
    "end": "1260470"
  },
  {
    "text": "It has effect on people's lives. So accuracy may be not\nthe right evaluation",
    "start": "1260470",
    "end": "1266350"
  },
  {
    "text": "measure for this classifier. And another question\nthat I could ask",
    "start": "1266350",
    "end": "1271840"
  },
  {
    "text": "is, for example, this\ncondition on this line, that we find out that white\nfemales have 99% accuracy,",
    "start": "1271840",
    "end": "1281590"
  },
  {
    "text": "but people with blond hair under\nage 25 have only 60% accuracy.",
    "start": "1281590",
    "end": "1287779"
  },
  {
    "text": "So what does it tell you\nabout this classifier? ",
    "start": "1287780",
    "end": "1304130"
  },
  {
    "text": "Right. So the data set\nitself is biased. This means,\nbasically, that people",
    "start": "1304130",
    "end": "1311029"
  },
  {
    "text": "with blond hair\nunder the age of 25 are underrepresented\nin your data set.",
    "start": "1311030",
    "end": "1317610"
  },
  {
    "text": "So there are all\nkinds of questions and all kinds of\nprobing questions that you can ask about the\nclassifier to understand,",
    "start": "1317610",
    "end": "1324180"
  },
  {
    "text": "is this the right\nproblem to solve? Who can be harmed? Am I optimizing towards\nthe right objective?",
    "start": "1324180",
    "end": "1331110"
  },
  {
    "text": "Is my data biased? And what is the cost\nof misclassification?",
    "start": "1331110",
    "end": "1336510"
  },
  {
    "text": "How do I assess the\npotential for dual use and how much harm\nthis technology can",
    "start": "1336510",
    "end": "1341580"
  },
  {
    "text": "bring, in addition to\nhow useful it can be?",
    "start": "1341580",
    "end": "1346620"
  },
  {
    "text": "And then one last\nquestion, which is a hard one, I want to\nask you who is responsible?",
    "start": "1346620",
    "end": "1352580"
  },
  {
    "text": "So I'm your manager in Google. And you're working in a country. And I ask you, please\nbuild an IQ classifier.",
    "start": "1352580",
    "end": "1360120"
  },
  {
    "text": "And you build an IQ classifier. And you publish a paper\nabout the IQ classifier. And this paper is\npublicized on media.",
    "start": "1360120",
    "end": "1368580"
  },
  {
    "text": "So then the question\nis, who is responsible? Is it the researcher\nor developer? Is it the manager?",
    "start": "1368580",
    "end": "1374490"
  },
  {
    "text": "Is it the reviewer who\ndidn't catch the problems with IQ classifier? Is it university or company?",
    "start": "1374490",
    "end": "1380970"
  },
  {
    "text": "Or is it society? ",
    "start": "1380970",
    "end": "1387029"
  },
  {
    "text": "Yeah. So there is one nice\nanswer that I want to read, is that all of us\nshould be responsible.",
    "start": "1387030",
    "end": "1393770"
  },
  {
    "text": "So in practice, there\nis very little awareness about understanding what\nproblems are ethical or not.",
    "start": "1393770",
    "end": "1400290"
  },
  {
    "text": "And there is no clear\npolicies here, right? This is a complicated issue.",
    "start": "1400290",
    "end": "1405410"
  },
  {
    "text": "And it's not clear\nwho is responsible. This is why assuming\nthat whoever",
    "start": "1405410",
    "end": "1410870"
  },
  {
    "text": "is aware of such dangers\nshould be responsible. So I don't know what is the\nright answer to this question.",
    "start": "1410870",
    "end": "1417585"
  },
  {
    "text": " So now, what is the difference\nbetween the chicken quantifier",
    "start": "1417585",
    "end": "1422730"
  },
  {
    "text": "and the IQ classifier? ",
    "start": "1422730",
    "end": "1434920"
  },
  {
    "text": "Right. So one of your answers is that\none affects people and one does",
    "start": "1434920",
    "end": "1440930"
  },
  {
    "text": "not, right? While chicken classifier\nactually affects chicken's lives and the IQ\nclassifier will not kill",
    "start": "1440930",
    "end": "1447890"
  },
  {
    "text": "anyone-- it can harm,\nbut will not kill-- we do feel that IQ\nclassifier currently can",
    "start": "1447890",
    "end": "1456250"
  },
  {
    "text": "have potentially worse impacts. So AI systems are\npervasive in our world.",
    "start": "1456250",
    "end": "1463130"
  },
  {
    "text": "And the question about\nethics are specifically raised commonly about\npeople-centered AI systems.",
    "start": "1463130",
    "end": "1470800"
  },
  {
    "text": "And these systems\nare really pervasive. So they interact with people\nlike conversational agents.",
    "start": "1470800",
    "end": "1477160"
  },
  {
    "text": "They reason about people,\nsuch as profiling applications",
    "start": "1477160",
    "end": "1482320"
  },
  {
    "text": "or recommendation systems. They affect people\nin other lives, like parole decision\napplications that I mentioned,",
    "start": "1482320",
    "end": "1490540"
  },
  {
    "text": "face recognition,\nvoice recognition. All of these, actually,\nhave this component",
    "start": "1490540",
    "end": "1496000"
  },
  {
    "text": "of predictive technology and\nthe human-centered technology. And this is why ethics\nis critical here.",
    "start": "1496000",
    "end": "1504410"
  },
  {
    "text": "So I want to move\nto a next study. The next study is a\nstudy of detecting--",
    "start": "1504410",
    "end": "1512320"
  },
  {
    "text": " so we, again,\nbuild a classifier.",
    "start": "1512320",
    "end": "1518600"
  },
  {
    "text": "And we want to\nidentify the ability to accurately identify\none's sexual orientation",
    "start": "1518600",
    "end": "1525830"
  },
  {
    "text": "from mere observation. So this study is\ncalled \"AI Gaydar.\"",
    "start": "1525830",
    "end": "1531980"
  },
  {
    "text": "And there are, as I mentioned,\nmany similar studies, studies that predict potential\nfor terrorist attacks, studies",
    "start": "1531980",
    "end": "1540529"
  },
  {
    "text": "that predict\npredictive policing. And also, if you heard\nabout Cambridge Analytica,",
    "start": "1540530",
    "end": "1546770"
  },
  {
    "text": "all of them incorporate\nvery similar technology. So let's talk about\nthis \"AI Gaydar\" study.",
    "start": "1546770",
    "end": "1554419"
  },
  {
    "text": "And the goal is to understand,\nagain, what kind of questions we could ask about this study\nand what kind of pitfalls",
    "start": "1554420",
    "end": "1565070"
  },
  {
    "text": "we could prevent if we\nwould ask these questions. So to summarize the study,\nthe research question",
    "start": "1565070",
    "end": "1573799"
  },
  {
    "text": "is, we need to identify\nthe sexual orientation from people's images.",
    "start": "1573800",
    "end": "1580880"
  },
  {
    "text": "And the data\ncollection process is that we can download photos\nfrom a popular American dating",
    "start": "1580880",
    "end": "1587510"
  },
  {
    "text": "website. And there are 35,000 pictures,\nall white, equal representation",
    "start": "1587510",
    "end": "1600050"
  },
  {
    "text": "for gay and straight,\nfor male and female. Everybody is represented evenly.",
    "start": "1600050",
    "end": "1605090"
  },
  {
    "text": " The method that was used\nis a deep learning model",
    "start": "1605090",
    "end": "1611670"
  },
  {
    "text": "to extract facial features\nand grooming features, and then a logistic\nregression classifier",
    "start": "1611670",
    "end": "1617159"
  },
  {
    "text": "applied to classify the\nfinal label, gay or straight.",
    "start": "1617160",
    "end": "1622450"
  },
  {
    "text": "And the accuracy of this\nclassifier is 81% for men and 74% for women.",
    "start": "1622450",
    "end": "1628750"
  },
  {
    "text": "So this is a summary\nof the study. And I see rightfully, you\nask questions, why would",
    "start": "1628750",
    "end": "1635130"
  },
  {
    "text": "we ever need such an AI system? This is a good question. But I don't want to\npublicize this study",
    "start": "1635130",
    "end": "1644160"
  },
  {
    "text": "or disparage a\nspecific researcher. But this is a good\nstudy to present as an example of\nwhat could go wrong",
    "start": "1644160",
    "end": "1650760"
  },
  {
    "text": "at all levels of the study. So this is why I'm\ndiscussing it now.",
    "start": "1650760",
    "end": "1656220"
  },
  {
    "text": "So what went wrong here? So let's start with an ethics\nof the research question.",
    "start": "1656220",
    "end": "1663400"
  },
  {
    "text": "So is it ethical at all to\npredict sexual orientation",
    "start": "1663400",
    "end": "1669030"
  },
  {
    "text": "from facial-- from any kind of features?",
    "start": "1669030",
    "end": "1674160"
  },
  {
    "text": "And I see a lot of comments. And thank you for the comments.",
    "start": "1674160",
    "end": "1679830"
  },
  {
    "text": "So first of all, this is\nnot a new research question. ",
    "start": "1679830",
    "end": "1685529"
  },
  {
    "text": "From 19th century, there\nwere multiple studies to correlate sexual identity\nwith some external features.",
    "start": "1685530",
    "end": "1693870"
  },
  {
    "text": "And then it was genetics. People were looking for gay\ngenes, gay brains, gay ring",
    "start": "1693870",
    "end": "1699210"
  },
  {
    "text": "fingers, and so on. So maybe moving from 19th\ncentury to 21 century,",
    "start": "1699210",
    "end": "1709530"
  },
  {
    "text": "we can again ask, who can\nbenefit from such a classifier?",
    "start": "1709530",
    "end": "1714990"
  },
  {
    "text": "And who can be harmed\nby such a classifier? So what do you think? Who could benefit from\nsuch a classifier?",
    "start": "1714990",
    "end": "1722430"
  },
  {
    "start": "1722430",
    "end": "1729100"
  },
  {
    "text": "So autocratic\ngovernments, right. ",
    "start": "1729100",
    "end": "1736090"
  },
  {
    "text": "But also maybe dating\napps, advertisers, conservative religious\ngroups, and so on.",
    "start": "1736090",
    "end": "1742910"
  },
  {
    "text": "So we could think, who would\nwant to use such a classifier? ",
    "start": "1742910",
    "end": "1749830"
  },
  {
    "text": "Then, maybe who can be\nharmed by such a classifier? ",
    "start": "1749830",
    "end": "1757030"
  },
  {
    "text": "Now again, assuming-- we are\nnot thinking if it's possible at all build such a classifier. And as you can guess, we will\nsee that it's not possible.",
    "start": "1757030",
    "end": "1765950"
  },
  {
    "text": "But what would stop you from\nbuilding such a classifier? What do you think could be\nharmful in this classifier?",
    "start": "1765950",
    "end": "1774220"
  },
  {
    "start": "1774220",
    "end": "1782760"
  },
  {
    "text": "So yeah, thank you\nfor your answers.",
    "start": "1782760",
    "end": "1788430"
  },
  {
    "text": "I will summarize them. So many people can be\nharmed by such classifier.",
    "start": "1788430",
    "end": "1797040"
  },
  {
    "text": "And I summarized, basically,\nmany of your answers here on this slide. So this potentially can\nbe a dangerous technology.",
    "start": "1797040",
    "end": "1805059"
  },
  {
    "text": "So in many countries, being gay\nperson is prosecutable by law.",
    "start": "1805060",
    "end": "1810360"
  },
  {
    "text": "And it can lead even\nto death penalty. It might affect people's\nemployment, relationships,",
    "start": "1810360",
    "end": "1816059"
  },
  {
    "text": "health opportunities, right? Importantly, this is not only\nabout sexual orientation.",
    "start": "1816060",
    "end": "1825330"
  },
  {
    "text": "So there are many attributes,\nincluding sexual identity,",
    "start": "1825330",
    "end": "1830510"
  },
  {
    "text": "that are private for people,\nthat are protected attributes.",
    "start": "1830510",
    "end": "1836060"
  },
  {
    "text": "And they can be non-binary. They can be intimate and\nnot visible publicly.",
    "start": "1836060",
    "end": "1843260"
  },
  {
    "text": "And most importantly is\nthat these attributes are specifically those\nattributes against which people",
    "start": "1843260",
    "end": "1850820"
  },
  {
    "text": "are discriminated against. And this is why it is\nespecially dangerous to build such a technology.",
    "start": "1850820",
    "end": "1857790"
  },
  {
    "text": "So in the paper, in the\npublished paper, the argument",
    "start": "1857790",
    "end": "1863060"
  },
  {
    "text": "for building this-- for presenting this\nstudy was that this study",
    "start": "1863060",
    "end": "1872450"
  },
  {
    "text": "is an alert how easy it is\nto build such a classifier,",
    "start": "1872450",
    "end": "1879250"
  },
  {
    "text": "and basically an\nalert for to expose the threat to the privacy\nand safety of people.",
    "start": "1879250",
    "end": "1887830"
  },
  {
    "text": "And then I would be\ninterested to hear if you have counter\narguments for this argument.",
    "start": "1887830",
    "end": "1893620"
  },
  {
    "start": "1893620",
    "end": "1908420"
  },
  {
    "text": "OK, so basically, there can\nbe many counterarguments.",
    "start": "1908420",
    "end": "1914220"
  },
  {
    "text": "One of them is that this is a\nclassifier, this technology. So like a knife is a technology.",
    "start": "1914220",
    "end": "1920460"
  },
  {
    "text": "And with a knife,\nyou can kill people and you can cook food, right?",
    "start": "1920460",
    "end": "1925590"
  },
  {
    "text": "And you don't necessarily\nneed to kill people with a knife to expose\nthe dangers and harms",
    "start": "1925590",
    "end": "1933030"
  },
  {
    "text": "of this technology, right? And another issue is\nthat this is actually not",
    "start": "1933030",
    "end": "1940940"
  },
  {
    "text": "possible to build such a\nclassifier with the data that researchers had. And this is what\nwe will see when",
    "start": "1940940",
    "end": "1947660"
  },
  {
    "text": "we will discuss additional\ndetails about the data.",
    "start": "1947660",
    "end": "1953540"
  },
  {
    "text": "And another comment\nis, as I said, this is only one instance\nof such a technology.",
    "start": "1953540",
    "end": "1958940"
  },
  {
    "text": "Here is another instance, which\nis a successful startup called Faception, which has\ndrawn a lot of funding.",
    "start": "1958940",
    "end": "1969409"
  },
  {
    "text": "And its goal is to\nidentify terrorists based on facial features.",
    "start": "1969410",
    "end": "1974450"
  },
  {
    "text": "And unlike in the\nprevious study, this startup doesn't show what\ntechnology they developed.",
    "start": "1974450",
    "end": "1981390"
  },
  {
    "text": "But you can guess that it\ncan have similar dangers. So in general, building\npredictive technology",
    "start": "1981390",
    "end": "1989540"
  },
  {
    "text": "is very pervasive. It's ubiquitous. But it is always-- and sometimes, it's not as\nkind of clear cut unethical.",
    "start": "1989540",
    "end": "2000550"
  },
  {
    "text": "For example, many people\nin NLP published papers",
    "start": "2000550",
    "end": "2006220"
  },
  {
    "text": "on predicting gender\nfrom comments. And it is not clear, basically,\nwhen the technology is clearly",
    "start": "2006220",
    "end": "2015670"
  },
  {
    "text": "harmful and unethical and\nwhere it can be actually used in good ways.",
    "start": "2015670",
    "end": "2021770"
  },
  {
    "text": "For example, we all want\nour research to work, right?",
    "start": "2021770",
    "end": "2027520"
  },
  {
    "text": "And to work well and\nto be personalized, the algorithm actually needs\nto know something about us.",
    "start": "2027520",
    "end": "2034450"
  },
  {
    "text": "So again, this is\nnot an easy question. But in the case\nof the classifier,",
    "start": "2034450",
    "end": "2040500"
  },
  {
    "text": "maybe it's already\non the extreme. OK, let's move to\nthe data, again,",
    "start": "2040500",
    "end": "2050580"
  },
  {
    "text": "to discuss basically\nwhat questions could be asked about the data. So here's how the\ndata was collected.",
    "start": "2050580",
    "end": "2057399"
  },
  {
    "text": "So photos were downloaded\nfrom a popular American dating website. They were public.",
    "start": "2057400",
    "end": "2062638"
  },
  {
    "text": "And there were a few thousands\nof images, all white,",
    "start": "2062639",
    "end": "2069419"
  },
  {
    "text": "and a balanced data set. And my first question is, what\ncan you say about the data?",
    "start": "2069420",
    "end": "2078830"
  },
  {
    "text": "So is it OK to use this data\nif there is no robots to steal, the photos are public.",
    "start": "2078830",
    "end": "2084770"
  },
  {
    "text": "What can be counter arguments\nto using people's photos from a dating website?",
    "start": "2084770",
    "end": "2090320"
  },
  {
    "start": "2090320",
    "end": "2104010"
  },
  {
    "text": "There is a hint on the slide. ",
    "start": "2104010",
    "end": "2112319"
  },
  {
    "text": "Lack of consent. People did not intend\nfor their photos",
    "start": "2112320",
    "end": "2117450"
  },
  {
    "text": "to be used to build\na classifier-- private information. Right. So thank you for your answers.",
    "start": "2117450",
    "end": "2123510"
  },
  {
    "text": "So the points that I\nwanted to emphasize here is that it was legal\nto collect this data.",
    "start": "2123510",
    "end": "2130260"
  },
  {
    "text": "But again, it's not\nclear whether it was ethical to collect this\ndata, because as you said, people did not provide--",
    "start": "2130260",
    "end": "2136890"
  },
  {
    "text": "maybe did not provide-- 35,000 people did not\nprovide consent for using, specifically, this data.",
    "start": "2136890",
    "end": "2143070"
  },
  {
    "text": "And there are more\nimportant global issue here is that there is a difference\nbetween data that is public",
    "start": "2143070",
    "end": "2150120"
  },
  {
    "text": "and data which is publicized. So public, it's fine,\nbecause these people",
    "start": "2150120",
    "end": "2155880"
  },
  {
    "text": "want to be found by\nthe social circle that they are targeting when\nthey publish their photo",
    "start": "2155880",
    "end": "2162120"
  },
  {
    "text": "on the dating website. But this does not\nmean necessarily that they want to be found\nby a broader social circle,",
    "start": "2162120",
    "end": "2169950"
  },
  {
    "text": "by their families, by their\ncolleagues, and so on. So there is a big difference\nbetween the data that is public",
    "start": "2169950",
    "end": "2178859"
  },
  {
    "text": "and the data which\nis publicized. So overall, even if they did\nnot violate terms of service,",
    "start": "2178860",
    "end": "2189280"
  },
  {
    "text": "I don't know about it. I didn't read in\ndepth, actually. But they did violate\nthe social contract",
    "start": "2189280",
    "end": "2197289"
  },
  {
    "text": "because this was not the intent\nof the user for their data to be used in this way.",
    "start": "2197290",
    "end": "2205810"
  },
  {
    "text": "Next question about data-- so what do we think\nabout this data set?",
    "start": "2205810",
    "end": "2211500"
  },
  {
    "text": "It's 35,000 pictures,\nall white, balanced",
    "start": "2211500",
    "end": "2216540"
  },
  {
    "text": "in terms of sexual\norientation and balanced in terms of gender. ",
    "start": "2216540",
    "end": "2226569"
  },
  {
    "text": "It's white.  OK, so does not\nrepresent the population.",
    "start": "2226570",
    "end": "2236890"
  },
  {
    "text": "So, right. So basically, you can\nguess that this dataset. has many, many\nbiases incorporated.",
    "start": "2236890",
    "end": "2243750"
  },
  {
    "text": "It contains only white people,\nonly people who self-disclose their sexual identity.",
    "start": "2243750",
    "end": "2250470"
  },
  {
    "text": "It represents very certain\nsocial groups-- people who put their photos on the\ndating website of specific age",
    "start": "2250470",
    "end": "2258420"
  },
  {
    "text": "of specific ethnicity. And basically, these\nare photos that were carefully selected to\nbe attractive to their target",
    "start": "2258420",
    "end": "2269430"
  },
  {
    "text": "audience. So this data set contains\nmany types of biases.",
    "start": "2269430",
    "end": "2274859"
  },
  {
    "text": "And also, as one\nof you mentioned and as is written on\nthis slide, the data set",
    "start": "2274860",
    "end": "2281010"
  },
  {
    "text": "is balanced, which does not\nrepresent the true distribution in the population.",
    "start": "2281010",
    "end": "2287080"
  },
  {
    "text": "So what does it mean? This means that this model is\nbuilt on a very biased data",
    "start": "2287080",
    "end": "2294390"
  },
  {
    "text": "set. And you as students at Stanford,\nyou understand that it cannot",
    "start": "2294390",
    "end": "2305230"
  },
  {
    "text": "be used, for example, on\na non-white population. It cannot be used on the photos\nof people not from that dating",
    "start": "2305230",
    "end": "2314140"
  },
  {
    "text": "website. We don't actually know what\nthis classifier learned. Maybe the most\nimportant features",
    "start": "2314140",
    "end": "2321160"
  },
  {
    "text": "were the watermark of\nthis specific website. I don't know. Or some other confounds,\nspurious confounds.",
    "start": "2321160",
    "end": "2329859"
  },
  {
    "text": "But the point is that once\nthe classifier is out, those who want to\nuse it maliciously,",
    "start": "2329860",
    "end": "2336550"
  },
  {
    "text": "they don't know that this\ntechnology is actually not applicable for\nany other data set",
    "start": "2336550",
    "end": "2342430"
  },
  {
    "text": "except for this\nspecific data set. So this technology is biased. And it also shows\nthat it's basically",
    "start": "2342430",
    "end": "2351040"
  },
  {
    "text": "not a credible result.",
    "start": "2351040",
    "end": "2356620"
  },
  {
    "text": "OK, so let's move on. And final question is that--",
    "start": "2356620",
    "end": "2362410"
  },
  {
    "text": "this is a deep learning model. It's a black box model. And then there is the question\nof how to analyze errors",
    "start": "2362410",
    "end": "2371069"
  },
  {
    "text": "in the learning model,\nspecifically when we work on such a critical,\nsuch sensitive topics",
    "start": "2371070",
    "end": "2379680"
  },
  {
    "text": "like predictive technology,\nnot necessarily predicting sexual orientation,\nbut for example,",
    "start": "2379680",
    "end": "2385410"
  },
  {
    "text": "predicting gender, which is,\nagain used in many companies?",
    "start": "2385410",
    "end": "2390640"
  },
  {
    "text": "So it is very difficult\nto understand whether it is OK to use this technology.",
    "start": "2390640",
    "end": "2397310"
  },
  {
    "text": "But the point is\nthat we need to be able to analyze it and\nevaluate it properly.",
    "start": "2397310",
    "end": "2405160"
  },
  {
    "text": "And the last point is\nabout the accuracy again.",
    "start": "2405160",
    "end": "2410200"
  },
  {
    "text": "And I'm going back to the\npoints that I mentioned also in the IQ classifier.",
    "start": "2410200",
    "end": "2415630"
  },
  {
    "text": "So the accuracy of this\nclassifier is 81% for men and 74% for women.",
    "start": "2415630",
    "end": "2423670"
  },
  {
    "text": "Is it a good accuracy,\nor is it a bad accuracy? So the numbers are OK for some\ntasks, but not for others.",
    "start": "2423670",
    "end": "2432460"
  },
  {
    "text": "But importantly, for\nthis type of problem, it's important to\nunderstand that the cost",
    "start": "2432460",
    "end": "2439270"
  },
  {
    "text": "of misclassification is\nnot equal to the cost of correct prediction.",
    "start": "2439270",
    "end": "2444829"
  },
  {
    "text": "And here is kind\nof visual examples. So if my algorithm misclassifies\nmy dog as a cookie,",
    "start": "2444830",
    "end": "2456880"
  },
  {
    "text": "is the cost of this\nmisclassification is high or low?",
    "start": "2456880",
    "end": "2463400"
  },
  {
    "text": "So I guess it's\njust funny, right? It's funny. There is nothing offensive here.",
    "start": "2463400",
    "end": "2469690"
  },
  {
    "text": "And then the next question-- if my algorithm\nmisclassifies me with my dog,",
    "start": "2469690",
    "end": "2476690"
  },
  {
    "text": "is the cost of\nmisclassification high or low? It can be funny, but\nmaybe not for everyone.",
    "start": "2476690",
    "end": "2483630"
  },
  {
    "text": "We already don't know. And then the photo that I\ndon't put here but the one",
    "start": "2483630",
    "end": "2488760"
  },
  {
    "text": "that maybe many\nof you have heard is the gorilla incident that\nhappened in Google in 2016.",
    "start": "2488760",
    "end": "2496020"
  },
  {
    "text": "So in this case, there\nwas a misclassification of an African-American\nwoman as a gorilla.",
    "start": "2496020",
    "end": "2503820"
  },
  {
    "text": "And to understand\nhow expensive is the cost of this\nmisclassification, we need to understand the\nwhole history of dehumanization",
    "start": "2503820",
    "end": "2512580"
  },
  {
    "text": "of Black people in\nthe US, and so on. So we can see the difference.",
    "start": "2512580",
    "end": "2519150"
  },
  {
    "text": "For the same algorithm\nand same types of errors, there are different\ntypes of errors that",
    "start": "2519150",
    "end": "2525540"
  },
  {
    "text": "are more expensive than others. So this is why it is\nimportant to assess",
    "start": "2525540",
    "end": "2533390"
  },
  {
    "text": "AI systems adversarially. And now I just want to\nreiterate the types of questions",
    "start": "2533390",
    "end": "2539359"
  },
  {
    "text": "that I ask because these are\nthe kinds of questions that you might want to ask yourself next\ntime when you need to build",
    "start": "2539360",
    "end": "2545690"
  },
  {
    "text": "another predictive technology. And the first is to understand\nthe ethics of the research",
    "start": "2545690",
    "end": "2551240"
  },
  {
    "text": "question. And sometimes it's not\nvery easy to understand. But just ask yourself these\nmore specific questions.",
    "start": "2551240",
    "end": "2560329"
  },
  {
    "text": "If I build this\ntechnology, who could benefit from such a technology,\nand who can be harmed by it?",
    "start": "2560330",
    "end": "2566430"
  },
  {
    "text": "So try to see the corner cases. And also, what about the data? Could sharing this data have\nmajor effect on people's lives,",
    "start": "2566430",
    "end": "2575780"
  },
  {
    "text": "like in the case of\nAI data classifier?",
    "start": "2575780",
    "end": "2581330"
  },
  {
    "text": "The next question that we\ncan ask is about privacy. What did we discuss? Who owns the data?",
    "start": "2581330",
    "end": "2587220"
  },
  {
    "text": "And is this data not only\npublic or legal to use, but also",
    "start": "2587220",
    "end": "2592280"
  },
  {
    "text": "are we violating the social\ncircles to which the data is",
    "start": "2592280",
    "end": "2597410"
  },
  {
    "text": "publicized? Are we violating a social\ncontract in the way that the public data\nis expected to be used?",
    "start": "2597410",
    "end": "2605910"
  },
  {
    "text": "And user consent is not\nalways possible to obtain.",
    "start": "2605910",
    "end": "2611609"
  },
  {
    "text": "But we need to understand\nimplicit assumptions of people who put their data online.",
    "start": "2611610",
    "end": "2619290"
  },
  {
    "text": "Now the next question is, what\nare possible biases in data? What are artifacts in data?",
    "start": "2619290",
    "end": "2626220"
  },
  {
    "text": "What are distributions\nfor specific populations and subpopulations?",
    "start": "2626220",
    "end": "2631530"
  },
  {
    "text": "How representative, or what\nkind of misrepresentations are in my data?",
    "start": "2631530",
    "end": "2637950"
  },
  {
    "text": "And next is, what is basically a\npotential bias in these models?",
    "start": "2637950",
    "end": "2644099"
  },
  {
    "text": "When I build this\nmodel, do I control for confounding variables? And do I optimize for\nthe right objective,",
    "start": "2644100",
    "end": "2651390"
  },
  {
    "text": "like in the case of\nthe IQ classifier? And also, if I have biases,\ndoes my system amplify biases?",
    "start": "2651390",
    "end": "2659040"
  },
  {
    "text": "And finally, it is not\nenough to measure accuracy because the semantics of false\npositives and false negatives",
    "start": "2659040",
    "end": "2668789"
  },
  {
    "text": "can be different. Sometimes the cost\nof misclassification is much higher than the\ncost of correct prediction.",
    "start": "2668790",
    "end": "2676270"
  },
  {
    "text": "So I also need to\nunderstand how to evaluate the models properly.",
    "start": "2676270",
    "end": "2683350"
  },
  {
    "text": "And why is it\nespecially relevant now is because, as\nyou all know, there",
    "start": "2683350",
    "end": "2689320"
  },
  {
    "text": "is an exponential growth\nof user-generated data. And it's really easy\nto build the tools.",
    "start": "2689320",
    "end": "2695230"
  },
  {
    "text": "Each of us can build the\nIQ classifier or gaydar. But the question is, what kind\nof technology we will produce?",
    "start": "2695230",
    "end": "2705060"
  },
  {
    "text": "So in this, I finish the\nfirst part of the discussion. And I put in this slide some\nrecommended papers and talks",
    "start": "2705060",
    "end": "2713280"
  },
  {
    "text": "specifically on the introductory\ntopics, on the impact of NLP.",
    "start": "2713280",
    "end": "2718590"
  },
  {
    "text": "And I think these are-- there are hundreds or\nthousands of similar talks. But these are my favorites.",
    "start": "2718590",
    "end": "2725010"
  },
  {
    "text": "So if you want to read\nmore, please take a look. ",
    "start": "2725010",
    "end": "2731060"
  },
  {
    "text": "Should I stop for questions? Or should we move\nto the next part? ",
    "start": "2731060",
    "end": "2738735"
  },
  {
    "text": "Right at the\nmoment, there aren't any outstanding questions. So maybe it's OK to\nmove on, unless anyone's",
    "start": "2738735",
    "end": "2744340"
  },
  {
    "text": "desperately typing. OK. And the chat window\nis really nice.",
    "start": "2744340",
    "end": "2751450"
  },
  {
    "text": "There are so many responses. Thank you all. And I hope to get,\nlater, this chat.",
    "start": "2751450",
    "end": "2757590"
  },
  {
    "text": "I will read it. I think we can save it, yeah. Thank you. OK, then we can move\non to the second part",
    "start": "2757590",
    "end": "2764260"
  },
  {
    "text": "about algorithmic bias. So what are the topics in the\nintersection of ethics and NLP?",
    "start": "2764260",
    "end": "2774349"
  },
  {
    "text": "So the first one is\nalgorithmic bias. And this is about bias\nin data and NLP models.",
    "start": "2774350",
    "end": "2779997"
  },
  {
    "text": "And this is something\nthat I will talk more in the second part. But the important to understand\nthat the field is much broader.",
    "start": "2779998",
    "end": "2786420"
  },
  {
    "text": "So the next topic is\nincivility, so ability to develop NLP tools to\nidentify-- to actually, data",
    "start": "2786420",
    "end": "2795170"
  },
  {
    "text": "analytics to understand the hate\nspeech, toxicity, incivility online.",
    "start": "2795170",
    "end": "2800390"
  },
  {
    "text": "And this is a very\ncomplicated field because it's not only about\nbuilding the right classifiers.",
    "start": "2800390",
    "end": "2806180"
  },
  {
    "text": "There are many, many questions,\nsuch as if I post hateful comment, who does this\ncomment belong to?",
    "start": "2806180",
    "end": "2813710"
  },
  {
    "text": "Does it belong to a company? Does it belong to me?",
    "start": "2813710",
    "end": "2818960"
  },
  {
    "text": "Should it be removed or not? Because it is not clear\nwhere is the boundary between the free speech\nand the moderated speech,",
    "start": "2818960",
    "end": "2828620"
  },
  {
    "text": "how to minimize the harms\nbut defend the democracy, these questions are\nvery subjective.",
    "start": "2828620",
    "end": "2834530"
  },
  {
    "text": "And they're not regulated. So this is a big,\ndifficult field.",
    "start": "2834530",
    "end": "2841910"
  },
  {
    "text": "The next field is about privacy. So again, who does\nthis data belong to, and how to protect privacy?",
    "start": "2841910",
    "end": "2849200"
  },
  {
    "text": "This field of privacy\nis actually very, very under-explored in NLP.",
    "start": "2849200",
    "end": "2854210"
  },
  {
    "text": "Among other fields,\nI think there is some research on incivility,\non algorithmic bias, on other fields, but very\nlittle research on privacy.",
    "start": "2854210",
    "end": "2863195"
  },
  {
    "text": " Misinformation-- so\ninformation manipulation,",
    "start": "2863195",
    "end": "2870480"
  },
  {
    "text": "opinion manipulation, fake news. So there is a\nwhole range of ways",
    "start": "2870480",
    "end": "2875820"
  },
  {
    "text": "that data can be\nmanipulated, from generated texts and disinformation to just\nadvertisement and more subtle,",
    "start": "2875820",
    "end": "2888320"
  },
  {
    "text": "propaganda and subtle\nopinion manipulation. And there are many, many\ninteresting research projects",
    "start": "2888320",
    "end": "2895100"
  },
  {
    "text": "that can be done, really with\nthe focus on the language of manipulation, which I think\nis just an interesting topic",
    "start": "2895100",
    "end": "2902720"
  },
  {
    "text": "to explore. And finally, the\ntechnological divide--",
    "start": "2902720",
    "end": "2908200"
  },
  {
    "text": "so when we build our\ntools, even if it's a part of speech\ntagger, what are",
    "start": "2908200",
    "end": "2913710"
  },
  {
    "text": "populations for which are\nserved by these tools. So there is a certain\ndivide that technologies",
    "start": "2913710",
    "end": "2920820"
  },
  {
    "text": "are built unequally. There is no one language,\nno two languages. There are 6,000\nlanguages in the world.",
    "start": "2920820",
    "end": "2928150"
  },
  {
    "text": "And there are many populations. And there are\ncertain areas of NLP which are completely\nunder-explored.",
    "start": "2928150",
    "end": "2936390"
  },
  {
    "text": "For example,\nlanguage varieties-- we think we can solve\na problem for English,",
    "start": "2936390",
    "end": "2941609"
  },
  {
    "text": "for the problem of\ndependency parsing. But we don't account for\ndifferent varieties of English.",
    "start": "2941610",
    "end": "2946710"
  },
  {
    "text": "What about Nigerian English? What about\nAfrican-American English? What about Indian English?",
    "start": "2946710",
    "end": "2953550"
  },
  {
    "text": "So there is a\ntechnological divide that is currently present.",
    "start": "2953550",
    "end": "2959670"
  },
  {
    "text": "And as you see in the\nbottom, this picture shows that the field is\nhighly interdisciplinary.",
    "start": "2959670",
    "end": "2968369"
  },
  {
    "text": "So AI researchers cannot\nactually solve the problem of misinformation alone.",
    "start": "2968370",
    "end": "2976079"
  },
  {
    "text": "To be able to address the\nproblem of misinformation or hate speech, we need to have\nnot only engineers, but also",
    "start": "2976080",
    "end": "2985440"
  },
  {
    "text": "ethicists and social scientists\nand activists and politicians",
    "start": "2985440",
    "end": "2994900"
  },
  {
    "text": "who actually are responsible\nfor policies and the linguists,",
    "start": "2994900",
    "end": "3000510"
  },
  {
    "text": "because many of these phenomena\nare interesting phenomena which are not in words, but\nmore in pragmatics.",
    "start": "3000510",
    "end": "3008079"
  },
  {
    "text": "So this is a very interesting\nfield scientifically, but also very challenging to work on.",
    "start": "3008080",
    "end": "3015380"
  },
  {
    "text": "And there are some\nrecommended resources. And in particular, the one that\nmight be interesting to you all is CS 384, is a\nseminar by Dan Jurafsky,",
    "start": "3015380",
    "end": "3025900"
  },
  {
    "text": "that is an amazing, also, list. So if you want to take a\nlook on specific subfields--",
    "start": "3025900",
    "end": "3033990"
  },
  {
    "text": "so this is a general overview. And now I want to talk\nabout one of these subfields",
    "start": "3033990",
    "end": "3039810"
  },
  {
    "text": "and give some\nexplanation, why do we have algorithmic\nbias in our models?",
    "start": "3039810",
    "end": "3045790"
  },
  {
    "text": "So let's start, again,\nwith interaction.",
    "start": "3045790",
    "end": "3051555"
  },
  {
    "text": "I know you have the slides. But don't look forward. And I want to ask you questions. And please type.",
    "start": "3051555",
    "end": "3057510"
  },
  {
    "text": "Which word is more likely to\nbe used by a female, giggle or laugh? ",
    "start": "3057510",
    "end": "3064440"
  },
  {
    "text": "Just type quickly. Don't think too much. So please look at the chat\nand see the majority response.",
    "start": "3064440",
    "end": "3075240"
  },
  {
    "text": "It is absolutely giggle.",
    "start": "3075240",
    "end": "3080380"
  },
  {
    "text": "You're right. Next question--\nwhich word is more likely to be used by a\nfemale, brutal or fierce?",
    "start": "3080380",
    "end": "3085915"
  },
  {
    "text": " Oh, lovely.",
    "start": "3085915",
    "end": "3092306"
  },
  {
    "text": "It's like 99% fierce. Thank you.",
    "start": "3092306",
    "end": "3098020"
  },
  {
    "text": "Next question--\nwhich word is more likely to be used by an older\nperson, impressive or amazing?",
    "start": "3098020",
    "end": "3103760"
  },
  {
    "text": " So actually, from what I\nsee, it's 100% impressive.",
    "start": "3103760",
    "end": "3112320"
  },
  {
    "text": "Very impressive. Thank you for your answers. Which word is more\nlikely to be used by a person of a higher\noccupational class, suggestions",
    "start": "3112320",
    "end": "3120990"
  },
  {
    "text": "or proposals?  Do you see how correctly\nyou answer my questions?",
    "start": "3120990",
    "end": "3127900"
  },
  {
    "text": " So next question--\nwhy do we intuitively recognize the default\nsocial-- why do you all",
    "start": "3127900",
    "end": "3134980"
  },
  {
    "text": "know the right answer? ",
    "start": "3134980",
    "end": "3141270"
  },
  {
    "text": "Right. Our brains are biased. So this is about implicit\nbiases in our brains.",
    "start": "3141270",
    "end": "3148200"
  },
  {
    "text": "And this is a very good example. And you can also see\nhow language perpetuates and propagates biases, right?",
    "start": "3148200",
    "end": "3154050"
  },
  {
    "text": "It's all in the language. If you can could know\nfrom one word who is the person who said it, you\ncan imagine what kind of biases",
    "start": "3154050",
    "end": "3161520"
  },
  {
    "text": "we can extract\nfrom a longer text. So to understand what's\nhappening with biases,",
    "start": "3161520",
    "end": "3168220"
  },
  {
    "text": "we need to understand\nhow cognition works. So we have-- this was introduced\nby Kahneman and Tversky.",
    "start": "3168220",
    "end": "3175240"
  },
  {
    "text": "So conceptually,\nour brain is divided into System 1 and System 2. So System 1 is our autopilot.",
    "start": "3175240",
    "end": "3182830"
  },
  {
    "text": "It is used to make\ndecisions without thinking. It is very fast, parallel,\neffortless, and so on.",
    "start": "3182830",
    "end": "3190030"
  },
  {
    "text": "System 2 is our logical part. It knows how to analyze\nand make decisions that are unusual for us.",
    "start": "3190030",
    "end": "3196510"
  },
  {
    "text": "So it is not automatic. And it's slow,\nserial, controlled. It requires a lot\nof mental energy.",
    "start": "3196510",
    "end": "3205550"
  },
  {
    "text": "So our brain constantly\nreceives signals through all the sensors,\nthrough eyes, ears.",
    "start": "3205550",
    "end": "3212960"
  },
  {
    "text": "There is a lot of incoming data. There is a lot of\npixels here around me. But the actual part of\nSystem 2 is only able",
    "start": "3212960",
    "end": "3221270"
  },
  {
    "text": "to produce a very small\nportion of the signals that we received. So System 1 is automatic.",
    "start": "3221270",
    "end": "3227940"
  },
  {
    "text": "System 2 is effortful. But in practice, over\n95% of the signals",
    "start": "3227940",
    "end": "3236339"
  },
  {
    "text": "that we receive from the world\nis relegated to System 1. And the funny thing\nis, as Kahneman wrote,",
    "start": "3236340",
    "end": "3244080"
  },
  {
    "text": "is that we identify\nourselves with System 2. We believe that we are\nconscious and reasonable beings.",
    "start": "3244080",
    "end": "3252069"
  },
  {
    "text": "But in practice, most of our\ndecisions are made by System 1.",
    "start": "3252070",
    "end": "3258020"
  },
  {
    "text": "So since we are using\nautopilot most of our time,",
    "start": "3258020",
    "end": "3264920"
  },
  {
    "text": "our brain get-- all the information\nthat we perceive gets categorized, clusters,\nand labeled automatically.",
    "start": "3264920",
    "end": "3275160"
  },
  {
    "text": "And this is how cognitive\nstereotypes are created. ",
    "start": "3275160",
    "end": "3280600"
  },
  {
    "text": "And there are multiple,\nmultiple cognitive stereotypes that are aimed to fill\nthe gaps if we don't",
    "start": "3280600",
    "end": "3288099"
  },
  {
    "text": "have enough meaning, or reduce\ninformation-- generalize-- if we have too much information,\nor to complete the facts if we",
    "start": "3288100",
    "end": "3297850"
  },
  {
    "text": "are missing facts, and so on. And this leads to all\nkinds of cognitive biases.",
    "start": "3297850",
    "end": "3303109"
  },
  {
    "text": "So examples of biases would\nbe in-group favoritism. So we grew up seeing the\nmajority of specific people.",
    "start": "3303110",
    "end": "3312460"
  },
  {
    "text": "And we tend to like those\npeople more than the minorities. Halo effect-- that\nwe know very little",
    "start": "3312460",
    "end": "3320170"
  },
  {
    "text": "about the person or a\nspecific social group, but we tend to generalize. Based on one trait,\nwe could generalize",
    "start": "3320170",
    "end": "3327460"
  },
  {
    "text": "about the whole group and\nto other traits, and so on.",
    "start": "3327460",
    "end": "3333970"
  },
  {
    "text": "There are many biases. And thanks to these\nstereotypes, if I",
    "start": "3333970",
    "end": "3340840"
  },
  {
    "text": "asked you the\nquestions, the words, or if I show you these\npictures, you immediately know this is\ncalming, cute, tasty.",
    "start": "3340840",
    "end": "3348520"
  },
  {
    "text": "And if I show\nthese pictures, you would know that maybe it's\ndangerous, unpleasant.",
    "start": "3348520",
    "end": "3353770"
  },
  {
    "text": "And automatically,\nwhen we see a snake, we will automatically step\ndown, right, step back.",
    "start": "3353770",
    "end": "3360910"
  },
  {
    "text": "And we only need to invoke\nSystem 2, for example,",
    "start": "3360910",
    "end": "3367940"
  },
  {
    "text": "if we decide to touch it. But most of our\ndecisions are automatic. And this is the\nsame exact mechanism",
    "start": "3367940",
    "end": "3375250"
  },
  {
    "text": "that creates social\nstereotypes in our brains. So we exactly, in\nthe same mechanism,",
    "start": "3375250",
    "end": "3382420"
  },
  {
    "text": "internalize these associations\nand make generalizations",
    "start": "3382420",
    "end": "3389470"
  },
  {
    "text": "about specific groups. And this is why, when I ask you\nwhich word is more likely to be",
    "start": "3389470",
    "end": "3396070"
  },
  {
    "text": "used by an older person, 100%\nof you typed the word impressive",
    "start": "3396070",
    "end": "3403260"
  },
  {
    "text": "would be used. And importantly, these implicit\nbiases are very pervasive.",
    "start": "3403260",
    "end": "3409470"
  },
  {
    "text": "And they operate unconsciously. And one important property\nthat they are transitive.",
    "start": "3409470",
    "end": "3415080"
  },
  {
    "text": "So specifically, we are\nseeing that the Black person is playing basketball. And in the movie, we see that\na Black person uses drugs.",
    "start": "3415080",
    "end": "3422850"
  },
  {
    "text": "And we immediately\nconnect it and reinforce, for example, the associations\nwith the specific groups.",
    "start": "3422850",
    "end": "3430240"
  },
  {
    "text": "And the social stereotypes are\nnot necessarily all negative.",
    "start": "3430240",
    "end": "3437140"
  },
  {
    "text": "I can name some on-the-surface\npositive stereotypes. For example, Asians\nare good in math.",
    "start": "3437140",
    "end": "3444339"
  },
  {
    "text": "Importantly, they all\nhave negative effects, even seemingly\npositive stereotypes,",
    "start": "3444340",
    "end": "3450340"
  },
  {
    "text": "because they\npigeonhole individuals and put expectations of them.",
    "start": "3450340",
    "end": "3455500"
  },
  {
    "text": "Or they can just be harmful. And then, how do\nthese biases manifest?",
    "start": "3455500",
    "end": "3461829"
  },
  {
    "text": "They manifest in language. And for example, they manifest\nin subtle microaggressions.",
    "start": "3461830",
    "end": "3469410"
  },
  {
    "text": "And importantly,\nmicroaggressions should not correlate\nnecessarily with sentiment.",
    "start": "3469410",
    "end": "3476730"
  },
  {
    "text": "So sentiment analysis tools\nwould not detect them. On the surface level,\nmicroaggression",
    "start": "3476730",
    "end": "3483660"
  },
  {
    "text": "can be negative,\nneutral, or positive, like in these examples. But they actually\nbring prolonged harms,",
    "start": "3483660",
    "end": "3491640"
  },
  {
    "text": "even if they're meant\nas a compliment. And there was a lot of\nresearch in social sciences",
    "start": "3491640",
    "end": "3498060"
  },
  {
    "text": "that showed that\nthey can even bring more harms than\novert hate speech,",
    "start": "3498060",
    "end": "3504210"
  },
  {
    "text": "because they kind of bring\na significant emotional harm",
    "start": "3504210",
    "end": "3509369"
  },
  {
    "text": "and reinforce\nproblematic stereotypes. So if I will collect these\nconversations from Twitter,",
    "start": "3509370",
    "end": "3516360"
  },
  {
    "text": "do I look OK? You are so pretty. Is this a positive or negative? It's probably a\npositive interaction.",
    "start": "3516360",
    "end": "3523680"
  },
  {
    "text": "And then the next interaction-- check out my new physics paper. Is it positive or\nnegative interaction?",
    "start": "3523680",
    "end": "3531329"
  },
  {
    "text": "Why physics? You're so pretty. So we don't know. We don't have the right context.",
    "start": "3531330",
    "end": "3537150"
  },
  {
    "text": "And then for this\nquestion, do I look OK? And all kinds of\nresponses-- for example, you",
    "start": "3537150",
    "end": "3542789"
  },
  {
    "text": "are so pretty for your age. In this case,\nthese are negative. These are microaggressions. They make us cringe, right?",
    "start": "3542790",
    "end": "3549569"
  },
  {
    "text": "And then the problem is that all\nof this human-generated data, which necessarily incorporates\na lot of microaggressions",
    "start": "3549570",
    "end": "3558450"
  },
  {
    "text": "and just stereotypes\nthat we all have-- we are not aware of them-- it is fed to our systems.",
    "start": "3558450",
    "end": "3567560"
  },
  {
    "text": "So there is a lot\nof bias in language, like the stereotypes\nor historical biases",
    "start": "3567560",
    "end": "3573700"
  },
  {
    "text": "that are perpetuated. For example, there\nare more photos of male doctors than\nfemale doctors on the web.",
    "start": "3573700",
    "end": "3581440"
  },
  {
    "text": "Or human reporting biases-- and later, there are biases\nalso in our data sets.",
    "start": "3581440",
    "end": "3588520"
  },
  {
    "text": "So for example,\nwhat kind of data was sampled for annotation? From which kind of populations?",
    "start": "3588520",
    "end": "3594160"
  },
  {
    "text": "From which language varieties? From which locations? And then, who are we\nchoosing as annotators?",
    "start": "3594160",
    "end": "3600290"
  },
  {
    "text": "So there is a bias\nin who are annotators that annotate our data.",
    "start": "3600290",
    "end": "3605410"
  },
  {
    "text": "And then there is\ncognitive biases of annotators\nthemselves, how they treat what is a microaggression\nand what is not,",
    "start": "3605410",
    "end": "3612490"
  },
  {
    "text": "or other questions. And all these types\nof biases later",
    "start": "3612490",
    "end": "3618670"
  },
  {
    "text": "propagate into our\ncomputational systems. And this is how we get\nfrom cognitive bias,",
    "start": "3618670",
    "end": "3624790"
  },
  {
    "text": "social cognitive biases,\nto algorithmic biases, because if you remember\nthe System 1 and System 2,",
    "start": "3624790",
    "end": "3631540"
  },
  {
    "text": "currently the way we develop\nsystems, AI is only System 1.",
    "start": "3631540",
    "end": "3637460"
  },
  {
    "text": "And why is that? Because currently, the\nway we develop our tools,",
    "start": "3637460",
    "end": "3644650"
  },
  {
    "text": "the dominant paradigm is\na data-centric approach. So we need a lot of data\nto train good models.",
    "start": "3644650",
    "end": "3652270"
  },
  {
    "text": "And we do know well how\nto leverage a lot of data. But again, language\nis about people.",
    "start": "3652270",
    "end": "3658700"
  },
  {
    "text": "It is produced by people. But our existing\nsystems, they do not",
    "start": "3658700",
    "end": "3668240"
  },
  {
    "text": "leverage social\ncultural context. We don't know how\nto incorporate--",
    "start": "3668240",
    "end": "3675170"
  },
  {
    "text": "we don't do it, usually. We don't incorporate which\nsocial biases are positive",
    "start": "3675170",
    "end": "3680660"
  },
  {
    "text": "and which inductive\nbiases are good and which inductive\nbiases are bad to have.",
    "start": "3680660",
    "end": "3685970"
  },
  {
    "text": "So overall, our models\nare really powerful. And they're powerful at\nmaking generalizations.",
    "start": "3685970",
    "end": "3693359"
  },
  {
    "text": "But we don't know how to control\nfor the right inductive biases, which inductive biases\nare good and which",
    "start": "3693360",
    "end": "3699860"
  },
  {
    "text": "inductive biases are not. And then to go into\nthe next point, is that these models\nare opaque, right?",
    "start": "3699860",
    "end": "3706640"
  },
  {
    "text": "We don't also know how to\ninterpret well deep learning networks, which means it's not\neasy to analyze them and spot",
    "start": "3706640",
    "end": "3714980"
  },
  {
    "text": "the problems. And as you can guess,\nthis is not only related",
    "start": "3714980",
    "end": "3720770"
  },
  {
    "text": "to the field of ethical NLP. These are just interesting\nresearch questions,",
    "start": "3720770",
    "end": "3725900"
  },
  {
    "text": "how to incorporate social\nand cultural knowledge into deep learning models, or\nhow to develop interpretation",
    "start": "3725900",
    "end": "3733849"
  },
  {
    "text": "approaches.  So what is missing?",
    "start": "3733850",
    "end": "3742670"
  },
  {
    "text": "Today, what is\nmissing, for example, is that existing\nclassifiers for toxicity",
    "start": "3742670",
    "end": "3748580"
  },
  {
    "text": "detection-- if we want to build\ndata analytics to clean up our data before it propagates\nthrough the models,",
    "start": "3748580",
    "end": "3756830"
  },
  {
    "text": "we know only how to detect overt\ntoxic language, such as hate speech, because we are\nprimarily sampling our data",
    "start": "3756830",
    "end": "3764630"
  },
  {
    "text": "and training our data\nbased on lexicons. And there is almost no focus\non actual microaggressions",
    "start": "3764630",
    "end": "3773880"
  },
  {
    "text": "and more subtle\nbiases, which are often not in words but in pragmatics\nof the conversation,",
    "start": "3773880",
    "end": "3780940"
  },
  {
    "text": "and understanding who\nare the people involved in the conversation.",
    "start": "3780940",
    "end": "3786210"
  },
  {
    "text": "So today's tools,\nthey could be applied to these kind of\nmicroaggressions or hate speech",
    "start": "3786210",
    "end": "3792839"
  },
  {
    "text": "detection or sentiment analysis,\nbut they will necessarily fail.",
    "start": "3792840",
    "end": "3799170"
  },
  {
    "text": "The next point is that, again,\nour models do not incorporate social cultural knowledge.",
    "start": "3799170",
    "end": "3804540"
  },
  {
    "text": "And basically, the same comment\ncan be toxic or non-toxic",
    "start": "3804540",
    "end": "3811200"
  },
  {
    "text": "depending on who are the people\ninvolved in the conversation. But our models are data-centric\nand not people-centric.",
    "start": "3811200",
    "end": "3821160"
  },
  {
    "text": "And the more general problem\nis that the deep learning models are really good at\npicking spurious correlations.",
    "start": "3821160",
    "end": "3830849"
  },
  {
    "text": "And this is why, for\nexample, in this paper, the three comments which the\nonly difference in these three",
    "start": "3830850",
    "end": "3841230"
  },
  {
    "text": "sentences is a\nname, and probably the association of this name\nwith race or ethnicity--",
    "start": "3841230",
    "end": "3851020"
  },
  {
    "text": "so our models do pick up\non spurious confounds. So we think we\npredict sentiment.",
    "start": "3851020",
    "end": "3856769"
  },
  {
    "text": "But we also predict\nall kinds of labels that correlate with sentiment,\nbut not necessarily are",
    "start": "3856770",
    "end": "3863100"
  },
  {
    "text": "true predictors of sentiment-- for example, gender or race. And this is something\nvery pervasive.",
    "start": "3863100",
    "end": "3871760"
  },
  {
    "text": "And finally, the models\nare not explainable. So we kind of have\nthese deficiencies",
    "start": "3871760",
    "end": "3877660"
  },
  {
    "text": "just in core approaches\nto deep learning. And we have all this data.",
    "start": "3877660",
    "end": "3882790"
  },
  {
    "text": "And with this data, we\ntrain conversational agents, personal assistants,\nall kinds of systems.",
    "start": "3882790",
    "end": "3888490"
  },
  {
    "text": "And why do we care now? Because it can bring harms. So what kind of unintended\nharms it can bring?",
    "start": "3888490",
    "end": "3898330"
  },
  {
    "text": "Here is an example\nof an image search. If you search for three Black\nteenagers-- and this is,",
    "start": "3898330",
    "end": "3903730"
  },
  {
    "text": "I searched for it when\nI prepared this talk for the first time. So it was fixed, I guess. But this is how it\nwas in June 2017.",
    "start": "3903730",
    "end": "3912990"
  },
  {
    "text": "And then when you\nsearch for a doctor, you'll get primarily\nmale doctors, right,",
    "start": "3912990",
    "end": "3920400"
  },
  {
    "text": "and primarily white. And if you search\nfor a nurse, this is a stereotypical\nimage of a nurse.",
    "start": "3920400",
    "end": "3928130"
  },
  {
    "text": "And if you search\nfor a homemaker, this is just top search\nresults for these query words.",
    "start": "3928130",
    "end": "3935760"
  },
  {
    "text": "And if you search for\nCEO, it's a very specific",
    "start": "3935760",
    "end": "3941020"
  },
  {
    "text": "stereotypical image of CEO. And if you search\nfor a professor,",
    "start": "3941020",
    "end": "3946190"
  },
  {
    "text": "this one is my\npersonal favorite. So you can see all male images.",
    "start": "3946190",
    "end": "3951650"
  },
  {
    "text": "And there is only one woman. But if you look\nat her background, you can see these are\nsimple math facts.",
    "start": "3951650",
    "end": "3958170"
  },
  {
    "text": "So it's just an\nerror in the search. She's not a professor.",
    "start": "3958170",
    "end": "3963410"
  },
  {
    "text": "And this is the result of,\nfor example, face recognition.",
    "start": "3963410",
    "end": "3968579"
  },
  {
    "text": "So these are two examples. One camera does not\nrecognize Asian faces",
    "start": "3968580",
    "end": "3975950"
  },
  {
    "text": "and thinks they are blinked. On the right, the camera-- this is a video of a\nface-tracking camera that",
    "start": "3975950",
    "end": "3982880"
  },
  {
    "text": "is able to track white\nfaces, but immediately shuts down when the Black face comes.",
    "start": "3982880",
    "end": "3988740"
  },
  {
    "text": "So it is not able to\ntrack Black faces. So these are all consequences\nof the biased data that",
    "start": "3988740",
    "end": "3996099"
  },
  {
    "text": "propagates into models that do\nnot incorporate, intentionally,",
    "start": "3996100",
    "end": "4001260"
  },
  {
    "text": "basically safeguards against\nvery specific biases. Now what's going on with\nnatural language processing?",
    "start": "4001260",
    "end": "4008180"
  },
  {
    "text": "So this is the slide from\nthe very beginning that just lists all possible\napplications that I",
    "start": "4008180",
    "end": "4013230"
  },
  {
    "text": "could think about. As you can guess,\nsince 2016, there are many, many\npapers that just--",
    "start": "4013230",
    "end": "4021000"
  },
  {
    "text": "I don't think there is\nany application or core technologies of NLP left\nwhich did not expose biases",
    "start": "4021000",
    "end": "4028109"
  },
  {
    "text": "in the NLP technologies. So here is an example of bias\nin the machine translation.",
    "start": "4028110",
    "end": "4034510"
  },
  {
    "text": "So this is visual. This is why I'm showing it. So there are other languages\nthat mark third person pronouns",
    "start": "4034510",
    "end": "4043800"
  },
  {
    "text": "with gender, and other languages\nthat do not mark third person pronoun with gender.",
    "start": "4043800",
    "end": "4049240"
  },
  {
    "text": "So if you translate words from\na language such as Hungarian or from Estonian, that\ndon't mark third person",
    "start": "4049240",
    "end": "4058320"
  },
  {
    "text": "pronoun with gender,\ninto English, which does mark third person\npronoun as a gender,",
    "start": "4058320",
    "end": "4064200"
  },
  {
    "text": "you might see similar results. You will not see them now. This is what was exposed\nmaybe a year or two ago.",
    "start": "4064200",
    "end": "4072280"
  },
  {
    "text": "So basically, translation\nfrom they are a nurse, this would be, she's a nurse.",
    "start": "4072280",
    "end": "4078760"
  },
  {
    "text": "But they are the scientists. The translations would\nbe, he's a scientist. And same for engineer,\nbaker, teacher--",
    "start": "4078760",
    "end": "4086260"
  },
  {
    "text": "so all the stereotypes,\njust historical stereotypes that you\ncould think about.",
    "start": "4086260",
    "end": "4093339"
  },
  {
    "text": "So what are\npossibilities to fix it?",
    "start": "4093340",
    "end": "4098399"
  },
  {
    "text": "One way to fix it\nis actually simple. You could treat\nthe target gender",
    "start": "4098399",
    "end": "4106439"
  },
  {
    "text": "just as a target language\nis multilingual NMT. So I don't know, but I suspect\nyou did look at this paper",
    "start": "4106439",
    "end": "4113670"
  },
  {
    "text": "on the multilingual neural\nmachine translation. And basically, you can add in\nanother token, for example.",
    "start": "4113670",
    "end": "4121559"
  },
  {
    "text": "And you can\ncontrollability generate into female or male translation.",
    "start": "4121560",
    "end": "4126790"
  },
  {
    "text": "So the fix is not\ndifficult. But you need to be aware of\npotential dangers to be able to fix the model.",
    "start": "4126790",
    "end": "4134399"
  },
  {
    "text": "And importantly,\nthis is not only about fixing the model itself,\nbut also about fixing the user",
    "start": "4134399",
    "end": "4140160"
  },
  {
    "text": "interface, right? So the way Google\nfixed this interface is they provided\ndifferent translations,",
    "start": "4140160",
    "end": "4146740"
  },
  {
    "text": "so basically all\npossible translations for different genders. ",
    "start": "4146740",
    "end": "4153089"
  },
  {
    "text": "So the similar kind of\nharms were shown especially in dialogue systems.",
    "start": "4153090",
    "end": "4159350"
  },
  {
    "text": "So occasionally, such models\nmake big headings in news, like Microsoft's\nTay chatbot that",
    "start": "4159350",
    "end": "4166370"
  },
  {
    "text": "became very racist\nand sexist overnight, and the GPT-3 based models that\nwas offering suicide advice.",
    "start": "4166370",
    "end": "4176630"
  },
  {
    "text": "And about two weeks ago,\nthere was a Korean chatbot that became extremely\nhomophobic very quickly",
    "start": "4176630",
    "end": "4183770"
  },
  {
    "text": "and had to be removed. So these titles come up again. These headlines coming\nagain and again and again.",
    "start": "4183770",
    "end": "4191239"
  },
  {
    "text": "And I guess the point here is\nthat the way we do NLP today,",
    "start": "4191240",
    "end": "4196880"
  },
  {
    "text": "I call it a reactive approach. So we expose a specific\nproblem, a problem in search,",
    "start": "4196880",
    "end": "4205160"
  },
  {
    "text": "a problem in chatbots,\nracist chatbots,",
    "start": "4205160",
    "end": "4210170"
  },
  {
    "text": "or a problem in\nmachine translation. And then it creates\nbad publicity.",
    "start": "4210170",
    "end": "4216210"
  },
  {
    "text": "And then we start\nrevising the models. But it's not necessarily\nthat we need to develop",
    "start": "4216210",
    "end": "4222440"
  },
  {
    "text": "the tools in this way, right? So I hope that in\nthe future, we make",
    "start": "4222440",
    "end": "4227660"
  },
  {
    "text": "a paradigm shift towards\na more proactive approach. And the specific-- what would\na proactive approach require",
    "start": "4227660",
    "end": "4241060"
  },
  {
    "text": "is, for example, building\nnew data analytics.",
    "start": "4241060",
    "end": "4247250"
  },
  {
    "text": "So basically, rather than\nexposing biases, going further up the pipeline and starting\nactually with the data,",
    "start": "4247250",
    "end": "4254950"
  },
  {
    "text": "and building automatic\nmoderators and data analytics that can\nidentify problematic text,",
    "start": "4254950",
    "end": "4260949"
  },
  {
    "text": "problematic images, beyond\nthe words of hate speech, and then incorporating\nthe right inductive biases",
    "start": "4260950",
    "end": "4267070"
  },
  {
    "text": "into the models\nand understanding",
    "start": "4267070",
    "end": "4273409"
  },
  {
    "text": "moving from the\ndata-centered approaches to people-centered approaches,\nincorporating social, cultural,",
    "start": "4273410",
    "end": "4280880"
  },
  {
    "text": "and pragmatic knowledge, and\nin modeling the right research questions on how to the\nmost spurious confounds,",
    "start": "4280880",
    "end": "4289010"
  },
  {
    "text": "but predict only the target\nlabel, not necessarily picking",
    "start": "4289010",
    "end": "4295130"
  },
  {
    "text": "up on spurious\ncorrelations, and finally, on building more\ninterpretable models.",
    "start": "4295130",
    "end": "4300290"
  },
  {
    "text": "And importantly, that these\nare not orthogonal research directions-- so for example, to build\ngood data analytics,",
    "start": "4300290",
    "end": "4308210"
  },
  {
    "text": "you necessarily need to maybe\nhave an interpretable model, and also be able to incorporate\nthe right social cultural",
    "start": "4308210",
    "end": "4317000"
  },
  {
    "text": "knowledge, because again,\nthe microaggressions, the text is not\nnecessarily in words.",
    "start": "4317000",
    "end": "4324630"
  },
  {
    "text": "So what I was going\nto do, if I had time,",
    "start": "4324630",
    "end": "4330030"
  },
  {
    "text": "is I was going to show two case\nstudies from-- research studies",
    "start": "4330030",
    "end": "4335400"
  },
  {
    "text": "from my group that specifically\nfocus on these data analytics, identifying unsupervised bias\nor an interpretable model",
    "start": "4335400",
    "end": "4345370"
  },
  {
    "text": "for making hate speech\nclassifiers more robust. But I will skip it because\nwe are out of time.",
    "start": "4345370",
    "end": "4352290"
  },
  {
    "text": "You could show one. You still have a few minutes. You could show one\nquickly for five minutes.",
    "start": "4352290",
    "end": "4357900"
  },
  {
    "text": "So basically, we have we\nare trying to build these-- green boxes is what we\nhave today, hate speech",
    "start": "4357900",
    "end": "4365150"
  },
  {
    "text": "and sentiment analysis. But we are trying to build\na new kind of class of model",
    "start": "4365150",
    "end": "4370679"
  },
  {
    "text": "specifically for\nsocial bias analysis. And in these models,\nwe would want",
    "start": "4370680",
    "end": "4375720"
  },
  {
    "text": "to detect who are\nthe people involved, so who the comment, for\nexample, is directed to,",
    "start": "4375720",
    "end": "4381600"
  },
  {
    "text": "if it's a conversational\ndomain, and also to understand what kinds of\nmicroaggressions these are,",
    "start": "4381600",
    "end": "4388350"
  },
  {
    "text": "and also to maybe\ngenerate explanations or interpretations through\nbuilding more interpretable",
    "start": "4388350",
    "end": "4395460"
  },
  {
    "text": "models. And these are the two papers\nthat I was going to talk about.",
    "start": "4395460",
    "end": "4402580"
  },
  {
    "text": "One is an unsupervised approach\nto detection gender bias.",
    "start": "4402580",
    "end": "4407890"
  },
  {
    "text": "And one is on if we\nhave just few examples of microaggressions in the\nclassifier of hate speech,",
    "start": "4407890",
    "end": "4416153"
  },
  {
    "text": "these examples of\nmicroaggressions are adversarial examples\nto the classifier. The classifier is not\nable to deal with them.",
    "start": "4416153",
    "end": "4423960"
  },
  {
    "text": "But what we can do, we can\nfocus on interpretability of the classifier\nand, specifically,",
    "start": "4423960",
    "end": "4430740"
  },
  {
    "text": "making the classifier\nunderstanding for each probing example, which examples\nin the training data",
    "start": "4430740",
    "end": "4438430"
  },
  {
    "text": "influenced the\nclassifier's decision-- so changing the approach\nthrough interpretability from",
    "start": "4438430",
    "end": "4444489"
  },
  {
    "text": "interpreting specific\nsalient words in the input of the classifier into\nlooking at the training data,",
    "start": "4444490",
    "end": "4451540"
  },
  {
    "text": "sorting the training\ndata and identifying which examples where most\ninfluential for classifier",
    "start": "4451540",
    "end": "4459040"
  },
  {
    "text": "predictions. So using inference\nfunctions, for example-- this is a paper that\nPercy published in 2017.",
    "start": "4459040",
    "end": "4469239"
  },
  {
    "text": "And through this\nclassifier, we are able to surface microaggressions\ndespite that the classifier",
    "start": "4469240",
    "end": "4476710"
  },
  {
    "text": "makes the wrong prediction. So this is just the high-level,\nvery high-level summary without talking about\nthe actual studies.",
    "start": "4476710",
    "end": "4484920"
  },
  {
    "text": "So I will skip-- let me just skip\nthe actual papers.",
    "start": "4484920",
    "end": "4492480"
  },
  {
    "text": "And the slides are there. I'm happy to discuss later. I just don't want\nto go over time.",
    "start": "4492480",
    "end": "4498350"
  },
  {
    "text": "So to summarize, the field\nof computational ethics",
    "start": "4498350",
    "end": "4503600"
  },
  {
    "text": "is super interesting. And there are\ninteresting problems that are technically interesting. They're challenging.",
    "start": "4503600",
    "end": "4509090"
  },
  {
    "text": "So you don't need to\nhave a separate kind of important problems and the\ntechnical interesting problems.",
    "start": "4509090",
    "end": "4515990"
  },
  {
    "text": "We can work on important\nproblems which are also technically interesting, and\nfocus on important things",
    "start": "4515990",
    "end": "4521660"
  },
  {
    "text": "like building better\ndeep learning models. And these are\ninteresting subfields.",
    "start": "4521660",
    "end": "4527570"
  },
  {
    "text": "And if some of\nyou are interested in specific projects, so\nit's just in our course,",
    "start": "4527570",
    "end": "4532880"
  },
  {
    "text": "we just put together\na presentation",
    "start": "4532880",
    "end": "4537989"
  },
  {
    "text": "that just summarizes all\nkinds of possible projects",
    "start": "4537990",
    "end": "4543190"
  },
  {
    "text": "you could do. Thank you very much.",
    "start": "4543190",
    "end": "4548530"
  },
  {
    "text": "I wish I could see the audience. This is so weird, but that's OK. Thank you, Yulia,\nfor that great talk.",
    "start": "4548530",
    "end": "4555360"
  },
  {
    "text": "Yeah, so if people would like\nto ask some questions to Yulia,",
    "start": "4555360",
    "end": "4562710"
  },
  {
    "text": "if you raise your hand, we can\npromote you to be panelists. And I think, then, we can even\nhave you turn on your cameras",
    "start": "4562710",
    "end": "4570240"
  },
  {
    "text": "if you want to show\nYulia a real human being.",
    "start": "4570240",
    "end": "4576030"
  },
  {
    "text": "But if we're waiting to\nsee if there are people who would like to\ndo that, I mean,",
    "start": "4576030",
    "end": "4582059"
  },
  {
    "text": "there is one question that's\noutstanding at the moment, which is, \"Do these\nbots become racist,",
    "start": "4582060",
    "end": "4589550"
  },
  {
    "text": "sexist so quickly after exposure\nto the public due to the public intentionally trying\nto bias them, or is it",
    "start": "4589550",
    "end": "4595680"
  },
  {
    "text": "that common talk\namong the public is racist, sexist enough to\nbias any model upon exposure?\"",
    "start": "4595680",
    "end": "4602349"
  },
  {
    "text": "So I think this is both. But in the case, for example,\nof Tay bot the way it was built is it's a continual\nlearning system.",
    "start": "4602350",
    "end": "4610380"
  },
  {
    "text": "So it collects\ninputs from people and then uses them as a\nkind of training examples",
    "start": "4610380",
    "end": "4617969"
  },
  {
    "text": "to generate forward answers. And as usual, people pick up\non such things very quickly.",
    "start": "4617970",
    "end": "4624690"
  },
  {
    "text": "And then they intentionally\nbecame racist and sexist",
    "start": "4624690",
    "end": "4630030"
  },
  {
    "text": "against the bot. And the bot very quickly\nlearned to just mimic the people's behavior.",
    "start": "4630030",
    "end": "4636100"
  },
  {
    "text": "So it was some malicious\nattempt to kind of turn this bot into racist and sexist.",
    "start": "4636100",
    "end": "4641355"
  },
  {
    "text": " But this is how the\nmodel was designed,",
    "start": "4641355",
    "end": "4646960"
  },
  {
    "text": "to collect input\nfrom people but not",
    "start": "4646960",
    "end": "4652660"
  },
  {
    "text": "monitor the kind of sentences\nthat are used or not used in the training data.",
    "start": "4652660",
    "end": "4657830"
  },
  {
    "text": "So this is, again, going\nback to the discussion of that we actually don't\nhave good analytics.",
    "start": "4657830",
    "end": "4663130"
  },
  {
    "text": "Many of these analytics are\njust blacklists or whitelists.",
    "start": "4663130",
    "end": "4668860"
  },
  {
    "text": "They're very, very primitive. It's not very easy to\nincorporate such constraints",
    "start": "4668860",
    "end": "4674650"
  },
  {
    "text": "into generation, automatic\nfiltering of data. ",
    "start": "4674650",
    "end": "4681761"
  },
  {
    "text": "There is another question.  Can I ask my question?",
    "start": "4681762",
    "end": "4687550"
  },
  {
    "text": "Yes. Yes. So I guess we are doing\nQ&A with live people. So you can ask the\nquestion and then--",
    "start": "4687550",
    "end": "4695335"
  },
  {
    "text": "Yes. I'm curious, can you go\ninto a little bit more about how you measure\nyour model's performance?",
    "start": "4695335",
    "end": "4702720"
  },
  {
    "text": "Are there actually public\nbenchmark data sets you can-- or any sort of\nwell-defined metric--",
    "start": "4702720",
    "end": "4709349"
  },
  {
    "text": "that you can sort of\nobjectively measure your model's improvements?",
    "start": "4709350",
    "end": "4715450"
  },
  {
    "text": "Are you talking about\nspecifically our papers that I skipped? Or just in general. Are there-- I mean, I know\nit's a very new field.",
    "start": "4715450",
    "end": "4722970"
  },
  {
    "text": "And maybe it's harder\nto define really objective measure of bias.",
    "start": "4722970",
    "end": "4729265"
  },
  {
    "text": "So how do you measure\nprogress in general? Or you could also just mention--",
    "start": "4729265",
    "end": "4734300"
  },
  {
    "text": "This is a good question. It's very difficult.\nIt's like there is growing body of data sets.",
    "start": "4734300",
    "end": "4740980"
  },
  {
    "text": "For example, in U Dub,\nYejin Choi's group created the Social\nBias Inference Corpus.",
    "start": "4740980",
    "end": "4748020"
  },
  {
    "text": "I don't remember what bias-- SBIC. Overall, the problem of\nevaluation is actually",
    "start": "4748020",
    "end": "4755750"
  },
  {
    "text": "very difficult. And there are\nsome problems in which there are existing\nevaluation data sets.",
    "start": "4755750",
    "end": "4763270"
  },
  {
    "text": "If you think about hate\nspeech, for example, there are all kinds of data sets.",
    "start": "4763270",
    "end": "4768429"
  },
  {
    "text": "There are many data sets\nfor training and evaluating performance of hate\nspeech classifiers.",
    "start": "4768430",
    "end": "4776560"
  },
  {
    "text": "But when we think about\nbiases, there are much less. And the big problem here-- it's\nnot easy to collect such a data",
    "start": "4776560",
    "end": "4784910"
  },
  {
    "text": "set. So if you think-- let me actually show why it\nis difficult to collect a data",
    "start": "4784910",
    "end": "4791880"
  },
  {
    "text": "set of, say, microaggressions. So a naive solution\nwould be to--",
    "start": "4791880",
    "end": "4797880"
  },
  {
    "text": " so if you think about\nthe standard way",
    "start": "4797880",
    "end": "4804610"
  },
  {
    "text": "of data collection, so\nwe would sample some data from the internet. And we give it to\nMechanical Turk annotators.",
    "start": "4804610",
    "end": "4813100"
  },
  {
    "text": "And then they would analyze,\nis it biased or not? And we build this\nsupervised classifier. So this is what we cannot do in\nthe case of more subtle biases.",
    "start": "4813100",
    "end": "4821199"
  },
  {
    "text": "First, because we don't\nhave a strong lexical sieve to sample the\nright data because,",
    "start": "4821200",
    "end": "4827170"
  },
  {
    "text": "again, these biases\nare not in words. Like you would just sample\nfrom the whole Reddit corpus? It's not clear how to\nannotate to make it feasible,",
    "start": "4827170",
    "end": "4834580"
  },
  {
    "text": "not too expensive. But more importantly,\nthat every annotator",
    "start": "4834580",
    "end": "4840020"
  },
  {
    "text": "will incorporate\ntheir own biases. So you actually need very\nwell-trained annotators and multiple\nannotations per sample.",
    "start": "4840020",
    "end": "4847360"
  },
  {
    "text": "So the question of how\nto create such a dataset is very, very difficult. In our study, we collected the--",
    "start": "4847360",
    "end": "4854950"
  },
  {
    "text": "so there is a website\ncalled microaggressions.com",
    "start": "4854950",
    "end": "4859990"
  },
  {
    "text": "that has self-reported\nmicroaggressions, when people actually recall experiences of\nmicroaggressions against them,",
    "start": "4859990",
    "end": "4870920"
  },
  {
    "text": "and they quote them. And this is what we used\nto evaluate our data.",
    "start": "4870920",
    "end": "4877110"
  },
  {
    "text": "But data collection\nis as a big problem currently as just modeling.",
    "start": "4877110",
    "end": "4883430"
  },
  {
    "text": " Do you want to ask a question?",
    "start": "4883430",
    "end": "4889079"
  },
  {
    "start": "4889080",
    "end": "4899890"
  },
  {
    "text": "Right now? Yeah.  I don't have any other\nquestions to ask.",
    "start": "4899890",
    "end": "4906720"
  },
  {
    "text": "Oh, sorry. OK, so I should go on. ",
    "start": "4906720",
    "end": "4915389"
  },
  {
    "text": "OK, can you hear me? Yes. Thanks for the great lecture.",
    "start": "4915390",
    "end": "4921679"
  },
  {
    "text": "It's a very appropriate\ntopic for the guest lecture. ",
    "start": "4921680",
    "end": "4927690"
  },
  {
    "text": "So I took the\ncourse CS 182, which introduced many\nnotions of fairness",
    "start": "4927690",
    "end": "4934400"
  },
  {
    "text": "through case studies\nand assignments. And I've been thinking a lot\nabout some of these functions.",
    "start": "4934400",
    "end": "4940830"
  },
  {
    "text": "And in the course,\nthere was mentioned for research done\nby Kleinberg who",
    "start": "4940830",
    "end": "4946730"
  },
  {
    "text": "showed three different\nnotions of fairness can be simultaneously satisfied.",
    "start": "4946730",
    "end": "4952940"
  },
  {
    "text": "The calibration, which was\nlike the probability of outcome given risk scores, the\nfalse positive rate,",
    "start": "4952940",
    "end": "4959840"
  },
  {
    "text": "the false negative rate cannot\nall be completely independent across protective traits.",
    "start": "4959840",
    "end": "4967310"
  },
  {
    "text": "So if past a certain\npoint, these metrics just become direct trade-offs,\nis it the case that fairness",
    "start": "4967310",
    "end": "4975950"
  },
  {
    "text": "becomes subjective after that? And I guess more generally,\nin ethics research,",
    "start": "4975950",
    "end": "4982520"
  },
  {
    "text": "has there been frameworks of\ncreating sort of upper bounds or constraints among\nthese different metrics",
    "start": "4982520",
    "end": "4990810"
  },
  {
    "text": "so we sort of measure how\nclose we get to the ideal? This is a very\ndifficult question.",
    "start": "4990810",
    "end": "4997580"
  },
  {
    "text": "So right, the fairness research,\nthere are actually proofs that you cannot satisfy both\nthe measures of performance",
    "start": "4997580",
    "end": "5008730"
  },
  {
    "text": "and inclusivity. And this is why they\nare measured separately,",
    "start": "5008730",
    "end": "5015670"
  },
  {
    "text": "the false positives,\nfalse negatives. And the question is whether--",
    "start": "5015670",
    "end": "5022639"
  },
  {
    "text": "because of this issue,\nwhether it becomes subjective? ",
    "start": "5022640",
    "end": "5029570"
  },
  {
    "text": "It's even bigger, I guess. The question here is even\nbigger because the question",
    "start": "5029570",
    "end": "5037320"
  },
  {
    "text": "of inclusivity-- so it competes with the\nquestion of monetization.",
    "start": "5037320",
    "end": "5043350"
  },
  {
    "text": "If you think who are\nthe main owners of data and how they train\nalgorithms, the goal",
    "start": "5043350",
    "end": "5048599"
  },
  {
    "text": "is to basically have\nbetter monetization. Like, who will see\nthis advertisement?",
    "start": "5048600",
    "end": "5054000"
  },
  {
    "text": "But there is a competing\nobjective of inclusivity.",
    "start": "5054000",
    "end": "5059700"
  },
  {
    "text": "Will this\nadvertisement reach out to all kinds of populations?",
    "start": "5059700",
    "end": "5065790"
  },
  {
    "text": "And it's not only-- it's not subjective. There is kind of\na clear incentive,",
    "start": "5065790",
    "end": "5072180"
  },
  {
    "text": "for example, in companies\nto maximize monetization rather than inclusivity, right,\nbecause it's also internal.",
    "start": "5072180",
    "end": "5078570"
  },
  {
    "text": " I don't have an\neasy answer to this.",
    "start": "5078570",
    "end": "5083590"
  },
  {
    "text": "I agree. It can be subjective. Or it can be worse\nthan subjective",
    "start": "5083590",
    "end": "5088840"
  },
  {
    "text": "because these\nobjectives compete. So would you say\nit's sort of more--",
    "start": "5088840",
    "end": "5096660"
  },
  {
    "text": "the field of ethics\nresearch overall is more interdisciplinary\nand a lot of these answers\nto these questions",
    "start": "5096660",
    "end": "5104040"
  },
  {
    "text": "are more context-dependent? It's very\ncontext-dependent, right. It is very context-dependent.",
    "start": "5104040",
    "end": "5110400"
  },
  {
    "text": "As I mentioned, for example,\nthe same application, in different\ncontexts, can be used",
    "start": "5110400",
    "end": "5120130"
  },
  {
    "text": "for good and for bad, right? And different thresholds\non performance can be applied for\ndifferent types of settings.",
    "start": "5120130",
    "end": "5130250"
  },
  {
    "text": "And also, I really think\nI'm not qualified even to answer this question, right?",
    "start": "5130250",
    "end": "5135560"
  },
  {
    "text": "We should ask maybe philosopher\nor experts in policy, right,",
    "start": "5135560",
    "end": "5141450"
  },
  {
    "text": "because eventually, I'm-- I know how to build the tools.",
    "start": "5141450",
    "end": "5149160"
  },
  {
    "text": "And I'm trying to make\ntechnologies more ethical.",
    "start": "5149160",
    "end": "5155490"
  },
  {
    "text": "But the question of\nhow they are deployed and what are the\nspecific decisions,",
    "start": "5155490",
    "end": "5160710"
  },
  {
    "text": "it's very difficult\nto control for and to kind of give\ndefinite answers about this.",
    "start": "5160710",
    "end": "5166860"
  },
  {
    "text": "OK, so here's another\ndifficult question for you from [AUDIO OUT]. And you can't use\nthat cop-out answer.",
    "start": "5166860",
    "end": "5174580"
  },
  {
    "text": "So you said,\n\"Earlier, you showed the example of \"AI\nGaydar\" with the question",
    "start": "5174580",
    "end": "5179940"
  },
  {
    "text": "of why would we\nwant to study this? The author is\njustified by claiming that, given the widespread\nuse of facial recognition,",
    "start": "5179940",
    "end": "5187560"
  },
  {
    "text": "our findings have critical\nimplications for the protection of civil liberties.",
    "start": "5187560",
    "end": "5192720"
  },
  {
    "text": "Given that some unscrupulous\ngovernments may indeed implement such technology\nto oppress minorities",
    "start": "5192720",
    "end": "5198210"
  },
  {
    "text": "based on such things\nas orientation, do social scientists\nhave an obligation",
    "start": "5198210",
    "end": "5203309"
  },
  {
    "text": "to get ahead of this\nthreat by understanding the properties of such models? How do we weigh the\nethical trade-offs?\"",
    "start": "5203310",
    "end": "5210540"
  },
  {
    "text": "Oh, gosh. Now I need to respond\nfrom the point of view of all social scientists.",
    "start": "5210540",
    "end": "5217110"
  },
  {
    "text": "I can't. I don't want to answer\nphilosophical questions.",
    "start": "5217110",
    "end": "5222360"
  },
  {
    "text": "But I kind of have\nthe answer about--",
    "start": "5222360",
    "end": "5229440"
  },
  {
    "text": "maybe a simple\nanswer to why I don't agree with the\nclaim of researchers",
    "start": "5229440",
    "end": "5235800"
  },
  {
    "text": "that we need to expose\nthis technology, to publish, actually,\nthis paper to expose the dangers of this technology.",
    "start": "5235800",
    "end": "5244090"
  },
  {
    "text": "One of them, like the\nknife analogy that I gave, is one of the answers. So if you think about\na similar field--",
    "start": "5244090",
    "end": "5253380"
  },
  {
    "text": "not a similar field,\nbut a similar type of interaction in\nsecurity, right,",
    "start": "5253380",
    "end": "5260099"
  },
  {
    "text": "in cybersecurity,\nso it's very common to kind of break the algorithm\nto show its vulnerabilities,",
    "start": "5260100",
    "end": "5268780"
  },
  {
    "text": "and then to iteratively fix it. So this is the approach\nthat researchers took. Let's show the\nvulnerabilities, that we",
    "start": "5268780",
    "end": "5276840"
  },
  {
    "text": "are able to build\nthis technology, to expose these threats. But unlike with\nthe security field,",
    "start": "5276840",
    "end": "5287310"
  },
  {
    "text": "here the kind of exposure\nof this technology, publication of this technology,\ncan have real implications",
    "start": "5287310",
    "end": "5296290"
  },
  {
    "text": "of human life. So again, the cost of\nmisclassification-- and if we think\nabout other problems,",
    "start": "5296290",
    "end": "5306970"
  },
  {
    "text": "like similar problems,\nlike the problems of-- I can give many other\nsimilar problems",
    "start": "5306970",
    "end": "5312910"
  },
  {
    "text": "in which we can expose\nthis technology which will harm people. Like let's create a deep\nfakes video, a porn video",
    "start": "5312910",
    "end": "5323380"
  },
  {
    "text": "with the professors. We can do it, right, to expose\nthe danger of technology",
    "start": "5323380",
    "end": "5328420"
  },
  {
    "text": "of deep fakes. But what kind of harm it will\nbring to specific people who",
    "start": "5328420",
    "end": "5334720"
  },
  {
    "text": "were involved in\nthis kind of exposure of the harm of this technology?",
    "start": "5334720",
    "end": "5340000"
  },
  {
    "text": "So I have the\nanswer of why it was wrong to publish this\nstudy in the first place",
    "start": "5340000",
    "end": "5346139"
  },
  {
    "text": "and why it's not\nproductive, is not helpful. But it's very difficult to\nanswer the question, what",
    "start": "5346140",
    "end": "5351929"
  },
  {
    "text": "should social scientists do? I don't know. OK. Well, could I ask\na question next?",
    "start": "5351930",
    "end": "5360491"
  },
  {
    "text": "Or it's just disappeared. [INAUDIBLE]? I think-- can you hear me now? Yeah.",
    "start": "5360492",
    "end": "5366210"
  },
  {
    "text": "OK. Thank you so much for the talk. It's really interesting,\nand as we've just seen,",
    "start": "5366210",
    "end": "5372500"
  },
  {
    "text": "really challenging stuff. I guess my question is a\nlittle bit more practical. So maybe that's a\nreprieve for you.",
    "start": "5372500",
    "end": "5379250"
  },
  {
    "text": "But unfortunately, it seems\nlike in a lot of NLP and AI",
    "start": "5379250",
    "end": "5386910"
  },
  {
    "text": "more broadly, some of\nthis ethics and bias stuff is like kind of an afterthought.",
    "start": "5386910",
    "end": "5392150"
  },
  {
    "text": "A lot of projects don't\nreally necessarily take it into account from the outset. And it's more sort\nof incidental.",
    "start": "5392150",
    "end": "5399889"
  },
  {
    "text": "So my question is, as we're\nworking on NLP projects,",
    "start": "5399890",
    "end": "5405590"
  },
  {
    "text": "maybe even for our final\ncourse project, what are some kind of concrete\nsteps that we could take",
    "start": "5405590",
    "end": "5411380"
  },
  {
    "text": "or a systematic\napproach that we can use to sort of incorporating\nsome of this ethics knowledge",
    "start": "5411380",
    "end": "5418430"
  },
  {
    "text": "in things that might not\nexplicitly seem like they have a lot to do with ethics? ",
    "start": "5418430",
    "end": "5427420"
  },
  {
    "text": "So it depends on\nthe project, right? I cannot answer generally.",
    "start": "5427420",
    "end": "5433810"
  },
  {
    "text": "The first part of the lecture\nwas exactly about this. If I build my project,\nwhat kind of questions",
    "start": "5433810",
    "end": "5440770"
  },
  {
    "text": "I can ask to know if\nthere are some pitfalls? If it's a different\nproject, I think overall,",
    "start": "5440770",
    "end": "5455390"
  },
  {
    "text": "these are important\nquestions which are general for deep\nlearning models, which",
    "start": "5455390",
    "end": "5463440"
  },
  {
    "text": "could be later used to\ncreate a better technology. So how to incorporate\nunderstanding",
    "start": "5463440",
    "end": "5471780"
  },
  {
    "text": "who are the people who\nproduced the language, or who are the users?",
    "start": "5471780",
    "end": "5477280"
  },
  {
    "text": "Incorporating the\nright inductive biases, or a technology for\ndemoting confounds--",
    "start": "5477280",
    "end": "5482320"
  },
  {
    "text": "it doesn't have\nto be specifically on ethics-related problems\nor interpretability",
    "start": "5482320",
    "end": "5489160"
  },
  {
    "text": "of deep learning. It doesn't have to be\nethics kind of project.",
    "start": "5489160",
    "end": "5495970"
  },
  {
    "text": "But the kind of\ntechnology, if you can develop such a\ntechnology, eventually it can be useful for building such\nbetter models that proactively",
    "start": "5495970",
    "end": "5506409"
  },
  {
    "text": "prevent unintended harms. ",
    "start": "5506410",
    "end": "5512159"
  },
  {
    "text": "So I guess the strategy would\nbe sort of to just lay out these general topics, pose these\nquestions to yourself-- maybe",
    "start": "5512160",
    "end": "5521780"
  },
  {
    "text": "write them down or\njust think about them-- and then proceed as such? Yeah, so this is how\nI think about it.",
    "start": "5521780",
    "end": "5528990"
  },
  {
    "text": "I think about potential-- if this technology\nwould be deployed, what could be corner cases?",
    "start": "5528990",
    "end": "5535430"
  },
  {
    "text": "What is potential for dual use? If it works, kind of,\nhow can it be misused?",
    "start": "5535430",
    "end": "5543530"
  },
  {
    "text": "And also, when it doesn't\nwork, what kind of errors can be harmful?",
    "start": "5543530",
    "end": "5549420"
  },
  {
    "text": "So this is-- if you go\nback to in the slides to the beginning of the\nlecture, these questions,",
    "start": "5549420",
    "end": "5556190"
  },
  {
    "text": "they could be applied to\nmany kinds of technology. And they give a more clear kind\nof guidelines to what things--",
    "start": "5556190",
    "end": "5565890"
  },
  {
    "text": "how bad outcomes\ncould be prevented. I'm sure maybe I'm missing\nsomething totally, right?",
    "start": "5565890",
    "end": "5573690"
  },
  {
    "text": "All of this content--\nmuch of this content is just I came up\nwith it by reading",
    "start": "5573690",
    "end": "5579810"
  },
  {
    "text": "a lot of different papers. But there is no\nclear guidelines. It's such a new field.",
    "start": "5579810",
    "end": "5585700"
  },
  {
    "text": "So maybe there are things\nthat I'm also missing. Here's another question\nfrom [AUDIO OUT]..",
    "start": "5585700",
    "end": "5593700"
  },
  {
    "text": "\"Are models wrong\nfor being biased? In the end, they just learn\nwhat they're designed to learn.",
    "start": "5593700",
    "end": "5599940"
  },
  {
    "text": "And isn't our intervention to\ncorrect this behavior actually can cause a bias?\"",
    "start": "5599940",
    "end": "5607510"
  },
  {
    "text": "This is a good question. So it is kind of a\nquestion that I also",
    "start": "5607510",
    "end": "5613289"
  },
  {
    "text": "have been thinking about. Are models wrong for, for\nexample, reflecting accurately",
    "start": "5613290",
    "end": "5619950"
  },
  {
    "text": "the real world, right? Do we need to devise\nactively models to make them fair when\nthe world is not fair,",
    "start": "5619950",
    "end": "5628490"
  },
  {
    "text": "when our data is not fair? So first of all, the way\nwe train models today,",
    "start": "5628490",
    "end": "5635690"
  },
  {
    "text": "they don't only\nperpetuate biases. They amplify biases.",
    "start": "5635690",
    "end": "5641160"
  },
  {
    "text": "So this is a natural behavior\nof a machine learning model",
    "start": "5641160",
    "end": "5648500"
  },
  {
    "text": "that basically, when you have\nan input example for which",
    "start": "5648500",
    "end": "5654320"
  },
  {
    "text": "the confidence is lower, it will\ndefault to a majority class. This is why if your\ndata contains biases,",
    "start": "5654320",
    "end": "5664099"
  },
  {
    "text": "these biases will be amplified\nin a machine learning model trained on this data.",
    "start": "5664100",
    "end": "5669949"
  },
  {
    "text": "And this is clearly wrong. Whether it is wrong to\nbuild models that do not",
    "start": "5669950",
    "end": "5678570"
  },
  {
    "text": "reflect the actual true\ndistribution in the data, it's much more\ndifficult questions.",
    "start": "5678570",
    "end": "5684300"
  },
  {
    "text": "But there are\nclear kind of cases",
    "start": "5684300",
    "end": "5691550"
  },
  {
    "text": "in which I would say it is\nwrong to build a model that,",
    "start": "5691550",
    "end": "5699929"
  },
  {
    "text": "in the search for CEO, shows\nonly male CEOs, while kind of--",
    "start": "5699930",
    "end": "5707210"
  },
  {
    "text": "because it amplifies\nbiases, but yeah. This is already a\nsubjective kind of answer.",
    "start": "5707210",
    "end": "5714170"
  },
  {
    "text": "It's just my personal\nopinion, because not much to do with the research\nthat we are doing, right?",
    "start": "5714170",
    "end": "5722760"
  },
  {
    "text": "Well, would you like\nto ask your question? ",
    "start": "5722760",
    "end": "5728270"
  },
  {
    "text": "Yeah, one sec. Let me, okay, I was\ntrying to set my video. But it's saying I can't. So I guess I'm\njust going to ask.",
    "start": "5728270",
    "end": "5734210"
  },
  {
    "text": "By the way, I don't\nsee people, anyway. Well, you could at the\nstart of the video. But anyway, let's just go.",
    "start": "5734210",
    "end": "5741060"
  },
  {
    "text": "Yeah, I'll just ask. So thank you so\nmuch for your talk. I have a question\non microaggressions?",
    "start": "5741060",
    "end": "5746869"
  },
  {
    "text": "So who is to decide what is\nconsidered a microaggression? Is it the people who\nmicroaggressions are",
    "start": "5746870",
    "end": "5753410"
  },
  {
    "text": "potentially targeted against? Is it philosophers\nor social scientists or just people in education?",
    "start": "5753410",
    "end": "5759650"
  },
  {
    "text": "And in case opinions differ, do\nwe just listen to the majority? It all seems crucially important\nto me, but very, very difficult",
    "start": "5759650",
    "end": "5767020"
  },
  {
    "text": "to standardize and kind of\nreach a consensus, especially because different cultures\nperceive things differently.",
    "start": "5767020",
    "end": "5774033"
  },
  {
    "text": "Thank you very much\nfor these questions. These are amazing questions.",
    "start": "5774033",
    "end": "5779140"
  },
  {
    "text": "These are difficult questions. This is why we did not\ncreate our own corpus",
    "start": "5779140",
    "end": "5785400"
  },
  {
    "text": "of microaggressions,\nright, because it's culturally dependent. It's very, very personal. It's subjective.",
    "start": "5785400",
    "end": "5791050"
  },
  {
    "text": "This is why we first focused\non a corpus of perceived microaggressions, people\nthat actually felt",
    "start": "5791050",
    "end": "5803159"
  },
  {
    "text": "that the interactions\nwere negative,",
    "start": "5803160",
    "end": "5808710"
  },
  {
    "text": "because they knew that\nthese were microaggressions. ",
    "start": "5808710",
    "end": "5817880"
  },
  {
    "text": "Who is to decide whether\nsomething is microaggression? Mm.",
    "start": "5817880",
    "end": "5823080"
  },
  {
    "text": " Yeah, I don't know.",
    "start": "5823080",
    "end": "5831469"
  },
  {
    "text": "This is very\ndifficult. What I can think about practical\nsolutions about it--",
    "start": "5831470",
    "end": "5839030"
  },
  {
    "text": "I would say have very\nwell-trained annotators",
    "start": "5839030",
    "end": "5844130"
  },
  {
    "text": "who understand what\nmicroaggression is. So kind of, we explain\nwhat is microaggression,",
    "start": "5844130",
    "end": "5850820"
  },
  {
    "text": "they see many examples, they\nunderstand that for example,",
    "start": "5850820",
    "end": "5857960"
  },
  {
    "text": "a sentence that targets\na minority group,",
    "start": "5857960",
    "end": "5863750"
  },
  {
    "text": "and other things. And then have many\nannotators per one sentence.",
    "start": "5863750",
    "end": "5868790"
  },
  {
    "text": "So like in the, say, the study-- this is about other social\nconcepts that are abstract.",
    "start": "5868790",
    "end": "5879120"
  },
  {
    "text": "For example, in Dan\nJurafsky's study on respect in\npolice interactions,",
    "start": "5879120",
    "end": "5885170"
  },
  {
    "text": "respect is also a\nsubjective thing, right? So what they did, they\ntook every utterance,",
    "start": "5885170",
    "end": "5892340"
  },
  {
    "text": "and they had multiple\nannotators, multiple trained annotators, for each\nutterance, and just increased",
    "start": "5892340",
    "end": "5899449"
  },
  {
    "text": "the number of voters of\nwhether an utterance is respectful or not.",
    "start": "5899450",
    "end": "5906110"
  },
  {
    "text": "So practically, I\nthink this should be the procedure for creating\ncorpus of microaggressions.",
    "start": "5906110",
    "end": "5915380"
  },
  {
    "text": "But a more philosophical\nquestion is, who is to decide? And it's a more difficult one.",
    "start": "5915380",
    "end": "5924370"
  },
  {
    "text": "Yeah, got it. Thank you. So there's still more questions. But you're allowed\nto say that you're worn out at any point, Yulia.",
    "start": "5924370",
    "end": "5930400"
  },
  {
    "text": "And if you're not,\nthe next question-- I feel bad about not being\nable to answer big questions",
    "start": "5930400",
    "end": "5935650"
  },
  {
    "text": "about society and-- but yeah, I'm happy to answer.",
    "start": "5935650",
    "end": "5941350"
  },
  {
    "text": "The next question is,\n\"Can you talk a bit more about the unsupervised approach\nto identifying implicit bias?\"",
    "start": "5941350",
    "end": "5951150"
  },
  {
    "text": "I can. ",
    "start": "5951150",
    "end": "5958240"
  },
  {
    "text": "I just need to think how to\ntalk about it in a few words. ",
    "start": "5958240",
    "end": "5966410"
  },
  {
    "text": "So intuitively, we cannot create\na corpus which has an utterance",
    "start": "5966410",
    "end": "5978090"
  },
  {
    "text": "and then a label. Is that sentence biased or not? So we create a causal\nframework in which our target",
    "start": "5978090",
    "end": "5987210"
  },
  {
    "text": "label is more objective. Who is this sentence\ndirected to?",
    "start": "5987210",
    "end": "5992219"
  },
  {
    "text": "To a man or to a woman? So our labels are gender labels.",
    "start": "5992220",
    "end": "5997680"
  },
  {
    "text": "And in a naive way, if given\na sentence towards a person,",
    "start": "5997680",
    "end": "6003170"
  },
  {
    "text": "without looking at the actual\nperson and their comment, just by this comment\ntowards the person,",
    "start": "6003170",
    "end": "6008750"
  },
  {
    "text": "we can predict if it's\ndirected to a woman. We can say that there is\nsome bias in the model--",
    "start": "6008750",
    "end": "6016870"
  },
  {
    "text": "that there is some\nbias in the sentence. But it's a naive\napproach because there",
    "start": "6016870",
    "end": "6022940"
  },
  {
    "text": "are other kinds of ways in\nwhich we can predict the target",
    "start": "6022940",
    "end": "6028340"
  },
  {
    "text": "gender, but which will not\nbe associated with bias.",
    "start": "6028340",
    "end": "6033880"
  },
  {
    "text": "For example, it's the context\nof the current station, the traits of the person that\nwe are writing to, and so on.",
    "start": "6033880",
    "end": "6040220"
  },
  {
    "text": "So the crux of our technology\nis that we predict gender, but demote all\nkinds of confounds",
    "start": "6040220",
    "end": "6046930"
  },
  {
    "text": "in the task of detecting bias. So we demote the signals\nof the source sentence.",
    "start": "6046930",
    "end": "6055480"
  },
  {
    "text": "We demote the latent traits\nof the target person. And so we make this\ntask very difficult",
    "start": "6055480",
    "end": "6062620"
  },
  {
    "text": "to detect what is\nthe target gender. And if, after all these\ndemotions of the confounds,",
    "start": "6062620",
    "end": "6070030"
  },
  {
    "text": "given an utterance that is\ndirected to a specific person, we can still classify\nthis utterance that",
    "start": "6070030",
    "end": "6076119"
  },
  {
    "text": "is clearly directed to a woman. It is likely that this\nutterance contains bias.",
    "start": "6076120",
    "end": "6082400"
  },
  {
    "text": "So what I was going to talk\nis all kinds of demotion approaches that we develop. But once we demote\nthese approaches",
    "start": "6082400",
    "end": "6089860"
  },
  {
    "text": "and we can still predict\nthe utterances, what",
    "start": "6089860",
    "end": "6096190"
  },
  {
    "text": "is the gender of the\ntarget that would receive, we actually can surface\nsome biased sentences.",
    "start": "6096190",
    "end": "6102100"
  },
  {
    "text": "So these are the main findings. For example, if we look\nat comments directed",
    "start": "6102100",
    "end": "6107470"
  },
  {
    "text": "to politicians, after\nall these demotions, and we see comments\nthat kind of clearly",
    "start": "6107470",
    "end": "6114640"
  },
  {
    "text": "predict the target\ngender, we can see that common for politicians\ntalk about their spouses,",
    "start": "6114640",
    "end": "6121330"
  },
  {
    "text": "about their family life, and\nalso about their competence",
    "start": "6121330",
    "end": "6126430"
  },
  {
    "text": "Maybe question their competence. And if we look at common for\npublic figures, like actresses,",
    "start": "6126430",
    "end": "6133420"
  },
  {
    "text": "we can see a lot of words\nthat are just related to objectification,\nsexualization, regardless",
    "start": "6133420",
    "end": "6138910"
  },
  {
    "text": "of their source content. So they can talk\nabout their movie. But the comments will always\nbe about that she's sexy.",
    "start": "6138910",
    "end": "6148040"
  },
  {
    "text": "And this is what our\nmodel is able to surface. But again, it's\njust initial study.",
    "start": "6148040",
    "end": "6154110"
  },
  {
    "text": "Needs a lot of work. OK, so around here also\nasks, \"If microaggressions",
    "start": "6154110",
    "end": "6161840"
  },
  {
    "text": "are pulled from a site where\npeople can list what they have experienced, isn't that\ndata very vulnerable",
    "start": "6161840",
    "end": "6167330"
  },
  {
    "text": "to social engineering?\" ",
    "start": "6167330",
    "end": "6177020"
  },
  {
    "text": "Yes. This data is vulnerable. So in our case, we\nanonymize this data.",
    "start": "6177020",
    "end": "6184760"
  },
  {
    "text": "We extract only\nquotes from the data. We remove the actual\nusers who published it,",
    "start": "6184760",
    "end": "6192590"
  },
  {
    "text": "remove all the text\naround these quotes. And this is a good question.",
    "start": "6192590",
    "end": "6200469"
  },
  {
    "text": "Maybe we should also not make\nthis data public even yet. ",
    "start": "6200470",
    "end": "6207329"
  },
  {
    "text": "Hey. And there are more questions. Good questions. Thank you. More questions.",
    "start": "6207330",
    "end": "6212770"
  },
  {
    "text": "So-- By the way, like if-- Chris, John. I'm saying Chris, John because\nthese are the two faces,",
    "start": "6212770",
    "end": "6218850"
  },
  {
    "text": "the only faces that\nI see on my screen. So please let me know\nwhen we need to finish.",
    "start": "6218850",
    "end": "6224867"
  },
  {
    "text": "I'm happy to continue answering. We've often gone on\nfor a few more minutes. So I can ask a couple\nmore questions. OK.",
    "start": "6224867",
    "end": "6231050"
  },
  {
    "text": "So here's one that's\nvery prominent in AI right at the moment.",
    "start": "6231050",
    "end": "6237510"
  },
  {
    "text": "\"Do you think it is fair\nfor AI scientists in tech and academia, who\nare definitely not",
    "start": "6237510",
    "end": "6243030"
  },
  {
    "text": "representative of the\ngeneral population, to decide what is\nbiased and what is not,",
    "start": "6243030",
    "end": "6248610"
  },
  {
    "text": "i.e., the act of de-biasing\nitself might be biased?\" Yeah, this is one\nproblem also that--",
    "start": "6248610",
    "end": "6257820"
  },
  {
    "text": "this is like more general\nproblem, that researchers-- even those who work\non the bias, can",
    "start": "6257820",
    "end": "6262870"
  },
  {
    "text": "incorporate their own biases. ",
    "start": "6262870",
    "end": "6270559"
  },
  {
    "text": "We currently don't have any\nother alternative, right? We don't have a\ntraining how to do it.",
    "start": "6270560",
    "end": "6275680"
  },
  {
    "text": "We don't have-- I think it's a good thing\nto work on these topics, to try to promote these\ntopics as much as possible,",
    "start": "6275680",
    "end": "6284739"
  },
  {
    "text": "with an awareness that we as\nresearchers can incorporate our own biases.",
    "start": "6284740",
    "end": "6289790"
  },
  {
    "text": "So this is what we also, right,\nin the ethical implications",
    "start": "6289790",
    "end": "6295780"
  },
  {
    "text": "sections in the paper, that\nwe try to identify bias. We try to de-bias.",
    "start": "6295780",
    "end": "6301389"
  },
  {
    "text": "But there are\nlimitations to this study because we could incorporate our\nown biases into our analysis,",
    "start": "6301390",
    "end": "6308785"
  },
  {
    "text": "right? This is how we\ninterpreted these results. Maybe this is what\nwe were looking for",
    "start": "6308785",
    "end": "6314260"
  },
  {
    "text": "and this is a confirmation bias. ",
    "start": "6314260",
    "end": "6319950"
  },
  {
    "text": "Yeah. OK, maybe should\njust have one more--",
    "start": "6319950",
    "end": "6325989"
  },
  {
    "text": "oh, no. A new question just turned up. Maybe it'll have to\nbe two questions more. [INAUDIBLE] They're still coming in.",
    "start": "6325990",
    "end": "6332190"
  },
  {
    "text": " Now it's-- I mean, maybe I\nshould do that one immediately,",
    "start": "6332190",
    "end": "6338340"
  },
  {
    "text": "because it directly relates\nto that answer, which was,",
    "start": "6338340",
    "end": "6344710"
  },
  {
    "text": "\"How are the perspectives of\ncommunity stakeholders-- i.e., people from minoritized groups--\nincluded when these systems are",
    "start": "6344710",
    "end": "6351460"
  },
  {
    "text": "being built? This is a wonderful\nquestion too, yeah.",
    "start": "6351460",
    "end": "6357760"
  },
  {
    "text": "Currently, not very good. Actually, we currently\nhave a paper also in submission about analysis\nof how race have been treated",
    "start": "6357760",
    "end": "6366699"
  },
  {
    "text": "in NLP systems, starting\nfrom data sets to models to potential users.",
    "start": "6366700",
    "end": "6373100"
  },
  {
    "text": "And one of the\nthings that we found is that even people who\nwork on identifying racism,",
    "start": "6373100",
    "end": "6380989"
  },
  {
    "text": "they don't involve\nactually in-group members. ",
    "start": "6380990",
    "end": "6388060"
  },
  {
    "text": "Yeah, you identified yet another\nproblem in the community. The perspectives of community\nare not often incorporated.",
    "start": "6388060",
    "end": "6397070"
  },
  {
    "text": "In our acquisition paper, we try\nto advocate for its importance. But all these questions\nare very good.",
    "start": "6397070",
    "end": "6405159"
  },
  {
    "text": "But maybe somebody like Chris,\nwho has a lot of influence,",
    "start": "6405160",
    "end": "6411520"
  },
  {
    "text": "could make changes\nin the community. It's very difficult\nto make such changes.",
    "start": "6411520",
    "end": "6416650"
  },
  {
    "text": " I guess I'm hopeful\nthat there's actually starting to be a bit\nof change right now.",
    "start": "6416650",
    "end": "6424420"
  },
  {
    "text": "I mean, one can be\npessimistic given the history,",
    "start": "6424420",
    "end": "6430210"
  },
  {
    "text": "and one can be pessimistic\ngiven the current statistics.",
    "start": "6430210",
    "end": "6436690"
  },
  {
    "text": "But you know, I\nactually believe that through recent events of Black\nLives Matter and other things,",
    "start": "6436690",
    "end": "6444940"
  },
  {
    "text": "that there's actually\njust more genuine attempts",
    "start": "6444940",
    "end": "6450580"
  },
  {
    "text": "to create change around-- well, certainly the Stanford\ncomputer science department,",
    "start": "6450580",
    "end": "6457550"
  },
  {
    "text": "but I think more generally\naround the field of AI, than there's been at any\ntime in the past 30 years",
    "start": "6457550",
    "end": "6465310"
  },
  {
    "text": "when I've been watching it. Right. Even when I did my postdoc\nat Stanford in 2016,",
    "start": "6465310",
    "end": "6472930"
  },
  {
    "text": "we started working on the\nproblem of gender bias. And it was a total outlier. I didn't know if what I'm doing\nwill be relevant to anyone.",
    "start": "6472930",
    "end": "6482139"
  },
  {
    "text": "And then now look. We discuss this as a kind\nof relevant question. This is already an amazing\nchange in the community.",
    "start": "6482140",
    "end": "6489830"
  },
  {
    "text": "And if there will\nbe more focus also on the right hiring\nwhich clearly now has more awareness than ever.",
    "start": "6489830",
    "end": "6497400"
  },
  {
    "text": "Right. I'm also more optimistic now\nthan, say, three years ago.",
    "start": "6497400",
    "end": "6503159"
  },
  {
    "text": "OK, well maybe I'll do this-- But these are wonderful\nquestions for which there are no good answers yet, yeah.",
    "start": "6503160",
    "end": "6510170"
  },
  {
    "text": "Maybe you can do this\nas the last question, unless you say something that\nreally elicits a lot more.",
    "start": "6510170",
    "end": "6519680"
  },
  {
    "text": "\"What do you think\nthe social and ethics space might look like, say,\n5 to 10 years down the line?",
    "start": "6519680",
    "end": "6525410"
  },
  {
    "text": "Do you think the\nindustry might come down to a unified standard of\nethics for AI systems,",
    "start": "6525410",
    "end": "6531350"
  },
  {
    "text": "given that a lot of the\nchallenges come from the fact that social and\nethics discussions are often subjective?\"",
    "start": "6531350",
    "end": "6536880"
  },
  {
    "text": " Yeah, I'm also optimistic\nabout the field",
    "start": "6536880",
    "end": "6543230"
  },
  {
    "text": "of ethics in five years. These are difficult problems. The field of ethics,\nby the way, itself",
    "start": "6543230",
    "end": "6548840"
  },
  {
    "text": "is 2000 years old, right? Aristotle already\nasked these questions.",
    "start": "6548840",
    "end": "6554330"
  },
  {
    "text": "And now we're asking\nthese questions about AI. But given the current awareness\nand the bad publicity,",
    "start": "6554330",
    "end": "6568060"
  },
  {
    "text": "is that currently, companies\nare the main players, right? It's more even than governments.",
    "start": "6568060",
    "end": "6575200"
  },
  {
    "text": "And there is a big\nincentive at companies to fix things because\nof the bad publicity.",
    "start": "6575200",
    "end": "6583180"
  },
  {
    "text": "And for example, today\nI read an article",
    "start": "6583180",
    "end": "6588250"
  },
  {
    "text": "about that Google will\nstop advertisement",
    "start": "6588250",
    "end": "6597150"
  },
  {
    "text": "that track the profile\nusers, like these plug-ins.",
    "start": "6597150",
    "end": "6603290"
  },
  {
    "text": "So overall, I do see a\nvery positive trajectory. It's very difficult to\npredict what exactly",
    "start": "6603290",
    "end": "6608690"
  },
  {
    "text": "will be like in five years. I don't think all the\nproblems will be resolved. But overall, I'm optimistic also\nabout that new policies will",
    "start": "6608690",
    "end": "6621980"
  },
  {
    "text": "be already not entirely\nin the hands of decisions",
    "start": "6621980",
    "end": "6627170"
  },
  {
    "text": "of companies. And definitely about research,\nbecause I see how many students",
    "start": "6627170",
    "end": "6634170"
  },
  {
    "text": "now are interested in\nthese topics, which is totally amazing. ",
    "start": "6634170",
    "end": "6641480"
  },
  {
    "text": "OK. Oh, and another comment that\nI want to make is actually, NLP is important in all this,\nwhich was much less said.",
    "start": "6641480",
    "end": "6649510"
  },
  {
    "text": "The field of fairness\nvery much focused on the image recognition. But I think more and more,\nwe'll see more and more research",
    "start": "6649510",
    "end": "6656260"
  },
  {
    "text": "on NLP and language,\nwhich is also exciting. ",
    "start": "6656260",
    "end": "6662610"
  },
  {
    "text": "OK, well maybe we should\ncall it quits at that point. Thank you so much, Yulia.",
    "start": "6662610",
    "end": "6668489"
  },
  {
    "text": "Thank you very much. ",
    "start": "6668490",
    "end": "6675000"
  }
]