[
  {
    "text": "Hey, everyone. I guess let's get started. Please take a seat.",
    "start": "5170",
    "end": "12150"
  },
  {
    "text": "So today we're going to\ntalk about kernel method. So I'm going to take a\nsec, define what it is.",
    "start": "12150",
    "end": "17789"
  },
  {
    "text": "So in some sense, this\nis a general technique for dealing with\nnonlinearity in your data.",
    "start": "17789",
    "end": "24539"
  },
  {
    "text": "So we're going to see more\nother general techniques like neural networks.",
    "start": "24539",
    "end": "30310"
  },
  {
    "text": "But this is one\nof the techniques that we will deal\nwith nonlinearity before deep learning took off.",
    "start": "30310",
    "end": "36500"
  },
  {
    "text": "And we will introduce\nthis technique mostly in the setting\nof supervised learning in the kind of discriminative\nalgorithm type of settings.",
    "start": "36500",
    "end": "45820"
  },
  {
    "text": "But these kind of\ntechniques can also applies to enterprise\nlearning or other settings,",
    "start": "45820",
    "end": "51570"
  },
  {
    "text": "just with similar type of ideas. OK, so I guess,\nlet me start by--",
    "start": "51570",
    "end": "57510"
  },
  {
    "text": "I guess start with\na random example. So suppose you have some data. I guess I'm thinking about\nsupervised learning-- just",
    "start": "57510",
    "end": "63950"
  },
  {
    "text": "a simple regression settings,\nwhere you have some x",
    "start": "63950",
    "end": "69220"
  },
  {
    "text": "and you have some y. And you have some data--\nsomething like this. I'm just drawing something.",
    "start": "69220",
    "end": "75750"
  },
  {
    "text": "I'm making up some data. And in the first\ntwo weeks, I think",
    "start": "75750",
    "end": "81111"
  },
  {
    "text": "Chris talked about, you fit\na linear regression, right? So that means that you have to\nfit some linear model on top",
    "start": "81111",
    "end": "87170"
  },
  {
    "text": "of this, so. But you can see that I\ndesigned this data such that a linear model is\nprobably not correct.",
    "start": "87170",
    "end": "94460"
  },
  {
    "text": "So apparently, in\nthis data, you may want to use something like\nmaybe a cubic polynomial.",
    "start": "94460",
    "end": "102610"
  },
  {
    "text": "And if you have\ndone the homework, I think there's one more\nquestion, which actually also talks about how do you\ndeal with this kind of situation",
    "start": "102610",
    "end": "110960"
  },
  {
    "text": "where your data is\nnot linear in x? So you shouldn't assume\na linear model in x.",
    "start": "110960",
    "end": "117840"
  },
  {
    "text": "The homework is still\nthis Wednesday, right? Yeah. So it's one of the last\nhomework questions.",
    "start": "117840",
    "end": "122920"
  },
  {
    "text": "But you don't have to know\nexactly the homework questions. I'm going to actually basically\nkind of review the homework",
    "start": "122920",
    "end": "128649"
  },
  {
    "text": "questions to some extent. So how do you deal with\nthis kind of case, right? So a simple way to do\nit is the following.",
    "start": "128649",
    "end": "136050"
  },
  {
    "text": "So you say I'm going to\nhave a cubic polynomial to fit this data. So basically, I'm\ngoing to assume",
    "start": "136050",
    "end": "141660"
  },
  {
    "text": "my model is something like h\ntheta of x is a function of x power matches by theta.",
    "start": "141660",
    "end": "146950"
  },
  {
    "text": "So it's something\nlike theta 3 times x cubed plus theta\nsquared times x squared--",
    "start": "146950",
    "end": "153230"
  },
  {
    "text": "sorry, theta 2 times x squared\nplus theta 1 x1 plus theta 0.",
    "start": "153230",
    "end": "159170"
  },
  {
    "text": "So here, my x is\na real number just for the sake of simplicity.",
    "start": "159170",
    "end": "164709"
  },
  {
    "text": "So I have this nonlinear model.",
    "start": "164709",
    "end": "170550"
  },
  {
    "text": "And sorry, any questions?",
    "start": "170550",
    "end": "176400"
  },
  {
    "text": "OK. So you fit this, right? So how do you deal with this?",
    "start": "176400",
    "end": "182780"
  },
  {
    "text": "So one thing to realize\nis that either you have such a model that\nis nonlinear in x.",
    "start": "182780",
    "end": "187819"
  },
  {
    "text": "But actually, this\nis linear in theta,",
    "start": "187819",
    "end": "194240"
  },
  {
    "text": "so linear function of theta. It's a nonlinear function of x.",
    "start": "194240",
    "end": "203269"
  },
  {
    "text": "But actually, when you\nreally do linear regression, it really doesn't matter\nwhether it's linear in x or not.",
    "start": "203269",
    "end": "208879"
  },
  {
    "text": "What really matter\nis whether you are linear in your parameter\nbecause you are optimizing over the parameter space. So in other words, more\nexplicitly, so what you can do",
    "start": "208879",
    "end": "216709"
  },
  {
    "text": "is that you can turn\nthis into a linear, a standard linear\nregression problem by doing a simple change.",
    "start": "216709",
    "end": "224330"
  },
  {
    "text": "So you can say that I'm going to\ndefine this function, phi of x,",
    "start": "224330",
    "end": "231500"
  },
  {
    "text": "to be this function that\nmaps x to this vector.",
    "start": "231500",
    "end": "238020"
  },
  {
    "text": "So this function,\nmaybe I should've called it phi of-- maybe\nlet's just call it phi. Phi is a function\nthat maps R to R4.",
    "start": "238020",
    "end": "247900"
  },
  {
    "text": "And then you can rewrite this\nnonlinear model, nonlinear x",
    "start": "247900",
    "end": "255430"
  },
  {
    "text": "to be something more\nlike a linear model. This is almost exactly the\nsame as the linear model",
    "start": "255430",
    "end": "263410"
  },
  {
    "text": "we have talked about, right? So this is a vector. Theta 1's something like this\ntimes 1, x, x squared, x cubed,",
    "start": "263410",
    "end": "275460"
  },
  {
    "text": "which you can write this as\ntheta transpose times phi of x.",
    "start": "275460",
    "end": "283620"
  },
  {
    "text": "So in some sense, after you\ndo this simple rewriting, right, now h theta x\nis linear in theta,",
    "start": "283620",
    "end": "294639"
  },
  {
    "text": "is still linear in theta. And also, it's linear in\nthe coordinate of phi of x.",
    "start": "294639",
    "end": "302870"
  },
  {
    "text": "So this is how you\nmake everything linear. And then you can, in some sense,\ninvoke the linear regression",
    "start": "302870",
    "end": "308138"
  },
  {
    "text": "algorithm that you have used. What does that mean? That means that you\nbasically can just",
    "start": "308139",
    "end": "313388"
  },
  {
    "text": "view this as your new input. So before, input was x. And now, input becomes\nhigher dimensional.",
    "start": "313389",
    "end": "319900"
  },
  {
    "text": "Before, input was\none-dimensional x, right? And now, input becomes\nphi of x, which",
    "start": "319900",
    "end": "325811"
  },
  {
    "text": "is a four-dimensional vector. And that's it.",
    "start": "325811",
    "end": "330889"
  },
  {
    "text": "So in other words,\nsuppose before we have a data set, which\nis x1, y1, up to xn,",
    "start": "330889",
    "end": "337640"
  },
  {
    "text": "yn, where x is a scalar-- the input is scalar-- y i\nis always a scalar, I say.",
    "start": "337640",
    "end": "346039"
  },
  {
    "text": "And now, in some sense,\nyou just turn this into-- you can turn this\ninto a new data set.",
    "start": "346040",
    "end": "358050"
  },
  {
    "text": "And this new data\nset is going to be--",
    "start": "358050",
    "end": "364849"
  },
  {
    "text": "the input will be phi of x i. And the label or the\noutput is the same.",
    "start": "364850",
    "end": "377680"
  },
  {
    "text": "Of course, your\ninput dimensionality becomes 4 now instead of 1. But we are able to deal\nwith high-dimensional input",
    "start": "377680",
    "end": "384520"
  },
  {
    "text": "for linear regression. So basically, you can just now\nview this as our new inputs.",
    "start": "384520",
    "end": "395330"
  },
  {
    "text": "Or sometimes people\ncall it features. I guess, probably, we\nhave talked about this.",
    "start": "395330",
    "end": "402870"
  },
  {
    "text": "Features sometimes\nis a word that has multiple or similar\nmeanings in machine learning.",
    "start": "402870",
    "end": "413539"
  },
  {
    "text": "So in many cases, feature\njust means inputs. But I'll clarify in a\nlater set of situations.",
    "start": "413539",
    "end": "420560"
  },
  {
    "text": "Sometimes features means\nslightly different things. But sometimes you can just\nsay this is a new input.",
    "start": "420560",
    "end": "426770"
  },
  {
    "text": "This is a new output. The new output will be the same. And you just work on linear\nregression on a new data set.",
    "start": "426770",
    "end": "432690"
  },
  {
    "text": "And that's how you learn a cubic\nfunction for this data set-- by turning it back\nand reducing it",
    "start": "432690",
    "end": "439418"
  },
  {
    "text": "to a linear regression problem. And I think this is a piece\nthat, one, I think it's Q4.",
    "start": "439419",
    "end": "449770"
  },
  {
    "text": "But maybe we change the order\nof the questions a little bit.",
    "start": "449770",
    "end": "454830"
  },
  {
    "text": "Right, any question so far? How do you exactly\nget to know the data?",
    "start": "454830",
    "end": "466039"
  },
  {
    "text": "This might actually be better\nif you apply that phi function. Do we just experiment on\nall of the data to see?",
    "start": "466039",
    "end": "472820"
  },
  {
    "text": "How do I know this is\nbetter than the what? Yeah, how do we know\nthat it might be better",
    "start": "472820",
    "end": "481450"
  },
  {
    "text": "to apply to this\ntransformation of the data? Than no transformation? Yep.",
    "start": "481450",
    "end": "487400"
  },
  {
    "text": "OK, so yeah, so I\nguess it's just-- of course, eventually, you\nhave to validate your method",
    "start": "487400",
    "end": "492979"
  },
  {
    "text": "and see how it works. But the intuition-- so here,\nI'm not saying it's better. I'm just saying\nthat if you believe",
    "start": "492979",
    "end": "498760"
  },
  {
    "text": "that you should use\na cubic function, this is how you\nimplement it, right? So whether you believe\nthat you should use a cubic function instead\nof a linear function,",
    "start": "498760",
    "end": "504620"
  },
  {
    "text": "that depends on the\nproperty of data. Maybe you should\nexperiment with it. You should probably look\nat data to see whether it",
    "start": "504620",
    "end": "510639"
  },
  {
    "text": "is a linear function. There are other ways to decide\nwhich model we're going to use. I think, actually, in two\nor three lectures later,",
    "start": "510639",
    "end": "517900"
  },
  {
    "text": "we are going to talk about how\ndo you pick different models. OK. Yeah.",
    "start": "517900",
    "end": "523630"
  },
  {
    "text": "OK, so yeah, so exactly. So this is a way to implement\na cubic function at least",
    "start": "523630",
    "end": "530800"
  },
  {
    "text": "for one dimension, OK? And what you can do\nis the following.",
    "start": "530800",
    "end": "538870"
  },
  {
    "text": "So how do you proceed, right? So you basically\njust repeat what we have known about linear\nregression-- you just",
    "start": "538870",
    "end": "546670"
  },
  {
    "text": "write down algorithm on\nthis data set, right, and implement a linear\nregression algorithm",
    "start": "546670",
    "end": "552290"
  },
  {
    "text": "on this data set. I'm going to repeat\nthe procedures. But this is basically\njust exactly-- you just invoke what we have\nlearned from the first two",
    "start": "552290",
    "end": "560420"
  },
  {
    "text": "weeks. But I'm going to repeat\nit because this part, I'm going to use it. We use it again to\ndemonstrate something else,",
    "start": "560420",
    "end": "567190"
  },
  {
    "text": "to demonstrate the\ncurrent metric. So how do we redo this? So if you do linear regression\non this new data set--",
    "start": "567190",
    "end": "585140"
  },
  {
    "text": "so guys, let's say we\nuse gradient descent, OK? So let's use gradient descent.",
    "start": "585140",
    "end": "591310"
  },
  {
    "text": "And so on the loss function,\nsomething like this, right?",
    "start": "591310",
    "end": "602190"
  },
  {
    "text": "You compare your label with-- you know, suppose you compare\na label with the model, which",
    "start": "602190",
    "end": "617911"
  },
  {
    "text": "is phi of x i and squared.",
    "start": "617911",
    "end": "623320"
  },
  {
    "text": "Recall that if you do the\nstandard linear regression, what will happen is that\nfor the standard case,",
    "start": "623320",
    "end": "630230"
  },
  {
    "text": "then this will be x i. And this will be x i in\nthe most standard case.",
    "start": "630230",
    "end": "636550"
  },
  {
    "text": "And what I'm doing\nhere is just replacing that x i by phi of x i. And then, if you\nwork with it, and you",
    "start": "636550",
    "end": "646199"
  },
  {
    "text": "said, what's the\nformula, the formula would be something like-- we\nare going to have a loop, right?",
    "start": "646200",
    "end": "651940"
  },
  {
    "text": "And each time, you are\nupdating your theta by theta plus some learning\nrate times the gradient.",
    "start": "651940",
    "end": "663670"
  },
  {
    "text": "And the gradient,\nif you compute it, it will be something like\ny i minus theta transpose",
    "start": "663670",
    "end": "670510"
  },
  {
    "text": "phi of x i times phi of x i.",
    "start": "670510",
    "end": "679740"
  },
  {
    "text": "And for the standard case\nthen, this two things will just be both x i.",
    "start": "679740",
    "end": "685300"
  },
  {
    "text": "If you look up\nthe lecture notes, they will be just both x i. And now they becomes this\ntransformed version of this.",
    "start": "685300",
    "end": "693000"
  },
  {
    "text": "I guess the [? question, ?]\nthis notation, like this is supposed to be that\nyou update your theta by theta",
    "start": "693000",
    "end": "698779"
  },
  {
    "text": "plus this. Does the notation make sense? Yeah, OK. Sounds good. So in some sense,\nit's just a shorthand.",
    "start": "698779",
    "end": "704910"
  },
  {
    "text": "If you really want to be very-- if you read it in a mathematical\npaper, maybe they will call it",
    "start": "704910",
    "end": "711300"
  },
  {
    "text": "theta t plus y is equal\nto theta t plus this. [INAUDIBLE] x i if\nyou put all of that.",
    "start": "711300",
    "end": "718170"
  },
  {
    "text": "Is it the same? Oh, I'm saying that\nin a standard case, this will be x i, in\nthe first two weeks.",
    "start": "718170",
    "end": "725790"
  },
  {
    "text": "Now I just replaced it, yeah. All right, OK, so.",
    "start": "725790",
    "end": "733300"
  },
  {
    "text": "And there is some\nsmall differences, which is that if in the\nstandard case-- so now, what's the dimension of theta?",
    "start": "733300",
    "end": "739430"
  },
  {
    "text": "The dimension of\ntheta is now the same as the dimension\nof the x, right? So suppose you say x\nis of dimension Rd--",
    "start": "739430",
    "end": "748589"
  },
  {
    "text": "maybe I should use\nblack color just to be more consistent, so-- and phi of x is a\nfunction that maps",
    "start": "748590",
    "end": "756880"
  },
  {
    "text": "Rd to some other dimension. It doesn't have to be\nexactly the same dimension. But before, we map from one\ndimension to four dimension.",
    "start": "756880",
    "end": "764620"
  },
  {
    "text": "But you can, from any\ndimension to another dimension. So let's call this dimension p. So p is the dimension of the\nso-called the new feature,",
    "start": "764620",
    "end": "772730"
  },
  {
    "text": "the new inputs. And that means that\nyour theta also has to be in the\nsame space as phi",
    "start": "772730",
    "end": "778829"
  },
  {
    "text": "because your theta will\nbe a linear combination of the transformed inputs.",
    "start": "778829",
    "end": "784250"
  },
  {
    "text": "So theta is also\nliving in this space. So basically, we are updating\nin this p-dimensional space",
    "start": "784250",
    "end": "792450"
  },
  {
    "text": "instead of x-dimensional space. And \nlet me also just briefly",
    "start": "792450",
    "end": "803370"
  },
  {
    "text": "talk about the terminologies. So I think this is often\ncalled a feature map, so.",
    "start": "803370",
    "end": "814779"
  },
  {
    "text": "And this phi of x-- maybe I think I should call\nthis something like this.",
    "start": "814779",
    "end": "820980"
  },
  {
    "text": "Maybe the right\nnotation should be this. So phi, as a function,\nit maps from Rd to Rp.",
    "start": "820980",
    "end": "826699"
  },
  {
    "text": "And phi of x, I\nthink people often call this, at least in this\ncontext, call these features.",
    "start": "826699",
    "end": "832930"
  },
  {
    "text": "And if you look at this kind\nof like papers or research, in this context,\noften, people call",
    "start": "832930",
    "end": "838529"
  },
  {
    "text": "x the attributes just\nto distinguish it from the features.",
    "start": "838529",
    "end": "844529"
  },
  {
    "text": "The terminology doesn't\nreally matter that much. In most of cases, you can\ninfer the terms if you really",
    "start": "844529",
    "end": "851510"
  },
  {
    "text": "know what they mean, so. But I'm just giving you\nsome kind of context",
    "start": "851510",
    "end": "857579"
  },
  {
    "text": "when, if you read\nthe paper which says that x is the attributes\nand phi of x, the features, then you know what they mean.",
    "start": "857579",
    "end": "862720"
  },
  {
    "text": "But sometimes other papers\nwould call it differently. Maybe some people would\ncall x raw features and call these features.",
    "start": "862720",
    "end": "868149"
  },
  {
    "text": "There are multiple\nways to name them, which is a little unfortunate.",
    "start": "868149",
    "end": "873340"
  },
  {
    "text": "OK, anyway, but feature map-- I think people always\ncall this feature map. Maybe sometimes we call\nit this feature extractor.",
    "start": "873340",
    "end": "880200"
  },
  {
    "text": "But they all mean\nthe same thing. And regarding this\nword features,",
    "start": "880200",
    "end": "886570"
  },
  {
    "text": "I guess maybe let's just-- so my\nextrapolation from reading all of these papers, I\nrealized that features,",
    "start": "886570",
    "end": "894520"
  },
  {
    "text": "even though sometimes they\nmean different things, often, at least from my\nstatistical learning, inference of this word, I\nthink this word seems to mean--",
    "start": "894520",
    "end": "902070"
  },
  {
    "text": "it always means that something\nthat you use a linear function on top of this.",
    "start": "902070",
    "end": "908070"
  },
  {
    "text": "So basically, if there\nis a linear thing on top of something,\nthen that something will be called features.",
    "start": "908070",
    "end": "917220"
  },
  {
    "text": "So basically, it often refers\nto those variables in which your model is linear, yeah.",
    "start": "917220",
    "end": "925850"
  },
  {
    "text": "OK, so are we done, right? So this sounds great, right?",
    "start": "925850",
    "end": "931060"
  },
  {
    "text": "So everything is so\nclear and simple. Nothing seems to be complicated. You just replace\nx i by phi of x i. But we are not done.",
    "start": "931060",
    "end": "938310"
  },
  {
    "text": "So why? Because in some\ncases, this is great.",
    "start": "938310",
    "end": "944000"
  },
  {
    "text": "For example, in the example\nof the homework question, if you look at it, we\nbasically ask you exactly",
    "start": "944000",
    "end": "949110"
  },
  {
    "text": "implement something like this. I think we ask you to implement\na different algorithm. We didn't ask you to implement\nthe gradient descent.",
    "start": "949110",
    "end": "954209"
  },
  {
    "text": "Instead, we ask you\nto implement the-- you use the exact solver. You use the inverse of\nthe tan, some matrix,",
    "start": "954210",
    "end": "962639"
  },
  {
    "text": "some gradients kind of\nthings, but the same thing, where you just\napply our existing",
    "start": "962639",
    "end": "968089"
  },
  {
    "text": "algorithm on this new data set. But this is not done because\nof the efficiency issue.",
    "start": "968089",
    "end": "973960"
  },
  {
    "text": "But I think I saw a question. Yeah? So why is it [INAUDIBLE]? So why is 1/2 here?",
    "start": "973960",
    "end": "979579"
  },
  {
    "text": "Yeah. I think this is so that the\ngradient doesn't have 1/2. It doesn't really matter. But if you have 1/2 in front\nof the grid, there's no 2.",
    "start": "979579",
    "end": "989440"
  },
  {
    "text": "It's just convention, yeah. I think if you read the first\ntwo weeks, also, we have a 1/2.",
    "start": "989440",
    "end": "998220"
  },
  {
    "text": "Typically, there is 1/2. OK, so-- all right. OK, so why we are not done.",
    "start": "998220",
    "end": "1004319"
  },
  {
    "text": "The reason is that\nsometimes there's a efficiency issue with\nthis kind of approach.",
    "start": "1004319",
    "end": "1009860"
  },
  {
    "text": "And the reason is\nthat this feature map can be very high-dimensional. So p can be very\nlarge in some cases.",
    "start": "1009860",
    "end": "1020089"
  },
  {
    "text": "For example, suppose\nin this case, p is 4, which sounds pretty fun.",
    "start": "1020089",
    "end": "1027168"
  },
  {
    "text": "But what if your x\nis high-dimensional? So here, x is one-dimensional. And p, the features,\nare four-dimensional.",
    "start": "1027169",
    "end": "1034610"
  },
  {
    "text": "But what if you say I'm going\nto have x1 up to xd, right?",
    "start": "1034610",
    "end": "1039980"
  },
  {
    "text": "I have d columns in my row data. And now if I want\nto create something",
    "start": "1039980",
    "end": "1046949"
  },
  {
    "text": "like this, that have\nall the cubic monomials",
    "start": "1046949",
    "end": "1053200"
  },
  {
    "text": "of the coordinates,\nthen what you have do is that you're going\nto have something like phi of x needs\nto be something",
    "start": "1053200",
    "end": "1060049"
  },
  {
    "text": "like a gigantic vector,\nwhere maybe you have 1 here. You have x1, x2--",
    "start": "1060049",
    "end": "1065840"
  },
  {
    "text": "all the degree 1 monomials\nof the coordinates.",
    "start": "1065840",
    "end": "1071880"
  },
  {
    "text": "And then you have the\ndegree 2 thing, x1 x2. Maybe we should start\nwith x1 squared--",
    "start": "1071880",
    "end": "1077440"
  },
  {
    "text": "x1 times x1, x1 x2,\nup to x1 xd, and then x2 x1, so on and so forth.",
    "start": "1077440",
    "end": "1085440"
  },
  {
    "text": "Maybe x2 x1 can be omitted\nbecause there is a repetition. But it doesn't really\nchange the point.",
    "start": "1085440",
    "end": "1090600"
  },
  {
    "text": "So you're going to need to have\na very long vector that list all the possible combinations,\nthe products of this",
    "start": "1090600",
    "end": "1096460"
  },
  {
    "text": "coordinates. So you're going to have maybe\nx1 cubed up to eventually, xd",
    "start": "1096460",
    "end": "1102970"
  },
  {
    "text": "cubed, something like this, so. And only when you\nhave this, then",
    "start": "1102970",
    "end": "1112519"
  },
  {
    "text": "you can claim all the\ndegree 3 polynomials can be written as a linear\nfunction of these features,",
    "start": "1112520",
    "end": "1121860"
  },
  {
    "text": "right? And then what is p?",
    "start": "1121860",
    "end": "1127289"
  },
  {
    "text": "What's the dimensionality\nof this new features? So if you count it-- so I'm going to do\nsome rough counting.",
    "start": "1127289",
    "end": "1134179"
  },
  {
    "text": "So this one, there's 1. And for this degree 1 thing,\nyou have d of this, right?",
    "start": "1134180",
    "end": "1141549"
  },
  {
    "text": "And for degree 2\nthing, I guess-- so you're going to have\ndegree 2 thing maybe.",
    "start": "1141549",
    "end": "1149090"
  },
  {
    "text": "So they are d squared of this. Let's say we ignore\nthe repetitions. The repetitions only a\nchange a constant factor,",
    "start": "1149090",
    "end": "1154480"
  },
  {
    "text": "which doesn't really\nchange the point. So you have d squared. And then for degree 3 thing,\nyou're going to start from x1,",
    "start": "1154480",
    "end": "1161480"
  },
  {
    "text": "x1 cubed to xd cubed. And with all the\ncombinations, you're going to have d\ncubed of these terms.",
    "start": "1161480",
    "end": "1168260"
  },
  {
    "text": "I guess this is a little bit--",
    "start": "1168260",
    "end": "1175360"
  },
  {
    "text": "OK. Right, so then this means\nthat even though-- of course--",
    "start": "1175360",
    "end": "1184640"
  },
  {
    "text": "the good thing is that\nevery degree 3 polynomial can be written as theta\ntranspose phi of x.",
    "start": "1184640",
    "end": "1196390"
  },
  {
    "text": "But the bad news\nis that phi of x",
    "start": "1196390",
    "end": "1203380"
  },
  {
    "text": "is in this Rp, where p is\nsomething like d cubed.",
    "start": "1203380",
    "end": "1213340"
  },
  {
    "text": "Technically, p\nis, in this case-- if I do this p is\nequal to 1 plus d--",
    "start": "1213340",
    "end": "1220419"
  },
  {
    "text": "plus d squared plus d cubed,\nthe dominating term is d cubed.",
    "start": "1220419",
    "end": "1225509"
  },
  {
    "text": "And this is very\nhigh-dimensional vector. So if you think about this--",
    "start": "1225510",
    "end": "1230590"
  },
  {
    "text": "for example, let's\nsay maybe suppose",
    "start": "1230590",
    "end": "1236669"
  },
  {
    "text": "it's just some\nback-of-envelope calculation. Suppose d is 10 to the\nthird-- say, 1,000--",
    "start": "1236669",
    "end": "1243630"
  },
  {
    "text": "then p will be a billion, so.",
    "start": "1243630",
    "end": "1249669"
  },
  {
    "text": "And why this is a\nproblem, this is a problem because if you run algorithm,\nthis algorithm, then how many",
    "start": "1249669",
    "end": "1257570"
  },
  {
    "text": "iterations, how much computation\nyou have to pay here? So if we look at\nthis, I guess if you",
    "start": "1257570",
    "end": "1269190"
  },
  {
    "text": "count how much computation\nwe have to pay, this product requires O\nof p computation, O of p",
    "start": "1269190",
    "end": "1278520"
  },
  {
    "text": "so you are taking inner\nproduct of two vectors, two",
    "start": "1278520",
    "end": "1285081"
  },
  {
    "text": "p-dimensional vectors. You have to pay p numerical\noperations, p multiplications.",
    "start": "1285081",
    "end": "1291820"
  },
  {
    "text": "And you also have to sum. The sum also has p operations. So it's O of p operations\nfor this in the product.",
    "start": "1291820",
    "end": "1297980"
  },
  {
    "text": "And then it becomes scalar. And you take the difference\nbetween this and this. That's fine.",
    "start": "1297980",
    "end": "1303120"
  },
  {
    "text": "And then you multiply\nby this, which also-- this is scalar times this, which\nstill takes O of p operations.",
    "start": "1303120",
    "end": "1310440"
  },
  {
    "text": "So evaluating this whole\nthing takes O of p operations. That's still kind of OK. But you have to\nhave-- take the sum.",
    "start": "1310440",
    "end": "1317559"
  },
  {
    "text": "And that's a\nmultiplicative factor because you have to\ndo it repeatedly. So totally evaluating\nthe whole thing",
    "start": "1317559",
    "end": "1323169"
  },
  {
    "text": "takes O of np\nmultiplications or additions.",
    "start": "1323169",
    "end": "1330820"
  },
  {
    "text": "So you have to take\nO of n times p time. So basically, the runtime\ndepends a lot on p.",
    "start": "1330820",
    "end": "1336880"
  },
  {
    "text": "It's linear in p. And if p is something\nlike 10 to the 9, then your runtime will\nbe 10 to the 9 times",
    "start": "1336880",
    "end": "1342960"
  },
  {
    "text": "the number of samples,\nwhich is often prohibitive. So this is very,\nvery slow empirically",
    "start": "1342960",
    "end": "1351840"
  },
  {
    "text": "if you really just implement\nthis, when d is a solid.",
    "start": "1351840",
    "end": "1357200"
  },
  {
    "text": "Any question so far? OK, so the main\ngoal of this lecture",
    "start": "1357200",
    "end": "1367970"
  },
  {
    "text": "is to find out a way to\nspeed up this algorithm. That's the kernel trick.",
    "start": "1367970",
    "end": "1373610"
  },
  {
    "text": "What's the kernel trick about? It's about how to implement this\nnonlinear model using this idea",
    "start": "1373610",
    "end": "1381820"
  },
  {
    "text": "but implementing\nit in a way such that you can be\ncomputationally efficient.",
    "start": "1381820",
    "end": "1387060"
  },
  {
    "text": "So the final algorithm will be\nequivalent to this algorithm but will just be faster and\nactually much, much faster.",
    "start": "1387060",
    "end": "1394780"
  },
  {
    "text": "So we're don't\ngoing to manipulate [INAUDIBLE] to be enough to\nfocus on that [INAUDIBLE]..",
    "start": "1394780",
    "end": "1403910"
  },
  {
    "text": "And phi is going\nto stay the same. We are going to try to\nimplement this algorithm",
    "start": "1403910",
    "end": "1411470"
  },
  {
    "text": "in some other ways. But you'll see phi will-- I'm not sure whether\nI would say yes to--",
    "start": "1411470",
    "end": "1418440"
  },
  {
    "text": "we will do something\nwith phi, yeah. Right, so the whole point\nof the rest of the lecture",
    "start": "1418440",
    "end": "1427620"
  },
  {
    "text": "is to have a faster\nalgorithm and maybe just",
    "start": "1427620",
    "end": "1434830"
  },
  {
    "text": "a side philosophical remark. I think machine learning\nis really a lot about--",
    "start": "1434830",
    "end": "1440159"
  },
  {
    "text": "machine learning is about\ncomputational efficiency, even though these days,\nsometimes you can use GPUs, so.",
    "start": "1440159",
    "end": "1447010"
  },
  {
    "text": "But I think at the\nheart, at least a good fraction of\nmachine learning is about computational\nefficiency",
    "start": "1447010",
    "end": "1452279"
  },
  {
    "text": "because many of these kind\nof statistical questions, in some sense, you can say they\nare studied well in some sense,",
    "start": "1452279",
    "end": "1459890"
  },
  {
    "text": "like in statistics. And at least in\nmachine learning, I think, at least\ncomparatively, I",
    "start": "1459890",
    "end": "1467960"
  },
  {
    "text": "think we focus a little bit\nmore on computational part. But of course, computational\nis only one part, right?",
    "start": "1467960",
    "end": "1473010"
  },
  {
    "text": "So you also have to care\nabout statistical perspective. But I'm just saying that\ncomputation is important.",
    "start": "1473010",
    "end": "1479361"
  },
  {
    "text": "It's not just a merely\nor a separate thing that you can resort\nto an oracle.",
    "start": "1479361",
    "end": "1485160"
  },
  {
    "text": "So in many things, you have to\nreally think about computation because otherwise, you cannot\nimplement the algorithm.",
    "start": "1485160",
    "end": "1490669"
  },
  {
    "text": "You cannot run your\nalgorithm on a large dataset. Then you wouldn't\nsee good results.",
    "start": "1490669",
    "end": "1495799"
  },
  {
    "text": "And this is one way\nto speed up things. And we are going\nto see other ways.",
    "start": "1495799",
    "end": "1501410"
  },
  {
    "text": "OK, so how do we speed up this. So here is a key observation\nto speed up this.",
    "start": "1501410",
    "end": "1507830"
  },
  {
    "text": "So \nthe observation is",
    "start": "1507830",
    "end": "1516470"
  },
  {
    "text": "that the theta can always be represented as--",
    "start": "1516470",
    "end": "1530830"
  },
  {
    "text": "you may find this a little\nbit kind of surprising on the first side. But I'm going to explain.",
    "start": "1530830",
    "end": "1542990"
  },
  {
    "text": "So this vector\ntheta can always be represented as a linear\ncombination of the features",
    "start": "1542990",
    "end": "1551440"
  },
  {
    "text": "for some scalar. Or this beta i is-- the scalar is for the linear\ncombination for some beta 1",
    "start": "1551440",
    "end": "1561049"
  },
  {
    "text": "up to beta n. And each of these\nbeta is in R. And I",
    "start": "1561050",
    "end": "1568130"
  },
  {
    "text": "guess there's a condition\nhere-- so if theta 0 is 0.",
    "start": "1568130",
    "end": "1575010"
  },
  {
    "text": "So I guess I'm\ngoing to change just for the sake of simplicity. I'm just going to\ndecide my initialization",
    "start": "1575010",
    "end": "1581039"
  },
  {
    "text": "for this algorithm is\nthat theta is equal to 0, is initialized to 0.",
    "start": "1581040",
    "end": "1586400"
  },
  {
    "text": "So then I'm not going\nto have this condition. So I'm going to claim-- my algorithm-- I'm going to\nsay that my algorithm just",
    "start": "1586400",
    "end": "1593250"
  },
  {
    "text": "start with theta is 0. In initialization,\nprobably doesn't matter that much\nbecause either way, you're going to update\na lot of times, so.",
    "start": "1593250",
    "end": "1599450"
  },
  {
    "text": "So if you start with 0, then in\nthis algorithm, no matter what",
    "start": "1599450",
    "end": "1606570"
  },
  {
    "text": "time, theta can\nalways be represented as a linear combination\nof our features.",
    "start": "1606570",
    "end": "1613950"
  },
  {
    "text": "And why this is useful,\nwhy will this be useful? You will see roughly\nspeaking, the reason why",
    "start": "1613950",
    "end": "1619690"
  },
  {
    "text": "this is useful is because theta\nis a p-dimensional vector, right?",
    "start": "1619690",
    "end": "1624700"
  },
  {
    "text": "And I'm in a regime\nwhere p is probably-- I'm in a regime where regime\nis that p is larger than n.",
    "start": "1624700",
    "end": "1637509"
  },
  {
    "text": "So let's say p is really, really\nbig, extremely big, right, even bigger than n. So theta is a p-dimensional\nvector, which is extremely big.",
    "start": "1637510",
    "end": "1645580"
  },
  {
    "text": "But now you represent this\nvector by the scalars. And you have n scalars.",
    "start": "1645580",
    "end": "1651140"
  },
  {
    "text": "So in some sense, you\nreduce the degree of freedom in your representation\nat least, right? So before, we have to\nrepresent p numbers.",
    "start": "1651140",
    "end": "1658150"
  },
  {
    "text": "But now you only have to store\nn numbers if this claim is true.",
    "start": "1658150",
    "end": "1663529"
  },
  {
    "text": "So in some sense,\nyou will see that-- I will tell you\nmore about details. In some sense, you'll see\nthat the way we speed up this",
    "start": "1663529",
    "end": "1670160"
  },
  {
    "text": "is that we never store\ntheta explicitly. We always store\nthe representations of theta, where the beta i\nis as my surrogate for theta.",
    "start": "1670160",
    "end": "1680580"
  },
  {
    "text": "And that's enough for\nmany of my computation. You create a [INAUDIBLE]\nvector, [INAUDIBLE]",
    "start": "1680580",
    "end": "1687158"
  },
  {
    "text": "isn't that the scalar? Isn't the sum there is scalar?",
    "start": "1687159",
    "end": "1692549"
  },
  {
    "text": "This is a scalar. But this is a vector, right? This is a p-dimensional vector.",
    "start": "1692549",
    "end": "1698460"
  },
  {
    "text": "So this is scalar, so. But you don't have to store this\nbecause these are already there in some sense.",
    "start": "1698460",
    "end": "1703970"
  },
  {
    "text": "These are not\nchanging even, right? So only the beta\nwill be changing, where beta is a surrogate\nfor theta in some sense.",
    "start": "1703970",
    "end": "1712309"
  },
  {
    "text": "So you don't have to\nstore how beta changes. And that implicitly\ntells you how theta will change over time.",
    "start": "1712310",
    "end": "1718230"
  },
  {
    "text": "We'll see exactly\nhow this works. Right, so-- but maybe\nbefore going to that,",
    "start": "1718230",
    "end": "1726549"
  },
  {
    "text": "let me just show you why this\nis true, why this is true.",
    "start": "1726549",
    "end": "1732090"
  },
  {
    "text": "The reason is actually\nrelatively simple if you look at this. So the reason is that pretty\nmuch, in short, in a nutshell,",
    "start": "1732090",
    "end": "1740620"
  },
  {
    "text": "the reason is that\nevery time you update, you always update\ntheta by a scalar.",
    "start": "1740620",
    "end": "1746789"
  },
  {
    "text": "This whole thing is a scalar. This parentheses is a scalar,\nwhatever scalar it is. You always update by a scalar\ntimes this vector, phi of x.",
    "start": "1746789",
    "end": "1754990"
  },
  {
    "text": "So basically, every time\nyou update, your update is of this kind of form. It's a linear combination\nof phi of x i. So that's why you\nkeep having this form.",
    "start": "1754990",
    "end": "1763190"
  },
  {
    "text": "If you want to prove\nthat more formally, I think the statement, you\nhave to do the induction.",
    "start": "1763190",
    "end": "1769770"
  },
  {
    "text": "So let me also try to do that. So by induction-- so first of\nall, you check iteration 0.",
    "start": "1769770",
    "end": "1779130"
  },
  {
    "text": "So at iteration 0, so indeed,\nour theta is equal to 0.",
    "start": "1779130",
    "end": "1787039"
  },
  {
    "text": "That's my choice\nof initialization. And this is indeed equals to\nsum of linear combinations",
    "start": "1787039",
    "end": "1793659"
  },
  {
    "text": "of the feature\nvectors because I just",
    "start": "1793659",
    "end": "1799299"
  },
  {
    "text": "have to choose my beta i\nto be 0 exactly, right? That's easy.",
    "start": "1799299",
    "end": "1805230"
  },
  {
    "text": "And maybe I can also, just\nto build up intuition, let's also look at iteration this step is necessary\nif you really",
    "start": "1805230",
    "end": "1812148"
  },
  {
    "text": "care about the formal proof. This is just for intuition. And iteration 1,\nwhat you do, you",
    "start": "1812149",
    "end": "1818010"
  },
  {
    "text": "update theta to be equal to\nthe old theta, which is here is 0 plus this\ngradient, which is",
    "start": "1818010",
    "end": "1826000"
  },
  {
    "text": "alpha times i from 1 to n\ny i minus theta transpose",
    "start": "1826000",
    "end": "1833620"
  },
  {
    "text": "phi of x i phi of x i.",
    "start": "1833620",
    "end": "1839940"
  },
  {
    "text": "And this thing is a scalar. Actually, it's equal to y\nbecause theta is 0, right?",
    "start": "1839940",
    "end": "1845740"
  },
  {
    "text": "This theta was 0 in\nthe previous division. So this will be just\ny i times phi of x i.",
    "start": "1845740",
    "end": "1850950"
  },
  {
    "text": "Theta was 0.",
    "start": "1850951",
    "end": "1858809"
  },
  {
    "text": "And you plug in 0 here. You get this. So this is indeed a\nlinear combination of the feature vectors.",
    "start": "1858809",
    "end": "1864370"
  },
  {
    "text": "So this is a vector. This is a scalar. And so alpha times y i will\nbe my beta i in this one.",
    "start": "1864370",
    "end": "1873490"
  },
  {
    "text": "So this thing plus\nthis thing together will be my-- maybe I'll\njust write alpha here.",
    "start": "1873490",
    "end": "1879990"
  },
  {
    "text": "If I write alpha here,\nthen this whole thing will be my beta i\nat iteration 1, OK?",
    "start": "1879990",
    "end": "1889830"
  },
  {
    "text": "And for the future\nsteps, I wouldn't be able to explicitly write\nbeta i that carefully.",
    "start": "1889830",
    "end": "1896380"
  },
  {
    "text": "But I'm going to use\nan induction, right? So suppose at\niteration t, I already",
    "start": "1896380",
    "end": "1906880"
  },
  {
    "text": "know that theta is equal to\nsomething like sum of beta i phi of x i.",
    "start": "1906880",
    "end": "1913710"
  },
  {
    "text": "This is my inductive hypothesis. And then at next\niteration, you can",
    "start": "1913710",
    "end": "1922060"
  },
  {
    "text": "see that theta is updated to\nbe equal to theta plus alpha--",
    "start": "1922060",
    "end": "1927720"
  },
  {
    "text": "OK, I'm just going to\ncopy this formula again.",
    "start": "1927720",
    "end": "1936740"
  },
  {
    "text": "Actually, I just want\nto prove the induction.",
    "start": "1936740",
    "end": "1944760"
  },
  {
    "text": "I don't even have to plug in\nwhat theta is because if I just care about the\ninduction-- so I know",
    "start": "1944760",
    "end": "1949769"
  },
  {
    "text": "this is a linear\ncombination of phi of x i. And this is a linear\ncombination of phi of x i. Then the sum of them will\nbe linear combination",
    "start": "1949770",
    "end": "1956610"
  },
  {
    "text": "of phi of x i. But I'm going to do a little\nbit more detailed steps because the steps will\nbe useful for me as well.",
    "start": "1956610",
    "end": "1963169"
  },
  {
    "text": "So I'm just going to\nplug in everything. So I'm going to plug in-- I'm going to replace theta\nby my inductive hypothesis.",
    "start": "1963169",
    "end": "1976919"
  },
  {
    "text": "And then I'm going\nto replace this theta by my inductive\nhypothesis as well. So I'm going to\nhave alpha times--",
    "start": "1976919",
    "end": "1984710"
  },
  {
    "text": "OK, what is this? So theta is equals to--",
    "start": "1984710",
    "end": "1991220"
  },
  {
    "text": "let me see. One moment.",
    "start": "1991220",
    "end": "1996970"
  },
  {
    "text": "There's something--\nOK, I guess let me--",
    "start": "1996970",
    "end": "2005020"
  },
  {
    "text": "maybe let me just keep this, so.",
    "start": "2005020",
    "end": "2013070"
  },
  {
    "text": "And this is equal to just the\nsum of theta i plus alpha y",
    "start": "2013070",
    "end": "2024700"
  },
  {
    "text": "i minus the transpose\nof phi of x i.",
    "start": "2024701",
    "end": "2033700"
  },
  {
    "text": "I didn't do anything\nreally complex. It's just a super\nsimple manipulation.",
    "start": "2033700",
    "end": "2038710"
  },
  {
    "text": "I know this whole\nthing is a scalar.",
    "start": "2038710",
    "end": "2045149"
  },
  {
    "text": "Whatever scalar it\nis, this will be my new beta i in the next step.",
    "start": "2045150",
    "end": "2053629"
  },
  {
    "text": "Any questions? [INAUDIBLE] random\ninitialization of theta?",
    "start": "2053630",
    "end": "2059710"
  },
  {
    "text": "Yeah, this doesn't work\nfor random initialization.",
    "start": "2059710",
    "end": "2065800"
  },
  {
    "text": "But you don't need to use-- actually, in initialization 0\nis actually probably the best.",
    "start": "2065800",
    "end": "2073889"
  },
  {
    "text": "Maybe there is some\nreason for this. But maybe let's discuss\nthat offline, yeah.",
    "start": "2073890",
    "end": "2079088"
  },
  {
    "text": "So initialization, it\ndoes matter a little bit.",
    "start": "2079089",
    "end": "2084570"
  },
  {
    "text": "But let's say we just take\ninitialization to be 0.",
    "start": "2084570",
    "end": "2094820"
  },
  {
    "text": "OK, so now let's\nproceed with my plan. My plan was that I'm\ngoing to replace--",
    "start": "2094820",
    "end": "2101350"
  },
  {
    "text": "so far, I just want to\nprove the claim, right? The claim is that I can always\nrepresent theta by beta.",
    "start": "2101350",
    "end": "2107619"
  },
  {
    "text": "And now I'm going to\njust only mention beta. So basically, my\nplan is I'm just",
    "start": "2107619",
    "end": "2112880"
  },
  {
    "text": "only going to maintain\nbeta, but not theta.",
    "start": "2112880",
    "end": "2118509"
  },
  {
    "text": "So that means that I'm going\nto start from p parameters. Before, it was p parameters.",
    "start": "2118510",
    "end": "2124089"
  },
  {
    "text": "And now it becomes n parameters.",
    "start": "2124089",
    "end": "2129940"
  },
  {
    "text": "And if p is very large,\nthen this means some saving. So that means that I\nhave to understand how",
    "start": "2129940",
    "end": "2136400"
  },
  {
    "text": "the beta is changing, right? So I have told you that\nbeta exists, right, so when beta is equal to this. But I want to have an\nupdate of beta that only--",
    "start": "2136400",
    "end": "2143690"
  },
  {
    "text": "so here, right now,\nbeta depends on theta. So if you want to compute\nbeta, the new beta, I have to know the old\ntheta, which kind of defeats",
    "start": "2143690",
    "end": "2151780"
  },
  {
    "text": "the purpose, right? So if you have to know\nwhat's the existing theta, then you have to\ncompute it, right?",
    "start": "2151780",
    "end": "2158079"
  },
  {
    "text": "So then you waste p time. So what I'm going\nto do is I'm going to find out how does beta\ndepend on beta itself",
    "start": "2158079",
    "end": "2166190"
  },
  {
    "text": "without going through theta. So that's what I'm going to do.",
    "start": "2166190",
    "end": "2171470"
  },
  {
    "text": "So where should I? Maybe-- yeah, I'll write\non the left-hand side.",
    "start": "2171470",
    "end": "2182140"
  },
  {
    "text": "I guess, maybe I'll just\nwrite somewhere here so that there's some locality.",
    "start": "2182140",
    "end": "2193890"
  },
  {
    "text": "Maybe I can erase this.",
    "start": "2193890",
    "end": "2200119"
  },
  {
    "text": "So this is the\nupdate for beta i.",
    "start": "2200119",
    "end": "2206769"
  },
  {
    "text": "So how do I update beta i? So beta i is supposed\nto be equal to beta i plus alpha times this, right?",
    "start": "2206770",
    "end": "2214710"
  },
  {
    "text": "Times this. So this is my rule\nfor my starting point. I know that beta is equal to\nalpha times y i minus theta",
    "start": "2214710",
    "end": "2222880"
  },
  {
    "text": "transpose phi of\nx i, phi of x i.",
    "start": "2222880",
    "end": "2228359"
  },
  {
    "text": "But this view is\nnot great because it does have theta here. So I have to maintain theta. I'm going to get rid of theta\nby plugging the definition",
    "start": "2228359",
    "end": "2236319"
  },
  {
    "text": "of theta in terms of beta. So this is equals to beta i\nplus alpha times y i minus--",
    "start": "2236319",
    "end": "2245010"
  },
  {
    "text": "what's the definition of theta? So if you-- sorry, what's\nthe representation of theta? So theta is equal to this.",
    "start": "2245010",
    "end": "2252650"
  },
  {
    "text": "This is the relationship\nbetween theta and beta.",
    "start": "2252650",
    "end": "2259869"
  },
  {
    "text": "This is a j because I'm\nhaving some insight.",
    "start": "2259869",
    "end": "2266769"
  },
  {
    "text": "And this transpose, times\nphi of x i and parenthesis.",
    "start": "2266770",
    "end": "2276790"
  },
  {
    "text": "OK, and then I'm\ngoing to continue.",
    "start": "2276790",
    "end": "2284520"
  },
  {
    "text": "So we organize\nthis a little bit. So I'm going to get beta i\nplus alpha times y i minus,",
    "start": "2284520",
    "end": "2299109"
  },
  {
    "text": "I'm going to put this\ninside, so sum over j from 1 to n beta j phi of\nxj times phi of x i.",
    "start": "2299109",
    "end": "2313369"
  },
  {
    "text": "This means inner product, right? So a transpose b, I guess this\nis just a inner product with b.",
    "start": "2313369",
    "end": "2319069"
  },
  {
    "text": "That's what I'm using, so. And this is i. And this is j.",
    "start": "2319069",
    "end": "2326109"
  },
  {
    "text": "OK? So what I have achieved,\nI got rid of theta.",
    "start": "2326109",
    "end": "2348840"
  },
  {
    "text": "So now it's up the rule from\nbeta to beta itself, right?",
    "start": "2348840",
    "end": "2355590"
  },
  {
    "text": "So if you know the old beta,\nyou can configure new beta. That's what this is saying.",
    "start": "2355590",
    "end": "2360640"
  },
  {
    "text": "But did I really save any time? Not yet, right,\nbecause if you want to compute the new\nbeta from the old beta,",
    "start": "2360640",
    "end": "2367200"
  },
  {
    "text": "you still have to do\nthis inner product. This inner product between\ntwo p-dimensional vectors still takes p time.",
    "start": "2367200",
    "end": "2372630"
  },
  {
    "text": "And you still have to sum over n\nindices, so still O of np time.",
    "start": "2372630",
    "end": "2378450"
  },
  {
    "text": "So I haven't really saved much\njust because the phi still shows up here. However, here is the magic.",
    "start": "2378450",
    "end": "2384460"
  },
  {
    "text": "The magic is that\nsomehow you can compute--",
    "start": "2384460",
    "end": "2392260"
  },
  {
    "text": "so this is still O of np time.",
    "start": "2392260",
    "end": "2397740"
  },
  {
    "text": "And this is, so far,\nnot really much saving. But there are two things we\ncan notice to make it faster.",
    "start": "2397740",
    "end": "2406890"
  },
  {
    "text": "So one thing is that this inner\nproduct can be preprocessed.",
    "start": "2406890",
    "end": "2412650"
  },
  {
    "text": "So you don't have to come here\nevery time because this is just",
    "start": "2412650",
    "end": "2426309"
  },
  {
    "text": "something that doesn't\nreally change over time in your algorithm. By over time, I mean in the--",
    "start": "2426310",
    "end": "2431980"
  },
  {
    "text": "as your algorithm is\nrunning, this quantity is not changing at all\nfor every i, j, right?",
    "start": "2431980",
    "end": "2439560"
  },
  {
    "text": "The beta will be changing. But this wouldn't be changing. And another more\nimportant thing is that--",
    "start": "2439560",
    "end": "2447170"
  },
  {
    "text": "so this means that\nyou can do it once and you don't have to\ncomplete it every time. So at least you only have to do\nit once in the very beginning.",
    "start": "2447170",
    "end": "2454130"
  },
  {
    "text": "And another thing is that\noftentimes, this inner product",
    "start": "2454130",
    "end": "2461670"
  },
  {
    "text": "actually can be computed\nfaster than you thought, right? The trivial way\nto implement this",
    "start": "2461670",
    "end": "2467390"
  },
  {
    "text": "is that you just compute\nthis factor, this factor, and you take the inner product. But actually, you\ncan do some math to speed up this in many cases.",
    "start": "2467390",
    "end": "2476569"
  },
  {
    "text": "So this can be computed without\neven evaluating phi explicitly.",
    "start": "2476570",
    "end": "2491920"
  },
  {
    "text": "If you have to evaluate\neach of these factors, you are going to waste p time. But sometimes you\ncan do some math",
    "start": "2491920",
    "end": "2498060"
  },
  {
    "text": "to not even evaluate the phi. That's what I'm\ngoing to show next.",
    "start": "2498060",
    "end": "2527290"
  },
  {
    "text": "Any questions? Basically, we are all\nrepresenting something in the space of Rp in terms\nof the linear combinations",
    "start": "2527290",
    "end": "2538430"
  },
  {
    "text": "of vector [INAUDIBLE]. So actually, we have\nless number of factors than the dimensional\nspace itself.",
    "start": "2538430",
    "end": "2544890"
  },
  {
    "text": "So does it make it a\nlower-end approximation of the initial\nresult [INAUDIBLE]?? Or is that precise?",
    "start": "2544890",
    "end": "2551730"
  },
  {
    "text": "It's still precise\nbecause so far, you see that I have never done\nany approximation, right?",
    "start": "2551730",
    "end": "2556990"
  },
  {
    "text": "But your question\ndefinitely makes sense. It's a very legitimate question. So why you can\nsomehow magically save",
    "start": "2556990",
    "end": "2563290"
  },
  {
    "text": "any degree of freedom, right? I think the reason is that-- this is maybe a little\nbit vague at this level.",
    "start": "2563290",
    "end": "2570500"
  },
  {
    "text": "But the reason is that even you\nhad this p-dimensional degree of freedom, p degree\nof freedom in theta,",
    "start": "2570500",
    "end": "2576490"
  },
  {
    "text": "when your data is\nnot as large as p, you cannot use all of them.",
    "start": "2576490",
    "end": "2583030"
  },
  {
    "text": "You are not fundamentally\nusing all the dimensionalities or all the degrees of freedom\nin the theta in some sense.",
    "start": "2583030",
    "end": "2592920"
  },
  {
    "text": "If you have n samples, the\nmaximum degree of freedom you can have is\n[INAUDIBLE] And that's",
    "start": "2592920",
    "end": "2599110"
  },
  {
    "text": "why you can say even without\nany approximation, right? So here, it's really\nto see reimplementation",
    "start": "2599110",
    "end": "2604800"
  },
  {
    "text": "of the same algorithm. We didn't lose any-- it's not like you're\ndoing any approximation. That's the point.",
    "start": "2604800",
    "end": "2614109"
  },
  {
    "text": "[INAUDIBLE] you can only do But this randomly initialize\nthe beta coefficient.",
    "start": "2614109",
    "end": "2619589"
  },
  {
    "text": "When we initialize the beta? Yeah. I think no. I think you have to\ninitialize with beta is 0.",
    "start": "2619589",
    "end": "2626460"
  },
  {
    "text": "All of this only works\nwith 0 initialization. So theta has to be\ninitialized to be 0. And beta also has\nto be initialized",
    "start": "2626460",
    "end": "2633069"
  },
  {
    "text": "to be 0 just because\nthat's the correspondence at the beginning. So there is a number of\nexamples which is [much]",
    "start": "2633070",
    "end": "2642509"
  },
  {
    "text": "bigger than the\n[INAUDIBLE] p [INAUDIBLE]??",
    "start": "2642510",
    "end": "2648140"
  },
  {
    "text": "If [INAUDIBLE],, so then\nthis wouldn't help you. It probably would\nhurt you, yeah.",
    "start": "2648140",
    "end": "2654630"
  },
  {
    "text": "We will discuss a\nlittle bit at the end when this will be useful. I think this will be useful only\nwhen p is really, really big.",
    "start": "2654630",
    "end": "2661010"
  },
  {
    "text": "Yeah, but sometimes p is\nreally big like in the case [INAUDIBLE]. But even if beta [INAUDIBLE],,\nbeta still [INAUDIBLE]..",
    "start": "2661010",
    "end": "2668710"
  },
  {
    "text": "Yes, yes. OK, so if your question is, what\nhappens when my beta is random,",
    "start": "2668710",
    "end": "2686260"
  },
  {
    "text": "I think, in most of the cases-- maybe all cases.",
    "start": "2686260",
    "end": "2692210"
  },
  {
    "text": "I don't know. In most of cases, I\nthink you probably would get the same\nanswer eventually when you run this\nalgorithm in a beta space.",
    "start": "2692210",
    "end": "2699670"
  },
  {
    "text": "But the correspondence is\nonly shown with beta is 0. So there's another reason why--",
    "start": "2699670",
    "end": "2706360"
  },
  {
    "text": "OK, so maybe here is the\nexact answer to this. So suppose you only care\nabout what happens with beta",
    "start": "2706360",
    "end": "2712750"
  },
  {
    "text": "is initialized to be random. Then I think it still can work.",
    "start": "2712750",
    "end": "2717980"
  },
  {
    "text": "And it can actually still\ngive the same solution in most of cases as beta\nis 0, initialized to be 0.",
    "start": "2717980",
    "end": "2724880"
  },
  {
    "text": "But that's a different reason. There's a different\nreason for this to be true because\njust because you're",
    "start": "2724880",
    "end": "2730340"
  },
  {
    "text": "doing some complex\noptimization, eventually, it always converges to the\nsame solution, probably that's what you are already on--",
    "start": "2730340",
    "end": "2736588"
  },
  {
    "text": "you mean as well. But this correspondence between\ntheta and beta, on this level, it only works for beta is 0.",
    "start": "2736589",
    "end": "2742260"
  },
  {
    "text": "This makes sense? OK, any other questions? For the first thing you said\nabout how theta of x i times",
    "start": "2742260",
    "end": "2754540"
  },
  {
    "text": "theta of xj to be preprocessed. We still have to calculate--",
    "start": "2754540",
    "end": "2760180"
  },
  {
    "text": "we're preprocessing it for\nevery single combination, right? How is that speeding\nanything up?",
    "start": "2760180",
    "end": "2767480"
  },
  {
    "text": "Right, so that means you only\nhave to do n square pairs, right? So there's still a lot of time.",
    "start": "2767480",
    "end": "2775980"
  },
  {
    "text": "I agree. But the difference is that\nif you do it on the fly,",
    "start": "2775980",
    "end": "2781481"
  },
  {
    "text": "they have to do this all i,\nj pair for every iteration. So I'm saying that\ncomparatively, this is faster.",
    "start": "2781481",
    "end": "2788078"
  },
  {
    "text": "I'm not saying,\nabsolutely, it's very fast. So suppose you do this\nin a pair every time, then you still have\nto do it for every i",
    "start": "2788079",
    "end": "2793990"
  },
  {
    "text": "and j because here,\nyou have a j here. You have a sum over j. And also, you have to\nupdate for every i.",
    "start": "2793990",
    "end": "2799630"
  },
  {
    "text": "So you basically have\nto do all of these pairs at every iteration. So I'm saying that\nat least you can",
    "start": "2799630",
    "end": "2807380"
  },
  {
    "text": "save that, at least\nyou don't have to do it for every iteration. So the number of iterations\nthat we want to happen",
    "start": "2807380",
    "end": "2815770"
  },
  {
    "text": "will be higher than the number-- how many iterations\nwill we end up having? The number of\niterations, I think,",
    "start": "2815770",
    "end": "2821579"
  },
  {
    "text": "that's a little bit tricky. But let's say you have\nt iterations, right? So before, we have\nt times this number.",
    "start": "2821580",
    "end": "2829260"
  },
  {
    "text": "And now you just say that t. That's only thing I'm saying. OK.",
    "start": "2829260",
    "end": "2834630"
  },
  {
    "text": "The biggest saving is probably\ncoming from here, which I'm going to show right now.",
    "start": "2834630",
    "end": "2839829"
  },
  {
    "text": "Each of these inner product is\nactually cheaper, much cheaper that we typically think.",
    "start": "2839829",
    "end": "2846550"
  },
  {
    "text": "OK, so why that's the case? So of course, this is not\na universal statement.",
    "start": "2846550",
    "end": "2853349"
  },
  {
    "text": "It's not like for every\nphi, you can do this. But for many of the\nphis that we designed,",
    "start": "2853349",
    "end": "2858510"
  },
  {
    "text": "that make sense, that\nintuitively make sense, then you can speed up. And actually, later on,\nthis becomes a principle.",
    "start": "2858510",
    "end": "2864990"
  },
  {
    "text": "You only design phis\nsuch that this is fast. You don't care about\nany other phis.",
    "start": "2864990",
    "end": "2870140"
  },
  {
    "text": "But let me say-- let me not get into there. Let me just talk about\nthis particular one.",
    "start": "2870140",
    "end": "2876660"
  },
  {
    "text": "So for this phi, I'm going to\nshow you why this can be fast. And the reason is very simple.",
    "start": "2876660",
    "end": "2882680"
  },
  {
    "text": "It's just can do some math\nto make your formula easier. So for the phi that\nI defined here,",
    "start": "2882680",
    "end": "2889810"
  },
  {
    "text": "so phi of x times phi of z--",
    "start": "2889810",
    "end": "2895370"
  },
  {
    "text": "this, what is this? I'm abstracting a little bit. I have x and z, just\nthe two things, right?",
    "start": "2895370",
    "end": "2901720"
  },
  {
    "text": "You can think of this\nas x i, this as xj. I'm just using more\nabstract notation.",
    "start": "2901720",
    "end": "2907700"
  },
  {
    "text": "So this is just the inner\nproduct of two vectors. One vector is 1, x1 up to xd,\nx1 squared, so and so forth.",
    "start": "2907700",
    "end": "2918380"
  },
  {
    "text": "And the other one\nis a column vector, which is 1, z1, z2 up\nto zd, and z1 squared,",
    "start": "2918380",
    "end": "2925270"
  },
  {
    "text": "so and so forth,\nsomething like this. And you just take the\nentry-wise product",
    "start": "2925270",
    "end": "2931609"
  },
  {
    "text": "and take the sum, right? So this times this will be 1.",
    "start": "2931609",
    "end": "2938180"
  },
  {
    "text": "These bunch of things\ntimes these bunch of things will be sum of x i z\ni, i from 1 to d, OK?",
    "start": "2938180",
    "end": "2948990"
  },
  {
    "text": "So now let's do the degree 2\nparts, the degree 2 monomials.",
    "start": "2948990",
    "end": "2956829"
  },
  {
    "text": "So what are these quantities\nright here, the degree 2 thing? So they are all of the\nform x i and xj, right?",
    "start": "2956829",
    "end": "2963630"
  },
  {
    "text": "And here you have\nthis bunch of z's. They're all of the\nform z i zj, right?",
    "start": "2963630",
    "end": "2969578"
  },
  {
    "text": "And you take the corresponding\nproduct and take the sum. So basically, what\nhappens is you loop over all the possible\nchoices of i and j.",
    "start": "2969579",
    "end": "2977680"
  },
  {
    "text": "And you take x i\nxj times z i zj. And then you do it\nfor the degree 3",
    "start": "2977680",
    "end": "2987380"
  },
  {
    "text": "part, which is the same. So you loop over. So all the degree 3 part,\nthey're of the form x i xj xk.",
    "start": "2987380",
    "end": "2994420"
  },
  {
    "text": "And on the other side, you\nhave the form z i zj zk. And you take the sum of all\npossible combinations of i, j,",
    "start": "2994420",
    "end": "3004950"
  },
  {
    "text": "k. So this i is from 1. j is from 1 to d, OK?",
    "start": "3004950",
    "end": "3015680"
  },
  {
    "text": "So now let's simplify this. And so you can\nsimplify this by--",
    "start": "3015680",
    "end": "3023140"
  },
  {
    "text": "so this one, we don't simplify. This is already pretty simple. Notational-wise, you\ncan simplify x times z.",
    "start": "3023140",
    "end": "3030589"
  },
  {
    "text": "But this doesn't\nreally change anything. It's still the same computation. But then for the second term,\nwhat we can do is that we can--",
    "start": "3030589",
    "end": "3037809"
  },
  {
    "text": "we have a two sum. We can factorize the two sum. So you can write this as--",
    "start": "3037809",
    "end": "3044578"
  },
  {
    "text": "you first take the sum over i. You look at all the\nterms that depends on i.",
    "start": "3044579",
    "end": "3049970"
  },
  {
    "text": "That's z, x i, and z i. And then you also have the part\nthat depends on j, so xj, zj.",
    "start": "3049970",
    "end": "3062000"
  },
  {
    "text": "So in some sense,\nI'm just using a-- abstractly speaking,\nI'm using the fact that if you have sum of ui times\nwj, where i is from 1 to d,",
    "start": "3062000",
    "end": "3071599"
  },
  {
    "text": "j is from 1 to d, then\nthis is equals to sum of ui times sum of wj, right?",
    "start": "3071599",
    "end": "3083970"
  },
  {
    "text": "So that's just the\nform that I'm using. And ui corresponds to x i zi.",
    "start": "3083970",
    "end": "3090400"
  },
  {
    "text": "And wj corresponds to xj zj.",
    "start": "3090400",
    "end": "3096490"
  },
  {
    "text": "That's how I use this\nabstract formula, so.",
    "start": "3096490",
    "end": "3101700"
  },
  {
    "text": "OK, and then I can do the same\nthing for the third one, which",
    "start": "3101700",
    "end": "3106750"
  },
  {
    "text": "will be equals to-- again, you factorize\nbased on i, j, k.",
    "start": "3106750",
    "end": "3113339"
  },
  {
    "text": "You collect all the terms that\ndepends on i, which is x i z i. And you collect\nall the terms that",
    "start": "3113339",
    "end": "3118340"
  },
  {
    "text": "depends on j, which is xj zk. And you collect\nall the terms that depends on k, which is xk zk.",
    "start": "3118340",
    "end": "3129799"
  },
  {
    "text": "So what's good about this? What's good about this\nis you can see this one",
    "start": "3129800",
    "end": "3135680"
  },
  {
    "text": "and this one are\nactually the same thing. You are just changing the\nway you index the terms.",
    "start": "3135680",
    "end": "3142400"
  },
  {
    "text": "But anyway, you are taking a\nloop sum over all the terms. But it doesn't matter\nwhat indices you use.",
    "start": "3142400",
    "end": "3149210"
  },
  {
    "text": "And the same for all of this--\nall of these are all the same. All of these are equals to\nx transpose x inner product",
    "start": "3149210",
    "end": "3155670"
  },
  {
    "text": "with z. So essentially, what you got is\njust the x inner product with z",
    "start": "3155670",
    "end": "3167460"
  },
  {
    "text": "plus x inner product with z\nsquared plus x inner product",
    "start": "3167460",
    "end": "3172799"
  },
  {
    "text": "with z cubed. And why this is helping\nyou in competition,",
    "start": "3172799",
    "end": "3178940"
  },
  {
    "text": "the reason is that\nnow it takes O of d time to compute x comma\nz-- inner product with z, right?",
    "start": "3178940",
    "end": "3192520"
  },
  {
    "text": "That's of d time,\nbut now, p, right? This is just your raw\nfeature, your input dimension.",
    "start": "3192520",
    "end": "3198990"
  },
  {
    "text": "And then after you\nget this, you can take the power, the second power. This one is just a scalar after\nyou get x inner product z.",
    "start": "3198990",
    "end": "3205119"
  },
  {
    "text": "You just take a scalar square. You get this. And you take a scalar cube. You get this, right? So the whole runtime\nis really just",
    "start": "3205119",
    "end": "3211790"
  },
  {
    "text": "x, the time for doing\nthis, plus I think,",
    "start": "3211790",
    "end": "3217340"
  },
  {
    "text": "four rows plus some\nconstant because you just take the power. And you take the sum.",
    "start": "3217340",
    "end": "3222800"
  },
  {
    "text": "So the total runtime is-- total time is also O of d.",
    "start": "3222800",
    "end": "3230869"
  },
  {
    "text": "I'm ignoring the constant. I'm assuming d is big. Any questions?",
    "start": "3230869",
    "end": "3239260"
  },
  {
    "text": "So I guess if we\nchoose a phi smartly",
    "start": "3239260",
    "end": "3248140"
  },
  {
    "text": "and appropriately,\nthen we might not even need to compute the whole--",
    "start": "3248140",
    "end": "3253520"
  },
  {
    "text": "we might not even need to\nput p to the entire data set.",
    "start": "3253520",
    "end": "3259339"
  },
  {
    "text": "Right, so here you\ndon't need it, right? So here you don't have to\nimplicitly say phi because--",
    "start": "3259339",
    "end": "3265029"
  },
  {
    "text": "OK, so exactly. That's a good question. So basically, so what happens\nright now-- so basically,",
    "start": "3265030",
    "end": "3271170"
  },
  {
    "text": "you compute this quantity. But you don't have\nto know what phi is. You just complete this\nquantity using this algorithm.",
    "start": "3271170",
    "end": "3278089"
  },
  {
    "text": "And you compute all\nof these in advance. And then you run this algorithm.",
    "start": "3278089",
    "end": "3284600"
  },
  {
    "text": "So let me write down\nthis more formally. So formally, what\nyou do is you--",
    "start": "3284600",
    "end": "3292779"
  },
  {
    "text": "let me also introduce\nsome notation",
    "start": "3292780",
    "end": "3304750"
  },
  {
    "text": "to kind of abstractify\nthis because it would be also useful,\nespecially if you read",
    "start": "3304750",
    "end": "3311290"
  },
  {
    "text": "other papers and related works. So there is a notation.",
    "start": "3311290",
    "end": "3317670"
  },
  {
    "text": "People call this-- we find this\nso-called kernel function is",
    "start": "3317670",
    "end": "3322920"
  },
  {
    "text": "defined to be this,\nprecisely, this quantity we care about-- the inner\nproduct between features.",
    "start": "3322920",
    "end": "3330140"
  },
  {
    "text": "This is called kernel function.",
    "start": "3330140",
    "end": "3337769"
  },
  {
    "text": "So this kernel function,\nas the definition shows, is something that\ntakes in two vectors",
    "start": "3337770",
    "end": "3348490"
  },
  {
    "text": "and outputs a single scalar. And the algorithm is just that--",
    "start": "3348490",
    "end": "3355530"
  },
  {
    "text": "so if you use the kernel\nmethod, basically, we have shown you the steps. But now I'm going to\ngroup everything together.",
    "start": "3355530",
    "end": "3360950"
  },
  {
    "text": "So basically, the final\nalgorithm is the following-- and maybe here.",
    "start": "3360950",
    "end": "3379319"
  },
  {
    "text": "So what you do is you\nsay you first compute-- you pre-process all\nof the inner product.",
    "start": "3379320",
    "end": "3385500"
  },
  {
    "text": "So you compute k of x i, xj.",
    "start": "3385500",
    "end": "3390620"
  },
  {
    "text": "Recall that this is defined\nto be the inner product of these two things.",
    "start": "3390620",
    "end": "3401180"
  },
  {
    "text": "So you can compute this for\nall i and j from 1 to n, OK?",
    "start": "3401180",
    "end": "3411460"
  },
  {
    "text": "And then you say\nyou have a loop.",
    "start": "3411460",
    "end": "3419150"
  },
  {
    "text": "So I guess you start\nwith, say, beta 0. And so beta is a vector in Rn.",
    "start": "3419150",
    "end": "3426180"
  },
  {
    "text": "So beta is the collection\nof beta 1 up to beta n. And so you start with beta 0.",
    "start": "3426180",
    "end": "3432829"
  },
  {
    "text": "And then you do a loop. And this loop will\njust be something like for every i\nfrom 1 to n, I'm",
    "start": "3432829",
    "end": "3445920"
  },
  {
    "text": "going to use this\nupdated rule for beta. So what does that mean? That means that\nI'm going to update",
    "start": "3445920",
    "end": "3452260"
  },
  {
    "text": "beta i to be beta i plus alpha\ntimes y i minus sum of beta j,",
    "start": "3452260",
    "end": "3463000"
  },
  {
    "text": "sum of beta j times this inner\nproduct, this inner product, which is something I've computed\nand I've denoted by k of x i",
    "start": "3463000",
    "end": "3471950"
  },
  {
    "text": "and xj. Right, so that's my algorithm.",
    "start": "3471950",
    "end": "3490369"
  },
  {
    "text": "And now we can take\nanother kind of accounting to see what's the runtime for\nthis particular polynomial",
    "start": "3490369",
    "end": "3498319"
  },
  {
    "text": "feature. So the runtime is-- let's see.",
    "start": "3498320",
    "end": "3503890"
  },
  {
    "text": "OK, so I guess you probably\ncan also get the runtime. So to compute this, we are\nusing a formula like this",
    "start": "3503890",
    "end": "3512060"
  },
  {
    "text": "to compute the part that\nwe're using in this formula. So for every pair,\nyou need O of d time.",
    "start": "3512060",
    "end": "3518359"
  },
  {
    "text": "So this is O of d for i, j,\nso O of n squared d in total.",
    "start": "3518359",
    "end": "3531680"
  },
  {
    "text": "Right, and here our\nruntime would be--",
    "start": "3531680",
    "end": "3540650"
  },
  {
    "text": "let me see. I guess I don't have\nit on my notes somehow.",
    "start": "3540650",
    "end": "3551789"
  },
  {
    "text": "I don't know why. Anyway, but I can\ndo it on the fly. So you already know this number. So basically, we need to\npay n times O of n time",
    "start": "3551790",
    "end": "3561369"
  },
  {
    "text": "to compute this sum. And you also have to update\neach of this and each of the i.",
    "start": "3561369",
    "end": "3569480"
  },
  {
    "text": "So for every i, you\nhave to pay n times. So totally, this is\nO of n squared time,",
    "start": "3569480",
    "end": "3574770"
  },
  {
    "text": "n squared time per iteration.",
    "start": "3574770",
    "end": "3585030"
  },
  {
    "text": "So the catch is that there's no\np involved anymore whatsoever.",
    "start": "3585030",
    "end": "3592049"
  },
  {
    "text": "So you just only have n and d. And if your n is small\nenough, then in some cases--",
    "start": "3592049",
    "end": "3598069"
  },
  {
    "text": "if n squared for example is less\nthan np, then you are winning,",
    "start": "3598069",
    "end": "3603190"
  },
  {
    "text": "right? Recall that before when we\nrun this every iteration, we have to take n\ntimes p duration.",
    "start": "3603190",
    "end": "3608970"
  },
  {
    "text": "And now every iteration, we have\nto take n squared iteration. So if n is less than p,\nthen you are winning.",
    "start": "3608970",
    "end": "3614500"
  },
  {
    "text": "And in many cases, n\nis much smaller than p. It's because p could be\na d to the third, right?",
    "start": "3614500",
    "end": "3623880"
  },
  {
    "text": "That's just very big, yeah, so. At least I'm not saying this\nis universal, by the way. Actually, I'm only discussing\nwhen this kernel method is",
    "start": "3623880",
    "end": "3631240"
  },
  {
    "text": "better, especially--\nas you can see, it's better when p is very\nbig and n is small, right?",
    "start": "3631240",
    "end": "3636420"
  },
  {
    "text": "But in those cases, you\ndo save a lot of time. [INAUDIBLE] to do stochastic\nwhere you can fully update",
    "start": "3636420",
    "end": "3646752"
  },
  {
    "text": "one beta at a time [INAUDIBLE]? You do update one\nbeta at each time? So I think that would\nbe called coordinate gradient descent instead of\nstochastic gradient descent,",
    "start": "3646752",
    "end": "3654309"
  },
  {
    "text": "right, because\neach of the beta i should be considered as a\ncoordinate of your parameter.",
    "start": "3654309",
    "end": "3663069"
  },
  {
    "text": "But that is just terminology. I think if you update\neach of them one by one,",
    "start": "3663069",
    "end": "3668140"
  },
  {
    "text": "you probably have to\nuse smaller alpha. But in theory, it\nshould still work if you use small enough alpha.",
    "start": "3668140",
    "end": "3675099"
  },
  {
    "text": "But I don't think you\nwould gain much by doing--",
    "start": "3675099",
    "end": "3681760"
  },
  {
    "text": "maybe you can gain a little bit. But it wouldn't be a\nfundamental difference. Maybe it would be\nfaster a little bit.",
    "start": "3681760",
    "end": "3687549"
  },
  {
    "text": "Any other questions? So you said that the final\nruntime was O of n squared.",
    "start": "3687549",
    "end": "3693069"
  },
  {
    "text": "Oh, per iteration. How many interations do I need? That's a little bit hard to\ndecide because it depends",
    "start": "3693069",
    "end": "3699279"
  },
  {
    "text": "on the problem and the-- but so that's why we only\ncompare per iteration so far.",
    "start": "3699280",
    "end": "3705069"
  },
  {
    "text": "Maybe this is just demonstrating\na lack of knowledge of runtime",
    "start": "3705069",
    "end": "3710980"
  },
  {
    "text": "on my part. If each calculation the\nruntime for all function,",
    "start": "3710980",
    "end": "3717650"
  },
  {
    "text": "the total is O of nd\nsquared or n squared d.",
    "start": "3717650",
    "end": "3723430"
  },
  {
    "text": "How is d-- so is the total\nruntime going to be O of n",
    "start": "3723430",
    "end": "3728619"
  },
  {
    "text": "squared plus O of\nn squared d times--",
    "start": "3728619",
    "end": "3734450"
  },
  {
    "text": "Oh, I think I see what you mean. So if you really\ncare about the total, the final, final\nruntime, I think",
    "start": "3734450",
    "end": "3740430"
  },
  {
    "text": "you would call it\nsomething like n squared d plus O of n squared\ntimes T, where",
    "start": "3740430",
    "end": "3749260"
  },
  {
    "text": "T is the number of iterations. But what is T? The reason why I don't discuss\nlike that just because T",
    "start": "3749260",
    "end": "3755400"
  },
  {
    "text": "is a little bit tricky. It's a little bit more\nproblem-dependent.",
    "start": "3755400",
    "end": "3764140"
  },
  {
    "text": "So that's why. And yeah, but what's\na good guess for T?",
    "start": "3764140",
    "end": "3769789"
  },
  {
    "text": "It really depends on a problem. Sometimes it's pretty small. Sometimes it could\nbe a bit larger.",
    "start": "3769790",
    "end": "3775569"
  },
  {
    "text": "So suppose you ignore\nthe preprocessing time, then you can compare-- roughly speaking, can compare\nthe per-iteration runtime.",
    "start": "3775569",
    "end": "3783670"
  },
  {
    "text": "So maybe for\nsimplicity, so there's no way to exactly compare\neverything exactly",
    "start": "3783670",
    "end": "3789280"
  },
  {
    "text": "unless you have more\nspecifications of the problem. But roughly, speaking, I think\nthe idea is that here there's",
    "start": "3789280",
    "end": "3796279"
  },
  {
    "text": "no p showing up. So it has to be better. In some sense, it's pretty\neasy to be better than--",
    "start": "3796279",
    "end": "3803759"
  },
  {
    "text": "so for example, p is\nreally, really large. I suppose p is like So whatever you do\nhere, it probably",
    "start": "3803760",
    "end": "3809240"
  },
  {
    "text": "is going to be better than\nsomething that has p in it.",
    "start": "3809240",
    "end": "3817869"
  },
  {
    "text": "So right.",
    "start": "3817869",
    "end": "3824260"
  },
  {
    "text": "So I think-- and this\nobservation that you only have to compute the\ninner product of this,",
    "start": "3824260",
    "end": "3830740"
  },
  {
    "text": "of the features, it sometimes\nhas more profound implication.",
    "start": "3830740",
    "end": "3838350"
  },
  {
    "text": "The thing is that\nwhen we realize that this inner product is\nthe only thing we care about,",
    "start": "3838350",
    "end": "3845838"
  },
  {
    "text": "then you start to wonder\nwhether you really have to think about what phi is. So maybe you just have\nto start with the kernel.",
    "start": "3845839",
    "end": "3853119"
  },
  {
    "text": "And then as now as the\nphi-- so basically, here is a change of mindset\nthat researchers",
    "start": "3853119",
    "end": "3858970"
  },
  {
    "text": "have, in some sense, done\nfor this kernel method. So we started with the phi.",
    "start": "3858970",
    "end": "3865779"
  },
  {
    "text": "And we defined the kernel. But you can also\nstart with the kernel. So you say I'm going to\ndefine a function like this.",
    "start": "3865779",
    "end": "3872880"
  },
  {
    "text": "And then as long as\nthere exists a phi, then you are done\nbecause you don't have",
    "start": "3872880",
    "end": "3878130"
  },
  {
    "text": "to know what phi is, right? So you only have to\nknow the kernel is an inner product of something. But you really have to\ncare about what phi is.",
    "start": "3878130",
    "end": "3885390"
  },
  {
    "text": "Anyway, it's not implemented. It's not used at all in\nyour algorithm, right? So roughly speaking,\nI think one way",
    "start": "3885390",
    "end": "3892950"
  },
  {
    "text": "to do this is you just define k\nand you just forget about phi.",
    "start": "3892950",
    "end": "3898970"
  },
  {
    "text": "Question? So we have n data points.",
    "start": "3898970",
    "end": "3904480"
  },
  {
    "text": "And we have [INAUDIBLE] look\ninto p-dimensional space.",
    "start": "3904480",
    "end": "3912190"
  },
  {
    "text": "This p is [INAUDIBLE]. So this one, we're\noverfitting this case.",
    "start": "3912190",
    "end": "3919140"
  },
  {
    "text": "So yeah. Yeah, yeah, I think\nthat's a great question. You don't necessarily\nhave to overfit. But this is-- the short answer\nis that you don't necessarily",
    "start": "3919140",
    "end": "3928029"
  },
  {
    "text": "have to overfit because-- OK, I guess I'm using\nlanguages that we haven't",
    "start": "3928029",
    "end": "3936760"
  },
  {
    "text": "discussed in this course. The norm of your\nsolution may not be that big even though the\nnumber of parameters is a lot.",
    "start": "3936760",
    "end": "3944330"
  },
  {
    "text": "But you can have a\nsmall enough solution. And that still can\nhelp you generalize. But this is something\nthat I don't",
    "start": "3944330",
    "end": "3950789"
  },
  {
    "text": "think we are going to discuss in\nvery much detail in this course even though we're going to\ntouch on this a little bit,",
    "start": "3950789",
    "end": "3956569"
  },
  {
    "text": "but not much.",
    "start": "3956569",
    "end": "3961599"
  },
  {
    "text": "OK, so basically, let me\ncontinue with this change",
    "start": "3961599",
    "end": "3970599"
  },
  {
    "text": "of mindset in some sense. So what people do is that\nyou just forget about phi.",
    "start": "3970599",
    "end": "3978619"
  },
  {
    "text": "And you just only\nwork with the kernel because all the\nalgorithms, you don't have to even know what phi is.",
    "start": "3978619",
    "end": "3984980"
  },
  {
    "text": "So basically, one thing at\nleast that is tempting to do is that you can just define a k.",
    "start": "3984980",
    "end": "3991190"
  },
  {
    "text": "And then you just\nrun this algorithm because after you define a k,\nyou can run this algorithm, right?",
    "start": "3991190",
    "end": "3996240"
  },
  {
    "text": "But of course, you cannot\njust use an arbitrary k because if you use\nan arbitrary k,",
    "start": "3996240",
    "end": "4002340"
  },
  {
    "text": "then your algorithm doesn't\nhave an interpretation. You don't know what\nyou are really doing. It's just the algorithm, right?",
    "start": "4002340",
    "end": "4007809"
  },
  {
    "text": "So what you really want\nis that you want a k that satisfies this for some phi.",
    "start": "4007809",
    "end": "4014609"
  },
  {
    "text": "But you don't have to\nknow what exactly phi is. So that means that\nyou have to understand",
    "start": "4014609",
    "end": "4022480"
  },
  {
    "text": "in what cases you can define\na k and that algorithm still makes sense. So basically, people call\nthis, if k is a valid kernel,",
    "start": "4022480",
    "end": "4033309"
  },
  {
    "text": "if on there exist of a function\nphi such that k of x comma z",
    "start": "4033309",
    "end": "4045980"
  },
  {
    "text": "is equal to inner product.",
    "start": "4045980",
    "end": "4052350"
  },
  {
    "text": "So basically, you can\njust design any function k as long as you guarantee\nthat the existence of phi.",
    "start": "4052350",
    "end": "4058809"
  },
  {
    "text": "As long as you guarantee that\nthis is a valid kernel, then you can try to run\nthat algorithm. And you know your\nalgorithm is really",
    "start": "4058809",
    "end": "4065640"
  },
  {
    "text": "just doing a linear model\non top of the feature phi.",
    "start": "4065640",
    "end": "4071640"
  },
  {
    "text": "But you don't have\nto know what phi is. [INAUDIBLE] don't we\nneed phi because once we",
    "start": "4071640",
    "end": "4079720"
  },
  {
    "text": "get these final data as\nwe apply them to phi, that phi version of the data.",
    "start": "4079720",
    "end": "4085829"
  },
  {
    "text": "And then we get the [INAUDIBLE]. That's a fantastic question. That's great, so. Yeah, I think I\nmissed the small part.",
    "start": "4085830",
    "end": "4092109"
  },
  {
    "text": "[CHUCKLES] I forgot. My bad. But yeah, this is exactly what--",
    "start": "4092109",
    "end": "4098600"
  },
  {
    "text": "let me talk about this. So the test time, you\nalso don't need the phi.",
    "start": "4098600",
    "end": "4105278"
  },
  {
    "text": "That's the thing. So the test time-- by test time, I mean, if you\ngive me a new data point x,",
    "start": "4105279",
    "end": "4113210"
  },
  {
    "text": "so given a new data point\nx, so the question is",
    "start": "4113210",
    "end": "4119080"
  },
  {
    "text": "how to compute this thing.",
    "start": "4119080",
    "end": "4124940"
  },
  {
    "text": "And the question\nwas that it sounds like you have to have to\ncompute phi and the theta",
    "start": "4124940",
    "end": "4130810"
  },
  {
    "text": "from the beta. And then you compute this. But actually, you don't\nhave to because you can just",
    "start": "4130810",
    "end": "4137100"
  },
  {
    "text": "plug in the formula, so\ntheta transpose phi of x. You use the representation as\nmuch as possible and use math.",
    "start": "4137100",
    "end": "4146190"
  },
  {
    "text": "So you try to replace theta by\nthis linear combination, beta i times phi of x i\ntranspose phi of x.",
    "start": "4146190",
    "end": "4157920"
  },
  {
    "text": "And now you regroup. You found that this is still\nabout inner product of two",
    "start": "4157920",
    "end": "4163460"
  },
  {
    "text": "things. So this becomes sum of beta\ni times the kernel function",
    "start": "4163460",
    "end": "4169609"
  },
  {
    "text": "applied on x i and x. So if your kernel function\nis evaluated very fast,",
    "start": "4169609",
    "end": "4176440"
  },
  {
    "text": "then you don't have\nto know what phi is. That's a good question. Thanks for reminding me.",
    "start": "4176440",
    "end": "4181448"
  },
  {
    "text": "Right, so if you know k,\nyou can do the training. You can do the test.",
    "start": "4181449",
    "end": "4186529"
  },
  {
    "text": "And the only thing\nyou have to guarantee is that this k is a\nvalid kernel so that you are doing something sensible.",
    "start": "4186530",
    "end": "4192580"
  },
  {
    "text": "By doing something\nsensible, it means that if you know\nit's a valid kernel, you know that you are\ndoing linear regression",
    "start": "4192580",
    "end": "4199870"
  },
  {
    "text": "with the phi as the feature. So in some sense, if\nyou design a good phi,",
    "start": "4199870",
    "end": "4212440"
  },
  {
    "text": "a good feature is kind\nof almost the same as designing a good k because\nanyway, you don't know what",
    "start": "4212440",
    "end": "4223870"
  },
  {
    "text": "phi is-- good or not, right? So probably, you should just\ngo directly designing k. And as long as k\nis a valid kernel,",
    "start": "4223870",
    "end": "4230020"
  },
  {
    "text": "you just run it and see\nwhether it's good or not. Questions?",
    "start": "4230020",
    "end": "4235869"
  },
  {
    "text": "[INAUDIBLE],, so I\nguess x i is the--",
    "start": "4235870",
    "end": "4245500"
  },
  {
    "text": "what is x i? Is [INAUDIBLE]\nsamples or training",
    "start": "4245500",
    "end": "4251219"
  },
  {
    "text": "samples or testing samples? This is the training samples.",
    "start": "4251219",
    "end": "4256650"
  },
  {
    "text": "Yes, that's a great question. So x i's are still\nthe training samples. And an x is a test example.",
    "start": "4256650",
    "end": "4263540"
  },
  {
    "text": "So it's interesting\nthat in the test time, you still have to look\nat a training example",
    "start": "4263540",
    "end": "4269460"
  },
  {
    "text": "to do this test. It's not like you just\nhave to remember the-- [INAUDIBLE] Right, right.",
    "start": "4269460",
    "end": "4275409"
  },
  {
    "text": "So like in a typical case,\nyou just have to store theta. And then you can forget\nabout the training side. But now you cannot\nforget the training side.",
    "start": "4275409",
    "end": "4287400"
  },
  {
    "text": "Right, so and I guess\nI have 15 minutes.",
    "start": "4287400",
    "end": "4292560"
  },
  {
    "text": "So I'll briefly\ndiscuss when do you know this is a valid\nkernel because this is-- if you want to use\nthis equivalence",
    "start": "4292560",
    "end": "4302080"
  },
  {
    "text": "and just forget\nabout k, you have to have some way to tell\nwhether your kernel is valid in some sense, so.",
    "start": "4302080",
    "end": "4309350"
  },
  {
    "text": "And there are some\nmathematical characterization of when the kernel is valid.",
    "start": "4309350",
    "end": "4316040"
  },
  {
    "text": "Let me write down the theorem. Let's see. Which board did I--",
    "start": "4316040",
    "end": "4322239"
  },
  {
    "text": "which one should I erase? Maybe this one.",
    "start": "4322239",
    "end": "4345810"
  },
  {
    "text": "So there's a necessary\nand sufficient condition.",
    "start": "4345810",
    "end": "4355790"
  },
  {
    "text": "So a necessary condition--",
    "start": "4355790",
    "end": "4364190"
  },
  {
    "text": "so if k is valid,\nthis implies that--",
    "start": "4364190",
    "end": "4369600"
  },
  {
    "text": "OK, maybe let me first\ndefine some notation.",
    "start": "4369600",
    "end": "4376170"
  },
  {
    "text": "So in the literature, you\ncan define the following. Suppose you have x1 up to xn.",
    "start": "4376170",
    "end": "4384070"
  },
  {
    "text": "These are n data points.",
    "start": "4384070",
    "end": "4389130"
  },
  {
    "text": "You can define the so-called-- which use the same\nthing, the same notation.",
    "start": "4389130",
    "end": "4394409"
  },
  {
    "text": "I don't know how\npeople do this, but. So now this is, with abuse of\nnotation, you define a matrix K",
    "start": "4394409",
    "end": "4401280"
  },
  {
    "text": "and call it kernel matrix.",
    "start": "4401280",
    "end": "4406800"
  },
  {
    "text": "And this matrix is a\nn-by-n-dimensional matrix, where k ij is equals to--",
    "start": "4406800",
    "end": "4414510"
  },
  {
    "text": "this k is the kernel\nfunction applied on this, the pairs of data.",
    "start": "4414510",
    "end": "4425190"
  },
  {
    "text": "I know this is all\na bit confusing. I don't know why\npeople keep doing this. On the left-hand side,\nthis is a matrix.",
    "start": "4425190",
    "end": "4431309"
  },
  {
    "text": "You define a matrix based on the\nfunction, the kernel function. So every kernel\nfunction, you can use that to define a matrix.",
    "start": "4431310",
    "end": "4437620"
  },
  {
    "text": "And this matrix is\nbasically the evaluation of this kernel function\nof particular data points.",
    "start": "4437620",
    "end": "4445890"
  },
  {
    "text": "I think, probably, we should\njust call this k another thing, whatever you call it basically. It's just there are\ntwo different things.",
    "start": "4445890",
    "end": "4451870"
  },
  {
    "text": "But people seems to use\nthe same notation for it.",
    "start": "4451870",
    "end": "4457000"
  },
  {
    "text": "And what you know is that--",
    "start": "4457000",
    "end": "4462210"
  },
  {
    "text": "so maybe I'll just probably talk\nabout the full condition given that we don't have\na lot of time.",
    "start": "4462210",
    "end": "4468460"
  },
  {
    "text": "So a necessary, sufficient\ncondition is that if the k--",
    "start": "4468460",
    "end": "4477800"
  },
  {
    "text": "so k is a valid kernel function.",
    "start": "4477800",
    "end": "4483110"
  },
  {
    "text": "So that means I'm talking about\na function, but not a matrix.",
    "start": "4483110",
    "end": "4493820"
  },
  {
    "text": "This is equivalent\nfor every x1 and xn",
    "start": "4493820",
    "end": "4499880"
  },
  {
    "text": "if you choose any n examples. And the kernel matrix K--",
    "start": "4499880",
    "end": "4518100"
  },
  {
    "text": "this is a matrix\ndefined like this-- is PSD, is positive semidefinite.",
    "start": "4518100",
    "end": "4525480"
  },
  {
    "text": "One side of this\nclaim is pretty easy to show because from\nthe very kernel to this,",
    "start": "4525480",
    "end": "4533020"
  },
  {
    "text": "I think it's really pretty\nmuch just a simple calculation. So you just plug\nin the definition that k is the inner product of\ntwo feature functions-- feature",
    "start": "4533020",
    "end": "4541300"
  },
  {
    "text": "vectors. And then you can pretty\nmuch verify this matrix PSD. The reverse direction is a\nlittle bit kind of tricky.",
    "start": "4541300",
    "end": "4548310"
  },
  {
    "text": "And also, my statement\nof the theorem, I think I'm missing some\nregularity condition if you really care\nabout the exact math.",
    "start": "4548310",
    "end": "4556140"
  },
  {
    "text": "But up to a small, minor\nregularity condition, you have to say this\nK is bounded though",
    "start": "4556140",
    "end": "4562139"
  },
  {
    "text": "by continuous kind of things. Right, so basically, after\nyou have the theorem,",
    "start": "4562139",
    "end": "4570630"
  },
  {
    "text": "then the workflow is that-- so in some sense,\nthe workflow is",
    "start": "4570630",
    "end": "4597840"
  },
  {
    "text": "kind of like you first design\na k, a kernel function, right? This is our kernel function k.",
    "start": "4597840",
    "end": "4613620"
  },
  {
    "text": "And then you verify k is valid.",
    "start": "4613620",
    "end": "4621110"
  },
  {
    "text": "And how do you verify? Maybe you can use\nthe theorem above. But that doesn't really\nmean that it's easy",
    "start": "4621110",
    "end": "4627150"
  },
  {
    "text": "because you still have to\nuse this theorem in some way, try to prove that the\nkernel mixture is PSD.",
    "start": "4627150",
    "end": "4634080"
  },
  {
    "text": "But there are some\nways to verify this. Or you verify either\nby using a theorem or by constructing the explicit\nfeature map that makes K valid.",
    "start": "4634080",
    "end": "4655690"
  },
  {
    "text": "So you do something like this. And then you just\nrun your algorithm.",
    "start": "4655690",
    "end": "4663920"
  },
  {
    "text": "You run this algorithm. That's how we define somewhere,\nI guess, here, with k.",
    "start": "4663920",
    "end": "4677730"
  },
  {
    "text": "But here we are using-- so how do we get this algorithm?",
    "start": "4677730",
    "end": "4683489"
  },
  {
    "text": "We're using the regression loss. We're using the square loss\nand linear model, right?",
    "start": "4683490",
    "end": "4688600"
  },
  {
    "text": "But you can also use\nother starting point. For example, suppose you\nstart with logistic regression",
    "start": "4688600",
    "end": "4696409"
  },
  {
    "text": "with the feature. And then you can do the\nsame kind of operations like we have done and then\narrive at a different algorithm",
    "start": "4696409",
    "end": "4706270"
  },
  {
    "text": "in the kernel space. So in some sense, this is called\nkernel trick, so basically--",
    "start": "4706270",
    "end": "4715520"
  },
  {
    "text": "a kernel trick. Or sometimes we'll\ncall this kernelized.",
    "start": "4715520",
    "end": "4721820"
  },
  {
    "text": "It means that you\nturn an algorithm-- so you turn an algorithm\ninto this algorithm--",
    "start": "4721820",
    "end": "4732699"
  },
  {
    "text": "algorithm about phi x\ninto algorithm about k,",
    "start": "4732699",
    "end": "4744920"
  },
  {
    "text": "this kernel function. So you may start with\nlogistic regression with phi x as the feature.",
    "start": "4744920",
    "end": "4750750"
  },
  {
    "text": "I didn't tell you\nhow to do it, but you can do the same\ntype of derivations",
    "start": "4750750",
    "end": "4756510"
  },
  {
    "text": "as we have done today to\nget another algorithm that only uses k. And the algorithm\nwould look something",
    "start": "4756510",
    "end": "4763369"
  },
  {
    "text": "like this, but not\nexactly the same. It would be different. And if you can do\nthis, then you say,",
    "start": "4763370",
    "end": "4769619"
  },
  {
    "text": "but not all the algorithms\ncan be done in this way. Not all the\nalgorithms is amenable",
    "start": "4769620",
    "end": "4776090"
  },
  {
    "text": "to this so-called kernel trick. So some algorithm is possible. Some algorithm is not possible.",
    "start": "4776090",
    "end": "4781860"
  },
  {
    "text": "I think in the homework, we\nhave this perception algorithm for homework 2 which can\nbe turned into kernel--",
    "start": "4781860",
    "end": "4790599"
  },
  {
    "text": "can be kernelized. I guess I'll write kernelized. Or you can apply\nthe kernel trick.",
    "start": "4790600",
    "end": "4796230"
  },
  {
    "text": "In logistic regression, I think\nyou can apply kernel trick. But some of the other\nalgorithms, if you use L1",
    "start": "4796230",
    "end": "4802150"
  },
  {
    "text": "regularization-- if\nyou have heard of it. I know we haven't talked\nabout L1 regularization. But some of the other algorithms\ncannot be kernelized just",
    "start": "4802150",
    "end": "4809330"
  },
  {
    "text": "because your algorithm-- and also, maybe one thing-- in what case you\ncan kernelize this?",
    "start": "4809330",
    "end": "4815650"
  },
  {
    "text": "So the key is that\neverything can be written--",
    "start": "4815650",
    "end": "4821428"
  },
  {
    "text": "the only way you\nuse the features are about the inner product between\ntwo data points, the features at two data points.",
    "start": "4821429",
    "end": "4827730"
  },
  {
    "text": "If your algorithm\nsomehow can be turned into a way such that the only\nthing you care about is this,",
    "start": "4827730",
    "end": "4833020"
  },
  {
    "text": "then you can kernelize it\nbecause just replace this by the kernel. But if your algorithm cannot\nbe written in a way such that",
    "start": "4833020",
    "end": "4840040"
  },
  {
    "text": "the entire algorithm only\ncares about this inner product, then you wouldn't be\nable to kernelize it.",
    "start": "4840040",
    "end": "4848620"
  },
  {
    "text": "Right. OK, and then let me also briefly\nsay a few words about some",
    "start": "4848620",
    "end": "4857960"
  },
  {
    "text": "of the other kernels, right? So I only showed you one kernel. Some of the other\nkernels include--",
    "start": "4857960",
    "end": "4863190"
  },
  {
    "text": "they are actually not that many.",
    "start": "4863190",
    "end": "4868390"
  },
  {
    "text": "Sometimes they are not\nthat many general ones. But sometimes, for\nparticular applications,",
    "start": "4868390",
    "end": "4874400"
  },
  {
    "text": "you can design\nparticular kernels that looks that they\nare useful for you.",
    "start": "4874400",
    "end": "4880290"
  },
  {
    "text": "So some of the kernels are\nsomething like, for example, we have defined this k x, z,\nwhich is something like--",
    "start": "4880290",
    "end": "4889460"
  },
  {
    "text": "for example, this\nis something that is very similar to what we have.",
    "start": "4889460",
    "end": "4896420"
  },
  {
    "text": "So kx is equal to x\ninner product with z plus c to the power k.",
    "start": "4896420",
    "end": "4903780"
  },
  {
    "text": "So for any choice of c\nand k, this is a kernel.",
    "start": "4903780",
    "end": "4911750"
  },
  {
    "text": "And actually, you can write this\nas something like for k is 2--",
    "start": "4911750",
    "end": "4918670"
  },
  {
    "text": "I guess, I actually know-- so for k is 2, actually, I\neven know what's the feature.",
    "start": "4918670",
    "end": "4924039"
  },
  {
    "text": "The feature is\nsomething like this-- c, square root 2c x1, square\nroot 2c x2, so and so forth,",
    "start": "4924040",
    "end": "4935290"
  },
  {
    "text": "and x1 squared up to xd squared. You don't have to know\nexactly what this is. I don't even remember.",
    "start": "4935290",
    "end": "4941138"
  },
  {
    "text": "But for this case,\nfor k is 2, you can write explicitly\nwhat the features are. And for other case, you can also\nwrite explicitly what they are.",
    "start": "4941139",
    "end": "4950040"
  },
  {
    "text": "But you don't have to\ncare that much about it. You only have to know\nthe existence of it. And then when we run it,\nyou just use this kernel.",
    "start": "4950040",
    "end": "4956580"
  },
  {
    "text": "And another kernel is\ncalled Gaussian kernel, which is something like\nexponential minus x minus z",
    "start": "4956580",
    "end": "4967500"
  },
  {
    "text": "squared over 2 sigma squared. Here this is 2 more.",
    "start": "4967500",
    "end": "4974230"
  },
  {
    "text": "And this one, you\ncan also write it as an inner product between\nphi of x and phi of z.",
    "start": "4974230",
    "end": "4980570"
  },
  {
    "text": "But this phi will be\nmuch more complicated. Actually, you can construct-- sorry.",
    "start": "4980570",
    "end": "4986199"
  },
  {
    "text": "You can construct an explicit\nphi such that this is true.",
    "start": "4986199",
    "end": "4991590"
  },
  {
    "text": "But this construction\nwill be very complicated. And also, another thing is\nthat this has to-- so phi has to be infinite dimension.",
    "start": "4991590",
    "end": "5005280"
  },
  {
    "text": "So sometimes phi has to be\ninfinite-dimensional vectors. I know we didn't talk\nabout what exactly",
    "start": "5005280",
    "end": "5012680"
  },
  {
    "text": "an infinite-dimensional\nvector really means. But that's kind of the idea. So you have to really\nuse a lot of dimensions",
    "start": "5012680",
    "end": "5021940"
  },
  {
    "text": "to express this kernel. But you don't really have to\nknow what exactly the phis are",
    "start": "5021940",
    "end": "5027650"
  },
  {
    "text": "because we run the algorithm. You just don't care\nabout this, right? So in some sense,\nthis is the way",
    "start": "5027650",
    "end": "5032820"
  },
  {
    "text": "that we can deal with\ninfinite-dimensional features. So if your features--",
    "start": "5032820",
    "end": "5038810"
  },
  {
    "text": "it's not a problem. As long as your inner\nproduct between two features can be computed efficiently,\nthen the dimensionality",
    "start": "5038810",
    "end": "5045949"
  },
  {
    "text": "of the features\njust don't matter. It can be very large. Or it could be like infinite.",
    "start": "5045950",
    "end": "5051630"
  },
  {
    "text": "It doesn't really matter.",
    "start": "5051630",
    "end": "5061989"
  },
  {
    "text": "And another thing is\nthat sometimes people think of these features\nas a similarity metric,",
    "start": "5061989",
    "end": "5067730"
  },
  {
    "text": "where you can think of\nthis function as a way to measure x and z.",
    "start": "5067730",
    "end": "5073120"
  },
  {
    "text": "Of course, that's just, in\nsome sense, interpretation or intuition.",
    "start": "5073120",
    "end": "5078520"
  },
  {
    "text": "So when x and z are\nsimilar, I think at least in this case,\nwhen x and z are similar,",
    "start": "5078520",
    "end": "5085420"
  },
  {
    "text": "you will get a larger number. So that's why you can think\nof a similarity metric, so.",
    "start": "5085420",
    "end": "5092440"
  },
  {
    "text": "All right, so maybe\na final comment is that, how do you think about\nthis kernel trick in the more",
    "start": "5092440",
    "end": "5099960"
  },
  {
    "text": "than-- by more than, I mean in the last\nfive years or last ten years. So you can see that the\nruntime here, so they always",
    "start": "5099960",
    "end": "5108500"
  },
  {
    "text": "depends on n squared. Actually, it's always n squared. So you save a lot in terms\nof the p the dependency.",
    "start": "5108500",
    "end": "5116520"
  },
  {
    "text": "You just even get rid of\nthe p dependency completely. But you what you lose is that\nyou get n squared instead of n.",
    "start": "5116520",
    "end": "5122770"
  },
  {
    "text": "Recall that before, when\nyou have the vanilla way",
    "start": "5122770",
    "end": "5127811"
  },
  {
    "text": "to implement it,\nyou get n times p. The p is there, but the\npower of n is only 1, right?",
    "start": "5127811",
    "end": "5134219"
  },
  {
    "text": "So but now the power\non n is 2, which",
    "start": "5134219",
    "end": "5139330"
  },
  {
    "text": "means that if your\nn is very large, then this square is\nactually a bad thing. n squared is much worse than\nn So you lose a lot in terms",
    "start": "5139330",
    "end": "5147360"
  },
  {
    "text": "of the dependency on n. And this is probably\none of the reason why kernel method is not\nused a lot these days.",
    "start": "5147360",
    "end": "5155810"
  },
  {
    "text": "That's one reason. I don't think it's the\nfundamental reason. I don't think it's the\nmost important reason. But this is one of the reason.",
    "start": "5155810",
    "end": "5163920"
  },
  {
    "text": "So the reason is that this\ndata in your data set, at least in some applications,\nn it could be a million.",
    "start": "5163920",
    "end": "5169760"
  },
  {
    "text": "And when you have a million\nsquared, that's 10-- that's a trillion, right?",
    "start": "5169760",
    "end": "5175239"
  },
  {
    "text": "So that's just prohibitive. So that's the problem. But of course, there\nare also other ways",
    "start": "5175239",
    "end": "5181180"
  },
  {
    "text": "to speed up this a\nlittle bit if you really care about the runtime. So the second reason why\nthe kernel method is not",
    "start": "5181180",
    "end": "5189190"
  },
  {
    "text": "used as often as before\nis that all of these requires a design--",
    "start": "5189190",
    "end": "5194250"
  },
  {
    "text": "you have to design\nyour k or phi. So whatever you do, it's\nreally a so-called handcrafted",
    "start": "5194250",
    "end": "5201100"
  },
  {
    "text": "feature, right? You do define a\nfunction phi yourself. You can choose them.",
    "start": "5201100",
    "end": "5207159"
  },
  {
    "text": "But it's by human design. Or you design k. So at least one way to interpret\nwhat the neural network will do",
    "start": "5207160",
    "end": "5216250"
  },
  {
    "text": "is that-- I know we didn't\ndiscuss neural networks. But I assume some of\nyou know a little bit. But generally, in high\nlevel, at least one way",
    "start": "5216250",
    "end": "5225750"
  },
  {
    "text": "to interpret why neural\nnetwork is better is that you can say neural\nnetwork is actually,",
    "start": "5225750",
    "end": "5232150"
  },
  {
    "text": "in some sense, learning\nthis function phi. So as you will see, I think\nwhen you have neural networks,",
    "start": "5232150",
    "end": "5238849"
  },
  {
    "text": "in some sense, you can view\nthis, the model, as something like theta transpose phi of x.",
    "start": "5238850",
    "end": "5246369"
  },
  {
    "text": "But this phi of x is\nparameterized by-- maybe let's call them some parameter w.",
    "start": "5246369",
    "end": "5252619"
  },
  {
    "text": "So you can view the\nneural network like this. So it's still a linear function\non top of some feature. But this feature itself is\nlearnable based on a data.",
    "start": "5252619",
    "end": "5260860"
  },
  {
    "text": "So you have a learnable\nfeature instead of a handcrafted feature. And that's one of the\nmany intuitions why",
    "start": "5260860",
    "end": "5268020"
  },
  {
    "text": "neural networks can do better. But we do see these very often.",
    "start": "5268020",
    "end": "5273030"
  },
  {
    "text": "So these days, we have a lot of\nthis kind of feature extractors that are learned by data\nand more and more of this.",
    "start": "5273030",
    "end": "5280730"
  },
  {
    "text": "And if you use them,\nthen you just-- it's much, much better\nthan just a random--",
    "start": "5280730",
    "end": "5286888"
  },
  {
    "text": "much, much better than\nthe kind of polynomial features we designed\nin this lecture.",
    "start": "5286889",
    "end": "5293960"
  },
  {
    "text": "OK, I guess that's all\nI want to say for today. Thanks,",
    "start": "5293960",
    "end": "5297830"
  }
]