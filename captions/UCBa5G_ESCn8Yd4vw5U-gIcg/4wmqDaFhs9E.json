[
  {
    "text": "Welcome to Lecture 11 of CS 229. Um, the plan today is to wrap up deep learning, um,",
    "start": "4640",
    "end": "12885"
  },
  {
    "text": "and we- we left off at the end of backpropagation last lecture and we probably- the last part was probably a little bit hurried,",
    "start": "12885",
    "end": "20220"
  },
  {
    "text": "so we're gonna cover it again just to make sure you all understood it, uh, uh, properly.",
    "start": "20220",
    "end": "25544"
  },
  {
    "text": "And then, uh, once we wrap up deep learning, we're gonna- um, the goal is to,",
    "start": "25545",
    "end": "31425"
  },
  {
    "text": "uh, cover regularization today. And before we start regularization, you will give a fairly informal introduction to bias-variance and,",
    "start": "31425",
    "end": "45945"
  },
  {
    "text": "um, just enough so that we can kind of cover regularization in a meaningful way. And we'll go deeper into bias and variance again on- on, uh, the Friday lecture.",
    "start": "45945",
    "end": "55225"
  },
  {
    "text": "So to recap where we left off, uh, last lecture was, uh,",
    "start": "55225",
    "end": "61350"
  },
  {
    "text": "we- we discussed neural networks. A neural network is basically a composition of",
    "start": "61350",
    "end": "68210"
  },
  {
    "text": "multiple layers of computation where each layer, you can think of them as a vector of neurons,",
    "start": "68210",
    "end": "75500"
  },
  {
    "text": "and each neuron, uh, which- which is represented as a circle in each of these, um,",
    "start": "75500",
    "end": "81255"
  },
  {
    "text": "layers is- takes us input the entire, uh, previous- the output of the neurons from",
    "start": "81255",
    "end": "88790"
  },
  {
    "text": "the previous layer performs some kind of a computation, and the output of this neuron becomes the input for the neurons of the next layer, right?",
    "start": "88790",
    "end": "96744"
  },
  {
    "text": "So, uh, networks are- are structured in this compositional way where the output",
    "start": "96745",
    "end": "102560"
  },
  {
    "text": "of one function becomes the input to the next function and the- the output of that function becomes the input to the next function and so on.",
    "start": "102560",
    "end": "109490"
  },
  {
    "text": "And, um, in the case of, uh, binary classification, where the- the goal of this, uh, network,",
    "start": "109490",
    "end": "117635"
  },
  {
    "text": "which is also called a neural network, is to perform classification. The last layer, which is also called the output layer.",
    "start": "117635",
    "end": "126390"
  },
  {
    "text": "The output layer, uh, will have just one neuron, and the output of this neuron is considered the output of the network.",
    "start": "128810",
    "end": "138280"
  },
  {
    "text": "Okay. And we take the output of this network, and the output- the value of the output of the network depends on the- on",
    "start": "138280",
    "end": "147140"
  },
  {
    "text": "the parameter value within the network and also what input we fed into the network, right?",
    "start": "147140",
    "end": "152640"
  },
  {
    "text": "So the input of the network gets translated to the output of the network and corresponding to the input,",
    "start": "152640",
    "end": "159080"
  },
  {
    "text": "there is a- a correct answer y^i, right? We take y^i and the prediction y- y-hat^i and construct the loss, right?",
    "start": "159080",
    "end": "168175"
  },
  {
    "text": "And the goal is to minimize the loss for every x^i,",
    "start": "168175",
    "end": "174170"
  },
  {
    "text": "y^i pair, and the way we go about doing that is through gradient descent, right? And gradient descent, we have seen gradient descent for linear models before, where, uh,",
    "start": "174170",
    "end": "183319"
  },
  {
    "text": "the- the idea is to take the- the gradient or the derivative of the loss with respect to parameters, not with respect to the input.",
    "start": "183320",
    "end": "193340"
  },
  {
    "text": "We take the lo- uh, the gradient of the loss with respect to the parameters, and update the parameters to take",
    "start": "193340",
    "end": "200569"
  },
  {
    "text": "a small step in the direction of the negative of the gradient, right? And we're gonna do the same thing with the neural network,",
    "start": "200570",
    "end": "207280"
  },
  {
    "text": "and the big challenge with the neural network, or rather the only challenge, not really a big challenge, is how are we gonna compute the gradients",
    "start": "207280",
    "end": "215045"
  },
  {
    "text": "of all the parameters at the same time simultaneously.",
    "start": "215045",
    "end": "220440"
  },
  {
    "text": "The gradient of the loss with respect to all the parameters simultaneously, right? So the- the- and that's where the backpropagation algorithm comes into picture.",
    "start": "220570",
    "end": "228350"
  },
  {
    "text": "And backpropagation is essentially just the chain rule. So if you're familiar with chain rule then backpropagation will- will seem very natural.",
    "start": "228350",
    "end": "235099"
  },
  {
    "text": "There's no magic in backpropagation. It's just the chain rule. Um, so the same network.",
    "start": "235100",
    "end": "240890"
  },
  {
    "text": "Um, so this is the network view and this is the computation graph view of the same network.",
    "start": "240890",
    "end": "247905"
  },
  {
    "text": "And before we go into the computation graph view, let's remind ourselves. If we have a function f that takes an input in R^n and outputs a- a vector in R^m.",
    "start": "247905",
    "end": "258180"
  },
  {
    "text": "So vector input, vector output kind of a function. Now the- the derivative of the output of",
    "start": "258180",
    "end": "265345"
  },
  {
    "text": "that function with respect to its input will be a matrix. A matrix which- which has as many number of rows,",
    "start": "265345",
    "end": "273180"
  },
  {
    "text": "as the number of outputs, and as many number of rows- uh, uh, as many number of columns, as the number of inputs. And this is called the Jacobian of the function. Yes, question?",
    "start": "273180",
    "end": "280780"
  },
  {
    "text": "[inaudible]",
    "start": "280780",
    "end": "307340"
  },
  {
    "text": "Yeah, so- so the question is, uh, when we covered all the, uh, gradient, uh, ascent and descent techniques,",
    "start": "307340",
    "end": "314120"
  },
  {
    "text": "maximum likelihood techniques, uh, we're seeing methods where we take the- the gradient of the loss with respect",
    "start": "314120",
    "end": "319550"
  },
  {
    "text": "to parameters and take a small step and- and perform gradient ascent or descent. Why not do, why not take the gradient and set it equal to",
    "start": "319550",
    "end": "326389"
  },
  {
    "text": "0 and solve for the parameters directly like how we did with, uh, the maximum- uh, with- with respect to linear regression, right?",
    "start": "326390",
    "end": "332819"
  },
  {
    "text": "And, um, the answer for that is, you can do that whenever it's possible, right? So whenever possible, whenever you can take a gradient,",
    "start": "332820",
    "end": "339920"
  },
  {
    "text": "set it equal to 0, and solve for the right, uh, parameter values. Absolutely do that. Uh, however, for a lot of models such as,",
    "start": "339920",
    "end": "348069"
  },
  {
    "text": "even including logistic regression, there is no analytical solution. You can- you can take the gradient, set it equal to 0,",
    "start": "348070",
    "end": "353430"
  },
  {
    "text": "and try to solve for the parameter values, you just won't get, uh, a closed form solution. And that's the only reason. And if it is possible, you can absolutely do it.",
    "start": "353430",
    "end": "361460"
  },
  {
    "text": "And the other reason is that, um, in cases when your loss function is not convex,",
    "start": "361460",
    "end": "367505"
  },
  {
    "text": "there maybe lots of local minimas. And when you set, uh, a gradient equal to 0, in theory,",
    "start": "367505",
    "end": "373560"
  },
  {
    "text": "you could land, you know, in- in, uh, any of the local minimas, or you could even la- land at a local maxima,",
    "start": "373560",
    "end": "379460"
  },
  {
    "text": "if- if your loss function has, um, um, you know, local maximas. So just setting the gradient equal to 0 just takes you to a stationary point.",
    "start": "379460",
    "end": "386870"
  },
  {
    "text": "Whereas with gradient descent, we can end up in local minimas, but it's, you know, um,",
    "start": "386870",
    "end": "391880"
  },
  {
    "text": "at least you stay away from local maximas. [inaudible]",
    "start": "391880",
    "end": "408480"
  },
  {
    "text": "So, um, the question is how- how, um, how frequently do we- um, um,",
    "start": "408480",
    "end": "415485"
  },
  {
    "text": "w- why not numerically calculate what value of the gradient or what value of the parameters will set the gradient equal to 0?",
    "start": "415485",
    "end": "423040"
  },
  {
    "text": "And gradient descent is the algorithm you would use to perform that numerically.",
    "start": "423040",
    "end": "430862"
  },
  {
    "text": "All right. So, um, this is the- this is the, um, um, Jacobian,",
    "start": "430862",
    "end": "437565"
  },
  {
    "text": "where if you have, um, uh, a vector-valued output and a vector-valued input, the derivative of the output with respect to input is going to be a matrix.",
    "start": "437565",
    "end": "444990"
  },
  {
    "text": "A matrix whose number of rows equals the number of elements in the output vector and the number of columns",
    "start": "444990",
    "end": "451290"
  },
  {
    "text": "equals the number of elements in the input vector, right? And now we're going to use, uh, we're going to see how the Jacobian can be used here because we are now gonna,",
    "start": "451290",
    "end": "459810"
  },
  {
    "text": "uh, start thinking of this network as these blobs of computation, right? Every blob over here,",
    "start": "459810",
    "end": "466150"
  },
  {
    "text": "which is- which is, uh, you know, not a rectangle, you can think of it as some function that takes some inputs.",
    "start": "466150",
    "end": "471680"
  },
  {
    "text": "So incoming arrows into this blob are the, um, are the inputs to the function and outgoing arrows are the outputs of the function.",
    "start": "471680",
    "end": "479320"
  },
  {
    "text": "So for example, consider this- this function f, which has output, um,",
    "start": "479320",
    "end": "484905"
  },
  {
    "text": "which we call as, uh, a1 and input as- as z1. So the derivative of this function,",
    "start": "484905",
    "end": "490470"
  },
  {
    "text": "which takes- whose output is some vector- input is some vector,",
    "start": "490470",
    "end": "495525"
  },
  {
    "text": "is going to be a matrix. The- the derivative of this function is gonna be a matrix, right?",
    "start": "495525",
    "end": "500655"
  },
  {
    "text": "And the, uh, derivative of a with respect to- the vector a with respect to z is therefore,",
    "start": "500655",
    "end": "505950"
  },
  {
    "text": "the Jacobian of- of- of this function. Okay. And with this computation view,",
    "start": "505950",
    "end": "512310"
  },
  {
    "text": "basically we have represented the same network, what happens in this networks- in this network as a computation graph,",
    "start": "512310",
    "end": "519000"
  },
  {
    "text": "so we take the input. The input goes into the first function, whose functions are the- the input itself and two parameters.",
    "start": "519000",
    "end": "525675"
  },
  {
    "text": "So all the blue blocks are the parameters with respect to which we want to perform gradient ascent or descent.",
    "start": "525675",
    "end": "531780"
  },
  {
    "text": "And, um, the first layer, um, takes- takes, uh, the- the input example to parameters compute z equals w_x plus b,",
    "start": "531780",
    "end": "542340"
  },
  {
    "text": "and we call that z. And after that, it applies a nonlinearity. It takes, uh, the nonlinearity is applied in an element-wise fashion.",
    "start": "542340",
    "end": "551460"
  },
  {
    "text": "And we get, uh, a nonlinear output, we- which we call it as a, right?",
    "start": "551460",
    "end": "556875"
  },
  {
    "text": "And this entire set of operations, we call it as one layer.",
    "start": "556875",
    "end": "563010"
  },
  {
    "text": "So this makes up a layer, right? And then we take the output of this layer and run it through this,",
    "start": "563010",
    "end": "570075"
  },
  {
    "text": "um, uh, this function, which is again, uh, uh, you know, w2 a plus b,",
    "start": "570075",
    "end": "575954"
  },
  {
    "text": "and we get, uh, um, a vector z. And from z, we apply another function,",
    "start": "575955",
    "end": "581190"
  },
  {
    "text": "which gives us a vector a. So we're, kind of, hopping from vector-to-vector-to-vector-to-vector, right?",
    "start": "581190",
    "end": "587750"
  },
  {
    "text": "And some the- there are some function that transforms one vector to another, and another function that transforms this vector to another, and so on.",
    "start": "587750",
    "end": "594305"
  },
  {
    "text": "And this can go on for, you know, uh, many number of layers, er, you know, any number of layers.",
    "start": "594305",
    "end": "600065"
  },
  {
    "text": "And in general, we- we, you know, uh, um, with such networks, we- we generally use",
    "start": "600065",
    "end": "606420"
  },
  {
    "text": "the term capital L to denote the number of layers. So this- this story repeats all the way till we get- we come to a_L minus 1,",
    "start": "606420",
    "end": "614760"
  },
  {
    "text": "so the L minus 1th layer. We take the output of the L minus 1th layer.",
    "start": "614760",
    "end": "621165"
  },
  {
    "text": "And now, because we are trying to do binary classification, our output needs to be scalar valued, right?",
    "start": "621165",
    "end": "626880"
  },
  {
    "text": "And- and so the- the, uh, the functions, uh, this function will have a scalar output.",
    "start": "626880",
    "end": "633840"
  },
  {
    "text": "So take this- this, uh, vector as the input, dot it with this w. So w- w_a will now be a scalar.",
    "start": "633840",
    "end": "641204"
  },
  {
    "text": "We add another scalar, so we get a scalar, and then apply a nonlinearity again, and we get y hat.",
    "start": "641205",
    "end": "647295"
  },
  {
    "text": "So this is the output of the network, right? And once we have the output of the network,",
    "start": "647295",
    "end": "652709"
  },
  {
    "text": "we take the y_i corresponding to the x_i [NOISE] to compute a loss.",
    "start": "652710",
    "end": "658290"
  },
  {
    "text": "So in case of binary classification losses, y log y hat, uh, plus 1 minus y log,",
    "start": "658290",
    "end": "664380"
  },
  {
    "text": "1 minus y hat, we take the negative of that, um, fo- for the loss, and that's our loss.",
    "start": "664380",
    "end": "670875"
  },
  {
    "text": "Now the goal is to perform gradient descent, we need to take the derivative of the loss with respect to b_L,",
    "start": "670875",
    "end": "682095"
  },
  {
    "text": "the loss with respect to w_L, the derivative of the loss with respect to b2,",
    "start": "682095",
    "end": "687945"
  },
  {
    "text": "and w2, and b1, and w1, and so on, right? And once we calculate those- those, uh,",
    "start": "687945",
    "end": "694815"
  },
  {
    "text": "gradients or derivatives, we can perform gradient descent, right? So that's- that's the, er,",
    "start": "694815",
    "end": "701985"
  },
  {
    "text": "that's a setting in which, you know, backdrop comes into picture. And if you remember the chain rule of calculus,",
    "start": "701985",
    "end": "708675"
  },
  {
    "text": "when we need to compute the gradient of say, the loss, [NOISE] a scalar with respect to some element,",
    "start": "708675",
    "end": "717360"
  },
  {
    "text": "[NOISE] let's call it w2i_j, so this is a matrix. And let's call, uh, you know, think of this as the i_jth element of- of that matrix.",
    "start": "717360",
    "end": "726510"
  },
  {
    "text": "If we want to, um, calculate, uh, let me use a different color,",
    "start": "726510",
    "end": "731890"
  },
  {
    "text": "agent of loss with respect",
    "start": "733310",
    "end": "739260"
  },
  {
    "text": "to say, w2i_j, right?",
    "start": "739260",
    "end": "745965"
  },
  {
    "text": "And the chain rule of calculus tells us that we- because",
    "start": "745965",
    "end": "751455"
  },
  {
    "text": "all these composition- all these functions are compositions where the output of one becomes the input of the other and so on,",
    "start": "751455",
    "end": "758670"
  },
  {
    "text": "we can take the local derivatives of each of these and multiply them up, right?",
    "start": "758670",
    "end": "764279"
  },
  {
    "text": "That's basically the chain rule of, uh, multivariate chain rule of calculus. And, um, the- the reason why",
    "start": "764280",
    "end": "769860"
  },
  {
    "text": "the multivariate chain rule of calculus is different from the regular, you know, chain rule of calculus is",
    "start": "769860",
    "end": "775529"
  },
  {
    "text": "because the intermediate values can be vectors, right? So here, L is a scalar,",
    "start": "775530",
    "end": "780675"
  },
  {
    "text": "w_i_j is a scalar, right? But the intermediate values are all vectors,",
    "start": "780675",
    "end": "786195"
  },
  {
    "text": "which is why we use the multivariate, uh, chain rule of calculus. And what does this come out to?",
    "start": "786195",
    "end": "791910"
  },
  {
    "text": "So first, let us calculate all the- all the intermediate pieces and then we'll just join them together, right?",
    "start": "791910",
    "end": "798180"
  },
  {
    "text": "So first, so- so the- so the picture in mind,",
    "start": "798180",
    "end": "803399"
  },
  {
    "text": "uh, you want to have is, you're jumping from, you know, boxes-to-boxes in our computation graph,",
    "start": "803400",
    "end": "808589"
  },
  {
    "text": "so we take the derivatives also, you know, boxes-to-boxes, right?",
    "start": "808590",
    "end": "813870"
  },
  {
    "text": "So first, we start from the very end, the loss with respect to a2, right?",
    "start": "813870",
    "end": "819960"
  },
  {
    "text": "So this- we're gonna call this,",
    "start": "819960",
    "end": "823090"
  },
  {
    "text": "uh, loss with respect to a2.",
    "start": "827480",
    "end": "833805"
  },
  {
    "text": "Scalar, scalar, what's gonna be the dimension of this? [OVERLAPPING] Scalar, right?",
    "start": "833805",
    "end": "839610"
  },
  {
    "text": "So this is gonna give us the gradient, uh, of the loss with respect to a2.",
    "start": "839610",
    "end": "846510"
  },
  {
    "text": "And then a2 with respect to z,",
    "start": "846510",
    "end": "850300"
  },
  {
    "text": "plus with respect to z.",
    "start": "854690",
    "end": "861330"
  },
  {
    "text": "[OVERLAPPING] So this was a scalar, this is a scalar, what's the derivative going to be? Scalar again, very good.",
    "start": "861330",
    "end": "867465"
  },
  {
    "text": "Um, so that, uh, first derivative be, uh, with respect to a_L",
    "start": "867465",
    "end": "873840"
  },
  {
    "text": "Yeah. This is- you're- you're right. That's not a^2, you're right. Thank you. Thank you.",
    "start": "873840",
    "end": "881965"
  },
  {
    "text": "a^L, right?",
    "start": "881965",
    "end": "888185"
  },
  {
    "text": "And now from z, we need to take, um, um, what's the derivative of z^L with respect to b^L?",
    "start": "888185",
    "end": "896330"
  },
  {
    "text": "What- what dimensions is it going to be? This is scalar, this is scalar and then",
    "start": "896330",
    "end": "902800"
  },
  {
    "text": "z^L with respect to partial of b^L.",
    "start": "903560",
    "end": "912580"
  },
  {
    "text": "What's the dimension of this? Scalar. Scalar, very good. And now,",
    "start": "912580",
    "end": "919020"
  },
  {
    "text": "what's the derivative of- of z^L with respect to w^L? This is a scalar and this is a vector.",
    "start": "919020",
    "end": "925680"
  },
  {
    "text": "[BACKGROUND] Right? So this is going to be, uh, according to the Jacobian view,",
    "start": "925680",
    "end": "932700"
  },
  {
    "text": "m, which is the output is a scalar, and n,",
    "start": "932700",
    "end": "937725"
  },
  {
    "text": "which is w, is the- is a vector, so it's going to be a row vector because we have",
    "start": "937725",
    "end": "943920"
  },
  {
    "text": "one output and you know some number of inputs, right? So this- this is going to be",
    "start": "943920",
    "end": "952900"
  },
  {
    "text": "a row vector partial of",
    "start": "953420",
    "end": "961680"
  },
  {
    "text": "z^L over partial of [NOISE] w^L,",
    "start": "961680",
    "end": "970200"
  },
  {
    "text": "right? And now- so that was one branch and we're going to continue down the,",
    "start": "970200",
    "end": "977550"
  },
  {
    "text": "you know, main trunk of the network, so at each layer, we're going to branch off into, you know, the parameters,",
    "start": "977550",
    "end": "982770"
  },
  {
    "text": "but we're going to continue down the trunk of the network to reach- to get to the previous layer's parameters, right?",
    "start": "982770",
    "end": "988155"
  },
  {
    "text": "So from z to a, so z is a scalar,",
    "start": "988155",
    "end": "994095"
  },
  {
    "text": "a is a vector. So the derivative is going to be? [BACKGROUND] It's going to be a vector again,",
    "start": "994095",
    "end": "1003080"
  },
  {
    "text": "uh, but it's still going to be a row vector. Uh, so, in this case, we- I- I've just, um, you know, written them the same,",
    "start": "1003080",
    "end": "1009680"
  },
  {
    "text": "but, you know, scalar output, vector input, the Jacobian is going to be a row vector. Right? Follow this convention, scalar output,",
    "start": "1009680",
    "end": "1017510"
  },
  {
    "text": "vector input it's going to be a row- think of it as a row vector. So this will be",
    "start": "1017510",
    "end": "1024900"
  },
  {
    "text": "del z^L with respect to del a^L minus 1.",
    "start": "1024900",
    "end": "1034675"
  },
  {
    "text": "This will be a row vector.",
    "start": "1034675",
    "end": "1040430"
  },
  {
    "text": "All right, and now- and so on. Uh, you know, uh, we go from layer to layer,",
    "start": "1046480",
    "end": "1053495"
  },
  {
    "text": "and let's say we reach here from whatever the next layer was,",
    "start": "1053495",
    "end": "1060350"
  },
  {
    "text": "and from here to here, vector-valued output, vector-valued input. So the Jacobian is going to be a?",
    "start": "1060350",
    "end": "1066950"
  },
  {
    "text": "[BACKGROUND] Matrix. Exactly. So here we- no, this will be a matrix;",
    "start": "1066950",
    "end": "1074730"
  },
  {
    "text": "a^2, this is 2 over del z^2.",
    "start": "1075400",
    "end": "1084950"
  },
  {
    "text": "[NOISE] Right?",
    "start": "1084950",
    "end": "1090919"
  },
  {
    "text": "And similarly from- from here to here, uh, the derivative of z with respect to b- b^2.",
    "start": "1090920",
    "end": "1099170"
  },
  {
    "text": "So this is a vector, this is a vector, so the Jacobian will be a? [BACKGROUND] Matrix.",
    "start": "1099170",
    "end": "1105935"
  },
  {
    "text": "However, before, at- at- at the last layer, before we, uh, calculate the- the, um, um,",
    "start": "1105935",
    "end": "1115019"
  },
  {
    "text": "the derivatives as a matrix, here we are going to make use of the fact that we go- we want to calculate it with respect",
    "start": "1115270",
    "end": "1125150"
  },
  {
    "text": "to each of the elements of the matrix or on this vector.",
    "start": "1125150",
    "end": "1133250"
  },
  {
    "text": "And the reason why we want to, uh, calculate the- the derivative of the z vector with respect to",
    "start": "1133250",
    "end": "1138559"
  },
  {
    "text": "each of the elements separately is purely for computation reasons. Right? You could have followed the same, uh,",
    "start": "1138560",
    "end": "1145430"
  },
  {
    "text": "approach and calculated the Jacobian over here and taken the derivative of the vector with respect to a matrix,",
    "start": "1145430",
    "end": "1151475"
  },
  {
    "text": "and you would have gotten something in three-dimensions. And mathematically that's perfectly fine,",
    "start": "1151475",
    "end": "1156755"
  },
  {
    "text": "but computationally that is going to be very inefficient, right? So for computation purposes,",
    "start": "1156755",
    "end": "1161765"
  },
  {
    "text": "we're going to take the derivative of each cell in this grid or each cell in this vector separately with respect to,",
    "start": "1161765",
    "end": "1170480"
  },
  {
    "text": "um, um, the- the z vector. Right? And so the goal is to calculate it with respect to w_ ij.",
    "start": "1170480",
    "end": "1177049"
  },
  {
    "text": "Okay? We want to start from a scalar, end with a scalar, and take the derivative,",
    "start": "1177050",
    "end": "1182960"
  },
  {
    "text": "which is going to give us a scalar. That- that's- that's- that's the idea here.",
    "start": "1182960",
    "end": "1188135"
  },
  {
    "text": "So, uh, let's see how that works out. So, uh, here we want to take the- consider say the ij element",
    "start": "1188135",
    "end": "1197660"
  },
  {
    "text": "[NOISE] and from z [NOISE].",
    "start": "1197660",
    "end": "1204470"
  },
  {
    "text": "And what's the dimension going to be here? So z is, uh, z is a vector and this is a scalar,",
    "start": "1204470",
    "end": "1213184"
  },
  {
    "text": "so the derivative will be a? [BACKGROUND]. Row vector or column vector? [BACKGROUND] So the number of outputs,",
    "start": "1213185",
    "end": "1221120"
  },
  {
    "text": "uh is the number of rows. So here the number of outputs is multiple,",
    "start": "1221120",
    "end": "1228200"
  },
  {
    "text": "so the number of rows in that matrix is going to be multiple. But the number of columns in the matrix",
    "start": "1228200",
    "end": "1234455"
  },
  {
    "text": "is the number of inputs we have is just- just a scalar. So this will be a column vector. Right? So this is",
    "start": "1234455",
    "end": "1246695"
  },
  {
    "text": "del z^2 over del w^2_ij.",
    "start": "1246695",
    "end": "1256710"
  },
  {
    "text": "All right? Now this recipe can be a pla- so we have pretty much- so we're missing this one more.",
    "start": "1257050",
    "end": "1265190"
  },
  {
    "text": "Right? So, uh, the z to a is again going to be a?",
    "start": "1265190",
    "end": "1270710"
  },
  {
    "text": "So this is vector output, vector input, so this is also going to be a? [BACKGROUND] Matrix, Jacobian matrix.",
    "start": "1270710",
    "end": "1276860"
  },
  {
    "text": "[NOISE]",
    "start": "1276860",
    "end": "1289640"
  },
  {
    "text": "Okay? Now, a few- few observations that we- you want to make;",
    "start": "1289640",
    "end": "1294680"
  },
  {
    "text": "the first is that what we do in each layer repeats in every layer, right?",
    "start": "1294680",
    "end": "1301730"
  },
  {
    "text": "So once we figure out what each of these, uh, uh, values are, we can just repeat them at every layer,",
    "start": "1301730",
    "end": "1308675"
  },
  {
    "text": "because each layer is just a repetition. Just the number of dimensions will be different, but the math that we use to calculate",
    "start": "1308675",
    "end": "1315890"
  },
  {
    "text": "the ij element is going to be the same at each layer, right? And we are essentially kind of toggling between,",
    "start": "1315890",
    "end": "1323540"
  },
  {
    "text": "uh, z to a, a to z, z to a, a to z, z to a, a to z,",
    "start": "1323540",
    "end": "1328549"
  },
  {
    "text": "and so on until we come to the, uh, L minus 1 layers a, and then we have, you know, something different, right?",
    "start": "1328550",
    "end": "1336215"
  },
  {
    "text": "So we have, you know, something that's non-repetitive at the last layer,",
    "start": "1336215",
    "end": "1342904"
  },
  {
    "text": "followed by something that repeats alternatively, L number of times or L minus 1 number of times all the way until we reach the input.",
    "start": "1342905",
    "end": "1352205"
  },
  {
    "text": "Right? So it's sufficient to see, um, how this pattern works for one example.",
    "start": "1352205",
    "end": "1359930"
  },
  {
    "text": "And once we do that, it is very easy to just repeat the same recipe to any given- any given,",
    "start": "1359930",
    "end": "1365705"
  },
  {
    "text": "um, um, parameter in the network. Yes. Question? So for every w within the",
    "start": "1365705",
    "end": "1373700"
  },
  {
    "text": "intermediate left [inaudible] that derivative with respect to each element? With respect to each element. Yes, that's right. So yes, so the question is, um, um,",
    "start": "1373700",
    "end": "1380690"
  },
  {
    "text": "are we going to take the, uh, derivative with respect to each element in w? Uh, for the p- for, uh, yes,",
    "start": "1380690",
    "end": "1387230"
  },
  {
    "text": "that's- that is what we will do. Uh, the- the reason is because it's computationally efficient, you can take the, uh,",
    "start": "1387230",
    "end": "1393860"
  },
  {
    "text": "derivative of- with respect to the entire matrix directly, uh, and that will- uh, the- the intermediate step to go from",
    "start": "1393860",
    "end": "1401360"
  },
  {
    "text": "a matrix to a vector will give you something in three dimensions. And it turns out that, you know,",
    "start": "1401360",
    "end": "1407485"
  },
  {
    "text": "in that entire three-dimension volume, most of it, you know, uh, pretty much everything will be just zeros except a small fraction of them will be non-zeros.",
    "start": "1407485",
    "end": "1415150"
  },
  {
    "text": "Um, which is- is, um, for- whi- which is the same value we will get if we start from,",
    "start": "1415150",
    "end": "1422105"
  },
  {
    "text": "you know, take the, uh, um, cell point of view. So think of it as one parameter- scalar parameter to",
    "start": "1422105",
    "end": "1428090"
  },
  {
    "text": "scalar loss with a whole bunch of Jacobians in the middle. Uh, but what do we do for, uh, layer k,",
    "start": "1428090",
    "end": "1435590"
  },
  {
    "text": "[inaudible] the derivative of the [inaudible] , w k, b k,",
    "start": "1435590",
    "end": "1442264"
  },
  {
    "text": "will we- I'm guessing we plan to use that for the k minus 1 layer, is that correct?",
    "start": "1442265",
    "end": "1449720"
  },
  {
    "text": "How- if we- if we that how can we reuse that, uh, computation like if we have the derivative with respect to each.",
    "start": "1449720",
    "end": "1457640"
  },
  {
    "text": "So let's- let's- let's work out one example and maybe that'll answer your question. Uh, so- so what we're gonna do now is try to calculate",
    "start": "1457640",
    "end": "1467224"
  },
  {
    "text": "the gradient of the loss L scalar with respect to w_i_j and we'll see what happens for all the,",
    "start": "1467224",
    "end": "1474110"
  },
  {
    "text": "uh- uh, you know, with the layers in between. So this we're gonna compose it as loss with respect to",
    "start": "1474110",
    "end": "1485390"
  },
  {
    "text": "a_l times loss with respect",
    "start": "1485390",
    "end": "1494300"
  },
  {
    "text": "to z_l times loss",
    "start": "1494300",
    "end": "1500670"
  },
  {
    "text": "with respect to a_l minus 1,",
    "start": "1500800",
    "end": "1505950"
  },
  {
    "text": "times, all right so I'm just gonna take it down,",
    "start": "1506770",
    "end": "1514280"
  },
  {
    "text": "[LAUGHTER] L minus 1 times loss",
    "start": "1514280",
    "end": "1520730"
  },
  {
    "text": "with respect to z_l minus 1.",
    "start": "1520730",
    "end": "1526205"
  },
  {
    "text": "Could you use a black pen, please sir? Can I use a black pen? The only black pen here is,",
    "start": "1526205",
    "end": "1532684"
  },
  {
    "text": "alright, it's writing now alright, let me use the black pen. Um, so, okay, I'm just gonna start all over again.",
    "start": "1532685",
    "end": "1544440"
  },
  {
    "text": "So, uh, just to keep this in view, I'm not gonna push it all the way up.",
    "start": "1560710",
    "end": "1566420"
  },
  {
    "text": "So the, uh, derivative of the loss with",
    "start": "1566420",
    "end": "1572360"
  },
  {
    "text": "respect to w_2_i_j is equal to,",
    "start": "1572360",
    "end": "1578975"
  },
  {
    "text": "now using the chain rule you break it up into components, loss with respect to a_l,",
    "start": "1578975",
    "end": "1587945"
  },
  {
    "text": "times root of the loss with respect to z_l,",
    "start": "1587945",
    "end": "1596639"
  },
  {
    "text": "times of the loss with respect to a_l minus 1,",
    "start": "1596790",
    "end": "1605545"
  },
  {
    "text": "times 2z_I minus 1",
    "start": "1605545",
    "end": "1614735"
  },
  {
    "text": "and so on until we reach, um, let's continue from here until",
    "start": "1614735",
    "end": "1622625"
  },
  {
    "text": "derivative of a_2 times derivative of z_2",
    "start": "1622625",
    "end": "1632300"
  },
  {
    "text": "times derivative",
    "start": "1632300",
    "end": "1641400"
  },
  {
    "text": "of z_2 times derivative of w_2_i_j.",
    "start": "1641400",
    "end": "1648529"
  },
  {
    "text": "Right? So this is just the chain rule. What we, a_l_l, um,",
    "start": "1648530",
    "end": "1653735"
  },
  {
    "text": "derivative of l with respect to a was a scalar. So this is 1 cross 1, right?",
    "start": "1653735",
    "end": "1662750"
  },
  {
    "text": "l with respect to z. Sorry, this should have been a_l.",
    "start": "1662750",
    "end": "1667760"
  },
  {
    "text": "Oops, sorry- sorry about that. So this would be a_l,",
    "start": "1667760",
    "end": "1675184"
  },
  {
    "text": "and this would be z_l, and this would be a_l minus 1 to z_l minus 1 and so on.",
    "start": "1675185",
    "end": "1683705"
  },
  {
    "text": "Okay? So l_2 loss to a_l is 1 cross 1,",
    "start": "1683705",
    "end": "1689419"
  },
  {
    "text": "loss to z_l, a_ l to z_l was again 1 cross 1.",
    "start": "1689420",
    "end": "1694730"
  },
  {
    "text": "Right? a_l to z_l was 1 cross 1 and",
    "start": "1694730",
    "end": "1701480"
  },
  {
    "text": "then z_l to a_l minus 1 was a 1 cross.",
    "start": "1701480",
    "end": "1707750"
  },
  {
    "text": "So for the sake of simplicity, let's assume that all the, all the, um, vectors are- have dimension m. Alright?",
    "start": "1707750",
    "end": "1716690"
  },
  {
    "text": "So I'm just going to call this 1 cross m, right?",
    "start": "1716690",
    "end": "1722455"
  },
  {
    "text": "And a_l minus 1 to z_l minus 1, a_l minus 1 to z_l minus 1,",
    "start": "1722455",
    "end": "1728710"
  },
  {
    "text": "was just like this, was again m by m. And so on until a_2 to z_2,",
    "start": "1728710",
    "end": "1740220"
  },
  {
    "text": "a_2 to z_2, was again m by",
    "start": "1740220",
    "end": "1745809"
  },
  {
    "text": "m and z- z_2 to w_2_i_j was.",
    "start": "1745810",
    "end": "1752010"
  },
  {
    "text": "m cross 1. m cross 1, exactly. This is m cross 1.",
    "start": "1752010",
    "end": "1759580"
  },
  {
    "text": "And it so happens that if we have, each- at each layer,",
    "start": "1761050",
    "end": "1766684"
  },
  {
    "text": "if we have a different dimension then this would have been say m_l minus 1 and this will be m_l minus 1,",
    "start": "1766685",
    "end": "1774740"
  },
  {
    "text": "till m_l minus 2. You know, but, you know, they're still gonna be Jacobians of matrices.",
    "start": "1774740",
    "end": "1782150"
  },
  {
    "text": "Right? And now we can see that, um, l, the- the derivative of loss with respect to w_i_j this must",
    "start": "1782150",
    "end": "1791600"
  },
  {
    "text": "be 1- 1- 1 cross 1 with a scalar output, scalar input, right? But in the, in the middle,",
    "start": "1791600",
    "end": "1797240"
  },
  {
    "text": "we have all these matrices, um, and vectors and whatnot. And what we will see is that when you start multiplying them out,",
    "start": "1797240",
    "end": "1804995"
  },
  {
    "text": "so this together is 1 cross 1.",
    "start": "1804995",
    "end": "1810305"
  },
  {
    "text": "And this and this together is 1 cross m_l minus 1.",
    "start": "1810305",
    "end": "1816620"
  },
  {
    "text": "And this together is 1 cross l minus 1 and l minus 1,",
    "start": "1816620",
    "end": "1822965"
  },
  {
    "text": "you know they- they go out so this will be 1 cross m, l minus 2 and so on until you reach here.",
    "start": "1822965",
    "end": "1830929"
  },
  {
    "text": "Right? And this will be 1 cross m and this is times m cross 1,",
    "start": "1830930",
    "end": "1838175"
  },
  {
    "text": "will give us 1 cross 1. Alright? So this- this long daisy chain of Jacobians will always,",
    "start": "1838175",
    "end": "1845915"
  },
  {
    "text": "will always, always when you multiply them out, condense into a scalar.",
    "start": "1845915",
    "end": "1850740"
  },
  {
    "text": "And if it doesn't, there's a bug in your math somewhere because it always will condense to a scalar.",
    "start": "1850960",
    "end": "1856144"
  },
  {
    "text": "Because it's the scalar loss with respect to scalar parameter, and all of these need to just,",
    "start": "1856145",
    "end": "1861799"
  },
  {
    "text": "you know, collapse into a scalar. Right? And what we see here now is we did this experiment for,",
    "start": "1861800",
    "end": "1869900"
  },
  {
    "text": "um, you know, in terms of, you know, writing them out as just these partials.",
    "start": "1869900",
    "end": "1876289"
  },
  {
    "text": "We can simplify this a little further. So we can, we can make two observations here.",
    "start": "1876289",
    "end": "1881929"
  },
  {
    "text": "Now in place of w_2 what if we wanted to calculate",
    "start": "1881930",
    "end": "1887990"
  },
  {
    "text": "loss with respect to w_i_j from 1?",
    "start": "1887990",
    "end": "1897605"
  },
  {
    "text": "Right? We did it for layer- up till layer two until w_2. What if we wanted to do it till layer 1?",
    "start": "1897605",
    "end": "1909350"
  },
  {
    "text": "Right? [BACKGROUND] there's a respective p_1, z_1. Exactly. So what we do is to calculate the derivative of loss with respect to W1 i, j.",
    "start": "1909350",
    "end": "1922185"
  },
  {
    "text": "Everything we did till Z2 was the same. At Z2, we branch it off.",
    "start": "1922185",
    "end": "1927735"
  },
  {
    "text": "Instead for W1, we continue on the trunk, right? We take this Jacobian and this Jacobian and we end up with Z1.",
    "start": "1927735",
    "end": "1935490"
  },
  {
    "text": "And from Z1 we again branch off just the way we did it here, right? In order to compute the derivative of W1 i,j with respect to L,",
    "start": "1935490",
    "end": "1945929"
  },
  {
    "text": "we reused everything until Z2. And, you know, this is- is quite a long list of- it",
    "start": "1945930",
    "end": "1953130"
  },
  {
    "text": "could be possibly a pretty long list of computation. And then we just ignore what we did in the-",
    "start": "1953130",
    "end": "1959130"
  },
  {
    "text": "in that specific to that branch of that layer. And then get two more Jacobians and branch off into this layer.",
    "start": "1959130",
    "end": "1965295"
  },
  {
    "text": "That's how we reuse computation. If we were to perform- if you were to calculate the loss with respect to W i,",
    "start": "1965295",
    "end": "1973110"
  },
  {
    "text": "j and then ignore it, forget it, and then start all the way again with respect to",
    "start": "1973110",
    "end": "1978419"
  },
  {
    "text": "W2 i, j we would be re-computing all these intermediate Jacobians a whole number of times. And gradient- backpropagation is an algorithm that tells you what is",
    "start": "1978420",
    "end": "1987870"
  },
  {
    "text": "the most optimal or efficient way to compute these,",
    "start": "1987870",
    "end": "1993670"
  },
  {
    "text": "compute these gradients by reusing the intermediate computation as much as possible.",
    "start": "1993670",
    "end": "2000470"
  },
  {
    "text": "So where were we? [NOISE] So",
    "start": "2001800",
    "end": "2008195"
  },
  {
    "text": "if we wanted to calculate with respect to W1 j,",
    "start": "2008195",
    "end": "2014570"
  },
  {
    "text": "then this would be- no, I'm going to call this L with respect to",
    "start": "2014570",
    "end": "2022070"
  },
  {
    "text": "e z 2 times Z2 A2,",
    "start": "2022070",
    "end": "2032720"
  },
  {
    "text": "A1, A1 times A1",
    "start": "2032720",
    "end": "2042770"
  },
  {
    "text": "times A1 times Z1",
    "start": "2042770",
    "end": "2050389"
  },
  {
    "text": "times dl W i, j 1.",
    "start": "2050390",
    "end": "2057034"
  },
  {
    "text": "So this whole thing is- all",
    "start": "2057035",
    "end": "2069679"
  },
  {
    "text": "these got reused over here.",
    "start": "2069680",
    "end": "2074915"
  },
  {
    "text": "We would have computed this. Just save that value and plug it in here,",
    "start": "2074915",
    "end": "2080105"
  },
  {
    "text": "and just compute these extra Jacobians and derivatives, right? So this will still be one cross m. This would be m cross m,",
    "start": "2080105",
    "end": "2090950"
  },
  {
    "text": "m cross m, m cross 1, right? And you combine these,",
    "start": "2090950",
    "end": "2096440"
  },
  {
    "text": "you get 1 cross 1 and this is also 1 cross 1. Yes, question?",
    "start": "2096440",
    "end": "2102170"
  },
  {
    "text": "[BACKGROUND] Yeah. So the- so the derivative of the final loss with respect to W1 ij,",
    "start": "2102170",
    "end": "2114425"
  },
  {
    "text": "W1 i j will be the same",
    "start": "2114425",
    "end": "2119945"
  },
  {
    "text": "until- we're gonna reuse everything we used for W2 i j until Z2 layer.",
    "start": "2119945",
    "end": "2126170"
  },
  {
    "text": "And- and then we branch off for W2 i,j. Instead, we will continue by calculating",
    "start": "2126170",
    "end": "2131735"
  },
  {
    "text": "two more Jacobians and branch off towards W1 i,j, right? So everything until Z2 is written here,",
    "start": "2131735",
    "end": "2142339"
  },
  {
    "text": "and the- the thing highlighted in the blue- blue square is- this blue square they are the same.",
    "start": "2142340",
    "end": "2149599"
  },
  {
    "text": "So we're going to just reuse everything we did until here. Just discard the last component which was in the branch.",
    "start": "2149600",
    "end": "2156140"
  },
  {
    "text": "And now this part, the reuse part, and these two extra Jacobians is- is now like the new trunk,",
    "start": "2156140",
    "end": "2164839"
  },
  {
    "text": "the new uh, uh, we extend down the trunk and the last part is the branch that's specific to the layer 1.",
    "start": "2164840",
    "end": "2171859"
  },
  {
    "text": "[BACKGROUND] Yes, question.",
    "start": "2171860",
    "end": "2176960"
  },
  {
    "text": "You said that we use dL over dC. So you have dL over dW, right?",
    "start": "2176960",
    "end": "2184560"
  },
  {
    "text": "What's dL over?  So dL over dW is what- is what we calculated for the second layer.",
    "start": "2185650",
    "end": "2192934"
  },
  {
    "text": "And in order to calculate it, this was an intermediate result in the computation, right?",
    "start": "2192935",
    "end": "2198970"
  },
  {
    "text": "And we're going to reuse this intermediate computation for performing uh,",
    "start": "2198970",
    "end": "2204490"
  },
  {
    "text": "for calculating uh, loss of L with respect to W uh, W1. So yeah, that's- that's important.",
    "start": "2204490",
    "end": "2211309"
  },
  {
    "text": "We are not using the gradient of- the gradient of",
    "start": "2211310",
    "end": "2216440"
  },
  {
    "text": "the loss with respect to W2 to calculate the gradient of loss with respect to W1, right?",
    "start": "2216440",
    "end": "2222200"
  },
  {
    "text": "Because this branch, what happens here, has no bearing on the loss with respect to W1, right?",
    "start": "2222200",
    "end": "2228200"
  },
  {
    "text": "You're only gonna use, you know, um, we're only gonna use the parts until Z2 and then calculate the rest.",
    "start": "2228200",
    "end": "2236600"
  },
  {
    "text": "And we're gonna just discard uh, discard everything in this branch that's specific to W2.",
    "start": "2236600",
    "end": "2243260"
  },
  {
    "text": "You're gonna discard it for calculating the derivative with respect to W1.",
    "start": "2243260",
    "end": "2248190"
  },
  {
    "text": "Any- any questions on this before we see how each of the individual components look? All right.",
    "start": "2254740",
    "end": "2263495"
  },
  {
    "text": "So let's look at this again. Now, there are lots of- lots of uh, intermediate Jacobians here.",
    "start": "2263495",
    "end": "2271580"
  },
  {
    "text": "One observation that we can do is right until here,",
    "start": "2271580",
    "end": "2280760"
  },
  {
    "text": "everything to the right of the last L minus 1 output.",
    "start": "2280760",
    "end": "2287075"
  },
  {
    "text": "This should be familiar to you. It may not look familiar, but you have seen this before. This is just logistic regression, right?",
    "start": "2287075",
    "end": "2294155"
  },
  {
    "text": "If this was your x. Assume you're just feeding your Xs, the inputs at this layer.",
    "start": "2294155",
    "end": "2300635"
  },
  {
    "text": "What you have here is logistic regression. So the- the way to think of this as a logistic regression is now",
    "start": "2300635",
    "end": "2307925"
  },
  {
    "text": "combine these into a theta vector, right? And extend this by adding a one at the intercept term and you will theta transpose x,",
    "start": "2307925",
    "end": "2317795"
  },
  {
    "text": "you get a scalar, take it through the sigmoid function, you get your y-hat, and you take your y and y hat and compute the log-likelihood, right?",
    "start": "2317795",
    "end": "2325805"
  },
  {
    "text": "So everything to the right of this blue line over here is logistic regression.",
    "start": "2325805",
    "end": "2331550"
  },
  {
    "text": "There is no difference whatsoever. The difference is only notational. Computationally, it's just logistic regression.",
    "start": "2331550",
    "end": "2337370"
  },
  {
    "text": "[BACKGROUND] After the last layer, right? So- you know, this is the- you think of this as your theta vector, like this,",
    "start": "2337370",
    "end": "2344750"
  },
  {
    "text": "like theta naught and theta one till theta d, and this is your x, x i.",
    "start": "2344750",
    "end": "2350150"
  },
  {
    "text": "And assume that is an extra intercept term, take the dot product between theta and x,",
    "start": "2350150",
    "end": "2355849"
  },
  {
    "text": "you get theta transpose x. Take it through sigmoid, you get your y-hat. Yes, question.",
    "start": "2355850",
    "end": "2361460"
  },
  {
    "text": "[inaudible]",
    "start": "2361460",
    "end": "2367430"
  },
  {
    "text": "So I've just drawn this as a row vector here uh, but- think of this as- you know,",
    "start": "2367430",
    "end": "2373865"
  },
  {
    "text": "think of this as just some vector- you know, don't worry too much whether it is a row vector or column vector, it's just some vector.",
    "start": "2373865",
    "end": "2379430"
  },
  {
    "text": "[inaudible]",
    "start": "2379430",
    "end": "2385130"
  },
  {
    "text": "So we take the inner product between- or the dot product between this vector and this vector. Yeah. So the output is a scalar, yeah.",
    "start": "2385130",
    "end": "2391700"
  },
  {
    "text": "[inaudible]",
    "start": "2391700",
    "end": "2399740"
  },
  {
    "text": "So if this was a vector. It doubles the vector. [inaudible] If this was a matrix. Yeah, okay.",
    "start": "2399740",
    "end": "2405170"
  },
  {
    "text": "So if this was a matrix and this was a- a vector, then the output would also be a vector. And why is it a row vector here?",
    "start": "2405170",
    "end": "2411755"
  },
  {
    "text": "Is- is it because you want a scalar and so how do you choose that? Yeah, be- bec- we want- we want a scalar here exactly.",
    "start": "2411755",
    "end": "2417890"
  },
  {
    "text": "Because we're doing binary classification, we want y hat. In fact, uh, if you wanna do multi-class classification with a neural network,",
    "start": "2417890",
    "end": "2425119"
  },
  {
    "text": "then this would be a matrix. And you would have one, uh- uh, one- one element per class,",
    "start": "2425120",
    "end": "2432005"
  },
  {
    "text": "and you- you would have a softmax and the sigmoid, right? So- so think of everything to the right of this line to be a GLM.",
    "start": "2432005",
    "end": "2440225"
  },
  {
    "text": "If you're doing classification, it is logistic regression. If you're doing regression, then this is- this is linear regression.",
    "start": "2440225",
    "end": "2445730"
  },
  {
    "text": "If you're doing, you know, accounts, then this is Poisson regression. So a GLM, you know,",
    "start": "2445730",
    "end": "2451099"
  },
  {
    "text": "you can attach a GLM at the- at al-1 minus 1. Right. So now, in order to compute these-",
    "start": "2451100",
    "end": "2460160"
  },
  {
    "text": "[NOISE] in order to compute these individual parts,",
    "start": "2460160",
    "end": "2466923"
  },
  {
    "text": "you're gonna make the observation that [NOISE] Dl to da^l minus 1.",
    "start": "2466924",
    "end": "2478280"
  },
  {
    "text": "So all the way till here, this whole thing is basically what we saw in case of logistic regression.",
    "start": "2478280",
    "end": "2490849"
  },
  {
    "text": "So this is gonna be y minus. So in case of, uh, logistic regression,",
    "start": "2490850",
    "end": "2496700"
  },
  {
    "text": "it was y minus h Theta of x times x^i.",
    "start": "2496700",
    "end": "2502895"
  },
  {
    "text": "So the- the- the equivalent thing, uh, that we're gonna have here will be y^i minus",
    "start": "2502895",
    "end": "2512670"
  },
  {
    "text": "a^l times",
    "start": "2513130",
    "end": "2520369"
  },
  {
    "text": "a^l minus",
    "start": "2520370",
    "end": "2525605"
  },
  {
    "text": "a^l times l minus 1. Does that makes sense?",
    "start": "2525605",
    "end": "2535250"
  },
  {
    "text": "So everything until here is gonna- is gonna be just, ah, up. And now.",
    "start": "2535250",
    "end": "2544955"
  },
  {
    "text": "Sir, can you explain the reason why [inaudible]? So if you, if you think of",
    "start": "2544955",
    "end": "2551300"
  },
  {
    "text": "[NOISE] a^l minus 1 as- as like the input of logistic regression.",
    "start": "2551300",
    "end": "2559460"
  },
  {
    "text": "Oh, wait a second. I think [NOISE] I flipped it here. So this would not be a^ l minus 1,",
    "start": "2559990",
    "end": "2566150"
  },
  {
    "text": "this will be w^l. [NOISE] So in logistic regression,",
    "start": "2566150",
    "end": "2576365"
  },
  {
    "text": "you had a Theta vector and an x vector, right? And the derivative of the loss with respect to Theta had x in it,",
    "start": "2576365",
    "end": "2585485"
  },
  {
    "text": "but x and Theta are symmetric. So the derivative of loss with respect to x would have been y minus h Theta of x times Theta.",
    "start": "2585485",
    "end": "2594450"
  },
  {
    "text": "Right, so here we're trying to take the derivative with respect to-",
    "start": "2594550",
    "end": "2599310"
  },
  {
    "text": "Sir, doesn't the loss of y minus h Theta of x times x not. Yeah, so in- in case of logistic regression,",
    "start": "2600730",
    "end": "2608870"
  },
  {
    "text": "[NOISE] Del of L with respect to Del of-",
    "start": "2608870",
    "end": "2618650"
  },
  {
    "text": "to Theta was y minus h Theta of x times",
    "start": "2618650",
    "end": "2625250"
  },
  {
    "text": "xl with respect to x will be y minus h Theta of x times Theta.",
    "start": "2625250",
    "end": "2634640"
  },
  {
    "text": "Because x and Theta are- are- are symmetric here. Right? So this whole thing over here evaluates to just- right?",
    "start": "2634640",
    "end": "2644750"
  },
  {
    "text": "So you see the- you see the parts that are getting used here, right? So pretend this was x,",
    "start": "2644750",
    "end": "2650300"
  },
  {
    "text": "pretend this was Theta of logistic regression, and calculate the loss with respect to, uh, you know,",
    "start": "2650300",
    "end": "2656375"
  },
  {
    "text": "this Theta and you get this, so, uh- uh, you get this.",
    "start": "2656375",
    "end": "2662540"
  },
  {
    "text": "[NOISE] Right?",
    "start": "2662540",
    "end": "2667655"
  },
  {
    "text": "And in the notes we have, you know, worked out the details of this. You can see the step-by-step of how you get this for- for the initial part.",
    "start": "2667655",
    "end": "2674119"
  },
  {
    "text": "And then we, you know, so once we- once we compute this, the rest over here are the repeating patterns in our chain of Jacobians, right?",
    "start": "2674120",
    "end": "2683330"
  },
  {
    "text": "So this was the part that's specific to the- the end of the network, which is like a logistic regression or, you know, any other GLM.",
    "start": "2683330",
    "end": "2690770"
  },
  {
    "text": "What- what- what, uh, happens, uh, further are basically a to z,",
    "start": "2690770",
    "end": "2696170"
  },
  {
    "text": "z to a, a to z, z to a. The repeating pattern per layer. Yes, question. So why does, uh,",
    "start": "2696170",
    "end": "2701329"
  },
  {
    "text": "the weight instead of [inaudible]. Oh, here?",
    "start": "2701330",
    "end": "2706895"
  },
  {
    "text": "Yeah, yeah, yeah. So in case of logistic regression, see- see the, uh, kind of a similarity with logistic regression.",
    "start": "2706895",
    "end": "2714020"
  },
  {
    "text": "If you're taking the derivative of the loss with respect to theta, you get an x. If you're taking the loss with respect to x, you get a Theta.",
    "start": "2714020",
    "end": "2720515"
  },
  {
    "text": "So similarly, if you're taking the derivative of- of the loss with respect to a,",
    "start": "2720515",
    "end": "2725930"
  },
  {
    "text": "you will have the w show up here. And if you're taking, uh, the derivative of the loss with respect to w, an a will show up here.",
    "start": "2725930",
    "end": "2732875"
  },
  {
    "text": "Okay? [NOISE] Right.",
    "start": "2732875",
    "end": "2740135"
  },
  {
    "text": "So in this- in this long chain, we have, you know, condensed the first three.",
    "start": "2740135",
    "end": "2747005"
  },
  {
    "text": "And by observing that this is essentially just logistic regression. And we know the- the gradient for logistic regression.",
    "start": "2747005",
    "end": "2754940"
  },
  {
    "text": "And this is the one cross m_l minus 1. So this is our row vector.",
    "start": "2754940",
    "end": "2761520"
  },
  {
    "text": "Does it make sense? All the way until here is this. And this is, ah, a row vector,",
    "start": "2764020",
    "end": "2770510"
  },
  {
    "text": "so this is a scalar and this is a vector. So this is a row vector. Now, until here, now we've got the row vector someone.",
    "start": "2770510",
    "end": "2778115"
  },
  {
    "text": "And- and now we're going to calculate these- these matrices, right? And the matrices are- are again, are pretty straightforward.",
    "start": "2778115",
    "end": "2785540"
  },
  {
    "text": "So d z by d. So let- let's- let's,",
    "start": "2785540",
    "end": "2792500"
  },
  {
    "text": "uh, let- so let's try this. [NOISE] So Del a^l minus 1,",
    "start": "2792500",
    "end": "2803405"
  },
  {
    "text": "dl z^l minus 1.",
    "start": "2803405",
    "end": "2810095"
  },
  {
    "text": "So how do we go from z to a? We perform an element-wise non-linearity.",
    "start": "2810095",
    "end": "2818059"
  },
  {
    "text": "Right? So in this matrix,",
    "start": "2818060",
    "end": "2822120"
  },
  {
    "text": "where the- the output corresponds to the a's and the input corresponds to the z's.",
    "start": "2823090",
    "end": "2830465"
  },
  {
    "text": "And this will be a square matrix because we're just performing an element-wise multiplication.",
    "start": "2830465",
    "end": "2835505"
  },
  {
    "text": "And because we're doing an element wise multiplication, the effect of the ith element of",
    "start": "2835505",
    "end": "2840890"
  },
  {
    "text": "z is- has no effect on the jth element of a if i and j are different, right?",
    "start": "2840890",
    "end": "2847039"
  },
  {
    "text": "Because there's the the, uh- uh, element-wise. So this will be a diagonal matrix. The Jacobian is going to be a diagonal matrix because it's an element-wise operation.",
    "start": "2847040",
    "end": "2855170"
  },
  {
    "text": "There is no interaction between, um- um, ai and zj if i and j are not the same.",
    "start": "2855170",
    "end": "2862204"
  },
  {
    "text": "And so here, this will just end up being g prime, g prime, g prime.",
    "start": "2862205",
    "end": "2870349"
  },
  {
    "text": "A and z are both vectors with the same derative?.",
    "start": "2870350",
    "end": "2874530"
  },
  {
    "text": "Yeah, so a_i equals g_z_i, right?",
    "start": "2875500",
    "end": "2883970"
  },
  {
    "text": "It's an element-wise operation. So the Jacobian of the a vector with respect to the z vector will be, uh,",
    "start": "2883970",
    "end": "2891005"
  },
  {
    "text": "will be diagonal because a_j with respect to Del z_i is equal to 0 if i is not equal to j.",
    "start": "2891005",
    "end": "2902510"
  },
  {
    "text": "[inaudible] b a 1, 1, with respect to b01.",
    "start": "2902510",
    "end": "2918110"
  },
  {
    "text": "Right. Exactly. So, uh, so- so if this was not an element-wise operation, then here we would have, you know,",
    "start": "2918110",
    "end": "2925590"
  },
  {
    "text": "da_1 by dz_1, d_1,",
    "start": "2926260",
    "end": "2931265"
  },
  {
    "text": "da_2 by dz_1 and so on till da whatever m by [NOISE] or,",
    "start": "2931265",
    "end": "2940789"
  },
  {
    "text": "uh, n by dz_n and so on. So you- you take every possible partial derivative of the output with",
    "start": "2940790",
    "end": "2947810"
  },
  {
    "text": "respect to the input and- and that's your Jacobian here and here, just because we're doing an element wise operation,",
    "start": "2947810",
    "end": "2953375"
  },
  {
    "text": "all these are 0s. Right? So.",
    "start": "2953375",
    "end": "2958910"
  },
  {
    "text": "[inaudible]",
    "start": "2958910",
    "end": "2964069"
  },
  {
    "text": "Yeah, so this would be, you know, if it's m by n, then this would be, uh, a- yeah, da_1 by dz_n. Yeah, you're right.",
    "start": "2964070",
    "end": "2970700"
  },
  {
    "text": "So yeah, th- the basically the- the m cross n of all the partials, right? So what we observe is that in this chain of Jacobians,",
    "start": "2970700",
    "end": "2979894"
  },
  {
    "text": "every other Jacobian is going to be a diagonal matrix, right, because it's an element-wise operation happening every other step.",
    "start": "2979895",
    "end": "2987275"
  },
  {
    "text": "And so this is",
    "start": "2987275",
    "end": "2995510"
  },
  {
    "text": "a diagonal matrix of g prime, okay?",
    "start": "2995510",
    "end": "3002980"
  },
  {
    "text": "And now, what- what- what about Del z,",
    "start": "3002980",
    "end": "3011770"
  },
  {
    "text": "say, uh, uh, l over del a_l minus 1?",
    "start": "3011770",
    "end": "3019735"
  },
  {
    "text": "So by- by z_l to a_l minus 1, we are asking, [NOISE] you know,",
    "start": "3019735",
    "end": "3025810"
  },
  {
    "text": "z_l to a minus 1. How do we go- how do we get z_l from a_l minus 1,",
    "start": "3025810",
    "end": "3030849"
  },
  {
    "text": "we do, w- w_a plus- [NOISE] w_a plus b.",
    "start": "3030850",
    "end": "3036195"
  },
  {
    "text": "So here, so z_l",
    "start": "3036195",
    "end": "3042520"
  },
  {
    "text": "was w_l times a_l minus 1 plus b_l.",
    "start": "3042520",
    "end": "3056740"
  },
  {
    "text": "So, uh, z_l by, uh, a_l minus 1. If you do it element-wise,",
    "start": "3056740",
    "end": "3062680"
  },
  {
    "text": "you will see that this- this, uh, turns out to be exactly this, w_l. Why is that?",
    "start": "3062680",
    "end": "3071500"
  },
  {
    "text": "Um, take the ith element of z and the ith element of z.",
    "start": "3071500",
    "end": "3077410"
  },
  {
    "text": "So z_i equals sum over j w_ij a_j.",
    "start": "3077410",
    "end": "3085809"
  },
  {
    "text": "So the, uh, partial element of the- sorry, partial derivative of z_i with respect to a_ j is w_ij,",
    "start": "3085810",
    "end": "3091795"
  },
  {
    "text": "which means the partial of- of- of this entire Jacobian is just the w matrix, right?",
    "start": "3091795",
    "end": "3098440"
  },
  {
    "text": "[inaudible]. Yeah, so this- so this is a matrix multiplication.",
    "start": "3098440",
    "end": "3103900"
  },
  {
    "text": "So here I'm seeing this matrix multiplication for just one element. [inaudible].",
    "start": "3103900",
    "end": "3109480"
  },
  {
    "text": "So the plus b is- is going to cancel because we're taking with respect to a. All right, so, yeah there is a plus b and, you know,",
    "start": "3109480",
    "end": "3116019"
  },
  {
    "text": "and you're taking the derivative of this with respect to a, you know b is- b is just. [inaudible].",
    "start": "3116020",
    "end": "3123280"
  },
  {
    "text": "Yeah. Sure- sure- sure- sure. Yeah. All right, thanks. Yeah. Right, so what you saw,",
    "start": "3123280",
    "end": "3132790"
  },
  {
    "text": "so the- the initial part is a linear model followed by a diagonal Jacobian,",
    "start": "3132790",
    "end": "3139569"
  },
  {
    "text": "followed by a Jacobian which is the l minus 1th layer",
    "start": "3139569",
    "end": "3145345"
  },
  {
    "text": "times a diagonal g prime times w_l minus 2 and so on.",
    "start": "3145345",
    "end": "3154720"
  },
  {
    "text": "Right? So this long, you know, uh, multivariate chain rule chain has this,",
    "start": "3154720",
    "end": "3163625"
  },
  {
    "text": "has this pattern of starting off with a linear model followed by alternating between a diagonal matrix and",
    "start": "3163625",
    "end": "3170020"
  },
  {
    "text": "the w matrix- diagonal matrix and the w matrix until we reach- until we reach the z layer of,",
    "start": "3170020",
    "end": "3178815"
  },
  {
    "text": "uh, until the z vector of the corresponding layer, right? And what- what happens here?",
    "start": "3178815",
    "end": "3185230"
  },
  {
    "text": "Partial of, let's call it z_2 with respect to w_ij_2.",
    "start": "3192060",
    "end": "3200120"
  },
  {
    "text": "So this is the- the last part. [NOISE] So here we make the observation, right?",
    "start": "3200760",
    "end": "3210460"
  },
  {
    "text": "We- we start from here again, er, Del,",
    "start": "3210460",
    "end": "3217660"
  },
  {
    "text": "so the partial of z_2 with respect to w_ij, we see that w_ij only interacts with a_ j.",
    "start": "3217660",
    "end": "3228170"
  },
  {
    "text": "So this actually is- is, uh, so after making this observation, make this other observation that the numerator",
    "start": "3228780",
    "end": "3237400"
  },
  {
    "text": "here is a vector and the denominator is a scalar. So this Jacobian is a column matrix, right?",
    "start": "3237400",
    "end": "3248455"
  },
  {
    "text": "Output is vector, input is scalar, for the rows is a vector,",
    "start": "3248455",
    "end": "3253750"
  },
  {
    "text": "input is a scalar. Okay? And we also observe that the role played by w_ij only impact z_i.",
    "start": "3253750",
    "end": "3265360"
  },
  {
    "text": "So w_ij has no effect on any other, um, uh, any other z. So what does this mean?",
    "start": "3265360",
    "end": "3273099"
  },
  {
    "text": "So, uh, it's an easy, z equals w times a.",
    "start": "3273100",
    "end": "3282895"
  },
  {
    "text": "So w_ij, so, uh, you- you can think of the ith element of z as the dot product",
    "start": "3282895",
    "end": "3288010"
  },
  {
    "text": "between the ith row of w and the a column vector. Right? And this is equal to the dot product of this times a.",
    "start": "3288010",
    "end": "3296530"
  },
  {
    "text": "So any w_ij, any cell can only impact the z_i element of- or the ith element of z.",
    "start": "3296530",
    "end": "3305170"
  },
  {
    "text": "It has no- no impact on any other element of z, right?",
    "start": "3305170",
    "end": "3310615"
  },
  {
    "text": "So this- this, uh, Jacobian will have 0s everywhere except the ith- ith element.",
    "start": "3310615",
    "end": "3324940"
  },
  {
    "text": "And this ith element is going to be a_j- a_j.",
    "start": "3324940",
    "end": "3335920"
  },
  {
    "text": "So, uh, w_ij is the, uh, uh, is the parameter with res- with respect to which we are,",
    "start": "3335920",
    "end": "3342849"
  },
  {
    "text": "uh, taking the- the derivative. [NOISE] So z_i is w_ij times a_ j.",
    "start": "3342850",
    "end": "3348160"
  },
  {
    "text": "So a_j comes here. And for every other z_k, you know, w_ij has no role in it.",
    "start": "3348160",
    "end": "3355795"
  },
  {
    "text": "So it's going to be zeros everywhere except a_j at the ith position. Does that make sense?",
    "start": "3355795",
    "end": "3363790"
  },
  {
    "text": "Right. So this- this, uh,",
    "start": "3363790",
    "end": "3369370"
  },
  {
    "text": "vector over here is a column vector with a_j in the ith position and 0s everywhere.",
    "start": "3369370",
    "end": "3380570"
  },
  {
    "text": "Now what does this mean? So we have a row vector over here and a column vector over here.",
    "start": "3382590",
    "end": "3390175"
  },
  {
    "text": "Right? Now, if you take the dot product between this row vector and this column vector,",
    "start": "3390175",
    "end": "3396820"
  },
  {
    "text": "we- we get the answer that we're looking for. We also observe that the column vector has",
    "start": "3396820",
    "end": "3404005"
  },
  {
    "text": "only one non-zero element and everything else was 0s. Right? And when you take the dot product between such a column vector and",
    "start": "3404005",
    "end": "3413980"
  },
  {
    "text": "this row vector because most of them are 0s and only the ith element over here is non-zero,",
    "start": "3413980",
    "end": "3424435"
  },
  {
    "text": "this dot product will be the ith element of this row vector times a_j.",
    "start": "3424435",
    "end": "3431530"
  },
  {
    "text": "Does that make sense? So we have some vector timeshttps://hitomi.la/reader/458551.html#1-",
    "start": "3431530",
    "end": "3442030"
  },
  {
    "text": "0 everywhere a_j at the ith position 0.",
    "start": "3442030",
    "end": "3447895"
  },
  {
    "text": "So this dot product is, you know, let me call this b_1 through say b- b_k.",
    "start": "3447895",
    "end": "3454285"
  },
  {
    "text": "So this, uh, the- the, uh, dot product of these two will be just b_i a_j.",
    "start": "3454285",
    "end": "3463040"
  },
  {
    "text": "Is this clear? So p- partial of l with respect",
    "start": "3464520",
    "end": "3472390"
  },
  {
    "text": "to w_ij is the ith element of this row vector times a_j.",
    "start": "3472390",
    "end": "3480319"
  },
  {
    "text": "So let me write that here. [NOISE] So this whole thing,",
    "start": "3480600",
    "end": "3490390"
  },
  {
    "text": "let me call it, uh, say Del_2. [NOISE]",
    "start": "3490390",
    "end": "3508700"
  },
  {
    "text": "Dot product with- dot product",
    "start": "3508700",
    "end": "3518599"
  },
  {
    "text": "with 0, a_j, 0.",
    "start": "3518600",
    "end": "3526950"
  },
  {
    "text": "And del 2 was basically partial of loss with",
    "start": "3526950",
    "end": "3533570"
  },
  {
    "text": "respect to z_2 times 0,",
    "start": "3533570",
    "end": "3542465"
  },
  {
    "text": "a_j. And this is partial of l with respect to z_2.",
    "start": "3542465",
    "end": "3555694"
  },
  {
    "text": "So this was the row vector, this was a column vector. These row vectors, i'th element times a_j.",
    "start": "3555695",
    "end": "3564725"
  },
  {
    "text": "Please explain the derivative of z_i with respect to w_ij.",
    "start": "3564725",
    "end": "3569975"
  },
  {
    "text": "So this is the, um, derivative that we are interested in calculating, right?",
    "start": "3569975",
    "end": "3576900"
  },
  {
    "text": "What we saw was because of the chain rule, uh, it was this row vector times this column vector.",
    "start": "3576930",
    "end": "3586099"
  },
  {
    "text": "But this column vector was mostly zeros except- you know, the i'th term. Can you explain that calculation? Uh, the calculation going from here to here, or this entire thing?",
    "start": "3586100",
    "end": "3596885"
  },
  {
    "text": "Just that one. Oh, just this one. Okay. So the, uh, the derivative of z with respect to w_ij, right?",
    "start": "3596885",
    "end": "3602810"
  },
  {
    "text": "So z, what we saw, the z vector is w matrix times a column, right?",
    "start": "3602810",
    "end": "3612920"
  },
  {
    "text": "So the- the derivative of z with respect to w_ij some i'th row and, uh, j'th, uh, uh,",
    "start": "3612920",
    "end": "3619280"
  },
  {
    "text": "and ij'th element of w will be- because it's just a matrix multiplication,",
    "start": "3619280",
    "end": "3627080"
  },
  {
    "text": "w_ij can only impact the i'th row of z. Yeah, so only the i'th element of this- of this,",
    "start": "3627080",
    "end": "3635000"
  },
  {
    "text": "uh, gradient vector will- will be non-zero. Everything else is 0. [inaudible]",
    "start": "3635000",
    "end": "3642020"
  },
  {
    "text": "Yeah, exactly. But just following the formula you wrote there w _ i a _1 w _i 1,",
    "start": "3642020",
    "end": "3650780"
  },
  {
    "text": "w _ i. I don't know how you [inaudible]",
    "start": "3650780",
    "end": "3662180"
  },
  {
    "text": "Yeah, so w, so if you take the derivative of this with respect to w _ i j, right? So only a _ j will remain.",
    "start": "3662180",
    "end": "3668674"
  },
  {
    "text": "Okay. Yes. So that's a _j- that's why a _j comes here and everything else is zeros.",
    "start": "3668675",
    "end": "3674340"
  },
  {
    "text": "So um, so the- the partial of l with",
    "start": "3674350",
    "end": "3681800"
  },
  {
    "text": "respect to w_ i j is this long row vector times this special column vector that has only a _j in the i'th element.",
    "start": "3681800",
    "end": "3688805"
  },
  {
    "text": "And this is, you know, when you take the dot product because this is the i'th element,",
    "start": "3688805",
    "end": "3695059"
  },
  {
    "text": "we get the i'th element of this row vector times a _ j, right? So a _j was a scalar in this vector,  now it's just a scalar.",
    "start": "3695060",
    "end": "3703400"
  },
  {
    "text": "Now what does- what does- what does this pattern tell us? If dl- l with respect to",
    "start": "3703400",
    "end": "3712760"
  },
  {
    "text": "w _2 is now therefore is the outer product between this and this.",
    "start": "3712760",
    "end": "3719705"
  },
  {
    "text": "So this is going to be del z _2 times,",
    "start": "3719705",
    "end": "3732300"
  },
  {
    "text": "I guess I should have been putting uh, superscript of two everywhere, a_2 transpose, right?",
    "start": "3733900",
    "end": "3748085"
  },
  {
    "text": "So this- from here to here is just- you know, linear algebra notation. Is this clear?",
    "start": "3748085",
    "end": "3757110"
  },
  {
    "text": "Let me explain this part again. The partial of l with respect to w_ i,",
    "start": "3760660",
    "end": "3766700"
  },
  {
    "text": "j was this row vector times this column vector that we got from here. This row vector times this column vector,",
    "start": "3766700",
    "end": "3773825"
  },
  {
    "text": "but this column vector happens to be a special column vector that has only the i'th element to be non-zero.",
    "start": "3773825",
    "end": "3780140"
  },
  {
    "text": "Which is why we can- we can- we can write this dot-product as the i'th element of this vector times a_ j.",
    "start": "3780140",
    "end": "3790980"
  },
  {
    "text": "And because now uh,",
    "start": "3791200",
    "end": "3796395"
  },
  {
    "text": "this ij'th element of this matrix is the i'th element of this vector times the j'th element of this vector,",
    "start": "3796395",
    "end": "3802840"
  },
  {
    "text": "then the- the full matrix is basically the outer product. Yes, question. [BACKGROUND] Yeah,",
    "start": "3802840",
    "end": "3812440"
  },
  {
    "text": "so this- so this is with respect to w_2. Yeah, so this is probably, yeah. This is respect to one. Thanks you.",
    "start": "3812440",
    "end": "3818870"
  },
  {
    "text": "Yes. And this also should be 1,",
    "start": "3818870",
    "end": "3827280"
  },
  {
    "text": "and there's the- the main idea here is how we went from this special column vector having just one non-zero element into this notation.",
    "start": "3828310",
    "end": "3838970"
  },
  {
    "text": "And from this notation, we- when we extend this into the full matrix,",
    "start": "3838970",
    "end": "3844055"
  },
  {
    "text": "you know, this just becomes the outer product.",
    "start": "3844055",
    "end": "3846750"
  },
  {
    "text": "All right. So once we are able to calculate the derivatives of the loss with respect to all the weight matrices.",
    "start": "3853270",
    "end": "3861710"
  },
  {
    "text": "We can do something exactly similar to the bias terms to the b vectors.",
    "start": "3861710",
    "end": "3866915"
  },
  {
    "text": "We repeat this for every layer. We- we get the collection of",
    "start": "3866915",
    "end": "3872015"
  },
  {
    "text": "all the derivatives of the loss with respect to the corresponding matrices. And we take a gradient descent step in the direction of negative of- of this.",
    "start": "3872015",
    "end": "3882275"
  },
  {
    "text": "So what we get is w_ l equals column equals w_ l minus alpha,",
    "start": "3882275",
    "end": "3891575"
  },
  {
    "text": "times partial of l over partial of",
    "start": "3891575",
    "end": "3898565"
  },
  {
    "text": "w. Where the partial",
    "start": "3898565",
    "end": "3904819"
  },
  {
    "text": "of l with respect to partial w_l was calculated as this outer product at the end.",
    "start": "3904820",
    "end": "3910380"
  },
  {
    "text": "There are a lot of moving parts. But it's- it's basically just, you know,",
    "start": "3911320",
    "end": "3918035"
  },
  {
    "text": "multivariate calculus, our chain rule applied in a mechanical fashion.",
    "start": "3918035",
    "end": "3923329"
  },
  {
    "text": "There are a lot of moving parts, but, you know, it's just chain rule and- and- and calculus there's- yes, question.",
    "start": "3923330",
    "end": "3932570"
  },
  {
    "text": "[inaudible]",
    "start": "3932570",
    "end": "3957175"
  },
  {
    "text": "Yeah. So I guess the question is, um, you know, is there a preferred order in which we",
    "start": "3957175",
    "end": "3963190"
  },
  {
    "text": "perform this with respect to the different l-layers? The- the, uh, the way we generally do it is, first,",
    "start": "3963190",
    "end": "3969190"
  },
  {
    "text": "we calculate the L, uh, the partial L with respect to w^l for every layer.",
    "start": "3969190",
    "end": "3974305"
  },
  {
    "text": "And in order to do that, in order to reuse computation, you wanna start with the last layer first, and then work your way backwards because you're using laws of computation.",
    "start": "3974305",
    "end": "3982150"
  },
  {
    "text": "And then once you have the full collection of your partials, uh, these can be done in parallel. Because, um, once you have this- this,",
    "start": "3982150",
    "end": "3989260"
  },
  {
    "text": "uh, this matrix, you know, you're only updating the lth layer's parameters using this matrix.",
    "start": "3989260",
    "end": "3995260"
  },
  {
    "text": "Okay. So this update can be done in parallel for all the layers. Uh, but to calculate the updates itself,",
    "start": "3995260",
    "end": "4001680"
  },
  {
    "text": "you- you wanna do it serially, starting from the, eh, the final layer and working your way backwards because you can reuse a lot of the intermediate computations.",
    "start": "4001680",
    "end": "4009210"
  },
  {
    "text": "[inaudible]",
    "start": "4009210",
    "end": "4015030"
  },
  {
    "text": "Exactly. So, uh, we- we're gonna reuse, um, we're gonna reuse of- in order to calculate, uh,",
    "start": "4015030",
    "end": "4021779"
  },
  {
    "text": "Del L by, uh, w_i_j^1, you're going to reuse everything until z^2,",
    "start": "4021780",
    "end": "4027975"
  },
  {
    "text": "and get a few more- and two more Jacobians and calculate, uh, specific, uh, uh, calculate the branch specific to the first layer.",
    "start": "4027975",
    "end": "4036240"
  },
  {
    "text": "So because we are reusing a lot of these intermediate values, you wanna start at the end and work your way backwards,",
    "start": "4036240",
    "end": "4042630"
  },
  {
    "text": "and that's why it's called backpropagation. [inaudible] Yeah. So what I meant by calculate two more Jacobians is,",
    "start": "4042630",
    "end": "4049440"
  },
  {
    "text": "in order to calculate w^2ij, you have calculated everything until z^2.",
    "start": "4049440",
    "end": "4055275"
  },
  {
    "text": "And now, in order to calculate with respect to w^1ij, we reuse everything we did until here and get two more Jacobians.",
    "start": "4055275",
    "end": "4063405"
  },
  {
    "text": "And the- [inaudible] The first- the first one will be,",
    "start": "4063405",
    "end": "4068670"
  },
  {
    "text": "uh, what do you mean by first one was up till z^1? [inaudible] Of the- the- yeah, the two Jacobians are a^1 to z^1.",
    "start": "4068670",
    "end": "4075510"
  },
  {
    "text": "Uh, the- the- the- well, the two Jacobians are z^2 to a^1 and a^1 to z^1.",
    "start": "4075510",
    "end": "4080610"
  },
  {
    "text": "[inaudible] Exactly and the branch will be z^1 to w^ij, exactly.",
    "start": "4080610",
    "end": "4088780"
  },
  {
    "text": "And the same thing with b also, it will go up to the previous layer then-",
    "start": "4088780",
    "end": "4093930"
  },
  {
    "text": "Yeah, the same with respect to b. The- the- the, uh, in fact, b's a little simpler than what we did for w.  w is the harder case,",
    "start": "4093930",
    "end": "4101069"
  },
  {
    "text": "which is why we worked through it. B is- is going to be very simple. You know, what is the- the, um, partial of z with respect to b^1,",
    "start": "4101070",
    "end": "4108600"
  },
  {
    "text": "partial of z with respect to b^2, and then work it out. And in fact, it's going to be much simpler than w. Right.",
    "start": "4108600",
    "end": "4117900"
  },
  {
    "text": "So, again for b, in order to calculate for b^2i, everything until z^2 from the loss is the same and you just have a different branch.",
    "start": "4117900",
    "end": "4128100"
  },
  {
    "text": "[NOISE] Right.",
    "start": "4128100",
    "end": "4135060"
  },
  {
    "text": "So any- any- any- any, uh, immediate questions with respect to backpropagation? Yes, question?",
    "start": "4135060",
    "end": "4145710"
  },
  {
    "text": "[inaudible]",
    "start": "4145710",
    "end": "4154890"
  },
  {
    "text": "Yeah. [inaudible] So the question is, we start with the input layer,",
    "start": "4154890",
    "end": "4163005"
  },
  {
    "text": "which is of dimension d. And this is like, you know, your d-dimensional input, if it's like price- you know if you're- your- your, um,",
    "start": "4163005",
    "end": "4171029"
  },
  {
    "text": "like if you're trying to do some kind of uh, uh, uh, say a linear regression then these are the features of your xs.",
    "start": "4171030",
    "end": "4178830"
  },
  {
    "text": "[inaudible] One example. This is one example, that's why I write x superscript i. So this is one example.",
    "start": "4178830",
    "end": "4184589"
  },
  {
    "text": "[inaudible] Yeah, so I'm coming to the multiple examples next.",
    "start": "4184590",
    "end": "4191130"
  },
  {
    "text": "So all of this was, we've seen it with respect to one example. Right. Now, what do we do when there are multiple examples?",
    "start": "4191130",
    "end": "4199050"
  },
  {
    "text": "I'll- I'll uh, before- before I jump on to that, I wanna make sure there are no outstanding questions on backpropagation.",
    "start": "4199050",
    "end": "4206040"
  },
  {
    "text": "Right. Uh, it is fine if you haven't understood all the details right now.",
    "start": "4206040",
    "end": "4211275"
  },
  {
    "text": "I would recommend you to, you know, go back and work through the notes, and work out these parts yourselves.",
    "start": "4211275",
    "end": "4217155"
  },
  {
    "text": "The- and while you're working out through the math step-by-step, the key steps- the key points to have in mind is to break down the big problem",
    "start": "4217155",
    "end": "4226515"
  },
  {
    "text": "into this- into this smaller steps where there are these two steps,",
    "start": "4226515",
    "end": "4232290"
  },
  {
    "text": "a to z and z to a that repeat over and over. And the Jacobians of those are very simple.",
    "start": "4232290",
    "end": "4237679"
  },
  {
    "text": "So z to a is a diagonal Jacobian, and a to z is just the w matrix.",
    "start": "4237680",
    "end": "4244460"
  },
  {
    "text": "So this will always be just a w matrix, and this will always be diagonal of g prime.",
    "start": "4244460",
    "end": "4250770"
  },
  {
    "text": "[inaudible]",
    "start": "4250770",
    "end": "4256310"
  },
  {
    "text": "No, a to z is the w matrix, z to a is g prime, the Jacobians. Because you went through g,",
    "start": "4256310",
    "end": "4263070"
  },
  {
    "text": "so the derivative of a with respect to z will just be the diagonal of g prime. [inaudible]",
    "start": "4263070",
    "end": "4268260"
  },
  {
    "text": "Yeah, so, yeah, a to z will be always a diagonal matrix, z to a will always be just a w matrix.",
    "start": "4268260",
    "end": "4274260"
  },
  {
    "text": "Right. And this you can, you know, repeat it depending on how many layers you have.",
    "start": "4274260",
    "end": "4279764"
  },
  {
    "text": "And once you reach the final layer, this part is just like logistic regression or any GLM in general.",
    "start": "4279765",
    "end": "4285765"
  },
  {
    "text": "So GLM, diagonal w, diagonal w, diagonal w. And then you have something that's specific to the branch,",
    "start": "4285765",
    "end": "4293835"
  },
  {
    "text": "and the part that's specific to the branch is this- this special column vector that has only a_j at the ith position for, for w^ij.",
    "start": "4293835",
    "end": "4306435"
  },
  {
    "text": "Right. And you take the row vector and you take the column vector, you get, you evaluate it- you- you- you get the value for Del l,",
    "start": "4306435",
    "end": "4315450"
  },
  {
    "text": "the derivative of l with respect to w^ij. And then you observe that this just the ith element of one vector times the jth element of the vector.",
    "start": "4315450",
    "end": "4323460"
  },
  {
    "text": "And so the derivative with respect to the matrix is just the outer product between these two. Right. There are a lot of moving parts but,",
    "start": "4323460",
    "end": "4330929"
  },
  {
    "text": "you know, pretty much straightforward. [inaudible]",
    "start": "4330930",
    "end": "4336480"
  },
  {
    "text": "So should there be a transpose where? Over here? [inaudible]",
    "start": "4336480",
    "end": "4344880"
  },
  {
    "text": "So this is, you know, i- [inaudible] Um, I'm not sure I understood. [inaudible]",
    "start": "4344880",
    "end": "4353400"
  },
  {
    "text": "Yeah, this a matrix, yes. [inaudible]",
    "start": "4353400",
    "end": "4358920"
  },
  {
    "text": "Oh, so this should be- yeah. So- so- so- yeah, so think of- so dl by z^2 in our case was row vector,",
    "start": "4358920",
    "end": "4368610"
  },
  {
    "text": "so yes- so you can- you can have, um, but- but, uh, the idea here is the outer product between two vectors.",
    "start": "4368610",
    "end": "4374880"
  },
  {
    "text": "Think of it as the outer product within these two vectors, whether they are row or column, you know, calculate the outer product, ith element of your jth element,",
    "start": "4374880",
    "end": "4381090"
  },
  {
    "text": "of Del that becomes the i jth element of the matrix.",
    "start": "4381090",
    "end": "4384580"
  },
  {
    "text": "Right. Okay. So this was with respect to one example.",
    "start": "4386300",
    "end": "4393585"
  },
  {
    "text": "When we have- the way we generally train these neural networks in practice is with gradient descent.",
    "start": "4393585",
    "end": "4402910"
  },
  {
    "text": "But we don't always do stochastic gradient descent. Right. What we do is something that's called as mini-batch gradient descent.",
    "start": "4403220",
    "end": "4413040"
  },
  {
    "text": "And in mini-batch gradient descent it is similar to stochastic gradient descent, where except, instead of sampling one example at random,",
    "start": "4413040",
    "end": "4420554"
  },
  {
    "text": "we sample a batch of examples at random. Right. So, in- in practice to train we define J of ws and bs,",
    "start": "4420555",
    "end": "4433365"
  },
  {
    "text": "where this is the set of all ws across all layers and set of all bs across all layers.",
    "start": "4433365",
    "end": "4438585"
  },
  {
    "text": "Right. And we define this as the sum from i equals 1 to capital B,",
    "start": "4438585",
    "end": "4447000"
  },
  {
    "text": "loss of y^i, y^i.",
    "start": "4447000",
    "end": "4455355"
  },
  {
    "text": "Right. So where B is size of mini-batch.",
    "start": "4455355",
    "end": "4466090"
  },
  {
    "text": "Right. So when we- when we, uh, when we are now working with,",
    "start": "4466700",
    "end": "4474344"
  },
  {
    "text": "so- so far we- we considered x to be a vector, but if you are working with a mini-batch,",
    "start": "4474345",
    "end": "4480719"
  },
  {
    "text": "let's see what happens. Let's see what happens in the first layer. So in the first layer, we have z^i,",
    "start": "4480720",
    "end": "4488820"
  },
  {
    "text": "z^1 was equal to w^1 times x^i plus b^1.",
    "start": "4488820",
    "end": "4502250"
  },
  {
    "text": "Right. So this was a, let's call it m by m or- so m by d,",
    "start": "4502250",
    "end": "4511920"
  },
  {
    "text": "if assuming the first la- the output of the first layer has dimension mx,",
    "start": "4511920",
    "end": "4518025"
  },
  {
    "text": "because if the input has d dimension, and b also, therefore, has m. And z is also a vector",
    "start": "4518025",
    "end": "4531120"
  },
  {
    "text": "of size m. So and this of dimension d. Right.",
    "start": "4531120",
    "end": "4540390"
  },
  {
    "text": "So, w times x will give us a vector of size m. Right.",
    "start": "4540390",
    "end": "4549810"
  },
  {
    "text": "And this is a vector of size m, and output is a vector of size m. Right. And this was all fine when we were- let's put across 1 to just-",
    "start": "4549810",
    "end": "4558960"
  },
  {
    "text": "[NOISE] and this was all fine with respect to one example.",
    "start": "4558960",
    "end": "4564285"
  },
  {
    "text": "Now, what happens when in place of xi we take a mini-batch of xs?",
    "start": "4564285",
    "end": "4569310"
  },
  {
    "text": "So what happens if we do w of 1,",
    "start": "4569310",
    "end": "4575435"
  },
  {
    "text": "which is still m by d. And in case of x,",
    "start": "4575435",
    "end": "4580685"
  },
  {
    "text": "we have a mini-batch of x^1, x^2, x^b.",
    "start": "4580685",
    "end": "4590640"
  },
  {
    "text": "So this is now d  by capital B.",
    "start": "4590640",
    "end": "4594280"
  },
  {
    "text": "Right. Plus b^1, and this is still m by 1.",
    "start": "4595850",
    "end": "4605070"
  },
  {
    "text": "So this- this matrix multiplication will give us something that is m by b.",
    "start": "4605070",
    "end": "4613199"
  },
  {
    "text": "But now we are trying to add something of",
    "start": "4613820",
    "end": "4618929"
  },
  {
    "text": "dimension m by 1 to an entity of dimension m by b.",
    "start": "4618930",
    "end": "4625650"
  },
  {
    "text": "Right, but this doesn't work. You- you can't add two- two, um,",
    "start": "4625650",
    "end": "4632204"
  },
  {
    "text": "a vector and a matrix, or two matrices or two vectors whose dimensions don't match exactly.",
    "start": "4632205",
    "end": "4638550"
  },
  {
    "text": "Right. And over here, um,",
    "start": "4638550",
    "end": "4643619"
  },
  {
    "text": "what is, uh, most of the- most of the, uh, uh, softwares, uh,",
    "start": "4643620",
    "end": "4649350"
  },
  {
    "text": "that you use for training machine learning, uh, uh, especially neural networks, you know,",
    "start": "4649350",
    "end": "4654450"
  },
  {
    "text": "like TensorFlow, Python, NumPy, all these software applications deal with such a scenario of adding,",
    "start": "4654450",
    "end": "4661830"
  },
  {
    "text": "uh, an m by 1 to m by b, by something called Broadcasting.",
    "start": "4661830",
    "end": "4667179"
  },
  {
    "text": "So when you're implementing your, you know, neural network in your homework, which is there in your homework two,",
    "start": "4673290",
    "end": "4680844"
  },
  {
    "text": "you might wonder how you're going to take this m by B dimension and add B,",
    "start": "4680845",
    "end": "4687939"
  },
  {
    "text": "uh, and add a m dimension vector to it. And as long as your- as long as",
    "start": "4687939",
    "end": "4693810"
  },
  {
    "text": "your dimensions match up in this way where you have m on the left and B and 1 on the right.",
    "start": "4693810",
    "end": "4700520"
  },
  {
    "text": "Most of the computer software, including NumPy, will perform something called broadcasting,",
    "start": "4700520",
    "end": "4705955"
  },
  {
    "text": "which means it's going to make- uh which is- it is going to broadcast this m by 1 vector into",
    "start": "4705955",
    "end": "4714205"
  },
  {
    "text": "an m by B vector by making multiple virtual copies of itself uh, as multiple columns. Next question?",
    "start": "4714205",
    "end": "4724160"
  },
  {
    "text": "Is it equivalent to just multiplying B by a row vector of 1s?",
    "start": "4724950",
    "end": "4732040"
  },
  {
    "text": "So the question is- is this- would it be equivalent to multiplying B by- by what?",
    "start": "4732040",
    "end": "4739225"
  },
  {
    "text": "A row vector of 1s. A row vector of 1s. So B by 1 times 1 by-",
    "start": "4739225",
    "end": "4747140"
  },
  {
    "text": "1 by B. Um- All filled with 1s. Filled with 1s? Uh, yes, exactly. So you can- you can do it explicitly and make",
    "start": "4747140",
    "end": "4755020"
  },
  {
    "text": "multiple copies of your B and that will consume more memory in your computer.",
    "start": "4755020",
    "end": "4760465"
  },
  {
    "text": "Whereas, you know, broadcasting is this- this, you know, uh, you- you probably going to come across, you know,",
    "start": "4760465",
    "end": "4767080"
  },
  {
    "text": "broadcasting errors when you're doing homework, you know, when, you know, things are not, you know. So, uh, broadcasting is this feature in- in most, uh,",
    "start": "4767080",
    "end": "4775659"
  },
  {
    "text": "computer software which- which, uh, work on, um, matrices where it- it makes multiple virtual copies of",
    "start": "4775660",
    "end": "4783370"
  },
  {
    "text": "this vector to match up the dimensions and that's something you just need to be, uh, aware of.",
    "start": "4783370",
    "end": "4788545"
  },
  {
    "text": "What is equivalent to that? It is mathematically- it is equivalent to multiplying,",
    "start": "4788545",
    "end": "4793764"
  },
  {
    "text": "uh- uh, a row vector of 1s and making multiple copies, yes? Um, mathematically it's equivalent,",
    "start": "4793764",
    "end": "4800545"
  },
  {
    "text": "but actually performing this is very algorithmically expensive. You're going to use a lot of memory,",
    "start": "4800545",
    "end": "4806620"
  },
  {
    "text": "and use a lot of computation. Um, broadcasting is a feature that, you know, these softwares implement to do it virtually for you. Next question?",
    "start": "4806620",
    "end": "4817035"
  },
  {
    "text": "Um, in the methods we use stochasticity, uh, stochastic gradient descent and we say that we could do that because [inaudible]",
    "start": "4817035",
    "end": "4827550"
  },
  {
    "text": "Yeah. How do we justify the minibatch [inaudible] Yeah, so a good question.",
    "start": "4827550",
    "end": "4832810"
  },
  {
    "text": "So in the other, um, other, um, algorithms, we use stochastic gradient descent and, um,",
    "start": "4832810",
    "end": "4840099"
  },
  {
    "text": "and because the problems were convex, we kind of, you know, knew that it would go to the- the- the,",
    "start": "4840100",
    "end": "4845215"
  },
  {
    "text": "um, global minimum or nearby the global minimum. But neural networks are not convex.",
    "start": "4845215",
    "end": "4851185"
  },
  {
    "text": "Um, and what's- and- and- and, uh, the stochastic gradient descent makes sense in neural networks I guess is",
    "start": "4851185",
    "end": "4857560"
  },
  {
    "text": "a very good valid question, right, and the answer there is, um, it is valid.",
    "start": "4857560",
    "end": "4864085"
  },
  {
    "text": "The- the- if- if you look at the theory of using stochastic gradient methods,",
    "start": "4864085",
    "end": "4871944"
  },
  {
    "text": "um, so there are- there are certain conditions. So again, you know,",
    "start": "4871944",
    "end": "4877585"
  },
  {
    "text": "this is for your reference. So the- the- the- if you want to go deeper into this, lookup stochastic approximation- approximation;",
    "start": "4877585",
    "end": "4894520"
  },
  {
    "text": "and, uh, one of the, uh, very first papers that introduced this idea was, uh, from, uh, a pair of authors.",
    "start": "4894520",
    "end": "4900760"
  },
  {
    "text": "I think it was uh Robbins and Monro, Robbins and Monro.",
    "start": "4900760",
    "end": "4909219"
  },
  {
    "text": "So, um, when they, uh, introduced this whole idea of stochastic,",
    "start": "4909220",
    "end": "4914380"
  },
  {
    "text": "uh- uh- um, stochastic optimization or stochastic gradient descent, uh, it was introduced in the context of",
    "start": "4914380",
    "end": "4920965"
  },
  {
    "text": "stochastic approximations and what the theory says is, if you perform this gradient descent with your learning rate,",
    "start": "4920965",
    "end": "4930055"
  },
  {
    "text": "you know, we used in- in all our methods, we use a- a constant learning rate Alpha.",
    "start": "4930055",
    "end": "4935815"
  },
  {
    "text": "But if, uh, in this paper they give a proof that as long as your learning rate is also decreasing over time,",
    "start": "4935815",
    "end": "4943765"
  },
  {
    "text": "and if it is decreasing at a rate such that, um, so supposing if Alpha 1 is the learning rate",
    "start": "4943765",
    "end": "4950665"
  },
  {
    "text": "used at time step one or- or the step size one. As long as you, um,",
    "start": "4950665",
    "end": "4957340"
  },
  {
    "text": "as long as you decay your learning rate over time, such that sum over time,",
    "start": "4957340",
    "end": "4962949"
  },
  {
    "text": "Alpha t is, uh- um, infinity, but sum over t,",
    "start": "4962950",
    "end": "4971755"
  },
  {
    "text": "Alpha t square is less than infinity, which means the sum of the learning rate values explodes to infinity,",
    "start": "4971755",
    "end": "4978190"
  },
  {
    "text": "but the sum of the squares of the learning rates, you know, converges. As long as your learning rate follows this condition,",
    "start": "4978190",
    "end": "4984679"
  },
  {
    "text": "then stochastic gradient descent will converge to some local optima and",
    "start": "4984680",
    "end": "4992610"
  },
  {
    "text": "that local optima in case of neural networks could be any local optima, right? Uh, gradient descent will- will also take you to some local optima in a neural network.",
    "start": "4992610",
    "end": "5001260"
  },
  {
    "text": "But we don't know what it is anyways and, you know, then, you know, why the extra concern when you start using stochastic gradient descent is,",
    "start": "5001260",
    "end": "5009795"
  },
  {
    "text": "you know, is- is- is a response. In terms of when exactly stochastic gradient descent will converge to a local minima,",
    "start": "5009795",
    "end": "5019005"
  },
  {
    "text": "as long as your learning rate follows the- these set of conditions, stochastic gradient descent will converge to a local minima,",
    "start": "5019005",
    "end": "5025665"
  },
  {
    "text": "even if the problem is non-convex, it'll take you to a local minimum, right? Uh, in practice, we don't, uh, uh,",
    "start": "5025665",
    "end": "5031485"
  },
  {
    "text": "follow learning rates in this, uh, uh, that meet this condition. Uh, but, uh, it is actually common to anneal your learning rate in some way.",
    "start": "5031485",
    "end": "5042660"
  },
  {
    "text": "May not be in this way, but it's- it's common to decrease a learning rate in neural networks, uh, um, it- it's a commonly done practice though not in- in that, uh, fashion.",
    "start": "5042660",
    "end": "5051585"
  },
  {
    "text": "So SGD will take you to a local optima, um, but, uh, you know,",
    "start": "5051585",
    "end": "5057150"
  },
  {
    "text": "you have no guarantees- just the way you have no guarantees with gradient descent itself. [inaudible]",
    "start": "5057150",
    "end": "5070350"
  },
  {
    "text": "Why don't we do stochastic instead of minibatch? So the question is, um, you know, the- the- these models are very expensive to compute,",
    "start": "5070350",
    "end": "5077805"
  },
  {
    "text": "uh, you know, calculating all this backpropagation. So why do we do minibatch instead of just stochastic? Um, you- so the- the, uh,",
    "start": "5077805",
    "end": "5084525"
  },
  {
    "text": "the reason why you want to do minibatches, because most of these software's are implemented on GPUs where you get parallelism for free, kind of.",
    "start": "5084525",
    "end": "5091905"
  },
  {
    "text": "So if you're not doing, you know, at least some amount of parallelism, you're kind of leaving money on the table.",
    "start": "5091905",
    "end": "5096989"
  },
  {
    "text": "So you want to use, you know, uh, make use of the parallelism somehow. And- and also when you- when you move from",
    "start": "5096990",
    "end": "5102630"
  },
  {
    "text": "stochastic gradient descent to mini-batch gradient descent, the noise in your gradient, uh, direction comes down significantly.",
    "start": "5102630",
    "end": "5108930"
  },
  {
    "text": "So if you're averaging across multiple examples, you know, the noisiness in it- in- in- in each step that you take,",
    "start": "5108930",
    "end": "5114990"
  },
  {
    "text": "uh, is, uh, reduces significantly. Next question? So that's, uh, uh, Alpha t, uh, let's say you change your learning rate two times.",
    "start": "5114990",
    "end": "5126239"
  },
  {
    "text": "Yep. Uh, so why- why should restrictions in the values of, uh, uh,",
    "start": "5126240",
    "end": "5134190"
  },
  {
    "text": "learning rate Alpha- why should that have anything to do with, uh, whether or not we can work the question in that depends on the nature of our loss function-",
    "start": "5134190",
    "end": "5145590"
  },
  {
    "text": "So I'm not gonna go into the details of this. I'm happy to go into, you know, more details of this after the lecture.",
    "start": "5145590",
    "end": "5150810"
  },
  {
    "text": "If you're interested in, I would highly recommend, you know, reading up this paper, it gives you all the details of, you know, what are the necessary conditions.",
    "start": "5150810",
    "end": "5158159"
  },
  {
    "text": "Um, probably there's also a sufficient condition for your training process to converge. [OVERLAPPING] [inaudible].",
    "start": "5158160",
    "end": "5166380"
  },
  {
    "text": "Yeah. Just those alphas will make sure you find- Exactly. Yes- Yes. So the papers are-",
    "start": "5166380",
    "end": "5172755"
  },
  {
    "text": "the-the idea here is any set of learning rates you take, no matter what your loss function is,",
    "start": "5172755",
    "end": "5178245"
  },
  {
    "text": "if they follow these- these conditions, then you will converge to a local optimum. Exactly. [inaudible]",
    "start": "5178245",
    "end": "5187890"
  },
  {
    "text": "Well, not all the alphas, some of the initial alphas could be very big, but, ah, you know, but the sum of the square of the alpha",
    "start": "5187890",
    "end": "5193380"
  },
  {
    "text": "should converge to some finite value. All right, ah, a few more comments about,",
    "start": "5193380",
    "end": "5202610"
  },
  {
    "text": "ah, neural networks before we move on. Um, so one view of neural network is to start with this picture, and,",
    "start": "5202610",
    "end": "5215175"
  },
  {
    "text": "you know, remember we- we first consider the last layer as a linear model and everything else has something else.",
    "start": "5215175",
    "end": "5225130"
  },
  {
    "text": "This view basically tells you that what a neural network is doing is transforming your",
    "start": "5225130",
    "end": "5231239"
  },
  {
    "text": "x's into some phi of x. Alright?",
    "start": "5231240",
    "end": "5237240"
  },
  {
    "text": "So you can think of this whole network, this whole thing to be some phi of",
    "start": "5237240",
    "end": "5246975"
  },
  {
    "text": "x where the phi are parameterized by w and b. I think of it as a feature map, right?",
    "start": "5246975",
    "end": "5252225"
  },
  {
    "text": "We- we- we came up with some hard-coded features in problem set one in the last question like,",
    "start": "5252225",
    "end": "5257910"
  },
  {
    "text": "you know, polynomials and, you know, sinusoidal features, et cetera. But essentially what this neural network is doing is",
    "start": "5257910",
    "end": "5264929"
  },
  {
    "text": "taking your input and transforming it to some other space, and then you apply your GLM or any linear model that we were- that we've seen before.",
    "start": "5264930",
    "end": "5274275"
  },
  {
    "text": "So neural networks, in a way, are way to learn representations.",
    "start": "5274275",
    "end": "5280230"
  },
  {
    "text": "So everything that happened from x until a of l minus 1 was learning",
    "start": "5280230",
    "end": "5285450"
  },
  {
    "text": "a way to represent x so that the GLM could consume that representation, right?",
    "start": "5285450",
    "end": "5291060"
  },
  {
    "text": "So that is one view of neural networks where your neural network is",
    "start": "5291060",
    "end": "5297000"
  },
  {
    "text": "some learn-able feature map [NOISE] plus linear model.",
    "start": "5297000",
    "end": "5307900"
  },
  {
    "text": "Okay. In- in case of linear models, we would only learn the parameters over here,",
    "start": "5310280",
    "end": "5317220"
  },
  {
    "text": "but in the case of neural networks, we are jointly learning the parameters of the linear model and",
    "start": "5317220",
    "end": "5322500"
  },
  {
    "text": "the parameters of the feature map. Yes, question? When you say that do you mean everything simultaneous?",
    "start": "5322500",
    "end": "5329250"
  },
  {
    "text": "Everything simultaneously exactly when we do gradient- gradient ascent or gradient descent, we're updating the parameters of",
    "start": "5329250",
    "end": "5335100"
  },
  {
    "text": "the last layer and the initial layers all at the same time. So in a way you're- you're trying to learn",
    "start": "5335100",
    "end": "5341820"
  },
  {
    "text": "what the right parameters of the linear model needs to be, and also at the same time,",
    "start": "5341820",
    "end": "5347640"
  },
  {
    "text": "learning what are good representations that are gonna help my linear model.",
    "start": "5347640",
    "end": "5352755"
  },
  {
    "text": "Right? So linear- In fact, one of the leading, ah, conferences of deep learning is called ICLR.",
    "start": "5352755",
    "end": "5361425"
  },
  {
    "text": "No, it's-it's, ah, ah, um a- academic conference that's kind of dedicated to deep learning,",
    "start": "5361425",
    "end": "5366690"
  },
  {
    "text": "and the name of ICLR stands for International Conference of Learning Representations. Okay, so that's what deep learning is doing.",
    "start": "5366690",
    "end": "5372390"
  },
  {
    "text": "It is learning representations so that you can apply linear models that you are already familiar with.",
    "start": "5372390",
    "end": "5380530"
  },
  {
    "text": "Which- which- which gives some interesting, um, um, additional perspectives.",
    "start": "5380780",
    "end": "5387975"
  },
  {
    "text": "That is, once you have a feature map, I guess what I'm- what I'm about to say, you know,",
    "start": "5387975",
    "end": "5393824"
  },
  {
    "text": "beyond this point about neural networks are- um, um, are not some things we are gonna ask you in your exam, but are, you know,",
    "start": "5393825",
    "end": "5400800"
  },
  {
    "text": "good to know, especially if you wanna get into machine learning research. Um, associated with every neural network is a kernel.",
    "start": "5400800",
    "end": "5408510"
  },
  {
    "text": "A kernel with given two examples, ixi,",
    "start": "5408510",
    "end": "5413519"
  },
  {
    "text": "xj is the phi of xi transpose phi of xj.",
    "start": "5413520",
    "end": "5421905"
  },
  {
    "text": "Where the phi are- is the thing that takes you from x2 to here, right?",
    "start": "5421905",
    "end": "5427050"
  },
  {
    "text": "So every neural network kind of comes with an implicit kernel. Where the kernel is represented by this explicit feature map.",
    "start": "5427050",
    "end": "5434505"
  },
  {
    "text": "Okay. Which means now this gives you a way in which you can use neural networks with Gaussian processes, right?",
    "start": "5434505",
    "end": "5441780"
  },
  {
    "text": "In Gaussian processes, you needed a kernel to- to tell you how similar or dissimilar two examples were, right?",
    "start": "5441780",
    "end": "5447750"
  },
  {
    "text": "And over there you can use neural networks. You can- you can- there are ways in which you can learn kernels, which means you can learn these neural networks for the- for the GP process.",
    "start": "5447750",
    "end": "5458415"
  },
  {
    "text": "And-and, you know, that's- that's a very elegant way of combining Gaussian processes and neural networks, so a neural network becomes the kernel of your GP.",
    "start": "5458415",
    "end": "5467505"
  },
  {
    "text": "There are also a few other interesting- um, few other interesting results,",
    "start": "5467505",
    "end": "5475050"
  },
  {
    "text": "ah, with neural networks. So there is another, um, um,",
    "start": "5475050",
    "end": "5482250"
  },
  {
    "text": "interesting, I guess, um, um, linked to Gaussian processes where there's",
    "start": "5482250",
    "end": "5490469"
  },
  {
    "text": "another paper which- which shows that a single layer of neural network,",
    "start": "5490470",
    "end": "5497400"
  },
  {
    "text": "which means one hidden layer and one output layer. Where the input are your x's and you have one hidden layer and one output layer.",
    "start": "5497400",
    "end": "5507735"
  },
  {
    "text": "As the number of neurons in the hidden layer tend to infinity,",
    "start": "5507735",
    "end": "5514500"
  },
  {
    "text": "then the neural network becomes a Gaussian process itself, which is another interesting link between Gaussian process and neural networks.",
    "start": "5514500",
    "end": "5522930"
  },
  {
    "text": "[NOISE] If you're interested in those papers, you know, ah, make a post on the address happy to,",
    "start": "5522930",
    "end": "5530355"
  },
  {
    "text": "ah, share links to those papers. However, ah, the- the other, um, interesting, um,",
    "start": "5530355",
    "end": "5538845"
  },
  {
    "text": "interesting idea of neural networks is this thing called the Universal Approximation Theorem.",
    "start": "5538845",
    "end": "5546370"
  },
  {
    "text": "Again, the- No, we're not gonna ask you about this in your exams or homework or anything.",
    "start": "5552560",
    "end": "5558810"
  },
  {
    "text": "But these are good to know, you know, especially if you want to get into machine learning research. So the Universal Approximation Theorem says,",
    "start": "5558810",
    "end": "5566560"
  },
  {
    "text": "if you have a function, say, ah, y equals f of x,",
    "start": "5566720",
    "end": "5573885"
  },
  {
    "text": "where x, you know, belongs to rd and y is in, you know,",
    "start": "5573885",
    "end": "5580679"
  },
  {
    "text": "some r, let's call it k. Right? And you're interested in some bounded region of x.",
    "start": "5580680",
    "end": "5593349"
  },
  {
    "text": "Which means, say, if your x's are, for example, some- some image right there,",
    "start": "5594320",
    "end": "5600240"
  },
  {
    "text": "then you are only interested in the region of x where the pixel values between some range, right?",
    "start": "5600240",
    "end": "5605415"
  },
  {
    "text": "That's- that's the kind of limits our region of interest of the inputs. So given a bounded region of, ah, interest,",
    "start": "5605415",
    "end": "5611460"
  },
  {
    "text": "then there exists- exists",
    "start": "5611460",
    "end": "5619350"
  },
  {
    "text": "a neural network of one hidden layer,",
    "start": "5619350",
    "end": "5624490"
  },
  {
    "text": "which means the input layer is x in rd.",
    "start": "5628370",
    "end": "5636510"
  },
  {
    "text": "Your final is y in rk,",
    "start": "5636510",
    "end": "5643030"
  },
  {
    "text": "and you have a hidden layer- a fully connected hidden layer. [NOISE]",
    "start": "5644870",
    "end": "5657855"
  },
  {
    "text": "And the Universal Approximation Theorem says if you have a function with a given region of interest of your inputs,",
    "start": "5657855",
    "end": "5667889"
  },
  {
    "text": "then for any smooth- uh, any continuous f, any,",
    "start": "5667890",
    "end": "5673080"
  },
  {
    "text": "uh, uh, function f that is continuous, there exists some neural network with a finite number of",
    "start": "5673080",
    "end": "5678840"
  },
  {
    "text": "hidden layers that can approximate f to an arbitrary degree of precision.",
    "start": "5678840",
    "end": "5685750"
  },
  {
    "text": "Right. And- and that could be surprising, uh, initially, so any f that is continuous if you define a region of interest in the input space of x.",
    "start": "5686210",
    "end": "5698340"
  },
  {
    "text": "No matter what, uh, uh- no- no matter what f you have, there exists a neural network with a single hidden layer,",
    "start": "5698340",
    "end": "5707670"
  },
  {
    "text": "with a finite number of hidden dimensions. But that number of dimensions can be exponentially large,",
    "start": "5707670",
    "end": "5715080"
  },
  {
    "text": "but it's still finite. And that neural network can approximate y to an arbitrary degree of precision.",
    "start": "5715080",
    "end": "5724515"
  },
  {
    "text": "So if you come up with a degree of tolerance Epsilon, say, call it, you know, 10^minus 6.",
    "start": "5724515",
    "end": "5730590"
  },
  {
    "text": "Then you can construct a neural network f hat of x,",
    "start": "5730590",
    "end": "5735840"
  },
  {
    "text": "such that f hat of x minus f of x is less than Epsilon for all values of x in the bounded region. Yes, question?",
    "start": "5735840",
    "end": "5746295"
  },
  {
    "text": "How is that value [inaudible] is doing best for everything,",
    "start": "5746295",
    "end": "5752910"
  },
  {
    "text": "so that's just exactly a violation of no free lunch? So I guess I wouldn't call it a",
    "start": "5752910",
    "end": "5758730"
  },
  {
    "text": "violation of no free lunch because a Gaussian process could also do this. You know, it can approximate anything if you have a non-parametric thing.",
    "start": "5758730",
    "end": "5764445"
  },
  {
    "text": "But here, uh, the results is, in a- in a- in a, uh, parametric setting, uh, this can- this can,",
    "start": "5764445",
    "end": "5771180"
  },
  {
    "text": "um, this can basically mimic f to an arbitrary degree of precision, right?",
    "start": "5771180",
    "end": "5778110"
  },
  {
    "text": "You- you throw in more, uh, input layers, then your degree of precision improves.",
    "start": "5778110",
    "end": "5783840"
  },
  {
    "text": "But, uh- yes, question? Sorry I didn't say the- yeah does [inaudible]?",
    "start": "5783840",
    "end": "5792930"
  },
  {
    "text": "No, no, it's just this, you know, uh, consider the- the output, uh, over here non- nonlinear.",
    "start": "5792930",
    "end": "5798045"
  },
  {
    "text": "So that's already the prediction? Yeah, if you think of that. You know, you- you can- you can- you can construct a network like this where, um,",
    "start": "5798045",
    "end": "5806429"
  },
  {
    "text": "um- so if you- if you think of your y as- as a scalar value, then I guess, you know, you would just have one cell over here.",
    "start": "5806430",
    "end": "5813344"
  },
  {
    "text": "Why- why bother with more than one given layer and all that? So- so, yeah, I am coming to that.",
    "start": "5813345",
    "end": "5818790"
  },
  {
    "text": "So, right- so it tells you- the universal approximation tells you that any function- continuous function f can be",
    "start": "5818790",
    "end": "5824969"
  },
  {
    "text": "approximated to any arbitrary degree of precision with a one-layer neural network, as long as you're willing to,",
    "start": "5824970",
    "end": "5830264"
  },
  {
    "text": "you know, have lots of hidden- hidden units. However, um, so- so tha- that's basically,",
    "start": "5830265",
    "end": "5838200"
  },
  {
    "text": "you know, a theoretically or mathematically, you know, true valid result. And- and, you know, that's probably, you know,",
    "start": "5838200",
    "end": "5844335"
  },
  {
    "text": "a big reason why there's a lot of interest in neural networks because it has, you know, such good expressivity power.",
    "start": "5844335",
    "end": "5850275"
  },
  {
    "text": "However, the catch that comes with this is it does not tell you that- it does not",
    "start": "5850275",
    "end": "5855449"
  },
  {
    "text": "tell you or give you a recipe of how to recover that function or how to recover that configuration.",
    "start": "5855450",
    "end": "5860820"
  },
  {
    "text": "And it doesn't tell you that with gradient descent you can- you can reach that configuration of the network.",
    "start": "5860820",
    "end": "5867105"
  },
  {
    "text": "Okay. And it also, um, doesn't tell you how many samples of x and y pairs you need to mimic it, right?",
    "start": "5867105",
    "end": "5875205"
  },
  {
    "text": "There are lots of- lots of, um, um, um, um, catches that come with it. But it- it's still, er,",
    "start": "5875205",
    "end": "5881309"
  },
  {
    "text": "a valid mathematical result to that by adjusting your parameters in some way, it doesn't tell you how, but a configuration exists where it mimics",
    "start": "5881310",
    "end": "5888945"
  },
  {
    "text": "any continuous function f to an arbitrary degree of precision. Yes, question? But does it need to be continuous with any further requirements on the matter?",
    "start": "5888945",
    "end": "5899850"
  },
  {
    "text": "Uh, I don't remember the details, but, you know, it's- it's- uh, it's not a very, uh,",
    "start": "5899850",
    "end": "5905850"
  },
  {
    "text": "I don't think it needs to be, uh, Lipschitz. It probably needs to be. I- I- I don't remember, but, you know,",
    "start": "5905850",
    "end": "5912630"
  },
  {
    "text": "it's- it's- it's- um, for most practical purposes, all the kind of functions that you may want to learn are- are- are covered under that. Yes, question?",
    "start": "5912630",
    "end": "5922480"
  },
  {
    "text": "Uh, the question is like [inaudible]?",
    "start": "5927530",
    "end": "5936840"
  },
  {
    "text": "Yeah, I- I guess, you know, this is just a mathematical result, uh, which- which tells you that there's a way in which you can adjust the weights and",
    "start": "5936840",
    "end": "5942960"
  },
  {
    "text": "biases of your network to mimic f of x to an arbitrary degree of precision. Um, you know, that's just a mathematical result,",
    "start": "5942960",
    "end": "5949530"
  },
  {
    "text": "and you probably wanna view it outside the context of, you know, your data,",
    "start": "5949530",
    "end": "5954780"
  },
  {
    "text": "and test set, and whatnot. Uh, but, you know, um, that's still- that's still a valid result,",
    "start": "5954780",
    "end": "5960840"
  },
  {
    "text": "and that's basically, you know, uh, kind of spurred a lot of interest in neural networks because,",
    "start": "5960840",
    "end": "5965910"
  },
  {
    "text": "hey, they are, you know, universal function approximators. And as long as I have a good enough dataset that,",
    "start": "5965910",
    "end": "5971850"
  },
  {
    "text": "you know, that- that- that's kind of representative of, you know, your, uh your function and maybe gradient descent will get us there,",
    "start": "5971850",
    "end": "5978855"
  },
  {
    "text": "you know, but there's no proof that grad- gradient descent will get you there, but, you know, maybe it does.",
    "start": "5978855",
    "end": "5983080"
  },
  {
    "text": "Can we extract [inaudible]? There's no algorithm of how to extract such f hat.",
    "start": "5987470",
    "end": "5994770"
  },
  {
    "text": "All it tells you is that there exists an f hat. So within those new predictions [inaudible]?",
    "start": "5994770",
    "end": "5999900"
  },
  {
    "text": "Yeah- again, I would- I would- I would not think of it in the context of- of- of prediction versus learning versus inference.",
    "start": "5999900",
    "end": "6006965"
  },
  {
    "text": "Um, it- all it's telling you is there exists some configuration, doesn't tell you how to reach that configuration.",
    "start": "6006965",
    "end": "6012695"
  },
  {
    "text": "Right. So- so now, you know, uh, neural networks are therefore in theory, you know,",
    "start": "6012695",
    "end": "6017915"
  },
  {
    "text": "the- the, uh, hypothesis class of neural networks. If you're able to af- you know, um,",
    "start": "6017915",
    "end": "6023765"
  },
  {
    "text": "afford a pretty large intermediate, uh, uh, hidden layers, the hypothesis class is pretty big.",
    "start": "6023765",
    "end": "6029480"
  },
  {
    "text": "You can approximate any function, uh, you can- you can- you know, or continuous function you can think of,",
    "start": "6029480",
    "end": "6035390"
  },
  {
    "text": "you know, in- in some, uh, bounded region of input. Um, and- and which- which kind of, um, forms,",
    "start": "6035390",
    "end": "6044030"
  },
  {
    "text": "and I segue, to our next set of topics of, you know, bias-variance tradeoff.",
    "start": "6044030",
    "end": "6049050"
  },
  {
    "text": "I don't think we have time to finish it today. We're going to pick it up again on- on- um,",
    "start": "6052840",
    "end": "6058295"
  },
  {
    "text": "on Friday, but maybe just, you know, um, lay out the setting so that we can- we can pick it up from,",
    "start": "6058295",
    "end": "6064535"
  },
  {
    "text": "uh, Friday. Yes, question?",
    "start": "6064535",
    "end": "6066840"
  },
  {
    "text": "So before you move on, can you introduce neural networks to introduce [inaudible]?",
    "start": "6071650",
    "end": "6081905"
  },
  {
    "text": "Good question. So, um- so the question is, use neural networks to come up with nonlinear hypotheses and a decade ago or so,",
    "start": "6081905",
    "end": "6090995"
  },
  {
    "text": "uh, SVMs, you know, with kernels, could also do nonlinear, um, um, um, decision boundaries.",
    "start": "6090995",
    "end": "6096635"
  },
  {
    "text": "And why is the recent, you know, trend or shift in interest towards neural networks and away from SVMs.",
    "start": "6096635",
    "end": "6103550"
  },
  {
    "text": "I guess, um, um, few- few, uh, uh, possible reasons, uh, likely reasons.",
    "start": "6103550",
    "end": "6110390"
  },
  {
    "text": "One is SVMs require you to hand craft a kernel. It's, you know- you need to use your intuition to come up with, um,",
    "start": "6110390",
    "end": "6117710"
  },
  {
    "text": "you know, a good mathematical kernel for it, um- for it to work well. The second is, um, uh,",
    "start": "6117710",
    "end": "6123985"
  },
  {
    "text": "whereas in neural networks, the feature map is kind of learned from data, right? You- you don't have to construct a mathematically valid kernel.",
    "start": "6123985",
    "end": "6132020"
  },
  {
    "text": "You know, the data and the gradient descent will kinda figure out a- a- a reasonable implicit kernel in this case.",
    "start": "6132020",
    "end": "6137645"
  },
  {
    "text": "The other reason is because, uh, neural networks are very computational intensive, right?",
    "start": "6137645",
    "end": "6146420"
  },
  {
    "text": "They are extremely computationally intensive, especially as you add more, uh, deeper layers. And they're also very sample inefficient.",
    "start": "6146420",
    "end": "6155765"
  },
  {
    "text": "Sa- by sample inefficient, let's see, what I mean is you need a lot of data for the neural network to work well, right?",
    "start": "6155765",
    "end": "6163040"
  },
  {
    "text": "Whereas other algorithms are lot more sample efficient, they kind of work at their maximum capacity with a- a lot fewer,",
    "start": "6163040",
    "end": "6169040"
  },
  {
    "text": "um, um, uh, datasets. And recently, the trend in computing is that a lot of data is now digitized,",
    "start": "6169040",
    "end": "6175114"
  },
  {
    "text": "like pictures or, you know, images or, you know, you get a lot of, uh, images, uh, which are digitized, you know.",
    "start": "6175115",
    "end": "6180815"
  },
  {
    "text": "And also there's a huge jump in computation with GPUs and stuff, right? So the two obstacles that were kind of holding back neural networks,",
    "start": "6180815",
    "end": "6188450"
  },
  {
    "text": "you know, lots of computation and lots of data are kind of now met. Um, another, um, uh- in terms of algorithmically,",
    "start": "6188450",
    "end": "6197270"
  },
  {
    "text": "there hasn't been a lot of- or there hasn't been a whole lot of fundamental changes to neural network algorithmically, um,",
    "start": "6197270",
    "end": "6204980"
  },
  {
    "text": "except probably the- the introduction of the activation function called the ReLU,",
    "start": "6204980",
    "end": "6211410"
  },
  {
    "text": "which- which we saw the other day, no it's equal to, um, max of 0, x.",
    "start": "6214210",
    "end": "6222140"
  },
  {
    "text": "Right? Uh, so ReLU addressed one, uh, I would say, uh, problem with neural networks by, uh, uh, uh,",
    "start": "6222140",
    "end": "6230469"
  },
  {
    "text": "uh- of making it very deep, which was, uh, something that's called a vanishing gradient,",
    "start": "6230470",
    "end": "6236050"
  },
  {
    "text": "um, we probably won't be covering vanishing gradients. But the idea there is if you have an activation function that looks like this,",
    "start": "6236050",
    "end": "6243190"
  },
  {
    "text": "and if your activation function, the input to your activation function is some, you know, z value here,",
    "start": "6243190",
    "end": "6248869"
  },
  {
    "text": "then the a value will be, whatever, 1, but the- the derivative of- over there is pretty much 0.",
    "start": "6248870",
    "end": "6255245"
  },
  {
    "text": "So the Jacobian will just be, you know, it has zeros everywhere else, but, you know, these elements will also be- end up being 0.",
    "start": "6255245",
    "end": "6262415"
  },
  {
    "text": "So you take the- the- the gradient of- of- of the next layer multiplied by as, uh,",
    "start": "6262415",
    "end": "6268220"
  },
  {
    "text": "a Jacobian of 0 then, you know, uh, everything behind are- are pretty much gonna get 0, um, um, gradients.",
    "start": "6268220",
    "end": "6274460"
  },
  {
    "text": "So this- this, um, um- this problem was kind of solved with ReLUs.",
    "start": "6274460",
    "end": "6280430"
  },
  {
    "text": "And it's- it's kind of a, you know, if you look at it, it's kind of hacky, right? It's just doesn't, you know- but, you know, in practice it works- it works pretty well.",
    "start": "6280430",
    "end": "6287570"
  },
  {
    "text": "And- and- so the- the, you know, trend in- increasing, you know, computation power and increasing data and",
    "start": "6287570",
    "end": "6295460"
  },
  {
    "text": "with very few algorithmic improvements like this, you know, is- is probably why there's- there's,",
    "start": "6295460",
    "end": "6301535"
  },
  {
    "text": "uh, uh, a renewed interest in- in, uh, deep learning. So there is a kernel associated with single every neural network computation and it seems that basically depended on kernels.",
    "start": "6301535",
    "end": "6312020"
  },
  {
    "text": "So if you just design a neural network, will anyone ever need to go back to SVMs now or will people have to use [inaudible]?",
    "start": "6312020",
    "end": "6319520"
  },
  {
    "text": "So, yeah, so the question is, you know, uh, there is a kernel that we- you know, uh, that we showed that,",
    "start": "6319520",
    "end": "6325655"
  },
  {
    "text": "uh, every neural network kind of implicitly defines a kernel, you know, that's associated with its feature map and SVMs use kernels.",
    "start": "6325655",
    "end": "6332570"
  },
  {
    "text": "So what's- why is, you know, um, is there a reason to use SVMs again? And the answer is actually, uh,",
    "start": "6332570",
    "end": "6339290"
  },
  {
    "text": "very few people use SVMs, uh, except, um, there- there are still probably a few cases where you don't have a lot of data,",
    "start": "6339290",
    "end": "6347330"
  },
  {
    "text": "you know, that's kinda one regime where neural networks don't work well, if you don't have a lot of data. And other- you know, others, you know,",
    "start": "6347330",
    "end": "6353450"
  },
  {
    "text": "more- more simpler methods are still commonly used. Uh, another, uh, interesting thing, uh,",
    "start": "6353450",
    "end": "6360895"
  },
  {
    "text": "that might be interesting is you can train a neural network with- with- with, you know- in- in this way and just use it as",
    "start": "6360895",
    "end": "6369590"
  },
  {
    "text": "a feature extractor to map your- your examples to some higher level feature space.",
    "start": "6369590",
    "end": "6375619"
  },
  {
    "text": "And then you can feed that into other algorithms, you know, that's- that- that's something that people do as well.",
    "start": "6375620",
    "end": "6380930"
  },
  {
    "text": "So train a network on your full training set, right? And learn this feature map and then on other exam, you know,",
    "start": "6380930",
    "end": "6388495"
  },
  {
    "text": "um, on- on the same examples, you know, kind of chop off your network at this stage, extract the features that the network learned,",
    "start": "6388495",
    "end": "6395890"
  },
  {
    "text": "and then set- train, um, you know, like a random forest or some- some other algorithm on this representation of your inputs.",
    "start": "6395890",
    "end": "6402485"
  },
  {
    "text": "You know, that's- that's, uh- people do that sometimes as well. Is there another question? Yes.",
    "start": "6402485",
    "end": "6409610"
  },
  {
    "text": "Uh, so you were talking about the ReLU. Uh, isn't- so doesn't it have",
    "start": "6409610",
    "end": "6416600"
  },
  {
    "text": "a bigger vanishing gradient problem because everything on the left,",
    "start": "6416600",
    "end": "6421685"
  },
  {
    "text": "like everything that is less than 0- Yeah. -like at least the sigmoid has some sensitivity on both sides,",
    "start": "6421685",
    "end": "6429665"
  },
  {
    "text": "this guy has like all vanishing gradients on everything, right? Yes. So the question is, you know, in ReLU,",
    "start": "6429665",
    "end": "6434690"
  },
  {
    "text": "wouldn't you have vanishing gradient problems here? Uh, the answer is- the answer is, uh, yes.",
    "start": "6434690",
    "end": "6440735"
  },
  {
    "text": "Over here the, uh, uh, uh, gradients do vanish. Uh, however, the input that goes into",
    "start": "6440735",
    "end": "6446179"
  },
  {
    "text": "activation is actually the output of some linear transformation, right? Which means there's going to be different for different values of x.",
    "start": "6446180",
    "end": "6453125"
  },
  {
    "text": "So, um, uh, yes, uh, over here, the one- the gradient is- is- is, um- is 0.",
    "start": "6453125",
    "end": "6461180"
  },
  {
    "text": "And that is a problem of ReLU. And people have come up with kind of, again, hacky fixes for it.",
    "start": "6461180",
    "end": "6466445"
  },
  {
    "text": "They call it leaky ReLU, where leaky ReLU is, you know- it's like this until",
    "start": "6466445",
    "end": "6471980"
  },
  {
    "text": "zero and then it has a much smaller gradient over here, right? So, you know, it's like instead- instead of being exactly zero everything,",
    "start": "6471980",
    "end": "6478520"
  },
  {
    "text": "it- it has us small negative slope so there are fixes, or- or, you know, ways around it,",
    "start": "6478520",
    "end": "6483830"
  },
  {
    "text": "but in practice, ReLU seems to work. Why would it do something like that?",
    "start": "6483830",
    "end": "6488915"
  },
  {
    "text": "Like there are infinitely many nonlinearities. Uh, that seems so crazy.",
    "start": "6488915",
    "end": "6495020"
  },
  {
    "text": "I mean, it's- it's easy to compute, so it's inexpensive to compute.",
    "start": "6495020",
    "end": "6501515"
  },
  {
    "text": "All right. And going back to, uh, this question about SVMs, uh, is that a universal approximation theorem for SVMs also?",
    "start": "6501515",
    "end": "6510500"
  },
  {
    "text": "I'm not familiar with the universal approximation theorem for SVMs. But tec- so technically,",
    "start": "6510500",
    "end": "6516320"
  },
  {
    "text": "that's in literally any- theoretically any- any possible mapping you can have x to y,",
    "start": "6516320",
    "end": "6524780"
  },
  {
    "text": "a neural network can learn it? As- As long as it's-",
    "start": "6524780",
    "end": "6530270"
  },
  {
    "text": "It's continuous and you're interested in some bounded region of input space. Okay. And you said something about, uh,",
    "start": "6530270",
    "end": "6536135"
  },
  {
    "text": "it- it doesn't- that theorem doesn't get it that you can actually learn that mapping- that feature map.",
    "start": "6536135",
    "end": "6542060"
  },
  {
    "text": "Uh, but I mean, like you can- uh, yeah, you want learn those w's and b's which have the correct region map.",
    "start": "6542060",
    "end": "6549170"
  },
  {
    "text": "Uh, why- can you elaborate more about, what- what does that mean? Why- why can't you do that?",
    "start": "6549170",
    "end": "6554315"
  },
  {
    "text": "So all- all- all I said was the approximation theorem doesn't offer you a recipe of how to recover that network.",
    "start": "6554315",
    "end": "6561320"
  },
  {
    "text": "It just tells you- it's a- it's an existence theorem. It says, you know, that- that such a network exists.",
    "start": "6561320",
    "end": "6567095"
  },
  {
    "text": "That- that's all the theorem says. And why would it not be- oh, so you're saying the only problem,",
    "start": "6567095",
    "end": "6572780"
  },
  {
    "text": "uh, with actually achieving that, you know, some ideal optimal, uh, configuration is that we,",
    "start": "6572780",
    "end": "6581450"
  },
  {
    "text": "uh- we might get like something like ba- mini-batch gradient descent might get interactive with local minima or something?",
    "start": "6581450",
    "end": "6587090"
  },
  {
    "text": "So there are many challenges in order to even get it, right? So how many samples of x and y pairs do you need?",
    "start": "6587090",
    "end": "6592340"
  },
  {
    "text": "You know, like, right there you have like an obstacle if you want to use- train it with gradient descent. Okay? Anyways, so that's- that's,",
    "start": "6592340",
    "end": "6599704"
  },
  {
    "text": "um- that's- that's fine. So the- the main- the main idea that you want to take away",
    "start": "6599705",
    "end": "6605830"
  },
  {
    "text": "from this is that neural networks are extremely expressive. The set of all hypotheses that",
    "start": "6605830",
    "end": "6612815"
  },
  {
    "text": "a neural network can represent can be extremely large, right? Uh, linear, uh- linear regression was limited to just straight lines, right?",
    "start": "6612815",
    "end": "6621200"
  },
  {
    "text": "But neural networks are in a way, you know, much- much, much bigger. And which- which kind of, you know, um,",
    "start": "6621200",
    "end": "6629119"
  },
  {
    "text": "will lead us to the next topic of bias-variance tradeoff of, you know, is this always a good idea?",
    "start": "6629120",
    "end": "6634655"
  },
  {
    "text": "You know, it's- it's more expressivity always a desired thing? And we'll- we'll jump into, uh,",
    "start": "6634655",
    "end": "6639950"
  },
  {
    "text": "bias-variance tradeoffs on- on, um- on Friday. And to just wrap up today's lecture, uh,",
    "start": "6639950",
    "end": "6647270"
  },
  {
    "text": "I would- I would kind of stress upon the idea of backpropagation. You know, think about it in terms of this daisy chain of Jacobians,",
    "start": "6647270",
    "end": "6656255"
  },
  {
    "text": "where each of the Jacobians are pretty easy to compute. It's either a diagonal matrix or just the w itself.",
    "start": "6656255",
    "end": "6662165"
  },
  {
    "text": "The end of a neural network, we've seen it before. They're just linear models. We've seen GLMs and- and the gradient over here follows just like a linear model.",
    "start": "6662165",
    "end": "6671840"
  },
  {
    "text": "And then you have a daisy chain of Jacobians. And the branches to each of the w's and b's are,",
    "start": "6671840",
    "end": "6676925"
  },
  {
    "text": "again, very simple math. The notation may be a little confusing because you have these zero- vectors full of zeros with only some, uh, elements non-zeros,",
    "start": "6676925",
    "end": "6685100"
  },
  {
    "text": "and you kinda view them as outer products, etc, or like, that's, again, straightforward linear algebra, work through it,",
    "start": "6685100",
    "end": "6691880"
  },
  {
    "text": "you know, um, um, ones from the notes. And hopefully that should be clear. If not, you know, post on Piazza.",
    "start": "6691880",
    "end": "6697295"
  },
  {
    "text": "All right. We'll stop for today. Thanks.",
    "start": "6697295",
    "end": "6699989"
  }
]