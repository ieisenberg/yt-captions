[
  {
    "start": "0",
    "end": "88000"
  },
  {
    "start": "0",
    "end": "5900"
  },
  {
    "text": "Hi. In this module, I'm going to\ntalk about the EM algorithm for learning Bayesian\nnetworks when we have unobserved\nvariables, our training data.",
    "start": "5900",
    "end": "13296"
  },
  {
    "text": "So let's start with our\nfamiliar movie rating example. So here, remember\nthis Bayesian network.",
    "start": "13296",
    "end": "18590"
  },
  {
    "text": "We have a genre. It could be drama\nor comedy, and we have two people, Jim and\nMartha, who are going to rate--",
    "start": "18590",
    "end": "26300"
  },
  {
    "text": "produce ratings of this movie. And I denote as R1 and R2.",
    "start": "26300",
    "end": "32240"
  },
  {
    "text": "And before, when we observe all\nthe variables in our training data, we could just use maximum\nlikelihood, which amounts",
    "start": "32240",
    "end": "39470"
  },
  {
    "text": "to counting and normalizing. But this only works if we\nobserve all the variables",
    "start": "39470",
    "end": "45690"
  },
  {
    "text": "in each training example. So data collection is expensive. What happens if we don't\nobserve some of them?",
    "start": "45690",
    "end": "52670"
  },
  {
    "text": "For example, what\nhappens if we don't know the genre of the\nmovies, but we only observe the pairs of\nratings of Martha and Jim?",
    "start": "52670",
    "end": "63710"
  },
  {
    "text": "So what can we do in this case? You know, intuitively, it\nseems kind of hopeless.",
    "start": "63710",
    "end": "69220"
  },
  {
    "text": "How can we learn\na Bayesian network relating G and R when we\ndon't even see examples of G?",
    "start": "69220",
    "end": "76070"
  },
  {
    "text": "But we'll show that\nthis is actually possible in many cases, but\ncertainly not all cases,",
    "start": "76070",
    "end": "82820"
  },
  {
    "text": "and that's kind\nof the magic of EM and unsupervised\nlearning in general.",
    "start": "82820",
    "end": "89410"
  },
  {
    "start": "88000",
    "end": "196000"
  },
  {
    "text": "So let's try to approach\nthis problem top-down. What are the principles\nthat we have?",
    "start": "89410",
    "end": "94950"
  },
  {
    "text": "Well, maximum\nlikelihood was something that was served quite well. So let's try to see if\nwe can make that work.",
    "start": "94950",
    "end": "102100"
  },
  {
    "text": "So generally, we have\na set of variables, which are hidden, called\nBig H. And we also",
    "start": "102100",
    "end": "108479"
  },
  {
    "text": "have some variables big\nE, which are observed. So in this movie rating example,\nwe have G as a hidden variable.",
    "start": "108480",
    "end": "118320"
  },
  {
    "text": "The two ratings is\nan observed variable, and we have some\nlittle e denoting",
    "start": "118320",
    "end": "124020"
  },
  {
    "text": "what they're observed to. And in this case,\nwe have remember the set of parameters, which\nis the probability of G",
    "start": "124020",
    "end": "131130"
  },
  {
    "text": "and the probability of R given. So the principle of\nmaximum marginal likelihood",
    "start": "131130",
    "end": "140310"
  },
  {
    "text": "says, well, just maximize\nthe probability of the data.",
    "start": "140310",
    "end": "145540"
  },
  {
    "text": "Tweak the parameters to\nmake that probability as high as possible. So what this means\nfor us is that we're",
    "start": "145540",
    "end": "151020"
  },
  {
    "text": "going to try to find\nthe theta that maximizes the probability over all\nthe observed observations",
    "start": "151020",
    "end": "160110"
  },
  {
    "text": "that we have in\nthe training data of the probability of that\nobservation given theta.",
    "start": "160110",
    "end": "166900"
  },
  {
    "text": "So this looks very much\nlike maximum likelihood with the exception that\nwe are marginalizing out",
    "start": "166900",
    "end": "173310"
  },
  {
    "text": "the hidden variable. And just to spell this out,\nwhat this quantity really is,",
    "start": "173310",
    "end": "180799"
  },
  {
    "text": "is the summation over possible\nvalues of the hidden variables of H equals h and E equals e.",
    "start": "180800",
    "end": "191200"
  },
  {
    "text": "So this is the principle\nthat we want to adhere to.",
    "start": "191200",
    "end": "196340"
  },
  {
    "start": "196000",
    "end": "395000"
  },
  {
    "text": "So it turns out that\nthe EM algorithm is one way of trying to\noptimize this objective,",
    "start": "196340",
    "end": "205698"
  },
  {
    "text": "but we're going\nto try to motivate EM in a more intuitive way. So EM, you should think\nabout it as a generalization",
    "start": "205698",
    "end": "213760"
  },
  {
    "text": "of the K-means algorithm. Remember in K-means\nfor clustering, we also had a similar problem\nwhere we have cluster centroids",
    "start": "213760",
    "end": "222910"
  },
  {
    "text": "and cluster assignments,\nboth of which we didn't know. In our case, the\ncluster centroids",
    "start": "222910",
    "end": "229720"
  },
  {
    "text": "are going to be generalized\nto parameters of a Bayesian network in general, and\nthe cluster assignments",
    "start": "229720",
    "end": "235000"
  },
  {
    "text": "are going to be generalized\nto the hidden variables.",
    "start": "235000",
    "end": "240300"
  },
  {
    "text": "So here are the variables. We have E, H. And here is\nthe expectation maximization",
    "start": "240300",
    "end": "248940"
  },
  {
    "text": "algorithm or\notherwise known as EM. So we're first going to\ninitialize the parameters",
    "start": "248940",
    "end": "254880"
  },
  {
    "text": "randomly, and they're going\nto repeat until convergence.",
    "start": "254880",
    "end": "259970"
  },
  {
    "text": "It's going to alternate\nbetween two steps-- the E-step and the M-step.",
    "start": "259970",
    "end": "265009"
  },
  {
    "text": "In the E-step, what\nwe're going to first do is try to use the parameters\nto guess the hidden variables.",
    "start": "265010",
    "end": "272789"
  },
  {
    "text": "So we're going to\ncompute q of h. There's going to\nbe a distribution",
    "start": "272790",
    "end": "278090"
  },
  {
    "text": "over possible values that the\nhidden variables could take on. And this is going to be equal\nto simply the probability",
    "start": "278090",
    "end": "285949"
  },
  {
    "text": "of the hidden\nvariable conditioned on the evidence or the\nobservations that we saw.",
    "start": "285950",
    "end": "293520"
  },
  {
    "text": "And again, this is\ndepends on the parameters at the current iteration.",
    "start": "293520",
    "end": "300890"
  },
  {
    "text": "We're going to do this for\nevery possible value of h. How do we do this?",
    "start": "300890",
    "end": "306740"
  },
  {
    "text": "Well, we've already\nseen how we can compute these types of quantities\ngiven a fixed Bayesian network,",
    "start": "306740",
    "end": "312580"
  },
  {
    "text": "and this is called\nprobabilistic inference. So in case, h is small, we can\njust do it kind of brute force.",
    "start": "312580",
    "end": "319970"
  },
  {
    "text": "If h is HMM, we can use-- if the Bayesian network is HMM,\nwe can use forward-backward.",
    "start": "319970",
    "end": "326150"
  },
  {
    "text": "In general, we can use\nGibbs sampling, et cetera. So now, what do we have?",
    "start": "326150",
    "end": "332440"
  },
  {
    "text": "We have these\nweights for every h,",
    "start": "332440",
    "end": "339020"
  },
  {
    "text": "and now we can create fully\nobserved examples now. We can just pair a particular\nh with our observations",
    "start": "339020",
    "end": "347300"
  },
  {
    "text": "and put a weight\nnext to that example. And the important\nthing is how now we",
    "start": "347300",
    "end": "353620"
  },
  {
    "text": "have a set of weighted examples\nwhich are fully observed. And what do we know?",
    "start": "353620",
    "end": "359020"
  },
  {
    "text": "How do we deal with\nfully-observed examples, well, we can get a\nmaximum likelihood now.",
    "start": "359020",
    "end": "365110"
  },
  {
    "text": "So we take these\nweighted examples, and then we just\ncount and normalize. And that gives us a fresh set\nof parameters, which we then",
    "start": "365110",
    "end": "373540"
  },
  {
    "text": "can go back and repeat the\nE-step and the M-step over and over again.",
    "start": "373540",
    "end": "378880"
  },
  {
    "text": " So the EM algorithm\nis guaranteed",
    "start": "378880",
    "end": "383990"
  },
  {
    "text": "to converge to a local\noptimum just like K-means, but it can get stuck\nin local optimum",
    "start": "383990",
    "end": "389420"
  },
  {
    "text": "and not actually solve their\nglobal optimization problem.",
    "start": "389420",
    "end": "395780"
  },
  {
    "start": "395000",
    "end": "840000"
  },
  {
    "text": "So let's do an example. We're just going to\ndo one iteration of EM on our sample Bayesian network.",
    "start": "395780",
    "end": "403060"
  },
  {
    "text": "So suppose our training\ndata includes two examples--",
    "start": "403060",
    "end": "408490"
  },
  {
    "text": "R1 and R2 equals 2 and\n2, and the second example where it's 1 and 2.",
    "start": "408490",
    "end": "413770"
  },
  {
    "text": "And the genre is unobserved? OK, so suppose we have\nparameters that look like this.",
    "start": "413770",
    "end": "421970"
  },
  {
    "text": "So probability of\nG is just uniform, and probability of r given g\nis just taken by this table.",
    "start": "421970",
    "end": "433290"
  },
  {
    "text": "OK, so now we're going\nto do the E-step. So remember, the E-step is\ntrying to guess what g is given",
    "start": "433290",
    "end": "441060"
  },
  {
    "text": "each of these examples\nbecause we don't know what-- So let's look at 2, 2, OK?",
    "start": "441060",
    "end": "448849"
  },
  {
    "text": "So the first example. Well, g could be either c or d,\nbut there's two possibilities.",
    "start": "448850",
    "end": "455490"
  },
  {
    "text": "And for each one,\nI'm going to compute the probability of the\njoint assignment for now.",
    "start": "455490",
    "end": "464140"
  },
  {
    "text": "So here, I'm just going to\nby definition of the Bayesian network.",
    "start": "464140",
    "end": "469360"
  },
  {
    "text": "This is going to be\nprobability of g equals c-- that's 0.5-- times\nprobability of gr.",
    "start": "469360",
    "end": "478800"
  },
  {
    "text": "r equals 2 given cg. g equals c-- that's 0.6.",
    "start": "478800",
    "end": "487139"
  },
  {
    "text": "And then r2 equals 2 given g-- that's 0.6.",
    "start": "487140",
    "end": "492360"
  },
  {
    "text": "That gives me 0.18. And now we look at\nthe other possibility,",
    "start": "492360",
    "end": "497470"
  },
  {
    "text": "which is g equals d. And the probability\nof g equals d is 0.5.",
    "start": "497470",
    "end": "505540"
  },
  {
    "text": "And the probability\nof r1 equals 2 given g equals d, that is 0.4 here--",
    "start": "505540",
    "end": "515309"
  },
  {
    "text": "0.4 down here. And probability of\nr2 equals 2 given g d that's also another 0.4, OK?",
    "start": "515309",
    "end": "523659"
  },
  {
    "text": "So now I have\nthese probabilities next to each of these possible\nextensions of this assignment.",
    "start": "523659",
    "end": "534550"
  },
  {
    "text": "Now I can normalize, and that's\nhow I get my q distribution.",
    "start": "534550",
    "end": "541130"
  },
  {
    "text": "So if I normalize\ndistribution, I'm going to get 0.69 and point--",
    "start": "541130",
    "end": "547360"
  },
  {
    "text": "so there's more probability\nmass on g equals c.",
    "start": "547360",
    "end": "554019"
  },
  {
    "text": "And if I were to\nguess, then g equals d. So now we move on to\nthe second data point,",
    "start": "554020",
    "end": "561162"
  },
  {
    "text": "and I'm going to\ndo the same thing. So 1, 2 could either be c or--",
    "start": "561162",
    "end": "566720"
  },
  {
    "text": "and I'm going to\ncompute the probability of each possible\nassignment to g.",
    "start": "566720",
    "end": "573649"
  },
  {
    "text": "So I have a probability\nof g equals c. That's 0.5 times probability of\nr1 equals 1 given g equals c--",
    "start": "573650",
    "end": "582680"
  },
  {
    "text": "that's 0.4. And I have what is\nthe probability of r2",
    "start": "582680",
    "end": "588830"
  },
  {
    "text": "equals 2 given g equals c-- that's 0.6. And analogously, I can compute\nthe same quantity for g",
    "start": "588830",
    "end": "596930"
  },
  {
    "text": "equal to e. And again, I normalize,\nand I get 0.5 and 0.5.",
    "start": "596930",
    "end": "604570"
  },
  {
    "text": "OK, so at this point, at\nthe end of the E-step, what I have are four\nfleshed out data points.",
    "start": "604570",
    "end": "612083"
  },
  {
    "text": "I started with two\ndata points, but it's kind of been expanded into the\npossible continuations of g.",
    "start": "612083",
    "end": "618970"
  },
  {
    "text": "And each data point is\nweighted by some probability",
    "start": "618970",
    "end": "624399"
  },
  {
    "text": "q of g, which is essentially how\nmuch I think that data point is",
    "start": "624400",
    "end": "631750"
  },
  {
    "text": "valid, in some sense. OK, so now we move\non to the M-step.",
    "start": "631750",
    "end": "637140"
  },
  {
    "text": "And the M-step is just going\nto take these four data points and count them up and normalize.",
    "start": "637140",
    "end": "644190"
  },
  {
    "text": "So this should be very familiar. So first, we're going to\nestimate the probability of g.",
    "start": "644190",
    "end": "650940"
  },
  {
    "text": "So g can take on two\nvalues-- c and d. So I count them up.",
    "start": "650940",
    "end": "656820"
  },
  {
    "text": "How many times did\ng equals c occur? What shows up in the first\nand the third data points,",
    "start": "656820",
    "end": "663938"
  },
  {
    "text": "and I'm just going to add\ntheir weights together, which is 0.69 and 0.5.",
    "start": "663938",
    "end": "671399"
  },
  {
    "text": "And what about d? Well, g equals d shows up in\nthe second and the fourth rows, and that's 0.31 plus 0.5.",
    "start": "671400",
    "end": "678980"
  },
  {
    "text": "And then I'm just\ngoing to normalize this into an actual distribution.",
    "start": "678980",
    "end": "686440"
  },
  {
    "text": "So now I move on to the\nprobability of r given g. So for each possible\nconfiguration here,",
    "start": "686440",
    "end": "693420"
  },
  {
    "text": "I'm going to count. So c 1 shows up here once,\nand that has a weight of 5.",
    "start": "693420",
    "end": "704930"
  },
  {
    "text": "What about c 2? c 2 shows up three times. One here, one with\nr2, and one down here.",
    "start": "704930",
    "end": "713870"
  },
  {
    "text": "If I add the weights\nof those, I'm going to get 0.5\nplus 0.69 plus 0.69.",
    "start": "713870",
    "end": "720740"
  },
  {
    "text": "Remember notice\nthat this example is used twice because I'm\ngenerating two bytes from c.",
    "start": "720740",
    "end": "731949"
  },
  {
    "text": "OK, so now I have these counts. I normalize this distribution\nto get a distribution of r given",
    "start": "731950",
    "end": "741819"
  },
  {
    "text": "g equal c. So now I move on to what\nhappens when g is d.",
    "start": "741820",
    "end": "749089"
  },
  {
    "text": "So I look at d 1. d 1 shows up once\nhere with weight 0.5.",
    "start": "749090",
    "end": "756000"
  },
  {
    "text": "And what about d equals 2? Well, that shows up 3 times.",
    "start": "756000",
    "end": "761480"
  },
  {
    "text": "Twice here by 2, 3 once. And then once here.",
    "start": "761480",
    "end": "766980"
  },
  {
    "text": "I've another 0.5. I'm going to add and normalize,\nand I get a distribution.",
    "start": "766980",
    "end": "773380"
  },
  {
    "text": "So the only difference\nbetween maximum likelihood and the M-step is that now I'm\nadding these fractional counts",
    "start": "773380",
    "end": "780480"
  },
  {
    "text": "rather than integer accounts. But otherwise, the logic and\nthe code is exactly the same.",
    "start": "780480",
    "end": "788980"
  },
  {
    "text": "So what have we done\nstepping back a little bit? Intuitively, we've gone from a\npreliminary set of parameters.",
    "start": "788980",
    "end": "797130"
  },
  {
    "text": "And I'm guessing what g is. And then using that guess\nof g to further refine",
    "start": "797130",
    "end": "802529"
  },
  {
    "text": "my estimate of the parameters. And you'll see that the\nparameters over here",
    "start": "802530",
    "end": "808230"
  },
  {
    "text": "were 0.4 and 0.6,\nand now they've been pushed to 0.2 and 0.8.",
    "start": "808230",
    "end": "816580"
  },
  {
    "text": "So in general, EM is going tends\nto polarize the probabilities",
    "start": "816580",
    "end": "822540"
  },
  {
    "text": "because that's the\nbest way to maximize the likelihood of the data.",
    "start": "822540",
    "end": "828730"
  },
  {
    "text": "And now, this is just\none iteration of EM. Now I would take\nthese parameters and go through the same process\nand go through the same process",
    "start": "828730",
    "end": "835329"
  },
  {
    "text": "until I convert. ",
    "start": "835330",
    "end": "841610"
  },
  {
    "start": "840000",
    "end": "884000"
  },
  {
    "text": "OK, so now, let's turn to an\ninteresting application of EM,",
    "start": "841610",
    "end": "846800"
  },
  {
    "text": "and that's decipherment. So this is an\nexample of a cipher. It's called a\nCopiale cipher, which",
    "start": "846800",
    "end": "853320"
  },
  {
    "text": "is an 105-page encrypted volume\ndating back from the 1730s. It looks like this.",
    "start": "853320",
    "end": "859540"
  },
  {
    "text": "So for a long time, no one\nknew what these words were.",
    "start": "859540",
    "end": "864690"
  },
  {
    "text": "It was finally cracked in\n2011 with the help of EM",
    "start": "864690",
    "end": "870020"
  },
  {
    "text": "by Kevin Knight,\nan NLP researcher. So the Copiale cipher is\nactually very complex.",
    "start": "870020",
    "end": "875830"
  },
  {
    "text": "So what we're going\nto do is motivate the idea of using Bayesian\nnetworks for decipherment",
    "start": "875830",
    "end": "881700"
  },
  {
    "text": "with a simple\nsubstitution cipher. So the idea behind the\nsubstitution cipher",
    "start": "881700",
    "end": "886959"
  },
  {
    "start": "884000",
    "end": "964000"
  },
  {
    "text": "is that suppose you wanted\nto send an encrypted message to someone, so you're going\nto generate a substitution",
    "start": "886960",
    "end": "894910"
  },
  {
    "text": "table which specifies how\neach letter gets transformed",
    "start": "894910",
    "end": "900879"
  },
  {
    "text": "into another letter. Cipher is going to be a\npermutation of another line.",
    "start": "900880",
    "end": "906350"
  },
  {
    "text": "And so then you have a\nmessage you want to send. Suppose you want\nto say hello world.",
    "start": "906350",
    "end": "913060"
  },
  {
    "text": "You're going to use\nthis substitution table, apply it to this plain text\nto produce a ciphertext.",
    "start": "913060",
    "end": "919600"
  },
  {
    "text": "And this is done by taking\na mapping h to n, e to m,",
    "start": "919600",
    "end": "926509"
  },
  {
    "text": "l to y, and l to y,\nand o to t, and so on.",
    "start": "926510",
    "end": "932830"
  },
  {
    "text": "So now you hide the\nsubstitution table, and then you hand\nsomeone the ciphertext,",
    "start": "932830",
    "end": "938850"
  },
  {
    "text": "or you put in a book and bury it\nfor someone to discover later.",
    "start": "938850",
    "end": "944100"
  },
  {
    "text": "So now the question is, when\nsomeone receives a ciphertext-- is given the ciphertext, can\nthey recover the plaintext?",
    "start": "944100",
    "end": "953970"
  },
  {
    "text": "Importantly, the plaintext\nis obviously unknown, but also the substitution\ntable is also unknown.",
    "start": "953970",
    "end": "961529"
  },
  {
    "text": "This is a very\nchallenging problem. But let's see how we can\nuse Bayesian networks,",
    "start": "961530",
    "end": "967070"
  },
  {
    "text": "in particular, HMM to\ntry to address this. So remember the\nprocess of using HMM",
    "start": "967070",
    "end": "974120"
  },
  {
    "text": "you have to think about what\nis the generative story of how this data arose?",
    "start": "974120",
    "end": "980250"
  },
  {
    "text": "So I'm going to model\nthis as follows. I'm going to have a sequence of\nletters, which are plaintext,",
    "start": "980250",
    "end": "988350"
  },
  {
    "text": "and these are hidden. And we have a corresponding\nsequence of characters",
    "start": "988350",
    "end": "993980"
  },
  {
    "text": "in the ciphertext. And I'm going to define a joint\ndistribution over all of these",
    "start": "993980",
    "end": "999680"
  },
  {
    "text": "by first generating the\nplaintext letters according",
    "start": "999680",
    "end": "1006370"
  },
  {
    "text": "to a Markov model via by a start\nand a bunch of transitions.",
    "start": "1006370",
    "end": "1013510"
  },
  {
    "text": "And then, for each\nplaintext letter, I'm going to generate\na ciphertext letter",
    "start": "1013510",
    "end": "1019660"
  },
  {
    "text": "via some emission. So the parameters\nof HMM remember",
    "start": "1019660",
    "end": "1025030"
  },
  {
    "text": "are the probability of start,\nthe probability of transition, and the probability of emission.",
    "start": "1025030",
    "end": "1031240"
  },
  {
    "text": "So intuitively,\nthe transitions are going to capture kind of\nthe cohesion of plaintext",
    "start": "1031240",
    "end": "1037179"
  },
  {
    "text": "because it's actually supposed\nto be readable and have structures, not random letters. And the emission is\ngoing to-- distribution",
    "start": "1037180",
    "end": "1044980"
  },
  {
    "text": "is going to capture\nthe substitution table. ",
    "start": "1044980",
    "end": "1050840"
  },
  {
    "start": "1050000",
    "end": "1194000"
  },
  {
    "text": "So how are we going to\nestimate this HMM, OK?",
    "start": "1050840",
    "end": "1057580"
  },
  {
    "text": "So first of all, we're going\nto make some just simplifying choices here, but we'll show\nthat it's kind of sufficient.",
    "start": "1057580",
    "end": "1065960"
  },
  {
    "text": "So we're going to set\na P start to a uniform. You could be a little\nbit more clever, but I'm just going to leave\nit alone for simplicity.",
    "start": "1065960",
    "end": "1075049"
  },
  {
    "text": "Then the transition\nprobabilities. So this is dressed up as a\nbigram model over characters.",
    "start": "1075050",
    "end": "1083434"
  },
  {
    "text": "And this model tells you what\nlooks like English or not.",
    "start": "1083434",
    "end": "1090659"
  },
  {
    "text": "And the really cool\nthing about this is that if we know the plain\ntext is supposed to be English, we can just go and\ngrab a ton of English",
    "start": "1090660",
    "end": "1097740"
  },
  {
    "text": "and estimate a distribution\nover that text, and that gives us P trans.",
    "start": "1097740",
    "end": "1104100"
  },
  {
    "text": "We don't even look\nat the ciphertext. And then finally,\nthe key part is",
    "start": "1104100",
    "end": "1110920"
  },
  {
    "text": "that the emission distribution\nis the substitution table, and that's what we're\ngoing to estimate from EM.",
    "start": "1110920",
    "end": "1119679"
  },
  {
    "text": "So notice that P\nemission is actually more general than a substitution. It says for every\nplaintext character,",
    "start": "1119680",
    "end": "1127160"
  },
  {
    "text": "I can actually\ngenerate a distribution over ciphertext letters,\nwhereas the substitution",
    "start": "1127160",
    "end": "1133480"
  },
  {
    "text": "table says there's exactly one. And this is more\nout of convenience because it makes\noptimization easier.",
    "start": "1133480",
    "end": "1141230"
  },
  {
    "text": "But in principle, you can\nalso think about P emit as being constrained to\njust a one to one mapping.",
    "start": "1141230",
    "end": "1146455"
  },
  {
    "text": " OK, so why do we think that\nthis will work intuitively?",
    "start": "1146455",
    "end": "1154350"
  },
  {
    "text": "Well, so the transition\ndistributions, which we've already\nestimated on English,",
    "start": "1154350",
    "end": "1159600"
  },
  {
    "text": "it's going to favor\na plain text that looks like English, while\nthe emission distribution is",
    "start": "1159600",
    "end": "1165140"
  },
  {
    "text": "going to try to favor consistent\ncharacter substitutions. So we don't want\nit to be the case that a maps to a t here\nand a v here and a f there.",
    "start": "1165140",
    "end": "1174980"
  },
  {
    "text": "We want some consistency. And by having this\nemission distribution and maximizing\nlikelihood, it's going",
    "start": "1174980",
    "end": "1180530"
  },
  {
    "text": "to try to encourage that\nkind of consistency. So we have these two forces\nkind of at play with each other",
    "start": "1180530",
    "end": "1187700"
  },
  {
    "text": "while we're trying to\nestimate both the hidden variables and the parameters. ",
    "start": "1187700",
    "end": "1195400"
  },
  {
    "start": "1194000",
    "end": "1340000"
  },
  {
    "text": "So let's try to actually\nstep into the EM algorithm",
    "start": "1195400",
    "end": "1201470"
  },
  {
    "text": "and say what kind\nof computations are needed to estimate this HMM.",
    "start": "1201470",
    "end": "1206620"
  },
  {
    "text": "So in the E-step,\nwhat I need to do is to compute the distribution\nover the hidden variables",
    "start": "1206620",
    "end": "1214110"
  },
  {
    "text": "conditioned on the observations. And t do that, we introduced\nthe forward-backward algorithm",
    "start": "1214110",
    "end": "1221910"
  },
  {
    "text": "a while back. And forward-backward algorithm\nis computing these smoothing queries, which is exactly\nwhat's the probability",
    "start": "1221910",
    "end": "1229890"
  },
  {
    "text": "of a plaintext letter being\na particular value h given",
    "start": "1229890",
    "end": "1237180"
  },
  {
    "text": "the ciphertext that we observe. And I'm going to do this for\neach position in this text--",
    "start": "1237180",
    "end": "1248430"
  },
  {
    "text": "in the ciphertext and\nevery potential character. So I'm going to define qi\nof h to be this probability.",
    "start": "1248430",
    "end": "1259210"
  },
  {
    "text": "So this is my best guess\nat a particular location. What do I think the\nplaintext character is?",
    "start": "1259210",
    "end": "1268180"
  },
  {
    "text": "So now, given these\nguesses, the M-step is going to re-estimate\nthe substitution table",
    "start": "1268180",
    "end": "1274330"
  },
  {
    "text": "or the emission distribution. So I'm going to count-- a fractional count and normalize\nfor all of the characters",
    "start": "1274330",
    "end": "1282250"
  },
  {
    "text": "e and h, OK? So for every possible\nplaintext letter",
    "start": "1282250",
    "end": "1291120"
  },
  {
    "text": "and every ciphertext\nletter, I'm going to look at all the positions\nwhere the ciphertext was",
    "start": "1291120",
    "end": "1298890"
  },
  {
    "text": "actually e, and I'm going to\nadd this probability or weight",
    "start": "1298890",
    "end": "1304260"
  },
  {
    "text": "qi of h. So this is going to tell me\nhow many times in expectation",
    "start": "1304260",
    "end": "1311440"
  },
  {
    "text": "we believe that a\nparticular plaintext letter and a particular\nciphertext letter are together.",
    "start": "1311440",
    "end": "1320880"
  },
  {
    "text": "And now, I'm just going to\nnormalize this distribution. So P emit of a\nciphertext letter given",
    "start": "1320880",
    "end": "1326460"
  },
  {
    "text": "a plaintext letter is\nproportional to this count emit of h and e.",
    "start": "1326460",
    "end": "1332370"
  },
  {
    "text": " OK, so that's it, and we\njust run the EM algorithm,",
    "start": "1332370",
    "end": "1338289"
  },
  {
    "text": "and we hope for the best. OK, so just to make this a\nlittle bit more exciting,",
    "start": "1338290",
    "end": "1343309"
  },
  {
    "start": "1340000",
    "end": "2074000"
  },
  {
    "text": "I'm going to try to\ncode this up in Python so we can see it in action.",
    "start": "1343310",
    "end": "1348920"
  },
  {
    "text": "All right, so a\nfew things first. So here is our ciphertext. You shouldn't be\nable to read this,",
    "start": "1348920",
    "end": "1355000"
  },
  {
    "text": "and we're going to\ntry to decipher this. And we also have\nthis lm.train, which",
    "start": "1355000",
    "end": "1363220"
  },
  {
    "text": "is this, quote unquote,\n\"large amount of English text\"",
    "start": "1363220",
    "end": "1369460"
  },
  {
    "text": "that we can draw from. OK, so I'm going to--",
    "start": "1369460",
    "end": "1375370"
  },
  {
    "text": "we also have this utility\nfile, which I'll just review, so it allows you to read text.",
    "start": "1375370",
    "end": "1380590"
  },
  {
    "text": "We're going to convert text\ninto a sequence of integers just for simplicity, and\nwe also importantly",
    "start": "1380590",
    "end": "1388640"
  },
  {
    "text": "have implemented this\nforward-backward algorithm, which is going to take a\nsequence of observations and the parameters of HMM.",
    "start": "1388640",
    "end": "1396220"
  },
  {
    "text": "And it's going to\nreturn q, which is a two-dimensional\narray where it's",
    "start": "1396220",
    "end": "1402700"
  },
  {
    "text": "a qi for each position\nwe have a distribution over possible values of hi.",
    "start": "1402700",
    "end": "1409152"
  },
  {
    "text": " OK, so let's decipher--",
    "start": "1409152",
    "end": "1415159"
  },
  {
    "text": "let's decipher some ciphertext. OK, so import util.",
    "start": "1415160",
    "end": "1421250"
  },
  {
    "text": "So I'm going to declare k to\nbe the number of characters.",
    "start": "1421250",
    "end": "1429440"
  },
  {
    "text": "So this is lowercase\nletters plus space but normalize the text.",
    "start": "1429440",
    "end": "1437790"
  },
  {
    "text": "The first thing I want to\ndo is initialize the HMM. So remember the\nparameters of HMM.",
    "start": "1437790",
    "end": "1443390"
  },
  {
    "text": "I have start probabilities. So this is going\nto be p_start of h.",
    "start": "1443390",
    "end": "1450770"
  },
  {
    "text": "And I'm just going to set this\nto the uniform distribution in all the data. So startProbs equals 1\ndivided by K for h in range K.",
    "start": "1450770",
    "end": "1465370"
  },
  {
    "text": "So that's going to be just\na uniform distribution. ",
    "start": "1465370",
    "end": "1472980"
  },
  {
    "text": "So now, what about the\ntransition probabilities? So transition probably\ngoes from h1 to h2.",
    "start": "1472980",
    "end": "1480216"
  },
  {
    "text": "And this is P trans\nof h2 given h1. So note the order\nis switched here",
    "start": "1480216",
    "end": "1485639"
  },
  {
    "text": "because I want transitionProbs\nof h1 to be actually an array which specifies\na distribution over h2.",
    "start": "1485640",
    "end": "1493590"
  },
  {
    "text": "So here, we're going to\nestimate this from plain text.",
    "start": "1493590",
    "end": "1502789"
  },
  {
    "text": "So I'm going to have raw text. This is I'm going to read\nit from lm.train, which",
    "start": "1502790",
    "end": "1509480"
  },
  {
    "text": "we saw earlier, and\nI'm going to convert it into an integer sequence, OK?",
    "start": "1509480",
    "end": "1516020"
  },
  {
    "text": "So let's see what\nthat looks like. So that's just a\nsequence of integers. ",
    "start": "1516020",
    "end": "1522960"
  },
  {
    "text": "OK, so now I'm going to estimate\nP trans from this raw text.",
    "start": "1522960",
    "end": "1529520"
  },
  {
    "text": "So this is actually going\nto be just a standard, fully observable estimation problem.",
    "start": "1529520",
    "end": "1537020"
  },
  {
    "text": "So I'm going to look\nover all positions.",
    "start": "1537020",
    "end": "1544560"
  },
  {
    "text": "I'm sorting from 1 to the end. And then I'm going to define--",
    "start": "1544560",
    "end": "1551360"
  },
  {
    "text": "look at h1 and h2 to be\nconsecutive characters",
    "start": "1551360",
    "end": "1559160"
  },
  {
    "text": "in this broad\ncharacter sequence. And then I'm going to\nincrement a counter.",
    "start": "1559160",
    "end": "1566090"
  },
  {
    "text": "So I'm going to define\ntransitionCounts to be for each h1 in range K\nand then for each h2 in range K.",
    "start": "1566090",
    "end": "1580660"
  },
  {
    "text": "I'm going to have a 0, OK? So this is going to\nbe a K by K 0 matrix.",
    "start": "1580660",
    "end": "1588399"
  },
  {
    "text": "And then I'm going to\njust increment this count. Solve this count once, and\nthen I'm going to normalize.",
    "start": "1588400",
    "end": "1596265"
  },
  {
    "text": "So the way I'm\ngoing to normalize is we're going to define\ntransition probabilities to be",
    "start": "1596265",
    "end": "1601570"
  },
  {
    "text": "for each h1, I'm going to call\nnormalized on transitionCounts",
    "start": "1601570",
    "end": "1611980"
  },
  {
    "text": "counts of h1, OK? And so for every h1, this\ngives me a distribution over h2",
    "start": "1611980",
    "end": "1620059"
  },
  {
    "text": "if I normalize it. That's going to be my\ntransition probability. I'm done with transition\nprobabilities.",
    "start": "1620060",
    "end": "1626600"
  },
  {
    "text": "So what about emission\nprobabilities? So here I have for every h,\nI have a distribution over e.",
    "start": "1626600",
    "end": "1636940"
  },
  {
    "text": "So this is going to be\njust to write it out in our mathematical language. This is going to be e given h.",
    "start": "1636940",
    "end": "1644950"
  },
  {
    "text": "So here, I just want\nto initialize it to the uniform distribution.",
    "start": "1644950",
    "end": "1652920"
  },
  {
    "text": "So just to document\nthis a little bit more, so we have a uniform\ndistribution. This is done-- estimate\nthis from plaintext,",
    "start": "1652920",
    "end": "1659480"
  },
  {
    "text": "this is, we're going\nto estimate this. This is just an initialization.",
    "start": "1659480",
    "end": "1667460"
  },
  {
    "text": "OK, so I'm going to\ninitialize this to for each h in the domain of h,\nfor each e in a similar domain.",
    "start": "1667460",
    "end": "1680299"
  },
  {
    "text": "I'm just going to\nhave a 1 over k. So this is a uniform\ndistribution.",
    "start": "1680300",
    "end": "1688350"
  },
  {
    "text": "And now, I'm going to run EM\nto estimate only this emission",
    "start": "1688350",
    "end": "1695390"
  },
  {
    "text": "probability there. So let's make it larger. So to run EM, I'm going\nto load my ciphertext in.",
    "start": "1695390",
    "end": "1703670"
  },
  {
    "text": "So observations equals\nread the text ciphertext,",
    "start": "1703670",
    "end": "1709760"
  },
  {
    "text": "and then I'm going to\nconvert this into a sequence. ",
    "start": "1709760",
    "end": "1717470"
  },
  {
    "text": "OK, so now I'm going to\niterate a number of times. Let's just call it 200.",
    "start": "1717470",
    "end": "1724580"
  },
  {
    "text": "And then I'm going to do the\nE-step and the M-step, OK?",
    "start": "1724580",
    "end": "1731120"
  },
  {
    "text": "So what happens in the E-step? I'm going to use my current\nsetting of parameters",
    "start": "1731120",
    "end": "1737390"
  },
  {
    "text": "to guess at what\nthe plaintext is?",
    "start": "1737390",
    "end": "1742460"
  },
  {
    "text": "So I'm going to run\nforward-backward on the observations and passing\nthe parameters of the HMM.",
    "start": "1742460",
    "end": "1753170"
  },
  {
    "text": " Larger-- and this is\ngoing to return q.",
    "start": "1753170",
    "end": "1762179"
  },
  {
    "text": "Just to note that\nq of h equals-- on a mathematical notation,\nthis is the probability",
    "start": "1762180",
    "end": "1768840"
  },
  {
    "text": "at that hi equals h given the\nevidence, which is observations",
    "start": "1768840",
    "end": "1777419"
  },
  {
    "text": "here. Print out our best guess so far. So let's see how we're doing.",
    "start": "1777420",
    "end": "1782559"
  },
  {
    "text": "We're going to do this\nat each iteration. So to do this, so for every--",
    "start": "1782560",
    "end": "1789809"
  },
  {
    "text": "let's define n equals the length\nof the number of observations here.",
    "start": "1789810",
    "end": "1794890"
  },
  {
    "text": "So for each position,\nI'm going to look at q1--",
    "start": "1794890",
    "end": "1801960"
  },
  {
    "text": "so this gives me a\ndistribution over h. And I'm going to\ntake the one that",
    "start": "1801960",
    "end": "1808560"
  },
  {
    "text": "has the highest probability. So then I'm going\nto convert this",
    "start": "1808560",
    "end": "1814610"
  },
  {
    "text": "to string and print it out, OK?",
    "start": "1814610",
    "end": "1821799"
  },
  {
    "start": "1821800",
    "end": "1827960"
  },
  {
    "text": "And now that finally,\nthe M-step is we're just going to count\nand normalize here.",
    "start": "1827960",
    "end": "1835550"
  },
  {
    "text": "So I'm going to define a new\ntemporary variable, which is emissionCounts.",
    "start": "1835550",
    "end": "1841430"
  },
  {
    "text": "And this is going to be-- let me just actually\ncheat a little bit,",
    "start": "1841430",
    "end": "1847540"
  },
  {
    "text": "and I'm going to\ncall emissionCounts to be 0 for the\nsame dimensionality",
    "start": "1847540",
    "end": "1855519"
  },
  {
    "text": "as emissionProbs. This is a matrix of zeros. OK, so now we're going to go\nthrough each position here,",
    "start": "1855520",
    "end": "1866179"
  },
  {
    "text": "i in range of n. And for each position, what\nare the possible values",
    "start": "1866180",
    "end": "1873100"
  },
  {
    "text": "it can take? So that's going to be h. And I'm going to up the\nemission counts of--",
    "start": "1873100",
    "end": "1881950"
  },
  {
    "text": "so emission remember is h, e. So h and e is going to\nbe observations of i",
    "start": "1881950",
    "end": "1888490"
  },
  {
    "text": "plus equal to q, i, h.",
    "start": "1888490",
    "end": "1894010"
  },
  {
    "text": "So this is probably the\nmost important line here.",
    "start": "1894010",
    "end": "1899060"
  },
  {
    "text": "So remember q, i, h is what is\nthe weight on a particular h",
    "start": "1899060",
    "end": "1907700"
  },
  {
    "text": "at position i. And then emissionCounts\nis going to be",
    "start": "1907700",
    "end": "1913570"
  },
  {
    "text": "h of that particular\nobservation, and I want to just\nupdate that count, OK?",
    "start": "1913570",
    "end": "1919910"
  },
  {
    "text": "So now all you need\nto do is normalize. So emission probabilities is\nfor each possible value of h.",
    "start": "1919910",
    "end": "1929640"
  },
  {
    "text": "I'm going to normalize\nemissionCounts of h, OK?",
    "start": "1929640",
    "end": "1938180"
  },
  {
    "text": "So that's it. So just to review this briefly.",
    "start": "1938180",
    "end": "1943230"
  },
  {
    "text": "So I first initialize HMM. The starting probabilities\nare just uniform.",
    "start": "1943230",
    "end": "1948529"
  },
  {
    "text": "And then I'm going to estimate\nthe transition probabilities in a fully-supervised way\nfrom plaintext, where I just",
    "start": "1948530",
    "end": "1956840"
  },
  {
    "text": "simply count and normalize. And then I'm going to initialize\nthe emission probabilities",
    "start": "1956840",
    "end": "1963290"
  },
  {
    "text": "to just uniform for now. Then I'm going to run the EM\nalgorithm to actually update",
    "start": "1963290",
    "end": "1971930"
  },
  {
    "text": "the emission probability, OK? So I read in the\nobservations, and then I'm",
    "start": "1971930",
    "end": "1977630"
  },
  {
    "text": "going to iterate between\nthe E-step and the M-step, where in the E-step I run\nforward-backward to compute",
    "start": "1977630",
    "end": "1985580"
  },
  {
    "text": "the distribution over\nparticular possible values of h",
    "start": "1985580",
    "end": "1991265"
  },
  {
    "text": "at each position and\nprint out my best guess. And then I'm going to\naccount and normalize.",
    "start": "1991265",
    "end": "1998720"
  },
  {
    "text": "All right, so let's\nsee how this does. So decipher.py. ",
    "start": "1998720",
    "end": "2006250"
  },
  {
    "text": "So at each step, it's going\nto print out its best guess. And over time, you can see\nthat this jumble of letters",
    "start": "2006250",
    "end": "2013180"
  },
  {
    "text": "is going to slowly evolve as\nEM is trying to figure out both the plain text as well\nas the substitution table.",
    "start": "2013180",
    "end": "2021470"
  },
  {
    "text": "So this isn't\ngoing to be perfect because we've used a\nfairly simple model, and we don't have too much data,\nbut you can see some structure",
    "start": "2021470",
    "end": "2028840"
  },
  {
    "text": "emerging. So I woved my woke\nalone without-- so that's a real word.",
    "start": "2028840",
    "end": "2035200"
  },
  {
    "text": "I need one that I could really-- and so on.",
    "start": "2035200",
    "end": "2041380"
  },
  {
    "text": "And plain-- there's probably\nsomething, and so on, OK?",
    "start": "2041380",
    "end": "2048320"
  },
  {
    "text": "So just for comparison, this\nis actually the plain text. So this is a little passage\nfrom the Little Prince.",
    "start": "2048320",
    "end": "2057440"
  },
  {
    "text": "So I lived my life\nalone without anyone that I could really\ntalk to, until I",
    "start": "2057440",
    "end": "2064399"
  },
  {
    "text": "had an accident with my plane. So definitely far from\nperfect, but given",
    "start": "2064400",
    "end": "2069679"
  },
  {
    "text": "that we just did it in a\nminute, it's maybe not bad.",
    "start": "2069679",
    "end": "2076129"
  },
  {
    "start": "2074000",
    "end": "2248000"
  },
  {
    "text": "OK, so let me summarize. We presented the EM\nalgorithm for estimating the parameters of\na Bayesian network when there are\nunobserved variables.",
    "start": "2076130",
    "end": "2083820"
  },
  {
    "text": "So the overarching\nprinciple is that of maximum marginal likelihood. We're going to find the\nparameters such that",
    "start": "2083820",
    "end": "2090889"
  },
  {
    "text": "that drives up the\nprobability of the variables that we did observe\nas much as possible.",
    "start": "2090890",
    "end": "2098230"
  },
  {
    "text": "So the EM algorithm\nis going to optimize the marginal\nlikelihood objective,",
    "start": "2098230",
    "end": "2105590"
  },
  {
    "text": "but fundamentally it's a\nchicken and egg problem just like in K-mean. We don't know the\nhidden variables,",
    "start": "2105590",
    "end": "2111530"
  },
  {
    "text": "and we also don't\nknow the parameters. So what we're going\nto do is to iterate between one and the other.",
    "start": "2111530",
    "end": "2119120"
  },
  {
    "text": "So in the E-step, we're going to\nperform probabilistic inference given a fixed set of\nparameters to produce",
    "start": "2119120",
    "end": "2125620"
  },
  {
    "text": "our best guess over what some\nof the hidden variables are. And then in M-step, we're going\nto use these probabilities",
    "start": "2125620",
    "end": "2134830"
  },
  {
    "text": "as weights of examples\nand then we're just going to count and\nnormalize to parameters,",
    "start": "2134830",
    "end": "2140680"
  },
  {
    "text": "and then we are going to\nestimate the hidden variables and estimate the parameters,\nand so on, and so forth.",
    "start": "2140680",
    "end": "2147650"
  },
  {
    "text": "So finally, once you've\nlearned your Bayesian network, you can go off and perform\ninference and answer",
    "start": "2147650",
    "end": "2153130"
  },
  {
    "text": "all sorts of questions,\nwhich could involve asking about these\nunobserved variables",
    "start": "2153130",
    "end": "2158950"
  },
  {
    "text": "that you didn't see\non new test examples, or it could be used\nto ask questions",
    "start": "2158950",
    "end": "2163990"
  },
  {
    "text": "about the observed variables\ngiven some other variables.",
    "start": "2163990",
    "end": "2169490"
  },
  {
    "text": "And in general, this highlights\nkind of the flexibility of Bayesian networks. Just because you had a\ncertain pattern of messiness",
    "start": "2169490",
    "end": "2176859"
  },
  {
    "text": "at training time\ndoesn't mean you have to commit to that at test time.",
    "start": "2176860",
    "end": "2182190"
  },
  {
    "text": "So there is many applications\nof Bayesian networks, including involving\nthe EM algorithm.",
    "start": "2182190",
    "end": "2187530"
  },
  {
    "text": "We looked at decipherment,\nwhere the goal is to infer the plaintext\nfrom the ciphertext.",
    "start": "2187530",
    "end": "2192569"
  },
  {
    "text": "EM could also be\nused to reconstruct phylogenetic trees given\nthe DNA of modern organisms.",
    "start": "2192570",
    "end": "2199530"
  },
  {
    "text": "And it can also be used to infer\nthe unknown label of a data",
    "start": "2199530",
    "end": "2204750"
  },
  {
    "text": "point where the observations\nare the possible noisy labels provided by crowd workers.",
    "start": "2204750",
    "end": "2211440"
  },
  {
    "text": "So finally, EM is the\nmost canonical version of a broader class of techniques\ncalled variational inference,",
    "start": "2211440",
    "end": "2218010"
  },
  {
    "text": "which actually includes things\nlike variational autoencoders, which some of you\nmight have heard of.",
    "start": "2218010",
    "end": "2223950"
  },
  {
    "text": "In that case, the q\nactually the encoder,",
    "start": "2223950",
    "end": "2229560"
  },
  {
    "text": "and it's given by\na neural network. And the decoder is\nthe Bayesian network.",
    "start": "2229560",
    "end": "2235210"
  },
  {
    "text": "So there's a lot more to\nconnections to be explored, and I encourage you to read\nup on this by yourself.",
    "start": "2235210",
    "end": "2243080"
  },
  {
    "start": "2243080",
    "end": "2248000"
  }
]