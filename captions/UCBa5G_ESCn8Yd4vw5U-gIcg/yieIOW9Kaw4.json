[
  {
    "text": "Okay. Let's get started. Ah, welcome back. So we're going to continue on to Lecture 7 today.",
    "start": "4100",
    "end": "11940"
  },
  {
    "text": "And the topic for today is going to be generative learning algorithms.",
    "start": "11940",
    "end": "17325"
  },
  {
    "text": "Um, so the plan for today is to- is to,",
    "start": "17325",
    "end": "22380"
  },
  {
    "text": "uh, cover Gaussian discriminant analysis, GDA. This is also on your homework.",
    "start": "22380",
    "end": "27480"
  },
  {
    "text": "And then move on to Naive Bayes, um, another generative learning algorithm.",
    "start": "27480",
    "end": "32544"
  },
  {
    "text": "And before we dive into, uh, generative models, let's look at what we have covered so far.",
    "start": "32545",
    "end": "38495"
  },
  {
    "text": "So, so far we have covered linear regression, logistic regression, and generalized linear models.",
    "start": "38495",
    "end": "44555"
  },
  {
    "text": "All three of these are what you may call discriminative algorithms. We call them discriminative algorithms because, uh,",
    "start": "44555",
    "end": "52530"
  },
  {
    "text": "they directly model p of y given x, where y is the desired output and x is the input, right?",
    "start": "52530",
    "end": "57710"
  },
  {
    "text": "And supervised learning, in the supervised learning setting, we are interested in learning map- mappings from x to y.",
    "start": "57710",
    "end": "62920"
  },
  {
    "text": "And, um, when y given x is assumed to be a normal,",
    "start": "62920",
    "end": "68545"
  },
  {
    "text": "uh, distribution, what we get as a consequence is linear regression. And when y given x is assumed to be Bernoulli,",
    "start": "68545",
    "end": "74750"
  },
  {
    "text": "as a consequence, we get logistic regression. And when y given x is assumed to be in the exponential family,",
    "start": "74750",
    "end": "81229"
  },
  {
    "text": "we get generalized linear models. And we saw that generalized linear models is a broad family.",
    "start": "81230",
    "end": "87184"
  },
  {
    "text": "And it includes normal and Bernoulli as special cases. So linear regression and logistic regression are special cases of,",
    "start": "87184",
    "end": "94800"
  },
  {
    "text": "um, generalized linear models. And now, [NOISE] right now,",
    "start": "94800",
    "end": "105120"
  },
  {
    "text": "we're going to start focusing on x, right?",
    "start": "105120",
    "end": "108010"
  },
  {
    "text": "In discriminative algorithms, x is assumed to be given. In fact, when we were, uh,",
    "start": "110920",
    "end": "117975"
  },
  {
    "text": "looking at these, um, at these methods, each of the xj features or attributes could have been real valued,",
    "start": "117975",
    "end": "126560"
  },
  {
    "text": "could have been integer valued, could have been Booleans. And we were kind of agnostic to it. All right? But now we're going to focus on x.",
    "start": "126560",
    "end": "134730"
  },
  {
    "text": "And we will model p of x,y.",
    "start": "134730",
    "end": "143830"
  },
  {
    "text": "Instead of modeling p of y given x, that was the conditional distribution, you're now going to model p of x,y.",
    "start": "143840",
    "end": "151220"
  },
  {
    "text": "That's the full joint distribution. Right? And this, ah, joint distribution is also, uh,",
    "start": "151220",
    "end": "158675"
  },
  {
    "text": "commonly called as the model, right? And we see that this can be factorized using the chain rule of",
    "start": "158675",
    "end": "167090"
  },
  {
    "text": "probability into p of x given y times p of y.",
    "start": "167090",
    "end": "174700"
  },
  {
    "text": "All right? And p of y is also commonly called as the class prior.",
    "start": "174700",
    "end": "182610"
  },
  {
    "text": "Why is it called the class- class prior? So the, um, we will be focusing mostly on the cases where y is discrete valued,",
    "start": "184640",
    "end": "195560"
  },
  {
    "text": "which means these generative models, ah, that we're going to, ah, discuss today, Gaussian discriminant analysis and Naive Bayes",
    "start": "195560",
    "end": "202565"
  },
  {
    "text": "are- are best suited for classification problems. Um, so y is- is- is,",
    "start": "202565",
    "end": "208909"
  },
  {
    "text": "um, indicates the class of- of a given example. And p of y is just the marginal probability of- of- um,",
    "start": "208910",
    "end": "216995"
  },
  {
    "text": "of- of- of y. And the class prior generally, um, you- you think of it as what is the fraction of",
    "start": "216995",
    "end": "225190"
  },
  {
    "text": "examples which belong to a particular class without even looking at, you know, the x features. You know, overall in your- in your population,",
    "start": "225190",
    "end": "232180"
  },
  {
    "text": "what fraction of your examples belong to a particular class? All right? And p of x given y is generally high dimensional.",
    "start": "232180",
    "end": "241660"
  },
  {
    "text": "[NOISE] All right?",
    "start": "241660",
    "end": "247185"
  },
  {
    "text": "So p of y given x in- in these, uh, in these two examples,",
    "start": "247185",
    "end": "252914"
  },
  {
    "text": "y was a scalar. It's either a real-valued or a 0 or 1, right?",
    "start": "252914",
    "end": "258384"
  },
  {
    "text": "Whereas generally p of x given y is high-dimensional, where x is the set of all your features, all right?",
    "start": "258385",
    "end": "264295"
  },
  {
    "text": "And- and so p of x given y, we are trying to model, um, uh,",
    "start": "264295",
    "end": "270340"
  },
  {
    "text": "um, a high- a high-dimensional probability distribution, um, because your- your, uh, features can be very high valued.",
    "start": "270340",
    "end": "276319"
  },
  {
    "text": "And that's in general, um, a harder problem. Ah, high-dimensional probability is, ah, in general,",
    "start": "276320",
    "end": "282790"
  },
  {
    "text": "um, ah, harder both for you know, analytical and computational reasons. Yes, question.",
    "start": "282790",
    "end": "288449"
  },
  {
    "text": "[inaudible]",
    "start": "288450",
    "end": "300210"
  },
  {
    "text": "So right, so the question is, um, you know, ah, why do we want to do generative modeling?",
    "start": "300210",
    "end": "307380"
  },
  {
    "text": "Why do we want to model p of x? Because x is generally given to us? Is that the question? Yeah, so, uh, there are a few reasons why you want to do this and, uh, uh,",
    "start": "307380",
    "end": "315935"
  },
  {
    "text": "we will- we will touch upon that towards the end of Gaussian discriminant analysis, you know, uh, at this point, where we kind of compare, ah, um,",
    "start": "315935",
    "end": "322430"
  },
  {
    "text": "you know, when we want to do one versus the other. And that may be a- a good time to kind of re-ask the question.",
    "start": "322430",
    "end": "328670"
  },
  {
    "text": "Um, all right? So this is, um,",
    "start": "328670",
    "end": "333330"
  },
  {
    "text": "so we want to model p of, um, x given y. And at prediction time,",
    "start": "333860",
    "end": "339275"
  },
  {
    "text": "when we want to make a prediction on- on, uh, a new example, x test that's given to us.",
    "start": "339275",
    "end": "346225"
  },
  {
    "text": "So p of y given x, where x is like a new- new, uh, example.",
    "start": "346225",
    "end": "352775"
  },
  {
    "text": "We can use the Bayes rule to, um, swap it around. This will be p of x given y times p of y over p of x, all right?",
    "start": "352775",
    "end": "366510"
  },
  {
    "text": "P of x given y and p of y are- are- is the direct, uh, chain rule expansion of the joint.",
    "start": "366510",
    "end": "373324"
  },
  {
    "text": "And we get a p of x in the denominator. And this can also, um, generally,",
    "start": "373325",
    "end": "379155"
  },
  {
    "text": "in cases when y is- is, uh, binary, we can write this as p of x given y times p of",
    "start": "379155",
    "end": "390200"
  },
  {
    "text": "y over p of x given y equals 0,",
    "start": "390200",
    "end": "396375"
  },
  {
    "text": "and p of y equals 0 plus p of x given y equals 1 times p of y equals 1.",
    "start": "396375",
    "end": "406245"
  },
  {
    "text": "All right? And- and we can write it this way because p of x is exactly equal to this when y is,",
    "start": "406245",
    "end": "414425"
  },
  {
    "text": "ah, y is binary. All right? So this is the, um, the Bayes rule to get p of y given x,",
    "start": "414425",
    "end": "422010"
  },
  {
    "text": "and p of y given x is generally called the posterior distribution.",
    "start": "422010",
    "end": "426600"
  },
  {
    "text": "And when we want to make a prediction as a class rather than a probability, right?",
    "start": "432410",
    "end": "439060"
  },
  {
    "text": "So p of y given x will give you the probability of say, y equals 1 given x or y equals 0 given x. Um,",
    "start": "439060",
    "end": "445535"
  },
  {
    "text": "but when you want to make a prediction of whether, er, example,",
    "start": "445535",
    "end": "450595"
  },
  {
    "text": "x belongs to the class y equals 0 or y equals 1, we generally, um, do it this way.",
    "start": "450595",
    "end": "457255"
  },
  {
    "text": "So y-hat equals arg max of y,",
    "start": "457255",
    "end": "464770"
  },
  {
    "text": "p of y given x, all right?",
    "start": "464770",
    "end": "470520"
  },
  {
    "text": "Which is the y that has the highest posterior probability? Does y equals 0 have a higher posterior probability,",
    "start": "470520",
    "end": "476870"
  },
  {
    "text": "or does y equals 1 have a higher posterior probability. All right? And this is- this is the- the ah,",
    "start": "476870",
    "end": "481965"
  },
  {
    "text": "common recipe for making predictions whether we are using Gaussian discriminant analysis or Naive Bayes",
    "start": "481965",
    "end": "488900"
  },
  {
    "text": "or any generative model, uh, in general. And in order to do this prediction,",
    "start": "488900",
    "end": "495604"
  },
  {
    "text": "you may observe that p of y given x,",
    "start": "495604",
    "end": "501230"
  },
  {
    "text": "we can write it like this p of, um, so the arg max over y.",
    "start": "501230",
    "end": "506540"
  },
  {
    "text": "And p of y given x was, uh, p of x given y,",
    "start": "506540",
    "end": "512104"
  },
  {
    "text": "p of y over p of x. Right? And in order to calculate the arg max over y,",
    "start": "512105",
    "end": "521150"
  },
  {
    "text": "we observe that p of x, the denominator, is just some constant.",
    "start": "521180",
    "end": "526645"
  },
  {
    "text": "And it's going to evaluate to the same value for the cases of y equals 0 and for the case of y equals 1,",
    "start": "526645",
    "end": "532975"
  },
  {
    "text": "which means this is the same as arg max over y,",
    "start": "532975",
    "end": "540639"
  },
  {
    "text": "p of x given y times p of y, right?",
    "start": "540640",
    "end": "546665"
  },
  {
    "text": "So, um, for the purposes of making a prediction of which class we want to assign an example to,",
    "start": "546665",
    "end": "556795"
  },
  {
    "text": "we don't even have to calculate the denominator. Yes? There's a question? [inaudible]",
    "start": "556795",
    "end": "585050"
  },
  {
    "text": "Yeah, so the question is, you know, P of y has-has, um, a meaningful interpretation.",
    "start": "585050",
    "end": "591399"
  },
  {
    "text": "You can think of it as a fraction of examples that are positive. You know, what's- what does p of x mean? Right? And p of x means,",
    "start": "591400",
    "end": "598345"
  },
  {
    "text": "it can mean different things in different problems. Um, so p of x, uh, you can think of it as what's the probability that you're gonna encounter",
    "start": "598345",
    "end": "607285"
  },
  {
    "text": "this example- this input in your population?",
    "start": "607285",
    "end": "612504"
  },
  {
    "text": "Right? That's one way to think of it. And, um, it could have, you know, um, it could be- it could belong to class 0 or class 1.",
    "start": "612505",
    "end": "620454"
  },
  {
    "text": "But you're, you know, given- given, uh, an input example, you know, you're trying to answer the question,",
    "start": "620455",
    "end": "626380"
  },
  {
    "text": "what's the probability of encountering that input. Right? Yeah, that's- that's a rough intuition.",
    "start": "626380",
    "end": "633985"
  },
  {
    "text": "Yes. Question. What is the significance of maximizing y?",
    "start": "633985",
    "end": "640345"
  },
  {
    "text": "So, um, what we're calculating here is we are trying to find out",
    "start": "640345",
    "end": "648834"
  },
  {
    "text": "which value of y results in the highest posterior probability because we want to-",
    "start": "648835",
    "end": "654385"
  },
  {
    "text": "we want to consider that value of y as our prediction. Does it make sense? So if our goal is to just, uh,",
    "start": "654385",
    "end": "666130"
  },
  {
    "text": "make a decision of choosing y equals 0 or y equals 1,",
    "start": "666130",
    "end": "671650"
  },
  {
    "text": "then for that problem of making a decision of y equals 0 or y equals 1, we don't even have to calculate the denominator.",
    "start": "671650",
    "end": "679000"
  },
  {
    "text": "But if you want to calculate the probability of- of y given x,",
    "start": "679000",
    "end": "684370"
  },
  {
    "text": "then we absolutely need to calculate the denominator. Right? The two are different problems. Right? If you want to calculate the exact probability, is it 0.77 or 0.5 or 0.2,",
    "start": "684370",
    "end": "694465"
  },
  {
    "text": "then you absolutely need to calculate the denominator. But if your problem is to only decide whether probability of",
    "start": "694465",
    "end": "702520"
  },
  {
    "text": "y equals 0 given x is greater than or lesser than probability of y equals 1 given x,",
    "start": "702520",
    "end": "708730"
  },
  {
    "text": "just to do the- the greater than or lesser than then you don't need the denominator. Yes, question. [inaudible]",
    "start": "708730",
    "end": "720520"
  },
  {
    "text": "So in this case, the question is 1 the probability of x a change depend on what class it came from.",
    "start": "720520",
    "end": "726175"
  },
  {
    "text": "So in this case, we have marginalized out the class, right? So you're just considering all the examples in-",
    "start": "726175",
    "end": "732160"
  },
  {
    "text": "in one common pool where, you know, you know, they're mixed with all the classes and just going to pick one example at random",
    "start": "732160",
    "end": "737380"
  },
  {
    "text": "and ask what's the- what's the probability that you observe the input? Right? So, so with- in this setting,",
    "start": "737380",
    "end": "746500"
  },
  {
    "text": "we're gonna cover two different algorithms.",
    "start": "746500",
    "end": "749510"
  },
  {
    "text": "Right? Two algorithms for today.",
    "start": "752580",
    "end": "758150"
  },
  {
    "text": "And for both the algorithms, y will be discrete.",
    "start": "763740",
    "end": "771590"
  },
  {
    "text": "Right? So the first algorithm is GDA: Gaussian discriminant analysis.",
    "start": "773910",
    "end": "779259"
  },
  {
    "text": "And this x is continuous, continuous, right?",
    "start": "779259",
    "end": "782660"
  },
  {
    "text": "And the other algorithm you gotta consider is naive Bayes. And this x is discrete.",
    "start": "791160",
    "end": "799280"
  },
  {
    "text": "Okay? And, you know, the example that we're going to use is- is text classification, right?",
    "start": "802050",
    "end": "809269"
  },
  {
    "text": "So in discriminative algorithms, when- we- we had two different kinds of algorithms,",
    "start": "813960",
    "end": "821410"
  },
  {
    "text": "depending on whether y was continuous or discrete, over here our focus is on x.",
    "start": "821410",
    "end": "828055"
  },
  {
    "text": "And we're gonna get two different kinds of algorithms depending on whether x was continuous or x was discrete.",
    "start": "828055",
    "end": "834340"
  },
  {
    "text": "But we're limiting ourselves to the- to the- to the case of classification only. So we're gonna limit ourselves to,",
    "start": "834340",
    "end": "839755"
  },
  {
    "text": "you know, y equals, uh, 0 or 1. Uh, but your, uh, x's can be continuous or- or, uh, discrete, right?",
    "start": "839755",
    "end": "846980"
  },
  {
    "text": "And before we jump into our, uh, first learning algorithm, first G, um,",
    "start": "846990",
    "end": "855490"
  },
  {
    "text": "jumping into GDA, just some terminology. Um, so for the model,",
    "start": "855490",
    "end": "862675"
  },
  {
    "text": "we call the joint probability of p of x, y as the model. So this is the joint p of x, y.",
    "start": "862675",
    "end": "871654"
  },
  {
    "text": "And it is common practice to define our model or express our model as a data generating process, right?",
    "start": "871655",
    "end": "880550"
  },
  {
    "text": "Data generating process. And this process is- is represented as a hierarchy",
    "start": "885960",
    "end": "895500"
  },
  {
    "text": "of steps with which we generate our model.",
    "start": "895500",
    "end": "903130"
  },
  {
    "text": "And this has a- a one-to-one correspondence with the way we factorize our joint probability.",
    "start": "903130",
    "end": "910279"
  },
  {
    "text": "Okay? Now, what does this mean? Let's- let's see the case of GDA to get a concrete sense of what- what we mean by this.",
    "start": "915090",
    "end": "924860"
  },
  {
    "text": "So GDA. This is our first generative model.",
    "start": "931440",
    "end": "938500"
  },
  {
    "text": "So in GDA, we define the model like this as a data generating process.",
    "start": "938500",
    "end": "943810"
  },
  {
    "text": "So y sample from Bernoulli",
    "start": "943810",
    "end": "950950"
  },
  {
    "text": "with parameter Phi and x given y is sampled.",
    "start": "950950",
    "end": "958330"
  },
  {
    "text": "x equals y equals 0, that's sampled from a normal distribution with mean mu naught and covariance sigma.",
    "start": "958330",
    "end": "968800"
  },
  {
    "text": "x given y equals 1 is sampled from a normal distribution with mean mu 1 and covariance Sigma.",
    "start": "968800",
    "end": "978834"
  },
  {
    "text": "Okay? So this is a hierarchy of how- a hierarchy of steps with which we can generate our data.",
    "start": "978835",
    "end": "988135"
  },
  {
    "text": "So first sample, a class variable from a Bernoulli distribution with",
    "start": "988135",
    "end": "993940"
  },
  {
    "text": "parameter Phi that decides whether the example that we're about to generate belongs to the class y equals 0,",
    "start": "993940",
    "end": "1000089"
  },
  {
    "text": "or does it belong to the class y equals 1. And then we generate the input x given y",
    "start": "1000090",
    "end": "1006480"
  },
  {
    "text": "equals 0 according to a normal distribution with some mean and covariance. And if y equals 1, we generate the example as",
    "start": "1006480",
    "end": "1013290"
  },
  {
    "text": "a sample from a different normal distribution with mean mu 1. But in this case, we are sharing the covariance matrix and-",
    "start": "1013290",
    "end": "1019800"
  },
  {
    "text": "and we'll go into the details of why we do that later. But for now, assume both the normal distributions",
    "start": "1019800",
    "end": "1025049"
  },
  {
    "text": "share the covariance matrix but have different means. And for this sequence of steps,",
    "start": "1025050",
    "end": "1033030"
  },
  {
    "text": "the corresponding distributions will be p of y equals Phi y times 1 minus Phi to the 1 minus y.",
    "start": "1033030",
    "end": "1046964"
  },
  {
    "text": "And this is the Bernoulli, um, the Bernoulli distribution. And p of x given y equals 0 is equal to 1 over 2 pi to",
    "start": "1046965",
    "end": "1061470"
  },
  {
    "text": "the d by 2 and one half minus half",
    "start": "1061470",
    "end": "1071940"
  },
  {
    "text": "x minus mu naught transpose inverse minus mu naught.",
    "start": "1071940",
    "end": "1078610"
  },
  {
    "text": "Similarly p of xa, x1 over.",
    "start": "1082010",
    "end": "1086080"
  },
  {
    "text": "And here y is in 0, 1 and x is in-",
    "start": "1110750",
    "end": "1118935"
  },
  {
    "text": "oops [NOISE] R^d, right?",
    "start": "1118935",
    "end": "1126720"
  },
  {
    "text": "So, uh, what do we have here? So p of y is- is a simple, um, Bernoulli.",
    "start": "1126720",
    "end": "1134969"
  },
  {
    "text": "And let me just write the parameters in a different color. So these are all parameters. [NOISE] Okay.",
    "start": "1134969",
    "end": "1144220"
  },
  {
    "text": "And p of, uh, x given, uh, y equals 0 is parameter here.",
    "start": "1147050",
    "end": "1156929"
  },
  {
    "text": "[NOISE] Mu naught,",
    "start": "1156930",
    "end": "1166080"
  },
  {
    "text": "Sigma inverse, Mu naught. And similarly, this case here.",
    "start": "1166080",
    "end": "1171540"
  },
  {
    "text": "[NOISE]",
    "start": "1171540",
    "end": "1184850"
  },
  {
    "text": "Okay? [NOISE] So y is- is, um, is discrete, it's either 0 or 1.",
    "start": "1184850",
    "end": "1190069"
  },
  {
    "text": "And depending on what y we sample, we generate an x, that's the, um,",
    "start": "1190069",
    "end": "1196500"
  },
  {
    "text": "input based on what either one of the two Gaussian distributions with different means,",
    "start": "1196500",
    "end": "1201510"
  },
  {
    "text": "but they share the same covariance structure. Right? So, um, the generative algorithms,",
    "start": "1201510",
    "end": "1209804"
  },
  {
    "text": "one way to think of them as in discriminative algorithms, you are asked to differentiate,",
    "start": "1209805",
    "end": "1216315"
  },
  {
    "text": "or given- given, um, an input example, you are asked to decide whether that example belong to,",
    "start": "1216315",
    "end": "1222510"
  },
  {
    "text": "you know, one class or another class, right? Um, where you're kind of acting like a critic, where, you know,",
    "start": "1222510",
    "end": "1228090"
  },
  {
    "text": "you're given an x and you are asked to classify it as x versus y. But as in generative algorithms, the way you think of them as you are asked to kind of generate the input.",
    "start": "1228090",
    "end": "1236130"
  },
  {
    "text": "You kind of here now, you're kind of acting like an artist. You are trying to come up with, construct an example x, which look different for different,",
    "start": "1236130",
    "end": "1241965"
  },
  {
    "text": "uh, different examples, right? And- or for different classes. So the- the, um,",
    "start": "1241965",
    "end": "1247425"
  },
  {
    "text": "so the goals here are- are kind of, uh, different in some way, where in one case, in discriminative algorithms,",
    "start": "1247425",
    "end": "1255375"
  },
  {
    "text": "all you are asked to do is have good power to discriminate whether an example belongs to x equals- y equals 0 or y equals 1,",
    "start": "1255375",
    "end": "1264665"
  },
  {
    "text": "which means you may limit yourself to just look for certain- certain cues in the input that",
    "start": "1264665",
    "end": "1270840"
  },
  {
    "text": "helped you discriminate it and just ignore the rest of- rest of the example. Whereas in generative algorithms,",
    "start": "1270840",
    "end": "1277410"
  },
  {
    "text": "you're asked to generate an entire example, right? You're- you're- you're expected to come up with a full description of what makes up a good algorithm of,",
    "start": "1277410",
    "end": "1285915"
  },
  {
    "text": "uh, a good image or a good, you know, um, um, example of y equals 0 or y equals 1 class, right?",
    "start": "1285915",
    "end": "1292230"
  },
  {
    "text": "So in general, uh, generative model or generative modeling is um, is um, a harder problem compared to, um, discriminative modeling.",
    "start": "1292230",
    "end": "1303315"
  },
  {
    "text": "And you can also see that, um, because p of x, y is equal to p of y given x times p of x,",
    "start": "1303315",
    "end": "1314490"
  },
  {
    "text": "which means in order to learn a generative model, you have to learn a discriminative model and also learn something else, right?",
    "start": "1314490",
    "end": "1323970"
  },
  {
    "text": "So generative modeling in- in general is- is a harder problem compared to just the problem of- of,",
    "start": "1323970",
    "end": "1330765"
  },
  {
    "text": "uh, learning one of the conditions, right? So this is- think of this as discriminative and this is generative.",
    "start": "1330765",
    "end": "1342345"
  },
  {
    "text": "And you have something extra as well, you know, uh, compared to a discriminative model. Yes, question. Let's say someone who puts",
    "start": "1342345",
    "end": "1349034"
  },
  {
    "text": "some data [inaudible] you'll learn [inaudible] all the parameters to be non-Sigma and we will try",
    "start": "1349035",
    "end": "1355660"
  },
  {
    "text": "to generate more examples and different strategies [inaudible]. So the question is, um, with a generative model,",
    "start": "1355660",
    "end": "1361770"
  },
  {
    "text": "uh, given some, uh, um, training set, for example, we learn how to generate new training examples,",
    "start": "1361770",
    "end": "1367049"
  },
  {
    "text": "but we won't make a prediction. Is that the question? Yeah. So- so the, um, the question, uh, er,",
    "start": "1367050",
    "end": "1372365"
  },
  {
    "text": "the answer to that is- is, uh, no. Um, because we are studying these in the context of supervised learning,",
    "start": "1372365",
    "end": "1382095"
  },
  {
    "text": "which means our end goal, at least in this part of the course, is still making predictions, right?",
    "start": "1382095",
    "end": "1387929"
  },
  {
    "text": "And what we are doing is that we are learning the joint distribution, which means not only can we generate new examples,",
    "start": "1387930",
    "end": "1395970"
  },
  {
    "text": "we can also discriminate them, right? [inaudible]",
    "start": "1395970",
    "end": "1402200"
  },
  {
    "text": "Exactly. So when we want to discriminate, we will use the Bayes rule to construct a posterior distribution,",
    "start": "1402200",
    "end": "1409165"
  },
  {
    "text": "and the posterior distribution, um, you know, and here's a recipe to construct the posterior distribution using the generative modeling components.",
    "start": "1409165",
    "end": "1416669"
  },
  {
    "text": "And we can use this recipe to make predictions on new examples. [NOISE] All right? All right,",
    "start": "1416669",
    "end": "1428610"
  },
  {
    "text": "so, um, back to GDA. We have defined the model like this,",
    "start": "1428610",
    "end": "1434835"
  },
  {
    "text": "where y is in, um, y is binary, it's 0 or 1, x is continuous.",
    "start": "1434835",
    "end": "1440505"
  },
  {
    "text": "And the- the parameters of this model, [NOISE] the parameters are",
    "start": "1440505",
    "end": "1448980"
  },
  {
    "text": "[NOISE] Phi that belongs to the, uh, Bernoulli.",
    "start": "1448980",
    "end": "1456179"
  },
  {
    "text": "Mu naught that belongs to, uh, this distribution.",
    "start": "1456180",
    "end": "1461205"
  },
  {
    "text": "Mu_1 that belongs to, uh, uh, the sec- second- third distribution,",
    "start": "1461205",
    "end": "1466349"
  },
  {
    "text": "and Sigma that's common to both. All right? So first we define the data generating process.",
    "start": "1466349",
    "end": "1472500"
  },
  {
    "text": "In order to come up with- with this step, you don't need any data, right? This is just math.",
    "start": "1472500",
    "end": "1478020"
  },
  {
    "text": "You're- you're making an assumption that this is the way your data's generated, right? And with this assumption,",
    "start": "1478020",
    "end": "1483480"
  },
  {
    "text": "we have already defined our probability densities. We have determined what our parameters are.",
    "start": "1483480",
    "end": "1488700"
  },
  {
    "text": "And then we observe data. And when we observe data, we can use the data to fit our model and learn these parameters using maximum likelihood.",
    "start": "1488700",
    "end": "1497549"
  },
  {
    "text": "Right? So [NOISE] maximum likelihood",
    "start": "1497550",
    "end": "1508499"
  },
  {
    "text": "[NOISE] to learn parameters.",
    "start": "1508499",
    "end": "1514510"
  },
  {
    "text": "In order to, uh, perform maximum likelihood, the first step is to write out the likelihood function or the log-likelihood function.",
    "start": "1519290",
    "end": "1527985"
  },
  {
    "text": "And in this case the, uh, log-likelihood [NOISE] looks like this.",
    "start": "1527985",
    "end": "1537135"
  },
  {
    "text": "So it is, the likelihood is a function over the parameters, not the data, right?",
    "start": "1537135",
    "end": "1542325"
  },
  {
    "text": "A probability density is a function of the data and assumes, uh, um, parameters.",
    "start": "1542325",
    "end": "1547815"
  },
  {
    "text": "And the likelihood is a function of p parameters, Phi, Mu naught, Mu_1 Sigma.",
    "start": "1547815",
    "end": "1556470"
  },
  {
    "text": "[NOISE] And this is [NOISE] equals 1 to",
    "start": "1556470",
    "end": "1564900"
  },
  {
    "text": "n. The joint p of x^i comma y^i.",
    "start": "1564900",
    "end": "1571575"
  },
  {
    "text": "Right? And this is where generative modeling differs from discriminative modeling, right?",
    "start": "1571575",
    "end": "1576960"
  },
  {
    "text": "In discriminative models, we use P of y given x. And in case of generative models, we use the joint P of x,  y.",
    "start": "1576960",
    "end": "1584380"
  },
  {
    "text": "And this is the keys- key difference between generative models and discriminative models.",
    "start": "1584450",
    "end": "1590115"
  },
  {
    "text": "In generative models, we are interested in the joint probability of P of x comma y.",
    "start": "1590115",
    "end": "1595545"
  },
  {
    "text": "And in discriminative models, we only care about P of y given x. Yes, question?",
    "start": "1595545",
    "end": "1600600"
  },
  {
    "text": "[NOISE]. [inaudible]",
    "start": "1600600",
    "end": "1614910"
  },
  {
    "text": "Yes. So- so the question is, what's- what's the, uh, intuition about- about this? And, um, yeah so the intuition is- is uh,",
    "start": "1614910",
    "end": "1622695"
  },
  {
    "text": "exactly that, what's the- what's the uh, probability? Or you know what- probability is the wrong word in this case,",
    "start": "1622695",
    "end": "1628985"
  },
  {
    "text": "you know it's the likelihood. But if you just focus on this, then you know, the uh, it's- it's the probability of finding a specific x,",
    "start": "1628985",
    "end": "1636240"
  },
  {
    "text": "y pair if you just sample it from your population of examples. [inaudible]",
    "start": "1636240",
    "end": "1645540"
  },
  {
    "text": "So in- in the- the- um, the analogy, uh, in case of p of y given x is you're given an x.",
    "start": "1645540",
    "end": "1653924"
  },
  {
    "text": "Right. You had no- x was not a sample. You know, somebody gave it to you, right? And now for that given x,",
    "start": "1653925",
    "end": "1660105"
  },
  {
    "text": "what values of y could it possibly have, right? We- we- we kind of limit ourselves to a universe where we have only one example, right?",
    "start": "1660105",
    "end": "1668460"
  },
  {
    "text": "P of y given x. And given that x, you know, most of the times it could be say,",
    "start": "1668460",
    "end": "1673980"
  },
  {
    "text": "y equals 1, but sometimes it can be y equals 0, and that's your probability of y given x. [inaudible].",
    "start": "1673980",
    "end": "1700020"
  },
  {
    "text": "So the question in the- in the discriminative sense, why did this product makes sense? Yes. So, in the- in the discriminative case we had you know,",
    "start": "1700020",
    "end": "1706140"
  },
  {
    "text": "product I equals 1 to n p of y i given x i.",
    "start": "1706140",
    "end": "1713145"
  },
  {
    "text": "In this case, for each example we are- we are assuming x got decided somehow beyond our scope.",
    "start": "1713145",
    "end": "1720390"
  },
  {
    "text": "You know, somebody else decided x for us, for this example, right. And for- and there are n such",
    "start": "1720390",
    "end": "1726765"
  },
  {
    "text": "situations where x got decided to different values in each situation. And we are only interested in modeling y for those specific examples, right.",
    "start": "1726765",
    "end": "1738659"
  },
  {
    "text": "And so here we get one probability, here we get a different probability, another probability and the joint is just the product of them because of the independence assumption.",
    "start": "1738660",
    "end": "1745860"
  },
  {
    "text": "And similarly here, um, in this case, um, you know, the- the uh, joint is just a product of the individuals because of the IID assumption.",
    "start": "1745860",
    "end": "1755230"
  },
  {
    "text": "And uh, this we can write it as log of i equals 1 to n. I'll just break it down,",
    "start": "1755300",
    "end": "1764070"
  },
  {
    "text": "p of x_i given y_i times p of y_i.",
    "start": "1764070",
    "end": "1774000"
  },
  {
    "text": "And we know what p of x_i given y_i and p of y_i are, and they are right here.",
    "start": "1774000",
    "end": "1779774"
  },
  {
    "text": "So we plug them in here, right? Take these- take these definitions of x given y and p of y, plug them in here,",
    "start": "1779775",
    "end": "1789195"
  },
  {
    "text": "right, and solve, right?",
    "start": "1789195",
    "end": "1798794"
  },
  {
    "text": "Solve the- take the log-likelihood and maximize it with respect to the parameters, right?",
    "start": "1798795",
    "end": "1805305"
  },
  {
    "text": "And you get estimates for theta hat, phi hat, mu hat, sigma hat.",
    "start": "1805305",
    "end": "1816375"
  },
  {
    "text": "Once you solve for the gradient of this with respect to your parameters equal to 0, solve for them, and those are your estimates, right?",
    "start": "1816375",
    "end": "1823875"
  },
  {
    "text": "We've done this for- we've done this for linear regression. And you can follow something very similar here. Yes, question.",
    "start": "1823875",
    "end": "1831240"
  },
  {
    "text": "[inaudible]",
    "start": "1831240",
    "end": "1854250"
  },
  {
    "text": "So the, uh, so the question is, uh, in this case, why- why- why don't we just maximize the conditional of y given x, right?",
    "start": "1854250",
    "end": "1862215"
  },
  {
    "text": "And in order to make- to define what is y- uh, y given x, in this case.",
    "start": "1862215",
    "end": "1867299"
  },
  {
    "text": "Um, going by this data generative process, you know, y given x does not have a straight answer.",
    "start": "1867300",
    "end": "1873900"
  },
  {
    "text": "Right? So the- the- the data-generating process only tells us what is x given y, and that's normal, right?",
    "start": "1873900",
    "end": "1879225"
  },
  {
    "text": "So what's y given x here, you have to apply Bayes rule. Right? And in order to apply Bayes rule, you need to be able to compute the individual parts of the Bayes rule.",
    "start": "1879225",
    "end": "1888220"
  },
  {
    "text": "And in order to compute them, you need to have solved for the parameter values. And the way we solve for the parameter values is by doing maximum likelihood, right?",
    "start": "1888220",
    "end": "1899445"
  },
  {
    "text": "So maximize the joint likelihood. You get the- you get the uh,",
    "start": "1899445",
    "end": "1904455"
  },
  {
    "text": "parameter values which fits the data. And then you apply Bayes rule to get your conditional p of y given x. Yes, question.",
    "start": "1904455",
    "end": "1913830"
  },
  {
    "text": "[inaudible]",
    "start": "1913830",
    "end": "1919350"
  },
  {
    "text": "Yep. So uh, coming to that next, uh, was there another question? Yes. [inaudible]",
    "start": "1919350",
    "end": "1934350"
  },
  {
    "text": "So the question is, should I be dividing by p of, uh, x over here? [inaudible]",
    "start": "1934350",
    "end": "1940049"
  },
  {
    "text": "So- so, uh, so the question is, you know, do we need uh,",
    "start": "1940050",
    "end": "1945135"
  },
  {
    "text": "p of x here over here? And we need the p of x over here only to get p of y given x, right?",
    "start": "1945135",
    "end": "1953145"
  },
  {
    "text": "But in generative models, we are, you know, for p of x, y, you don't need a p of x in the denominator, right?",
    "start": "1953145",
    "end": "1959370"
  },
  {
    "text": "In generative modeling, this is our likelihood objective. Right? And- and- and that's- that's",
    "start": "1959370",
    "end": "1965130"
  },
  {
    "text": "the fundamental difference between discriminative models and generative models, right? In discriminative models, this will be p of y given x,",
    "start": "1965130",
    "end": "1971520"
  },
  {
    "text": "in which case, you technically need a p of x here. But for generative models we want to- we want to have the ability to generate a new,",
    "start": "1971520",
    "end": "1981420"
  },
  {
    "text": "you know, full dataset by just sampling from- from the model.",
    "start": "1981420",
    "end": "1986350"
  },
  {
    "text": "So, um, yeah, so um, moving on.",
    "start": "1986750",
    "end": "1993690"
  },
  {
    "text": "So take the derivative, set it equal to 0, solve for, um, solve for the different, uh, parameters.",
    "start": "1993690",
    "end": "2001535"
  },
  {
    "text": "And what we get is phi hat",
    "start": "2001535",
    "end": "2006510"
  },
  {
    "text": "equals 1 over n, i equals 1 to",
    "start": "2010750",
    "end": "2016385"
  },
  {
    "text": "n equal to 1.",
    "start": "2016385",
    "end": "2026340"
  },
  {
    "text": "And these are the closed form solutions for- if- if- if you,",
    "start": "2106530",
    "end": "2113020"
  },
  {
    "text": "ah, take this objective, plug in the, ah, density, treat it as a likelihood. Differentiate, take the partials with respect to",
    "start": "2113020",
    "end": "2120355"
  },
  {
    "text": "each parameter of this entire expression, set it- ah, set the partial is equal to 0 and solve for the parameters.",
    "start": "2120355",
    "end": "2127433"
  },
  {
    "text": "These are the so- so- solutions you get. And this is what is there in your homework. Right? Yes, question?",
    "start": "2127434",
    "end": "2135040"
  },
  {
    "text": "[inaudible]. So- ah, so the question is, okay.",
    "start": "2135040",
    "end": "2140559"
  },
  {
    "text": "So, ah, let- let's go deeper into what- what each of these terms mean. Over here. We are using this- this notation, right?",
    "start": "2140560",
    "end": "2154165"
  },
  {
    "text": "So this is, ah, you know, supposing there's some expression here, this is basically equal to 1 if expression is true and 0,",
    "start": "2154165",
    "end": "2168835"
  },
  {
    "text": "you know, if- if it's false, right? So each of these, um- um,",
    "start": "2168835",
    "end": "2175795"
  },
  {
    "text": "so y^i is 1 for a few examples, 0 for a few other examples.",
    "start": "2175795",
    "end": "2181750"
  },
  {
    "text": "So here we're basically counting the number of examples for which y^i equals 1,",
    "start": "2181750",
    "end": "2186980"
  },
  {
    "text": "and dividing it by n. So what fraction of your examples are labeled 1 is what,",
    "start": "2186980",
    "end": "2192930"
  },
  {
    "text": "ah, ah, ah, ah, Phi hat will evaluate to. Over here, for Mu_0,",
    "start": "2192930",
    "end": "2199965"
  },
  {
    "text": "we are summing over those x^i's for which y^i equals 0, right?",
    "start": "2199965",
    "end": "2208915"
  },
  {
    "text": "Because, um, this indicator function- so this notation is called an indicator variable.",
    "start": "2208915",
    "end": "2215305"
  },
  {
    "text": "So this indicator function will evaluate to- this indicator function will",
    "start": "2215305",
    "end": "2224605"
  },
  {
    "text": "evaluate to 1 only for those examples for which y^i equals 0. So you're only summing over x^i's for which y_i equals 0,",
    "start": "2224605",
    "end": "2232435"
  },
  {
    "text": "and you're- here you're counting the number of examples for which y^i equals 0, right? And similarly here you have, ah,",
    "start": "2232435",
    "end": "2238735"
  },
  {
    "text": "you- you're doing the same for, ah, Mu_1. Now what is- what's- ah, ah, what's the deal with Sigma here?",
    "start": "2238735",
    "end": "2244480"
  },
  {
    "text": "There are no indicator variables here. But what we observe here is that for each example, we are",
    "start": "2244480",
    "end": "2249869"
  },
  {
    "text": "subtracting it from the mean that is specific to that example. So we have two means,",
    "start": "2249870",
    "end": "2255580"
  },
  {
    "text": "one mean for y^i equals 0 and another mean for y^i equals 1.",
    "start": "2255580",
    "end": "2261040"
  },
  {
    "text": "What this means is, subtract from x the mean which belongs to its y^i,",
    "start": "2261040",
    "end": "2268510"
  },
  {
    "text": "does the relation make sense? Subtract from x the mean that corresponds to its label, okay?",
    "start": "2270060",
    "end": "2280885"
  },
  {
    "text": "And, ah, this is basically, you know, um, um, these are the solutions, and in your homework you will be deriving it",
    "start": "2280885",
    "end": "2287710"
  },
  {
    "text": "and- and proving that these are actually the solutions, um, which is basically just calculus. It's, um, the- the- the- the computation can be a little,",
    "start": "2287710",
    "end": "2296440"
  },
  {
    "text": "um, verbose but, you know, it's- it's pretty straightforward. And, ah, yeah, so these are- these are the, ah,",
    "start": "2296440",
    "end": "2304900"
  },
  {
    "text": "um, MLE, ah, the maximum likelihood estimates of, ah, the Gaussian discriminant analysis.",
    "start": "2304900",
    "end": "2310000"
  },
  {
    "text": "So we start with defining a model. Get the corresponding density,",
    "start": "2310000",
    "end": "2316840"
  },
  {
    "text": "so the probability distributions. Identify what our parameters are, right?",
    "start": "2316840",
    "end": "2322555"
  },
  {
    "text": "Write out the log-likelihood objective as the product of the joints, because that's what we model in- in a generative models,",
    "start": "2322555",
    "end": "2329710"
  },
  {
    "text": "we model the joint distribution. And break it down into components. And the way we break down, ah, uh,",
    "start": "2329710",
    "end": "2336790"
  },
  {
    "text": "broke this down as p of x given y times p of y, was because that is the factorization that we readily have at our hand.",
    "start": "2336790",
    "end": "2345954"
  },
  {
    "text": "Plug-in the appropriate, ah, ah, densities for p of x given defined p of y in this, uh, likelihood objective.",
    "start": "2345955",
    "end": "2355165"
  },
  {
    "text": "And take the derivative of the entire, uh, likelihood objective with respect to the parameters,",
    "start": "2355165",
    "end": "2360984"
  },
  {
    "text": "set them equal to 0, solve for them and you get these. Yes, question. Shouldn't that be Sigma hat?",
    "start": "2360985",
    "end": "2367100"
  },
  {
    "text": "Yes, this should be Sigma hat. Thank you. Yes, question. [inaudible] let's say if",
    "start": "2367100",
    "end": "2378690"
  },
  {
    "text": "we don't like indicator functions can we just write that as summation over [inaudible] you know?",
    "start": "2378690",
    "end": "2384690"
  },
  {
    "text": "[NOISE] Yes. So if we don't like indicator functions, you could, for example, write this as, um.",
    "start": "2384690",
    "end": "2391200"
  },
  {
    "text": "[inaudible]. Yeah, yeah, that's just equivalent notation i such that y_i equals 1,",
    "start": "2391200",
    "end": "2402815"
  },
  {
    "text": "x_i over summation over i such that y_i equals 1 times 1.",
    "start": "2402815",
    "end": "2413150"
  },
  {
    "text": "So this is- this just another notation which calculates or results in the same- same value, right?",
    "start": "2413700",
    "end": "2421555"
  },
  {
    "text": "Here we are, ah, explicitly multiplying some of the x's by 0,",
    "start": "2421555",
    "end": "2426940"
  },
  {
    "text": "and here we are just skipping over them in the index. Both are equal- equal notations. Yes, question?",
    "start": "2426940",
    "end": "2434829"
  },
  {
    "text": "[inaudible]",
    "start": "2434830",
    "end": "2441880"
  },
  {
    "text": "So the question is, ah, can we have, um, normal for one class, Poisson for another?",
    "start": "2441880",
    "end": "2448940"
  },
  {
    "text": "I mean, um, you technically can, you can define your model anyway you want,",
    "start": "2449490",
    "end": "2455290"
  },
  {
    "text": "the question is, is that meaningful, right? Um, a Poisson is basically counts,",
    "start": "2455290",
    "end": "2461650"
  },
  {
    "text": "so you know- you know, and a normal is- is real valued. So if you already know that some of your data is- or counts belong to",
    "start": "2461650",
    "end": "2468099"
  },
  {
    "text": "one class and the data that take real values belong to another class then, you know, you kind of already, you know,",
    "start": "2468100",
    "end": "2473890"
  },
  {
    "text": "why- why- why go through this exercise in a way. Ah, but, you know, from- from- from a statistical point of view,",
    "start": "2473890",
    "end": "2479350"
  },
  {
    "text": "you can absolutely do that, um, except, um, you know, those x's that have Poisson must be count valued or- or- or integers,",
    "start": "2479350",
    "end": "2490270"
  },
  {
    "text": "positive integers, and those x that belong to the normal must be real valued. So if you have a way to kind of,",
    "start": "2490270",
    "end": "2496270"
  },
  {
    "text": "you know, make your math compatible with, you know, some x's having a different data type versus,",
    "start": "2496270",
    "end": "2501549"
  },
  {
    "text": "ah, other x's then yes, you can- you can absolutely do that. Yes, question.",
    "start": "2501550",
    "end": "2506680"
  },
  {
    "text": "[inaudible] as a concave function or we just take it as prior knowledge?",
    "start": "2506680",
    "end": "2514569"
  },
  {
    "text": "So the question is, um, in this case, will- will this be a- a- a concave, uh, function?",
    "start": "2514570",
    "end": "2521710"
  },
  {
    "text": "Uh, in this particular case, I believe it is, um, but in general it need not be, right?",
    "start": "2521710",
    "end": "2528190"
  },
  {
    "text": "In general it need not be, but I think in this particular case it is. [inaudible].",
    "start": "2528190",
    "end": "2539530"
  },
  {
    "text": "Yeah, so in- in cases when- ah, so the question is, you know, is this- is this concave and how can we do this without knowing?",
    "start": "2539530",
    "end": "2545694"
  },
  {
    "text": "In this case it is con- concave, and so this works. Ah, in general, uh,",
    "start": "2545695",
    "end": "2551680"
  },
  {
    "text": "just taking the derivative and setting, uh, it equal to 0, ah, may not work, but, um, in those cases where, you know, um,",
    "start": "2551680",
    "end": "2559375"
  },
  {
    "text": "so if- if you- if you, ah, uh, er, it so happens that for cases where closed form solutions exist,",
    "start": "2559375",
    "end": "2569320"
  },
  {
    "text": "they tend to be concave, um, and for cases where you don't have closed form solutions,",
    "start": "2569320",
    "end": "2574855"
  },
  {
    "text": "you tend to do gradient ascent. So you can- you can- you can optimize this using gradient ascent.",
    "start": "2574855",
    "end": "2580360"
  },
  {
    "text": "And in- in- in that case, you know, if you're doing gradient ascent, um, you- your- you will reach some kind of a local maximum. Yes, question.",
    "start": "2580360",
    "end": "2592630"
  },
  {
    "text": "[inaudible].",
    "start": "2592630",
    "end": "2602440"
  },
  {
    "text": "Yes, we are coming to that next. Okay. All right, so let's move on.",
    "start": "2602440",
    "end": "2608570"
  },
  {
    "text": "So here we see that, um, by- by following the above steps we- we calculate the, ah,",
    "start": "2611220",
    "end": "2619735"
  },
  {
    "text": "parameter values for the joint and using the recipe given here, you can construct, um, a posterior distribution for y given x using, you know,",
    "start": "2619735",
    "end": "2628905"
  },
  {
    "text": "a plug-in p of x given y, which is Gaussian, p of y, Bernoulli, Gaussian, Bernoulli, Gaussian, Bernoulli, and you will get an expression for p of y given x.",
    "start": "2628905",
    "end": "2639214"
  },
  {
    "text": "And it so happens, ah, that for the case of Gaussian discriminant analysis with the ha- with",
    "start": "2639215",
    "end": "2645610"
  },
  {
    "text": "the- with the shared Sigma variables, shared covariance, we see that, ah,",
    "start": "2645610",
    "end": "2653755"
  },
  {
    "text": "the posterior distribution takes the form of the logistic- ah,",
    "start": "2653755",
    "end": "2659529"
  },
  {
    "text": "ah, ah, of the logistic regression. What do I mean by that? Let me write it out to make it more concrete.",
    "start": "2659530",
    "end": "2666859"
  },
  {
    "text": "Right. So if you go through this exercise of calculating the posterior distribution for Gaussian discriminant analysis,",
    "start": "2688240",
    "end": "2695480"
  },
  {
    "text": "we get p of y equals 1 given x can be",
    "start": "2695480",
    "end": "2703160"
  },
  {
    "text": "written in the form 1 over 1 plus exp of minus Theta transpose x,",
    "start": "2703160",
    "end": "2711460"
  },
  {
    "text": "where Theta depends only on Mu naught,",
    "start": "2711460",
    "end": "2721430"
  },
  {
    "text": "Mu_1, Phi, and Sigma. And this is also in your homework where you need to- where- where you show that, uh,",
    "start": "2721430",
    "end": "2729230"
  },
  {
    "text": "given, um, a Gaussian discriminant analysis, um, model, you can represent the posterior distribution of y given",
    "start": "2729230",
    "end": "2737030"
  },
  {
    "text": "x can be represented in this form where Theta is- um,",
    "start": "2737030",
    "end": "2742580"
  },
  {
    "text": "um, Theta has- Theta can be represented only with,",
    "start": "2742580",
    "end": "2748235"
  },
  {
    "text": "uh, Mu naught, Mu_1, Sigma, and- and Phi. Right. So this means the, um,",
    "start": "2748235",
    "end": "2755075"
  },
  {
    "text": "intuition to have here is [NOISE] let's assume we have a Mu naught here,",
    "start": "2755075",
    "end": "2764210"
  },
  {
    "text": "so here, um, this- this is x1, xd.",
    "start": "2764210",
    "end": "2771920"
  },
  {
    "text": "[NOISE] Let's say this is Mu_0, Mu_1, [NOISE] we'll use a different color",
    "start": "2771920",
    "end": "2781910"
  },
  {
    "text": "for this, [NOISE] Mu_1.",
    "start": "2781910",
    "end": "2787895"
  },
  {
    "text": "And they share- so we have two different- two different, um, means.",
    "start": "2787895",
    "end": "2794240"
  },
  {
    "text": "Each mean is specific to each class, a blue class and a red class. And the- both of them share the same covariance structure, right?",
    "start": "2794240",
    "end": "2802490"
  },
  {
    "text": "So if- if we write out the- um, um, the contour lines for some of- for- for,",
    "start": "2802490",
    "end": "2808775"
  },
  {
    "text": "um, these examples, they might look like this.",
    "start": "2808775",
    "end": "2811799"
  },
  {
    "text": "These are the contour plot for the density of class 1, and these are the contour plots",
    "start": "2819460",
    "end": "2828515"
  },
  {
    "text": "for the density of-",
    "start": "2828515",
    "end": "2837589"
  },
  {
    "text": "of, um, the other class. And our examples are sampled from these Gaussians.",
    "start": "2837590",
    "end": "2843875"
  },
  {
    "text": "So let's say we have- [NOISE]",
    "start": "2843875",
    "end": "2866750"
  },
  {
    "text": "similarly for the red class, here are some examples. [NOISE]",
    "start": "2866750",
    "end": "2880610"
  },
  {
    "text": "Right. So we start with just the- with just the data points, and we fit a GDA model.",
    "start": "2880610",
    "end": "2888380"
  },
  {
    "text": "And your GDA model would learn these to be your Mu's or- or values sufficiently close to be the two Mu's and the two Sigmas,",
    "start": "2888380",
    "end": "2896405"
  },
  {
    "text": "um, and- and the common shared, uh, covariance structure. And, um, Phi would just be the- the fraction of examples in the red and fraction of- uh,",
    "start": "2896405",
    "end": "2905900"
  },
  {
    "text": "to the total fraction, for example. And the posterior distribution,",
    "start": "2905900",
    "end": "2911390"
  },
  {
    "text": "which you can think of as the- as the- um,",
    "start": "2911390",
    "end": "2916744"
  },
  {
    "text": "or the ar- uh, separating hyperplane, which will- which corresponds to p of [NOISE] y equals 1 given x equals 0.5,",
    "start": "2916745",
    "end": "2928309"
  },
  {
    "text": "which is also equal to p of y equals 0 given x. So, um, the set of all points, x points,",
    "start": "2928310",
    "end": "2936380"
  },
  {
    "text": "for which the y equals one class and y equals 0 class assign, you know,",
    "start": "2936380",
    "end": "2943130"
  },
  {
    "text": "uh, where the point has equal probability of belonging to the y equals 1 class or the y equals 0 class would be something like this.",
    "start": "2943130",
    "end": "2952770"
  },
  {
    "text": "Right. And the claim here is that for any Gaussian discriminant analysis model that share the same Sigma,",
    "start": "2954730",
    "end": "2964640"
  },
  {
    "text": "the posterior distribution of y given x can be represented as",
    "start": "2964640",
    "end": "2972079"
  },
  {
    "text": "a logistic regression model where the Thetas only depend on- on the,",
    "start": "2972080",
    "end": "2979855"
  },
  {
    "text": "uh, parameters of the model. So which means GDA, any GDA, can be written as a logistic regression model.",
    "start": "2979855",
    "end": "2990170"
  },
  {
    "text": "Now, the reason why the two, um,",
    "start": "2995910",
    "end": "3000974"
  },
  {
    "text": "covariance structures or the two- um,",
    "start": "3000975",
    "end": "3004810"
  },
  {
    "text": "the two covariance structures are similarly oriented and similar in shape and size is because they,",
    "start": "3006360",
    "end": "3013690"
  },
  {
    "text": "you know, they- they co- they come off the same covariance matrix. And these, um, ellipsoids are- are- are- um, the shape,",
    "start": "3013690",
    "end": "3022450"
  },
  {
    "text": "and- and orientation, and size are completely determined by the- the covariance matrix.",
    "start": "3022450",
    "end": "3029095"
  },
  {
    "text": "And the principal axes of this covariance matrix are basically the eigenvectors of this covariance matrix,",
    "start": "3029095",
    "end": "3038215"
  },
  {
    "text": "and the eigenvalues, um, tell you how- how, uh, spread they are in each of those,",
    "start": "3038215",
    "end": "3044380"
  },
  {
    "text": "uh, axes. Yes, question. [inaudible]",
    "start": "3044380",
    "end": "3051490"
  },
  {
    "text": "Yes, the Phi- Phi also decides where the hyperplane will reside, yes. The hyperplane will def- uh, will depend on Phi.",
    "start": "3051490",
    "end": "3057835"
  },
  {
    "text": "It's gonna depend on Phi, Mu naught, Mu_1, and- and, uh, Sigma all the- all the way.",
    "start": "3057835",
    "end": "3063190"
  },
  {
    "text": "[inaudible] Exactly. So- so this Theta over here is- is vector value.",
    "start": "3063190",
    "end": "3071200"
  },
  {
    "text": "It dep- you know, um, and the- the- uh, the value of- of, uh, uh,",
    "start": "3071200",
    "end": "3077125"
  },
  {
    "text": "Theta naught, Theta_1, Theta_2 will depend on- will be a function of these parameters. Yes, question.",
    "start": "3077125",
    "end": "3083530"
  },
  {
    "text": "[inaudible] Yeah.",
    "start": "3083530",
    "end": "3090785"
  },
  {
    "text": "So",
    "start": "3090785",
    "end": "3098410"
  },
  {
    "text": "p of y equals 1 given x, [inaudible].",
    "start": "3101430",
    "end": "3107920"
  },
  {
    "text": "Actually, you're right. So this should- this should not be- yeah,",
    "start": "3107920",
    "end": "3114670"
  },
  {
    "text": "it shouldn't be- uh, it should just be equal to, it shouldn't be 0.5. You're right. Thank you.",
    "start": "3114670",
    "end": "3123190"
  },
  {
    "text": "[inaudible] So it- it's a line because you can represent it in",
    "start": "3123190",
    "end": "3129190"
  },
  {
    "text": "this form and this is your logistic regression and- and, you know. Yes, there's a question.",
    "start": "3129190",
    "end": "3135339"
  },
  {
    "text": "[inaudible]",
    "start": "3135340",
    "end": "3140680"
  },
  {
    "text": "So the question is what if they have different covariance matrices? We made an assumption that, um, you're gonna- you're gonna assume that both the classes",
    "start": "3140680",
    "end": "3147579"
  },
  {
    "text": "are gonna share the same covariance matrix, right? And, uh, if- if the two classes don't have the same covariance matrix,",
    "start": "3147580",
    "end": "3154630"
  },
  {
    "text": "um, you can think of it like this. Right. So one class will- let's say is concentrated over here.",
    "start": "3154630",
    "end": "3162530"
  },
  {
    "text": "And, um, which- what did I write here?",
    "start": "3163410",
    "end": "3170680"
  },
  {
    "text": "So this- this, I- I completely messed up, sorry. So thi- this, uh, the- the line that corresponds to p of x given y equals 0,",
    "start": "3170680",
    "end": "3181840"
  },
  {
    "text": "equals p of x given y equals 1, so. Right? Does that make sense?",
    "start": "3181840",
    "end": "3187660"
  },
  {
    "text": "The- the set of all points which have the same density under class 1 and same eq- and- and,",
    "start": "3187660",
    "end": "3194560"
  },
  {
    "text": "uh, the same density on the class 0, right, that would be a- a straight line in this case.",
    "start": "3194560",
    "end": "3200200"
  },
  {
    "text": "Um, in case of- uh, if- if you have two different covariances,",
    "start": "3200200",
    "end": "3205750"
  },
  {
    "text": "your examples may look like this, [NOISE] right?",
    "start": "3205750",
    "end": "3215515"
  },
  {
    "text": "So you have some examples that are, say, more concentrated here, and other class examples are more dispersed, right?",
    "start": "3215515",
    "end": "3222025"
  },
  {
    "text": "And in this case, the set of all points that have the same probability under class 0 and class 1 would- would actually look something like,",
    "start": "3222025",
    "end": "3233000"
  },
  {
    "text": "right, instead of a straight line. So these would be the set of all points which have",
    "start": "3235850",
    "end": "3242640"
  },
  {
    "text": "the same probability under class 0 or under class 1. If both of them share the same covariance structure,",
    "start": "3242640",
    "end": "3250860"
  },
  {
    "text": "then this line will be a straight line. And if- if they are- if they have unequal covariances,",
    "start": "3250860",
    "end": "3258090"
  },
  {
    "text": "then it would be- it'll- it'll actually- it can actually be represented as a quadratic instead of a linear. [inaudible]",
    "start": "3258090",
    "end": "3268450"
  },
  {
    "text": "Yes, you can think of it as logistic regression with polynomial features, yeah. That's one way to think of it. Yes.",
    "start": "3268450",
    "end": "3274390"
  },
  {
    "text": "[inaudible]",
    "start": "3274390",
    "end": "3289299"
  },
  {
    "text": "Uh, I- I- I would encourage you to plot it and you'll see that, you know, it- it, it's- it's going to curve towards the- the one which is more concentrated.",
    "start": "3289300",
    "end": "3296875"
  },
  {
    "text": "Now just- just- just, you know, take two covariant structures and plot the set of points which are- which have",
    "start": "3296875",
    "end": "3301944"
  },
  {
    "text": "equal probability under both classes and it's going to come like this. Any other questions? Okay. So, um,",
    "start": "3301945",
    "end": "3311200"
  },
  {
    "text": "a GDA model- for a- a- a- a given GDA model will uniquely determine",
    "start": "3311200",
    "end": "3318475"
  },
  {
    "text": "a logistic regression model for its posterior distribution. Whereas the converse is not true.",
    "start": "3318475",
    "end": "3324355"
  },
  {
    "text": "If you have a logistic regression model, it may not be the posterior distribution of a GDA model, right?",
    "start": "3324355",
    "end": "3331210"
  },
  {
    "text": "Uh, it- it- it could be, it could- it need not be a posterior distribution of any model,",
    "start": "3331210",
    "end": "3337480"
  },
  {
    "text": "or it could be a posterior distribution of some other model. Whereas if you have a GDA model, its posterior is always a logistic regression model, right?",
    "start": "3337480",
    "end": "3345430"
  },
  {
    "text": "And the- the, um, the GDA model is making a stronger assumption in the sense that",
    "start": "3345430",
    "end": "3356920"
  },
  {
    "text": "it is making a stronger assumption that your data is actually distributed according to Gaussian distributions.",
    "start": "3356920",
    "end": "3364060"
  },
  {
    "text": "The two classes actually have a Gaussian distributions with the shared covariant structure, it's making that assumption. Now, if that assumption is true,",
    "start": "3364060",
    "end": "3371920"
  },
  {
    "text": "then your GDA model will tend to be more efficient or more asymptotically efficient or more sample efficient compared to logistic regression, okay?",
    "start": "3371920",
    "end": "3380575"
  },
  {
    "text": "When- when the assumption holds true, then GDA is a better model. You probably need a lot fewer examples, uh,",
    "start": "3380575",
    "end": "3386965"
  },
  {
    "text": "to get some level of accuracy, um, compared to a logistic regression, um.",
    "start": "3386965",
    "end": "3392230"
  },
  {
    "text": "However, logistic regression does not make any assumption and tends to be more robust.",
    "start": "3392230",
    "end": "3399025"
  },
  {
    "text": "It might need a little more data, um, in cases where, um,",
    "start": "3399025",
    "end": "3404065"
  },
  {
    "text": "the assumptions are true, but it tends to work well even when, you know, the assumptions are- are- are not met.",
    "start": "3404065",
    "end": "3410260"
  },
  {
    "text": "It's a pretty robust algorithm. And in practice, you know, um, almost always logistic regression should probably be our first choice of,",
    "start": "3410260",
    "end": "3420365"
  },
  {
    "text": "um, first choice of algorithm to try out on a given data set, uh, because, you know, um, uh, assumptions may or may not hold true.",
    "start": "3420365",
    "end": "3427910"
  },
  {
    "text": "In cases where the assumptions do hold true, uh, GDA will be slightly more efficient in terms of,",
    "start": "3427910",
    "end": "3434175"
  },
  {
    "text": "um, the number of examples required. [NOISE] Right.",
    "start": "3434175",
    "end": "3439435"
  },
  {
    "text": "That's- that's, uh, and- and in your homework you will, uh, we'll also basically see, um, you know, um.",
    "start": "3439435",
    "end": "3446585"
  },
  {
    "text": "You'll- you'll be seeing, you know, this phenomenon in your homework as well where- when, when you- when you, um, um, plot your data,",
    "start": "3446585",
    "end": "3453415"
  },
  {
    "text": "some of which, you know, may be, uh, um, distributed according to Gaussian, some of which are not and then see your, you know,",
    "start": "3453415",
    "end": "3459025"
  },
  {
    "text": "the model performance between GDA and logistic regression on- on both those, uh, situations. All, uh, you know, all that is part of your homework question 1.",
    "start": "3459025",
    "end": "3466435"
  },
  {
    "text": "Any questions before we move on to naive Bayes. Yes.",
    "start": "3466435",
    "end": "3472885"
  },
  {
    "text": "Under what assumptions can you map logistic regression back to GDA? So under what, um,",
    "start": "3472885",
    "end": "3479710"
  },
  {
    "text": "assumptions can you map logistic regression back to GDA? The extra assumption is that your x's are,",
    "start": "3479710",
    "end": "3486025"
  },
  {
    "text": "um, are coming from a Gaussian and- and having same covariance. [NOISE] All right, let's move on to naive Bayes.",
    "start": "3486025",
    "end": "3496450"
  },
  {
    "text": "[NOISE]",
    "start": "3496450",
    "end": "3509910"
  },
  {
    "text": "[inaudible] The center line has- okay. The center line has to be linear. Yes, so if the covariances are- are the same, then the, uh,",
    "start": "3509910",
    "end": "3516775"
  },
  {
    "text": "the line that has equal [NOISE] probability under the two classes will be a straight line. Yes. [inaudible].",
    "start": "3516775",
    "end": "3531820"
  },
  {
    "text": "So when you solve for 1 over 1 plus E to the minus Theta transpose x, uh,",
    "start": "3531820",
    "end": "3536875"
  },
  {
    "text": "equal to 0.5, you will see that Theta transpose x must be equal to 0, so that will be a, that would be a line.",
    "start": "3536875",
    "end": "3543070"
  },
  {
    "text": "[NOISE]",
    "start": "3543070",
    "end": "3550070"
  },
  {
    "text": "All right, naive Bayes. So, uh, in GDA,",
    "start": "3558170",
    "end": "3563280"
  },
  {
    "text": "we saw that, um, the x's are real valued. They live in some d-dimensional real space.",
    "start": "3563280",
    "end": "3570335"
  },
  {
    "text": "But that's generally not how all, [NOISE] you know, um, that does not account for all the different ways in which we count the data.",
    "start": "3570335",
    "end": "3577960"
  },
  {
    "text": "For example, our data may be, you know, text messages, emails and we- we may wanna build a spam classifier for whether a given,",
    "start": "3577960",
    "end": "3586555"
  },
  {
    "text": "uh, uh, text message or email is- is, uh, spam or not. In those cases, you know,",
    "start": "3586555",
    "end": "3591970"
  },
  {
    "text": "your inputs are- are basically strings. They're- they're, you know, it's- it's a- it's a list of words and that doesn't fit well as,",
    "start": "3591970",
    "end": "3598090"
  },
  {
    "text": "you know, as- as- as a- as a multivariate Gaussian. And so when our, uh,",
    "start": "3598090",
    "end": "3604880"
  },
  {
    "text": "x's are discrete value, [NOISE] discrete, that's when we use naive Bayes.",
    "start": "3605880",
    "end": "3616369"
  },
  {
    "text": "In order to use naive Bayes, x is discrete value.",
    "start": "3620250",
    "end": "3625450"
  },
  {
    "text": "Most commonly we use it for text classification. [NOISE] Okay?",
    "start": "3625450",
    "end": "3629240"
  },
  {
    "text": "For example spam filters. [NOISE] That's going to be the running example today.",
    "start": "3632100",
    "end": "3640375"
  },
  {
    "text": "And this would be a good time to review conditional independence, a concept in probability, okay?",
    "start": "3640375",
    "end": "3653829"
  },
  {
    "text": "[NOISE] So conditional independence [NOISE] means, um, given two random variables, x_j and x_k.",
    "start": "3653830",
    "end": "3664165"
  },
  {
    "text": "We say they are independent if probability of x_j given x_k equals probability of x_j, right?",
    "start": "3664165",
    "end": "3675475"
  },
  {
    "text": "This is the definition of independence. [NOISE] We say they are,",
    "start": "3675475",
    "end": "3680740"
  },
  {
    "text": "uh, so this is independence. We say they are co-conditionally independent when conditioned on some new random variable,",
    "start": "3680740",
    "end": "3690460"
  },
  {
    "text": "say y, let's say y, p_ x_j given",
    "start": "3690460",
    "end": "3695785"
  },
  {
    "text": "x_k,y equals p_x_jy.",
    "start": "3695785",
    "end": "3703720"
  },
  {
    "text": "This is conditional independence",
    "start": "3703720",
    "end": "3712060"
  },
  {
    "text": "when conditioned on y, right?",
    "start": "3712060",
    "end": "3717550"
  },
  {
    "text": "Now, can anybody tell me, does independence imply conditional independence?",
    "start": "3717550",
    "end": "3724490"
  },
  {
    "text": "Does conditional independence imply independence? No. And what is the answer to the first one,",
    "start": "3725160",
    "end": "3731170"
  },
  {
    "text": "does independence imply conditional independence?",
    "start": "3731170",
    "end": "3733940"
  },
  {
    "text": "So does independence imply conditional independence and does conditional independence imply independence?",
    "start": "3740190",
    "end": "3746200"
  },
  {
    "text": "Both don't. Yeah, that's correct. So, both- the answer is no to both, right? In- in- in general, conditional independence, um,",
    "start": "3746200",
    "end": "3754494"
  },
  {
    "text": "your random variables could be conditionally independent that doesn't say anything of whether they are independent or not.",
    "start": "3754495",
    "end": "3761050"
  },
  {
    "text": "There may not be conditionally independent and even that does not say anything about where they are independent- independent or not.",
    "start": "3761050",
    "end": "3766885"
  },
  {
    "text": "Conditional independence, uh, assumes you're- you're, uh, uh, um, assumes that both sides are conditioned on a third variable like y.",
    "start": "3766885",
    "end": "3777700"
  },
  {
    "text": "And in this case we are gonna, um, we're gonna make use of conditional independence. Um, and- and this just a- this just a refresher for what,",
    "start": "3777700",
    "end": "3786235"
  },
  {
    "text": "uh, conditional independence is. Yes, question. Okay. So the first,",
    "start": "3786235",
    "end": "3792460"
  },
  {
    "text": "in naive Bayes, we are going to consider two different kind of event models. The first thing- the first model that we are going",
    "start": "3792460",
    "end": "3797950"
  },
  {
    "text": "to consider is called the Bernoulli event model. [NOISE]",
    "start": "3797950",
    "end": "3809350"
  },
  {
    "text": "So in the Bernoulli event model, [NOISE] the, uh, mental picture to have is, let's say we have a text message,",
    "start": "3809350",
    "end": "3816085"
  },
  {
    "text": "uh, that says, you know, \"Buy our lottery.\" [NOISE] Right. [NOISE] So this is,",
    "start": "3816085",
    "end": "3826720"
  },
  {
    "text": "uh, a sequence of alphabets, right? And we want to convert this into some kind of,",
    "start": "3826720",
    "end": "3833845"
  },
  {
    "text": "you know, um, uh, some kind of a vector, some- and the way we do that is come up with",
    "start": "3833845",
    "end": "3839910"
  },
  {
    "text": "what's called a multi-hot representation where imagine you have a long vector where each component of",
    "start": "3839910",
    "end": "3849670"
  },
  {
    "text": "the vector is associated with some word in the dictionary, right?",
    "start": "3849670",
    "end": "3855055"
  },
  {
    "text": "So let's say the first word- first component belongs to the word a. And, uh, if- if- if you go through [NOISE] some- some dictionary,",
    "start": "3855055",
    "end": "3863650"
  },
  {
    "text": "here we're gonna assume English, but, you know, um, the concepts apply for anything. So second word in the dictionary would be say,",
    "start": "3863650",
    "end": "3869515"
  },
  {
    "text": "aardvark [NOISE] and the third is aardwolf, so on.",
    "start": "3869515",
    "end": "3878920"
  },
  {
    "text": "Then the word buy is gonna show up in your dictionary, uh, lottery,",
    "start": "3878920",
    "end": "3885460"
  },
  {
    "text": "[NOISE] our and say",
    "start": "3885460",
    "end": "3891990"
  },
  {
    "text": "some last words, zymurgy, all right.",
    "start": "3891990",
    "end": "3897390"
  },
  {
    "text": "So- so- some word, right? And first we're going to convert our sequence of words into a fixed length vector.",
    "start": "3897390",
    "end": "3905840"
  },
  {
    "text": "Where you have 0s in places for those words that do not appear in the message, right?",
    "start": "3905840",
    "end": "3912850"
  },
  {
    "text": "And 1 in places where those words do appear in- in the message.",
    "start": "3912850",
    "end": "3918340"
  },
  {
    "text": "[NOISE]",
    "start": "3918340",
    "end": "3930970"
  },
  {
    "text": "Okay. So we're gonna to assume a fixed set of words which is called the vocabulary,",
    "start": "3930970",
    "end": "3938800"
  },
  {
    "text": "right, and we will- we assume this vocabulary,",
    "start": "3940380",
    "end": "3946900"
  },
  {
    "text": "uh, for each word to have a fixed location in this long vector. Okay. And given a new- new,",
    "start": "3946900",
    "end": "3953515"
  },
  {
    "text": "um, input text message, we will convert it into a fixed-length vector where we have 0s in",
    "start": "3953515",
    "end": "3961630"
  },
  {
    "text": "places where word- the word does not appear in the text message and 1 in places where the word appears in a text message.",
    "start": "3961630",
    "end": "3967390"
  },
  {
    "text": "It doesn't matter how many times it appears, the question is, does it appear once or more. Yes, question?",
    "start": "3967390",
    "end": "3973599"
  },
  {
    "text": "[inaudible]. Oh, We- we're gonna- we're gonna talk about it, right?",
    "start": "3973600",
    "end": "3981880"
  },
  {
    "text": "Um, so here, um, x is- so if our vocabulary has d dimensions, okay,",
    "start": "3981880",
    "end": "3993670"
  },
  {
    "text": "so i- i- if- if this count is d, then x belongs to 0, 1 to the power d. This is how we write,",
    "start": "3993670",
    "end": "4002745"
  },
  {
    "text": "um, a d dimensional vector where each element is either 0 or 1, right?",
    "start": "4002745",
    "end": "4012325"
  },
  {
    "text": "And any given x_j is either 0 or 1, right?",
    "start": "4012325",
    "end": "4019170"
  },
  {
    "text": "and you make the assumption that pi of x_j [NOISE] and we write out our model like this.",
    "start": "4019170",
    "end": "4031125"
  },
  {
    "text": "So this is our model. Just as the way in case of the Gaussian discriminate analysis,",
    "start": "4031125",
    "end": "4037364"
  },
  {
    "text": "we- we had a hierarchical model that described how the data was generated. Similarly, we're gonna have a model here,",
    "start": "4037364",
    "end": "4043485"
  },
  {
    "text": "pi of y equals 1 equals phi_",
    "start": "4043485",
    "end": "4049875"
  },
  {
    "text": "y and p of x_j given y equals 0,",
    "start": "4049875",
    "end": "4057855"
  },
  {
    "text": "equals phi_ j given y equals 0. [NOISE]",
    "start": "4057855",
    "end": "4111120"
  },
  {
    "text": "Okay. So we're gonna to assume that there's one Bernoulli distribution parameterized by phi_y,",
    "start": "4111120",
    "end": "4118515"
  },
  {
    "text": "our phi_ y is one value between 0 and 1, and that's the parameter of this Bernoulli distribution,",
    "start": "4118515",
    "end": "4124665"
  },
  {
    "text": "which tells us what fraction of our overall text messages are spammy versus not, right?",
    "start": "4124665",
    "end": "4131609"
  },
  {
    "text": "It's just the class prior. And then we have two sets or two collections of Bernoulli variables;",
    "start": "4131610",
    "end": "4139859"
  },
  {
    "text": "one set for the class y equals 0. So for y equals 0,",
    "start": "4139860",
    "end": "4145275"
  },
  {
    "text": "we have a phi_j for each j, from 1 to d, which tells us what's the probability that x_ j equals 1 when the class is 0.",
    "start": "4145275",
    "end": "4156660"
  },
  {
    "text": "And another set of Bernoulli variables, a full set of Bernoulli variables corresponding to y equals 1,",
    "start": "4156660",
    "end": "4163245"
  },
  {
    "text": "which tells us what's the probability that x_j equals 1 when the class is 1? [inaudible].",
    "start": "4163245",
    "end": "4175560"
  },
  {
    "text": "So what the, uh, what this means is, what's the probability that this word- the word, um,",
    "start": "4175560",
    "end": "4181694"
  },
  {
    "text": "corresponding to the index j will show up in an email that is spammy.",
    "start": "4181695",
    "end": "4188969"
  },
  {
    "text": "And this tells you what's the probability that the- the word indexed by j shows up in a-n email or text message that is not spammy. Yes, question?",
    "start": "4188970",
    "end": "4197960"
  },
  {
    "text": "[inaudible] So what's the number of parameters here? So here we have one parameter,",
    "start": "4197960",
    "end": "4205395"
  },
  {
    "text": "here we have- technically, we have d minus one parameters. Oh, no.",
    "start": "4205395",
    "end": "4211815"
  },
  {
    "text": "So here we have d parameters, and here also we have d parameters. So 2d plus 1 parameters. Good question.",
    "start": "4211815",
    "end": "4222195"
  },
  {
    "text": "Does it make sense? We have one parameter to- to, uh, give us the class priors,",
    "start": "4222195",
    "end": "4227385"
  },
  {
    "text": "whether just, you know what, in the entire population, what fraction of messages are spammy versus not.",
    "start": "4227385",
    "end": "4233550"
  },
  {
    "text": "And then condition on, you know, by limiting ourselves to say, only the spam emails,",
    "start": "4233550",
    "end": "4239565"
  },
  {
    "text": "then we have, um, one parameter for each word that tells us the probability that the word will appear",
    "start": "4239565",
    "end": "4245370"
  },
  {
    "text": "in a spam email or the probability that it'll appear in a non-spammy. Yes?",
    "start": "4245370",
    "end": "4250410"
  },
  {
    "text": "[inaudible].",
    "start": "4250410",
    "end": "4257730"
  },
  {
    "text": "So there are d such distributions for each j. So j can be from one to d. So you have d such Bernoulli distributions.",
    "start": "4257730",
    "end": "4266670"
  },
  {
    "text": "[inaudible].",
    "start": "4266670",
    "end": "4278340"
  },
  {
    "text": "I'm sorry, I- I didn't get the question. [inaudible].",
    "start": "4278340",
    "end": "4283920"
  },
  {
    "text": "No, there- there are two different sets of variables, right, two different sets of distributions, right?",
    "start": "4283920",
    "end": "4289545"
  },
  {
    "text": "And for this, um, so this is our model. And we can write out the likelihood function",
    "start": "4289545",
    "end": "4298905"
  },
  {
    "text": "for phi of y phi of j given y equals 0,",
    "start": "4298905",
    "end": "4306270"
  },
  {
    "text": "pi of j given y equals 1. So basically, think of this as,",
    "start": "4306270",
    "end": "4311805"
  },
  {
    "text": "um, so this is one parameter, this is a set of d parameters, this is another set of d parameters,",
    "start": "4311805",
    "end": "4318134"
  },
  {
    "text": "and the likelihood can now be written as probability of- or",
    "start": "4318134",
    "end": "4323789"
  },
  {
    "text": "the log probability i equals 1 to n phi of x_i, y_ i.",
    "start": "4323790",
    "end": "4333760"
  },
  {
    "text": "Given the full set of d, and this can now be written as i equals 1 to",
    "start": "4335900",
    "end": "4344400"
  },
  {
    "text": "n p of y_i",
    "start": "4344400",
    "end": "4353205"
  },
  {
    "text": "given- p of y_i,",
    "start": "4353205",
    "end": "4356770"
  },
  {
    "text": "phi of y times product j equals 1 to",
    "start": "4359660",
    "end": "4372090"
  },
  {
    "text": "be p of x_j i given y_ i phi.",
    "start": "4372090",
    "end": "4385170"
  },
  {
    "text": "[NOISE] And the way we can, we can, um, we came up with this term as the product of all the different, uh,",
    "start": "4385170",
    "end": "4392070"
  },
  {
    "text": "x_js is using the- the conditional independence assumption. What are basically, u h, the way we came up with that is phi of, say x_1,",
    "start": "4392070",
    "end": "4401085"
  },
  {
    "text": "x_2, x_d given y, this is p of x given y,",
    "start": "4401085",
    "end": "4406665"
  },
  {
    "text": "can be written, uh, using the chain rule as phi of x_1 given y times p",
    "start": "4406665",
    "end": "4415680"
  },
  {
    "text": "of x_2 given x_1 over y times p of x_3 given x_1,",
    "start": "4415680",
    "end": "4425775"
  },
  {
    "text": "x_2, y and so on. And with the conditional independence assumption,",
    "start": "4425775",
    "end": "4432105"
  },
  {
    "text": "each of this becomes p of x_1 given y, and with the conditional,",
    "start": "4432105",
    "end": "4437670"
  },
  {
    "text": "er, uh, independence assumption, this becomes p of x_2 given y, and this becomes p of x_3 given y, and so on.",
    "start": "4437670",
    "end": "4445980"
  },
  {
    "text": "And this product is what's written here.",
    "start": "4445980",
    "end": "4448600"
  },
  {
    "text": "So, you know, just the way the IID assumption allows us to factor",
    "start": "4451460",
    "end": "4457980"
  },
  {
    "text": "out different exam- the- the probability or the likelihood of- or- or the probability of the full data,",
    "start": "4457980",
    "end": "4463580"
  },
  {
    "text": "uh, into product of the, um, individual probabilities. Similarly, conditional independence allows us to break down, um,",
    "start": "4463580",
    "end": "4472730"
  },
  {
    "text": "this full, ah, ah, collection given y as, you know, the product of the individuals, right.",
    "start": "4472730",
    "end": "4480150"
  },
  {
    "text": "So this is the, ah, likelihood. And, uh, each of these,",
    "start": "4480150",
    "end": "4486270"
  },
  {
    "text": "uh, we know is a Bernoulli. And we know the, um, probability density of a Bernoulli,",
    "start": "4486270",
    "end": "4492735"
  },
  {
    "text": "plug them in, take the gradient, take the partials with respect to each parameter,",
    "start": "4492735",
    "end": "4498269"
  },
  {
    "text": "set it equal to zero. And by doing that, we get the maximum likelihood estimates,",
    "start": "4498270",
    "end": "4504240"
  },
  {
    "text": "MLE estimates phi j given y equals 1",
    "start": "4504240",
    "end": "4514770"
  },
  {
    "text": "is equal to i equals 1 to n h",
    "start": "4514770",
    "end": "4525060"
  },
  {
    "text": "j 1 [BACKGROUND]",
    "start": "4525060",
    "end": "4544270"
  },
  {
    "text": "[NOISE]. What does this mean? The probability that the- the word",
    "start": "4544270",
    "end": "4551598"
  },
  {
    "text": "belonging to the jth index in the dictionary is- is- is,",
    "start": "4551599",
    "end": "4559010"
  },
  {
    "text": "uh, the probability that the jth word in the- in the dic- in the vocabulary is going to show up in a spammy email is equal",
    "start": "4559010",
    "end": "4567200"
  },
  {
    "text": "to the number of times the word appears in spammy emails divided by the total number of spammy emails, right?",
    "start": "4567200",
    "end": "4577370"
  },
  {
    "text": "This expression looks a little cryptic, but it's actually pretty simple. What it's doing is, it is- it scans over every email or every text message",
    "start": "4577370",
    "end": "4585605"
  },
  {
    "text": "i equals 1 to n and count the number of times or,",
    "start": "4585605",
    "end": "4590735"
  },
  {
    "text": "uh, count the number of, uh, text messages where the jth word appears and",
    "start": "4590735",
    "end": "4597155"
  },
  {
    "text": "the message is spammy and divide it by the total number of emails,",
    "start": "4597155",
    "end": "4603094"
  },
  {
    "text": "uh, total number of text messages that are spammy. It's just calculating in what fraction of the spammy emails does the jth word appear, right?",
    "start": "4603095",
    "end": "4611375"
  },
  {
    "text": "The syntax looks a little cryptic, but, you know, it's the- the idea is very simple. Yes, question. This Phi?",
    "start": "4611375",
    "end": "4624485"
  },
  {
    "text": "So this Phi is, basically the full collection or just, uh, written it as Phi. It's 2d plus 1.",
    "start": "4624485",
    "end": "4632760"
  },
  {
    "text": "Yes, question. [BACKGROUND] Yeah so- yeah so,",
    "start": "4634360",
    "end": "4641135"
  },
  {
    "text": "um, and similarly, uh, Phi_j given y equals 0 is equal to same story,",
    "start": "4641135",
    "end": "4649625"
  },
  {
    "text": "i equals 1_n indicator x_i_j equals 1 and so this is the logical and, right,",
    "start": "4649625",
    "end": "4659810"
  },
  {
    "text": "and y_i equals 0 divided by i",
    "start": "4659810",
    "end": "4666950"
  },
  {
    "text": "equals 1_n indicator y_i equals 0, okay.",
    "start": "4666950",
    "end": "4674315"
  },
  {
    "text": "And finally, Phi_y is equal to",
    "start": "4674315",
    "end": "4680400"
  },
  {
    "text": "1_n indicator y_i equals one divided by n, right?",
    "start": "4680980",
    "end": "4689500"
  },
  {
    "text": "So the Phi_y parameter is estimated as the number of spammy messages divided by n, right?",
    "start": "4689500",
    "end": "4698165"
  },
  {
    "text": "And each- for each class of, you know, um, spammy versus not spammy,",
    "start": "4698165",
    "end": "4704329"
  },
  {
    "text": "you get a full collection of Bernoulli's per word and they are estimated as the number of- the number of messages in which the word appears,",
    "start": "4704330",
    "end": "4713105"
  },
  {
    "text": "uh, divided by the total number of messages in that class and it's the same definition here,",
    "start": "4713105",
    "end": "4718295"
  },
  {
    "text": "but we just switch y equals 1 here. Yes, question. [BACKGROUND] This one?",
    "start": "4718295",
    "end": "4727325"
  },
  {
    "text": "So here, we just broke down the joint into the factors, right?",
    "start": "4727325",
    "end": "4732515"
  },
  {
    "text": "So think of, you know, uh, we start with this joint and you can write this as",
    "start": "4732515",
    "end": "4738590"
  },
  {
    "text": "p_x_i given y_i times p_y_i, right?",
    "start": "4738590",
    "end": "4748880"
  },
  {
    "text": "And- and this one,",
    "start": "4748880",
    "end": "4754925"
  },
  {
    "text": "we factorize it using the conditional independence assumption, right?",
    "start": "4754925",
    "end": "4761449"
  },
  {
    "text": "So this- this is just, uh, this is just the factorization of this by making use of the condition assumption, conditional independence assumption.",
    "start": "4761450",
    "end": "4768140"
  },
  {
    "text": "[BACKGROUND] Yeah, this parameterized by Phi_y.",
    "start": "4768140",
    "end": "4774410"
  },
  {
    "text": "Yeah. Yes, question. [BACKGROUND] So the,",
    "start": "4774410",
    "end": "4788045"
  },
  {
    "text": "uh, uh, assu- what does conditional assumption mean here? It means, um, when you know whether- when a- a message is spammy,",
    "start": "4788045",
    "end": "4795500"
  },
  {
    "text": "then the probability of word a appearing is independent of whether word b appeared or not in it.",
    "start": "4795500",
    "end": "4803510"
  },
  {
    "text": "[BACKGROUND] Yes, so if certain words always appear together,",
    "start": "4803510",
    "end": "4811579"
  },
  {
    "text": "uh, it does violate the conditional independence assumption. But we're just going to make that assumption anyways,",
    "start": "4811580",
    "end": "4818840"
  },
  {
    "text": "just to make the math simple. Yes. [BACKGROUND] Yes,",
    "start": "4818840",
    "end": "4828020"
  },
  {
    "text": "this ignores the order in which the words come. Yes. [BACKGROUND]",
    "start": "4828020",
    "end": "4841099"
  },
  {
    "text": "Yes. So, uh, in- in a, um, spammy email, uh, some words are not necessarily spammy, right?",
    "start": "4841100",
    "end": "4849875"
  },
  {
    "text": "Like words like the, and are not necessarily spammy. But, uh, what we see is that by- by making this assumption and,",
    "start": "4849875",
    "end": "4859250"
  },
  {
    "text": "uh, by th- this, uh, uh, this set of assumptions generally tends to work well in practice even though,",
    "start": "4859250",
    "end": "4867275"
  },
  {
    "text": "you know, there are, um, um, the- the- the intuition that is that if a word, um,",
    "start": "4867275",
    "end": "4872495"
  },
  {
    "text": "has no significance in terms of its indicative power, then it's going to contribute",
    "start": "4872495",
    "end": "4877880"
  },
  {
    "text": "equally to both the classes and its effect kind of cancels out. That's the intuition to have like common words are-",
    "start": "4877880",
    "end": "4883655"
  },
  {
    "text": "will have equal weight in those in- in- in, uh, for both classes. But words that have, uh,",
    "start": "4883655",
    "end": "4888785"
  },
  {
    "text": "a high indicative power of spamminess will- will- will bump up the prob- probability higher, right?",
    "start": "4888785",
    "end": "4897844"
  },
  {
    "text": "So that's the, uh, maximum likelihood estimate and from this, how do we make, uh, predictions now?",
    "start": "4897845",
    "end": "4906480"
  },
  {
    "text": "So we follow MLE and we estimate all the parameters and once we have, uh,",
    "start": "4908140",
    "end": "4914555"
  },
  {
    "text": "estimated the parameters, we're going to make predictions using the Bayes rule.",
    "start": "4914555",
    "end": "4925200"
  },
  {
    "text": "So Bayes rule tells us p_y equals 1 given x is equal to p_x given y, uh,",
    "start": "4931510",
    "end": "4943019"
  },
  {
    "text": "p_y given x times p_y divided by p_x",
    "start": "4943570",
    "end": "4953849"
  },
  {
    "text": "and this is equal to p_x given y times p_y",
    "start": "4954370",
    "end": "4962660"
  },
  {
    "text": "over p_x given y equals 0 times p equals 0 plus p. [NOISE]",
    "start": "4962660",
    "end": "4976410"
  },
  {
    "text": "Now, to make predictions on a new email x, you're gonna first convert it into a feature vector of d dimensions,",
    "start": "4976410",
    "end": "4985050"
  },
  {
    "text": "and we're gonna calculate these terms. Um, so p of y,",
    "start": "4985050",
    "end": "4990930"
  },
  {
    "text": "we estimated, uh, the, uh, and- and here we're just gonna use, you know, phi y, [NOISE] um,",
    "start": "4990930",
    "end": "4996750"
  },
  {
    "text": "and this is basically the product i equals 1, j equals 1 to d x,",
    "start": "4996750",
    "end": "5005699"
  },
  {
    "text": "j, i given y times p of y divided by the denominator.",
    "start": "5005920",
    "end": "5017175"
  },
  {
    "text": "Right? And using- again- again, using the conditional independence assumption,",
    "start": "5017175",
    "end": "5022340"
  },
  {
    "text": "we can break down the- the, uh, the long vector of- of, um, the feature vector of the new email into product of the individual Bernoullis.",
    "start": "5022340",
    "end": "5033275"
  },
  {
    "text": "Right? And, um, multiplied by the class probability, we get the probability of- of,",
    "start": "5033275",
    "end": "5040910"
  },
  {
    "text": "um, p of y equals 1. So we do p of y equals 1 given x equals, and similarly,",
    "start": "5040910",
    "end": "5050240"
  },
  {
    "text": "you can calculate p of y equals 0 given x to be the same thing with- but, um, [NOISE]",
    "start": "5050240",
    "end": "5070720"
  },
  {
    "text": "denominator. Okay? And in order to make a prediction, um, what's commonly done is you calculate numerator",
    "start": "5070720",
    "end": "5078200"
  },
  {
    "text": "1 and you calculate numerator 2 and see which of that, you know- two numerators is bigger than the other.",
    "start": "5078200",
    "end": "5084329"
  },
  {
    "text": "All right? So, uh, one problem that we may encounter with this method.",
    "start": "5085540",
    "end": "5093470"
  },
  {
    "text": "So any questions on this so far? Yes, question. [inaudible] add this to one? So these two add up to 1.",
    "start": "5093470",
    "end": "5100025"
  },
  {
    "text": "Once you, uh, calculate the denominator and normalize them, they will add up to 1. But just the two numerators need not add up to 1.",
    "start": "5100025",
    "end": "5106520"
  },
  {
    "text": "[inaudible]. Yeah. If you want to get the probability value,",
    "start": "5106520",
    "end": "5112100"
  },
  {
    "text": "then you need to, um, then you need to, uh, calculate the denominator. But if all what you want is to make",
    "start": "5112100",
    "end": "5118670"
  },
  {
    "text": "a decision of which of the two has a higher probability, then you don't need to calculate the denominator.",
    "start": "5118670",
    "end": "5123840"
  },
  {
    "text": "Okay? So one problem that, uh, you may encounter with this method is,",
    "start": "5124030",
    "end": "5130070"
  },
  {
    "text": "what happens if at test time we encounter a word that was never seen in the training set, right?",
    "start": "5130070",
    "end": "5139730"
  },
  {
    "text": "So our, um, so i- in- in this model,",
    "start": "5139730",
    "end": "5145070"
  },
  {
    "text": "[NOISE] what if the words say aardvark?",
    "start": "5145070",
    "end": "5153710"
  },
  {
    "text": "Never showed up in our training set, neither in positive examples nor in negative examples.",
    "start": "5153710",
    "end": "5159395"
  },
  {
    "text": "And we end up estimating the Phis for the word aardvark to be 0 and",
    "start": "5159395",
    "end": "5166370"
  },
  {
    "text": "0 because the numerator is zero in both the places. And at test time, what do we do?",
    "start": "5166370",
    "end": "5172250"
  },
  {
    "text": "At test time it means, [NOISE] for the word aardvark,",
    "start": "5172250",
    "end": "5179975"
  },
  {
    "text": "at test time, we are gonna end up with 0 over 0 plus 0. Thus this is gonna be 0.",
    "start": "5179975",
    "end": "5185075"
  },
  {
    "text": "If the word aardvark appears in the email, um, the- the- the probability corresponding",
    "start": "5185075",
    "end": "5190159"
  },
  {
    "text": "to the word aardvark will make the entire thing equal to 0.",
    "start": "5190160",
    "end": "5195170"
  },
  {
    "text": "And similarly, you know, this will also factorize and the word aardvark, we made this 0, this 0, and you're gonna get a 0 over 0,",
    "start": "5195170",
    "end": "5202429"
  },
  {
    "text": "um, when you encounter words that you have not seen before. So basically with this, with- with this method,",
    "start": "5202430",
    "end": "5208130"
  },
  {
    "text": "a- a problem, uh, is if we encounter a message at test time that has a word that never appeared in the training set,",
    "start": "5208130",
    "end": "5217235"
  },
  {
    "text": "either as class 1 or as class 2, we'd- we- the method does not tell us what our prediction should be, right?",
    "start": "5217235",
    "end": "5228409"
  },
  {
    "text": "And for that, uh, what is commonly done is a technique called Laplace smoothing.",
    "start": "5228410",
    "end": "5235400"
  },
  {
    "text": "[NOISE] So Laplace smoothing",
    "start": "5235400",
    "end": "5241190"
  },
  {
    "text": "was invented by Laplace.",
    "start": "5241190",
    "end": "5246560"
  },
  {
    "text": "And though the- the origins of Laplace smoothing was- is,",
    "start": "5246560",
    "end": "5254180"
  },
  {
    "text": "um, it goes something like this. So Laplace was basically trying to calculate the probability,",
    "start": "5254180",
    "end": "5259505"
  },
  {
    "text": "what- what's the probability that the sun is gonna rise tomorrow? What's the probability that the sun is gonna rise the next day?",
    "start": "5259505",
    "end": "5264680"
  },
  {
    "text": "Right? And in your training set, you basically have only positive examples, right?",
    "start": "5264680",
    "end": "5270140"
  },
  {
    "text": "So every day in your- you know, if you look at the past 100 days or 1,000 days, you start collecting your data, right?",
    "start": "5270140",
    "end": "5275974"
  },
  {
    "text": "Every day the sun is rising, right? And in your, um, so your training set, um,",
    "start": "5275975",
    "end": "5280985"
  },
  {
    "text": "has only positive examples. And now when you estimate the probability of what of- the sun, uh,",
    "start": "5280985",
    "end": "5287150"
  },
  {
    "text": "rising the next day, your MLE estimates will give you a probability equal to 1.",
    "start": "5287150",
    "end": "5292775"
  },
  {
    "text": "Right? It will always be exactly equal to 1. And, uh, his claim was- was that, um,",
    "start": "5292775",
    "end": "5299150"
  },
  {
    "text": "[NOISE] that's- that is sub-optimal, right? It's- it's- the- the- the- the probability that the sun will",
    "start": "5299150",
    "end": "5306250"
  },
  {
    "text": "rise the next day at someday in the future is gonna be 0. It might be, you know, the sun may not rise,",
    "start": "5306250",
    "end": "5312110"
  },
  {
    "text": "um, you know, uh, uh, uh, someday. And- and so, um, he invented with, uh,",
    "start": "5312110",
    "end": "5318094"
  },
  {
    "text": "the- the- that the correction that he came up with is commonly called as, um, Laplace smoothing.",
    "start": "5318095",
    "end": "5323510"
  },
  {
    "text": "And the idea of Laplace smoothing is supposing you have,",
    "start": "5323510",
    "end": "5328519"
  },
  {
    "text": "um, let's say you're doing coin tosses, right? So let's say you're trying to estimate,",
    "start": "5328520",
    "end": "5334219"
  },
  {
    "text": "um, the bias of a coin, let's call it Phi, and, uh, your x's are 0 and 1.",
    "start": "5334220",
    "end": "5341000"
  },
  {
    "text": "[NOISE] Now, you take- you take a coin, you know, flip it 10 times,",
    "start": "5341000",
    "end": "5346010"
  },
  {
    "text": "and let's say the, uh, the- the- the coin turns up heads all the,",
    "start": "5346010",
    "end": "5351020"
  },
  {
    "text": "uh- uh, for all the 10, uh, for all the 10 trials. What is our estimate of Phi in that case if Phi is supposed to be probability of heads?",
    "start": "5351020",
    "end": "5360365"
  },
  {
    "text": "Right? So the- the MLE estimate tells us that this is gonna be 10 over 10, [NOISE] and, uh,",
    "start": "5360365",
    "end": "5365810"
  },
  {
    "text": "and- and so, uh, our, uh, parameters would be, uh, um, uh, uh, the Phi would be 1.",
    "start": "5365810",
    "end": "5371960"
  },
  {
    "text": "But what Laplace smoothing tells you is the way to kind of, uh, think about that is, um,",
    "start": "5371960",
    "end": "5377915"
  },
  {
    "text": "let's say you, um, you're- you're- you're- you're, um, conducting- you're keeping two counts,",
    "start": "5377915",
    "end": "5384875"
  },
  {
    "text": "so [NOISE] count of heads and count of tails, right?",
    "start": "5384875",
    "end": "5393695"
  },
  {
    "text": "In pure maximum likelihood, we start with the two counts being 0,",
    "start": "5393695",
    "end": "5399005"
  },
  {
    "text": "and as we keep conducting trials, we add a 1, [NOISE] and so on.",
    "start": "5399005",
    "end": "5408440"
  },
  {
    "text": "And then we sum up the, uh, count H over sum over count H plus sum over count tail, right?",
    "start": "5408440",
    "end": "5422119"
  },
  {
    "text": "What Laplace smoothing tells us is before we even start conducting our experiments,",
    "start": "5422120",
    "end": "5428520"
  },
  {
    "text": "start with a count of 1 for all your classes, right? Assume a uniform probability before we start conducting our experiment,",
    "start": "5428710",
    "end": "5437840"
  },
  {
    "text": "and then start your trials and observing data and incrementing the counts, right?",
    "start": "5437840",
    "end": "5444905"
  },
  {
    "text": "And with this technique, we see that because we start with a count of 1 on both sides,",
    "start": "5444905",
    "end": "5450605"
  },
  {
    "text": "even if all our 10 experiments come up with a plus 1,",
    "start": "5450605",
    "end": "5455840"
  },
  {
    "text": "the probability of heads in this method will be 10 over 11.",
    "start": "5455840",
    "end": "5461525"
  },
  {
    "text": "Right? It will never be exactly 1. And similarly, the probability of tails will keep going down as we collect more data.",
    "start": "5461525",
    "end": "5468830"
  },
  {
    "text": "It's gonna start with 0.5, and as we keep running experiments and- and the probability, uh,",
    "start": "5468830",
    "end": "5474980"
  },
  {
    "text": "and, uh, we keep getting only heads, this keeps coming down, right, but it will never be exactly 0.",
    "start": "5474980",
    "end": "5481429"
  },
  {
    "text": "Right? This the- this is the, um, idea of Laplace smoothing. And this Laplace smoothing can be applied to,",
    "start": "5481430",
    "end": "5489994"
  },
  {
    "text": "um, our- our spam classifier as well, right? So assume that we have seen",
    "start": "5489995",
    "end": "5495980"
  },
  {
    "text": "every word once in a spammy email and once in a non-spammy email,",
    "start": "5495980",
    "end": "5501660"
  },
  {
    "text": "right, initialize your counts to that, and then start observing your data and- and,",
    "start": "5501660",
    "end": "5507985"
  },
  {
    "text": "uh, uh, accounting for your data. So the, uh, Laplace smooth version of-[NOISE]",
    "start": "5507985",
    "end": "5523580"
  },
  {
    "text": "so the, uh, Laplace smooth version of our estimates will be like this.",
    "start": "5523580",
    "end": "5529650"
  },
  {
    "text": "So for comparison, here is the, uh, maximum likelihood estimate and the Laplace smooth estimate would be [NOISE] phi",
    "start": "5530230",
    "end": "5538130"
  },
  {
    "text": "j given y equals 1 equals 1 plus sum over y equals 1 to n,",
    "start": "5538130",
    "end": "5548370"
  },
  {
    "text": "x, j, i equals 1,",
    "start": "5548620",
    "end": "5556220"
  },
  {
    "text": "and y^i equals 1 divided by 2 plus sum over n. Right?",
    "start": "5556220",
    "end": "5577430"
  },
  {
    "text": "So the idea here is, um, you add 1 to your, uh,",
    "start": "5577430",
    "end": "5582500"
  },
  {
    "text": "to the count, and you add the number of classes in your problem to the denominator.",
    "start": "5582500",
    "end": "5589310"
  },
  {
    "text": "So you add 1 to the numerator to say you've seen that word at least once, and you divide it by 2,",
    "start": "5589310",
    "end": "5595650"
  },
  {
    "text": "um, for to- to, uh, uh, account for the fact that, uh,",
    "start": "5595650",
    "end": "5601409"
  },
  {
    "text": "that this evaluation of probability, of valid probability density, a prop- va- valid probability distribution.",
    "start": "5601450",
    "end": "5608719"
  },
  {
    "text": "Right? And similarly, the Laplace smooth version of a j given y equal 0 will be 1 plus [NOISE]",
    "start": "5608720",
    "end": "5638945"
  },
  {
    "text": "So this component, in the denominator, comes from the data, the number of messages which are spammy.",
    "start": "5638945",
    "end": "5647870"
  },
  {
    "text": "And then you assume, you've seen two more messages. One message in which the word appear,",
    "start": "5647870",
    "end": "5653555"
  },
  {
    "text": "and one message in which the word did not appear. Right. And over here,",
    "start": "5653555",
    "end": "5658945"
  },
  {
    "text": "since this is the probability that the word appears, uh, you assume you've seen the word appear in one message,",
    "start": "5658945",
    "end": "5665005"
  },
  {
    "text": "and also in the rest of the messages where- where the word actually appear. Right, that's the idea of Laplace smoothing.",
    "start": "5665005",
    "end": "5670440"
  },
  {
    "text": "Right. And this is, um, if you're familiar with Bayesian statistics,",
    "start": "5670440",
    "end": "5675710"
  },
  {
    "text": "this is like imposing a Beta prior on your parameters. If you don't know what that is, don't bother.",
    "start": "5675710",
    "end": "5681280"
  },
  {
    "text": "But, um, it's- it's, not- not only is it just a heuristic, um, heuristic,",
    "start": "5681280",
    "end": "5686830"
  },
  {
    "text": "uh, argument, there's also, uh, a principled explanation of why this is a good thing to do.",
    "start": "5686830",
    "end": "5693030"
  },
  {
    "text": "All right. So that's Laplace smoothing. And now we, um,",
    "start": "5693030",
    "end": "5700295"
  },
  {
    "text": "and this- this- this model is what's also called as the Bernoulli Event Model.",
    "start": "5700295",
    "end": "5707929"
  },
  {
    "text": "And there's another variant of Naive Bayes, which is called the Multinomial Event Model.",
    "start": "5707930",
    "end": "5714740"
  },
  {
    "text": "[NOISE]",
    "start": "5714740",
    "end": "5725540"
  },
  {
    "text": "Right. So in the Multinomial Event Model, which is a slight variation of this,",
    "start": "5725540",
    "end": "5733790"
  },
  {
    "text": "which means a lot of the terminology was gonna show up here, but may mean slightly different things.",
    "start": "5733790",
    "end": "5739790"
  },
  {
    "text": "So please pay attention. Uh, it is- it is a slightly different model, which makes a different set of assumptions.",
    "start": "5739790",
    "end": "5747230"
  },
  {
    "text": "And the assumption it makes is that, you know,",
    "start": "5747230",
    "end": "5752810"
  },
  {
    "text": "y again, comes from a Bernoulli parameter Phi.",
    "start": "5752810",
    "end": "5760220"
  },
  {
    "text": "But then each word in your email, which is either, you know,",
    "start": "5760220",
    "end": "5765530"
  },
  {
    "text": "um, which is either spammy or not spammy, so x given y equals 0,",
    "start": "5765530",
    "end": "5772835"
  },
  {
    "text": "is sampled from a multinomial.",
    "start": "5772835",
    "end": "5777090"
  },
  {
    "text": "Well, so the correct, is actually categorical- is- comes from",
    "start": "5779620",
    "end": "5784850"
  },
  {
    "text": "a categorical distribution and we're gonna still call it Phi.",
    "start": "5784850",
    "end": "5793030"
  },
  {
    "text": "Except now Phi is, uh, D-dimensional categorical, uh, uh, uh, parameter vector.",
    "start": "5793030",
    "end": "5802005"
  },
  {
    "text": "In fact, here it will be- so this is one-dimensional, Phi of y.",
    "start": "5802005",
    "end": "5807574"
  },
  {
    "text": "And Phi of k given y equals 0 will be d- [NOISE] let's not call it d,",
    "start": "5807575",
    "end": "5816140"
  },
  {
    "text": "will be the size of your vocabulary minus one-dimension.",
    "start": "5816140",
    "end": "5821510"
  },
  {
    "text": "What does this mean now? So first I'm gonna write out the expressions and then,",
    "start": "5821510",
    "end": "5827840"
  },
  {
    "text": "you know, give you some intuitions about this. [NOISE] Right.",
    "start": "5827840",
    "end": "5833014"
  },
  {
    "text": "So the- so the",
    "start": "5833015",
    "end": "5840605"
  },
  {
    "text": "MLE estimates here, look like this. Phi k given y equals 1.",
    "start": "5840605",
    "end": "5850520"
  },
  {
    "text": "[NOISE]",
    "start": "5850520",
    "end": "5908120"
  },
  {
    "text": "Right. Here the assumption is that your x now indicate- x_j indicates the word in the jth position of the- of your text message.",
    "start": "5908120",
    "end": "5918350"
  },
  {
    "text": "And that can be- that is a word with- that exists in your dictionary.",
    "start": "5918350",
    "end": "5924484"
  },
  {
    "text": "So v are- is the vo- the vocabulary. And [NOISE] this means the size of your vocabulary.",
    "start": "5924484",
    "end": "5930560"
  },
  {
    "text": "So x_j or the jth word in a message, takes a value between 1 and the size of your vocabulary.",
    "start": "5930560",
    "end": "5936469"
  },
  {
    "text": "And x^i, which is the ith message, is the full- full, uh, message of your, uh, uh,",
    "start": "5936470",
    "end": "5942380"
  },
  {
    "text": "ith example is- has length di. So each message can have a different length.",
    "start": "5942380",
    "end": "5949295"
  },
  {
    "text": "Right. And x^i is now a vector of length di,",
    "start": "5949295",
    "end": "5955340"
  },
  {
    "text": "where each component can take one of the words in our vocabulary. Right. Now, the, um,",
    "start": "5955340",
    "end": "5961660"
  },
  {
    "text": "maximum likelihood estimate, for this model, will say Ph- uh, Phi of k, where k is the word in the vocabulary,",
    "start": "5961660",
    "end": "5970895"
  },
  {
    "text": "is given by this term, where you're summing over every- every message.",
    "start": "5970895",
    "end": "5976220"
  },
  {
    "text": "And within each message you're summing over every word, and counting only those words in those messages,",
    "start": "5976220",
    "end": "5983074"
  },
  {
    "text": "which happen to be k and belong to the class that we're interested in. And divide it by the total number of words across all messages in that class.",
    "start": "5983075",
    "end": "5993470"
  },
  {
    "text": "Right. And similarly, Phi of k given y equals 0,",
    "start": "5993470",
    "end": "6001270"
  },
  {
    "text": "equals, basically the same thing, just replace y equals 1 by y equals 0.",
    "start": "6001270",
    "end": "6006655"
  },
  {
    "text": "Right. And you can also have a Laplace model version of this. [NOISE] I'm gonna write out the Laplace model version and then,",
    "start": "6006655",
    "end": "6016465"
  },
  {
    "text": "you know, give some more intuitions about this. Phi k given y equals 1,",
    "start": "6016465",
    "end": "6022390"
  },
  {
    "text": "equals 1 plus, i equals 1 to n,",
    "start": "6022390",
    "end": "6029455"
  },
  {
    "text": "j equals 1 to di, x^i, j equals k,",
    "start": "6029455",
    "end": "6039699"
  },
  {
    "text": "and y^i equals 1, divided by the vocabulary size plus.",
    "start": "6039700",
    "end": "6048849"
  },
  {
    "text": "[NOISE]",
    "start": "6048850",
    "end": "6063130"
  },
  {
    "text": "All right. So this is the, uh, similarly you have, uh, Phi k given y equals 0,",
    "start": "6063130",
    "end": "6069400"
  },
  {
    "text": "equals, you know, a similar thing, just replace, uh, y equals 1 with 0. And the, um, the way to think about this- the,",
    "start": "6069400",
    "end": "6077470"
  },
  {
    "text": "um, the Bernoulli Event model and the Multinomial Event model, [NOISE] is to probably, um,",
    "start": "6077470",
    "end": "6084475"
  },
  {
    "text": "look at how an algorithmic implementation of- of these two models would look like.",
    "start": "6084475",
    "end": "6090730"
  },
  {
    "text": "[NOISE]",
    "start": "6090730",
    "end": "6100840"
  },
  {
    "text": "So supposing we have a training set. Let's call this, you know, buy our lottery, and,",
    "start": "6100840",
    "end": "6113440"
  },
  {
    "text": "um, whatever, buy this watch.",
    "start": "6113440",
    "end": "6118730"
  },
  {
    "text": "Right. You have a few examples. And let's say you also have a non-spammy,",
    "start": "6119430",
    "end": "6125260"
  },
  {
    "text": "um, example, for example, when is your exam [NOISE]",
    "start": "6125260",
    "end": "6134800"
  },
  {
    "text": "or when is the homework due? [NOISE] Right. Now, um.",
    "start": "6134800",
    "end": "6149630"
  },
  {
    "text": "First, let's look at the Bernoulli. [NOISE] So in the Bernoulli,",
    "start": "6153900",
    "end": "6160285"
  },
  {
    "text": "we convert this into a fixed-length vector where you- we have a aardvark,",
    "start": "6160285",
    "end": "6168760"
  },
  {
    "text": "buy, exam, and so on.",
    "start": "6168760",
    "end": "6177159"
  },
  {
    "text": "And what- what we do is-",
    "start": "6177160",
    "end": "6179900"
  },
  {
    "text": "right. What we do is for each example,",
    "start": "6190020",
    "end": "6195190"
  },
  {
    "text": "we- we use an indicator variable to say whether, you know, the word appear or not, right?",
    "start": "6195190",
    "end": "6202525"
  },
  {
    "text": "Similarly here, let's say you buy and that's going to be, uh, a watch, you know, and 0s everywhere.",
    "start": "6202525",
    "end": "6209815"
  },
  {
    "text": "Right. So this is the fixed-length representation which is D-dimensional, where D is the- is the size of your dictionary.",
    "start": "6209815",
    "end": "6217495"
  },
  {
    "text": "So the way you wanna think about this is, in case of the Bernoulli model, first, we're gonna convert a given message",
    "start": "6217495",
    "end": "6223675"
  },
  {
    "text": "into a multi-hot vector which means it is 1 in multiple places. The word may appear many times in this- in a given message,",
    "start": "6223675",
    "end": "6231460"
  },
  {
    "text": "but we count it only once. Right. So, um, um, maybe to clarify this,",
    "start": "6231460",
    "end": "6237190"
  },
  {
    "text": "buy this watch, [NOISE] this watch only. Right.",
    "start": "6237190",
    "end": "6245690"
  },
  {
    "text": "And we then- and we do the sa- repeat the same thing for the non-spammy.",
    "start": "6246630",
    "end": "6253210"
  },
  {
    "text": "So here, we will have, you know, exam will be one. You know, your will be one,",
    "start": "6253210",
    "end": "6259480"
  },
  {
    "text": "the others will be zero. [NOISE] Similarly, this would be a few places.",
    "start": "6259480",
    "end": "6264550"
  },
  {
    "text": "[NOISE] Right. So we have [NOISE] a set of spammy examples and a set of non-spammy examples.",
    "start": "6264550",
    "end": "6278560"
  },
  {
    "text": "Right. And for each of these two different classes, we have a separate collection of Bernoullis per word.",
    "start": "6278560",
    "end": "6287395"
  },
  {
    "text": "So we have one Bernoulli for the word A, one Bernoulli for the word aardvark,",
    "start": "6287395",
    "end": "6293520"
  },
  {
    "text": "one Bernoulli for the word buy. And each of these Bernoullis is estimated by summing over the number of",
    "start": "6293520",
    "end": "6300730"
  },
  {
    "text": "times the word has a 1 divided by the number of words in that class.",
    "start": "6300730",
    "end": "6305995"
  },
  {
    "text": "Similarly over here, another set of- of Bernoullis. So this is five- say,",
    "start": "6305995",
    "end": "6312220"
  },
  {
    "text": "5J- 5J for y equals 1. This is 5J for y equals 0.",
    "start": "6312220",
    "end": "6320455"
  },
  {
    "text": "And each of these 5Js, you just count the fraction of- the fraction of words, uh,",
    "start": "6320455",
    "end": "6326605"
  },
  {
    "text": "the fraction of, uh, um, messages in which the word appears divided by the number of messages.",
    "start": "6326605",
    "end": "6334000"
  },
  {
    "text": "And similarly, the frac- here, [NOISE] this will be 1 over 2, this will be 2 over 2, and so on.",
    "start": "6334000",
    "end": "6340764"
  },
  {
    "text": "Right. This is the Bernoulli model that we discussed first.",
    "start": "6340765",
    "end": "6346405"
  },
  {
    "text": "Right. And similarly, the multinomial, however, is a little different.",
    "start": "6346405",
    "end": "6351410"
  },
  {
    "text": "The multinomial, okay, if you were to implement this algorithmically.",
    "start": "6353520",
    "end": "6359545"
  },
  {
    "text": "Right. Assume you have the same set of words here. A, aardvark, buy, exam, and so on.",
    "start": "6359545",
    "end": "6372460"
  },
  {
    "text": "In this case, however, instead of, um, using an indicator, just count the number of times the word appears.",
    "start": "6372460",
    "end": "6379060"
  },
  {
    "text": "Right. So 0, 0. No, buy is 1, lottery is 1.",
    "start": "6379060",
    "end": "6386020"
  },
  {
    "text": "Over here again, buy is 1, watch is 2. This is watch.",
    "start": "6386020",
    "end": "6391659"
  },
  {
    "text": "Watch was 0 here, you know. Only is 1, whatever. Right. And now, we do the same normalization by summing up the counts.",
    "start": "6391660",
    "end": "6403960"
  },
  {
    "text": "So the number of times the word appeared across all examples in your training set.",
    "start": "6403960",
    "end": "6411685"
  },
  {
    "text": "Right. Whereas over here, it was the number of messages or number of examples in which the word appeared.",
    "start": "6411685",
    "end": "6418120"
  },
  {
    "text": "Here, it is the number of times the word appeared across all messages. Right. That's the main difference between the Bernoulli model and the multinomial model.",
    "start": "6418120",
    "end": "6427915"
  },
  {
    "text": "And so here, our- um, so we're gonna sum up the counts. So here, it's going to be 2, 2,",
    "start": "6427915",
    "end": "6434830"
  },
  {
    "text": "2, and maybe some of them, it is 3, it is 4. In the Bernoulli model,",
    "start": "6434830",
    "end": "6439900"
  },
  {
    "text": "each one of them was its own Bernoulli variable, so we normalize it locally.",
    "start": "6439900",
    "end": "6446349"
  },
  {
    "text": "Right. In the multinomial model, we want the distribution over words. So we're gonna normalize this entire thing by summing over 4 plus 2 plus 3 plus 2 plus 2.",
    "start": "6446350",
    "end": "6459145"
  },
  {
    "text": "Right. So the- the multinomial model gives us a distribution over words of",
    "start": "6459145",
    "end": "6465520"
  },
  {
    "text": "how frequently that word appears in spammy messages or non-spammy messages.",
    "start": "6465520",
    "end": "6471520"
  },
  {
    "text": "And it doesn't care about whether the word appears 1,000 times in one message or appears once in 1,000 different messages.",
    "start": "6471520",
    "end": "6479515"
  },
  {
    "text": "The multinomial model doesn't care. The Bernoulli model, however, counts what fraction of the messages has this word.",
    "start": "6479515",
    "end": "6488635"
  },
  {
    "text": "The Bernoulli model cares, it- it doesn't care how many times the word appeared in a message.",
    "start": "6488635",
    "end": "6494440"
  },
  {
    "text": "It's just going to count it once per word- once per message. Right. And when you apply Laplace smoothing,",
    "start": "6494440",
    "end": "6501985"
  },
  {
    "text": "what it basically does is you add two messages,",
    "start": "6501985",
    "end": "6508400"
  },
  {
    "text": "one message has 0 everywhere, and one message has 1 everywhere.",
    "start": "6509550",
    "end": "6516880"
  },
  {
    "text": "[NOISE] Right. And then, um, we normalize it locally,",
    "start": "6516880",
    "end": "6523179"
  },
  {
    "text": "you get the Laplace smooth version of the Bernoulli event model. Similarly, you know, if you extend the 0s here,",
    "start": "6523179",
    "end": "6530770"
  },
  {
    "text": "one messa- one message that has no words and one message that has all the words, [NOISE] and we normalize it,",
    "start": "6530770",
    "end": "6537969"
  },
  {
    "text": "you get the Laplace smooth version of the multinomial event model.",
    "start": "6537970",
    "end": "6543020"
  },
  {
    "text": "Any questions? Yes, question. [inaudible]",
    "start": "6544350",
    "end": "6557050"
  },
  {
    "text": "Yes. [inaudible]?",
    "start": "6557050",
    "end": "6562090"
  },
  {
    "text": "So the question is if there is- if there is one, um, if there is one spammy message that",
    "start": "6562090",
    "end": "6568989"
  },
  {
    "text": "has a spammy word repeated 10,000 times but does not appear in any other message, then this model will up weight that word, like quite a lot.",
    "start": "6568990",
    "end": "6577705"
  },
  {
    "text": "This model will- will think that, you know, there's just one message with that word. That's the main difference. [NOISE]",
    "start": "6577705",
    "end": "6594700"
  },
  {
    "text": "[inaudible]. So in this- in this model, um, it, uh, it- it is just summing up the words, you know.",
    "start": "6594700",
    "end": "6600489"
  },
  {
    "text": "So one way to think about this model is, take all your spammy messages concatenate it into one long example.",
    "start": "6600490",
    "end": "6606715"
  },
  {
    "text": "You- you get the same model, even if you have one message that's a concatenation of all your, um, you know, messages.",
    "start": "6606715",
    "end": "6613165"
  },
  {
    "text": "But as with this, you're- you're counting how many number of messages a word appears.",
    "start": "6613165",
    "end": "6618715"
  },
  {
    "text": "Right. That's the main difference between them. Yes, question. [inaudible]",
    "start": "6618715",
    "end": "6632620"
  },
  {
    "text": "Yes. [inaudible] Exactly. [inaudible]",
    "start": "6632620",
    "end": "6637660"
  },
  {
    "text": "Here, you're only, uh, uh, summing up. Right. So only the second one will sum up plus v. Right.",
    "start": "6637660",
    "end": "6646494"
  },
  {
    "text": "When you- when you add up all these, you know, the- the- you know, you're getting 0s over here, and here, you're getting one per word.",
    "start": "6646495",
    "end": "6654205"
  },
  {
    "text": "So that's- that's what gives you the plus v. [inaudible]",
    "start": "6654205",
    "end": "6668170"
  },
  {
    "text": "So, um, so the Laplace smooth version, we have a plus- you know, you're adding the number,",
    "start": "6668170",
    "end": "6673975"
  },
  {
    "text": "the- the- the vocabulary size to the denominator. Right. And that is basically achieved,",
    "start": "6673975",
    "end": "6680170"
  },
  {
    "text": "uh, because you're adding a plus 1 per word. So there are, you know, v such words.",
    "start": "6680170",
    "end": "6687640"
  },
  {
    "text": "And when you- when you sum up, you know, the- when you sum up, uh, all the counts here, you get a plus v in the denominator.",
    "start": "6687640",
    "end": "6694780"
  },
  {
    "text": "[inaudible] So buy- so, uh,",
    "start": "6694780",
    "end": "6700885"
  },
  {
    "text": "the Laplace smooth version of buy will be, in this case, 2 plus 1,",
    "start": "6700885",
    "end": "6707515"
  },
  {
    "text": "3 over the original count, whatever it was, plus v. Right.",
    "start": "6707515",
    "end": "6716440"
  },
  {
    "text": "It's gonna be- this is 2 plus 1. So without Laplace smoothing,",
    "start": "6716440",
    "end": "6722230"
  },
  {
    "text": "it was 2 plus the original count. Now, it's gonna be 2 plus 1 because you got one from the new message,",
    "start": "6722230",
    "end": "6729969"
  },
  {
    "text": "and plus v in the denominator because every word got 1. [inaudible]",
    "start": "6729970",
    "end": "6737100"
  },
  {
    "text": "Sum of all the words, yeah. You're summing over all the number of words. Right. So over here, you're summing over the number of words in your spam class or",
    "start": "6737100",
    "end": "6746550"
  },
  {
    "text": "the number of words in your non-spam class. Yes, question.",
    "start": "6746550",
    "end": "6752079"
  },
  {
    "text": "[inaudible] to your first example,",
    "start": "6752080",
    "end": "6757480"
  },
  {
    "text": "should it be, um, one, um, [inaudible] example, it's all words [NOISE] over",
    "start": "6757480",
    "end": "6764215"
  },
  {
    "text": "one instead of one [inaudible] all words zero. [NOISE]",
    "start": "6764215",
    "end": "6770110"
  },
  {
    "text": "I don't think I got- I- I- I mean, those two actual samples [inaudible]",
    "start": "6770110",
    "end": "6775780"
  },
  {
    "text": "Yeah, so the idea is, you know, these counts may all be 1s or all be 0s, and you don't want them to dominate,",
    "start": "6775780",
    "end": "6781870"
  },
  {
    "text": "so you add a 0 and a 1 for each word. [inaudible]",
    "start": "6781870",
    "end": "6799030"
  },
  {
    "text": "So with- with Laplace smoothing, we add one positive and one negative per word.",
    "start": "6799030",
    "end": "6804835"
  },
  {
    "text": "One positive and one negative per word. Okay. All right. Uh, I think we're over time,",
    "start": "6804835",
    "end": "6812590"
  },
  {
    "text": "so that's about it. If you have any more questions, you know, feel free to walk up and, um, I can answer them.",
    "start": "6812590",
    "end": "6818660"
  }
]