[
  {
    "text": "so this is lecture 10 we're going to take a brief respit from scaling laws um",
    "start": "5120",
    "end": "10160"
  },
  {
    "text": "and we're going to talk about inference so the question of inference is a very simple one given a fixed model",
    "start": "10160",
    "end": "17600"
  },
  {
    "text": "that we've trained generate responses given prompts okay",
    "start": "17600",
    "end": "23400"
  },
  {
    "text": "um first we're going to start by understanding what the implications of",
    "start": "23400",
    "end": "28880"
  },
  {
    "text": "inference are um and the workload um that it entails um and then we're going",
    "start": "28880",
    "end": "34320"
  },
  {
    "text": "to talk about ways of making inference uh faster and throughout this lecture",
    "start": "34320",
    "end": "39760"
  },
  {
    "text": "you're going to see that there's a lot of inference is a very deep topic it's actually the uh we didn't do inference",
    "start": "39760",
    "end": "46160"
  },
  {
    "text": "uh last year in lectures so this is the first year we're doing it but there's actually many many topics that could span multiple lectures which I'll try to",
    "start": "46160",
    "end": "52879"
  },
  {
    "text": "condense into one so inference shows up in multiple different places the most",
    "start": "52879",
    "end": "58879"
  },
  {
    "text": "obvious place is if you actually want to use a model you want to use it to chat um you're using cursor or something to",
    "start": "58879",
    "end": "65760"
  },
  {
    "text": "do code completion if you're running batch a data processing job using your language model all of these cases demand",
    "start": "65760",
    "end": "72960"
  },
  {
    "text": "inference because you need to generate tokens from your actual model but it also shows up in other contexts if you",
    "start": "72960",
    "end": "79360"
  },
  {
    "text": "want to even evaluate your model um let's say on instruction following you need to do inference um there's a lot of",
    "start": "79360",
    "end": "87280"
  },
  {
    "text": "interest in test time compute which means uh thinking more before um you",
    "start": "87280",
    "end": "92640"
  },
  {
    "text": "actually output some um uh the final answer and that's also more inference",
    "start": "92640",
    "end": "99600"
  },
  {
    "text": "because thinking is basically generate tokens and then finally even training itself if you're using reinforcement",
    "start": "99600",
    "end": "105680"
  },
  {
    "text": "learning you need to sample responses and then evaluate them based on some",
    "start": "105680",
    "end": "111520"
  },
  {
    "text": "reward and that also requires inference so inference isn't just I want to put up",
    "start": "111520",
    "end": "117200"
  },
  {
    "text": "a chatbot demo inference actually is going to underly many of the basic functions of a language model and even",
    "start": "117200",
    "end": "124240"
  },
  {
    "text": "though it's one lecture I want to stress how actually important it is for uh many things and we'll probably come back to",
    "start": "124240",
    "end": "130800"
  },
  {
    "text": "this when we talk about um in alignment um later in the class so now inference",
    "start": "130800",
    "end": "137680"
  },
  {
    "text": "is important so the theme of this class is efficiency and efficiency clearly",
    "start": "137680",
    "end": "143200"
  },
  {
    "text": "matters training is a one-time cost but inference you repeat multiple times so here's some you know anecdotal uh um",
    "start": "143200",
    "end": "152080"
  },
  {
    "text": "stats on why inference is a big deal so Sam says OpenAI generates a 100red",
    "start": "152080",
    "end": "157760"
  },
  {
    "text": "billion words a day which is quite a quite a lot um and even cursor which is",
    "start": "157760",
    "end": "163760"
  },
  {
    "text": "not that new of a a product is um allegedly generating a billion lines of accepted code um each day",
    "start": "163760",
    "end": "171640"
  },
  {
    "text": "so um it's just giving you an idea of how much inference is accounting for and",
    "start": "171640",
    "end": "178560"
  },
  {
    "text": "you know costs of inference compared to training um are definitely you know",
    "start": "178560",
    "end": "184760"
  },
  {
    "text": "increasing so how do you measure what inference good inference looks like so",
    "start": "184760",
    "end": "190720"
  },
  {
    "text": "there's time to first token TTFT so this is how long a user an individual user",
    "start": "190720",
    "end": "196640"
  },
  {
    "text": "needs to wait before any generation happens at all and this matters clearly for interactive applications if you have",
    "start": "196640",
    "end": "202640"
  },
  {
    "text": "a big prompt and then you have to wait there for 10 seconds that may not be a good user experience latency is how fast",
    "start": "202640",
    "end": "209440"
  },
  {
    "text": "tokens are arriving after maybe the first first token um this also matters",
    "start": "209440",
    "end": "215920"
  },
  {
    "text": "for interactive um applications throughput is is something a bit different throughput is um how",
    "start": "215920",
    "end": "222720"
  },
  {
    "text": "many tokens in general are generated per not for overall users so this is it's",
    "start": "222720",
    "end": "228480"
  },
  {
    "text": "particularly useful in batch processing applications so you can think about the throughput is um high throughput doesn't",
    "start": "228480",
    "end": "234599"
  },
  {
    "text": "mean low latency because uh some you know requests might just take a very",
    "start": "234599",
    "end": "240319"
  },
  {
    "text": "long time and you still have high throughput latency is is kind of like the you know worst case over any individual",
    "start": "240319",
    "end": "247159"
  },
  {
    "text": "user so what do you need to uh think about when you think about the efficiency of inference um so in",
    "start": "247159",
    "end": "254319"
  },
  {
    "text": "training um the the key idea is that you get to see all the tokens at least a supervised",
    "start": "254319",
    "end": "261040"
  },
  {
    "text": "training which means that you can parallelize over the the sequence this is exploited heavily in the transformer",
    "start": "261040",
    "end": "267120"
  },
  {
    "text": "right so you've done the transformer training you know that you basically construct these tensors over the entire",
    "start": "267120",
    "end": "272160"
  },
  {
    "text": "sequence and it's just like tensor tensor tensor uh you know mats and then you get your output but the the key",
    "start": "272160",
    "end": "280240"
  },
  {
    "text": "defining feature of inference at least for transformers is that you have to generate sequentially you can't",
    "start": "280240",
    "end": "286000"
  },
  {
    "text": "parallelize because um the generation of a a token depends on all the past so",
    "start": "286000",
    "end": "293600"
  },
  {
    "text": "this is going to be the key thing that's going to make inference a lot harder and",
    "start": "293600",
    "end": "298639"
  },
  {
    "text": "in particular it's going to be harder to utilize all the compute that's uh available and it's going to be memory",
    "start": "298639",
    "end": "303759"
  },
  {
    "text": "limited as we'll see uh in detail later so a lot of people are doing inference",
    "start": "303759",
    "end": "308960"
  },
  {
    "text": "um anyone who's actually has a product and platform quickly realizes that these",
    "start": "308960",
    "end": "314080"
  },
  {
    "text": "uh cost in doing large models is going to go up so they spent a lot of uh time",
    "start": "314080",
    "end": "319120"
  },
  {
    "text": "and you know engineering effort trying to reduce that time so both providers",
    "start": "319120",
    "end": "324240"
  },
  {
    "text": "serving closed models and providers serve open m open weight models uh pay a",
    "start": "324240",
    "end": "329600"
  },
  {
    "text": "lot of attention to um to imprints um more so than I think the average",
    "start": "329600",
    "end": "335039"
  },
  {
    "text": "academic because we're not actually serving any models we're just training and getting a score and putting in the paper but people who are actually",
    "start": "335039",
    "end": "341120"
  },
  {
    "text": "serving models pay a lot of attention to inference um so there's also a bunch of open",
    "start": "341120",
    "end": "346479"
  },
  {
    "text": "source packages which are um interesting to to look at uh as",
    "start": "346479",
    "end": "352120"
  },
  {
    "text": "well okay so um I want to understand the inference workload uh kind of in detail",
    "start": "352120",
    "end": "360720"
  },
  {
    "text": "so I'm going to review briefly um the sort of this transformer math that we uh",
    "start": "360720",
    "end": "366479"
  },
  {
    "text": "um you did in assignment one and we talked a little bit about it during the the first week of of class so this is",
    "start": "366479",
    "end": "372960"
  },
  {
    "text": "from the scaling um JXML book which um",
    "start": "372960",
    "end": "379039"
  },
  {
    "text": "is is something you guys should really take a look at i think it does an excellent job of outlining many of the",
    "start": "379039",
    "end": "384080"
  },
  {
    "text": "of the key concepts um here um and they have this really nice diagram that shows",
    "start": "384080",
    "end": "390560"
  },
  {
    "text": "essentially the the computation graph taking an input and having it go through",
    "start": "390560",
    "end": "396240"
  },
  {
    "text": "attention and the MLP layers um in particular um we're going to use this notation so just to kind of review this",
    "start": "396240",
    "end": "403280"
  },
  {
    "text": "this quickly so B is the number of sequences in your batch l is the number of layers t is the sequence length uh",
    "start": "403280",
    "end": "411520"
  },
  {
    "text": "you can think about as the number of tokens you're going to generate u or or",
    "start": "411520",
    "end": "416560"
  },
  {
    "text": "query using s is also the sequence length but how many you're um kind of conditioning on in your prompt v is the",
    "start": "416560",
    "end": "423199"
  },
  {
    "text": "vocabulary d is the dimensionality of model f is the MLP hidden dimension",
    "start": "423199",
    "end": "428639"
  },
  {
    "text": "which is usually four times D um h is the attention head dimension n is the",
    "start": "428639",
    "end": "434479"
  },
  {
    "text": "number of query heads so generally N times H equals D um and then in",
    "start": "434479",
    "end": "440440"
  },
  {
    "text": "GQA group query attention you have a different number of key value heads as query heads usually K is smaller than N",
    "start": "440440",
    "end": "448639"
  },
  {
    "text": "and um G is the number of um of groups um so K * G equals N okay okay and this",
    "start": "448639",
    "end": "456720"
  },
  {
    "text": "diagram shows that you take your X you feed through the the Q KV matrices",
    "start": "456720",
    "end": "464479"
  },
  {
    "text": "um and you do a bunch of bunch of things um okay",
    "start": "464479",
    "end": "472199"
  },
  {
    "text": "so so remember that the flops required for a fever pass is six times the number",
    "start": "472199",
    "end": "478400"
  },
  {
    "text": "of of tokens which is B * T times the number of parameters uh plus for the",
    "start": "478400",
    "end": "484319"
  },
  {
    "text": "tension there's another order t so t * t is t ^2 um",
    "start": "484319",
    "end": "490520"
  },
  {
    "text": "dependence okay so let's also review arithmetic and intensity which is going",
    "start": "490520",
    "end": "496240"
  },
  {
    "text": "to help us characterize when something is compute limited versus memory limited um so just to start with the basic you",
    "start": "496240",
    "end": "503599"
  },
  {
    "text": "know u map mall so let's take a matrix X which is B by B by D and a matrix W D by",
    "start": "503599",
    "end": "511560"
  },
  {
    "text": "F and you know just to give some color to this uh computation B is the batch",
    "start": "511560",
    "end": "518719"
  },
  {
    "text": "size D is the hidden dimension and F is the up projection matrix in the gated MLP um so next let's do count the number",
    "start": "518719",
    "end": "527120"
  },
  {
    "text": "of flops and memory um read and writes u for just doing X times W okay so we're",
    "start": "527120",
    "end": "535600"
  },
  {
    "text": "going to start with um initialize to zero and what one has to do for this is",
    "start": "535600",
    "end": "541440"
  },
  {
    "text": "we're going to um read X from HBM so you",
    "start": "541440",
    "end": "548000"
  },
  {
    "text": "that means it incurs a memory cost of 2 * B * D assuming everything is in BF 16",
    "start": "548000",
    "end": "554160"
  },
  {
    "text": "um you also read W so that's 2 * D * F",
    "start": "554160",
    "end": "559600"
  },
  {
    "text": "um then you do the the mat mole and that encurs 2 * um b * d * f flops so",
    "start": "559600",
    "end": "568000"
  },
  {
    "text": "remember this is from the first lecture so hopefully this is review um and then you have to write it back out which is",
    "start": "568000",
    "end": "574240"
  },
  {
    "text": "you uh have to pay another transfer okay so the total number of flops is uh just",
    "start": "574240",
    "end": "581279"
  },
  {
    "text": "the the mammal and the number of bytes transferred is essentially the size of all the the matrices that are read and",
    "start": "581279",
    "end": "588399"
  },
  {
    "text": "written and arithmetic intensity is basically the ratio um so the ratio is",
    "start": "588399",
    "end": "594480"
  },
  {
    "text": "this expression um and in general just to simplify things a bit um generally",
    "start": "594480",
    "end": "600399"
  },
  {
    "text": "the batch size is much less than D and F um B maybe you know hundreds and a DNF",
    "start": "600399",
    "end": "607519"
  },
  {
    "text": "might be you know thousands or tens of thousands um so I'm using simp here just to keep",
    "start": "607519",
    "end": "615760"
  },
  {
    "text": "myself from making silly mistakes um so basically I'm letting C go to infinity",
    "start": "615760",
    "end": "622720"
  },
  {
    "text": "um and D scales as C * B and F scales as C * B and that gets you a simplified",
    "start": "622720",
    "end": "629760"
  },
  {
    "text": "equation of B okay so the arithmetic intensity is B for this particular",
    "start": "629760",
    "end": "636480"
  },
  {
    "text": "matrix multiplication um and um and the",
    "start": "636480",
    "end": "643320"
  },
  {
    "text": "okay so the way to interpret this is how",
    "start": "643320",
    "end": "648399"
  },
  {
    "text": "many flops are done per bite that was transferred so now um the second part is you look at",
    "start": "648399",
    "end": "656560"
  },
  {
    "text": "the accelerator which for H100 flops per second is um",
    "start": "656560",
    "end": "663240"
  },
  {
    "text": "98989 teraflops memory bandwidth 3.3 ter",
    "start": "663240",
    "end": "668320"
  },
  {
    "text": "bytes per second and you divide and that gives you what is called the accelerator",
    "start": "668320",
    "end": "674360"
  },
  {
    "text": "intensity and um if you look at the computational intensity um which is B if",
    "start": "674360",
    "end": "681760"
  },
  {
    "text": "it's greater accelerated intensity that means you're compute limited that means you're able to use all the the GPUs or",
    "start": "681760",
    "end": "686880"
  },
  {
    "text": "TPUs and if you're less than that then you're memory limited which is uh you",
    "start": "686880",
    "end": "692640"
  },
  {
    "text": "know bad and so you're computer limited in this matrix multiplication case if B",
    "start": "692640",
    "end": "697760"
  },
  {
    "text": "is greater than uh 295 uh for a H100 and all of this is a bit idealized the",
    "start": "697760",
    "end": "704480"
  },
  {
    "text": "actual details there's it's this is giving you a kind of a first order approximation",
    "start": "704480",
    "end": "710959"
  },
  {
    "text": "um so in extreme extreme case so generally if you that means if you use batches of size let's say 300 then",
    "start": "710959",
    "end": "717519"
  },
  {
    "text": "you'll be be able to saturate the GPU but what happens if your batch is really small so in particular B equals 1 which",
    "start": "717519",
    "end": "724560"
  },
  {
    "text": "essentially corresponds to a matrix vector product then the arithmetic intensity is basically one and that is",
    "start": "724560",
    "end": "731360"
  },
  {
    "text": "really really bad that means you're going to be um memory limited and which",
    "start": "731360",
    "end": "738079"
  },
  {
    "text": "which kind of makes sense because basically you're reading and writing this D times or actually you're just",
    "start": "738079",
    "end": "744560"
  },
  {
    "text": "reading this D* F matrix and you're performing essentially in the same number of flops right so the ratio",
    "start": "744560",
    "end": "750560"
  },
  {
    "text": "between the flops and the reads is the same which is gives you one and one is bad you want a lot of flops to be done",
    "start": "750560",
    "end": "756639"
  },
  {
    "text": "for any memory read because memory reads are uh slow But this is in in essence what",
    "start": "756639",
    "end": "764000"
  },
  {
    "text": "happens with generation right because you're proceeding token by token we'll see that basically um your arithmetic",
    "start": "764000",
    "end": "771120"
  },
  {
    "text": "intensity is going to be like one and that's why uh generation is going to be",
    "start": "771120",
    "end": "776320"
  },
  {
    "text": "memory limited um and not compute limited okay so this is a very simple example that I think gets at the core of",
    "start": "776320",
    "end": "782800"
  },
  {
    "text": "why uh generation is going to be slow so maybe I'll pause and take any questions",
    "start": "782800",
    "end": "788240"
  },
  {
    "text": "on this um just to make sure everyone's clear",
    "start": "788240",
    "end": "793440"
  },
  {
    "text": "I mean when doing why don't we have a batch size of say larger than one so I",
    "start": "794880",
    "end": "800720"
  },
  {
    "text": "think I heard the question why don't we have a batch size more than one so I'll get to why you can uh but there's um",
    "start": "800720",
    "end": "806800"
  },
  {
    "text": "batch size is going to mean batch size time sequence length um",
    "start": "806800",
    "end": "812399"
  },
  {
    "text": "later okay so in summary um matrix multiplications are the kind",
    "start": "816040",
    "end": "823120"
  },
  {
    "text": "of the core computation so we just studied a matrix multiplication and counted the number of flops it requires",
    "start": "823120",
    "end": "829600"
  },
  {
    "text": "over the number of read and writes and we show that that ratio which is",
    "start": "829600",
    "end": "835360"
  },
  {
    "text": "arithmetic intensity depends on one of the dimensions in case in this case the batch dimension and that's why big",
    "start": "835360",
    "end": "842320"
  },
  {
    "text": "matrices are are good because that can saturate your um your compute whereas if you have even a thin matrix um B equals",
    "start": "842320",
    "end": "849839"
  },
  {
    "text": "one that's really bad because you're spending a lot of time reading",
    "start": "849839",
    "end": "854880"
  },
  {
    "text": "from memory and not doing that much uh compute okay so um now let's talk about",
    "start": "854880",
    "end": "863120"
  },
  {
    "text": "the arithmetic intensity of of inference um okay so let's let's just uh kind of",
    "start": "863120",
    "end": "871279"
  },
  {
    "text": "get more into the weeds of what inference looks like so so the naive thing you can imagine doing and all",
    "start": "871279",
    "end": "878160"
  },
  {
    "text": "these nice pictures are taken from this this book um is that you have a",
    "start": "878160",
    "end": "883240"
  },
  {
    "text": "transformer you give the prompt in you gives you logits over the the vocabulary",
    "start": "883240",
    "end": "890880"
  },
  {
    "text": "of the next token and you just sample from that and then once you get that you",
    "start": "890880",
    "end": "895920"
  },
  {
    "text": "attach it to the prompt and then you feed it through the transformer and you",
    "start": "895920",
    "end": "901519"
  },
  {
    "text": "look at the logic sample again and you repeat right so that's um the sort of",
    "start": "901519",
    "end": "907279"
  },
  {
    "text": "most naive thing to do um and you know",
    "start": "907279",
    "end": "912639"
  },
  {
    "text": "the complexity here is is pretty bad because each token you generate is like",
    "start": "912639",
    "end": "919199"
  },
  {
    "text": "n squ or t squ um you know computation through the through the",
    "start": "919199",
    "end": "926120"
  },
  {
    "text": "transformer okay so that that's no good but if you look at this closely you'll",
    "start": "926120",
    "end": "932720"
  },
  {
    "text": "notice that you're doing a lot of redundant work right all these um the",
    "start": "932720",
    "end": "938800"
  },
  {
    "text": "work um in basically encoding the prefix",
    "start": "938800",
    "end": "944320"
  },
  {
    "text": "basically stays the same so this is for a uh for a birectional uh",
    "start": "944320",
    "end": "951040"
  },
  {
    "text": "transformer it would be different but at least for a auto reggressive causal transformer it is the case that you",
    "start": "951040",
    "end": "957839"
  },
  {
    "text": "should be able to share a lot between prefixes and so the solution is you cache and you cache in the IBM HPM",
    "start": "957839",
    "end": "966480"
  },
  {
    "text": "because that's uh where you have enough space to store stuff so this is what it looks like if you have a KV cache in",
    "start": "966480",
    "end": "973279"
  },
  {
    "text": "schematically um so you take your prompt the prefill step",
    "start": "973279",
    "end": "979920"
  },
  {
    "text": "is you feed it through the transformer and you compute this KV cache",
    "start": "979920",
    "end": "985120"
  },
  {
    "text": "um and then you generate the logits over the next token and then you",
    "start": "985120",
    "end": "991199"
  },
  {
    "text": "um put that into um you know take that generated token and the the cache and",
    "start": "991199",
    "end": "998399"
  },
  {
    "text": "then you can feed it through the transformer but you've already computed these so you don't have to do that again you just need to compute this new um KV",
    "start": "998399",
    "end": "1005360"
  },
  {
    "text": "uh vector for this token and now that allows you to more quickly generate the",
    "start": "1005360",
    "end": "1010720"
  },
  {
    "text": "next um token and and so on so basically you're filling up this KV cache which",
    "start": "1010720",
    "end": "1016800"
  },
  {
    "text": "corresponds to the tokens that you've either prefilled with or that you've",
    "start": "1016800",
    "end": "1021920"
  },
  {
    "text": "generated so far okay so instead of T squ per token it's going to be more like",
    "start": "1021920",
    "end": "1029319"
  },
  {
    "text": "T okay so concretely the KV cache is for every sequence in your batch for every",
    "start": "1029319",
    "end": "1035038"
  },
  {
    "text": "token in your sequence for every layer of the transformer for every head you're going to store a hdimensional uh",
    "start": "1035039",
    "end": "1041720"
  },
  {
    "text": "vector so you might think that this is going to take a lot of memory and you wouldn't be",
    "start": "1041720",
    "end": "1046839"
  },
  {
    "text": "wrong okay so there's two stages of inference so prefill is you're given your prompt encoded a vector so this is",
    "start": "1046839",
    "end": "1054640"
  },
  {
    "text": "just like what you do in training it's paralyzable it's fast your compute limited life is good and then you're",
    "start": "1054640",
    "end": "1060400"
  },
  {
    "text": "doing generation which is you're generating response tokens one by one sequentially and this is a part that's",
    "start": "1060400",
    "end": "1066480"
  },
  {
    "text": "going to give us a lot of trouble in terms of uh efficiency so now let's compute the",
    "start": "1066480",
    "end": "1072960"
  },
  {
    "text": "flops and memory IO for both the for the transformer so we're going to break it down into MLP layers and the attention",
    "start": "1072960",
    "end": "1079760"
  },
  {
    "text": "layers um and um just notationwise we're",
    "start": "1079760",
    "end": "1085600"
  },
  {
    "text": "going to do this computation with s being the number of tokens we're conditioning on think about the length",
    "start": "1085600",
    "end": "1090640"
  },
  {
    "text": "of the prompt and T is the number of tokens we're generating or or quering",
    "start": "1090640",
    "end": "1096480"
  },
  {
    "text": "using um and in prefill t is going to be s because we're sort of I mean we're not",
    "start": "1096480",
    "end": "1103120"
  },
  {
    "text": "generating uh t tokens but we're sort of like querying um using each of these",
    "start": "1103120",
    "end": "1108840"
  },
  {
    "text": "tokens and a generation where t is just one okay so hopefully the matrix",
    "start": "1108840",
    "end": "1116480"
  },
  {
    "text": "multiplication is still fresh in your head because this is going to be essentially that but a little bit more",
    "start": "1116480",
    "end": "1121919"
  },
  {
    "text": "complicated because it's a transformer so we're going to count the flops and bytes generated um so first we're going",
    "start": "1121919",
    "end": "1128880"
  },
  {
    "text": "to um take x which is a um b by t by d u",
    "start": "1128880",
    "end": "1135919"
  },
  {
    "text": "matrix i think maybe these T's should be S's um but um anyway so um so that",
    "start": "1135919",
    "end": "1143520"
  },
  {
    "text": "involves doing a bunch of uh transfers um uh basically the size of that matrix",
    "start": "1143520",
    "end": "1150000"
  },
  {
    "text": "times two because BF 16 um then there's the the three-way matrices the up",
    "start": "1150000",
    "end": "1156320"
  },
  {
    "text": "projection the gate and the down projection they're all the same um you know size up to transposition um so you",
    "start": "1156320",
    "end": "1164400"
  },
  {
    "text": "need to transfer those um then you do the",
    "start": "1164400",
    "end": "1170240"
  },
  {
    "text": "um you know up projection um that's some number of flops so B * D * C* F so",
    "start": "1170240",
    "end": "1177919"
  },
  {
    "text": "whenever you multiply you know two tensors basically the contracting dimension only gets counted once whereas",
    "start": "1177919",
    "end": "1184160"
  },
  {
    "text": "other dimensions you just you kind of gather together um you need to write it out you also have the gate which is the",
    "start": "1184160",
    "end": "1192000"
  },
  {
    "text": "uh same same thing you write it out you compute uh your um nonlinearity you",
    "start": "1192000",
    "end": "1200320"
  },
  {
    "text": "multiply some stuff and you down project um and that's b * t * d * f which is",
    "start": "1200320",
    "end": "1207679"
  },
  {
    "text": "basically the same number of flops and you write um out uh the the result okay",
    "start": "1207679",
    "end": "1214640"
  },
  {
    "text": "okay so if you look at the the counting um I guess maybe I'll just uh you know",
    "start": "1214640",
    "end": "1221120"
  },
  {
    "text": "you can check the um you know results actually you don't need to check it because this is simp and it's guaranteed",
    "start": "1221120",
    "end": "1227360"
  },
  {
    "text": "to be you know correct um so but again",
    "start": "1227360",
    "end": "1233120"
  },
  {
    "text": "we're going to assume that B * D is much smaller than DN and F and we get that",
    "start": "1233120",
    "end": "1238880"
  },
  {
    "text": "the intensity is B * T okay so this is analogous to the matrix uh",
    "start": "1238880",
    "end": "1244320"
  },
  {
    "text": "multiplication case where the in arithmetic intensity which we want to be high depends on how large your batches",
    "start": "1244320",
    "end": "1251039"
  },
  {
    "text": "and how many tokens you're you're essentially generating okay okay so now if you look at the two stages prefill",
    "start": "1251039",
    "end": "1258559"
  },
  {
    "text": "life is good remember because we can just make BT large enough you use a batch size even a batch size of one",
    "start": "1258559",
    "end": "1265520"
  },
  {
    "text": "actually is is maybe okay if you have long enough uh sequence um so that's not",
    "start": "1265520",
    "end": "1271120"
  },
  {
    "text": "a problem now generation this is where it becomes a little bit harder because",
    "start": "1271120",
    "end": "1277280"
  },
  {
    "text": "you're generating one token at a time so T is one right so if t is one that means for",
    "start": "1277280",
    "end": "1284400"
  },
  {
    "text": "bt to be large you need b to be large and b is essentially the number of concurrent",
    "start": "1284400",
    "end": "1290200"
  },
  {
    "text": "requests okay so this is kind of interesting because your your sort of efficiency depends on um having large",
    "start": "1290200",
    "end": "1297280"
  },
  {
    "text": "batch sizes because I mean intuitively it makes sense if you can take a lot of requests batch them together then uh you",
    "start": "1297280",
    "end": "1303440"
  },
  {
    "text": "can get um you know better uh efficiency at least throughput um but this also",
    "start": "1303440",
    "end": "1311440"
  },
  {
    "text": "depends on what um B is because if it's um you're only getting a few requests at",
    "start": "1311440",
    "end": "1317039"
  },
  {
    "text": "a time then you're not going to be able to use your hardware very efficiently um and this is talks speaks",
    "start": "1317039",
    "end": "1324480"
  },
  {
    "text": "to the sort of the very dynamic aspect of inference which we'll come back to later in the",
    "start": "1324480",
    "end": "1329640"
  },
  {
    "text": "lecture okay so now what about attention turns out attention is even worse um for",
    "start": "1329640",
    "end": "1336320"
  },
  {
    "text": "reasons I'll I'll try to get into so let's do the counting flops um bytes",
    "start": "1336320",
    "end": "1342360"
  },
  {
    "text": "transferred okay so I'm going to read the QKV",
    "start": "1342360",
    "end": "1347640"
  },
  {
    "text": "um matrices from HPM i'm going to compute the the tension",
    "start": "1347640",
    "end": "1356640"
  },
  {
    "text": "which is a matrix which is Q * K and the number of flops is B * S * T * D so",
    "start": "1356640",
    "end": "1364720"
  },
  {
    "text": "remember S and T are the same during a prefill so that's your um sequence length squared times b * d and then um",
    "start": "1364720",
    "end": "1375039"
  },
  {
    "text": "I'm I'm sort of only looking at the matrix multiplications because the flops from the other steps don't really matter",
    "start": "1375039",
    "end": "1381200"
  },
  {
    "text": "um and then you project out to your uh oh sorry you you take a combination of",
    "start": "1381200",
    "end": "1386880"
  },
  {
    "text": "this and v so actually this is mathematically incorrect because there's some soft maxes there but the essence of",
    "start": "1386880",
    "end": "1394400"
  },
  {
    "text": "the map malls are are the are the same so that's the same number of flops and then you write to",
    "start": "1394400",
    "end": "1402120"
  },
  {
    "text": "HPM okay so and here I'm assuming there'll be more bytes transferred if",
    "start": "1402120",
    "end": "1408720"
  },
  {
    "text": "you didn't use fly flash attention flash attention means that you don't have to keep on writing back to HPM intermediate",
    "start": "1408720",
    "end": "1415200"
  },
  {
    "text": "steps so um but the order is actually not u really affected",
    "start": "1415200",
    "end": "1421200"
  },
  {
    "text": "so qualitatively doesn't really uh matter whether you use flash attention or not but the the math here depends on",
    "start": "1421200",
    "end": "1427360"
  },
  {
    "text": "the con the constants matter um but let's look at the flops and the bytes",
    "start": "1427360",
    "end": "1432520"
  },
  {
    "text": "transferred and if you uh divide and simplify you get this rather nice",
    "start": "1432520",
    "end": "1439880"
  },
  {
    "text": "expression i mean nice in that it's simple not nice in that it's uh it's good efficiency which is S * T / S plus",
    "start": "1439880",
    "end": "1449320"
  },
  {
    "text": "T okay so let's try to interpret this a bit so in",
    "start": "1449320",
    "end": "1455400"
  },
  {
    "text": "prefill T equals S so that means your prefill intensity",
    "start": "1455400",
    "end": "1460880"
  },
  {
    "text": "is order S so that's good right because as long as you have long enough uh you",
    "start": "1460880",
    "end": "1466960"
  },
  {
    "text": "know sequences then uh you're you're you know good to go right and generally the",
    "start": "1466960",
    "end": "1473279"
  },
  {
    "text": "sequences can you assume are kind of long enough um during generation however",
    "start": "1473279",
    "end": "1479520"
  },
  {
    "text": "um you'll see that the entity is uh essentially one s over s+ one but that's",
    "start": "1479520",
    "end": "1486080"
  },
  {
    "text": "basically one um and remember one is is really bound",
    "start": "1486080",
    "end": "1492400"
  },
  {
    "text": "okay so but but notice like what there's no dependence on V at",
    "start": "1492400",
    "end": "1498440"
  },
  {
    "text": "all so unlike in MLPS remember in MLPS um the the generation the prefill was BT",
    "start": "1498440",
    "end": "1508000"
  },
  {
    "text": "which is great um and then arithmetic intensity was B which was not great",
    "start": "1508000",
    "end": "1513600"
  },
  {
    "text": "because it depends on the whims of your users and workloads but still could be",
    "start": "1513600",
    "end": "1518880"
  },
  {
    "text": "larger than one whereas um for attention it's actually just always less than one",
    "start": "1518880",
    "end": "1524880"
  },
  {
    "text": "no matter how many how long your sequences there are how how how many users there are um it's always one so",
    "start": "1524880",
    "end": "1531360"
  },
  {
    "text": "why why is this intuitively that there's no dependence on on B the batch",
    "start": "1531360",
    "end": "1538120"
  },
  {
    "text": "dimension so the reason is that in the MLP layers intuitively every sequence",
    "start": "1538120",
    "end": "1544400"
  },
  {
    "text": "hits the same MLP weights so whereas in attention layer each",
    "start": "1544400",
    "end": "1551440"
  },
  {
    "text": "sequence has its own uh KV cache right because the KV cache is sequence",
    "start": "1551440",
    "end": "1558360"
  },
  {
    "text": "specific which means that you you can't really you know",
    "start": "1558360",
    "end": "1563960"
  },
  {
    "text": "uh use in ML case you can kind of read all the weights and then you process a batch intuitively whereas in attention",
    "start": "1563960",
    "end": "1570960"
  },
  {
    "text": "case um every sequence kind of requires additional memory you don't get any kind of savings if you um if you kind of",
    "start": "1570960",
    "end": "1578960"
  },
  {
    "text": "batch them up mathematically I guess you can look at it through here where um the",
    "start": "1578960",
    "end": "1584880"
  },
  {
    "text": "number of flops there's a b here which is expected but the number of bytes transferred is um you know b times um",
    "start": "1584880",
    "end": "1593039"
  },
  {
    "text": "there's a scaling in in b so when you divide that b uh",
    "start": "1593039",
    "end": "1598360"
  },
  {
    "text": "cancels um whereas over here um there is a b here but um we're assuming the df f",
    "start": "1598360",
    "end": "1607120"
  },
  {
    "text": "dominates so when you divide um basically there's no b essentially left",
    "start": "1607120",
    "end": "1612320"
  },
  {
    "text": "in the the denominator so you can look at it mathematically or you can just kind of reason about it intuitively as",
    "start": "1612320",
    "end": "1620039"
  },
  {
    "text": "um during for the attention the kv cache is sort of every sequence's own unique",
    "start": "1620039",
    "end": "1627520"
  },
  {
    "text": "uh snowflake okay so the summary is prefill",
    "start": "1627520",
    "end": "1632880"
  },
  {
    "text": "is compute limited whereas generation is memory limited um the MLP arithmetic",
    "start": "1632880",
    "end": "1639279"
  },
  {
    "text": "intensity is B um which to make good enough you need bunch of concurrent",
    "start": "1639279",
    "end": "1644480"
  },
  {
    "text": "requests but attention intensity is one which and it's also impossible to improve",
    "start": "1644480",
    "end": "1649720"
  },
  {
    "text": "that okay i'll pause bit for any any",
    "start": "1649720",
    "end": "1655278"
  },
  {
    "text": "questions okay so let's let's move on um so now we know that inference it due",
    "start": "1655799",
    "end": "1664400"
  },
  {
    "text": "thanks to generation is memory limited let's uh try to study the throughput and",
    "start": "1664400",
    "end": "1670400"
  },
  {
    "text": "latency um um at least in in theory um so let's focus on",
    "start": "1670400",
    "end": "1680600"
  },
  {
    "text": "um let's see actually okay so um we're going to make some assumptions so all of",
    "start": "1680840",
    "end": "1687360"
  },
  {
    "text": "this is sort of napkin math is a little bit stylized um but it gives you roughly the right kind of scaling and the right",
    "start": "1687360",
    "end": "1694880"
  },
  {
    "text": "way to think about things so we're going to assume that commu communication and compute can be perfectly",
    "start": "1694880",
    "end": "1700440"
  },
  {
    "text": "um overlapped which is obviously false but it's it's good enough for for making",
    "start": "1700440",
    "end": "1707039"
  },
  {
    "text": "these qualitative uh um estimates um so what we're going to do is we're going to",
    "start": "1707039",
    "end": "1713760"
  },
  {
    "text": "instantiate the latency and throughput for a llama 2 13B on H100 okay so for a",
    "start": "1713760",
    "end": "1721360"
  },
  {
    "text": "13B um here are the values so the se let's",
    "start": "1721360",
    "end": "1726399"
  },
  {
    "text": "just put the sequence length to be a th00and hidden dimension to be model dimension to be 5,000 four times uh I",
    "start": "1726399",
    "end": "1733600"
  },
  {
    "text": "don't know if that's that's not four times but anyway f is um some multiple of that number of heads number of uh key",
    "start": "1733600",
    "end": "1741320"
  },
  {
    "text": "value um I guess uh query heads number of key value heads which for llama 2 is",
    "start": "1741320",
    "end": "1747760"
  },
  {
    "text": "the same we'll get to that point later um and and so on and for the memory bandwidth of H100 that's the",
    "start": "1747760",
    "end": "1754960"
  },
  {
    "text": "Okay so that's uh the config and we're going to compute the the memory latency",
    "start": "1754960",
    "end": "1762240"
  },
  {
    "text": "and throughput okay so Oh okay",
    "start": "1762240",
    "end": "1769360"
  },
  {
    "text": "um so first uh let's just quickly get the number of parameters you guys did",
    "start": "1769559",
    "end": "1775120"
  },
  {
    "text": "this in assignment one so I won't bel labor this but it's some expression um",
    "start": "1775120",
    "end": "1780799"
  },
  {
    "text": "that depends on all the different knowh um and uh the to store the parameters",
    "start": "1780799",
    "end": "1788960"
  },
  {
    "text": "we're going to use f B bf16 um because inference is generally going",
    "start": "1788960",
    "end": "1794240"
  },
  {
    "text": "to be 16 bit not 32-bit um so we're going to multiply between two so that's the memory that the parameters take",
    "start": "1794240",
    "end": "1802880"
  },
  {
    "text": "okay we don't need gradients we don't need optimizer states because we're not training but we do have to store the KV",
    "start": "1802880",
    "end": "1810200"
  },
  {
    "text": "cache which are the act some of the activations um not all the activations",
    "start": "1810200",
    "end": "1815919"
  },
  {
    "text": "but some of them for every sequence of length s and how much we have to per",
    "start": "1815919",
    "end": "1821600"
  },
  {
    "text": "store per sequence it's basically the sequence lengths times the number of key",
    "start": "1821600",
    "end": "1828000"
  },
  {
    "text": "value uh um heads times the dimension of that head times the number of layers times um",
    "start": "1828000",
    "end": "1836480"
  },
  {
    "text": "basically two for basically both a key and a value and two for BF 16 okay so",
    "start": "1836480",
    "end": "1842320"
  },
  {
    "text": "that's how much the cache size uh takes um and so the total memory is batch size",
    "start": "1842320",
    "end": "1849440"
  },
  {
    "text": "times the cache per sequence plus the memories uh plus the parameter size so",
    "start": "1849440",
    "end": "1856159"
  },
  {
    "text": "now latency is going to be determined by memory remember it's memory limited so",
    "start": "1856159",
    "end": "1863679"
  },
  {
    "text": "we're just going to compute how much memory needs to be transferred",
    "start": "1863679",
    "end": "1869120"
  },
  {
    "text": "uh into the GPU to do this computation is simply memory over the memory bandwidth um and throughput is",
    "start": "1869120",
    "end": "1877360"
  },
  {
    "text": "essentially the inverse of latency but scaled up by B because we're looking at generating B tokens in in",
    "start": "1877360",
    "end": "1884919"
  },
  {
    "text": "parallel okay so now if we substitute our um llama 2 config um we'll see that",
    "start": "1884919",
    "end": "1893600"
  },
  {
    "text": "the number of parameters checks out it's it's um you know 13 billion roughly the",
    "start": "1893600",
    "end": "1899480"
  },
  {
    "text": "memory latency and throughput have these expressions um so memory you know grows obviously",
    "start": "1899480",
    "end": "1906799"
  },
  {
    "text": "this is the parameter size this is the key value uh cache size times times B",
    "start": "1906799",
    "end": "1913120"
  },
  {
    "text": "latency also goes up as a function of B throughput um um increases but you'll see that it",
    "start": "1913120",
    "end": "1920320"
  },
  {
    "text": "increases up to a point the B shows up in both the numerator and the denominator um so there's limits to how",
    "start": "1920320",
    "end": "1925600"
  },
  {
    "text": "much you can stretch throughput even if you could fit everything in memory okay so those are um the",
    "start": "1925600",
    "end": "1932360"
  },
  {
    "text": "expressions uh for latency throughput and memory for this particular model so",
    "start": "1932360",
    "end": "1939840"
  },
  {
    "text": "now let's instantiate with different batch sizes so if B equals 1",
    "start": "1939840",
    "end": "1945279"
  },
  {
    "text": "um then the latency is about 8 milliseconds so 8 mill every 8",
    "start": "1945279",
    "end": "1951360"
  },
  {
    "text": "milliseconds you generate a token and the throughput is 124 uh tokens uh per",
    "start": "1951360",
    "end": "1957799"
  },
  {
    "text": "second okay so that's 13B on a H100 um if you're using batch size of one so now",
    "start": "1957799",
    "end": "1965279"
  },
  {
    "text": "what happens if you use um batch size of 16",
    "start": "1965279",
    "end": "1970399"
  },
  {
    "text": "so you'll see that the um the memory usage increases because you need to",
    "start": "1970399",
    "end": "1976480"
  },
  {
    "text": "store the KV cache for all 64 sequences now the latency goes up um because you",
    "start": "1976480",
    "end": "1985600"
  },
  {
    "text": "kind of have to instead of just processing one you have to kind of wait for everything to finish um but the",
    "start": "1985600",
    "end": "1991840"
  },
  {
    "text": "throughput also goes up actually quite a lot okay so you're seeing kind of this",
    "start": "1991840",
    "end": "1997679"
  },
  {
    "text": "uh immediate trade-off between latency and throughput if you want low latency you just use one um one uh B equals 1",
    "start": "1997679",
    "end": "2006480"
  },
  {
    "text": "but if you want high throughput you want larger B in general um what happens if you use a batch size of even larger so",
    "start": "2006480",
    "end": "2015159"
  },
  {
    "text": "256 um you'll see that the latency goes up throughput goes up but you see that",
    "start": "2015159",
    "end": "2021440"
  },
  {
    "text": "throughput isn't going up that much because you get diminishing returns after a while but the most kind of uh you can't",
    "start": "2021440",
    "end": "2028960"
  },
  {
    "text": "actually do this on a 100 because if you look at the uh the memory it's 240 uh",
    "start": "2028960",
    "end": "2035000"
  },
  {
    "text": "gigs so it doesn't even fit okay so batch size you can only",
    "start": "2035000",
    "end": "2040960"
  },
  {
    "text": "increase to a certain point um because of memory okay so just to recap there's",
    "start": "2040960",
    "end": "2046799"
  },
  {
    "text": "a trade-off between latency and throughput smaller batch sizes yield battery latency larger batch sizes yield",
    "start": "2046799",
    "end": "2053440"
  },
  {
    "text": "better uh throughput and finally there's uh we",
    "start": "2053440",
    "end": "2058480"
  },
  {
    "text": "talked last week talked about parallelism for training and it was you know kind of complicated annoying um at",
    "start": "2058480",
    "end": "2066398"
  },
  {
    "text": "least one type of parallelism for inferences is really really nice and",
    "start": "2066399",
    "end": "2071638"
  },
  {
    "text": "simple um you just launch M copies of the model okay no communication because",
    "start": "2071639",
    "end": "2077679"
  },
  {
    "text": "the model you don't need to update the models the latency is the same and the throughput increases by m so that's",
    "start": "2077679",
    "end": "2084720"
  },
  {
    "text": "that's pretty good so always remember that uh you know don't forget easy",
    "start": "2084720",
    "end": "2090079"
  },
  {
    "text": "things um now there are cases where you if you have a large enough model then uh",
    "start": "2090079",
    "end": "2097200"
  },
  {
    "text": "maybe it doesn't even fit on a single uh GPU and you need to shard the model and",
    "start": "2097200",
    "end": "2102400"
  },
  {
    "text": "in this case you also want to uh start sharding the KV cache in some cases um",
    "start": "2102400",
    "end": "2108079"
  },
  {
    "text": "to get you know better you know efficiency so um there's for more details uh check out this uh book",
    "start": "2108079",
    "end": "2116119"
  },
  {
    "text": "chapter okay so uh the time to first token um which is a metric I mentioned",
    "start": "2116119",
    "end": "2122079"
  },
  {
    "text": "earlier is essentially a function of of the prefill it's basically how long does",
    "start": "2122079",
    "end": "2128000"
  },
  {
    "text": "it take to encode the prompt and usually it's uh you know this is compute limited",
    "start": "2128000",
    "end": "2133119"
  },
  {
    "text": "so you're basically going as fast as you can um and there's not much you can do",
    "start": "2133119",
    "end": "2138720"
  },
  {
    "text": "about it given a fixed architecture um and uh well okay so sorry you you can",
    "start": "2138720",
    "end": "2147040"
  },
  {
    "text": "improve it if you um if you reduce the the batch size uh still um but if you",
    "start": "2147040",
    "end": "2154560"
  },
  {
    "text": "want to improve the throughput you have to um you know increase the batch",
    "start": "2154560",
    "end": "2161160"
  },
  {
    "text": "size okay so any uh questions about that so this was on computing the throughput",
    "start": "2161160",
    "end": "2167520"
  },
  {
    "text": "and latency and because of the memory limited uh argument that I gave in the",
    "start": "2167520",
    "end": "2173680"
  },
  {
    "text": "the previous um part I just focus on memory and compute how many bytes need",
    "start": "2173680",
    "end": "2180960"
  },
  {
    "text": "to be sent and that gives me uh a rough bound on um the latency in practice the",
    "start": "2180960",
    "end": "2187200"
  },
  {
    "text": "compute there are some regimes where compute does matter but I'm sort of ignoring that just u to keep things",
    "start": "2187200",
    "end": "2192800"
  },
  {
    "text": "simple okay a question is this assuming a",
    "start": "2192800",
    "end": "2197920"
  },
  {
    "text": "single GPU uh the qu Yes this is assuming a single GPU",
    "start": "2197920",
    "end": "2204320"
  },
  {
    "text": "uh while creating a batch from multiple users uh each of these will be completed",
    "start": "2204320",
    "end": "2212119"
  },
  {
    "text": "different tokens",
    "start": "2212119",
    "end": "2216119"
  },
  {
    "text": "yeah so the question is if you have multiple users and you're batching them together they might arrive at different times they're going to finish at",
    "start": "2217280",
    "end": "2222480"
  },
  {
    "text": "different times so we're going to get to that um that's going to be a special issue that we're going to have to deal",
    "start": "2222480",
    "end": "2228839"
  },
  {
    "text": "with any other",
    "start": "2228839",
    "end": "2232400"
  },
  {
    "text": "questions okay so now we have a good handle on what the inference workload",
    "start": "2236359",
    "end": "2242000"
  },
  {
    "text": "looks like u we looked at the arithmetic intensity we looked at the the",
    "start": "2242000",
    "end": "2247240"
  },
  {
    "text": "transformer inference with respect to in arithmetic intensity we saw that it was",
    "start": "2247240",
    "end": "2252320"
  },
  {
    "text": "memory limited thanks to the tension where the KV cache has to be special for every sequence and then using that we",
    "start": "2252320",
    "end": "2259280"
  },
  {
    "text": "can compute throughput and and latency which are the main inference metrics that we we care about now how do we make",
    "start": "2259280",
    "end": "2267200"
  },
  {
    "text": "things better okay so there are some things that you can do on that are",
    "start": "2267200",
    "end": "2273280"
  },
  {
    "text": "lossless you can write better kernels you can improve your your systems but I",
    "start": "2273280",
    "end": "2278320"
  },
  {
    "text": "would say that the the high kind of there's a lot you can do if you're willing to take uh you know shortcuts um",
    "start": "2278320",
    "end": "2286800"
  },
  {
    "text": "and these are kind of really interesting because technically this lecture is on inference but secretly it's on model",
    "start": "2286800",
    "end": "2294119"
  },
  {
    "text": "architectures because what you'll see is that a lot of the changes in model architecture are going to have direct",
    "start": "2294119",
    "end": "2301280"
  },
  {
    "text": "impact on inference and we're actually inspired by needing to do inference quickly",
    "start": "2301280",
    "end": "2307359"
  },
  {
    "text": "okay so the big bottleneck here is the KV cache right because remember memory",
    "start": "2307359",
    "end": "2314160"
  },
  {
    "text": "limited which means that the less memory stuff takes um then the faster you go",
    "start": "2314160",
    "end": "2321520"
  },
  {
    "text": "not just because of flops even though that's the part of it but mostly due to",
    "start": "2321520",
    "end": "2326640"
  },
  {
    "text": "memory because um it's mostly about the memory transfers if that's one thing you take away from this lecture it's like",
    "start": "2326640",
    "end": "2333119"
  },
  {
    "text": "all about the the kind of the memory uh for speed okay so the the problem is that if",
    "start": "2333119",
    "end": "2341839"
  },
  {
    "text": "you just start whacking away at the KV cache you're you might lose accuracy so",
    "start": "2341839",
    "end": "2347359"
  },
  {
    "text": "how can you make sure you don't lose too much accuracy but still uh maintain um",
    "start": "2347359",
    "end": "2352560"
  },
  {
    "text": "your your KV cache uh small so there's a bunch of ideas I'm going to",
    "start": "2352560",
    "end": "2359359"
  },
  {
    "text": "go through um that all essentially try to change the architecture to reduce",
    "start": "2359359",
    "end": "2364720"
  },
  {
    "text": "your KV cache some of these ideas I think you you've seen um but I'll go",
    "start": "2364720",
    "end": "2369760"
  },
  {
    "text": "through them kind of in this sort of more systematic way so there's this idea called group query attention so",
    "start": "2369760",
    "end": "2377040"
  },
  {
    "text": "multi-headed attention which is the vanilla transformer keeps around uh",
    "start": "2377040",
    "end": "2382560"
  },
  {
    "text": "basically number of heads and for each of the that number you have same number of keys values and queries",
    "start": "2382560",
    "end": "2390359"
  },
  {
    "text": "um there was one time a multi-query attention which you only have one uh key",
    "start": "2390359",
    "end": "2397359"
  },
  {
    "text": "and one value basically one key value head uh turned out that that was not",
    "start": "2397359",
    "end": "2403280"
  },
  {
    "text": "very expressive so there is a sort of intermediate point where um you have a",
    "start": "2403280",
    "end": "2408800"
  },
  {
    "text": "reduced number of keys and values and then you have more queries so why are we doing this well",
    "start": "2408800",
    "end": "2416800"
  },
  {
    "text": "remember we want to reduce the KV cache size so the fewer keys and values there are the better so the batch size and the",
    "start": "2416800",
    "end": "2422880"
  },
  {
    "text": "sequence length doesn't uh get changed but it's and the dimensionality of the",
    "start": "2422880",
    "end": "2428160"
  },
  {
    "text": "the these vectors don't change but it's the number of key value heads that we're reducing",
    "start": "2428160",
    "end": "2435280"
  },
  {
    "text": "okay so so that's basically the idea and this paper shows that you do get latency",
    "start": "2435280",
    "end": "2443760"
  },
  {
    "text": "and throughput um improvements so times per sle sample and as you increase the",
    "start": "2443760",
    "end": "2449920"
  },
  {
    "text": "number of groups um then uh up to eight or so basically there's a negligible um",
    "start": "2449920",
    "end": "2458400"
  },
  {
    "text": "uh is it's really fast compared to um the full attention um and as you",
    "start": "2458400",
    "end": "2463839"
  },
  {
    "text": "increase the number of groups obviously you kind of end up at the the original so that's uh latency and throughput",
    "start": "2463839",
    "end": "2470240"
  },
  {
    "text": "improvements um and just to actually do this kind of",
    "start": "2470240",
    "end": "2475440"
  },
  {
    "text": "more rigorously so we have our llama 213 uh B model and if we compute the the",
    "start": "2475440",
    "end": "2483760"
  },
  {
    "text": "statistics um this is using a batch size of 64",
    "start": "2483760",
    "end": "2490640"
  },
  {
    "text": "remember this is what we got um uh I guess I should have printed out latency here oh well um and then",
    "start": "2490640",
    "end": "2501079"
  },
  {
    "text": "um if you run it with GQA you see that",
    "start": "2501079",
    "end": "2506720"
  },
  {
    "text": "the memory is reduced and the throughput goes way up so this is actually great um",
    "start": "2506720",
    "end": "2513599"
  },
  {
    "text": "so this is what happens if I take the llama to 13b architecture and I just",
    "start": "2513599",
    "end": "2519040"
  },
  {
    "text": "reduce for every query head I'm going so for every key value head I have five",
    "start": "2519040",
    "end": "2524720"
  },
  {
    "text": "query heads that's what 1:5 ratio means um so which this also means we can use a",
    "start": "2524720",
    "end": "2532400"
  },
  {
    "text": "larger batch size because remember last time we tried to do 256 it didn't even fit in um an H100's memory so now um we",
    "start": "2532400",
    "end": "2542240"
  },
  {
    "text": "can actually comfortably fit into the H100 memory and then we can further improve the throughput by using a larger",
    "start": "2542240",
    "end": "2547440"
  },
  {
    "text": "batch size so you can see kind of a lot of different effects here by reducing",
    "start": "2547440",
    "end": "2552880"
  },
  {
    "text": "the number of uh key value pairs the memory of the KV cache reduces um that",
    "start": "2552880",
    "end": "2559520"
  },
  {
    "text": "means the the throughput and latency go up automatically because fewer memory transfers and furthermore as a secondary",
    "start": "2559520",
    "end": "2566240"
  },
  {
    "text": "effect I can increase the batch size within the GPU and that further improves the throughput",
    "start": "2566240",
    "end": "2573680"
  },
  {
    "text": "okay so that's that's wonderful um we have to also make sure the accuracies",
    "start": "2575280",
    "end": "2580560"
  },
  {
    "text": "doesn't drop so this is um this original paper that shows that uh this is full",
    "start": "2580560",
    "end": "2588000"
  },
  {
    "text": "attention this is GQA um the time is much less but the accuracy is basically",
    "start": "2588000",
    "end": "2594160"
  },
  {
    "text": "the same okay now um you know what actually",
    "start": "2594160",
    "end": "2599280"
  },
  {
    "text": "happened so I don't uh so llama 2 did not use this ratio but llama 3 actually",
    "start": "2599280",
    "end": "2606800"
  },
  {
    "text": "picked up you know GQA and um you know probably motivated by the kind of",
    "start": "2606800",
    "end": "2611920"
  },
  {
    "text": "inference costs actually llama 2 I think the 70 the large model did have GQA but",
    "start": "2611920",
    "end": "2617119"
  },
  {
    "text": "not the smaller ones okay so that's GQA there's another",
    "start": "2617119",
    "end": "2624000"
  },
  {
    "text": "way to reduce the key value cache and this comes from deepseek So this is actually from the",
    "start": "2624000",
    "end": "2630480"
  },
  {
    "text": "deepseek v2 paper and it's called multi head latent which tatsu lectured about",
    "start": "2630480",
    "end": "2636720"
  },
  {
    "text": "uh previously but I'll try to um talk about it in the context of inference and",
    "start": "2636720",
    "end": "2641839"
  },
  {
    "text": "its implications so the basic idea is uh here's full attention and GQA says I'm",
    "start": "2641839",
    "end": "2649040"
  },
  {
    "text": "going to use fewer keys and values mla says I'm not going to change the number",
    "start": "2649040",
    "end": "2654960"
  },
  {
    "text": "of key and values i'm going to project these into a lower dimensional space so",
    "start": "2654960",
    "end": "2661200"
  },
  {
    "text": "it's another way of shrinking the KV size but just in a I guess in a different um",
    "start": "2661200",
    "end": "2666520"
  },
  {
    "text": "dimension so instead of using N* H dimensions for each uh um for the KV",
    "start": "2666520",
    "end": "2674000"
  },
  {
    "text": "cache of each uh you know uh token I'm going to project out to C dimensions um",
    "start": "2674000",
    "end": "2680640"
  },
  {
    "text": "and this is what Deep Seek did it's actually quite a aggressive reduction from 16,000 to 512 only wrinkle is that",
    "start": "2680640",
    "end": "2687680"
  },
  {
    "text": "this is not compatible with rope so they need to add a few more more dimensions to put rope back in but overall this is",
    "start": "2687680",
    "end": "2695359"
  },
  {
    "text": "actually quite uh promising um from a KV",
    "start": "2695359",
    "end": "2700560"
  },
  {
    "text": "reduction perspective i'm not going to do the math you can but you can just trust me that you can see kind of how",
    "start": "2700560",
    "end": "2706880"
  },
  {
    "text": "the KV cache would be reduced a lot and you get the same kind of latency and throughput advantages um and in terms of",
    "start": "2706880",
    "end": "2714480"
  },
  {
    "text": "accuracy they actually showed um that compared to GQA um the MH uh sorry the",
    "start": "2714480",
    "end": "2723640"
  },
  {
    "text": "um actually maybe I'm showing the wrong thing here",
    "start": "2723640",
    "end": "2729160"
  },
  {
    "text": "um maybe okay um I meant to show that the um MLA actually improves but this",
    "start": "2729160",
    "end": "2737680"
  },
  {
    "text": "table does not show that so I'll have to um dig that up later but but anyway it MLA does um preserve the the accuracy as",
    "start": "2737680",
    "end": "2747319"
  },
  {
    "text": "well okay so there's another idea which says well um you know GQA basically",
    "start": "2747319",
    "end": "2755920"
  },
  {
    "text": "shares you can think about it as a sharing key value uh vectors right",
    "start": "2755920",
    "end": "2761760"
  },
  {
    "text": "within um a token and within a um sequence um but we can also look at",
    "start": "2761760",
    "end": "2768880"
  },
  {
    "text": "something called cross layer attention which uh there's a paper on this but I think many people have been kind of uh",
    "start": "2768880",
    "end": "2775760"
  },
  {
    "text": "thinking about this and doing this uh so I don't know know if this is actually the first paper but basically if you look at the transformer I diagram um you",
    "start": "2775760",
    "end": "2783920"
  },
  {
    "text": "have the key value projection of one layer and then you have the next layer and these key value um vectors are",
    "start": "2783920",
    "end": "2792400"
  },
  {
    "text": "separate um usually um but IDE idea here with uh CLA is that we're just going to",
    "start": "2792400",
    "end": "2799119"
  },
  {
    "text": "use the same key value projection across layers that's why it's called cross layer attention",
    "start": "2799119",
    "end": "2805800"
  },
  {
    "text": "um so um so just as GQA shares across heads",
    "start": "2805800",
    "end": "2811560"
  },
  {
    "text": "CLA shares across layers so here we they",
    "start": "2811560",
    "end": "2816800"
  },
  {
    "text": "show that they empirically improve the paralo frontier of accuracy um and the",
    "start": "2816800",
    "end": "2823040"
  },
  {
    "text": "KV cache uh size so KV cache size which relates to throughput and latency um you",
    "start": "2823040",
    "end": "2830000"
  },
  {
    "text": "want to be small and you want perplexity also to be um you know small so they're",
    "start": "2830000",
    "end": "2836480"
  },
  {
    "text": "able to you know improve uh that",
    "start": "2836480",
    "end": "2842280"
  },
  {
    "text": "um okay so so notice that I mean for example H6 64 heads you know the cache",
    "start": "2842280",
    "end": "2850880"
  },
  {
    "text": "size goes uh it gets reduced but the validation perplexity does go up a little bit but overall the the there's",
    "start": "2850880",
    "end": "2859760"
  },
  {
    "text": "kind of advantage um in you know making that trade-off",
    "start": "2859760",
    "end": "2865520"
  },
  {
    "text": "okay so there's yet another way to do things um so local attention",
    "start": "2866640",
    "end": "2872800"
  },
  {
    "text": "um which has been explored actually quite a bit since even kind of the there's a long former there's an open AI",
    "start": "2872800",
    "end": "2879680"
  },
  {
    "text": "paper and then Mistro and I think many others use this as well um it's a very I",
    "start": "2879680",
    "end": "2886079"
  },
  {
    "text": "guess a natural idea instead of if you look at a full attention diagram it's dense n squared um and that's where a",
    "start": "2886079",
    "end": "2892800"
  },
  {
    "text": "lot of your complexity comes from um and basically the idea is uh you're going to",
    "start": "2892800",
    "end": "2899680"
  },
  {
    "text": "just attend to only the past K tokens um which means that in the KV",
    "start": "2899680",
    "end": "2907680"
  },
  {
    "text": "cache as you're generating the sequence you don't have to remember everything as as soon as the token kind of falls",
    "start": "2907680",
    "end": "2913880"
  },
  {
    "text": "outside your window that you have of attention you can just like throw it away right so local contention is very",
    "start": "2913880",
    "end": "2921280"
  },
  {
    "text": "uh you you could say that the KV cache size remains constant as opposed to",
    "start": "2921280",
    "end": "2927599"
  },
  {
    "text": "growing with the sequence length so this is really good right because that means for even long se sequences you can have",
    "start": "2927599",
    "end": "2935680"
  },
  {
    "text": "um quite a small cache okay",
    "start": "2935680",
    "end": "2940760"
  },
  {
    "text": "um but you know the problem is that this still you know hurts accuracy because if",
    "start": "2940760",
    "end": "2947680"
  },
  {
    "text": "you just think about it like why are we doing attention instead of RNN's is that",
    "start": "2947680",
    "end": "2953040"
  },
  {
    "text": "we needed to have long range to MAC model run long range dependencies and",
    "start": "2953040",
    "end": "2958720"
  },
  {
    "text": "this is in some sense even to call it attention is a little bit kind of a um overselling this this is only looking at",
    "start": "2958720",
    "end": "2964960"
  },
  {
    "text": "the local context um which is not very expressive so what you do here is you",
    "start": "2964960",
    "end": "2972559"
  },
  {
    "text": "can interle local attention with full global attention hybrid layers um so for",
    "start": "2972559",
    "end": "2979280"
  },
  {
    "text": "example character AI used um for every six layers they had one global attent",
    "start": "2979280",
    "end": "2985520"
  },
  {
    "text": "global layer and five um local layers so it looks something in addition to cross",
    "start": "2985520",
    "end": "2991680"
  },
  {
    "text": "layer attention so it looks something like uh this um where full attention",
    "start": "2991680",
    "end": "2997440"
  },
  {
    "text": "every layer um you have to store the you know KV cache um and for um what they",
    "start": "2997440",
    "end": "3005440"
  },
  {
    "text": "did is that for every six layers you have the full attention but in between",
    "start": "3005440",
    "end": "3011680"
  },
  {
    "text": "you have this local attention and on top of that they have KV cache sharing um",
    "start": "3011680",
    "end": "3018119"
  },
  {
    "text": "locally both at the for the local attention and the global attention",
    "start": "3018119",
    "end": "3023839"
  },
  {
    "text": "so this is like all the tricks kind of you know not all the tricks but many of the tricks kind of combined you know",
    "start": "3023839",
    "end": "3030440"
  },
  {
    "text": "together um so in summary these are a few ways to reduce the KV cache size",
    "start": "3030440",
    "end": "3037119"
  },
  {
    "text": "because remember inference is memory limited um so you want to reduce the cache size but you don't want to hurt",
    "start": "3037119",
    "end": "3042960"
  },
  {
    "text": "accuracy too much and there's many ways to do it you can lower the",
    "start": "3042960",
    "end": "3048559"
  },
  {
    "text": "dimensionality of the KV cache you can have few KV cache vectors you can reduce the dimensionality of a a KV vector you",
    "start": "3048559",
    "end": "3057760"
  },
  {
    "text": "can share the KV cache across layers um and also you can use local attention on",
    "start": "3057760",
    "end": "3065040"
  },
  {
    "text": "some of the layers okay any questions about um the",
    "start": "3065040",
    "end": "3070640"
  },
  {
    "text": "set of tricks for reducing the KV cache uh yeah yeah question about the",
    "start": "3070640",
    "end": "3077880"
  },
  {
    "text": "costion so like how many like I feel like the weights are all being shared",
    "start": "3077880",
    "end": "3082960"
  },
  {
    "text": "across the like the same layers like do you just have like one set of weights which just like KV that's shared across",
    "start": "3082960",
    "end": "3090720"
  },
  {
    "text": "the blue ones yeah so the the question is are the weights shared so the KV",
    "start": "3090720",
    "end": "3097839"
  },
  {
    "text": "cache is shared but the weights are shared so what happens is the weights for doing the um you know the projection",
    "start": "3097839",
    "end": "3104960"
  },
  {
    "text": "need to be shared so there's some consistency yeah",
    "start": "3104960",
    "end": "3110240"
  },
  {
    "text": "yeah there was another question",
    "start": "3110240",
    "end": "3114760"
  },
  {
    "text": "the the context size is is too large every you know and then it increases the KV catch as well the contact size when",
    "start": "3115599",
    "end": "3122559"
  },
  {
    "text": "you pump the context that is given to the trans",
    "start": "3122559",
    "end": "3127880"
  },
  {
    "text": "yeah long then it increases the size so we try reducing those context by",
    "start": "3127880",
    "end": "3134160"
  },
  {
    "text": "summarizing the context context yeah so the question is uh if you have really long context let's say your prompt is",
    "start": "3134160",
    "end": "3139599"
  },
  {
    "text": "huge that's going to intrinsically take a lot of KV cache um so all these tricks",
    "start": "3139599",
    "end": "3147040"
  },
  {
    "text": "can try to reduce uh that um you can do more aggressive things that like you",
    "start": "3147040",
    "end": "3152800"
  },
  {
    "text": "know there's ideas like you know just tokens or ways to summarize the prompt um which we're not going to talk about",
    "start": "3152800",
    "end": "3158960"
  },
  {
    "text": "in this class but there's ways of that address the long prompt situation as",
    "start": "3158960",
    "end": "3165040"
  },
  {
    "text": "Okay so now I'm going to talk about even more radical ways of uh making inference",
    "start": "3167800",
    "end": "3174720"
  },
  {
    "text": "go faster by changing the transformer so the KV cache these are basically",
    "start": "3174720",
    "end": "3180480"
  },
  {
    "text": "variants of the transformer um but maybe you can actually go um",
    "start": "3180480",
    "end": "3188800"
  },
  {
    "text": "outside the transformer and and do better because the transformer wasn't",
    "start": "3188800",
    "end": "3193920"
  },
  {
    "text": "really designed with you know heavy inference workloads in mind they were just trying to try and train a good model that um efficiently it was mostly",
    "start": "3193920",
    "end": "3200800"
  },
  {
    "text": "about training efficiency and the auto regression as we sort of pointed out is",
    "start": "3200800",
    "end": "3206240"
  },
  {
    "text": "really causing this kind of um you know bottleneck here with the auto regression plus the kind of the full",
    "start": "3206240",
    "end": "3213800"
  },
  {
    "text": "attention so um we're going to talk about two directions uh state space models and diffusion models um this is",
    "start": "3213800",
    "end": "3220079"
  },
  {
    "text": "going to be fairly quick um so the idea of state space models is",
    "start": "3220079",
    "end": "3227680"
  },
  {
    "text": "actually drawing ideas from you know signal processing and control theory um initially the motivation was trying to",
    "start": "3227680",
    "end": "3234240"
  },
  {
    "text": "model long context uh sequences without suffering the n squ blowup so it wasn't",
    "start": "3234240",
    "end": "3240079"
  },
  {
    "text": "necessary about inference speed but it turns out if you solve that problem you get faster inference too so there's a",
    "start": "3240079",
    "end": "3246000"
  },
  {
    "text": "kind of early paper on S S4 U which uses classical uh state space models which",
    "start": "3246000",
    "end": "3253200"
  },
  {
    "text": "are basically these kind of linear dynamical systems u which are you been",
    "start": "3253200",
    "end": "3259520"
  },
  {
    "text": "used to kind of model along context and sort of shoehorning them into the kind of a modern neural um setup um this work",
    "start": "3259520",
    "end": "3267520"
  },
  {
    "text": "is uh allows has is you know nice in that it has sort of this RNN kind of",
    "start": "3267520",
    "end": "3274480"
  },
  {
    "text": "interpretation due to the linearity structure and also has a you know convolution",
    "start": "3274480",
    "end": "3281200"
  },
  {
    "text": "um interpretation as well so they published this uh this uh this paper and",
    "start": "3281200",
    "end": "3288040"
  },
  {
    "text": "um you know showed that it I think worked really well on these long context you know synthetic tasks um but what",
    "start": "3288040",
    "end": "3296800"
  },
  {
    "text": "they found is that what I guess what was uh discovered is that they don't really",
    "start": "3296800",
    "end": "3302720"
  },
  {
    "text": "work well for language modeling uh and that's obviously kind of a",
    "start": "3302720",
    "end": "3308480"
  },
  {
    "text": "disappointment because a lot of the value of transformers is being able to do language well um so in a series of",
    "start": "3308480",
    "end": "3315760"
  },
  {
    "text": "papers there was um they sort of identified a set of kind of synthetic",
    "start": "3315760",
    "end": "3321280"
  },
  {
    "text": "tasks that captured the essence of why these models weren't working well and",
    "start": "3321280",
    "end": "3326319"
  },
  {
    "text": "that's basically these associative recall tasks so here's a synthetic task where you're given a basically a",
    "start": "3326319",
    "end": "3331440"
  },
  {
    "text": "sequence of key value pairs and um and the goal is to predict basically",
    "start": "3331440",
    "end": "3338400"
  },
  {
    "text": "look up the key and output the value so in some sense it's kind of a logically a trivial task but it's long sequence",
    "start": "3338400",
    "end": "3345359"
  },
  {
    "text": "because I can have a lot of key value pairs and um I'm going to have to look far back it can be arbitrary long",
    "start": "3345359",
    "end": "3352079"
  },
  {
    "text": "dependence and you can see that local attention is not going to work very well because it's just going to remember the last few sequences and um what the",
    "start": "3352079",
    "end": "3361040"
  },
  {
    "text": "problem with states space models is that they're sort of good for these kind of signal processing tasks but um really",
    "start": "3361040",
    "end": "3367440"
  },
  {
    "text": "this is like you need to go isolate a particular key value pair and pull out the the answer and for those type of",
    "start": "3367440",
    "end": "3374000"
  },
  {
    "text": "tasks it didn't really work so there's a bunch of work um I'm not citing there's like hyena h3 and then mamba um which",
    "start": "3374000",
    "end": "3382839"
  },
  {
    "text": "basically tweaked the or changed the hssm um to basically handle these associative",
    "start": "3382839",
    "end": "3389280"
  },
  {
    "text": "recall tasks and eventually it worked better up to kind of 1b scale was matching you know transformers",
    "start": "3389280",
    "end": "3396400"
  },
  {
    "text": "um and uh mamb idea of mamba has been popular and scaled up um even to a 52b",
    "start": "3396400",
    "end": "3405559"
  },
  {
    "text": "um um by AI21 folks um notice that in",
    "start": "3405559",
    "end": "3411760"
  },
  {
    "text": "this case they still had to use a transformer so a transformer but only",
    "start": "3411760",
    "end": "3417200"
  },
  {
    "text": "every I guess eight layers they had a transformer the rest of them were mamba layers um and so that still get led to a",
    "start": "3417200",
    "end": "3425040"
  },
  {
    "text": "fairly big you know uh savings and and speed up um and",
    "start": "3425040",
    "end": "3432000"
  },
  {
    "text": "uh but more recently there's this kind of revival of this older idea called",
    "start": "3432000",
    "end": "3437280"
  },
  {
    "text": "linear attention um where instead of um let's see if I can",
    "start": "3437280",
    "end": "3443520"
  },
  {
    "text": "make this bigger um is actually a kind of a very simple idea so you know what local attention or",
    "start": "3443520",
    "end": "3449760"
  },
  {
    "text": "sliding window attention is um linear attention is this idea that uh you",
    "start": "3449760",
    "end": "3458440"
  },
  {
    "text": "essentially in so in this basically in the um attention computation there's a",
    "start": "3458440",
    "end": "3464000"
  },
  {
    "text": "key and a query and you uh dotproduct them and you take the x of that which is",
    "start": "3464000",
    "end": "3470640"
  },
  {
    "text": "basically giving you a x kernel so you can basically take a tailor expansion of that and write that computation as",
    "start": "3470640",
    "end": "3478160"
  },
  {
    "text": "basically dotproducts of some nonlinear map so then what you essentially have is",
    "start": "3478160",
    "end": "3483839"
  },
  {
    "text": "you you can think about for every uh key value position you are basically",
    "start": "3483839",
    "end": "3490000"
  },
  {
    "text": "applying some sort of nonlinearity blowing up into some space and then doing some linear computation over it",
    "start": "3490000",
    "end": "3495760"
  },
  {
    "text": "and because it's linear attention it actually kind of behaves like an RNN and",
    "start": "3495760",
    "end": "3500880"
  },
  {
    "text": "it's linear in the the sequence length rather than quadratic i know that was a",
    "start": "3500880",
    "end": "3506799"
  },
  {
    "text": "little bit uh fast but um I just want to give you sort of the the taste of it and",
    "start": "3506799",
    "end": "3512480"
  },
  {
    "text": "so this idea has been actually uh scaled up quite successfully",
    "start": "3512480",
    "end": "3517520"
  },
  {
    "text": "so there's this um um organization called Miniax that's training um pretty",
    "start": "3517520",
    "end": "3524079"
  },
  {
    "text": "legitimate models um up to 456 billion",
    "start": "3524079",
    "end": "3529799"
  },
  {
    "text": "parameteres um and they use this basically linear attention idea now they",
    "start": "3529799",
    "end": "3535200"
  },
  {
    "text": "have to use full attention um still once in a while it seems it I don't think um",
    "start": "3535200",
    "end": "3543920"
  },
  {
    "text": "people have been able to get around having some full attention but at least",
    "start": "3543920",
    "end": "3549680"
  },
  {
    "text": "it seems like people have been able to get rid of most of full attention in like most of the layers are not full",
    "start": "3549680",
    "end": "3556160"
  },
  {
    "text": "attention anymore they're just either linear layers or local attention layers which are much much more efficient",
    "start": "3556160",
    "end": "3564559"
  },
  {
    "text": "okay so the linear plus local attention um you know now are actually yielding",
    "start": "3564559",
    "end": "3572240"
  },
  {
    "text": "serious state-of-the-art models and it's probably you know safe to say that well",
    "start": "3572240",
    "end": "3577920"
  },
  {
    "text": "I don't know what's uh exactly the the closed model providers are doing but I would suspect that there would be at",
    "start": "3577920",
    "end": "3584799"
  },
  {
    "text": "least as kind of advance in terms of as efficient as um this and leveraging sparsity",
    "start": "3584799",
    "end": "3592160"
  },
  {
    "text": "So it's it's kind of an interesting question when people ask like well you know is trans attention all you need is",
    "start": "3593280",
    "end": "3598640"
  },
  {
    "text": "transformers it well you know yes and no I mean I guess in some sense there is",
    "start": "3598640",
    "end": "3604559"
  },
  {
    "text": "still the sense squared there maybe we'll be able to get rid of it but most of the transformer has been like pretty",
    "start": "3604559",
    "end": "3610960"
  },
  {
    "text": "radically changed by having other much lighter weight you know components and",
    "start": "3610960",
    "end": "3616480"
  },
  {
    "text": "you're still able to get much of the same and kind of accuracy",
    "start": "3616480",
    "end": "3621839"
  },
  {
    "text": "And all of this is you know really helpful for inference because on these",
    "start": "3622319",
    "end": "3627839"
  },
  {
    "text": "non-ful attention layers you're basically replacing the ordered t KV",
    "start": "3627839",
    "end": "3632880"
  },
  {
    "text": "cache which grows as a sequence length with something that's uh constant and there's there's papers by",
    "start": "3632880",
    "end": "3641440"
  },
  {
    "text": "um uh that followup I think they uh on the based paper where did that go",
    "start": "3641440",
    "end": "3646880"
  },
  {
    "text": "there's some f uh either in this paper or in follow-up work analyzing basically the trade-off",
    "start": "3646880",
    "end": "3653040"
  },
  {
    "text": "between the KV size and the ability to do uh various types of kind of recall",
    "start": "3653040",
    "end": "3658480"
  },
  {
    "text": "tasks which makes sense because if you don't store very much you won't be able",
    "start": "3658480",
    "end": "3663599"
  },
  {
    "text": "to solve certain tasks but there is this trade-off curve that you can try to play",
    "start": "3663599",
    "end": "3669559"
  },
  {
    "text": "with okay so that's all I'll say about um the state space models",
    "start": "3669559",
    "end": "3676559"
  },
  {
    "text": "So now let's talk about a completely different style of uh generation models",
    "start": "3676559",
    "end": "3681680"
  },
  {
    "text": "diffusion models so diffusion models have been very popular image generation but they turn out to be fairly tricky to",
    "start": "3681680",
    "end": "3687839"
  },
  {
    "text": "get working in text although there recently have been some advances here so",
    "start": "3687839",
    "end": "3693040"
  },
  {
    "text": "the idea of diffusion is that you instead of generating all regressively you just generate every token in",
    "start": "3693040",
    "end": "3699200"
  },
  {
    "text": "parallel so obviously if you only do that you know via some simple layer it's",
    "start": "3699200",
    "end": "3704319"
  },
  {
    "text": "not going to be very good you can't generate all the words in parallel and expect it to be coherent but what you do",
    "start": "3704319",
    "end": "3709520"
  },
  {
    "text": "is you iterate and you keep on refining um this this generation until it gets to",
    "start": "3709520",
    "end": "3715440"
  },
  {
    "text": "your final uh generation that you output and",
    "start": "3715440",
    "end": "3720599"
  },
  {
    "text": "the the idea behind generating in parallel you're no longer auto",
    "start": "3720599",
    "end": "3726760"
  },
  {
    "text": "reggressively you know bound and that generating all tokens in parallel well",
    "start": "3726760",
    "end": "3732000"
  },
  {
    "text": "can be done in parallel so you get to saturate your your G GPUs relatively",
    "start": "3732000",
    "end": "3738000"
  },
  {
    "text": "easy as long as your context length is large enough um so recently there's this",
    "start": "3738000",
    "end": "3743680"
  },
  {
    "text": "uh and session labs has um produced some pretty interesting models um there's not",
    "start": "3743680",
    "end": "3750480"
  },
  {
    "text": "much written about them but you can see kind of a a demo of the generation and process it just kind of generates um",
    "start": "3750480",
    "end": "3758559"
  },
  {
    "text": "code instantaneously but it's obviously kind of broken code and then it uh you",
    "start": "3758559",
    "end": "3764559"
  },
  {
    "text": "know kind of refineses over time so and this is one of their their",
    "start": "3764559",
    "end": "3770960"
  },
  {
    "text": "benchmarks that show that at least on coding I'm not sure about other uh tasks",
    "start": "3770960",
    "end": "3776160"
  },
  {
    "text": "that if you look at that at tokens per second these models are like way out here in terms of speed compared to",
    "start": "3776160",
    "end": "3782640"
  },
  {
    "text": "anything that's transformer even Jamba remember is like a hybrid mamba um um uh",
    "start": "3782640",
    "end": "3789760"
  },
  {
    "text": "transformer architecture is is quite slow compared to these diffusion models",
    "start": "3789760",
    "end": "3795520"
  },
  {
    "text": "so now whether diffusion models are you know be will be kind of general purpose and powerful enough in all these that",
    "start": "3795520",
    "end": "3801440"
  },
  {
    "text": "remains to be seen but I think it's you know you have such a lead on the the kind of the token speed here that um",
    "start": "3801440",
    "end": "3808960"
  },
  {
    "text": "even if you um I think you can put more compute and kind of recover some of the",
    "start": "3808960",
    "end": "3815200"
  },
  {
    "text": "accuracy uh losses if you if you need to okay so the the summary here is that",
    "start": "3815200",
    "end": "3823119"
  },
  {
    "text": "I think this whole kind of architecture novel architecture thing is actually really exciting for inference",
    "start": "3823119",
    "end": "3831599"
  },
  {
    "text": "um because um they allow you to sidestep kind of fundamental obstacles right so",
    "start": "3831599",
    "end": "3838160"
  },
  {
    "text": "if you're dealing with attention you just have this fundamental KV cache obstacle that you can quantize you can",
    "start": "3838160",
    "end": "3844880"
  },
  {
    "text": "optimize but it's still there and and so by making a kind of safe space model",
    "start": "3844880",
    "end": "3851039"
  },
  {
    "text": "you're shrinking that to like a constant size and as long as you can keep up the accuracy which is big if um then you win",
    "start": "3851039",
    "end": "3857839"
  },
  {
    "text": "big time same with the diffusion models auto reggressive generation is a key bottleneck now if you just generalize",
    "start": "3857839",
    "end": "3864559"
  },
  {
    "text": "generate things in parallel now all of a sudden you kind of like change the game you know completely so there's much more",
    "start": "3864559",
    "end": "3870240"
  },
  {
    "text": "work to be done here um in proving inference so as you can see now",
    "start": "3870240",
    "end": "3876720"
  },
  {
    "text": "inference is the inference game is is much broader than it seems at first",
    "start": "3876720",
    "end": "3881839"
  },
  {
    "text": "sight it's not really about kind of necessarily the systems optimizations to make it fast although you obviously need",
    "start": "3881839",
    "end": "3887359"
  },
  {
    "text": "those but I think the real gains are coming from um like real radical changes",
    "start": "3887359",
    "end": "3892400"
  },
  {
    "text": "in architecture um okay so I have about 10 minutes left i'll go through these",
    "start": "3892400",
    "end": "3897640"
  },
  {
    "text": "quickly uh quantization and model pruning so quantization the key idea is",
    "start": "3897640",
    "end": "3903359"
  },
  {
    "text": "just reduce the precision of the numbers okay so easily e very easy to do and the",
    "start": "3903359",
    "end": "3911119"
  },
  {
    "text": "thought is that less memory means uh less bytes transferred higher sorry",
    "start": "3911119",
    "end": "3917200"
  },
  {
    "text": "there should be lower latency higher throughput um and you do have to worry",
    "start": "3917200",
    "end": "3922480"
  },
  {
    "text": "about accuracy of course that's the the trade-off um if you look at the different types of uh you know formats",
    "start": "3922480",
    "end": "3931119"
  },
  {
    "text": "FP32 used for training not used for inference really um BF-16 is sort of the",
    "start": "3931119",
    "end": "3937440"
  },
  {
    "text": "default for inference you can go down to FP8 or INT8 which now is um is less",
    "start": "3937440",
    "end": "3946160"
  },
  {
    "text": "accurate but much you know cheaper than even FP8 um so people do do a bunch of",
    "start": "3946160",
    "end": "3951839"
  },
  {
    "text": "inference in int 8 which if you look at the range I mean it's an integer between 127 minus 128 which is not that uh it's",
    "start": "3951839",
    "end": "3960640"
  },
  {
    "text": "pretty low precision and people even going down to in4 which is they're not",
    "start": "3960640",
    "end": "3965920"
  },
  {
    "text": "oh yeah so int4 is pretty you know low um there's also other ways you can do",
    "start": "3965920",
    "end": "3973520"
  },
  {
    "text": "okay so you can so once you kind of decide that you want to",
    "start": "3973520",
    "end": "3980960"
  },
  {
    "text": "um quantize i guess you could do several things you can train with a",
    "start": "3980960",
    "end": "3987000"
  },
  {
    "text": "quantization but obviously that means you need to retrain models and more I guess commonly you do post-training",
    "start": "3987000",
    "end": "3993039"
  },
  {
    "text": "quantization where you take an existing model and you try to quantize it and try not to screw things up too much um so",
    "start": "3993039",
    "end": "4001240"
  },
  {
    "text": "the there's a paper called element int 8 which I'll I'll talk through you know",
    "start": "4001240",
    "end": "4006319"
  },
  {
    "text": "kind of briefly so in quantization um basically what happens is that you",
    "start": "4006319",
    "end": "4011760"
  },
  {
    "text": "take your you know your your vector which is let's say FP16",
    "start": "4011760",
    "end": "4016880"
  },
  {
    "text": "um and then you need to figure out the dynamic range right if you want to pack it into int 8 you need to figure out",
    "start": "4016880",
    "end": "4022240"
  },
  {
    "text": "what the largest value is and once you figure that out you can kind of um you know divide uh by that and multiply by",
    "start": "4022240",
    "end": "4029359"
  },
  {
    "text": "128 and then you get your integers and then if you need to um dequantize then",
    "start": "4029359",
    "end": "4035839"
  },
  {
    "text": "you kind of go the other way so so basically quantization means that remember memory is a bandwidth right so",
    "start": "4035839",
    "end": "4042720"
  },
  {
    "text": "bottleneck so you're all your transfers are happening in um in in end date but",
    "start": "4042720",
    "end": "4048480"
  },
  {
    "text": "when you actually do the you know I guess uh you sometimes have to upcast to",
    "start": "4048480",
    "end": "4054960"
  },
  {
    "text": "um a floating point to actually do the arithmetic um okay so the problem with int8 is that",
    "start": "4054960",
    "end": "4063359"
  },
  {
    "text": "not everything you know fits nicely and you have these outliers which appear in",
    "start": "4063359",
    "end": "4068400"
  },
  {
    "text": "larger networks that screw things up so what this paper did is that you take a",
    "start": "4068400",
    "end": "4073920"
  },
  {
    "text": "this matrix you identify the really large outlier values and then you handle them separately use in full 16bit",
    "start": "4073920",
    "end": "4080880"
  },
  {
    "text": "precision and then do the most the vast majority in 88 um so this works well but is actually",
    "start": "4080880",
    "end": "4089280"
  },
  {
    "text": "a bit um you know uh slower so the motivation here wasn't inference speed but more even being able to fit your",
    "start": "4089280",
    "end": "4096159"
  },
  {
    "text": "model into u memory there's another paper called activation aware",
    "start": "4096159",
    "end": "4101400"
  },
  {
    "text": "quantization and here uh the idea is that you you're kind of quantizing the",
    "start": "4101400",
    "end": "4108880"
  },
  {
    "text": "bas kind of the the weights but you're going to figure out which weights to quantize based on the the activations um",
    "start": "4108880",
    "end": "4117758"
  },
  {
    "text": "you know really quickly this you're going down to actually in three um and this obviously reduces memory by quite a",
    "start": "4117759",
    "end": "4124798"
  },
  {
    "text": "bit and leads to a 3x uh you know speed up And so the general idea here is that",
    "start": "4124799",
    "end": "4131199"
  },
  {
    "text": "you want to um you get a trained model and it just happens that some of the",
    "start": "4131199",
    "end": "4136798"
  },
  {
    "text": "weights or activations are going to be abnormally large so for those you handle separately and then everything else you",
    "start": "4136799",
    "end": "4143838"
  },
  {
    "text": "can kind of work in low you know precision okay um talk about model",
    "start": "4143839",
    "end": "4149679"
  },
  {
    "text": "pruning um ideas very like quantization it's sort of the basic idea is very",
    "start": "4149679",
    "end": "4155679"
  },
  {
    "text": "simple you just rip out parts of an expensive model to make it cheaper and then you fix it up so in this Nvidia",
    "start": "4155679",
    "end": "4162480"
  },
  {
    "text": "paper what they do is they first identify important either layers or heads or",
    "start": "4162480",
    "end": "4168960"
  },
  {
    "text": "hidden dimensions um using a small calibration size say use some you know",
    "start": "4168960",
    "end": "4174400"
  },
  {
    "text": "uh simple scores to compute that and then you just remove the unimportant",
    "start": "4174400",
    "end": "4180159"
  },
  {
    "text": "layers or hidden units or heads and then now if you just take that model it's",
    "start": "4180159",
    "end": "4185600"
  },
  {
    "text": "going to be clearly worse right but so then the last step is you distill the",
    "start": "4185600",
    "end": "4191120"
  },
  {
    "text": "original model into the prune model so you kind of repair the model from the initialization which is your prune so",
    "start": "4191120",
    "end": "4197760"
  },
  {
    "text": "you're not starting from scratch you're starting from something that's worse but hopefully not worse and hopefully",
    "start": "4197760",
    "end": "4203760"
  },
  {
    "text": "retains a lot of the same structural properties of the original model it's just maybe not kind of like calibrated",
    "start": "4203760",
    "end": "4210239"
  },
  {
    "text": "in some sense um and the results are pretty good on that so they have these",
    "start": "4210239",
    "end": "4215360"
  },
  {
    "text": "15 billion parameter models that they're able to reduce to 8B with hardly any",
    "start": "4215360",
    "end": "4221440"
  },
  {
    "text": "drop in this I guess at least according to MMLU and then down to 4B with uh some",
    "start": "4221440",
    "end": "4228000"
  },
  {
    "text": "drop but you also are going down quite a bit to a 4B model",
    "start": "4228000",
    "end": "4234159"
  },
  {
    "text": "okay so um so maybe summarize this taking shortcuts idea you can uh reduce",
    "start": "4234159",
    "end": "4241360"
  },
  {
    "text": "inference complexity without hurting accuracy you can do it from scratch where you just define a fresh architecture that's by construction fast",
    "start": "4241360",
    "end": "4248000"
  },
  {
    "text": "and just train it or you can distill you define that architecture you can take a",
    "start": "4248000",
    "end": "4254080"
  },
  {
    "text": "slow model and you figure out a some sort of scheme to initialize the new",
    "start": "4254080",
    "end": "4260159"
  },
  {
    "text": "model with the old model and then you basically do",
    "start": "4260159",
    "end": "4265320"
  },
  {
    "text": "distillation um so okay so now all of these are a little",
    "start": "4265320",
    "end": "4273920"
  },
  {
    "text": "bit unsatisfying because they're lossy so you get massive speed ups but you",
    "start": "4273920",
    "end": "4279840"
  },
  {
    "text": "always wonder well maybe this model isn't as actually good as original so",
    "start": "4279840",
    "end": "4285440"
  },
  {
    "text": "speculative decoding or speculative sampling allows you to um basically have your cake and eat it too so recall",
    "start": "4285440",
    "end": "4292960"
  },
  {
    "text": "there's two stages of inference you prefill um which you're given a sequence you",
    "start": "4292960",
    "end": "4298320"
  },
  {
    "text": "encode all the tokens in parallel this is compute limited um which is great notice that this also gives you log",
    "start": "4298320",
    "end": "4305120"
  },
  {
    "text": "probabilities for each of the tokens and then there's generation which is one token at a time it's memory limit it's",
    "start": "4305120",
    "end": "4311480"
  },
  {
    "text": "slow so in other words checking is faster than generation so intuitively",
    "start": "4311480",
    "end": "4317520"
  },
  {
    "text": "this makes sense but hopefully now you also appreciate the math behind why this is true um and the speculative sampling",
    "start": "4317520",
    "end": "4325920"
  },
  {
    "text": "idea is actually really really again it was proposed in parallel by these two",
    "start": "4325920",
    "end": "4331040"
  },
  {
    "text": "independent teams from Google um and uh the idea is",
    "start": "4331040",
    "end": "4337640"
  },
  {
    "text": "to use a cheap draft model P to just run ahead and generate some",
    "start": "4337640",
    "end": "4343560"
  },
  {
    "text": "tokens and then you're going to evaluate those tokens with a target model and",
    "start": "4343560",
    "end": "4348800"
  },
  {
    "text": "because evaluation of given tokens is just prefilled so you can do that in parallel which is fast and then you",
    "start": "4348800",
    "end": "4354640"
  },
  {
    "text": "accept it if it looks good so uh this is what it looks like in",
    "start": "4354640",
    "end": "4360640"
  },
  {
    "text": "uh real life so if you're using a big model generating one token at a time that's slow but in speculative decoding",
    "start": "4360640",
    "end": "4367520"
  },
  {
    "text": "you have the draft model that's racing ahead generating um a lot of tokens and",
    "start": "4367520",
    "end": "4373199"
  },
  {
    "text": "using the big model to essentially verify and sometimes it will reject and",
    "start": "4373199",
    "end": "4379040"
  },
  {
    "text": "sometimes it'll accept and the acceptance rate basically determines how fast of a speed up you you",
    "start": "4379040",
    "end": "4385960"
  },
  {
    "text": "have okay so here is the more formal um algorithm um so you're going to have a",
    "start": "4385960",
    "end": "4393440"
  },
  {
    "text": "look ahead of K so you're going to use your draft model and generate K tokens auto",
    "start": "4393440",
    "end": "4399080"
  },
  {
    "text": "reggressively so this is hopefully fast because your draft model is small and then you're given these K tokens that",
    "start": "4399080",
    "end": "4405440"
  },
  {
    "text": "you generated and I'm going to score them based on I'm going to compute the probability under the target model Q now",
    "start": "4405440",
    "end": "4412560"
  },
  {
    "text": "I'm going to decide whether I want to accept this not or not so I go through each token and I'm going to essentially",
    "start": "4412560",
    "end": "4419840"
  },
  {
    "text": "accept it with probability um Q over P and the one just is uh make sure this",
    "start": "4419840",
    "end": "4427440"
  },
  {
    "text": "you know probabilities are between zero and one um if this kind of looks like if people are familiar with Metropolis",
    "start": "4427440",
    "end": "4433840"
  },
  {
    "text": "Hastings this is kind of where this uh this kind of comes from um so intuitively you're you're sampling with",
    "start": "4433840",
    "end": "4439840"
  },
  {
    "text": "P um so you need to divide that out because you don't want P you want Q so this is kind of importance weight um on",
    "start": "4439840",
    "end": "4446640"
  },
  {
    "text": "this so if you accept it then great you kind of move on and you look at the next draft token and so on um and if you",
    "start": "4446640",
    "end": "4454239"
  },
  {
    "text": "don't accept it then you're going to sample from the the the target model the",
    "start": "4454239",
    "end": "4461760"
  },
  {
    "text": "slow model but you kind of do this correction where you've already tried to sample using P so you don't need to do",
    "start": "4461760",
    "end": "4467440"
  },
  {
    "text": "that anymore you subtract it out and you sample from Q so this is basically kind of a rejection",
    "start": "4467440",
    "end": "4474080"
  },
  {
    "text": "sampling with a proposal P and a target uh sorry target Q um the only difference",
    "start": "4474080",
    "end": "4480080"
  },
  {
    "text": "is that um you are sampling you you injection sampling you if you reject",
    "start": "4480080",
    "end": "4486719"
  },
  {
    "text": "then you reject and you just try again and you try again um and here we don't want to keep on kind of looping forever",
    "start": "4486719",
    "end": "4493600"
  },
  {
    "text": "uh because if you reject we're just going to say \"Okay fine we'll bite the bullet and just sample from the the more",
    "start": "4493600",
    "end": "4499520"
  },
  {
    "text": "expensive model.\" So the cool thing here is that you're guaranteed to get an exact sample",
    "start": "4499520",
    "end": "4507360"
  },
  {
    "text": "from the target model okay so those of you familiar with sampling this is it shouldn't be too surprising you're able",
    "start": "4507360",
    "end": "4513679"
  },
  {
    "text": "to use kind of prior information to speed up sampling um but in a language",
    "start": "4513679",
    "end": "4519120"
  },
  {
    "text": "modeling context this is kind of nice um I'm going to skip the uh this is not really a proof this is just kind of some",
    "start": "4519120",
    "end": "4525679"
  },
  {
    "text": "derivation to show that for a case of vocab 2 it why this these formulas kind",
    "start": "4525679",
    "end": "4531440"
  },
  {
    "text": "of give you the right unbiased sampling procedure um and it works uh pretty well",
    "start": "4531440",
    "end": "4539679"
  },
  {
    "text": "so the accuracy uh is you know it should be actually the the same since it's the",
    "start": "4539679",
    "end": "4545760"
  },
  {
    "text": "same model but maybe there's some randomness there but the speed up is you're getting a factor of two speed up",
    "start": "4545760",
    "end": "4551640"
  },
  {
    "text": "essentially um so and so in practice what you do is you",
    "start": "4551640",
    "end": "4559199"
  },
  {
    "text": "take you have something like a 70B model and your draft model is much much smaller um and if your target model is",
    "start": "4559199",
    "end": "4566080"
  },
  {
    "text": "80B 8B then your draft model might be 1B and you generally want to make the draft",
    "start": "4566080",
    "end": "4571360"
  },
  {
    "text": "model as closest to your target as possible um and so if you're doing some",
    "start": "4571360",
    "end": "4576960"
  },
  {
    "text": "distillation that could um make it even better there's a bunch of this is a",
    "start": "4576960",
    "end": "4582080"
  },
  {
    "text": "pretty uh hot area of of research and inference there's a lot of ways to",
    "start": "4582080",
    "end": "4587920"
  },
  {
    "text": "improve this process um you can use um Medusa which is this uh way to have the",
    "start": "4587920",
    "end": "4594480"
  },
  {
    "text": "draft model instead of generating auto reggressively sample multiple tokens in parallel or eagle where you're actually",
    "start": "4594480",
    "end": "4600320"
  },
  {
    "text": "taking highle features of the target model and pumping them into the draft model to generate so the draft model",
    "start": "4600320",
    "end": "4605840"
  },
  {
    "text": "doesn't actually have to stand alone it can be kind of glom on to the target model to help it generate",
    "start": "4605840",
    "end": "4612280"
  },
  {
    "text": "um so uh summary exact sampling from the target model thanks to math um and this",
    "start": "4612280",
    "end": "4620320"
  },
  {
    "text": "exploits the symmetry between checking and generation right so or um prefill",
    "start": "4620320",
    "end": "4627440"
  },
  {
    "text": "and generation and there's actually a lot of room for innovation on a draft model um which can you know everything",
    "start": "4627440",
    "end": "4635920"
  },
  {
    "text": "that we talked about before where you can have different radical architectures different ways of quantizing all of",
    "start": "4635920",
    "end": "4642239"
  },
  {
    "text": "those apply right the only thing is that um you get to basically guarantee that",
    "start": "4642239",
    "end": "4647760"
  },
  {
    "text": "you're getting exact act uh sample okay so now I'll go uh I'm out of time but",
    "start": "4647760",
    "end": "4653679"
  },
  {
    "text": "quickly go through the um the question that came up earlier which is that in",
    "start": "4653679",
    "end": "4659920"
  },
  {
    "text": "practice when you're serving there's live traffic requests come at different times um they finish at different times",
    "start": "4659920",
    "end": "4666560"
  },
  {
    "text": "they have some of them have shared prefixes some of them don't they have different lengths so it's very",
    "start": "4666560",
    "end": "4672320"
  },
  {
    "text": "heterogeneous um in comparison to training where you get basically a dense block of tokens",
    "start": "4672320",
    "end": "4679120"
  },
  {
    "text": "and you're basically going to p push it through your GPU at at full speed so",
    "start": "4679120",
    "end": "4684480"
  },
  {
    "text": "what do you do in this case so there's a series of papers that kind of ex explore",
    "start": "4684480",
    "end": "4689679"
  },
  {
    "text": "this um and the the basic idea is is this is so the last two parties are more",
    "start": "4689679",
    "end": "4695520"
  },
  {
    "text": "of a kind of systems level um contribution so the idea is that you",
    "start": "4695520",
    "end": "4700560"
  },
  {
    "text": "don't wait for batches to you know the the train leaves there's no the train doesn't um wait for you um so when a new",
    "start": "4700560",
    "end": "4708800"
  },
  {
    "text": "batch comes you're just going to put it in right which means that um the the sort of this uh the the the um the",
    "start": "4708800",
    "end": "4717679"
  },
  {
    "text": "worker that's generating tokens needs to kind of hand control back to theuler every step so you generate a token come",
    "start": "4717679",
    "end": "4724400"
  },
  {
    "text": "back to the scheduleuler and say if there's new um uh new u requests then",
    "start": "4724400",
    "end": "4730480"
  },
  {
    "text": "they get stuck in and then it kind of continues so you're kind of not wasting any time waiting around for requests um",
    "start": "4730480",
    "end": "4738080"
  },
  {
    "text": "now there's a kind of a problem with batching I think which is behind the question um batching works when",
    "start": "4738080",
    "end": "4743679"
  },
  {
    "text": "everything's the same dimensionality but every request might be a different length so there's this idea of selective",
    "start": "4743679",
    "end": "4749280"
  },
  {
    "text": "batching where um you basically break up your computation for attention",
    "start": "4749280",
    "end": "4754719"
  },
  {
    "text": "everything has to be handled separately but for your MPs remember which are the bulk of the computation you can actually",
    "start": "4754719",
    "end": "4761600"
  },
  {
    "text": "take tensors of different sizes and you just flatten them and because they don't interact um they can just like be kind",
    "start": "4761600",
    "end": "4769199"
  },
  {
    "text": "of kind of along for the ride in the batch dimension okay I know that was fast but uh I'll just quickly go over",
    "start": "4769199",
    "end": "4775760"
  },
  {
    "text": "page detention now um this is the the paper behind VLM which some of you",
    "start": "4775760",
    "end": "4780960"
  },
  {
    "text": "probably have used um and this addresses the memory usage problem so if you have",
    "start": "4780960",
    "end": "4786159"
  },
  {
    "text": "a KV cache and prompts are coming in and finishing then your cache is going to get fragmented so you're going to have",
    "start": "4786159",
    "end": "4792960"
  },
  {
    "text": "um you're going to allocate a bunch of uh space for a request but you don't",
    "start": "4792960",
    "end": "4798080"
  },
  {
    "text": "know how many tokens are getting generated so um there's going to be internal fragmentation and then there's",
    "start": "4798080",
    "end": "4803360"
  },
  {
    "text": "also going to be external fragmentation where there's padding between the the requests and responses so that's no good",
    "start": "4803360",
    "end": "4809920"
  },
  {
    "text": "so the page attention basically says remember operating systems we have um and how kind of virtual memory works we",
    "start": "4809920",
    "end": "4817120"
  },
  {
    "text": "divide the KV cache into a sequence of contiguous blocks and then we just put them wherever um we find white space so",
    "start": "4817120",
    "end": "4825199"
  },
  {
    "text": "if you have two requests coming in then um they might just you know the first",
    "start": "4825199",
    "end": "4830880"
  },
  {
    "text": "request might be here here and here and the second request might be here and here so the blocks are the things that",
    "start": "4830880",
    "end": "4836159"
  },
  {
    "text": "you're going to keep contiguous and that's going to give you your allow you time to coales your memory",
    "start": "4836159",
    "end": "4843480"
  },
  {
    "text": "um so you can also play these tricks where if you have uh sharing of",
    "start": "4843480",
    "end": "4850040"
  },
  {
    "text": "prefixes then there's another idea from uh operating systems which is copy on",
    "start": "4850040",
    "end": "4856080"
  },
  {
    "text": "write so you basically maintain reference counters for how many basically uh sequences are using this",
    "start": "4856080",
    "end": "4863280"
  },
  {
    "text": "particular block and then if you need to kind of diverge and have blocks go in different directions then you copy and",
    "start": "4863280",
    "end": "4870400"
  },
  {
    "text": "you reduce the reference count um there's a bunch of other VLM optimizations which I won't go through",
    "start": "4870400",
    "end": "4876719"
  },
  {
    "text": "but basically the summary is remember your your operating systems classes you",
    "start": "4876719",
    "end": "4882640"
  },
  {
    "text": "can apply them to inference as well okay so quick summary inference is really",
    "start": "4882640",
    "end": "4888000"
  },
  {
    "text": "really important it's where um and the characteristics are distinct from",
    "start": "4888000",
    "end": "4893920"
  },
  {
    "text": "training you're memory limited and it's also dynamic so which leads a bunch of uh uh new challenges we saw a whole host",
    "start": "4893920",
    "end": "4902560"
  },
  {
    "text": "of different techniques around new architectures quantization pruning distillation um speculative decoding uh",
    "start": "4902560",
    "end": "4910080"
  },
  {
    "text": "there's ideas from systems which can allow you to better use your memory um",
    "start": "4910080",
    "end": "4916639"
  },
  {
    "text": "overlap communication and compute and things like that but but I would say that there's probably even more um",
    "start": "4916639",
    "end": "4923679"
  },
  {
    "text": "opportunity in sort of the modeling and architecture because if you think about it all you you don't inference narrowly",
    "start": "4923679",
    "end": "4931280"
  },
  {
    "text": "is inference in a particular model how do I run this particular model but who cares running about that particular",
    "start": "4931280",
    "end": "4936800"
  },
  {
    "text": "model you care about delivering good accuracy given your resource budget so a",
    "start": "4936800",
    "end": "4942080"
  },
  {
    "text": "lot of these ideas that are trying to change the reduce the KV cache",
    "start": "4942080",
    "end": "4948400"
  },
  {
    "text": "changing the transformer are basically ways to sidestep the problem and say",
    "start": "4948400",
    "end": "4954080"
  },
  {
    "text": "well I have something that's more efficient and if I can train in a way that gets me better accuracy then then I",
    "start": "4954080",
    "end": "4960280"
  },
  {
    "text": "win okay so that's all I have and I will see you next time then we're back to scaling lots",
    "start": "4960280",
    "end": "4968440"
  }
]