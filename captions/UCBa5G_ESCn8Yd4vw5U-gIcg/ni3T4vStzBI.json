[
  {
    "start": "0",
    "end": "4820"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "4820",
    "end": "6120"
  },
  {
    "text": "This is the 10th\nand final screencast",
    "start": "6120",
    "end": "8059"
  },
  {
    "text": "in our series on\ncontextual representation.",
    "start": "8060",
    "end": "10460"
  },
  {
    "text": "I'd like to just\nbriefly wrap up.",
    "start": "10460",
    "end": "12410"
  },
  {
    "text": "In doing that, I'd like\nto do three things.",
    "start": "12410",
    "end": "14550"
  },
  {
    "text": "First, just take stock of\nwhat we did a little bit.",
    "start": "14550",
    "end": "17270"
  },
  {
    "text": "Second, I'd like to make\namends for really interesting",
    "start": "17270",
    "end": "20360"
  },
  {
    "text": "architectures and innovations\nthat I didn't have time",
    "start": "20360",
    "end": "22850"
  },
  {
    "text": "to mention in the course series.",
    "start": "22850",
    "end": "24560"
  },
  {
    "text": "And then finally, I'd like\nto look to the future,",
    "start": "24560",
    "end": "26900"
  },
  {
    "text": "both for the course\nand also for the field.",
    "start": "26900",
    "end": "31220"
  },
  {
    "text": "Let me start by trying to\nmake amends a little bit",
    "start": "31220",
    "end": "33770"
  },
  {
    "text": "for some noteworthy\narchitectures",
    "start": "33770",
    "end": "35480"
  },
  {
    "text": "that I didn't have time for.",
    "start": "35480",
    "end": "37610"
  },
  {
    "text": "Transformer XL is an\nearly and very innovative",
    "start": "37610",
    "end": "41270"
  },
  {
    "text": "attempt to bring\nin long contexts.",
    "start": "41270",
    "end": "43520"
  },
  {
    "text": "And it does this by\nessentially caching",
    "start": "43520",
    "end": "46370"
  },
  {
    "text": "earlier parts of a\nlong sequence and then",
    "start": "46370",
    "end": "49850"
  },
  {
    "text": "creating some recurrent\nconnections across those cache",
    "start": "49850",
    "end": "52640"
  },
  {
    "text": "states into the computation\nfor the current set of states.",
    "start": "52640",
    "end": "56329"
  },
  {
    "text": "Very innovative.",
    "start": "56330",
    "end": "57740"
  },
  {
    "text": "The ideas for transformer\nXL were carried forward",
    "start": "57740",
    "end": "60620"
  },
  {
    "text": "into XLNet.",
    "start": "60620",
    "end": "61850"
  },
  {
    "text": "And the core of\nXLNet is the goal",
    "start": "61850",
    "end": "64250"
  },
  {
    "text": "of having bidirectional\ncontext while nonetheless",
    "start": "64250",
    "end": "67250"
  },
  {
    "text": "having an autoregressive\nlanguage modeling loss.",
    "start": "67250",
    "end": "70460"
  },
  {
    "text": "And they do this in this kind\nof really interesting way",
    "start": "70460",
    "end": "73700"
  },
  {
    "text": "of sampling different sequence\norders so that you process left",
    "start": "73700",
    "end": "77630"
  },
  {
    "text": "to right while nonetheless\nsampling enough sequence",
    "start": "77630",
    "end": "80509"
  },
  {
    "text": "orders that you\nessentially have the power",
    "start": "80510",
    "end": "82880"
  },
  {
    "text": "of bidirectional context.",
    "start": "82880",
    "end": "85790"
  },
  {
    "text": "And then DeBERTa is\nreally interesting",
    "start": "85790",
    "end": "88460"
  },
  {
    "text": "from the perspective\nof our discussion",
    "start": "88460",
    "end": "90440"
  },
  {
    "text": "of positional encoding.",
    "start": "90440",
    "end": "91790"
  },
  {
    "text": "In that screencast,\nI expressed a concern",
    "start": "91790",
    "end": "94160"
  },
  {
    "text": "that the positional encoding\nrepresentations were exerting,",
    "start": "94160",
    "end": "97820"
  },
  {
    "text": "in some cases,\ntoo much influence",
    "start": "97820",
    "end": "100100"
  },
  {
    "text": "on the representations of words.",
    "start": "100100",
    "end": "102619"
  },
  {
    "text": "DeBERTa can be\nseen as an attempt",
    "start": "102620",
    "end": "104870"
  },
  {
    "text": "to decouple word from\nposition somewhat.",
    "start": "104870",
    "end": "107510"
  },
  {
    "text": "It does that by decoupling\nthose core representations",
    "start": "107510",
    "end": "110540"
  },
  {
    "text": "and then having distinct\nattention mechanisms",
    "start": "110540",
    "end": "113480"
  },
  {
    "text": "to those two parts.",
    "start": "113480",
    "end": "115520"
  },
  {
    "text": "My guiding intuition\nhere is that DeBERTa",
    "start": "115520",
    "end": "117649"
  },
  {
    "text": "will allow words to have more\nof their wordhood separate",
    "start": "117650",
    "end": "121190"
  },
  {
    "text": "from where they might have\nappeared in the input string.",
    "start": "121190",
    "end": "124920"
  },
  {
    "text": "And that seems\nvery healthy to me.",
    "start": "124920",
    "end": "127960"
  },
  {
    "text": "When I talked about\nBERT, I listed out",
    "start": "127960",
    "end": "130119"
  },
  {
    "text": "some known limitations.",
    "start": "130120",
    "end": "131379"
  },
  {
    "text": "There were four of them.",
    "start": "131380",
    "end": "133180"
  },
  {
    "text": "I gave credit to RoBERTa for\naddressing the first one, which",
    "start": "133180",
    "end": "136420"
  },
  {
    "text": "was around design decisions.",
    "start": "136420",
    "end": "138459"
  },
  {
    "text": "And I gave credit to ELECTRA for\naddressing items 2 and 3, where",
    "start": "138460",
    "end": "142870"
  },
  {
    "text": "2 was about the artificial\nnature of the MASK token,",
    "start": "142870",
    "end": "146049"
  },
  {
    "text": "and 3 was about the\ninefficiency of MLM training",
    "start": "146050",
    "end": "149650"
  },
  {
    "text": "in the BERT context.",
    "start": "149650",
    "end": "151510"
  },
  {
    "text": "I haven't yet touched\non the 4th item.",
    "start": "151510",
    "end": "153700"
  },
  {
    "text": "The 4th item is from Yang et\nal., which is the XLNet paper.",
    "start": "153700",
    "end": "157180"
  },
  {
    "text": "And XLNet indeed\naddresses this concern.",
    "start": "157180",
    "end": "159790"
  },
  {
    "text": "And the concern\nis just that BERT",
    "start": "159790",
    "end": "161170"
  },
  {
    "text": "assumes the predicted tokens are\nindependent of each other given",
    "start": "161170",
    "end": "165459"
  },
  {
    "text": "the unmasked tokens\nwhich is oversimplified",
    "start": "165460",
    "end": "168100"
  },
  {
    "text": "as high-order,\nlong-range dependency is",
    "start": "168100",
    "end": "170830"
  },
  {
    "text": "prevalent in natural language.",
    "start": "170830",
    "end": "172780"
  },
  {
    "text": "And the guiding\nidea behind XLNet",
    "start": "172780",
    "end": "175300"
  },
  {
    "text": "is that in having an\nautoregressive language",
    "start": "175300",
    "end": "177880"
  },
  {
    "text": "modeling loss, we\nbring in some of",
    "start": "177880",
    "end": "180070"
  },
  {
    "text": "the conditional\nprobabilities that",
    "start": "180070",
    "end": "182170"
  },
  {
    "text": "help us overcome this artificial\nstatistical nature of the MLM",
    "start": "182170",
    "end": "186130"
  },
  {
    "text": "objective.",
    "start": "186130",
    "end": "187360"
  },
  {
    "text": "But remember, the\ninteresting aspect of XLNet",
    "start": "187360",
    "end": "190250"
  },
  {
    "text": "is that we still have\nbidirectional context,",
    "start": "190250",
    "end": "192890"
  },
  {
    "text": "and this comes from sampling\nall of those permutation",
    "start": "192890",
    "end": "196069"
  },
  {
    "text": "orders of the input string.",
    "start": "196070",
    "end": "198350"
  },
  {
    "text": "Really interesting to\nthink about, and also",
    "start": "198350",
    "end": "201230"
  },
  {
    "text": "a lovely insight about\nthe nature of BERT itself.",
    "start": "201230",
    "end": "206310"
  },
  {
    "text": "I didn't get to discuss\npretraining data really",
    "start": "206310",
    "end": "209640"
  },
  {
    "text": "at all in this series, and\nI feel guilty about that",
    "start": "209640",
    "end": "212310"
  },
  {
    "text": "because I think we can now\nsee that pretraining data is",
    "start": "212310",
    "end": "216300"
  },
  {
    "text": "an incredibly important\ningredient in shaping",
    "start": "216300",
    "end": "219420"
  },
  {
    "text": "the behaviors of these\nlarge language models.",
    "start": "219420",
    "end": "221910"
  },
  {
    "text": "So I have listed out here some\ncore pretraining resources.",
    "start": "221910",
    "end": "225810"
  },
  {
    "text": "OpenBookCorpus, The Pile,\nBig Science Data, Wikipedia,",
    "start": "225810",
    "end": "230790"
  },
  {
    "text": "and Reddit.",
    "start": "230790",
    "end": "231750"
  },
  {
    "text": "And I have listed\nthese here not really",
    "start": "231750",
    "end": "234300"
  },
  {
    "text": "to encourage you to go off and\ntrain your own large language",
    "start": "234300",
    "end": "237060"
  },
  {
    "text": "model, but rather to think\nabout auditing these datasets",
    "start": "237060",
    "end": "240959"
  },
  {
    "text": "as a way of more\ndeeply understanding",
    "start": "240960",
    "end": "243330"
  },
  {
    "text": "the artifacts that\nwe do have and coming",
    "start": "243330",
    "end": "245820"
  },
  {
    "text": "to an understanding of where\nthey're likely to be successful",
    "start": "245820",
    "end": "248760"
  },
  {
    "text": "and where they might be\nactually very problematic.",
    "start": "248760",
    "end": "251670"
  },
  {
    "text": "A lot of that is going to trace\nto the nature of the input",
    "start": "251670",
    "end": "255240"
  },
  {
    "text": "data.",
    "start": "255240",
    "end": "256898"
  },
  {
    "text": "And then finally,\nlet's look ahead",
    "start": "256899",
    "end": "258398"
  },
  {
    "text": "to the future,\nsome current trends",
    "start": "258399",
    "end": "260018"
  },
  {
    "text": "to the best of my estimation.",
    "start": "260019",
    "end": "262870"
  },
  {
    "text": "This is likely the\nsituation we're",
    "start": "262870",
    "end": "264610"
  },
  {
    "text": "in and what we're going\nto see going forward.",
    "start": "264610",
    "end": "267229"
  },
  {
    "text": "First, it seems like\nautoregressive architectures",
    "start": "267230",
    "end": "270250"
  },
  {
    "text": "have taken over.",
    "start": "270250",
    "end": "271580"
  },
  {
    "text": "That's the rise of GPT.",
    "start": "271580",
    "end": "273939"
  },
  {
    "text": "But this may be simply\nbecause the field",
    "start": "273940",
    "end": "276730"
  },
  {
    "text": "is so focused on\ngeneration right now.",
    "start": "276730",
    "end": "280190"
  },
  {
    "text": "And so I would still\nmaintain that if you simply",
    "start": "280190",
    "end": "283090"
  },
  {
    "text": "want to represent examples\nfor the sake of having",
    "start": "283090",
    "end": "286120"
  },
  {
    "text": "a sentence embedding\nor understanding",
    "start": "286120",
    "end": "288430"
  },
  {
    "text": "how different representations\ncompare to each other,",
    "start": "288430",
    "end": "291110"
  },
  {
    "text": "it seems to me that\nbidirectional models like BERT",
    "start": "291110",
    "end": "293830"
  },
  {
    "text": "might still have the edge\nover models like GPT.",
    "start": "293830",
    "end": "298150"
  },
  {
    "text": "Seq2seq models are\nstill a dominant choice",
    "start": "298150",
    "end": "301449"
  },
  {
    "text": "for tasks that have\nthat structure.",
    "start": "301450",
    "end": "303430"
  },
  {
    "text": "It seems like they might\nhave an edge in terms",
    "start": "303430",
    "end": "305650"
  },
  {
    "text": "of an architectural bias\nthat helps them understand",
    "start": "305650",
    "end": "308680"
  },
  {
    "text": "the tasks themselves.",
    "start": "308680",
    "end": "310270"
  },
  {
    "text": "Although item 1\nhere is important",
    "start": "310270",
    "end": "312400"
  },
  {
    "text": "as we get these really\nlarge pure language models,",
    "start": "312400",
    "end": "315979"
  },
  {
    "text": "we might find\nourselves moving more",
    "start": "315980",
    "end": "317870"
  },
  {
    "text": "toward autoregressive\nformulations even of tasks",
    "start": "317870",
    "end": "321199"
  },
  {
    "text": "that have a seq2seq structure.",
    "start": "321200",
    "end": "323420"
  },
  {
    "text": "We shall see.",
    "start": "323420",
    "end": "325030"
  },
  {
    "text": "And then finally, and maybe this\nis the most interesting point",
    "start": "325030",
    "end": "328090"
  },
  {
    "text": "of all.",
    "start": "328090",
    "end": "328990"
  },
  {
    "text": "People are still\nobsessed with scaling up",
    "start": "328990",
    "end": "331479"
  },
  {
    "text": "to ever larger language models.",
    "start": "331480",
    "end": "333490"
  },
  {
    "text": "But happily, we are\nseeing a countermovement",
    "start": "333490",
    "end": "336400"
  },
  {
    "text": "towards \"smaller\" models.",
    "start": "336400",
    "end": "338169"
  },
  {
    "text": "I've put smaller in quotes\nhere because we're still",
    "start": "338170",
    "end": "340720"
  },
  {
    "text": "talking about\nartifacts that have",
    "start": "340720",
    "end": "342700"
  },
  {
    "text": "on the order of 10\nbillion parameters,",
    "start": "342700",
    "end": "344920"
  },
  {
    "text": "but that is substantially\nsmaller than these really,",
    "start": "344920",
    "end": "348350"
  },
  {
    "text": "really massive language models.",
    "start": "348350",
    "end": "350140"
  },
  {
    "text": "And there are a\nlot of incentives",
    "start": "350140",
    "end": "352690"
  },
  {
    "text": "that are going to encourage\nthese smaller models to become",
    "start": "352690",
    "end": "356050"
  },
  {
    "text": "very good.",
    "start": "356050",
    "end": "356740"
  },
  {
    "text": "We can deploy them\nin more places.",
    "start": "356740",
    "end": "358840"
  },
  {
    "text": "We can train them\nmore efficiently.",
    "start": "358840",
    "end": "360699"
  },
  {
    "text": "We can train more of them.",
    "start": "360700",
    "end": "362260"
  },
  {
    "text": "And we might have\nmore control of them",
    "start": "362260",
    "end": "364720"
  },
  {
    "text": "in the end for the things\nthat we want to do.",
    "start": "364720",
    "end": "367190"
  },
  {
    "text": "So all the incentives are there.",
    "start": "367190",
    "end": "368710"
  },
  {
    "text": "This is a moment of\nintense innovation",
    "start": "368710",
    "end": "371259"
  },
  {
    "text": "and a lot of change\nin this space.",
    "start": "371260",
    "end": "373240"
  },
  {
    "text": "I have no idea what\nthese small models are",
    "start": "373240",
    "end": "375699"
  },
  {
    "text": "going to be able to\ndo a year from now,",
    "start": "375700",
    "end": "377770"
  },
  {
    "text": "but I would exhort\nall of you to think",
    "start": "377770",
    "end": "380379"
  },
  {
    "text": "about how you might participate\nin this exciting moment",
    "start": "380380",
    "end": "383320"
  },
  {
    "text": "and help us reach\nthe point where",
    "start": "383320",
    "end": "385160"
  },
  {
    "text": "relatively small and efficient\nmodels are nonetheless",
    "start": "385160",
    "end": "389480"
  },
  {
    "text": "incredibly performant\nand useful to us.",
    "start": "389480",
    "end": "393340"
  },
  {
    "start": "393340",
    "end": "398000"
  }
]