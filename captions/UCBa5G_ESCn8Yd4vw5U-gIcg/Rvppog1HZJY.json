[
  {
    "text": " Welcome, everyone.",
    "start": "0",
    "end": "7069"
  },
  {
    "text": "This is CS336, language\nmodels from scratch.",
    "start": "7070",
    "end": "12270"
  },
  {
    "text": "And this is the co-staff. So I'm Percy, one\nof your instructors.",
    "start": "12270",
    "end": "17930"
  },
  {
    "text": "I'm really excited\nabout this class because it really allows you to\nsee the whole language modeling, building pipeline\nend-to-end, including",
    "start": "17930",
    "end": "24350"
  },
  {
    "text": "data systems and modeling. Tatsu, I'll be\nco-teaching with him, so I'll let everyone\nintroduce themselves.",
    "start": "24350",
    "end": "31430"
  },
  {
    "text": "Hi, everyone. I'm Tatsu, I'm one of\nthe co-instructors. I'll be giving lecture in a week\nor two, probably a few weeks.",
    "start": "31430",
    "end": "38780"
  },
  {
    "text": "I'm really excited\nabout this class. Percy and I spent a while being\na little disgruntled, thinking",
    "start": "38780",
    "end": "44090"
  },
  {
    "text": "like, what's the really\ndeep technical stuff that we can teach our students today? And I think one\nof the things that",
    "start": "44090",
    "end": "49235"
  },
  {
    "text": "has really got to build it\nfrom scratch to understand it. So I'm hoping that\nthat's of the ethos that you'll take away from.",
    "start": "49235",
    "end": "54545"
  },
  {
    "text": "Goodbye. As Tatsu mentioned,\nthis is the second time we're teaching the class, we've\ngrown the class by around 50%",
    "start": "54545",
    "end": "61990"
  },
  {
    "text": "One big thing is we're making\nall the lectures on YouTube so that the world can\nlearn how to build",
    "start": "61990",
    "end": "69610"
  },
  {
    "text": "language models from scratch. OK, so why do we decide\nto make this course",
    "start": "69610",
    "end": "76420"
  },
  {
    "text": "and endure all the pain? So let's ask GPT-4.",
    "start": "76420",
    "end": "81560"
  },
  {
    "text": "So if you ask it,\nwhy teach a course on building language\nmodels from scratch?",
    "start": "81560",
    "end": "86680"
  },
  {
    "text": "The reply is teaching\na course provides foundational understanding\nof techniques,",
    "start": "86680",
    "end": "92299"
  },
  {
    "text": "fosters innovation, the\ntypical generic bladders.",
    "start": "92300",
    "end": "97370"
  },
  {
    "text": "OK, so here's the real reason. So we're in a bit of\na crisis, I would say.",
    "start": "97370",
    "end": "103090"
  },
  {
    "text": "Researchers are becoming\nmore and more disconnected from the underlying technology.",
    "start": "103090",
    "end": "108460"
  },
  {
    "text": "Eight years ago, researchers\nwould implement and train their own models in AI.",
    "start": "108460",
    "end": "114380"
  },
  {
    "text": "Even six years ago, you would at\nleast take the models like BERT, and download them\nand fine-tune them.",
    "start": "114380",
    "end": "121400"
  },
  {
    "text": "And now many people\ncan just get away with prompting a\nproprietary model.",
    "start": "121400",
    "end": "127460"
  },
  {
    "text": "So this is not necessarily bad. Because as you enter these\nlayers of abstraction,",
    "start": "127460",
    "end": "133549"
  },
  {
    "text": "we can all do more. And a lot of research\nhas been unlocked by the simplicity of being able\nto prompt a language model.",
    "start": "133550",
    "end": "141140"
  },
  {
    "text": "And I do my share of prompting. so there's nothing\nwrong with that.",
    "start": "141140",
    "end": "146360"
  },
  {
    "text": "But also remember that these\nabstractions are leaky. So in contrast to programming\nlanguages or operating systems,",
    "start": "146360",
    "end": "154150"
  },
  {
    "text": "you don't really understand\nwhat the abstraction is. It's a string in and\nstring out, I guess.",
    "start": "154150",
    "end": "160780"
  },
  {
    "text": "And I would say\nthat there's still a lot of fundamental research to\nbe done that required tearing up",
    "start": "160780",
    "end": "166180"
  },
  {
    "text": "the stack and co-designing\ndifferent aspects of the data and the systems and the model. And I think really that full\nunderstanding of this technology",
    "start": "166180",
    "end": "174550"
  },
  {
    "text": "is necessary for\nfundamental research. So that's why this class exists.",
    "start": "174550",
    "end": "180160"
  },
  {
    "text": "We want to enable the\nfundamental research to continue. And our philosophy\nis to understand it,",
    "start": "180160",
    "end": "186760"
  },
  {
    "text": "you have to build it. So there's one\nsmall problem here. And this is because of\nthe industrialization",
    "start": "186760",
    "end": "195629"
  },
  {
    "text": "of language models. So GPT-4 has rumored to be a\n1.8 trillion parameters, cost",
    "start": "195630",
    "end": "203400"
  },
  {
    "text": "$100 million to train. You have xAI building the\nclusters with 200,000 H100s,",
    "start": "203400",
    "end": "211780"
  },
  {
    "text": "if you can imagine that. There's an investment of\nover 500 billion, supposedly",
    "start": "211780",
    "end": "218640"
  },
  {
    "text": "over four years. So these are pretty\nlarge numbers. And furthermore, there's\nno public details",
    "start": "218640",
    "end": "225300"
  },
  {
    "text": "on how these models are\nbeing built. Here from GPT-4, this is even two years ago.",
    "start": "225300",
    "end": "231629"
  },
  {
    "text": "They very honestly say that due\nto the competitive landscape and safety limitations, we're\ngoing to disclose no details.",
    "start": "231630",
    "end": "240270"
  },
  {
    "text": "So this is the state\nof the world right now. And so in some sense, frontier\nmodels are out of reach for us.",
    "start": "240270",
    "end": "248800"
  },
  {
    "text": "So if you came into\nthis class thinking you're each going to train\nyour own GPT-4, sorry.",
    "start": "248800",
    "end": "256350"
  },
  {
    "text": "So we're going to build\nsmall language models. But the problem is that these\nmight not be representative.",
    "start": "256350",
    "end": "263920"
  },
  {
    "text": "And here's some of two\nexamples to illustrate why. So here's a simple one.",
    "start": "263920",
    "end": "271380"
  },
  {
    "text": "If you look at the\nfraction of flops spent in attention layers of\na transformer versus an MLP,",
    "start": "271380",
    "end": "279180"
  },
  {
    "text": "this changes quite a bit. So this is a tweet\nfrom Steven Fowler from quite a few years ago.",
    "start": "279180",
    "end": "285400"
  },
  {
    "text": "But this is still true. If you look at small\nmodels, it looks",
    "start": "285400",
    "end": "290460"
  },
  {
    "text": "like the number of flops in the\nattention versus the MLP layers are roughly comparable, but\nif you go up to 175 billion,",
    "start": "290460",
    "end": "298910"
  },
  {
    "text": "then the MLP is really dominate. So why does this matter?",
    "start": "298910",
    "end": "304729"
  },
  {
    "text": "Well, if you spend a lot\nof time at small scale and you're optimizing\nthe tension,",
    "start": "304730",
    "end": "309800"
  },
  {
    "text": "you might be optimizing\nthe wrong thing because at larger scale,\nit gets washed out.",
    "start": "309800",
    "end": "318640"
  },
  {
    "text": "This is a simple example,\nbecause you can literally make this plot without\nactually any compute.",
    "start": "318640",
    "end": "324050"
  },
  {
    "text": "You just do it's napkin math. Here's something that's a little\nbit harder to grapple with,",
    "start": "324050",
    "end": "330400"
  },
  {
    "text": "is this emergent behavior. So this is a paper from\nJason Wei, in 2022.",
    "start": "330400",
    "end": "335600"
  },
  {
    "text": "And this plot shows that as you\nincrease the amount of training",
    "start": "335600",
    "end": "342190"
  },
  {
    "text": "flops and you look at\naccuracy on a bunch of tasks, you'll see that for a while,\nit looks like the accuracy,",
    "start": "342190",
    "end": "349790"
  },
  {
    "text": "nothing is happening. And all of a sudden\nyou get these emergent",
    "start": "349790",
    "end": "354850"
  },
  {
    "text": "of various phenomena\nlike in-context learning. So if you were hanging\naround at this scale,",
    "start": "354850",
    "end": "360289"
  },
  {
    "text": "you would be concluding that,\nwell, these language models really don't work,\nwhen in fact, you",
    "start": "360290",
    "end": "365650"
  },
  {
    "text": "had to scale up to\nget that behavior. So don't despair.",
    "start": "365650",
    "end": "371810"
  },
  {
    "text": "We can still learn\nsomething in this class. But we have to be very precise\nabout what we're learning.",
    "start": "371810",
    "end": "378380"
  },
  {
    "text": "So there's three\ntypes of knowledge. There's the mechanics\nof how things work.",
    "start": "378380",
    "end": "383569"
  },
  {
    "text": "This we can teach you, we can\nteach you what a transformer is. You can implement a transformer. We can teach you how model\nparallelism leverages",
    "start": "383570",
    "end": "390790"
  },
  {
    "text": "GPUs efficiently. These are just the raw\ningredients, the mechanics.",
    "start": "390790",
    "end": "396949"
  },
  {
    "text": "So that's fine. We can also teach you mindset. So this is something a\nbit more subtle and seems",
    "start": "396950",
    "end": "403840"
  },
  {
    "text": "like a little bit, fuzzy. But this is actually\nin some ways",
    "start": "403840",
    "end": "410409"
  },
  {
    "text": "more important, I would say,\nbecause the mindset that we're going to take is that\nwe want to squeeze",
    "start": "410410",
    "end": "417039"
  },
  {
    "text": "as most out of the\nhardware as possible and take scaling seriously. Because in some sense, the\nmechanics, all of those",
    "start": "417040",
    "end": "424360"
  },
  {
    "text": "will see later that all\nof these ingredients have been around for a\nwhile, but it was really, I think, the scaling\nmindset that OpenAI",
    "start": "424360",
    "end": "431980"
  },
  {
    "text": "pioneered that led to this\nnext generation of AI models.",
    "start": "431980",
    "end": "437080"
  },
  {
    "text": "So mindset, I\nthink, hopefully, we can bang into you that to\nthink in a certain way.",
    "start": "437080",
    "end": "442670"
  },
  {
    "text": "And then thirdly is intuitions. And this is about\nwhich data and modeling",
    "start": "442670",
    "end": "449380"
  },
  {
    "text": "decisions lead to good models? This unfortunately we can\nonly partially teach you.",
    "start": "449380",
    "end": "455240"
  },
  {
    "text": "And this is because\nwhat architectures and what data sets\nwork at low scales",
    "start": "455240",
    "end": "461080"
  },
  {
    "text": "might not be the same ones\nthat work at large scales. ",
    "start": "461080",
    "end": "467500"
  },
  {
    "text": "But hopefully you got\n2 and 1/2 out of 3, so that's pretty good\nbang for your buck.",
    "start": "467500",
    "end": "473360"
  },
  {
    "text": "OK, speaking of intuitions,\nthere's I guess, sad reality of things that,\nyou can tell a lot of stories",
    "start": "473360",
    "end": "481370"
  },
  {
    "text": "about why certain things\nin the transformer are the way they are,\nbut sometimes it's",
    "start": "481370",
    "end": "486440"
  },
  {
    "text": "just come you do the experiments\nand the experiments speak.",
    "start": "486440",
    "end": "491570"
  },
  {
    "text": "So, for example, there's\nthis known shazeer paper that introduced swiGLU this\nglue, which is something that we'll see a bit\nmore in this class, which",
    "start": "491570",
    "end": "498890"
  },
  {
    "text": "is a type of non-linearlity. ",
    "start": "498890",
    "end": "504800"
  },
  {
    "text": "The results are quite good. And this got adopted. But in the conclusion, there\nis this honest statement that we offer no\nexplanation except for this",
    "start": "504800",
    "end": "512630"
  },
  {
    "text": "is divine benevolence. So there you go. This is the extent\nour understanding.",
    "start": "512630",
    "end": "521990"
  },
  {
    "text": "OK, so now let's talk\nabout this bitter lesson that I'm sure people\nhave heard about.",
    "start": "521990",
    "end": "527459"
  },
  {
    "text": "I think there's a misconception\nthat a bitter lesson means that scale is all that matters.",
    "start": "527460",
    "end": "533480"
  },
  {
    "text": "Algorithms don't matter. All you do is pump more\ncapital into building the model and you're good to go.",
    "start": "533480",
    "end": "539420"
  },
  {
    "text": "I think this couldn't be\nfurther from the truth. I think the right interpretation\nis that algorithms at scale",
    "start": "539420",
    "end": "545259"
  },
  {
    "text": "is what matters. And because at the end of the\nday, your accuracy of your model",
    "start": "545260",
    "end": "550840"
  },
  {
    "text": "is really a product\nof your efficiency and the number of\nresources you put in.",
    "start": "550840",
    "end": "556780"
  },
  {
    "text": "And actually efficiency,\nif you think about it, is way more important\nat larger scale.",
    "start": "556780",
    "end": "562940"
  },
  {
    "text": "Because if you're spending\nhundreds of millions of dollars, you cannot afford to be\nwasteful in the same way that",
    "start": "562940",
    "end": "569440"
  },
  {
    "text": "if you're looking at running\na job on your local cluster,",
    "start": "569440",
    "end": "575450"
  },
  {
    "text": "you might run it again. You fail, you debug it. And if you look at actually\nthe utilization and the use,",
    "start": "575450",
    "end": "582920"
  },
  {
    "text": "I'm sure OpenAI has way more\nefficient than any of us right now. So efficiency\nreally is important.",
    "start": "582920",
    "end": "591040"
  },
  {
    "text": "And furthermore, this I think is\nthis point is maybe not as well appreciated in the scaling\nrhetoric, so to speak,",
    "start": "591040",
    "end": "599950"
  },
  {
    "text": "which is that if you\nlook at efficiency, which is a combination of\nhardware and algorithms, but if you just look at\nthe algorithm efficiency,",
    "start": "599950",
    "end": "607540"
  },
  {
    "text": "there's a nice OpenAI paper from\n2020 that showed over the period",
    "start": "607540",
    "end": "613750"
  },
  {
    "text": "of 2012 to 2019,\nthere was a 44x, if algorithmic efficiency\nimprovement in the time that it",
    "start": "613750",
    "end": "622390"
  },
  {
    "text": "took to train ImageNet to a\ncertain level of accuracy. So this is huge.",
    "start": "622390",
    "end": "628040"
  },
  {
    "text": "And I think if you,\nI don't know if you could see the abstract here. This is faster than Moore's law.",
    "start": "628040",
    "end": "634600"
  },
  {
    "text": "So algorithms do matter. If you didn't have\nthis efficiency you would be paying\n44 times more cost.",
    "start": "634600",
    "end": "642710"
  },
  {
    "text": "This is for image models. But there's some results\nfor language as well.",
    "start": "642710",
    "end": "647800"
  },
  {
    "text": "OK, so with all that, I think\nthe right framing or mindset to have is what is\nthe best model one",
    "start": "647800",
    "end": "653980"
  },
  {
    "text": "can build given a certain\ncompute and data budget? And this question\nmakes sense no matter",
    "start": "653980",
    "end": "659950"
  },
  {
    "text": "what scale you're at because\nit's accuracy per resources.",
    "start": "659950",
    "end": "666380"
  },
  {
    "text": "And of course, if you\ncan raise the capital and get more resources,\nyou'll get better models. But as researchers, our goal\nis to improve the efficiency",
    "start": "666380",
    "end": "674470"
  },
  {
    "text": "of the algorithms. So maximize efficiency. We're going to\nhear a lot of that.",
    "start": "674470",
    "end": "682520"
  },
  {
    "text": "So now let me talk a little\nbit about the current landscape",
    "start": "682520",
    "end": "688330"
  },
  {
    "text": "and a little bit of, I\nguess, obligatory history. So language models have\nbeen around for a while now.",
    "start": "688330",
    "end": "697480"
  },
  {
    "text": "Going back to Shannon. Who looked at language\nmodels as a way to estimate the\nentropy of English.",
    "start": "697480",
    "end": "705410"
  },
  {
    "text": "I think in AI, they really\nwere prominent in NLP, where they were a component\nof larger systems like machine",
    "start": "705410",
    "end": "712970"
  },
  {
    "text": "translation speech recognition. And one thing that's maybe\nnot as appreciated these days",
    "start": "712970",
    "end": "718130"
  },
  {
    "text": "is that if you\nlook back in 2007, Google was training fairly\nlarge n-gram models.",
    "start": "718130",
    "end": "725130"
  },
  {
    "text": "So 5 gram models, over\ntwo trillion tokens, which is a lot more\ntokens than GPT-3.",
    "start": "725130",
    "end": "730190"
  },
  {
    "text": "And it was only, I guess\nin the last two years that we've gotten to\nthat in token count.",
    "start": "730190",
    "end": "737900"
  },
  {
    "text": "But they were n-gram models,\nso they didn't really exhibit any of the\ninteresting phenomena that we know of\nlanguage models today.",
    "start": "737900",
    "end": "745205"
  },
  {
    "text": "So in the 2010s, you\ncan think about this, a lot of the deep learning\nrevolution happened and a lot",
    "start": "745205",
    "end": "752270"
  },
  {
    "text": "of the ingredients\nfalling into place. So there was the first\nneural language model",
    "start": "752270",
    "end": "757850"
  },
  {
    "text": "from Joshua Bengio's group. And back in 2003, there\nis seq2seq models.",
    "start": "757850",
    "end": "764060"
  },
  {
    "text": "This I think was a big deal for. How do you basically\nmodel sequences",
    "start": "764060",
    "end": "770840"
  },
  {
    "text": "from Ilya and Google folks? There's an Adam\noptimizer, which still",
    "start": "770840",
    "end": "777889"
  },
  {
    "text": "is used by the majority of\npeople dating over a decade ago. There's a tension\nmechanism, which",
    "start": "777890",
    "end": "784190"
  },
  {
    "text": "was developed in the context of\nmachine translation, which then",
    "start": "784190",
    "end": "789920"
  },
  {
    "text": "led up to the famous attention\nalso you need or the a.k.a. the transformer paper.",
    "start": "789920",
    "end": "795480"
  },
  {
    "text": "In 2017, people\nwere looking at how to scale a mixture of experts. There was a lot of work around\nlate 2010s on how to essentially",
    "start": "795480",
    "end": "806149"
  },
  {
    "text": "do model parallelism. And they were\nactually figuring out how you could train 100\nbillion parameter models.",
    "start": "806150",
    "end": "812820"
  },
  {
    "text": "They didn't train\nit for very long because these were\nmore system work, but all the ingredients\nwere in place before,",
    "start": "812820",
    "end": "823060"
  },
  {
    "text": "by the time the\n2020 came around. So I think other trend,\nwhich was starting",
    "start": "823060",
    "end": "832780"
  },
  {
    "text": "NLP was the idea of\nthese foundation models that could be trained\non a lot of text",
    "start": "832780",
    "end": "838149"
  },
  {
    "text": "and adapt it to a wide\nrange of downstream tasks. So ELMo, BERT, T5.",
    "start": "838150",
    "end": "845060"
  },
  {
    "text": "These were models that were\nfor their time, very exciting.",
    "start": "845060",
    "end": "851420"
  },
  {
    "text": "We maybe forget\nhow excited people were about things like\nBERT, but it was a big deal.",
    "start": "851420",
    "end": "857839"
  },
  {
    "text": "And then I think, this\nis abbreviated history, but I think one critical\npiece of the puzzle",
    "start": "857840",
    "end": "865660"
  },
  {
    "text": "is OpenAI, this taking\nthese ingredients, they",
    "start": "865660",
    "end": "870879"
  },
  {
    "text": "and applying very\nnice engineering and really pushing\non the scaling laws,",
    "start": "870880",
    "end": "878600"
  },
  {
    "text": "embracing it as, this\nis the mindset piece and that led to GPT-2 and GPT-3.",
    "start": "878600",
    "end": "885180"
  },
  {
    "text": "Google obviously was in the game\nand trying to compete as well.",
    "start": "885180",
    "end": "893100"
  },
  {
    "text": "But that paved the way,\nI think, to another line",
    "start": "893100",
    "end": "898440"
  },
  {
    "text": "of work, which is these\nwere all closed models, so models that weren't released.",
    "start": "898440",
    "end": "903910"
  },
  {
    "text": "And you can only access via API. But there were also\nan open models, starting with early\nwork by Eleuther right",
    "start": "903910",
    "end": "912030"
  },
  {
    "text": "after GPT-3 came out. Meta's early attempt, which\ndidn't work maybe as quite as",
    "start": "912030",
    "end": "918269"
  },
  {
    "text": "well, Bloom. And then meta Alibaba,\ndeep seek AI-2.",
    "start": "918270",
    "end": "924210"
  },
  {
    "text": "There's a few others\nwhich I haven't listed have been creating\nthese open models where",
    "start": "924210",
    "end": "931740"
  },
  {
    "text": "the weights are released. One other piece\nof about openness",
    "start": "931740",
    "end": "938040"
  },
  {
    "text": "is important is that there's\nmany levels of openness. There's closed\nmodels like GPT-4.",
    "start": "938040",
    "end": "943050"
  },
  {
    "text": "There's open weight models\nwhere the weights are available, and there's actually a paper,\na very nice paper with lots",
    "start": "943050",
    "end": "949910"
  },
  {
    "text": "of architectural details, but\nno details about the data set. And then there's\nOpen-Source models",
    "start": "949910",
    "end": "956480"
  },
  {
    "text": "where all the weights and data\nare available, on the paper that where they're\nhonestly trying",
    "start": "956480",
    "end": "961730"
  },
  {
    "text": "to explain as much as they can. But of course, you can't really\ncapture everything in a paper.",
    "start": "961730",
    "end": "969779"
  },
  {
    "text": "And there's no\nsubstitute for learning how to build a except\nfor doing yourself.",
    "start": "969780",
    "end": "976750"
  },
  {
    "text": "So that leads to\nthe present day, where there's a whole host of\nfrontier models from OpenAI,",
    "start": "976750",
    "end": "985170"
  },
  {
    "text": "Anthropic xAI Google, Meta,\nDeepSeek Alibaba, Tencent, and probably a few others that\ndominate the current landscape.",
    "start": "985170",
    "end": "995480"
  },
  {
    "text": "So we're interesting\ntime where you just reflect a lot of the\ningredients, like I said,",
    "start": "995480",
    "end": "1003350"
  },
  {
    "text": "were developed, which\nis good because I think we're going to revisit\nsome of those ingredients",
    "start": "1003350",
    "end": "1009010"
  },
  {
    "text": "and trace how these\ntechniques work. And then we're going to try\nto move as close as we can",
    "start": "1009010",
    "end": "1015759"
  },
  {
    "text": "to best practices\non frontier models. But using information from\nessentially the open community.",
    "start": "1015760",
    "end": "1026619"
  },
  {
    "text": "And reading between the lines\nfrom what we know about the closed models.",
    "start": "1026619",
    "end": "1032589"
  },
  {
    "text": "OK, so just as an interlude. ",
    "start": "1032589",
    "end": "1038260"
  },
  {
    "text": "What are you looking at here? So this is a executable lecture.",
    "start": "1038260",
    "end": "1044599"
  },
  {
    "text": "So it's a program where\nI'm stepping through and it delivers the\ncontent of lecture. So one thing that I\nthink is interesting here",
    "start": "1044599",
    "end": "1051940"
  },
  {
    "text": "is that you can embed code. So if you can just\nstep through code.",
    "start": "1051940",
    "end": "1058600"
  },
  {
    "text": "And I think this is a smaller\nscreen than I'm used to. But you can look at the\nenvironment variables",
    "start": "1058600",
    "end": "1063840"
  },
  {
    "text": "as you're stepping through code. So that's useful later when\nwe start actually trying",
    "start": "1063840",
    "end": "1069809"
  },
  {
    "text": "to drill down and\ngiving code examples, you can see the hierarchical\nstructure of a lecture like we're in this module.",
    "start": "1069810",
    "end": "1075370"
  },
  {
    "text": "And you see where it\nwas called from main. And you can jump to definitions\nlike supervised fine-tuning,",
    "start": "1075370",
    "end": "1082659"
  },
  {
    "text": "which we'll talk about later. OK, and if you think this\nlooks like a Python program,",
    "start": "1082660",
    "end": "1090180"
  },
  {
    "text": "well, it is a Python program. But I've processed it for\nyour viewing pleasure, OK.",
    "start": "1090180",
    "end": "1101180"
  },
  {
    "text": "So let's move on to the\ncourse logistics now.",
    "start": "1101180",
    "end": "1106205"
  },
  {
    "text": " Actually, maybe I'll\npause for questions.",
    "start": "1106205",
    "end": "1112649"
  },
  {
    "text": "Any questions about what\nwe're learning in this class.",
    "start": "1112650",
    "end": "1120140"
  },
  {
    "text": "Yeah So would you expect that\ngraduate from this class",
    "start": "1120140",
    "end": "1125150"
  },
  {
    "text": "to be able to lead a team\nto build a frontier model or are their skills\nstill necessary?",
    "start": "1125150",
    "end": "1130970"
  },
  {
    "text": "So the question is,\nwould I expect a graduate from this class to be\nable to lead a team and build a frontier model?",
    "start": "1130970",
    "end": "1136679"
  },
  {
    "text": "Of course with like a\nbillion dollars of capital. Yeah, of course. I would say that\nit's a good step.",
    "start": "1136680",
    "end": "1144900"
  },
  {
    "text": "But there's definitely many\npieces that are missing. And I think we thought\nabout we should really",
    "start": "1144900",
    "end": "1152120"
  },
  {
    "text": "teach like a series of classes\nthat eventually leads up to as close as we can get.",
    "start": "1152120",
    "end": "1157830"
  },
  {
    "text": "But I think this is maybe\nthe first step of the puzzle. But there are a lot\nof things, and I'm",
    "start": "1157830",
    "end": "1163730"
  },
  {
    "text": "happy to talk\noffline about that. But I like the ambition. That's what you should\nbe taking the class.",
    "start": "1163730",
    "end": "1170160"
  },
  {
    "text": "So you can go lead teams\nand build frontier models. OK",
    "start": "1170160",
    "end": "1177200"
  },
  {
    "text": "Let's talk a little\nbit about the course. So here's a website. Everything's online.",
    "start": "1177200",
    "end": "1182510"
  },
  {
    "text": "This is a five unit class. But I think that maybe it\ndoesn't express the level",
    "start": "1182510",
    "end": "1190809"
  },
  {
    "text": "here as well as this\nquote that I pulled out from a course evaluation. The entire assignment was\napproximately the same amount",
    "start": "1190810",
    "end": "1197559"
  },
  {
    "text": "of work as all five\nassignments from the CS224N, plus the final project. And that's the first\nhomework assignment.",
    "start": "1197560",
    "end": "1203808"
  },
  {
    "text": "So not to all scare you off, but\njust give you some data here.",
    "start": "1203808",
    "end": "1210610"
  },
  {
    "text": "So why should you endure that? Why should you do it? I think this class is really for\npeople who have obsessive need",
    "start": "1210610",
    "end": "1220240"
  },
  {
    "text": "to understand how things work\nall the way down to the atom, so to speak.",
    "start": "1220240",
    "end": "1225500"
  },
  {
    "text": "And I think when you\nget through this class, I think you will have\nreally leveled up",
    "start": "1225500",
    "end": "1230890"
  },
  {
    "text": "in terms of your\nresearch, engineering, and the level of\ncomfort that you'll have in building\nML systems at scale",
    "start": "1230890",
    "end": "1237010"
  },
  {
    "text": "would just be I think something. There's also a bunch\nof reasons that you",
    "start": "1237010",
    "end": "1242890"
  },
  {
    "text": "shouldn't take the class. For example, if you want to get\nany research done this quarter,",
    "start": "1242890",
    "end": "1248330"
  },
  {
    "text": "maybe this class isn't for you. If you're interested in\nlearning just about the hottest",
    "start": "1248330",
    "end": "1253720"
  },
  {
    "text": "new techniques, there\nare many other classes that can probably\ndeliver on that",
    "start": "1253720",
    "end": "1259029"
  },
  {
    "text": "better than, for\nexample, you spending a lot of time debugging BP.",
    "start": "1259030",
    "end": "1265060"
  },
  {
    "text": "And this is really I think\na class about the primitives",
    "start": "1265060",
    "end": "1270100"
  },
  {
    "text": "and learning things bottom\nup as opposed to the latest.",
    "start": "1270100",
    "end": "1276250"
  },
  {
    "text": "And also if you're interested in\nbuilding language models or 4X,",
    "start": "1276250",
    "end": "1283330"
  },
  {
    "text": "this is probably not the first\nclass you would take, I think,",
    "start": "1283330",
    "end": "1288860"
  },
  {
    "text": "practically speaking. As much as I made\nfun of prompting. Prompting is great.",
    "start": "1288860",
    "end": "1294220"
  },
  {
    "text": "Fine-tuning is great. If you can do that\nand it works, then I think that is something you\nshould absolutely start with.",
    "start": "1294220",
    "end": "1301000"
  },
  {
    "text": "So I don't want people\ntaking this class and thinking that,\ngreat, any problem.",
    "start": "1301000",
    "end": "1306310"
  },
  {
    "text": "The first step is to train a\nlanguage model from scratch. That is not the right\nway of thinking about it.",
    "start": "1306310",
    "end": "1314624"
  },
  {
    "text": "And I know that many of you,\nsome of you are enrolled,",
    "start": "1314624",
    "end": "1320157"
  },
  {
    "text": "but we didn't. We did have a cap, so we\nweren't able to enroll everyone. And also for the people online,\nyou can follow it at home.",
    "start": "1320157",
    "end": "1327812"
  },
  {
    "text": "All the lecture\nmaterials and assignments are online so you\ncan look at them. The lectures are also\nrecorded and will",
    "start": "1327812",
    "end": "1335190"
  },
  {
    "text": "be put on YouTube,\nalthough there will be some number of week lag there.",
    "start": "1335190",
    "end": "1341039"
  },
  {
    "text": "And also we'll offer\nthis class next year. So if you were not able to\ntake it this year, don't fret,",
    "start": "1341040",
    "end": "1348190"
  },
  {
    "text": "there will be next time. OK, so the class\nhas five assignments",
    "start": "1348190",
    "end": "1355519"
  },
  {
    "text": "and each of the assignments we\ndon't provide scaffolding code",
    "start": "1355520",
    "end": "1361070"
  },
  {
    "text": "in the sense that\nyou're literally giving you a blank\nfile and you're supposed to build things up.",
    "start": "1361070",
    "end": "1369050"
  },
  {
    "text": "And in the spirit of learning,\nbuilding from scratch. But we're not that mean.",
    "start": "1369050",
    "end": "1375740"
  },
  {
    "text": "We do provide unit tests\nand some adapter interfaces that allow you to check\ncorrectness of different pieces.",
    "start": "1375740",
    "end": "1383279"
  },
  {
    "text": "And also the assignment write\nup, if you walk through it, does do it for a gentle\njob of doing that,",
    "start": "1383280",
    "end": "1388980"
  },
  {
    "text": "but you are on your own for\nmaking good software design decisions and figuring out\nwhat you name your functions",
    "start": "1388980",
    "end": "1396290"
  },
  {
    "text": "and how to organize your\ncode, which is a useful skill, I think.",
    "start": "1396290",
    "end": "1402860"
  },
  {
    "text": "So one strategy\nfor all assignments is that there is a\npiece of assignment, which is just\nimplement the thing",
    "start": "1402860",
    "end": "1410000"
  },
  {
    "text": "and make sure it's correct. That mostly you can do\nlocally on your laptop.",
    "start": "1410000",
    "end": "1415799"
  },
  {
    "text": "You shouldn't need\ncompute for that. And then we have\na cluster that you",
    "start": "1415800",
    "end": "1420830"
  },
  {
    "text": "can run for bench marking\nboth accuracy and speed. So I want everyone\nto embrace this idea",
    "start": "1420830",
    "end": "1427940"
  },
  {
    "text": "of that you want to\nuse a small data set, or as few resources as\npossible to prototype",
    "start": "1427940",
    "end": "1435050"
  },
  {
    "text": "before running large jobs. You shouldn't be debugging with\none billion parameter models on the cluster if\nyou can help it.",
    "start": "1435050",
    "end": "1442669"
  },
  {
    "text": "OK, there's some\nassignments, which will have a leaderboard\nwhich usually is of the form.",
    "start": "1442670",
    "end": "1451529"
  },
  {
    "text": "Do things to make perplexity go\ndown given a particular training budget. Last year it was, I\nthink, pretty exciting",
    "start": "1451530",
    "end": "1459470"
  },
  {
    "text": "for people to try different\nthings that you either learn from the class\nor you read online.",
    "start": "1459470",
    "end": "1468320"
  },
  {
    "text": "And then finally, I\nguess this year is, this was less of a\nproblem last year",
    "start": "1468320",
    "end": "1474650"
  },
  {
    "text": "because I guess\nCoPilot wasn't as good. But Cursor is pretty good. So I think our general strategy\nis that AI tools can take away",
    "start": "1474650",
    "end": "1485210"
  },
  {
    "text": "from learning because there are\ncases where it can just solve the thing you want it to do.",
    "start": "1485210",
    "end": "1490559"
  },
  {
    "text": "But I think you can obviously\nuse them judiciously. But use it at your own risk.",
    "start": "1490560",
    "end": "1496230"
  },
  {
    "text": "You're responsible for your\nown learning experience here. OK, so we do have a cluster.",
    "start": "1496230",
    "end": "1503759"
  },
  {
    "text": "So thank you Together\nAI for providing a bunch of H100s for us. There's a guide please\nread it carefully",
    "start": "1503760",
    "end": "1511190"
  },
  {
    "text": "to learn how to use the cluster\nand start your assignments early, because the\ncluster will fill up",
    "start": "1511190",
    "end": "1519620"
  },
  {
    "text": "towards the end of a\ndeadline as everyone's trying to get their\nlarge runs in. ",
    "start": "1519620",
    "end": "1526710"
  },
  {
    "text": "OK, any questions about that? You mentioned it was\na five minute class.",
    "start": "1526710",
    "end": "1532360"
  },
  {
    "text": "Are you able to sign up for\nit for less than five units? Because I noticed\nthat it was on. So the question is, can you sign\nup for less than five minutes.",
    "start": "1532360",
    "end": "1539980"
  },
  {
    "text": "I think administratively, if\nyou have to sign up for less. That is possible, but\nit's the same class",
    "start": "1539980",
    "end": "1546393"
  },
  {
    "text": "and the same workload.  Any other questions?",
    "start": "1546393",
    "end": "1552316"
  },
  {
    "text": " OK, So in this part\nI'm going to go",
    "start": "1552316",
    "end": "1559710"
  },
  {
    "text": "through all the different\ncomponents of the course and just give a broad\noverview, a preview of what",
    "start": "1559710",
    "end": "1565919"
  },
  {
    "text": "you're going to experience. So remember, it's all\nabout efficiency given hardware and data, how do\nyou train the best model",
    "start": "1565920",
    "end": "1574710"
  },
  {
    "text": "given your resources? So for example, if I give you\na Common Crawl dump, a web dump",
    "start": "1574710",
    "end": "1579720"
  },
  {
    "text": "and 32 H100s for two\nweeks, what should you do? There are a lot of\ndifferent design decisions.",
    "start": "1579720",
    "end": "1587270"
  },
  {
    "text": "There's questions about the\ntokenizer, the architecture, systems optimizations you can\ndo, data things you can do.",
    "start": "1587270",
    "end": "1594600"
  },
  {
    "text": "And we've organized the\nclass into these five units or pillars.",
    "start": "1594600",
    "end": "1600480"
  },
  {
    "text": "So I'm going to go through\neach of them in turn and talk about what\nwe'll cover, what",
    "start": "1600480",
    "end": "1607610"
  },
  {
    "text": "the assignment will involve. And then I'll wrap up.",
    "start": "1607610",
    "end": "1613100"
  },
  {
    "text": "OK, so the goal\nof the basics unit is just get a basic version\nof a full pipeline working.",
    "start": "1613100",
    "end": "1619500"
  },
  {
    "text": "So here you implement a\ntokenizer model architecture and training. So I'll just say a bit more\nabout what these components are.",
    "start": "1619500",
    "end": "1627510"
  },
  {
    "text": "So a tokenizer is something\nthat converts between strings",
    "start": "1627510",
    "end": "1632630"
  },
  {
    "text": "and sequences of integers. Intuitively you can think about\nthe integers corresponding",
    "start": "1632630",
    "end": "1638149"
  },
  {
    "text": "to breaking up the\nstring into segments and mapping each\nsegment to an integer.",
    "start": "1638150",
    "end": "1644870"
  },
  {
    "text": "And the idea is that\nyour sequence of integers is what goes into the\nactual model, which has",
    "start": "1644870",
    "end": "1650720"
  },
  {
    "text": "to be like a fixed dimension. OK, so in this course we'll talk\nabout the Byte-Pair encoding,",
    "start": "1650720",
    "end": "1658280"
  },
  {
    "text": "BPE tokenizer, which\nis relatively simple and still is used.",
    "start": "1658280",
    "end": "1668150"
  },
  {
    "text": "There are promising\nset of methods",
    "start": "1668150",
    "end": "1673400"
  },
  {
    "text": "on tokenizer free approaches. So these are methods that\njust start with the raw bytes",
    "start": "1673400",
    "end": "1678860"
  },
  {
    "text": "and don't do\ntokenization and develop a particular architecture\nthat just takes the raw bytes.",
    "start": "1678860",
    "end": "1686659"
  },
  {
    "text": "This work is promising. But so far I haven't seen it\nbeen scaled to the frontier yet.",
    "start": "1686660",
    "end": "1692670"
  },
  {
    "text": "So we'll go with BP for now. OK, So once you've tokenized\nyour sequence or strings",
    "start": "1692670",
    "end": "1699580"
  },
  {
    "text": "into a sequence of integers. Now we define a\nmodel architecture over these sequences.",
    "start": "1699580",
    "end": "1706010"
  },
  {
    "text": "So the starting point here\nis original transformer. That's what is the backbone of\nbasically all frontier models.",
    "start": "1706010",
    "end": "1717730"
  },
  {
    "text": "And here's our\narchitectural diagram. We won't go into details here\nbut there's an attention piece.",
    "start": "1717730",
    "end": "1725780"
  },
  {
    "text": "And then there's a MLP layer\nwith some normalization.",
    "start": "1725780",
    "end": "1732580"
  },
  {
    "text": "So a lot has actually\nhappened since 2017. I think there's a sense in which\nall the transformer was invented",
    "start": "1732580",
    "end": "1741620"
  },
  {
    "text": "and then everyone's just\nusing transformer n. To a first approximation\nthat's true. We're still using\nthe same recipe,",
    "start": "1741620",
    "end": "1747289"
  },
  {
    "text": "but there have been a bunch\nof smaller improvements that do make a substantial difference\nwhen you add them all up.",
    "start": "1747290",
    "end": "1755120"
  },
  {
    "text": "So for example, there is\nthe activation, non-linear activation function.",
    "start": "1755120",
    "end": "1760809"
  },
  {
    "text": "So swiGLU which we saw\na little bit before. Positional embeddings. There's new\npositional embeddings",
    "start": "1760810",
    "end": "1767250"
  },
  {
    "text": "these rotary\npositional embeddings which we'll talk about. Normalization.",
    "start": "1767250",
    "end": "1774799"
  },
  {
    "text": "instead of using layer norm\nwe're going to look at something called RMSNorm, which\nis similar but simpler.",
    "start": "1774800",
    "end": "1780510"
  },
  {
    "text": "There's a question of where you\nplace the normalization, which has been changed from\nthe original transformer.",
    "start": "1780510",
    "end": "1786210"
  },
  {
    "text": "The MLP use the canonical\nversion is a dense MLP, and you can replace that\nwith mixture of experts.",
    "start": "1786210",
    "end": "1793920"
  },
  {
    "text": "Attention is something that\nhas actually been gaining a lot of attention, I guess.",
    "start": "1793920",
    "end": "1800250"
  },
  {
    "text": "There's full attention\nand then there's sliding window attention\nand linear attention.",
    "start": "1800250",
    "end": "1805780"
  },
  {
    "text": "All of these are trying to\nprevent the quadratic blowup. There's also lower dimensional\nversions like gqa and MLA,",
    "start": "1805780",
    "end": "1814390"
  },
  {
    "text": "which we'll get to in a\nsecond oh, not in a second, but in a future lecture.",
    "start": "1814390",
    "end": "1820000"
  },
  {
    "text": "And then the most\nmaybe radical thing is other alternatives\nto the transformer,",
    "start": "1820000",
    "end": "1826750"
  },
  {
    "text": "like state-space\nmodels like hyena, where they're not\ndoing attention,",
    "start": "1826750",
    "end": "1832240"
  },
  {
    "text": "but some other operation. And sometimes you get\nbest of both worlds",
    "start": "1832240",
    "end": "1839039"
  },
  {
    "text": "by making a hybrid\nmodel that mixes these in with transformers.",
    "start": "1839040",
    "end": "1845669"
  },
  {
    "text": "OK, so once you define your\narchitecture, you need to train. So design decisions\ninclude optimizer.",
    "start": "1845670",
    "end": "1852549"
  },
  {
    "text": "So AdamW which is\nbasically Adam fixed up",
    "start": "1852550",
    "end": "1858090"
  },
  {
    "text": "is still very prominent. So we'll mostly work with that. But it is worth\nmentioning that there",
    "start": "1858090",
    "end": "1864390"
  },
  {
    "text": "is more recent\noptimizers like muon and SOAP that have\nshown promise.",
    "start": "1864390",
    "end": "1870000"
  },
  {
    "text": "Learning rate\nschedule, batch size whether you do\nregularization or not.",
    "start": "1870000",
    "end": "1876100"
  },
  {
    "text": "Hyperparameters there's\na lot of details here, and I think this class is one\nwhere the details do matter,",
    "start": "1876100",
    "end": "1884950"
  },
  {
    "text": "because you can easily\nhave order of magnitude difference between a\nwell-tuned architecture",
    "start": "1884950",
    "end": "1890580"
  },
  {
    "text": "and something that's just\nlike a vanilla transformer. So in assignment\none, basically you'll",
    "start": "1890580",
    "end": "1896760"
  },
  {
    "text": "implement the BPE tokenizer. I'll warn you that\nthis is actually",
    "start": "1896760",
    "end": "1903600"
  },
  {
    "text": "the part that seems to have\nbeen a lot of surprising, maybe a lot of work for people.",
    "start": "1903600",
    "end": "1910300"
  },
  {
    "text": "So you're warned. And you also implement the\ntransformer cross-entropy loss",
    "start": "1910300",
    "end": "1918330"
  },
  {
    "text": "to AdamW optimizer\nand training loop. So again the whole stack. And we're not making you\nimplement PyTorch from scratch.",
    "start": "1918330",
    "end": "1927490"
  },
  {
    "text": "So you can use PyTorch but\nyou can't use the transformer implementation for PyTorch.",
    "start": "1927490",
    "end": "1934540"
  },
  {
    "text": "There's a small list of\nfunctions that you can use and you can only use those.",
    "start": "1934540",
    "end": "1940539"
  },
  {
    "text": "OK, so we're going to have some\nTinyStories and OpenWeb Text,",
    "start": "1940540",
    "end": "1945760"
  },
  {
    "text": "data sets that you'll train on. And then there will\nbe a leaderboard to minimize the Open\nWeb Text perplexity.",
    "start": "1945760",
    "end": "1952240"
  },
  {
    "text": "Will give you 90 minutes on\nH100s and see what you can do. So this is last year.",
    "start": "1952240",
    "end": "1959860"
  },
  {
    "text": "So see we have the top. So this is the number to\nbeat for this year OK.",
    "start": "1959860",
    "end": "1966670"
  },
  {
    "text": "All right. So that's the basics. Now after basics I mean,\nin some sense you're done.",
    "start": "1966670",
    "end": "1974840"
  },
  {
    "text": "You have ability to\ntrain a transformer. What else do you need? So the system part\nreally goes into how",
    "start": "1974840",
    "end": "1985120"
  },
  {
    "text": "you can optimize this further. So how do you get the\nmost out of hardware. And for this, we need to take\na closer look at the hardware",
    "start": "1985120",
    "end": "1992530"
  },
  {
    "text": "and how we can leverage it. So there's kernels,\nparallelism and inference",
    "start": "1992530",
    "end": "1997740"
  },
  {
    "text": "are the three\ncomponents of this unit. OK, so to first\ntalk about kernels",
    "start": "1997740",
    "end": "2004970"
  },
  {
    "text": "let's talk a little bit about\nwhat a GPU looks like, OK. So a GPU which will\nget much more into",
    "start": "2004970",
    "end": "2013130"
  },
  {
    "text": "is basically a huge array\nof these little units that",
    "start": "2013130",
    "end": "2020540"
  },
  {
    "text": "do floating point operations. And maybe the one thing to note\nis that this is the GPU chip.",
    "start": "2020540",
    "end": "2028590"
  },
  {
    "text": "And here is the memory\nthat's actually off chip.",
    "start": "2028590",
    "end": "2034580"
  },
  {
    "text": "And then there's some\nother memory like L2 caches and L1 caches on chip.",
    "start": "2034580",
    "end": "2039930"
  },
  {
    "text": "And so the basic idea is that\ncompute has to happen here.",
    "start": "2039930",
    "end": "2045480"
  },
  {
    "text": "Your data might\nbe somewhere else. And how do you basically\norganize your compute so that you can\nbe most efficient.",
    "start": "2045480",
    "end": "2054330"
  },
  {
    "text": "So one quick analogy is imagine\nthat your memory is where",
    "start": "2054330",
    "end": "2061280"
  },
  {
    "text": "you can store your data\nand model parameters, is like a warehouse and your\ncompute is like the factory.",
    "start": "2061280",
    "end": "2070879"
  },
  {
    "text": "And what ends up\nbeing a big bottleneck is just data movement costs.",
    "start": "2070880",
    "end": "2078169"
  },
  {
    "text": "So the thing that\nwe have to do is how do you organize the compute,\neven a matrix multiplication,",
    "start": "2078170",
    "end": "2085739"
  },
  {
    "text": "to maximize the\nutilization of the GPUs by minimizing the data movement?",
    "start": "2085739",
    "end": "2091980"
  },
  {
    "text": "And there's a\nbunch of techniques like fusion and tiling\nthat allow you to do that.",
    "start": "2091980",
    "end": "2098309"
  },
  {
    "text": "So we'll get all into\nthe details of that. And to implement and\nleverage the kernel",
    "start": "2098310",
    "end": "2104450"
  },
  {
    "text": "we're going to look at Triton. There's other things you\ncan do with various levels of sophistication,\nbut we're going",
    "start": "2104450",
    "end": "2110650"
  },
  {
    "text": "to use Triton,\nwhich is developed by OpenAI in a popular\nway to build kernels. OK, so we're going to\nwrite some kernels.",
    "start": "2110650",
    "end": "2117020"
  },
  {
    "text": "That's for one GPU. So now in general, you\nhave these big runs",
    "start": "2117020",
    "end": "2122800"
  },
  {
    "text": "take tens of thousands of GPUs.",
    "start": "2122800",
    "end": "2129032"
  },
  {
    "text": "But even at eight it\nstarts becoming interesting because you have a lot of GPUs.",
    "start": "2129032",
    "end": "2134420"
  },
  {
    "text": "They're connected\nto some CPU nodes. And they also are directly\nconnected via NVswitch NVLink",
    "start": "2134420",
    "end": "2142539"
  },
  {
    "text": "and it's the same idea.",
    "start": "2142540",
    "end": "2147610"
  },
  {
    "text": "Now, the only thing is that\ndata movement between GPUs is even slower.",
    "start": "2147610",
    "end": "2153100"
  },
  {
    "text": "And so we need to\nfigure out how to put model parameters and\nactivations and gradients",
    "start": "2153100",
    "end": "2161200"
  },
  {
    "text": "and put them on the GPUs and\ndo the computation to minimize the amount of movement.",
    "start": "2161200",
    "end": "2168390"
  },
  {
    "text": "And then so we're\ngoing to explore different type of techniques\nlike data parallelism and tensor\nparallelism and so on.",
    "start": "2168390",
    "end": "2176260"
  },
  {
    "text": "So that's all I'll\nsay about that. And finally,\ninference is something",
    "start": "2176260",
    "end": "2184080"
  },
  {
    "text": "that we didn't actually\ndo last year in the class, although we had a guest lecture.",
    "start": "2184080",
    "end": "2190380"
  },
  {
    "text": "But this is important because\ninference is how you actually",
    "start": "2190380",
    "end": "2196079"
  },
  {
    "text": "use a model. It's basically the task\nof generating tokens given a prompt, given\na trained model.",
    "start": "2196080",
    "end": "2202570"
  },
  {
    "text": "And it also turns\nout to be really useful for a bunch\nof other things besides just chatting\nwith your favorite model.",
    "start": "2202570",
    "end": "2212020"
  },
  {
    "text": "You need it for\nreinforcement learning test time compute, which has\nbeen, very popular lately.",
    "start": "2212020",
    "end": "2217750"
  },
  {
    "text": "And even evaluating models,\nyou need to do inference. So we're going to spend some\ntime talking about inference.",
    "start": "2217750",
    "end": "2224670"
  },
  {
    "text": "Actually if you think about\nthe globally, the cost that's spent on inference\nit's eclipsing the cost",
    "start": "2224670",
    "end": "2234890"
  },
  {
    "text": "that it is used to train models. Because training, despite\nit being very intensive,",
    "start": "2234890",
    "end": "2240630"
  },
  {
    "text": "is ultimately a one time cost. And inference is cost\nscales with every use.",
    "start": "2240630",
    "end": "2246420"
  },
  {
    "text": "And the more people\nuse your model, the more you'll need\ninference to be efficient.",
    "start": "2246420",
    "end": "2252980"
  },
  {
    "text": "OK, so in inference\nthere's two phases.",
    "start": "2252980",
    "end": "2259020"
  },
  {
    "text": "There's a pre-fill and a decode. Pre-fill is you take\nthe prompt and you can run it through the model,\nand get some activations.",
    "start": "2259020",
    "end": "2266460"
  },
  {
    "text": "And then decode is you go\nautoregressively one by one and generate tokens.",
    "start": "2266460",
    "end": "2271670"
  },
  {
    "text": "So pre-fill all the\ntokens are given. So you can process\neverything at once.",
    "start": "2271670",
    "end": "2276900"
  },
  {
    "text": "So this is exactly what\nyou see at training time. And generally, this\nis a good setting",
    "start": "2276900",
    "end": "2282110"
  },
  {
    "text": "to be in because it's\nnaturally parallel and you're mostly compute bound.",
    "start": "2282110",
    "end": "2287910"
  },
  {
    "text": "What makes inference\nspecial and difficult is that this\nautoregressive decoding,",
    "start": "2287910",
    "end": "2293160"
  },
  {
    "text": "you need to generate\none token at a time, and it's hard to actually\nsaturate all your GPUs. And it becomes memory bound\nbecause you're constantly",
    "start": "2293160",
    "end": "2300590"
  },
  {
    "text": "moving data around. And we'll talk about a few\nways to speed the models up.",
    "start": "2300590",
    "end": "2306590"
  },
  {
    "text": "To speed inference up you\ncan use a cheaper model, you can use this\nreally cool technique",
    "start": "2306590",
    "end": "2312313"
  },
  {
    "text": "called speculative\ndecoding, where you use a cheaper model to\nscout ahead and generate",
    "start": "2312313",
    "end": "2318500"
  },
  {
    "text": "multiple tokens. And then if these tokens happen\nto be good, for some definition",
    "start": "2318500",
    "end": "2324200"
  },
  {
    "text": "good, you can have the full\nmodel just score in and accept",
    "start": "2324200",
    "end": "2329450"
  },
  {
    "text": "them all in parallel. And then there's a bunch\nof systems optimizations",
    "start": "2329450",
    "end": "2334460"
  },
  {
    "text": "that you can do as well. OK, so after the systems-- oh, OK, assignment two.",
    "start": "2334460",
    "end": "2340799"
  },
  {
    "text": "So you're going to\nimplement a kernel, you're going to implement\nsome parallelism.",
    "start": "2340800",
    "end": "2348000"
  },
  {
    "text": "So data parallel\nis very natural.",
    "start": "2348000",
    "end": "2353220"
  },
  {
    "text": "And so we'll do that. Some of the model\nparallelism like FSDP",
    "start": "2353220",
    "end": "2359510"
  },
  {
    "text": "turns out to be a bit\ncomplicated due from scratch. So we'll do a baby\nversion of that.",
    "start": "2359510",
    "end": "2367790"
  },
  {
    "text": "But I encourage you to learn\nand know about the full version. We'll go over the\nfull version in class,",
    "start": "2367790",
    "end": "2374130"
  },
  {
    "text": "but implementing from scratch\nmight be a bit too much.",
    "start": "2374130",
    "end": "2379609"
  },
  {
    "text": "And then I think\nan important thing is getting in the\nhabit of always bench-marking and profile. I think that's actually probably\nthe most important thing is",
    "start": "2379610",
    "end": "2388130"
  },
  {
    "text": "that you can implement things,\nbut unless you have feedback on how well your\nimplementation is going",
    "start": "2388130",
    "end": "2394519"
  },
  {
    "text": "and where the\nbottlenecks are, you're just going to be flying blind.",
    "start": "2394520",
    "end": "2399830"
  },
  {
    "text": "OK, so unit 3 is scaling laws.",
    "start": "2399830",
    "end": "2406140"
  },
  {
    "text": "And here the goal is you want\nto do experiments at small scale and figure things out, and then\npredict the hyperparameters",
    "start": "2406140",
    "end": "2414270"
  },
  {
    "text": "and loss at large scale. So here's a\nfundamental question.",
    "start": "2414270",
    "end": "2420970"
  },
  {
    "text": "So if I give you a flops budget\nwhat model size should you use?",
    "start": "2420970",
    "end": "2427080"
  },
  {
    "text": "If you use a larger\nmodel that means you can train on less data. And if you use a smaller model,\nyou can train on more data.",
    "start": "2427080",
    "end": "2432428"
  },
  {
    "text": "So what's the\nright balance here? And this has been\nstudied, quite extensively",
    "start": "2432428",
    "end": "2437550"
  },
  {
    "text": "and figured out by a series of\npapers from OpenAI and DeepMind. So if you hear the term\nChinchilla optimal,",
    "start": "2437550",
    "end": "2444400"
  },
  {
    "text": "this is what this\nis referring to. And the basic idea is that\nfor every compute budget",
    "start": "2444400",
    "end": "2451320"
  },
  {
    "text": "number of flops, you can\nvary the number of parameters of your model.",
    "start": "2451320",
    "end": "2456810"
  },
  {
    "text": "OK, and then you measure\nhow good that model is. So for every level\nof compute, you",
    "start": "2456810",
    "end": "2462270"
  },
  {
    "text": "can get the optimal\nparameter count. And then what you do is you\ncan fit a curve to extrapolate",
    "start": "2462270",
    "end": "2473790"
  },
  {
    "text": "and see if you had, let's say,\n1e22 FLOPS what would there be",
    "start": "2473790",
    "end": "2479460"
  },
  {
    "text": "the parameter size. And it turns out these\nminima, when you plot them, it's actually\nremarkably linear which",
    "start": "2479460",
    "end": "2488400"
  },
  {
    "text": "leads to this very, actually\nsimple but useful rule of thumb,",
    "start": "2488400",
    "end": "2493720"
  },
  {
    "text": "which is that if you have a\nparticular model of size n,",
    "start": "2493720",
    "end": "2501849"
  },
  {
    "text": "if you multiply by 20,\nthat's the number of tokens you should train on essentially. So that means if I say,\n1.4 billion parameter model",
    "start": "2501850",
    "end": "2511020"
  },
  {
    "text": "should be trained on\n28 billion tokens. OK, but this doesn't take\ninto account inference cost.",
    "start": "2511020",
    "end": "2517990"
  },
  {
    "text": "This is literally how can you\ntrain the best model, regardless of how big that model is.",
    "start": "2517990",
    "end": "2523661"
  },
  {
    "text": "So there are some\nlimitations here, but it's nonetheless\nbeen extremely useful for model development.",
    "start": "2523662",
    "end": "2529100"
  },
  {
    "text": "So in this assignment,\nthis is fun because we define a quote\nunquote \"training API, which",
    "start": "2529100",
    "end": "2537050"
  },
  {
    "text": "you can query with a particular\nset of hyperparameters, you specify the architecture,\nand batch size and so on.",
    "start": "2537050",
    "end": "2544740"
  },
  {
    "text": "And we return you a loss that\nyou your decisions will get",
    "start": "2544740",
    "end": "2549830"
  },
  {
    "text": "you.\" OK, so your job is you\nhave a FLOPS budget and you're going to\ntry to figure out",
    "start": "2549830",
    "end": "2556490"
  },
  {
    "text": "how to train a bunch of models,\nand then gather the data. You're going to fit a scaling\nlaw to the gathered data,",
    "start": "2556490",
    "end": "2564000"
  },
  {
    "text": "and then you're going to\nsubmit your prediction on what you would choose to\nbe, the hyperparameters, what",
    "start": "2564000",
    "end": "2570950"
  },
  {
    "text": "model size, and so\non at a larger scale.",
    "start": "2570950",
    "end": "2576410"
  },
  {
    "text": "OK,so this is a case where\nyou have to be really, we want to put you in this\nposition where there's some",
    "start": "2576410",
    "end": "2582910"
  },
  {
    "text": "stakes. I mean, this is not like\nburning real compute but once you run out of\nyour FLOPS budget that's it.",
    "start": "2582910",
    "end": "2590079"
  },
  {
    "text": "So you have to be very careful\nin terms of how you prioritize what experiments to\nrun, which is something",
    "start": "2590080",
    "end": "2597460"
  },
  {
    "text": "that the frontier labs\nhave to do all the time. And there will be a leaderboard\nfor this, which is minimize",
    "start": "2597460",
    "end": "2604120"
  },
  {
    "text": "loss given your FLOPS budget. Question?",
    "start": "2604120",
    "end": "2609910"
  },
  {
    "text": "I see those are links from 2024. Yeah. So if we're working\nahead, should we",
    "start": "2609910",
    "end": "2615940"
  },
  {
    "text": "expect assignments to\nchange over time or are these going to be the final? Yeah, so the question is that\nthese links are from 2024.",
    "start": "2615940",
    "end": "2626990"
  },
  {
    "text": "The rough structure will\nbe the same from 2025. There will be some\nmodifications. But if you look at these\nshould have a pretty good idea",
    "start": "2626990",
    "end": "2633640"
  },
  {
    "text": "of what to expect.  OK, so let's go into data now.",
    "start": "2633640",
    "end": "2642750"
  },
  {
    "text": "OK, so up until now\nyou have scaling laws. You have systems. You have your transformer\nimplementation everything.",
    "start": "2642750",
    "end": "2651280"
  },
  {
    "text": "You're really good to go. But data I would say is\na really key ingredient",
    "start": "2651280",
    "end": "2657660"
  },
  {
    "text": "that, I think differentiates\nin some sense. And the question to ask here is\nwhat do I want this model to do?",
    "start": "2657660",
    "end": "2666810"
  },
  {
    "text": "Because what the\nmodel does is I mean, mostly determined by the data.",
    "start": "2666810",
    "end": "2673829"
  },
  {
    "text": "If I train a\nmultilingual data, it will have multilingual\ncapabilities. If I trained on code or\nhave code capabilities.",
    "start": "2673830",
    "end": "2679710"
  },
  {
    "text": "It's very natural. And usually data sets\nare a conglomeration",
    "start": "2679710",
    "end": "2685320"
  },
  {
    "text": "of a lot of different pieces. This is from a pile,\nwhich is four years ago.",
    "start": "2685320",
    "end": "2691390"
  },
  {
    "text": "But the same idea I think holds,\nyou have data from the web. This is Common Crawl.",
    "start": "2691390",
    "end": "2698099"
  },
  {
    "text": "You have stackexchange,\nWikipedia, GitHub and different sources,\nwhich are curated.",
    "start": "2698100",
    "end": "2704310"
  },
  {
    "text": "And so in the data\nsection, we're going to start talking\nabout evaluation,",
    "start": "2704310",
    "end": "2709540"
  },
  {
    "text": "which is given a\nmodel, how do you evaluate whether it's any good. So we're going to talk about\nperplexity where measures are",
    "start": "2709540",
    "end": "2718380"
  },
  {
    "text": "standardized testing like MMLU. If you have models that\ngenerate utterances",
    "start": "2718380",
    "end": "2724950"
  },
  {
    "text": "for instruction following,\nhow do you evaluate that? There's also decisions about\nif you can ensemble or do",
    "start": "2724950",
    "end": "2733170"
  },
  {
    "text": "train of thought at test time. How does that affect\nyour evaluation? And then you can talk\nabout entire systems.",
    "start": "2733170",
    "end": "2743400"
  },
  {
    "text": "Evaluation of entire\nsystem, not just the language model because\nlanguage models often get these days plugged into some\nagentic system or something.",
    "start": "2743400",
    "end": "2753570"
  },
  {
    "text": "OK, so now after\nestablishing a valuation let's look at data curation.",
    "start": "2753570",
    "end": "2759869"
  },
  {
    "text": "So this is I think, an important\npoint that people don't realize. I often hear people\nsay, oh, we're training",
    "start": "2759870",
    "end": "2767030"
  },
  {
    "text": "the model on the internet. This just doesn't make sense. Data doesn't just\nfall from the sky.",
    "start": "2767030",
    "end": "2775740"
  },
  {
    "text": "And there's the internet that\nyou can pipe into your model.",
    "start": "2775740",
    "end": "2781430"
  },
  {
    "text": "Data has to always be\nactively acquired somehow. So even if you--",
    "start": "2781430",
    "end": "2790250"
  },
  {
    "text": "just as an example I always\ntell people, look at the data.",
    "start": "2790250",
    "end": "2795530"
  },
  {
    "text": "And so let's look at some data. So this is some\nCommon Crawl data.",
    "start": "2795530",
    "end": "2802500"
  },
  {
    "text": "We're going to\ntake 10 documents. And I think hopefully\nthis works OK.",
    "start": "2802500",
    "end": "2807660"
  },
  {
    "text": "I think the rendering\nis off but you can see.",
    "start": "2807660",
    "end": "2813250"
  },
  {
    "text": "This is a random\nsample of Common Crawl.",
    "start": "2813250",
    "end": "2820450"
  },
  {
    "text": "And you can see that this is\nmaybe not exactly the data.",
    "start": "2820450",
    "end": "2829700"
  },
  {
    "text": "Oh, here's some\nactual real text here. That's cool. But if you look at\nmost of Common Crawl",
    "start": "2829700",
    "end": "2835512"
  },
  {
    "text": "this is a different language. But you can also see this\nis very spammy sites. And you'll quickly realize that\na lot of the web is just trash.",
    "start": "2835512",
    "end": "2845329"
  },
  {
    "text": "And so well, OK, maybe\nthat's not that's surprising, but it's more trash than\nyou would actually expect,",
    "start": "2845330",
    "end": "2852200"
  },
  {
    "text": "I promise. So what I'm saying is\nthat there's a lot of work",
    "start": "2852200",
    "end": "2857440"
  },
  {
    "text": "that needs to happen in data. So you can crawl\nthe internet, you can take books, archives\nof papers, GitHub,",
    "start": "2857440",
    "end": "2866230"
  },
  {
    "text": "and there's actually a lot of\nprocessing that needs to happen. There's also legal questions\nabout what data you can",
    "start": "2866230",
    "end": "2873569"
  },
  {
    "text": "train on which we'll touch on. Nowadays, a lot\nof frontier models have to actually buy\ndata, because the data",
    "start": "2873570",
    "end": "2881609"
  },
  {
    "text": "on the internet that's\npublicly accessible is actually turns out to be, a bit limited\nfor the really frontier",
    "start": "2881610",
    "end": "2891809"
  },
  {
    "text": "performance. And also, I think it's\nimportant to remember that this data that's scraped,\nit's not actually text.",
    "start": "2891810",
    "end": "2898570"
  },
  {
    "text": "First of all, it's\nHTML or it's PDFs. Or in the case of code. It's just directories.",
    "start": "2898570",
    "end": "2903880"
  },
  {
    "text": "So there has to be an explicit\nprocess that takes this data and turns it in a text.",
    "start": "2903880",
    "end": "2910230"
  },
  {
    "text": "OK, so we're going to talk about\nthe transformation from HTML to text.",
    "start": "2910230",
    "end": "2916950"
  },
  {
    "text": "And this is going to\nbe a lossy process. So the trick is how can you\npreserve the content and some",
    "start": "2916950",
    "end": "2925470"
  },
  {
    "text": "of the structure without\nbasically just having an HTML.",
    "start": "2925470",
    "end": "2933150"
  },
  {
    "text": "Filtering as you\ncould surmise is going to be very important, both\nfor getting high quality data",
    "start": "2933150",
    "end": "2939329"
  },
  {
    "text": "but also removing\nharmful content. Generally, people train\nclassifiers to do this.",
    "start": "2939330",
    "end": "2944740"
  },
  {
    "text": "Deduplication is also\nan important step, which we'll talk about.",
    "start": "2944740",
    "end": "2949780"
  },
  {
    "text": "OK, so assignment four\nis all about data. We're going to give you the\nraw Common Crawl dump so you",
    "start": "2949780",
    "end": "2957240"
  },
  {
    "text": "can see just how bad it is. And you're going to\ntrain classifiers dedupe.",
    "start": "2957240",
    "end": "2963280"
  },
  {
    "text": "And then there's going\nto be a leaderboard where you're going to try to\nminimize perplexity given",
    "start": "2963280",
    "end": "2968820"
  },
  {
    "text": "your token budget. So now you have the data.",
    "start": "2968820",
    "end": "2974050"
  },
  {
    "text": "You've done this. Build all your fancy\nkernels you've trained. Now you can really train models.",
    "start": "2974050",
    "end": "2979570"
  },
  {
    "text": "But at this point,\nwhat you'll get is a model that can\ncomplete the next token.",
    "start": "2979570",
    "end": "2988050"
  },
  {
    "text": "this is called\nessentially base model. And I think about it as a model\nthat has a lot of raw potential,",
    "start": "2988050",
    "end": "2993569"
  },
  {
    "text": "but it needs to be aligned\nor modified in some way. And alignment is the\nprocess of making it useful.",
    "start": "2993570",
    "end": "2999720"
  },
  {
    "text": "So alignment captures a\nlot of different things.",
    "start": "2999720",
    "end": "3004940"
  },
  {
    "text": "But three things I\nthink it captures is that you want\nto get the language model to follow instructions.",
    "start": "3004940",
    "end": "3011930"
  },
  {
    "text": "Completing the next token\nis not necessarily following the instruction, it will\njust complete the instruction",
    "start": "3011930",
    "end": "3017050"
  },
  {
    "text": "or whatever it thinks will\nfollow the instruction. You get to here specify the\nstyle of the generation,",
    "start": "3017050",
    "end": "3025940"
  },
  {
    "text": "whether you want it to be a\nlong or short, whether you want bullets, whether you want\nit to be witty or have SaaS",
    "start": "3025940",
    "end": "3033099"
  },
  {
    "text": "or not. And when you play with\nChatGPT versus Grok,",
    "start": "3033100",
    "end": "3038900"
  },
  {
    "text": "you'll see that there's\ndifferent alignment that has happened. And then also safety.",
    "start": "3038900",
    "end": "3045370"
  },
  {
    "text": "One important thing\nis for these models to be able to refuse\nanswers that can be harmful.",
    "start": "3045370",
    "end": "3051130"
  },
  {
    "text": "So that's where\nalignment also kicks in. So there's generally\ntwo phases of alignment.",
    "start": "3051130",
    "end": "3057220"
  },
  {
    "text": "There's supervised fine-tuning. And here the goal is I\nmean it's very simple.",
    "start": "3057220",
    "end": "3062829"
  },
  {
    "text": "You basically gather a set\nof user assistant pairs.",
    "start": "3062830",
    "end": "3069660"
  },
  {
    "text": "So prompt response pairs. And then you do\nsupervised learning.",
    "start": "3069660",
    "end": "3075970"
  },
  {
    "text": "And the idea here is that\nthe base model already has the raw potential.",
    "start": "3075970",
    "end": "3082270"
  },
  {
    "text": "So just fine-tuning it on a\nfew examples is sufficient.",
    "start": "3082270",
    "end": "3087790"
  },
  {
    "text": "Of course, the more examples you\nhave, the better the results. But there's papers like this\none that shows even like 1,000",
    "start": "3087790",
    "end": "3094770"
  },
  {
    "text": "examples suffices to give\nyou instruction following capabilities from\na good base model.",
    "start": "3094770",
    "end": "3100950"
  },
  {
    "text": "So this part is\nactually very simple. And it's not that different\nfrom, pre-training",
    "start": "3100950",
    "end": "3107010"
  },
  {
    "text": "because it's just\nyou're given text and you're just maximize\nthe probability of the text.",
    "start": "3107010",
    "end": "3113580"
  },
  {
    "text": "So the second part is\na bit more interesting from an algorithmic perspective. So the idea here is that\neven with soft phase,",
    "start": "3113580",
    "end": "3121990"
  },
  {
    "text": "you will have a decent model. And now how do you improve it?",
    "start": "3121990",
    "end": "3127119"
  },
  {
    "text": "Well, you can get\nmore soft data, but that can be very\nexpensive because you have to have have someone\nsit down and annotate data.",
    "start": "3127120",
    "end": "3134200"
  },
  {
    "text": "So there the goal of\nlearning from feedback is that you can leverage\nlighter forms of annotation",
    "start": "3134200",
    "end": "3143340"
  },
  {
    "text": "and have the algorithms\ndo a bit more work. OK, so one type of data you can\nlearn from is preference data.",
    "start": "3143340",
    "end": "3150250"
  },
  {
    "text": "So this is where you\ngenerate multiple responses from a model to a given\nprompt like A or B.",
    "start": "3150250",
    "end": "3156580"
  },
  {
    "text": "And the user rates,\nwhether a or B is better. And so the data might look\nlike it generates what's",
    "start": "3156580",
    "end": "3164900"
  },
  {
    "text": "the best way to train a language\nmodel, use a large data set or use a small data set. And of course, the\nanswer should be A. So",
    "start": "3164900",
    "end": "3171650"
  },
  {
    "text": "that is a unit of\nexpressing preferences. Another type of supervision you\ncould have is using verifiers.",
    "start": "3171650",
    "end": "3180090"
  },
  {
    "text": "So for some domains\nyou're lucky enough to have a formal verifier\nlike for math or code.",
    "start": "3180090",
    "end": "3185730"
  },
  {
    "text": "Or you can use learned\nverifiers where you train an actual language\nmodel to rate the response.",
    "start": "3185730",
    "end": "3196740"
  },
  {
    "text": "And of course, this relates\nto evaluation again. Algorithms this is we're in the\nrealm of reinforcement learning.",
    "start": "3196740",
    "end": "3205110"
  },
  {
    "text": "So one of the\nearliest algorithms that was developed that was\napplied to instruction tuning",
    "start": "3205110",
    "end": "3212990"
  },
  {
    "text": "models was PPO Proximal\nPolicy Optimization.",
    "start": "3212990",
    "end": "3219410"
  },
  {
    "text": "It turns out that if you\njust have preference data, there's a much simpler\nalgorithm called DPO.",
    "start": "3219410",
    "end": "3224660"
  },
  {
    "text": "That works really\nwell, but in general, if you want to learn\nfrom verifiable data,",
    "start": "3224660",
    "end": "3231710"
  },
  {
    "text": "it's not preference data, so\nyou have to embrace RL fully. And there's this\nmethod, which will",
    "start": "3231710",
    "end": "3240160"
  },
  {
    "text": "do in this class, which is\ncalled group relative preference optimization, which simplifies\nPPO and makes it more efficient",
    "start": "3240160",
    "end": "3247090"
  },
  {
    "text": "by removing the value\nfunction developed by DeepSeek which seems to work pretty well.",
    "start": "3247090",
    "end": "3253630"
  },
  {
    "text": "OK, so assignment five. Implements supervised tuning,\nDPO and JPO and of course,",
    "start": "3253630",
    "end": "3260660"
  },
  {
    "text": "evaluate. Question. You gave I think, a quote\nfrom the course evaluation",
    "start": "3260660",
    "end": "3268119"
  },
  {
    "text": "about assignment one. Did people have similar things\nto say about assignments two through five or.",
    "start": "3268120",
    "end": "3273310"
  },
  {
    "text": "Yeah, the question is assignment\none seems a bit daunting. What about the other ones?",
    "start": "3273310",
    "end": "3278570"
  },
  {
    "text": "I would say that assignment\none and two are definitely the most heavy and hardest.",
    "start": "3278570",
    "end": "3284790"
  },
  {
    "text": "Assignment three is a bit more\nof a breather and assignment",
    "start": "3284790",
    "end": "3290220"
  },
  {
    "text": "four and five at\nleast last year were, I would say, a notch below\nassignment one and two.",
    "start": "3290220",
    "end": "3296940"
  },
  {
    "text": "Although it depends\non we haven't fully worked out the\ndetails for this year. ",
    "start": "3296940",
    "end": "3305340"
  },
  {
    "text": "Yeah, it does get better. OK, so just to recap of\nthe different pieces here,",
    "start": "3305340",
    "end": "3314849"
  },
  {
    "text": "remember efficiency is\nthis driving principle. And there's a bunch of\ndifferent design decisions.",
    "start": "3314850",
    "end": "3321069"
  },
  {
    "text": "And I think if you view\nefficiency everything",
    "start": "3321070",
    "end": "3326220"
  },
  {
    "text": "through the lens of efficiency\nI think a lot of things make sense. And importantly,\nI think it's worth",
    "start": "3326220",
    "end": "3335369"
  },
  {
    "text": "pointing out there\nwe are currently in this compute\nconstrained regime, at least this class,\nand most people who",
    "start": "3335370",
    "end": "3341760"
  },
  {
    "text": "are somewhat GPU poor. So we have a lot of data, but\nwe don't have that much compute.",
    "start": "3341760",
    "end": "3346840"
  },
  {
    "text": "And so these design decisions\nwill reflect squeezing the most out of the hardware. So for example, data\nprocessing, we're",
    "start": "3346840",
    "end": "3353160"
  },
  {
    "text": "filtering fairly\naggressively because we don't want to waste precious\ncompute on bad or relevant data.",
    "start": "3353160",
    "end": "3359280"
  },
  {
    "text": "Tokenization, like it's nice\nto have a model over bytes",
    "start": "3359280",
    "end": "3364440"
  },
  {
    "text": "that's very elegant. But it's very compute\ninefficient with today's model architectures. So we have to do tokenization\nas an efficiency gain.",
    "start": "3364440",
    "end": "3373170"
  },
  {
    "text": "Model architecture there are a\nlot of design decisions there that are essentially\nmotivated by efficiency.",
    "start": "3373170",
    "end": "3380250"
  },
  {
    "text": "Training I think\nthe fact that we're most of what we're going to\ndo is just a single epoch.",
    "start": "3380250",
    "end": "3385810"
  },
  {
    "text": "This is clearly\nwe're in a hurry. We just need to see more data as\nopposed to spend a lot of time",
    "start": "3385810",
    "end": "3391770"
  },
  {
    "text": "on any given data point. Scaling laws is completely\nabout efficiency. We use less compute to figure\nout the hyperparameters.",
    "start": "3391770",
    "end": "3399720"
  },
  {
    "text": "And alignment is maybe\na little bit different",
    "start": "3399720",
    "end": "3405520"
  },
  {
    "text": "but the connection\nto efficiency is that if you can put\nresources into alignment",
    "start": "3405520",
    "end": "3412740"
  },
  {
    "text": "then you actually require\nsmaller base models.",
    "start": "3412740",
    "end": "3418800"
  },
  {
    "text": "OK, so there's two paths. If your use case\nis fairly narrow,",
    "start": "3418800",
    "end": "3425140"
  },
  {
    "text": "you can probably\nuse a smaller model, you align it or fine tune\nit and you can do well. But if your CEU\ncases are very broad,",
    "start": "3425140",
    "end": "3432700"
  },
  {
    "text": "then there might\nnot be a substitute for training a big model. So that's today.",
    "start": "3432700",
    "end": "3438910"
  },
  {
    "text": "So increasingly now at\nleast for frontier labs, they're becoming\ndata constrained,",
    "start": "3438910",
    "end": "3445779"
  },
  {
    "text": "which is interesting\nbecause I think that the design decisions will\npresumably completely change.",
    "start": "3445780",
    "end": "3452560"
  },
  {
    "text": "Well, I mean, compute\nwill always be important, but I think the design\ndecisions will change, for example, taking\none epoch of your data,",
    "start": "3452560",
    "end": "3462350"
  },
  {
    "text": "I think doesn't\nreally make sense. If you have more\ncompute, why wouldn't you take more epochs at least,\nor do something smarter.",
    "start": "3462350",
    "end": "3469339"
  },
  {
    "text": "Or maybe there will be different\narchitectures, for example, because the transformer\nwas really motivated",
    "start": "3469340",
    "end": "3476530"
  },
  {
    "text": "by compute efficiency. So that's something to ponder.",
    "start": "3476530",
    "end": "3482060"
  },
  {
    "text": "So it's about efficiency. But the design decisions\nreflect what regime you're in. ",
    "start": "3482060",
    "end": "3489490"
  },
  {
    "text": "OK, so now I'm going to\ndive into the first unit.",
    "start": "3489490",
    "end": "3497710"
  },
  {
    "text": "Before that any questions. ",
    "start": "3497710",
    "end": "3504940"
  },
  {
    "text": "We have a Slack for Ed. The question is, if\nwe have a Slack or Ed? We will have a Slack.",
    "start": "3504940",
    "end": "3511220"
  },
  {
    "text": "We'll send out details\nafter this class. Yeah, well, students\nauditing the course,",
    "start": "3511220",
    "end": "3517480"
  },
  {
    "text": "also have access to\nthe same material? The question is,\nstudents auditing the class will have access\nto all the online, materials,",
    "start": "3517480",
    "end": "3528609"
  },
  {
    "text": "assignments. And we'll give you\naccess to Canvas so you can watch\nthe lecture videos.",
    "start": "3528610",
    "end": "3538260"
  },
  {
    "text": "Yeah, what's the grading\nof the assignments? What's the grading\nof the assignments?",
    "start": "3538260",
    "end": "3544349"
  },
  {
    "text": "Good question. So there will be a\nset of unit tests that you will have to pass.",
    "start": "3544350",
    "end": "3550660"
  },
  {
    "text": "So part of the grading is did\nyou implement this correctly. There will be also parts\nof the grade which will.",
    "start": "3550660",
    "end": "3556779"
  },
  {
    "text": "Did you implement a model that\nachieved a certain level of loss or is efficient enough?",
    "start": "3556780",
    "end": "3562319"
  },
  {
    "text": "In the assignment. Every problem part has a number\nof points associated with it,",
    "start": "3562320",
    "end": "3568600"
  },
  {
    "text": "and so that gives you\na fairly granular level of what grading looks like.",
    "start": "3568600",
    "end": "3574070"
  },
  {
    "text": " OK, let's jump\ninto tokenization.",
    "start": "3574070",
    "end": "3581660"
  },
  {
    "text": "OK, so Andrej Karpathy\nhas this really nice video on tokenization. And in general, he makes a\nlot of these videos on that.",
    "start": "3581660",
    "end": "3590890"
  },
  {
    "text": "Actually inspired a\nlot of this class. How you can build\nthings from scratch. So you should go check\nout some of his videos.",
    "start": "3590890",
    "end": "3599290"
  },
  {
    "text": "So tokenization as\nwe talked about it, is the process of taking\nraw text, which is generally",
    "start": "3599290",
    "end": "3606280"
  },
  {
    "text": "represented as Unicode\nstrings and turning it into a set of\nintegers essentially,",
    "start": "3606280",
    "end": "3614300"
  },
  {
    "text": "and where each integer,\nrepresents a token. So we need a procedure that\nencodes strings to tokens",
    "start": "3614300",
    "end": "3621970"
  },
  {
    "text": "and decodes them\nback into strings. And the vocabulary size is\njust the number of values",
    "start": "3621970",
    "end": "3629350"
  },
  {
    "text": "that a token take on. The range of the integers. OK, so just to give you an\nexample of how tokenizers work,",
    "start": "3629350",
    "end": "3638820"
  },
  {
    "text": "let's play around with this\nreally nice website, which allows you to look at\ndifferent tokenizers",
    "start": "3638820",
    "end": "3644210"
  },
  {
    "text": "and just type in something\nlike, hello, hello or whatever.",
    "start": "3644210",
    "end": "3653330"
  },
  {
    "text": "Maybe I'll do this. And one thing it does is it\nshows you the list of integers.",
    "start": "3653330",
    "end": "3662609"
  },
  {
    "text": "This is the output of tokenizer. It also nicely maps\nout the decomposition",
    "start": "3662610",
    "end": "3667760"
  },
  {
    "text": "of the original string\ninto a bunch of segments. And a few things to note.",
    "start": "3667760",
    "end": "3674369"
  },
  {
    "text": "First of all, the space\nis part of a token. So unlike classical NLP where\nthe space just disappears,",
    "start": "3674370",
    "end": "3682200"
  },
  {
    "text": "everything is accounted for. These are meant to be reversible\noperations, tokenization.",
    "start": "3682200",
    "end": "3689240"
  },
  {
    "text": "And by convention\nfor whatever reason, the space is usually\npreceding the token.",
    "start": "3689240",
    "end": "3697570"
  },
  {
    "text": "Also notice that hello is a\ncompletely different token than space hello, which you\nmight make you a little bit",
    "start": "3697570",
    "end": "3707890"
  },
  {
    "text": "squeamish, but seems\nit can cause problems. But that's just how it is.",
    "start": "3707890",
    "end": "3713840"
  },
  {
    "text": "Question. I was going to ask, is\nthe space being leading instead of trailing\nintentional, or is it just an artifact of the process?",
    "start": "3713840",
    "end": "3721510"
  },
  {
    "text": "So the question is the spacing\nbefore intentional or not?",
    "start": "3721510",
    "end": "3727300"
  },
  {
    "text": "So in the BP process\nthat we'll talk about you're actually pre-tokenize.",
    "start": "3727300",
    "end": "3733646"
  },
  {
    "text": " And then you tokenize each part.",
    "start": "3733646",
    "end": "3738740"
  },
  {
    "text": "And I think the pre-tokenizer\ndoes put the space in the front. So it is built\ninto the algorithm.",
    "start": "3738740",
    "end": "3744230"
  },
  {
    "text": "You could put it at the end. But I think it probably\nmakes more sense to put it in the beginning,\nbut actually, no.",
    "start": "3744230",
    "end": "3753619"
  },
  {
    "text": "Well, I guess it\ncould go either way. It's my sense.",
    "start": "3753620",
    "end": "3758770"
  },
  {
    "text": "OK, so then if you\nlook at numbers, you see that the\nnumbers are chopped down",
    "start": "3758770",
    "end": "3765010"
  },
  {
    "text": "into different pieces. It's a little bit interesting\nthat it's left to right.",
    "start": "3765010",
    "end": "3772817"
  },
  {
    "text": "So it's definitely not grouping\nby thousands or anything like semantic. But anyway I encourage\nyou to play with it",
    "start": "3772817",
    "end": "3778990"
  },
  {
    "text": "and get a sense of what these\nexisting tokenizers look like. So this is the tokenizer\nfor GPT-4o for example.",
    "start": "3778990",
    "end": "3789339"
  },
  {
    "text": "So there are some\nobservations that we made. So if you look at the\nGPT-2 tokenizer which will",
    "start": "3789340",
    "end": "3797769"
  },
  {
    "text": "use this as a reference, OK. Let me see if I can. ",
    "start": "3797770",
    "end": "3805450"
  },
  {
    "text": "OK, hopefully this is,\nlet me know if this is getting too small in the back. You take a string.",
    "start": "3805450",
    "end": "3812079"
  },
  {
    "text": "If you apply the GPT-2\ntokenizer, you get your indices. So it maps strings to\nindices, and then you",
    "start": "3812080",
    "end": "3819730"
  },
  {
    "text": "can decode to get\nback the string. And this is just a\nsanity check to make sure",
    "start": "3819730",
    "end": "3825880"
  },
  {
    "text": "that you actually round trips. Another thing that I guess,\ninteresting to look at",
    "start": "3825880",
    "end": "3832510"
  },
  {
    "text": "is this compression\nratio, which is if you look at the\nnumber of bytes divided",
    "start": "3832510",
    "end": "3837970"
  },
  {
    "text": "by the number of tokens. So how many bytes are\nrepresented by a token? And the answer here is 1.6 OK.",
    "start": "3837970",
    "end": "3847069"
  },
  {
    "text": "So every token represents\n1.6 bytes of data, OK. So that's just the GPT\ntokenizer that OpenAI trained.",
    "start": "3847070",
    "end": "3856270"
  },
  {
    "text": "To motivate BPE, I want to go\nthrough a sequence of attempts.",
    "start": "3856270",
    "end": "3861710"
  },
  {
    "text": "So suppose you wanted\nto do tokenization. What would be the\nsimplest thing?",
    "start": "3861710",
    "end": "3867070"
  },
  {
    "text": "The simplest thing is probably\ncharacter-based tokenization. A Unicode string is a sequence\nof Unicode characters,",
    "start": "3867070",
    "end": "3873839"
  },
  {
    "text": "and each character can be\nconverted into an integer and called a code point.",
    "start": "3873840",
    "end": "3878900"
  },
  {
    "text": "OK, so a maps to 97. The world emoji maps to 127,757.",
    "start": "3878900",
    "end": "3886830"
  },
  {
    "text": "And you can see that\nit converts back. So you can define a tokenizer\nwhich simply maps each character",
    "start": "3886830",
    "end": "3898280"
  },
  {
    "text": "into a code point. OK, so what's one\nproblem with this?",
    "start": "3898280",
    "end": "3907819"
  },
  {
    "text": "Yeah. Compression ratio was one? The compression ratio was one.",
    "start": "3907820",
    "end": "3914150"
  },
  {
    "text": "Well, actually the compression\nratio is not quite one because a character\nis not a byte,",
    "start": "3914150",
    "end": "3919340"
  },
  {
    "text": "but it's maybe not\nas good as you want. One problem with that\nif you look at some code",
    "start": "3919340",
    "end": "3925550"
  },
  {
    "text": "points they're\nactually really large. So you're basically allocating\neach one slot in your vocabulary",
    "start": "3925550",
    "end": "3935110"
  },
  {
    "text": "for every character uniformly. And some characters appear way\nmore frequently than others.",
    "start": "3935110",
    "end": "3940550"
  },
  {
    "text": "So this is not a very\neffective use of your budget.",
    "start": "3940550",
    "end": "3946570"
  },
  {
    "text": "OK, so the vocabulary\nsize is huge. I mean, the vocabulary\nsize being 127",
    "start": "3946570",
    "end": "3953200"
  },
  {
    "text": "is actually a big deal. But the bigger problem is\nthat some characters are rare.",
    "start": "3953200",
    "end": "3959269"
  },
  {
    "text": "And this is inefficient\nuse of the vocab. So the compression ratio\nis 1.5 in this case",
    "start": "3959270",
    "end": "3969370"
  },
  {
    "text": "because it's the number\nof bytes per token and a character can\nbe multiple bytes.",
    "start": "3969370",
    "end": "3978940"
  },
  {
    "text": "OK, so that was a\nvery naive approach. On the other hand, you can\ndo byte-based tokenization.",
    "start": "3978940",
    "end": "3986619"
  },
  {
    "text": "So Unicode strings\ncan be represented as a sequence of bytes.",
    "start": "3986620",
    "end": "3991690"
  },
  {
    "text": "Because every string can\njust be converted into bytes.",
    "start": "3991690",
    "end": "3999890"
  },
  {
    "text": "OK, so a, is already\njust kind of one byte. But some characters take\nup as many as four bytes.",
    "start": "3999890",
    "end": "4009640"
  },
  {
    "text": "And this is using the\nUTF-8 encoding of Unicode. There's other encodings, but\nthis is the most common one",
    "start": "4009640",
    "end": "4015900"
  },
  {
    "text": "that's dynamic. So let's just convert everything\ninto bytes and see what happens.",
    "start": "4015900",
    "end": "4025770"
  },
  {
    "text": "So if you do it into bytes. Now, all the indices\nare between 0 and 256,",
    "start": "4025770",
    "end": "4031000"
  },
  {
    "text": "because there are only\n256 possible values for a byte by definition. So your vocabulary is very\nsmall and each byte is.",
    "start": "4031000",
    "end": "4040300"
  },
  {
    "text": "I guess, not all bytes\nare equally used, but you don't have that\nmany sparsity problems.",
    "start": "4040300",
    "end": "4047945"
  },
  {
    "text": "But what's the problem\nwith byte-based encoding? ",
    "start": "4047945",
    "end": "4053780"
  },
  {
    "text": "Long sequences. Yeah, long sequences. I mean, in some ways I really\nwish encoding would work.",
    "start": "4053780",
    "end": "4060599"
  },
  {
    "text": "It's the most elegant thing. But you have long sequences.",
    "start": "4060600",
    "end": "4065970"
  },
  {
    "text": "Your compression ratio is\none, one byte per token. And this is just terrible.",
    "start": "4065970",
    "end": "4072117"
  },
  {
    "text": "A compression ratio\nof one is terrible because your sequences\nwill be really long. Attention is quadratic naively\nin the sequence length.",
    "start": "4072117",
    "end": "4079740"
  },
  {
    "text": "So you're just going to have a\nbad time in terms of efficiency. OK, so that wasn't really good.",
    "start": "4079740",
    "end": "4089180"
  },
  {
    "text": "So now the thing that\nyou might think about is, well, maybe we have\nto be adaptive here.",
    "start": "4089180",
    "end": "4098193"
  },
  {
    "text": "We can't allocate a character\nor a byte per token, but maybe some tokens can\nrepresent lots of bytes",
    "start": "4098194",
    "end": "4103839"
  },
  {
    "text": "and some tokens can\nrepresent few bytes. So one way to do this is\nword-based tokenization.",
    "start": "4103840",
    "end": "4109189"
  },
  {
    "text": "And this is something that I was\nactually very classic in NLP. So here's a string.",
    "start": "4109189",
    "end": "4114859"
  },
  {
    "text": "And you can just split it into\na sequence of segments, OK.",
    "start": "4114859",
    "end": "4123318"
  },
  {
    "text": "And you can call\neach of these tokens. So you just use a\nregular expression.",
    "start": "4123319",
    "end": "4128410"
  },
  {
    "text": "Here's a different\nregular expression that GPT-2 uses to pre-tokenize.",
    "start": "4128410",
    "end": "4133810"
  },
  {
    "text": "And it just splits your string\ninto a sequence of strings.",
    "start": "4133810",
    "end": "4142720"
  },
  {
    "text": "So and then what you\ndo with each segment is you assign each of these to an\ninteger and then you're done.",
    "start": "4142720",
    "end": "4150880"
  },
  {
    "text": "OK, so what's the\nproblem with this? ",
    "start": "4150880",
    "end": "4157420"
  },
  {
    "text": "Vocabulary size. Yeah, so the problem is\nthat your vocabulary size is unbounded or not,\nmaybe not quite unbounded,",
    "start": "4157420",
    "end": "4165080"
  },
  {
    "text": "but you don't know\nhow big it is. Because on a given\nnew input, you",
    "start": "4165080",
    "end": "4170560"
  },
  {
    "text": "might get a segment that just\nyou've never seen before.",
    "start": "4170560",
    "end": "4175670"
  },
  {
    "text": "And that's actually\na big problem. This is actually where bases is\na really big pain in the butt",
    "start": "4175670",
    "end": "4180880"
  },
  {
    "text": "because some real\nwords are rare. And, actually, it's\nreally annoying",
    "start": "4180880",
    "end": "4189250"
  },
  {
    "text": "because new words have to\nreceive this UNC token. And if you're not careful about\nhow you compute the perplexity,",
    "start": "4189250",
    "end": "4198010"
  },
  {
    "text": "then you're just\ngoing to mess up. So word-based I\nthink it captures",
    "start": "4198010",
    "end": "4205660"
  },
  {
    "text": "the right intuition\nof adaptivity, but it's not exactly\nwhat we want here.",
    "start": "4205660",
    "end": "4211989"
  },
  {
    "text": "So here, we're finally going\nto talk about the BPE encoding or byte pair encoding.",
    "start": "4211990",
    "end": "4218440"
  },
  {
    "text": "So this was actually\na very old algorithm developed by Phillip Gage\nin '94 for data compression.",
    "start": "4218440",
    "end": "4226410"
  },
  {
    "text": "And it was first\nintroduced into NLP for neural machine translation.",
    "start": "4226410",
    "end": "4232060"
  },
  {
    "text": "So before papers that\ndid machine translation or any basically all NLP\nused word-based tokenization.",
    "start": "4232060",
    "end": "4239970"
  },
  {
    "text": "And again, we're\nword-based with a pain. So this paper pioneered\nthis idea of, well,",
    "start": "4239970",
    "end": "4247470"
  },
  {
    "text": "we can use this nice\nalgorithm form 94. And we can just make the\ntokenization round trip.",
    "start": "4247470",
    "end": "4254710"
  },
  {
    "text": "And we don't have to deal with\nangst or any of that stuff. And then finally, this entered\nthe language modeling era",
    "start": "4254710",
    "end": "4262980"
  },
  {
    "text": "through GPT-2, which was trained\non using the BP tokenizer.",
    "start": "4262980",
    "end": "4271470"
  },
  {
    "text": "So the basic idea is\ninstead of defining some of preconceived\nnotion of how to split up.",
    "start": "4271470",
    "end": "4278820"
  },
  {
    "text": "We're going to train the\ntokenizer on raw text. That's the basic\ninsight if you will.",
    "start": "4278820",
    "end": "4284430"
  },
  {
    "text": "And so organically\ncommon sequences that span multiple\ncharacters, we're",
    "start": "4284430",
    "end": "4291050"
  },
  {
    "text": "going to try to\nrepresent as one token. And rare sequences\nare going to be represented by multiple tokens.",
    "start": "4291050",
    "end": "4298670"
  },
  {
    "text": "There's a slight detail,\nwhich is for efficiency. The GPT-2 paper uses word based\ntokenizer as a pre-processing",
    "start": "4298670",
    "end": "4307010"
  },
  {
    "text": "to break it up into\nsegments, and then runs BP on each of the segments,\nwhich is what you're going to do in this class as well.",
    "start": "4307010",
    "end": "4313640"
  },
  {
    "text": "The algorithm BP is\nactually very simple. So we first convert the\nstring into a sequence",
    "start": "4313640",
    "end": "4319010"
  },
  {
    "text": "of bytes, which we\nalready did when we talked about byte-based tokenization. And now We're going\nto successively",
    "start": "4319010",
    "end": "4324140"
  },
  {
    "text": "merge the most common pair of\nadjacent tokens over and over again. So the intuition is\nthat of a pair of tokens",
    "start": "4324140",
    "end": "4331100"
  },
  {
    "text": "it shows up a lot. Then we're going to\ncompress it into one token. We're going to dedicate\nspace for that.",
    "start": "4331100",
    "end": "4337330"
  },
  {
    "text": "OK, so let's walk through what\nthis algorithm looks like. So we're going to use this\ncat in hat as an example.",
    "start": "4337330",
    "end": "4343330"
  },
  {
    "text": "And we're going to convert this\ninto a sequence of integers.",
    "start": "4343330",
    "end": "4350660"
  },
  {
    "text": "These are the bytes. And then we're going to keep\ntrack of what we've merged.",
    "start": "4350660",
    "end": "4357020"
  },
  {
    "text": "So remember merges is a\nmap from two integers which",
    "start": "4357020",
    "end": "4362320"
  },
  {
    "text": "can represent bytes or\nother pre-existing tokens. And we're going to\ncreate a new token.",
    "start": "4362320",
    "end": "4368740"
  },
  {
    "text": "And the vocab is just\ngoing to be a handy way to represent the\nindex to two bytes.",
    "start": "4368740",
    "end": "4375895"
  },
  {
    "text": " The BP algorithm. I mean it's very simple.",
    "start": "4375895",
    "end": "4381080"
  },
  {
    "text": "So I'm just actually going\nto run through the code. You're going to do this\nnum_mergers of times. So number is 3.",
    "start": "4381080",
    "end": "4386930"
  },
  {
    "text": "In this case, we're\ngoing to first count up the number of occurrences\nof pairs of bytes.",
    "start": "4386930",
    "end": "4393290"
  },
  {
    "text": "So hopefully this\ndoesn't become too small. So we're going to just\nstep through this sequence",
    "start": "4393290",
    "end": "4400690"
  },
  {
    "text": "and we're going to see that. OK so what's 116, 104\nwe're going to increment",
    "start": "4400690",
    "end": "4406119"
  },
  {
    "text": "that count 104 101. Increment that count. We're going to go\nthrough the sequence and we're going to\ncount up the bytes.",
    "start": "4406120",
    "end": "4414610"
  },
  {
    "text": "OK, so now after we\nhave these counts we're going to find\nthe pair that occurs",
    "start": "4414610",
    "end": "4424060"
  },
  {
    "text": "the most number of times. So I guess there's\nmultiple ones. But we're just going to break\nties and say 116 and 104.",
    "start": "4424060",
    "end": "4432430"
  },
  {
    "text": "So that occurs twice. So now we're going\nto merge that pair. So we're going to create a\nnew slot in our vocab which",
    "start": "4432430",
    "end": "4441880"
  },
  {
    "text": "is going to be 256. So far it's 031255.",
    "start": "4441880",
    "end": "4447170"
  },
  {
    "text": "And now we're expanding\nthe vocab to 256. And we're going to say every\ntime we see 116 and 104",
    "start": "4447170",
    "end": "4453760"
  },
  {
    "text": "we're going to\nreplace it with 256. OK, and then we're\ngoing to just apply",
    "start": "4453760",
    "end": "4464970"
  },
  {
    "text": "that merge to our training set. So after we do that\nthe 116 104 became 256.",
    "start": "4464970",
    "end": "4474460"
  },
  {
    "text": "And this 256 remember\noccurred twice. OK, so now we're just going to\nloop through this algorithm one",
    "start": "4474460",
    "end": "4482400"
  },
  {
    "text": "more time. The second time it decided\nto merge 256 and 101.",
    "start": "4482400",
    "end": "4489540"
  },
  {
    "text": "And now I'm going to\nreplace that in indices. And notice that the\nindices is going",
    "start": "4489540",
    "end": "4495840"
  },
  {
    "text": "to shrink because\nour compression ratio is getting better as we\nmake room for more vocabulary",
    "start": "4495840",
    "end": "4502470"
  },
  {
    "text": "items. And we have a greater vocabulary\nto represent everything. So let me do this one more time.",
    "start": "4502470",
    "end": "4510300"
  },
  {
    "text": "And then the next\nmerge is 257 3. And this is shrinking\none more time.",
    "start": "4510300",
    "end": "4517010"
  },
  {
    "text": "ok, and then now we're done. So let's try out this tokenizer.",
    "start": "4517010",
    "end": "4523590"
  },
  {
    "text": "So we have the string,\nthe quick brown fox we're going to encode into\na sequence of indices.",
    "start": "4523590",
    "end": "4533400"
  },
  {
    "text": "And then we're going to use\nour BP tokenizer to decode. Let's actually step through\nwhat that looks like.",
    "start": "4533400",
    "end": "4541195"
  },
  {
    "text": " This well, actually\nmaybe decoding",
    "start": "4541195",
    "end": "4546800"
  },
  {
    "text": "isn't actually interesting. Sorry I should have\ngone through the encode. Let's go back to encode.",
    "start": "4546800",
    "end": "4554210"
  },
  {
    "text": "So encode you take a string,\nyou convert it to indices. And you just replay the merges\nimportantly in the order",
    "start": "4554210",
    "end": "4562700"
  },
  {
    "text": "that it occurred. So I'm going to\nreplay these merges",
    "start": "4562700",
    "end": "4571570"
  },
  {
    "text": "and then I'm going\nto get my indices. OK, and then verify\nthat this works.",
    "start": "4571570",
    "end": "4578869"
  },
  {
    "text": "OK, so it's pretty simple. ",
    "start": "4578870",
    "end": "4584680"
  },
  {
    "text": "Because it's simple it\nwas also very inefficient. For example encode\nloops over the merges. You should only loops over\nthe merges that matter.",
    "start": "4584680",
    "end": "4592870"
  },
  {
    "text": "And there's some other\nbells and whistles like there's special\ntokens pre-tokenization.",
    "start": "4592870",
    "end": "4598280"
  },
  {
    "text": "And so in your\nassignment, you're going to essentially take\nthis as a starting point.",
    "start": "4598280",
    "end": "4604630"
  },
  {
    "text": "I mean, I guess you should\nimplement your own from scratch. But your goal is to make\nthe implementation fast.",
    "start": "4604630",
    "end": "4611680"
  },
  {
    "text": "And you can parallelize\nit if you want. You can go have fun. OK, so a summary\nof tokenization.",
    "start": "4611680",
    "end": "4619100"
  },
  {
    "text": "So tokenizer maps between\nstrings and sequences of integers.",
    "start": "4619100",
    "end": "4624670"
  },
  {
    "text": "We looked at character\nbase, byte base, word base. They're highly suboptimal\nfor various reasons.",
    "start": "4624670",
    "end": "4631969"
  },
  {
    "text": "BP is a very old\nalgorithm from '94 that still proves to be\nan effective heuristic.",
    "start": "4631970",
    "end": "4638343"
  },
  {
    "text": "And the important\nthing is that it looks at your corpus statistics\nto make sensible decisions about how to best adaptively\nallocate vocabulary to represent",
    "start": "4638343",
    "end": "4647920"
  },
  {
    "text": "sequences of characters. And I hope that one\nday I won't have",
    "start": "4647920",
    "end": "4654520"
  },
  {
    "text": "to give this lecture because\nwe'll just have architectures that map from bytes.",
    "start": "4654520",
    "end": "4659750"
  },
  {
    "text": "But until then, we'll have\nto deal with tokenization. OK, so that's it for today.",
    "start": "4659750",
    "end": "4665989"
  },
  {
    "text": "Next time, we're going to dive\ninto the details of PyTorch and give you the\nbuilding blocks,",
    "start": "4665990",
    "end": "4671360"
  },
  {
    "text": "and pay attention to\nresource accounting. All of you have presumably\nimplemented PyTorch programs,",
    "start": "4671360",
    "end": "4677750"
  },
  {
    "text": "but we're going to\nreally look at where all the flops are going. OK, see you next time.",
    "start": "4677750",
    "end": "4683640"
  },
  {
    "start": "4683640",
    "end": "4689000"
  }
]