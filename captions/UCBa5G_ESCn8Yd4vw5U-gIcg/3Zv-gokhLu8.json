[
  {
    "start": "0",
    "end": "6320"
  },
  {
    "text": "So today, we're\ngoing to be talking about generative\nadversarial networks.",
    "start": "6320",
    "end": "12179"
  },
  {
    "text": "So we're going to start\nintroducing yet another class of generative models.",
    "start": "12180",
    "end": "19250"
  },
  {
    "text": "Just as a recap, this is the\nhigh level story, high level",
    "start": "19250",
    "end": "25190"
  },
  {
    "text": "roadmap for the kind of things. So we're going to be talking\nabout in this course.",
    "start": "25190",
    "end": "30270"
  },
  {
    "text": "The high level idea when\nyou build a generative model is that you start\nwith some data,",
    "start": "30270",
    "end": "35390"
  },
  {
    "text": "and you assume that\nthe data is basically a set of iid samples from some\nunknown probability distribution",
    "start": "35390",
    "end": "43460"
  },
  {
    "text": "that we denote Pdata. Then you have a\nmodel family, which is a set of probability\ndistributions",
    "start": "43460",
    "end": "49129"
  },
  {
    "text": "that are parameterized\nusually by neural networks. And then what you\ndo is you define",
    "start": "49130",
    "end": "54950"
  },
  {
    "text": "some kind of notion\nof similarity between the data distribution\nand the model distribution.",
    "start": "54950",
    "end": "60320"
  },
  {
    "text": "And then you try\nto optimize over the set of probability\ndistribution in your model family. And you try to find one that is\nclose to the data distribution",
    "start": "60320",
    "end": "69750"
  },
  {
    "text": "according to some\nnotion of similarity. And we've seen different ways\nof basically constructing",
    "start": "69750",
    "end": "77549"
  },
  {
    "text": "probability distributions\nin this set. And we've seen\nautoregressive models, where you have chain rule.",
    "start": "77550",
    "end": "83729"
  },
  {
    "text": "And you break down basically\nthe generative modeling problem as a sequence of simple\nprediction problems.",
    "start": "83730",
    "end": "90360"
  },
  {
    "text": "We've seen variational\nautoencoders, where we are essentially,\nagain, modeling",
    "start": "90360",
    "end": "95700"
  },
  {
    "text": "the density over the data using\nessentially like a big mixture",
    "start": "95700",
    "end": "100799"
  },
  {
    "text": "model. And then the last class\nof models we've seen is this idea of a\nnormalizing flow model where,",
    "start": "100800",
    "end": "108030"
  },
  {
    "text": "which is kind of like a\nvariational autoencoder with a special type of\ndecoder, which is just a deterministic invertible\ntransformation where, again, we",
    "start": "108030",
    "end": "116340"
  },
  {
    "text": "get these densities through\nthe change of variable rule. But the key thing is that\nwe essentially always try",
    "start": "116340",
    "end": "124290"
  },
  {
    "text": "to model the probability\nassigned by the model to any particular data point.",
    "start": "124290",
    "end": "130489"
  },
  {
    "text": "And the reason we do that\nis that if we can do that, then we can do maximum\nlikelihood training. So if you know how to\nevaluate probabilities",
    "start": "130490",
    "end": "138230"
  },
  {
    "text": "according to the\nmodel then there is a very natural way of\ntraining the models, which",
    "start": "138230",
    "end": "143240"
  },
  {
    "text": "is basically this\nidea of minimizing the KL divergence between\nthe data distribution and the model distribution,\nwhich, as we know,",
    "start": "143240",
    "end": "150540"
  },
  {
    "text": "is equivalent to\nmaximizing likelihoods. And so there's a very\nnatural and very principled",
    "start": "150540",
    "end": "156920"
  },
  {
    "text": "way of comparing probability\ndistributions that works very well when you\nhave access to likelihoods.",
    "start": "156920",
    "end": "164209"
  },
  {
    "text": "And a lot of this\nmachinery kind of involves way of\nsetting up models such",
    "start": "164210",
    "end": "171740"
  },
  {
    "text": "that you can evaluate\nlikelihoods efficiently. And that's one way\nof doing things. What we're going to see today\nis basically a different way",
    "start": "171740",
    "end": "181430"
  },
  {
    "text": "of comparing similarity\nor of measuring similarity between\nprobability distributions. So we're going to change\nthis piece of the story.",
    "start": "181430",
    "end": "189600"
  },
  {
    "text": "And we're going\nto try to compare probability distributions\nin a different way. And by doing that, we will\nget a lot of flexibility",
    "start": "189600",
    "end": "196710"
  },
  {
    "text": "in terms of defining\nthe model family, because we will not\nhave to essentially--",
    "start": "196710",
    "end": "202782"
  },
  {
    "text": "the training\nobjective is not going to be based on maximum\nlikelihood anymore. And so we're going to get\nmore flexibility, essentially,",
    "start": "202782",
    "end": "209880"
  },
  {
    "text": "in terms of defining the\ngenerative model itself. So remember that, again, like\nwhat we've been doing so far",
    "start": "209880",
    "end": "219570"
  },
  {
    "text": "is training models by\nmaximum likelihood. So the idea is\nthat we have access to the density or\nthe probability mass",
    "start": "219570",
    "end": "226170"
  },
  {
    "text": "function over each data point. So we can ask the\nmodel how likely are you to generate\nthis particular data",
    "start": "226170",
    "end": "235560"
  },
  {
    "text": "point, xi in this case. And if we can do\nthat, then we can also",
    "start": "235560",
    "end": "241890"
  },
  {
    "text": "try to choose parameters such\nthat we maximize the probability that the model generated-- the\ntraining data set that we have",
    "start": "241890",
    "end": "249690"
  },
  {
    "text": "access to or equivalently\nwe can choose parameters to try to maximize the average\nlog probability assigned",
    "start": "249690",
    "end": "256769"
  },
  {
    "text": "by the model to\nour training set. And there is good reasons\nfor choosing this learning",
    "start": "256769",
    "end": "263520"
  },
  {
    "text": "objective. In particular, it can\nbe shown that this is optimal in a certain sense.",
    "start": "263520",
    "end": "269889"
  },
  {
    "text": "And what I mean\nis that basically, under some assumptions,\nwhich are not necessarily",
    "start": "269890",
    "end": "276120"
  },
  {
    "text": "true in practice-- but\nunder some ideal assumptions and an ideal setting where\nyou have a sufficiently",
    "start": "276120",
    "end": "283889"
  },
  {
    "text": "powerful model, and\nthere is some kind of identifiability condition--\nnot super important.",
    "start": "283890",
    "end": "290550"
  },
  {
    "text": "But under some\ntechnical conditions,",
    "start": "290550",
    "end": "296259"
  },
  {
    "text": "you can prove that\nbasically trying to estimate the\nparameters of the models by maximizing likelihood--\nbut basically solving",
    "start": "296260",
    "end": "303190"
  },
  {
    "text": "this particular\noptimization problem is the most efficient\nway of using the data. So basically, there is going\nto be other learning objectives",
    "start": "303190",
    "end": "310960"
  },
  {
    "text": "that you can set up\nthat would potentially give you estimates of the\ntrue parameters of the model.",
    "start": "310960",
    "end": "320560"
  },
  {
    "text": "But among all these\nvarious techniques, the maximum likelihood\none is the one that converges the fastest,\nwhich basically means",
    "start": "320560",
    "end": "327910"
  },
  {
    "text": "that given a certain\namount of data, this is the best\nthing you can do.",
    "start": "327910",
    "end": "333289"
  },
  {
    "text": "It's the one that will give you\nthe right answer, basically, using the least amount of data.",
    "start": "333290",
    "end": "340340"
  },
  {
    "text": "And so that's why using\nmaximum likelihood is a good idea,\nbecause in some sense",
    "start": "340340",
    "end": "346330"
  },
  {
    "text": "you're making the best\npossible use of the data you have access to under\nsome technical conditions.",
    "start": "346330",
    "end": "355750"
  },
  {
    "text": "And the other reason\nthat maximum likelihood is a good training\nobjective is that we've",
    "start": "355750",
    "end": "362350"
  },
  {
    "text": "seen that it corresponds\nto a compression problem. So if you can achieve high\nlikelihood on a data set,",
    "start": "362350",
    "end": "371000"
  },
  {
    "text": "then it means that you\nwould do reasonably well at compressing the data. And we know that compression\nis a reasonable kind",
    "start": "371000",
    "end": "377980"
  },
  {
    "text": "of learning objective. It's one of-- in\nsome sense, if you're able to compress\nthe data, then it",
    "start": "377980",
    "end": "384220"
  },
  {
    "text": "means that you can kind\npredict the things that could happen pretty well. And it's a good way of forcing\nyou to understand what--",
    "start": "384220",
    "end": "393730"
  },
  {
    "text": "the patterns in the data. And so compression is typically\na pretty good learning",
    "start": "393730",
    "end": "401510"
  },
  {
    "text": "objective. However, it might not be\nnecessarily what we want.",
    "start": "401510",
    "end": "407250"
  },
  {
    "text": "And so what we'll see\nfirst is that there are cases in which\nachieving high likelihood",
    "start": "407250",
    "end": "414950"
  },
  {
    "text": "might not necessarily be\ncorrelated with, let's say, achieving good sample quality.",
    "start": "414950",
    "end": "419970"
  },
  {
    "text": "So if you're thinking about\ntraining a generative model over images, for example, it's\npossible to construct models",
    "start": "419970",
    "end": "425780"
  },
  {
    "text": "that would give you high\nlikelihood and terrible samples in terms of quality.",
    "start": "425780",
    "end": "431210"
  },
  {
    "text": "And vice versa, it's going to\nbe possible to train models that have very good sample quality,\nmeaning they produce images that",
    "start": "431210",
    "end": "439550"
  },
  {
    "text": "are very realistic, but they\nhave terrible likelihoods at the same time. And so although training\nabout maximum likelihood",
    "start": "439550",
    "end": "450050"
  },
  {
    "text": "has good properties, it\nmight not be necessarily what we want if what you\ncare about is, let's say,",
    "start": "450050",
    "end": "456650"
  },
  {
    "text": "generating pretty\nsamples or pretty images. And so that's going to be\nsome motivation for choosing",
    "start": "456650",
    "end": "463980"
  },
  {
    "text": "different sort of\ntraining objectives that are not\nnecessarily going to be based on a maximum likelihood.",
    "start": "463980",
    "end": "470830"
  },
  {
    "text": "So the-- let's see what does\nthis mean a little bit more",
    "start": "470830",
    "end": "477990"
  },
  {
    "text": "rigorously. First, what we know is\nthat if somehow you're",
    "start": "477990",
    "end": "484650"
  },
  {
    "text": "able to find the true global\noptimum of this optimization",
    "start": "484650",
    "end": "490830"
  },
  {
    "text": "problem-- so if you're really able to\nfind a model distribution that perfectly matches the\ndata distribution--",
    "start": "490830",
    "end": "498550"
  },
  {
    "text": "so somehow, if you go\nback to this picture, if you are able to make\nthis distance exactly 0,",
    "start": "498550",
    "end": "505980"
  },
  {
    "text": "the kl divergence between the\ndata and the model is truly 0, so you're able to reach\nthe global optimum",
    "start": "505980",
    "end": "511650"
  },
  {
    "text": "of this optimization problem,\nthen you are in good shape because, well, you get the\nbest possible likelihood",
    "start": "511650",
    "end": "519029"
  },
  {
    "text": "and the samples that you\nproduce are perfect essentially by definition. Because your model is exactly\nequal to your data distribution,",
    "start": "519030",
    "end": "527320"
  },
  {
    "text": "and so if you sample\nfrom the model, it's like sampling\nfrom the data, and so that's as good as it gets.",
    "start": "527320",
    "end": "535040"
  },
  {
    "text": "But what we're going to see-- is\nthat as long as the match is not perfect-- as long as there is a little bit\nof a gap which, in practice, is",
    "start": "535040",
    "end": "542290"
  },
  {
    "text": "always going to\nbe the case, then you being close in KL\ndivergence or equivalently",
    "start": "542290",
    "end": "551312"
  },
  {
    "text": "doing well with\nrespect to likelihood, it doesn't necessarily mean that\nyou are achieving good sample",
    "start": "551312",
    "end": "556930"
  },
  {
    "text": "quality. But somehow, if you're\nreally able to get the true global optimum,\nthen you are good.",
    "start": "556930",
    "end": "565180"
  },
  {
    "text": "But for imperfect models,\nachieving high likelihoods does not necessarily mean that\nyou achieve good sample quality",
    "start": "565180",
    "end": "573250"
  },
  {
    "text": "and vice versa. [? It sounds ?] like these\n[INAUDIBLE] around the optimum,",
    "start": "573250",
    "end": "578980"
  },
  {
    "text": "where suddenly you get\ngetting the best results, and up until that,\nyou're getting",
    "start": "578980",
    "end": "584199"
  },
  {
    "text": "lackluster but decent results? So we'll see that this,\nunfortunately, not really.",
    "start": "584200",
    "end": "590110"
  },
  {
    "text": "And here is an example where you\ncan get very good likelihoods,",
    "start": "590110",
    "end": "596560"
  },
  {
    "text": "but very bad samples. And so to do that, you can\nbasically imagine a situation",
    "start": "596560",
    "end": "602080"
  },
  {
    "text": "like this, where you come\nup with this model, which is",
    "start": "602080",
    "end": "608290"
  },
  {
    "text": "a mixture of two distribution. It's a mixture of the true data\ndistribution and some garbage--",
    "start": "608290",
    "end": "615460"
  },
  {
    "text": "pure noise distribution. And so the sampling process\nis something like this here. You flip a coin or-- and\nthen with 99% probability,",
    "start": "615460",
    "end": "624790"
  },
  {
    "text": "you generate noise\nyou generate garbage. And with 1% probability,\nyou generate a true sample",
    "start": "624790",
    "end": "630579"
  },
  {
    "text": "from the data distribution. Of course, in practice,\nyou cannot really do this. But this is just to show\nthat there exists models",
    "start": "630580",
    "end": "638260"
  },
  {
    "text": "that achieve very\ngood likelihoods as we'll see but very\ngood sample quality.",
    "start": "638260",
    "end": "644180"
  },
  {
    "text": "And what I mean-- the sample quality is bad\nbecause 99% of the time, you are generating pure\ngarbage and only 1%",
    "start": "644180",
    "end": "653139"
  },
  {
    "text": "of the time you're\ngenerating good samples. And what we'll see is that even\nthough this model is generating",
    "start": "653140",
    "end": "661840"
  },
  {
    "text": "very bad samples, it actually\nachieves very good likelihoods. And to see that, it's actually\na relatively simple kind",
    "start": "661840",
    "end": "668860"
  },
  {
    "text": "of derivation. When you evaluate the\nprobability of a data point x, according to this model,\nyou get a sum of two terms.",
    "start": "668860",
    "end": "676240"
  },
  {
    "text": "It's the true probability\nunder the data distribution, and it's the probability\nunder the noise distribution.",
    "start": "676240",
    "end": "682660"
  },
  {
    "text": "And even though the\nnoise distribution could be really bad, the\nprobability is at least as good",
    "start": "682660",
    "end": "688600"
  },
  {
    "text": "as the-- there's a 1% probability\nof sampling from the data. And so the probability\nassigned to this data point",
    "start": "688600",
    "end": "695750"
  },
  {
    "text": "is at least as large as this sum\nof two non-negative quantities, and so this log is\nat least as large",
    "start": "695750",
    "end": "702079"
  },
  {
    "text": "as the log of that little\ncontribution that comes from the data distribution.",
    "start": "702080",
    "end": "707900"
  },
  {
    "text": "And because we're taking logs,\nthe log of 1% times P data",
    "start": "707900",
    "end": "713390"
  },
  {
    "text": "is equal to the log of P\ndata minus this log of 100.",
    "start": "713390",
    "end": "718880"
  },
  {
    "text": "So somehow, basically\nwhat we're seeing here is that the log probability\nassigned by this model",
    "start": "718880",
    "end": "726380"
  },
  {
    "text": "to a data point is the\nbest log probability you can get-- the one that you\nget according to the true data",
    "start": "726380",
    "end": "733700"
  },
  {
    "text": "distribution shifted\ndown by some constant. ",
    "start": "733700",
    "end": "738840"
  },
  {
    "text": "And in particular,\nwhat this means is that if you take an\nexpectation of this with respect",
    "start": "738840",
    "end": "744876"
  },
  {
    "text": "to the data\ndistribution-- so you want to see what is the average\nlog likelihood that this model achieves, if you\ntake an expectation",
    "start": "744877",
    "end": "751470"
  },
  {
    "text": "of the left-hand side,\nyou take an expectation of the right-hand side, you get\nthat on average, these models",
    "start": "751470",
    "end": "758370"
  },
  {
    "text": "performs reasonably well.  In the sense that it\nperforms as well as what",
    "start": "758370",
    "end": "765320"
  },
  {
    "text": "you would get if you\nwere to use the true data distribution as a model\nshifted by some constant.",
    "start": "765320",
    "end": "774550"
  },
  {
    "text": "And we know because KL\ndivergence is non-negative that, somehow, this is\nthe best you can do.",
    "start": "774550",
    "end": "782380"
  },
  {
    "text": "The average log likelihood for\nany model cannot be possibly better than the log likelihood\nthat you would get if you were",
    "start": "782380",
    "end": "789520"
  },
  {
    "text": "to use the true data\ndistribution to evaluate likelihoods of samples produced\nby the data distribution.",
    "start": "789520",
    "end": "795980"
  },
  {
    "text": "This is just-- the KL divergence\nis non-negative, which just-- if you just move the\nlog on the other side,",
    "start": "795980",
    "end": "803110"
  },
  {
    "text": "it's just saying that\nthe data distribution-- if the data is coming from\nthe data distribution, the best model of the world you\ncan possibly have is the one",
    "start": "803110",
    "end": "811000"
  },
  {
    "text": "that produced the data-- is the data distribution. And no matter how clever\nyou are in choosing data, you cannot possibly do better\nthan using the true model that",
    "start": "811000",
    "end": "819730"
  },
  {
    "text": "produced the data. And so you can see that kind\nof this performance that we get",
    "start": "819730",
    "end": "831350"
  },
  {
    "text": "is kind of bounded above\nby this basically entropy of the data distribution,\nand below by the same thing",
    "start": "831350",
    "end": "838700"
  },
  {
    "text": "shifted by a little bit. And what I argue\nis that constant doesn't matter too much.",
    "start": "838700",
    "end": "846010"
  },
  {
    "text": "Because if you think about\nit, as we increase the number of dimensions-- so as we go in\nhigher and higher dimensions,",
    "start": "846010",
    "end": "854290"
  },
  {
    "text": "the likelihood-- so this piece will\nbasically scale linearly",
    "start": "854290",
    "end": "859720"
  },
  {
    "text": "in the number of dimensions\nwhile the constant is fixed. It doesn't depend on how many\nvariables you're modeling.",
    "start": "859720",
    "end": "867790"
  },
  {
    "text": "So-- and the intuition\nis that if, for example, you look at factorize--",
    "start": "867790",
    "end": "873970"
  },
  {
    "text": "this is a mistake. This is meant to be Pdata. Sorry. There's a typo here. But basically, if you use-- if you factorize the\ntrue data distribution",
    "start": "873970",
    "end": "882400"
  },
  {
    "text": "according to the\nchain rule, you can see that this term here--\nthe log data scales linearly",
    "start": "882400",
    "end": "889060"
  },
  {
    "text": "in the number of\nvariables that you have, while the second\npiece does not depend",
    "start": "889060",
    "end": "894339"
  },
  {
    "text": "on the number of variables. And so you can kind of see\nthat in high dimensions,",
    "start": "894340",
    "end": "900069"
  },
  {
    "text": "this model is basically doing\nas well as you can hope to do. The likelihood of this model,\nwhich is producing garbage",
    "start": "900070",
    "end": "907600"
  },
  {
    "text": "in 99% of the time, is\npretty close to the best you can possibly achieve.",
    "start": "907600",
    "end": "914500"
  },
  {
    "text": "And so I think back\nto your question it means that there\nis a model that is very close to\nthe optimal one,",
    "start": "914500",
    "end": "921769"
  },
  {
    "text": "and it's still producing very,\nvery bad results, especially in high dimensions. ",
    "start": "921770",
    "end": "928690"
  },
  {
    "text": "Yeah? [INAUDIBLE] but like doesn't\nthis also [INAUDIBLE] models not",
    "start": "928690",
    "end": "933760"
  },
  {
    "text": "only on like good images,\nbut assigning bad qualities to bad images or [INAUDIBLE]\nimages [INAUDIBLE]",
    "start": "933760",
    "end": "940390"
  },
  {
    "text": "very easily generate bad images? Yeah, that's a good question. Yeah, to what extent could\nyou use, let's say, bad data",
    "start": "940390",
    "end": "947200"
  },
  {
    "text": "and somehow train\nthe models that way. It's not obvious\nhow you would do it with the maximum likelihood.",
    "start": "947200",
    "end": "953410"
  },
  {
    "text": "But using GANs, which is what\nwe're going to talk about today, it's actually pretty\nstraightforward to incorporate negative data.",
    "start": "953410",
    "end": "961250"
  },
  {
    "text": "So if you know that there are\ncertain things that are clearly not possible or\nthings you don't like,",
    "start": "961250",
    "end": "966910"
  },
  {
    "text": "then it's pretty\nstraightforward to incorporate that sort of negative data\naugmentation into your training",
    "start": "966910",
    "end": "973390"
  },
  {
    "text": "objective. For example, if you take-- your training on\nimages, and you apply",
    "start": "973390",
    "end": "978490"
  },
  {
    "text": "some kind of Jigsaw\noperator, where you kind of like\nproduce a puzzle and then you move\nthe pieces around,",
    "start": "978490",
    "end": "983920"
  },
  {
    "text": "you get an image that has\nthe right local texture, but it's not something you want.",
    "start": "983920",
    "end": "989690"
  },
  {
    "text": "And you can incorporate that\nkind of data augmentation, essentially-- or negative\ndata augmentation",
    "start": "989690",
    "end": "994850"
  },
  {
    "text": "to improve the training. So that applies generally. It's a little bit trickier to\ndo with likelihood based models,",
    "start": "994850",
    "end": "1004149"
  },
  {
    "text": "but that's a good idea. Could you use some\ntype of ensemble to model the noise separately?",
    "start": "1004150",
    "end": "1010420"
  },
  {
    "text": "Like I can do it for like\nstandard supervised learning, but not sure if\nyou can also do it in a generation setting,\nwhere you just model noise",
    "start": "1010420",
    "end": "1018430"
  },
  {
    "text": "separately, take it\nout, and then have, I guess, true data\nthat you're modeling? So you're saying it kind of have\na mixture that is trying to--",
    "start": "1018430",
    "end": "1027520"
  },
  {
    "text": "where would the noise come-- to try to filter it out or? Yeah, [INAUDIBLE] if you\ntrain several ensembles, one",
    "start": "1027520",
    "end": "1035227"
  },
  {
    "text": "of the part that they\npick up is [INAUDIBLE] the true distribution,\nand then the noise part is picked up separately.",
    "start": "1035227",
    "end": "1040814"
  },
  {
    "text": "But if you average\na lot of them, then you only reinforce\nthe common signals, and then the noise components\ncancel each other out.",
    "start": "1040815",
    "end": "1047685"
  },
  {
    "text": "I'm just curious if you\ncould do something similar. I think in general, we don't'-- we're not in the setting where\nwe are assuming that there",
    "start": "1047685",
    "end": "1054370"
  },
  {
    "text": "is even noise in\nthe training data, so-- or I think what\nwe were talking about",
    "start": "1054370",
    "end": "1059650"
  },
  {
    "text": "is a setting where\nwhat you don't want, and you kind of take\nadvantage of that. But you don't have to figure out\nwhat is noise and what is not,",
    "start": "1059650",
    "end": "1066610"
  },
  {
    "text": "because you already know. And here, we are in\nthe setting where we're assuming the data is clean.",
    "start": "1066610",
    "end": "1072159"
  },
  {
    "text": "The data is really just a\nbunch of samples from the data distribution, so you do\nwant to use everything",
    "start": "1072160",
    "end": "1077180"
  },
  {
    "text": "you have access to, and there\nis no need to filter the noise. This is just a model of the\nworld that is kind of made up,",
    "start": "1077180",
    "end": "1084860"
  },
  {
    "text": "but it's illustrating the point\nthat optimizing likelihoods might not give you\ngood sample quality,",
    "start": "1084860",
    "end": "1091330"
  },
  {
    "text": "because at least\nconceptually, it's possible that by optimizing\nlikelihood, you end up with a model like this,\nwhich would produce garbage",
    "start": "1091330",
    "end": "1098960"
  },
  {
    "text": "99% of the time but gives\nyou high likelihoods. And so there is that\npotential issue.",
    "start": "1098960",
    "end": "1106855"
  },
  {
    "text": " And conversely,\nit's possible to get",
    "start": "1106855",
    "end": "1112440"
  },
  {
    "text": "models that produce great\nsamples and very bad log",
    "start": "1112440",
    "end": "1118080"
  },
  {
    "text": "likelihoods. Anybody have a guess on\nhow you could do that? ",
    "start": "1118080",
    "end": "1126820"
  },
  {
    "text": "How would you-- yeah? Overfitting. Overfitting-- yeah,\nthat's probably the simplest way to do it.",
    "start": "1126820",
    "end": "1132010"
  },
  {
    "text": "Just memorize the training set. So you build a model that\nputs all the probability mass",
    "start": "1132010",
    "end": "1137170"
  },
  {
    "text": "uniform, let's say, distribution\nover the training set. And then if you sample\nfrom this model,",
    "start": "1137170",
    "end": "1142779"
  },
  {
    "text": "the samples would look great. I mean, they are by definition\njust training samples, so basically you cannot\ndo better than that.",
    "start": "1142780",
    "end": "1150710"
  },
  {
    "text": "But the test likelihood\nwould be as bad as it gets, because it's going\nto assign basically 0 probability to anything\nthat the model hasn't",
    "start": "1150710",
    "end": "1157450"
  },
  {
    "text": "seen during training, and\nso again, a terrible log",
    "start": "1157450",
    "end": "1162610"
  },
  {
    "text": "likelihood. So again, this is\nsort of suggesting",
    "start": "1162610",
    "end": "1168830"
  },
  {
    "text": "that it might be useful to\ndisentangle a little bit sample quality and likelihood.",
    "start": "1168830",
    "end": "1177380"
  },
  {
    "text": "Even though we had some\nsuccess training models by maximum likelihood,\nit's not guaranteed",
    "start": "1177380",
    "end": "1183140"
  },
  {
    "text": "that that's always the case. And there might be other\ntraining objectives that will give us good\nresults in practice.",
    "start": "1183140",
    "end": "1189755"
  },
  {
    "text": " And so that's the main\nmotivation behind--",
    "start": "1189755",
    "end": "1198320"
  },
  {
    "text": "the key idea behind generative\nadversarial networks. It's a different kind\nof training objective",
    "start": "1198320",
    "end": "1203960"
  },
  {
    "text": "that will not depend on\nthe likelihood function. And so back to our\nhigh level picture,",
    "start": "1203960",
    "end": "1210565"
  },
  {
    "text": "basically, what\nwe're going to do is we're going to change this\nperformance measure here.",
    "start": "1210565",
    "end": "1216091"
  },
  {
    "text": "We're going to\nchange the way we're comparing how good our\nmodel is by throwing away KL divergence, which is what\nwe've been doing so far.",
    "start": "1216092",
    "end": "1224700"
  },
  {
    "text": "And we're going to try some\nalternative way of comparing two probability\ndistributions that",
    "start": "1224700",
    "end": "1230330"
  },
  {
    "text": "will not rely on likelihood. Yeah? So in the third\ncase, how is examples",
    "start": "1230330",
    "end": "1238720"
  },
  {
    "text": "looking exactly\nlike the training set [? great ?] samples? Are we-- how do you\ndefine great samples?",
    "start": "1238720",
    "end": "1244255"
  },
  {
    "text": "Yeah, it's a good question. What is a great sample? Maybe you want the samples to\nhave diversity, in which case, maybe this wouldn't be.",
    "start": "1244255",
    "end": "1250310"
  },
  {
    "text": "But if you think about images,\nif you were to look at them, they would look perfect. They would have the\nright structure,",
    "start": "1250310",
    "end": "1256060"
  },
  {
    "text": "they would be good except\nthat there is not maybe",
    "start": "1256060",
    "end": "1261070"
  },
  {
    "text": "enough variety because\nyou're not really generating anything new. But presumably, it would be--",
    "start": "1261070",
    "end": "1268330"
  },
  {
    "text": "in terms of just quality\nof the individual samples, this should be good.",
    "start": "1268330",
    "end": "1274090"
  },
  {
    "text": "[INAUDIBLE] Yeah, for each\nindividual sample, the quality would be good.",
    "start": "1274090",
    "end": "1279190"
  },
  {
    "text": " Great. So let's see how we can do it.",
    "start": "1279190",
    "end": "1285140"
  },
  {
    "text": "And so the basic\nidea is that we're going to think about exactly\nthat problem of deciding",
    "start": "1285140",
    "end": "1292660"
  },
  {
    "text": "whether or not two distributions\nare similar by looking at the kind of samples you\nget if you went through sample",
    "start": "1292660",
    "end": "1300520"
  },
  {
    "text": "from one or the other. And so you can\nimagine this setting, where I give you a bunch\nof samples that are sampled",
    "start": "1300520",
    "end": "1307779"
  },
  {
    "text": "from some distribution P. And I\ngive you a bunch of samples that are coming from\nsome distribution Q",
    "start": "1307780",
    "end": "1314590"
  },
  {
    "text": "And what we can try to do is we\ncan try to figure out if there",
    "start": "1314590",
    "end": "1321190"
  },
  {
    "text": "is a way to tell\nwhether or not P, this distribution that we use\nto generate the first batch--",
    "start": "1321190",
    "end": "1328630"
  },
  {
    "text": "this group of\nsamples is actually the same as the\ndistribution that we used to generate the\nright group of samples.",
    "start": "1328630",
    "end": "1335904"
  },
  {
    "text": " If you can do that, then that\ncould be a good way of comparing",
    "start": "1335905",
    "end": "1344362"
  },
  {
    "text": "probability distributions,\nbecause if we don't have a way of\ntelling whether or not the distributions are\ndifferent, then it probably",
    "start": "1344363",
    "end": "1351770"
  },
  {
    "text": "means that the distributions\nare close to each other. And so that's\nbasically what we're",
    "start": "1351770",
    "end": "1357800"
  },
  {
    "text": "going to do-- is we're\ngoing to use something called a two-sample test,\nwhich is basically a procedure",
    "start": "1357800",
    "end": "1363529"
  },
  {
    "text": "for determining whether or not\nthese two groups of samples,",
    "start": "1363530",
    "end": "1369120"
  },
  {
    "text": "S1 and S2, are coming from\nthe same distribution-- are generated by the same\nprobability distribution or not.",
    "start": "1369120",
    "end": "1376929"
  },
  {
    "text": "And so it's an hypothesis\ntesting problem, which you might have seen\nbefore in other contexts, where",
    "start": "1376930",
    "end": "1383160"
  },
  {
    "text": "basically there is a\nnull hypothesis, which is the distributions are the same. So P is equal to Q, so the\nsamples in the first group",
    "start": "1383160",
    "end": "1392700"
  },
  {
    "text": "have the same distribution as\nthe samples in the second group. And then there is an\nalternative hypothesis,",
    "start": "1392700",
    "end": "1398049"
  },
  {
    "text": "which is that the\ndistributions are different. Yeah? Can you explain\none more time why",
    "start": "1398050",
    "end": "1404270"
  },
  {
    "text": "do you want to compare\nthe distributions? Yeah, we want to compare the\ndistributions because, I guess,",
    "start": "1404270",
    "end": "1410190"
  },
  {
    "text": "we need a training objective. So we typically\nuse KL divergence to figure out how close\nour model is to the data.",
    "start": "1410190",
    "end": "1417660"
  },
  {
    "text": "And we kind of said,\nOK, KL divergence is good, but perhaps\nnot ideal in the sense",
    "start": "1417660",
    "end": "1423333"
  },
  {
    "text": "that you can get pretty\nclose in KL divergence and have terrible samples. So maybe there is room for\nchoosing different comparisons--",
    "start": "1423333",
    "end": "1433353"
  },
  {
    "text": "different ways of comparing\nprobability distributions that are closer to perceptual\nquality of the samples",
    "start": "1433353",
    "end": "1439520"
  },
  {
    "text": "and will allow us to\ndo things differently. And that's kind of\nthe main motivation. So one way to go\nabout it is to say,",
    "start": "1439520",
    "end": "1446279"
  },
  {
    "text": "let's say I have a bunch\nof samples from Pdata, say I have a bunch of\nsamples from P theta-- how do I compare them?",
    "start": "1446280",
    "end": "1452000"
  },
  {
    "text": "And the most basic\nthing to do is to say, can I decide whether or\nnot they are different, because if I cannot figure\nout if they are different,",
    "start": "1452000",
    "end": "1459510"
  },
  {
    "text": "then it means that I'm close. They are the same. So if I fail at this\nhypothesis testing problem,",
    "start": "1459510",
    "end": "1468170"
  },
  {
    "text": "or it's very hard to\ndo well in this task, then it means the\ndistribution are similar.",
    "start": "1468170",
    "end": "1474980"
  },
  {
    "text": "And so I have a\npretty good way of-- and then I have a\nreasonable way of comparing. Then maybe I'm doing well\non my learning problem,",
    "start": "1474980",
    "end": "1481980"
  },
  {
    "text": "so that's kind of\nlike the intuition. But doesn't then also the issue\nof the randomness of the P",
    "start": "1481980",
    "end": "1488270"
  },
  {
    "text": "value come into play? Because like for example, if\nyou set the P value at 5%, then just like 5% of the time,\nso even if they are the same,",
    "start": "1488270",
    "end": "1495630"
  },
  {
    "text": "you would reject. Right. So the way-- and this is\ngetting at how do you do it. Typically, what you would\ndo is you would come up",
    "start": "1495630",
    "end": "1502559"
  },
  {
    "text": "with a test statistic, which\nis just a function that you use to compare these two sets.",
    "start": "1502560",
    "end": "1510040"
  },
  {
    "text": "For example, you\nmight try to look at what is the mean of the\nsamples in the first group",
    "start": "1510040",
    "end": "1516390"
  },
  {
    "text": "versus the mean of the\nsamples in the second group. Or you could look\nat the variance--",
    "start": "1516390",
    "end": "1521970"
  },
  {
    "text": "the sample variance of the\nsamples in S1 versus the sample variance in S2.",
    "start": "1521970",
    "end": "1528030"
  },
  {
    "text": "And presumably, if\nindeed P is equal to Q, then you would expect the\nmean of the samples in here",
    "start": "1528030",
    "end": "1535140"
  },
  {
    "text": "to be similar to the means\nof the samples in here. And you would expect the\nvariances to be similar.",
    "start": "1535140",
    "end": "1541030"
  },
  {
    "text": "So for example, one\nstatistic you could try is to do something\nlike this, where",
    "start": "1541030",
    "end": "1546720"
  },
  {
    "text": "you compute the mean\nin the first group, you compute the mean\nin the second group,",
    "start": "1546720",
    "end": "1551850"
  },
  {
    "text": "and then you compare them. And what you could\ndo is you could say,",
    "start": "1551850",
    "end": "1557140"
  },
  {
    "text": "if this statistic is\nlarger than some threshold, then I reject the\nnull hypothesis.",
    "start": "1557140",
    "end": "1564880"
  },
  {
    "text": "Otherwise, I say that H0,\nso the null hypothesis",
    "start": "1564880",
    "end": "1570000"
  },
  {
    "text": "is consistent with\nmy observation. And as you were\nsaying, there is always some kind of type I and\ntype II error, in the sense",
    "start": "1570000",
    "end": "1576480"
  },
  {
    "text": "that T is random, because\nS1 and S2 are random. And so even when\nP is equal to Q,",
    "start": "1576480",
    "end": "1583560"
  },
  {
    "text": "it's possible that the\nmeans are different just because of randomness.",
    "start": "1583560",
    "end": "1588970"
  },
  {
    "text": "It's not going to be\nsuper important to what we're going to do.",
    "start": "1588970",
    "end": "1594130"
  },
  {
    "text": "But yeah, there is no--\nit's a hard problem. It's-- even if you\nhad a good statistic,",
    "start": "1594130",
    "end": "1600510"
  },
  {
    "text": "there is still going to be\nsome probability of error. But you could ask\nthe question of, OK, what's the best statistic for\ndetermining whether or not",
    "start": "1600510",
    "end": "1609690"
  },
  {
    "text": "the tool to solve this\nhypothesis testing problem. And you could try to minimize\ntype I and type II errors.",
    "start": "1609690",
    "end": "1615600"
  },
  {
    "text": "There's going to\nbe false positives, there's going to\nbe false negatives, but you can try to choose\na statistic that minimizes",
    "start": "1615600",
    "end": "1622169"
  },
  {
    "text": "these types of errors. Does it have to be like a\ndifferentiable [? function? ?] Can I have human\npreference as my assistant?",
    "start": "1622170",
    "end": "1629310"
  },
  {
    "text": "We'll see how to\ncome up with this T. This is just a\nkind of handcrafted and it's not the best one, but\nthis is going to be similar to--",
    "start": "1629310",
    "end": "1636260"
  },
  {
    "text": "well, it's going to\nbe learned, basically. Yeah. And so here, are you\ntrying to distinguish",
    "start": "1636260",
    "end": "1641640"
  },
  {
    "text": "between two different\ndistributions where initial samples coming\nfrom, [? or ?] also two different types of\ngenerative models that",
    "start": "1641640",
    "end": "1649140"
  },
  {
    "text": "are making the final output? It could be both. In practice, one is going to be\nreal data, one is going to be--",
    "start": "1649140",
    "end": "1656530"
  },
  {
    "text": "let's say S1 is going to\nbe samples from the data distribution and S2 is going\nto be sampled from our model,",
    "start": "1656530",
    "end": "1661799"
  },
  {
    "text": "but it doesn't have to. It could be two\nmodels, it could be-- yeah, this is pretty generic.",
    "start": "1661800",
    "end": "1668070"
  },
  {
    "text": "But the way we're going\nto actually use it for a generative\nadversarial network, it's going to be one group of\nsamples are going to be real,",
    "start": "1668070",
    "end": "1674159"
  },
  {
    "text": "one group of samples are\ngoing to come from that model. So they're going to be fake. And then we're going to\nuse some kind of statistic",
    "start": "1674160",
    "end": "1680250"
  },
  {
    "text": "to try to determine\nwhether or not these two samples are similar. ",
    "start": "1680250",
    "end": "1688990"
  },
  {
    "text": "And yeah, the key observation\nhere is that at least--",
    "start": "1688990",
    "end": "1695470"
  },
  {
    "text": "that there is some room for\nchoosing different kind of test statistics. And you can choose some\nstatistics which do not",
    "start": "1695470",
    "end": "1704080"
  },
  {
    "text": "depend on the likelihood\nof P or Q. For example, if you just look\nat the means, you don't need to know\nthe probability--",
    "start": "1704080",
    "end": "1712260"
  },
  {
    "text": "you don't need to be able to\nevaluate probabilities under P, you don't need to\nevaluate probabilities under Q. You can compute\nthis function just",
    "start": "1712260",
    "end": "1718480"
  },
  {
    "text": "based on a bunch of samples. You mentioned you just\noverfit and replicate Pdata.",
    "start": "1718480",
    "end": "1725260"
  },
  {
    "text": "How would these two sigma\ntests deal with that? It's going to be the\nsame [? samples. ?] Yeah, so the\nquestion is, does it",
    "start": "1725260",
    "end": "1732190"
  },
  {
    "text": "solve the overfitting problem? You still have the\noverfitting problem just even if you do\nmaximum likelihood.",
    "start": "1732190",
    "end": "1738730"
  },
  {
    "text": "You can still have\noverfitting problem, so this does not directly\naddress the overfitting directly.",
    "start": "1738730",
    "end": "1745779"
  },
  {
    "text": "Although, you can sometimes--\nat least you can use like validation and other things\nto see what would be the--",
    "start": "1745780",
    "end": "1753760"
  },
  {
    "text": "yeah, so we'll see. Yeah? So the [? test ?] statistic,\nwhich is the-- if you choose it",
    "start": "1753760",
    "end": "1762010"
  },
  {
    "text": "to be your cross-entropy\nloss, then it's the same thing as maximizing your\n[? K ?] levels?",
    "start": "1762010",
    "end": "1768370"
  },
  {
    "text": "You could-- yeah,\nso you could try to choose test\nstatistics that are based on the likelihood of the\nmodel, but you don't have to.",
    "start": "1768370",
    "end": "1775810"
  },
  {
    "text": " And so again, sort\nof like the setting",
    "start": "1775810",
    "end": "1783710"
  },
  {
    "text": "of generative modeling\nwith two sample tests, is one where we\nhave there's going to be a bunch of samples that\nare just going to be coming",
    "start": "1783710",
    "end": "1791600"
  },
  {
    "text": "from the data distribution. Recall that that's\nall we-- we always assume that somebody is giving\nus access to a bunch of samples",
    "start": "1791600",
    "end": "1798770"
  },
  {
    "text": "that are coming from\nthe data distribution, and that's our training set. And so that's going to be\nthe first group of samples.",
    "start": "1798770",
    "end": "1806220"
  },
  {
    "text": "Then, just like before,\nwe have a set of models. We have a set of\ndistributions P theta that we",
    "start": "1806220",
    "end": "1812420"
  },
  {
    "text": "are willing to consider. And as long as\nthese distributions are easy to sample from, which\nis not a terrible requirement,",
    "start": "1812420",
    "end": "1820304"
  },
  {
    "text": "because presumably,\nyou're going to use the sample the model to\ngenerate samples anyways-- but as long as you can\nsomehow sample from the model,",
    "start": "1820305",
    "end": "1827570"
  },
  {
    "text": "you can always generate\nthis second set of samples S2, which\nare just basically",
    "start": "1827570",
    "end": "1832700"
  },
  {
    "text": "samples from the model. And then what you can do\nis you can try to train--",
    "start": "1832700",
    "end": "1840460"
  },
  {
    "text": "basically, the\nhigh level idea is going to be instead of trying\nto find a model P theta, which",
    "start": "1840460",
    "end": "1849470"
  },
  {
    "text": "is going to optimize\nover the set so that we minimize the KL\ndivergence between these two distributions, we\nare going to try",
    "start": "1849470",
    "end": "1856670"
  },
  {
    "text": "to find a model that tries to\nminimize this whatever test",
    "start": "1856670",
    "end": "1863120"
  },
  {
    "text": "statistic that\nwe've decided to use to compare two set of samples.",
    "start": "1863120",
    "end": "1870240"
  },
  {
    "text": "And for example, in\nthe previous example, it could be\nsomething try to make",
    "start": "1870240",
    "end": "1876320"
  },
  {
    "text": "sure that the means of\nthe samples that I produce matches the mean of the samples\nthat I had in the training set,",
    "start": "1876320",
    "end": "1883638"
  },
  {
    "text": "which would not be very useful. But that's the high level idea. ",
    "start": "1883638",
    "end": "1889990"
  },
  {
    "text": "And so the problem is that\nfinding a good statistic",
    "start": "1889990",
    "end": "1895440"
  },
  {
    "text": "is not easy. For example, if you\nwere to just try",
    "start": "1895440",
    "end": "1901320"
  },
  {
    "text": "to minimize this kind of\ntest that statistic here, you would end up with\na generative model",
    "start": "1901320",
    "end": "1907260"
  },
  {
    "text": "that produces samples with the\nsame mean as the training set. Which is a-- the property\nthat is desirable,",
    "start": "1907260",
    "end": "1919010"
  },
  {
    "text": "but it's not sufficient. If you just match the\nmean, you can still produce very bad samples and\nfool this test statistic.",
    "start": "1919010",
    "end": "1930160"
  },
  {
    "text": "And that's the problem-- is that in high\ndimensions, it's very hard",
    "start": "1930160",
    "end": "1935169"
  },
  {
    "text": "to find some good\ntest statistic. And so kind of\nintuitively, you could say,",
    "start": "1935170",
    "end": "1941710"
  },
  {
    "text": "let's say that you start\ncomparing probability distributions by\nmatching the mean. And so if you compare\nthe means, you",
    "start": "1941710",
    "end": "1948112"
  },
  {
    "text": "would be able to\ndistinguish-- let's say that this green\nGaussian is different from this red Gaussian.",
    "start": "1948113",
    "end": "1954850"
  },
  {
    "text": "But then you could\nsay, just matching the means is not sufficient. Here are two Gaussians\nthat have the same mean,",
    "start": "1954850",
    "end": "1962500"
  },
  {
    "text": "but different variances. So if you just\ncompare the means, you would not be able to\ndistinguish between those two",
    "start": "1962500",
    "end": "1969070"
  },
  {
    "text": "Gaussians. And then maybe you-- and if you match, let's say,\nthe mean and the variance,",
    "start": "1969070",
    "end": "1975549"
  },
  {
    "text": "you could have\ndifferent distributions that have the same first moments\nor same mean and same variance,",
    "start": "1975550",
    "end": "1981260"
  },
  {
    "text": "like a Gaussian and\nLaplace density. Same means and same variances,\nbut different shapes.",
    "start": "1981260",
    "end": "1988110"
  },
  {
    "text": "So kind of like-- you can kind\nget a sense that the more, especially the more\ndimensions you have--",
    "start": "1988110",
    "end": "1995120"
  },
  {
    "text": "they are modeling a lot of\npixels or a lot of tokens-- there are many different\nways in which two--",
    "start": "1995120",
    "end": "2001293"
  },
  {
    "text": "or many different\nthings you could look at when you compare two\nprobability distributions. There's many different\nfeatures that you",
    "start": "2001293",
    "end": "2007870"
  },
  {
    "text": "could try to compare\nsamples with respect to. And so kind of like handcrafting\na test statistic and just say,",
    "start": "2007870",
    "end": "2018170"
  },
  {
    "text": "let's try to train a model based\non that is unlikely to work, because you're going to\nmatch the test statistic,",
    "start": "2018170",
    "end": "2024851"
  },
  {
    "text": "and there's going to\nbe other differences that you didn't think about that\nactually matter in practice.",
    "start": "2024852",
    "end": "2032590"
  },
  {
    "text": "So what we are going to do\nis we're going to try to--",
    "start": "2032590",
    "end": "2039549"
  },
  {
    "text": "instead of picking a hand-- a fixed handcrafted\ntest statistic,",
    "start": "2039550",
    "end": "2044590"
  },
  {
    "text": "we're going to try to learn\none to automatically identify in which ways these\ntwo set of samples--",
    "start": "2044590",
    "end": "2051638"
  },
  {
    "text": "that one from the data and\nthe one from the model-- they differ from each other.",
    "start": "2051639",
    "end": "2057388"
  },
  {
    "text": "Instead of keeping it fixed and\njust say let's compare the mean, we're going to try\nto learn what makes",
    "start": "2057389",
    "end": "2063780"
  },
  {
    "text": "these two samples different. And how to do that?",
    "start": "2063780",
    "end": "2070310"
  },
  {
    "text": "Any guess?  We're going to try to\njust basically train",
    "start": "2070310",
    "end": "2076399"
  },
  {
    "text": "a classifier, essentially,\nwhich in this the language of generative\nadversarial networks",
    "start": "2076400",
    "end": "2081980"
  },
  {
    "text": "is called a discriminator. So what we're going to-- machine learning--\nthat's exactly what you would do if you are trying\nto solve a classification",
    "start": "2081980",
    "end": "2090080"
  },
  {
    "text": "problem. You're trying to figure out what\ndistinguishes the positive class from the negative class.",
    "start": "2090080",
    "end": "2096919"
  },
  {
    "text": "The job of a classifier or, for\nexample, a deep neural network is to identify\nfeatures that allow",
    "start": "2096920",
    "end": "2102650"
  },
  {
    "text": "you to discriminate\nand distinguish these two groups of samples. So we're going to use\nthe same kind of idea",
    "start": "2102650",
    "end": "2108410"
  },
  {
    "text": "to figure out what sort\nof features of the data should we look at\nto discriminate between the green curve and\nthe red curve, essentially.",
    "start": "2108410",
    "end": "2116600"
  },
  {
    "text": " And so that's basically\nwhat we're going to do,",
    "start": "2116600",
    "end": "2121970"
  },
  {
    "text": "is we're going to\ntrain a classifier-- again, it's called a\ndiscriminator in this context",
    "start": "2121970",
    "end": "2127400"
  },
  {
    "text": "to basically classify. We're going to use a\nbinary classifier--",
    "start": "2127400",
    "end": "2132680"
  },
  {
    "text": "for example, a neural network-- to try to distinguish real\nsamples, which are basically",
    "start": "2132680",
    "end": "2138668"
  },
  {
    "text": "the ones from the\nfirst set-- the one generated by the\ndata distribution, which let's say label one,\nfrom fake samples which",
    "start": "2138668",
    "end": "2147680"
  },
  {
    "text": "are the ones\ngenerated by P theta by our model, which we can\nsay, for example, label 0.",
    "start": "2147680",
    "end": "2153560"
  },
  {
    "text": "And we can train a\nclassifier to do that. And as a test statistic,\nwe can use the minus",
    "start": "2153560",
    "end": "2161660"
  },
  {
    "text": "the loss of the classifier. ",
    "start": "2161660",
    "end": "2167230"
  },
  {
    "text": "Why do we do this? Well, what happens if the loss\nof the classifier is high? ",
    "start": "2167230",
    "end": "2175290"
  },
  {
    "text": "If-- well, let's [? say ?]\nif the loss of the classifier is low, then it means that\nyou're doing a very good job",
    "start": "2175290",
    "end": "2182369"
  },
  {
    "text": "at distinguishing the two. They are very well separated. Your classifier is doing\na very good job as--",
    "start": "2182370",
    "end": "2189930"
  },
  {
    "text": "at distinguishing these\ntwo groups of samples. And so they are very\ndifferent, so we want the test",
    "start": "2189930",
    "end": "2197280"
  },
  {
    "text": "statistic to be small--  to be large, sorry.",
    "start": "2197280",
    "end": "2202710"
  },
  {
    "text": "And if we have a high loss, then\nthe real and the fake samples",
    "start": "2202710",
    "end": "2207869"
  },
  {
    "text": "are hard to\ndistinguish, and so we expect that them to be similar.",
    "start": "2207870",
    "end": "2214120"
  },
  {
    "text": "Yeah? How does [INAUDIBLE]. I'm just a little bit\nconfused, because if we're",
    "start": "2214120",
    "end": "2220120"
  },
  {
    "text": "predicting the-- if we're doing this\nclassification task, the likelihood-- like the\nloss is still [INAUDIBLE]..",
    "start": "2220120",
    "end": "2226450"
  },
  {
    "text": "It's-- yeah, good-- so\nyou're exactly right. So it's based on the\nlikelihood of the classifier.",
    "start": "2226450",
    "end": "2232480"
  },
  {
    "text": "It's not based on the likelihood\nof the generative model. So by doing things\nthis way, we're",
    "start": "2232480",
    "end": "2237580"
  },
  {
    "text": "going to use the\nlikelihood, but it's going to be the likelihood\nof a classifier. And that's much easier\nbecause that's just",
    "start": "2237580",
    "end": "2242890"
  },
  {
    "text": "going to be a likelihood over\nbasically a binary variable. And so essentially,\nit doesn't really",
    "start": "2242890",
    "end": "2248380"
  },
  {
    "text": "put any restriction\non the neural network you can use as opposed to\nthe likelihood of a high--",
    "start": "2248380",
    "end": "2254200"
  },
  {
    "text": "over x over the\ninput, which requires you to either use\nautoregressive models",
    "start": "2254200",
    "end": "2259450"
  },
  {
    "text": "or invertible neural networks. It puts a lot of restrictions\non the kind of architectures you can use.",
    "start": "2259450",
    "end": "2264473"
  },
  {
    "text": "Here, it's just going\nto be a likelihood over basically a binary\nrandom variable, which is the class label.",
    "start": "2264473",
    "end": "2270410"
  },
  {
    "text": "And so all you have\nto do is you have to have a softmax at the end\nthat maps it to a probability.",
    "start": "2270410",
    "end": "2276410"
  },
  {
    "text": "But then you can do whatever\nyou want with respect to the x. So you can extract whatever\nfeatures you want of the input",
    "start": "2276410",
    "end": "2283780"
  },
  {
    "text": "x to come up with\na good classifier. And there is really\nno restriction on the kind of architecture.",
    "start": "2283780",
    "end": "2289060"
  },
  {
    "text": "[INAUDIBLE] it's much\neasier to optimize. Yeah. ",
    "start": "2289060",
    "end": "2295000"
  },
  {
    "text": "And so the goal of\nthis classifier-- the way we're going to train\nthis classifier is to maximize",
    "start": "2295000",
    "end": "2304370"
  },
  {
    "text": "the two sample test statistic\nto basically try to make the--",
    "start": "2304370",
    "end": "2309710"
  },
  {
    "text": "try to figure out what\nkind of statistic, basically, can\nmaximally distinguish",
    "start": "2309710",
    "end": "2315770"
  },
  {
    "text": "between the data\nand the model, which is the same as minimizing\nthe classification loss.",
    "start": "2315770",
    "end": "2321060"
  },
  {
    "text": "So we're going to train the\nclassifier the usual way to minimize the loss,\nbecause that's also",
    "start": "2321060",
    "end": "2326180"
  },
  {
    "text": "what gives us the most power to\nbasically distinguish these two distributions-- these\ntwo set of samples.",
    "start": "2326180",
    "end": "2333080"
  },
  {
    "text": "Yeah? To clarify, before,\nyou were talking about what statistics should\nwe use in order to figure out",
    "start": "2333080",
    "end": "2339350"
  },
  {
    "text": "if they're different. In this case, if I'm\nunderstanding correctly, that we don't end up really\neven thinking about what that statistic necessarily is.",
    "start": "2339350",
    "end": "2345482"
  },
  {
    "text": "We're just doing\nthe classification in order to figure\nout if it exceeds or-- It's going to be\nactually a statistic.",
    "start": "2345482",
    "end": "2350610"
  },
  {
    "text": "So this is the actual statistic. It could be something like this. So it's actually\nmore like a family of statistics, which\nare all the ones",
    "start": "2350610",
    "end": "2356390"
  },
  {
    "text": "that you can get as you change-- I mean, you have a\nclassifier, and then you can imagine changing the\nparameters of the classifier.",
    "start": "2356390",
    "end": "2362930"
  },
  {
    "text": "And then you can\nthink about if you were to try to find a\nclassifier that maximizes",
    "start": "2362930",
    "end": "2371840"
  },
  {
    "text": "this objective function,\nwhich is just minimizing cross entropy, which you\napproximate based on data",
    "start": "2371840",
    "end": "2378513"
  },
  {
    "text": "because you only have\nsamples from Pdata and you only have\nsamples from the model, you end up with something\nthat looks like this, which",
    "start": "2378513",
    "end": "2384260"
  },
  {
    "text": "is going to be the statistic. Remember before, we were just\ntaking the mean of x in S1",
    "start": "2384260",
    "end": "2389300"
  },
  {
    "text": "and the mean of x in-- of the samples in S2. Now we don't just\nlook at the means.",
    "start": "2389300",
    "end": "2395150"
  },
  {
    "text": "Now we look at\nwhat the classifier says on these two samples. And that's what\nwe're going to use.",
    "start": "2395150",
    "end": "2401599"
  },
  {
    "text": "And basically, as we discussed\nbefore, if the [? log-- ?] which this is just the--",
    "start": "2401600",
    "end": "2407210"
  },
  {
    "text": "this is the negative loss of\nthe classifier [INAUDIBLE] maximizing. And so if this\nquantity is large,",
    "start": "2407210",
    "end": "2416520"
  },
  {
    "text": "then it means that you're doing\na good job at separating them, and it means that\nthey are different.",
    "start": "2416520",
    "end": "2423720"
  },
  {
    "text": "And if the loss is-- if this quantity is low, then it\nmeans that you're very confused.",
    "start": "2423720",
    "end": "2429440"
  },
  {
    "text": "You're not doing a good\njob at distinguishing them, which supports the idea\nthat probably they are similar.",
    "start": "2429440",
    "end": "2436070"
  },
  {
    "text": "It's not hard-- it's\nhard to distinguish. ",
    "start": "2436070",
    "end": "2441529"
  },
  {
    "text": "Can you just clarify the\nnotation [INAUDIBLE]?? Yeah, so what we have here is--",
    "start": "2441530",
    "end": "2446750"
  },
  {
    "text": "the setting is the\none we had before, where we're saying\nwe're going to use--",
    "start": "2446750",
    "end": "2452210"
  },
  {
    "text": "as a statistic, we're going\nto use a classifier, which we're going to denote\nas the D5, because it's",
    "start": "2452210",
    "end": "2457760"
  },
  {
    "text": "going to be trainable. It's going to be\nanother neural network that we're going to train. And they're going to\ntrain this neural network",
    "start": "2457760",
    "end": "2465110"
  },
  {
    "text": "to try to distinguish\nbetween the samples in S1 and the samples in S2, where\nthe samples in S1 are the--",
    "start": "2465110",
    "end": "2473940"
  },
  {
    "text": "it's just a sample from-- a group of iid samples from\nthe data distribution, and S2",
    "start": "2473940",
    "end": "2480349"
  },
  {
    "text": "is just a group of samples from\nthe model distribution P theta. And if you maximize\nthis objective function",
    "start": "2480350",
    "end": "2487819"
  },
  {
    "text": "over the classifier,\nyou are basically trying to do as\nwell as you can--",
    "start": "2487820",
    "end": "2493010"
  },
  {
    "text": "you're basically just training\nthe classifier the usual way by minimizing cross-entropy.",
    "start": "2493010",
    "end": "2498460"
  },
  {
    "text": "And this is going to be\nour statistic in the sense that the loss of a classifier\nwill tell us how well--",
    "start": "2498460",
    "end": "2505960"
  },
  {
    "text": "how similar, basically, these\ntwo groups of samples are. And basically, yeah, the\ndiscriminator-- the phi",
    "start": "2505960",
    "end": "2516579"
  },
  {
    "text": "is performing binary\nclassification with the cross-entropy\nobjective. And you can kind of see that\nyou are going to do well here,",
    "start": "2516580",
    "end": "2525220"
  },
  {
    "text": "what you're supposed to do\nis you're supposed to assign probability 1 to all the\nsamples that come from Pdata,",
    "start": "2525220",
    "end": "2531970"
  },
  {
    "text": "and you're supposed to assign\nprobability 0 to all the samples that come from the model if you\nwant to maximize that quantity.",
    "start": "2531970",
    "end": "2538030"
  },
  {
    "text": " Which again is\nbasically what you would do if you were to train\na classifier to distinguish",
    "start": "2538030",
    "end": "2544930"
  },
  {
    "text": "the two groups of samples. And that's just like the\nnegative cross-entropy.",
    "start": "2544930",
    "end": "2551720"
  },
  {
    "text": "So for now, P theta\nis just fixed. It's the model distribution.",
    "start": "2551720",
    "end": "2559220"
  },
  {
    "text": "The data distribution\nis, as usual, fixed. You just have a bunch\nof samples from it. That's S1.",
    "start": "2559220",
    "end": "2564950"
  },
  {
    "text": "And what we're\nseeing is we're going to try to optimize\nthe classifier to do as well as it can at\nthis task of distinguishing",
    "start": "2564950",
    "end": "2574030"
  },
  {
    "text": "between real samples\nand fake samples, because the loss\nof the classifier",
    "start": "2574030",
    "end": "2579730"
  },
  {
    "text": "will basically tell us how\nsimilar the samples that come from the model are to\nsamples that come from the data.",
    "start": "2579730",
    "end": "2587980"
  },
  {
    "text": " Yeah? Can you please explain again\nhow the loss is telling us",
    "start": "2587980",
    "end": "2593792"
  },
  {
    "text": "if it's what we want? Yeah, so imagine--\ndo I have it here?",
    "start": "2593792",
    "end": "2601310"
  },
  {
    "text": "Yeah, so imagine\nthat somehow the-- maybe-- yeah.",
    "start": "2601310",
    "end": "2607430"
  },
  {
    "text": "Imagine that p-- that the two\ndistributions are the same.",
    "start": "2607430",
    "end": "2612540"
  },
  {
    "text": "So P theta is the\nsame as P data. Then these two samples would\ncome from the same distribution.",
    "start": "2612540",
    "end": "2620380"
  },
  {
    "text": "So the classifier basically\ncannot do better than chance, because you are literally just\ntaking two groups of samples",
    "start": "2620380",
    "end": "2626930"
  },
  {
    "text": "that come from the\nsame distribution. And then there is no\nway to distinguish them because they are actually coming\nfrom the same distribution.",
    "start": "2626930",
    "end": "2632790"
  },
  {
    "text": "So you cannot do\nbetter than chance, and so you would\nhave a high loss.",
    "start": "2632790",
    "end": "2639570"
  },
  {
    "text": "On the other hand, if the\nsamples were very different, the classifier would do a pretty\ngood job of separating them,",
    "start": "2639570",
    "end": "2646550"
  },
  {
    "text": "and then in which case,\nthe loss would be small. And so based on\nthat, we can come up",
    "start": "2646550",
    "end": "2652130"
  },
  {
    "text": "with a statistic that\nwould basically say, based on the loss\nof the classifier, we're going to\ndecide whether or not",
    "start": "2652130",
    "end": "2657260"
  },
  {
    "text": "the samples are similar or not. So to the extent that you\ncan separate them well using",
    "start": "2657260",
    "end": "2662780"
  },
  {
    "text": "a classifier, then we say\nthat they are different. If somehow, they are\nkind of all overlapping",
    "start": "2662780",
    "end": "2667970"
  },
  {
    "text": "and there is no way of coming up\nwith a good decision boundary, then we were saying, OK,\nthen probably the two samples",
    "start": "2667970",
    "end": "2675020"
  },
  {
    "text": "are similar. Why do we need [INAUDIBLE]\ndiscriminator is--",
    "start": "2675020",
    "end": "2682790"
  },
  {
    "text": "even if it's not optimal, we can\njust set that to [INAUDIBLE]?? You need the first term,\nbecause I guess you do",
    "start": "2682790",
    "end": "2690650"
  },
  {
    "text": "need to contrast\nit to something. Otherwise, it would\nbe trivial to-- as is the usual--",
    "start": "2690650",
    "end": "2697940"
  },
  {
    "text": "maybe if you need to-- if you\nhave a classification problem, you do need the two data\nfrom the two classes.",
    "start": "2697940",
    "end": "2704810"
  },
  {
    "text": "Otherwise, there is not\nreally much to learn. [INAUDIBLE] if my model-- my generative model,\nit should just",
    "start": "2704810",
    "end": "2711240"
  },
  {
    "text": "learn to fool the\ndiscriminator [INAUDIBLE].. So it should-- it can\nonly control samples",
    "start": "2711240",
    "end": "2716847"
  },
  {
    "text": "from [INAUDIBLE]. Oh, yeah. It has nothing to\ndo with [INAUDIBLE].. Yeah, you are already\none step ahead. You're already thinking\nabout optimizing with respect",
    "start": "2716847",
    "end": "2723750"
  },
  {
    "text": "to theta, which we are\nnot doing here right now. Yeah, but for now, we're\njust optimizing phi, and that depends\non both clearly.",
    "start": "2723750",
    "end": "2729990"
  },
  {
    "text": "You are right. When you optimize\nwith respect to theta, you only care about\nthe second term.",
    "start": "2729990",
    "end": "2735480"
  },
  {
    "text": "And indeed, the gradients\nwill only involve that term. Yeah? [INAUDIBLE] notation again?",
    "start": "2735480",
    "end": "2741810"
  },
  {
    "text": "Yeah, so the notation\nhere is saying-- recall that we have a\ngroup of samples S1 that",
    "start": "2741810",
    "end": "2749370"
  },
  {
    "text": "are coming from Pdata. This is your training\nset, or a mini batch of samples from Pdata. And then we have another\ngroup of samples S2",
    "start": "2749370",
    "end": "2756150"
  },
  {
    "text": "that are coming from a\nmodel distribution P theta. And then we have some kind\nof objective function here",
    "start": "2756150",
    "end": "2762599"
  },
  {
    "text": "that depends on the model\nand the discriminator. And what we're seeing is\nthat the discriminator",
    "start": "2762600",
    "end": "2770609"
  },
  {
    "text": "is going to try to optimize\nthis quantity, which depends on the model\nand the discriminator.",
    "start": "2770610",
    "end": "2778180"
  },
  {
    "text": "And it's going to\ntry to maximize it. And that's equivalent\nto basically trying to do as well as it\ncan at distinguishing",
    "start": "2778180",
    "end": "2785520"
  },
  {
    "text": "real samples from fake samples,\nwhich are coming from P theta.",
    "start": "2785520",
    "end": "2791190"
  },
  {
    "text": "The reason we have\nthis V is that we're going to also try\nto then optimize",
    "start": "2791190",
    "end": "2796470"
  },
  {
    "text": "this function with respect\nto theta, because we want to train the generative model. And so what we\nwill show up later",
    "start": "2796470",
    "end": "2802109"
  },
  {
    "text": "is basically a minimax\noptimization problem, where we're going to optimize\nthis V both with respect",
    "start": "2802110",
    "end": "2810210"
  },
  {
    "text": "to theta and with\nrespect to phi. So there's going to\nbe a competing game where the\ndiscriminator is trying",
    "start": "2810210",
    "end": "2817020"
  },
  {
    "text": "to optimize this V quantity-- it's trying to maximize\nthis V quantity, and the model is going to try\nto minimize that quantity,",
    "start": "2817020",
    "end": "2827280"
  },
  {
    "text": "because the model is--\nwe're trying to make it-- we're trying to change P theta\nto fool the discriminator,",
    "start": "2827280",
    "end": "2834359"
  },
  {
    "text": "or try to make the\nclassification problem as hard as possible. So later, there will be an\nouter minimization with respect",
    "start": "2834360",
    "end": "2841770"
  },
  {
    "text": "to theta, and that's\nhow we train the model. ",
    "start": "2841770",
    "end": "2848819"
  },
  {
    "text": "Cool. And it turns out that the\noptimal discriminator actually",
    "start": "2848820",
    "end": "2855710"
  },
  {
    "text": "has this form,\nwhich makes sense.",
    "start": "2855710",
    "end": "2861490"
  },
  {
    "text": "This is just the-- if you just use\nBayes' rule and you compute what is the true\nconditional probability",
    "start": "2861490",
    "end": "2870540"
  },
  {
    "text": "of a point x belonging to\nthe positive class, which in this case is, let's say, the\ndata distribution real samples.",
    "start": "2870540",
    "end": "2879210"
  },
  {
    "text": "Well, the true\nconditional distribution is basically the\nprobability that point",
    "start": "2879210",
    "end": "2884400"
  },
  {
    "text": "was generated by the\ndata distribution divided by the total probability\nthat that point was actually",
    "start": "2884400",
    "end": "2890010"
  },
  {
    "text": "generated by either the model\nor the real data distribution.",
    "start": "2890010",
    "end": "2897320"
  },
  {
    "text": "And so in particular,\nyou can see that if x is only possible\naccording to the data",
    "start": "2897320",
    "end": "2908270"
  },
  {
    "text": "distribution, then the\nmodel should assign the optimal discriminator\nwould assign 1,",
    "start": "2908270",
    "end": "2914960"
  },
  {
    "text": "because you have\nbasically 1 over 1 plus 0, and then it would be 1.",
    "start": "2914960",
    "end": "2920270"
  },
  {
    "text": "While for example, if the\ntwo models are the same-- so if a point is equally likely\nto come from Pdata or P theta--",
    "start": "2920270",
    "end": "2929780"
  },
  {
    "text": "then this quantity\nshould be 1/2. Which kind of makes\nsense, because if x",
    "start": "2929780",
    "end": "2935840"
  },
  {
    "text": "is equally likely under\nP theta and the Pdata, then the best you\ncan do is to say 1/2.",
    "start": "2935840",
    "end": "2942530"
  },
  {
    "text": "probability While if\na point is much more likely to have come from Pdata\nbecause this ratio is large,",
    "start": "2942530",
    "end": "2948839"
  },
  {
    "text": "then the classifier should\nassign high probability to that point.",
    "start": "2948840",
    "end": "2953880"
  },
  {
    "text": "And if a point is very unlikely\nto have come from Pdata because the numerator is\nsmall, then you should assign--",
    "start": "2953880",
    "end": "2960920"
  },
  {
    "text": "the classifier should assign\nlow probability to that point. And so in this case, the most\nconfused the discriminator can",
    "start": "2960920",
    "end": "2968220"
  },
  {
    "text": "[? be-- ?] because if\nP theta equals Pdata, it seems like we're going back\nto the objective we were trying",
    "start": "2968220",
    "end": "2973890"
  },
  {
    "text": "to get away from, which is\nthat you don't want to minimize the KL divergence between\nthe two distributions.",
    "start": "2973890",
    "end": "2979089"
  },
  {
    "text": "So I guess how do we\nescape that problem? So we don't, because it's\ntrue that the KL divergence is",
    "start": "2979090",
    "end": "2985380"
  },
  {
    "text": "going to be minimized when the\ntwo distributions are the same. And that's the global\noptimum of the KL divergence.",
    "start": "2985380",
    "end": "2991530"
  },
  {
    "text": "So whatever we do,\nwe're still going to go towards that\nglobal optimum. In practice, you cannot\nactually reach it.",
    "start": "2991530",
    "end": "2997780"
  },
  {
    "text": "And. So really what matters is that\nif you have an imperfect model-- so you cannot really\nachieve this--",
    "start": "2997780",
    "end": "3005570"
  },
  {
    "text": "the KL divergence will\ntake some non-zero value. This quantity might take\nsome other non-zero value.",
    "start": "3005570",
    "end": "3011970"
  },
  {
    "text": "And what we're-- the argument\ncould be that perhaps among",
    "start": "3011970",
    "end": "3017420"
  },
  {
    "text": "the suboptimal models, you\nshould prefer one that cannot fool disc--",
    "start": "3017420",
    "end": "3023300"
  },
  {
    "text": "cannot fool a discriminator as\nopposed to one that gives you high compression, because maybe\nthat's more aligned to what you",
    "start": "3023300",
    "end": "3029380"
  },
  {
    "text": "care about.  Can I have a discriminator\nthat's just a generative model",
    "start": "3029380",
    "end": "3035600"
  },
  {
    "text": "with a softmax over the final P\nof [? x? ?] [? Does ?] it need to be like a binary--",
    "start": "3035600",
    "end": "3040910"
  },
  {
    "text": "I can have another generative\nmodel trained with, let's say, a normal [? KL ?]\ndivergence, and I use that as my\ndiscriminator, [INAUDIBLE]..",
    "start": "3040910",
    "end": "3047690"
  },
  {
    "text": " So you're saying can you use\na likelihood-based model to--",
    "start": "3047690",
    "end": "3053630"
  },
  {
    "text": "Do the discriminator. --do the discriminator? Yeah, you can. There are actually\nvariants of ways of training generative\nmodels that are kind of",
    "start": "3053630",
    "end": "3063410"
  },
  {
    "text": "along those lines, where-- we're going to talk a\nlittle bit about that when we talk about noise\ncontrastive estimation.",
    "start": "3063410",
    "end": "3068930"
  },
  {
    "text": "I think it's going to be\npretty similar to what you're suggesting. So yeah, if you have access\nto a likelihood or part",
    "start": "3068930",
    "end": "3075740"
  },
  {
    "text": "of the likelihood, then you\ncan take advantage of it and try to design. But then that\ndefeats the purpose.",
    "start": "3075740",
    "end": "3081710"
  },
  {
    "text": "As what we'll see is that\nthe main advantage of this is that you don't have to\nhave access to a likelihood.",
    "start": "3081710",
    "end": "3086892"
  },
  {
    "text": "The only thing you need\nis to be able to sample from the model\nefficiently, which means that you can use very--\nessentially, an arbitrary neural",
    "start": "3086892",
    "end": "3093690"
  },
  {
    "text": "network to define the\ngenerative process, which is a big advantage of\nthis kind of procedure.",
    "start": "3093690",
    "end": "3099900"
  },
  {
    "start": "3099900",
    "end": "3105608"
  },
  {
    "text": "Yeah, that's what I was saying. If you check, you can see that\nif P theta is equal to ptheta,",
    "start": "3105608",
    "end": "3111490"
  },
  {
    "text": "then this quantity is going\nto be 1/2 for every x, which basically means that the best\nyou can do-- the classifier",
    "start": "3111490",
    "end": "3119740"
  },
  {
    "text": "will output 0.5 for\nevery x, which is indeed the best you can do.",
    "start": "3119740",
    "end": "3125680"
  },
  {
    "text": "If the distributions\nare the same, you cannot possibly\ndo better than chance. So this is when the classifier\nis maximally confused,",
    "start": "3125680",
    "end": "3133059"
  },
  {
    "text": "basically. ",
    "start": "3133060",
    "end": "3139290"
  },
  {
    "text": "So now, how do we\nget the next step? How do we use--",
    "start": "3139290",
    "end": "3144900"
  },
  {
    "text": "now that we've\ndecided that that's going to be our\nnotion of the way we're going to compare\nhow similar basically",
    "start": "3144900",
    "end": "3153069"
  },
  {
    "text": "Pdata is to P theta, now\nwe can define a learning objective where we try to\noptimize P theta to basically",
    "start": "3153070",
    "end": "3160710"
  },
  {
    "text": "fool the discriminator. And so it's going to\nbe kind of like a game.",
    "start": "3160710",
    "end": "3166150"
  },
  {
    "text": "It's going to be a\nminimax optimization problem between a\ngenerator, which is just your generative model and this\ndiscriminator-- this classifier.",
    "start": "3166150",
    "end": "3175450"
  },
  {
    "text": "And the generator\nis just going to be a generative model typically\nthat basically looks like a flow",
    "start": "3175450",
    "end": "3184300"
  },
  {
    "text": "model, in the sense that you\nstart with a latent variable z,",
    "start": "3184300",
    "end": "3189610"
  },
  {
    "text": "and then you map it\nto a sample through some deterministic\nmapping, which",
    "start": "3189610",
    "end": "3197430"
  },
  {
    "text": "is parameterized by\na neural network, and we're going to\ncall it G theta. And the-- so the\nsampling procedure",
    "start": "3197430",
    "end": "3204540"
  },
  {
    "text": "is the same as a flow model. You sample z from a simple\nprior-- for example, a Gaussian-- and\nthen you transform it",
    "start": "3204540",
    "end": "3210569"
  },
  {
    "text": "through this neural network. And crucially, this\nis similar to a flow,",
    "start": "3210570",
    "end": "3217180"
  },
  {
    "text": "but the mapping does not\nneed to be invertible. It can be an arbitrary\nneural network. It's an arbitrary sampler.",
    "start": "3217180",
    "end": "3223020"
  },
  {
    "text": "You start with\nsome random vector, and you transform\nit into a sample. No restrictions on what G is.",
    "start": "3223020",
    "end": "3230190"
  },
  {
    "text": "[INAUDIBLE] you have\nthe same dimensions. [INAUDIBLE] could have--",
    "start": "3230190",
    "end": "3235599"
  },
  {
    "text": "Exactly. It doesn't need to have\nthe same dimensions. No restrictions,\nbasically, on what G is.",
    "start": "3235600",
    "end": "3241020"
  },
  {
    "text": "Yeah? We can just use any generative\nmodel, like a [? VAE? ?] You could use any\ngenerative model.",
    "start": "3241020",
    "end": "3246450"
  },
  {
    "text": "Yeah, but the-- yeah, the\nadvantage-- the typical-- well, we'll see that it's\nactually convenient--",
    "start": "3246450",
    "end": "3253380"
  },
  {
    "text": "well, to train, it would\nbe good if you can backprop through the generative process.",
    "start": "3253380",
    "end": "3261099"
  },
  {
    "text": "But to some extent, you\ncan use-- you can indeed use other generative models.",
    "start": "3261100",
    "end": "3266140"
  },
  {
    "text": "But the advantage is\nthat basically you don't have any restrictions\non this neural network.",
    "start": "3266140",
    "end": "3272210"
  },
  {
    "text": "So we don't have to-- there is going to be some\ndistribution over the outputs",
    "start": "3272210",
    "end": "3278289"
  },
  {
    "text": "of this neural network,\nbut we're not ever going to compute it.",
    "start": "3278290",
    "end": "3283700"
  },
  {
    "text": "So unlike autoregressive\nmodels, or flow models, or VAE where we were always\nvery worried about being",
    "start": "3283700",
    "end": "3289630"
  },
  {
    "text": "able to compute given an x what\nwas the likelihood that my model produces that particular x,\nfor these kind of models,",
    "start": "3289630",
    "end": "3297940"
  },
  {
    "text": "we don't even\ncare, because we're going to use two sample tests\nto compare to train them.",
    "start": "3297940",
    "end": "3303140"
  },
  {
    "text": "And so we don't need to be\nable to evaluate likelihoods. And so we don't have any\nrestriction basically on what",
    "start": "3303140",
    "end": "3311650"
  },
  {
    "text": "this sampling procedure does. It can essentially be anything. ",
    "start": "3311650",
    "end": "3317730"
  },
  {
    "text": "And what we do\nthen is we're going to train this generator to\ndo the opposite, basically,",
    "start": "3317730",
    "end": "3325940"
  },
  {
    "text": "of the discriminator. The generator is going to try\nto change this mapping, which",
    "start": "3325940",
    "end": "3332599"
  },
  {
    "text": "implicitly also\nchanges the samples it produces to try to minimize\nthis statistic that we",
    "start": "3332600",
    "end": "3340910"
  },
  {
    "text": "were using in support\nof the fact that-- of this null\nhypothesis that says",
    "start": "3340910",
    "end": "3346280"
  },
  {
    "text": "the data is equal to the\ndistribution of samples that I get by using this model.",
    "start": "3346280",
    "end": "3353520"
  },
  {
    "text": "And so the end result is this. You have this minimax\noptimization problem,",
    "start": "3353520",
    "end": "3359390"
  },
  {
    "text": "where the function V\nis the same as this-- basically, the loss-- of the\nnegative loss of the classifier.",
    "start": "3359390",
    "end": "3366859"
  },
  {
    "text": "And then these two players\nin the game-- the generator and the discriminator, they\nhave opposing objectives.",
    "start": "3366860",
    "end": "3374750"
  },
  {
    "text": "The discriminator\nis going to try to maximize this\nwhich, again, this is",
    "start": "3374750",
    "end": "3379820"
  },
  {
    "text": "the same as what we had before. This is just saying classifier--\nthe discriminator is trying to do as well as it can to\ndistinguish samples coming",
    "start": "3379820",
    "end": "3387530"
  },
  {
    "text": "from the data [? to ?]\nsamples coming from this generative\nmodel-- from this generator.",
    "start": "3387530",
    "end": "3393619"
  },
  {
    "text": "And the generator is\ngoing to do the opposite. It's going to try to minimize\nthis objective function, which",
    "start": "3393620",
    "end": "3401300"
  },
  {
    "text": "basically means the\ngenerator is trying to confuse the classifier\nas much as it can.",
    "start": "3401300",
    "end": "3407110"
  },
  {
    "text": "So it's going to try\nto produce samples such that the best classifier\nyou can throw at that--",
    "start": "3407110",
    "end": "3413869"
  },
  {
    "text": "when you compare them to\nthe data distribution, the best classifier is\ngoing to perform poorly.",
    "start": "3413870",
    "end": "3420090"
  },
  {
    "text": "Which supports the fact that if\na classifier cannot distinguish the samples I produce from the\nsamples that are in the data",
    "start": "3420090",
    "end": "3428330"
  },
  {
    "text": "set, then I probably\nhave pretty good samples. And that's like the\ntraining objective",
    "start": "3428330",
    "end": "3434750"
  },
  {
    "text": "that we're going\nto use for training this class of generative models.",
    "start": "3434750",
    "end": "3439820"
  },
  {
    "start": "3439820",
    "end": "3445800"
  },
  {
    "text": "And so now, it\nturns out that this is related to a notion of\nsimilarity between probability",
    "start": "3445800",
    "end": "3454020"
  },
  {
    "text": "distributions. We know that-- what the\noptimal discriminator.",
    "start": "3454020",
    "end": "3459645"
  },
  {
    "text": "It's just the density ratio\nPdata over P theta plus P model, basically.",
    "start": "3459645",
    "end": "3465450"
  },
  {
    "text": "And we know that the\noptimal discriminator is going to depend on\nwhat the generator does.",
    "start": "3465450",
    "end": "3472620"
  },
  {
    "text": "And we can evaluate the value\nof this objective function when, basically, the second player--",
    "start": "3472620",
    "end": "3479040"
  },
  {
    "text": "the discriminator is picking\nthe best possible thing it can do given what the\ngenerator is doing and--",
    "start": "3479040",
    "end": "3488670"
  },
  {
    "text": "because we know what\nthat looks like. We know that when the\ndiscriminator is optimal,",
    "start": "3488670",
    "end": "3494197"
  },
  {
    "text": "the discriminator is just\ngoing to give us these density ratios-- Pdata over P theta plus P model.",
    "start": "3494197",
    "end": "3500110"
  },
  {
    "text": "So we can plug it\ninto this expression, and we get this\nsort of equation.",
    "start": "3500110",
    "end": "3506420"
  },
  {
    "text": "So this is the\noptimal loss that you get by choosing-- whenever\nyou choose a generator G,",
    "start": "3506420",
    "end": "3514870"
  },
  {
    "text": "If the classifier picks\nthe-- if we pick the best classifier given that G and\ngiven the data distribution,",
    "start": "3514870",
    "end": "3520029"
  },
  {
    "text": "this is the value of\nthat objective function. And this kind of looks\nlike a KL divergence.",
    "start": "3520030",
    "end": "3528310"
  },
  {
    "text": "It's an expectation of a\nlog of some density ratios.",
    "start": "3528310",
    "end": "3533540"
  },
  {
    "text": "Remember, divergence\nis expectation of under P of log P over\nQ. This has the flavor.",
    "start": "3533540",
    "end": "3540970"
  },
  {
    "text": "Now, the denominators here are\nnot probability distributions.",
    "start": "3540970",
    "end": "3547099"
  },
  {
    "text": "They are not normalized. You have to divide by\n2 if you want to get something that integrates to 1.",
    "start": "3547100",
    "end": "3554510"
  },
  {
    "text": "But you can basically just\ndivide by 2 here and here, and then subtract off\nthat logarithm of 4",
    "start": "3554510",
    "end": "3561590"
  },
  {
    "text": "that you just added\nin the denominators. And now, this is really\njust two KL divergences.",
    "start": "3561590",
    "end": "3569660"
  },
  {
    "text": "You can see that this is the\nKL divergence between the data and a mixture of data and model.",
    "start": "3569660",
    "end": "3577980"
  },
  {
    "text": "And this is KL divergence\nbetween model and a mixture-- the same thing-- mixture\nof data and model.",
    "start": "3577980",
    "end": "3583515"
  },
  {
    "text": " And then it shifted by this log\n4, which is just because I had--",
    "start": "3583515",
    "end": "3590560"
  },
  {
    "text": "I added these two here and here. And so you need a log 4\nthere to make it equal.",
    "start": "3590560",
    "end": "3596560"
  },
  {
    "text": "And so what this is saying\nis that this objective as a function of\nthe generator is",
    "start": "3596560",
    "end": "3604600"
  },
  {
    "text": "equal to this sum\nof KL divergences, which actually has a name.",
    "start": "3604600",
    "end": "3610010"
  },
  {
    "text": "It's called the\nJensen-Shannon divergence between the data distribution\nand the model distribution.",
    "start": "3610010",
    "end": "3619520"
  },
  {
    "text": "So this thing is essentially\n2 times this quantity called",
    "start": "3619520",
    "end": "3624580"
  },
  {
    "text": "the Jensen-Shannon\ndivergence and-- which is also known as\nsymmetric KL divergence.",
    "start": "3624580",
    "end": "3631690"
  },
  {
    "text": "If you look at that\nexpression, it's basically that-- if\nyou want to compute this Jensen-Shannon\ndivergence between P and Q,",
    "start": "3631690",
    "end": "3638050"
  },
  {
    "text": "you basically do the\nKL divergence between P and a mixture of 1/2 P and 1/2\nQ, and then you do the reverse.",
    "start": "3638050",
    "end": "3645730"
  },
  {
    "text": "KL divergence between Q\nand a mixture of P and Q. And this has nice properties.",
    "start": "3645730",
    "end": "3654380"
  },
  {
    "text": "We know KL divergence is\nnon-negative sum of two KL divergences. Also has to be non-negative.",
    "start": "3654380",
    "end": "3661220"
  },
  {
    "text": "What is the global\noptimum of this? When can when can it be 0? [INAUDIBLE]",
    "start": "3661220",
    "end": "3667560"
  },
  {
    "text": "Yeah, so it also has\nthe nice property that the global\noptimum can be achieved if, and only if, the\ndistributions are the same.",
    "start": "3667560",
    "end": "3675779"
  },
  {
    "text": "It's symmetric, which is nice. Remember, KL divergence\nwas not symmetric to-- PQ is not the same as KL QP.",
    "start": "3675780",
    "end": "3682620"
  },
  {
    "text": "This is symmetrized,\nbasically, by definition. And it also has\ntriangular inequality,",
    "start": "3682620",
    "end": "3689550"
  },
  {
    "text": "but not super important. And so what this means\nis that if somehow you",
    "start": "3689550",
    "end": "3697890"
  },
  {
    "text": "can optimize this quantity\nhere as a function of G--",
    "start": "3697890",
    "end": "3704680"
  },
  {
    "text": "so if you minimize this V\nas a function of G, which is what you do here\non the outside,",
    "start": "3704680",
    "end": "3710530"
  },
  {
    "text": "you will basically choose\ndata-- a model distribution",
    "start": "3710530",
    "end": "3715690"
  },
  {
    "text": "that matches the data\ndistribution exactly. So the global optimum\nis the same as what",
    "start": "3715690",
    "end": "3722170"
  },
  {
    "text": "you would get with\nKL divergence, and you would get kind\nof that optimal loss.",
    "start": "3722170",
    "end": "3729490"
  },
  {
    "text": "So the summary is\nthat-- basically, as a recap, what we're doing\nis we're changing the way",
    "start": "3729490",
    "end": "3734650"
  },
  {
    "text": "we're comparing the data\ndistribution and the model distribution. And we choose this similarity\nbased on a two sample test",
    "start": "3734650",
    "end": "3743799"
  },
  {
    "text": "statistic, and the\nstatistic is obtained by optimizing a classifier. And under ideal conditions,\nso that the classifier",
    "start": "3743800",
    "end": "3751119"
  },
  {
    "text": "can basically be optimal,\nwhich in practice, it's not going to be. Because if you use\na neural network, it might not be able to\nlearn that density ratio.",
    "start": "3751120",
    "end": "3759550"
  },
  {
    "text": "But under ideal\nconditions, this basically corresponds to not using KL\ndivergence here, and instead",
    "start": "3759550",
    "end": "3766870"
  },
  {
    "text": "using this Jensen-Shannon\ndivergence to compare the data to the model.",
    "start": "3766870",
    "end": "3772420"
  },
  {
    "text": " And the pros are that\nto evaluate the loss",
    "start": "3772420",
    "end": "3779440"
  },
  {
    "text": "and optimize it you only\nneed samples from P theta. You don't need to evaluate\nlikelihoods, which is great",
    "start": "3779440",
    "end": "3784900"
  },
  {
    "text": "because that means you\ndon't have restrictions on autoregressive, normalizing\nthings, normalizing flows.",
    "start": "3784900",
    "end": "3791350"
  },
  {
    "text": "You don't have to\nworry about It lots of flexibility in choosing the\narchitecture for the generator.",
    "start": "3791350",
    "end": "3797200"
  },
  {
    "text": "Basically, it just has to\ndefine a valid sampling procedure, which is\nessentially always the case.",
    "start": "3797200",
    "end": "3803059"
  },
  {
    "text": "If you feed in random noise\ninto a neural network, you get a sampling procedure--\na valid sampling procedure.",
    "start": "3803060",
    "end": "3808359"
  },
  {
    "text": "That's really all you need. And it's fast\nsampling, because you can generate a sample\nin a single pass",
    "start": "3808360",
    "end": "3815260"
  },
  {
    "text": "through the generator. So it's not like autoregressive\nmodels-- one variable at a time. Everything is generated\nin a single pass.",
    "start": "3815260",
    "end": "3823059"
  },
  {
    "text": "The con is that it's very\ndifficult to actually train in practice, because you have\nthis minimax optimization",
    "start": "3823060",
    "end": "3831430"
  },
  {
    "text": "problem. And so in practice what\nyou would have to do is you would have to do\nsomething like this.",
    "start": "3831430",
    "end": "3837850"
  },
  {
    "text": "You would have to, let's\nsay, start with a mini batch of training examples.",
    "start": "3837850",
    "end": "3843839"
  },
  {
    "text": "And then you get a\nsample of noise vectors from the prior of the generator.",
    "start": "3843840",
    "end": "3849640"
  },
  {
    "text": "And then you pass them through-- you pass these noise\nnoise vectors through G",
    "start": "3849640",
    "end": "3856710"
  },
  {
    "text": "to generate m fake samples. And then you basically have\nthese two mini batches.",
    "start": "3856710",
    "end": "3864790"
  },
  {
    "text": "You have m real data\npoints and m fake data points, which is what\nyou get by passing zi",
    "start": "3864790",
    "end": "3871109"
  },
  {
    "text": "through the generator. And then what you do\nis you try to optimize",
    "start": "3871110",
    "end": "3876590"
  },
  {
    "text": "your classifier--\nyour discriminator to maximize that objective.",
    "start": "3876590",
    "end": "3882500"
  },
  {
    "text": "The classifier is-- which\nis just the usual training of a classifier. You just try to take a step in\na gradient ascent in this case--",
    "start": "3882500",
    "end": "3892410"
  },
  {
    "text": "step on that objective\nfunction to try to improve this",
    "start": "3892410",
    "end": "3899150"
  },
  {
    "text": "optimization objective\nto do better, basically, at classifying-- at distinguishing real\ndata from fake data.",
    "start": "3899150",
    "end": "3907230"
  },
  {
    "text": "And as it was mentioned before,\nthen the generator is also",
    "start": "3907230",
    "end": "3915359"
  },
  {
    "text": "trying to [INAUDIBLE]---- is also\nlooking at the same objective function, but it has\nan opposite objective.",
    "start": "3915360",
    "end": "3922140"
  },
  {
    "text": "The generator is\ntrying to minimize that objective function. You can still do\ngradient descent.",
    "start": "3922140",
    "end": "3927720"
  },
  {
    "text": "And what you do is you\ncompute the gradient of this quantity with\nrespect to theta, which",
    "start": "3927720",
    "end": "3933930"
  },
  {
    "text": "are the generator parameters. And the first term does\nnot depend on theta. That's just the data.",
    "start": "3933930",
    "end": "3939990"
  },
  {
    "text": "So you cannot change\nwhat the data looks like, but what you can do is you can\ntry to adjust the parameters",
    "start": "3939990",
    "end": "3948180"
  },
  {
    "text": "of G-- so the parameters\nof the generator. So that the samples\nthat you produce by passing random noise\nthrough G kind of look",
    "start": "3948180",
    "end": "3957470"
  },
  {
    "text": "like the real data as measured\nby this discriminator D phi, which is what\nyou get by taking",
    "start": "3957470",
    "end": "3964430"
  },
  {
    "text": "this kind of gradient descent\nstep on that objective. For that [INAUDIBLE] the\ngradient descent step,",
    "start": "3964430",
    "end": "3971540"
  },
  {
    "text": "do we use the updated phi\nor the previous [? one? ?] Yeah, so it's unfortunately\nvery tricky to train this,",
    "start": "3971540",
    "end": "3980720"
  },
  {
    "text": "and this is not\nguaranteed to converge. And it can be--",
    "start": "3980720",
    "end": "3986420"
  },
  {
    "text": "in practice, you\nwould use the new phi, and you would do--\nyou would keep going and trying to playing this\ngame where each player is trying",
    "start": "3986420",
    "end": "3995810"
  },
  {
    "text": "to play a little bit better\nand hope that it converges. You repeat this and hope\nthat something good happens.",
    "start": "3995810",
    "end": "4004190"
  },
  {
    "text": "And here is kind\nof a visualization of what happens if you do this. So what's happening\nhere is you can",
    "start": "4004190",
    "end": "4011920"
  },
  {
    "text": "imagine there are a\nbunch of z vectors that are then mapped by\nG to different locations.",
    "start": "4011920",
    "end": "4019109"
  },
  {
    "text": "So here, z and x are\njust one-dimensional. And that is giving\nus a distribution,",
    "start": "4019110",
    "end": "4024330"
  },
  {
    "text": "which is this green curve. So you see that most of the\nsamples from the generator end up here.",
    "start": "4024330",
    "end": "4029720"
  },
  {
    "text": "And then there is a\ndata distribution, which is just this red curve. That's fixed.",
    "start": "4029720",
    "end": "4035150"
  },
  {
    "text": "It is whatever it is. And then let's say you start\nwith a discriminator, which is not very good, and it's this\nwiggly kind of blue line here.",
    "start": "4035150",
    "end": "4045230"
  },
  {
    "text": "Now, given that you have\na bunch of red samples and you have a bunch\nof green samples--",
    "start": "4045230",
    "end": "4050330"
  },
  {
    "text": "so real samples and\nfake ones coming from the current generator--",
    "start": "4050330",
    "end": "4056270"
  },
  {
    "text": "what you would do is\nyou would try to come up with a good classifier.",
    "start": "4056270",
    "end": "4061890"
  },
  {
    "text": "And the better\nclassifier that you get after you update\nthe discriminator is this blue curve.",
    "start": "4061890",
    "end": "4067810"
  },
  {
    "text": "So as you can see, if x's are\ncoming from this left side, then they're probably coming\nfrom the real data distribution.",
    "start": "4067810",
    "end": "4075690"
  },
  {
    "text": "There is no chance they\ncome from the generator, because the generator has\nvery low probability here.",
    "start": "4075690",
    "end": "4080790"
  },
  {
    "text": "The data is pretty high. And so the discriminator\nshould say, samples around here should have\nhigh probability of being real--",
    "start": "4080790",
    "end": "4087990"
  },
  {
    "text": "samples around here should\nhave high probability of being fake or low\nprobability of being real.",
    "start": "4087990",
    "end": "4093660"
  },
  {
    "text": "And then here in\nbetween, it's unclear. And then you just\ndecrease the probability",
    "start": "4093660",
    "end": "4099299"
  },
  {
    "text": "as you move towards the right. So that's what happens\nwhen you optimize phi.",
    "start": "4099300",
    "end": "4104790"
  },
  {
    "text": "You basically come up with\na better discriminator. Once you have, the\nbetter discriminator,",
    "start": "4104790",
    "end": "4110939"
  },
  {
    "text": "you can try to\nupdate the generator to fool this discriminator.",
    "start": "4110939",
    "end": "4116409"
  },
  {
    "text": "And so what would happen is you\nwould change these arrows here, which are basically the\nG, which is telling you",
    "start": "4116410",
    "end": "4123180"
  },
  {
    "text": "how you map the random\nnoise from the prior-- the z to [? axis ?] that you like.",
    "start": "4123180",
    "end": "4130028"
  },
  {
    "text": "And in particular,\nif you're trying to fool the\ndiscriminator, you are",
    "start": "4130029",
    "end": "4135120"
  },
  {
    "text": "going to try to shift\nprobability mass to the left. And so you might end up\nwith a new generator that",
    "start": "4135120",
    "end": "4142009"
  },
  {
    "text": "looks like this, and that\nconfuses the discriminator more,",
    "start": "4142010",
    "end": "4147839"
  },
  {
    "text": "because you can see\nit's overlapping more with the red curve,\nand the discriminator",
    "start": "4147840",
    "end": "4154489"
  },
  {
    "text": "is going to have a hard time\ntrying to distinguish these two samples. And then you keep going\nuntil you reach, hopefully,",
    "start": "4154490",
    "end": "4160920"
  },
  {
    "text": "this kind of convergence,\nwhere the discriminator where the generator matches\nthe data distribution.",
    "start": "4160920",
    "end": "4166689"
  },
  {
    "text": "So the green and the red\ncurves are overlapping. They're identical. And the discriminator is\nmaximally confused and is",
    "start": "4166689",
    "end": "4172859"
  },
  {
    "text": "producing 1/2 everywhere,\nbecause it cannot do better than chance at that point.",
    "start": "4172859",
    "end": "4179205"
  },
  {
    "text": "So in terms of actual\nimplementation, I guess this would not\nlook like a nested loop.",
    "start": "4179205",
    "end": "4185390"
  },
  {
    "text": "This is actually like a single\nfor loop in which you are doing alternate [? updates. ?]",
    "start": "4185390",
    "end": "4191500"
  },
  {
    "text": "Yeah.  And so in-- as part of\nwhat you need to do,",
    "start": "4191500",
    "end": "4199850"
  },
  {
    "text": "the job of the discriminator\nis to basically look at a bunch of\nsamples, like these 2.",
    "start": "4199850",
    "end": "4205190"
  },
  {
    "text": "And you need to decide\nwhich one is real and which one is\nfake, essentially.",
    "start": "4205190",
    "end": "4211110"
  },
  {
    "text": "And so which one do you think\nis real and which one is fake? First one. It's actually both are fake. [CHUCKLING]",
    "start": "4211110",
    "end": "4218060"
  },
  {
    "text": "Yeah. Wow. So yeah, and these kind\nof technologies-- these",
    "start": "4218060",
    "end": "4227540"
  },
  {
    "text": "improved a lot over the years. You can see from 2014\nall the way to 2018,",
    "start": "4227540",
    "end": "4232639"
  },
  {
    "text": "and there are even\nbetter improvements now. Very successful\nin a lot of tasks,",
    "start": "4232640",
    "end": "4238770"
  },
  {
    "text": "but it's very\nchallenging in practice to get them to work because\nof the unstable optimization. [INAUDIBLE] generates?",
    "start": "4238770",
    "end": "4246420"
  },
  {
    "text": "Just a random Gaussian\nnoise [INAUDIBLE].. There's no caption here. Yeah, it's just random\nnoise, and then you",
    "start": "4246420",
    "end": "4251910"
  },
  {
    "text": "pass them through\na neural network that turns them into images. Yeah, and there are\nseveral problems with GANs.",
    "start": "4251910",
    "end": "4261150"
  },
  {
    "text": "The first one is unstable\noptimization kind of like-- because you have\nthis minimax objective. It's very tricky to train them.",
    "start": "4261150",
    "end": "4267960"
  },
  {
    "text": "It's very hard to even\nknow when to stop, because it's no longer\nlike likelihood. And you can see it going\ndown, and at some point,",
    "start": "4267960",
    "end": "4274560"
  },
  {
    "text": "you can just stop when-- or up, let's say. Or you're maximizing likelihood.",
    "start": "4274560",
    "end": "4280620"
  },
  {
    "text": "You can see it goes, up. And at some point, you see\nit's not improving anymore. You can stop. Here is no longer the case\nthat when to stop basically.",
    "start": "4280620",
    "end": "4288030"
  },
  {
    "text": "And it can have\nthis problem called mode collapse, which we'll see\nbasically while KL divergence is",
    "start": "4288030",
    "end": "4296730"
  },
  {
    "text": "mode covering will try to put\nprobability mass everywhere, because otherwise you get\na huge penalty if something",
    "start": "4296730",
    "end": "4302370"
  },
  {
    "text": "is possible under Pdata. But you put 0 probability,\nthen you have infinite penalty. This GAN tend to be\nmuch more mode seeking,",
    "start": "4302370",
    "end": "4309900"
  },
  {
    "text": "and so they might just focus\non a few types of data points and completely stop generating\nother kinds of data points",
    "start": "4309900",
    "end": "4317820"
  },
  {
    "text": "that are present in\nthe training set. And so in practice, you\nneed a lot of tricks",
    "start": "4317820",
    "end": "4323010"
  },
  {
    "text": "to train these models. And I'm going to point you to\nsome reference for how this--",
    "start": "4323010",
    "end": "4330210"
  },
  {
    "text": "yeah, where you can\nsee some of them. I mean, in theory under some\nvery unrealistic assumptions,",
    "start": "4330210",
    "end": "4335969"
  },
  {
    "text": "that kind of procedure where you\ndo updates on the discriminator",
    "start": "4335970",
    "end": "4341980"
  },
  {
    "text": "and the generator-- or at\nevery step or like you find the optimal discriminator-- is supposed to work.",
    "start": "4341980",
    "end": "4348780"
  },
  {
    "text": "In practice, it doesn't. In practice, what you see\nis that the loss keeps oscillating during training.",
    "start": "4348780",
    "end": "4354880"
  },
  {
    "text": "So it might look\nsomething like this, where you have the generator\nloss, which is the green one.",
    "start": "4354880",
    "end": "4363320"
  },
  {
    "text": "The discriminator loss, and\nthe two types of samples the real and the fake ones. You can see it kind\nof keeps oscillating,",
    "start": "4363320",
    "end": "4371900"
  },
  {
    "text": "because you're not reaching\nkind of like the-- yeah, it doesn't converge, basically,\nthrough this gradient procedure.",
    "start": "4371900",
    "end": "4379820"
  },
  {
    "text": "And there is no robust\nstopping criteria. You don't know when\nshould you stop.",
    "start": "4379820",
    "end": "4385199"
  },
  {
    "text": "You don't really know. The only thing you can do\nis you look at the samples, and kind of see it's doing\nsomething meaningful,",
    "start": "4385200",
    "end": "4390690"
  },
  {
    "text": "and then you just stop. But it's a very hard to come\nup with a principled way of deciding when to\nstop the training.",
    "start": "4390690",
    "end": "4398420"
  },
  {
    "text": "And so the other problem is that\nyou have mode collapse, which is this problem that, again,\nthe generator is basically",
    "start": "4398420",
    "end": "4406610"
  },
  {
    "text": "collapsing on a few\ntypes of samples, and it doesn't generate\nthe other ones.",
    "start": "4406610",
    "end": "4411690"
  },
  {
    "text": "And you can see an\nexample here, where if you look at the\nsamples, it really",
    "start": "4411690",
    "end": "4417230"
  },
  {
    "text": "likes this type of data\npoints, and it keeps generating it over and over.",
    "start": "4417230",
    "end": "4424330"
  },
  {
    "text": "And you can see a more\ntoy example that gives you a sense of what's going on. Like imagine the\ndata distribution",
    "start": "4424330",
    "end": "4431610"
  },
  {
    "text": "is just kind of like a mixture\nof a bunch of Gaussians that are in 2D, and\nthey are kind of",
    "start": "4431610",
    "end": "4436949"
  },
  {
    "text": "like lying on this circle. And then what happens is that\nas you train your generator,",
    "start": "4436950",
    "end": "4444570"
  },
  {
    "text": "it kind of keeps moving around. So maybe at some point it\nproduces one of the modes.",
    "start": "4444570",
    "end": "4450090"
  },
  {
    "text": "And then the discriminator\nis doing a good job at distinguishing what it\ndoes from the real data.",
    "start": "4450090",
    "end": "4455838"
  },
  {
    "text": "Then the generator is moving\nthe probability [? mass ?] on a different mode. And it keeps moving around,\nbut it never actually covers",
    "start": "4455838",
    "end": "4462090"
  },
  {
    "text": "all of them at the same time. [INAUDIBLE] ",
    "start": "4462090",
    "end": "4479540"
  },
  {
    "text": "--fit into one mode. Yeah, so there are all kinds\nof tricks that you have to use. And here in as another\nexample on [? MNIST, ?]",
    "start": "4479540",
    "end": "4487040"
  },
  {
    "text": "where you can see how it's kind\nof like collapsing on generating one particular digit, and it\nkind of like stops learning.",
    "start": "4487040",
    "end": "4494570"
  },
  {
    "text": "And there's this great post-- blog post, ganhacks, where\nyou can see all sorts of hacks",
    "start": "4494570",
    "end": "4500840"
  },
  {
    "text": "that you can use to get\nGANs to work in practice. And there are all\nkinds of techniques,",
    "start": "4500840",
    "end": "4507389"
  },
  {
    "text": "including noise and\nvarious tricks that you can look up on that website.",
    "start": "4507390",
    "end": "4513680"
  },
  {
    "text": "Unfortunately, it's\nall very empirical. There is nothing that\nis guaranteed to work, and you kind of have to try.",
    "start": "4513680",
    "end": "4519679"
  },
  {
    "text": "And there are better\narchitectures that are-- tricks that sometimes\nworks and sometimes don't.",
    "start": "4519680",
    "end": "4525900"
  },
  {
    "text": "But I would say the fact\nthat these models are so hard to train is why they\nare no longer kind of state",
    "start": "4525900",
    "end": "4531719"
  },
  {
    "text": "of the art, and people\nhave kind of like largely given up on GANs, and people are\nusing diffusion models instead,",
    "start": "4531720",
    "end": "4537510"
  },
  {
    "text": "because they are\nmuch easier to train, and they have a clean\nloss that you can evaluate",
    "start": "4537510",
    "end": "4543630"
  },
  {
    "text": "during training, and\nyou know how to stop, and there is no instability. It's just a single optimization\nproblem that you have to do.",
    "start": "4543630",
    "end": "4550480"
  },
  {
    "text": "And I would say this is the main\nreason GANs are no longer so",
    "start": "4550480",
    "end": "4556140"
  },
  {
    "text": "much in fashion anymore. But people are still using\nthem, and it's a powerful idea",
    "start": "4556140",
    "end": "4563219"
  },
  {
    "text": "and it might come back. But I think that's the\nmain drawback-- very, very hard to train.",
    "start": "4563220",
    "end": "4569220"
  },
  {
    "text": "[INAUDIBLE] the\nloss of [INAUDIBLE]?? It seems like the\ngenerative model never",
    "start": "4569220",
    "end": "4576150"
  },
  {
    "text": "really sees the actual data. It seems like it's being\ntrained in a very--",
    "start": "4576150",
    "end": "4582660"
  },
  {
    "text": "signal from the discriminator. So it's like [INAUDIBLE]. Yeah, so the discriminator\nis seeing the data,",
    "start": "4582660",
    "end": "4589679"
  },
  {
    "text": "and then you-- the\ngenerator is kind of learning from what the\ndiscriminator has learned about the data, essentially.",
    "start": "4589680",
    "end": "4596010"
  },
  {
    "text": "So it's a very narrow signal. It's a very narrow signal. Yeah, and that's why you\nalso keep moving around,",
    "start": "4596010",
    "end": "4601590"
  },
  {
    "text": "because then depending\non what kind of features the discriminator\nis looking for, you might kind of try\nto catch up with those.",
    "start": "4601590",
    "end": "4607783"
  },
  {
    "text": "But then it just keep\nchanging, and then you never really\nconverge to anything. Can you combine GAN with the\nstandard generative model",
    "start": "4607783",
    "end": "4615180"
  },
  {
    "text": "and hope for the best? Yeah, in fact, there\nis even a paper-- recent papers at\nICML this year, where they were taking\na diffusion model,",
    "start": "4615180",
    "end": "4621385"
  },
  {
    "text": "and then they had a clever\nway of incorporating basically a discriminator to\nimprove performance.",
    "start": "4621385",
    "end": "4627210"
  },
  {
    "text": "And often, if you can\nthrow in maybe a little bit of discriminator to compare\nsamples in a meaningful way,",
    "start": "4627210",
    "end": "4636910"
  },
  {
    "text": "that often helps. So yeah, that's why we're\nstill talking about this idea,",
    "start": "4636910",
    "end": "4641918"
  },
  {
    "text": "because it's actually powerful. And you can use\nit in combination with other existing models.",
    "start": "4641918",
    "end": "4646989"
  },
  {
    "text": "And yeah, it's still\nvaluable for other things. Yeah?",
    "start": "4646990",
    "end": "4652660"
  },
  {
    "text": "Would it be a good idea to\ninitialize the generative model here with a pre-trained\nVAE, or would you",
    "start": "4652660",
    "end": "4658720"
  },
  {
    "text": "prefer to train it from scratch\nwhen you have enough data? What is the-- People usually\ntraining from scratch.",
    "start": "4658720",
    "end": "4664040"
  },
  {
    "text": "Yeah. ",
    "start": "4664040",
    "end": "4670290"
  },
  {
    "text": "And I think-- yeah, we're\npretty much at the end.",
    "start": "4670290",
    "end": "4675570"
  },
  {
    "text": "But this, I think, was the first\ngenerative model generated art.",
    "start": "4675570",
    "end": "4682619"
  },
  {
    "text": "It was auctioned at\nChristie's a few years ago. This is a painting\ngenerated by a GAN,",
    "start": "4682620",
    "end": "4690600"
  },
  {
    "text": "and one of the best\none at that time. I think it was expected to sell\nfor something in that range,",
    "start": "4690600",
    "end": "4695710"
  },
  {
    "text": "and I think it went for\nalmost half a million dollars. [CHUCKLING] So yeah.",
    "start": "4695710",
    "end": "4701545"
  },
  {
    "text": "[INAUDIBLE] Oh, yeah, it was-- but it was-- yeah, I guess\na novelty kind of thing.",
    "start": "4701545",
    "end": "4710140"
  },
  {
    "text": "So yeah, cool. I think that's it. ",
    "start": "4710140",
    "end": "4719000"
  }
]