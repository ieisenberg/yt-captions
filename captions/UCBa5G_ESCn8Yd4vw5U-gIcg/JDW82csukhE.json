[
  {
    "start": "0",
    "end": "14000"
  },
  {
    "text": "Thank you for the opportunity and I'm excited to talk about",
    "start": "4100",
    "end": "7859"
  },
  {
    "text": "my work on pre-training graph neural networks.",
    "start": "7860",
    "end": "11950"
  },
  {
    "text": "Okay, so, um, here I, uh,",
    "start": "13280",
    "end": "17610"
  },
  {
    "start": "14000",
    "end": "82000"
  },
  {
    "text": "I want to talk about applying",
    "start": "17610",
    "end": "19245"
  },
  {
    "text": "graph neural networks to application in scientific domains.",
    "start": "19245",
    "end": "23340"
  },
  {
    "text": "Uh, for example, uh, in chemistry,",
    "start": "23340",
    "end": "25710"
  },
  {
    "text": "we have this molecular graphs where each node is",
    "start": "25710",
    "end": "28230"
  },
  {
    "text": "an atom and each edge represents the chemical bond.",
    "start": "28230",
    "end": "31800"
  },
  {
    "text": "And here we are interested in predicting the properties of molecules.",
    "start": "31800",
    "end": "36645"
  },
  {
    "text": "For example, given this kind of molecular graph,",
    "start": "36645",
    "end": "39075"
  },
  {
    "text": "we want to predict uh,",
    "start": "39075",
    "end": "40350"
  },
  {
    "text": "whether it's toxic or not.",
    "start": "40350",
    "end": "42225"
  },
  {
    "text": "In biology, there's also a graph problem.",
    "start": "42225",
    "end": "44629"
  },
  {
    "text": "For example, uh, we have a",
    "start": "44630",
    "end": "46325"
  },
  {
    "text": "protein-protein association graph where each node is a protein and the edge represents",
    "start": "46325",
    "end": "51380"
  },
  {
    "text": "certain association between proteins and the associations are",
    "start": "51380",
    "end": "55130"
  },
  {
    "text": "very important to determine the functionality of the protein.",
    "start": "55130",
    "end": "59180"
  },
  {
    "text": "And our goal is to given this, uh,",
    "start": "59180",
    "end": "62105"
  },
  {
    "text": "subgraph centered around the protein node,",
    "start": "62105",
    "end": "64985"
  },
  {
    "text": "we want to predict whether",
    "start": "64985",
    "end": "66590"
  },
  {
    "text": "the center protein node has certain biological activity or not.",
    "start": "66590",
    "end": "70835"
  },
  {
    "text": "So what I want to say is that in bio- in many scientific domains, this, uh,",
    "start": "70835",
    "end": "75065"
  },
  {
    "text": "graph problem appears a lot and we want to apply GNNs to solve this problem.",
    "start": "75065",
    "end": "81210"
  },
  {
    "text": "And just to review how GNNs can be used for graph classification.",
    "start": "81470",
    "end": "87995"
  },
  {
    "start": "82000",
    "end": "155000"
  },
  {
    "text": "So GNNs obtain an embedding of the entire graph by following two steps.",
    "start": "87995",
    "end": "92455"
  },
  {
    "text": "First, um, given this,",
    "start": "92455",
    "end": "94340"
  },
  {
    "text": "the molecular graph, let's say graph, let's say molecule,",
    "start": "94340",
    "end": "97173"
  },
  {
    "text": "we obtain the embedding of node by either the aggregate neighboring information.",
    "start": "97174",
    "end": "102844"
  },
  {
    "text": "Once we do that,",
    "start": "102844",
    "end": "104450"
  },
  {
    "text": "we- once we obtain this node embedding,",
    "start": "104450",
    "end": "106520"
  },
  {
    "text": "we can pool this node embedding globally to obtain the embedding of the entire graph.",
    "start": "106520",
    "end": "112429"
  },
  {
    "text": "And once you obtain this embedding of the entire graph, we can use it,",
    "start": "112430",
    "end": "115680"
  },
  {
    "text": "or we can apply a linear function on top of it to predict whether this entire graph is,",
    "start": "115680",
    "end": "121205"
  },
  {
    "text": "let's say, toxic or not.",
    "start": "121205",
    "end": "123600"
  },
  {
    "text": "And just to, [NOISE] uh, say what this means is that, uh,",
    "start": "123710",
    "end": "128350"
  },
  {
    "text": "here the node embedding is trying to capture",
    "start": "128350",
    "end": "131150"
  },
  {
    "text": "the local neighborhood structure, uh, around each node.",
    "start": "131150",
    "end": "134599"
  },
  {
    "text": "If we apply k-hop GNNs,",
    "start": "134600",
    "end": "137120"
  },
  {
    "text": "we will just know that it should capture k-hop local neighborhood.",
    "start": "137120",
    "end": "140980"
  },
  {
    "text": "And then the embedding of the entire graph can, uh,",
    "start": "140980",
    "end": "145010"
  },
  {
    "text": "is basically aggregating this local structure though,",
    "start": "145010",
    "end": "148025"
  },
  {
    "text": "the node- node embedding that is capturing the local neighborhood structure.",
    "start": "148025",
    "end": "153000"
  },
  {
    "text": "And, uh, and, uh,",
    "start": "153320",
    "end": "155595"
  },
  {
    "start": "155000",
    "end": "222000"
  },
  {
    "text": "this is how GNN works,",
    "start": "155595",
    "end": "157200"
  },
  {
    "text": "and now I am going to talk about challenges of how- applying machine learning to,",
    "start": "157200",
    "end": "162105"
  },
  {
    "text": "uh, the scientific domains.",
    "start": "162105",
    "end": "163805"
  },
  {
    "text": "And there are basically two fundamental challenges.",
    "start": "163805",
    "end": "166385"
  },
  {
    "text": "The first challenge of applying machine learning to",
    "start": "166385",
    "end": "169189"
  },
  {
    "text": "the scientific domains is that all the scarcity of labeled data.",
    "start": "169190",
    "end": "173195"
  },
  {
    "text": "Obtaining labels in these scientific domains requires expensive lab experiments.",
    "start": "173195",
    "end": "177895"
  },
  {
    "text": "For example, to, or say whether a molecule is toxic or not,",
    "start": "177895",
    "end": "181850"
  },
  {
    "text": "you need to perform wet lab experiments,",
    "start": "181850",
    "end": "184415"
  },
  {
    "text": "and these are very expensive.",
    "start": "184415",
    "end": "185959"
  },
  {
    "text": "As a result, we cannot get that,",
    "start": "185960",
    "end": "188330"
  },
  {
    "text": "a lot of training data and machinery models tend to overfit to this small training data.",
    "start": "188330",
    "end": "193375"
  },
  {
    "text": "Another important issue is that out-of-distribution prediction, meaning that,",
    "start": "193375",
    "end": "198220"
  },
  {
    "text": "test examples we want to make a prediction on",
    "start": "198220",
    "end": "200675"
  },
  {
    "text": "tend to be very different from training examples.",
    "start": "200675",
    "end": "203365"
  },
  {
    "text": "And this is really the nature of scientific discovery because in scientific discovery,",
    "start": "203365",
    "end": "207770"
  },
  {
    "text": "we want- we want to discover some new molecule that is",
    "start": "207770",
    "end": "210710"
  },
  {
    "text": "inherently very different from- from your training molecules.",
    "start": "210710",
    "end": "214460"
  },
  {
    "text": "And in these domains,",
    "start": "214460",
    "end": "216080"
  },
  {
    "text": "machine learning models are extrapolate poorly.",
    "start": "216080",
    "end": "219930"
  },
  {
    "text": "And especially these two challenges are becoming more challenging for deep learning.",
    "start": "220900",
    "end": "226670"
  },
  {
    "start": "222000",
    "end": "382000"
  },
  {
    "text": "The first, uh, the first point of the label scarcity,",
    "start": "226670",
    "end": "230870"
  },
  {
    "text": "deep learning models have a lot of parameters to train,",
    "start": "230870",
    "end": "233870"
  },
  {
    "text": "uh, typically in the order of millions.",
    "start": "233870",
    "end": "236329"
  },
  {
    "text": "And uh- and in this regime,",
    "start": "236329",
    "end": "238265"
  },
  {
    "text": "the number of training data is much less than the number of parameters.",
    "start": "238265",
    "end": "242030"
  },
  {
    "text": "And deep-learning models are extremely prone to",
    "start": "242030",
    "end": "244220"
  },
  {
    "text": "overfitting on small data- small labeled data.",
    "start": "244220",
    "end": "247490"
  },
  {
    "text": "Another issue is that deep learning models are known to extrapolate poorly.",
    "start": "247490",
    "end": "252605"
  },
  {
    "text": "And that's reported that models often make, uh,",
    "start": "252605",
    "end": "256234"
  },
  {
    "text": "predictions based on spurious correlation in a dataset without",
    "start": "256235",
    "end": "259879"
  },
  {
    "text": "understanding the true causal mechanism of how to make prediction.",
    "start": "259880",
    "end": "264475"
  },
  {
    "text": "So let's consider this a toy example of",
    "start": "264475",
    "end": "267830"
  },
  {
    "text": "a- a image classification between polar bear and brown bear,",
    "start": "267830",
    "end": "271340"
  },
  {
    "text": "uh, this image is shown here.",
    "start": "271340",
    "end": "273505"
  },
  {
    "text": "So during training, let's say, uh,",
    "start": "273505",
    "end": "276120"
  },
  {
    "text": "our training data for- in our training data,",
    "start": "276120",
    "end": "278675"
  },
  {
    "text": "most polar bears have snow background like this,",
    "start": "278675",
    "end": "282020"
  },
  {
    "text": "and most brown bears have a grass background like this.",
    "start": "282020",
    "end": "285895"
  },
  {
    "text": "And as a result, the model can learn to predict,",
    "start": "285895",
    "end": "289370"
  },
  {
    "text": "make prediction of whether it's, uh,",
    "start": "289370",
    "end": "291680"
  },
  {
    "text": "the given image is polar bear or brown bear based on the image background rather than",
    "start": "291680",
    "end": "297350"
  },
  {
    "text": "animal itself because that's sufficient",
    "start": "297350",
    "end": "300380"
  },
  {
    "text": "to make predictions over the- over the training dataset.",
    "start": "300380",
    "end": "304025"
  },
  {
    "text": "But what if at the time same,",
    "start": "304025",
    "end": "306050"
  },
  {
    "text": "if we see polar bear on the grass,",
    "start": "306050",
    "end": "308150"
  },
  {
    "text": "then the model will, uh,",
    "start": "308150",
    "end": "309634"
  },
  {
    "text": "because the model is not understanding the- this prediction task,",
    "start": "309635",
    "end": "315020"
  },
  {
    "text": "the model will perform poorly on the- on",
    "start": "315020",
    "end": "318110"
  },
  {
    "text": "the data or test data that is different from training data.",
    "start": "318110",
    "end": "321409"
  },
  {
    "text": "And the model is, uh, just capturing this kind of",
    "start": "321410",
    "end": "323810"
  },
  {
    "text": "spurious, spurious correlation in the training dataset.",
    "start": "323810",
    "end": "328200"
  },
  {
    "text": "And our key idea or the goal, uh,",
    "start": "328670",
    "end": "333465"
  },
  {
    "text": "given these challenges is that we want to improve",
    "start": "333465",
    "end": "335880"
  },
  {
    "text": "model's out-of-distribution performance even with limited data.",
    "start": "335880",
    "end": "339950"
  },
  {
    "text": "And the way we do this is to inject domain knowledge",
    "start": "339950",
    "end": "344345"
  },
  {
    "text": "into a model before we apply them on scarcely-labeled tasks.",
    "start": "344345",
    "end": "349315"
  },
  {
    "text": "And that, this work- this may work because",
    "start": "349315",
    "end": "352860"
  },
  {
    "text": "the model already knows the domain knowledge before the model is",
    "start": "352860",
    "end": "356990"
  },
  {
    "text": "training on- on our downstream data so that the model can generalize",
    "start": "356990",
    "end": "361009"
  },
  {
    "text": "well without many task-specific labeled data and uh,",
    "start": "361010",
    "end": "365190"
  },
  {
    "text": "and uh, the model may be able to extract essential,",
    "start": "365190",
    "end": "368765"
  },
  {
    "text": "non-spurious, uh, pattern in the data, uh,",
    "start": "368765",
    "end": "372520"
  },
  {
    "text": "which allows the model to better extrapolate to the-",
    "start": "372520",
    "end": "376474"
  },
  {
    "text": "to the test data that still- that still very different from training data.",
    "start": "376475",
    "end": "381880"
  },
  {
    "text": "And a very effective solution to inject",
    "start": "381880",
    "end": "385220"
  },
  {
    "start": "382000",
    "end": "427000"
  },
  {
    "text": "domain knowledge into model is called pre-training.",
    "start": "385220",
    "end": "388160"
  },
  {
    "text": "And we pre-train a model on relevant tasks that's different",
    "start": "388160",
    "end": "392360"
  },
  {
    "text": "from a downstream task where data is abundant.",
    "start": "392360",
    "end": "396754"
  },
  {
    "text": "And after we pre-train the model,",
    "start": "396755",
    "end": "399320"
  },
  {
    "text": "the model's parameter, uh,",
    "start": "399320",
    "end": "401870"
  },
  {
    "text": "already contains some domain knowledge and once this is done,",
    "start": "401870",
    "end": "407330"
  },
  {
    "text": "we can transfer this pre-trained model parameter to the downstream task,",
    "start": "407330",
    "end": "411979"
  },
  {
    "text": "which is what we care about and which where we have small number of data.",
    "start": "411980",
    "end": "415990"
  },
  {
    "text": "And we can start from the pre-trained parameter and fine-tune the-",
    "start": "415990",
    "end": "420555"
  },
  {
    "text": "the parameters on the downstream tasks.",
    "start": "420555",
    "end": "426000"
  },
  {
    "text": "Um, and just to mention that pre-training has been hugely successful in- in the domain of",
    "start": "426000",
    "end": "433430"
  },
  {
    "start": "427000",
    "end": "494000"
  },
  {
    "text": "computer vision and natural language processing and it's",
    "start": "433430",
    "end": "436370"
  },
  {
    "text": "reported that pre-training improves label efficiency.",
    "start": "436370",
    "end": "440419"
  },
  {
    "text": "Also, pre-training improves out-of-distribution performance.",
    "start": "440420",
    "end": "447320"
  },
  {
    "text": "And because of this and within",
    "start": "447320",
    "end": "453350"
  },
  {
    "text": "pre-training can be a very powerful solution to the scientific applications.",
    "start": "453350",
    "end": "458780"
  },
  {
    "text": "And those two challenges are scarce labels and out-of-distribution prediction.",
    "start": "458780",
    "end": "463250"
  },
  {
    "text": "So now we motivated",
    "start": "463250",
    "end": "465950"
  },
  {
    "text": "this pre-training GNNs to solve an important problem in scientific applications.",
    "start": "465950",
    "end": "472115"
  },
  {
    "text": "So let's consider, you know, actually pre-trained GNNs.",
    "start": "472115",
    "end": "475595"
  },
  {
    "text": "And our work is about designing",
    "start": "475595",
    "end": "479570"
  },
  {
    "text": "GNN's pre-training strategies and we want to",
    "start": "479570",
    "end": "482120"
  },
  {
    "text": "systematically embedded- investigate the following two questions.",
    "start": "482120",
    "end": "486439"
  },
  {
    "text": "How effective is pre-training GNNs,",
    "start": "486439",
    "end": "489005"
  },
  {
    "text": "and what is the effective pre-training strategies.",
    "start": "489005",
    "end": "492605"
  },
  {
    "text": "So as a running example,",
    "start": "492605",
    "end": "495350"
  },
  {
    "start": "494000",
    "end": "554000"
  },
  {
    "text": "let's think about molecular property prediction task,",
    "start": "495350",
    "end": "497960"
  },
  {
    "text": "prediction for drug discovery.",
    "start": "497960",
    "end": "500134"
  },
  {
    "text": "For a given molecule, we want to predict its toxicity or biological activity.",
    "start": "500135",
    "end": "505315"
  },
  {
    "text": "And the very naive strategy is multi-task supervised pre-training on relevant labels.",
    "start": "505315",
    "end": "511700"
  },
  {
    "text": "This means that in the, for example,",
    "start": "511700",
    "end": "514010"
  },
  {
    "text": "chemical database we have, uh, all the,",
    "start": "514010",
    "end": "516680"
  },
  {
    "text": "uh, experimental measurements of",
    "start": "516680",
    "end": "519560"
  },
  {
    "text": "tox- various toxicity and biological activities of a lot of molecules.",
    "start": "519560",
    "end": "524450"
  },
  {
    "text": "And we can first pre-train GNNs to predict those,",
    "start": "524450",
    "end": "528380"
  },
  {
    "text": "uh, very diverse biological activity of toxicity.",
    "start": "528380",
    "end": "532595"
  },
  {
    "text": "And then we expect GNNs parameter to capture",
    "start": "532595",
    "end": "536569"
  },
  {
    "text": "some chemistry domain knowledge which can be tran-",
    "start": "536570",
    "end": "539510"
  },
  {
    "text": "and then we can transfer that parameter to our downstream task.",
    "start": "539510",
    "end": "544710"
  },
  {
    "text": "And the setting that we consider is, uh, to,",
    "start": "545600",
    "end": "549505"
  },
  {
    "text": "to study whether this naive strategy is effective,",
    "start": "549505",
    "end": "553279"
  },
  {
    "text": "we consider this setting.",
    "start": "553279",
    "end": "555335"
  },
  {
    "start": "554000",
    "end": "667000"
  },
  {
    "text": "We consider this binary classification of molecules.",
    "start": "555335",
    "end": "558350"
  },
  {
    "text": "Given molecule, we want to judge whether it's negative or positive.",
    "start": "558350",
    "end": "562329"
  },
  {
    "text": "And- and we, for the supervised pre-training part, uh,",
    "start": "562330",
    "end": "566600"
  },
  {
    "text": "we consider- we consider predicting",
    "start": "566600",
    "end": "568910"
  },
  {
    "text": "more than 1000 diverse binary bioassays annotated over 450,000 molecules.",
    "start": "568910",
    "end": "578685"
  },
  {
    "text": "So there are a lot of data,",
    "start": "578685",
    "end": "579810"
  },
  {
    "text": "uh, in this pre-training stage.",
    "start": "579810",
    "end": "582065"
  },
  {
    "text": "And then we apply transfer the parameter to or downstream task,",
    "start": "582065",
    "end": "586775"
  },
  {
    "text": "which is eight molecular classification datasets,",
    "start": "586775",
    "end": "589625"
  },
  {
    "text": "those are relatively small, uh,",
    "start": "589625",
    "end": "591530"
  },
  {
    "text": "about 1,000 to 100,000 molecules.",
    "start": "591530",
    "end": "594065"
  },
  {
    "text": "And- and for the data split,",
    "start": "594065",
    "end": "596590"
  },
  {
    "text": "uh, for the downstream task,",
    "start": "596590",
    "end": "598730"
  },
  {
    "text": "we consider the scaffold split,",
    "start": "598730",
    "end": "600755"
  },
  {
    "text": "which makes the test molecules out of distribution.",
    "start": "600755",
    "end": "604860"
  },
  {
    "text": "And it turns out that the Naive strategy of",
    "start": "604900",
    "end": "609350"
  },
  {
    "text": "this multi-task supervised pre-training on relevant labels doesn't work very well.",
    "start": "609350",
    "end": "614360"
  },
  {
    "text": "It gives a limited performance improvement on downstream tasks and even, uh,",
    "start": "614360",
    "end": "620325"
  },
  {
    "text": "leads to negative transfer,",
    "start": "620325",
    "end": "622455"
  },
  {
    "text": "which means that the pre-trained model performs worse than randomly initialized model.",
    "start": "622455",
    "end": "628135"
  },
  {
    "text": "So here's the, uh, table,",
    "start": "628135",
    "end": "630275"
  },
  {
    "text": "and here is a figure.",
    "start": "630275",
    "end": "631430"
  },
  {
    "text": "So we have our eight, uh,",
    "start": "631430",
    "end": "633755"
  },
  {
    "text": "downstream datasets and -and",
    "start": "633755",
    "end": "637035"
  },
  {
    "text": "the y-axis is the ROCAUC improvement over no pre-training baseline.",
    "start": "637035",
    "end": "642605"
  },
  {
    "text": "So this purple is a no pre-train baseline.",
    "start": "642605",
    "end": "645355"
  },
  {
    "text": "And we see that Naive strategy sometimes work well,",
    "start": "645355",
    "end": "649425"
  },
  {
    "text": "but for these two datasets,",
    "start": "649425",
    "end": "652475"
  },
  {
    "text": "it's actually performing worse than non pre-trained randomly initialized baseline.",
    "start": "652475",
    "end": "657730"
  },
  {
    "text": "So this is kind of not desirable,",
    "start": "657730",
    "end": "661545"
  },
  {
    "text": "we want pre-trained model to perform better.",
    "start": "661545",
    "end": "664709"
  },
  {
    "text": "So how- what is the- then we move on to our next question,",
    "start": "666330",
    "end": "671170"
  },
  {
    "start": "667000",
    "end": "785000"
  },
  {
    "text": "what is the effective pre-training strategy?",
    "start": "671170",
    "end": "673510"
  },
  {
    "text": "And our key idea is to pre-train both node and graph",
    "start": "673510",
    "end": "678340"
  },
  {
    "text": "embeddings so that GNNs can capture domain-specific knowledge about the graph,",
    "start": "678340",
    "end": "684310"
  },
  {
    "text": "uh, at the both, uh,",
    "start": "684310",
    "end": "685840"
  },
  {
    "text": "local and global level.",
    "start": "685840",
    "end": "687595"
  },
  {
    "text": "So- so in the naive strategy,",
    "start": "687595",
    "end": "690654"
  },
  {
    "text": "we pre-train GNNs, uh,",
    "start": "690655",
    "end": "693325"
  },
  {
    "text": "on the- at the level of graph, use this, uh,",
    "start": "693325",
    "end": "695875"
  },
  {
    "text": "graph embedding to make a various prediction.",
    "start": "695875",
    "end": "699100"
  },
  {
    "text": "But we think, uh,",
    "start": "699100",
    "end": "700720"
  },
  {
    "text": "it's important to also pre-training, uh, this, uh,",
    "start": "700720",
    "end": "703524"
  },
  {
    "text": "node embedding because these node embeddings are",
    "start": "703525",
    "end": "706165"
  },
  {
    "text": "aggregated to generate the embedding of the entire graph.",
    "start": "706165",
    "end": "709375"
  },
  {
    "text": "So we- we need to have a high-quality node embedding",
    "start": "709375",
    "end": "712465"
  },
  {
    "text": "that captures  the local neighborhood structure well.",
    "start": "712465",
    "end": "716380"
  },
  {
    "text": "So and the- the intuition behind this is that- so here's the figure,",
    "start": "716380",
    "end": "724870"
  },
  {
    "text": "the- the upper side means the node embedding",
    "start": "724870",
    "end": "728005"
  },
  {
    "text": "and these node embedding are proved to generate the embedding of the graph,",
    "start": "728005",
    "end": "731515"
  },
  {
    "text": "and we want the, uh,",
    "start": "731515",
    "end": "733020"
  },
  {
    "text": "the quality of the node embedding and graph embedding to be both high quality,",
    "start": "733020",
    "end": "738090"
  },
  {
    "text": "capture the semantics in the- in the- this is,",
    "start": "738090",
    "end": "743050"
  },
  {
    "text": "uh, different from the naive pre-training strategy where, for example,",
    "start": "743050",
    "end": "747459"
  },
  {
    "text": "if we- on the node-level pre-training,",
    "start": "747460",
    "end": "750580"
  },
  {
    "text": "we capture the semantic at the level of nodes,",
    "start": "750580",
    "end": "753175"
  },
  {
    "text": "but if those node embeddings they are globally aggregated,",
    "start": "753175",
    "end": "757060"
  },
  {
    "text": "we may not be able to obtain nice, uh,",
    "start": "757060",
    "end": "759550"
  },
  {
    "text": "graph embedding, um, or if we only pre-training,",
    "start": "759550",
    "end": "763330"
  },
  {
    "text": "uh, GNNs at the level of graph,",
    "start": "763330",
    "end": "765430"
  },
  {
    "text": "we can obtain nice graph embedding,",
    "start": "765430",
    "end": "768220"
  },
  {
    "text": "but the- there's no guarantee that the- before aggregation,",
    "start": "768220",
    "end": "772899"
  },
  {
    "text": "those node embeddings are of high-quality,",
    "start": "772900",
    "end": "775299"
  },
  {
    "text": "and this may lead to a negative transfer,",
    "start": "775299",
    "end": "779170"
  },
  {
    "text": "um, because the node embeddings are kind of not robust.",
    "start": "779170",
    "end": "783350"
  },
  {
    "text": "So- so in order to realize this, uh,",
    "start": "784140",
    "end": "788335"
  },
  {
    "start": "785000",
    "end": "818000"
  },
  {
    "text": "strategy to pre-train GNNs at the level of both nodes and graphs,",
    "start": "788335",
    "end": "792340"
  },
  {
    "text": "we- we designed three concrete methods.",
    "start": "792340",
    "end": "795790"
  },
  {
    "text": "Uh, two self-supervised method, uh,",
    "start": "795790",
    "end": "799089"
  },
  {
    "text": "meaning there is no need for external labels for node-level pre-training,",
    "start": "799090",
    "end": "803155"
  },
  {
    "text": "and one graph-level of prediction of pre-training strategy,",
    "start": "803155",
    "end": "809110"
  },
  {
    "text": "pre-training method, um, that's,",
    "start": "809110",
    "end": "811660"
  },
  {
    "text": "uh, that's, uh, supervised.",
    "start": "811660",
    "end": "813595"
  },
  {
    "text": "So let me go through these three concrete methods.",
    "start": "813595",
    "end": "817060"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "817060",
    "end": "818920"
  },
  {
    "start": "818000",
    "end": "864000"
  },
  {
    "text": "So the first method is the attribute masking.",
    "start": "818920",
    "end": "821500"
  },
  {
    "text": "It's a node-level pre-training method.",
    "start": "821500",
    "end": "822970"
  },
  {
    "text": "Um, the algorithm is quite simple.",
    "start": "822970",
    "end": "826584"
  },
  {
    "text": "So given that input graph, we first mask,",
    "start": "826585",
    "end": "829930"
  },
  {
    "text": "uh, uh, rand- randomly mask our node attributes.",
    "start": "829930",
    "end": "833770"
  },
  {
    "text": "So let's say we mask these two node attributes.",
    "start": "833770",
    "end": "836470"
  },
  {
    "text": "In the molecular graphs, this node that we are",
    "start": "836470",
    "end": "838779"
  },
  {
    "text": "basically masking the identity of the atom.",
    "start": "838780",
    "end": "841315"
  },
  {
    "text": "Um, and then we can use GNNs to generate, uh,",
    "start": "841315",
    "end": "845140"
  },
  {
    "text": "node embeddings of these two mask nodes,",
    "start": "845140",
    "end": "847930"
  },
  {
    "text": "and we use these node embeddings to predict the identity of the mask attributes.",
    "start": "847930",
    "end": "853600"
  },
  {
    "text": "So here we want to use the node embedding to predict the- that",
    "start": "853600",
    "end": "857605"
  },
  {
    "text": "this X is the- is the oxygen and this X is the carbon.",
    "start": "857605",
    "end": "862495"
  },
  {
    "text": "And the intuition behind this is that",
    "start": "862495",
    "end": "865750"
  },
  {
    "start": "864000",
    "end": "888000"
  },
  {
    "text": "the- through solving the mask attribute prediction task,",
    "start": "865750",
    "end": "868780"
  },
  {
    "text": "our GNN is forced to learn",
    "start": "868780",
    "end": "871210"
  },
  {
    "text": "the domain knowledge because it kind of needs to solve this quiz, and, uh, and, uh,",
    "start": "871210",
    "end": "876160"
  },
  {
    "text": "and- and through this,",
    "start": "876160",
    "end": "878394"
  },
  {
    "text": "solving this kind of quiz, GNN, uh,",
    "start": "878395",
    "end": "880720"
  },
  {
    "text": "can learn the- the parameter of the GNN can capture the- the chemistry domain.",
    "start": "880720",
    "end": "887120"
  },
  {
    "text": "The next self-supervised task that we propose is called the context prediction.",
    "start": "887700",
    "end": "893590"
  },
  {
    "start": "888000",
    "end": "973000"
  },
  {
    "text": "And, uh, and here the idea- the algorithm is as follows.",
    "start": "893590",
    "end": "898225"
  },
  {
    "text": "So for each graph,",
    "start": "898225",
    "end": "899920"
  },
  {
    "text": "uh, we sample one center node.",
    "start": "899920",
    "end": "901945"
  },
  {
    "text": "Let's say this is a,",
    "start": "901945",
    "end": "903385"
  },
  {
    "text": "um, um, red nodes,",
    "start": "903385",
    "end": "906265"
  },
  {
    "text": "and we, for this, uh,",
    "start": "906265",
    "end": "908410"
  },
  {
    "text": "center node, we extract neighborhood and context graph.",
    "start": "908410",
    "end": "911440"
  },
  {
    "text": "So neighborhood graph is just the k-hop neighborhood graph, uh, in this case,",
    "start": "911440",
    "end": "917320"
  },
  {
    "text": "two-hop neighbor graph like this,",
    "start": "917320",
    "end": "919570"
  },
  {
    "text": "and then the context graph is the- is",
    "start": "919570",
    "end": "921760"
  },
  {
    "text": "the surrounding graph that's directly attached to this,",
    "start": "921760",
    "end": "925390"
  },
  {
    "text": "uh, k-hop neighborhood graph.",
    "start": "925390",
    "end": "927200"
  },
  {
    "text": "Once we do extract this graph,",
    "start": "927200",
    "end": "929940"
  },
  {
    "text": "we can use two separate GNNs to encode this neighborhood graph into, you know, uh,",
    "start": "929940",
    "end": "937020"
  },
  {
    "text": "one- use one GNN to encode this neighborhood graph into a vector,",
    "start": "937020",
    "end": "941670"
  },
  {
    "text": "another GNN is to encode this context graph into a vector.",
    "start": "941670",
    "end": "945440"
  },
  {
    "text": "And our objective is to maximize, uh,",
    "start": "945440",
    "end": "949045"
  },
  {
    "text": "or maximize the inner product between the true neighborhood context pair",
    "start": "949045",
    "end": "953740"
  },
  {
    "text": "while minimizing the inner product between the false or negative,",
    "start": "953740",
    "end": "959274"
  },
  {
    "text": "uh, uh, false on neighborhood context pair,",
    "start": "959274",
    "end": "963490"
  },
  {
    "text": "and these false pairs can be obtained by, uh,",
    "start": "963490",
    "end": "966865"
  },
  {
    "text": "randomly sample context graph from other neighborhoods.",
    "start": "966865",
    "end": "971305"
  },
  {
    "text": "And here the intuition is that we're using- we are assuming that",
    "start": "971305",
    "end": "975790"
  },
  {
    "start": "973000",
    "end": "1012000"
  },
  {
    "text": "subgraphs that are surrounded by similar contexts are semantically similar.",
    "start": "975790",
    "end": "981685"
  },
  {
    "text": "So we are basically pushing, uh,",
    "start": "981685",
    "end": "984220"
  },
  {
    "text": "the embedding of the neighborhood to be",
    "start": "984220",
    "end": "987730"
  },
  {
    "text": "similar if they are- have similar context graphs.",
    "start": "987730",
    "end": "991285"
  },
  {
    "text": "And this is the kind of widely used, uh,",
    "start": "991285",
    "end": "995529"
  },
  {
    "text": "assumption in natural language processing,",
    "start": "995530",
    "end": "998590"
  },
  {
    "text": "it's called distributional hypothesis.",
    "start": "998590",
    "end": "1000690"
  },
  {
    "text": "So words that are appearing with similar- similar contexts have similar meaning,",
    "start": "1000690",
    "end": "1004800"
  },
  {
    "text": "and it's exploited in the famous, uh, Word2vec model.",
    "start": "1004800",
    "end": "1008115"
  },
  {
    "text": "Um, so finally, um,",
    "start": "1008115",
    "end": "1013050"
  },
  {
    "start": "1012000",
    "end": "1036000"
  },
  {
    "text": "I'm going to talk about this, uh,",
    "start": "1013050",
    "end": "1014730"
  },
  {
    "text": "graph-level pre-training task, which is essentially what I introduced before.",
    "start": "1014730",
    "end": "1019709"
  },
  {
    "text": "So multi-task supervised pre-training on many relevant graph level- labels.",
    "start": "1019710",
    "end": "1025755"
  },
  {
    "text": "And this is really the direct approach, uh,",
    "start": "1025755",
    "end": "1028454"
  },
  {
    "text": "to inject, uh, domain knowledge into the model.",
    "start": "1028455",
    "end": "1031289"
  },
  {
    "text": "[NOISE] So to summarize the overall strategy,",
    "start": "1031290",
    "end": "1038069"
  },
  {
    "start": "1036000",
    "end": "1073000"
  },
  {
    "text": "we perform- we first perform node-level pre-training,",
    "start": "1038070",
    "end": "1041880"
  },
  {
    "text": "uh, to- to obtain good node representation.",
    "start": "1041880",
    "end": "1045944"
  },
  {
    "text": "We- we use that, uh,",
    "start": "1045945",
    "end": "1047595"
  },
  {
    "text": "pre-training parameter, uh, to- and then, uh,",
    "start": "1047595",
    "end": "1051780"
  },
  {
    "text": "to- to further perform pre-training, uh,",
    "start": "1051780",
    "end": "1054960"
  },
  {
    "text": "at the level of graph using supervised graph-level pre-training.",
    "start": "1054960",
    "end": "1058605"
  },
  {
    "text": "Once- once we've done,",
    "start": "1058605",
    "end": "1060765"
  },
  {
    "text": "uh, this, both steps,",
    "start": "1060765",
    "end": "1063030"
  },
  {
    "text": "we fine-tune, uh, the parameters on the downstream task that we care about.",
    "start": "1063030",
    "end": "1069345"
  },
  {
    "text": "So this is the overall strategy.",
    "start": "1069345",
    "end": "1072030"
  },
  {
    "text": "And it turns out that our strategy works out pretty well.",
    "start": "1072030",
    "end": "1076515"
  },
  {
    "start": "1073000",
    "end": "1147000"
  },
  {
    "text": "Uh, as you can see, these, uh,",
    "start": "1076515",
    "end": "1078045"
  },
  {
    "text": "green dots are our strategy,",
    "start": "1078045",
    "end": "1080189"
  },
  {
    "text": "and our strategy first avoids the negative transfer in these, uh, two datasets,",
    "start": "1080189",
    "end": "1085365"
  },
  {
    "text": "and also consistently perform better than- than- than this,",
    "start": "1085365",
    "end": "1090825"
  },
  {
    "text": "uh, orange naive strategy baseline across the datasets.",
    "start": "1090825",
    "end": "1094860"
  },
  {
    "text": "Uh, and also, um,",
    "start": "1094860",
    "end": "1099270"
  },
  {
    "text": "another interesting side note is that, uh,",
    "start": "1099270",
    "end": "1102135"
  },
  {
    "text": "we fix the pre-training strategy and the pre-training different GNNs models,",
    "start": "1102135",
    "end": "1107475"
  },
  {
    "text": "uh, use different GNNs models for pre-training.",
    "start": "1107475",
    "end": "1110745"
  },
  {
    "text": "And what we found out is that the most expressive GNN model,",
    "start": "1110745",
    "end": "1114930"
  },
  {
    "text": "namely GIN, that we've learned in the lecture,",
    "start": "1114930",
    "end": "1117690"
  },
  {
    "text": "um, um, benefits most from pre-training.",
    "start": "1117690",
    "end": "1121620"
  },
  {
    "text": "Uh, as you can see here,",
    "start": "1121620",
    "end": "1123210"
  },
  {
    "text": "the gain of pre-trained model versus non-pre-trained model is the- is the largest,",
    "start": "1123210",
    "end": "1128970"
  },
  {
    "text": "um, in terms of accuracy.",
    "start": "1128970",
    "end": "1131220"
  },
  {
    "text": "And- and the intuition here is that",
    "start": "1131220",
    "end": "1133679"
  },
  {
    "text": "the expressive GNN model can learn to",
    "start": "1133680",
    "end": "1136860"
  },
  {
    "text": "capture more domain knowledge than less expressive model,",
    "start": "1136860",
    "end": "1140100"
  },
  {
    "text": "especially learned from large amounts of,",
    "start": "1140100",
    "end": "1142515"
  },
  {
    "text": "uh, data during pre-training.",
    "start": "1142515",
    "end": "1145630"
  },
  {
    "text": "So to summarize of our GNNs, we've, uh,",
    "start": "1145700",
    "end": "1150510"
  },
  {
    "start": "1147000",
    "end": "1207000"
  },
  {
    "text": "said- learned that the GNNs have important application",
    "start": "1150510",
    "end": "1154020"
  },
  {
    "text": "in scientific domains like molecular property prediction or,",
    "start": "1154020",
    "end": "1157800"
  },
  {
    "text": "um, protein function prediction,",
    "start": "1157800",
    "end": "1160214"
  },
  {
    "text": "but, uh, those application domains present the challenges",
    "start": "1160214",
    "end": "1163650"
  },
  {
    "text": "of label scarcity and out-of-distribution prediction,",
    "start": "1163650",
    "end": "1167280"
  },
  {
    "text": "and we argue that pre-training is a promising,",
    "start": "1167280",
    "end": "1170655"
  },
  {
    "text": "uh, framework to tackle both of the challenges.",
    "start": "1170655",
    "end": "1174825"
  },
  {
    "text": "However, we found that naive pre-training strategy of this, uh,",
    "start": "1174825",
    "end": "1179174"
  },
  {
    "text": "supervised graph level pre-training gives",
    "start": "1179175",
    "end": "1181170"
  },
  {
    "text": "sub-optimal performance and even leads to negative transfer.",
    "start": "1181170",
    "end": "1184995"
  },
  {
    "text": "And our strategy is to, um,",
    "start": "1184995",
    "end": "1187545"
  },
  {
    "text": "effective strategy is to pre-train both node and graph embeddings,",
    "start": "1187545",
    "end": "1191475"
  },
  {
    "text": "and we found this strategy leads to",
    "start": "1191475",
    "end": "1193320"
  },
  {
    "text": "a significant performance gain on diverse downstream tasks.",
    "start": "1193320",
    "end": "1197730"
  },
  {
    "text": "Um, yeah, thank you for listening.",
    "start": "1197730",
    "end": "1201910"
  }
]