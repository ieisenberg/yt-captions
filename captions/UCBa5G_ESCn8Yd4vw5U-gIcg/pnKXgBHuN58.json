[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5592"
  },
  {
    "text": "Hi, in this module,\nI'm going to talk",
    "start": "5592",
    "end": "7360"
  },
  {
    "start": "6000",
    "end": "13000"
  },
  {
    "text": "about neural networks, a way to\nconstruct nonlinear predictors",
    "start": "7360",
    "end": "10720"
  },
  {
    "text": "via problem decomposition.",
    "start": "10720",
    "end": "14280"
  },
  {
    "start": "13000",
    "end": "65000"
  },
  {
    "text": "So when we started, we talked\nabout linear predictors.",
    "start": "14280",
    "end": "17119"
  },
  {
    "text": "And they were\nlinear in two ways.",
    "start": "17120",
    "end": "19500"
  },
  {
    "text": "First is that the feature vector\nwas a linear function of x,",
    "start": "19500",
    "end": "23360"
  },
  {
    "text": "and the way that the\nfeature vector interacted",
    "start": "23360",
    "end": "25880"
  },
  {
    "text": "with the prediction\nwas also linear.",
    "start": "25880",
    "end": "27859"
  },
  {
    "text": "This gave you rise to lines.",
    "start": "27860",
    "end": "31340"
  },
  {
    "text": "Next, we talked about\nnonlinear predictors",
    "start": "31340",
    "end": "33920"
  },
  {
    "text": "but keeping the same\nlinear machinery",
    "start": "33920",
    "end": "38120"
  },
  {
    "text": "but just playing around\nwith the feature vector,",
    "start": "38120",
    "end": "40910"
  },
  {
    "text": "and by adding terms\nlike x squared,",
    "start": "40910",
    "end": "43040"
  },
  {
    "text": "you could get quadratic\npredictors and so on.",
    "start": "43040",
    "end": "46350"
  },
  {
    "text": "So now what we're\ngoing to do is we're",
    "start": "46350",
    "end": "47989"
  },
  {
    "text": "going to define neural\nnetworks, where we can just",
    "start": "47990",
    "end": "50780"
  },
  {
    "text": "leave phi of x alone, the\nfeature vector alone, and play",
    "start": "50780",
    "end": "55519"
  },
  {
    "text": "with the way that the\nfeature vector results",
    "start": "55520",
    "end": "59000"
  },
  {
    "text": "in the prediction.",
    "start": "59000",
    "end": "60980"
  },
  {
    "text": "And that will allow us to\nget all sorts of fancy stuff.",
    "start": "60980",
    "end": "65378"
  },
  {
    "start": "65000",
    "end": "190000"
  },
  {
    "text": "So let me begin with\na motivating example.",
    "start": "65379",
    "end": "68820"
  },
  {
    "text": "So suppose you're trying to\npredict whether two cars are",
    "start": "68820",
    "end": "71250"
  },
  {
    "text": "going to collide or not.",
    "start": "71250",
    "end": "72970"
  },
  {
    "text": "So the input are the\npositions of the two cars.",
    "start": "72970",
    "end": "76650"
  },
  {
    "text": "So x1 is the position\nof car 1, and x2",
    "start": "76650",
    "end": "79950"
  },
  {
    "text": "is the position of car 2.",
    "start": "79950",
    "end": "82170"
  },
  {
    "text": "And what you like to output\nis whether y equals 1,",
    "start": "82170",
    "end": "85770"
  },
  {
    "text": "whether it's safe, or\ny equals 1 whether--",
    "start": "85770",
    "end": "88899"
  },
  {
    "text": "y equals minus 1 or whether\nthey'll collide or not.",
    "start": "88900",
    "end": "92980"
  },
  {
    "text": "And what is unknown\nhere, is that we're",
    "start": "92980",
    "end": "98660"
  },
  {
    "text": "going to say that cars are safe\nif they're sufficiently far.",
    "start": "98660",
    "end": "101990"
  },
  {
    "text": "So the distance between\nthem is at least 1,",
    "start": "101990",
    "end": "104479"
  },
  {
    "text": "then we're going to be safe.",
    "start": "104480",
    "end": "106400"
  },
  {
    "text": "If we can visualize this, a\ntrue predictor, as follows.",
    "start": "106400",
    "end": "111410"
  },
  {
    "text": "So here is x1 and x2 and\nwhat is going to happen",
    "start": "111410",
    "end": "117500"
  },
  {
    "text": "is they're going to draw\nthese two lines, here.",
    "start": "117500",
    "end": "120960"
  },
  {
    "text": "And anything, any point\nthat is over here,",
    "start": "120960",
    "end": "126430"
  },
  {
    "text": "and anything that\nis over here, is",
    "start": "126430",
    "end": "128949"
  },
  {
    "text": "going to be labeled as\nplus, which is safe.",
    "start": "128949",
    "end": "133210"
  },
  {
    "text": "And anything that's\nin between, is",
    "start": "133210",
    "end": "135640"
  },
  {
    "text": "going to be labeled as minus,\nor that they'll collide.",
    "start": "135640",
    "end": "140130"
  },
  {
    "text": "OK, so let's do\nsome examples, here.",
    "start": "140130",
    "end": "142030"
  },
  {
    "text": "So suppose we have a point 0,\n2, which is this point here.",
    "start": "142030",
    "end": "148290"
  },
  {
    "text": "This is safe.",
    "start": "148290",
    "end": "151170"
  },
  {
    "text": "So y equals 1.",
    "start": "151170",
    "end": "152120"
  },
  {
    "text": "2, 0 is also safe.",
    "start": "152120",
    "end": "155900"
  },
  {
    "text": "And 0, 0 is here,\nwhich is not safe.",
    "start": "155900",
    "end": "161689"
  },
  {
    "text": "And 2, 2 is minus 1,\nwhich is also not safe.",
    "start": "161690",
    "end": "167710"
  },
  {
    "text": "So as an aside, this\nconfiguration points",
    "start": "167710",
    "end": "171490"
  },
  {
    "text": "is what was historically\nknown as an XOR problem",
    "start": "171490",
    "end": "174640"
  },
  {
    "text": "and was shown that pure\nlinear classifiers could not",
    "start": "174640",
    "end": "178030"
  },
  {
    "text": "be used to solve this problem.",
    "start": "178030",
    "end": "179720"
  },
  {
    "text": "You couldn't draw\na line to separate",
    "start": "179720",
    "end": "181600"
  },
  {
    "text": "the blue and the orange points.",
    "start": "181600",
    "end": "184110"
  },
  {
    "text": "But nonetheless, we're going\nto show how neural networks can",
    "start": "184110",
    "end": "186610"
  },
  {
    "text": "be used to solve this.",
    "start": "186610",
    "end": "188920"
  },
  {
    "start": "188920",
    "end": "191730"
  },
  {
    "start": "190000",
    "end": "305000"
  },
  {
    "text": "OK, so the key intuition is the\nidea of problem decomposition.",
    "start": "191730",
    "end": "196360"
  },
  {
    "text": "So instead of solving\nthe problem all at once,",
    "start": "196360",
    "end": "198390"
  },
  {
    "text": "we're going to decompose\nit into two subproblems.",
    "start": "198390",
    "end": "201630"
  },
  {
    "text": "But first, we're going to test\nif car 1 is to the far right",
    "start": "201630",
    "end": "204600"
  },
  {
    "text": "of car 2, and in\nthe picture here,",
    "start": "204600",
    "end": "207420"
  },
  {
    "text": "that corresponds to simply\nthis region over here,",
    "start": "207420",
    "end": "212110"
  },
  {
    "text": "which we're going to call h1.",
    "start": "212110",
    "end": "213900"
  },
  {
    "text": "So h1 is whether x1 minus x2\nis greater than or equal to 1.",
    "start": "213900",
    "end": "220297"
  },
  {
    "text": "And then, we're going to\nfind another subproblem,",
    "start": "220298",
    "end": "222340"
  },
  {
    "text": "testing whether car 2 is to\nthe far right of car 1, which",
    "start": "222340",
    "end": "225940"
  },
  {
    "text": "is called h2, that corresponds\nto this region over here.",
    "start": "225940",
    "end": "233200"
  },
  {
    "text": "And then, we're going to predict\nsafe if at least one of them",
    "start": "233200",
    "end": "236709"
  },
  {
    "text": "is true.",
    "start": "236710",
    "end": "237430"
  },
  {
    "text": "So we just add the two\nhere, which is either 1 or 0",
    "start": "237430",
    "end": "241180"
  },
  {
    "text": "and if at least\none of them is 1,",
    "start": "241180",
    "end": "244269"
  },
  {
    "text": "then we're going\nto return plus 1.",
    "start": "244270",
    "end": "248060"
  },
  {
    "text": "And by convention, we're going\nto assume that the sign of 0",
    "start": "248060",
    "end": "250670"
  },
  {
    "text": "is minus 1.",
    "start": "250670",
    "end": "254860"
  },
  {
    "text": "OK, so here are\nsome examples, here.",
    "start": "254860",
    "end": "258260"
  },
  {
    "text": "So suppose we have 0, 2, again.",
    "start": "258260",
    "end": "261049"
  },
  {
    "text": "So this point, h1 says,\nnope, that's not on my side.",
    "start": "261050",
    "end": "266020"
  },
  {
    "text": "h2 says, yep, that's on\nmy side and at least one",
    "start": "266020",
    "end": "269379"
  },
  {
    "text": "is enough to make the\nprediction plus 1.",
    "start": "269380",
    "end": "272710"
  },
  {
    "text": "If you take 2, 0, that's\nthis point, h1 says, yep.",
    "start": "272710",
    "end": "277900"
  },
  {
    "text": "h2 says, nope, and then, that is\n1, because all it takes is one.",
    "start": "277900",
    "end": "282759"
  },
  {
    "text": "0, 0 is this point, both of\nthem say, no, and it's minus 1.",
    "start": "282760",
    "end": "288520"
  },
  {
    "text": "And same with 2, 2,\nboth of them say, no.",
    "start": "288520",
    "end": "291819"
  },
  {
    "text": "It's minus.",
    "start": "291820",
    "end": "292405"
  },
  {
    "start": "292405",
    "end": "295170"
  },
  {
    "text": "OK, so so far, we've just\ndefined the true function f.",
    "start": "295170",
    "end": "301320"
  },
  {
    "text": "Of course, we don't know f,\nso what we're going to do",
    "start": "301320",
    "end": "304770"
  },
  {
    "text": "is try to move gradually to\ndefining a hypothesis class.",
    "start": "304770",
    "end": "310110"
  },
  {
    "start": "305000",
    "end": "452000"
  },
  {
    "text": "And the next step is to rewrite\nf using vector notation.",
    "start": "310110",
    "end": "316219"
  },
  {
    "text": "So here are the two\nintermediate subproblems.",
    "start": "316220",
    "end": "319000"
  },
  {
    "text": "And the predictor is\nf of x equals a sign.",
    "start": "319000",
    "end": "323570"
  },
  {
    "text": "And what we're going\nto do is to write",
    "start": "323570",
    "end": "325670"
  },
  {
    "text": "this in terms of a dot\nproduct between a weight",
    "start": "325670",
    "end": "329190"
  },
  {
    "text": "vector and a feature vector.",
    "start": "329190",
    "end": "331200"
  },
  {
    "text": "So here's a feature\nvector, 1, x1, x2.",
    "start": "331200",
    "end": "336520"
  },
  {
    "text": "And then, we're going to\ndefine a weight vector, which",
    "start": "336520",
    "end": "338770"
  },
  {
    "text": "is minus 1, and if you\nlook at the dot product,",
    "start": "338770",
    "end": "341780"
  },
  {
    "text": "it's going to be-- so it's\nminus 1, plus x1, minus x2.",
    "start": "341780",
    "end": "348010"
  },
  {
    "text": "And if that quantity\nis greater than 0,",
    "start": "348010",
    "end": "350680"
  },
  {
    "text": "then we're going to return\n1, otherwise, return 0.",
    "start": "350680",
    "end": "355870"
  },
  {
    "text": "And you can verify that.",
    "start": "355870",
    "end": "356979"
  },
  {
    "text": "This is exactly just a\nrewrite of this expression.",
    "start": "356980",
    "end": "361010"
  },
  {
    "text": "And similarly, if you reverse\nthe roles of x1 and x2,",
    "start": "361010",
    "end": "363680"
  },
  {
    "text": "then you can rewrite h2 in\nvector notation, as well.",
    "start": "363680",
    "end": "370275"
  },
  {
    "text": "And now what we're\ngoing to do is",
    "start": "370275",
    "end": "371650"
  },
  {
    "text": "we're going to just combine\nh1 and h2 by stacking them.",
    "start": "371650",
    "end": "377210"
  },
  {
    "text": "So we're going to define\nthis matrix, which",
    "start": "377210",
    "end": "379600"
  },
  {
    "text": "is just the two weight\nvectors here, stacked up.",
    "start": "379600",
    "end": "383050"
  },
  {
    "text": "So we have two rows, here.",
    "start": "383050",
    "end": "385659"
  },
  {
    "text": "And we're going to multiply this\nmatrix by the feature vector.",
    "start": "385660",
    "end": "390820"
  },
  {
    "text": "So remember, left\nmultiplication by a matrix",
    "start": "390820",
    "end": "393880"
  },
  {
    "text": "is just taking the\ndot product with each",
    "start": "393880",
    "end": "395980"
  },
  {
    "text": "of the rows of that matrix.",
    "start": "395980",
    "end": "398440"
  },
  {
    "text": "And now, this produces a\ntwo dimensional vector,",
    "start": "398440",
    "end": "402580"
  },
  {
    "text": "and we're going to test\nwhether each component is",
    "start": "402580",
    "end": "405250"
  },
  {
    "text": "greater than or equal to 0.",
    "start": "405250",
    "end": "407740"
  },
  {
    "text": "So in the end, h of x is\ngoing to be a two dimensional",
    "start": "407740",
    "end": "412569"
  },
  {
    "text": "vector, OK?",
    "start": "412570",
    "end": "415030"
  },
  {
    "text": "And now, given that we can\nrewrite the predictor as simply",
    "start": "415030",
    "end": "419230"
  },
  {
    "text": "the sign of the dot\nproduct between 1, 1 and h",
    "start": "419230",
    "end": "422320"
  },
  {
    "text": "of x, which is simply the\nsum of the two components.",
    "start": "422320",
    "end": "427610"
  },
  {
    "text": "So now, we've written f of x,\nwhich is the true function,",
    "start": "427610",
    "end": "431379"
  },
  {
    "text": "in terms of a bunch of\nmatrix or vector multipliers.",
    "start": "431380",
    "end": "438940"
  },
  {
    "text": "Now, everything in red\nhere, are just numbers.",
    "start": "438940",
    "end": "442270"
  },
  {
    "text": "And so far, we've\nspecified what they are,",
    "start": "442270",
    "end": "445039"
  },
  {
    "text": "but in general, we're\nnot going to know them,",
    "start": "445040",
    "end": "447640"
  },
  {
    "text": "and we're going to have\nto learn them from data.",
    "start": "447640",
    "end": "452000"
  },
  {
    "start": "452000",
    "end": "620000"
  },
  {
    "text": "But before we do\nthat, we're going",
    "start": "452000",
    "end": "454120"
  },
  {
    "text": "to preemptively see one problem\nthat's going to come up.",
    "start": "454120",
    "end": "458340"
  },
  {
    "text": "And this problem we saw\nbefore, when we tried",
    "start": "458340",
    "end": "460290"
  },
  {
    "text": "to optimize the zero-one loss.",
    "start": "460290",
    "end": "463000"
  },
  {
    "text": "So let's look at the gradient\nof h1 of x with respect to v1.",
    "start": "463000",
    "end": "468750"
  },
  {
    "text": "We can plot this as follows.",
    "start": "468750",
    "end": "470560"
  },
  {
    "text": "So here is the score z,\nwhich is the dot product.",
    "start": "470560",
    "end": "477490"
  },
  {
    "text": "And this is h1, and this\nis just a step function.",
    "start": "477490",
    "end": "482979"
  },
  {
    "text": "So the step function\nof threshold function",
    "start": "482980",
    "end": "484950"
  },
  {
    "text": "is just whether z\nis greater than 0.",
    "start": "484950",
    "end": "488460"
  },
  {
    "text": "It's 1 over here,\nand 0 over here.",
    "start": "488460",
    "end": "492229"
  },
  {
    "text": "OK, so now, if you tried to\ndo gradient descent on this,",
    "start": "492230",
    "end": "494990"
  },
  {
    "text": "you're just going to get stuck\nbecause the gradients are going",
    "start": "494990",
    "end": "497573"
  },
  {
    "text": "to be 0, basically everywhere.",
    "start": "497573",
    "end": "500350"
  },
  {
    "text": "So the solution is to replace\nthis threshold function",
    "start": "500350",
    "end": "503260"
  },
  {
    "text": "with a more general activation\nfunction, sigma, which",
    "start": "503260",
    "end": "507100"
  },
  {
    "text": "has more friendly gradients.",
    "start": "507100",
    "end": "510840"
  },
  {
    "text": "So classically, and by\nclassic I mean, like",
    "start": "510840",
    "end": "513570"
  },
  {
    "text": "in the '80s and '90s, people\nused the logistic function",
    "start": "513570",
    "end": "519450"
  },
  {
    "text": "as activation function,\nwhich looks like this.",
    "start": "519450",
    "end": "523820"
  },
  {
    "text": "And this is just a\nkind of a smoothed",
    "start": "523820",
    "end": "525730"
  },
  {
    "text": "out version of the\nthreshold function.",
    "start": "525730",
    "end": "528399"
  },
  {
    "text": "And in particular, its\ngradients are 0 nowhere,",
    "start": "528400",
    "end": "533140"
  },
  {
    "text": "so that's just great.",
    "start": "533140",
    "end": "534250"
  },
  {
    "text": "So the gradient, it can\nalways move making progress.",
    "start": "534250",
    "end": "537400"
  },
  {
    "text": "There is a caveat here, which\nis that if you look out here,",
    "start": "537400",
    "end": "541830"
  },
  {
    "text": "this function is\npretty flat, which",
    "start": "541830",
    "end": "543880"
  },
  {
    "text": "means that the gradient\nis actually approaching 0,",
    "start": "543880",
    "end": "546760"
  },
  {
    "text": "which means that if\nyou're out here, then,",
    "start": "546760",
    "end": "548680"
  },
  {
    "text": "you can get stuck, or at\nleast make very slow progress.",
    "start": "548680",
    "end": "554320"
  },
  {
    "text": "So in 2012, the\nReLU activation was",
    "start": "554320",
    "end": "558690"
  },
  {
    "text": "invented, which just\ntakes a max of z and 0,",
    "start": "558690",
    "end": "563310"
  },
  {
    "text": "so that looks like this.",
    "start": "563310",
    "end": "565390"
  },
  {
    "text": "So if the input to the\nReLU is less than 0,",
    "start": "565390",
    "end": "569970"
  },
  {
    "text": "I'm just going to\nkeep it, clip it 0.",
    "start": "569970",
    "end": "572639"
  },
  {
    "text": "And then, otherwise, I'm\ngoing to just leave it alone.",
    "start": "572640",
    "end": "576980"
  },
  {
    "text": "So now, this function actually\nhas nice gradients over here.",
    "start": "576980",
    "end": "581449"
  },
  {
    "text": "So the gradient never vanishes.",
    "start": "581450",
    "end": "583120"
  },
  {
    "text": "It's always positive\nand bound away from 0.",
    "start": "583120",
    "end": "588339"
  },
  {
    "text": "Although, over here, it is 0.",
    "start": "588340",
    "end": "591330"
  },
  {
    "text": "So it turns out empirically,\nthe ReLU activation function",
    "start": "591330",
    "end": "594300"
  },
  {
    "text": "works really well.",
    "start": "594300",
    "end": "595330"
  },
  {
    "text": "It's simpler in a lot of ways.",
    "start": "595330",
    "end": "596740"
  },
  {
    "text": "So it's become the activation\nfunction of choice, here.",
    "start": "596740",
    "end": "602350"
  },
  {
    "text": "So the solution here is to\nreplace this threshold step",
    "start": "602350",
    "end": "606779"
  },
  {
    "text": "function with an\nactivation function.",
    "start": "606780",
    "end": "609210"
  },
  {
    "text": "Choose your favorite, I\nwould choose the ReLU.",
    "start": "609210",
    "end": "611610"
  },
  {
    "text": "And now you have something that\nhas non-vanishing gradients.",
    "start": "611610",
    "end": "618000"
  },
  {
    "start": "618000",
    "end": "621500"
  },
  {
    "start": "620000",
    "end": "775000"
  },
  {
    "text": "So let's now define\ntwo-layer neural networks",
    "start": "621500",
    "end": "625640"
  },
  {
    "text": "using the machinery\nthat we've seen so far.",
    "start": "625640",
    "end": "628950"
  },
  {
    "text": "So we're going to define some\nintermediate subproblems.",
    "start": "628950",
    "end": "633350"
  },
  {
    "text": "So we start with a\nfeature vector, phi of x.",
    "start": "633350",
    "end": "636740"
  },
  {
    "text": "Now, I'm going to represent\nvectors and matrices",
    "start": "636740",
    "end": "639470"
  },
  {
    "text": "using these dots.",
    "start": "639470",
    "end": "642000"
  },
  {
    "text": "So this is a six\ndimensional feature vector,",
    "start": "642000",
    "end": "645470"
  },
  {
    "text": "but in general,\nit's d dimensional.",
    "start": "645470",
    "end": "648079"
  },
  {
    "text": "I'm going to next multiply it\nby this weight matrix, which",
    "start": "648080",
    "end": "651950"
  },
  {
    "text": "is going to be a 3 by 6, but\nin general, a k by d matrix.",
    "start": "651950",
    "end": "657500"
  },
  {
    "text": "And now, that generates a three\ndimensional or a k dimensional",
    "start": "657500",
    "end": "661970"
  },
  {
    "text": "vector.",
    "start": "661970",
    "end": "662990"
  },
  {
    "text": "I'm going to send it through\nthis nonlinearity activation",
    "start": "662990",
    "end": "666410"
  },
  {
    "text": "function, like the\nReLU or the logistic.",
    "start": "666410",
    "end": "668930"
  },
  {
    "text": "And I'm going to get a vector,\nwhich I'm going to call h of x.",
    "start": "668930",
    "end": "674740"
  },
  {
    "text": "OK, so now, given this h of\nx, I can now do prediction",
    "start": "674740",
    "end": "679120"
  },
  {
    "text": "by taking h of x and\nsimply dot producting it",
    "start": "679120",
    "end": "682960"
  },
  {
    "text": "with a weight vector, w.",
    "start": "682960",
    "end": "686350"
  },
  {
    "text": "And if I take the sign,\nthat gives me the prediction",
    "start": "686350",
    "end": "691389"
  },
  {
    "text": "of that neural network.",
    "start": "691390",
    "end": "694610"
  },
  {
    "text": "So one thing that's\ninteresting here",
    "start": "694610",
    "end": "696700"
  },
  {
    "text": "is that if you look\nat this equation,",
    "start": "696700",
    "end": "699080"
  },
  {
    "text": "it really pretty much\nlooks like the equation",
    "start": "699080",
    "end": "701440"
  },
  {
    "text": "for a linear classifier.",
    "start": "701440",
    "end": "703060"
  },
  {
    "text": "The only difference\nis now we have",
    "start": "703060",
    "end": "704500"
  },
  {
    "text": "h of x instead of phi of x.",
    "start": "704500",
    "end": "707930"
  },
  {
    "text": "So one way to interpret what\nneural networks are doing",
    "start": "707930",
    "end": "711080"
  },
  {
    "text": "is that instead of using\nthe original feature vector,",
    "start": "711080",
    "end": "715040"
  },
  {
    "text": "we've learned a\nsmarter representation.",
    "start": "715040",
    "end": "718977"
  },
  {
    "text": "And at the end of\nthe day, we're still",
    "start": "718977",
    "end": "720560"
  },
  {
    "text": "doing a linear\nclassification on top",
    "start": "720560",
    "end": "722690"
  },
  {
    "text": "of that feature representation.",
    "start": "722690",
    "end": "724628"
  },
  {
    "text": "So often people think\nabout neural networks",
    "start": "724628",
    "end": "726420"
  },
  {
    "text": "as doing feature learning\nfor precisely this reason.",
    "start": "726420",
    "end": "731149"
  },
  {
    "text": "And finally, now we can define\nthe hypothesis class, F, is",
    "start": "731150",
    "end": "735220"
  },
  {
    "text": "equal to the set\nof all predictors,",
    "start": "735220",
    "end": "738730"
  },
  {
    "text": "and the predictor is\ngiven, parameterized",
    "start": "738730",
    "end": "740980"
  },
  {
    "text": "by a weight matrix, V, and a\nweight vector, w, defined up",
    "start": "740980",
    "end": "747339"
  },
  {
    "text": "here.",
    "start": "747340",
    "end": "747970"
  },
  {
    "text": "And we can let the weight matrix\nbe any arbitrary k by d matrix.",
    "start": "747970",
    "end": "755170"
  },
  {
    "text": "And we let w be any\nd dimensional vector.",
    "start": "755170",
    "end": "760420"
  },
  {
    "text": "Sorry, this d should\nactually be a k, there.",
    "start": "760420",
    "end": "763029"
  },
  {
    "text": "I will fix that.",
    "start": "763030",
    "end": "763810"
  },
  {
    "start": "763810",
    "end": "766450"
  },
  {
    "text": "OK, we have to find a\nhypothesis class that",
    "start": "766450",
    "end": "769570"
  },
  {
    "text": "corresponds to two-layer neural\nnetworks for classification.",
    "start": "769570",
    "end": "775250"
  },
  {
    "start": "775000",
    "end": "885000"
  },
  {
    "text": "Now, we can kind of\npush this farther.",
    "start": "775250",
    "end": "778380"
  },
  {
    "text": "We can go and talk about\ndeep neural networks.",
    "start": "778380",
    "end": "780550"
  },
  {
    "text": "So remember, going\nback to single layer",
    "start": "780550",
    "end": "783029"
  },
  {
    "text": "neural networks, a.k.a.",
    "start": "783030",
    "end": "784470"
  },
  {
    "text": "linear predictors, we see that\nwe take the feature vector.",
    "start": "784470",
    "end": "788069"
  },
  {
    "text": "We take the dot product with\nrespect to a weight vector.",
    "start": "788070",
    "end": "791280"
  },
  {
    "text": "And you get the score, which\ncan be used to drive prediction",
    "start": "791280",
    "end": "794730"
  },
  {
    "text": "directly in the regression\nor take the sign to get",
    "start": "794730",
    "end": "797250"
  },
  {
    "text": "classification predictions.",
    "start": "797250",
    "end": "800370"
  },
  {
    "text": "For two-layer neural\nnetworks, we take phi of x.",
    "start": "800370",
    "end": "803010"
  },
  {
    "text": "We take the dot product between\nlayer one's weight matrix,",
    "start": "803010",
    "end": "807930"
  },
  {
    "text": "take element wise\nactivation function.",
    "start": "807930",
    "end": "811350"
  },
  {
    "text": "And then, multiply the dot\nproduct with a weight vector.",
    "start": "811350",
    "end": "814899"
  },
  {
    "text": "You get the score.",
    "start": "814900",
    "end": "816730"
  },
  {
    "text": "And now, the key thing is this\npiece, V, apply V and then,",
    "start": "816730",
    "end": "822910"
  },
  {
    "text": "apply sigma, you can just\niterate over and over again.",
    "start": "822910",
    "end": "826970"
  },
  {
    "text": "So here's a three-layer\nneural network,",
    "start": "826970",
    "end": "829569"
  },
  {
    "text": "take phi of x, which\nis a feature vector.",
    "start": "829570",
    "end": "832390"
  },
  {
    "text": "You multiply by some matrix,\nV1, take a nonlinearity,",
    "start": "832390",
    "end": "837040"
  },
  {
    "text": "multiply by another matrix,\ntake a nonlinearity.",
    "start": "837040",
    "end": "840639"
  },
  {
    "text": "And then, finally,\nyou get some vector",
    "start": "840640",
    "end": "843760"
  },
  {
    "text": "that you take the\ndot product with w,",
    "start": "843760",
    "end": "846502"
  },
  {
    "text": "and you get the\nscore, which can be",
    "start": "846502",
    "end": "847960"
  },
  {
    "text": "used to power your predictions.",
    "start": "847960",
    "end": "851590"
  },
  {
    "text": "So one small note is that I've\nleft out all the bias terms",
    "start": "851590",
    "end": "855540"
  },
  {
    "text": "for notational simplicity.",
    "start": "855540",
    "end": "856829"
  },
  {
    "text": "In practice, you\nwould have bias terms.",
    "start": "856830",
    "end": "861780"
  },
  {
    "text": "And you can imagine just\niterating this over and over",
    "start": "861780",
    "end": "865320"
  },
  {
    "text": "again.",
    "start": "865320",
    "end": "866640"
  },
  {
    "text": "But what is this doing?",
    "start": "866640",
    "end": "868570"
  },
  {
    "text": "It kind of looks like a little\nbit of abstract nonsense.",
    "start": "868570",
    "end": "871302"
  },
  {
    "text": "You're just\nmultiplying by matrices",
    "start": "871302",
    "end": "872760"
  },
  {
    "text": "and sending them\nthrough a nonlinearity,",
    "start": "872760",
    "end": "874680"
  },
  {
    "text": "and you hope something\ngood happens.",
    "start": "874680",
    "end": "876600"
  },
  {
    "text": "And that's not completely false,\nbut there are some intuitions",
    "start": "876600",
    "end": "882899"
  },
  {
    "text": "which we can derive.",
    "start": "882900",
    "end": "885480"
  },
  {
    "start": "885000",
    "end": "941000"
  },
  {
    "text": "So one intuition is\nthinking about layers",
    "start": "885480",
    "end": "888089"
  },
  {
    "text": "as representing multiple\nlevels of abstraction.",
    "start": "888090",
    "end": "891810"
  },
  {
    "text": "So in computer vision, let's\nsay the input is an image.",
    "start": "891810",
    "end": "897950"
  },
  {
    "text": "So you can think\nabout the first layer",
    "start": "897950",
    "end": "899710"
  },
  {
    "text": "as computing some sort\nof notion of edges.",
    "start": "899710",
    "end": "903640"
  },
  {
    "text": "And the second layer when\nyou multiply a matrix,",
    "start": "903640",
    "end": "905770"
  },
  {
    "text": "and you take a\nnonlinearity, you compute",
    "start": "905770",
    "end": "908080"
  },
  {
    "text": "some notion of object parts.",
    "start": "908080",
    "end": "910540"
  },
  {
    "text": "And then that third layer,\nyou multiply by a matrix",
    "start": "910540",
    "end": "916320"
  },
  {
    "text": "and then apply\nsome nonlinearity,",
    "start": "916320",
    "end": "917790"
  },
  {
    "text": "you get some notion of objects.",
    "start": "917790",
    "end": "921399"
  },
  {
    "text": "Now this is kind\nof just a story,",
    "start": "921400",
    "end": "923670"
  },
  {
    "text": "and we haven't talked\nat all about learning.",
    "start": "923670",
    "end": "926170"
  },
  {
    "text": "So this is definitely not true\nfor all of neural networks.",
    "start": "926170",
    "end": "929070"
  },
  {
    "text": "It turns out that\nwhen you actually",
    "start": "929070",
    "end": "931560"
  },
  {
    "text": "learn a network to data, and you\nvisualize what the weights are,",
    "start": "931560",
    "end": "935010"
  },
  {
    "text": "you actually do get some\ninterpretable results, which",
    "start": "935010",
    "end": "937530"
  },
  {
    "text": "is kind of interesting\nand somewhat surprising.",
    "start": "937530",
    "end": "942930"
  },
  {
    "start": "941000",
    "end": "1042000"
  },
  {
    "text": "So now, there's a\nquestion of depth.",
    "start": "942930",
    "end": "946180"
  },
  {
    "text": "So the fact that you take\na feature vector and you",
    "start": "946180",
    "end": "948750"
  },
  {
    "text": "apply some sort\nof transformation",
    "start": "948750",
    "end": "953040"
  },
  {
    "text": "again, and again, and\nagain to get a score.",
    "start": "953040",
    "end": "956160"
  },
  {
    "text": "So why do we do this?",
    "start": "956160",
    "end": "958060"
  },
  {
    "text": "So one intuition that\nwe talked about already",
    "start": "958060",
    "end": "960210"
  },
  {
    "text": "is this is representing\ndifferent levels",
    "start": "960210",
    "end": "962640"
  },
  {
    "text": "of abstraction to kind\nof low level pixels,",
    "start": "962640",
    "end": "965820"
  },
  {
    "text": "to high level object\nparts and objects.",
    "start": "965820",
    "end": "969518"
  },
  {
    "text": "Another way to\nthink about this is",
    "start": "969518",
    "end": "970935"
  },
  {
    "text": "this is performing multiple\nsteps of computation.",
    "start": "970935",
    "end": "973920"
  },
  {
    "text": "Just like in a\nclassic program if you",
    "start": "973920",
    "end": "975690"
  },
  {
    "text": "get more steps of\ncomputation, it gives you",
    "start": "975690",
    "end": "978450"
  },
  {
    "text": "more expressive power.",
    "start": "978450",
    "end": "979710"
  },
  {
    "text": "You can do more things.",
    "start": "979710",
    "end": "981420"
  },
  {
    "text": "You can think about\neach of these operations",
    "start": "981420",
    "end": "984000"
  },
  {
    "text": "as simply doing some compute.",
    "start": "984000",
    "end": "987300"
  },
  {
    "text": "Now, it's maybe a kind of\na foreign type of compute",
    "start": "987300",
    "end": "990149"
  },
  {
    "text": "because you're multiplying\nby a crazy unknown matrix.",
    "start": "990150",
    "end": "993670"
  },
  {
    "text": "But one way we can\nthink about this",
    "start": "993670",
    "end": "996269"
  },
  {
    "text": "is that you set up\nthis computation",
    "start": "996270",
    "end": "999028"
  },
  {
    "text": "and the learning algorithm\nis going to figure out",
    "start": "999028",
    "end": "1001070"
  },
  {
    "text": "what kind of\ncomputation makes sense",
    "start": "1001070",
    "end": "1003260"
  },
  {
    "text": "for making the best prediction.",
    "start": "1003260",
    "end": "1006698"
  },
  {
    "text": "Another piece of intuition\nis that empirically it",
    "start": "1006698",
    "end": "1008740"
  },
  {
    "text": "just happens to\nwork really well,",
    "start": "1008740",
    "end": "1011089"
  },
  {
    "text": "which is not to be understated.",
    "start": "1011090",
    "end": "1014320"
  },
  {
    "text": "If you're looking for a\nmore theoretical reason,",
    "start": "1014320",
    "end": "1020830"
  },
  {
    "text": "the jury's kind of\nstill out on this.",
    "start": "1020830",
    "end": "1023140"
  },
  {
    "text": "You can have intuitions how\ndeeper logical circuits can",
    "start": "1023140",
    "end": "1029560"
  },
  {
    "text": "capture more than\nshallower ones,",
    "start": "1029560",
    "end": "1031630"
  },
  {
    "text": "but then, there's the kind of\nrelationship between circuits",
    "start": "1031630",
    "end": "1034540"
  },
  {
    "text": "and neural networks,\nwhich requires",
    "start": "1034540",
    "end": "1036636"
  },
  {
    "text": "a little bit of massaging.",
    "start": "1036637",
    "end": "1037720"
  },
  {
    "text": "So this is still kind of a\npretty active area of research.",
    "start": "1037720",
    "end": "1040928"
  },
  {
    "start": "1040929",
    "end": "1043760"
  },
  {
    "start": "1042000",
    "end": "1115000"
  },
  {
    "text": "To summarize, we start out with\na very toy problem, the XOR",
    "start": "1043760",
    "end": "1047500"
  },
  {
    "text": "problem, testing whether two\ncars are going to collide",
    "start": "1047500",
    "end": "1050020"
  },
  {
    "text": "or not.",
    "start": "1050020",
    "end": "1050770"
  },
  {
    "text": "And we used it to motivate\nproblem decomposition",
    "start": "1050770",
    "end": "1053950"
  },
  {
    "text": "and eventually, defining\nneural networks.",
    "start": "1053950",
    "end": "1059470"
  },
  {
    "text": "We saw that intuitively\nneural networks",
    "start": "1059470",
    "end": "1063770"
  },
  {
    "text": "allow you to define\nnonlinear predictors,",
    "start": "1063770",
    "end": "1066200"
  },
  {
    "text": "but in a particular way.",
    "start": "1066200",
    "end": "1069320"
  },
  {
    "text": "And the way is to decompose\nthe original problem",
    "start": "1069320",
    "end": "1072019"
  },
  {
    "text": "into intermediate\nsubproblems, testing",
    "start": "1072020",
    "end": "1074900"
  },
  {
    "text": "if the car is to the far\nright or the far left",
    "start": "1074900",
    "end": "1077570"
  },
  {
    "text": "and then combining\nthem over time.",
    "start": "1077570",
    "end": "1080789"
  },
  {
    "text": "And you can kind of\ntake this idea further",
    "start": "1080790",
    "end": "1083000"
  },
  {
    "text": "and iterate on\nthis decomposition",
    "start": "1083000",
    "end": "1084800"
  },
  {
    "text": "multiple times giving\nrise to multiple levels",
    "start": "1084800",
    "end": "1087380"
  },
  {
    "text": "of abstraction, multiple\nsteps of computation.",
    "start": "1087380",
    "end": "1091930"
  },
  {
    "text": "The hypothesis\nclass is now larger.",
    "start": "1091930",
    "end": "1095770"
  },
  {
    "text": "It contains all predictors where\nthe weights of all the layers",
    "start": "1095770",
    "end": "1099910"
  },
  {
    "text": "can vary freely.",
    "start": "1099910",
    "end": "1103342"
  },
  {
    "text": "And then next up,\nwe're going to show you",
    "start": "1103342",
    "end": "1105050"
  },
  {
    "text": "how to actually learn the\nweights of a neural network.",
    "start": "1105050",
    "end": "1108680"
  },
  {
    "text": "That is the end.",
    "start": "1108680",
    "end": "1110710"
  },
  {
    "start": "1110710",
    "end": "1115000"
  }
]