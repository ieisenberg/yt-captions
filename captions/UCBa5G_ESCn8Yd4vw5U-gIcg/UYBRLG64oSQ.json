[
  {
    "start": "0",
    "end": "5640"
  },
  {
    "text": "OK. I guess let's get started. This is the last\nlecture of this course.",
    "start": "5640",
    "end": "11389"
  },
  {
    "text": "I guess we're going to continue\nwith the spectral approach",
    "start": "11390",
    "end": "16640"
  },
  {
    "text": "for clustering. So I'll provide some of the\nreviews of the last lectures.",
    "start": "16640",
    "end": "23000"
  },
  {
    "text": " So last lecture, I think we\ndid the stochastic block model,",
    "start": "23000",
    "end": "29480"
  },
  {
    "text": "and one of the main\nfindings is that if you",
    "start": "29480",
    "end": "34850"
  },
  {
    "text": "do eigendecomposition-- so our goal was to do\neigendecomposition on the graph",
    "start": "34850",
    "end": "40620"
  },
  {
    "text": "G from the stochastic\nblock model. And we have shown that if\nyou do eigendecomposition",
    "start": "40620",
    "end": "46609"
  },
  {
    "text": "on the average graph G, the\nexpectation of G, then does give the heading community\nS and S bar, right?",
    "start": "46610",
    "end": "54290"
  },
  {
    "text": "I think last time, we showed\nthat the second eigenvector is something called\nu, which will look",
    "start": "54290",
    "end": "60410"
  },
  {
    "text": "like 1 1 1 and minus 1 minus 1. And this is S and this is S bar.",
    "start": "60410",
    "end": "66920"
  },
  {
    "text": "So basically, if you just\ntake the second eigenvector of the expecting graph G, then\nyou get the hidden community.",
    "start": "66920",
    "end": "76820"
  },
  {
    "text": "And we have argued\nthat suffices to show that the graph G and the\nexpectation graph, expectation",
    "start": "76820",
    "end": "83445"
  },
  {
    "text": "G are closing up in\nan operator norm. And this is because if you\nconsider this equation, right,",
    "start": "83445",
    "end": "91690"
  },
  {
    "text": "so you subtract the\nfirst eigenvalue from G, then what you get is that G\nminus the first eigen component",
    "start": "91690",
    "end": "100560"
  },
  {
    "text": "is equal to this perturbation\nmatrix plus the contribution of the second eigenvector.",
    "start": "100560",
    "end": "105930"
  },
  {
    "text": "And if you take the\neigendecomposition of this matrix,\nwhich is something you can compute easily, then\nyou take the top eigenvector",
    "start": "105930",
    "end": "115110"
  },
  {
    "text": "of the left hand side\nof this equation, then you are expected to\nfind something close to u,",
    "start": "115110",
    "end": "125100"
  },
  {
    "text": "as long as G minus expectation\nG is something small. Now how small is it? I didn't really\nformally do this,",
    "start": "125100",
    "end": "131470"
  },
  {
    "text": "but essentially, you need this\nperturbation to be much smaller than the signal, right?",
    "start": "131470",
    "end": "137290"
  },
  {
    "text": "So you need a perturbation\nin operating norm much smaller than the rank\n1 signal in operator norm.",
    "start": "137290",
    "end": "142560"
  },
  {
    "text": "And you can compute operator\nnorm for the runtime signal very easily, which is sometimes\np minus q over 2 times n.",
    "start": "142560",
    "end": "150060"
  },
  {
    "text": "So basically, we\nare trying to show that the concentration,\nright, this is a concentration inequality\nbecause you are trying",
    "start": "150060",
    "end": "156600"
  },
  {
    "text": "to prove that G concentrates\naround expectation of G in this factor norm sense.",
    "start": "156600",
    "end": "163196"
  },
  {
    "text": "I love to show this proof. This is a little\ntechnical proof, but the proof is not very long\nand also it kind of relates",
    "start": "163197",
    "end": "168540"
  },
  {
    "text": "back to what we discussed\nin lecture 3 or 4 where I guess you\nprobably remember",
    "start": "168540",
    "end": "173909"
  },
  {
    "text": "that I said that this\nconcentration inequality is probably one of the most\nimportant thing for this course because this is the--",
    "start": "173910",
    "end": "181110"
  },
  {
    "text": "that if you pick one technical\ntool in statistical machine learning, I think is probably\nconcentration inequality",
    "start": "181110",
    "end": "186390"
  },
  {
    "text": "in my own opinion. So it's probably\nuseful to just review why the concentration\ninequality can help",
    "start": "186390",
    "end": "193350"
  },
  {
    "text": "us to do something like this. So I'll give a proof for this.",
    "start": "193350",
    "end": "198630"
  },
  {
    "text": "So the proof look like-- So we're going to prove that-- ",
    "start": "198630",
    "end": "210640"
  },
  {
    "text": "so our lemma is that\nwith high probability,",
    "start": "210640",
    "end": "217480"
  },
  {
    "text": "G minus expectation\nG in operator norm is less than square root n\nlog n up to a constant factor.",
    "start": "217480",
    "end": "226720"
  },
  {
    "text": " And the first side is\nthis is not exactly",
    "start": "226720",
    "end": "233200"
  },
  {
    "text": "the type of\nconcentration inequality we have talked about before\nbecause before, we are talking about scalars, right?",
    "start": "233200",
    "end": "239990"
  },
  {
    "text": "So we are saying that if\nyou have expectation of-- if some random variables\nof some empirical samples",
    "start": "239990",
    "end": "245650"
  },
  {
    "text": "and the empirical\naverage concentrates around the population\non average.",
    "start": "245650",
    "end": "254239"
  },
  {
    "text": "So here it's a little bit\ndifferent because G is a matrix and the expectation\nof G is also a matrix.",
    "start": "254240",
    "end": "259838"
  },
  {
    "text": "So you are doing some kind\nof matrix concentration to some extent. And your measure\nof the similarity",
    "start": "259839",
    "end": "266800"
  },
  {
    "text": "is not just the absolute\nvalue in the difference of-- the absolute value\nof the difference, but it's about something\nlike the operator",
    "start": "266800",
    "end": "272810"
  },
  {
    "text": "norm of the differences\nof the matrices. ",
    "start": "272810",
    "end": "281260"
  },
  {
    "text": "However, you can actually\nturn this into something that we are familiar\nwith very easily.",
    "start": "281260",
    "end": "287120"
  },
  {
    "text": "So what you do is the following. This is still uniform\nconvergence as you will see.",
    "start": "287120",
    "end": "294870"
  },
  {
    "text": "That's the main idea. And why is this the case? This is because you can easily\ninterpret operator norms",
    "start": "294870",
    "end": "301580"
  },
  {
    "text": "as follows. So G minus expectation\nG, operator norm, this",
    "start": "301580",
    "end": "308180"
  },
  {
    "text": "is equals to the max over v. Let\nme write it down and explain.",
    "start": "308180",
    "end": "317389"
  },
  {
    "start": "317390",
    "end": "325965"
  },
  {
    "text": "This is just because\nthe operator norm in a persymmetric matrix. I think the definition is if you\nhave a symmetric matrix A, then",
    "start": "325965",
    "end": "338930"
  },
  {
    "text": "the operator norm-- I guess there's\nabsolute value here. The operator norm of\nmatrix A is exactly",
    "start": "338930",
    "end": "345920"
  },
  {
    "text": "equal to the maximum\nquadrative form that you can achieve by hitting\nit with a norm 1 vector.",
    "start": "345920",
    "end": "353840"
  },
  {
    "text": " And once you do this, you see\nthat this becomes a scalar now",
    "start": "353840",
    "end": "359930"
  },
  {
    "text": "because this\nquantity is a scalar. And you can decompose this\ninto max v2 norm square,",
    "start": "359930",
    "end": "368560"
  },
  {
    "text": "and then you get v transpose Gv\nminus v transpose expectation",
    "start": "368560",
    "end": "373639"
  },
  {
    "text": "Gv.  And this is a sum--",
    "start": "373640",
    "end": "379169"
  },
  {
    "text": "So what is this? Maybe let me write\ndown more explicitly. This is max.",
    "start": "379170",
    "end": "384960"
  },
  {
    "text": " This is sum of vi and\nvj, Gij, both ie and in",
    "start": "384960",
    "end": "401680"
  },
  {
    "text": "minus the expectation\nof this random variable. ",
    "start": "401680",
    "end": "411990"
  },
  {
    "text": "And now this becomes a sum of\nindependent random variables,",
    "start": "411990",
    "end": "419990"
  },
  {
    "text": "and this becomes the\nexpectation of this sum of independent random variables.",
    "start": "419990",
    "end": "427018"
  },
  {
    "text": "So now you can use\nthe concentratoin. If you don't have the max, you\ncan use concentration points. But this is what exactly\nHoeffding inequality is for.",
    "start": "427018",
    "end": "434650"
  },
  {
    "text": "And how do you\ndeal with the max? Then the max, this will be the\npart about uniform convergence.",
    "start": "434650",
    "end": "443410"
  },
  {
    "text": "Recall that the whole point\nof this uniform convergence is that if you fix the\nparameter-- suppose you",
    "start": "443410",
    "end": "450460"
  },
  {
    "text": "think of me as the parameter. So the point of\nuniform convergence is that you can fix\nthe parameter you",
    "start": "450460",
    "end": "455962"
  },
  {
    "text": "can use Hoeffding inequality\nto prove the concentration, to prove that empirical is not\nvery far away from population.",
    "start": "455962",
    "end": "463150"
  },
  {
    "text": "And the challenge of\nuniform convergence is about how do\nyou take the max, and here you still have a max.",
    "start": "463150",
    "end": "468310"
  },
  {
    "text": " So I guess there\nare multiple ways to deal with this concentration.",
    "start": "468310",
    "end": "474069"
  },
  {
    "text": "Of course, the easiest way\nis probably just invoke some existing theorem. There are some theorems\nin the literature as well.",
    "start": "474070",
    "end": "480723"
  },
  {
    "text": "But if you want\nto do it yourself, I guess there are two ways. So one way is that you can\nuse the Radamacher complexity",
    "start": "480723",
    "end": "488780"
  },
  {
    "text": "machinery. ",
    "start": "488780",
    "end": "494469"
  },
  {
    "text": "The Radamacher\ncomplexity machinery. ",
    "start": "494470",
    "end": "500012"
  },
  {
    "text": "I guess it's probably\na while back. We discussed this\nprobably five weeks ago. And I think one\nof the techniques",
    "start": "500012",
    "end": "506710"
  },
  {
    "text": "is that you do symmetrization.  So so far, this is\nnot a symmetrical form",
    "start": "506710",
    "end": "512979"
  },
  {
    "text": "and you introduce some\nRadamacher variable and you symmetrize\nit, and then you can proceed with all the random.",
    "start": "512980",
    "end": "518140"
  },
  {
    "text": "You can essentially view this\nas a Radamacher complexity of some function class.",
    "start": "518140",
    "end": "523498"
  },
  {
    "text": "So with that, I\nthink that's actually a pretty clean and nice way. I'm going to leave this. If you're interested,\nyou can do it yourself.",
    "start": "523498",
    "end": "530530"
  },
  {
    "text": "I believe it's not\nvery difficult. What I will show\nhere is that I'm going to show us even more\nbrute force methods which",
    "start": "530530",
    "end": "538480"
  },
  {
    "text": "use actually the first\ntechnique we introduced in our class, the brute\nforce discretization.",
    "start": "538480",
    "end": "547540"
  },
  {
    "text": "Recall that before we talk\nabout Radamacher complexity, we said that in\nmany cases, actually you can just deal with\nthe uniform convergence",
    "start": "547540",
    "end": "555519"
  },
  {
    "text": "for continuous function\nclass with a very simple discretization.",
    "start": "555520",
    "end": "562520"
  },
  {
    "text": " So what we do here is\ngoing to be just that you--",
    "start": "562520",
    "end": "568899"
  },
  {
    "text": " for fixed v with 2 norm 1.",
    "start": "568900",
    "end": "579410"
  },
  {
    "text": "We can use Hoeffding inequality. It's Hoeffding inequality. ",
    "start": "579410",
    "end": "589900"
  },
  {
    "text": "So what you've got is\nthat with probability, at most exponential minus\nepsilon squared over 2.",
    "start": "589900",
    "end": "602140"
  },
  {
    "text": "I'm not expecting you\nto check it on the fly, but you can just basically plug\nin the Hoeffding inequality",
    "start": "602140",
    "end": "609340"
  },
  {
    "text": "without any modification. It can be vjGij is close\nto the expectation.",
    "start": "609340",
    "end": "617440"
  },
  {
    "start": "617440",
    "end": "624030"
  },
  {
    "text": "The probability that it\ndeviates from expectation is at most exponential minus\nepsilon squared over 2.",
    "start": "624030",
    "end": "629810"
  },
  {
    "text": "And then you take epsilon to\nbe something like O square root",
    "start": "629810",
    "end": "635800"
  },
  {
    "text": "n log n. So their failure\nprobability, this means that exponential minus\nepsilon square over n.",
    "start": "635800",
    "end": "642280"
  },
  {
    "text": "This is something like\nexponential minus O n log n.",
    "start": "642280",
    "end": "647740"
  },
  {
    "text": "This is a pretty small\nfailure probability. And then you take a\ndiscretization of the unit ball",
    "start": "647740",
    "end": "666660"
  },
  {
    "text": "with granularity, something\nlike sum of 1 over poly n.",
    "start": "666660",
    "end": "680810"
  },
  {
    "text": "This is what we did-- it's a long time\nago, I know, but I think this is what we\ndid in lecture 3 I think.",
    "start": "680810",
    "end": "687560"
  },
  {
    "text": "You take a very, very precise--",
    "start": "687560",
    "end": "693140"
  },
  {
    "text": "you use a very\nsmall granularity. But it doesn't really matter\nbecause at the end of the day, the dependency on\nthe granularity",
    "start": "693140",
    "end": "699470"
  },
  {
    "text": "is only logarithmic. So the size of this cover\nis exponential n log n.",
    "start": "699470",
    "end": "711480"
  },
  {
    "text": "And then you can\ntake a union bound",
    "start": "711480",
    "end": "718670"
  },
  {
    "text": "over this discretized set. ",
    "start": "718670",
    "end": "726240"
  },
  {
    "text": "And then because your\ngranularity is very small, it's only inverse\npoly so you only",
    "start": "726240",
    "end": "731430"
  },
  {
    "text": "lose the inverse poly\nand inverse is smaller than any of the inequalities. So then basically\neventually you've",
    "start": "731430",
    "end": "737400"
  },
  {
    "text": "got that-- it's a union bound\nwe got with high probability. We have this it is less than\nepsilon, which is chosen",
    "start": "737400",
    "end": "753240"
  },
  {
    "text": "to be square root n log n. ",
    "start": "753240",
    "end": "758760"
  },
  {
    "text": "I'm skipping a lot of details\nbecause I think today we don't have a lot of time to\ncomplete all the materials,",
    "start": "758760",
    "end": "765870"
  },
  {
    "text": "so I'm making it a little brief. But I think you kind\nof get the rough point.",
    "start": "765870",
    "end": "771990"
  },
  {
    "text": "It will take too much\ntime to work out details. And I kind of like\nthis method 2.",
    "start": "771990",
    "end": "778170"
  },
  {
    "text": "If I were to say my preference\nis between these methods, sometimes I like the\nmethod 2 because you",
    "start": "778170",
    "end": "784380"
  },
  {
    "text": "can do this very\nquickly yourself and you know exactly where\nthe dependency comes from. And if you do the\nRadamacher complexity,",
    "start": "784380",
    "end": "790800"
  },
  {
    "text": "it will be much cleaner. You will get better constants,\nyou'll get cleaner proofs. But sometimes it's a\nlittle bit less transparent",
    "start": "790800",
    "end": "797010"
  },
  {
    "text": "because you have to go\nthrough this whole machinery. ",
    "start": "797010",
    "end": "802279"
  },
  {
    "text": "And why this is useful. This is useful because now\nwe got this lemma, right?",
    "start": "802280",
    "end": "808110"
  },
  {
    "text": "So lemma is the G\nand E. A case of G is only different on the\norder of square root n.",
    "start": "808110",
    "end": "815332"
  },
  {
    "text": "And you can compare\nthat with the signal. ",
    "start": "815332",
    "end": "820860"
  },
  {
    "text": "So now compare another level,\nwhich is O square root n log n versus the signal level,\nwhich is p over q times",
    "start": "820860",
    "end": "828630"
  },
  {
    "text": "n and p minus q over 2 times n. So then this means that if p\nminus q is much bigger than 1",
    "start": "828630",
    "end": "838790"
  },
  {
    "text": "over square root of n,\nthen I recover the vector u",
    "start": "838790",
    "end": "847649"
  },
  {
    "text": "approximately.  So we can see that\nyou only need p and q",
    "start": "847650",
    "end": "854290"
  },
  {
    "text": "to have some separation but\nnot a lot of separation. And the separation\ndepends on the size of the graph, which also makes\nsome sense because the more",
    "start": "854290",
    "end": "862120"
  },
  {
    "text": "vertices you see, the clearer\nthe structure is in some sense. You have more kind of-- ",
    "start": "862120",
    "end": "868660"
  },
  {
    "text": "suppose you just\nthese two users, everything's kind of\ntwo randoms and you could tell which one is\nfrom which community.",
    "start": "868660",
    "end": "874300"
  },
  {
    "text": "But if you see a\nmillion users, you can use a lot of different\nusers to crossvalidate in some sense [INAUDIBLE]\nwe have the two communities.",
    "start": "874300",
    "end": "883670"
  },
  {
    "text": "All right. So I guess this concludes the\nstochastic block model part.",
    "start": "883670",
    "end": "890690"
  },
  {
    "text": "I guess there are some\nother small remarks which are not super important. So you can also actually can\nrecover the exact community",
    "start": "890690",
    "end": "910590"
  },
  {
    "text": "by some post-processing.  So here, what I showed\nis that you only",
    "start": "910590",
    "end": "916800"
  },
  {
    "text": "can recover the vector\nu approximately, but actually you\ncan post-process to get the exact community\nand their setting conditions.",
    "start": "916800",
    "end": "923970"
  },
  {
    "text": "I think under the conditions\nthat I'm giving here, you can do it. And actually because\nthis is a very precise",
    "start": "923970",
    "end": "931830"
  },
  {
    "text": "mathematical structure here-- so there are a lot of works\nin the literature on this.",
    "start": "931830",
    "end": "937650"
  },
  {
    "text": "And you can actually get\neven the exact constant here. So here I am writing\np minus q is larger than 1 over square root n.",
    "start": "937650",
    "end": "943363"
  },
  {
    "text": "So it's definitely very loose. You can get the\nprecise dependencies",
    "start": "943363",
    "end": "949620"
  },
  {
    "text": "that you need to recover and you\ncan have the precise threshold. Below that threshold you\ncannot recover anything,",
    "start": "949620",
    "end": "954870"
  },
  {
    "text": "above that threshold you\ncan recover something, and above another threshold\nyou can recover exactly.",
    "start": "954870",
    "end": "960090"
  },
  {
    "text": "So all of these are\nin the literature if you are interested. And you can extend this to\nmultiple blocks and so forth.",
    "start": "960090",
    "end": "966750"
  },
  {
    "text": "OK. So this concludes with the\nstochastic block model. And now I'm going to move\non to another kind of,",
    "start": "966750",
    "end": "977009"
  },
  {
    "text": "in my opinion, pretty\nimportant literature, which is about clustering\nthe worst-case graph. ",
    "start": "977010",
    "end": "983940"
  },
  {
    "text": "And still the thing is that\nif you do eigendecomposition, you are going to recover\nsome approximate structures",
    "start": "983940",
    "end": "991770"
  },
  {
    "text": "in the graph. So we are still going to\nuse eigendecomposition, but the analysis\nwill be different because here we don't have the\nstochasticity from the graph.",
    "start": "991770",
    "end": "999480"
  },
  {
    "text": " And because you have\na worst-case graph, you have to also\nsomehow define what",
    "start": "999480",
    "end": "1006225"
  },
  {
    "text": "you mean by the\nhidden community, right, because before,\nin the stochastic graph, you start with community\nand you generate a graph.",
    "start": "1006225",
    "end": "1012650"
  },
  {
    "text": "And now you are just\nscaling the graph. The graph is just\nsome aggregates. You have to say what you\nare trying to recover.",
    "start": "1012650",
    "end": "1019129"
  },
  {
    "text": "So let's start with\nthat, what's our goal? So this requires us\nto offer definitions.",
    "start": "1019130",
    "end": "1024810"
  },
  {
    "text": "So let's say given a graph G\nand the vertices is called E",
    "start": "1024810",
    "end": "1030369"
  },
  {
    "text": "and edges is called E-- sorry, vertices is\ncalled V and edges is called E. So let's define\nthis so-called conductance.",
    "start": "1030369",
    "end": "1038920"
  },
  {
    "text": "This is actually a\npretty important notion which shows up in many\ndifferent areas of math.",
    "start": "1038920",
    "end": "1045819"
  },
  {
    "text": "So of course it's\na different form. So here it's a vertical\nof a graph and edges.",
    "start": "1045819",
    "end": "1051880"
  },
  {
    "text": "In other cases, you\ncan define conductance in high-dimensional space as\nwell, which are essentially",
    "start": "1051880",
    "end": "1057130"
  },
  {
    "text": "the same definition,\nbut it could look a little bit different. So the conductance for graph--\nso suppos you have a cut,",
    "start": "1057130",
    "end": "1066190"
  },
  {
    "text": "let's call it S and S bar. You cut the graph into\ntwo parts, S and S bar. And the conductance of S is\ndefined to be the following.",
    "start": "1066190",
    "end": "1074480"
  },
  {
    "text": "So you have the\nnumber of edges, which are S and S bar, over the\nvolume of S. Let's define",
    "start": "1074480",
    "end": "1083200"
  },
  {
    "text": "both of this more. Clearly so E S S bar,\nthis is the total number",
    "start": "1083200",
    "end": "1089290"
  },
  {
    "text": "of edges from S to S bar.",
    "start": "1089290",
    "end": "1098410"
  },
  {
    "text": "But this is an undirected graph. Maybe I should call it between\nS and S bar to be precise.",
    "start": "1098410",
    "end": "1107990"
  },
  {
    "text": " Mathematically, this\nis really the sum",
    "start": "1107990",
    "end": "1114030"
  },
  {
    "text": "of i over iSj and S bar Gij.",
    "start": "1114030",
    "end": "1121140"
  },
  {
    "text": "If I use Gij as\nadjacency matrix-- I'm overusing the\nallocation a little bit.",
    "start": "1121140",
    "end": "1128130"
  },
  {
    "text": "Both are given on the\ngraph, and also this is matrix of the graph.",
    "start": "1128130",
    "end": "1133810"
  },
  {
    "text": "And the volume of S,\nthis is the total number",
    "start": "1133810",
    "end": "1146260"
  },
  {
    "text": "of edges connecting to S.\nWhich means that you look",
    "start": "1146260",
    "end": "1156040"
  },
  {
    "text": "at how many edges satisfies\nthat one endpoint is in S.",
    "start": "1156040",
    "end": "1161440"
  },
  {
    "text": "So i needs to be in S\nand j can be anything. And you have Gij.",
    "start": "1161440",
    "end": "1167530"
  },
  {
    "text": "So if you draw a graph,\nsomething like this-- suppose you draw a graph,\nlike this and this and this,",
    "start": "1167530",
    "end": "1177040"
  },
  {
    "text": "and you define this cut-- suppose this is S,\nthen what is ESS bar.",
    "start": "1177040",
    "end": "1182410"
  },
  {
    "text": "So ESS bar will be counting\nthese two right edges",
    "start": "1182410",
    "end": "1188290"
  },
  {
    "text": "because this is from S to S bar.  And the volume of that will\nbe counting all the edges",
    "start": "1188290",
    "end": "1195200"
  },
  {
    "text": "connected to S, which means\nbasically all the edges drawn here. All the green edges are counted.",
    "start": "1195200",
    "end": "1203880"
  },
  {
    "text": "And you can see that\nby the definition, it's true that the\nvolume of S is always-- ",
    "start": "1203880",
    "end": "1213120"
  },
  {
    "text": "so what this definition is for. This is trying to\ncharacterize how-- I guess the word\nconductance in the case",
    "start": "1213120",
    "end": "1221190"
  },
  {
    "text": "it's kind of trying\nto characterize how good the cut\nis in some sense,",
    "start": "1221190",
    "end": "1227220"
  },
  {
    "text": "like how separated\nS and S bar are. The smaller it is, the more\nseparated S and S bar is.",
    "start": "1227220",
    "end": "1233670"
  },
  {
    "text": "But you do have to\nnormalize by the volume. So in some sense, the number\nof edges between S and S",
    "start": "1233670",
    "end": "1240120"
  },
  {
    "text": "bar is already capturing how\nseparate S and S bar are, but you normalize\nwith the volume",
    "start": "1240120",
    "end": "1247110"
  },
  {
    "text": "to make it more meaningful. I guess that's what I'm\ngoing to argue in the next. So I guess before\nthat, let me just",
    "start": "1247110",
    "end": "1253530"
  },
  {
    "text": "get some basic information. So the volume of S is\nbigger than the number",
    "start": "1253530",
    "end": "1262020"
  },
  {
    "text": "of edges between S and S bar. That's trivial. So this means that the\nconductance is always",
    "start": "1262020",
    "end": "1267059"
  },
  {
    "text": "less than 1. And you are trying to\nmake the conductance as small as possible. And another thing is that the\nvolume of S plus the volume",
    "start": "1267060",
    "end": "1274150"
  },
  {
    "text": "of S bar is equal to the volume\nof V. This is a total of edges.",
    "start": "1274150",
    "end": "1282250"
  },
  {
    "start": "1282250",
    "end": "1287350"
  },
  {
    "text": "So this means that\nif the volume of S",
    "start": "1287350",
    "end": "1298770"
  },
  {
    "text": "is less than the volume of V\nover 2, then the volume of S",
    "start": "1298770",
    "end": "1304890"
  },
  {
    "text": "is also less than\nthe volume of S bar,",
    "start": "1304890",
    "end": "1309950"
  },
  {
    "text": "and this means that\nthe conductance of S is bigger than the\nconductance of S bar. ",
    "start": "1309950",
    "end": "1316850"
  },
  {
    "text": "So you should have a definition\nthat somehow doesn't depend on how you name S and S bar.",
    "start": "1316850",
    "end": "1322040"
  },
  {
    "text": "S and S bar is symmetric, but\nhere the conductance of S and S bar are different, right?",
    "start": "1322040",
    "end": "1327830"
  },
  {
    "text": "So that's how to remove this\nconfusion between a symmetry,",
    "start": "1327830",
    "end": "1332899"
  },
  {
    "text": "you just insist that you're\nalways talking about-- so we will insist that we\nalways only talk about S such",
    "start": "1332900",
    "end": "1348620"
  },
  {
    "text": "that the conductance of\nS-- sorry, the volume of S",
    "start": "1348620",
    "end": "1358490"
  },
  {
    "text": "is less than the\nvolume of v over 2. So you're only taking\na smaller part of S and use that to define the\nconductance of the cut.",
    "start": "1358490",
    "end": "1367830"
  },
  {
    "text": "Why don't we just\ndefine conductance so that normalizes\nphi volume of V? ",
    "start": "1367830",
    "end": "1374080"
  },
  {
    "text": "Yes. So if you normalize by volume\nof V, first of all, the problem is that it means that\nyou need to normalize",
    "start": "1374080",
    "end": "1379919"
  },
  {
    "text": "because V is a constant. You have to normalize\nagainst something. I'm going to tell you why\nyou have to normalize,",
    "start": "1379920",
    "end": "1385950"
  },
  {
    "text": "but if you want to normalize\nyou have to normalize something that changes as S changes.",
    "start": "1385950",
    "end": "1391500"
  },
  {
    "text": "So here I'm only trying to\ndeal with the symmetry so far. You only need kind of\nconductance on the smaller set.",
    "start": "1391500",
    "end": "1399570"
  },
  {
    "text": "This is not that\nmuch because you don't want to cheat by saying\nI have a very, very large set",
    "start": "1399570",
    "end": "1404760"
  },
  {
    "text": "and I only have\none point in S bar. And it sounds like my\nconductance is very small,",
    "start": "1404760",
    "end": "1410279"
  },
  {
    "text": "but actually it should\nmeasure the other side. ",
    "start": "1410280",
    "end": "1420534"
  },
  {
    "text": "Maybe before proceeding,\nanswer the question why we have to normalize. So we can also define the v of\nG. This is the conductance of--",
    "start": "1420534",
    "end": "1428070"
  },
  {
    "text": " this is the so-called\nsparsest cut of G.",
    "start": "1428070",
    "end": "1439040"
  },
  {
    "text": "A sparsest cut variable\nof G is defined to be the minimum\npossible conductance.",
    "start": "1439040",
    "end": "1445590"
  },
  {
    "text": "But again, you require that S\nis the smaller side of the two",
    "start": "1445590",
    "end": "1454500"
  },
  {
    "text": "cuts. So you minimize over\nthe conductance. So first, you minimize\nthe conductance first",
    "start": "1454500",
    "end": "1460710"
  },
  {
    "text": "with the constraint\nthat the volume of S is less than the volume of\nV. So basically, you just",
    "start": "1460710",
    "end": "1468390"
  },
  {
    "text": "want to find a cut that\nhas smallest conductance. Now let's talk\nabout normalization,",
    "start": "1468390",
    "end": "1473820"
  },
  {
    "text": "so why we have to normalize. ",
    "start": "1473820",
    "end": "1481830"
  },
  {
    "text": "I think the reason\nis pretty much just because if you don't\nnormalize, then if you just",
    "start": "1481830",
    "end": "1495380"
  },
  {
    "text": "minimize-- if you\njust look at ESS bar,",
    "start": "1495380",
    "end": "1502160"
  },
  {
    "text": "it's typically minimized\nwhen S is small. ",
    "start": "1502160",
    "end": "1515059"
  },
  {
    "text": "So suppose you draw a graph,\nfor example, I guess-- ",
    "start": "1515060",
    "end": "1523490"
  },
  {
    "text": "if you don't normalize,\nbasically you prefer to pick a\nset S that itself is very small so that it doesn't\nconnect to the other part.",
    "start": "1523490",
    "end": "1531410"
  },
  {
    "text": "So for example, let's see. Suppose you have\na graph like this. ",
    "start": "1531410",
    "end": "1541800"
  },
  {
    "text": "What I'm doing\nhere is I have a-- suppose you have a completely\nconnected subgraph.",
    "start": "1541800",
    "end": "1549130"
  },
  {
    "text": "So do you have n over 2\nnodes, n over 2 nodes. And within each of the subgraph,\nyou have complete connection",
    "start": "1549130",
    "end": "1555340"
  },
  {
    "text": "with each other. And then you have\nsome very small number",
    "start": "1555340",
    "end": "1560800"
  },
  {
    "text": "of connections\nbetween them, maybe every node is connected\nto all of them like this. OK.",
    "start": "1560800",
    "end": "1567620"
  },
  {
    "text": "So it sounds pretty clear\nthat you should just really-- the best cut you should get is\nthis bar graph because within",
    "start": "1567620",
    "end": "1575029"
  },
  {
    "text": "the cluster, you have full\nconnection and across the two clusters, we have\nsome number of--",
    "start": "1575030",
    "end": "1580490"
  },
  {
    "text": "let's say two edges per node. So it sounds pretty\nclear we should do this.",
    "start": "1580490",
    "end": "1586190"
  },
  {
    "text": "But if you use the\nmatrix ESS bar, then you see that\nsome other cuts will",
    "start": "1586190",
    "end": "1592970"
  },
  {
    "text": "have smaller number of\nedges across the thing because you can just take\nthis to be S1 because S1 just",
    "start": "1592970",
    "end": "1601870"
  },
  {
    "text": "consists one node. So then E of S1, ES1,\nS1 bar is basically",
    "start": "1601870",
    "end": "1608170"
  },
  {
    "text": "how many edges comes from S1\nto S1 bar, basically the number",
    "start": "1608170",
    "end": "1613240"
  },
  {
    "text": "of edges connected to S1. This is n over 2. ",
    "start": "1613240",
    "end": "1620170"
  },
  {
    "text": "Let's say the good cut is S2. So ES2, S2 bar is definitely\nsomething bigger than n over 2",
    "start": "1620170",
    "end": "1626200"
  },
  {
    "text": "because you have\nn over 2 probably times the number of blue\nedges, something like two here.",
    "start": "1626200",
    "end": "1635380"
  },
  {
    "text": "I'm joining basically\ntwo edges per node. ",
    "start": "1635380",
    "end": "1645260"
  },
  {
    "text": "So basically, it sounds\nlike you should get S2, but if you use the unnormalized\nversion, you would get S1.",
    "start": "1645260",
    "end": "1655020"
  },
  {
    "text": "However, if you normalize,\nthen it's a different game. So if you normalize, if you\nlook at the conductance of S1,",
    "start": "1655020",
    "end": "1661779"
  },
  {
    "text": "then this is E of S1 S1\nbar over the volume of S1.",
    "start": "1661780",
    "end": "1668710"
  },
  {
    "text": "This is n over 2\ntimes n over n over 2. I think the volume\non S minus n over 2.",
    "start": "1668710",
    "end": "1678960"
  },
  {
    "text": "So this is 1. So if you look at phi of S2,\nthen this is n over 2 times 2,",
    "start": "1678960",
    "end": "1700890"
  },
  {
    "text": "something like this. And then you have\nthe total number of edges connected with 2. That's actually a big number.",
    "start": "1700890",
    "end": "1706390"
  },
  {
    "text": "That's probably something like\nn over 2 times n over 2 minus 1. This is the number of\nedges within the S2,",
    "start": "1706390",
    "end": "1712779"
  },
  {
    "text": "and there are some\nedges between S2 and S2 bar, something like this. And this would be something\nlike roughly I think 2 over n.",
    "start": "1712780",
    "end": "1722390"
  },
  {
    "text": "So the conductance of S2 is much\nsmaller than conductance of S1 if you normalize.",
    "start": "1722390",
    "end": "1727871"
  },
  {
    "text": " Questions so far? ",
    "start": "1727871",
    "end": "1736220"
  },
  {
    "text": "OK, cool.  So now we have to kind\nof define the goal.",
    "start": "1736220",
    "end": "1741850"
  },
  {
    "text": "Because you have a worst-case\ngraph your goal is to-- so we have said that the goal\nis to find approximate sparsest",
    "start": "1741850",
    "end": "1754750"
  },
  {
    "text": "cut, S hat, meaning that\nyou want S hat to satisfy",
    "start": "1754750",
    "end": "1763260"
  },
  {
    "text": "that the phi of S hat is close\nto the sparsest possible cut,",
    "start": "1763260",
    "end": "1769240"
  },
  {
    "text": "phi of G. And the approach\nwe're going to describe",
    "start": "1769240",
    "end": "1779490"
  },
  {
    "text": "is still eigendecomposition. ",
    "start": "1779490",
    "end": "1788720"
  },
  {
    "text": "So how do I do this?",
    "start": "1788720",
    "end": "1796870"
  },
  {
    "start": "1796870",
    "end": "1805658"
  },
  {
    "text": "There's [AUDIO OUT] to even\nstate what we mean exactly by eigendecomposition and what\nkind of results we can have.",
    "start": "1805658",
    "end": "1813010"
  },
  {
    "text": "So first of all, let's di to\nbe the volume of the known i.",
    "start": "1813010",
    "end": "1823020"
  },
  {
    "text": "You take a single node, you\ntake the volume, this di. And this is really just the\ndegree of node i, right?",
    "start": "1823020",
    "end": "1835220"
  },
  {
    "text": "The volume of the node is\nreally the degree of the node. And lets take d to be\nthe diagonal matrix that",
    "start": "1835220",
    "end": "1844520"
  },
  {
    "text": "contains di of x entry. ",
    "start": "1844520",
    "end": "1850886"
  },
  {
    "text": "And let's define this. So a normalized\nadjacency matrix is",
    "start": "1850886",
    "end": "1866850"
  },
  {
    "text": "called A bar, which is D\nminus 1/2, G times minus 1/2,",
    "start": "1866850",
    "end": "1874830"
  },
  {
    "text": "where G is the adjacency matrix. Recall this is our notation,\nwith a little bit of notation.",
    "start": "1874830",
    "end": "1882680"
  },
  {
    "text": "So what does this really mean? This really just means\nthat 1 over square root d1",
    "start": "1882680",
    "end": "1888600"
  },
  {
    "text": "up to 1 over square\ndn times G times 1 over square root d1 up\nto 1 over square root dn.",
    "start": "1888600",
    "end": "1896030"
  },
  {
    "text": "And a diagonal matrix\nmultiplied on the left means that you will\nscale all of the rows",
    "start": "1896030",
    "end": "1903780"
  },
  {
    "text": "and the diagonal matrix at the\nright hand side multiplication means you'll scale\nall the columns.",
    "start": "1903780",
    "end": "1909790"
  },
  {
    "text": "So basically you'll scale the\ncolumns and rows simultaneously with these numbers. If you do the [INAUDIBLE]\nwhat it really means",
    "start": "1909790",
    "end": "1917040"
  },
  {
    "text": "is that the Aij, the ij of the\nnormalized adjacency matrix is really just the adjacency\nmatrix over square root",
    "start": "1917040",
    "end": "1925259"
  },
  {
    "text": "di times square root dj.  So this sounds a\nlittle complicated,",
    "start": "1925260",
    "end": "1932300"
  },
  {
    "text": "but in most of the cases-- I'm only just stating\nthis mostly for formality because sometimes\nsometimes the key thing",
    "start": "1932300",
    "end": "1941990"
  },
  {
    "text": "can be seen by assuming\nthe graph is regular. So in most cases, suffice it to\nthink of G as a regular graph.",
    "start": "1941990",
    "end": "1964830"
  },
  {
    "text": " A regular graph means that\nall the degrees are the same.",
    "start": "1964830",
    "end": "1977060"
  },
  {
    "text": "So let's say suppose that G is\na kappa regular graph, meaning",
    "start": "1977060",
    "end": "1984970"
  },
  {
    "text": "di is equal to\nkappa for every i, then its adjacency matrix is\nreally just a 1 over kappa--",
    "start": "1984970",
    "end": "1994630"
  },
  {
    "text": "normalized adjacency matrix is\njust 1 over kappa times Gij. So in some sense,\nwe really didn't",
    "start": "1994630",
    "end": "1999850"
  },
  {
    "text": "do much except for just\nchanging the scaling of this. But this scaling is kind of\nimportant in the formal sense",
    "start": "1999850",
    "end": "2008549"
  },
  {
    "text": "because it can make them\nformally very clean. But it's not fundamentally\nsuper important.",
    "start": "2008550",
    "end": "2016290"
  },
  {
    "text": "So this is pretty much-- if you don't want to think\nabout the di and djs,",
    "start": "2016290",
    "end": "2022614"
  },
  {
    "text": "you pretty much can\nthink of this simple case where you have a regular graph.",
    "start": "2022614",
    "end": "2028230"
  },
  {
    "text": "And once we define a\nnormalized adjacency matrix, you can also define\nthe so-called Laplacian",
    "start": "2028230",
    "end": "2033930"
  },
  {
    "text": "matrix, which is i minus the\nnormalized adjacency matrix.",
    "start": "2033930",
    "end": "2039815"
  },
  {
    "start": "2039815",
    "end": "2046399"
  },
  {
    "text": "I think you'll probably see that\none of the reason why we have to normalize is that\nif you don't normalize,",
    "start": "2046400",
    "end": "2052040"
  },
  {
    "text": "it doesn't makes sense to\nsubtract, take the differences between it and an identity.",
    "start": "2052040",
    "end": "2057320"
  },
  {
    "text": "Identity is something\nthat doesn't have a scale. So you have to normalize it\nso that you can kind of take the dif with identity.",
    "start": "2057320",
    "end": "2064820"
  },
  {
    "text": "And this Laplacian matrix is\nreally not doing that much.",
    "start": "2064820",
    "end": "2069919"
  },
  {
    "text": "It's not that different from\nnormalized adjacency matrix anyway because they are-- pretty much everything\ncorresponds to each other.",
    "start": "2069920",
    "end": "2076379"
  },
  {
    "text": "So the eigenvector\nof L is the same",
    "start": "2076380",
    "end": "2082589"
  },
  {
    "text": "as the eigenvector on A bar.",
    "start": "2082590",
    "end": "2087989"
  },
  {
    "text": "And the spectrums are just\nflipped with each other. So let's say suppose L\nhas eigenvalue lambda 1",
    "start": "2087989",
    "end": "2101850"
  },
  {
    "text": "up to lambda n,\nlet's say suppose-- I think in this literature,\nyou always want to order them.",
    "start": "2101850",
    "end": "2115830"
  },
  {
    "text": "And then with the\neigenvector u1 up to un.",
    "start": "2115830",
    "end": "2125130"
  },
  {
    "text": "Then this means that this\nis equivalent to A bar as eigenvalue 1 minus lambda\n1 up to 1 minus lambda n.",
    "start": "2125130",
    "end": "2136760"
  },
  {
    "text": "Now I'm searching in\na decreasing order and with the same eigenvectors.",
    "start": "2136760",
    "end": "2148430"
  },
  {
    "start": "2148430",
    "end": "2154819"
  },
  {
    "text": "So you don't even have to\nthink about the Laplacian. The Laplacian will come into\nplay at some later places,",
    "start": "2154820",
    "end": "2162890"
  },
  {
    "text": "but so far you can\njust think of Laplacian is a flipped version of\nnormalized adjacency matrix. Nothing really different.",
    "start": "2162890",
    "end": "2169350"
  },
  {
    "text": " So these are some little\nbit abstract preparations.",
    "start": "2169350",
    "end": "2176110"
  },
  {
    "text": "And now let's see what\nwe can do with this. So this is the in my opinion,\npretty important theorem.",
    "start": "2176110",
    "end": "2187910"
  },
  {
    "text": "It's called Cheegers inequality. Actually, this dates back\nto 1969 by Jeff Cheeger.",
    "start": "2187910",
    "end": "2195270"
  },
  {
    "text": "So it says the following. It says that lambda 2, this is\nthe second eigenvalue, over 2",
    "start": "2195270",
    "end": "2204560"
  },
  {
    "text": "is less than the conductance\nof G, which is less than square root 2 lambda 2.",
    "start": "2204560",
    "end": "2211040"
  },
  {
    "text": "So why this is a\nvery important thing, it connects the\nconductance, the sparsest",
    "start": "2211040",
    "end": "2216980"
  },
  {
    "text": "cut to something linear\nalgebra to the eigenvectors.",
    "start": "2216980",
    "end": "2222230"
  },
  {
    "text": "So the sparsest cut is a\nvery combinatorial stuff where if you really want\nto find the sparsest cut, you'll probably want to\nenumerate all the possible cuts",
    "start": "2222230",
    "end": "2229052"
  },
  {
    "text": "and vice versa. At least the definition\nis a combinatorial thing.",
    "start": "2229052",
    "end": "2234410"
  },
  {
    "text": "But this inequality is saying\nthat somehow, the sparsest cut",
    "start": "2234410",
    "end": "2239690"
  },
  {
    "text": "value has a lot to do with the\neigenvalues of the Laplacian",
    "start": "2239690",
    "end": "2244940"
  },
  {
    "text": "or the adjacency matrix. And in particular, it's very\nclose to the second eigenvalue",
    "start": "2244940",
    "end": "2251120"
  },
  {
    "text": "of the Laplacian matrix. And moreover, you\ncan also find the--",
    "start": "2251120",
    "end": "2257620"
  },
  {
    "text": "you can find the\napproximate cut S",
    "start": "2257620",
    "end": "2264500"
  },
  {
    "text": "hat such that this cut S hat--",
    "start": "2264500",
    "end": "2271190"
  },
  {
    "text": "the conductance is less\nthan square root 2 lambda 2, which is less than 2\ntimes square root phi of G",
    "start": "2271190",
    "end": "2280640"
  },
  {
    "text": "computationally efficiently. ",
    "start": "2280640",
    "end": "2287250"
  },
  {
    "text": "And not only computationally\nefficiently but also actually pretty explicitly,\nwhat you can do is the following, by\nrounding the eigenvectors.",
    "start": "2287250",
    "end": "2296790"
  },
  {
    "text": "I guess rounding really\nmeans the following. This is the rounding in the\napproximation algorithm.",
    "start": "2296790",
    "end": "2304307"
  },
  {
    "text": "If you don't know what the term\ncomes from, it doesn't matter. So here is the procedure\nto find such a set S hat.",
    "start": "2304307",
    "end": "2310980"
  },
  {
    "text": "So suppose you take u2. Suppose the u2 is equal to--",
    "start": "2310980",
    "end": "2318780"
  },
  {
    "text": "the coordinates are\nbeta 1 up to beta n. It's the second eigenvector. It's the second eigenvector.",
    "start": "2318780",
    "end": "2327968"
  },
  {
    "text": " So you can take a threshold,\nwhich is T to the beta i,",
    "start": "2327968",
    "end": "2344390"
  },
  {
    "text": "and consider S hat i to be\nall the coordinates that--",
    "start": "2344390",
    "end": "2354289"
  },
  {
    "text": "so this flexibility\nis less than tau. So you take the\nthreshold but you don't have to consider all\nthe possible thresholds.",
    "start": "2354290",
    "end": "2361365"
  },
  {
    "text": "It's not necessary\nbecause I don't think so. You take a threshold\ntau, and a threshold",
    "start": "2361365",
    "end": "2369020"
  },
  {
    "text": "is chosen from one\nof the coordinates. And you say you look\nat all the coordinates that are smaller\nthan the threshold,",
    "start": "2369020",
    "end": "2376250"
  },
  {
    "text": "and that's your S hat.  So you have basically\nall of these sites,",
    "start": "2376250",
    "end": "2382490"
  },
  {
    "text": "S1 hat, S2 hat, S3\nhat, and so forth. So one of these S hat's\nsatisfy phi Si hat",
    "start": "2382490",
    "end": "2398520"
  },
  {
    "text": "is less than 2 times\nsquare root phi. So one of these sites\nwill be a good cut.",
    "start": "2398520",
    "end": "2404520"
  },
  {
    "text": "So I guess I'm thinking\nthis in a formal way. It seems a little bit confusing. So what you really are\ndoing is the following.",
    "start": "2404520",
    "end": "2413160"
  },
  {
    "text": "So you sort, I guess\nin plain language or in more informal language,\nyou first sort the coordinates.",
    "start": "2413160",
    "end": "2421537"
  },
  {
    "text": "Suppose you sort the\ncoordinates first and you get beta 1 less than\nbeta 2, less than beta n.",
    "start": "2421537",
    "end": "2427010"
  },
  {
    "text": "And then it's saying\nthat if you take-- this will be S hat i. But this will be the first i\ncoordinate, and that would--",
    "start": "2427010",
    "end": "2434790"
  },
  {
    "text": "and one of these hats\nwill be a good cut.",
    "start": "2434790",
    "end": "2440220"
  },
  {
    "text": "So you can try one cut,\nwhich is like this, you can try another cut,\nwhich is beta 1 beta 2,",
    "start": "2440220",
    "end": "2445740"
  },
  {
    "text": "and you can try another\ncut, which is beta 1, beta 2 up to beta i. And one of these cuts will\nbe a good cut of the graph",
    "start": "2445740",
    "end": "2453150"
  },
  {
    "text": "with a small conductance. And of course, you\nhave to restore the-- you have to remap\nthe coordinates back",
    "start": "2453150",
    "end": "2460170"
  },
  {
    "text": "to the original\ncoordinate system because you have\nstarted the coordinates.",
    "start": "2460170",
    "end": "2466950"
  },
  {
    "text": "But this is the manner. Any questions? ",
    "start": "2466950",
    "end": "2476859"
  },
  {
    "text": "Another way to think about it\nis that in a stochastic block model case, the\nsecond eigenvector was something like this.",
    "start": "2476860",
    "end": "2482050"
  },
  {
    "text": " And pretty much in that case,\nif you take a threshold,",
    "start": "2482050",
    "end": "2489100"
  },
  {
    "text": "the smaller values\ncorrespond to one cut and the larger values\ncorrespond to another cut. But here you don't know where\nthe exact threshold should be.",
    "start": "2489100",
    "end": "2495280"
  },
  {
    "text": "You should try all the\nthresholds, beta 1 up to beta",
    "start": "2495280",
    "end": "2500530"
  },
  {
    "text": "n, all of them. OK, cool. So this is a pretty magical\ntheorem in my opinion.",
    "start": "2500530",
    "end": "2507335"
  },
  {
    "text": " I'm not going to prove it.",
    "start": "2507335",
    "end": "2512570"
  },
  {
    "text": "If you are interested,\nI think there are a lot of lecture\nnotes that can prove this. ",
    "start": "2512570",
    "end": "2519710"
  },
  {
    "text": "I guess what I'm going\nto do is I'm going to-- [INAUDIBLE] exactly\none of these S hat",
    "start": "2519710",
    "end": "2526369"
  },
  {
    "text": "i that says that or additional? Additional. ",
    "start": "2526370",
    "end": "2535940"
  },
  {
    "text": "And if you are able to-- and you can enumerate\nall of them. Just try all of\nthem and see which one is better than the other.",
    "start": "2535940",
    "end": "2542638"
  },
  {
    "start": "2542638",
    "end": "2550310"
  },
  {
    "text": "So the proof is\npretty nontrivial.",
    "start": "2550310",
    "end": "2555470"
  },
  {
    "text": "It's not very long, but\nit's kind of non-trivial. So I'm going to skip the\nproof and I'm going to link--",
    "start": "2555470",
    "end": "2563250"
  },
  {
    "text": "I see some questions here. ",
    "start": "2563250",
    "end": "2571250"
  },
  {
    "text": "So the question online\nhere is that the hat Sj found this way is in the\nbest possible cut, right?",
    "start": "2571250",
    "end": "2578920"
  },
  {
    "text": "Yes. So you are not guaranteed to\nfind the best possible cut. You're only guaranteed\nto find a cut",
    "start": "2578920",
    "end": "2585460"
  },
  {
    "text": "such that the cut\nvalue phi of S hat i",
    "start": "2585460",
    "end": "2590650"
  },
  {
    "text": "satisfies that it's less than\n2 times square root phi of G. If you get phi of G here,\nsuppose you magically",
    "start": "2590650",
    "end": "2597609"
  },
  {
    "text": "change this to phi\nof G, then that means you can best\ncut because phi of G",
    "start": "2597610",
    "end": "2602809"
  },
  {
    "text": "is the value of the best cut. Of course, maybe there are\nmultiple best cuts as well, but you definitely find\none of the best cut.",
    "start": "2602810",
    "end": "2611180"
  },
  {
    "text": "However, we don't have\nthat strong theorem. We've only shown that 2 times\nsquare root phi of G because--",
    "start": "2611180",
    "end": "2618440"
  },
  {
    "text": "so you lose something. Square root phi of G is bigger\nthan phi of G, by the way,",
    "start": "2618440",
    "end": "2623750"
  },
  {
    "text": "because phi of G is less than 1. So you'll lose some factor\nin terms of the best possible",
    "start": "2623750",
    "end": "2633470"
  },
  {
    "text": "vector conductance.  I hope that answers\nthe question.",
    "start": "2633470",
    "end": "2640109"
  },
  {
    "text": "Anyway, sometimes you\nhave to lose a little bit to some extent because I guess\nthis is sometimes post-mortem.",
    "start": "2640110",
    "end": "2650550"
  },
  {
    "text": "But if you think about-- in retrospect, one\nof these quantities",
    "start": "2650550",
    "end": "2656130"
  },
  {
    "text": "is very combinatorial,\nthe sparsest cut, and the other point is\nvery linear algebraic.",
    "start": "2656130",
    "end": "2661677"
  },
  {
    "text": "it's sounds unlikely that they\ncan be exactly the same, right? ",
    "start": "2661677",
    "end": "2666935"
  },
  {
    "text": "So it's already\nkind of fortunate that they are somewhat\nrelated in my opinion.",
    "start": "2666935",
    "end": "2672673"
  },
  {
    "text": "And there are also\nkind of like the most discussed some of the\nintuitions or kind of more",
    "start": "2672673",
    "end": "2678210"
  },
  {
    "text": "basic qualities. Like some of the intuition is\nwhy this can be possible to,",
    "start": "2678210",
    "end": "2684720"
  },
  {
    "text": "but I won't give the full proof. The statement up there that\nsays we can find beta S such",
    "start": "2684720",
    "end": "2691440"
  },
  {
    "text": "that it's less than\nsquare root 2 over 2, which then is less than.",
    "start": "2691440",
    "end": "2698263"
  },
  {
    "text": "Is that that we're\nactually finding then we're just saying, transitive\nproperty [INAUDIBLE]..",
    "start": "2698263",
    "end": "2704700"
  },
  {
    "text": "Then that's less\nthan 2 square root. And in that case-- ",
    "start": "2704700",
    "end": "2712080"
  },
  {
    "text": "I asked if we just care about\nthe relation to phi of G, right? You just care about?",
    "start": "2712080",
    "end": "2717849"
  },
  {
    "text": "The comparison of r\nhat to the cut of G. It's not significant but it's--",
    "start": "2717850",
    "end": "2724990"
  },
  {
    "text": "the square root 2 lambda\n2 is insignificant. Other than that, it lets\nus share this inequality.",
    "start": "2724990",
    "end": "2731560"
  },
  {
    "text": "Sure. So first of all,\nyes, you are right. So how do you get\nthis inequality?",
    "start": "2731560",
    "end": "2736900"
  },
  {
    "text": "This is just by using this part. So that's true. And second, yes,\nprobably the first bit",
    "start": "2736900",
    "end": "2742750"
  },
  {
    "text": "you care about is\ncomparing with phi of G, and these are just some\nintermediate things. ",
    "start": "2742750",
    "end": "2756870"
  },
  {
    "text": "That's the first other bit. But I think if you look at\nthe proof, you do have to--",
    "start": "2756870",
    "end": "2761970"
  },
  {
    "text": "the eigenvalues have\nto show up somewhere. Do you use any--",
    "start": "2761970",
    "end": "2767940"
  },
  {
    "text": "do you not use\nanything that came out of that second\nHoeffding inequality? So maybe your point is\nactually pretty good",
    "start": "2767940",
    "end": "2775530"
  },
  {
    "text": "because 2 lambda 2 is\nactually relatively small, but then you use more\nof that second volume.",
    "start": "2775530",
    "end": "2782212"
  },
  {
    "text": "I think it's possible\nbut we don't really know. It's kind of very hard to-- I think there are hard\ninstances in both cases.",
    "start": "2782212",
    "end": "2791190"
  },
  {
    "text": "This thing can be both\nclose to lambda 2 over 2 or it could be very\nclose to this side.",
    "start": "2791190",
    "end": "2796549"
  },
  {
    "text": " Cool. So I guess I'll focus on\nsome intuitions and why.",
    "start": "2796550",
    "end": "2806980"
  },
  {
    "text": "The first thing\nI want to discuss is that I think this is\nagain about the scaling",
    "start": "2806980",
    "end": "2815260"
  },
  {
    "text": "to some extent. So first of all, the\nsmallest eigenvector, why you take the\nsecond eigenvector.",
    "start": "2815260",
    "end": "2820900"
  },
  {
    "text": "I think that's always something\nthat seems to be magical to me at first sight. And then after I\nspend some time--",
    "start": "2820900",
    "end": "2829480"
  },
  {
    "text": "when I first started, I\nrealized the top eigenvector--",
    "start": "2829480",
    "end": "2834880"
  },
  {
    "text": "kind of like say last time. The top eigenvector is\nkind of like a background. So either the smallest\neigenvector of L",
    "start": "2834880",
    "end": "2843720"
  },
  {
    "text": "or the top eigenvector of A bar.",
    "start": "2843720",
    "end": "2850520"
  },
  {
    "text": "This is kind of not\nthat interesting. ",
    "start": "2850520",
    "end": "2855780"
  },
  {
    "text": "And what why it's\nnot interesting is it's pretty much\nonly trying to get the--",
    "start": "2855780",
    "end": "2861060"
  },
  {
    "text": "only capturing in some\nsense I call it background. It's kind of like a\nbackground density.",
    "start": "2861060",
    "end": "2867210"
  },
  {
    "text": " So what I really mean by\nthis is that off the graph.",
    "start": "2867210",
    "end": "2874773"
  },
  {
    "text": "What I really mean by this\nis that let's say suppose when G is kappa regular.",
    "start": "2874773",
    "end": "2879954"
  },
  {
    "text": " I think we actually have stated\nthis in a previous lecture.",
    "start": "2879955",
    "end": "2887299"
  },
  {
    "text": "So L1 vector is top eigenvector\nof G of adjacency matrix,",
    "start": "2887300",
    "end": "2902550"
  },
  {
    "text": "and thus also top\neigenvector of A bar,",
    "start": "2902550",
    "end": "2913130"
  },
  {
    "text": "which is just 1 kappa times G.",
    "start": "2913130",
    "end": "2919859"
  },
  {
    "text": "So when G is regular,\nthen the top eigenvector is really just about-- it's just an L1 vector.",
    "start": "2919860",
    "end": "2927230"
  },
  {
    "text": "And in more general\ncase, it really just involves the\nscaling based on density.",
    "start": "2927230",
    "end": "2933920"
  },
  {
    "text": "So for general G, what happens\nis that the top eigenvector is really just this one, the square\nroot d1 up to square root dn.",
    "start": "2933920",
    "end": "2944840"
  },
  {
    "text": "The scale doesn't matter here\nbecause the eigenvectors is-- and multiplication of\nthis is also eigenvector,",
    "start": "2944840",
    "end": "2950780"
  },
  {
    "text": "so I didn't care\nabout the scaling. So this is the top\neigenvector of A bar, so",
    "start": "2950780",
    "end": "2959300"
  },
  {
    "text": "smallest, which\nmeans the smallest eigenvector of Laplacian.",
    "start": "2959300",
    "end": "2970820"
  },
  {
    "text": "Why this is the case, you can\nverify this relatively easily. So A bar times u1, this\nis the mentioned location.",
    "start": "2970820",
    "end": "2980619"
  },
  {
    "text": "And you look at j's\ncoordinate i's coordinate, this is equal to\nthe sum of j over j,",
    "start": "2980620",
    "end": "2988390"
  },
  {
    "text": "sum of Aij bar times uj. And Aij bar is a scaled\nversion of the graph.",
    "start": "2988390",
    "end": "2996685"
  },
  {
    "text": "So Gij over square\nroot di square root dj.",
    "start": "2996685",
    "end": "3001960"
  },
  {
    "text": "And uj is square root dj. And this is sum over j.",
    "start": "3001960",
    "end": "3008690"
  },
  {
    "text": "So you first cancel\nthese two, and you get 1 over square root di in front.",
    "start": "3008690",
    "end": "3014870"
  },
  {
    "text": "You get this.  And recall that this is\nactually a precise definition",
    "start": "3014870",
    "end": "3020560"
  },
  {
    "text": "of the degree. This is the total number of\nedges connected to the graph. So get 1 over square root di\ntimes di into square root di.",
    "start": "3020560",
    "end": "3029460"
  },
  {
    "text": "So that verifies that\nu1 is an eigenvector. This means A bar\nu1 is equal to u1.",
    "start": "3029460",
    "end": "3036310"
  },
  {
    "text": " So basically as before, the top\neigenvector is not doing much,",
    "start": "3036310",
    "end": "3043049"
  },
  {
    "text": "it's really just capturing\nthe degrees of the graph. And the second\neigenvector starts",
    "start": "3043050",
    "end": "3049430"
  },
  {
    "text": "to talk about the\ninterconnections. It has more about the\nrelationship between edges",
    "start": "3049430",
    "end": "3056780"
  },
  {
    "text": "and hidden communities. And now let's look at\nsome intuitions about why",
    "start": "3056780",
    "end": "3077710"
  },
  {
    "text": "somehow this eigenvector\nis related to the cut. So here is another\nway to think about it.",
    "start": "3077710",
    "end": "3084269"
  },
  {
    "text": "So if you look at\nthe contracting form of the Laplacian.",
    "start": "3084270",
    "end": "3089760"
  },
  {
    "text": "So what is this? This is the v transpose i times\nv minus v transpose A bar times",
    "start": "3089760",
    "end": "3095530"
  },
  {
    "text": "v. Let's just put\nfirstly right this. This is sum of vi squared i\nfrom 1 to n minus the sum of ij.",
    "start": "3095530",
    "end": "3108630"
  },
  {
    "text": " Let's do it.",
    "start": "3108630",
    "end": "3113880"
  },
  {
    "text": "vivj, A bar Aj. And this is sum of vi squared\nminus sum by vivj GIj square",
    "start": "3113880",
    "end": "3126510"
  },
  {
    "text": "root di square root dj. And Gij is 1 when\nthe eigen is 1.",
    "start": "3126510",
    "end": "3132630"
  },
  {
    "text": "So what we got is that sum vi\nsquared minus ij is the edge.",
    "start": "3132630",
    "end": "3138230"
  },
  {
    "text": " But ij and ji are\nboth at the source,",
    "start": "3138230",
    "end": "3143350"
  },
  {
    "text": "so that's why you get 2 here. 2 times vi over square root di\nminus vj over square root dj.",
    "start": "3143350",
    "end": "3153730"
  },
  {
    "text": "And now you can bring\nthis first thing i,",
    "start": "3153730",
    "end": "3158770"
  },
  {
    "text": "and then you take jvi-- ",
    "start": "3158770",
    "end": "3166849"
  },
  {
    "text": "I guess maybe let's\nspread this way. So I'm claiming that\nthis is equal to sum of vi over square root vi minus\nvj over square root vj squared.",
    "start": "3166850",
    "end": "3178020"
  },
  {
    "text": "And ij is E. And why this\nis true, this is true--",
    "start": "3178020",
    "end": "3184230"
  },
  {
    "text": "you can expand this\nequation into terms.",
    "start": "3184230",
    "end": "3189450"
  },
  {
    "text": "And you can see the cross\nwill match this one, the only thing is to see the\nother terms match the vi's,",
    "start": "3189450",
    "end": "3195247"
  },
  {
    "text": "right? So we can verify that by\nlooking at some ij in E,",
    "start": "3195247",
    "end": "3201630"
  },
  {
    "text": "vi squared over di. ",
    "start": "3201630",
    "end": "3206970"
  },
  {
    "text": "This is on sum over i, sum\nover j, such that i, j, and e--",
    "start": "3206970",
    "end": "3218885"
  },
  {
    "start": "3218885",
    "end": "3224620"
  },
  {
    "text": "I think this is\nprobably obvious, but I'm making it a little\nbit too complicated. So vi squared over\ndi times this one.",
    "start": "3224620",
    "end": "3233170"
  },
  {
    "text": "If you sum over i first and sum\nover j second, first sum over j and then sum over i. So how many edges are connected\nto i, that's basically di.",
    "start": "3233170",
    "end": "3243339"
  },
  {
    "text": "So you guys this vi\nsquared over ti times di.",
    "start": "3243340",
    "end": "3250020"
  },
  {
    "text": "So that's why it's\nsum of vi squared. ",
    "start": "3250020",
    "end": "3262940"
  },
  {
    "text": "OK, sounds good.  I guess I'm somehow missing\na constant somewhere.",
    "start": "3262940",
    "end": "3268910"
  },
  {
    "text": "I'm not sure what it is. Did I miss a constant somewhere?",
    "start": "3268910",
    "end": "3276452"
  },
  {
    "text": "I will double check, I\nthink the constant might be off by 2 somewhere, but\nyou get it just this way.",
    "start": "3276452",
    "end": "3282421"
  },
  {
    "text": "OK. ",
    "start": "3282421",
    "end": "3289010"
  },
  {
    "text": "And if this is regular\ngraph, if G is regular graph,",
    "start": "3289010",
    "end": "3297730"
  },
  {
    "text": "say kappa regular, then\nyou can ignore the is. You can just say v transpose\nLv is 1 over kappa times",
    "start": "3297730",
    "end": "3308049"
  },
  {
    "text": "ij in E. vi must be j square. OK.",
    "start": "3308050",
    "end": "3313960"
  },
  {
    "text": "So but why I care about\nthe-- why I did so much work to guide this equation. I think the equation is\nvery important because this",
    "start": "3313960",
    "end": "3321160"
  },
  {
    "text": "is how it links to-- how these algebraic quantities\nlinks to the conductance.",
    "start": "3321160",
    "end": "3326260"
  },
  {
    "text": "So this is the\nalgebraic quantity. It's something like\nlinear algebra. It's quadratic form.",
    "start": "3326260",
    "end": "3332330"
  },
  {
    "text": "However, if you-- here, suppose\nnow you you restrict it? You restrict v to be binary.",
    "start": "3332330",
    "end": "3338770"
  },
  {
    "text": "So suppose v is--",
    "start": "3338770",
    "end": "3344540"
  },
  {
    "text": "we take v to be binary. It's a binary vector. ",
    "start": "3344540",
    "end": "3351870"
  },
  {
    "text": "And you would take s to be the\nsupport of v. So the indices",
    "start": "3351870",
    "end": "3357560"
  },
  {
    "text": "where the entry, the v is 1. Then you can see that\nfrom this formula,",
    "start": "3357560",
    "end": "3363900"
  },
  {
    "text": "v transpose L. V is 1 over kappa\ntimes E vi minus vj square.",
    "start": "3363900",
    "end": "3378020"
  },
  {
    "text": "And when is this 1? This is 1 when i\nand j are in both-- are in different.",
    "start": "3378020",
    "end": "3384440"
  },
  {
    "text": "So this is only 1 if i\nin s, and j is in s bar,",
    "start": "3384440",
    "end": "3391410"
  },
  {
    "text": "or i is in s bar,\nj is in s, right?",
    "start": "3391410",
    "end": "3396660"
  },
  {
    "text": "So basically, this sum is the\nnumber of i and j's between s and s bar, because only\nthen the i and j is",
    "start": "3396660",
    "end": "3404910"
  },
  {
    "text": "an average across the groups. This vi minus vj square\nis v equals to 1.",
    "start": "3404910",
    "end": "3410400"
  },
  {
    "text": "Otherwise, it's going to be 0. So this is why it's 1 over\nkappa times the number of ij's",
    "start": "3410400",
    "end": "3416910"
  },
  {
    "text": "across N minus s bar. So the quadratic form\nconnects to the number",
    "start": "3416910",
    "end": "3424410"
  },
  {
    "text": "of ij's across the two\ngroups when v is not binary. If v is not binary, of\ncourse, it's not true.",
    "start": "3424410",
    "end": "3430168"
  },
  {
    "text": "But if it is binary, it's true.  Or in other words, you\ncan write v transpose lv",
    "start": "3430168",
    "end": "3437220"
  },
  {
    "text": "is 1 over kappa,\nthe support of v--",
    "start": "3437220",
    "end": "3442290"
  },
  {
    "text": "a support. ",
    "start": "3442290",
    "end": "3447850"
  },
  {
    "text": "So and now suppose if the\nsupport of v, the size",
    "start": "3447850",
    "end": "3453000"
  },
  {
    "text": "is less than n over 2. So you have the\nvolume is less than. So this means that\nthe volume of this s",
    "start": "3453000",
    "end": "3462710"
  },
  {
    "text": "is less than the\nvolume of v over 2. Because this is a\nregular graph the volume,",
    "start": "3462710",
    "end": "3468740"
  },
  {
    "text": "is really just the\nsize of the set. And then in this\ncase, v transpose Lv",
    "start": "3468740",
    "end": "3474650"
  },
  {
    "text": "over this ratio, v transpose\nLv over norm of v square--",
    "start": "3474650",
    "end": "3482329"
  },
  {
    "text": "this becomes 1 over kappa times\nthe number of edges between s and s bar.",
    "start": "3482330",
    "end": "3488000"
  },
  {
    "text": "And what is the volume of-- what is the v norm square? This v norm square\nis really the size.",
    "start": "3488000",
    "end": "3494270"
  },
  {
    "text": "This is just equal\nto the size of s. The size of s is really just\nthe volume of s over kappa.",
    "start": "3494270",
    "end": "3500510"
  },
  {
    "text": " The volume is the number\nof edges connected",
    "start": "3500510",
    "end": "3505550"
  },
  {
    "text": "to s and its regular graph. That's why the volume is just\nkappa times the size of s. So then you cancel the\nkappa and you get E of s,",
    "start": "3505550",
    "end": "3512780"
  },
  {
    "text": "s bar over the volume of s. So this is the conductance of s.",
    "start": "3512780",
    "end": "3519320"
  },
  {
    "text": "So basically, the\nconductance of s can be written as\nthis, this form. And this form is some kind\nof linear algebraic form.",
    "start": "3519320",
    "end": "3527059"
  },
  {
    "text": "And I think this\nis v transpose Lv over norm of v. This is\ncalled Rayleigh quotient.",
    "start": "3527060",
    "end": "3538580"
  },
  {
    "start": "3538580",
    "end": "3545570"
  },
  {
    "text": "This is named Rayleigh quotient. And the point here is that\nthis Rayleigh quotient connects to the conductance.",
    "start": "3545570",
    "end": "3551960"
  },
  {
    "text": " But of course, it's not\nexact, because it requires",
    "start": "3551960",
    "end": "3558780"
  },
  {
    "text": "when v is binary, right? So if you do\neigenvectors, you are",
    "start": "3558780",
    "end": "3565280"
  },
  {
    "text": "trying to-- so\neigenvectors means you are minimizing\nRayleigh quotient",
    "start": "3565280",
    "end": "3580910"
  },
  {
    "text": "without any constraints, right? Constraints on v, right?",
    "start": "3580910",
    "end": "3590890"
  },
  {
    "text": "But the minimal cut\nis the sparsest cut. Basically means minimize\nRayleigh quotient",
    "start": "3590890",
    "end": "3604130"
  },
  {
    "text": "with the binary constraint. ",
    "start": "3604130",
    "end": "3612349"
  },
  {
    "text": "And in some sense, this\n[INAUDIBLE] the core is really nice even\nwithout a constraint.",
    "start": "3612350",
    "end": "3618363"
  },
  {
    "text": "With a constraint,\nwithout a constraint, you don't really\ndiffer by that much. So actually, the proof\nworks out like something",
    "start": "3618363",
    "end": "3625970"
  },
  {
    "text": "like you first try\nto find eigenvectors.",
    "start": "3625970",
    "end": "3631220"
  },
  {
    "text": "And then somehow you get\nsome real number of v's.",
    "start": "3631220",
    "end": "3636340"
  },
  {
    "text": "And then the eigenvectors\nhave real numbers right, in d. And then you round.",
    "start": "3636340",
    "end": "3641840"
  },
  {
    "text": "You round it into\nbinary vectors. And then you say\nby rounding it, you don't lose too much of\nthe Rayleigh quotient.",
    "start": "3641840",
    "end": "3649400"
  },
  {
    "text": "And that's how the proof,\nroughly speaking, works. So I guess that's the intuition.",
    "start": "3649400",
    "end": "3660560"
  },
  {
    "text": "And all of this can be\nextended to a weighted graph. The intuition is the same for\nweighted graph or for non--",
    "start": "3660560",
    "end": "3668760"
  },
  {
    "text": "for graph that are not\nsimilar, not regular, and also for graph that are weighted. So here, the graph are\njust binary, like 0, 1.",
    "start": "3668760",
    "end": "3675270"
  },
  {
    "text": "There's no one the other ways. You can also do it\nfor weighted graph. ",
    "start": "3675270",
    "end": "3689600"
  },
  {
    "text": "So, great. So I think-- I hope that I've\nconvinced you that eigenvectors",
    "start": "3689600",
    "end": "3694790"
  },
  {
    "text": "are very related to the\ngraph clustering by these two examples, stochastic block model\nand this worst-case situation.",
    "start": "3694790",
    "end": "3702650"
  },
  {
    "text": "And this kind of\nalgorithm has been used. So if you do this on spectral\nclustering, this is--",
    "start": "3702650",
    "end": "3707780"
  },
  {
    "text": " and you can actually use this--",
    "start": "3707780",
    "end": "3713400"
  },
  {
    "text": "OK, how do I say this? So the materials I\npresented, this mostly come",
    "start": "3713400",
    "end": "3719430"
  },
  {
    "text": "from the theoretical\ncomputer science community. And there it that\ndoesn't have much to do",
    "start": "3719430",
    "end": "3725460"
  },
  {
    "text": "with machine learning, right? So what the people care\nabout is that you just want to partition a\ngraph into two clusters.",
    "start": "3725460",
    "end": "3731365"
  },
  {
    "text": "So you're going to\nhave to go-- machine learning to these kind of\nproblems and study this. And I think there is a so-called\nspectral clustering approach--",
    "start": "3731365",
    "end": "3744450"
  },
  {
    "text": "spectral clustering. This was bring to\nmachine-learning community I think around 2000, I think by--",
    "start": "3744450",
    "end": "3750780"
  },
  {
    "text": "I guess I said this paper by\nShi and Malik and Ng, Jordan,",
    "start": "3750780",
    "end": "3761840"
  },
  {
    "text": "Weiss. This one's 2000.",
    "start": "3761840",
    "end": "3768220"
  },
  {
    "text": "And the way that you do it\nis that you find a graph from the machine-learning\ndata, and then you",
    "start": "3768220",
    "end": "3774400"
  },
  {
    "text": "apply this algorithm. So basically, this\nbrings us to question",
    "start": "3774400",
    "end": "3780040"
  },
  {
    "text": "on how to choose\nthis graph, so how to choose or design the\ngraph, because the graph--",
    "start": "3780040",
    "end": "3789170"
  },
  {
    "text": "in TCS, the graph was given\nto you, maybe some graph that somebody give you.",
    "start": "3789170",
    "end": "3794240"
  },
  {
    "text": "But machine learning, you have\nto somehow choose your graph, right? So in Andrew's paper, the\ndefinition of the graph",
    "start": "3794240",
    "end": "3804641"
  },
  {
    "text": "is something like this. So you first say you're\ngiven some raw data,",
    "start": "3804642",
    "end": "3811615"
  },
  {
    "text": "say x1 up to x10. So these are in\nbetween data points. And then you define a graph G\nto be something like Gi and j.",
    "start": "3811615",
    "end": "3823170"
  },
  {
    "text": "This is a weighted graph. Well, I didn't really\ndiscuss the weighted graph, but there's a natural\nextension to weighted graph.",
    "start": "3823170",
    "end": "3829830"
  },
  {
    "text": "And in the weighted graph,\nthe weights between i and j is something like\nexponential minus xi minus xj",
    "start": "3829830",
    "end": "3837480"
  },
  {
    "text": "2 norm over 2 sigma squared. I guess this is\nprobably is something that is very familiar to you.",
    "start": "3837480",
    "end": "3842980"
  },
  {
    "text": "This is just the RBF\nkernel, the Gaussian kernel. So you define this with\nsome training parameters.",
    "start": "3842980",
    "end": "3849967"
  },
  {
    "text": "Or you can have some\nother variables, right? So I guess you define a\ngraph based on some distances",
    "start": "3849967",
    "end": "3855330"
  },
  {
    "text": "between your examples.  And then what we do\nis we say you do this.",
    "start": "3855330",
    "end": "3862065"
  },
  {
    "text": "You get a spectral cluster. You run a-- you get\nthe eigenvectors. ",
    "start": "3862065",
    "end": "3871370"
  },
  {
    "text": "So the first time,\nyou define a graph G and get eigenvectors\nof the Laplacian or normalized adjacency matrix.",
    "start": "3871370",
    "end": "3877865"
  },
  {
    "text": " And here is not\nonly two clusters. You can do multiple clusters.",
    "start": "3877865",
    "end": "3884000"
  },
  {
    "text": "And when you do multiple\nclusters, what you do is you say you get eigenvectors,\nsay, u1, u2, up to uk.",
    "start": "3884000",
    "end": "3893869"
  },
  {
    "text": "Suppose you want\nto have k cluster. And this is a matrix of\ndimension R of n by k.",
    "start": "3893870",
    "end": "3903400"
  },
  {
    "text": "So each column is\nan eigenvector. And you have three of\nthese eigenvectors.",
    "start": "3903400",
    "end": "3910359"
  },
  {
    "text": "And now what you do\nis you say you take the rows as the embeddings. ",
    "start": "3910360",
    "end": "3918530"
  },
  {
    "text": "Or in the modern word,\nyou have representation, because I probably-- some of\nyou heard of representation learning, and for the full--",
    "start": "3918530",
    "end": "3930490"
  },
  {
    "text": "the ith example.  So basically, you can--",
    "start": "3930490",
    "end": "3936180"
  },
  {
    "text": "so for every example xi, now\nit becomes represented as-- maybe let's call them 0 vi,\nvi, which is the dimension k.",
    "start": "3936180",
    "end": "3946400"
  },
  {
    "text": "And k is-- k corresponds to\nhow many eigenvectors you take.",
    "start": "3946400",
    "end": "3951723"
  },
  {
    "text": "And then you've got these low\ndimensional representations. Maybe it'll tell you\nsomething up to all three. And then in the original\npaper of Andrew's paper,",
    "start": "3951723",
    "end": "3960079"
  },
  {
    "text": "I think you do some kind of\nother-- another k-mean cluster and some other\nclusters, so k-means.",
    "start": "3960080",
    "end": "3968450"
  },
  {
    "text": "I'm not-- I guess probably\nyou've heard of k-means-- k-means on the\nrepresentations to vn,",
    "start": "3968450",
    "end": "3981440"
  },
  {
    "text": "and to cluster them again. So this is the so-called\nspectral clustering algorithm.",
    "start": "3981440",
    "end": "3987500"
  },
  {
    "text": "And there were actually later-- I think around 2014, 2013,\nthere were a few papers",
    "start": "3987500",
    "end": "3992570"
  },
  {
    "text": "to analyze this and show\nthat you can actually get reasonable representations\nand clusters by using",
    "start": "3992570",
    "end": "4000987"
  },
  {
    "text": "this approach. ",
    "start": "4000987",
    "end": "4007520"
  },
  {
    "text": "Any questions? ",
    "start": "4007520",
    "end": "4019740"
  },
  {
    "text": "So what are the-- or\nwhat's the issue with this? The issue with this\nis that the graph G",
    "start": "4019740",
    "end": "4027140"
  },
  {
    "text": "could be not very meaningful. So in high dimension, so all the\ndata points are very far away",
    "start": "4027140",
    "end": "4037043"
  },
  {
    "text": "from each other. ",
    "start": "4037043",
    "end": "4045160"
  },
  {
    "text": "All the training data points-- I should be precise-- ",
    "start": "4045160",
    "end": "4058740"
  },
  {
    "text": "far away from each other. ",
    "start": "4058740",
    "end": "4067680"
  },
  {
    "text": "And the Euclidean\ndistance becomes-- the Euclidean distance between\nthese training data points becomes pretty much\nmeaningless, because",
    "start": "4067680",
    "end": "4077405"
  },
  {
    "text": "in particular, Euclidean\nsystem of cat and dog versus the Euclidean\ndifference between dog and dog,",
    "start": "4077405",
    "end": "4082920"
  },
  {
    "text": "you probably wouldn't\nsee much differences, because two dogs\ncould still have very big Euclidean distance--",
    "start": "4082920",
    "end": "4089099"
  },
  {
    "text": "two random dogs. And I think this is\nthe-- is sometimes the problem with [INAUDIBLE]\nbecause the graph itself is not",
    "start": "4089100",
    "end": "4097734"
  },
  {
    "text": "meaningful. So you need to find the\nsparsest cut for the graph. If the graph itself\nis not very useful,",
    "start": "4097735",
    "end": "4103210"
  },
  {
    "text": "even finding the sparsest\ncut is not that important. It's not that useful for you.",
    "start": "4103210",
    "end": "4108699"
  },
  {
    "text": "So that's why the\ntheory, the analysis for this spectral\nclustering algorithm,",
    "start": "4108700",
    "end": "4113790"
  },
  {
    "text": "doesn't really deliver that\nmuch, because it didn't really consider how the\ngraph was generated.",
    "start": "4113790",
    "end": "4118799"
  },
  {
    "text": "All of this theory says that\nif you're given good graph, you can find the sparsest\ncut for this graph using",
    "start": "4118800",
    "end": "4125339"
  },
  {
    "text": "this approach. But it doesn't\nreally say anything about how the\ngraph is generated.",
    "start": "4125340",
    "end": "4130630"
  },
  {
    "text": "So I think for the\nlast 15 minutes, I'm going to discuss,\nbriefly discuss, one of the--",
    "start": "4130630",
    "end": "4137520"
  },
  {
    "text": "This is one of the work\nin my group recently, so where we try to re-use this\nclassic idea, but use it for--",
    "start": "4137520",
    "end": "4147230"
  },
  {
    "text": "in a different way.  So this is in thiks paper by\nHaochen et al in my group.",
    "start": "4147230",
    "end": "4157769"
  },
  {
    "text": "So what we are trying\nto do is that we say you consider infinite graph. ",
    "start": "4157770",
    "end": "4170670"
  },
  {
    "text": "So G, v, w. So this is the vertices.",
    "start": "4170670",
    "end": "4175899"
  },
  {
    "text": "This is the weights on the\nedge, and where we take v to be all the possible inputs.",
    "start": "4175899",
    "end": "4185830"
  },
  {
    "text": "So this is all possible\ndata, data points.",
    "start": "4185830",
    "end": "4194340"
  },
  {
    "text": "So this graph would depend on\nthe population space, right? So actually, it's\nthe best fit space",
    "start": "4194340",
    "end": "4200130"
  },
  {
    "text": "of all possible,\nlet's say, images. And your graph is defined on-- each image corresponds\nto a vertex.",
    "start": "4200130",
    "end": "4206310"
  },
  {
    "text": "So before, the graph has size\nlittle n, where-- oh, sorry. This is E. So before, the\ngraph has size little n, right?",
    "start": "4206310",
    "end": "4214080"
  },
  {
    "text": "It's a little n by\nlittle n matrix. And now the graph has\na much bigger size.",
    "start": "4214080",
    "end": "4219809"
  },
  {
    "text": "The size is the same\nas the commonality of all possible data points\nwhich could be infinity.",
    "start": "4219810",
    "end": "4225790"
  },
  {
    "text": " So it's possibility,\nmaybe, let's say--",
    "start": "4225790",
    "end": "4233110"
  },
  {
    "text": "possibility, let's\nsay, you have to find the number of possible images\nthat then could be exponential.",
    "start": "4233110",
    "end": "4244770"
  },
  {
    "text": "So firstly, let's say we\nhave exponential size graph. ",
    "start": "4244770",
    "end": "4252739"
  },
  {
    "text": "So on this graph, what you do\nis you define w, x, x prime.",
    "start": "4252740",
    "end": "4258140"
  },
  {
    "text": "The weight is split in\ntwo nodes, two vertices. Let's say we find\nthis to be large only",
    "start": "4258140",
    "end": "4267170"
  },
  {
    "text": "when x and x prime are close-- are close in L2 distance.",
    "start": "4267170",
    "end": "4276829"
  },
  {
    "text": "So I'm still using L2 distance. I'm still probably using-- I didn't-- I'm not specifying\nexactly what's the definition",
    "start": "4276830",
    "end": "4282048"
  },
  {
    "text": "here, because I think that\nrequires too much trouble, which I cannot\nfit in 10 minutes. But still, we are using--",
    "start": "4282048",
    "end": "4287510"
  },
  {
    "text": "pretty much you\ncan think of this as almost the same as\nthe previous definition of the graph, where x\nand x2 prime are close.",
    "start": "4287510",
    "end": "4295130"
  },
  {
    "text": "But I guess the point is\nthat this is very close. So before, you have to choose\nthe signal very subtly,",
    "start": "4295130",
    "end": "4304910"
  },
  {
    "text": "because all the points are\nvery far away from each other. But now you say that I don't\nhave all those points that far",
    "start": "4304910",
    "end": "4310820"
  },
  {
    "text": "away from each other. I just care about\nthose two points that are very close\nto each other, right? So suppose you have\ntwo dogs, running dogs.",
    "start": "4310820",
    "end": "4316880"
  },
  {
    "text": "You say they are not close. If you only have one\ndog, and then you have a perturbation of that\nsame dog, you say there are two.",
    "start": "4316880",
    "end": "4324770"
  },
  {
    "text": "They are dogs that are\nconnected to each other. So then this graph\nbecomes more meaningful",
    "start": "4324770",
    "end": "4329840"
  },
  {
    "text": "because you only connect very\nnearby cats and dogs or very",
    "start": "4329840",
    "end": "4335510"
  },
  {
    "text": "nearby images. And then, so the graph\nbecomes more meaningful,",
    "start": "4335510",
    "end": "4340560"
  },
  {
    "text": "so the pros is that the\ngraph is more meaningful.",
    "start": "4340560",
    "end": "4346592"
  },
  {
    "text": " I guess the cons is that it\nbecomes infinite dimensional.",
    "start": "4346592",
    "end": "4353480"
  },
  {
    "text": " And you don't have this\ngraph because you don't know",
    "start": "4353480",
    "end": "4359260"
  },
  {
    "text": "all the possible data points. You only have some\nsample data points, so",
    "start": "4359260",
    "end": "4366199"
  },
  {
    "text": "infinite or exponential\nexpansion dimension.",
    "start": "4366200",
    "end": "4372153"
  },
  {
    "text": "And you don't have\naccess to this graph. ",
    "start": "4372153",
    "end": "4383360"
  },
  {
    "text": "So what we do is the following. So the way we fix the\ncolumns is the following. And also maybe\nanother way-- cons--",
    "start": "4383360",
    "end": "4389200"
  },
  {
    "text": "is that even the eigenvector\nitself, right, the eigenvector",
    "start": "4389200",
    "end": "4396290"
  },
  {
    "text": "is also high-dimensional, right? It's infinite dimensional\nbecause the eigenvector--",
    "start": "4396290",
    "end": "4402827"
  },
  {
    "text": "the dimension of the\neigenvector is the same as the dimension of the graph.",
    "start": "4402827",
    "end": "4410550"
  },
  {
    "text": "So over here what\nwe are doing is that we use the new ideas,\nthe different ideas,",
    "start": "4410550",
    "end": "4417590"
  },
  {
    "text": "you know, to kind of-- actually, the real research\nis the reverse direction. We somehow try to explain\nthe different ideas.",
    "start": "4417590",
    "end": "4424250"
  },
  {
    "text": "But here in this context,\nyou can think of this as you use the parametric\nnetwork of ideas",
    "start": "4424250",
    "end": "4429710"
  },
  {
    "text": "to try to deal with these cons. So what you do is\nyou say, suppose you have an eigenvector nu.",
    "start": "4429710",
    "end": "4436160"
  },
  {
    "text": "This is an eigenvector. So this is the eigenvector nu.",
    "start": "4436160",
    "end": "4442520"
  },
  {
    "text": "And here, the eigenvector is\na high-dimensional vector. So you can say\nthis is nu x, where it's indexed by all\nthe possible data",
    "start": "4442520",
    "end": "4449960"
  },
  {
    "text": "points in the capital X, right? This is of dimension something\nlike maybe R to the capital N",
    "start": "4449960",
    "end": "4457040"
  },
  {
    "text": "or R to infinity,\ndepending on how many vertices are in your set.",
    "start": "4457040",
    "end": "4462739"
  },
  {
    "text": "And you don't even have space\nto save all of this, yeah? Even if it's a single\nvector you don't have any space to save it.",
    "start": "4462740",
    "end": "4469349"
  },
  {
    "text": "But what you do is you say\nyou represent this u, u sub x",
    "start": "4469350",
    "end": "4477130"
  },
  {
    "text": "by a neural network applied\non the raw data point x,",
    "start": "4477130",
    "end": "4488250"
  },
  {
    "text": "so where f theta is a\nparameterized model.",
    "start": "4488250",
    "end": "4494340"
  },
  {
    "start": "4494340",
    "end": "4500090"
  },
  {
    "text": "So if you do this,\nthen at least you can describe the\neigenvector by theta. Now you don't have to specify\nall the capital N numbers",
    "start": "4500090",
    "end": "4511240"
  },
  {
    "text": "to specify the eigenvector. You only have to\nspecify the theta to describe this eigenvector.",
    "start": "4511240",
    "end": "4517390"
  },
  {
    "text": "Of course, if you believe that\nf theta is powerful enough, then you can express\neigenvectors.",
    "start": "4517390",
    "end": "4522460"
  },
  {
    "text": "But at first, obviously the\nproblem wouldn't be enough. So you have to make\nsome assumption that neural networks\ncan represent",
    "start": "4522460",
    "end": "4528940"
  },
  {
    "text": "these kind of eigenvectors. But suppose under\nthat assumption, then you can at least represent\nthe eigenvectors by theta.",
    "start": "4528940",
    "end": "4537350"
  },
  {
    "text": "And now basically the\nquestion becomes, or so the question changes\nto you want to find",
    "start": "4537350",
    "end": "4543910"
  },
  {
    "text": "theta such that this\nvector f theta x,",
    "start": "4543910",
    "end": "4550710"
  },
  {
    "text": "this very\nhigh-dimensional vector, is an eigenvector\nof the graph G.",
    "start": "4550710",
    "end": "4563643"
  },
  {
    "text": "So at least you\nare trying to find a low-dimensional--\nyou are trying to find a parameter theta. You are not trying to find\nthe-- a high-dimensional vector",
    "start": "4563643",
    "end": "4570370"
  },
  {
    "text": "anymore. And it turns out\nthat if you do this,",
    "start": "4570370",
    "end": "4575650"
  },
  {
    "text": "I guess maybe\neigenvector, Laplacian. Let's see.",
    "start": "4575650",
    "end": "4581230"
  },
  {
    "text": "I think I have time to-- ",
    "start": "4581230",
    "end": "4586577"
  },
  {
    "text": "it turns out that\nif you do this, then this is basically\ntrying to do the--",
    "start": "4586577",
    "end": "4594150"
  },
  {
    "text": "this gives a-- you\ncan use an algorithm to try to achieve this.",
    "start": "4594150",
    "end": "4599869"
  },
  {
    "text": "I guess I'm trying to-- let me\nsee whether I have time to-- I guess what we can\ndo is the following.",
    "start": "4599870",
    "end": "4605490"
  },
  {
    "text": "So what you do is you\nsay, I'm going to-- how do I find the\neigenvector of L like this?",
    "start": "4605490",
    "end": "4612750"
  },
  {
    "text": "So suppose I have the access to\nthe whole graph, which I don't. But suppose I have it.",
    "start": "4612750",
    "end": "4618890"
  },
  {
    "text": "What I can do is I can\nminimize the following thing. So I can say I'm\ngoing to minimize",
    "start": "4618890",
    "end": "4624139"
  },
  {
    "text": "F. Let me write it down-- I L n. ",
    "start": "4624140",
    "end": "4634884"
  },
  {
    "text": "So maybe that's what it is. [INAUDIBLE] isn't\ngiven by images.",
    "start": "4634884",
    "end": "4643130"
  },
  {
    "text": "So first of all, I\nclaim that this gives the top eigenvector of A bar.",
    "start": "4643130",
    "end": "4652980"
  },
  {
    "text": "This is because--\nthis is something that I probably wouldn't have\ntime to explain that much.",
    "start": "4652980",
    "end": "4660480"
  },
  {
    "text": "But if you want to fit a\nlow graph matrix [INAUDIBLE]",
    "start": "4660480",
    "end": "4667230"
  },
  {
    "text": "the top K eigenvector. So if you want to fit a low\nrank matrix to the matrix A bar,",
    "start": "4667230",
    "end": "4676110"
  },
  {
    "text": "the best fit would be to\nuse eigenvectors of A bar. ",
    "start": "4676110",
    "end": "4685800"
  },
  {
    "text": "You can invoke a\ntheorem to show this. Basically, F is going\nto be some version of--",
    "start": "4685800",
    "end": "4691980"
  },
  {
    "text": "the minimizer of this\nwill be some version of the eigenvector. So I think F will be--\nthe minimizer of this will be some scaling\nof the eigenvectors.",
    "start": "4691980",
    "end": "4699630"
  },
  {
    "text": "And then if you\nuse this objective, then you can replace the capital\nF, which is non-parametric--",
    "start": "4699630",
    "end": "4705120"
  },
  {
    "text": "it's a very big matrix-- by-- so you can say that\nthe capital F not-- for now is you--",
    "start": "4705120",
    "end": "4711239"
  },
  {
    "text": "supposed to be something\nlike this, right? And then you write\nit out as this. You write it as f\ntheta x transpose,",
    "start": "4711240",
    "end": "4722340"
  },
  {
    "text": "maybe x1 up to x\ntheta, x n transpose.",
    "start": "4722340",
    "end": "4727940"
  },
  {
    "text": "So you replace the row by\nthe paramaterized version. So you say that every row now\nis a network of the raw data.",
    "start": "4727940",
    "end": "4736519"
  },
  {
    "text": "And then what you can get\nis that this will be-- if you write this as--",
    "start": "4736520",
    "end": "4743110"
  },
  {
    "text": "in this version,\nthis is like a-- first of all, you\nwrite the real sum as the sum over ij in\nN, A bar ij minus--",
    "start": "4743110",
    "end": "4753000"
  },
  {
    "text": "so FF transpose i and j. The ijth entry\nactually is the ith row",
    "start": "4753000",
    "end": "4759929"
  },
  {
    "text": "in a product with the jth row. So that's why this is equal\nto f theta xi times f theta xj",
    "start": "4759930",
    "end": "4771040"
  },
  {
    "text": "square.  And now I can change this\nto-- instead of minimizing F,",
    "start": "4771040",
    "end": "4778320"
  },
  {
    "text": "now you are minimizing theta.  And I guess I don't have time\nto go through all the details.",
    "start": "4778320",
    "end": "4787170"
  },
  {
    "text": "This is basically-- this\nis a now objective function that you can optimize.",
    "start": "4787170",
    "end": "4793423"
  },
  {
    "text": "Of course, the problem\nis that you still have this sum, this big sum. You can replace this by\nthe empirical version.",
    "start": "4793423",
    "end": "4799349"
  },
  {
    "text": " So you can get\nminimize over theta.",
    "start": "4799350",
    "end": "4804810"
  },
  {
    "text": "You can take some\nrandom samples.",
    "start": "4804810",
    "end": "4809890"
  },
  {
    "text": "So you can take--  so I'm not sure whether I have\na way to simply write this.",
    "start": "4809890",
    "end": "4819630"
  },
  {
    "text": "OK, maybe I'll just say\nyou can sub-sample this. ",
    "start": "4819630",
    "end": "4828510"
  },
  {
    "text": "Sub-- so estimate this\nusing an estimate, using an empirical estimate,\nusing empirical examples.",
    "start": "4828510",
    "end": "4836570"
  },
  {
    "start": "4836570",
    "end": "4843780"
  },
  {
    "text": "And actually, it turns out that\nyou can simplify this formula. This will be something similar\nto the contrastive learning",
    "start": "4843780",
    "end": "4850710"
  },
  {
    "text": "algorithm that is\nused in practice. I guess this part, I don't\nreally have time to show.",
    "start": "4850710",
    "end": "4857482"
  },
  {
    "text": "I guess I will refer\nyou to the paper. ",
    "start": "4857482",
    "end": "4862969"
  },
  {
    "text": "I think probably I\nshould just stop here. Are there any questions first?",
    "start": "4862970",
    "end": "4868415"
  },
  {
    "text": " I know this part\nis a little vague.",
    "start": "4868415",
    "end": "4874510"
  },
  {
    "text": "Feel free to ask any questions. Do you know of any\ncontrastive learning",
    "start": "4874510",
    "end": "4880170"
  },
  {
    "text": "paper we could look at\noff the top of your head? The paper I think probably\nis good to write just a--",
    "start": "4880170",
    "end": "4886850"
  },
  {
    "text": "[INAUDIBLE] Yeah. I think the loss is not\nexactly the contrastive learning loss used in practice.",
    "start": "4886850",
    "end": "4893040"
  },
  {
    "text": "So we're going to have something\nwe call spectral contrast enhanced, so which-- so\nbasically, actually this--",
    "start": "4893040",
    "end": "4898050"
  },
  {
    "text": "that-- if we have all this at top then\nthis stuff is pretty trivial. ",
    "start": "4898050",
    "end": "4905903"
  },
  {
    "text": "So eventually, you could\nsimplify this a little bit. You could write this, that you\ngot one term which is minus 1/2",
    "start": "4905903",
    "end": "4912449"
  },
  {
    "text": "of theta xi of theta xj. ",
    "start": "4912450",
    "end": "4919330"
  },
  {
    "text": "And this is something that the-- the term that tries to make two\nexemplars closer to each other.",
    "start": "4919330",
    "end": "4926110"
  },
  {
    "text": "And there's another term\nthat tries to contrast them. But there's another--\nso anyway, so I",
    "start": "4926110",
    "end": "4932350"
  },
  {
    "text": "guess I'll probably just refer\nyou to the paper, of our paper. I think the title of\nthe paper is that,",
    "start": "4932350",
    "end": "4937989"
  },
  {
    "text": "somewhat \"Provable\nSelf-Supervised Learning Via",
    "start": "4937990",
    "end": "4952220"
  },
  {
    "text": "Contrast-- Spectral Contrastive Loss.\" Spectral contrastive\nloss-- something like this.",
    "start": "4952220",
    "end": "4960050"
  },
  {
    "text": "I think from this, you\ncan search the fun title. ",
    "start": "4960050",
    "end": "4965780"
  },
  {
    "text": "So before-- the session\njust before this one, you mentioned looking up--",
    "start": "4965780",
    "end": "4973400"
  },
  {
    "text": "that you can look\nat the eigenvectors, line them up, and then complete\nthe first row of that matrix.",
    "start": "4973400",
    "end": "4979820"
  },
  {
    "text": "And say that corresponds\nto the first data points.",
    "start": "4979820",
    "end": "4986000"
  },
  {
    "text": "What exactly is the worst one? Is it-- how come the first\nand second row are similar?",
    "start": "4986000",
    "end": "4994380"
  },
  {
    "text": "Then the first two data\npoints should be similar. ",
    "start": "4994380",
    "end": "5002260"
  },
  {
    "text": "I think I got the question. So I think there is something\nthat I kind of like--",
    "start": "5002260",
    "end": "5009790"
  },
  {
    "text": "I understand why there\nis a lot of confusion, because I skipped\nsomething about the k, how do you deal with k clusters.",
    "start": "5009790",
    "end": "5016119"
  },
  {
    "text": "But I think this could be\nseen where you have only-- if you take a\nlittle leap of faith between k clusters\nand two clusters,",
    "start": "5016120",
    "end": "5022480"
  },
  {
    "text": "you're just gonna\nsay this two cluster. And then if you look at the--",
    "start": "5022480",
    "end": "5027880"
  },
  {
    "text": "let's see. So where did we discuss this? I think we discussed\nthis somewhat implicit",
    "start": "5027880",
    "end": "5033400"
  },
  {
    "text": "in-- several times. So I guess, for example,\nsuppose you go back to here. And recall that the second\nargument beta 1 after beta n.",
    "start": "5033400",
    "end": "5041770"
  },
  {
    "text": "Right? And we discussed that\nyou take a threshold. And then you can separate the\ntwo groups with threshold.",
    "start": "5041770",
    "end": "5049179"
  },
  {
    "text": "So in this case, basically\nsuppose you have two clusters. And in this case,\nbasically the beta i",
    "start": "5049180",
    "end": "5055890"
  },
  {
    "text": "is your representation\nof the ith vertex. ",
    "start": "5055890",
    "end": "5062920"
  },
  {
    "text": "So that's the row right\nthere, and beta y's the first row, right? Beta 2 is the second row, right? So beta i is the representation\nof the ith vertex.",
    "start": "5062920",
    "end": "5069340"
  },
  {
    "text": "And why beta i is better\nthan the row data? I think this is because at least\nwith a threshold of beta i,",
    "start": "5069340",
    "end": "5077430"
  },
  {
    "text": "you get the groups right. So basically, in\nsome sense, beta i-- in some sense, maybe the\nideal thing is as follows.",
    "start": "5077430",
    "end": "5084090"
  },
  {
    "text": "So suppose you in the\nstochastic block model get this. And then I guess\nyou probably can",
    "start": "5084090",
    "end": "5089840"
  },
  {
    "text": "agree that these numbers\nare better representations than our original\ndata, because now, you",
    "start": "5089840",
    "end": "5096910"
  },
  {
    "text": "make all the vertices\nin the same group to 1.",
    "start": "5096910",
    "end": "5102442"
  },
  {
    "text": "You lost all the\nother information. You just get-- the\nrepresentation just exactly tells you about\na group membership.",
    "start": "5102442",
    "end": "5108220"
  },
  {
    "text": "And you don't know\nanything about else, right? So the group membership is\nthe only thing you care about.",
    "start": "5108220",
    "end": "5114460"
  },
  {
    "text": "So that's why these\nnumbers are more-- better representations\nthan the--",
    "start": "5114460",
    "end": "5119790"
  },
  {
    "text": "Is it similar to the low\nrank matrix approximation, approximate a low rank matrix\nand that's just a better",
    "start": "5119790",
    "end": "5129820"
  },
  {
    "text": "representation, because you've\ntaken the most important parts of the representation?",
    "start": "5129820",
    "end": "5134829"
  },
  {
    "text": "Exactly. And what's the\nmost important one? The most important one here-- in this case, the\nmost important one",
    "start": "5134830",
    "end": "5139929"
  },
  {
    "text": "is the clustering\nstructure, so which group you belong to, right? So that's why the\nonly thing you care--",
    "start": "5139930",
    "end": "5147587"
  },
  {
    "text": "suppose you think that's the\nmost important information, that your representation\nshould just be that. You ignore any\nother information.",
    "start": "5147587",
    "end": "5153035"
  },
  {
    "text": "You just say the group\nID is my representation. And that's the best\nrepresentation. But that's only the\ncase in this case, where",
    "start": "5153035",
    "end": "5159500"
  },
  {
    "text": "we said there's two clusters. Right. [AUDIO OUT] ",
    "start": "5159500",
    "end": "5165754"
  },
  {
    "text": "Then we want to-- we care about the 2-cluster\nkind of representation,",
    "start": "5165754",
    "end": "5174880"
  },
  {
    "text": "but we also care about maybe\nlike a 3-cluster representation and how close things\nare based on that.",
    "start": "5174880",
    "end": "5181090"
  },
  {
    "text": "And so by taking\nmultiple eigenvectors, we can get a bigger\npicture just--",
    "start": "5181090",
    "end": "5187480"
  },
  {
    "text": "in this cluster, we're not. Exactly, exactly. So if you have\nmore eigenvectors, then you can get 3-cluster\ninformation or even",
    "start": "5187480",
    "end": "5195280"
  },
  {
    "text": "more information. And also, this can be-- some of this information\ncan be recombined to get even richer information, right?",
    "start": "5195280",
    "end": "5201670"
  },
  {
    "text": "So because eventually,\nyou'll probably used this representation\nby a linear plus--",
    "start": "5201670",
    "end": "5207670"
  },
  {
    "text": "use a linear hat\non top of it. fit. So suppose you have two type\nof information in your system. Then you'd have to combine\nthem to get more information.",
    "start": "5207670",
    "end": "5215060"
  },
  {
    "text": "Yes. But you are right. So basically, you can\nget more eigenvectors. You get more richer\ninformation from the graph.",
    "start": "5215060",
    "end": "5221179"
  },
  {
    "text": " Yeah, so essentially,\nyou are-- it's kind",
    "start": "5221180",
    "end": "5227270"
  },
  {
    "text": "of like making experimentation. You distill the\ninformation in a graph to smaller amount\nof information.",
    "start": "5227270",
    "end": "5234633"
  },
  {
    "text": "And the question we\nare trying to answer is then what information you\nkeep in the eigenvectors. So it's not that surprising\nthat the eigenvectors",
    "start": "5234633",
    "end": "5241310"
  },
  {
    "text": "has more specific\ninformation about the graph.",
    "start": "5241310",
    "end": "5247160"
  },
  {
    "text": "The question is,\nwhat can we glean? And the graph\nintuition is that it does keep the clustering\nstructure in the graph,",
    "start": "5247160",
    "end": "5255930"
  },
  {
    "text": "but not other things. I used the low-- the smallest\neigenvectors are trying to keep the class and\nstructure of the graph.",
    "start": "5255930",
    "end": "5262790"
  },
  {
    "text": " OK.",
    "start": "5262790",
    "end": "5267820"
  },
  {
    "text": "Great. I think this will be the\nend of the quarter, I guess. I hope you liked the course.",
    "start": "5267820",
    "end": "5273280"
  },
  {
    "text": "I guess we discussed\nquite a bunch of topics. Actually, this quarter I think\nwe covered the most compared",
    "start": "5273280",
    "end": "5279970"
  },
  {
    "text": "to all the previous quarters,\nbecause-- partly because we have more than-- we have 10 minutes every\nclass and every lecture.",
    "start": "5279970",
    "end": "5287170"
  },
  {
    "text": "And also, we have two more-- well, two more lectures,\nbecause we have fewer holidays in this quarter.",
    "start": "5287170",
    "end": "5294070"
  },
  {
    "text": "Yeah, I guess I\nhope you like it. Thanks. Thanks so much for attending.",
    "start": "5294070",
    "end": "5299969"
  },
  {
    "start": "5299970",
    "end": "5305000"
  }
]