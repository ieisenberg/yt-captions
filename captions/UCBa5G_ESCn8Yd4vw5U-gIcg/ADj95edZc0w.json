[
  {
    "text": "So I guess last time we talked\nabout Gaussian discriminant",
    "start": "5080",
    "end": "10469"
  },
  {
    "text": "analysis. I'll have a very brief\nreview and talk about some",
    "start": "10470",
    "end": "15849"
  },
  {
    "text": "of the remaining points\nthat I didn't get time to mention last time. And then I'm going to move\non to another case about spam",
    "start": "15849",
    "end": "22230"
  },
  {
    "text": "filtering, where we're\ngoing to have discrete x instead of continuous x.",
    "start": "22230",
    "end": "27359"
  },
  {
    "text": "So last time, we talked about\nGaussian discriminant analysis. And the general idea\nis that you first model",
    "start": "27359",
    "end": "33809"
  },
  {
    "text": "p of x given y, and p of y. And what we do is that\nwe say p of x given",
    "start": "33809",
    "end": "41260"
  },
  {
    "text": "y is a Gaussian for\ny is 1 or y is 0. I guess I need to\nremember to write bigger.",
    "start": "41260",
    "end": "48660"
  },
  {
    "text": "So x given y is equal to 0 is\nfrom some Gaussian distribution",
    "start": "48660",
    "end": "53880"
  },
  {
    "text": "with mu 0 and covariance sigma. And x given y is 1 is from\nsome Gaussian distribution mu",
    "start": "53880",
    "end": "60390"
  },
  {
    "text": "And we recall that we have this\nillustrative situations where",
    "start": "60390",
    "end": "66010"
  },
  {
    "text": "you have some\nexamples like this. These are positive examples. There are some\nnegative examples.",
    "start": "66010",
    "end": "71530"
  },
  {
    "text": "And you kind of believe that\neach of this subpopulation come from a Gaussian\ndistribution,",
    "start": "71530",
    "end": "77630"
  },
  {
    "text": "with different means, but\nwith the same covariance. So that's the\nmethodology we have.",
    "start": "77630",
    "end": "83049"
  },
  {
    "text": "That's the starting\npoint we have last time. And then, what we\ndo is that we say, after you have this\nprobabilistic model,",
    "start": "83049",
    "end": "90600"
  },
  {
    "text": "we also have this p of\ny is 1 is equal to phi. After you have the\nprobabilistic model,",
    "start": "90600",
    "end": "96680"
  },
  {
    "text": "then you can learn the\nprobabilistic model from data by MLE.",
    "start": "96680",
    "end": "102469"
  },
  {
    "text": "So we learn by maximize. We take the arg max over our\nparameters, phi, mu 0, mu 1,",
    "start": "102470",
    "end": "112509"
  },
  {
    "text": "and sigma. And we take the arg max\nof the log likelihood. It's the same as the likelihood.",
    "start": "112509",
    "end": "119538"
  },
  {
    "text": "Arg max of the log\nlikelihood is the same as the max of the likelihood. So I'm just writing\nthe log likelihood,",
    "start": "119539",
    "end": "125340"
  },
  {
    "text": "which is the log of the\nprobability of xi given yi,",
    "start": "125340",
    "end": "134810"
  },
  {
    "text": "plus the sum of the log\nof the probability y. Under, I guess technically\nunder this parameter, phi, mu 1,",
    "start": "134810",
    "end": "148069"
  },
  {
    "text": "mu 2, sigma.",
    "start": "148069",
    "end": "154499"
  },
  {
    "text": "I don't have to write\nphi here, just because it doesn't depend on phi. OK. So that's the methodology.",
    "start": "154499",
    "end": "160660"
  },
  {
    "text": "And then we skipped\na lot of steps, because these are\nhomework questions.",
    "start": "160660",
    "end": "165830"
  },
  {
    "text": "And we told-- and I told you\nthat the solution of this MLE problem, you can analytically\nsolve it in this case,",
    "start": "165830",
    "end": "171850"
  },
  {
    "text": "because this objective\nfunction is nice enough, it's kind of like a\nquadratic function. So you can solve the\nmaximization problem",
    "start": "171850",
    "end": "178750"
  },
  {
    "text": "analytically, and you get some\nformula for these quantities.",
    "start": "178750",
    "end": "184960"
  },
  {
    "text": "So the formula for\nphi was something like something of this form.",
    "start": "184960",
    "end": "190909"
  },
  {
    "text": "Of this indicator function,\nwhich is here-- basically,",
    "start": "190910",
    "end": "197489"
  },
  {
    "text": "the numerator here is the\nnumber of positive examples. And divided by the total\nnumber of examples.",
    "start": "197489",
    "end": "204520"
  },
  {
    "text": "And we also have formulas\nfor mu 1 and the mu 2. So mu 1 was something\nlike the average",
    "start": "204520",
    "end": "214220"
  },
  {
    "text": "of xi in the positive group.",
    "start": "214220",
    "end": "219710"
  },
  {
    "text": "In the positive examples. So you take the. Average you divide by the total\nnumber of positive examples.",
    "start": "219710",
    "end": "225099"
  },
  {
    "text": "And you can also have\nmu 0, and also sigma. So each of these\nhas a formula, which",
    "start": "225099",
    "end": "230661"
  },
  {
    "text": "is a formula that\ndepends on the data. So this is how you learn the\nparameters from the data.",
    "start": "230661",
    "end": "240200"
  },
  {
    "text": "And then we talk about\nwithin these parameters, how do you do the test?",
    "start": "240200",
    "end": "245370"
  },
  {
    "text": "How do you do the inference? How do you do the prediction? Like we already learned\nthese parameters.",
    "start": "245370",
    "end": "251049"
  },
  {
    "text": "Now you are given an example x. How do you predict y using the\nparameters and given the x?",
    "start": "251049",
    "end": "259268"
  },
  {
    "text": "So and we said, that you are\ntrying to compute the arg max over the two choice of y's.",
    "start": "259269",
    "end": "270370"
  },
  {
    "text": "And you want to look\nat which choice of y give you the largest\nprobability, given x.",
    "start": "270370",
    "end": "277060"
  },
  {
    "text": "And given the parameter.",
    "start": "277060",
    "end": "282100"
  },
  {
    "text": "And these parameters\nare the solutions you have computed from\nthese formulas, right?",
    "start": "282100",
    "end": "287289"
  },
  {
    "text": "So you compute which one-- oh, sorry. My bad. This is y.",
    "start": "287290",
    "end": "292370"
  },
  {
    "text": "You compute which one has\nthe largest probability. And also, we discussed\nthat to know this, actually, you just\nhave to, in some sense,",
    "start": "292370",
    "end": "299530"
  },
  {
    "text": "you have a decision boundary. So the decision boundary\nbetween the two choices is those cases where\nis those axes where",
    "start": "299530",
    "end": "315419"
  },
  {
    "text": "these two probabilities\nare exactly the same. So where the p of y is 1\ngiven x is equal to p of y",
    "start": "315419",
    "end": "323280"
  },
  {
    "text": "is 0, given x is equal to 1/2. This is the decision boundary. And we have computed\nthe decision boundary,",
    "start": "323280",
    "end": "330540"
  },
  {
    "text": "which turns out to be something\nlike a linear function.",
    "start": "330540",
    "end": "337180"
  },
  {
    "text": "So decision boundary\nturns out that this p of y is equal to 1 given x.",
    "start": "337180",
    "end": "342270"
  },
  {
    "text": "This is equal to-- this is equal to\nsomething like--",
    "start": "342270",
    "end": "350069"
  },
  {
    "text": "I guess maybe I'll\njust directly write out the decision boundary.",
    "start": "350069",
    "end": "356150"
  },
  {
    "text": "We found out that\nthe decision boundary is the set of x such that\ntheta transpose x plus theta 0",
    "start": "356150",
    "end": "365000"
  },
  {
    "text": "is equal to 0. Where theta and theta 0 are\nfunctions of the parameters",
    "start": "365000",
    "end": "372580"
  },
  {
    "text": "that you have learned. There are some specific\nformulas to describe",
    "start": "372580",
    "end": "377960"
  },
  {
    "text": "this theta and theta 0, which\nare the homework questions. But theta and theta of phi, mu 0, mu 1, and sigma.",
    "start": "377960",
    "end": "385440"
  },
  {
    "text": "So that's why when you really\nmake the prediction, what you do is you say-- you find out this\ndecision boundary.",
    "start": "385440",
    "end": "391599"
  },
  {
    "text": "This is the family of x,\nsuch that theta transpose x plus theta 0 is equal to 0.",
    "start": "391599",
    "end": "397009"
  },
  {
    "text": "And then if this quantity\nis bigger than 0, then you would\nsay it's positive. And if this point is less\nthan 0, you say it's negative.",
    "start": "397010",
    "end": "408050"
  },
  {
    "text": "This is a very\nquick five minutes review of the last lecture. Very, very quick.",
    "start": "408050",
    "end": "413370"
  },
  {
    "text": "Any questions? What would be the\ndecision boundary",
    "start": "413370",
    "end": "419250"
  },
  {
    "text": "or actually would there\nbe a decision boundary if we have multiple labels? Yeah, there will be a decision\nboundary for multiple labels.",
    "start": "419250",
    "end": "425490"
  },
  {
    "text": "And you basically just\ncompute the decision boundary using this methodology. You're going to get--",
    "start": "425490",
    "end": "430940"
  },
  {
    "text": "So how about [INAUDIBLE]? Can you say it again? Like [INAUDIBLE].",
    "start": "430940",
    "end": "436470"
  },
  {
    "text": "What would be the decision\nboundary [INAUDIBLE]?? For multiple labels? Yeah. I think if you have\nthe same covariance,",
    "start": "436470",
    "end": "442180"
  },
  {
    "text": "the decision boundary\nis still linear. But if you have\ndifferent models,",
    "start": "442180",
    "end": "447280"
  },
  {
    "text": "you may have different types\nof decision boundaries. So when it's going to\nbe equal to a half,",
    "start": "447280",
    "end": "452650"
  },
  {
    "text": "do we use the same [INAUDIBLE]? It wouldn't be a\nhalf, because you're going to have multiple\nchoices y's here, right? So your decision\nboundary will be like--",
    "start": "452650",
    "end": "460729"
  },
  {
    "text": "I think it's going to\nbe more complicated. So you have to\nreally decide when-- suppose this is like 0,",
    "start": "460729",
    "end": "468660"
  },
  {
    "text": "So then you really have to just\nreally literally solve this, right? You have to know--\nwhat is the region such",
    "start": "468660",
    "end": "474720"
  },
  {
    "text": "that the maximizer of this\nis equal to, say, label two? So that's going to\nbe something that",
    "start": "474720",
    "end": "480080"
  },
  {
    "text": "describes by some linear region,\nsome kind of linear boundaries. But it's going to\nbe more complicated.",
    "start": "480080",
    "end": "485380"
  },
  {
    "text": "Yeah. That's a great question. Any question? Can the boundary be quadratic?",
    "start": "485380",
    "end": "492650"
  },
  {
    "text": "For-- it could be, depending\non what probabilistic model you define, right?",
    "start": "492650",
    "end": "497800"
  },
  {
    "text": "So I think if sigma are\nall the same, the capital sigma all the same, I think\nit's going to be linear.",
    "start": "497800",
    "end": "504850"
  },
  {
    "text": "And if you have\ndifferent sigma, you have two classes, but\njust sigma 1 and sigma 2, then I know as a\nfact it's quadratic.",
    "start": "504850",
    "end": "513240"
  },
  {
    "text": "I saw some other\nquestions as well. Any questions are welcome.",
    "start": "513240",
    "end": "522229"
  },
  {
    "text": "OK. So this is just to give you a\nquick overview, a quick review of what we did last time.",
    "start": "522230",
    "end": "529759"
  },
  {
    "text": "And you will see that our new-- when we discuss\nthe new problem, we are going to have a\nsimilar methodology.",
    "start": "529760",
    "end": "536630"
  },
  {
    "text": "You define a\nprobabilistic model. You solve the MLE. You get some formula. And then you get\nsome parameters,",
    "start": "536630",
    "end": "542410"
  },
  {
    "text": "and then use those parameters to\npredict what's the most likely y's. OK.",
    "start": "542410",
    "end": "547860"
  },
  {
    "text": "So but before we're\ngoing to do that, I'm going to discuss one\nimportant thing, which many people actually\nasked in the last lecture, at the end of the last lecture.",
    "start": "547860",
    "end": "553279"
  },
  {
    "text": "They are great questions. So the question I'm\ngoing to discuss next is that why this is different\nfrom what we have seen",
    "start": "553279",
    "end": "561450"
  },
  {
    "text": "in the first two weeks, right? So at the end of that, you\nhave a decision boundary, which is linear, right? So basically, at\nleast superficially,",
    "start": "561450",
    "end": "567290"
  },
  {
    "text": "it sounds like you are using\na linear model to decide-- you are going to use this linear\nfunction to decide whether it's",
    "start": "567290",
    "end": "574420"
  },
  {
    "text": "positive or negative. You compare this\nwith 0, and if it's-- if logarithm 0 is positive. Otherwise, it's negative.",
    "start": "574420",
    "end": "580390"
  },
  {
    "text": "And this is the same as\nthe logistic regression that Chris talked about\nin the first two weeks.",
    "start": "580390",
    "end": "585410"
  },
  {
    "text": "So why this are-- why this is different from\nthe so-called discriminative",
    "start": "585410",
    "end": "591810"
  },
  {
    "text": "methods that Chris talked about? So I think here is the\nway I think about this.",
    "start": "591810",
    "end": "606160"
  },
  {
    "text": "So if you have GDA on\nthe left, and suppose you have a logistic\nregression on the right.",
    "start": "606160",
    "end": "615810"
  },
  {
    "text": "So first of all, in terms of the\nassumption they are different,",
    "start": "615810",
    "end": "625370"
  },
  {
    "text": "so for GDA, I guess I\nwrote the assumption there. So maybe I just\nwrote here again. So this is Gaussian.",
    "start": "625370",
    "end": "634019"
  },
  {
    "text": "And this is also Gaussian. Something like this. And you also have y,\nwhich is from Bernoulli.",
    "start": "634019",
    "end": "642889"
  },
  {
    "text": "And when you do\nlogistic regression, then you just literally say that\np of y is equal to 1 given x.",
    "start": "642889",
    "end": "651760"
  },
  {
    "text": "Your probabilistic\nmodel assumes that this has the form 1 over 1 plus e\nto the minus theta transpose x.",
    "start": "651760",
    "end": "666750"
  },
  {
    "text": "And you recall that here,\nwhen you write this, in places we are\nsaying that x0 is 1. That's Chris assumption\nlike in the first two weeks.",
    "start": "666750",
    "end": "674220"
  },
  {
    "text": "But suppose you don't\nhave that assumption. You're going to have another-- suppose x doesn't contain x0.",
    "start": "674220",
    "end": "679560"
  },
  {
    "text": "Then you need to have another\nset of 0, something like this. But if you contain\nthe x0, then you are going to have a clean form.",
    "start": "679560",
    "end": "685120"
  },
  {
    "text": "But they are the same, right? Because if suppose I say,\nfor today, let's say x,",
    "start": "685120",
    "end": "690519"
  },
  {
    "text": "you drop that convention. So x doesn't contain x0. Then you're going to\nwrite like this, right? And this exactly\nmatch the form here.",
    "start": "690519",
    "end": "696980"
  },
  {
    "text": "So basically, you can see that\none thing is that, for GDA, you",
    "start": "696980",
    "end": "702000"
  },
  {
    "text": "assume this bunch of things. Aand you found out\nthat it implies that y given x has this form.",
    "start": "702000",
    "end": "709120"
  },
  {
    "text": "Recall that maybe I\ndidn't write this. But this is equal to 1 over",
    "start": "709120",
    "end": "717779"
  },
  {
    "text": "transpose x plus theta 0, right? So let's say, how\ndo I write this?",
    "start": "717780",
    "end": "724570"
  },
  {
    "text": "So p of y is equal So this is a\nconclusion that we got",
    "start": "724570",
    "end": "737440"
  },
  {
    "text": "from the probabilistic\nmodeling, from our mathematical derivation. We conclude that y given\nx should have this form.",
    "start": "737440",
    "end": "744839"
  },
  {
    "text": "And in logistic regression,\nyou just directly assume it. So in GDA, that's a\nconclusion, in some sense.",
    "start": "744840",
    "end": "751830"
  },
  {
    "text": "Right? And I guess we have to\nstress this many times.",
    "start": "751830",
    "end": "756959"
  },
  {
    "text": "So here, you model the joint\nprobability distribution. Because given-- if you know\nx given y, and you know y,",
    "start": "756960",
    "end": "763490"
  },
  {
    "text": "then you also have the\ndensity of x comma y, right? But here, you only\nalways have y given x.",
    "start": "763490",
    "end": "769810"
  },
  {
    "text": "But you never have\nanything about x. You only have the\nconditional probability. So in some sense, the main\ndifference between these two",
    "start": "769810",
    "end": "778430"
  },
  {
    "text": "is that on the left hand side,\nyou have more assumptions. So I think this is the key.",
    "start": "778430",
    "end": "787510"
  },
  {
    "text": "You have stronger assumptions\non the left-hand side than on the right-hand side. And in some sense, the general--",
    "start": "787510",
    "end": "793779"
  },
  {
    "text": "the general kind of like a\nphenomenal general principle",
    "start": "793779",
    "end": "800180"
  },
  {
    "text": "about how do you model\nwhat part of the world you want to model in your\nmachine learning algorithm,",
    "start": "800180",
    "end": "806190"
  },
  {
    "text": "right? So do you want to model both? Or you want to model\nonly y given x, right? So how do you decide how much\nyou want to model the world.",
    "start": "806190",
    "end": "815010"
  },
  {
    "text": "So the pros and cons are-- the trade off is that if you\nhave more assumptions, then",
    "start": "815010",
    "end": "822389"
  },
  {
    "text": "typically if this is\nalso correct assumption, if your assumption is correct,\nthen this pretty much implies",
    "start": "822389",
    "end": "833860"
  },
  {
    "text": "you have better performance. Of course, this is a not\nmathematical statement.",
    "start": "833860",
    "end": "840899"
  },
  {
    "text": "But I think it's kind\nof reasonably intuitive, because if you have\nmore assumptions, it means that you are\nusing your prior knowledge",
    "start": "840899",
    "end": "848399"
  },
  {
    "text": "about the world. So here, you are using a\npriori knowledge that x given y is Gaussian. And if you use that\nprior knowledge typically",
    "start": "848399",
    "end": "855260"
  },
  {
    "text": "if you use it\ncorrectly, then you should have better performance. Like if I tell you\neverything, then of course",
    "start": "855260",
    "end": "861899"
  },
  {
    "text": "you have better performance. If I tell you a little more-- suppose I tell you everything\nabout mu and sigma, then of course your\nperformance is the best.",
    "start": "861899",
    "end": "868130"
  },
  {
    "text": "But if you tell you a\nlittle more about x, then you should always have a\nlittle better performance.",
    "start": "868130",
    "end": "874339"
  },
  {
    "text": "But the problem is\nwith this is that-- so this is the good thing\nabout more assumptions. But the risk is that you may\nhave wrong assumptions, right?",
    "start": "874339",
    "end": "883050"
  },
  {
    "text": "You might make a wrong or\napproximate or kind of not",
    "start": "883050",
    "end": "890779"
  },
  {
    "text": "exactly correct\nassumptions, right?",
    "start": "890779",
    "end": "897250"
  },
  {
    "text": "So if you make wrong\nassumptions, then in most of the cases, probably\nGDA will be worse off.",
    "start": "897250",
    "end": "904050"
  },
  {
    "text": "For example, what if the data\nare not really Gaussian, right? So what if this doesn't\nlook Gaussian at all,",
    "start": "904050",
    "end": "909069"
  },
  {
    "text": "and you still make assumption\nthat they are Gaussian. Then you are going to\nhave a worse performance.",
    "start": "909069",
    "end": "918059"
  },
  {
    "text": "And another thing\nI want to stress is that even though,\nsuperficially, you see the form here is the same, right?",
    "start": "918059",
    "end": "923860"
  },
  {
    "text": "So theta transpose. So when you make\nthe GDA assumptions, you get this y given\nx of this form.",
    "start": "923860",
    "end": "930980"
  },
  {
    "text": "But suppose you go through this\nleft hand side and get this, and you numerically\ncompute, so theta. Theta and theta 0.",
    "start": "930980",
    "end": "937480"
  },
  {
    "text": "It wouldn't be the\nsame theta and theta",
    "start": "937480",
    "end": "942899"
  },
  {
    "text": "from logistic regression. So you're going to get a\ndifferent side of like theta and theta.",
    "start": "942899",
    "end": "948060"
  },
  {
    "text": "makes a difference. So this theta and theta 0,\nfrom logistic regression, is how do you get it?",
    "start": "948060",
    "end": "953769"
  },
  {
    "text": "You just directly\naffect your linear model using logistic regression. That's how you got\ntheta and theta 0. But if you go from here\nto here, then you're",
    "start": "953769",
    "end": "960080"
  },
  {
    "text": "going to first learn the mu,\nthe sigma, the mu 1, mu 2, mu 0,",
    "start": "960080",
    "end": "965579"
  },
  {
    "text": "mu 1, sigma. And then you compute theta,\nusing mu 0, mu 1, sigma. So that will result\nin a different set",
    "start": "965579",
    "end": "971480"
  },
  {
    "text": "of theta and theta 0. That's why it does\nmake a difference. And whether it's better or\nnot, I think, as I said,",
    "start": "971480",
    "end": "977550"
  },
  {
    "text": "basically it depends on whether\nassumptions are correct, or how correct your\nassumptions are. Probably, you never can have\nexactly correct assumption.",
    "start": "977550",
    "end": "985310"
  },
  {
    "text": "But maybe our\nassumption, when your are assumption is\napproximately correct, then you should do GDA. If your assumption is\njust completely wrong,",
    "start": "985310",
    "end": "990730"
  },
  {
    "text": "then you probably should\ndo logistic regression. Any questions so far?",
    "start": "990730",
    "end": "997110"
  },
  {
    "text": "Well, [INAUDIBLE]\nbecause [INAUDIBLE].. You said that you have-- like\nwe have a different design",
    "start": "997110",
    "end": "1006240"
  },
  {
    "text": "in multiple cases. So how would this\n[INAUDIBLE] different?",
    "start": "1006240",
    "end": "1012449"
  },
  {
    "text": "Is it because we are computing\nin that equation [INAUDIBLE]..",
    "start": "1012449",
    "end": "1017839"
  },
  {
    "text": "Are they like-- I guess that's the formatting\n[INAUDIBLE] we end up not retrieving all\nthe parameters, right,",
    "start": "1017839",
    "end": "1023910"
  },
  {
    "text": "but we're not gaining\nall those [INAUDIBLE].. Right. When you do GDA, you are not\ntraining on theta directly. You are training on mus\nand sigma's directly,",
    "start": "1023910",
    "end": "1030780"
  },
  {
    "text": "and using those formulas. And then you compute theta as\na function of mu and sigma,",
    "start": "1030780",
    "end": "1036160"
  },
  {
    "text": "right? So it's definitely a\ndifferent process, right? You are not getting the theta\nand theta 0 in these two cases",
    "start": "1036160",
    "end": "1041760"
  },
  {
    "text": "using the same process, right? So in one process, it's\nkind of circuitious, right? You first have to compute\nmu and then theta.",
    "start": "1041760",
    "end": "1047400"
  },
  {
    "text": "In the other case,\nyou just directly fit this using a\nnumerical algorithm. Like gradient descent.",
    "start": "1047400",
    "end": "1052779"
  },
  {
    "text": "[INAUDIBLE] is it possible that\nwe can get a different equation",
    "start": "1052780",
    "end": "1058940"
  },
  {
    "text": "for [INAUDIBLE] applied given\nx, if we have let's say,",
    "start": "1058940",
    "end": "1064090"
  },
  {
    "text": "different assumptions? Definitely. You may have-- if you\nchange your assumption, you may have a different\nequation for p of y given x.",
    "start": "1064090",
    "end": "1071100"
  },
  {
    "text": "And there's actually\nan interesting point. I'm going to mention,\nmaybe after I answer. Are there any other questions?",
    "start": "1071100",
    "end": "1078500"
  },
  {
    "text": "So then, there's actually\nanother interesting point I'm going to mention next is\nthat even you change the--",
    "start": "1078500",
    "end": "1084320"
  },
  {
    "text": "actually, it's possible\nyou change your assumption, you still have the same\nformula on the right hand side.",
    "start": "1084320",
    "end": "1090720"
  },
  {
    "text": "So you can have both. If you change the\nassumption, maybe you still have the same formula. Maybe you still have\na different formula.",
    "start": "1090720",
    "end": "1096520"
  },
  {
    "text": "So this is a complex\ncase, which I think what I said is actually\neven more surprising. So you change the assumption\non the left hand side,",
    "start": "1096520",
    "end": "1103020"
  },
  {
    "text": "you still have the same y\ngiven x on the right hand side. That is the same form. So this is an example.",
    "start": "1103020",
    "end": "1109980"
  },
  {
    "text": "So I guess here I'm looking\nat, I think one dimensional. So suppose x is one dimensional.",
    "start": "1109980",
    "end": "1118790"
  },
  {
    "text": "It's not very\nimportant like exactly. So suppose you do x given y is distribution.",
    "start": "1118790",
    "end": "1125299"
  },
  {
    "text": "How do I--",
    "start": "1125299",
    "end": "1130940"
  },
  {
    "text": "Poisson distribution. Sorry.",
    "start": "1130940",
    "end": "1136580"
  },
  {
    "text": "And then you do this.",
    "start": "1136580",
    "end": "1143519"
  },
  {
    "text": "And I misspelled\nthis word, sorry. But you do this. So it's no longer a Gaussian.",
    "start": "1143520",
    "end": "1150100"
  },
  {
    "text": "It is some other distribution. And I guess x is--",
    "start": "1150100",
    "end": "1156809"
  },
  {
    "text": "because this is a\nPoisson distribution, so it has to be integer. Something like an integer.",
    "start": "1156809",
    "end": "1163690"
  },
  {
    "text": "And then you still have y is\np of y is 1, is equal to phi.",
    "start": "1163690",
    "end": "1170799"
  },
  {
    "text": "So we change our assumption. And then, this\nactually still implies",
    "start": "1170799",
    "end": "1175950"
  },
  {
    "text": "that p of y given x\nhas this form, 1 plus",
    "start": "1175950",
    "end": "1184799"
  },
  {
    "text": "e to the minus theta\ntranspose x plus theta 0. So still, the form\nlooks similar.",
    "start": "1184799",
    "end": "1192020"
  },
  {
    "text": "Of course, if you really\nnumerically compute this, it would be a different\ntheta than theta 0. Because theta and\ntheta 0, here I'm",
    "start": "1192020",
    "end": "1198580"
  },
  {
    "text": "just writing them as a\ngeneric variable, right? But actually, they\nhave meanings, right?",
    "start": "1198580",
    "end": "1204240"
  },
  {
    "text": "Theta is a function\nof lambda 1, lambda 0. And theta 0 is also a function\nof lambda 1 and lambda 0.",
    "start": "1204240",
    "end": "1210908"
  },
  {
    "text": "So the form is still the same. But you could have,\nnumerically, if you",
    "start": "1210909",
    "end": "1217250"
  },
  {
    "text": "use this model\ninstead of GDA, you are going to get different\ntheta and theta 0.",
    "start": "1217250",
    "end": "1222330"
  },
  {
    "text": "So a linear form\ndoesn't necessarily mean everything, right?",
    "start": "1222330",
    "end": "1228200"
  },
  {
    "text": "It also depends on how you\nlearn this linear function. So actually here, we have\nthree ways to learn it.",
    "start": "1228200",
    "end": "1233900"
  },
  {
    "text": "We can use this model. We can use the GDA. We can use the\nlogistic regression.",
    "start": "1233900",
    "end": "1239679"
  },
  {
    "text": "They will all give you\ndifferent theta and theta And then which one\nwill be better,",
    "start": "1239679",
    "end": "1245200"
  },
  {
    "text": "I think if you compare GDA\nwith this Poisson version of the generative\nlearning algorithm,",
    "start": "1245200",
    "end": "1251720"
  },
  {
    "text": "then I guess the answer\nwould be that probably it depends most on whether which\nassumption is more correct.",
    "start": "1251720",
    "end": "1258140"
  },
  {
    "text": "More likely to be correct. Of course, there is also\nsome type thing here, because here, your\nthing is like your N.",
    "start": "1258140",
    "end": "1263720"
  },
  {
    "text": "But suppose you have a\ndifferent model, which also deal with like r. Basically, I'm saying\nthat when you compare it--",
    "start": "1263720",
    "end": "1271070"
  },
  {
    "text": "forget about this\naxis here, anything. Suppose you don't care\nabout that differences.",
    "start": "1271070",
    "end": "1276240"
  },
  {
    "text": "Then which model\nwill work better probably depends on whether your\nassumption is correct or not. Or which assumption is\nmore likely to be correct.",
    "start": "1276240",
    "end": "1283830"
  },
  {
    "text": "And when you compare\nthe generative algorithm with the discriminant one,\nI think it's the same thing, right?",
    "start": "1283830",
    "end": "1289159"
  },
  {
    "text": "So if your generative assumption\nis likely to be correct, then you should gain\nsomething from it. Otherwise, maybe you should\njust use logistic regression.",
    "start": "1289159",
    "end": "1299158"
  },
  {
    "text": "I think in some sense, like\nthese states, if you look at-- and also, another\nthing is that, OK.",
    "start": "1299159",
    "end": "1308909"
  },
  {
    "text": "And also, maybe a little\nmore general discussion is that in some sense, your\nmodel has two sorts of like--",
    "start": "1308909",
    "end": "1320370"
  },
  {
    "text": "so in some sense, they are-- in some sense, you have\ntwo sources of knowledge.",
    "start": "1320370",
    "end": "1327190"
  },
  {
    "text": "The model learns from two\nthings, in some sense. So one thing is that\nyour assumption.",
    "start": "1327190",
    "end": "1333740"
  },
  {
    "text": "And the other thing is\nyou have data, right?",
    "start": "1333740",
    "end": "1339750"
  },
  {
    "text": "So the assumptions means like\nhow do I probabilistically model all of these quantities? And data is really just\nwhat you see, right?",
    "start": "1339750",
    "end": "1347919"
  },
  {
    "text": "If you have more data of\ncourse it's good right, if you have more assumptions. If the assumption is\ncorrect then that's good.",
    "start": "1347919",
    "end": "1353240"
  },
  {
    "text": "But on the other hand,\nsuppose that, for example, you have already a lot of data.",
    "start": "1353240",
    "end": "1359010"
  },
  {
    "text": "Then you have less need\nto use prior knowledge, because your data already\nare very telling, right?",
    "start": "1359010",
    "end": "1364340"
  },
  {
    "text": "So like the data is\nsufficient for you to extract whatever\ninformation you want.",
    "start": "1364340",
    "end": "1370640"
  },
  {
    "text": "Then you don't really need\nto use prior knowledge. Because if you use\nprior knowledge, you always have a risk\nto use it wrongly.",
    "start": "1370640",
    "end": "1375750"
  },
  {
    "text": "So basically, in\nsome sense I think the modern trend\nis that we're going",
    "start": "1375750",
    "end": "1381309"
  },
  {
    "text": "to talk about neural\nnetworks and deep learning in two lectures.",
    "start": "1381310",
    "end": "1386580"
  },
  {
    "text": "The modern trend is that\nnow we're in this setting that we have more and more\ndata for many applications.",
    "start": "1386580",
    "end": "1393419"
  },
  {
    "text": "And then, that's why the modern\ntechniques, like deep learning networks, those techniques use\nfewer and fewer assumptions",
    "start": "1393420",
    "end": "1401530"
  },
  {
    "text": "about our data. So just because it's not\nreally worth it, right? So I have to think about how\ndo I model my images, right?",
    "start": "1401530",
    "end": "1408970"
  },
  {
    "text": "Suppose you apply\nthis to images. You have to have a model\nfor x, for the image. Is that really worth\nthe effort to do that?",
    "start": "1408970",
    "end": "1416789"
  },
  {
    "text": "Probably not. Of course, it depends on cases. But if you just want the\nfirst kind of results, you don't have to model\nyour x, because modeling",
    "start": "1416789",
    "end": "1424620"
  },
  {
    "text": "x is very difficult.\nIt's very challenging. And you may make mistakes. So you probably\nshould just directly go for the more kind of\nlike discriminative analysis",
    "start": "1424620",
    "end": "1433340"
  },
  {
    "text": "type of approach, right? So you just directly model\ny given x if x is an image, right?",
    "start": "1433340",
    "end": "1438919"
  },
  {
    "text": "But in some other\napplications, for example, if you think about\nmedical applications, or some of the\nother applications",
    "start": "1438919",
    "end": "1445710"
  },
  {
    "text": "of machine learning, where\nyou don't have enough data-- in those cases, I\nthink, still, you",
    "start": "1445710",
    "end": "1451408"
  },
  {
    "text": "have to use as much prior\nknowledge as possible. And actually, many\ntimes, people even do",
    "start": "1451409",
    "end": "1457309"
  },
  {
    "text": "even much more complicated\nmodeling of your x. So maybe you can use some\nother more advanced ways",
    "start": "1457309",
    "end": "1462639"
  },
  {
    "text": "to model your x,\nbecause you know how does each\ncoordinate of x, what each coordinate of x means,\nand what's their relationship.",
    "start": "1462640",
    "end": "1470500"
  },
  {
    "text": "And you put all of this prior\nknowledge into modeling for x. And then you get finally some y\ngiven x x using this machinery.",
    "start": "1470500",
    "end": "1477410"
  },
  {
    "text": "And then you predict. And that's more likely to work. So yeah.",
    "start": "1477410",
    "end": "1483850"
  },
  {
    "text": "I guess that's-- and another\nthing is that if you use more assumptions, then\nyou are specializing to your application.",
    "start": "1483850",
    "end": "1490270"
  },
  {
    "text": "That's another reason probably\nwhy in the modern time, like people somehow\ndon't do this kind of--",
    "start": "1490270",
    "end": "1496649"
  },
  {
    "text": "use these prior notations\noften, because you guys probably heard of neural\nnetworks, and one of the kind of magic\nabout it is that it",
    "start": "1496649",
    "end": "1502761"
  },
  {
    "text": "works for many, many cases\nwithout much customization, right? If you use prior assumptions,\nyou use more assumptions,",
    "start": "1502761",
    "end": "1509080"
  },
  {
    "text": "you are-- you have to use\ndifferent assumptions for different applications.",
    "start": "1509080",
    "end": "1514900"
  },
  {
    "text": "Because for different\napplications, their data will probably\nhave different structures. So you have to do it one by one.",
    "start": "1514900",
    "end": "1520390"
  },
  {
    "text": "And that's actually what people\ndo a lot of times, right? So they look at their\ndomain, their questions,",
    "start": "1520390",
    "end": "1525860"
  },
  {
    "text": "and study the structure. But these days, you\nknow, like as you see, like when people\nuse neural networks,",
    "start": "1525860",
    "end": "1531860"
  },
  {
    "text": "because you have enough data\nand you just drop assumptions, and you make a model\nkind of very general,",
    "start": "1531860",
    "end": "1537789"
  },
  {
    "text": "not specialized at all, and\nthen you supply to everything just without thinking much\nabout what the data look like.",
    "start": "1537789",
    "end": "1544880"
  },
  {
    "text": "So yeah, so I guess that's-- we will talk more\nabout neural networks. But this is the\na preview or kind",
    "start": "1544880",
    "end": "1551270"
  },
  {
    "text": "of connections to what we're\ngoing to talk about next. And another kind\nof a big picture",
    "start": "1551270",
    "end": "1556680"
  },
  {
    "text": "is that one of the reason,\nas I said, in some sense, I was saying that this\nkind of GDA analysis",
    "start": "1556680",
    "end": "1562640"
  },
  {
    "text": "is not used that often, at least\nnot used as often as before, right?",
    "start": "1562640",
    "end": "1567900"
  },
  {
    "text": "Before, you have to use\nthese kind of things. And now you can-- at\nleast you have the choice to try something\nlike neural networks,",
    "start": "1567900",
    "end": "1574690"
  },
  {
    "text": "because you have more data. But still, I think if\nyou are able to model",
    "start": "1574690",
    "end": "1581419"
  },
  {
    "text": "the x, in many cases\nyou can do better. And also, in some cases\nyou just don't have y.",
    "start": "1581420",
    "end": "1586460"
  },
  {
    "text": "So even, for example, when\nyou have images, right? So sometimes you\ndon't have y at all. You just only have x.",
    "start": "1586460",
    "end": "1592380"
  },
  {
    "text": "And in those cases,\nyou just really have to model x, because\nthat's the only place where you can get information from it.",
    "start": "1592380",
    "end": "1598029"
  },
  {
    "text": "And another possibility,\nanother case is we're going to\ntalk about languages,",
    "start": "1598029",
    "end": "1603390"
  },
  {
    "text": "especially like\nthis lecture, I'm going to talk about\nlanguage as well.",
    "start": "1603390",
    "end": "1608980"
  },
  {
    "text": "But later, we're also going\nto talk about languages, solving language problems\nwith neural networks.",
    "start": "1608980",
    "end": "1614050"
  },
  {
    "text": "And there, you are just getting\nkind of like a lot of text",
    "start": "1614050",
    "end": "1619920"
  },
  {
    "text": "from the internet, right? And there's no labels. There is nobody's\ntelling you like which",
    "start": "1619920",
    "end": "1626770"
  },
  {
    "text": "web page is about which, right? So you just have raw text. We only have x. And in those cases, you\nreally have to model x.",
    "start": "1626770",
    "end": "1633270"
  },
  {
    "text": "There is no way you\ncan get around this. So still, like modeling\nx is very important.",
    "start": "1633270",
    "end": "1638679"
  },
  {
    "text": "It's just like,\nfor example, it's less important for certain\nkind of applications because we have more data.",
    "start": "1638680",
    "end": "1646510"
  },
  {
    "text": "Any questions? OK, cool.",
    "start": "1646510",
    "end": "1655130"
  },
  {
    "text": "So I guess, now, I'm going to\nmove on to the next question.",
    "start": "1655130",
    "end": "1662840"
  },
  {
    "text": "So I think the question is the\nso-called spam classification.",
    "start": "1662840",
    "end": "1668059"
  },
  {
    "text": "So you are trying to understand\nwhether a piece of text is a spam email or not, right?",
    "start": "1668059",
    "end": "1674460"
  },
  {
    "text": "So you have an\nemail spam filter, and you want to know whether\nyour email is a spam or not.",
    "start": "1674460",
    "end": "1680669"
  },
  {
    "text": "And we are still going to do\ngenerative learning algorithm.",
    "start": "1680669",
    "end": "1689940"
  },
  {
    "text": "And we are going\nto have discrete x. So this is another\nexample of how",
    "start": "1689940",
    "end": "1696340"
  },
  {
    "text": "you do this generative\nlearning algorithm. You model x given y,\nand you execute this",
    "start": "1696340",
    "end": "1702230"
  },
  {
    "text": "like a pipeline in some sense,\nand learn something out of it.",
    "start": "1702230",
    "end": "1711370"
  },
  {
    "text": "So the first thing. So I'm going to get into\nall of the more details. How do we really\napproach this question.",
    "start": "1711370",
    "end": "1718140"
  },
  {
    "text": "So the first thing\nyou probably have to-- I have one quick question.",
    "start": "1718140",
    "end": "1725930"
  },
  {
    "text": "With GDA, in the spam filter,\ngenerative [INAUDIBLE] so there are more. There are many, many more.",
    "start": "1725930",
    "end": "1732800"
  },
  {
    "text": "These are just examples. Right. So I guess-- so the first\nquestion, first question",
    "start": "1732800",
    "end": "1740380"
  },
  {
    "text": "we have to do to approach\nthis is that how do you represent a text, right? Text are symbols,\nlike ABCD, right?",
    "start": "1740380",
    "end": "1747399"
  },
  {
    "text": "So you need to make a\nnumerical, at least, to make the computer\nrecognize them.",
    "start": "1747400",
    "end": "1755028"
  },
  {
    "text": "In some naive way,\nat least, right? So the first question is, how do\nyou change the text to some x?",
    "start": "1755029",
    "end": "1762140"
  },
  {
    "text": "Maybe you call this\nfeature, this is--",
    "start": "1762140",
    "end": "1767730"
  },
  {
    "text": "you can call this\nfeature vector. Or representation or\nsomething like that. So you want to change this\nto x in some dimension d.",
    "start": "1767730",
    "end": "1776080"
  },
  {
    "text": "And then you model x. So the first question is,\nhow do you represent text? So there are many ways\nto represent text.",
    "start": "1776080",
    "end": "1782529"
  },
  {
    "text": "So the way I'm going to tell\nyou is a very naive way. This is like-- probably\nI wouldn't say naive.",
    "start": "1782530",
    "end": "1790100"
  },
  {
    "text": "But this is a very simple way. In this states we are going\nto have-- if you really",
    "start": "1790100",
    "end": "1796019"
  },
  {
    "text": "deal with the\ntext, you are going to use more advanced\nto deep learning based approach,\nwhich we are going",
    "start": "1796019",
    "end": "1801049"
  },
  {
    "text": "to cover a little bit in\nprobably three or five weeks. So this way, so here the way\nthat we do it is very simple.",
    "start": "1801049",
    "end": "1809610"
  },
  {
    "text": "So what you do is that-- maybe I should have some--",
    "start": "1809610",
    "end": "1823610"
  },
  {
    "text": "So what you do is\nyou say you have-- you first look at\nthe vocabulary. Suppose you have a vocabulary\nof maybe 10K words.",
    "start": "1823610",
    "end": "1832610"
  },
  {
    "text": "And suppose you\nsay, you list all of these words in a sequence,\nbased on alphabetical order.",
    "start": "1832610",
    "end": "1838210"
  },
  {
    "text": "I guess if you open\nup a dictionary, the first letter-- the first\nword is probably always A.",
    "start": "1838210",
    "end": "1843870"
  },
  {
    "text": "I think the second,\naccording to some dictionary,",
    "start": "1843870",
    "end": "1850730"
  },
  {
    "text": "the second word is\nthis word aardvark. I think it's a kind of animal. And the third one is aardwolf.",
    "start": "1850730",
    "end": "1858290"
  },
  {
    "text": "I think it's another\nkind of animal. Something like this.",
    "start": "1858290",
    "end": "1863679"
  },
  {
    "text": "And then you list all the words. And maybe, at some point,\nyou can have a word book. And eventually, the\nlast word, I think,",
    "start": "1863679",
    "end": "1870039"
  },
  {
    "text": "in mind of the\ndictionary, is this thing I don't even know how to-- I don't even remember\nhow to pronounce it.",
    "start": "1870039",
    "end": "1877410"
  },
  {
    "text": "I think I used to know\nwhen the first time I teach this lecture, and then\nI forgot after a few years.",
    "start": "1877410",
    "end": "1883110"
  },
  {
    "text": "Anyway, so you\nlisted all of this. And then you say that\nyou have-- suppose you",
    "start": "1883110",
    "end": "1891380"
  },
  {
    "text": "have a piece of text, right? So maybe say, suppose you have\na sentence, or maybe an email.",
    "start": "1891380",
    "end": "1899000"
  },
  {
    "text": "Suppose this email\njust has one sentence, something like I buy a book.",
    "start": "1899000",
    "end": "1906370"
  },
  {
    "text": "So you want to turn\nthis into a vector. And how do you turn\nit into a vector? So the way that we do it\nhere is that you turn it",
    "start": "1906370",
    "end": "1911870"
  },
  {
    "text": "to a vector that\nis of dimension-- its x is of dimension d.",
    "start": "1911870",
    "end": "1917669"
  },
  {
    "text": "Remember, I said d is the\nsize of the vocabulary. d is equal to the\nnumber of words.",
    "start": "1917670",
    "end": "1924180"
  },
  {
    "text": "And then, and this\nvector that represents this piece of the sentence is\ngoing to be a zero-one vector.",
    "start": "1924180",
    "end": "1935260"
  },
  {
    "text": "So x is actually-- it's really from\nzero-one to the power d There are only two choices.",
    "start": "1935260",
    "end": "1943250"
  },
  {
    "text": "And every entry-- so you\nhave so many entries. And what you do is you\nsay, if this word shows up",
    "start": "1943250",
    "end": "1950830"
  },
  {
    "text": "in the sentence, then you\nhave entry 1 in this entry. So the word \"a\" shows up in the\nsentence, then I have 1 here.",
    "start": "1950830",
    "end": "1958669"
  },
  {
    "text": "And aardvark doesn't show up\nin that sentence, I have 0. And then at some point you have\nthis corresponding entry book.",
    "start": "1958669",
    "end": "1967289"
  },
  {
    "text": "Book shows up in the sentence. I have 1 here. And maybe somewhere, I think\nthere is a word probably \"I\" here in the list.",
    "start": "1967289",
    "end": "1973419"
  },
  {
    "text": "And then that one\nwould also have a 1. That word also corresponds to 1.",
    "start": "1973419",
    "end": "1978889"
  },
  {
    "text": "And for all the other entries\nthat-- all the other words that don't show up in the\nsentence, you just fill in 0",
    "start": "1978889",
    "end": "1985789"
  },
  {
    "text": "for the corresponding entries. And you call this x\nyour representation",
    "start": "1985790",
    "end": "1992220"
  },
  {
    "text": "or your feature vector for\nthis email, for this sentence.",
    "start": "1992220",
    "end": "1999429"
  },
  {
    "text": "So basically,\ntechnically, I'm just going to say that\nxi is equal to 1 if",
    "start": "1999430",
    "end": "2010049"
  },
  {
    "text": "and only if the i-th\nword occurs in an email.",
    "start": "2010049",
    "end": "2032490"
  },
  {
    "text": "So there are actually\nmany other ways to encode, even before using deep\nlearning techniques.",
    "start": "2032490",
    "end": "2038799"
  },
  {
    "text": "But this is probably\none of the simplest one. And you can see that this\nrepresentation of the sentence",
    "start": "2038799",
    "end": "2044020"
  },
  {
    "text": "is, in some sense, like a\nsuper, I guess like simplistic,",
    "start": "2044020",
    "end": "2050730"
  },
  {
    "text": "because for example,\nyou don't care about the orders\nof the word, right? Suppose you have another\nsentence, \"I a book buy,\"",
    "start": "2050730",
    "end": "2057000"
  },
  {
    "text": "right? That sentence still has\nthe same representation. Exactly the same. And it doesn't care about\nthe frequency of the word.",
    "start": "2057000",
    "end": "2063870"
  },
  {
    "text": "For example, suppose\nI have a sentence \"I buy a book and a\npencil,\" then of course",
    "start": "2063870",
    "end": "2071770"
  },
  {
    "text": "the representation will change,\nbut these two words, \"a\" and \"a\" here, will\nbe connected--",
    "start": "2071770",
    "end": "2076960"
  },
  {
    "text": "it will still have\na coordinate 1 here, because the word \"a\"\nshows up in this sentence.",
    "start": "2076960",
    "end": "2082300"
  },
  {
    "text": "But you don't care about how\nmany times the word \"a\" shows up in the sentence.",
    "start": "2082300",
    "end": "2088520"
  },
  {
    "text": "So you don't care\nabout the frequency of the words in the sentence. You just care about\nwhether each word shows up.",
    "start": "2088520",
    "end": "2096010"
  },
  {
    "text": "And there are many other\nprobability issues with this, like what else? Yeah, I guess probably these two\nare the most important thing.",
    "start": "2096010",
    "end": "2104040"
  },
  {
    "text": "Where you don't have\nthe order, and you don't care about the frequency. But that's what we kind of deal\nwith, because this is easy,",
    "start": "2104040",
    "end": "2110300"
  },
  {
    "text": "and you can somewhat\nkind of do all of the math with\nthis kind of model.",
    "start": "2110300",
    "end": "2118599"
  },
  {
    "text": "Now, what's the next question? The next question is\nthat we need to build",
    "start": "2118600",
    "end": "2124119"
  },
  {
    "text": "a generative model for x and y. And we do have a model for y. And then we do MLE.",
    "start": "2124119",
    "end": "2130950"
  },
  {
    "text": "And we solve the\nparameter, so on so forth.",
    "start": "2130950",
    "end": "2144440"
  },
  {
    "text": "So basically, I think\nI can just erase this. And now, I'm going to\nredefine this three things.",
    "start": "2144440",
    "end": "2153730"
  },
  {
    "text": "So that's what I'm going to do. So let's take-- and\nhow do you proceed?",
    "start": "2153730",
    "end": "2159380"
  },
  {
    "text": "So now, because this x\nnull is a binary vector, it's only taking 0 and 1.",
    "start": "2159380",
    "end": "2165539"
  },
  {
    "text": "So you need a\ndistribution that can generate binary vectors, right?",
    "start": "2165540",
    "end": "2170800"
  },
  {
    "text": "So you cannot use Gaussian here. And the kind of the techniques\nthat we are using here",
    "start": "2170800",
    "end": "2177700"
  },
  {
    "text": "is that so-called naive Bayes.",
    "start": "2177700",
    "end": "2182859"
  },
  {
    "text": "So what does this mean is that-- this means that you\njust assume x1 up to xd.",
    "start": "2182860",
    "end": "2192640"
  },
  {
    "text": "Or maybe I'll\nerase this for now.",
    "start": "2192640",
    "end": "2201799"
  },
  {
    "text": "Are independent\nconditioned on y.",
    "start": "2201800",
    "end": "2214170"
  },
  {
    "text": "So given y, you just\nindependently draw x1 up to xd.",
    "start": "2214170",
    "end": "2219250"
  },
  {
    "text": "Of course, this is\nnot realistic, right? This is definitely not exact. How realistic it is, I\nthink that's subjective.",
    "start": "2219250",
    "end": "2224920"
  },
  {
    "text": "But at least, this\nis not exactly how people generate\nemails, right? You are not saying that I'm\ngoing to generate spam filter.",
    "start": "2224920",
    "end": "2230619"
  },
  {
    "text": "At first, I'm going to\ngenerate a spamming email. And the first thing I\ndecide is I decide y.",
    "start": "2230619",
    "end": "2236849"
  },
  {
    "text": "And then after I said y, I just\nstart stringing words randomly. That's probably not how\npeople write spam emails.",
    "start": "2236849",
    "end": "2247020"
  },
  {
    "text": "And also, that's not how\npeople write the usual like good emails. But it turns out that many,\nmany, many cases, these kind",
    "start": "2247020",
    "end": "2256400"
  },
  {
    "text": "of assumptions are pretty-- already pretty good. So in the homework,\nactually, we haven't",
    "start": "2256400",
    "end": "2263519"
  },
  {
    "text": "decided whether we are going to\ninclude that homework question. But at least, there\nare cases where",
    "start": "2263520",
    "end": "2270240"
  },
  {
    "text": "you can see this kind of things\ncan be very effective, right? Actually, if you just\nreally use this model, even you make this kind\nof crazy assumption,",
    "start": "2270240",
    "end": "2277529"
  },
  {
    "text": "and you learn some\nparameter and use this model to classify spams,\nI think you're going to get more than 90% accuracy.",
    "start": "2277530",
    "end": "2284900"
  },
  {
    "text": "Maybe these days, you\nknow, as of like 2012, maybe it's not that effective\nbecause all the spammers,",
    "start": "2284900",
    "end": "2291950"
  },
  {
    "text": "they are adversarial. So they know what your\nprediction algorithm is. They can change their\nalgorithm to kind of fool you.",
    "start": "2291950",
    "end": "2298750"
  },
  {
    "text": "But at least, this\nis a reasonable one if you go back 10 years ago. For sure, right? So it's kind of\ninteresting, right?",
    "start": "2298750",
    "end": "2305500"
  },
  {
    "text": "So even you make-- all this obviously kind of not\nexactly the right assumption,",
    "start": "2305500",
    "end": "2311190"
  },
  {
    "text": "but sometimes you can still,\nbecause the assumption probably is somewhat correct\nto some extent,",
    "start": "2311190",
    "end": "2316690"
  },
  {
    "text": "you can still get a useful\nkind of outcome from it. And for our purposes, I think,\nhere I guess to some extent,",
    "start": "2316690",
    "end": "2325619"
  },
  {
    "text": "I'm not really that-- I don't care that much\nabout the assumptions. It's more like I'm trying to\ndemonstrate the methodology.",
    "start": "2325619",
    "end": "2333160"
  },
  {
    "text": "Like how do you-- I just want to\ngive a new example where you execute this\nprobabilistic model, this flow,",
    "start": "2333160",
    "end": "2340859"
  },
  {
    "text": "this pipeline, right, and\nshow how to solve them. [INAUDIBLE]",
    "start": "2340860",
    "end": "2346880"
  },
  {
    "text": "One example like for one.",
    "start": "2346880",
    "end": "2352559"
  },
  {
    "text": "Right. So the question is, how does the\ntext-- the length of the text",
    "start": "2352560",
    "end": "2358099"
  },
  {
    "text": "matter, right? So here, so one\ninteresting about this",
    "start": "2358100",
    "end": "2363180"
  },
  {
    "text": "is that in this representation,\nthe length of the text doesn't really matter. You can encode any\nlength into a vector",
    "start": "2363180",
    "end": "2368831"
  },
  {
    "text": "of dimension D. Which\nis you can say this is a good thing or bad thing.",
    "start": "2368831",
    "end": "2375970"
  },
  {
    "text": "So basically, you can encode\nany length, any sentence, or any documents\ninto a single vector.",
    "start": "2375970",
    "end": "2381890"
  },
  {
    "text": "And how do you decide\nwhat is the window size? Here, I don't think\nit matters that much.",
    "start": "2381890",
    "end": "2387500"
  },
  {
    "text": "Maybe you just\ntake entire email. Of course, if you change the--",
    "start": "2387500",
    "end": "2393680"
  },
  {
    "text": "I think you should just\ninclude the entire email, because that's the unit you\nare working with, right?",
    "start": "2393680",
    "end": "2399960"
  },
  {
    "text": "Like for every email, you are\nclassifying whether it's a spam or not. You are not classifying whether\na sentence is of spam or not.",
    "start": "2399960",
    "end": "2407650"
  },
  {
    "text": "So that's why you treat\nemail as a single example. x1 through xd are\nindependent conditioned on y.",
    "start": "2407650",
    "end": "2417950"
  },
  {
    "text": "Does that mean\nthat x1, like one x appears, that tells us nothing\nabout whether another x is",
    "start": "2417950",
    "end": "2431080"
  },
  {
    "text": "likely to appear? Or does it mean that all x's\nare equally likely to appear when y is zero?",
    "start": "2431080",
    "end": "2440619"
  },
  {
    "text": "I think it's is more about-- it's more the first case. So because I do--",
    "start": "2440619",
    "end": "2447619"
  },
  {
    "text": "like certainly not all the words\nare equally likely to appear, right? So basically, I'm assuming\nthat given y, given you",
    "start": "2447619",
    "end": "2454700"
  },
  {
    "text": "have decided that whether\nthis is spam email or not, every word is--",
    "start": "2454700",
    "end": "2460150"
  },
  {
    "text": "they are independent\nwith each other. But they may have different\nkind of probabilities.",
    "start": "2460150",
    "end": "2473040"
  },
  {
    "text": "So I guess let me proceed. So what does this really mean? OK, now I'm going to do some\nmath, so to kind of expand",
    "start": "2473040",
    "end": "2480619"
  },
  {
    "text": "this and parameterize it. So this really means that you\nhave this p of x1 given xd.",
    "start": "2480619",
    "end": "2490590"
  },
  {
    "text": "Given y, you write\nthis as p of x1 given y times p of xd given y.",
    "start": "2490590",
    "end": "2500670"
  },
  {
    "text": "And now, I just\nneed to parameterize each of this probability\ndistribution by some parameter.",
    "start": "2500670",
    "end": "2507048"
  },
  {
    "text": "And if you think about\nthis, what is this? This is really just\nthe distribution, a Bernoulli distribution.",
    "start": "2507049",
    "end": "2513210"
  },
  {
    "text": "Because xi can only\nhave two choices, 0, 1. So basically, you just have\nto describe this probability",
    "start": "2513210",
    "end": "2519260"
  },
  {
    "text": "by two numbers, right? Actually, by one number, right? So basically, what you do\nis that you parameterize",
    "start": "2519260",
    "end": "2528300"
  },
  {
    "text": "parameters of the model\nis you have phi, say j,",
    "start": "2528300",
    "end": "2539109"
  },
  {
    "text": "So there is some\nindex, which I'm going to explain in a moment.",
    "start": "2539109",
    "end": "2544490"
  },
  {
    "text": "This is xj is equal to So basically, for\ny is equal to 1,",
    "start": "2544490",
    "end": "2551109"
  },
  {
    "text": "you're asking, what's the\nprobability of xj is 1. And you denote that\nprobability as this.",
    "start": "2551109",
    "end": "2557140"
  },
  {
    "text": "And then, once you have this,\nyou know the probability of xj is equals to 0, given y is 1,\nis going to be 1 minus phi,",
    "start": "2557140",
    "end": "2566990"
  },
  {
    "text": "j given y is 1. And this thing is\njust a notation.",
    "start": "2566990",
    "end": "2572200"
  },
  {
    "text": "It's not like it's equally\nthe same if I write j comma 1.",
    "start": "2572200",
    "end": "2577838"
  },
  {
    "text": "It's just like I write\nthis subscript because it's a little bit more intuitive.",
    "start": "2577839",
    "end": "2583599"
  },
  {
    "text": "But I just need an\nindex, in some sense. That makes sense? So basically, for every j,\nI'm going to have a parameter.",
    "start": "2583599",
    "end": "2591099"
  },
  {
    "text": "For every j and 1,\nI'm going to have a parameter that's called phi,\nand its parameter is in 0, 1,",
    "start": "2591099",
    "end": "2599609"
  },
  {
    "text": "right? And this parameter\ndescribes this distribution.",
    "start": "2599609",
    "end": "2605700"
  },
  {
    "text": "And for y is 0, I'm also\ngoing to have a parameter. So I'm going to write\nthis as j given y is 0.",
    "start": "2605700",
    "end": "2614240"
  },
  {
    "text": "So this is the parameter\nfor the distribution of xj given y is 0.",
    "start": "2614240",
    "end": "2622800"
  },
  {
    "text": "[INAUDIBLE] Yes, it's between 0 and 1.",
    "start": "2622800",
    "end": "2633550"
  },
  {
    "text": "[INAUDIBLE] This is between--\nthis is the bracket-- the hard bracket.",
    "start": "2633550",
    "end": "2642349"
  },
  {
    "text": "So basically, I'm\nsaying that if I--",
    "start": "2642349",
    "end": "2649369"
  },
  {
    "text": "with this parameter\nand this parameter, I can describe all of this.",
    "start": "2649369",
    "end": "2654569"
  },
  {
    "text": "I can write out all\nof these numbers. Because I have all\nthe quantities, right?",
    "start": "2654569",
    "end": "2660338"
  },
  {
    "text": "Because for example,\nwhat is p of xj given xj is 0 given y is 0.",
    "start": "2660339",
    "end": "2665980"
  },
  {
    "text": "This is going to be equal\nto 1 minus phi j y is 0.",
    "start": "2665980",
    "end": "2684990"
  },
  {
    "text": "And then, I also have, if I\nhave like something for this,",
    "start": "2684990",
    "end": "2690650"
  },
  {
    "text": "to parameterize the\ndistribution of y. We call that-- we call this\nphi before, in the GDA case.",
    "start": "2690650",
    "end": "2697220"
  },
  {
    "text": "And now, just for the\nsake of distinguishing it from the other phi's, I'm\ngoing to call it phi y.",
    "start": "2697220",
    "end": "2702799"
  },
  {
    "text": "But this serves as\nthe same row as phi in before, in the GDA case.",
    "start": "2702800",
    "end": "2718578"
  },
  {
    "text": "Any questions?",
    "start": "2718579",
    "end": "2746210"
  },
  {
    "text": "And then how do I proceed? I'm going to write\nthe likelihood and maximize the likelihood. All right. So the likelihood\nis the probability",
    "start": "2746210",
    "end": "2756859"
  },
  {
    "text": "of seeing a data given\nyour parameters, right? So likelihood is a\nfunction of the parameters. And this is the probability of\nthe data, given the parameters.",
    "start": "2756859",
    "end": "2765440"
  },
  {
    "text": "So what are the parameters? The parameters are phi y\nis one of the parameters. And also, all of these phi's.",
    "start": "2765440",
    "end": "2771430"
  },
  {
    "text": "Phi j given y is 0,\nand phi j given y is 1. So basically, I have phi",
    "start": "2771430",
    "end": "2776950"
  },
  {
    "text": "d, given y is 0.",
    "start": "2776950",
    "end": "2782940"
  },
  {
    "text": "And then phi 1 given y is These are all your parameters.",
    "start": "2782940",
    "end": "2791180"
  },
  {
    "text": "And my likelihood here, there\nare two ways I can expand this.",
    "start": "2791180",
    "end": "2800630"
  },
  {
    "text": "So the first thing is\nthat, because by definition the likelihood is the product\nof the likelihood of each",
    "start": "2800630",
    "end": "2807089"
  },
  {
    "text": "of the example, because all\nyour examples are independent. This is not naive Bayes yet.",
    "start": "2807089",
    "end": "2812190"
  },
  {
    "text": "This is the examples\nare independent. So you can just write this\nas probability of xi, yi,",
    "start": "2812190",
    "end": "2820160"
  },
  {
    "text": "given all the parameters. And here, if you are\ncareful, then you know--",
    "start": "2820160",
    "end": "2825309"
  },
  {
    "text": "OK, so basically, all the-- I guess all the parameters\nI'm going to write,",
    "start": "2825309",
    "end": "2831459"
  },
  {
    "text": "I think in my notes\nthe notation is this.",
    "start": "2831459",
    "end": "2838349"
  },
  {
    "text": "But I think this\nreally just means-- this is just a\nconvenience notation that denotes all of the parameters.",
    "start": "2838349",
    "end": "2843890"
  },
  {
    "text": "This is the same as this.",
    "start": "2843890",
    "end": "2850079"
  },
  {
    "text": "And then you-- first,\nyou-- so so far, it's",
    "start": "2850080",
    "end": "2855369"
  },
  {
    "text": "the same as the GDA. And then you can also\ndo the chain rule to make this xi given yi\nconditional parameters.",
    "start": "2855369",
    "end": "2866539"
  },
  {
    "text": "Given the parameters,\nand then times p of y, given the parameters.",
    "start": "2866540",
    "end": "2875380"
  },
  {
    "text": "So when I use dot, dot, dot, I\njust mean all the parameters. Of course, sometimes you\ncan drop some parameters, because they are not--\nthey don't matter.",
    "start": "2875380",
    "end": "2882460"
  },
  {
    "text": "And then, I'm going\nto, again, factorize in the dimension of x. Now I'm going to use my G--",
    "start": "2882460",
    "end": "2889609"
  },
  {
    "text": "Like this Naive\nBayes assumption, which is the factorization\nacross the coordinates of x.",
    "start": "2889609",
    "end": "2895240"
  },
  {
    "text": "Where you call that this is x\nsub i is the coordinate of x. The superscript i\nis the i-th example.",
    "start": "2895240",
    "end": "2902599"
  },
  {
    "text": "So what I'm going\nto do is that I'm going to use that assumption\nfor each of the examples.",
    "start": "2902599",
    "end": "2908599"
  },
  {
    "text": "So what I'm going to\nget is i from 1 to n.",
    "start": "2908599",
    "end": "2916940"
  },
  {
    "text": "I'm going to put this in\nfront, just to make a simple--",
    "start": "2916940",
    "end": "2922318"
  },
  {
    "text": "make it easier. And if I'm careful, I only\nhave to write phi y here,",
    "start": "2922319",
    "end": "2928859"
  },
  {
    "text": "because the distribution\nof y only depends on phi y. And then I'm going to\nfactorize this thing",
    "start": "2928859",
    "end": "2934680"
  },
  {
    "text": "across the coordinates. So I'm going to have a product\nacross the coordinates, have D",
    "start": "2934680",
    "end": "2941520"
  },
  {
    "text": "coordinates, and\nthen have p of x sub j, i, is the j-th\ncoordinate of the i-th example,",
    "start": "2941520",
    "end": "2950990"
  },
  {
    "text": "given the label for\nthe yi-th example and all the parameters phi jy.",
    "start": "2950990",
    "end": "2956780"
  },
  {
    "text": "Phi jy just means-- I guess, maybe I will just--",
    "start": "2956780",
    "end": "2962259"
  },
  {
    "text": "so phi jy, this just\nmeans a shorthand for this family of parameters.",
    "start": "2962260",
    "end": "2967609"
  },
  {
    "text": "Does it make sense? Maybe, I would just--\nsorry, this is bad notation.",
    "start": "2967609",
    "end": "2978520"
  },
  {
    "text": "So phi jy just means a\nshorthand for this collection of parameters.",
    "start": "2978520",
    "end": "2994280"
  },
  {
    "text": "So for this case there's\nonly two parameters, right? The j [INAUDIBLE]\nand [INAUDIBLE]..",
    "start": "2994280",
    "end": "3002030"
  },
  {
    "text": "So you mean here? Yeah.",
    "start": "3002030",
    "end": "3008780"
  },
  {
    "text": "Yep, that's right. If you only look at\nthis j and this j,",
    "start": "3008780",
    "end": "3016690"
  },
  {
    "text": "yes, you only care\nabout the-- you just care about the j and the fixed\nj, and the y is 0 and y is 1.",
    "start": "3016690",
    "end": "3037828"
  },
  {
    "text": "You said earlier you\nare not [INAUDIBLE]..",
    "start": "3037829",
    "end": "3044030"
  },
  {
    "text": "So we have two times that we\nfactorize these probabilities. So the first time is here.",
    "start": "3044030",
    "end": "3049410"
  },
  {
    "text": "So here, I'm using the\nfact that all the examples are independent examples.",
    "start": "3049410",
    "end": "3054690"
  },
  {
    "text": "So that's why I say the\njoint probability is the overall examples is the\nproduct of the probabilities",
    "start": "3054690",
    "end": "3061530"
  },
  {
    "text": "of each of the example. And now, for every\nexample, of course, I first do the chain\nrule to get x given y.",
    "start": "3061530",
    "end": "3069290"
  },
  {
    "text": "And then I factorize this\none into this product again. And this is the label\nof the Naive Bayes.",
    "start": "3069290",
    "end": "3075079"
  },
  {
    "text": "This is using Naive Bayes.",
    "start": "3075080",
    "end": "3080309"
  },
  {
    "text": "Cool. And then, so I guess you can\nexpect what we're going to do.",
    "start": "3080309",
    "end": "3087970"
  },
  {
    "text": "We are going to maximize this. And we know that if\nyou maximize this, it's the same as\nmaximize-- suppose let's",
    "start": "3087970",
    "end": "3095670"
  },
  {
    "text": "called this L. So\nmaximize L. This is the same as arg max of log L.",
    "start": "3095670",
    "end": "3102690"
  },
  {
    "text": "And log L is going to be a sum\nof the log of this probability.",
    "start": "3102690",
    "end": "3108520"
  },
  {
    "text": "So log L will be a sum--",
    "start": "3108520",
    "end": "3113809"
  },
  {
    "text": "you turn all of this\nto sum, and you have to log in front of the terms. So you have log p\nyi and dy, plus here",
    "start": "3113809",
    "end": "3125170"
  },
  {
    "text": "you have a double sum. Sum over i from 1 to L.\nSum over j from 1 to D. And the log of this.",
    "start": "3125170",
    "end": "3148230"
  },
  {
    "text": "And then, you analytically\nplug in all of this. So for example,\nfor this one, you",
    "start": "3148230",
    "end": "3153549"
  },
  {
    "text": "can-- you know what\nis this, right? This is equal to--\nyou know what is this? This is equal to\nphi y if yi is 1.",
    "start": "3153549",
    "end": "3161319"
  },
  {
    "text": "This is equal to 1 minus\nphi y if y is equal to 0. For each of this,\nyou can write them",
    "start": "3161319",
    "end": "3167680"
  },
  {
    "text": "as a formula of all of the\ndata and the parameters.",
    "start": "3167680",
    "end": "3173460"
  },
  {
    "text": "And then you do the\nmaximum likelihood. So the maximum likelihood, I\nguess I'll also just tell you",
    "start": "3173460",
    "end": "3180450"
  },
  {
    "text": "the solution.",
    "start": "3180450",
    "end": "3196808"
  },
  {
    "text": "So if you look at a gradient\nof L is 0, this is the--",
    "start": "3196809",
    "end": "3205020"
  },
  {
    "text": "right, this is a sufficient. It's a necessary condition\nfor you are being-- for you to be a maximizer.",
    "start": "3205020",
    "end": "3211619"
  },
  {
    "text": "I guess, technically I\nshould write nabla phi y.",
    "start": "3211619",
    "end": "3220069"
  },
  {
    "text": "And you compute this gradient,\nand you solve this equation. This is your family\nof equations. And then this\nsolving it gives some",
    "start": "3220070",
    "end": "3231500"
  },
  {
    "text": "formulas for the parameters\nyou are recovering. So the final solution\nwill look like this.",
    "start": "3231500",
    "end": "3237780"
  },
  {
    "text": "Phi y is equal to--",
    "start": "3237780",
    "end": "3245530"
  },
  {
    "text": "over n.",
    "start": "3245530",
    "end": "3252200"
  },
  {
    "text": "This is pretty intuitive. This is the fraction\nof positive examples.",
    "start": "3252200",
    "end": "3258150"
  },
  {
    "text": "Actually, it's the same formula\nas we have seen for the GDA.",
    "start": "3258150",
    "end": "3263770"
  },
  {
    "text": "And then phi jy is 1. This is the probability to\nsee xj is 1 given y is 1.",
    "start": "3263770",
    "end": "3272380"
  },
  {
    "text": "It turns out that this\nis also something simple. So you look at--",
    "start": "3272380",
    "end": "3278950"
  },
  {
    "text": "maybe let me write down the\nformula, and then interpret it.",
    "start": "3278950",
    "end": "3295609"
  },
  {
    "text": "So \nwhat is this numerator?",
    "start": "3295609",
    "end": "3304170"
  },
  {
    "text": "This is the number of\noccurrences of i-th word,",
    "start": "3304170",
    "end": "3321579"
  },
  {
    "text": "right? It's only 1 when the i-th word-- the i-th example\ncontains the j-th word.",
    "start": "3321580",
    "end": "3329190"
  },
  {
    "text": "So the j-th word-- this is x ji is 1 means\nthat j-th word shows up",
    "start": "3329190",
    "end": "3335730"
  },
  {
    "text": "in i-th example.",
    "start": "3335730",
    "end": "3342650"
  },
  {
    "text": "And you also require\nthat the i-th example is positive example. So basically, the sum\nis the total number of occurrences of j-th\nword in positive examples.",
    "start": "3342650",
    "end": "3354000"
  },
  {
    "text": "And this is the number\nof positive examples.",
    "start": "3354000",
    "end": "3364160"
  },
  {
    "text": "So you can see\nthat even though we have done a lot of\ncalculation and modeling, at the end of day, the\nformula is pretty intuitive.",
    "start": "3364160",
    "end": "3371290"
  },
  {
    "text": "You are in some\nsense just counting-- it's kind of like\nsomething about counting. You're doing some statistics. You're counting how many\ntimes the j-th word shows up",
    "start": "3371290",
    "end": "3378890"
  },
  {
    "text": "in positive examples. And you divide that by how many\ntotal positive examples there",
    "start": "3378890",
    "end": "3385080"
  },
  {
    "text": "are. So for example, suppose\nlike the word book shows up in 10 positive\nexamples, and they",
    "start": "3385080",
    "end": "3392000"
  },
  {
    "text": "are like a million\npositive examples. That means this is",
    "start": "3392000",
    "end": "3397660"
  },
  {
    "text": "Which kind of means\nthat book doesn't seem to have much correlation\nwith positive examples, if it is-- this\nnumber is smaller.",
    "start": "3397660",
    "end": "3402700"
  },
  {
    "text": "If this number is\nsmall, it means that it's unlikely to\nsee the j-th word given positive example.",
    "start": "3402700",
    "end": "3408520"
  },
  {
    "text": "So when it's unlikely-- it's unlikely because--\npartly because the data, they don't show up.",
    "start": "3408520",
    "end": "3415390"
  },
  {
    "text": "And for the negative\nexamples, it's the same. So you just write--",
    "start": "3415390",
    "end": "3421480"
  },
  {
    "text": "it's kind of symmetric. So phi of j given y is 0 is\nequal to something like--",
    "start": "3421480",
    "end": "3432380"
  },
  {
    "text": "so you can guess\nwhat it is, right? Basically just change every--",
    "start": "3432380",
    "end": "3440000"
  },
  {
    "text": "the value of y to 0. Any question?",
    "start": "3440000",
    "end": "3447609"
  },
  {
    "text": "So basically, we\nare done with this.",
    "start": "3447609",
    "end": "3465869"
  },
  {
    "text": "At least we are almost done.",
    "start": "3465869",
    "end": "3472849"
  },
  {
    "text": "There's one small thing\nthat we have to address. But in terms of the i-th\nparameter, we are done.",
    "start": "3472849",
    "end": "3490589"
  },
  {
    "text": "And we got a parameter. And at a prediction time,\nas before, right, so we will do the prediction.",
    "start": "3490589",
    "end": "3498170"
  },
  {
    "text": "As usual, you want to\ncompute p of y is 1 given x. And if this number\nis larger than 0.5,",
    "start": "3498170",
    "end": "3504750"
  },
  {
    "text": "then you say this is\na positive example. If this number is\nless than 0.5, you say this is a negative example.",
    "start": "3504750",
    "end": "3511359"
  },
  {
    "text": "So we have to compute this. And how do we compute this? This is, again, I think\nour general methodology use",
    "start": "3511360",
    "end": "3519450"
  },
  {
    "text": "the Bayes rule. And divide by p of x. So this is still the same.",
    "start": "3519450",
    "end": "3528029"
  },
  {
    "text": "But there is one\nsmall caveat here, which is that what if you have\na 0 divided by 0 situation?",
    "start": "3528030",
    "end": "3536859"
  },
  {
    "text": "So before, we have Gaussian. The density, no\nmatter what you do, the density is always\nnon-zero at every places.",
    "start": "3536859",
    "end": "3543320"
  },
  {
    "text": "Even though sometimes\nit could be very small. The density could be very small. But still, the px in\nall of these quantities",
    "start": "3543320",
    "end": "3548680"
  },
  {
    "text": "are all positive,\nstrictly positive. At least you are going\nto get a ratio here.",
    "start": "3548680",
    "end": "3555140"
  },
  {
    "text": "But here, there might be\nsome cases where your p of x is just literally zero. And why that can happen--",
    "start": "3555140",
    "end": "3561240"
  },
  {
    "text": "I guess, maybe let me\njust give you an example. So you may think that some\nexample just never shows up,",
    "start": "3561240",
    "end": "3567980"
  },
  {
    "text": "just because of some-- I guess, let me show\nthe example of cases. So suppose, maybe\nlet's say suppose--",
    "start": "3567980",
    "end": "3578270"
  },
  {
    "text": "so suppose maybe the\nword aardvark never",
    "start": "3578270",
    "end": "3586619"
  },
  {
    "text": "appears \nin training set.",
    "start": "3586619",
    "end": "3595849"
  },
  {
    "text": "And I'm claiming that\nthis would mean that-- this would mean\nthat if you have a--",
    "start": "3595849",
    "end": "3601960"
  },
  {
    "text": "but your test\nexample ",
    "start": "3601960",
    "end": "3608920"
  },
  {
    "text": "contain it. So let's say test example\nis called x, contains it.",
    "start": "3608920",
    "end": "3616010"
  },
  {
    "text": "And I'm going to claim that p\nof x will be considered as 0. Why?",
    "start": "3616010",
    "end": "3622220"
  },
  {
    "text": "I guess, let's somewhat\nsimilar this algorithm, and see what the phi will-- we're going to\ncompute from this.",
    "start": "3622220",
    "end": "3640069"
  },
  {
    "text": "So aardvark is the\nsecond word, right?",
    "start": "3640069",
    "end": "3645420"
  },
  {
    "text": "So j is 2. So you know x to j, x\nto i is 0 for every i.",
    "start": "3645420",
    "end": "3657500"
  },
  {
    "text": "This is a mathematical\ntranslation of aardvark never shows up in\nthe training set.",
    "start": "3657500",
    "end": "3662590"
  },
  {
    "text": "For every example, the\nsecond word never shows up. That's why the x to i is 0.",
    "start": "3662590",
    "end": "3668578"
  },
  {
    "text": "And when the x to i is into this formula\nthat tries to estimate",
    "start": "3668579",
    "end": "3674789"
  },
  {
    "text": "this parameter-- suppose let's\ntry to ask me the parameter phi to y is 1, right?",
    "start": "3674789",
    "end": "3680240"
  },
  {
    "text": "This parameter intuitively\nmeans that how likely the second word will show\nup in a positive example.",
    "start": "3680240",
    "end": "3686230"
  },
  {
    "text": "And recall that this\nformula is really about the total\nnumber of occurrences",
    "start": "3686230",
    "end": "3691869"
  },
  {
    "text": "that i-th word shows up, and\ndivided by the total number of positive examples. So this will be 0, because no\noccurrences of the second word.",
    "start": "3691869",
    "end": "3701160"
  },
  {
    "text": "Divided by the total number\nof positive examples. And this will be 0.",
    "start": "3701160",
    "end": "3706390"
  },
  {
    "text": "That's still fun. So far, it's not a problem.",
    "start": "3706390",
    "end": "3711990"
  },
  {
    "text": "So 0 divided by a positive\nnumber, that's fine. And phi to y0 is the same.",
    "start": "3711990",
    "end": "3717568"
  },
  {
    "text": "It's 0 over total number\nof negative examples.",
    "start": "3717569",
    "end": "3725270"
  },
  {
    "text": "This is also 0. So basically, according to\nyour estimate, MLE estimate,",
    "start": "3725270",
    "end": "3732990"
  },
  {
    "text": "this word aardvark just\ncannot show up at all, right? Which makes sense, because\nit didn't show up--",
    "start": "3732990",
    "end": "3738550"
  },
  {
    "text": "it didn't show up\nin the training set. In your estimate,\nyou also say it shouldn't show up at all in this\nunder this set of parameters.",
    "start": "3738550",
    "end": "3747548"
  },
  {
    "text": "But that's a\nproblem, because now, if you compute p\nof x, for example x",
    "start": "3747549",
    "end": "3754490"
  },
  {
    "text": "that does contain\nthe second word, then you're going to\nwrite this as p of x.",
    "start": "3754490",
    "end": "3759578"
  },
  {
    "text": "So how do you compute this? You use the total\nlaw of probability. You say this is\nequal to the case",
    "start": "3759579",
    "end": "3766880"
  },
  {
    "text": "when y is 1, plus\nthe case when y is 0.",
    "start": "3766880",
    "end": "3774298"
  },
  {
    "text": "And of course, this\nis a positive number, this is a positive number. That's fine.",
    "start": "3774299",
    "end": "3779520"
  },
  {
    "text": "But this one is equal to-- maybe I'll just write\nit-- this one is",
    "start": "3779520",
    "end": "3785119"
  },
  {
    "text": "equal to the--\nusing Naive Bayes-- this is equal to\nxj given y is 1. And you have sum--",
    "start": "3785120",
    "end": "3791460"
  },
  {
    "text": "product over j from j to D.",
    "start": "3791460",
    "end": "3797029"
  },
  {
    "text": "And now, I know that my x2 is 1. Because this word does\nshow up in this email.",
    "start": "3797029",
    "end": "3804710"
  },
  {
    "text": "And that means that p-- you're going to have\np of x2 equals to 1,",
    "start": "3804710",
    "end": "3809970"
  },
  {
    "text": "given y is 1 as the second\nterm in this product, right?",
    "start": "3809970",
    "end": "3815160"
  },
  {
    "text": "So you're going to have this\nterm shows up in this product. But this one is equal to phi",
    "start": "3815160",
    "end": "3824799"
  },
  {
    "text": "So just because of the second\nword, according to your model, it's not supposed to show up. But it does show up.",
    "start": "3824799",
    "end": "3830600"
  },
  {
    "text": "That means that\nthis example just does have zero probability\nin your probabilistic model. So that's why this is zero.",
    "start": "3830600",
    "end": "3837539"
  },
  {
    "text": "And for the same reason,\nso why this one is going to be equals to \ny is 0.",
    "start": "3837539",
    "end": "3847309"
  },
  {
    "text": "And also, you have this term p\nof x2 equals to 1 given y is 0.",
    "start": "3847309",
    "end": "3853789"
  },
  {
    "text": "And recall that under a\nprobabilistic model you learned, you just think this\nword cannot show up at all.",
    "start": "3853789",
    "end": "3859240"
  },
  {
    "text": "The chance is zero. So that's why the chance to\nsee this word shows up is zero. So this is zero.",
    "start": "3859240",
    "end": "3865130"
  },
  {
    "text": "So that's why this is also zero. So because this zero terms are\neventually, I guess if you--",
    "start": "3865130",
    "end": "3871960"
  },
  {
    "text": "because there's a\nzero here, there's a zero here, so you get\nzero just eventually. So basically, you\nconclude that this example",
    "start": "3871960",
    "end": "3878710"
  },
  {
    "text": "isn't supposed to show up. Like this example\nhas zero probability under this probabilistic\nmodel that you learned.",
    "start": "3878710",
    "end": "3887670"
  },
  {
    "text": "And then, that's a problem,\nbecause now this p of x is zero. This is zero.",
    "start": "3887670",
    "end": "3893640"
  },
  {
    "text": "So you have something\ndivided by zero. Actually, this is also\nzero, if you think about it. Because this is\njust the one term.",
    "start": "3893640",
    "end": "3899500"
  },
  {
    "text": "The numerator is just one term\nin the decomposition of px. The numerator is just this term.",
    "start": "3899500",
    "end": "3905410"
  },
  {
    "text": "And px is the sum\nof the two terms. So both of these, the\nnumerator and denominator, they are both zero.",
    "start": "3905410",
    "end": "3912258"
  },
  {
    "text": "And you have the zero\ndivided by zero situation.",
    "start": "3912259",
    "end": "3917970"
  },
  {
    "text": "And what do you do? So this is an issue. And this is a reasonably\nrealistic issue,",
    "start": "3917970",
    "end": "3924328"
  },
  {
    "text": "because sometimes, you just\nsee a new word in your test example. You haven't seen this work\nat all in the training set.",
    "start": "3924329",
    "end": "3931869"
  },
  {
    "text": "So the way we deal with this is\nso-called Laplacian smoothing.",
    "start": "3931869",
    "end": "3936930"
  },
  {
    "text": "In some sense, this as a way to\nintroduce a little bit prior, so that you don't 100%\ntrust your training set.",
    "start": "3936930",
    "end": "3945599"
  },
  {
    "text": "So basically, what\nwe are seeing here is that you trust the training\nset just 100%, like religiously in some sense.",
    "start": "3945599",
    "end": "3951820"
  },
  {
    "text": "Like you haven't seen any\nword, the word aardvark in the training set.",
    "start": "3951820",
    "end": "3958660"
  },
  {
    "text": "Because of that, you trust it. You just say, this word, this\nshouldn't show up at all. That's why if you see this-- if\nthis word is in this x, right,",
    "start": "3958660",
    "end": "3966539"
  },
  {
    "text": "so then the probability\nof x is just zero. So what we are going to\ndo is we're going to say, just maybe we shouldn't\ntrust the data exactly.",
    "start": "3966539",
    "end": "3973990"
  },
  {
    "text": "We are going to allow any new\nword to show up a little bit, with some small chance.",
    "start": "3973990",
    "end": "3980220"
  },
  {
    "text": "And in some sense, this\nis a local adjustment by using some prior\nknowledge, which",
    "start": "3980220",
    "end": "3986620"
  },
  {
    "text": "is called Laplace smoothing.",
    "start": "3986620",
    "end": "4004780"
  },
  {
    "text": "So I guess just to-- the best way to\ndescribe this method",
    "start": "4004780",
    "end": "4010700"
  },
  {
    "text": "is start with\nsomething abstract. For the moment, let's\nforget about that",
    "start": "4010700",
    "end": "4016079"
  },
  {
    "text": "for the moment and just think\nabout the abstract question. Suppose you have--\nso a simple example.",
    "start": "4016079",
    "end": "4023250"
  },
  {
    "text": "Maybe you can call this example\nor abstraction in some sense.",
    "start": "4023250",
    "end": "4028390"
  },
  {
    "text": "So suppose you think about your\nestimate, the bias of a coin.",
    "start": "4028390",
    "end": "4040390"
  },
  {
    "text": "Suppose you have a coin. This coin is biased. It's not 50/50. So how do you know\nthe bias of the coin?",
    "start": "4040390",
    "end": "4047568"
  },
  {
    "text": "In mathematical\nlanguage, it really means that you have a\nrandom variable z, which is from Bernoulli, where\nwith a parameter phi.",
    "start": "4047569",
    "end": "4059820"
  },
  {
    "text": "And this phi is\nsomething unknown. It's not a half. And you want to know\nwhat the phi is.",
    "start": "4059820",
    "end": "4065170"
  },
  {
    "text": "And you want to know it by\nlooking at some data, right? So we draw a few copies\nfrom this distribution. And we want to know what's phi?",
    "start": "4065170",
    "end": "4072280"
  },
  {
    "text": "You want to estimate\nphi by using the data.",
    "start": "4072280",
    "end": "4079020"
  },
  {
    "text": "And you want to solve\nthis problem, right? This is still a\nprobabilistic model. We can still do the same\nthing where you can write out",
    "start": "4079020",
    "end": "4084481"
  },
  {
    "text": "a probabilistic-- you\ncan write out the MLE, and you can write out-- you\ncan maximize the MLE, right?",
    "start": "4084481",
    "end": "4091588"
  },
  {
    "text": "So maybe let's try to do this. So suppose you have\nmaybe n trials.",
    "start": "4091589",
    "end": "4096870"
  },
  {
    "text": "And let's call this\nz1, z2, and zn.",
    "start": "4096870",
    "end": "4105500"
  },
  {
    "text": "And each of these is either 1 or",
    "start": "4105500",
    "end": "4110810"
  },
  {
    "text": "Maybe I guess in my notes\nit's called tail and heads. So I'll just call\nit tail and heads.",
    "start": "4110810",
    "end": "4117130"
  },
  {
    "text": "Something like this. And you want to\nestimate what phi is.",
    "start": "4117130",
    "end": "4122698"
  },
  {
    "text": "And if you follow our\ngeneral principle, right, we will try to\nwrite out the likelihood. And the likelihood,\nwhat is the likelihood?",
    "start": "4122699",
    "end": "4135369"
  },
  {
    "text": "So the likelihood of phi,\nright, of the parameter phi,",
    "start": "4135370",
    "end": "4142159"
  },
  {
    "text": "is the chance to see this data\nset given the parameter phi. So what's the chance\nto see this data set?",
    "start": "4142159",
    "end": "4148579"
  },
  {
    "text": "The chance to see the\nfirst one is 1 minus phi. The chance to see the second\none is one, I guess tail means--",
    "start": "4148580",
    "end": "4156270"
  },
  {
    "text": "I think I need to\ndefine tail means what? Tail means 0, let's say. And heads means 1.",
    "start": "4156270",
    "end": "4163298"
  },
  {
    "text": "So the chance to-- and you\nhave the probability to see 1",
    "start": "4163299",
    "end": "4169259"
  },
  {
    "text": "is phi, and probability\nto see 0 is 1 minus phi. So that's why the probability\nto see the first example",
    "start": "4169260",
    "end": "4174380"
  },
  {
    "text": "is 1 minus phi, and\nyou have 1 minus phi for the second example. And you might have a bunch. And then you multiply\nphi at the end.",
    "start": "4174380",
    "end": "4180678"
  },
  {
    "text": "This is the likelihood. And if you organize\nthis, you count how many 1 minus phi there\nare, how many phi's there are.",
    "start": "4180679",
    "end": "4187818"
  },
  {
    "text": "This will be 1 minus phi to the\npower of the number of tails,",
    "start": "4187819",
    "end": "4196800"
  },
  {
    "text": "and times phi to power of heads. Actually, if you really\nlook at this example,",
    "start": "4196800",
    "end": "4205280"
  },
  {
    "text": "you will see that the variables\nare very, very similar. Actually, this example is a toy\ncase for that, in some sense.",
    "start": "4205280",
    "end": "4211840"
  },
  {
    "text": "So you get this. And then you can take the\narg max of this like log",
    "start": "4211840",
    "end": "4218650"
  },
  {
    "text": "likelihood. You take the arg max\nof the log likelihood. So the arg max--",
    "start": "4218650",
    "end": "4225239"
  },
  {
    "text": "maybe let's call this i of phi. So the arg max of log of\ni of phi, if you solve it,",
    "start": "4225239",
    "end": "4236630"
  },
  {
    "text": "you are going to\nget the following. So it's going to be\nthe number of heads over the number of heads\nplus the number of tails.",
    "start": "4236630",
    "end": "4250849"
  },
  {
    "text": "Which also makes sense\nbecause this is basically the empirical frequency\nof seeing the heads. The empirical, I just mean\nlike the-- the frequency",
    "start": "4250850",
    "end": "4259670"
  },
  {
    "text": "that you see the heads\nin the training set. And that's your most\nlikely estimate for phi.",
    "start": "4259670",
    "end": "4274240"
  },
  {
    "text": "Right.",
    "start": "4274240",
    "end": "4281480"
  },
  {
    "text": "So far, everything seems\nto make sense, right? But now, let's consider\na somewhat extreme case.",
    "start": "4281480",
    "end": "4289740"
  },
  {
    "text": "So what if you see-- all the examples you\nsee are tails, right? And you don't have a lot\nof-- suppose you just have",
    "start": "4289740",
    "end": "4297770"
  },
  {
    "text": "three draws, z1, z2, and z3. And they are all tail.",
    "start": "4297770",
    "end": "4307300"
  },
  {
    "text": "So according to this formula,\nyou are going to get the phi, the best estimate\nof phi is 0 over 0",
    "start": "4307300",
    "end": "4313050"
  },
  {
    "text": "plus 3, which is equal to 0. But do you really\ntrust this, right?",
    "start": "4313050",
    "end": "4320659"
  },
  {
    "text": "Do you really trust that\nthis coin just never give you a head?",
    "start": "4320660",
    "end": "4326719"
  },
  {
    "text": "What does this mean? This means that this coin is\nnever give you a head at all, forever, right?",
    "start": "4326719",
    "end": "4332070"
  },
  {
    "text": "Should you really trust that? This is a little bit subjective.",
    "start": "4332070",
    "end": "4337239"
  },
  {
    "text": "If you really, really trust\nthe data, you'll probably you can say yes. But maybe you have\nsome prior knowledge that most of the coins\nare not that crazy.",
    "start": "4337239",
    "end": "4345389"
  },
  {
    "text": "So you probably-- at\nleast have some chance to see the head\nwith some chance.",
    "start": "4345390",
    "end": "4350699"
  },
  {
    "text": "So in some sense, you can-- so basically, on\nthe flip side, you can say, OK, maybe this\nis just some coincidence.",
    "start": "4350699",
    "end": "4357320"
  },
  {
    "text": "It just happens that\nyou see three tails. Even this phi is a half, right? Seeing this example,\nseeing this case is--",
    "start": "4357320",
    "end": "4365080"
  },
  {
    "text": "the probability to see this case\nis at least 1 over 8, right? It's something like 1 over 8. So maybe it's just\ncoincidence of the data set.",
    "start": "4365080",
    "end": "4372869"
  },
  {
    "text": "So maybe you shouldn't trust\nthe data set that much. So the so-called\nLaplace smoothing is a way to, in some sense,\nincorporate the prior",
    "start": "4372870",
    "end": "4381010"
  },
  {
    "text": "so that you say, look, my coin\nshouldn't be extreme, shouldn't be too extreme like this.",
    "start": "4381010",
    "end": "4387250"
  },
  {
    "text": "So basically, the\nLaplace smoothing refers to the\nfollowing estimator.",
    "start": "4387250",
    "end": "4392369"
  },
  {
    "text": "So if you use Laplace\nsmoothing, you're going to have a\ndifferent formula. Your phi will be equal\nto the number of heads,",
    "start": "4392370",
    "end": "4400190"
  },
  {
    "text": "plus 1 over the number of\nheads, plus the number of tails,",
    "start": "4400190",
    "end": "4407400"
  },
  {
    "text": "plus 2. So this formula, you\nknow, I didn't tell you--",
    "start": "4407400",
    "end": "4414410"
  },
  {
    "text": "and it's like a\nmathematical justification. Actually, if you really\nlook in the there they are mathematical\njustifications.",
    "start": "4414410",
    "end": "4420389"
  },
  {
    "text": "So far, I'm just\ntelling you the formula. But it would solve\nthis problem, right? Like at least to some extent.",
    "start": "4420389",
    "end": "4425639"
  },
  {
    "text": "Because for this\nparticular case, you are going to get",
    "start": "4425639",
    "end": "4431678"
  },
  {
    "text": "And 4 for the denominator. So instead of getting 0,\nyou're getting 1 over 4.",
    "start": "4431679",
    "end": "4437710"
  },
  {
    "text": "So still, you say, OK,\nthe chance to see the head is pretty small, right? It's smaller than the\nchance to see the tail.",
    "start": "4437710",
    "end": "4444280"
  },
  {
    "text": "But you don't have a\nvery extreme estimate. And you can see\nthat this Laplace",
    "start": "4444280",
    "end": "4450579"
  },
  {
    "text": "smoothing is most\nuseful when you have not enough data, right? If you have a lot of data,\nthen this Laplace smoothing",
    "start": "4450580",
    "end": "4456880"
  },
  {
    "text": "is not doing much. Maybe let me give you\na kind of a example. For example, suppose you\nhave a big data, where",
    "start": "4456880",
    "end": "4465670"
  },
  {
    "text": "the number of heads, let's say,\nI guess I'm making up this, right? So 100.",
    "start": "4465670",
    "end": "4471170"
  },
  {
    "text": "And number of tails is 60. So if you use the\nstandard approach, if you use the\nstandard approach,",
    "start": "4471170",
    "end": "4478210"
  },
  {
    "text": "or maybe I shouldn't call--\nlike the vanilla approach. Then this is like the--",
    "start": "4478210",
    "end": "4483769"
  },
  {
    "text": "phi would be equals to 60\nover 60 plus 100, right?",
    "start": "4483770",
    "end": "4492320"
  },
  {
    "text": "This is-- what is this? This is like-- I don't know it's\nsomething like this.",
    "start": "4492320",
    "end": "4499850"
  },
  {
    "text": "And then if you use\nthe Laplace smoothing, then this phi will be 60 plus",
    "start": "4499850",
    "end": "4514560"
  },
  {
    "text": "which will be 61 over 162. This is 60 over 160.",
    "start": "4514560",
    "end": "4521990"
  },
  {
    "text": "And these two are just\nthose very similar. Just because the one, whatever\nyou change here, 1, 2, right?",
    "start": "4521990",
    "end": "4527650"
  },
  {
    "text": "It doesn't really\nmatter that much, because the dominating\nterm is the 60 and 100.",
    "start": "4527650",
    "end": "4533210"
  },
  {
    "text": "And sometimes,\nyou achieve like-- you achieve a kind of balance. If you have enough\ndata, then your prior--",
    "start": "4533210",
    "end": "4540350"
  },
  {
    "text": "or like your Laplace\nsmoothing is not doing much. It doesn't really change much. You trust your data. And if you don't\nhave enough data,",
    "start": "4540350",
    "end": "4546720"
  },
  {
    "text": "then Laplace smoothing will\ntry to make it not too extreme. Will try to make your\nestimate not too extreme.",
    "start": "4546720",
    "end": "4571600"
  },
  {
    "text": "So now, let's go\nback to our problem. So let me see, where is the\nbest place to write this.",
    "start": "4571600",
    "end": "4582989"
  },
  {
    "text": "I think I erased the-- oh, I erased the formula.",
    "start": "4582989",
    "end": "4588719"
  },
  {
    "text": "But I will write it again.",
    "start": "4588719",
    "end": "4605640"
  },
  {
    "text": "So going back to our\nspam filtering thing. I guess, actually, there's\none thing that is left here.",
    "start": "4605640",
    "end": "4611830"
  },
  {
    "text": "This is our estimate, right? And our other\nestimate was something",
    "start": "4611830",
    "end": "4617260"
  },
  {
    "text": "like phi j y is 1 is equal to--",
    "start": "4617260",
    "end": "4623590"
  },
  {
    "text": "am I writing the\nsuperscript correctly?",
    "start": "4623590",
    "end": "4630638"
  },
  {
    "text": "I think this is i, my bad.",
    "start": "4630639",
    "end": "4644989"
  },
  {
    "text": "Right. So if you apply\nthis to our case,",
    "start": "4644989",
    "end": "4651500"
  },
  {
    "text": "so you can recall\nthat in this case, it's kind of like you are\nsaying that the numerator is",
    "start": "4651500",
    "end": "4658630"
  },
  {
    "text": "the number of times\nthis word shows up in the positive example. And the denominator is\nlike the total number",
    "start": "4658630",
    "end": "4668179"
  },
  {
    "text": "of positive examples. So in some sense, the analogy\nis that this denominator is very similar to the\nnumber of heads and tails.",
    "start": "4668179",
    "end": "4676000"
  },
  {
    "text": "Because the heads means\nthat this word shows up. The tails means this\nword doesn't show up.",
    "start": "4676000",
    "end": "4681650"
  },
  {
    "text": "And here, the denominator\nis like the total number of times-- the total number of\nexamples, positive examples.",
    "start": "4681650",
    "end": "4689400"
  },
  {
    "text": "And here, the numerator\nis kind of like the heads. So basically, heads means\nthat the word show up",
    "start": "4689400",
    "end": "4696801"
  },
  {
    "text": "in positive example. And tails means that\nthe word doesn't show up in a positive example.",
    "start": "4696801",
    "end": "4702520"
  },
  {
    "text": "So that's why this is like\nheads, number of heads. And this is number of heads\nand plus number of tails. And if you use\nLaplace smoothing,",
    "start": "4702520",
    "end": "4709099"
  },
  {
    "text": "then you add 1 to the\ndenominator, and you add-- to the numerator, and you\nadd 2 to the denominator.",
    "start": "4709100",
    "end": "4717030"
  },
  {
    "text": "So that's the Laplace smoothing. And the same thing\nfor the other formula.",
    "start": "4717030",
    "end": "4723340"
  },
  {
    "text": "So you're going to\nadd 1 and add 2 here. So that's the Laplace smoothing.",
    "start": "4723340",
    "end": "4729579"
  },
  {
    "text": "And while this\nsolves our problem, this solves our problem because\nnow we just never found--",
    "start": "4729580",
    "end": "4737369"
  },
  {
    "text": "we just never estimate\nany parameter phi to be exactly 0, right?",
    "start": "4737370",
    "end": "4742870"
  },
  {
    "text": "So recall that when we have\nthis aardvark issue, right,",
    "start": "4742870",
    "end": "4751360"
  },
  {
    "text": "so the issue was that these\ntwo parameters for the aardvark was exactly zero.",
    "start": "4751360",
    "end": "4756639"
  },
  {
    "text": "Yeah. Let me ask. Do we do the same for\n[INAUDIBLE] though it's",
    "start": "4756639",
    "end": "4768690"
  },
  {
    "text": "really frequent [INAUDIBLE]. That's a fantastic question. So the question was\nthat whether you",
    "start": "4768690",
    "end": "4773820"
  },
  {
    "text": "want to do the same Laplace\nsmoothing for the phi y. Phi y was the single scalar to\ndescribe the probability of y--",
    "start": "4773820",
    "end": "4783920"
  },
  {
    "text": "to describe the probability\nof each of the class. And you are right.",
    "start": "4783920",
    "end": "4789199"
  },
  {
    "text": "So suppose one class just\nnever shows up, right? And you only have negative\nclass or positive class.",
    "start": "4789199",
    "end": "4794489"
  },
  {
    "text": "Then you probably should use\nLaplace smoothing for that. But I think this is a\nlittle bit less important,",
    "start": "4794489",
    "end": "4800239"
  },
  {
    "text": "because in most of\nthe data set, you have to see positive examples\nand negative examples. You have to see a\nreasonable number of them.",
    "start": "4800239",
    "end": "4806530"
  },
  {
    "text": "Maybe 50 of positive, And then Laplace smoothing will\nnot really matter that much. You can still use it, but it\nwouldn't matter that much.",
    "start": "4806530",
    "end": "4817030"
  },
  {
    "text": "So going back to this. So recall that our\nproblem was that when",
    "start": "4817030",
    "end": "4822059"
  },
  {
    "text": "you have the parameters,\nyou get this zero, right? You thought aardvark\nshouldn't show up at all.",
    "start": "4822060",
    "end": "4827480"
  },
  {
    "text": "You get this very\nextreme estimate. And now, when you add\nthis 1 and 2 here,",
    "start": "4827480",
    "end": "4835190"
  },
  {
    "text": "so what is going to happen\nis that instead of having 0 over the number of\npositive examples, you got 0 plus 1\nover this plus 2.",
    "start": "4835190",
    "end": "4843280"
  },
  {
    "text": "Yeah. I don't have a\ndifferent color, sorry. So I have to just modify.",
    "start": "4843280",
    "end": "4848850"
  },
  {
    "text": "I just only have black pens. So you get this. And then this will be\nat least no longer--",
    "start": "4848850",
    "end": "4855270"
  },
  {
    "text": "so this will be still\na pretty small number. This is 1 over the number\nof positive example. Plus 2.",
    "start": "4855270",
    "end": "4861280"
  },
  {
    "text": "But at least, this\nis bigger than 0.",
    "start": "4861280",
    "end": "4867869"
  },
  {
    "text": "And the same thing\nfor this, right? So you're going to get plus 1.",
    "start": "4867870",
    "end": "4874330"
  },
  {
    "text": "And this plus 2. And this is bigger than 0. So if both of these\nare bigger than 0, then when you\nevaluate this formula,",
    "start": "4874330",
    "end": "4880179"
  },
  {
    "text": "you are not going\nto evaluate to 0. So our p of x will be\nsome positive number.",
    "start": "4880179",
    "end": "4885329"
  },
  {
    "text": "So then you can get a number\nof the p of y given x.",
    "start": "4885330",
    "end": "4902110"
  },
  {
    "text": "And maybe just a\nvery small extension. We have two minutes,\ntwo or three minutes so if you have more\nthan two classes, you can--",
    "start": "4902110",
    "end": "4911489"
  },
  {
    "text": "this is just for interest. So suppose you have maybe a\ndice, something like that.",
    "start": "4911489",
    "end": "4919300"
  },
  {
    "text": "Instead of like a coin. So suppose you have z, which\nis from something like 0,",
    "start": "4919300",
    "end": "4926150"
  },
  {
    "text": "So you have k choices.",
    "start": "4926150",
    "end": "4931469"
  },
  {
    "text": "Then the Laplace-- the\ngeneral Laplace smoothing would be something like if you\ndon't do Laplace smoothing,",
    "start": "4931469",
    "end": "4937510"
  },
  {
    "text": "what you're going to\nhave is that you estimate this to be something like\nthe number of times zi",
    "start": "4937510",
    "end": "4948380"
  },
  {
    "text": "is equal to j, over the\ntotal number of examples.",
    "start": "4948380",
    "end": "4953920"
  },
  {
    "text": "So you count how many examples. Kind of end up to the choice j. And you divide by the\ntotal of examples.",
    "start": "4953920",
    "end": "4960380"
  },
  {
    "text": "And if you use\nLaplace smoothing, you're going to\nadd 1 to the top, and you add k to the bottom.",
    "start": "4960380",
    "end": "4965929"
  },
  {
    "text": "Where k is the\nnumber of choices. So this is just a small\nextension of Laplace smoothing.",
    "start": "4965929",
    "end": "4972730"
  },
  {
    "text": "No, I don't think this\ncourse will use it again. But for interest.",
    "start": "4972730",
    "end": "4978340"
  },
  {
    "text": "OK cool. I guess that's pretty\nmuch all for today. Any questions?",
    "start": "4978340",
    "end": "4984130"
  },
  {
    "text": "OK, great. Yeah, in the next\nlecture I guess",
    "start": "4984130",
    "end": "4993480"
  },
  {
    "text": "we're going to talk\nabout kernel method, and then we're going to\ntalk with deep learning.",
    "start": "4993480",
    "end": "4998090"
  }
]