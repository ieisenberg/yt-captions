[
  {
    "start": "0",
    "end": "55000"
  },
  {
    "start": "0",
    "end": "940"
  },
  {
    "text": "Welcome back.",
    "start": "940",
    "end": "2200"
  },
  {
    "text": "In the last section, we\ntalked about validation,",
    "start": "2200",
    "end": "4300"
  },
  {
    "text": "and we saw some drawbacks\nwith that method.",
    "start": "4300",
    "end": "6260"
  },
  {
    "text": "Now, we're going to talk about\nk-fold cross-validation, which",
    "start": "6260",
    "end": "8802"
  },
  {
    "text": "will solve some\nof these problems.",
    "start": "8802",
    "end": "10608"
  },
  {
    "text": "This is actually a very\nimportant technique",
    "start": "10608",
    "end": "12400"
  },
  {
    "text": "that we're going to use\nthroughout the course",
    "start": "12400",
    "end": "14410"
  },
  {
    "text": "in various sections, and\nalso something that we",
    "start": "14410",
    "end": "16368"
  },
  {
    "text": "use in our work all the time.",
    "start": "16368",
    "end": "18160"
  },
  {
    "text": "So it's really\nimportant to understand",
    "start": "18160",
    "end": "19990"
  },
  {
    "text": "k-fold cross-validation.",
    "start": "19990",
    "end": "21400"
  },
  {
    "text": "It's used for a lot\nof methods, it's",
    "start": "21400",
    "end": "24400"
  },
  {
    "text": "extremely a flexible and\npowerful, powerful technique",
    "start": "24400",
    "end": "27150"
  },
  {
    "text": "for estimating prediction\nerror and to get",
    "start": "27150",
    "end": "28900"
  },
  {
    "text": "an idea of model complexity.",
    "start": "28900",
    "end": "30670"
  },
  {
    "text": "So what's the idea of\nk-fold cross-validation.",
    "start": "30670",
    "end": "34160"
  },
  {
    "text": "Well, it's really in the name.",
    "start": "34160",
    "end": "35510"
  },
  {
    "text": "It's validation, as we've\nseen, but done a K part play.",
    "start": "35510",
    "end": "41300"
  },
  {
    "text": "It's done k times\nwith each part,",
    "start": "41300",
    "end": "44870"
  },
  {
    "text": "again, to play the role\nof the validation set,",
    "start": "44870",
    "end": "47770"
  },
  {
    "text": "and the other k minus 1 parts\nplaying the role of the training",
    "start": "47770",
    "end": "51400"
  },
  {
    "text": "set, which I say here.",
    "start": "51400",
    "end": "53360"
  },
  {
    "text": "But let me go to\nthe picture here,",
    "start": "53360",
    "end": "55940"
  },
  {
    "start": "55000",
    "end": "151000"
  },
  {
    "text": "and I'll point the\npicture as I say it.",
    "start": "55940",
    "end": "57960"
  },
  {
    "text": "So here we're doing\nfive-fold cross-validation.",
    "start": "57960",
    "end": "60710"
  },
  {
    "text": "As we'll talk about\nin more detail,",
    "start": "60710",
    "end": "62302"
  },
  {
    "text": "the best choices for k,\nin the number of folds",
    "start": "62303",
    "end": "64220"
  },
  {
    "text": "is usually about 5 or 10.",
    "start": "64220",
    "end": "66410"
  },
  {
    "text": "And I'll explain.",
    "start": "66410",
    "end": "68157"
  },
  {
    "text": "We'll talk about\nthat in a few minutes",
    "start": "68157",
    "end": "69740"
  },
  {
    "text": "about why those\nare good choices.",
    "start": "69740",
    "end": "71189"
  },
  {
    "text": "But let's fix here k equals 5.",
    "start": "71190",
    "end": "73498"
  },
  {
    "text": "So I've taken the data\nset, I've divided it",
    "start": "73498",
    "end": "75290"
  },
  {
    "text": "at random, the samples\ninto five parts,",
    "start": "75290",
    "end": "78020"
  },
  {
    "text": "again, of size about the same. s",
    "start": "78020",
    "end": "81409"
  },
  {
    "text": "The red box looks\na bit bigger, eh?",
    "start": "81410",
    "end": "83780"
  },
  {
    "text": "OK, well that's my lack\nof drawing ability.",
    "start": "83780",
    "end": "88177"
  },
  {
    "text": "But anyway, this is\nsupposed to be the same,",
    "start": "88177",
    "end": "90010"
  },
  {
    "text": "let's try to squish\nthe word validation in.",
    "start": "90010",
    "end": "91840"
  },
  {
    "text": "So the boxes are supposed\nto be about the same size",
    "start": "91840",
    "end": "93965"
  },
  {
    "text": "in the number of observations.",
    "start": "93965",
    "end": "95799"
  },
  {
    "text": "But in this case, the first\npart is the validation set,",
    "start": "95800",
    "end": "98710"
  },
  {
    "text": "the other four are\nthe training parts.",
    "start": "98710",
    "end": "100299"
  },
  {
    "text": "So what we're going to do,\nwhat cross validation does.",
    "start": "100300",
    "end": "103920"
  },
  {
    "text": "It forms these five parts.",
    "start": "103920",
    "end": "107350"
  },
  {
    "text": "We're going to train the model\non the four training parts,",
    "start": "107350",
    "end": "110010"
  },
  {
    "text": "put together as one big\nblock, pick the fitted model,",
    "start": "110010",
    "end": "113070"
  },
  {
    "text": "and then predict on\nthe validation part,",
    "start": "113070",
    "end": "116460"
  },
  {
    "text": "and record the error.",
    "start": "116460",
    "end": "117610"
  },
  {
    "text": "And then that's phase one.",
    "start": "117610",
    "end": "119650"
  },
  {
    "text": "Phase two, we're going\nto the validation set,",
    "start": "119650",
    "end": "121950"
  },
  {
    "text": "will be part two, this block.",
    "start": "121950",
    "end": "125070"
  },
  {
    "text": "All the other four parts\nwill be the training set.",
    "start": "125070",
    "end": "127650"
  },
  {
    "text": "We fit the model to the\ntraining set, and then apply it",
    "start": "127650",
    "end": "130259"
  },
  {
    "text": "to this validation part.",
    "start": "130259",
    "end": "131910"
  },
  {
    "text": "And in the third stage, this is\nthe validation piece, et cetera.",
    "start": "131910",
    "end": "134910"
  },
  {
    "text": "So we have five stages\nwhere in each stage,",
    "start": "134910",
    "end": "139080"
  },
  {
    "text": "one part gets to play the\nrole of validation set,",
    "start": "139080",
    "end": "141430"
  },
  {
    "text": "the other four parts\nare the training set.",
    "start": "141430",
    "end": "143819"
  },
  {
    "text": "We take all the prediction\nerrors from all five parts,",
    "start": "143820",
    "end": "146400"
  },
  {
    "text": "we add them together,\nand that gives",
    "start": "146400",
    "end": "147900"
  },
  {
    "text": "us what's called the\ncross-validation error.",
    "start": "147900",
    "end": "150939"
  },
  {
    "text": "So now in algebra,\nI'll basically",
    "start": "150940",
    "end": "154180"
  },
  {
    "start": "151000",
    "end": "247000"
  },
  {
    "text": "give you the details of\nwhat I said in words.",
    "start": "154180",
    "end": "156400"
  },
  {
    "text": "So we'll let the k\nparts be of the data,",
    "start": "156400",
    "end": "159939"
  },
  {
    "text": "be C1 through Ck, so these\nare the observations that",
    "start": "159940",
    "end": "163390"
  },
  {
    "text": "are in each of the\nfive parts, remember,",
    "start": "163390",
    "end": "165220"
  },
  {
    "text": "k was five in our\nexample, and we'll",
    "start": "165220",
    "end": "166912"
  },
  {
    "text": "try to make the number of\nobservations about the same",
    "start": "166912",
    "end": "169120"
  },
  {
    "text": "in every part.",
    "start": "169120",
    "end": "170000"
  },
  {
    "text": "Of course, if n is not\na multiple of K of five,",
    "start": "170000",
    "end": "172570"
  },
  {
    "text": "we can't do that exactly, but\nwe'll do it approximately.",
    "start": "172570",
    "end": "175020"
  },
  {
    "text": "So we'll let n sub k be\nthe number of observations",
    "start": "175020",
    "end": "178240"
  },
  {
    "text": "in the K part.",
    "start": "178240",
    "end": "180610"
  },
  {
    "text": "So here's the cross\nvalidation error rate.",
    "start": "180610",
    "end": "184160"
  },
  {
    "text": "Basically, this is\nthe mean square error",
    "start": "184160",
    "end": "186800"
  },
  {
    "text": "we get by applying\nthe fit to the.",
    "start": "186800",
    "end": "193980"
  },
  {
    "text": "the k minus 1 parts\nthat don't evolve",
    "start": "193980",
    "end": "196200"
  },
  {
    "text": "part number k, that gives us our\nfit, Yi hat for observation i.",
    "start": "196200",
    "end": "200855"
  },
  {
    "text": "That's 4/5 of the\ndata in this case.",
    "start": "200855",
    "end": "203019"
  },
  {
    "text": "And then we add up the error,\nthis is the mean square error",
    "start": "203020",
    "end": "206030"
  },
  {
    "text": "that we obtain now on\nthe validation part,",
    "start": "206030",
    "end": "207990"
  },
  {
    "text": "using that model.",
    "start": "207990",
    "end": "209270"
  },
  {
    "text": "So this is for the K, part.",
    "start": "209270",
    "end": "212100"
  },
  {
    "text": "And now we do this\nfor all five parts",
    "start": "212100",
    "end": "214190"
  },
  {
    "text": "in turn, the five\nacts of the play,",
    "start": "214190",
    "end": "216170"
  },
  {
    "text": "and we get the\ncross-validation error rate.",
    "start": "216170",
    "end": "218140"
  },
  {
    "start": "218140",
    "end": "222831"
  },
  {
    "text": "And a special case of this is\nleave-one-out cross-validation,",
    "start": "222831",
    "end": "225599"
  },
  {
    "text": "where the number of\nfolds is the same",
    "start": "225600",
    "end": "226830"
  },
  {
    "text": "as the number of observations.",
    "start": "226830",
    "end": "228080"
  },
  {
    "text": "So that means, in this\npicture there would actually",
    "start": "228080",
    "end": "231830"
  },
  {
    "text": "would be one box\nper observation,",
    "start": "231830",
    "end": "235580"
  },
  {
    "text": "and in leave-one-out\ncross-validation,",
    "start": "235580",
    "end": "237830"
  },
  {
    "text": "each observation by\nitself gets to play",
    "start": "237830",
    "end": "240140"
  },
  {
    "text": "the role of the validation\nset, the other n minus 1",
    "start": "240140",
    "end": "242630"
  },
  {
    "text": "are the training set.",
    "start": "242630",
    "end": "243505"
  },
  {
    "start": "243505",
    "end": "248090"
  },
  {
    "start": "247000",
    "end": "419000"
  },
  {
    "text": "Now actually, leaving\nout cross-validation",
    "start": "248090",
    "end": "250269"
  },
  {
    "text": "has a nice special case of that,\nit represents a nice special",
    "start": "250270",
    "end": "254740"
  },
  {
    "text": "in the sense that\nthis cross-validation",
    "start": "254740",
    "end": "256450"
  },
  {
    "text": "can be done without\nactually having",
    "start": "256450",
    "end": "257907"
  },
  {
    "text": "to refit the model at all.",
    "start": "257908",
    "end": "259930"
  },
  {
    "text": "So leaving out cross-validation,\nat least for least",
    "start": "259930",
    "end": "264690"
  },
  {
    "text": "squares model, or\na polynomial model,",
    "start": "264690",
    "end": "268810"
  },
  {
    "text": "if the leave-one-out\ncross-validation",
    "start": "268810",
    "end": "272639"
  },
  {
    "text": "has the following form.",
    "start": "272640",
    "end": "273940"
  },
  {
    "text": "so the Yi hat is now just\na fit on the full data set.",
    "start": "273940",
    "end": "277590"
  },
  {
    "text": "Hi is the diagonal\nof the hat matrix.",
    "start": "277590",
    "end": "283290"
  },
  {
    "text": "So have a look in\nthe book for details,",
    "start": "283290",
    "end": "285130"
  },
  {
    "text": "but the hat matrix is\nthe projection matrix",
    "start": "285130",
    "end": "288030"
  },
  {
    "text": "that projects y onto the column\nspace of x to give you the fit.",
    "start": "288030",
    "end": "291630"
  },
  {
    "text": "This is something that can\nget computed easily when you",
    "start": "291630",
    "end": "294060"
  },
  {
    "text": "fit your least squares model.",
    "start": "294060",
    "end": "295680"
  },
  {
    "text": "So go ahead, Trevor.",
    "start": "295680",
    "end": "296759"
  },
  {
    "text": "Yeah,",
    "start": "296760",
    "end": "297810"
  },
  {
    "text": "We haven't emphasized\nit, but it's available.",
    "start": "297810",
    "end": "301030"
  },
  {
    "text": "It's one of the things\nthat's available when you",
    "start": "301030",
    "end": "303030"
  },
  {
    "text": "fit your least squares model.",
    "start": "303030",
    "end": "304540"
  },
  {
    "text": "So the overall point\nof this is to do",
    "start": "304540",
    "end": "306218"
  },
  {
    "text": "a leave-one-out cross-validation\nfor these particular models,",
    "start": "306218",
    "end": "308759"
  },
  {
    "text": "you don't actually have\nto leave anything out,",
    "start": "308760",
    "end": "310150"
  },
  {
    "text": "you can do the fit on\nthe overall data set,",
    "start": "310150",
    "end": "312210"
  },
  {
    "text": "and then extract\nthe information you",
    "start": "312210",
    "end": "313710"
  },
  {
    "text": "need to get the\ncross-validation sum of squares.",
    "start": "313710",
    "end": "316600"
  },
  {
    "text": "And it's interesting,\nbecause the hi tells you",
    "start": "316600",
    "end": "318810"
  },
  {
    "text": "how much influence an\nobservation has on its own fit.",
    "start": "318810",
    "end": "322030"
  },
  {
    "text": "It's a number between 0 and 1.",
    "start": "322030",
    "end": "324340"
  },
  {
    "text": "And so if an observation is\nvery influential on its own fit,",
    "start": "324340",
    "end": "328210"
  },
  {
    "text": "you can see it punishes\nthe residual, because it",
    "start": "328210",
    "end": "330520"
  },
  {
    "text": "divides by a number\nthat's small,",
    "start": "330520",
    "end": "333099"
  },
  {
    "text": "and it inflates the residual.",
    "start": "333100",
    "end": "334700"
  },
  {
    "text": "So it does the right thing.",
    "start": "334700",
    "end": "336290"
  },
  {
    "start": "336290",
    "end": "339540"
  },
  {
    "text": "But leave-one-out\ncross-validation",
    "start": "339540",
    "end": "342570"
  },
  {
    "text": "does have this nice\ncomputational formula.",
    "start": "342570",
    "end": "344430"
  },
  {
    "text": "For most of the methods we\ntalk about in this book,",
    "start": "344430",
    "end": "346710"
  },
  {
    "text": "and most statistical\nlearning methods,",
    "start": "346710",
    "end": "348389"
  },
  {
    "text": "it's better to choose\nk to be 5 or 10,",
    "start": "348390",
    "end": "350640"
  },
  {
    "text": "rather than leave-one-out\ncross-validation.",
    "start": "350640",
    "end": "354450"
  },
  {
    "text": "And why is that?",
    "start": "354450",
    "end": "355240"
  },
  {
    "text": "Well, one problem with\nleave-one-out cross-validation",
    "start": "355240",
    "end": "359405"
  },
  {
    "text": "is that each of\nthe training sets",
    "start": "359405",
    "end": "360780"
  },
  {
    "text": "look very much like\nthe other ones,",
    "start": "360780",
    "end": "362238"
  },
  {
    "text": "they only differ\nby one observation.",
    "start": "362238",
    "end": "364590"
  },
  {
    "text": "So when you take\ncross-validation, is",
    "start": "364590",
    "end": "368400"
  },
  {
    "text": "you take the average of\nerrors over the n folds,",
    "start": "368400",
    "end": "372210"
  },
  {
    "text": "and in leave-one-out\ncross-validation,",
    "start": "372210",
    "end": "374457"
  },
  {
    "text": "the n folds look very\nsimilar to each other,",
    "start": "374457",
    "end": "376290"
  },
  {
    "text": "because the training\nsets are almost the same,",
    "start": "376290",
    "end": "378207"
  },
  {
    "text": "they're only differ\nby one observation.",
    "start": "378207",
    "end": "379930"
  },
  {
    "text": "So as a result that average\nhas a high variance,",
    "start": "379930",
    "end": "382259"
  },
  {
    "text": "because the ingredients\nare highly correlated.",
    "start": "382260",
    "end": "384450"
  },
  {
    "text": "So that's the main\nreason why it's thought,",
    "start": "384450",
    "end": "388720"
  },
  {
    "text": "and we also we agree,\nthat a better choice for k",
    "start": "388720",
    "end": "391710"
  },
  {
    "text": "in cross-validation is 5 or 10.",
    "start": "391710",
    "end": "393930"
  },
  {
    "text": "On the other hand, the\nleave-one-out cross-validation",
    "start": "393930",
    "end": "397690"
  },
  {
    "text": "is actually trying to\nestimate the error rate",
    "start": "397690",
    "end": "400390"
  },
  {
    "text": "for the training sample of\nalmost the same size as what",
    "start": "400390",
    "end": "403090"
  },
  {
    "text": "you have.",
    "start": "403090",
    "end": "403990"
  },
  {
    "text": "So it's got low bias, but\nas Rob said, high variance.",
    "start": "403990",
    "end": "407690"
  },
  {
    "text": "So actually picking k is also\na bias variance trade-off",
    "start": "407690",
    "end": "411970"
  },
  {
    "text": "for prediction area,\nand as Rob said,",
    "start": "411970",
    "end": "414730"
  },
  {
    "text": "k equals 5 or 10 tend\nto be a good choice.",
    "start": "414730",
    "end": "418298"
  },
  {
    "text": "So in the next slide, we've got\na comparison of leave-one-out",
    "start": "418298",
    "end": "420840"
  },
  {
    "start": "419000",
    "end": "496000"
  },
  {
    "text": "cross-validation in 10-fold\nCV for the auto data.",
    "start": "420840",
    "end": "425590"
  },
  {
    "text": "Remember, before we start with\nthis validation into two parts,",
    "start": "425590",
    "end": "430800"
  },
  {
    "text": "we got a lot of\nvariability between when",
    "start": "430800",
    "end": "434400"
  },
  {
    "text": "we change the sample the\nhalf sample that we took.",
    "start": "434400",
    "end": "438160"
  },
  {
    "text": "Now, let's see what happens.",
    "start": "438160",
    "end": "439330"
  },
  {
    "text": "With leave-one-out\ncross-validation,",
    "start": "439330",
    "end": "440830"
  },
  {
    "text": "we get a curve that's,\nagain, got the minimum",
    "start": "440830",
    "end": "443099"
  },
  {
    "text": "around the same place,\nas we saw before,",
    "start": "443100",
    "end": "445230"
  },
  {
    "text": "and it's pretty flat after that.",
    "start": "445230",
    "end": "447710"
  },
  {
    "text": "A 10-fold cross\nvalidation, now, again,",
    "start": "447710",
    "end": "449850"
  },
  {
    "text": "it's also showing the\nminimum around two,",
    "start": "449850",
    "end": "451960"
  },
  {
    "text": "but what we're seeing here is\nthe 10-fold cross validation",
    "start": "451960",
    "end": "457259"
  },
  {
    "text": "as we take different partitions\ninto 10 parts of the data.",
    "start": "457260",
    "end": "463810"
  },
  {
    "text": "And we see there's\nnot much variability,",
    "start": "463810",
    "end": "465480"
  },
  {
    "text": "they're pretty consistent.",
    "start": "465480",
    "end": "466930"
  },
  {
    "text": "In contrast to the\ndivide-into-two-parts,",
    "start": "466930",
    "end": "470289"
  },
  {
    "text": "we got much more variability.",
    "start": "470290",
    "end": "473680"
  },
  {
    "text": "And those get averaged as well,\nthose curves on the right.",
    "start": "473680",
    "end": "476830"
  },
  {
    "text": "Right, which we\nsaw here, they're",
    "start": "476830",
    "end": "481258"
  },
  {
    "text": "averaged together to give\nus the overall estimate",
    "start": "481258",
    "end": "483300"
  },
  {
    "text": "of cross-validation, which the\noverall cross-validation curve",
    "start": "483300",
    "end": "487979"
  },
  {
    "text": "will look very much like this,\nwith its minimum around two.",
    "start": "487980",
    "end": "493145"
  },
  {
    "start": "493145",
    "end": "500259"
  },
  {
    "start": "496000",
    "end": "652000"
  },
  {
    "text": "This is figure 5.6\nfrom the textbook,",
    "start": "500260",
    "end": "502480"
  },
  {
    "text": "and this is the\nsimulated data example,",
    "start": "502480",
    "end": "505190"
  },
  {
    "text": "which was figured from\nfigure 2.9 of the book.",
    "start": "505190",
    "end": "508690"
  },
  {
    "text": "Just recall that this\nis smoothing splines",
    "start": "508690",
    "end": "511120"
  },
  {
    "text": "in three different situations.",
    "start": "511120",
    "end": "512929"
  },
  {
    "text": "In this case, the two-error\ncurve is the blue curve.",
    "start": "512929",
    "end": "517012"
  },
  {
    "text": "And again, these are\nthree different functions",
    "start": "517012",
    "end": "518929"
  },
  {
    "text": "that we're examining.",
    "start": "518929",
    "end": "520880"
  },
  {
    "text": "This is mean square\nerror for simulated data.",
    "start": "520880",
    "end": "523110"
  },
  {
    "text": "The true error curve,\nhow did we get that, Rob?",
    "start": "523110",
    "end": "525380"
  },
  {
    "text": "Well, simulated data.",
    "start": "525380",
    "end": "528590"
  },
  {
    "text": "So we can get a very\nbig test set exactly",
    "start": "528590",
    "end": "531920"
  },
  {
    "text": "and estimate the error exactly.",
    "start": "531920",
    "end": "534260"
  },
  {
    "text": "Leave-one-out cross-validation\nis the black broken line,",
    "start": "534260",
    "end": "539510"
  },
  {
    "text": "and the orange curve is\n10-fold cross validation.",
    "start": "539510",
    "end": "543080"
  },
  {
    "text": "So what do we see?",
    "start": "543080",
    "end": "544500"
  },
  {
    "text": "Well, here we see that test\nerror curve is a little higher",
    "start": "544500",
    "end": "548420"
  },
  {
    "text": "than the 10-fold and\nleave-one-out cross validation.",
    "start": "548420",
    "end": "553130"
  },
  {
    "text": "The minimums are fairly\nclose, but the minimum",
    "start": "553130",
    "end": "557510"
  },
  {
    "text": "of cross-validation is around\n8, whereas the true curve",
    "start": "557510",
    "end": "560990"
  },
  {
    "text": "is minimized around 6.",
    "start": "560990",
    "end": "562790"
  },
  {
    "text": "In this case, the two\ncross-validation methods",
    "start": "562790",
    "end": "565519"
  },
  {
    "text": "are doing a better job of\napproximating the test error",
    "start": "565520",
    "end": "568040"
  },
  {
    "text": "curve and have the minimum.",
    "start": "568040",
    "end": "569570"
  },
  {
    "text": "Well, the minimum's\nagain, fairly close,",
    "start": "569570",
    "end": "571580"
  },
  {
    "text": "not exactly on the mark.",
    "start": "571580",
    "end": "573960"
  },
  {
    "text": "Blockers minimized around\n6, and the true error curve",
    "start": "573960",
    "end": "577790"
  },
  {
    "text": "is minimized around maybe 3.",
    "start": "577790",
    "end": "580440"
  },
  {
    "text": "Although those error\ncurves are fairly flat,",
    "start": "580440",
    "end": "583280"
  },
  {
    "text": "so there's obviously a\nhigh variance in where",
    "start": "583280",
    "end": "586220"
  },
  {
    "text": "the minimum should be, it\ndoesn't really matter where.",
    "start": "586220",
    "end": "589918"
  },
  {
    "text": "That's right. it's not\ngoing to matter much",
    "start": "589918",
    "end": "591710"
  },
  {
    "text": "if you choose a model\nwith flexibility 2",
    "start": "591710",
    "end": "594088"
  },
  {
    "text": "or maybe even 10 here,\nbecause the error is",
    "start": "594088",
    "end": "595880"
  },
  {
    "text": "pretty flat in that region.",
    "start": "595880",
    "end": "597440"
  },
  {
    "text": "And then the third example,\nthe two cross-validation curves",
    "start": "597440",
    "end": "600470"
  },
  {
    "text": "do quite a good job of\napproximating the test error",
    "start": "600470",
    "end": "602810"
  },
  {
    "text": "curve than the minimums around\n10 o'clock in each case.",
    "start": "602810",
    "end": "605650"
  },
  {
    "start": "605650",
    "end": "611990"
  },
  {
    "text": "So actually I said this\nalready, but I'll say it again,",
    "start": "611990",
    "end": "614339"
  },
  {
    "text": "that one issue with\ncross-validation",
    "start": "614340",
    "end": "616658"
  },
  {
    "text": "is that since the\ntraining set is not",
    "start": "616658",
    "end": "618200"
  },
  {
    "text": "as big as the\noriginal training set,",
    "start": "618200",
    "end": "620030"
  },
  {
    "text": "the essence of prediction\nerror will be biased up",
    "start": "620030",
    "end": "623510"
  },
  {
    "text": "a little bit, because we have\nless data we're working with,",
    "start": "623510",
    "end": "626210"
  },
  {
    "text": "working with.",
    "start": "626210",
    "end": "626755"
  },
  {
    "start": "626755",
    "end": "629390"
  },
  {
    "text": "And I also said,\nand I'll say again,",
    "start": "629390",
    "end": "631730"
  },
  {
    "text": "that leave-one-out\ncross-validation has",
    "start": "631730",
    "end": "635510"
  },
  {
    "text": "smaller bias in this\nsense, because the training",
    "start": "635510",
    "end": "637610"
  },
  {
    "text": "set is almost the same\nsize as the original set.",
    "start": "637610",
    "end": "639690"
  },
  {
    "text": "But on the other hand,\nit's got higher variance,",
    "start": "639690",
    "end": "641120"
  },
  {
    "text": "because the training\nsets that it's",
    "start": "641120",
    "end": "642578"
  },
  {
    "text": "using are almost the\nsame as the original set,",
    "start": "642578",
    "end": "645120"
  },
  {
    "text": "the only difference\nby one observation.",
    "start": "645120",
    "end": "646940"
  },
  {
    "text": "So k equals 5 or 10-fold is a\ngood compromise for this bias",
    "start": "646940",
    "end": "650930"
  },
  {
    "text": "variance trade-off.",
    "start": "650930",
    "end": "652740"
  },
  {
    "start": "652000",
    "end": "814000"
  },
  {
    "text": "So we talked about\ncross-validation",
    "start": "652740",
    "end": "655620"
  },
  {
    "text": "for quantitative response,\nand we used mean square error.",
    "start": "655620",
    "end": "658788"
  },
  {
    "text": "For classification\nproblems, the idea",
    "start": "658788",
    "end": "660330"
  },
  {
    "text": "is exactly the same, the\nonly thing that changes",
    "start": "660330",
    "end": "662330"
  },
  {
    "text": "is a measure of error.",
    "start": "662330",
    "end": "664500"
  },
  {
    "text": "Of course, no longer use square\nerror, but a misclassification",
    "start": "664500",
    "end": "667230"
  },
  {
    "text": "error.",
    "start": "667230",
    "end": "667750"
  },
  {
    "text": "Otherwise, cross validation\nprocess is exactly the same.",
    "start": "667750",
    "end": "670710"
  },
  {
    "text": "Divide the data up into k parts,\nwe train on k minus 1 parts,",
    "start": "670710",
    "end": "675100"
  },
  {
    "text": "we record the error\non the k part,",
    "start": "675100",
    "end": "677399"
  },
  {
    "text": "and we add things up together to\nget the overall cross validation",
    "start": "677400",
    "end": "680180"
  },
  {
    "text": "error.",
    "start": "680180",
    "end": "680680"
  },
  {
    "text": "It looks like a weighted\naverage in that formula,",
    "start": "680680",
    "end": "684180"
  },
  {
    "text": "there's nk over n.",
    "start": "684180",
    "end": "685560"
  },
  {
    "text": "Well, do you want\nto explain that?",
    "start": "685560",
    "end": "687210"
  },
  {
    "text": "Because each of the faults\nmight not be exactly",
    "start": "687210",
    "end": "690330"
  },
  {
    "text": "the same size, so we actually\ncompute a weight, which",
    "start": "690330",
    "end": "694590"
  },
  {
    "text": "is which is the relative\nsize of the fold,",
    "start": "694590",
    "end": "698790"
  },
  {
    "text": "and then use a weighted average.",
    "start": "698790",
    "end": "700750"
  },
  {
    "text": "And if we are lucky that the\nend divides by k exactly,",
    "start": "700750",
    "end": "705400"
  },
  {
    "text": "then that weight will\njust be 1 over k.",
    "start": "705400",
    "end": "709798"
  },
  {
    "text": "One other thing to add, which is\nthat since the cross-validation",
    "start": "709798",
    "end": "713589"
  },
  {
    "text": "error is just an average, the\nstandard error of that average",
    "start": "713590",
    "end": "716770"
  },
  {
    "text": "also gives us a standard\nerror of the cross validation",
    "start": "716770",
    "end": "719400"
  },
  {
    "text": "estimate.",
    "start": "719400",
    "end": "719900"
  },
  {
    "text": "So we take the error rates\nfrom each of the folds,",
    "start": "719900",
    "end": "722060"
  },
  {
    "text": "their average is the cross\nvalidation error rate.",
    "start": "722060",
    "end": "724810"
  },
  {
    "text": "Their standard error is\nthe standard deviation",
    "start": "724810",
    "end": "728440"
  },
  {
    "text": "of the cross\nvalidation estimate.",
    "start": "728440",
    "end": "730010"
  },
  {
    "text": "So here's the formula for that.",
    "start": "730010",
    "end": "731480"
  },
  {
    "text": "So this is a useful quantity.",
    "start": "731480",
    "end": "734029"
  },
  {
    "text": "And when we draw CV\ncurves, we should always",
    "start": "734030",
    "end": "736030"
  },
  {
    "text": "put a standard error\nband around the curve",
    "start": "736030",
    "end": "738190"
  },
  {
    "text": "to get any of the variability.",
    "start": "738190",
    "end": "739580"
  },
  {
    "text": "So in these previous\npictures, we",
    "start": "739580",
    "end": "741280"
  },
  {
    "text": "should have had a\nstandard error band",
    "start": "741280",
    "end": "742870"
  },
  {
    "text": "around the curves to give us an\nidea of how variable they are.",
    "start": "742870",
    "end": "748240"
  },
  {
    "text": "I see here it's used\nless, but not quite valid.",
    "start": "748240",
    "end": "752910"
  },
  {
    "text": "Why is that?",
    "start": "752910",
    "end": "754290"
  },
  {
    "text": "Dr. Hastie?",
    "start": "754290",
    "end": "755880"
  },
  {
    "text": "Well, I wonder why.",
    "start": "755880",
    "end": "757420"
  },
  {
    "text": "Well, the thing is, we\ncompute in a standard error",
    "start": "757420",
    "end": "760529"
  },
  {
    "text": "as if these were\nindependent observations,",
    "start": "760530",
    "end": "762600"
  },
  {
    "text": "but they're not\nstrictly independent.",
    "start": "762600",
    "end": "764829"
  },
  {
    "text": "Error sub k overlaps\nwith error sub",
    "start": "764830",
    "end": "769050"
  },
  {
    "text": "j because they share\nsome training samples.",
    "start": "769050",
    "end": "773070"
  },
  {
    "text": "So there's some\ncorrelation between them.",
    "start": "773070",
    "end": "775000"
  },
  {
    "text": "But we use this anyway.",
    "start": "775000",
    "end": "776380"
  },
  {
    "text": "We use it, and it's actually\nquite a good estimate.",
    "start": "776380",
    "end": "778505"
  },
  {
    "text": "And people have shown\nthis mathematically.",
    "start": "778505",
    "end": "780588"
  },
  {
    "text": "Again, the important\npoint being that",
    "start": "780588",
    "end": "782130"
  },
  {
    "text": "is that the\ncross-validation separates",
    "start": "782130",
    "end": "784560"
  },
  {
    "text": "the training part of the data\nfrom the validation part.",
    "start": "784560",
    "end": "787450"
  },
  {
    "text": "When we talk about\nthe bootstrap method",
    "start": "787450",
    "end": "789190"
  },
  {
    "text": "in the next part\nof this section,",
    "start": "789190",
    "end": "791382"
  },
  {
    "text": "we'll see that\nthat's not the case,",
    "start": "791382",
    "end": "792840"
  },
  {
    "text": "and that's going\nto cause a problem.",
    "start": "792840",
    "end": "794340"
  },
  {
    "text": "So cross-validation explicitly\nseparates the training",
    "start": "794340",
    "end": "796620"
  },
  {
    "text": "set from the validation\nset in order to get",
    "start": "796620",
    "end": "798900"
  },
  {
    "text": "a good idea of test error.",
    "start": "798900",
    "end": "801380"
  },
  {
    "text": "So this again, I reemphasize\nthat cross-validation",
    "start": "801380",
    "end": "805850"
  },
  {
    "text": "is a very important\ntechnique to understand,",
    "start": "805850",
    "end": "808579"
  },
  {
    "text": "both for, for quantitative\nresponse and classification.",
    "start": "808580",
    "end": "814000"
  }
]