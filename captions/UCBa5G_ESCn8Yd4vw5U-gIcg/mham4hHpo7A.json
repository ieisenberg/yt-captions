[
  {
    "start": "0",
    "end": "105000"
  },
  {
    "start": "0",
    "end": "5600"
  },
  {
    "text": "OK. Hi, everyone. Yeah, let's get started. So I guess today, we're\ngoing to talk about--",
    "start": "5600",
    "end": "13610"
  },
  {
    "text": "continue to talk about the\nimplicit regularization. So last time we\nhave talked about",
    "start": "13610",
    "end": "22140"
  },
  {
    "text": "the implicit regularization\nof initialization, and today--",
    "start": "22140",
    "end": "32860"
  },
  {
    "text": "this is last lecture. Actually, last week we--",
    "start": "32860",
    "end": "38165"
  },
  {
    "text": "in the last two\nlectures, we have talked about implicit\nregularization of initialization, and today\nwe're going to have two parts.",
    "start": "38165",
    "end": "45280"
  },
  {
    "text": "So one part is we continue with\nthe implicit regularization,",
    "start": "45280",
    "end": "53590"
  },
  {
    "text": "and this is a better\ncharacterization in certain cases, as\nI will describe more.",
    "start": "53590",
    "end": "58690"
  },
  {
    "text": "And another is that\nwe're going to talk about classification problem.",
    "start": "58690",
    "end": "65820"
  },
  {
    "text": "In all the past few examples, we\nare talking about a regression problem. And it turns out that for\nclassification problem,",
    "start": "65820",
    "end": "71652"
  },
  {
    "text": "the behavior is a\nlittle bit different. ",
    "start": "71652",
    "end": "77390"
  },
  {
    "text": "And instead of converging to\nsome minimum norm solution, you converge to max\nmargin solution, which is, in some sense,\nsimilar, but not",
    "start": "77390",
    "end": "84450"
  },
  {
    "text": "exactly the same to\nthe regression case. OK? So with this\nlecture, we're going",
    "start": "84450",
    "end": "90210"
  },
  {
    "text": "to conclude the discussion about\nthe implicit regularization of initialization,\nand then next lecture",
    "start": "90210",
    "end": "97090"
  },
  {
    "text": "we're going to talk\nabout the stochasticity. And that will be\nthe last lecture about the implicit\nregularization.",
    "start": "97090",
    "end": "102750"
  },
  {
    "text": " So today we're going to\nhave-- so the first part--",
    "start": "102750",
    "end": "108960"
  },
  {
    "start": "105000",
    "end": "225000"
  },
  {
    "text": "this is number one, number two. So the first part,\nwe're going to talk about a more precise\ncharacterization,",
    "start": "108960",
    "end": "119534"
  },
  {
    "text": "certain characteristics about\nthe implicit regularization effect of initialization. You can see exactly how the\ninitialization influenced",
    "start": "119535",
    "end": "126350"
  },
  {
    "text": "the regularizer. And to have some preparation,\nfor today's lecture",
    "start": "126350",
    "end": "134730"
  },
  {
    "text": "we're going to talk about\nthe so-called gradient flow. I was trying to avoid\nthis notion in the past,",
    "start": "134730",
    "end": "141830"
  },
  {
    "text": "but I think the spirit has\nshown up in the past as well.",
    "start": "141830",
    "end": "147180"
  },
  {
    "text": "So basically this is gradient\ndescent with infinitesimal-- gradient descent with\ninfinitesimal learning rate.",
    "start": "147180",
    "end": "160240"
  },
  {
    "text": " And the reason\nwhy this is useful is because in certain\ncases we have infinitesimal",
    "start": "160240",
    "end": "169690"
  },
  {
    "text": "where you can ignore some\nof the second order effect from the learning rate. And so this is just\na mixed analysis.",
    "start": "169690",
    "end": "177034"
  },
  {
    "text": "It's much simpler. You don't have to say\nhow small the number is. You don't have to deal with\nthe second order effect",
    "start": "177035",
    "end": "182050"
  },
  {
    "text": "because the second order\neffect is just literally zero. There's no second order effect. And the way to do this is\nthat it's actually also",
    "start": "182050",
    "end": "189390"
  },
  {
    "text": "kind of a pretty clean\nformulation of optimization, even though it's\ncontinuous time.",
    "start": "189390",
    "end": "196600"
  },
  {
    "text": "So what you do is you say\nyou have a loss function. Let's say Lw is\nthe loss function.",
    "start": "196600",
    "end": "202989"
  },
  {
    "text": "So if you do gradient\ndescent, then what you do is you say you take wt plus 1.",
    "start": "202990",
    "end": "208560"
  },
  {
    "text": "And now I'm using the\nparentheses for time because I'm going to use\nthat for the continuous time. So wt plus 1 is equal\nto wt minus eta times",
    "start": "208560",
    "end": "217080"
  },
  {
    "text": "the gradient of\nthe loss at time t. This is what you will do\nwith gradient descent.",
    "start": "217080",
    "end": "224019"
  },
  {
    "text": "And if you scale the time by\neta, what I mean is that now,",
    "start": "224020",
    "end": "234460"
  },
  {
    "start": "225000",
    "end": "335000"
  },
  {
    "text": "currently, we will do\ngradient descent every time you increase the\nstep counter by 1. So before the time is t, and\nnow the time is t plus 1, right?",
    "start": "234460",
    "end": "243010"
  },
  {
    "text": "And now suppose\nyou don't do that. You change the time scale. You say, every update I only\nadvance the time counter",
    "start": "243010",
    "end": "251290"
  },
  {
    "text": "by eta instead of by 1. So what I'm going to get is\nthat I get w of t plus eta",
    "start": "251290",
    "end": "257560"
  },
  {
    "text": "is equal to wt minus\neta nabla L wt.",
    "start": "257560",
    "end": "262780"
  },
  {
    "text": "And these two process,\neffective, are the same. It's just that the\ntime, the unit of time,",
    "start": "262780",
    "end": "267910"
  },
  {
    "text": "changed by a factor\nof eta or 1 over eta. And now if you scale\nthe time, then you",
    "start": "267910",
    "end": "274150"
  },
  {
    "text": "can take eta to go to 0. So take eta to go\nto 0, and then this",
    "start": "274150",
    "end": "279160"
  },
  {
    "text": "becomes a differential\nequation or kind of like a continuous process.",
    "start": "279160",
    "end": "284570"
  },
  {
    "text": "So you can write\nthis as wt plus dt is equal to wt minus\neta gradient L wt.",
    "start": "284570",
    "end": "293630"
  },
  {
    "text": "I guess depending on\nwhat kind of community you come from, you can also\nread this wt dot, right?",
    "start": "293630",
    "end": "300950"
  },
  {
    "text": "This is a derivative,\nwith respect to-- I guess or I think here\nyou also replace eta by dt.",
    "start": "300950",
    "end": "306470"
  },
  {
    "text": "This is how you take\nthe eta to go to 0. And this is effective\nsaying that the gradient--",
    "start": "306470",
    "end": "313470"
  },
  {
    "text": "so that the derivative\nof w respect to t, which we denote by w dot\nt is equal to minus gradient L",
    "start": "313470",
    "end": "323210"
  },
  {
    "text": "at w at time t. Where w dot t, this\nis just a derivative",
    "start": "323210",
    "end": "330750"
  },
  {
    "text": "of w with respect to the time t. ",
    "start": "330750",
    "end": "336310"
  },
  {
    "start": "335000",
    "end": "610000"
  },
  {
    "text": "And in some sense,\nthis allows us to ignore the eta squared\nterm because eta squared here",
    "start": "336310",
    "end": "346340"
  },
  {
    "text": "becomes dt squared, which\nis 0 compared to dt. So that's why this is useful.",
    "start": "346340",
    "end": "354110"
  },
  {
    "text": "This will be useful for us. In some sense, this is mostly\nto simplify the equation.",
    "start": "354110",
    "end": "361310"
  },
  {
    "text": "All the technical meat, in\nsome sense, are the same. It just makes the\nanalysis cleaner.",
    "start": "361310",
    "end": "369560"
  },
  {
    "text": "And for the next two examples,\nin both of the examples, I'm going to use this\ngradient flow formulation",
    "start": "369560",
    "end": "375960"
  },
  {
    "text": "for gradient descent. OK? So now let's talk about the\nmodel we're going to discuss.",
    "start": "375960",
    "end": "385210"
  },
  {
    "text": "So the model is a variant\nof the last lecture,",
    "start": "385210",
    "end": "390699"
  },
  {
    "text": "and there are some\nreasons for changing the model a little bit. I'm going to discuss that,\nbut it's not super important.",
    "start": "390700",
    "end": "396700"
  },
  {
    "text": "So the model we're\ngoing to do is that you have some quadratically\nparameterized linear model",
    "start": "396700",
    "end": "405590"
  },
  {
    "text": "in some sense. So we have two parts. Let me write it down. So you have some\nvector w plus, and you",
    "start": "405590",
    "end": "412280"
  },
  {
    "text": "take this o dot 2 minus w\nminus o dot 2, transpose x",
    "start": "412280",
    "end": "417320"
  },
  {
    "text": "where we use this notation. xo dot 2 just means xo dot x.",
    "start": "417320",
    "end": "422910"
  },
  {
    "text": "And the dot, this dot o,\nis this entry wise product.",
    "start": "422910",
    "end": "432840"
  },
  {
    "text": "So w squared minus w\nminus squared transpose x. And w and w minus, these\nare both vectors in rd.",
    "start": "432840",
    "end": "443520"
  },
  {
    "text": "And you can write w as the\nconcatenation of w plus and w minus as the parameter.",
    "start": "443520",
    "end": "449521"
  },
  {
    "start": "449521",
    "end": "454810"
  },
  {
    "text": "And so basically, this\nis very similar to what we did last time. So last time we had\nsomething like f beta",
    "start": "454810",
    "end": "463220"
  },
  {
    "text": "of x, which is beta over\ndot beta transpose, right? ",
    "start": "463220",
    "end": "471270"
  },
  {
    "text": "So basically, now you\nhave a negative term in it instead of just positive. And the reason is\nthere are two reasons.",
    "start": "471270",
    "end": "477950"
  },
  {
    "text": "So there are two benefits\ncompared to last time. So they are not super important,\nbut I know why you mention it.",
    "start": "477950",
    "end": "484650"
  },
  {
    "text": "The one thing is\nthat it's at beta, so this model last\ntime can only represent",
    "start": "484650",
    "end": "495840"
  },
  {
    "text": "positive linear\ncombination of x.",
    "start": "495840",
    "end": "506030"
  },
  {
    "text": "And now this fwx can\nrepresent any linear model.",
    "start": "506030",
    "end": "512000"
  },
  {
    "start": "512000",
    "end": "518490"
  },
  {
    "text": "Right? Because before if you take\nthe entry wise product of w and beta and beta, it's always\npositive and non-negative",
    "start": "518490",
    "end": "525260"
  },
  {
    "text": "so you can only have a\nnon-negative linear combination of the coordinates\nof x, and now it",
    "start": "525260",
    "end": "530720"
  },
  {
    "text": "can have negative coordinates. And another benefit is that--",
    "start": "530720",
    "end": "538430"
  },
  {
    "text": "now if you initialize-- if you initialize\nw plus 0, I times",
    "start": "538430",
    "end": "548430"
  },
  {
    "text": "0 to be equal to w minus at\ntime 0 to be the same thing. Then, as you can see,\nfw0x is equal to 0.",
    "start": "548430",
    "end": "558360"
  },
  {
    "text": "And I guess for every x, it's\njust because the positive part cancel with the negative part.",
    "start": "558360",
    "end": "564360"
  },
  {
    "text": "And I guess you have seen\nthese kind of things before. This is mostly for convenience.",
    "start": "564360",
    "end": "569829"
  },
  {
    "text": "So this will make the\nanalysis even more convenient because initialization has zero\nfunctionality, and we have--",
    "start": "569830",
    "end": "578079"
  },
  {
    "text": "kind of use this for\nthe NTK, and this will",
    "start": "578080",
    "end": "584260"
  },
  {
    "text": "be useful for our analysis. And actually, what\nwe are going to do today is that with\nthis thing, we're going to see that if you\nchange initialization,",
    "start": "584260",
    "end": "591160"
  },
  {
    "text": "actually you're going to\nget different regularization and you can precisely\ncharacterize, how does the regularization\ndepends on initialization?",
    "start": "591160",
    "end": "597760"
  },
  {
    "text": "And in one of the cases,\nit will be the NTK. In the other case, it\nwill be similar to what we discussed in the last time.",
    "start": "597760",
    "end": "603070"
  },
  {
    "text": "So yeah, maybe I\nshould mention that. ",
    "start": "603070",
    "end": "615949"
  },
  {
    "text": "And continue with the setup. So the loss function,\nthe L times w",
    "start": "615950",
    "end": "620990"
  },
  {
    "text": "will be just the square loss. ",
    "start": "620990",
    "end": "630060"
  },
  {
    "text": "And we can consider\ninitialization, as we discussed.",
    "start": "630060",
    "end": "637556"
  },
  {
    "text": "We'll consider initialization\nw plus 0 is equal to w minus 0, so that's-- you get the 0 initialization\nas the functionality",
    "start": "637556",
    "end": "646329"
  },
  {
    "text": "of the initial model as 0. And also for\nsimplicity, we choose this to be alpha\ntimes L1 vector,",
    "start": "646330",
    "end": "652689"
  },
  {
    "text": "and alpha is the thing\nwe're going to change. We're going to see that how\ndoes the regularization,",
    "start": "652690",
    "end": "659350"
  },
  {
    "text": "implicit regularization effect,\ndepends on the scale of alpha. So when alpha is small,\nit gives you something.",
    "start": "659350",
    "end": "666550"
  },
  {
    "text": "If alpha is big, it\ngives you something else. And this L1 vector\nis chosen as before.",
    "start": "666550",
    "end": "674080"
  },
  {
    "text": "In the previous lecture,\nit's chosen somewhat for convenience. You can still do it with\nother initialization,",
    "start": "674080",
    "end": "680709"
  },
  {
    "text": "it's just that the form will be\na little bit more complicated. And you can also have this--",
    "start": "680710",
    "end": "688600"
  },
  {
    "text": "we can also have\nthis theta space. So suppose theta\ncorresponds to w plus.",
    "start": "688600",
    "end": "696819"
  },
  {
    "text": "So let's say that w is defined\nto be w plus the power of 2",
    "start": "696820",
    "end": "703570"
  },
  {
    "text": "minus w minus to a power of 2. This is the actual linear\nfunction you compute,",
    "start": "703570",
    "end": "710110"
  },
  {
    "text": "the linear function of w, right? And we can define--",
    "start": "710110",
    "end": "715430"
  },
  {
    "text": "and we are interested in\nwhat kind of linear model we are learning in eventually. So let's say w infinity.",
    "start": "715430",
    "end": "724220"
  },
  {
    "text": "Let's define this to be\nthe limit in time infinity. So this is how the\ngradient flow--",
    "start": "724220",
    "end": "730910"
  },
  {
    "text": "where the gradient\nflow converges to. And let theta sub\nalpha infinity,",
    "start": "730910",
    "end": "740790"
  },
  {
    "text": "and I think sometimes\nwe just call this-- this is basically w, the model\ncorresponding to w infinity.",
    "start": "740790",
    "end": "753600"
  },
  {
    "text": "So this is the coefficient\nthat we care about. We care about when you\nconverge to infinity,",
    "start": "753600",
    "end": "759630"
  },
  {
    "text": "what's the corresponding\ntheta you get? So what's the property\nof this theta?",
    "start": "759630",
    "end": "766230"
  },
  {
    "text": "And sometimes we just call\nit, for simplicity, we",
    "start": "766230",
    "end": "771930"
  },
  {
    "text": "just write theta alpha\nand omit the infinity.",
    "start": "771930",
    "end": "780760"
  },
  {
    "text": "But this is where\nit converges to. OK? And for the sake of the\nsimplicity of the lecture,",
    "start": "780760",
    "end": "787820"
  },
  {
    "text": "we assume everything kind of\nhave a limit, so and so forth. So all the regularity conditions\nare assumed to be met.",
    "start": "787820",
    "end": "794728"
  },
  {
    "text": "OK. And also, just to\nset up some notation, let this x to be\nthe data matrix.",
    "start": "794728",
    "end": "803280"
  },
  {
    "start": "800000",
    "end": "920000"
  },
  {
    "text": " And this is the matrix\nin 2D, and let's say",
    "start": "803280",
    "end": "810449"
  },
  {
    "text": "y vec is the label matrix-- label vector. OK. ",
    "start": "810450",
    "end": "816700"
  },
  {
    "text": "So now, here is the\ntheorem that characterized how does it characterize\nthe implicit regularization.",
    "start": "816700",
    "end": "824110"
  },
  {
    "text": "Let me write it down\nand then interpret. So for any alpha assume\nthat you converge",
    "start": "824110",
    "end": "850449"
  },
  {
    "text": "to a feasible solution. So assume that-- to a\nsolution that fits the data.",
    "start": "850450",
    "end": "866045"
  },
  {
    "text": " So in a sense that x theta\nalpha is equal to y vec.",
    "start": "866045",
    "end": "873959"
  },
  {
    "text": "So if this is\nsatisfied, it means that we fit the data exactly.",
    "start": "873960",
    "end": "878970"
  },
  {
    "text": "And I'm using a\npurple color for this is because I don't feel\nthat this is necessary have",
    "start": "878970",
    "end": "884550"
  },
  {
    "text": "to be assumption. You can prove this, actually. The paper did assume\nthis in the theorem,",
    "start": "884550",
    "end": "889620"
  },
  {
    "text": "but I don't think that\nyou have to do it. And actually, I just checked\nwith [INAUDIBLE],, the author,",
    "start": "889620",
    "end": "895530"
  },
  {
    "text": "two days ago, and he also\nthinks that you don't need it.",
    "start": "895530",
    "end": "901320"
  },
  {
    "text": "But this is not formally stated\nin the theorem, so I assume-- but I do strongly\nbelieve that you",
    "start": "901320",
    "end": "907260"
  },
  {
    "text": "can prove that you can converge\nto such a feasible solution. You don't necessarily\nhave to assume it.",
    "start": "907260",
    "end": "912400"
  },
  {
    "text": "But anyway, let's assume\nit just so that we are consistent with the paper.",
    "start": "912400",
    "end": "918190"
  },
  {
    "text": "By the way, this\nis the paper by-- I guess I'll probably\nadd the link. ",
    "start": "918190",
    "end": "925949"
  },
  {
    "start": "920000",
    "end": "1010000"
  },
  {
    "text": "Woodworth in 2020 as part\nof the paper as something like \"Rich and Kernel Regime.\"",
    "start": "925950",
    "end": "933040"
  },
  {
    "text": " I don't know. I don't-- I'm\nforgetting what's the--",
    "start": "933040",
    "end": "940040"
  },
  {
    "text": "\"Rich and Kernel Regime in\nOverparameterized Models. ",
    "start": "940040",
    "end": "949340"
  },
  {
    "text": "It's a pretty recent paper. It just showed up,\nlike, two years ago.",
    "start": "949340",
    "end": "954589"
  },
  {
    "text": "One year ago, actually. So suppose you-- OK. So suppose you have this.",
    "start": "954590",
    "end": "960540"
  },
  {
    "text": "Then we know theta alpha is\nnot only a feasible solution.",
    "start": "960540",
    "end": "967493"
  },
  {
    "text": "It's not only zero\norder solution, but also it's actually\nthe minimum norm solution or minimum complex\nsolution according",
    "start": "967493",
    "end": "973350"
  },
  {
    "text": "to the following\ncomplex dimension. That is the minimum\ncomplex solution.",
    "start": "973350",
    "end": "979580"
  },
  {
    "text": "So you get arg min over theta\nsuch that x theta equals to y. So among all the\nfeasible solution,",
    "start": "979580",
    "end": "985330"
  },
  {
    "text": "you will try to find\nthe minimum complexity where the complexity's\ndefined by Q alpha.",
    "start": "985330",
    "end": "991320"
  },
  {
    "text": "And what is Q alpha? So Q alpha is a\nfunction of alpha, so the complex measure does\nchange as you change alpha.",
    "start": "991320",
    "end": "998700"
  },
  {
    "text": "And Q alpha is equal to\nalpha squared times-- the alpha squared doesn't really\nmatter because it's a scalar.",
    "start": "998700",
    "end": "1005210"
  },
  {
    "text": "I'm going to do an argument\ntimes some function of each",
    "start": "1005210",
    "end": "1010370"
  },
  {
    "text": "of the coordinates q of\ntheta i over alpha squared.",
    "start": "1010370",
    "end": "1020839"
  },
  {
    "text": " And what is this little q? This little q is a one\ndimensional function",
    "start": "1020840",
    "end": "1028709"
  },
  {
    "text": "where this little q is a\nfunction that maps r to r.",
    "start": "1028710",
    "end": "1034260"
  },
  {
    "text": "Actually, it maps r to\nr non-negative, I think. And qz is equal to\nsomething that I",
    "start": "1034260",
    "end": "1041699"
  },
  {
    "text": "don't expect you\nto interpret, but I think we're going to look\nat special cases which",
    "start": "1041700",
    "end": "1047329"
  },
  {
    "text": "we can interpret. So this arc sinh.",
    "start": "1047329",
    "end": "1052560"
  },
  {
    "text": "I guess this is pronounced\nas \"sunge\" in US, or \"shine\"? \"Singe.\" OK. Anyway, arcSinh, sin\nhyperbolic, right, so z over 2.",
    "start": "1052560",
    "end": "1064860"
  },
  {
    "text": "I think UK it's called \"shine.\" I don't know why. Yeah. OK.",
    "start": "1064860",
    "end": "1071640"
  },
  {
    "text": "All right. OK. So the first other bit is that\neven though you didn't minimize",
    "start": "1071640",
    "end": "1077550"
  },
  {
    "text": "this complex machine algorithm,\nyou only ran gradient descent, somehow you find the\nminimum complexity solution,",
    "start": "1077550",
    "end": "1083850"
  },
  {
    "text": "and the complexity is defined\nby something like this.",
    "start": "1083850",
    "end": "1089309"
  },
  {
    "text": "And now let's try to interpret--\nthis is the abstract theorem, but the important thing\nis that in particular,",
    "start": "1089310",
    "end": "1097870"
  },
  {
    "text": "when alpha goes to\ninfinity-- so if you have a very large transition,\nthen this complex match-up q",
    "start": "1097870",
    "end": "1104350"
  },
  {
    "text": "with theta alpha, this is\nsomething like theta i alpha",
    "start": "1104350",
    "end": "1112220"
  },
  {
    "text": "squared? This is something like theta\ni squared over alpha to the 4. ",
    "start": "1112220",
    "end": "1123260"
  },
  {
    "text": "So which means that the Q\nalpha theta is something",
    "start": "1123260",
    "end": "1128780"
  },
  {
    "text": "like, I guess, 1 over\nalpha squared times",
    "start": "1128780",
    "end": "1134160"
  },
  {
    "text": "the 2 norm of theta squared.",
    "start": "1134160",
    "end": "1139690"
  },
  {
    "text": "So basically if alpha\ngoes to infinity, then the so-called complex\nmeasure, the q alpha,",
    "start": "1139690",
    "end": "1144909"
  },
  {
    "text": "is the L2 norm of theta. ",
    "start": "1144910",
    "end": "1150750"
  },
  {
    "text": "And so if alpha goes to 0, then\nwhat's the complexity measure?",
    "start": "1150750",
    "end": "1156120"
  },
  {
    "text": "So the complexity measure is\nwhat is the regularization effect or complexity measure?",
    "start": "1156120",
    "end": "1161970"
  },
  {
    "text": "This is q theta i\nover alpha squared. This is roughly\nsomething like theta i",
    "start": "1161970",
    "end": "1171029"
  },
  {
    "text": "absolute value over\nalpha squared times log 1 over alpha squared. I don't expect you to verify\nthis limit because you have",
    "start": "1171030",
    "end": "1178529"
  },
  {
    "text": "to do some kind of Taylor\nexpansion to see this, but this is the thing.",
    "start": "1178530",
    "end": "1183960"
  },
  {
    "text": "So then this means that a q\nalpha theta is, in some sense,",
    "start": "1183960",
    "end": "1190049"
  },
  {
    "text": "the 1 norm of theta. I guess over alpha squared\nterms log 1 over alpha squared.",
    "start": "1190050",
    "end": "1196268"
  },
  {
    "text": "But the content\ndoesn't really matter because it doesn't change\nthe order of different theta.",
    "start": "1196268",
    "end": "1203580"
  },
  {
    "text": "So [INAUDIBLE] this the\nQ theta [INAUDIBLE]??",
    "start": "1203580",
    "end": "1209600"
  },
  {
    "text": " Oh, sorry. Sorry, sorry, sorry.",
    "start": "1209600",
    "end": "1214990"
  },
  {
    "text": "That's-- yeah. Yeah, that's what I mean. ",
    "start": "1214990",
    "end": "1224040"
  },
  {
    "text": "Cool. So basically this is-- or in summary. So in summary, when\nalpha goes to infinity,",
    "start": "1224040",
    "end": "1234630"
  },
  {
    "text": "then this is minimum\nL2 norm solution",
    "start": "1234630",
    "end": "1240140"
  },
  {
    "text": "in a theta space of theta and\nan L4 norm for the W. Right?",
    "start": "1240140",
    "end": "1252230"
  },
  {
    "text": "Because theta is\nthe square of w. And when alpha\ngoes to 0, this is similar to what we\ndiscussed the last lecture.",
    "start": "1252230",
    "end": "1259200"
  },
  {
    "text": "So you have the minimum\nL1 norm of theta, which is",
    "start": "1259200",
    "end": "1266139"
  },
  {
    "text": "the minimum L2 norm of the w.",
    "start": "1266140",
    "end": "1271690"
  },
  {
    "text": "So this regime is what we\nhave seen in the last lecture, and with a very\nsimilar model, right? ",
    "start": "1271690",
    "end": "1279549"
  },
  {
    "text": "But this characterized\nthe whole regime. And between alpha\nand 0, you basically",
    "start": "1279550",
    "end": "1285010"
  },
  {
    "text": "have interpolation, some\nkind of interpolation, between L1 and L2\nregularization.",
    "start": "1285010",
    "end": "1295104"
  },
  {
    "text": " So that's why this is\nmore precise than before.",
    "start": "1295104",
    "end": "1302170"
  },
  {
    "text": "So you know how those\nthings interplay. Of course, it's kind of like\nfor any particular alpha, this q function is a\nlittle complicated.",
    "start": "1302170",
    "end": "1309640"
  },
  {
    "text": "But it's just the sum-- kind of like sum of some power\nof theta i in some sense,",
    "start": "1309640",
    "end": "1315150"
  },
  {
    "text": "but the power is kind\nof between 1 and 2. You can think of like that. But it's not exactly a power,\nbut it's something like that.",
    "start": "1315150",
    "end": "1322934"
  },
  {
    "text": "So alpha [INAUDIBLE]\nscale [INAUDIBLE]?? ",
    "start": "1322935",
    "end": "1328440"
  },
  {
    "text": "Oh, OK. Say again? Alpha is the scale of this? Yes. Yes. Alpha is the scale.",
    "start": "1328440",
    "end": "1334012"
  },
  {
    "text": "This is the only\nthing that depends on alpha in the algorithm. So we say that\nthe initialization",
    "start": "1334012",
    "end": "1339220"
  },
  {
    "text": "is alpha times L1 vector. And this L1 vector actually\ncan change it as well.",
    "start": "1339220",
    "end": "1344303"
  },
  {
    "text": "I think if you change\nit to an arbitrary vector for this\nregime, for this case,",
    "start": "1344303",
    "end": "1352000"
  },
  {
    "text": "you actually don't\nchange anything. So for this limit. So for the other limit, I\nthink you change a little bit",
    "start": "1352000",
    "end": "1359620"
  },
  {
    "text": "because the L2 norm becomes\nweighted by the initialization, by the particular initialization\nif it's not L1 vector.",
    "start": "1359620",
    "end": "1367042"
  },
  {
    "text": "If it's L1 vector, the\nweighting is just all the same for all coordinates. If they are not L1 vector,\nit's not L1 vector,",
    "start": "1367042",
    "end": "1373309"
  },
  {
    "text": "then you have a\ndifferent weighting. Yeah. Those can be found in the paper. For simplicity, I didn't\nshow the exact weighting.",
    "start": "1373310",
    "end": "1380414"
  },
  {
    "start": "1380415",
    "end": "1385760"
  },
  {
    "text": "Right. And so here are some intuitions\nabout this interpolation.",
    "start": "1385760",
    "end": "1392690"
  },
  {
    "text": "And in some sense, just to\nconnect to what we have-- in some sense, this\nis kind of like--",
    "start": "1392690",
    "end": "1398442"
  },
  {
    "text": "you can view this\nas a unification of what we have discussed\nin the past few lectures.",
    "start": "1398442",
    "end": "1404700"
  },
  {
    "text": "When alpha is small, this\nis small initialization. ",
    "start": "1404700",
    "end": "1412670"
  },
  {
    "text": "So this is basically similar\nto the previous case, a similar intuition\nto previous lecture.",
    "start": "1412670",
    "end": "1421960"
  },
  {
    "text": " But there is a small thing.",
    "start": "1421960",
    "end": "1428540"
  },
  {
    "text": "So it's not-- note is\nthat it's not exactly--",
    "start": "1428540",
    "end": "1435050"
  },
  {
    "text": "so this is only when\nit goes to a limit. So you have the minimum\nL2 norm solution.",
    "start": "1435050",
    "end": "1441980"
  },
  {
    "text": "When alpha is not 0,\nit's not exactly minimum.",
    "start": "1441980",
    "end": "1448544"
  },
  {
    "start": "1448545",
    "end": "1454500"
  },
  {
    "text": "The regularization effect is\nnot exactly closest, closest",
    "start": "1454500",
    "end": "1461000"
  },
  {
    "text": "solution to initialization. ",
    "start": "1461000",
    "end": "1467530"
  },
  {
    "text": "I think the paper shows-- there's some tiny\ndifferences, in some sense.",
    "start": "1467530",
    "end": "1473840"
  },
  {
    "text": "So only when alpha goes\nto 0 you can basically say this is the closest\nsolution to initialization.",
    "start": "1473840",
    "end": "1480180"
  },
  {
    "text": "But generally, this is something\nwe have discussed last time.",
    "start": "1480180",
    "end": "1485760"
  },
  {
    "text": "And when alpha goes to infinity,\nactually, this is indeed-- this is the NTK regime. ",
    "start": "1485760",
    "end": "1495559"
  },
  {
    "text": "And why this is the NTK regime? I'm going to show you. This is kind of similar to\nwhat we have discussed before,",
    "start": "1495560",
    "end": "1500990"
  },
  {
    "text": "but let me just do it again. So why this is NTK regime? So let's look at the--",
    "start": "1500990",
    "end": "1506570"
  },
  {
    "text": "recall that in the NTK regime,\nwe have these so-called two parameters, sigma and the beta.",
    "start": "1506570",
    "end": "1512059"
  },
  {
    "text": "So beta there was the\nsmoothness or the Lipschitzness",
    "start": "1512060",
    "end": "1517340"
  },
  {
    "text": "of the gradient and this is\nthe condition number, right?",
    "start": "1517340",
    "end": "1523860"
  },
  {
    "text": " And we say that-- and recall that we\nhad this discussion",
    "start": "1523860",
    "end": "1530260"
  },
  {
    "text": "that sigma beta over\nsigma squared mattered.",
    "start": "1530260",
    "end": "1536970"
  },
  {
    "text": "Right. So if this goes to 0, then\nyou are in NTK regime. You can approximate\nthat quadratic. And now let's compute what\nsigma beta is in this case.",
    "start": "1536970",
    "end": "1545309"
  },
  {
    "text": "So the gradient with respect\nto the initialization, x-- so w",
    "start": "1545310",
    "end": "1550590"
  },
  {
    "text": "is 0.  w0.",
    "start": "1550590",
    "end": "1555620"
  },
  {
    "text": "Let's take this to\nbe alpha L1 vector. w plus and w minus\nwill be both L1 vector.",
    "start": "1555620",
    "end": "1562580"
  },
  {
    "text": "Alpha times L1 vector. ",
    "start": "1562580",
    "end": "1569070"
  },
  {
    "text": "And we can compute the\ngradient initialization x.",
    "start": "1569070",
    "end": "1575630"
  },
  {
    "text": "So there are two\nset of parameters. So the gradient\nfor the w plus, I think you can-- if you compute,\nis something like this.",
    "start": "1575630",
    "end": "1583380"
  },
  {
    "text": "And the gradient for the\nw minus is this to this. ",
    "start": "1583380",
    "end": "1588860"
  },
  {
    "text": "Oh, sorry. This is-- I'm sorry. Elementwise product. You can do this easily just by\nchain rule for every dimension.",
    "start": "1588860",
    "end": "1596920"
  },
  {
    "text": "And this is just equals to--",
    "start": "1596920",
    "end": "1602460"
  },
  {
    "text": "I guess there's a vector\ntwo here, which I oftentimes probably don't look at.",
    "start": "1602460",
    "end": "1607950"
  },
  {
    "text": "And this is 2 alpha-- because these two\nare just L1 vector, so this is 2 alpha x minus x.",
    "start": "1607950",
    "end": "1617020"
  },
  {
    "text": "Sometimes this. Right? And now you can see that\nsigma and beta both linearly",
    "start": "1617020",
    "end": "1629840"
  },
  {
    "text": "depend on alpha, right? So what is sigma? Sigma is the\ncondition number of--",
    "start": "1629840",
    "end": "1636200"
  },
  {
    "text": "sigma is the condition number\nof the gradient matrix, the feature matrix that consists\nof the gradient for every data",
    "start": "1636200",
    "end": "1643520"
  },
  {
    "text": "point. And the condition number\nscales linearly in alpha, and beta is the Lipschitzness,\nwhich also scales linearly",
    "start": "1643520",
    "end": "1650720"
  },
  {
    "text": "in alpha. It's because alpha is multiplied\nin front of the gradient.",
    "start": "1650720",
    "end": "1655790"
  },
  {
    "text": "So both of these scale linearly,\nso that's why beta over sigma squared will converge\nto 0 as alpha",
    "start": "1655790",
    "end": "1662870"
  },
  {
    "text": "goes to infinity\nbecause below you have to go to dependency on alpha.",
    "start": "1662870",
    "end": "1669230"
  },
  {
    "text": "And in the\ndenominator, you have-- in the numerator\nyou have degree--",
    "start": "1669230",
    "end": "1674240"
  },
  {
    "text": "you have a linear\ndependency on alpha. So that's why this\nwhole thing goes to 0",
    "start": "1674240",
    "end": "1679610"
  },
  {
    "text": "as alpha goes to infinity. ",
    "start": "1679610",
    "end": "1684750"
  },
  {
    "text": "And also, when alpha\ngoes to infinity, this is the NTK regime\nfor the trivial feature",
    "start": "1684750",
    "end": "1692650"
  },
  {
    "start": "1685000",
    "end": "2040000"
  },
  {
    "text": "because now the xr field\nof x is this thing.",
    "start": "1692650",
    "end": "1698840"
  },
  {
    "text": "This feature map is really just\nthat literally-- this, right? This is just literally\nthe trivial feature",
    "start": "1698840",
    "end": "1704330"
  },
  {
    "text": "because the only\nthing you did is that you flipped the x,\nwhich doesn't really make any difference, essentially. ",
    "start": "1704330",
    "end": "1711747"
  },
  {
    "text": "So basically you've got\nthe minimum norm solution so you got-- so if you believe in\nthe NTK perspective,",
    "start": "1711748",
    "end": "1718070"
  },
  {
    "text": "you should get the\nminimum norm solution. So the NTK perspective\nwill also tell you that you get minimum\nnorm solution.",
    "start": "1718070",
    "end": "1724400"
  },
  {
    "start": "1724400",
    "end": "1730170"
  },
  {
    "text": "Like, according to\nthe features, right? Minimum norm solution--\nminimum L2 norm solution",
    "start": "1730170",
    "end": "1735660"
  },
  {
    "text": "using the feature x minus x.",
    "start": "1735660",
    "end": "1741850"
  },
  {
    "text": "And x minus x is, in\nterms of a feature, is not very different\nfrom x itself. So basically you just get\nessentially the minimum L2 norm",
    "start": "1741850",
    "end": "1749140"
  },
  {
    "text": "solution for the linear model. So that's the same\nas we discussed--",
    "start": "1749140",
    "end": "1755790"
  },
  {
    "text": "the same conclusion\nas we discussed above. ",
    "start": "1755790",
    "end": "1763280"
  },
  {
    "text": "Any questions so far? ",
    "start": "1763280",
    "end": "1776850"
  },
  {
    "text": "So the question is why NTK gives\nyou the minimum norm solution. I think this is just because\nwe will do the kernel method.",
    "start": "1776850",
    "end": "1783270"
  },
  {
    "text": " So when you use kernel method-- so NTK tells you that you\nare doing kernel method",
    "start": "1783270",
    "end": "1790610"
  },
  {
    "text": "with certain features, right? So NTK means kernel method.",
    "start": "1790610",
    "end": "1797059"
  },
  {
    "text": "And the feature just turns out\nto be this trivial feature, and kernel method\nwith the feature",
    "start": "1797060",
    "end": "1803180"
  },
  {
    "text": "just gives you the\nminimum norm solution. That's what the\nkernel method does. So because what kernel\nmethod does is that you--",
    "start": "1803180",
    "end": "1809780"
  },
  {
    "text": " that's just what the\nkernel method do when",
    "start": "1809780",
    "end": "1817130"
  },
  {
    "text": "you don't have enough data. When your feature\ndimension is bigger than the number of examples\nin the kernel method,",
    "start": "1817130",
    "end": "1825170"
  },
  {
    "text": "you are learning the minimum\nnorm solution for the features",
    "start": "1825170",
    "end": "1832500"
  },
  {
    "text": "because otherwise you have\nto define something, right? So the kernel method,\neverything is L2, so you are minimizing L2 norm.",
    "start": "1832500",
    "end": "1838620"
  },
  {
    "text": "That's implicitly in\nthe kernel method. [INAUDIBLE]",
    "start": "1838620",
    "end": "1844974"
  },
  {
    "text": " Yeah, that doesn't\ndepend on initialization because it's a complex problem. ",
    "start": "1844974",
    "end": "1851710"
  },
  {
    "text": "Yeah. And you use a\nparticular algorithm when you do the kernel method.",
    "start": "1851710",
    "end": "1856830"
  },
  {
    "text": "That algorithm gave you\nthe minimum norm solution. ",
    "start": "1856830",
    "end": "1868150"
  },
  {
    "text": "OK, cool.  OK. Any question?",
    "start": "1868150",
    "end": "1873930"
  },
  {
    "text": "One question? [INAUDIBLE] going to infinity\n[INAUDIBLE] respective",
    "start": "1873930",
    "end": "1881130"
  },
  {
    "text": "[INAUDIBLE],, like, it could\nlead you off of the [INAUDIBLE]??",
    "start": "1881130",
    "end": "1887380"
  },
  {
    "text": "But I guess that goes a\nlittle bit back [INAUDIBLE].. We would say that\nthere is [INAUDIBLE]..",
    "start": "1887380",
    "end": "1893523"
  },
  {
    "start": "1893524",
    "end": "1900840"
  },
  {
    "text": "Yep. So I guess, in some sense,\nrepeating the question and also answering it.",
    "start": "1900840",
    "end": "1905920"
  },
  {
    "text": "So when alpha goes to infinity-- so yes.",
    "start": "1905920",
    "end": "1911340"
  },
  {
    "text": "So your problem will\nbe very ill posed. In some sense, the\noptimization landscape",
    "start": "1911340",
    "end": "1917220"
  },
  {
    "text": "will be very bad just\nbecause your function will be not very smooth.",
    "start": "1917220",
    "end": "1923490"
  },
  {
    "text": "And this part is hidden\nhere because you are using gradient flow so you can--",
    "start": "1923490",
    "end": "1928890"
  },
  {
    "text": "so you are using infinitesimal\nsmall learning rate. So that's why it's\nhidden under the-- it's swept under the rug.",
    "start": "1928890",
    "end": "1935070"
  },
  {
    "text": "And then practically, you\nalso don't necessarily want to use the\nlarge learning rate.",
    "start": "1935070",
    "end": "1941430"
  },
  {
    "text": "For one reason is\nthe optimization, and the other reason is that\nmaybe the L2 norm solution is also not good, right? So you also want-- you have\nan L1 regularization, at least",
    "start": "1941430",
    "end": "1949230"
  },
  {
    "text": "for this particular setting. So that's another\nreason why you don't want to use very\nlarge learning rate.",
    "start": "1949230",
    "end": "1956820"
  },
  {
    "text": "sorry, very large\ninitialization. And another thing is that in\npractice, people sometimes",
    "start": "1956820",
    "end": "1964590"
  },
  {
    "text": "do use-- this is about the\nempirical setup. Sometimes you do use\nlarge initialization,",
    "start": "1964590",
    "end": "1972530"
  },
  {
    "text": "but people don't use\ninfinitesimal small learning rate. So then you still cannot\nget into the NTK regime.",
    "start": "1972530",
    "end": "1979642"
  },
  {
    "text": "But that's a good\nthing because you don't want to go to the NTK regime. So that's why, at the beginning,\nsome people have confusions",
    "start": "1979642",
    "end": "1986090"
  },
  {
    "text": "because at the very first\npaper by this NTK paper, I think they are claiming the\ninitialization scheme they",
    "start": "1986090",
    "end": "1993590"
  },
  {
    "text": "are studying is actually\nwhat people do in practice, and that's kind of true. It's very close to the\nKaiming He initialization",
    "start": "1993590",
    "end": "2002500"
  },
  {
    "text": "or the Xavier initialization\nin terms of the scale. But because they are using\nvery, very small learning rate,",
    "start": "2002500",
    "end": "2009880"
  },
  {
    "text": "so it's actually not really-- the theoretical setup requires\nvery, very small learning rate. But empirically, you don't\nuse those small learning rate,",
    "start": "2009880",
    "end": "2017205"
  },
  {
    "text": "and also the theoretical setup\ndoesn't have the stochasticity. So all of this together, it\nmakes the theoretical setup",
    "start": "2017205",
    "end": "2022690"
  },
  {
    "text": "different from the\nempirical setting.",
    "start": "2022690",
    "end": "2027799"
  },
  {
    "text": "And that's a good thing\nbecause the theoretical setup says that you don't\nreally do anything super different from kernels.",
    "start": "2027800",
    "end": "2036300"
  },
  {
    "text": "Yeah. OK, so now let's discuss\nthe proof of this theorem.",
    "start": "2036300",
    "end": "2042000"
  },
  {
    "start": "2040000",
    "end": "2550000"
  },
  {
    "text": "So I don't have-- there is a little bit--",
    "start": "2042000",
    "end": "2047520"
  },
  {
    "text": "this proof is kind of\ninteresting in the sense that the proof is similar to\nactually the linear regression",
    "start": "2047520",
    "end": "2056169"
  },
  {
    "text": "model similar to the\nlinear regression proof,",
    "start": "2056170",
    "end": "2065260"
  },
  {
    "text": "but not similar to what\nwe discussed last time.",
    "start": "2065260",
    "end": "2071379"
  },
  {
    "text": "Not similar to last lecture. You would probably guess this\nis similar to last lecture because the last lecture\nhas almost the same model",
    "start": "2071380",
    "end": "2078052"
  },
  {
    "text": "as this one and is only\ndoing a subcase of this when alpha goes to 0. But it turns out\nthat the proof is",
    "start": "2078052",
    "end": "2084429"
  },
  {
    "text": "very similar to the\nlinear regression one, and you have these two steps. The first step is that you\nfind the invariance maintained",
    "start": "2084429",
    "end": "2099530"
  },
  {
    "text": "by the algorithm,\nby the optimizer. And recall that\nthis invariance was",
    "start": "2099530",
    "end": "2108170"
  },
  {
    "text": "that theta is in the span of\nxi for the linear regression.",
    "start": "2108170",
    "end": "2117890"
  },
  {
    "text": "This was probably two\nor three lectures ago when we analyzed the\nimplicit regularization",
    "start": "2117890",
    "end": "2124970"
  },
  {
    "text": "effect of initialization.\nfor linear regression we say that because you need a\n0 and you use gradient descent,",
    "start": "2124970",
    "end": "2130280"
  },
  {
    "text": "you always need a\nspan of the data. And here we're going to find\na different environment, which",
    "start": "2130280",
    "end": "2136010"
  },
  {
    "text": "is more complicated and\neven harder to express, but we're going to\nfind the invariance. And then you use the invariance.",
    "start": "2136010",
    "end": "2143880"
  },
  {
    "text": "So step two, in some\nsense, characterized the--",
    "start": "2143880",
    "end": "2150609"
  },
  {
    "text": "I guess characterize\nis a very vague term, but characterized the\nsolution using the invariance. And sometimes you\nuse the invariance",
    "start": "2150610",
    "end": "2156520"
  },
  {
    "text": "as additional information,\nright, to pin down",
    "start": "2156520",
    "end": "2162900"
  },
  {
    "text": "which solution you converge to. In some sense, the\ndifficulty is that if you--",
    "start": "2162900",
    "end": "2169770"
  },
  {
    "text": "without any additional\nthing, you just know that you convert to\na zero order solution. You don't know which\none it converts to,",
    "start": "2169770",
    "end": "2174779"
  },
  {
    "text": "and the invariance tells you\nthat which one it converts to and the invariance\ndepends on alpha. ",
    "start": "2174780",
    "end": "2181740"
  },
  {
    "text": "And there's nothing about\npopulation versus empirical.",
    "start": "2181740",
    "end": "2191760"
  },
  {
    "text": "Everything is empirical here. I didn't even define\nwhere the data comes from.",
    "start": "2191760",
    "end": "2197330"
  },
  {
    "text": "I only tell you that this is\nthe minimum norm solution such that the empirical\nerror is zero.",
    "start": "2197330",
    "end": "2203580"
  },
  {
    "text": "I don't have to care\nabout population at all. So yeah.",
    "start": "2203580",
    "end": "2208710"
  },
  {
    "text": "How does this kind\nof technique compare with the techniques we\ndiscussed last time where you use the fact that the\ndata, the empirical loss,",
    "start": "2208710",
    "end": "2216750"
  },
  {
    "text": "concentrates around\nthe population loss in certain regions and you\nsomehow do some kind of control",
    "start": "2216750",
    "end": "2222480"
  },
  {
    "text": "of the dynamics? I don't know how. It's kind of hard to compare. These are two\ndifferent approach.",
    "start": "2222480",
    "end": "2228869"
  },
  {
    "text": "There is some good thing\nabout this kind of approach because this doesn't\nrequire population. That sounds a good thing.",
    "start": "2228870",
    "end": "2236299"
  },
  {
    "text": "But the bad things\nabout this approach seems to be that it's\nvery hard to find invariance for harder models,\nfor more complex models.",
    "start": "2236300",
    "end": "2246240"
  },
  {
    "text": "You will see the invariance is\na little bit kind of magical somehow. But that's that.",
    "start": "2246240",
    "end": "2252060"
  },
  {
    "text": "For more complex models,\neven the previous approach, the approach we discussed last\ntime, wouldn't work either.",
    "start": "2252060",
    "end": "2257940"
  },
  {
    "text": "So it's hard to say. Anyway, so let's proceed to\nsee how does the proof work.",
    "start": "2257940",
    "end": "2264730"
  },
  {
    "text": "So we need a little bit\nnotation to somehow simplify our x position. So let's say let this x tilde\nto be the extended data matrix.",
    "start": "2264730",
    "end": "2275060"
  },
  {
    "text": "So you extend the matrix to-- this is to deal with the-- ",
    "start": "2275060",
    "end": "2283495"
  },
  {
    "text": "you concatenate x and minus x so\nthat you get a n by 2d matrix.",
    "start": "2283495",
    "end": "2288610"
  },
  {
    "text": "And sometimes this is just\nto try to write everything in matrix notation\nso that you don't have to have the minus thing.",
    "start": "2288610",
    "end": "2294000"
  },
  {
    "text": "So we would take wt to\nbe the concatenation of w plus t and w minus t.",
    "start": "2294000",
    "end": "2301450"
  },
  {
    "text": "This is of dimension 2d. And let's take a wt o dot 2.",
    "start": "2301450",
    "end": "2308960"
  },
  {
    "text": "I guess we say that this is\nthe entry wise power of wt.",
    "start": "2308960",
    "end": "2315230"
  },
  {
    "text": "And this means that\nwith this notation, x tilde times wt o dot 2, this\nis x minus x times w plus t",
    "start": "2315230",
    "end": "2329775"
  },
  {
    "text": "dot 2, w minus t dot 2. And you can verify\nthis is really the same as the-- this is just\nthe way the model computes,",
    "start": "2329775",
    "end": "2336667"
  },
  {
    "text": "right? ",
    "start": "2336667",
    "end": "2344960"
  },
  {
    "text": "This is the model\noutput in data points. So I just want to use this\nso that you have the matrix",
    "start": "2344960",
    "end": "2353490"
  },
  {
    "text": "notation, and now you can\ncompute with a derivative of t,",
    "start": "2353490",
    "end": "2360330"
  },
  {
    "text": "with w dot t. This is the gradient because\nwe are doing gradient flow, so this is equal to the\ngradient of L at time t.",
    "start": "2360330",
    "end": "2367770"
  },
  {
    "text": "And what's the gradient\nof L at time t? This is gradient of\nthis loss function, and now can be written as--",
    "start": "2367770",
    "end": "2374830"
  },
  {
    "text": "I guess the loss\nfunction of wt now can be written as\nx tilde wt o dot",
    "start": "2374830",
    "end": "2385880"
  },
  {
    "text": "2 minus y vec 2 norm squared. That's because I\nvectorize everything.",
    "start": "2385880",
    "end": "2391100"
  },
  {
    "text": "So I copy and paste here and\nthen I take the gradient.",
    "start": "2391100",
    "end": "2397620"
  },
  {
    "text": "Taking the gradient, you\ncan use the chain rule. So if you believe v\nhas got a a direction",
    "start": "2397620",
    "end": "2404100"
  },
  {
    "text": "with this is x tilde transpose\nrt entry wise times wt.",
    "start": "2404100",
    "end": "2413530"
  },
  {
    "text": "Where rt is equal to x tilde\ntimes wt o dot 2 minus y vec",
    "start": "2413530",
    "end": "2423530"
  },
  {
    "text": "is the residual vector. ",
    "start": "2423530",
    "end": "2428823"
  },
  {
    "text": "So if you are familiar\nwith linear regression, you will realize that\nthis is kind of like--",
    "start": "2428823",
    "end": "2435700"
  },
  {
    "text": "this is what you got from the-- so this is if it was\nlinear regression.",
    "start": "2435700",
    "end": "2441970"
  },
  {
    "text": " If it is a linear\nregression, then this term",
    "start": "2441970",
    "end": "2447560"
  },
  {
    "text": "will be our gradient. And now it's not\nlinear regression because you have the\nquadratic parameterized model. That's why you also\nhave to do chain",
    "start": "2447560",
    "end": "2454040"
  },
  {
    "text": "rule to look at the derivative\nof the quadratic of wt. That's why this-- this is\nbecause it's quadratic.",
    "start": "2454040",
    "end": "2461630"
  },
  {
    "text": " Anyway, so this is one way to\nthink about why this is true.",
    "start": "2461630",
    "end": "2470040"
  },
  {
    "text": "But the formal\nverification would be just that you look at-- you do chain rules [INAUDIBLE].",
    "start": "2470040",
    "end": "2475638"
  },
  {
    "text": "[INAUDIBLE] Oh, sorry, sorry. ",
    "start": "2475638",
    "end": "2480977"
  },
  {
    "text": "There should be a-- I think there's a 2 here. Wait. ",
    "start": "2480978",
    "end": "2488390"
  },
  {
    "text": "Let me see.  I think I wanted to make it 2.",
    "start": "2488390",
    "end": "2495000"
  },
  {
    "text": "So that means I think my loss\nfunction should have a 1/2.",
    "start": "2495000",
    "end": "2500070"
  },
  {
    "text": "So where's my loss function?  The loss function has a 1/2.",
    "start": "2500070",
    "end": "2508190"
  },
  {
    "text": "I guess I also defined a loss\nfunction somewhere else before. ",
    "start": "2508190",
    "end": "2519730"
  },
  {
    "text": "Here. Right? That sounds good. ",
    "start": "2519730",
    "end": "2527230"
  },
  {
    "text": "My brain just automatically\nremoved all the constants",
    "start": "2527230",
    "end": "2533330"
  },
  {
    "text": "so it's very hard for\nme to deal with this. ",
    "start": "2533330",
    "end": "2545370"
  },
  {
    "text": "OK. Cool.  All right. OK.",
    "start": "2545370",
    "end": "2550400"
  },
  {
    "start": "2550000",
    "end": "2848000"
  },
  {
    "text": "So now, how do we-- we said that we want to have\nsome invariance for this",
    "start": "2550400",
    "end": "2556420"
  },
  {
    "text": "in some sense, so we want to\nsomehow solve this differential equation. But you cannot really\nsolve it exactly.",
    "start": "2556420",
    "end": "2563369"
  },
  {
    "text": "I'm not an expert on solving\ndifferential equations, but I think this is\nbeyond the scope of--",
    "start": "2563370",
    "end": "2570970"
  },
  {
    "text": "this is something you cannot\nreally have a closed form solution.",
    "start": "2570970",
    "end": "2576500"
  },
  {
    "text": "But interestingly, you can do-- you can get something\nwithout solving it, exactly.",
    "start": "2576500",
    "end": "2582410"
  },
  {
    "text": "So we claim that--  so actually, this\nis a-- in a paper,",
    "start": "2582410",
    "end": "2589020"
  },
  {
    "text": "they claim it's\neasy to verify this. You can claim that wt\nsatisfies the following.",
    "start": "2589020",
    "end": "2597940"
  },
  {
    "text": "This by this times exponential\nminus 2x 2 transposed t rs ds.",
    "start": "2597940",
    "end": "2607640"
  },
  {
    "start": "2607640",
    "end": "2613450"
  },
  {
    "text": "OK. Why this is the case? So first of all, this\nis not a solution. This is not like--",
    "start": "2613450",
    "end": "2618625"
  },
  {
    "text": "depending on what\nyou mean by solution. This is not necessarily\nmy definition of closed form solution because\nrs still is a function of w.",
    "start": "2618625",
    "end": "2627720"
  },
  {
    "start": "2627720",
    "end": "2635950"
  },
  {
    "text": "But it's going to be\nsomething very useful for us. And why this is true?",
    "start": "2635950",
    "end": "2642589"
  },
  {
    "text": "It's actually relatively simple,\nbut so here is the reason. So this is because\nwhat you can do--",
    "start": "2642590",
    "end": "2653240"
  },
  {
    "text": "suppose you have a differential\nequation something, like u dot t is equal to vt times ut.",
    "start": "2653240",
    "end": "2662809"
  },
  {
    "text": "So I'm trying to abstractify\nit a little bit so that I can give a clean analysis.",
    "start": "2662810",
    "end": "2669630"
  },
  {
    "text": "So you can see that this is\na good abstraction of what we had before because before\non the left hand side,",
    "start": "2669630",
    "end": "2675622"
  },
  {
    "text": "you have the derivative w\nand on the right hand side, you have something\ntimes w itself. So this will be u.",
    "start": "2675622",
    "end": "2681180"
  },
  {
    "text": "This will be v, this will\nbe u, and this u dot t. That's my abstraction.",
    "start": "2681180",
    "end": "2686430"
  },
  {
    "text": "And then suppose you\nhave such a thing, then you can always\ndo the following. You can say that u dot t\nover ut is equal to vt.",
    "start": "2686430",
    "end": "2698609"
  },
  {
    "text": "Right? That's always true. And then the left\nhand side, this is a magical thing\nin many cases.",
    "start": "2698610",
    "end": "2706609"
  },
  {
    "text": "This is log of-- of the log of ut. ",
    "start": "2706610",
    "end": "2715390"
  },
  {
    "text": "It's by chain rule. I think probably you have\nseen this in other contexts like policy gradients\nand other cases,",
    "start": "2715390",
    "end": "2722278"
  },
  {
    "text": "depending on whether\nyou know any of those. But anyway. And then you can\nintegrate both sides.",
    "start": "2722278",
    "end": "2729290"
  },
  {
    "text": "So you can say,\nif you integrate, you got a log of\nut minus log of u0",
    "start": "2729290",
    "end": "2740660"
  },
  {
    "text": "is equal to the integration of\nthe right hand side like this.",
    "start": "2740660",
    "end": "2748289"
  },
  {
    "text": "And now you remove the log\nand you get exponentials. Ut over u0 is exponential\ntimes integration of this.",
    "start": "2748290",
    "end": "2758970"
  },
  {
    "start": "2758970",
    "end": "2764190"
  },
  {
    "text": "And now if you map u\nto w and v to this--",
    "start": "2764190",
    "end": "2772579"
  },
  {
    "text": "I guess u to a\ncoordinate of w and v to be a coordinate of\nthis x transpose rti,",
    "start": "2772580",
    "end": "2782990"
  },
  {
    "text": "then you can apply this and\nyou get the desired result. And by the way, I think I\nneed to make a remark here",
    "start": "2782990",
    "end": "2788900"
  },
  {
    "text": "that this is entry wise\napplication of exponential.",
    "start": "2788900",
    "end": "2796270"
  },
  {
    "text": " So this is a vector. This is another matrix.",
    "start": "2796270",
    "end": "2801859"
  },
  {
    "text": "So a matrix from a vector. This becomes a vector\nand you actually take entry wise\nexponential and you take",
    "start": "2801860",
    "end": "2807080"
  },
  {
    "text": "the entry wise product with w0. ",
    "start": "2807080",
    "end": "2813530"
  },
  {
    "text": "OK. Any questions so far? ",
    "start": "2813530",
    "end": "2820000"
  },
  {
    "text": "So now, let's see\nwhy this is useful. It's a little bit\nmagical, in my opinion.",
    "start": "2820000",
    "end": "2825150"
  },
  {
    "text": "I don't have a--  conceptually I\nthink this is fun,",
    "start": "2825150",
    "end": "2830520"
  },
  {
    "text": "but I think the proof\non a proof level is-- somehow there is\na little kind of--",
    "start": "2830520",
    "end": "2836570"
  },
  {
    "text": "either you can call it\ncoincidence or magic. So there turns out\nthat this is all",
    "start": "2836570",
    "end": "2841650"
  },
  {
    "text": "you need to verify this\nis a good solution. This is the minimizer\nof the solution.",
    "start": "2841650",
    "end": "2848200"
  },
  {
    "start": "2848000",
    "end": "3183000"
  },
  {
    "text": "So first of all, we turn this\ninto something about theta. So now we have a\ncharacterization for w,",
    "start": "2848200",
    "end": "2854130"
  },
  {
    "text": "and let's turn it into\nsomething about theta. So recall that-- and also we\nsimplify this a little bit.",
    "start": "2854130",
    "end": "2859470"
  },
  {
    "text": "Recall that w plus 0\nis alpha L1 vector. w minus 0 is alpha\ntimes L1 vector.",
    "start": "2859470",
    "end": "2867960"
  },
  {
    "text": "That means that w0 is also L1-- alpha times L1 vector. This is in 2d dimension because\nw is a concatenation of w plus",
    "start": "2867960",
    "end": "2876930"
  },
  {
    "text": "and w minus. So that's why this thing, w0,\nis basically not important.",
    "start": "2876930",
    "end": "2882690"
  },
  {
    "text": "You just have alpha. So in the theta t to\nthe theta at times t is w plus 1 power of 2\nminus w minus t to power 2.",
    "start": "2882690",
    "end": "2896560"
  },
  {
    "text": "And this will be-- ",
    "start": "2896560",
    "end": "2902540"
  },
  {
    "text": "OK, let's use this formula. Let's use this formula. Let's call this one using one.",
    "start": "2902540",
    "end": "2909430"
  },
  {
    "text": "So the w0 doesn't matter. The only thing it\ncontributes to is alpha, so we get alpha squared.",
    "start": "2909430",
    "end": "2914605"
  },
  {
    "start": "2914605",
    "end": "2922665"
  },
  {
    "text": "I guess I'm not sure. Let's see. So maybe I'll just do this\nsmall characterization here.",
    "start": "2922665",
    "end": "2929310"
  },
  {
    "text": "So x tilde transpose is x\ntranspose minus x transpose.",
    "start": "2929310",
    "end": "2934780"
  },
  {
    "text": "So that's why\nexponential minus 2x 2 dot transpose some vector v--",
    "start": "2934780",
    "end": "2942325"
  },
  {
    "text": "v will be this integral. So this will be a vector like--",
    "start": "2942325",
    "end": "2949825"
  },
  {
    "text": " let's say suppose you\ntake this, the power of 2.",
    "start": "2949825",
    "end": "2958079"
  },
  {
    "text": "Then this will be exponential\nminus 2x transpose",
    "start": "2958080",
    "end": "2968820"
  },
  {
    "text": "v because this part\nis from the first part and the exponential minus 2--",
    "start": "2968820",
    "end": "2974545"
  },
  {
    "text": " times 2x transpose t\nand then to the power 2.",
    "start": "2974545",
    "end": "2983010"
  },
  {
    "text": "OK. And then this-- and this\npower 2 will become 4 because this exponential. So we get exponential 4 minus\n4x transpose v exponential 4x",
    "start": "2983010",
    "end": "2994320"
  },
  {
    "text": "transpose v. So this\nsmall derivation is trying to deal\nwith this, so you",
    "start": "2994320",
    "end": "3000520"
  },
  {
    "text": "know that this\nthing to the power 2 will be something like this. And then the first\npower corresponds to--",
    "start": "3000520",
    "end": "3008859"
  },
  {
    "text": "the first power corresponds\nto w plus and second power corresponds w\nminus, so that's why you get something like here.",
    "start": "3008860",
    "end": "3017890"
  },
  {
    "text": "w plus squared\nwill be exponential",
    "start": "3017890",
    "end": "3023789"
  },
  {
    "text": "minus 4x transpose this\nand minus exponential",
    "start": "3023790",
    "end": "3037150"
  },
  {
    "text": "4x transpose this. ",
    "start": "3037150",
    "end": "3043100"
  },
  {
    "text": "I guess what I'm\ndoing here is just trying to make you believe\nthat this derivation is true, but it should be\ntrivial derivation.",
    "start": "3043100",
    "end": "3050210"
  },
  {
    "text": "There is nothing difficult. OK,\nso this is the characterization",
    "start": "3050210",
    "end": "3057310"
  },
  {
    "text": "of theta. And you can see that this\nis this exponential of this",
    "start": "3057310",
    "end": "3065349"
  },
  {
    "text": "minus x minus of\nthe same thing, you can write this more\nsuccinctly as the sinh,",
    "start": "3065350",
    "end": "3071050"
  },
  {
    "text": "the sinh for x transpose.",
    "start": "3071050",
    "end": "3078610"
  },
  {
    "text": "This is just a better\ndefinition of the sinh. I think sinh uses something\nlike exponential t plus exponential minus t over 2.",
    "start": "3078610",
    "end": "3084910"
  },
  {
    "text": "Something like that. OK. So basically, we have\na calculation of theta. ",
    "start": "3084910",
    "end": "3094180"
  },
  {
    "text": "Right? And then you know that the theta\nalpha is equal to theta alpha",
    "start": "3094180",
    "end": "3100380"
  },
  {
    "text": "at theta at infinity. So this is equal to 2 alpha\nsquared minus 4x transpose.",
    "start": "3100380",
    "end": "3111920"
  },
  {
    "text": " 0 to infinity rs ds.",
    "start": "3111920",
    "end": "3118424"
  },
  {
    "text": "OK.  So this is something we\nknow that theta alpha,",
    "start": "3118424",
    "end": "3124730"
  },
  {
    "text": "the final point, satisfied. Maybe let's call\nthis equation two. And we also know\nthat x theta alpha",
    "start": "3124730",
    "end": "3130369"
  },
  {
    "text": "is equal to y because we\nassume, or we can prove-- I guess we discussed this. I think we can prove that\nyou converge to a feasible",
    "start": "3130370",
    "end": "3138830"
  },
  {
    "text": "solution. And I'm claiming\nthat one and two-- so two and three,\nthese two things,",
    "start": "3138830",
    "end": "3146600"
  },
  {
    "text": "turned out to be the optimality\ncondition of the program.",
    "start": "3146600",
    "end": "3157545"
  },
  {
    "text": " Let's call it one. So one is this arg min theta.",
    "start": "3157545",
    "end": "3166898"
  },
  {
    "text": "OK, I guess it's far, far away. So here. ",
    "start": "3166898",
    "end": "3173530"
  },
  {
    "text": "Let's call this\nprogram program I. So you want to say that\ntheta alpha is the minimizer",
    "start": "3173530",
    "end": "3180140"
  },
  {
    "text": "of this problem one. And it turns out that theta\nalpha satisfies these two equations, two and three.",
    "start": "3180140",
    "end": "3186160"
  },
  {
    "text": "And these two equations are\nthe optimality condition of that optimization\nprogram, program one.",
    "start": "3186160",
    "end": "3192325"
  },
  {
    "text": "And that optimization\nproblem only has one solution\nbecause it's convex",
    "start": "3192325",
    "end": "3198549"
  },
  {
    "text": "so that's why this theta\nalpha is the solution. That's the plan for the next.",
    "start": "3198550",
    "end": "3204480"
  },
  {
    "text": "Right. Sounds good. So by optimizing condition,\nI really mean KKT condition.",
    "start": "3204480",
    "end": "3210500"
  },
  {
    "text": "So I'm not sure\nwhether all of you are familiar with\nthe KKT condition,",
    "start": "3210500",
    "end": "3215569"
  },
  {
    "text": "so I guess there are two\nways to think about this. This is just a\nsmall thing about--",
    "start": "3215570",
    "end": "3221520"
  },
  {
    "text": "background about KKT condition. So these are optimality\nconditions for constraint",
    "start": "3221520",
    "end": "3229000"
  },
  {
    "text": "optimization problem. ",
    "start": "3229000",
    "end": "3234710"
  },
  {
    "text": "To be honest, I\nnever really remember exactly what the KKT\ncondition in many cases. So what I am going to show you\nis one way to think about it,",
    "start": "3234710",
    "end": "3244640"
  },
  {
    "text": "which is probably\nnot exactly the same as what you can\nread from the book,",
    "start": "3244640",
    "end": "3251860"
  },
  {
    "text": "but it's going to\nbe very similar. So suppose you have\nthese kind of things, optimization programs like this,\nand q theta is a convex thing.",
    "start": "3251860",
    "end": "3260210"
  },
  {
    "text": "And so first of all, the KKT\ncondition is the following.",
    "start": "3260210",
    "end": "3267320"
  },
  {
    "text": "So it says that q theta is\nto be equal to x transpose v",
    "start": "3267320",
    "end": "3273100"
  },
  {
    "text": "for some v in dimension. I think this dimension is n.",
    "start": "3273100",
    "end": "3281059"
  },
  {
    "text": "And then x theta\nneeds to equals to y. So this is the KKT condition\nfor this kind of program.",
    "start": "3281060",
    "end": "3287610"
  },
  {
    "text": "And one thing you can do is\nyou can just look up a book and just invoke a theorem from\na book which says that this is",
    "start": "3287610",
    "end": "3295710"
  },
  {
    "text": "KKT-- the optimality condition. The way I think about\nit is the following, if you're interested in it.",
    "start": "3295710",
    "end": "3301660"
  },
  {
    "text": "So the way I remember this\nis that I remember this,",
    "start": "3301660",
    "end": "3307539"
  },
  {
    "text": "or I derive this every time\nif I need it, as follows.",
    "start": "3307540",
    "end": "3315030"
  },
  {
    "text": "So I think the insight is\nthat optimality at least",
    "start": "3315030",
    "end": "3320120"
  },
  {
    "text": "means that there is\nno first order update. There is no first order\nlocal improvement.",
    "start": "3320120",
    "end": "3328900"
  },
  {
    "text": "So if you perturb\nyour solution, you shouldn't have a first\norder improvement locally. If your perturb solution\nlocally a little bit",
    "start": "3328900",
    "end": "3335700"
  },
  {
    "text": "by a infinitesimally\nsmall amount, you shouldn't get a bound to\nyour first order improvement.",
    "start": "3335700",
    "end": "3342562"
  },
  {
    "text": " But you also have to satisfy\nthe constraint, so you also--",
    "start": "3342562",
    "end": "3349650"
  },
  {
    "text": "no further other\nlocal improvements satisfying the constraint.",
    "start": "3349650",
    "end": "3356549"
  },
  {
    "text": "Satisfy the constraint\nalso up to first order because you may not be able to.",
    "start": "3356550",
    "end": "3363680"
  },
  {
    "text": "So what does this\nmean in this case? It means that suppose you\nconsider the perturbation the alpha theta.",
    "start": "3363680",
    "end": "3369560"
  },
  {
    "text": "This is a perturbation. So how do you satisfy\nthe constraint? To satisfy the\nconstraint, you have",
    "start": "3369560",
    "end": "3375940"
  },
  {
    "text": "to say that the\nperturbation needs to be orthogonal to the lowest\nspan of x because if it's not",
    "start": "3375940",
    "end": "3384210"
  },
  {
    "text": "in the lowest span\nof x, you perturb it, you may change the x theta and\nthen you change the-- you don't",
    "start": "3384210",
    "end": "3390058"
  },
  {
    "text": "satisfy the constraint anymore. So this is the way to satisfy\nconstraint so that x 0 theta",
    "start": "3390058",
    "end": "3395570"
  },
  {
    "text": "is 0. That's how you make\nthe constraint work. And now let's look at\ntheta plus the other theta,",
    "start": "3395570",
    "end": "3401480"
  },
  {
    "text": "the local perturbation. And so this still\nsatisfies the constraint, and let's see what's the value.",
    "start": "3401480",
    "end": "3406910"
  },
  {
    "text": " So let's see what's\nthe value of q.",
    "start": "3406910",
    "end": "3413400"
  },
  {
    "text": "So q theta plus the other theta. This is equals to, up\nto the first order,",
    "start": "3413400",
    "end": "3419190"
  },
  {
    "text": "q theta plus the other theta. [AUDIO OUT]",
    "start": "3419190",
    "end": "3425412"
  },
  {
    "start": "3425412",
    "end": "3515510"
  },
  {
    "text": "We cannot hear you.  Maybe let's try this.",
    "start": "3515510",
    "end": "3520980"
  },
  {
    "text": "Can you hear me now? Thanks for letting me know. Yes, sir. Thanks.",
    "start": "3520980",
    "end": "3527250"
  },
  {
    "text": "Is the audio good or not? Is it OK? ",
    "start": "3527250",
    "end": "3532665"
  },
  {
    "text": "Yeah, it's OK. OK. So I'm using my\nlaptop's microphone, so maybe let me turn it in some\nway so that it works better.",
    "start": "3532665",
    "end": "3540609"
  },
  {
    "text": "Yeah. Thanks for letting me know. So maybe I'll rewind\na little bit back. I don't know for how long\ntime you have lost me.",
    "start": "3540610",
    "end": "3547380"
  },
  {
    "text": " So I guess maybe I'll just\nbriefly go through the steps",
    "start": "3547380",
    "end": "3554030"
  },
  {
    "text": "that we have discussed. So I guess I was saying that if\nyou have the perturbation, you",
    "start": "3554030",
    "end": "3564625"
  },
  {
    "text": "always satisfy the constraint\nbecause the perturbation is the lowest span of x. It's orthogonal to the lowest\nspan of x that you always",
    "start": "3564625",
    "end": "3570990"
  },
  {
    "text": "satisfy the constraint. And we want to understand-- we want to figure out\nunder what condition",
    "start": "3570990",
    "end": "3577109"
  },
  {
    "text": "this perturbation will never\nimprove your function-- will make the function bigger. Because if it makes\nthe function bigger,",
    "start": "3577110",
    "end": "3583635"
  },
  {
    "text": "it means that its\npoint is not optimal. So that's why you look at the\nTaylor expansion of this q",
    "start": "3583635",
    "end": "3591359"
  },
  {
    "text": "and you found out the first\norder changes is this term and you want this\nterm to be always",
    "start": "3591360",
    "end": "3598020"
  },
  {
    "text": "non-negative-- nonpositive\nbecause if it's positive, then it violates the\noptimality assumption.",
    "start": "3598020",
    "end": "3605880"
  },
  {
    "text": "So it's a necessary condition\nis that this term is always nonpositive, but this\nterm is very easy to make sign flip because we\ncan use the flip the other theta",
    "start": "3605880",
    "end": "3614960"
  },
  {
    "text": "by whatever sign you want. So that basically means\nthat for every theta in the orthogonal space of\nlowest span of x, this term",
    "start": "3614960",
    "end": "3622620"
  },
  {
    "text": "has to be just literally\n0 because if it's not 0, you can flip the other\ntheta to make it positive.",
    "start": "3622620",
    "end": "3628540"
  },
  {
    "text": "So that's why we are\nsaying that here, for every delta theta that\nis orthogonal to the lowest",
    "start": "3628540",
    "end": "3634365"
  },
  {
    "text": "span of x, this term is 0. And that really just\nmeans that this vector--",
    "start": "3634365",
    "end": "3640990"
  },
  {
    "text": "so because every delta theta\nintegrals in this subspace is orthogonal to\nthis vector, that",
    "start": "3640990",
    "end": "3646350"
  },
  {
    "text": "means that this vector is\nin a complementary subspace of the subspace 0 theta.",
    "start": "3646350",
    "end": "3652590"
  },
  {
    "text": "So that's why this\nvector q theta needs to be in the row span of x so\nthat for every vector delta",
    "start": "3652590",
    "end": "3664710"
  },
  {
    "text": "theta orthogonal\nto the lowest span, their inner product is zero. So that's why this,\nit can be written",
    "start": "3664710",
    "end": "3671369"
  },
  {
    "text": "as x transpose times\nmu because x transpose",
    "start": "3671370",
    "end": "3677020"
  },
  {
    "text": "will be the lowest span. x transpose mu-- x transpose--",
    "start": "3677020",
    "end": "3682210"
  },
  {
    "text": "I think that's called\nv. x transpose v is",
    "start": "3682210",
    "end": "3688180"
  },
  {
    "text": "the representation of a vector\nin the lowest span of x. So that's why this is the--",
    "start": "3688180",
    "end": "3695335"
  },
  {
    "text": "that's how we develop\nthe KKT condition. The KKT condition was that you\nhave to be in the lowest span.",
    "start": "3695335",
    "end": "3700539"
  },
  {
    "text": "The gradient of q\nas theta has to be the lowest span of x and also\nhas to be a feasible solution.",
    "start": "3700540",
    "end": "3707320"
  },
  {
    "text": "OK. Cool. So this is some digression\nabout KKT conditions. If you're not familiar with it,\nthen the only important thing",
    "start": "3707320",
    "end": "3714940"
  },
  {
    "text": "is that this is the\ncharacterization of the optimal solution i theta.",
    "start": "3714940",
    "end": "3720550"
  },
  {
    "text": "And we can-- now it's just\npattern matching, right? So this corresponds\nto this, obviously.",
    "start": "3720550",
    "end": "3730390"
  },
  {
    "text": "And this one, really, this\ncorresponds to equation two because equation two--",
    "start": "3730390",
    "end": "3737940"
  },
  {
    "text": "OK. That's what I'm-- OK,\nIt's not trivial yet, but let's see that. ",
    "start": "3737940",
    "end": "3746020"
  },
  {
    "text": "So KKT tells you that\nthe gradient of q theta needs to be something\nlike x transpose v",
    "start": "3746020",
    "end": "3753039"
  },
  {
    "text": "and the invariance or\nthe differential equation tells us that--",
    "start": "3753040",
    "end": "3758480"
  },
  {
    "text": "let me just copy paste it. Let me just rewrite it. So theta alpha is equal\nto 2 alpha squared sinh.",
    "start": "3758480",
    "end": "3769350"
  },
  {
    "start": "3769350",
    "end": "3777840"
  },
  {
    "text": "So first of all, let's\nrewrite this as-- so simplify this and rewrite it\nas v times v prime, let's say.",
    "start": "3777840",
    "end": "3789280"
  },
  {
    "text": "Because what v is\ndoesn't matter. ",
    "start": "3789280",
    "end": "3796920"
  },
  {
    "text": "And then you can-- ",
    "start": "3796920",
    "end": "3804130"
  },
  {
    "text": "I guess, let's also work\non the Q side of things. The other Q side you\ncan compute this.",
    "start": "3804130",
    "end": "3809799"
  },
  {
    "text": "And sometimes when you derive\nthe Q, we are verifying it, but actually what you have\nto do is to reverse engineer",
    "start": "3809800",
    "end": "3816880"
  },
  {
    "text": "to do this in other direction. But if you just\nverify that, suppose you are given a Q for\nthis one to prove it,",
    "start": "3816880",
    "end": "3823210"
  },
  {
    "text": "you can find the derivative\nof Q will be just arcsinh 1",
    "start": "3823210",
    "end": "3830000"
  },
  {
    "text": "over 2 alpha\nsquared times theta. This is a derivative\nQ. It makes sense",
    "start": "3830000",
    "end": "3836620"
  },
  {
    "text": "because a Q is a sum of\nsome function of theta i. And the derivative of Q is a--",
    "start": "3836620",
    "end": "3842890"
  },
  {
    "text": "each answer is the sum\nfunction of theta i. And so then you can see that\nthis, if you plug in the theta",
    "start": "3842890",
    "end": "3852140"
  },
  {
    "text": "alpha here to this thing. So arg sinh r\nsquared theta alpha.",
    "start": "3852140",
    "end": "3860930"
  },
  {
    "text": "This is equal to just\nthe 4x transpose v prime.",
    "start": "3860930",
    "end": "3865977"
  },
  {
    "start": "3865977",
    "end": "3872180"
  },
  {
    "text": "So basically that's\nwhy gradient q theta alpha is equal to minus\n4x transpose v prime.",
    "start": "3872180",
    "end": "3879769"
  },
  {
    "text": "And this satisfies\nthe KKT condition. The form doesn't matter\nbecause v can be any vector,",
    "start": "3879770",
    "end": "3886190"
  },
  {
    "text": "so that's why q alpha\nsatisfies the KKT condition.",
    "start": "3886190",
    "end": "3892495"
  },
  {
    "text": " Let me grab that.",
    "start": "3892496",
    "end": "3898000"
  },
  {
    "text": "So it's the global mean. It's the global mean. ",
    "start": "3898000",
    "end": "3904030"
  },
  {
    "text": "I guess it's the last step. Satisfying the KKT\ncondition means global mean.",
    "start": "3904030",
    "end": "3909190"
  },
  {
    "text": "This requires the\nconvexity of this program. The constraint is linear. It's convex.",
    "start": "3909190",
    "end": "3914230"
  },
  {
    "text": "The objective you\ncan verify still. It's also complex.",
    "start": "3914230",
    "end": "3920125"
  },
  {
    "text": "It's something between\nL1 norm and L2. Both of them are convex. ",
    "start": "3920125",
    "end": "3934289"
  },
  {
    "text": "Any questions? ",
    "start": "3934290",
    "end": "3957448"
  },
  {
    "text": "So if there's no\nquestions, I'm going to move on to the next thing,\nwhich is about classification problem.",
    "start": "3957448",
    "end": "3962590"
  },
  {
    "start": "3962590",
    "end": "3968880"
  },
  {
    "text": "Yeah? I see many of you are\nstarting like this. Proof, still. Like, this proof--",
    "start": "3968880",
    "end": "3975668"
  },
  {
    "text": "I don't know. I don't have-- the plan\nsounds very intuitive. So how do you prove something is\nthe minimizer of a optimization",
    "start": "3975668",
    "end": "3983810"
  },
  {
    "text": "proof? You have to verify it satisfies\nthe KKT condition, I guess. That's probably more\nor less the only way",
    "start": "3983810",
    "end": "3990789"
  },
  {
    "text": "to do it if you want\nto show something as the optimizer of some\noptimization program.",
    "start": "3990790",
    "end": "3996630"
  },
  {
    "text": "But it's kind of magical\nwhy it just happens to satisfy the KKT condition.",
    "start": "3996630",
    "end": "4001760"
  },
  {
    "text": "So of course, there's\nsomething that we can choose. We can choose the q to make\nit satisfy the KKT condition.",
    "start": "4001760",
    "end": "4007147"
  },
  {
    "text": "That's something you can choose. But the magical thing\nis that other things all match up, like the form, the\nx transpose times something.",
    "start": "4007147",
    "end": "4013730"
  },
  {
    "text": "All of those things\nare all matched up, and also you can somewhat--\nin some sense, you can always",
    "start": "4013730",
    "end": "4019040"
  },
  {
    "text": "work with each\ncoordinate independently in this special case. See, that's also\nsomething that's",
    "start": "4019040",
    "end": "4025819"
  },
  {
    "text": "maybe a little bit special\nto this special model that we consider. ",
    "start": "4025820",
    "end": "4032520"
  },
  {
    "text": "All right. OK, so now let's move on\nclassification problem, and we are looking\nat separable data",
    "start": "4032520",
    "end": "4039110"
  },
  {
    "text": "as we always do for\nclassification problem. And here we are\ngoing to only discuss",
    "start": "4039110",
    "end": "4045530"
  },
  {
    "text": "one result, which says that\nyou could do gradient descent.",
    "start": "4045530",
    "end": "4050690"
  },
  {
    "text": "It converges to a\nmax margin solution. And this is actually--",
    "start": "4050690",
    "end": "4055940"
  },
  {
    "text": " doesn't require\nany initialization.",
    "start": "4055940",
    "end": "4063960"
  },
  {
    "text": "It works for any initialization. So the only thing you need is\ngradient descent and some loss",
    "start": "4063960",
    "end": "4069720"
  },
  {
    "text": "functions, which [INAUDIBLE]. And no regularization, you\njust compute gradient descent on the loss function. You run for a long time.",
    "start": "4069720",
    "end": "4075533"
  },
  {
    "text": "You're going to converge\nto the max margin solution.  So I'm going to have to,\nagain, start with a setup.",
    "start": "4075533",
    "end": "4084760"
  },
  {
    "text": "So now we have a data set xi\nyi i from 1 to n and xi ERd.",
    "start": "4084760",
    "end": "4095180"
  },
  {
    "text": " And yi is a binary\nlabel, plus 1 minus 1.",
    "start": "4095180",
    "end": "4103605"
  },
  {
    "text": "Question about the-- Sure. [INAUDIBLE] OK. So instead of assuming the\nbottom to be w squared--",
    "start": "4103605",
    "end": "4114810"
  },
  {
    "text": "OK. [INAUDIBLE] Mm-hmm. OK. Then to address the proof,\nthere's not guide proof",
    "start": "4114810",
    "end": "4122509"
  },
  {
    "text": "because it breaks\ndown that-- like, when you compute the\ndelta [INAUDIBLE]?? ",
    "start": "4122510",
    "end": "4128880"
  },
  {
    "text": "OK. Yeah. So the question is about\nthe previous thing, and the question is about\nif you don't use w squared,",
    "start": "4128880",
    "end": "4134778"
  },
  {
    "text": "you use w to the k. And this is a very good\nquestion, actually. This is exactly what\nthe paper studied",
    "start": "4134779",
    "end": "4139969"
  },
  {
    "text": "in the more technical part. And the short answer is\nthat everything can still",
    "start": "4139970",
    "end": "4145880"
  },
  {
    "text": "go through, but the eventual\nq would be different.",
    "start": "4145880",
    "end": "4150989"
  },
  {
    "text": "So the form of your q would\nbe something not L1, L2. I think it's something like--",
    "start": "4150990",
    "end": "4156500"
  },
  {
    "text": "it depends on the power. So I think if the power is\nP, I don't exactly remember, but I think it's something\nlike 1 over P norm",
    "start": "4156500",
    "end": "4163160"
  },
  {
    "text": "when alpha is close to zero. When alpha is going\nto infinity, I think everything\nis still the same.",
    "start": "4163160",
    "end": "4170339"
  },
  {
    "text": "The NTK regime is not\nsensitive to this. And then technically, why'd\neverything go through?",
    "start": "4170340",
    "end": "4177299"
  },
  {
    "text": "I think the reason why\neverything goes through is that, roughly\nspeaking, you are only",
    "start": "4177300",
    "end": "4187040"
  },
  {
    "text": "playing with this single\ndimensional function in some sense, right?",
    "start": "4187040",
    "end": "4192920"
  },
  {
    "text": "So it won't be sinh\nanymore, probably. There will be some constant,\nsome other function.",
    "start": "4192920",
    "end": "4199909"
  },
  {
    "text": "But still this x transpose\nsomething is still there, I think. It's not changed.",
    "start": "4199910",
    "end": "4205219"
  },
  {
    "text": "And so eventually you just\nhave to add in your different Q to make everything work.",
    "start": "4205220",
    "end": "4212210"
  },
  {
    "text": "And the Q is still-- only depends on the coordinates. You do something on each\ncorner, you take the sum,",
    "start": "4212210",
    "end": "4218989"
  },
  {
    "text": "but Q still had this form. So that's why it's\nstill somewhat doable. ",
    "start": "4218990",
    "end": "4227120"
  },
  {
    "text": "OK. Cool. So going back to the\nclassification problem. So this is our\nsetup, and here we",
    "start": "4227120",
    "end": "4233560"
  },
  {
    "text": "are only going to do the\nlinear model even though some of this theory still\nworks for nonlinear model",
    "start": "4233560",
    "end": "4239250"
  },
  {
    "text": "with roughly similar technique\nand similar conclusion.",
    "start": "4239250",
    "end": "4244980"
  },
  {
    "text": "And here we're going to\nhave a loss function. So the loss function\nwill be L hat w is the--",
    "start": "4244980",
    "end": "4257545"
  },
  {
    "text": "let's say we do\nthe cross entropy loss, the logistic loss.",
    "start": "4257545",
    "end": "4263605"
  },
  {
    "text": "I mean, cross entropy loss. ",
    "start": "4263605",
    "end": "4268730"
  },
  {
    "text": "Times hw xi. ",
    "start": "4268730",
    "end": "4280910"
  },
  {
    "text": "Where this loss is this\nlogistic loss, which is log of 1 plus exponential minus 1.",
    "start": "4280910",
    "end": "4289465"
  },
  {
    "text": " OK. Cool. ",
    "start": "4289465",
    "end": "4297560"
  },
  {
    "text": "And the first thing is\nthat to get some intuition. So first of all, we have\nmultiple global mean",
    "start": "4297560",
    "end": "4308130"
  },
  {
    "text": "if separable data.  So this is a premises for any\nimplicit regularization buffer.",
    "start": "4308130",
    "end": "4316650"
  },
  {
    "text": "If you don't have\none global mean and you can converge\nthe global mean, there's no implicit\nregularization buffer.",
    "start": "4316650",
    "end": "4322730"
  },
  {
    "text": "But why is there are\nmultiple global mean? This is just because\nyou can always",
    "start": "4322730",
    "end": "4328890"
  },
  {
    "text": "have an infinite number of\nseparators, pretty much. Unless in a very extreme\ncase you just happen to get",
    "start": "4328890",
    "end": "4335100"
  },
  {
    "text": "stuck at it exactly. So for example, I think\nit's probably easier to draw something. So suppose you have some\ndata points like this",
    "start": "4335100",
    "end": "4342660"
  },
  {
    "text": "and you have so many\ndifferent possible separators. As long as you have one,\nyou perturb a little bit,",
    "start": "4342660",
    "end": "4349240"
  },
  {
    "text": "it's still separate. So there's this, the infinite\nmany w such that w transpose xi",
    "start": "4349240",
    "end": "4364360"
  },
  {
    "text": "yi is bigger than\n0 for every arc. So you have so many separators,\nand for every w for--",
    "start": "4364360",
    "end": "4373820"
  },
  {
    "text": "maybe let's say\ninfinite number of w bar such that where w bar is unit.",
    "start": "4373820",
    "end": "4379570"
  },
  {
    "text": "w bar is unit vector. This statement doesn't\nreally depend on the log,",
    "start": "4379570",
    "end": "4384920"
  },
  {
    "text": "so you can always scale it. So for every w bar-- for any w bar such that\nthis, you can scale it.",
    "start": "4384920",
    "end": "4392219"
  },
  {
    "text": "So if you look at\nL hat alpha w bar",
    "start": "4392220",
    "end": "4399410"
  },
  {
    "text": "will go to 0 as alpha\ngoes to infinity. So any scaling of\nthis unit separator,",
    "start": "4399410",
    "end": "4408260"
  },
  {
    "text": "if you scale it\nextremely, then you are going to get\na loss close to 0.",
    "start": "4408260",
    "end": "4413960"
  },
  {
    "text": "So basically you have so\nmany directions in it, so you can go to infinity\nin different directions",
    "start": "4413960",
    "end": "4419210"
  },
  {
    "text": "and still converge\nto a zero loss. So basically, in\nsome sense, if you",
    "start": "4419210",
    "end": "4426140"
  },
  {
    "text": "are a little sloppy about\nall of this infinity times w bar are global minimum\nof this loss function",
    "start": "4426140",
    "end": "4435660"
  },
  {
    "text": "just because the loss function\ngoes to zero at infinity. So the loss function-- maybe\nI should also draw this.",
    "start": "4435660",
    "end": "4441450"
  },
  {
    "text": "The loss function\nlooks like this. This is the Lt. When t goes to\ninfinity, you get close to 0",
    "start": "4441450",
    "end": "4449270"
  },
  {
    "text": "because the zero loss. And what's inside-- what's t?\nt is y times w transpose xi,",
    "start": "4449270",
    "end": "4456769"
  },
  {
    "text": "and this thing will go\nto infinity as you scale the norm of the constraint.",
    "start": "4456770",
    "end": "4464620"
  },
  {
    "text": "So you have so many\ndirections that you can find. There are so many\nglobal minimums. The question is which\ndirection you'll find.",
    "start": "4464620",
    "end": "4477150"
  },
  {
    "text": " If you don't use any--",
    "start": "4477150",
    "end": "4482950"
  },
  {
    "text": "if you just invoke a\ntheorem about opposition, you know that it will find\na solution with error close",
    "start": "4482950",
    "end": "4489099"
  },
  {
    "text": "to zero-- with loss close to zero, but\nyou don't know which direction it is. You still have a bunch\nof flexibilities there.",
    "start": "4489100",
    "end": "4496070"
  },
  {
    "text": "Many directions can-- if you go\nto infinity in many directions, you can get the loss back to 0.",
    "start": "4496070",
    "end": "4503760"
  },
  {
    "text": "So that's the question\nthat we're actually going to address.  And we're going to say\nthat this actually converge",
    "start": "4503760",
    "end": "4510580"
  },
  {
    "text": "to max margin solution. So let's define-- so the\nanswer is max margin solution.",
    "start": "4510580",
    "end": "4516300"
  },
  {
    "start": "4516300",
    "end": "4521540"
  },
  {
    "text": "Direction. So I guess let's define, maybe\nfirst, the marginalized--",
    "start": "4521540",
    "end": "4530080"
  },
  {
    "text": "the margin and\nnormalized margin. So I guess we have\ndefined a margin.",
    "start": "4530080",
    "end": "4537980"
  },
  {
    "text": "And this cross many cases\nwhere-- in many cases, the margin is the minimum, this.",
    "start": "4537980",
    "end": "4544030"
  },
  {
    "text": " And we also assume--",
    "start": "4544030",
    "end": "4549860"
  },
  {
    "text": "we always assume\nlinearly separable.",
    "start": "4549860",
    "end": "4556500"
  },
  {
    "text": " So this definition is\nonly defined for cases",
    "start": "4556500",
    "end": "4563380"
  },
  {
    "text": "where it's linearly separable. And normalized margin\nis defined to be--",
    "start": "4563380",
    "end": "4572560"
  },
  {
    "text": "we normalize this\nby the norm of w because otherwise you can make\nthe norm arbitrarily big--",
    "start": "4572560",
    "end": "4583860"
  },
  {
    "text": "arbitrarily small. OK? So max margin solution is\ndefined to be for all w.",
    "start": "4583860",
    "end": "4600230"
  },
  {
    "text": "Which one give you the\nmaximum normalized margin?",
    "start": "4600230",
    "end": "4605444"
  },
  {
    "text": "And let w star be the maximizer.",
    "start": "4605444",
    "end": "4612210"
  },
  {
    "text": " This is the direction of\nthe max margin solution",
    "start": "4612210",
    "end": "4618310"
  },
  {
    "text": "and with unit norm. Because if you only\nlook at this objective,",
    "start": "4618310",
    "end": "4625070"
  },
  {
    "text": "it doesn't depend on the scale\nbecause the scale has already normalized all. So we define w start to\nmaximizer of every single nor,",
    "start": "4625070",
    "end": "4633346"
  },
  {
    "text": "OK?. So basically we're\ngoing to prove that if you do gradient descent,\nyou're going to go to infinity.",
    "start": "4633346",
    "end": "4638790"
  },
  {
    "text": "But each will go to\ninfinity, but only along the direction of w star.",
    "start": "4638790",
    "end": "4643860"
  },
  {
    "text": "That's the theorem. ",
    "start": "4643860",
    "end": "4650200"
  },
  {
    "text": "So gradient flow.  I guess here we're talking\nabout gradient flow",
    "start": "4650200",
    "end": "4657400"
  },
  {
    "text": "just because it's\nconvenient, as we discussed. So converges to the direction\nof max margin solutions",
    "start": "4657400",
    "end": "4675680"
  },
  {
    "text": "in the sense that I think\nwe don't really exactly see the convergence in\ndirection, we only",
    "start": "4675680",
    "end": "4682100"
  },
  {
    "text": "see the convergence in the sines\nof the value of the margins.",
    "start": "4682100",
    "end": "4688470"
  },
  {
    "text": "I think you really want to\ndo the exact convergence and direction, of course it\nwill be a little more work.",
    "start": "4688470",
    "end": "4693670"
  },
  {
    "text": "So what we say is that\nthe margin of your iterate will converge to the maximum\npossible margin, gamma bar.",
    "start": "4693670",
    "end": "4699390"
  },
  {
    "text": " Right.",
    "start": "4699390",
    "end": "4704450"
  },
  {
    "text": "So as t goes to infinity. ",
    "start": "4704450",
    "end": "4710990"
  },
  {
    "text": "And wt is the iterate at time t. ",
    "start": "4710990",
    "end": "4727119"
  },
  {
    "text": "So in the next five\nminutes, I'm going to discuss a little bit about\nintuition, why this is working.",
    "start": "4727120",
    "end": "4733630"
  },
  {
    "text": "This intuition against\nwhy this is working and how do we improve it, and\nsome kind of a mixture of both of these two.",
    "start": "4733630",
    "end": "4738850"
  },
  {
    "text": "And then in the next\nlecture, I guess I would prove the\nthing more rigorously.",
    "start": "4738850",
    "end": "4744140"
  },
  {
    "text": "So why this is going-- why this is working? So the intuition is that-- so I guess I have\na few steps here.",
    "start": "4744140",
    "end": "4752480"
  },
  {
    "text": "So step one, this loss\nfunction, L hat wt,",
    "start": "4752480",
    "end": "4759290"
  },
  {
    "text": "is going to 0 by standard\noptimization arguments, which",
    "start": "4759290",
    "end": "4767930"
  },
  {
    "text": "is not covered by this course. But I think you can believe\nit if your optimization is working--",
    "start": "4767930",
    "end": "4774875"
  },
  {
    "text": "if your optimization is working,\nthen your loss should go to 0.",
    "start": "4774875",
    "end": "4780090"
  },
  {
    "text": "And second. So this is observation one.",
    "start": "4780090",
    "end": "4788430"
  },
  {
    "text": "And observation two, I guess\nthe loss, this loss function,",
    "start": "4788430",
    "end": "4799300"
  },
  {
    "text": "which we defined to be\nthe logistic loss, right? Some like this. This loss function is\nactually close to exponential",
    "start": "4799300",
    "end": "4812000"
  },
  {
    "text": "for large C. This\nis just because you",
    "start": "4812000",
    "end": "4818410"
  },
  {
    "text": "do Taylor expansion. Log of 1 plus x is\napproximately x.",
    "start": "4818410",
    "end": "4823488"
  },
  {
    "text": "That's why you can get\nrid of the log at 1. ",
    "start": "4823488",
    "end": "4831179"
  },
  {
    "text": "And this is actually\nan interesting thing. So you call it logistic\nloss, but actually it's closer to exponential loss. ",
    "start": "4831180",
    "end": "4840370"
  },
  {
    "text": "So logistic loss is close\nto exponential loss. ",
    "start": "4840370",
    "end": "4850363"
  },
  {
    "text": "So most of the\nproof, actually, we are only going to do\nthe logistic loss. I think the proof,\nactually, I'm just-- sorry.",
    "start": "4850363",
    "end": "4856028"
  },
  {
    "text": "I'm going to do the\nexponential loss. So in the proof, I'm going to\njust assume it's exponential. Even though you can still--\nthe small differences",
    "start": "4856028",
    "end": "4863210"
  },
  {
    "text": "can be dealt with\nrelatively easily. But the third observation is\nthat because of one, the wt",
    "start": "4863210",
    "end": "4876180"
  },
  {
    "text": "has to go-- the norm has to go to infinity. And the reason is that if you\njust don't go to infinity,",
    "start": "4876180",
    "end": "4882360"
  },
  {
    "text": "you never make the loss\nclose to zero now, right? This is just because if wt is\nbounded, however, let's say,",
    "start": "4882360",
    "end": "4891920"
  },
  {
    "text": "it's bounded by B. Suppose it's always bounded. Then you can always\nbounds these y times",
    "start": "4891920",
    "end": "4900280"
  },
  {
    "text": "w transpose xi by something\nlike AB times the norm of xi.",
    "start": "4900280",
    "end": "4906309"
  },
  {
    "text": "You have some bound. So this is bounded. And then your loss,\nL hat wt, this",
    "start": "4906310",
    "end": "4914640"
  },
  {
    "text": "is bounded by exponential\nminus d times xi.",
    "start": "4914640",
    "end": "4924010"
  },
  {
    "text": "Something like this. And this is bounded\nbelow by zero. And this contradicts with what?",
    "start": "4924010",
    "end": "4929790"
  },
  {
    "start": "4929790",
    "end": "4935460"
  },
  {
    "text": "Right. So if your norm\nis always bounded, then your loss is going\nto be low by some number. The number is very\nclose to zero,",
    "start": "4935460",
    "end": "4941660"
  },
  {
    "text": "but still is bounded by some\nnumber which contradicts with the convergence to zero. ",
    "start": "4941660",
    "end": "4948980"
  },
  {
    "text": "So now it comes to the\nmost important thing. So with all of\nthis preparation--",
    "start": "4948980",
    "end": "4954740"
  },
  {
    "text": "so we know that the norm goes\nto infinity, and then suppose, let's say, let's only\nlook at the final case,",
    "start": "4954740",
    "end": "4961160"
  },
  {
    "text": "the later regime\nwhere wt is very big. So suppose wt q\nnorm is really big.",
    "start": "4961160",
    "end": "4969380"
  },
  {
    "text": "That's called q. It's very big. ",
    "start": "4969380",
    "end": "4979950"
  },
  {
    "text": "Then let's try to\nsimplify the loss function and see where the\nloss functions are. So the loss function I can--",
    "start": "4979950",
    "end": "4987150"
  },
  {
    "text": "maybe let's remove the\nt just for simplicity. Let's just look at-- suppose you want to\nlook at some w such",
    "start": "4987150",
    "end": "4992610"
  },
  {
    "text": "that the w norm is very big. So L hat w is the sum\nof this logistic loss",
    "start": "4992610",
    "end": "5000640"
  },
  {
    "text": "or exponential loss. We're not distinguishing\nthem for now. ",
    "start": "5000640",
    "end": "5013030"
  },
  {
    "text": "Let's say this is roughly\nequals to the exponential minus yi times w transpose xi.",
    "start": "5013030",
    "end": "5020320"
  },
  {
    "text": " And because the loss would\nbe very close to zero,",
    "start": "5020320",
    "end": "5026020"
  },
  {
    "text": "it's actually more informative\nto look at the log space. So if we took a log\nspace of L hat w,",
    "start": "5026020",
    "end": "5033190"
  },
  {
    "text": "then this is roughly\nequal to the log of sum of exponential minus\nyi times w transpose times xi.",
    "start": "5033190",
    "end": "5046190"
  },
  {
    "text": "So this is a log\nsum exponential. I'm not sure whether this\nrung a bell to some of you, so this is basically soft max.",
    "start": "5046190",
    "end": "5054030"
  },
  {
    "text": "So I'm going to claim that this\nlog exponential is close to max",
    "start": "5054030",
    "end": "5060559"
  },
  {
    "text": "of this minus this.",
    "start": "5060560",
    "end": "5069190"
  },
  {
    "text": " Am I-- yes.",
    "start": "5069190",
    "end": "5075690"
  },
  {
    "text": "So why this is the case? Let's do some, again,\nabstract derivation.",
    "start": "5075690",
    "end": "5081750"
  },
  {
    "text": "I guess I'm running late. So if you have log-- oh, I guess-- sorry.",
    "start": "5081750",
    "end": "5087790"
  },
  {
    "text": "I think I'm-- probably I\nshould have another step, so let's first another step.",
    "start": "5087790",
    "end": "5093210"
  },
  {
    "text": " So this is log sum exponential,\nbut also I can try to--",
    "start": "5093210",
    "end": "5099530"
  },
  {
    "text": "I want to use the fact\nthat w has a large norm. So let's get a\nnormal wq in front.",
    "start": "5099530",
    "end": "5106120"
  },
  {
    "text": "Again, yi times w\nbar transpose xi where w bar is equal\nto normalization w.",
    "start": "5106120",
    "end": "5114640"
  },
  {
    "text": "So I know I'm going to claim\nthat this is close to max minus qyi w transpose xi i over.",
    "start": "5114640",
    "end": "5125510"
  },
  {
    "text": "So why this is\nthe case is this-- I guess for those who\nare familiar with this, log sum exponential is\nkind of like a soft max.",
    "start": "5125510",
    "end": "5133100"
  },
  {
    "text": "So if you look at log\nsum exponential of sum,",
    "start": "5133100",
    "end": "5138270"
  },
  {
    "text": "I said q times ui. I'm trying to kind of abstract\nit by a little bit, right?",
    "start": "5138270",
    "end": "5146520"
  },
  {
    "text": "So you have a q\nthat is very large and the ui is something fixed. I claim that this is close.",
    "start": "5146520",
    "end": "5152170"
  },
  {
    "text": "This is roughly q\ntimes the max of ui iEn",
    "start": "5152170",
    "end": "5159719"
  },
  {
    "text": "plus something like little q. Something that doesn't impact-- doesn't depend on q as\nmuch as q goes to infinity.",
    "start": "5159720",
    "end": "5167699"
  },
  {
    "text": "So when the q is very big, then\nthis is really doing the max. And sometimes this is\nkind of like when you do a temperature in a soft max.",
    "start": "5167700",
    "end": "5174329"
  },
  {
    "text": "If you make it big, then\nsoft max becomes hard max. And if you do the--",
    "start": "5174330",
    "end": "5180820"
  },
  {
    "text": "then this is really-- and sometimes soft max.  And if you want how\nto improve this,",
    "start": "5180820",
    "end": "5188170"
  },
  {
    "text": "just say the sum of exponential\nqui is proven upper bound.",
    "start": "5188170",
    "end": "5194020"
  },
  {
    "text": "The upper bound is\nthat it's a log of-- replace each of these\nby the biggest one.",
    "start": "5194020",
    "end": "5202000"
  },
  {
    "text": "Exponential q times the max ui. And this is only log plus\nq times the max of ui.",
    "start": "5202000",
    "end": "5216159"
  },
  {
    "text": "And so the log is\nsmall compared to q because q will go to\ninfinity and I need something",
    "start": "5216160",
    "end": "5222219"
  },
  {
    "text": "fixed in this abstraction. And on the other hand,\nyou just take one term.",
    "start": "5222220",
    "end": "5227440"
  },
  {
    "text": "We just only keep the term\nwhere you have the max, then you get q max ui. You drop all the other terms.",
    "start": "5227440",
    "end": "5233600"
  },
  {
    "text": "There's no sum anymore. Log cancels to be exponential. You get it.",
    "start": "5233600",
    "end": "5239350"
  },
  {
    "text": "So basically, this\nlog sum exponential is close to the max up\nto some factor log n.",
    "start": "5239350",
    "end": "5247010"
  },
  {
    "text": "But this factor log n will be\nsmall if q goes to infinity,",
    "start": "5247010",
    "end": "5252090"
  },
  {
    "text": "and that justifies this step. So unless you have\nthis step, this--",
    "start": "5252090",
    "end": "5260230"
  },
  {
    "text": "what's going off here. So if you think that you're\nminimizing the loss-- you're minimizing\na loss, so that's",
    "start": "5260230",
    "end": "5266640"
  },
  {
    "text": "why you minimize a\nlog loss as well. So minimizing loss is kind of\nlike trying to maximize this,",
    "start": "5266640",
    "end": "5272940"
  },
  {
    "text": "and that means you\nare maximizing the-- so you are trying to\nminimize this quantity.",
    "start": "5272940",
    "end": "5282035"
  },
  {
    "text": " Minimize the quantity max\nminus q yi about transpose xi,",
    "start": "5282035",
    "end": "5294239"
  },
  {
    "text": "which means it's the same as\nmaximizing, which means that--",
    "start": "5294240",
    "end": "5300098"
  },
  {
    "text": "sorry.  You are minimizing what you\nare maximizing, the min of q yi",
    "start": "5300098",
    "end": "5311140"
  },
  {
    "text": "w transpose xi. ",
    "start": "5311140",
    "end": "5317475"
  },
  {
    "text": "So you just fill out this time. Minimizing the max of this is\nthe same as maximizing the min,",
    "start": "5317475",
    "end": "5324320"
  },
  {
    "text": "no? This is just literally the\nsame thing without any--",
    "start": "5324320",
    "end": "5329650"
  },
  {
    "text": "it's not like you're\nswitching min and max. It's just really the sign. You have the minus sign.",
    "start": "5329650",
    "end": "5335020"
  },
  {
    "text": "Maximize something\nis the same as-- the max of this is\nequal to the minus,",
    "start": "5335020",
    "end": "5343090"
  },
  {
    "text": "the min qy of the transpose xi.",
    "start": "5343090",
    "end": "5349320"
  },
  {
    "text": "And then you can put this\nminus also to minimize it. OK? ",
    "start": "5349320",
    "end": "5355450"
  },
  {
    "text": "All right. So basically you are\nmaximizing the margin. That's what this is. So if you do this\napproximation, then you",
    "start": "5355450",
    "end": "5362610"
  },
  {
    "text": "have maximized margin\nif q goes to infinity. And next time we are\ngoing to make this more formal with a little but--",
    "start": "5362610",
    "end": "5370230"
  },
  {
    "text": "with essentially\nthe same situation, but the proof would\nbe more clean. It's not exactly like this.",
    "start": "5370230",
    "end": "5375750"
  },
  {
    "text": "It's not like you're\ndealing with errors. It's going to be a\nvery clean proof.",
    "start": "5375750",
    "end": "5381210"
  },
  {
    "text": "OK. I think that's-- yeah. That's all for today. Thanks.",
    "start": "5381210",
    "end": "5386559"
  },
  {
    "start": "5386560",
    "end": "5391000"
  }
]