[
  {
    "start": "0",
    "end": "93000"
  },
  {
    "text": "All right, let's get started. So today, we're\nfinishing our first kind of piece of the tour of the very\nbasics of supervised learning.",
    "start": "5100",
    "end": "14700"
  },
  {
    "text": "We're going to talk about\nthese family of models that's called the\nexponential family of models. And why we care about these\nmodels, as we talked about,",
    "start": "14700",
    "end": "20699"
  },
  {
    "text": "is they're going to allow\nus to generalize basically the kinds of models that we\nwere using before to a wider",
    "start": "20699",
    "end": "26130"
  },
  {
    "text": "range of error modes, OK? Of different kinds of errors. And they'll also come back\nand play a starring role",
    "start": "26130",
    "end": "31910"
  },
  {
    "text": "when we start to tackle\nunsupervised learning, where we don't have access\nto a target variable.",
    "start": "31910",
    "end": "37980"
  },
  {
    "text": "And the underlying mechanics\nthat we'll use here will set us up quite\nnicely for that. Just in terms of\npacing, in terms",
    "start": "37980",
    "end": "43360"
  },
  {
    "text": "of the course, what happens is\nI go away for a little while, [INAUDIBLE] going\nto come in and talk to you about a bunch\nof different things--",
    "start": "43360",
    "end": "48640"
  },
  {
    "text": "kernels, SVMs,\nand deep learning. And then I'll come\nback to teach you a little bit about the\nunsupervised learning",
    "start": "48640",
    "end": "55070"
  },
  {
    "text": "piece, which is, again,\nlike a two week block where we see, from\nfirst principles,",
    "start": "55070",
    "end": "61019"
  },
  {
    "text": "how those things work. And that area, I\nhave to say just as a plug for what's coming,\nthat area is something that's",
    "start": "61020",
    "end": "67759"
  },
  {
    "text": "been really exciting,\nkind of thrilling over the last couple of years, how\nmuch we can learn without label data, or with really\nweak sources of data.",
    "start": "67760",
    "end": "74909"
  },
  {
    "text": "That's been like a revolution\nin machine learning. So hopefully I can share some\nof that excitement with you.",
    "start": "74909",
    "end": "79940"
  },
  {
    "text": "The thread is up-- the\nlecture thread is up on Ed, if you want to ask\nquestions, as usual.",
    "start": "79940",
    "end": "85070"
  },
  {
    "text": "And everything is online\nbefore the lecture. I didn't put out\na template today, because I'm going to\nhandwrite almost everything.",
    "start": "85070",
    "end": "91549"
  },
  {
    "text": "All right, so what\nare we doing today? We're going to learn about\nthese exponential family models,",
    "start": "91549",
    "end": "97060"
  },
  {
    "start": "93000",
    "end": "267000"
  },
  {
    "text": "and they're basically going\nto be what you already with slightly fancier notation. Basically we've been ramping\nup the fancier notation each time, and generalizing,\nas appropriate,",
    "start": "97060",
    "end": "104560"
  },
  {
    "text": "how we want to go through them. We'll do the definition\nand the motivation. And the definition, at first,\nwill look simultaneously",
    "start": "104560",
    "end": "111770"
  },
  {
    "text": "a little bit weird and kind of\nlike, oh, that doesn't really mean anything, it\ndoesn't have any content. And then it will\nalso look to you",
    "start": "111770",
    "end": "117520"
  },
  {
    "text": "like it's impossible to satisfy. And that's kind of true, right? So these are fairly\ninteresting objects",
    "start": "117520",
    "end": "123670"
  },
  {
    "text": "we're going to be looking at,\nbut they have a very nice kind of canonical form. We'll then do a\nbunch of examples,",
    "start": "123670",
    "end": "130849"
  },
  {
    "text": "a couple of different examples. And the notes here, the master\nnotes here, are really good. I would definitely recommend\ngoing through them,",
    "start": "130849",
    "end": "137459"
  },
  {
    "text": "the type notes,\njust that you work through a couple of details. This is something\nthat you're going to get a high-level piece\nof how we go through it.",
    "start": "137459",
    "end": "144900"
  },
  {
    "text": "Just do the calculations\nonce, and you will be convinced of like\nall the different claims that are in the lecture.",
    "start": "144900",
    "end": "150200"
  },
  {
    "text": "If you try to reason about them\nwithout doing the calculations, it just makes your life more\ndifficult than it needs to be.",
    "start": "150200",
    "end": "155239"
  },
  {
    "text": "So just go through it once. It shouldn't take\ntoo long, but I'll give you kind of a\nhigh-level tour today.",
    "start": "155239",
    "end": "161080"
  },
  {
    "text": "So we'll do that\ndefinition of motivation. We'll do a couple of examples. And then last time,\nwe were talking about this question of how do\nwe deal with multiple classes,",
    "start": "161080",
    "end": "169519"
  },
  {
    "text": "right? Last time we were talking about\nbinary classes, yes or no. Now, if we want to have\nmultiple classes out there--",
    "start": "169519",
    "end": "175020"
  },
  {
    "text": "you want to know if there's\na dog, or a pig, or a horse, or whatever in\nthere, this is called multiclass classification.",
    "start": "175020",
    "end": "180480"
  },
  {
    "text": "And we'll talk a little bit\nabout our friend softmax. What you really\nget out of this is that it will look kind of\nto you in the end like,",
    "start": "180480",
    "end": "186060"
  },
  {
    "text": "oh, OK, that seems\npretty reasonable. But you'll learn about\nsome encoding that is fairly widespread,\ncalled the one-hot encoding,",
    "start": "186060",
    "end": "192500"
  },
  {
    "text": "which you would need\npractically if you were going to actually use\nany of these kind of things. I believe also your homework\nis out, but don't quote me",
    "start": "192500",
    "end": "200640"
  },
  {
    "text": "on that. I think it's out now, and I saw\nthere were some questions ahead of class about that. Any other questions\nbefore we get started?",
    "start": "200640",
    "end": "205879"
  },
  {
    "text": "Oh, please. Just a quick question in\nregards to the homework. Oh yeah, I'm the wrong person.",
    "start": "205879",
    "end": "213989"
  },
  {
    "text": "You're free to ask. Like, I'm just\ntelling you I don't-- [INAUDIBLE]",
    "start": "213989",
    "end": "219799"
  },
  {
    "text": "Oh yeah, go ahead. Yeah. So we are blessed at Stanford\nwith many great things. We have wonderful weather.",
    "start": "219799",
    "end": "224909"
  },
  {
    "text": "We have incredible faculty. We have the best\nstudents on the planet. And we also have some great core\nsupport, and so I have no idea.",
    "start": "224909",
    "end": "234890"
  },
  {
    "text": "All right, so let's\nsee what's next. OK, so the exponential family.",
    "start": "234890",
    "end": "240510"
  },
  {
    "text": "Now I want to be clear,\nI have mixed feelings about how I present this. Because on one hand, I\nwant to convey to you",
    "start": "240510",
    "end": "247069"
  },
  {
    "text": "the unbelievable historical\nsignificance of this, and why you should this kind of\nby rights of machine learning.",
    "start": "247069",
    "end": "254709"
  },
  {
    "text": "On the other side,\nthere's an argument to be made that a lot of\nmodern machine learning is not going to use this\nformulation and this framework.",
    "start": "254709",
    "end": "262759"
  },
  {
    "text": "But if you start to\nread papers, it's canonical enough\nthat it will come up in various different places, OK?",
    "start": "262759",
    "end": "268100"
  },
  {
    "text": "So I don't want you\nthinking like, oh, this is all you can do when you\nmodel machine learning. This is, like, where\nthe field is stopped.",
    "start": "268100",
    "end": "274919"
  },
  {
    "text": "It's important historically,\nit is very nice to understand, it has a bunch of\nproperties we care about,",
    "start": "274919",
    "end": "280229"
  },
  {
    "text": "but it's not the state\nof the art, right? It's not what I would-- go home and use\nexponential family models.",
    "start": "280229",
    "end": "285870"
  },
  {
    "text": "It's weird that I go home\nand use any of these things, and I do, but it's not this one. All right.",
    "start": "285870",
    "end": "291979"
  },
  {
    "text": "So what we want to do is we\nwant to have the following idea here, is that-- and this is\nwhy it was-- it's so beautiful,",
    "start": "291979",
    "end": "299289"
  },
  {
    "text": "and will come back as a form\nthat we want to think about. If p has a special form,\nwhich I'll show below--",
    "start": "299290",
    "end": "304978"
  },
  {
    "text": "special form-- then some\nquestions come for free.",
    "start": "304979",
    "end": "314000"
  },
  {
    "text": "Some inference,\nlearning come for free.",
    "start": "314000",
    "end": "320979"
  },
  {
    "text": "What do I mean by for free? I mean what you already know\nautomatically applies to them. And when we start to worry\nabout more complicated models,",
    "start": "320979",
    "end": "328720"
  },
  {
    "text": "this will form a subroutine\nthat we'll use again and again. Like oh, if we can\nreduce it to that form, then we're in good shape.",
    "start": "328720",
    "end": "334370"
  },
  {
    "text": "That'll be the way\nwe get to things like unsupervised learning. All right. Now, the form looks like this.",
    "start": "334370",
    "end": "341850"
  },
  {
    "text": "OK? Now, one thing I\nshould highlight as I go through this--",
    "start": "341850",
    "end": "349020"
  },
  {
    "text": "so this is the data\nwhich you know. You already know this character. So data are labeled.",
    "start": "349020",
    "end": "354479"
  },
  {
    "text": "This thing is called the\nnatural parameters, OK? And I'll define the\nform in a second.",
    "start": "354479",
    "end": "360060"
  },
  {
    "text": "The reason I want to highlight\nthat is one place where you're likely to get kind of tripped\nup when we go through this",
    "start": "360060",
    "end": "365910"
  },
  {
    "text": "is that there are three\nsets of parameters. And I'll come back\nto that later. So if you're confused,\nthere'll be natural parameters,",
    "start": "365910",
    "end": "370960"
  },
  {
    "text": "canonical parameters, whatever. Doesn't matter. You'll see them all\nwritten down at one point, and that will kind of explain\nthe mappings between them.",
    "start": "370960",
    "end": "378669"
  },
  {
    "text": "But there are many\ndifferent names for parameters in this lecture,\nand that's like the essence to understand it.",
    "start": "378669",
    "end": "383810"
  },
  {
    "text": "And the reason\nthat's important is the natural parameters,\nif you like, are so we can write this\nform-- this functional form.",
    "start": "383810",
    "end": "390380"
  },
  {
    "text": "b of y exponential-- this is where it gets\nthe exponential name.",
    "start": "390380",
    "end": "397509"
  },
  {
    "text": "t of y minus of a of-- OK. So I'm going to unpack\nthis for a second, OK?",
    "start": "397509",
    "end": "406910"
  },
  {
    "text": "Now this form says,\nbasically, my probability distribution\nfactors, if you like, or can be written in this form.",
    "start": "406910",
    "end": "414039"
  },
  {
    "text": "Not every probability\ndistribution can. The way you show that a\nprobability distribution can be written in this form is\nyou write it in this form.",
    "start": "414039",
    "end": "420990"
  },
  {
    "text": "There's no secret\nshortcut here, right? You have to be\nable to express it as some linear thing\nin the parameter.",
    "start": "420990",
    "end": "427100"
  },
  {
    "text": "So this is the parameters\nhere, these natural parameters, times sum T of y-- I'll unpack that in\none second, what T of y",
    "start": "427100",
    "end": "432300"
  },
  {
    "text": "is-- minus this thing, which\nis the partition function. So that's what it says. It says that your\nfunction, right, there",
    "start": "432300",
    "end": "438490"
  },
  {
    "text": "are many functions that\nare in the world that could be probability densities. This one has this\ntechnical form.",
    "start": "438490",
    "end": "444110"
  },
  {
    "text": "We'll unpack this. This shouldn't be like obvious\nthat these things are important",
    "start": "444110",
    "end": "449699"
  },
  {
    "text": "or exist. So T of y is called the\nsufficient statistics.",
    "start": "449699",
    "end": "455050"
  },
  {
    "start": "450000",
    "end": "815000"
  },
  {
    "text": "Sufficient statistics. Now in this course, primarily\nwe'll use T of y equal to y.",
    "start": "455050",
    "end": "466690"
  },
  {
    "text": "We won't kind of\nmassage the data. But you can kind of\nthink about T of y is capturing everything that's\nrelevant to your data, right?",
    "start": "466690",
    "end": "472710"
  },
  {
    "text": "These are the things that you're\nmodeling that are in your. Data. And so in this example, like,\nwe're keeping everything.",
    "start": "472710",
    "end": "478500"
  },
  {
    "text": "T of y be equal to y\nmeans just its y itself. Now, another thing people\nget confused about.",
    "start": "478500",
    "end": "484630"
  },
  {
    "text": "This is necessarily the\nsame dimension as eta. Right, why is that?",
    "start": "484630",
    "end": "494340"
  },
  {
    "text": "Well, we take their dot product. This is a dot product here. You can think about this. I'll write it above,\njust so you're clear.",
    "start": "494340",
    "end": "500480"
  },
  {
    "text": "Also you could\nwrite it like this, if that's more clear to you. It's an inner product\nbetween the two, OK?",
    "start": "500480",
    "end": "505520"
  },
  {
    "text": "So to take the inner product,\nand for it to be meaningful, those have to be vectors of\nexactly the same dimension.",
    "start": "505520",
    "end": "510669"
  },
  {
    "text": "So if you want to have\nso many parameters, you have to have so many\nsufficient statistics. OK, so far, so good?",
    "start": "510669",
    "end": "518649"
  },
  {
    "text": "b of y is called\nthe base measure.",
    "start": "518649",
    "end": "525580"
  },
  {
    "text": "It's not the most critical\nelement here, but you need it. The intellectual content of\nthat is that B depends on y,",
    "start": "525580",
    "end": "532330"
  },
  {
    "text": "but it does not depend on-- not depend on-- eta.",
    "start": "532330",
    "end": "539889"
  },
  {
    "text": "So it says, basically, if you\nlike what's going on here, is this term has all\nthe interactions with y. This term has some\ninteractions with eta.",
    "start": "539890",
    "end": "546720"
  },
  {
    "text": "And the only way that eta and Ty\ninteract is through this term, OK?",
    "start": "546720",
    "end": "552260"
  },
  {
    "text": "It's really a statement\nabout how they interact. These functions are\npretty powerful, right? Those are just\narbitrary functions. But when they interact, they\ninteract in a linear way.",
    "start": "552260",
    "end": "558860"
  },
  {
    "text": "Sometimes these are\ngeneralized linear models. All right, want to\nkeep staring at it?",
    "start": "558860",
    "end": "567520"
  },
  {
    "text": "This character, a, is often\ncalled the log partition function.",
    "start": "567520",
    "end": "574120"
  },
  {
    "text": "Now this is a weird comment\nthat I'm going to make,",
    "start": "574120",
    "end": "580070"
  },
  {
    "text": "and it does not depend\non, well, its friend. Doesn't depend on y,\njust as I just said. It does not depend on y.",
    "start": "580070",
    "end": "587930"
  },
  {
    "text": "OK, now this thing\nis picked effectively as a way of normalizing\nthe distribution.",
    "start": "587930",
    "end": "594360"
  },
  {
    "text": "So it's a probability. So it sums to 1\nwhen I integrate it, or I sum up over\nthe discrete values",
    "start": "594360",
    "end": "599760"
  },
  {
    "text": "all the y's, it's\ngoing to sum to 1. So that may make you\nthink that A, in some way,",
    "start": "599760",
    "end": "604980"
  },
  {
    "text": "is not the star\nof the show, it's just this thing that kind\nof like adds up everything. But actually, this\nlog partition function",
    "start": "604980",
    "end": "611640"
  },
  {
    "text": "contains almost all the\ninformation, it turns out, of the function. And that's a weird\nthing, but it's true.",
    "start": "611640",
    "end": "618230"
  },
  {
    "text": "So we'll talk about that. So this contains a, and then\nyou have this linear interaction term where the statistics\ninteract with the data.",
    "start": "618230",
    "end": "625930"
  },
  {
    "text": "OK, now, just to\nmake sure it's clear,",
    "start": "625930",
    "end": "631820"
  },
  {
    "text": "y, a, and b are scalars-- scalar functions.",
    "start": "631820",
    "end": "639480"
  },
  {
    "text": "So a of eta, b of\ny, are scalars, OK?",
    "start": "639480",
    "end": "645170"
  },
  {
    "text": "Just to make sure\nthe types are clear. And these two characters\nhave the same dimension.",
    "start": "645170",
    "end": "652529"
  },
  {
    "text": "OK. So far, so good, right? So let's highlight\nwhere everyone--",
    "start": "652530",
    "end": "658149"
  },
  {
    "text": "where all these characters are. So you just see them\nvisually the same n-wise.",
    "start": "658149",
    "end": "664380"
  },
  {
    "text": "OK, so far, so good. All right, now here's\nthe crazy thing.",
    "start": "664380",
    "end": "671149"
  },
  {
    "text": "Many of the\ndistributions that you've encountered in your life-- I don't know how often you\nencounter distributions.",
    "start": "671149",
    "end": "676610"
  },
  {
    "text": "But if you encounter them\nat a relative frequency, a lot of them are of this form. And that was a huge win for\nstatistics because they said,",
    "start": "676610",
    "end": "684019"
  },
  {
    "text": "all the stuff we're doing on\ndistributions, a lot of it can be mapped into this\nwhat looks like, as I said, a trivial statement,\nand at the same time,",
    "start": "684019",
    "end": "691529"
  },
  {
    "text": "seems also impossible\nthat it would be there. So let's look at some\nexamples, and see how we map into this form. Probably you're\nthinking about-- you",
    "start": "691529",
    "end": "697680"
  },
  {
    "text": "are going through some\ndistributions in your head, if you look at them,\nand you're like, I'm not sure that\nit is of that form.",
    "start": "697680",
    "end": "703490"
  },
  {
    "text": "So let's look at the\nsimplest version of that, and see that actually, yep,\nthese things are of the form. So let's look at some examples.",
    "start": "703490",
    "end": "711959"
  },
  {
    "text": "Before doing that\nexample, I just want to-- are there any questions\nabout the content of this? And I'm happy to\ndefer, so please",
    "start": "711959",
    "end": "718310"
  },
  {
    "text": "feel free to ask\na question, and I can tell you to defer if\nthere's something else. Clear enough?",
    "start": "718310",
    "end": "723810"
  },
  {
    "text": "Yeah, please. Of eta x disappeared because, I\nguess, I didn't know we were-- There's no x in this equation.",
    "start": "723810",
    "end": "730019"
  },
  {
    "text": "Don't worry about it. He'll come back in a minute. Yeah, wonderful,\nwonderful point. Yeah, there's no x here. There's just a y,\nwhich is your data.",
    "start": "730020",
    "end": "736190"
  },
  {
    "text": "There's no x in\nfeatures, and they're going to make a-- they're going\nto make an appearance later. They will be one of\nthese parameters.",
    "start": "736190",
    "end": "742660"
  },
  {
    "text": "Yeah? Also, in [INAUDIBLE] there\nwas a couple of people-- are we talking about\nthe whole [INAUDIBLE]??",
    "start": "742660",
    "end": "748390"
  },
  {
    "text": "Yeah, it's a PDF. Oh. It's a PDF. And it says y given eta. It's not given.",
    "start": "748390",
    "end": "754380"
  },
  {
    "text": "Remember, it's our semicolon. Those are the\nparameters, remember? There's a bar which says\ngiven, which is condition,",
    "start": "754380",
    "end": "760250"
  },
  {
    "text": "and then there's\na semicolon, which says these are parameters. Yeah, see these etas are the\nparameters of the model, right?",
    "start": "760250",
    "end": "766690"
  },
  {
    "text": "And they're going to sweep\nup all that nasty notation that I talked about last time. Wonderful questions. Please?",
    "start": "766690",
    "end": "772200"
  },
  {
    "text": "[INAUDIBLE] and we said\nthat t of y is equal to y.",
    "start": "772200",
    "end": "781140"
  },
  {
    "text": "Does that mean qy\nis also [INAUDIBLE]?? In some of the examples\nthat we'll see later, yes. Yeah. It doesn't need to be.",
    "start": "781140",
    "end": "786389"
  },
  {
    "text": "Just to make my life a little\nbit simpler in this class. It's not a requirement\nof the model.",
    "start": "786389",
    "end": "792440"
  },
  {
    "text": "Awesome. Wonderful questions. OK, this is a requirement,\njust so we're clear.",
    "start": "792440",
    "end": "800870"
  },
  {
    "text": "These have to have\nthe same dimensions, otherwise it doesn't make sense. I'm not saying something deep. I'm just saying,\notherwise it will-- your brain should seg fault.\nLike, what am I taking a thing",
    "start": "800870",
    "end": "807350"
  },
  {
    "text": "and multiplying it by? Because this has--\nthis entire expression has to be a scalar\nfor it to make sense.",
    "start": "807350",
    "end": "815449"
  },
  {
    "start": "815000",
    "end": "1770000"
  },
  {
    "text": "OK, great. So let's take a\nlook at an example. So probably the first example\nthat you should think about,",
    "start": "815449",
    "end": "821320"
  },
  {
    "text": "I would guess, are\nBernoulli's, right? Bernoulli random variables. So what are these?",
    "start": "821320",
    "end": "827260"
  },
  {
    "text": "So we're going to have some phi\nprobability of an event, right? I guess the most common\nthing people use is",
    "start": "827260",
    "end": "832820"
  },
  {
    "text": "like flipping a biased coin-- some probability that it's\nhead, some probability that it's tails, something like this.",
    "start": "832820",
    "end": "840000"
  },
  {
    "text": "So here, when we've seen this\nbefore, phi, it had this form.",
    "start": "840000",
    "end": "846180"
  },
  {
    "text": "Remember this form that we used? Maybe this looks a\nlittle bit familiar.",
    "start": "846180",
    "end": "853139"
  },
  {
    "text": "So the probability of y for it's this term. When y is 0, it's this term.",
    "start": "853139",
    "end": "862890"
  },
  {
    "text": "Just a compact way to\nwrite it. p and 1 minus p is all I'm writing\nhere, if you think about a heads-tails\ndistribution.",
    "start": "862890",
    "end": "868329"
  },
  {
    "text": "All right, so this is not\nobviously of the form. Let's go get the form.",
    "start": "868329",
    "end": "873630"
  },
  {
    "text": "Let's go get our friend-- from up above. How are we going to\nput it in this form?",
    "start": "873630",
    "end": "883360"
  },
  {
    "text": "And I'll draw a box around it. All right?",
    "start": "883360",
    "end": "890730"
  },
  {
    "text": "Now when we do this,\nwe will be like, well, we've got to get an\nx somewhere, right?",
    "start": "890730",
    "end": "896570"
  },
  {
    "text": "I mean, that seems\npretty natural. So we're going to\nput an x in there, and we're going to x above\ny log y plus 1 minus y log",
    "start": "896570",
    "end": "909300"
  },
  {
    "text": "one minus y. OK, so far so good. We're making a little\nbit of progress.",
    "start": "909300",
    "end": "915589"
  },
  {
    "text": "Well, the problem\nwe have right now is the y's are interacting\nin two places, right?",
    "start": "915589",
    "end": "922460"
  },
  {
    "text": "So what do we have to do? Well, we have to bring\nall the y's together because the y's and the\nphis, the parameters,",
    "start": "922460",
    "end": "927709"
  },
  {
    "text": "like, they're not etas yet. We'll give ourselves some\nfreedom, but intuitively, like, the parameters should\nall be looped, all",
    "start": "927709",
    "end": "933700"
  },
  {
    "text": "be somewhere together. So what is that going to be? Phi 1 minus 5 plus log 1 by phi.",
    "start": "933700",
    "end": "944380"
  },
  {
    "text": "Now we seem to be in kind\nof the right situation because we have\nthis character here, which kind of looks like the\ninteraction terms, right?",
    "start": "944380",
    "end": "951540"
  },
  {
    "text": "y and model parameters\nare interacting. And we have\nsomething here that's isolated that's just a function\nof the model parameters.",
    "start": "951540",
    "end": "958230"
  },
  {
    "text": "Now it's going to turn\nout that phi is not going to be equal to eta, right,\nbecause this thing here is not",
    "start": "958230",
    "end": "963620"
  },
  {
    "text": "of the right functional form. So we had to figure that out. So what's our what's\nour best guess for what",
    "start": "963620",
    "end": "969860"
  },
  {
    "text": "this eta will be? Well, why not what\nit looks like?",
    "start": "969860",
    "end": "977319"
  },
  {
    "text": "So we want to set-- to show\nthat it's of the right form, we postulate that this thing\nis actually equal to eta, OK?",
    "start": "977319",
    "end": "985949"
  },
  {
    "text": "Now, that means this thing here\nhas to be some function of eta, right?",
    "start": "985949",
    "end": "991860"
  },
  {
    "text": "Now that seems, at one hand,\nkind of obvious, right? Because it only depends\non phi, and that's a local transformation for phi.",
    "start": "991860",
    "end": "997350"
  },
  {
    "text": "But we're going to\nsolve for it explicitly, if that kind of hand-waving\nimplicit function theorem kind of argument bothers you, OK?",
    "start": "997350",
    "end": "1004569"
  },
  {
    "text": "All right. So here, our goal is we're\ngoing to take Ty is, again, going to be equal to y.",
    "start": "1004569",
    "end": "1009620"
  },
  {
    "text": "We're going to\ntake eta that way, and we want to understand\nwhat is the value of a. And what we claim it is,\nright, a of eta or of nu",
    "start": "1009620",
    "end": "1017800"
  },
  {
    "text": "is we claim that that's going\nto be minus log of 1 by phi.",
    "start": "1017800",
    "end": "1024188"
  },
  {
    "text": "All right, let's check\nthat claim, right? That shouldn't be hard. And so why does that work?",
    "start": "1024189",
    "end": "1030319"
  },
  {
    "text": "Well, copy this guy. Oops.",
    "start": "1030319",
    "end": "1037410"
  },
  {
    "text": "So this goes to, well, I just\ntake e of both sides, 1 by phi.",
    "start": "1037410",
    "end": "1043610"
  },
  {
    "text": "Then I move this thing\nacross, right, so that I can do whatever I like.",
    "start": "1043610",
    "end": "1053930"
  },
  {
    "text": "Then I want to make sure I\nget all the phis on one side. So I get phi 5\ntimes e eta plus 1.",
    "start": "1053930",
    "end": "1064440"
  },
  {
    "text": "And I'm off to the races, right? So now I have phi is equal to 1\nplus e to the minus eta over 1,",
    "start": "1064440",
    "end": "1075539"
  },
  {
    "text": "OK? Now, this means, by the way,\nthat log of 1 minus phi, well,",
    "start": "1075539",
    "end": "1085030"
  },
  {
    "text": "that's going to be equal to--\noh, sorry, did I screw that up? Let's make sure I\ndidn't screw that up.",
    "start": "1085030",
    "end": "1092309"
  },
  {
    "text": "I'm just going to\nredo this piece just to make sure, because I\ndidn't do it in my notes.",
    "start": "1092309",
    "end": "1098790"
  },
  {
    "text": "So it's going to be-- oh, no. It's 1-- I'm right. E to the [INAUDIBLE] OK, so\nnow I take 1 minus this, right.",
    "start": "1098790",
    "end": "1107539"
  },
  {
    "text": "So that's going to\nbe equal log of-- this is a lot of arithmetic\nto do in one sitting,",
    "start": "1107539",
    "end": "1112820"
  },
  {
    "text": "but that's going to be log\nof 1 plus e to the eta.",
    "start": "1112820",
    "end": "1121860"
  },
  {
    "text": "OK, so far, so good. So why does that satisfy me? Because that's a\nfunction of eta, right?",
    "start": "1121860",
    "end": "1128130"
  },
  {
    "text": "Now as I said, it's kind of\nstraightforward from here, because these things are\njust functions of each other. But this is technically\nwhat we needed",
    "start": "1128130",
    "end": "1134351"
  },
  {
    "text": "to do to show that these\nthings are actually equal. And in fact, we're\nin good shape now.",
    "start": "1134351",
    "end": "1140590"
  },
  {
    "text": "OK? So what am I saying? I'm saying this seems\ntrivial on one hand because you're like,\nwow, I could just put in whatever I want.",
    "start": "1140590",
    "end": "1146130"
  },
  {
    "text": "But you can't put in\nwhatever you want. You have to first separate\nout the interaction between y in the form, and\nthen you have to be",
    "start": "1146130",
    "end": "1152010"
  },
  {
    "text": "able to pull out the term\nthat depends only on nu here, the parameters. Does that make sense?",
    "start": "1152010",
    "end": "1158159"
  },
  {
    "text": "Let's see another example. Please? I guess it's fine to have\neta in the follow function",
    "start": "1158159",
    "end": "1164778"
  },
  {
    "text": "of the parameter,\nand not necessarily the parameter [INAUDIBLE]",
    "start": "1164779",
    "end": "1170480"
  },
  {
    "text": "Right, and it is a function. Yeah. Right, that's just\na function of a. Just this happens to\nbe a different one, OK?",
    "start": "1170480",
    "end": "1175880"
  },
  {
    "text": "Now here's what's weird. I don't know if I should\nreally mention this, but you should look. This thing actually--\nremember I told you",
    "start": "1175880",
    "end": "1181350"
  },
  {
    "text": "all the action was in there? You can kind of look\nat this encoding. This is the log\npartition function. So a log is what we expected.",
    "start": "1181350",
    "end": "1189360"
  },
  {
    "text": "This 1 plus en\nthing, well, if you think about the different\nweights on the spaces, this is actually\nencoding the fact",
    "start": "1189360",
    "end": "1195480"
  },
  {
    "text": "that there's like a positive\nand a negative state. And try and think about why\nthat might actually be the case. In fact, if we start to compute\nthe derivative of this thing,",
    "start": "1195480",
    "end": "1203050"
  },
  {
    "text": "it's actually the\nexpected value of y. That turns out not to\nbe an accident, OK?",
    "start": "1203050",
    "end": "1210460"
  },
  {
    "text": "All right. Before we get into that,\nlet's go down one more. So what did I hope that\nyou got it from this.",
    "start": "1210460",
    "end": "1216850"
  },
  {
    "text": "It's a tedious thing\nto walk through. You can and should walk\nthrough these on your own. There are three examples. You should also walk through\nthe logistic kinds of examples,",
    "start": "1216850",
    "end": "1223960"
  },
  {
    "text": "and the others. Basically, the whole\nthing is I want to make it super clear\nwhat the statement means.",
    "start": "1223960",
    "end": "1229000"
  },
  {
    "text": "I don't expect anything\nhere to be mind-blowing. I don't think like\nour use of fractions is going to change your lives.",
    "start": "1229000",
    "end": "1234890"
  },
  {
    "text": "I'm just saying that this-- is the content of the\nstatement clear, right? You give me a probability\ndistribution in one form.",
    "start": "1234890",
    "end": "1241010"
  },
  {
    "text": "I'm going to translate it into\na same functional form such that it has--",
    "start": "1241010",
    "end": "1247190"
  },
  {
    "text": "satisfies these conditions. And then, in that\ncase, this eta is now",
    "start": "1247190",
    "end": "1253730"
  },
  {
    "text": "what's called in\nnatural parameters, OK? And you're typically\nnot given the function in the natural\nparameters, right?",
    "start": "1253730",
    "end": "1260159"
  },
  {
    "text": "And you're going to be\nresponsible, on homework and in other places, to do this. And the reason why is, if\nyou can do this mapping,",
    "start": "1260160",
    "end": "1266500"
  },
  {
    "text": "then a bunch of stuff gets easy. Inference gets easy,\nlearning gets easy because now it turns\nout that you can show--",
    "start": "1266500",
    "end": "1271799"
  },
  {
    "text": "and we'll talk\nabout in a second, you can do gradient descent\non these parameters, and it's going to be concave.",
    "start": "1271799",
    "end": "1277110"
  },
  {
    "text": "And that's wild\nthat you can solve all of these models\nthe same way, OK? So I just want to make sure that\nthe functional form is clear.",
    "start": "1277110",
    "end": "1283240"
  },
  {
    "text": "And the reason we're\ndoing it is because it's going to simplify some stuff. Please? Isn't e to the t function to\nmassage it into that form for--",
    "start": "1283240",
    "end": "1292789"
  },
  {
    "text": "Yeah, awesome. So could you use the T function\nto massage in the form? Now, in this class,\nif you find yourself doing that too aggressively,\nyou've probably done something",
    "start": "1292789",
    "end": "1298600"
  },
  {
    "text": "wrong, just as,\nlike, a heads up, because we don't\nuse it too much. But yeah, you\ncould do that in T. If T were expressed in\nsome way, and you were only",
    "start": "1298600",
    "end": "1306100"
  },
  {
    "text": "modeling a piece of it\nas a result of this, and saying the\nprobability distribution didn't depend on one part--\nlike, T was a projection-- you",
    "start": "1306100",
    "end": "1312539"
  },
  {
    "text": "could do that, too. But this is pretty\nhard to get around. So I think you're thinking\nin exactly the right way. Kind of like, how do I\nget around and break this?",
    "start": "1312539",
    "end": "1319059"
  },
  {
    "text": "And basically what it's saying\nis your interactions are arbitrary, and y-- arbitrary-- excuse\nme-- arbitrary nu.",
    "start": "1319060",
    "end": "1326230"
  },
  {
    "text": "And the interactions between\nthem occur in the exp, right, and that are linear.",
    "start": "1326230",
    "end": "1331610"
  },
  {
    "text": "And once they have this linear\ninteraction term, whatever the function T is, those\nsufficient statistics, that's what you're\nmodeling up to.",
    "start": "1331610",
    "end": "1337980"
  },
  {
    "text": "And some folks asked, and\nI answered some advanced questions on Ed, which none of\nyou are responsible to know,",
    "start": "1337980",
    "end": "1343380"
  },
  {
    "text": "about deriving\nthings like why is logistic regression calibrated\nin certain ways in data.",
    "start": "1343380",
    "end": "1349460"
  },
  {
    "text": "Those features, those\nsufficient statistics, are what feed into\nthose arguments. So up to these\nsufficient statistics,",
    "start": "1349460",
    "end": "1354570"
  },
  {
    "text": "this is how well you do. So when you play the\ngame of massaging it, you're either throwing away\ninformation or you're not.",
    "start": "1354570",
    "end": "1360250"
  },
  {
    "text": "In which case, that's\nwhat we do here. So it's a modeling choice. You could do it, nothing\nthat prevents you.",
    "start": "1360250",
    "end": "1366100"
  },
  {
    "text": "But it means something\nabout what you're doing underneath the covers. OK, this is awesome. So now you learn something\nwhich, again hopefully seems",
    "start": "1366100",
    "end": "1374020"
  },
  {
    "text": "sort of trivial. You're like, oh, I can take-- I can take that distribution\nof heads and tails, and put it into this\nweird functional form.",
    "start": "1374020",
    "end": "1379919"
  },
  {
    "text": "And that will be interesting\nbecause one thing you should test your understanding\nof is, can you now, given a bunch of samples\nfrom heads and tails,",
    "start": "1379919",
    "end": "1387070"
  },
  {
    "text": "estimate the\nunderlying parameters? Computing derivatives here\nseems a little bit nasty, right? These are like phi's and\ny's that are up in things.",
    "start": "1387070",
    "end": "1394890"
  },
  {
    "text": "It looks like a weird function. Once I put it in this form,\nall of a sudden like it's nice, and convex, and life is good.",
    "start": "1394890",
    "end": "1400570"
  },
  {
    "text": "But I have to look at\nestimating this parameter, not the original one. Is that making sense?",
    "start": "1400570",
    "end": "1405820"
  },
  {
    "text": "Please ask me a question if not. We'll see one more example,\nand we'll come back to it.",
    "start": "1405820",
    "end": "1412340"
  },
  {
    "text": "This is the only important thing\nfrom what we need to do now. Let's look at the\nGaussian, example two.",
    "start": "1412340",
    "end": "1417750"
  },
  {
    "text": "We'll only do these\ntwo examples, OK? This is the Gaussian\nwith fixed variance.",
    "start": "1417750",
    "end": "1429100"
  },
  {
    "text": "This one's really\ngood because, remember what the probability of y is?",
    "start": "1429100",
    "end": "1435500"
  },
  {
    "text": "It's going to look like-- sorry-- 1 over 2 pi exp--",
    "start": "1435500",
    "end": "1446429"
  },
  {
    "text": "and there is a negative-- y minus mu squared over",
    "start": "1446429",
    "end": "1454730"
  },
  {
    "text": "Let's make sigma squared\nequal to 1, actually, to make my life a little easier. OK?",
    "start": "1454730",
    "end": "1460160"
  },
  {
    "text": "Just for no reason. Now how do we get that\nin our favorite form?",
    "start": "1460160",
    "end": "1465360"
  },
  {
    "text": "So let me go copy\nour favorite form. Seems to be pretty close, right? I mean, we're in\npretty good shape.",
    "start": "1465360",
    "end": "1471140"
  },
  {
    "text": "What do we need to do? Well, we have to\nfactor it in some way.",
    "start": "1471140",
    "end": "1476730"
  },
  {
    "text": "So this constant, we\ncan absorb anywhere. We don't care about that. We have to somehow pull\nthis thing apart, OK?",
    "start": "1476730",
    "end": "1483340"
  },
  {
    "text": "So how are we going to do that? So we're going to put the 1 over",
    "start": "1483340",
    "end": "1490040"
  },
  {
    "text": "And then we're going to pull out\nthe e minus y squared over 2. So I'm just going to\nfactor this, right? This is going to be minus\ny squared plus u squared",
    "start": "1490040",
    "end": "1498820"
  },
  {
    "text": "minus 2 mu y over 1/2. That's what this term\nis going to be, right?",
    "start": "1498820",
    "end": "1504440"
  },
  {
    "text": "So it's straight multiplication. So I factor out\nthe mu y squared. What does that leave me with?",
    "start": "1504440",
    "end": "1510049"
  },
  {
    "text": "It leaves me with x of this\ncharacter, mu y, minus 1/2 mu",
    "start": "1510049",
    "end": "1519960"
  },
  {
    "text": "squared-- oops. Minus 1/2 mu squared.",
    "start": "1519960",
    "end": "1526240"
  },
  {
    "text": "So what are our\nnatural parameters? Well, eta is mu, which\nis why I accidentally wrote it right at the beginning,\nwhich would have been a little",
    "start": "1526240",
    "end": "1532309"
  },
  {
    "text": "bit weird to do. T of y and a of n\nequals 1/2 eta square--",
    "start": "1532309",
    "end": "1539520"
  },
  {
    "text": "oops, mu square. Well-- oh, yeah.",
    "start": "1539520",
    "end": "1547090"
  },
  {
    "text": "OK. Right, so because this is mu. Now again, notice I\ndifferentiate this thing.",
    "start": "1547090",
    "end": "1554028"
  },
  {
    "text": "What's the expected\nvalue of this character? Well, it's mu, right?",
    "start": "1554029",
    "end": "1559230"
  },
  {
    "text": "But when I differentiate\nay, I get exactly back mu, which is kind of interesting.",
    "start": "1559230",
    "end": "1565000"
  },
  {
    "text": "This is trivial here. In the last example,\nit was non-trivial. It was like a weird function\nthat I differentiated, and I got back the actual\nprobability distribution,",
    "start": "1565000",
    "end": "1571580"
  },
  {
    "text": "right? Which is kind of bizarre\nthat I would get that back. OK? By the way, if that's not clear,\ndifferentiate this function",
    "start": "1571580",
    "end": "1579510"
  },
  {
    "text": "with respect to eta, and\nthen see what you get. OK? Does that makes sense? So just to make sure we're\nverifying everything,",
    "start": "1579510",
    "end": "1586669"
  },
  {
    "text": "yep, this looks right. This is my B term. Oh, sorry, this is my B term.",
    "start": "1586670",
    "end": "1593730"
  },
  {
    "text": "I'll highlight different colors. So this is by, and this thing\nis the log partition function.",
    "start": "1593730",
    "end": "1599039"
  },
  {
    "text": "And what I'm just\ntrying to highlight is like, this thing contains\na lot of information about the distribution in both\nof the examples we've seen.",
    "start": "1599039",
    "end": "1607669"
  },
  {
    "text": "Please. [INAUDIBLE] No.",
    "start": "1607669",
    "end": "1614039"
  },
  {
    "text": "It's a wonderful question. So right now, we're worried\nabout the case where y is going to be a\nscalar, which makes",
    "start": "1614039",
    "end": "1619059"
  },
  {
    "text": "our lives a little bit easier. And so we're going\nto look at that, but you just have to have that\nthe Ty and eta actually resolve",
    "start": "1619059",
    "end": "1626919"
  },
  {
    "text": "to a scalar that the same type. So if your y has\nmultiple dimensions, then you need more\nnatural parameters.",
    "start": "1626919",
    "end": "1633110"
  },
  {
    "text": "And that's actually\npretty important because that's why\nwe call them natural. It's like, your problem\nhas dimension D?",
    "start": "1633110",
    "end": "1639950"
  },
  {
    "text": "Then you need D for\nyour parameters. [INAUDIBLE] There is not, not.",
    "start": "1639950",
    "end": "1647630"
  },
  {
    "text": "The thing is, I wanted to\nhave sigma squared be fixed. Like, I didn't want it\nto change per data point.",
    "start": "1647630",
    "end": "1654510"
  },
  {
    "text": "That was important,\nand so it was easier to just write 1 there. If you put sigma squared in\nhere and it was just a constant,",
    "start": "1654510",
    "end": "1659528"
  },
  {
    "text": "then you would just push it\ninto the appropriate spots and be done. They would just fold into this. If sigma squared were\nsomething that we're",
    "start": "1659529",
    "end": "1664900"
  },
  {
    "text": "changing per data\npoint, right, like we were trying to estimate\nfor every data point, not only its mean, but\nit's possible variance.",
    "start": "1664900",
    "end": "1670570"
  },
  {
    "text": "Like, I give you a\ntemperature reading, and I say like from\nall the data I've seen, I think it's 30 degrees.",
    "start": "1670570",
    "end": "1676080"
  },
  {
    "text": "But I know that I\nhaven't seen enough data, so I'm like plus\nor minus 2 degrees. If I've seen tons of data,\nand I'm very confident,",
    "start": "1676080",
    "end": "1681320"
  },
  {
    "text": "I'll say plus or minus That is an estimation where\nsigma is part of the model,",
    "start": "1681320",
    "end": "1687610"
  },
  {
    "text": "and then you would have\nanother free parameter for it. Great question, and\ntry and write that out. I don't know if that example\nis written out in the notes.",
    "start": "1687610",
    "end": "1693120"
  },
  {
    "text": "If you get stuck or whatever,\nplease send me a note. I'll write it up for you on Ed. Does that make sense that--\ncapture your question?",
    "start": "1693120",
    "end": "1700289"
  },
  {
    "text": "Yeah. Awesome. OK. Yeah, awesome. So two questions\non the live thread.",
    "start": "1700289",
    "end": "1708539"
  },
  {
    "text": "The examples to go\nthrough on your own are the ones in the hand-- in\nthe typed, written notes, which contain these and\none more, but just go",
    "start": "1708539",
    "end": "1714640"
  },
  {
    "text": "through them on your own. Like, I'll just wax\npoetic for one minute. If you haven't studied for a\nmathematically-minded course,",
    "start": "1714640",
    "end": "1720399"
  },
  {
    "text": "the way that I always\ndo it-- did it-- I still actually read\ntextbooks, and course notes, and everything else--\nis I read them,",
    "start": "1720399",
    "end": "1725539"
  },
  {
    "text": "I watch the lecture,\nwhatever it is. Then I try and remember\nthose key spots, and I try to derive them myself. It's the fastest way to\nfigure out what didn't stick.",
    "start": "1725539",
    "end": "1732519"
  },
  {
    "text": "So if you're like, oh, I\ncan write the derivative in two minutes, and you\nwalk away, well, OK, you know everything for the lecture. If you get stuck, it's\na really good signal",
    "start": "1732520",
    "end": "1739259"
  },
  {
    "text": "that you don't do it. If you don't do it, and\nthen that builds over time, something that you\nthought was trivial and you didn't actually\nput the time into we'll",
    "start": "1739260",
    "end": "1746278"
  },
  {
    "text": "end up biting it, right? Just lesson that I learned\nfrom too many years",
    "start": "1746279",
    "end": "1751639"
  },
  {
    "text": "of the university. Why is the derivative equal\nto the expected value? I'm not going to prove that. I will just assert it here. I just want to show\nthat-- oops, sorry--",
    "start": "1751640",
    "end": "1757400"
  },
  {
    "text": "I just want you to observe\nthat in both cases, it's true. It's a wonderful question.",
    "start": "1757400",
    "end": "1762529"
  },
  {
    "text": "It just takes a little\nbit of arithmetic to show. It's a wonderful question, yeah.",
    "start": "1762529",
    "end": "1768360"
  },
  {
    "text": "All right. OK, so I'm just I'm going to put\nthose assertions in here now.",
    "start": "1768360",
    "end": "1776000"
  },
  {
    "text": "So why do we care\nabout this form?",
    "start": "1776000",
    "end": "1790310"
  },
  {
    "text": "The first is what we\nsaid, inference is easy.",
    "start": "1790310",
    "end": "1796169"
  },
  {
    "text": "The expected value\nof y given eta",
    "start": "1796169",
    "end": "1803070"
  },
  {
    "text": "is the partial\nderivative with respect to the natural\nparameters of a of n, OK?",
    "start": "1803070",
    "end": "1808900"
  },
  {
    "text": "I would encourage you to compute\nthis on all the examples. Proving the general\nthing just takes",
    "start": "1808900",
    "end": "1814190"
  },
  {
    "text": "a little bit of an extra thing. But on all the examples so far\nyou've seen, it's clearly true. And then why that's\ntrue in general",
    "start": "1814190",
    "end": "1819760"
  },
  {
    "text": "is because it's the\nlog of the sum of all the possible outcomes. Proving it for continuous stuff\ntakes a little bit more effort,",
    "start": "1819760",
    "end": "1826600"
  },
  {
    "text": "OK? So don't worry about it. But we've seen this is true, OK? And this pattern holds.",
    "start": "1826600",
    "end": "1832710"
  },
  {
    "text": "The variance is also the\nsecond derivative of an, OK?",
    "start": "1832710",
    "end": "1842018"
  },
  {
    "text": "Now this pattern,\nyou may think, holds. Like oh, the third power's got--\nno, it doesn't work that way.",
    "start": "1842019",
    "end": "1847169"
  },
  {
    "text": "Just these first two. Those are the only\nones that matter. These are only ones that work. So why is this so\ninteresting to me?",
    "start": "1847169",
    "end": "1854700"
  },
  {
    "text": "One, because once you\ntake your distribution, whatever the crazy\ndistribution is, however wild it is,\nand distributions",
    "start": "1854700",
    "end": "1860350"
  },
  {
    "text": "can get pretty insane-- there's\nuncountable many of them-- you put them into this form. You basically have a mechanical\nprocedure to do inference,",
    "start": "1860350",
    "end": "1867150"
  },
  {
    "text": "and to do variance estimation. Inference is more\nimportant to us, but that also means that\nyou can do learning, right?",
    "start": "1867150",
    "end": "1876148"
  },
  {
    "text": "And in fact, learning\nis well defined. OK?",
    "start": "1876149",
    "end": "1883049"
  },
  {
    "text": "In particular, this function\nis going to be concave in eta.",
    "start": "1883049",
    "end": "1888130"
  },
  {
    "text": "OK, let me write it this way. The MLE-- remember we did the\nmaximum likelihood estimator",
    "start": "1888130",
    "end": "1895429"
  },
  {
    "text": "for all those\nthings previously-- is concave, OK?",
    "start": "1895429",
    "end": "1903090"
  },
  {
    "text": "Please. [INAUDIBLE] That helps.",
    "start": "1903090",
    "end": "1909419"
  },
  {
    "text": "Yeah, so that is\ndefinitely a piece of it. You have to do one extra\nstep, but you're exactly on the right thing.",
    "start": "1909419",
    "end": "1915130"
  },
  {
    "text": "That's exactly how you\ncan go and prove it. Just compute it\ndirectly, and see that it's positive in\nthe way that you think. If the second derivative\nis positive everywhere,",
    "start": "1915130",
    "end": "1921330"
  },
  {
    "text": "then it's then it's\nconvex, but yeah. So you have to-- there's\na negative in front,",
    "start": "1921330",
    "end": "1926518"
  },
  {
    "text": "but that's a small-- Wonderful. Yeah? [INAUDIBLE]",
    "start": "1926519",
    "end": "1931890"
  },
  {
    "text": "Exactly right. If you remember\nlast time, the way we framed all of our\nestimation problems was take the log likelihood\nthat was there, l of theta,",
    "start": "1931890",
    "end": "1938960"
  },
  {
    "text": "and then use gradient\ndescent on that. And this is basically saying\nthat the resulting formulation,",
    "start": "1938960",
    "end": "1944070"
  },
  {
    "text": "if I use the natural\nparameters, is always guaranteed to be concave. Please.",
    "start": "1944070",
    "end": "1950720"
  },
  {
    "text": "[INAUDIBLE] Yeah. So the thing is you can\ncompute this directly",
    "start": "1950720",
    "end": "1956768"
  },
  {
    "text": "by looking at exactly when\nyou compute the derivative, and pull it out. The way that you do it-- I'm not going to prove it in\nclass, but I'll just tell you.",
    "start": "1956769",
    "end": "1963580"
  },
  {
    "text": "It's not a mysterious statement. What you do is you look\nat for the discrete case. You look at the fact that\nit's a log partition.",
    "start": "1963580",
    "end": "1969500"
  },
  {
    "text": "That means it's the sum over\nall the possible worlds, meaning all the\npossible ways that y could be assigned, right?",
    "start": "1969500",
    "end": "1974870"
  },
  {
    "text": "So there's going to be a term\nin there for each one of them, because it sums\nover all of them. That's what it does. So maybe this is getting way\ntoo abstract a mysterious.",
    "start": "1974870",
    "end": "1981850"
  },
  {
    "text": "Maybe it's better to prove. So if I look here, look at\nthis part of the distribution,",
    "start": "1981850",
    "end": "1989760"
  },
  {
    "text": "this may not sum to 1, right,\nif I just started to do it. So if I took and summed over y--",
    "start": "1989760",
    "end": "1996240"
  },
  {
    "text": "oops. If I sum y of this expression,\nlet's call this gy,",
    "start": "1996240",
    "end": "2004139"
  },
  {
    "text": "it may not equal 1. It's not guaranteed\nto be equal to 1, OK? Because it's just some\ncollection of values, and some other stuff.",
    "start": "2004139",
    "end": "2009860"
  },
  {
    "text": "This thing is-- the\nfunction, this eta here, makes sure that it's equal to 1.",
    "start": "2009860",
    "end": "2015900"
  },
  {
    "text": "So it's the scalar, as we were\ntalking about before, that make sure whatever this sums to,\nit's going to divide over it.",
    "start": "2015900",
    "end": "2021299"
  },
  {
    "text": "So it's sometimes written as 1\nover z, the partition function. However that means that\nthe way you get it,",
    "start": "2021299",
    "end": "2027929"
  },
  {
    "text": "one derivation of it is you sum\nover all the different values. So it is like the sum\nof everything over 1",
    "start": "2027929",
    "end": "2034190"
  },
  {
    "text": "is actually what--\nso this character has to be equal to this, right?",
    "start": "2034190",
    "end": "2043580"
  },
  {
    "text": "Because it has to cancel out. It has to be actually equal\nto 1 when I compute it. And that means\nthat it's basically of the form, a sum over all\nthe possible values of y.",
    "start": "2043580",
    "end": "2052829"
  },
  {
    "text": "And so if you compute the\nderivative inside, when you take the log\nof that, each one of the y's is going to\ncome down next to exactly",
    "start": "2052829",
    "end": "2058378"
  },
  {
    "text": "this functional form. All right? So if that's too mysterious,\nI didn't want to prove it, because it gets strange.",
    "start": "2058379",
    "end": "2064349"
  },
  {
    "text": "But I'm happy to\nwrite out the proof. It's super straightforward. OK, awesome.",
    "start": "2064349",
    "end": "2071760"
  },
  {
    "text": "So all I care about is this,\nthough, the thing that I'm trying to get across--\nbecause I'm trying to give you a guided tour. I don't want to get to-- I want to do enough details\nthat you see all the pieces,",
    "start": "2071760",
    "end": "2078500"
  },
  {
    "text": "so you can go back and\nunderstand how they work, but I don't want to get\nbogged down in things that I don't think are\nlike super critical for you",
    "start": "2078500",
    "end": "2084490"
  },
  {
    "text": "to understand. And also, I don't\nwant to do things that I think you\nshould do on your own, because doing them mechanically\nwill teach you better than me",
    "start": "2084490",
    "end": "2090090"
  },
  {
    "text": "like inscrutably writing\nfor the hundredth time how to prove this. But if I'm wrong, and you want\nto watch me inscrutably write,",
    "start": "2090090",
    "end": "2096030"
  },
  {
    "text": "I can-- I don't know, you can log in\nto twitch stream or something. Awesome. All right, all good.",
    "start": "2096030",
    "end": "2103170"
  },
  {
    "text": "So clear enough,\nlike, why we did this, barring these assertions,\nif you get your probability",
    "start": "2103170",
    "end": "2108540"
  },
  {
    "text": "distribution into this form,\nthen all of a sudden you get inference, and learning,\nand a bunch of other stuff for free.",
    "start": "2108540",
    "end": "2114170"
  },
  {
    "text": "However, one of the things is,\nas was correctly pointed out, there was a little bit of\na bait and switch here. We started, last time, to think\nabout various different models,",
    "start": "2114170",
    "end": "2121680"
  },
  {
    "text": "and how we put\nthose models inside. So where did the data in x go? And that's what we actually\nneed to figure out here, OK?",
    "start": "2121680",
    "end": "2127369"
  },
  {
    "start": "2127000",
    "end": "2403000"
  },
  {
    "text": "All right, so let's talk about\ngeneralized linear models.",
    "start": "2127369",
    "end": "2137619"
  },
  {
    "text": "OK.",
    "start": "2137620",
    "end": "2144970"
  },
  {
    "text": "So the point that I really\nwant you to get across is, these are all\ndesign assumptions, OK?",
    "start": "2144970",
    "end": "2150849"
  },
  {
    "text": "So these are all design\nchoices that you can actually make in your model. And we'll get to them, and\ntalk about what you want to do.",
    "start": "2150849",
    "end": "2156230"
  },
  {
    "text": "And you can also think\nabout them as assumptions. All right, so first,\nwhat we're going to say",
    "start": "2156230",
    "end": "2163660"
  },
  {
    "text": "is we're going to claim that\nthe distribution of our label, given x for some\nparameters, theta,",
    "start": "2163660",
    "end": "2170410"
  },
  {
    "text": "follows an exponential\nfamily, OK? Now I claim, without\nmuch justification",
    "start": "2170410",
    "end": "2176160"
  },
  {
    "text": "here, that this is\nan important family. Now, you can say it's\nan important family because, as we'll talk about,\nmany different data types fall",
    "start": "2176160",
    "end": "2183210"
  },
  {
    "text": "into this that you've seen. So if you look at\nbinary things, you want to do y is binary, well,\nthat's Bernoulli, right?",
    "start": "2183210",
    "end": "2190250"
  },
  {
    "text": "So we looked at\nthat classification. If you want to have\nreal-valued y's, well, we have a couple of them\nthat we could use,",
    "start": "2190250",
    "end": "2196270"
  },
  {
    "text": "but we saw Gaussians. If you want to do counts,\nlike you're actually",
    "start": "2196270",
    "end": "2201690"
  },
  {
    "text": "counting how many people\nwalk by a particular thing, or how many packets arrive at a\nserver, or something like that,",
    "start": "2201690",
    "end": "2209160"
  },
  {
    "text": "then that's a\ndifferent distribution. It's called Poisson, OK? If it's-- all you want all\nthe whole real line, right?",
    "start": "2209160",
    "end": "2216480"
  },
  {
    "text": "You want real positive line. Well, there's two different\nfancy distributions that are called gamma, right? You don't need to\nknow these per se,",
    "start": "2216480",
    "end": "2221849"
  },
  {
    "text": "but what I'm trying\nto explain to you, kind of proof by writing a\nlot and gesturing wildly,",
    "start": "2221849",
    "end": "2227630"
  },
  {
    "text": "is that these-- oh, sorry. Exponential. Leplace is also in this. If you want distributions,\nthere's one more.",
    "start": "2227630",
    "end": "2235609"
  },
  {
    "text": "Then this is called the\nDirichlet distribution. Dirichlet. So what am I trying\nto get across here?",
    "start": "2235609",
    "end": "2241830"
  },
  {
    "text": "These are probably most of the\ndistributions you've heard of. There are more that\nI'm not writing down, but they all fall into\nthis exponential family.",
    "start": "2241830",
    "end": "2248880"
  },
  {
    "text": "Now one hypothesis is that we\nfigured out this technology, and that's why we described\ndistributions this way,",
    "start": "2248880",
    "end": "2255190"
  },
  {
    "text": "but that's actually not true. We went the other way around. We were doing things ad-hoc\nfor each one of them, and this tied them up and\nput them nicely together.",
    "start": "2255190",
    "end": "2262040"
  },
  {
    "text": "So you pick your error\nmode, and the way you pick the error mode is it\nhas to have the right type. If you're observing\nbinary data, you",
    "start": "2262040",
    "end": "2268190"
  },
  {
    "text": "want to use a Bernoulli, right? Or something that\nhas a binary type. If you're observing\nreal data, you want to use a Gaussian,\ncounts, Poisson, and so on.",
    "start": "2268190",
    "end": "2274530"
  },
  {
    "text": "So there's a data\ntype mapping here. The second thing\nthat you have is",
    "start": "2274530",
    "end": "2281960"
  },
  {
    "text": "that your natural\nparameters-- and this is where they come\nback in-- are going to be of this following\nform, with theta",
    "start": "2281960",
    "end": "2288260"
  },
  {
    "text": "element of Rd plus 1, and x\nalso element of Rd plus 1.",
    "start": "2288260",
    "end": "2294500"
  },
  {
    "text": "OK? So here we're going to make\nthe assumption that, after--",
    "start": "2294500",
    "end": "2300420"
  },
  {
    "text": "subject to noise, our\nmodel varies linearly with some underlying features.",
    "start": "2300420",
    "end": "2308180"
  },
  {
    "text": "Now you're going\nto see later-- this is actually a more powerful\nassumption than you realize. If you take your model\nto be very, very large,",
    "start": "2308180",
    "end": "2314119"
  },
  {
    "text": "and have a huge\nnumber of features, almost everything becomes\nlinear in that space. High-dimensional geometry\nis very, very weird.",
    "start": "2314120",
    "end": "2319619"
  },
  {
    "text": "So it's not like if you think\nin low dimensions you're like, oh, there's only so\nmany lines I can draw. I can't separate out my data.",
    "start": "2319619",
    "end": "2325750"
  },
  {
    "text": "If you take your\ndata, and put it up into a huge\ndimensional space, odds are it will be\nlinearly separable.",
    "start": "2325750",
    "end": "2331060"
  },
  {
    "text": "There'll be a line through it. We'll back to that, OK? So this is just the thing there. And then, three, once\nyou've made that assumption,",
    "start": "2331060",
    "end": "2339680"
  },
  {
    "text": "your inference is super easy. Right?",
    "start": "2339680",
    "end": "2345420"
  },
  {
    "text": "Test time. You output E of y\ngiven x data, OK?",
    "start": "2345420",
    "end": "2358289"
  },
  {
    "text": "Said another way,\nh of theta of x equals E of y given x of theta.",
    "start": "2358290",
    "end": "2369369"
  },
  {
    "text": "OK, so hopefully\nthis makes sense. So I'll walk through\nexactly what's",
    "start": "2369369",
    "end": "2374440"
  },
  {
    "text": "going to happen in one second,\njust to make sure it's clear, but this means that we're\ndoing this prediction. And one thing that\nwe sneaked past you",
    "start": "2374440",
    "end": "2380480"
  },
  {
    "text": "was that, when we went to\nlogistic regression, all of a sudden, we started\nwith these hypothesis where, instead of\nreturning like yes or no,",
    "start": "2380480",
    "end": "2386369"
  },
  {
    "text": "we just returned a probability\ndistribution over it. And that was a change, right? When we're doing regression,\nwe return to just a value.",
    "start": "2386370",
    "end": "2393318"
  },
  {
    "text": "Here we're just\nreturning, actually, like your probability that you\nthink y has a particular value. That's what you\ndo for inference.",
    "start": "2393319",
    "end": "2398420"
  },
  {
    "text": "That's how inference is defined. So let me make sure this is\nsuper clear how this works. Your data comes in as x.",
    "start": "2398420",
    "end": "2404578"
  },
  {
    "start": "2403000",
    "end": "2582000"
  },
  {
    "text": "It then goes into\nyour linear model. You compute phi transpose\nx with your parameters.",
    "start": "2404579",
    "end": "2412770"
  },
  {
    "text": "This is your box. You get out eta, right? Theta T becomes eta in the\nparameter you had before.",
    "start": "2412770",
    "end": "2420630"
  },
  {
    "text": "You feed that to your\nexponential model.",
    "start": "2420630",
    "end": "2427690"
  },
  {
    "text": "Ex model does whatever it does. There's b's, and T's, and\nwhatever in there, but you now your value for eta, right?",
    "start": "2427690",
    "end": "2432790"
  },
  {
    "text": "You can do whatever you want. I'm using both. And then if you\nwant to train, you do max over phi of log pyx phi.",
    "start": "2432790",
    "end": "2444869"
  },
  {
    "text": "If you want to do\ninference, you do eyx phi. This is learning,\nthis is in inference.",
    "start": "2444869",
    "end": "2453540"
  },
  {
    "text": "OK, so all you pick here\nis you pick your data. You pick the\nfeatures, and then you",
    "start": "2453540",
    "end": "2458931"
  },
  {
    "text": "run this procedure, and\neverything's kind of automated for you in the sense\nthat-- yeah, in the sense",
    "start": "2458931",
    "end": "2466589"
  },
  {
    "text": "that you now a general recipe\nto do maximum likelihood estimation and do inference. And it's not obvious how\nto do that, by the way.",
    "start": "2466589",
    "end": "2472740"
  },
  {
    "text": "There are scenarios\nwhere we don't know how to do maximum\nlikelihood estimation. So right now, your\nuniverse is like, oh, everything you shown me,\nyou've done maximum likelihood",
    "start": "2472740",
    "end": "2478640"
  },
  {
    "text": "estimation. It's been really easy. I'm like, yeah, that's fair. But there's a big world\nout there of stuff that is hard to cram into this.",
    "start": "2478640",
    "end": "2484780"
  },
  {
    "text": "And so what this says is, if\nyou can put it into this form, maximum likelihood\nestimation and inference becomes super, super easy.",
    "start": "2484780",
    "end": "2491650"
  },
  {
    "text": "And learning here\nhas a nice form. Data j looks like this, and\nyou can directly check this.",
    "start": "2491650",
    "end": "2499800"
  },
  {
    "text": "Plus alpha yi minus\nh theta of xi xji.",
    "start": "2499800",
    "end": "2513980"
  },
  {
    "text": "So why is that the case? Well, we saw that it was a\ncase in all the other models. You just have to go through,\nand compute the derivative,",
    "start": "2513980",
    "end": "2519190"
  },
  {
    "text": "and convince yourself. But now that it's\nin this form, just computing the derivative\nis with respect to theta as you go through here\nbecause it occurs only",
    "start": "2519190",
    "end": "2525540"
  },
  {
    "text": "in the state of Tx-- we'll get that out. So one of the reasons I\nwas delaying, proving it in the special cases\nwas because you",
    "start": "2525540",
    "end": "2530870"
  },
  {
    "text": "have to do all of\nthese transformations to put it into the\nright, nice form. And then when I\ncompute the derivative,",
    "start": "2530870",
    "end": "2536319"
  },
  {
    "text": "my life gets\nreally, really easy. If you compute it in\nthe natural parameters, it looks weird, but-- I mean, if you compute it\nin the original parameters,",
    "start": "2536320",
    "end": "2542200"
  },
  {
    "text": "it looks weird. But if you compute it this\nway, life is pretty good. And this, by the way,\nhere is always this thing.",
    "start": "2542200",
    "end": "2552220"
  },
  {
    "text": "Right? Always hypotheses.",
    "start": "2552220",
    "end": "2557359"
  },
  {
    "text": "OK. So far, so good?",
    "start": "2557359",
    "end": "2563770"
  },
  {
    "text": "Please. I want to ask, is this kind of\ndifferent from what we did in",
    "start": "2563770",
    "end": "2571369"
  },
  {
    "text": "[INAUDIBLE]? Yeah, so let's-- actually,\nit's a great question. Let me do one more example,\nand then hopefully that",
    "start": "2571369",
    "end": "2578359"
  },
  {
    "text": "will become clear\nabout how they relate. I want to-- so let me just run\nthrough logistic regression,",
    "start": "2578359",
    "end": "2584680"
  },
  {
    "text": "then. I think that will probably\nmaybe answer that question. So it'll show exactly\nhow they fit together.",
    "start": "2584680",
    "end": "2592079"
  },
  {
    "text": "Terminology. OK, so there's the\nmodel parameter.",
    "start": "2592080",
    "end": "2602039"
  },
  {
    "text": "This is phi. There's the natural parameter,\nwhich in a linear model,",
    "start": "2602040",
    "end": "2611200"
  },
  {
    "text": "we always substitute-- a generalized linear model,\nwe always substitute. So this was the eta before. And then there's the\ncanonical parameters.",
    "start": "2611200",
    "end": "2622010"
  },
  {
    "text": "So these were like\nphi for Bernoulli, OK? Or mu and sigma\nsquared for Gaussians.",
    "start": "2622010",
    "end": "2630390"
  },
  {
    "text": "OK? And this g here, we're going\nto call the canonical response, and this g inverse.",
    "start": "2630390",
    "end": "2636950"
  },
  {
    "text": "So g is called the\ncanonical response. OK, so why do I do this? I want to make sure it's really\nclear what all the pieces are,",
    "start": "2636950",
    "end": "2642750"
  },
  {
    "text": "and what's going on here. There are some model parameters. That's the thing that we're\ngoing to solve with respect to.",
    "start": "2642750",
    "end": "2649010"
  },
  {
    "text": "That's the thing\nthat we're going to do the gradient descent on. That's the thing that\nwe're going to do the h theta with respect to. Theta is then dotted into\nthe model or the data.",
    "start": "2649010",
    "end": "2656960"
  },
  {
    "text": "That's what this x. This is data right here.",
    "start": "2656960",
    "end": "2664220"
  },
  {
    "text": "That becomes the\nnatural parameter, which then goes to the-- which\nthe exponential model now tells you how to\noperate on, and then I",
    "start": "2664220",
    "end": "2671140"
  },
  {
    "text": "can do everything I want\non the natural parameter. That's what tells\nme the distribution. And so in the case we're\ndoing, logistic regression,",
    "start": "2671140",
    "end": "2676829"
  },
  {
    "text": "which we'll talk\nabout in one second, you have a linear\nthing, and then your errors are of the form-- I make an error that I sometimes\nswitch the class, if you like.",
    "start": "2676829",
    "end": "2684160"
  },
  {
    "text": "I get the wrong answer\nwith some probability. And then there's\nthis link, which",
    "start": "2684160",
    "end": "2689441"
  },
  {
    "text": "are these canonical parameters. And this is the content of\nwhat we're talking about here is you write them down in\nthese canonical parameters,",
    "start": "2689441",
    "end": "2695950"
  },
  {
    "text": "and then they have--\npeople write them down in whatever messy\nform we found them. And I'm asserting\nthat a lot of them",
    "start": "2695950",
    "end": "2701240"
  },
  {
    "text": "can be put into that\nnice exponential form through what's called\nthis canonical response function or its link\nfunction, and that allows us",
    "start": "2701240",
    "end": "2707530"
  },
  {
    "text": "to treat them in the same way. So this is super important\nbecause when you encounter one of these distributions,\nyou probably encounter it",
    "start": "2707530",
    "end": "2713060"
  },
  {
    "text": "in this form, you have\nto put it into this form, and then that lets you do\neverything that we just talked about in one clean way.",
    "start": "2713060",
    "end": "2719869"
  },
  {
    "text": "Learning becomes this\nnice, simple rule. Inference becomes this\nnice, simple rule. OK, awesome.",
    "start": "2719869",
    "end": "2726880"
  },
  {
    "text": "All right.",
    "start": "2726880",
    "end": "2732269"
  },
  {
    "text": "So let's look at\nlogistic regression, just so it's super\nclear what that means. All right, so h theta x.",
    "start": "2732270",
    "end": "2742599"
  },
  {
    "text": "Well, we said it's the\nexpected value of y given x and some parameters, theta.",
    "start": "2742599",
    "end": "2750990"
  },
  {
    "text": "This theta-- so we have-- so there's theta equals",
    "start": "2750990",
    "end": "2757250"
  },
  {
    "text": "And then the model,",
    "start": "2757250",
    "end": "2762940"
  },
  {
    "text": "oops-- e to the minus theta Tx.",
    "start": "2762940",
    "end": "2770308"
  },
  {
    "text": "So this piece here, theta\nwas our-- sorry, theta--",
    "start": "2770309",
    "end": "2776619"
  },
  {
    "text": "so this was our\nmodel parameter, this was after we transformed to\nthe natural parameters-- that's this character-- and this is\nwhat we wrote down last time.",
    "start": "2776619",
    "end": "2784260"
  },
  {
    "text": "So when we went to do\nlogistic regression, remember we had a loss\nfunction that looked like this. This was our sigmoid\nor logistic function.",
    "start": "2784260",
    "end": "2790140"
  },
  {
    "text": "And then what I'm\nsaying is that we-- right now, to get\nthe derivatives and do everything else,\nwhich I skated on last time,",
    "start": "2790140",
    "end": "2795230"
  },
  {
    "text": "I now no longer have to\nskate on because I just made it more abstract. I transform it into this\nparameter space, and I'm good.",
    "start": "2795230",
    "end": "2803049"
  },
  {
    "text": "So in one hand, as I said,\nit's totally trivial. I'm just doing a transformation\nof how I represent the numbers,",
    "start": "2803050",
    "end": "2808560"
  },
  {
    "text": "but it also seems weird\nthat I can do this. And I'm asserting that\nin these cases, I can. And that's what allows\nus to go and treat",
    "start": "2808560",
    "end": "2815160"
  },
  {
    "text": "all those different\ndistributions in some way. So if I give you some\nfeatures, you dot forward, you learn a model.",
    "start": "2815160",
    "end": "2820520"
  },
  {
    "text": "And then you have an error\nlike, I'm looking at counts, I'm observing them. Counts have a very different\ndistribution than the errors I would expect on 0, 1\nthings, or the errors",
    "start": "2820520",
    "end": "2827349"
  },
  {
    "text": "I would expect on a\nlinear regression. I just plug that model\nin, that natural model in, and out falls a pretty\nreasonable class",
    "start": "2827350",
    "end": "2834160"
  },
  {
    "text": "of machine learning models. And you may say, the\nthing that people usually react to is they say\nsomething like, well,",
    "start": "2834160",
    "end": "2840700"
  },
  {
    "text": "what if I want to\ndo something that's more complicated than linear? But linear is pretty powerful. I can take my features\nand square them.",
    "start": "2840700",
    "end": "2846210"
  },
  {
    "text": "I can multiply them together. It's still linear, right? I can take my--\nfeature five could be the product of\nthe preceding seven.",
    "start": "2846210",
    "end": "2853710"
  },
  {
    "text": "And so this turns out to\nbe a wildly popular class of machine learning models. In fact, there are\nentire books that are written about\ngeneralized linear models.",
    "start": "2853710",
    "end": "2860569"
  },
  {
    "text": "And there's a citation\nto McCullough, which is the standard reference. I'm not sure I advise\nnecessarily reading it,",
    "start": "2860569",
    "end": "2865780"
  },
  {
    "text": "but it's the standard reference. Not because it's not a great\nbook, just, like, it's long. OK, please. In your personal\nopinion, do you think",
    "start": "2865780",
    "end": "2871300"
  },
  {
    "text": "that there will be more\nimprovements in the theory",
    "start": "2871300",
    "end": "2879990"
  },
  {
    "text": "of machine learning, such that\nquadratic and other models are more applicable the\nnew school [INAUDIBLE] is extremely powerful, it\ndoesn't make a lot of sense?",
    "start": "2879990",
    "end": "2885760"
  },
  {
    "text": "Yeah. [INAUDIBLE] Man, I really feel like I\nshould buy you something.",
    "start": "2885760",
    "end": "2891440"
  },
  {
    "text": "So that's a great question. So there's a lot of folks that\nhave done through the years-- as we'll talk about\nwhen you get to kernels,",
    "start": "2891440",
    "end": "2896670"
  },
  {
    "text": "things like polynomial kernels\nand exponential kernels. And those are very powerful\nways to model the world. What I was kind of hinting\nat before is the linear model",
    "start": "2896670",
    "end": "2903640"
  },
  {
    "text": "has this sneaky out\nin the back, which is get to pick the features. So if up you know up\nfront that like you",
    "start": "2903640",
    "end": "2909099"
  },
  {
    "text": "want-- that the squares\nof the temperatures are more indicative\nthan the raw values, you can just put\nthat into your model,",
    "start": "2909099",
    "end": "2915349"
  },
  {
    "text": "and learn more\nand more features. And so it's not a\nquestion of eventually we're going to get powerful\nand use those features.",
    "start": "2915349",
    "end": "2921109"
  },
  {
    "text": "We can use them today. The crazy thing is we can\nreduce a lot of the things you would naturally\ndo to this model.",
    "start": "2921110",
    "end": "2926880"
  },
  {
    "text": "And you'll see\neventually, someone was asking about these infinite\ndimensional feature models. Those are what kernels are. You can reduce those to linear\nin an infinite dimensional",
    "start": "2926880",
    "end": "2934450"
  },
  {
    "text": "space. So it's wildly, like,\nimportant there to do this. The other bit, which is\nalso my personal opinion",
    "start": "2934450",
    "end": "2941380"
  },
  {
    "text": "on these things,\none of the things that has really bitten\nme again and again is that simple stupid things\nwork extraordinarily well",
    "start": "2941380",
    "end": "2950049"
  },
  {
    "text": "with a-- given enough data. And in fact, the trend has\nbeen larger and larger amounts of data for the last 40 years.",
    "start": "2950049",
    "end": "2956470"
  },
  {
    "text": "And every time we think\nit's going to run out of gas and get fancy, a bunch\nof fancy people academics start writing papers about\nclever ways to do x, y, or z,",
    "start": "2956470",
    "end": "2963680"
  },
  {
    "text": "and usually they get smoked by\nmore data, and linear stuff. And there's a great\npaper about this that I can post of\ndifferent eras of machine",
    "start": "2963680",
    "end": "2970390"
  },
  {
    "text": "learning when this happened. And like, the thing that's\nremarkable about modern machine learning to me is not\nhow sophisticated it is,",
    "start": "2970390",
    "end": "2976260"
  },
  {
    "text": "but how we basically\ndo the same thing, and just pour in\nmountains of data. Like right now, there's\na particular model",
    "start": "2976260",
    "end": "2981390"
  },
  {
    "text": "that's very hot. In five years, will it be hot? I don't know, right? Maybe. Five years ago It wasn't--",
    "start": "2981390",
    "end": "2986740"
  },
  {
    "text": "I guess five years ago,\nit was kind of hot. But, you know, and then\na new one will come. But the pattern of like we just\ndump in all this information,",
    "start": "2986740",
    "end": "2992540"
  },
  {
    "text": "and just optimize the crap\nout of these parameters, that works really well. So the thing that I'm trying\nto get across there is--",
    "start": "2992540",
    "end": "2998950"
  },
  {
    "text": "as you said, is the\nfuture of machine learning seems to me to be more tied\nwith understanding relatively simple applications when we\nhave huge amounts of data",
    "start": "2998950",
    "end": "3006270"
  },
  {
    "text": "underneath the covers. Yeah, but I'm a zealot there. I taught the large\nlanguage model course we taught last quarter.",
    "start": "3006270",
    "end": "3012700"
  },
  {
    "text": "So I'm a believer. You can think it's nonsense. It's a personal opinion,\nbut wonderful question. Yeah, awesome.",
    "start": "3012700",
    "end": "3020180"
  },
  {
    "text": "Others? All right. So again, you know, same thing.",
    "start": "3020180",
    "end": "3026789"
  },
  {
    "text": "I'll do Gaussian, just\nto stall, to make sure this is clear, because I hope--",
    "start": "3026790",
    "end": "3031990"
  },
  {
    "text": "if I'm very optimistic,\nI mean, you're like, oh, this is obvious. There's no content here. I understand it perfectly.",
    "start": "3031990",
    "end": "3038230"
  },
  {
    "text": "If we are in that universe,\nI will be extremely happy. If you're baffled,\nplease ask me a question.",
    "start": "3038230",
    "end": "3044170"
  },
  {
    "text": "[CHUCKLES] This is a Gaussian. How do I do prediction? Well, when I have something,\nI pick the mean value",
    "start": "3044170",
    "end": "3050200"
  },
  {
    "text": "that I would have. That's exactly the\nsame piece that's here. How do I do the estimation? Well, that's exactly theta Tx.",
    "start": "3050200",
    "end": "3056190"
  },
  {
    "text": "That's what we were doing\nbefore, when we fit a line. Same thing.",
    "start": "3056190",
    "end": "3066260"
  },
  {
    "text": "So all that's-- all I'm saying\nis what we've done so far in the first K lectures we've\nnow compressed to basically one",
    "start": "3066260",
    "end": "3072109"
  },
  {
    "text": "equation, one schema\nof this thing.",
    "start": "3072109",
    "end": "3077640"
  },
  {
    "text": "We now know how to do inference,\nand we know how to do learning. And maybe it's tough to\nappreciate in the sense that you're like, well, I didn't\nencounter 1,000 models before.",
    "start": "3077640",
    "end": "3085040"
  },
  {
    "text": "But now, all of these\ndifferent models can be shoehorned into this,\nand that's quite powerful. And we'll use that\nquite a bit later when we do much more advanced stuff.",
    "start": "3085040",
    "end": "3092260"
  },
  {
    "text": "All right, awesome. If there are no questions, I'm\ngoing to move on to multiclass.",
    "start": "3092260",
    "end": "3100349"
  },
  {
    "start": "3097000",
    "end": "3599000"
  },
  {
    "text": "All right, so the\nlast thing I want to describe here, which\nis important for you, and I think you have to\nuse in your homework,",
    "start": "3100349",
    "end": "3106610"
  },
  {
    "text": "is how we deal with\nmulticlass classification. And I should say the trend has\nbeen, in machine learning, not",
    "start": "3106610",
    "end": "3113701"
  },
  {
    "text": "only these big models that I was\njust excited and ranting about, which you can take\nor leave, but is",
    "start": "3113701",
    "end": "3121828"
  },
  {
    "text": "that you train models\non a variety of tasks more broad than even\na variety of classes.",
    "start": "3121829",
    "end": "3128010"
  },
  {
    "text": "You train them to do\nmany things at once. In fact, weirdly, as\nI may have told you, the thing that we seem to\nbe doing as a field right",
    "start": "3128010",
    "end": "3134020"
  },
  {
    "text": "now is training a model\nto do something, task A, and then using it for\ntask B. And weirdly, that",
    "start": "3134020",
    "end": "3141190"
  },
  {
    "text": "makes the model more robust. So the typical task that\nwe do there, by the way, is predicting the next\nword, which is a really-- seems like a really basic task.",
    "start": "3141190",
    "end": "3147730"
  },
  {
    "text": "You look at a sentence,\nand you produce-- you view the words\nas individually. This is oversimplified,\nbut how it works.",
    "start": "3147730",
    "end": "3154529"
  },
  {
    "text": "Every token is a class, right? So it's the word\ncat, dog, whatever. You just take a vocabulary\nof 50,000 words let's say,",
    "start": "3154530",
    "end": "3161440"
  },
  {
    "text": "and then you predict. Which one do you think is\nlikely to be in the next space? So I read the first part of\nthe sentence, and I predict.",
    "start": "3161440",
    "end": "3167309"
  },
  {
    "text": "This turns the entire web\ninto a training corpus, right? Because now any\npiece of text, I can",
    "start": "3167309",
    "end": "3172460"
  },
  {
    "text": "evaluate in a multiclass way. We train it just to do\nthat, and weird behavior",
    "start": "3172460",
    "end": "3178328"
  },
  {
    "text": "emerges, like it can write\npieces of code for you. It can answer questions\nin narrative form.",
    "start": "3178329",
    "end": "3184240"
  },
  {
    "text": "And only when we train it\non lots, and lots, and lots of data, and it's a\nlittle bit spooky. So anyway-- so this is\nwhat multiclass is for,",
    "start": "3184240",
    "end": "3190760"
  },
  {
    "text": "and this is actually something\nthat we use every day.",
    "start": "3190760",
    "end": "3195990"
  },
  {
    "text": "Awesome, wonderful question. What is the disconnect between\nthe power of linear models,",
    "start": "3195990",
    "end": "3201270"
  },
  {
    "text": "and the need for non-linear\ncomponents in a neural network? Wonderful question. So right now what we've\ndone is we've said--",
    "start": "3201270",
    "end": "3206980"
  },
  {
    "text": "and the exchange was wonderful--\nhey, what about more powerful feature representations? What neural nets basically\nare, and why they're so amazing",
    "start": "3206980",
    "end": "3213690"
  },
  {
    "text": "is they take your\ndata, if you like, and they pick out\nwhat those features are, what those x's should\nbe from the underlying data.",
    "start": "3213690",
    "end": "3221099"
  },
  {
    "text": "And then usually on the end,\nyou just have a linear model. There's little tweaks\nand variations, but that's pretty\nmuch what you have.",
    "start": "3221099",
    "end": "3226230"
  },
  {
    "text": "So the question of where\ndoes the x come from, you give me an\nimage of a cat, how do I get good\nfeatures about images?",
    "start": "3226230",
    "end": "3232920"
  },
  {
    "text": "That's what the neural\nnet is actually solving, and that's where we\nneed non-linearity. But it comes in at that piece,\nnot at the prediction piece.",
    "start": "3232920",
    "end": "3239230"
  },
  {
    "text": "Great questions. All right, so here we're going\nto look at discrete values.",
    "start": "3239230",
    "end": "3245230"
  },
  {
    "text": "If you're familiar\nwith distributions up to sum fixed k-- so we have a cat, dog, car,\nI don't know, whatever else.",
    "start": "3245230",
    "end": "3257349"
  },
  {
    "text": "Oh, I wanted one another one. Oh, bus. OK. So here, k is 4.",
    "start": "3257349",
    "end": "3263700"
  },
  {
    "text": "All right? So I want to predict\namong that set. It's kind of weird I\npromise you that you're only",
    "start": "3263700",
    "end": "3268829"
  },
  {
    "text": "going to see a cat,\na dog, a car, a bus. You could ask, well, what\nif I show it a horse? It doesn't have to predict it.",
    "start": "3268830",
    "end": "3274329"
  },
  {
    "text": "It can predict whatever it\nwants in that situation, right? But for this case, imagine\nthat I'm just distinguishing",
    "start": "3274329",
    "end": "3280140"
  },
  {
    "text": "among those four classes. Or the crazy example\nI gave you where your classes are every word\nin, say, the English language.",
    "start": "3280140",
    "end": "3286630"
  },
  {
    "text": "All right. How do you encode this?",
    "start": "3286630",
    "end": "3293089"
  },
  {
    "text": "It's encoded as\na one-hot vector. It's called the\none-hot vector, right?",
    "start": "3293089",
    "end": "3298328"
  },
  {
    "text": "And the distribution--\nthe error distribution is the thing that we just\ntalked about to the k, OK?",
    "start": "3298329",
    "end": "3308900"
  },
  {
    "text": "So-- such that some y equals 1-- to the k equals 1.",
    "start": "3308900",
    "end": "3316930"
  },
  {
    "text": "OK? So there's a vector\nthat's in 0, 1 but if precisely\none thing is lit up.",
    "start": "3316930",
    "end": "3323440"
  },
  {
    "text": "So, for example, you could\nhave the cat one is 1000.",
    "start": "3323440",
    "end": "3328450"
  },
  {
    "text": "This could be cat.",
    "start": "3328450",
    "end": "3334810"
  },
  {
    "text": "You get the pattern. represent car, and so on.",
    "start": "3334810",
    "end": "3340288"
  },
  {
    "text": "So those vectors, basically\nthese one-hot vectors, they seem pretty\nwasteful, but you don't have to store all the zeroes.",
    "start": "3340289",
    "end": "3345710"
  },
  {
    "text": "It's not as bad as it looks. But like this is\nhow you intuitively think, mathematically think,\nabout how the data looks, OK?",
    "start": "3345710",
    "end": "3353710"
  },
  {
    "text": "clear enough? So we've reduced our\nproblem from dealing with these categorical labels\nto dealing with vectors.",
    "start": "3353710",
    "end": "3362369"
  },
  {
    "text": "Now let's try to\nclassify them, all right? So let's draw a quick picture.",
    "start": "3362369",
    "end": "3369160"
  },
  {
    "text": "Oops. So let's imagine our\ndata looks like this.",
    "start": "3369160",
    "end": "3374550"
  },
  {
    "text": "So there are some class ones,\nwhich I guess are cats here. There are some buses\nhere, which are 4's.",
    "start": "3374550",
    "end": "3382140"
  },
  {
    "text": "There are some dogs here. I'm just drawing them all\nthe nice and clustered. Of course, your data never\nreally looks like this,",
    "start": "3382140",
    "end": "3388020"
  },
  {
    "text": "but that's OK. All right, so what do what\ndo I want in this situation?",
    "start": "3388020",
    "end": "3395490"
  },
  {
    "text": "I want lines. So this is corresponding\nto one class. As I said, this\nis the cat class,",
    "start": "3395490",
    "end": "3404529"
  },
  {
    "text": "this is the dog\nclass, and so on.",
    "start": "3404530",
    "end": "3410250"
  },
  {
    "text": "So how does multidimensional--\nhow does this multiclass thing work?",
    "start": "3410250",
    "end": "3415970"
  },
  {
    "text": "That's too close. The colors don't really\nmatter, but I have started,",
    "start": "3415970",
    "end": "3422109"
  },
  {
    "text": "so now I'm going to finish. All right. Car, bus. So what do I want here?",
    "start": "3422110",
    "end": "3428109"
  },
  {
    "text": "What I need to do\nis, when I pick, because we're looking at\nlinear separators for this, I kind of want to look at--",
    "start": "3428109",
    "end": "3434750"
  },
  {
    "text": "what I'll do is I'll pick a line\nthat, for example, separates",
    "start": "3434750",
    "end": "3440559"
  },
  {
    "text": "the cats from everything else. So this will be something. This will be theta 1\ndot x is equal to 0.",
    "start": "3440559",
    "end": "3449630"
  },
  {
    "text": "So this is the line\nI'm drawing here. So I want to pick theta,\nright, so that on one side",
    "start": "3449630",
    "end": "3454730"
  },
  {
    "text": "are the cats, and on the\nother side is everything else. Does that intuitively\nmake sense?",
    "start": "3454730",
    "end": "3460130"
  },
  {
    "text": "Right. For theta 2, for theta 4,\nwhat would I like to do? Well, I'd also like to\npick something where--",
    "start": "3460130",
    "end": "3465970"
  },
  {
    "text": "here. Oops. Theta 4 dot x equals 0.",
    "start": "3465970",
    "end": "3472019"
  },
  {
    "text": "So I'd like it so that, again,\nthe buses are on one side, and everything else is there. Now if you look at\nthis geometrically,",
    "start": "3472020",
    "end": "3477150"
  },
  {
    "text": "it becomes pretty clear, by the\nway, there's lots of choices. I could have picked here, I\ncould have picked there, right?",
    "start": "3477150",
    "end": "3483440"
  },
  {
    "text": "What we will try and prefer\nsometimes called max margin. We actually prefer that\nit's kind of as far away from the two data\npoints as possible.",
    "start": "3483440",
    "end": "3489410"
  },
  {
    "text": "It's like as close to in\nthe middle as possible. And you can verify that\nactually makes sense, and you can verify that\nthat's actually what",
    "start": "3489410",
    "end": "3495109"
  },
  {
    "text": "we'll-- you'll hope will happen. You get something like this. All right? This is x rate.",
    "start": "3495109",
    "end": "3502960"
  },
  {
    "text": "So in this case, by the\nway, which is really nice, everything is nice, and\nlinear, or separable, right?",
    "start": "3502960",
    "end": "3508369"
  },
  {
    "text": "This side has the\ndogs on it, this side has the cars on it and so on. So there's one line\nthat explains--",
    "start": "3508369",
    "end": "3517039"
  },
  {
    "text": "that kind of can separate\neach one of them. Now the question is,\nhow do you pick, right?",
    "start": "3517039",
    "end": "3528140"
  },
  {
    "text": "So the way you do it, right,\nis when you get a point,",
    "start": "3528140",
    "end": "3534210"
  },
  {
    "text": "you're going to\ncompute its value against each one of them--\ntheta 1, theta 2, theta 3.",
    "start": "3534210",
    "end": "3539369"
  },
  {
    "text": "I'm going to draw the\nfirst ones, first three-- theta 4, whatever. And these are going to\ngive me some values, OK?",
    "start": "3539369",
    "end": "3546199"
  },
  {
    "text": "And the values are\ngoing to be-- basically you can think about\nas, how close are they to the various lines?",
    "start": "3546199",
    "end": "3551200"
  },
  {
    "text": "OK? It's a dot product. Please. Can you please remind me\nwhy you put it equal to 0?",
    "start": "3551200",
    "end": "3557970"
  },
  {
    "text": "I'm just saying this is the\nline where, on this side, theta is going to be-- so for this side, theta\nis going to be positive.",
    "start": "3557970",
    "end": "3564770"
  },
  {
    "text": "And on this side, it's going\nto be negative all over here, right? And so I'm just drawing\nthe deciding line.",
    "start": "3564770",
    "end": "3569950"
  },
  {
    "text": "So as you're more\ncatlike, you're getting a higher\nscore from this. You're getting something\nthat's like a larger score.",
    "start": "3569950",
    "end": "3575940"
  },
  {
    "text": "As you're here, you're\ngetting a negative cat score.",
    "start": "3575940",
    "end": "3581940"
  },
  {
    "text": "So the point is each one of\nthese things, because of that, is going to give you--\ngreat question-- is going to give you some score. So maybe it's a cat, so it\nlooks like this thing, 0.1.",
    "start": "3581940",
    "end": "3592099"
  },
  {
    "text": "And everybody else is like,\nyeah, I'm not really sure. Maybe I'm kind of\nlike borderline about all the other classes. This is what you would\nhope would happen, OK?",
    "start": "3592099",
    "end": "3599450"
  },
  {
    "text": "Each model gives me a\nscore, and then-- oh, did you have a question? Yes, I had a quick\nquestion about the center. So what happens we\nwere in the center?",
    "start": "3599450",
    "end": "3606380"
  },
  {
    "text": "Yeah, yeah, great question. So right now, remember\nlike, you give me a horse, and you put the horse in a\ncenter, who knows, right?",
    "start": "3606380",
    "end": "3612680"
  },
  {
    "text": "Who knows what I'll get. I will be able to\nrun this procedure, and it will get confused. So one of the things that\nhappens in neural nets,",
    "start": "3612680",
    "end": "3619030"
  },
  {
    "text": "by the way, is that they're--\nin large scale models is, when you're really\nhigh dimensional, and you have a bunch of these\nlines, and other things,",
    "start": "3619030",
    "end": "3626338"
  },
  {
    "text": "there will be pockets that\nare actually nowhere close to anything. But now, look, it's\ntotally well-defined. This character here\nhas a normal this way.",
    "start": "3626339",
    "end": "3633220"
  },
  {
    "text": "This one will get a negative--\nit will get a negative score from everybody, right? Now you can look and say,\nif it gets a negative score",
    "start": "3633220",
    "end": "3639048"
  },
  {
    "text": "from every single thing, maybe\nI should be suspicious of it. But that's not, in general,\nsomething that will happen. One of these, unfortunately,\nwill be higher than the others.",
    "start": "3639049",
    "end": "3645930"
  },
  {
    "text": "Like if it's here, it\nmay be closer to cat. And so it says, oh, a cat\nthat was near the border, or something like\nthat, and it will pick.",
    "start": "3645930",
    "end": "3651660"
  },
  {
    "text": "But let's get to\nthe procedure first before talking about\nthe exceptions, OK? So we have all of these.",
    "start": "3651660",
    "end": "3657280"
  },
  {
    "text": "Then what happens? We exponentiate them, right?",
    "start": "3657280",
    "end": "3667750"
  },
  {
    "text": "And then this actually\nleads to probabilities. Actually, let's make\nthis really negative. This is like minus 10.",
    "start": "3667750",
    "end": "3673190"
  },
  {
    "text": "e to the minus 10,\nthen these things are approximately like",
    "start": "3673190",
    "end": "3679920"
  },
  {
    "text": "I get the values out,\nand then I normalize them",
    "start": "3679920",
    "end": "3686558"
  },
  {
    "text": "by summing all of them. So I sum all these together. So I sum all these characters. Sum theta i dot x, and that\ngives me some value, z.",
    "start": "3686559",
    "end": "3694150"
  },
  {
    "text": "And I divide this number by\nz, divide this number by z. And that's going to give\nme a number between 0 and 1 because I sum them up,\nand they're all positive.",
    "start": "3694150",
    "end": "3701210"
  },
  {
    "text": "So 0.5, 0.17, and so on-- The point is I compute this\nexponential, I sum-- oops,",
    "start": "3701210",
    "end": "3709099"
  },
  {
    "text": "this should be e of this thing-- and then that is my\nnormalizing term.",
    "start": "3709099",
    "end": "3714890"
  },
  {
    "text": "I sum it up. So let me write it\nin a cleaner form. It's the following thing.",
    "start": "3714890",
    "end": "3722599"
  },
  {
    "text": "So the probability\nof y equals to x--",
    "start": "3722600",
    "end": "3727980"
  },
  {
    "text": "probability of y equal to k is--",
    "start": "3727980",
    "end": "3733338"
  },
  {
    "text": "given x and theta has\nthe following form, it's x of theta k dot x over\nsum k sum j exp theta j dot x.",
    "start": "3733339",
    "end": "3746750"
  },
  {
    "text": "OK? j goes from 1 to k. OK, I just described the\nprocedure in elaborate detail",
    "start": "3746750",
    "end": "3754289"
  },
  {
    "text": "here. But this is basically\nwhat's going on. This is exactly what's going on. So basically-- please.",
    "start": "3754289",
    "end": "3762760"
  },
  {
    "text": "[INAUDIBLE] So two things.",
    "start": "3762760",
    "end": "3768359"
  },
  {
    "text": "One is this makes\nsure that everything is-- so exactly as we\nwere talking about before, these are each--\nthink about these",
    "start": "3768359",
    "end": "3773609"
  },
  {
    "text": "as each like a logistic\nregression model. So in the logistic\nregression model, you take this, and get\nthe natural parameter,",
    "start": "3773609",
    "end": "3778619"
  },
  {
    "text": "then you exponentiate it. If you had an offset, like if\nyou had some function there, you would still need, just as\nwe did in the exponential model,",
    "start": "3778619",
    "end": "3785829"
  },
  {
    "text": "you need to make it a\nprobability distribution. So that's where the z comes up. So the exp is\nbecause we're doing",
    "start": "3785830",
    "end": "3791119"
  },
  {
    "text": "these general linear\nmodels, and that gives us the nice kind\nof functional form that we wanted\nunderneath the covers that we've been using in\nall our predictive problems",
    "start": "3791119",
    "end": "3797359"
  },
  {
    "text": "for in the class or not. Think about logistic\nregression binary or not. And then we have many\ndifferent scores,",
    "start": "3797359",
    "end": "3802720"
  },
  {
    "text": "and then the procedure is\njust to normalize them, OK? And this kind of\nmakes sense, right?",
    "start": "3802720",
    "end": "3807740"
  },
  {
    "text": "This is saying like how strong\na classification I am, right? If these cats-- LIKE if\nthere's a cat way over here,",
    "start": "3807740",
    "end": "3812849"
  },
  {
    "text": "maybe this is a super-cat. Like, the clearest picture\nof a cat you've ever seen. In that case, like, it should\nget a really high score,",
    "start": "3812849",
    "end": "3818330"
  },
  {
    "text": "and you should be\nreally confident in it. That's the intuition. Now, is that always true? No, certainly not. Picture of a horse\ncould show up over here.",
    "start": "3818330",
    "end": "3826430"
  },
  {
    "text": "We hope it doesn't. But mechanically, is\nthis clear what happens? Right, please. Last class, I asked this to you,\nand you said that ultimately we",
    "start": "3826430",
    "end": "3835130"
  },
  {
    "text": "will have to do one versus\nall classification for each of these classes. So I think that's true here.",
    "start": "3835130",
    "end": "3842318"
  },
  {
    "text": "Yeah. Yeah, so-- exactly. So this is basically a\ncompact form of what we call one-versus-all,\nwhich is like, OK,",
    "start": "3842319",
    "end": "3848369"
  },
  {
    "text": "how confident is\nmy cat detector, how confident is\nmy dog detector, how confident is my,\nwhatever this is, car--",
    "start": "3848369",
    "end": "3854579"
  },
  {
    "text": "I don't know what that is. Yeah, car detector. Does that make sense? And so you just bake off\ntheir relative strengths.",
    "start": "3854580",
    "end": "3861299"
  },
  {
    "text": "That's how you do multiclass\nclassification at once. All right, so how\ndo you train this?",
    "start": "3861299",
    "end": "3867980"
  },
  {
    "text": "Well, what you're given-- because you're given something\nthat looks like this.",
    "start": "3867980",
    "end": "3879609"
  },
  {
    "text": "So 1, 2, 3, put 4 in there.",
    "start": "3879609",
    "end": "3888200"
  },
  {
    "text": "You're told it's a cat. You have probability 1 here,\nand you have 0 everywhere else. This is what you're\nactually given. That's what the\nlabel looks like, OK?",
    "start": "3888200",
    "end": "3895819"
  },
  {
    "text": "Your probability, your\np hat, your estimate will not look like that. They will actually\nlook like, oh, I'm",
    "start": "3895819",
    "end": "3903339"
  },
  {
    "text": "pretty confident it's a cat,\nbut I have a little pieces of each one of the others--",
    "start": "3903339",
    "end": "3912190"
  },
  {
    "text": "This is that inference time. What I'm trying to get\nacross is, when you actually",
    "start": "3912190",
    "end": "3918970"
  },
  {
    "text": "look at these things. They will give you\nsmall probabilities that it's everywhere because\nit's doing this normalization.",
    "start": "3918970",
    "end": "3925160"
  },
  {
    "text": "OK? Now, one thing that\npeople do by the way,",
    "start": "3925160",
    "end": "3930779"
  },
  {
    "text": "is something they\ncall label smoothing. They take this, and they\npush a small amount of mass everywhere else, right?",
    "start": "3930779",
    "end": "3937049"
  },
  {
    "text": "So you take the\none, and you kind of say, like, I'm going to put\na small, tiny amount of mass everywhere else. And that's basically\nto account for the fact",
    "start": "3937050",
    "end": "3943130"
  },
  {
    "text": "that your labels are\noften wrong, right? Even very popular,\nwell-studied benchmarks",
    "start": "3943130",
    "end": "3948900"
  },
  {
    "text": "will have 3% of their labels\nbe wrong or something. So you can imagine how\nyou would kind of smooth, and why that would\nbe bad when you're",
    "start": "3948900",
    "end": "3954880"
  },
  {
    "text": "training a system\nif you're like, it's definitely not a cat. You're like, no, there's a\nsmall possibility it's a cat. I should admit that possibility.",
    "start": "3954880",
    "end": "3960980"
  },
  {
    "text": "It's a very different statement\nif you imagine those two.",
    "start": "3960980",
    "end": "3966340"
  },
  {
    "text": "OK, awesome. So what happens here?",
    "start": "3966340",
    "end": "3971440"
  },
  {
    "text": "Well, the great thing\nis is, like, this follows exactly what we've\nbeen doing the whole time.",
    "start": "3971440",
    "end": "3978960"
  },
  {
    "text": "So we now introduce the--\nthis is also sometimes the cross-entropy term, right,\nwhich is equal to this guy.",
    "start": "3978960",
    "end": "3986180"
  },
  {
    "text": "But this follows basically\nour basic recipe, which is-- so py equals k\ntimes log of p hat of y.",
    "start": "3986180",
    "end": "3999588"
  },
  {
    "text": "This equals minus\nlog py hat of yi.",
    "start": "3999589",
    "end": "4007700"
  },
  {
    "text": "So this is the\nground truth label. OK, so this is in case\nwe don't do smoothing,",
    "start": "4007700",
    "end": "4015579"
  },
  {
    "text": "basically our loss, to\nminimize the cross-entropy, is the same as minimizing\nthe log of our expected",
    "start": "4015579",
    "end": "4021210"
  },
  {
    "text": "probability. And that thing has\na very fancy name. That's a logit.",
    "start": "4021210",
    "end": "4026250"
  },
  {
    "text": "And that's the-- you'll see\nthese negative log likelihood",
    "start": "4026250",
    "end": "4034490"
  },
  {
    "text": "things all over machine\nlearning packages. That's what they're doing, OK? And this equals-- just\nso we're super clear--",
    "start": "4034490",
    "end": "4040980"
  },
  {
    "text": "it equals minus log of x data\ni dot x over sum exp theta j",
    "start": "4040980",
    "end": "4050750"
  },
  {
    "text": "dot x. j goes from 1 to k, OK? And this is what we minimize. And that's it.",
    "start": "4050750",
    "end": "4057520"
  },
  {
    "text": "And so how do you\nsolve this model? Just one gradient descent.",
    "start": "4057520",
    "end": "4063048"
  },
  {
    "text": "All right? That's it. All right, any\nquestions about this?",
    "start": "4063049",
    "end": "4073339"
  },
  {
    "text": "What part of that\nis the logit again?",
    "start": "4073339",
    "end": "4078540"
  },
  {
    "text": "Oh, the logit is\na log probability. Great question. So this is the terminology\nfor a log probability.",
    "start": "4078540",
    "end": "4087319"
  },
  {
    "text": "So this thing comes up--\nthe reason I call it that is you'll probably\nencounter that term. I just wanted you to\nbe familiar with it.",
    "start": "4087319",
    "end": "4092579"
  },
  {
    "text": "And you don't usually predict-- you don't actually write out\nthe probability functions. You don't take the exp.",
    "start": "4092579",
    "end": "4097678"
  },
  {
    "text": "You actually just take the\nlog of those probabilities, and that's what you\nactually minimize. And so you will use them,\nand they're in log scale.",
    "start": "4097679",
    "end": "4103318"
  },
  {
    "text": "So you'll often see\nmachine learning code spit out these negative",
    "start": "4103319",
    "end": "4109139"
  },
  {
    "text": "That's what they are. They're logits. You'll see that term,\nlogit, everywhere. Yes, please.",
    "start": "4109140",
    "end": "4115349"
  },
  {
    "text": "Why is there a negative sign? Because otherwise it\nwould be a maximization.",
    "start": "4115350",
    "end": "4121270"
  },
  {
    "text": "Yeah, it's just to\nmake the function sign so that we have a minimization. Wonderful question. Yeah, please.",
    "start": "4121270",
    "end": "4127599"
  },
  {
    "text": "So that function that\nyou're minimizing-- Yeah. --it doesn't look like it has\nany dependence on the label,",
    "start": "4127600",
    "end": "4133270"
  },
  {
    "text": "though. Oh, awesome question. What a wonderful question. Yeah, it's hidden right here. This theta i, this i is\nthe ground truth i, right?",
    "start": "4133270",
    "end": "4140930"
  },
  {
    "text": "That's what this statement is. This is the ground truth. Wonderful. So that's not your perimeter-- No, it's just like you\npicked out that one,",
    "start": "4140930",
    "end": "4147048"
  },
  {
    "text": "and then you get-- your loss function kind of\nlike perfectly encodes it. If you put in the label\nsmoothing, then in fact,",
    "start": "4147049",
    "end": "4152699"
  },
  {
    "text": "it's not just 1\ntheta i that's there. They each get a weighting\nassociated with them as well.",
    "start": "4152700",
    "end": "4157849"
  },
  {
    "text": "But here, that was the trick. This yi is the same as\nthe actual ground truth. Wonderful, wonderful question.",
    "start": "4157850",
    "end": "4164278"
  },
  {
    "text": "[INAUDIBLE] Yeah, great question.",
    "start": "4164279",
    "end": "4173759"
  },
  {
    "text": "Yeah, so here it's the\ntheta x, and then I'm",
    "start": "4173760",
    "end": "4179028"
  },
  {
    "text": "taking the exponential. But you would also have\nan x1 over x thing, yeah. But here, for this\nmodel, I don't do that.",
    "start": "4179029",
    "end": "4186390"
  },
  {
    "text": "So what you're saying\nis an alternate model where you take all of\nthe logistic regressions, and you just say, like, what's\nthe probability of each one,",
    "start": "4186390",
    "end": "4192680"
  },
  {
    "text": "and then you compare them. And I'm saying, no, you do it\nin terms of the logits here, and this is how you break\nthem off all at the same time.",
    "start": "4192680",
    "end": "4199520"
  },
  {
    "text": "The difference between\nthem is not super critical, but this is the one we use. But it's great to point\nthat they're different.",
    "start": "4199520",
    "end": "4204620"
  },
  {
    "text": "So I did kind of, by\nsleight of hand here, I said, oh, this is like you're\ndoing logistic regression. But this, again,\nis like saying--",
    "start": "4204620",
    "end": "4211449"
  },
  {
    "text": "so if you think about\nlogistic regression as there's a yes\nclass and a no class. And the yes class\nI have, the weight",
    "start": "4211449",
    "end": "4217230"
  },
  {
    "text": "is x of theta T to the x\nThis is each saying that. This is the weight of every\none of those possible worlds,",
    "start": "4217230",
    "end": "4222928"
  },
  {
    "text": "and then I'm summing\nover all of them. And so that's why\nthis is consistent with logistic regression. Imagine a null world\nthat occurred here",
    "start": "4222929",
    "end": "4229960"
  },
  {
    "text": "that said it's either the class And so its feature I'm\njust going to default",
    "start": "4229960",
    "end": "4235180"
  },
  {
    "text": "to 1, because I don't care. It could be any scalar. Then that gets you exactly\nback to logistic regression. Does that make the\nconnection clear?",
    "start": "4235180",
    "end": "4240530"
  },
  {
    "text": "Yeah. So they really are the same. I just derive them in a\nslightly different way.",
    "start": "4240530",
    "end": "4245889"
  },
  {
    "text": "Wonderful question. [INAUDIBLE] something we've seen before,\na modification of that?",
    "start": "4245890",
    "end": "4253320"
  },
  {
    "text": "Or is this a new\none [INAUDIBLE]?? Yeah, so if you've seen--\nso this is a new function,",
    "start": "4253320",
    "end": "4262020"
  },
  {
    "text": "I guess, for us potentially. We have seen it because\nof the discussion we just had in another guise. It actually is-- the\nbinary cross-entropy",
    "start": "4262020",
    "end": "4268599"
  },
  {
    "text": "is the logistic function. And so this is a\ngeneralization of that. And if you've ever seen\nentropy or cross-entropy,",
    "start": "4268600",
    "end": "4274850"
  },
  {
    "text": "which I think-- it doesn't matter if you've\nseen it or not, this is it. It's just a functional\nform that we care about. It's kind of a distance between\nprobability distributions.",
    "start": "4274850",
    "end": "4281770"
  },
  {
    "text": "Yeah. I care-- the reason\nI say this is not because I think there's\nsomething mystical here.",
    "start": "4281770",
    "end": "4287449"
  },
  {
    "text": "So if you haven't\nseen entropy before, I guess this would\nbe mysterious. And potentially even if you\nhave seen entropy before, because entropy is\na mysterious thing.",
    "start": "4287450",
    "end": "4294430"
  },
  {
    "text": "But this is the loss\nfunction that you use, and it's-- the reason you use\nit is because it generalizes in the way we just talked about.",
    "start": "4294430",
    "end": "4300770"
  },
  {
    "text": "I don't think you need\nto know any of that. It's fine. Please.",
    "start": "4300770",
    "end": "4306000"
  },
  {
    "text": "Just to clarify, so where you\nhave the clusters-- so the x in that is x of y, right? There's no y in this\ndiagram, but yeah.",
    "start": "4306000",
    "end": "4312430"
  },
  {
    "text": "So what does the x equal? So this-- here, x is a\ntwo-dimensional vector. So this is a little\nbit of a weird plot,",
    "start": "4312430",
    "end": "4318560"
  },
  {
    "text": "right? x is a cat as a\ntwo-dimensional vector, because I only have two\ndimensions to draw on, and so I put them there.",
    "start": "4318560",
    "end": "4323730"
  },
  {
    "text": "And that's why this picture\nlooks a little bit different than maybe you're thinking about\nit as a function, or a curve or something. But that's the difference.",
    "start": "4323730",
    "end": "4329760"
  },
  {
    "text": "So [INAUDIBLE] features with the\nhigher dimensions, but it's-- Exactly right.",
    "start": "4329760",
    "end": "4335150"
  },
  {
    "text": "So in higher dimensions,\nwhat you would expect is, rather than lines which\nare one-dimensional, you'd expect D minus",
    "start": "4335150",
    "end": "4340300"
  },
  {
    "text": "were separating everything\nout-- hyperplanes. And then they would be-- live\non one side or the other, and you would care about the\ndistance, effectively, there.",
    "start": "4340300",
    "end": "4346949"
  },
  {
    "text": "And so I'm drawing like\nthe contour representation of the function, right? Yeah. Awesome question.",
    "start": "4346949",
    "end": "4352650"
  },
  {
    "text": "And then there's\nno-- the y's are encoded, as we were\njust discussing, by which index I use. For image recognition,\ndo you normally",
    "start": "4352650",
    "end": "4358290"
  },
  {
    "text": "work in [INAUDIBLE] space,\nor some different information like PCA?",
    "start": "4358290",
    "end": "4363699"
  },
  {
    "text": "Oh, awesome question, yeah. So we'll talk about PCA when\nI come back to join you,",
    "start": "4363699",
    "end": "4368949"
  },
  {
    "text": "and why we use that. PCA is a method. So the two things\nPCA fix for you is if you have x's that\nhave different scales,",
    "start": "4368949",
    "end": "4376960"
  },
  {
    "text": "or have different meanings. Like if all your temperatures\nare between 80 and 82 degrees,",
    "start": "4376960",
    "end": "4382090"
  },
  {
    "text": "but they're really\nsignificant, PCA is a way of centering\nand whitening your data. Meaning, like, subtracting off\nthe mean, and standardizing,",
    "start": "4382090",
    "end": "4387881"
  },
  {
    "text": "and normalizing that. And so that's a technique that\nis very, very commonly used in statistical analysis.",
    "start": "4387881",
    "end": "4393250"
  },
  {
    "text": "And so we'll talk about that,\nand how to probably find that, and what its\njustifications for here. When you're doing things\nlike image analysis,",
    "start": "4393250",
    "end": "4399150"
  },
  {
    "text": "actually, the methods\nhave been more toward raw features\nand raw pixels over the last couple of\nyears where you-- the things",
    "start": "4399150",
    "end": "4405900"
  },
  {
    "text": "that we're all excited\nabout is to try and have no-hand coding in the pipeline. You can talk\nphilosophically about why",
    "start": "4405900",
    "end": "4411600"
  },
  {
    "text": "we're obsessed with this. But basically, the newest\nmodels just take the image raw, and they try to\nhave as minimal what",
    "start": "4411600",
    "end": "4417451"
  },
  {
    "text": "we call-- they call\nit inductive bias, but I won't define that term. But as minimal of\ninformation about the model",
    "start": "4417451",
    "end": "4422520"
  },
  {
    "text": "to learn from them. So one weird fact that got me\nvery excited a year or two ago",
    "start": "4422520",
    "end": "4427530"
  },
  {
    "text": "was that we have\none model for text, and we're using that same\nmodel, and getting nearly state-of-the-art accuracy\non text, and images,",
    "start": "4427530",
    "end": "4432889"
  },
  {
    "text": "and audio in a bunch\nof different places. And that is the thing\nthat's really interesting, and it starts from\nthe raw pixels. And it learns the\nedge detectors,",
    "start": "4432889",
    "end": "4439510"
  },
  {
    "text": "and all the stuff we\nused to do by hand. So that's been the trend. Here we'll talk a little\nbit more about feature prep,",
    "start": "4439510",
    "end": "4445860"
  },
  {
    "text": "because that stuff\ndoesn't always work. And when it breaks, you want to\nhave a library to fall back on. Wonderful question.",
    "start": "4445860",
    "end": "4451080"
  },
  {
    "text": "But that will be in, like,\nI guess, week four or five. Awesome. Please. What if you [INAUDIBLE]",
    "start": "4451080",
    "end": "4456300"
  },
  {
    "text": "Oh my God, what\na great question. Yeah, what if you can't\ndraw this pretty picture,",
    "start": "4456300",
    "end": "4461570"
  },
  {
    "text": "it doesn't make sense? Yeah, that's possible. So what if your\nfeatures are bad, right? What if it turns out that\nyour features about cats",
    "start": "4461570",
    "end": "4468590"
  },
  {
    "text": "were like, they\nsleep on couches? And you're like, well, a dog\nsometimes sleeps on couches. Then you couldn't possibly\nseparate the dogs in the cats.",
    "start": "4468590",
    "end": "4474511"
  },
  {
    "text": "It's a stupid example, but\nlike your features could be so weak that they're not\nable to actually separate",
    "start": "4474512",
    "end": "4481600"
  },
  {
    "text": "your class. You could imagine putting\na lot more features in, and that's why these\nmodels that have",
    "start": "4481600",
    "end": "4486830"
  },
  {
    "text": "bigger and bigger\ndimensions come in, to separate automatically\nall the different classes. But now your real question\nis like, OK-- well, OK,",
    "start": "4486830",
    "end": "4494178"
  },
  {
    "text": "there's a fix. But I have my features,\nand I train, what happens? You just make misclassification,\nand that's actually",
    "start": "4494179",
    "end": "4500080"
  },
  {
    "text": "the default. You have a\nsmall number of features, and then you fit those features,\nand you do the best you can.",
    "start": "4500080",
    "end": "4506409"
  },
  {
    "text": "So like if a cat\njumped over here-- there was a cat that was\nhere, you just misclassify it,",
    "start": "4506409",
    "end": "4512600"
  },
  {
    "text": "and you get it wrong, right? Go from here over to here,\nand then you be toast. So that does happen quite a bit,\nsometimes due to label error.",
    "start": "4512600",
    "end": "4519719"
  },
  {
    "text": "And sometimes like-- there's\na subfield of machine learning that is kind of\nobsessed with this,",
    "start": "4519719",
    "end": "4525320"
  },
  {
    "text": "and my students\nwrite papers in it. There's a great benchmark\ncalled [? WILDS ?] from Percy, and Chelsea,\nand a couple of other folks",
    "start": "4525320",
    "end": "4531190"
  },
  {
    "text": "on the faculty,\n[INAUDIBLE] where machine learning picks out the\nwrong features systematically. Like you take a bird that\nnormally is on water,",
    "start": "4531190",
    "end": "4538698"
  },
  {
    "text": "and you photograph it\nwith a land background. And the machine learning\nmodel is like, oh, that's a land bird, not a water bird.",
    "start": "4538699",
    "end": "4544219"
  },
  {
    "text": "There's a lot of that going on. So that absolutely 100% happens. Wonderful question.",
    "start": "4544219",
    "end": "4550699"
  },
  {
    "text": "Awesome. Any other questions? All right. So just to recap,\nwhat did we do today?",
    "start": "4550699",
    "end": "4560270"
  },
  {
    "text": "We went through this\nexponential family of models, and now we've\nhopefully tied in a bow the fact that we have\nthis method to our madness",
    "start": "4560270",
    "end": "4565349"
  },
  {
    "text": "about doing binary,\nand the real value-- we went real value\nin binary, because we had seen fitting lines.",
    "start": "4565350",
    "end": "4570630"
  },
  {
    "text": "We did classification,\nand we tied them all up and these exponential\nfamily models. We talked about why inference\nand learning were basically",
    "start": "4570630",
    "end": "4577699"
  },
  {
    "text": "the same in these\nmodels, and that let us generalize to\na whole host of them. This is the workhorse of\nsupervised machine learning.",
    "start": "4577699",
    "end": "4584040"
  },
  {
    "text": "But the questions\nthat you're asking now are exactly the right questions. Where do these\nfeatures come from? What if the features\ndon't fit the data?",
    "start": "4584040",
    "end": "4590370"
  },
  {
    "text": "How do I get more\nexpressive things? You're going to see\nthings about kernels, and SVM, and neural nets in\nthe next couple of lectures.",
    "start": "4590370",
    "end": "4596679"
  },
  {
    "text": "And that will tell you, how\ndo you pick your features, and how do you get to these\nmore expressive models? And that will form the bulk of\nsupervised machine learning.",
    "start": "4596679",
    "end": "4603260"
  },
  {
    "text": "Then the next section,\nwe'll come to unsupervised, and we'll forget the whys,\nand figure out what structure, can we get there.",
    "start": "4603260",
    "end": "4608800"
  },
  {
    "text": "And that's when we'll get\ninto questions like PCA. We'll get into these\nquestions about what are called exponential\nfamily models, or EM models, where you\nhave a supervised predictor,",
    "start": "4608800",
    "end": "4616480"
  },
  {
    "text": "and an unsupervised thing. And this allows you to do some\npretty wild stuff, like fit",
    "start": "4616480",
    "end": "4621820"
  },
  {
    "text": "data from quasars and stuff. And we'll walk through\nall that stuff. And in fact, that\nwe'll also have a thing",
    "start": "4621820",
    "end": "4627719"
  },
  {
    "text": "on self-supervision,\nwhich is a new lecture -- just this time. I guess my student\n[AUDIO OUT] gave it last year,",
    "start": "4627719",
    "end": "4633408"
  },
  {
    "text": "but [INAUDIBLE] is going\nto do it this year. Thanks so much for your\ntime and attention. Have a wonderful\nrest of your week.",
    "start": "4633409",
    "end": "4637869"
  }
]