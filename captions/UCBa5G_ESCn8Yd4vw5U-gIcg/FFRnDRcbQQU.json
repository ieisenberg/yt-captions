[
  {
    "start": "0",
    "end": "57000"
  },
  {
    "start": "0",
    "end": "5430"
  },
  {
    "text": "OK. Hi, everyone. So we'll get\nstarted again, we're now into week seven of CS224N.",
    "start": "5430",
    "end": "14080"
  },
  {
    "text": "If you're following along\nthe syllabus really closely, we actually did a little bit\nof a rearrangement of classes.",
    "start": "14080",
    "end": "21100"
  },
  {
    "text": "And so today it's\nme, and I'm going to talk about coreference\nresolution, which",
    "start": "21100",
    "end": "26910"
  },
  {
    "text": "is another chance we get to\ntake a deeper dive into a more linguistic topic. That will also show you a couple\nof new things for deep learning",
    "start": "26910",
    "end": "35010"
  },
  {
    "text": "models at the same time. And then the lecture\nthat had previously been scheduled at\nthis point, which",
    "start": "35010",
    "end": "41550"
  },
  {
    "text": "was going to be John on\nexplanation in neural models, is being shifted later down\ninto week 9, I think it is.",
    "start": "41550",
    "end": "50489"
  },
  {
    "text": "But you'll still get him later. Before getting underway, just\na couple of announcements",
    "start": "50490",
    "end": "56070"
  },
  {
    "text": "on things. Well first of all,\ncongratulations on surviving\nassignment five I hope,",
    "start": "56070",
    "end": "62790"
  },
  {
    "text": "I know it was a bit of a\nchallenge for some of you, but I hope it was a rewarding\nstate of the art learning",
    "start": "62790",
    "end": "68760"
  },
  {
    "text": "experience on the\nlatest in neural nets. And in any rate, this was\na brand new assignment",
    "start": "68760",
    "end": "74730"
  },
  {
    "text": "that we used for the\nfirst time this year. So we'll really\nappreciate later on when we do the second survey,\ngetting your feedback on that.",
    "start": "74730",
    "end": "82860"
  },
  {
    "text": "We've been busy reading people's\nfinal project proposals, thanks. Lots of interesting stuff there.",
    "start": "82860",
    "end": "88860"
  },
  {
    "text": "Our goal is to get them\nback to you tomorrow. But as soon as you've had a good\nnight's sleep after assignment",
    "start": "88860",
    "end": "95610"
  },
  {
    "text": "5, now is also a great\ntime to get started working on your final\nprojects, because there's",
    "start": "95610",
    "end": "101460"
  },
  {
    "text": "just not that much time\ntill the end of quarter. And I particularly want\nto encourage all of you",
    "start": "101460",
    "end": "106860"
  },
  {
    "text": "to chat to your mentor\nregularly, go and visit office hours and keep in\ntouch, get advice, just",
    "start": "106860",
    "end": "113490"
  },
  {
    "text": "talking through things is a\ngood way to keep you on track. We also plan to be getting\nback assignment 4 grades",
    "start": "113490",
    "end": "120000"
  },
  {
    "text": "later this week. There's sort of, the work\nnever stops at this point.",
    "start": "120000",
    "end": "125259"
  },
  {
    "text": "So the next thing for\nthe final project, is the final project milestone.",
    "start": "125260",
    "end": "131160"
  },
  {
    "text": "So that we handed out the\ndetails of that last Friday, and it's due a week from today.",
    "start": "131160",
    "end": "138190"
  },
  {
    "text": "So the idea of this\nfinal project milestone is really to help\nkeep you on track,",
    "start": "138190",
    "end": "144629"
  },
  {
    "text": "and keep things\nmoving towards having a successful final project. So our hope is that\nsort of most of what",
    "start": "144630",
    "end": "151710"
  },
  {
    "text": "you write for the final project\nmilestone is material you can also include in\nyour final project,",
    "start": "151710",
    "end": "157140"
  },
  {
    "text": "except for a few\nparagraphs of here's exactly where I'm up to now. So the overall hope is that\ndoing this in two parts",
    "start": "157140",
    "end": "165540"
  },
  {
    "text": "and having a milestone\nbefore the final thing, it's just making you\nmake progress and be",
    "start": "165540",
    "end": "170610"
  },
  {
    "text": "on track for having a\nsuccessful final project. Finally, the next\nclass on Thursday",
    "start": "170610",
    "end": "177990"
  },
  {
    "text": "is going to be Colin\nRaffel, and this is going to be super exciting. So he's going to be\ntalking more about the very",
    "start": "177990",
    "end": "184650"
  },
  {
    "text": "latest in large pre-trained\nlanguage models, both what some of their\nsuccesses are and also what",
    "start": "184650",
    "end": "191370"
  },
  {
    "text": "some of the\ndisconcerting, not quite so good aspects, of\nthose models are. So that should be a really\ngood interesting lecture, when",
    "start": "191370",
    "end": "199590"
  },
  {
    "text": "we had him come and\ntalk to our NLP seminar, we had several hundred\npeople come along for that.",
    "start": "199590",
    "end": "207580"
  },
  {
    "text": "And so for this\ntalk again, we're asking that you write a\nreaction paragraph following",
    "start": "207580",
    "end": "214379"
  },
  {
    "text": "the same instructions as\nlast time about what's in this lecture.",
    "start": "214380",
    "end": "219930"
  },
  {
    "text": "And someone asked\nin the questions, well what about last Thursday's?",
    "start": "219930",
    "end": "225599"
  },
  {
    "text": "The answer to that is no. So the distinction here is,\nwe're only doing the reaction",
    "start": "225600",
    "end": "230610"
  },
  {
    "text": "paragraphs for outside\nguest speakers, and although it\nwas great to have",
    "start": "230610",
    "end": "236520"
  },
  {
    "text": "Antoine Bosselut for\nlast Thursday's lecture, he's a postdoc at Stanford.",
    "start": "236520",
    "end": "242010"
  },
  {
    "text": "So we don't count him as\nan outside guest speaker, and so nothing needs to\nbe done for that one.",
    "start": "242010",
    "end": "247510"
  },
  {
    "text": "So there are three classes\nfor which you need to do it. So there was the one before,\nfrom Danqi Chen, Colin Raffel",
    "start": "247510",
    "end": "257040"
  },
  {
    "text": "which is Thursday, and then\ntowards the end of the course there's Yulia Tsvetkov. ",
    "start": "257040",
    "end": "265610"
  },
  {
    "start": "265000",
    "end": "358000"
  },
  {
    "text": "OK. So this is the plan today,\nso in the first part of it I'm actually going to\nspend a bit of time talking",
    "start": "265610",
    "end": "272570"
  },
  {
    "text": "about what coreference is. What different kinds of\nreference in language are.",
    "start": "272570",
    "end": "278120"
  },
  {
    "text": "And then I'm going to move\non and talk about some of the kind of methods that\npeople have used for solving",
    "start": "278120",
    "end": "283850"
  },
  {
    "text": "coreference resolution. Now there's one bug in\nour course design, which",
    "start": "283850",
    "end": "291110"
  },
  {
    "text": "was a lot of years we've\nhad a whole lecture on doing convolutional neural nets\nfor language applications.",
    "start": "291110",
    "end": "298069"
  },
  {
    "text": "And that slight bug\nappeared the other day when Danqi talked\nabout the I to F model,",
    "start": "298070",
    "end": "308140"
  },
  {
    "text": "because she sort of slipped\nin oh, there's a character CNN representation of words, and we\nhadn't actually covered that.",
    "start": "308140",
    "end": "316100"
  },
  {
    "text": "And so that was a\nslight oopsie, I mean actually for\napplications and coreference as well people commonly make\nuse of character level ConvNets.",
    "start": "316100",
    "end": "324640"
  },
  {
    "text": "So I wanted to sort\nof spend a few minutes sort of doing basics of\nConvNets for language.",
    "start": "324640",
    "end": "331130"
  },
  {
    "text": "The sort of reality here\nis that, given that there's no exam week this\nyear, to give people",
    "start": "331130",
    "end": "337330"
  },
  {
    "text": "more time for final projects,\nwe sort of shortened the content by a week this year, and so\nyou're getting a little bit",
    "start": "337330",
    "end": "343750"
  },
  {
    "text": "less of that content. Then going on from\nthere, say some stuff",
    "start": "343750",
    "end": "350200"
  },
  {
    "text": "about the state of the art\nneural coreference system, and right at the end, talk\nabout how coref is evaluated",
    "start": "350200",
    "end": "356860"
  },
  {
    "text": "and what some of\nthe results are. Yeah, so first of all, what is\nthis conference resolution term",
    "start": "356860",
    "end": "363490"
  },
  {
    "start": "358000",
    "end": "383000"
  },
  {
    "text": "that I've been\ntalking about a lot. So coreference\nresolution is meaning",
    "start": "363490",
    "end": "370390"
  },
  {
    "text": "to find all the mentions\nin a piece of text that refer to the same entity.",
    "start": "370390",
    "end": "376889"
  },
  {
    "text": "And so that's a typo. It should be in the\nworld not in the word. So let's make this\nconcrete, so here's",
    "start": "376890",
    "end": "384699"
  },
  {
    "start": "383000",
    "end": "780000"
  },
  {
    "text": "part of a short story by\nShruthi Rao called The Star. Now I have to make\na confession here,",
    "start": "384700",
    "end": "390820"
  },
  {
    "text": "because this is an NLP class,\nnot a literature class,",
    "start": "390820",
    "end": "396280"
  },
  {
    "text": "I crudely made some\ncuts to the story to be able to have relevant\nparts appear on my slide,",
    "start": "396280",
    "end": "403630"
  },
  {
    "text": "in a decent sized font for\nillustrating coreference. So it's not quite the\nfull original text,",
    "start": "403630",
    "end": "409420"
  },
  {
    "text": "but it basically is a\npiece of this story.",
    "start": "409420",
    "end": "414560"
  },
  {
    "text": "Right, so what we do in\ncoreference resolution is, we're working out what\npeople are mentioned, OK.",
    "start": "414560",
    "end": "422380"
  },
  {
    "text": "So here's a mention\nof a person, Vanaja, and here's a mention of\nanother person, Akhila.",
    "start": "422380",
    "end": "428919"
  },
  {
    "text": "And well, mentions don't\nhave to be people, right, so the local park,\nthat's also a mention.",
    "start": "428920",
    "end": "435100"
  },
  {
    "text": "And then here's Akhila\nagain, and Akhila's son,",
    "start": "435100",
    "end": "440200"
  },
  {
    "text": "and then there's\nPrajwal, then there's another son here, and\nthen her son, and Akash,",
    "start": "440200",
    "end": "450250"
  },
  {
    "text": "and they both went\nto the same school. And then there's\na pre-school play,",
    "start": "450250",
    "end": "458740"
  },
  {
    "text": "and there's Prajwal\nagain, and then",
    "start": "458740",
    "end": "463750"
  },
  {
    "text": "there's a naughty\nchild, Lord Krishna. And there's some that are\na bit complicated like,",
    "start": "463750",
    "end": "472340"
  },
  {
    "text": "the lead role, is\nthat a mention? It's sort of more of a\nfunctional specification",
    "start": "472340",
    "end": "477890"
  },
  {
    "text": "of something in the play. There's Akash, and it's a tree.",
    "start": "477890",
    "end": "484215"
  },
  {
    "text": "I won't go through\nthe whole thing yet, but I mean in general,\nthere are noun phrases",
    "start": "484215",
    "end": "489410"
  },
  {
    "text": "that are mentioning\nthings in the world. And so then what we want to\ndo for coreference resolution",
    "start": "489410",
    "end": "497780"
  },
  {
    "text": "is work out which\nof these mentions are talking about the\nsame real world entity.",
    "start": "497780",
    "end": "503250"
  },
  {
    "text": "So if we start off,\nso there's Vanaja. ",
    "start": "503250",
    "end": "511889"
  },
  {
    "text": "And so Vanaja is the\nsame person as her there,",
    "start": "511890",
    "end": "517809"
  },
  {
    "text": "and then we can read through. She resigned herself,\nso that's both Vanaja.",
    "start": "517809",
    "end": "527760"
  },
  {
    "text": "She bought him a brown T-shirt\nand brown trousers, then oops,",
    "start": "527760",
    "end": "534220"
  },
  {
    "text": "then she made a large cutout\ntree, she attached, right.",
    "start": "534220",
    "end": "542930"
  },
  {
    "text": "So all of that's about Vanaja. But then, we can\nhave another person.",
    "start": "542930",
    "end": "550380"
  },
  {
    "text": "So here's Akhila,\nand here's Akhila,",
    "start": "550380",
    "end": "559240"
  },
  {
    "text": "and maybe those are the\nonly mentions of Akhila.",
    "start": "559240",
    "end": "565220"
  },
  {
    "text": "So then we can go on from there. OK and so then, there's Prajwal,\nbut note that Prajwal is also",
    "start": "565220",
    "end": "582270"
  },
  {
    "text": "Akhila's son. So really Akhila's\nson is also Prajwal.",
    "start": "582270",
    "end": "590500"
  },
  {
    "text": "And so an interesting\nthing here is that you can get nested\nsyntactic structure,",
    "start": "590500",
    "end": "596339"
  },
  {
    "text": "so that we have these\nsort of noun phrases. ",
    "start": "596340",
    "end": "603899"
  },
  {
    "text": "So that if overall, we have sort\nof this noun phrase Akhila's son Prajwal, which consists of\ntwo noun phrases in apposition,",
    "start": "603900",
    "end": "614089"
  },
  {
    "text": "here's Prajwal. And then for the noun\nphrase Akhila's son,",
    "start": "614090",
    "end": "619449"
  },
  {
    "text": "it sort of breaks down to itself\nhaving an extra possessive noun phrase in it and then a noun,\nso that you have Akhila's, and",
    "start": "619450",
    "end": "630640"
  },
  {
    "text": "then this is son. So that you have these\nmultiple noun phrases",
    "start": "630640",
    "end": "636399"
  },
  {
    "text": "and so that you can\nthen be sort of having different parts of this be\none person in the coreference",
    "start": "636400",
    "end": "646570"
  },
  {
    "text": "but this noun phrase\nhere referring to a different person\nin the coreference.",
    "start": "646570",
    "end": "652779"
  },
  {
    "text": "OK so back to Prajwal. ",
    "start": "652780",
    "end": "661899"
  },
  {
    "text": "All right, so well, there's\nsome easy other Prajwals, right, so it's Prajwal\nhere, and then you've",
    "start": "661900",
    "end": "673670"
  },
  {
    "text": "got some more\ncomplicated things. So one of the\ncomplicated cases here is that, we have they\nwent to the same school.",
    "start": "673670",
    "end": "683390"
  },
  {
    "text": "So that 'they' there\nis what gets referred",
    "start": "683390",
    "end": "688400"
  },
  {
    "text": "to as split antecedents,\nbecause the 'they' refers",
    "start": "688400",
    "end": "697470"
  },
  {
    "text": "to both Prajwal and Akash. And that's an\ninteresting phenomenon,",
    "start": "697470",
    "end": "704459"
  },
  {
    "text": "and so I could try and\nshow that somehow, I could put some splashes\nin or something",
    "start": "704460",
    "end": "710310"
  },
  {
    "text": "and I might get a\ndifferent color. Akash, we have\nAkash and her son,",
    "start": "710310",
    "end": "716940"
  },
  {
    "text": "and then this one's sort\nof both of them at once.  Right, so human languages\nhave this phenomenon",
    "start": "716940",
    "end": "724620"
  },
  {
    "text": "of split antecedents,\nbut one of the things that you should notice when we\nstart talking about algorithms",
    "start": "724620",
    "end": "733830"
  },
  {
    "text": "that people use for doing\ncoreference resolution is, that they make some\nsimplified assumptions",
    "start": "733830",
    "end": "740880"
  },
  {
    "text": "as to how they go about\ntreating the problem. And one of the simplifications\nthat most algorithms make",
    "start": "740880",
    "end": "750210"
  },
  {
    "text": "is for any noun phrase\nlike this pronoun, they, that's trying to work out\nwhat does it coreference with.",
    "start": "750210",
    "end": "758620"
  },
  {
    "text": "And the answer is one thing, and\nso actually most NLP algorithms",
    "start": "758620",
    "end": "764160"
  },
  {
    "text": "for coreference resolution just\ncannot get split antecedents right, any time it occurs in\na text, they guess something,",
    "start": "764160",
    "end": "772260"
  },
  {
    "text": "and they always get it wrong. So that's the bit of\na sad state of affairs but that's the\ntruth of how it is.",
    "start": "772260",
    "end": "780389"
  },
  {
    "start": "780000",
    "end": "996000"
  },
  {
    "text": "OK so then going ahead,\nwe have Akash here,",
    "start": "780390",
    "end": "785610"
  },
  {
    "text": "and then we have\nanother tricky one. So moving on from there,\nwe then have this 'a tree'",
    "start": "785610",
    "end": "795270"
  },
  {
    "text": "so well in this\ncontext of this story,",
    "start": "795270",
    "end": "802060"
  },
  {
    "text": "Akash is going to be the tree. So you could feel that it was\nOK to say well this tree is also",
    "start": "802060",
    "end": "813010"
  },
  {
    "text": "Akash. You could also feel that\nthat's a little bit weird and not want to do\nthat, and I mean",
    "start": "813010",
    "end": "820300"
  },
  {
    "text": "actually, different\npeople's coreference data sets differ in this.",
    "start": "820300",
    "end": "827769"
  },
  {
    "text": "So really that we're predicating\nan identity relationship here between Akash and the\nproperty of being a tree,",
    "start": "827770",
    "end": "836410"
  },
  {
    "text": "so do we regard the tree as\nthe same as Akash or not, and people make different\ndecisions there.",
    "start": "836410",
    "end": "842890"
  },
  {
    "text": "OK but then going ahead,\nwe have here's Akash and she bought him,\nso that's Akash.",
    "start": "842890",
    "end": "852190"
  },
  {
    "text": "And then we have Akash\nhere, and so then we go on.",
    "start": "852190",
    "end": "859100"
  },
  {
    "text": "OK. So then, if we don't regard\nthe tree as the same as Akash,",
    "start": "859100",
    "end": "869899"
  },
  {
    "text": "we have a tree here. But then note that the\nnext place over here,",
    "start": "869900",
    "end": "877069"
  },
  {
    "text": "where we have a mention of a\ntree, the best tree, that's",
    "start": "877070",
    "end": "884810"
  },
  {
    "text": "sort of really a functional\ndescription of possible trees,",
    "start": "884810",
    "end": "890760"
  },
  {
    "text": "making someone the\nbest tree it's not really referential to a tree.",
    "start": "890760",
    "end": "896250"
  },
  {
    "text": "And so it seems like that's\nnot really coreferent. But if we go on,\nthere's definitely",
    "start": "896250",
    "end": "904490"
  },
  {
    "text": "more mention of a tree, so when\nshe has made the tree, truly",
    "start": "904490",
    "end": "911209"
  },
  {
    "text": "the nicest tree, or well I'm not\nsure, is that one coreferent? It is definitely\nreferring to our tree,",
    "start": "911210",
    "end": "918200"
  },
  {
    "text": "but maybe this one\nagain is a sort of a functional description that\nisn't referring to the tree.",
    "start": "918200",
    "end": "926380"
  },
  {
    "text": "OK, and so maybe this\none though where's a tree",
    "start": "926380",
    "end": "936320"
  },
  {
    "text": "is referring to the tree. What I hope to have\nillustrated from this,",
    "start": "936320",
    "end": "942020"
  },
  {
    "text": "is most of the time when\nwe do coreference in NLP",
    "start": "942020",
    "end": "947210"
  },
  {
    "text": "we just make it look sort of\nlike the conceptual phenomenon",
    "start": "947210",
    "end": "953000"
  },
  {
    "text": "is kind of obvious. That there's a mention of\nSarah and then it says she,",
    "start": "953000",
    "end": "961250"
  },
  {
    "text": "and you say ah, they're\ncoreferent, this is easy. But if you actually start\nlooking at real text,",
    "start": "961250",
    "end": "969709"
  },
  {
    "text": "especially when you're\nlooking at something like this that is a\npiece of literature. The kind of phenomena you get\nfor coreference and overlapping",
    "start": "969710",
    "end": "979100"
  },
  {
    "text": "reference, and various other\nphenomena that I'll talk about, they actually get\npretty complex.",
    "start": "979100",
    "end": "987500"
  },
  {
    "text": "And there are a\nlot of hard cases that you actually\nhave to think about as to what things you think\nabout as coreferent or not.",
    "start": "987500",
    "end": "996820"
  },
  {
    "text": "OK, but basically we do want\nto be able to do something with coreference because it's\nuseful for a lot of things",
    "start": "996820",
    "end": "1006300"
  },
  {
    "text": "that we'd like to do in\nnatural language processing. So for one task that we've\nalready talked about,",
    "start": "1006300",
    "end": "1011730"
  },
  {
    "text": "question answering but\nequally for other tasks, such as summarization\ninformation extraction.",
    "start": "1011730",
    "end": "1017490"
  },
  {
    "text": "If you're doing\nsomething like reading through a piece of\ntext, and you've",
    "start": "1017490",
    "end": "1023100"
  },
  {
    "text": "got a sentence like\n'he was born in 1961' you really want to\nknow who 'he' refers",
    "start": "1023100",
    "end": "1029774"
  },
  {
    "text": "to, to know if this is a good\nanswer to the question of when",
    "start": "1029774",
    "end": "1036329"
  },
  {
    "text": "was Barack Obama born\nor something like that. It turns out also that it's\nuseful in machine translation.",
    "start": "1036329",
    "end": "1045599"
  },
  {
    "start": "1040000",
    "end": "1100000"
  },
  {
    "text": "So in most languages,\npronouns have",
    "start": "1045599",
    "end": "1050630"
  },
  {
    "text": "features for gender and\nnumber and in quite a lot of languages, nouns\nand adjectives",
    "start": "1050630",
    "end": "1059000"
  },
  {
    "text": "also show features of\ngender, number and case. And so when you're\ntranslating a sentence,",
    "start": "1059000",
    "end": "1066980"
  },
  {
    "text": "you want to be aware\nof these features and what is coreferent\nwith want to be able to get",
    "start": "1066980",
    "end": "1076940"
  },
  {
    "text": "the translations correct. So if you want to be able\nto work out a translation",
    "start": "1076940",
    "end": "1085010"
  },
  {
    "text": "and know whether saying\nAlicia likes Juan because he's smart or Alicia likes\nJuan because she's smart,",
    "start": "1085010",
    "end": "1092300"
  },
  {
    "text": "then you have to be sensitive\nto coreference relationships to be able to choose\nthe right translation.",
    "start": "1092300",
    "end": "1098840"
  },
  {
    "text": " When people build\ndialogue systems,",
    "start": "1098840",
    "end": "1104970"
  },
  {
    "text": "dialogue systems also have\nissues of coreference a lot of the time.",
    "start": "1104970",
    "end": "1111100"
  },
  {
    "text": "So if it's sort of book\ntickets to see James Bond, and the system replies, Spectre\nis playing near you at 2.00",
    "start": "1111100",
    "end": "1119790"
  },
  {
    "text": "and 3.00 today. Well there's actually\ncoreference relation, sorry, there's a reference relation\nbetween Spectre and James Bond",
    "start": "1119790",
    "end": "1127320"
  },
  {
    "text": "because Spectre is\na James Bond film, I'll come back to\nthat one in a minute. But then it's how many\ntickets would you like?",
    "start": "1127320",
    "end": "1134850"
  },
  {
    "text": "Two tickets for the\nshowing at 3.00. That 3 is not just the number\n3, that 3 is then a coreference",
    "start": "1134850",
    "end": "1145020"
  },
  {
    "text": "relationship back to the 3.00\nPM showing that was mentioned by the agent in the\ndialogue system.",
    "start": "1145020",
    "end": "1152730"
  },
  {
    "text": "So again, to\nunderstand these, we need to be understanding the\ncoreference relationships.",
    "start": "1152730",
    "end": "1159820"
  },
  {
    "text": "So how now can you go\nabout doing coreference? So the standard\ntraditional answer",
    "start": "1159820",
    "end": "1167220"
  },
  {
    "text": "which I will present\nfirst is, coreference is done in two steps. On the first step,\nwhat we do is detect",
    "start": "1167220",
    "end": "1175620"
  },
  {
    "text": "mentions in the piece of\ntext, and that's actually a pretty easy problem.",
    "start": "1175620",
    "end": "1180659"
  },
  {
    "text": "And then in the second\nstep, we work out how to cluster the\nmentions, so as",
    "start": "1180660",
    "end": "1186510"
  },
  {
    "text": "in my example from\nthe Shruthi Rao text, basically what you're\ndoing with coreference",
    "start": "1186510",
    "end": "1193200"
  },
  {
    "text": "is, you're building up these\nclusters, sets of mentions that refer to the same\nentity in the world.",
    "start": "1193200",
    "end": "1201690"
  },
  {
    "start": "1201000",
    "end": "1322000"
  },
  {
    "text": "So if we explore a\nlittle how we could do that, it's a 2-step\nsolution, the first part",
    "start": "1201690",
    "end": "1207890"
  },
  {
    "text": "was detecting the mentions. And so pretty much,\nthere are three kinds",
    "start": "1207890",
    "end": "1215370"
  },
  {
    "text": "of things, different\nkinds of noun phrases that can be mentions.",
    "start": "1215370",
    "end": "1220870"
  },
  {
    "text": "There are pronouns like\nI, your, it, she, him and also some demonstrative\npronouns like this and that",
    "start": "1220870",
    "end": "1227850"
  },
  {
    "text": "and things like that. There are explicitly\nnamed things, so things like Paris,\nJoe Biden, Nike.",
    "start": "1227850",
    "end": "1235470"
  },
  {
    "text": "And then there are\nplain noun phrases that describe things, so a\ndog, the big fluffy cat stuck",
    "start": "1235470",
    "end": "1242670"
  },
  {
    "text": "in the tree. And so all of these\nare things that we do like to identify as mentions.",
    "start": "1242670",
    "end": "1249240"
  },
  {
    "text": "And the straightforward way\nto identify these mentions is to use natural\nlanguage processing",
    "start": "1249240",
    "end": "1256770"
  },
  {
    "text": "tools, several of which\nwe've talked about already. So to work out pronouns,\nwe can use what's",
    "start": "1256770",
    "end": "1264750"
  },
  {
    "text": "called a part of speech tagger. I'll change this twice. ",
    "start": "1264750",
    "end": "1272630"
  },
  {
    "text": "We can use a part\nof speech tagger which we haven't really\nexplicitly talked about,",
    "start": "1272630",
    "end": "1278059"
  },
  {
    "text": "but you use when you\nbuild dependency parsers. So that first of all assigns\nparts of speech to each word,",
    "start": "1278060",
    "end": "1285350"
  },
  {
    "text": "and so that we can just\nfind the words that are pronouns For\nnamed entities, we",
    "start": "1285350",
    "end": "1292050"
  },
  {
    "text": "did talk just a little bit\nabout named entities recognizers as a use of sequence\nmodels for neural networks.",
    "start": "1292050",
    "end": "1298780"
  },
  {
    "text": "So we can pick out things like\nperson names and company names.",
    "start": "1298780",
    "end": "1303810"
  },
  {
    "text": "And then for the ones\nlike a big fluffy dog,",
    "start": "1303810",
    "end": "1309390"
  },
  {
    "text": "we could then be\nsort of picking out from syntactic\nstructure, noun phrases and regarding them as\ndescriptions of things.",
    "start": "1309390",
    "end": "1318660"
  },
  {
    "text": "So we could use\nall of these tools and those would give us\nbasically our mentions, it's a little bit\nmore subtle than that.",
    "start": "1318660",
    "end": "1325679"
  },
  {
    "start": "1322000",
    "end": "1418000"
  },
  {
    "text": "Because it turns out there are\nsome noun phrases and things",
    "start": "1325680",
    "end": "1331890"
  },
  {
    "text": "of all of those kinds\nwhich don't actually refer, so that they're not\nreferential in the world.",
    "start": "1331890",
    "end": "1338680"
  },
  {
    "text": "So when you say 'it is sunny'\nit doesn't really refer, when you make universal claims\nlike 'every student', well",
    "start": "1338680",
    "end": "1346710"
  },
  {
    "text": "'every student' isn't referring\nto something you can point to in the world. And more dramatically when you\nhave 'no student' and making",
    "start": "1346710",
    "end": "1354639"
  },
  {
    "text": "a negative universal claim, it's\nnot referential to anything. There are also things that you\ncan describe functionally which",
    "start": "1354640",
    "end": "1364260"
  },
  {
    "text": "don't have any clear reference. So if I say 'the best\ndonut in the world'",
    "start": "1364260",
    "end": "1370560"
  },
  {
    "text": "that that's a functional claim,\nbut it doesn't necessarily have reference. Like if I've\nestablished that I think",
    "start": "1370560",
    "end": "1379200"
  },
  {
    "text": "a particular kind of donut is\nthe best donut in the world, I could then say to you I ate\nthe best donut in the world",
    "start": "1379200",
    "end": "1389370"
  },
  {
    "text": "yesterday, and you know what I\nmean, it might have reference. But if I say something\nlike, I'm going around",
    "start": "1389370",
    "end": "1395100"
  },
  {
    "text": "to all the donut stores\ntrying to find the best donut in the world, then it\ndoesn't have any reference yet,",
    "start": "1395100",
    "end": "1400770"
  },
  {
    "text": "it's just a sort of a\nfunctional description I'm trying to satisfy. You also then have\nthings like quantities,",
    "start": "1400770",
    "end": "1407340"
  },
  {
    "text": "'100 miles' it's that quantity\nthat is not really something that has any\nparticular reference,",
    "start": "1407340",
    "end": "1413850"
  },
  {
    "text": "you can mark out 100\nmiles all sorts of places.",
    "start": "1413850",
    "end": "1418860"
  },
  {
    "start": "1418000",
    "end": "1461000"
  },
  {
    "text": "So how do we deal\nwith those things that aren't really mentions.",
    "start": "1418860",
    "end": "1423910"
  },
  {
    "text": "Well one way is, we could train\na machine learning classifier to get rid of those\nspurious mentions,",
    "start": "1423910",
    "end": "1431270"
  },
  {
    "text": "but actually mostly\npeople don't do that. Most commonly if you're using\nthis kind of pipeline model,",
    "start": "1431270",
    "end": "1440190"
  },
  {
    "text": "where you use the parser and\nthe Named Entity Recognizer, you regard everything that you\nfound as a candidate mention,",
    "start": "1440190",
    "end": "1447890"
  },
  {
    "text": "and then you try and\nrun your coref system. And some of them,\nlike those ones,",
    "start": "1447890",
    "end": "1453200"
  },
  {
    "text": "hopefully aren't made\ncoreferent with anything else. And so then you just discard\nthem at the end of the process.",
    "start": "1453200",
    "end": "1460910"
  },
  {
    "text": "Hi Chris? Yeah. Got an interesting question that\nlinguists experience on this,",
    "start": "1460910",
    "end": "1467150"
  },
  {
    "start": "1461000",
    "end": "1784000"
  },
  {
    "text": "a student asks,\ncan we say that 'it is sunny' has its\nreferent to the weather?",
    "start": "1467150",
    "end": "1474750"
  },
  {
    "text": "I think that's-- So that's a fair question. Yeah, so people\nhave actually tried",
    "start": "1474750",
    "end": "1483990"
  },
  {
    "text": "to suggest that when\nyou say 'it is sunny' it means the weather\nis sunny, but I",
    "start": "1483990",
    "end": "1492510"
  },
  {
    "text": "guess the majority opinion at\nleast is that isn't plausible.",
    "start": "1492510",
    "end": "1498880"
  },
  {
    "text": "And I mean for I\nguess many of you aren't native\nspeakers of English,",
    "start": "1498880",
    "end": "1504870"
  },
  {
    "text": "but similar phenomena occur\nin many other languages. I mean it just intuitively\ndoesn't seem plausible",
    "start": "1504870",
    "end": "1513270"
  },
  {
    "text": "when you say it's sunny\nor it's raining today, that you're really saying that\nas a shortcut for the weather",
    "start": "1513270",
    "end": "1521970"
  },
  {
    "text": "is raining today, it just seems\nlike really what the case is. English likes to have something\nfilling the subject position,",
    "start": "1521970",
    "end": "1530310"
  },
  {
    "text": "and when there's nothing better\nto fill the subject position, you stick it in there\nand get, it's raining.",
    "start": "1530310",
    "end": "1539040"
  },
  {
    "text": "And so in general,\nit's believed that you get this phenomenon of having\nthese empty dummy 'it's' that",
    "start": "1539040",
    "end": "1544950"
  },
  {
    "text": "appear in various places. I mean another place in which\nit seems like you clearly get dummy 'it's' is that,\nwhen you have clauses that",
    "start": "1544950",
    "end": "1554610"
  },
  {
    "text": "are subjects of a\nverb, you can move them to the end of the sentence. So if you have a sentence where\nyou put a clause in the subject",
    "start": "1554610",
    "end": "1562440"
  },
  {
    "text": "position, they normally, in\nEnglish, sound fairly awkward. So you have a\nsentence, something",
    "start": "1562440",
    "end": "1569850"
  },
  {
    "text": "like 'that CS224N\nis a lot of work is known by all students'\npeople don't normally",
    "start": "1569850",
    "end": "1578370"
  },
  {
    "text": "say that, the normal\nthing to do is to shift the clause to\nthe end of the sentence. But when you do that, you\nstick in the dummy 'it' to fill",
    "start": "1578370",
    "end": "1585060"
  },
  {
    "text": "the subject position, so you\nthen have 'it is known by all students that CS224N\nis a lot of work'.",
    "start": "1585060",
    "end": "1593590"
  },
  {
    "text": "So that's the general\nfeeling that this is a dummy 'it' that\ndoesn't have any reference.",
    "start": "1593590",
    "end": "1598649"
  },
  {
    "text": " OK, there's one more\nquestion, so if someone",
    "start": "1598650",
    "end": "1606000"
  },
  {
    "text": "says it is sunny\namong other things, and we ask how is the weather?",
    "start": "1606000",
    "end": "1611430"
  },
  {
    "text": "OK good point, you've\ngot me on that one. If someone says\n'how's the weather'",
    "start": "1611430",
    "end": "1616590"
  },
  {
    "text": "and you answer 'it\nis sunny' it then does seem like the 'it' is\nin reference to the weather.",
    "start": "1616590",
    "end": "1622830"
  },
  {
    "text": "I'll buy that, well\nyou know I guess, this is what our coreference\nsystems are built trying to do,",
    "start": "1622830",
    "end": "1629700"
  },
  {
    "text": "in situations like that they're\nmaking a decision if it's coreference or not. And I guess, what you'd\nwant to say in that case is,",
    "start": "1629700",
    "end": "1636870"
  },
  {
    "text": "it seems reasonable to\nregard this one as coreferent to that weather that\ndid appear before it.",
    "start": "1636870",
    "end": "1643560"
  },
  {
    "text": "I mean but that also\nindicates another reason to think that in the normal\ncase it's not coreferent, right,",
    "start": "1643560",
    "end": "1650370"
  },
  {
    "text": "because normally\npronouns are only used when their reference\nis established. That you've referred to a\nnoun like 'John is answering",
    "start": "1650370",
    "end": "1660750"
  },
  {
    "text": "questions' and then you can\nsay, 'he types really quickly'. And it'd seem odd to just start\nthe conversation by 'he types",
    "start": "1660750",
    "end": "1669240"
  },
  {
    "text": "really quickly' because it\ndoesn't have any established reference words. Whereas that doesn't\nseem to be the case,",
    "start": "1669240",
    "end": "1674980"
  },
  {
    "text": "it seems like you can\njust start a conversation by saying 'it's raining\nreally hard today'",
    "start": "1674980",
    "end": "1681540"
  },
  {
    "text": "and that doesn't\nsound odd at all. OK so I've presented\nthe traditional picture,",
    "start": "1681540",
    "end": "1693340"
  },
  {
    "text": "but this traditional\npicture doesn't mean something that was done\nlast millennium before you",
    "start": "1693340",
    "end": "1698680"
  },
  {
    "text": "were born. I mean, essentially that was\nthe picture until about 2016.",
    "start": "1698680",
    "end": "1708460"
  },
  {
    "text": "That essentially every\ncoreference system that was built used tools like\npart of speech taggers, NER",
    "start": "1708460",
    "end": "1715299"
  },
  {
    "text": "systems and parsers\nto analyze sentences, to identify mentions,\nand to give you",
    "start": "1715300",
    "end": "1721270"
  },
  {
    "text": "features to\ncoreference resolution, and I'll show a bit\nmore about that later.",
    "start": "1721270",
    "end": "1726670"
  },
  {
    "text": "But more recently in\nour neural systems, people have moved to avoiding\ntraditional pipeline systems,",
    "start": "1726670",
    "end": "1735630"
  },
  {
    "text": "and are doing one shot end\nto end coreference resolution",
    "start": "1735630",
    "end": "1741180"
  },
  {
    "text": "systems. So if I skip directly\nto the second bullet,",
    "start": "1741180",
    "end": "1746730"
  },
  {
    "text": "there's a new generation\nof neural systems where you just start with\nyour sequence of words,",
    "start": "1746730",
    "end": "1752430"
  },
  {
    "text": "and you do the\nmaximally dumb thing. You just say let's\ntake all spans commonly",
    "start": "1752430",
    "end": "1759180"
  },
  {
    "text": "with some heuristics\nfor efficiency, but conceptually all\nsubsequences of this sentence,",
    "start": "1759180",
    "end": "1765480"
  },
  {
    "text": "they might be mentions. Let's feed them in to\na neural network which",
    "start": "1765480",
    "end": "1770820"
  },
  {
    "text": "will simultaneously\ndo mention detection and coreference resolution\nend to end in one model.",
    "start": "1770820",
    "end": "1777360"
  },
  {
    "text": "And I'll give an example\nof that kind of system later in the lecture. ",
    "start": "1777360",
    "end": "1783720"
  },
  {
    "text": "OK is everything good to\nthere and I should go on? ",
    "start": "1783720",
    "end": "1790059"
  },
  {
    "text": "OK. So I'm going to get on to how\nto do conference resolution",
    "start": "1790060",
    "end": "1797020"
  },
  {
    "text": "systems, but before I\ndo that, I do actually want to show a little bit\nmore of the linguistics",
    "start": "1797020",
    "end": "1803919"
  },
  {
    "text": "of coreference. Because there are actually a\nfew more interesting things",
    "start": "1803920",
    "end": "1808960"
  },
  {
    "text": "to understand and\nknow here, I mean when we say coreference\nresolution, we really",
    "start": "1808960",
    "end": "1816130"
  },
  {
    "text": "confuse together two\nlinguistic things which are overlapping, but different.",
    "start": "1816130",
    "end": "1823218"
  },
  {
    "text": "And so it's really actually good\nto understand the difference between these things. So there are two\nthings that can happen.",
    "start": "1823218",
    "end": "1829730"
  },
  {
    "text": "One is that you\ncan have mentions which are essentially\nstandalone,",
    "start": "1829730",
    "end": "1836530"
  },
  {
    "text": "but happen to refer to the\nsame entity in the world. So if I have a\npiece of text that",
    "start": "1836530",
    "end": "1843730"
  },
  {
    "text": "said Barack Obama traveled\nyesterday to Nebraska, Obama was there to open\na new meat processing",
    "start": "1843730",
    "end": "1852370"
  },
  {
    "text": "plant or something like that. I've mentioned with\nBarack Obama and Obama, there are two mentions there.",
    "start": "1852370",
    "end": "1859090"
  },
  {
    "text": "They refer to the same person in\nthe world, they are coreferent. So that is true coreference.",
    "start": "1859090",
    "end": "1864970"
  },
  {
    "text": "But there's a different but\nrelated linguistic concept called anaphora. And anaphora is when you\nhave a textual dependence",
    "start": "1864970",
    "end": "1872860"
  },
  {
    "text": "of an anaphor on another\nterm which is the antecedent. And in this case, the\nmeaning of the anaphor",
    "start": "1872860",
    "end": "1881590"
  },
  {
    "text": "is determined by the antecedent\nin a textual context. And the canonical case\nof this is pronouns.",
    "start": "1881590",
    "end": "1889539"
  },
  {
    "text": "So when it's, Barack Obama\nsaid he would sign the bill, 'he' is an anaphor,\nit's not a word",
    "start": "1889540",
    "end": "1896440"
  },
  {
    "text": "that independently\nwe can work out what its meaning is in the world\napart from knowing the vaguest",
    "start": "1896440",
    "end": "1902500"
  },
  {
    "text": "feature that it's referring\nto something probably male.",
    "start": "1902500",
    "end": "1908050"
  },
  {
    "text": "But in the context\nof this text, we have that this anaphor is\ntextually dependent on Barack",
    "start": "1908050",
    "end": "1916539"
  },
  {
    "text": "Obama, and so then we have\nan anaphoric relationship, which sort of means they refer\nto the same thing in the world.",
    "start": "1916540",
    "end": "1924679"
  },
  {
    "text": "And so therefore you can\nsay they are coreferent. So the picture we have\nis like this, right.",
    "start": "1924680",
    "end": "1931049"
  },
  {
    "start": "1928000",
    "end": "1973000"
  },
  {
    "text": "So for coreference, we\nhave these separate textual mentions, which are\nbasically standalone,",
    "start": "1931050",
    "end": "1938409"
  },
  {
    "text": "which refer to the same\nthing in the world. Whereas, in anaphora we actually\nhave a textual relationship.",
    "start": "1938410",
    "end": "1945520"
  },
  {
    "text": "And you essentially have\nto use pronouns like he and she in legitimate ways,\nin which the hearer can",
    "start": "1945520",
    "end": "1955060"
  },
  {
    "text": "reconstruct the\nrelationship from the text, because they can't\nwork out what 'he' refers to if that's not there.",
    "start": "1955060",
    "end": "1964770"
  },
  {
    "text": "And so, that's a fair\nbit of the distinction, but it's actually\na little bit more",
    "start": "1964770",
    "end": "1971610"
  },
  {
    "text": "to realize because there are\nmore complex forms of anaphora which aren't coreferents.",
    "start": "1971610",
    "end": "1979740"
  },
  {
    "start": "1973000",
    "end": "2148000"
  },
  {
    "text": "Because you have a\ntextual dependence, but it's not actually\none of reference.",
    "start": "1979740",
    "end": "1987040"
  },
  {
    "text": "And so this comes back to things\nlike these quantifier noun phrases, that don't\nhave reference.",
    "start": "1987040",
    "end": "1993970"
  },
  {
    "text": "So when you have\nsentences like these ones, every dancer twisted\nher knee, well",
    "start": "1993970",
    "end": "2000560"
  },
  {
    "text": "this 'her' here has an anaphoric\ndependency on every dancer.",
    "start": "2000560",
    "end": "2006830"
  },
  {
    "text": "Or even more clearly\nwith, no dancer twisted her knee, the 'her'\nhere has an anaphoric dependence",
    "start": "2006830",
    "end": "2013580"
  },
  {
    "text": "on no dancer. But for, no dancer\ntwisted her knee,",
    "start": "2013580",
    "end": "2018980"
  },
  {
    "text": "'no dancer' isn't\nreferential, it's not referring to\nanything in the world. And so there's no\ncoreferential relationship",
    "start": "2018980",
    "end": "2026780"
  },
  {
    "text": "because there's no\nreference relationship. But there's still an\nanaphoric relationship",
    "start": "2026780",
    "end": "2032510"
  },
  {
    "text": "between these two noun phrases.  And then you have this\nother complex case",
    "start": "2032510",
    "end": "2040920"
  },
  {
    "text": "that turns up quite\na bit, where you can have for the things being\ntalked about do have reference,",
    "start": "2040920",
    "end": "2048330"
  },
  {
    "text": "but an anaphoric relationship\nis more subtle than identity. So you commonly get\nconstructions like this one,",
    "start": "2048330",
    "end": "2058530"
  },
  {
    "text": "we went to a concert\nlast night, the tickets were really expensive, well\nthe concert and the tickets",
    "start": "2058530",
    "end": "2066299"
  },
  {
    "text": "are two different things,\nthey're not coreferential.",
    "start": "2066300",
    "end": "2071908"
  },
  {
    "text": "But in interpreting this\nsentence, what this really means is the tickets\nto the concert right,",
    "start": "2071909",
    "end": "2081810"
  },
  {
    "text": "and so there's sort of\nthis hidden, not said, dependence where this is\nreferring back to the concert.",
    "start": "2081810",
    "end": "2089460"
  },
  {
    "text": "And so what we say is\nthat the tickets does have an anaphoric\ndependence on the concert,",
    "start": "2089460",
    "end": "2097349"
  },
  {
    "text": "but they're not coreferential. And so that's referred\nto as bridging anaphora. And so overall, there's the\nsimple case and the common case",
    "start": "2097350",
    "end": "2107130"
  },
  {
    "text": "which is pronominal\nanaphora, where it's both coreference and anaphora.",
    "start": "2107130",
    "end": "2113069"
  },
  {
    "text": "You then have other\ncases of coreference",
    "start": "2113070",
    "end": "2118170"
  },
  {
    "text": "such as every time you see a\nmention of the-- every time the United States has said it's\ncoreferential with every other",
    "start": "2118170",
    "end": "2125460"
  },
  {
    "text": "mention of the United States. But those don't have any textual\ndependence on each other.",
    "start": "2125460",
    "end": "2130980"
  },
  {
    "text": "And then you have\ntextual dependencies like bridging anaphora,\nwhich aren't coreference.",
    "start": "2130980",
    "end": "2137580"
  },
  {
    "text": "Phew, that's probably about as-- I was going to say that's\nprobably as much linguistics",
    "start": "2137580",
    "end": "2143400"
  },
  {
    "text": "as you wanted to\nhear, but actually I have one more point\nof linguistics.",
    "start": "2143400",
    "end": "2149210"
  },
  {
    "text": "One or two of you,\nbut probably not many, might have been\ntroubled by the fact",
    "start": "2149210",
    "end": "2155690"
  },
  {
    "text": "that the term anaphora\nas a classical term",
    "start": "2155690",
    "end": "2162020"
  },
  {
    "text": "means that you are looking\nbackward for your antecedent. That the 'ana' part\nof anaphora means",
    "start": "2162020",
    "end": "2169010"
  },
  {
    "text": "that you're looking backward\nfor your antecedent, and in sort of\nclassical terminology",
    "start": "2169010",
    "end": "2176990"
  },
  {
    "text": "you have both anaphora\nand cataphora. And it's cataphora\nwhere you look forward",
    "start": "2176990",
    "end": "2183770"
  },
  {
    "text": "for your antecedent. Cataphora isn't that\ncommon, but it does occur. Here's a beautiful\nexample of cataphora,",
    "start": "2183770",
    "end": "2191180"
  },
  {
    "text": "so this is from Oscar Wilde. \"From the corner of the\ndivan of Persian saddlebags",
    "start": "2191180",
    "end": "2196640"
  },
  {
    "text": "on which he was lying,\nsmoking, as was his custom, innumerable cigarettes,\nLord Henry Wotton",
    "start": "2196640",
    "end": "2203210"
  },
  {
    "text": "could just catch the\ngleam of the honey-sweet and honey-colored\nblossoms of a laburnum.\"",
    "start": "2203210",
    "end": "2210770"
  },
  {
    "text": "OK so in this\nexample here, right, that 'he' and then\nthis 'his' actually",
    "start": "2210770",
    "end": "2218810"
  },
  {
    "text": "referring to Lord Henry Wotton. And so these are both\nexamples of cataphora.",
    "start": "2218810",
    "end": "2227360"
  },
  {
    "text": "But in modern linguistics\neven though most reference to",
    "start": "2227360",
    "end": "2235270"
  },
  {
    "text": "pronouns is backwards,\nwe don't distinguish on",
    "start": "2235270",
    "end": "2241780"
  },
  {
    "text": "in terms of order. And so the term\nanaphor in anaphora is used for textual dependence\nregardless of whether it's",
    "start": "2241780",
    "end": "2248800"
  },
  {
    "text": "forward or backwards. OK. A lot of details\nthere, but taking",
    "start": "2248800",
    "end": "2254920"
  },
  {
    "start": "2254000",
    "end": "2377000"
  },
  {
    "text": "stock of this, so the basic\nobservation is language",
    "start": "2254920",
    "end": "2261790"
  },
  {
    "text": "is interpreted in context. That in general you can't work\nout the meaning or reference",
    "start": "2261790",
    "end": "2267940"
  },
  {
    "text": "of things without\nlooking at the context of the linguistic utterance.",
    "start": "2267940",
    "end": "2273579"
  },
  {
    "text": "So we've seen some\nsimple examples before, so for something like\nword-sense disambiguation.",
    "start": "2273580",
    "end": "2281350"
  },
  {
    "text": "So if you see just\nthe words, 'the bank' you don't know what\nit means, and you need to look at a context to\nget some sense as to whether it",
    "start": "2281350",
    "end": "2288700"
  },
  {
    "text": "means a financial institution,\nor the bank of a river, or something like that.",
    "start": "2288700",
    "end": "2293720"
  },
  {
    "text": "And so anaphora and coreference\ngive us additional examples",
    "start": "2293720",
    "end": "2299470"
  },
  {
    "text": "where you need to be doing\ncontextual interpretation of language. So when you see a\npronoun, you need",
    "start": "2299470",
    "end": "2306310"
  },
  {
    "text": "to be looking at the context\nto see what it refers to. And so if you think\nabout text understanding",
    "start": "2306310",
    "end": "2314470"
  },
  {
    "text": "as a human being does it,\nreading a story or an article, that we progress through the\narticle from beginning to end,",
    "start": "2314470",
    "end": "2322060"
  },
  {
    "text": "and as we do it we build up a\npretty complex discourse model in which new entities are\nintroduced by mentions,",
    "start": "2322060",
    "end": "2331150"
  },
  {
    "text": "and then they're\nreferred back to and relationships between\nthem are established and they take actions\nand things like that.",
    "start": "2331150",
    "end": "2338270"
  },
  {
    "text": "And it sort of seems\nlike in our head that we sort of build up a\ncomplex graph-like discourse",
    "start": "2338270",
    "end": "2344200"
  },
  {
    "text": "representation of\na piece of text with all these relationships. And so part of that is these\nanaphoric relationships",
    "start": "2344200",
    "end": "2352600"
  },
  {
    "text": "and coreference that\nwe're talking about here. And indeed in terms of\nCS224N, the only kind",
    "start": "2352600",
    "end": "2358460"
  },
  {
    "text": "of whole discourse meaning\nthat we're going to look at is, looking a bit at\nanaphora and coreference.",
    "start": "2358460",
    "end": "2366520"
  },
  {
    "text": "but if you want to\nsee more about higher level, natural\nlanguage understanding,",
    "start": "2366520",
    "end": "2371650"
  },
  {
    "text": "you can get more of this\nnext quarter in CS224U.",
    "start": "2371650",
    "end": "2376960"
  },
  {
    "text": "So I want to tell you a bit\nabout several different ways",
    "start": "2376960",
    "end": "2382540"
  },
  {
    "start": "2377000",
    "end": "2449000"
  },
  {
    "text": "of doing coreference. So broadly, there are\nfour different kinds",
    "start": "2382540",
    "end": "2388329"
  },
  {
    "text": "of coreference models. So the traditional\nold way of doing it was, rule-based systems.",
    "start": "2388330",
    "end": "2395319"
  },
  {
    "text": "And this isn't the\ntopic of this class, and this is pretty\narchaic at this point,",
    "start": "2395320",
    "end": "2401680"
  },
  {
    "text": "this is stuff from\nlast millennium. But I wanted to say a\nlittle bit about it,",
    "start": "2401680",
    "end": "2407020"
  },
  {
    "text": "because it's actually\ninteresting, a food for thought as to how far along\nwe are and in solving",
    "start": "2407020",
    "end": "2414940"
  },
  {
    "text": "artificial\nintelligence and really being able to understand texts.",
    "start": "2414940",
    "end": "2420520"
  },
  {
    "text": "Then there is sort of classic\nmachine learning methods of doing it, which you\ncan sort of divide up as,",
    "start": "2420520",
    "end": "2426100"
  },
  {
    "text": "mention pair methods,\nmention ranking methods, and really clustering methods.",
    "start": "2426100",
    "end": "2431560"
  },
  {
    "text": "And I'm going to sort of skip\nthe clustering methods today, because most of the\nwork, especially most of the recent work,\nimplicitly makes clusters",
    "start": "2431560",
    "end": "2440200"
  },
  {
    "text": "by using either mention pair\nor mention ranking methods. And so I'm going to talk about\na couple of neural methods",
    "start": "2440200",
    "end": "2446440"
  },
  {
    "text": "for doing that. OK but first of all,\nlet me just tell you a little bit about\nrule-based coreference.",
    "start": "2446440",
    "end": "2455200"
  },
  {
    "start": "2449000",
    "end": "2561000"
  },
  {
    "text": "So there's a famous\nhistorical algorithm in NLP for doing pronoun\nanaphora resolution, which",
    "start": "2455200",
    "end": "2465670"
  },
  {
    "text": "is referred to as\nthe Hobbs algorithm. So everyone just refers to\nit as the Hobbs algorithm,",
    "start": "2465670",
    "end": "2472130"
  },
  {
    "text": "and if you look up a\ntextbook like Jurafsky and Martin's textbook,\nit's referred to",
    "start": "2472130",
    "end": "2477880"
  },
  {
    "text": "as the Hobbs algorithm. But actually, if you go\nback to Jerry Hobbs, that's",
    "start": "2477880",
    "end": "2482890"
  },
  {
    "text": "his picture over\nthere in the corner, if you actually go back\nto his original paper, he refers to it as\nthe naive algorithm.",
    "start": "2482890",
    "end": "2492100"
  },
  {
    "text": "And then this naive algorithm\nfor pronoun coreference, was this sort of intricate\nhandwritten set of rules",
    "start": "2492100",
    "end": "2501430"
  },
  {
    "text": "to work out coreference. So this is the start of\nthe set of the rules, but there are more rules, or\nmore clauses of these rules",
    "start": "2501430",
    "end": "2510400"
  },
  {
    "text": "for working out coreference. And this looks like a hot\nmess, but the funny thing",
    "start": "2510400",
    "end": "2517720"
  },
  {
    "text": "was, that this set of rules\nfor determining coreference were actually pretty good.",
    "start": "2517720",
    "end": "2523920"
  },
  {
    "text": "And so in the sort of 1990s\nand 2000s decade, even",
    "start": "2523920",
    "end": "2529900"
  },
  {
    "text": "when people were using\nmachine learning based systems for doing coreference,\nthey'd hide",
    "start": "2529900",
    "end": "2535960"
  },
  {
    "text": "into those machine\nlearning based systems, that one\nof their features was the Hobbs' algorithm.",
    "start": "2535960",
    "end": "2541780"
  },
  {
    "text": "And that the predictions it\nmade with a certain weight, was then a feature in\nmaking your final decisions.",
    "start": "2541780",
    "end": "2549369"
  },
  {
    "text": "And it's only really\nin the last five years that people have moved away\nfrom using the Hobbs algorithm.",
    "start": "2549370",
    "end": "2556060"
  },
  {
    "text": "Let me give you a little bit\nof a sense of how it works, OK. So from the Hobbs algorithm,\nhere's is our example.",
    "start": "2556060",
    "end": "2564119"
  },
  {
    "start": "2561000",
    "end": "3270000"
  },
  {
    "text": "This is an example from\na Guardian book review, Niall Ferguson is prolific,\nwell-paid and a snappy dresser,",
    "start": "2564120",
    "end": "2570580"
  },
  {
    "text": "Stephen Moss hated him. OK. So what the Hobbs algorithm does\nis, we start with the pronouns,",
    "start": "2570580",
    "end": "2580980"
  },
  {
    "text": "we start with the\npronoun, and then it says step one, go to the\nNP that's immediately",
    "start": "2580980",
    "end": "2587640"
  },
  {
    "text": "dominating the pronoun. And then it says go up\nto the first NP or S,",
    "start": "2587640",
    "end": "2593070"
  },
  {
    "text": "call this X, and\nthe path P. Then",
    "start": "2593070",
    "end": "2598830"
  },
  {
    "text": "it says traverse all\nbranches below X, to the left of P,\nleft-to-right breadth first.",
    "start": "2598830",
    "end": "2605910"
  },
  {
    "text": "So then it's saying to go left\nto right for other branches below breadth first, so that's\nsort of working down the tree,",
    "start": "2605910",
    "end": "2614790"
  },
  {
    "text": "so we're going down and left\nto right, and look for an NP.",
    "start": "2614790",
    "end": "2621690"
  },
  {
    "text": "OK, and here is an NP, but then\nwe have to read more carefully",
    "start": "2621690",
    "end": "2627660"
  },
  {
    "text": "and say, propose as\nantecedent any NP that has an NP or S\nbetween it and X. Well",
    "start": "2627660",
    "end": "2636240"
  },
  {
    "text": "this NP here has no NP\nor S between NP and X.",
    "start": "2636240",
    "end": "2644070"
  },
  {
    "text": "So this isn't a\npossible antecedent. So this is all very\ncomplex and handwritten,",
    "start": "2644070",
    "end": "2650880"
  },
  {
    "text": "but basically he sort\nof fit into the clauses of this kind of a lot\nof facts about how",
    "start": "2650880",
    "end": "2657360"
  },
  {
    "text": "the grammar of English works. And so what this\nis capturing is,",
    "start": "2657360",
    "end": "2662460"
  },
  {
    "text": "if you imagine a\ndifferent sentence, if you imagine the sentence,\nStephen Moss's brother",
    "start": "2662460",
    "end": "2674050"
  },
  {
    "text": "hated him, well then\nStephen Moss would naturally be coreferent with him.",
    "start": "2674050",
    "end": "2680710"
  },
  {
    "text": "And in that case, well\nprecisely what you'd have is, the noun phrase with\nthe noun brother,",
    "start": "2680710",
    "end": "2692140"
  },
  {
    "text": "and you'd have another\nnoun phrase inside it for the Steven\nMoss, and then that",
    "start": "2692140",
    "end": "2700460"
  },
  {
    "text": "would go up to the sentence. So in the case of\nSteven Moss's brother, when you looked at\nthis noun phrase,",
    "start": "2700460",
    "end": "2707900"
  },
  {
    "text": "there would be an\nintervening noun phrase before you got to the\nnode X. And therefore,",
    "start": "2707900",
    "end": "2716090"
  },
  {
    "text": "Steven Moss is a possible,\nand in fact good, antecedent of him,\nand the algorithm",
    "start": "2716090",
    "end": "2724280"
  },
  {
    "text": "would choose Steven Moss. But the algorithm correctly\ncaptures that when you have",
    "start": "2724280",
    "end": "2729500"
  },
  {
    "text": "the sentence, Steven\nMoss hated him, that 'him' cannot\nrefer to Steven Moss.",
    "start": "2729500",
    "end": "2735319"
  },
  {
    "text": "OK, so having worked\nthat out, it then says if X is the highest\nS in the sentence,",
    "start": "2735320",
    "end": "2742099"
  },
  {
    "text": "OK, so my X here is definitely\nthe highest S in the sentence, because I wrote\nthe whole sentence.",
    "start": "2742100",
    "end": "2748020"
  },
  {
    "text": "What you should do\nis then traverse the parse trees of\nprevious sentences",
    "start": "2748020",
    "end": "2754280"
  },
  {
    "text": "in the order of recency. So what I should do\nnow is, work backwards",
    "start": "2754280",
    "end": "2760130"
  },
  {
    "text": "in the text, one\nsentence at a time, going backwards looking\nfor an antecedent.",
    "start": "2760130",
    "end": "2767569"
  },
  {
    "text": "And then for each tree,\ntraverse each tree left to right, breadth first.",
    "start": "2767570",
    "end": "2772970"
  },
  {
    "text": "So then within each\ntree, I'm doing the same of going breadth first,\nso working down and then going",
    "start": "2772970",
    "end": "2781250"
  },
  {
    "text": "left to right with\nan equal breadth. And so hidden inside\nthese clauses,",
    "start": "2781250",
    "end": "2786484"
  },
  {
    "text": "it's capturing a lot\nof the facts of how coreference typically works.",
    "start": "2786485",
    "end": "2793290"
  },
  {
    "text": "So what you find in\nEnglish I'll say, but in general, this is true\nof lots of languages is,",
    "start": "2793290",
    "end": "2802550"
  },
  {
    "text": "that there are\ngeneral preferences and tendencies for coreference. So a lot of the\ntime, a pronoun will",
    "start": "2802550",
    "end": "2809720"
  },
  {
    "text": "be coreferent with something\nin the same sentence like, Stephen Moss's\nbrother hated him,",
    "start": "2809720",
    "end": "2814850"
  },
  {
    "text": "but it can't be if\nit's too close to it. So you can't say, Stephen Moss\nhated him, and have the 'him'",
    "start": "2814850",
    "end": "2820640"
  },
  {
    "text": "be Stephen Moss. And if you're then looking\nfor coreference that's further away, the thing it's coreferent\nwith is normally close by,",
    "start": "2820640",
    "end": "2830849"
  },
  {
    "text": "and so that's why you work\nbackwards through sentences one by one. But then once you're looking\nwithin a particular sentence,",
    "start": "2830850",
    "end": "2838880"
  },
  {
    "text": "the most likely thing it's\ngoing to be coreferent to is, a topical noun phrase, and\ndefault topics in English",
    "start": "2838880",
    "end": "2847190"
  },
  {
    "text": "are subjects. So by doing things breadth\nfirst left-to-right,",
    "start": "2847190",
    "end": "2853010"
  },
  {
    "text": "a preferred antecedent\nis then a subject. And so this algorithm I won't go\nthrough all the complex clauses",
    "start": "2853010",
    "end": "2860060"
  },
  {
    "text": "5 through 9, ends up saying,\nOK, what you should do is propose Niall\nFerguson as what",
    "start": "2860060",
    "end": "2867380"
  },
  {
    "text": "is coreferent to him, which\nis the obvious correct reading in this example.",
    "start": "2867380",
    "end": "2872750"
  },
  {
    "text": "OK, phew. We probably didn't want to\nknow that, and in some sense the details of that\naren't interesting.",
    "start": "2872750",
    "end": "2880730"
  },
  {
    "text": "But what is I think, actually\nstill interesting in 2021 is,",
    "start": "2880730",
    "end": "2887000"
  },
  {
    "text": "what point Jerry\nHobbs was actually trying to make last millennium.",
    "start": "2887000",
    "end": "2894950"
  },
  {
    "text": "And the point he\nwas trying to make was the following,\nso Jerry Hobbs",
    "start": "2894950",
    "end": "2901760"
  },
  {
    "text": "wrote this algorithm, the naive\nalgorithm, because what he said",
    "start": "2901760",
    "end": "2907370"
  },
  {
    "text": "was, well look if you want\nto try and crudely determine",
    "start": "2907370",
    "end": "2912800"
  },
  {
    "text": "coreference, well there are\nthese various preferences, right. There's the preference\nfor same sentence,",
    "start": "2912800",
    "end": "2919880"
  },
  {
    "text": "there's the preference\nfor recency, there's a preference for\ntopical things like subject,",
    "start": "2919880",
    "end": "2925069"
  },
  {
    "text": "and there are things\nwhere if it has gender, it has to agree in gender. So there are sort of strong\nconstraints of that sort.",
    "start": "2925070",
    "end": "2934230"
  },
  {
    "text": "So I can write an algorithm\nusing my linguisticness, which captures all the\nmain preferences,",
    "start": "2934230",
    "end": "2942260"
  },
  {
    "text": "and actually it\nworks pretty well. Doing that is a pretty\nstrong baseline system.",
    "start": "2942260",
    "end": "2950670"
  },
  {
    "text": "But what Jerry Hobbs\nwanted to argue is that this algorithm\njust isn't something",
    "start": "2950670",
    "end": "2959330"
  },
  {
    "text": "you should believe in. This isn't a solution\nto the problem, this is just sort of making\na best guess according",
    "start": "2959330",
    "end": "2968720"
  },
  {
    "text": "to the preferences of what's\nmost likely without actually",
    "start": "2968720",
    "end": "2973820"
  },
  {
    "text": "understanding what's going\non in the text at all. And so actually what Jerry\nHobbs wanted to argue",
    "start": "2973820",
    "end": "2981350"
  },
  {
    "text": "was the so-called\nHobbs' algorithm now. He wasn't a fan of\nthe Hobbs algorithm,",
    "start": "2981350",
    "end": "2986420"
  },
  {
    "text": "he was wanting to\nargue that the Hobbs algorithm is completely\ninadequate as a solution",
    "start": "2986420",
    "end": "2991670"
  },
  {
    "text": "to the problem. And the only way we'll\nactually make progress in natural language\nunderstanding,",
    "start": "2991670",
    "end": "2996800"
  },
  {
    "text": "is by building systems that\nactually really understand the text.",
    "start": "2996800",
    "end": "3002490"
  },
  {
    "text": "And this is actually something\nthat has come to the fore again more recently.",
    "start": "3002490",
    "end": "3009509"
  },
  {
    "text": "So the suggestion is\nthat in general you can't work out coreference\nor pronominal anaphora",
    "start": "3009510",
    "end": "3017480"
  },
  {
    "text": "in particular, unless\nyou're really understanding the meaning of the text. And people look at pairs of\nexamples like these ones,",
    "start": "3017480",
    "end": "3025640"
  },
  {
    "text": "so she poured water from\nthe pitcher into the cup until it was full, so think\nfor just half a moment.",
    "start": "3025640",
    "end": "3032359"
  },
  {
    "text": "Well, what is it in that example\nthat is full, so what's full",
    "start": "3032360",
    "end": "3041300"
  },
  {
    "text": "there is the cup. But then if I say, she\npoured water from the pitcher into the cup until it was\nempty, well what's empty,",
    "start": "3041300",
    "end": "3050600"
  },
  {
    "text": "well that's the pitcher. And the point that is being\nmade with these examples is",
    "start": "3050600",
    "end": "3057990"
  },
  {
    "text": "the only thing that's been\nchanged in these examples is the adjective right here.",
    "start": "3057990",
    "end": "3065110"
  },
  {
    "text": "So these two\nexamples have exactly the same grammatical structure.",
    "start": "3065110",
    "end": "3071530"
  },
  {
    "text": "So in terms of the\nHobbs naive algorithm, the Hobbs naive\nalgorithm necessarily",
    "start": "3071530",
    "end": "3078930"
  },
  {
    "text": "has to predict the same\nanswer for both of these, but that's wrong.",
    "start": "3078930",
    "end": "3084360"
  },
  {
    "text": "You just cannot determine the\ncorrect pronoun antecedent based on grammatical preferences\nof the kind that are used",
    "start": "3084360",
    "end": "3092849"
  },
  {
    "text": "in the naive algorithm, you\nactually have to conceptually understand about pitchers,\nand cups, and water, and full,",
    "start": "3092850",
    "end": "3100170"
  },
  {
    "text": "and empty to be able to\nchoose the right antecedent. Here's another famous example\nthat goes along the same lines,",
    "start": "3100170",
    "end": "3109800"
  },
  {
    "text": "so Terry Winograd shown\nhere as a young man. So long, long ago Terry\nWinograd came to Stanford",
    "start": "3109800",
    "end": "3117570"
  },
  {
    "text": "as the natural language\nprocessing faculty. And Terry Winograd\nbecame disillusioned",
    "start": "3117570",
    "end": "3124800"
  },
  {
    "text": "with the symbolic\nAI of those days, and just gave it up altogether.",
    "start": "3124800",
    "end": "3130109"
  },
  {
    "text": "And he reinvented himself\nas being an HCI person, and so Terry was\nthen essentially",
    "start": "3130110",
    "end": "3136050"
  },
  {
    "text": "the person who established\nthe HCI program at Stanford. But before he lost\nfaith in symbolic AI,",
    "start": "3136050",
    "end": "3145740"
  },
  {
    "text": "he talked about the\ncoreference problem and pointed out a similar\npair of examples here.",
    "start": "3145740",
    "end": "3152290"
  },
  {
    "text": "So we have, the city\ncouncil refused the women a permit because\nthey feared violence,",
    "start": "3152290",
    "end": "3158700"
  },
  {
    "text": "versus the city council\nrefused the women a permit because they\nadvocated violence.",
    "start": "3158700",
    "end": "3164500"
  },
  {
    "text": "So again, you have this\nsituation where these two sentences have identical\nsyntactic structure",
    "start": "3164500",
    "end": "3170760"
  },
  {
    "text": "and they differ only in\nthe choice of verb here. But once you add\ncommonsense knowledge",
    "start": "3170760",
    "end": "3178560"
  },
  {
    "text": "of how the human\nworld works, well, how this would pretty\nobviously be interpreted that",
    "start": "3178560",
    "end": "3187560"
  },
  {
    "text": "in the first one that 'they' is\nreferring to the city council, whereas in the second\none that 'they'",
    "start": "3187560",
    "end": "3194490"
  },
  {
    "text": "is referring to the women. And so coming off of\nthat example of Terry,",
    "start": "3194490",
    "end": "3201240"
  },
  {
    "text": "these have been referred\nto as Winograd schemas. So a Winograd schema challenge\nis choosing the right reference",
    "start": "3201240",
    "end": "3210990"
  },
  {
    "text": "here, and so it's basically\njust doing pronominal anaphora. But the interesting\nthing is, people",
    "start": "3210990",
    "end": "3218220"
  },
  {
    "text": "have been interested in what are\ntests of general intelligence, and one famous general test of\nintelligence that I won't talk",
    "start": "3218220",
    "end": "3226480"
  },
  {
    "text": "about now is the Turing test. And there's been a lot of debate\nabout problems with the Turing test and is it good?",
    "start": "3226480",
    "end": "3232800"
  },
  {
    "text": "And some particular\nHector Levesque is a very well-known\nsenior AI person.",
    "start": "3232800",
    "end": "3239920"
  },
  {
    "text": "He actually proposed\nthat a better alternative to the Turing test might\nbe to do what he then",
    "start": "3239920",
    "end": "3245880"
  },
  {
    "text": "dubbed Winograd schema. And Winograd schema is just\nsolving pronominal coreference",
    "start": "3245880",
    "end": "3252030"
  },
  {
    "text": "in cases like these where\nyou have to have knowledge about the situation in the\nworld to get the answer right,",
    "start": "3252030",
    "end": "3258090"
  },
  {
    "text": "and so he's basically\narguing that you can review really solving\ncoreference and solving",
    "start": "3258090",
    "end": "3265200"
  },
  {
    "text": "artificial intelligence. And that's the position that\nHobbs wanted to advocate,",
    "start": "3265200",
    "end": "3272910"
  },
  {
    "text": "so what he actually said\nabout his algorithm was that \"the naive approach is\nquite good, computationally",
    "start": "3272910",
    "end": "3279480"
  },
  {
    "text": "speaking, it will be a long\ntime before a semantically based algorithm is sophisticated\nenough to perform as well.",
    "start": "3279480",
    "end": "3286180"
  },
  {
    "text": "And these results set\na very high standard for any other\napproach to aim for\". And he was proven\nright about that,",
    "start": "3286180",
    "end": "3291930"
  },
  {
    "text": "because there was sort of\nreally talk to around 2015 before people thought they could\ndo without the Hobbs algorithm.",
    "start": "3291930",
    "end": "3299250"
  },
  {
    "text": "But then he notes, \"yet\nthere is every reason to pursue a semantically\nbased approach,",
    "start": "3299250",
    "end": "3304349"
  },
  {
    "text": "the naive algorithm\ndoes not work. Anyone can think of\nexamples where it fails.",
    "start": "3304350",
    "end": "3309480"
  },
  {
    "text": "In these cases,\nit not only fails, it gives no indication\nthat it has failed,",
    "start": "3309480",
    "end": "3314670"
  },
  {
    "text": "and offers no help in\nfinding the real antecedent\". And so I think this is actually\nstill interesting stuff",
    "start": "3314670",
    "end": "3321030"
  },
  {
    "text": "to think about, because\nreally for the kind of machine learning based coreference\nsystems that we're building,",
    "start": "3321030",
    "end": "3329190"
  },
  {
    "text": "they're not a hot mess of\nrules like the Hobbs algorithm. But basically,\nthey're still sort",
    "start": "3329190",
    "end": "3334970"
  },
  {
    "text": "of working out statistical\npreferences of what",
    "start": "3334970",
    "end": "3340530"
  },
  {
    "text": "patterns are most likely,\nand choosing the antecedent. That way, they really have\nexactly the same deficiencies",
    "start": "3340530",
    "end": "3351210"
  },
  {
    "text": "still what Hobbs was\ntalking about, right. That they fail in\nvarious cases, it's",
    "start": "3351210",
    "end": "3357410"
  },
  {
    "text": "easy to find places\nwhere they fail, the algorithms give you\nno idea of when they fail,",
    "start": "3357410",
    "end": "3363710"
  },
  {
    "text": "they're not really\nunderstanding the text in a way that a human does to\ndetermine the antecedent.",
    "start": "3363710",
    "end": "3369349"
  },
  {
    "text": "So we still actually have\na lot more work to do, before we're really doing\nfull artificial intelligence.",
    "start": "3369350",
    "end": "3376760"
  },
  {
    "text": "But I'd best get\non now and actually tell you a bit about some\ncoreference algorithms, right.",
    "start": "3376760",
    "end": "3383210"
  },
  {
    "start": "3377000",
    "end": "3599000"
  },
  {
    "text": "So the simple way of\nthinking about coreference, is to say that you're making\njust a binary decision",
    "start": "3383210",
    "end": "3391880"
  },
  {
    "text": "about a reference pair. So if you have your\nmentions, you can then say,",
    "start": "3391880",
    "end": "3399900"
  },
  {
    "text": "well I've come to\nmy next mention 'she' I want to work out\nwhat it's coreferent with.",
    "start": "3399900",
    "end": "3407240"
  },
  {
    "text": "And I can just look at\nall of the mentions that came before it and say,\nis that coreferent or not,",
    "start": "3407240",
    "end": "3413810"
  },
  {
    "text": "and do a binary decision. So at training time,\nI'll be able to say I have positive\nexamples, assuming",
    "start": "3413810",
    "end": "3420890"
  },
  {
    "text": "I've got some data labeled\nfor what's coreferent to what, and so these ones\nare coreferent.",
    "start": "3420890",
    "end": "3426050"
  },
  {
    "text": "And I've got some negative\nexamples of these ones are not coreferent.",
    "start": "3426050",
    "end": "3431280"
  },
  {
    "text": "And what I want to\ndo is build a model that learns to predict\ncoreferent things.",
    "start": "3431280",
    "end": "3436610"
  },
  {
    "text": "And I can do that fairly\nstraightforwardly in the kind of ways that we\nhave talked about.",
    "start": "3436610",
    "end": "3442160"
  },
  {
    "text": "So I train with the regular kind\nof cross entropy loss, where",
    "start": "3442160",
    "end": "3448280"
  },
  {
    "text": "I'm now summing over every\npairwise binary decision, as to whether two mentions\nare coreferent to each other",
    "start": "3448280",
    "end": "3456560"
  },
  {
    "text": "or not. And so then when I'm at\ntest time, what I want to do",
    "start": "3456560",
    "end": "3462380"
  },
  {
    "text": "is cluster the mentions that\ncorrespond to the same entity, and I do that by making\nuse of my pairwise scorer.",
    "start": "3462380",
    "end": "3470569"
  },
  {
    "text": "So I can run my\npairwise scorer and it will give a\nprobability or a score",
    "start": "3470570",
    "end": "3478400"
  },
  {
    "text": "that any two mentions\nare coreferent. So by picking some\nthreshold like 0.5,",
    "start": "3478400",
    "end": "3484010"
  },
  {
    "text": "I can add coreference links\nfor when the classifier says it's above the threshold.",
    "start": "3484010",
    "end": "3491060"
  },
  {
    "text": "And then I do one more step\nto give me a clustering, I then say OK, let's also\nmake the transitive closures",
    "start": "3491060",
    "end": "3499600"
  },
  {
    "text": "to give me clusters. So it thought that 'I'\nand 'she' were coreferent and 'my' and 'she' were\ncoreferent therefore,",
    "start": "3499600",
    "end": "3507440"
  },
  {
    "text": "I also have to regard 'I'\nand 'my' as coreferent. and so that's sort of the\ncompletion by transitivity.",
    "start": "3507440",
    "end": "3516040"
  },
  {
    "text": "And so since we always\ncomplete by transitivity, note that this algorithm is very\nsensitive to making any mistake",
    "start": "3516040",
    "end": "3525370"
  },
  {
    "text": "in a positive sense. Because if you make one\nmistake, for example you say that, 'he' and\n'my' are coreferent, then",
    "start": "3525370",
    "end": "3533380"
  },
  {
    "text": "by transitivity all of the\nmentions in the sentence become one big cluster.",
    "start": "3533380",
    "end": "3540110"
  },
  {
    "text": "And that they're all\ncoreferent with each other. So that's a workable algorithm\nand people have often",
    "start": "3540110",
    "end": "3547030"
  },
  {
    "text": "used that. But often people go a\nlittle bit beyond that, and prefer a mention\nranking model.",
    "start": "3547030",
    "end": "3555320"
  },
  {
    "text": "So let me just explain\nthe advantages of that. That normally, if you\nhave a long document where",
    "start": "3555320",
    "end": "3562120"
  },
  {
    "text": "it's Ralph Nader, and he\ndid this and some of them did something to him\nand we visited his house",
    "start": "3562120",
    "end": "3567135"
  },
  {
    "text": "and blah, blah,\nblah, blah, blah, and then somebody voted\nfor Nader because he--",
    "start": "3567135",
    "end": "3573220"
  },
  {
    "text": "In terms of building a\ncoreference classifier, it seems like it's easy\nand reasonable to be",
    "start": "3573220",
    "end": "3583180"
  },
  {
    "text": "able to recover this\n'he' refers to Nader. But in terms of building\na classifier for it",
    "start": "3583180",
    "end": "3591700"
  },
  {
    "text": "to recognize that\nthis he' should be referring to\nthis Nader, which might be three paragraphs back.",
    "start": "3591700",
    "end": "3598000"
  },
  {
    "text": "Seems unreasonable how you're\ngoing to recover that, so those far away ones\nmight be almost",
    "start": "3598000",
    "end": "3604270"
  },
  {
    "text": "impossible to get correct. And so that suggests\nthat maybe we should have a different way\nof configuring this task.",
    "start": "3604270",
    "end": "3613369"
  },
  {
    "text": "So instead of doing it that\nway, what we should say is, well this 'he' here has\nvarious possible antecedents,",
    "start": "3613370",
    "end": "3622930"
  },
  {
    "text": "and our job is to just\nchoose one of them. And that's almost sufficient.",
    "start": "3622930",
    "end": "3629349"
  },
  {
    "text": "Apart from we need to\nadd one more choice which",
    "start": "3629350",
    "end": "3634750"
  },
  {
    "text": "is, while some mentions won't\nbe coreferent with anything that proceeds, because we are\nintroducing a new entity",
    "start": "3634750",
    "end": "3642130"
  },
  {
    "text": "into the discourse. So we can add one more dummy\nmention, the NA mention,",
    "start": "3642130",
    "end": "3649750"
  },
  {
    "text": "so it doesn't refer to anything\npreviously in the discourse.",
    "start": "3649750",
    "end": "3654970"
  },
  {
    "text": "And then our job\nat each point is to do mention ranking to\nchoose which one of these",
    "start": "3654970",
    "end": "3661030"
  },
  {
    "text": "she refers to. And then at that point, rather\nthan doing binary yes/no",
    "start": "3661030",
    "end": "3667150"
  },
  {
    "text": "classifiers, that what we\ncan do is say, a-ha, this is choose one classification.",
    "start": "3667150",
    "end": "3673210"
  },
  {
    "text": "And then we can use\nthe softmax classifiers that we've seen at\nmany points previously.",
    "start": "3673210",
    "end": "3680700"
  },
  {
    "text": "OK. So that gets us in business\nfor building systems and either of these\nkind of models,",
    "start": "3680700",
    "end": "3688800"
  },
  {
    "text": "there are several ways in\nwhich we can build the system. We could use any kind\nof traditional machine",
    "start": "3688800",
    "end": "3694890"
  },
  {
    "text": "learning classifier, we could\nuse a simple neural network,",
    "start": "3694890",
    "end": "3700049"
  },
  {
    "text": "we can use more advanced\nones with all of the tools that we've been learning\nabout more recently.",
    "start": "3700050",
    "end": "3705390"
  },
  {
    "text": "Let me just quickly show\nyou a simple neural network way of doing it.",
    "start": "3705390",
    "end": "3711490"
  },
  {
    "text": "So this is the model that\nmy Phd student Kevin Clark",
    "start": "3711490",
    "end": "3716820"
  },
  {
    "text": "did in 2015, so\nnot that long ago. But what he was doing was\ndoing coreference resolution",
    "start": "3716820",
    "end": "3724290"
  },
  {
    "text": "based on the mentions\nwith a simple feed forward neural network, kind\nof in some sense like we",
    "start": "3724290",
    "end": "3730650"
  },
  {
    "text": "did dependency parsing\nwith a simple feed forward neural network. So for the mention, it\nhad word embeddings,",
    "start": "3730650",
    "end": "3739890"
  },
  {
    "text": "antecedent had word embeddings. There were some\nadditional features of each of the mention,\nthe candidate antecedent.",
    "start": "3739890",
    "end": "3747450"
  },
  {
    "text": "And then there was some\nfinal additional features that captured things\nlike distance away,",
    "start": "3747450",
    "end": "3752850"
  },
  {
    "text": "which you can't see from either\nthe mentions of the candidate. And all of those\nfeatures were just",
    "start": "3752850",
    "end": "3758340"
  },
  {
    "text": "fed into several feed forward\nlayers of a neural network, and it gave you a score of are\nthese things coreferent or not?",
    "start": "3758340",
    "end": "3768660"
  },
  {
    "text": "And that by itself just\nworked pretty well. ",
    "start": "3768660",
    "end": "3775210"
  },
  {
    "text": "And I won't say more\ndetails about that. But what I do want\nto show is sort of a more advanced and modern\nneural coreference system.",
    "start": "3775210",
    "end": "3786290"
  },
  {
    "text": "But before I do that, I\nwant to take a digression and sort of say\na few words about",
    "start": "3786290",
    "end": "3792099"
  },
  {
    "text": "convolutional neural networks.  So, the idea of when you apply\na convolutional neural network",
    "start": "3792100",
    "end": "3802480"
  },
  {
    "text": "to language i.e. to sequences,\nis that what you're going to do",
    "start": "3802480",
    "end": "3808150"
  },
  {
    "text": "is you're going to compute\nvectors, features effectively, for every possible\nword subsequence",
    "start": "3808150",
    "end": "3815500"
  },
  {
    "text": "of a certain length. So that if you have\na piece of text like, \"tentative deal reached\nto keep government open\"",
    "start": "3815500",
    "end": "3822070"
  },
  {
    "text": "you might say, I'm going to\ntake every three words of that, i.e. tentative deal\nreached, deal reached to,",
    "start": "3822070",
    "end": "3829480"
  },
  {
    "text": "reached to keep. And I'm going to\ncompute a vector based on that subsequence\nof words, and use",
    "start": "3829480",
    "end": "3838300"
  },
  {
    "text": "those computed\nvectors in my model by somehow grouping\nthem together.",
    "start": "3838300",
    "end": "3844460"
  },
  {
    "text": "So the canonical case of\nconvolutional neural networks",
    "start": "3844460",
    "end": "3849800"
  },
  {
    "text": "is envision, and so if\nafter this, next quarter you go along to CS231N\nyou'll be able to spend",
    "start": "3849800",
    "end": "3859190"
  },
  {
    "text": "weeks doing convolutional\nneural networks for vision. And so the idea\nthere is that you've",
    "start": "3859190",
    "end": "3865760"
  },
  {
    "text": "got these convolutional filters\nthat you slide over an image",
    "start": "3865760",
    "end": "3872060"
  },
  {
    "text": "and you compute a\nfunction of each place. So there are little red\nnumbers are showing you",
    "start": "3872060",
    "end": "3879140"
  },
  {
    "text": "what you're computing,\nbut then you'll slide it over to the next\nposition and fill in this cell,",
    "start": "3879140",
    "end": "3885110"
  },
  {
    "text": "and then you'll slide it\nover the next position, fill in this cell, and then\nyou'll slide it down and fill",
    "start": "3885110",
    "end": "3891890"
  },
  {
    "text": "in this cell. And so you've got\nthis little function of a patch which you're\nsliding over your image",
    "start": "3891890",
    "end": "3899480"
  },
  {
    "text": "and computing a convolution,\nwhich is just a dot product effectively,\nthat you're then",
    "start": "3899480",
    "end": "3906350"
  },
  {
    "text": "using to get an extra\nlayer of representation. And so by sliding things over,\nyou can pick out features",
    "start": "3906350",
    "end": "3914540"
  },
  {
    "text": "and you've got a\nfeature identifier that runs across every\npiece of the image.",
    "start": "3914540",
    "end": "3921320"
  },
  {
    "text": "Well, for language we've\njust got a sequence, that you can do\nbasically the same thing",
    "start": "3921320",
    "end": "3928130"
  },
  {
    "text": "and what you then have is\na 1D convolution for text. So if here is my\nsentence, \"tentative deal",
    "start": "3928130",
    "end": "3935120"
  },
  {
    "text": "reached to keep the government\nopen,\" that, what I can do is have, so these words\nhave our word representation",
    "start": "3935120",
    "end": "3945470"
  },
  {
    "text": "which if so, this is my\nvector for each word. And then I can have a filter,\nsometimes called a kernel,",
    "start": "3945470",
    "end": "3954930"
  },
  {
    "text": "which I use for my convolution. And what I'm going\nto do is slide that down the text so I can\nstart with the first three",
    "start": "3954930",
    "end": "3963809"
  },
  {
    "text": "words, and then I\nsort of treat them as sort of elements I\ncan dot product and sum.",
    "start": "3963810",
    "end": "3970569"
  },
  {
    "text": "And then I can compute a value\nas to what they all add up to, which is minus 1, it turns out.",
    "start": "3970570",
    "end": "3977950"
  },
  {
    "text": "And so then I might have\na bias that I add on, and get an updated value\nif my bias is plus 1.",
    "start": "3977950",
    "end": "3984915"
  },
  {
    "text": " And then I'd run it\nthrough a non-linearity,",
    "start": "3984915",
    "end": "3990420"
  },
  {
    "text": "and that will give\nme a final value. And then I'll slide\nmy filter down,",
    "start": "3990420",
    "end": "3996120"
  },
  {
    "text": "and I'd work out a computation\nfor this window of three words,",
    "start": "3996120",
    "end": "4002600"
  },
  {
    "text": "and take 0.5 times 3 plus\n0.2 times 1, et cetera,",
    "start": "4002600",
    "end": "4008000"
  },
  {
    "text": "and that comes\nout as this value. I add the bias, I'm going to\nput it through my non-linearity.",
    "start": "4008000",
    "end": "4015650"
  },
  {
    "text": "And then I keep on sliding\ndown and I'll do the next three words, and keep on going down.",
    "start": "4015650",
    "end": "4022670"
  },
  {
    "text": "And so that gives\nme a 1D convolution and computes a\nrepresentation of the text.",
    "start": "4022670",
    "end": "4027890"
  },
  {
    "text": " You might have noticed\nin the previous example,",
    "start": "4027890",
    "end": "4033619"
  },
  {
    "text": "that I started here\nwith seven words, that because I wanted to have a\nwindow of 3 for my convolution,",
    "start": "4033620",
    "end": "4041690"
  },
  {
    "text": "the end result is\nthat things shrunk. So in the output I only\nhad five things, that's",
    "start": "4041690",
    "end": "4047720"
  },
  {
    "text": "not necessarily desirable. So commonly, people will\ndeal with that with padding.",
    "start": "4047720",
    "end": "4053780"
  },
  {
    "text": "So if I put padding\non both sides, I can then start my 3 by 3\nconvolution, my 3 size, not 3",
    "start": "4053780",
    "end": "4061340"
  },
  {
    "text": "by 3, my 3 convolutional\nhere and compute this one,",
    "start": "4061340",
    "end": "4066740"
  },
  {
    "text": "and then slide it down\n1, and compute this one. And so now my output is the\nsame size as my real input,",
    "start": "4066740",
    "end": "4076280"
  },
  {
    "text": "and so that's a\nconvolution with padding. ",
    "start": "4076280",
    "end": "4081950"
  },
  {
    "text": "OK. So that was the start\nof things but how you get more power of\nthe convolutional network",
    "start": "4081950",
    "end": "4088900"
  },
  {
    "text": "is, you don't only\nhave one filter, you have several filters. So if I have three\nfilters, each of which",
    "start": "4088900",
    "end": "4096100"
  },
  {
    "text": "will have their own\nbias and non-linearity, I can then get a three\ndimensional representation",
    "start": "4096100",
    "end": "4102700"
  },
  {
    "text": "coming out the end. And sort of you can think of\nthese as conceptually computing",
    "start": "4102700",
    "end": "4108220"
  },
  {
    "text": "different features of your text.  OK, so that gives us a sort of\na new feature re-representation",
    "start": "4108220",
    "end": "4118720"
  },
  {
    "text": "of our text. But commonly, we then want to\nsomehow summarize what we have.",
    "start": "4118720",
    "end": "4127600"
  },
  {
    "text": "And a very common way of\nsummarizing what we have is to then do pooling.",
    "start": "4127600",
    "end": "4133880"
  },
  {
    "text": "So if we sort of think\nof these features as detecting different things\nin the text, so they might even",
    "start": "4133880",
    "end": "4141399"
  },
  {
    "text": "be high level features\nlike, does this show",
    "start": "4141399",
    "end": "4146979"
  },
  {
    "text": "signs of toxicity\nor hate speech, is there a reference\nto something.",
    "start": "4146979",
    "end": "4153140"
  },
  {
    "text": "So if you want to be interested\nin does it occur anywhere in the text, what\npeople often then do is, the max pooling operation.",
    "start": "4153140",
    "end": "4160299"
  },
  {
    "text": "Where for each\nfeature, they simply sort of compute\nthe maximum value",
    "start": "4160300",
    "end": "4166240"
  },
  {
    "text": "it ever achieved in any position\nas you went through the text and say that, this\nvector ends up",
    "start": "4166240",
    "end": "4172660"
  },
  {
    "text": "as the sentence representation. Sometimes for other purposes,\nrather than max pooling,",
    "start": "4172660",
    "end": "4178210"
  },
  {
    "text": "people use average\npooling, where you take the averages\nof the different vectors",
    "start": "4178210",
    "end": "4183370"
  },
  {
    "text": "to get the sentence\nrepresentation. But in general max\npooling has been found to be more successful.",
    "start": "4183370",
    "end": "4189890"
  },
  {
    "text": "And that's kind\nof because if you think of it as feature\ndetectors, that are wanting to detect was this\npresent somewhere,",
    "start": "4189890",
    "end": "4197110"
  },
  {
    "text": "then something like\npositive sentiment isn't going to be present in\nevery three word subsequence",
    "start": "4197110",
    "end": "4205960"
  },
  {
    "text": "you choose. The first there are\nsomewhere there, and so often max pooling works better.",
    "start": "4205960",
    "end": "4213660"
  },
  {
    "text": "And so that's a very quick\nlook at convolutional neural networks, except to\nsay, this example",
    "start": "4213660",
    "end": "4221430"
  },
  {
    "text": "was doing 1D\nconvolutions with words. But a very common place that\nconvolutional neural networks",
    "start": "4221430",
    "end": "4228780"
  },
  {
    "text": "are being used in\nnatural language, is actually using\nthem with characters.",
    "start": "4228780",
    "end": "4233900"
  },
  {
    "text": "And so what you can do is,\nyou can do convolutions",
    "start": "4233900",
    "end": "4238969"
  },
  {
    "text": "over subsequences of the\ncharacters in the same way. And if you do that, this allows\nyou to compute a representation",
    "start": "4238970",
    "end": "4247850"
  },
  {
    "text": "for any sequence of characters. So you don't have any problems\nwith being out of vocabulary,",
    "start": "4247850",
    "end": "4253820"
  },
  {
    "text": "or anything like that, because\nfor any sequence of characters you just compute your\nconvolutional representation",
    "start": "4253820",
    "end": "4260690"
  },
  {
    "text": "and max pool it. And so quite commonly, people\nuse a character convolution",
    "start": "4260690",
    "end": "4267980"
  },
  {
    "text": "to give a\nrepresentation of words, perhaps as the only\nrepresentation of words.",
    "start": "4267980",
    "end": "4273770"
  },
  {
    "text": "But otherwise it's something\nthat you use in addition to a word vector and so on.",
    "start": "4273770",
    "end": "4280280"
  },
  {
    "text": "And so in both by\ndepth in the model I'm about to show,\nthat at the base level it makes use of both a\nword vector representation",
    "start": "4280280",
    "end": "4288020"
  },
  {
    "text": "that we saw right at the\nbeginning of the text, and a character\nlevel convolutional",
    "start": "4288020",
    "end": "4293510"
  },
  {
    "text": "representation of the words. OK. With that said, I\nnow want to show you",
    "start": "4293510",
    "end": "4300410"
  },
  {
    "text": "before time runs out an\nend-to-end neural coref model. So the model I'm going to\nshow you is Kenton Lee's",
    "start": "4300410",
    "end": "4307730"
  },
  {
    "text": "one from University\nof Washington 2017,",
    "start": "4307730",
    "end": "4313220"
  },
  {
    "text": "this is no longer\nthe state of the art, I'll mention the state\nof the art at the end. But this was the\nfirst model that",
    "start": "4313220",
    "end": "4320929"
  },
  {
    "text": "really said, get rid of all\nof that old stuff of having pipelines and mention\ndetection first, just build",
    "start": "4320930",
    "end": "4327560"
  },
  {
    "text": "one end-to-end big\nmodel, that does everything and returns\ncoreference so it's a good one",
    "start": "4327560",
    "end": "4332570"
  },
  {
    "text": "to show. So compared to the earlier\nsimple thing I saw,",
    "start": "4332570",
    "end": "4338150"
  },
  {
    "text": "we're now going to process\nthe text with BiLSTMs. We're going to make\nuse of attention,",
    "start": "4338150",
    "end": "4344179"
  },
  {
    "text": "and we're going to do\nall of mention detection and coreference in\none step, end-to-end.",
    "start": "4344180",
    "end": "4349730"
  },
  {
    "text": "And the way it tells\nthat is by considering every span of the text\nup to a certain length",
    "start": "4349730",
    "end": "4357170"
  },
  {
    "text": "as a candidate mentioned,\nand just figures out a representation for\nit and whether it's",
    "start": "4357170",
    "end": "4362360"
  },
  {
    "text": "coreferent to other things. So what we do at\nthe start is, we start with the\nsequence of words,",
    "start": "4362360",
    "end": "4369530"
  },
  {
    "text": "and we calculate from those\nour standard word embedding, and a character level\nCNN's embedding.",
    "start": "4369530",
    "end": "4377240"
  },
  {
    "text": "We then feed those as inputs\ninto a bidirectional LSTM",
    "start": "4377240",
    "end": "4383720"
  },
  {
    "text": "of the kind that we saw\nquite a lot of before, but then after this, what we do\nis we compute representations",
    "start": "4383720",
    "end": "4393140"
  },
  {
    "text": "for spans. So when we have a\nsequence of words,",
    "start": "4393140",
    "end": "4398720"
  },
  {
    "text": "we're then going to work\nout a representation of a sequence of words\nwhich we can then",
    "start": "4398720",
    "end": "4403730"
  },
  {
    "text": "put into our coreference model. So I can't fully\nillustrate in this picture,",
    "start": "4403730",
    "end": "4411960"
  },
  {
    "text": "but sort of subsequences of\ndifferent lengths, so like, General, General Electric,\nGeneral Electric said,",
    "start": "4411960",
    "end": "4418980"
  },
  {
    "text": "will all have a\nspan representation, which I've only shown a\nsubset of them in green.",
    "start": "4418980",
    "end": "4425820"
  },
  {
    "text": "So how are those computed? Well the way they're computed\nis that the span representation",
    "start": "4425820",
    "end": "4433079"
  },
  {
    "text": "is a vector that\nconcatenates several vectors, and it consists of four parts.",
    "start": "4433080",
    "end": "4438920"
  },
  {
    "text": "It consists of\nthe representation that was computed for the start\nof the span from the BiLSTM.",
    "start": "4438920",
    "end": "4451230"
  },
  {
    "text": "The representation for\nthe end from the BiLSTM that's over here, and\nthen it has a third part",
    "start": "4451230",
    "end": "4457470"
  },
  {
    "text": "that's kind of interesting. This is an attention\nbased representation that is calculated\nfrom the whole span,",
    "start": "4457470",
    "end": "4465840"
  },
  {
    "text": "but particularly sort of\nlooks for the head of a span. And then there are still\na few additional features.",
    "start": "4465840",
    "end": "4472390"
  },
  {
    "text": "So it turns out that some\nof these additional things unlike length and so on\nare still a bit useful.",
    "start": "4472390",
    "end": "4480880"
  },
  {
    "text": "So to work out the\nfinal part, is not",
    "start": "4480880",
    "end": "4486510"
  },
  {
    "text": "the beginning and\nthe end, what's done is to calculate an attention\nweighted average of the word embeddings.",
    "start": "4486510",
    "end": "4492450"
  },
  {
    "text": "So what you're doing is,\nyou're taking the X star representation of the\nfinal word of the span,",
    "start": "4492450",
    "end": "4500610"
  },
  {
    "text": "and you're feeding that\ninto a neural network to get attention scores for\nevery word in the stand, which",
    "start": "4500610",
    "end": "4509790"
  },
  {
    "text": "are these three. And that's giving you an\nattention distribution as we've seen previously.",
    "start": "4509790",
    "end": "4516090"
  },
  {
    "text": "And then you're calculating\nthe third component of this as an attention weighted\nsum of the different words",
    "start": "4516090",
    "end": "4528130"
  },
  {
    "text": "in the span. And so therefore\nyou've got the sort of the soft average\nof the representations",
    "start": "4528130",
    "end": "4533793"
  },
  {
    "text": "of the words of the span.  OK.",
    "start": "4533793",
    "end": "4539260"
  },
  {
    "text": "So then once you've got\nthat, what you're doing",
    "start": "4539260",
    "end": "4545500"
  },
  {
    "text": "is then feeding\nthese representations into having scores for where the\nspans are coreferent mentions.",
    "start": "4545500",
    "end": "4555400"
  },
  {
    "text": "So you have a representation\nof the two spans,",
    "start": "4555400",
    "end": "4561940"
  },
  {
    "text": "you have a score\nthat's calculated for whether two different\nspans look coreferent,",
    "start": "4561940",
    "end": "4568690"
  },
  {
    "text": "and that overall\nyou're getting a score for are different spans\nlooking coreferent or not.",
    "start": "4568690",
    "end": "4577400"
  },
  {
    "text": "And so this model is just\nrun end-to-end models spans, now sort of would\nget intractable",
    "start": "4577400",
    "end": "4584500"
  },
  {
    "text": "if you scored literally every\nspan in a long piece of text. So they do some pruning,\nthey sort of only allow",
    "start": "4584500",
    "end": "4591520"
  },
  {
    "text": "spans up to a\ncertain maximum size, they only consider\npairs of spans",
    "start": "4591520",
    "end": "4598030"
  },
  {
    "text": "that aren't too distant\nfrom each other, et cetera, et cetera. But basically, it's sort\nof an approximation to just",
    "start": "4598030",
    "end": "4605890"
  },
  {
    "text": "a complete comparison of spans. And this turns into a\nvery effective coreference",
    "start": "4605890",
    "end": "4610940"
  },
  {
    "text": "resolution algorithm. Today it's not the best\ncoreference resolution algorithm, because\nmaybe not surprisingly",
    "start": "4610940",
    "end": "4619239"
  },
  {
    "text": "like everything else that\nwe've been dealing with, there's now been these\ntransformer models",
    "start": "4619240",
    "end": "4625090"
  },
  {
    "text": "like BERT have come\nalong, and that they produce even better results. So the best coreference\nsystems now make use of BERT.",
    "start": "4625090",
    "end": "4635500"
  },
  {
    "text": "In particular, when\nDanqi spoke, she briefly mentioned SpanBERT which\nis a variant of BERT which",
    "start": "4635500",
    "end": "4643810"
  },
  {
    "text": "blanks out for reconstruction\nsubsequences of words, rather than just a single word.",
    "start": "4643810",
    "end": "4650140"
  },
  {
    "text": "And SpanBERT has actually proven\nto be very effective for doing coreference, perhaps\nbecause she didn't",
    "start": "4650140",
    "end": "4656829"
  },
  {
    "text": "blank out whole mentions. People have also gotten\ngains actually, finally,",
    "start": "4656830",
    "end": "4663790"
  },
  {
    "text": "by treating coref as a\nquestion answering task. So effectively you can find a\nmention like he or the person",
    "start": "4663790",
    "end": "4675340"
  },
  {
    "text": "and say what is its antecedent,\nand get on the question answering answer, and\nthat that's a good way",
    "start": "4675340",
    "end": "4682090"
  },
  {
    "text": "to do coreference. So if we put that together,\nas time is running out,",
    "start": "4682090",
    "end": "4689110"
  },
  {
    "text": "let me just sort of\ngive you some sense of how results come out\nfor coreference systems.",
    "start": "4689110",
    "end": "4697090"
  },
  {
    "text": "So I'm skipping a\nbit actually that you can find in the slides, which\nis how coreference is scored.",
    "start": "4697090",
    "end": "4704230"
  },
  {
    "text": "But essentially it's scored\non a clustering metric, so a perfect clustering\nwill give you 100,",
    "start": "4704230",
    "end": "4710170"
  },
  {
    "text": "and something that makes\nno correct decisions would give you 0.",
    "start": "4710170",
    "end": "4715540"
  },
  {
    "text": "And so this is sort of how\nthe coreference numbers have been panning out.",
    "start": "4715540",
    "end": "4722590"
  },
  {
    "text": "So back in 2010, actually\nthis was a Stanford system.",
    "start": "4722590",
    "end": "4728070"
  },
  {
    "text": "This was a state of the\nart system for coreference that won a competition, it was\nactually a non-machine learning",
    "start": "4728070",
    "end": "4735380"
  },
  {
    "text": "model, because again we\nwanted to sort of prove how these rule based methods in\npractice worked kind of well.",
    "start": "4735380",
    "end": "4742170"
  },
  {
    "text": "And so its accuracy was around\n55 for English, 50 for Chinese.",
    "start": "4742170",
    "end": "4748489"
  },
  {
    "text": "Then gradually,\nmachine learning, this was sort of statistical\nmachine learning models",
    "start": "4748490",
    "end": "4754220"
  },
  {
    "text": "got a bit better. Wiseman was the very first\nneural coreference system,",
    "start": "4754220",
    "end": "4760070"
  },
  {
    "text": "and that gave some gains. Here's a system that\nKevin Clark and I did, which gave a little\nbit further gains.",
    "start": "4760070",
    "end": "4768440"
  },
  {
    "text": "So Lee is the model\nthat I've just shown you, as the\nend to end model",
    "start": "4768440",
    "end": "4774500"
  },
  {
    "text": "and it got a bit\nof further gains. But then again what gave\nthe huge breakthrough just",
    "start": "4774500",
    "end": "4782300"
  },
  {
    "text": "like question answering, was\nthat the use of SpanBERT. So once we moved to here\nwe're now using SpanBERT,",
    "start": "4782300",
    "end": "4789730"
  },
  {
    "text": "but that's giving you\nabout an extra 10% or so. The coref QA technique\nproved to be useful.",
    "start": "4789730",
    "end": "4797880"
  },
  {
    "text": "And then the very latest\nbest results are effectively combining together\nSpanBERT and well,",
    "start": "4797880",
    "end": "4804980"
  },
  {
    "text": "a larger version of SpanBERT and\ncoref QA and getting up to 83.",
    "start": "4804980",
    "end": "4811489"
  },
  {
    "text": "So you might think\nfrom that, that coref is sort of doing really well,\nand is getting close to solved",
    "start": "4811490",
    "end": "4819410"
  },
  {
    "text": "like other NLP tasks. Well it's certainly true\nthat in neural times",
    "start": "4819410",
    "end": "4825440"
  },
  {
    "text": "the results have been\ngetting way, way better than they had been before. But I would caution you that\nthese results that I just",
    "start": "4825440",
    "end": "4832820"
  },
  {
    "text": "showed were on a corpus\ncalled OntoNotes, which is mainly newswire. And it turns out that newswire\ncoreference is pretty easy.",
    "start": "4832820",
    "end": "4841820"
  },
  {
    "text": "I mean in particular,\nthere's a lot of mention of the\nsame entities, right? So the newspaper articles are\nfull of mentions of the United",
    "start": "4841820",
    "end": "4849980"
  },
  {
    "text": "States and China, and leaders\nof the different countries, and it's sort of\nvery easy to work out",
    "start": "4849980",
    "end": "4856940"
  },
  {
    "text": "what they're coreferent to. And so the coreference\nscores are fairly high,",
    "start": "4856940",
    "end": "4863510"
  },
  {
    "text": "whereas if what you\ndo is take something like a page of\ndialogue from a novel",
    "start": "4863510",
    "end": "4870139"
  },
  {
    "text": "and feed that into a system\nand say, OK, do the coreference correctly, you'll\nfind pretty rapidly",
    "start": "4870140",
    "end": "4877580"
  },
  {
    "text": "that the performance of the\nmodels is much more modest. If you'd like to try\nout a coreference system",
    "start": "4877580",
    "end": "4884240"
  },
  {
    "text": "for yourself, there are\npointers to a couple of them here where the top one Al's\nfrom sort of Kevin Clark's",
    "start": "4884240",
    "end": "4894110"
  },
  {
    "text": "neural coreference,\nand this is one that goes with the\nHugging Face repository that we've mentioned.",
    "start": "4894110",
    "end": "4901360"
  },
  {
    "start": "4901360",
    "end": "4905238"
  }
]