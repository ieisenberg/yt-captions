[
  {
    "text": "so hopefully everyone's having a good time with assignment one Uh it's due tonight Uh let us know if you need an",
    "start": "5120",
    "end": "10960"
  },
  {
    "text": "extension Assignment due two is uh coming out soon We're putting on the finishing touches onto some of the the",
    "start": "10960",
    "end": "17920"
  },
  {
    "text": "Triton stuff Um hopefully you'll enjoy it You'll get to implement um Flash Attention 2 or parts of Flash Attention",
    "start": "17920",
    "end": "24240"
  },
  {
    "text": "2 which I think will be nice So today we're going to talk about uh GPUs GPUs are the thing that makes our",
    "start": "24240",
    "end": "32880"
  },
  {
    "text": "language models go So they're pretty critical to get right Um and if you",
    "start": "32880",
    "end": "38320"
  },
  {
    "text": "haven't really studied you know the hardware that makes you know your models run they can seem pretty mysterious So",
    "start": "38320",
    "end": "45040"
  },
  {
    "text": "my goal today is to try to make CUDA and GPUs less magic Um and one of the things",
    "start": "45040",
    "end": "51920"
  },
  {
    "text": "that I want to demystify you don't have to understand the plot There's a there's a lot on the slide I know Um you know",
    "start": "51920",
    "end": "57760"
  },
  {
    "text": "why do GPUs get slow And they get slow in very mysterious ways You know I will",
    "start": "57760",
    "end": "63199"
  },
  {
    "text": "try to talk through this plot um near towards the end of lecture As you increase the size of your matrix",
    "start": "63199",
    "end": "69040"
  },
  {
    "text": "multiplies you might expect you know either gets slower or faster or whatever",
    "start": "69040",
    "end": "74320"
  },
  {
    "text": "you get these very unpredictable looking wavelike patterns and you're like why is my GPU fast at certain multiples of",
    "start": "74320",
    "end": "82000"
  },
  {
    "text": "certain numbers and slow at others Right It's very mysterious We'll try to understand that The other thing is we",
    "start": "82000",
    "end": "88640"
  },
  {
    "text": "would like to understand how to make fast algorithms Um I think almost all of you have heard of flash attention Um",
    "start": "88640",
    "end": "94479"
  },
  {
    "text": "it's the thing that makes you know much longer context possible by very cleverly computing the attention operation inside",
    "start": "94479",
    "end": "101360"
  },
  {
    "text": "a transformer And so maybe you would like to you know come up with new algorithms or new implementations like",
    "start": "101360",
    "end": "107840"
  },
  {
    "text": "flash attention right like what primitives and what components do we need to understand in order to be able",
    "start": "107840",
    "end": "112960"
  },
  {
    "text": "to do that right so those are kind of the two learning goals of today the first one is you know by the end of the lecture you should feel kind of",
    "start": "112960",
    "end": "118719"
  },
  {
    "text": "comfortable with GPUs you should kind of understand how they work and the second one is you should feel comfortable accelerating certain parts of your",
    "start": "118719",
    "end": "125040"
  },
  {
    "text": "algorithms you make a new architecture you should hopefully feel like you can try to to accelerate that with CUDA Um",
    "start": "125040",
    "end": "132879"
  },
  {
    "text": "and because hardware is not necessarily the domain in which I work you know there's uh special resources that I have",
    "start": "132879",
    "end": "138800"
  },
  {
    "text": "to give a lot of credit to especially Horus Heath's blog where he's got a lot of fun GPU facts that you can learn",
    "start": "138800",
    "end": "144160"
  },
  {
    "text": "about For example why are matrix multiplies that are filled with zeros faster than ones that are not filled",
    "start": "144160",
    "end": "149280"
  },
  {
    "text": "with zeros You can learn by going to his blog There's also other resources that I've drawn from like the CUDA mode group",
    "start": "149280",
    "end": "155120"
  },
  {
    "text": "and the and the nice TPU book from Google Um if this topic interests you you know I'd encourage you to go and",
    "start": "155120",
    "end": "160160"
  },
  {
    "text": "look at those resources to learn more because this is in in some ways like a shallow but hopefully you know complete",
    "start": "160160",
    "end": "165519"
  },
  {
    "text": "coverage of the hardware So today we're only going to focus on uh you know nonp parallel parts",
    "start": "165519",
    "end": "172239"
  },
  {
    "text": "of the hardware stack So we're going to study the GPU like a single accelerator in depth how they work and some",
    "start": "172239",
    "end": "178080"
  },
  {
    "text": "important parts I'm also going to talk very very briefly about TPUs because in some ways they're very similar",
    "start": "178080",
    "end": "183840"
  },
  {
    "text": "conceptually to a GPU And so my discussion here is going to carry over Um and then once we understand kind of",
    "start": "183840",
    "end": "189519"
  },
  {
    "text": "the hardware and execution model of the GPU then we're going to try to understand what makes GPUs go fast on",
    "start": "189519",
    "end": "195920"
  },
  {
    "text": "certain workloads what makes them slow We're going to understand the performance And in the last part this is kind of going to be almost like a",
    "start": "195920",
    "end": "201840"
  },
  {
    "text": "hands-on piece Um I'm going to try to walk through flash attention right I'm going to take all the lessons that we've",
    "start": "201840",
    "end": "207760"
  },
  {
    "text": "learned and try to walk you through flash attention saying see here's how it all comes together Right So that's the",
    "start": "207760",
    "end": "212799"
  },
  {
    "text": "last part um of today's lecture So you know many of you have you",
    "start": "212799",
    "end": "219920"
  },
  {
    "text": "know taken an NLP course and these days in an NLP course I think you teach some amount of scaling laws and so you've",
    "start": "219920",
    "end": "225280"
  },
  {
    "text": "probably seen this right and so this is just setting the context Um we know that",
    "start": "225280",
    "end": "230480"
  },
  {
    "text": "having more compute is helpful for training large language models Um this is a pre-training scaling chart but you",
    "start": "230480",
    "end": "236080"
  },
  {
    "text": "could replace this with an inference scaling chart if you would like Um it's generally agreed upon that the more",
    "start": "236080",
    "end": "241120"
  },
  {
    "text": "compute you have the more processing you can do on your data You can ingest more data you can train larger models all of",
    "start": "241120",
    "end": "246480"
  },
  {
    "text": "those lead to improved performance right So you might think of of course you know deep learning is really important but",
    "start": "246480",
    "end": "252080"
  },
  {
    "text": "what's really driven performance is you know faster hardware better utilization improved parallelization right So that's",
    "start": "252080",
    "end": "258639"
  },
  {
    "text": "kind of setting the stage of why hardware is important to understand And of course you know once you think about",
    "start": "258639",
    "end": "264880"
  },
  {
    "text": "compute scaling you ask okay how do we get compute scaling How do we get our models to train faster So kind of in the",
    "start": "264880",
    "end": "271919"
  },
  {
    "text": "early days you know of of semiconductor scaling um if you were thinking about okay are CPUs how do they get faster um",
    "start": "271919",
    "end": "279360"
  },
  {
    "text": "they were they you know would scale under something called Dennard scaling right um with Moors law you would sort",
    "start": "279360",
    "end": "284800"
  },
  {
    "text": "of double the the amount of transistors on a chip every year um and if you have this doubling what you end up is um",
    "start": "284800",
    "end": "292240"
  },
  {
    "text": "darded scaling where smaller and smaller transistors can be driven at faster and faster clock speeds with lower and lower",
    "start": "292240",
    "end": "297440"
  },
  {
    "text": "power um which in turn give you more performance right and in the in the 1980s to 2000s this sort of tapped out",
    "start": "297440",
    "end": "303680"
  },
  {
    "text": "You can kind of see in this chart here by Hennessy and Patterson that single thread performance that's the blue dots",
    "start": "303680",
    "end": "308720"
  },
  {
    "text": "here um that basically started to taper out Of course the number of transistors didn't really you know start falling off",
    "start": "308720",
    "end": "314800"
  },
  {
    "text": "You did have you know chips with higher and higher transistor densities but that wasn't helpful It wasn't giving you higher uh throughput on single threads",
    "start": "314800",
    "end": "322240"
  },
  {
    "text": "Um and so this means that we can't just do computation faster in absolute terms",
    "start": "322240",
    "end": "327600"
  },
  {
    "text": "You know what we have to make up for it with is parallel scaling right So the",
    "start": "327600",
    "end": "332960"
  },
  {
    "text": "story of scaling for deep learning and neural networks is going from single thread scaling which is just doing your",
    "start": "332960",
    "end": "339280"
  },
  {
    "text": "computation faster in absolute terms um to parallel scaling where you have a lot of workloads that are all computed at",
    "start": "339280",
    "end": "345440"
  },
  {
    "text": "once Um and this is is one of my favorite you know uh sort of compute scaling charts by by Bill Dowy in his",
    "start": "345440",
    "end": "351840"
  },
  {
    "text": "keynote Um where you know he's showing the super exponential increase in the number of um sort of integer operations",
    "start": "351840",
    "end": "358720"
  },
  {
    "text": "per second um going from you know the earliest K20s um to the H100 right And",
    "start": "358720",
    "end": "364639"
  },
  {
    "text": "it's kind of like this really remarkable uh exponential or super exponential curve And so you know we have to really",
    "start": "364639",
    "end": "372160"
  },
  {
    "text": "understand how to take advantage of this curve in order to really get the most out of our language model right So",
    "start": "372160",
    "end": "378800"
  },
  {
    "text": "that's kind of going to be our goal And so I've already hinted at this",
    "start": "378800",
    "end": "384319"
  },
  {
    "text": "this kind of important difference right CPU is is something that I think everyone's familiar with once you sort",
    "start": "384319",
    "end": "389840"
  },
  {
    "text": "start doing programming right It's this execution model of you have a program it goes through and in a single thread it",
    "start": "389840",
    "end": "395360"
  },
  {
    "text": "executes step by step what's happening And in order to support that kind of an execution model what what do you need",
    "start": "395360",
    "end": "400800"
  },
  {
    "text": "Well you need big control units You just need to generally run these things very quickly because you have a lot of",
    "start": "400800",
    "end": "406720"
  },
  {
    "text": "branching and you have a lot of conditional control logic right So a CPU this is a abstracted diagram is going to",
    "start": "406720",
    "end": "412960"
  },
  {
    "text": "dedicate you know a lot of of its chip towards you know large control branch",
    "start": "412960",
    "end": "418160"
  },
  {
    "text": "prediction and it's going to run these you know very quickly because it doesn't have that many threads you know there",
    "start": "418160",
    "end": "423280"
  },
  {
    "text": "there are CPUs with lots and lots of cores now but compared to a GPU it's almost nothing And so in in contrast the",
    "start": "423280",
    "end": "430160"
  },
  {
    "text": "GPU has really tons and tons of compute units ALUS right So there's the little",
    "start": "430160",
    "end": "435759"
  },
  {
    "text": "green boxes and there's much smaller amounts of the chip dedicated to control So there's a little bit of control logic",
    "start": "435759",
    "end": "442240"
  },
  {
    "text": "sort of orchestrating tons and tons of compute units you know operating in parallel Um and I think mentally so this",
    "start": "442240",
    "end": "449759"
  },
  {
    "text": "is kind of the the picture of what is being emphasized in in a CPU versus GPU But if you kind of look at what's the",
    "start": "449759",
    "end": "455919"
  },
  {
    "text": "design goals are they they designed for very different sort of goals So you can think about CPUs as optimizing for",
    "start": "455919",
    "end": "462639"
  },
  {
    "text": "latency I want to finish my tasks as quickly as possible So if I have tasks um T1 through uh T4 here on on the right",
    "start": "462639",
    "end": "469840"
  },
  {
    "text": "side you know in a CPU I'm going to try to finish each task as quickly as possible And so if you want any one of",
    "start": "469840",
    "end": "476319"
  },
  {
    "text": "these tasks to be finished quickly T1's going to complete really quickly In GPU you're optimizing for high throughput",
    "start": "476319",
    "end": "482720"
  },
  {
    "text": "Like I don't care about latency I just want all of my tasks that I have in aggregate to complete as quickly as",
    "start": "482720",
    "end": "488080"
  },
  {
    "text": "possible And to support that you know maybe you have lots of threads and these threads can go to sleep and wake up very quickly Um and in the end you know you",
    "start": "488080",
    "end": "495199"
  },
  {
    "text": "you finish all of your workload T1 through T4 you know before the the CPU one does even though individually all of",
    "start": "495199",
    "end": "501919"
  },
  {
    "text": "these have sort of higher latency right So they have different sort of design principles um and design",
    "start": "501919",
    "end": "507879"
  },
  {
    "text": "goals Okay And so um a GPU has a has a pretty different anatomy And I don't",
    "start": "507879",
    "end": "514000"
  },
  {
    "text": "know if you know you all have ever looked at what a a GPU sort of layout diagram looks like I'll actually show",
    "start": "514000",
    "end": "519440"
  },
  {
    "text": "you the the chip um figures in a moment here Um but the core idea and this is",
    "start": "519440",
    "end": "525600"
  },
  {
    "text": "important conceptual concepts behind a GPU um is that a GPU executes you know",
    "start": "525600",
    "end": "531680"
  },
  {
    "text": "uh many many SM streaming multipprocessors and a streaming multipprocessor you can kind of think of",
    "start": "531680",
    "end": "537519"
  },
  {
    "text": "as an atomic unit when you're programming in something like Triton they're going to operate at the level of a of an SM and within each SM they're",
    "start": "537519",
    "end": "545200"
  },
  {
    "text": "going to uh it contains many SPS streaming processors and a streaming processor is going to execute a whole",
    "start": "545200",
    "end": "552800"
  },
  {
    "text": "bunch of threads in parallel so one way to think about it is SM has a bunch of control logic It can decide what to",
    "start": "552800",
    "end": "559440"
  },
  {
    "text": "execute It can do for example branching SPs are going to operate to to take the",
    "start": "559440",
    "end": "564560"
  },
  {
    "text": "same instruction and apply it to many different pieces of data right And so you can do tons and tons of parallel",
    "start": "564560",
    "end": "570560"
  },
  {
    "text": "computation um under this model An SM is sort of each granular unit of control SP",
    "start": "570560",
    "end": "576080"
  },
  {
    "text": "can do a lot of computation individually And if you look at an A100 which is the previous generation GPU at this point um",
    "start": "576080",
    "end": "582720"
  },
  {
    "text": "you've got 128 SM you know that's a lot more than than most cores for for CPUs And each of these SM is going to have a",
    "start": "582720",
    "end": "589519"
  },
  {
    "text": "very large number of uh SPS and specialized sort of matrix multiply units um inside",
    "start": "589519",
    "end": "596720"
  },
  {
    "text": "them And so that's kind of the the compute model Uh was there a question Sorry",
    "start": "598040",
    "end": "603080"
  },
  {
    "text": "Yeah to get the slide before GPUs So is this",
    "start": "603080",
    "end": "609360"
  },
  {
    "text": "GPU the same as a GPU So the question was is this GPU the same as that GPU Yes Like this is a this is a kind of cartoon",
    "start": "609360",
    "end": "615760"
  },
  {
    "text": "version of of this You can kind of think of each row as being SM It's got its own control units Each green block might be",
    "start": "615760",
    "end": "622880"
  },
  {
    "text": "sort of uh one of these green blocks here like a SP32 uh sort of processing unit inside of it And each SM can sort",
    "start": "622880",
    "end": "629440"
  },
  {
    "text": "of you know operate various pieces that it that it owns like the tensor cores to do",
    "start": "629440",
    "end": "636279"
  },
  {
    "text": "computation Cool Okay And there's going to be uh two",
    "start": "636279",
    "end": "642720"
  },
  {
    "text": "important things You think of GPU as you know computers they compute but actually computation is only one of the two",
    "start": "642720",
    "end": "648720"
  },
  {
    "text": "important things we have to keep track of right memory is arguably more important um at this point and it will",
    "start": "648720",
    "end": "653920"
  },
  {
    "text": "continue to be more important in terms of the performance profiles of how we run our programs on the GPU Um and so to",
    "start": "653920",
    "end": "660640"
  },
  {
    "text": "understand memory you kind of have to understand the physical layout of the GPU and the chip because in some sense",
    "start": "660640",
    "end": "665680"
  },
  {
    "text": "the you know when you're operating at such fast speeds the physical proximity of the memory starts to matter quite a",
    "start": "665680",
    "end": "672079"
  },
  {
    "text": "bit And so I will show you kind of the physical proximity of how things are laid out and how that relates to how you should think about memory access um and",
    "start": "672079",
    "end": "679079"
  },
  {
    "text": "performance So the closer a piece of memory is to to each SM um the faster",
    "start": "679079",
    "end": "684640"
  },
  {
    "text": "it's going to be So there's going to be certain very very very fast kinds of memory like L1 um and shared memory and",
    "start": "684640",
    "end": "691040"
  },
  {
    "text": "that's going to live inside uh of the SM right And that's going to be really fast right Things like registers things like",
    "start": "691040",
    "end": "696959"
  },
  {
    "text": "things you're reading and writing very frequently you're going to want to put into the L1 and shared memory L2 cache",
    "start": "696959",
    "end": "703120"
  },
  {
    "text": "Um as you can kind of see there's these green areas which are SM And then there's these blue areas This is on the",
    "start": "703120",
    "end": "709040"
  },
  {
    "text": "GPU chip right These are L2 uh memory that's sort of right next to the SM",
    "start": "709040",
    "end": "714079"
  },
  {
    "text": "right So they're they're not inside the SM but they're physically still quite close right Um and these are still",
    "start": "714079",
    "end": "719360"
  },
  {
    "text": "pretty fast Um you know they're they're still a factor of 10 slower but they're",
    "start": "719360",
    "end": "725120"
  },
  {
    "text": "still reasonably fast And then outside of the chip itself this is sort of a you know I think this is like a 3090 card or",
    "start": "725120",
    "end": "732079"
  },
  {
    "text": "something like this or maybe a PCIe 100 Oh this is a PCI 100 Um you know you've got your your GPU here and you've got",
    "start": "732079",
    "end": "738399"
  },
  {
    "text": "actually DRAM sort of living next to the chip right it has to actually go physically outside of the chip um and",
    "start": "738399",
    "end": "745200"
  },
  {
    "text": "connect And you can kind of see on on this uh chip diagram here these yellow connectors at the edges These are HPM",
    "start": "745200",
    "end": "751120"
  },
  {
    "text": "connectors Um these are connecting to the the DRAM chips that are outside of",
    "start": "751120",
    "end": "757440"
  },
  {
    "text": "the actual GPU And you can kind of see the the speed that it takes to access these right the onsm memory is much much",
    "start": "757440",
    "end": "765040"
  },
  {
    "text": "faster like 20 clock cycles to access something from there whereas it's going to take something like 200 or 300 clock",
    "start": "765040",
    "end": "770639"
  },
  {
    "text": "cycles to access something from the L2 cache or global memory right and this factor of 10 is going to hurt you real",
    "start": "770639",
    "end": "776480"
  },
  {
    "text": "bad right so if you if you have um a piece of computation that requires you to access global memory right it might",
    "start": "776480",
    "end": "783680"
  },
  {
    "text": "mean that you actually run out of work to do on your SM you've multiplied all the matrices you've run out now you just have to idle right so utilization won't",
    "start": "783680",
    "end": "790240"
  },
  {
    "text": "be good and this will be a really key theme thinking about memories in some sense the key to thinking about um how",
    "start": "790240",
    "end": "795839"
  },
  {
    "text": "GPUs work and in assignment two you're going to you know actually be writing um high",
    "start": "795839",
    "end": "802480"
  },
  {
    "text": "performance code for a GPU so you have to actually think about um the execution model of how a GPU actually executes",
    "start": "802480",
    "end": "808720"
  },
  {
    "text": "things um and this is somewhat complicated but not not insanely so um",
    "start": "808720",
    "end": "814800"
  },
  {
    "text": "the there's sort of three granularities of things that you need to think about there's blocks there's warps and there's",
    "start": "814800",
    "end": "821440"
  },
  {
    "text": "threads and that's the order in which kind of the granularity narrows down right Blocks are kind of these big",
    "start": "821440",
    "end": "827760"
  },
  {
    "text": "groups of threads and each block is going to be assigned to a SM So think about this as each SM is kind of a",
    "start": "827760",
    "end": "834320"
  },
  {
    "text": "worker It's its own autonomous unit and a block is going to be assigned to an SM for to to process right So this is each",
    "start": "834320",
    "end": "840639"
  },
  {
    "text": "granular unit Now then within these blocks are a whole bunch of threads Each thread is a sort of a piece of task that",
    "start": "840639",
    "end": "847199"
  },
  {
    "text": "needs to be done And when these threads execute they're going to execute in groups Um and this is a thing called a",
    "start": "847199",
    "end": "853440"
  },
  {
    "text": "warp right So you take a block which is a collection of threads and you're going to take you know threads from that block",
    "start": "853440",
    "end": "859680"
  },
  {
    "text": "and they're going to execute in groups of 32 consecutively numbered threads um each time And that's sort of called you",
    "start": "859680",
    "end": "864880"
  },
  {
    "text": "know warps And so you can kind of see at this diagram here what's happening You've got a bunch of blocks Each block",
    "start": "864880",
    "end": "870480"
  },
  {
    "text": "is assigned to a different SM And within each block there's going to be many different warps And each warp is going",
    "start": "870480",
    "end": "876320"
  },
  {
    "text": "to consist of a whole bunch of threads and all of these threads are going to execute um the same instruction on",
    "start": "876320",
    "end": "882720"
  },
  {
    "text": "different data right And so this is kind of the execution model Right Now it's going to it seems probably mysterious",
    "start": "882720",
    "end": "888639"
  },
  {
    "text": "you know what these blocks and warps and threads are They will have important implications uh for our performance in",
    "start": "888639",
    "end": "893839"
  },
  {
    "text": "how we design things like uh CUDA kernels um later So hopefully you can kind of remember this I'll I'll refresh",
    "start": "893839",
    "end": "899199"
  },
  {
    "text": "your memory um kind of as we go Um hopefully that's clear",
    "start": "899199",
    "end": "904720"
  },
  {
    "text": "So that was the kind of logical execution model of a GPU Um and if you understand that you kind of understand",
    "start": "904720",
    "end": "910079"
  },
  {
    "text": "how GPUs execute things Um there's also a logical sort of memory model of a GPU",
    "start": "910079",
    "end": "915440"
  },
  {
    "text": "So you know now I'm not showing you the physical hardware This is just kind of how you think about the programming um of a GPU And so there's registers So",
    "start": "915440",
    "end": "923199"
  },
  {
    "text": "these are really fast you know storing single numbers type storage You've got local memory you got shared memory and",
    "start": "923199",
    "end": "929360"
  },
  {
    "text": "you've got global memory right And that that increases in sort of the the memory hierarchy gets slower and slower and",
    "start": "929360",
    "end": "935600"
  },
  {
    "text": "slower Um and your code can sort of write to to global memory It can also write to constant memory which is not",
    "start": "935600",
    "end": "941360"
  },
  {
    "text": "something that's that's used too often Um and so each thread can access you",
    "start": "941360",
    "end": "946480"
  },
  {
    "text": "know its own register and shared memory But information that goes across blocks need to be written to global memory This",
    "start": "946480",
    "end": "952320"
  },
  {
    "text": "is actually quite important right So now it means that you know whenever you write a thread that executes something",
    "start": "952320",
    "end": "958399"
  },
  {
    "text": "ideally it's operating on sort of the same small amount of data So you load that small amount of data into shared",
    "start": "958399",
    "end": "964079"
  },
  {
    "text": "memory all the threads are very happy accessing that shared memory It terminates it's done right That would be",
    "start": "964079",
    "end": "969199"
  },
  {
    "text": "a great execution model Instead if you have a thread that needs to access data all over the place you know that's going",
    "start": "969199",
    "end": "974320"
  },
  {
    "text": "to have to access global memory that's very very slow This theme will come back you know as we talk about different ways",
    "start": "974320",
    "end": "979680"
  },
  {
    "text": "of of operating uh on a GPU Um hopefully that's clear Um that's kind of the the",
    "start": "979680",
    "end": "985600"
  },
  {
    "text": "very you know high level four slide overview of a GPU If you have kind of questions about how any of that works",
    "start": "985600",
    "end": "991680"
  },
  {
    "text": "feel free to ask me um as I go on Okay so here's a side thread Um last",
    "start": "991680",
    "end": "998880"
  },
  {
    "text": "year I didn't cover this because I think resources on TPU was a little thin Um but the nice TPU book or internet",
    "start": "998880",
    "end": "1005600"
  },
  {
    "text": "website that I mentioned at the start of the lecture came out um and that has actually a lot of nice details and I and",
    "start": "1005600",
    "end": "1011120"
  },
  {
    "text": "I talked to a few Google people about the TPU um and at a high level it's very very similar um to a GPU and so I want",
    "start": "1011120",
    "end": "1017279"
  },
  {
    "text": "to just talk for a moment about TPUs you may never you know operate on a TPU but I think it's important to understand",
    "start": "1017279",
    "end": "1023839"
  },
  {
    "text": "that these alternative accelerators operate in in in many ways very similarly",
    "start": "1023839",
    "end": "1029120"
  },
  {
    "text": "um so here's a diagram of what a TPU looks like um there's kind of a so there's something called a tensor core",
    "start": "1029120",
    "end": "1035520"
  },
  {
    "text": "and mentally you can think about a tensor core as being similar to SM or streaming multipprocessor Each of these are are kind of its own atomic units",
    "start": "1035520",
    "end": "1042160"
  },
  {
    "text": "that can operate on data There's a scalar unit which is basically a control unit and it can also do CPU like",
    "start": "1042160",
    "end": "1048640"
  },
  {
    "text": "arbitrary things You've got a vector unit that can operate on vectors So if you got a vector and you want to operate",
    "start": "1048640",
    "end": "1054080"
  },
  {
    "text": "entry-wise on it that's a good place to do it And then it's got a very big specialized you know part of the chip",
    "start": "1054080",
    "end": "1059760"
  },
  {
    "text": "dedicated to just doing matrix multiplies called the MXU Um and then it's got very fast memory for vector",
    "start": "1059760",
    "end": "1065919"
  },
  {
    "text": "memory and SME Both of these are very fast onchip or like on tensor core memory and then there's high bandwidth",
    "start": "1065919",
    "end": "1072080"
  },
  {
    "text": "memory that lives outside of the chip Right So hopefully you see the similarities to an SM right There's slow memory outside very fast memory inside",
    "start": "1072080",
    "end": "1079280"
  },
  {
    "text": "and there's specialized hardware to do matrix multiplication Core structure is very much the same Um the difference is",
    "start": "1079280",
    "end": "1086240"
  },
  {
    "text": "um I'll talk about this in the parallelism lecture next week You know how the accelerators are together is a little bit different Um and then also",
    "start": "1086240",
    "end": "1093200"
  },
  {
    "text": "you know mention I didn't notice I didn't talk about warps I didn't talk about any of that other stuff Um tensor",
    "start": "1093200",
    "end": "1098640"
  },
  {
    "text": "cores are in some ways very simple because they're optimized to just do matrix multiplies right Like the tensor",
    "start": "1098640",
    "end": "1104080"
  },
  {
    "text": "core unlike the GPU doesn't attempt to do anything but that And so that's in some ways very very simple Uh much",
    "start": "1104080",
    "end": "1109600"
  },
  {
    "text": "simpler in architecture but conceptually doing the same thing",
    "start": "1109600",
    "end": "1115360"
  },
  {
    "text": "Yes Is it tensor also in some ways optimized",
    "start": "1115360",
    "end": "1121720"
  },
  {
    "text": "to general tensor or this is just enough to work on",
    "start": "1121720",
    "end": "1127120"
  },
  {
    "text": "Yeah So so the question was you know is it called tensor because it can operate on arbitrary tensors Um so it can",
    "start": "1127120",
    "end": "1133840"
  },
  {
    "text": "operate on arbitrary tensors like can do the indexing the operations that MXU performs is a matrix multiply and so it",
    "start": "1133840",
    "end": "1139840"
  },
  {
    "text": "would always be like a batch matrix multiply operating on a tensor So it's kind of both a yes and a no answer if",
    "start": "1139840",
    "end": "1145039"
  },
  {
    "text": "that makes sense So they operate on tensors but the operations they always perform are matrix multiplies not more",
    "start": "1145039",
    "end": "1150400"
  },
  {
    "text": "complicated tensor operations that you can do Cool Um the reason why the GPU has",
    "start": "1150400",
    "end": "1157760"
  },
  {
    "text": "been so successful is that you know it scales up really easily If you want more processing power just add more SMS right",
    "start": "1157760",
    "end": "1164880"
  },
  {
    "text": "you don't have to worry about driving the clock faster and getting more heat dissipation problems Um programming wise",
    "start": "1164880",
    "end": "1170960"
  },
  {
    "text": "um CUDA is intimidating but it's actually you know not as horrendous to program because of the its programming",
    "start": "1170960",
    "end": "1176960"
  },
  {
    "text": "model like the way it works is within each SM right you have a thread and it executes the same instruction on a bunch",
    "start": "1176960",
    "end": "1183280"
  },
  {
    "text": "of different pieces of data right that's conceptually sort of easy to reason about you can think through what that means and especially it's nice if you're",
    "start": "1183280",
    "end": "1189280"
  },
  {
    "text": "operating over a matrix and you're doing sort of very simple operations it's exactly this kind of simp model um",
    "start": "1189280",
    "end": "1194960"
  },
  {
    "text": "finally each of these threads are very lightweight and they can be kind of stopped and started at any time And so",
    "start": "1194960",
    "end": "1200000"
  },
  {
    "text": "if you need to wait for another thread um or if you need to sort of like evict something and like start another process",
    "start": "1200000",
    "end": "1205679"
  },
  {
    "text": "all these threads are very lightweight So this just kind of means that there's not much state associated with the threads and they can kind of be stopped",
    "start": "1205679",
    "end": "1211440"
  },
  {
    "text": "and started which allows GPUs to get high utilization um within sort of each",
    "start": "1211440",
    "end": "1218160"
  },
  {
    "text": "SM So GPUs you know obviously graphics processing units Um and for for much of",
    "start": "1218679",
    "end": "1224720"
  },
  {
    "text": "its life you know in the early days it was not used to do scientific computing Um but you know people because it was",
    "start": "1224720",
    "end": "1232159"
  },
  {
    "text": "programmable researchers figured out how to use you know early NVIDIA GPUs to do",
    "start": "1232159",
    "end": "1237200"
  },
  {
    "text": "fast matrix multiplies Um this is one of the early papers on you know doing fast matrix multiplies with graphics hardware",
    "start": "1237200",
    "end": "1244000"
  },
  {
    "text": "Um and it shows you know how you can you know hack kind of things like the texture buffer and so on uh to get it to do matrix multiplies right And so you",
    "start": "1244000",
    "end": "1251039"
  },
  {
    "text": "know even without specific support for mapm you know researchers figured out how to do it but I think now you know",
    "start": "1251039",
    "end": "1258640"
  },
  {
    "text": "especially in this day and age nvidia and others have realized matrix multiplies are special like if you're doing deep learning right most of your",
    "start": "1258640",
    "end": "1265280"
  },
  {
    "text": "workload is matrix multiplies and so matrix multiplies are in some sense blessed operations so this is um a chart",
    "start": "1265280",
    "end": "1272080"
  },
  {
    "text": "showing um the number of teraflops per second by different generations of Nvidia GPUs and the orange line is your",
    "start": "1272080",
    "end": "1278960"
  },
  {
    "text": "map mo flops right Like with your performance you can get if you're doing matt moles The blue line is your non-matmo flops right And you see kind",
    "start": "1278960",
    "end": "1285840"
  },
  {
    "text": "of this big big gap at v 100's when um they started putting in sort of tensor cores that were specialized hardware to",
    "start": "1285840",
    "end": "1293600"
  },
  {
    "text": "do matrix multiplies And you see this gigantic gap uh in the matrix multiply performance relative to the non-mappable",
    "start": "1293600",
    "end": "1300080"
  },
  {
    "text": "performance right Um and so if you're going to design any sort of a neural architecture I was saying this you know",
    "start": "1300080",
    "end": "1305679"
  },
  {
    "text": "in the architecture part as well you have to have most of your workload be matrix multiplies because that's the",
    "start": "1305679",
    "end": "1311520"
  },
  {
    "text": "thing that's you know orders of magnitude faster than any other operation that you're going to be able to do uh on a GPU right So if you make",
    "start": "1311520",
    "end": "1317919"
  },
  {
    "text": "like a non-natmo based neural network you're going to be in a big big trouble",
    "start": "1317919",
    "end": "1323440"
  },
  {
    "text": "And then kind of the the last thing that I want you to kind of understand as just general facts Um you know matt moles is",
    "start": "1323440",
    "end": "1329840"
  },
  {
    "text": "fast is one thing but the other thing that's important to remember is kind of the relative scaling of the different",
    "start": "1329840",
    "end": "1335679"
  },
  {
    "text": "components of the GPU So this is a very nice chart that shows you know how quickly different components of the uh",
    "start": "1335679",
    "end": "1343440"
  },
  {
    "text": "GPU or different components of the let's call it like LM training stack are scaling So the blue line is the",
    "start": "1343440",
    "end": "1351080"
  },
  {
    "text": "connectivity from the GPU to the host right Like the the server that it's attached to right So you you can use",
    "start": "1351080",
    "end": "1357559"
  },
  {
    "text": "PCIe you can use NVLink you can use all these these fancy interconnects They are growing but they're growing somewhat",
    "start": "1357559",
    "end": "1363280"
  },
  {
    "text": "slowly right So so this chart is like normalized scaling you know bandwidth relative to to when you know the the",
    "start": "1363280",
    "end": "1369600"
  },
  {
    "text": "first generation um of interconnects The green line this is the uh global memory",
    "start": "1369600",
    "end": "1375760"
  },
  {
    "text": "speed right So you go from GDDR to HBM2E and that's much much faster right this is log scale it's 100x faster um but",
    "start": "1375760",
    "end": "1382559"
  },
  {
    "text": "this is still kind of slow scaling right and the gray line here right this is compute scaling this is the number of",
    "start": "1382559",
    "end": "1388799"
  },
  {
    "text": "floatingoint operations if you're you know considering the map flops this is this is how fast the compute has been",
    "start": "1388799",
    "end": "1394720"
  },
  {
    "text": "scaling and this is astoundingly fast it's like one to 100,000 times faster um",
    "start": "1394720",
    "end": "1400640"
  },
  {
    "text": "and so kind of in the early days of the scaling maybe your problems were flops based right like you you just didn't",
    "start": "1400640",
    "end": "1406720"
  },
  {
    "text": "have enough flops to do your matrix multiplications But now you know all the way to the right with the H100's you",
    "start": "1406720",
    "end": "1412080"
  },
  {
    "text": "know these are astoundingly fast GPUs your bottlenecks are probably going to end up being memory right Because the",
    "start": "1412080",
    "end": "1417919"
  },
  {
    "text": "memory is not growing as fast right And as we go into the future you know this is not really going to change DRAM is",
    "start": "1417919",
    "end": "1423200"
  },
  {
    "text": "very hard to scale You're going to keep getting this bigger and bigger gap right So if you're ever designing you know hardware efficient algorithms you're",
    "start": "1423200",
    "end": "1429280"
  },
  {
    "text": "going to have to think more and more about memory right And so we're going to keep a lookout on that I'm going to keep emphasizing this It's one of the",
    "start": "1429280",
    "end": "1435280"
  },
  {
    "text": "important themes um in GPUs Okay so you know I've been kind of throwing lots of",
    "start": "1435280",
    "end": "1441280"
  },
  {
    "text": "GPU facts at you um especially if you haven't you know seen this recently and maybe kind of new So just to recap right",
    "start": "1441280",
    "end": "1448640"
  },
  {
    "text": "GPUs are these massively parallel processing systems They have same instructions applied across many",
    "start": "1448640",
    "end": "1454720"
  },
  {
    "text": "different threads and they have these things called SM which are kind of like cores that you know there's many many of",
    "start": "1454720",
    "end": "1459840"
  },
  {
    "text": "them in the GPUs um compute and and matrix multiplies have scaled really fast and they have scaled faster than",
    "start": "1459840",
    "end": "1466080"
  },
  {
    "text": "memory and that is an important part of the characteristics that you think about about GPUs but there is some fast memory",
    "start": "1466080",
    "end": "1472960"
  },
  {
    "text": "right it's not like everything is slow so there's nothing we can do there's the memory hierarchy right so some kinds of",
    "start": "1472960",
    "end": "1478080"
  },
  {
    "text": "memory are very very fast other kinds of memories are slow and so if we exploit this hierarchy maybe we can get things",
    "start": "1478080",
    "end": "1484000"
  },
  {
    "text": "that are really really fast right so that's kind of things to remember about the GPU and if you remember these facts",
    "start": "1484000",
    "end": "1489919"
  },
  {
    "text": "you know you're going to be able think pretty cleanly about the performance components that I'm going to talk about next Um any questions before I I move on",
    "start": "1489919",
    "end": "1496640"
  },
  {
    "text": "to the next part Okay",
    "start": "1496640",
    "end": "1502559"
  },
  {
    "text": "cool So now you all are GPU experts and what we would like to do is we would",
    "start": "1502760",
    "end": "1507919"
  },
  {
    "text": "like to make machine learning workloads go very fast on a GPU Um and so I'm",
    "start": "1507919",
    "end": "1513520"
  },
  {
    "text": "going to start with this chart and one of our goals will be to understand what this chart exactly is I think it'll be a",
    "start": "1513520",
    "end": "1518880"
  },
  {
    "text": "good puzzle to get us motivated Um and so here what we are doing is we are",
    "start": "1518880",
    "end": "1524000"
  },
  {
    "text": "multiplying square matrices together right So the x-axis is the size of my square matrix multiplies And you know",
    "start": "1524000",
    "end": "1531200"
  },
  {
    "text": "the y-axis here this is the number of operations per second that I'm doing So you can kind of think of this as",
    "start": "1531200",
    "end": "1537039"
  },
  {
    "text": "hardware utilization on the y ais right Um and so as I get bigger and bigger matrices I'm going to get better and",
    "start": "1537039",
    "end": "1543520"
  },
  {
    "text": "better hardware utilization because you know I I have more work to do So I don't you know that overwhelms the overhead of",
    "start": "1543520",
    "end": "1550320"
  },
  {
    "text": "sort of you know launching jobs and things like this Um but there's all these weird things that are happening right You see one two three different",
    "start": "1550320",
    "end": "1557919"
  },
  {
    "text": "four different lines right And each of these lines are kind of wavy in a way that's kind of you know looks very",
    "start": "1557919",
    "end": "1564640"
  },
  {
    "text": "unpredictable right Um and so we would like to kind of understand what exactly is going on uh with these lines And by",
    "start": "1564640",
    "end": "1570640"
  },
  {
    "text": "the end of this section um my promise is that you will kind of understand exactly each one of these phenomenon You'll be",
    "start": "1570640",
    "end": "1576080"
  },
  {
    "text": "able to say \"Yeah that plot looks totally normal That is a natural thing for a GPU to do.\"",
    "start": "1576080",
    "end": "1582000"
  },
  {
    "text": "Okay so the very first part right is if you look at that plot you will notice that it looks a little bit like this",
    "start": "1582000",
    "end": "1588480"
  },
  {
    "text": "right And if you've taken a systems hardware course you know you should you should remember this as kind of the roof line model Um the roofline model",
    "start": "1588480",
    "end": "1595360"
  },
  {
    "text": "basically says if we're looking at you know throughput or utilization um you know what we're going to find is you",
    "start": "1595360",
    "end": "1601760"
  },
  {
    "text": "know there's two regimes There's going to be a regime that is sort of memory limited right That is on the left side",
    "start": "1601760",
    "end": "1607679"
  },
  {
    "text": "of this curve on the green over here And then there's a part that is throughput limited on the right side In some sense",
    "start": "1607679",
    "end": "1613279"
  },
  {
    "text": "you can kind of think of it as on the right side we have we are fully utilizing our compute units All the",
    "start": "1613279",
    "end": "1618640"
  },
  {
    "text": "matrix multiply units are multiplying all the time Um and on the diagonal here we just have some sort of memory",
    "start": "1618640",
    "end": "1625120"
  },
  {
    "text": "bottleneck and so our ability to to do computation is limited by kind of the",
    "start": "1625120",
    "end": "1630960"
  },
  {
    "text": "amount of sort of intensity that we have the amount of flops per bite that we have Right So we want to avoid being in",
    "start": "1630960",
    "end": "1636720"
  },
  {
    "text": "this left side region where we're memory bound and we would like to be on this right side where we're getting in some sense full utilization of all of our",
    "start": "1636720",
    "end": "1643440"
  },
  {
    "text": "compute units Right So that's in some sense the goal and hopefully this roofine model looks something like this",
    "start": "1643440",
    "end": "1649679"
  },
  {
    "text": "Right Like we've got sort of this diagonal part and then we've got this flat part all the way at the top here So that's one part of the",
    "start": "1649679",
    "end": "1657000"
  },
  {
    "text": "mystery Um and so this is this turns out to be kind of complex right Um the",
    "start": "1657000",
    "end": "1663440"
  },
  {
    "text": "simple way to say this is let's make sure that we're not accessing memory unnecessarily right We have as few",
    "start": "1663440",
    "end": "1669520"
  },
  {
    "text": "memory accesses to slow global memory as possible But it turns out that in order to do that we need a large array of",
    "start": "1669520",
    "end": "1676240"
  },
  {
    "text": "tricks Um there's a lot of different things that you could do that would mess you up that would make you very slow And",
    "start": "1676240",
    "end": "1682799"
  },
  {
    "text": "the first one's not a memory bottleneck I'll just mention it Um it doesn't come up too often we'll get it out of the way and then we'll talk about the remaining",
    "start": "1682799",
    "end": "1689039"
  },
  {
    "text": "uh five items that in some sense are really core to thinking about GPU",
    "start": "1689039",
    "end": "1695520"
  },
  {
    "text": "performance Okay so the first thing that I want to talk about is conditionals Um",
    "start": "1695880",
    "end": "1701760"
  },
  {
    "text": "so as I said before GPUs their execution model is something called SIM key right single instruction multi-thread Um and",
    "start": "1701760",
    "end": "1708640"
  },
  {
    "text": "so every thread in a warp is going to execute the same instruction um and it's going to do so on different data And so",
    "start": "1708640",
    "end": "1715600"
  },
  {
    "text": "what happens if I write uh a piece of code that looks like this I have an if statement and if you know the thread",
    "start": "1715600",
    "end": "1721120"
  },
  {
    "text": "index is less than four do something If the thread index is greater than or equal to four then do something else Right I have this very simple",
    "start": "1721120",
    "end": "1727120"
  },
  {
    "text": "conditional uh model If I run this on the GPU um what's going to happen is",
    "start": "1727120",
    "end": "1732480"
  },
  {
    "text": "that I'm going to run the a uh instruction on four of my threads I will",
    "start": "1732480",
    "end": "1738480"
  },
  {
    "text": "actually pause my other four threads which are supposed to be executing the else part And then these other four",
    "start": "1738480",
    "end": "1744159"
  },
  {
    "text": "threads will come live and they will execute X and these my original four threads will will go to sleep and I will",
    "start": "1744159",
    "end": "1749600"
  },
  {
    "text": "just alternate executing each of these instructions Why is that I can't in I can't execute A and X at the same time",
    "start": "1749600",
    "end": "1756720"
  },
  {
    "text": "on these different threads Right As I said again um every thread has to execute the same instruction So",
    "start": "1756720",
    "end": "1761760"
  },
  {
    "text": "conditional statements within a single warp um can be really really damaging because they will force you to pause any",
    "start": "1761760",
    "end": "1768399"
  },
  {
    "text": "of the threads that are not doing exactly the the main sort of control flow uh",
    "start": "1768399",
    "end": "1775480"
  },
  {
    "text": "execution Okay so that was the the only non-memory thing that I wanted to mention Um and it should be kind of",
    "start": "1775480",
    "end": "1780799"
  },
  {
    "text": "obvious that you should probably not be putting conditionals um into sort of your uh massively parallel compute unit",
    "start": "1780799",
    "end": "1787200"
  },
  {
    "text": "But once we've gotten that out of the way sort of the other tricks that we need to consider are all kind of memory",
    "start": "1787200",
    "end": "1792320"
  },
  {
    "text": "based Um the first thing I want to sort of mention is lower precision And this is a big trick This is an important",
    "start": "1792320",
    "end": "1798720"
  },
  {
    "text": "trick You should do it all the time Um there's kind of a going back to this plot um of of Billy Um there's a slight",
    "start": "1798720",
    "end": "1805520"
  },
  {
    "text": "of hand here Um this looks really good because the the numbers are going up and up and up But if you look at you know",
    "start": "1805520",
    "end": "1812080"
  },
  {
    "text": "what's driving GPU progress over all these years um you actually kind of see",
    "start": "1812080",
    "end": "1817840"
  },
  {
    "text": "that it's number representations You go from FP32 to FP16 to intate to to so on",
    "start": "1817840",
    "end": "1823760"
  },
  {
    "text": "uh you get many orders of magnitude gains from just having lower and lower precision in your GPU operations Um and",
    "start": "1823760",
    "end": "1830960"
  },
  {
    "text": "let me let me sort of clarify why that's so important right If you have fewer bits in all the things that you're",
    "start": "1830960",
    "end": "1836320"
  },
  {
    "text": "computing and your weights and so on you have much fewer bits to move right So even if you're accessing these bits from",
    "start": "1836320",
    "end": "1841440"
  },
  {
    "text": "global memory um they become much much less of a concern So let's just give a",
    "start": "1841440",
    "end": "1846880"
  },
  {
    "text": "simple example and let's just think about kind of arithmetic intensity of a simple element-wise operation Right So",
    "start": "1846880",
    "end": "1852399"
  },
  {
    "text": "I'm going to do it in values So that's x equals max zero and x and I'm going to do that on a vector of size n Let's say",
    "start": "1852399",
    "end": "1858640"
  },
  {
    "text": "naively I'm going to do this on float 32 Right So so how many memory accesses do I have I have to read my x I have to",
    "start": "1858640",
    "end": "1865760"
  },
  {
    "text": "write the result of if x less than zero Um and that's all in float 32 So that's kind of eight bytes right And how many",
    "start": "1865760",
    "end": "1872960"
  },
  {
    "text": "operations do I do Well I have to do x less than zero So that's one comparison operation And I do one flop right So so",
    "start": "1872960",
    "end": "1878960"
  },
  {
    "text": "I do you know eight bytes per single floatingoint operation If I do this in float 16 now well you know I haven't",
    "start": "1878960",
    "end": "1885600"
  },
  {
    "text": "changed the flops intensity here but I've have the memory access And so now I have four bytes per flop right In some",
    "start": "1885600",
    "end": "1891600"
  },
  {
    "text": "sense I've like gotten double the memory bandwidth for free assuming that I can get away with flop 16",
    "start": "1891600",
    "end": "1898240"
  },
  {
    "text": "And this is a key part of of how a lot of things are designed Part of the assignment is going to be you're going",
    "start": "1898240",
    "end": "1903360"
  },
  {
    "text": "to you know try and and play with uh various like mixed precision or low precision uh training and and other",
    "start": "1903360",
    "end": "1910399"
  },
  {
    "text": "kinds of things Um and a key part here is that not all the parts of your network and your training algorithm",
    "start": "1910399",
    "end": "1916480"
  },
  {
    "text": "should be put into low precision right So let me give you an example of matrix multiplies So in matrix multiplies that",
    "start": "1916480",
    "end": "1923440"
  },
  {
    "text": "are mixed precision what you would do is you would have you know your inputs be 16 bit So these are low precision Um and",
    "start": "1923440",
    "end": "1930240"
  },
  {
    "text": "then you're going to do your your multiplication in full 32bit right and that's useful because the intermediate",
    "start": "1930240",
    "end": "1936000"
  },
  {
    "text": "computations as you're like accumulating partial sums you would like that to be in high precision Um and so you're",
    "start": "1936000",
    "end": "1941760"
  },
  {
    "text": "accumulating this with uh FP32 accumulator and then you know your tensor core uh will return a FP32 result",
    "start": "1941760",
    "end": "1950240"
  },
  {
    "text": "um which you can you know downcast if you would like um back into into 16 bit right and so we have our inputs in 16",
    "start": "1950240",
    "end": "1956799"
  },
  {
    "text": "bit but things like the accumulation um we might want to do in 32 right so",
    "start": "1956799",
    "end": "1962240"
  },
  {
    "text": "there's lots of different things there's operations that can use 16- bit storage there's operations that might need more",
    "start": "1962240",
    "end": "1967600"
  },
  {
    "text": "precision so you want to keep it in like either FP32 or FP16 Um think you might",
    "start": "1967600",
    "end": "1972880"
  },
  {
    "text": "want to have operations that need more range like X functions If you don't have sort of the dynamic range they might",
    "start": "1972880",
    "end": "1978159"
  },
  {
    "text": "blow up or zero out And so you might want to put those in BF-16 There's a lot of sort of careful engineering um that",
    "start": "1978159",
    "end": "1984880"
  },
  {
    "text": "has to happen in order to make sure that you know these these models uh are actually stable when they're being",
    "start": "1984880",
    "end": "1990480"
  },
  {
    "text": "trained with lower precision But if you can do it that's really great because you've basically doubled the throughput",
    "start": "1990480",
    "end": "1995600"
  },
  {
    "text": "of your bottleneck going from 32 to 16 bit right if your if your memory is your",
    "start": "1995600",
    "end": "2001600"
  },
  {
    "text": "bottleneck Okay the other one and I think this is kind of what a lot of people think of when they say like I'm",
    "start": "2003159",
    "end": "2009600"
  },
  {
    "text": "going to write a CUDA kernel or something Um operator fusion is kind of both very intuitive and both a like a",
    "start": "2009600",
    "end": "2016320"
  },
  {
    "text": "fun natural one to think about So one memory or sorry one mental model of how a GPU works and how memory works is is",
    "start": "2016320",
    "end": "2023679"
  },
  {
    "text": "this kind of fun diagram of a factory um from Horus Heath right So imagine you have a factory and your factory is your",
    "start": "2023679",
    "end": "2029919"
  },
  {
    "text": "compute part right And so you know it takes in little uh box widgets and then",
    "start": "2029919",
    "end": "2034960"
  },
  {
    "text": "outputs little triangle widgets Um and if you grow your compute but your belt conveyor you know that takes memory to",
    "start": "2034960",
    "end": "2041760"
  },
  {
    "text": "compute is you know finite bandwidth you know you're not going to be able to use your second factory right Like you're",
    "start": "2041760",
    "end": "2046880"
  },
  {
    "text": "still capped by the speed at which you can transfer things um from memory to compute And so you've got this this",
    "start": "2046880",
    "end": "2052839"
  },
  {
    "text": "bottleneck Now of course you already knew that right I've been sort of hammering in the the memory bottleneck thing But I think one insidious way in",
    "start": "2052839",
    "end": "2060079"
  },
  {
    "text": "which you can uh incur a ton of overhead without really realizing it is kind of this left-hand side computation pattern",
    "start": "2060079",
    "end": "2066638"
  },
  {
    "text": "right So you know imagine the left side of this this plot is where the memory is The right side is your compute unit Um",
    "start": "2066639",
    "end": "2073440"
  },
  {
    "text": "and so to do computation I start with a square and I move my squares from my memory to my compute I do some operation",
    "start": "2073440",
    "end": "2079200"
  },
  {
    "text": "I turn them into triangles Right Now I ship my triangles back to memory And then you know okay I realize I need a",
    "start": "2079200",
    "end": "2084720"
  },
  {
    "text": "triangles again So I ship them back into the compute unit Now the triangles become circles and then so on and so forth right I send my compute sort of",
    "start": "2084720",
    "end": "2091118"
  },
  {
    "text": "back and forth and back and forth back to memory And you might call this kind of a very naive approach And if you were",
    "start": "2091119",
    "end": "2096800"
  },
  {
    "text": "just doing operations naively on the GPU and just shipping the results straight back to global memory this is what you'd",
    "start": "2096800",
    "end": "2102720"
  },
  {
    "text": "end up with right And if you count the number of times a piece of data went back and forth this is this is pretty terrible You've incurred tons of memory",
    "start": "2102720",
    "end": "2108800"
  },
  {
    "text": "overhead Now uh you should be able to realize that if you look at the right side well this compute well there's no",
    "start": "2108800",
    "end": "2115200"
  },
  {
    "text": "dependency so I should be able to go square to triangle to circle to to rectangle and ship the rectangle back",
    "start": "2115200",
    "end": "2121280"
  },
  {
    "text": "right I can just keep everything in the compute unit the whole time right and that's the right hand side diagram and",
    "start": "2121280",
    "end": "2126400"
  },
  {
    "text": "this is the mental model of a fused kernel right you have a bunch of operations that are going to happen on a piece of data in sequence instead of",
    "start": "2126400",
    "end": "2133520"
  },
  {
    "text": "writing it back into storage what I'm going to do is I'm going to do all the computation as much as I can in one place and then only when I have to ship",
    "start": "2133520",
    "end": "2140720"
  },
  {
    "text": "it back to memory Right So that's this idea of of kernel",
    "start": "2140720",
    "end": "2145599"
  },
  {
    "text": "fusion Okay Um there's some very simple examples of how if you write some naive",
    "start": "2146599",
    "end": "2152720"
  },
  {
    "text": "code um you might you know get sort of a naive set of launches So here's an example Um I wrote a little let's say",
    "start": "2152720",
    "end": "2159359"
  },
  {
    "text": "neural network module um you know let's say let's say I write a neural network module that takes in uh s x and it",
    "start": "2159359",
    "end": "2165599"
  },
  {
    "text": "produces sin^ squar x and cosine^ squ x right simple code now if I run this you",
    "start": "2165599",
    "end": "2171520"
  },
  {
    "text": "know the computation graph in pietorch is going to look something like this and it's going to you know launch a whole",
    "start": "2171520",
    "end": "2176880"
  },
  {
    "text": "bunch of cuda kernels it's going to launch take in the x and it'll it'll launch a cuda kernel to compute sin x",
    "start": "2176880",
    "end": "2182880"
  },
  {
    "text": "it'll launch one to compute cosine x then sin square of x and cosine square of x and sin^ square x plus cosine",
    "start": "2182880",
    "end": "2188160"
  },
  {
    "text": "square of x right so there's a bunch of back and forth that has to happen in order to do this computation It's exactly the lefth hand side figure uh",
    "start": "2188160",
    "end": "2194640"
  },
  {
    "text": "that I showed you before Um but if you were a little smarter right and you either wrote your",
    "start": "2194640",
    "end": "2201440"
  },
  {
    "text": "own CUDA kernel or you use something like torch compile um well you can easily realize that those five",
    "start": "2201440",
    "end": "2207200"
  },
  {
    "text": "operations um don't really depend on very much like they they use only a little bit of memory And so you can fuse",
    "start": "2207200",
    "end": "2213760"
  },
  {
    "text": "them into a single operation that does everything on GPU on a single thread without sending things back to global",
    "start": "2213760",
    "end": "2219359"
  },
  {
    "text": "memory Right So um really easy fusion operations like this can be done automatically by compilers I just",
    "start": "2219359",
    "end": "2225440"
  },
  {
    "text": "mentioned torch compile Um if you aren't already doing this you know you should you should consider strongly thinking",
    "start": "2225440",
    "end": "2230640"
  },
  {
    "text": "about using torch compile everywhere Um we'll show you in the assignment um torch compile as well It's it's pretty",
    "start": "2230640",
    "end": "2236960"
  },
  {
    "text": "uh nice Okay Um so I've gone through uh",
    "start": "2236960",
    "end": "2242800"
  },
  {
    "text": "precision and fusion If anyone has questions let me know uh before I move on to to recomputation um and other",
    "start": "2242800",
    "end": "2249599"
  },
  {
    "text": "kinds of tricks that we can do on the GPU Okay",
    "start": "2249599",
    "end": "2254760"
  },
  {
    "text": "good So another thing that we can do is called recomputation Um and",
    "start": "2254760",
    "end": "2260079"
  },
  {
    "text": "recomputation is this idea of sort of spending more compute to uh avoid having",
    "start": "2260079",
    "end": "2265520"
  },
  {
    "text": "to do memory access right Um so remember back your you know original back",
    "start": "2265520",
    "end": "2270960"
  },
  {
    "text": "propagation lecture This one's actually from CS221 Um what do we do Well we take our our inputs at the very bottom These",
    "start": "2270960",
    "end": "2277119"
  },
  {
    "text": "are the yellow ones Um and then we propagate activations upwards Those are also the yellow values on the on the",
    "start": "2277119",
    "end": "2282720"
  },
  {
    "text": "tree Um and then we compute the Jacobians backwards Those are the green values on the edges And then to compute",
    "start": "2282720",
    "end": "2288480"
  },
  {
    "text": "my gradients I'm going to propagate you multiply sort of the Jacobian and the activations I'm going to propagate the",
    "start": "2288480",
    "end": "2294640"
  },
  {
    "text": "the gradients backward right Um well if you think about it those yellow values",
    "start": "2294640",
    "end": "2300000"
  },
  {
    "text": "after the forward pass have to be stored right And then they're stored and then they have to be taken from global memory",
    "start": "2300000",
    "end": "2305359"
  },
  {
    "text": "where I stored them and put them into the compute units right Mechanically that's how it has to happen Um but that",
    "start": "2305359",
    "end": "2311119"
  },
  {
    "text": "might actually be a ton of sort of memory inputs and outputs happening Instead you might actually be",
    "start": "2311119",
    "end": "2318079"
  },
  {
    "text": "able to avoid this So let me give you an example of how recomputation can speed things up Um here's another sort of",
    "start": "2318079",
    "end": "2324800"
  },
  {
    "text": "silly uh function that I might write I'm just going to stack three sigmoids on top of each other right You can look at",
    "start": "2324800",
    "end": "2330720"
  },
  {
    "text": "the left That's the forward graph Um that should be exactly you know your mental model of three sigmoids on top of",
    "start": "2330720",
    "end": "2336160"
  },
  {
    "text": "each other Now you know the the computation graph for this I'm going to compute the sigmoids and I'm going to",
    "start": "2336160",
    "end": "2341359"
  },
  {
    "text": "store S1 and S2 which are the activations of the sigmoids and I have my outputs Um and then you know that's",
    "start": "2341359",
    "end": "2347520"
  },
  {
    "text": "my sort of forward pass Now the backward pass in this is kind of terrible when I do my backward graph I need to go and",
    "start": "2347520",
    "end": "2354320"
  },
  {
    "text": "take S1 and S2 um and I need to take you know the the gradients coming sort of backwards into this out box um and then",
    "start": "2354320",
    "end": "2361440"
  },
  {
    "text": "push it into this you know backwards computation and I'll get the gradient of X right so I need to have three memory",
    "start": "2361440",
    "end": "2368000"
  },
  {
    "text": "reads one memory right in order to compute the backwards pass and then for the forward pass I need to do one memory",
    "start": "2368000",
    "end": "2373119"
  },
  {
    "text": "read of X and I need to do three memory rights for S1 S2 and out right so hopefully that's clear this is you know",
    "start": "2373119",
    "end": "2379520"
  },
  {
    "text": "a decent amount of of uh memory reads and writes have to do eight of them and I have very low arithmetic intensity",
    "start": "2379520",
    "end": "2384880"
  },
  {
    "text": "because I have no matrix multiplies um at all So the idea of recomputation is to",
    "start": "2384880",
    "end": "2390960"
  },
  {
    "text": "say I don't want to store those activations at all Right Like I'm not going to put them into memory I'm just",
    "start": "2390960",
    "end": "2396240"
  },
  {
    "text": "going to recmp compute them on the fly in my backward pass Right So now in my new forward pass I don't store S1 and S2",
    "start": "2396240",
    "end": "2403040"
  },
  {
    "text": "I take X as input I compute my sigmoids and I get my output Right So now that's one memory read for X one memory right",
    "start": "2403040",
    "end": "2409680"
  },
  {
    "text": "for out Right Um now in my backward pass right I don't have activations anymore",
    "start": "2409680",
    "end": "2414800"
  },
  {
    "text": "So what I'm going to do is I'm going to get both D out which is you know the backward signal coming in from above Um",
    "start": "2414800",
    "end": "2420800"
  },
  {
    "text": "and then X which is my input right So I'm going to take two of those which is two memory reads Um and then sort of on",
    "start": "2420800",
    "end": "2426800"
  },
  {
    "text": "the fly in my SM in my local memory I'm going to compute each of these sigmoids and I'm going to put them into the",
    "start": "2426800",
    "end": "2432720"
  },
  {
    "text": "backward graph right I'm going to recmp compute S1 S2 uh and out on the fly inside sort of my local memory Um and",
    "start": "2432720",
    "end": "2439119"
  },
  {
    "text": "because I do that there's no global memory reads happening here Um and then I have one memory right which is dx",
    "start": "2439119",
    "end": "2445359"
  },
  {
    "text": "Right So now if you compare the two um I have 5/8 of the memory access for the",
    "start": "2445359",
    "end": "2450400"
  },
  {
    "text": "exact same computation Right The price that we paid is that I'm going to have to recomputee these three sigmoids But",
    "start": "2450400",
    "end": "2456160"
  },
  {
    "text": "if you were running sort of idle anyway because you were memory capped this is a a great trade-off right Like you would",
    "start": "2456160",
    "end": "2462000"
  },
  {
    "text": "be very happy with this because now you've traded compute which you have too much of for memory bandwidth which you",
    "start": "2462000",
    "end": "2467280"
  },
  {
    "text": "had too little of Right Right So this is one great way of trading um one thing you need for another thing uh that you",
    "start": "2467280",
    "end": "2474920"
  },
  {
    "text": "have And of course this is different uh it's the same trick as um sort of",
    "start": "2474920",
    "end": "2480160"
  },
  {
    "text": "gradient checkpointing and recmp computing activations for memory savings Um but this is being done for different",
    "start": "2480160",
    "end": "2485440"
  },
  {
    "text": "reasons This is for uh sort of execution speed not just because you're running out of memory Right So it's it's the",
    "start": "2485440",
    "end": "2490800"
  },
  {
    "text": "same technique but for different goals Okay And then this one I think um",
    "start": "2490800",
    "end": "2498319"
  },
  {
    "text": "is actually kind of a really interesting one and and not one that I knew until I started sort of really looking into how",
    "start": "2498319",
    "end": "2503359"
  },
  {
    "text": "the hardware model of a GPU uh and DRAM works Um so the slow memory the global memory",
    "start": "2503359",
    "end": "2511280"
  },
  {
    "text": "called DRAM in a GPU um that's actually very very slow And in order to to make",
    "start": "2511280",
    "end": "2517280"
  },
  {
    "text": "it faster there's certain optimizations that are being done at the hardware level And one of the optimizations",
    "start": "2517280",
    "end": "2522880"
  },
  {
    "text": "that's done at a hardware level for for DRAM is that when you go and read a piece of memory you don't actually get",
    "start": "2522880",
    "end": "2528880"
  },
  {
    "text": "just that value back You actually get a whole chunk of the memory back Um and this is called burst mode So um let's",
    "start": "2528880",
    "end": "2535839"
  },
  {
    "text": "say I went on and uh tried to read the very first value of this big memory block right Instead of just the memory",
    "start": "2535839",
    "end": "2542240"
  },
  {
    "text": "giving me back zero it would actually give me back 012 3 right It would give me back four values at once It'll be",
    "start": "2542240",
    "end": "2547760"
  },
  {
    "text": "like here you go You know I'm sure you'll need the one two and three too in the future Um and so each address space",
    "start": "2547760",
    "end": "2553440"
  },
  {
    "text": "is cut up into what's called burst sections And then you're given the entire burst section rather than just",
    "start": "2553440",
    "end": "2558640"
  },
  {
    "text": "what you looked for And this might seem very mystifying like why would the memory give you three",
    "start": "2558640",
    "end": "2564640"
  },
  {
    "text": "extra you know bytes for free uh when you're just asking for one Um there's sort of like a very interesting hardware",
    "start": "2564640",
    "end": "2571200"
  },
  {
    "text": "reason which is that when you're addressing into the memory you know in order to send the signal out from the memory that those bytes have to be moved",
    "start": "2571200",
    "end": "2578240"
  },
  {
    "text": "to an amplifier That's the slow step And once you've done that you can get many many bytes for free And so that's why",
    "start": "2578240",
    "end": "2584240"
  },
  {
    "text": "sort of this burst section thing exists it's kind of masking this more expensive step of actually moving where the the",
    "start": "2584240",
    "end": "2590000"
  },
  {
    "text": "data is stored to this amplifier but kind of regardless um this",
    "start": "2590000",
    "end": "2596240"
  },
  {
    "text": "kind of means that we might be able to significantly accelerate sort of our memory access if the pattern of memory",
    "start": "2596240",
    "end": "2602000"
  },
  {
    "text": "access is good right so if I want to read um this entire you know block over",
    "start": "2602000",
    "end": "2607119"
  },
  {
    "text": "here if I access it in random order right then I'm going to have to you know basically query uh a number of times",
    "start": "2607119",
    "end": "2614079"
  },
  {
    "text": "equal roughly to the length of my query Right But if I sort of go and I check",
    "start": "2614079",
    "end": "2620000"
  },
  {
    "text": "the very first value then I'm going to get all this entire burst section at once And then if I go and check number",
    "start": "2620000",
    "end": "2625359"
  },
  {
    "text": "four I'll get this burst section the second burst section at once And so I can you know uh basically get four times",
    "start": "2625359",
    "end": "2631839"
  },
  {
    "text": "the throughput if I'm really clever about my memory accesses and only access just the bits I need from each uh burst",
    "start": "2631839",
    "end": "2639000"
  },
  {
    "text": "section So this is called memory coallesing So if all the threads uh in",
    "start": "2639000",
    "end": "2644640"
  },
  {
    "text": "in a warp fall within the same burst um then basically the sort of smart hardware and programming model will",
    "start": "2644640",
    "end": "2651040"
  },
  {
    "text": "basically group those queries instead of querying 0 1 2 3 it will group them and say just give me zero and then I will be",
    "start": "2651040",
    "end": "2657359"
  },
  {
    "text": "able to read out all the 0 1 2 3 at once from this kind of burst mode uh DRAM",
    "start": "2657359",
    "end": "2662480"
  },
  {
    "text": "right so remember that you know a warp is 32 sort of numbered threads and so memory accesses from a warp happen",
    "start": "2662480",
    "end": "2668720"
  },
  {
    "text": "together and so when these warps are reading in to these kind of burst sections there's optimizations that can",
    "start": "2668720",
    "end": "2674079"
  },
  {
    "text": "be done so that you're getting all four bytes at once rather than getting one of them at a time individually and so that",
    "start": "2674079",
    "end": "2679599"
  },
  {
    "text": "will 4x uh the throughput that you have on your memory Right So these are kind",
    "start": "2679599",
    "end": "2684640"
  },
  {
    "text": "of very simple things but they're actually very important Like imagine I'm going to do matrix multiplications right",
    "start": "2684640",
    "end": "2690400"
  },
  {
    "text": "This is a core thing that you're going to have to do a ton if you were to sort of implement let's say uh neural network",
    "start": "2690400",
    "end": "2696000"
  },
  {
    "text": "really from scratch in CUDA Um in this case imagine I'm going to read my my uh",
    "start": "2696000",
    "end": "2701680"
  },
  {
    "text": "matrices in one of two ways I can read it by traversing the rows right So each thread is going to traverse the row Or I",
    "start": "2701680",
    "end": "2708640"
  },
  {
    "text": "can sort of read it in sort of column order So each thread is going to go down a column right Um turns out that this",
    "start": "2708640",
    "end": "2715760"
  },
  {
    "text": "left one where you're sort of going across different rows so each thread is accessing a different Oh sorry each",
    "start": "2715760",
    "end": "2721440"
  },
  {
    "text": "thread is going through columns This left model is going to be quite slow Um",
    "start": "2721440",
    "end": "2726960"
  },
  {
    "text": "because the memory reads are not going to be coalesed Um whereas if you're going to this right side where each of",
    "start": "2726960",
    "end": "2732800"
  },
  {
    "text": "the threads are going down so they're they're incrementing in rows then these memory reads will be coalesed Um and so",
    "start": "2732800",
    "end": "2739920"
  },
  {
    "text": "you know you can think about it for a moment why this is true Um when I first looked at this diagram I was like isn't it reversed It's actually not This is",
    "start": "2739920",
    "end": "2746079"
  },
  {
    "text": "this is the correct one Um and the way to think about this right um is let's",
    "start": "2746079",
    "end": "2751520"
  },
  {
    "text": "say uh on this right hand side diagram over here I'm going to have a thread that's trying to a series of threads",
    "start": "2751520",
    "end": "2757839"
  },
  {
    "text": "that's trying to access you know left to right So each thread is going to try to load you know the very first element and",
    "start": "2757839",
    "end": "2763839"
  },
  {
    "text": "then in the next time step I'm going to the load the element from the uh this column the second column and then the",
    "start": "2763839",
    "end": "2770400"
  },
  {
    "text": "third column and the fourth column and so on So if that happens what happens at time step one Right At time step one my",
    "start": "2770400",
    "end": "2777119"
  },
  {
    "text": "first thread loads this point and then the second thread loads this point and then this point and that point right So",
    "start": "2777119",
    "end": "2782240"
  },
  {
    "text": "those can't be coalesed at all They're reading different burst sections And so that means that I have to read this",
    "start": "2782240",
    "end": "2787440"
  },
  {
    "text": "entire chunk of memory in order to perform any sort of an operation Instead",
    "start": "2787440",
    "end": "2792880"
  },
  {
    "text": "if I was sort of going in the column direction all the threads will be reading within the single burst section",
    "start": "2792880",
    "end": "2798240"
  },
  {
    "text": "And then so only one memory read operation needs to be performed and you get all the memory at once Right This is",
    "start": "2798240",
    "end": "2803440"
  },
  {
    "text": "a very low-level optimization but this is very important Right If your memory traversal order is all wrong you you",
    "start": "2803440",
    "end": "2808960"
  },
  {
    "text": "will actually get much slower memory accesses than you really want",
    "start": "2808960",
    "end": "2814760"
  },
  {
    "text": "Okay So then uh that brings us to kind of the very last and kind of big one Um",
    "start": "2814760",
    "end": "2821040"
  },
  {
    "text": "and this is the idea of uh tiling Um and tiling is this idea that you would like",
    "start": "2821040",
    "end": "2826560"
  },
  {
    "text": "to group together memory accesses um in order to minimize the amount of global memory access that we have to do And so",
    "start": "2826560",
    "end": "2834319"
  },
  {
    "text": "to explain this one uh I'm going to try to go through this example of a matrix multiply And hopefully I'll be able to",
    "start": "2834319",
    "end": "2840480"
  },
  {
    "text": "sort of explain to you um why sort of a naive algorithm for doing matrix multiply is going to be very problematic",
    "start": "2840480",
    "end": "2847599"
  },
  {
    "text": "And then afterwards I'm going to give you a tiled version of the same idea And hopefully you'll be able to see why",
    "start": "2847599",
    "end": "2852960"
  },
  {
    "text": "that's going to reduce the number of global memory reads um that you have to do So let's start with this very simple",
    "start": "2852960",
    "end": "2860000"
  },
  {
    "text": "matrix multiply uh algorithm So you know I've got a matrix you know I got this M matrix on the left side I've got my N",
    "start": "2860000",
    "end": "2866960"
  },
  {
    "text": "matrix on the top Um and in order to compute you know the matrix matrix product right I'm going to have to",
    "start": "2866960",
    "end": "2873200"
  },
  {
    "text": "traverse over the rows of M and the columns of N and then take the inner product and store that into uh this P",
    "start": "2873200",
    "end": "2880160"
  },
  {
    "text": "matrix right the corresponding rows Um and I've written out here um each of the threads the thread 01 1 0 1",
    "start": "2880160",
    "end": "2887280"
  },
  {
    "text": "corresponding to where they're sort of storing their outputs and sort of the access order in which they access each",
    "start": "2887280",
    "end": "2893040"
  },
  {
    "text": "of the individual elements Now notice here that you know what's going to happen is that the memory access here is",
    "start": "2893040",
    "end": "2899839"
  },
  {
    "text": "not coalesed like the row um uh matrices here these are going to be accessed in a",
    "start": "2899839",
    "end": "2905280"
  },
  {
    "text": "non-co order and I have repeated memory accesses right so I've got m00 0 being",
    "start": "2905280",
    "end": "2911839"
  },
  {
    "text": "accessed in the first thread m 0 accessed here n0 n10 being accessed in",
    "start": "2911839",
    "end": "2917119"
  },
  {
    "text": "two different threads you know so these values are being kind of read um over and over from global memory into many",
    "start": "2917119",
    "end": "2923680"
  },
  {
    "text": "different threads And so this is going to be potentially very slow So there's a question of can",
    "start": "2923680",
    "end": "2930079"
  },
  {
    "text": "we avoid having too many global memory reads and writes What I would ideally",
    "start": "2930079",
    "end": "2935119"
  },
  {
    "text": "like to do right So let me explain kind of the the ideal outcome first and then I'll explain the algorithm The ideal",
    "start": "2935119",
    "end": "2940960"
  },
  {
    "text": "outcome is that I would like to spend one sort of you know chunk of time loading pieces from global memory to",
    "start": "2940960",
    "end": "2948319"
  },
  {
    "text": "shared memory where things are fast I want to do a ton of computation in shared memory and then I want to kind of",
    "start": "2948319",
    "end": "2953359"
  },
  {
    "text": "be done with that piece of uh data Right That's the ideal outcome I've minimized my global memory accesses So now how can",
    "start": "2953359",
    "end": "2960079"
  },
  {
    "text": "I do this um in this matrix multiply world So now what I'm going to do is I'm going to take my matrices both the M",
    "start": "2960079",
    "end": "2966400"
  },
  {
    "text": "matrix and the N matrix and I'm going to cut them up right into tiles So here I've cut this up into 2x2 tiles So I've",
    "start": "2966400",
    "end": "2972240"
  },
  {
    "text": "got a 2x2 M tile and a 2x2 N tile right So I've got basically uh smaller",
    "start": "2972240",
    "end": "2977599"
  },
  {
    "text": "submatrices within each of the matrix And now imagine that my shared memory is big enough to be able to fit these",
    "start": "2977599",
    "end": "2983040"
  },
  {
    "text": "submatrices right Uh within each of these SM So now this gives a very very simple uh algorithm with which we can do",
    "start": "2983040",
    "end": "2990680"
  },
  {
    "text": "computation So uh what I'm going to do is I'm going to first load you know let's say this m00 tile on the top left",
    "start": "2990680",
    "end": "2998000"
  },
  {
    "text": "over here and I'm going to also load my N00 tile um into shared memory here",
    "start": "2998000",
    "end": "3003760"
  },
  {
    "text": "Right Um so now I have these partial sums that I can compute I can take you",
    "start": "3003760",
    "end": "3008880"
  },
  {
    "text": "know the the row product of m00 z m01 with n z n 0 and I can increment that",
    "start": "3008880",
    "end": "3015359"
  },
  {
    "text": "into p 0 I can do the same with all the different submatrices that I can fill out over here Right now then once I'm",
    "start": "3015359",
    "end": "3022880"
  },
  {
    "text": "completely done sort of processing these two tiles then I can load a new tile over here And then I can repeat that",
    "start": "3022880",
    "end": "3029119"
  },
  {
    "text": "computation with my M tile and my N2.0 tile loaded into shared memory And then I can sort of increment my partial sums",
    "start": "3029119",
    "end": "3035520"
  },
  {
    "text": "in P Right So now I've really sort of consolidated and reduced the amount of global memory access I have to do Right",
    "start": "3035520",
    "end": "3041920"
  },
  {
    "text": "I I load as much memory as I can at once into shared memory I do all of my sort",
    "start": "3041920",
    "end": "3046960"
  },
  {
    "text": "of submatrix computations on that tile that I can and then I move on to the next one Right Um and of course the",
    "start": "3046960",
    "end": "3053760"
  },
  {
    "text": "other nice thing is that because I'm loading um an entire tile you know I can traverse these submatrices matrices in",
    "start": "3053760",
    "end": "3060079"
  },
  {
    "text": "whatever order I want like column measure or row measure And so I can coales all the memory accesses whenever",
    "start": "3060079",
    "end": "3065280"
  },
  {
    "text": "I'm loading a tile from global to shared memory Right So so there's kind of winds all around here um when we tile our",
    "start": "3065280",
    "end": "3073160"
  },
  {
    "text": "accesses So we can do a little bit of of tiling math Um so we've got let's say a",
    "start": "3073160",
    "end": "3078720"
  },
  {
    "text": "matrix A a matrix B and a matrix C So let's say the full matrices these are square matrices are of size N And let's",
    "start": "3078720",
    "end": "3085680"
  },
  {
    "text": "say I have a tile of size T right Oh yes question Previous slide of",
    "start": "3085680",
    "end": "3093280"
  },
  {
    "text": "load m0 So three loading m00 0 again",
    "start": "3093280",
    "end": "3099119"
  },
  {
    "text": "So in that case I just wrote it for for completeness but m00 z let's say is just you know stored in shared memory Let's",
    "start": "3099119",
    "end": "3104720"
  },
  {
    "text": "just keep it cached I won't load it again That that's definitely just there for completeness Not that you would",
    "start": "3104720",
    "end": "3110319"
  },
  {
    "text": "actually like discard and reload the the uh matrix again That would be kind of insane Cool Okay Um and so we can kind",
    "start": "3110319",
    "end": "3119599"
  },
  {
    "text": "of do very simple tiling math to think about you know what's happening So let's say I'm going to do a n byn matrix",
    "start": "3119599",
    "end": "3125359"
  },
  {
    "text": "multiply right Um so if I do a non-tiled matrix multiply if I'm just going over rows and columns then every input every",
    "start": "3125359",
    "end": "3132240"
  },
  {
    "text": "time I process it has to come from global memory So each input is read sort of n times from global memory right So",
    "start": "3132240",
    "end": "3138079"
  },
  {
    "text": "each of these is read sort of n times Um if I do a tiled matrix multiply well you",
    "start": "3138079",
    "end": "3144240"
  },
  {
    "text": "know the the global reads are operating over a tile So I'm reading each input n over t times from global memory and I'm",
    "start": "3144240",
    "end": "3151440"
  },
  {
    "text": "reading t times within each tile right of course I'm doing matrix matrix multiplies so I can't reduce the total",
    "start": "3151440",
    "end": "3157040"
  },
  {
    "text": "number of reads I have to read all the matrix elements but I can shift the reads into uh basically fast shared",
    "start": "3157040",
    "end": "3163760"
  },
  {
    "text": "memory right so I do t times um memory reads into shared memory and n overt times from global memory um and that's",
    "start": "3163760",
    "end": "3170559"
  },
  {
    "text": "great because if we have a big shared memory that can store big tiles that's a factor of t reduction in the total",
    "start": "3170559",
    "end": "3176240"
  },
  {
    "text": "amount of data that has to come from global memory Right So tiling can be really really powerful um of an idea",
    "start": "3176240",
    "end": "3182000"
  },
  {
    "text": "when you're operating over matrices and you can move things um into shared memory Um tiling is is quite complex Um",
    "start": "3182000",
    "end": "3190319"
  },
  {
    "text": "this is the source of many many sort of uh confusing things about GPU and matrix",
    "start": "3190319",
    "end": "3196079"
  },
  {
    "text": "multiply performance Um one thing that can happen right once we start tiling things you start asking things about",
    "start": "3196079",
    "end": "3202520"
  },
  {
    "text": "discretization right Um so imagine I have a tile size of 128 That seems like a nice good round tile size Um but then",
    "start": "3202520",
    "end": "3211760"
  },
  {
    "text": "um you know when I have a a full matrix of 256 size that's great That's a 2x2 tile Things load nicely Now let's say I",
    "start": "3211760",
    "end": "3219359"
  },
  {
    "text": "have a 257 size tile um on the column side Now this is a bad time because I",
    "start": "3219359",
    "end": "3225280"
  },
  {
    "text": "need to have six tiles in order to cover this matrix And the two tiles on the right are very very sparse there's just",
    "start": "3225280",
    "end": "3232400"
  },
  {
    "text": "not much stuff in there right And the problem with this is that each tile is going to be assigned to SM right So each",
    "start": "3232400",
    "end": "3239839"
  },
  {
    "text": "of these tiles is going to be a block and each thread is going to be operating within each tile So those two tiles on",
    "start": "3239839",
    "end": "3246400"
  },
  {
    "text": "the right they're not going to be doing very much at all right Those SM are going to be basically be sitting idle Um",
    "start": "3246400",
    "end": "3251520"
  },
  {
    "text": "and if you were kind of compute capped you would have wanted to more evenly distribute the load uh between SM right",
    "start": "3251520",
    "end": "3256960"
  },
  {
    "text": "So um you have to basically optimize your tile sizes to try to avoid these kinds of scenarios But in reality right",
    "start": "3256960",
    "end": "3264240"
  },
  {
    "text": "there's a lot of complex things that go into setting the tile size right Um remember you have to coales your memory",
    "start": "3264240",
    "end": "3269680"
  },
  {
    "text": "accesses So you have to think carefully about that You have to um uh m you have to not exceed your shared memory size",
    "start": "3269680",
    "end": "3276000"
  },
  {
    "text": "right So so the tiles can't be too big And you have to divide the matrix dimension hopefully evenly or as close",
    "start": "3276000",
    "end": "3281520"
  },
  {
    "text": "to evenly as possible so you don't end up with this situation of sort of an underutilized SM um at the very end",
    "start": "3281520",
    "end": "3288599"
  },
  {
    "text": "here Um yes so you have say smaller sizes do something like would GPUs do",
    "start": "3288599",
    "end": "3296800"
  },
  {
    "text": "something like where they can like fetch the tile beforehand and if so like would",
    "start": "3296800",
    "end": "3303599"
  },
  {
    "text": "that happen the level Yeah So you're you're asking about whether or not you",
    "start": "3303599",
    "end": "3310079"
  },
  {
    "text": "can like overlap uh memory reads and computation and yeah that's that's naturally done in uh GPUs like they're",
    "start": "3310079",
    "end": "3317520"
  },
  {
    "text": "always like trying to use the the available bandwidth Like as long as shared memory is available they can go and put things into it The issue is that",
    "start": "3317520",
    "end": "3324640"
  },
  {
    "text": "whenever you're you know effectively utilizing um your SMS you're basically maxed out on your shared memory right",
    "start": "3324640",
    "end": "3330640"
  },
  {
    "text": "that's like the the the bottlenecked resource and so there is no place to prefetch in some sense",
    "start": "3330640",
    "end": "3338319"
  },
  {
    "text": "Cool Okay Um and the other thing that is very very",
    "start": "3338319",
    "end": "3344640"
  },
  {
    "text": "you know we're getting into the weeds here um complex is the interaction between tiling um and sort of burst",
    "start": "3344640",
    "end": "3352160"
  },
  {
    "text": "sections Um so imagine I have a a matrix layout that's kind of like this Um where",
    "start": "3352160",
    "end": "3359520"
  },
  {
    "text": "you know I have my nice burst sections Um and each burst section lines up nicely with a tile So to read this tile",
    "start": "3359520",
    "end": "3365839"
  },
  {
    "text": "all I have to do is to you know get four different burst sections and I've gotten this entire tile Now imagine what",
    "start": "3365839",
    "end": "3373200"
  },
  {
    "text": "happens if I add sort of one element extra and the way the the matrix is laid",
    "start": "3373200",
    "end": "3379119"
  },
  {
    "text": "out you know my sort of tile start sort of my burst sections flow over So now",
    "start": "3379119",
    "end": "3384480"
  },
  {
    "text": "what's happening is when I load my tile I'm going to load this first part and that's really great I get the entire",
    "start": "3384480",
    "end": "3390160"
  },
  {
    "text": "first row as a burst section Now in the second row this actually belongs to two different burst sections And so I have",
    "start": "3390160",
    "end": "3396319"
  },
  {
    "text": "to do two reads in order to get this second row and so on and so forth So I've essentially doubled the number of",
    "start": "3396319",
    "end": "3403200"
  },
  {
    "text": "memory accesses because I've added a single extra element at the very end there that's kind of bumped up the",
    "start": "3403200",
    "end": "3409280"
  },
  {
    "text": "alignment of my burst section and my align layout And so basically if tiles or your matrix sizes aren't multiples of",
    "start": "3409280",
    "end": "3416640"
  },
  {
    "text": "your burst section you can easily end up with situations like this where the rows don't line up with the burst section and",
    "start": "3416640",
    "end": "3423040"
  },
  {
    "text": "you've doubled the amount of memory access that you have to do Um and the way to get around this is you have to do",
    "start": "3423040",
    "end": "3429040"
  },
  {
    "text": "padding to be able to kind of get nice round matrix sizes so that your burst sections line up with the size of your",
    "start": "3429040",
    "end": "3435839"
  },
  {
    "text": "tiles Right So this is this is getting very into the weeds here Um but if you really want to squeeze out all the",
    "start": "3435839",
    "end": "3441040"
  },
  {
    "text": "performance from your matrix multiplies these are the kinds of things you have to think about right And you will get",
    "start": "3441040",
    "end": "3446240"
  },
  {
    "text": "bitten by this um if you're not thinking about it Um and of course I guess like uh things",
    "start": "3446240",
    "end": "3453119"
  },
  {
    "text": "like torch compile and and all the CUDA optimizations for matrix multiplies they're doing exactly the kinds of stuff",
    "start": "3453119",
    "end": "3458640"
  },
  {
    "text": "that that I just talked about right That's the way you you get better performance Um and so you know all this",
    "start": "3458640",
    "end": "3465280"
  },
  {
    "text": "matrix complexity you know ends up in situations like this um where you know",
    "start": "3465280",
    "end": "3470559"
  },
  {
    "text": "the I'm reading Andre's this tweet here but you know the the most dramatic optimization to nano GPT is to increase",
    "start": "3470559",
    "end": "3476880"
  },
  {
    "text": "the vocab size from 5257 to 5304 um which is the nearest multiple 64 um",
    "start": "3476880",
    "end": "3483920"
  },
  {
    "text": "which gives you much much higher occupancy um careful with your powers of two right so that's a 25% speed up from",
    "start": "3483920",
    "end": "3490559"
  },
  {
    "text": "adding uh how many it's like 50 uh 57 no 47 uh dimensions to your vocap like",
    "start": "3490559",
    "end": "3496640"
  },
  {
    "text": "that's that's kind of like you know how does that happen right um and so that",
    "start": "3496640",
    "end": "3502400"
  },
  {
    "text": "kind of brings us back to the mystery like you know I I was dragging you through all the GPU details um in the",
    "start": "3502400",
    "end": "3509040"
  },
  {
    "text": "hopes that you know you'll have a full understanding of all the performance characteristics but in some sense the payoff is you know I now get to explain",
    "start": "3509040",
    "end": "3515520"
  },
  {
    "text": "to you how this chart comes to be and at the end you won't find matrix multiply performance to be so uh mysterious or",
    "start": "3515520",
    "end": "3521839"
  },
  {
    "text": "scary um at the end here right So the very first part is very very simple like",
    "start": "3521839",
    "end": "3527119"
  },
  {
    "text": "we understand compute intensity right This is exactly the roof line that I pointed out at the very beginning right",
    "start": "3527119",
    "end": "3532559"
  },
  {
    "text": "So so up until here which is about 1536 right Um there's just not enough matrix",
    "start": "3532559",
    "end": "3538640"
  },
  {
    "text": "multiply work to do right The just loading the matrix and doing very basic",
    "start": "3538640",
    "end": "3543680"
  },
  {
    "text": "IO right that you have to do is becoming a bottleneck below this point right So throughput is going to fall through to",
    "start": "3543680",
    "end": "3550000"
  },
  {
    "text": "the ground uh uh past this point you just don't have enough memory bandwidth to support your compute units Now on the",
    "start": "3550000",
    "end": "3557839"
  },
  {
    "text": "right side here in theory right if you if I draw the upper envelope this is the kind of maximum achievable performance",
    "start": "3557839",
    "end": "3563839"
  },
  {
    "text": "So it's possible up here to saturate all of my compute units and get really great performance But if you kind of mess up",
    "start": "3563839",
    "end": "3570640"
  },
  {
    "text": "your matrix sizing you can end up in these kind of really weird places and within each one of these you can kind of",
    "start": "3570640",
    "end": "3576160"
  },
  {
    "text": "end up in a weird trough And so we're going to kind of think a little bit about you know why do you have all these",
    "start": "3576160",
    "end": "3581520"
  },
  {
    "text": "different places you can end up Um so the very first thing um this",
    "start": "3581520",
    "end": "3587599"
  },
  {
    "text": "first line here um this is a a tiling alignment issue So if you look at um",
    "start": "3587599",
    "end": "3593200"
  },
  {
    "text": "kind of the multiples here so I've now colored each of these lines based on kind of the divisibility of the matrix",
    "start": "3593200",
    "end": "3600319"
  },
  {
    "text": "size um and this is the size by which it's divisible So if it's divisible by 32 then you're in good shape you're in",
    "start": "3600319",
    "end": "3606880"
  },
  {
    "text": "these purple dots up here If you're divisible by uh 16 um you're actually uh",
    "start": "3606880",
    "end": "3613040"
  },
  {
    "text": "still up here There's two colors And then if you're green your k equals 8 you're up here If you're orange you're k",
    "start": "3613040",
    "end": "3618559"
  },
  {
    "text": "equals 2 And if you're k equals 1 you're all the way down here If you're not divisible by any number uh don't pick",
    "start": "3618559",
    "end": "3624400"
  },
  {
    "text": "prime dimensions You're not going to get very good throughput on your matrix multiplies Um and a big part of this is",
    "start": "3624400",
    "end": "3631440"
  },
  {
    "text": "going to be you know once you get to kind of k equals 2 and k equals 1 you are basically forcing the situation",
    "start": "3631440",
    "end": "3638400"
  },
  {
    "text": "where you can no longer read tiles in the sort of nicely aligned way with your burst reads And that's going to lead to",
    "start": "3638400",
    "end": "3645359"
  },
  {
    "text": "to some serious issues So so that's kind of a problem But then okay So so that's one",
    "start": "3645359",
    "end": "3651680"
  },
  {
    "text": "part of the mystery but I think another part of the mystery remains Like so within this orange line you know I think",
    "start": "3651680",
    "end": "3657280"
  },
  {
    "text": "if you zoom into here you see this giant drop right from from this point all the way down to this point where you're just",
    "start": "3657280",
    "end": "3663520"
  },
  {
    "text": "kind of wondering what happened here How could I lose so much performance increasing my dimension by",
    "start": "3663520",
    "end": "3669640"
  },
  {
    "text": "two Um and so let's just look at these numbers Um and it's just I think this is a fun puzzle So I'm just going to walk",
    "start": "3669640",
    "end": "3675599"
  },
  {
    "text": "you through the puzzle Um this is going to happen when you transition from 1792 to 1790 uh I guess three or four um size",
    "start": "3675599",
    "end": "3684559"
  },
  {
    "text": "Let's say four here Um just so that it's a factor of two still Well why does that happen Okay Well let's say that we're",
    "start": "3684559",
    "end": "3691280"
  },
  {
    "text": "using a tile size of 256x 128 That's a pretty natural size Um as a fun fun fact",
    "start": "3691280",
    "end": "3697520"
  },
  {
    "text": "you know the matrix multiply units in these GPUs they're they're naturally operating on matrices of roughly size",
    "start": "3697520",
    "end": "3703359"
  },
  {
    "text": "128 So 256 x 128 is a is a very nice tile size right So that means how many",
    "start": "3703359",
    "end": "3709520"
  },
  {
    "text": "tiles are there Well there's seven * 14 tiles right Because we're dividing the dimension of the matrix by the size of",
    "start": "3709520",
    "end": "3715520"
  },
  {
    "text": "our tiles That's a total of 98 different tiles Um and if we increase this by one",
    "start": "3715520",
    "end": "3720720"
  },
  {
    "text": "well you know we're going to have to round up each one of our coordinates And so we're going to have a lot more tiles 120 of them right Um so we've increased",
    "start": "3720720",
    "end": "3727839"
  },
  {
    "text": "the number of tiles by quite a bit Um well you know what's going to happen is",
    "start": "3727839",
    "end": "3733200"
  },
  {
    "text": "not only did we significantly increase the tiles and some of them have lower utilization which is bad but actually",
    "start": "3733200",
    "end": "3738799"
  },
  {
    "text": "even worse an A100 has 108 SMS right Um and if I if you go all the way back to",
    "start": "3738799",
    "end": "3745440"
  },
  {
    "text": "kind of the GPU execution model right SM can can execute in parallel and they're kind of the execution units And so when",
    "start": "3745440",
    "end": "3751440"
  },
  {
    "text": "you have 98 SMS they all go and run right You can you can dispatch them all All the SM are running you know you got",
    "start": "3751440",
    "end": "3757119"
  },
  {
    "text": "great utilization Once you go to 120 um tiles now you've got more tiles than SMS",
    "start": "3757119",
    "end": "3762880"
  },
  {
    "text": "So 108 of those will execute and then you will go back and you'll say all right I've got some more SMS at very",
    "start": "3762880",
    "end": "3768160"
  },
  {
    "text": "very low utilization you're going to execute the remaining 12 and wait for those to complete right and that's going to be really bad So if you look at your",
    "start": "3768160",
    "end": "3774240"
  },
  {
    "text": "utilization you got good utilization for a while you'll drop off a cliff and then you'll sort of finish up your job right",
    "start": "3774240",
    "end": "3779280"
  },
  {
    "text": "So this is something called wave quantization Um and so ideally your tile sizes are either much bigger than the",
    "start": "3779280",
    "end": "3785839"
  },
  {
    "text": "number of SMS or you know they're they're not like this where you're just like barely over the SM and you've",
    "start": "3785839",
    "end": "3790880"
  },
  {
    "text": "caused this quantization uh sort of error uh additionally",
    "start": "3790880",
    "end": "3796200"
  },
  {
    "text": "Cool All right I know this is this is low-level details but in many ways you know I I've been saying through through",
    "start": "3796200",
    "end": "3803039"
  },
  {
    "text": "many classes that language models and deep learning is attention to detail Um and these kinds of attention to detail",
    "start": "3803039",
    "end": "3808559"
  },
  {
    "text": "is the things that allow people to scale up LMS um to really really large sizes and get great performance Um so it's",
    "start": "3808559",
    "end": "3814319"
  },
  {
    "text": "worth knowing even if you're not a person that's going to do systems engineering So what were the tricks right Um key ideas here First one is you",
    "start": "3814319",
    "end": "3822880"
  },
  {
    "text": "got to reduce the amount of memory accesses right So there's lots of ways to do it You can do coalesing right so",
    "start": "3822880",
    "end": "3828880"
  },
  {
    "text": "that you're not you can sort of reuse reads that you're getting for free Um you can do fusion so that you know you",
    "start": "3828880",
    "end": "3834640"
  },
  {
    "text": "can fuse multiple operations together and avoid unnecessary reads and writes",
    "start": "3834640",
    "end": "3840079"
  },
  {
    "text": "You can move memory to shared memory So you know even if you're going to do reads they're going to be from much faster memory Um and that's going to be",
    "start": "3840079",
    "end": "3846559"
  },
  {
    "text": "sort of tiling tricks that you can do Um and then finally you can kind of trade memory for other resources that you do",
    "start": "3846559",
    "end": "3852559"
  },
  {
    "text": "have right So you can trade it for compute which is going to be um recomputation or you can trade it for",
    "start": "3852559",
    "end": "3858000"
  },
  {
    "text": "just numerical precision or stability which is going to be quantization right so there's lots of bags of tricks that you have in order to get sort of",
    "start": "3858000",
    "end": "3864960"
  },
  {
    "text": "performance um out right so so there's lots of things you can do um you just have to be really mindful of kind of the",
    "start": "3864960",
    "end": "3871119"
  },
  {
    "text": "role that memory plays in the performance of a GPU right that's kind of the key thing to get the m the most",
    "start": "3871119",
    "end": "3877400"
  },
  {
    "text": "out cool any questions on on that before I sort of move to the final part with flash attention",
    "start": "3877400",
    "end": "3884640"
  },
  {
    "text": "Okay good All right so now I'm going to put it all together right Like I'm going",
    "start": "3885280",
    "end": "3890480"
  },
  {
    "text": "to try to make it so that all the the tricks that I taught you aren't these like random disconnected facts about GPUs They're kind of part of the",
    "start": "3890480",
    "end": "3897200"
  },
  {
    "text": "standard performance optimization toolkit and flash attention and flash attention 2 will hopefully teach you how",
    "start": "3897200",
    "end": "3903520"
  },
  {
    "text": "that all comes together to to build one of the you know the foundations I guess of modern high performance transformers",
    "start": "3903520",
    "end": "3910079"
  },
  {
    "text": "So um flash attention you know we know that it dramatically accelerates attention um and most of you probably",
    "start": "3910079",
    "end": "3915920"
  },
  {
    "text": "know that that's done through some CUDA kernel magic um but maybe you don't know all the details right so you know what",
    "start": "3915920",
    "end": "3922720"
  },
  {
    "text": "the the paper says is okay so there's one part that's happening which is you know you do attention on a unoptimized",
    "start": "3922720",
    "end": "3929119"
  },
  {
    "text": "you know pietorch transformer implementation if you fuse the kernel and you do some things you can get significant significant speed ups um and",
    "start": "3929119",
    "end": "3937280"
  },
  {
    "text": "from the paper you know they say we apply two established techniques techniques tiling and recomputation to overcome the technical challenge of",
    "start": "3937280",
    "end": "3942480"
  },
  {
    "text": "computing exact attention in sub quadratic HBM accesses right so so it's not sub quadratic you know computation",
    "start": "3942480",
    "end": "3948799"
  },
  {
    "text": "because you can't do that you have to compute you know attention in general but they're going to get subquadratic accesses to the uh high bandwidth or",
    "start": "3948799",
    "end": "3956640"
  },
  {
    "text": "global memory right and so that's really the key if your memory is the bottleneck you know you want to make that not",
    "start": "3956640",
    "end": "3961760"
  },
  {
    "text": "quadratic so that at least you can pay for quadratic cost with your compute rather than with your memory",
    "start": "3961760",
    "end": "3969319"
  },
  {
    "text": "So just for a really quick recap you know at this point you've implemented attention many many times in many classes Um right so it's going to be",
    "start": "3969359",
    "end": "3976559"
  },
  {
    "text": "three different matrix multiplies You've got a K Q and V with a soft max in between Um so the matrix multiplies are",
    "start": "3976559",
    "end": "3984160"
  },
  {
    "text": "pretty simple that can be you know done with tiling I've showed you examples like that And what's different about",
    "start": "3984160",
    "end": "3989920"
  },
  {
    "text": "attention Well there's a softmax thing that's going to be the real tricky bit And then once we can deal with the",
    "start": "3989920",
    "end": "3995359"
  },
  {
    "text": "softmax um all of the sort of matrix multiply things I was talking about will just come into play So um the matrix",
    "start": "3995359",
    "end": "4002799"
  },
  {
    "text": "multiply as I said before is exactly what I taught you So if you look at the figure one from the flash attention",
    "start": "4002799",
    "end": "4008400"
  },
  {
    "text": "paper this is really just a simple tiled matrix multiply right You see you know the the K matrix the Q matrix you see it",
    "start": "4008400",
    "end": "4015760"
  },
  {
    "text": "cut up into small blocks you know small blocks of it are being copied to SRAMM they're being multiplied and then",
    "start": "4015760",
    "end": "4021920"
  },
  {
    "text": "they're being you know accumulate or they sent to the HBM where you do soft maxes um and then you multiply um with a",
    "start": "4021920",
    "end": "4029599"
  },
  {
    "text": "V right So this is all just really simple um in terms of the KQV matrix multiply But now we have to think about",
    "start": "4029599",
    "end": "4036720"
  },
  {
    "text": "the softmax right Like what's going on with the softmax So the key thing here is the",
    "start": "4036720",
    "end": "4041839"
  },
  {
    "text": "softmax Sorry I'm going to I'm going to roll back one step So the issue with the softmax What's the problem with the",
    "start": "4041839",
    "end": "4047039"
  },
  {
    "text": "softmax It's a global operation right The softmax in an attention operates row by row You have to sum the entire row",
    "start": "4047039",
    "end": "4054160"
  },
  {
    "text": "right To compute sort of the sum normalizing term of the softmax And that's very problematic If I have tiles",
    "start": "4054160",
    "end": "4059920"
  },
  {
    "text": "right ideally I want to do everything within the tiles right I don't ever want to have to write back to the big matrix",
    "start": "4059920",
    "end": "4065920"
  },
  {
    "text": "And so I need a softmax that can be computed online within each tile right I",
    "start": "4065920",
    "end": "4071119"
  },
  {
    "text": "want to do as much computation within each tile as possible So the key thing here is to uh use what's called the",
    "start": "4071119",
    "end": "4078720"
  },
  {
    "text": "online softmax Um and so what is that If you have a stream of values right",
    "start": "4078720",
    "end": "4084079"
  },
  {
    "text": "normally the batch version of the softmax you take all of your x1 through x of ns and you would exponentiate them",
    "start": "4084079",
    "end": "4090400"
  },
  {
    "text": "sum them and you would divide them right That's what you would do in your normal softmax And then you would you know maybe compute the maximum value and you",
    "start": "4090400",
    "end": "4096480"
  },
  {
    "text": "subtract that in order to be able to make this numerically stable right So this is the the standard numerically",
    "start": "4096480",
    "end": "4101838"
  },
  {
    "text": "stable softmax on the the left side Um so the online softmax I've taken this",
    "start": "4101839",
    "end": "4106880"
  },
  {
    "text": "from from Mikallof and Gimmelstein in 2018 Um well you can sort of realize that you can pull out um via sort of",
    "start": "4106880",
    "end": "4114640"
  },
  {
    "text": "like a telescoping some kind of an argument um basically the current running uh sort of normalizer term and",
    "start": "4114640",
    "end": "4122318"
  },
  {
    "text": "the current sort of top term of e to the x i minus max of x k right so what you're going to do is you're going to",
    "start": "4122319",
    "end": "4128000"
  },
  {
    "text": "maintain your current max that you've seen over x1 through x of j which is my",
    "start": "4128000",
    "end": "4133120"
  },
  {
    "text": "current iteration and then I'm also going to maintain sort of this uh correction term if my max updated This",
    "start": "4133120",
    "end": "4139679"
  },
  {
    "text": "is going to basically correct my max and then I'm going to add my sort of new term over here Right So this d of j is",
    "start": "4139679",
    "end": "4146560"
  },
  {
    "text": "going to track online the top term of this equation term two over here and",
    "start": "4146560",
    "end": "4151758"
  },
  {
    "text": "then you know at the end I can also then compute the the normalizer and then sort",
    "start": "4151759",
    "end": "4157359"
  },
  {
    "text": "of get the normalized y of i that I want right this d of v is itself sort of the normalization term um that I need So the",
    "start": "4157359",
    "end": "4165199"
  },
  {
    "text": "key thing here is that this can be done online I don't need the x1 through x ofn up front All I need need is sort of the",
    "start": "4165199",
    "end": "4171758"
  },
  {
    "text": "stream of x1 through xn And that's really key because I can now compute the softmax tile by tile Right Within each",
    "start": "4171759",
    "end": "4178480"
  },
  {
    "text": "tile I can run this algorithm and that will let me compute kind of the partial softmax for that tile And then I can",
    "start": "4178480",
    "end": "4185120"
  },
  {
    "text": "sort of write back if I need to all the components that I sort of I'm keeping track of And that's all that I kind of",
    "start": "4185120",
    "end": "4190719"
  },
  {
    "text": "need in order to do this computation Right So I never have to materialize the full um n squ matrix in order to compute",
    "start": "4190719",
    "end": "4198159"
  },
  {
    "text": "the softmax And so that's basically it But",
    "start": "4198159",
    "end": "4203520"
  },
  {
    "text": "once you have that you know you've put it all together and you can get the forward pass um of flash attention And",
    "start": "4203520",
    "end": "4209040"
  },
  {
    "text": "if you go and look at the flash attention to paper which is um going to be a thing that we're going to ask you",
    "start": "4209040",
    "end": "4214400"
  },
  {
    "text": "to implement So you you're going to be following through kind of these these steps here Um you're going to see exactly this idea So first you're going",
    "start": "4214400",
    "end": "4221440"
  },
  {
    "text": "to have your your KQ matrix multiply and this is going to be tiled So these are little tiled chunks and they're going to",
    "start": "4221440",
    "end": "4227280"
  },
  {
    "text": "be multiplied And how am I going to compute the softmax Well I'm going to maintain sort of a running value of",
    "start": "4227280",
    "end": "4233679"
  },
  {
    "text": "these sort of exponentiated sums And then I'm going to keep incrementally updating it and correcting for the",
    "start": "4233679",
    "end": "4239120"
  },
  {
    "text": "maximum terms And by doing that I can compute all the necessary quantities kind of tile by tile sort of going from",
    "start": "4239120",
    "end": "4246560"
  },
  {
    "text": "one tile to another and then just multiply once again with tiles with V in the end and that will give me sort of my",
    "start": "4246560",
    "end": "4253199"
  },
  {
    "text": "full soft max output right um yes so we won't be able to compute that output",
    "start": "4253199",
    "end": "4258719"
  },
  {
    "text": "until we compute the like 2k multiplication across all tiles right so",
    "start": "4258719",
    "end": "4265440"
  },
  {
    "text": "we do have to double back on each um so the question was you can't compute this until you you are done um with all the",
    "start": "4265440",
    "end": "4272880"
  },
  {
    "text": "tiles and so you have to double back um on all the tiles um So you won't have built up that denominator sum until you",
    "start": "4272880",
    "end": "4280480"
  },
  {
    "text": "see every tile Um that's right So so you will have to before you can output your softmax you will have to go through all",
    "start": "4280480",
    "end": "4285840"
  },
  {
    "text": "the tiles This is correct Um but by let's say I do all the tiles once right",
    "start": "4285840",
    "end": "4291120"
  },
  {
    "text": "like I do all n squed tiles Um at that point I have all the components that I need in order to directly output the",
    "start": "4291120",
    "end": "4296560"
  },
  {
    "text": "soft maps At that point I don't have to re do recomputation because I have the normalizer terms already right by going",
    "start": "4296560",
    "end": "4302719"
  },
  {
    "text": "through each of these kind of tiles at the end of going through all these tiles I've built up you know L3 or L L of N",
    "start": "4302719",
    "end": "4310239"
  },
  {
    "text": "which is the sum of all of the exponentiated terms So I already have that in my sort of uh in my shared",
    "start": "4310239",
    "end": "4315920"
  },
  {
    "text": "memory for this last tile And then that allows me to exponentiate and divide and",
    "start": "4315920",
    "end": "4321120"
  },
  {
    "text": "then return all the components Okay Um so the backward pass",
    "start": "4321120",
    "end": "4328960"
  },
  {
    "text": "I'm not going to cover um you can do recomputation tile by tile which will allow you to avoid storing the softmax",
    "start": "4328960",
    "end": "4335440"
  },
  {
    "text": "right remember you know I always want to avoid storing anything that's of size n squared and so here I've been sort of",
    "start": "4335440",
    "end": "4341440"
  },
  {
    "text": "clever with the tiles so that I don't have to store any of the n squ components when I'm computing for example the softmax but in the backwards",
    "start": "4341440",
    "end": "4348400"
  },
  {
    "text": "pass if I store the activations that's already something that's n squed sized right so I don't want to store my n",
    "start": "4348400",
    "end": "4354000"
  },
  {
    "text": "squed activations I'm going to have to recmp compute it on the fly tile by tile when I do the backwards pass Right So",
    "start": "4354000",
    "end": "4360080"
  },
  {
    "text": "that's a really key other trick that they do um in order to to make the backwards pass possible But otherwise",
    "start": "4360080",
    "end": "4365840"
  },
  {
    "text": "it's fairly standard It's really the same thing as computing the gradients just tile by tile and doing that",
    "start": "4365840",
    "end": "4371159"
  },
  {
    "text": "computation Um so okay that that brings us to the",
    "start": "4371159",
    "end": "4377360"
  },
  {
    "text": "end here Um hopefully you've kind of seen how all of the pieces I talked about about tiling and coalesing and",
    "start": "4377360",
    "end": "4382520"
  },
  {
    "text": "recomputation um come together to to give you uh flash attention and all these really cool things that make your",
    "start": "4382520",
    "end": "4388080"
  },
  {
    "text": "transformers go much faster Um so to to you know recap for the whole lecture",
    "start": "4388080",
    "end": "4393920"
  },
  {
    "text": "right Hardware is kind of the thing that has really powered all of the language models that we have today Um and so if",
    "start": "4393920",
    "end": "4400080"
  },
  {
    "text": "you really want to leverage your hardware you have to understand the low-level details I think all the systems advances really engage with a",
    "start": "4400080",
    "end": "4406159"
  },
  {
    "text": "lot of the the concepts that I taught today Um and the current GPU sort of scaling you know that plot is really the",
    "start": "4406159",
    "end": "4412239"
  },
  {
    "text": "one you should remember Um really really incentivizes and encourages you uh to think about memory movement right The",
    "start": "4412239",
    "end": "4419040"
  },
  {
    "text": "memory movement is the bottleneck in all of this And so you don't want to just think about oh how do I reduce the number of flops That's important too Um",
    "start": "4419040",
    "end": "4426560"
  },
  {
    "text": "really you really have to think about okay how do I make my memory movements more efficient Um and then finally if",
    "start": "4426560",
    "end": "4433040"
  },
  {
    "text": "you you know have to do a certain amount of computation well to optimize things the way to do it is to optimize your",
    "start": "4433040",
    "end": "4439360"
  },
  {
    "text": "data movement right to to be able to avoid as much movement from the the high bandwidth memory or the global memory um",
    "start": "4439360",
    "end": "4445920"
  },
  {
    "text": "as possible You want to reduce that and have everything in the very very fast shared memory and that leads to good performance um on things like um flash",
    "start": "4445920",
    "end": "4453360"
  },
  {
    "text": "attention Um thanks everyone",
    "start": "4453360",
    "end": "4457960"
  }
]