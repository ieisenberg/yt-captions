[
  {
    "start": "0",
    "end": "4288"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4288",
    "end": "5830"
  },
  {
    "text": "This is part two in our\nseries on contextual word",
    "start": "5830",
    "end": "8100"
  },
  {
    "text": "representations.",
    "start": "8100",
    "end": "8928"
  },
  {
    "text": "We're going to be talking about\nthe transformer architecture,",
    "start": "8928",
    "end": "11470"
  },
  {
    "text": "which is the central piece\nfor all the models we'll",
    "start": "11470",
    "end": "13980"
  },
  {
    "text": "be exploring in this unit.",
    "start": "13980",
    "end": "15929"
  },
  {
    "text": "Let's dive into the\nmodel structure,",
    "start": "15930",
    "end": "17700"
  },
  {
    "text": "we'll work through this\nusing a simple example.",
    "start": "17700",
    "end": "19830"
  },
  {
    "text": "At the bottom here I've got\nthe input sequence, \"the Rock",
    "start": "19830",
    "end": "22440"
  },
  {
    "text": "rules\" and I've indicated\nin red that we're",
    "start": "22440",
    "end": "24830"
  },
  {
    "text": "going to be keeping\ntrack of the positions",
    "start": "24830",
    "end": "26580"
  },
  {
    "text": "of each one of those\ntokens in the sequence.",
    "start": "26580",
    "end": "29508"
  },
  {
    "text": "But the first step\nis a familiar one.",
    "start": "29508",
    "end": "31050"
  },
  {
    "text": "We're going to look at both\nthe words and the positions",
    "start": "31050",
    "end": "33480"
  },
  {
    "text": "in separate embedding spaces.",
    "start": "33480",
    "end": "35700"
  },
  {
    "text": "Those are fixed embedding\nspaces that we'll",
    "start": "35700",
    "end": "37500"
  },
  {
    "text": "learn as part of learning all\nthe parameters in this model.",
    "start": "37500",
    "end": "40320"
  },
  {
    "text": "I've given the word\nembeddings in light",
    "start": "40320",
    "end": "42480"
  },
  {
    "text": "gray and the positional\nembeddings in dark gray.",
    "start": "42480",
    "end": "45900"
  },
  {
    "text": "To form what we think of\nas the actual embedding",
    "start": "45900",
    "end": "48120"
  },
  {
    "text": "for this model, we do\nan elementwise addition",
    "start": "48120",
    "end": "50550"
  },
  {
    "text": "of the word embedding with\nthe positional embedding.",
    "start": "50550",
    "end": "53309"
  },
  {
    "text": "And that gives us\nthe representations",
    "start": "53310",
    "end": "54990"
  },
  {
    "text": "that are in green here,\nand you can see that",
    "start": "54990",
    "end": "56880"
  },
  {
    "text": "on the right side\nof the slide, I'm",
    "start": "56880",
    "end": "58338"
  },
  {
    "text": "going to be keeping track of all\nof the calculations with regard",
    "start": "58338",
    "end": "61260"
  },
  {
    "text": "to this C column here\nand they're completely",
    "start": "61260",
    "end": "63629"
  },
  {
    "text": "parallel for columns A and B.",
    "start": "63630",
    "end": "65790"
  },
  {
    "text": "So to form C input, we do\nelementwise addition of X34,",
    "start": "65790",
    "end": "69540"
  },
  {
    "text": "the embedding for\nthe word \"rules\"",
    "start": "69540",
    "end": "71310"
  },
  {
    "text": "and P3 which is the embedding\nfor position 3 in this model.",
    "start": "71310",
    "end": "76390"
  },
  {
    "text": "The next layer is really the\nhallmark of this architecture",
    "start": "76390",
    "end": "79270"
  },
  {
    "text": "and what gives the\npaper its title,",
    "start": "79270",
    "end": "80799"
  },
  {
    "text": "Attention is All You Need.",
    "start": "80800",
    "end": "82520"
  },
  {
    "text": "We're going to form a\nbunch of dense dot product",
    "start": "82520",
    "end": "84880"
  },
  {
    "text": "connections between all\nof these representations.",
    "start": "84880",
    "end": "87850"
  },
  {
    "text": "So you can think\nof those as forming",
    "start": "87850",
    "end": "89380"
  },
  {
    "text": "these connections\nthat look like there's",
    "start": "89380",
    "end": "91240"
  },
  {
    "text": "a dense thicket of them.",
    "start": "91240",
    "end": "92979"
  },
  {
    "text": "On the right here, I've\ngiven the core calculation",
    "start": "92980",
    "end": "95230"
  },
  {
    "text": "and it should be familiar\nfrom part one in this unit.",
    "start": "95230",
    "end": "97870"
  },
  {
    "text": "It's exactly the calculation\nI've presented there",
    "start": "97870",
    "end": "100360"
  },
  {
    "text": "with just two small changes,\nbut fundamentally if our target",
    "start": "100360",
    "end": "103930"
  },
  {
    "text": "vector is the C\ninput here, we're",
    "start": "103930",
    "end": "105640"
  },
  {
    "text": "attending to inputs A\nand B, and we do that",
    "start": "105640",
    "end": "109479"
  },
  {
    "text": "by forming the\ndot products here.",
    "start": "109480",
    "end": "111250"
  },
  {
    "text": "And the one twist from before\nis that instead of just taking",
    "start": "111250",
    "end": "113890"
  },
  {
    "text": "those dot products,\nwe'll normalize them",
    "start": "113890",
    "end": "116020"
  },
  {
    "text": "by the square root of the\ndimensionality of the model dk.",
    "start": "116020",
    "end": "119799"
  },
  {
    "text": "dk is an important value\nhere because of the way",
    "start": "119800",
    "end": "122320"
  },
  {
    "text": "we combine representations\nin the transformer.",
    "start": "122320",
    "end": "125020"
  },
  {
    "text": "All of the outputs and\nall the layers we look at",
    "start": "125020",
    "end": "127689"
  },
  {
    "text": "have to have the same\ndimensionality as given by dk.",
    "start": "127690",
    "end": "130728"
  },
  {
    "text": "And so what we're doing\nhere is essentially",
    "start": "130728",
    "end": "132520"
  },
  {
    "text": "scaling these dot products\nto kind of keep them",
    "start": "132520",
    "end": "134478"
  },
  {
    "text": "within a sensible range.",
    "start": "134478",
    "end": "137300"
  },
  {
    "text": "That gives us a score\nvector off the tilde,",
    "start": "137300",
    "end": "140040"
  },
  {
    "text": "we softmax normalize that.",
    "start": "140040",
    "end": "141422"
  },
  {
    "text": "And then the other\ntwist is that instead",
    "start": "141423",
    "end": "143090"
  },
  {
    "text": "of using mean here as we did\nbefore, we use summation.",
    "start": "143090",
    "end": "146060"
  },
  {
    "text": "But the actual vector is the\none we calculated before, we're",
    "start": "146060",
    "end": "148790"
  },
  {
    "text": "going to take weighted\nversions of A input and B input",
    "start": "148790",
    "end": "152090"
  },
  {
    "text": "according to this vector of\nweights that we created here.",
    "start": "152090",
    "end": "155690"
  },
  {
    "text": "That gives us the\nrepresentation C attention",
    "start": "155690",
    "end": "158150"
  },
  {
    "text": "as given in orange\nhere, and we do",
    "start": "158150",
    "end": "160250"
  },
  {
    "text": "that of course for all the\nother positions in the model.",
    "start": "160250",
    "end": "163690"
  },
  {
    "text": "The next step is\nkind of interesting,",
    "start": "163690",
    "end": "165500"
  },
  {
    "text": "we're creating what's called\na residual connection.",
    "start": "165500",
    "end": "167800"
  },
  {
    "text": "So to get CA layer\nhere in yellow,",
    "start": "167800",
    "end": "170650"
  },
  {
    "text": "we add up C input and this\nattention representation",
    "start": "170650",
    "end": "174579"
  },
  {
    "text": "that we just created\nand apply dropout",
    "start": "174580",
    "end": "176830"
  },
  {
    "text": "as a regularization step there.",
    "start": "176830",
    "end": "178640"
  },
  {
    "text": "And that gives us CA layer,\nthe interesting thing",
    "start": "178640",
    "end": "180877"
  },
  {
    "text": "there of course is this\nresidual connection.",
    "start": "180877",
    "end": "182709"
  },
  {
    "text": "Instead of simply feeding\nforward C attention,",
    "start": "182710",
    "end": "185830"
  },
  {
    "text": "we feed forward\nactually a version of it",
    "start": "185830",
    "end": "187540"
  },
  {
    "text": "that's combined with our initial\npositionally encoded embedding.",
    "start": "187540",
    "end": "193170"
  },
  {
    "text": "If we follow that with a step\nof layer normalization, which",
    "start": "193170",
    "end": "196042"
  },
  {
    "text": "should help with\noptimization, it's",
    "start": "196042",
    "end": "197500"
  },
  {
    "text": "going to kind of\nscale the weights",
    "start": "197500",
    "end": "199030"
  },
  {
    "text": "in these representations.",
    "start": "199030",
    "end": "201220"
  },
  {
    "text": "The next step is\nmore meaningful.",
    "start": "201220",
    "end": "202670"
  },
  {
    "text": "This is a series of\ntwo dense layers,",
    "start": "202670",
    "end": "205180"
  },
  {
    "text": "so we'll take Ca\nnorm here and feed it",
    "start": "205180",
    "end": "207340"
  },
  {
    "text": "through this dense layer\nwith a non-linearity followed",
    "start": "207340",
    "end": "210370"
  },
  {
    "text": "by another linear\nlayer to give us Cfx",
    "start": "210370",
    "end": "213430"
  },
  {
    "text": "that's given in dark blue here.",
    "start": "213430",
    "end": "215553"
  },
  {
    "text": "And that's followed\nby another one",
    "start": "215553",
    "end": "216970"
  },
  {
    "text": "of these interesting\nresidual connections,",
    "start": "216970",
    "end": "218920"
  },
  {
    "text": "so we'll apply dropout to Cff.",
    "start": "218920",
    "end": "221349"
  },
  {
    "text": "And then add that\nin with Ca norm,",
    "start": "221350",
    "end": "222970"
  },
  {
    "text": "as given down here\nat the bottom,",
    "start": "222970",
    "end": "224720"
  },
  {
    "text": "and that gives us the second\nyellow representation.",
    "start": "224720",
    "end": "227790"
  },
  {
    "text": "And we follow that by one more\nstep of layer normalization",
    "start": "227790",
    "end": "231099"
  },
  {
    "text": "and that gives us the output\nfor this block of transformer",
    "start": "231100",
    "end": "234160"
  },
  {
    "text": "representations.",
    "start": "234160",
    "end": "236305"
  },
  {
    "text": "And you can imagine\nof course as you'll",
    "start": "236305",
    "end": "237930"
  },
  {
    "text": "see that we can stack up\nthese transformer blocks,",
    "start": "237930",
    "end": "240329"
  },
  {
    "text": "and the way that we\ndo that is essentially",
    "start": "240330",
    "end": "242790"
  },
  {
    "text": "by taking these dark green\nrepresentations at the top",
    "start": "242790",
    "end": "245790"
  },
  {
    "text": "here and using them as inputs\nand all the calculations",
    "start": "245790",
    "end": "248280"
  },
  {
    "text": "are the same.",
    "start": "248280",
    "end": "248920"
  },
  {
    "text": "So you might imagine that\nwe could continue here",
    "start": "248920",
    "end": "251220"
  },
  {
    "text": "by just doing a dense\nseries of attention",
    "start": "251220",
    "end": "253200"
  },
  {
    "text": "connections across\nthese and then",
    "start": "253200",
    "end": "255030"
  },
  {
    "text": "continuing on with the\ncalculations I just presented.",
    "start": "255030",
    "end": "257739"
  },
  {
    "text": "And in that way, we could\nstack up transformer blocks.",
    "start": "257740",
    "end": "260535"
  },
  {
    "text": "And I'll return\nto that later on.",
    "start": "260535",
    "end": "263075"
  },
  {
    "text": "There are a few\nother things that",
    "start": "263075",
    "end": "264450"
  },
  {
    "text": "are worth pointing\nout that are kind",
    "start": "264450",
    "end": "265620"
  },
  {
    "text": "of noteworthy about this model.",
    "start": "265620",
    "end": "267229"
  },
  {
    "text": "It looks like a complicated\nseries of calculations,",
    "start": "267230",
    "end": "270512"
  },
  {
    "text": "but I would say that\nfundamentally what's",
    "start": "270512",
    "end": "272220"
  },
  {
    "text": "happening here is we're doing\npositional encoding to get",
    "start": "272220",
    "end": "275610"
  },
  {
    "text": "embeddings, so that they\nare position-sensitive",
    "start": "275610",
    "end": "278280"
  },
  {
    "text": "representations of words.",
    "start": "278280",
    "end": "280560"
  },
  {
    "text": "We follow that within\nan attention layer",
    "start": "280560",
    "end": "282630"
  },
  {
    "text": "which creates that dense\nthicket of connections",
    "start": "282630",
    "end": "284970"
  },
  {
    "text": "between all of the words\nas positionally encoded.",
    "start": "284970",
    "end": "288330"
  },
  {
    "text": "Then we have these\noptimization things woven in,",
    "start": "288330",
    "end": "290490"
  },
  {
    "text": "but fundamentally we're\nfollowing that attention step",
    "start": "290490",
    "end": "293220"
  },
  {
    "text": "with two series of\nfeed-forward layer steps",
    "start": "293220",
    "end": "295560"
  },
  {
    "text": "here, followed by the same\nprocess of dropout and layer",
    "start": "295560",
    "end": "299260"
  },
  {
    "text": "normalization.",
    "start": "299260",
    "end": "300450"
  },
  {
    "text": "So if you kind of elided\nthe yellow and the purple,",
    "start": "300450",
    "end": "303410"
  },
  {
    "text": "you would see that\nwhat we're really doing",
    "start": "303410",
    "end": "305160"
  },
  {
    "text": "is attention followed\nby feed-forward.",
    "start": "305160",
    "end": "307237"
  },
  {
    "text": "And then as we stack\nthese things that",
    "start": "307237",
    "end": "308820"
  },
  {
    "text": "would be attention\nfeed-forward, attention",
    "start": "308820",
    "end": "310880"
  },
  {
    "text": "feed-forward as we climbed up.",
    "start": "310880",
    "end": "312750"
  },
  {
    "text": "And interwoven into\nthere are some things",
    "start": "312750",
    "end": "315060"
  },
  {
    "text": "that I would say help\nwith optimization.",
    "start": "315060",
    "end": "317919"
  },
  {
    "text": "Another noteworthy\nthing about this model",
    "start": "317920",
    "end": "320410"
  },
  {
    "text": "is that the only\nsense in which we",
    "start": "320410",
    "end": "322330"
  },
  {
    "text": "are keeping track of the\nlinear order of the sequence",
    "start": "322330",
    "end": "325210"
  },
  {
    "text": "is in those\npositional embeddings,",
    "start": "325210",
    "end": "327100"
  },
  {
    "text": "if not for them the column order\nwould be completely irrelevant.",
    "start": "327100",
    "end": "330640"
  },
  {
    "text": "Because of course,\nwe've created all",
    "start": "330640",
    "end": "332140"
  },
  {
    "text": "of these symmetric connections\nat the attention layer.",
    "start": "332140",
    "end": "335090"
  },
  {
    "text": "And there are no\nother connections",
    "start": "335090",
    "end": "336669"
  },
  {
    "text": "across these columns.",
    "start": "336670",
    "end": "337970"
  },
  {
    "text": "So the only sense in\nwhich column order, that",
    "start": "337970",
    "end": "340900"
  },
  {
    "text": "is word order matters here,\nis via those positional",
    "start": "340900",
    "end": "344020"
  },
  {
    "text": "embeddings.",
    "start": "344020",
    "end": "344996"
  },
  {
    "start": "344997",
    "end": "347800"
  },
  {
    "text": "Here's a more detailed look\nat the attention calculations",
    "start": "347800",
    "end": "350699"
  },
  {
    "text": "themselves.",
    "start": "350700",
    "end": "351332"
  },
  {
    "text": "I just want to bring\nup how this actually",
    "start": "351332",
    "end": "353040"
  },
  {
    "text": "works at a mechanical level.",
    "start": "353040",
    "end": "354610"
  },
  {
    "text": "So this is the calculation\nas I presented it",
    "start": "354610",
    "end": "356520"
  },
  {
    "text": "on the previous slide and\nin part one of this unit.",
    "start": "356520",
    "end": "360000"
  },
  {
    "text": "In the paper, and\nnow commonly it's",
    "start": "360000",
    "end": "362070"
  },
  {
    "text": "presented in this matrix format.",
    "start": "362070",
    "end": "363750"
  },
  {
    "text": "And if you're like\nme, it's not obvious",
    "start": "363750",
    "end": "366120"
  },
  {
    "text": "right away that these are\nequivalent calculations.",
    "start": "366120",
    "end": "368870"
  },
  {
    "text": "So what I've done for\nthese next two slides",
    "start": "368870",
    "end": "371010"
  },
  {
    "text": "is to show you via\nworked out examples",
    "start": "371010",
    "end": "374190"
  },
  {
    "text": "how those calculations\nwork and how they arrive",
    "start": "374190",
    "end": "376740"
  },
  {
    "text": "at exactly the same values.",
    "start": "376740",
    "end": "378569"
  },
  {
    "text": "I'm not gonna spend too\nmuch time on this here,",
    "start": "378570",
    "end": "380580"
  },
  {
    "text": "this is really just\nhere for you if you",
    "start": "380580",
    "end": "382560"
  },
  {
    "text": "would like to work through\nthe calculations in detail,",
    "start": "382560",
    "end": "384810"
  },
  {
    "text": "which I strongly encourage\nbecause this is really",
    "start": "384810",
    "end": "387480"
  },
  {
    "text": "the fundamental\nstep in this model.",
    "start": "387480",
    "end": "390092"
  },
  {
    "text": "And here's all the\ndetails that you",
    "start": "390092",
    "end": "391550"
  },
  {
    "text": "would need to get hands\non with these ideas.",
    "start": "391550",
    "end": "395030"
  },
  {
    "text": "Now so far, I've\npresented attention",
    "start": "395030",
    "end": "397100"
  },
  {
    "text": "in a kind of simplified way.",
    "start": "397100",
    "end": "398720"
  },
  {
    "text": "A hallmark of attention\nis in the transformer",
    "start": "398720",
    "end": "401450"
  },
  {
    "text": "is that it is typically\nmulti-headed attention.",
    "start": "401450",
    "end": "403880"
  },
  {
    "text": "So let me unpack that idea\na little bit concretely.",
    "start": "403880",
    "end": "406970"
  },
  {
    "text": "We'll start with our\ninput sequence from before",
    "start": "406970",
    "end": "410120"
  },
  {
    "text": "and we'll be looking at these\ngreen representations here.",
    "start": "410120",
    "end": "412940"
  },
  {
    "text": "And the idea behind the\nmulti-headed attention",
    "start": "412940",
    "end": "415370"
  },
  {
    "text": "mechanisms that\nwe're gonna inject",
    "start": "415370",
    "end": "416840"
  },
  {
    "text": "a bunch of learned parameters\ninto this process to encourage",
    "start": "416840",
    "end": "420199"
  },
  {
    "text": "diversity as part of\nthe learning process,",
    "start": "420200",
    "end": "422420"
  },
  {
    "text": "and are really diverse and\ninteresting representations.",
    "start": "422420",
    "end": "425540"
  },
  {
    "text": "So here's how that works.",
    "start": "425540",
    "end": "427200"
  },
  {
    "text": "We're gonna form three\nrepresentations here using",
    "start": "427200",
    "end": "429410"
  },
  {
    "text": "that same dot product\nmechanism as before.",
    "start": "429410",
    "end": "431630"
  },
  {
    "text": "And fundamentally it's the\nsame calculation except now",
    "start": "431630",
    "end": "435282"
  },
  {
    "text": "we're gonna have a bunch of\nlearned weight parameters",
    "start": "435282",
    "end": "437490"
  },
  {
    "text": "that's given in orange here.",
    "start": "437490",
    "end": "439224"
  },
  {
    "text": "And those will help\nus with two things.",
    "start": "439225",
    "end": "440850"
  },
  {
    "text": "First injecting diversity\ninto this process,",
    "start": "440850",
    "end": "443210"
  },
  {
    "text": "and also smushing\nthe dimensionality",
    "start": "443210",
    "end": "445220"
  },
  {
    "text": "of the representations down\nto one third of the size",
    "start": "445220",
    "end": "448010"
  },
  {
    "text": "that we're targeting SDK for\nour model dimensionality.",
    "start": "448010",
    "end": "451190"
  },
  {
    "text": "And you'll see why that\nhappens in a second.",
    "start": "451190",
    "end": "453590"
  },
  {
    "text": "But fundamentally\nwhat we're doing",
    "start": "453590",
    "end": "455180"
  },
  {
    "text": "is exactly the calculation\nwe did before but now",
    "start": "455180",
    "end": "457910"
  },
  {
    "text": "with these learned\nparameters injected into it.",
    "start": "457910",
    "end": "460038"
  },
  {
    "text": "So if you squint, you can\nsee that this is really",
    "start": "460038",
    "end": "462080"
  },
  {
    "text": "the dot product of c input\nwith a input as before.",
    "start": "462080",
    "end": "465590"
  },
  {
    "text": "But now, it's transformed\nby these learned parameters",
    "start": "465590",
    "end": "468260"
  },
  {
    "text": "that are given in orange.",
    "start": "468260",
    "end": "469520"
  },
  {
    "text": "And that repeats for\nthese other calculations.",
    "start": "469520",
    "end": "472751"
  },
  {
    "text": "So we're going to do\nthat for position a.",
    "start": "472752",
    "end": "474460"
  },
  {
    "text": "And we do it also\nfor position b.",
    "start": "474460",
    "end": "476590"
  },
  {
    "text": "And it's the same\ncalculation, but now it's",
    "start": "476590",
    "end": "478690"
  },
  {
    "text": "new parameters for\nthe second position.",
    "start": "478690",
    "end": "482090"
  },
  {
    "text": "And then for the third head,\nexactly the same calculation",
    "start": "482090",
    "end": "485080"
  },
  {
    "text": "but new learned parameters\nup at the top here.",
    "start": "485080",
    "end": "487550"
  },
  {
    "text": "So this is\nthree-headed attention.",
    "start": "487550",
    "end": "489669"
  },
  {
    "text": "And then we actually form\nthe representations that",
    "start": "489670",
    "end": "492700"
  },
  {
    "text": "proceed with the rest of the\ncalculation of the transformer",
    "start": "492700",
    "end": "495580"
  },
  {
    "text": "architectures presented before\nis by concatenating the three",
    "start": "495580",
    "end": "500020"
  },
  {
    "text": "representations we created\nfor each one of these units.",
    "start": "500020",
    "end": "502610"
  },
  {
    "text": "So the A column is the\nfirst representation",
    "start": "502610",
    "end": "506199"
  },
  {
    "text": "in each one of these heads.",
    "start": "506200",
    "end": "507760"
  },
  {
    "text": "The B column is the second\nrepresentation in each head",
    "start": "507760",
    "end": "511060"
  },
  {
    "text": "and similarly for\nthe C column it's",
    "start": "511060",
    "end": "513010"
  },
  {
    "text": "the third representation\nin each one of these heads.",
    "start": "513010",
    "end": "515229"
  },
  {
    "text": "And that's why each\none of these needs",
    "start": "515230",
    "end": "517149"
  },
  {
    "text": "to add one-third\nthe dimensionality",
    "start": "517150",
    "end": "519039"
  },
  {
    "text": "of our full model so that we\ncan concatenate them and then",
    "start": "519039",
    "end": "522400"
  },
  {
    "text": "feed those into the\nsubsequent calculations.",
    "start": "522400",
    "end": "525610"
  },
  {
    "text": "The idea here, of course, is\nthat injecting all of these",
    "start": "525610",
    "end": "528070"
  },
  {
    "text": "learned parameters into all\nof these different heads we're",
    "start": "528070",
    "end": "531520"
  },
  {
    "text": "providing the model a chance\nto learn lots of diverse ways",
    "start": "531520",
    "end": "534520"
  },
  {
    "text": "of relating the words\nin the sequence.",
    "start": "534520",
    "end": "538592"
  },
  {
    "text": "The final point is one I've\nalready mentioned before,",
    "start": "538592",
    "end": "540800"
  },
  {
    "text": "which is that typically we\ndon't have just one transformer",
    "start": "540800",
    "end": "543217"
  },
  {
    "text": "block but rather a\nwhole stack of them,",
    "start": "543217",
    "end": "545470"
  },
  {
    "text": "we can repeat them N times.",
    "start": "545470",
    "end": "547480"
  },
  {
    "text": "For models you're working with\nyou might have 12 or 24 or even",
    "start": "547480",
    "end": "550810"
  },
  {
    "text": "more blocks in the\ntransformer architecture.",
    "start": "550810",
    "end": "553990"
  },
  {
    "text": "And the way we do that\nas I said is simply",
    "start": "553990",
    "end": "555940"
  },
  {
    "text": "by taking the dark green\nrepresentations of the output",
    "start": "555940",
    "end": "559060"
  },
  {
    "text": "layer here and using them as\ninputs to subsequent blocks",
    "start": "559060",
    "end": "562090"
  },
  {
    "text": "so they get attended\nto, and we proceed",
    "start": "562090",
    "end": "564190"
  },
  {
    "text": "with the subsequent\nregularization",
    "start": "564190",
    "end": "566110"
  },
  {
    "text": "and feed-forward\nsteps just as before.",
    "start": "566110",
    "end": "568899"
  },
  {
    "text": "And when you work with these\nmodels in Hugging Face,",
    "start": "568900",
    "end": "571480"
  },
  {
    "text": "if you ask for all of the hidden\nstates, what you're getting",
    "start": "571480",
    "end": "574209"
  },
  {
    "text": "is a grid of representations\ncorresponding to these output",
    "start": "574210",
    "end": "577570"
  },
  {
    "text": "blocks in green here.",
    "start": "577570",
    "end": "579970"
  },
  {
    "text": "And of course,\njust as a reminder",
    "start": "579970",
    "end": "581529"
  },
  {
    "text": "I'm not indicating it\nhere but there is actually",
    "start": "581530",
    "end": "583690"
  },
  {
    "text": "multi-headed attention of\neach one of these blocks",
    "start": "583690",
    "end": "586180"
  },
  {
    "text": "through each one of the layers.",
    "start": "586180",
    "end": "587500"
  },
  {
    "text": "So there are a lot of learned\nparameters in this model,",
    "start": "587500",
    "end": "590540"
  },
  {
    "text": "especially if you have\n12 or 24 attention heads.",
    "start": "590540",
    "end": "595524"
  },
  {
    "text": "At this point, I'm hoping\nthat you can now fruitfully",
    "start": "595525",
    "end": "598110"
  },
  {
    "text": "return to the original\nVaswani, et al paper",
    "start": "598110",
    "end": "600870"
  },
  {
    "text": "and look at their model\ndiagram and get more out of it.",
    "start": "600870",
    "end": "603360"
  },
  {
    "text": "For me, it's kind of\nhyper compressed but now",
    "start": "603360",
    "end": "605640"
  },
  {
    "text": "that we've done a deep\ndive into all the pieces,",
    "start": "605640",
    "end": "607770"
  },
  {
    "text": "I think this serves as a kind\nof useful shorthand for how",
    "start": "607770",
    "end": "610530"
  },
  {
    "text": "all the pieces fit together.",
    "start": "610530",
    "end": "611920"
  },
  {
    "text": "So let's just do that quickly.",
    "start": "611920",
    "end": "613500"
  },
  {
    "text": "As before we have positional\nencodings and input word",
    "start": "613500",
    "end": "616560"
  },
  {
    "text": "embeddings, and\nthose get added up",
    "start": "616560",
    "end": "618450"
  },
  {
    "text": "to give us the intuitive notion\nof an embedding in this model.",
    "start": "618450",
    "end": "622090"
  },
  {
    "text": "That's followed by the\nattention layer as we discussed,",
    "start": "622090",
    "end": "624630"
  },
  {
    "text": "and it has a residual\nconnection here into that layer",
    "start": "624630",
    "end": "627930"
  },
  {
    "text": "normalization part.",
    "start": "627930",
    "end": "629610"
  },
  {
    "text": "That's fed into the\nfeed-forward blocks",
    "start": "629610",
    "end": "631620"
  },
  {
    "text": "and that's followed by that\nsame process of kind of dropout",
    "start": "631620",
    "end": "634589"
  },
  {
    "text": "attention layer normalization.",
    "start": "634590",
    "end": "637050"
  },
  {
    "text": "And this is essentially\nsaying each one",
    "start": "637050",
    "end": "639330"
  },
  {
    "text": "of these is repeated for\nevery step in the encoder",
    "start": "639330",
    "end": "642270"
  },
  {
    "text": "process, every one of the\ncolumns that we looked at.",
    "start": "642270",
    "end": "646720"
  },
  {
    "text": "And if because\nthen in the paper,",
    "start": "646720",
    "end": "648970"
  },
  {
    "text": "they're working with an\nencoder/decoder model.",
    "start": "648970",
    "end": "651009"
  },
  {
    "text": "Each decoder state self attends\nwith all of the fellow decoder",
    "start": "651010",
    "end": "654250"
  },
  {
    "text": "states and with all of the\nencoder states, so right?",
    "start": "654250",
    "end": "656960"
  },
  {
    "text": "Imagine this double sequence,\nwe have a dense area",
    "start": "656960",
    "end": "659620"
  },
  {
    "text": "to potentially connect some\nconnections across both parts",
    "start": "659620",
    "end": "662890"
  },
  {
    "text": "of the representation.",
    "start": "662890",
    "end": "665310"
  },
  {
    "text": "On the right side when\nwe're doing decoding,",
    "start": "665310",
    "end": "667440"
  },
  {
    "text": "again, we repeat this block\nfor every decoder state.",
    "start": "667440",
    "end": "671400"
  },
  {
    "text": "And if every state there\nhas an output as for machine",
    "start": "671400",
    "end": "674850"
  },
  {
    "text": "translation or some kind\nof generation process,",
    "start": "674850",
    "end": "677370"
  },
  {
    "text": "then we'll have something\nlike this output",
    "start": "677370",
    "end": "679140"
  },
  {
    "text": "stack at every one of\nthose output states.",
    "start": "679140",
    "end": "681750"
  },
  {
    "text": "If by contrast we're\ndoing something",
    "start": "681750",
    "end": "683430"
  },
  {
    "text": "like NLI or sentiment\nclassification problem,",
    "start": "683430",
    "end": "686600"
  },
  {
    "text": "maybe just one of\nthose states will have",
    "start": "686600",
    "end": "688380"
  },
  {
    "text": "one of these outputs on them.",
    "start": "688380",
    "end": "690963"
  },
  {
    "text": "And then the attention gets\na little bit complicated",
    "start": "690963",
    "end": "693130"
  },
  {
    "text": "if you are doing decoding for\na model like natural language",
    "start": "693130",
    "end": "696070"
  },
  {
    "text": "generation or machine\ntranslation, in the decoder,",
    "start": "696070",
    "end": "698860"
  },
  {
    "text": "you can't attend into the future\nas you're doing generation.",
    "start": "698860",
    "end": "701829"
  },
  {
    "text": "So there's a\nmasking process that",
    "start": "701830",
    "end": "703690"
  },
  {
    "text": "limits self-attention to the\npreceding words in the sequence",
    "start": "703690",
    "end": "706900"
  },
  {
    "text": "that you're creating.",
    "start": "706900",
    "end": "708720"
  },
  {
    "start": "708720",
    "end": "712000"
  }
]