[
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "All right, so let's get started. The plan for today is to talk\nabout autoregressive models,",
    "start": "5490",
    "end": "13290"
  },
  {
    "text": "which is going to be the\nfirst type of or first family of generative models that we're\ngoing to consider in the class.",
    "start": "13290",
    "end": "22199"
  },
  {
    "text": "This is the kind of technology\nbehind large language models, things like ChatGPT.",
    "start": "22200",
    "end": "27330"
  },
  {
    "start": "27330",
    "end": "32820"
  },
  {
    "text": "So yeah, just as a\nrecap, remember, sort of like this high level overview.",
    "start": "32820",
    "end": "38640"
  },
  {
    "text": "Whenever you want to train a\ngenerative model, you need data. So samples from some IID,\nunknown probability distribution",
    "start": "38640",
    "end": "48300"
  },
  {
    "text": "P-data. And then you need to define\na model family, which is going to be a\nset of probability",
    "start": "48300",
    "end": "55050"
  },
  {
    "text": "distributions over the same\nspace over which your data is defined.",
    "start": "55050",
    "end": "60690"
  },
  {
    "text": "And these probability\ndistributions are typically\nparameterized somehow.",
    "start": "60690",
    "end": "66730"
  },
  {
    "text": "For example, using it could be\nconditional probability tables in the case of a\nBayesian network,",
    "start": "66730",
    "end": "73659"
  },
  {
    "text": "as we have seen in\nthe last lecture. For the most part, we're\ngoing to be thinking",
    "start": "73660",
    "end": "78860"
  },
  {
    "text": "about probability\ndistributions that are defined in terms of neural networks. So you can think of theta\nthere in that picture",
    "start": "78860",
    "end": "85090"
  },
  {
    "text": "as being kind of like the\nparameters of the neural network that you're going to use\nto define this probability",
    "start": "85090",
    "end": "91060"
  },
  {
    "text": "distribution. And then you're going to\ndefine some sort of notion of similarity or divergence\nbetween the data distribution",
    "start": "91060",
    "end": "99250"
  },
  {
    "text": "and your model distribution. And then we're going\nto try to optimize the parameters of\nthe neural network to make your model distribution\nas close as possible to the data",
    "start": "99250",
    "end": "107440"
  },
  {
    "text": "distribution. The caveat being that\nyou only have access to samples from the data\ndistribution, right?",
    "start": "107440",
    "end": "113590"
  },
  {
    "text": "So you can't evaluate the\nprobability of an image under the data distribution.",
    "start": "113590",
    "end": "118700"
  },
  {
    "text": "The only thing you have access\nto are a bunch of samples. And once you have this\nprobability distribution,",
    "start": "118700",
    "end": "125948"
  },
  {
    "text": "then you can do several things. You can sample from it. So you can choose a\nvector x with probability.",
    "start": "125948",
    "end": "134950"
  },
  {
    "text": "There's many different x's\nthat you could choose from. Each one of them is assigned\na probability by your model.",
    "start": "134950",
    "end": "140500"
  },
  {
    "text": "And you can choose one\naccording to this probability",
    "start": "140500",
    "end": "146350"
  },
  {
    "text": "distribution. So you sample from it. And this is what you need\nto generate new data.",
    "start": "146350",
    "end": "152950"
  },
  {
    "text": "We're going to be interested\nin evaluating probabilities for several reasons.",
    "start": "152950",
    "end": "158570"
  },
  {
    "text": "One is that evaluating\nprobabilities is useful for\ntraining the models. So if somehow you have\na way of figuring out",
    "start": "158570",
    "end": "167530"
  },
  {
    "text": "how likely is any particular\nimage according to your model, then that gives you\na pretty natural way",
    "start": "167530",
    "end": "173350"
  },
  {
    "text": "of training the model, kind of\nlike solving this optimization problem or trying to\nfind the point that",
    "start": "173350",
    "end": "179590"
  },
  {
    "text": "is as close as possible\nto the data distribution. And one way to do that is to\njust do maximum likelihood.",
    "start": "179590",
    "end": "184689"
  },
  {
    "text": "You can try to find the\nparameters of your model that maximize the probability of\nobserving a particular data set.",
    "start": "184690",
    "end": "192163"
  },
  {
    "text": "The other thing you can\ndo, if you have access to probabilities, is you can do\nthings like anomaly detection.",
    "start": "192163",
    "end": "197270"
  },
  {
    "text": "So given an input, you can see,\nis this input likely or not,",
    "start": "197270",
    "end": "203150"
  },
  {
    "text": "so kind of like what we\ndiscussed in the last lecture. One advantage of\ngenerative models compared",
    "start": "203150",
    "end": "208180"
  },
  {
    "text": "to discriminative\nmodels is that you can reason about\nthe possible inputs that you might be\ngiven access to.",
    "start": "208180",
    "end": "214659"
  },
  {
    "text": "So you might, for example, try\nto detect adversarial examples. Because perhaps\nthey are different",
    "start": "214660",
    "end": "221200"
  },
  {
    "text": "from the natural\nimages that you've used for training your model. And so if your\ngenerative model is good,",
    "start": "221200",
    "end": "226990"
  },
  {
    "text": "you might be able to\nidentify that something is odd about a particular input. Maybe the likelihood is\nlower than it should be.",
    "start": "226990",
    "end": "234710"
  },
  {
    "text": "And so you can say, OK,\nthis is perhaps an anomaly. Maybe I shouldn't be very\nconfident about the kind",
    "start": "234710",
    "end": "241090"
  },
  {
    "text": "of decisions or the\nkind of predictions that I make about this\nparticular data point.",
    "start": "241090",
    "end": "246700"
  },
  {
    "text": "And as we discussed\nanother thing you can do is potentially unsupervised\nrepresentation learning.",
    "start": "246700",
    "end": "253160"
  },
  {
    "text": "And so in order to do well at\nlearning a good approximation",
    "start": "253160",
    "end": "258398"
  },
  {
    "text": "of the data\ndistribution, you often need to understand the\nstructure of the data. And so in some cases, it's\ngoing to be a little bit tricky",
    "start": "258399",
    "end": "265302"
  },
  {
    "text": "for autoregressive models,\nwhich is what we're going to talk about today. But for other types of models,\nit's going to be pretty natural.",
    "start": "265303",
    "end": "271420"
  },
  {
    "text": "There's going to be a pretty\nnatural way of extracting features as a\nbyproduct, basically,",
    "start": "271420",
    "end": "277420"
  },
  {
    "text": "of training a good\ngenerative model. So the first question is, how\nto represent these probability",
    "start": "277420",
    "end": "286030"
  },
  {
    "text": "distributions. So how do you define this\nset in a meaningful way?",
    "start": "286030",
    "end": "291460"
  },
  {
    "text": "And today we're going to talk\nabout autoregressive models, which are built on the idea of\nusing Chain Rule, essentially.",
    "start": "291460",
    "end": "299050"
  },
  {
    "text": "And next we're going to\ntalk about how to learn it. So recall that there\nis this general result",
    "start": "299050",
    "end": "308260"
  },
  {
    "text": "that you can take any\nprobability distribution defined over an arbitrarily large\nnumber of variables n",
    "start": "308260",
    "end": "315280"
  },
  {
    "text": "and you can always factor it\nas a product of conditionals. So if you have four random\nvariables, x1 through x4,",
    "start": "315280",
    "end": "323080"
  },
  {
    "text": "you can always write it\ndown as the probability of x1, the probability of\nx2 given x1, and so forth.",
    "start": "323080",
    "end": "329630"
  },
  {
    "text": "And this is just fully general. You don't need to make any\nassumptions on the distribution.",
    "start": "329630",
    "end": "336729"
  },
  {
    "text": "Every distribution can be\nfactorized this way exactly. And in particular, you can\nalso use any ordering you want.",
    "start": "336730",
    "end": "345389"
  },
  {
    "text": "So in this case,\nI'm factorizing it based on the ordering\nx1, x2, x3, and x4.",
    "start": "345390",
    "end": "351110"
  },
  {
    "text": "But you could choose\na different ordering. So you could decide,\nyou could write it down as the probability of x4 times\nthe probability of x3 given x4,",
    "start": "351110",
    "end": "358849"
  },
  {
    "text": "and so forth. And here you start to see\nthat, yeah, in general, you can always do it.",
    "start": "358850",
    "end": "364010"
  },
  {
    "text": "But perhaps some orderings\nmight be better than others.",
    "start": "364010",
    "end": "369270"
  },
  {
    "text": "So if there is some kind\nof natural causal structure in the data, then perhaps\nmodeling the data along",
    "start": "369270",
    "end": "375380"
  },
  {
    "text": "that direction is easier. But the Chain Rule doesn't care. It works regardless of whatever\nordering you're going to use.",
    "start": "375380",
    "end": "382685"
  },
  {
    "text": " Bayes Net essentially\nexploit this idea.",
    "start": "382685",
    "end": "390289"
  },
  {
    "text": "And they make\nprogress by basically simplifying these conditionals.",
    "start": "390290",
    "end": "396110"
  },
  {
    "text": "So we've seen that, in general,\neven when the random variables are discrete, representing those\nconditionals as tables doesn't",
    "start": "396110",
    "end": "404990"
  },
  {
    "text": "scale, doesn't work. And so Bayesian\nnetworks essentially make some kind of conditional\nindependence assumption.",
    "start": "404990",
    "end": "412698"
  },
  {
    "text": "They assume that certain\nthings are conditionally independent from other things. And then that gives you\npotentially simpler factors",
    "start": "412698",
    "end": "421400"
  },
  {
    "text": "that you can\nrepresent as tables. And the other way\nto go about it is",
    "start": "421400",
    "end": "426590"
  },
  {
    "text": "to use a neural model\nwhere instead you're going to give up on the\ntabular representation.",
    "start": "426590",
    "end": "434220"
  },
  {
    "text": "So it's no longer\na lookup table. Now it's going to be some\nkind of function parameterized",
    "start": "434220",
    "end": "439640"
  },
  {
    "text": "by a neural network\nthat you're going to use to map different kind\nof assignments to the variables",
    "start": "439640",
    "end": "447800"
  },
  {
    "text": "you're conditioning\non to parameters for the conditional distribution\nover the next variable",
    "start": "447800",
    "end": "456440"
  },
  {
    "text": "in this ordering\nthat you're using. So in this kind of neural\nmodels, what we're going to do",
    "start": "456440",
    "end": "462289"
  },
  {
    "text": "is we're going to\nstart from Chain Rule, and then we're going\nto try to approximate the true conditionals\nusing neural networks.",
    "start": "462290",
    "end": "471190"
  },
  {
    "text": "And this works to the extent\nthat the neural network is sufficiently powerful\nthat it can well",
    "start": "471190",
    "end": "478210"
  },
  {
    "text": "approximate these conditional\nprobabilities, which could be potentially very complicated.",
    "start": "478210",
    "end": "484310"
  },
  {
    "text": "If you think about\nthose as tables, there could be really\ncomplicated relationships between the entries\nin the table.",
    "start": "484310",
    "end": "490160"
  },
  {
    "text": "And this kind of factorization\nusing neural models works to the extent that the\nneural network is sufficiently",
    "start": "490160",
    "end": "495730"
  },
  {
    "text": "flexible that it can capture\nthe structure of what you would get if you had a\nfully [? general ?] tabular",
    "start": "495730",
    "end": "503320"
  },
  {
    "text": "representation. And the good news is that a\nsufficiently deep neural network",
    "start": "503320",
    "end": "510840"
  },
  {
    "text": "can, in principle,\napproximate any function. And so that's kind of where the\nmagic of deep learning comes in.",
    "start": "510840",
    "end": "518340"
  },
  {
    "text": "If you can use very\ndeep neural networks, there's a good chance you might\nbe able to actually come up",
    "start": "518340",
    "end": "523740"
  },
  {
    "text": "with a decent approximation\nto these conditionals. And that's why these models\ntend to work in practice.",
    "start": "523740",
    "end": "529920"
  },
  {
    "text": " So remember that the machinery\nthat we're going to use",
    "start": "529920",
    "end": "537449"
  },
  {
    "text": "is going to be the same as\nthe one you use in regular, let's say, classification. So you want to predict\na binary label, given",
    "start": "537450",
    "end": "543780"
  },
  {
    "text": "a bunch of input features. You just care about the\nconditional distribution of a single variable, given\na potentially large number",
    "start": "543780",
    "end": "552180"
  },
  {
    "text": "of other variables. But the important thing\nis that you're just trying to predict one thing at\na time, a single variable Y.",
    "start": "552180",
    "end": "558840"
  },
  {
    "text": "And so you can use things like\nlogistic regression or neural networks to do these\nkind of things.",
    "start": "558840",
    "end": "564490"
  },
  {
    "text": "And in particular, we've\nseen that logistic regression is kind of like assuming a\nrelatively simple dependency",
    "start": "564490",
    "end": "572200"
  },
  {
    "text": "between the values\nof the covariates x, or the features that\nyou are conditioning on, and the conditional\nprobability of Y given x.",
    "start": "572200",
    "end": "579662"
  },
  {
    "text": "It's basically\nassuming that there is a linear dependency that\nthen is fed through a sigmoid",
    "start": "579663",
    "end": "585760"
  },
  {
    "text": "to get a non-negative\nnumber that has the right kind\nof normalization.",
    "start": "585760",
    "end": "590980"
  },
  {
    "text": "And you can make\nthings more flexible by assuming some kind of\nnon-linear dependence.",
    "start": "590980",
    "end": "597700"
  },
  {
    "text": "And that's where you use\nneural networks, right? So you can take your inputs\nx, you can transform them",
    "start": "597700",
    "end": "604540"
  },
  {
    "text": "by applying linear\ntransformations, non-linearities. You can stack them\nin any way you want.",
    "start": "604540",
    "end": "610459"
  },
  {
    "text": "And then at the end\nof the day, you still have some sort of\ntransformation that gives you the parameters of this\nconditional distribution",
    "start": "610460",
    "end": "617200"
  },
  {
    "text": "over what you're\ntrying to predict, given what you have access to. And so maybe at the end, you use\nsome kind of sigmoid function",
    "start": "617200",
    "end": "626740"
  },
  {
    "text": "or a softmax function to\nbasically normalize the output to a probability distribution.",
    "start": "626740",
    "end": "632540"
  },
  {
    "text": "So it's more flexible. You have more parameters, which\nis good because the model,",
    "start": "632540",
    "end": "638830"
  },
  {
    "text": "you can capture a richer\nset of dependencies between the variables. The price you pay is that you\nhave more parameters to learn.",
    "start": "638830",
    "end": "645670"
  },
  {
    "text": "You need more memory. And you might imagine that\nyou might need more data. ",
    "start": "645670",
    "end": "654830"
  },
  {
    "text": "Cool. So that's the building block. And then basically, the whole\nidea of autoregressive models",
    "start": "654830",
    "end": "660160"
  },
  {
    "text": "is that once you\nknow how to predict one thing using a neural\nnetwork, you can combine them.",
    "start": "660160",
    "end": "666070"
  },
  {
    "text": "And you can always think of\na high-dimensional output-- let's say an image--",
    "start": "666070",
    "end": "673630"
  },
  {
    "text": "as a number of\nindividual components. And Chain Rule gives\nyou a way of predicting",
    "start": "673630",
    "end": "679030"
  },
  {
    "text": "the individual components,\ngiven the previous ones. And so then you can plug\nin your neural network",
    "start": "679030",
    "end": "684700"
  },
  {
    "text": "to get a generative model. And that's what neural\nautoregressive models essentially do.",
    "start": "684700",
    "end": "691306"
  },
  {
    "text": "All right? So for example,\nlet's say that you wanted to learn a generative\nmodel over images.",
    "start": "691306",
    "end": "699550"
  },
  {
    "text": "So just for\nsimplicity, let's say that you wanted to work\nwith the binarized MNIST.",
    "start": "699550",
    "end": "705010"
  },
  {
    "text": "So MNIST is kind of\nlike a classic data set of handwritten digits.",
    "start": "705010",
    "end": "710540"
  },
  {
    "text": "So if you binarize them\nso that every pixel is either 0 or 1, black or white,\nthen they might look like this.",
    "start": "710540",
    "end": "718580"
  },
  {
    "text": "So you see that they kind of\nlook like handwritten digits. And each image has\n28 by 28 pixels.",
    "start": "718580",
    "end": "726980"
  },
  {
    "text": "So you have 28 times 28\nrandom variables to model. And the variables are binary--",
    "start": "726980",
    "end": "734500"
  },
  {
    "text": "0, 1, black or white. And the goal is to basically\nlearn a probability distribution",
    "start": "734500",
    "end": "740740"
  },
  {
    "text": "over these 784 random variables\nsuch that when you sample",
    "start": "740740",
    "end": "748140"
  },
  {
    "text": "from it, the images that you\nget hopefully look like the ones that you have in\nthe training set. Or in other words, you're hoping\nthat the distribution that you",
    "start": "748140",
    "end": "757709"
  },
  {
    "text": "learn is a good approximation\nto the data distribution that generated these samples\nIID, independent identically",
    "start": "757710",
    "end": "766019"
  },
  {
    "text": "distributed samples\nthat you have access to in the training set. And again this is\nchallenging because there's",
    "start": "766020",
    "end": "772360"
  },
  {
    "text": "a lot of possible images. And you need to be able\nto assign a probability to each one of them. And so recall the recipe is you\ndefine a family of probability",
    "start": "772360",
    "end": "784060"
  },
  {
    "text": "distributions parameterized\nby theta, which we're going to see in this lecture. And then you define some\nkind of learning objective",
    "start": "784060",
    "end": "790990"
  },
  {
    "text": "to search over the\nparameter space to do some kind of optimization. Reduce the learning problem\nto optimization over theta,",
    "start": "790990",
    "end": "798290"
  },
  {
    "text": "over the parameters that\ndefine the distribution to try to find a good\napproximation of the data",
    "start": "798290",
    "end": "804700"
  },
  {
    "text": "distribution, which is going\nto be the next lecture. ",
    "start": "804700",
    "end": "810830"
  },
  {
    "text": "So the way to use an\nautoregressive model to define this\nprobability distribution",
    "start": "810830",
    "end": "816100"
  },
  {
    "text": "is you first need\nto pick an ordering. So remember if you\nwant to use Chain Rule,",
    "start": "816100",
    "end": "821350"
  },
  {
    "text": "you have to pick an ordering. And for an image,\nit's not even obvious what the ordering should be.",
    "start": "821350",
    "end": "828100"
  },
  {
    "text": "There is not an obvious\nkind of causal structure. Like, you're not\nmodeling a time series where you might expect that\nthere is some causal structure",
    "start": "828100",
    "end": "836890"
  },
  {
    "text": "and maybe predicting the\nfuture given the past is easier than going backwards.",
    "start": "836890",
    "end": "842560"
  },
  {
    "text": "But any ordering\nworks, in principle. And so for example, you can\ntake a raster scan ordering.",
    "start": "842560",
    "end": "848410"
  },
  {
    "text": "And so you can go from\ntop-left to bottom-right. You can order the\n784 pixels that way.",
    "start": "848410",
    "end": "856822"
  },
  {
    "text": "And then you can apply chain\nrule to this probability distribution. And so you know that\nwithout loss of generality,",
    "start": "856822",
    "end": "865550"
  },
  {
    "text": "there is always a way to write\ndown this distribution that way, basically as the probability\nof choosing an arbitrary",
    "start": "865550",
    "end": "873140"
  },
  {
    "text": "value for the first\nrandom variable and then choosing a value for\nthe second, given the first, and so forth.",
    "start": "873140",
    "end": "878810"
  },
  {
    "text": "And so that's how you break\ndown a generative modeling problem that is tricky to\na sequence, a small number",
    "start": "878810",
    "end": "887810"
  },
  {
    "text": "of classification, regression,\nsomething we know how to handle. Each one of these\nconditionals is only",
    "start": "887810",
    "end": "894980"
  },
  {
    "text": "over a single random variable. And that's the kind of setting\nyou know how to deal with",
    "start": "894980",
    "end": "901753"
  },
  {
    "text": "or you typically\nconsider when you think about\nclassification, regression, those kind of problems.",
    "start": "901753",
    "end": "907940"
  },
  {
    "text": "And you cannot do tabular form. So a Bayesian network is\nout of the question here.",
    "start": "907940",
    "end": "915600"
  },
  {
    "text": "And so instead we're going\nto try to basically model these conditionals using some\nkind of neural model, some kind",
    "start": "915600",
    "end": "923700"
  },
  {
    "text": "of functional form\nthat will allow us to map the different\nconfigurations of the pixels",
    "start": "923700",
    "end": "931030"
  },
  {
    "text": "we are conditioning\non to a probability distribution over the\nnext pixel that we need to work with in this\nparticular ordering that we've",
    "start": "931030",
    "end": "938470"
  },
  {
    "text": "chosen. And so in particular,\nI mean, if you",
    "start": "938470",
    "end": "943480"
  },
  {
    "text": "think about the first\nprobability distribution, you can represent it as a\nconditional probability table.",
    "start": "943480",
    "end": "949540"
  },
  {
    "text": "That's just a binary\nrandom variable. You just need one\nparameter for that. So that's why I'm saying PCPT\nhere means that you can actually",
    "start": "949540",
    "end": "957730"
  },
  {
    "text": "store that one separately. But the other ones\nbecome complicated. And so you kind of have to make\nsome sort of approximation.",
    "start": "957730",
    "end": "966340"
  },
  {
    "text": "And one simple thing you\ncan do is to just use logistic regression.",
    "start": "966340",
    "end": "971580"
  },
  {
    "text": "So you can try to use logistic\nregression to basically predict the next pixel, given\nthe previous pixels.",
    "start": "971580",
    "end": "976940"
  },
  {
    "text": "And that gives you a\ngenerative model, basically. And if you do that,\nnotice that you",
    "start": "976940",
    "end": "985290"
  },
  {
    "text": "don't have a single\nclassification problem. You have a sequence of\nclassification problems.",
    "start": "985290",
    "end": "991500"
  },
  {
    "text": "Like, you need to be able to\npredict the second pixel, given the first one. You need to be able to\npredict the third pixel, given",
    "start": "991500",
    "end": "997649"
  },
  {
    "text": "the first two. You need to be able to\npredict the last pixel, the one in the bottom right,\ngiven everything else.",
    "start": "997650",
    "end": "1004680"
  },
  {
    "text": "And so all these\nclassification problems are basically\ndifferent and separate. You even have a different number\nof covariates or variables",
    "start": "1004680",
    "end": "1013279"
  },
  {
    "text": "that you are conditioning on. And so, in general,\nyou can potentially use different parameters,\ndifferent models",
    "start": "1013280",
    "end": "1019610"
  },
  {
    "text": "for each one of them. And this is kind of like\nwhat I'm alluding here. There is a different\nvector of coefficients",
    "start": "1019610",
    "end": "1027099"
  },
  {
    "text": "alpha for your\nlogistic regression model for each\nclassification problem. And so more explicitly,\nfor example,",
    "start": "1027099",
    "end": "1034970"
  },
  {
    "text": "you would have the\nfirst prior distribution over the first pixel, which\nis just a single number.",
    "start": "1034970",
    "end": "1041240"
  },
  {
    "text": "It tells you how often do\nyou choose the first pixel to be white versus black.",
    "start": "1041240",
    "end": "1047410"
  },
  {
    "text": "So if you think about the\nstructure of these images, this pixel here at the top-left\nis almost always black.",
    "start": "1047410",
    "end": "1055390"
  },
  {
    "text": "So you probably would want to\nchoose this number to be close to 0, assuming 0 means black.",
    "start": "1055390",
    "end": "1063590"
  },
  {
    "text": "Sort of like you want that\npixel to be often black. And then you need to\nbe able to specify",
    "start": "1063590",
    "end": "1070900"
  },
  {
    "text": "a way of predicting the second\npixel given the first one. And you can do it using a\nsimple logistic regression model",
    "start": "1070900",
    "end": "1078380"
  },
  {
    "text": "and so forth. Right? And that's a\nmodeling assumption.",
    "start": "1078380",
    "end": "1086010"
  },
  {
    "text": "Whether or not this type\nof generative model works well depends on\nwhether or not it's",
    "start": "1086010",
    "end": "1091440"
  },
  {
    "text": "easy to predict the\nvalue of a pixel given the previous ones in this\nparticular arbitrary order",
    "start": "1091440",
    "end": "1098070"
  },
  {
    "text": "that I've chosen for the pixels.  And whether this works,\nagain it depends on how good",
    "start": "1098070",
    "end": "1109430"
  },
  {
    "text": "this approximation is. So it might work well or\nit might not work well. Because maybe these\ndependencies are too simple.",
    "start": "1109430",
    "end": "1116480"
  },
  {
    "text": "Maybe regardless of how\nyou choose this alpha, there is not a good\nway of figuring out",
    "start": "1116480",
    "end": "1121789"
  },
  {
    "text": "how you should choose\nthe value, whether or not a pixel is white or\nblack in this case.",
    "start": "1121790",
    "end": "1127460"
  },
  {
    "text": " But you can think of it as\nan autoregressive model.",
    "start": "1127460",
    "end": "1135340"
  },
  {
    "text": "Because essentially what\nyou're doing is you're trying to regress. You're trying to predict the\nstructure of the data itself,",
    "start": "1135340",
    "end": "1145190"
  },
  {
    "text": "right? So you're regressing\non yourself. Like, you're trying to predict\nparts of each data point, given",
    "start": "1145190",
    "end": "1153190"
  },
  {
    "text": "other parts of the data point. ",
    "start": "1153190",
    "end": "1158240"
  },
  {
    "text": "And this kind of\nmodeling assumption",
    "start": "1158240",
    "end": "1165500"
  },
  {
    "text": "has been tried before. This kind of model is called\na fully visible sigmoid belief",
    "start": "1165500",
    "end": "1173120"
  },
  {
    "text": "network. It's kind of like a\nrelatively simple early type",
    "start": "1173120",
    "end": "1178570"
  },
  {
    "text": "of generative model\nthat as we'll see, is not going to work\nparticularly well. But it's kind of useful\nto work it through so",
    "start": "1178570",
    "end": "1186610"
  },
  {
    "text": "that you get a certain level of\nunderstanding of exactly what it means to model a joint\ndistribution in terms",
    "start": "1186610",
    "end": "1192070"
  },
  {
    "text": "of simple classification models. So when you think about\nwhat we're doing here,",
    "start": "1192070",
    "end": "1198940"
  },
  {
    "text": "when you think\nabout chain rule, we have all these individual\npixels that we're",
    "start": "1198940",
    "end": "1204640"
  },
  {
    "text": "modeling conditionally\non all the ones that come before it in the order. And so when you model\nthe probability of Xi",
    "start": "1204640",
    "end": "1214090"
  },
  {
    "text": "given all the variables that\ncome before it in the ordering, let's say, using a\nlogistic regression model,",
    "start": "1214090",
    "end": "1221720"
  },
  {
    "text": "you're basically outputting\nthe conditional probability of that pixel being on\nor off, given the values",
    "start": "1221720",
    "end": "1229090"
  },
  {
    "text": "of the previous pixels. And we're often going to denote\nthis using this symbol here,",
    "start": "1229090",
    "end": "1236279"
  },
  {
    "text": "x is smaller than\ni, which basically",
    "start": "1236280",
    "end": "1241530"
  },
  {
    "text": "means given all the indexes j\nare strictly smaller than i.",
    "start": "1241530",
    "end": "1251040"
  },
  {
    "text": "And in the case of\nlogistic regression, that conditional\nprobability is given",
    "start": "1251040",
    "end": "1257280"
  },
  {
    "text": "by this relatively\nsimple expression, a linear combination. And then you pass it\nthrough a sigmoid.",
    "start": "1257280",
    "end": "1264020"
  },
  {
    "text": "Now, how would you evaluate? If somebody gives\nyou a data point and you want to know how likely\nis this data point according",
    "start": "1264020",
    "end": "1271250"
  },
  {
    "text": "to my model, which is the kind\nof computation you would have to do if you want to train a\nmodel by maximum likelihood,",
    "start": "1271250",
    "end": "1279170"
  },
  {
    "text": "how would you evaluate\nthat joint probability, given that somehow you have\nall these values for alpha?",
    "start": "1279170",
    "end": "1286490"
  },
  {
    "start": "1286490",
    "end": "1292410"
  },
  {
    "text": "So what you would have to do is\nyou would go back to chain rule. So you basically just multiply\ntogether all these factors.",
    "start": "1292410",
    "end": "1299180"
  },
  {
    "text": "And so more specifically,\nthe first pixel X1 will have a value.",
    "start": "1299180",
    "end": "1304790"
  },
  {
    "text": "Well, I guess here I have\nan example with, let's say, imagine that you\nonly have 4 pixels.",
    "start": "1304790",
    "end": "1310930"
  },
  {
    "text": "There's four random variable. And let's say that we are\nobserving the value 0, 1, 1, 0.",
    "start": "1310930",
    "end": "1318440"
  },
  {
    "text": "Then, you basically need\nto multiply together all these values,\nwhich are basically",
    "start": "1318440",
    "end": "1326110"
  },
  {
    "text": "the predicted probability that\na pixel takes a particular value given the others.",
    "start": "1326110",
    "end": "1331299"
  },
  {
    "text": "And these predicted\nprobabilities depend on the values of the\nprevious pixels in the ordering.",
    "start": "1331300",
    "end": "1337055"
  },
  {
    "text": "Right.  So x-hat-i, which is the\npredicted probability",
    "start": "1337055",
    "end": "1343990"
  },
  {
    "text": "for the i-th pixel,\ndepends on all the pixels that come before\nit in the ordering.",
    "start": "1343990",
    "end": "1349240"
  },
  {
    "text": "So a little bit\nmore explicitly, it would look something\nlike this where",
    "start": "1349240",
    "end": "1354700"
  },
  {
    "text": "you would have to compute\nthe conditional probability of the second pixel when\nthe first pixel is 0.",
    "start": "1354700",
    "end": "1361221"
  },
  {
    "text": "You would have to compute\nthe conditional probability of the third pixel being,\nlet's say, in this case,",
    "start": "1361222",
    "end": "1366830"
  },
  {
    "text": "given that the previous two\nare 0 and 1, and so forth. And then you would basically\nreplace that expression here",
    "start": "1366830",
    "end": "1373759"
  },
  {
    "text": "for x-hat with the standard\nsigmoid logistic function thing.",
    "start": "1373760",
    "end": "1379430"
  },
  {
    "text": "And that would give\nyou the number. How would you sample\nfrom this distribution?",
    "start": "1379430",
    "end": "1386880"
  },
  {
    "text": "So let's say that somehow\nyou've trained the model and now you want to generate\nimages according to this model.",
    "start": "1386880",
    "end": "1392779"
  },
  {
    "text": " The good thing about\nan autoregressive model",
    "start": "1392780",
    "end": "1397860"
  },
  {
    "text": "is that it also gives you\na recipe to sample from it. In general, it might not\nbe obvious how you do this.",
    "start": "1397860",
    "end": "1405450"
  },
  {
    "text": "Like, OK, you have a\nrecipe to evaluate how likely different samples are.",
    "start": "1405450",
    "end": "1410880"
  },
  {
    "text": "But then how do you pick one\nwith the right probability, right? So would you randomly\ngenerate one image,",
    "start": "1410880",
    "end": "1418320"
  },
  {
    "text": "evaluate the\nprobability, and then do some sort of rejection sampling? You could do things like that.",
    "start": "1418320",
    "end": "1424650"
  },
  {
    "text": "You could use generic kind\nof like inference schemes. If you have a way of\nevaluating probabilities, you could try to even brute\nforce and kind of like invert",
    "start": "1424650",
    "end": "1432540"
  },
  {
    "text": "the CDF and try to do something\nlike that that, of course, would never scale\nto the situation",
    "start": "1432540",
    "end": "1440070"
  },
  {
    "text": "where you have hundreds\nof random variables. The good news is that\nyou can basically do it,",
    "start": "1440070",
    "end": "1445800"
  },
  {
    "text": "you can use chain rule\nagain and decide the values",
    "start": "1445800",
    "end": "1450810"
  },
  {
    "text": "of the pixels one by one. So what you would\ndo is, we know what",
    "start": "1450810",
    "end": "1456240"
  },
  {
    "text": "is the prior essentially\nprobability that the first pixel is on or off. And we can just pick a\nvalue for the first pixel.",
    "start": "1456240",
    "end": "1463200"
  },
  {
    "text": "Now, once we know the\nvalue of the first pixel, we know how to figure out\na value, probabilistically,",
    "start": "1463200",
    "end": "1469889"
  },
  {
    "text": "for the second pixel. So we can plug it into\nthe previous expression. Or you could do something like\nthis, just to be very pedantic.",
    "start": "1469890",
    "end": "1477210"
  },
  {
    "text": "There is some prior probability. And perhaps you always\nchoose it to be black because all the\nimages are like that.",
    "start": "1477210",
    "end": "1483029"
  },
  {
    "text": "But then you pick a value. And then you basically sample\nthe second random variable,",
    "start": "1483030",
    "end": "1489540"
  },
  {
    "text": "given the conditional\ndistribution. And this conditional\ndistribution, you can get the parameter\nby using this expression.",
    "start": "1489540",
    "end": "1497770"
  },
  {
    "text": "So the logistic\nregression model will try to predict the second\npixel, given the first one.",
    "start": "1497770",
    "end": "1503850"
  },
  {
    "text": "And you're going to\nget a number from this. And then you can sample from it. Then you have two pixels now\nthat you've chosen values for.",
    "start": "1503850",
    "end": "1515740"
  },
  {
    "text": "Then you can fit it to the\nnext logistic regression model. And you can keep generating\nthe image one pixel at a time.",
    "start": "1515740",
    "end": "1521440"
  },
  {
    "text": " So that's the recipe.",
    "start": "1521440",
    "end": "1527409"
  },
  {
    "text": "And it's good news because\nsampling is, to some extent,",
    "start": "1527410",
    "end": "1534580"
  },
  {
    "text": "easy. I mean, it's not\ngreat because you have to sequentially go\nthrough every random variable",
    "start": "1534580",
    "end": "1539950"
  },
  {
    "text": "that you're working with. But it's better\nthan alternatives like having to run it using\na Markov chain Monte Carlo",
    "start": "1539950",
    "end": "1547180"
  },
  {
    "text": "methods or other more\ncomplicated techniques that we might have to resort\nto for other classes of models.",
    "start": "1547180",
    "end": "1554110"
  },
  {
    "text": "The good news is that\nfor these kind of models, sampling is relatively easy.",
    "start": "1554110",
    "end": "1559330"
  },
  {
    "text": " Conditional sampling\nmight not be. So if you wanted to sample\npixel values based on--",
    "start": "1559330",
    "end": "1570440"
  },
  {
    "text": "you know, if wanted to do\ninpainting because you already have a piece of the image,\nyou want to generate the rest.",
    "start": "1570440",
    "end": "1576740"
  },
  {
    "text": "Depending on what you know about\nthe image, it might be easier or it might be hard.",
    "start": "1576740",
    "end": "1581810"
  },
  {
    "text": "So it's not straightforward. The fact that you can\ndo this efficiently is a nice benefit of\nthese type of models.",
    "start": "1581810",
    "end": "1587620"
  },
  {
    "text": " OK. Now, how many\nparameters do we have?",
    "start": "1587620",
    "end": "1593735"
  },
  {
    "text": " So if we have a bunch\nof alpha vectors, these alpha vectors\nhave different lengths",
    "start": "1593735",
    "end": "1600519"
  },
  {
    "text": "because they are logistic\nregression models of different sizes basically. Any guess?",
    "start": "1600520",
    "end": "1605905"
  },
  {
    "text": " For this model\nbasically that's say",
    "start": "1605905",
    "end": "1612430"
  },
  {
    "text": "two parameters and this one\nwe have 3 and then 4 and 5",
    "start": "1612430",
    "end": "1617530"
  },
  {
    "text": "It's n-squared. Yeah. [INAUDIBLE] It's n-squared, 1 plus\nroughly n-squared.",
    "start": "1617530",
    "end": "1622870"
  },
  {
    "text": "Right. So potentially not great,\nbut maybe manageable.",
    "start": "1622870",
    "end": "1628659"
  },
  {
    "text": " Cool. Now, as I kind of\nmentioned before,",
    "start": "1628660",
    "end": "1635320"
  },
  {
    "text": "this doesn't actually\nwork particularly well. So now I don't have\nthe results on MNIST.",
    "start": "1635320",
    "end": "1641260"
  },
  {
    "text": "But if you train it on this\ndata set of the Caltech 101, so the samples are on the left.",
    "start": "1641260",
    "end": "1648920"
  },
  {
    "text": "And you can see that\nthey kind of have shapes. Like, there is like\nobjects of different types.",
    "start": "1648920",
    "end": "1657080"
  },
  {
    "text": "And then you can train\nthis simple model based on logistic\nregression classifiers.",
    "start": "1657080",
    "end": "1663380"
  },
  {
    "text": "Then you can sample from it. And you get these kind of blobs. So not great.",
    "start": "1663380",
    "end": "1670790"
  },
  {
    "text": "And the reason is that basically\nthe logistic regression model is not sufficiently\npowerful to describe",
    "start": "1670790",
    "end": "1677300"
  },
  {
    "text": "these potentially relatively\ncomplicated dependencies that you have on\nthe pixel values.",
    "start": "1677300",
    "end": "1682685"
  },
  {
    "text": " So how can we make\nthings more better?",
    "start": "1682685",
    "end": "1689040"
  },
  {
    "text": "Let's use a deeper\nneural network, right? That's the natural thing to do.",
    "start": "1689040",
    "end": "1697029"
  },
  {
    "text": "And if you do that,\nyou get a model that is called NADE, neural\nautoregressive density estimation.",
    "start": "1697030",
    "end": "1702540"
  },
  {
    "text": "And the simplest\nthing you can do is to just use a\nsingle layer neural network to replace the\nlogistic regression classifier.",
    "start": "1702540",
    "end": "1711840"
  },
  {
    "text": "And so what would it look like? Basically, what you do is for\nevery index i, for every pixel,",
    "start": "1711840",
    "end": "1721460"
  },
  {
    "text": "you take all the\nprevious pixel values and you pass them through\nfirst a linear layer, then",
    "start": "1721460",
    "end": "1727759"
  },
  {
    "text": "some non-linearity. And then you pass\nthe non-linearity.",
    "start": "1727760",
    "end": "1736070"
  },
  {
    "text": "What you get, these features,\nthese hidden vectors that you get through a logistic\nregression final output",
    "start": "1736070",
    "end": "1742400"
  },
  {
    "text": "layer, that would give you the\nparameters of this Bernoulli random variable.",
    "start": "1742400",
    "end": "1747570"
  },
  {
    "text": "So it will tell\nyou whether or not what is the probability that\ni-th pixel is on or off.",
    "start": "1747570",
    "end": "1754100"
  },
  {
    "text": "And as you can see now, we have\na slightly more flexible model. Because you don't\njust have the alphas,",
    "start": "1754100",
    "end": "1761059"
  },
  {
    "text": "the parameters of the\nlogistic regression classifier of the final\nlayer of the network. But now you also\nhave the first layer.",
    "start": "1761060",
    "end": "1767600"
  },
  {
    "text": "So you have a slightly\nmore flexible model. ",
    "start": "1767600",
    "end": "1773970"
  },
  {
    "text": "And so it would look\nsomething like this. And again, the issue\nhere is that if there",
    "start": "1773970",
    "end": "1785447"
  },
  {
    "text": "are n random variables,\nyou have n separate kind of classification problems. So in general, you\ncould use completely",
    "start": "1785447",
    "end": "1792990"
  },
  {
    "text": "sort of like decoupled models. And so the first\nmodel would have, let's say, a single input x1.",
    "start": "1792990",
    "end": "1800970"
  },
  {
    "text": "And so the shape of this matrix\nwould be just a column vector, basically. And then if you have\ntwo inputs, x1 and x2,",
    "start": "1800970",
    "end": "1808530"
  },
  {
    "text": "to predict the third\npixel, then this matrix would have two columns\nessentially and so forth.",
    "start": "1808530",
    "end": "1816330"
  },
  {
    "text": "And, yeah? Why do we have two sigmoids? Like, why do we have a sigmoid\nover h and then sigmoid over--",
    "start": "1816330",
    "end": "1822538"
  },
  {
    "text": "the second sigmoid makes sense. But why do we have\na sigmoid for h? I don't think you necessarily\nhave it to have it.",
    "start": "1822538",
    "end": "1828909"
  },
  {
    "text": "It's just here I'm having an\nextra non-linearity there. But you don't\nnecessarily need it. Yeah?",
    "start": "1828910",
    "end": "1834460"
  },
  {
    "text": "If you don't have that\nsigmoid, wouldn't it just-- It'd just be another\nlinear layer, yeah. Yeah.",
    "start": "1834460",
    "end": "1839510"
  },
  {
    "text": "Well, it is gonna be\nthe same [INAUDIBLE].. Yeah. So it's better to have\na non-linearity, yeah. But this is just for\nillustration purposes.",
    "start": "1839510",
    "end": "1845733"
  },
  {
    "text": "You can imagine\ndifferent architectures. It doesn't have to be a sigmoid. It could be a ReLU. It could be other things.",
    "start": "1845733",
    "end": "1850940"
  },
  {
    "text": "It's just-- yeah? So over here, you have\nthree rows in your A matrix. Are we trying to predict three\nseparate features for our y?",
    "start": "1850940",
    "end": "1859780"
  },
  {
    "text": "I thought it was\njust one probability. ",
    "start": "1859780",
    "end": "1866370"
  },
  {
    "text": "Oh, I see what you mean. So this is just\nbasically a hidden vector h, which could have, it's\nnot necessarily a scalar.",
    "start": "1866370",
    "end": "1875960"
  },
  {
    "text": "That hidden vector is then\npassed to a logistic regression classifier. And so it's then mapped down to\na scalar through this expression",
    "start": "1875960",
    "end": "1884300"
  },
  {
    "text": "here, which might be, so\nthere's a dot product there. ",
    "start": "1884300",
    "end": "1893669"
  },
  {
    "text": "Right. And so this in\nprinciple all works. But you can kind of see the\nissue is that we're separately",
    "start": "1893670",
    "end": "1901770"
  },
  {
    "text": "training different models\nfor every pixel, which doesn't seem great. Perhaps there is some\ncommon structure.",
    "start": "1901770",
    "end": "1907290"
  },
  {
    "text": "At the end of the\nday, we're kind of like solving related problems. We're kind of trying to\npredict a pixel, given part",
    "start": "1907290",
    "end": "1913440"
  },
  {
    "text": "of an image, given the\nprevious part of the image. And so there might\nbe an opportunity for doing something\nslightly better",
    "start": "1913440",
    "end": "1920130"
  },
  {
    "text": "by tying the weights to reduce\nthe number of parameters and, as a byproduct,\nspeed up the computation.",
    "start": "1920130",
    "end": "1929470"
  },
  {
    "text": "And so what you can do here is\nyou can basically tie together",
    "start": "1929470",
    "end": "1935159"
  },
  {
    "text": "all these matrices, A2,\nA3, A4, that you would",
    "start": "1935160",
    "end": "1940320"
  },
  {
    "text": "have if you were to think of\nthem as separate classification problems. What you can do is you\ncan basically just have",
    "start": "1940320",
    "end": "1947820"
  },
  {
    "text": "a single matrix and\nthen you kind of tie together all\nthe weights that you",
    "start": "1947820",
    "end": "1956538"
  },
  {
    "text": "use in the prediction problems. We're basically selecting\nthe corresponding slice of some bigger matrix.",
    "start": "1956538",
    "end": "1962605"
  },
  {
    "text": "All right, so before we\nhad this the first matrix that we used to call A2\nand then A3 and then A4.",
    "start": "1962605",
    "end": "1970060"
  },
  {
    "text": "And they were\ncompletely decoupled. You could choose any values\nyou want for the [? entries ?]",
    "start": "1970060",
    "end": "1975340"
  },
  {
    "text": "of those matrices. What you can do here\nis you can basically choose the first column to\ntake some set of values.",
    "start": "1975340",
    "end": "1984190"
  },
  {
    "text": "And then you're gonna use that\nfor all the subsequent kind of like classification problems.",
    "start": "1984190",
    "end": "1990260"
  },
  {
    "text": "So you're equivalently\nkind of trying to extract the same features about x1.",
    "start": "1990260",
    "end": "1997120"
  },
  {
    "text": "And then you're kind\nof going to use them throughout all the\nclassification problems that you have when you're\ntrying to model the full image.",
    "start": "1997120",
    "end": "2009000"
  },
  {
    "text": "Yeah? Is reducing overfitting\nalso a motivation for this?",
    "start": "2009000",
    "end": "2014820"
  },
  {
    "text": "Yeah. So the question is overfitting\nalso potentially a concern?",
    "start": "2014820",
    "end": "2020035"
  },
  {
    "text": "Yeah, reducing the\nnumber of parameters is also good for\noverfitting issues.",
    "start": "2020035",
    "end": "2026040"
  },
  {
    "text": "Tying together the\nclassification problems might be good. You might learn\na better solution",
    "start": "2026040",
    "end": "2031740"
  },
  {
    "text": "that generalizes better. And as we'll see, it\nalso makes it faster. I'm curious, like, empirically,\nif it makes more sense",
    "start": "2031740",
    "end": "2038820"
  },
  {
    "text": "to invert your Xs. You're saying, like, you always\n[? depend ?] the same way based off the last thing\nyou predict instead of,",
    "start": "2038820",
    "end": "2045330"
  },
  {
    "text": "like, saying the n-th term\nshould have the same weight for x1, for example.",
    "start": "2045330",
    "end": "2051899"
  },
  {
    "text": "What's the suggestion? Sorry, I didn't quite-- I guess over here, we're always\nmultiplying the first w1x1w1x1",
    "start": "2051900",
    "end": "2059340"
  },
  {
    "text": "for every single xi\nwe're predicting. Instead of that, would\nit make more sense to invert your Xs\nso that W1 looks",
    "start": "2059340",
    "end": "2067169"
  },
  {
    "text": "at Xi minus 1, W2 looks at Xi\nminus 2, and so on and so forth?",
    "start": "2067170",
    "end": "2073190"
  },
  {
    "text": "So you're just looking at one\npreceding entry, two preceding entries, and so on.",
    "start": "2073190",
    "end": "2078798"
  },
  {
    "text": "Oh, that could also work. Yeah, that's a different\nkind of parameterization. That is more like a\nconvolutional kind of thing,",
    "start": "2078798",
    "end": "2084850"
  },
  {
    "text": "I would say. We're gonna talk\nabout that, too. This is what they did in\nthis particular model.",
    "start": "2084850",
    "end": "2090888"
  },
  {
    "text": "A question about notation. What is the w-dot comma\nsmaller than [INAUDIBLE]..",
    "start": "2090889",
    "end": "2096790"
  },
  {
    "text": "What is the dot? It's just the matrix. Yeah.",
    "start": "2096790",
    "end": "2104200"
  },
  {
    "text": "I don't think you probably\ndidn't need the dot. Or I guess it means the\npiece of a bigger matrix.",
    "start": "2104200",
    "end": "2109775"
  },
  {
    "text": "I think that was the\nintended notation. But yeah, you get\nthe idea sort of. ",
    "start": "2109775",
    "end": "2116810"
  },
  {
    "text": "And the good news is\nthat this can reduce the number of parameters. So if you have a size d\nfor this hidden vector h",
    "start": "2116810",
    "end": "2129760"
  },
  {
    "text": "that you're using to\nmake the predictions, how many parameters do you need?",
    "start": "2129760",
    "end": "2135234"
  },
  {
    "text": " [INAUDIBLE] n squared\nover 2 times t?",
    "start": "2135235",
    "end": "2142840"
  },
  {
    "text": "It's no longer quadratic in n. That's the kind of big takeaway.",
    "start": "2142840",
    "end": "2149710"
  },
  {
    "text": "Before we had something\nthat was quadratic in n. Now it's basically linear. Because there's a single\nmatrix that you have to store.",
    "start": "2149710",
    "end": "2156670"
  },
  {
    "text": "And then you can\nreuse it all the time. All right. ",
    "start": "2156670",
    "end": "2163470"
  },
  {
    "text": "So that's good.  Now, the other advantage that\nyou have with this kind of model",
    "start": "2163470",
    "end": "2171810"
  },
  {
    "text": "is that you can evaluate\nprobabilities more efficiently. Because basically,\nremember if you",
    "start": "2171810",
    "end": "2178710"
  },
  {
    "text": "want to evaluate the\nprobability of a data point, you have to evaluate\nall these conditionals. So you have to go\nthrough every conditional",
    "start": "2178710",
    "end": "2185790"
  },
  {
    "text": "and you basically\nhave to evaluate this kind of computation. If there is no structure\non the matrices, then you have to redo the\ncomputation because there",
    "start": "2185790",
    "end": "2192720"
  },
  {
    "text": "is nothing shared. But if you have some\nshared structure, then you can reuse\nthe computation.",
    "start": "2192720",
    "end": "2199240"
  },
  {
    "text": "So if you've already\ncomputed this dot product, this product here, this\nmatrix vector product here,",
    "start": "2199240",
    "end": "2205090"
  },
  {
    "text": "and then if you are\nadding an extra column,",
    "start": "2205090",
    "end": "2210930"
  },
  {
    "text": "then you can reuse\nthe computation that you've done before. You can just add in\nan extra [? column. ?]",
    "start": "2210930",
    "end": "2216990"
  },
  {
    "text": "Is the bias vector c also shared\namongst all the hidden layers? Yeah, I guess it could be.",
    "start": "2216990",
    "end": "2222600"
  },
  {
    "text": "Or it doesn't have to be. I think you could\nmake it either way. I think I actually forgot\nto because it didn't fit.",
    "start": "2222600",
    "end": "2227880"
  },
  {
    "text": "But, yeah, there should be a c. And you could change it, yeah.",
    "start": "2227880",
    "end": "2233430"
  },
  {
    "text": "The weight columns are\nupdated in each step. So like in the fourth\nstep here, it also",
    "start": "2233430",
    "end": "2238890"
  },
  {
    "text": "updates the first\nand second column. What do you mean update?",
    "start": "2238890",
    "end": "2244680"
  },
  {
    "text": "Like, my understanding was\nwith the weight matrix, you basically build\nit column by column.",
    "start": "2244680",
    "end": "2251040"
  },
  {
    "text": "But you try to learn by\noverseeing, like, many examples. So I was wondering, like,\nas the model learns,",
    "start": "2251040",
    "end": "2257910"
  },
  {
    "text": "it basically updates\nalso in every step all the previous columns that\nit has learned, right? Yeah, yeah, yeah.",
    "start": "2257910",
    "end": "2263068"
  },
  {
    "text": "So it's all tied together. And then we haven't\ntalked about how you would do learning, but yeah. So then you can see that\nthe first column matters",
    "start": "2263068",
    "end": "2270780"
  },
  {
    "text": "for all the prediction tasks. So you would be\nable to learn it. You would get some signal\nfrom every learning problem.",
    "start": "2270780",
    "end": "2278530"
  },
  {
    "text": "Yeah? Yeah, just to clarify,\nin this model, you are sharing weights, right?",
    "start": "2278530",
    "end": "2284510"
  },
  {
    "text": "Yeah. And does that imply\nany assumptions you're making about the\ndata you're looking at?",
    "start": "2284510",
    "end": "2291970"
  },
  {
    "text": "It's an assumption. Again, you're kind of saying\nthat these conditional probability tables\ncould be arbitrary,",
    "start": "2291970",
    "end": "2297850"
  },
  {
    "text": "somehow can be captured\nby prediction models that have this sort of structure.",
    "start": "2297850",
    "end": "2303970"
  },
  {
    "text": "So somehow there is\nsome relationship between the way\nyou would predict one pixel, different\npixels in an image.",
    "start": "2303970",
    "end": "2311240"
  },
  {
    "text": "Whether or not\nit's reasonable, it becomes an empirical question. I think I have the results here.",
    "start": "2311240",
    "end": "2318460"
  },
  {
    "text": "And it tends to work\nsignificantly better than, let's say, the previous\nlogistic regression model.",
    "start": "2318460",
    "end": "2323750"
  },
  {
    "text": "So it does seem like\nthis kind of structure helps modeling natural images or\ntoy kind of images like MNIST.",
    "start": "2323750",
    "end": "2333160"
  },
  {
    "text": "So here you can\nsee some examples. You have MNIST binarized.",
    "start": "2333160",
    "end": "2338910"
  },
  {
    "text": "Or no actually, I don't\nhave the samples for MNIST. Here, what you have here is\nsamples from the model trained",
    "start": "2338910",
    "end": "2346050"
  },
  {
    "text": "on MNIST on the left and the\nconditional probabilities corresponding to these\nsamples on the right.",
    "start": "2346050",
    "end": "2353270"
  },
  {
    "text": "So remember that when\nyou generate samples autoregressively,\nyou actually get probabilities for each pixel,\ngiven the previous ones.",
    "start": "2353270",
    "end": "2361700"
  },
  {
    "text": "And then you sample from them\nto actually pick a value. And so the images on the\nleft are binary, 0 and 1.",
    "start": "2361700",
    "end": "2370990"
  },
  {
    "text": "The images on the right are kind\nof soft because for every pixel, you get a number between 0\nand 1 that you sample from",
    "start": "2370990",
    "end": "2377830"
  },
  {
    "text": "to generate a color. In this case, 0, 1. And so you can see\nthey look a little bit",
    "start": "2377830",
    "end": "2383200"
  },
  {
    "text": "better because they're\na little bit more soft. But you can see that it's doing\na reasonable job at capturing",
    "start": "2383200",
    "end": "2390849"
  },
  {
    "text": "the structure of these images. Why are the numbers on the\nright table look just almost",
    "start": "2390850",
    "end": "2400360"
  },
  {
    "text": "exactly like the left one? Why don't they just create\nsome variation, I mean, some other kind of variations?",
    "start": "2400360",
    "end": "2406510"
  },
  {
    "text": "So the numbers are corresponding\nto the samples that you see. So basically what\nthis is saying is",
    "start": "2406510",
    "end": "2412089"
  },
  {
    "text": "that what you would actually\ndo when you sample is you would take the first pixel.",
    "start": "2412090",
    "end": "2418670"
  },
  {
    "text": "You have a probability. And then you plot\nit on the right. Then you sample a value\nfrom that on the left.",
    "start": "2418670",
    "end": "2423740"
  },
  {
    "text": "Then based on that value, based\non the actual binary value, you come up with a probability\nfor the second pixel, which is",
    "start": "2423740",
    "end": "2431690"
  },
  {
    "text": "just a number between 0 and 1. You plot it on the right image. Then you sample from\nit and you keep going.",
    "start": "2431690",
    "end": "2439990"
  },
  {
    "text": "So the right table doesn't\ncome from [INAUDIBLE] what [? actually ?] learning? Yeah, it does, it does.",
    "start": "2439990",
    "end": "2445580"
  },
  {
    "text": "So it's basically these numbers,\nthe predicted probabilities for every pixel, which are the\nx-hat-i, so the probability",
    "start": "2445580",
    "end": "2452720"
  },
  {
    "text": "that pixel is on or off. And they are matching so\nthat's why they look the same.",
    "start": "2452720",
    "end": "2457920"
  },
  {
    "text": "Because the sample\nthat you see on the left is what\nyou get by sampling from those distributions.",
    "start": "2457920",
    "end": "2465860"
  },
  {
    "text": "Yeah? I am noticing that it\nis agnostic of what the label should be.",
    "start": "2465860",
    "end": "2472190"
  },
  {
    "text": "Is that the right call\nto make for generation? So the question is, should\nwe take advantage of the fact",
    "start": "2472190",
    "end": "2479960"
  },
  {
    "text": "that maybe we have\nlabels for the data set and so we know that there is\ndifferent types of digits, that there is maybe\n10 digits and then we",
    "start": "2479960",
    "end": "2486680"
  },
  {
    "text": "want to take advantage of that? So here I'm assuming\nthat we don't have access",
    "start": "2486680",
    "end": "2492710"
  },
  {
    "text": "to the label y. If you had access\nto the label y, you could imagine trying to\nlearn a joint distribution",
    "start": "2492710",
    "end": "2498920"
  },
  {
    "text": "between x and y. And perhaps you would\nget a better model. Or perhaps you can\nassume you don't",
    "start": "2498920",
    "end": "2505280"
  },
  {
    "text": "have that kind of structure. You just learn a model. And you can try to use the model\nto see whether it indeed figured",
    "start": "2505280",
    "end": "2513859"
  },
  {
    "text": "out that there are 10\nclusters of data points and that there's a\nbunch of data points that kind of have this shape\nthat look kind of like an oval.",
    "start": "2513860",
    "end": "2522170"
  },
  {
    "text": "And that's a 0. And that's the kind\nof third point of, how do you get features\nout of these models?",
    "start": "2522170",
    "end": "2528270"
  },
  {
    "text": "Like, presumably, if\nyou have a model that can generate digits that\nhave the right structure and it generates them in\nthe right proportions,",
    "start": "2528270",
    "end": "2535530"
  },
  {
    "text": "it has learned something about\nthe structure of the images and what they have in common. And so that was kind of like the\nthird point of getting features,",
    "start": "2535530",
    "end": "2542730"
  },
  {
    "text": "unsupervised learning. We'll talk about how to do that. But, yeah, there is\ntwo ways to see it.",
    "start": "2542730",
    "end": "2548920"
  },
  {
    "text": "You can either do\nit unsupervised. Or if you have access\nto the label, then perhaps you can include\nit into the model.",
    "start": "2548920",
    "end": "2554320"
  },
  {
    "text": "You can do\nconditional generation or you can jointly learn a\ndistribution over x and y.",
    "start": "2554320",
    "end": "2560349"
  },
  {
    "text": "Yeah? So in this case,\nwhen you sample, you can get any one\nof the 10 digits?",
    "start": "2560350",
    "end": "2565530"
  },
  {
    "text": "Well, if the model\ndoes well, yes. For example, to check whether\nthe model is doing a good job, you could try to see\nwhat is the proportion.",
    "start": "2565530",
    "end": "2572440"
  },
  {
    "text": "Like, if in the original\ntraining set, all the images, you see an equal proportion\nof the different digits,",
    "start": "2572440",
    "end": "2580060"
  },
  {
    "text": "then you now apply an MNIST\nclassifier to your samples and you can see does it generate\ndigits in the right proportion?",
    "start": "2580060",
    "end": "2587349"
  },
  {
    "text": "If it doesn't, then\nthere's probably something wrong with the model. If it does, it's\ndoing something right.",
    "start": "2587350",
    "end": "2592839"
  },
  {
    "text": "Whether it's correct or\nnot, it's hard to say. So, like, here it\nseems like you're",
    "start": "2592840",
    "end": "2597890"
  },
  {
    "text": "injecting the stronger\n[? prior ?] into the model. So if you had an\ninfinite data set, would you expect the\noriginal approach",
    "start": "2597890",
    "end": "2603680"
  },
  {
    "text": "to perform better than this one? The original meaning the? [? That ?] one's got structure\nimposed by us, right?",
    "start": "2603680",
    "end": "2610460"
  },
  {
    "text": "So the representation\nit should learn should theoretically be richer? That one is actually\nmore structured.",
    "start": "2610460",
    "end": "2617688"
  },
  {
    "text": "Like, you have less parameters. It's less flexible. If you had infinite data\nand infinite compute, the best thing would be\nconditional probability tables,",
    "start": "2617688",
    "end": "2625490"
  },
  {
    "text": "Bayesian network. That one would be\nable to, in principle, capture any relationship. With infinite data, you would\nbe able to learn that table.",
    "start": "2625490",
    "end": "2632600"
  },
  {
    "text": "That would be perfect. Modular overfitting, I mean,\nbut if you have infinite data, you don't have to worry\nabout that either.",
    "start": "2632600",
    "end": "2639830"
  },
  {
    "text": "I might have missed something. So on the left picture\nis the actual samples generated from the model.",
    "start": "2639830",
    "end": "2645740"
  },
  {
    "text": "The right is we somehow code\nthe conditional probabilities into a grayscale?",
    "start": "2645740",
    "end": "2650900"
  },
  {
    "text": "Yeah, just between 0 and 1. The conditional\nprobabilities would be numbers between 0\nand 1 and [INAUDIBLE].. So [? that's a ?] grayscale.",
    "start": "2650900",
    "end": "2656837"
  },
  {
    "text": "Yeah, it's just a grayscale. Cool. So that's the NADE.",
    "start": "2656837",
    "end": "2664220"
  },
  {
    "text": "Now, you might\nwonder, what do you do if you want to model\ncolor images, let's say?",
    "start": "2664220",
    "end": "2671869"
  },
  {
    "text": "So if the variables\nare no longer binary, but if they can take, let's\nsay, K different values, maybe",
    "start": "2671870",
    "end": "2679700"
  },
  {
    "text": "pixel intensities ranging from\n0 to 255, how do you do it?",
    "start": "2679700",
    "end": "2684740"
  },
  {
    "text": "Now, what you need to do is\nthe output of the model has to be a categorical\ndistribution over however",
    "start": "2684740",
    "end": "2690799"
  },
  {
    "text": "many different values the\nrandom variables can take. So you can basically\ndo the same thing.",
    "start": "2690800",
    "end": "2695930"
  },
  {
    "text": "You first get this kind\nof hidden vector or latent",
    "start": "2695930",
    "end": "2701690"
  },
  {
    "text": "representation h. And then instead of applying\nsome kind of mapping it down",
    "start": "2701690",
    "end": "2708560"
  },
  {
    "text": "to just the parameters of a\nBernoulli random variable, you can use some kind\nof softmax output layer",
    "start": "2708560",
    "end": "2714710"
  },
  {
    "text": "to map it down to a vector of-- if you have k different\noutputs that you care about,",
    "start": "2714710",
    "end": "2721549"
  },
  {
    "text": "a vector of k probabilities,\npi-1 through pi-k,",
    "start": "2721550",
    "end": "2730950"
  },
  {
    "text": "which basically would\nrepresent the probability that the i-th random variable\nshould take one of the K",
    "start": "2730950",
    "end": "2736349"
  },
  {
    "text": "different values that the\nrandom variable can take. And that's the\nnatural generalization",
    "start": "2736350",
    "end": "2742860"
  },
  {
    "text": "of the sigmoid\nfunction we had before. It's just one way to take\nK numbers, which are not",
    "start": "2742860",
    "end": "2748980"
  },
  {
    "text": "necessarily non-negative and\nthey might not be normalized, and it's just a way\nto normalize them",
    "start": "2748980",
    "end": "2753990"
  },
  {
    "text": "so that they become a valid\nprobability distribution. So specifically, you just\ndo something like this.",
    "start": "2753990",
    "end": "2761490"
  },
  {
    "text": "If you have a vector\nof arbitrary numbers, you apply the softmax operation. It produces another vector.",
    "start": "2761490",
    "end": "2767320"
  },
  {
    "text": "You apply an exponential\nto every component to make sure it's non-negative.",
    "start": "2767320",
    "end": "2773309"
  },
  {
    "text": "And then you divide by the sum\nof these exponentials, which is basically making sure that\nthe entries are normalized.",
    "start": "2773310",
    "end": "2780180"
  },
  {
    "text": "So if you sum the probabilities\nof all the possible things that can happen, you get 1.",
    "start": "2780180",
    "end": "2785310"
  },
  {
    "text": " And so natural generalization\nof what we had before.",
    "start": "2785310",
    "end": "2794590"
  },
  {
    "text": "Now, you might wonder\nwhat do you do if you want to model continuous data.",
    "start": "2794590",
    "end": "2800760"
  },
  {
    "text": "So maybe you're dealing with\nspeech and it's not very natural to discretize the--",
    "start": "2800760",
    "end": "2807980"
  },
  {
    "text": "I mean, even for\nimages perhaps you don't want to discretize\nthe random variables",
    "start": "2807980",
    "end": "2812990"
  },
  {
    "text": "and you want to model them as\ncontinuous random variables.",
    "start": "2812990",
    "end": "2818190"
  },
  {
    "text": "So the solution is\nto basically, again, use the same architecture.",
    "start": "2818190",
    "end": "2823260"
  },
  {
    "text": "But now the output\nof the neural network will be the parameters of\nsome continuous distribution.",
    "start": "2823260",
    "end": "2830728"
  },
  {
    "text": "So it's no longer the\nparameter of a Bernoulli or the parameters\nof a categorical. It could be the\nparameters of a Gaussian",
    "start": "2830728",
    "end": "2836800"
  },
  {
    "text": "or a logistic or some continuous\nprobability density function",
    "start": "2836800",
    "end": "2844580"
  },
  {
    "text": "that you think should work\nwell for your data set. And so, for example,\none thing you could do",
    "start": "2844580",
    "end": "2851869"
  },
  {
    "text": "is you could use a\nmixture of K Gaussians. ",
    "start": "2851870",
    "end": "2857310"
  },
  {
    "text": "So what you have\nto do is you need to make sure that the output\nof your neural network",
    "start": "2857310",
    "end": "2862710"
  },
  {
    "text": "gives you the parameters of\nK different Gaussians, which",
    "start": "2862710",
    "end": "2867869"
  },
  {
    "text": "are then [? mixtured ?]\ntogether, let's say, uniformly\nto obtain a relatively",
    "start": "2867870",
    "end": "2873869"
  },
  {
    "text": "flexible kind of\nprobability density function like you see here,\nan example where",
    "start": "2873870",
    "end": "2879000"
  },
  {
    "text": "there is three Gaussians\nwith different means and different\nstandard deviations. Then you combine\nthem together and you",
    "start": "2879000",
    "end": "2885120"
  },
  {
    "text": "get a nice kind of red curve\nwhere you kind of allowed to move the probability mass.",
    "start": "2885120",
    "end": "2891210"
  },
  {
    "text": "And you are allowed\nto say maybe there is two different values that\nthe random variable can take,",
    "start": "2891210",
    "end": "2899760"
  },
  {
    "text": "two modes, one\nhere and one here. And you're allowed to move\nthe probability mass around by changing the mean and\nthe standard deviation",
    "start": "2899760",
    "end": "2906360"
  },
  {
    "text": "of the Gaussians. So in this case, would\nxi have two K values?",
    "start": "2906360",
    "end": "2911670"
  },
  {
    "text": "Yeah. Yeah. So I think I have, the\nmore precise thing here.",
    "start": "2911670",
    "end": "2917120"
  },
  {
    "text": "So you would say the\nconditional probability of xi, given all the\nprevious values, is",
    "start": "2917120",
    "end": "2922660"
  },
  {
    "text": "a mixture of K Gaussians,\neach one of them having a different mean and a\ndifferent standard deviation.",
    "start": "2922660",
    "end": "2930220"
  },
  {
    "text": "And as usual, you have to\nbasically use the neural network to get the parameters\nof these distributions.",
    "start": "2930220",
    "end": "2937790"
  },
  {
    "text": "So in this case,\nas was suggested, you could use the same trick. And then as an output\nlayer, you can no longer",
    "start": "2937790",
    "end": "2944200"
  },
  {
    "text": "use a softmax or a sigmoid. You have to use\nsomething else that gives you the parameters\nof these random variables.",
    "start": "2944200",
    "end": "2950359"
  },
  {
    "text": "And so you need\ntwo K numbers, you need K means and you need\nK standard deviations.",
    "start": "2950360",
    "end": "2956940"
  },
  {
    "text": "And I guess, you need\nto be careful about,",
    "start": "2956940",
    "end": "2963442"
  },
  {
    "text": "depending on how\nyou parameterize, like if you\nparameterize a variance, then it has to be non-negative. But that's relatively\neasy to enforce.",
    "start": "2963443",
    "end": "2969944"
  },
  {
    "text": " OK. Now, as a way to get a\ndeeper understanding of what",
    "start": "2969945",
    "end": "2982610"
  },
  {
    "text": "these kind of\nmodels do, you might notice that they look a\nlot like autoencoders.",
    "start": "2982610",
    "end": "2989450"
  },
  {
    "text": "Like, if you look at this\nkind of computation graph that I have here where you\nhave the data point x1,",
    "start": "2989450",
    "end": "2998210"
  },
  {
    "text": "x2, x3, and x4 that is\nbeing mapped to this predicted probability,\nx1-hat, x2-hat, x3-hat,",
    "start": "2998210",
    "end": "3006110"
  },
  {
    "text": "and so forth, it kind of looks a\nlittle bit like an autoencoder,",
    "start": "3006110",
    "end": "3011620"
  },
  {
    "text": "where you take your\ninput x and then you map it to some kind of\npredicted reconstruction",
    "start": "3011620",
    "end": "3019869"
  },
  {
    "text": "of the input. And more specifically,\nan autoencoder",
    "start": "3019870",
    "end": "3026050"
  },
  {
    "text": "is just a model that\nis often used again in unsupervised learning. It has two components.",
    "start": "3026050",
    "end": "3032000"
  },
  {
    "text": "An encoder takes a\ndata point and maps it to some kind of\nlatent representation.",
    "start": "3032000",
    "end": "3037610"
  },
  {
    "text": "And then, for\nexample, it could be, again, a simple neural network,\na two-layer net like this.",
    "start": "3037610",
    "end": "3045000"
  },
  {
    "text": "And then there is\na decoder whose job is to try to invert\nthis transformation.",
    "start": "3045000",
    "end": "3050010"
  },
  {
    "text": "And the job of the decoder is to\ntake the output of the encoder and map it back to the\noriginal data point.",
    "start": "3050010",
    "end": "3057200"
  },
  {
    "text": "And in this case in this\ngraph that I have here, it could be another\nneural network",
    "start": "3057200",
    "end": "3063020"
  },
  {
    "text": "that takes the\noutput of the encoder and maps it back to some\nreconstruction of the input.",
    "start": "3063020",
    "end": "3068720"
  },
  {
    "text": " And the loss function\nthat you would use",
    "start": "3068720",
    "end": "3075930"
  },
  {
    "text": "would be some kind of\nreconstruction loss. So you would try to train\nthe encoder and the decoder",
    "start": "3075930",
    "end": "3082300"
  },
  {
    "text": "so that for every\ndata point, when",
    "start": "3082300",
    "end": "3087690"
  },
  {
    "text": "you apply the decoder\nto the encoder, you get back something close\nto the original data point. So depending on whether the\ndata is discrete or continuous,",
    "start": "3087690",
    "end": "3095609"
  },
  {
    "text": "this could be something\nlike a square loss where you try to make sure\nthat at every coordinate,",
    "start": "3095610",
    "end": "3101640"
  },
  {
    "text": "your reconstructed i-th variable\nis close to the original one.",
    "start": "3101640",
    "end": "3106950"
  },
  {
    "text": "If you have discrete\ndata, it's more like, is the model doing a good\njob at predicting the value",
    "start": "3106950",
    "end": "3114420"
  },
  {
    "text": "for the i-th-- let's say, in this\ncase, it's binary here-- for the i-th random variable\nthat I'm actually observing.",
    "start": "3114420",
    "end": "3121710"
  },
  {
    "text": "So if the i-th random\nvariable is true, is 1, is the model giving me a high\nprobability for the value 1.",
    "start": "3121710",
    "end": "3132539"
  },
  {
    "text": "Not super important,\nbut kind of like this is how you\nwould try to learn the decoder and the\nencoder so that they",
    "start": "3132540",
    "end": "3138810"
  },
  {
    "text": "satisfy this condition. And of course, there\nis a trivial solution. There is the identity mapping.",
    "start": "3138810",
    "end": "3143950"
  },
  {
    "text": "So if the encoder is\njust an identity function and the decoder is\nsome identity function,",
    "start": "3143950",
    "end": "3149250"
  },
  {
    "text": "then you do very well at this. And it's not what\nyou want typically. So typically, you would\nconstrain the architecture",
    "start": "3149250",
    "end": "3157440"
  },
  {
    "text": "somehow so that it cannot\nlearn an identity function. But that has kind\nof like the flavor",
    "start": "3157440",
    "end": "3164700"
  },
  {
    "text": "of what we're doing with these\nsort of autoregressive models. We're taking the\ndata point and then",
    "start": "3164700",
    "end": "3172230"
  },
  {
    "text": "we're trying to use\nparts of the data point to reconstruct itself.",
    "start": "3172230",
    "end": "3177900"
  },
  {
    "text": "Or we feed it through\nthese networks and then we output\nthese predicted values. And if you were\nto think about how",
    "start": "3177900",
    "end": "3184427"
  },
  {
    "text": "you would train one of\nthese models by, let's say, maximum likelihood,\nyou would get losses that are very similar to this.",
    "start": "3184427",
    "end": "3192060"
  },
  {
    "text": "If you were to train these\nlogistic regression classifiers, you would get something very\nsimilar to this, where you would",
    "start": "3192060",
    "end": "3198410"
  },
  {
    "text": "try to predict the\nvalue that you actually see in the data point. I'm just trying to understand\nthe encoder-decoder kind",
    "start": "3198410",
    "end": "3206040"
  },
  {
    "text": "of mechanism. Is it the main point of the\nencoder is just to compress",
    "start": "3206040",
    "end": "3212970"
  },
  {
    "text": "all the previous\n[? informations ?] into a very low dimensional\nkind of like vector? Is that the main--",
    "start": "3212970",
    "end": "3218340"
  },
  {
    "text": "Yeah, yeah. So the question is, what\nare autoencoders used for? Yes, one typical\nuse case would be",
    "start": "3218340",
    "end": "3225810"
  },
  {
    "text": "to learn a compressed\nrepresentation of the data. Somehow if you can\ndo this, maybe you",
    "start": "3225810",
    "end": "3233190"
  },
  {
    "text": "force the output dimension\nof the encoder to be small. And then in order to do a\ngood job at reconstruction,",
    "start": "3233190",
    "end": "3240720"
  },
  {
    "text": "it has to capture the\nkey factors of variation in the data. And so you can kind think of it\nas some sort of nonlinear PCA",
    "start": "3240720",
    "end": "3248400"
  },
  {
    "text": "kind of thing that will\ntry to discover structure in the data in an\nunsupervised way.",
    "start": "3248400",
    "end": "3255839"
  },
  {
    "text": "Yeah? Right, can we do sampling\nwhen we train the autoencoder?",
    "start": "3255840",
    "end": "3261047"
  },
  {
    "text": "The question is, can we do\nsampling with an autoencoder? No, an autoencoder is not\nquite a generative model. So these two things\nare not quite the same.",
    "start": "3261047",
    "end": "3267819"
  },
  {
    "text": "But they are related. And that's why we're\ngoing to see next. So yeah, this was coming up.",
    "start": "3267820",
    "end": "3274000"
  },
  {
    "text": "Typically, you would train this\nto do representation learning, try to find good\nrepresentations.",
    "start": "3274000",
    "end": "3279430"
  },
  {
    "text": " If you think about\nwhat we just said,",
    "start": "3279430",
    "end": "3286400"
  },
  {
    "text": "if you have an autoencoder, it's\nnot really a generative model.",
    "start": "3286400",
    "end": "3291460"
  },
  {
    "text": "Like, how do you generate\ndata from an autoencoder? [? Wouldn't ?] you just\nremove the encoder and just",
    "start": "3291460",
    "end": "3296975"
  },
  {
    "text": "use the decoder? But what's the input to the-- so the suggestion is, OK,\nlet's throw away the encoder.",
    "start": "3296976",
    "end": "3302950"
  },
  {
    "text": "Let's just use the decoder. What do you feed into the\ndecoder to generate data? Let's just handcraft the e of x.",
    "start": "3302950",
    "end": "3310690"
  },
  {
    "text": "Yeah, that's the solution\nfor a variational autoencoder actually. So the variational\nautoencoder will be, let's try to learn a simple\ngenerative model to feed inputs,",
    "start": "3310690",
    "end": "3319930"
  },
  {
    "text": "fake inputs to your decoder. And so you can kind\nof fake the process",
    "start": "3319930",
    "end": "3326350"
  },
  {
    "text": "and you can use it to generate. So that's the variational\nautoencoder solution that we'll talk about later.",
    "start": "3326350",
    "end": "3332390"
  },
  {
    "text": "But there's not an obvious\nway to generate the inputs to the decoder,\nunless you have data.",
    "start": "3332390",
    "end": "3337610"
  },
  {
    "text": "But at that point, you're\nnot really sampling, right? Could you like feed in a\nlatent variable that you can",
    "start": "3337610",
    "end": "3344650"
  },
  {
    "text": "[INAUDIBLE]. Yeah, that's the [INAUDIBLE]\nsolution, basically, that we'll talk about, yeah. What if you, like, had a\nregularization term that",
    "start": "3344650",
    "end": "3351790"
  },
  {
    "text": "forces your hidden\nrepresentation to just look like a Gaussian or\nsomething like that?",
    "start": "3351790",
    "end": "3356890"
  },
  {
    "text": "Yes. So again that's the\nsolution imposed by the-- that's basically\na variational autoencoder. Literally, a\nvariational autoencoder",
    "start": "3356890",
    "end": "3363130"
  },
  {
    "text": "is this plus what you\nsuggested, forcing the latent representations to be\ndistributed according",
    "start": "3363130",
    "end": "3369400"
  },
  {
    "text": "to a simple\ndistribution, a Gaussian. And if that happens\nto work well, then you can sample\nfrom that distribution,",
    "start": "3369400",
    "end": "3374612"
  },
  {
    "text": "feed the inputs to the\ndecoder, and that works. But that requires a different\nkind of regularization.",
    "start": "3374612",
    "end": "3382300"
  },
  {
    "text": "The relationship here is that\nalthough these two things look similar, it's not\nquite the same.",
    "start": "3382300",
    "end": "3389120"
  },
  {
    "text": "And the reason is that we cannot\nget a generative model from",
    "start": "3389120",
    "end": "3394960"
  },
  {
    "text": "an autoencoder because somehow\nwe're not putting enough structure on this kind\nof computation graph.",
    "start": "3394960",
    "end": "3401440"
  },
  {
    "text": "And there is not an ordering. Remember that to get an\nautoregressive model, we need an ordering. We need chain rule.",
    "start": "3401440",
    "end": "3407850"
  },
  {
    "text": "So one way to actually get or\nto connect these two things is to enforce an ordering\non the autoencoder.",
    "start": "3407850",
    "end": "3416330"
  },
  {
    "text": "And if you do that,\nyou get back basically an autoregressive model.",
    "start": "3416330",
    "end": "3421520"
  },
  {
    "text": "And so basically, if you are\nwilling to put constraints on the weight matrices\nof these neural networks",
    "start": "3421520",
    "end": "3431100"
  },
  {
    "text": "so that there is a corresponding\nbasically Bayesian network or chain rule\nfactorization, then you",
    "start": "3431100",
    "end": "3440150"
  },
  {
    "text": "can actually get an\nautoregressive model from an autoencoder. And the idea is that basically\nif you think about it,",
    "start": "3440150",
    "end": "3448260"
  },
  {
    "text": "the issue is that we don't know\nwhat to feed to the decoder. So somehow we need a way to\ngenerate the data sequentially",
    "start": "3448260",
    "end": "3456589"
  },
  {
    "text": "to feed it into this decoder\nthat we have access to. And so one way to do it is to\nset up the computation graph",
    "start": "3456590",
    "end": "3465720"
  },
  {
    "text": "so that the first reconstructed\nrandom variable does not depend on any of the inputs.",
    "start": "3465720",
    "end": "3472505"
  },
  {
    "text": " If that's the case,\nthen you can come up",
    "start": "3472505",
    "end": "3477800"
  },
  {
    "text": "with the first output\nof this decoder yourself because you don't need any\nparticular input to do that.",
    "start": "3477800",
    "end": "3485150"
  },
  {
    "text": "And then you can feed your\npredicted first random variable",
    "start": "3485150",
    "end": "3490490"
  },
  {
    "text": "into then, let's say\nat the generation time, then you don't need it. Now, if the predicted value\nfor the second random variable",
    "start": "3490490",
    "end": "3503060"
  },
  {
    "text": "depends on x1, that's\nfine because we can make up a value for x1.",
    "start": "3503060",
    "end": "3510020"
  },
  {
    "text": "Then we can fit it\ninto the computation and we can predict\na value for x2. Then we can think of this value,\nwe can take the first two,",
    "start": "3510020",
    "end": "3517880"
  },
  {
    "text": "feed them into the\nautoencoder kind of thing, and predict a value for x3.",
    "start": "3517880",
    "end": "3522950"
  },
  {
    "text": "And we can keep going. And it's the same thing as\nan autoregressive model.",
    "start": "3522950",
    "end": "3528480"
  },
  {
    "text": "So if you look at this\nkind of computation graph, you can see that the\npredicted value for x1",
    "start": "3528480",
    "end": "3533660"
  },
  {
    "text": "depends on all the\ninputs, in general.",
    "start": "3533660",
    "end": "3539000"
  },
  {
    "text": "And so if you look\nat the arrows, all the inputs have an effect\non the first predicted value.",
    "start": "3539000",
    "end": "3547170"
  },
  {
    "text": "And so that's a problem because\nwe cannot get an autoregressive model if we do it that way. But if we somehow mask the\nweights in the right way,",
    "start": "3547170",
    "end": "3555160"
  },
  {
    "text": "we can get an\nautoregressive model. And then as a bonus, then we\nhave a single neural network",
    "start": "3555160",
    "end": "3561580"
  },
  {
    "text": "that does the whole thing. So it's not like before that\nwe had different classification",
    "start": "3561580",
    "end": "3569950"
  },
  {
    "text": "models or that they were\ntied together somehow. If we can do this, then\nit's a single neural network",
    "start": "3569950",
    "end": "3578440"
  },
  {
    "text": "that in a single forward pass\ncan produce all the parameters that we need. I was wondering in\nsome tasks, for example",
    "start": "3578440",
    "end": "3585190"
  },
  {
    "text": "this digit task we\nearlier discussed, is there not a risk of the model\nlearning just how to shift it",
    "start": "3585190",
    "end": "3592060"
  },
  {
    "text": "by like one pixel or something? Not obvious that you can\njust shift because, I mean,",
    "start": "3592060",
    "end": "3599800"
  },
  {
    "text": "you cannot cheat, right? So you cannot look\nat the next pixel. You have to pick an ordering\nand you have to predict.",
    "start": "3599800",
    "end": "3607329"
  },
  {
    "text": "Yes, but you can\nbegin with the left and then begin putting the\npixels to the left of it,",
    "start": "3607330",
    "end": "3613420"
  },
  {
    "text": "for example. [INAUDIBLE],, you\nhaven't seen them. You haven't seen\nthe right pixels. So you don't know exactly\nwhat to copy, right?",
    "start": "3613420",
    "end": "3621440"
  },
  {
    "text": "No, you do because if before\norder goes left to right,",
    "start": "3621440",
    "end": "3626869"
  },
  {
    "text": "you will know what\nthe pixel in the input image to the left of it is.",
    "start": "3626870",
    "end": "3632000"
  },
  {
    "text": "So you can just put that. And you can draw a\nblack line on the left.",
    "start": "3632000",
    "end": "3637520"
  },
  {
    "text": "So like I was wondering if our\nmetric needs to really prevent",
    "start": "3637520",
    "end": "3642550"
  },
  {
    "text": "that from happening? No, you don't need to\nprevent that from happening. And partially, it's because\nthese models would then be",
    "start": "3642550",
    "end": "3650079"
  },
  {
    "text": "trained by maximum likelihood. And that's a separate\nthing that we're going to talk about how to evaluate.",
    "start": "3650080",
    "end": "3655310"
  },
  {
    "text": "So that solution\nmight not actually give you a good score from\nthe perspective of a learning algorithm, even though maybe\nthe samples would look fine.",
    "start": "3655310",
    "end": "3665210"
  },
  {
    "text": "But, yeah, I haven't seen\nthat happening in practice. ",
    "start": "3665210",
    "end": "3672440"
  },
  {
    "text": "OK, so the bonus\nwould be single pass you can get everything, as\nopposed to n different passes.",
    "start": "3672440",
    "end": "3679910"
  },
  {
    "text": "And the way you do it\nis to basically mask.",
    "start": "3679910",
    "end": "3685710"
  },
  {
    "text": "Right? So what you have to enforce\nis some kind of ordering. And so you basically have to\ntake the general computation",
    "start": "3685710",
    "end": "3693860"
  },
  {
    "text": "graph that you have\nfrom an autoencoder and you have to mask out some\nconnections so that there",
    "start": "3693860",
    "end": "3701540"
  },
  {
    "text": "is some ordering that then\nyou can use to generate data. And the ordering\ncan be anything.",
    "start": "3701540",
    "end": "3709559"
  },
  {
    "text": "So for example, you\ncan pick an ordering where we choose this x2,\nx3 and x1 which corresponds",
    "start": "3709560",
    "end": "3718110"
  },
  {
    "text": "to the chain rule\nfactorization of probability of x2, x3 given x2, and\nx1 given the other two.",
    "start": "3718110",
    "end": "3725240"
  },
  {
    "text": "And then what you can\ndo is you can mask out some connections in\nthis neural network",
    "start": "3725240",
    "end": "3730770"
  },
  {
    "text": "so that the\nreconstruction for x2 does not depend on\nany of the inputs.",
    "start": "3730770",
    "end": "3736474"
  },
  {
    "text": " And then you can mask\nout the parameters",
    "start": "3736475",
    "end": "3742410"
  },
  {
    "text": "of this neural network so\nthat the parameter of x3",
    "start": "3742410",
    "end": "3747480"
  },
  {
    "text": "is only allowed to depend on x2.",
    "start": "3747480",
    "end": "3752840"
  },
  {
    "text": "And the parameter of x1 is\nallowed to depend on everything,",
    "start": "3752840",
    "end": "3759450"
  },
  {
    "text": "just like according to the\nchain rule factorization. And so one way to do\nit, yeah, so that's I",
    "start": "3759450",
    "end": "3766920"
  },
  {
    "text": "think what I just said. One way to do it is you\ncan basically keep track",
    "start": "3766920",
    "end": "3774360"
  },
  {
    "text": "for every unit in\nyour hidden layers, you can basically keep track\nof what inputs it depends on.",
    "start": "3774360",
    "end": "3785113"
  },
  {
    "text": "And so what you could do is\nyou could pick for every unit, you can pick an integer i. And you can say I'm only going\nto allow this unit to depend",
    "start": "3785113",
    "end": "3793740"
  },
  {
    "text": "on the inputs up to index i.",
    "start": "3793740",
    "end": "3799980"
  },
  {
    "text": "And so you can see here\nthat there's this 2-1-2-2.",
    "start": "3799980",
    "end": "3805380"
  },
  {
    "text": "This basically means it's only\nallowed to depend, for example, this unit is only allowed to\ndepend on the units 1 and 2.",
    "start": "3805380",
    "end": "3813359"
  },
  {
    "text": "This unit here is labeled 1. So it's only allowed to\ndepend on the first input, according to the\nordering, which is x2.",
    "start": "3813360",
    "end": "3820210"
  },
  {
    "text": " And then you basically\nrecursively add the masks",
    "start": "3820210",
    "end": "3829030"
  },
  {
    "text": "to preserve this invariant. So when you go to the\nnext layer and you have a node that is\nlabeled 1, then you",
    "start": "3829030",
    "end": "3835859"
  },
  {
    "text": "are only allowing a connection\nto the nodes that are labeled up to 1 in the previous layer.",
    "start": "3835860",
    "end": "3843120"
  },
  {
    "text": "And the way you achieve\nit is by basically masking out and setting to\n0, basically, some",
    "start": "3843120",
    "end": "3848940"
  },
  {
    "text": "of the elements of\nthe matrix that you would use for that layer\nof the neural network. ",
    "start": "3848940",
    "end": "3856240"
  },
  {
    "text": "And if you do that, then\nyou preserve this invariant. And you can see that indeed\nthe probability of x2, which",
    "start": "3856240",
    "end": "3863470"
  },
  {
    "text": "is the second output\nof the neural network, does not depend on any\ninput, which is what we want",
    "start": "3863470",
    "end": "3871880"
  },
  {
    "text": "for a chain rule factorization. And if you look at the\nparameter of x3, which is the third output, you'll\nsee that if you follow",
    "start": "3871880",
    "end": "3879650"
  },
  {
    "text": "all these paths,\nthey should only depend on basically\nthe second, on x2,",
    "start": "3879650",
    "end": "3888079"
  },
  {
    "text": "which is the variable that\ncome before it in the ordering. And so by maintaining\nthis invariant,",
    "start": "3888080",
    "end": "3893900"
  },
  {
    "text": "you get an autoencoder,\nwhich is actually an autoregressive model. ",
    "start": "3893900",
    "end": "3901309"
  },
  {
    "text": "You are essentially\nforcing the model not to cheat by looking at\nfuture outputs to predict.",
    "start": "3901310",
    "end": "3907070"
  },
  {
    "text": "And it can only use past inputs\nto predict future outputs, essentially.",
    "start": "3907070",
    "end": "3912870"
  },
  {
    "text": "And this is one\narchitecture that would enforce this kind of invariant.",
    "start": "3912870",
    "end": "3918630"
  },
  {
    "text": "Yeah? Sorry, is this something that's\nlike done during training? Or do you train an\nautoencoder and then",
    "start": "3918630",
    "end": "3925260"
  },
  {
    "text": "mask during the generation? This is done during training. So during training,\nyou basically",
    "start": "3925260",
    "end": "3931050"
  },
  {
    "text": "have to set up an\narchitecture that is masked so that\nit's not allowed to cheat while you train.",
    "start": "3931050",
    "end": "3937920"
  },
  {
    "text": "Because if you didn't\nmask, then it could, when trying to predict\nthe x2, you just",
    "start": "3937920",
    "end": "3943710"
  },
  {
    "text": "look at the actual value\nand you use it, right? And so this is very similar if\nyou've seen language models,",
    "start": "3943710",
    "end": "3950250"
  },
  {
    "text": "you also have to\nmask to basically not allow it to look into future\ntokens to make a prediction.",
    "start": "3950250",
    "end": "3956470"
  },
  {
    "text": "If you're allowed to look into\nthe future to predict tokens, then it's going to cheat.",
    "start": "3956470",
    "end": "3962138"
  },
  {
    "text": "And you're not going\nto do the right thing. And this is the same\nthing at that level,",
    "start": "3962138",
    "end": "3968760"
  },
  {
    "text": "this is a different\ncomputation graph that basically achieves\nthe same sort of result.",
    "start": "3968760",
    "end": "3973800"
  },
  {
    "text": "And the benefits of single pass\nis during training time, right? For inference, we obviously\nstill have to [INAUDIBLE]..",
    "start": "3973800",
    "end": "3980880"
  },
  {
    "text": "Yes, good question. Yeah, yeah, yeah. So the question is, is the\nbenefit only at training time or inference time?",
    "start": "3980880",
    "end": "3986110"
  },
  {
    "text": "So the benefit is\nonly at training time. Because at inference\ntime, you still have the sequential\nthing that you would have to come up with a\nvalue for the first variable",
    "start": "3986110",
    "end": "3994350"
  },
  {
    "text": "and fit it in. So it would still\nhave to be sequential. That's unavoidable. Every autoregressive model has\nthat kind of annoying flavor,",
    "start": "3994350",
    "end": "4002840"
  },
  {
    "text": "basically. How do you choose the ordering? So the recipe in\nthis paper is random.",
    "start": "4002840",
    "end": "4010130"
  },
  {
    "text": "You mean the value\nor the ordering? The ordering. Oh, the ordering,\nthat's also very hard.",
    "start": "4010130",
    "end": "4017059"
  },
  {
    "text": "I think if you have something\nwhere you know the structure and you know again\nthat there is time,",
    "start": "4017060",
    "end": "4025280"
  },
  {
    "text": "maybe there is a reasonable\nway of picking an ordering. Otherwise, you would have to\neither choose many orderings.",
    "start": "4025280",
    "end": "4032359"
  },
  {
    "text": "Maybe you have basically\nhave a mixture. Choose one at random. But there is not a good way of\nbasically selecting an ordering.",
    "start": "4032360",
    "end": "4039165"
  },
  {
    "text": "There is actually\nresearch where people have been trying to learn\nautoregressive models and an ordering so you can\ndefine a family of models",
    "start": "4039165",
    "end": "4047000"
  },
  {
    "text": "where you can search\nover possible orderings and search over factorizations\nover that ordering.",
    "start": "4047000",
    "end": "4053660"
  },
  {
    "text": "But you can imagine,\nthere is, like, n-factorial different\norderings to search over and it's discrete. So it's a very tough kind\nof optimization problem",
    "start": "4053660",
    "end": "4061310"
  },
  {
    "text": "to find the right ordering. If 1 is not dependent\non anything,",
    "start": "4061310",
    "end": "4067220"
  },
  {
    "text": "how does the model output 1? It should only output\n2 and 3, right? 1 should be [INAUDIBLE].",
    "start": "4067220",
    "end": "4075590"
  },
  {
    "text": "You would have to. I mean, depending on\nthe loss function. You cannot depend on anything. But you can still basically make\na guess based on no evidence.",
    "start": "4075590",
    "end": "4084180"
  },
  {
    "text": "So you would basically\nchoose the prior, right? So if, let's say, the second\nvariable is always true,",
    "start": "4084180",
    "end": "4091380"
  },
  {
    "text": "then depending on the\ntraining objective, you would still try to\nchoose an output here. It's a constant.",
    "start": "4091380",
    "end": "4097889"
  },
  {
    "text": "But you will try to match,\nbasically, the most likely value in the training set.",
    "start": "4097890",
    "end": "4102929"
  },
  {
    "text": "Or if you have a\nproper scoring rule, then you would try to match\nthe distribution that you see in the training set.",
    "start": "4102930",
    "end": "4108399"
  },
  {
    "text": "Depending on the loss\nfunction, you still try to choose a value\nthat makes sense. But it's fixed.",
    "start": "4108399",
    "end": "4114339"
  },
  {
    "text": "So you can only choose one. And so you can't do much. But you would still try to do\nyour best to capture the data",
    "start": "4114340",
    "end": "4122278"
  },
  {
    "text": "depending on the training loss. Yeah? Are there redundancies?",
    "start": "4122279",
    "end": "4127509"
  },
  {
    "text": "Like, in the second\nto last layer, there's two nodes which\njust have 1 as an input.",
    "start": "4127510",
    "end": "4132649"
  },
  {
    "text": "So is it kind of redundant\nto have multiple nodes which just have one input?",
    "start": "4132649",
    "end": "4139299"
  },
  {
    "text": "I mean, the weights\nare different. So even though there is multiple\nnodes that have only one input, they might be extracting\ndifferent features",
    "start": "4139300",
    "end": "4145960"
  },
  {
    "text": "for that input. So it's not necessarily\nredundant, I would say.",
    "start": "4145960",
    "end": "4151490"
  },
  {
    "text": "So the objective of the\nautoencoder is to reconstruct. Yes. So how do you reconcile\nthe loss function?",
    "start": "4151490",
    "end": "4158568"
  },
  {
    "text": "It's predicting one\nthing at a time. How do you make the loss\nfunction for reconstruction?",
    "start": "4158569",
    "end": "4164477"
  },
  {
    "text": "Yeah, so the loss\nfunction would be the ones that we have here,\nwhich would be basically",
    "start": "4164477",
    "end": "4171920"
  },
  {
    "text": "you would try to make the\npredictions close to what you have in the data. So the loss function\nwouldn't change. It's just that the way you\nmake predictions is you're",
    "start": "4171920",
    "end": "4179028"
  },
  {
    "text": "not allowed to\ncheat, for example. You're not allowed to look\nat xi when you predict xi.",
    "start": "4179029",
    "end": "4185479"
  },
  {
    "text": "And you're only\nallowed to predict it based on previous\nvariables in some ordering.",
    "start": "4185479",
    "end": "4190818"
  },
  {
    "text": "And it turns out that that\nwould be exactly the same loss that you would have\nif you were to train the autoregressive model.",
    "start": "4190819",
    "end": "4196010"
  },
  {
    "text": "It depends on the model\nfamily that you choose. But if you have logistic\nregression models, it would be exactly the\nsame loss, for example.",
    "start": "4196010",
    "end": "4204620"
  },
  {
    "text": "In your last layer,\nlike you said, you pick, each of\nthe number of times it looks previously randomly.",
    "start": "4204620",
    "end": "4211750"
  },
  {
    "text": "So if you happen\nto pick 2-2-2, how would you predict the\nfirst entry after that? Yeah, so you would\nbasically be, you're not",
    "start": "4211750",
    "end": "4219788"
  },
  {
    "text": "allowed many connections. And you would do\na pretty bad job because you would be less\nflexible than you could be.",
    "start": "4219788",
    "end": "4226680"
  },
  {
    "text": "It would still be a valid model. It wouldn't be a\ngood one, I guess. So that's why people\noften have kind",
    "start": "4226680",
    "end": "4232320"
  },
  {
    "text": "of like an ensemble\nof these models where you have multiple masks\nand you just do it that way.",
    "start": "4232320",
    "end": "4238010"
  },
  {
    "text": "Yeah. Cool. Let's see now an\nalternative way to approach",
    "start": "4238010",
    "end": "4248670"
  },
  {
    "text": "this is to use RNN, some kind of\nrecursive style of computation",
    "start": "4248670",
    "end": "4255989"
  },
  {
    "text": "to basically predict the\nnext random variable, given",
    "start": "4255990",
    "end": "4263550"
  },
  {
    "text": "the previous ones,\naccording to some ordering. Right. At the end of the\nday, this is what",
    "start": "4263550",
    "end": "4271120"
  },
  {
    "text": "the key problem whenever you\nbuild an autoregressive model is, solving a bunch of coupled\nkind of prediction problems",
    "start": "4271120",
    "end": "4278580"
  },
  {
    "text": "where you predict a\nsingle variable, given the other variables that come\nbefore it in some ordering.",
    "start": "4278580",
    "end": "4285270"
  },
  {
    "text": "And the issue is that\nthis history kind of keeps getting longer.",
    "start": "4285270",
    "end": "4290890"
  },
  {
    "text": "So you are conditioning\non more and more things. And RNNs are pretty\ngood at or it's one way",
    "start": "4290890",
    "end": "4298230"
  },
  {
    "text": "to handle this kind\nof situation and try",
    "start": "4298230",
    "end": "4304800"
  },
  {
    "text": "to keep a summary of all\nthe information of all the things you've\nconditioned on so far",
    "start": "4304800",
    "end": "4309810"
  },
  {
    "text": "and recursively update it. And so a computation graph\nwould look something like this.",
    "start": "4309810",
    "end": "4317420"
  },
  {
    "text": "So there is a summary h. Let's say h of t\nor h of t-plus-1,",
    "start": "4317420",
    "end": "4323560"
  },
  {
    "text": "which basically is a vector that\nsummarizes all the inputs up to that time.",
    "start": "4323560",
    "end": "4331850"
  },
  {
    "text": "And you initialize it somehow\nbased on some initialization. And then you\nrecursively update it",
    "start": "4331850",
    "end": "4338600"
  },
  {
    "text": "by saying the new\nsummary of the history is some transformation of\nthe history I've seen so far.",
    "start": "4338600",
    "end": "4346430"
  },
  {
    "text": "And the new input for\nthat time step, xt-plus-1.",
    "start": "4346430",
    "end": "4351830"
  },
  {
    "text": "And maybe this is one\nway to implement it. You do some kind of linear\ntransformation of ht, xt-plus-1,",
    "start": "4351830",
    "end": "4359690"
  },
  {
    "text": "you apply some non-linearity. And that gives you the new\nsummary up to time t-plus-1.",
    "start": "4359690",
    "end": "4368310"
  },
  {
    "text": "And then what you\ncan do is, just like what we've done so\nfar, is then you transform h",
    "start": "4368310",
    "end": "4376469"
  },
  {
    "text": "and you map it to\neither, let's say, the parameters of a categorical\nrandom variable or a Bernoulli",
    "start": "4376470",
    "end": "4384750"
  },
  {
    "text": "random variable or a\nmixture of Gaussians. Whatever it is that you need\nto predict, you do it through--",
    "start": "4384750",
    "end": "4391245"
  },
  {
    "text": "well, I guess you probably also\nwould need some non-linearities here. But there is some\noutput, which is the thing you use\nfor prediction, which",
    "start": "4391245",
    "end": "4397890"
  },
  {
    "text": "is going to depend only on this\nhistory vector or this summary vector of all the things\nyou've seen so far.",
    "start": "4397890",
    "end": "4404895"
  },
  {
    "text": " And the good thing about\nthis is that basically it",
    "start": "4404895",
    "end": "4413550"
  },
  {
    "text": "has a very small\nnumber of parameters. Like regardless of\nhow long the history is, there is a fixed number\nof learnable parameters",
    "start": "4413550",
    "end": "4420180"
  },
  {
    "text": "which are all these matrices\nthat you use to recursively update your summary of all the\ninformation you've seen so far.",
    "start": "4420180",
    "end": "4431750"
  },
  {
    "text": "And so it's constant\nwith respect to n. Remember, we had the things\nthat were linear in n",
    "start": "4431750",
    "end": "4438648"
  },
  {
    "text": "and we had things that\nwere quadratic in n. This thing is actually constant. The dimensions are fixed and\nyou just keep applying them.",
    "start": "4438648",
    "end": "4446217"
  },
  {
    "text": "[INAUDIBLE] squared\nsharing weights? Exactly. It's extreme weight sharing.",
    "start": "4446217",
    "end": "4451639"
  },
  {
    "text": "And then you try to do\neverything through [INAUDIBLE].. Yes. [? We ?] are imposing\na Markov assumption",
    "start": "4451640",
    "end": "4458390"
  },
  {
    "text": "on the conditional\nprobabilities? This is still not. So the question is, is\nthis a Markov assumption?",
    "start": "4458390",
    "end": "4465227"
  },
  {
    "text": "This is not a Markov\nassumption in the sense that if you think\nabout xt, it's not just",
    "start": "4465227",
    "end": "4471330"
  },
  {
    "text": "a function of the previous\nxt-minus-1, right? It still depends on all the past\nrandom variables in a, again,",
    "start": "4471330",
    "end": "4483900"
  },
  {
    "text": "not entirely general way. So you can only capture\nthe dependencies",
    "start": "4483900",
    "end": "4489840"
  },
  {
    "text": "that you can write down in\nterms of this sort of recursion. And so it's definitely\nnot a Markov assumption.",
    "start": "4489840",
    "end": "4500490"
  },
  {
    "text": "And if you think about\nthe computation graph, it does depend on all\nthe previous inputs. ",
    "start": "4500490",
    "end": "4509620"
  },
  {
    "text": "And so this is an example of how\nyou would use this kind of model",
    "start": "4509620",
    "end": "4514690"
  },
  {
    "text": "to model text. So the idea is that in\nthis simple example,",
    "start": "4514690",
    "end": "4520250"
  },
  {
    "text": "we have only, let's say, four\ndifferent characters, h, e, l, and o.",
    "start": "4520250",
    "end": "4526300"
  },
  {
    "text": "Then you would\nbasically encode them, let's say, using some\nkind of one-hot encoding.",
    "start": "4526300",
    "end": "4532809"
  },
  {
    "text": "So h is 1-0-0, e is\n0-1-0-0, and so forth.",
    "start": "4532810",
    "end": "4539060"
  },
  {
    "text": "And then as usual you\nwould use some kind of autoregressive factorization. So you write it down.",
    "start": "4539060",
    "end": "4544420"
  },
  {
    "text": "In this case, the ordering is\nthe one from left to right. So you write the probability\nof choosing the first character",
    "start": "4544420",
    "end": "4550240"
  },
  {
    "text": "in your piece of text,\nthen the probability of choosing the second\ncharacter, given the first one,",
    "start": "4550240",
    "end": "4555340"
  },
  {
    "text": "and so forth. And what you would do\nis you would basically",
    "start": "4555340",
    "end": "4561820"
  },
  {
    "text": "obtain these probabilities\nfrom the hidden layer",
    "start": "4561820",
    "end": "4567250"
  },
  {
    "text": "of this recurrent\nneural network. So you have these\nhidden layers that are updated according\nto that recursion",
    "start": "4567250",
    "end": "4573330"
  },
  {
    "text": "that I showed you before. And then you would\nuse the hidden layer. You would transform it\ninto an output layer,",
    "start": "4573330",
    "end": "4581895"
  },
  {
    "text": "which is just four numbers. And then you can take\na softmax to basically map that to four non-negative\nnumbers between 0 and 1",
    "start": "4581895",
    "end": "4591540"
  },
  {
    "text": "that sum to 1. And so in this case, for\nexample, we have a hidden layer.",
    "start": "4591540",
    "end": "4598280"
  },
  {
    "text": "And then we apply some\nlinear transformation to get these four numbers. And we're trying to basically\nchoose the values such",
    "start": "4598280",
    "end": "4607400"
  },
  {
    "text": "that the second entry of\nthat vector is very large. Because that would put\na lot of probability on the second sort of\npossible character, which",
    "start": "4607400",
    "end": "4615950"
  },
  {
    "text": "happens to be e,\nwhich is the one we want for the second position.",
    "start": "4615950",
    "end": "4621813"
  },
  {
    "text": "And so then when you\ntrain these models, the game is to choose\nvalues for these matrices so that, let's say, you maximize\nthe probability of observing",
    "start": "4621813",
    "end": "4629240"
  },
  {
    "text": "a particular data\npoint or data set. ",
    "start": "4629240",
    "end": "4636380"
  },
  {
    "text": "And, yeah, so\nagain, the key thing",
    "start": "4636380",
    "end": "4641690"
  },
  {
    "text": "here is that there are a very\nsmall number of parameters. And then you use the\nhidden state of the RNN",
    "start": "4641690",
    "end": "4648380"
  },
  {
    "text": "to get the conditional\nprobabilities that you need in an\nautoregressive factorization. ",
    "start": "4648380",
    "end": "4659389"
  },
  {
    "text": "And then you can\nsee the recursion. Then you would compute\nthe next hidden state",
    "start": "4659390",
    "end": "4664909"
  },
  {
    "text": "by taking the current history,\nthe new character that you have access to.",
    "start": "4664910",
    "end": "4669980"
  },
  {
    "text": "You update your recursion and\nyou get a new hidden state. You use that hidden\nstate to come up with a vector of\npredicted probabilities",
    "start": "4669980",
    "end": "4676027"
  },
  {
    "text": "for the next character,\nand so forth. So it's the same\nmachinery as before.",
    "start": "4676027",
    "end": "4681210"
  },
  {
    "text": "But instead of having multiple\nkind of logistic regression classifiers, we have\na bunch of classifiers",
    "start": "4681210",
    "end": "4688070"
  },
  {
    "text": "that are tied together\nby this recursion. ",
    "start": "4688070",
    "end": "4694719"
  },
  {
    "text": "And the pro is that you\ncan apply it to sequences of arbitrary length.",
    "start": "4694720",
    "end": "4699969"
  },
  {
    "text": "And it's actually,\nin theory at least, RNNs are pretty\ngeneral in the sense that they can\nessentially represent",
    "start": "4699970",
    "end": "4708600"
  },
  {
    "text": "any computable function,\nat least in theory. In practice, they\nare tricky to learn.",
    "start": "4708600",
    "end": "4714929"
  },
  {
    "text": "And you still need to\npick an ordering, which is always a problem for\nautoregressive models.",
    "start": "4714930",
    "end": "4721469"
  },
  {
    "text": "The key issue with\nthese sort of RNNs is that they're very slow\nduring training time.",
    "start": "4721470",
    "end": "4727590"
  },
  {
    "text": "Because you have to\nunroll this recursion to compute the probabilities.",
    "start": "4727590",
    "end": "4733800"
  },
  {
    "text": "And that's a problem. But I'll just show\nyou some examples. And then I think\nwe can end here.",
    "start": "4733800",
    "end": "4740489"
  },
  {
    "text": "It actually works\nreasonably well. Like, if you take a\nsimple three-layer RNN and you train it on all\nthe works of Shakespeare",
    "start": "4740490",
    "end": "4748830"
  },
  {
    "text": "at the character level, so it's\nliterally what I just showed you, just a three-layer RNA.",
    "start": "4748830",
    "end": "4754883"
  },
  {
    "text": "And then you sample\nfrom the model and you can get\nthings like this, which has a little bit of the\nflavor of Shakespeare, I guess.",
    "start": "4754883",
    "end": "4767219"
  },
  {
    "text": "If you think about it, this\nis at the character level. It's literally generating\ncharacter by character. It's actually pretty impressive.",
    "start": "4767220",
    "end": "4773430"
  },
  {
    "text": "Like, it needs to learn\nwhich words are valid and which ones are not,\nthe grammar, punctuation.",
    "start": "4773430",
    "end": "4779760"
  },
  {
    "text": "It's pretty impressive that\na relatively simple model like this working at the level\nof characters can do like this.",
    "start": "4779760",
    "end": "4787020"
  },
  {
    "text": "You could train it on Wikipedia\nand then you can sample and you can make\nup fake Wikipedia pages like this one on the\nItaly that conquered India.",
    "start": "4787020",
    "end": "4800610"
  },
  {
    "text": "It's pretty interesting\nmade-up stuff.",
    "start": "4800610",
    "end": "4805650"
  },
  {
    "text": "But again, you can see\nit's pretty interesting how it has the right markdown syntax\nand it's closing the brackets",
    "start": "4805650",
    "end": "4812190"
  },
  {
    "text": "after opening them,\nwhich it has to remember through this single\nhidden state, right, that it's carrying over.",
    "start": "4812190",
    "end": "4820500"
  },
  {
    "text": "Yeah. So, you know, it's\neven making up links",
    "start": "4820500",
    "end": "4827830"
  },
  {
    "text": "for these made-up facts\nthat it generates. And train it on baby\nnames and then you",
    "start": "4827830",
    "end": "4834097"
  },
  {
    "text": "can sample from the model. You can get new names. ",
    "start": "4834097",
    "end": "4840040"
  },
  {
    "text": "So, yeah, it works\nsurprisingly well. I guess the main issue\nthat hopefully then maybe",
    "start": "4840040",
    "end": "4847220"
  },
  {
    "text": "we'll go over it next time. The reason this is not used for\nstate-of-the-art language models",
    "start": "4847220",
    "end": "4852760"
  },
  {
    "text": "is that you have this bottleneck\nthat you need to capture all the information up to\ntime t in a single vector,",
    "start": "4852760",
    "end": "4859600"
  },
  {
    "text": "which is a problem. And the sequential evaluation,\nthat's the main bottleneck.",
    "start": "4859600",
    "end": "4865219"
  },
  {
    "text": "So it cannot take advantage\nof modern kind of GPUs because in order to\ncompute the probabilities,",
    "start": "4865220",
    "end": "4871530"
  },
  {
    "text": "you really have to\nunroll the computation. And you have to go\nthrough it step by step. And that's kind of\nthe main challenge.",
    "start": "4871530",
    "end": "4879150"
  },
  {
    "start": "4879150",
    "end": "4883000"
  }
]