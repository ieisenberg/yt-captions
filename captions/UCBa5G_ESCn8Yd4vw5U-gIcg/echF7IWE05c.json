[
  {
    "start": "0",
    "end": "5380"
  },
  {
    "text": "Well, hello, everyone. So I guess-- so in the next--",
    "start": "5380",
    "end": "14090"
  },
  {
    "text": "I guess, in this\nlecture, what we're going to do is we're\ngoing to bound Rademacher",
    "start": "14090",
    "end": "23220"
  },
  {
    "text": "complexity by some concrete\nformula for concrete models.",
    "start": "23220",
    "end": "33545"
  },
  {
    "text": " And by concrete models,\nI really just mean linear models for this lecture.",
    "start": "33545",
    "end": "39790"
  },
  {
    "text": "And in the few\nlectures later, we're going to talk about\nneural networks.",
    "start": "39790",
    "end": "45210"
  },
  {
    "text": "And just as a review to\nconnect to past lectures, we have to prove that\ngeneralization error,",
    "start": "45210",
    "end": "52080"
  },
  {
    "text": "excess risk or the\ngeneralization error is upper bounded by\nRademacher complexity. That's what we have done last\ntime in the last few lectures.",
    "start": "52080",
    "end": "60330"
  },
  {
    "text": "And in the next\nfew lectures, we're going to talk about how to upper\nbound Rademacher complexity",
    "start": "60330",
    "end": "66570"
  },
  {
    "text": "exactly for concrete models,\nlike linear models of networks. ",
    "start": "66570",
    "end": "73950"
  },
  {
    "text": "And also, we are going to deal\nwith the classification loss.",
    "start": "73950",
    "end": "80789"
  },
  {
    "text": "So there is something to\ndo with classification loss",
    "start": "80790",
    "end": "86370"
  },
  {
    "text": "because it's a binary loss. It's not continuous, so\nwe have to deal with it using some special technique.",
    "start": "86370",
    "end": "92579"
  },
  {
    "text": "So that's the overview\nfor this lecture. So let's first set up the basic\nthings, its classification.",
    "start": "92580",
    "end": "99570"
  },
  {
    "text": "So we're going to deal\nwith binary classification. ",
    "start": "99570",
    "end": "107790"
  },
  {
    "text": "So I guess as you\nprobably would expect it, you have y which is in minus\n1 and 1 and some classifier",
    "start": "107790",
    "end": "116510"
  },
  {
    "text": "h, which maps the input\nspace x to real number R. So here the classifier maps--",
    "start": "116510",
    "end": "124070"
  },
  {
    "text": "we think of h as\nthe function that maps the input to the real\nnumber, the logit, for example.",
    "start": "124070",
    "end": "131540"
  },
  {
    "text": "And when you make\nthe prediction, you take a sign of the\noutput, of the classifier.",
    "start": "131540",
    "end": "139950"
  },
  {
    "text": "So you take the sign of h(x). And this gives you\nthe classifier, right?",
    "start": "139950",
    "end": "145769"
  },
  {
    "text": "The h outputs positive numbers. You output 1. And otherwise, you\nget negative 1. And H is the family of h's.",
    "start": "145770",
    "end": "158570"
  },
  {
    "text": "That's orientation. And the loss function\non each example",
    "start": "158570",
    "end": "164380"
  },
  {
    "text": "x, y is equals to\nthe indicator of y",
    "start": "164380",
    "end": "170380"
  },
  {
    "text": "is not equal to the two\nexa-- true label is not equal to the sign of h(x).",
    "start": "170380",
    "end": "178802"
  },
  {
    "text": "That's our setup. ",
    "start": "178802",
    "end": "187709"
  },
  {
    "text": "Right. So I guess the first\nthing is that I'm",
    "start": "187710",
    "end": "193250"
  },
  {
    "text": "going to very briefly mention\nthis finite hypothesis case.",
    "start": "193250",
    "end": "199180"
  },
  {
    "text": "That's just a very\nquick kind of note. Or we have already done finite\nhypothesis class, right?",
    "start": "199180",
    "end": "205130"
  },
  {
    "text": "So it's probably useful to\nknow that you can recover the same bound for\nfinite hypothesis",
    "start": "205130",
    "end": "210460"
  },
  {
    "text": "class using this machinery\nof Rademacher complexity. Right? That's some kind of\nlike a-- probably a",
    "start": "210460",
    "end": "215470"
  },
  {
    "text": "reasonable requirement if\nyou think that Rademacher complexity is a powerful tool. So there is indeed\nsuch a theorem,",
    "start": "215470",
    "end": "222080"
  },
  {
    "text": "which I'm not going to\nprove today, because the way to prove it actually is\nmore related to something",
    "start": "222080",
    "end": "229060"
  },
  {
    "text": "more advanced later. So I'm just going to\nstate the theorem. So saying that if f satisfies\nthat for every f in f,",
    "start": "229060",
    "end": "245470"
  },
  {
    "text": "the sum of f Zi\nsquared, this is--",
    "start": "245470",
    "end": "251340"
  },
  {
    "text": "it's less than n squared. This is a condition that\nis a little kind of not",
    "start": "251340",
    "end": "257519"
  },
  {
    "text": "super intuitive. But actually, what\nreally this is saying is that this is implied.",
    "start": "257519",
    "end": "263610"
  },
  {
    "text": "This is a weaker\nversion of just assuming",
    "start": "263610",
    "end": "270194"
  },
  {
    "text": "f(z) is bounded less than n. So if f(z) is binded\nby n, then, of course,",
    "start": "270195",
    "end": "277140"
  },
  {
    "text": "the average of the square\nis binded by n squared. Right. So it's just I'm assuming--",
    "start": "277140",
    "end": "282225"
  },
  {
    "text": "I'm stating the state,\nthe weakest version of the theorem for generality. And if you know\nthis, then you have",
    "start": "282225",
    "end": "288690"
  },
  {
    "text": "that the empirical Rademacher\ncomplexity on this example",
    "start": "288690",
    "end": "294160"
  },
  {
    "text": "is z1 up to zn, right? So s is equal to-- s is equal to z1\nup to zn, right?",
    "start": "294160",
    "end": "302520"
  },
  {
    "text": "So it is bounded by\nsomething, by the size of this hypothesis class.",
    "start": "302520",
    "end": "310285"
  },
  {
    "text": "And it's bounded\nby the logarithm of the size of the hypothesis\nclass times something",
    "start": "310285",
    "end": "315819"
  },
  {
    "text": "like M squared, the range\nof this function class n, function class f. And also, you divide by n.",
    "start": "315820",
    "end": "321858"
  },
  {
    "text": "You take the square root.  So this is-- essentially,\nbasically log f over n,",
    "start": "321858",
    "end": "328759"
  },
  {
    "text": "square root log f over n. . And if you apply this to\nthe finite hypothesis class that we have talked about like,\nfor example, if you apply this",
    "start": "328760",
    "end": "336020"
  },
  {
    "text": "to a loss function, a\nbinary loss function, you get what we had before.",
    "start": "336020",
    "end": "341240"
  },
  {
    "text": "It's kind of-- it's almost\nexactly the same bounds eventually. ",
    "start": "341240",
    "end": "347345"
  },
  {
    "text": "And we are not\ngoing to prove this. But we are going to prove\nit in the future lectures. Today, we are not\ngoing to prove it",
    "start": "347345",
    "end": "352928"
  },
  {
    "text": "because the techniques is\nmore related to something what we are going to use later.",
    "start": "352928",
    "end": "359180"
  },
  {
    "text": "But this is not-- this is just saying that\nwe can achieve what we had, but it's more interesting\nwhen you apply this Rademacher",
    "start": "359180",
    "end": "366560"
  },
  {
    "text": "complexity for continuous\nfunction class, right? And we have also\ntalked about what's the limitation of having\nfinite hypothesis class.",
    "start": "366560",
    "end": "375540"
  },
  {
    "text": "For example, the\nlimitation is that even if you do this with some\nkind of discretization",
    "start": "375540",
    "end": "382400"
  },
  {
    "text": "with continuous models you're\ngoing to lose a parameter p in your bound, right? So if you do this plus some\ndiscretization, then likely",
    "start": "382400",
    "end": "390878"
  },
  {
    "text": "what you're going\nto get is something like p over n, where p is the\ndimensionality of the model,",
    "start": "390878",
    "end": "398517"
  },
  {
    "text": "it's number of\nparameters in the model if you do some discretization. And they wouldn't\nbe super impressive",
    "start": "398517",
    "end": "404930"
  },
  {
    "text": "given that we already have\ndone those brute-force discretizations that\nwe had done before.",
    "start": "404930",
    "end": "411560"
  },
  {
    "text": "Right? So today what we're\ngoing to do is that we are going to have a\ndifferent way to upper bound",
    "start": "411560",
    "end": "418290"
  },
  {
    "text": "Rademacher complexity not\nusing this kind of tools. And the way that we\ndo it is actually",
    "start": "418290",
    "end": "424220"
  },
  {
    "text": "more algebraic and\nanalytical as we'll see. ",
    "start": "424220",
    "end": "429900"
  },
  {
    "text": "So before doing that,\nwe are going to-- so first deal with the\nloss function, right?",
    "start": "429900",
    "end": "435530"
  },
  {
    "text": "So you can look at\nthe loss, right? This is L01 x, y, h.",
    "start": "435530",
    "end": "441199"
  },
  {
    "text": "All right, it's this.  So the thing is--",
    "start": "441200",
    "end": "447650"
  },
  {
    "text": "the tricky thing is that\nthere is a sign here, right? So if you don't have the\nsign and h of x is outputting",
    "start": "447650",
    "end": "454850"
  },
  {
    "text": "something like binary, where\nwe call that we have done this in one of the previous\nlecture, so in that case,",
    "start": "454850",
    "end": "460740"
  },
  {
    "text": "we assume h(x) is-- so in previous\nlecture, we have shown",
    "start": "460740",
    "end": "467140"
  },
  {
    "text": "that if h(x) is outputting\nsomething like binary, right, it outputs\nplus 1 or minus 1,",
    "start": "467140",
    "end": "472720"
  },
  {
    "text": "then you can show that the\nRademacher complexity of f, the Rademacher complexity of\nthe loss functions, the losses,",
    "start": "472720",
    "end": "479650"
  },
  {
    "text": "is basically on the same\norder as the Rademacher complexity of the\nhypothesis class, h right?",
    "start": "479650",
    "end": "486972"
  },
  {
    "text": "That's what we did last time. But now, we are doing a slightly\ndifferent definition of the h, right? So the h is the function that\noutputs the real number, right?",
    "start": "486972",
    "end": "496060"
  },
  {
    "text": "It's the one that before\nthe sign function. So then this kind of like--",
    "start": "496060",
    "end": "504160"
  },
  {
    "text": "this kind of like reduction\ndoesn't work anymore, right? So, of course, you can\nstill apply the same thing",
    "start": "504160",
    "end": "510160"
  },
  {
    "text": "to the sign of h,\nbut then you're going to get a Rademacher\ncomplexity of the sign of h,",
    "start": "510160",
    "end": "515200"
  },
  {
    "text": "which again is also-- it's kind of like you\ndidn't solve the problem. You had the problem\nin a different way.",
    "start": "515200",
    "end": "522159"
  },
  {
    "text": "So we're going to express a\ndeal with this issue first. And that's called--\nsometimes I think",
    "start": "522159",
    "end": "528850"
  },
  {
    "text": "people call it margin theory. So we're going to\nintroduce a bunch of tools to deal with this sign issue.",
    "start": "528850",
    "end": "534310"
  },
  {
    "text": "In some sense, you have\nto convert the real number to the binary number\nin some effective way.",
    "start": "534310",
    "end": "540640"
  },
  {
    "text": "And then we're going\nto bound the Rademacher complexity of linear models\nusing analytical tools.",
    "start": "540640",
    "end": "547600"
  },
  {
    "text": "So that's the plan. ",
    "start": "547600",
    "end": "553430"
  },
  {
    "text": "OK. So I guess the kind\nof the intuition",
    "start": "553430",
    "end": "558500"
  },
  {
    "text": "is that the scales,\nin some sense, like matter, when you do\nclassification implicitly.",
    "start": "558500",
    "end": "566740"
  },
  {
    "text": "But even though at\nthe end of the day, your scale doesn't matter. So kind of the-- the kind\nof the motivating example",
    "start": "566740",
    "end": "572272"
  },
  {
    "text": "is the following. For example, suppose you have\na classification class, right? I'm using red for\npositive data, and you",
    "start": "572272",
    "end": "578290"
  },
  {
    "text": "have some kind of like--  you kind of have the--",
    "start": "578290",
    "end": "585266"
  },
  {
    "text": "I use circle for\nthe negative data. And if you think about\ndifferent classifiers-- for example, this classifier\nand this one, right?",
    "start": "585266",
    "end": "595750"
  },
  {
    "text": "So these two classifiers just-- intuitively, I'm not\nclaiming anything rigorously.",
    "start": "595750",
    "end": "601300"
  },
  {
    "text": "If you intuitively think\nabout these two classifiers, the pink one probably\nshould generalize",
    "start": "601300",
    "end": "606550"
  },
  {
    "text": "worse than the blue\none because you only see these eight examples.",
    "start": "606550",
    "end": "612040"
  },
  {
    "text": "Maybe if you-- kind of like\nyour new example, test example, maybe it looks like--",
    "start": "612040",
    "end": "617170"
  },
  {
    "text": "maybe it's here. Then the pink one would have a\nmistake on this test example,",
    "start": "617170",
    "end": "622839"
  },
  {
    "text": "right? But the blue one sounds like-- less likely to make\nmistakes on test example.",
    "start": "622840",
    "end": "630010"
  },
  {
    "text": "So intuitively,\nthe blue one seems to have somewhat\nbetter generalization",
    "start": "630010",
    "end": "636850"
  },
  {
    "text": "just because this kind of like\nseparates the two clusters more clearly and more confidently.",
    "start": "636850",
    "end": "642250"
  },
  {
    "text": "So the confidence order-- so in some sense, you can\nthink of this h of x itself",
    "start": "642250",
    "end": "647470"
  },
  {
    "text": "as the confidence because\nthis is a real number. In some sense, this is-- the little bit bigger it\nis, the more confidence",
    "start": "647470",
    "end": "653950"
  },
  {
    "text": "you are about this example. And this does matter\nto some extent.",
    "start": "653950",
    "end": "659458"
  },
  {
    "text": "And this is what we\nare going to say. So like how do you\nsomehow reason about this",
    "start": "659458",
    "end": "664690"
  },
  {
    "text": "and make them matter in\nsome sense in your analysis? So here is the more formal\napproach towards this.",
    "start": "664690",
    "end": "673930"
  },
  {
    "text": "So let's first define-- OK, so I guess--",
    "start": "673930",
    "end": "678940"
  },
  {
    "text": "let's first assume--\nthis is a assumption throughout this lecture. So we assume that we classify\nall the examples correctly.",
    "start": "678940",
    "end": "687070"
  },
  {
    "text": "So assume the\ntraining error is 0.",
    "start": "687070",
    "end": "693690"
  },
  {
    "text": "So perfect classification\nfor training data.",
    "start": "693690",
    "end": "699895"
  },
  {
    "text": " And you can see that\nthis is, in some sense,",
    "start": "699895",
    "end": "705730"
  },
  {
    "text": "reasonable, especially\ngiven the more of the kind of like success\nof large network, right?",
    "start": "705730",
    "end": "711670"
  },
  {
    "text": "So typically, you can make\nthe training error very small. And this was actually\na reasonable assumption",
    "start": "711670",
    "end": "716950"
  },
  {
    "text": "even before deep\nlearning came into play. Before deep learning,\nwhat people did",
    "start": "716950",
    "end": "722560"
  },
  {
    "text": "was that you add more\nand more features, and your dimensionality\nof the features becomes higher and higher.",
    "start": "722560",
    "end": "728050"
  },
  {
    "text": "And at some point, the\ndimensionality of the features becomes bigger than\nthe number of examples, and then you can always theta\ntune data with zero error.",
    "start": "728050",
    "end": "738730"
  },
  {
    "text": "So formally what that\nmeans is that if you look at a training\nexample, it's always equal to some h oops, this.",
    "start": "738730",
    "end": "746515"
  },
  {
    "text": "That's what I mean by\ntraining error zero. And under this kind of training\nerror zero hypothesis class,",
    "start": "746515",
    "end": "753760"
  },
  {
    "text": "you can define the\nso-called margin. So this margin,\ntechnically, I think,",
    "start": "753760",
    "end": "759100"
  },
  {
    "text": "is only defined\nfor-- at least if you don't do any modification of it,\nyou probably should only define",
    "start": "759100",
    "end": "765040"
  },
  {
    "text": "for zero error classifier. And this is the so-called\nunnormalized margin. ",
    "start": "765040",
    "end": "776300"
  },
  {
    "text": "So the margin of x--",
    "start": "776300",
    "end": "781540"
  },
  {
    "text": "this is really just the\ny times h theta of x. You multiply x with\ny just because you",
    "start": "781540",
    "end": "790691"
  },
  {
    "text": "want to make it a\npositive number, right? So if y is positive, right,\nwe want a positive class,",
    "start": "790692",
    "end": "796620"
  },
  {
    "text": "you want h(x) to be big. And if y is negative, you\nwant h(x) to be small, right?",
    "start": "796620",
    "end": "803790"
  },
  {
    "text": "So in some sense, margin is kind\nof like a very informal version of confidence. It's not a\nprobability, of course.",
    "start": "803790",
    "end": "810180"
  },
  {
    "text": "It's between 0 and the\ninfinity of, right? So this is always nonactive if\nyou create a classifier exactly",
    "start": "810180",
    "end": "816360"
  },
  {
    "text": "on the training data. So this is always\nnonactive when you",
    "start": "816360",
    "end": "822300"
  },
  {
    "text": "are correct on this data point\nwhen y and h theta x y is",
    "start": "822300",
    "end": "828149"
  },
  {
    "text": "equals the sign of\nh x, theta x, yeah.  OK?",
    "start": "828150",
    "end": "833580"
  },
  {
    "text": "So this is the definition of\nthe margin of a single example. And then you can define\nthe margin of this data",
    "start": "833580",
    "end": "838680"
  },
  {
    "text": "set by the margin\nof our data set, margin of the classifier\non the data set.",
    "start": "838680",
    "end": "843900"
  },
  {
    "text": "So this is defined to be-- when you look at the minimum\nmargin over all examples--",
    "start": "843900",
    "end": "850150"
  },
  {
    "text": "so you take the minimum\nover yi times h theta of Xi.",
    "start": "850150",
    "end": "856120"
  },
  {
    "text": "Of course, this margin is a\nfunction of the classifier. If you change the classifier,\nyou have different margins, right? In some sense, the blue one--",
    "start": "856120",
    "end": "864670"
  },
  {
    "text": "as I drew there, right? Has a bigger margin, a pink\none, because in some sense,",
    "start": "864670",
    "end": "870970"
  },
  {
    "text": "this pink one has some example\nwhich has very small margin. And the blue one, for all the\nexamples, you have big margins.",
    "start": "870970",
    "end": "877660"
  },
  {
    "text": "So you take the minimum. Then over all the\nexamples, you still have relatively big margin.",
    "start": "877660",
    "end": "884320"
  },
  {
    "text": "But I guess here I'm defining\nthe unnormalized margin. So if you look at\nunnormalized margin,",
    "start": "884320",
    "end": "890079"
  },
  {
    "text": "it's not exactly the\ndistance from the example to the hyperplane. So you have to normalize\nit so that it becomes",
    "start": "890080",
    "end": "896589"
  },
  {
    "text": "the distance to the hyperplane. But I think we-- I think, in this course, we\ndon't need to actually define",
    "start": "896590",
    "end": "902529"
  },
  {
    "text": "the normalized margin per se. So if you normal--\nso for linear model--",
    "start": "902530",
    "end": "909370"
  },
  {
    "text": "I guess you probably you have\nlearned this from CS229, right? So for linear model, if\nyou normalize this margin",
    "start": "909370",
    "end": "914560"
  },
  {
    "text": "with the norm of\nthe theta, then it's going to be the\ndistance between example to the linear separate\nto the hyperplane.",
    "start": "914560",
    "end": "923889"
  },
  {
    "text": "And the minimum margin would\nbe the minimum distance of all the examples\nto the hyperplane.",
    "start": "923890",
    "end": "932775"
  },
  {
    "text": "And our goal would\nbe something like-- basically, you are going\nto bound the generalization",
    "start": "932775",
    "end": "938640"
  },
  {
    "text": "error or the Rademacher-- I guess we bound the\ngeneralization error binded by the Rademacher complexity.",
    "start": "938640",
    "end": "944740"
  },
  {
    "text": "That's what we did in the past\nfor the Rademacher complexity and then this guy, some\nfunction of the parameter",
    "start": "944740",
    "end": "955120"
  },
  {
    "text": "and the parameter norm to the\nnormal theta and some function",
    "start": "955120",
    "end": "961610"
  },
  {
    "text": "of the margin. That's what we'll eventually\nget out of this lecture. ",
    "start": "961610",
    "end": "968890"
  },
  {
    "text": "And while we need to\ndefine these margins, the reason is that this\npartly come from a technique",
    "start": "968890",
    "end": "975140"
  },
  {
    "text": "to deal with the loss function. So we're going to introduce a\nsurrogate loss function that",
    "start": "975140",
    "end": "988340"
  },
  {
    "text": "takes in a margin-- that takes the\nmargin into account. ",
    "start": "988340",
    "end": "997290"
  },
  {
    "text": "I guess we-- at the end of all\nof this will be all together--",
    "start": "997290",
    "end": "1002470"
  },
  {
    "text": "intuitively, the\nreason we want to do this is that we somehow\nbelieve that margin matters for the generalization. Then you probably want\nto have a bounds that",
    "start": "1002470",
    "end": "1010330"
  },
  {
    "text": "depends on the margin. And you also have a\nloss function that also depends on the margin. So so far, if you\nlook at the 0-1 loss,",
    "start": "1010330",
    "end": "1016415"
  },
  {
    "text": "it doesn't depend on\nthe margin, right? How large h(x) is doesn't change\nyour 0-1 loss on an example,",
    "start": "1016415",
    "end": "1022709"
  },
  {
    "text": "right? So as long as the\nsign doesn't change, you don't really care, right? So we want something that\nkind of depends on the margin.",
    "start": "1022710",
    "end": "1032107"
  },
  {
    "text": "So the loss function\nis called ramp loss. I think sometimes it's also\ncalled just margin loss.",
    "start": "1032107",
    "end": "1038779"
  },
  {
    "text": " So this loss function\nhas a parameter, gamma.",
    "start": "1038779",
    "end": "1045890"
  },
  {
    "text": "And gamma is kind of\nlike the target margin, in some sense, or kind\nof a reference margin. You can think of it like that.",
    "start": "1045890",
    "end": "1051870"
  },
  {
    "text": "So this is the loss function\nthat takes into a single number t, and it outputs--",
    "start": "1051870",
    "end": "1058669"
  },
  {
    "text": "maybe let me draw it first. I guess maybe let's write\ndown the technical equation.",
    "start": "1058670",
    "end": "1064650"
  },
  {
    "text": "So if t is greater than gamma-- OK, let me draw it. So this function is a\nfunction that looks like this.",
    "start": "1064650",
    "end": "1074149"
  },
  {
    "start": "1074150",
    "end": "1079780"
  },
  {
    "text": "And here this is gamma,\nand here this is 1. So when t is larger\nthan gamma, you",
    "start": "1079780",
    "end": "1089000"
  },
  {
    "text": "make it 0 when t is equal to--",
    "start": "1089000",
    "end": "1094310"
  },
  {
    "text": "when t is less\nthan 0, you make it 1 that corresponds to the flat\narea on the left hand-side",
    "start": "1094310",
    "end": "1102200"
  },
  {
    "text": "of the origin. And then when you\nare between 0 and 1, you linearly interpolate.",
    "start": "1102200",
    "end": "1107840"
  },
  {
    "text": "So this is the way to linearly\ninterpolate is 1 minus t over gamma if you are\nbetween 0 and gamma.",
    "start": "1107840",
    "end": "1117671"
  },
  {
    "text": "Right. So this is the linear region. And why you are--",
    "start": "1117671",
    "end": "1123730"
  },
  {
    "text": "why you're interested in this? The reason is that\nthis is, in some sense, a extension of the 0-1 loss.",
    "start": "1123730",
    "end": "1131560"
  },
  {
    "text": "So maybe let me first\ndefine notation, so with a bit of use of notation. ",
    "start": "1131560",
    "end": "1146490"
  },
  {
    "text": "You can also write l gamma x, y. You look at the\nmargin loss applied",
    "start": "1146490",
    "end": "1151710"
  },
  {
    "text": "on this classifier\nh that's defined to be l gamma y times h of x.",
    "start": "1151710",
    "end": "1160049"
  },
  {
    "text": "This is the definition. But these two l gamma\nhave different meanings on the left hand-side and the\nright-hand side as you can see.",
    "start": "1160050",
    "end": "1167460"
  },
  {
    "text": "So this one is the\none we just defined. So basically, first\nof all, before,",
    "start": "1167460",
    "end": "1173010"
  },
  {
    "text": "when you talk about\nloss functions, right, it takes in two\narguments, y and y hat, right?",
    "start": "1173010",
    "end": "1178843"
  },
  {
    "text": "But for classification, the\nonly thing that matters is you take the product of them. So that's why you only\ncare about y times",
    "start": "1178843",
    "end": "1185100"
  },
  {
    "text": "of h times h of x, right? So basically, in\nthis notation, we",
    "start": "1185100",
    "end": "1190590"
  },
  {
    "text": "have noted the ideal loss\nfunction, l 0-1 loss function of x and y, which x is equals\nto some indicator of y times",
    "start": "1190590",
    "end": "1200400"
  },
  {
    "text": "h(x) is y times sign of h(x).",
    "start": "1200400",
    "end": "1206298"
  },
  {
    "text": "I guess that's the\nsame thing, right? So y times h(x) is\nlarger than 0, right?",
    "start": "1206298",
    "end": "1215390"
  },
  {
    "text": "So this is the 0. This is a different way to\nwrite 0-1 loss function, right? So if y and h(x) doesn't\nhave the wait, go back--",
    "start": "1215390",
    "end": "1224180"
  },
  {
    "text": "this is less than 0. ",
    "start": "1224180",
    "end": "1246745"
  },
  {
    "text": "Sorry, I have to mark that. I have a typo here. So that I can fix\nit for the future.",
    "start": "1246745",
    "end": "1252530"
  },
  {
    "text": "OK. So this is the binary\nclassification loss, and this is the\nso-called ramp loss.",
    "start": "1252530",
    "end": "1258710"
  },
  {
    "text": "And you can see the difference\nis that the indicator function would just look like this. ",
    "start": "1258710",
    "end": "1266630"
  },
  {
    "text": "This is indicator,\nt is less than 0. And what we do is that we\nextend-- we make this indicator",
    "start": "1266630",
    "end": "1273639"
  },
  {
    "text": "function more continuous. That's basically\nwhat we are doing. So in some-- and\nyou can know that.",
    "start": "1273640",
    "end": "1280580"
  },
  {
    "text": "So from this, you can see that\nthe l gamma y h(x) is always",
    "start": "1280580",
    "end": "1288769"
  },
  {
    "text": "bigger than the indicator\nof y h(x) is less than 0",
    "start": "1288770",
    "end": "1294020"
  },
  {
    "text": "just because the function\nabove [INAUDIBLE] is bigger pointwise than\nthe function below, right,",
    "start": "1294020",
    "end": "1302630"
  },
  {
    "text": "which means that if you look\nat the 0-1 loss of an example,",
    "start": "1302630",
    "end": "1308460"
  },
  {
    "text": "it's always less\nthan the ramp loss,",
    "start": "1308460",
    "end": "1314299"
  },
  {
    "text": "at an example, which means that\nif you look at the test loss-- right, if you take the\nexpectation over x,",
    "start": "1314300",
    "end": "1322700"
  },
  {
    "text": "y joined from p. ",
    "start": "1322700",
    "end": "1329860"
  },
  {
    "text": "All right. So this is the final thing\nyou really care about,",
    "start": "1329860",
    "end": "1335080"
  },
  {
    "text": "the test error, right? So this is the fundamental\nthing you care about. You can at least upper bound\nthis by the population error",
    "start": "1335080",
    "end": "1344250"
  },
  {
    "text": "under the ramp-- under the population\nloss under the ramp loss. ",
    "start": "1344250",
    "end": "1350910"
  },
  {
    "text": "Right. So by doing this,\nyou change the-- you make the loss bigger, right?",
    "start": "1350910",
    "end": "1356549"
  },
  {
    "text": "And then we're\ngoing to bound this. So basically, with\nthis, eventually, what we're going to\ndo is we're going",
    "start": "1356550",
    "end": "1362640"
  },
  {
    "text": "to bound the test error, the\ntest loss under the ramp loss,",
    "start": "1362640",
    "end": "1368600"
  },
  {
    "text": "which is the upper bound\non the binary loss. Right.",
    "start": "1368600",
    "end": "1374820"
  },
  {
    "text": "So this is our goal,\nupper bound this. ",
    "start": "1374820",
    "end": "1382570"
  },
  {
    "text": "OK. And how do we upper bound this? ",
    "start": "1382570",
    "end": "1390700"
  },
  {
    "text": "So I think it's\nkind of probably-- at least when I read this, at\nthe first time from a book, it's unclear why you want\nto do this continuation.",
    "start": "1390700",
    "end": "1398740"
  },
  {
    "text": "It will come just-- it will come in a moment. So one of the reasons is\nyou want to make a Lipschitz so that you can somehow\nget rid of the loss.",
    "start": "1398740",
    "end": "1406630"
  },
  {
    "text": "But before doing\nthat, we have to-- I guess, let's first clear\nup the hig-level thing first",
    "start": "1406630",
    "end": "1414430"
  },
  {
    "text": "and then let's look at the\nlow-level detail about how to use the loss. So the high-level\nplan would just",
    "start": "1414430",
    "end": "1420580"
  },
  {
    "text": "be that you let I hat gamma. This is the empirical loss\ncorresponding to the ramp loss.",
    "start": "1420580",
    "end": "1429340"
  },
  {
    "start": "1429340",
    "end": "1435850"
  },
  {
    "text": "And you can also define-- I think this is a function of h. ",
    "start": "1435850",
    "end": "1445350"
  },
  {
    "text": "You can define the\npopulation loss. As I said, is this\nl gamma x, y h.",
    "start": "1445350",
    "end": "1454240"
  },
  {
    "text": "And then if you use Rademacher\ncomplexity, the machinery we",
    "start": "1454240",
    "end": "1459900"
  },
  {
    "text": "have developed you\nget the population loss minus the empirical\nloss is bounded by 2 times",
    "start": "1459900",
    "end": "1475750"
  },
  {
    "text": "the empirical Rademacher\ncomplexity plus 3 times log 2 delta over n.",
    "start": "1475750",
    "end": "1482920"
  },
  {
    "text": "Right. This is what we did in the\nprevious lecture, right? The generalization\nbound can be bound. The generalization\nerror can be bounded",
    "start": "1482920",
    "end": "1489330"
  },
  {
    "text": "by the empirical Rademacher\ncomplexity, where f is this family of losses\ndefined by the ramp loss.",
    "start": "1489330",
    "end": "1499040"
  },
  {
    "start": "1499040",
    "end": "1504350"
  },
  {
    "text": "Right. So this is saying\nthat eventually you are going to just need to--",
    "start": "1504350",
    "end": "1510100"
  },
  {
    "text": "basically, this will be\nthe goal next, right? Because if you have the bound\non this Rademacher complexity,",
    "start": "1510100",
    "end": "1519289"
  },
  {
    "text": "you have the bound on the-- we are going to do this\nmore carefully after we",
    "start": "1519290",
    "end": "1524788"
  },
  {
    "text": "have the Rademacher complexity. But roughly speaking, once you\nhave the Rademacher complexity, you have an upper bound on\nthe population ramp loss.",
    "start": "1524788",
    "end": "1532710"
  },
  {
    "text": "And the population ramp loss\nupper bound the population binary loss. ",
    "start": "1532710",
    "end": "1542210"
  },
  {
    "text": "Right.  OK. [INAUDIBLE]",
    "start": "1542210",
    "end": "1549930"
  },
  {
    "text": "Where is the sup? [INAUDIBLE] Sure. So yes.",
    "start": "1549930",
    "end": "1557270"
  },
  {
    "text": "Yeah, but without sup,\nit's also true, I guess. Yeah. So I guess for\naverage, this is true.",
    "start": "1557270",
    "end": "1562669"
  },
  {
    "text": "All right. ",
    "start": "1562670",
    "end": "1569920"
  },
  {
    "text": "With high probability for\naverage, this is true. Technically.",
    "start": "1569920",
    "end": "1574998"
  },
  {
    "text": " OK. So now let's talk about\nthe Rademacher complexity.",
    "start": "1574998",
    "end": "1582100"
  },
  {
    "start": "1582100",
    "end": "1588270"
  },
  {
    "text": "And this review is why we\ncare about the ramp loss. So Rademacher\ncomplexity of our f",
    "start": "1588270",
    "end": "1596250"
  },
  {
    "text": "relates to the\nRademacher complexity of h in a pretty nice way. And here is the--",
    "start": "1596250",
    "end": "1602412"
  },
  {
    "text": "here's the lemma\nthat relates them. It's called Talagrand's lemma.",
    "start": "1602412",
    "end": "1607740"
  },
  {
    "start": "1607740",
    "end": "1613170"
  },
  {
    "text": "So seeing the following-- so\nsuppose you have a function of p--",
    "start": "1613170",
    "end": "1618500"
  },
  {
    "text": "it's a one-dimensional function. And it's a Lipschitz function,\nkappa Lipschitz function. ",
    "start": "1618500",
    "end": "1628680"
  },
  {
    "text": "I guess we have kind-- of\ndefine the Lipschitz function. So this really means\nthat if you have two--",
    "start": "1628680",
    "end": "1636150"
  },
  {
    "text": "any two numbers, p\nof x minus p of y is less than kappa x minus y.",
    "start": "1636150",
    "end": "1641850"
  },
  {
    "text": "And here is just absolute\nvalue because all everything is one dimensional, OK? And once you have\nthis, then you can",
    "start": "1641850",
    "end": "1649140"
  },
  {
    "text": "look at the composition of\nthis one-dimensional function with any hypothesis class.",
    "start": "1649140",
    "end": "1656159"
  },
  {
    "text": "So this is defined to\nbe-- you compose phi.",
    "start": "1656160",
    "end": "1662370"
  },
  {
    "text": "So basically, you map phi to\nthe composition of p of h of z,",
    "start": "1662370",
    "end": "1668970"
  },
  {
    "text": "right. So this is the mapping, right? This is a-- phi will be the\nloss function basically.",
    "start": "1668970",
    "end": "1676460"
  },
  {
    "text": "So here is abstract. So you can compose\nany function of phi with the hypothesis class.",
    "start": "1676460",
    "end": "1681660"
  },
  {
    "text": "And to get this phi,\ncompose with h and--",
    "start": "1681660",
    "end": "1686750"
  },
  {
    "text": "why I'm changing the color? Anyway, so then you\ncan get what you--",
    "start": "1686750",
    "end": "1691760"
  },
  {
    "text": "then what you can get is\nthat the composition-- the composed hypothesis\nclass is bounded by kappa times Rs over H.",
    "start": "1691760",
    "end": "1703355"
  },
  {
    "text": "So basically, they're saying\nthat if you compose anything on top of the existing\nhypothesis class,",
    "start": "1703355",
    "end": "1708370"
  },
  {
    "text": "if what you composed with\nthe p function is Lipschitz, then you just only blow\nup the effect of kappa",
    "start": "1708370",
    "end": "1714700"
  },
  {
    "text": "by the Lipschitzness. And so with this,\nyou can probably see why we care about\nrelaxing the binary function",
    "start": "1714700",
    "end": "1722980"
  },
  {
    "text": "because the indicator of\nfunction is now Lipschitz. But if you use the ramp\nfunction, it will be Lipschitz.",
    "start": "1722980",
    "end": "1728860"
  },
  {
    "text": "And that's what we do next. By the way, this theorem is-- this lemma doesn't have\na very simple proof.",
    "start": "1728860",
    "end": "1739840"
  },
  {
    "text": "We're not going to\nprove it in the lecture. It does require some--",
    "start": "1739840",
    "end": "1745030"
  },
  {
    "text": "it's kind of like something,\nkind of like pretty-- in my own opinion, it's kind\nof pretty novel and deep",
    "start": "1745030",
    "end": "1753610"
  },
  {
    "text": "to me, like I-- I used to be able to prove\nit on-- like I prove it once myself.",
    "start": "1753610",
    "end": "1759160"
  },
  {
    "text": "But I think all the\nexisting proofs I know is kind of like somewhat a\nlittle bit mysterious to me.",
    "start": "1759160",
    "end": "1768840"
  },
  {
    "text": "But the high-level--\nthe intuition probably is reasonable because\nyou have a hypothesis class.",
    "start": "1768840",
    "end": "1775530"
  },
  {
    "text": "You compose it with\nsomething that doesn't really introduce additional\nfluctuation that much, right?",
    "start": "1775530",
    "end": "1781320"
  },
  {
    "text": "So that's why you don't\nmake the hypothesis class",
    "start": "1781320",
    "end": "1786600"
  },
  {
    "text": "much more complicated. But if you look at the-- kind\nof like the-- if you look",
    "start": "1786600",
    "end": "1793289"
  },
  {
    "text": "at exactly what this formula\nis saying-- this is saying that you take the sup\nof h in H. So how do you",
    "start": "1793290",
    "end": "1801580"
  },
  {
    "text": "write the left-hand side\nis something like this.  sigma i p of h of Zi.",
    "start": "1801580",
    "end": "1814409"
  },
  {
    "text": "And we want to show that\nthis is bounded by h Zi.",
    "start": "1814410",
    "end": "1825910"
  },
  {
    "text": "Right. That's the goal. That's what this thing is saying\nbecause some would kind of like",
    "start": "1825910",
    "end": "1832889"
  },
  {
    "text": "imagine why this is difficult to\nprove because you cannot really",
    "start": "1832890",
    "end": "1839110"
  },
  {
    "text": "change the order of\nexpectation with sup. If you do that, you make\nthe inequality to lose.",
    "start": "1839110",
    "end": "1844690"
  },
  {
    "text": "And somehow there's\na phi somewhere in the middle of this equation. It's kind of very, very\nhard to pull it off.",
    "start": "1844690",
    "end": "1851230"
  },
  {
    "text": "Anyway, this is just my personal\ncomment about this dilemma. It sounds pretty deep to me.",
    "start": "1851230",
    "end": "1857604"
  },
  {
    "text": "OK. Anyway, so we're going to\ntake-- we're going to use this. And I think it's\nprobably somewhat--",
    "start": "1857604",
    "end": "1867520"
  },
  {
    "text": "I mean, obvious\nhow do we use it? We are going to take this phi\nfunction to be the ramp loss.",
    "start": "1867520",
    "end": "1873300"
  },
  {
    "text": "Right. So l gamma t. Then because the ramp loss--",
    "start": "1873300",
    "end": "1878360"
  },
  {
    "text": "I guess let's go back to here. So the ramp loss is\nthe Lipschitz function.",
    "start": "1878360",
    "end": "1883650"
  },
  {
    "text": "The Lipschitz constant\ndepends on gamma because here the\nLipschitz constant's 0. Here is completely flat.",
    "start": "1883650",
    "end": "1889860"
  },
  {
    "text": "So how the Lipschitz\nstays depends on what's the slope here. And the slope there\nis 1 over gamma.",
    "start": "1889860",
    "end": "1895890"
  },
  {
    "text": "Because this is\ngamma and this is 1, so the slope here\nis 1 over gamma. So the Lipschitzness\nof the ramp loss is--",
    "start": "1895890",
    "end": "1903855"
  },
  {
    "start": "1903855",
    "end": "1909870"
  },
  {
    "text": "of phi is equal to 1 over gamma. Right.",
    "start": "1909870",
    "end": "1916110"
  },
  {
    "text": "And if you take-- ",
    "start": "1916110",
    "end": "1926840"
  },
  {
    "text": "so if you take H to be--  I guess let's take\nH prime to be this.",
    "start": "1926840",
    "end": "1933720"
  },
  {
    "text": "You map x, y to y times h(x),\nwhere h is in H. H prime",
    "start": "1933720",
    "end": "1940757"
  },
  {
    "text": "is still not exactly the\nsame as h because there's a y multiplied with h.",
    "start": "1940757",
    "end": "1946169"
  },
  {
    "text": "And then you take f to be\nphi composed with H prime.",
    "start": "1946170",
    "end": "1952310"
  },
  {
    "text": "And then by the Talagrand's\nlemma, what you have",
    "start": "1952310",
    "end": "1963170"
  },
  {
    "text": "is that the Rademacher\ncomplex of f, which is what we care\nabout, is less than 1",
    "start": "1963170",
    "end": "1968179"
  },
  {
    "text": "over gamma the Lipschitzness\ntimes Rs of H prime.",
    "start": "1968180",
    "end": "1975330"
  },
  {
    "text": "OK. So we kind of get rid of the\neffect of the loss function",
    "start": "1975330",
    "end": "1981049"
  },
  {
    "text": "by using this Talagrand's lemma. And then you can also relate\nH prime to h much easier now",
    "start": "1981050",
    "end": "1987800"
  },
  {
    "text": "because H prime and h--\nwhat's the difference? The only difference\nis you have a sample. Rademacher complexity is not\nvery sensitive to sampling.",
    "start": "1987800",
    "end": "1996200"
  },
  {
    "text": "This is just because Rs-- I guess we have\ndone this before.",
    "start": "1996200",
    "end": "2001270"
  },
  {
    "text": "At least interpolating\nsome other proof, right? So the Rademacher complex of H\nprime is something like this.",
    "start": "2001270",
    "end": "2008170"
  },
  {
    "text": " sigma i yi yi times h xi.",
    "start": "2008170",
    "end": "2025100"
  },
  {
    "text": "That's using the definition. And now you look at this. sigma i yi has the\nsame distribution.",
    "start": "2025100",
    "end": "2034328"
  },
  {
    "text": "i's y i, so sigma i, because\nanyway you flip it, right.",
    "start": "2034328",
    "end": "2042380"
  },
  {
    "text": "So that's why this\nis equals to-- ",
    "start": "2042380",
    "end": "2047950"
  },
  {
    "text": "you can basically\nget rid of the yi. You get just this.",
    "start": "2047950",
    "end": "2054250"
  },
  {
    "text": "And then the\n[INAUDIBLE] constant is the Rademacher\ncomplexity of h. ",
    "start": "2054250",
    "end": "2069310"
  },
  {
    "text": "OK. So with all of this,\nwhat we got is that-- so this is basically\ncombining these two things.",
    "start": "2069310",
    "end": "2075629"
  },
  {
    "text": "What we got is the Rs of F\nis less than 1 over gamma times Rs of H.",
    "start": "2075630",
    "end": "2082882"
  },
  {
    "text": "And you can see that the\ninteresting thing is-- first of all, the loss is gone. And second, y is\nalso gone, right?",
    "start": "2082882",
    "end": "2089010"
  },
  {
    "text": "So you don't have any y's in\nthe right-hand side anymore. So basically, at the\nend of-- the only thing",
    "start": "2089010",
    "end": "2095580"
  },
  {
    "text": "that matters now is the h(x). ",
    "start": "2095580",
    "end": "2105110"
  },
  {
    "text": "OK. And with this, we can put\nthis-- all of this thing",
    "start": "2105110",
    "end": "2110420"
  },
  {
    "text": "together to get a [INAUDIBLE]\nbound on a binary test error. So recall that we assume--",
    "start": "2110420",
    "end": "2117705"
  },
  {
    "text": " the perfect classification will\nassume that yi times h of xi",
    "start": "2117705",
    "end": "2126380"
  },
  {
    "text": "is bigger than 0 for every i\nwill assume a perfect fitting.",
    "start": "2126380",
    "end": "2132410"
  },
  {
    "text": "Perfect. Good. And then you can take\ngamma min to be.--",
    "start": "2132410",
    "end": "2141955"
  },
  {
    "start": "2141955",
    "end": "2147460"
  },
  {
    "text": "right. So this is the-- so let me see why I'm using--",
    "start": "2147460",
    "end": "2155900"
  },
  {
    "text": "so this gamma min\nis the empirical is the minimum margin\nfor this data set, right?",
    "start": "2155900",
    "end": "2164410"
  },
  {
    "text": "So now, you have that,\nI guess, in technical--",
    "start": "2164410",
    "end": "2175799"
  },
  {
    "text": "let's see what's-- I actually have a\ntypo here, sorry.",
    "start": "2175800",
    "end": "2180850"
  },
  {
    "text": "So let's just call this gamma. We use gamma to\ndefine this, right? So then if you look at L\nhat gamma h, this is what--",
    "start": "2180850",
    "end": "2192970"
  },
  {
    "text": "this is going to be-- I think it's going to be 0\nbecause you have l gamma y i",
    "start": "2192970",
    "end": "2199610"
  },
  {
    "text": "h(x) i.  And y hi is always bigger\nthan gamma and recall",
    "start": "2199610",
    "end": "2206349"
  },
  {
    "text": "that in this ramp loss, if you\nare bigger than gamma, then you are 0. So basically, every example has\nzero loss under the ramp loss.",
    "start": "2206350",
    "end": "2215930"
  },
  {
    "text": "So for the training\nexample is the binary loss and the rent plus\nare not different because they are both 0.",
    "start": "2215930",
    "end": "2223269"
  },
  {
    "text": "And therefore, you can\nhave the following sequence",
    "start": "2223270",
    "end": "2229270"
  },
  {
    "text": "of inequality. So you first bound to the 0\n1 loss of h by the ramp loss.",
    "start": "2229270",
    "end": "2235430"
  },
  {
    "text": "This is because the ramp\nloss is always better than-- larger than 0-1 loss. And then you say\nthat this is smaller",
    "start": "2235430",
    "end": "2243530"
  },
  {
    "text": "than I hat gamma age\nplus the Rademacher",
    "start": "2243530",
    "end": "2249320"
  },
  {
    "text": "complexity plus something\nlike, O Rs H over--",
    "start": "2249320",
    "end": "2255920"
  },
  {
    "text": "let's do it a\nlittle bit slowly so the Rademacher of\ncomplexity of f-- ",
    "start": "2255920",
    "end": "2270370"
  },
  {
    "text": "plus some square\nlog 2 over delta n. And then you use the\ninequality between f and h.",
    "start": "2270370",
    "end": "2283710"
  },
  {
    "text": "So you get Rs of h over gamma-- plus square root log\n2 over delta over n.",
    "start": "2283710",
    "end": "2291119"
  },
  {
    "text": "And then this one\nis 0 as we claimed because this is the\nempirical ramp loss for the--",
    "start": "2291120",
    "end": "2298890"
  },
  {
    "text": "from the training data. So this becomes 0. So you got just the big\nO of Rs of H over gamma.",
    "start": "2298890",
    "end": "2308740"
  },
  {
    "text": "Right. So gamma right, so, gamma. ",
    "start": "2308740",
    "end": "2315650"
  },
  {
    "text": "So there's a caveat with\nwith this inequality.",
    "start": "2315650",
    "end": "2324319"
  },
  {
    "text": "I'm not sure whether any\nof you have noticed that. But if you have noticed\nthat, maybe hold on.",
    "start": "2324320",
    "end": "2329740"
  },
  {
    "text": "For a second, let's\nfirst somewhat interpret. ",
    "start": "2329740",
    "end": "2336700"
  },
  {
    "text": "OK, maybe let's just\nexpand this notation. So the problem-- so\nwhat's the caveat here? There's actually a mistake in\nsome sense, not a serious one,",
    "start": "2336700",
    "end": "2344910"
  },
  {
    "text": "but there is an issue\nwith this derivation.",
    "start": "2344910",
    "end": "2350220"
  },
  {
    "text": "The reason is that-- what is the definition of gamma?",
    "start": "2350220",
    "end": "2356130"
  },
  {
    "text": "Right. So here the definition of\ngamma depends on the data. And then you just mash up all\nthe independents, all the--",
    "start": "2356130",
    "end": "2365790"
  },
  {
    "text": "so if gamma-- when we do\nall of these things before-- so gamma is a constant.",
    "start": "2365790",
    "end": "2371550"
  },
  {
    "text": "And then you have\nthe gamma first, and then you draw\nyour data points. You have your Rademacher\ncomplexity, so and so forth,",
    "start": "2371550",
    "end": "2378650"
  },
  {
    "text": "right? So but here we take the gamma\nto be something that depends on the data which will break\nall the Rademacher complexity",
    "start": "2378650",
    "end": "2388190"
  },
  {
    "text": "machinery because in a\nRademacher complexity machinery, you cannot let your\nloss function or your function",
    "start": "2388190",
    "end": "2393350"
  },
  {
    "text": "class depend on data, right? So theory-- the function\nclass F, this cannot be--",
    "start": "2393350",
    "end": "2402410"
  },
  {
    "text": "this cannot depend on data.",
    "start": "2402410",
    "end": "2408799"
  },
  {
    "text": "Right? So we did want to deal with\na uniform convergence, the h hat, the classifier, the\nfinal classifier you bound,",
    "start": "2408800",
    "end": "2415580"
  },
  {
    "text": "can't depend on data. That's the benefit of\nuniform convergence, but the function class\nf cannot depend on data.",
    "start": "2415580",
    "end": "2422970"
  },
  {
    "text": "So that's the small caveat. But this is not a very big deal. But if you choo--",
    "start": "2422970",
    "end": "2428630"
  },
  {
    "text": "OK. But if you choose gamma to be\nsomething that depends on data, then your function\nclass depends on data. And then you break this way.",
    "start": "2428630",
    "end": "2435590"
  },
  {
    "text": " But I think-- I'm not going to deal\nwith this very formally",
    "start": "2435590",
    "end": "2442430"
  },
  {
    "text": "just because this\nis not a super-- for mathematical rigor, of\ncourse, you can do this,",
    "start": "2442430",
    "end": "2449600"
  },
  {
    "text": "but it's relatively easy to fix. The way to fix it is that\nyou do another union bound",
    "start": "2449600",
    "end": "2464430"
  },
  {
    "text": "on the choice of gamma.",
    "start": "2464430",
    "end": "2470240"
  },
  {
    "text": "So now you choose gamma\nto be the minimum margin, depending on data.",
    "start": "2470240",
    "end": "2475640"
  },
  {
    "text": "But what you should do\nis you should also prove this for every gamma, right? So if you can prove this\ninequality for every gamma,",
    "start": "2475640",
    "end": "2482780"
  },
  {
    "text": "this bunch of inequalites. This is, of course, here. For every gamma of, I guess--",
    "start": "2482780",
    "end": "2490350"
  },
  {
    "text": "you can prove it until\nhere for every gamma. And then in the last type,\nyou can choose the gamma",
    "start": "2490350",
    "end": "2495630"
  },
  {
    "text": "to be the one that you wanted\nbecause you're already done with the Rademacher complexity. So you just plug in whatever\ngamma you want, right?",
    "start": "2495630",
    "end": "2504210"
  },
  {
    "text": "And the way to do it is\nactually relatively easy. Roughly speaking, what\nyou do is you say-- you look at the-- gamma\nis a single number, right?",
    "start": "2504210",
    "end": "2510940"
  },
  {
    "text": "So to do uniform convergence\nover a single parameter is always relatively easy. And after here, it's\neven easier because you",
    "start": "2510940",
    "end": "2518000"
  },
  {
    "text": "don't have to care about\nmultiplicative bounds. So suppose you have a\nbound on what's the largest",
    "start": "2518000",
    "end": "2523589"
  },
  {
    "text": "possible gamma you have. Let's say, you have\nB. Then what you can do is you can discretize\nthis into multiple buckets,",
    "start": "2523590",
    "end": "2532307"
  },
  {
    "text": "something like this. Maybe you can-- so you can have\none bucket is B over 2 to B. Another bucket is B\nover 4 to 2 B over 2.",
    "start": "2532307",
    "end": "2540970"
  },
  {
    "text": "And you prove this for every\npoint in this discretization.",
    "start": "2540970",
    "end": "2546210"
  },
  {
    "text": "It would discretize-- so\nbasically, within every bucket, you just don't really\nchange much, right? The only difference between\ntwo numbers in the same bucket",
    "start": "2546210",
    "end": "2554250"
  },
  {
    "text": "is only a factor of 2. So at most, you\nlose a factor of 2. So basically, this is\nsaying that you only",
    "start": "2554250",
    "end": "2561000"
  },
  {
    "text": "have to show a bounds for those\npoints between each bucket,",
    "start": "2561000",
    "end": "2568390"
  },
  {
    "text": "right? Those kind of boundary points. And how many points they are? There are only log B\npoints, in some sense.",
    "start": "2568390",
    "end": "2574960"
  },
  {
    "text": "And you can also do a\nuniform convergence. So all of these points\nactually get even technical. You can even get\nlog-log B dependency.",
    "start": "2574960",
    "end": "2582330"
  },
  {
    "text": "But anyway, so this\nis the rough idea about how to do this last\nstep of uniform convergence.",
    "start": "2582330",
    "end": "2591090"
  },
  {
    "text": "Because it's relatively easy\nif you look at the papers, I think most of the papers\ndon't actually do this step just",
    "start": "2591090",
    "end": "2600540"
  },
  {
    "text": "for simplicity. Of course, the state of\ntheorem in a different way so that the theorem\nis still correct.",
    "start": "2600540",
    "end": "2605730"
  },
  {
    "text": "So they just don't do\nthis very, very last step",
    "start": "2605730",
    "end": "2610740"
  },
  {
    "text": "to make it simpler. So, yeah. So that's also what\nI'm going to do.",
    "start": "2610740",
    "end": "2616230"
  },
  {
    "text": "I'm not going to prove with you\nlike a super rigorous theorem.",
    "start": "2616230",
    "end": "2622407"
  },
  {
    "text": "I guess, if you really\nknow how to prove it, what's going to happen\nis the following. So suppose you really\nwant to have a theorem.",
    "start": "2622407",
    "end": "2628150"
  },
  {
    "text": "The theorem statement\nwill look like this. So it will say that\nwith probability larger than 1 minus\ndelta for every--",
    "start": "2628150",
    "end": "2636150"
  },
  {
    "text": " so for every gamma between\nsome 0 and gamma max, then",
    "start": "2636150",
    "end": "2644850"
  },
  {
    "text": "you can say that for every h, L\ngamma h less than L hat gamma h",
    "start": "2644850",
    "end": "2651660"
  },
  {
    "text": "plus some big O of this\nplus square root with log",
    "start": "2651660",
    "end": "2658510"
  },
  {
    "text": "1 over delta over n plus\nsquare root log gamma",
    "start": "2658510",
    "end": "2664850"
  },
  {
    "text": "max over something like this. I guess maybe gamma max should\nbe larger than 1 so that you",
    "start": "2664850",
    "end": "2672807"
  },
  {
    "text": "don't, yeah.. ",
    "start": "2672808",
    "end": "2679300"
  },
  {
    "text": "And as a corollary, you\ncan have L01 h, which",
    "start": "2679300",
    "end": "2688180"
  },
  {
    "text": "had for the hypothesis\nyou care about, right, is less than something\nlike O of Rs of H over r gamma",
    "start": "2688180",
    "end": "2701850"
  },
  {
    "text": "plus square root of log\n1 over delta over n. So here this is the\nempirical gamma.",
    "start": "2701850",
    "end": "2710390"
  },
  {
    "text": "Maybe I should\ncall it gamma mean. I think I somehow have a\nlittle bit inconsistent notations here, sorry.",
    "start": "2710390",
    "end": "2716329"
  },
  {
    "text": "So this is min over i y.",
    "start": "2716330",
    "end": "2722650"
  },
  {
    "start": "2722650",
    "end": "2734309"
  },
  {
    "text": "OK?  Can you [INAUDIBLE] for\ngammas [INAUDIBLE] so you",
    "start": "2734310",
    "end": "2740488"
  },
  {
    "text": "can make [INAUDIBLE]\nbut then so like, you just have a max equal 1\nso that last one [INAUDIBLE]",
    "start": "2740488",
    "end": "2764950"
  },
  {
    "text": "OK. I guess-- I think the question\nis whether why don't take gamma",
    "start": "2764950",
    "end": "2772230"
  },
  {
    "text": "max to be really small, right? So first of all, it's not\nclear whether you can always prove that the final gamma\nyou have on the empirical data",
    "start": "2772230",
    "end": "2781200"
  },
  {
    "text": "can be really small. That's probably not-- actually,\nyou want the gamma to be big. And you want the empirical\ndata to have bigger margin",
    "start": "2781200",
    "end": "2787320"
  },
  {
    "text": "so that your generalization\nis smaller, right? So you do want to make\nsomehow the gamma like--",
    "start": "2787320",
    "end": "2793260"
  },
  {
    "text": "at least that's the\ninteresting regime. This very, very\nsmall gamma regime is probably not the\nmost interesting one because your bound would be--",
    "start": "2793260",
    "end": "2799800"
  },
  {
    "text": "[INAUDIBLE] Your right-hand side\nwould be very big. So actually, if the gamma\nis really, really small,",
    "start": "2799800",
    "end": "2806252"
  },
  {
    "text": "you probably don't\nneed the third-- oh, I'm sorry. I think I know, sorry, my bad. There is a third\nterm in this as well.",
    "start": "2806252",
    "end": "2813320"
  },
  {
    "text": "Let me fix that first. But suppose your gamma is\nreally, really small, right?",
    "start": "2813320",
    "end": "2818357"
  },
  {
    "text": "So you probably don't\neven need a third term because the first term is\nalready very, very big. That already kind of governs\nyour generalization bound.",
    "start": "2818357",
    "end": "2826309"
  },
  {
    "text": "So you do care about\nsomewhat a large gamma. But there's still a question\nabout why you want gamma--",
    "start": "2826310",
    "end": "2832495"
  },
  {
    "text": "what if all the scales are\nvery, very small, right? So I think it's\nreally just that-- ",
    "start": "2832495",
    "end": "2843920"
  },
  {
    "text": "I think technically you-- let me see. So does that answer\nthe question?",
    "start": "2843920",
    "end": "2850911"
  },
  {
    "text": "It did. [INAUDIBLE]",
    "start": "2850912",
    "end": "2858170"
  },
  {
    "text": "Yeah. Yeah I think there are\nsome kind of small things--",
    "start": "2858170",
    "end": "2866170"
  },
  {
    "text": "for example, what\nif your everything is your super small, right? What if all the numbers\nare extremely small?",
    "start": "2866170",
    "end": "2873370"
  },
  {
    "text": "I think you can make this\nbound a little bit tighter in some ways.",
    "start": "2873370",
    "end": "2878470"
  },
  {
    "text": "Yeah. I think there's\nanother question.",
    "start": "2878470",
    "end": "2884410"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2884410",
    "end": "2891559"
  },
  {
    "text": "This one? Oh, this is a log gamma max. ",
    "start": "2891560",
    "end": "2900154"
  },
  {
    "text": "And the same thing here. ",
    "start": "2900155",
    "end": "2905700"
  },
  {
    "text": "Yep. OK, so I guess\n[INAUDIBLE]---- generally, I don't recommend to spend too\nmuch time thinking about this",
    "start": "2905700",
    "end": "2912240"
  },
  {
    "text": "small subtlety here. The most important thing is\nto do the first term, right? So let's try to--",
    "start": "2912240",
    "end": "2918000"
  },
  {
    "text": "so I guess maybe\nthe interpretation is more important. So the first term, this is\nRs of h over gamma, right?",
    "start": "2918000",
    "end": "2927675"
  },
  {
    "text": "Gamma min, where gamma min,\nthis is the empirical margin",
    "start": "2927675",
    "end": "2934390"
  },
  {
    "text": "of the entire data set. And this is saying that if you\nare very confident about all",
    "start": "2934390",
    "end": "2941073"
  },
  {
    "text": "the training\nexamples, then you're going to have better\noptimization bound. Your bound will be smaller. Or if all the examples are\nvery, very close to be--",
    "start": "2941073",
    "end": "2950559"
  },
  {
    "text": "all the training examples\nare close to zero, right? This is min i, yi, h xi.",
    "start": "2950560",
    "end": "2955990"
  },
  {
    "text": "So if all the H outputs\non your training examples",
    "start": "2955990",
    "end": "2961130"
  },
  {
    "text": "are very close to 0, that\nmeans your gamma max-- Actually, only one of\nthem in this definition.",
    "start": "2961130",
    "end": "2966640"
  },
  {
    "text": "As long as one of your-- example, chain example,\nhas a very small h value,",
    "start": "2966640",
    "end": "2974720"
  },
  {
    "text": "has a value very close to 0,\nthen your generalization bound would be quite worse.",
    "start": "2974720",
    "end": "2982130"
  },
  {
    "text": "So you want all the examples\nto be very far away from 0, like very confident\nin some sense.",
    "start": "2982130",
    "end": "2987830"
  },
  {
    "text": "And, on the other hand,\nyou want your classifier-- you want the denominator,\nthe numerator",
    "start": "2987830",
    "end": "2993244"
  },
  {
    "text": "to be as small as possible. You want your\nclassifier to output--",
    "start": "2993245",
    "end": "3000195"
  },
  {
    "text": "to be less complex. And also, there's another\nthing to check here, which is the scale does match.",
    "start": "3000195",
    "end": "3006040"
  },
  {
    "text": "So the scaling actually\nis the right thing. So for example, we have\ntalked about that Rademacher",
    "start": "3006040",
    "end": "3013230"
  },
  {
    "text": "complexity depends on the\nscale of your function. Right? So if you say you multiply\nall your function by a half,",
    "start": "3013230",
    "end": "3020040"
  },
  {
    "text": "then your Rademacher complexity\nwill be reduced by half. And you can see here that this\nbound makes sense because you",
    "start": "3020040",
    "end": "3026849"
  },
  {
    "text": "cannot cheat by doing that. So if you multiply\nyour function h-- sorry.",
    "start": "3026850",
    "end": "3033980"
  },
  {
    "text": "So suppose you say\nyou have a h prime, which is all the functions\ndivided, let's say,",
    "start": "3033980",
    "end": "3043329"
  },
  {
    "text": "100 h, Right? Suppose you divide by 100. Then what happens is that the\nRademacher complexity of h",
    "start": "3043330",
    "end": "3050950"
  },
  {
    "text": "prime indeed it's\ndivided by 100. But the gamma min will\nalso be divided by 100.",
    "start": "3050950",
    "end": "3059400"
  },
  {
    "text": "So that's why you cannot cheat\nthese bounds by a trivial rescaling of your\nhypothesis class.",
    "start": "3059400",
    "end": "3064410"
  },
  {
    "text": "And that's also kind of shows\nthat something like this will have to show\nup here, right? If you don't have this,\nyou only have R's of h,",
    "start": "3064410",
    "end": "3071059"
  },
  {
    "text": "then your bound\nwouldn't be right. Right? Because your bound wouldn't\nbe variant to scale it.",
    "start": "3071060",
    "end": "3076849"
  },
  {
    "text": " So basically, I'm saying that\nthis is invariant scaling.",
    "start": "3076850",
    "end": "3084670"
  },
  {
    "start": "3084670",
    "end": "3095933"
  },
  {
    "text": "OK? So this basically concludes with\nour treatment about the loss.",
    "start": "3095934",
    "end": "3104980"
  },
  {
    "text": "So basically, the take\nhome is that you have-- [INAUDIBLE] what is the margin? And the other is the\nRademacher complexity.",
    "start": "3104980",
    "end": "3112410"
  },
  {
    "text": "And now let's bound the\nRademacher complexity",
    "start": "3112410",
    "end": "3122905"
  },
  {
    "text": "for linear models. ",
    "start": "3122905",
    "end": "3128942"
  },
  {
    "text": "So what I'm going to\ndo is that I'm going to do the linear model today. And next lecture, we'll talk\nabout generally deep learning.",
    "start": "3128942",
    "end": "3136340"
  },
  {
    "text": "And from next\nlecture, we're going to talk more about nonlinear\nmodels, in general. So I'm going to have first have\na overview of our deep learning",
    "start": "3136340",
    "end": "3143210"
  },
  {
    "text": "and then come back to this\nto talk about the Rademacher complexity of nonlinear models. That's the high-level plan.",
    "start": "3143210",
    "end": "3150240"
  },
  {
    "text": "So for linear models,\nhere is a theorem. ",
    "start": "3150240",
    "end": "3156200"
  },
  {
    "text": "So suppose you have\na hypothesis class h which maps x to w\ntranspose x, where",
    "start": "3156200",
    "end": "3163780"
  },
  {
    "text": "w is now your parameter,\nunless your parameter w has 2 norm less than\nB. And also let's",
    "start": "3163780",
    "end": "3173460"
  },
  {
    "text": "assume the data\ndistribution has bounded--",
    "start": "3173460",
    "end": "3189099"
  },
  {
    "text": "L2 norm has a bound, right? So this is the L2 norm\nsquare and expectation",
    "start": "3189100",
    "end": "3194109"
  },
  {
    "text": "is bounded by C square. Suppose you know\nthese two things, then you can bound the\nRademacher complexity.",
    "start": "3194110",
    "end": "3200500"
  },
  {
    "text": " The empirical\nRademacher complexity",
    "start": "3200500",
    "end": "3205610"
  },
  {
    "text": "is bounded by B over n times\nsquare root sum of x i.",
    "start": "3205610",
    "end": "3213890"
  },
  {
    "text": " So I guess this is\nnot immediately.",
    "start": "3213890",
    "end": "3220760"
  },
  {
    "text": "No. The scale is on the\nright dimension. But I think the average\none is easier to interpret.",
    "start": "3220760",
    "end": "3227599"
  },
  {
    "text": "If you look at the average\nRademacher complexity, you can bound this by B times\nC over square root of n.",
    "start": "3227600",
    "end": "3235529"
  },
  {
    "text": "So first of all, you get the\nsquare root independency, which is very typical for\nRademacher complexity bound.",
    "start": "3235530",
    "end": "3242030"
  },
  {
    "text": "And second, in\nthe numerator, you got B, which is the bound of\nthe L2 norm of the parameter.",
    "start": "3242030",
    "end": "3248270"
  },
  {
    "text": "I also get a C\nwhich is basically talking about how large\nyour data is, what's",
    "start": "3248270",
    "end": "3254710"
  },
  {
    "text": "the norm of data points, right? So you should have both of\nthese two things come into play",
    "start": "3254710",
    "end": "3260710"
  },
  {
    "text": "because, again,\nRademacher complexity is sensitive to scale. So you shouldn't be-- so you\nshould have all the scaling",
    "start": "3260710",
    "end": "3267730"
  },
  {
    "text": "things there because otherwise,\nyou can cheat, right? So like for example, what\nif you don't have C here",
    "start": "3267730",
    "end": "3273400"
  },
  {
    "text": "then your bound wouldn't be\ntrue because you can scale your x arbitrarily to\nmake the Rademacher complexity arbitrarily big.",
    "start": "3273400",
    "end": "3279850"
  },
  {
    "text": "So you have to have all\nthe scalings all right. So right.",
    "start": "3279850",
    "end": "3287660"
  },
  {
    "text": "So that's the-- the\nfirst thing we're going to show up our linear models.",
    "start": "3287660",
    "end": "3293660"
  },
  {
    "text": "We're going to have some other\ntheorems about linear models under other constraints,\nand then we're going to compare them\nand also compare them",
    "start": "3293660",
    "end": "3300110"
  },
  {
    "text": "with the previous bound as well. But let's first\nprove the theorem. ",
    "start": "3300110",
    "end": "3307760"
  },
  {
    "text": "And this kind of\nalso demonstrates how do you generally bound\nRademacher complexity using",
    "start": "3307760",
    "end": "3313490"
  },
  {
    "text": "this kind of like a somewhat\nanalytical approach. So we start with the empirical\none, empirical Rademacher",
    "start": "3313490",
    "end": "3323240"
  },
  {
    "text": "complexity. The definition is\nthat you draw some-- you draw some sigmas.",
    "start": "3323240",
    "end": "3328430"
  },
  {
    "text": "And then you look at\nthe sup of this sum.",
    "start": "3328430",
    "end": "3333770"
  },
  {
    "text": "And here you write\nw transpose Xi because this is\nthe model output.",
    "start": "3333770",
    "end": "3341150"
  },
  {
    "text": "And you take sup over w and\nwhat's the constraint of w? The constraint of w is that\nL2 norm is less than B.",
    "start": "3341150",
    "end": "3351060"
  },
  {
    "text": "And now, let's do\nsome derivations.",
    "start": "3351060",
    "end": "3356820"
  },
  {
    "text": "So first-- so we basically\nwant to solve the sup first.",
    "start": "3356820",
    "end": "3362220"
  },
  {
    "text": "So to solve this sup,\nI want to understand what this thing is, right?",
    "start": "3362220",
    "end": "3369530"
  },
  {
    "text": "Though we realize\nthat this is actually a linear function of w. And you can write this\nas the inner product of w",
    "start": "3369530",
    "end": "3378180"
  },
  {
    "text": "with the sum of sigma i. Xi.",
    "start": "3378180",
    "end": "3383940"
  },
  {
    "text": "This is just because you can\npull the linear term in front [INAUDIBLE].",
    "start": "3383940",
    "end": "3389047"
  },
  {
    "text": " And now, what's the sup of this?",
    "start": "3389048",
    "end": "3395895"
  },
  {
    "text": "This is easy because I guess--",
    "start": "3395895",
    "end": "3403330"
  },
  {
    "text": "what is the sup of w in a\nproduct with some vector that's quite, say v where you\ntake sup over 2 norm of w",
    "start": "3403330",
    "end": "3413140"
  },
  {
    "text": "less than B, right? So basically, we want\nto find the vector that has maximum correlation\nwith some vector v. And you",
    "start": "3413140",
    "end": "3420670"
  },
  {
    "text": "have some constraint on\nthe norm of v, right? So this is just\nliterally equals to--",
    "start": "3420670",
    "end": "3425958"
  },
  {
    "text": "I guess there are\nmultiple ways to do this. For example, one way to do it\nis you can use Cauchy-Schwarz.",
    "start": "3425958",
    "end": "3431380"
  },
  {
    "text": "So you can say, w v is less than\nthe norm of W times norm of v.",
    "start": "3431380",
    "end": "3440109"
  },
  {
    "text": "So this is less than\nv times norm of v.",
    "start": "3440110",
    "end": "3445610"
  },
  {
    "text": "And actually, this can be\nattained by searching W.",
    "start": "3445610",
    "end": "3451050"
  },
  {
    "text": "So the answer is that\nthe sup is equal to B",
    "start": "3451050",
    "end": "3457910"
  },
  {
    "text": "times the 2 norm of v And how\ndo you attain this inequality?",
    "start": "3457910",
    "end": "3463280"
  },
  {
    "text": "To attain equality,\nyou just choose w to be in the same\ndirection of v so that your Cauchy-Schwarz,\nthis inequality, is tight,",
    "start": "3463280",
    "end": "3470450"
  },
  {
    "text": "and you got the\nright number, right? So I think this is probably-- I think this is one of\nthe homework zero question",
    "start": "3470450",
    "end": "3477407"
  },
  {
    "text": "that guys had for homework. OK. And you can apply\nthis thing to here.",
    "start": "3477407",
    "end": "3483329"
  },
  {
    "text": "So basically, you get B\ntimes the vector v. The v corresponds to this thing.",
    "start": "3483330",
    "end": "3489710"
  },
  {
    "text": " So you got rid of the sup. That's a big thing\nfor us because the sup",
    "start": "3489710",
    "end": "3497160"
  },
  {
    "text": "is very hard to deal with. And now we have a norm\nof a random variable. And this random variable is\na sum of random variables.",
    "start": "3497160",
    "end": "3507487"
  },
  {
    "text": "And note that here we are\ntalking about empirical random or complexity, so the\nonly randomness come from sigma but not x.",
    "start": "3507487",
    "end": "3513537"
  },
  {
    "text": "But still, this is\na random variable. It's a random\nmixing of this axis.",
    "start": "3513538",
    "end": "3518730"
  },
  {
    "text": "And how do you deal with this? So we are going to use\nthe Cauchy-Schwarz again.",
    "start": "3518730",
    "end": "3525120"
  },
  {
    "text": "So I guess maybe for preparation\nlet's first get rid of the-- move all the B the and--",
    "start": "3525120",
    "end": "3531410"
  },
  {
    "text": "B and N so, you get the sum\nof sigma i and xi 2 norm. ",
    "start": "3531410",
    "end": "3537960"
  },
  {
    "text": "And you say that this is\nless than the square root",
    "start": "3537960",
    "end": "3543020"
  },
  {
    "text": "of the expectation\nof the square. ",
    "start": "3543020",
    "end": "3548970"
  },
  {
    "text": "This is just because\nexpectation of B is less than square root\nexpectation B square.",
    "start": "3548970",
    "end": "3557600"
  },
  {
    "text": "So I guess maybe I\nshouldn't call it B for any random variable\nx, I say x square.",
    "start": "3557600",
    "end": "3562819"
  },
  {
    "text": " The nice thing about\nit is if you square it,",
    "start": "3562820",
    "end": "3569280"
  },
  {
    "text": "then I think we have seen this\nkind of equation not only once. In some other cases,\nwe also seen this--",
    "start": "3569280",
    "end": "3576360"
  },
  {
    "text": "the nice thing about square is\nthat you can expand it, right? So you can just expand what's\ninside this expectation.",
    "start": "3576360",
    "end": "3588230"
  },
  {
    "text": "This is equals to sum of sigma\ni square Xi square plus sum",
    "start": "3588230",
    "end": "3595510"
  },
  {
    "text": "of sigma i sigma j Xi\ninner product with Xj.",
    "start": "3595510",
    "end": "3603140"
  },
  {
    "text": "This is just an expansion. And then another thing\nis that this is--",
    "start": "3603140",
    "end": "3615069"
  },
  {
    "text": "here you have i\nis not equal to j. And because i is\nnot equals to j, expectation of sigma\nor sigma j is 0,",
    "start": "3615070",
    "end": "3627940"
  },
  {
    "text": "because they are independent\non the variables. This is equals to expectations\nsigma i, expectation sigma j,",
    "start": "3627940",
    "end": "3633820"
  },
  {
    "text": "which is equal to 0.  So this term is gone. So what we have is B over n\ntimes expectation, sum of sigma",
    "start": "3633820",
    "end": "3644350"
  },
  {
    "text": "i square. Sigma squared is\nactually 1 because it's Radamacher variable.",
    "start": "3644350",
    "end": "3650890"
  },
  {
    "text": "So Xi 2 norm squared. ",
    "start": "3650890",
    "end": "3657370"
  },
  {
    "text": "And then the Xi 2 norm\nsquare and the expectation",
    "start": "3657370",
    "end": "3662560"
  },
  {
    "text": "is over sigma, right? It's always over sigma. So there's no-- it's\nequivalent to no expectation",
    "start": "3662560",
    "end": "3668770"
  },
  {
    "text": "because Xi is not a\nfunction of sigma. So we just have this.",
    "start": "3668770",
    "end": "3674220"
  },
  {
    "text": "And this is our bound.",
    "start": "3674220",
    "end": "3679420"
  },
  {
    "text": "Our bound was\nthis, exactly this. ",
    "start": "3679420",
    "end": "3687930"
  },
  {
    "text": "So here it appears that\nit decays, 1 over n. But actually, in the sum, you\nalso have something about n,",
    "start": "3687930",
    "end": "3695640"
  },
  {
    "text": "right, the sum grows\nas n grows to infinity. So you balance them.",
    "start": "3695640",
    "end": "3700675"
  },
  {
    "text": "You get actually 1 over square\nroot dependecy, which will be [INAUDIBLE] the average.",
    "start": "3700675",
    "end": "3706870"
  },
  {
    "text": "Right? If you average over x again. We call that-- the average\nRademacher complexity",
    "start": "3706870",
    "end": "3713359"
  },
  {
    "text": "is the average of\nempirical Rademacher complexity over the\nrandomness of the data set.",
    "start": "3713360",
    "end": "3719750"
  },
  {
    "text": "Then you got B over\nn times expectation over the randomness\nof s. s is the--",
    "start": "3719750",
    "end": "3725599"
  },
  {
    "text": "the concatenate is the\nset of Xi's and you get this square root. ",
    "start": "3725600",
    "end": "3737430"
  },
  {
    "text": "So now you get into\nthis exact situation where you have some square root\ninside the expectation, which",
    "start": "3737430",
    "end": "3743480"
  },
  {
    "text": "is not very convenient. So you raise to the higher\npower like Cauchy-Schwarz using this thing.",
    "start": "3743480",
    "end": "3750120"
  },
  {
    "text": "So you get B over\nn times expectation of the square of this.",
    "start": "3750120",
    "end": "3756185"
  },
  {
    "text": " I'm sorry. I should use x superscript i.",
    "start": "3756185",
    "end": "3761480"
  },
  {
    "text": " Right.",
    "start": "3761480",
    "end": "3766610"
  },
  {
    "text": " And now it's B\nover n square root.",
    "start": "3766610",
    "end": "3774920"
  },
  {
    "start": "3774920",
    "end": "3783530"
  },
  {
    "text": "And each of the Xi's has\nthe same distribution. And then we also assume\nthat this is equals to--",
    "start": "3783530",
    "end": "3789800"
  },
  {
    "text": "I think this is our\nassumption, C, about C, right? We assume this is\nequal to C squared.",
    "start": "3789800",
    "end": "3795620"
  },
  {
    "text": "So this is C squared. So you got--  C squared when you\ntake square root.",
    "start": "3795620",
    "end": "3801720"
  },
  {
    "text": "So you get B C square root of n. ",
    "start": "3801720",
    "end": "3813740"
  },
  {
    "text": "OK, sounds good. Any questions? ",
    "start": "3813740",
    "end": "3831310"
  },
  {
    "text": "OK. So next, I'm going\nto show another-- go ahead.",
    "start": "3831310",
    "end": "3836609"
  },
  {
    "text": "[INAUDIBLE] like Cauchy\nSchwarz, you could just bound it by [INAUDIBLE]",
    "start": "3836610",
    "end": "3848750"
  },
  {
    "text": "Yes. So this is a great question. The question is, what if you\ndon't use Cauchy-Schwarz?",
    "start": "3848750",
    "end": "3854420"
  },
  {
    "text": "You use triangle inequality. I think I-- that's actually a\nvery good question I should--",
    "start": "3854420",
    "end": "3860630"
  },
  {
    "text": "Actually, let's try\nto do it a little bit. So I guess what you\nmean here is that you--",
    "start": "3860630",
    "end": "3868115"
  },
  {
    "text": "wait, by the way, where\nyou want to use this? In the second application,\nlike you've already [INAUDIBLE] Like once you already, for\nexample, here, where, here?",
    "start": "3868115",
    "end": "3875260"
  },
  {
    "start": "3875260",
    "end": "3881200"
  },
  {
    "text": "Like, sorry, yeah, yeah. From here to here,\nbasically, right?",
    "start": "3881200",
    "end": "3887970"
  },
  {
    "text": "Yeah. So that's a very good question. So if you don't do\nthis, if you say,",
    "start": "3887970",
    "end": "3894680"
  },
  {
    "text": "there's no different\ncolor to indicate this. So if you say that you\ndo triangle inequality,",
    "start": "3894680",
    "end": "3900360"
  },
  {
    "text": "you're going to bound it by\nB over n times expectation of the sum of Xi 2 norm.",
    "start": "3900360",
    "end": "3911340"
  },
  {
    "text": "OK? So then let's say you also\ntake expectation over Xi just--",
    "start": "3911340",
    "end": "3917758"
  },
  {
    "text": "maybe we don't have to do that. So let's say this is v over\nn times sum of expectation",
    "start": "3917758",
    "end": "3925230"
  },
  {
    "text": "of Xi Xi 2 norm. ",
    "start": "3925230",
    "end": "3932230"
  },
  {
    "text": "And let's see what-- and you\ncan see what happens here. So you have n terms here.",
    "start": "3932230",
    "end": "3937920"
  },
  {
    "text": "And each of these term is on\nsome constant scale, I see. And so basically, the\nsum will be on all of n.",
    "start": "3937920",
    "end": "3945360"
  },
  {
    "text": "And then you cancel it with\nn, so you get B. So basically, at the end of that, you don't\nhave dependency on n anymore.",
    "start": "3945360",
    "end": "3951870"
  },
  {
    "text": "So that's strictly worse\nbecause we do want to have a dependency, something\nlike 1 over square root n.",
    "start": "3951870",
    "end": "3957119"
  },
  {
    "text": "That's something that goes\nto 0 as n goes to infinity. And the reason on why this\nis a loose inequality,",
    "start": "3957120",
    "end": "3962970"
  },
  {
    "text": "the reason is because here-- if you look at\nthis, this is a sum",
    "start": "3962970",
    "end": "3968670"
  },
  {
    "text": "of things that can cancel each\nother because sigma i flipped things, right. ",
    "start": "3968670",
    "end": "3977593"
  },
  {
    "text": "And if you do\ntriangle inequality, you are basically assuming\nall of these factors in the same direction, right?",
    "start": "3977593",
    "end": "3983000"
  },
  {
    "text": "So even with the flip,\nright, so it's possible that all the xi's are in\nexactly the same direction.",
    "start": "3983000",
    "end": "3988680"
  },
  {
    "text": "Let's say that's\nalready the case, right? But with the flip, they\ncancel with each other, right? So one of them is\ngoing this direction.",
    "start": "3988680",
    "end": "3995270"
  },
  {
    "text": "The other is in the\nopposite direction. So you have the cancelation. And that's why this\nCauchy-Schwarz is",
    "start": "3995270",
    "end": "4001870"
  },
  {
    "text": "or this Jensen inequality\nis more kind of like height. And this is exactly\nthe gist here, right,",
    "start": "4001870",
    "end": "4007150"
  },
  {
    "text": "because you have to\nuse the concentration, cancellation between\nthe sigma i's. If you don't use it--",
    "start": "4007150",
    "end": "4012540"
  },
  {
    "text": "actually, if you don't use it,\nstrongly enough in some sense you wouldn't have a\ngood bound eventually.",
    "start": "4012540",
    "end": "4020520"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "4020520",
    "end": "4028670"
  },
  {
    "text": "Right. Exactly, exactly. ",
    "start": "4028670",
    "end": "4034845"
  },
  {
    "text": "Yeah. ",
    "start": "4034845",
    "end": "4040070"
  },
  {
    "text": "OK, cool. And the next thing would\nbe another theorem.",
    "start": "4040070",
    "end": "4049030"
  },
  {
    "text": "And this different theorem would\nstill deal with linear models but have different norm\nmeasurement of the parameter.",
    "start": "4049030",
    "end": "4058509"
  },
  {
    "text": "And you will see\na different bound. And this is one of the\nreasons, one of the things that I motivated,\none of the reason I",
    "start": "4058510",
    "end": "4066010"
  },
  {
    "text": "use to motivate Rademacher\ncomplexity because I was saying that you can get more precise\ndependencies on what norms",
    "start": "4066010",
    "end": "4072490"
  },
  {
    "text": "you want to use,\nfor example, right? So suppose we still\nhave the same H",
    "start": "4072490",
    "end": "4078160"
  },
  {
    "text": "or we have the different\nH. It's still linear model but the constraint on the\nparameter now becomes L1 norm.",
    "start": "4078160",
    "end": "4085570"
  },
  {
    "text": " So this is L1.",
    "start": "4085570",
    "end": "4090970"
  },
  {
    "text": "And we assume that now\nthe infinity norm of x",
    "start": "4090970",
    "end": "4099950"
  },
  {
    "text": "is less than C. for all i. ",
    "start": "4099950",
    "end": "4108189"
  },
  {
    "text": "For all i. And also, let's\nspecify a dimension. Xi is in Rt.",
    "start": "4108189",
    "end": "4114383"
  },
  {
    "text": " Actually, this is an\ninteresting thing. So before, we didn't even\nspecify the dimension",
    "start": "4114384",
    "end": "4120288"
  },
  {
    "text": "of x in the previous\ntheorem because it doesn't show up in the bound. And it has to be some vector\nin some dimension of course,",
    "start": "4120288",
    "end": "4126509"
  },
  {
    "text": "right, but it doesn't matter\nwhat the dimensionality is. Actually, you can apply\nthis to even infinite",
    "start": "4126510",
    "end": "4133680"
  },
  {
    "text": "dimensional vectors as long as\nthe norm of x is bounded by C.",
    "start": "4133680",
    "end": "4140189"
  },
  {
    "text": "But next thing will\ndepend on dimension. The dimension is D. And then the\nempirical Rademacher complexity",
    "start": "4140189",
    "end": "4149910"
  },
  {
    "text": "is less than B times\nC times square root 2",
    "start": "4149910",
    "end": "4157259"
  },
  {
    "text": "log d over square root of n. ",
    "start": "4157260",
    "end": "4165189"
  },
  {
    "text": "Right. And you can see now the\n1 norm starts to matter.",
    "start": "4165189",
    "end": "4170520"
  },
  {
    "text": "So it's still kind of-- so\nlet's suppose you ignore log d. So basically, still\nsomething like B times C,",
    "start": "4170520",
    "end": "4176120"
  },
  {
    "text": "but it's a different\nmeasurement. On the B and C, the\ndefinitions are different. Now the B is the\n1 norm of W. And C",
    "start": "4176120",
    "end": "4185750"
  },
  {
    "text": "is the infinite norm of x. We'll compare this two\ntheorems after we prove it.",
    "start": "4185750",
    "end": "4192390"
  },
  {
    "text": "Let me see how much time I have.",
    "start": "4192390",
    "end": "4202330"
  },
  {
    "text": "I think I do have time to\nprove it and then compare. So the proof won't be\ncomplete, in the sense",
    "start": "4202330",
    "end": "4213720"
  },
  {
    "text": "that I have to invoke some\nlemma, which I will prove-- actually, will be proved\nby you in the homework.",
    "start": "4213720",
    "end": "4222190"
  },
  {
    "text": "But let's do the\nmost of the stuff. So the definition\nis the same thing.",
    "start": "4222190",
    "end": "4228060"
  },
  {
    "start": "4228060",
    "end": "4237850"
  },
  {
    "text": "And now it becomes\nsomething like--",
    "start": "4237850",
    "end": "4242960"
  },
  {
    "text": "again, you can\nview this as some w",
    "start": "4242960",
    "end": "4248370"
  },
  {
    "text": "times some v, where v\nis this 1 over n times the sum of sigma i Xi.",
    "start": "4248370",
    "end": "4258409"
  },
  {
    "text": "I'm writing here, this is\nsigma i w transpose x squared. So we are doing the\nsame decomposition.",
    "start": "4258410",
    "end": "4265280"
  },
  {
    "text": "But now, you are taking\na sup over 1 norm. And you know that\nif you take the sup",
    "start": "4265280",
    "end": "4271010"
  },
  {
    "text": "of over 1 norm,\nconstant P V times W,",
    "start": "4271010",
    "end": "4278760"
  },
  {
    "text": "this is also relatively\neasy to prove. I mean, this is equal to B\ntimes the infinite- sorry,",
    "start": "4278760",
    "end": "4286560"
  },
  {
    "text": "it's W times V. W times V. This is going to be\nequals to infinite norm of v.",
    "start": "4286560",
    "end": "4299080"
  },
  {
    "text": "So that's how we\neliminate W. So we just got the infinite number\nof B, Which I see.",
    "start": "4299080",
    "end": "4314450"
  },
  {
    "text": "Sorry, yeah. So you see what's going on here. ",
    "start": "4314450",
    "end": "4322720"
  },
  {
    "text": "I think this is-- Oh-- I see.",
    "start": "4322720",
    "end": "4342460"
  },
  {
    "text": "So, right. So this is equals to this. However, so now\nwe've got a problem.",
    "start": "4342460",
    "end": "4359350"
  },
  {
    "text": "So we have this infinity norm. And so how do we proceed, right?",
    "start": "4359350",
    "end": "4364710"
  },
  {
    "text": "So you can, for example, use\ntriangle inequality, right? So then, again, we don't\nuse the cancelations, right?",
    "start": "4364710",
    "end": "4371143"
  },
  {
    "text": "So if you use triangle\ninequality you're going to flip-- you swapped the sum\nwith the infinity norm.",
    "start": "4371143",
    "end": "4376540"
  },
  {
    "text": "But you don't have\nthe cancelation. So how do I deal with this? So in some sense, I'm going to--",
    "start": "4376540",
    "end": "4382075"
  },
  {
    "text": " kind of like the infinity\nnorm is kind of like something",
    "start": "4382075",
    "end": "4389980"
  },
  {
    "text": "different, then you cannot\nuse this analytical tool. So what I'm going\nto do is I'm going to do a different approach or\nsomewhat different approach.",
    "start": "4389980",
    "end": "4397460"
  },
  {
    "text": "So from here-- so\nthis inequality. So this is a data in some sense.",
    "start": "4397460",
    "end": "4402740"
  },
  {
    "text": "So what you do is that you say-- you look at what\nthis really means.",
    "start": "4402740",
    "end": "4409190"
  },
  {
    "text": "So you can say that this\nis equals to B over n times",
    "start": "4409190",
    "end": "4424810"
  },
  {
    "text": "the sup of W is in\nsorry, B such that--",
    "start": "4424810",
    "end": "4440040"
  },
  {
    "text": "I think I probably got the\nwrong version of the notes. That's why I was like a little\nbit of kind of like surprised",
    "start": "4440040",
    "end": "4449070"
  },
  {
    "text": "by the notes because I think-- the newer version\nshouldn't be like this. But anyway, so the\nfirst thing you can do",
    "start": "4449070",
    "end": "4457140"
  },
  {
    "text": "is you can normalize it to be\n1, so you get w times v, right?",
    "start": "4457140",
    "end": "4462630"
  },
  {
    "text": "That's w bar times v,\ntimes v. That's easy. And then what you can say is\nthat if you think about what",
    "start": "4462630",
    "end": "4471570"
  },
  {
    "text": "maximize this inner product\namong all the L1 non-bounding vector--",
    "start": "4471570",
    "end": "4476730"
  },
  {
    "text": "actually, the only thing\nyou care about is-- so the sup is actually\nliterally close to--",
    "start": "4476730",
    "end": "4482030"
  },
  {
    "text": "if w1, wbar is between plus\nminus e1 plus minus e2,",
    "start": "4482030",
    "end": "4487730"
  },
  {
    "text": "so and so forth, plus minus ed. ",
    "start": "4487730",
    "end": "4494060"
  },
  {
    "text": "This is my claim. And the reason for\nthis claim is just that if you look at sum\nof W for i Vi, right,",
    "start": "4494060",
    "end": "4501650"
  },
  {
    "text": "suppose i is index\nfor the coordinates. OK. So how does-- so basically,\nyou can say that--",
    "start": "4501650",
    "end": "4509735"
  },
  {
    "text": " so basically, you know\nthat in this sup W bar",
    "start": "4509735",
    "end": "4516300"
  },
  {
    "text": "V is equals to v infinity norm.",
    "start": "4516300",
    "end": "4522989"
  },
  {
    "text": "You know this. And what you care about is that\nwhat are the extreme point?",
    "start": "4522990",
    "end": "4529700"
  },
  {
    "text": "Right, in what case you\nachieve this equality?",
    "start": "4529700",
    "end": "4535170"
  },
  {
    "text": "And it turns out that\nthe way that you achieve this equality is is that w--",
    "start": "4535170",
    "end": "4541050"
  },
  {
    "text": "basically, you want to\ntake a w bar i to be 1",
    "start": "4541050",
    "end": "4548389"
  },
  {
    "text": "for i is the largest entry. ",
    "start": "4548390",
    "end": "4554040"
  },
  {
    "text": "So for i such that Vi is the\nlargest, is the max over Vk.",
    "start": "4554040",
    "end": "4563780"
  },
  {
    "start": "4563780",
    "end": "4569280"
  },
  {
    "text": "j is in d. Right. So I'm not sure\nwhether this is--",
    "start": "4569280",
    "end": "4575237"
  },
  {
    "text": "this probably requires a\nlittle bit of thinking offline. ",
    "start": "4575237",
    "end": "4582500"
  },
  {
    "text": "So at least you can verify\nin this case if you choose wi to be 1 for this case and all\nthe wi bar to be 0 for all",
    "start": "4582500",
    "end": "4589429"
  },
  {
    "text": "the other i's. Then what you get is that\nyou get sum of wi bar Vi is equals to just Vi.",
    "start": "4589430",
    "end": "4595460"
  },
  {
    "text": "And Vi is equals\nto V infinity norm because Vi is the largest one. ",
    "start": "4595460",
    "end": "4605790"
  },
  {
    "text": "I suppose we take the\nabsolute value here. Then this is exactly\ncorrect, right? So if you choose\nthis wi, then you--",
    "start": "4605790",
    "end": "4614510"
  },
  {
    "text": "and all the wi bar 0-- so\nyou've got Vi infinite norm.",
    "start": "4614510",
    "end": "4619880"
  },
  {
    "text": "And that's equals to V--\nthe Vi absolute value, that's the V infinite norm. Right?",
    "start": "4619880",
    "end": "4626690"
  },
  {
    "text": "And also, if you don't\nhave absolute value on the left-hand\nside, you can also flip the Wi to be\neither 1 or minus 1.",
    "start": "4626690",
    "end": "4636825"
  },
  {
    "text": "Does that make sense?  Yeah, it's relatively\neasy, so it probably",
    "start": "4636825",
    "end": "4645700"
  },
  {
    "text": "requires a little bit of\nthinking offline as well. So but the basic\nclaim is that when you do this kind of like\nmaximization over the simplex,",
    "start": "4645700",
    "end": "4653139"
  },
  {
    "text": "you always get\nthe vertex, right? The extremal point\nis always the vertex. That's another way\nto think about it.",
    "start": "4653140",
    "end": "4658510"
  },
  {
    "text": "And the vertex are\nthose natural bases. And then we basically got into--",
    "start": "4658510",
    "end": "4665843"
  },
  {
    "text": "so what happens is that this\nis a finite hypothesis class. ",
    "start": "4665843",
    "end": "4674897"
  },
  {
    "text": "What does that mean? So basically, you can think\nof your hypothesis class. H bar is something\nthat x maps to W bar",
    "start": "4674897",
    "end": "4683380"
  },
  {
    "text": "transpose x, where W bar is\nonly inside this family of plus",
    "start": "4683380",
    "end": "4690380"
  },
  {
    "text": "minus e1 up to plus minus e.",
    "start": "4690380",
    "end": "4695469"
  },
  {
    "text": "Right. So you don't have all the\nlinear classifiers anymore.",
    "start": "4695470",
    "end": "4703330"
  },
  {
    "text": "You just have 2D linear\nclassifiers now, right? So basically, this thing\nis just equals to the--",
    "start": "4703330",
    "end": "4712680"
  },
  {
    "text": "basically, if you\nput a B outside, you get this plus\nminus ei, wbar V.",
    "start": "4712680",
    "end": "4724840"
  },
  {
    "text": "And this is just equals to V\ntimes the Rademacher complexity of this hypothesis class H bar.",
    "start": "4724840",
    "end": "4731710"
  },
  {
    "start": "4731710",
    "end": "4737781"
  },
  {
    "text": "OK? And we have a\nclaim that in the-- and also, we have claim in\none of the very early part",
    "start": "4737781",
    "end": "4748200"
  },
  {
    "text": "of the lecture. So we claim that for hypothesis\nclass you can burn it by--",
    "start": "4748200",
    "end": "4756300"
  },
  {
    "text": " so this is-- let's go back\nto the lecture, to the notes",
    "start": "4756300",
    "end": "4763940"
  },
  {
    "text": "before. So this is the lemma underneath. So the Rademacher complexity is\nthe log of the hypothesis class",
    "start": "4763940",
    "end": "4770500"
  },
  {
    "text": "size is bounded by the log of\nhypothesis class size times something like M square,\nwhere M square is the largest",
    "start": "4770500",
    "end": "4777400"
  },
  {
    "text": "possible value we can output\nfrom this hypothesis class. So let's compute\nwhat M square is.",
    "start": "4777400",
    "end": "4783850"
  },
  {
    "text": "The size of f is pretty clear. It's 2d. And what's the-- so\nthis is equal to 2d.",
    "start": "4783850",
    "end": "4792489"
  },
  {
    "text": "But what is the\ncorresponding n, right? So we can say that for every\nW bar in this plus minus ei,",
    "start": "4792490",
    "end": "4800440"
  },
  {
    "text": "you know that it looked a lot--",
    "start": "4800440",
    "end": "4806570"
  },
  {
    "text": "I guess you know\nthat W bar Xi this is",
    "start": "4806570",
    "end": "4811739"
  },
  {
    "text": "bound by the L1 norm of\nwbar times the L inifinity norm of Xi.",
    "start": "4811740",
    "end": "4819710"
  },
  {
    "text": "And this is bounded by\n1 times C, where C is",
    "start": "4819710",
    "end": "4825500"
  },
  {
    "text": "the infinity norm bound for Xi. And that means that if\nyou look at these things",
    "start": "4825500",
    "end": "4833910"
  },
  {
    "text": "that we have to\nverify in the lemma-- Right. So we have to\nverify that the sum",
    "start": "4833910",
    "end": "4840010"
  },
  {
    "text": "of the squares of the output\nis less than M square. Right?",
    "start": "4840010",
    "end": "4845380"
  },
  {
    "text": "That's what we have to verify. So then because each of\nthe term is less than this, so we can just verify\nthe sum of squares.",
    "start": "4845380",
    "end": "4852820"
  },
  {
    "text": "This is 1 over n\ntimes n C square, which is equal to C square.",
    "start": "4852820",
    "end": "4860170"
  },
  {
    "text": "So basically, the\ncorresponding n will be the C square. n square\nwill be C square so that's",
    "start": "4860170",
    "end": "4865810"
  },
  {
    "text": "why Rs square is less than\na square root 2C squared,",
    "start": "4865810",
    "end": "4872320"
  },
  {
    "text": "log of H over n. This is just the square root\nof 2C square log d over n.",
    "start": "4872320",
    "end": "4881739"
  },
  {
    "text": "And now recall that we have\na B here, which we got.",
    "start": "4881740",
    "end": "4888090"
  },
  {
    "text": "So Rs (H) is less than B times\nRS(H) bar, which equals to--",
    "start": "4888090",
    "end": "4898099"
  },
  {
    "text": "which is less than B times\nC times square root 2 log",
    "start": "4898100",
    "end": "4903780"
  },
  {
    "text": "d over square root of n. ",
    "start": "4903780",
    "end": "4915620"
  },
  {
    "text": "OK, any questions? ",
    "start": "4915620",
    "end": "4921650"
  },
  {
    "text": "So yeah, I think\nwe're about time. So I guess next lecture\nat the beginning,",
    "start": "4921650",
    "end": "4927470"
  },
  {
    "text": "I would discuss\nhow do you compare, or how do you interpret\nthese two theorems? Right.",
    "start": "4927470",
    "end": "4933170"
  },
  {
    "text": "So these two theorems have their\nstrengths on different cases, depending on what\nkind of W's you are--",
    "start": "4933170",
    "end": "4938480"
  },
  {
    "text": "what kind of data you have\nand what kind of W's you can fit from the data.",
    "start": "4938480",
    "end": "4943667"
  },
  {
    "text": "I will do that in\nthe next lecture. ",
    "start": "4943667",
    "end": "4949380"
  },
  {
    "text": "OK, sounds good. I guess that's all for today. See you next Monday.",
    "start": "4949380",
    "end": "4955640"
  },
  {
    "start": "4955640",
    "end": "4961000"
  }
]