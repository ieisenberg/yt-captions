[
  {
    "start": "0",
    "end": "5740"
  },
  {
    "text": "OK, let's get started. I guess everything's\nworking now. OK, cool.",
    "start": "5740",
    "end": "11440"
  },
  {
    "text": "So last time we\ntalked about the-- we started talking about\nthis so-called implicit",
    "start": "11440",
    "end": "16660"
  },
  {
    "text": "regularization effect\nof the optimizers, and last time we discussed\nthe very basic one,",
    "start": "16660",
    "end": "27880"
  },
  {
    "text": "which is that if you use\ninitialization zero and then",
    "start": "27880",
    "end": "36570"
  },
  {
    "text": "you see gradient descent\nand you have a regression problem, a linear regression\nproblem, then what you get",
    "start": "36570",
    "end": "43600"
  },
  {
    "text": "is that you get the\nminimum norm solution. ",
    "start": "43600",
    "end": "52780"
  },
  {
    "text": "This is from last\ntime, and today we're going to talk about a case\nwhere we have nonlinear models.",
    "start": "52780",
    "end": "59300"
  },
  {
    "text": "And we'll see similar\nphenomena, but we're going to have a somewhat\ndifferent proof. ",
    "start": "59300",
    "end": "87030"
  },
  {
    "text": "Right.  So OK, so I guess so let's\ndive into the details.",
    "start": "87030",
    "end": "98299"
  },
  {
    "text": "So this is the nonlinear\nmodel that we-- you know, you will see that\nthis model is nonlinear,",
    "start": "98300",
    "end": "105492"
  },
  {
    "text": "but it's not actually that much\ndifferent from linear model you-- as you will see.",
    "start": "105492",
    "end": "111130"
  },
  {
    "text": "There is a paper that can do\na little bit more than this, but generally we\ndon't know how to deal with very complex models\nlike deep networks.",
    "start": "111130",
    "end": "118060"
  },
  {
    "text": " So this is the nonlinear\nmodel we're going to consider.",
    "start": "118060",
    "end": "125220"
  },
  {
    "text": "So suppose you have, beta is the\nparameter and x is the input. And the model is fx is equal to\nthe inner product between beta",
    "start": "125220",
    "end": "136940"
  },
  {
    "text": "times O-dot beta and x. So O dot is the\nHadamard product,",
    "start": "136940",
    "end": "144030"
  },
  {
    "text": "meaning the entry-wise product. So, basically you entry-wise\nsquare the parameter,",
    "start": "144030",
    "end": "150090"
  },
  {
    "text": "and then you take the\ninner product with x. So this is still linear in x,\nbut it's not linear in beta.",
    "start": "150090",
    "end": "159720"
  },
  {
    "start": "159720",
    "end": "168900"
  },
  {
    "text": "OK, so in terms of-- and still the loss\nfunction will be nonconvex",
    "start": "168900",
    "end": "180590"
  },
  {
    "text": "because it's nonlinear in beta. And you do the loss function,\nyou take the square, then it becomes nonconvex.",
    "start": "180590",
    "end": "186319"
  },
  {
    "text": "So it's not that interesting\nin terms of the model itself because any way you are\ndoing a linear model,",
    "start": "186320",
    "end": "192440"
  },
  {
    "text": "but from the algorithm-- the\nimplicit regularization effect perspective, it's\nstill interesting because you have a nonconvex\nobjective function.",
    "start": "192440",
    "end": "199910"
  },
  {
    "text": "And we're going to\nmake this even more interesting by considering\na special case where,",
    "start": "199910",
    "end": "205310"
  },
  {
    "text": "suppose you have ground truth\nis that y is equal to beta star",
    "start": "205310",
    "end": "214270"
  },
  {
    "text": "O-dot beta star times x,\nwhere beta star is r-sparse.",
    "start": "214270",
    "end": "223340"
  },
  {
    "text": "So the reason why we want\nbeta star to be r-sparse is that r-sparse means that the\n0 norm of beta is less than r,",
    "start": "223340",
    "end": "232120"
  },
  {
    "text": "that you only have\nr nonzero entries. ",
    "start": "232120",
    "end": "239423"
  },
  {
    "text": "And the reason why\nwe want to have this restriction on\nbeta star is because we want to consider\noverparameterized models.",
    "start": "239423",
    "end": "246030"
  },
  {
    "text": "If you have overparameterized\nmodels, meaning-- so we consider the case\nwhere n is smaller than d.",
    "start": "246030",
    "end": "255130"
  },
  {
    "text": "When n is smaller than d,\nif beta is fully general, beta star is fully\ngeneral, then there's no way you can hope\nto learn anything",
    "start": "255130",
    "end": "261459"
  },
  {
    "text": "from less than dimensionality\nnumber of data points. So, basically we make\nsure beta star is sparse,",
    "start": "261459",
    "end": "269560"
  },
  {
    "text": "and so we assume-- we're going to assume that\nn is smaller than d but n",
    "start": "269560",
    "end": "275570"
  },
  {
    "text": "is larger than some poly r. That's the setting where\nwe are going to work with.",
    "start": "275570",
    "end": "282620"
  },
  {
    "text": "And more specific\nand for simplicity, with all those generality\nlet's assume also beta star",
    "start": "282620",
    "end": "291050"
  },
  {
    "text": "is larger than zero\nentry-wise, because you can see that the\nsign of beta star",
    "start": "291050",
    "end": "296370"
  },
  {
    "text": "doesn't really matter in\nterms of the functionality, in terms of the\nground truth model. And actually for\nsimplicity of this lecture,",
    "start": "296370",
    "end": "309640"
  },
  {
    "text": "let's also assume\nthat beta is just some indicator of some\nsubset of coordinates,",
    "start": "309640",
    "end": "316150"
  },
  {
    "text": "where S is a subset of\ncoordinates and the size of S",
    "start": "316150",
    "end": "322300"
  },
  {
    "text": "is equal to r. This is only for\nsimplicity of this lecture. ",
    "start": "322300",
    "end": "329370"
  },
  {
    "text": "OK, and now let's\ndefine our data. So I guess we have talked\nabout that we are going to have overparameterized model.",
    "start": "329370",
    "end": "335598"
  },
  {
    "text": "So we have n data points. And n is less than d,\nso these n data points are denoted by x1 up\nto xn, as they are iid",
    "start": "335598",
    "end": "344200"
  },
  {
    "text": "from Gaussian of dimension d--",
    "start": "344200",
    "end": "349700"
  },
  {
    "text": "of spherical covariance. And yi is generated from\nthis model without any error,",
    "start": "349700",
    "end": "358400"
  },
  {
    "text": "so the yi is inner product,\nbeta star times beta star-- is this inner product of\nsquare of beta star times xi.",
    "start": "358400",
    "end": "367800"
  },
  {
    "text": "So-- and n is much,\nmuch less than d, but we'll assume\nthat n is bigger",
    "start": "367800",
    "end": "376259"
  },
  {
    "text": "than omega tilde of r squared. So n is roughly\nbigger than r squared.",
    "start": "376260",
    "end": "383520"
  },
  {
    "text": "And this, so this amount of\ndata points in principle is-- allows us to recover beta star.",
    "start": "383520",
    "end": "389340"
  },
  {
    "text": "Actually, you only\nneed omega of r to recover beta star if you\ncount dimensionality, right,",
    "start": "389340",
    "end": "395670"
  },
  {
    "text": "so they're r degree of\nfreedom, approximately. So you only need n to\nbe larger than omega r, but for the theory\nto work, we have",
    "start": "395670",
    "end": "402060"
  },
  {
    "text": "to allow it to be\nlarger than r squared. But still, if r is very\nsmall, then you still can make n much, much smaller\nthan d and still bigger",
    "start": "402060",
    "end": "409715"
  },
  {
    "text": "than r squared. Let's say r is a constant, OK? That's probably the right\nway to think about it. Any polynomial\ndependency on r is fine,",
    "start": "409715",
    "end": "416490"
  },
  {
    "text": "and so that n is just\nsomething like a big constant. But n could be much less than d.",
    "start": "416490",
    "end": "422660"
  },
  {
    "text": "OK, so-- and maybe so far, after\nwe define this you may wonder why we are--",
    "start": "422660",
    "end": "428430"
  },
  {
    "text": "why we have to use this\nnonlinear model, right? The answer is,\nno, you don't have to use it if you really\nwant to solve the problem,",
    "start": "428430",
    "end": "434889"
  },
  {
    "text": "so the nonlinear model\nis only introduced to study this effect, right? Suppose you really care\nabout solving the question,",
    "start": "434890",
    "end": "440730"
  },
  {
    "text": "then you can use the\nclassical solution, which is called lasso. So-- or in the more\nkind of, like, terms--",
    "start": "440730",
    "end": "453480"
  },
  {
    "text": "using terms that we\nused in this lecture, you can use L1 regularization. So, basically-- so to\nleverage sparsity, I think--",
    "start": "453480",
    "end": "468637"
  },
  {
    "text": "I'm not sure whether you\nall have this background, but typically people use\nL1 to, in some sense,",
    "start": "468637",
    "end": "473930"
  },
  {
    "text": "encourage sparse vectors. I'm not going to get\ninto detail there, but you can show\nthat if you minimize",
    "start": "473930",
    "end": "482690"
  },
  {
    "text": "the L1 norm of\ntheta of the model, then you can reconstruct\nsparse vectors.",
    "start": "482690",
    "end": "492300"
  },
  {
    "text": "So in particular,\nI think, suppose you have this model f-theta\nx, which is a linear x.",
    "start": "492300",
    "end": "501139"
  },
  {
    "text": "Then this is so-called lasso. This is the L1\nregression objective, which is something\nlike this plus lambda",
    "start": "501140",
    "end": "512539"
  },
  {
    "text": "times the L1 norm of theta. So-- and the classical\nversion of the theory,",
    "start": "512539",
    "end": "518880"
  },
  {
    "text": "I'm not going to go\ninto detail here. In some sense this\nis-- you know, if you don't know\nthe background, you probably just somewhat\nmemorized it or kind of",
    "start": "518880",
    "end": "526339"
  },
  {
    "text": "like treated it as a fact. So the classical theory says\nthat if n is larger than r,",
    "start": "526340",
    "end": "538520"
  },
  {
    "text": "say-- I think you need a pair of\nlogarithmic factors here. If n is larger than r, then\nthis objective function",
    "start": "538520",
    "end": "546380"
  },
  {
    "text": "recovers the ground\ntruth, right? So objective above recovers\nthe ground truth, theta star.",
    "start": "546380",
    "end": "561350"
  },
  {
    "text": "Theta star, which is the-- you can-- I guess you probably\nalready see that theta of theta",
    "start": "561350",
    "end": "566529"
  },
  {
    "text": "corresponds to beta up to this\nsquare, squaring the thing, so approximately. ",
    "start": "566530",
    "end": "574524"
  },
  {
    "text": "So, basically, if you just\nreally care about solving this question, you view this\nas a linear model-- you don't have to care\nabout the quadratic thing--",
    "start": "574525",
    "end": "580890"
  },
  {
    "text": "and then use L1 regularization\nto recover sparse vector. There's a rich-- you know, a\nlot of existing kind of theory",
    "start": "580890",
    "end": "589170"
  },
  {
    "text": "about this. I'm not going into details,\nbut this is something-- somewhat believable\nbecause you are using",
    "start": "589170",
    "end": "596880"
  },
  {
    "text": "the sparsity of the vector. And also, another thing to\nnote is that because beta",
    "start": "596880",
    "end": "603420"
  },
  {
    "text": "and theta-- the relationship\nbetween beta and theta is that theta corresponds to\nbeta O-dot beta, right,",
    "start": "603420",
    "end": "610730"
  },
  {
    "text": "the entry-wise square, so-- and then the 1 norm\nof theta is equals",
    "start": "610730",
    "end": "618330"
  },
  {
    "text": "the 2-norm square of beta. The 1 norm of theta\nis the sum of entries",
    "start": "618330",
    "end": "623760"
  },
  {
    "text": "of theta, which is equals the\nsum of beta-i squares, which is the 2-norm square of beta.",
    "start": "623760",
    "end": "630370"
  },
  {
    "text": "So, basically, if you do\nthe quadratic one, you can-- you should regularize\nL2 norm, right?",
    "start": "630370",
    "end": "636959"
  },
  {
    "text": "So, basically this\nobjective 1 corresponds to L2-regularized objective.",
    "start": "636960",
    "end": "647040"
  },
  {
    "text": "So if you really want to use the\nquadratic one, you should do-- the quadratic parameterization,\nyou should do this f-beta x-i",
    "start": "647040",
    "end": "660529"
  },
  {
    "text": "square plus L lambda\n2-norm beta square-- beta 2-norm square, right?",
    "start": "660530",
    "end": "666560"
  },
  {
    "text": "This is the objective if\nwe select beta, right? So in the beta space, you should\nregularize L2-norm square.",
    "start": "666560",
    "end": "674300"
  },
  {
    "text": "In the theta space, you\nshould regularize L1, right? So this is the\nclassical solution,",
    "start": "674300",
    "end": "681660"
  },
  {
    "text": "and now when I talk about\nimplicit regularization,",
    "start": "681660",
    "end": "688889"
  },
  {
    "text": "I think our goal is\nessentially, basically saying that if you use\nsmall initialization,",
    "start": "688890",
    "end": "698130"
  },
  {
    "text": "this is, you know,\nwithout regularization, without explicit\nregularization--",
    "start": "698130",
    "end": "704540"
  },
  {
    "text": "this is basically doing the same\nthing as, let's call this, 2.",
    "start": "704540",
    "end": "710329"
  },
  {
    "text": "So as long as you use\nsmall initialization with beta parameterization,\nyou automatically",
    "start": "710330",
    "end": "717290"
  },
  {
    "text": "get this L2-norm regularization,\nfor free to some extent. This is not exactly the\nexact way to phrase--",
    "start": "717290",
    "end": "724610"
  },
  {
    "text": "to state a theorem,\nbut this is the rough-- basically the main idea.",
    "start": "724610",
    "end": "730190"
  },
  {
    "text": "So more concretely,\nwhere we are interested",
    "start": "730190",
    "end": "735800"
  },
  {
    "text": "is the objective L-hat beta. Let's formally define it. I think I normalized\nby 4 here just",
    "start": "735800",
    "end": "742430"
  },
  {
    "text": "because it makes the\ngradient look cleaner, so I--",
    "start": "742430",
    "end": "747790"
  },
  {
    "text": "but it's just a constant vector. So 1 over 4n times the square\nloss, the min square error,",
    "start": "747790",
    "end": "756410"
  },
  {
    "text": "right? No regularization, right? So this is our\nobjective, and we will-- we are going to do\nthat the optimizer will",
    "start": "756410",
    "end": "764899"
  },
  {
    "text": "be that you do GD on L-hat\nbeta with small initialization.",
    "start": "764900",
    "end": "774560"
  },
  {
    "start": "774560",
    "end": "779590"
  },
  {
    "text": "And more concretely,\nso the algorithm is for some very small\nalpha larger than zero.",
    "start": "779590",
    "end": "788699"
  },
  {
    "text": "We initialize beta to be\nalpha times L1 vector.",
    "start": "788700",
    "end": "794580"
  },
  {
    "text": "So you don't know\nthe support of beta, of course, so you\nneed to initialize all the entries by alpha.",
    "start": "794580",
    "end": "800970"
  },
  {
    "text": "And then you take a gradient\ndescent update every time, so you say the beta-t\nplus 1 is equal to beta-t",
    "start": "800970",
    "end": "808170"
  },
  {
    "text": "minus beta times the\ngradient at beta-t. ",
    "start": "808170",
    "end": "817632"
  },
  {
    "text": "OK, so this is the objective\nwe are going to study, and we will claim\nthat this objective--",
    "start": "817632",
    "end": "822709"
  },
  {
    "text": "sorry, this is the optimizer\nwe're going to study. We're going to claim that\nthis optimizer actually",
    "start": "822710",
    "end": "829540"
  },
  {
    "text": "finds the beta star\neven though there's no explicit regularization.",
    "start": "829540",
    "end": "837280"
  },
  {
    "text": "Any questions so far? ",
    "start": "837280",
    "end": "849660"
  },
  {
    "text": "So here is the theorem. So the theorem is that--",
    "start": "849660",
    "end": "857540"
  },
  {
    "text": "basically, the shorter\nversion of the theorem is that when n is-- so n is omega squared.",
    "start": "857540",
    "end": "863750"
  },
  {
    "text": "We can converge to this\nalgorithm, converges",
    "start": "863750",
    "end": "869110"
  },
  {
    "text": "to beta star.",
    "start": "869110",
    "end": "880852"
  },
  {
    "text": "And let's-- but I think there is\na little bit kind of like with small alpha. ",
    "start": "880853",
    "end": "888200"
  },
  {
    "text": "But there is a\nlittle bit of detail, so let me state\nthe main theorem. ",
    "start": "888200",
    "end": "893860"
  },
  {
    "text": "So I guess, suppose n is\nbigger than big O r-squared",
    "start": "893860",
    "end": "904240"
  },
  {
    "text": "log-squared d, so alpha--",
    "start": "904240",
    "end": "919910"
  },
  {
    "text": "let me see. Maybe let's write in this way, I\nthink, just to avoid confusion.",
    "start": "919910",
    "end": "925530"
  },
  {
    "text": "So let c be a sufficiently\nlarge constant. ",
    "start": "925530",
    "end": "935840"
  },
  {
    "text": "Suppose n is bigger than 3 times\nr squared times log-squared d.",
    "start": "935840",
    "end": "943050"
  },
  {
    "text": "I think the dependency\nof the logarithmic factor is suboptimal. The dependency on the r\nprobably is also suboptimal.",
    "start": "943050",
    "end": "950490"
  },
  {
    "text": "And let alpha to be less\nthan some inverse poly",
    "start": "950490",
    "end": "955990"
  },
  {
    "text": "and to the c, then\nwhen the time t--",
    "start": "955990",
    "end": "963450"
  },
  {
    "text": "t is total number of steps--",
    "start": "963450",
    "end": "969490"
  },
  {
    "text": "is less than 1 over eta times\nsquare root d alpha and bigger",
    "start": "969490",
    "end": "975550"
  },
  {
    "text": "than log d over alpha over\neta, so for this range",
    "start": "975550",
    "end": "981870"
  },
  {
    "text": "of time steps, we have-- ",
    "start": "981870",
    "end": "987873"
  },
  {
    "text": "you can recover beta O-dot beta\nin L2 norm with our alpha times",
    "start": "987873",
    "end": "1000959"
  },
  {
    "text": "square root d. ",
    "start": "1000960",
    "end": "1009836"
  },
  {
    "text": "OK, so how do we interpret this? So I guess there are a few\nremarks for interpretations.",
    "start": "1009836",
    "end": "1018510"
  },
  {
    "text": " So the first thing is that--",
    "start": "1018510",
    "end": "1024803"
  },
  {
    "text": "I guess this is\nsomething probably I should have mentioned\nearly, so L-hat beta",
    "start": "1024803",
    "end": "1031439"
  },
  {
    "text": "has many global mins. ",
    "start": "1031440",
    "end": "1037760"
  },
  {
    "text": "And why? This is because of\noverparameterization,",
    "start": "1037760",
    "end": "1046890"
  },
  {
    "text": "because you have-- like, if\nyou count a degree of freedom, you have n data points\nand d parameters, right?",
    "start": "1046890",
    "end": "1052950"
  },
  {
    "text": "So you have more\ndegree of freedom than number of constraints, so\nyou have many, many minimums,",
    "start": "1052950",
    "end": "1059530"
  },
  {
    "text": "right? So that's one of the reasons\nwhy you have implicit bias. If you only have\none global minimum,",
    "start": "1059530",
    "end": "1065100"
  },
  {
    "text": "there's no way you can\nhave implicit bias. And second thing, how\ndo we interpret all",
    "start": "1065100",
    "end": "1072210"
  },
  {
    "text": "of these quantites in a bound? So the runtime lower\nbound depends only",
    "start": "1072210",
    "end": "1085870"
  },
  {
    "text": "on the logarithmic of alpha. So this means that you can\nchoose alpha to be anything",
    "start": "1085870",
    "end": "1090970"
  },
  {
    "text": "inverse polynomial. So alpha can be\ninverse poly, right?",
    "start": "1090970",
    "end": "1097960"
  },
  {
    "text": "You can choose basically\nthe constancy to be as-- to be a constant,\nso then the runtime",
    "start": "1097960",
    "end": "1103600"
  },
  {
    "text": "wouldn't be affected too much. And the error depends on alpha.",
    "start": "1103600",
    "end": "1111973"
  },
  {
    "text": "So, basically, if you want\na very, very small error, inverse-poly error, you can just\ntake alpha to be inverse poly,",
    "start": "1111973",
    "end": "1117950"
  },
  {
    "text": "and then our runtime is\nnot changed too much. And there's an upper\nbound on runtime,",
    "start": "1117950",
    "end": "1124260"
  },
  {
    "text": "which means that you need\nto do a early stopping or come to this bound.",
    "start": "1124260",
    "end": "1129495"
  },
  {
    "text": " So if you really\nbelieve in this,",
    "start": "1129495",
    "end": "1135059"
  },
  {
    "text": "you have to do early stopping,\nbut the early stopping is pretty mild, but\npretty mild because you",
    "start": "1135060",
    "end": "1140760"
  },
  {
    "text": "can see that the\nupper bound actually depends on inverse alpha. So if you take\nsomething like alpha to be 1 over d to the power\n10, then your upper bound",
    "start": "1140760",
    "end": "1147630"
  },
  {
    "text": "is pretty relaxed. You can run stuff for\na long time, right? So and you-- and actually,\nin practice we never",
    "start": "1147630",
    "end": "1153450"
  },
  {
    "text": "observed that you have to. If you really do the-- this synthetic example,\nreally run experiments,",
    "start": "1153450",
    "end": "1158790"
  },
  {
    "text": "you never have to early-stop. And that probably--\nI don't believe that you have to early-stop. This is more or less an\nartifact of the proof.",
    "start": "1158790",
    "end": "1166590"
  },
  {
    "text": "But this artifact is not\ntoo restricted anyways because it depends on alpha,\non the inverse of the alpha,",
    "start": "1166590",
    "end": "1174602"
  },
  {
    "text": "so you can take alpha\nto be small to make the bound very relaxed. So we didn't pay the attention\nto remove that completely",
    "start": "1174602",
    "end": "1182794"
  },
  {
    "text": "even though we believe\nthat it's possible.  Anyway, so basically,\nthe right way to use this",
    "start": "1182795",
    "end": "1189000"
  },
  {
    "text": "is that you take alpha to\nbe something super small, and then your error is very\nsmall and your runtime is--",
    "start": "1189000",
    "end": "1195342"
  },
  {
    "text": "your runtime lower bound is\nthe logarithmic in alpha.  OK so there is one small thing,\nis that alpha cannot be zero,",
    "start": "1195342",
    "end": "1207919"
  },
  {
    "text": "so why you don't create\nalpha to be zero? The only reason why alpha\ncannot be zero is because 0,",
    "start": "1207920",
    "end": "1214220"
  },
  {
    "text": "beta is 0, is a saddle point,\nso the nabla L-hat beta, it--",
    "start": "1214220",
    "end": "1225919"
  },
  {
    "text": "at 0 it's 0. This is the part that comes from\nthe quadratic parameterization,",
    "start": "1225920",
    "end": "1232910"
  },
  {
    "text": "right, so the-- we'll compute the gradient. You will see that\nbecause you have the quadratic\nparametrization, everything,",
    "start": "1232910",
    "end": "1239360"
  },
  {
    "text": "the gradient's always\nmultiplied by beta itself. So if beta is 0, then\nyou just have 0 gradient.",
    "start": "1239360",
    "end": "1246280"
  },
  {
    "text": "So if the gradient's 0\nand you don't-- if you-- and we are analyzing gradient\ndescent, no noise, nothing, right?",
    "start": "1246280",
    "end": "1251730"
  },
  {
    "text": "So null 1 is stochasticity,\nso if you started at 0, you would just\nstay there forever.",
    "start": "1251730",
    "end": "1257130"
  },
  {
    "text": "That's why you cannot use 0. That's in transition, but\nanything close to 0 is fine.",
    "start": "1257130",
    "end": "1264670"
  },
  {
    "text": "In some sense, this log 1\nover alpha that you'll pay, this log 1 over alpha\nis what-- how much time",
    "start": "1264670",
    "end": "1271090"
  },
  {
    "text": "you have to pay to\nleave the saddle point. So-- and leaving the saddle\npoint is actually very fast.",
    "start": "1271090",
    "end": "1276309"
  },
  {
    "text": "That's-- in some sense you\ncan kind of believe it, right, because you have\na saddle point-- how do I draw it? Like something like this, right?",
    "start": "1276310",
    "end": "1285840"
  },
  {
    "text": "So leaving it is kind\nof like you have-- it's optimizing a concave\nfunction, and you are going downhill, right?",
    "start": "1285840",
    "end": "1291930"
  },
  {
    "text": "You basically accelerate\nso fast that eventually you leave it very quickly.",
    "start": "1291930",
    "end": "1297190"
  },
  {
    "text": "So OK, cool. So right, and in some sense\nyou can interpret this as,",
    "start": "1297190",
    "end": "1314110"
  },
  {
    "text": "a gradient descent\nis preferring, prefers minimum\nnorm solution in L2.",
    "start": "1314110",
    "end": "1325390"
  },
  {
    "text": "So maybe preferring--\nsorry, I should-- like, preferring\nglobal minimum closest",
    "start": "1325390",
    "end": "1336620"
  },
  {
    "text": "to initialization, all right,\nbecause we have kind of claimed",
    "start": "1336620",
    "end": "1344730"
  },
  {
    "text": "that you are-- so actually here, I think we\nhave somewhat alluded to this,",
    "start": "1344730",
    "end": "1353130"
  },
  {
    "text": "but just to be\nformal in this case, you can prove the following. So you can prove that\nbeta star is actually",
    "start": "1353130",
    "end": "1360210"
  },
  {
    "text": "the argmin of the norm,\nwith the constraint",
    "start": "1360210",
    "end": "1366990"
  },
  {
    "text": "that you'll fit the data. Suppose you try to find a global\nminimum with the minimum L2",
    "start": "1366990",
    "end": "1374760"
  },
  {
    "text": "norm, right, so this is\nthe constraint, right? So this means you have a\nglobal minimum if it satisfies",
    "start": "1374760",
    "end": "1382230"
  },
  {
    "text": "everything and you\nminimized the 2 norm, then this is actually\nequal to beta star.",
    "start": "1382230",
    "end": "1388769"
  },
  {
    "text": "And the reason why\nthis is true is kind of similar to why the L1\nnorm works, right, just",
    "start": "1388770",
    "end": "1394350"
  },
  {
    "text": "because the minimum-- the 2 norm of beta is the\nsame as the 1 norm of theta.",
    "start": "1394350",
    "end": "1400950"
  },
  {
    "text": "And this one, if we replace this\nwith theta, then this is true.",
    "start": "1400950",
    "end": "1408639"
  },
  {
    "text": "And this part is by the standard\ntheory, which I didn't show,",
    "start": "1408640",
    "end": "1418930"
  },
  {
    "text": "but you know that if you look\nat all the linear models that fit the data and you're\nlooking at sparsest one,",
    "start": "1418930",
    "end": "1424600"
  },
  {
    "text": "it's going to be the theta. Actually, technically, I\nthink this should be argmin",
    "start": "1424600",
    "end": "1431650"
  },
  {
    "text": "to the O-dot 2 because-- sorry-- so the square\nroot because there",
    "start": "1431650",
    "end": "1437904"
  },
  {
    "text": "is-- there's a translation. But the objective is the\nsame, but the argmin, it",
    "start": "1437905",
    "end": "1443230"
  },
  {
    "text": "has the translation. I'm not sure what that makes. That's right, so like-- so if\nyou-- so maybe I-- let's see.",
    "start": "1443230",
    "end": "1449852"
  },
  {
    "text": "Maybe the easiest way to\nwrite this is the following. So these two are\nexactly the same.",
    "start": "1449852",
    "end": "1457000"
  },
  {
    "text": "This is because you\nhave a translation if you look at the min. All right, and for\nthe first objective,",
    "start": "1457000",
    "end": "1463688"
  },
  {
    "text": "the argmin is beta\nstar, and then you can somehow see that the\nargmin also transfers just",
    "start": "1463688",
    "end": "1470297"
  },
  {
    "text": "by taking the square root.  So-- and this is also the case\nfor linear regression, right?",
    "start": "1470297",
    "end": "1476410"
  },
  {
    "text": "Recall that we also\nproved that if you start with gradient descent with zero\nand you do linear regression,",
    "start": "1476410",
    "end": "1481562"
  },
  {
    "text": "you get the minimum norm\nsolution that fits the data. So it's very similar, at least\nfrom the-- on the surface,",
    "start": "1481562",
    "end": "1487120"
  },
  {
    "text": "from the formula. Like, you have almost\nthe same guarantee.",
    "start": "1487120",
    "end": "1494440"
  },
  {
    "text": "But I don't necessarily\nbelieve that this is always the case for all the-- like, I don't feel like you\nalways find the minimum norm",
    "start": "1494440",
    "end": "1502990"
  },
  {
    "text": "solution, the solution that\nis closest to initialization that fits the data.",
    "start": "1502990",
    "end": "1508030"
  },
  {
    "text": "I don't think this\nis always true. It's probably-- I think\nthere's still something",
    "start": "1508030",
    "end": "1513410"
  },
  {
    "text": "special about these examples. You know, we cannot just\nextrapolate generically.",
    "start": "1513410",
    "end": "1519700"
  },
  {
    "text": "OK, so now we are going\nto try to prove this.",
    "start": "1519700",
    "end": "1526179"
  },
  {
    "text": "Any questions so far? ",
    "start": "1526180",
    "end": "1536190"
  },
  {
    "text": "The proof of this theorem\nis pretty complicated.",
    "start": "1536190",
    "end": "1541570"
  },
  {
    "text": "I will try to finish it in\none lecture, but if we cannot, I think I'm going to\nrefer you to the notes.",
    "start": "1541570",
    "end": "1547740"
  },
  {
    "text": "The notes has a pretty\ndetailed derivation. So to kind of get\nsome preparation,",
    "start": "1547740",
    "end": "1556300"
  },
  {
    "text": "let's try to understand some\nbasic stuff about this loss function.",
    "start": "1556300",
    "end": "1561820"
  },
  {
    "text": "So first of all, let's look\nat the population risk.",
    "start": "1561820",
    "end": "1571669"
  },
  {
    "text": "This is the population\nthat we had. The population risk is y minus\nbeta O-dot beta times x square.",
    "start": "1571670",
    "end": "1584710"
  },
  {
    "text": "Right, so-- and you can try\nto get rid of the expectation",
    "start": "1584710",
    "end": "1592220"
  },
  {
    "text": "because this is population. So what you'd do is, you'd\nplug in a definition of y,",
    "start": "1592220",
    "end": "1598160"
  },
  {
    "text": "so you'd get beta star O-dot\nbeta star minus beta O-dot beta",
    "start": "1598160",
    "end": "1603620"
  },
  {
    "text": "times x square. ",
    "start": "1603620",
    "end": "1609910"
  },
  {
    "text": "And then this gives you--  I think I have 1/4 here.",
    "start": "1609910",
    "end": "1615960"
  },
  {
    "text": "That's my population,\nright, because I know I have an additional\n1/4 everywhere, so then this",
    "start": "1615960",
    "end": "1623940"
  },
  {
    "text": "becomes 1/4 times the norm--",
    "start": "1623940",
    "end": "1629519"
  },
  {
    "text": "the difference in norm. ",
    "start": "1629520",
    "end": "1637470"
  },
  {
    "text": "This is just because\nthe expectation of some vector times x\nsquared, if x is Gaussian,",
    "start": "1637470",
    "end": "1645370"
  },
  {
    "text": "this is equal to a norm of\nv, 2 norm, square of that.",
    "start": "1645370",
    "end": "1652650"
  },
  {
    "text": "So-- and I'm going to\nclaim the following.",
    "start": "1652650",
    "end": "1661030"
  },
  {
    "text": "So you are going to have uniform\nconvergence for sparse beta,",
    "start": "1661030",
    "end": "1672990"
  },
  {
    "text": "so-- but we don't have\nuniform convergence over the entire space,\nright, because we have overparameterization. There's the-- if you\nhave uniform convergence",
    "start": "1672990",
    "end": "1680610"
  },
  {
    "text": "for everything,\nthen they wouldn't have an impressive\nregularization effect. So that would be kind\nof the classical theory",
    "start": "1680610",
    "end": "1688320"
  },
  {
    "text": "that we discussed in the first\npart of the course, right?",
    "start": "1688320",
    "end": "1695529"
  },
  {
    "text": "So but we claim that if\nyou look at sparse beta, then you have\nuniform convergence. So here is the--",
    "start": "1695530",
    "end": "1702179"
  },
  {
    "text": "I'm going to build towards this. So first there's a claim which\nis, with high probability",
    "start": "1702180",
    "end": "1713420"
  },
  {
    "text": "over the choice of data-- ",
    "start": "1713420",
    "end": "1722309"
  },
  {
    "text": "so for-- if n is bigger than\nsomething like O tilde of r",
    "start": "1722310",
    "end": "1730400"
  },
  {
    "text": "over delta square, then for\nevery v such that with--",
    "start": "1730400",
    "end": "1742865"
  },
  {
    "text": " so support of v is less\nthan r-- oh, I guess,",
    "start": "1742865",
    "end": "1750110"
  },
  {
    "text": "alternatively we can write the\n0 norm of v is less than r-- then you have the following,\nso the empirical average",
    "start": "1750110",
    "end": "1762800"
  },
  {
    "text": "of this kind of thing, right? So why do we care about this? I guess this is--\nprobably can be seen here.",
    "start": "1762800",
    "end": "1767815"
  },
  {
    "text": "All right, so this is-- the\npopulation has this form, something like v dot x squared,\nand you take this expectation.",
    "start": "1767815",
    "end": "1773779"
  },
  {
    "text": "And this is the empirical one. I'm going to be more\nexplicit in a moment, but this is kind of\nlike a small tool.",
    "start": "1773780",
    "end": "1781787"
  },
  {
    "text": "So we are saying that if you\nhave this empirical version of this v dot x\nsquared and it's going",
    "start": "1781787",
    "end": "1787030"
  },
  {
    "text": "to be very close\nto the population version-- the population is\njust the 2 norm of v squared, right, so it's going to be\nvery close to the population,",
    "start": "1787030",
    "end": "1800460"
  },
  {
    "text": "but only for v that is sparse. So this concentration\nonly works--",
    "start": "1800460",
    "end": "1806220"
  },
  {
    "text": "so if you have\nenough n's, right, if n is infinite or\nclose to infinite,",
    "start": "1806220",
    "end": "1812580"
  },
  {
    "text": "then you should expect\nthis to work for every v just because this is\nthe law of large number",
    "start": "1812580",
    "end": "1817860"
  },
  {
    "text": "or, like, concentration\ninequalities, right? But here the\nconcentration inequality is more subtle or more--",
    "start": "1817860",
    "end": "1824400"
  },
  {
    "text": "kind of like there's\na finesse here because you only care\nabout v's that are sparse,",
    "start": "1824400",
    "end": "1831179"
  },
  {
    "text": "and also you only have\nthis many of examples. You don't have a lot of\nexamples. n is not bigger",
    "start": "1831180",
    "end": "1838050"
  },
  {
    "text": "than d, even. n is only\nbigger than the sparsity of d. So this is-- and actually\nthis is also called so--",
    "start": "1838050",
    "end": "1848820"
  },
  {
    "text": "and also just for\nthe language, I guess this is something\nuseful to know. You know, we don't\nreally have the--",
    "start": "1848820",
    "end": "1855930"
  },
  {
    "text": "depend on these\nkind of properties, but we say that if a vector\nsatisfies this condition--",
    "start": "1855930",
    "end": "1865520"
  },
  {
    "text": "suppose I call it 3, so\nsuppose xi's satisfy 3.",
    "start": "1865520",
    "end": "1878790"
  },
  {
    "text": "Then we call-- then we say this\nsatisfies the RIP condition.",
    "start": "1878790",
    "end": "1888130"
  },
  {
    "text": "So, basically a 3 is called\nr,delta-RIP condition.",
    "start": "1888130",
    "end": "1895940"
  },
  {
    "text": "The acronym is a little\nbit kind of weird, but I think there-- it's\nstanding for Restricted",
    "start": "1895940",
    "end": "1903379"
  },
  {
    "text": "Isometry condition. The reason why it's\ncalled restricted",
    "start": "1903380",
    "end": "1909460"
  },
  {
    "text": "is because you are only\nrestricting to vectors v, right, but if you are not\nrestricting to vector v,",
    "start": "1909460",
    "end": "1914980"
  },
  {
    "text": "then this is kind of\nlike isometry condition because you are basically\nsaying that all the xi's are",
    "start": "1914980",
    "end": "1920919"
  },
  {
    "text": "isometric, right? They are kind of\nspreading the entire-- all the directions\nequally, right?",
    "start": "1920920",
    "end": "1926260"
  },
  {
    "text": "The xi's have\nconverged that energy. That's pretty much\nwhat you are saying. Right, so if you don't-- if\nyou require this for every v,",
    "start": "1926260",
    "end": "1933520"
  },
  {
    "text": "then you are saying that the\ncovariance of-- so what this is really saying, this equation\n3 is really just equivalent",
    "start": "1933520",
    "end": "1940750"
  },
  {
    "text": "to sum of xi xi-transpose\ntimes v transpose times v, is--",
    "start": "1940750",
    "end": "1949850"
  },
  {
    "text": " right, this is bounded by\nv-transpose I times v times 1",
    "start": "1949850",
    "end": "1959990"
  },
  {
    "text": "plus delta, larger than 1 minus\ndelta times v-transpose I times v. Right, so if you\nrequire this for every v,",
    "start": "1959990",
    "end": "1968150"
  },
  {
    "text": "so \"suppose require\nfor every v,\"",
    "start": "1968150",
    "end": "1977670"
  },
  {
    "text": "then what this is saying is that\nthe sum of xi xi-transpose 1",
    "start": "1977670",
    "end": "1983962"
  },
  {
    "text": "over n is, in PSD sense-- ",
    "start": "1983962",
    "end": "1990780"
  },
  {
    "text": "how do I write this? Wait, how do I write in PS--",
    "start": "1990780",
    "end": "1996350"
  },
  {
    "text": "wait. Oh, OK. Right, we'll put that\nkind of notation for PSD,",
    "start": "1996350",
    "end": "2002284"
  },
  {
    "text": "yeah, like this, less than\n1 plus delta times an entity and larger than 1 minus\ndelta times an entity.",
    "start": "2002285",
    "end": "2009355"
  },
  {
    "text": "So if you require\nit for every v, you are basically saying the\ncovariance of xi are iso--",
    "start": "2009355",
    "end": "2014760"
  },
  {
    "text": " I think it's called--\nnot called isometric.",
    "start": "2014760",
    "end": "2019950"
  },
  {
    "text": "It's called iso-- isometric? Isoparametric-- it's just\ncovariance itself, so entity,",
    "start": "2019950",
    "end": "2027420"
  },
  {
    "text": "right? That's-- I'm blanking\non the words. So, basically you are saying the\ncovariance close to an entity. But you are not requiring\nit for every v, right,",
    "start": "2027420",
    "end": "2034970"
  },
  {
    "text": "and then also this is\nnot true for if you don't have enough data, right? So we only have n is less than\nd data points, so in our case,",
    "start": "2034970",
    "end": "2042380"
  },
  {
    "text": "this matrix is not\neven full rank. How come you can expect\na full-- this is-- this only has rank r because--",
    "start": "2042380",
    "end": "2049790"
  },
  {
    "text": "it only has rank n because\nwe only have n data points and it's less than d, so it's\nnot even full-rank matrix.",
    "start": "2049790",
    "end": "2055489"
  },
  {
    "text": "How come this can be\nclose to an entity? There's no way, right? But if we look at\nthe quadratic form,",
    "start": "2055489",
    "end": "2061054"
  },
  {
    "text": "right, so if you look\nat the quadratic form and you only look at the\nquadratic form evaluated",
    "start": "2061055",
    "end": "2066530"
  },
  {
    "text": "on sparse vector\nv, then this matrix becomes effected to\nlook like an entity.",
    "start": "2066530",
    "end": "2073822"
  },
  {
    "text": "That's basically what\nthis condition is saying. ",
    "start": "2073822",
    "end": "2079000"
  },
  {
    "text": "Right, OK, so and once\nyou have this lemma,",
    "start": "2079000",
    "end": "2084070"
  },
  {
    "text": "or this claim, then\nwe know that you have the uniform convergence for\nbeta sparse, so a sparse beta.",
    "start": "2084070",
    "end": "2097390"
  },
  {
    "text": " And this is just because L-hat\nbeta is this 1 times 4 times",
    "start": "2097390",
    "end": "2110089"
  },
  {
    "text": "1 over n times sum\nof beta O-dot beta",
    "start": "2110090",
    "end": "2115400"
  },
  {
    "text": "minus beta star O-dot\nbeta star times xi square.",
    "start": "2115400",
    "end": "2121799"
  },
  {
    "text": "And this is of this form, right? So you can treat this as v,\nand then you are in this form,",
    "start": "2121800",
    "end": "2128450"
  },
  {
    "text": "the v dot xi square, right? And this v is sparse\nif beta is sparse",
    "start": "2128450",
    "end": "2133840"
  },
  {
    "text": "and beta stars are sparse. Beta star is already sparse. That's our assumption. And if beta is sparse, then\nthis thing is also sparse.",
    "start": "2133840",
    "end": "2141760"
  },
  {
    "text": "You pay 2 r-sparse, right? They were r-sparse, and\nnow this whole thing",
    "start": "2141760",
    "end": "2149050"
  },
  {
    "text": "would be probably 2\nr-sparse, at most. So then this means that\nthis is close to the norm,",
    "start": "2149050",
    "end": "2157600"
  },
  {
    "text": "right, so 1 times 4\ntimes the norm of this. ",
    "start": "2157600",
    "end": "2164359"
  },
  {
    "text": "And this is equal to L beta. Right, so for sparse beta,\nyou have uniform convergence,",
    "start": "2164360",
    "end": "2172250"
  },
  {
    "text": "but you don't have\nuniform convergence over the entire space. So in some sense--",
    "start": "2172250",
    "end": "2177320"
  },
  {
    "text": " so and also you can\nhave uniform convergence",
    "start": "2177320",
    "end": "2187089"
  },
  {
    "text": "for the gradient of this if\nyou really care about it.",
    "start": "2187090",
    "end": "2193150"
  },
  {
    "text": "I guess-- I think I will\nshow this later for sparse.",
    "start": "2193150",
    "end": "2201345"
  },
  {
    "text": "Right, so you can\neven show the gradient concentrates around\nthe expected gradient.",
    "start": "2201345",
    "end": "2208450"
  },
  {
    "text": "The empirical\ngradient concentrates around the population\ngradient for sparse beta.",
    "start": "2208450",
    "end": "2216820"
  },
  {
    "text": "So however, on the other hand,\nthere exists dense theta.",
    "start": "2216820",
    "end": "2224070"
  },
  {
    "text": " Such that, for example,\nL hat beta is 0.",
    "start": "2224070",
    "end": "2235329"
  },
  {
    "text": "But L beta is very\nmuch lower than 0. So they are\noverweighting positions.",
    "start": "2235330",
    "end": "2242530"
  },
  {
    "text": "There are places where you\ndon't have the proper training and test loss are similar.",
    "start": "2242530",
    "end": "2248810"
  },
  {
    "text": "So but those are dense beta. OK. So the question is, why you are\nfinding a sparse one but not",
    "start": "2248810",
    "end": "2255850"
  },
  {
    "text": "a dense one, right? Because the dense one doesn't\nhave the nice property. So the main intuition\nis the following.",
    "start": "2255850",
    "end": "2262110"
  },
  {
    "text": "So we have done quite\nsome preparation. So the main intuition or what\nwe believe to be happening",
    "start": "2262110",
    "end": "2270490"
  },
  {
    "text": "is that following. So you can think of\nthis maybe different Xr to be the site of\nvectors that are sparse.",
    "start": "2270490",
    "end": "2280569"
  },
  {
    "text": "So beta such that beta r sparse. Let's see whether I used the--",
    "start": "2280570",
    "end": "2288630"
  },
  {
    "text": "so and-- so supposed\nyou look at a space. So you have an entire\nspace which is probably",
    "start": "2288630",
    "end": "2295430"
  },
  {
    "text": "something very large. And 0 is somewhere here. It's the origin.",
    "start": "2295430",
    "end": "2301760"
  },
  {
    "text": "And you have some family\nof, let's call this, Xr.",
    "start": "2301760",
    "end": "2307130"
  },
  {
    "text": "This is the family\nof sparse vectors. And you know in this Xr,\neverything behaves so nicely.",
    "start": "2307130",
    "end": "2315120"
  },
  {
    "text": "The training and tests are\njust basically the same up",
    "start": "2315120",
    "end": "2320155"
  },
  {
    "text": "to some small error, right? So the training on test-- and\nalso in terms of gradients, they are similar. The gradient of L hat and\ngradient of L are similar.",
    "start": "2320155",
    "end": "2327270"
  },
  {
    "text": "And I think basically\nwhat happens is that you start from\nsomewhere close to 0. And the reason you cannot start\nat 0 just because the setup",
    "start": "2327270",
    "end": "2334349"
  },
  {
    "text": "point, not very important. And you can think of\na gradient descent.",
    "start": "2334350",
    "end": "2339420"
  },
  {
    "text": "So you are doing\ngradient descent on the empirical\nloss, L hat beta.",
    "start": "2339420",
    "end": "2347450"
  },
  {
    "text": "And because you have\nuniform convergence, you're basically\ndoing the same thing as gradient descent on\nL beta as long as you",
    "start": "2347450",
    "end": "2356210"
  },
  {
    "text": "don't leave this at Xr, right? So if you leave it, there\nis all bets are off. But if you don't leave\nit, it's fine, right?",
    "start": "2356210",
    "end": "2363440"
  },
  {
    "text": "So it turns out\nthat what happens is that if you do\ngradient descent, you can consider the\nalternative world where",
    "start": "2363440",
    "end": "2369859"
  },
  {
    "text": "you do gradient descent\non a population. So let's say this is\nthe gradient descent",
    "start": "2369860",
    "end": "2377280"
  },
  {
    "text": "on a population loss L beta. And it turns out, if\nyou do gradient descent on a population, you\nare going to reach",
    "start": "2377280",
    "end": "2383880"
  },
  {
    "text": "a point, which is\nbeta star, which is on the boundary of this set.",
    "start": "2383880",
    "end": "2389700"
  },
  {
    "text": "And also in this trajectory,\nyou never leave this set Xr.",
    "start": "2389700",
    "end": "2396410"
  },
  {
    "text": "So now, because we believe that\nthe black trajectory is similar",
    "start": "2396410",
    "end": "2401480"
  },
  {
    "text": "to the purple trajectory as\nlong as they are in the set Xr. And the purple trajectory\nnever leaves the set Xr.",
    "start": "2401480",
    "end": "2410000"
  },
  {
    "text": "So that's why the\nblack trajectory also converts to beta star. ",
    "start": "2410000",
    "end": "2416210"
  },
  {
    "text": "I'm not sure if\nthat makes sense. So basically, the purple one\nis the population trajectory.",
    "start": "2416210",
    "end": "2421280"
  },
  {
    "text": "And the black one is the\nempirical trajectory. So you know that the empirical\ntrajectory and the population",
    "start": "2421280",
    "end": "2427340"
  },
  {
    "text": "trajectory are\nsimilar in the set Xr. You don't know anything\nabout outside world, right?",
    "start": "2427340",
    "end": "2433910"
  },
  {
    "text": "And also, the purple trajectory\nnever leave the set Xr. Then the black one probably\nshouldn't leave as well.",
    "start": "2433910",
    "end": "2439410"
  },
  {
    "text": "And the black one should be\nsimilar to the purple one. So for example, suppose\nthe purple trajectory",
    "start": "2439410",
    "end": "2445130"
  },
  {
    "text": "looks like this.  Suppose that's what's happening.",
    "start": "2445130",
    "end": "2450970"
  },
  {
    "text": "Then you lose control. Because at the\nbeginning, you are following the proper structure.",
    "start": "2450970",
    "end": "2456970"
  },
  {
    "text": "And then you leave the set. And then all bets are off. You don't have any\ncontrol anymore.",
    "start": "2456970",
    "end": "2462190"
  },
  {
    "text": "But this turns out to\nbe not what's happening. What's happening is that\nthe purple one actually stays in the set\nXr for a long time",
    "start": "2462190",
    "end": "2469960"
  },
  {
    "text": "until it reaches beta star. And then, it's\nstay at beta star. So that's why this alternative\nsituation doesn't happen.",
    "start": "2469960",
    "end": "2478430"
  },
  {
    "text": "This doesn't happen. This is not what's happening. And inside this Xr,\neverything behave nicely.",
    "start": "2478430",
    "end": "2485587"
  },
  {
    "text": "There's only a global\nminimum, which is beta star. There's nothing else. And all set Xr, there are a\nbunch of different things.",
    "start": "2485587",
    "end": "2491099"
  },
  {
    "text": "So all set Xr you can\nimagine that there is a-- let's do a different color. So outside Xr there is\nprobably quite a bunch",
    "start": "2491100",
    "end": "2498950"
  },
  {
    "text": "of overfitting solutions. ",
    "start": "2498950",
    "end": "2504359"
  },
  {
    "text": "So these are all solutions\nthat makes your protocols 0.",
    "start": "2504360",
    "end": "2510560"
  },
  {
    "text": "There are so many\nof these solutions. But you never get to actually\neven go to those places just because your\nblack trajectory is",
    "start": "2510560",
    "end": "2519079"
  },
  {
    "text": "emitting the purple one. And the purple one didn't\ngo to those places. And the black one doesn't go\nto those places either, right?",
    "start": "2519080",
    "end": "2526300"
  },
  {
    "text": "So that's the intuition\nwhy this is working. Any questions?",
    "start": "2526300",
    "end": "2533216"
  },
  {
    "text": "[INAUDIBLE] beta doesn't\nleave the [INAUDIBLE]??",
    "start": "2533216",
    "end": "2540250"
  },
  {
    "text": "But why the purple one\ndoesn't leave the-- yeah. So that's not-- I don't have a-- I didn't give a\njustification either, right?",
    "start": "2540250",
    "end": "2546833"
  },
  {
    "text": "That's something\nwe're going to prove. Yeah. And I don't think this is\nsomething about the property",
    "start": "2546833",
    "end": "2554700"
  },
  {
    "text": "of this problem. And if you see the proof\nis not that surprising.",
    "start": "2554700",
    "end": "2561900"
  },
  {
    "text": "Because you are gradually-- you\nare in some sense trying to-- it's a local search\nalgorithm, right? So you are trying to search\nyour neighborhood first, right?",
    "start": "2561900",
    "end": "2569730"
  },
  {
    "text": "So you gradually-- you start\nfrom 0, somewhere close to 0. You are gradually\nsearching your neighborhood",
    "start": "2569730",
    "end": "2576030"
  },
  {
    "text": "until you find a global minimum. That's why you don't\nwant-- probably wouldn't go this circuitous way.",
    "start": "2576030",
    "end": "2583950"
  },
  {
    "text": "So you're going to go more\nstraight to the closest point.",
    "start": "2583950",
    "end": "2589500"
  },
  {
    "text": "But the real proof has to\ngo through the math, yeah. My other question is the\ninitialization scheme we described before\nisn't in X bar.",
    "start": "2589500",
    "end": "2598050"
  },
  {
    "text": "It's [INAUDIBLE]?  Yeah. That's a great question.",
    "start": "2598050",
    "end": "2603329"
  },
  {
    "text": "So the initialization\nalpha times 1, right, so this is literally\nspeaking is not in Xr,",
    "start": "2603330",
    "end": "2611085"
  },
  {
    "text": "right, it's this. And I don't ask this\nquestion many times.",
    "start": "2611085",
    "end": "2616290"
  },
  {
    "text": "And I think the right\nway to think about this, I think I have some\nremarks somewhere else. But you ask me earlier.",
    "start": "2616290",
    "end": "2621630"
  },
  {
    "text": "I should probably should\njust answer it here. So the question is why the\ntransition is not in Xr?",
    "start": "2621630",
    "end": "2627420"
  },
  {
    "text": "But I think the thing to think\nabout is that, of course, it's not exactly\nin this sparse set.",
    "start": "2627420",
    "end": "2633680"
  },
  {
    "text": "But it's close. And close in what sense? Close in a sense that\nalpha 1 is very close to 0.",
    "start": "2633680",
    "end": "2642020"
  },
  {
    "text": "And 0 is in this set. So that's kind of the\nproperty we're going to use.",
    "start": "2642020",
    "end": "2649010"
  },
  {
    "text": "So yes, so you are\nright, that we can never say exactly a set Xr. You are going to say that you\nare in the neighborhood of Xr",
    "start": "2649010",
    "end": "2655760"
  },
  {
    "text": "with a little bit small error. And the error is very small. It depends on alpha. So that's why we have to\nchoose alpha to be very small.",
    "start": "2655760",
    "end": "2664180"
  },
  {
    "text": "So in some sense, you\nreally want to choose 0. So from all of this discussion,\nthe only thing you want to do",
    "start": "2664180",
    "end": "2670570"
  },
  {
    "text": "is to choose 0. And 0 it just happens\nto be a saddle point. That's unfortunate.",
    "start": "2670570",
    "end": "2675640"
  },
  {
    "text": "So you have to perturb\nit a little bit. ",
    "start": "2675640",
    "end": "2681276"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2681276",
    "end": "2695360"
  },
  {
    "text": "The question is whether\nthis particular property has anything to do with the\npositivity of beta, right?",
    "start": "2695360",
    "end": "2701420"
  },
  {
    "text": "So I don't think so.",
    "start": "2701420",
    "end": "2708329"
  },
  {
    "text": "Because so are you talking about\na positive beta start or beta, the variable beta?",
    "start": "2708330",
    "end": "2714132"
  },
  {
    "text": "Beta star. Beta star. Right, so we assume the\nbeta star to be positive.",
    "start": "2714133",
    "end": "2720789"
  },
  {
    "text": "I think, no matter beta\nstar or beta is positive. Sorry. No matter whether\nbeta star is positive,",
    "start": "2720790",
    "end": "2728360"
  },
  {
    "text": "beta star square\nis always positive. ",
    "start": "2728360",
    "end": "2733940"
  },
  {
    "text": "So you can-- if you\ninitialize from this, then you just always\ngo to the positive. It's always keep being positive.",
    "start": "2733940",
    "end": "2739710"
  },
  {
    "text": "So basically you just learn the\nabsolute value of beta star. And if learning the\nabsolute value of beta star",
    "start": "2739710",
    "end": "2744980"
  },
  {
    "text": "is not that different\nfrom learning beta star. So basically, suppose you\ndon't restrict beta star",
    "start": "2744980",
    "end": "2751670"
  },
  {
    "text": "to be positive. Then you cannot claim that\nyou recover beta star. You can only say you recover\nthe absolute value of beta star.",
    "start": "2751670",
    "end": "2760849"
  },
  {
    "text": "But the picture, the\nintuition is still the same after that changes. ",
    "start": "2760850",
    "end": "2770799"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2770800",
    "end": "2786660"
  },
  {
    "text": "Yeah. So the question-- [INAUDIBLE]  Yeah.",
    "start": "2786660",
    "end": "2792170"
  },
  {
    "text": "So I guess the question\nis whether we really have to be exactly alpha\ntimes 0,1 vector, right?",
    "start": "2792170",
    "end": "2797299"
  },
  {
    "text": "So and the answer is-- this is a great question--\nthe answer is no, you don't have to do that.",
    "start": "2797300",
    "end": "2802343"
  },
  {
    "text": "The only thing you have to do is\nthat you initialization beta 0. This is a vector. I think you only need to\nmake sure every entry of it",
    "start": "2802343",
    "end": "2809390"
  },
  {
    "text": "is very small. So you only need to make\nsure this infinite norm is very small, less than\nsomething like alpha.",
    "start": "2809390",
    "end": "2815780"
  },
  {
    "text": "And yeah, and you can even\ninitialize negatively, I think.",
    "start": "2815780",
    "end": "2821420"
  },
  {
    "text": "So if you initialize\nnegatively, then the action will become negative eventually. But the sum doesn't\nmatter that much.",
    "start": "2821420",
    "end": "2827250"
  },
  {
    "text": "So that's why.  Yeah, but yeah, so and\nI'm only doing this just",
    "start": "2827250",
    "end": "2835670"
  },
  {
    "text": "for convenience. Because it makes\nthe proof cleaner. ",
    "start": "2835670",
    "end": "2845550"
  },
  {
    "text": "So given this plan,\nthis intuition, so it's natural that we should\nstart analyzing the population",
    "start": "2845550",
    "end": "2852240"
  },
  {
    "text": "trajectory, right,\nthe purple one. So and then we try to\nsay that the black one is",
    "start": "2852240",
    "end": "2857610"
  },
  {
    "text": "close to purple one. So let's start with the\npopulation trajectory. ",
    "start": "2857610",
    "end": "2873299"
  },
  {
    "text": "So you can sometimes think\nof this as a warm-up. Or in some sense,\nthis is also a kind",
    "start": "2873300",
    "end": "2879540"
  },
  {
    "text": "of sanity check for\nthis approach, right? ",
    "start": "2879540",
    "end": "2884910"
  },
  {
    "text": "So this is-- let me state\nthe theorem formally. But I think you are expecting\nwhat the theorem is saying.",
    "start": "2884910",
    "end": "2890070"
  },
  {
    "text": "Where GD on the population loss\nwill converge to beta star.",
    "start": "2890070",
    "end": "2896560"
  },
  {
    "text": "And in, I think, O\nof log 1 over epsilon",
    "start": "2896560",
    "end": "2904430"
  },
  {
    "text": "alpha over eta iteration with\nepsilon error in L2 distance,",
    "start": "2904430",
    "end": "2915680"
  },
  {
    "text": "right? ",
    "start": "2915680",
    "end": "2920980"
  },
  {
    "text": "So but I guess the formal\ntheorem matters less than the proof.",
    "start": "2920980",
    "end": "2926470"
  },
  {
    "text": "Let's see how the proof goes. The proof is kind\nof brute force. Because you just really\nliterally control what",
    "start": "2926470",
    "end": "2935410"
  },
  {
    "text": "each of the\ncoordinates is doing. So it's pretty explicit. And you see how the\ncoordinates are changing.",
    "start": "2935410",
    "end": "2943630"
  },
  {
    "text": "But explicitness is actually\na weakness in some sense. Because we are doing\nso explicit derivation,",
    "start": "2943630",
    "end": "2949180"
  },
  {
    "text": "it's great for this problem. But it's harder\nto be extendable.",
    "start": "2949180",
    "end": "2954369"
  },
  {
    "text": "I think that's a general thing. So if you have a various kind\nof strong analysis for toy case,",
    "start": "2954370",
    "end": "2960668"
  },
  {
    "text": "then that's not necessarily\nalways the good case. Because if it's too\nstrong, too explicit, then the expandability, the\napplicability to broader case",
    "start": "2960668",
    "end": "2970150"
  },
  {
    "text": "becomes a problem. And this is, in my opinion,\nprobably the main reason why we",
    "start": "2970150",
    "end": "2976990"
  },
  {
    "text": "cannot extend to more general\ncases other than this simple quadratic one. There's a small-- there's\nextension to the matrices case,",
    "start": "2976990",
    "end": "2985960"
  },
  {
    "text": "but not fundamental extension. So you can change all\nof these to matrices into the vectors,\nthat's still fine.",
    "start": "2985960",
    "end": "2991359"
  },
  {
    "text": "But not beyond that. So but still, anyway,\nlet me do an analysis.",
    "start": "2991360",
    "end": "2997040"
  },
  {
    "text": "So the proof sketch is that\nyou first compute the gradient. We call that L beta.",
    "start": "2997040",
    "end": "3002340"
  },
  {
    "text": " So L beta is equal\nto 1/4 times beta",
    "start": "3002340",
    "end": "3009470"
  },
  {
    "text": "over the beta minus beta star,\nover beta star 2 norm square.",
    "start": "3009470",
    "end": "3014619"
  },
  {
    "text": "And you can compute a\ngradient with respect to beta. It becomes beta\nover the beta when",
    "start": "3014620",
    "end": "3020267"
  },
  {
    "text": "it's beta star over beta\nstar times O dot beta.",
    "start": "3020267",
    "end": "3026620"
  },
  {
    "text": "I guess you can verify this\nwith pretty much scalars. But the vector\nversion is pretty much",
    "start": "3026620",
    "end": "3033400"
  },
  {
    "text": "taking the sum of the scalar,\nall the dimensions, right? So here, all the\ndimensions are separated.",
    "start": "3033400",
    "end": "3039730"
  },
  {
    "text": "So basically, this is just\nthe sum of the objective. And each objective is\nabout one coordinate.",
    "start": "3039730",
    "end": "3046240"
  },
  {
    "text": "And this is just a\nsimple chain rule. And you can see that everything\nis multiplied by beta always, right?",
    "start": "3046240",
    "end": "3051910"
  },
  {
    "text": "The gradient is always\nmultiplied with beta. And this is why\ngradient L0 is 0.",
    "start": "3051910",
    "end": "3058390"
  },
  {
    "text": "So and now, let's\nlook at the update. The update will be\nbeta t plus 1 equals",
    "start": "3058390",
    "end": "3065050"
  },
  {
    "text": "to beta t minus eta\ntimes this beta t O dot, beta t minus O dot\nbeta star, times O dot beta t.",
    "start": "3065050",
    "end": "3076450"
  },
  {
    "text": " So and this is really-- this is\neverything is in B-dimension.",
    "start": "3076450",
    "end": "3082780"
  },
  {
    "text": "But really, you can view\nthis as d separate update",
    "start": "3082780",
    "end": "3092490"
  },
  {
    "text": "in d coordinates. Because each coordinates\nare not doing--",
    "start": "3092490",
    "end": "3098520"
  },
  {
    "text": "having any kind of correlation\nwith anything else. So this is really\njust the saying",
    "start": "3098520",
    "end": "3104520"
  },
  {
    "text": "that Bti is equals to Bti minus\neta Bti squared minus B star i",
    "start": "3104520",
    "end": "3115230"
  },
  {
    "text": "squared times Bti, OK? ",
    "start": "3115230",
    "end": "3121280"
  },
  {
    "text": "So every coordinate are\njust doing separate things. And maybe it's useful-- but this\ndifferent coordinates, right,",
    "start": "3121280",
    "end": "3130800"
  },
  {
    "text": "has a little bit differences\nbecause this one is different. Otherwise, all the coordinates\nare doing the same thing.",
    "start": "3130800",
    "end": "3137170"
  },
  {
    "text": "So the target is different. So in some sense, you\nare basically-- so when",
    "start": "3137170",
    "end": "3145110"
  },
  {
    "text": "i is in a support of beta\nstar, which is denoted to be s,",
    "start": "3145110",
    "end": "3152240"
  },
  {
    "text": "so then your update\nis basically beta i is update to be beta i\nminus eta beta i squared minus",
    "start": "3152240",
    "end": "3159710"
  },
  {
    "text": "1 beta i. And when I guess that\nmoment at t, just",
    "start": "3159710",
    "end": "3167460"
  },
  {
    "text": "for notational simplicity. And if i is not in a\nsupport of beta star,",
    "start": "3167460",
    "end": "3173860"
  },
  {
    "text": "then beta i is just\nupdate to be beta i minus eta beta i squared cube.",
    "start": "3173860",
    "end": "3181440"
  },
  {
    "text": " So and you can see all of\nthis intuitively makes sense.",
    "start": "3181440",
    "end": "3187339"
  },
  {
    "text": "Because suppose this\nis the case, then if-- so beta i is supposedly\nis less than 1 between 0.",
    "start": "3187340",
    "end": "3195350"
  },
  {
    "text": "Let's suppose that's\nthe current beta i. And this number is negative.",
    "start": "3195350",
    "end": "3201080"
  },
  {
    "text": "This is positive. So this whole thing is negative.",
    "start": "3201080",
    "end": "3206330"
  },
  {
    "text": "So you are trying to\nincrease beta i, right? So basically, this\nupdate is trying",
    "start": "3206330",
    "end": "3212000"
  },
  {
    "text": "to increase beta i if beta\ni is not yet reaching 1. And this update is doing the\nreverse direction, right?",
    "start": "3212000",
    "end": "3219380"
  },
  {
    "text": "So this is trying to say that\nas long as your beta i is bigger than 0, then you are\ntrying to decrease your beta i.",
    "start": "3219380",
    "end": "3226020"
  },
  {
    "text": "So basically here, this\nencourages beta i to go to 1,",
    "start": "3226020",
    "end": "3231970"
  },
  {
    "text": "and this encourages\nbeta i to go to 0. And that makes sense,\nbecause 1 is the beta 1 star,",
    "start": "3231970",
    "end": "3237819"
  },
  {
    "text": "and 0 is the beta i star\nin the other case, right? ",
    "start": "3237820",
    "end": "3246967"
  },
  {
    "text": "OK. So now, let's try to do a\nmore detailed calculation to see what happens\nin each of this case.",
    "start": "3246967",
    "end": "3254460"
  },
  {
    "text": "So let's first look\nat the case one. Let's say this is case\none, and this is case two.",
    "start": "3254460",
    "end": "3259770"
  },
  {
    "text": "So case one.  So here we are trying\nto-- the update",
    "start": "3259770",
    "end": "3265400"
  },
  {
    "text": "is trying to increase\nbeta i until it reaches 1. So there are still\ntwo separate cases.",
    "start": "3265400",
    "end": "3271260"
  },
  {
    "text": "The first case is that\nsuppose beta i is less than-- at some time t is less than 1/2.",
    "start": "3271260",
    "end": "3277559"
  },
  {
    "text": "So you are only 1/2\ndone with your work. And then, you can see\nwhat's the changes.",
    "start": "3277560",
    "end": "3283099"
  },
  {
    "text": "So beta t plus 1 is equals\nto beta i t minus eta, beta i",
    "start": "3283100",
    "end": "3291330"
  },
  {
    "text": "t squared minus 1, beta i t.",
    "start": "3291330",
    "end": "3296580"
  },
  {
    "text": "And we argue that this is\ntrying to increase beta i. And we can see how much\nit increases beta i.",
    "start": "3296580",
    "end": "3304450"
  },
  {
    "text": "It increases beta i\nby this factor, eta times 1 minus beta i t square.",
    "start": "3304450",
    "end": "3312829"
  },
  {
    "text": "So you have a\nmultiplicative factor. So in some sense, you're\nmultiplying your beta i",
    "start": "3312830",
    "end": "3317840"
  },
  {
    "text": "to make it bigger. But how much you\ncan make it bigger, it depends on the\nvalue of beta i itself.",
    "start": "3317840",
    "end": "3324540"
  },
  {
    "text": "But if we know that\nbeta i is not too big, then we can bond this\nby beta i t times 1",
    "start": "3324540",
    "end": "3330440"
  },
  {
    "text": "plus eta, 1 minus 1/4,\nwhich is bigger than beta,",
    "start": "3330440",
    "end": "3338420"
  },
  {
    "text": "is equal to beta i t 1\nplus 3/4 times eta, right?",
    "start": "3338420",
    "end": "3346868"
  },
  {
    "text": " So you have exponential growth. ",
    "start": "3346868",
    "end": "3359170"
  },
  {
    "text": "And if beta i is already\nbigger than half, right,",
    "start": "3359170",
    "end": "3367290"
  },
  {
    "text": "then let's see what happens. So now the growth rate\nmight be slow down. Because if you see if beta\ni is kind of close to 1,",
    "start": "3367290",
    "end": "3374520"
  },
  {
    "text": "then this constant\nbecomes close to 0.",
    "start": "3374520",
    "end": "3379710"
  },
  {
    "text": "So your growth rate slows down. And that's true. But what you can do is you can\nanalyze how far you are away",
    "start": "3379710",
    "end": "3388050"
  },
  {
    "text": "from 1, from your target. And if you look at how\nfar you are away from 1,",
    "start": "3388050",
    "end": "3393990"
  },
  {
    "text": "then you get the\nfollowing recursion. ",
    "start": "3393990",
    "end": "3400810"
  },
  {
    "text": "Minus eta times 1 minus\nbeta i t square, beta i t.",
    "start": "3400810",
    "end": "3408920"
  },
  {
    "text": "And so, let's try to\nreorganize this a little bit.",
    "start": "3408920",
    "end": "3415059"
  },
  {
    "text": "So I guess let's say, I\nthink let's assume this.",
    "start": "3415060",
    "end": "3421380"
  },
  {
    "text": "Let's assume this\nis also less than 1. ",
    "start": "3421380",
    "end": "3430390"
  },
  {
    "text": "And then, this is less than-- ",
    "start": "3430390",
    "end": "3437640"
  },
  {
    "text": "sorry. Let me think.  I guess, I don't necessarily\nhave to assume this.",
    "start": "3437640",
    "end": "3444480"
  },
  {
    "text": "Let's remove this for a moment. This is less than 1 minus\nbeta i t minus eta t",
    "start": "3444480",
    "end": "3453250"
  },
  {
    "text": "squared times 1/2, right? Because beta i t\nis bigger than 1/2.",
    "start": "3453250",
    "end": "3459580"
  },
  {
    "text": "And now, you can\nfactorize this to get a factor 1 minus beta i t out.",
    "start": "3459580",
    "end": "3466160"
  },
  {
    "text": "And this 1 minus eta, 1\nplus beta i t times 1/2.",
    "start": "3466160",
    "end": "3472190"
  },
  {
    "start": "3472190",
    "end": "3478329"
  },
  {
    "text": "And I guess it might be a little\nbit you may feel like this",
    "start": "3478330",
    "end": "3486070"
  },
  {
    "text": "is a little bit unnatural. But I think if you\nsee my final target, I guess it's\nprobably actually not",
    "start": "3486070",
    "end": "3491890"
  },
  {
    "text": "super difficult to guess how\nto do the intermediate steps. So now I'm going to use the fact\nthat beta i t is bigger than 0.",
    "start": "3491890",
    "end": "3500119"
  },
  {
    "text": "So get 1 minus beta i t,\n1 minus eta times 1/2.",
    "start": "3500120",
    "end": "3505720"
  },
  {
    "text": " So my point is that if you\nlook at the final outcome,",
    "start": "3505720",
    "end": "3512030"
  },
  {
    "text": "you'll see that now you are\nnot growing exponentially. But you are converging\nto 1 exponentially fast.",
    "start": "3512030",
    "end": "3518420"
  },
  {
    "text": "So you are decreasing\nyour error-- you are decreasing your distance\nto 1 in exponential fast rate.",
    "start": "3518420",
    "end": "3526610"
  },
  {
    "start": "3526610",
    "end": "3531910"
  },
  {
    "text": "So in some sense, this\nbehavior of these dynamics has two regime, right? So when you are small, you\nare growing very, very fast.",
    "start": "3531910",
    "end": "3538380"
  },
  {
    "text": "And then when it becomes\nbigger, then your growth rate slows down, but you are\nconverging to 1 exponentially",
    "start": "3538380",
    "end": "3543940"
  },
  {
    "text": "fast. ",
    "start": "3543940",
    "end": "3549119"
  },
  {
    "text": "So that's why if you combine\nthese two regime then you are-- and also you can see\nthat this maintains",
    "start": "3549120",
    "end": "3560859"
  },
  {
    "text": "beta I is less than 1, right? So because if you-- before\nyou are less than 1 and later",
    "start": "3560860",
    "end": "3565900"
  },
  {
    "text": "you are going to be\nalso less than 1. So basically, the behavior\nis that if you summarize--",
    "start": "3565900",
    "end": "3572470"
  },
  {
    "text": "so basically, in log 1 over\nalpha over eta iteration,",
    "start": "3572470",
    "end": "3583290"
  },
  {
    "text": "you are in the first regime. So this is beta i t grows\nto 1/2 exponentially fast.",
    "start": "3583290",
    "end": "3594357"
  },
  {
    "text": "Exponentially.  And you only need to use\nthis number of iterations",
    "start": "3594357",
    "end": "3600160"
  },
  {
    "text": "because you-- initially it's alpha. And you want to grow to 1/2. So I guess technically you also\nhave a tool here if you want.",
    "start": "3600160",
    "end": "3608050"
  },
  {
    "text": " So and you have a\nlearning with eta.",
    "start": "3608050",
    "end": "3614420"
  },
  {
    "text": "So basically, this\nis because 1 over eta is 1/2 to the power\nof this t1. t1,",
    "start": "3614420",
    "end": "3620690"
  },
  {
    "text": "this is something\nlike at least-- Sorry. This is because\nthis is your growth,",
    "start": "3620690",
    "end": "3628750"
  },
  {
    "text": "the factor of your growth. And you find some power to it. And then you want to grow at\nleast 1/2 over alpha factor.",
    "start": "3628750",
    "end": "3636549"
  },
  {
    "text": "And that's how you\nsolve this number t1. OK, right. But anyway, so this is how--",
    "start": "3636550",
    "end": "3644275"
  },
  {
    "text": "OK, the first part. And then, in log 1 over\nepsilon over eta iteration,",
    "start": "3644275",
    "end": "3656220"
  },
  {
    "text": "beta i t converges\nto 1 minus epsilon.",
    "start": "3656220",
    "end": "3663710"
  },
  {
    "text": "And this is because you want to\nstart from 1/2, the error 1/2 to error epsilon. And each time you decrease\nby 1 minus eta over 2,",
    "start": "3663710",
    "end": "3671380"
  },
  {
    "text": "so that's why you have to\npay this number of iteration.",
    "start": "3671380",
    "end": "3676869"
  },
  {
    "text": "Does this make sense?  I guess there is a small\nthing that I probably",
    "start": "3676870",
    "end": "3683339"
  },
  {
    "text": "escaped in some sense. So this is about how do you\nderive how many iterations",
    "start": "3683340",
    "end": "3688820"
  },
  {
    "text": "you need. So if you want 1 plus\neta to the power t to be bigger than some\nnumber R, then this means t",
    "start": "3688820",
    "end": "3695220"
  },
  {
    "text": "needs to be bigger\nthan log R over eta.",
    "start": "3695220",
    "end": "3707099"
  },
  {
    "text": "So this is just something\nthat was burned into my head. But you can derive\nit yourself as well.",
    "start": "3707100",
    "end": "3716390"
  },
  {
    "text": "OK. Cool. All right. So that's what happens\nwith those coordinates",
    "start": "3716391",
    "end": "3724049"
  },
  {
    "text": "that you want to converge to 1. And you also have case two. ",
    "start": "3724050",
    "end": "3730280"
  },
  {
    "text": "And you can do the same thing. I don't necessarily want to bore\nyou with all the derivations. But I guess the\nderivations here is easy.",
    "start": "3730280",
    "end": "3737320"
  },
  {
    "text": "Because you are just\ntrying to say beta i is decreasing in\nthis feed, right?",
    "start": "3737320",
    "end": "3745369"
  },
  {
    "text": "So and here, let's see.",
    "start": "3745370",
    "end": "3756145"
  },
  {
    "text": " Now here, interestingly, so if\nyou really look at-- literally",
    "start": "3756145",
    "end": "3766880"
  },
  {
    "text": "look at this. This is actually saying\nthat you are decreasing beta i eventually to 0.",
    "start": "3766880",
    "end": "3775190"
  },
  {
    "text": "But somehow we care about\nsomething weaker that is more-- ",
    "start": "3775190",
    "end": "3782150"
  },
  {
    "text": "let me see. ",
    "start": "3782150",
    "end": "3788858"
  },
  {
    "text": "Am I missing a plus here? So I don't know why I'm-- I think I'm not missing a plus.",
    "start": "3788858",
    "end": "3794526"
  },
  {
    "start": "3794526",
    "end": "3803053"
  },
  {
    "text": "Just one moment. Let me make sure I don't\nmake any mistake here. ",
    "start": "3803053",
    "end": "3818240"
  },
  {
    "text": "OK, so I think what I do here,\nI have some derivation here,",
    "start": "3818240",
    "end": "3825770"
  },
  {
    "text": "some small claim here,\nwhich is particularly useful for the\nempirical case, which",
    "start": "3825770",
    "end": "3835700"
  },
  {
    "text": "I'm not sure I can get into. So I'm going to skip this part. So at least for now\nit's not trivial to see",
    "start": "3835700",
    "end": "3840829"
  },
  {
    "text": "that beta i is decreasing. If you start with\nalpha, then you",
    "start": "3840830",
    "end": "3846120"
  },
  {
    "text": "keep being smaller than alpha. That sounds trivial to see. And maybe let's\njust leave it there.",
    "start": "3846120",
    "end": "3852900"
  },
  {
    "text": "This is enough for us to deal\nwith the population case.",
    "start": "3852900",
    "end": "3858039"
  },
  {
    "text": "So basically, our\nconclusion is-- you can see that\nthe conclusion is that you converge\nto something close",
    "start": "3858040",
    "end": "3863980"
  },
  {
    "text": "to 1 in this number\nof iterations. And the iteration count\nis something logarithmic",
    "start": "3863980",
    "end": "3869890"
  },
  {
    "text": "times 1 over eta. And you also have\nthis property that you are always less than 1.",
    "start": "3869890",
    "end": "3876370"
  },
  {
    "text": "All the entries are less than 1. And also, the small\nentries are never growing.",
    "start": "3876370",
    "end": "3884920"
  },
  {
    "text": "So basically, your\nbeta t at any time",
    "start": "3884920",
    "end": "3891630"
  },
  {
    "text": "basically looks like\nthere are a bunch of entries which is growing. So the s and also in the s\ncomplement, in the s complement",
    "start": "3891630",
    "end": "3900809"
  },
  {
    "text": "all of these entries is\nless than alpha forever. And in the s-coordinates,\nyou are growing potentially.",
    "start": "3900810",
    "end": "3909340"
  },
  {
    "text": "So you can see\nthat this is still always approximately R sparse. Because at least you only\nhave our big non-zero entries,",
    "start": "3909340",
    "end": "3917079"
  },
  {
    "text": "and all the other\nentries are very small. So approximately, this is still\nalways approximately in the Xr,",
    "start": "3917080",
    "end": "3924663"
  },
  {
    "text": "just because the small\nentries are keep being small. ",
    "start": "3924663",
    "end": "3938240"
  },
  {
    "text": "So now, let's try to talk\nabout the empirical case a little bit.",
    "start": "3938240",
    "end": "3944882"
  },
  {
    "text": "I think this full\nanalysis probably wouldn't fit within\n15 minutes but I think I can give you\nsome idea about it.",
    "start": "3944882",
    "end": "3951170"
  },
  {
    "start": "3951170",
    "end": "3960030"
  },
  {
    "text": "And actually, I'm going to\nonly do the case when R is 1. Because when R is\nmore than 1, it's",
    "start": "3960030",
    "end": "3965970"
  },
  {
    "text": "kind of a little\nbit complicated. So I'm only going to\ndo the R is 1 case.",
    "start": "3965970",
    "end": "3974030"
  },
  {
    "text": "So get that for some\ndelta, and when R is 1,",
    "start": "3974030",
    "end": "3986680"
  },
  {
    "text": "and it's less than omega of 1. So basically, you only have to\nhave logarithm and of examples.",
    "start": "3986680",
    "end": "3993220"
  },
  {
    "text": "And then, gd on beta hat-- ",
    "start": "3993220",
    "end": "3998800"
  },
  {
    "text": "this is just a\nsimplification of the theorem we have already stated. I guess now this is\nalso actually weaker.",
    "start": "3998800",
    "end": "4007065"
  },
  {
    "text": " Maybe I should say it's\na weakness theorem.",
    "start": "4007065",
    "end": "4015340"
  },
  {
    "text": "Not only weaker in the\nsense of simplification, but also weaker. So weaker and simplified. ",
    "start": "4015340",
    "end": "4024880"
  },
  {
    "text": "So you get this iteration steps.",
    "start": "4024880",
    "end": "4031119"
  },
  {
    "text": "We have-- it's less than O\nto the form square root of 2.",
    "start": "4031120",
    "end": "4049460"
  },
  {
    "text": "So here, I guess\nwhy this is weaker than what we have said before. Before, the error can\ngoes to 0 as long as you",
    "start": "4049460",
    "end": "4056050"
  },
  {
    "text": "take alpha to be small enough. So this is weaker because\nerror doesn't go to 0.",
    "start": "4056050",
    "end": "4066315"
  },
  {
    "text": " So before we can make the error\ngoes to 0 as alpha goes to 0.",
    "start": "4066315",
    "end": "4075430"
  },
  {
    "text": "And now you only prove that\nit depends on something like the number of examples.",
    "start": "4075430",
    "end": "4080600"
  },
  {
    "text": "This is just a technicality. If you want to prove the case\nwhen the error goes to 0, you have to do extra\nwork, which is probably",
    "start": "4080600",
    "end": "4087513"
  },
  {
    "text": "too much for this course.  So how do we do this?",
    "start": "4087513",
    "end": "4094430"
  },
  {
    "text": "So in some sense, the\nproof is trying to--",
    "start": "4094430",
    "end": "4101620"
  },
  {
    "text": "in some sense maybe\nlet me-- the proof idea in some sense is\npretty intuitive.",
    "start": "4101620",
    "end": "4108068"
  },
  {
    "text": "Given that this figure\nwe have to draw. So you are just trying\nto show that L hat",
    "start": "4108069",
    "end": "4114939"
  },
  {
    "text": "beta is close to L beta. And that's something you\ncan prove very easily. So we try to prove that--",
    "start": "4114939",
    "end": "4123250"
  },
  {
    "text": "so one you, try to\nprove that L hat beta is close to L beta\nfor every beta that is--",
    "start": "4123250",
    "end": "4132500"
  },
  {
    "text": "I guess, technically,\nI have to say this is approximately sparse. ",
    "start": "4132500",
    "end": "4138330"
  },
  {
    "text": "Because you can never be\nexactly sparse, as we discussed. So that's something\nthat is relatively easy.",
    "start": "4138330",
    "end": "4144470"
  },
  {
    "text": "And the second thing\nis that you want to say that the\nbeta t are under--",
    "start": "4144470",
    "end": "4151630"
  },
  {
    "text": "for the empirical case,\nthe empirical trajectory,",
    "start": "4151630",
    "end": "4159549"
  },
  {
    "text": "in the empirical trajectory\nnever leaves this Xr very far--",
    "start": "4159550",
    "end": "4175829"
  },
  {
    "text": "never leave it significantly. ",
    "start": "4175830",
    "end": "4183509"
  },
  {
    "text": "So how do you show two? Basically, you are\ntrying to say that you are staying close to\nthe-- so you basically",
    "start": "4183510",
    "end": "4189500"
  },
  {
    "text": "want an error to not blow up.",
    "start": "4189500",
    "end": "4196280"
  },
  {
    "text": "So what does that mean? So it means that maybe\nlet's draw something here. So you are trying to show\ntwo trajectories that",
    "start": "4196280",
    "end": "4202690"
  },
  {
    "text": "are close to each\nother just forever. So you have a trajectory,\nwhich is the purple one. This is the green descent.",
    "start": "4202690",
    "end": "4209330"
  },
  {
    "text": "And what happens is, after\nyou take the first step you have some error here, right?",
    "start": "4209330",
    "end": "4215510"
  },
  {
    "text": "So now, these two\ntrajectories are not doing the same thing anymore. Initially, you are taking the\ngradient at the same point.",
    "start": "4215510",
    "end": "4222380"
  },
  {
    "text": "And now this purple one is\ntaking gradient at this point. And the black one is taking\na gradient at this point.",
    "start": "4222380",
    "end": "4229020"
  },
  {
    "text": "So you have this error in\nnot in terms of the gradient are different but also in\nterms of the a difference",
    "start": "4229020",
    "end": "4236000"
  },
  {
    "text": "of the points where you are\nevaluating your gradient. So it's empirical\nversus population, that's one difference.",
    "start": "4236000",
    "end": "4241530"
  },
  {
    "text": "And the other thing\nis that you are evaluating the\nempirical and population gradient at different places.",
    "start": "4241530",
    "end": "4247940"
  },
  {
    "text": "And that could introduce\na bigger error. And then it will introduce\neven bigger error.",
    "start": "4247940",
    "end": "4253630"
  },
  {
    "text": "So if you don't\ndo this carefully, then it's possible that\neventually you go this way and the other one goes\nthe other way, just",
    "start": "4253630",
    "end": "4260150"
  },
  {
    "text": "because the error\nkeep blowing up, keep being bigger and bigger. So basically, you have to\ncontrol how the error control--",
    "start": "4260150",
    "end": "4269180"
  },
  {
    "text": "how the error changes. ",
    "start": "4269180",
    "end": "4274659"
  },
  {
    "text": "So that's the key part. And this boils down\nto a lot of at least",
    "start": "4274660",
    "end": "4281470"
  },
  {
    "text": "on the surface seemingly\nvery boring calculations. If you really want to do\nall this calculation well,",
    "start": "4281470",
    "end": "4286870"
  },
  {
    "text": "you have to understand a little\nbit about what each term means. And it does require\nsome extra work.",
    "start": "4286870",
    "end": "4296140"
  },
  {
    "text": "But the first level thing is\nthat at least this whole thing",
    "start": "4296140",
    "end": "4303050"
  },
  {
    "text": "is a simplification\nof one of the paper I wrote a few years back. And when we did this thing,\nthe first thing we tried",
    "start": "4303050",
    "end": "4311020"
  },
  {
    "text": "is that we just try\nto do the calculation. So and you try to understand\nwhich term is problematic,",
    "start": "4311020",
    "end": "4316310"
  },
  {
    "text": "which term may cause\na bigger blow-up, and then you focus\nmore on that term, and then try to understand\na little bit better,",
    "start": "4316310",
    "end": "4322360"
  },
  {
    "text": "and then maybe use some-- devise some inequalities. But basically, below this level\nit becomes quite technical.",
    "start": "4322360",
    "end": "4330730"
  },
  {
    "text": " So I think I'm going to probably\nspend another five minutes",
    "start": "4330730",
    "end": "4336850"
  },
  {
    "text": "to do a little bit a thing. So I think the thing\nfor you to control",
    "start": "4336850",
    "end": "4343810"
  },
  {
    "text": "this error is that one\nthing we realize is useful is that actually this is\nactually important conceptual--",
    "start": "4343810",
    "end": "4351850"
  },
  {
    "text": "in a semi-conceptual\nkind of thing. That we realize,\nso to control how",
    "start": "4351850",
    "end": "4357820"
  },
  {
    "text": "this error is good to\nrepresent your iterate in a convenient way.",
    "start": "4357820",
    "end": "4363650"
  },
  {
    "text": "So what does that mean? So it means that the beta star,\nwe will assume R is 1 already.",
    "start": "4363650",
    "end": "4370000"
  },
  {
    "text": "So Let's assume\nbeta star is just e1 So it's just 1, 0, 0, 0, right?",
    "start": "4370000",
    "end": "4378070"
  },
  {
    "text": "So you just want to say that\nit converges to this vector. And one of the\nuseful thing we did",
    "start": "4378070",
    "end": "4384550"
  },
  {
    "text": "is that we take beta t to be-- we write beta t to be rt times\ne1 plus an arrow vector zeta t.",
    "start": "4384550",
    "end": "4392310"
  },
  {
    "text": "So explicitly, you write\nit is a multiplication of beta star and some error.",
    "start": "4392310",
    "end": "4399470"
  },
  {
    "text": "So in some sense,\nbeta star is here. And you are starting from 0. And you try to say how much you\nare different from this line.",
    "start": "4399470",
    "end": "4409929"
  },
  {
    "text": "So this is our zeta t. And this is the rt times e1. That's how you represent\nwhere you are at time t.",
    "start": "4409930",
    "end": "4419010"
  },
  {
    "text": "And what we did is that\nwe want to say that rt, the plan is that you want\nto say rt is going to 1.",
    "start": "4419010",
    "end": "4428070"
  },
  {
    "text": "Because eventually\nyou want to go to e1. And zeta t, the error\nterm, is always small.",
    "start": "4428070",
    "end": "4434895"
  },
  {
    "text": " I think we prove\nit to be smaller than O of alpha for any t.",
    "start": "4434895",
    "end": "4443695"
  },
  {
    "text": " That's the next level, right?",
    "start": "4443695",
    "end": "4451120"
  },
  {
    "text": "So and then, you basically\nwhat you have to do, you have to try to derive a\nrecursion for rt and zeta t.",
    "start": "4451120",
    "end": "4467180"
  },
  {
    "text": "And when we remove the recursion\nfor both of these two things you can always keep in\nmind that what happens",
    "start": "4467180",
    "end": "4472250"
  },
  {
    "text": "with the population, right?  So the precursor for rt, and\nyou can have the same recursion",
    "start": "4472250",
    "end": "4479360"
  },
  {
    "text": "for the population case. So basically you're\ngoing to have-- so suppose there's, for\nexample, talk about--",
    "start": "4479360",
    "end": "4486699"
  },
  {
    "text": "let me see which one I\ncan talk about easily. ",
    "start": "4486700",
    "end": "4492080"
  },
  {
    "text": "Let's see. ",
    "start": "4492080",
    "end": "4501410"
  },
  {
    "text": "How do I quickly\nsimplify these notes? I think I had some backup plan. Yes, here.",
    "start": "4501410",
    "end": "4506539"
  },
  {
    "start": "4506540",
    "end": "4514730"
  },
  {
    "text": "So basically, for\nexample, if you look at the recursion\nfor rt, it looks",
    "start": "4514730",
    "end": "4519770"
  },
  {
    "text": "like rt plus rt is equal to rt\nminus eta rt squared minus 1,",
    "start": "4519770",
    "end": "4526610"
  },
  {
    "text": "times t, minus some term\nthat depends on zeta t.",
    "start": "4526610",
    "end": "4534420"
  },
  {
    "text": "So if you do-- I have all of these\nformulas written here. But I don't want to\nshow all the details.",
    "start": "4534420",
    "end": "4539700"
  },
  {
    "text": "So and if you look at this, this\nis very similar to the thing",
    "start": "4539700",
    "end": "4544750"
  },
  {
    "text": "that we had before. I guess, I don't know why I'm-- let me also change\nthe superscript.",
    "start": "4544750",
    "end": "4554580"
  },
  {
    "text": "I think I should probably have-- yeah. In my notes it's\nalso superscript.",
    "start": "4554580",
    "end": "4559895"
  },
  {
    "text": "OK. ",
    "start": "4559895",
    "end": "4573320"
  },
  {
    "text": "I guess let me not\nchange everything. Just you know the superscript\nis the same as the here.",
    "start": "4573320",
    "end": "4579420"
  },
  {
    "text": "So if you look at\nthis one, this part, this is the same as the\nupdate for the beta we had.",
    "start": "4579420",
    "end": "4587360"
  },
  {
    "text": "So where is the update for beta? ",
    "start": "4587360",
    "end": "4594120"
  },
  {
    "text": "Here, right?  So this is the case when you\nare looking at a coordinate",
    "start": "4594120",
    "end": "4601590"
  },
  {
    "text": "where you have a\nNG1 in a beta star. So and this is the update.",
    "start": "4601590",
    "end": "4606989"
  },
  {
    "text": "So and if you just\nreplace beta i to the rt, you get the same formula.",
    "start": "4606990",
    "end": "4612540"
  },
  {
    "text": "So rt has the same formula. So basically, this part is what\nthe population gradient does.",
    "start": "4612540",
    "end": "4619940"
  },
  {
    "text": " And you already analyzed\nthis part already.",
    "start": "4619940",
    "end": "4626850"
  },
  {
    "text": "So basically, the only\nthing you have to deal with is, how does the\nerror term affect you.",
    "start": "4626850",
    "end": "4634820"
  },
  {
    "text": "And you inductively\nshow the error is small. So under the assumption\nthat error is small,",
    "start": "4634820",
    "end": "4639850"
  },
  {
    "text": "then you can show\nthat the update for rt is basically doing the\nsame thing as the update",
    "start": "4639850",
    "end": "4645390"
  },
  {
    "text": "for the beta t before. So that's how you deal with rt.",
    "start": "4645390",
    "end": "4650520"
  },
  {
    "text": "But how do we know\nthe zeta t is small? That becomes even\nmore complicated. Because zeta t also\nhas a derivative--",
    "start": "4650520",
    "end": "4656940"
  },
  {
    "text": "has a recursion, right? So zeta t has a\nrecursion, which is-- I think I don't even see\na simple way to write it.",
    "start": "4656940",
    "end": "4664680"
  },
  {
    "text": "Actually, I have\nsomething like this. So zeta t is equal to\nzeta t minus something",
    "start": "4664680",
    "end": "4673610"
  },
  {
    "text": "like some matrix mt times\nzeta t, some vector, sorry.",
    "start": "4673610",
    "end": "4681532"
  },
  {
    "text": " Some vector low t times\nzeta t, something like this.",
    "start": "4681533",
    "end": "4689420"
  },
  {
    "text": "I'm not going to define low t. And what you do is\nthat this is somewhat",
    "start": "4689420",
    "end": "4697780"
  },
  {
    "text": "similar to the case\nto those i not in s,",
    "start": "4697780",
    "end": "4710309"
  },
  {
    "text": "the beta i t recursion.",
    "start": "4710310",
    "end": "4715590"
  },
  {
    "text": "So it's not-- so what's\nthe beta i t recursion? The beta i t recursion\nwas something like beta i t plus 1 is equals\nto beta i t and it's eta beta i",
    "start": "4715590",
    "end": "4725182"
  },
  {
    "text": "t cubed. So I think this, if you\nreally look at the derivation,",
    "start": "4725182",
    "end": "4731060"
  },
  {
    "text": "I think this is something\nlike beta i t squared minus 0 times beta i t.",
    "start": "4731060",
    "end": "4738960"
  },
  {
    "text": "So if you really do\nthe match the terms,",
    "start": "4738960",
    "end": "4745130"
  },
  {
    "text": "I think these two matches\nand these two matches. And something here,\nwhich also matches if you look at the details,\nto some extent, not exactly.",
    "start": "4745130",
    "end": "4752630"
  },
  {
    "text": "But you somehow have\na-- there's no way to match everything exactly.",
    "start": "4752630",
    "end": "4757760"
  },
  {
    "text": "But you just use this\nas-- use the beta update as a reference for you. And you know that\nsomething already matches.",
    "start": "4757760",
    "end": "4764120"
  },
  {
    "text": "And what doesn't\nmatch is this rho t, which I didn't define\nto this beta t square.",
    "start": "4764120",
    "end": "4770480"
  },
  {
    "text": "And you do some kind\nof concentration to show that they are similar. And what exactly\nconcentration you show,",
    "start": "4770480",
    "end": "4776780"
  },
  {
    "text": "it's also actually up\nto the exact terms. And somehow, sometimes you\nrelate the zeta t recursion",
    "start": "4776780",
    "end": "4783500"
  },
  {
    "text": "to the beta t recursion\nunder the hood so that you can show that\nt doesn't grow eventually.",
    "start": "4783500",
    "end": "4791690"
  },
  {
    "text": "Because you knew\nbeta t doesn't grow. That's what we proved easily. And once you can relate\nzeta t to beta t,",
    "start": "4791690",
    "end": "4799130"
  },
  {
    "text": "then you can also try to show\nzeta t doesn't grow eventually. ",
    "start": "4799130",
    "end": "4806420"
  },
  {
    "text": "I think that's pretty\nmuch the best thing I can",
    "start": "4806420",
    "end": "4811640"
  },
  {
    "text": "do in a short amount of time. ",
    "start": "4811640",
    "end": "4816679"
  },
  {
    "text": "And the details\nare in the notes.  Any questions?",
    "start": "4816680",
    "end": "4823283"
  },
  {
    "text": "The [INAUDIBLE]? ",
    "start": "4823284",
    "end": "4828594"
  },
  {
    "text": "Sorry. Yeah. Yes. I think it was rt\nplus 1 when I changed the superscript to a subscript.",
    "start": "4828595",
    "end": "4834360"
  },
  {
    "text": "I forgot. Yeah. Thanks. ",
    "start": "4834360",
    "end": "4842665"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4842665",
    "end": "4864610"
  },
  {
    "text": "So the question is that-- [INAUDIBLE] So I guess the question is\nthat at least in this lecture,",
    "start": "4864610",
    "end": "4870760"
  },
  {
    "text": "in last lecture, we\nsaw two examples where gradient designs converges\nto a solution that is closest",
    "start": "4870760",
    "end": "4877780"
  },
  {
    "text": "to the-- in tradition. And but why\nempirically you still have to use the explicit\nregularization of weight decay.",
    "start": "4877780",
    "end": "4884600"
  },
  {
    "text": "So I think I would like to argue\nthat empirically, the weight",
    "start": "4884600",
    "end": "4892810"
  },
  {
    "text": "decay is actually\nnot very strong.",
    "start": "4892810",
    "end": "4898240"
  },
  {
    "text": "So it's not even clear whether\nthe weight decay is really doing a regularization.",
    "start": "4898240",
    "end": "4903780"
  },
  {
    "text": "Because with the\nsame weight decay, actually you can memorize\nthe training data.",
    "start": "4903780",
    "end": "4909750"
  },
  {
    "text": "You can even memorize-- sorry. You can even memorize training\ndata with random labels.",
    "start": "4909750",
    "end": "4915219"
  },
  {
    "text": "So suppose you permute\nyour label arbitrarily. So that there is no pattern.",
    "start": "4915220",
    "end": "4921320"
  },
  {
    "text": "This is random label. You can still use\nthe same weight decay and train your network\nwith the same weight decay",
    "start": "4921320",
    "end": "4927760"
  },
  {
    "text": "to find a zero error solution. So that seems like that's the\nweight decay is not really doing that much of a\nregularization, at least",
    "start": "4927760",
    "end": "4934870"
  },
  {
    "text": "not as strong as the\ntheoretical setting will say. Well, for example, in\nthis case, suppose--",
    "start": "4934870",
    "end": "4940719"
  },
  {
    "text": "or in this case or\nthe previous case, we're supposed to\nuse weight decay. You usually find the\nminimum norm solution.",
    "start": "4940720",
    "end": "4946300"
  },
  {
    "text": "Then use a strong\nregularizer to say you want to find a\nsolution with small norm.",
    "start": "4946300",
    "end": "4952120"
  },
  {
    "text": "Then you cannot fit\nrandom labels anymore. So and also, another\nkind of tricky thing",
    "start": "4952120",
    "end": "4959650"
  },
  {
    "text": "is that the weight\ndecay in practice also has some other effect that\nregulates, for example, how",
    "start": "4959650",
    "end": "4969660"
  },
  {
    "text": "the batch normalization\nis working. And for example, I think if\nyou have rationalization,",
    "start": "4969660",
    "end": "4977810"
  },
  {
    "text": "then the model becomes\nscale environment. So it becomes if you multiply\nall the weights by 2,",
    "start": "4977810",
    "end": "4988500"
  },
  {
    "text": "technically you don't\nchange anything. But somehow you want\nto regularize that. You want to kind of\nregularize that in some way.",
    "start": "4988500",
    "end": "4995340"
  },
  {
    "text": "Because in certain cases,\nit changes the optimization. So basically, I guess I\ndon't have a very concrete--",
    "start": "4995340",
    "end": "5002120"
  },
  {
    "text": "this is a good question. I don't have a very\nconcrete answer. But I think the thing we believe\nis that weight decay is not",
    "start": "5002120",
    "end": "5008690"
  },
  {
    "text": "actually doing a\nstrong work in terms of the standard\nnormalization of a norm, like regularization of a norm.",
    "start": "5008690",
    "end": "5014930"
  },
  {
    "text": "And also, we somewhat\nsuspect weight decay has some other effect\nto some extent. And also, sometimes the weight\ndecay is not even important.",
    "start": "5014930",
    "end": "5022130"
  },
  {
    "text": "So if you remove\nthe weight decay, you still get pretty good\nresults in certain cases.",
    "start": "5022130",
    "end": "5028230"
  },
  {
    "text": "So I guess that's the best\nwe know for now. yeah. ",
    "start": "5028230",
    "end": "5035760"
  },
  {
    "text": "Any other questions? ",
    "start": "5035760",
    "end": "5044390"
  },
  {
    "text": "OK, sounds good. I guess I will see\nyou on Wednesday. ",
    "start": "5044390",
    "end": "5053000"
  }
]