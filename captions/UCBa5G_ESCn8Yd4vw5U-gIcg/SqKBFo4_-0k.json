[
  {
    "start": "0",
    "end": "10010"
  },
  {
    "text": "So deep dive into one project\nthat I led specifically,",
    "start": "10010",
    "end": "15260"
  },
  {
    "text": "but then also zoom\nout and give you a little bit of an overview\nof what robotics at Fair is doing in general\nfrom my perspective.",
    "start": "15260",
    "end": "22770"
  },
  {
    "text": "So I have a very\nparticular perspective, which is more focused on\ngeneralization and adaptation.",
    "start": "22770",
    "end": "29870"
  },
  {
    "text": "And other people would\ntell you other stories. So you get my story today.",
    "start": "29870",
    "end": "35030"
  },
  {
    "text": "All right. Just to ground the\ndiscussion, I always like to start with an overview\nof these kind of tasks,",
    "start": "35030",
    "end": "43500"
  },
  {
    "text": "which we humans can do, all\nof them with some success.",
    "start": "43500",
    "end": "49710"
  },
  {
    "text": "Some of them we have\nto learn, but we are very capable doing\na diverse set of skills",
    "start": "49710",
    "end": "55760"
  },
  {
    "text": "with infinitely many variations. And here a variation I\nmean, different tones, different objects.",
    "start": "55760",
    "end": "64930"
  },
  {
    "text": "It's just the amount of\ngeneralization capabilities we have is staggering whenever\nI look at a robot trying",
    "start": "64930",
    "end": "71890"
  },
  {
    "text": "to get the same.  And the question then I\npose often is, well, how can",
    "start": "71890",
    "end": "80350"
  },
  {
    "text": "we enable learning of\nthese diverse set of skills in everyone's homes?",
    "start": "80350",
    "end": "85450"
  },
  {
    "text": "How do we get there? Now, in the past, I have\nbeen much more focused",
    "start": "85450",
    "end": "93909"
  },
  {
    "text": "on the adaptation parts,\nbut since joining Meta, I've started to\nlook more at, well,",
    "start": "93910",
    "end": "99260"
  },
  {
    "text": "let's first train models that\ngeneralize as much as they can. And then let's figure\nout how to adapt",
    "start": "99260",
    "end": "106030"
  },
  {
    "text": "them efficiently and\nautonomously once you've deployed a robot. Fundamentally, I believe\nwe cannot prepare a robot",
    "start": "106030",
    "end": "113740"
  },
  {
    "text": "for everything that will ever\nhappen in the real world. But we should try to give them\nas much knowledge as we can",
    "start": "113740",
    "end": "120170"
  },
  {
    "text": "before then trying to\ncontinuously adapt and make them fully autonomous from scratch.",
    "start": "120170",
    "end": "126390"
  },
  {
    "text": "This may be really\nhard in a new home. So today, I will focus on\nthat generalization part.",
    "start": "126390",
    "end": "132750"
  },
  {
    "text": "There will be a\nbit of adaptation. ",
    "start": "132750",
    "end": "138470"
  },
  {
    "text": "What does generalization mean? Well, generalization comes in\nvery many forms in robotics,",
    "start": "138470",
    "end": "145320"
  },
  {
    "text": "and I think that's\nwhat makes it so hard. We have different home\nenvironments-- or not",
    "start": "145320",
    "end": "150380"
  },
  {
    "text": "even homes-- any\nkind of environment. We have different\ntasks and skills we want the robot to learn.",
    "start": "150380",
    "end": "155790"
  },
  {
    "text": "There are different\nembodiments and objects, but then there is also, for\nexample, what I didn't put down",
    "start": "155790",
    "end": "162410"
  },
  {
    "text": "here is we want them to\neventually collaborate with humans. And humans are very different. They come in various\nembodiments and so on.",
    "start": "162410",
    "end": "169950"
  },
  {
    "text": "So the axis of generalization\nin robotics that we have to worry about is--",
    "start": "169950",
    "end": "177400"
  },
  {
    "text": "yeah, it's large. It's much larger than any other\nproblem area that I'm aware of.",
    "start": "177400",
    "end": "184870"
  },
  {
    "text": "So before I dive into\nwhat we at Meta are doing about this problem\nof getting generalization,",
    "start": "184870",
    "end": "191480"
  },
  {
    "text": "I actually wanted\nto get a quick sense of what you guys think\nis necessary to achieve",
    "start": "191480",
    "end": "196720"
  },
  {
    "text": "this generalization. How do we get robots\ninto our homes and be able to perform\nany type of task?",
    "start": "196720",
    "end": "202400"
  },
  {
    "text": "Do you think-- and I will\ncall out all of them. But before I call them out, do\nyou think we need more robot",
    "start": "202400",
    "end": "210280"
  },
  {
    "text": "data, do you think we\nneed better architectures, or do you need\nbetter algorithms? And I want you to pick one.",
    "start": "210280",
    "end": "216410"
  },
  {
    "text": "I don't want the\nanswer, all of them. So tell me, if you are thinking,\nthe most important part",
    "start": "216410",
    "end": "224890"
  },
  {
    "text": "is more robot data,\nplease raise your hand. Interesting.",
    "start": "224890",
    "end": "230050"
  },
  {
    "text": "OK. That's a large part. If you think it's\nbetter architectures,",
    "start": "230050",
    "end": "235227"
  },
  {
    "text": "please raise your hand.  OK, interesting.",
    "start": "235227",
    "end": "240599"
  },
  {
    "text": "I hope some people are\nnot double dipping here. [LAUGHTER] If you need better algorithms,\nplease raise your hand.",
    "start": "240600",
    "end": "249920"
  },
  {
    "text": "OK. I'm going to-- it's\ninteresting, the data access",
    "start": "249920",
    "end": "255950"
  },
  {
    "text": "is what people seem\nto generally believe is the most important\naccess to push on right now.",
    "start": "255950",
    "end": "263760"
  },
  {
    "text": "Maybe it's a\nconvergence of time. It's the right time right\nnow to try to get more data,",
    "start": "263760",
    "end": "270900"
  },
  {
    "text": "or because we see what\nlarge-scale data can actually achieve in other fields. ",
    "start": "270900",
    "end": "278060"
  },
  {
    "text": "I personally think-- obviously\nI didn't make myself pick-- I think it's a\ncombination of all three,",
    "start": "278060",
    "end": "284610"
  },
  {
    "text": "but I made you pick\nbecause I wanted to see.  At Meta, we have the\nresources to collect our data.",
    "start": "284610",
    "end": "293780"
  },
  {
    "text": "And we have a ton of compute. So I think it's\nprobably not a surprise if I say that we are pushing\nvery heavy on the data, data,",
    "start": "293780",
    "end": "300610"
  },
  {
    "text": "data front. I classified this\ninto three categories.",
    "start": "300610",
    "end": "307000"
  },
  {
    "text": "The first one, which\nis passive data, you can think about image\ndata sets and video--",
    "start": "307000",
    "end": "314210"
  },
  {
    "text": "most importantly, video\ndata sets out there that we can use to train skills\nor model parts for robots.",
    "start": "314210",
    "end": "323330"
  },
  {
    "text": "Then there is, of course,\nthe large-scale reinforcement learning or behavior\ncloning in simulation",
    "start": "323330",
    "end": "329000"
  },
  {
    "text": "and then doing some\n[INAUDIBLE] transfer. And then there is\nthe final axis that I",
    "start": "329000",
    "end": "335080"
  },
  {
    "text": "will touch on today, which is\nthe teleoperation data, which is becoming very big now as well.",
    "start": "335080",
    "end": "344110"
  },
  {
    "text": "My work has been primarily\nin the first axis, and that's where I\nwill do the deep dive",
    "start": "344110",
    "end": "349420"
  },
  {
    "text": "and then I will zoom out and\ntalk about other projects at Meta, at Fair in robotics\nspecifically, that touch",
    "start": "349420",
    "end": "358350"
  },
  {
    "text": "on these other fronts. What does it mean to\nutilize passive data",
    "start": "358350",
    "end": "365310"
  },
  {
    "text": "to train diverse robot skills? Well, the first\nstep we took at this was actually to use\nself-supervised pre-training",
    "start": "365310",
    "end": "374160"
  },
  {
    "text": "of visual representations\non out-of-domain video data,",
    "start": "374160",
    "end": "380170"
  },
  {
    "text": "and use that as a strong\nprior before trying to train a policy\non top of that, and then fine-tune, if needed,\nthe visual representation",
    "start": "380170",
    "end": "387570"
  },
  {
    "text": "as well for a diverse\nset of downstream tasks. ",
    "start": "387570",
    "end": "393300"
  },
  {
    "text": "So this looks like that. Basically, you have first a\ntask-agnostic pre-training stage",
    "start": "393300",
    "end": "402180"
  },
  {
    "text": "where you have some generic\nself-supervised signal that helps you train,\nlet's say in this case",
    "start": "402180",
    "end": "408970"
  },
  {
    "text": "a visual perception model. And then you have a\ntask-specific adaptation phase.",
    "start": "408970",
    "end": "414350"
  },
  {
    "text": "We have seen this in computer\nvision and classification and so on. In our case, it was\ncontrol basically.",
    "start": "414350",
    "end": "421360"
  },
  {
    "text": "This project was\nstarted in simulation and then we did do evaluations\non hardware as well.",
    "start": "421360",
    "end": "428350"
  },
  {
    "text": "So we called this the\nartificial visual cortex. When we started, we had\nsome fun naming this.",
    "start": "428350",
    "end": "436900"
  },
  {
    "text": "Before we started\nwith this project, we actually wanted\nto understand,",
    "start": "436900",
    "end": "442330"
  },
  {
    "text": "do we already have such a\ngeneral visual perception",
    "start": "442330",
    "end": "447669"
  },
  {
    "text": "module out there. At the time we started\nthis, other people had started looking\nat this already.",
    "start": "447670",
    "end": "452840"
  },
  {
    "text": "And so we created a benchmark,\nwhich we called cortex bench.",
    "start": "452840",
    "end": "458380"
  },
  {
    "text": "And it consists of seven\nbenchmarks, 17 tasks within those seven benchmarks.",
    "start": "458380",
    "end": "464870"
  },
  {
    "text": "And we are tracking two metrics. I will go more into this later. And the benchmark tasks are\ncovering different observation",
    "start": "464870",
    "end": "473940"
  },
  {
    "text": "spaces, different action\nspaces, different ways of specifying the goal.",
    "start": "473940",
    "end": "479830"
  },
  {
    "text": "And both policy\nlearning was done either with imitation learning\nor reinforcement learning.",
    "start": "479830",
    "end": "484970"
  },
  {
    "text": " And then you can\nalso see there's some variety of the simulators\nthat we actually tested.",
    "start": "484970",
    "end": "492520"
  },
  {
    "text": "So there's some three\nhabitat simulators and three",
    "start": "492520",
    "end": "498120"
  },
  {
    "text": "for MuJoCo environments. ",
    "start": "498120",
    "end": "505560"
  },
  {
    "text": "We used some of the models that\nwere there at the time like MDP",
    "start": "505560",
    "end": "512279"
  },
  {
    "text": "with CLIP and R3M,\nand we evaluated them on all of these tasks.",
    "start": "512280",
    "end": "518620"
  },
  {
    "text": "And what we found is that\nfor one, if you use one--",
    "start": "518620",
    "end": "529670"
  },
  {
    "text": "if you use MVP, it is\nbest on three tasks. If you use R3M, it's best\non three different tasks.",
    "start": "529670",
    "end": "537170"
  },
  {
    "text": "And then CLIP was best\non one specific task. So there was not one model that\nwas outperforming all of them.",
    "start": "537170",
    "end": "546629"
  },
  {
    "text": "And it is kind of\nexpected just there is-- the simulation environments\nare very diverse",
    "start": "546630",
    "end": "553589"
  },
  {
    "text": "and the tasks are\nalso very diverse. And these representations\nweren't pre-trained",
    "start": "553590",
    "end": "559850"
  },
  {
    "text": "with these tasks in\nmind, and neither did we, but we did cover much more\ndiverse pre-training data, which",
    "start": "559850",
    "end": "567860"
  },
  {
    "text": "I think helped at the end. But let me get into how we\ntrained our visual cortex model.",
    "start": "567860",
    "end": "575839"
  },
  {
    "text": "I will go over\npre-training data sets, very quickly touch on\npre-training objective",
    "start": "575840",
    "end": "583230"
  },
  {
    "text": "and the model architecture. But those are the choices\nthat we explored and studied.",
    "start": "583230",
    "end": "593190"
  },
  {
    "text": "So we used Ego4D\nas a base data set.",
    "start": "593190",
    "end": "598450"
  },
  {
    "text": "So any VC-1 variant we\ntrained had always Ego4D. And then we tested what happens\nif we add extra manipulation",
    "start": "598450",
    "end": "607230"
  },
  {
    "text": "video data sets like\nSomething-Something, EPIC-KITCHENS, 100 Day of Hands.",
    "start": "607230",
    "end": "614160"
  },
  {
    "text": "And that allowed us\nto study how much does it help if we add these\nmanipulation-focused data sets",
    "start": "614160",
    "end": "621180"
  },
  {
    "text": "on these benchmarks. We also studied\nwhat happens if we add purely navigation data\nsets like RealEstate10K",
    "start": "621180",
    "end": "629370"
  },
  {
    "text": "and OpenHouse. I don't know if you've seen\nthese type of data sets, but they walk through\nhouses and so on.",
    "start": "629370",
    "end": "635850"
  },
  {
    "text": "And then we also thought,\nwell, let's study what happens if we add something\nlike ImageNet, which doesn't",
    "start": "635850",
    "end": "641149"
  },
  {
    "text": "have necessarily anything in\nparticular to do with the tasks that we care about,\nbut it's more data.",
    "start": "641150",
    "end": "647790"
  },
  {
    "text": "So let's see.  We used ViT architecture\nand self-masked autoencoding",
    "start": "647790",
    "end": "660170"
  },
  {
    "text": "as a pre-training objective. And we studied what\nhappens if we also increase",
    "start": "660170",
    "end": "668540"
  },
  {
    "text": "the size of the encoder. So we use the ViT-Base\nand then ViT-Large. ",
    "start": "668540",
    "end": "677450"
  },
  {
    "text": "So here's the first takeaway. First of all,\nscaling model sizes",
    "start": "677450",
    "end": "683840"
  },
  {
    "text": "maybe unsurprisingly, but\nthat is the worth studying.",
    "start": "683840",
    "end": "690320"
  },
  {
    "text": "The larger model outperformed\nthe smaller models.",
    "start": "690320",
    "end": "696000"
  },
  {
    "text": "And these numbers that\nyou're seeing, these are-- maybe I forgot about that part.",
    "start": "696000",
    "end": "702750"
  },
  {
    "text": "Basically, we pre-train\nthe visual representation",
    "start": "702750",
    "end": "708180"
  },
  {
    "text": "on these various combinations\nof video data sets. And then we freeze that visual\nrepresentation and train",
    "start": "708180",
    "end": "714720"
  },
  {
    "text": "policies for each\ntask, basically using those visual representations. And the numbers you see\nare the performance,",
    "start": "714720",
    "end": "721865"
  },
  {
    "text": "like how successful\nthese policies end up being in their specific areas. So the visual representations\nare not fine-tuned here.",
    "start": "721865",
    "end": "729880"
  },
  {
    "text": "They are used frozen\nand just the policy is trained on top of that.",
    "start": "729880",
    "end": "736380"
  },
  {
    "text": "So larger ViTs were\nbetter than smaller.",
    "start": "736380",
    "end": "745410"
  },
  {
    "text": "Scaling the data sets,\noverall, more data was helpful.",
    "start": "745410",
    "end": "752009"
  },
  {
    "text": "Adding a computer vision data\nset like ImageNet helps, which I thought would not be the\ncase, but I was proven wrong",
    "start": "752010",
    "end": "759260"
  },
  {
    "text": "by my AI resident. ",
    "start": "759260",
    "end": "764360"
  },
  {
    "text": "Cross-domain diversity,\nand by that, I mean, for example, taking\nboth manipulation data set",
    "start": "764360",
    "end": "772639"
  },
  {
    "text": "and navigation set into account\nwas more important than adding many different manipulation\ndata sets, interestingly.",
    "start": "772640",
    "end": "780110"
  },
  {
    "text": "So that was also an\ninteresting insight. ",
    "start": "780110",
    "end": "786650"
  },
  {
    "text": "So unsurprisingly, I wouldn't\nbe talking about this if our model wasn't at the\nend, at least at the time we",
    "start": "786650",
    "end": "793550"
  },
  {
    "text": "did this, the best, according\nto the benchmarks we measured",
    "start": "793550",
    "end": "798950"
  },
  {
    "text": "performance on.  Actually it's because\nwe wanted to get--",
    "start": "798950",
    "end": "805770"
  },
  {
    "text": "so you see two types of-- so first of all, you see a\nranking of the models here.",
    "start": "805770",
    "end": "811230"
  },
  {
    "text": "The top is the model\nthat was trained on Ego4D plus manipulation\nplus navigation and ImageNet,",
    "start": "811230",
    "end": "819470"
  },
  {
    "text": "and that using a ViT-Large,\nand that ended up performing the best. Now, all the darker\npurple ones are variations",
    "start": "819470",
    "end": "826360"
  },
  {
    "text": "that we trained as well,\nbut they ended up not performing as well. For example, the model\nwithout ImageNet is third.",
    "start": "826360",
    "end": "834585"
  },
  {
    "text": " And then all the other and\nlighter colors are baselines,",
    "start": "834585",
    "end": "842810"
  },
  {
    "text": "whether it's other pre-trained\nrepresentations or randomly-- most notably, the randomly\ninitialized models",
    "start": "842810",
    "end": "849250"
  },
  {
    "text": "that we ended up\nfine-tuning for the tasks. So you do see actually a\nsignificant improvement",
    "start": "849250",
    "end": "859630"
  },
  {
    "text": "by pre-training on these\nvideo data sets as compared to randomly fine-tuning--",
    "start": "859630",
    "end": "865450"
  },
  {
    "text": "randomly initializing and then\nfine-tuning on those tasks. ",
    "start": "865450",
    "end": "875580"
  },
  {
    "text": "Yes? What kind of\ninformation or knowledge in the pre-training\nphase [INAUDIBLE]",
    "start": "875580",
    "end": "881810"
  },
  {
    "text": "when looking at\nthe parameters, did you see that they\nwere able to grasp? ",
    "start": "881810",
    "end": "889220"
  },
  {
    "text": "So when we\npre-trained, we didn't have any of the simulation\ntasks in the loop.",
    "start": "889220",
    "end": "896579"
  },
  {
    "text": " So we just look at\nthe losses converging.",
    "start": "896580",
    "end": "905340"
  },
  {
    "text": "But what I can show you later-- I took this out\nbecause I thought the talk would be too long--",
    "start": "905340",
    "end": "911240"
  },
  {
    "text": "I can show you the\nfeatures that we trained. And I think maybe\nI have features of different variations of\nthese, or at least one baseline.",
    "start": "911240",
    "end": "919020"
  },
  {
    "text": "And it's quite\ninteresting to see the different types of features\nthat are being learned. So that's what I use to analyze\nwhat those models-- like,",
    "start": "919020",
    "end": "926760"
  },
  {
    "text": "are those models doing well? I looked at the\nvisualizations of--",
    "start": "926760",
    "end": "932560"
  },
  {
    "text": "I think we use something like\nGrad-CAM, which back-propagates based on the task.",
    "start": "932560",
    "end": "939709"
  },
  {
    "text": "What activations? What is it attending to\nin the image, basically?",
    "start": "939710",
    "end": "944870"
  },
  {
    "text": "And we visualize that. It's quite interesting. Yeah, remind me to\nbring that back.",
    "start": "944870",
    "end": "950540"
  },
  {
    "text": " OK. The best competitor was MVP.",
    "start": "950540",
    "end": "955850"
  },
  {
    "text": "It is very similar\nin the approach but it used different data sets.",
    "start": "955850",
    "end": "961850"
  },
  {
    "text": "And we did use a slightly\ndifferent pre-training algorithm as well. But the primary\ndifference was, I think,",
    "start": "961850",
    "end": "967090"
  },
  {
    "text": "the pre-training data set. And then the worst\nperforming model",
    "start": "967090",
    "end": "972220"
  },
  {
    "text": "was CLIP for these\ntasks, at least. ",
    "start": "972220",
    "end": "980185"
  },
  {
    "text": "So this was the\npre-training phase. And then the question was how--",
    "start": "980185",
    "end": "987590"
  },
  {
    "text": "so let me actually first\nshow you this figure. This is the performance we\ngot on all the seven tasks",
    "start": "987590",
    "end": "994740"
  },
  {
    "text": "where benchmarks\nwere in the red line. You see the best possible\nresult that the community had",
    "start": "994740",
    "end": "1002120"
  },
  {
    "text": "produced by then in any means. We didn't care. We just picked\nwhatever number was",
    "start": "1002120",
    "end": "1008300"
  },
  {
    "text": "the highest we found reported. And in blue, you find our recipe\nof pre-training on video data",
    "start": "1008300",
    "end": "1016940"
  },
  {
    "text": "and then keeping it\nfrozen and training the policy on top of that.",
    "start": "1016940",
    "end": "1022080"
  },
  {
    "text": "And you can see\nthat on some tasks, it did as well, slightly\nbetter, some tasks",
    "start": "1022080",
    "end": "1027680"
  },
  {
    "text": "it was lagging behind. And on average, it was\nstill very impressive result",
    "start": "1027680",
    "end": "1033109"
  },
  {
    "text": "to get this out of the\nbox, this zero shot transfer of visual\nfeatures for robotics.",
    "start": "1033109",
    "end": "1039299"
  },
  {
    "text": "But we wanted to be the\nbest on all of these tasks. So we thought, OK, so now it's\ntime to do this adaptation.",
    "start": "1039300",
    "end": "1047790"
  },
  {
    "text": "Can we adapt VC-1, the\nvisual features, either",
    "start": "1047790",
    "end": "1054210"
  },
  {
    "text": "through end-to-end fine-tuning\nwhile we're learning the policy, or we also studied the MAE-based\nadaptation on in-domain data.",
    "start": "1054210",
    "end": "1062860"
  },
  {
    "text": "That means we collected some\nadditional in-domain data and fused that into\nthe MAE pre-training",
    "start": "1062860",
    "end": "1071520"
  },
  {
    "text": "and fine-tuned with\nMAE basically on top of the initially pre-trained.",
    "start": "1071520",
    "end": "1077130"
  },
  {
    "text": "And now you see we're not still\nalways really 100% the best,",
    "start": "1077130",
    "end": "1083620"
  },
  {
    "text": "but we're basically\non par or better than the prior reported\nnumbers on these tasks",
    "start": "1083620",
    "end": "1091740"
  },
  {
    "text": "with one approach.  I was very excited about that. ",
    "start": "1091740",
    "end": "1099840"
  },
  {
    "text": "We also studied\nthis on hardware, of course, because simulation\ntasks are simulation tasks",
    "start": "1099840",
    "end": "1105630"
  },
  {
    "text": "and I always care about, does\nthis actually work on robots?",
    "start": "1105630",
    "end": "1111170"
  },
  {
    "text": "We studied five different\nmanipulation tasks and did few shot\nimitation learning",
    "start": "1111170",
    "end": "1117110"
  },
  {
    "text": "because large-scale\nreinforcement learning isn't really a thing on\nhardware right now. ",
    "start": "1117110",
    "end": "1124310"
  },
  {
    "text": "We did a TriFinger, and\nClaire helped with this.",
    "start": "1124310",
    "end": "1130070"
  },
  {
    "text": "Then we did random\nreaching with a Franka Arm. We picked up a bottle\nwith the Franka Arm.",
    "start": "1130070",
    "end": "1138720"
  },
  {
    "text": "We had a little\nsetup of a toaster, moving the toaster\nbutton up and down",
    "start": "1138720",
    "end": "1145070"
  },
  {
    "text": "and then also opening a drawer. So those are the tasks. You see the number of\ndemonstrations we used.",
    "start": "1145070",
    "end": "1153890"
  },
  {
    "text": "They are fairly\nsmall, considering what we use these\ndays, and that's",
    "start": "1153890",
    "end": "1159920"
  },
  {
    "text": "what we used to do a few\nshot imitation learning",
    "start": "1159920",
    "end": "1165570"
  },
  {
    "text": "for policies on hardware. And then we compared this to--",
    "start": "1165570",
    "end": "1172093"
  },
  {
    "text": " so these were internal\nbenchmarks that we had.",
    "start": "1172093",
    "end": "1177130"
  },
  {
    "text": "So we compared this to our\nbest-performing baselines that we had internally. And we were able to\nshow that with VC-1, we",
    "start": "1177130",
    "end": "1186150"
  },
  {
    "text": "were able to outperform those\ninternal baselines we had. Just to give you a\nsense of these tasks--",
    "start": "1186150",
    "end": "1194070"
  },
  {
    "text": "the Franka arm different\npicking up the bottle. We might have included some\nfailure in this video as well.",
    "start": "1194070",
    "end": "1202290"
  },
  {
    "text": "There's extreme parts that\nare connected to the model? Yes. So we were-- it's kind of\nan automatic task-resetting",
    "start": "1202290",
    "end": "1209280"
  },
  {
    "text": "mechanism. ",
    "start": "1209280",
    "end": "1215280"
  },
  {
    "text": "Yeah, struggled a\nbit with that one. Then the opening of the drawer.",
    "start": "1215280",
    "end": "1221633"
  },
  {
    "start": "1221633",
    "end": "1230600"
  },
  {
    "text": "Pushing the cube\ninto a goal image. Yeah.",
    "start": "1230600",
    "end": "1235790"
  },
  {
    "text": "That's it. That's just a few examples. If you want to learn more\nabout this specific project,",
    "start": "1235790",
    "end": "1242429"
  },
  {
    "text": "you can go and find this. We open-sourced the benchmarks\nas well as the models.",
    "start": "1242430",
    "end": "1251870"
  },
  {
    "text": "So yeah? I have a question. Yes? So you had this one figure where\nyou showed, through adaptation,",
    "start": "1251870",
    "end": "1259820"
  },
  {
    "text": "that everything got better. How was that done? Like, fine-tuning or-- Yes, we did end-to-end\nfine-tuning.",
    "start": "1259820",
    "end": "1266430"
  },
  {
    "text": "And whether it was imitation\nlearning or reinforcement learning depended on the task,\nbecause some of these tasks",
    "start": "1266430",
    "end": "1271580"
  },
  {
    "text": "were done via\nreinforcement learning and some were imitation\nlearning, basically. OK. ",
    "start": "1271580",
    "end": "1278900"
  },
  {
    "text": "Yeah? So just a quick follow\nup on that area. Was it a combination\nof the domain",
    "start": "1278900",
    "end": "1284770"
  },
  {
    "text": "MAE objective and the\ndownstream task objective?",
    "start": "1284770",
    "end": "1290410"
  },
  {
    "text": "We studied both of them. I think the numbers\nthat I showed you were with the\nend-to-end fine-tuning.",
    "start": "1290410",
    "end": "1296150"
  },
  {
    "text": "And we also have numbers with\nthe MAE-based adaptation only. OK, thanks.",
    "start": "1296150",
    "end": "1302020"
  },
  {
    "text": "And so I think the end-to-end\nfine-tuning ended up working a little bit better. ",
    "start": "1302020",
    "end": "1309700"
  },
  {
    "text": "Interesting tidbit about\nthis is for the MuJoCo tasks, the domain gap was\nactually quite large",
    "start": "1309700",
    "end": "1316000"
  },
  {
    "text": "because we trained on real data. So that one were actually\nthe hardest to close the gap.",
    "start": "1316000",
    "end": "1323049"
  },
  {
    "text": "And that's also why I wanted\nto move to real robot tasks, because I was like, I don't\nwant to worry about fixing",
    "start": "1323050",
    "end": "1328539"
  },
  {
    "text": "the Real2Sim gap right now. ",
    "start": "1328540",
    "end": "1336870"
  },
  {
    "text": "So what's next on this front? Because that's mostly the area\nI am focusing on right now.",
    "start": "1336870",
    "end": "1342860"
  },
  {
    "text": "Well, this was all about\njust visual representation,",
    "start": "1342860",
    "end": "1347910"
  },
  {
    "text": "which is all good, but\nwhat about actions? At the end of the day, we want\nto train policies or models that",
    "start": "1347910",
    "end": "1355429"
  },
  {
    "text": "can help robots perform\ntasks in the real world.",
    "start": "1355430",
    "end": "1361710"
  },
  {
    "text": "And so I particularly, I'm\ninterested now in world models.",
    "start": "1361710",
    "end": "1366720"
  },
  {
    "text": "So I'm looking at\naction-conditioned world models, pre-training them\non data out there",
    "start": "1366720",
    "end": "1372860"
  },
  {
    "text": "and getting action\ndata in there. So yeah, if you're\ncurious about this, please ping me because I think\nabout this a lot right now.",
    "start": "1372860",
    "end": "1380990"
  },
  {
    "text": "So this is now where I zoom out\nand talk about some projects from other people in the\nrobotics team at Fair.",
    "start": "1380990",
    "end": "1387130"
  },
  {
    "text": " So the second data hypothesis\nis large-scale reinforcement",
    "start": "1387130",
    "end": "1395300"
  },
  {
    "text": "learning or behavior\ncloning in simulation. So what Fair Robotics\nhas worked on is",
    "start": "1395300",
    "end": "1401679"
  },
  {
    "text": "the development of visually\nrealistic simulators and benchmarks, then train and\nevaluate high-level reasoning",
    "start": "1401680",
    "end": "1410080"
  },
  {
    "text": "policies, particularly focused\non high-level reasoning, so far, in simulation, and then\ntransfer that to the real world.",
    "start": "1410080",
    "end": "1420008"
  },
  {
    "text": "Of course, the definition\nof high-level reasoning is a bit fuzzy, but you'll\nsee one concrete example.",
    "start": "1420008",
    "end": "1427660"
  },
  {
    "text": "Just to give you some\nsnapshots of the simulator.",
    "start": "1427660",
    "end": "1432860"
  },
  {
    "text": "So we have habitat 2.0\nhere that came out in 2021.",
    "start": "1432860",
    "end": "1439150"
  },
  {
    "text": "And it was all about\ntraining home robots to rearrange their habitat.",
    "start": "1439150",
    "end": "1445250"
  },
  {
    "text": "So it was basically\na general definition of pick and place\ntasks with diverse home",
    "start": "1445250",
    "end": "1453490"
  },
  {
    "text": "environments, diverse objects,\nand also home environments and so on. That was open sourced.",
    "start": "1453490",
    "end": "1460140"
  },
  {
    "text": "The simulator as well\nas the data sets. So the RDF, the\nassets to generate",
    "start": "1460140",
    "end": "1467340"
  },
  {
    "text": "these visually realistic\nscenes were all open source. And there's also a\nchallenge around this.",
    "start": "1467340",
    "end": "1474250"
  },
  {
    "text": "So people can actually study\nthis themselves as well. And then there was\na recent follow-up",
    "start": "1474250",
    "end": "1480660"
  },
  {
    "text": "on this, the Habitat\n3.0, which is now focused on also getting\nhumans in the same simulation",
    "start": "1480660",
    "end": "1486900"
  },
  {
    "text": "and studying social\nrearrangements",
    "start": "1486900",
    "end": "1493110"
  },
  {
    "text": "or social navigation,\nthat's how we call it. But it's moving towards\ncollaborating with humans",
    "start": "1493110",
    "end": "1499740"
  },
  {
    "text": "basically. Yeah? How are the humans\ninteractions guided?",
    "start": "1499740",
    "end": "1507000"
  },
  {
    "text": "You mean how are they modeled? There's an agent basically\nthat represents the human.",
    "start": "1507000",
    "end": "1514840"
  },
  {
    "text": "Yeah? All right. ",
    "start": "1514840",
    "end": "1522500"
  },
  {
    "text": "One of the projects--\nso of course, when you have this\nhypothesis, at some point,",
    "start": "1522500",
    "end": "1527570"
  },
  {
    "text": "you have to show that if\nyou're training in simulation, that actually leads to something\nthat works in the real world.",
    "start": "1527570",
    "end": "1533440"
  },
  {
    "text": "This is one of the\nprojects of one of my collaborators,\nAkshara, who basically",
    "start": "1533440",
    "end": "1540460"
  },
  {
    "text": "took this general purpose\nmobile manipulation task of I want to be able to\npick and place objects",
    "start": "1540460",
    "end": "1550389"
  },
  {
    "text": "from random location and\nrandom unseen objects and perform this task many\ntimes and be successful.",
    "start": "1550390",
    "end": "1559960"
  },
  {
    "text": "So this is the task\ndescription that you see here. You have a robot\ninitialized somewhere. It needs to navigate\nto the right place,",
    "start": "1559960",
    "end": "1568420"
  },
  {
    "text": "pick the right object, and\nthen navigate to the second-- the placing target and\nthen place it there.",
    "start": "1568420",
    "end": "1575590"
  },
  {
    "text": "Basically, so that's\nthe task definition. ",
    "start": "1575590",
    "end": "1581931"
  },
  {
    "text": "There were two phases of\ntraining, both of these phases are trained in simulation.",
    "start": "1581931",
    "end": "1587850"
  },
  {
    "text": "Akshara decided to train some\nbasic skills in simulation. Those were the navigation\nskills, the pick skill,",
    "start": "1587850",
    "end": "1595860"
  },
  {
    "text": "and the place skill. And they were trained with\ngeneralization in mind. So in simulation, you basically\ntake a bunch of different home",
    "start": "1595860",
    "end": "1605190"
  },
  {
    "text": "environments and objects\nand then train those skills to perform the task basically.",
    "start": "1605190",
    "end": "1612700"
  },
  {
    "text": "And then there was\na high-level policy that does the coordination\nbetween those skills,",
    "start": "1612700",
    "end": "1622530"
  },
  {
    "text": "like decides when\nto pick what skill. And also it has an\nadaptation layer",
    "start": "1622530",
    "end": "1628200"
  },
  {
    "text": "to adapt to any\ndisturbances at test time.",
    "start": "1628200",
    "end": "1634885"
  },
  {
    "text": "And I linked the\nproject link because I don't know all the details\nof this project anymore.",
    "start": "1634885",
    "end": "1641560"
  },
  {
    "text": "But if you're curious,\nplease follow that link. So ASC was trained entirely\nwithin the habitat simulator.",
    "start": "1641560",
    "end": "1650980"
  },
  {
    "text": "And there were two types\nof data sets used-- HM3D data set which was used\nfor the navigation training",
    "start": "1650980",
    "end": "1658100"
  },
  {
    "text": "and then the ReplicaCAD was\nused to train the pick, place, and coordination. And then on the right\nside, you actually",
    "start": "1658100",
    "end": "1664270"
  },
  {
    "text": "see our Fremont\napartment that we have at Meta, which we use as\na held-out test validation set.",
    "start": "1664270",
    "end": "1672110"
  },
  {
    "text": "So our real-world testing suite.",
    "start": "1672110",
    "end": "1677450"
  },
  {
    "text": "And you see the robot\nperforming the tasks. There are many more videos on\nthat link if you're curious.",
    "start": "1677450",
    "end": "1684850"
  },
  {
    "text": "And yes, this is a smart robot.  Any questions about\nthis part of the talk?",
    "start": "1684850",
    "end": "1691910"
  },
  {
    "start": "1691910",
    "end": "1699060"
  },
  {
    "text": "I also want to highlight\ntwo recent benchmarks that we put out. So this is actually something\nthat I was involved in as well--",
    "start": "1699060",
    "end": "1706210"
  },
  {
    "text": "OpenEQA. So they are both-- the two benchmarks\nI'll be talking about, they're both using\nhabitat as a foundation.",
    "start": "1706210",
    "end": "1714180"
  },
  {
    "text": "And this is an agent that\nwe drop an agent into a home",
    "start": "1714180",
    "end": "1719730"
  },
  {
    "text": "environment, and then we ask\nopen vocabulary questions like what color are the\nplants or what food is next.",
    "start": "1719730",
    "end": "1727270"
  },
  {
    "text": "And then the agent will have\nto navigate to that place,",
    "start": "1727270",
    "end": "1732540"
  },
  {
    "text": "or it has to do it from\nmemory, like assuming it has been able to\nexplore the whole space",
    "start": "1732540",
    "end": "1738179"
  },
  {
    "text": "and then answer those questions.  This was the first open\nvocabulary benchmark",
    "start": "1738180",
    "end": "1746400"
  },
  {
    "text": "data set for EQA, which is\nEmbodied Question Answering.",
    "start": "1746400",
    "end": "1752270"
  },
  {
    "text": "And it supports both use cases. This episodic memory,\nwhere you assume the robot",
    "start": "1752270",
    "end": "1757610"
  },
  {
    "text": "has already explored\nthe whole space, and it can answer from memory,\nor the active exploration use",
    "start": "1757610",
    "end": "1762950"
  },
  {
    "text": "case where it says\noh, I don't know. And now it has to\nfigure out where to go.",
    "start": "1762950",
    "end": "1769520"
  },
  {
    "text": "It contains over 1,600 high\nquality human-generated questions from over\n100 real world--",
    "start": "1769520",
    "end": "1777080"
  },
  {
    "text": "and 180 real-world environments. And because this is\na benchmark, once you",
    "start": "1777080",
    "end": "1784670"
  },
  {
    "text": "have these open\nvocabulary questions, you actually need a\nway to test how well",
    "start": "1784670",
    "end": "1791210"
  },
  {
    "text": "the agent is performing. We have developed an\nautomatic language model powered evaluation protocol\nthat shows excellent correlation",
    "start": "1791210",
    "end": "1799669"
  },
  {
    "text": "with human judgment. So that is one of\nthe benchmarks.",
    "start": "1799670",
    "end": "1806970"
  },
  {
    "text": "And the other benchmark\nthat PARTNR team put out very recently,\nactually is called PARTNR.",
    "start": "1806970",
    "end": "1815919"
  },
  {
    "text": "And this is about enabling\nthis multi-agent reasoning",
    "start": "1815920",
    "end": "1821040"
  },
  {
    "text": "and planning, so\nstudying that aspect. And primarily, we have in\nmind a robot and a human agent",
    "start": "1821040",
    "end": "1829950"
  },
  {
    "text": "basically. And the tasks are\nall about maybe",
    "start": "1829950",
    "end": "1835169"
  },
  {
    "text": "cleaning the home environment. And the robot agent has to-- in our case, the way\nwe think about it",
    "start": "1835170",
    "end": "1840840"
  },
  {
    "text": "has to adapt to all\nthe human is picking up the pillow on the floor. So I'll pick up something else\nbecause that's more efficient,",
    "start": "1840840",
    "end": "1848650"
  },
  {
    "text": "so those kind of things. PARTNR comprises 100,000 natural\nlanguage tasks designed to study",
    "start": "1848650",
    "end": "1856259"
  },
  {
    "text": "this multi-agent\nreasoning and planning. It also uses LLMs to generate\nthese tasks at scales--",
    "start": "1856260",
    "end": "1862910"
  },
  {
    "text": "Excuse me. No worries. That exhibits characteristics\nof everyday tasks",
    "start": "1862910",
    "end": "1868039"
  },
  {
    "text": "where you have variations in\nspatial-temporal constraints.",
    "start": "1868040",
    "end": "1873320"
  },
  {
    "text": "Temporal constraint\ncould be something like something has to be\ndone before the next part can be done.",
    "start": "1873320",
    "end": "1878845"
  },
  {
    "text": " And it also enables\nevaluation of AI agents",
    "start": "1878845",
    "end": "1884600"
  },
  {
    "text": "with real human partners. So a human in the\nloop infrastructure.",
    "start": "1884600",
    "end": "1890240"
  },
  {
    "text": "Go check out those\nbenchmarks if you're curious about these type of problems. ",
    "start": "1890240",
    "end": "1896585"
  },
  {
    "text": "So now, I will get\nto the final piece-- the teleoperation piece.",
    "start": "1896585",
    "end": "1902710"
  },
  {
    "text": " We are particularly\nfocused on teleoperation",
    "start": "1902710",
    "end": "1909350"
  },
  {
    "text": "for dexterous\nmanipulation tasks. This is where a\nlot of work still needs to be done to get really\ngood teleoperation with also",
    "start": "1909350",
    "end": "1919460"
  },
  {
    "text": "tactile sensing in the loop. So I'll actually touch on\nfirst the tactile sensing",
    "start": "1919460",
    "end": "1926080"
  },
  {
    "text": "and the general models\nwe are training for that and then how we start using them\nto build teleoperation stacks",
    "start": "1926080",
    "end": "1935260"
  },
  {
    "text": "for dexterous manipulation. And here the question\nis now general.",
    "start": "1935260",
    "end": "1941110"
  },
  {
    "text": "I think for dexterous\nmanipulation, we all have this intuition\nthat we-- or at least for us,",
    "start": "1941110",
    "end": "1946580"
  },
  {
    "text": "we have this intuition that we\nwill need this real-world data. We will need this\ntactile feedback to actually train\nrobust robot policies.",
    "start": "1946580",
    "end": "1954110"
  },
  {
    "text": "And so that's why we're focusing\nthe efforts on that area",
    "start": "1954110",
    "end": "1959170"
  },
  {
    "text": "right now.  I don't know how many\nof you have seen it,",
    "start": "1959170",
    "end": "1965269"
  },
  {
    "text": "but we have recently had a big\nrelease of the Digit 360, which was a big collaboration across\nmany different organizations",
    "start": "1965270",
    "end": "1973870"
  },
  {
    "text": "as well. It's the latest development\nof this tactile sensor, which",
    "start": "1973870",
    "end": "1981200"
  },
  {
    "text": "is a vision-based tactile\nsensor, has a silicone fingertip, hyperfisheye\nlens and so on.",
    "start": "1981200",
    "end": "1991490"
  },
  {
    "text": "And we actually put those\non the Allegro Hand. So we have an Allegro\nHand where each fingertip",
    "start": "1991490",
    "end": "1997159"
  },
  {
    "text": "is one of those digits. And that's what we\nwork with at Meta. ",
    "start": "1997160",
    "end": "2003850"
  },
  {
    "text": "What's interesting\nis that we have-- so we have had\nvariations of this.",
    "start": "2003850",
    "end": "2009410"
  },
  {
    "text": "I mean, GelSight had the\nfirst variation of this,",
    "start": "2009410",
    "end": "2014830"
  },
  {
    "text": "but then we started\nbuilding on top of this. And so now we actually have\na family of visual tactile",
    "start": "2014830",
    "end": "2020740"
  },
  {
    "text": "sensors, and we want to be able\nto use the data from all of them because it is so hard to\nactually get that data.",
    "start": "2020740",
    "end": "2027620"
  },
  {
    "text": "So the part of the team,\nthe robotics team studied learning general touch\nrepresentations that",
    "start": "2027620",
    "end": "2034150"
  },
  {
    "text": "can take in tactile images\nfrom any of these sensors, and basically trained\nan encoder also",
    "start": "2034150",
    "end": "2041730"
  },
  {
    "text": "with self-supervised\nlearning that can then be used for many tactile sensing\ntasks like force estimation,",
    "start": "2041730",
    "end": "2049510"
  },
  {
    "text": "slip detection, or also policy\nlearning at the end of the day.",
    "start": "2049510",
    "end": "2054600"
  },
  {
    "text": "And so we applied this also\nto these different sensors. And this is super cool that we\nstart using this encoder now",
    "start": "2054600",
    "end": "2063570"
  },
  {
    "text": "for training policies\nor even work models. ",
    "start": "2063570",
    "end": "2072960"
  },
  {
    "text": "Now, on the teleoperation\nside, the thing",
    "start": "2072960",
    "end": "2080849"
  },
  {
    "text": "that we've been working on is-- so you see the-- let me actually first\nshow you the videos.",
    "start": "2080850",
    "end": "2086980"
  },
  {
    "text": "I think that is better. So what we're aiming to do is\nwe have a human with a glove",
    "start": "2086980",
    "end": "2095040"
  },
  {
    "text": "and we want to mimic, on the\nrobot side, that movement",
    "start": "2095040",
    "end": "2101890"
  },
  {
    "text": "that the human is doing. And on this side,\nthe left side, you",
    "start": "2101890",
    "end": "2108220"
  },
  {
    "text": "saw what happens if you\ndon't have DexGen, that's what people ended up calling\nit, which I will talk about",
    "start": "2108220",
    "end": "2116560"
  },
  {
    "text": "in a second. And then on the\nright-hand side, you",
    "start": "2116560",
    "end": "2122080"
  },
  {
    "text": "see how it looks like if\nyou use the controller that was trained by our team.",
    "start": "2122080",
    "end": "2128569"
  },
  {
    "text": "So this is the end goal-- to improve this teleoperation\nwith tactile gloves.",
    "start": "2128570",
    "end": "2140293"
  },
  {
    "text": " And the way the team went about\nthis is, again, in two phases",
    "start": "2140293",
    "end": "2147790"
  },
  {
    "text": "where we have a pre-training\nstage in simulation. Interestingly, where we divide--\nlike we have multiple tasks",
    "start": "2147790",
    "end": "2158130"
  },
  {
    "text": "defined in simulation, and\nwe train controllers for--",
    "start": "2158130",
    "end": "2163500"
  },
  {
    "text": "we train one controller\nfor all of these tasks. And out of that controller comes\na learned action distribution,",
    "start": "2163500",
    "end": "2174340"
  },
  {
    "text": "so something that maps. Yeah, we basically\nhave a distribution",
    "start": "2174340",
    "end": "2181140"
  },
  {
    "text": "over all possible actions that\nwe've trained in simulation. And then at test time\nnow on the real robot,",
    "start": "2181140",
    "end": "2190359"
  },
  {
    "text": "we take, either from\na policy or a teleop,",
    "start": "2190360",
    "end": "2195690"
  },
  {
    "text": "the desired motion that we want\nto accomplish, so the command,",
    "start": "2195690",
    "end": "2200849"
  },
  {
    "text": "feed that into this controller. And what the controller\nends up doing is projecting it into what\nis the safe space, basically",
    "start": "2200850",
    "end": "2213680"
  },
  {
    "text": "for the hand to perform. I don't know all the details\nabout it, I have to admit,",
    "start": "2213680",
    "end": "2220230"
  },
  {
    "text": "but it's quite\ninteresting to see. And again, I think the results-- so this was just open source\nI think, very recently,",
    "start": "2220230",
    "end": "2227790"
  },
  {
    "text": "like two weeks ago or so. So yeah, that is where we're\nheading with the teleoperation.",
    "start": "2227790",
    "end": "2236810"
  },
  {
    "text": "Overall, what we\nare aiming to do is to put the Allegro\nHand on the Franka Arm",
    "start": "2236810",
    "end": "2242960"
  },
  {
    "text": "and the Franka Arm\non a mobile base and now start collecting\ndata for many diverse tasks.",
    "start": "2242960",
    "end": "2249000"
  },
  {
    "text": "But that gives us\nvision, tactile sensing, and proprioception\ncombined, and then",
    "start": "2249000",
    "end": "2256280"
  },
  {
    "text": "start collecting\ndata, data, data, and then train better\nand better models.",
    "start": "2256280",
    "end": "2262940"
  },
  {
    "text": "That's it. That's all I have for today. If you have any questions\nabout any of these projects,",
    "start": "2262940",
    "end": "2269640"
  },
  {
    "text": "I will not maybe be able\nto answer all of them, but I can forward you\nto the relevant person.",
    "start": "2269640",
    "end": "2276250"
  },
  {
    "text": "So please ping me. And also the robotics team\nis hiring, so please let me know if you're interested\nin interviewing.",
    "start": "2276250",
    "end": "2284378"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "2284378",
    "end": "2290440"
  },
  {
    "text": "Great. And it's time for questions. Yes. ",
    "start": "2290440",
    "end": "2296430"
  },
  {
    "text": "Any questions? Yes? Can you share your opinion\nabout what are the [INAUDIBLE]",
    "start": "2296430",
    "end": "2305550"
  },
  {
    "text": "or challenges for wold models? For world models? [INAUDIBLE] Yes, I can share a little\nbit more on that front.",
    "start": "2305550",
    "end": "2312900"
  },
  {
    "text": "So for world models, where I\nsee the biggest challenge right now is actually correctly\npredicting object interactions.",
    "start": "2312900",
    "end": "2323250"
  },
  {
    "text": "We have some very first results\nfor reaching movements, which was impressively, we\nused no training data--",
    "start": "2323250",
    "end": "2332760"
  },
  {
    "text": "so we used the Droid data set. We took a video world model and\nfine-tuned that with action data",
    "start": "2332760",
    "end": "2339290"
  },
  {
    "text": "from Detroit data set. And that zero shot transferred\nto our Franka setup",
    "start": "2339290",
    "end": "2345740"
  },
  {
    "text": "for reaching tasks. But now when we try to push\nan object or grasp an object,",
    "start": "2345740",
    "end": "2350819"
  },
  {
    "text": "we start to see the\nlimitations of world models. So I actually think we need\nto get contact in those world",
    "start": "2350820",
    "end": "2359000"
  },
  {
    "text": "models or some tactile sensing. So that's one of the\naxes we're exploring.",
    "start": "2359000",
    "end": "2364280"
  },
  {
    "text": "But at the same time, it's\nstill just more data basically that I think we need to\ntrain better world models.",
    "start": "2364280",
    "end": "2373550"
  },
  {
    "text": "Yeah? I have questions on, what is\nyour opinions on the bottleneck",
    "start": "2373550",
    "end": "2380930"
  },
  {
    "text": "in the VLA model? What is the next big thing for\nvision-language-action model?",
    "start": "2380930",
    "end": "2387940"
  },
  {
    "text": "So the world model\nor a policy like-- What is next biggest bottleneck\nto make vision-language-action?",
    "start": "2387940",
    "end": "2397420"
  },
  {
    "text": "So we do-- we're actually also\nlooking at this right now. It's just we don't have\nanything put out yet.",
    "start": "2397420",
    "end": "2403630"
  },
  {
    "text": "I mean, to be honest, I\nfeel like the generalization of these models isn't yet\nthere where we need them.",
    "start": "2403630",
    "end": "2411700"
  },
  {
    "text": "And so to me, it's\nunderstanding, how can we leverage these\nreal arms that give us",
    "start": "2411700",
    "end": "2419650"
  },
  {
    "text": "very good semantic\nunderstanding of the world and connecting it to a policy. I don't necessarily\nthink the right way is",
    "start": "2419650",
    "end": "2425620"
  },
  {
    "text": "to try to make the real\nand predict an action. That doesn't feel right\nto me, but who knows?",
    "start": "2425620",
    "end": "2433280"
  },
  {
    "text": "Someone might prove\nme wrong again. But I think connecting that\nknowledge to the policies",
    "start": "2433280",
    "end": "2443920"
  },
  {
    "text": "is how to do this effectively,\nis a good question to study.",
    "start": "2443920",
    "end": "2451069"
  },
  {
    "text": "And then I just\nreally think we need to continue pushing on\nthe generalization access,",
    "start": "2451070",
    "end": "2456240"
  },
  {
    "text": "because we are downloading\nsome of these models and testing them out.",
    "start": "2456240",
    "end": "2461310"
  },
  {
    "text": "And what I'm seeing is not like\nzero shot transfer to our setup, basically.",
    "start": "2461310",
    "end": "2466980"
  },
  {
    "text": "So it's not there\nyet, in my opinion. Oh, we've seen some zero-shot\ntransfer of these models.",
    "start": "2466980",
    "end": "2474359"
  },
  {
    "text": "We've seen some zero-- We've seen some, yeah. OK, so what tasks? Pick and place,\nfolding, stacking,",
    "start": "2474360",
    "end": "2482390"
  },
  {
    "text": "some low-precision insertion. Interesting. OK. Yeah, without any fine-tuning?",
    "start": "2482390",
    "end": "2488589"
  },
  {
    "text": "Without any fine-tuning. OK. But also our just general\nlab is in the Droid data set,",
    "start": "2488590",
    "end": "2495660"
  },
  {
    "text": "but just not that particular\nsetup and that table and all of this. Yeah, we are struggling a\nlittle bit with getting this.",
    "start": "2495660",
    "end": "2502170"
  },
  {
    "text": "That's interesting to know. Yeah, interesting. Is that how it come to\n[INAUDIBLE] does it connect",
    "start": "2502170",
    "end": "2507550"
  },
  {
    "text": "to 3D world? Oh, that's a very good question.",
    "start": "2507550",
    "end": "2512590"
  },
  {
    "text": "I don't know. [LAUGHS] I think one question I\nhave thought about a lot is this",
    "start": "2512590",
    "end": "2520210"
  },
  {
    "text": "question of memory, how\nto represent memory, because the 3D world is a\ngood representation to store",
    "start": "2520210",
    "end": "2529270"
  },
  {
    "text": "information basically, that\nyou've observed, so you don't-- ",
    "start": "2529270",
    "end": "2535510"
  },
  {
    "text": "you don't have to re-explore\nthe home every time when you're trying to\nfind an object because you",
    "start": "2535510",
    "end": "2542109"
  },
  {
    "text": "have seen it probably before. And the 3D world is a good\nway to store that information.",
    "start": "2542110",
    "end": "2550029"
  },
  {
    "text": "But I don't-- yeah, tokenization\nfor 3D representations is very,",
    "start": "2550030",
    "end": "2557210"
  },
  {
    "text": "very much behind tokenization\nfor 2D representation. So it's a good research\ntopic to look at.",
    "start": "2557210",
    "end": "2565020"
  },
  {
    "text": "For manipulation tasks,\nI wonder if 3D or 2D, like implicit or explicit\nrepresentations are the right",
    "start": "2565020",
    "end": "2571850"
  },
  {
    "text": "thing to do. So that's another thing\nI think a lot about. Any questions on Zoom?",
    "start": "2571850",
    "end": "2577560"
  },
  {
    "text": "Yes, there's one\nquestion from Zoom. So how is tactile\nsensing used in DexGen?",
    "start": "2577560",
    "end": "2583565"
  },
  {
    "text": "It's not used yet but\nit's coming up now. ",
    "start": "2583565",
    "end": "2590000"
  },
  {
    "text": "Next question. Yes. Sorry. No, no, no. No. I thought you were [INAUDIBLE].",
    "start": "2590000",
    "end": "2596270"
  },
  {
    "text": "I don't know. Maybe related to that previous\nquestion, but have you released any open\nsource data sets",
    "start": "2596270",
    "end": "2602750"
  },
  {
    "text": "that have tactile\nsensing and vision? So I think we released\nthe SparGe data set.",
    "start": "2602750",
    "end": "2610680"
  },
  {
    "text": "I'd have to quickly google\nthis, but I don't know if we-- so there's a link here. I don't know if we also gave\nimages probably not-- like RGB,",
    "start": "2610680",
    "end": "2620300"
  },
  {
    "text": "I don't think so. But yeah, that's what we\nwould be working towards.",
    "start": "2620300",
    "end": "2627460"
  },
  {
    "text": "That is in the near horizon? Well, I don't know how near,\nlet's say maybe this year,",
    "start": "2627460",
    "end": "2635480"
  },
  {
    "text": "I don't know. It's hard to say. ",
    "start": "2635480",
    "end": "2642487"
  },
  {
    "text": "You talked about some research-- I'm not sure if it was yours\nor somebody else at Fair, doing navigation, pick,\nand place with Spot robot?",
    "start": "2642487",
    "end": "2650060"
  },
  {
    "text": "Yeah. What was the 3D\nrepresentation there? Or was there one?",
    "start": "2650060",
    "end": "2657010"
  },
  {
    "text": "No, there was actually-- so there were some assumptions\nthat were made like you knew",
    "start": "2657010",
    "end": "2666130"
  },
  {
    "text": "the-- you saw that the robot\nhad a 3D location of itself,",
    "start": "2666130",
    "end": "2671619"
  },
  {
    "text": "and it also knew the 3D\nlocation of the receptacles. So there was a little bit of\na map there-- yeah, a course",
    "start": "2671620",
    "end": "2678810"
  },
  {
    "text": "map basically. Is that provided by\nthe simulator itself? It was provided\nby the simulator.",
    "start": "2678810",
    "end": "2685509"
  },
  {
    "text": "And then in the\nreal world, I think it was provided by\nmanually measuring things.",
    "start": "2685510",
    "end": "2691490"
  },
  {
    "text": " More questions?",
    "start": "2691490",
    "end": "2697060"
  },
  {
    "text": " So you created some\nvisual representation",
    "start": "2697060",
    "end": "2704250"
  },
  {
    "text": "with your visual cortex model. And how are you translating\nthat into robot actions",
    "start": "2704250",
    "end": "2712320"
  },
  {
    "text": "for downstream tasks? Yeah, I rushed over this-- ",
    "start": "2712320",
    "end": "2721110"
  },
  {
    "text": "yeah, I took that slide. And I don't even think I\nhave it in the backup slides. So it's a little hard\nto say this generally",
    "start": "2721110",
    "end": "2730410"
  },
  {
    "text": "because we have these\nvery diverse tasks. But when you look\nat the MuJoCo tasks,",
    "start": "2730410",
    "end": "2737620"
  },
  {
    "text": "the TriFinger,\nMeta-World, they were done with demonstration data. So we had a set of demonstration\ndata that we had collected",
    "start": "2737620",
    "end": "2746049"
  },
  {
    "text": "and some low-shot number, I\ndon't remember the exact number. And then we did behavior\ncloning for those",
    "start": "2746050",
    "end": "2753160"
  },
  {
    "text": "where we had demonstration\ndata and end-to-end fine-tuned through that behavior cloning.",
    "start": "2753160",
    "end": "2759250"
  },
  {
    "text": "For ImageNav, that was a\nlarge-scale reinforcement",
    "start": "2759250",
    "end": "2764800"
  },
  {
    "text": "learning policy\ntrained in simulation. So that one doesn't assume\nany prior information.",
    "start": "2764800",
    "end": "2772130"
  },
  {
    "text": "It just takes\nsensory observations and trains a policy through\nreinforcement learning.",
    "start": "2772130",
    "end": "2779740"
  },
  {
    "text": "ObjectNav is a\nlarge-scale behavior cloning data set that we used.",
    "start": "2779740",
    "end": "2787119"
  },
  {
    "text": "And the Mobile-Pick skill is\nalso reinforcement learning, so it depended on the task.",
    "start": "2787120",
    "end": "2794345"
  },
  {
    "text": " So for some of these\ntasks, like you're",
    "start": "2794345",
    "end": "2801810"
  },
  {
    "text": "seeing some improvement\non some of them and on others\ncomparable performance.",
    "start": "2801810",
    "end": "2811710"
  },
  {
    "text": "I'm wondering if aside from\nfinal performance or success",
    "start": "2811710",
    "end": "2817230"
  },
  {
    "text": "rate on these tasks, whether\nyou saw other ancillary benefits of pre-training these\nrepresentations,",
    "start": "2817230",
    "end": "2823710"
  },
  {
    "text": "because final performance,\nyou might be bottlenecked here by the actual task data, for\nexample imitation learning.",
    "start": "2823710",
    "end": "2830589"
  },
  {
    "text": "But were you able to\nconverge faster in training or your more sample efficient\nfor these sorts of questions?",
    "start": "2830590",
    "end": "2837369"
  },
  {
    "text": "Yeah, we didn't-- I had this intuition\nthat it probably will have to be more\nsample-efficient.",
    "start": "2837370",
    "end": "2843210"
  },
  {
    "text": "We just didn't end up studying\nit because for time reasons,",
    "start": "2843210",
    "end": "2848820"
  },
  {
    "text": "I know that in the\nreinforcement learning-- on the reinforcement\nlearning side, we did look at speed\nof convergence.",
    "start": "2848820",
    "end": "2855770"
  },
  {
    "text": "And the thing is, the reason\nwhy we ended up reporting it like this is if you just give\nenough time to the policy,",
    "start": "2855770",
    "end": "2864140"
  },
  {
    "text": "starting with the randomly\ninitialized controller, if it can catch UP, That\nwas OK with us.",
    "start": "2864140",
    "end": "2870300"
  },
  {
    "text": "We just wanted to report\nthe best number we can get, but we did definitely\nsee faster--",
    "start": "2870300",
    "end": "2876329"
  },
  {
    "text": "with the pre-trained\nvisual model, you get to a better\nnumber faster basically.",
    "start": "2876330",
    "end": "2882140"
  },
  {
    "text": "So that initial pickup-- that initial gain\nis faster basically.",
    "start": "2882140",
    "end": "2887730"
  },
  {
    "text": " More questions?",
    "start": "2887730",
    "end": "2894349"
  },
  {
    "text": "So we see a big gap\nbetween the generalization",
    "start": "2894350",
    "end": "2899510"
  },
  {
    "text": "you showed at the first slide\nand the cortex [INAUDIBLE]. So can you think of any\nsimulated environment",
    "start": "2899510",
    "end": "2908120"
  },
  {
    "text": "or simulated bench that\ncan bring the gap closer?",
    "start": "2908120",
    "end": "2913510"
  },
  {
    "text": "What I mean is, if we\nhave a eight instead of seven bench over there, it\nwill probably be more general.",
    "start": "2913510",
    "end": "2923280"
  },
  {
    "text": "But can we think of\nmaybe one simple test that if the model solves\nthis in simulation,",
    "start": "2923280",
    "end": "2932130"
  },
  {
    "text": "it will be more generalizable\nfor that kind of scenarios?",
    "start": "2932130",
    "end": "2938039"
  },
  {
    "text": "Sorry, are you asking\ncan we find that one experiment in simulation\nthat will tell us",
    "start": "2938040",
    "end": "2945089"
  },
  {
    "text": "whether it's a general-- Yeah, maybe help us maybe--",
    "start": "2945090",
    "end": "2953340"
  },
  {
    "text": "I haven't found that yet. And worse with-- depending\non the simulator you use,",
    "start": "2953340",
    "end": "2961410"
  },
  {
    "text": "the Sim2Real transfer isn't\nalways very clear either. So we actually did\na correlation study",
    "start": "2961410",
    "end": "2966990"
  },
  {
    "text": "between if we use\nVC-1 and simulation and then transfer the policies\nwith different parameters",
    "start": "2966990",
    "end": "2976160"
  },
  {
    "text": "or different\nbackbones to hardware, is the are they ranked the same?",
    "start": "2976160",
    "end": "2981990"
  },
  {
    "text": "Are the hyperparameter results\nranked the same in simulation",
    "start": "2981990",
    "end": "2987830"
  },
  {
    "text": "as they are on hardware? So we did a test of that. And unfortunately, I\ncan't say that there",
    "start": "2987830",
    "end": "2994790"
  },
  {
    "text": "was, for all simulators,\na very clear signal that I can use\nsimulation as a way",
    "start": "2994790",
    "end": "3003070"
  },
  {
    "text": "to test-- like pick\nhyperparameters that then also work the best on hardware.",
    "start": "3003070",
    "end": "3008470"
  },
  {
    "text": "But for specifically the\nImageNav task, which is I mean, basically you\nget a goal image",
    "start": "3008470",
    "end": "3015490"
  },
  {
    "text": "and you navigate to\nthat goal, you have to-- the agent has to navigate\nto that goal image.",
    "start": "3015490",
    "end": "3022870"
  },
  {
    "text": "And there we saw a very good\ncorrelation and transfer. And it kind of makes sense\nbecause it's a mostly basically,",
    "start": "3022870",
    "end": "3030880"
  },
  {
    "text": "the driving over the\nsurfaces was abstracted away. And it's mostly a visual,\nlike moving through space",
    "start": "3030880",
    "end": "3037700"
  },
  {
    "text": "and a visual task. And the simulator\nwas photorealistic, so the transfer was fairly\ngood from the policy.",
    "start": "3037700",
    "end": "3046520"
  },
  {
    "text": "So there, the ranking was\ngood, very well-correlated. But then for other\ntasks, the ranking wasn't very well-correlated.",
    "start": "3046520",
    "end": "3053340"
  },
  {
    "text": "So basically, what I'm\ntelling you is no, I'm sorry. And it's worse than just\ntrying to find one metric.",
    "start": "3053340",
    "end": "3059260"
  },
  {
    "text": "We can't even just say, oh, we\nhave these results in simulation",
    "start": "3059260",
    "end": "3064280"
  },
  {
    "text": "and they will automatically\ntransfer to hardware. Maybe time for\none more question.",
    "start": "3064280",
    "end": "3071720"
  },
  {
    "text": "Nothing on Zoom.  OK, well, then let's\nthank the speaker again.",
    "start": "3071720",
    "end": "3078270"
  },
  {
    "text": "Thank you. Thank you. [APPLAUSE] ",
    "start": "3078270",
    "end": "3086000"
  }
]