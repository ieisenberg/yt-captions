[
  {
    "text": "So I guess today we're\ngoing to do the last lecture on reinforcement learning. And I will probably\nspend like five minutes",
    "start": "5520",
    "end": "12240"
  },
  {
    "text": "to briefly wrap up\nthe whole course. But mostly, we're going to talk\nabout reinforcement learning.",
    "start": "12240",
    "end": "16500"
  },
  {
    "text": "So this is supposed to be a more\nkind of introductory lecture",
    "start": "18180",
    "end": "23700"
  },
  {
    "text": "on a little more advanced topics\nin reinforcement learning. So I won't talk about\na lot about details,",
    "start": "23700",
    "end": "29520"
  },
  {
    "text": "but mostly, I'm going to\ndefine some terms so that it's easier for you to\nkind of either take",
    "start": "29520",
    "end": "35400"
  },
  {
    "text": "another course on\nreinforcement learning or read some of the\nliteratures yourself. So I guess, last time\nwe have introduced",
    "start": "35400",
    "end": "42480"
  },
  {
    "text": "the basic concept of MDP,\nand Markov decision process. That's the main\nlanguage that people",
    "start": "42480",
    "end": "48540"
  },
  {
    "text": "use to think about\nreinforcement questions. I'm going to start\nby just reviewing some of the key ideas.",
    "start": "48540",
    "end": "54660"
  },
  {
    "text": "So recall that you have an\nMDP, Markov decision process.",
    "start": "54660",
    "end": "58440"
  },
  {
    "text": "It's described by a\nfew important concepts. So one thing is the state space.",
    "start": "61560",
    "end": "67740"
  },
  {
    "text": "You have to specify the state\nspace to specify the MDP. You have to specify\nthe action space.",
    "start": "67740",
    "end": "74160"
  },
  {
    "text": "And the MDP has this\ntransition probability, which is called PSa for every\ns and every a is in the--",
    "start": "74820",
    "end": "85140"
  },
  {
    "text": "s is a state. a is the action. For every s and a you have\nthis so-called transition",
    "start": "87660",
    "end": "92700"
  },
  {
    "text": "probability matrix,\ntransition probabilities, which is to describe what is the\nprobability that if you start",
    "start": "93960",
    "end": "101940"
  },
  {
    "text": "with State s and take action\na what is the probability to arrive at a new state?",
    "start": "101940",
    "end": "106440"
  },
  {
    "text": "And there is this so-called\ndiscount factor, gamma, and a reward function, R. So\nafter specifying these five",
    "start": "108600",
    "end": "115020"
  },
  {
    "text": "quantities, you get MDP. And we also talk about\na concept of a policy.",
    "start": "115020",
    "end": "119460"
  },
  {
    "text": "So a policy is\nsomething you are trying to learn from interacting with\nthe system, the environment.",
    "start": "120480",
    "end": "126780"
  },
  {
    "text": "You are trying to learn\nthis so-called policy, which is a function that maps\nfrom the state and action.",
    "start": "127740",
    "end": "133320"
  },
  {
    "text": "So this policy tells\nyou what you do, which action do you take when\nyou see a state S. So pi of S",
    "start": "133320",
    "end": "142200"
  },
  {
    "text": "is the action you take when\nyou are at the state s. And we also introduce\nthese two concept,",
    "start": "142200",
    "end": "147780"
  },
  {
    "text": "two type of value function. So the first type\nof value function is the value function\nof the state S.",
    "start": "148620",
    "end": "154380"
  },
  {
    "text": "So this is the value of the\nstate s under the policy pi. So this is the expected\nreward, expected",
    "start": "154380",
    "end": "164220"
  },
  {
    "text": "future payoff  ",
    "start": "164220",
    "end": "164880"
  },
  {
    "text": "of executing\nthe policy pi from state s.",
    "start": "170640",
    "end": "182640"
  },
  {
    "text": "So you keep taking action\nfrom the policy pi. You start with\nstate s and then you",
    "start": "182640",
    "end": "189600"
  },
  {
    "text": "compute what is the total\nfuture payoff in expectation. And that is V pi of s.",
    "start": "190200",
    "end": "196620"
  },
  {
    "text": "And we also discuss this\nso-called V star of s.",
    "start": "196620",
    "end": "201780"
  },
  {
    "text": "This is oblivious\nto the policy pi. This is asking what's the\nmaximum possible reward",
    "start": "202800",
    "end": "209580"
  },
  {
    "text": "you can get from starting\nfrom state s, right? So you maximize over\nall possible policies,",
    "start": "209580",
    "end": "214980"
  },
  {
    "text": "and you look at-- you maximize the V pi of s.",
    "start": "214980",
    "end": "220260"
  },
  {
    "text": "And the maximizer of this\nprocess, the arg max of this is the optimal policy\nyou care about.",
    "start": "220260",
    "end": "226500"
  },
  {
    "text": "You want to find out what\nis the optimal policy. So I think we probably\ndidn't have time",
    "start": "226500",
    "end": "232560"
  },
  {
    "text": "to define this\nformula last time. So the optimal policy pi star is\nthe so-called the arg max pi V5",
    "start": "232560",
    "end": "245400"
  },
  {
    "text": "S. And this is actually--",
    "start": "245400",
    "end": "249299"
  },
  {
    "text": "there's a unique pi\nstar that maximized the V pi s for every s just\nbecause the policy itself is",
    "start": "250500",
    "end": "257820"
  },
  {
    "text": "already a function of s, right? So you're finding a policy\nthat maps the function s to the action. You already can take different\nactions for different state s.",
    "start": "257820",
    "end": "267300"
  },
  {
    "text": "And this is one\nway to define this, and another way to define\nthis is the following. So this is another way to\ndefine optimal policies.",
    "start": "269940",
    "end": "277380"
  },
  {
    "text": "You say that this is the greedy\npolicy  ",
    "start": "277380",
    "end": "279960"
  },
  {
    "text": "with respect to V star. So this is an\nalternative definition",
    "start": "284940",
    "end": "290880"
  },
  {
    "text": "of the optimal policy,\nwhich is defined to be-- pi star of s is\ndefined to be the best",
    "start": "290880",
    "end": "297660"
  },
  {
    "text": "action you take such that you\nmaximize your future payoff. What's your future payoff?",
    "start": "297660",
    "end": "302700"
  },
  {
    "text": "The future payoff\nis equal to R of s plus the payoff you get from\nthe future steps, which is gamma",
    "start": "302700",
    "end": "309900"
  },
  {
    "text": "times this P s, a s prime.",
    "start": "309900",
    "end": "315660"
  },
  {
    "text": "This is sum over s prime. This is the probability of\ns prime after take action a, which is the variable here.",
    "start": "315660",
    "end": "322319"
  },
  {
    "text": "And then you times V\nstar s prime, right?",
    "start": "322920",
    "end": "328680"
  },
  {
    "text": "So this part is the\nexpected reward, the best reward you can get\nafter you take action a.",
    "start": "328680",
    "end": "337800"
  },
  {
    "text": "You take action a, you have some\nchance to arrive at s prime, and you multiply the best payoff\nyou can get after starting from",
    "start": "337800",
    "end": "345419"
  },
  {
    "text": "s prime. And so that's why this is the\nbest possible expected payoff",
    "start": "345420",
    "end": "350820"
  },
  {
    "text": "after you take action a. And this is the payoff\nyou get at state s. So the total thing\nis the best payoff,",
    "start": "350820",
    "end": "357060"
  },
  {
    "text": "including the current return,\nthe best future payoff if you take action a.",
    "start": "357060",
    "end": "362699"
  },
  {
    "text": "And you maximize over a,\nand that's the best policy.",
    "start": "362700",
    "end": "366720"
  },
  {
    "text": "That's the definition of\nthe optimal policy, right, because this is already\nthe optimal choice, you're already thinking\nabout the optimal choice",
    "start": "368100",
    "end": "374820"
  },
  {
    "text": "for all the future steps. And then you take\nthe action a that is the optimal for\nthis step, taking",
    "start": "374820",
    "end": "381660"
  },
  {
    "text": "into account the future steps. And that's the optimal\npolicy for the state s.",
    "start": "381660",
    "end": "385900"
  },
  {
    "text": "Any questions?",
    "start": "388080",
    "end": "388860"
  },
  {
    "text": "Sometimes this is the way to\nfind out the optimal policy because if you find\nout what's the V star, then you can find out\nthe optimal policy",
    "start": "395280",
    "end": "401759"
  },
  {
    "text": "because you just take the greedy\npolicy with respect to V star.",
    "start": "401760",
    "end": "405600"
  },
  {
    "text": "And we also introduced\nthis important concept of Bellman equation, which\nis the main tool that we",
    "start": "409020",
    "end": "414060"
  },
  {
    "text": "use to find out, to\ncompute V pi and V star. So for V pi, if\nyou are given pi,",
    "start": "414060",
    "end": "420060"
  },
  {
    "text": "then the Bellman equation\nfor V pi is equal--",
    "start": "420060",
    "end": "427139"
  },
  {
    "text": "is this. So V pi of s is equal to Rs\nplus gamma times this,  ",
    "start": "427140",
    "end": "436380"
  },
  {
    "text": "right? So the Bellman equations you can\npretty much verify intuitively",
    "start": "446820",
    "end": "452880"
  },
  {
    "text": "yourself, right, because\nwhat is the reward, the payoff when you are at\nstate s, actaully policy pi.",
    "start": "452880",
    "end": "460320"
  },
  {
    "text": "You first look at what's the\ncurrent reward for this step, and then, OK, what's\nthe future reward? The future reward can be\ncomputed by considering",
    "start": "460320",
    "end": "468960"
  },
  {
    "text": "all different possible outcomes\nof executing pi of s, right? So if you apply\npi of S, then you",
    "start": "468960",
    "end": "476220"
  },
  {
    "text": "have some probability\nto arrive at s prime, and you multiply\nthat probability with the payoff you get after\narriving at state s prime.",
    "start": "476220",
    "end": "486060"
  },
  {
    "text": "OK. And one of the\nimportant thing here was that this is actually\nlinear in the variable V pi S,",
    "start": "489180",
    "end": "499139"
  },
  {
    "text": "so linear in a variable\nV pi 1 up to V pi--",
    "start": "499920",
    "end": "509100"
  },
  {
    "text": "I think I used m as the\nnumber of total states. So this is a linear\nequation of these variables.",
    "start": "511500",
    "end": "519120"
  },
  {
    "text": "So you can solve\nthe linear equation by any linear system solver. And we also introduced\nthis Bellman equation",
    "start": "519120",
    "end": "527820"
  },
  {
    "text": "for V star, which is\nof a familiar form. But the difference is that\nnow you don't have the pi.",
    "start": "527820",
    "end": "534240"
  },
  {
    "text": "You have to maximize the action. So you have Rs plus the max.",
    "start": "534240",
    "end": "541200"
  },
  {
    "text": "You take the best\npossible action that",
    "start": "541200",
    "end": "546240"
  },
  {
    "text": "maximize the future reward.",
    "start": "546240",
    "end": "548700"
  },
  {
    "text": "Right, so now this is not a\nlinear system of equations in terms of V star, but\nyou can use the so-called--",
    "start": "556260",
    "end": "563700"
  },
  {
    "text": "the algorithm we\nintroduced last time was this iterative algorithm\nthat you do the Bellman update",
    "start": "565200",
    "end": "570540"
  },
  {
    "text": "iteratively. You can do this iterative\nalgorithm to find out V star.",
    "start": "570540",
    "end": "575040"
  },
  {
    "text": "Any questions so far? This is basically a review\nof the last lecture.",
    "start": "580380",
    "end": "584400"
  },
  {
    "text": "So OK, so so far,\nin the last lecture",
    "start": "589140",
    "end": "596280"
  },
  {
    "text": "we have deal with\nknown dynamics. So in all of this, so we\nhave described the algorithm,",
    "start": "596280",
    "end": "602460"
  },
  {
    "text": "so and so forth. Everything was\nunder the assumption that the algorithm-- so\nbasically the algorithm",
    "start": "602460",
    "end": "609240"
  },
  {
    "text": "to solve the V pi\nor the algorithm to solve V star, the\niterative algorithm. I guess I'm not sure whether you\nstill remember the algorithm.",
    "start": "609240",
    "end": "615300"
  },
  {
    "text": "The algorithm here was\njust something very simple. So you take a loop, and you\njust say I'm going to update V--",
    "start": "615300",
    "end": "622080"
  },
  {
    "text": "I have a working memory\nfor V. I update V like V S,",
    "start": "622080",
    "end": "631140"
  },
  {
    "text": "update it to be something\nlike RS plus max.",
    "start": "631140",
    "end": "635100"
  },
  {
    "text": "You just compute the\nright-hand side and then with the V plugging\nin here, and then",
    "start": "643200",
    "end": "648720"
  },
  {
    "text": "you update the left-hand\nside with itself. So this is called\nvalue iteration.",
    "start": "648720",
    "end": "653040"
  },
  {
    "text": "The algorithm is\ncalled value iteration. So both the value\niteration algorithm",
    "start": "654260",
    "end": "660300"
  },
  {
    "text": "and the algorithm that\nsolve the linear system equation, in this case,\nboth of these two algorithms",
    "start": "660300",
    "end": "665940"
  },
  {
    "text": "are assuming you have\na known dynamics. So the PSA is known.",
    "start": "665940",
    "end": "673260"
  },
  {
    "text": "All the family of PSA\nis known, like in both",
    "start": "674940",
    "end": "681120"
  },
  {
    "text": "of these two algorithms because\nyou have to compute PSA, right? So in our language\nyou are saying",
    "start": "681120",
    "end": "689700"
  },
  {
    "text": "that this means that you have\nthe known transmission dynamics or the known environment.",
    "start": "689700",
    "end": "696180"
  },
  {
    "text": "That's how people refer\nto these kind of settings. But in reality, what happens\nis that this PSA is not",
    "start": "696180",
    "end": "703380"
  },
  {
    "text": "known anymore. So for example,\nsometimes you do know it. For example, suppose\nyou consider this is",
    "start": "703380",
    "end": "710340"
  },
  {
    "text": "a game like playing Go, right? You are playing Go or chess. So you do know the\nrules of the game.",
    "start": "710340",
    "end": "716160"
  },
  {
    "text": "You know what happens\nif you play an action a what will happen next, right? So you're going to move each\nof the piece in some way.",
    "start": "716160",
    "end": "723420"
  },
  {
    "text": "So you know the rules, then\nin those cases P s, a's good. But in many other cases,\nthe transmission dynamics",
    "start": "723420",
    "end": "729720"
  },
  {
    "text": "is not known, right,\nso for example, when you control\nthe robot, right? So in some cases you\nprobably know a little bit",
    "start": "729720",
    "end": "735240"
  },
  {
    "text": "about if you control this,\nthe robot would move forward. But sometimes, if you're\ndoing the low level control, you are changing the\njoint of the robotic arm",
    "start": "735240",
    "end": "742920"
  },
  {
    "text": "or the robotic hand. You don't exactly know how\nthe everything moves exactly.",
    "start": "742920",
    "end": "750060"
  },
  {
    "text": "You probably have\nsome rough sense, but you are never able\nto model them exactly. Actually, this is a challenge.",
    "start": "750060",
    "end": "755640"
  },
  {
    "text": "So this is actually the\nreason why now people are using more and more\nlearning techniques.",
    "start": "755640",
    "end": "761160"
  },
  {
    "text": "So I think in the early days,\nI think, for example, there was a company called\nBoston Dynamics.",
    "start": "761160",
    "end": "766320"
  },
  {
    "text": "So what they do is that they\nbasically just use rule-based-- so basically they\nbuild this P s, a. They try to figure\nout from physics",
    "start": "766320",
    "end": "773160"
  },
  {
    "text": "what exactly this dynamical\nsystems should look like. And then they build\ntheir policies",
    "start": "773160",
    "end": "778980"
  },
  {
    "text": "based on that dynamic ecosystem. But these days, I think\npeople are at least",
    "start": "778980",
    "end": "785220"
  },
  {
    "text": "trying to apply more learning\ntechniques because there",
    "start": "786060",
    "end": "791279"
  },
  {
    "text": "is no way you can figure\nout exactly what PSA is just from the physical rules.",
    "start": "791280",
    "end": "796620"
  },
  {
    "text": "You have to use some kind\nof learning-based technique. Sometimes also it involves\ninteraction with environments.",
    "start": "796620",
    "end": "801959"
  },
  {
    "text": "For example, suppose\nI have a robot that is moving on this carpet. Then the speed would be\ndifferent from the robot moving",
    "start": "801960",
    "end": "808920"
  },
  {
    "text": "on the hardwood floor, right? So the other\nenvironments also part--",
    "start": "808920",
    "end": "814500"
  },
  {
    "text": "so like the floor,\nthe other thing is also part of the environment. So then you can ever model\neverything perfectly,",
    "start": "814500",
    "end": "819959"
  },
  {
    "text": "where you would never know the\ntexture of the floor exactly. So that's why we have to learn\nthe dynamics to some extent.",
    "start": "819960",
    "end": "827100"
  },
  {
    "text": "So I think, in\nsome sense, that's the real problem in\nreinforcement learning",
    "start": "827940",
    "end": "835320"
  },
  {
    "text": "where the dynamics is unknown. So when you don't have the\ndynamics, right, so what can",
    "start": "835320",
    "end": "840480"
  },
  {
    "text": "you do? So you have to know\nsomething, right? You have to somehow have some\ninformation about dynamics.",
    "start": "840480",
    "end": "847440"
  },
  {
    "text": "So the typical assumption,\nwhich is that-- so the P s, a this is unknown.",
    "start": "847440",
    "end": "854280"
  },
  {
    "text": "But you have the-- but given a state\ns and the action a,",
    "start": "855840",
    "end": "865920"
  },
  {
    "text": "we can sample s prime from\nthis transition dynamics.",
    "start": "868980",
    "end": "876300"
  },
  {
    "text": "Basically, we can just try\nthe robot in the real world. We can just say I'm going to try\nmy action a in the real world",
    "start": "876300",
    "end": "882959"
  },
  {
    "text": "and see what the next\ns prime is, right? The s prime will involve\na level of stocasticity. There's some\nrandomness, but you're",
    "start": "882960",
    "end": "888540"
  },
  {
    "text": "going to observe one random\nsample from this transition dynamics.",
    "start": "888540",
    "end": "892680"
  },
  {
    "text": "So that's why when people\ncall sample complexity means that how many times you have\nto try this, for how many state",
    "start": "895800",
    "end": "901920"
  },
  {
    "text": "and action you have to\ntry to see s prime by just try it in the real world.",
    "start": "901920",
    "end": "906600"
  },
  {
    "text": "So that's how people\ngenerally learn. So we learn by interacting\nwith the environment, by trying all these actions.",
    "start": "908880",
    "end": "914700"
  },
  {
    "text": "And you somehow learn\nthe dynamics in some way. So that's the basic assumption. And then there are two\ntypes of algorithms",
    "start": "916200",
    "end": "923460"
  },
  {
    "text": "in reinforcement learning\nthat are the most popular. I think most of the algorithm\ncan be either categorized",
    "start": "923460",
    "end": "931740"
  },
  {
    "text": "into one of these groups. So one type of algorithm\nis called model-based RL.",
    "start": "931740",
    "end": "936840"
  },
  {
    "text": "So here the model means\nthe dynamical model or the transition dynamics.",
    "start": "940740",
    "end": "946500"
  },
  {
    "text": "So as the name\nsomewhat suggests, basically, model-based RL\nmeans that you explicitly",
    "start": "946500",
    "end": "952080"
  },
  {
    "text": "learn the transition dynamics.",
    "start": "956280",
    "end": "959400"
  },
  {
    "text": "How do you learn it? There are multiple variants. It depends on the situation. It depends on how\ncomplex a dynamic is.",
    "start": "963000",
    "end": "969180"
  },
  {
    "text": "But the transition dynamics are\nthe transition probabilities.",
    "start": "969180",
    "end": "976080"
  },
  {
    "text": "I guess they mean exactly\nthe same thing for-- they probably just always\nmean exactly the same thing.",
    "start": "976080",
    "end": "981660"
  },
  {
    "text": "But sometimes people\nhave different terms. So basically, you learn\nthis PSA explicitly.",
    "start": "982440",
    "end": "987900"
  },
  {
    "text": "You build a model to\ndescribe this PSA, and you learn this\nmodel from the samples.",
    "start": "987900",
    "end": "995819"
  },
  {
    "text": "The samples are the\ndata you learn from, and then you build\nsome approximate PSA",
    "start": "996420",
    "end": "1002240"
  },
  {
    "text": "from the samples. And that's the typical\ntype of table types",
    "start": "1002240",
    "end": "1013280"
  },
  {
    "text": "of model-based algorithm,\nso from samples.",
    "start": "1013280",
    "end": "1018320"
  },
  {
    "text": "And there is another\ntype of algorithm, which is called model-free RL.",
    "start": "1020840",
    "end": "1023900"
  },
  {
    "text": "I don't think there is really\na precise definition of any of this, but in some\nsense, the model-free RL,",
    "start": "1027440",
    "end": "1033439"
  },
  {
    "text": "I would just say, is a\nnegation of this, right? So you don't explicitly learn\na transition probability.",
    "start": "1033440",
    "end": "1039799"
  },
  {
    "text": "So just the-- doesn't\nlearn the transition.",
    "start": "1040520",
    "end": "1050180"
  },
  {
    "text": "So it sounds a little bit like--",
    "start": "1053600",
    "end": "1055940"
  },
  {
    "text": "if you don't have any\ncontext, this sounds might be a little bit\ntricky because how come you don't\nlearn the dynamics",
    "start": "1058760",
    "end": "1064460"
  },
  {
    "text": "but still learn the policy? So it's possible. For example, sometimes\nyou can just directly optimize this without\nlearning the dynamics, right?",
    "start": "1064460",
    "end": "1073340"
  },
  {
    "text": "So you can probably\njust use some like a--",
    "start": "1073340",
    "end": "1078500"
  },
  {
    "text": "so there are ways to not\nlearn explicitly the dynamics.",
    "start": "1078500",
    "end": "1085520"
  },
  {
    "text": "Of course, eventually,\nany algorithm needs to somehow have some\nunderstanding about dynamics",
    "start": "1085520",
    "end": "1090980"
  },
  {
    "text": "internally in some sense. But you don't have to\nexplicitly build one, right?",
    "start": "1090980",
    "end": "1096140"
  },
  {
    "text": "So for example,\none possible option is just that you\noptimize this function V pi s over the policy pi.",
    "start": "1096140",
    "end": "1102680"
  },
  {
    "text": "Suppose you can somehow take\ngradient descent over a policy pi, then you can avoid\ndealing with the dynamics",
    "start": "1102680",
    "end": "1111860"
  },
  {
    "text": "where you don't use\nthe Bellman equation. You just somehow\ncompute the derivative",
    "start": "1111860",
    "end": "1117020"
  },
  {
    "text": "of this with respect to pi. So some of the buzzwords are--",
    "start": "1117020",
    "end": "1121760"
  },
  {
    "text": "some of the\nalgorithms, Q-learning is one type of algorithm. And another type of algorithm\nis called policy gradients.",
    "start": "1122660",
    "end": "1129020"
  },
  {
    "text": "So I don't think we\nhave time to discuss any of this model-free algorithm. I just want to write them\ndown here, just the buzzword,",
    "start": "1132980",
    "end": "1140120"
  },
  {
    "text": "so if you happen to\ncame across them, then you know they are\nmodel-free algorithm.",
    "start": "1140120",
    "end": "1146120"
  },
  {
    "text": "In some of the quarters\nwe do cover this. Some of the quarters have\none or two more lectures,",
    "start": "1146780",
    "end": "1151760"
  },
  {
    "text": "depending on how many\nholidays there are.",
    "start": "1152780",
    "end": "1154220"
  },
  {
    "text": "And another some of the\ndefinition of terms people",
    "start": "1158600",
    "end": "1164059"
  },
  {
    "text": "use here is that, so there\nis something called, when we say tabular case,\nor the tabular RL,",
    "start": "1164060",
    "end": "1171440"
  },
  {
    "text": "this means that\nyou have discrete S space, discrete action,\nand state space.",
    "start": "1171440",
    "end": "1181640"
  },
  {
    "text": "So in other words, the size\nof the state space is finite.",
    "start": "1187040",
    "end": "1194120"
  },
  {
    "text": "And the size of the action\nspace is also finite. And actually, implicitly\nyou are, in some sense,",
    "start": "1195800",
    "end": "1201620"
  },
  {
    "text": "what is finite or\nnot probably is not the most important thing. The real important thing is that\nthe state and the action space",
    "start": "1202160",
    "end": "1207500"
  },
  {
    "text": "are not too big. They are not like a\nbuilding or something. They are something\nthat is reasonable.",
    "start": "1207500",
    "end": "1212419"
  },
  {
    "text": "In the last lecture, we are\nbasically assuming this. We assume that the state\nspace has mentries, right?",
    "start": "1213140",
    "end": "1220760"
  },
  {
    "text": "And we didn't talk that\nmuch about action space, but we implicitly say that. Actually, all the algorithm\nrequired the action space",
    "start": "1220760",
    "end": "1226700"
  },
  {
    "text": "to be somewhat finite. And you can see, for example,\nthis linear system solver",
    "start": "1226700",
    "end": "1235039"
  },
  {
    "text": "algorithm, if you want to solve\nthe linear system equations, what are the variables? The variables are the V\npi 1 up to V pi m, where",
    "start": "1235040",
    "end": "1240620"
  },
  {
    "text": "m is the number of states. So if m is infinite, you\ncannot solve this linear system of equations, where you\nhave infinite variables.",
    "start": "1240620",
    "end": "1247040"
  },
  {
    "text": "Even if m is not infinite, even\nif m is something super big, say, exponentially big or\nsomething like a billion,",
    "start": "1247040",
    "end": "1254600"
  },
  {
    "text": "then you cannot really\nafford time to solve this set of equations. So in some sense, the\nmost important thing",
    "start": "1254600",
    "end": "1261440"
  },
  {
    "text": "is that you really\ndon't want to have-- if you are a tabular\ncase, you are implicitly assuming that the state\nspace is not huge.",
    "start": "1261440",
    "end": "1267980"
  },
  {
    "text": "But as you can see, sometimes\nthe state spaces just have to be infinite or very\nbig because the state space",
    "start": "1269540",
    "end": "1276800"
  },
  {
    "text": "is continuous, right? So that's another case where\nyou have continuous state space.",
    "start": "1276800",
    "end": "1283700"
  },
  {
    "text": "Sometimes you have continuous\naction space as well. So for example, the state\nspace is something like Rd.",
    "start": "1287180",
    "end": "1293060"
  },
  {
    "text": "So you have a d dimensional\nvector to describe the state. And typically, the action space\nis smaller than a state space.",
    "start": "1294020",
    "end": "1300140"
  },
  {
    "text": "Maybe the action space\nis something like Rk, where k is smaller than d. Sometimes d can be like",
    "start": "1300920",
    "end": "1306139"
  },
  {
    "text": "even more than 100. Action space typically,\nif it's continuous, then k probably would be",
    "start": "1307820",
    "end": "1312380"
  },
  {
    "text": "And sometimes you have\nthe combination, right? So you have continuous state\nspace but finite action space.",
    "start": "1314900",
    "end": "1320000"
  },
  {
    "text": "That's also possible. For example, one\ntypical case where",
    "start": "1320000",
    "end": "1326360"
  },
  {
    "text": "you have continuous state\nspace but finite action space is Atari game. You play this Atari\ngame, and the state space",
    "start": "1326360",
    "end": "1332840"
  },
  {
    "text": "is this pixel space where\nyou see the pixels that",
    "start": "1332840",
    "end": "1338779"
  },
  {
    "text": "is shown to you. And the actions are\nactually finite. You just have a few\nbuttons and maybe",
    "start": "1338780",
    "end": "1343640"
  },
  {
    "text": "some kind of handle\nyou can choose to play. So the action space\nis somewhat finite.",
    "start": "1344180",
    "end": "1350059"
  },
  {
    "text": "All right, so these are\njust some terms just in case they are useful when you\nread some of the other books",
    "start": "1354680",
    "end": "1362960"
  },
  {
    "text": "or literatures. OK, so then in the\nnext 20 minutes,",
    "start": "1362960",
    "end": "1368960"
  },
  {
    "text": "I'm going to discuss how\ndo you do the model-- maybe in the next 30\nminutes-- how do you",
    "start": "1368960",
    "end": "1374300"
  },
  {
    "text": "do the model-based URL\nfor the tabular case? So basically, you can see\nthat they are model-based,",
    "start": "1374300",
    "end": "1380360"
  },
  {
    "text": "model-free, and\ntabular continuous, where it's like you have\nlike four combinations. And we're going to do the\nmodel-based plus tabular.",
    "start": "1380360",
    "end": "1387620"
  },
  {
    "text": "And this is actually\nnot very hard. So what you really want to\ndo here is that basically",
    "start": "1389540",
    "end": "1399020"
  },
  {
    "text": "model-based, tabular-- you want to learn an\nexplicit model that's",
    "start": "1400820",
    "end": "1409280"
  },
  {
    "text": "kind of somewhat similar\nto true model PSA. So what you have is that\nyou have a collection.",
    "start": "1409280",
    "end": "1415880"
  },
  {
    "text": "So suppose we have a\ncollection of trajectories.",
    "start": "1416960",
    "end": "1424220"
  },
  {
    "text": "I forgot whether I\ndefined trajectories. By trajectories, I just mean\nyour sequence of state actions.",
    "start": "1428360",
    "end": "1433100"
  },
  {
    "text": "So suppose you say you\nhave some state s0.",
    "start": "1433940",
    "end": "1438919"
  },
  {
    "text": "Maybe let's say  ",
    "start": "1440960",
    "end": "1441980"
  },
  {
    "text": "we\nstart from state s0, and you take some action,\nmaybe a0, and you arrive at s1.",
    "start": "1447980",
    "end": "1454880"
  },
  {
    "text": "And you take some action a2, a1,\nand you arrive at S2, so and so",
    "start": "1454880",
    "end": "1460220"
  },
  {
    "text": "forth. So this is the one trajectory,\na sequence of state actions,",
    "start": "1460220",
    "end": "1465380"
  },
  {
    "text": "right? So so far I didn't really tell\nyou how I got the structure,",
    "start": "1465380",
    "end": "1470900"
  },
  {
    "text": "but I'm going to\ntalk about how do you learn the transition\nprobabilities from some given",
    "start": "1470900",
    "end": "1477140"
  },
  {
    "text": "trajectories. So suppose I give\none trajectory, and often you need more\nthan one trajectory. So then you say, after\nI take a bunch of steps,",
    "start": "1477140",
    "end": "1484940"
  },
  {
    "text": "maybe I take two steps. I reset. So I reset, and then I\nget another trajectory.",
    "start": "1484940",
    "end": "1490640"
  },
  {
    "text": "Maybe let's call this,\ntrajectory s01, a01. I have to superscript\njust to indicate that this",
    "start": "1491180",
    "end": "1498560"
  },
  {
    "text": "is the first trajectory. And then I restart. I get a new initial state,\nand I call this s02,",
    "start": "1498560",
    "end": "1504080"
  },
  {
    "text": "and I apply a02, s1 2,\na1 2, so on and so forth.",
    "start": "1505220",
    "end": "1517520"
  },
  {
    "text": "And maybe I can get more. I get a bunch of trajectories. Here, the subscript\nis indexing the time,",
    "start": "1517520",
    "end": "1523880"
  },
  {
    "text": "and the superscript is indexing\nwhich trajectory you are in. And then how do you estimate\nthe transition dynamics?",
    "start": "1523880",
    "end": "1531200"
  },
  {
    "text": "Recall that all of\nthese state and actions are discrete variables\nbecause I'm in a tabular case.",
    "start": "1531200",
    "end": "1535280"
  },
  {
    "text": "So basically, I just have to\nestimate P s,a as primary. What's the chance to start\nwith S and take action a",
    "start": "1536300",
    "end": "1544880"
  },
  {
    "text": "and arrive at s prime? So this is, in some sense, the\nproblem is the same as some",
    "start": "1544880",
    "end": "1553520"
  },
  {
    "text": "of this-- I think we discussed this\ngenerative learning algorithm, where we have this\nevent model, where",
    "start": "1553520",
    "end": "1559220"
  },
  {
    "text": "everything is counting-based. So basically, you can\ncompute a maximum likelihood",
    "start": "1559220",
    "end": "1564559"
  },
  {
    "text": "of this transition. You view this as a\nparameter, right? So this is something\nyou want to learn. But this is a parameter.",
    "start": "1565280",
    "end": "1571400"
  },
  {
    "text": "And then you try to find\nout the maximum likelihood estimate for this parameter. And it turns out that\nit's just-- as usual,",
    "start": "1572360",
    "end": "1580279"
  },
  {
    "text": "it's the most natural choice. Basically, you just count the\nfrequency to see this, right?",
    "start": "1580280",
    "end": "1587600"
  },
  {
    "text": "You say, in the denominator, you\nsay, I look at how many times--",
    "start": "1587600",
    "end": "1592580"
  },
  {
    "text": "let's say we took\naction a at state s..",
    "start": "1599060",
    "end": "1605420"
  },
  {
    "text": "So you basically\nlook at all the cases where you take\naction a and state s. And then in the numerator\nyou count how many times",
    "start": "1607280",
    "end": "1614300"
  },
  {
    "text": "we took action a at--",
    "start": "1617120",
    "end": "1623420"
  },
  {
    "text": "I guess maybe I'm a\nlittle too wordy here. So basically, the\nnumber of times",
    "start": "1623420",
    "end": "1629179"
  },
  {
    "text": "if you take action a at state\ns, and you arrive at s prime.",
    "start": "1629180",
    "end": "1636860"
  },
  {
    "text": "So the denominator\nis the total number of times you take the\naction a at state s. And the numerator is how many--\namong all of these occurrences",
    "start": "1636860",
    "end": "1645380"
  },
  {
    "text": "of s and a, how many times\nindeed arrive at s prime.",
    "start": "1645380",
    "end": "1651380"
  },
  {
    "text": "And then that's your\ntransition probability. That's your estimated\ntransition probability. This is an estimate for P s,a.",
    "start": "1651380",
    "end": "1660020"
  },
  {
    "text": "Right, so any questions so far?",
    "start": "1663140",
    "end": "1666440"
  },
  {
    "text": "And once you have\nthis, it's just a counting-based algorithm.",
    "start": "1674120",
    "end": "1679400"
  },
  {
    "text": "You just count how many\nfractions you really arrive at s prime. Among all the s,a, how many\nfraction of those really arrive",
    "start": "1679400",
    "end": "1687620"
  },
  {
    "text": "at s prime? And the empirical\nfrequencies is your estimate for the transition probability.",
    "start": "1687620",
    "end": "1693380"
  },
  {
    "text": "Am I using the right thing? I should use the black color. OK, and once you have this\ntool to estimate the transition",
    "start": "1701060",
    "end": "1709940"
  },
  {
    "text": "probability, then you can have\na model-based RL algorithm.",
    "start": "1709940",
    "end": "1714679"
  },
  {
    "text": "I'll still use the\nred if it's OK. The black one doesn't\nseems to be very good.",
    "start": "1719480",
    "end": "1723980"
  },
  {
    "text": "So the model-based RL algorithm\nis doing the following. So it's pretty intuitive.",
    "start": "1724760",
    "end": "1731780"
  },
  {
    "text": "So first of all, you initialize\nsome policy pi, maybe randomly,",
    "start": "1731780",
    "end": "1738860"
  },
  {
    "text": "let's say. And then you have some data set.",
    "start": "1738860",
    "end": "1746600"
  },
  {
    "text": "Initially, you have no data. So the data set d is empty. I'm just defining\nnotation basically.",
    "start": "1746600",
    "end": "1752840"
  },
  {
    "text": "I'm going to use this\ndata set d in some way. And then what you really do is\nthat you say, I'm going to--",
    "start": "1752840",
    "end": "1761540"
  },
  {
    "text": "I have two steps. The first step is that\nI'm going to estimate. I'm going to collect some data.",
    "start": "1764000",
    "end": "1771680"
  },
  {
    "text": "So collect data by\nexecuting policy pi.",
    "start": "1772220",
    "end": "1782600"
  },
  {
    "text": "So if you execute policy\npi in a real environment, you're going to get\nsome samples, right? That's our assumption.",
    "start": "1788000",
    "end": "1794299"
  },
  {
    "text": "Our assumption is that you\nare able to get samples from the real environment. You don't know the P s,a, but\nyou can get samples from the P",
    "start": "1794300",
    "end": "1800600"
  },
  {
    "text": "s,a. So you actually have policy\npi to get your environment. So basically, what\nyou do is you get a family of the environment.",
    "start": "1801260",
    "end": "1807260"
  },
  {
    "text": "Sorry, you execute your\npolicy pi to get some samples. And the samples who got, let's\nsay, they are denoted by s01,",
    "start": "1807260",
    "end": "1815360"
  },
  {
    "text": "something like\nand you apply a01. And then you get s1",
    "start": "1817820",
    "end": "1823820"
  },
  {
    "text": "And you have s1, s02, right?",
    "start": "1824600",
    "end": "1829820"
  },
  {
    "text": "This is the same\nset of samples here. So you get some\nsamples, and then you",
    "start": "1829820",
    "end": "1836180"
  },
  {
    "text": "add all of these\ntrajectories at--",
    "start": "1836180",
    "end": "1841140"
  },
  {
    "text": "the trajectories to D. So D\nis kind of like a set of data.",
    "start": "1841700",
    "end": "1848899"
  },
  {
    "text": "And then, I guess I'm using the\nboard space in a awkward way.",
    "start": "1850340",
    "end": "1857779"
  },
  {
    "text": "How do I-- I think I'll just have to erase.",
    "start": "1859520",
    "end": "1866700"
  },
  {
    "text": "So let's see, I'm going to--",
    "start": "1870260",
    "end": "1871820"
  },
  {
    "text": "so you've got this data,\nthis kind of data, right?",
    "start": "1881060",
    "end": "1884540"
  },
  {
    "text": "So this is a set of data\nthat is like this, OK? So and then you\nadd them to D. So",
    "start": "1891920",
    "end": "1904700"
  },
  {
    "text": "on the second step--\nthis is the first step, you collect some data. And then in the second\nstep you estimate",
    "start": "1904700",
    "end": "1917120"
  },
  {
    "text": "the P s, a using data in D, OK?",
    "start": "1917120",
    "end": "1927740"
  },
  {
    "text": "And let's still use P s,\na as the estimator, right? Suppose we get\nsome P s, a that is",
    "start": "1931340",
    "end": "1937340"
  },
  {
    "text": "supposed to be an estimate for\nthe real transmission dynamics. And then in the step\nc, so you can use,",
    "start": "1937340",
    "end": "1946940"
  },
  {
    "text": "for example, it's\nvalue iteration.",
    "start": "1946940",
    "end": "1950419"
  },
  {
    "text": "It could be also\npolicy iteration. I guess we didn't have\ntime to discuss policy iteration in the last lecture.",
    "start": "1955820",
    "end": "1961220"
  },
  {
    "text": "But if you are interested,\nyou can read lecture notes to see what's the\npolicy iteration.",
    "start": "1961220",
    "end": "1966440"
  },
  {
    "text": "But let's suppose you use\nvalue iteration to get",
    "start": "1966440",
    "end": "1971779"
  },
  {
    "text": "V star, the value function,\nfor the estimated P",
    "start": "1973160",
    "end": "1981940"
  },
  {
    "text": "s, a, the estimates dynamics.",
    "start": "1981940",
    "end": "1985399"
  },
  {
    "text": "So just pretend that\nthe estimated dynamics is the real dynamics, and then\nyou solve the best policy,",
    "start": "1988340",
    "end": "1993380"
  },
  {
    "text": "the optimal value function\nfor that dynamics. And then you take\npi star to be--",
    "start": "1995420",
    "end": "2002980"
  },
  {
    "text": "you take pi to be the optimal\npolicy  ",
    "start": "2002980",
    "end": "2010840"
  },
  {
    "text": "for the estimated",
    "start": "2015280",
    "end": "2018520"
  },
  {
    "text": "dynamics.",
    "start": "2022780",
    "end": "2024460"
  },
  {
    "text": "OK. So are we done? So it sounds like\nwe are done, right,",
    "start": "2030100",
    "end": "2036760"
  },
  {
    "text": "because we estimate some-- everything's simple. You collect some data. You estimate the\ndynamics, and then you",
    "start": "2036760",
    "end": "2044679"
  },
  {
    "text": "get the best policy of one\nof the estimated dynamics. But actually, what\nyou really have to do",
    "start": "2044680",
    "end": "2051700"
  },
  {
    "text": "is you have to have another\nloop, outer loop that",
    "start": "2051700",
    "end": "2056740"
  },
  {
    "text": "repeat this process. So what you really need to do\nis you have to take a loop.",
    "start": "2056740",
    "end": "2060820"
  },
  {
    "text": "And so after you\nhave some policy pi,",
    "start": "2062140",
    "end": "2067659"
  },
  {
    "text": "you want to collect some more\ndata using the current policy pi. Initially, the\npolicy pi was random,",
    "start": "2067660",
    "end": "2073960"
  },
  {
    "text": "and you collect data\nfrom the random policy. And after you get\nsome policy pi, then you should take another\nloop to collect more data.",
    "start": "2073960",
    "end": "2081760"
  },
  {
    "text": "And then re-estimate\nyour transition dynamics, and then recompute your policy.",
    "start": "2081760",
    "end": "2087399"
  },
  {
    "text": "And then you keep doing this. You probably don't have\nto do a lot of loops, but you have to do\nsome iterations.",
    "start": "2087400",
    "end": "2092919"
  },
  {
    "text": "So the question is, why you\nneed this loop, outer loop?",
    "start": "2094420",
    "end": "2099460"
  },
  {
    "text": "Why we can't just go\nwith the first dynamics we have estimated it by? So if you ask me one dynamics,\nif it's accurate enough,",
    "start": "2099460",
    "end": "2105460"
  },
  {
    "text": "then why we cannot go with that? The reason is that\nin RL there is",
    "start": "2105460",
    "end": "2111040"
  },
  {
    "text": "this problem with this so-called\nexploration exploitation tradeoff, which I'm\ngoing to elaborate.",
    "start": "2111040",
    "end": "2117940"
  },
  {
    "text": "So the immediate problem\nis the following. So it's possible that\nin the first round, when you collect data, your\ndata is not very good.",
    "start": "2117940",
    "end": "2125260"
  },
  {
    "text": "It's very bad,\nlow-quality data, right? So for example, suppose you\nwant to control a robot.",
    "start": "2125260",
    "end": "2130960"
  },
  {
    "text": "And you initialize\na policy randomly. You just do some random-- you\njust push the random buttons",
    "start": "2130960",
    "end": "2137380"
  },
  {
    "text": "or control the robot\nin a random way. Then the data you collect are\nbasically just some kind of--",
    "start": "2137380",
    "end": "2142540"
  },
  {
    "text": "the robot is just wiggling\naround a little bit. It doesn't really move much. So the data you collect is\nactually very, very bad.",
    "start": "2143500",
    "end": "2149800"
  },
  {
    "text": "And then even you\nhave a lot of data, you see your data call\nis not good enough just because the robot\ndoesn't really do much.",
    "start": "2149800",
    "end": "2157480"
  },
  {
    "text": "And then your estimated\ndynamics is also not going to be good enough. And then your policy is also\nnot going to be good enough.",
    "start": "2157480",
    "end": "2166300"
  },
  {
    "text": "So what you really want is you\nwant to do this iteratively so that next round, when your\npolicy is reasonably OK,",
    "start": "2166300",
    "end": "2172120"
  },
  {
    "text": "you collect some\nhigher quality data. And then you do this\nagain, and then your policy",
    "start": "2172120",
    "end": "2178119"
  },
  {
    "text": "becomes even better. And then you get even\nbetter, higher quality data. So that's why you\nhave this loop.",
    "start": "2178120",
    "end": "2184119"
  },
  {
    "text": "Another example\nis the following. So suppose you have-- this is another example.",
    "start": "2184120",
    "end": "2189460"
  },
  {
    "text": "Suppose, for example, let's\nsay, I guess you probably",
    "start": "2189460",
    "end": "2196660"
  },
  {
    "text": "all-- some of you have used\nthis kind of automatic robot to do the cleaning\nfor the house,",
    "start": "2196660",
    "end": "2203020"
  },
  {
    "text": "where you have this small\nvacuum, a robotic vacuum that",
    "start": "2203020",
    "end": "2209980"
  },
  {
    "text": "can move in your house. So if you have used that,\nI think how it works is that it first\nexplores your whole room",
    "start": "2209980",
    "end": "2216760"
  },
  {
    "text": "and to try to figure out\nwhat your room looks like. And then the next round is\ngoing to take some trajectories",
    "start": "2216760",
    "end": "2225100"
  },
  {
    "text": "to clean your room so that it\ncovers every part, right, so something like that.",
    "start": "2225100",
    "end": "2230020"
  },
  {
    "text": "However, but if you think about\nthis where suppose you have, for example, say\nsomething like a big room.",
    "start": "2230920",
    "end": "2236740"
  },
  {
    "text": "And then you have a small\nroom adjacent to it. So you have some robot that\ntries to clean the room",
    "start": "2236740",
    "end": "2244060"
  },
  {
    "text": "and navigate through\nthe room, right? So what if at the beginning your\nrobot only goes to this part,",
    "start": "2244060",
    "end": "2249220"
  },
  {
    "text": "right? So in the first run, your policy\njust only look at this room. Then your dynamic model\nwill only be able to--",
    "start": "2249220",
    "end": "2256660"
  },
  {
    "text": "it's only accurate\nfor this room, right? You only know what's\nhappening in this room. What's the chairs? What's the stairs or\nother kind of like--",
    "start": "2256660",
    "end": "2264760"
  },
  {
    "text": "so far, so and so forth, right? But you don't know\nanything about this one.",
    "start": "2264760",
    "end": "2269980"
  },
  {
    "text": "So that's why you cannot-- so there's a typical situation\nwhere the quality of the data",
    "start": "2270580",
    "end": "2279100"
  },
  {
    "text": "is not good enough because\nyour quality of the data doesn't even cover\nsome part of the room. So then if you don't\ndo anything special,",
    "start": "2279100",
    "end": "2286240"
  },
  {
    "text": "then your robot wouldn't have\nincentive to go to this room because the robot doesn't\neven know the existence of this room in some sense.",
    "start": "2286240",
    "end": "2292420"
  },
  {
    "text": "So this is actually an\neven more challenging case because here, even\nyou just do this loop,",
    "start": "2293260",
    "end": "2298420"
  },
  {
    "text": "you wouldn't be able to\nnecessarily-- wouldn't be able to figure out the small\nroom because at the beginning",
    "start": "2298420",
    "end": "2305260"
  },
  {
    "text": "you just only see\nthe large room, and you never look\nat a small room. And then you figure\nout the optimal policy",
    "start": "2305260",
    "end": "2311260"
  },
  {
    "text": "to clean a large\nroom, and you still don't know the existence\nof the small room. And you just keep doing this.",
    "start": "2311260",
    "end": "2316420"
  },
  {
    "text": "Eventually, just only\nclean the large room, and you just never know the\nexistence of the small room.",
    "start": "2316420",
    "end": "2321820"
  },
  {
    "text": "So in these kind of cases\nyou need even something more than this kind of\nalgorithm to be able to work.",
    "start": "2321820",
    "end": "2331119"
  },
  {
    "text": "And typically, this is a\nphenomenon called exploitation",
    "start": "2332500",
    "end": "2335800"
  },
  {
    "text": "versus exploration. So exploitation\nbasically means that you",
    "start": "2338320",
    "end": "2345520"
  },
  {
    "text": "believe your current\ntransition dynamics. You just strongly believe their\ncurrent transition dynamics",
    "start": "2345520",
    "end": "2350320"
  },
  {
    "text": "or your current understanding\nabout the world, the environment, right? And you just try to\nfind an optimal policy",
    "start": "2351220",
    "end": "2356500"
  },
  {
    "text": "for the current\nunderstanding of the world. And exploration\nmeans that you try",
    "start": "2356500",
    "end": "2362740"
  },
  {
    "text": "to explore different\nstrategies to see whether you miss anything in this world. So in this case, maybe\nyou missed the small room.",
    "start": "2362740",
    "end": "2369220"
  },
  {
    "text": "So you want to do\nexploration to figure out the existence of the small room.",
    "start": "2369220",
    "end": "2374440"
  },
  {
    "text": "And exploitation\nreally just means that you just basically do the\nbest thing for the current map.",
    "start": "2374440",
    "end": "2379420"
  },
  {
    "text": "So as you can see, you need some\nexploration to at least cover the entire world so you know the\nexistence of any other options.",
    "start": "2380560",
    "end": "2389260"
  },
  {
    "text": "So typically, if you really\nwant this kind of reinforcement learning algorithm,\nyou have to add some randomness in the\npolicy pi so that you can",
    "start": "2389260",
    "end": "2397240"
  },
  {
    "text": "have some exploration, right? So you don't want to just\nalways, in every round, you just always collect\ndata from the policy pi",
    "start": "2397240",
    "end": "2404140"
  },
  {
    "text": "that is optimal for the\ncurrent environment. You also want to\nhave some exploration",
    "start": "2404140",
    "end": "2409600"
  },
  {
    "text": "by adding some randomness. So basically, what you really do\nis that when you collect data, you add some actually\npi with some randomness.",
    "start": "2409600",
    "end": "2417820"
  },
  {
    "text": "So in this case, you\nhave some small chance to go into the small\nroom so that you can see the small room.",
    "start": "2421660",
    "end": "2426940"
  },
  {
    "text": "And then you figure out\nshould actually kind of clean that small room with\nsome kind of trajectory,",
    "start": "2426940",
    "end": "2433780"
  },
  {
    "text": "with some actions. So maybe another example is\nthat, for example, suppose you can figure out which\nrestaurant you want to go",
    "start": "2434320",
    "end": "2440859"
  },
  {
    "text": "in Palo Alto downtown, right? So suppose so far you\nknow two restaurants,",
    "start": "2441820",
    "end": "2447580"
  },
  {
    "text": "and you know one of them is\nbetter than the other for you. And the exploitation\nwould mean that you just",
    "start": "2447580",
    "end": "2453339"
  },
  {
    "text": "always go to the\nbetter restaurant for your taste, right? And then you just keep go to\nthat better restaurant for you.",
    "start": "2453340",
    "end": "2460600"
  },
  {
    "text": "However, you may also\nconsider some exploration because there are many other\nrestaurants in Palo Alto,",
    "start": "2460600",
    "end": "2466840"
  },
  {
    "text": "and you don't know the\nexistence of them even. Or you don't know whether\nthey are good or bad. Or you don't know their taste,\nwhether their taste fits you.",
    "start": "2466840",
    "end": "2474220"
  },
  {
    "text": "So exploration means\nthat you should try some of the other\nrestaurants even at a risk that those\nrestaurants are not as",
    "start": "2474220",
    "end": "2481120"
  },
  {
    "text": "good as the one you have known. But you want to try to\nexplore those restaurants.",
    "start": "2481120",
    "end": "2486880"
  },
  {
    "text": "And exploitation\nmeans that you just believe that OK,\nthere is nothing I should explore anymore. I believe in my\ncurrent evaluation",
    "start": "2486880",
    "end": "2493839"
  },
  {
    "text": "of all the restaurants. I just take the best one\nand keep going to that one.",
    "start": "2493840",
    "end": "2498339"
  },
  {
    "text": "And there is a tradeoff because\nif you keep doing exploration, then you keep trying all\ndifferent restaurants,",
    "start": "2499360",
    "end": "2505780"
  },
  {
    "text": "then inevitably you are going\nto find some restaurants that are not very good, right,\nand you will suffer.",
    "start": "2505780",
    "end": "2510820"
  },
  {
    "text": "You're going to say, OK,\nthis doesn't worth the money. You can have some bad\ndinners in some sense.",
    "start": "2510820",
    "end": "2518260"
  },
  {
    "text": "But on the other hand, if\nyou only do exploitation, you're going to miss\nother opportunities. So it really depends on--",
    "start": "2520600",
    "end": "2526720"
  },
  {
    "text": "and if you really go\ninto the RL literature, there are different ways to\ntrade off these two things",
    "start": "2526720",
    "end": "2532300"
  },
  {
    "text": "depending on how\nconfident you are with each of the other choices. You may decide to\nexploit more, or you",
    "start": "2532300",
    "end": "2539500"
  },
  {
    "text": "might decide to explore more. OK. But I think we are not\ngoing to have enough time",
    "start": "2539500",
    "end": "2546400"
  },
  {
    "text": "to go into the details. There's a huge literature here. Even just when you talk about\nthis going to restaurants,",
    "start": "2546400",
    "end": "2552940"
  },
  {
    "text": "which doesn't have\na sequential aspect. You just go to the restaurant\nand have lunch or dinner. There's no sequential decision.",
    "start": "2552940",
    "end": "2559840"
  },
  {
    "text": "You still have to think\nabout the exploitation and the exploration tradeoff. So to do it optimally, you\nhave to be somewhat careful.",
    "start": "2559840",
    "end": "2568900"
  },
  {
    "text": "OK, any questions so far? So [INAUDIBLE] when we\nare doing the exploration,",
    "start": "2572620",
    "end": "2585037"
  },
  {
    "text": "so we know that we are\ngetting better understanding about the world we are in or we\nare just trying to find random.",
    "start": "2585037",
    "end": "2593920"
  },
  {
    "text": "Yeah, so I guess, maybe\nyou're asking about how do we do exploration, right?",
    "start": "2593920",
    "end": "2599080"
  },
  {
    "text": "So what's the guiding principle\nfor doing exploration? And it seems to suggest\nthat one principle could",
    "start": "2599080",
    "end": "2604720"
  },
  {
    "text": "be that you only want\nto explore when you can collect more information. Yes, I think that basically it's\npretty much like what you said.",
    "start": "2604720",
    "end": "2614079"
  },
  {
    "text": "But sometimes random\nactually can serve that need. So if you just do\nrandom perturbation",
    "start": "2615100",
    "end": "2620620"
  },
  {
    "text": "of your current\noption, typically, you get a good amount\nof information. But of course, in\nsome other cases,",
    "start": "2620620",
    "end": "2627880"
  },
  {
    "text": "you have to directly go for\nthose in certain places.",
    "start": "2627880",
    "end": "2632500"
  },
  {
    "text": "Actually, you are exactly right. So for the tabular RL case,\ntypically, what people do",
    "start": "2633040",
    "end": "2638320"
  },
  {
    "text": "is not just random exploration. What people do is you say,\nyou take those actions that",
    "start": "2638320",
    "end": "2643420"
  },
  {
    "text": "are most uncertain for you. So suppose you have some\naction that you don't know what the outcome would be.",
    "start": "2643420",
    "end": "2648520"
  },
  {
    "text": "You have no idea what\noutcome would be, or you have very little idea. We have huge uncertainty about\nwhat the outcome would be.",
    "start": "2648520",
    "end": "2654579"
  },
  {
    "text": "You try those actions more\nas exploration strategy.",
    "start": "2654580",
    "end": "2658780"
  },
  {
    "text": "But for the continuous\nstate space, so I think it turns out that\nmost of the algorithm works",
    "start": "2659620",
    "end": "2668140"
  },
  {
    "text": "are a local randomized\nexploration. You don't try some crazy option. You try in your neighborhood.",
    "start": "2668140",
    "end": "2674620"
  },
  {
    "text": "I think that's\nactually probably make a lot of sense in\nthose cases where you have so many\ndifferent actions, right?",
    "start": "2674620",
    "end": "2681580"
  },
  {
    "text": "So for example, if you think\nabout your career planning, if you really\nhave-- suppose you--",
    "start": "2681580",
    "end": "2688540"
  },
  {
    "text": "in theory, you have really,\nreally a lot of actions you can try. Instead of being\nat Stanford, you",
    "start": "2688540",
    "end": "2694360"
  },
  {
    "text": "can try to be a professional\nsoccer player, right? So you can be a musician.",
    "start": "2694360",
    "end": "2700240"
  },
  {
    "text": "You can be many different--\nthere could be many things. And you are very uncertain\nabout some of those, probably.",
    "start": "2700240",
    "end": "2706180"
  },
  {
    "text": "So I wouldn't know\nif I try to be a musician what would happen.",
    "start": "2706180",
    "end": "2709539"
  },
  {
    "text": "But I think we have so\nmany actions, typically-- I wouldn't say-- this is\nnot 100% true, like when you",
    "start": "2711520",
    "end": "2719020"
  },
  {
    "text": "talk about technical details. But typically, we\nhave so many actions, I think somehow the algorithms-- most of the working algorithms\ntries to explore locally.",
    "start": "2719020",
    "end": "2727060"
  },
  {
    "text": "So for example, I'm a\nstudent here at Stanford, and then I try something\nsomewhat similar,",
    "start": "2727060",
    "end": "2732340"
  },
  {
    "text": "maybe going to intern at Google\nor maybe try in graduate school or something like that.",
    "start": "2732340",
    "end": "2737680"
  },
  {
    "text": "But I wouldn't try something\ncompletely different. No, we don't have a\nreally strong theory",
    "start": "2737680",
    "end": "2745240"
  },
  {
    "text": "to say exactly\nwhat you should do. So it's a mixture of some\ntheoretical explanation",
    "start": "2745240",
    "end": "2751540"
  },
  {
    "text": "and some empirical observations.",
    "start": "2751540",
    "end": "2754120"
  },
  {
    "text": "OK. So I guess, so I'm going\nto use the rest of the 40",
    "start": "2765040",
    "end": "2773560"
  },
  {
    "text": "minutes, or 30, 35 minutes to\ntalk about continuous state space. So I have talked about\nthe model-based RL, right?",
    "start": "2773560",
    "end": "2780400"
  },
  {
    "text": "So now I'm going to\ntalk about model-based plus continuous state space. And the idea is\npretty much similar.",
    "start": "2780400",
    "end": "2788440"
  },
  {
    "text": "It's just that you have to-- somehow you have to deal with\nthe continuous state space.",
    "start": "2789160",
    "end": "2795238"
  },
  {
    "text": "You will see there\nis some challenge. By the way, this reinforcement\nis  ",
    "start": "2795238",
    "end": "2797980"
  },
  {
    "text": "interesting  ",
    "start": "2811540",
    "end": "2812260"
  },
  {
    "text": "area, at least when I-- after I learn it,\nI feel like it has",
    "start": "2822220",
    "end": "2829960"
  },
  {
    "text": "a lot of things to do with\nyour life decision as well. Of course, you can't only",
    "start": "2829960",
    "end": "2833980"
  },
  {
    "text": "It's not like you should\nimplement the algorithm in your life decisions. But there is some\ninsights theory that",
    "start": "2837400",
    "end": "2843940"
  },
  {
    "text": "is useful for the general-- You can model your life as\na reinforcement algorithm.",
    "start": "2843940",
    "end": "2850779"
  },
  {
    "text": "It's just that, one difference\nis that in real life you have much more information.",
    "start": "2850780",
    "end": "2856359"
  },
  {
    "text": "So in a theoretical\nformulation, you are only collecting\ninformation from the samples. So you know nothing\nabout the environment",
    "start": "2858220",
    "end": "2864280"
  },
  {
    "text": "at the beginning, and anything\nyou have to try it out. If you want to know\nanything about PSA,",
    "start": "2864280",
    "end": "2869560"
  },
  {
    "text": "you have to try it out. And sometimes you can try\nmore, and sometimes you can try a bit less. But you still have to try.",
    "start": "2869560",
    "end": "2874960"
  },
  {
    "text": "In life decisions, in many\ncases, you don't have to try. You can already predict, to some\nextent, what the outcome is.",
    "start": "2875980",
    "end": "2882220"
  },
  {
    "text": "Other than that, I feel\nit's pretty similar. Just my two cents. Anyway, so what do you do\nwith continuous state space?",
    "start": "2883000",
    "end": "2892600"
  },
  {
    "text": "So one easy case, just to\nstart with, is when D is 2. Suppose the state space\nis only dimension 2.",
    "start": "2905260",
    "end": "2912400"
  },
  {
    "text": "I think in this case,\nbasically, your state space is a two-dimensional plane.",
    "start": "2912400",
    "end": "2918280"
  },
  {
    "text": "You have maybe 2 axis. And one way to do it is you\njust discretize your state space",
    "start": "2918280",
    "end": "2926140"
  },
  {
    "text": "into discrete variables. So before each of the\nstate is two real number,",
    "start": "2926140",
    "end": "2931240"
  },
  {
    "text": "but then you say I'm going to\ndiscretize the state space. There's some\nboundary, of course, because the state\ncannot be too big.",
    "start": "2931240",
    "end": "2936700"
  },
  {
    "text": "And then you discretize\nsomething like this.",
    "start": "2936700",
    "end": "2939339"
  },
  {
    "text": "And then for every\ncell, you say, all the states--\nand there are even number of states in\neach of the cell.",
    "start": "2942460",
    "end": "2947740"
  },
  {
    "text": "But you say all the\nstates in that cell is going to be\ntreated as one state",
    "start": "2947740",
    "end": "2952840"
  },
  {
    "text": "just because they are\nall pretty much similar. So as long as you have a fine\nenough granularity here, then",
    "start": "2952840",
    "end": "2958900"
  },
  {
    "text": "you can basically treat\nevery cell as a single state. And suppose you have a\ngranularity of epsilon,",
    "start": "2958900",
    "end": "2965680"
  },
  {
    "text": "then you are going to have We have epsilon choices\nfor the first one. We have epsilon choices\nfor the second one.",
    "start": "2965680",
    "end": "2971619"
  },
  {
    "text": "So you have 1 epsilon\nsquared choices of states. And you can probably\ntake epsilon",
    "start": "2971620",
    "end": "2977079"
  },
  {
    "text": "to be something like 0.01. I don't know exactly. But you only get a\nreasonable number of states.",
    "start": "2977080",
    "end": "2982360"
  },
  {
    "text": "Maybe quite a lot, but not maybe But it wouldn't be too bad.",
    "start": "2982360",
    "end": "2988840"
  },
  {
    "text": "So that's the easy choice. However, this doesn't really\nwork for many dimensions",
    "start": "2989620",
    "end": "2995680"
  },
  {
    "text": "higher than two. I think when dimension is because, if you think\nabout you are discretizing",
    "start": "2995680",
    "end": "3002938"
  },
  {
    "text": "a three-dimensional\nspace, then suppose d3,",
    "start": "3002938",
    "end": "3008640"
  },
  {
    "text": "and you do this 1 over epsilon. So the size of each of\nthese cell is epsilon,",
    "start": "3008640",
    "end": "3016620"
  },
  {
    "text": "then you need 1 over epsilon",
    "start": "3016620",
    "end": "3018420"
  },
  {
    "text": "I guess it depends on\nwhat epsilon you choose, but if you choose epsilon\nto be something like 0.01,",
    "start": "3023100",
    "end": "3028140"
  },
  {
    "text": "then you have 100\nto the power 3. That's like a minute, which\nis already a lot, right?",
    "start": "3028140",
    "end": "3033540"
  },
  {
    "text": "So maybe sometimes it's still\nOK, but generally, when d is 3, it's already tricky.",
    "start": "3033540",
    "end": "3038640"
  },
  {
    "text": "And then you can see this\ndoesn't scale very well because if you have d-- for any d, this would be 1\nover epsilon to the power d.",
    "start": "3038640",
    "end": "3045720"
  },
  {
    "text": "And when d is 5, basically,\nit's kind of impossible, completely impossible.",
    "start": "3046440",
    "end": "3051480"
  },
  {
    "text": "So we need a other approach\nfor-- but actually, just to clarify.",
    "start": "3052140",
    "end": "3057300"
  },
  {
    "text": "So when d is 2, actually, this\nis typically a pretty good idea because it's simple and clean. And we don't have to deal\nwith any other complications.",
    "start": "3057300",
    "end": "3064860"
  },
  {
    "text": "The only thing is that you have But there's no any\nother complications.",
    "start": "3064860",
    "end": "3071160"
  },
  {
    "text": "It's actually a\npretty good solution. I think it probably\nshould work in most cases.",
    "start": "3072000",
    "end": "3076019"
  },
  {
    "text": "But when d is more than 3,\nit's going to be a problem. So basically what\nwe do is we are",
    "start": "3077820",
    "end": "3085140"
  },
  {
    "text": "going to redesign\neverything that we have discussed with continuous\nstate space in mind.",
    "start": "3085140",
    "end": "3091320"
  },
  {
    "text": "So I guess there\nare two questions. One question is how to learn\nP s, a for continuous state",
    "start": "3092340",
    "end": "3101940"
  },
  {
    "text": "space. And then question\ntwo is how do you do the value iteration of both\nfor continuous state space?",
    "start": "3101940",
    "end": "3114540"
  },
  {
    "text": "So for question one, so\nlet's discuss each of them.",
    "start": "3116640",
    "end": "3121079"
  },
  {
    "text": "And the basic idea is that\nyou try to kind of extend what you have done to\nthe continuous state",
    "start": "3122580",
    "end": "3127619"
  },
  {
    "text": "space in some way.",
    "start": "3127620",
    "end": "3128520"
  },
  {
    "text": "So regarding question\none, so how do you do it? The first question\nprobably is that how",
    "start": "3133680",
    "end": "3139620"
  },
  {
    "text": "do you even represent\nthe PSA, right? So now you have infinite\nnumber of S here.",
    "start": "3139620",
    "end": "3145440"
  },
  {
    "text": "So before, for every S and a\nyou have a vector to represent, right?",
    "start": "3145440",
    "end": "3150300"
  },
  {
    "text": "So for every Sa, this P\ns, a is a distribution, and basically it's a vector\nover m possible choices,",
    "start": "3151800",
    "end": "3158040"
  },
  {
    "text": "if m is number of states. But now you have even\nnumber of S here or maybe exponential number of S there.",
    "start": "3158040",
    "end": "3164099"
  },
  {
    "text": "And for every P\ns, a, this vector is actually a very\nhigh-dimensional vector, maybe an exponential\ndimensional vector",
    "start": "3164100",
    "end": "3170640"
  },
  {
    "text": "or even dimensional vector. So how do you even\nrepresent this? So the idea is that you can\nchange the way to represent it.",
    "start": "3170640",
    "end": "3178740"
  },
  {
    "text": "You don't represent this-- you can do the following.",
    "start": "3178740",
    "end": "3184380"
  },
  {
    "text": "There are many ways,\nbut this is one way that is probably common.",
    "start": "3184380",
    "end": "3187740"
  },
  {
    "text": "Learning a dynamics.",
    "start": "3189960",
    "end": "3191700"
  },
  {
    "text": "So what you do is\nyou first say I'm going to model this process\nas prime example from P s,",
    "start": "3199020",
    "end": "3206020"
  },
  {
    "text": "a by assuming  ",
    "start": "3206020",
    "end": "3208980"
  },
  {
    "text": "S prime is equal\nto f s, a plus some noise.",
    "start": "3213060",
    "end": "3217740"
  },
  {
    "text": "This is one option,\nnot the only option. But one option is you assume\nthat S prime is computed",
    "start": "3218880",
    "end": "3226980"
  },
  {
    "text": "by applying some function of s\nand a, and then add some noise. That's my way to sample S\nprime from this distribution.",
    "start": "3226980",
    "end": "3235800"
  },
  {
    "text": "So this is a way to\ndefine a distribution. The distribution\nbasically has mean f s, a. And some Gaussian has\nsome variance, like a ksi,",
    "start": "3235800",
    "end": "3244980"
  },
  {
    "text": "the same as the ksi. So this will give\nyou a random variable s prime, given s and a.",
    "start": "3244980",
    "end": "3250920"
  },
  {
    "text": "And this f s, a let's say,\nmaybe f s,a is deterministic. This is just some function\nyou want to learn.",
    "start": "3252840",
    "end": "3261060"
  },
  {
    "text": "And this part is the noise that\ngives you the stochasticity.",
    "start": "3262560",
    "end": "3266160"
  },
  {
    "text": "Maybe let's say ksi is\nmaybe it's from some 0 with some covariant sigma.",
    "start": "3268140",
    "end": "3274140"
  },
  {
    "text": "So sometimes you just,\nI guess, probably have to see that this is\nalmost the same as what",
    "start": "3275460",
    "end": "3280740"
  },
  {
    "text": "we do with supervised learning. You just treat this as a\nsupervised learning problem.",
    "start": "3280740",
    "end": "3287100"
  },
  {
    "text": "So s prime is my label. s and a are my inputs. So I'm just trying to predict\nmy label from the inputs.",
    "start": "3287100",
    "end": "3295079"
  },
  {
    "text": "And the way I model\nthis is I model this by some function\nplus noise, right?",
    "start": "3295080",
    "end": "3301619"
  },
  {
    "text": "And then you can do\nmaximum likelihood. And the maximum likelihood\nis just the square loss.",
    "start": "3301620",
    "end": "3306060"
  },
  {
    "text": "And you  ",
    "start": "3306720",
    "end": "3307440"
  },
  {
    "text": "learn this model\nf by some square loss.",
    "start": "3312720",
    "end": "3315000"
  },
  {
    "text": "So for example, you\nhave to introduce some parameterization for f. So for example, f could\nbe a linear model.",
    "start": "3320940",
    "end": "3329220"
  },
  {
    "text": "Suppose you say you have\nsome parameter A and B, and your f could be just\nA S plus B, little a.",
    "start": "3329220",
    "end": "3337020"
  },
  {
    "text": "So this could be the f, which\nis parameterized by A and B.",
    "start": "3337980",
    "end": "3342180"
  },
  {
    "text": "So A and B are parameters\nS, and they are inputs. So you just have\nthis linear model.",
    "start": "3343080",
    "end": "3346799"
  },
  {
    "text": "I think this is called\nlinear dynamical models. So this is a linear model.",
    "start": "3348840",
    "end": "3351600"
  },
  {
    "text": "And another option\nis that you say, I'm going to say f s,\na is equal to maybe--",
    "start": "3355320",
    "end": "3363779"
  },
  {
    "text": "you can use the same\nidea as the feature like when you do the kernel. You can say this is something\nlike A times phi of s plus B",
    "start": "3365160",
    "end": "3376740"
  },
  {
    "text": "times-- I don't know. I forgot what this is called. We just have some other\nfeature, something like this.",
    "start": "3376740",
    "end": "3387599"
  },
  {
    "text": "So both of those two phi's,\nthese are some features.",
    "start": "3388440",
    "end": "3395160"
  },
  {
    "text": "So this is like what we did\nwith kernel method, right? So you introduce some\nfeatures, and then you",
    "start": "3400260",
    "end": "3405720"
  },
  {
    "text": "are linear in the feature space. Of course, we can\nalso say that f s, a--",
    "start": "3405720",
    "end": "3414270"
  },
  {
    "text": "so I guess here the\nparameter is A and B as well. You can also say f s, a, if they\nare parameterized some theta,",
    "start": "3414270",
    "end": "3421079"
  },
  {
    "text": "is a neural network neural\nnetwork applied on s",
    "start": "3421080",
    "end": "3427860"
  },
  {
    "text": "and a, something like this. And this new network is maybe\nsay parameterized by theta.",
    "start": "3427860",
    "end": "3434579"
  },
  {
    "text": "So you just say s and\na are concatenated as inputs of a network. And you apply this network\nwith parameter theta.",
    "start": "3435300",
    "end": "3441720"
  },
  {
    "text": "And the output will be f Sa. And in each of\nthese case, you can",
    "start": "3441720",
    "end": "3447420"
  },
  {
    "text": "model your s prime\nlike this, and then you can try to find out the--",
    "start": "3447420",
    "end": "3454080"
  },
  {
    "text": "and this becomes a\nsupervised learning problem. So what I mean is that  ",
    "start": "3455760",
    "end": "3461520"
  },
  {
    "text": "once\nwe have the parameterization right, so then you can--",
    "start": "3470160",
    "end": "3478020"
  },
  {
    "text": "the learning loss\nfunction is that-- say suppose you have some data.",
    "start": "3478740",
    "end": "3484680"
  },
  {
    "text": "Suppose data are\nsomething like-- I guess, I've written\nthis several times--",
    "start": "3488580",
    "end": "3494640"
  },
  {
    "text": "S01, a01, S1 1, so and so forth.",
    "start": "3495660",
    "end": "3502200"
  },
  {
    "text": "So I have a bunch\nof trajectories.",
    "start": "3502980",
    "end": "3504600"
  },
  {
    "text": "Sorry, I messed up the index.",
    "start": "3515400",
    "end": "3517500"
  },
  {
    "text": "So you have these\ntrajectories, and then you",
    "start": "3522300",
    "end": "3524220"
  },
  {
    "text": "break these trajectories\ninto three tuples. So you mean that you\nview this,  ",
    "start": "3527340",
    "end": "3533760"
  },
  {
    "text": "view them as a collection of three tuples.",
    "start": "3538560",
    "end": "3544980"
  },
  {
    "text": "So you say you have\ns01, a01 comma s1 1.",
    "start": "3544980",
    "end": "3554040"
  },
  {
    "text": "So this is the first\nthree things here, and you view this as\nthe input and this",
    "start": "3555840",
    "end": "3563520"
  },
  {
    "text": "as the so-called\nlabel or output.",
    "start": "3563520",
    "end": "3565140"
  },
  {
    "text": "And then you say, I'm going to\nhave these three things, which",
    "start": "3568860",
    "end": "3574320"
  },
  {
    "text": "is s1 1, a1 1, and s1 2, right?",
    "start": "3574320",
    "end": "3584940"
  },
  {
    "text": "So this, again, is the\ninput, and this is a label.",
    "start": "3584940",
    "end": "3590099"
  },
  {
    "text": "And you do this for every three\ntuples in every trajectory.",
    "start": "3591840",
    "end": "3597120"
  },
  {
    "text": "So you basically get a\nsequence of like three tuples.",
    "start": "3597120",
    "end": "3600600"
  },
  {
    "text": "So eventually you get s-- t minus 1 n-- n is the number\nof trajectories--",
    "start": "3606480",
    "end": "3613494"
  },
  {
    "text": "on a t minus 1 n I guess the--",
    "start": "3613495",
    "end": "3620640"
  },
  {
    "text": "a lot of indexing here. But basically, just view every\nthree consecutive numbers",
    "start": "3621780",
    "end": "3630540"
  },
  {
    "text": "as a tuple, and you view the\nfirst s an a as to the input. And the outcome as the\noutput, something like this.",
    "start": "3630540",
    "end": "3638580"
  },
  {
    "text": "OK. So you have a data\nset of this, right? So this is a data\nset of size n times",
    "start": "3641220",
    "end": "3647520"
  },
  {
    "text": "t. n is the number\nof trajectories. t is the length of the trajectory. And then you do a\nsupervised learning,",
    "start": "3647520",
    "end": "3653880"
  },
  {
    "text": "so you just use\nsupervised learning.",
    "start": "3653880",
    "end": "3656460"
  },
  {
    "text": "So you do some kind of\nregression, let's say.",
    "start": "3662280",
    "end": "3665400"
  },
  {
    "text": "You say I'm going to\nminimize over my parameter. Maybe let's call it theta.",
    "start": "3667920",
    "end": "3673140"
  },
  {
    "text": "And I minimize this over--",
    "start": "3673680",
    "end": "3677819"
  },
  {
    "text": "I minimize the loss\nover all data points. So what is the loss? The loss-- so there is i.",
    "start": "3678720",
    "end": "3685740"
  },
  {
    "text": "There is also a t. i is indexing the\ntrajectories, and t is",
    "start": "3686400",
    "end": "3694620"
  },
  {
    "text": "in the indexing the timestamp. And every time you\nwant to predict what you want to\npredict S t plus 1",
    "start": "3694620",
    "end": "3700500"
  },
  {
    "text": "in the ith tracjectory\nusing this function f theta s t i a ti.",
    "start": "3701220",
    "end": "3712740"
  },
  {
    "text": "So this is the input of this\nsupervised learning problem. You apply the\nmodel, and then you",
    "start": "3715860",
    "end": "3721140"
  },
  {
    "text": "try to match it with the\noutput, a label of this problem.",
    "start": "3721140",
    "end": "3726960"
  },
  {
    "text": "And here I'm using the L2 norm\nbecause the label is a vector. So in many of the cases you\nsee this parentheses square",
    "start": "3728520",
    "end": "3735360"
  },
  {
    "text": "because, in those\ncases, when you do the typical supervised\nlearning problem, your label is a real number,\nright, house price prediction,",
    "start": "3735360",
    "end": "3743640"
  },
  {
    "text": "right? So the label is the house\nprice, which is a real number. But here the label is a vector. It's the same.",
    "start": "3743640",
    "end": "3748920"
  },
  {
    "text": "You just do the L2 norm squared. Sometimes you can change\nthe loss function. You don't have to always\nuse L2 norm squared.",
    "start": "3748920",
    "end": "3755099"
  },
  {
    "text": "Maybe you don't even\nneed to square sometimes. You can use other loss,\nL1, something like that.",
    "start": "3755100",
    "end": "3760800"
  },
  {
    "text": "And once you do this,\nyou get a theta. And this f theta\nwill be our model.",
    "start": "3763920",
    "end": "3768000"
  },
  {
    "text": "Any questions?",
    "start": "3774540",
    "end": "3775500"
  },
  {
    "text": "So I think the benefit\nhere is that before you have to specify P s, a for every\ns and a, right, as a vector,",
    "start": "3783360",
    "end": "3791700"
  },
  {
    "text": "right? So now, you learn\nthis function f theta. So the number of\nparameters is theta,",
    "start": "3791700",
    "end": "3797400"
  },
  {
    "text": "and then for every s\nand a you can compute f s, a if you know the theta.",
    "start": "3797400",
    "end": "3802200"
  },
  {
    "text": "So that's how you do\nthe model-based RL part. Sorry, that's how you do the\nmodel estimation part, how",
    "start": "3802740",
    "end": "3809100"
  },
  {
    "text": "you estimate the dynamics. And then you also need to\ndeal with the value iteration.",
    "start": "3809100",
    "end": "3815160"
  },
  {
    "text": "How do you do the\nvalue iteration for continuous state space? I don't think I\nhave the math there.",
    "start": "3815160",
    "end": "3822240"
  },
  {
    "text": "So when you do the\nvalue iteration, before you are trying\nto update the value function for every state\nfor every time, right?",
    "start": "3822240",
    "end": "3830339"
  },
  {
    "text": "So now that's not possible\nanymore because you have so many states. You cannot update the value\nfunction for every state. Actually, it's\nnot even clear how",
    "start": "3830340",
    "end": "3836520"
  },
  {
    "text": "do you even describe a value\nfunction because before the way you describe a value\nfunction is you say,",
    "start": "3836520",
    "end": "3842940"
  },
  {
    "text": "for every V star s, I\nhave a number, right? But before you describe\nit by listing all the s.",
    "start": "3842940",
    "end": "3850800"
  },
  {
    "text": "For every s you have a number. That's how you describe V star. But now you cannot really\ndescribe it like that. So what you do is you say I'm\ngoing to parameterize the value",
    "start": "3850800",
    "end": "3859200"
  },
  {
    "text": "function by, again,\nkind of like before, by some kind of network\nor linear model, right?",
    "start": "3859200",
    "end": "3864660"
  },
  {
    "text": "So you can say I parameterized-- I write my V s as on\nsomething like maybe one",
    "start": "3864660",
    "end": "3876000"
  },
  {
    "text": "choice is you say this\nis theta transposed have some feature of s. So this is a feature, and\nthis is the parameter.",
    "start": "3876000",
    "end": "3883920"
  },
  {
    "text": "That's my option. And another option\nis you say my V of s is a neural network  ",
    "start": "3883920",
    "end": "3889140"
  },
  {
    "text": "with\nparameterized theta applied on a state s. So then theta is a description\nof the value function.",
    "start": "3893640",
    "end": "3901500"
  },
  {
    "text": "And you need to learn theta.",
    "start": "3902100",
    "end": "3904780"
  },
  {
    "text": "Of course, there are\nother ways to do this. For example, one way\nto do it is that you can, for example,\ndesign the right",
    "start": "3907560",
    "end": "3917700"
  },
  {
    "text": "features using\nphysical intuitions. Sometimes you know\nthat some coordinates it's only meaningful when it\ncombines with other coordinates",
    "start": "3917700",
    "end": "3925260"
  },
  {
    "text": "in some meaningful way. So I think some mixture\nof this could also work because you could\ndesign some features",
    "start": "3925260",
    "end": "3931500"
  },
  {
    "text": "and then use these features\nas inputs to a neural network. But generally, you just want\nto have parameterized form.",
    "start": "3931500",
    "end": "3938640"
  },
  {
    "text": "So we have done-- so after we have the\nrepresentation of the value function, the next\nquestion is, how do you",
    "start": "3940140",
    "end": "3945840"
  },
  {
    "text": "do the update, right? So before what we did was that--",
    "start": "3945840",
    "end": "3951660"
  },
  {
    "text": "so recall the update\nwas something like V",
    "start": "3951660",
    "end": "3957720"
  },
  {
    "text": "s is able to be something like\nRs plus gamma max a  ",
    "start": "3957720",
    "end": "3966420"
  },
  {
    "text": "s prime. That's what we did. So we have some\nworking value for V, and we compute the right-hand\nside of the Bellman equation.",
    "start": "3972840",
    "end": "3979920"
  },
  {
    "text": "And we update V with\nthe right-hand side, and then we repeat. But you cannot do this\nbecause you don't have--",
    "start": "3979920",
    "end": "3985740"
  },
  {
    "text": "before we do it for every s. But that's not possible anymore.",
    "start": "3985740",
    "end": "3990839"
  },
  {
    "text": "So what we do is we try to\nmake this is true for the s we have seen. So basically, for\ncontinuous state space,",
    "start": "3991500",
    "end": "3997500"
  },
  {
    "text": "so for the continuous\ncase we just try to ensure a Bellman equation\nfor states that we have seen.",
    "start": "3997500",
    "end": "4014720"
  },
  {
    "text": "It only ensure this for\nevery state s anymore. You just ensure it for\nthe states you have seen. And if you just want\nto do that, then you",
    "start": "4018830",
    "end": "4025520"
  },
  {
    "text": "can do some kind of loss\nfunction to ensure that. So let me elaborate on\nwhat do I mean here.",
    "start": "4025520",
    "end": "4031339"
  },
  {
    "text": "So  ",
    "start": "4041000",
    "end": "4042020"
  },
  {
    "text": "the first step\nis the following. So you estimate the right-hand\nside of the Bellman equation",
    "start": "4053180",
    "end": "4062180"
  },
  {
    "text": "for states say s1 up to sn.",
    "start": "4062840",
    "end": "4075620"
  },
  {
    "text": "So I haven't told you exactly\nhow I got these states, right? So suppose I got\nsome states that I have seen in the algorithm.",
    "start": "4076880",
    "end": "4083839"
  },
  {
    "text": "And I want to only ensure\nthis Bellman equation-- ensure this to be equal\nto this, or somewhat",
    "start": "4083840",
    "end": "4090380"
  },
  {
    "text": "encourage they are the same\nonly for this choice of states, s is equal to 1 of this.",
    "start": "4090380",
    "end": "4096140"
  },
  {
    "text": "That's my compromise because I\ncannot do it for every state. So what I do is that\nI try to first compute",
    "start": "4096680",
    "end": "4105799"
  },
  {
    "text": "the right-hand side. So how do I get the right-hand\nside for every state S? So what I do is I just--",
    "start": "4105800",
    "end": "4111320"
  },
  {
    "text": "so basically I want to compute\nR s i plus this max thing.",
    "start": "4114080",
    "end": "4119240"
  },
  {
    "text": "But the problem is that here I\nhave a sum over s prime again. That's, again, a lot of\ndifferent choices for s.",
    "start": "4132500",
    "end": "4139040"
  },
  {
    "text": "So what I do is I'm\ngoing to first turn this into an expectation.",
    "start": "4139040",
    "end": "4146240"
  },
  {
    "text": "We write this as\nexpectation of V s prime. And s prime sampled from P si a.",
    "start": "4148820",
    "end": "4156380"
  },
  {
    "text": "So after you have\nthe expectation, you can use an empirical sample\nto estimate this quantity.",
    "start": "4159320",
    "end": "4163759"
  },
  {
    "text": "So basically, what you do\nis you say,  ",
    "start": "4164360",
    "end": "4166100"
  },
  {
    "text": "so for every-- by the way, I think I forgot\nto mention one thing, which is that now I'm talking\nabout continuous state",
    "start": "4173840",
    "end": "4186799"
  },
  {
    "text": "space but finite action space.",
    "start": "4186800",
    "end": "4190460"
  },
  {
    "text": "Just this is a slightly simpler\nquestion than both of them are continuous just\nbecause I don't want",
    "start": "4192380",
    "end": "4198860"
  },
  {
    "text": "to over complicate too much. So it's continuous state\nspace and finite action space.",
    "start": "4198860",
    "end": "4203660"
  },
  {
    "text": "So what I do is that\nI first estimate this by sampling some s prime.",
    "start": "4204560",
    "end": "4210199"
  },
  {
    "text": "And then I take\nthe max because you have finite number of states. Actions you can\njust take the max.",
    "start": "4210200",
    "end": "4216440"
  },
  {
    "text": "So what you do is you say for\nevery A you get k samples.",
    "start": "4216440",
    "end": "4226460"
  },
  {
    "text": "Let's call it s1\nprime up to sk prime. These are sampled from these\ntransmission dynamics P s, a.",
    "start": "4229400",
    "end": "4239180"
  },
  {
    "text": "P s as i a, right? So the state si, you\ntry the action a, and you see what is the outcome.",
    "start": "4242480",
    "end": "4247580"
  },
  {
    "text": "You have a bunch\nof random outcome. And then you define\nQ a to be Rs i plus 1",
    "start": "4247580",
    "end": "4261620"
  },
  {
    "text": "over k times this average. You use the empirical\naverage basically.",
    "start": "4261620",
    "end": "4266300"
  },
  {
    "text": "The empirical average\nof PV sj prime.",
    "start": "4268100",
    "end": "4276440"
  },
  {
    "text": "So this is supposed\nto estimate Rs--",
    "start": "4276440",
    "end": "4280219"
  },
  {
    "text": "I think I'm missing\na gamma here, sorry. It's supposed to\nestimate Rs plus  ",
    "start": "4282500",
    "end": "4287360"
  },
  {
    "text": "Rs i plus gamma expectation V S i.",
    "start": "4292220",
    "end": "4299599"
  },
  {
    "text": "So I'm supposed to use this to\nestimate this without the max. And then I take the max.",
    "start": "4301460",
    "end": "4307099"
  },
  {
    "text": "I call y i to be\nthe max of this qa.",
    "start": "4307100",
    "end": "4316700"
  },
  {
    "text": "So then this is supposed\nto estimate this one. So y i is supposed to\nestimate this quantity.",
    "start": "4317420",
    "end": "4322580"
  },
  {
    "text": "And for every i I\nhave estimate, right? For every i have estimate\nfor the right-hand side",
    "start": "4324260",
    "end": "4329840"
  },
  {
    "text": "of this Bellman equation. And then I ensure that\nthis Bellman equation",
    "start": "4329840",
    "end": "4335000"
  },
  {
    "text": "to be somewhat correct for all\nthe i's that I have considered.",
    "start": "4335000",
    "end": "4341720"
  },
  {
    "text": "So step--  ",
    "start": "4341720",
    "end": "4345920"
  },
  {
    "text": "so step b, I\nneed to enforce like V Si",
    "start": "4357080",
    "end": "4372020"
  },
  {
    "text": "to be close to y i because,\nrecall that we have spent so much effort to\ncompute this y i,",
    "start": "4372800",
    "end": "4378440"
  },
  {
    "text": "it's y i supposed to\nbe the right-hand side of the Bellman equation\nevaluated at Si.",
    "start": "4378440",
    "end": "4385400"
  },
  {
    "text": "And I want this to be close\nto the left-hand side. And how do I do that? I do that by, say,\nI'm computing theta",
    "start": "4385400",
    "end": "4392660"
  },
  {
    "text": "to be arg max of this loss\nfunction that encourages this.",
    "start": "4393380",
    "end": "4399860"
  },
  {
    "text": "And the loss function is\nsimply just some L2 loss. Say I take the average\nover all the possible Si's,",
    "start": "4399860",
    "end": "4409219"
  },
  {
    "text": "and I say V--",
    "start": "4409220",
    "end": "4412220"
  },
  {
    "text": "my Vsi is parameterized by some\nsay, network, like this way. So let's only talk\nabout neural networks,",
    "start": "4416240",
    "end": "4422540"
  },
  {
    "text": "but it works for anything. So say this is a neural\nnetwork with theta applied",
    "start": "4422540",
    "end": "4431780"
  },
  {
    "text": "on si minus y i squared. So basically you want this\nneural network to output",
    "start": "4431780",
    "end": "4438920"
  },
  {
    "text": "the right value function\nthat matches the target. The target is the right-hand\nside of the Bellman equation.",
    "start": "4440120",
    "end": "4445280"
  },
  {
    "text": "Actually, in the RL papers,\nyou do call this target.",
    "start": "4445280",
    "end": "4450559"
  },
  {
    "text": "So this is the target\nyou want to match. And you want to\nchoose the thetas after the theta such that\nyour left-hand side, which",
    "start": "4450560",
    "end": "4458239"
  },
  {
    "text": "is this neural network\nSi to match the target. And that's your theta\nthat you got in this step.",
    "start": "4458240",
    "end": "4465200"
  },
  {
    "text": "And again, you have to do-- a survival iteration\nyou have to iterate because, if you\nget theta this way,",
    "start": "4466880",
    "end": "4472280"
  },
  {
    "text": "you have to iterate to\nrecompute your right-hand side and then up to the left-hand\nside, where you call it,",
    "start": "4472280",
    "end": "4478520"
  },
  {
    "text": "do the value iteration. You compute the\nright-hand side and you update the left-hand side. You do this iteratively.",
    "start": "4478520",
    "end": "4483619"
  },
  {
    "text": "So eventually, you also have to\nhave iteration, which is say--",
    "start": "4485060",
    "end": "4489500"
  },
  {
    "text": "so you iterate between\nthese two steps. So you say you have a loop, but\nI think there is a little bit--",
    "start": "4492680",
    "end": "4504500"
  },
  {
    "text": "did I specify everything? Yes, I think I specify--",
    "start": "4506000",
    "end": "4509420"
  },
  {
    "text": "OK, so this is the loop--",
    "start": "4511640",
    "end": "4513920"
  },
  {
    "text": "maybe sorry.",
    "start": "4517580",
    "end": "4518480"
  },
  {
    "text": "Yes, I should save\nsome space before.",
    "start": "4522980",
    "end": "4532640"
  },
  {
    "text": "So let's say this is a\nand I have a loop here.",
    "start": "4532640",
    "end": "4536660"
  },
  {
    "text": "But this is still not\nenough because this loop is doing what? This loop is only doing the\nvalue iteration for this s1",
    "start": "4545000",
    "end": "4556214"
  },
  {
    "text": "up to sn. I haven't told you even\nhow do you get S1 up to Sn. It's just a bunch of\nstates that you have seen.",
    "start": "4556214",
    "end": "4562760"
  },
  {
    "text": "And also, you can update--\nyou can do this for every s. Value iteration is\nnot enough because",
    "start": "4563600",
    "end": "4569659"
  },
  {
    "text": "our model-based algorithm\nhas something even an outlier of the value iteration.",
    "start": "4569660",
    "end": "4574940"
  },
  {
    "text": "So you're estimating the model,\nand you do value iteration. And then you estimate\nthe model again, and you do value iteration. Or you have to do\nanother outer loop",
    "start": "4574940",
    "end": "4582020"
  },
  {
    "text": "to update your samples,\nright, because when you do the\nmodel-based algorithm,",
    "start": "4582020",
    "end": "4587600"
  },
  {
    "text": "you already have a loop\nall set of value iteration. So this is the loop for\nthe value iteration, and then we have\nanother loop outside it,",
    "start": "4587600",
    "end": "4593540"
  },
  {
    "text": "which is supposed to\niteratively update the model. So basically, you have\nanother loop, and in this loop",
    "start": "4594740",
    "end": "4603980"
  },
  {
    "text": "you collect some\ndata,  ",
    "start": "4603980",
    "end": "4605120"
  },
  {
    "text": "say s1 up to sn. We collect this data\nfrom current policy.",
    "start": "4610040",
    "end": "4618560"
  },
  {
    "text": "You collect the\ndata, and then you do the value iteration\non this data. And then you collect it again,\nand you do value iteration.",
    "start": "4623000",
    "end": "4632420"
  },
  {
    "text": "So this is how it imitates\nthe model-based RL algorithm for tabular case because\nfor the tabular case",
    "start": "4633080",
    "end": "4640040"
  },
  {
    "text": "we also have this outer\nloop where you collect data. And you do value\niteration, and you collect data do value iteration.",
    "start": "4640040",
    "end": "4645800"
  },
  {
    "text": "So OK, sorry, I think I'm\nmissing one more step.",
    "start": "4646340",
    "end": "4649940"
  },
  {
    "text": "You collect data,\nand also you need to also-- in this\nalso loop you also have to estimate the dynamics.",
    "start": "4653120",
    "end": "4658040"
  },
  {
    "text": "Collect data, my bad. So collect data, maybe 1,\nand 2, you estimate dynamics.",
    "start": "4659060",
    "end": "4666739"
  },
  {
    "text": "And then 3 you do\nthe value iteration. And the value iteration consists\nof another loop, which is this.",
    "start": "4671720",
    "end": "4678800"
  },
  {
    "text": "Any questions?",
    "start": "4690920",
    "end": "4691940"
  },
  {
    "text": "I see some confusing faces. So basically, I\nthink I erased that. Oh, actually, it\nwas partly here.",
    "start": "4701600",
    "end": "4707360"
  },
  {
    "text": "So this was the\npart that we deal with the tabular case, right? So we have the two steps.",
    "start": "4707360",
    "end": "4713420"
  },
  {
    "text": "I think maybe I should\nrename them s1, 2, and 3.",
    "start": "4713420",
    "end": "4717920"
  },
  {
    "text": "And there was one\nstep that was erased, which is the first step, which\nis to collect data, right?",
    "start": "4719780",
    "end": "4727699"
  },
  {
    "text": "So before, when you\ndo the tabular case, you alternate between\nthe three steps, collect data, estimate the\ndynamics, value iterations.",
    "start": "4727700",
    "end": "4734840"
  },
  {
    "text": "And now, for the\ncontinuous database,",
    "start": "4735620",
    "end": "4741380"
  },
  {
    "text": "number one still the same. Number two was done by this\npart of the board, where",
    "start": "4742400",
    "end": "4748340"
  },
  {
    "text": "we learn this with the\nsupervised learning. And number three\nis done by here,",
    "start": "4748340",
    "end": "4755180"
  },
  {
    "text": "this part of the\nboard, where we do this A and B step, the A and\nB to do the value iteration.",
    "start": "4755180",
    "end": "4762980"
  },
  {
    "text": "So number three is\nthis part, where we do the value iteration.",
    "start": "4763580",
    "end": "4768080"
  },
  {
    "text": "And eventually, we have\nto do a loop for this 1,",
    "start": "4769520",
    "end": "4772220"
  },
  {
    "text": "I guess that's all for today. And for this whole\ncourse I guess we have covered\nsupervised learning,",
    "start": "4780980",
    "end": "4786620"
  },
  {
    "text": "unsupervised learning, and\nreinforcement learning. Yeah, and we try to\ncover more and more",
    "start": "4786620",
    "end": "4794420"
  },
  {
    "text": "deep learning these days. But also, some of this\nmath are included mostly",
    "start": "4794420",
    "end": "4801080"
  },
  {
    "text": "because I think these are\nthe foundations of machine learning. So I hope you had some\nfun with the course.",
    "start": "4801080",
    "end": "4810200"
  },
  {
    "text": "Thanks. [APPLAUSE]",
    "start": "4810200",
    "end": "4815960"
  }
]