[
  {
    "start": "0",
    "end": "5830"
  },
  {
    "text": "Good afternoon, CS109. How are you guys doing today? Woo-hoo. Yay, it's the day\nbefore Thanksgiving.",
    "start": "5830",
    "end": "10893"
  },
  {
    "text": "How are you guys feeling? Woo! OK, it's got to be nice to have\na little bit of a break coming",
    "start": "10893",
    "end": "17020"
  },
  {
    "text": "up. We have an exciting class today. We're going to be talking\nabout Naive Bayes. It is not only one of the things\nthat you will be implementing",
    "start": "17020",
    "end": "23529"
  },
  {
    "text": "on your final\nassignment in CS109, but it's going to be our first\nmachine learning algorithm that we learn about in CS109.",
    "start": "23530",
    "end": "30610"
  },
  {
    "text": "Exciting day, but it's a Friday. So because it's a Friday, I\nwas going to tell you guys a little bit of a story.",
    "start": "30610",
    "end": "35985"
  },
  {
    "text": "And I was going to tell\nyou a little bit of a story of my lovely daughter. So this is my lovely\ndaughter [MUTED].. And I think you guys know I\nhave a daughter at this point.",
    "start": "35985",
    "end": "44170"
  },
  {
    "text": "But there's so much\nI can say about it. Like, wow, parenthood, what\nan adventure it's been.",
    "start": "44170",
    "end": "51010"
  },
  {
    "text": "I have learned so much more\nin this last year and a half than maybe I learned\nin an entire PhD",
    "start": "51010",
    "end": "56800"
  },
  {
    "text": "about life, how people learn. It's just been such\nan exciting time,",
    "start": "56800",
    "end": "62110"
  },
  {
    "text": "how to teach people boundaries. ",
    "start": "62110",
    "end": "67810"
  },
  {
    "text": "I'd love to share\nsomething interesting that we've also gone through as\nparents that not every parent",
    "start": "67810",
    "end": "73780"
  },
  {
    "text": "goes through, which is when\n[MUTED] was very young, so she was born, it was this\ncrazy experience.",
    "start": "73780",
    "end": "79660"
  },
  {
    "text": "And then the next two\nweeks were like this sleep deprived bliss of like this\nbrand new soul in the world.",
    "start": "79660",
    "end": "86660"
  },
  {
    "text": "But then when she\nwas two weeks old, we got some interesting news. We found out at\nthat point that she",
    "start": "86660",
    "end": "91840"
  },
  {
    "text": "was born not able\nto hear at all, which is surprisingly common. In about 1 in 1,000\nbabies this happens.",
    "start": "91840",
    "end": "98410"
  },
  {
    "text": "And there wasn't\na family history. So it was a little\nbit surprising for us. And again, I feel like I\nimmediately learned a lot.",
    "start": "98410",
    "end": "106100"
  },
  {
    "text": "It wasn't what we expected,\nbut, boy, did we learn so much. First, we learned sign language. We learned about this whole new\nwonderful culture that exists",
    "start": "106100",
    "end": "113440"
  },
  {
    "text": "in the US and around the world. And we learned a little\nbit about as a parent",
    "start": "113440",
    "end": "119110"
  },
  {
    "text": "not having clear\nexpectations for the life your child is going to lead. They're always going\nto lead different lives",
    "start": "119110",
    "end": "125143"
  },
  {
    "text": "than you designed for them. They get to choose. We just were learning a\nlot about that at week two.",
    "start": "125143",
    "end": "130550"
  },
  {
    "text": "But then also an\ninteresting thing is she has these things\ncalled cochlear implants.",
    "start": "130550",
    "end": "135620"
  },
  {
    "text": "And you guys might have\nheard of those before. Basically, she has a microphone. And that microphone\ngoes to a processor.",
    "start": "135620",
    "end": "142160"
  },
  {
    "text": "The processor goes to a coil. The coil sends a little\nsignal to a receiver on the inside of her\nhead that's connected",
    "start": "142160",
    "end": "148459"
  },
  {
    "text": "to her auditory nerve, and it\nstimulates her auditory nerve. So basically, it's a microphone\nthat stimulates directly",
    "start": "148460",
    "end": "154340"
  },
  {
    "text": "this thing in the\ninner part of the ear. And it's incredible. If you didn't know, the biggest\nbreakthrough in this device",
    "start": "154340",
    "end": "160760"
  },
  {
    "text": "happened here at Stanford,\nactually in the EE Department. They figured out a\nwhole bunch of key ideas",
    "start": "160760",
    "end": "166069"
  },
  {
    "text": "to make this possible in humans. And now she hears so well. Now, it's such a\nhard story to tell",
    "start": "166070",
    "end": "172819"
  },
  {
    "text": "because I was so ready\nto embrace not hearing. Signing was the coolest thing.",
    "start": "172820",
    "end": "177960"
  },
  {
    "text": "And I was really excited\nfor that journey too, but now it looks like our\njourney has hearing and talking",
    "start": "177960",
    "end": "184160"
  },
  {
    "text": "and dancing. In fact, every day starts\nwith a dance party at 6:00 AM, but I must say that's\nlike one of the things",
    "start": "184160",
    "end": "192860"
  },
  {
    "text": "that I'm very thankful for. So going into\nThanksgiving, I'm just so thankful to have this\nwonderful soul in my life",
    "start": "192860",
    "end": "199670"
  },
  {
    "text": "who has taught me\nso many things. I am also thankful for\nthe people who worked hard in technologies to give\nher access to this one",
    "start": "199670",
    "end": "206210"
  },
  {
    "text": "facet of life that I have\nenjoyed and also thankful for all the dance\nparties in the house.",
    "start": "206210",
    "end": "212060"
  },
  {
    "text": "So that's my Friday story\nfor getting us started. Bringing us back\ninto CS109, we are",
    "start": "212060",
    "end": "219069"
  },
  {
    "text": "having this wonderful\ncapstone experience in CS109 where we are building\nup towards learning some of the fundamental\nalgorithms that are currently",
    "start": "219070",
    "end": "226090"
  },
  {
    "text": "changing the landscape\nof computer science and artificial intelligence. We will be working up\ntowards deep learning, a.k.a.",
    "start": "226090",
    "end": "232390"
  },
  {
    "text": "neural networks. To get there, we are going to\nstart learning some algorithms on computer science. And we've done a lot of work.",
    "start": "232390",
    "end": "238450"
  },
  {
    "text": "We've built a strong\nfoundation of the core theory that you need to know. And now it's time\nto celebrate, use",
    "start": "238450",
    "end": "244750"
  },
  {
    "text": "that core theory to\nlearn the algorithms that are powering the technology\nthat you hear so much about.",
    "start": "244750",
    "end": "250040"
  },
  {
    "text": "And today is going to be\nthat first so exciting days and we're going to pick off\none of these two critical AI",
    "start": "250040",
    "end": "256000"
  },
  {
    "text": "algorithms called Naive Bayes. Now that we're getting\nto machine learning, I thought it'd be\nhelpful to just give you",
    "start": "256000",
    "end": "262680"
  },
  {
    "text": "a picture of how\nmachine learning fits into the world of\nprobabilistic models. A lot of times we think\nof machine learning",
    "start": "262680",
    "end": "269030"
  },
  {
    "text": "as being a probabilistic model. You'll imagine\ngetting some inputs. And then your\nprobabilistic model",
    "start": "269030",
    "end": "275680"
  },
  {
    "text": "will have some parameters. And based on those parameters,\nit will make a prediction. And it'll often do\nso probabilistically.",
    "start": "275680",
    "end": "281800"
  },
  {
    "text": "And these parameters are\njust like all the parameters we've seen in\nparameter estimation. Really, you should still\nthink about them as sliders.",
    "start": "281800",
    "end": "288400"
  },
  {
    "text": "And if you had the perfect\nsettings for these sliders, you'd have a good\nprobabilistic model that could do this input/output\nprediction quite well.",
    "start": "288400",
    "end": "297870"
  },
  {
    "text": "Parameter estimation is that\nart form of setting sliders. And that art form\nof setting sliders",
    "start": "297870",
    "end": "303180"
  },
  {
    "text": "is going to be the core\ntheoretical basis for making good probabilistic models. And we really have\ntwo great ideas",
    "start": "303180",
    "end": "309060"
  },
  {
    "text": "for how you could\nset those sliders. The first great idea is\nmaximum likelihood estimation.",
    "start": "309060",
    "end": "314850"
  },
  {
    "text": "You're going to choose\nthe slider values that make data in a training data\nset look as likely as possible.",
    "start": "314850",
    "end": "321247"
  },
  {
    "text": "And you work through\nsome of the math. That means we're going\nto set our sliders using this beautiful little equation.",
    "start": "321247",
    "end": "327270"
  },
  {
    "text": "That's only one perspective. And on Wednesday's class,\nwe saw a totally different perspective. And the totally\ndifferent perspective",
    "start": "327270",
    "end": "332982"
  },
  {
    "text": "was instead of choosing\nthe slider values that make the data look as\nlikely as possible,",
    "start": "332982",
    "end": "337990"
  },
  {
    "text": "we're going to choose the most\nlikely slider values given the data that we've seen.",
    "start": "337990",
    "end": "343540"
  },
  {
    "text": "And they're both defensible. People use both of them. They have just one\nbig difference when",
    "start": "343540",
    "end": "349599"
  },
  {
    "text": "you get to the final equation. In MLE, we're just going to\nhave the log of the likelihood",
    "start": "349600",
    "end": "356037"
  },
  {
    "text": "and we're going to be\nchoosing the parameters that maximize this. In MAP, that's the fancy\nversion for the Bayesian idea",
    "start": "356037",
    "end": "364360"
  },
  {
    "text": "of choosing the most\nlikely parameters, you have basically\nthe same equation, but you now are also going\nto have a term that's",
    "start": "364360",
    "end": "371169"
  },
  {
    "text": "related to how likely you\nthought the parameter value was to take on its current\nvalue before you",
    "start": "371170",
    "end": "378310"
  },
  {
    "text": "saw any data, a prior. So you have a Bayesian\nprior, and it's just like the prior in any\nof your Bayes' equations",
    "start": "378310",
    "end": "386229"
  },
  {
    "text": "that you've seen before. And you'll have a\nterm for that prior, and the rest is\nexactly the same.",
    "start": "386230",
    "end": "391680"
  },
  {
    "text": "And so realistically, these\naren't all that different. And the biggest 10\nout of 10 takeaway",
    "start": "391680",
    "end": "397970"
  },
  {
    "text": "is you now have tools\nthat could allow you to take data and\nany probabilistic model",
    "start": "397970",
    "end": "404360"
  },
  {
    "text": "and choose good\nvalues for parameters. An important notation,\nthough, that we're",
    "start": "404360",
    "end": "410830"
  },
  {
    "text": "going to see a lot is\neven though there's always random variables, like for\nexample, in this MAP equation,",
    "start": "410830",
    "end": "417639"
  },
  {
    "text": "there should be a random\nvariable for the parameters, there should be a random\nvariable for the first data",
    "start": "417640",
    "end": "422680"
  },
  {
    "text": "point, there should be a random\nvariable for the n-th data point. And realistically, the\nprobabilistic statements should be talking\nabout equalities",
    "start": "422680",
    "end": "429100"
  },
  {
    "text": "of the random variables\ntaking on specific values. It's too much to write. Nobody writes out\nthe full expression.",
    "start": "429100",
    "end": "435580"
  },
  {
    "text": "Instead, people will often\nwrite that same expression using shorthand\nlike below where you",
    "start": "435580",
    "end": "441789"
  },
  {
    "text": "say, how likely are\nthe parameters given the data points? It implies that there\nis a random variable",
    "start": "441790",
    "end": "447430"
  },
  {
    "text": "for each of these non-random\nobserved variables. So like x1, that's\nthe observed x1,",
    "start": "447430",
    "end": "454479"
  },
  {
    "text": "and it's implied\nthat we're talking about the event of\nthe random variable x1 taking on that value.",
    "start": "454480",
    "end": "460660"
  },
  {
    "text": "I don't want to confuse\nanyone on notation. Certainly in lots of\nother probability classes, they'll just show you\nthis and work with it.",
    "start": "460660",
    "end": "467440"
  },
  {
    "text": "And people will just\nbe like, OK, great, it's the likelihood of the\nparameters given the data. If you want to get\nreally, really specific,",
    "start": "467440",
    "end": "474560"
  },
  {
    "text": "this is the slide\nthat you could use for making sure you're\nunderstanding the notation.",
    "start": "474560",
    "end": "480740"
  },
  {
    "text": "In this review, I did\nwant to just give you another perspective\non MAP and I wanted",
    "start": "480740",
    "end": "485900"
  },
  {
    "text": "to combine it with what we\nlearned about with betas. So in a beta\ndistribution, you're",
    "start": "485900",
    "end": "491150"
  },
  {
    "text": "estimating a single\nparameter, which is the p of a Bernoulli\nor a binomial.",
    "start": "491150",
    "end": "496640"
  },
  {
    "text": "And when you're estimating\nthat p of binomial, after you've seen data and after\nyou've incorporated your prior,",
    "start": "496640",
    "end": "502700"
  },
  {
    "text": "you calculate what we\ncall the posterior beta. And beta distributions\nstory ended there.",
    "start": "502700",
    "end": "508960"
  },
  {
    "text": "Maximally posterior,\nif you are estimating that particular parameter\nwould then take this posterior",
    "start": "508960",
    "end": "514539"
  },
  {
    "text": "and it's just going to choose\nwhichever argument maximizes the likelihood of the parameter.",
    "start": "514539",
    "end": "519969"
  },
  {
    "text": "That's also called the mode. So the mode is the value\nthat is most likely.",
    "start": "519970",
    "end": "526390"
  },
  {
    "text": "OK, I'm almost done with review. And maybe if there is just\none thing to really focus on",
    "start": "526390",
    "end": "531940"
  },
  {
    "text": "in this review, it\nwould be this takeaway about MAP for Bernoulli.",
    "start": "531940",
    "end": "537850"
  },
  {
    "text": "So when we learned\nabout betas, we said, if you're estimating\nthe p of a Bernoulli or a binomial, same\nthing, you could",
    "start": "537850",
    "end": "545560"
  },
  {
    "text": "imagine that before\nyou see anything, you can have your belief in\nthe probability expressed",
    "start": "545560",
    "end": "550839"
  },
  {
    "text": "as a beta. Then you see some\ndata and you end up with what we call the posterior\nbelief, your belief after data,",
    "start": "550840",
    "end": "556900"
  },
  {
    "text": "and that was a beta as well. And if you took the\nmode of that beta, then you'd get the\nMAP prediction,",
    "start": "556900",
    "end": "562790"
  },
  {
    "text": "which is given by\nthis little formula. It leaves this question\nfor the user, which",
    "start": "562790",
    "end": "568450"
  },
  {
    "text": "is, what was your prior belief? Can I just give you\nthe prior belief that I always use if I\ndon't have any other reason",
    "start": "568450",
    "end": "574840"
  },
  {
    "text": "to choose a different prior? I use a particular prior,\nwhich is a beta 2, 2.",
    "start": "574840",
    "end": "580560"
  },
  {
    "text": "That's imagining 1 imaginary\nsuccess and 1 imaginary failure before I see anything.",
    "start": "580560",
    "end": "586210"
  },
  {
    "text": "And with this particular\nprior, you end up with this particular\nposterior, and the MAP estimate",
    "start": "586210",
    "end": "591990"
  },
  {
    "text": "is just this simple equation. So how many successes\ndid you see in your data?",
    "start": "591990",
    "end": "597870"
  },
  {
    "text": "Add 1 to that and divide\nthat by how many successes plus how many failures plus 2. And it's just a very, very,\nvery reasonable estimate.",
    "start": "597870",
    "end": "606180"
  },
  {
    "text": "This particular prior\nhas a fancy name called Laplace prior. And Laplace prior\nis just a fancy way",
    "start": "606180",
    "end": "611640"
  },
  {
    "text": "of saying one imaginary success\nand one imaginary failure before you get going. Yes, question?",
    "start": "611640",
    "end": "616832"
  },
  {
    "text": "Yeah, I was just going to ask,\nis there a particular reason why you picked 2,\n2 instead of 1, 1?",
    "start": "616832",
    "end": "622000"
  },
  {
    "text": "Yeah, because the\nbeta-- oh my gosh, like if somebody\nsays beta 2, 2-- ",
    "start": "622000",
    "end": "628860"
  },
  {
    "text": "so a beta 2, 2 means\nthat alpha equals 2. Do you remember what alpha was?",
    "start": "628860",
    "end": "634740"
  },
  {
    "text": "It was number of\nsuccesses plus 1. So 2, 2 means 1 imagined\nsuccess 1 imagined failure.",
    "start": "634740",
    "end": "641790"
  },
  {
    "text": "If you had beta 1, 1,\nthat's 0 imagined successes 0 imagined failures.",
    "start": "641790",
    "end": "647009"
  },
  {
    "text": "The Laplace actually has\na really good intuition for why you would\nwant the Laplace.",
    "start": "647010",
    "end": "654839"
  },
  {
    "text": "You could never end up\nwith a probability of 0 or a probability of 1. And for some reason, it's like\nno matter what I'm estimating,",
    "start": "654840",
    "end": "663269"
  },
  {
    "text": "whenever I'm estimating\na probability, I never want to think\nit's impossible. And 0 or 1 can represent\ndifferent versions",
    "start": "663270",
    "end": "669930"
  },
  {
    "text": "of impossible. But because of that, Laplace\nhas that nice property. It's the least assumption you\ncan make, guaranteeing you",
    "start": "669930",
    "end": "677699"
  },
  {
    "text": "you'll never get a 0. So you maybe never get\ndivide by 0 in your equation. OK, good, good, good question.",
    "start": "677700",
    "end": "684670"
  },
  {
    "text": "Yes? Can you help me\nunderstand what MAP is? Is it like a beta\ndistribution or is it",
    "start": "684670",
    "end": "691720"
  },
  {
    "text": "a way to get the\nbeta [INAUDIBLE]?? So MAP at its core\nis a philosophy.",
    "start": "691720",
    "end": "697725"
  },
  {
    "text": "It is a philosophy for how to\nestimate parameters in general. If you applied that philosophy\nto estimating the parameter",
    "start": "697725",
    "end": "704829"
  },
  {
    "text": "p in a binomial, you would\nend up with the beta. And you would end up\nwith not just the beta,",
    "start": "704830",
    "end": "710920"
  },
  {
    "text": "but you would end up\nwith this posterior. It's assuming a prior. But with this prior\nand the MAP philosophy",
    "start": "710920",
    "end": "717910"
  },
  {
    "text": "applied to the Bernoulli,\nyou get this posterior. So the data is just a\nway of having this belief",
    "start": "717910",
    "end": "724000"
  },
  {
    "text": "and updating it based on\nthe belief and MAP is that? Yeah, and MAP,\nthey're so related.",
    "start": "724000",
    "end": "730300"
  },
  {
    "text": "MAP will just choose one number,\nwhereas the beta represents a whole belief\ndistribution, but they're",
    "start": "730300",
    "end": "735310"
  },
  {
    "text": "so related because\nthey're both doing the exact same Bayesian thing. Good question.",
    "start": "735310",
    "end": "740710"
  },
  {
    "text": "OK, and big picture,\nphilosophically, we have two ways of saying\nparameters, and both of them",
    "start": "740710",
    "end": "746600"
  },
  {
    "text": "give us numbers for\nevery parameter. They don't give\nus distributions. They actually give\na cold, hard number",
    "start": "746600",
    "end": "751940"
  },
  {
    "text": "for every single parameter\nthey're supposed to estimate. OK, and that's what\nwe've got for review.",
    "start": "751940",
    "end": "759760"
  },
  {
    "text": "Shall we jump into it? Let's see some machine learning. So we've got our last estimator.",
    "start": "759760",
    "end": "765040"
  },
  {
    "text": "And it's time to start thinking\nabout machine learning. Let me introduce\nyou to the tasks you are going to be able to\ndo on your final problem set.",
    "start": "765040",
    "end": "772485"
  },
  {
    "text": "On your final\nproblem set, you're going to write two machine\nlearning algorithms. And those two machine\nlearning algorithms",
    "start": "772485",
    "end": "777668"
  },
  {
    "text": "can all do tasks that will apply\nto these different data sets. All the tasks are going\nto have training data.",
    "start": "777668",
    "end": "785060"
  },
  {
    "text": "And the training data is\ngoing to have both inputs and outputs. I'll talk about that\na little bit more.",
    "start": "785060",
    "end": "790070"
  },
  {
    "text": "This is how I\ncould introduce it. These aren't updating fast.",
    "start": "790070",
    "end": "796167"
  },
  {
    "text": "So I could introduce\nit with those equation, but let's come back\nto these equations, and let me explain to you what\nthese equations are saying.",
    "start": "796167",
    "end": "804070"
  },
  {
    "text": "Let's imagine your task is\nyou have some input movies and whether or not\nusers like them",
    "start": "804070",
    "end": "809589"
  },
  {
    "text": "and you want to predict\nif the user will like a particular output movie. Does that sound like\na reasonable task?",
    "start": "809590",
    "end": "815980"
  },
  {
    "text": " You'll be given a whole\nbunch of training data.",
    "start": "815980",
    "end": "821700"
  },
  {
    "text": "And in that training\ndata, you'll be given historical users. And for every\nhistorical user, you'll",
    "start": "821700",
    "end": "828380"
  },
  {
    "text": "be told what movies\ndid they like, and you'll be told the truth\nabout whether or not they",
    "start": "828380",
    "end": "834740"
  },
  {
    "text": "like the output movie. We think about this as a tuple,\na combination of the input",
    "start": "834740",
    "end": "840605"
  },
  {
    "text": "things that you would\nuse for your prediction and the true label\nof whether or not, for this historical\nexample, the person actually",
    "start": "840605",
    "end": "847040"
  },
  {
    "text": "did like the movie. And I'm going to give you\na whole bunch of these. And your machine\nlearning task is",
    "start": "847040",
    "end": "852740"
  },
  {
    "text": "to build a little system\nthat could take the inputs and predict the output.",
    "start": "852740",
    "end": "857880"
  },
  {
    "text": "So let's be clear, the\ninputs in this case",
    "start": "857880",
    "end": "863270"
  },
  {
    "text": "are all of these movies\nand whether or not the user liked\nthem, and the output is whether or not the user\nliked a particular target movie.",
    "start": "863270",
    "end": "870860"
  },
  {
    "text": "In this case, it's\nMiss Congeniality. Now, a little bit\nabout notation.",
    "start": "870860",
    "end": "877390"
  },
  {
    "text": "Notice how our inputs\nare x's and the thing we're trying to predict is y? I will always be\nconsistent about that.",
    "start": "877390",
    "end": "884009"
  },
  {
    "text": "Notice this superscript i. That means the i'th person\nin our training example.",
    "start": "884010",
    "end": "889920"
  },
  {
    "text": "And there's a very\nspecific reason why we use superscript for\ni and not subscript for i. And we'll use this\nnotation in 229.",
    "start": "889920",
    "end": "897089"
  },
  {
    "text": "They'll use this\nnotation in 221. They'll use this notation. It's because you see\nthis x, it's a little bit",
    "start": "897090",
    "end": "903360"
  },
  {
    "text": "weightier than the y. It's bold. And does anyone know why you\nwould bold a random variable",
    "start": "903360",
    "end": "909180"
  },
  {
    "text": "or a variable in general? Yeah? It' a vector. It's a vector!",
    "start": "909180",
    "end": "914280"
  },
  {
    "text": "Why is it a vector? Because vector is\nmathematics for a list. And it's a list of numbers.",
    "start": "914280",
    "end": "919530"
  },
  {
    "text": "It's not just one number. The x itself has a whole\nbunch of numbers in it. Because of that, when\nwe're writing it,",
    "start": "919530",
    "end": "925860"
  },
  {
    "text": "you'll sometimes see it bold. And there's just\none other thing I'd like to say about that\nvector notation, which is since xi is a\nlist, you may want",
    "start": "925860",
    "end": "935320"
  },
  {
    "text": "to talk about a particular\nelement in that list. And this is what the\nnotation looks like. And I'm giving you the\nfull professional version",
    "start": "935320",
    "end": "942368"
  },
  {
    "text": "so that in later classes\nyou'll be able to follow along using the\nexact same notation.",
    "start": "942368",
    "end": "947480"
  },
  {
    "text": "But if you want to talk\nabout this particular value, well, it's from the\ni-th user and this",
    "start": "947480",
    "end": "953320"
  },
  {
    "text": "could be the j-th value. In this particular\ncase, it's user number",
    "start": "953320",
    "end": "958600"
  },
  {
    "text": "two and this is movie number m. And because we use\nthe superscript,",
    "start": "958600",
    "end": "963880"
  },
  {
    "text": "it's easy to differentiate\nbetween the particular training data point we're on\nversus if you want",
    "start": "963880",
    "end": "969790"
  },
  {
    "text": "to talk about a\nparticular location within the list,\nwhich is the inputs.",
    "start": "969790",
    "end": "975950"
  },
  {
    "text": "Notation can be so complicated. Please, what's\nconfusing about this? ",
    "start": "975950",
    "end": "984149"
  },
  {
    "text": "Yeah? So if we were to\napply to this case, the superscript would\napply to which--",
    "start": "984150",
    "end": "989330"
  },
  {
    "text": "which row are we on? This is row 2. And then the subscript is\nwhat column with that row?",
    "start": "989330",
    "end": "995220"
  },
  {
    "text": "Exactly. Exactly. And hey, notice this, computer\nscientists like to be 0 index",
    "start": "995220",
    "end": "1000860"
  },
  {
    "text": "and mathematicians\nlike to be 1 index. It's caused all sorts\nof bugs everywhere. But just know that.",
    "start": "1000860",
    "end": "1007410"
  },
  {
    "text": "Doo doo doo doo doo. By 1 index, I mean they\noften think about user one, user two in notation.",
    "start": "1007410",
    "end": "1013320"
  },
  {
    "text": "OK, great. So we've got some notation. And the beautiful thing\nthat we're going to do is we're going to\ntalk about a lot",
    "start": "1013320",
    "end": "1019589"
  },
  {
    "text": "of different interesting\ntasks you might do that all have a very similar format. Another task you\nmight care about",
    "start": "1019590",
    "end": "1026280"
  },
  {
    "text": "is we're going to look at\ndifferent regions of interest, that's what ROI stands\nfor, in someone's heart.",
    "start": "1026280",
    "end": "1032310"
  },
  {
    "text": "And based on whether or\nnot we see abnormalities in those regions\nof interest, we're going to predict if they\nhave a healthy heart or not.",
    "start": "1032310",
    "end": "1039959"
  },
  {
    "text": "So again, this follows\nexactly the same format. If I talk about input, so\nx, from the first person",
    "start": "1039960",
    "end": "1047569"
  },
  {
    "text": "and the second value\nwithin that list, I might be talking about\nthis particular number, which is a 1.",
    "start": "1047569",
    "end": "1054160"
  },
  {
    "text": "So if I talk about this\nwhole second person, this whole second\nperson has two things.",
    "start": "1054160",
    "end": "1059320"
  },
  {
    "text": "They have all their inputs,\nthe regions of interest from which we'll make our\nprediction, and the output.",
    "start": "1059320",
    "end": "1065260"
  },
  {
    "text": "You're told whether or not this\nperson had a healthy heart. To be clear, the\ninteresting thing about this",
    "start": "1065260",
    "end": "1072029"
  },
  {
    "text": "is going to be based\non this training data, you make a machine so that\nif someone doesn't tell you",
    "start": "1072030",
    "end": "1077757"
  },
  {
    "text": "whether or not somebody\nhas a healthy heart, you can predict it. So you're going to\nbuild a machine that",
    "start": "1077757",
    "end": "1083130"
  },
  {
    "text": "can take inputs and make a\nprediction about the output. Yeah? So just based on\nthese two examples,",
    "start": "1083130",
    "end": "1089630"
  },
  {
    "text": "it seems like everything\nis quite binary, and it's just like\na true or false for both the inputs\nand the outputs.",
    "start": "1089630",
    "end": "1094970"
  },
  {
    "text": "Yes. So if were to make it not\nbinary, so for example, for the movies there was\nlike some sort of rating",
    "start": "1094970",
    "end": "1100550"
  },
  {
    "text": "that they could do, how much\nmore complicated would it be? It's a good question. Not that much more complicated.",
    "start": "1100550",
    "end": "1105960"
  },
  {
    "text": "So the question was, hey, Chris,\nI noticed that everything's binary. And you're astute. Everything is binary. I'm going to keep\nit binary because I",
    "start": "1105960",
    "end": "1112052"
  },
  {
    "text": "want to explain all the\ntheory kind of keeping things as simple as possible. And then your\nquestion was if you want to break that assumption\nhow much more complicated",
    "start": "1112052",
    "end": "1119179"
  },
  {
    "text": "will it be. Well, once you see\nthe algorithms, you'll realize, not that\nmuch more complicated, though it is so helpful to\nunderstand these algorithms",
    "start": "1119180",
    "end": "1126500"
  },
  {
    "text": "just thinking about the\nsimplest case, which is binary. Yes? These ROI images\nthat are going in,",
    "start": "1126500",
    "end": "1132300"
  },
  {
    "text": "are they just looking if that\nfeature is present on the heart and how does that actually work? So this is a real data set\nfrom real people's hearts.",
    "start": "1132300",
    "end": "1140600"
  },
  {
    "text": "And what doctors\nwould do is there would be different\nspecialists who could look at different\nregions and say whether or not",
    "start": "1140600",
    "end": "1146780"
  },
  {
    "text": "they saw something. And no specialist was able\nto predict healthy hearts.",
    "start": "1146780",
    "end": "1152070"
  },
  {
    "text": "But if you took all the\nspecialists together and their perspective\non each region, it turns out that\npeople were able to make",
    "start": "1152070",
    "end": "1158385"
  },
  {
    "text": "much better predictions\nabout healthy hearts. So the output is just measuring\nwhether the hearts overall are healthy? It's not like given\na new heart and then",
    "start": "1158385",
    "end": "1166527"
  },
  {
    "text": "testing if that's healthy? Yeah, but once you build this\nmachine, you could imagine a new person comes in.",
    "start": "1166527",
    "end": "1172090"
  },
  {
    "text": "Doctors look at each of\nthe regions of interest, we put those numbers those\n1's and 0's into the machine, and it says, this person's\ngot an unhealthy heart,",
    "start": "1172090",
    "end": "1178890"
  },
  {
    "text": "we need to go look\nfurther into it. Got it. Yes, great question.",
    "start": "1178890",
    "end": "1184320"
  },
  {
    "text": "A little bit more\non notation just to make sure we're\nfollowing along, if I talk about x2, that's the\ninput for the second person.",
    "start": "1184320",
    "end": "1191010"
  },
  {
    "text": "And if I talk about\ny2, that's the thing we're trying to predict,\nand it's the label that we were given in the\nsecond training example.",
    "start": "1191010",
    "end": "1199200"
  },
  {
    "text": "OK, and then another data\nset was ancestry classifier. So some companies\ncan look at DNA",
    "start": "1199200",
    "end": "1207149"
  },
  {
    "text": "and they can predict\nour ancestry. And the sorts of data they\nhave is whether or not there is a change at a different\nparticular nucleotide location.",
    "start": "1207150",
    "end": "1216498"
  },
  {
    "text": "We don't have to worry\ntoo much about that. But you'll be given m\ndifferent nucleotide locations.",
    "start": "1216498",
    "end": "1221517"
  },
  {
    "text": "And for each user,\nyou'll say whether or not they have a change\nat that location, and then you'll have an\noutput of particular ancestry",
    "start": "1221517",
    "end": "1228882"
  },
  {
    "text": "that you might want to predict.  So the question that was\nasked earlier, everything",
    "start": "1228883",
    "end": "1235200"
  },
  {
    "text": "I showed you was binary. And certainly there's lots of\nalgorithms that break this. You can have continuous values.",
    "start": "1235200",
    "end": "1241380"
  },
  {
    "text": "You can have discrete values. You could mix\ncontinuous and binaries. You could predict things\nthat are non-binary.",
    "start": "1241380",
    "end": "1247500"
  },
  {
    "text": "There's lots of cool\nextensions of this. But first understand\nthe core with binary, and then it'll be easy to\ntalk about these extensions.",
    "start": "1247500",
    "end": "1255750"
  },
  {
    "text": "So bringing it\nall together, this is how the problem\nis formulated. I'm going to give you a\nbunch of training data.",
    "start": "1255750",
    "end": "1263690"
  },
  {
    "text": "And from that training\ndata, your job is to build a classifying\nmachine, a machine that",
    "start": "1263690",
    "end": "1268910"
  },
  {
    "text": "can take in an input\nand predict an output. Every training data is\nan input/output pair.",
    "start": "1268910",
    "end": "1274799"
  },
  {
    "text": "So we call it labeled because\nit's like I gave you the input and I label whether or\nnot it's a 1 or a 0, and I give you n of these.",
    "start": "1274800",
    "end": "1282100"
  },
  {
    "text": "We talk about how many\nfeatures your training data has by looking at any\ninput and saying,",
    "start": "1282100",
    "end": "1287860"
  },
  {
    "text": "how long was that list,\nhow big was that vector? And that's how many features we\nsay your training data set has.",
    "start": "1287860",
    "end": "1296210"
  },
  {
    "text": "OK.  Machine learning is this\nprocess of taking training data",
    "start": "1296210",
    "end": "1303950"
  },
  {
    "text": "and learning parameters\nfor a probabilistic model. And it's ubiquitous. This is one particular\nversion of machine learning",
    "start": "1303950",
    "end": "1310640"
  },
  {
    "text": "that we call classification. And we call it\nclassification because you're predicting a 1 or a 0,\nyou're predicting the y.",
    "start": "1310640",
    "end": "1318920"
  },
  {
    "text": "And since the y is like one\nof a small set of classes, that's where we get this\nname classification.",
    "start": "1318920",
    "end": "1324169"
  },
  {
    "text": "There is other versions\nof machine learning. Maybe you're predicting\ncontinuous numbers. Maybe you're predicting actions.",
    "start": "1324170",
    "end": "1329450"
  },
  {
    "text": "There are other versions,\nbut classification is the heart and soul\nand where it all starts.",
    "start": "1329450",
    "end": "1335390"
  },
  {
    "text": "One metaphor I have for\nthis task you are given is you're making the\nHarry Potter sorting hat.",
    "start": "1335390",
    "end": "1341510"
  },
  {
    "text": "And I love to think about this. Everybody is curious about this\nHarry Potter sorting hat, what would it be like to see it.",
    "start": "1341510",
    "end": "1347390"
  },
  {
    "text": "And Stanford, not only do\nwe get to think about it but we get to learn,\nwhat could possibly go into making your\nown sorting hat?",
    "start": "1347390",
    "end": "1356530"
  },
  {
    "text": "This classification\nalgorithm that we're going to use the sorting\nhat as a little metaphor",
    "start": "1356530",
    "end": "1362530"
  },
  {
    "text": "for, you can imagine\nit looks at some input. So it gets this list of\nfeatures, 0's and 1's.",
    "start": "1362530",
    "end": "1368800"
  },
  {
    "text": "And its job is to\npredict, label a 1 or a 0. Are you guys following\nalong the task?",
    "start": "1368800",
    "end": "1374950"
  },
  {
    "text": "Ask questions if there's\nanything confusing, certainly worth clarifying\nbefore we jump into. How are we going to solve this?",
    "start": "1374950",
    "end": "1380425"
  },
  {
    "text": " Yeah, Naive Bayes\ncertainly is how we're",
    "start": "1380425",
    "end": "1386010"
  },
  {
    "text": "going to start by solving this. OK, so we've got these\nthree different problems.",
    "start": "1386010",
    "end": "1391250"
  },
  {
    "text": "Let's jump into one of\nthem, the Netflix one. And let's think about\nthis a little bit. So I've got a very simple\nversion of this problem.",
    "start": "1391250",
    "end": "1400040"
  },
  {
    "text": "My version for you is\nthat for every user, there's only one\nmovie in the inputs",
    "start": "1400040",
    "end": "1405679"
  },
  {
    "text": "that we're going to look,\nat Independence Day. And the output is going\nto predicting if somebody likes Life is Beautiful.",
    "start": "1405680",
    "end": "1411980"
  },
  {
    "text": "So every single value in\nour feature, it's 0, 1. And to be clear, every single\nvalue in our predictions",
    "start": "1411980",
    "end": "1417950"
  },
  {
    "text": "either is 0 or 1. 1 means they will\nlike the movie. 0 means they won't.",
    "start": "1417950",
    "end": "1423860"
  },
  {
    "text": "And I'm going to\ngently start us. How could we build a\nmachine learning algorithm to do this task if you\nonly had one feature input?",
    "start": "1423860",
    "end": "1433350"
  },
  {
    "text": "And I have an\nalgorithm for you, it is called brute\nBayes classifier.",
    "start": "1433350",
    "end": "1438929"
  },
  {
    "text": "And no one ever\nuses this algorithm, but my promise for you is\nthis is easier to understand.",
    "start": "1438930",
    "end": "1444870"
  },
  {
    "text": "And if you understand this\nand you see why it breaks, then understanding Naive Bayes\nwill make a lot of sense.",
    "start": "1444870",
    "end": "1450340"
  },
  {
    "text": "So you guys want to hear\nmy total hacky algorithm? Here's my hacky algorithm.",
    "start": "1450340",
    "end": "1456149"
  },
  {
    "text": "We are going to\nbuild this machine. And it's going to be a\nprobabilistic machine learning machine. And it's going to take in\ninputs, whether or not somebody",
    "start": "1456150",
    "end": "1463140"
  },
  {
    "text": "likes Independence Day. And it's going to predict\nan output whether or not they like Life is Beautiful.",
    "start": "1463140",
    "end": "1469300"
  },
  {
    "text": "Now, in general, the input\nwill be more than one number, but originally,\nthis input is just",
    "start": "1469300",
    "end": "1474580"
  },
  {
    "text": "going to be a single 1 or a 0. What I'm going to\nput into my machine",
    "start": "1474580",
    "end": "1481060"
  },
  {
    "text": "is I'm going to have a little\nfunction which calculates how likely is it that\nthe output is a 1",
    "start": "1481060",
    "end": "1488200"
  },
  {
    "text": "given the inputs I observe. And if the probability\nthat the output is 1 given the inputs I\nobserve is high enough,",
    "start": "1488200",
    "end": "1495630"
  },
  {
    "text": "then I'll predict a 1. And particularly,\nI might do this.",
    "start": "1495630",
    "end": "1501920"
  },
  {
    "text": "If I can say, put a 0 into this.",
    "start": "1501920",
    "end": "1507990"
  },
  {
    "text": "And if I put a 0 into this and\nit says, the probability that y equals 0, given the\ninputs I observe, is 0.62,",
    "start": "1507990",
    "end": "1514950"
  },
  {
    "text": "then maybe I should\nbe predicting a 0. Does that make sense? I'm just going to figure out the\nprobability of the output given",
    "start": "1514950",
    "end": "1522750"
  },
  {
    "text": "the input. Straightforward, right? Seems like a\nreasonable thing to do. You gave me an input value.",
    "start": "1522750",
    "end": "1528330"
  },
  {
    "text": "I calculate the\nprobability of the output. And if the probability\nof the output being 0 is greater than\n0.5, I predict 0.",
    "start": "1528330",
    "end": "1534029"
  },
  {
    "text": "Otherwise, I predict 1. Now, hopefully, if I put\nin a 0 and I got 0.62.",
    "start": "1534030",
    "end": "1540659"
  },
  {
    "text": "Then if I put y in\nas 1, the probability that y takes on\nthe value 1 will be whatever makes those\ntwo things add up to 1.",
    "start": "1540660",
    "end": "1547620"
  },
  {
    "text": "Making sense? Yeah. So when you say output, does\nthat mean specifically 1, or, 0",
    "start": "1547620",
    "end": "1553300"
  },
  {
    "text": "or any output? This whole machine\nlearning algorithm has to give back a 1 or 0. 1 meaning you think they're\ngoing to like the movie.",
    "start": "1553300",
    "end": "1560440"
  },
  {
    "text": "0 means you think they won't. So classification, you don't\nhave to give me anything other than a 1 or a 0.",
    "start": "1560440",
    "end": "1567250"
  },
  {
    "text": "But the probability\ndoesn't matter in the output that's given. Yeah, I'm just going to use\nthe probability as a step",
    "start": "1567250",
    "end": "1573039"
  },
  {
    "text": "to get to the answer. If the probability of y\nequals 1 is less than 0.5, then I predict 0.",
    "start": "1573040",
    "end": "1578350"
  },
  {
    "text": "So it's a step towards\nmy final answer, which will be a 1 or a 0. Very good question. OK, now to put this a\nlittle bit mathematically,",
    "start": "1578350",
    "end": "1587630"
  },
  {
    "text": "one way of saying\nthis is I'm going to choose a 0 if the\nprobability that y equals 0 given x is greater\nthan if you put a 1 in for this.",
    "start": "1587630",
    "end": "1597480"
  },
  {
    "text": "So if you put in a 1\nfor probability of y equals 1 given x,\nif that's larger than if you put a 0\nin for this y, then",
    "start": "1597480",
    "end": "1604247"
  },
  {
    "text": "we're going to be predicting 1. So just in some\nnotation, I'm just going to say it's the argmax, or\nmy choice of y's, whichever one",
    "start": "1604248",
    "end": "1610730"
  },
  {
    "text": "is larger for the probability\nof that value given x.",
    "start": "1610730",
    "end": "1616429"
  },
  {
    "text": "So here's how I want\nto make my prediction. It's either going to be a 0, 1.",
    "start": "1616430",
    "end": "1621770"
  },
  {
    "text": "And it's going to\nbe whichever one makes this probability large. So well, just to\nbe clear, y hat is",
    "start": "1621770",
    "end": "1628865"
  },
  {
    "text": "going to be a prediction will\nthey like Life is Beautiful and x is going to\nbe whether or not they liked Independence Day.",
    "start": "1628865",
    "end": "1634450"
  },
  {
    "text": "And I call this\nbrute force Bayes because it turns out maybe\nestimating this probability is",
    "start": "1634450",
    "end": "1639657"
  },
  {
    "text": "a little hard. So I'm going to use\nsome Bayes' theorem. Just to be clear,\nif y equals 1, that",
    "start": "1639657",
    "end": "1644840"
  },
  {
    "text": "means they like\nLife is Beautiful. So I'm going to use some\nBayes' theorem here. I'm going to say, OK, this is\nthe same as if I took that term",
    "start": "1644840",
    "end": "1652500"
  },
  {
    "text": "and I replaced it with Bayes'\ntheorem-- the probability of x given y times probability y\ndivided by probability of x.",
    "start": "1652500",
    "end": "1658760"
  },
  {
    "text": "Beautiful thing about\nargmax is that argmax doesn't care about constants. So this normalization constant\non the bottom, it's a constant.",
    "start": "1658760",
    "end": "1665940"
  },
  {
    "text": "It doesn't change as y changes. Therefore, it doesn't\naffect whichever 0, 1",
    "start": "1665940",
    "end": "1671360"
  },
  {
    "text": "is going to be\nthe argument which maximizes this expression. It goes away. Fantastic.",
    "start": "1671360",
    "end": "1677210"
  },
  {
    "text": "So I haven't done too much here. I've just basically done-- I'm going to do Bayes' theorem.",
    "start": "1677210",
    "end": "1683640"
  },
  {
    "text": "And I'm going to get rid of\nthe normalization constant because normalization constant\ngets lost in the argmax.",
    "start": "1683640",
    "end": "1690650"
  },
  {
    "text": "Questions about that? Such a weird thing, the\nnormalization constant, like why doesn't it change as\nyou put in-- if y was 0 or y",
    "start": "1690650",
    "end": "1699260"
  },
  {
    "text": "was 1. It's crazy to think that\nthe probability of x doesn't change. That's mind blowing.",
    "start": "1699260",
    "end": "1704370"
  },
  {
    "text": "But I promise you\nit doesn't change. It is a constant. That's why it got his name. Because it's a constant, it\ndoesn't affect the argmax.",
    "start": "1704370",
    "end": "1712330"
  },
  {
    "text": "I don't really understand\nwhy we go from probability of y given x to argmax of y.",
    "start": "1712330",
    "end": "1719240"
  },
  {
    "text": "Oh, yeah, because the output\nof this little machine isn't supposed to\nbe a probability. The output of this machine\nhas got to be a 0 or a 1.",
    "start": "1719240",
    "end": "1726340"
  },
  {
    "text": "And so the argmax is just a\nreally fancy way of saying, I'm going to turn this\nprobability, which",
    "start": "1726340",
    "end": "1732190"
  },
  {
    "text": "is a continuous number\nbetween 0 and 1. And I'm going to just discretize\nit into either 0 or a 1.",
    "start": "1732190",
    "end": "1739760"
  },
  {
    "text": "That makes sense? It does a little. Yeah, if you go\nback to machine-- OK, here's my claim.",
    "start": "1739760",
    "end": "1744980"
  },
  {
    "text": "No, it should make sense. The machine doesn't give\nyou back a probability. The machine's got to\ngive you back 0 or 1.",
    "start": "1744980",
    "end": "1751490"
  },
  {
    "text": "So if you can calculate the\nprobability that y equals 1, you're not done.",
    "start": "1751490",
    "end": "1757830"
  },
  {
    "text": "You have to do the\nsimplest thing. If you figure out the\nprobability y equals 1-- let's say I told you the\nprobability y equals 1 is 0.7.",
    "start": "1757830",
    "end": "1765200"
  },
  {
    "text": "What do you do to predict?  [INAUDIBLE] Oh, no much easier.",
    "start": "1765200",
    "end": "1771530"
  },
  {
    "text": "I just told you that\nbased on your x, I told you that the\nprobability that y equals 1. So if I say the\nprobability that y",
    "start": "1771530",
    "end": "1778600"
  },
  {
    "text": "equals 1 given the\nx that you observed. If I tell you that\nthis number is 0.7,",
    "start": "1778600",
    "end": "1786850"
  },
  {
    "text": "I'm telling you that\nbased on your inputs, the probability that\nthe output is 1 is 0.7.",
    "start": "1786850",
    "end": "1793890"
  },
  {
    "text": "You have to predict something. Do you predict that y is 1 or 0? 1. Yeah, so this gets turned\ninto a prediction of 1.",
    "start": "1793890",
    "end": "1801820"
  },
  {
    "text": " But rounds it up.",
    "start": "1801820",
    "end": "1807910"
  },
  {
    "text": "Yeah, exactly. Argmax is a fancy way\nof saying round it up. Now, argmax is a little\nbit more elegant than just",
    "start": "1807910",
    "end": "1815140"
  },
  {
    "text": "saying round it up. The reason argmax is a little\nbit more elegant than rounding it up is it saying,\nwell, I'm going",
    "start": "1815140",
    "end": "1821350"
  },
  {
    "text": "to choose whichever\none is bigger-- y equals 0 or y equals 1. And the reason it's nicer to\nput it as an argmax is exactly",
    "start": "1821350",
    "end": "1828250"
  },
  {
    "text": "this part, where\nround it up wouldn't allow you to get rid of\na normalization constant,",
    "start": "1828250",
    "end": "1834312"
  },
  {
    "text": "whereas argmax which is doing\nthe same thing as rounding it up has this really nice\nproperty that if you ever have a constant, it just\ndoesn't come into your equation.",
    "start": "1834312",
    "end": "1841215"
  },
  {
    "text": "And you don't have to bother\ncalculating probability of x. Good question. Does it make sense now? Yeah. OK, fantastic.",
    "start": "1841215",
    "end": "1846603"
  },
  {
    "text": "That's critical. If there's something\nthat doesn't make sense, just stop me. This is not too much\ncontent in this lecture. I just want to make sure\neverybody understands it",
    "start": "1846603",
    "end": "1853092"
  },
  {
    "text": "because you're going\nto have to code it up. OK, so here's my\nbrute force Bayes.",
    "start": "1853092",
    "end": "1859220"
  },
  {
    "text": "It's just our warm\nup to Naive Bayes. And the simple idea is I've\ngot a tiny little probability",
    "start": "1859220",
    "end": "1864470"
  },
  {
    "text": "engine. And it's going to\npredict 1 or 0. Now, in order to do this,\nI'm going to be able--",
    "start": "1864470",
    "end": "1871130"
  },
  {
    "text": "I need to be able\nto know these terms. I have to know the\nprobability of y equals 1",
    "start": "1871130",
    "end": "1876140"
  },
  {
    "text": "and the probability\nof y equals 0. And I have to know\nthe probability that x equals 0 given y equals 0.",
    "start": "1876140",
    "end": "1881870"
  },
  {
    "text": "The probability x equals\n1, given y equals 0. Probability of x equals\n0 given y equals 1. And probability of x\nequals 1, given y equals 1.",
    "start": "1881870",
    "end": "1890590"
  },
  {
    "text": "So what are the parameters? I told you, machine\nlearning, you'd have these little\nprobabilistic machine.",
    "start": "1890590",
    "end": "1896860"
  },
  {
    "text": "We'd estimate the parameters. And then using those parameters,\nwe could make predictions. Where are the parameters here?",
    "start": "1896860",
    "end": "1902755"
  },
  {
    "text": " And my answer for you\nis that the parameters",
    "start": "1902755",
    "end": "1909620"
  },
  {
    "text": "are in these two things\nthat you have to calculate. Your brute Force Bayes' going\nto make a prediction based off",
    "start": "1909620",
    "end": "1915890"
  },
  {
    "text": "these. And so you need to be able to--\nwhen you're coding this up, you have to say, what's the\nprobability that y equals 0",
    "start": "1915890",
    "end": "1921650"
  },
  {
    "text": "and what's the\nprobability y equals 1? And then for any value in this\nconditional probability table,",
    "start": "1921650",
    "end": "1927870"
  },
  {
    "text": "if y equals 0 and x\nequals 0, you need to say, what's the probability that x\nequals 0 given that y equals 0?",
    "start": "1927870",
    "end": "1936150"
  },
  {
    "text": "So you need 1, 2, 3,\n4, 5, 6 parameters.",
    "start": "1936150",
    "end": "1942190"
  },
  {
    "text": "And if you could estimate\nall these parameters from your training\ndata, then you'd have a probabilistic model.",
    "start": "1942190",
    "end": "1947610"
  },
  {
    "text": "And you could make predictions. I'm going to take a\nmoment here, now that I've told you about the parameters.",
    "start": "1947610",
    "end": "1952960"
  },
  {
    "text": "And I want you to talk to\nthe person next to you. And I want you guys to do one\nof those, like, ask the person, hey, what's\nconfusing about this?",
    "start": "1952960",
    "end": "1958690"
  },
  {
    "text": "And see if you can generate\na good question for me. So what's confusing about this? Here's my parameters. Think about it for a second\nwith the person next to you.",
    "start": "1958690",
    "end": "1965770"
  },
  {
    "text": "And then we'll talk\nabout it all together. OK, go for it. [SIDE CONVERSATIONS]",
    "start": "1965770",
    "end": "1971122"
  },
  {
    "start": "1971122",
    "end": "2057649"
  },
  {
    "text": "OK, hopefully that\nwas a good chance to come up with a good question. I took some-- I took that moment to write up--",
    "start": "2057650",
    "end": "2065690"
  },
  {
    "text": "there's three critical\nrandom variables here, x, the input movies.",
    "start": "2065690",
    "end": "2070728"
  },
  {
    "text": "In this case, Independence Day. Did they like the input movies? And that will take\non some values. y is, do they like\nthe target movie?",
    "start": "2070728",
    "end": "2078020"
  },
  {
    "text": "And I just want to point\nout that y and y hat are slightly different. y is\ndo they like the target movie.",
    "start": "2078020",
    "end": "2083658"
  },
  {
    "text": "And y hat is your prediction\nfor whether or not they like the target movie. And so we do have slightly\ndifferent notation",
    "start": "2083659",
    "end": "2089294"
  },
  {
    "text": "for predictions versus\nthe actual truth. OK, questions that came up.",
    "start": "2089295",
    "end": "2094369"
  },
  {
    "text": "Yes. We were wondering why you had\nsix parameters to be like--",
    "start": "2094370",
    "end": "2100859"
  },
  {
    "text": "that we need to find. So just like four because\nwe have four data points.",
    "start": "2100860",
    "end": "2106200"
  },
  {
    "text": "I could have a lot\nmore data points. I could give you hundreds. I can give you--\nactually, this is Netflix.",
    "start": "2106200",
    "end": "2111330"
  },
  {
    "text": "I got a million data\npoints for you, no problem. So I have a million data\npoints, but there's still",
    "start": "2111330",
    "end": "2117480"
  },
  {
    "text": "another question which\nis, do you actually need all these parameters? I could hone in on\nthese two parameters.",
    "start": "2117480",
    "end": "2125850"
  },
  {
    "text": "If I told you that this\nnumber was the probability that y equals 0. If you want to know\nprobability of y equals 1",
    "start": "2125850",
    "end": "2132477"
  },
  {
    "text": "but you knew the\nprobability of y equals 0, could you figure it out? There's a little bit\nof redundancy here.",
    "start": "2132478",
    "end": "2137618"
  },
  {
    "text": "We're not going to\nworry too much about it, but you could have a slightly\nmore compressed parameter set if you wanted to.",
    "start": "2137618",
    "end": "2143280"
  },
  {
    "text": "But this is the\nsimple algorithm. We're not going to worry\nabout optimizations. Is the x vector here\nrepresenting a single user",
    "start": "2143280",
    "end": "2150380"
  },
  {
    "text": "or a single data area? Oh, good question. So the x-- on some\nlevel when you train,",
    "start": "2150380",
    "end": "2156859"
  },
  {
    "text": "you'll be given many users. And for each user, you\ncould be given many movies. In this case though, x is\ngoing to be for the i-th user.",
    "start": "2156860",
    "end": "2164550"
  },
  {
    "text": "So we're going to talk about-- particularly for like-- this\nis after you've trained. So a user comes in and\nthey like list of movies x.",
    "start": "2164550",
    "end": "2173690"
  },
  {
    "text": "And in this case,\nthe list is just one because we're doing it\nfor the simple case where there's only one thing we're\nbasing our decision off of.",
    "start": "2173690",
    "end": "2180000"
  },
  {
    "text": "So if we wanted to do same\nthing but for a ton of people, you just basically\nfor loop over this? Exactly, you would\njust for loop over this",
    "start": "2180000",
    "end": "2186290"
  },
  {
    "text": "and you'd make your\nprediction for each person. Yes, good question. Yes. Wouldn't we also need\na variable called x",
    "start": "2186290",
    "end": "2192800"
  },
  {
    "text": "hat for that specific user? You know what?",
    "start": "2192800",
    "end": "2198030"
  },
  {
    "text": "That's a really good question. We're always told their history. You don't have to\nguess at their history because I tell you,\nfor x's, whether or not",
    "start": "2198030",
    "end": "2205119"
  },
  {
    "text": "they like this movie. There's no guessing involved. So how is x different from the--",
    "start": "2205120",
    "end": "2213160"
  },
  {
    "text": "y? Yeah, y. OK, good, good, good.\nx is like I tell you",
    "start": "2213160",
    "end": "2218740"
  },
  {
    "text": "this about the person and you\nhave to predict this stuff. So the x is the\nstuff you're told.",
    "start": "2218740",
    "end": "2223900"
  },
  {
    "text": "And y is the stuff\nyou're predicting. How's that [INAUDIBLE]? So you're going to\nmake a prediction.",
    "start": "2223900",
    "end": "2230349"
  },
  {
    "text": "And we have two things to\nseparate the guess you made and what we know, whether\nor not they actually",
    "start": "2230350",
    "end": "2237733"
  },
  {
    "text": "turn out to like this movie. This is what we call unobserved. You genuinely don't know\nif they like the movie.",
    "start": "2237733",
    "end": "2246109"
  },
  {
    "text": "And so you just make\nyour prediction. And then maybe in\nthe future you'll observe if you got it right.",
    "start": "2246110",
    "end": "2251830"
  },
  {
    "text": "OK. I just want to be clear\nabout those two things. Yes. Could you explain, again,\nhow the six [INAUDIBLE]",
    "start": "2251830",
    "end": "2259060"
  },
  {
    "text": "probabilities? Yeah, each of these is\ngoing to be a probability. And so there's all\nthese different things.",
    "start": "2259060",
    "end": "2265049"
  },
  {
    "text": "You should know the\nprobability that x-- whether or not they like\nIndependence Day given",
    "start": "2265050",
    "end": "2271778"
  },
  {
    "text": "whether or not they\nlike Life is Beautiful. You should know that problem. And if you know each\nof these probabilities,",
    "start": "2271778",
    "end": "2276780"
  },
  {
    "text": "then we'll be able to calculate\nthis expression no matter what x or y are. If you told me each of these\nprobabilities-- so yes,",
    "start": "2276780",
    "end": "2284070"
  },
  {
    "text": "we should estimate all\nof those probabilities. Every theta will\nbe a probability.",
    "start": "2284070",
    "end": "2289820"
  },
  {
    "text": "Do you want to know how we\nget those probabilities? Counting. Yeah, yeah. On some level, counting\nis a very good answer.",
    "start": "2289820",
    "end": "2298650"
  },
  {
    "text": "And just to be clear, I'm\ngoing to put all these into a conditional\nprobability table which says, condition on y's value,\nthe probability of x",
    "start": "2298650",
    "end": "2306230"
  },
  {
    "text": "taking on these values. And I'm just going to put it all\ninto one little structure here.",
    "start": "2306230",
    "end": "2311539"
  },
  {
    "text": "OK, there is a question. How do you take\nyour million users",
    "start": "2311540",
    "end": "2317359"
  },
  {
    "text": "and estimate all of\nthese probabilities? And then you just said\nit, you're like counting. What a reasonable thing to do?",
    "start": "2317360",
    "end": "2325370"
  },
  {
    "text": "MLE says you should just count. You can think of\nthis as a binomial",
    "start": "2325370",
    "end": "2330860"
  },
  {
    "text": "and you're estimating\nthe probability p. And MLE says, just count. You'll say, what's\nthe fraction of users",
    "start": "2330860",
    "end": "2337490"
  },
  {
    "text": "who like this movie\ngiven whether or not they liked the movie y?",
    "start": "2337490",
    "end": "2343010"
  },
  {
    "text": "And if it turns out nobody did,\nyou'll get a probability of 0. And you could end up\nwith a probability of 1",
    "start": "2343010",
    "end": "2349130"
  },
  {
    "text": "if everybody who watched\nthis movie liked it. So MLE says, just count.",
    "start": "2349130",
    "end": "2354350"
  },
  {
    "text": "Figure out what fraction of\npeople who liked movie y also liked movie x.",
    "start": "2354350",
    "end": "2359960"
  },
  {
    "text": "MAP has a different opinion. MAP says, just count, but\nsince this is a binomial",
    "start": "2359960",
    "end": "2367550"
  },
  {
    "text": "like we talked\nabout in review, you can add in your imaginary\ntrials for your beta prior.",
    "start": "2367550",
    "end": "2373029"
  },
  {
    "text": "And as I told you, the\nimaginary number of trials that I like to use is what's\ncalled the Laplace prior, which",
    "start": "2373030",
    "end": "2378420"
  },
  {
    "text": "is basically, you add\nin one imaginary trial for each outcome. So if y equals 0, I'm going\nto add one imaginary trial",
    "start": "2378420",
    "end": "2385860"
  },
  {
    "text": "for x equals 0 and 1 imaginary\ntrial for x equals 1. So you can imagine\nyou're like, this here",
    "start": "2385860",
    "end": "2392580"
  },
  {
    "text": "is a beta p for binomial\nthat I'm estimating. And I can use MLE or\nI can use MAP for it.",
    "start": "2392580",
    "end": "2401280"
  },
  {
    "text": "MAP or MLE, they're\nboth very, very similar. They're both basically\njust counting.",
    "start": "2401280",
    "end": "2407339"
  },
  {
    "text": "It's just MAP adds in\none imaginary trial for each outcome.",
    "start": "2407340",
    "end": "2412540"
  },
  {
    "text": "At this point, I've\ngiven you a theory. I've told you how we\ncan get training data",
    "start": "2412540",
    "end": "2418462"
  },
  {
    "text": "and we can estimate\nthe parameters. What I haven't told you\nis what it actually looks like to make our prediction. So let's just go through this.",
    "start": "2418462",
    "end": "2424700"
  },
  {
    "text": "Let's imagine you've learned all\nthese probabilities from data. You learned all your parameters. Here's how you could use it.",
    "start": "2424700",
    "end": "2431470"
  },
  {
    "text": "A new user shows up and\nthey like Independence Day. So that means that x1 is 1.",
    "start": "2431470",
    "end": "2437740"
  },
  {
    "text": "And we can say, OK, we're\ngoing to argmax this. And we're going to\nloop over the value",
    "start": "2437740",
    "end": "2443320"
  },
  {
    "text": "y equals 0 and y equals 1. Let's start with y equals 0. This expression, when\nyou put in y equals 0",
    "start": "2443320",
    "end": "2449620"
  },
  {
    "text": "and you take into account\nthat x1 was 1, becomes this. It becomes probability\nof x1 equals 1,",
    "start": "2449620",
    "end": "2456100"
  },
  {
    "text": "given y equals 0 times the\nprobability y equals 0. And this is just\ngoing to be taking",
    "start": "2456100",
    "end": "2461460"
  },
  {
    "text": "in two different numbers. Probability of y equals 0,\nwe've got a parameter for that. The probability that x1\nequals 1 given y equals 0,",
    "start": "2461460",
    "end": "2468270"
  },
  {
    "text": "we've got a parameter for that\nin our conditional probability table. You just multiply\nthose two things.",
    "start": "2468270",
    "end": "2473790"
  },
  {
    "text": "Now, we should also\nlook into the case that maybe they do\nlike Life is Beautiful.",
    "start": "2473790",
    "end": "2479049"
  },
  {
    "text": "So if we set y equals\n1, we can take, what's the probability of y equals 1? We know that parameter.",
    "start": "2479050",
    "end": "2484350"
  },
  {
    "text": "And we can know, what's the\nprobability that x1 equal to 1 given that y equals 1? We know that parameter. And so we can just\nmultiply those two things",
    "start": "2484350",
    "end": "2490890"
  },
  {
    "text": "and we get a different number. This was the expression\nwhen y equals 0.",
    "start": "2490890",
    "end": "2497580"
  },
  {
    "text": "This was the expression\nwhen y equals 1. What value of y is the\nargument that maximizes?",
    "start": "2497580",
    "end": "2503670"
  },
  {
    "text": "You guys can just say it. [INAUDIBLE] But say it with confidence.",
    "start": "2503670",
    "end": "2508920"
  },
  {
    "text": "1. Yeah, fantastic. It is in fact 1. Notice how I don't change x.",
    "start": "2508920",
    "end": "2514348"
  },
  {
    "text": "I was told that they\nlike Independence Day. I'm not going to be\nmessing around with x1. I know it's a 1. What I'm trying to guess\nis why and so this argmax",
    "start": "2514348",
    "end": "2522109"
  },
  {
    "text": "tries the argument y equals 0. It tries the\nargument y equals 1. Which everyone had\na higher expression,",
    "start": "2522110",
    "end": "2527210"
  },
  {
    "text": "that will be my output. So I'll be predicting, in\nthis case, that y equals 1, they will in fact like\nLife is Beautiful.",
    "start": "2527210",
    "end": "2534200"
  },
  {
    "text": "Yes. This may not apply\nhere, but should the law",
    "start": "2534200",
    "end": "2540180"
  },
  {
    "text": "of total probability say\nthat both of those numbers should be adding up to 1?",
    "start": "2540180",
    "end": "2545430"
  },
  {
    "text": "They will not add up to 1. Why don't they add up to 1?",
    "start": "2545430",
    "end": "2552980"
  },
  {
    "text": "That's such an\ninteresting question. You're right. They don't. You're glancing at this\nas, this is less than 5.",
    "start": "2552980",
    "end": "2560869"
  },
  {
    "text": "And that's 0.2. They don't add up to 1. Oh, my god. Did we do our math wrong? No, everything about\nthis math is fine.",
    "start": "2560870",
    "end": "2566630"
  },
  {
    "text": "What happened? An idea. That is because we're not\nconsidering the case where they don't like Independence.",
    "start": "2566630",
    "end": "2572850"
  },
  {
    "text": "And if we did and we use all the\nconditional probabilities then we'd get to 1. That's a very good guess.",
    "start": "2572850",
    "end": "2577982"
  },
  {
    "text": "It's a very good guess. That's not in fact it, but that\nwas a very, very good guess. There's another thing\nthat we got rid of.",
    "start": "2577982",
    "end": "2586250"
  },
  {
    "text": "It's your question. Is it because we got rid of\nthe normalization constant? It's because we got rid of\nthe normalization constant.",
    "start": "2586250",
    "end": "2593180"
  },
  {
    "text": "This is no longer a probability. If this were to be the\nprobability that y equals 0,",
    "start": "2593180",
    "end": "2599359"
  },
  {
    "text": "you'd have to divide it by\nthe normalization constant. Similarly, if you want this\nto be the probability that y equals 1, You'd\nhave to divide it",
    "start": "2599360",
    "end": "2605030"
  },
  {
    "text": "by the normalization constant. Because we got rid\nof those two numbers, they're no longer\ntrue probabilities. And it's no longer the case\nthat this is the probability",
    "start": "2605030",
    "end": "2611960"
  },
  {
    "text": "that y equals 0 and this\nprobability that y equals 1. Instead, these are\nthe expressions. Whichever one is largest\nwill be our prediction.",
    "start": "2611960",
    "end": "2619960"
  },
  {
    "text": "Yeah, question, question. Why are we trying to predict x? We're not trying to predict x.",
    "start": "2619960",
    "end": "2626290"
  },
  {
    "text": "x is always 1. Then why are we\ndoing the [INAUDIBLE]",
    "start": "2626290",
    "end": "2633640"
  },
  {
    "text": "were trying to work out\nthe probability of Life is Beautiful? Yeah, it was just Bayes' theorem\nflipped that conditional.",
    "start": "2633640",
    "end": "2640424"
  },
  {
    "text": "So Bayes' theorem\nsaid it's going to be the probability that they\nlike Independence Day given whether or not they like y.",
    "start": "2640425",
    "end": "2646153"
  },
  {
    "text": "[INAUDIBLE] we actually\nhave to calculate both terms because even one of\nthe most [INAUDIBLE] is nothing but [INAUDIBLE].",
    "start": "2646153",
    "end": "2651410"
  },
  {
    "text": "Exactly, because when\nyou calculate this one and it is 0.2, you're not\ndone because you didn't do the normalization constant.",
    "start": "2651410",
    "end": "2657113"
  },
  {
    "text": "So you have to do this term. And you have to do\nboth of them otherwise or alternatively, you\ncould go and figure out",
    "start": "2657113",
    "end": "2662329"
  },
  {
    "text": "what the normalization\nconstant was, but we don't like to do that. We like to avoid the\nnormalization constant. Yes.",
    "start": "2662330",
    "end": "2667700"
  },
  {
    "text": "So can we use these results in\nan actual probability context comparatively saying,\nit's twice as likely",
    "start": "2667700",
    "end": "2674120"
  },
  {
    "text": "that they like it then they\ndon't or do we only say that we predict that they like it? At the moment, all\nyour job is to say",
    "start": "2674120",
    "end": "2680450"
  },
  {
    "text": "predict they like it or not. But it turns out you could, in\nfact, take these two numbers",
    "start": "2680450",
    "end": "2686810"
  },
  {
    "text": "and figure out what is\nthe normalization constant because remember, the\nnormalization constant",
    "start": "2686810",
    "end": "2692329"
  },
  {
    "text": "will be whatever constant\nthis thing was divided by. If we use the fact that we\nknew that the two probabilities",
    "start": "2692330",
    "end": "2699080"
  },
  {
    "text": "should add up to 1 and that\nthey have the same normalization constant, once you\nhave these two numbers, you could backwards compute\nif you wanted a probability.",
    "start": "2699080",
    "end": "2706330"
  },
  {
    "text": "So yes, we could\nget more insights, but our job is so simple,\npredict a 0 or 1 for now.",
    "start": "2706330",
    "end": "2712820"
  },
  {
    "text": "OK, but this isn't\nthe real base. But it was just great. That was fantastic.",
    "start": "2712820",
    "end": "2718215"
  },
  {
    "text": "There's nothing wrong\nwith that theory. It was a really good way\nof making prediction. Are you guys ready for brute\nforce Bayes m equals 2?",
    "start": "2718215",
    "end": "2726530"
  },
  {
    "text": "What's m? It's the number of\nfeatures per user. And by features, I mean how many\ninputs do you have for user.",
    "start": "2726530",
    "end": "2732480"
  },
  {
    "text": "Now, I've got two users. We're throwing in Zootopia. So now you're going to\npredict whether or not",
    "start": "2732480",
    "end": "2737540"
  },
  {
    "text": "somebody likes\nLife is Beautiful, but not just based\non Independence Day also based on Zootopia.",
    "start": "2737540",
    "end": "2743580"
  },
  {
    "text": "And we use the same theory. We're going to be like, OK,\nI'm going to either predict a 0 or 1 based on\nwhich one of these",
    "start": "2743580",
    "end": "2750420"
  },
  {
    "text": "makes this term the largest,\nthe probability of y given my inputs. It seems like a very nice\ntheory to start out with.",
    "start": "2750420",
    "end": "2756390"
  },
  {
    "text": "I like it. Fantastic. And then we get to use Bayes. That makes me so happy. You guys know whenever\nI get to use Bayes,",
    "start": "2756390",
    "end": "2762480"
  },
  {
    "text": "a little warmth\nin my heart grows. So we're very happy here. This is a great theory. We get to use Bayes.",
    "start": "2762480",
    "end": "2767849"
  },
  {
    "text": "And it leads to this expression. And one of the things\nthat makes us so so happy is that the only thing\nwe don't like about Bayes",
    "start": "2767850",
    "end": "2774170"
  },
  {
    "text": "is normalization constant, but\nbecause there's an argmax here, we don't even need to bother\nwith the normalization",
    "start": "2774170",
    "end": "2779849"
  },
  {
    "text": "constant. Whatever value this is, it\nwon't change which argument maximizes the expression.",
    "start": "2779850",
    "end": "2785650"
  },
  {
    "text": "And therefore, we just\nneed to calculate this. This is exactly the\nsame as when m equals 1.",
    "start": "2785650",
    "end": "2791750"
  },
  {
    "text": "But now, it's worth\nrecalling that x is a vector. And in this case,\nremember that x",
    "start": "2791750",
    "end": "2797329"
  },
  {
    "text": "will have two numbers because\nthere's now two movies that are in our inputs. Making sense?",
    "start": "2797330",
    "end": "2804080"
  },
  {
    "text": "OK, so what are you going to do? This is the joint probability of\nthem liking movie one and movie",
    "start": "2804080",
    "end": "2809360"
  },
  {
    "text": "two given x. And when you hear a joint,\nyou should be like, ooh, ooh. Joints? Don't those get big? They do get big.",
    "start": "2809360",
    "end": "2816190"
  },
  {
    "text": "Well, let's look at\nall our parameters. This doesn't include the\nparameter for probability y.",
    "start": "2816190",
    "end": "2821237"
  },
  {
    "text": "I'm just looking at\nall the parameters you need to be able to\ncompute this expression. To compute this expression,\nyou have to say,",
    "start": "2821237",
    "end": "2827410"
  },
  {
    "text": "if y equals 0, what's\nthe joint probability of any combination of x1, x2.",
    "start": "2827410",
    "end": "2832920"
  },
  {
    "text": "If y equals 1, what's\nthe joint probability of any combination of x1, x2?",
    "start": "2832920",
    "end": "2838180"
  },
  {
    "text": "And that's not so bad. We have eight parameters for\nthis term, two more for this. We've only got 10 parameters. That's not too bad.",
    "start": "2838180",
    "end": "2844560"
  },
  {
    "text": "And if you want to\nestimate all of them, you could do that based on data. You can say, counting based on\nprobability that y equals 0.",
    "start": "2844560",
    "end": "2851700"
  },
  {
    "text": "I could count for each\nof these and come up with a pretty\nreasonable estimate. Fine, not a big issue.",
    "start": "2851700",
    "end": "2858520"
  },
  {
    "text": "Brute force Bayes m equals 3. We're throwing in\na random TV show.",
    "start": "2858520",
    "end": "2863723"
  },
  {
    "text": "And now, we have to\nmake our prediction. Same theory. Starts with that wonderful\nplace of, oh, let's just",
    "start": "2863723",
    "end": "2870250"
  },
  {
    "text": "predict whichever one is\nmost likely given the inputs. Then we can use Bayes'\ntheorem and rewrite this.",
    "start": "2870250",
    "end": "2877485"
  },
  {
    "text": "And we can get rid\nof the normalization constant, fun times like\nwe've always been having. And the only thing\nthat's different",
    "start": "2877485",
    "end": "2883610"
  },
  {
    "text": "now is that x is a list. It will have whether\nor not the movie one, whether or not they\nlike movie two,",
    "start": "2883610",
    "end": "2890030"
  },
  {
    "text": "and whether or not\nthey like movie three . And we'll talk about the\nprobability of all three of those given y.",
    "start": "2890030",
    "end": "2895490"
  },
  {
    "text": "So we're going to have\nto have a parameter. If y equals 0, I'm\ngoing to have to know",
    "start": "2895490",
    "end": "2900560"
  },
  {
    "text": "the probability of the joint\nassignment of x1, x2, x3 in that case. So if y equals 0, I couldn't\nmake a three-dimensional table.",
    "start": "2900560",
    "end": "2909150"
  },
  {
    "text": "But if I could, I would have put\na three-dimensional table here. And a three-dimensional\ntable is, say, for any combination\nof x1, x2, and x3.",
    "start": "2909150",
    "end": "2918450"
  },
  {
    "text": "Well, I guess that is a\nthree-dimensional table. Well, OK, any\ncombination of x1, x2. And this is for x3 equals 0.",
    "start": "2918450",
    "end": "2923900"
  },
  {
    "text": "And this is x3 equals 1. There is these eight values\nto represent the case where",
    "start": "2923900",
    "end": "2929600"
  },
  {
    "text": "y equals 0 and 8 values for\nthe case where y equals 1. Oh, 16 parameters now.",
    "start": "2929600",
    "end": "2935720"
  },
  {
    "text": "That's fine. 16 parameters is not a problem. We've got millions of users. We can estimate all 16 of those\nparameters, just counting.",
    "start": "2935720",
    "end": "2941420"
  },
  {
    "text": "What if m equals 100. Well, we go back to our\nlovely little theory.",
    "start": "2941420",
    "end": "2948140"
  },
  {
    "text": "And it starts with a probability\nthat y is 1 or 0 given x. Fantastic. We use Bayes' theorem. Fantastic.",
    "start": "2948140",
    "end": "2953360"
  },
  {
    "text": "That makes us happy. We get rid the\nnormalization constant. We do our little normalization\nconstant has gone dance. And then we end up with\nthis final expression.",
    "start": "2953360",
    "end": "2960110"
  },
  {
    "text": "And we're like,\nOK, this just needs us to have a parameter for the\nprobability of any assignment",
    "start": "2960110",
    "end": "2966350"
  },
  {
    "text": "to the 100 movies given y.  Oh, actually, Chris,\nthat might be pretty big.",
    "start": "2966350",
    "end": "2975820"
  },
  {
    "text": "It might be quite a\nlot of parameters. How many parameters\nwould there be?",
    "start": "2975820",
    "end": "2981900"
  },
  {
    "text": "And one way to answer that\nquestion is, if y equals 0, how many unique\nassignments could",
    "start": "2981900",
    "end": "2987839"
  },
  {
    "text": "you make to x1, x2,\nx3, x100 because you'll need a parameter for\neach unique assignment.",
    "start": "2987840",
    "end": "2993000"
  },
  {
    "text": " Somebody who hasn't\nsaid anything yet today?",
    "start": "2993000",
    "end": "2998505"
  },
  {
    "start": "2998505",
    "end": "3008970"
  },
  {
    "text": "Yes. A lot. A lot. Yes, there's so many. Do you want to\nget more specific?",
    "start": "3008970",
    "end": "3015850"
  },
  {
    "text": "We could walk\nthrough it together. OK, x1, what are the values\nthat x1 could take on?",
    "start": "3015850",
    "end": "3023030"
  },
  {
    "text": "Exactly 1 or 0. So OK, there's two choices here. What are the values\nthat x2 can take on?",
    "start": "3023030",
    "end": "3028950"
  },
  {
    "text": "Two. Yeah, and no matter what x1\nis, x2 could still be 1 or 0. So there's two choices here, two\nchoices here, two choices here.",
    "start": "3028950",
    "end": "3036200"
  },
  {
    "text": "And there's two choices\nfor that hundredth one. 2 to the power of 100. 2 to the power of 100.",
    "start": "3036200",
    "end": "3042000"
  },
  {
    "text": "Oh, that's a lot. 2 to the power of 100. And that's just for y equals 0.",
    "start": "3042000",
    "end": "3047990"
  },
  {
    "text": "You'll need another 2\nto 100 for y equals 1. So it's actually\ntechnically 2 to 101.",
    "start": "3047990",
    "end": "3055390"
  },
  {
    "text": "Woof. That's a lot. Oops, we have more\nparameters than the number of atoms in the universe.",
    "start": "3055390",
    "end": "3061280"
  },
  {
    "text": "I hate when that happens. That's going to take\na lot to estimate. There might be a lot\nof people on Netflix, but there's not\nthat many people.",
    "start": "3061280",
    "end": "3067155"
  },
  {
    "text": "And now there's Disney Plus\nso there's other issues. Big O for number of parameters. So if m is your\nnumber of features",
    "start": "3067155",
    "end": "3074840"
  },
  {
    "text": "and every feature can either\ntake on the value 0 or 1, this brute force\nBayes has a problem",
    "start": "3074840",
    "end": "3080360"
  },
  {
    "text": "because the number of\nparameters will be big O 2 to the power of m. And that's just too much.",
    "start": "3080360",
    "end": "3085869"
  },
  {
    "text": "So even assuming each\nfeature is binary, this is not going to cut it.",
    "start": "3085870",
    "end": "3093090"
  },
  {
    "text": "Brute force Bayes,\nyou are so kind to us. You tried so hard. But you tried to\nuse too much force",
    "start": "3093090",
    "end": "3100410"
  },
  {
    "text": "for what should have been\na more delicate problem. OK, a pedagogical pause. Think about your life,\nhave two minutes,",
    "start": "3100410",
    "end": "3106180"
  },
  {
    "text": "try and summarize what\nwe've learned so far. If there's something\nconfusing, come ask me. Or if you need to\nrun to the bathroom, go run to the bathroom.",
    "start": "3106180",
    "end": "3112108"
  },
  {
    "text": "I'll give you guys two minutes. And if we could\nswitch over to music, I'm going to try and play.",
    "start": "3112108",
    "end": "3117375"
  },
  {
    "start": "3117375",
    "end": "3341670"
  },
  {
    "text": "OK, so just to bring us\nback to the plot line.",
    "start": "3341670",
    "end": "3350130"
  },
  {
    "text": "This was going to be so cool. We had this great idea. It was going to allow us\nto completely calculate",
    "start": "3350130",
    "end": "3356849"
  },
  {
    "text": "the whichever choice of\n0 or 1 would maximize the probability of y given x. What a good, good idea, it\njust ran into this problem",
    "start": "3356850",
    "end": "3365010"
  },
  {
    "text": "that this term here\nis, in fact, talking about the probability of the\njoint assignment to x given y.",
    "start": "3365010",
    "end": "3375720"
  },
  {
    "text": "And when computer scientists\ncame across this problem, they're like, well,\nthat's exponential. That's going to\nbe a huge problem.",
    "start": "3375720",
    "end": "3381849"
  },
  {
    "text": "And in very computer\nscientist fashion, they're like, well,\nlet's just hack it.",
    "start": "3381850",
    "end": "3388140"
  },
  {
    "text": "And they came up\nwith an assumption. Assumption that is so wrong it\nmight shake you to your core,",
    "start": "3388140",
    "end": "3394800"
  },
  {
    "text": "but assumption\nthat is so helpful is going to make this\nvery, very straightforward.",
    "start": "3394800",
    "end": "3401230"
  },
  {
    "text": "They call this assumption\nthe Naive Bayes assumption. And here's what\nthe assumption is,",
    "start": "3401230",
    "end": "3406380"
  },
  {
    "text": "is this is hard to calculate? What if we just assumed\nthat every value of xi",
    "start": "3406380",
    "end": "3415440"
  },
  {
    "text": "was independent of each\nother conditioned on y? So if you tell me\nwhether or not somebody",
    "start": "3415440",
    "end": "3420839"
  },
  {
    "text": "likes Life is\nBeautiful, I'm going to assume that whether or not\nthey like every other movie independent of each other.",
    "start": "3420840",
    "end": "3426670"
  },
  {
    "text": "Now first of all, if you\nmake that assumption, it's got this\nsweet, sweet payoff",
    "start": "3426670",
    "end": "3431910"
  },
  {
    "text": "that this hard term becomes\nthe product of a bunch of much easier to think about terms.",
    "start": "3431910",
    "end": "3438150"
  },
  {
    "text": "It's the product of\nthinking about each movie x on its own given y. You don't have to think\nabout anything jointly",
    "start": "3438150",
    "end": "3444990"
  },
  {
    "text": "once you assume independence. So that's the good news. What's the bad news?",
    "start": "3444990",
    "end": "3450240"
  },
  {
    "text": "Is there any? Yeah, there is. The bad news is,\nthat's not right.",
    "start": "3450240",
    "end": "3456420"
  },
  {
    "text": "That's not true at all. Let's say this is whether or\nnot somebody likes Zootopia.",
    "start": "3456420",
    "end": "3462450"
  },
  {
    "text": "And then this movie is\nwhether they like Zootopia 2. Are those truly going to be\nindependent of each other?",
    "start": "3462450",
    "end": "3469440"
  },
  {
    "text": "No, whether or not\nyou like Zootopia 1 very much changes whether\nor not you like Zootopia 2.",
    "start": "3469440",
    "end": "3474540"
  },
  {
    "text": "Now, the assumption\nsays like, oh, oh, oh, but if you condition\noff whether or not they like Life is\nBeautiful, then it's",
    "start": "3474540",
    "end": "3480840"
  },
  {
    "text": "mostly, mostly independent. And it's just wrong, but it\nmakes the math really sweet.",
    "start": "3480840",
    "end": "3486180"
  },
  {
    "text": "Question. What do we do with most\ndata points in a sample, don't we always assume\nindependence anyway?",
    "start": "3486180",
    "end": "3492150"
  },
  {
    "text": "So why is it so\nawful in this case? Yes. What we assume, we've always\nbeen assuming IID samples.",
    "start": "3492150",
    "end": "3499290"
  },
  {
    "text": "And to be clear\nwhat that means is if we go back to our data set--",
    "start": "3499290",
    "end": "3504462"
  },
  {
    "text": "where's my mouse. There is my mouse. So if we go back\nto our data set, the IID sample\nassumption is saying,",
    "start": "3504462",
    "end": "3511530"
  },
  {
    "text": "I assume user 1 is\nindependent of user 2. And they come from\nthe same distribution.",
    "start": "3511530",
    "end": "3518460"
  },
  {
    "text": "So maybe the data points\ncan have some relationship because they're coming\nfrom the same distribution, but I assume that they're\nindependent from one another.",
    "start": "3518460",
    "end": "3525579"
  },
  {
    "text": "And assuming that\npeople are independent is pretty reasonable. Everyone's very happy with that.",
    "start": "3525580",
    "end": "3531310"
  },
  {
    "text": "It's once you start assuming\nthat the features are independent that it\nbecomes a little bit like, oh, that's actually\npretty wrong.",
    "start": "3531310",
    "end": "3537970"
  },
  {
    "text": "Every assumption is\nwrong on some level, but users being independent is\na lot less wrong and something",
    "start": "3537970",
    "end": "3544060"
  },
  {
    "text": "we're much more happy with. And features being independent\nfeels very, very, very wrong.",
    "start": "3544060",
    "end": "3550490"
  },
  {
    "text": "That makes sense? It's fair to think\nabout each person being an independent pull\nfrom the distribution. Think about your\npeople in [INAUDIBLE]..",
    "start": "3550490",
    "end": "3556777"
  },
  {
    "text": "We assume that every pull\nis an independent pull from the distribution. And independence of each pull\nis a very reasonable thing.",
    "start": "3556777",
    "end": "3563820"
  },
  {
    "text": "So it's a great\nquestion, but hopefully I gave some clarity there. OK, I'm seeing a nod. ",
    "start": "3563820",
    "end": "3571070"
  },
  {
    "text": "So parameters of the\nuniverse, pedagogical pause, Naive Bayes assumption.",
    "start": "3571070",
    "end": "3576470"
  },
  {
    "text": "OK, so this is the\nlast great idea. And if you incorporate\nthis great idea,",
    "start": "3576470",
    "end": "3581990"
  },
  {
    "text": "you'd have Naive Bayes. So we'll just take\na little bit of time to unroll what this idea means. But just to be\nclear, Naive Bayes",
    "start": "3581990",
    "end": "3588289"
  },
  {
    "text": "is going to use the same\ncorrect mathematics that we've been using this whole class. And then it's going to make\na very wrong assumption",
    "start": "3588290",
    "end": "3595039"
  },
  {
    "text": "that the joint probability\nof your inputs given the thing you're\ntrying to predict is equal to the product of\nthe probability of each input",
    "start": "3595040",
    "end": "3604039"
  },
  {
    "text": "feature on its own\nconditioned on y. So remember, each x sub i\nmight be a particular movie.",
    "start": "3604040",
    "end": "3611510"
  },
  {
    "text": "So this is the joint\nprobability of all movies in the inputs given y. And this is thinking about\neach movie on its own",
    "start": "3611510",
    "end": "3617480"
  },
  {
    "text": "and just multiplying. For those of you guys who are\na fan of Bayesian networks, there is an implied\nBayesian network here.",
    "start": "3617480",
    "end": "3624200"
  },
  {
    "text": "The implied Bayesian\nnetwork is there is a random variable for\nwhich is whether or not",
    "start": "3624200",
    "end": "3629359"
  },
  {
    "text": "you like the The, target movie. And we're going to say that\neach of the input movies",
    "start": "3629360",
    "end": "3637579"
  },
  {
    "text": "are random variables. And each of those\nrandom variables are caused by whether or not\nyou like the target movie,",
    "start": "3637580",
    "end": "3643250"
  },
  {
    "text": "but they're not\naffecting one another. That's what it would look\nlike if you were to draw it as a Bayesian network.",
    "start": "3643250",
    "end": "3649130"
  },
  {
    "text": "And particularly,\nthis Bayesian network tells you the joint\nprobability of somebody liking the target movie and\nliking the input movies.",
    "start": "3649130",
    "end": "3656085"
  },
  {
    "text": "It says, it's going\nto be a probability that you like the target\nmovie times the probability that you like the input movies\ngiven the target movie, which",
    "start": "3656085",
    "end": "3662480"
  },
  {
    "text": "becomes a big old product. Wild.",
    "start": "3662480",
    "end": "3667540"
  },
  {
    "text": "And this leads to a\nNaive Bayes classifier. Naive Bayes starts exactly the\nsame way brute force Bayes did.",
    "start": "3667540",
    "end": "3673420"
  },
  {
    "text": "This gx is just a\nfancy thing people use when they're talking\nabout your machine learning algorithm. You can ignore it for\nnow, but you can just",
    "start": "3673420",
    "end": "3679810"
  },
  {
    "text": "say, the prediction I make is\nwhatever value 0 or 1 makes this as the largest.",
    "start": "3679810",
    "end": "3686030"
  },
  {
    "text": "So if y equals 1\nmakes this largest, we're going to predict a 1. Bayes' theorem allows\nus to flip this.",
    "start": "3686030",
    "end": "3692820"
  },
  {
    "text": "And because argmax gets rid\nof the normalizing constant, we only have to think\nabout the numerator.",
    "start": "3692820",
    "end": "3699540"
  },
  {
    "text": "And then if we employ the\nNaive Bayes assumption, this is no longer\nthe joint probability",
    "start": "3699540",
    "end": "3705000"
  },
  {
    "text": "of all of the inputs given y. It's the probability of each\nfeature on its own given",
    "start": "3705000",
    "end": "3710069"
  },
  {
    "text": "y, all multiplied together. And then this term is\nexactly the same as before.",
    "start": "3710070",
    "end": "3716790"
  },
  {
    "text": "And just to be clear,\nsometimes we're going to have numerical\nstability problems. So argmax of an\nexpression is the same",
    "start": "3716790",
    "end": "3724440"
  },
  {
    "text": "as the argmax of the\nlog of expression. So a lot of times, people will\nactually do log probabilities. And if you did\nlog probabilities,",
    "start": "3724440",
    "end": "3730710"
  },
  {
    "text": "it would be whichever\nvalue 0 or 1 makes this expression the largest. Exact same thing as\nbrute force Bayes",
    "start": "3730710",
    "end": "3737619"
  },
  {
    "text": "is just when you\ncome to calculate the probability of\nthe inputs given y, we're going to use this\nNaive Bayes assumption.",
    "start": "3737620",
    "end": "3743920"
  },
  {
    "text": "Let's break it on down. You still need to do\ntraining, but training",
    "start": "3743920",
    "end": "3749260"
  },
  {
    "text": "becomes much easier. You have to estimate\nthe probability that y equals 1 and the\nprobability y equals 0.",
    "start": "3749260",
    "end": "3754330"
  },
  {
    "text": "And that will just be counting. If you want to know the\nprobability of y equals 1, is just out of all\nyour training examples,",
    "start": "3754330",
    "end": "3760039"
  },
  {
    "text": "how many had y equals 1? Not that complicated. If you want to estimate the\nprobability that xi equals",
    "start": "3760040",
    "end": "3765520"
  },
  {
    "text": "1 given y equals\n0, then we can just use conditional\nprobability counting. Hey, take all your\nexamples where y equals 0.",
    "start": "3765520",
    "end": "3773680"
  },
  {
    "text": "How many of those, what\nfraction also had xi equal to 1? That's MLE. MLE says you should\njust do this counting.",
    "start": "3773680",
    "end": "3781300"
  },
  {
    "text": "MAP that really\ncomplicated extension. You guys ready to see\nhow map changes from MLE,",
    "start": "3781300",
    "end": "3786880"
  },
  {
    "text": "made a big deal about it? Bloop. This is MAP with a\nLaplace smoothing,",
    "start": "3786880",
    "end": "3793040"
  },
  {
    "text": "which sounds so fancy. Is that the fanciest way\nyou've ever heard of somebody say add 1 to the numerator\nand add 2 to the denominator?",
    "start": "3793040",
    "end": "3801400"
  },
  {
    "text": "Anyways, it just has\nthis prior belief that we've seen at least\none example of y equals 1 and one example of y equals 2.",
    "start": "3801400",
    "end": "3806930"
  },
  {
    "text": "Therefore, it just adds 1 to\nyour counting in your numerator and 2 both for the y equals\n1 and y equals 2 case, so",
    "start": "3806930",
    "end": "3814099"
  },
  {
    "text": "your denominator. And same thing over here. You add 1 to the numerator\nand 2 to the denominator. That's the Laplace\nprior, so beta 2, 2.",
    "start": "3814100",
    "end": "3821940"
  },
  {
    "text": "We have ways now to estimate\nall of our parameters. And I'd like to take a\nmoment to just show you",
    "start": "3821940",
    "end": "3831280"
  },
  {
    "text": "how you could use this. So let's imagine a\ncase where x1 denotes like Star Wars and x2\ndenotes likes Harry Potter.",
    "start": "3831280",
    "end": "3837849"
  },
  {
    "text": "And now, we're going to try\nand predict the variable likes Lord of the Rings. Yes, this is the nerdiest\nof movie examples.",
    "start": "3837850",
    "end": "3845474"
  },
  {
    "text": "What are you going\nto have to estimate? First of all, you have to\nestimate the probability y equals 0 and y equals 1.",
    "start": "3845475",
    "end": "3850640"
  },
  {
    "text": "If this is the number of\npeople who had y equals 0 and this is the\nprobability number of people who had\ny equals 1, then",
    "start": "3850640",
    "end": "3856340"
  },
  {
    "text": "to estimate these using MLE\nis just going to be counting. It'll be what fraction\nhad y equals 1",
    "start": "3856340",
    "end": "3861590"
  },
  {
    "text": "and what fraction\nhad y equals 0. Then we need to calculate the\nprobability of x1 given y.",
    "start": "3861590",
    "end": "3867970"
  },
  {
    "text": "And for x1 given y, we can\nthink about all the cases where y is 0 and x1 is 0, all\nthe cases where y is 1 and x",
    "start": "3867970",
    "end": "3875080"
  },
  {
    "text": "is 0. And if you do MLE,\nit'll just be counting to figure out that here, there's\n13 people with y equals 0.",
    "start": "3875080",
    "end": "3885420"
  },
  {
    "text": "And out of those, 10 out\nof 13 had x1 equals 1. There's 13 people\nwith y equals 0,",
    "start": "3885420",
    "end": "3892470"
  },
  {
    "text": "of which 23% had x1 equals 0. For both of your\nmovies, you'll create",
    "start": "3892470",
    "end": "3899190"
  },
  {
    "text": "a table that's a four by four. And this is going to be true. If you had 10 movies,\nyou just have 10 tables",
    "start": "3899190",
    "end": "3904790"
  },
  {
    "text": "that are four by four. It's going to scale very\nnicely with number of movies. Cough, cough, Naive\nBayes assumption.",
    "start": "3904790",
    "end": "3909997"
  },
  {
    "text": "We know you're doing\nsomething wrong, but I get that you're\nmaking it easier. So now somebody\nactually walks in",
    "start": "3909998",
    "end": "3916230"
  },
  {
    "text": "and they say they like\nStar Wars but they don't like Harry Potter. And now you have to\npredict whether or not they like Lord of the Rings.",
    "start": "3916230",
    "end": "3922589"
  },
  {
    "text": "Again, we're just going to be\nusing the brute force Bayes idea. It's just when we\nget to this term,",
    "start": "3922590",
    "end": "3928128"
  },
  {
    "text": "we're going to use a Naive\nBayes assumption which says, we're going to think about each\nfeature on its own multiplied together.",
    "start": "3928128",
    "end": "3934109"
  },
  {
    "text": "So probability that x1\ntakes on the value 1 given y, x2 takes on the value 0 given\ny, and the probability of y.",
    "start": "3934110",
    "end": "3943120"
  },
  {
    "text": "We'll try this both for y\nequals 0 and y equals 1. And we'll choose\nwhichever one is bigger. So it'll be the argmax.",
    "start": "3943120",
    "end": "3950020"
  },
  {
    "text": "We put in x1 is 1, x2 equals 0. We try what happens when y is 0.",
    "start": "3950020",
    "end": "3958030"
  },
  {
    "text": "And we get this expression. We can look up all those\nterms, multiply them together, get a number.",
    "start": "3958030",
    "end": "3964089"
  },
  {
    "text": "We can think about\nwhen y equals 1. And then you get, OK,\nyes, they liked movie one.",
    "start": "3964090",
    "end": "3969970"
  },
  {
    "text": "They didn't like movie two. And now we're thinking\nabout the case where they do like the\npredictor movie, y equals 1.",
    "start": "3969970",
    "end": "3976119"
  },
  {
    "text": "And then again,\nit's three lookups. For each of these probabilities,\nwe've got a parameter. We can look them up in our\ntables, multiply them together,",
    "start": "3976120",
    "end": "3983740"
  },
  {
    "text": "and we get another number. Notice, these two\nnumbers don't add up to 1 because we drop the\nnormalization constant,",
    "start": "3983740",
    "end": "3991360"
  },
  {
    "text": "but it'll still be the case\nthat whichever argument y equals 1 or y equals 0 maximizes\nthis expression,",
    "start": "3991360",
    "end": "3997840"
  },
  {
    "text": "will be our prediction. In this case, y equals 1\nmaximize the prediction.",
    "start": "3997840",
    "end": "4003760"
  },
  {
    "text": "So in this case,\nprobability that y equals-- or will make the\nprediction that y equals 1. I mentioned this\nin words earlier,",
    "start": "4003760",
    "end": "4010170"
  },
  {
    "text": "but it's worth restating. There is a\nnormalization constant. There is some constant we\ndidn't think about here.",
    "start": "4010170",
    "end": "4015810"
  },
  {
    "text": "And that exact same\nconstant existed in the case where y equals 0. Because we know\nprobability equals",
    "start": "4015810",
    "end": "4021330"
  },
  {
    "text": "y equals 1 plus probability\ny equals 0 should equal 1, we could solve for\nwhat the constant is.",
    "start": "4021330",
    "end": "4026337"
  },
  {
    "text": "And then you could\nfigure out exactly what's probability y equals 1. But as I said, our\njob isn't to do that.",
    "start": "4026337",
    "end": "4032550"
  },
  {
    "text": "Our job is to break 0 or 1. Now, that was MLE.",
    "start": "4032550",
    "end": "4037930"
  },
  {
    "text": "What if we had to do MAP? If we have to do MAP, we\nhave to choose a prior.",
    "start": "4037930",
    "end": "4043900"
  },
  {
    "text": "And what prior\nbetter than Laplace? Laplace is, for every\nprobability p I'm estimating,",
    "start": "4043900",
    "end": "4051309"
  },
  {
    "text": "I'm going to imagine one example\nof 1 and one example of a 0. So if I'm predicting\na pi, I'm going",
    "start": "4051310",
    "end": "4057190"
  },
  {
    "text": "to imagine one\nexample of success and one example of failure,\nwhich is going to mean,",
    "start": "4057190",
    "end": "4062740"
  },
  {
    "text": "calculate how many\ntimes i happen divided by how many samples I had. So it's very, very similar.",
    "start": "4062740",
    "end": "4068750"
  },
  {
    "text": "It's just when you get here,\ninstead of thinking about 13 and 17 we're going to add an\nimaginary trial for y equals 0,",
    "start": "4068750",
    "end": "4074620"
  },
  {
    "text": "an imaginary trial\nfor y equals 1, and carry that on for the x's. ",
    "start": "4074620",
    "end": "4080530"
  },
  {
    "text": "Long story short,\nthat's Naive Bayes. Training Naive Bayes is\nas simple as counting.",
    "start": "4080530",
    "end": "4086390"
  },
  {
    "text": "Well, actually, this\nshould be for a binomial not a multinomial.",
    "start": "4086390",
    "end": "4092920"
  },
  {
    "text": "Training is just counting. OK, a little bit of a fun aside.",
    "start": "4092920",
    "end": "4099818"
  },
  {
    "text": "If you ever get an email like\nthis, which is clearly spam. If you look into the metadata,\nespecially at Stanford,",
    "start": "4099819",
    "end": "4109200"
  },
  {
    "text": "you'll see something\nthat looks like this. And it's hard to interpret. But at the very end,\nyou might see something",
    "start": "4109200",
    "end": "4114930"
  },
  {
    "text": "that says 8.0 BAYES_99. And then it'll have\na tag which says, the Bayesian spam\nprobability is 99% to 100%.",
    "start": "4114930",
    "end": "4122880"
  },
  {
    "text": "And you might wonder, what is\nBayes doing in my mail server? And the answer to that is, one\nof the first huge applications",
    "start": "4122880",
    "end": "4130799"
  },
  {
    "text": "of Naive Bayes was in\nspam classification. And fun fact, the person who\ndid that was our very own Mehran",
    "start": "4130800",
    "end": "4138060"
  },
  {
    "text": "Sahami, when he was\nworking at Google or I think this is before\nhe worked at Google, got him a job at Google.",
    "start": "4138060",
    "end": "4143818"
  },
  {
    "text": "And then they\nimplemented at Google. And using Naive Bayes,\nthey would try and predict whether or not an\nemail was spam or not.",
    "start": "4143819",
    "end": "4150179"
  },
  {
    "text": "It's a 1 or a 0\nclassification problem. And even though spam became\nmore and more prevalent,",
    "start": "4150180",
    "end": "4156790"
  },
  {
    "text": "as they got more\nand more data, they got better and better at\nestimating the parameters. They got very good,\neven using Naive Bayes",
    "start": "4156790",
    "end": "4163089"
  },
  {
    "text": "at predicting whether or\nnot something was spam. The source in case\nyou're curious.",
    "start": "4163090",
    "end": "4169229"
  },
  {
    "text": "So if you want to\npredict spam emails, you could use this\nNaive Bayes classifier.",
    "start": "4169229",
    "end": "4176299"
  },
  {
    "text": "What you need to do, though,\nis you need to take each email. And particularly in\nyour training emails,",
    "start": "4176300",
    "end": "4182520"
  },
  {
    "text": "you have to take that email\nand turn it into x and a y. What are the x's going to be?",
    "start": "4182520",
    "end": "4187830"
  },
  {
    "text": "Well, what they did is\nthey took, in English, 100,000 words in English.",
    "start": "4187830",
    "end": "4193439"
  },
  {
    "text": "And they would\ncreate their vector of x to be, how many\ntimes did word one occur?",
    "start": "4193439",
    "end": "4198900"
  },
  {
    "text": "How many times did\nword two occur? Or sometimes just whether or\nnot word one was there at all,",
    "start": "4198900",
    "end": "4205300"
  },
  {
    "text": "whether or not word two\nis there, whether or not word m was there. So every variable\nxi was a 1 or a 0",
    "start": "4205300",
    "end": "4212150"
  },
  {
    "text": "whether or not that word\nappeared in the spam-- in the email or not. Now, just to be\nclear, m is huge.",
    "start": "4212150",
    "end": "4219830"
  },
  {
    "text": "So they had to make this\nNaive Bayes assumption if you want to do the spam\nclassification really quickly",
    "start": "4219830",
    "end": "4225050"
  },
  {
    "text": "for every single\nemail that came in. In this case, what's y? So if x is the indication\nof whether or not",
    "start": "4225050",
    "end": "4231260"
  },
  {
    "text": "every single word in\nEnglish is in the email, y is going to just be simple\n0 for not spam 1 for spam.",
    "start": "4231260",
    "end": "4238960"
  },
  {
    "text": "This is going to be trained. You're going to learn your\nparameters on historical data. And so what they did is they\ntook a whole ton of old emails,",
    "start": "4238960",
    "end": "4247600"
  },
  {
    "text": "manually decided if\nthey were spam or not. And then we use\nthose historical data to train all of\ntheir parameters.",
    "start": "4247600",
    "end": "4254739"
  },
  {
    "text": "And you could have\ndone this too. So if we flashback\n20 years, you guys could have changed the world of\nspam classification just using",
    "start": "4254740",
    "end": "4261730"
  },
  {
    "text": "what we learned in class today. And it's the exact same thing.",
    "start": "4261730",
    "end": "4267950"
  },
  {
    "text": "Fun fact, they in fact used\nMAP with Laplace estimation. And one of the reasons\nthat they wanted",
    "start": "4267950",
    "end": "4274130"
  },
  {
    "text": "to use MAP with\nLaplace estimation is in their training data,\nit was very believable",
    "start": "4274130",
    "end": "4279380"
  },
  {
    "text": "that there could be some\nword that doesn't show up at all for spam or\nnot spam, which could",
    "start": "4279380",
    "end": "4285980"
  },
  {
    "text": "lead to a probability of 0. And a probability of 0 becomes\nvery problematic in Naive Bayes because when you're multiplying\nall your probabilities of xi's",
    "start": "4285980",
    "end": "4293960"
  },
  {
    "text": "together-- so Naive Bayes has\nthis term, which has the probability over i,\nthe probability of feature i",
    "start": "4293960",
    "end": "4302810"
  },
  {
    "text": "given the true label. If any single one of these\nterms ends up being 0,",
    "start": "4302810",
    "end": "4311700"
  },
  {
    "text": "it just hoses the whole thing. It's like crashing the party. It's like, yeah, woo-hoo. We're all 0.",
    "start": "4311700",
    "end": "4317333"
  },
  {
    "text": "I don't know,\nsomething like that. Anyway, so you don't want to\nhave any of those terms be 0. It's very problematic.",
    "start": "4317333",
    "end": "4323130"
  },
  {
    "text": "You lose all information\nif a single term is 0. And so actually, this Laplace\nprior is very helpful.",
    "start": "4323130",
    "end": "4329040"
  },
  {
    "text": "Laplace prior gives\nyou the guarantee that no probability\nwill either be 0 or 1.",
    "start": "4329040",
    "end": "4335080"
  },
  {
    "text": "OK, and then classification. A new email comes in. You would say, OK, which\nwords are in this email?",
    "start": "4335080",
    "end": "4341530"
  },
  {
    "text": "You would calculate the\nNaive Bayes prediction. And then you decide,\nis this spam or not?",
    "start": "4341530",
    "end": "4347110"
  },
  {
    "text": "What a fun time? And it basically\ncomes down to this-- argmax over the inputs given\ny times the probability of y,",
    "start": "4347110",
    "end": "4355060"
  },
  {
    "text": "apply the Naive\nBayes assumption. And the Naive Bayes\nassumption, just to remind you, is that this joint probability\nof your inputs given y,",
    "start": "4355060",
    "end": "4361750"
  },
  {
    "text": "remember there's\n100,000 of these, can just be thought of each\ninput on its own given y.",
    "start": "4361750",
    "end": "4367900"
  },
  {
    "text": "OK. OK, so training Naive\nBayes, estimating parameters just of Bernoulli.",
    "start": "4367900",
    "end": "4373760"
  },
  {
    "text": "And it's just counting. OK, now, interesting question\nyou might ask, at this point,",
    "start": "4373760",
    "end": "4381800"
  },
  {
    "text": "we've finished the plot line. We want to do ML. Woo-hoo. Let's do classification, yeah.",
    "start": "4381800",
    "end": "4388310"
  },
  {
    "text": "Oh, we can't just do\nthe brute Force Bayes. Sad times. But if we make the\nNaive Bayes assumption,",
    "start": "4388310",
    "end": "4393470"
  },
  {
    "text": "we've got a whole pipeline. We can take a training data\nset, estimate our parameters.",
    "start": "4393470",
    "end": "4399320"
  },
  {
    "text": "Then we've got a\nprobabilistic model with parameters, which means\nif we get a new example, we can make a prediction.",
    "start": "4399320",
    "end": "4405890"
  },
  {
    "text": "But there's one step I\nhaven't told you about. How do we tell if our\npredictions are any good?",
    "start": "4405890",
    "end": "4412100"
  },
  {
    "text": "Hopefully, they're good\nbecause they're based on data, but maybe the y hats and the\ny's are actually different",
    "start": "4412100",
    "end": "4417530"
  },
  {
    "text": "quite a lot. What people do is they\nactually hold out some data that they call their test set.",
    "start": "4417530",
    "end": "4424740"
  },
  {
    "text": "So you train on some\ndata with spammy emails and non-spammy emails.",
    "start": "4424740",
    "end": "4429889"
  },
  {
    "text": "And we're going to hold\nout a special set of data where we know the answer. We know if it's spam or not.",
    "start": "4429890",
    "end": "4435840"
  },
  {
    "text": "And we use that to quiz\nthe machine learning model you just built. So you just built a\nmachine learning model.",
    "start": "4435840",
    "end": "4441560"
  },
  {
    "text": "I've got a whole quiz to give\nyour machine learning model. I've got all these emails. And I know if\nthey're spam or not.",
    "start": "4441560",
    "end": "4446640"
  },
  {
    "text": "And I'm going to test the\nmachine learning model. I'll give the machine learning\nmodel each email one at a time. And you'll predict\nspam or not spam.",
    "start": "4446640",
    "end": "4453420"
  },
  {
    "text": "And then I'll see\nif you're right. And if I take this whole\nquiz and I give it to machine",
    "start": "4453420",
    "end": "4458560"
  },
  {
    "text": "learning model, we\ncall that testing. And there's a couple of numbers\nthat people really care about.",
    "start": "4458560",
    "end": "4465040"
  },
  {
    "text": "One is accuracy. What was your raw\nscore on the quiz? If there was 100 emails in the\nquiz, did you get 90 correct?",
    "start": "4465040",
    "end": "4473410"
  },
  {
    "text": "That would be an accuracy of 90. If there is 10,000\nyou got 9,000 correct, that would still be\nan accuracy of 90%.",
    "start": "4473410",
    "end": "4480890"
  },
  {
    "text": "There's two other numbers that\npeople sometimes care about. One is precision\nand one is recall.",
    "start": "4480890",
    "end": "4487010"
  },
  {
    "text": "And basically, recall\nsays, how well do you",
    "start": "4487010",
    "end": "4493670"
  },
  {
    "text": "do at predicting when we think\nabout you actually have spam?",
    "start": "4493670",
    "end": "4503199"
  },
  {
    "text": "So when something is spam, how\ngood are you at pulling it out? That's the recall.",
    "start": "4503200",
    "end": "4508639"
  },
  {
    "text": "So out of all the quiz\nitems that were spam, how well did you do? So on that particular subset.",
    "start": "4508640",
    "end": "4514570"
  },
  {
    "text": "Precision means, hey, you\ntold me something was spam. So take all the quiz items\nwhere the ML model actually",
    "start": "4514570",
    "end": "4524050"
  },
  {
    "text": "predicted spam and how well does\nit do when it predicts spam? And you can think about\nthat as if it says spam,",
    "start": "4524050",
    "end": "4530500"
  },
  {
    "text": "should you trust it? And this is, if it really\nwas spam, did it find it?",
    "start": "4530500",
    "end": "4536409"
  },
  {
    "text": "They are clunky ideas\nto get your head around, but people do really\ncare about these. And one way of telling\nyou this is precision",
    "start": "4536410",
    "end": "4545740"
  },
  {
    "text": "gets close to 100%. People got really almost perfect\non this measure of the quiz.",
    "start": "4545740",
    "end": "4552610"
  },
  {
    "text": "And recall was pretty high. What does precision mean? Precision means, if I say\nit's spam, it is spam.",
    "start": "4552610",
    "end": "4560960"
  },
  {
    "text": "And why might you\ncare about that? Because you don't want to\ntake that very important email",
    "start": "4560960",
    "end": "4566560"
  },
  {
    "text": "and call it spam. So precision means that when\nthe machine learning model says it's spam, you can trust it.",
    "start": "4566560",
    "end": "4572390"
  },
  {
    "text": "It probably is. And recall means that\nif this was 100%, no spam would end\nup in your inbox.",
    "start": "4572390",
    "end": "4579380"
  },
  {
    "text": "And if it's less\nthan that, some spam will actually make\nit into your inbox. And at the time that they were\nusing Naive Bayes in Gmail,",
    "start": "4579380",
    "end": "4587200"
  },
  {
    "text": "they were getting really\nhigh recall rates, not 100%. Now, just to be clear, just\nusing words as features",
    "start": "4587200",
    "end": "4595329"
  },
  {
    "text": "was pretty good. But it turns out, if you look at\na couple other things like who is the sender, is the\nemail signed properly,",
    "start": "4595330",
    "end": "4602050"
  },
  {
    "text": "all those things\nwere extra features that they threw in\nthe Naive Bayes. There you have it.",
    "start": "4602050",
    "end": "4608270"
  },
  {
    "text": "So in summary, you have learned\nNaive Bayes classification.",
    "start": "4608270",
    "end": "4614110"
  },
  {
    "text": "And you will go build it. When you go build it, here's\nwhat I want you to know. Training is just counting.",
    "start": "4614110",
    "end": "4620080"
  },
  {
    "text": "You have to estimate\nall of these parameters, but it will be just counting\nin your training data set.",
    "start": "4620080",
    "end": "4625510"
  },
  {
    "text": "MLE is pure counting, but\nit doesn't take into account the fact that maybe you want\nto imagine some successes",
    "start": "4625510",
    "end": "4632650"
  },
  {
    "text": "and failures to avoid\nprobabilities of 0. MAP just adds in a\ncouple other cases.",
    "start": "4632650",
    "end": "4637719"
  },
  {
    "text": "Once you've done your\nparameter estimation, you can make your\nprediction by doing",
    "start": "4637720",
    "end": "4642790"
  },
  {
    "text": "the argmax of the\nprobability of y times the probability of\neach xi given y.",
    "start": "4642790",
    "end": "4647830"
  },
  {
    "text": "We will often use logs to\navoid numerical underflow when it comes to computers. And that is the\nend of the story.",
    "start": "4647830",
    "end": "4655239"
  },
  {
    "text": "And you know what that means? I'm pretty sure this is\nyour last class, which means I'm pretty sure we\ncan start Thanksgiving",
    "start": "4655240",
    "end": "4663070"
  },
  {
    "text": "break two minutes early. Have a fantastic Thanksgiving. I'm so thankful for\nall of you guys. Thanks for working\nso hard in CS109.",
    "start": "4663070",
    "end": "4669770"
  },
  {
    "text": "Have a great break. And see you back\nwhen you're done. ",
    "start": "4669770",
    "end": "4678000"
  }
]