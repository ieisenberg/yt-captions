[
  {
    "start": "0",
    "end": "47000"
  },
  {
    "text": "Hi, everyone. Welcome to CS 25\nTransformers United V2. This was a course that\nwas held at Stanford",
    "start": "5990",
    "end": "11120"
  },
  {
    "text": "in the winter of 2023. This course is not\nabout robots that can transform into cars as\nthis picture might suggest.",
    "start": "11120",
    "end": "17325"
  },
  {
    "text": "Rather, it's about\ndeep learning models that have taken\nthe world by storm and have revolutionized\nthe field of AI and others.",
    "start": "17325",
    "end": "23440"
  },
  {
    "text": "Starting from natural\nlanguage processing, transformers have\nbeen applied all over, computer vision, reinforcement\nlearning, biology, robotics,",
    "start": "23440",
    "end": "30320"
  },
  {
    "text": "et cetera. We have an exciting set\nof videos lined up for you with some truly fascinating\nspeakers, talks, presenting",
    "start": "30320",
    "end": "37719"
  },
  {
    "text": "how they're applying\ntransformers to the research in\ndifferent fields and areas.",
    "start": "37720",
    "end": "41495"
  },
  {
    "text": "We hope you'll enjoy and\nlearn from these videos. So without any further\nado, let's get started.",
    "start": "44070",
    "end": "52130"
  },
  {
    "start": "47000",
    "end": "199000"
  },
  {
    "text": "This is a purely\nintroductory lecture. And we'll go into the building\nblocks of transformers.",
    "start": "52130",
    "end": "58750"
  },
  {
    "text": "So first, let's start with\nintroducing the instructors. So for me, I'm currently on a\ntemporary deferral from the PhD",
    "start": "58750",
    "end": "66110"
  },
  {
    "text": "program, and I'm leading AI at a\nrobotics startup, Collaborative Robotics, that are working on\nsome general purpose robots,",
    "start": "66110",
    "end": "73579"
  },
  {
    "text": "somewhat like [INAUDIBLE]. And I'm very passionate about\nrobotics and building FSG learning algorithms.",
    "start": "73580",
    "end": "79513"
  },
  {
    "text": "My research interests are\nin reinforcement learning, computer vision, and\nremodeling, and I have a bunch of\npublications in robotics,",
    "start": "79513",
    "end": "85820"
  },
  {
    "text": "autonomous driving,\nand other areas. My undergrad was at Cornell. If someone is from Cornell,\nso nice to [INAUDIBLE]..",
    "start": "85820",
    "end": "93850"
  },
  {
    "text": "So I'm Stephen, currently\na first-year CS PhD here. Previously did my master's at\nCMU and undergrad at Waterloo.",
    "start": "93850",
    "end": "100610"
  },
  {
    "text": "I'm mainly into NLP research,\nanything involving language and text, but more\nrecently, I've",
    "start": "100610",
    "end": "105880"
  },
  {
    "text": "been getting more into computer\nvision as well as [INAUDIBLE] And just some stuff I do\nfor fun, a lot of music",
    "start": "105880",
    "end": "111520"
  },
  {
    "text": "stuff, mainly piano. Some self-promo of what I post\na lot on my Insta, YouTube, and TikTok, so if you\nguys want to check it out.",
    "start": "111520",
    "end": "118780"
  },
  {
    "text": "My friends and I are also\nstarting a Stanford piano club, so if anybody's interested,\nfeel free to email",
    "start": "118780",
    "end": "124540"
  },
  {
    "text": "or DM me for details. Other than that, martial arts,\nbodybuilding, and huge fan",
    "start": "124540",
    "end": "131530"
  },
  {
    "text": "of k-dramas, anime,\nand occasional gamer. [LAUGHS]",
    "start": "131530",
    "end": "138230"
  },
  {
    "text": "OK, cool. Yeah, so my name is Rylan. Instead of talking\nabout myself, I just want to very briefly\nsay that I'm super excited to take this class.",
    "start": "138730",
    "end": "144790"
  },
  {
    "text": "I took it the last time--\nsorry-- to teach this. Excuse me. I took it the last\ntime I was offered. I had a bunch of fun.",
    "start": "144790",
    "end": "150280"
  },
  {
    "text": "I thought we brought in a\nreally great group of speakers last time. I'm super excited\nfor this offering.",
    "start": "150280",
    "end": "155286"
  },
  {
    "text": "And yeah, I'm thankful\nthat you're all here, and I'm looking forward to a\nreally fun quarter together. Thank you. Yeah, so fun fact, Rylan was\nthe most outspoken student",
    "start": "155287",
    "end": "162130"
  },
  {
    "text": "last year. And so if someone wants to\nbecome an instructor next year, you know what to do. [LAUGHTER]",
    "start": "162130",
    "end": "169954"
  },
  {
    "text": "OK, cool. Let's see. OK, I think we\nhave a few minutes.",
    "start": "170870",
    "end": "176510"
  },
  {
    "text": "So what we hope you will learn\nin this class is, first of all, how do transformers\nwork, how they",
    "start": "176510",
    "end": "182585"
  },
  {
    "text": "are being applied,\njust beyond NLP, and nowadays, like they\nare pretty [INAUDIBLE] them everywhere in\nAI machine learning.",
    "start": "182585",
    "end": "190290"
  },
  {
    "text": "And what are some new and\ninteresting directions of research in these topics.",
    "start": "190290",
    "end": "194360"
  },
  {
    "text": "Cool, so this class is\njust an introductory. So we're just talking about\nthe basics of transformers, introducing them, talking about\nthe self-attention mechanism",
    "start": "197760",
    "end": "204930"
  },
  {
    "start": "199000",
    "end": "215000"
  },
  {
    "text": "on which they're founded. And we'll do a deep dive\nmore on models like BERT",
    "start": "204930",
    "end": "210870"
  },
  {
    "text": "to GPT, stuff like that. So with that, happy\nto get started. OK, so let me start with\npresenting the attention",
    "start": "210870",
    "end": "218280"
  },
  {
    "start": "215000",
    "end": "301000"
  },
  {
    "text": "timeline. Attention all started\nwith this one paper. [INAUDIBLE] by\nVaswani et al in 2017.",
    "start": "218280",
    "end": "226270"
  },
  {
    "text": "That was the beginning\nof transformers. Before that, we had\nthe prehistoric error,",
    "start": "226270",
    "end": "231489"
  },
  {
    "text": "where we had models\nlike RNM, LSDMs, and simple attention\nmechanisms that didn't work",
    "start": "231490",
    "end": "237910"
  },
  {
    "text": "or [INAUDIBLE]. Starting 2017, we saw this\nexplosion of transformers",
    "start": "237910",
    "end": "242995"
  },
  {
    "text": "into NLP, where people started\nusing it for everything. I even heard this\nquote from Google.",
    "start": "242995",
    "end": "248680"
  },
  {
    "text": "It's like our performance\nincreased every time we [INAUDIBLE] [CHUCKLES]",
    "start": "248680",
    "end": "253183"
  },
  {
    "text": "For the [INAUDIBLE]\nafter 2018 to 2020, we saw this explosion\nof transformers into other fields like vision,\na bunch of other stuff,",
    "start": "255070",
    "end": "263500"
  },
  {
    "text": "and like biology as a whole. And in last year,\n2021 was the start of the generative era, where we\ngot a lot of genetic modeling,",
    "start": "263500",
    "end": "271224"
  },
  {
    "text": "started models like\nCodex, GPT, DALL-E, stable diffusions,\nor a lot of things",
    "start": "271225",
    "end": "277360"
  },
  {
    "text": "happening in genetic modeling. And we started scaling up in AI.",
    "start": "277360",
    "end": "284229"
  },
  {
    "text": "And now, the present. So this is 2022 and\nthe startup in '23.",
    "start": "284230",
    "end": "289270"
  },
  {
    "text": "And now we have models\nlike ChatGPT, Whisperer, a bunch of others.",
    "start": "289270",
    "end": "294550"
  },
  {
    "text": "And we're scaling onwards\nwithout splitting up, so that's great. So that's the future.",
    "start": "294550",
    "end": "301650"
  },
  {
    "start": "301000",
    "end": "370000"
  },
  {
    "text": "So going more into this,\nso once there were RNNs.",
    "start": "301650",
    "end": "306940"
  },
  {
    "text": "So we had Seq2Seq\nmodels, LSTMs, GRU. What worked there was that they\nwere good at encoding history,",
    "start": "306940",
    "end": "313840"
  },
  {
    "text": "but what did not work was they\ndidn't encode long sequences and they were very bad\nat encoding context.",
    "start": "313840",
    "end": "321650"
  },
  {
    "text": "So consider this example. Consider trying to predict\nthe last word in the text,",
    "start": "321650",
    "end": "327530"
  },
  {
    "text": "\"I grew up in France,\ndot, dot, dot. I speak fluent Dutch.\" Here, you need to understand\nthe context for it",
    "start": "327530",
    "end": "333740"
  },
  {
    "text": "to predict French, and\nattention mechanism is very good at that, whereas\nif they're just using LSDMs,",
    "start": "333740",
    "end": "339425"
  },
  {
    "text": "it doesn't here work that well. Another thing transformers\nare good at is,",
    "start": "339425",
    "end": "346400"
  },
  {
    "text": "more based on content, is\nalso context prediction is like finding attention maps.",
    "start": "346400",
    "end": "352729"
  },
  {
    "text": "If I have something\nlike a word like it, what noun does it correlate to.",
    "start": "352730",
    "end": "357980"
  },
  {
    "text": "And we can give a\nproperty attention on one of the\npossible activations.",
    "start": "357980",
    "end": "365240"
  },
  {
    "text": "And this works better\nthan existing mechanisms.",
    "start": "365240",
    "end": "370360"
  },
  {
    "start": "370000",
    "end": "450000"
  },
  {
    "text": "OK, so where we were in 2021,\nwe were on the verge of takeoff.",
    "start": "370360",
    "end": "376465"
  },
  {
    "text": "We were starting to realize\nthe potential of transformers in different fields. We solved a lot of\nlong sequence problems",
    "start": "376465",
    "end": "383115"
  },
  {
    "text": "like protein folding,\nAlphaFold, offline RL.",
    "start": "383115",
    "end": "386340"
  },
  {
    "text": "We started to see few-shots,\nzero-shot generalization. We saw multimodal\ntasks and applications",
    "start": "388860",
    "end": "394425"
  },
  {
    "text": "like generating\nimages from language. So that was DALL-E. And\nit feels like [INAUDIBLE]..",
    "start": "394425",
    "end": "400998"
  },
  {
    "text": "And this was also a\ntalk on transformers that you can watch on YouTube. Yeah, cool.",
    "start": "403866",
    "end": "411129"
  },
  {
    "text": "And this is where we were\ngoing from 2021 to 2022, which is we have gone from\nthe version of [INAUDIBLE]",
    "start": "411130",
    "end": "418815"
  },
  {
    "text": "And now, we are seeing\nunique applications in audio generation,\nart, music, storytelling. We are starting to see\nthese new capabilities",
    "start": "418815",
    "end": "425620"
  },
  {
    "text": "like commonsense,\nlogical reasoning, mathematical reasoning. We are also able to now\nget human enlightenment",
    "start": "425620",
    "end": "432820"
  },
  {
    "text": "and interaction. They're able to use\nreinforcement learning and human feedback. That's how ChatGPT is trained\nto perform really good.",
    "start": "432820",
    "end": "439458"
  },
  {
    "text": "We have a lot of\nmechanisms for controlling toxicity bias and ethics now. And there are a\nlot of also, a lot",
    "start": "439458",
    "end": "446110"
  },
  {
    "text": "of developments in other\nareas like diffusion models. Cool.",
    "start": "446110",
    "end": "453320"
  },
  {
    "start": "450000",
    "end": "615000"
  },
  {
    "text": "So the future is a\nspaceship, and we are all excited about it.",
    "start": "453320",
    "end": "456320"
  },
  {
    "text": "And there's a lot\nof more applications that we can enable,\nand it'll be great",
    "start": "459402",
    "end": "464750"
  },
  {
    "text": "if you can see\ntransformers also up there. One big example is video\nunderstanding and generation.",
    "start": "464750",
    "end": "469940"
  },
  {
    "text": "That is something that\neveryone is interested in, and I'm hoping we'll\nsee a lot of models in this area this year,\nalso, finance, business.",
    "start": "469940",
    "end": "479840"
  },
  {
    "text": "I'll be very excited to\nsee GPT author a novel, but we need to solve very\nlong sequence modeling.",
    "start": "479840",
    "end": "484970"
  },
  {
    "text": "And most transformer\nmodels are still limited to 4,000 tokens\nor something like that. So we need to make them\ngeneralize much more",
    "start": "484970",
    "end": "493879"
  },
  {
    "text": "better on long sequences. We also want to have\ngeneralized agents",
    "start": "493880",
    "end": "499400"
  },
  {
    "text": "that can do a lot of multitask,\na multi-input predictions",
    "start": "499400",
    "end": "507880"
  },
  {
    "text": "like Gato. And so I think we will\nsee more of that, too. And finally, we also want\ndomain specific models.",
    "start": "507880",
    "end": "517240"
  },
  {
    "text": "So you might want\na GPT model, let's put it like maybe your health. So that could be like\na DoctorGPT model.",
    "start": "517240",
    "end": "523130"
  },
  {
    "text": "You might have a\nLawyerGPT model that's trained on only law data. So currently, we have GPT models\nthat are trained on everything.",
    "start": "523130",
    "end": "529210"
  },
  {
    "text": "But we might start to see\nmore niche models that are good at one task. And we could have a\nmixture of experts,",
    "start": "529210",
    "end": "535000"
  },
  {
    "text": "so it's like, you\ncan think this is a-- how you'd normally\nconsult an expert, you'll have expert AI models.",
    "start": "535000",
    "end": "540220"
  },
  {
    "text": "And you can go to a different AI\nmodel for your different needs. There are still a lot\nof missing ingredients",
    "start": "540220",
    "end": "547269"
  },
  {
    "text": "to make this all successful. The first of all\nis external memory.",
    "start": "547270",
    "end": "552415"
  },
  {
    "text": "We are already starting to\nsee this with the models like ChatGPT, where the\ninflections are short-lived.",
    "start": "552415",
    "end": "558520"
  },
  {
    "text": "There's no long-term\nmemory, and they don't have ability\nto remember or store conversations for long-term.",
    "start": "558520",
    "end": "565970"
  },
  {
    "text": "And this is something\nyou want to fix. Second is reducing the\ncomputation complexity.",
    "start": "565970",
    "end": "572779"
  },
  {
    "text": "So attention mechanism is\nquadratic over the sequence length, which is slow. And we want to reduce\nit and make it faster.",
    "start": "572780",
    "end": "580450"
  },
  {
    "text": "Another thing we\nwant to do is we want to enhance the\ncontrollability of these models like a lot of these\nmodels can be stochastic.",
    "start": "582855",
    "end": "588760"
  },
  {
    "text": "And we want to be able to\ncontrol what sort of outputs we get from them. And you might have\nexperienced the ChatGPT,",
    "start": "588760",
    "end": "594100"
  },
  {
    "text": "if you just refresh, you get\ndifferent output each time. But you might want to have\na mechanism that controls what sort of things you get.",
    "start": "594100",
    "end": "601180"
  },
  {
    "text": "And finally, we want to align\nour state of art language models with how the\nhuman brain works.",
    "start": "601180",
    "end": "606200"
  },
  {
    "text": "And we are seeing the\nsurge, but we still need more research on seeing\nhow they can make more informed.",
    "start": "606200",
    "end": "612010"
  },
  {
    "text": "Thank you. Great, hi. Yes, I'm excited to be here.",
    "start": "612010",
    "end": "618270"
  },
  {
    "start": "615000",
    "end": "639000"
  },
  {
    "text": "I live very nearby, so I got\nthe invites to come to class. And I was like, OK,\nI'll just walk over.",
    "start": "618270",
    "end": "623500"
  },
  {
    "text": "But then I spent like\n10 hours on the slides, so it wasn't as simple. So yeah, I'm going to\ntalk about transformers.",
    "start": "623500",
    "end": "630710"
  },
  {
    "text": "I'm going to skip the\nfirst two over there. I'm not going to\ntalk about those. We'll talk about that one\njust to simplify the lecture",
    "start": "630710",
    "end": "636390"
  },
  {
    "text": "since we don't have time. OK, so I wanted to provide\na little bit of context",
    "start": "636390",
    "end": "641600"
  },
  {
    "start": "639000",
    "end": "3630000"
  },
  {
    "text": "on why does this transformers\nclass even exist. So a little bit of\nhistorical context. I feel like Bilbo over there.",
    "start": "641600",
    "end": "647570"
  },
  {
    "text": "I joined like telling\nyou guys about this. I don't know if you guys\nsaw Lord of the Rings.",
    "start": "647570",
    "end": "652670"
  },
  {
    "text": "And basically, I joined AI in\nroughly 2012, the full course, so maybe a decade ago.",
    "start": "652670",
    "end": "658010"
  },
  {
    "text": "And back then, you\nwouldn't even say that you joined AI by the way. That was like a dirty word. Now, it's OK to talk\nabout, but back then, it",
    "start": "658010",
    "end": "664535"
  },
  {
    "text": "was not even deep learning. It was machine learning. That was the term we would\nuse if you were serious. But now, now, AI is\nOK to use, I think.",
    "start": "664535",
    "end": "671960"
  },
  {
    "text": "So basically, do\nyou even realize how lucky you are\npotentially entering this area in roughly 2023?",
    "start": "671960",
    "end": "677420"
  },
  {
    "text": "So back then, in 2011 or so\nwhen I was working specifically on computer vision, your\npipeline's looked like this.",
    "start": "677420",
    "end": "685960"
  },
  {
    "text": "So you wanted to\nclassify some images, you would go to a paper, and I\nthink this is representative. You would have three pages\nin the paper describing",
    "start": "685960",
    "end": "692933"
  },
  {
    "text": "all kinds of a zoo,\nof kitchen sink, of different kinds of\nfeatures, descriptors. And you would go\nto a poster session",
    "start": "692933",
    "end": "698853"
  },
  {
    "text": "and in computer\nvision conference, and everyone would have their\nfavorite feature descriptor that they're proposing. And it's totally\nridiculous, and you",
    "start": "698853",
    "end": "704200"
  },
  {
    "text": "would take notes on which\none you should incorporate into your pipeline because\nyou would extract all of them, and then you would\nput an SVM on top.",
    "start": "704200",
    "end": "709882"
  },
  {
    "text": "So that's what you would do. So there's two pages. Make sure you get your\n[? Spar ?] SIFT histograms, your SSIMs, your color\nhistograms, textiles,",
    "start": "709882",
    "end": "716110"
  },
  {
    "text": "tiny images. And don't forget the\ngeometry specific histograms. All of them have basically\ncomplicated code by themselves.",
    "start": "716110",
    "end": "722225"
  },
  {
    "text": "So you're collecting code from\neverywhere and running it, and it was a total nightmare. So on top of that,\nit also didn't work.",
    "start": "722225",
    "end": "730990"
  },
  {
    "text": "[LAUGHTER] So this would be, I think,\nit represents the prediction from that time. You would just get predictions\nlike this once in a while,",
    "start": "730990",
    "end": "737680"
  },
  {
    "text": "and you'd be like, you\njust shrug your shoulders like that just happens\nonce in a while. Today, you would be\nlooking for a bug.",
    "start": "737680",
    "end": "743680"
  },
  {
    "text": "And worse than that,\nevery single chunk of AI",
    "start": "743680",
    "end": "750640"
  },
  {
    "text": "had their own completely\nseparate vocabulary that they work with. So if you go to NLP\npapers, those papers",
    "start": "750640",
    "end": "756810"
  },
  {
    "text": "would be completely different. So you're reading the NLP\npaper, and you're like, what is this part\nof speech tagging,",
    "start": "756810",
    "end": "762490"
  },
  {
    "text": "morphological analysis,\nand tactic parsing, co-reference resolution? What is MPBTKJ?",
    "start": "762490",
    "end": "768190"
  },
  {
    "text": "And you're confused. So the vocabulary and everything\nwas completely different. And you couldn't\nread papers, I would say, across different areas.",
    "start": "768190",
    "end": "775100"
  },
  {
    "text": "So now, that\nchanged a little bit starting 2012 when Al Krizhevsky\nand colleagues basically",
    "start": "775100",
    "end": "782380"
  },
  {
    "text": "demonstrated that if you\nscale a large neural network on large data set, you can\nget very strong performance.",
    "start": "782380",
    "end": "788460"
  },
  {
    "text": "And so up till then, there was\na lot of focus on algorithms. But this showed that actually\nneural nets scale very well. So you need to now worry\nabout compute and data,",
    "start": "788460",
    "end": "795160"
  },
  {
    "text": "and you can scale it up. It works pretty well. And then that recipe\nactually did copy paste across many areas of AI.",
    "start": "795160",
    "end": "801520"
  },
  {
    "text": "So we start to see neural\nnetworks pop up everywhere since 2012. So we saw them in computer\nvision, and NLP, and speech,",
    "start": "801520",
    "end": "808060"
  },
  {
    "text": "and translation in RL and so on. So everyone started to use\nthe same kind of modeling toolkit, modeling framework.",
    "start": "808060",
    "end": "813985"
  },
  {
    "text": "And now when you go to NLP, and\nyou start reading papers there, in machine translation,\nfor example, this is a sequence\nto sequence paper",
    "start": "813985",
    "end": "820210"
  },
  {
    "text": "which we'll come\nback to in a bit. You start to read those\npapers, and you're like, OK, I can recognize these words.",
    "start": "820210",
    "end": "825340"
  },
  {
    "text": "Like there's a neural network. There's some parameters. There's an optimizer, and\nit starts to read things that you know of.",
    "start": "825340",
    "end": "830950"
  },
  {
    "text": "So that decreased tremendously\nthe barrier to entry across the different areas.",
    "start": "830950",
    "end": "836490"
  },
  {
    "text": "And then, I think,\nthe big deal is that when the transformer\ncame out in 2017, it's not even that just the tool\nkits and the neural networks",
    "start": "836490",
    "end": "842860"
  },
  {
    "text": "were similar-- there's that\nliterally the architectures converged to like one\narchitecture that you copy paste across\neverything seemingly.",
    "start": "842860",
    "end": "850180"
  },
  {
    "text": "So this was kind of an\nunassuming machine translation paper at the time, proposing\nto transformer architecture. But what we found since then\nis that you can just basically",
    "start": "850180",
    "end": "857964"
  },
  {
    "text": "copy paste this architecture\nand use it everywhere. And what's changing is\nthe details of the data,",
    "start": "857965",
    "end": "863890"
  },
  {
    "text": "and the chunking of the\ndata, and how you feed it in. And that's a\ncaricature, but it's kind of like a correct\nfirst order statement.",
    "start": "863890",
    "end": "869960"
  },
  {
    "text": "And so now, papers are\neven more similar looking because everyone's\njust using transformer. And so this convergence\nwas remarkable to watch",
    "start": "869960",
    "end": "878769"
  },
  {
    "text": "and unfolded over\nthe last decade. And it's pretty crazy to me. What I find\ninteresting is I think",
    "start": "878770",
    "end": "884038"
  },
  {
    "text": "this is some kind of a hint\nthat we're maybe converging to something that maybe\nthe brain is doing because the brain is very\nhomogeneous and uniform",
    "start": "884038",
    "end": "890560"
  },
  {
    "text": "across the entire\nsheet of your cortex. And OK, maybe some of\nthe details are changing, but those feel like\nhyperparameters",
    "start": "890560",
    "end": "896410"
  },
  {
    "text": "like a transformer. But your auditory cortex\nand your visual cortex and everything else\nlooks very similar. And so maybe we're\nconverging to some kind",
    "start": "896410",
    "end": "902780"
  },
  {
    "text": "of a uniform powerful\nlearning algorithm here. Something like that, I think,\nis interesting and exciting.",
    "start": "902780",
    "end": "909060"
  },
  {
    "text": "OK, so I want to talk about\nwhere the transformer came from briefly, historically. So I want to start in 2003.",
    "start": "909060",
    "end": "915430"
  },
  {
    "text": "I like this paper quite a bit. It was the first popular\napplication of neural networks",
    "start": "915430",
    "end": "921190"
  },
  {
    "text": "to the problem of\nlanguage modeling, so predicting in this\ncase, the next word in the sequence, which\nallows you to build generative models over text.",
    "start": "921190",
    "end": "927320"
  },
  {
    "text": "And in this case, they were\nusing multi-layer perceptron, so very simple neural net. The neural nets took three words\nand predicted the probability",
    "start": "927320",
    "end": "933443"
  },
  {
    "text": "distribution for the\nfourth word in a sequence. So this was well and\ngood at this point.",
    "start": "933443",
    "end": "939520"
  },
  {
    "text": "Now, over time, people\nstarted to apply this to machine translation. So that brings us to\nsequence to sequence paper",
    "start": "939520",
    "end": "945760"
  },
  {
    "text": "from 2014 that was\npretty influential, and the big problem\nhere was OK, we don't just want to take three\nwords and predict the fourth.",
    "start": "945760",
    "end": "952270"
  },
  {
    "text": "We want to predict how to\ngo from an English sentence to a French sentence. And the key problem\nwas OK, you can",
    "start": "952270",
    "end": "958387"
  },
  {
    "text": "have arbitrary number of words\nin English and arbitrary number of words in French,\nso how do you get an architecture\nthat can process",
    "start": "958387",
    "end": "964750"
  },
  {
    "text": "this variably sized input? And so here they used a\nLSDM, and there's basically",
    "start": "964750",
    "end": "970330"
  },
  {
    "text": "two chunks of this, which are\ncovered by the slack, by this.",
    "start": "970330",
    "end": "976160"
  },
  {
    "text": "But basically have an\nencoder LSDM on the left, and it just consumes\none word at a time",
    "start": "976160",
    "end": "982190"
  },
  {
    "text": "and builds up a context\nof what it has read. And then that acts as\na conditioning vector to the decoder RNN or LSDM.",
    "start": "982190",
    "end": "989020"
  },
  {
    "text": "That basically\ngoes chonk, chonk, chonk for the next\nword in a sequence, translating the English to\nFrench or something like that.",
    "start": "989020",
    "end": "995438"
  },
  {
    "text": "Now, the big problem with\nthis, that people identified, I think, very quickly\nand tried to resolve is that there's what's called\nthis encoder bottleneck.",
    "start": "995438",
    "end": "1003320"
  },
  {
    "text": "So this entire English sentence\nthat we are trying to condition on is packed into\na single vector that goes from the\nencoder to the decoder.",
    "start": "1003320",
    "end": "1010880"
  },
  {
    "text": "And so this is just\ntoo much information to potentially maintain\nin a single vector, and that didn't seem correct. And so people who are\nlooking around for ways",
    "start": "1010880",
    "end": "1017455"
  },
  {
    "text": "to alleviate the attention of\nthe encoder bottleneck as it was called at the time. And so that brings\nus to this paper,",
    "start": "1017455",
    "end": "1023773"
  },
  {
    "text": "Neural Machine Translation\nby Jointly Learning to Align and Translate. And here, just quoting from\nthe abstract, \"in this paper,",
    "start": "1023773",
    "end": "1031553"
  },
  {
    "text": "we conjectured that the use\nof a fixed length vector is a bottleneck in\nimproving the performance of the basic\nencoder-decoder architecture",
    "start": "1031553",
    "end": "1037303"
  },
  {
    "text": "and propose to extend\nthis by allowing the model to\nautomatically soft search for parts of the source sentence\nthat are relevant to predicting",
    "start": "1037304",
    "end": "1044366"
  },
  {
    "text": "a target word without\nhaving to form these parts or hard\nsegments exclusively.\"",
    "start": "1044367",
    "end": "1050050"
  },
  {
    "text": "So this was a way to look\nback to the words that are coming from the encoder.",
    "start": "1050050",
    "end": "1055950"
  },
  {
    "text": "And it was achieved\nusing this soft search. So as you are\ndecoding in the words",
    "start": "1055950",
    "end": "1062250"
  },
  {
    "text": "here, while you\nare decoding them, you are allowed to\nlook back at the words at the encoder via this soft\nattention mechanism proposed",
    "start": "1062250",
    "end": "1069150"
  },
  {
    "text": "in this paper. And so this paper, I think,\nis the first time that I saw, basically, attention.",
    "start": "1069150",
    "end": "1075690"
  },
  {
    "text": "So your context vector\nthat comes from the encoder is a weighted sum\nof the hidden states",
    "start": "1075690",
    "end": "1081150"
  },
  {
    "text": "of the words in the encoding. And then the weights\nof this sum come",
    "start": "1081150",
    "end": "1087450"
  },
  {
    "text": "from a softmax that is based\non these compatibilities between the current\nstate as you're decoding",
    "start": "1087450",
    "end": "1093300"
  },
  {
    "text": "and the hidden states\ngenerated by the encoder. And so this is the first\ntime that really you start to look at it, and this\nis the current modern equations",
    "start": "1093300",
    "end": "1102059"
  },
  {
    "text": "of the attention. And I think this was the\nfirst paper that I saw it in. It's the first time\nthat there's a word",
    "start": "1102060",
    "end": "1107670"
  },
  {
    "text": "attention used, as far as I\nknow, to call this mechanism. So I actually tried to dig\ninto the details of the history",
    "start": "1107670",
    "end": "1114480"
  },
  {
    "text": "of the attention. So the first author\nhere, Dzmitry, I had an email\ncorrespondence with him,",
    "start": "1114480",
    "end": "1120059"
  },
  {
    "text": "and I basically\nsent him an email. I'm like, Dzmitry, this\nis really interesting. Just rumors have taken over. Where did you come up\nwith the soft attention",
    "start": "1120060",
    "end": "1125820"
  },
  {
    "text": "mechanism that ends up being\nthe heart of the transformer? And to my surprise, he wrote me\nback this massive email, which",
    "start": "1125820",
    "end": "1132037"
  },
  {
    "text": "was really fascinating. So this is an excerpt\nfrom that email.",
    "start": "1132037",
    "end": "1134578"
  },
  {
    "text": "So basically, he talks about\nhow he was looking for a way to avoid this bottleneck\nbetween the encoder and decoder.",
    "start": "1137120",
    "end": "1142490"
  },
  {
    "text": "He had some ideas\nabout cursors that traverse the sequences\nthat didn't quite work out. And then here, \"so one\nday, I had this thought",
    "start": "1142490",
    "end": "1148910"
  },
  {
    "text": "that it would be nice\nto enable the decoder RNN to learn to search where\nto put the cursor in the source sequence.",
    "start": "1148910",
    "end": "1154610"
  },
  {
    "text": "This was sort of inspired\nby translation exercises that learning English in\nmy middle school involved.",
    "start": "1154610",
    "end": "1161150"
  },
  {
    "text": "Your gaze shifts back and forth\nbetween source and target, sequence as you translate.\" So literally, I thought that\nthis was kind of interesting,",
    "start": "1161150",
    "end": "1167150"
  },
  {
    "text": "that he's not a native\nEnglish speaker, and here, that gave him an edge\nin this machine translation that led to attention and\nthen led to transformer.",
    "start": "1167150",
    "end": "1174519"
  },
  {
    "text": "So that's really fascinating. \"I expressed a soft\nsearch a softmax and then weighted averaging\nof the [INAUDIBLE] states.",
    "start": "1174520",
    "end": "1180920"
  },
  {
    "text": "And basically, to\nmy great excitement, this worked from\nthe very first try.\" So really, I think,\ninteresting piece of history.",
    "start": "1180920",
    "end": "1188390"
  },
  {
    "text": "And as it later turned out\nthat the name of RNN search was kind of lame, so the\nbetter name attention came",
    "start": "1188390",
    "end": "1194060"
  },
  {
    "text": "from Yoshua on one\nof the final passes as they went over the paper. So maybe Attention\nis All You Need",
    "start": "1194060",
    "end": "1200960"
  },
  {
    "text": "would have been called RNN\nSearch is All You Need, but we have Yoshua\nBengio to thank for a little bit of\nbetter name, I would say.",
    "start": "1200960",
    "end": "1207049"
  },
  {
    "text": "So apparently,\nthat's the history of this, which I\nthought was interesting. OK, so that brings us to\n2017, which is Attention",
    "start": "1207050",
    "end": "1213710"
  },
  {
    "text": "is All You Need. So this attention\ncomponent, which in Dzmitry's paper was\njust one small segment,",
    "start": "1213710",
    "end": "1219020"
  },
  {
    "text": "and there's all this\nbidirectional RNN, RNN and decoder, and this Attention\nAll You Need paper is saying,",
    "start": "1219020",
    "end": "1225235"
  },
  {
    "text": "OK, you can actually\ndelete everything. What's making this\nwork very well is just attention by itself. And so delete everything,\nkeep attention.",
    "start": "1225235",
    "end": "1232100"
  },
  {
    "text": "And then what's remarkable about\nthis paper actually is usually, you see papers that\nare very incremental. They add one thing, and\nthey show that it's better.",
    "start": "1232100",
    "end": "1239810"
  },
  {
    "text": "But I feel like\nAttention is All You Need was like a mix of multiple\nthings at the same time. They were combined\nin a very unique way,",
    "start": "1239810",
    "end": "1246380"
  },
  {
    "text": "and then also achieve a\nvery good local minimum in the architecture space. And so to me, this is\nreally a landmark paper",
    "start": "1246380",
    "end": "1252530"
  },
  {
    "text": "that is quite\nremarkable and, I think, had quite a lot of\nwork behind the scenes.",
    "start": "1252530",
    "end": "1258650"
  },
  {
    "text": "So delete all the RNN,\njust keep attention. Because attention\noperates over sets-- and I'm going to go\nto this in a second--",
    "start": "1258650",
    "end": "1265270"
  },
  {
    "text": "you now need to positionally\nencode your inputs because attention doesn't have\nthe notion of space by itself.",
    "start": "1265270",
    "end": "1270240"
  },
  {
    "text": "I have to be very careful. They adopted this\nresidual network structure",
    "start": "1274685",
    "end": "1279905"
  },
  {
    "text": "from resonance. They interspersed attention\nwith multi-layer perceptrons. They used layer norms, which\ncame from a different paper.",
    "start": "1279905",
    "end": "1287013"
  },
  {
    "text": "They introduced the concept\nof multiple heads of attention that were applied in parallel. And they gave us, I think,\nlike a fairly good set",
    "start": "1287013",
    "end": "1293000"
  },
  {
    "text": "of hyperparameters that\nto this day are used. So the expansion factor in the\nmulti-layer perceptron goes up",
    "start": "1293000",
    "end": "1299510"
  },
  {
    "text": "by 4X-- and we'll go into\na bit more detail-- and this 4X has stuck around. And I believe there's\na number of papers",
    "start": "1299510",
    "end": "1304968"
  },
  {
    "text": "that try to play with all\nkinds of little details of the transformer, and nothing\nsticks because this is actually",
    "start": "1304968",
    "end": "1310730"
  },
  {
    "text": "quite good. The only thing to my\nknowledge that didn't stick was this reshuffling\nof the layer norms",
    "start": "1310730",
    "end": "1316820"
  },
  {
    "text": "to go into the prenorm\nversion where here you see the layer norms are after\nthe multiheaded attention feed",
    "start": "1316820",
    "end": "1321920"
  },
  {
    "text": "forward. They just put them\nbefore instead. So just reshuffling of\nlayer norms, but otherwise, the TPTs and everything else\nthat you're seeing today",
    "start": "1321920",
    "end": "1328568"
  },
  {
    "text": "is basically the 2017\narchitecture from 5 years ago. And even though everyone\nis working on it,",
    "start": "1328568",
    "end": "1333680"
  },
  {
    "text": "it's been proven\nremarkably resilient, which I think is\nreal interesting. There are innovations\nthat, I think,",
    "start": "1333680",
    "end": "1338780"
  },
  {
    "text": "have been adopted also\nin positional encoding. It's more common to use\ndifferent rotary and relative",
    "start": "1338780",
    "end": "1344000"
  },
  {
    "text": "positional encoding and so on. So I think there have been\nchanges, but for the most part, it's proven very resilient.",
    "start": "1344000",
    "end": "1351070"
  },
  {
    "text": "So really quite an\ninteresting paper. Now, I wanted to go into\nthe attention mechanism.",
    "start": "1351070",
    "end": "1356720"
  },
  {
    "text": "And I think, the way I interpret\nit is not similar to the ways",
    "start": "1356720",
    "end": "1363091"
  },
  {
    "text": "that I've seen it\npresented before. So let me try a different\nway of how I see it. Basically, to me, attention is\nkind of like the communication",
    "start": "1363092",
    "end": "1369960"
  },
  {
    "text": "phase of the transformer,\nand the transformer interweaves two phases of the\ncommunication phase, which",
    "start": "1369960",
    "end": "1375617"
  },
  {
    "text": "is the multi-headed\nattention, and the computation stage, which is this\nmultilayered perceptron or [INAUDIBLE].",
    "start": "1375617",
    "end": "1381539"
  },
  {
    "text": "So in the communication\nphase, it's really just a data\ndependent message passing on directed graphs.",
    "start": "1381540",
    "end": "1387280"
  },
  {
    "text": "And you can think of it\nas OK, forget everything with machine\ntranslation, everything. Let's just-- we have\ndirected graphs.",
    "start": "1387280",
    "end": "1393120"
  },
  {
    "text": "At each node, you\nare storing a vector. And then let me talk now\nabout the communication",
    "start": "1393120",
    "end": "1398715"
  },
  {
    "text": "phase of how these\nvectors talk to each other and this directed graph. And then the compute\nphase later is just a multi-perceptron, which then\nbasically acts on every node",
    "start": "1398715",
    "end": "1407700"
  },
  {
    "text": "individually. But how do these nodes\ntalk to each other in this directed graph?",
    "start": "1407700",
    "end": "1412930"
  },
  {
    "text": "So I wrote like\nsome simple Python-- I wrote this in Python\nbasically to create",
    "start": "1412930",
    "end": "1419340"
  },
  {
    "text": "one round of communication\nof using attention as the message passing scheme.",
    "start": "1419340",
    "end": "1426550"
  },
  {
    "text": "So here, a node has this\nprivate data vector, as you can think of it\nas private information",
    "start": "1426550",
    "end": "1433080"
  },
  {
    "text": "to this node. And then it can also emit a\nkey, a query, and a value. And simply, that's done\nby linear transformation",
    "start": "1433080",
    "end": "1440400"
  },
  {
    "text": "from this node. So the key is what are\nthe things that I am--",
    "start": "1440400",
    "end": "1447220"
  },
  {
    "text": "sorry. The query is what are the\nthings that I'm looking for? The key is what other\nthe things that I have? And the value is what are the\nthings that I will communicate?",
    "start": "1447220",
    "end": "1455049"
  },
  {
    "text": "And so then when you\nhave your graph that's made up of nodes in some\nrandom edges, when you actually have these nodes communicating,\nwhat's happening is",
    "start": "1455050",
    "end": "1461380"
  },
  {
    "text": "you loop over all the\nnodes individually in some random order,\nand you're at some node,",
    "start": "1461380",
    "end": "1467110"
  },
  {
    "text": "and you get the\nquery vector q, which is, I'm a node in\nsome graph, and this",
    "start": "1467110",
    "end": "1472595"
  },
  {
    "text": "is what I'm looking for. And so that's just achieved\nvia this linear transformation here. And then we look at all the\ninputs that point to this node,",
    "start": "1472595",
    "end": "1479717"
  },
  {
    "text": "and then they broadcast what\nare the things that I have, which is their keys. So they broadcast the keys.",
    "start": "1479717",
    "end": "1485680"
  },
  {
    "text": "I have the query, then those\ninteract by dot product to get scores.",
    "start": "1485680",
    "end": "1491210"
  },
  {
    "text": "So basically, simply\nby doing dot product, you get some\nunnormalized weighting of the interestingness of all\nof the information in the nodes",
    "start": "1491210",
    "end": "1499870"
  },
  {
    "text": "that point to me and to\nthe things I'm looking for. And then when you normalize\nthat with softmax, so it just sums to\n1, you basically just",
    "start": "1499870",
    "end": "1506743"
  },
  {
    "text": "end up using those scores, which\nnow sum to 1 in our probability distribution, and you do a\nweighted sum of the values",
    "start": "1506743",
    "end": "1513280"
  },
  {
    "text": "to get your update. So I have a query. They have keys, dot products\nto get interestingness or like",
    "start": "1513280",
    "end": "1521500"
  },
  {
    "text": "affinity, softmax to\nnormalize it, and then weighted sum of those values\nflow to me and update me.",
    "start": "1521500",
    "end": "1527398"
  },
  {
    "text": "And this is happening for\neach node individually. And then we update at the end. And so this kind of a\nmessage passing scheme",
    "start": "1527398",
    "end": "1532540"
  },
  {
    "text": "is at the heart of\nthe transformer. And it happens in the more\nvectorized batched way",
    "start": "1532540",
    "end": "1540205"
  },
  {
    "text": "that is more confusing and is\nalso interspersed with layer norms and things like that\nto make the training behave",
    "start": "1540205",
    "end": "1546640"
  },
  {
    "text": "better. But that's roughly what's\nhappening in the attention mechanism, I think,\non a high level.",
    "start": "1546640",
    "end": "1551140"
  },
  {
    "text": "So yeah, so in the communication\nphase of the transformer, then",
    "start": "1553720",
    "end": "1559030"
  },
  {
    "text": "this message passing\nscheme happens in every head in parallel and\nthen in every layer in series",
    "start": "1559030",
    "end": "1566490"
  },
  {
    "text": "and with different\nweights each time. And that's it as far as the\nmulti-headed attention goes.",
    "start": "1566490",
    "end": "1573150"
  },
  {
    "text": "And so if you look at these\nencooder-decoder models, you can think of it then in\nterms of the connectivity of these nodes in the graph.",
    "start": "1573150",
    "end": "1579210"
  },
  {
    "text": "You can think of it as like,\nOK, all these tokens that are in the encoder that\nwe want to condition on, they are fully\nconnected to each other.",
    "start": "1579210",
    "end": "1585600"
  },
  {
    "text": "So when they communicate,\nthey communicate fully when you calculate\ntheir features. But in the decoder,\nbecause we are",
    "start": "1585600",
    "end": "1592140"
  },
  {
    "text": "trying to have a\nlanguage model, we don't want to have\ncommunication for future tokens because they give away\nthe answer at this step.",
    "start": "1592140",
    "end": "1598169"
  },
  {
    "text": "So the tokens in the\ndecoder are fully connected from all the encoder\nstates, and then they",
    "start": "1598170",
    "end": "1603645"
  },
  {
    "text": "are also fully connected from\neverything that is decoding. And so you end up with\nthis triangular structure",
    "start": "1603645",
    "end": "1609149"
  },
  {
    "text": "in the data graph. But that's the\nmessage passing scheme that this basically implements.",
    "start": "1609150",
    "end": "1614815"
  },
  {
    "text": "And then you have to be also\na little bit careful because in the cross attention\nhere with the decoder, you consume the features\nfrom the top of the encoder.",
    "start": "1614815",
    "end": "1621620"
  },
  {
    "text": "So think of it as\nin the encoder, all the nodes are\nlooking at each other, all the tokens are looking at\neach other many, many times.",
    "start": "1621620",
    "end": "1628320"
  },
  {
    "text": "And they really figure\nout what's in there, and then the decoder when it's\nlooking only at the top nodes.",
    "start": "1628320",
    "end": "1632302"
  },
  {
    "text": "So that's roughly the\nmessage passing scheme. I was going to go into\nmore of an implementation of a transformer. I don't know if there's\nany questions about this.",
    "start": "1634875",
    "end": "1643125"
  },
  {
    "text": "[INAUDIBLE] self-attention\nand multi-headed attention, but what is the\nadvantage of [INAUDIBLE]??",
    "start": "1643125",
    "end": "1650420"
  },
  {
    "text": "Yeah, so self-attention and\nmulti-headed attention, so the multi-headed attention is\njust this attention scheme,",
    "start": "1650420",
    "end": "1658000"
  },
  {
    "text": "but it's just applied\nmultiple times in parallel. Multiple heads just means\nindependent applications of the same attention.",
    "start": "1658000",
    "end": "1664970"
  },
  {
    "text": "So this message passing\nscheme basically just happens in parallel\nmultiple times with different weights for\nthe query, key, and value.",
    "start": "1664970",
    "end": "1672940"
  },
  {
    "text": "So you can almost look at\nit like in parallel, I'm looking for, I'm seeking\ndifferent kinds of information from different nodes.",
    "start": "1672940",
    "end": "1679030"
  },
  {
    "text": "And I'm collecting it\nall in the same node. It's all done in parallel. So heads is really just\ncopy-paste in parallel.",
    "start": "1679030",
    "end": "1686980"
  },
  {
    "text": "And layers are\ncopy-paste but in series.",
    "start": "1686980",
    "end": "1692682"
  },
  {
    "text": "Maybe that makes sense. And self-attention, when\nit's self-attention,",
    "start": "1692682",
    "end": "1698610"
  },
  {
    "text": "what it's referring to\nis that the node here produces each node here. So as I described it here,\nthis is really self-attention",
    "start": "1698610",
    "end": "1705632"
  },
  {
    "text": "because every one of\nthese nodes produces a key query and a value\nfrom this individual node. When you have cross-attention,\nyou have one cross-attention",
    "start": "1705632",
    "end": "1713850"
  },
  {
    "text": "here, coming from the encoder. That just means that\nthe queries are still produced from this node,\nbut the keys and the values",
    "start": "1713850",
    "end": "1722400"
  },
  {
    "text": "are produced as a\nfunction of nodes that are coming from the encoder.",
    "start": "1722400",
    "end": "1728130"
  },
  {
    "text": "So I have my queries because\nI'm trying to decode some-- the fifth word in the sequence.",
    "start": "1728130",
    "end": "1733933"
  },
  {
    "text": "And I'm looking\nfor certain things because I'm the fifth word. And then the keys and\nthe values in terms of the source of information\nthat could answer my queries",
    "start": "1733933",
    "end": "1741350"
  },
  {
    "text": "can come from the previous\nnodes in the current decoding sequence or from the\ntop of the encoder.",
    "start": "1741350",
    "end": "1746670"
  },
  {
    "text": "So all the nodes that\nhave already seen all of the encoding tokens many,\nmany times cannot broadcast",
    "start": "1746670",
    "end": "1752120"
  },
  {
    "text": "what they contain in\nterms of information. So I guess, to summarize,\nthe self-attention is--",
    "start": "1752120",
    "end": "1758652"
  },
  {
    "text": "sorry, cross-attention\nand self-attention only differ in where the piece\nand the values come from.",
    "start": "1758652",
    "end": "1764200"
  },
  {
    "text": "Either the keys and values\nare produced from this node, or they are produced from some\nexternal source like an encoder",
    "start": "1764200",
    "end": "1771340"
  },
  {
    "text": "and the nodes over there. But algorithmically, is the\nsame mathematical operations.",
    "start": "1771340",
    "end": "1779000"
  },
  {
    "text": "Question. Yeah, OK. So two questions for you. First question is, in the\nmessage passing [INAUDIBLE]",
    "start": "1779000",
    "end": "1788690"
  },
  {
    "text": "So think of-- so each one\nof these nodes is a token.",
    "start": "1796690",
    "end": "1800799"
  },
  {
    "text": "I guess they don't have\na very good picture of it in the transformer. But this node here could\nrepresent the third word",
    "start": "1804068",
    "end": "1814929"
  },
  {
    "text": "in the output in the decoder,\nand in the beginning, it is just the\nembedding of the word.",
    "start": "1814930",
    "end": "1821290"
  },
  {
    "text": "And then, OK, I have to\nthink through this analogy a little bit more. I came up with it this morning.",
    "start": "1827120",
    "end": "1832712"
  },
  {
    "text": "[LAUGHTER] [INAUDIBLE]",
    "start": "1832712",
    "end": "1835830"
  },
  {
    "text": "What example of instantiation\n[INAUDIBLE] nodes",
    "start": "1839940",
    "end": "1845865"
  },
  {
    "text": "as in in blocks were embedding? These nodes are\nbasically the vectors.",
    "start": "1845865",
    "end": "1853202"
  },
  {
    "text": "I'll go to an implementation. I'll go to the implementation,\nand then maybe I'll make the connections\nto the graph.",
    "start": "1853202",
    "end": "1858780"
  },
  {
    "text": "So let me try to first\ngo to-- let me now go to, with this intuition\nin mind, at least, to a nanoGPT, which is a\nconcrete implementation",
    "start": "1858780",
    "end": "1865260"
  },
  {
    "text": "of a transformer\nthat is very minimal. So I worked on this\nover the last few days, and here it is reproducing\nGPT-2 on open web text.",
    "start": "1865260",
    "end": "1871737"
  },
  {
    "text": "So it's a pretty serious\nimplementation that reproduces GPT-2, I would say, and\nprovide it enough compute--",
    "start": "1871738",
    "end": "1877870"
  },
  {
    "text": "This was one node of 8 GPUs\nfor 38 hours or something like that, if I\nremember correctly. And it's very readable.",
    "start": "1877870",
    "end": "1883910"
  },
  {
    "text": "It's 300 lines, so everyone\ncan take a look at it. And yeah, let me basically\nbriefly step through it.",
    "start": "1883910",
    "end": "1890622"
  },
  {
    "text": "So let's try to have a\ndecoder-only transformer. So what that means is that\nit's a language model.",
    "start": "1890622",
    "end": "1896120"
  },
  {
    "text": "It tries to model the\nnext word in the sequence or the next character\nin the sequence.",
    "start": "1896120",
    "end": "1901520"
  },
  {
    "text": "So the data that\nwe train on this is always some kind of text. So here's some fake Shakespeare. Sorry, this is real Shakespeare.",
    "start": "1901520",
    "end": "1907190"
  },
  {
    "text": "We're going to produce\nfake Shakespeare. So this is called\na Tiny Shakespeare dataset, which is one of\nmy favorite toy datasets.",
    "start": "1907190",
    "end": "1912346"
  },
  {
    "text": "You take all of\nShakespeare, concatenate it, and it's 1 megabyte\nfile, and then you can train\nlanguage models on it and get infinite\nShakespeare, if you like,",
    "start": "1912347",
    "end": "1918440"
  },
  {
    "text": "which I think is kind of cool. So we have a text. The first thing we\nneed to do is we need to convert it to\na sequence of integers",
    "start": "1918440",
    "end": "1925159"
  },
  {
    "text": "because transformers\nnatively process-- you can't plug text\ninto transformer.",
    "start": "1925160",
    "end": "1930662"
  },
  {
    "text": "You need to somehow encode it. So the way that\nencoding is done is we convert, for example,\nin the simplest case, every character gets an\ninteger, and then instead of \"hi",
    "start": "1930662",
    "end": "1938810"
  },
  {
    "text": "there,\" we would have\nthis sequence of integers. So then you can encode every\nsingle character as an integer",
    "start": "1938810",
    "end": "1945490"
  },
  {
    "text": "and get a massive\nsequence of integers. You just concatenate\nit all into one large, long\none-dimensional sequence.",
    "start": "1945490",
    "end": "1951419"
  },
  {
    "text": "And then you can train on it. Now, here, we only\nhave a single document. In some cases, if you have\nmultiple independent documents,",
    "start": "1951420",
    "end": "1956980"
  },
  {
    "text": "what people like to do\nis create special tokens, and they intersperse\nthose documents with those special\nend of text tokens",
    "start": "1956980",
    "end": "1962500"
  },
  {
    "text": "that they splice in between\nto create boundaries. But those boundaries actually\ndon't have any modeling impact.",
    "start": "1962500",
    "end": "1970860"
  },
  {
    "text": "It's just that the\ntransformer is supposed to learn via backpropagation\nthat the end of document sequence means that you\nshould wipe the memory.",
    "start": "1970860",
    "end": "1980020"
  },
  {
    "text": "OK, so then we produce batches. So these batches\nof data just mean that we go back to the\none-dimensional sequence,",
    "start": "1980020",
    "end": "1986380"
  },
  {
    "text": "and we take out chunks\nof this sequence. So say, if the block size is 8,\nThen the block size indicates",
    "start": "1986380",
    "end": "1993775"
  },
  {
    "text": "the maximum length of context\nthat your transformer will process. So if our block size\nis 8, that means",
    "start": "1993775",
    "end": "2000510"
  },
  {
    "text": "that we are going to have up\nto eight characters of context to predict the ninth\ncharacter in a sequence.",
    "start": "2000510",
    "end": "2006630"
  },
  {
    "text": "And the batch size indicates\nhow many sequences in parallel we're going to process. And we want this to be\nas large as possible,",
    "start": "2006630",
    "end": "2011880"
  },
  {
    "text": "so we're fully taking\nadvantage of the GPU and the parallels [INAUDIBLE]\nSo in this example, we're doing a 4 by 8 batches.",
    "start": "2011880",
    "end": "2018000"
  },
  {
    "text": "So every row here is\nindependent example and then every row here is a\nsmall chunk of the sequence",
    "start": "2018000",
    "end": "2027412"
  },
  {
    "text": "that we're going to train on. And then we have both the\ninputs and the targets at every single point here.",
    "start": "2027412",
    "end": "2032580"
  },
  {
    "text": "So to fully spell out what's\ncontained in a single 4 by 8 batch to the transformer-- I sort of compact it here--",
    "start": "2032580",
    "end": "2039110"
  },
  {
    "text": "so when the input is 47, by\nitself, the target is 58.",
    "start": "2039110",
    "end": "2044670"
  },
  {
    "text": "And when the input is\nthe sequence 47, 58, the target is one. And when it's 47, 58, 1,\nthe target is 51 and so on.",
    "start": "2044670",
    "end": "2053020"
  },
  {
    "text": "So actually, the single batch\nof examples that score by 8 actually has a ton of\nindividual examples that we are expecting\na transformer",
    "start": "2053020",
    "end": "2058949"
  },
  {
    "text": "to learn on in parallel. And so you'll see that\nthe batches are learned on completely independently, but\nthe time dimension here along",
    "start": "2058949",
    "end": "2068459"
  },
  {
    "text": "horizontally is also\ntrained on in parallel. So your real batch size\nis more like B times T.",
    "start": "2068459",
    "end": "2074310"
  },
  {
    "text": "And it's just that the\ncontext grows linearly for the predictions that you\nmake along the T direction",
    "start": "2074310",
    "end": "2081330"
  },
  {
    "text": "in the model. So this is all the examples\nthat the model will learn from, this single batch.",
    "start": "2081330",
    "end": "2088830"
  },
  {
    "text": "So now, this is the GPT class. And because this is\na decoder-only model,",
    "start": "2088830",
    "end": "2095947"
  },
  {
    "text": "so we're not going to have\nan encoder because there's no English we're translating from-- we're not trying to condition\nin some other external",
    "start": "2095947",
    "end": "2102119"
  },
  {
    "text": "information. We're just trying to produce\na sequence of words that follow each other or likely to.",
    "start": "2102120",
    "end": "2108090"
  },
  {
    "text": "So this is all PyTorch, and\nI'm going slightly faster because I'm assuming people\nhave taken 231 or something along those lines.",
    "start": "2108090",
    "end": "2115210"
  },
  {
    "text": "But here in the forward\npass, we take these indices, and then we both encode the\nidentity of the indices,",
    "start": "2115210",
    "end": "2124500"
  },
  {
    "text": "just via an embedding\nlookup table. So every single integer, we\nindex into a lookup table of",
    "start": "2124500",
    "end": "2131190"
  },
  {
    "text": "vectors in this, and end\nup embedding, and pull out the word vector for that token.",
    "start": "2131190",
    "end": "2138099"
  },
  {
    "text": "And then because the\ntransformer by itself doesn't actually-- the\nprocess is set natively.",
    "start": "2138100",
    "end": "2143390"
  },
  {
    "text": "So we need to also positionally\nencode these vectors so that we basically\nhave both the information about the token identity and\nits place in the sequence from 1",
    "start": "2143390",
    "end": "2151680"
  },
  {
    "text": "to block size. Now, the information\nabout what and where is combined additively,\nso the token embeddings",
    "start": "2151680",
    "end": "2158880"
  },
  {
    "text": "and the positional embeddings\nare just added exactly as here. So then there's\noptional dropout,",
    "start": "2158880",
    "end": "2166800"
  },
  {
    "text": "this x here basically\njust contains the set of words\nand their positions,",
    "start": "2166800",
    "end": "2174870"
  },
  {
    "text": "and that feeds into the\nblocks of transformer. And we're going to look\ninto what's block here. But for here, for now,\nthis is just a series",
    "start": "2174870",
    "end": "2180600"
  },
  {
    "text": "of blocks in a transformer. And then in the end,\nthere's a layer norm, and then you're\ndecoding the logits",
    "start": "2180600",
    "end": "2186800"
  },
  {
    "text": "for the next word or next\ninteger in a sequence, using the linear projection of\nthe output of this transformer",
    "start": "2186800",
    "end": "2193470"
  },
  {
    "text": "So LM head here, a short\ncore language model head. It's just a linear function.",
    "start": "2193470",
    "end": "2198946"
  },
  {
    "text": "So basically, positionally\nencode all the words, feed them into a\nsequence of blocks,",
    "start": "2198946",
    "end": "2205230"
  },
  {
    "text": "and then apply a linear\nlayer to get the probability distribution for\nthe next character.",
    "start": "2205230",
    "end": "2210337"
  },
  {
    "text": "And then if we have\nthe targets, which we produced in the data order-- and you'll notice that\nthe targets are just",
    "start": "2210337",
    "end": "2215850"
  },
  {
    "text": "the inputs offset\nby one in time-- then those targets feed\ninto a cross entropy loss.",
    "start": "2215850",
    "end": "2221380"
  },
  {
    "text": "So this is just a\nnegative log likelihood typical classification loss. So now let's drill into\nwhat's here in the blocks.",
    "start": "2221380",
    "end": "2228840"
  },
  {
    "text": "So these blocks that are\napplied sequentially, there's, again, as I\nmentioned, this communicate phase and the compute phase.",
    "start": "2228840",
    "end": "2235000"
  },
  {
    "text": "So in the communicate\nphase, all the nodes get to talk to each other, and\nso these nodes are basically,",
    "start": "2235000",
    "end": "2241260"
  },
  {
    "text": "if our block size\nis 8, then we are going to have eight\nnodes in this graph.",
    "start": "2241260",
    "end": "2246404"
  },
  {
    "text": "There's eight nodes\nin this graph. The first node is pointed\nto only by itself. The second node is pointed to\nby the first node and itself.",
    "start": "2246405",
    "end": "2253325"
  },
  {
    "text": "The third node is pointed\nto by the first two nodes and itself, et cetera. So there's eight nodes here.",
    "start": "2253325",
    "end": "2258940"
  },
  {
    "text": "So you apply-- there's a\nresidual pathway and x. You take it out. You apply a layer norm,\nand then the self-attention",
    "start": "2258940",
    "end": "2265450"
  },
  {
    "text": "so that these communicate,\nthese eight nodes communicate. But you have to keep in\nmind that the batch is 4. So because batch is 4,\nthis is also applied--",
    "start": "2265450",
    "end": "2274180"
  },
  {
    "text": "so we have eight\nnodes communicating, but there's a batch of four of\nthem individually communicating in one of those eight nodes.",
    "start": "2274180",
    "end": "2279880"
  },
  {
    "text": "There's no crisscross across\nthe batch dimension, of course. There's no batch\nanywhere luckily. And then once they've\nchanged information,",
    "start": "2279880",
    "end": "2286809"
  },
  {
    "text": "they are processed using\nthe multi-layer perceptron. And that's the compute phase.",
    "start": "2286810",
    "end": "2292630"
  },
  {
    "text": "And then also here we are\nmissing the cross-attention",
    "start": "2292630",
    "end": "2298138"
  },
  {
    "text": "because this is a\ndecoder-only model. So all we have is\nthis step here, the multi-headed\nattention, and that's this line, the\ncommunicate phase.",
    "start": "2298138",
    "end": "2304580"
  },
  {
    "text": "And then we have the feed\nforward, which is the MLP, and that's the compute phase.",
    "start": "2304580",
    "end": "2309710"
  },
  {
    "text": "I'll take question's\na bit later. Then the MLP here is\nfairly straightforward.",
    "start": "2309710",
    "end": "2314745"
  },
  {
    "text": "The MLP is just individual\nprocessing on each node, just transforming the feature\nrepresentation at that node.",
    "start": "2314745",
    "end": "2321530"
  },
  {
    "text": "So applying a\ntwo-layer neural net with a GELU nonlinearity,\nwhich is just",
    "start": "2321530",
    "end": "2327205"
  },
  {
    "text": "think of it as a ReLU\nor something like that. It's just a nonlinearity. And then MLP is straightforward.",
    "start": "2327205",
    "end": "2333609"
  },
  {
    "text": "I don't think there's\nanything too crazy there. And then this is the\ncausal self-attention part, the communication phase.",
    "start": "2333610",
    "end": "2339750"
  },
  {
    "text": "So this is like\nthe meat of things and the most complicated part. It's only complicated\nbecause of the batching",
    "start": "2339750",
    "end": "2346599"
  },
  {
    "text": "and the implementation detail\nof how you mask the connectivity in the graph so that\nyou can't obtain",
    "start": "2346600",
    "end": "2353620"
  },
  {
    "text": "any information\nfrom the future when you're predicting your token. Otherwise, it gives\naway the information. So if I'm the fifth token and\nif I'm the fifth position,",
    "start": "2353620",
    "end": "2363099"
  },
  {
    "text": "then I'm getting the fourth\ntoken coming into the input, and I'm attending to the\nthird, second, and first,",
    "start": "2363100",
    "end": "2369010"
  },
  {
    "text": "and I'm trying to figure\nout what is the next token. Well then, in this batch,\nin the next element",
    "start": "2369010",
    "end": "2374589"
  },
  {
    "text": "over in the time dimension,\nthe answer is at the input. So I can't get any\ninformation from there.",
    "start": "2374590",
    "end": "2380360"
  },
  {
    "text": "So that's why this\nis all tricky, but basically, in\nthe forward pass, we are calculating the queries,\nkeys, and values based on x.",
    "start": "2380360",
    "end": "2390658"
  },
  {
    "text": "So these are the keys,\nqueries, and values. Here, when I'm\ncomputing the attention, I have the queries matrix\nmultiplying the piece.",
    "start": "2390658",
    "end": "2398020"
  },
  {
    "text": "So this is the dot product in\nparallel for all the queries and all the keys\nin all the heads.",
    "start": "2398020",
    "end": "2403400"
  },
  {
    "text": "So I failed to mention\nthat there's also the aspect of the heads, which\nis also done all in parallel",
    "start": "2403400",
    "end": "2408680"
  },
  {
    "text": "here. So we have the batch\ndimension, the time dimension, and the head dimension,\nand you end up with five-dimensional tensors,\nand it's all really confusing.",
    "start": "2408680",
    "end": "2414780"
  },
  {
    "text": "So I invite you to step through\nit later and convince yourself that this is actually\ndoing the right thing. But basically, you have the\nbatch dimension, the head",
    "start": "2414780",
    "end": "2421550"
  },
  {
    "text": "dimension and the\ntime dimension, and then you have\nfeatures at them. And so this is evaluating for\nall the batch elements, for all",
    "start": "2421550",
    "end": "2428630"
  },
  {
    "text": "the head elements, and\nall the time elements, the simple Python that I gave\nyou earlier, which is query",
    "start": "2428630",
    "end": "2434030"
  },
  {
    "text": "dot product p. Then here, we do a masked_fill,\nand what this is doing is it's basically clamping the\nattention between the nodes",
    "start": "2434030",
    "end": "2444260"
  },
  {
    "text": "that are not supposed to\ncommunicate to be negative infinity. And we're doing\nnegative infinity because we're about to softmax,\nand so negative infinity will",
    "start": "2444260",
    "end": "2451220"
  },
  {
    "text": "make basically the attention\nthat those elements be zero. And so here we are going\nto basically end up",
    "start": "2451220",
    "end": "2456590"
  },
  {
    "text": "with the weights, the affinities\nbetween these nodes, optional",
    "start": "2456590",
    "end": "2463370"
  },
  {
    "text": "dropout. And then here, attention\nmatrix multiply v is basically",
    "start": "2463370",
    "end": "2468460"
  },
  {
    "text": "the gathering of the information\naccording to the affinities we calculated. And this is just a\nweighted sum of the values",
    "start": "2468460",
    "end": "2474530"
  },
  {
    "text": "at all those nodes. So this matrix multiplies\nis doing that weighted sum. And then transpose\ncontiguous view",
    "start": "2474530",
    "end": "2480993"
  },
  {
    "text": "because it's all\ncomplicated and batched in five-dimensional\ntensors, but it's really not doing anything,\noptional drop out,",
    "start": "2480993",
    "end": "2486890"
  },
  {
    "text": "and then a linear projection\nback to the residual pathway. So this is implementing the\ncommunication phase here.",
    "start": "2486890",
    "end": "2494710"
  },
  {
    "text": "Then you can train\nthis transformer. And then you can generate\ninfinite Shakespeare.",
    "start": "2494710",
    "end": "2501170"
  },
  {
    "text": "And you will simply do this by-- because our block size is 8,\nwe start with a sum token,",
    "start": "2501170",
    "end": "2507170"
  },
  {
    "text": "say like, I used\nin this case, you can use something like a\nnew line as the start token.",
    "start": "2507170",
    "end": "2513050"
  },
  {
    "text": "And then you communicate\nonly to yourself because there's a\nsingle node, and you get the probability\ndistribution for the first word",
    "start": "2513050",
    "end": "2519560"
  },
  {
    "text": "in the sequence. And then you decode it\nfor the first character in the sequence. You decode the character.",
    "start": "2519560",
    "end": "2525560"
  },
  {
    "text": "And then you bring\nback the character, and you re-encode\nit as an integer. And now, you have\nthe second thing.",
    "start": "2525560",
    "end": "2530605"
  },
  {
    "text": "And so you get-- OK, we're at the first\nposition, and this is whatever integer it is,\nadd the positional encodings,",
    "start": "2530605",
    "end": "2537660"
  },
  {
    "text": "goes into the sequence,\ngoes in the transformer, and again, this token\nnow communicates with the first token\nand it's identity.",
    "start": "2537660",
    "end": "2546690"
  },
  {
    "text": "And so you just keep\nplugging it back. And once you run out of the\nblock size, which is eight, you start to crawl,\nbecause you can never",
    "start": "2546690",
    "end": "2553130"
  },
  {
    "text": "have watt size more than\neight in the way you've trained this transformer. So we have more and more\ncontext until eight. And then if you want to\ngenerate beyond eight,",
    "start": "2553130",
    "end": "2559190"
  },
  {
    "text": "you have to start cropping\nbecause the transformer only works for eight elements\nin time dimension. And so all of these transformers\nin the [INAUDIBLE] setting",
    "start": "2559190",
    "end": "2567170"
  },
  {
    "text": "have a finite block\nsize or context length, and in typical models, this\nwill be 1,024 tokens or 2,048",
    "start": "2567170",
    "end": "2574460"
  },
  {
    "text": "tokens, something like that. But these tokens are\nusually like BPE tokens, or SentencePiece tokens,\nor WorkPiece tokens.",
    "start": "2574460",
    "end": "2580435"
  },
  {
    "text": "There's many\ndifferent encodings. So it's not like that long. And so that's why, I\nthink, [INAUDIBLE].. We really want to\nexpand the context size,",
    "start": "2580435",
    "end": "2586790"
  },
  {
    "text": "and it gets gnarly\nbecause the attention is sporadic in the\n[INAUDIBLE] case. Now, if you want to implement\nan encoder instead of a decoder",
    "start": "2586790",
    "end": "2596760"
  },
  {
    "text": "attention. Then all you have to\ndo is this [INAUDIBLE] and you just delete that line.",
    "start": "2596760",
    "end": "2603340"
  },
  {
    "text": "So if you don't\nmask the attention, then all the nodes\ncommunicate to each other, and everything is\nallowed, and information",
    "start": "2603340",
    "end": "2609390"
  },
  {
    "text": "flows between all the nodes. So if you want to have the\nencoder here, just delete.",
    "start": "2609390",
    "end": "2615750"
  },
  {
    "text": "All the encoder blocks\nwill use attention where this line is deleted. That's it. So you're allowing whatever--\nthis encoder might store say,",
    "start": "2615750",
    "end": "2624480"
  },
  {
    "text": "10 tokens, 10 nodes,\nand they are all allowed to communicate to each\nother going up the transformer.",
    "start": "2624480",
    "end": "2631240"
  },
  {
    "text": "And then if you want to\nimplement cross-attention, so you have a full\nencoder-decoder transformer, not just a decoder-only\ntransformer or a GPT.",
    "start": "2631240",
    "end": "2639329"
  },
  {
    "text": "Then we need to also add\ncross-attention in the middle. So here, there is a\nself-attention piece where all",
    "start": "2639330",
    "end": "2645809"
  },
  {
    "text": "the-- there's a self-attention\npiece, a cross-attention piece, and this MLP. And in the\ncross-attention, we need",
    "start": "2645810",
    "end": "2652320"
  },
  {
    "text": "to take the features from\nthe top of the encoder. We need to add one\nmore line here, and this would be the\ncross-attention instead of a--",
    "start": "2652320",
    "end": "2660090"
  },
  {
    "text": "I should have implemented\nit instead of just pointing, I think. But there will be a\ncross-attention line here.",
    "start": "2660090",
    "end": "2665310"
  },
  {
    "text": "So we'll have three\nlines because we need to add another block. And the queries will\ncome from x but the keys",
    "start": "2665310",
    "end": "2671400"
  },
  {
    "text": "and the values will come\nfrom the top of the encoder. And there will be\nbasic code information",
    "start": "2671400",
    "end": "2676710"
  },
  {
    "text": "flowing from the\nencoder, strictly to all the nodes inside x. And then that's it.",
    "start": "2676710",
    "end": "2682750"
  },
  {
    "text": "So it's a very\nsimple modifications on the decoder attention. So you'll hear people\ntalk that you have",
    "start": "2682750",
    "end": "2689470"
  },
  {
    "text": "a decoder-only model like GPT. You can have an encoder-only\nmodel like BERT, or you can have an\nencoder-decoder model",
    "start": "2689470",
    "end": "2695427"
  },
  {
    "text": "like say T5, doing things\nlike machine translation. And in BERT, you can't train\nit using this language modeling",
    "start": "2695427",
    "end": "2704142"
  },
  {
    "text": "setup that's utter\naggressive, and you're just trying to predict next\n[INAUDIBLE] in the sequence. You're training it doing\nslightly different objectives.",
    "start": "2704143",
    "end": "2709720"
  },
  {
    "text": "You're putting in\nthe full sentence, and, the full sentence is\nallowed to communicate fully. And then you're trying to\nclassify sentiment or something",
    "start": "2709720",
    "end": "2716830"
  },
  {
    "text": "like that. So you're not trying to model\nthe next token in the sequence. So these are trained\nslightly different",
    "start": "2716830",
    "end": "2726650"
  },
  {
    "text": "using masking and other\ndenoising techniques.",
    "start": "2726650",
    "end": "2731789"
  },
  {
    "text": "OK. So that's like the transformer. I'm going to continue. So yeah, maybe more questions.",
    "start": "2731790",
    "end": "2738566"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2738566",
    "end": "2749349"
  },
  {
    "text": "This is like we are enforcing\nthese constraints on it by just masking [INAUDIBLE]",
    "start": "2761710",
    "end": "2772610"
  },
  {
    "text": "So I'm not sure\nif I fully follow. So there's different ways\nto look at this analogy, but one analogy is\nyou can interpret",
    "start": "2772610",
    "end": "2778330"
  },
  {
    "text": "this graph as really fixed. It's just that every time\nwe do the communicate, we are using different weights.",
    "start": "2778330",
    "end": "2783400"
  },
  {
    "text": "You can look at it that way. So if we have block size\nof eight in my example, we would have eight nodes. Here we have 2, 4, 6.",
    "start": "2783400",
    "end": "2789310"
  },
  {
    "text": "OK, so we'd have eight nodes. They would be connected in-- you lay them out, and you only\nconnect from left to right.",
    "start": "2789310",
    "end": "2795460"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2795460",
    "end": "2797859"
  },
  {
    "text": "Why would they\nconnect-- usually, the connections don't change\nas a function of the data or something like that-- [INAUDIBLE]",
    "start": "2802635",
    "end": "2811990"
  },
  {
    "text": "I don't think I've seen\na single example where the connectivity\nchanges dynamically in the function data. Usually, the\nconnectivity is fixed.",
    "start": "2820293",
    "end": "2825480"
  },
  {
    "text": "If you have an encoder,\nand you're training a BERT, you have how many\ntokens you want, and they are fully connected.",
    "start": "2825480",
    "end": "2831640"
  },
  {
    "text": "And if you have a\ndecoder-only model, you have this triangular\nthing, and if you have encoder-decoder,\nthen you have",
    "start": "2831640",
    "end": "2836748"
  },
  {
    "text": "awkwardly two pools of nodes. Yeah.",
    "start": "2836748",
    "end": "2841770"
  },
  {
    "text": "Go ahead. [INAUDIBLE] I wonder, you\nknow much more about this",
    "start": "2844640",
    "end": "2865010"
  },
  {
    "text": "than I know. But do you have a sense of\nlike if you ran [INAUDIBLE]",
    "start": "2865010",
    "end": "2880630"
  },
  {
    "text": "In my head, I'm thinking\n[INAUDIBLE] but then you also",
    "start": "2880630",
    "end": "2888555"
  },
  {
    "text": "have different things for\none or more of [INAUDIBLE]---- Yeah, it's really\nhard to say, so that's",
    "start": "2888555",
    "end": "2895000"
  },
  {
    "text": "why I think this paper is so\ninteresting because like, yeah, usually, you'd\nsee like the path, and maybe they had\npath internally. They just didn't publish it.",
    "start": "2895000",
    "end": "2900982"
  },
  {
    "text": "All you can see is things that\ndidn't look like a transformer. I mean, you have ResNets,\nwhich have lots of this.",
    "start": "2900982",
    "end": "2906250"
  },
  {
    "text": "But a ResNet would be\nlike this, but there's no self-attention component. But the MLP is there\nkind of in a ResNet.",
    "start": "2906250",
    "end": "2915579"
  },
  {
    "text": "So a ResNet looks\nvery much like this except there's no-- you can\nuse layer norms in ResNets, I believe, as well.",
    "start": "2915580",
    "end": "2921220"
  },
  {
    "text": "Typically, sometimes,\nthey can be batch norms. So it is kind of like a ResNet. It is like they took\na ResNet, and they",
    "start": "2921220",
    "end": "2927190"
  },
  {
    "text": "put in a self-attention\nblock in addition to the preexisting\nMLP block, which is kind of like convolutions.",
    "start": "2927190",
    "end": "2933742"
  },
  {
    "text": "And MLP was strictly\nspeaking deconvolution, one by one convolution,\nbut I think",
    "start": "2933742",
    "end": "2939099"
  },
  {
    "text": "the idea is similar in that MLP\nis just like a typical weights,",
    "start": "2939100",
    "end": "2944110"
  },
  {
    "text": "nonlinearity weights operation.",
    "start": "2944110",
    "end": "2946210"
  },
  {
    "text": "But I will say, yeah, this\nis kind of interesting because a lot of\nwork is not there, and then they give\nyou this transformer.",
    "start": "2951048",
    "end": "2957635"
  },
  {
    "text": "And then it turns\nout 5 years later, it's not changed, even though\neveryone's trying to change it. So it's interesting to me\nthat it's like a package,",
    "start": "2957635",
    "end": "2963095"
  },
  {
    "text": "in like a package,\nwhich I think is really interesting historically. And I also talked\nto paper authors,",
    "start": "2963095",
    "end": "2970100"
  },
  {
    "text": "and they were\nunaware of the impact that the transformer\nwould have at the time. So when you read this paper,\nactually, it's unfortunate",
    "start": "2970100",
    "end": "2977420"
  },
  {
    "text": "because this is the paper\nthat changed everything, but when people read it,\nit's like question marks because it reads like a pretty\nrandom machine translation",
    "start": "2977420",
    "end": "2985100"
  },
  {
    "text": "paper. It's like, oh, we're\ndoing machine translation. Oh, here's a cool architecture. OK, great, good results.",
    "start": "2985100",
    "end": "2991265"
  },
  {
    "text": "It doesn't know what's\ngoing to happen. [LAUGHS] And so when\npeople read it today, I think they're\nconfused potentially.",
    "start": "2991265",
    "end": "3000550"
  },
  {
    "text": "I will have some\ntweets at the end, but I think I would\nhave renamed it with the benefit of hindsight\nof like, well, I'll get to it.",
    "start": "3000550",
    "end": "3008755"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "3008755",
    "end": "3015111"
  },
  {
    "text": "Yeah, I think that's a\ngood question as well. Currently, I mean,\nI certainly don't love the autoregressive\nmodeling approach.",
    "start": "3020920",
    "end": "3027330"
  },
  {
    "text": "I think it's kind of\nweird to sample a token and then commit to it. So maybe there are\nsome ways, some hybrids",
    "start": "3027330",
    "end": "3036810"
  },
  {
    "text": "with the Fusion as\nan example, which I think would be\nreally cool, or we'll find some other ways to edit\nthe sequences later but still",
    "start": "3036810",
    "end": "3044319"
  },
  {
    "text": "in our regressive framework. But I think the Fusion is\nlike an up and coming modeling",
    "start": "3044320",
    "end": "3049510"
  },
  {
    "text": "approach that I personally\nfind much more appealing. When I sample text, I don't\ngo chunk, chunk, chunk, and commit.",
    "start": "3049510",
    "end": "3055365"
  },
  {
    "text": "I do a draft one, and then\nI do a better draft two. And that feels like\na diffusion process.",
    "start": "3055365",
    "end": "3060880"
  },
  {
    "text": "So that would be my hope. OK, also a question.",
    "start": "3060880",
    "end": "3067760"
  },
  {
    "text": "So yeah, you'd think\nthe [INAUDIBLE]",
    "start": "3067760",
    "end": "3080338"
  },
  {
    "text": "And then once we\nhave the edge rates, we just have to multiply\nit by the values, and then you just\n[INAUDIBLE] it. Yes, yeah, it's right.",
    "start": "3080338",
    "end": "3087160"
  },
  {
    "text": "And you think there's MLG\nwithin graph neural networks and they'll potentially--",
    "start": "3087160",
    "end": "3092590"
  },
  {
    "text": "I find the graph neural\nnetworks like a confusing term because, I mean,\nyeah, previously,",
    "start": "3092590",
    "end": "3098210"
  },
  {
    "text": "there, was this notion of-- I feel like maybe today\neverything is a graph neural network because a transformer\nis a graph neural network",
    "start": "3098210",
    "end": "3104800"
  },
  {
    "text": "processor. The native representation that\nthe transformer operates over is sets that are connected\nby edges in a direct way.",
    "start": "3104800",
    "end": "3111680"
  },
  {
    "text": "And so that's the native\nrepresentation, and then, yeah. OK, I should go on because\nI still have 30 slides.",
    "start": "3111680",
    "end": "3117720"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "3117720",
    "end": "3119540"
  },
  {
    "text": "Oh yeah, yeah, the root\nDE, I think, it basically like if you're initializing\nwith random weights",
    "start": "3128100",
    "end": "3134130"
  },
  {
    "text": "setup from a [INAUDIBLE] as\nyour dimension size grows, so does your values,\nthe variance grows.",
    "start": "3134130",
    "end": "3139349"
  },
  {
    "text": "And then your softmax will just\nbecome the one half vector. So it's just a way to\ncontrol the variance",
    "start": "3139350",
    "end": "3145410"
  },
  {
    "text": "and bring it to always be\nin a good range for softmax and nice diffused distribution.",
    "start": "3145410",
    "end": "3151670"
  },
  {
    "text": "OK, so it's almost like\nan initialization thing.",
    "start": "3151670",
    "end": "3157869"
  },
  {
    "text": "OK, so transformers\nhave been applied to all the other fields,\nand the way this was done",
    "start": "3157870",
    "end": "3164320"
  },
  {
    "text": "is in my opinion,\nridiculous ways honestly because I was a\ncomputer vision person,",
    "start": "3164320",
    "end": "3169390"
  },
  {
    "text": "and you have ComNets,\nand they make sense. So what we're doing now\nwith VITs as an example is you take an image and you chop\nit up into little squares.",
    "start": "3169390",
    "end": "3176215"
  },
  {
    "text": "And then those\nsquares, literally, feed into a\ntransformer, and that's it, which is kind of ridiculous.",
    "start": "3176215",
    "end": "3181900"
  },
  {
    "text": "And so, I mean, yeah,\nand so the transformer doesn't even, in the simplest\ncase, really know where",
    "start": "3181900",
    "end": "3188670"
  },
  {
    "text": "these patches might come from. They are usually\npositionally encoded, but it has to rediscover\na lot of the structure,",
    "start": "3188670",
    "end": "3196380"
  },
  {
    "text": "I think, of them in some ways. And it's kind of weird\nto approach it that way.",
    "start": "3196380",
    "end": "3203089"
  },
  {
    "text": "But it's just the\nsimplest baseline of just chomping up big\nimages into small squares and feeding them in as the\nindividual nodes actually",
    "start": "3203090",
    "end": "3209840"
  },
  {
    "text": "works fairly well. And then this is in a\ntransformer encoder, so all the patches are\ntalking to each other throughout the\nentire transformer.",
    "start": "3209840",
    "end": "3216960"
  },
  {
    "text": "And the number of nodes\nhere would be like nine.",
    "start": "3216960",
    "end": "3219494"
  },
  {
    "text": "Also, in speech recognition, you\njust take your melSpectrogram, and you chop it up into\nslices and you feed them into a transformer.",
    "start": "3222285",
    "end": "3227730"
  },
  {
    "text": "So there was paper like\nthis, but also Whisper. Whisper is a\ncopy-paste transformer. If you saw Whisper from OpenAI,\nyou just chop up melSpectrogram",
    "start": "3227730",
    "end": "3235200"
  },
  {
    "text": "and feed it into a\ntransformer and then pretend you're dealing with text. And it works very well.",
    "start": "3235200",
    "end": "3240870"
  },
  {
    "text": "Decision transformer in RL,\nyou take your states, actions, and reward that you\nexperience in environment, and you just pretend\nit's a language.",
    "start": "3240870",
    "end": "3247693"
  },
  {
    "text": "Then you start to model\nthe sequences of that, and then you can use\nthat for planning later. That works really well.",
    "start": "3247693",
    "end": "3253319"
  },
  {
    "text": "Even things AlphaFold,\nso we were briefly talking about molecules and\nhow you can plug them in. So at the heart of\nAlphaFold, computationally,",
    "start": "3253320",
    "end": "3259507"
  },
  {
    "text": "is also a transformer. One thing I wanted to also\nsay about transformers is I find that\nthey're very flexible,",
    "start": "3259507",
    "end": "3266290"
  },
  {
    "text": "and I really enjoy that. I'll give you an\nexample from Tesla. You have a ComNet\nthat takes an image",
    "start": "3266290",
    "end": "3272770"
  },
  {
    "text": "and makes predictions\nabout the image. And then the big\nquestion is, how do you feed in extra information? And it's not always\ntrivial like say, I",
    "start": "3272770",
    "end": "3278920"
  },
  {
    "text": "had additional\ninformation that I want to inform that I want\nthe outputs to be informed by. Maybe I have other\nsensors like Radar.",
    "start": "3278920",
    "end": "3285112"
  },
  {
    "text": "Maybe I have some map\ninformation, or a vehicle type, or some audio. And the question is, how do you\nfeed information into a ComNet?",
    "start": "3285112",
    "end": "3290710"
  },
  {
    "text": "Like where do you feed it in? Do you concatenate it? Do you add it? At what stage?",
    "start": "3290710",
    "end": "3296349"
  },
  {
    "text": "And so with a transformer,\nit's much easier because you just take\nwhatever you want, you chop it up into pieces, and you\nfeed it in with a set",
    "start": "3296350",
    "end": "3302500"
  },
  {
    "text": "of what you had before. And you let the\nself-attention figure out how everything\nshould communicate. And that actually\napparently works.",
    "start": "3302500",
    "end": "3307720"
  },
  {
    "text": "So just chop up everything\nand throw it into the mix is like the way. And it frees neural\nnets from this burgeon",
    "start": "3307720",
    "end": "3315760"
  },
  {
    "text": "of Euclidean space,\nwhere previously you had to arrange your computation\nto conform to the Euclidean",
    "start": "3315760",
    "end": "3321790"
  },
  {
    "text": "space or three dimensions of how\nyou're laying out the compute. Like the compute\nactually kind of happens in almost like 3D\nspace if you think about it.",
    "start": "3321790",
    "end": "3329859"
  },
  {
    "text": "But in attention,\neverything is just sets. So it's a very\nflexible framework, and you can just throw in stuff\ninto your conditioning set.",
    "start": "3329860",
    "end": "3335530"
  },
  {
    "text": "And everything just\nself-attended over. So it's quite beautiful\nfrom that perspective. OK, so now what exactly makes\ntransformers so effective?",
    "start": "3335530",
    "end": "3343220"
  },
  {
    "text": "I think a good\nexample of this comes from the GPT-3 paper, which\nI encourage people to read.",
    "start": "3343220",
    "end": "3348230"
  },
  {
    "text": "Language Models of\nFew-Shot Learners. I would have probably\nrenamed this a little bit. I would have said\nsomething like transformers",
    "start": "3348230",
    "end": "3354380"
  },
  {
    "text": "are capable of in-context\nlearning or meta-learning. That's like what makes\nthem really special.",
    "start": "3354380",
    "end": "3360097"
  },
  {
    "text": "So basically the setting\nthat they're working with is, OK, I have some\ncontext, and I'm trying-- like say, a passage. This is just one\nexample of many.",
    "start": "3360097",
    "end": "3366335"
  },
  {
    "text": "I have a passage, and I'm\nasking questions about it. And then as part of the\ncontext in the prompt,",
    "start": "3366335",
    "end": "3372762"
  },
  {
    "text": "I'm giving the questions\nand the answers. So I'm giving one example\nof question-answer, another example of\nquestion-answer, another example of\nquestion-answer, and so on.",
    "start": "3372762",
    "end": "3379890"
  },
  {
    "text": "And this becomes-- Oh yeah, people are going\nto have to leave soon, huh? OK, is this really important?",
    "start": "3379890",
    "end": "3385635"
  },
  {
    "text": "Let me think. OK, so what's really\ninteresting is basically",
    "start": "3385635",
    "end": "3391330"
  },
  {
    "text": "like with more examples\ngiven in a context, the accuracy improves.",
    "start": "3391330",
    "end": "3397200"
  },
  {
    "text": "And so what that can set\nis that the transformer is able to somehow\nlearn in the activations without doing any\ngradient descent",
    "start": "3397200",
    "end": "3403630"
  },
  {
    "text": "in a typical\nfine-tuning fashion. So if you fine-tune, you have to\ngive an example and the answer, and you fine-tune it,\nusing gradient descent.",
    "start": "3403630",
    "end": "3411247"
  },
  {
    "text": "But it looks like the\ntransformer internally in its weights is\ndoing something that looks like potentially\ngradient, some kind of a metalearning in the\nweights of the transformer",
    "start": "3411247",
    "end": "3417430"
  },
  {
    "text": "as it is reading the prompt. And so in this paper,\nthey go into, OK, distinguishing this outer\nloop with stochastic gradient",
    "start": "3417430",
    "end": "3423970"
  },
  {
    "text": "descent in this inner loop\nof the intercontext learning. So the inner loop is as\nthe transformer is reading the sequence almost and the\nouter loop is the training",
    "start": "3423970",
    "end": "3432339"
  },
  {
    "text": "by gradient descent. So basically,\nthere's some training happening in the activations\nof the transformer as it is consuming\na sequence that",
    "start": "3432340",
    "end": "3438730"
  },
  {
    "text": "may be very much looks\nlike gradient descent. And so there are some recent\npapers that hint at this and study it.",
    "start": "3438730",
    "end": "3443930"
  },
  {
    "text": "And so as an example,\nin this paper here, they propose something\ncalled the draw operator. And they argue that the\nraw operator is implemented",
    "start": "3443930",
    "end": "3452073"
  },
  {
    "text": "by transformer,\nand then they show that you can implement\nthings like ridge regression on top of the raw operator. And so this is giving--",
    "start": "3452073",
    "end": "3459012"
  },
  {
    "text": "There are papers\nhinting that maybe there is some thing that looks\nlike gradient-based learning inside the activations\nof the transformer.",
    "start": "3459012",
    "end": "3465250"
  },
  {
    "text": "And I think this is not\nimpossible to think through because what is\ngradient-based learning? Overpass, backward\npass, and then update.",
    "start": "3465250",
    "end": "3472180"
  },
  {
    "text": "Oh, that looks like\na ResNet, right, because you're adding\nto the weights. So the start of initial\nrandom set of weights,",
    "start": "3472180",
    "end": "3479512"
  },
  {
    "text": "forward pass, backward pass,\nand update your weights, and then forward pass, backward\npass, update the weights. Looks like a ResNet.",
    "start": "3479512",
    "end": "3484930"
  },
  {
    "text": "Transformer is a ResNet,\nso much more hand-wavey,",
    "start": "3484930",
    "end": "3490180"
  },
  {
    "text": "but basically, some\npapers are trying to hint at why that would\nbe potentially possible. And then I have a bunch of\ntweets I just copy-pasted here",
    "start": "3490180",
    "end": "3496900"
  },
  {
    "text": "in the end. This was like meant for\ngeneral consumption, so they're a bit more high-level\nand hypey a little bit.",
    "start": "3496900",
    "end": "3502900"
  },
  {
    "text": "But I'm talking about why this\narchitecture is so interesting and why potentially\nit became so popular.",
    "start": "3502900",
    "end": "3507995"
  },
  {
    "text": "And I think it\nsimultaneously optimizes three properties that, I\nthink, are very desirable. Number one, the\ntransformer is very",
    "start": "3507995",
    "end": "3513130"
  },
  {
    "text": "expressive in the forward pass. It sort of like it's\nable to implement very interesting functions,\npotentially functions",
    "start": "3513130",
    "end": "3519552"
  },
  {
    "text": "that can even do meta-learning. Number two, it is very\noptimizable thanks to things like residual\nconnections, layer nodes,",
    "start": "3519552",
    "end": "3525430"
  },
  {
    "text": "and so on. And number three, it's\nextremely efficient. This is not always appreciated,\nbut the transformer, if you look at the\ncomputational graph,",
    "start": "3525430",
    "end": "3531555"
  },
  {
    "text": "is a shallow, wide\nnetwork, which is perfect to take advantage\nof the parallelism of GPUs. So I think the transformer\nwas designed very deliberately",
    "start": "3531555",
    "end": "3538600"
  },
  {
    "text": "to run efficiently on GPUs. There's previous\nwork like neural GPU that I really enjoy as\nwell, which is really just",
    "start": "3538600",
    "end": "3545680"
  },
  {
    "text": "like how do we design neural\nnets that are efficient on GPUs and thinking backwards from the\nconstraints of the hardware, which I think is a\nvery interesting way",
    "start": "3545680",
    "end": "3551740"
  },
  {
    "text": "to think about it.",
    "start": "3551740",
    "end": "3552490"
  },
  {
    "text": "Oh yeah, so here, I'm saying,\nI probably would have called-- I probably would've called the\ntransformer a general purpose",
    "start": "3557930",
    "end": "3564490"
  },
  {
    "text": "efficient optimizable\ncomputer instead of attention is all you need. That's what I would have maybe\nin hindsight called that paper.",
    "start": "3564490",
    "end": "3571930"
  },
  {
    "text": "It's proposing a model that\nis very general purpose, so",
    "start": "3571930",
    "end": "3577349"
  },
  {
    "text": "forward passes, expressive. It's very efficient\nin terms of GPU usage and is easily optimizable by\ngradient descent and trains",
    "start": "3577350",
    "end": "3584720"
  },
  {
    "text": "very nicely. And then I have some\nother hype tweets here.",
    "start": "3584720",
    "end": "3588730"
  },
  {
    "text": "Anyway, so you can\nread them later. But I think this one\nis maybe interesting. So if previous neural nets\nare special purpose computers",
    "start": "3591490",
    "end": "3598360"
  },
  {
    "text": "designed for a\nspecific task, GPT is a general purpose computer,\nreconfigurable at runtime",
    "start": "3598360",
    "end": "3603790"
  },
  {
    "text": "to run natural\nlanguage programs. So the programs are\ngiven as prompts,",
    "start": "3603790",
    "end": "3608920"
  },
  {
    "text": "and then GPT runs the program\nby completing the document. So I really like these analogies\npersonally to computer.",
    "start": "3608920",
    "end": "3616960"
  },
  {
    "text": "It's just like a\npowerful computer, and it's optimizable\nby gradient descent.",
    "start": "3616960",
    "end": "3622200"
  },
  {
    "text": "And I don't know--",
    "start": "3622200",
    "end": "3630615"
  },
  {
    "start": "3630000",
    "end": "4301000"
  },
  {
    "text": "OK, yeah. That's it. [LAUGHTER] You can read the tweets\nlater, but that's for now. I'll just thank you.",
    "start": "3630615",
    "end": "3636050"
  },
  {
    "text": "I'll just leave this up.",
    "start": "3636050",
    "end": "3637050"
  },
  {
    "text": "Sorry, I just found this tweet. So turns out that if you\nscale up the training set and use a powerful enough\nneural net like a transformer,",
    "start": "3645368",
    "end": "3651940"
  },
  {
    "text": "the network becomes a\nkind of general purpose computer over text. So I think that's nice\nway to look at it. And instead of performing\na single text sequence,",
    "start": "3651940",
    "end": "3658570"
  },
  {
    "text": "you can design the\nsequence in the prompt. And because the transformer\nis both powerful but also is trained on large\nenough, very hard data set,",
    "start": "3658570",
    "end": "3665109"
  },
  {
    "text": "it becomes this general\npurpose text computer. And so I think that's kind of\ninteresting way to look at it.",
    "start": "3665110",
    "end": "3671200"
  },
  {
    "text": "Yeah. [INAUDIBLE]",
    "start": "3671200",
    "end": "3676750"
  },
  {
    "text": "And I guess my question\nis [INAUDIBLE] how much do you think [INAUDIBLE]?",
    "start": "3721290",
    "end": "3725598"
  },
  {
    "text": "really because it's mostly\nmore efficient or [INAUDIBLE]",
    "start": "3730020",
    "end": "3745795"
  },
  {
    "text": "So I think there's\na bit of that. Yeah, so I would say\nRNNs in principle, yes, they can implement\narbitrary programs.",
    "start": "3745795",
    "end": "3751457"
  },
  {
    "text": "I think, it's like a useless\nstatement to some extent because they're probably-- I'm not sure that they're\nprobably expressive",
    "start": "3751457",
    "end": "3757670"
  },
  {
    "text": "because in a sense of power\nand that they can implement these arbitrary functions.",
    "start": "3757670",
    "end": "3763070"
  },
  {
    "text": "But they're not optimizable. And they're certainly not\nefficient because they are serial computing devices.",
    "start": "3763070",
    "end": "3767750"
  },
  {
    "text": "So if you look at it\nas a compute graph, RNNs are very long,\nthin compute graph.",
    "start": "3770163",
    "end": "3778265"
  },
  {
    "text": "What if you stretched out\nthe neurons and you looked-- like take all the individual\nneurons interconnectivity, and stretch them out, and\ntry to visualize them.",
    "start": "3778265",
    "end": "3784460"
  },
  {
    "text": "RNNs would be like a very\nlong graph and that's bad. And it's bad also\nfor optimizability because I don't\nexactly know why,",
    "start": "3784460",
    "end": "3790980"
  },
  {
    "text": "but just the rough intuition\nis when you're backpropagating, you don't want to\nmake too many steps. And so transformers are a\nshallow wide graph, and so",
    "start": "3790980",
    "end": "3799385"
  },
  {
    "text": "from supervision to inputs is\na very small number of hops. And it's a long\nresidual pathways,",
    "start": "3799385",
    "end": "3805400"
  },
  {
    "text": "which make gradients\nflow very easily. And there's all\nthese layer norms to control the scales of\nall of those activations.",
    "start": "3805400",
    "end": "3812510"
  },
  {
    "text": "And so there's\nnot too many hops, and you're going from\nsupervision to input very quickly and just\nflows through the graph.",
    "start": "3812510",
    "end": "3820840"
  },
  {
    "text": "And it can all be\ndone in parallel, so you don't need to do this-- encoder and decoder RNNs, you\nhave to go from first word,",
    "start": "3820840",
    "end": "3826030"
  },
  {
    "text": "then second word,\nthen third word. But here in transformer,\nevery single word was processed completely in\nparallel, which is kind of a--",
    "start": "3826030",
    "end": "3834700"
  },
  {
    "text": "So I think all of these are\nreally important because all of these are really important. And I think number 3 is less\ntalked about but extremely",
    "start": "3834700",
    "end": "3840400"
  },
  {
    "text": "important because in deep\nlearning scale matters. And so the size of the\nnetwork that you can train it",
    "start": "3840400",
    "end": "3846099"
  },
  {
    "text": "gives you is\nextremely important. And so if it's efficient\non the current hardware, then you can make it bigger.",
    "start": "3846100",
    "end": "3851747"
  },
  {
    "text": "You mentioned that if you do\nit with multiple modalities of data, [INAUDIBLE].",
    "start": "3854945",
    "end": "3859740"
  },
  {
    "text": "How does that actually work? Do you leave the different\ndata as different token, or is it [INAUDIBLE]?",
    "start": "3861723",
    "end": "3869220"
  },
  {
    "text": "No, so yeah, so you\ntake your image, and you apparently chop\nthem up into patches. So there's the first\nthousand tokens or whatever.",
    "start": "3869220",
    "end": "3875369"
  },
  {
    "text": "And now, I have a special-- so radar could be also,\nbut I don't actually",
    "start": "3875370",
    "end": "3880934"
  },
  {
    "text": "want to make a\nrepresentation of radar. But you just need to\nchop it up and enter it.",
    "start": "3880935",
    "end": "3886075"
  },
  {
    "text": "And then you have to\nencode it somehow. Like the transformer\nneeds to know that they're coming from radar. So you create a special--",
    "start": "3886075",
    "end": "3892289"
  },
  {
    "text": "you have some kind of a\nspecial token of that to-- these radar tokens\nare what's slightly different in the\nrepresentation, and it's",
    "start": "3892290",
    "end": "3898760"
  },
  {
    "text": "learnable by gradient descent. And like vehicle\ninformation would also come in with a special embedded\ntoken that can be learned.",
    "start": "3898760",
    "end": "3907920"
  },
  {
    "text": "So-- So how do you line\nthose before really-- Actually, but you don't. It's all just a set.",
    "start": "3907920",
    "end": "3913938"
  },
  {
    "text": "And there's-- Even the [INAUDIBLE] Yeah, it's all just a set,\nbut you can positionally",
    "start": "3913938",
    "end": "3920870"
  },
  {
    "text": "encode these sets if you want. So positional\nencoding means you can",
    "start": "3920870",
    "end": "3926150"
  },
  {
    "text": "hardwire, for example,\nthe coordinates like using [INAUDIBLE]. You can hardwire\nthat, but it's better",
    "start": "3926150",
    "end": "3931310"
  },
  {
    "text": "if you don't hardwire\nthe position. It's just a vector\nthat is always hanging out the dislocation. Whatever content is\nthere, it just adds on it.",
    "start": "3931310",
    "end": "3937910"
  },
  {
    "text": "And this vector is\ntrainable by background. That's how you do it.",
    "start": "3937910",
    "end": "3940165"
  },
  {
    "text": "Good point. I don't really like\nthe [INAUDIBLE]..",
    "start": "3943458",
    "end": "3945994"
  },
  {
    "text": "They seem to work, but it\nseems like they're sometimes [INAUDIBLE]",
    "start": "3948736",
    "end": "3968868"
  },
  {
    "text": "I'm not sure if I\nunderstand your question. [LAUGHTER] So I mean the\npositional encoders like they're actually like not--",
    "start": "3968868",
    "end": "3974619"
  },
  {
    "text": "OK, so they have very little\ninductive bias or something like that. They're just vectors hanging\nout in location always,",
    "start": "3974620",
    "end": "3979637"
  },
  {
    "text": "and you're trying to help\nthe network in some way. And I think the\nintuition is good,",
    "start": "3979637",
    "end": "3988710"
  },
  {
    "text": "but if you have\nenough data, usually, trying to mess with\nit is a bad thing. Trying to enter\nknowledge when you",
    "start": "3988710",
    "end": "3995200"
  },
  {
    "text": "have enough\nknowledge in the data set itself is not\nusually productive. So it all really depends\non what scale you want. If you have infinity\ndata, then you actually",
    "start": "3995200",
    "end": "4001950"
  },
  {
    "text": "want to encode less and less. That turns out to work better. And if you have very little\ndata, then actually, you do want to encode some biases.",
    "start": "4001950",
    "end": "4007230"
  },
  {
    "text": "And maybe if you have a\nmuch smaller data set, then maybe convolutions\nare a good idea because you actually have this\nbias coming from your filters.",
    "start": "4007230",
    "end": "4015270"
  },
  {
    "text": "But I think-- so the transformer\nis extremely general, but there are ways to\nmess with the encodings",
    "start": "4015270",
    "end": "4021230"
  },
  {
    "text": "to put in more structure. Like you could, for example,\nencode [INAUDIBLE] and fix it, or you could actually go\nto the attention mechanism",
    "start": "4021230",
    "end": "4027165"
  },
  {
    "text": "and say, OK, if my image\nis chopped up into patches, this patch can only communicate\nto this neighborhood.",
    "start": "4027165",
    "end": "4033040"
  },
  {
    "text": "And you just do that in\nthe attention matrix, you just mask out whatever\nyou don't want to communicate.",
    "start": "4033040",
    "end": "4038151"
  },
  {
    "text": "And so people really\nplay with this because the full\nattention is inefficient. So they will intersperse,\nfor example, layers",
    "start": "4038152",
    "end": "4045160"
  },
  {
    "text": "that only communicate\nin little patches and then layers that\ncommunicate globally. And they will do all\nkinds of tricks like that.",
    "start": "4045160",
    "end": "4050680"
  },
  {
    "text": "So you can slowly bring\nin more inductive bias. You would do it, but\nthe inductive biases are like they're factored out\nfrom the core transformer.",
    "start": "4050680",
    "end": "4058990"
  },
  {
    "text": "And they are factored out,\nand the interconnectivity of the nodes. And they are factored\nout in the positionally--",
    "start": "4058990",
    "end": "4064910"
  },
  {
    "text": "and you can mess with\nthis for computation. [INAUDIBLE]",
    "start": "4064910",
    "end": "4081067"
  },
  {
    "text": "So there's probably about 200\npapers on this now if not more. They're kind of hard\nto keep track of.",
    "start": "4082530",
    "end": "4087990"
  },
  {
    "text": "Honestly, like my Safari\nbrowser, which is-- oh, it's all up on my computer,\nlike 200 open tabs.",
    "start": "4087990",
    "end": "4093750"
  },
  {
    "text": "But yes, I'm not\neven sure if I want",
    "start": "4093750",
    "end": "4100580"
  },
  {
    "text": "to pick my favorite honestly. Yeah, [INAUDIBLE]",
    "start": "4100580",
    "end": "4109903"
  },
  {
    "text": "Maybe you can use a transformer\nlike that [INAUDIBLE] The other one that I\nactually like even more is potentially, keep\nthe context length fixed",
    "start": "4122600",
    "end": "4129290"
  },
  {
    "text": "but allow the network to\nsomehow use a scratch pad. And so the way this works is\nyou will teach the transformer",
    "start": "4129290",
    "end": "4135545"
  },
  {
    "text": "somehow via examples\nin [INAUDIBLE] hey, you actually have a scratch pad. Basically, you can't\nremember too much.",
    "start": "4135545",
    "end": "4141890"
  },
  {
    "text": "Your context line is finite. But you can use a scratch pad. And you do that by emitting\na start scratch pad, and then writing whatever you\nwant to remember, and then",
    "start": "4141890",
    "end": "4148759"
  },
  {
    "text": "end scratch pad. And then you continue\nwith whatever you want. And then later\nwhen it's decoding,",
    "start": "4148760",
    "end": "4154345"
  },
  {
    "text": "you actually have\nspecial objects that when you detect\nstart scratch pad, you will like save\nwhatever it puts",
    "start": "4154345",
    "end": "4159739"
  },
  {
    "text": "in there in like external thing\nand allow it to attend over it. So basically, you can teach the\ntransformer just dynamically",
    "start": "4159740",
    "end": "4165140"
  },
  {
    "text": "because it's so meta-learned. You can teach it dynamically\nto use other gizmos and gadgets and allow it to expand\nits memory that way",
    "start": "4165140",
    "end": "4171926"
  },
  {
    "text": "if that makes sense. It's just like human learning\nto use a notepad, right. You don't have to\nkeep it in your brain.",
    "start": "4171927",
    "end": "4177200"
  },
  {
    "text": "So keeping things in your\nbrain is like the context line from the transformer. But maybe we can just\ngive it a notebook. And then it can query the\nnotebook, and read from it,",
    "start": "4177200",
    "end": "4185149"
  },
  {
    "text": "and write to it. [INAUDIBLE] transformer to\nplug in another transformer. [LAUGHTER]",
    "start": "4185149",
    "end": "4190645"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "4193090",
    "end": "4198139"
  },
  {
    "text": "I don't know if I detected that. I feel like-- did you feel\nlike there was more than just a long prompt that's unfolding?",
    "start": "4209140",
    "end": "4214720"
  },
  {
    "text": "Yeah, [INAUDIBLE]",
    "start": "4214720",
    "end": "4219930"
  },
  {
    "text": "I didn't try extensively, but\nI did see a [INAUDIBLE] event. And I felt like the block\nsize was just moved.",
    "start": "4219930",
    "end": "4225270"
  },
  {
    "text": "Maybe I'm wrong. I don't actually know about\nthe internals of ChatGPT. We have two online questions. So one question is, \"what do\nyou think about architecture",
    "start": "4228163",
    "end": "4235985"
  },
  {
    "text": "[INAUDIBLE]?\" S4? S4. I'm sorry.",
    "start": "4235985",
    "end": "4241430"
  },
  {
    "text": "I don't know S4. Which one is this one? The second question, this\none's a personal question.",
    "start": "4241430",
    "end": "4247710"
  },
  {
    "text": "\"What are you going\nto work on next?\" [INAUDIBLE] I mean, so right now, I'm\nworking on things like nanoGPT.",
    "start": "4247710",
    "end": "4253740"
  },
  {
    "text": "Where is nanoGPT?",
    "start": "4253740",
    "end": "4254940"
  },
  {
    "text": "I mean, I'm going basically\nslightly from computer vision and like computer\nvision-based products, do",
    "start": "4258765",
    "end": "4263870"
  },
  {
    "text": "a little bit in language domain. Where's ChatGPT? OK, nanoGPT. So originally, I had minGPT,\nwhich I rewrote to nanoGPT.",
    "start": "4263870",
    "end": "4270215"
  },
  {
    "text": "And I'm working on this. I'm trying to reproduce\nGPTs, and I mean, I think something\nlike ChatGPT, I think,",
    "start": "4270215",
    "end": "4276050"
  },
  {
    "text": "incrementally improved\nin a product fashion would be extremely interesting. And I think a lot\nof people feel it,",
    "start": "4276050",
    "end": "4283010"
  },
  {
    "text": "and that's why it went so wide. So I think there's\nsomething like a Google plus",
    "start": "4283010",
    "end": "4288020"
  },
  {
    "text": "plus plus to build that I\nthink is more interesting. Shall we give our speaker\na round of applause?",
    "start": "4288020",
    "end": "4294650"
  }
]