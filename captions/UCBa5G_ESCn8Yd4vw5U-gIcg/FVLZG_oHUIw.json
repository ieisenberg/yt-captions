[
  {
    "text": "So welcome to our continuing\nwork on understanding unsupervised learning.",
    "start": "4800",
    "end": "10500"
  },
  {
    "text": "We're going through two\nkind of Stalwart algorithms, which are pretty fun today. We're going to finish out PCA,\nwhich is this old standby.",
    "start": "11460",
    "end": "18180"
  },
  {
    "text": "We use it for\ndimensionality reduction, sometimes visualization. It's kind of a core canonical\nunsupervised algorithm.",
    "start": "18180",
    "end": "24480"
  },
  {
    "text": "And we'll talk about\nthat today and I'll try and refresh and be a\nlittle bit complete there because I know we got cut\noff kind of in the middle.",
    "start": "24480",
    "end": "30000"
  },
  {
    "text": "If we have time-- then we're going to go through\nICA, we will have time for ICA. This is a fun problem.",
    "start": "30600",
    "end": "36660"
  },
  {
    "text": "This is this cocktail problem. You'll implement it actually. And it's one of the ones\nthat's a highlight every year.",
    "start": "36660",
    "end": "42420"
  },
  {
    "text": "People talk about what\nare the favorite homework problems they did. And the reason we're\ngoing to go through it is it's a place where you make\na different assumption about how",
    "start": "42420",
    "end": "50820"
  },
  {
    "text": "the noise works. It turns out if\nyou make the kind of traditional\nGaussian assumption here, which we've been making\nthrough class, it breaks.",
    "start": "50820",
    "end": "58199"
  },
  {
    "text": "So it's just nice to have\nan exposure, like there's more interesting things to do. And it's also kind\nof a fun problem.",
    "start": "58200",
    "end": "64739"
  },
  {
    "text": "If we finish those two and\nthere's some chance we do, but if we ask lots of questions,\nthat's actually even better.",
    "start": "64740",
    "end": "69900"
  },
  {
    "text": "I'll show you some stuff\nabout weak supervision and I have some slides prepared\nfor that, which are not",
    "start": "69900",
    "end": "75300"
  },
  {
    "text": "in the main deck, but if we get\nto that, I'll post the slides. Weak supervision,\nthe reason I would want to talk to you about it is\nit's a more modern technique.",
    "start": "75300",
    "end": "82920"
  },
  {
    "text": "It's behind some products\nthat you've actually used in the last while. And it's one of\nthese things that",
    "start": "82920",
    "end": "90180"
  },
  {
    "text": "looks like an EM-style\nmodel, but you can solve it in a very direct way. And since we're not having\nanother exam in the course,",
    "start": "90180",
    "end": "96720"
  },
  {
    "text": "you won't be responsible for it. It's a little bit more advanced. And so I can talk\nto you about how",
    "start": "96720",
    "end": "102060"
  },
  {
    "text": "that works without\nhopefully freaking you out. OK. Cool. So that's what I\nwant to do today. So let's talk about PCA.",
    "start": "102060",
    "end": "107880"
  },
  {
    "text": "All right. So PCA, if you recall,\nwe were looking at these unstructured things. I'm just going to\nrun through some",
    "start": "108840",
    "end": "114180"
  },
  {
    "text": "of where we were last time. We're looking at the structure. And our structure is a subspace. And it's a\nnon-probabilistic method.",
    "start": "114180",
    "end": "120780"
  },
  {
    "text": "So the two things are\nwe wanted some structure that we were looking for\nunderneath the covers. Remember GMM and\nK.Means, there's",
    "start": "120780",
    "end": "127140"
  },
  {
    "text": "a clustering kind of structure. Here, we want to look\nfor a linear subspace. And we wanted the\nnon-probabilistic version",
    "start": "127140",
    "end": "132660"
  },
  {
    "text": "of that. We looked at this kind\nof contrived example where we had pairs of highway\nand city miles per gallon.",
    "start": "132660",
    "end": "139140"
  },
  {
    "text": "And we were plotting cars. And we kind of\nintuitively would guess, like there are\nhybrids up here, there are SUVs down here\nand trucks maybe,",
    "start": "139140",
    "end": "145260"
  },
  {
    "text": "and economy cars in the middle. And we wanted to\nunderstand, what is a notion of good miles per\ngallon that took into account",
    "start": "145260",
    "end": "151320"
  },
  {
    "text": "both highway and city? And this was a way of\nmotivating effectively that what we wanted\nto do was to draw",
    "start": "151320",
    "end": "158340"
  },
  {
    "text": "a line along the direction that\nexplained most of the variance. That was our intuition\nmost of the ways",
    "start": "158340",
    "end": "163739"
  },
  {
    "text": "that they varied inside the\ndata set that was descriptive. So the way we did that is\nour first mathematical thing",
    "start": "163740",
    "end": "170640"
  },
  {
    "text": "is that we centered the data. Recall, we took the data,\nwe subtracted off the mean and literally, last time I\njust copy-pasted this data",
    "start": "170640",
    "end": "177960"
  },
  {
    "text": "and centered it at\napproximately the center of the data, which is mu. And we transformed it.",
    "start": "177960",
    "end": "183540"
  },
  {
    "text": "And the reason we\ntransformed it is we're using a linear mapping. That's what we want to do. And linear spaces go\nthrough the origin.",
    "start": "183540",
    "end": "190620"
  },
  {
    "text": "So this just lets\nus use linear spaces and have one fewer degree\nof freedom when we map them.",
    "start": "190620",
    "end": "195540"
  },
  {
    "text": "It's not important really to\nyour conceptual understanding, although you should\nthink about it. But it's really critical\nif you run these methods. If they're askew,\nthere will be no line",
    "start": "197760",
    "end": "203040"
  },
  {
    "text": "through the origin\nthat potentially goes through your data. Once we did this, I want to\nremind you of some things",
    "start": "203040",
    "end": "209879"
  },
  {
    "text": "that we're going to use\nfrom linear algebra. We had this idea\naround, we're going to prove formally what\nwe mean by the direction",
    "start": "209880",
    "end": "216120"
  },
  {
    "text": "of maximal variation. And we thought that\nthis line, intuitively, would be that variation.",
    "start": "216120",
    "end": "221520"
  },
  {
    "text": "That is, when we projected\nthings on the line, either we had them\nmaximally spread out,",
    "start": "221520",
    "end": "226680"
  },
  {
    "text": "that means we captured most of\nthe variation, that's actually what that means, or we saw\nthis dual thing in a second",
    "start": "226680",
    "end": "232020"
  },
  {
    "text": "that the residual,\nthe amount of error in our prediction on\nthat line was small. And we'll talk about\nthat again in a second.",
    "start": "232020",
    "end": "238560"
  },
  {
    "text": "If you don't remember\nyour linear algebra, remember that you can write once\nyou have an orthogonal basis, you can write any\npoint in that space.",
    "start": "238560",
    "end": "246180"
  },
  {
    "text": "In that orthogonal basis. Namely, every one of the\nvectors that are orthogonal, U1, U2 in this example and their\ndistances along those lines.",
    "start": "248880",
    "end": "256920"
  },
  {
    "text": "We're going to recall\nthe math that does that. That gives you a new set of\ncoordinates for your data.",
    "start": "256920",
    "end": "261959"
  },
  {
    "text": "If your data were it was given to you\nin 1,000 dimensions, you could run PCA\nunderneath it and find",
    "start": "261960",
    "end": "268800"
  },
  {
    "text": "the direction of\nmaximal variation and maybe its second\nand third components.",
    "start": "268800",
    "end": "274440"
  },
  {
    "text": "And then you could take that which are 1,000 numbers\nthat you were given and compress it down to 3.",
    "start": "274440",
    "end": "280259"
  },
  {
    "text": "And that's what we mean by\ndimensionality reduction. What are the three best numbers,\nif you like in some sense, that",
    "start": "280260",
    "end": "285659"
  },
  {
    "text": "represent your data set? Now just remember that\nfact that every point can be written in the basis.",
    "start": "285660",
    "end": "292140"
  },
  {
    "text": "And really, it's\nlike the coordinates of how far do I\ngo along U1, then I'm going to go along the\nU2 direction and get to x1.",
    "start": "292140",
    "end": "298139"
  },
  {
    "text": "This is alpha 1 and\nthis is alpha 2, as we'll see below\nfrom last time.",
    "start": "298140",
    "end": "303720"
  },
  {
    "text": "Now the convention\nis that you U1 and U2 are unit\nvectors because we only care about their direction. We don't care about their\nlength or their magnitude.",
    "start": "304560",
    "end": "311640"
  },
  {
    "text": "And this is just\nwhat you do there. And here, u1 was how good\nis the miles per gallon?",
    "start": "311640",
    "end": "317039"
  },
  {
    "text": "And u2 was the difference\nbetween their highway and city miles per gallon, roughly.",
    "start": "317640",
    "end": "322980"
  },
  {
    "text": "Those aren't formal\nstatements, that's just to give you an intuition\nof what this basis looks like. So you can have an intuitive\nfeel of what you're doing.",
    "start": "322980",
    "end": "329400"
  },
  {
    "text": "You want one direction, which\nis the principal component of variation. And if you had 1,000\ndifferent points,",
    "start": "329400",
    "end": "336120"
  },
  {
    "text": "you would then look\nat the other data sets and say, \"what\nother direction is the second principal component,\nthird principal component?\"",
    "start": "336120",
    "end": "342300"
  },
  {
    "text": "And if you remember\nyour linear algebra, that is just how you\nwrite down a basis. That's all I'm describing.",
    "start": "342300",
    "end": "347639"
  },
  {
    "text": "But we have to order\nthose bases in some way. We have to figure out\nwhich components do we pick first and among all of them?",
    "start": "347640",
    "end": "353220"
  },
  {
    "text": "And that's what we're going\nto solve in PCA, right? OK. So this is all just saying, x\ncan be written in this form.",
    "start": "353220",
    "end": "359099"
  },
  {
    "text": "And we may here, compress from\ndimension 2 to dimension 1. What that would mean\nwas we would probably",
    "start": "359100",
    "end": "364260"
  },
  {
    "text": "just keep the number alpha 1. That would just be, we would\ntreat x as its projection onto this line and just\ntreat it as alpha u1, OK?",
    "start": "364260",
    "end": "371640"
  },
  {
    "text": "And that's what we mean by\n\"explains more variation\". OK. So we're going to find\nthese directions today",
    "start": "372480",
    "end": "378240"
  },
  {
    "text": "with some caveats and we're\ngoing to think about thousands of dimensions to\ntens of dimensions. And this will be a\ndimensionality reduction",
    "start": "378240",
    "end": "383879"
  },
  {
    "text": "because we're going to\nonly keep those components. So before I move on, there's a\nlot of linear algebra in there. Ask me questions and I'm\nsuper happy to unpack this",
    "start": "383880",
    "end": "391440"
  },
  {
    "text": "because we're going to assume\nthis information-- we tried it out of last time--\ngoing forward, but I'm super happy to answer\nquestions and derive whatever's",
    "start": "391440",
    "end": "398220"
  },
  {
    "text": "here that's unclear. And if you have the question,\nalmost certainly somebody else does. So please, go ahead and ask.",
    "start": "398220",
    "end": "402600"
  },
  {
    "text": "Please. What if these alpha, which\nis basically the coefficients",
    "start": "406260",
    "end": "412259"
  },
  {
    "text": "are negative? So they can be negative, right? So for example,\nin this direction,",
    "start": "412260",
    "end": "417300"
  },
  {
    "text": "u1 is going this way. If you were to\nhave a negative, it would mean that it was in the\nnegative direction of that.",
    "start": "417300",
    "end": "422880"
  },
  {
    "text": "These are signs. So they can certainly\nbe negative. They're not positive\nscalars here. That's why we worry\nabout their squares",
    "start": "422880",
    "end": "429000"
  },
  {
    "text": "for how much of the\nresidual is because they can be positive or\nnegative coefficients, just like you can have something\nthat first component is",
    "start": "429000",
    "end": "434699"
  },
  {
    "text": "positive or negative. And positive tells\nyou to go this way and then negative tells\nyou to go that way.",
    "start": "434700",
    "end": "438660"
  },
  {
    "text": "So are all of these\ncoordinates based on the squares of\nthe [INAUDIBLE]?? We don't know yet, right?",
    "start": "440660",
    "end": "446760"
  },
  {
    "text": "So what we're going\nto do is we're going to try and find-- so\nall we know at this point that we're trying to\nmake sure that we get there is that we can take an\nx, and we can write it down",
    "start": "446760",
    "end": "454259"
  },
  {
    "text": "in a basis. So x has its original\nthing that it's given to us, in Rn, some large space. We can write it in new numbers,\nalpha i's for these ui's.",
    "start": "454260",
    "end": "462780"
  },
  {
    "text": "And this is a different basis\nfor the same underlying space. And then what we're\ngoing to try and do is say, \"how do we pick a good\nset of UI's to represent this?\"",
    "start": "462780",
    "end": "470880"
  },
  {
    "text": "And what I'm trying\nto hint at is that we don't have to\npick a complete space. We don't have to pick n\northogonal basis vectors,",
    "start": "470880",
    "end": "476640"
  },
  {
    "text": "we're going to pick the\ntop K, in some sense. And that top K, as we hope,\ncaptures the most variation.",
    "start": "476640",
    "end": "482400"
  },
  {
    "text": "And we're going to make that\nprecise in the next couple of write ups. Awesome questions.",
    "start": "482400",
    "end": "488039"
  },
  {
    "text": "Please, any others? Just this last question, top\nK like you will tell us-- Yeah, we haven't gotten there.",
    "start": "489720",
    "end": "499135"
  },
  {
    "text": "We haven't gotten there yet. Wonderful point. So that's exactly what I mean. We're going to say, how do\nwe find those directions, OK?",
    "start": "499135",
    "end": "506040"
  },
  {
    "text": "And last time we talked\nabout this preprocessing, we're going to assume\nthis for the rest of time. I would just say at some\npoint, reflect on this.",
    "start": "506040",
    "end": "512820"
  },
  {
    "text": "We have to center\nthe data, that's because we're going\nto look through lines that go through the origin. We want linear subspace.",
    "start": "512820",
    "end": "518159"
  },
  {
    "text": "So that makes sense that\nwe would want our data to have a chance for\nthese lines to explain the maximal variation. That's why we center.",
    "start": "518160",
    "end": "523740"
  },
  {
    "text": "That's important. We will also re-scale\nthe components so that they have about\nthe same amount of space.",
    "start": "523740",
    "end": "530220"
  },
  {
    "text": "So imagine if one of your\ncomponents was miles per gallon and another one was\nfeet per gallon. The feet per gallon because\nit was 5,000 times--",
    "start": "530220",
    "end": "537839"
  },
  {
    "text": "the numbers were they would be at vastly\ndifferent scales, right? And if you go through\nthe calculations",
    "start": "537840",
    "end": "544259"
  },
  {
    "text": "below because you're taking\nsquares and doing things, because they're at different\nscales, that makes them",
    "start": "544260",
    "end": "549960"
  },
  {
    "text": "unreasonably important, right? So one of the things\nthat you'll typically do is then scale by the\nvariance in each direction",
    "start": "550560",
    "end": "556800"
  },
  {
    "text": "and then allow them to all\nbe spread about a Gaussian because they're\ncentered component-wise",
    "start": "556800",
    "end": "561960"
  },
  {
    "text": "and then divided\nby that variance. That's just the other piece of\npreprocessing that goes on, OK?",
    "start": "561960",
    "end": "568079"
  },
  {
    "text": "All right. So now we've done that. We've done those pieces.",
    "start": "569160",
    "end": "574200"
  },
  {
    "text": "We need a couple of\nbits of mathematics just to remind yourself\nof what it means to find these kind of components.",
    "start": "574200",
    "end": "580380"
  },
  {
    "text": "So here, we have two\ncomponents, u1 and u2. And they're unit vectors,\nas we talked about before.",
    "start": "580380",
    "end": "586379"
  },
  {
    "text": "And they're orthogonal. That means their dot\nproduct is equal to zero, they're perpendicular\nto one another.",
    "start": "587700",
    "end": "592320"
  },
  {
    "text": "Now what we want to find, one of\nthe things that we have to find is kind of a subroutine\nintellectually of what",
    "start": "593880",
    "end": "599459"
  },
  {
    "text": "we have to do is, to\nfind the coordinate alpha 1 on this\nline, it makes sense that it's the closest point\nactually on this line to X.",
    "start": "599460",
    "end": "609540"
  },
  {
    "text": "That would be the point\nthat we would project it on. That's what projection\nmeans, actually. It's the closest point on the\nline in this sense in Euclidean",
    "start": "610620",
    "end": "616620"
  },
  {
    "text": "geometry. So what is this line? This is the set\nof all T times u1.",
    "start": "616620",
    "end": "621720"
  },
  {
    "text": "So that's any scalar,\npositive or negative, that was pointed out, that\nscales this entire line.",
    "start": "621720",
    "end": "627180"
  },
  {
    "text": "So basically, our\noptimization problem to find this\ncomponent, alpha 1 is to find the T that gets\nme to this closest point.",
    "start": "627180",
    "end": "634260"
  },
  {
    "text": "Now geometrically,\nit will hopefully be intuitive that should be\nperpendicular to this line.",
    "start": "634260",
    "end": "639420"
  },
  {
    "text": "This line here will\nbe perpendicular. And the reason is the rest\nwill be explained by U2.",
    "start": "639420",
    "end": "643980"
  },
  {
    "text": "So let's find out how we find\nthe closest point to the plane. And I'm going to\nwrite it out now. So how do we do that?",
    "start": "646140",
    "end": "650940"
  },
  {
    "text": "Just to remind ourselves. So how do we do that? So alpha 1 is going to\nbe equal to the ArgMin,",
    "start": "651600",
    "end": "659160"
  },
  {
    "text": "this is just\nrewriting what I said, over all alpha,\nsuch that we have here, x minus alpha u1 square.",
    "start": "659160",
    "end": "666660"
  },
  {
    "text": "The square just makes our\nlife a little bit easier mathematically. Squares of norms are\njust easier to work with,",
    "start": "666660",
    "end": "673079"
  },
  {
    "text": "doesn't really make any\ndifference in the underlying piece. But this is just saying\namong all the-- oops-- among all the alphas--",
    "start": "673080",
    "end": "679440"
  },
  {
    "text": "among all the alphas, which are\nrunning up and down this line, I want the one that has\nthe closest distance to x.",
    "start": "680580",
    "end": "686100"
  },
  {
    "text": "That's what I'm\ncalling the projection. So what does that equal? Well, this is the same\nas doing the ArgMin--",
    "start": "686100",
    "end": "693060"
  },
  {
    "text": "let me write it on the\nnext line actually.",
    "start": "693060",
    "end": "695460"
  },
  {
    "text": "The ArgMin-- and I'm just\ngoing to expand this norm out.",
    "start": "699660",
    "end": "704339"
  },
  {
    "text": "So it's x squared plus alpha\nsquare, u1 square minus 2 alpha",
    "start": "705120",
    "end": "712980"
  },
  {
    "text": "x dot u1, OK? And this is-- I'm just writing\na dot product in a notation. Hopefully not too confusing,\nthis is just the dot product.",
    "start": "713760",
    "end": "720540"
  },
  {
    "text": "Just to make it\nclear without having little tiny dots to look at.",
    "start": "721200",
    "end": "724080"
  },
  {
    "text": "So a couple of\nthings right away. This term is one because\nit's a unit vector.",
    "start": "726240",
    "end": "734880"
  },
  {
    "text": "It's a unit vector. And this term is\nirrelevant for alpha.",
    "start": "734880",
    "end": "741420"
  },
  {
    "text": "Alpha's value here doesn't\nchange the value of x. x is given. So this is equivalent for us\nwhen we take the derivative--",
    "start": "741420",
    "end": "748140"
  },
  {
    "text": "we take the derivative\nwith respect to alpha of this expression.",
    "start": "750060",
    "end": "752940"
  },
  {
    "text": "This expression looks like to\nus, alpha squared minus 2 alpha",
    "start": "756180",
    "end": "763500"
  },
  {
    "text": "x, u1. And when we take the\nderivative with respect to alpha of this thing, this is\nderivative of respect to alpha",
    "start": "764280",
    "end": "771060"
  },
  {
    "text": "of this expression, that\nequals 2 times alpha, which is from the\nfirst one, minus x, u1.",
    "start": "771060",
    "end": "778380"
  },
  {
    "text": "Now to set this equal to This implies alpha\nequals x dotted into u1.",
    "start": "781080",
    "end": "788340"
  },
  {
    "text": "OK. That's all that's saying.",
    "start": "789480",
    "end": "793200"
  },
  {
    "text": "So this is just saying\nsomething you may have forgotten from linear algebra or\nyou're now remembering,",
    "start": "794640",
    "end": "799860"
  },
  {
    "text": "which is that the dot\nproduct of a unit vector is actually a projection.",
    "start": "799860",
    "end": "803640"
  },
  {
    "text": "So far, so good? All right.",
    "start": "807600",
    "end": "810120"
  },
  {
    "text": "Now one piece here is\nthat we can generalize to higher dimensions.",
    "start": "813420",
    "end": "818400"
  },
  {
    "text": "Do more components.",
    "start": "822000",
    "end": "823440"
  },
  {
    "text": "And it's worth actually thinking\nabout what this looks like, right? So the point is when\nwe write this down,",
    "start": "827580",
    "end": "834480"
  },
  {
    "text": "we're going to have here u1\nto uk, for some value of k,",
    "start": "835440",
    "end": "840900"
  },
  {
    "text": "this is just for an exercise\nfor arbitrary value of k, element of rd. Some x that's also living in rd.",
    "start": "840900",
    "end": "847560"
  },
  {
    "text": "And then here, we're going to\ncalculate coordinates, alpha",
    "start": "849360",
    "end": "853860"
  },
  {
    "text": "sum k equals 1 to d, alpha k,",
    "start": "857820",
    "end": "863340"
  },
  {
    "text": "uk is smallest.",
    "start": "863340",
    "end": "865980"
  },
  {
    "text": "Clear enough? This is finding the closest\npoint in the subspace. Instead of a line, we're\nfinding the closest point",
    "start": "868440",
    "end": "874200"
  },
  {
    "text": "in the subspace. Hopefully, clear and\nyou remember this. If not, please ask a question.",
    "start": "874200",
    "end": "880440"
  },
  {
    "text": "Super easy to explain.",
    "start": "880440",
    "end": "881700"
  },
  {
    "text": "So by the same basic reasoning,\nyou compute the derivative. You unpack it. You have to use the fact that\nthe uik's are orthogonal.",
    "start": "886020",
    "end": "893940"
  },
  {
    "text": "Why? When you expand the\nsquares, right, you're going to get products\nof uk dot into uj",
    "start": "893940",
    "end": "900060"
  },
  {
    "text": "and those will cancel out. So you'll basically\nhave just a bunch of expressions that are\npresent in which alpha k is",
    "start": "900780",
    "end": "907320"
  },
  {
    "text": "going to be X minus uk. And this is only because\nthey're orthogonal. Only because orthogonal.",
    "start": "907320",
    "end": "913080"
  },
  {
    "text": "uk are orthogonal.",
    "start": "915420",
    "end": "917339"
  },
  {
    "text": "And you can do that derivative\nvery, very, very quickly.",
    "start": "922980",
    "end": "925139"
  },
  {
    "text": "Now this quantity\nhere is important.",
    "start": "929100",
    "end": "933779"
  },
  {
    "text": "That's a terrible highlight. Let's use this one. This thing here is\ncalled the residual.",
    "start": "934440",
    "end": "940680"
  },
  {
    "text": "All right. This is the residual.",
    "start": "945240",
    "end": "949560"
  },
  {
    "text": "And what we care about when\nwe're going to do PCA is, we either want to find--",
    "start": "952380",
    "end": "959520"
  },
  {
    "text": "we want to find a set of\npoints, a set of directions, such that when we do this\nprojection onto them,",
    "start": "959520",
    "end": "965040"
  },
  {
    "text": "the sum of all the\nresiduals is minimized. OK? So this tells us the residual\nfor a given point in a given",
    "start": "965040",
    "end": "973019"
  },
  {
    "text": "basis. And now I have an\noptimization problem, right? And I'll write it out formally\nin a second, but intuitively,",
    "start": "973020",
    "end": "978240"
  },
  {
    "text": "you can think about-- what's going to\nhappen is I'm going to pick K of these U's that\nare going to be orthogonal.",
    "start": "978240",
    "end": "984600"
  },
  {
    "text": "There are many of them\nthat I could pick, many orthogonal bases\nthat I could pick. I pick one of them.",
    "start": "984600",
    "end": "990300"
  },
  {
    "text": "Then I project all of\nmy data onto that set. I measure how well I did by\neither how much I captured",
    "start": "990300",
    "end": "998339"
  },
  {
    "text": "in the data set or by\nhow much was missing, which is the residual. And this is the residual. I then have that per point.",
    "start": "998340",
    "end": "1004760"
  },
  {
    "text": "So I sum up over all of those. And this gives me a score\nof how good that basis was.",
    "start": "1004760",
    "end": "1010100"
  },
  {
    "text": "And among all the\nbases, I want to pick the one that\nminimizes the residual",
    "start": "1010100",
    "end": "1015260"
  },
  {
    "text": "or maximizes the\nprojected subspace. So let me write\nsome of that down. And then you ask\nquestions if you like.",
    "start": "1015260",
    "end": "1022760"
  },
  {
    "text": "So we can find\nPCA by two things. By the way, this is not-- this seems trivial, potentially.",
    "start": "1025100",
    "end": "1032659"
  },
  {
    "text": "That the maximizing\nthe projected space or variance and the minimizing\ndistance are the same.",
    "start": "1034640",
    "end": "1040760"
  },
  {
    "text": "It's actually not true in\ngeneral for other geometries. So it's actually quite a nice\nthing about Euclidean space. That doesn't matter to\nyou, but just a comment.",
    "start": "1040760",
    "end": "1048380"
  },
  {
    "text": "Minimize residual.",
    "start": "1050180",
    "end": "1050780"
  },
  {
    "text": "So let's do this one. This is the one we're\ngoing to do in class. Maximize the projected space.",
    "start": "1055580",
    "end": "1061880"
  },
  {
    "text": "So let's do it for one vector. So now we want to pick among\nall the possible u's, what's the version--",
    "start": "1062420",
    "end": "1068720"
  },
  {
    "text": "what's the particular\nsetting of u that explains the most about our data? So what that's going to be\nfrom our previous discussion is",
    "start": "1069320",
    "end": "1076820"
  },
  {
    "text": "this. Max u over Rd subject\nto u equals 1.",
    "start": "1076820",
    "end": "1082700"
  },
  {
    "text": "average over all the points. Although of course,\nsuch a constant doesn't really matter\nbecause we're maximizing. But it's nice to\nhave the right scale.",
    "start": "1086720",
    "end": "1093020"
  },
  {
    "text": "u dot xy squared.",
    "start": "1094640",
    "end": "1096980"
  },
  {
    "text": "So what we're\nsaying here, we pick a direction and\nthis direction we want to get the\nlargest dot product.",
    "start": "1099740",
    "end": "1105080"
  },
  {
    "text": "How much-- this is x1\nprojected into this. This is the alphas. The sum of those\nalpha squared I's,",
    "start": "1105080",
    "end": "1111140"
  },
  {
    "text": "we want to be as\nbig as possible. So we want to pick the direction\namong all the directions. So imagine and in 2D, you're\njust kind of spinning around",
    "start": "1111140",
    "end": "1118100"
  },
  {
    "text": "and you're judging how great\nthe subspace is by maximizing how much is present.",
    "start": "1118880",
    "end": "1124100"
  },
  {
    "text": "We need a couple of\nfacts to solve this. Hopefully, the point is clear. We need some facts\nto solve this.",
    "start": "1127580",
    "end": "1133279"
  },
  {
    "text": "OK. So first fact we\nneed to recall is, let A be a symmetric and square\nmatrix, which kind of makes",
    "start": "1141200",
    "end": "1155180"
  },
  {
    "text": "sense. Then it's a normal matrix.",
    "start": "1155180",
    "end": "1161240"
  },
  {
    "text": "And in particular, it\ncan be written like this. U lambda U transpose,\nwhere UUT equals I,",
    "start": "1161240",
    "end": "1170420"
  },
  {
    "text": "the basis is orthogonal. And lambda is diagonal.",
    "start": "1170420",
    "end": "1176600"
  },
  {
    "text": "Not all matrices are diagonal. Not all matrices are\northogonally diagonal.",
    "start": "1179540",
    "end": "1185240"
  },
  {
    "text": "But if it's symmetric and\nsquare, it's called normal",
    "start": "1186260",
    "end": "1190640"
  },
  {
    "text": "then it has this. And lambda has a\nnice interpretation. Lambda ii, since it's a diagonal\nmatrix, equals lambda i.",
    "start": "1192080",
    "end": "1201980"
  },
  {
    "text": "And we call these\nthe eigenvalues. Lambda 1, and they're\nreal by convention.",
    "start": "1201980",
    "end": "1210740"
  },
  {
    "text": "So we order them this way\njust because it's nice to talk about them as\nlambda 1 being the big one, lambda N being the small one.",
    "start": "1210740",
    "end": "1216679"
  },
  {
    "text": "OK? Now if you don't remember\nyour linear algebra, maybe this doesn't\nseem mysterious to you. But if you think about\nthe underlying model,",
    "start": "1217220",
    "end": "1223820"
  },
  {
    "text": "there's no reason in general\nthat these things should even be real valued. They could be complex\nvalued in general. But if they're symmetric and\nit's nice, then this happens.",
    "start": "1223820",
    "end": "1231380"
  },
  {
    "text": "They're real and you can order\nthem, which is really nice. So we're going to use that fact. And if that's confusing\nto you to remember",
    "start": "1231380",
    "end": "1237620"
  },
  {
    "text": "what happens when\nyou diagonalize over the complex plane,\ndon't worry about it at all. Just take this as a fact.",
    "start": "1237620",
    "end": "1242779"
  },
  {
    "text": "OK? So these characters\nhere, as I mentioned,",
    "start": "1244040",
    "end": "1251060"
  },
  {
    "text": "these are the eigenvalues.",
    "start": "1251060",
    "end": "1252320"
  },
  {
    "text": "All right. So recall if X equals sum K\nequals 1 to n, alpha K, UK,",
    "start": "1258560",
    "end": "1270799"
  },
  {
    "text": "where U1, UN equals U. We can\nwrite the following thing.",
    "start": "1272480",
    "end": "1280880"
  },
  {
    "text": "We can write alpha X-- AX, sorry, is equal to\nU lambda U transpose",
    "start": "1280880",
    "end": "1287060"
  },
  {
    "text": "X. This is equal to U lambda sum\nK equals 1 to N, alpha K, EK--",
    "start": "1287060",
    "end": "1296120"
  },
  {
    "text": "oh, sorry. EK-- sorry, let me\nwrite it like this.",
    "start": "1296120",
    "end": "1300020"
  },
  {
    "text": "Yeah, I do want to\nwrite it that way. OK. Perfect. Alpha K, EK, what's\ngone on here,",
    "start": "1301760",
    "end": "1308000"
  },
  {
    "text": "U transpose U is exactly the-- so all that's going on here is\nwhen I dot product UK into one",
    "start": "1308000",
    "end": "1318139"
  },
  {
    "text": "of these, which one survives? Exactly UK, right? If it's different, if\nit's UJ that's different,",
    "start": "1318140",
    "end": "1323180"
  },
  {
    "text": "then they're going\nto contribute 0. So this becomes alpha K\nin the standard basis.",
    "start": "1323180",
    "end": "1327860"
  },
  {
    "text": "This is a standard basis. That's confusing,\nask a question. What's going on here, again,\nX is written in this form.",
    "start": "1330380",
    "end": "1337400"
  },
  {
    "text": "What happens when I\nmultiply U transpose by X? It multiplies it by each of\nthe UI's, only one of them",
    "start": "1337400",
    "end": "1344000"
  },
  {
    "text": "survives. For the K-th term,\nonly the Kth one survives because\notherwise, they would be 0.",
    "start": "1344000",
    "end": "1351260"
  },
  {
    "text": "So I get this. Then when I multiply it\nby the diagonal matrix, I get U times sum, K equals 1\nto N, lambda K, alpha K, EK.",
    "start": "1351260",
    "end": "1362600"
  },
  {
    "text": "And then I get back to\nsum K equals 1 to N, I multiply by U again. I get lambda K, alpha K, UK.",
    "start": "1363740",
    "end": "1371060"
  },
  {
    "text": "Because again, when I\nmultiply EK, the basis vector, this is the vector where it only\nhas a 1 in the Kth position.",
    "start": "1371060",
    "end": "1377840"
  },
  {
    "text": "By U, it selects out\nUK and I get back. So what this means is, this is\na long winded way of saying,",
    "start": "1377840",
    "end": "1384140"
  },
  {
    "text": "if I multiply A\nin this basis, all it does is act by multiplying\nby the eigenvectors.",
    "start": "1384140",
    "end": "1388640"
  },
  {
    "text": "This is an eigendecomposition,\nif you remember it. That's all that's going on.",
    "start": "1389900",
    "end": "1393260"
  },
  {
    "text": "Please?",
    "start": "1400040",
    "end": "1400400"
  },
  {
    "text": "How do we [INAUDIBLE] U-- like, how do we generate a\n[INAUDIBLE] so that we get all of these lines properly where\nwe can get all of the U's? Because once we have the\nU's, then you can use-- that is residual\nminimizing polynomial here.",
    "start": "1407540",
    "end": "1413540"
  },
  {
    "text": "Yeah, so I think-- we haven't gotten\nback to PCA yet.",
    "start": "1413540",
    "end": "1419000"
  },
  {
    "text": "We need one more fact. Hold on just one minute and\nwe'll come back to that. We're just recalling facts\nfrom your linear algebra class.",
    "start": "1419000",
    "end": "1424700"
  },
  {
    "text": "But it's a great point. You should be thinking\nexactly that, right? So let me just\npoint out one fact",
    "start": "1426380",
    "end": "1431660"
  },
  {
    "text": "here, which is that if I take\nthe max over all the unit",
    "start": "1431660",
    "end": "1437060"
  },
  {
    "text": "vectors of X transpose X,\nA, it can also be written--",
    "start": "1437060",
    "end": "1442520"
  },
  {
    "text": "and this is the formula we've\nbeen kind of hinting at-- as alpha square,\nsum K equals 1 to N,",
    "start": "1442520",
    "end": "1450440"
  },
  {
    "text": "alpha K squared, lambda K. OK?",
    "start": "1451460",
    "end": "1457159"
  },
  {
    "text": "So now if you think\nabout this, how do you find the\nprincipal eigenvalue?",
    "start": "1461660",
    "end": "1465800"
  },
  {
    "text": "How do you find the largest\nway to express this? Well, since we\nonly get to spend--",
    "start": "1467060",
    "end": "1472700"
  },
  {
    "text": "since we only get to\nspend our alpha squared along the components and we\nonly have one unit to spend,",
    "start": "1472700",
    "end": "1479060"
  },
  {
    "text": "what maximizes this expression? Well, we want to put as\nmuch value-- go ahead.",
    "start": "1479720",
    "end": "1485180"
  },
  {
    "text": "Where do we get\nthe alpha squared? Because when we\nmultiply here by X,",
    "start": "1485180",
    "end": "1490580"
  },
  {
    "text": "alpha times X is going to\nbe equal to this expression. And then when we\ndot X in again, we",
    "start": "1491240",
    "end": "1497060"
  },
  {
    "text": "get the alpha squares because\nthey pair up to one another. I apologize for going too fast.",
    "start": "1497060",
    "end": "1500720"
  },
  {
    "text": "Great question. So we have the alpha squares\ntimes the lambda K's. Now the lambda K's, because\nlet's imagine they're 1 to 10,",
    "start": "1503060",
    "end": "1510500"
  },
  {
    "text": "where would you\nput all your mass if you wanted to maximize this? Well, on the largest one, right?",
    "start": "1510500",
    "end": "1515120"
  },
  {
    "text": "So how do you maximize\nthe amount of mass you put on the largest one? You put in our notation,\nalpha 1 equals 1.",
    "start": "1515660",
    "end": "1523160"
  },
  {
    "text": "Because the rest are\nall then equal to 0, this is going to\nbe the largest one. And that's the\nprincipal eigenvector. Imagine they're\nstrictly different.",
    "start": "1524000",
    "end": "1530000"
  },
  {
    "text": "Does that make sense? Please. [INAUDIBLE] hard lined,\nwhen  ",
    "start": "1530600",
    "end": "1538700"
  },
  {
    "text": "you multiply, did you do with that\nsummation many X? Yeah. Where, here? [INAUDIBLE] you have\nto do [INAUDIBLE]..",
    "start": "1538700",
    "end": "1547209"
  },
  {
    "text": "This thing? Yeah. So how does it suddenly\nbecome [INAUDIBLE].. This one is just the\nfact that it's diagonal.",
    "start": "1547209",
    "end": "1553460"
  },
  {
    "text": "So EK times a diagonal is\njust the Kth unit vector times that value.",
    "start": "1553460",
    "end": "1559159"
  },
  {
    "text": "So that pulls out lambda K,\nwhich is on the diagonal. And then it will give\nyou just [INAUDIBLE] do summation form of UK? Yeah.",
    "start": "1559160",
    "end": "1566660"
  },
  {
    "text": "And then when you\nhave it-- when you have a unit vector\nmultiplied by a matrix, it just selects out that column.",
    "start": "1566660",
    "end": "1571040"
  },
  {
    "text": "Yeah. Awesome. But is this clear? If I want to maximize this,\nI set alpha 1 equal 1. Because we know lambda 1.",
    "start": "1575120",
    "end": "1581420"
  },
  {
    "text": "Now what if lambda That is, there are two\neigenvalues that are present.",
    "start": "1581420",
    "end": "1589880"
  },
  {
    "text": "Lambda 1 equals lambda 2. Then it turns out there's an\nentire subspace of solutions.",
    "start": "1590540",
    "end": "1596540"
  },
  {
    "text": "I could pick any\nalpha 1 and alpha 2, such that they're squared sum\nto 1, anywhere on that circle.",
    "start": "1597500",
    "end": "1603380"
  },
  {
    "text": "And if lambda 1 equal\nlambda 2 equal lambda 3, now I can pick anywhere\nin a subspace of size 3.",
    "start": "1604040",
    "end": "1608540"
  },
  {
    "text": "That's going to\nbe important when we think about how well\ndefined PCA is because it's only well defined what\nthe principal component",
    "start": "1609200",
    "end": "1615260"
  },
  {
    "text": "of variation is if lambda 1 is\nstrictly bigger than lambda 2. If there's no gap,\nthen the coordinates",
    "start": "1615260",
    "end": "1621140"
  },
  {
    "text": "aren't well defined anymore. I can pick anything\nI just described. Does that make sense?",
    "start": "1621140",
    "end": "1624980"
  },
  {
    "text": "Please. Why is alpha equal to one? So alpha 1 is equal to 1.",
    "start": "1629180",
    "end": "1634520"
  },
  {
    "text": "And the reason is-- so if\nthis constraint, this norm constraint means that I\nhave to pick among all the",
    "start": "1634520",
    "end": "1640279"
  },
  {
    "text": "alpha, such that their\nsquares sum to 1. So if among all the ones that\nsquare sum to 1, which one's",
    "start": "1640280",
    "end": "1646460"
  },
  {
    "text": "going to give me\nthe biggest value? Intuitively, I want to put\nall my mass on lambda 1 because lambda 1 is the\nbiggest value, right?",
    "start": "1646460",
    "end": "1652520"
  },
  {
    "text": "So that setting, I would\nset alpha 1 equal to 1 and all the alpha K equal\nto 0 for K greater than 1.",
    "start": "1653900",
    "end": "1659180"
  },
  {
    "text": "That would be the value\nthat I would pick there because that one's guaranteed. I can't really do\nbetter than that.",
    "start": "1659180",
    "end": "1664880"
  },
  {
    "text": "If I slide off even an\nepsilon amount of mass, well then it's going\nto a smaller eigenvalue and because I lost\nthat mass, I would",
    "start": "1664880",
    "end": "1671299"
  },
  {
    "text": "get epsilon squared times that. You can also just compute the\nderivative using Lagrangian",
    "start": "1671300",
    "end": "1677480"
  },
  {
    "text": "if the intuitive thing\ndoesn't make sense, which we did two lectures ago.",
    "start": "1677480",
    "end": "1682700"
  },
  {
    "text": "Cool. So let's go back to PCA and\nsay, exactly where do we get this UI?",
    "start": "1684440",
    "end": "1690380"
  },
  {
    "text": "The thing I wanted to\npoint out that is here, I'm going to come back to\nthis point about what happens with lambda 1 and lambda 2. If you missed it, don't worry.",
    "start": "1690380",
    "end": "1696080"
  },
  {
    "text": "I just care that\nyou're aware what we're doing in the maximization. Now back to PCA.",
    "start": "1697040",
    "end": "1702799"
  },
  {
    "text": "So recall, I'll just go up here. Sorry, I'll copy because\nI'm extremely lazy. Where is our PCA?",
    "start": "1705800",
    "end": "1711320"
  },
  {
    "text": "Where did we do this? Oh, here. I just want to make\nsure you realize I'm not like doing\nsomething strange",
    "start": "1712760",
    "end": "1718700"
  },
  {
    "text": "and changing the expression. This is what we\nwanted to deal with. Well, this expression\nhere, we can rewrite.",
    "start": "1718700",
    "end": "1725659"
  },
  {
    "text": "And we can rewrite it as\nXI transpose, U transpose--",
    "start": "1726260",
    "end": "1732260"
  },
  {
    "text": "oh, sorry. X transpose U. U transpose, XI.",
    "start": "1733460",
    "end": "1739399"
  },
  {
    "text": "Sum, I goes from 1\nto N. 1 to N. Let me drag that and give myself\na little bit more space here",
    "start": "1740660",
    "end": "1746540"
  },
  {
    "text": "so it's not crowding\nyou too much. So this is equal. These two expressions are equal. I'm just expanding\nout the square.",
    "start": "1746540",
    "end": "1752659"
  },
  {
    "text": "But this is pretty nice\nbecause now I can pull out U-- oh, sorry.",
    "start": "1755960",
    "end": "1761480"
  },
  {
    "text": "I wrote it backwards. I didn't want to do this. Let me write it the other way. That'll just make my life\neasier in the next move.",
    "start": "1761480",
    "end": "1767000"
  },
  {
    "text": "U2-- I'm so sorry about\nthat, X transpose U.",
    "start": "1767000",
    "end": "1772040"
  },
  {
    "text": "Sorry, that was foolish. It's true, but foolish. OK. So now I can pull out the U's\nbecause they're on the outside.",
    "start": "1773420",
    "end": "1780680"
  },
  {
    "text": "This is U transpose sum I equals",
    "start": "1780680",
    "end": "1784100"
  },
  {
    "text": "Why is that? Because this is linear. So I can pull this\nout of the sum. And all of these\nthings are paired up.",
    "start": "1791300",
    "end": "1798140"
  },
  {
    "text": "This thing here,   because\nthis is a quantity",
    "start": "1799280",
    "end": "1805760"
  },
  {
    "text": "that you may\nremember previously. This is the covariance\nof your data.",
    "start": "1805760",
    "end": "1810200"
  },
  {
    "text": "Why is it the covariance? Because you subtract\nit off the mean. This is the sample covariance. And so here, I can\npush the 1 by N inside.",
    "start": "1814880",
    "end": "1822379"
  },
  {
    "text": "We do that to 1 by N.\nAnd then it's really the sample covariance.",
    "start": "1822980",
    "end": "1827779"
  },
  {
    "text": "This thing here.",
    "start": "1829880",
    "end": "1830540"
  },
  {
    "text": "So then what will U be if\nI want to maximize this U?",
    "start": "1836780",
    "end": "1841040"
  },
  {
    "text": "Well, it would be the\nprincipal eigenvalue. This is an eigenvalue problem,\njust as we went up here, it's now of this form.",
    "start": "1847700",
    "end": "1853700"
  },
  {
    "text": "We can verify, is it symmetric? Yeah. It's a symmetric. It's a sum of--\nthese are each are",
    "start": "1854780",
    "end": "1860240"
  },
  {
    "text": "a sum of symmetric matrices. So the covariance is symmetric. It is actually really\na covariance matrix.",
    "start": "1860240",
    "end": "1866059"
  },
  {
    "text": "It also happens to be\npositive semi-definite. That means all the\nlambdas are non-negative. So that's good news.",
    "start": "1866900",
    "end": "1872420"
  },
  {
    "text": "We didn't need that,\nbut that's nice to have. And now when we look at the\nU's as we go through there,",
    "start": "1873440",
    "end": "1879740"
  },
  {
    "text": "we have to pick a direction. Which direction do we pick? Well, we're going to pick the\ndirection that corresponds",
    "start": "1879740",
    "end": "1885980"
  },
  {
    "text": "to alpha 1, which is U1, right,\nwhich is the basis when we do the decomposition here, it's\nthe principal eigenvector",
    "start": "1885980",
    "end": "1893360"
  },
  {
    "text": "of this thing. So the best U to pick is\nthe principal eigenvector",
    "start": "1893360",
    "end": "1898460"
  },
  {
    "text": "of the covariance. Some of you are nodding. Some of you look like I\nsaid something horrible.",
    "start": "1898460",
    "end": "1904820"
  },
  {
    "text": "So please, both ask questions. Is this the A matrix\nthat I was asking about?",
    "start": "1904820",
    "end": "1909871"
  },
  {
    "text": "Yeah. This is a covariance. We're going to sub this\ncovariance matrix into the A. Yeah.",
    "start": "1909871",
    "end": "1914480"
  },
  {
    "text": "Awesome. What happens if we\nwant two components? What do you think we pick?",
    "start": "1919880",
    "end": "1924860"
  },
  {
    "text": "Anyone? The first two. The first two that correspond\nto the largest two eigenvectors.",
    "start": "1927860",
    "end": "1932899"
  },
  {
    "text": "Three, the largest three. If the first two are equal\nand you want one direction,",
    "start": "1932900",
    "end": "1939080"
  },
  {
    "text": "lambda 1 equals lambda because you have two potential\nrepresentations for it.",
    "start": "1939080",
    "end": "1944900"
  },
  {
    "text": "But you can still\npick A component of principal variation. And that's the way that PCA\nis potentially undefined.",
    "start": "1945500",
    "end": "1951500"
  },
  {
    "text": "Cool. That's all it is. So how do we represent\ndata with this, just to make sure it's clear?",
    "start": "1956480",
    "end": "1961040"
  },
  {
    "text": "Well, we map XI to sum--",
    "start": "1967340",
    "end": "1972860"
  },
  {
    "text": "let's say we picked\nD-dimensions. XI minus Uj, Uj.",
    "start": "1972860",
    "end": "1980120"
  },
  {
    "text": "Namely, this is its coordinates\nin the top K eigenvalues that we picked. On average, this captures the\nmost variance in our data.",
    "start": "1981560",
    "end": "1988700"
  },
  {
    "text": "And we just keep--\nthis is a scalar. This is what we were\ncalling alpha j earlier.",
    "start": "1988700",
    "end": "1993320"
  },
  {
    "text": "We just keep these K scalars. So this map, what it does,\nit takes in 1,000 dimensions.",
    "start": "1995120",
    "end": "2002200"
  },
  {
    "text": "Let's say, I started\nwith 1,000 dimensions. Then it's going to pick five.",
    "start": "2002200",
    "end": "2006640"
  },
  {
    "text": "Which five? Well, they're going to\nbe a blend of those. They're not going to be\nany individual dimension. They're going to be these\neigenvectors of the underlying",
    "start": "2007540",
    "end": "2014440"
  },
  {
    "text": "covariance. They captured the\nmost-- the reason we like them is they capture\nthe most of our data.",
    "start": "2014440",
    "end": "2019780"
  },
  {
    "text": "They throw away the\nleast amount of our data, which is the other\nresidual interpretation. And this gives us a map that\ngoes from R, let's call it big",
    "start": "2019780",
    "end": "2027580"
  },
  {
    "text": "D, down to R little d, OK?",
    "start": "2027580",
    "end": "2029260"
  },
  {
    "text": "And that's in what sense, this\nis a dimensionality reduction. And we can use that, for\nexample, to take our data",
    "start": "2033100",
    "end": "2038200"
  },
  {
    "text": "and take it from 100 dimensions\nand project it onto two and visualize it,\ndraw it on a map.",
    "start": "2038200",
    "end": "2042700"
  },
  {
    "text": "Yeah, awesome. Please. Before we get [INAUDIBLE]\non, [INAUDIBLE] can you give an estimate of\nwhat the small D will be here?",
    "start": "2044500",
    "end": "2053679"
  },
  {
    "text": "Yeah, great question. So you want to know how\ndo you pick K or D here? Little D. So here in contrast,\nfor the last three lectures,",
    "start": "2053680",
    "end": "2064720"
  },
  {
    "text": "I've been telling you, \"I\nhave no idea how to pick K.\" Here, I at least have an idea. Oh, go ahead. Oh. I'm just confused it's, like,\nwe just mapped everything",
    "start": "2064720",
    "end": "2071742"
  },
  {
    "text": "into a U [INAUDIBLE]. I think I confused\nyou with something.",
    "start": "2071743",
    "end": "2078820"
  },
  {
    "text": "So let me make sure. Let me just call this K,\njust for now to make this. K is less than what\nyou're thinking of as D.",
    "start": "2078820",
    "end": "2084159"
  },
  {
    "text": "How about that? Yeah. Yeah. So basically, we map\neverything into a subspace, but this subspace is\nstill D-dimensional",
    "start": "2084160",
    "end": "2090604"
  },
  {
    "text": "because of the lengths\nof the vector, D is to-- No. That's a great point.",
    "start": "2090604",
    "end": "2097359"
  },
  {
    "text": "So it's decomposed into\nD-dimensional vectors, but it only now takes K\ncoordinates to represent it.",
    "start": "2097360",
    "end": "2103540"
  },
  {
    "text": "So again, going back to the\ntwo dimensional picture. We have a two\ndimensional picture, but we projected\neverything onto the line.",
    "start": "2103540",
    "end": "2109480"
  },
  {
    "text": "So now we can represent\nthings by just its distance on the line, the alpha 1. So we've taken 2\nscalars, which are,",
    "start": "2109480",
    "end": "2114580"
  },
  {
    "text": "like, it's x and y-coordinates,\nand reduced it to just one, which is the alpha 1. Does that make sense? Makes sense. Awesome.",
    "start": "2114580",
    "end": "2120460"
  },
  {
    "text": "So let's see how we pick K here. How do we pick K?",
    "start": "2122080",
    "end": "2125619"
  },
  {
    "text": "And this won't be super\nsatisfying, but you know, whatever. It'll be fine.",
    "start": "2128860",
    "end": "2132760"
  },
  {
    "text": "OK. So this actually does\nhave some kind of trick.",
    "start": "2140140",
    "end": "2145180"
  },
  {
    "text": "So what does this actually mean? So this is basically\nlooking at the trace of A,",
    "start": "2145180",
    "end": "2149559"
  },
  {
    "text": "which is equal to this sum\non the bottom, lambda i.",
    "start": "2150760",
    "end": "2154120"
  },
  {
    "text": "So if you remember\nyour linear algebra. But basically what\nit's saying is, what I want to know is how much\nof the space am I explaining?",
    "start": "2155800",
    "end": "2162280"
  },
  {
    "text": "So imagine I have Intuitively, if I get\nthe top 10 of them,",
    "start": "2162280",
    "end": "2167620"
  },
  {
    "text": "the worst case is that they\nexplain .1 of my space. Meaning that my sum of those\neigenvalues are about 0.1",
    "start": "2167620",
    "end": "2173560"
  },
  {
    "text": "and I'm throwing away\nroughly 90% of my data. It's not what it means, but\njust kind of think intuitively. What this is saying is\nthat traditionally, people",
    "start": "2173560",
    "end": "2180100"
  },
  {
    "text": "will pick K here so that they\nexplain a lot of their data. So does your data, like,\nas a rough guide, if I",
    "start": "2180100",
    "end": "2187420"
  },
  {
    "text": "pick 10 components of your\ndata and do PCA on it, do I capture 90%, 95%?",
    "start": "2187420",
    "end": "2192880"
  },
  {
    "text": "Then that means that\nwas a good selection. And you can now compare\nK's based on how",
    "start": "2192880",
    "end": "2197920"
  },
  {
    "text": "their eigenvalues are ordered. If it goes down to your fifth component\nis 10 to the minus 10,",
    "start": "2197920",
    "end": "2203860"
  },
  {
    "text": "you don't need it. Pick the fourth instead. Just pick bases of four. So now it gives you a way to\nactually start to compare them.",
    "start": "2203860",
    "end": "2209680"
  },
  {
    "text": "And you can get\nerror guarantees. You may worry computing these\neigenvalues is super expensive",
    "start": "2209680",
    "end": "2215260"
  },
  {
    "text": "because you have to compute\nlike an SVD on some mega matrix. But you don't have to. And the reason is\nthis is a trace.",
    "start": "2215260",
    "end": "2220839"
  },
  {
    "text": "And if you remember\nyour trace qualities, you can just sum the\nvalues on the diagonal to get the trace, which is\nthe sum of the eigenvalues.",
    "start": "2220840",
    "end": "2228280"
  },
  {
    "text": "Please. So if you get a K that\nwas like too large, and say that this quotient\ncame out to be very, very high,",
    "start": "2228280",
    "end": "2236500"
  },
  {
    "text": "like 0.999-- Right. Would you say that\nyou're overfitting? Yeah, exactly right. It's like it's a thing\nyou're overfitting.",
    "start": "2236500",
    "end": "2242500"
  },
  {
    "text": "You don't need to do it. So it's kind of like\nif you're keeping those extra pieces of\ninformation, intuitively,",
    "start": "2242500",
    "end": "2248440"
  },
  {
    "text": "you want the smallest K that\nyou have most of your data. So you can tack on\nmore, but it's like, what's the additional\nvalue to do it?",
    "start": "2248440",
    "end": "2255100"
  },
  {
    "text": "Traditionally, you\nuse K, as I said, PCA kind of as a\nvisualization technique or to get some rough\nsense of the data.",
    "start": "2255100",
    "end": "2261339"
  },
  {
    "text": "And so you try not to\nhave K higher than 2 or 3. And it just provides\na sanity check. Like, if you run PCA\nand the first two",
    "start": "2261340",
    "end": "2268060"
  },
  {
    "text": "components of your\neigenvalues account for almost nothing in your\ndata, then it's not really clear",
    "start": "2268060",
    "end": "2273880"
  },
  {
    "text": "what conclusions\nyou can draw, right? The other place where\nit can cause you pain is the thing that I\nkeep illustrating,",
    "start": "2273880",
    "end": "2280060"
  },
  {
    "text": "which is that if\nlambda 1 and lambda 2 are equal to each other,\nwhich on real data is actually very unlikely,\nbut numerical issues",
    "start": "2280060",
    "end": "2286540"
  },
  {
    "text": "could make them come\ntogether, then the coordinates could be pretty fragile, right? So I run PCA once.",
    "start": "2286540",
    "end": "2292240"
  },
  {
    "text": "And I get 1, 2, 3 as my-- sorry, alpha 1,\nalpha 2, alpha 3. But then I run it the next time\nand because I chopped at 3,",
    "start": "2292240",
    "end": "2299920"
  },
  {
    "text": "and the first four were\nequal, some new coefficient comes in, right? So really, those are\nthe instabilities",
    "start": "2299920",
    "end": "2305860"
  },
  {
    "text": "that you worry about the most. If your lambdas, or\na bunch of lambdas are really, really\nclose to each other, then your coordinates\naren't really preserved",
    "start": "2305860",
    "end": "2313060"
  },
  {
    "text": "and anything inside\nthe subspace goes. So that's the other\nproblem with PCA. It really only makes sense\nwhen the spectrum is separated.",
    "start": "2313060",
    "end": "2319300"
  },
  {
    "text": "And people don't\nusually check that. And as a result, they lead\nto erroneous conclusions. So you tack on too\nmany you get there.",
    "start": "2319300",
    "end": "2326200"
  },
  {
    "text": "And then also you can have\nthese issues about non-fidelity in the representation.",
    "start": "2326200",
    "end": "2331540"
  },
  {
    "text": "Those are the two main ones. You got it perfectly.",
    "start": "2331540",
    "end": "2333339"
  },
  {
    "text": "OK. Awesome. Any other questions about PCA? Now you know it,\nyou love it so much. [CHUCKLE] All right.",
    "start": "2336700",
    "end": "2342400"
  },
  {
    "text": "Great. All right. Let's talk about ICA.",
    "start": "2342400",
    "end": "2347080"
  },
  {
    "text": "So ICA sounds very\nsimilar to PCA. We only change one letter, but\nit has nothing to do with it.",
    "start": "2349600",
    "end": "2354820"
  },
  {
    "text": "One has nothing to\ndo with the other. So that's refreshing. But they have something about\nthem that I really like.",
    "start": "2354820",
    "end": "2363220"
  },
  {
    "text": "All right. So first, I'm going to\ntell you the high level story of ICA, which is\nthis cocktail party story.",
    "start": "2366460",
    "end": "2371620"
  },
  {
    "text": "High level story. Then some key facts. These key facts are\nuseful because you will",
    "start": "2373360",
    "end": "2378460"
  },
  {
    "text": "run into them in your homework. And I just want\nto highlight them. And then we'll talk\nabout the model.",
    "start": "2378460",
    "end": "2383860"
  },
  {
    "text": "And the model will be the\nleast interesting part. So here's how it works.",
    "start": "2383860",
    "end": "2389619"
  },
  {
    "text": "Here's the high level story. We have people--\nand by the way, I think the homework is the\nworld's most boring cocktail",
    "start": "2389620",
    "end": "2396400"
  },
  {
    "text": "party. I think they count numbers\nto 10, or something. So it's not like you're going\nto hear some salacious gossip.",
    "start": "2396400",
    "end": "2401620"
  },
  {
    "text": "You're just going to hear\npeople counting to 10. Some TA from like\nfive years ago. All right? So anyway, you have people.",
    "start": "2401620",
    "end": "2407980"
  },
  {
    "text": "Here are two people. They're happy. Those are people 1 and 2.",
    "start": "2407980",
    "end": "2413260"
  },
  {
    "text": "We have microphones over\nhere, mic 1, mic 2, OK?",
    "start": "2413260",
    "end": "2418600"
  },
  {
    "text": "Now here's our problem. When the people speak,\nthey don't speak directly into one microphone. They're just\nambient microphones.",
    "start": "2418600",
    "end": "2425020"
  },
  {
    "text": "And so what happens when\nthis person compresses the sound waves is that,\nboom, it hits microphone one.",
    "start": "2425020",
    "end": "2431260"
  },
  {
    "text": "But also it hits microphone two. Similarly, when person two\nspeaks, they speak in some way.",
    "start": "2431260",
    "end": "2439420"
  },
  {
    "text": "This is supposed to be\nthe same wave, by the way. So the wave didn't\nchange based on them, but it just took longer\nto get to the microphone.",
    "start": "2440860",
    "end": "2446500"
  },
  {
    "text": "We're not going to worry\nabout length too much. But the point is, what\nmicrophone one and microphone see is a mixture of the sounds.",
    "start": "2446500",
    "end": "2455440"
  },
  {
    "text": "They don't just hear\none person, they hear two speakers,\nsimultaneously, superimposed on one another. And the goal is to take what\nwe record at these microphones",
    "start": "2455440",
    "end": "2463000"
  },
  {
    "text": "and recover kind of the time\nseries of what they actually said. And since if we had the\nwave, that's just the air",
    "start": "2463000",
    "end": "2468880"
  },
  {
    "text": "we could play it\nthrough a speaker and we would actually hear what\nperson one and person two said. And you'll be able to do this.",
    "start": "2468880",
    "end": "2475000"
  },
  {
    "text": "And it works, which\nis kind of wild. Now I do want to emphasize\nhaving built things that look sort of like\nthis, is a naive way",
    "start": "2475000",
    "end": "2481480"
  },
  {
    "text": "to look at the problem\nin the sense of what you would build if you were\ndoing this in industry, but so what? It gets you the core principles.",
    "start": "2481480",
    "end": "2486819"
  },
  {
    "text": "And you can look,\nthere's whole things about speaker identification. And in fact, there\nare certain companies",
    "start": "2486820",
    "end": "2492040"
  },
  {
    "text": "that when they ship\ntheir products, they brag about how\nthey can identify different speakers in the home. So as weird as this is,\npeople ship products",
    "start": "2492040",
    "end": "2498579"
  },
  {
    "text": "based on their\nability to do this. OK? All right. Please ask questions if the\nsetup doesn't make sense.",
    "start": "2498580",
    "end": "2507400"
  },
  {
    "text": "We have data X. And we'll\nsee it for some time. We're not going\nto assume things. We'll make this simple. The people aren't moving\naround in the room.",
    "start": "2509140",
    "end": "2515500"
  },
  {
    "text": "They're not changing. Just to make our\nlives a little easier. So we need to look at what\nis the actual data look like?",
    "start": "2516580",
    "end": "2523240"
  },
  {
    "text": "So as I mentioned,\nspeaker one is",
    "start": "2524260",
    "end": "2530020"
  },
  {
    "text": "going to be this time\nseries that looks like this. OK? Now we don't actually see\nthe whole continuous thing.",
    "start": "2530020",
    "end": "2538180"
  },
  {
    "text": "What we see is we\nsee speaker one. We don't even see\nthis, by the way. This is-- we get samples at\nvarious regular intervals.",
    "start": "2538900",
    "end": "2547060"
  },
  {
    "text": "That's the way we\nconceptualize the problem. And this is like how audio\nprocessing works, right?",
    "start": "2547060",
    "end": "2551320"
  },
  {
    "text": "Two at time 1. Let me just draw this\nand then I'll talk.",
    "start": "2553660",
    "end": "2557800"
  },
  {
    "text": "These should be evenly spaced,\nif I could draw properly and had enough patience.",
    "start": "2559540",
    "end": "2564160"
  },
  {
    "text": "The point is, is what you\nsee is that you just get these measures of intensity. So STJ is speaker J,\nintensity at time t.",
    "start": "2566620",
    "end": "2578500"
  },
  {
    "text": "OK. Now we want to recover this. If we had this,\nwe're in good shape. If you think about how\nyou actually record audio,",
    "start": "2580180",
    "end": "2586540"
  },
  {
    "text": "kind of high end audio is\nusually recorded around 44 kilohertz, give or take. You can probably understand\npeople at much lower kilohertz.",
    "start": "2587440",
    "end": "2594580"
  },
  {
    "text": "I don't know exactly\nwhere it breaks down. But my point is,\nthis is actually how digital recording works.",
    "start": "2594580",
    "end": "2601060"
  },
  {
    "text": "You sample it a bunch of\npoints, you get the intensities. And then you play it\nback through a speaker.",
    "start": "2601060",
    "end": "2605320"
  },
  {
    "text": "All right. And then there speaker\ntwo, which we also draw.",
    "start": "2607360",
    "end": "2610060"
  },
  {
    "text": "And then just so it's\nclear, we sample them at a bunch of points too. Maybe speaker 2, more\nloquacious, I don't know.",
    "start": "2613960",
    "end": "2621579"
  },
  {
    "text": "S, 1, 2. Oops, those look terrible. I don't know why I'm going\nto fix this, but I am.",
    "start": "2622960",
    "end": "2629440"
  },
  {
    "text": "And these are sampled\nat the same time points. So my drawing doesn't do-- is imperfect in many, many ways.",
    "start": "2631840",
    "end": "2637000"
  },
  {
    "text": "S, T2, so on. Blah, blah, blah. And these are going to be\nsampled at the same time",
    "start": "2639640",
    "end": "2645940"
  },
  {
    "text": "points, just to make\nour lives easier. So now we don't get to\nsee this, as I mentioned.",
    "start": "2645940",
    "end": "2652240"
  },
  {
    "text": "We don't see S1 and\nS2's time series.",
    "start": "2655000",
    "end": "2661840"
  },
  {
    "text": "We get to see the microphones. Only XT1 and XT2, which are\nsampled at the microphones.",
    "start": "2665860",
    "end": "2675400"
  },
  {
    "text": "So far, so good? So we need a model of how\nwe're going to do this. And of course, our model\nfrom what we described above",
    "start": "2683740",
    "end": "2691720"
  },
  {
    "text": "is going to look\nsomething like this. What we observe at\nmicrophone j, at time,",
    "start": "2691720",
    "end": "2699279"
  },
  {
    "text": "T, is a mixture of\nsomething from speaker 1",
    "start": "2699280",
    "end": "2704860"
  },
  {
    "text": "and something from speaker 2. OK? So microphone j just to\nmake sure it's clear here,",
    "start": "2707680",
    "end": "2715780"
  },
  {
    "text": "Mic j sees a mixture. And we're going to assume\nthis mixture is fixed just",
    "start": "2715780",
    "end": "2725020"
  },
  {
    "text": "for the moment, right? If they're moving\naround the room, that's not true any longer. But we're going to assume\nit's fixed for the moment.",
    "start": "2725020",
    "end": "2731980"
  },
  {
    "text": "OK? So I can write this compactly\nas, X of t equals A of S of t,",
    "start": "2732940",
    "end": "2739780"
  },
  {
    "text": "right? Now what do we know here? This, we observe.",
    "start": "2741940",
    "end": "2747220"
  },
  {
    "text": "This is the data.",
    "start": "2749140",
    "end": "2750160"
  },
  {
    "text": "Both of these are latent.",
    "start": "2757960",
    "end": "2759400"
  },
  {
    "text": "We don't know the mixture. We don't know the\nspeaker intensities.",
    "start": "2764380",
    "end": "2769000"
  },
  {
    "text": "OK? For the moment,\nI'm going to assume that the number of speakers\nand the number of microphones are the same.",
    "start": "2769900",
    "end": "2775599"
  },
  {
    "text": "You can imagine because\nthere's a matrix here, that if I have only\none microphone, this is going to be\nsubstantially harder, actually",
    "start": "2776380",
    "end": "2782380"
  },
  {
    "text": "impossible, to do\nthe reconstruction. But I'm going to take\nadvantage of the fact that I have different\nmixtures at these microphones.",
    "start": "2782380",
    "end": "2788080"
  },
  {
    "text": "OK? So assume I have a number\nof microphones the same as the number of speakers.",
    "start": "2788080",
    "end": "2792040"
  },
  {
    "text": "OK? So is the set up,\nthe high level story and set up clear, intuitively,\nwhat should happen? All right.",
    "start": "2793120",
    "end": "2798220"
  },
  {
    "text": "Let me write some math. [INAUDIBLE] Sure. What would A [INAUDIBLE] do?",
    "start": "2798220",
    "end": "2804370"
  },
  {
    "text": "A is actually the mixture. So A, right here, this model\nsays that what we see at time t",
    "start": "2804370",
    "end": "2809619"
  },
  {
    "text": "from at microphone\nj is some fixed mixture of what speaker",
    "start": "2809620",
    "end": "2813640"
  },
  {
    "text": "and speaker 2 said at time t. We're not modeling any\ndelay or anything like that. So they make the sound,\nthey go, \"ah\", and then",
    "start": "2814660",
    "end": "2820420"
  },
  {
    "text": "it hits the microphone. And then the mixture of\nperson one and person two's",
    "start": "2820420",
    "end": "2825700"
  },
  {
    "text": "pressure hits the\nmicrophone at the same time. And what do we [INAUDIBLE]?",
    "start": "2825700",
    "end": "2832090"
  },
  {
    "text": "They don't actually\nmatter for now. The physical units\ndon't matter in any way. But you can think about\nthem as any unit of pressure",
    "start": "2832090",
    "end": "2837220"
  },
  {
    "text": "that you want. A is unit-less. A is just a pure mixture. You can multiply and add\nthings because the S's",
    "start": "2837220",
    "end": "2843460"
  },
  {
    "text": "are the same type. Cool. All right. So let's make this a little bit\nmore mathematically precise.",
    "start": "2843460",
    "end": "2849460"
  },
  {
    "text": "So we're given X1,\nX, n, element of Rd.",
    "start": "2850120",
    "end": "2860740"
  },
  {
    "text": "And d is the number\nof mics and speakers.",
    "start": "2862660",
    "end": "2864880"
  },
  {
    "text": "What we have to do\nis find S1 to Sn",
    "start": "2871360",
    "end": "2878680"
  },
  {
    "text": "that's also an element of Rd. So I'm preferring to have\nthe notation over time.",
    "start": "2878680",
    "end": "2884260"
  },
  {
    "text": "We also are going to\nfind, although we really don't need it for the model,\nthis A that is D by D. Such",
    "start": "2885040",
    "end": "2894220"
  },
  {
    "text": "that, x of t equals AS of t. Now if we estimated\nA, right, there's",
    "start": "2894220",
    "end": "2901839"
  },
  {
    "text": "a pretty easy way to\nfind what we wanted. If we knew A, someone gave it\nto us, this problem is trivial.",
    "start": "2901840",
    "end": "2907000"
  },
  {
    "text": "Just take the X's,\nmultiply by A inverse, and you have yourself\nthe S's, right?",
    "start": "2907000",
    "end": "2912099"
  },
  {
    "text": "Now the terminology is we\ncall A the mixing matrix",
    "start": "2913900",
    "end": "2919359"
  },
  {
    "text": "for the reasons I just outlined. A, the mixing matrix.",
    "start": "2919360",
    "end": "2925240"
  },
  {
    "text": "And W equals A inverse, which\nwill use the un-mixing matrix. So why would I introduce\nand bother you with this?",
    "start": "2925240",
    "end": "2932260"
  },
  {
    "text": "It turns out that W is\nactually the right way to write a lot of\nthe guarantees. And you'll see why in a second.",
    "start": "2932260",
    "end": "2937060"
  },
  {
    "text": "OK? So we're going to write this\nas W is equal to W1 transpose",
    "start": "2938260",
    "end": "2947200"
  },
  {
    "text": "to Wd transpose. And I'm just doing this so\nthat I can write the following. So that-- this is just one\nway of writing the inverse--",
    "start": "2947200",
    "end": "2955120"
  },
  {
    "text": "so that Sjt equals Wj times Xt. Nothing happened here.",
    "start": "2955120",
    "end": "2961359"
  },
  {
    "text": "I'm just giving you notation\nof how I think about the mixing and then this inverse is\ngoing to be important. The inverse is kind\nof obviously important",
    "start": "2961360",
    "end": "2967780"
  },
  {
    "text": "because we want\nit-- if we the A, we would multiply by its\ninverse on both sides and that would tell us the\nspeaker that we were after.",
    "start": "2967780",
    "end": "2974440"
  },
  {
    "text": "So W is what we\nneed to multiply by.",
    "start": "2975340",
    "end": "2977620"
  },
  {
    "text": "Now the things that I\nactually find interesting about this model. So some caveats, we talked\nabout some of these.",
    "start": "2981220",
    "end": "2988060"
  },
  {
    "text": "A does not vary with time. We're assuming that.",
    "start": "2991060",
    "end": "2995560"
  },
  {
    "text": "If it did, we'd have to use\nsomething more complicated.",
    "start": "2998440",
    "end": "3001140"
  },
  {
    "text": "So we're not going to do that. Two, this is more\ninteresting to me. There is inherent\nambiguity in this model.",
    "start": "3003720",
    "end": "3010140"
  },
  {
    "text": "I like when there's\ninherent ambiguity when you can't tell two\nthings apart because it forces you to understand\nwhat the model is doing.",
    "start": "3011280",
    "end": "3017040"
  },
  {
    "text": "So one thing. We can't determine speaker\none versus speaker two.",
    "start": "3017880",
    "end": "3021779"
  },
  {
    "text": "We have no idea who\nspeaker one and who speaker two in real life. So we can only\ndetermine up to how",
    "start": "3023580",
    "end": "3028800"
  },
  {
    "text": "we promote their time series. So speaker ID is opaque to us. Maybe one time we\nrun the algorithm",
    "start": "3028800",
    "end": "3035220"
  },
  {
    "text": "are indistinguishable. We don't know the labels.",
    "start": "3039600",
    "end": "3041580"
  },
  {
    "text": "Of course, we can\ntell that there's one person who is saying\nnumbers in English and one who's saying\nnumbers in French.",
    "start": "3046740",
    "end": "3051780"
  },
  {
    "text": "We just can't tell who's\ndoing what in this model. The other thing, which is\nmaybe a little bit more subtle",
    "start": "3051780",
    "end": "3057540"
  },
  {
    "text": "is, we can't determine\nabsolute intensity. And I'll just\nwrite the equation.",
    "start": "3057540",
    "end": "3061380"
  },
  {
    "text": "And it's because we're\nmultiplying two things together. So notice that if\nI take any constant and multiply it times A and\nthen take that same constant",
    "start": "3065100",
    "end": "3072360"
  },
  {
    "text": "and multiply it times S, this\nis still equal to A of S of t.",
    "start": "3072360",
    "end": "3076740"
  },
  {
    "text": "So we can possibly only know\nthat the person-- we can't tell how loud S1 and S2 are. We can tell relatively\nhow loud they are.",
    "start": "3077700",
    "end": "3083640"
  },
  {
    "text": "But the mixing matrix, we\ncan multiply by a constant and it wouldn't change anything.",
    "start": "3083640",
    "end": "3089339"
  },
  {
    "text": "The scaling would go through. And because we're multiplying\nthem in our framework, we can't determine this either.",
    "start": "3090060",
    "end": "3095820"
  },
  {
    "text": "Now that has a surprising,\nsurprising thing.",
    "start": "3097080",
    "end": "3100860"
  },
  {
    "text": "And this is kind of why I like\nto teach it, intellectually. Surprising.",
    "start": "3102780",
    "end": "3106440"
  },
  {
    "text": "The speakers cannot be drawn\nfrom a Gaussian distribution. We're going to have to make\nsome statistical assumption, but they cannot be Gaussian.",
    "start": "3110160",
    "end": "3116640"
  },
  {
    "text": "Why? Suppose they were. Xi. would then be a normal drawn\nwith some mean from AAt.",
    "start": "3122160",
    "end": "3130859"
  },
  {
    "text": "But then as we saw\nbefore, if UTU equals I,",
    "start": "3132540",
    "end": "3137400"
  },
  {
    "text": "then AU generates the same data.",
    "start": "3138300",
    "end": "3143400"
  },
  {
    "text": "What does that mean? It means that any\nrotation of A generates the same data, multiplied by\nU. And the reason it happens",
    "start": "3145980",
    "end": "3154799"
  },
  {
    "text": "is because the data\nhere are rotationally invariant because their\ncovariance matrix is",
    "start": "3154800",
    "end": "3161700"
  },
  {
    "text": "A times A transpose. And that is not\nsensitive enough to tell about all these rotations. The same reason\nwe loved Gaussians",
    "start": "3161700",
    "end": "3168000"
  },
  {
    "text": "because they were rotationally\ninvariant means that symmetry, we can't recover\nanything in this problem.",
    "start": "3168000",
    "end": "3173040"
  },
  {
    "text": "And so at that\npoint, you may think, \"gosh, there's no way we're\ngoing to be able to do this. We have all these symmetries\naround the speaker",
    "start": "3174360",
    "end": "3179700"
  },
  {
    "text": "and we can rotate the\nintensities in any way we want. What are we going\nto be able to do?\"",
    "start": "3179700",
    "end": "3185460"
  },
  {
    "text": "And it turns out, you can\nrecover something here. And weirdly enough, as long\nas the distribution, roughly",
    "start": "3185460",
    "end": "3193260"
  },
  {
    "text": "speaking, is not Gaussian and\nis not rotationally invariant, you can recover it. And that's kind of remarkable\nand it's worth thinking about.",
    "start": "3193260",
    "end": "3199920"
  },
  {
    "text": "OK? Now the algorithm is\ngoing to be so trivial. The algorithm is just going to\nbe gradient descent and MLE.",
    "start": "3200640",
    "end": "3208440"
  },
  {
    "text": "We're just going to set\nup a likelihood function and run everything that\nwe've been running so far.",
    "start": "3211680",
    "end": "3215940"
  },
  {
    "text": "That's it. Now we need one trick, one\nhalf second, two minute",
    "start": "3216840",
    "end": "3225660"
  },
  {
    "text": "detour because this\ncauses problems every time people look at this.",
    "start": "3225660",
    "end": "3230339"
  },
  {
    "text": "It's just one little\ndetour about how random variables behave\nunder linear transformations.",
    "start": "3231120",
    "end": "3237360"
  },
  {
    "text": "Under linear transform.",
    "start": "3241320",
    "end": "3244440"
  },
  {
    "text": "All right. And this is just--\nthe reason I'm doing this is it's a key confusion. If you remember you're--",
    "start": "3247320",
    "end": "3254280"
  },
  {
    "text": "I don't know-- I'm not going to\nsay what you should remember, some calculus thing, just basic\nchange of variable formulas",
    "start": "3254280",
    "end": "3259920"
  },
  {
    "text": "for integrals, this\nwill make sense. But we can draw it\nin pictures and you don't need to know that at all. So here we go. So just imagine I have something\nthat's uniform on 0, 1.",
    "start": "3259920",
    "end": "3267000"
  },
  {
    "text": "And now I have a\nnew variable, U, which is equal to is the PDF  ",
    "start": "3268140",
    "end": "3273360"
  },
  {
    "text": "of U in terms of S? Now we're tempted  ",
    "start": "3277680",
    "end": "3282299"
  },
  {
    "text": "to write P of\nU, X over 2 equals P of S of X.",
    "start": "3286800",
    "end": "3297240"
  },
  {
    "text": "Now, let's take a\nlook at the PDFs.",
    "start": "3301140",
    "end": "3304260"
  },
  {
    "text": "So here's the PDF of\nU.  ",
    "start": "3306240",
    "end": "3308160"
  },
  {
    "text": "It goes from 0 to 1. And what's its height? Well, we integrate over the\nentire thing, it's 1, right?",
    "start": "3313980",
    "end": "3321960"
  },
  {
    "text": "This is our PDF of U-- sorry, of S. Sorry, PDF of\nS. So this is the uniform.",
    "start": "3322500",
    "end": "3330780"
  },
  {
    "text": "Now for U, what happens?",
    "start": "3333720",
    "end": "3339180"
  },
  {
    "text": "We go from 0, here's 1, to 2. We know we have\nsupport from 0 to 2",
    "start": "3339180",
    "end": "3344400"
  },
  {
    "text": "because it's a uniform\ndistribution, right? You grab a point here,\nyou multiply it by 2 and it's going to sit\nsomewhere in here.",
    "start": "3344400",
    "end": "3350520"
  },
  {
    "text": "It's going to sit\nsomewhere in here. But what is its height?",
    "start": "3352920",
    "end": "3356160"
  },
  {
    "text": "It's got to be 1/2. So this relationship is clear.",
    "start": "3358680",
    "end": "3363120"
  },
  {
    "text": "So PS of X is going to be equal\nto 1 if X element of 0, 1,",
    "start": "3365640",
    "end": "3370980"
  },
  {
    "text": "it's 0 otherwise. PU of X is going to be equal\nto PS of X over 2 times 1/2.",
    "start": "3371820",
    "end": "3383889"
  },
  {
    "text": "And that's just the\nnormalizing constant. So this key issue here is\nthis normalizing constant.",
    "start": "3386820",
    "end": "3391500"
  },
  {
    "text": "Normalizing constant. That's it. So when we do this in higher\ndimensions, we have PU of X",
    "start": "3394020",
    "end": "3405000"
  },
  {
    "text": "and we want to map\nby some linear, A.",
    "start": "3405660",
    "end": "3408420"
  },
  {
    "text": "So that is like AU, for example. What happens? I'm sorry-- AS, that's the way\nwe're writing this because I",
    "start": "3414120",
    "end": "3425220"
  },
  {
    "text": "want to keep the notation. AS equals U. What happens\nin higher dimensions?",
    "start": "3425220",
    "end": "3429720"
  },
  {
    "text": "Well, we still have PU of X\nis equal to PS of A inverse X.",
    "start": "3430380",
    "end": "3436259"
  },
  {
    "text": "But we need a normalizing\nconstant here. And how does it take-- if\nyou imagine, I'm taking a box",
    "start": "3436260",
    "end": "3441599"
  },
  {
    "text": "and I multiply it by A, a matrix\nA, how does the volume change?",
    "start": "3442380",
    "end": "3448559"
  },
  {
    "text": "That's the determinant. That's all the determinant does. It take a box and\nthe volume of it",
    "start": "3451200",
    "end": "3456240"
  },
  {
    "text": "is going to be\nexactly proportional to the assigned-- the absolute\nvalue of the determinate. The determinate is\nsigned because it's",
    "start": "3456240",
    "end": "3461940"
  },
  {
    "text": "a oriented measure, but\nthat's what you get. You can convince yourself in\ntwo dimensions pretty easily.",
    "start": "3461940",
    "end": "3468900"
  },
  {
    "text": "In higher dimensions,\nit actually requires a little bit of\nwork potentially to do it. And this you probably\nlearned as your change",
    "start": "3468900",
    "end": "3475560"
  },
  {
    "text": "of integrals-- change of\nvolume integrals formula at some point.",
    "start": "3475560",
    "end": "3480180"
  },
  {
    "text": "Times the determinant of W. Now the thing that I\nused here was the fact",
    "start": "3481140",
    "end": "3489840"
  },
  {
    "text": "that 1 over the\ndeterminant of A is equal to the determinant of\nA inverse when I did that.",
    "start": "3489840",
    "end": "3495420"
  },
  {
    "text": "OK? So the point is, when you\ndo change your variables, you have to take this\ndeterminant into account. You probably did\nthis with a Jacobian",
    "start": "3496080",
    "end": "3502140"
  },
  {
    "text": "at some point in your life. And if you didn't,\ndon't worry about it. It's not that big a deal. It just says if I take\na box and I map it",
    "start": "3502140",
    "end": "3508740"
  },
  {
    "text": "by a bilinear transformation,\nwhat's the volume of the box? Going back to this uniform case. And then if you care about how\nyou would probably prove this,",
    "start": "3508740",
    "end": "3514740"
  },
  {
    "text": "you just think about integrals. You break it into\ntiny little boxes. That's it.",
    "start": "3514740",
    "end": "3519119"
  },
  {
    "text": "Whatever space\nyou're integrating. We did it for one box, but\nyou can do it for many. OK.",
    "start": "3519900",
    "end": "3525360"
  },
  {
    "text": "So we're going to\nuse this formula for the rest of our time. Please. [INAUDIBLE] Q is equal to PSX--",
    "start": "3525360",
    "end": "3533032"
  },
  {
    "text": "PSX. And how does this   actually\nrelate to U equals 2S",
    "start": "3533032",
    "end": "3543094"
  },
  {
    "text": "because are we using U\nand S as a proctor, and-- Yeah. So the idea here is that\nwe have a probability distribution in one space.",
    "start": "3543094",
    "end": "3548820"
  },
  {
    "text": "Now I want a probability\ndistribution in my new space. So I have from 0 to 2. So I'm going to take the X and\nI'm going to divide it by 2.",
    "start": "3548820",
    "end": "3555059"
  },
  {
    "text": "And whatever the height is over\nthere, I'm going to get it. So if the height is 1, I get it. The height is 0, then that\nshould be right as well.",
    "start": "3555060",
    "end": "3560520"
  },
  {
    "text": "But when I do that, the\nproblem is the probability distribution I had,\nI'm using that value, will be 1 when I do that.",
    "start": "3560520",
    "end": "3567060"
  },
  {
    "text": "And I need to just\nmultiply by some constant. So I'm thinking here about the\nPDF for that random variable.",
    "start": "3567060",
    "end": "3572760"
  },
  {
    "text": "So basically, U and S\nhere is a [INAUDIBLE] of random variable [INAUDIBLE]. It's a random variable.",
    "start": "3573780",
    "end": "3579720"
  },
  {
    "text": "So a random variable\nis nothing more than like some function on the-- it's just an integral. OK.",
    "start": "3579720",
    "end": "3585180"
  },
  {
    "text": "Awesome question. Cool. So once we have this fact,\nthis problem is super easy.",
    "start": "3585180",
    "end": "3591060"
  },
  {
    "text": "ICA is MLE. Why is that?",
    "start": "3595140",
    "end": "3599880"
  },
  {
    "text": "P of S equals sum j equals",
    "start": "3600960",
    "end": "3603900"
  },
  {
    "text": "This is where we use the\nsources are independent. We have to assume\nthis in the model.",
    "start": "3607560",
    "end": "3612720"
  },
  {
    "text": "And they have some distribution\nthat's not Gaussian, but not Gaussian.",
    "start": "3614580",
    "end": "3618299"
  },
  {
    "text": "This equals-- so then P of X is\nnow equal to the probability,",
    "start": "3623580",
    "end": "3629100"
  },
  {
    "text": "J goes from 1 to D of PS W times\nX times the determinant of W.",
    "start": "3629100",
    "end": "3639420"
  },
  {
    "text": "But now this is something\nthat we can compute. This is written in terms\nof X and our matrix, A.",
    "start": "3644580",
    "end": "3651420"
  },
  {
    "text": "And we can just do\ngradient descent on it.",
    "start": "3652680",
    "end": "3654240"
  },
  {
    "text": "So how does that work? Here's the key technical bit.",
    "start": "3659280",
    "end": "3661860"
  },
  {
    "text": "We're going to set P sub\nX proportional to G prime",
    "start": "3669420",
    "end": "3675780"
  },
  {
    "text": "X for G of X equal to 1 plus\nE to the minus X inverse.",
    "start": "3675780",
    "end": "3682380"
  },
  {
    "text": "There's nothing really\nmagical about this function except for it's not\nrotationally invariant.",
    "start": "3682380",
    "end": "3686940"
  },
  {
    "text": "And then we can solve\nthe likelihood of W",
    "start": "3687960",
    "end": "3693720"
  },
  {
    "text": "is sum from time goes\nfrom 1 to N, sum,",
    "start": "3693720",
    "end": "3698760"
  },
  {
    "text": "j goes from 1 to\nD, log G plus Wj",
    "start": "3698760",
    "end": "3707520"
  },
  {
    "text": "Xt plus log determinant of W.",
    "start": "3708180",
    "end": "3714000"
  },
  {
    "text": "So maybe this looks\npretty intimidating, but what happened here? This G is just some\nlikelihood function.",
    "start": "3716820",
    "end": "3722880"
  },
  {
    "text": "I don't actually\ncare what it is. That's the thing that's weird. That's the thing I\nwant you to confront. It's not that it's some\nspecially chosen function.",
    "start": "3722880",
    "end": "3730440"
  },
  {
    "text": "When we would pick\nthe Gaussians, we were picking it because\nof computational and other reasons. Oh, it only had two\nmoments and we could",
    "start": "3730440",
    "end": "3736800"
  },
  {
    "text": "compute everything we wanted. This is basically\nsaying, honestly, I can pick almost any G I want.",
    "start": "3736800",
    "end": "3742200"
  },
  {
    "text": "Any likelihood function\nI want on the speakers. As long as it's not\nrotationally-- as long as the measure isn't\nrotation invariant,",
    "start": "3742200",
    "end": "3749340"
  },
  {
    "text": "there will be a unique solution\nif I look at enough data. That's kind of fun. That's kind of an interesting\nthing to think about.",
    "start": "3749340",
    "end": "3755760"
  },
  {
    "text": "And you say, \"what G do I pick?\" Well, I'm going to pick this\none because I know it's not rotationally invariant. People pick other ones.",
    "start": "3755760",
    "end": "3762180"
  },
  {
    "text": "How do you pick a measure? Well, you pick the\nPDF proportional to something that\nlooks like a CDF.",
    "start": "3762900",
    "end": "3769200"
  },
  {
    "text": "What does this\nfunction look like? It's just the sigmoid function. So I picked the\nprobability distribution.",
    "start": "3769200",
    "end": "3774840"
  },
  {
    "text": "So it's kind of low to high. It's not rotationally\ninvariant on each component. And I'm done.",
    "start": "3774840",
    "end": "3780140"
  },
  {
    "text": "And that's kind of wild, OK? I'm not super worried that\nyou grok absolutely everything",
    "start": "3780140",
    "end": "3786660"
  },
  {
    "text": "here. Again, these lectures\nare not supposed to walk you through line by\nline and read the book to you. What it's supposed to do is\ngive you a high level structure",
    "start": "3786660",
    "end": "3792900"
  },
  {
    "text": "for how this algorithm\nis going to go and what are the key twists. Now the one thing that's\nprobably scary to you",
    "start": "3792900",
    "end": "3798780"
  },
  {
    "text": "is you're like,\n\"well, I don't know how to compute the gradient\nof the log of a determinant.\" And weirdly enough, that\nactually has a form.",
    "start": "3798780",
    "end": "3803940"
  },
  {
    "text": "So that is actually--\nthat's an object that you can compute and\ncompute gradient descent on.",
    "start": "3804900",
    "end": "3810300"
  },
  {
    "text": "So you just run\ngradient descent on W. You have to do some\nderivatives in the old days.",
    "start": "3810300",
    "end": "3816180"
  },
  {
    "text": "I guess now you don't have to\ncompute derivatives anymore, you have autodiff\nsoftware for you, which will kind of\ndo it automatically. Like, PyTorch will do this\nfor you or JAX or something.",
    "start": "3816180",
    "end": "3823500"
  },
  {
    "text": "But maybe we make\nyou do it by hand. I don't know if we\nactually do that. But if we do, you can do it. It's not bad. You look it up or you derive it.",
    "start": "3823500",
    "end": "3830580"
  },
  {
    "text": "It's not super hard. But it is weird\nthat it works, OK? And miraculously, I'm not\ngoing to have to convince you",
    "start": "3830580",
    "end": "3837119"
  },
  {
    "text": "of this in class. You run the thing and you\nwill hear someone saying, \"one, two, three,\nfour, five,\" some TA.",
    "start": "3837120",
    "end": "3843540"
  },
  {
    "text": "And it will work, even though\nwhat's at the microphones is a mix. It's pretty wild.",
    "start": "3843540",
    "end": "3848400"
  },
  {
    "text": "All right. Well, good. Oh, by the way, so what is\nthe log of the absolute value",
    "start": "3850440",
    "end": "3856560"
  },
  {
    "text": "the determinant? It's the determinant is the\nproduct of all the eigenvalues.",
    "start": "3856560",
    "end": "3861600"
  },
  {
    "text": "So if you take their absolute\nvalue and take their log, it's going to be\nthe absolute values of the sums of the eigenvalues.",
    "start": "3862680",
    "end": "3868080"
  },
  {
    "text": "Now that looks a\nlittle trace-like. So you can imagine\nwhy this is actually relatively easy to compute. But your W is small.",
    "start": "3868080",
    "end": "3873660"
  },
  {
    "text": "You can brute force it\nfor these kinds of things. OK. Awesome.",
    "start": "3873660",
    "end": "3877920"
  },
  {
    "text": "Any questions about this? Please. [INAUDIBLE] Can you just repeat\nwhat is the piece of--",
    "start": "3878940",
    "end": "3884037"
  },
  {
    "text": "Oh, yeah, let's-- [INAUDIBLE] This piece right here?",
    "start": "3884037",
    "end": "3892860"
  },
  {
    "text": "Yeah, so what we're\ndoing is we have P of the speakers,\neach one of them, some probability distribution. I'm being vague\nat this point when",
    "start": "3892860",
    "end": "3899400"
  },
  {
    "text": "I was writing it\nbecause I wanted to basically get to the point. We can use any distribution. That's the weird thing. Basically, any distribution\nso long as it's",
    "start": "3899400",
    "end": "3905820"
  },
  {
    "text": "not rotationally invariant. So I don't know what PS is\nyet, but just imagine it. Now I'm going to move it\nto a distribution from SJ",
    "start": "3905820",
    "end": "3912660"
  },
  {
    "text": "to a distribution on X's. So the speakers,\nI can't observe. I don't get to measure them,\nbut I do get to measure the X's.",
    "start": "3912660",
    "end": "3918600"
  },
  {
    "text": "Now I only have one variable\nthat controls everything, that's W. Now I can do\ngradient descent on W.",
    "start": "3918600",
    "end": "3924599"
  },
  {
    "text": "I didn't know how to\ndo it on their product before, of setting S\nand W as a product, but because there's\nonly one now,",
    "start": "3924600",
    "end": "3930059"
  },
  {
    "text": "I can run gradient\ndescent on it. And it happens to be nicely\nconcave and all the rest of the stuff I want.",
    "start": "3930060",
    "end": "3935340"
  },
  {
    "text": "So you went from\none distribution to another using\nthis data, we'll call [INAUDIBLE] using\ndata [INAUDIBLE]..",
    "start": "3935340",
    "end": "3941340"
  },
  {
    "text": "Exactly right. And the thing that's weird here\nthat I want you to confront is-- and when you do\nthe assignment is, like, if you use the\nstandard distributions,",
    "start": "3941340",
    "end": "3948359"
  },
  {
    "text": "you don't get a unique answer. And weirdly enough, almost any\ndistribution you use will work. We happen to pick this\none, but you could",
    "start": "3949080",
    "end": "3956100"
  },
  {
    "text": "have picked something else. And that's kind of wild. So that's a weird thing.",
    "start": "3956100",
    "end": "3961380"
  },
  {
    "text": "Just have to be able\nto distinguish it. And what you think about there\nis, the prior on the speaker doesn't matter too much.",
    "start": "3962100",
    "end": "3967620"
  },
  {
    "text": "They just can't mix\nin some awkward way. You can't rotate the\nspeaker intensities",
    "start": "3968760",
    "end": "3974400"
  },
  {
    "text": "and have it make sense. You have to be able to\ndistinguish up versus down.",
    "start": "3974400",
    "end": "3978119"
  },
  {
    "text": "Fantastic. Other questions?",
    "start": "3979740",
    "end": "3981360"
  },
  {
    "text": "So I'm going to\nspend-- oh, please. [INAUDIBLE] About this G, yeah,\nlike, how exactly",
    "start": "3984900",
    "end": "3990450"
  },
  {
    "text": "do you get-- like, how\ndo you get the G of X? This one is directly\nproportional-- Yeah, you don't-- so I can\ntake any function I want",
    "start": "3990450",
    "end": "3997380"
  },
  {
    "text": "and normalize it as long\nas it has some variation that's smooth and other things,\nand normalize it from like -1",
    "start": "3997380",
    "end": "4003320"
  },
  {
    "text": "to 1, or minus\ninfinity to infinity, as long as it's integrable\nand make it into a probability distribution, as long\nas it's positive.",
    "start": "4003320",
    "end": "4008599"
  },
  {
    "text": "Positive, this\nfunction is positive. I want it to go-- I want it to go from\nlow to high so it",
    "start": "4008600",
    "end": "4013760"
  },
  {
    "text": "has that nice\nfeature to it and I don't care about anything else. But now that's the\nCDF because now I",
    "start": "4013760",
    "end": "4018980"
  },
  {
    "text": "have a function which the\nintegral of which is 0 to 1. And then what I'm\ngoing to do is I'm going to take a differential and\nthat's what gives me the PDF.",
    "start": "4018980",
    "end": "4025400"
  },
  {
    "text": "There's this probability\ntheory stuff. But yeah. And you're like-- the thing\nthat should bother you and is bothering you is why\ndid you pick this function?",
    "start": "4026060",
    "end": "4032660"
  },
  {
    "text": "And then I'm telling\nyou, that's disturbing. Go pick other\nfunctions, still works. But there's one\nfunction that if we",
    "start": "4032660",
    "end": "4037940"
  },
  {
    "text": "use, which we've been\nusing the whole quarter, all of a sudden\neverything breaks. That's weird. It just means that not\nevery prior matters.",
    "start": "4037940",
    "end": "4045020"
  },
  {
    "text": "Sometimes priors get these\nundefinability things. And it's coming from this\nrotational invariance.",
    "start": "4045020",
    "end": "4050900"
  },
  {
    "text": "And that's weird. It's going to bother you,\nbut then you run the code and you're like, \"oh, my God. It works.\" And you can play with it. See how close you can make it\nto rotation in the invariant,",
    "start": "4050900",
    "end": "4057200"
  },
  {
    "text": "if you're motivated. Or you just do the\nhomework and turn it in. Either way. But that's what's interesting.",
    "start": "4057200",
    "end": "4062000"
  },
  {
    "text": "Cool. I'm going to use our\nlast little bit of time together to tell you\nsomething kind of different, which was sometimes\nabout weak supervision.",
    "start": "4063320",
    "end": "4069560"
  },
  {
    "text": "This would be less\nmathematical and potentially, have more bizarre pictures.",
    "start": "4069560",
    "end": "4074780"
  },
  {
    "text": "We didn't get any\nactivity on the thread. I want to tell you\nabout one trend since this is our\nlast lecture together. Tengyu will take over.",
    "start": "4075800",
    "end": "4082820"
  },
  {
    "text": "And we'll see if we\nget through this. All right. OK.",
    "start": "4085760",
    "end": "4090200"
  },
  {
    "text": "So I want to tell you\nabout weak supervision. And I want to just motivate it\nfor you in the last 20 minutes and why we care about it.",
    "start": "4091100",
    "end": "4097279"
  },
  {
    "text": "And mathematically, the\nreason we care about it is, it's an example-- the underlying model\nthat will show you is an example of\nthe EM style models,",
    "start": "4097280",
    "end": "4103819"
  },
  {
    "text": "but we can solve it exactly\nin a lot of situations. These are my slides and I\nhave to give these keynote--",
    "start": "4103820",
    "end": "4109429"
  },
  {
    "text": "I had to give a\nkeynote this morning, I have two more this week,\non random conferences. They have lots of weird pictures\non them, just ignore them.",
    "start": "4109430",
    "end": "4114920"
  },
  {
    "text": "OK? So let me motivate for you why\nI care about weak supervision and then I'm going to\nskip over some stuff",
    "start": "4114920",
    "end": "4121580"
  },
  {
    "text": "and then we'll try and get to\nthe mathematics of the model. So the reason we started\nthinking about weak supervision and others too, and this\nthing we call data-centric AI,",
    "start": "4121580",
    "end": "4129440"
  },
  {
    "text": "was that, machine learning at\nleast in the supervised setting has three pieces. It has a model,\nwhich you're learning",
    "start": "4129440",
    "end": "4134540"
  },
  {
    "text": "a lot about in this course. It has training\ndata, which we've talked about a whole bunch. And it has hardware.",
    "start": "4134540",
    "end": "4139700"
  },
  {
    "text": "And the thing is, which is\nweird outside this class, and for a variety\nof good reasons, models have become commodities.",
    "start": "4140420",
    "end": "4146719"
  },
  {
    "text": "So all of you in this room\ncan go download Google, Microsoft Stanford's latest\nmodels, just the culture of AI",
    "start": "4146720",
    "end": "4153259"
  },
  {
    "text": "is that we put\neverything online. And there's been a ton\nof investment in that. So if you want a state of the\nart natural language model,",
    "start": "4153260",
    "end": "4159500"
  },
  {
    "text": "you can download it. Go to Hugging Face. Download it. You want the state of\nthe art vision model, there's a tutorial somewhere\nthat will allow you to do it.",
    "start": "4159500",
    "end": "4166400"
  },
  {
    "text": "And you can get it\ninstantaneously. And because of the cloud,\nhardware is a big problem. It's not a big problem. People can get access to it.",
    "start": "4166400",
    "end": "4172700"
  },
  {
    "text": "Still interesting\nproblem, but it's there. But training data is hard. This was the original idea. And the reason training\ndata is hard is",
    "start": "4172700",
    "end": "4180080"
  },
  {
    "text": "it's the way that you encode\nyour problem about the world. It's how you label it. Now when I tell you\nabout supervised machine",
    "start": "4180080",
    "end": "4189319"
  },
  {
    "text": "learning, the first day,\nwe have X and Y pairs. And I never tell you\nwhere they come from.",
    "start": "4189320",
    "end": "4194480"
  },
  {
    "text": "You have X and Y pairs. They came from god, herself. They landed there. And you're, like, well, this is\nwhere machine learning starts.",
    "start": "4195380",
    "end": "4201200"
  },
  {
    "text": "But that's not at\nall how anything works in machine learning. It doesn't start\nthen and it's not like hanging out with Beyonce,\nit's like living in sewer.",
    "start": "4201200",
    "end": "4207800"
  },
  {
    "text": "So if you want to do\nthis stuff in industry, it is much more like hanging\nout in a dirty, filthy sewer.",
    "start": "4207800",
    "end": "4213080"
  },
  {
    "text": "You have all these data streams,\nyou don't know where the heck they came from. They all have weird errors. They all have weird\ncorrelations with one another.",
    "start": "4213080",
    "end": "4218840"
  },
  {
    "text": "And it's a huge mess. So we started to\nlook at this problem a couple of years ago with\nmany other folks in the field.",
    "start": "4218840",
    "end": "4225020"
  },
  {
    "text": "And we just wanted to put\nmathematical and system structure around how to\nmake this less awful.",
    "start": "4225020",
    "end": "4230180"
  },
  {
    "text": "OK? That was the motivation, right? Now I'm not going to\nbore you with the fact that people actually do\nthis and all the rest.",
    "start": "4230180",
    "end": "4237080"
  },
  {
    "text": "It turns out that this\nis an interesting area, but I'm just going to\nshow you a toy example.",
    "start": "4237080",
    "end": "4243200"
  },
  {
    "text": "So here's a toy example and a\nlittle bit of light math, OK? So here's an\nexample of something we may want to do and\ngenerate training data.",
    "start": "4243200",
    "end": "4250280"
  },
  {
    "text": "And there's a whole longer talk\nhere of how exactly this works. Let's say you want to do\nNamed Entity Recognition.",
    "start": "4250280",
    "end": "4255860"
  },
  {
    "text": "You want to label persons\nand hospitals in text. You just want to know. You have some mentions,\nare these mentions",
    "start": "4255860",
    "end": "4261380"
  },
  {
    "text": "of people or text? Saint Francis, could be a\nperson, could be the hospital. Bob Jones, probably\na person, but I",
    "start": "4261380",
    "end": "4266960"
  },
  {
    "text": "guess could be a hospital too. OK? So here's how these weak\nsupervision frameworks work. They basically allow you\nto have these little voters",
    "start": "4266960",
    "end": "4274100"
  },
  {
    "text": "that you write to reuse training\ndata that you had before. An existing classifier\nthat's already voting person,",
    "start": "4274100",
    "end": "4279800"
  },
  {
    "text": "you just throw it in. Here's another kind of\nclassification rule, uppercase existing classifier\nsays person, you put that in.",
    "start": "4279800",
    "end": "4286580"
  },
  {
    "text": "And then hospital. Well, I have a dictionary\nof hospital names, it's called distant supervision,\nso this one votes hospital. And the point is, is each\none of those noisy sources",
    "start": "4286580",
    "end": "4294440"
  },
  {
    "text": "in the sewer is telling\nyou something different. And all you want to do\nis determine how likely",
    "start": "4294440",
    "end": "4300440"
  },
  {
    "text": "is answer A versus answer B? So this is how it works\nin a particular engine. This is the first\nsystem that did it.",
    "start": "4300440",
    "end": "4306199"
  },
  {
    "text": "There's many other since then. Don't worry too much about that. But what it did is it\nestimated this graphical model.",
    "start": "4306200",
    "end": "4312500"
  },
  {
    "text": "And I'll show you how it\nworks in just one second because it will look\nvery familiar to you. Take in all those\nsources and you",
    "start": "4312500",
    "end": "4318320"
  },
  {
    "text": "try to estimate how\naccurate is every source and how correlated is every\nsource with one another? And then you make an\nestimate over all the labels",
    "start": "4318320",
    "end": "4326000"
  },
  {
    "text": "for every point, how\nprobable, how likely is it as a person, a\nhospital, or whatever. OK?",
    "start": "4326000",
    "end": "4331280"
  },
  {
    "text": "That's what you want to do. So we model this as\na generative process. There's the graphical\nmodel there.",
    "start": "4331280",
    "end": "4336920"
  },
  {
    "text": "You've probably seen\nsomething like this. And the instance here is we\nwant to learn the Y value that's there, the label that's there,\nwithout any hand labeled data.",
    "start": "4336920",
    "end": "4345680"
  },
  {
    "text": "So we want to look at the\nvotes and somehow deduce the Y. There are reasons you want to\ndo this that are beyond this,",
    "start": "4345680",
    "end": "4351860"
  },
  {
    "text": "but we're not going to have\nany label data when we do it. And we want to learn\nthe correlations and the accuracies.",
    "start": "4351860",
    "end": "4356180"
  },
  {
    "text": "OK. Awesome. So it's not at all obvious\nthat you could do this. The reason it works is because\nyou can see these voters",
    "start": "4357380",
    "end": "4364460"
  },
  {
    "text": "on many data sources. Millions of data\npoints, every point that comes in, every voter\nregisters \"yes\", \"no\",",
    "start": "4364460",
    "end": "4369980"
  },
  {
    "text": "or \"indifferent\". And you can look at their-- that they're\noverlapping judgments and estimate their accuracy\nand their correlation.",
    "start": "4369980",
    "end": "4377300"
  },
  {
    "text": "If you knew, for example,\nsource one was always right, you just count how often every\nsource agreed with source one.",
    "start": "4377300",
    "end": "4382940"
  },
  {
    "text": "And that would tell\nyou the accuracy of source two and source three. OK? But they could be\ncorrelated, as we saw.",
    "start": "4382940",
    "end": "4388520"
  },
  {
    "text": "That first function and second\nfunction called each other. They could be\ncorrelated in some way and you'll have to deduce that.",
    "start": "4388520",
    "end": "4394040"
  },
  {
    "text": "So here's the way to solve\nthese underlying problems. And I'm just going to go through\nthis very quickly because we don't have all the math.",
    "start": "4397460",
    "end": "4402500"
  },
  {
    "text": "But it basically looks like\nthose covariance matrices that we saw from before. The problem here is that\nwe want what's in the red.",
    "start": "4402500",
    "end": "4408980"
  },
  {
    "text": "We want to know,\nhow often does Y, which we don't see how\noften this thing-- let's make it 0 or 1 so\nit's true, we don't",
    "start": "4408980",
    "end": "4416000"
  },
  {
    "text": "see how often Y is\ncorrelated with one source. We only see how\noften the sources are correlated with one another.",
    "start": "4416000",
    "end": "4421880"
  },
  {
    "text": "So said another way,\nwe can only observe what's in sigma\nsub O. We don't get to see how often the sources\ncorrelate with the ground",
    "start": "4421880",
    "end": "4428480"
  },
  {
    "text": "truth. OK. Now here's the thing that\nis pretty interesting.",
    "start": "4428480",
    "end": "4433880"
  },
  {
    "text": "It turns out that the inverse\nof this matrix has a structure. This is one of the most\nbeautiful mathematical facts",
    "start": "4433880",
    "end": "4439580"
  },
  {
    "text": "that comes from the\ngraphical models course. And I'll just state\nit here without proof. It turns out that every time\nthis graphical structure",
    "start": "4439580",
    "end": "4445520"
  },
  {
    "text": "is missing an edge, there's\na 0 in the covariance matrix.",
    "start": "4445520",
    "end": "4449420"
  },
  {
    "text": "This allows us to write this\nin terms of the observed values and some rank one parameters.",
    "start": "4452300",
    "end": "4457520"
  },
  {
    "text": "And these Zi's basically\nsay, if they're one, they have perfect accuracy,\nand 0, they're total noise.",
    "start": "4458060",
    "end": "4463460"
  },
  {
    "text": "Now why that's interesting\nis it lets us set up a bunch of these equations\nthat basically say, we know the left hand side\nis 0 because let's imagine",
    "start": "4465380",
    "end": "4472579"
  },
  {
    "text": "I know the structure, I\nknow how they're correlated. You can solve for\nthat if you wanted.",
    "start": "4472580",
    "end": "4476180"
  },
  {
    "text": "And it turns out-- this will\nbe kind of weird stuff-- it turns out that\nactually here, you",
    "start": "4477920",
    "end": "4482960"
  },
  {
    "text": "can complete this as\nlong as you have enough of these linear constraints. Now a couple of things here.",
    "start": "4482960",
    "end": "4488540"
  },
  {
    "text": "First Z and minus\nZ are solutions. So this goes back to\nthose symmetry questions I wanted to highlight.",
    "start": "4488540",
    "end": "4493699"
  },
  {
    "text": "You can't tell if everybody's\ncorrelated with the ground truth or the exact opposite. You have to make an assumption.",
    "start": "4493700",
    "end": "4499520"
  },
  {
    "text": "If you had a bunch\nof malicious labelers who are all telling you\nthe exact wrong thing and correlated to each\nother, would get fooled.",
    "start": "4499520",
    "end": "4506000"
  },
  {
    "text": "So you have to make\nsome assumption there. The second thing is\nwhen Zi is equal to 0,",
    "start": "4506900",
    "end": "4512600"
  },
  {
    "text": "that means it's a coin flip. If I added a label\nor I flipped a coin, that should give\nyou no information. And indeed, it won't.",
    "start": "4513620",
    "end": "4519860"
  },
  {
    "text": "It will zero out\nevery constraint that it's in, which\nsanity checks. You can't make your job easier\nby adding these labelers.",
    "start": "4519860",
    "end": "4527000"
  },
  {
    "text": "It turns out I won't bore you. There's a great\npaper from Virsheenan from like 2014-2015\nthat measures",
    "start": "4528440",
    "end": "4533539"
  },
  {
    "text": "the notion of rank\nin a continuous way, it's called effective rank. And this tells you the\nright statistical scaling for this problem.",
    "start": "4533540",
    "end": "4538940"
  },
  {
    "text": "The closer those\nZ's get to 0, when can you recover the matrix? Super fun stuff. It's a nice geometric piece.",
    "start": "4538940",
    "end": "4545360"
  },
  {
    "text": "So you heard all this stuff. I just wanted to\nshare it with you, it's stuff that I\nend up working on. You probably think\nat this point,",
    "start": "4545960",
    "end": "4552500"
  },
  {
    "text": "\"there's no way anyone would\never use these crazy covariance matrices.\" But I just want to share\none thing with you, which is wild to me because\nmy students did this.",
    "start": "4552500",
    "end": "4559340"
  },
  {
    "text": "It's actually used in\nall these applications. It's still in production\nall these years later. And this is a way that\npeople cope with the fact",
    "start": "4559340",
    "end": "4566000"
  },
  {
    "text": "that they have lots\nof noisy training data and put them together. So it's in search ads, it's in\nYouTube, Gmail, and products",
    "start": "4566000",
    "end": "4572480"
  },
  {
    "text": "from Apple. And so this weird framework of\nhow you program training data is just something I\nwanted to share with you.",
    "start": "4572480",
    "end": "4577700"
  },
  {
    "text": "And the underlying\nmodel is like classical from what you've seen, except\nfor instead of solving these EM",
    "start": "4577700",
    "end": "4583100"
  },
  {
    "text": "problems, you solve\nthese weird covariance matrix inverse problems and\nyou can solve them provably. And that turns out to be\nimportant for a variety",
    "start": "4583100",
    "end": "4590120"
  },
  {
    "text": "of weird issues of\nestimating source quality. You don't want to rely on EM,\nwhich we talked about doesn't have a unique answer.",
    "start": "4590120",
    "end": "4595580"
  },
  {
    "text": "For this, we can tell\nyou exactly where the unique answer is. Thank you for your\ntime and attention and Tengyu will see\nyou on Wednesday.",
    "start": "4595580",
    "end": "4601699"
  }
]