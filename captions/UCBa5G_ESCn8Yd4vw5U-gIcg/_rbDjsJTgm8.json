[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "text": "So far, we've introduced\nBayesian networks and talked about how to\nperform inference in them. In this module, we'll\nturn to the question",
    "start": "5960",
    "end": "12500"
  },
  {
    "start": "6000",
    "end": "15000"
  },
  {
    "text": "of how to learn them\nfrom [AUDIO OUT] So recall that a\nBayesian network consists",
    "start": "12500",
    "end": "17710"
  },
  {
    "start": "15000",
    "end": "82000"
  },
  {
    "text": "of a set of random variables. For example, of cold, allergies,\ncough, and itchy eyes.",
    "start": "17710",
    "end": "25910"
  },
  {
    "text": "The Bayesian network also\ncomes equipped with a DAG, specifying the qualitative\nrelationships between all",
    "start": "25910",
    "end": "33140"
  },
  {
    "text": "these different variables. Quantitatively, however,\nthe Bayesian network defines a set of local\nconditional distributions",
    "start": "33140",
    "end": "41579"
  },
  {
    "text": "over each variable Xi,\ngiven the parents of i.",
    "start": "41580",
    "end": "48170"
  },
  {
    "text": "And in this example, we\nwould have probability of c, given parents, which\nare none, probability",
    "start": "48170",
    "end": "53780"
  },
  {
    "text": "of a, probability of h, given\nthat's two parents, c and a, and probability of i given a.",
    "start": "53780",
    "end": "61250"
  },
  {
    "text": "So finally, if we multiply\nall of these probability distributions\ntogether, then we get",
    "start": "61250",
    "end": "68600"
  },
  {
    "text": "what is the joint distribution\noverall and variables. In this case, we\nhave C, A, H, and I.",
    "start": "68600",
    "end": "82520"
  },
  {
    "start": "82000",
    "end": "135000"
  },
  {
    "text": "Then there's a\nquestion of how you do inference in Bayesian networks. So inference, remember, you're\ngiven a Bayesian network.",
    "start": "82520",
    "end": "89450"
  },
  {
    "text": "You're given some evidence\nthat you observe, for example, H and I equals 1 and 1 for\na subset of the variables.",
    "start": "89450",
    "end": "97370"
  },
  {
    "text": "And then you're given a query\nvariable, which is something that you're interested in. Let's say, you're\ninterested in cold.",
    "start": "97370",
    "end": "103520"
  },
  {
    "text": "And the inference\nalgorithm is going to produce a distribution\nover your query of variables",
    "start": "103520",
    "end": "108859"
  },
  {
    "text": "conditioned on evidence. So for every possible setting\nof the query variable,",
    "start": "108860",
    "end": "113990"
  },
  {
    "text": "we have a probability. So we saw many\nways of doing this,",
    "start": "113990",
    "end": "119530"
  },
  {
    "text": "including, manually, by\nexhaustive enumeration. We can convert Bayesian\nnetworks into Markov networks",
    "start": "119530",
    "end": "125700"
  },
  {
    "text": "and do Gibbs sampling. And then for HMMs, we have\nspecialized techniques such as the forward-backward\nalgorithm and particle",
    "start": "125700",
    "end": "133140"
  },
  {
    "text": "filtering. So inference assumes that\nall these local conditional",
    "start": "133140",
    "end": "138420"
  },
  {
    "start": "135000",
    "end": "157000"
  },
  {
    "text": "distributions are known. But the big question is where\ndid all of these come from?",
    "start": "138420",
    "end": "145489"
  },
  {
    "text": "So all of these\nnumbers are called the parameters of the Bayesian\nnetwork, the red question",
    "start": "145490",
    "end": "151690"
  },
  {
    "text": "marks. And in general, we might\nnot know what they are.",
    "start": "151690",
    "end": "157090"
  },
  {
    "start": "157000",
    "end": "222000"
  },
  {
    "text": "So let's try to learn that. So again, in all learning\ntasks, we start with the data. So in this case,\nthe training data",
    "start": "157090",
    "end": "164220"
  },
  {
    "text": "is going to include examples,\nwhere each example is a complete assignment\nto X. So this",
    "start": "164220",
    "end": "170820"
  },
  {
    "text": "is the fully supervised\nsetting, which is the simplest one to start out with. And then the learning algorithm\nis going to produce parameters.",
    "start": "170820",
    "end": "178190"
  },
  {
    "text": "And the parameters are exactly\nall these red question marks. These are all the local\nconditional probabilities.",
    "start": "178190",
    "end": "184640"
  },
  {
    "text": "So we're going to go\nthrough a bunch of examples, and then later show a general\nprinciple that ties all of them",
    "start": "187610",
    "end": "192710"
  },
  {
    "text": "together. So you might be\nfeeling a little bit",
    "start": "192710",
    "end": "198680"
  },
  {
    "text": "that this might be\nvery challenging because probabilistic\ninference assumes you know the parameters,\nand it was already",
    "start": "198680",
    "end": "205520"
  },
  {
    "text": "pretty hard, both\ncomputationally and perhaps, conceptually even. But it turns out that for\nBayesian networks, at least,",
    "start": "205520",
    "end": "212810"
  },
  {
    "text": "somewhat surprising\nif you're learning under fully supervised data,\nlearning actually turns out",
    "start": "212810",
    "end": "219019"
  },
  {
    "text": "to be surprisingly easy. So let's begin.",
    "start": "219020",
    "end": "224390"
  },
  {
    "start": "222000",
    "end": "341000"
  },
  {
    "text": "So suppose, you're\ndeveloping Bayesian networks to model how people rate movies.",
    "start": "224390",
    "end": "230080"
  },
  {
    "text": "So let's start with the\nworld's simplest Bayesian network, which has one\nvariable, R, which represents",
    "start": "230080",
    "end": "236440"
  },
  {
    "text": "the rating of a movie. So the joint distribution\nis just p of r in this case. The movie rating\ncan be 1 through 5.",
    "start": "236440",
    "end": "244300"
  },
  {
    "text": "So first, we have to identify\nwhat the parameters are. So the parameters here, theta,\nis just a probability of 1,",
    "start": "244300",
    "end": "251032"
  },
  {
    "text": "probability of 2, probability\nof 3, probability of 4, probability of 5. There are five parameters.",
    "start": "251032",
    "end": "256600"
  },
  {
    "text": "And if you're a\nlittle bit clever, you only need four of them\nbecause the five numbers have to sum to 1.",
    "start": "256600",
    "end": "261819"
  },
  {
    "text": "But for the sake of\nsimplicity, let's just say that there's\nfive parameters, OK?",
    "start": "261820",
    "end": "267770"
  },
  {
    "text": "And now you're given\nsome training data. Some ratings from users. You have a one,\nyou have a three,",
    "start": "267770",
    "end": "273449"
  },
  {
    "text": "you have a bunch of\nfours and three fives. And now, the question is, how\ndo you estimate the parameters",
    "start": "273450",
    "end": "280510"
  },
  {
    "text": "given the training data? Let's just follow our nose here. Well, intuitively,\nyou would think",
    "start": "280510",
    "end": "285730"
  },
  {
    "text": "that the probability\nof a rating is proportional to the\nnumber of occurrences",
    "start": "285730",
    "end": "291430"
  },
  {
    "text": "of that particular rating\nin the training data. So now this is just intuition.",
    "start": "291430",
    "end": "296759"
  },
  {
    "text": "It might be a good thing or\nit might not be a good thing. Well, let's find out later. But let's just go\nwith that for now.",
    "start": "296760",
    "end": "303360"
  },
  {
    "text": "So here's the training data. And what I'm going to do\nis the parameters are--",
    "start": "303360",
    "end": "310460"
  },
  {
    "text": "it's a probability table. So we're going to\nsee a lot of these over the course of\nthe next few slides. So for every rating, I'm going\nto count the number of times",
    "start": "310460",
    "end": "317098"
  },
  {
    "text": "that shows up. 1 shows up once, 2 shows up zero\ntimes, 3 up once, 4 shows up",
    "start": "317098",
    "end": "324920"
  },
  {
    "text": "five times, and 5\nshows up three times. And now, I'm just going\nto sum up all the counts.",
    "start": "324920",
    "end": "332180"
  },
  {
    "text": "That gives me 10. I'm going to normalize\nto get my probabilities. And that's the\nprobability estimate.",
    "start": "332180",
    "end": "338630"
  },
  {
    "text": "That's it. Count and normalize. OK, so let's level\nup a little bit",
    "start": "338630",
    "end": "345520"
  },
  {
    "start": "341000",
    "end": "493000"
  },
  {
    "text": "and talk about two variables. Suppose that now, the rating\nis governed by the genre.",
    "start": "345520",
    "end": "351753"
  },
  {
    "text": "So in particular,\nBayesian network is you first generate\nthe genre and then you generate the rating given genre.",
    "start": "351753",
    "end": "358900"
  },
  {
    "text": "So now there's the parameters\nof this Bayesian network includes both the probability\nof the genre, which",
    "start": "358900",
    "end": "366190"
  },
  {
    "text": "contain two parameters\nand the probability rating given genre, which includes\n2 times 5 parameters.",
    "start": "366190",
    "end": "373500"
  },
  {
    "text": "So 10 parameters for a\ntotal of 12 parameters. Again, if you're being clever,\nyou can get that down to 9.",
    "start": "373500",
    "end": "379930"
  },
  {
    "text": "So now, we're giving\nsome training data. So we have each\ntraining a point.",
    "start": "379930",
    "end": "386180"
  },
  {
    "text": "Remember, it is\na full assignment to all the variables. So we have our G equals\nd and R equals 4 here.",
    "start": "386180",
    "end": "392650"
  },
  {
    "text": "So now, how long do we\nestimate the parameters given this more complicated\nBayesian network?",
    "start": "395520",
    "end": "402860"
  },
  {
    "text": "So following our\nnose again, there's an intuitive strategy\nis that we're just going to estimate each local\nconditional distribution",
    "start": "402860",
    "end": "409069"
  },
  {
    "text": "separately and see\nwhat happens, OK? So what does that mean? That means for\nprobability of G, I'm",
    "start": "409070",
    "end": "417350"
  },
  {
    "text": "just going to count the number\nof times particular values of G show up. So d shows up 1, 2, 3 times.",
    "start": "417350",
    "end": "425180"
  },
  {
    "text": "And c shows up twice. So notice that this is the\nkind of same calculation as we had before.",
    "start": "425180",
    "end": "432960"
  },
  {
    "text": "So now, this is 3/5 and 2/5\nif you sum up and normalize.",
    "start": "432960",
    "end": "439750"
  },
  {
    "text": "OK, so in estimating\np of g, I simply only look at the slice of the\nexamples that matter for this.",
    "start": "439750",
    "end": "447460"
  },
  {
    "text": "And same with a probability\nof R given G. So",
    "start": "447460",
    "end": "452530"
  },
  {
    "text": "now I'm going to look at\nall the possible assignments to the parents of a particular\nnode and also that node.",
    "start": "452530",
    "end": "462430"
  },
  {
    "text": "So that's a g and r. So d4 shows up twice,\nb5 shows up once,",
    "start": "462430",
    "end": "470500"
  },
  {
    "text": "c1 shows up once,\nand c5 shows up once. Now, I count and normalize and\nI get my probability estimate",
    "start": "470500",
    "end": "477789"
  },
  {
    "text": "of r and g, OK? So far so good. So in summary, consider each\nlocal conditional distribution",
    "start": "477790",
    "end": "486259"
  },
  {
    "text": "separately, and then count based\non the slice of a data that matters, and normalize.",
    "start": "486260",
    "end": "493880"
  },
  {
    "start": "493000",
    "end": "693000"
  },
  {
    "text": "So now, let's consider\nthree variables. So we have a genre,\nwhether the movie won",
    "start": "493880",
    "end": "500930"
  },
  {
    "text": "an award or not and the rating. So here, we have a\ngenre and whether it won an award, influencing\nwhether how well",
    "start": "500930",
    "end": "509210"
  },
  {
    "text": "the movie is rated. Joint distribution is p of g,\np of a, p of r, given g and a.",
    "start": "509210",
    "end": "515809"
  },
  {
    "text": "So now, we have local\nconditional distributions for each of these factors here.",
    "start": "515809",
    "end": "525200"
  },
  {
    "text": "So remember that V structures,\nthis type of structure, was really special\nin Bayesian networks.",
    "start": "525200",
    "end": "531519"
  },
  {
    "text": "It gives rise to\nexplaining away. It's the thing that if you\nmarginalize unobserved leaves,",
    "start": "531520",
    "end": "536590"
  },
  {
    "text": "you can render\nthings independent. And it was really a\nhallmark of vision now.",
    "start": "536590",
    "end": "541990"
  },
  {
    "text": "But from a perspective\nof learning, there's really\nnothing special here. And to see this, what\nwe're going to do is just--",
    "start": "541990",
    "end": "551510"
  },
  {
    "text": "suppose we have some\ntraining data, which includes assignments\nto all three variables.",
    "start": "551510",
    "end": "556850"
  },
  {
    "text": "We're just going to count\nand normalize again. And so here, we're going\nto solve with p of g.",
    "start": "556850",
    "end": "564529"
  },
  {
    "text": "This is exactly the\nsame thing as before. We just look at only the genre. And then we're going to look\nat p of a, which is analogously",
    "start": "564530",
    "end": "575200"
  },
  {
    "text": "looking at only 0, 1 goes 0, 1,\nand counting and normalizing.",
    "start": "575200",
    "end": "582970"
  },
  {
    "text": "And now the big local\ncondition of distribution is p of r given g and a.",
    "start": "582970",
    "end": "591339"
  },
  {
    "text": "So here, I'm going to look at\nthe parents of r and r itself.",
    "start": "591340",
    "end": "597120"
  },
  {
    "text": "I'm going to count the number of\ntimes this local configuration happens. So I have d, 0, 1 showing up\nonce, d 0, 3 showing up once,",
    "start": "597120",
    "end": "610900"
  },
  {
    "text": "And d, 1, 5 showing up once and\neach of these showing up once. And now, I want to\nnormalize so I have",
    "start": "610900",
    "end": "618785"
  },
  {
    "text": "to be a little bit careful. I don't want to add all\nthese numbers or normalize. Because this is\nconditioned on g and a.",
    "start": "618785",
    "end": "628509"
  },
  {
    "text": "So that means for every\nsetting of g and a, I have my distribution over r.",
    "start": "628510",
    "end": "636940"
  },
  {
    "text": "I'm going to look at d, 0. So I have one\noccurrence of r equals 1",
    "start": "636940",
    "end": "643420"
  },
  {
    "text": "and one occurrence\nof r equals 3. So if I normalize\nthat, it's going to be give me half and half.",
    "start": "643420",
    "end": "650610"
  },
  {
    "text": "And now for this\nsetting g and a, I only have one\npossibility of r.",
    "start": "650610",
    "end": "656890"
  },
  {
    "text": "So that has probability 1 and\nsame for these other ones.",
    "start": "656890",
    "end": "662360"
  },
  {
    "text": "So again, everything\nhas count and normalize. Where you have to pay attention\nto what you're normalizing",
    "start": "662360",
    "end": "669399"
  },
  {
    "text": "over, you're only normalizing\nover possible values of r, not g and a.",
    "start": "669400",
    "end": "676717"
  },
  {
    "text": "So one thing you might note is\nthat all of these probabilities are 1 and the probabilities that\nare not mentioned here are 0.",
    "start": "680060",
    "end": "688760"
  },
  {
    "text": "So you might wonder that\nif this is a good estimate, but we'll come\nback to that later.",
    "start": "688760",
    "end": "695120"
  },
  {
    "start": "693000",
    "end": "917000"
  },
  {
    "text": "So now, let's invert\nthe V structure. Let's look at a\ndifferent structure.",
    "start": "695120",
    "end": "700190"
  },
  {
    "text": "So we have the genre. And suppose we have two\npeople, Jim and Martha. And they're both going\nto rate this movie.",
    "start": "700190",
    "end": "706240"
  },
  {
    "text": "And both of them rate it\ndepending on the genres. G generates R1 and\nalso generates R2.",
    "start": "706240",
    "end": "712960"
  },
  {
    "text": "So now we have this three\nnode Bayesian network, and the estimation is\ngoing to be the same.",
    "start": "715510",
    "end": "724390"
  },
  {
    "text": "I'll just go through\nit very quickly. So we have parameters 1\nfor every variable here.",
    "start": "724390",
    "end": "730910"
  },
  {
    "text": "And so probability of g\nis count and normalize,",
    "start": "730910",
    "end": "737350"
  },
  {
    "text": "probability of R1, given g,\nis you count and normalize.",
    "start": "737350",
    "end": "743350"
  },
  {
    "text": "Again, remember that\nI'm normalizing over possible values of g.",
    "start": "743350",
    "end": "749185"
  },
  {
    "text": "So you can partition the\nrows based on the value of g. So here, I have 2 and 1.",
    "start": "749185",
    "end": "754810"
  },
  {
    "text": "And I'm normalizing 2/3 and 1/3. And g equals c is just handled\nin a separate normalization.",
    "start": "754810",
    "end": "763410"
  },
  {
    "text": "And then R2, given\ng is analogous, so I'm not going\nto go over this.",
    "start": "763410",
    "end": "771480"
  },
  {
    "text": "So this is fine, except for\nwhat I'm going to now do",
    "start": "771480",
    "end": "777019"
  },
  {
    "text": "is think about the\nsetting where suppose you have not just two users, but\n1,000 users or a million users.",
    "start": "777020",
    "end": "786290"
  },
  {
    "text": "Now, you might be a little\nbit worried because now, for every user, you\nmight have to have",
    "start": "786290",
    "end": "794600"
  },
  {
    "text": "their own local\nconditional distribution. And the number of\nparameters might just",
    "start": "794600",
    "end": "800930"
  },
  {
    "text": "go up, which means that\nestimation might be hard, especially for new users.",
    "start": "800930",
    "end": "807040"
  },
  {
    "text": "So we're going to consider\na slightly different-- it's going to be the same\nBayesian network here,",
    "start": "807040",
    "end": "815570"
  },
  {
    "text": "but the parameters\nare different. In particular, I'm\ngoing to consider",
    "start": "815570",
    "end": "822899"
  },
  {
    "text": "a single parameter of r give g,\ninstead of having p of R1 and p",
    "start": "822900",
    "end": "829260"
  },
  {
    "text": "of R2. So now how do I estimate\ndistribution of this model?",
    "start": "829260",
    "end": "835850"
  },
  {
    "text": "So let's begin. So probability of g\nis the same as before. And now the probability\nof r given g,",
    "start": "838490",
    "end": "846843"
  },
  {
    "text": "I'm just going to\ncount the number of times a particular\nlocal configuration",
    "start": "846843",
    "end": "852640"
  },
  {
    "text": "shows up, either\nwhere r is R1 or R2. So d, 3 shows up once here,\nd, 4 shows up three times.",
    "start": "852640",
    "end": "865420"
  },
  {
    "text": "You have 1 and 2 and 3. So notice I'm counting both\noccurrences of R1 and R2.",
    "start": "865420",
    "end": "875400"
  },
  {
    "text": "And d, 5 shows up twice here\nwith R1, and here with R2.",
    "start": "875400",
    "end": "883530"
  },
  {
    "text": "c, 1 shows up once, c, 2 shows\nup once, c, 4 shows up once, and c, 5 shows up once as well.",
    "start": "883530",
    "end": "891010"
  },
  {
    "text": "Now, I just count and normalize. So I look at all the b's and\nI count, sum, and normalize.",
    "start": "891010",
    "end": "899960"
  },
  {
    "text": "Now I look at all the c's,\nand I count and normalize.",
    "start": "899960",
    "end": "905100"
  },
  {
    "text": "OK, so when I have only\none distribution that is responsible for\ntwo nodes, I simply",
    "start": "905100",
    "end": "912149"
  },
  {
    "text": "aggregate their\ncounts and normalize. So this is an important slide.",
    "start": "912150",
    "end": "920339"
  },
  {
    "start": "917000",
    "end": "1090000"
  },
  {
    "text": "So the more general idea\nthat I want to highlight is this idea of parameter\nsharing in Bayesian networks.",
    "start": "920340",
    "end": "926570"
  },
  {
    "text": "And this happens when the\nlocal conditional distributions over different variables\nare actually the same.",
    "start": "926570",
    "end": "934010"
  },
  {
    "text": "And to be very\nprecise about that, I want you to look at\nthe following picture. So we have G, R1, and R2.",
    "start": "934010",
    "end": "942530"
  },
  {
    "text": "So far, we've looked\nat Bayesian networks through the lens\nof inference, where",
    "start": "945630",
    "end": "951210"
  },
  {
    "text": "we know that every\nvariable comes with a local conditional\ndistribution.",
    "start": "951210",
    "end": "956820"
  },
  {
    "text": "But we didn't worry about\nwhere that came from. It was just there,\nbut now, for learning",
    "start": "956820",
    "end": "962670"
  },
  {
    "text": "it matters where it came from. So what we should\nthink about is each",
    "start": "962670",
    "end": "968529"
  },
  {
    "text": "of these variables\nbeing empowered by a local conditional\ndistribution.",
    "start": "968530",
    "end": "973550"
  },
  {
    "text": "So g is powered by\nthis table here. R1 is powered by this table.",
    "start": "973550",
    "end": "980530"
  },
  {
    "text": "And in the case of\nparameter sharing, R2 is also powered\nby this table.",
    "start": "980530",
    "end": "985610"
  },
  {
    "text": "So we have a Bayesian network. And behind the\nscenes, you should think about all\nthese tables, which have arrows kind of hooking\nup and providing juice",
    "start": "985610",
    "end": "993560"
  },
  {
    "text": "to each of these variables. And now, if you didn't\nhave parameter sharing,",
    "start": "993560",
    "end": "999110"
  },
  {
    "text": "then R1 and R2 would be\npowered by different tables.",
    "start": "999110",
    "end": "1004480"
  },
  {
    "text": "Now, this is an important point. When we're doing inference,\nyou should think about that as reading from the parameters.",
    "start": "1004480",
    "end": "1011730"
  },
  {
    "text": "And where you're\nreading, you don't care whether you have\ntwo copies of something",
    "start": "1011730",
    "end": "1017010"
  },
  {
    "text": "or one copy of something because\nyou're getting the same thing. But in learning, we're\nwriting to the parameters",
    "start": "1017010",
    "end": "1023700"
  },
  {
    "text": "from the observed variables. In that case, you\nneed to worry about",
    "start": "1023700",
    "end": "1029189"
  },
  {
    "text": "whether you're writing to one,\na memory location, or two memory locations. So the right analogy is you\nthink about in programming, you",
    "start": "1029190",
    "end": "1036390"
  },
  {
    "text": "have pass by reference\nor pass by value. And in parameter sharing,\nwe're passing by reference.",
    "start": "1036390",
    "end": "1042390"
  },
  {
    "text": "So we're passing this parameter\ninto each of these nodes. And when we do\nlearning, we write back",
    "start": "1042390",
    "end": "1048779"
  },
  {
    "text": "into those parameters. And it matters whether they're\nthe same parameters or not.",
    "start": "1048780",
    "end": "1054419"
  },
  {
    "text": "So when would you do\nparameter sharing like this? Well, it's a trade off. And it's ultimately,\na multi-leg decision.",
    "start": "1057120",
    "end": "1064270"
  },
  {
    "text": "So by doing this, you\naggregate your data, which means that you have\nmore data per parameter,",
    "start": "1064270",
    "end": "1070429"
  },
  {
    "text": "which allows you to get\nmore reliable estimates. On the other hand, you end up\nwith less expressive models.",
    "start": "1070430",
    "end": "1076990"
  },
  {
    "text": "For example, if you\nhad a lot of users, you might lose\nability to personalize if you parameter share.",
    "start": "1076990",
    "end": "1082750"
  },
  {
    "text": "And there's, obviously,\nmany intermediate points",
    "start": "1082750",
    "end": "1087910"
  },
  {
    "text": "as well, which we won't get to. So let's look at some\nother Bayesian networks",
    "start": "1087910",
    "end": "1095010"
  },
  {
    "start": "1090000",
    "end": "1191000"
  },
  {
    "text": "with parameter sharing. So we already looked at\nthe naive Bayes before, but just to anchor\nit in this notation,",
    "start": "1095010",
    "end": "1103890"
  },
  {
    "text": "let's say we have a genre\nand we have a movie review, and we have a Bayesian\nnetwork, which",
    "start": "1103890",
    "end": "1111570"
  },
  {
    "text": "generates each word\nindependently conditioned on the genre.",
    "start": "1111570",
    "end": "1118990"
  },
  {
    "text": "And so the joint\ndistribution over everything is equal to probability under\ngenre of y times for each word",
    "start": "1118990",
    "end": "1128880"
  },
  {
    "text": "the probability of a\nparticular word given, y.",
    "start": "1128880",
    "end": "1134460"
  },
  {
    "text": "So the parameters of\nthis Bayesian network are the genre and P word.",
    "start": "1134460",
    "end": "1140220"
  },
  {
    "text": "So now, you can think about\ndoing a little exercise of how many parameters are there?",
    "start": "1140220",
    "end": "1147325"
  },
  {
    "text": "So you look at theta\nand you say, P genre. Well, that's two\nparameters, two genres.",
    "start": "1147325",
    "end": "1152919"
  },
  {
    "text": "Pword, that's 2 times\nthe number of words,",
    "start": "1152920",
    "end": "1160240"
  },
  {
    "text": "the number of values\nthat Wi can take on. And so that's it.",
    "start": "1160240",
    "end": "1166290"
  },
  {
    "text": "So notice, importantly, that\nthe number of parameters does not grow with L, even\nthough the number of variables",
    "start": "1166290",
    "end": "1175790"
  },
  {
    "text": "in the Bayesian\nnetwork grows with L. So now we see the kind of the\ncomplexities of the parameters",
    "start": "1175790",
    "end": "1181430"
  },
  {
    "text": "and the number of variables\nto be quite different. You can have a million\nvariable Bayesian network,",
    "start": "1181430",
    "end": "1186500"
  },
  {
    "text": "but you might have only\none parameter, for example. That's quite possible.",
    "start": "1186500",
    "end": "1192610"
  },
  {
    "start": "1191000",
    "end": "1377000"
  },
  {
    "text": "So here's another\nexample, our friendly HMM. So we have actual positions of\nobjects, H1 through Hn and E1",
    "start": "1192610",
    "end": "1200230"
  },
  {
    "text": "through En. And this should be\nvery familiar by now, so you have HMM, which has\na joint distribution, which",
    "start": "1200230",
    "end": "1207820"
  },
  {
    "text": "is given by three distributions,\nP start of h1 times",
    "start": "1207820",
    "end": "1212889"
  },
  {
    "text": "transition of hi given hi\nminus 1 times for each variable",
    "start": "1212890",
    "end": "1217930"
  },
  {
    "text": "the probability of\nemitting ei, given hi. Again, the parameters are P\nstart, P trans, and P emit.",
    "start": "1217930",
    "end": "1225220"
  },
  {
    "text": "And you can think about\nhow many parameters are in this Bayesian network.",
    "start": "1225220",
    "end": "1231309"
  },
  {
    "text": "Well, you have the\nnumber of positions times number of positions\nsquared times number",
    "start": "1231310",
    "end": "1238090"
  },
  {
    "text": "of positions times number of\npossible sensor reading values.",
    "start": "1238090",
    "end": "1243549"
  },
  {
    "text": "Again, there is no dependence\non the time window, the number",
    "start": "1243550",
    "end": "1249070"
  },
  {
    "text": "of time steps here, n. And this is useful because\nif you imagine tracking over",
    "start": "1249070",
    "end": "1254770"
  },
  {
    "text": "a long period of time, we may\nhave a million time steps, you don't want the number of\nparameters to be the same.",
    "start": "1254770",
    "end": "1261910"
  },
  {
    "text": "OK, so here, the training\ndata is going to, again, be full assignments to all\nthe parameter variables.",
    "start": "1261910",
    "end": "1270360"
  },
  {
    "text": "And later in a future module,\nwe'll come back to the case where in practice,\nyou might only",
    "start": "1270360",
    "end": "1276600"
  },
  {
    "text": "observe the sensor reading. But more on that later. So now, let's present\nthe general case.",
    "start": "1276600",
    "end": "1282480"
  },
  {
    "text": "Hopefully, the intuitions\nhave already been fleshed out. But I just want to write things\ndown with some formal notation.",
    "start": "1282480",
    "end": "1289990"
  },
  {
    "text": "So a Bayesian network,\nremember, includes variables X1 through Xn.",
    "start": "1289990",
    "end": "1295690"
  },
  {
    "text": "And now we have parameters,\nand the parameters is a collection\nof distributions.",
    "start": "1295690",
    "end": "1304929"
  },
  {
    "text": "So I'm going to write\nthat as p subscript d, where d indexes\ninto a subset.",
    "start": "1304930",
    "end": "1312520"
  },
  {
    "text": "And for the HMM, for example,\nbig D is start, trans, emit. So D is just a label,\nif you will, a name.",
    "start": "1312520",
    "end": "1321650"
  },
  {
    "text": "So each variable Xi is generated\nfrom some distribution. And now the notation\ngets a little bit hairy,",
    "start": "1321650",
    "end": "1329840"
  },
  {
    "text": "but its p sub di is\nthe distribution that points into Xi.",
    "start": "1329840",
    "end": "1336260"
  },
  {
    "text": "And I'm looking up that\ndistribution by name p.",
    "start": "1336260",
    "end": "1340910"
  },
  {
    "text": "So you can think about this\nmore formally as this is just the equation for defining\nwhat a Bayesian network is",
    "start": "1343985",
    "end": "1352160"
  },
  {
    "text": "of the joint distribution\nequals the product, the local conditional\ndistributions. But now, I'm being very\nexplicit that each variable, di,",
    "start": "1352160",
    "end": "1363860"
  },
  {
    "text": "every variable I has a\nparticular distribution that is powering that variable.",
    "start": "1363860",
    "end": "1370500"
  },
  {
    "text": "So the idea of parameter sharing\nis that di is just the same for multiple i's.",
    "start": "1370500",
    "end": "1377670"
  },
  {
    "start": "1377000",
    "end": "1455000"
  },
  {
    "text": "OK, so here is the\nlearning algorithm for general Bayesian networks.",
    "start": "1377670",
    "end": "1383190"
  },
  {
    "text": "So the input is a D train,\nconsisting of full segments to all the variables\nX1 through Xn.",
    "start": "1383190",
    "end": "1389340"
  },
  {
    "text": "And the output is going to be\nall these distributions here. So the algorithm is, again,\njust count and normalize.",
    "start": "1389340",
    "end": "1396990"
  },
  {
    "text": "So what we're going to do is go\nthrough every training example, which is a full assignment\nto all the variables.",
    "start": "1396990",
    "end": "1403440"
  },
  {
    "text": "For every variable in\nyour Bayesian network, we're just going to\nincrement a counter.",
    "start": "1403440",
    "end": "1410850"
  },
  {
    "text": "OK, so what this\ncounter is is, I look at which distribution\nis powering variable i,",
    "start": "1410850",
    "end": "1418320"
  },
  {
    "text": "and I'm going to\nincrement that counter for the local\nconfiguration, which",
    "start": "1418320",
    "end": "1424680"
  },
  {
    "text": "is looking at assignment\nto its parents and also the value of Xi.",
    "start": "1424680",
    "end": "1432110"
  },
  {
    "text": "And then I'm just\ngoing to normalize for each distribution and local\nassignment to its parents.",
    "start": "1432110",
    "end": "1440420"
  },
  {
    "text": "I'm going to set the probability\nunder that distribution of Xi given parents to be\nproportional to this count, OK?",
    "start": "1440420",
    "end": "1451240"
  },
  {
    "text": "And that's it. So far, we've presented this\ncount and normalize algorithm,",
    "start": "1451240",
    "end": "1458929"
  },
  {
    "start": "1455000",
    "end": "1815000"
  },
  {
    "text": "showing a lot of examples. And hopefully, this seems\nlike a reasonable thing to do.",
    "start": "1458930",
    "end": "1464180"
  },
  {
    "text": "But part of you might still\nbe wondering, well, why? Why is count and normalize\na reasonable thing to do?",
    "start": "1464180",
    "end": "1471500"
  },
  {
    "text": "And there is a higher\nprinciple here. And it's called\nmaximum likelihood.",
    "start": "1471500",
    "end": "1477040"
  },
  {
    "text": "So the principle of\nmaximum likelihood, which is a very old\nidea in statistics",
    "start": "1477040",
    "end": "1482490"
  },
  {
    "text": "is that we have our\ntraining data here.",
    "start": "1482490",
    "end": "1487610"
  },
  {
    "text": "So if we look at the\nproduct over all examples in the training data, and\nwe look at the probability",
    "start": "1487610",
    "end": "1494840"
  },
  {
    "text": "under the Bayesian network\nthat is assigned to that data.",
    "start": "1494840",
    "end": "1500700"
  },
  {
    "text": "And notice, I'm going to\nprovide semicolon theta here to recognize the fact that\nthis Bayesian network depends",
    "start": "1500700",
    "end": "1507830"
  },
  {
    "text": "on the parameters now. So this is the\nlikelihood of this theta",
    "start": "1507830",
    "end": "1513880"
  },
  {
    "text": "given these parameters. A maximum likelihood\nis saying, I want to tweak these parameters,\nso that this likelihood as",
    "start": "1513880",
    "end": "1521560"
  },
  {
    "text": "large as possible. So this should look\na little bit more like what we were doing in the\nmachine learning modules, where",
    "start": "1521560",
    "end": "1528730"
  },
  {
    "text": "we write down a loss function,\nwhich depends on parameters, and which is usually\na sum over the theta.",
    "start": "1528730",
    "end": "1534790"
  },
  {
    "text": "And we try to find the\nparameters at minimized loss. Here, it's the opposite.",
    "start": "1534790",
    "end": "1540049"
  },
  {
    "text": "We're trying to find\nthe parameters that maximize the likelihood. And if you just take a\nlog and you negate it,",
    "start": "1540050",
    "end": "1547750"
  },
  {
    "text": "you actually end up with\nminimum loss as well. But I will ignore that for now.",
    "start": "1547750",
    "end": "1554950"
  },
  {
    "text": "So intuitively, this is a\nreasonable principle as well. What you're trying to\ndo is for every setting",
    "start": "1554950",
    "end": "1562620"
  },
  {
    "text": "of parameters, that\ngives you some likelihood under the model of the data.",
    "start": "1562620",
    "end": "1567798"
  },
  {
    "text": "And you just want\nto keep on tweaking that until the likelihood\nas high as possible.",
    "start": "1567798",
    "end": "1572940"
  },
  {
    "text": "So having said\nthat, now I'm just going to claim that that\nalgorithm, which we called",
    "start": "1576030",
    "end": "1581610"
  },
  {
    "text": "count and normalize is exactly\nsolving the maximum likelihood objective.",
    "start": "1581610",
    "end": "1587669"
  },
  {
    "text": "So this is really nice\nbecause it gives us a closed form solution to this\nmaximum likelihood objective.",
    "start": "1587670",
    "end": "1594779"
  },
  {
    "text": "You don't have to take\nthe gradient of this and iterate and worry\nabout convergence. Also, it's just done.",
    "start": "1594780",
    "end": "1600450"
  },
  {
    "text": "And this is one of\nthe reasons that makes maximum likelihood\nestimation of Bayesian network so scalable and intuitive\nis that well, it is scalable",
    "start": "1600450",
    "end": "1612169"
  },
  {
    "text": "and well, that was a\nlittle bit tautological. All right, so I\nhaven't justified",
    "start": "1612170",
    "end": "1619330"
  },
  {
    "text": "why maximum likelihood\nprinciple leads to the count and normalize algorithm,\nbut let me just provide you a little\nbit of a taste of why",
    "start": "1619330",
    "end": "1627400"
  },
  {
    "text": "this might be the case. So let's take this small data\nset, d, 4, d, 5, and c, 5.",
    "start": "1627400",
    "end": "1635200"
  },
  {
    "text": "So if I write down the\nmaximum likelihood objective,",
    "start": "1635200",
    "end": "1640490"
  },
  {
    "text": "so I have two variables here,\nI'm going to expand that.",
    "start": "1640490",
    "end": "1647059"
  },
  {
    "text": "OK, so I have max over theta. And theta, really here, is\nthe probability of genre,",
    "start": "1647060",
    "end": "1653179"
  },
  {
    "text": "the probability of rating\nof given the genre is c,",
    "start": "1653180",
    "end": "1658850"
  },
  {
    "text": "and the probability of\nrating, given genre is d. So I have three distributions\nhere that I want to optimize.",
    "start": "1658850",
    "end": "1667160"
  },
  {
    "text": "And I'm just expand out\nbased on the definition of Bayesian network.",
    "start": "1667160",
    "end": "1673010"
  },
  {
    "text": "I have probability of D, given\nfor probability of rating 4, given D. So that is the\nprobability of the first data",
    "start": "1673010",
    "end": "1683480"
  },
  {
    "text": "point times if d, 5 given D,\nthat's the second data point.",
    "start": "1683480",
    "end": "1689760"
  },
  {
    "text": "And then e of c and p of 5 given\nc, that's the third data point. So I'm multiplying all\nthese probabilities",
    "start": "1689760",
    "end": "1696500"
  },
  {
    "text": "across all the points, and that\nis the probability of the data,",
    "start": "1696500",
    "end": "1702140"
  },
  {
    "text": "given a particular assignment\nto the local conditional",
    "start": "1702140",
    "end": "1707600"
  },
  {
    "text": "distribution. And now, I've color-coded\nthem on purpose because what we can do is we\ncan shuffle things around.",
    "start": "1707600",
    "end": "1716550"
  },
  {
    "text": "If you just look at probability\nof g, so I'm maxing over that. And that shows up in\nthese three places.",
    "start": "1716550",
    "end": "1725985"
  },
  {
    "text": "And it doesn't\naffect anything else. So I can just pull that out.",
    "start": "1725985",
    "end": "1731310"
  },
  {
    "text": "And I can pull the green\napart, which is r given c.",
    "start": "1731310",
    "end": "1736530"
  },
  {
    "text": "I can pull the blue stuff out,\nand that's maximizing over",
    "start": "1736530",
    "end": "1741840"
  },
  {
    "text": "if r given g equals d here. So the punchline here\nis that we can decompose",
    "start": "1741840",
    "end": "1753015"
  },
  {
    "text": "the maximum likelihood\nobjective, which looks like a big tangled mess\ninto actually sub problems,",
    "start": "1753015",
    "end": "1758360"
  },
  {
    "text": "one for every distribution\nand assignments to the parents of a\nparticular variable.",
    "start": "1758360",
    "end": "1766730"
  },
  {
    "text": "And now having done that,\nnow I have just one, a little, local\noptimization problem here,",
    "start": "1766730",
    "end": "1775440"
  },
  {
    "text": "which is basically a\nsolve in closed form.",
    "start": "1775440",
    "end": "1781940"
  },
  {
    "text": "You can do this. I'm not going to\ndo this for you, but you can introduce\na Lagrange multiplier for the sum-to-1 constraint.",
    "start": "1781940",
    "end": "1788809"
  },
  {
    "text": "And you can take some\nderivatives instead of 0,",
    "start": "1788810",
    "end": "1794210"
  },
  {
    "text": "and then you get that. The maximum\nlikelihood probability is proportional to\nthe [INAUDIBLE]..",
    "start": "1794210",
    "end": "1801840"
  },
  {
    "text": "And in this case,\nwhat we will estimate is that the probability of\nd is 2/3, probability of c",
    "start": "1801840",
    "end": "1809076"
  },
  {
    "text": "is 1/3 and so on and so forth.",
    "start": "1809076",
    "end": "1811908"
  },
  {
    "start": "1815000",
    "end": "1904000"
  },
  {
    "text": "OK, so let me summarize now. So we've talked about\nlearning in fully supervised",
    "start": "1815580",
    "end": "1821430"
  },
  {
    "text": "Bayesian networks, where\nwe're observing instances of all the variables here.",
    "start": "1821430",
    "end": "1827580"
  },
  {
    "text": "So one important\nconcept to take away is this idea of\nparameter sharing. So we have talked about\njust a Bayesian network,",
    "start": "1827580",
    "end": "1837159"
  },
  {
    "text": "which an inference doesn't care\nwhere these parameters come from.",
    "start": "1837160",
    "end": "1842433"
  },
  {
    "text": "But we should really think\nabout each of these nodes as being powered by a particular\nlocal conditional distribution.",
    "start": "1842433",
    "end": "1850990"
  },
  {
    "text": "And sometimes, two\nvariables could be powered by the same distribution.",
    "start": "1850990",
    "end": "1856679"
  },
  {
    "text": "And again, inference is\nreading from the parameters-- learning is writing into the\nparameters in which case,",
    "start": "1856680",
    "end": "1863210"
  },
  {
    "text": "it matters where these\narrows come from.",
    "start": "1863210",
    "end": "1866000"
  },
  {
    "text": "So secondly, we looked at the\nmaximum likelihood principle,",
    "start": "1868650",
    "end": "1873870"
  },
  {
    "text": "which is this kind of\nhigh-minded principle that says maximize the\nlikelihood of your data. And we show that this\nis equal to this very",
    "start": "1873870",
    "end": "1882810"
  },
  {
    "text": "pragmatic and simple intuitive\nprinciple of counting and normalizing.",
    "start": "1882810",
    "end": "1888300"
  },
  {
    "text": "And this is the simplicity,\nwhich makes Bayesian networks, especially Naive Bayes still\nvery kind of practical,",
    "start": "1888300",
    "end": "1895169"
  },
  {
    "text": "useful, and interpretable. That's the end.",
    "start": "1895170",
    "end": "1899480"
  }
]