[
  {
    "start": "0",
    "end": "5380"
  },
  {
    "text": "So welcome to the class. Welcome to the last\nweek of lectures.",
    "start": "5380",
    "end": "11270"
  },
  {
    "text": "So for today, we have\na very exciting topic",
    "start": "11270",
    "end": "16299"
  },
  {
    "text": "that Rex Ying, who's an\nassistant professor at Yale. He's an alumni from my research\ngroup here at Stanford.",
    "start": "16300",
    "end": "23020"
  },
  {
    "text": "And through this class, we\nlearned a lot of methods",
    "start": "23020",
    "end": "28660"
  },
  {
    "text": "that Rex developed. So the GraphSAGE is his\nwork, the recommender system",
    "start": "28660",
    "end": "36430"
  },
  {
    "text": "stuff we talked\nabout is Rex's work. Today, he will talk\nabout explainability",
    "start": "36430",
    "end": "42550"
  },
  {
    "text": "for graph-based machine\nlearning that is his work. And the graph generation\nwe talked about,",
    "start": "42550",
    "end": "50140"
  },
  {
    "text": "I think he also had\nhis fingers in there. So a lot of actually topics that\nwe talked about in the class",
    "start": "50140",
    "end": "56350"
  },
  {
    "text": "were papers and areas\npioneered by Rex. So what Rex is going to\ntalk to you about today",
    "start": "56350",
    "end": "63850"
  },
  {
    "text": "is explainability for\ngraph neural networks, and how do we think about\nthis kind of explainable",
    "start": "63850",
    "end": "71140"
  },
  {
    "text": "AI, trustworthy machine learning\nin the context of graph machine learning. So it'll be kind of a good,\nnice overview of that area.",
    "start": "71140",
    "end": "79840"
  },
  {
    "text": "Thank you. Thank you. ",
    "start": "79840",
    "end": "87060"
  },
  {
    "text": "OK, hello. Yeah. I think everyone can hear me. So yeah, it feels really\nnice to be here again.",
    "start": "87060",
    "end": "93299"
  },
  {
    "text": "I was here until the end of-- but yeah, it feels great to\ncome back to [INAUDIBLE] again.",
    "start": "93300",
    "end": "99030"
  },
  {
    "text": "And today, I'm going\nto share with you guys some exciting\nresearch in the area of trustworthy deep learning,\nespecially explainability",
    "start": "99030",
    "end": "106020"
  },
  {
    "text": "in the context of GNNs\nand graph learning. So let's get started.",
    "start": "106020",
    "end": "111490"
  },
  {
    "text": "So the title of the\ntalk is XAI for Graphs. And XAI is a very common term\nto describe explainable AI,",
    "start": "111490",
    "end": "121790"
  },
  {
    "text": "and today, we're going to\nfocus on graph and GNNs. So before the lecture, I\nusually give a list of readings.",
    "start": "121790",
    "end": "130860"
  },
  {
    "text": "So I suggested something that\nare more general explainable",
    "start": "130860",
    "end": "136020"
  },
  {
    "text": "models such as LIME\nand SHAP, and I also posted some reference to\nGNNExplainer and Explainability",
    "start": "136020",
    "end": "142500"
  },
  {
    "text": "Taxonomy and Trustworthy GNN. So some of these are from the\nlab that [INAUDIBLE] leads,",
    "start": "142500",
    "end": "148930"
  },
  {
    "text": "and some others are from\nother universities, that give a good overview of what\nexplainability is, especially",
    "start": "148930",
    "end": "155080"
  },
  {
    "text": "in the context of GNN. So before I dive\ninto explainability,",
    "start": "155080",
    "end": "160300"
  },
  {
    "text": "I also want to talk a bit about\ntrustworthy graph learning in general, because\nexplainability, in my opinion,",
    "start": "160300",
    "end": "168170"
  },
  {
    "text": "is really inseparable from\nmost other trustworthy context",
    "start": "168170",
    "end": "174220"
  },
  {
    "text": "or trustworthy research\ntopics in AI and GNN as well. So it covers, broadly\ncovers a wide range",
    "start": "174220",
    "end": "181330"
  },
  {
    "text": "of topics from explainability,\nfairness, robustness, privacy, basically any kind\nof issues that",
    "start": "181330",
    "end": "188560"
  },
  {
    "text": "would arise if we\nwere to let the model to run on like mission critical\nscenarios or when people are--",
    "start": "188560",
    "end": "195940"
  },
  {
    "text": "like human decision makers are\ninvolved in working together with the deep learning system.",
    "start": "195940",
    "end": "202200"
  },
  {
    "text": "And there are big challenges\nfor graph learning. In the context of\ngraph learning,",
    "start": "202200",
    "end": "208570"
  },
  {
    "text": "a lot of these problems\nbecome especially hard. So the role of graph\ntopology is, of course,",
    "start": "208570",
    "end": "214430"
  },
  {
    "text": "a key role when we talk\nabout graph learning, right? We always talk about message\npassing, structures, subgraphs,",
    "start": "214430",
    "end": "220900"
  },
  {
    "text": "things like that. Those are important\ncomponents that can contribute to model\nprediction, model robustness,",
    "start": "220900",
    "end": "228650"
  },
  {
    "text": "and so on, but\nexisting techniques that are common for\ndeep learning usually wouldn't capture those.",
    "start": "228650",
    "end": "234970"
  },
  {
    "text": "And also, it becomes\nespecially hard when those\ntopological structures are involved if we were to\ndo quantitative evaluation.",
    "start": "234970",
    "end": "241940"
  },
  {
    "text": "So this lecture will\npretty much cover introduction of some\nworks that try to solve",
    "start": "241940",
    "end": "248110"
  },
  {
    "text": "these kind of challenges. So the big picture is\nbasically like this. So I feel like in the area\nof trustworthy deep learning,",
    "start": "248110",
    "end": "256148"
  },
  {
    "text": "two very important topics\nare what we care about. Adversarial robustness\nand explainability.",
    "start": "256149",
    "end": "261459"
  },
  {
    "text": "And mostly other topics\nlike privacy, fairness, and accountability, all\nthese are usually borrow",
    "start": "261459",
    "end": "268830"
  },
  {
    "text": "or are built upon existing\ntechniques in robustness and explainability.",
    "start": "268830",
    "end": "274004"
  },
  {
    "text": "And each of these aspects\nplay a very critical role in gaining trust from users\nof deep learning systems,",
    "start": "274005",
    "end": "279270"
  },
  {
    "text": "from industry users to\nacademia to researchers, even to domain experts from\nlike interdisciplinary research",
    "start": "279270",
    "end": "286530"
  },
  {
    "text": "where we apply deep\nlearning to, say, the field of biology, physics, or\nother scientific disciplines.",
    "start": "286530",
    "end": "293340"
  },
  {
    "text": "And as I mentioned, the\nchallenge in the GNN context",
    "start": "293340",
    "end": "299190"
  },
  {
    "text": "is that the role of graph\ntopology plays a key role, and we need to quantify how much\nthe topology is contributing",
    "start": "299190",
    "end": "306090"
  },
  {
    "text": "to explaining a model\nor is contributing to robustness of a\nmodel, and we need the quantitative evaluation.",
    "start": "306090",
    "end": "312449"
  },
  {
    "text": "So today, I will first\ngive a brief introduction of what explainability is,\nbecause not everyone might",
    "start": "312450",
    "end": "321060"
  },
  {
    "text": "be familiar with\nthe term, and we'll talk about the problem\nsetting, especially in the context of graphs. And in the second part, I\nwill discuss GNNExplainer,",
    "start": "321060",
    "end": "330100"
  },
  {
    "text": "which is one of the very widely\nused algorithm for explaining GNN predictions right now.",
    "start": "330100",
    "end": "336460"
  },
  {
    "text": "And in the third part, I\nwill talk about evaluation. So I will talk\nabout in cases where",
    "start": "336460",
    "end": "343360"
  },
  {
    "text": "ground truth are not available,\nwhich is often the case, right? Like if you want\nto explain a model,",
    "start": "343360",
    "end": "348910"
  },
  {
    "text": "you don't always know\nwhat the model is doing, you don't know what should the\nground truth explanation be.",
    "start": "348910",
    "end": "355070"
  },
  {
    "text": "In those cases, how can we\nevaluate the performance of explainability? And there are actually\nmultiple dimensions",
    "start": "355070",
    "end": "361450"
  },
  {
    "text": "where we can say whether\nan explainability method is of a higher quality\nversus a lower quality.",
    "start": "361450",
    "end": "368860"
  },
  {
    "text": "So that's the three aspects. And we'll start with\nexplainability and its problem",
    "start": "368860",
    "end": "374880"
  },
  {
    "text": "setting. So we'll talk about\nmotivations, goals, and settings that we typically care about. And some of it or\nmost of it actually",
    "start": "374880",
    "end": "381250"
  },
  {
    "text": "applies to many, many other\nfields of deep learning as well. And I'll talk about\nthat, and I don't",
    "start": "381250",
    "end": "386882"
  },
  {
    "text": "talk about the specifics\nof it in the context of graph learning. So the key concept\nor the key problem",
    "start": "386882",
    "end": "395210"
  },
  {
    "text": "with any kind of trustworthy\ndeep learning aspects, including explainability,\nis that it",
    "start": "395210",
    "end": "401720"
  },
  {
    "text": "has this black-box nature, which\nI think by now every one of you should be familiar with.",
    "start": "401720",
    "end": "407250"
  },
  {
    "text": "So black-box nature\nbasically means that you do not know what\nexactly is the model using",
    "start": "407250",
    "end": "413840"
  },
  {
    "text": "as the important parts\nof the input that decides the prediction of the model.",
    "start": "413840",
    "end": "420080"
  },
  {
    "text": "And understanding or being\nable to open the black box is essential. And explainable AI or\nXAI is an umbrella term",
    "start": "420080",
    "end": "428510"
  },
  {
    "text": "for any research\nthat tries to solve this black-box problem for AI. And why is it useful?",
    "start": "428510",
    "end": "434940"
  },
  {
    "text": "First of all, it enables users\nto understand the decision making of the model.",
    "start": "434940",
    "end": "440420"
  },
  {
    "text": "It not only knows that\nthe model magically outputs this\nprediction, but we want to know why it goes from this--",
    "start": "440420",
    "end": "447710"
  },
  {
    "text": "how it arrives at the\nanswer given the input. It also helps us to gain\ntrust from human users",
    "start": "447710",
    "end": "454500"
  },
  {
    "text": "of the deep learning system. Imagine that you are\ncollaborating with a domain expert in, say,\nbiology, and they",
    "start": "454500",
    "end": "461310"
  },
  {
    "text": "want to know whether their\nknowledge in biological science is going to be applicable and\nis actually being captured",
    "start": "461310",
    "end": "467355"
  },
  {
    "text": "in the deep learning model. And that's super important. I also listed an archive paper\non a simple-to-read guide.",
    "start": "467355",
    "end": "473940"
  },
  {
    "text": "So this is for general\nexplainability. So if you're interested\nin this area in general, you should definitely\nhave a look.",
    "start": "473940",
    "end": "480300"
  },
  {
    "text": "But we're talking\nabout explainability for deep learning.",
    "start": "480300",
    "end": "485520"
  },
  {
    "text": "And we said it's\nespecially hard because of the black-box nature. But let's try to remember\nwhat was explainable",
    "start": "485520",
    "end": "492343"
  },
  {
    "text": "when we talk about the classical\nmachine learning methods, and maybe we can borrow\nsome of the insights",
    "start": "492343",
    "end": "497460"
  },
  {
    "text": "there in order to explain\nthe deep learning models.",
    "start": "497460",
    "end": "502620"
  },
  {
    "text": "So maybe let's have a quick\nrecap of what was explainable. So if I were to think about\nsomething that was explainable,",
    "start": "502620",
    "end": "512190"
  },
  {
    "text": "the first reaction that I\nhad was a linear regression. That's like the most\nfundamental method",
    "start": "512190",
    "end": "518729"
  },
  {
    "text": "you can ever think about\nin statistics and computer science. So you have some kind of\nprediction that is called y,",
    "start": "518730",
    "end": "525315"
  },
  {
    "text": "and then you have\nsome input features: x1, x2, x3, and all that. And then you have some\nweights, linear weights",
    "start": "525315",
    "end": "531750"
  },
  {
    "text": "associated with each of\nthese input features. So we call them\nW1, W2, and so on. And this was\nextremely explainable",
    "start": "531750",
    "end": "539310"
  },
  {
    "text": "just because you can\nhave a plot here. So you fit this data into\na line, a linear model.",
    "start": "539310",
    "end": "547290"
  },
  {
    "text": "And because it is\nlinear, you can naturally define this slope. So this green arrow here.",
    "start": "547290",
    "end": "554620"
  },
  {
    "text": "So it basically corresponds\nto each of these weights. Imagine that your x-axis\nis one of these axis,",
    "start": "554620",
    "end": "560220"
  },
  {
    "text": "like let's say x1, and\nyour y-axis is actually your prediction y, so\nyour slope directly",
    "start": "560220",
    "end": "566310"
  },
  {
    "text": "indicates how important the\nfeature x1 is to the output.",
    "start": "566310",
    "end": "573310"
  },
  {
    "text": "If the slope is very steep, that\nmeans the feature is actually critical for the model.",
    "start": "573310",
    "end": "578765"
  },
  {
    "text": "It has a large coefficient. If the slope is very\nflat, this feature",
    "start": "578765",
    "end": "585175"
  },
  {
    "text": "is basically not going to\ninfluence your prediction at all. So we can have some kind\nof explanation like this.",
    "start": "585175",
    "end": "591530"
  },
  {
    "text": "A change in delta x amount\nto, let's say, feature x1 is going to result in an\nincrease in prediction",
    "start": "591530",
    "end": "597680"
  },
  {
    "text": "by y amount and delta y amount. So that kind of\nsentence can tell us",
    "start": "597680",
    "end": "603050"
  },
  {
    "text": "how important a feature is. But this is not so easy\nin deep learning model, as you would imagine.",
    "start": "603050",
    "end": "608330"
  },
  {
    "text": "There isn't even a slope here. The second thing--\nintuition I would have",
    "start": "608330",
    "end": "613460"
  },
  {
    "text": "was dimension reduction. That's, again, like one of\nthe most fundamental models. We have some data in the\nvery high dimensional space.",
    "start": "613460",
    "end": "620840"
  },
  {
    "text": "We project them down into\nlow dimensional manifolds, and then we can do\nlike clustering,",
    "start": "620840",
    "end": "627860"
  },
  {
    "text": "we can-- importantly, we can\ncare about decision boundaries. Let's say we have the red\nclasses and the green classes,",
    "start": "627860",
    "end": "634370"
  },
  {
    "text": "and we can draw those\ndecision boundaries. Some of those give\nlinear boundaries, some models give\naccess line boundaries,",
    "start": "634370",
    "end": "640100"
  },
  {
    "text": "some models give\nnon-trivial boundaries. And in deep learning,\nthese kind of boundaries can be very complicated,\nbut it's something",
    "start": "640100",
    "end": "647660"
  },
  {
    "text": "that we want to understand\nin order to explain a deep learning model.",
    "start": "647660",
    "end": "653030"
  },
  {
    "text": "The last thing I want\nto talk about in terms of what was explainable\nis kind of decision tree.",
    "start": "653030",
    "end": "658670"
  },
  {
    "text": "Again, it's like something\nyou would imagine. It's extremely like\nself explainable.",
    "start": "658670",
    "end": "664610"
  },
  {
    "text": "Like I will say\nyou don't even have to have some additional\nthings to think about.",
    "start": "664610",
    "end": "670860"
  },
  {
    "text": "Like let's say you\nhave a decision tree, you know exactly what\neach feature plays",
    "start": "670860",
    "end": "676263"
  },
  {
    "text": "a role in the decision tree. And we can say something that\nis at the aggregate level.",
    "start": "676263",
    "end": "682440"
  },
  {
    "text": "For example, if the condition\nof some kind of node is met, let's say the node is the petal\nlength of this classical data",
    "start": "682440",
    "end": "690089"
  },
  {
    "text": "set, the petal length is less\nthan 4.75, it is likely-- 80% of the time, it is likely\nto be classified as true.",
    "start": "690090",
    "end": "698070"
  },
  {
    "text": "So it's just like a direct\nconsequence of a decision tree. OK, so now, having revisited\nsome of the traditionally very",
    "start": "698070",
    "end": "706200"
  },
  {
    "text": "explainable models,\nthe question now is, how can we\nbring some of these or retain some of\nthese explainability",
    "start": "706200",
    "end": "713490"
  },
  {
    "text": "when we transfer to a\ndeep version of the model? And indeed, there\nare many ways to do",
    "start": "713490",
    "end": "720190"
  },
  {
    "text": "that in the general\nfield of explainable AI. And typically,\ncorresponding to the slope,",
    "start": "720190",
    "end": "728180"
  },
  {
    "text": "the linear regression\nslope that I talked about, there are techniques\nsuch as the saliency that",
    "start": "728180",
    "end": "734589"
  },
  {
    "text": "talk about the importance\nof, say, words features, nodes in the graph.",
    "start": "734590",
    "end": "740990"
  },
  {
    "text": "It tries to extract these\nimportant structure. And importantly for\ngraphs, we might even have to extract the important\nscores for subgraphs.",
    "start": "740990",
    "end": "749440"
  },
  {
    "text": "So this is definitely\none approach that is inspired from\nlinear regression.",
    "start": "749440",
    "end": "754720"
  },
  {
    "text": "Another kind of approach\nis called attribution, which means that\nthis kind of feature",
    "start": "754720",
    "end": "760209"
  },
  {
    "text": "contributes to x percent\nof our prediction. This is also definitely a\nresult of dimension reduction",
    "start": "760210",
    "end": "767200"
  },
  {
    "text": "and linear regression\ninspired from this kind of explainable models. And then finally, there is\nalso concepts and prototypes.",
    "start": "767200",
    "end": "774410"
  },
  {
    "text": "So it talks about, in\ngeneral, this class has certain kind\nof characteristics. You can see them as\nlike a particular group",
    "start": "774410",
    "end": "779900"
  },
  {
    "text": "under like a branch\nof a decision tree. So those are methods\nthat are also",
    "start": "779900",
    "end": "786680"
  },
  {
    "text": "trying to bring some of\nthose traditionally very explainable models to the\ndeep learning context.",
    "start": "786680",
    "end": "793520"
  },
  {
    "text": "All right, so another aspect\nis that explainable AI can",
    "start": "793520",
    "end": "798710"
  },
  {
    "text": "be different from\ndifferent domains, but underlying all\nthese applications, we have a very\nsimilar principle.",
    "start": "798710",
    "end": "804650"
  },
  {
    "text": "So for example, in\ncomputer vision, what was explainable, right? So let's say we\nhave an image here,",
    "start": "804650",
    "end": "810770"
  },
  {
    "text": "and we want to talk about why\nis it being classified as, say, like a cat with\nsome probability or a dog",
    "start": "810770",
    "end": "816530"
  },
  {
    "text": "with some probability. And we can look at it with\nrespect to the dog logit.",
    "start": "816530",
    "end": "823070"
  },
  {
    "text": "So treat the vector on the\nright as the prediction logit after the softmax.",
    "start": "823070",
    "end": "828800"
  },
  {
    "text": "So if we look at\nthe cat, then it highlights the portion\nof the cat at the bottom. So this was kind of\nsaliency map that highlights",
    "start": "828800",
    "end": "836000"
  },
  {
    "text": "importance of each pixel. So it has a cat. And then if we switch\nit to the dog logit,",
    "start": "836000",
    "end": "843140"
  },
  {
    "text": "then it's going to tell us what\nwas the part of the image that",
    "start": "843140",
    "end": "849170"
  },
  {
    "text": "helps the model decide that\nit's kind of like a dog image, and then the dog\npart was highlighted. So this is an example\nin computer vision,",
    "start": "849170",
    "end": "856620"
  },
  {
    "text": "but we see the pattern, right? So we want to\nextract like a subset of the input that\nare particularly",
    "start": "856620",
    "end": "862230"
  },
  {
    "text": "important for the prediction. That's what we did in\nlinear regression, that's also what we're doing in\nCV, and as we will see,",
    "start": "862230",
    "end": "868740"
  },
  {
    "text": "it's also the case for\nNLP and graphs as well. Let's say in the context of\nnatural language processing,",
    "start": "868740",
    "end": "874020"
  },
  {
    "text": "we have some input, and\nwe're doing this sentiment classification. We're predicting\nwhether the sentence",
    "start": "874020",
    "end": "879930"
  },
  {
    "text": "has a positive or\nnegative sentiment. We run through some kind of\nlanguage model, whatever,",
    "start": "879930",
    "end": "885602"
  },
  {
    "text": "and the explanation\nis going to highlight the portion of\nthe sentences that contribute to the sentiment.",
    "start": "885602",
    "end": "890850"
  },
  {
    "text": "For example, the\nentertaining or cool effects, those have high\nimportance values",
    "start": "890850",
    "end": "896490"
  },
  {
    "text": "to the prediction of being\nlike positive sentiment. On the other hand, if you\nlook at the bottom row,",
    "start": "896490",
    "end": "901650"
  },
  {
    "text": "there's the negative part. There is a bad and peeling off.",
    "start": "901650",
    "end": "907410"
  },
  {
    "text": "So these words that have\nnegative connotations will get highlighted. So again, it's a\nsubset of the input",
    "start": "907410",
    "end": "912900"
  },
  {
    "text": "that particularly contributes\nto the prediction made by the model.",
    "start": "912900",
    "end": "918389"
  },
  {
    "text": "Again, same for graph learning. This is what we're going\nto talk about today. Again, the same principle.",
    "start": "918390",
    "end": "924270"
  },
  {
    "text": "We're going to extract\na subset of the input such that it contributes to\na particular prediction made",
    "start": "924270",
    "end": "930750"
  },
  {
    "text": "by the model. But in this case,\nit's more complicated because we not only have a\nsubset of feature dimensions,",
    "start": "930750",
    "end": "937320"
  },
  {
    "text": "which we usually\ncare about, let's say in similar to a subset of\npixels and stuff like that,",
    "start": "937320",
    "end": "943089"
  },
  {
    "text": "but there's also this\nstructural component. We want to extract\nthe subset of graphs,",
    "start": "943090",
    "end": "948209"
  },
  {
    "text": "the topological structure,\nlike a subgraph, such that it particularly contributes\nto the model prediction.",
    "start": "948210",
    "end": "954570"
  },
  {
    "text": "So keep in mind we have\nboth aspects to capture when we go through the methods. ",
    "start": "954570",
    "end": "963990"
  },
  {
    "text": "So why is this important? Why do we need to\ncapture structure? Why do we need to\ncapture feature at the same time for\nGNN explainability?",
    "start": "963990",
    "end": "971139"
  },
  {
    "text": "So typically, we have\ntwo kinds of purposes",
    "start": "971140",
    "end": "977040"
  },
  {
    "text": "in order to explain\nboth aspects. So we might sometimes\nwant the GNNs to,",
    "start": "977040",
    "end": "982980"
  },
  {
    "text": "or we want to explain the\nground truth phenomenon. Let's say I have a molecule\nproperty prediction task,",
    "start": "982980",
    "end": "989140"
  },
  {
    "text": "right? Given a molecule, I predict\nwhether it is toxic or not. Something like that. So we want to understand what\nmakes the molecule toxic.",
    "start": "989140",
    "end": "997920"
  },
  {
    "text": "We don't just want a model\nthat magically outputs me toxic versus\nnon toxic, but we",
    "start": "997920",
    "end": "1003259"
  },
  {
    "text": "want to find the reasons\nwhy it is toxic or not. So that's-- like trying to\ninvestigate the ground truth",
    "start": "1003260",
    "end": "1010460"
  },
  {
    "text": "phenomenon is particularly\nimportant in like any kind of science applications.",
    "start": "1010460",
    "end": "1015560"
  },
  {
    "text": "But on the bottom side,\nit's about explaining model prediction. Let's say this is\nanother scenario where",
    "start": "1015560",
    "end": "1023150"
  },
  {
    "text": "we have a customer that is\napplying for a loan in a bank, and we have a bank transaction\nnetwork, we run a graph neural",
    "start": "1023150",
    "end": "1030650"
  },
  {
    "text": "net on top of it, we\npredict whether it is advisable to approve\nthe loan for the customer.",
    "start": "1030650",
    "end": "1039380"
  },
  {
    "text": "So it's, again, a binary\nclassification, yes or no, and in this case, it says no.",
    "start": "1039380",
    "end": "1044510"
  },
  {
    "text": "So what we want to\nunderstand in this case is why does the model recommend\nno loan for this person?",
    "start": "1044510",
    "end": "1050150"
  },
  {
    "text": "Is it because it has\na low credit history, is it because it has-- the\nloan amount is too big, some kind of reason?",
    "start": "1050150",
    "end": "1056450"
  },
  {
    "text": "We don't just want\nto deny a customer because the model predicts no. We want to give them\nreasons that can help",
    "start": "1056450",
    "end": "1062690"
  },
  {
    "text": "them be successful next time. So instead of explaining\nthe ground truth, we're more like\nexplaining the model.",
    "start": "1062690",
    "end": "1069920"
  },
  {
    "text": "You can think of it as almost\nlike debugging the model to see why does\nthe model recommend",
    "start": "1069920",
    "end": "1075260"
  },
  {
    "text": "no loan for this person? Is it because of this feature,\nis it because of that feature, is this a fair thing to do?",
    "start": "1075260",
    "end": "1081980"
  },
  {
    "text": "Let's say if the model-- we discover through\nexplainability that the model is\nrecommending no loan because",
    "start": "1081980",
    "end": "1089630"
  },
  {
    "text": "of some protected\ncharacteristics, such as gender, race,\nthis kind of thing,",
    "start": "1089630",
    "end": "1095190"
  },
  {
    "text": "that's definitely like a no. We shouldn't be doing\nthat, and that's what explainability can help.",
    "start": "1095190",
    "end": "1101370"
  },
  {
    "text": "And in this case,\ngender, stuff like that can be like a feature, whereas\ntransaction behavior can",
    "start": "1101370",
    "end": "1109170"
  },
  {
    "text": "be like a subgraph. So that's exactly what\nwe talk about when we said a subgraph\nand a feature both",
    "start": "1109170",
    "end": "1115440"
  },
  {
    "text": "are important for explaining\nsuch a prediction.",
    "start": "1115440",
    "end": "1120840"
  },
  {
    "text": "And I also listed some\nreferences in case people are interested. So there have been a\nlot of works in terms",
    "start": "1120840",
    "end": "1126510"
  },
  {
    "text": "of explainable AI in general. So there are like\nproxy-based approach. SHAP is a very popular one\nused in industry and academia",
    "start": "1126510",
    "end": "1134309"
  },
  {
    "text": "as well. There are saliency or\ngradient-based methods such as Grad-CAM, there\nis attention mechanism.",
    "start": "1134310",
    "end": "1141960"
  },
  {
    "text": "We talked about graph\nattention models here. And we get those\nattention values, and they can be\nquite useful when",
    "start": "1141960",
    "end": "1147540"
  },
  {
    "text": "explaining model prediction. We can look at which one is the\ncurrent node paying attention to, and is that node--",
    "start": "1147540",
    "end": "1154500"
  },
  {
    "text": "that kind of pattern\nimportant for the prediction? So attention is definitely like\nanother important approach.",
    "start": "1154500",
    "end": "1161850"
  },
  {
    "text": "And the final part\nof section one is just going to be\nreasons why explainability",
    "start": "1161850",
    "end": "1168270"
  },
  {
    "text": "is an important topic. First of all, it gives\nus a sense of trust.",
    "start": "1168270",
    "end": "1173790"
  },
  {
    "text": "We understand what the\nBlack box is doing, we no longer treat\nit as magic, we are able to understand\nwhat are the features that",
    "start": "1173790",
    "end": "1179940"
  },
  {
    "text": "cause this model to make\ncertain predictions. That's very important for\nfairness and debugging",
    "start": "1179940",
    "end": "1185159"
  },
  {
    "text": "and many other reasons. And sometimes it also gives\nus a sense of causality.",
    "start": "1185160",
    "end": "1190559"
  },
  {
    "text": "It's not always the\ncase, but a lot of times, it gives us some\nkind of causality. So like certain feature\nor certain kind of input",
    "start": "1190560",
    "end": "1197850"
  },
  {
    "text": "causes a prediction\nto be like that. So we can-- depending on\nwhether the outcome is something",
    "start": "1197850",
    "end": "1203820"
  },
  {
    "text": "we desire, we might\nwant to fix it, or we might want to just utilize\nthat kind of inductive bias",
    "start": "1203820",
    "end": "1209880"
  },
  {
    "text": "to understand the\nphenomenon better. It also has a big implication\nin terms of transferability.",
    "start": "1209880",
    "end": "1217120"
  },
  {
    "text": "Let's give you a\nvery simple example. So if I'm-- let's say in my\ndata set, I have a lot of dogs.",
    "start": "1217120",
    "end": "1225220"
  },
  {
    "text": "I want to classify whether\nthe image has a dog or not. I have a lot of dogs, but\nevery time a dog appears,",
    "start": "1225220",
    "end": "1230230"
  },
  {
    "text": "there's also a\nperson that appears. So that's just a\nparticular characteristic of this data set.",
    "start": "1230230",
    "end": "1235390"
  },
  {
    "text": "And we can use\nexplainability to see whether the reason\nwe're predicting a dog",
    "start": "1235390",
    "end": "1241720"
  },
  {
    "text": "is because there's a dog\nor there is a person. And it's just like there is a\nhigh correlation between person",
    "start": "1241720",
    "end": "1248710"
  },
  {
    "text": "and dog in this\nparticular data set. So understanding which one\nis going to be the case",
    "start": "1248710",
    "end": "1254440"
  },
  {
    "text": "is going to have a big\nimplication in terms of transferability, because if\nyou have a new data set that",
    "start": "1254440",
    "end": "1259780"
  },
  {
    "text": "has dogs but don't\nhave any person in it, then of course your model\nwouldn't be able to transfer.",
    "start": "1259780",
    "end": "1268300"
  },
  {
    "text": "And finally, I talked about\nfairness and ethical decision making. We want to think about\nwhether the explainability",
    "start": "1268300",
    "end": "1275290"
  },
  {
    "text": "or explanation made by\nthe model is something that's reasonable to do. And if it's not, we should\ndo something that fixes it.",
    "start": "1275290",
    "end": "1282970"
  },
  {
    "text": "But explainability\nlet us at least investigate this phenomenon. ",
    "start": "1282970",
    "end": "1290650"
  },
  {
    "text": "So I will also talk\nabout the settings here. There are many ways\nwe can formulate",
    "start": "1290650",
    "end": "1296080"
  },
  {
    "text": "this problem of explainability. And here is different ways that\nwe can formulate the setting.",
    "start": "1296080",
    "end": "1303049"
  },
  {
    "text": "So first of all, we\ncan explain by target. So we can say, it\nis an explanation for a particular instance\nversus an explanation",
    "start": "1303050",
    "end": "1310810"
  },
  {
    "text": "for the entire model, or for a\nparticular class of the model. Instance-level means that I\nfocus on one particular data",
    "start": "1310810",
    "end": "1317140"
  },
  {
    "text": "point. Let's say this image or\nthis node in a graph, and say, why is this\nnode being predicted as no loan for this customer?",
    "start": "1317140",
    "end": "1326180"
  },
  {
    "text": "And so we're only going to\nexplain for that instance, right? The explanation is\nspecific to that instance. Whereas model-level\nexplanation would mean",
    "start": "1326180",
    "end": "1334330"
  },
  {
    "text": "what is the general\ncharacteristic of a toxic molecule? So it might give us some\nunderstanding of the functional",
    "start": "1334330",
    "end": "1342670"
  },
  {
    "text": "group of the molecule, some\ninteraction between atoms, what, in general, a toxic\nmolecule looks like.",
    "start": "1342670",
    "end": "1349610"
  },
  {
    "text": "So those are more like\nmodel-level or class-level explanation. And in particular,\nwe're going to focus on",
    "start": "1349610",
    "end": "1355600"
  },
  {
    "text": "in this instance level because\nI guess it's the first step. Model-level explanations\nare out there.",
    "start": "1355600",
    "end": "1361420"
  },
  {
    "text": "There are some papers\non it, but it's still a very active area of research.",
    "start": "1361420",
    "end": "1367820"
  },
  {
    "text": "And in fact, I'm doing\nlike an ongoing work for model-level explanation\non GNNS as well.",
    "start": "1367820",
    "end": "1374725"
  },
  {
    "text": "And settings can also be\nclassified in other ways. We can also say explainability\nsettings by stages.",
    "start": "1374725",
    "end": "1380289"
  },
  {
    "text": "We can have explainability being\ninjected into this pipeline at different stages.",
    "start": "1380290",
    "end": "1385360"
  },
  {
    "text": "We can inject it before\nthe model fitting. So we can think of it as like\nbeing inherent in the model.",
    "start": "1385360",
    "end": "1393140"
  },
  {
    "text": "Let's say I have a model\nthat is intrinsically explainable or\ninterpretable, which means that you can use attention\nor something like that,",
    "start": "1393140",
    "end": "1399720"
  },
  {
    "text": "and once the model is\noptimized, we automatically have that explanation. So that's often called\nante-hoc and sometimes called",
    "start": "1399720",
    "end": "1407210"
  },
  {
    "text": "intrinsic interpretability. And there's another\ntype of explainability which we are going\nto focus on that",
    "start": "1407210",
    "end": "1413330"
  },
  {
    "text": "is post-hoc explainability. So this means that I have a\nmodel that is already trained. Let's say it's a pre-trained\nmodel or like any kind",
    "start": "1413330",
    "end": "1421280"
  },
  {
    "text": "of network where we\nalready fixed the weights, it's already well optimized. And we want to have an\nadditional module that's",
    "start": "1421280",
    "end": "1428330"
  },
  {
    "text": "built on top of\nit, we just bring in this additional\nexplainability module",
    "start": "1428330",
    "end": "1433370"
  },
  {
    "text": "to investigate why the model\nmakes certain prediction. So that's often called\npost-hoc explainability.",
    "start": "1433370",
    "end": "1440390"
  },
  {
    "text": "The disadvantage is, of course,\nyou have an additional module component, it's not like\nintrinsic to the model,",
    "start": "1440390",
    "end": "1446780"
  },
  {
    "text": "so it requires post-processing\nand even like optimization, as we will see, to\ngenerate the explanation.",
    "start": "1446780",
    "end": "1453510"
  },
  {
    "text": "That's more time\nconsuming and so on, but the advantage is that it's\nkind of agnostic to the model.",
    "start": "1453510",
    "end": "1459150"
  },
  {
    "text": "We don't require the model\nitself to be interpretable. We don't require it\nto have attention.",
    "start": "1459150",
    "end": "1464830"
  },
  {
    "text": "We don't require\nit to have, say, some kind of linear components\nor anything like that.",
    "start": "1464830",
    "end": "1471240"
  },
  {
    "text": "Any kind of black-box\nmodel, we should be able to apply\npost-hoc explainability. So that's kind of the difference\nin terms of their stages.",
    "start": "1471240",
    "end": "1479020"
  },
  {
    "text": "And another-- Yes. A short question. So I'm assuming there's\nalso learned weights",
    "start": "1479020",
    "end": "1484630"
  },
  {
    "text": "for the explainability model\nthat happens post-hoc, right? Yeah. So what is stopping us from then\ngoing deep learning on that too",
    "start": "1484630",
    "end": "1490030"
  },
  {
    "text": "and then having a\nmodel that explains the explanation\nof the model that explains the explainer,\nthe explainer of the model? Yeah, yeah, yeah, there are\npeople who actually do that.",
    "start": "1490030",
    "end": "1496953"
  },
  {
    "text": "Like in CV for\nexample, last year, I remember there\nwas a paper, you use a deep learning model\nto generate an explanation.",
    "start": "1496953",
    "end": "1502820"
  },
  {
    "text": "In fact, a deep generative\nmodel to generate the explanation that\nexplains another deep image",
    "start": "1502820",
    "end": "1509800"
  },
  {
    "text": "classification model. So there are definitely\npeople who do that. And a lot of times,\nthose actually give us a lot\nbetter performance.",
    "start": "1509800",
    "end": "1517000"
  },
  {
    "text": "The only thing that can\nbe a little bit worrying is it kind of applies\nlike all the problems",
    "start": "1517000",
    "end": "1524530"
  },
  {
    "text": "of like learned classifier. So like we might be worried\nthat the model is not robust,",
    "start": "1524530",
    "end": "1530020"
  },
  {
    "text": "it's not predicting us\nthe correct explanation, is not stable. if I perturb the\npixels a little bit,",
    "start": "1530020",
    "end": "1537399"
  },
  {
    "text": "it predicts completely different\nthings, different things as explanation. So like all these issues.",
    "start": "1537400",
    "end": "1544710"
  },
  {
    "text": "But it's definitely like\na possible approach. People always try to do that. And you mentioned\nthis recursion, right?",
    "start": "1544710",
    "end": "1551180"
  },
  {
    "text": "Like explaining something\nthat is-- trying to explain something else. I haven't seen\npeople doing that. But yeah, I don't know if\nthere is any use case of such,",
    "start": "1551180",
    "end": "1561170"
  },
  {
    "text": "but I think people\nare on the first level currently, like trying\nto just generate",
    "start": "1561170",
    "end": "1566629"
  },
  {
    "text": "a good explanation for\na deep learning model, not really investigating what\nthe explainer itself is doing.",
    "start": "1566630",
    "end": "1572810"
  },
  {
    "text": "But you can imagine\nthat once we have a very good universal explainer,\nlet's say as good as, say,",
    "start": "1572810",
    "end": "1578510"
  },
  {
    "text": "a large language model nowadays,\nthen we can come up with ways additionally to investigate\nwhat the explainer is doing.",
    "start": "1578510",
    "end": "1586130"
  },
  {
    "text": "But that's definitely like\na natural line of thought. So yeah, anyways good question.",
    "start": "1586130",
    "end": "1592630"
  },
  {
    "text": "So the third kind of\nsetting difference is that applicability can\nalso specify the setting.",
    "start": "1592630",
    "end": "1600720"
  },
  {
    "text": "We can be model-specific. That means that the\nexplainability method is only going to be applied to a\ncertain class of models.",
    "start": "1600720",
    "end": "1607260"
  },
  {
    "text": "But we can also try to\nbe model agnostic, which means that no matter what the\nunderlying model is doing,",
    "start": "1607260",
    "end": "1612870"
  },
  {
    "text": "we are always able to\nexplain something out of it. Or at least model\nagnostic to some extent,",
    "start": "1612870",
    "end": "1618870"
  },
  {
    "text": "let's say any kind of GNNs\nor any kind of transformers,",
    "start": "1618870",
    "end": "1624315"
  },
  {
    "text": "things like that. So I think I've finished\neverything about explainability",
    "start": "1624315",
    "end": "1630080"
  },
  {
    "text": "and problem setting. Let me know if there is\nany question by the way. Feel free to raise your hand\nif there's any question.",
    "start": "1630080",
    "end": "1637550"
  },
  {
    "text": "But I will give you\nguys an example of what we have done on a GNN explainer,\nwhich is the first and very",
    "start": "1637550",
    "end": "1645380"
  },
  {
    "text": "commonly used GNN explainability\nmethods that is still quite popular nowadays.",
    "start": "1645380",
    "end": "1652010"
  },
  {
    "text": "And I'll go on to how\nwe evaluate this thing. So I think we talked about\nthis example already.",
    "start": "1652010",
    "end": "1659630"
  },
  {
    "text": "We have some kind of\ntransaction networks in the financial markets. And it can be formulated as\na graph, where we have nodes",
    "start": "1659630",
    "end": "1666560"
  },
  {
    "text": "connected through transactions. And we can use a\ngraph neural network to predict whether each user\nis trustworthy or risky client,",
    "start": "1666560",
    "end": "1674300"
  },
  {
    "text": "and then we can\ndecide whether we want to give a loan\nto the person or not. And there are a lot\nmore applications.",
    "start": "1674300",
    "end": "1680929"
  },
  {
    "text": "I'm sure you guys are\nalready exposed to them after taking this class. For example, in the\nrecommender system,",
    "start": "1680930",
    "end": "1687090"
  },
  {
    "text": "people get recommended\nwith different items. You can see it as a kind\nof a predicting what",
    "start": "1687090",
    "end": "1692940"
  },
  {
    "text": "the person might buy in future. It can also be a\nmolecule classification. I think in the\npast few lectures,",
    "start": "1692940",
    "end": "1698552"
  },
  {
    "text": "you might have talked about\nmolecule generation or molecule property prediction, predicting\nthe property of the molecule",
    "start": "1698552",
    "end": "1705240"
  },
  {
    "text": "as a whole graph. And I have the fraudulent, the\nfinancial network example here.",
    "start": "1705240",
    "end": "1710350"
  },
  {
    "text": "So in those cases,\nwe are explaining different kinds of targets. So in the first one, we're\nexplaining link prediction.",
    "start": "1710350",
    "end": "1716010"
  },
  {
    "text": "We're explaining what is\nconnecting each other, what will be the future connection.",
    "start": "1716010",
    "end": "1721860"
  },
  {
    "text": "In the second case, we're\nexplaining graph classification because we are having one\nlabel for an entire molecule",
    "start": "1721860",
    "end": "1729120"
  },
  {
    "text": "as a graph. And in the third\ncase, it's going to be a node classification.",
    "start": "1729120",
    "end": "1734429"
  },
  {
    "text": "Every node represents\na user, and we want to see-- investigate\nwhy the user is, say, classified as fraudulent\nor not trustworthy, risky,",
    "start": "1734430",
    "end": "1741570"
  },
  {
    "text": "and all that. so yeah, different targets. So let's consider all of these.",
    "start": "1741570",
    "end": "1747660"
  },
  {
    "text": "So as I said, this\npipeline is going to be a post-hoc\nframework, which",
    "start": "1747660",
    "end": "1753390"
  },
  {
    "text": "means that this is going to be\nan extra module that we bring",
    "start": "1753390",
    "end": "1758730"
  },
  {
    "text": "in and try to explain the GNN\nthat we have already trained.",
    "start": "1758730",
    "end": "1764190"
  },
  {
    "text": "it's the post-hoc setting. So how it works is going to\nbe like at training time, we optimize the GNN on the\ntraining graph as usual, right?",
    "start": "1764190",
    "end": "1770910"
  },
  {
    "text": "We train, let's say, a\nGNN for the molecule, and then we save\nthe training model. So that model is kind\nof fixed already.",
    "start": "1770910",
    "end": "1778179"
  },
  {
    "text": "And at test time,\nwe are explaining the prediction made by the GNN. So let's say GNN make prediction\nthat the molecule is toxic,",
    "start": "1778180",
    "end": "1786330"
  },
  {
    "text": "now say if we're on\nan unseen instance, we have never seen this\ninstance before, have never",
    "start": "1786330",
    "end": "1791554"
  },
  {
    "text": "seen this molecule\nbefore, and we want to understand what\nmakes this molecule toxic. And that is where we bring\nthe explainability module.",
    "start": "1791555",
    "end": "1798660"
  },
  {
    "text": "As the student just said,\nit can be a deep module, it can be a very shallow module.",
    "start": "1798660",
    "end": "1804930"
  },
  {
    "text": "And in fact, GNNExplainer is\nsort of like a shallow module, I would say. So it's not something\nlike a deep model that",
    "start": "1804930",
    "end": "1810420"
  },
  {
    "text": "is optimizing for explanation,\nbut like a lot of deep model",
    "start": "1810420",
    "end": "1815610"
  },
  {
    "text": "based explanations are\navailable nowadays as well. ",
    "start": "1815610",
    "end": "1822480"
  },
  {
    "text": "So these are the overall goals. We want to explain different\nkinds of tasks, right?",
    "start": "1822480",
    "end": "1828000"
  },
  {
    "text": "No classification, graph\nclassification, or whatever. We also want the\nexplanation to be post-hoc,",
    "start": "1828000",
    "end": "1833549"
  },
  {
    "text": "which means it is\nmodel agnostic. We want to be able to apply it\nto any kind of message passing networks.",
    "start": "1833550",
    "end": "1839290"
  },
  {
    "text": "And we also want\nthe predictions to-- like we also want\nto take into account",
    "start": "1839290",
    "end": "1845370"
  },
  {
    "text": "that some of the\npredictions are based on a complex combination of\nnodes, edges between them,",
    "start": "1845370",
    "end": "1850410"
  },
  {
    "text": "motifs/subgraphs. Like not just the feature\ninformation but also the complex\nstructural information",
    "start": "1850410",
    "end": "1856530"
  },
  {
    "text": "and how that contributes\nto the model prediction. And one thing to note is\nthat different from, say,",
    "start": "1856530",
    "end": "1863039"
  },
  {
    "text": "computer vision,\ngradient is typically less reliable on real-world\ngraphs because a lot of times,",
    "start": "1863040",
    "end": "1868380"
  },
  {
    "text": "there is this discrete\nnature of having an edge versus not having an edge. It's kind of 0, 1 scenario,\nand gradients are often",
    "start": "1868380",
    "end": "1877470"
  },
  {
    "text": "behaving less well compared\nto continuous things, where,",
    "start": "1877470",
    "end": "1883000"
  },
  {
    "text": "for example, like pixel, you\ncan have values between say 0 to 255 or something, like\nthat's more continuous.",
    "start": "1883000",
    "end": "1890370"
  },
  {
    "text": "So in those cases,\nsaliency or gradient, which was a common method in\nCV, typically works less well.",
    "start": "1890370",
    "end": "1899820"
  },
  {
    "text": "So we're looking at model\nagnostic explanation, right? So the target is basically\nto be able to explain",
    "start": "1899820",
    "end": "1907860"
  },
  {
    "text": "any kind of message passing\nnetwork, so anything that uses this kind of\nmessage passing structure.",
    "start": "1907860",
    "end": "1915360"
  },
  {
    "text": "And we want to discover\nwhat is important. So by the way, this\nhas the potential",
    "start": "1915360",
    "end": "1920549"
  },
  {
    "text": "to apply to transformer and\nanything like that as well. So any time you have those kind\nof message passing structure.",
    "start": "1920550",
    "end": "1927565"
  },
  {
    "text": "And there are two\nkinds of explanations we want to extract. One is the structural\nexplanation, and one is the\nfeature explanation.",
    "start": "1927565",
    "end": "1933210"
  },
  {
    "text": "So feature explanation should\nbe very easy to understand. I crossed out some of\nthe feature dimensions. So this basically means\nthat some of the features",
    "start": "1933210",
    "end": "1940110"
  },
  {
    "text": "are less important, some of\nfeatures are more important. So we want to understand what\nhas contributed to the model",
    "start": "1940110",
    "end": "1946720"
  },
  {
    "text": "prediction. Is it this node feature or\nthat node feature, right?",
    "start": "1946720",
    "end": "1951970"
  },
  {
    "text": "Something like that. And at the same\ntime, we also want to extract the structural\nexplanation, which",
    "start": "1951970",
    "end": "1958120"
  },
  {
    "text": "is about the topology\nof the graph. So let's say the green\npart of this graph",
    "start": "1958120",
    "end": "1963730"
  },
  {
    "text": "are the edges that\ncontribute to a prediction, from the green part\nof the topology,",
    "start": "1963730",
    "end": "1969400"
  },
  {
    "text": "we can perhaps\nunderstand what has-- what has been going on and why\nis this structure corresponding",
    "start": "1969400",
    "end": "1975370"
  },
  {
    "text": "to toxic molecule, let's say. So maybe in the transaction\nnetwork example, this structural\nexplanation can be",
    "start": "1975370",
    "end": "1982570"
  },
  {
    "text": "something like fraudulent\ntransaction patterns. Let's say in the\nmolecule, it can",
    "start": "1982570",
    "end": "1989170"
  },
  {
    "text": "be some kind of\nsubstructure of the molecule that forms an important\nfunctional group for being like toxic\nor whatever property",
    "start": "1989170",
    "end": "1997480"
  },
  {
    "text": "that you are predicting. And the goal is to extract\nall of these simultaneously.",
    "start": "1997480",
    "end": "2003240"
  },
  {
    "text": "So let's go into the\nalgorithm a little bit. And before that,\nlet's just define some simple symbols, which\nI'm sure you all should be",
    "start": "2003240",
    "end": "2010590"
  },
  {
    "text": "quite familiar at this point. We have the input\ncomputation graph. So note that I'm using this term\nlike computation graph, which",
    "start": "2010590",
    "end": "2017190"
  },
  {
    "text": "means that it can be different\nfrom the original graph structure. So this is the graph where we\nused to do message passing.",
    "start": "2017190",
    "end": "2024330"
  },
  {
    "text": "And message passing edges can\nbe multi-hop message passing or sampled message passing.",
    "start": "2024330",
    "end": "2030810"
  },
  {
    "text": "So it might not be\nthe original graph. So we denote it as\nG under subscript C",
    "start": "2030810",
    "end": "2040020"
  },
  {
    "text": "for computation graph\nof a particular node V. And adjacency matrix of Gc.",
    "start": "2040020",
    "end": "2045450"
  },
  {
    "text": "We denote it as Ac,\nit's just the 0, 1 kind of representation.",
    "start": "2045450",
    "end": "2051239"
  },
  {
    "text": "And we can also consider\nweighted edges, in which case, this will be weighted.",
    "start": "2051239",
    "end": "2057599"
  },
  {
    "text": "And node features, we use Xc. These are all like-- so\nwhenever there's a subscript C,",
    "start": "2057600",
    "end": "2062820"
  },
  {
    "text": "this means the\noriginal computation graph or the original\ndata that we use as input.",
    "start": "2062820",
    "end": "2068460"
  },
  {
    "text": "So these are the set\nof all the features that belong to this\ncomputation graph. ",
    "start": "2068460",
    "end": "2075399"
  },
  {
    "text": "And we also have our GNN model. Remember this is post-hoc\nexplanation, which means the model is being fixed\nbefore we try to explain it.",
    "start": "2075400",
    "end": "2083770"
  },
  {
    "text": "We already have a model\nthat is well trained. So we call this model phi. And basically, without\nloss of generality,",
    "start": "2083770",
    "end": "2091552"
  },
  {
    "text": "I'm assuming, let's\nsay, classification. But regression is going to\nbe the very similar case, you just switch the loss.",
    "start": "2091552",
    "end": "2097420"
  },
  {
    "text": "But let's say it's\na classification. So we use P of phi,\nwhich is the model,",
    "start": "2097420",
    "end": "2102730"
  },
  {
    "text": "denotes the prediction\nprobability. Let's say it's a\nmulti-class classification, it's going to be like a\nsoftmax kind of distribution.",
    "start": "2102730",
    "end": "2110920"
  },
  {
    "text": "So a discrete distribution\nthat sums to 1, so we call that P of phi. And note that we wrote\nit as like the y which",
    "start": "2110920",
    "end": "2118450"
  },
  {
    "text": "is the prediction condition\non the computation graph, because for GNNs, computation\ngraph is all we need in order",
    "start": "2118450",
    "end": "2126100"
  },
  {
    "text": "to output this prediction y. So that's all the\ninformation the model has.",
    "start": "2126100",
    "end": "2131950"
  },
  {
    "text": "So the explanation has to be\nlike a subset of, as you can imagine, subset of Ac and Xc.",
    "start": "2131950",
    "end": "2140000"
  },
  {
    "text": "And what GNN explainer\noutputs is a subset, right? So As is kind of a subset of\nAc as you can sort of see it",
    "start": "2140000",
    "end": "2148220"
  },
  {
    "text": "as like an adjacency matrix\nfor a subgraph of Gc, the original graph.",
    "start": "2148220",
    "end": "2154250"
  },
  {
    "text": "And similarly, Xs is kind\nof a subset of the feature dimensions.",
    "start": "2154250",
    "end": "2159930"
  },
  {
    "text": "So this is only\nspecific to features that are included in the\nnodes in As, in the subgraph,",
    "start": "2159930",
    "end": "2166430"
  },
  {
    "text": "and then masked by some feature. So you can see on the\nvisualization on the right. Let's say this entire thing is\nthe Gc, the computation graph.",
    "start": "2166430",
    "end": "2173910"
  },
  {
    "text": "And then I'm highlighting\nsome of these green nodes. So these are the nodes that\nare part of the Gs, or part of",
    "start": "2173910",
    "end": "2180260"
  },
  {
    "text": "the As. Those nodes are in the\nexplained subgraph. And then on top of that,\nfor each of these nodes in--",
    "start": "2180260",
    "end": "2186680"
  },
  {
    "text": "for each of the green\nnodes, we cross out some of the dimensions. So there are some\nimportant dimensions being kept for nodes that are in\nthe green part of the subgraph.",
    "start": "2186680",
    "end": "2195890"
  },
  {
    "text": "So that's all we have\nin the explanation, so the important features and the\nimportant subgraph information.",
    "start": "2195890",
    "end": "2204110"
  },
  {
    "text": "So that way, we can capture\nboth the substructure and the important\nfeature dimensions.",
    "start": "2204110",
    "end": "2210680"
  },
  {
    "text": "So we use this mask-- as a feature mask that masks out\nall the features that are not",
    "start": "2210680",
    "end": "2216200"
  },
  {
    "text": "important to the prediction. So the objective\nfor explainability,",
    "start": "2216200",
    "end": "2222350"
  },
  {
    "text": "as we frame in this\npaper, is going to be mutual information,\nwhich is a very commonly used",
    "start": "2222350",
    "end": "2227810"
  },
  {
    "text": "objective in many\nother scenarios as well whenever we want to\ntalk about explainability or correlation, right?",
    "start": "2227810",
    "end": "2233030"
  },
  {
    "text": "To understand how\nfeatures are related-- feature predictions are related.",
    "start": "2233030",
    "end": "2238280"
  },
  {
    "text": "So here, we are kind\nof trying to maximize the mutual information\nbetween the output feature",
    "start": "2238280",
    "end": "2244670"
  },
  {
    "text": "as well as the input subgraph. So the subgraph of\nthe computation graph, you can imagine that\nan important subgraph",
    "start": "2244670",
    "end": "2251750"
  },
  {
    "text": "or an important subset of\nfeatures for this prediction must be highly\ncorrelated or must",
    "start": "2251750",
    "end": "2257060"
  },
  {
    "text": "have a high mutual information\nwith the prediction that is called y.",
    "start": "2257060",
    "end": "2262970"
  },
  {
    "text": "And this basically is like very\nbasic information theory stuff. So relation to entropy with\nmutual information is--",
    "start": "2262970",
    "end": "2270590"
  },
  {
    "text": "mutual information of X;Y\nis the entropy of x minus the conditional entropy\nof x, conditional on y.",
    "start": "2270590",
    "end": "2277340"
  },
  {
    "text": "But it can also be written\nthe other way around, like entropy of y\nminus H(Y) given x.",
    "start": "2277340",
    "end": "2283250"
  },
  {
    "text": "And it's symmetric\nby definition. So mutual information of X;Y is\nbasically the same as y and x.",
    "start": "2283250",
    "end": "2290405"
  },
  {
    "text": " And our objective is\ngoing to be maximizing",
    "start": "2290405",
    "end": "2296599"
  },
  {
    "text": "between the mutual\ninformation of label-- between the label\nand the explanation. So the label here is the y, the\nexplanation is the subgraph--",
    "start": "2296600",
    "end": "2305720"
  },
  {
    "text": "subgraph and subset of features. Which by expanding the\nprevious equation, we get this.",
    "start": "2305720",
    "end": "2312200"
  },
  {
    "text": "But something to note is\nthat the H(Y) is already fixed because the model\nis already trained.",
    "start": "2312200",
    "end": "2319430"
  },
  {
    "text": "Remember it's the\npost-hoc setting. So the model is already\ntrained, so H(Y) is fixed. And the thing that we\nreally want to maximize",
    "start": "2319430",
    "end": "2326630"
  },
  {
    "text": "is essentially the second\npart of the term, right? So the conditional entropy\nof y given the subgraph",
    "start": "2326630",
    "end": "2332599"
  },
  {
    "text": "and the subset of features,\nwhich are termed As an Xs. Does it make sense?",
    "start": "2332600",
    "end": "2339090"
  },
  {
    "text": "Any questions so far in terms\nof objectives and settings?",
    "start": "2339090",
    "end": "2344920"
  },
  {
    "text": "OK. Hopefully [INAUDIBLE] Yeah,\nshould be pretty clear.",
    "start": "2344920",
    "end": "2350390"
  },
  {
    "text": "So we want to-- Yes. Somehow like getting\npushback from like statistical\nmechanics, don't you",
    "start": "2350390",
    "end": "2356753"
  },
  {
    "text": "have to assume something\nabout independence in order to have that relationship\napplicable for the mutual [INAUDIBLE].",
    "start": "2356753",
    "end": "2362700"
  },
  {
    "text": "I think it's true\nfor any-- like there is no requirement for x and y\nbeing independent or anything.",
    "start": "2362700",
    "end": "2368770"
  },
  {
    "text": "It's just a symmetric function. Like if you write out the\nmutual information formula, which I forgot to write\nhere, I might have--",
    "start": "2368770",
    "end": "2374670"
  },
  {
    "text": "I might have to actually\nwrite it on slides, you can see it's\nkind of flippable.",
    "start": "2374670",
    "end": "2381330"
  },
  {
    "text": "Yeah, it's basically\nlike a joint divided by conditional or something. But that doesn't assume\nany independence.",
    "start": "2381330",
    "end": "2388560"
  },
  {
    "text": "We do have a lot of assumptions\nhere, as you will see, but not now.",
    "start": "2388560",
    "end": "2394440"
  },
  {
    "text": "All right, here's\nthe assumptions or the approximations\nthat we're doing. So as we said by relational\nentropy, like what we care",
    "start": "2394440",
    "end": "2403470"
  },
  {
    "text": "is actually the second\nterm because the first term H(Y) is pretty much fixed, given\nthe model is already trained.",
    "start": "2403470",
    "end": "2411060"
  },
  {
    "text": "So maximizing the\nmutual information is essentially minimizing\nthe conditional entropy of y",
    "start": "2411060",
    "end": "2416740"
  },
  {
    "text": "given this As and Fs just as-- sorry, Xs.",
    "start": "2416740",
    "end": "2421750"
  },
  {
    "text": "Just as a reminder, As is the\nadjacency for the subgraph that is important for the\nprediction, and again, Xs",
    "start": "2421750",
    "end": "2428289"
  },
  {
    "text": "is the subset of\nfeatures that are important for the prediction. So I did try to identify\nthese subgraphs.",
    "start": "2428290",
    "end": "2433750"
  },
  {
    "text": "And this is what we put it here. So the max is over all\npossible As's, which",
    "start": "2433750",
    "end": "2439120"
  },
  {
    "text": "is all possible subgraphs of\nthe original computation graph.",
    "start": "2439120",
    "end": "2444340"
  },
  {
    "text": "And yeah, I guess we\ncan put the F as well.",
    "start": "2444340",
    "end": "2449620"
  },
  {
    "text": "So this part is just\nfor like extracting the structural explanation. So I put As, but you\ncan also put the F,",
    "start": "2449620",
    "end": "2455080"
  },
  {
    "text": "so the feature\nmaps there as well for optimizing for all\npossible feature subsets.",
    "start": "2455080",
    "end": "2461380"
  },
  {
    "text": "One issue with this is that\nthis is very computationally expensive. Minimizing this\nconditional entropy",
    "start": "2461380",
    "end": "2467590"
  },
  {
    "text": "across all possible\nsubgraphs is exponential. You can imagine there\nare exponentially",
    "start": "2467590",
    "end": "2472630"
  },
  {
    "text": "many possible subgraphs of\na given computation graph. And what we thought can\nbe a good approximation",
    "start": "2472630",
    "end": "2480460"
  },
  {
    "text": "is that we treat\nthis explanation as a plausible explanation-- a distribution of\nplausible explanations,",
    "start": "2480460",
    "end": "2487270"
  },
  {
    "text": "and which we kind of\nbasically replace it with the expectation\nof that distribution.",
    "start": "2487270",
    "end": "2493340"
  },
  {
    "text": "So that's just a\nway to justify why we are doing this relaxation. So there are some\nbenefits of doing that.",
    "start": "2493340",
    "end": "2499330"
  },
  {
    "text": "first of all, we can\ncapture multiple plausible explanations. You can imagine that sometimes\nwhen predicting something,",
    "start": "2499330",
    "end": "2508600"
  },
  {
    "text": "a lot of explanations\ncan be reasonable. We can say like the\nperson is classified",
    "start": "2508600",
    "end": "2514840"
  },
  {
    "text": "as being risky because it\nhas this kind of features and interaction patterns\nat this point in time,",
    "start": "2514840",
    "end": "2521829"
  },
  {
    "text": "and it might also be because\nit has this kind of interaction patterns at some other time\nwith some other organizations.",
    "start": "2521830",
    "end": "2528650"
  },
  {
    "text": "So we can treat it as like a\nmultiple plausible explanations and treat it as a\ndistribution, right?",
    "start": "2528650",
    "end": "2535119"
  },
  {
    "text": "And this will also turn into\nlike a continuous optimization",
    "start": "2535120",
    "end": "2540220"
  },
  {
    "text": "rather than discrete, which\nincurs exponential runtime. This is just by\ndoing, like taking",
    "start": "2540220",
    "end": "2545770"
  },
  {
    "text": "expectation of the matrix. And this is what we're\ndoing, like what I",
    "start": "2545770",
    "end": "2551098"
  },
  {
    "text": "mean by taking the expectation. So let's say we define\nthis distribution",
    "start": "2551098",
    "end": "2556750"
  },
  {
    "text": "of all plausible\nexplanations as this curly A. So instead of minimizing\nthis conditional entropy,",
    "start": "2556750",
    "end": "2566020"
  },
  {
    "text": "we're now just minimizing\nthe expectation of the conditional\nentropy with this--",
    "start": "2566020",
    "end": "2572230"
  },
  {
    "text": "across all the\nplausible explanations. And this is the step of\napproximation that we're doing.",
    "start": "2572230",
    "end": "2579780"
  },
  {
    "text": "You notice that we basically\nbring in this expectation inside. That is true for\na convex function,",
    "start": "2579780",
    "end": "2585880"
  },
  {
    "text": "that is not true in general. So this is kind of\nlike an approximation. ",
    "start": "2585880",
    "end": "2593290"
  },
  {
    "text": "But like in practice, we observe\nthat it kind of works OK, and it also makes us much\nfaster in terms of runtime.",
    "start": "2593290",
    "end": "2599440"
  },
  {
    "text": "So that's why we're doing this. But if you look at the second\nequation, what this expectation",
    "start": "2599440",
    "end": "2604690"
  },
  {
    "text": "is, it's basically\nlike saying I have a multiple of these\nadjacency matrix,",
    "start": "2604690",
    "end": "2610300"
  },
  {
    "text": "I take the average of all\nthese adjacency matrix, that's our kind of expectation of all\nplausible explanations of As.",
    "start": "2610300",
    "end": "2618250"
  },
  {
    "text": "And we can treat this as\nkind of a continuous masking of the original adjacency.",
    "start": "2618250",
    "end": "2624400"
  },
  {
    "text": "So we have the original\nAc, this is the adjacency for the computation\ngraph, and we just",
    "start": "2624400",
    "end": "2629470"
  },
  {
    "text": "basically do an element-wise\nmultiply on the mask. And the mask is something\nthat is between 0 and 1, which",
    "start": "2629470",
    "end": "2635290"
  },
  {
    "text": "means that if we completely\nmask off something, it's going to have zero entry.",
    "start": "2635290",
    "end": "2640666"
  },
  {
    "text": "If the mask is 1, it doesn't\nmask anything, it will be 1, but there's going to be\nsomething in between, right?",
    "start": "2640666",
    "end": "2647170"
  },
  {
    "text": "A lot of values\nbetween 0 and 1, which makes it continuous rather\nthan a discrete optimization.",
    "start": "2647170",
    "end": "2652610"
  },
  {
    "text": "And if it's continuous, we\nhave gradients and everything, now we can do gradient\ndescent, right?",
    "start": "2652610",
    "end": "2658069"
  },
  {
    "text": "Life is good. So if the mask is\nclose to 1, that means the intuition is\nthat we keep the edge",
    "start": "2658070",
    "end": "2664220"
  },
  {
    "text": "because this edge is important. If the mask is 0,\nwe kind of remove it because this edge has nothing--",
    "start": "2664220",
    "end": "2670595"
  },
  {
    "text": "it's not important\nfor the prediction. So let's say we have this\nmask, to be more concrete.",
    "start": "2670595",
    "end": "2678680"
  },
  {
    "text": "This mask can be like kind\nof the values that we learn. So these are the\nentries that we will",
    "start": "2678680",
    "end": "2684770"
  },
  {
    "text": "optimize with\ngradient descent as like this post-hoc explainer. So everything in this\nmask is learnable.",
    "start": "2684770",
    "end": "2690589"
  },
  {
    "text": "And after we learn\nthe parameters, it will pass through\nthis sigmoid. So the sigmoid is\nbasically a squash",
    "start": "2690590",
    "end": "2696890"
  },
  {
    "text": "that ensures that your\nvalue is between 0 and 1. And then we element-wise\nmultiply that",
    "start": "2696890",
    "end": "2703250"
  },
  {
    "text": "with the adjacency,\nas shown here. So we basically replace\nthat expectation",
    "start": "2703250",
    "end": "2708380"
  },
  {
    "text": "with the adjacency of the\noriginal computation graph, doing the element-wise\nmultiplication with this 0",
    "start": "2708380",
    "end": "2713780"
  },
  {
    "text": "to 1 mask. Is that clear? Any questions so far? OK, awesome.",
    "start": "2713780",
    "end": "2719400"
  },
  {
    "text": " So now we have this. Remember, we replace\nthis expectation",
    "start": "2719400",
    "end": "2725710"
  },
  {
    "text": "with Ac element-wise mask. And this is our objective now.",
    "start": "2725710",
    "end": "2732340"
  },
  {
    "text": "And this is tractable, right? After doing this approximation. So remember this P phi is\nbasically the prediction",
    "start": "2732340",
    "end": "2739660"
  },
  {
    "text": "probability distribution by GNN\nwith this pretrained parameters",
    "start": "2739660",
    "end": "2744970"
  },
  {
    "text": "theta. We already trained this\nGNN, so this distribution can be easily computed by\ndoing a forward propagation",
    "start": "2744970",
    "end": "2751120"
  },
  {
    "text": "of the model. And we are minimizing\nover all possible M's.",
    "start": "2751120",
    "end": "2756880"
  },
  {
    "text": "Remember that we replace that\nAs with this Ac element-wise M. And Ac is fixed, so all we need\nto do is to optimize for all",
    "start": "2756880",
    "end": "2765130"
  },
  {
    "text": "possible M's. And M's is continuous. Remember after sigmoid, it's\nlike a number between 0 and 1,",
    "start": "2765130",
    "end": "2770920"
  },
  {
    "text": "a real number. So as an interpretation, it\ncan be something like that.",
    "start": "2770920",
    "end": "2776200"
  },
  {
    "text": "So imagine that\nbefore training, this is our Ac, the original\ncomputation graph adjacency",
    "start": "2776200",
    "end": "2781700"
  },
  {
    "text": "matrix. And after multiplying by\nthe mask which we trained, some of the entries become\nlighter in this heatmap",
    "start": "2781700",
    "end": "2788600"
  },
  {
    "text": "because the values drop closer\nto zero, and some of the values kept as like a higher\nvalue as 1, right?",
    "start": "2788600",
    "end": "2794660"
  },
  {
    "text": "So we have a-- we\ntreat this as sort of like a soft subgraph of Ac.",
    "start": "2794660",
    "end": "2799760"
  },
  {
    "text": "And that is kind of\npotential explanation. So after that, we can\njust do a thresholding to get rid of all these\nlighter edges, the edges that",
    "start": "2799760",
    "end": "2808400"
  },
  {
    "text": "have been masked out. And translating it\nto the graph domain,",
    "start": "2808400",
    "end": "2813747"
  },
  {
    "text": "it's going to be\nlike before training, we have a large graph,\nand after training, because some of the\nedges are masked out,",
    "start": "2813747",
    "end": "2819410"
  },
  {
    "text": "we have a smaller graph. We can ensure that\nit's connected by some other ways\nlike regularization",
    "start": "2819410",
    "end": "2824660"
  },
  {
    "text": "or like when we do thresholding,\nwe encourage connectedness, if that's something that\npeople need like having",
    "start": "2824660",
    "end": "2831350"
  },
  {
    "text": "a connected explanation. So yeah, that's the thing.",
    "start": "2831350",
    "end": "2838925"
  },
  {
    "text": "And so that is basically\nthe structure explanation. So we have extracted a subgraph\nout of the original graph,",
    "start": "2838925",
    "end": "2845349"
  },
  {
    "text": "that is particularly\nimportant for some prediction. we did what we promised. And this is the feature part.",
    "start": "2845350",
    "end": "2851740"
  },
  {
    "text": "And that's basically almost\nthe same as the subgraph part, except we are selecting a\nset of feature sub-dimensions",
    "start": "2851740",
    "end": "2860350"
  },
  {
    "text": "rather than subgraphs. So in essence, it's\nlike easier, right?",
    "start": "2860350",
    "end": "2867160"
  },
  {
    "text": "But we do basically\nthe same thing, right? We have a mask that is learnable\nthat is applied on each",
    "start": "2867160",
    "end": "2872770"
  },
  {
    "text": "of these feature dimensions. And if it goes to\nzero, that means the feature is not\nimportant, if it's 1, that",
    "start": "2872770",
    "end": "2880630"
  },
  {
    "text": "means the feature is important. And that way, we kind\nof select a subset of features that dominates\nthe prediction as well.",
    "start": "2880630",
    "end": "2889390"
  },
  {
    "text": "So there's a slight issue that\nzero values can be important. That's not like-- that's kind\nof different from graphs, right?",
    "start": "2889390",
    "end": "2898540"
  },
  {
    "text": "or the structural explanation,\nbecause the fact that something is not present can\nbe a good signal",
    "start": "2898540",
    "end": "2905869"
  },
  {
    "text": "for predicting something. Like having a 0 in a\nparticular dimension doesn't mean that this\ndimension is not important.",
    "start": "2905870",
    "end": "2912300"
  },
  {
    "text": "It just means that there\nis a negative signal on that dimension. So in this case, we can also\ndo like this feature importance",
    "start": "2912300",
    "end": "2923180"
  },
  {
    "text": "measurement by changing\nhow we treat things as being important.",
    "start": "2923180",
    "end": "2929960"
  },
  {
    "text": "We can see how much it\ndeviates from basically a explainability baseline. So this is a common concept\ncalled baseline, common concept",
    "start": "2929960",
    "end": "2937730"
  },
  {
    "text": "in explainability, and\nit's applied in, say, integrated gradients\nand SHAP and everything. So I defined baseline here.",
    "start": "2937730",
    "end": "2945050"
  },
  {
    "text": "Baseline is basically treated\nas a null model of a feature. Or like a prior of the feature.",
    "start": "2945050",
    "end": "2951420"
  },
  {
    "text": "So if you don't know anything\njust from observing the data set, you don't know anything\nabout an instance, what's the expected value of\nthis feature dimension.",
    "start": "2951420",
    "end": "2960290"
  },
  {
    "text": "So you can imagine it as\nlike on average, or just treating it as the mean of the\nmarginal distribution, right?",
    "start": "2960290",
    "end": "2965540"
  },
  {
    "text": "On average, what is\nthe value of that? So we can treat that as\nthe null model, right? By default, it is that, and\n0 is the negative of that.",
    "start": "2965540",
    "end": "2973680"
  },
  {
    "text": "So we can now have a\nreparameterization. So if the mask is 1, that means\n0 is important, if mask is 0,",
    "start": "2973680",
    "end": "2982130"
  },
  {
    "text": "then we move the mean-- sorry, we move the value from\n0 to the marginal distribution.",
    "start": "2982130",
    "end": "2988670"
  },
  {
    "text": "That's just uninformative\nprior of the feature. So there's this small tweak. And if you want\nto know more, you",
    "start": "2988670",
    "end": "2994700"
  },
  {
    "text": "can refer to the paper\nfor more details. But other than that, most\nof the things are similar. Yes.",
    "start": "2994700",
    "end": "3000325"
  },
  {
    "text": "How do you handle\nredundancy in the features? Like it might be\nthat if you remove--",
    "start": "3000325",
    "end": "3005560"
  },
  {
    "text": "if you have a set of five\nfeatures and they're redundant, you remove any one of\nthem or three of them, you don't lose any\nexplainability.",
    "start": "3005560",
    "end": "3011380"
  },
  {
    "text": "But if you remove all\nof them, you lose a lot. Exactly. So are you asking\nabout how we measure",
    "start": "3011380",
    "end": "3017830"
  },
  {
    "text": "the effectiveness\nof explainability in terms of removing redundancy? Yeah. I guess I'm asking how\nyou discover which--",
    "start": "3017830",
    "end": "3027220"
  },
  {
    "text": "like potentially a\nlarge set is important, or you need some\nsubset of a large set. And how you assign\nrelative importance",
    "start": "3027220",
    "end": "3034150"
  },
  {
    "text": "to the members of it. Yeah. So we call that conciseness\nor complexity of explanation.",
    "start": "3034150",
    "end": "3039220"
  },
  {
    "text": "That's definitely a very\nimportant evaluation metric we'll talk about later, but\nwe essentially also want",
    "start": "3039220",
    "end": "3044590"
  },
  {
    "text": "to encourage conciseness. Actually I will\nshow that right now. I think so. Yes, right now.",
    "start": "3044590",
    "end": "3051100"
  },
  {
    "text": "So you look at the objective,\nwe have two additional terms on the right. So basically, it controls\nthe size of the explanation.",
    "start": "3051100",
    "end": "3059590"
  },
  {
    "text": "We have the mask kind of size\nand then the feature size. We basically don't want\nthe mask to be all 1's.",
    "start": "3059590",
    "end": "3065559"
  },
  {
    "text": "If it's all 1's,\nthen, of course, we recover the\noriginal prediction, we are very\naccurate, but then it",
    "start": "3065560",
    "end": "3071230"
  },
  {
    "text": "defeats the purpose\nof explaining because you're not\nmasking out any of these redundant features.",
    "start": "3071230",
    "end": "3076570"
  },
  {
    "text": "So that's why we add\nthis regularization term to enforce the mask to\nbe as concise as possible.",
    "start": "3076570",
    "end": "3082280"
  },
  {
    "text": "We want the explanations\nto say like having-- say no more than 10 edges\nor something like that. So all of these can be like\ndone through Lagrange multiplier",
    "start": "3082280",
    "end": "3092450"
  },
  {
    "text": "regularization,\nthis kind of issue to ensure that the\noutput is not redundant.",
    "start": "3092450",
    "end": "3098750"
  },
  {
    "text": "Does that answer your question? I think so, yeah. Can you get-- with\nusing this procedure,",
    "start": "3098750",
    "end": "3105190"
  },
  {
    "text": "can you get just the single\nmost concise explanation? ",
    "start": "3105190",
    "end": "3112769"
  },
  {
    "text": "So there's always going\nto be a trade-off. So the more concise\nit is, the less likely",
    "start": "3112770",
    "end": "3117933"
  },
  {
    "text": "it's going to recover\nthe original prediction. You might have some\ninaccuracy at the end. But the hope is you don't remove\nsomething that's too critical.",
    "start": "3117933",
    "end": "3125367"
  },
  {
    "text": "So there's always going\nto be the trade-off, and you can always plot the,\nI guess, conciseness versus",
    "start": "3125367",
    "end": "3132200"
  },
  {
    "text": "the prediction faithfulness\nof the explanation and then pick something that\nis on the sweet spot for you.",
    "start": "3132200",
    "end": "3139320"
  },
  {
    "text": "It can be application-specific. For example, in\nmolecules, let's say I have 30 atoms in a\nmolecule, the prediction--",
    "start": "3139320",
    "end": "3145859"
  },
  {
    "text": "the explanations, we\ntypically regard them as functional groups or\nimportant substructure, and typically these kind\nof functional groups",
    "start": "3145860",
    "end": "3152190"
  },
  {
    "text": "has like five\nnodes or something. That might be a prior--\nwe want to get-- we want the explanation to be\nlike no more than say 10 nodes.",
    "start": "3152190",
    "end": "3159630"
  },
  {
    "text": "Whereas in, let's say, a\nrecommender system, maybe I'm more interested in looking\nat the interaction patterns,",
    "start": "3159630",
    "end": "3164970"
  },
  {
    "text": "and some of these\npatterns are non-trivial. It can be multiple people\ninteracting with the same item,",
    "start": "3164970",
    "end": "3171090"
  },
  {
    "text": "or it can be the\nother way around. It can be like a cycle of people\nconnecting with each other,",
    "start": "3171090",
    "end": "3176170"
  },
  {
    "text": "or it can be a fully\nconnected clique. So this kind of\nstructure typically requires more edges\nand more nodes.",
    "start": "3176170",
    "end": "3181670"
  },
  {
    "text": "So we can increase the budget\nto make the explanation larger. So that's kind of\napplication-dependent.",
    "start": "3181670",
    "end": "3187270"
  },
  {
    "text": "Thank you. So one thing that I also\nwant to highlight here",
    "start": "3187270",
    "end": "3194020"
  },
  {
    "text": "is that the optimization\nis performed when explaining every instance. So remember that we talked\nabout the instance explanation",
    "start": "3194020",
    "end": "3199720"
  },
  {
    "text": "and the model-level\nexplanation, this is the instance-level\nexplanation. We want to explain what\nhappens to an instance.",
    "start": "3199720",
    "end": "3206410"
  },
  {
    "text": "Which means that the mask\nis specific to that instance as well. So we have a separate\nmask for every instance.",
    "start": "3206410",
    "end": "3212619"
  },
  {
    "text": "And how do we get the mask? We do this optimization. So luckily, it's not\nthat expensive after all this optimization or all\nthese simplification.",
    "start": "3212620",
    "end": "3221755"
  },
  {
    "text": "You typically run it\nfor like 50 iterations and it's already converged. And then the subgraph,\nthe computation graph",
    "start": "3221755",
    "end": "3228640"
  },
  {
    "text": "is also not likely to be big. So it's not like a big cost, but\nthen it's still a sizable cost.",
    "start": "3228640",
    "end": "3233817"
  },
  {
    "text": "And that's actually\na concern, right? So going back to\none of the questions that one of the\nstudents has asked,",
    "start": "3233818",
    "end": "3239650"
  },
  {
    "text": "can we use a deep model to\npredict the explanation? So that's actually an\nadvantage of using that. So instead of doing this\nkind of shallow model",
    "start": "3239650",
    "end": "3247150"
  },
  {
    "text": "where we have to do\noptimizations for every node when we're explaining\nthat node, we just",
    "start": "3247150",
    "end": "3252640"
  },
  {
    "text": "build a deep model that spits\nout all the explanations given any instance. There are already people who did\nthat, it's called PGExplainer.",
    "start": "3252640",
    "end": "3260650"
  },
  {
    "text": " And that kind of gets\nrid of this optimization",
    "start": "3260650",
    "end": "3266830"
  },
  {
    "text": "need at every instance. But other than that, most\nof the other methods-- most of the pipeline\nstill applies.",
    "start": "3266830",
    "end": "3273170"
  },
  {
    "text": "It's just how this\nparametrization of M, whether we\ntreat M as a learnable",
    "start": "3273170",
    "end": "3278650"
  },
  {
    "text": "parameter as we're doing\nnow in GNNExplainer, or we treat M as a model output\nof a certain explainability",
    "start": "3278650",
    "end": "3287020"
  },
  {
    "text": "method using a deep\nneural network. That's the only difference,\neverything else still applies.",
    "start": "3287020",
    "end": "3292330"
  },
  {
    "text": " So finally, we want to\njust talk a little bit",
    "start": "3292330",
    "end": "3298500"
  },
  {
    "text": "about the explainability\napplicability, applying to different\nkinds of tasks.",
    "start": "3298500",
    "end": "3304000"
  },
  {
    "text": "And as we said in\nnode classification, it's basically the\ncomputation graph is going to be the\nneighborhood of the node",
    "start": "3304000",
    "end": "3310372"
  },
  {
    "text": "that we are going to\nmake predictions on. So we optimize the mask just on\nthe node neighborhood itself.",
    "start": "3310372",
    "end": "3316050"
  },
  {
    "text": "In a link prediction, imagine\nwhat are the structure--",
    "start": "3316050",
    "end": "3321600"
  },
  {
    "text": "substructures that can\npotentially affect the link. It's going to be the\nneighborhood of one node union",
    "start": "3321600",
    "end": "3328343"
  },
  {
    "text": "the neighborhood\nof the other node because you're\npredicting that link. So naturally you can take\nthe union of these two",
    "start": "3328343",
    "end": "3333750"
  },
  {
    "text": "neighborhoods and\ntreat that as your Gc, everything else still applies. In the case of graph\nclassification,",
    "start": "3333750",
    "end": "3339450"
  },
  {
    "text": "it's basically on\nthe entire graph. You basically input the\nentire graph as your Gc,",
    "start": "3339450",
    "end": "3344610"
  },
  {
    "text": "and then you optimize\nthe same thing. It's the same method.  But for graph classification,\nusually the graph",
    "start": "3344610",
    "end": "3351390"
  },
  {
    "text": "itself is not too big. So it's still\ncomputationally feasible. And again, this can be\nadapted to any kind of message",
    "start": "3351390",
    "end": "3358420"
  },
  {
    "text": "passing architecture as long\nas there is message passing. So we assume-- in\nthe assumption,",
    "start": "3358420",
    "end": "3364408"
  },
  {
    "text": "we assume there is a message\npassing in the computation graph, and that's all we assume. So we can apply\nthis post-hoc method",
    "start": "3364408",
    "end": "3370510"
  },
  {
    "text": "to GAT, gated graph sequence,\ngraph networks, GraphSAGE, like any GNN methods that we\ntalk about in this course.",
    "start": "3370510",
    "end": "3381849"
  },
  {
    "text": "So any questions so far? OK, cool.",
    "start": "3381850",
    "end": "3387060"
  },
  {
    "text": "If not, I will switch--\ngradually switch to evaluation. So we can talk\nabout how we compare",
    "start": "3387060",
    "end": "3393220"
  },
  {
    "text": "with some alternative methods. But before I do that, I'll\njust give a quick overview of what are some baselines that\nyou can always consider when",
    "start": "3393220",
    "end": "3400440"
  },
  {
    "text": "trying to explain the model. One very simple approach is\ncalled saliency or gradient.",
    "start": "3400440",
    "end": "3406380"
  },
  {
    "text": "It's just like a quick analogy\nfrom the linear regression example that I\njust showed before. Before, we had this slope\nthat indicates importance,",
    "start": "3406380",
    "end": "3413790"
  },
  {
    "text": "now we have a gradient of\na very complex surface. Nevertheless, gradient is\nstill a local approximation",
    "start": "3413790",
    "end": "3419130"
  },
  {
    "text": "of the slope. So you can imagine that if one\nof the particular input feature",
    "start": "3419130",
    "end": "3425520"
  },
  {
    "text": "has a high gradient, it's going\nto affect the prediction a lot. That means if I change\nthe input a little bit,",
    "start": "3425520",
    "end": "3431430"
  },
  {
    "text": "the output prediction, the\nprobability will change a lot. Whereas a flat won't\ngive you that much.",
    "start": "3431430",
    "end": "3436470"
  },
  {
    "text": "So we can still have this\ngradient information, but please note that the\ngradient is with respect",
    "start": "3436470",
    "end": "3442710"
  },
  {
    "text": "to the input not the weights. So in the typical\nGNN optimization,",
    "start": "3442710",
    "end": "3447910"
  },
  {
    "text": "we take the gradient\nof the weights, but now we take the\ngradient of the input. The input can be\nthe adjacency edges,",
    "start": "3447910",
    "end": "3453760"
  },
  {
    "text": "or it can be the\nfeature dimensions. So these are the signals we can\nuse to threshold, to understand",
    "start": "3453760",
    "end": "3460570"
  },
  {
    "text": "which edges are\nimportant because they have high gradients. Another alternative approach\nis obviously through attention.",
    "start": "3460570",
    "end": "3467660"
  },
  {
    "text": "I think we covered GAT and\ntransformers in this class. So like all of\nthese architectures",
    "start": "3467660",
    "end": "3475420"
  },
  {
    "text": "have attention weights\nthat are between nodes. For example, in a\nrecommender system,",
    "start": "3475420",
    "end": "3481450"
  },
  {
    "text": "an item gets recommended to a\nuser where there's an attention value, and depending on\nthat attention value, we can just directly\nthreshold out",
    "start": "3481450",
    "end": "3488289"
  },
  {
    "text": "nodes that are not important. But there are some\nissue with that. It typically results in like\ndisconnected explanations,",
    "start": "3488290",
    "end": "3495880"
  },
  {
    "text": "and it also-- it's not as ideal as you\nwould think because there's",
    "start": "3495880",
    "end": "3501640"
  },
  {
    "text": "multi-head and\nthere's like attention across all the layers. It's just not as ideal\nas you would think.",
    "start": "3501640",
    "end": "3508250"
  },
  {
    "text": "But it's definitely\nlike one way. So let's say I want to\ncompare all these methods,",
    "start": "3508250",
    "end": "3513309"
  },
  {
    "text": "and how would we compare? So in the case of when\nground truth is available,",
    "start": "3513310",
    "end": "3519580"
  },
  {
    "text": "we can, of course, compare\nwith the ground truth. And that's what we did\nin this original paper. We designed this\nsynthetic task such",
    "start": "3519580",
    "end": "3526210"
  },
  {
    "text": "that we actually have some\nidea about the ground truth. So this is a particular\ntask that basically asks,",
    "start": "3526210",
    "end": "3532089"
  },
  {
    "text": "is a node part of a given motif? Let's say the motif is a\nhouse structure, on the left,",
    "start": "3532090",
    "end": "3537680"
  },
  {
    "text": "you see the green\nfive-node house. If the node is part of this\nmotif, I give it as label 1,",
    "start": "3537680",
    "end": "3544300"
  },
  {
    "text": "otherwise you give\nit as label 0. So a very simple\nstructure-based prediction task.",
    "start": "3544300",
    "end": "3550660"
  },
  {
    "text": "And you can imagine\nthat in those cases, how do I explain a node that\nis part of the given motif?",
    "start": "3550660",
    "end": "3556060"
  },
  {
    "text": "Of course, the\nmotif itself, right? Like if the explanation\ncan highlight this house, then I'm good.",
    "start": "3556060",
    "end": "3561400"
  },
  {
    "text": "Then the explanation\nrecovers what is needed for the prediction. And same thing goes for,\nlet's say, a six-cycle motif",
    "start": "3561400",
    "end": "3568359"
  },
  {
    "text": "or like a grid kind of motif. So in the data set, we attach\nthese kind of motifs randomly",
    "start": "3568360",
    "end": "3574720"
  },
  {
    "text": "to this big graph, let's say\nany kind of graph generator, like a Barabási-Albert graph or\na tree graph or anything like",
    "start": "3574720",
    "end": "3583280"
  },
  {
    "text": "that. And we just randomly\nattach a bunch of these and highlight these nodes\nas a particular label 1,",
    "start": "3583280",
    "end": "3588400"
  },
  {
    "text": "and then we try to\nexplain those nodes that belong to those motifs. And then we can also do it for\nother data sets like molecules",
    "start": "3588400",
    "end": "3597069"
  },
  {
    "text": "and social network recommender\nsystem and all these. The only caveat is\nthat these are not--",
    "start": "3597070",
    "end": "3604210"
  },
  {
    "text": "we don't have ground\ntruth for those. So I mentioned that for\nthe synthetic data set,",
    "start": "3604210",
    "end": "3609580"
  },
  {
    "text": "we already have\nthe ground truth, like the house, the grid,\nand so on, so we can directly see how accurate is the\nexplainability method",
    "start": "3609580",
    "end": "3618550"
  },
  {
    "text": "in recovering that\nparticular motif. And because we know exactly\nwhat the ground truth is, we can compute the\naccuracy for that, or AUC,",
    "start": "3618550",
    "end": "3627010"
  },
  {
    "text": "whatever metric you prefer. And we can compare our\nmethod with attention",
    "start": "3627010",
    "end": "3633070"
  },
  {
    "text": "and gradient-based methods\nfor these kind of data sets. So here's some kind\nof visualization.",
    "start": "3633070",
    "end": "3639140"
  },
  {
    "text": "And as you would expect,\nhaving a high score means that you\nrecover that motif. So notice that I manually\nset like a higher explanation",
    "start": "3639140",
    "end": "3647920"
  },
  {
    "text": "threshold because sometimes the\nmodel is not exactly accurate. So the house might not\nbe the top five nodes.",
    "start": "3647920",
    "end": "3657150"
  },
  {
    "text": "But if I say like I pick\ntop 10, then most likely, this top 5 is going to appear.",
    "start": "3657150",
    "end": "3662869"
  },
  {
    "text": "And then if we look at a\nmultiple of these instances, then of course,\nwe can understand maybe house is\nbasically the reason",
    "start": "3662870",
    "end": "3670280"
  },
  {
    "text": "for this particular\nprediction of class 1, something like that. So you can see that it roughly\naligns with the ground truth",
    "start": "3670280",
    "end": "3679580"
  },
  {
    "text": "pretty well for the\nsynthetic data sets as well. So I will talk about\nmore challenging settings",
    "start": "3679580",
    "end": "3685420"
  },
  {
    "text": "of evaluating this\nthing, especially in cases where the ground\ntruths are not available.",
    "start": "3685420",
    "end": "3690430"
  },
  {
    "text": "So we also did this paper on\nLearning on Graphs Conference last year on explainability\ntaxonomy and evaluation.",
    "start": "3690430",
    "end": "3698750"
  },
  {
    "text": "So we particularly studied\nissues or scenarios where these kind of ground\ntruths are not available,",
    "start": "3698750",
    "end": "3704170"
  },
  {
    "text": "and can we have a\ngood systematic way of evaluating that. So to recap, this is all we did\nin the GNNExplainer and most",
    "start": "3704170",
    "end": "3713350"
  },
  {
    "text": "of the explainability methods. We have some input\ngraph, we have this kind of mask generation thing.",
    "start": "3713350",
    "end": "3719010"
  },
  {
    "text": "It can be a deep model,\nit can be directly optimized and whatever. It picks out-- identifies\nimportant feature dimensions",
    "start": "3719010",
    "end": "3725470"
  },
  {
    "text": "and adjacency, and then\nyou get a feature mask, you get a graph\nmask, and then these",
    "start": "3725470",
    "end": "3730600"
  },
  {
    "text": "are the predictions or the\nexplanations for the model. And this is where we are.",
    "start": "3730600",
    "end": "3736760"
  },
  {
    "text": "So GNNExplainer is similar\nto many, many papers that kind of follow\nthis line of direction,",
    "start": "3736760",
    "end": "3743000"
  },
  {
    "text": "is a part of this\nperturbation-based approaches. But as you can\nsee, explainability is a very big landscape.",
    "start": "3743000",
    "end": "3750050"
  },
  {
    "text": "We have those gradient-based,\ndecomposition-based surrogate models and all these things.",
    "start": "3750050",
    "end": "3755190"
  },
  {
    "text": "And one note is that the thing\non the right, the pink one is the model-level explanation.",
    "start": "3755190",
    "end": "3761360"
  },
  {
    "text": "There's currently like\nmaybe one or two methods on that for like explaining GNN\npredictions at the model level.",
    "start": "3761360",
    "end": "3768750"
  },
  {
    "text": "So as I said, it's\ndefinitely like a major kind of area of research where\nit's still quite challenging.",
    "start": "3768750",
    "end": "3776750"
  },
  {
    "text": "So to evaluate\nall these methods, to understand which\nmethod is going to be used or more\neffective, we want",
    "start": "3776750",
    "end": "3784730"
  },
  {
    "text": "to build this framework that can\nmake these kind of comparison.",
    "start": "3784730",
    "end": "3789859"
  },
  {
    "text": "And we want to stress that\nthis kind of evaluation is typically\nmulti-dimensional, and some of the students that\nhave asked questions",
    "start": "3789860",
    "end": "3797224"
  },
  {
    "text": "brought up some aspects of it. First, we want to understand\nwhether the goal is",
    "start": "3797225",
    "end": "3803510"
  },
  {
    "text": "to model the\nphenomenon to explain the underlying\nscientific phenomenon or explaining why the model\nis making certain prediction.",
    "start": "3803510",
    "end": "3810260"
  },
  {
    "text": "So that's a major distinction. There's also masking\nstrategy difference, whether you want a soft mask\nversus a hard thresholded mask,",
    "start": "3810260",
    "end": "3817850"
  },
  {
    "text": "that will also\naffect the evaluation ranking of different methods.",
    "start": "3817850",
    "end": "3823010"
  },
  {
    "text": "And I'll talk about type, the\nnecessity and the sufficiency of the explanation.",
    "start": "3823010",
    "end": "3828770"
  },
  {
    "text": "So all these are being\nexplained in the paper. So let's start with the goal.",
    "start": "3828770",
    "end": "3834329"
  },
  {
    "text": "So as I said, it is\nmulti-dimensional. Keeping in mind, we\nhave multiple aspects. It's not like a single\nmetric dominates everything.",
    "start": "3834330",
    "end": "3840240"
  },
  {
    "text": "It's not like accuracy in\nmachine learning models. So phenomenon\nexplanation, as I said,",
    "start": "3840240",
    "end": "3847280"
  },
  {
    "text": "explains the underlying reason,\nwhereas model explanation explains why a model makes\na particular prediction.",
    "start": "3847280",
    "end": "3853370"
  },
  {
    "text": "And we will have different\nmetric, which we call fidelity here, to measure the\nperformance in both cases.",
    "start": "3853370",
    "end": "3859940"
  },
  {
    "text": "So here is the highlight. So this is the\nformula for fidelity.",
    "start": "3859940",
    "end": "3866150"
  },
  {
    "text": "And again, you already\nsee that it's not a one-dimensional thing. You already see two dimension. We have a metric\ncalled fidelity plus,",
    "start": "3866150",
    "end": "3872820"
  },
  {
    "text": "a metric called fidelity minus. ",
    "start": "3872820",
    "end": "3877850"
  },
  {
    "text": "So here, we classify the goals. Goal one is to explain\nthe phenomenon, the other is to explain\nwhat the models learn.",
    "start": "3877850",
    "end": "3883277"
  },
  {
    "text": "So let's say we take\nthe phenomenon part, so the left part\nof the equation. So fidelity plus\nand minus kind of",
    "start": "3883277",
    "end": "3891920"
  },
  {
    "text": "measures two different aspects\nof necessity and sufficiency of the explanation. So I guess to explain\nit a little bit,",
    "start": "3891920",
    "end": "3900140"
  },
  {
    "text": "so the y here without the\nhat is the ground truth. I highlighted that.",
    "start": "3900140",
    "end": "3905150"
  },
  {
    "text": "And the blue part,\nwhich has the y hat, these are the model prediction.",
    "start": "3905150",
    "end": "3910340"
  },
  {
    "text": "And note that the model\nprediction has a superscript there called Gc slash s.",
    "start": "3910340",
    "end": "3918800"
  },
  {
    "text": "So Gc is the competition graph. I hope you still\nremember the notation. And then s is, let's\nsay, the subset",
    "start": "3918800",
    "end": "3924860"
  },
  {
    "text": "of the features and\nthe substructure that are being extracted\nby the explainability.",
    "start": "3924860",
    "end": "3930240"
  },
  {
    "text": "So you can imagine\nthis as saying like what is the\nmodel prediction if I remove that explanation\nfrom the original graph?",
    "start": "3930240",
    "end": "3939780"
  },
  {
    "text": "Same thing goes for other stuff. So here is the--\nso these metrics",
    "start": "3939780",
    "end": "3945450"
  },
  {
    "text": "are actually being proposed in\none of the other papers called taxonomy, GNN explanation\ntaxonomy paper,",
    "start": "3945450",
    "end": "3952980"
  },
  {
    "text": "but we add like\nadditional insights on it. So fidelity plus, so\nfrom the equation,",
    "start": "3952980",
    "end": "3958200"
  },
  {
    "text": "here you can directly\ninterpret plus means that we basically remove\nthat important substructure",
    "start": "3958200",
    "end": "3963690"
  },
  {
    "text": "from the original graph. you say it is\nimportant, I remove it, see how it [? did. ?] So\nif it is really important,",
    "start": "3963690",
    "end": "3970410"
  },
  {
    "text": "you better show like\nremoving of that gives you very bad performance. That means that thing is\nessential or necessary,",
    "start": "3970410",
    "end": "3979269"
  },
  {
    "text": "let's say. So on the other hand, fidelity\nminus is on the green part.",
    "start": "3979270",
    "end": "3986880"
  },
  {
    "text": "Notice that we are only\nkeeping the important parts. So you say this is important,\nI remove everything else,",
    "start": "3986880",
    "end": "3992910"
  },
  {
    "text": "I only keep that and\nsee how you perform. If this thing is\nreally important,",
    "start": "3992910",
    "end": "3997950"
  },
  {
    "text": "you better show me that\nthe final performance or final model prediction\nis not too much different from the original prediction as\nif you didn't remove anything.",
    "start": "3997950",
    "end": "4006240"
  },
  {
    "text": "So you only keep\nthe important part, it should give you\nsimilar performance. So these are kind of the\nintuition of the metrics.",
    "start": "4006240",
    "end": "4012974"
  },
  {
    "text": "And actually the same thing goes\nfor the other part of the goal, except that we basically replace\nthe first part of the indicator",
    "start": "4012975",
    "end": "4021869"
  },
  {
    "text": "function with just one,\nbecause ground truth-- if I only care\nabout ground truth,",
    "start": "4021870",
    "end": "4027480"
  },
  {
    "text": "I just make it like 1\nor 0 depending on what the ground truth actually is. So that's just the\nonly difference.",
    "start": "4027480",
    "end": "4035460"
  },
  {
    "text": "But here, for the\nphenomenon, we still keep the original\nprediction because we care about the model behavior.",
    "start": "4035460",
    "end": "4041109"
  },
  {
    "text": "So we want to keep\nthe information about the original prediction\nconfidence here in the metric as well.",
    "start": "4041110",
    "end": "4048730"
  },
  {
    "text": "So after talking about\nthis, I want to also stress that the explanation--",
    "start": "4048730",
    "end": "4054610"
  },
  {
    "text": "I just talked about\nsufficiency and necessity. So you can imagine that\nfidelity plus is a necessity. Removing of that is bad.",
    "start": "4054610",
    "end": "4061180"
  },
  {
    "text": "And fidelity minus\nis sufficiency. Having that is\nalready good enough.",
    "start": "4061180",
    "end": "4066550"
  },
  {
    "text": "But that's just one aspect\nof explanation quality. And typically, people also\nconsider the other two aspects,",
    "start": "4066550",
    "end": "4072520"
  },
  {
    "text": "which is explanation\nstability, right? How stable is your explanation? You better have a consistent\nexplanation of the same thing,",
    "start": "4072520",
    "end": "4079900"
  },
  {
    "text": "of the same model no\nmatter what your seed is. And that's actually a\nproblem with a deep model.",
    "start": "4079900",
    "end": "4085330"
  },
  {
    "text": "If the deep model\nis going to make such prediction of\nsmall perturbation, it can sometimes\ncause the explanation",
    "start": "4085330",
    "end": "4090910"
  },
  {
    "text": "to be very different. And we don't like that. So it's definitely\nanother evaluation that we should be doing.",
    "start": "4090910",
    "end": "4097600"
  },
  {
    "text": "And the last is,\nagain, related to one of the other question about\nexplanation and complexity,",
    "start": "4097600",
    "end": "4104089"
  },
  {
    "text": "we want to make sure\nthat model captures all these good characteristics\nwith very concise explanation.",
    "start": "4104090",
    "end": "4110960"
  },
  {
    "text": "Imagine that we\nhave a model that gets very good fidelity\nmetrics, but only",
    "start": "4110960",
    "end": "4116060"
  },
  {
    "text": "if I keep 50% of the address. That's pretty bad. Like if my neighborhood\nhas 1,000 edges,",
    "start": "4116060",
    "end": "4121910"
  },
  {
    "text": "does that mean the\nexplanation is like 500 edges? That's pretty bad. We want the explanation\nto be concise.",
    "start": "4121910",
    "end": "4126950"
  },
  {
    "text": "And that's application\ndependent, but in any case, we should have this complexity\nbeing measured and evaluated,",
    "start": "4126950",
    "end": "4132770"
  },
  {
    "text": "especially like as\na trade off with, say the fidelity metric\nthat I proposed just now.",
    "start": "4132770",
    "end": "4139739"
  },
  {
    "text": "So I talked about the\nsufficiency and necessity of the fidelity metric, and\nwe can also combine them.",
    "start": "4139740",
    "end": "4145979"
  },
  {
    "text": "Like similar to how\nwe combine precision and recall, any of\nthese kind of necessity,",
    "start": "4145979",
    "end": "4151229"
  },
  {
    "text": "sufficiency trade off\nas a single metric. So the measurement is\nalso actually pretty easy.",
    "start": "4151229",
    "end": "4158219"
  },
  {
    "text": "Just do a harmonic mean\nof the fidelity plus and fidelity minus with\nsome of the weights.",
    "start": "4158220",
    "end": "4164700"
  },
  {
    "text": "The most common way to see it\nis like if I don't have a prior, just put the weights as 1.",
    "start": "4164700",
    "end": "4170189"
  },
  {
    "text": "That's the typical\nharmonic mean, or that's how we\ncompute a harmonic mean.",
    "start": "4170189",
    "end": "4176250"
  },
  {
    "text": "But if I care about\nsufficiency more, or if I care about necessity\nof the explanation more, then I can weight\nthem accordingly",
    "start": "4176250",
    "end": "4183689"
  },
  {
    "text": "depending on my\napplication needs. So this is a bit more\nexplanation on how this works.",
    "start": "4183689",
    "end": "4191489"
  },
  {
    "text": "So imagine that we\nhave this trade off. We have the fidelity plus. So it is 1 minus fidelity\nminus because fidelity--",
    "start": "4191490",
    "end": "4200460"
  },
  {
    "text": "0 is actually pretty good. It means 0 difference\nbetween the model output",
    "start": "4200460",
    "end": "4206070"
  },
  {
    "text": "if it uses the explain subgraph\nversus the original graph. So we use 1 minus\nfidelity minus.",
    "start": "4206070",
    "end": "4212640"
  },
  {
    "text": "And then the x-axis\nis the fidelity plus. So we treat this as kind\nof like a precision, recall",
    "start": "4212640",
    "end": "4218760"
  },
  {
    "text": "kind of thing. So we can draw this curve\nof characterization score.",
    "start": "4218760",
    "end": "4225345"
  },
  {
    "text": "And on the top right,\nit's going to be 1. And if you go towards the\ntop part of the triangle,",
    "start": "4225345",
    "end": "4232050"
  },
  {
    "text": "it's going to mean\nthat the model tends to be more sufficient. It can tend to have\nvery good metric",
    "start": "4232050",
    "end": "4237570"
  },
  {
    "text": "in terms of fidelity\nminus, whereas if it falls to the lower triangle,\nit means that it's kind of more like a sufficient--\nsorry, necessary explanation.",
    "start": "4237570",
    "end": "4246180"
  },
  {
    "text": "Means that removing these\nthings is going to be bad, but it's not like these\nthings alone will give you a good prediction.",
    "start": "4246180",
    "end": "4253410"
  },
  {
    "text": "So we have this\nkind of intuition. And then, again,\ncharacterization",
    "start": "4253410",
    "end": "4258990"
  },
  {
    "text": "is basically like a combination\nof these two metrics.",
    "start": "4258990",
    "end": "4264700"
  },
  {
    "text": "So let's look at\nsome performance. So given these\nmetrics, we can now measure them against all the\nproposed explanation methods.",
    "start": "4264700",
    "end": "4273340"
  },
  {
    "text": "So here, we have-- on the right,\nwe have a bunch of methods. Sa is saliency, occlusion,\nintegrated gradients, pagerank,",
    "start": "4273340",
    "end": "4280390"
  },
  {
    "text": "gradcam, gnnexplainer,\npgmexplainer, and a bunch of others. So like a bunch of GNN\nexplainability methods,",
    "start": "4280390",
    "end": "4288369"
  },
  {
    "text": "and we can now\nevaluate them to see which ones are good in\nterms of fidelity metric.",
    "start": "4288370",
    "end": "4294980"
  },
  {
    "text": "I highlighted two circles. There are a group\nof methods that are more on the upper triangle\nthat are more sufficient,",
    "start": "4294980",
    "end": "4301909"
  },
  {
    "text": "identifies the component\nthat actually gives us good prediction, and\nthen there this red group",
    "start": "4301910",
    "end": "4307929"
  },
  {
    "text": "of methods that tend\nto be more necessary. So these are the edges\nthat are essential,",
    "start": "4307930",
    "end": "4313420"
  },
  {
    "text": "removing them will\ngive us bad prediction, but it doesn't capture\nthe entire picture of the explanation.",
    "start": "4313420",
    "end": "4318675"
  },
  {
    "text": "So we can see these trade offs\nof these methods very well.",
    "start": "4318675",
    "end": "4323849"
  },
  {
    "text": "So another thing\nI want to mention is that the conclusion\nof the comparability",
    "start": "4323850",
    "end": "4329910"
  },
  {
    "text": "of different\nexplainability methods can be very different depending\non data sets and tasks.",
    "start": "4329910",
    "end": "4335640"
  },
  {
    "text": "This is a data set on\neBay, an e-commerce graph where we want to, say\nlike do anomaly detection",
    "start": "4335640",
    "end": "4341520"
  },
  {
    "text": "or a recommender system\nfor this kind of task. And in this case, the\nconclusion is very different.",
    "start": "4341520",
    "end": "4347628"
  },
  {
    "text": "So you can see this\nis very different from the previous one. I won't go into\ndetail, but you will see the rankings\nare quite different.",
    "start": "4347628",
    "end": "4355110"
  },
  {
    "text": "And notably, GNNExplainer does\npretty well in this large scale data set, and it shows that\nit's not just necessary,",
    "start": "4355110",
    "end": "4363060"
  },
  {
    "text": "but it's also giving\nus like very good sufficient explanations as well.",
    "start": "4363060",
    "end": "4369179"
  },
  {
    "text": "Sorry, the other way around. It's not just\nsufficient, but it also gives us necessary explanation.",
    "start": "4369180",
    "end": "4374220"
  },
  {
    "text": "So really identifies the key\naspects in the social network. ",
    "start": "4374220",
    "end": "4381300"
  },
  {
    "text": "So what I want to say\nis there is no a very fixed conclusion yet for all\nthe explainability methods,",
    "start": "4381300",
    "end": "4388870"
  },
  {
    "text": "and you will be\nadvised if you consider all these different aspects when\nevaluating how good a method is",
    "start": "4388870",
    "end": "4395560"
  },
  {
    "text": "in your task. So towards the end\nof the lecture,",
    "start": "4395560",
    "end": "4401150"
  },
  {
    "text": "I want to give maybe\ntwo additional slides on other types of explanations\nthat people care about",
    "start": "4401150",
    "end": "4408340"
  },
  {
    "text": "and we are doing\nactive research on. So these are the explanations\nthat are not fully developed",
    "start": "4408340",
    "end": "4415360"
  },
  {
    "text": "yet, but it's a very active\narea of research in case people are interested. One type of explanation is\ncounterfactual explanation.",
    "start": "4415360",
    "end": "4422390"
  },
  {
    "text": "So this doesn't care about what\nmakes the prediction this way, but it cares about\nwhat if scenario.",
    "start": "4422390",
    "end": "4430000"
  },
  {
    "text": "Like what if I\nmake some changes, will it become another class? What should I take to\nmake this input becoming--",
    "start": "4430000",
    "end": "4438295"
  },
  {
    "text": "to make the model predict the\ninput as some other class? And that's the\ncounterfactual setting. There are some papers on it.",
    "start": "4438295",
    "end": "4444440"
  },
  {
    "text": "In fact, we have a paper\nin submission as well. So it's useful in\nunderstanding the distinction",
    "start": "4444440",
    "end": "4449770"
  },
  {
    "text": "between classes. You can imagine\ntrying to understand what the decision boundary\nbetween different classes is.",
    "start": "4449770",
    "end": "4454810"
  },
  {
    "text": "How do I cross that\ndecision boundary basically by adjusting the input?",
    "start": "4454810",
    "end": "4459940"
  },
  {
    "text": "So in a real estate example,\nit can be like industry-- in a company, people or\nmanagers want to know",
    "start": "4459940",
    "end": "4467170"
  },
  {
    "text": "what does it take to convert\na user from being predicted as an inactive or\nchurn kind of class",
    "start": "4467170",
    "end": "4473380"
  },
  {
    "text": "to an active premium\nuser kind of class?",
    "start": "4473380",
    "end": "4478870"
  },
  {
    "text": "So that kind of\ngives people insight about how to retain user\nbetter, how to basically change",
    "start": "4478870",
    "end": "4487360"
  },
  {
    "text": "some of the input features\nthat they can control to attract the users more. That's just one example.",
    "start": "4487360",
    "end": "4492770"
  },
  {
    "text": "There are a lot of example usage\nof counterfactual explanations. And another one I have\nstressed a lot in the lecture",
    "start": "4492770",
    "end": "4500110"
  },
  {
    "text": "is model-level explanation. That's very important also. So we don't want to care about\nthe one particular instance,",
    "start": "4500110",
    "end": "4506620"
  },
  {
    "text": "but we want to extract\ninsights from all instances of a certain class\nor all instances that",
    "start": "4506620",
    "end": "4512590"
  },
  {
    "text": "are predicted to be a\ncertain class by the model. So it basically means a\ncommon characteristics",
    "start": "4512590",
    "end": "4517840"
  },
  {
    "text": "or some high-level\ninsights that are not specific to\nindividual classes, we can now sort of get\nit through like trying",
    "start": "4517840",
    "end": "4524680"
  },
  {
    "text": "a lot of instance-level\nexplanations and manually conclude\nwhat the conclusion is. But having a systematic\nway of doing that",
    "start": "4524680",
    "end": "4531550"
  },
  {
    "text": "is actually not very easy. For example, this\nmethod uses an-- called XGNN uses an\nRL-based approach",
    "start": "4531550",
    "end": "4537520"
  },
  {
    "text": "to add nodes and edges\nsuch that it achieves a high score for all the\ninstances in the class",
    "start": "4537520",
    "end": "4546490"
  },
  {
    "text": "just through\n[INAUDIBLE] gradient. So that's an example. But again, this is not\na very easy method,",
    "start": "4546490",
    "end": "4553510"
  },
  {
    "text": "and it's also not very stable. So people are still doing\nresearch on that as well.",
    "start": "4553510",
    "end": "4559600"
  },
  {
    "text": "OK, I think that's mostly what\nI want to cover for the lecture. So I want to conclude here.",
    "start": "4559600",
    "end": "4565730"
  },
  {
    "text": "So we talked about\nexplainability, and I want to say it's\na very essential area",
    "start": "4565730",
    "end": "4571390"
  },
  {
    "text": "in trustworthy GNN. Like all these aspects\nare related in my opinion. Like explainability has a\nlot to do with robustness",
    "start": "4571390",
    "end": "4577180"
  },
  {
    "text": "because being able to\nexplain a robust model will give you good stuff. But explaining something\nthat's not robust",
    "start": "4577180",
    "end": "4583810"
  },
  {
    "text": "will give you\nadversarial examples. They're pretty bad. And explainability, I\nalready mentioned how it's",
    "start": "4583810",
    "end": "4588970"
  },
  {
    "text": "related to fairness, privacy. We don't want to use\nprotected characteristics, we want to use explanations to\nensure that people's privacy is",
    "start": "4588970",
    "end": "4596829"
  },
  {
    "text": "protected. It's like all related, all\nthe trustworthy aspects.",
    "start": "4596830",
    "end": "4602980"
  },
  {
    "text": "And we talked\nabout GNNExplainer, which is a\nperturbation-based approach. The key insight is that we want\nto optimize this mask in order",
    "start": "4602980",
    "end": "4611590"
  },
  {
    "text": "to achieve high\nmutual information between the prediction and\nthe subset of the substructure and the features that are\nimportant for the prediction.",
    "start": "4611590",
    "end": "4619719"
  },
  {
    "text": "And we also talked about\nexplanation evaluation, which can be a challenging\nthing to do compared to normal machine learning\nevaluation because it",
    "start": "4619720",
    "end": "4627520"
  },
  {
    "text": "is multi-dimensional in nature. We talked about fidelity,\ncomplexity, stability, v these different metrics.",
    "start": "4627520",
    "end": "4633820"
  },
  {
    "text": "And sufficiency and\nnecessity of the explanations are something we\nhave to consider",
    "start": "4633820",
    "end": "4639070"
  },
  {
    "text": "depending on the\nactual application that we want to explain for.",
    "start": "4639070",
    "end": "4644199"
  },
  {
    "text": "And then, again, there are\ncounterfactual model-level explanations that are\nworth additional research.",
    "start": "4644200",
    "end": "4650140"
  },
  {
    "text": "OK, I think that's\neverything I wanted to cover. So we have one minute. So is there any\nquestion or something?",
    "start": "4650140",
    "end": "4656770"
  },
  {
    "start": "4656770",
    "end": "4662517"
  },
  {
    "text": "It can be related to [INAUDIBLE]\nor it can be more high level, it doesn't matter. ",
    "start": "4662517",
    "end": "4671060"
  },
  {
    "text": "Yes. What is one of the bigger\nchallenges for explainability?",
    "start": "4671060",
    "end": "4679040"
  },
  {
    "text": "Yeah. So the question is what\nis the major challenge? So I mentioned a lot\nof unknown things here.",
    "start": "4679040",
    "end": "4685940"
  },
  {
    "text": "Like for example,\ncounterfactual explanation, model-level explanations\nthat are not too clear.",
    "start": "4685940",
    "end": "4691040"
  },
  {
    "text": "And then there's this\ntrade-off between conciseness and fidelity metric.",
    "start": "4691040",
    "end": "4696619"
  },
  {
    "text": "That's also not very clear. And there are a lot\nof open direction. I would say like this\nis a pretty new area.",
    "start": "4696620",
    "end": "4701690"
  },
  {
    "text": "As you can see, the paper\nwe did was like 2019, and last year on the evaluation.",
    "start": "4701690",
    "end": "4707880"
  },
  {
    "text": "So it's a very recent topic, and\nthere are so many challenges, I think. And one other potential\ndirection I want to mention",
    "start": "4707880",
    "end": "4714620"
  },
  {
    "text": "is generation. Like can we explain\nsay a drug discovery or like a generation procedure?",
    "start": "4714620",
    "end": "4720440"
  },
  {
    "text": "What is being used by this\nstrategy to generate graphs? And another aspect\nis potentially",
    "start": "4720440",
    "end": "4726170"
  },
  {
    "text": "use generative\nmodels for generating these kind of explanation that\nare stable and also concise.",
    "start": "4726170",
    "end": "4731490"
  },
  {
    "text": "So I think a lot of\ninteresting directions. We know much less\nthan other fields.",
    "start": "4731490",
    "end": "4738810"
  },
  {
    "text": " Yes.",
    "start": "4738810",
    "end": "4743876"
  },
  {
    "text": "Is there any kind of way that\nwe can incorporate, let's say, some kind of inductive\nbias or understanding of people from certain fields\nwith explainability of GNNs",
    "start": "4743876",
    "end": "4753060"
  },
  {
    "text": "so we don't just expect\nthe models to learn what we want them and then\nidentify, but actually--",
    "start": "4753060",
    "end": "4759130"
  },
  {
    "text": "Yeah, that's a great question. [INAUDIBLE] That's a great question. So what we-- so the\nquestion is on whether we",
    "start": "4759130",
    "end": "4766680"
  },
  {
    "text": "can use inductive bias\nfrom domain experts to help in explainability. I think what currently\npeople usually",
    "start": "4766680",
    "end": "4773280"
  },
  {
    "text": "want to do is they extract\nexplanations for the model, and then they ask\ndomain experts, how many of these explanations\ndo you think make sense?",
    "start": "4773280",
    "end": "4780420"
  },
  {
    "text": "What are the things\nthat don't make sense? Is it because of\nthe model artifact, or is it because\nthere's something",
    "start": "4780420",
    "end": "4785910"
  },
  {
    "text": "that we haven't\ndiscovered in science yet? So it's usually like, we have\nto have very close collaboration",
    "start": "4785910",
    "end": "4792710"
  },
  {
    "text": "and human feedback\nfrom the domain experts to see are these\nexplanations good? If not, let's add more inductive\nbias and retrain this thing.",
    "start": "4792710",
    "end": "4800230"
  },
  {
    "text": "For example, like\nmaybe the expert says, I usually think that\nthis motif is important,",
    "start": "4800230",
    "end": "4806470"
  },
  {
    "text": "then maybe in our explanation,\nwe can specifically look at those motifs as an\naggregation of these masks",
    "start": "4806470",
    "end": "4814570"
  },
  {
    "text": "weights and then look\nat the weights there. So it's like a lot\nof back and forth,",
    "start": "4814570",
    "end": "4819790"
  },
  {
    "text": "I think, between the domain\nexperts and what we're doing. But yeah, like human feedback,\nI think, is very important.",
    "start": "4819790",
    "end": "4827110"
  },
  {
    "text": "And integrating that is\ndefinitely a good research direction.",
    "start": "4827110",
    "end": "4832760"
  },
  {
    "text": "Oh, last one. [INAUDIBLE] try to\ngenerate explainability for dynamic graph, the\ngraph that keeps [INAUDIBLE]",
    "start": "4832760",
    "end": "4840950"
  },
  {
    "text": "the way is actually changing? Yeah, that's a great\nquestion as well. There's one paper on\ntemporal GNN explanation.",
    "start": "4840950",
    "end": "4848060"
  },
  {
    "text": "I think it's either this\nyear [INAUDIBLE] last year. So it's a very,\nvery recent topic.",
    "start": "4848060",
    "end": "4853220"
  },
  {
    "text": "I've only seen one paper. And we're actually working\non another one that does like temporal explanation.",
    "start": "4853220",
    "end": "4858820"
  },
  {
    "text": "The idea is also borrowed from\none of the [INAUDIBLE] previous [INAUDIBLE] science and\nnature paper on the temporal--",
    "start": "4858820",
    "end": "4866460"
  },
  {
    "text": "based on the temporal motifs,\nthe [INAUDIBLE] of having [AUDIO OUT]. A very good topic.",
    "start": "4866460",
    "end": "4872060"
  },
  {
    "text": " Maybe we're out of time. Thank you, Rex. Great job.",
    "start": "4872060",
    "end": "4877640"
  },
  {
    "text": "Thank you. [AUDIENCE APPLAUDING]",
    "start": "4877640",
    "end": "4882790"
  },
  {
    "start": "4882790",
    "end": "4886000"
  }
]