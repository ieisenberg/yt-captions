[
  {
    "text": "So our next topic is ridge\nregression and the lasso.",
    "start": "0",
    "end": "3770"
  },
  {
    "text": "You'll be happy to know that\nthe Python professional is back.",
    "start": "3770",
    "end": "8280"
  },
  {
    "text": "Yes.",
    "start": "8280",
    "end": "8780"
  },
  {
    "text": "Well, thank you, Trevor.",
    "start": "8780",
    "end": "10219"
  },
  {
    "text": "OK.",
    "start": "10220",
    "end": "11510"
  },
  {
    "text": "So we just saw a forward\nstepwise selection,",
    "start": "11510",
    "end": "14930"
  },
  {
    "text": "which gives us a way to\nchoose important variables",
    "start": "14930",
    "end": "17120"
  },
  {
    "text": "in the model, and we'll see\nthese other two methods-- ridge",
    "start": "17120",
    "end": "21525"
  },
  {
    "text": "regression and lasso.",
    "start": "21525",
    "end": "22400"
  },
  {
    "text": "Ridge regression isn't\nreally a selection method,",
    "start": "22400",
    "end": "24442"
  },
  {
    "text": "but the lasso is something\nlike forward stepwise",
    "start": "24442",
    "end": "27500"
  },
  {
    "text": "that it does model selection\nas well as estimation.",
    "start": "27500",
    "end": "30365"
  },
  {
    "text": "OK.",
    "start": "30365",
    "end": "31820"
  },
  {
    "text": "So we're going to use\nthe elastic net method",
    "start": "31820",
    "end": "35870"
  },
  {
    "text": "from scikit-learn, and we use\nthat for simplicity and that",
    "start": "35870",
    "end": "40790"
  },
  {
    "text": "it fits both the ridge\nregression and the lasso.",
    "start": "40790",
    "end": "43460"
  },
  {
    "text": "In fact, it sort of\ninterpolates between them.",
    "start": "43460",
    "end": "45840"
  },
  {
    "text": "That's one of the things\nwe saw in the chapter.",
    "start": "45840",
    "end": "49190"
  },
  {
    "text": "And I'll just make a\nnote here that when",
    "start": "49190",
    "end": "51109"
  },
  {
    "text": "you run this code\nright now, there",
    "start": "51110",
    "end": "52880"
  },
  {
    "text": "are a lot of warnings for\nridge regression in particular,",
    "start": "52880",
    "end": "55790"
  },
  {
    "text": "and that has something to\ndo with, arguably, a bug.",
    "start": "55790",
    "end": "61110"
  },
  {
    "text": "In any case, the solution\nis actually fine.",
    "start": "61110",
    "end": "63340"
  },
  {
    "text": "So we can ignore those\nwarnings for now in any case,",
    "start": "63340",
    "end": "66690"
  },
  {
    "text": "but just a heads up.",
    "start": "66690",
    "end": "68070"
  },
  {
    "start": "68070",
    "end": "71610"
  },
  {
    "text": "So what we're going\nto do is we're",
    "start": "71610",
    "end": "73230"
  },
  {
    "text": "going to use the same data.",
    "start": "73230",
    "end": "75190"
  },
  {
    "text": "Now, in the chapter when we\ntalked about ridge regression",
    "start": "75190",
    "end": "78210"
  },
  {
    "text": "and lasso, we noted that\nit's common to scale",
    "start": "78210",
    "end": "80742"
  },
  {
    "text": "the data before we use ridge\nregression or lasso just",
    "start": "80742",
    "end": "82950"
  },
  {
    "text": "because-- so you're\npenalizing each variable",
    "start": "82950",
    "end": "85110"
  },
  {
    "text": "in a comparable fashion.",
    "start": "85110",
    "end": "86400"
  },
  {
    "text": "So we're going to\nscale the data first.",
    "start": "86400",
    "end": "88920"
  },
  {
    "text": "We could have used the\nstandard scalar method,",
    "start": "88920",
    "end": "91380"
  },
  {
    "text": "but we just, at this\npoint, hand-scale it.",
    "start": "91380",
    "end": "94719"
  },
  {
    "text": "And then what we're\ngoing to do is",
    "start": "94720",
    "end": "97110"
  },
  {
    "text": "we're going to get a\nridge solution path.",
    "start": "97110",
    "end": "99990"
  },
  {
    "text": "So that's using this\nscikit-learn ElasticNet.path",
    "start": "99990",
    "end": "103680"
  },
  {
    "text": "method.",
    "start": "103680",
    "end": "104340"
  },
  {
    "text": "And for that, we give it this\narray of values of lambda.",
    "start": "104340",
    "end": "109289"
  },
  {
    "text": "The argument lambda\nis called alpha",
    "start": "109290",
    "end": "111330"
  },
  {
    "text": "in the ElasticNet\npackage, but that's",
    "start": "111330",
    "end": "113850"
  },
  {
    "text": "what lambda is in the book.",
    "start": "113850",
    "end": "115229"
  },
  {
    "text": "And this l1 ratio here,\nthis is that parameter",
    "start": "115230",
    "end": "118170"
  },
  {
    "text": "that interpolates between the\nridge regression estimator",
    "start": "118170",
    "end": "121770"
  },
  {
    "text": "and the lasso estimator.",
    "start": "121770",
    "end": "123039"
  },
  {
    "text": "So at 0, this is the ridge\nregression estimator,",
    "start": "123040",
    "end": "125760"
  },
  {
    "text": "and at 1, it's the\nlasso estimator.",
    "start": "125760",
    "end": "127782"
  },
  {
    "text": "So we'll see later\nwhen we do the lasso.",
    "start": "127782",
    "end": "129449"
  },
  {
    "text": "All we have to change\nis this from 0 to 1.",
    "start": "129449",
    "end": "131810"
  },
  {
    "text": "OK.",
    "start": "131810",
    "end": "133709"
  },
  {
    "text": "And so we asked for this\npath, and it gives us",
    "start": "133710",
    "end": "137910"
  },
  {
    "text": "a 19 by 100 matrix.",
    "start": "137910",
    "end": "139680"
  },
  {
    "text": "There are 19 features, and\nwe have 100 values of lambda.",
    "start": "139680",
    "end": "142769"
  },
  {
    "text": "So each row of\nthis is going to be",
    "start": "142770",
    "end": "144840"
  },
  {
    "text": "a path for a single\ncoordinate, and we'll",
    "start": "144840",
    "end": "147300"
  },
  {
    "text": "plot those in just a second.",
    "start": "147300",
    "end": "148470"
  },
  {
    "start": "148470",
    "end": "152780"
  },
  {
    "text": "So we can look at the data.",
    "start": "152780",
    "end": "156298"
  },
  {
    "text": "And here, what I've done,\nscikit-learn doesn't actually",
    "start": "156298",
    "end": "158590"
  },
  {
    "text": "have variable names, but I've\nadded some variable names",
    "start": "158590",
    "end": "160930"
  },
  {
    "text": "to the features and\nan index of lambda",
    "start": "160930",
    "end": "163930"
  },
  {
    "text": "values so to be a\nlittle more informative.",
    "start": "163930",
    "end": "167439"
  },
  {
    "text": "But you can see now, I've\ntransposed that array.",
    "start": "167440",
    "end": "170780"
  },
  {
    "text": "So now, each column\nis a solution path",
    "start": "170780",
    "end": "172870"
  },
  {
    "text": "as a function of\nlambda down here.",
    "start": "172870",
    "end": "176191"
  },
  {
    "text": "Okay, let's get to the plot.",
    "start": "176191",
    "end": "177358"
  },
  {
    "text": "Here's a plot.",
    "start": "177358",
    "end": "178285"
  },
  {
    "start": "178285",
    "end": "182100"
  },
  {
    "text": "That's a pretty plot.",
    "start": "182100",
    "end": "183610"
  },
  {
    "text": "Yes.",
    "start": "183610",
    "end": "184300"
  },
  {
    "text": "They're nice, smooth curves,\nthe ridge regression estimator.",
    "start": "184300",
    "end": "187300"
  },
  {
    "text": "The legend's a little bit big.",
    "start": "187300",
    "end": "188950"
  },
  {
    "text": "We might want to put\nthat in a different place",
    "start": "188950",
    "end": "191440"
  },
  {
    "text": "if we were producing,\nbut it's in a good place.",
    "start": "191440",
    "end": "193690"
  },
  {
    "text": "At least it's not sitting\nover the coefficient path.",
    "start": "193690",
    "end": "195940"
  },
  {
    "text": "So Jonathan, each curve is what?",
    "start": "195940",
    "end": "197890"
  },
  {
    "text": "Each curve corresponds\nto one of the features.",
    "start": "197890",
    "end": "200270"
  },
  {
    "text": "So I think this blue\none here is AtBat.",
    "start": "200270",
    "end": "202510"
  },
  {
    "text": "And unfortunately, the color\nblue repeated a little bit,",
    "start": "202510",
    "end": "205959"
  },
  {
    "text": "but I'm going to say that this\nis the coefficient for AtBat.",
    "start": "205960",
    "end": "210970"
  },
  {
    "text": "This is minus log\nlambda down here.",
    "start": "210970",
    "end": "213540"
  },
  {
    "text": "So this corresponds to a\nvery large value of lambda,",
    "start": "213540",
    "end": "216260"
  },
  {
    "text": "and that's a lot\nof penalization,",
    "start": "216260",
    "end": "218030"
  },
  {
    "text": "and we can see that all\nthe coefficients under here",
    "start": "218030",
    "end": "220400"
  },
  {
    "text": "are right around 0.",
    "start": "220400",
    "end": "221629"
  },
  {
    "text": "And then as lambda gets\nsmaller and smaller, that",
    "start": "221630",
    "end": "224240"
  },
  {
    "text": "is minus log lambda\ngrows, we get",
    "start": "224240",
    "end": "226460"
  },
  {
    "text": "towards the ordinary\nsquare solution.",
    "start": "226460",
    "end": "228300"
  },
  {
    "text": "So these values here should\ncorrespond pretty closely",
    "start": "228300",
    "end": "230990"
  },
  {
    "text": "to if we just fit OLS.",
    "start": "230990",
    "end": "232550"
  },
  {
    "text": "Lovely plot.",
    "start": "232550",
    "end": "233270"
  },
  {
    "start": "233270",
    "end": "237902"
  },
  {
    "text": "OK.",
    "start": "237902",
    "end": "238402"
  },
  {
    "text": "So you can extract\nindividual coefficients",
    "start": "238402",
    "end": "242099"
  },
  {
    "text": "if you like, well, just by\nlooking at the solution path",
    "start": "242100",
    "end": "249690"
  },
  {
    "text": "we constructed, and\nwhat we're going",
    "start": "249690",
    "end": "251910"
  },
  {
    "text": "to see here is that for\ndifferent values of lambda--",
    "start": "251910",
    "end": "254730"
  },
  {
    "text": "so remember lambda is decreasing\nas we increase the index.",
    "start": "254730",
    "end": "258609"
  },
  {
    "text": "So we'll compare lambda at\nthe 40th entry, lambda 39",
    "start": "258610",
    "end": "262469"
  },
  {
    "text": "to the 50th lambda's 59,\nand what we should see",
    "start": "262470",
    "end": "266670"
  },
  {
    "text": "is that at 59, we sort\nof relaxed the penalty.",
    "start": "266670",
    "end": "269932"
  },
  {
    "text": "So it should be a bit bigger.",
    "start": "269932",
    "end": "271140"
  },
  {
    "text": "So we're just going\nto compute the--",
    "start": "271140",
    "end": "273562"
  },
  {
    "text": "we can inspect the\ncoefficients, but we'll just",
    "start": "273562",
    "end": "275520"
  },
  {
    "text": "compute the norms at the\ntwo different values.",
    "start": "275520",
    "end": "278099"
  },
  {
    "text": "So at the 40th lambda,\nthe coefficient vector",
    "start": "278100",
    "end": "281340"
  },
  {
    "text": "has length 24, and\nat the 60th lambda,",
    "start": "281340",
    "end": "285120"
  },
  {
    "text": "the coefficient\nvector has length 160.",
    "start": "285120",
    "end": "287370"
  },
  {
    "text": "And you can see that these\nare just two different places",
    "start": "287370",
    "end": "290880"
  },
  {
    "text": "on this axis.",
    "start": "290880",
    "end": "291930"
  },
  {
    "text": "As we go down the path,\nthe vector gets longer.",
    "start": "291930",
    "end": "295090"
  },
  {
    "text": "So that's the sum of squares of\nthe coefficients at those two",
    "start": "295090",
    "end": "298930"
  },
  {
    "text": "different values of lambda.",
    "start": "298930",
    "end": "300400"
  },
  {
    "text": "Yes.",
    "start": "300400",
    "end": "302090"
  },
  {
    "text": "Well, sorry, it's\nthe square root.",
    "start": "302090",
    "end": "303767"
  },
  {
    "text": "It's the norm.",
    "start": "303767",
    "end": "304350"
  },
  {
    "text": "It's the square root.",
    "start": "304350",
    "end": "305130"
  },
  {
    "text": "Just looking at that.",
    "start": "305130",
    "end": "306005"
  },
  {
    "text": "Yeah.",
    "start": "306005",
    "end": "306890"
  },
  {
    "text": "It is related.",
    "start": "306890",
    "end": "307790"
  },
  {
    "text": "I just wanted to\nanswer it correctly.",
    "start": "307790",
    "end": "309900"
  },
  {
    "text": "So I check the code.",
    "start": "309900",
    "end": "310852"
  },
  {
    "text": "So the code is helpful.",
    "start": "310852",
    "end": "311810"
  },
  {
    "text": "OK.",
    "start": "311810",
    "end": "312310"
  },
  {
    "start": "312310",
    "end": "314870"
  },
  {
    "text": "So we had computed the above\nwith using the path method.",
    "start": "314870",
    "end": "319440"
  },
  {
    "text": "But if you want\njust a single value,",
    "start": "319440",
    "end": "320970"
  },
  {
    "text": "you can use the\nElasticNet estimator",
    "start": "320970",
    "end": "323990"
  },
  {
    "text": "and give it a specific\nvalue of alpha",
    "start": "323990",
    "end": "326509"
  },
  {
    "text": "rather than a path of alphas.",
    "start": "326510",
    "end": "329130"
  },
  {
    "text": "So earlier, I said, we\ncould have used the scalar",
    "start": "329130",
    "end": "332120"
  },
  {
    "text": "to standardize our data.",
    "start": "332120",
    "end": "334940"
  },
  {
    "text": "We hand-standardized\ndata before.",
    "start": "334940",
    "end": "338030"
  },
  {
    "text": "We're going to instead\ncombine these methods now",
    "start": "338030",
    "end": "340940"
  },
  {
    "text": "in this thing called a pipeline.",
    "start": "340940",
    "end": "342990"
  },
  {
    "text": "So this is a\nscikit-learn object that",
    "start": "342990",
    "end": "344990"
  },
  {
    "text": "allows you to combine\ndifferent steps of a procedure,",
    "start": "344990",
    "end": "349220"
  },
  {
    "text": "and it might seem a\nlittle bit like overkill.",
    "start": "349220",
    "end": "351620"
  },
  {
    "text": "But what's nice about the\npipeline is we'll see later.",
    "start": "351620",
    "end": "356150"
  },
  {
    "text": "Each of these estimators have\ndifferent tuning parameters,",
    "start": "356150",
    "end": "358580"
  },
  {
    "text": "and you could wiggle them\na little bit and then",
    "start": "358580",
    "end": "361500"
  },
  {
    "text": "automatically\ncross-validate over",
    "start": "361500",
    "end": "363480"
  },
  {
    "text": "different grids\nof the parameters.",
    "start": "363480",
    "end": "365110"
  },
  {
    "text": "So I see.",
    "start": "365110",
    "end": "366960"
  },
  {
    "text": "So is the point here,\nJonathan, that each time if we",
    "start": "366960",
    "end": "370680"
  },
  {
    "text": "use cross-validation\nin each fold,",
    "start": "370680",
    "end": "372539"
  },
  {
    "text": "we want to standardize the\ntraining data in the fold.",
    "start": "372540",
    "end": "375370"
  },
  {
    "text": "Yeah.",
    "start": "375370",
    "end": "375870"
  },
  {
    "text": "That's what we're\nusing this pipeline.",
    "start": "375870",
    "end": "377310"
  },
  {
    "text": "That will do that.",
    "start": "377310",
    "end": "378000"
  },
  {
    "text": "And that will do it for you.",
    "start": "378000",
    "end": "378600"
  },
  {
    "text": "Yes.",
    "start": "378600",
    "end": "379100"
  },
  {
    "text": "That's very handy.",
    "start": "379100",
    "end": "379980"
  },
  {
    "text": "Yeah.",
    "start": "379980",
    "end": "380480"
  },
  {
    "text": "Whereas if we used our x-scale\nhere that we had done before,",
    "start": "380480",
    "end": "385050"
  },
  {
    "text": "it would have done a\nglobal standardization",
    "start": "385050",
    "end": "387000"
  },
  {
    "text": "not within each fold.",
    "start": "387000",
    "end": "388530"
  },
  {
    "text": "So for something\nlike standardization,",
    "start": "388530",
    "end": "390270"
  },
  {
    "text": "it may not matter too much.",
    "start": "390270",
    "end": "391509"
  },
  {
    "text": "But for other\ntransformations, you",
    "start": "391510",
    "end": "393270"
  },
  {
    "text": "might want to repeat\nit on each fold.",
    "start": "393270",
    "end": "394930"
  },
  {
    "text": "So this pipeline method\nallows you to do that.",
    "start": "394930",
    "end": "397020"
  },
  {
    "text": "I can see it's very useful.",
    "start": "397020",
    "end": "398400"
  },
  {
    "text": "Yes.",
    "start": "398400",
    "end": "398940"
  },
  {
    "text": "And so the pipeline\nmethod is just",
    "start": "398940",
    "end": "400920"
  },
  {
    "text": "another scikit-learn estimator.",
    "start": "400920",
    "end": "402940"
  },
  {
    "text": "So we can cross-validate it\njust like any other scikit-learn",
    "start": "402940",
    "end": "406470"
  },
  {
    "text": "estimator, and it will give us\nan estimate of whatever metric",
    "start": "406470",
    "end": "410310"
  },
  {
    "text": "we saw earlier.",
    "start": "410310",
    "end": "411585"
  },
  {
    "text": "We could compute\nnegative CP with this",
    "start": "411585",
    "end": "413490"
  },
  {
    "text": "or whatever our\ncustom metric is.",
    "start": "413490",
    "end": "415410"
  },
  {
    "text": "Can't wait to see the results.",
    "start": "415410",
    "end": "417720"
  },
  {
    "text": "Yes.",
    "start": "417720",
    "end": "418610"
  },
  {
    "text": "As we'll see, one of\nthe other nice things",
    "start": "418610",
    "end": "420360"
  },
  {
    "text": "that you can search over\nvariations in the parameters",
    "start": "420360",
    "end": "423539"
  },
  {
    "text": "here, right?",
    "start": "423540",
    "end": "424170"
  },
  {
    "text": "Ridge has the parameter\nwe saw up here.",
    "start": "424170",
    "end": "426937"
  },
  {
    "text": "Oh, it's right here.",
    "start": "426937",
    "end": "427770"
  },
  {
    "text": "Ridge has a parameter alpha.",
    "start": "427770",
    "end": "428936"
  },
  {
    "text": "If we give the\nsame set of alphas",
    "start": "428937",
    "end": "431670"
  },
  {
    "text": "that we gave to\nthe path argument,",
    "start": "431670",
    "end": "433290"
  },
  {
    "text": "it will search over those same\npaths to find the optimal CV.",
    "start": "433290",
    "end": "436270"
  },
  {
    "text": "OK.",
    "start": "436270",
    "end": "436770"
  },
  {
    "start": "436770",
    "end": "442259"
  },
  {
    "text": "So let's use the scikit-learn's\ncross-validation,",
    "start": "442260",
    "end": "448710"
  },
  {
    "text": "or in this case, we're going\nto do a test train split.",
    "start": "448710",
    "end": "453479"
  },
  {
    "text": "It's using the same\ncross-validation function,",
    "start": "453480",
    "end": "455640"
  },
  {
    "text": "but it's actually going\nto use the validation",
    "start": "455640",
    "end": "458910"
  },
  {
    "text": "method to estimate error.",
    "start": "458910",
    "end": "461770"
  },
  {
    "text": "And we know that it's using\nthe validation method because",
    "start": "461770",
    "end": "464580"
  },
  {
    "text": "instead of using KFold, we're\nusing this shuffle split.",
    "start": "464580",
    "end": "469639"
  },
  {
    "text": "Test size is 50%.",
    "start": "469640",
    "end": "471210"
  },
  {
    "text": "That corresponds to split\ntest and training equally,",
    "start": "471210",
    "end": "473789"
  },
  {
    "text": "and setting the random\nstate fixes the split so",
    "start": "473790",
    "end": "479640"
  },
  {
    "text": "that when we rerun this code,\nwe'll get the same answer.",
    "start": "479640",
    "end": "482740"
  },
  {
    "text": "So this gives us using--",
    "start": "482740",
    "end": "486539"
  },
  {
    "text": "oh, this is just using the--",
    "start": "486540",
    "end": "488160"
  },
  {
    "text": "oh, I guess I should have--",
    "start": "488160",
    "end": "489480"
  },
  {
    "text": "maybe we should\nhave used pipe here.",
    "start": "489480",
    "end": "492240"
  },
  {
    "text": "So this is the cross-validation\nerror of the ridge estimator,",
    "start": "492240",
    "end": "497190"
  },
  {
    "text": "and that's one of the\nparts of the pipeline.",
    "start": "497190",
    "end": "499680"
  },
  {
    "start": "499680",
    "end": "504560"
  },
  {
    "text": "And we can change the parameters\nof the estimator, rerun",
    "start": "504560",
    "end": "508100"
  },
  {
    "text": "cross-validation, and it\nwill use the same split",
    "start": "508100",
    "end": "511070"
  },
  {
    "text": "as it did previous because\nwe had fixed the state.",
    "start": "511070",
    "end": "513799"
  },
  {
    "text": "And at this point, we've used\na very large value of lambda",
    "start": "513799",
    "end": "517307"
  },
  {
    "text": "to get something\nlike the null model.",
    "start": "517308",
    "end": "518849"
  },
  {
    "text": "So the null cross-validation\nerror or null test error",
    "start": "518850",
    "end": "523909"
  },
  {
    "text": "with a test train\nsplit is about 230,000",
    "start": "523909",
    "end": "528500"
  },
  {
    "text": "compared to about 134,000\nfor the ridge estimator.",
    "start": "528500",
    "end": "531780"
  },
  {
    "text": "So quite an improvement.",
    "start": "531780",
    "end": "535420"
  },
  {
    "text": "And if we wanted to use\ncross-validation, a KFold,",
    "start": "535420",
    "end": "539842"
  },
  {
    "text": "of course, all we\nwould have to do",
    "start": "539843",
    "end": "541260"
  },
  {
    "text": "is change this argument here\nto be a KFold argument instead",
    "start": "541260",
    "end": "545700"
  },
  {
    "text": "of the test training split.",
    "start": "545700",
    "end": "548280"
  },
  {
    "text": "So now, we'll do the\nsame thing, but we're",
    "start": "548280",
    "end": "552990"
  },
  {
    "text": "going to use the pipe\nestimator, which actually scales",
    "start": "552990",
    "end": "556589"
  },
  {
    "text": "and standardizes each time.",
    "start": "556590",
    "end": "560910"
  },
  {
    "text": "This is an example\nhere that where",
    "start": "560910",
    "end": "562860"
  },
  {
    "text": "we'll see how to search\nover a grid of parameters.",
    "start": "562860",
    "end": "565709"
  },
  {
    "text": "So this function\nGridSearchCV, it",
    "start": "565710",
    "end": "569070"
  },
  {
    "text": "takes some form of\ncross-validation.",
    "start": "569070",
    "end": "571830"
  },
  {
    "text": "In this case, this test\ntrain split we saw above,",
    "start": "571830",
    "end": "575550"
  },
  {
    "text": "and it takes, of course--",
    "start": "575550",
    "end": "577720"
  },
  {
    "text": "we have to describe the\nvariation in the parameters.",
    "start": "577720",
    "end": "580180"
  },
  {
    "text": "So that's this\nargument param_grid.",
    "start": "580180",
    "end": "583335"
  },
  {
    "text": "The format of this is--",
    "start": "583335",
    "end": "586080"
  },
  {
    "text": "well, there was an\nestimator called ridge,",
    "start": "586080",
    "end": "588990"
  },
  {
    "text": "and it has an alpha parameter.",
    "start": "588990",
    "end": "590970"
  },
  {
    "text": "So this ridge__alpha, this tells\nsklearn to vary alpha along all",
    "start": "590970",
    "end": "598230"
  },
  {
    "text": "these values lambdas, and those\nare the same 100 values we saw",
    "start": "598230",
    "end": "600992"
  },
  {
    "text": "before.",
    "start": "600992",
    "end": "601492"
  },
  {
    "text": "OK.",
    "start": "601492",
    "end": "601992"
  },
  {
    "start": "601992",
    "end": "606410"
  },
  {
    "text": "And as I said, if we want to use\nKFold cross-validation instead",
    "start": "606410",
    "end": "610339"
  },
  {
    "text": "of the test train split, we\nwould just give this argument",
    "start": "610340",
    "end": "614210"
  },
  {
    "text": "the KFold argument.",
    "start": "614210",
    "end": "615930"
  },
  {
    "text": "So now, this, again, is\na scikit-learn estimator.",
    "start": "615930",
    "end": "621540"
  },
  {
    "text": "So we can fit it on our x and y.",
    "start": "621540",
    "end": "624440"
  },
  {
    "text": "And then at the end, it\nhas these new attributes--",
    "start": "624440",
    "end": "628490"
  },
  {
    "text": "the best_params and\nthe best_estimator.",
    "start": "628490",
    "end": "631940"
  },
  {
    "text": "And our grid of parameters\nhere only had a single entry.",
    "start": "631940",
    "end": "637730"
  },
  {
    "text": "So the best params has an entry\nfor ridge alpha, and this best",
    "start": "637730",
    "end": "643610"
  },
  {
    "text": "estimator is,\nwell, this pipeline",
    "start": "643610",
    "end": "646700"
  },
  {
    "text": "where the ridge alpha is\nset to the best alpha.",
    "start": "646700",
    "end": "649350"
  },
  {
    "text": "So that's what this returns.",
    "start": "649350",
    "end": "652334"
  },
  {
    "text": "OK.",
    "start": "652335",
    "end": "652835"
  },
  {
    "start": "652835",
    "end": "655730"
  },
  {
    "text": "Let's see.",
    "start": "655730",
    "end": "656389"
  },
  {
    "text": "And of course, for our\npipeline here, there's",
    "start": "656390",
    "end": "661250"
  },
  {
    "text": "not that many other interesting\nparameters in the scalar,",
    "start": "661250",
    "end": "663770"
  },
  {
    "text": "but there could have\nbeen something maybe",
    "start": "663770",
    "end": "665700"
  },
  {
    "text": "in the-- we could have\ntuned the l1 ratio to go",
    "start": "665700",
    "end": "669690"
  },
  {
    "text": "from ridge regression to lasso.",
    "start": "669690",
    "end": "671760"
  },
  {
    "text": "But we haven't even\nseen lasso yet,",
    "start": "671760",
    "end": "673440"
  },
  {
    "text": "so we'll see that coming up.",
    "start": "673440",
    "end": "674730"
  },
  {
    "text": "OK.",
    "start": "674730",
    "end": "675230"
  },
  {
    "start": "675230",
    "end": "678540"
  },
  {
    "text": "So besides finding\nthe best estimator,",
    "start": "678540",
    "end": "681430"
  },
  {
    "text": "it also computes the\nscore within each fold,",
    "start": "681430",
    "end": "686339"
  },
  {
    "text": "like we did by hand before.",
    "start": "686340",
    "end": "688200"
  },
  {
    "text": "So earlier, we saw\nfor forward stepwise,",
    "start": "688200",
    "end": "690540"
  },
  {
    "text": "we had 20 models and\n5 different folds.",
    "start": "690540",
    "end": "693759"
  },
  {
    "text": "So we had 20 by 5 matrix\nof mean squared error.",
    "start": "693760",
    "end": "697590"
  },
  {
    "text": "For this data here, there\nwill be 100 by 5 matrix,",
    "start": "697590",
    "end": "702690"
  },
  {
    "text": "and it actually computes\nthe mean test score for us",
    "start": "702690",
    "end": "707700"
  },
  {
    "text": "and the standard deviation of\nthe test score all by itself.",
    "start": "707700",
    "end": "711100"
  },
  {
    "text": "So it's done what we did\nby hand automatically.",
    "start": "711100",
    "end": "713730"
  },
  {
    "text": "Very good.",
    "start": "713730",
    "end": "715050"
  },
  {
    "text": "And so we can make a\nplot similar to what",
    "start": "715050",
    "end": "717630"
  },
  {
    "text": "we saw in forward stepwise,\nbut the axis here is different.",
    "start": "717630",
    "end": "720940"
  },
  {
    "text": "This is the\nregularization parameter.",
    "start": "720940",
    "end": "722700"
  },
  {
    "text": "In forward stepwise,\nthe complexity",
    "start": "722700",
    "end": "727440"
  },
  {
    "text": "here was going from the null\nmodel with just an intercept",
    "start": "727440",
    "end": "729990"
  },
  {
    "text": "to all 20 features.",
    "start": "729990",
    "end": "731850"
  },
  {
    "text": "Here, it goes from the null\nmodel to, well, the OLS model",
    "start": "731850",
    "end": "736170"
  },
  {
    "text": "but along this ridge\npath, and we'll",
    "start": "736170",
    "end": "738550"
  },
  {
    "text": "see for the lasso\nsimilar path shortly.",
    "start": "738550",
    "end": "740890"
  },
  {
    "text": "Similar a little bit\nto forward stepwise",
    "start": "740890",
    "end": "743470"
  },
  {
    "text": "and that it doesn't\nincrease at the end.",
    "start": "743470",
    "end": "746199"
  },
  {
    "text": "It just gets down\nand stays flat.",
    "start": "746200",
    "end": "748210"
  },
  {
    "text": "That's right.",
    "start": "748210",
    "end": "748930"
  },
  {
    "text": "So I mean, we talk\nabout in the book",
    "start": "748930",
    "end": "751690"
  },
  {
    "text": "that eventually, you'd expect\nthese curves to turn up.",
    "start": "751690",
    "end": "755740"
  },
  {
    "text": "But for the data sets\nwe've happen to use,",
    "start": "755740",
    "end": "757600"
  },
  {
    "text": "we haven't seen very\ndrastic overfitting.",
    "start": "757600",
    "end": "761620"
  },
  {
    "text": "There's a slight turn-up,\nbut it's not abrupt.",
    "start": "761620",
    "end": "764980"
  },
  {
    "text": "Then, again, it's\nonly 20 features,",
    "start": "764980",
    "end": "767019"
  },
  {
    "text": "so it's not that\ncomplicated to model.",
    "start": "767020",
    "end": "769690"
  },
  {
    "text": "When picking, optimizing\nparameters in a CV grid,",
    "start": "769690",
    "end": "774440"
  },
  {
    "text": "you have to pick your metric.",
    "start": "774440",
    "end": "775700"
  },
  {
    "text": "So here, we included this\nmetric we wanted to use--",
    "start": "775700",
    "end": "778750"
  },
  {
    "text": "mean_squared_error.",
    "start": "778750",
    "end": "779740"
  },
  {
    "text": "And of course, to maximize it\nit's neg_mean_squared_error.",
    "start": "779740",
    "end": "782360"
  },
  {
    "text": "So that's what that\nstring indicates.",
    "start": "782360",
    "end": "784149"
  },
  {
    "text": "If we hadn't done that,\nit would be r squared.",
    "start": "784150",
    "end": "786830"
  },
  {
    "text": "So that's what the\nlast plot here shows.",
    "start": "786830",
    "end": "789310"
  },
  {
    "text": "So without saying\nanything, the estimator",
    "start": "789310",
    "end": "794620"
  },
  {
    "text": "uses r squared as a metric.",
    "start": "794620",
    "end": "797380"
  },
  {
    "text": "Similarly, there'll be 100\nby 5 matrix of r squared.",
    "start": "797380",
    "end": "800880"
  },
  {
    "text": "We will average those and\nget the standard errors.",
    "start": "800880",
    "end": "804900"
  },
  {
    "text": "That's what this\nplot is producing.",
    "start": "804900",
    "end": "807732"
  },
  {
    "text": "And of course, it\nhas the same form",
    "start": "807732",
    "end": "809190"
  },
  {
    "text": "because r squared is\nvery closely related.",
    "start": "809190",
    "end": "810930"
  },
  {
    "text": "OK.",
    "start": "810930",
    "end": "811430"
  },
  {
    "start": "811430",
    "end": "813940"
  },
  {
    "text": "So lastly, I want to\ntalk about something",
    "start": "813940",
    "end": "818770"
  },
  {
    "text": "for both ridge\nregression and the lasso,",
    "start": "818770",
    "end": "823060"
  },
  {
    "text": "is that there are very fast\nmethods to do cross-validation.",
    "start": "823060",
    "end": "826220"
  },
  {
    "text": "So above, we\nsearched over lambda",
    "start": "826220",
    "end": "828970"
  },
  {
    "text": "using the grid, the pipeline,\nand the GridSearchCV method.",
    "start": "828970",
    "end": "833500"
  },
  {
    "text": "There are custom-built\nones for ElasticNet.",
    "start": "833500",
    "end": "836470"
  },
  {
    "text": "That's this ElasticNet\nCV function.",
    "start": "836470",
    "end": "838186"
  },
  {
    "text": "OK.",
    "start": "838187",
    "end": "840100"
  },
  {
    "text": "So what this is\nessentially going to do",
    "start": "840100",
    "end": "842139"
  },
  {
    "text": "is the same thing we just did,\nbut it's very specially built",
    "start": "842140",
    "end": "846250"
  },
  {
    "text": "for the ElasticNet.",
    "start": "846250",
    "end": "848020"
  },
  {
    "text": "That is the ridge estimator and\nupcoming the lasso estimator",
    "start": "848020",
    "end": "852010"
  },
  {
    "text": "because it's a\ncommonly used method.",
    "start": "852010",
    "end": "856400"
  },
  {
    "text": "So if we plot the\nMSE out of that,",
    "start": "856400",
    "end": "859540"
  },
  {
    "text": "this plot is unsurprisingly very\nsimilar to what we saw before.",
    "start": "859540",
    "end": "863144"
  },
  {
    "text": "This was just to show\nthere's a specialized method.",
    "start": "863145",
    "end": "865270"
  },
  {
    "text": "OK.",
    "start": "865270",
    "end": "865770"
  },
  {
    "start": "865770",
    "end": "874113"
  },
  {
    "text": "So I think I'm going to skip\nthis in the interest of time.",
    "start": "874113",
    "end": "876529"
  },
  {
    "text": "So what this section\nof code, what it does",
    "start": "876530",
    "end": "879010"
  },
  {
    "text": "is it recognizes that,\nin tuning a parameter",
    "start": "879010",
    "end": "882820"
  },
  {
    "text": "by cross-validation,\nwe are still",
    "start": "882820",
    "end": "886630"
  },
  {
    "text": "somewhat left in without an\nhonest estimate of error.",
    "start": "886630",
    "end": "889670"
  },
  {
    "text": "So what we've done here is\nsplit into training and test,",
    "start": "889670",
    "end": "893950"
  },
  {
    "text": "run cross-validation\non the training data,",
    "start": "893950",
    "end": "896290"
  },
  {
    "text": "and then evaluate the final\nestimator on the test data.",
    "start": "896290",
    "end": "900110"
  },
  {
    "text": "And so that's why\nthere's a split here",
    "start": "900110",
    "end": "901959"
  },
  {
    "text": "into training and test,\na KFold cross-validation.",
    "start": "901960",
    "end": "906250"
  },
  {
    "text": "Well, that's what the\ncode will compute here.",
    "start": "906250",
    "end": "910160"
  },
  {
    "text": "So it's 132,000.",
    "start": "910160",
    "end": "913048"
  },
  {
    "text": "That's not that different from\nthe actual cross-validated one",
    "start": "913048",
    "end": "915589"
  },
  {
    "text": "we saw before.",
    "start": "915590",
    "end": "916173"
  },
  {
    "start": "916173",
    "end": "918720"
  },
  {
    "text": "So our final method in\nthis lab is the lasso.",
    "start": "918720",
    "end": "923129"
  },
  {
    "text": "And as we mentioned\nearlier in the lab,",
    "start": "923130",
    "end": "924990"
  },
  {
    "text": "this is a method\nthat does selection,",
    "start": "924990",
    "end": "928290"
  },
  {
    "text": "like forward stepwise, as well\nas regularized estimation,",
    "start": "928290",
    "end": "932399"
  },
  {
    "text": "like ridge regression.",
    "start": "932400",
    "end": "934510"
  },
  {
    "text": "And so as we mentioned\nearlier, the difference",
    "start": "934510",
    "end": "937620"
  },
  {
    "text": "between ridge and lasso\nin terms of the code,",
    "start": "937620",
    "end": "940020"
  },
  {
    "text": "at least this version\nof the code we're using",
    "start": "940020",
    "end": "941970"
  },
  {
    "text": "is this l1 ratio.",
    "start": "941970",
    "end": "943560"
  },
  {
    "text": "At 0, this is the\nridge estimator.",
    "start": "943560",
    "end": "945630"
  },
  {
    "text": "At 1, it's the lasso estimator.",
    "start": "945630",
    "end": "948640"
  },
  {
    "text": "So we're going to fit a\nlasso path of solutions",
    "start": "948640",
    "end": "954360"
  },
  {
    "text": "using the same function\nElasticNet CV we saw above.",
    "start": "954360",
    "end": "959070"
  },
  {
    "text": "Though, it will\ngive us the lasso",
    "start": "959070",
    "end": "960810"
  },
  {
    "text": "fit this time instead\nof the ridge fit",
    "start": "960810",
    "end": "962490"
  },
  {
    "text": "because of this difference\nin the parameters.",
    "start": "962490",
    "end": "966709"
  },
  {
    "text": "So as we did for the\nridge regression,",
    "start": "966710",
    "end": "968905"
  },
  {
    "text": "we can make a data\nframe, and you",
    "start": "968905",
    "end": "970280"
  },
  {
    "text": "can inspect that, if you\nlike, of the solution paths",
    "start": "970280",
    "end": "973310"
  },
  {
    "text": "to the lasso, and we can\nalso make a plot like we",
    "start": "973310",
    "end": "978050"
  },
  {
    "text": "did for the ridge estimators.",
    "start": "978050",
    "end": "979440"
  },
  {
    "text": "So this plot is going to be\ninterpreted very similarly",
    "start": "979440",
    "end": "982220"
  },
  {
    "text": "to the ridge\nregression estimator.",
    "start": "982220",
    "end": "984050"
  },
  {
    "text": "Though, it looks a\nlittle bit different.",
    "start": "984050",
    "end": "987290"
  },
  {
    "text": "Let's see, Trevor.",
    "start": "987290",
    "end": "988040"
  },
  {
    "text": "What's the biggest thing\nyou notice about this plot?",
    "start": "988040",
    "end": "990620"
  },
  {
    "text": "The biggest thing I\nnotice is, it's not",
    "start": "990620",
    "end": "992990"
  },
  {
    "text": "as smooth as the earlier one,\nand some coefficients are",
    "start": "992990",
    "end": "997100"
  },
  {
    "text": "0 for a large\nportion of the path,",
    "start": "997100",
    "end": "1000160"
  },
  {
    "text": "and then they jump away from 0.",
    "start": "1000160",
    "end": "1002920"
  },
  {
    "text": "Of course, Trevor knew the\nanswer to that question before.",
    "start": "1002920",
    "end": "1005980"
  },
  {
    "text": "So yes, the lasso,\nif you note for--",
    "start": "1005980",
    "end": "1009100"
  },
  {
    "text": "At my age, you\ncan't rely on that.",
    "start": "1009100",
    "end": "1012430"
  },
  {
    "text": "So for some large values\nof lambda, remember,",
    "start": "1012430",
    "end": "1015520"
  },
  {
    "text": "this is minus log lambda.",
    "start": "1015520",
    "end": "1016820"
  },
  {
    "text": "So these are large\nvalues of lambda.",
    "start": "1016820",
    "end": "1018370"
  },
  {
    "text": "So a lot of the\ncoefficients are 0.",
    "start": "1018370",
    "end": "1020020"
  },
  {
    "text": "And in fact, hidden\nunder this legend, which",
    "start": "1020020",
    "end": "1022060"
  },
  {
    "text": "is a little unfortunate,\nthey're all 0 at the beginning.",
    "start": "1022060",
    "end": "1025670"
  },
  {
    "text": "And then some coefficients\ncome in one by one.",
    "start": "1025670",
    "end": "1028609"
  },
  {
    "text": "And sometimes they\nactually go out.",
    "start": "1028609",
    "end": "1030569"
  },
  {
    "text": "I'm not sure.",
    "start": "1030569",
    "end": "1031339"
  },
  {
    "text": "Yes, in this, we can see maybe\na little coefficient come off",
    "start": "1031339",
    "end": "1034459"
  },
  {
    "text": "and then go back to 0\nand then come off again.",
    "start": "1034460",
    "end": "1037069"
  },
  {
    "text": "So these paths are, well,\nas we change lambda,",
    "start": "1037069",
    "end": "1040250"
  },
  {
    "text": "the lasso solution changes.",
    "start": "1040250",
    "end": "1041449"
  },
  {
    "text": "And what's notable\nabout the lasso,",
    "start": "1041450",
    "end": "1042809"
  },
  {
    "text": "as we talked about\nin the lecture,",
    "start": "1042810",
    "end": "1044227"
  },
  {
    "text": "is, of course, that\nit can do selection.",
    "start": "1044227",
    "end": "1046760"
  },
  {
    "text": "It can set some\ncoefficients exactly to 0.",
    "start": "1046760",
    "end": "1049190"
  },
  {
    "text": "So if we select a value\nof lambda around here,",
    "start": "1049190",
    "end": "1052730"
  },
  {
    "text": "then this will correspond to--",
    "start": "1052730",
    "end": "1055820"
  },
  {
    "text": "we'll have a solution with,\nmaybe, this looks like seven",
    "start": "1055820",
    "end": "1058710"
  },
  {
    "text": "or eight non-zero coefficients\nout of a total of 20.",
    "start": "1058710",
    "end": "1062330"
  },
  {
    "text": "So this estimate this\nis in between the ridge",
    "start": "1062330",
    "end": "1064789"
  },
  {
    "text": "and forward stepwise, and it's\nquite a popular estimator.",
    "start": "1064790",
    "end": "1070280"
  },
  {
    "text": "We can look at our test error.",
    "start": "1070280",
    "end": "1076200"
  },
  {
    "text": "So using the optimal test error\nfor ridge, I guess going back",
    "start": "1076200",
    "end": "1081289"
  },
  {
    "text": "was 115,000.",
    "start": "1081290",
    "end": "1082610"
  },
  {
    "text": "For lasso, it's about 115,000.",
    "start": "1082610",
    "end": "1085670"
  },
  {
    "text": "Very similar.",
    "start": "1085670",
    "end": "1086330"
  },
  {
    "text": "Very similar.",
    "start": "1086330",
    "end": "1086990"
  },
  {
    "text": "Yes.",
    "start": "1086990",
    "end": "1087490"
  },
  {
    "start": "1087490",
    "end": "1090370"
  },
  {
    "text": "Let's just look at\none big difference",
    "start": "1090370",
    "end": "1092462"
  },
  {
    "text": "before I go back to the plot.",
    "start": "1092462",
    "end": "1093670"
  },
  {
    "text": "One big difference is that, when\nwe looked at the optimal choice",
    "start": "1093670",
    "end": "1098110"
  },
  {
    "text": "of lambda, the coefficient\nvector for the lasso actually",
    "start": "1098110",
    "end": "1101020"
  },
  {
    "text": "does have some 0's.",
    "start": "1101020",
    "end": "1102070"
  },
  {
    "text": "So it's actually\nselected, I think,",
    "start": "1102070",
    "end": "1104019"
  },
  {
    "text": "five or six coefficients\nout of the 19 to be 0.",
    "start": "1104020",
    "end": "1106828"
  },
  {
    "text": "Whereas the ridge, of course,\nall of them are non-zero.",
    "start": "1106828",
    "end": "1109120"
  },
  {
    "text": "Some of them might be\nsmall, but they're non-zero.",
    "start": "1109120",
    "end": "1112600"
  },
  {
    "text": "And as in the ridge\nregression example,",
    "start": "1112600",
    "end": "1115640"
  },
  {
    "text": "we can also make this plot of\ncross-validated mean squared",
    "start": "1115640",
    "end": "1118630"
  },
  {
    "text": "error to choose.",
    "start": "1118630",
    "end": "1119710"
  },
  {
    "text": "And I've indicated\nhere the lambda",
    "start": "1119710",
    "end": "1122230"
  },
  {
    "text": "selected as the best using the\nfive-fold k cross-validation.",
    "start": "1122230",
    "end": "1129429"
  },
  {
    "text": "So that wraps up the\nlab for chapter 6",
    "start": "1129430",
    "end": "1133030"
  },
  {
    "text": "for what we'll talk about today.",
    "start": "1133030",
    "end": "1134500"
  },
  {
    "text": "There is an additional section\non principal components",
    "start": "1134500",
    "end": "1137440"
  },
  {
    "text": "regression and partial least\nsquares regression that we",
    "start": "1137440",
    "end": "1140139"
  },
  {
    "text": "encourage you to do offline.",
    "start": "1140140",
    "end": "1142770"
  },
  {
    "start": "1142770",
    "end": "1148000"
  }
]