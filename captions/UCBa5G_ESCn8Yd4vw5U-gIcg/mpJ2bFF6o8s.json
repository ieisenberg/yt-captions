[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "Okay. Welcome back, everyone. Let's get started. So welcome to Lecture 10.",
    "start": "4100",
    "end": "11430"
  },
  {
    "text": "The topic for today will- will be, neural networks and backpropagation under the umbrella of deep learning.",
    "start": "11430",
    "end": "18330"
  },
  {
    "text": "So before we jump into today's topics, quick recap of what we covered,",
    "start": "18330",
    "end": "24029"
  },
  {
    "text": "in Lecture 9, last Friday. So Lecture 9was all about Bayesian methods and we saw two different kinds of Bayesian methods,",
    "start": "24030",
    "end": "31755"
  },
  {
    "text": "parametric and non-parametric with an example each. In parametric we saw Bayesian linear regression and the general approach in, with,",
    "start": "31755",
    "end": "42405"
  },
  {
    "start": "37000",
    "end": "113000"
  },
  {
    "text": "with Bayesian- with- with Bayesian methods for supervised machine learning in a parameterized setting is to",
    "start": "42405",
    "end": "47960"
  },
  {
    "text": "have a parameter Theta and we assume it is distributed according to a prior distribution,",
    "start": "47960",
    "end": "54245"
  },
  {
    "text": "sum P of Theta, right? And this is the assumption that distinguishes Bayesian methods from frequentist methods,",
    "start": "54245",
    "end": "61670"
  },
  {
    "text": "that we assign a prior probability to the unknown parameters and then,",
    "start": "61670",
    "end": "67830"
  },
  {
    "text": "again in the supervised setting, we assume Y comes from sum distribution P of Y given, Theta,",
    "start": "67830",
    "end": "75040"
  },
  {
    "text": "X which is also called the likelihood and from this, from the observed data, where,",
    "start": "75040",
    "end": "81320"
  },
  {
    "text": "X and Y are your training set, we construct the posterior distribution, where Theta hat,",
    "start": "81320",
    "end": "87125"
  },
  {
    "text": "which is now a random variable, is- is,",
    "start": "87125",
    "end": "92290"
  },
  {
    "text": "is derived as P of Theta given X, Y, using the Bayes rule and that's, you know, that's how the name Bayesian methods comes into picture because,",
    "start": "92290",
    "end": "99945"
  },
  {
    "text": "the way we construct the posterior is using the Bayes rule. There is no gradient descent,",
    "start": "99945",
    "end": "106370"
  },
  {
    "text": "there is no maximum likelihood. We just conditioned on the observed data and we get our posterior distribution,",
    "start": "106370",
    "end": "112775"
  },
  {
    "text": "and then using the posterior distribution, we can then construct the posterior predictive distribution, which,",
    "start": "112775",
    "end": "119149"
  },
  {
    "start": "113000",
    "end": "202000"
  },
  {
    "text": "which is used for making predictions on new unseen data examples.",
    "start": "119150",
    "end": "124714"
  },
  {
    "text": "So y star condition on x star, x, y, what this means is x,",
    "start": "124714",
    "end": "130399"
  },
  {
    "text": "y, is your training set, that you observed in the past and x star is the new input that you encounter and we need to-",
    "start": "130400",
    "end": "138319"
  },
  {
    "text": "to make a prediction about y star and what we do is construct a distribution over y star,",
    "start": "138320",
    "end": "144770"
  },
  {
    "text": "given the new input and all the past training inputs and that is called the posterior predictive distribution,",
    "start": "144770",
    "end": "152645"
  },
  {
    "text": "and this is what the posterior predictive distribution is defined to be. And we saw that there was also",
    "start": "152645",
    "end": "159770"
  },
  {
    "text": "a Piazza post where the steps for this was described in detail. We saw that this distribution can be expressed",
    "start": "159770",
    "end": "166430"
  },
  {
    "text": "as an expectation of P of y star given x star, and, Theta hat, where Theta hat is- it comes from the posterior distribution.",
    "start": "166430",
    "end": "176045"
  },
  {
    "text": "The interpretation of this is that, for every possible setting of the Theta vector,",
    "start": "176045",
    "end": "182825"
  },
  {
    "text": "we get a different model- a predictive model and the posterior predictive distribution, takes the average across all possible models that can possibly exist,",
    "start": "182825",
    "end": "193710"
  },
  {
    "text": "you know, that's infinitely many, but the average is a weighted average where the weights are decided according to the posterior distribution.",
    "start": "193710",
    "end": "200305"
  },
  {
    "text": "Right? And [NOISE] so- so, so this- this kind of averaging across models,",
    "start": "200305",
    "end": "208800"
  },
  {
    "start": "202000",
    "end": "272000"
  },
  {
    "text": "where we don't commit ourselves to only any one given single model,",
    "start": "208800",
    "end": "214905"
  },
  {
    "text": "but we're still kind of hedging our bets against all possible models, but we just weight them differently according to the posterior distribution and this is,",
    "start": "214905",
    "end": "223660"
  },
  {
    "text": "as, you know, there, there- there's, you might read this in several other places as well.",
    "start": "223660",
    "end": "229415"
  },
  {
    "text": "This makes Bayesian methods kind of prone to overfitting. Right. This- this makes it fundamentally prone to overfitting,",
    "start": "229415",
    "end": "237060"
  },
  {
    "text": "there is no- there is no other extra steps you need to take, in order to make your models not overfit and we'll",
    "start": "237060",
    "end": "243900"
  },
  {
    "text": "go into the- into more details about this, probably in Wednesday or Friday's lecture.",
    "start": "243900",
    "end": "249050"
  },
  {
    "text": "But this- this kind of model averaging where we don't commit ourselves to one single model,",
    "start": "249050",
    "end": "254080"
  },
  {
    "text": "which may- which could, which could be due to noise in your data, right. We- we don't commit ourselves to one single model, but instead,",
    "start": "254080",
    "end": "261305"
  },
  {
    "text": "we consider all the possible models- the infinitely many possible models that you get for different values of Theta",
    "start": "261305",
    "end": "266860"
  },
  {
    "text": "and we take a weighted average across the predictions where the weights are decided by the posterior distribution, right.",
    "start": "266860",
    "end": "272585"
  },
  {
    "start": "272000",
    "end": "503000"
  },
  {
    "text": "So this is in the parametric setting where a Theta exists, right. So this- this, this whole setting- this whole exercise is,",
    "start": "272585",
    "end": "278745"
  },
  {
    "text": "and revolves around Theta and- and that's why we call it, parametric Bayesian methods. In the non-parametric approach,",
    "start": "278745",
    "end": "285470"
  },
  {
    "text": "we assume y equals some function f, f of x plus,",
    "start": "285470",
    "end": "290625"
  },
  {
    "text": "some noise, where noise, Epsilon comes from a normal distribution",
    "start": "290625",
    "end": "300164"
  },
  {
    "text": "and we assume that there is a prior distribution on f. Just the way there was a prior distribution on Theta,",
    "start": "300165",
    "end": "306790"
  },
  {
    "text": "here we assume there is a prior distribution on- on f and that prior distribution is a Gaussian process.",
    "start": "306790",
    "end": "312755"
  },
  {
    "text": "So this is called a- a Gaussian process prior and in order to define a Gaussian process prior, its parameters, so to speak,",
    "start": "312755",
    "end": "319415"
  },
  {
    "text": "is a mean and a covariance function. So this is actually a mean function, which I've just written as 0, it means it evaluates to 0 everywhere,",
    "start": "319415",
    "end": "326560"
  },
  {
    "text": "and the way to think of a Gaussian process is with the same analogy as,",
    "start": "326560",
    "end": "331600"
  },
  {
    "text": "even from vector to functions, as an infinite dimension- an infinite extension of a vector,",
    "start": "331600",
    "end": "337755"
  },
  {
    "text": "similarly a Gaussian process is an infinite extension of a multivariate Gaussian distribution.",
    "start": "337755",
    "end": "343720"
  },
  {
    "text": "Right, and for vectors to functions, the relation was, indices of vectors become the domain of the function.",
    "start": "343720",
    "end": "351280"
  },
  {
    "text": "Similarly, indices of your multivariate Gaussian, become the domain of the functions over which we are- we are defining the GP.",
    "start": "351280",
    "end": "360949"
  },
  {
    "text": "Right, and we saw a few properties about, about multivariate Gaussians. So we saw the normalization property that if you integrate out all the data,",
    "start": "360950",
    "end": "369130"
  },
  {
    "text": "the density of the multivariate Gaussian must evaluate to 1. We saw the marginalization property where,",
    "start": "369130",
    "end": "377850"
  },
  {
    "text": "suppose in this example, if you want to marginalize out b from this multivariate distribution,",
    "start": "378470",
    "end": "384410"
  },
  {
    "text": "then all we get is a and Mu a and Sigma a squared. So all the rows and columns,",
    "start": "384410",
    "end": "389960"
  },
  {
    "text": "that correspond to the- the variable that we're marginalizing out, just wiped them out of your multivariate Gaussian. It's that simple.",
    "start": "389960",
    "end": "397940"
  },
  {
    "text": "Right. That's- that's a special property about Gaussians, which, don't- don't exist for other distributions in general and that's- that",
    "start": "397940",
    "end": "405060"
  },
  {
    "text": "makes Gaussian process- Gaussian distributions really nice, and then we saw the conditioning rule,",
    "start": "405060",
    "end": "411715"
  },
  {
    "text": "where the multivariate version appears complex at first,",
    "start": "411715",
    "end": "417190"
  },
  {
    "text": "but if we see the analogy in case of two dimensional Gaussians the conditioning looks like this,",
    "start": "417190",
    "end": "423995"
  },
  {
    "text": "a given b is again a normal distribution if a and b are jointly distributed then a given b is again a normal distribution,",
    "start": "423995",
    "end": "430870"
  },
  {
    "text": "whose mean is given by this. The interpretation here is, with- when, when the value of b is given, first,",
    "start": "430870",
    "end": "437949"
  },
  {
    "text": "we standardize it, that is divide it by its mean, and divide it by its standard deviation.",
    "start": "437950",
    "end": "443135"
  },
  {
    "text": "So it's like a z value. Transform the z value by the correlation coefficient and then do the reverse transform like the-the,",
    "start": "443135",
    "end": "450425"
  },
  {
    "text": "the inverse z, transform, back into Mu a by- by scaling it by a standard deviation and adding a as mean.",
    "start": "450425",
    "end": "458224"
  },
  {
    "text": "Right. That's how we- we, we update the a, as' I mean and then the covariance of a,",
    "start": "458225",
    "end": "468075"
  },
  {
    "text": "or the variance of a given b, is the variance of a scaled by 1 minus Rho square,",
    "start": "468075",
    "end": "473090"
  },
  {
    "text": "where Rho is the correlation coefficient. So what this means is if- if Rho is 1, if a and b are perfectly correlated,",
    "start": "473090",
    "end": "479840"
  },
  {
    "text": "then by observing b, the variance of a reduces to 0. Right. 1 minus 1 is 0.",
    "start": "479840",
    "end": "485539"
  },
  {
    "text": "Which means you know exactly what a is, and similarly, if the covariance between a and b is 0, if there's no covariance whatsoever,",
    "start": "485540",
    "end": "493145"
  },
  {
    "text": "then the variance of a given b, will still be Mu of a square. So there was no reduction in uncertainty",
    "start": "493145",
    "end": "498560"
  },
  {
    "text": "by observing b if there was no correlation between them. Right. So these, these",
    "start": "498560",
    "end": "504480"
  },
  {
    "start": "503000",
    "end": "708000"
  },
  {
    "text": "properties- using these properties we constructed a Gaussian process regression. So the analogues to this, we define,",
    "start": "504480",
    "end": "514360"
  },
  {
    "text": "f and f star where these are the function f evaluated at our training set and function",
    "start": "514360",
    "end": "520940"
  },
  {
    "text": "f evaluated at our test sets and this follows a multivariate Gaussian distribution.",
    "start": "520940",
    "end": "527750"
  },
  {
    "text": "Because that's the property of Gaussian processes where you take any finite subsample of your Gaussian process,",
    "start": "527750",
    "end": "533390"
  },
  {
    "text": "that will be a Gaussian distribution- multivariate Gaussian distribution and this is the kernel function evaluated,",
    "start": "533390",
    "end": "539375"
  },
  {
    "text": "at every possible pairs. Right, and so this is the- this is- we get this by marginalizing out",
    "start": "539375",
    "end": "546800"
  },
  {
    "text": "the GP from the GP all the variable- all the test exam- test and train examples that we have not observed.",
    "start": "546800",
    "end": "552540"
  },
  {
    "text": "So marginalize out everything that you've not seen and this condenses your Gaussian process into a multivariate Gaussian,",
    "start": "552540",
    "end": "558350"
  },
  {
    "text": "evaluated only at- at the test and train points. And the assumption was,",
    "start": "558350",
    "end": "564590"
  },
  {
    "text": "y equals f plus Epsilon. Right, and using the addition property of Gaussians,",
    "start": "564590",
    "end": "571760"
  },
  {
    "text": "which I forgot to recap, using the addition property of Gaussians, y equals f plus Epsilon.",
    "start": "571760",
    "end": "577990"
  },
  {
    "text": "So f plus Epsilon gives us this, Epsilon,",
    "start": "577990",
    "end": "583050"
  },
  {
    "text": "you still have the same Mu, but because, f and epsilon are independent, the covariances add up.",
    "start": "583050",
    "end": "589205"
  },
  {
    "text": "And Epsilon is, is a diagonal matrix, so you only add along the diagonals and you get,",
    "start": "589205",
    "end": "595160"
  },
  {
    "text": "a distribution for y is given x's and using this- using the conditioning property we construct the posterior predictive distribution,",
    "start": "595160",
    "end": "604555"
  },
  {
    "text": "which- which evaluates to some form. It might look complex, but this whole expression for the- for the variance is exactly this,",
    "start": "604555",
    "end": "614915"
  },
  {
    "text": "but a multivariate version of this. Right. It- it, it just looks more complex but it's essentially the same and similarly,",
    "start": "614915",
    "end": "622650"
  },
  {
    "text": "the mean is- is, is, is very similar to that and- and this is all Gaussian process is about.",
    "start": "622650",
    "end": "628410"
  },
  {
    "text": "It- it's very simple. You perform one conditioning, and- and that's it. All right. So any questions about GPs before we move on to deep learning today? Yes.",
    "start": "628410",
    "end": "638310"
  },
  {
    "text": "[inaudible] you said that they will be prone to overfitting. They will not be prone to overfitting. Yeah.",
    "start": "638310",
    "end": "644220"
  },
  {
    "text": "So because that's parametric, right? Or isn't [OVERLAPPING] So in general Bayesian methods are- are, you know, you consider them as not prone to overfitting.",
    "start": "644220",
    "end": "651800"
  },
  {
    "text": "Because the-the, the intuition there is that overfitting happens due to the fact that you're committing yourself to just one model,",
    "start": "651800",
    "end": "660230"
  },
  {
    "text": "given your training data, which- which may be swayed by- swayed due to noise, right.",
    "start": "660230",
    "end": "665805"
  },
  {
    "text": "And- and here we're kind of hedging our bets by not committing to any single model. We are considering all the possible models that could possibly exist",
    "start": "665805",
    "end": "673315"
  },
  {
    "text": "and we are taking the average across them where the average is weighted by the, weight- by the posterior distribution.",
    "start": "673315",
    "end": "679445"
  },
  {
    "text": "It could be [inaudible] very high, tempted not to run exactly to data and then that's what overfitting is.",
    "start": "679445",
    "end": "685260"
  },
  {
    "text": "Well, loo-um, uh, let-uh, we'll go into details to that on-on Wednesday and Friday and- and, um,",
    "start": "685260",
    "end": "691495"
  },
  {
    "text": "I will postpone that, you know, uh, answer to that uh-uh, for a couple more days. But in general, the intuition is that with Bayesian methods,",
    "start": "691495",
    "end": "698650"
  },
  {
    "text": "the- at- at least at the theoretical level, the concept of over-fitting does not exist.",
    "start": "698650",
    "end": "704600"
  },
  {
    "text": "All right. So today, the topic for today is neural networks and- and deep learning, [NOISE] right?",
    "start": "704780",
    "end": "716365"
  },
  {
    "text": "And again, in order to, uh, motivate deep learning and neural networks, right?",
    "start": "716365",
    "end": "721990"
  },
  {
    "text": "Um, the- the, uh, so the models that we've seen so far,",
    "start": "721990",
    "end": "727990"
  },
  {
    "text": "for example, um, we saw- we saw linear regression,",
    "start": "727990",
    "end": "733480"
  },
  {
    "text": "we saw logistic regression, GLMs, et cetera. They have all been linear. And one way we kind of introduced non linearity was using kernels, right?",
    "start": "733480",
    "end": "742630"
  },
  {
    "text": "And also using feature maps. And we saw that kernels and feature maps are kind of very tightly related to each other, right?",
    "start": "742630",
    "end": "749889"
  },
  {
    "text": "For every feature map is- a-a feature map is associated with some kernel, right?",
    "start": "749890",
    "end": "755730"
  },
  {
    "text": "And that was one way to introduce, uh, non linearity. And neural networks is another way, right?",
    "start": "755730",
    "end": "763650"
  },
  {
    "text": "In neural networks, the- the high-level picture to have in your mind is that when we defined a feature map for example,",
    "start": "763650",
    "end": "772270"
  },
  {
    "text": "in problem set one, the last question, we- we, uh, explore different kinds of feature maps on a simple regression task that gave us,",
    "start": "772270",
    "end": "780280"
  },
  {
    "text": "you know, very flexible, um, hypotheses. And the feature map we use there was you know,, for example,",
    "start": "780280",
    "end": "786550"
  },
  {
    "text": "we used a polynomial feature map p of x equals 1x,",
    "start": "786550",
    "end": "791800"
  },
  {
    "text": "x squared, x cubed and maybe things like sine x.",
    "start": "791800",
    "end": "797005"
  },
  {
    "text": "You can add, you can add, uh, more features. And the responsibility of coming up with",
    "start": "797005",
    "end": "803410"
  },
  {
    "text": "such a feature map was in the hands of the designer. The one who was training the model, the one, you know,",
    "start": "803410",
    "end": "809110"
  },
  {
    "text": "the data scientist who's trying to fit this model on- on, uh, on the data had to, you know, use you their intuition,",
    "start": "809110",
    "end": "815710"
  },
  {
    "text": "use their creativity to come up with a suitable feature map, right? And that is- is, uh, hard work.",
    "start": "815710",
    "end": "822265"
  },
  {
    "text": "You need to have some kind of an intuition you need to, you know, think hard to see to- to- to make some decisions about what features to include,",
    "start": "822265",
    "end": "830440"
  },
  {
    "text": "what features not to include. [NOISE] And at a high level, what neural networks, uh,",
    "start": "830440",
    "end": "836590"
  },
  {
    "text": "does for you is-is give you a way in which features themselves are learned automatically, right?",
    "start": "836590",
    "end": "845800"
  },
  {
    "text": "So the models that we've seen so far, given features, we constructed linear models for,",
    "start": "845800",
    "end": "854005"
  },
  {
    "text": "you know, regression classification, et cetera. With neural net-networks, not only are we learning that model,",
    "start": "854005",
    "end": "862060"
  },
  {
    "text": "given the features, but we're also learning what the right set of features are, right?",
    "start": "862060",
    "end": "867670"
  },
  {
    "text": "And the- the, [NOISE] uh,",
    "start": "867670",
    "end": "870920"
  },
  {
    "text": "it's probably best- best explained by looking at an example of- of what I mean by that.",
    "start": "873630",
    "end": "880600"
  },
  {
    "start": "878000",
    "end": "1538000"
  },
  {
    "text": "[NOISE] So supposing we have some input x,",
    "start": "880600",
    "end": "886884"
  },
  {
    "text": "you know, in Rd.",
    "start": "886885",
    "end": "891910"
  },
  {
    "text": "And let's say we represent",
    "start": "891910",
    "end": "898480"
  },
  {
    "text": "x as a set of nodes where we have d number of nodes.",
    "start": "898480",
    "end": "905800"
  },
  {
    "text": "So this corresponds to x1 and this corresponds to x-d, right?",
    "start": "905800",
    "end": "912055"
  },
  {
    "text": "And the way we constructed logistic regression, for example, you know, just to consider an example,",
    "start": "912055",
    "end": "917904"
  },
  {
    "text": "was we had a-a parameter vector and that parameter vector.",
    "start": "917905",
    "end": "926090"
  },
  {
    "text": "So here, uh, this, like SVMs, we're gonna switch our notation and use w and b for weights and biases.",
    "start": "939690",
    "end": "949690"
  },
  {
    "text": "Um, we had w_1, w_2, w_3,",
    "start": "949690",
    "end": "957440"
  },
  {
    "text": "w-d. And we had bias.",
    "start": "957440",
    "end": "965785"
  },
  {
    "text": "And over here, this was summation i",
    "start": "965785",
    "end": "974250"
  },
  {
    "text": "equals 1 to d x_i w_i plus b, right?",
    "start": "974250",
    "end": "982975"
  },
  {
    "text": "So this is, uh, you can think of this as Theta transpose x, where, uh,",
    "start": "982975",
    "end": "988435"
  },
  {
    "text": "Theta was the collection of w and b and, um, x is the x vector along with the intercept term one.",
    "start": "988435",
    "end": "996370"
  },
  {
    "text": "All right. And let's call this z, that is z equals x transpose w plus b.",
    "start": "996370",
    "end": "1007480"
  },
  {
    "text": "And on the other side let's call this a equals g of z,",
    "start": "1007880",
    "end": "1017520"
  },
  {
    "text": "where g is some kind of uh, a non-linearity. Okay. For example, in- in the case of logistic regression,",
    "start": "1017520",
    "end": "1025454"
  },
  {
    "text": "g was 1/1 plus e to the minus e, sorry, g of z was 1/1 plus e to the minus e, right?",
    "start": "1025455",
    "end": "1036630"
  },
  {
    "text": "So this was, uh, this was logistic regression, just visualized in a different way as nodes and connections, right?",
    "start": "1036630",
    "end": "1044669"
  },
  {
    "text": "And this is the view of- this is the interpretation that we're gonna use for extending this further,",
    "start": "1044670",
    "end": "1051570"
  },
  {
    "text": "uh, and construct neural networks out of them, right? So we used to have a parameter vector Theta.",
    "start": "1051570",
    "end": "1057750"
  },
  {
    "text": "Instead of that, we visualize parameters as connections between some node to another node, right?",
    "start": "1057750",
    "end": "1066915"
  },
  {
    "text": "And in neural-neural network terminology, these nodes are called neurons, right?",
    "start": "1066915",
    "end": "1072390"
  },
  {
    "text": "So, um, to clarify, um, the reason why they're called neurons is because, uh,",
    "start": "1072390",
    "end": "1080130"
  },
  {
    "text": "early in the development of neural networks, the design of- of, uh, this- this kind of a design was inspired by how",
    "start": "1080130",
    "end": "1087570"
  },
  {
    "text": "neurons possibly work based on the theory of how neurons worked back then.",
    "start": "1087570",
    "end": "1093269"
  },
  {
    "text": "But, um, it's-it's, you know, you see a lot of literature where people say",
    "start": "1093270",
    "end": "1100110"
  },
  {
    "text": "neural networks mimic neurons and that's just not the case, right? Neural networks when they were designed,",
    "start": "1100110",
    "end": "1107010"
  },
  {
    "text": "were inspired by how people thought neurons worked back then, right? And, uh, in no way whatsoever do neural networks mimic neurons or mimic the brain, right?",
    "start": "1107010",
    "end": "1117180"
  },
  {
    "text": "So, you know, just- just have that in the back of your mind and- and neurons is probably, uh, therefore, uh, a poor choice of word, you can call it nodes and that's just fine.",
    "start": "1117180",
    "end": "1127274"
  },
  {
    "text": "But, you know, we call it neurons. Assuming you guys know that, you know, this is not how actual neurons work. All right.",
    "start": "1127275",
    "end": "1135360"
  },
  {
    "text": "So there is some non-linearity, um, which takes-takes us from z to an output a.",
    "start": "1135360",
    "end": "1142890"
  },
  {
    "text": "We're gonna call a as the output of the nonlinearity, right? And together this- this entire assembly,",
    "start": "1142890",
    "end": "1150315"
  },
  {
    "text": "we'll call it a neuron. [NOISE] And in this particular,",
    "start": "1150315",
    "end": "1161955"
  },
  {
    "text": "uh-uh, configuration, x_1 to x_d is fixed, right?",
    "start": "1161955",
    "end": "1168794"
  },
  {
    "text": "There- there's no feature map here, x_1 through x_d is fixed. And, um, everything that- that we've seen over here is",
    "start": "1168795",
    "end": "1175740"
  },
  {
    "text": "basically exactly logistic regression with no difference whatsoever, right? And we could, for example,",
    "start": "1175740",
    "end": "1181140"
  },
  {
    "text": "if this was logistic regression, then we would call this, uh, our y hat as a, right?",
    "start": "1181140",
    "end": "1189465"
  },
  {
    "text": "And we would have the right label y. And we will construct a loss y,",
    "start": "1189465",
    "end": "1195960"
  },
  {
    "text": "y hat is equal to y log y hat",
    "start": "1195960",
    "end": "1201195"
  },
  {
    "text": "plus 1 minus y log 1 minus y-hat.",
    "start": "1201195",
    "end": "1206715"
  },
  {
    "text": "And in place of y hat, we would express y hat as a function of w's and b's and perform gradient descent, right?",
    "start": "1206715",
    "end": "1214610"
  },
  {
    "text": "That- that was logistic regression. Any questions on this of why this is similar to logistic regression?",
    "start": "1214610",
    "end": "1222629"
  },
  {
    "text": "There's a question, right? Yeah, so, uh, this- this is a logistic regression viewed in",
    "start": "1223220",
    "end": "1231750"
  },
  {
    "text": "terms of connections and- and neurons, so to speak. And now, what we're gonna do next is basically take this network,",
    "start": "1231750",
    "end": "1244590"
  },
  {
    "text": "now come back to this network view and start growing this network. So x_1 through x_d.",
    "start": "1244590",
    "end": "1258450"
  },
  {
    "text": "This is our input layer again, right? And in place of one neuron,",
    "start": "1258450",
    "end": "1269645"
  },
  {
    "text": "let's have a collection of neurons. Here, we had a set of weights connecting.",
    "start": "1269645",
    "end": "1280000"
  },
  {
    "text": "We had a set of weights that connected the input to as, uh, from x's, the input data to the input of this neuron.",
    "start": "1281770",
    "end": "1290965"
  },
  {
    "text": "And just like that, let's add a second neuron here and give it its own set of weights.",
    "start": "1290965",
    "end": "1298630"
  },
  {
    "text": "Right? And this is as if we are- we are trying to train two different logistic regression models in parallel.",
    "start": "1302000",
    "end": "1310680"
  },
  {
    "text": "You know that's the image to- you know, that's the picture to have. We have one set of- one set of,",
    "start": "1310680",
    "end": "1315765"
  },
  {
    "text": "uh, w's that connect x's to this neuron.",
    "start": "1315765",
    "end": "1321210"
  },
  {
    "text": "And another set of w's that connect the x's to the second neuron. They have their own bias term, as well.",
    "start": "1321210",
    "end": "1333720"
  },
  {
    "text": "And similar- similarly, there's gonna be a z here,",
    "start": "1333720",
    "end": "1341610"
  },
  {
    "text": "goes through the, uh, nonlinearity to get an a. Similarly, the- the- the z's are basically",
    "start": "1341610",
    "end": "1348540"
  },
  {
    "text": "the linear combinations of x's with w's plus b. The same thing over here.",
    "start": "1348540",
    "end": "1354435"
  },
  {
    "text": "And let's call this b1 and b2- b2,",
    "start": "1354435",
    "end": "1360420"
  },
  {
    "text": "and this is z1, z2, and a1 and a2.",
    "start": "1360420",
    "end": "1365820"
  },
  {
    "text": "So from z to a, we apply the g function, the nonlinearity, to take the linear combination.",
    "start": "1365820",
    "end": "1372270"
  },
  {
    "text": "We get a scalar, run it through the nonlinearity, we get another scalar, all right?",
    "start": "1372270",
    "end": "1378250"
  },
  {
    "text": "And we- we can continue this. So, similarly, we can have, uh, a z3 and an a3,",
    "start": "1379640",
    "end": "1387974"
  },
  {
    "text": "in that has its own sets of weights, and its own bias, b3.",
    "start": "1387974",
    "end": "1395550"
  },
  {
    "text": "So you get z3 as the sum of over w1,",
    "start": "1395550",
    "end": "1401055"
  },
  {
    "text": "x1, w2, x2, wd, xd plus b3. And then apply the nonlinearity and you get a3, all right?",
    "start": "1401055",
    "end": "1410080"
  },
  {
    "text": "And not only can we add multiple models like these.",
    "start": "1410240",
    "end": "1415425"
  },
  {
    "text": "We can then start nesting them. In the sense, these three, the outputs of these three, that is uh,",
    "start": "1415425",
    "end": "1422370"
  },
  {
    "text": "a1, a2, a3, can now become the input for another logistic regression.",
    "start": "1422370",
    "end": "1428020"
  },
  {
    "text": "It's gonna be a z and an a.",
    "start": "1428960",
    "end": "1439054"
  },
  {
    "text": "Similarly, a second logistic",
    "start": "1439055",
    "end": "1444930"
  },
  {
    "text": "regression using the inputs as the outputs from the first layer,",
    "start": "1444930",
    "end": "1450760"
  },
  {
    "text": "z and a, and so on.",
    "start": "1452120",
    "end": "1460980"
  },
  {
    "text": "We can nest these even further. Yes, question?",
    "start": "1460980",
    "end": "1470130"
  },
  {
    "text": "[inaudible]",
    "start": "1470130",
    "end": "1479970"
  },
  {
    "text": "We'll come to that, we'll come to that. So-so the question was, uh, wouldn't they all learn the same,",
    "start": "1479970",
    "end": "1485775"
  },
  {
    "text": "same, uh, same model? Aren't they gonna be just copies of the same model? Um, in- potentially it can and we'll-",
    "start": "1485775",
    "end": "1492299"
  },
  {
    "text": "we'll address that shortly. Was there another question? [inaudible]",
    "start": "1492300",
    "end": "1499860"
  },
  {
    "text": "Yeah, we'll come to that, we'll come to that. For now we are just constructing a network, uh, using base components that we've seen before.",
    "start": "1499860",
    "end": "1507240"
  },
  {
    "text": "Basically logistic regression, right? So, take logistic regression, create multiple copies of it. Take the outputs of those logistic regressions.",
    "start": "1507240",
    "end": "1514725"
  },
  {
    "text": "Feed them as input to, you know, a nested logistic regression and so on. And this kind of a network can be arbitrarily deep and arbitrarily wide, right?",
    "start": "1514725",
    "end": "1525915"
  },
  {
    "text": "And it can be arbitrarily wide to different levels at different layers. And we're going to call- start giving them names.",
    "start": "1525915",
    "end": "1533370"
  },
  {
    "text": "So, these collection of neurons, we're gonna call it the input layer, [NOISE] right?",
    "start": "1533370",
    "end": "1542625"
  },
  {
    "start": "1538000",
    "end": "1708000"
  },
  {
    "text": "And these over here",
    "start": "1542625",
    "end": "1554700"
  },
  {
    "text": "is the output layer [NOISE].",
    "start": "1554700",
    "end": "1561389"
  },
  {
    "text": "And these are called hidden layers. So this would be, [NOISE] hidden layer 1,",
    "start": "1561390",
    "end": "1567915"
  },
  {
    "text": "hidden layer 2, [NOISE] right?",
    "start": "1567915",
    "end": "1573435"
  },
  {
    "text": "And this overall network, you can call it maybe a three layer network, right?",
    "start": "1573435",
    "end": "1579045"
  },
  {
    "text": "So you have an output layer, hidden layer, or hidden layer 1, hidden layer 2. So, you have, um, um,",
    "start": "1579045",
    "end": "1585520"
  },
  {
    "text": "three layers and generally we don't count the input layer, you know, input is just input, right?",
    "start": "1585520",
    "end": "1590970"
  },
  {
    "text": "And, soon we're gonna give more, more precise terminologies to each, each of these, uh,",
    "start": "1590970",
    "end": "1596835"
  },
  {
    "text": "weights and- and, uh, weights and biases and activations. But before that, you know, it's important to get this general intuition",
    "start": "1596835",
    "end": "1603930"
  },
  {
    "text": "of- of how neural networks are kind of constructed. Um, this is an example of a very simple neural network.",
    "start": "1603930",
    "end": "1609810"
  },
  {
    "text": "Um, there are lots of different choices for the g function. You don't always have to use the logistic, uh, logistic function.",
    "start": "1609810",
    "end": "1616605"
  },
  {
    "text": "But the general idea is more or less the same. That is, we take, uh, a linear model with some activation.",
    "start": "1616605",
    "end": "1625125"
  },
  {
    "text": "Um, in GLMs we saw that, uh, for different kinds of outputs,",
    "start": "1625125",
    "end": "1630720"
  },
  {
    "text": "we use different kinds of g's. Similarly, uh, here we can use, uh, there is a wide variety of choices of the,",
    "start": "1630720",
    "end": "1637679"
  },
  {
    "text": "uh, activation functions that we can use. So g is also called the activation function, [NOISE] right?",
    "start": "1637680",
    "end": "1652424"
  },
  {
    "text": "And using this, this linear model building block.",
    "start": "1652425",
    "end": "1657750"
  },
  {
    "text": "We start assembling a big network as though these are, you know, think of them as Lego blocks, right?",
    "start": "1657750",
    "end": "1664515"
  },
  {
    "text": "You place one block here, one block here, one block here, and then start building yet another layer on top and so on, right?",
    "start": "1664515",
    "end": "1671160"
  },
  {
    "text": "And, [NOISE] and, and now, once we construct a layer,",
    "start": "1671160",
    "end": "1677850"
  },
  {
    "text": "uh, a network with, you know, some number of layers. Each layer being- you know, some having some particular width.",
    "start": "1677850",
    "end": "1685065"
  },
  {
    "text": "And finally, at- at the output layer. The output, the a of the last layer will be considered our y hat, right?",
    "start": "1685065",
    "end": "1695250"
  },
  {
    "text": "And then we're going to get the true label y. And we can apply- construct some kind of a loss, all right?",
    "start": "1695250",
    "end": "1702135"
  },
  {
    "text": "Previously we would take the output of the very first layer, y hat, and apply over the last.",
    "start": "1702135",
    "end": "1707880"
  },
  {
    "text": "But now, we're going to nest these building blocks into a more complex network.",
    "start": "1707880",
    "end": "1715125"
  },
  {
    "start": "1708000",
    "end": "1808000"
  },
  {
    "text": "And we're gonna construct the network that eventually we gonna have a single output. And that output will be the,",
    "start": "1715125",
    "end": "1723299"
  },
  {
    "text": "the prediction made by this network. On which we are gonna apply the loss, right?",
    "start": "1723300",
    "end": "1729000"
  },
  {
    "text": "This is the big picture to have in mind. Deep learning is basically adding depth by cascading simple building blocks,",
    "start": "1729000",
    "end": "1736965"
  },
  {
    "text": "using nonlinearities in the middle.",
    "start": "1736965",
    "end": "1740679"
  },
  {
    "text": "Any question about the big picture we- before we start defining our terminology and,",
    "start": "1742970",
    "end": "1747990"
  },
  {
    "text": "and giving, uh, specific, uh, uh, numbers here. Yes, question. [inaudible] Yeah.",
    "start": "1747990",
    "end": "1756500"
  },
  {
    "text": "[inaudible] Yes. [inaudible]. Yeah. [inaudible]",
    "start": "1756500",
    "end": "1764804"
  },
  {
    "text": "Yes. The question is, do the activation function, the choice that we use in the first layer,",
    "start": "1764805",
    "end": "1771480"
  },
  {
    "text": "should it be the same, say for the second layer, right? In- in theory they- they can be different.",
    "start": "1771480",
    "end": "1777570"
  },
  {
    "text": "You can use different activation functions for different neurons even, then it need not be the same within the same layer,",
    "start": "1777570",
    "end": "1784395"
  },
  {
    "text": "but in practice, generally tend to use the same activation function throughout the network,",
    "start": "1784395",
    "end": "1790305"
  },
  {
    "text": "but, you know, that's, that's- that's a convention. That's not a requirement. Any questions? All right.",
    "start": "1790305",
    "end": "1797280"
  },
  {
    "text": "Now, um, let's start, um, Yes question.",
    "start": "1797280",
    "end": "1803010"
  },
  {
    "text": "[inaudible]",
    "start": "1803010",
    "end": "1808140"
  },
  {
    "text": "Yeah, we are going to come to that. So- so the question was, if we have a loss function here, what are we going to update? The answer is, once we apply a loss function,",
    "start": "1808140",
    "end": "1816255"
  },
  {
    "text": "we need to calculate gradients with respect to every single connection and bias, right?",
    "start": "1816255",
    "end": "1822000"
  },
  {
    "text": "The- the goal of- of- of our training is to first come up with a network architecture,",
    "start": "1822000",
    "end": "1830700"
  },
  {
    "text": "and once you come up with a network architecture, you take your data, start feeding your input. You get some predicted output, calculate the loss,",
    "start": "1830700",
    "end": "1839625"
  },
  {
    "text": "and then calculate the gradient of that loss with respect to every connection in parallel,",
    "start": "1839625",
    "end": "1847850"
  },
  {
    "text": "and take a gradient descent step to minimize that loss in- along the direction of that gradient.",
    "start": "1847850",
    "end": "1854030"
  },
  {
    "text": "[inaudible] Yeah- yeah- yeah, we'll- we'll come to that of- you know,",
    "start": "1854030",
    "end": "1860430"
  },
  {
    "start": "1858000",
    "end": "2338000"
  },
  {
    "text": "generally stochastic gradient descent is the one that's most commonly used.",
    "start": "1860430",
    "end": "1865450"
  },
  {
    "text": "In fact a variant of that called mini Batch stochastic gradient, we'll come to that. All right?",
    "start": "1865490",
    "end": "1871559"
  },
  {
    "text": "Let's- let's start giving some precise terminology, so that we are clear of what we are- we are referring to.",
    "start": "1871560",
    "end": "1880780"
  },
  {
    "text": "We will use um, all the- all the connections.",
    "start": "1881570",
    "end": "1888600"
  },
  {
    "text": "We will use the letter w, so w will mean connection.",
    "start": "1888600",
    "end": "1893500"
  },
  {
    "text": "Right? And b will be bias. [NOISE] And z will be",
    "start": "1894560",
    "end": "1907049"
  },
  {
    "text": "w dot something plus b,",
    "start": "1907050",
    "end": "1912225"
  },
  {
    "text": "so z will always be linear in w, and linear in b, and linear with whatever we'll multiply b- uh, w by,",
    "start": "1912225",
    "end": "1920395"
  },
  {
    "text": "a will always be g of sum z,",
    "start": "1920395",
    "end": "1926290"
  },
  {
    "text": "and as- as, uh,",
    "start": "1927530",
    "end": "1932940"
  },
  {
    "text": "to distinguish the different z's, and the different a's, and the different b's and w's,",
    "start": "1932940",
    "end": "1938820"
  },
  {
    "text": "we're going to use the notation where a superscript identifies the layer.",
    "start": "1938820",
    "end": "1945610"
  },
  {
    "text": "A superscript is going to identify the layer, and a subscript is going to identify which node in that layer we are referring to.",
    "start": "1953120",
    "end": "1962595"
  },
  {
    "text": "Right? We'll have i j,",
    "start": "1962595",
    "end": "1969809"
  },
  {
    "text": "i, i, i, so the a's, there are only- a's and z's,",
    "start": "1969810",
    "end": "1975180"
  },
  {
    "text": "and b's is a vector per layer. Whereas the weight, the connections are- are a matrix.",
    "start": "1975180",
    "end": "1985080"
  },
  {
    "text": "These are vectors, so this is the vector a, this is the vector z, and this is the vector b,",
    "start": "1985080",
    "end": "1991679"
  },
  {
    "text": "and you have one such vector per layer, one vector of a, one vector of z, and one vector of b per layer,",
    "start": "1991680",
    "end": "1999270"
  },
  {
    "text": "but we have one matrix w per layer. so w is going to be- it's also called the weight matrix,",
    "start": "1999270",
    "end": "2007279"
  },
  {
    "text": "and that's why it has two indices, i and j, and the- the very first input,",
    "start": "2007279",
    "end": "2018060"
  },
  {
    "text": "x will also be called a0.",
    "start": "2018880",
    "end": "2025760"
  },
  {
    "text": "It's like the output of the 0th layer, so you're kind of bootstrapping this hierarchy,",
    "start": "2025760",
    "end": "2031939"
  },
  {
    "text": "by considering the inputs to be the output of the 0th player, you know, we're just going to consider the x's as just given.",
    "start": "2031939",
    "end": "2038780"
  },
  {
    "text": "Think of x's as the output of the activation from the 0th layer, and z1- z1,",
    "start": "2038780",
    "end": "2058945"
  },
  {
    "text": "we are referring to this vector z superscript square bracket 1, refers to this z vector,",
    "start": "2058945",
    "end": "2064585"
  },
  {
    "text": "and in that let's conserve z11, so z11 is, uh,",
    "start": "2064585",
    "end": "2072129"
  },
  {
    "text": "in the- in the z vector of the first layer, of the first element of that vector,",
    "start": "2072130",
    "end": "2077370"
  },
  {
    "text": "will be sum of w,",
    "start": "2077370",
    "end": "2085294"
  },
  {
    "text": "j- sum of j w1",
    "start": "2085295",
    "end": "2093345"
  },
  {
    "text": "of j x, or, [NOISE] j plus b1, right?",
    "start": "2093345",
    "end": "2113240"
  },
  {
    "text": "So w is now a matrix that has- so a matrix has- has two axes, two dimensions.",
    "start": "2113240",
    "end": "2121460"
  },
  {
    "text": "The number of rows in the matrix, is the number of neurons in that layer,",
    "start": "2121460",
    "end": "2128660"
  },
  {
    "text": "and the number of columns in that matrix, is the number of neurons in the input layer in the previous layer.",
    "start": "2128660",
    "end": "2135270"
  },
  {
    "text": "So w is a matrix. w l is the weight matrix,",
    "start": "2135910",
    "end": "2144109"
  },
  {
    "text": "where the number of",
    "start": "2144109",
    "end": "2155750"
  },
  {
    "text": "rows is the num- is the- is the number of neurons in the layer.",
    "start": "2155750",
    "end": "2163565"
  },
  {
    "text": "This is number of neurons in the lth layer,",
    "start": "2163565",
    "end": "2173970"
  },
  {
    "text": "and the number of columns is the number of neurons in the l minus 1th layer.",
    "start": "2174010",
    "end": "2182520"
  },
  {
    "text": "Right. So that's, uh, that's a w matrix.",
    "start": "2188560",
    "end": "2193680"
  },
  {
    "text": "And Z_1^ 1 is therefore the first row of the weight matrix,",
    "start": "2194200",
    "end": "2205109"
  },
  {
    "text": "dot producted with the a ^0 vector.",
    "start": "2205630",
    "end": "2210859"
  },
  {
    "text": "That is dot producted with the input, right? And that - that's basically, uh, doing this for you.",
    "start": "2210860",
    "end": "2217115"
  },
  {
    "text": "W1 X1, W2 X2, W3 X3, and so on plus b_1.",
    "start": "2217115",
    "end": "2225619"
  },
  {
    "text": "And similarly, z_2 of 1 is",
    "start": "2225620",
    "end": "2232850"
  },
  {
    "text": "equal to W_2 of 1.",
    "start": "2232850",
    "end": "2241205"
  },
  {
    "text": "This time, I'm gonna write it as a transpose, so that this I can simplify.",
    "start": "2241205",
    "end": "2247079"
  },
  {
    "text": "0 plus b_2. There's just the,",
    "start": "2248890",
    "end": "2254809"
  },
  {
    "text": "uh, uh, the dot product and - and in - in this world it's never written as a dot product.",
    "start": "2254810",
    "end": "2260525"
  },
  {
    "text": "Okay, and so on. And, so that gives - that gives us the collection of these.",
    "start": "2260525",
    "end": "2268635"
  },
  {
    "text": "And similarly then a^1,_1,",
    "start": "2268635",
    "end": "2274360"
  },
  {
    "text": "equals g of z^1_1.",
    "start": "2274360",
    "end": "2278550"
  },
  {
    "text": "The activation functions are applied element-wise, right?",
    "start": "2281260",
    "end": "2287880"
  },
  {
    "text": "For a given value of z that was calculated with the, uh, dot-product and adding up the sum.",
    "start": "2288160",
    "end": "2295760"
  },
  {
    "text": "The a function is applied element-wise separate and the g function is applied element-wise separately to each of the z values to get the corresponding a values.",
    "start": "2295760",
    "end": "2304880"
  },
  {
    "text": "[NOISE]",
    "start": "2304880",
    "end": "2337180"
  },
  {
    "text": "Right and so on and- and- and similarly, once we have constructed the a^1 vector,",
    "start": "2337180",
    "end": "2343030"
  },
  {
    "text": "so x^1 is the output of the first layer. So, ah, in this case,",
    "start": "2343030",
    "end": "2349865"
  },
  {
    "text": "the vector that comes out of the first layer will be a^1 okay,",
    "start": "2349865",
    "end": "2355670"
  },
  {
    "text": "and the dimension of this vector is equal to the number of neurons in the first layer.",
    "start": "2355670",
    "end": "2362615"
  },
  {
    "text": "Is that clear? Okay, and we can- we can continue this-this uh,",
    "start": "2362615",
    "end": "2368030"
  },
  {
    "text": "style of nesting even further. So we will- we will then have z^2.",
    "start": "2368030",
    "end": "2373309"
  },
  {
    "text": "Question. Yes question? So you were saying that uh,",
    "start": "2373310",
    "end": "2379670"
  },
  {
    "text": "z^2 is written in uh, matrix form? So w- so- so w is a matrix.",
    "start": "2379670",
    "end": "2386660"
  },
  {
    "text": "Like- oh yeah and then vector notation. I meant that uh, my question is that a is essentially agnostic of",
    "start": "2386660",
    "end": "2393620"
  },
  {
    "text": "which layer you're - I mean not layer but which- [NOISE] Good question.",
    "start": "2393620",
    "end": "2403070"
  },
  {
    "text": "Let's- let's, uh, [NOISE] is th- is this a the one you're referring to?",
    "start": "2403070",
    "end": "2409775"
  },
  {
    "text": "Yeah. Yeah. So a- a^1_1 means the output of the first layer and the first element of",
    "start": "2409775",
    "end": "2418220"
  },
  {
    "text": "that vector is equal to the activation function applied on the first element of the fir- z,",
    "start": "2418220",
    "end": "2427280"
  },
  {
    "text": "of, uh, the z vector of the first layer. So z vector is found the whole layer?",
    "start": "2427280",
    "end": "2434000"
  },
  {
    "text": "Not just one of the dots, on one [inaudible] Yeah. So- so z-. So let me- let me,",
    "start": "2434000",
    "end": "2440630"
  },
  {
    "text": "um, write this in vector notation. So here we were calculating it element-wise.",
    "start": "2440630",
    "end": "2446090"
  },
  {
    "text": "So this is Z^1_1, this is z^1_2, this is z^1_3",
    "start": "2446090",
    "end": "2451325"
  },
  {
    "text": "Okay. Right? And, if you wanna write it in vector form, then you would write this as z^1 is equal to w^1",
    "start": "2451325",
    "end": "2467059"
  },
  {
    "text": "times a^0 plus b^1, right?",
    "start": "2467060",
    "end": "2479225"
  },
  {
    "text": "Now we are writing this in vector notation where z^1 is this entire z vector.",
    "start": "2479225",
    "end": "2486500"
  },
  {
    "text": "Okay. Right? And that is equal to the w matrix times the input vector,",
    "start": "2486500",
    "end": "2494750"
  },
  {
    "text": "plus element-wise addition due to the, uh, summing up of the b vector, the bias, [OVERLAPPING] vector.",
    "start": "2495130",
    "end": "2502160"
  },
  {
    "text": "So z is just a linear combination of w's and a's [BACKGROUND] plus the b's.",
    "start": "2502160",
    "end": "2509825"
  },
  {
    "text": "So a is a probability? Uh, I would say don't think of a as a probability if you take it through a sigmoid,",
    "start": "2509825",
    "end": "2514990"
  },
  {
    "text": "you get a value between 0 and 1. Okay. Um, for now, don't treat it as a probability. Just think of it as, you know,",
    "start": "2514990",
    "end": "2520750"
  },
  {
    "text": "some- output of some non-linearity, right? So the a is, if,- if G is the sigmoid,",
    "start": "2520750",
    "end": "2525920"
  },
  {
    "text": "then a will be between 0 and 1. Yes. Good question. Any- any - any other questions?",
    "start": "2525920",
    "end": "2533270"
  },
  {
    "text": "All right. So this is a compact way of writing these element-wise operations, right?",
    "start": "2533270",
    "end": "2541220"
  },
  {
    "text": "So the full - the full z vector, that is uh, the vector z's before applying the non-linearity.",
    "start": "2541220",
    "end": "2547805"
  },
  {
    "text": "This vector of size 3 is equal to a matrix w. In this case,",
    "start": "2547805",
    "end": "2553430"
  },
  {
    "text": "the matrix w will be 3 cross d, where d is the um, uh, dimension of the input.",
    "start": "2553430",
    "end": "2560525"
  },
  {
    "text": "So the - the input dimension goes to the number of columns, and 3 is the number of outputs,",
    "start": "2560525",
    "end": "2567440"
  },
  {
    "text": "that's the number of um, number of neurons in this layer. So that would be 3 here, d here.",
    "start": "2567440",
    "end": "2573185"
  },
  {
    "text": "So w^1 will be 3 cross d, a^0, that is the uh,",
    "start": "2573185",
    "end": "2580460"
  },
  {
    "text": "a^0 is the x vector, right? A^0 is d-dimensional.",
    "start": "2580460",
    "end": "2587779"
  },
  {
    "text": "So this would be, according to this network, this is three cross d and this",
    "start": "2587780",
    "end": "2596255"
  },
  {
    "text": "is d dimension and this is three-dimension,",
    "start": "2596255",
    "end": "2602855"
  },
  {
    "text": "and z will also be therefore three dimension.",
    "start": "2602855",
    "end": "2608570"
  },
  {
    "text": "Is this clear? And - and then we apply the - the non-linearity on this z,",
    "start": "2608570",
    "end": "2618260"
  },
  {
    "text": "that is on the left half of the circles. You apply the non-linearity to jump over to the right half of the uh,",
    "start": "2618260",
    "end": "2624185"
  },
  {
    "text": "circle to get a's. And the g function is applied separately element-wise. Yes question?",
    "start": "2624185",
    "end": "2635464"
  },
  {
    "text": "Sorry to bother you but it looks like the three is the number of impute nodes, okay.",
    "start": "2635465",
    "end": "2640775"
  },
  {
    "text": "So in this case, three is the number of neurons. The number of input nodes is d-dimensional.",
    "start": "2640775",
    "end": "2647840"
  },
  {
    "text": "Okay, but then the number of neurons will also be d, right? So the number of neurons here,",
    "start": "2647840",
    "end": "2653930"
  },
  {
    "text": "we've chosen this to be 3. Okay. Right, and the w matrix takes you from d to 3, right?",
    "start": "2653930",
    "end": "2662480"
  },
  {
    "text": "So the w matrix takes you from d to 3 when you multiply it with x,",
    "start": "2662480",
    "end": "2669065"
  },
  {
    "text": "which is 3 uh - uh, which- which is d-dimensional. You multiply the w matrix with x,",
    "start": "2669065",
    "end": "2675755"
  },
  {
    "text": "and because the number of rows and w is three, you get a vector of dimension three.",
    "start": "2675755",
    "end": "2681545"
  },
  {
    "text": "And to that you add a b vector, which is also of dimension three.",
    "start": "2681545",
    "end": "2686780"
  },
  {
    "text": "So you get a z vector of dimension three.",
    "start": "2686780",
    "end": "2689940"
  },
  {
    "text": "Any other questions? So um, the thing to kind of keep in mind is whenever we- we- we see some kind of a subscript uh,",
    "start": "2693550",
    "end": "2703490"
  },
  {
    "text": "at - at least in case of uh, logistic regression, when - when we thought - when we saw an x_ j,",
    "start": "2703490",
    "end": "2708845"
  },
  {
    "text": "we thought of this as a scalar where x was in RD, right?",
    "start": "2708845",
    "end": "2715535"
  },
  {
    "text": "However, when we see a superscript, right? We're still referring to you know, the vector or scalar or whatever it is,",
    "start": "2715535",
    "end": "2722540"
  },
  {
    "text": "but it's only identifying which layer we are at, right? So z^1 is not the first element of z,",
    "start": "2722540",
    "end": "2728720"
  },
  {
    "text": "it just means it's the z vector of the first layer, right? So the superscript squared bracket refers to the layer number.",
    "start": "2728720",
    "end": "2734240"
  },
  {
    "text": "[NOISE]",
    "start": "2734240",
    "end": "2749825"
  },
  {
    "text": "If something is not clear, please stop me. It's- it's super important that, um, you understand this very well because, you know,",
    "start": "2749825",
    "end": "2756559"
  },
  {
    "start": "2750000",
    "end": "3133000"
  },
  {
    "text": "once- once we go to back propagation, we're just gonna use all these, um- um, all these notations very liberally there.",
    "start": "2756560",
    "end": "2762560"
  },
  {
    "text": "So, you know, if there's any question, feel free to stop me as many number of times you want, okay?",
    "start": "2762560",
    "end": "2767585"
  },
  {
    "text": "So, uh, so now a_1, loosely speaking,",
    "start": "2767585",
    "end": "2773390"
  },
  {
    "text": "we can call it g of z_1,",
    "start": "2773390",
    "end": "2779105"
  },
  {
    "text": "where you can think of g as, uh, the non-linearity, for example,",
    "start": "2779105",
    "end": "2784160"
  },
  {
    "text": "the sigmoid being applied element-wise separately to each, uh- uh, element in the input vector, right?",
    "start": "2784160",
    "end": "2791300"
  },
  {
    "text": "And then similarly, z_2 is now w_2",
    "start": "2791300",
    "end": "2798875"
  },
  {
    "text": "times a_1 plus b_2, right?",
    "start": "2798875",
    "end": "2808970"
  },
  {
    "text": "And this gives us a_3,uh, I'm sorry, a_2 equals g of",
    "start": "2808970",
    "end": "2819154"
  },
  {
    "text": "z_2, and so on, right?",
    "start": "2819155",
    "end": "2826550"
  },
  {
    "text": "And then z_3 equals w_3 of a_2 plus b_3,",
    "start": "2826550",
    "end": "2840785"
  },
  {
    "text": "and a_3 equals g of z_3.",
    "start": "2840785",
    "end": "2848280"
  },
  {
    "text": "And you can- and you can take this as arbitrarily deep as you want, right?",
    "start": "2849460",
    "end": "2856280"
  },
  {
    "text": "And this- this depth is what- is what- is the- basically the reason why we call this deep learning.",
    "start": "2856280",
    "end": "2865835"
  },
  {
    "text": "You know, the deeper your network is, you know, the more deeper your deep learning is, you know, so to speak, right?",
    "start": "2865835",
    "end": "2871280"
  },
  {
    "text": "[LAUGHTER] So the- the number of levels- the number of degrees of nesting you have is called the depth of your network, right?",
    "start": "2871280",
    "end": "2880640"
  },
  {
    "text": "And- and this is what distinguishes from, uh, simple linear models where the- there was pretty much no depth at all, right?",
    "start": "2880640",
    "end": "2887825"
  },
  {
    "text": "There was just one single layer and the output of the layer was the output of- of- of your hypothesis, okay?",
    "start": "2887825",
    "end": "2893630"
  },
  {
    "text": "And we can go on. And, uh, okay?",
    "start": "2893630",
    "end": "2907040"
  },
  {
    "text": "And now the question is, why do we have this g to be a non-linear function, right?",
    "start": "2907040",
    "end": "2916520"
  },
  {
    "text": "What if g was identity? That is, what if",
    "start": "2916520",
    "end": "2924425"
  },
  {
    "text": "g of z was just z? That is if a equals z,",
    "start": "2924425",
    "end": "2930770"
  },
  {
    "text": "what would happen in that case? Yes, question. [inaudible]",
    "start": "2930770",
    "end": "2939680"
  },
  {
    "text": "Exactly. So the reason why, um, you know, uh- uh,",
    "start": "2939680",
    "end": "2944750"
  },
  {
    "text": "the reason why it's important to have nonlinearities is because if z was just, uh- uh,",
    "start": "2944750",
    "end": "2954125"
  },
  {
    "text": "if g was just the identity function, then a_3 would have been- just to simplify the argument,",
    "start": "2954125",
    "end": "2965780"
  },
  {
    "text": "we're gonna ignore the b terms, uh, but you can include the b terms here and- and the same argument will still hold.",
    "start": "2965780",
    "end": "2971540"
  },
  {
    "text": "Let's- let's, uh, assume that the bs are just zeros for now. So a_3 will be g of z_3, right?",
    "start": "2971540",
    "end": "2978740"
  },
  {
    "text": "And g of z_3 is just z_3, right?",
    "start": "2978740",
    "end": "2983915"
  },
  {
    "text": "But z_3 is w_3 a_2, so this is w_3 a_2.",
    "start": "2983915",
    "end": "2996619"
  },
  {
    "text": "But a_2 is g of z_3, and g is identity. So this is w_3 of z_2, right?",
    "start": "2996620",
    "end": "3008035"
  },
  {
    "text": "But z_2 is w_2 of a_1, so w_3 w_2 of a_1.",
    "start": "3008035",
    "end": "3018250"
  },
  {
    "text": "And similarly, a_1 is g of z_1 if z is identity, then, you know, this becomes z_1, z_1.",
    "start": "3018250",
    "end": "3025130"
  },
  {
    "text": "And this, again, w_3 times w_2",
    "start": "3033060",
    "end": "3039550"
  },
  {
    "text": "times w_1 times x, right?",
    "start": "3039550",
    "end": "3046265"
  },
  {
    "text": "And this is basically a matrix times a matrix times a matrix and we can call this sum w tilde times x.",
    "start": "3046265",
    "end": "3055180"
  },
  {
    "text": "So if g were not a non-linear function, but assume it to be just some, you know, uh,",
    "start": "3055180",
    "end": "3060835"
  },
  {
    "text": "linear function, then this entire network can now be represented as a single matrix, right?",
    "start": "3060835",
    "end": "3068800"
  },
  {
    "text": "Which means we haven't really gone beyond the scope of linear models. So any such networks can be represented by a linear model if",
    "start": "3068800",
    "end": "3076345"
  },
  {
    "text": "g is not a non-linear function, right? And so g being non-linear is essential to have depth be meaningful.",
    "start": "3076345",
    "end": "3086920"
  },
  {
    "text": "Otherwise, all it- all- no matter how many levels deep your network is,",
    "start": "3086920",
    "end": "3093174"
  },
  {
    "text": "it can always be collapsed into a- a single matrix and it will effectively be as expressive as a single layer network. Yes, question.",
    "start": "3093175",
    "end": "3103090"
  },
  {
    "text": "But you said that the number of nodes in the three layers can be different, right?",
    "start": "3103090",
    "end": "3108235"
  },
  {
    "text": "Yeah. So then you can take this as a single matrix, I guess, then a linear sort of reduction can be present on the matrix but",
    "start": "3108235",
    "end": "3117745"
  },
  {
    "text": "is it lack of that because we're looking at the number of [inaudible] can't change across layers or is it-",
    "start": "3117745",
    "end": "3123565"
  },
  {
    "text": "Yes. So the question is, what is- what if- what if, uh- um, even though we have, you know, so many matrices,",
    "start": "3123565",
    "end": "3129009"
  },
  {
    "text": "why is it, you know, still effectively the same as one? One- one way to think about that is, you can think of these,",
    "start": "3129010",
    "end": "3134964"
  },
  {
    "start": "3133000",
    "end": "3343000"
  },
  {
    "text": "the- the w matrices in your networks which have arbitrary, you know, widths as being, uh,",
    "start": "3134965",
    "end": "3140725"
  },
  {
    "text": "a decomposition of your w tilde matrix, right? So you- you take a w tilde matrix and decompose it into product of, you know,",
    "start": "3140725",
    "end": "3148675"
  },
  {
    "text": "other matrices and those will be your weight matrices. Yes, question.",
    "start": "3148675",
    "end": "3154240"
  },
  {
    "text": "[inaudible] So we're gonna come to that. So, uh- um,",
    "start": "3154240",
    "end": "3160000"
  },
  {
    "text": "you mean why g is the sigmoid? Yeah. Yeah. So, um-",
    "start": "3160000",
    "end": "3165400"
  },
  {
    "text": "[inaudible] Yeah, so- so z- so g of z clearly should not be just z because then,",
    "start": "3165400",
    "end": "3176665"
  },
  {
    "text": "you know, there is no depth in the network. Then the question is, what can z be? What can g be rather?",
    "start": "3176665",
    "end": "3182170"
  },
  {
    "text": "So g of z can be any function which is, uh,",
    "start": "3182170",
    "end": "3187734"
  },
  {
    "text": "nonlinear, but in deep learning, we choose, you know, some kind of a non-linear function that's also monotonic, right?",
    "start": "3187735",
    "end": "3195145"
  },
  {
    "text": "So g of z equals 1 over 1 plus e to the minus z.",
    "start": "3195145",
    "end": "3200470"
  },
  {
    "text": "So that is the family of sigmoid function. You know, that works. g can sometimes be tanh.",
    "start": "3200470",
    "end": "3208855"
  },
  {
    "text": "So tanh is basically e_z plus, what is it?",
    "start": "3208855",
    "end": "3215080"
  },
  {
    "text": "Minus [NOISE] yeah, e",
    "start": "3215080",
    "end": "3223990"
  },
  {
    "text": "to the- g to the- e_z minus e to the minus z over e_z plus e_z.",
    "start": "3223990",
    "end": "3233365"
  },
  {
    "text": "This is the hyperbolic tangent, right? And this also looks very similar to, uh, the sigmoid,",
    "start": "3233365",
    "end": "3240940"
  },
  {
    "text": "except instead from, you know, 0 to1 it is minus 1 to plus 1, you know, just a different choice.",
    "start": "3240940",
    "end": "3247840"
  },
  {
    "text": "And a very common one of g is relu, it's also called the rectified linear unit.",
    "start": "3247840",
    "end": "3255100"
  },
  {
    "text": "And this is basically max of z,0.",
    "start": "3255100",
    "end": "3262090"
  },
  {
    "text": "What does this look like? It basically looks like this. [NOISE]",
    "start": "3262090",
    "end": "3282190"
  },
  {
    "text": "So [NOISE] this is z,",
    "start": "3282190",
    "end": "3287890"
  },
  {
    "text": "this is ReLU of z, right?",
    "start": "3287890",
    "end": "3293920"
  },
  {
    "text": "And similarly here, you know, this was z and this is tanh of z,",
    "start": "3293920",
    "end": "3299290"
  },
  {
    "text": "and here this was z and this a sigmoid of z, this is z and this is the ReLU of z, right?",
    "start": "3299290",
    "end": "3306085"
  },
  {
    "text": "What ReLU does is, um, if z is negative, it just sets it to 0.",
    "start": "3306085",
    "end": "3312610"
  },
  {
    "text": "If it's positive, it leaves it as it is. And this is a non-linearity, right?",
    "start": "3312610",
    "end": "3318625"
  },
  {
    "text": "And if- if g is- are the ReLU function, then you cannot represent w as, you know,",
    "start": "3318625",
    "end": "3324160"
  },
  {
    "text": "product of, uh, other w's. And- and these are,",
    "start": "3324160",
    "end": "3329994"
  },
  {
    "text": "you know, probably the, you know, three most common choices of- of the activation function that are",
    "start": "3329995",
    "end": "3335289"
  },
  {
    "text": "used in practice, right? So this is pretty much how we construct,",
    "start": "3335290",
    "end": "3344440"
  },
  {
    "start": "3343000",
    "end": "3599000"
  },
  {
    "text": "um, a neural network of this kind of an architecture. So this kind of an architecture is also called a fully connected network.",
    "start": "3344440",
    "end": "3352135"
  },
  {
    "text": "The reason why it's called fully connected is because in each layer,",
    "start": "3352135",
    "end": "3359965"
  },
  {
    "text": "to the next layer, we have a full bipartite connection. Every node in one layer is connected to every other node in the next layer,",
    "start": "3359965",
    "end": "3368850"
  },
  {
    "text": "or in the previous layer, right? And- and- and that's the reason these, this kind of a network architecture is called a fully connected neural network,",
    "start": "3368850",
    "end": "3377380"
  },
  {
    "text": "and the number of layers you wanna have and the number of neurons in each layer that you want to have,",
    "start": "3377380",
    "end": "3385030"
  },
  {
    "text": "they are all configuration choices and they are also called hyper-parameters. Yes question.",
    "start": "3385030",
    "end": "3390430"
  },
  {
    "text": "[BACKGROUND]",
    "start": "3390430",
    "end": "3405520"
  },
  {
    "text": "Yep. So the question is, um, I guess, you know, um-um,",
    "start": "3405520",
    "end": "3410710"
  },
  {
    "text": "do the number of layers, do the- uh, and the number of neurons that we have, you know, does that relate to overfitting and underfitting, right?",
    "start": "3410710",
    "end": "3417430"
  },
  {
    "text": "[BACKGROUND] Yeah. So how will we choose the right number of layers and how do we choose the right number of neurons per layer?",
    "start": "3417430",
    "end": "3425845"
  },
  {
    "text": "Or in general, how do we choose what activation function to choose- what activation function to use?",
    "start": "3425845",
    "end": "3432234"
  },
  {
    "text": "And the answer is almost always cross-validation. And, uh, the answer will be cross-validation for a lot of future questions as well.",
    "start": "3432235",
    "end": "3441265"
  },
  {
    "text": "And that's just the nature of machine learning, right? Um, you have a lot of freedom,",
    "start": "3441265",
    "end": "3448885"
  },
  {
    "text": "lots of degrees of freedom to tune a lot of different hyperparameters, and the right answer of what a hyper, uh,",
    "start": "3448885",
    "end": "3457615"
  },
  {
    "text": "parameter value needs to be is generally not obvious, uh, by, you know,",
    "start": "3457615",
    "end": "3463735"
  },
  {
    "text": "just reading the problem description on just by looking at the data. If it were obvious, there will be a lot of theory",
    "start": "3463735",
    "end": "3470020"
  },
  {
    "text": "and there isn't a lot of theory for, uh, deciding what is the right number of layers by just looking at the data or the word,",
    "start": "3470020",
    "end": "3478390"
  },
  {
    "text": "know what's the right activation to use looking at the data. You can use some intuitions which you might,",
    "start": "3478390",
    "end": "3485305"
  },
  {
    "text": "you know, develop over practice for a long time, but more often than not,",
    "start": "3485305",
    "end": "3490375"
  },
  {
    "text": "even like the world's best experts, they just generally tune it through cross-validation.",
    "start": "3490375",
    "end": "3495505"
  },
  {
    "text": "Have a holdout cross-validation set and fit your network on your training data and see how well it performs on,",
    "start": "3495505",
    "end": "3501535"
  },
  {
    "text": "you know, validation data. [BACKGROUND]",
    "start": "3501535",
    "end": "3514269"
  },
  {
    "text": "So what do you mean by alpha? Oh, the learning rate yeah? [BACKGROUND]",
    "start": "3514270",
    "end": "3527349"
  },
  {
    "text": "Yeah. Uh, and in fact, you still need to choose alpha as well. You know, like the choice of alpha hasn't gone away.",
    "start": "3527350",
    "end": "3533005"
  },
  {
    "text": "So you need to choose alpha. We need to choose the number of layers. We need to choose the number of neurons per layer.",
    "start": "3533005",
    "end": "3540325"
  },
  {
    "text": "We need to choose what activation functions to use, right? There are a lots of hyperparameters that you need to tune in neural network, right?",
    "start": "3540325",
    "end": "3547570"
  },
  {
    "text": "And, um, there are many strategies for doing cross-validation. For example, if it was just learning rate,",
    "start": "3547570",
    "end": "3555025"
  },
  {
    "text": "then you can- you could have done some kind of a binary search in a sweep some region and- and find a good a- a- a- a lear-learning rate.",
    "start": "3555025",
    "end": "3562600"
  },
  {
    "text": "But here, uh, you basically have this hypercube of different hyperparameters, right?",
    "start": "3562600",
    "end": "3570970"
  },
  {
    "text": "And each- each point in this hypercube represents some kind of a configuration. And the way you go about searching it is,",
    "start": "3570970",
    "end": "3578500"
  },
  {
    "text": "um, people sometimes do something, uh, that's called, um, a random search.",
    "start": "3578500",
    "end": "3583750"
  },
  {
    "text": "We- we- we- we're actually gonna cover strategies for tuning your hyperparameters later this week.",
    "start": "3583750",
    "end": "3590590"
  },
  {
    "text": "But in general, um, the answer of how you- how you choose these values is through cross-validation, right?",
    "start": "3590590",
    "end": "3598590"
  },
  {
    "text": "You know, try some, you know, um- um- um, come up with some, uh, possible configuration, see how well it works,",
    "start": "3598590",
    "end": "3604775"
  },
  {
    "text": "or try some other configuration, see how well it works. And, you know, keep trying that until you feel satisfied, you know,",
    "start": "3604775",
    "end": "3611140"
  },
  {
    "text": "which is not a very satisfactory answer, but that's exactly how it is done in practice. Yes question.",
    "start": "3611140",
    "end": "3616390"
  },
  {
    "text": "[BACKGROUND]. We're gonna come, why we need a learning rate.",
    "start": "3616390",
    "end": "3622810"
  },
  {
    "text": "We- we still haven't seen how we, you know, optimize this yet. We're still talking about the network architecture.",
    "start": "3622810",
    "end": "3628990"
  },
  {
    "text": "[BACKGROUND]",
    "start": "3628990",
    "end": "3642790"
  },
  {
    "text": "The width. Yeah. [BACKGROUND]",
    "start": "3642790",
    "end": "3650170"
  },
  {
    "text": "Yeah. In general, if- if the width of the network becomes extremely wide or if the depth of the network becomes extremely deep,",
    "start": "3650170",
    "end": "3656905"
  },
  {
    "text": "you're- you're absolutely, you know, it's absolutely possible that you may overfit your network- overfit on your data.",
    "start": "3656905",
    "end": "3663444"
  },
  {
    "text": "And um, and of which is why you need to do cross-validation well, you know, you measure the performance on a validation set and see",
    "start": "3663445",
    "end": "3670240"
  },
  {
    "text": "that your training performance is very good, test performance, uh, validations set is not so good so you overfit. Yes, question?",
    "start": "3670240",
    "end": "3676870"
  },
  {
    "text": "[BACKGROUND]",
    "start": "3676870",
    "end": "3685870"
  },
  {
    "text": "So- [OVERLAPPING]. Is it something, I mean, is it something that we set? Yeah, so the question is, is the depth and the width,",
    "start": "3685870",
    "end": "3691390"
  },
  {
    "text": "uh, something that the model learns on its own? Um, the answer is no. Uh, which is why it's called a hyperparameter, right?",
    "start": "3691390",
    "end": "3697870"
  },
  {
    "text": "So the parameters, in terms of terminology, parameters are those variables which",
    "start": "3697870",
    "end": "3703000"
  },
  {
    "text": "the model learns on its own using gradient descent or whatever. Hyperparameters are the ones, uh,",
    "start": "3703000",
    "end": "3709750"
  },
  {
    "text": "are the variables that you as a- as, you know, a human being who's building the model will choose.",
    "start": "3709750",
    "end": "3716725"
  },
  {
    "text": "Yeah. So, you know, the number of layers, uh, is something you decide before you start running",
    "start": "3716725",
    "end": "3722559"
  },
  {
    "text": "your gradient descent and that's called a hyperparameter. Uh, so for cross-validation you just try different configuration, it's not [inaudible]",
    "start": "3722560",
    "end": "3729670"
  },
  {
    "text": "Exactly for cross-validation, you- you try different hyperparameters and see how well each configuration of hyperparameter works on the validation set. Yes question?",
    "start": "3729670",
    "end": "3739750"
  },
  {
    "text": "Can you not just do something like [inaudible] [OVERLAPPING]. I'm not gonna go into that right away [LAUGHTER]",
    "start": "3739750",
    "end": "3747460"
  },
  {
    "text": "So that is- that is a lot of research where, uh, research of- on- on how to, um- um,",
    "start": "3747460",
    "end": "3754525"
  },
  {
    "text": "most efficiently or even learn what the right hyperparameters are gonna be and that's way beyond our scope right now.",
    "start": "3754525",
    "end": "3760825"
  },
  {
    "text": "Uh, but yes, there is- there is active research going on, um, to- to automatically learn the hyperparameters.",
    "start": "3760825",
    "end": "3767230"
  },
  {
    "text": "Yes. Yes question. [BACKGROUND]",
    "start": "3767230",
    "end": "3777700"
  },
  {
    "text": "Yes. So the question is, should the activation functions be bounded? Uh, 'coz tanh and sigmoid are bounded but ReLU is not.",
    "start": "3777700",
    "end": "3786160"
  },
  {
    "text": "Um, there are- there are good reasons to keep it, uh, bounded and there are good reasons to not keep it bounded as well.",
    "start": "3786160",
    "end": "3793119"
  },
  {
    "text": "Um, maybe we'll cover it later- later today or if not, uh, in the future. The- the, um, the problem with, right.",
    "start": "3793120",
    "end": "3803200"
  },
  {
    "text": "So maybe we're- we're kind of, um, uh, skipping ahead. But in general, um, [NOISE] the- the problem with a sigmoid network, uh,",
    "start": "3803200",
    "end": "3813310"
  },
  {
    "text": "sigmoid function like this is as you- as you kind of reach,",
    "start": "3813310",
    "end": "3819985"
  },
  {
    "text": "as- as your z value goes further and further away from 0,",
    "start": "3819985",
    "end": "3824840"
  },
  {
    "text": "the gradient of the sigmoid is pretty much 0, right?",
    "start": "3826410",
    "end": "3832599"
  },
  {
    "text": "So which means, um, which means if- if,",
    "start": "3832600",
    "end": "3838704"
  },
  {
    "text": "um, as we will see, uh, uh, later today, if your gradient becomes 0,",
    "start": "3838705",
    "end": "3843760"
  },
  {
    "text": "then your learning effectively stops because you know, when you're- when you're doing gradient update, if your gradient is 0, your learning effectively stops, right?",
    "start": "3843760",
    "end": "3850210"
  },
  {
    "text": "[BACKGROUND]. Yeah, so I'm going to postpone that for,",
    "start": "3850210",
    "end": "3857980"
  },
  {
    "text": "you know another 20 minutes or so. And when we talk backpropagation, probably that's going to be a better time to discuss that.",
    "start": "3857980",
    "end": "3864890"
  },
  {
    "text": "All right. So, um, we want- so this is how we- we,",
    "start": "3866210",
    "end": "3871944"
  },
  {
    "text": "uh, construct a fully connected network, and now, uh, once we have constructed it, um,",
    "start": "3871945",
    "end": "3877960"
  },
  {
    "text": "[NOISE] you're gonna see how we're gonna train this network. [NOISE] Right?",
    "start": "3877960",
    "end": "3883885"
  },
  {
    "text": "And that brings us to backpropagation. [NOISE]",
    "start": "3883885",
    "end": "3936405"
  },
  {
    "text": "So to recall, [NOISE] a naught is x, right?",
    "start": "3936405",
    "end": "3944145"
  },
  {
    "text": "And z_1 is w_1 times b naught,",
    "start": "3944145",
    "end": "3953325"
  },
  {
    "text": "plus b_1. A_1 is g of z_1,",
    "start": "3953325",
    "end": "3963825"
  },
  {
    "text": "and z_2 is w_2",
    "start": "3963825",
    "end": "3969300"
  },
  {
    "text": "of a_1 plus a_2,",
    "start": "3969300",
    "end": "3974400"
  },
  {
    "text": "and a_2 plus g of z_2.",
    "start": "3974400",
    "end": "3983244"
  },
  {
    "text": "And we see this pattern of alternating between z and a, z and a, all the way until,",
    "start": "3983245",
    "end": "3988425"
  },
  {
    "text": "let's call it, a of, say, L, where L is the number of layers in your network,",
    "start": "3988425",
    "end": "3994395"
  },
  {
    "text": "is equal to g of z of L. And then y hat is equal to a_L.",
    "start": "3994395",
    "end": "4005700"
  },
  {
    "text": "Right? And then our loss, we'll use script L of y,",
    "start": "4005700",
    "end": "4012730"
  },
  {
    "text": "y hat is equal to-, for now, we're gonna assume we're doing, um,",
    "start": "4012730",
    "end": "4018859"
  },
  {
    "text": "a classification problem rather than regression. If its regression you're gonna ha- you're gonna see the squared error here,",
    "start": "4018860",
    "end": "4023885"
  },
  {
    "text": "y minus y hat square. For regression, this is gonna be, y log y hat plus 1 minus y,",
    "start": "4023885",
    "end": "4034039"
  },
  {
    "text": "log 1 minus y hat. Yes, question?",
    "start": "4034040",
    "end": "4039200"
  },
  {
    "text": "[inaudible].",
    "start": "4039200",
    "end": "4046849"
  },
  {
    "text": "Uh, what should be L minus 1? [inaudible].",
    "start": "4046850",
    "end": "4054770"
  },
  {
    "text": "So, uh, if there are L layers, should this be L minus 1? [inaudible].",
    "start": "4054770",
    "end": "4062690"
  },
  {
    "text": "And we don't count that. We don't count the input layer. Okay. Yeah. Right. So this is our loss.",
    "start": "4062690",
    "end": "4070880"
  },
  {
    "text": "And the- in order to- to, to, uh, train our network,",
    "start": "4070880",
    "end": "4076820"
  },
  {
    "text": "the- our goal is",
    "start": "4076820",
    "end": "4085430"
  },
  {
    "text": "to do something like this. For l in 1, 2, L,",
    "start": "4085430",
    "end": "4099005"
  },
  {
    "text": "w_l equals-",
    "start": "4099005",
    "end": "4108330"
  },
  {
    "text": "equals w_L minus.",
    "start": "4109000",
    "end": "4116720"
  },
  {
    "text": "So, um, because we are treating this as a loss, this should be the negative of the likelihood- of the log-likelihood,",
    "start": "4116720",
    "end": "4125494"
  },
  {
    "text": "because we're thinking of this as a loss. And because it's the loss, we're doing gradient descent.",
    "start": "4125495",
    "end": "4131165"
  },
  {
    "text": "Minus Alpha times partial",
    "start": "4131165",
    "end": "4143330"
  },
  {
    "text": "derivative of L portion.",
    "start": "4143330",
    "end": "4150630"
  },
  {
    "text": "And similarly,",
    "start": "4151960",
    "end": "4154770"
  },
  {
    "text": "b of L minus Alpha times",
    "start": "4162700",
    "end": "4168799"
  },
  {
    "text": "partial of L over partial of.",
    "start": "4168800",
    "end": "4176730"
  },
  {
    "text": "Right. So very similar to- to gradient ascent that we solve for logistic regression.",
    "start": "4183040",
    "end": "4190640"
  },
  {
    "text": "In logistic regression, we had just one set of theta vectors, and we would perform gradient ascent on that theta vector.",
    "start": "4190640",
    "end": "4197855"
  },
  {
    "text": "Here we have a collection of w and b matrices and vectors. And we have L number of these matrices and vectors.",
    "start": "4197855",
    "end": "4207185"
  },
  {
    "text": "And what we wanna do is for each weight matrix and- and vec- uh, bias vector, take the gradient of the final loss, right,",
    "start": "4207185",
    "end": "4216440"
  },
  {
    "text": "final loss all the way at the end, with respect to the corresponding, um, weight layer, uh, or weight matrix of that layer,",
    "start": "4216440",
    "end": "4224195"
  },
  {
    "text": "and perform a gradient update like this. And similarly to the, uh, uh, bias term as well.",
    "start": "4224195",
    "end": "4229940"
  },
  {
    "text": "Right. So this is our goal. And the way we go about",
    "start": "4229940",
    "end": "4235520"
  },
  {
    "text": "calculating these gradients is using an algorithm called backpropagation.",
    "start": "4235520",
    "end": "4241205"
  },
  {
    "text": "Right. So backpropagation helps us calculate these gradients, but once we calculate the gradients,",
    "start": "4241205",
    "end": "4247130"
  },
  {
    "text": "we perform gradient descent on the last function. Yes, question? Is it stochastic?",
    "start": "4247130",
    "end": "4252970"
  },
  {
    "text": "So, uh, the question is, is this stochastic? In this case, it is stochastic because,",
    "start": "4252970",
    "end": "4259225"
  },
  {
    "text": "uh, uh, we are- we have considered just one example. All right. So, yeah, good question.",
    "start": "4259225",
    "end": "4265099"
  },
  {
    "text": "So think of this as y_i, so y hat i, and, you know, y_i.",
    "start": "4265100",
    "end": "4272070"
  },
  {
    "text": "Yeah, so this is just the ith example. Right. So for- for a given example, um,",
    "start": "4274390",
    "end": "4283670"
  },
  {
    "text": "this would be like the stochastic gradient descent update rule. All right. Now the question is, now how are we gonna calculate these, um,",
    "start": "4283670",
    "end": "4291500"
  },
  {
    "text": "these, uh, derivatives, uh, of L with respect to w and L with respect to b?",
    "start": "4291500",
    "end": "4296555"
  },
  {
    "text": "Right. And the short answer there is chain rule. Right. And if you're- if you're, uh,",
    "start": "4296555",
    "end": "4303755"
  },
  {
    "text": "familiar with multivariable calculus, if you are already experts at taking,",
    "start": "4303755",
    "end": "4308780"
  },
  {
    "text": "um, at- at, at, at ta-, uh, applying the chain rule in a multivariate setting, the rest of the lecture is probably boring for you because all what we're gonna",
    "start": "4308780",
    "end": "4316580"
  },
  {
    "text": "do is apply chain rule of calculus- multivariable calculus, to calculate these gradients.",
    "start": "4316580",
    "end": "4321635"
  },
  {
    "text": "Right. And that's pretty much all what backpropagation is. So backpropagation is a fancy name,",
    "start": "4321635",
    "end": "4327260"
  },
  {
    "text": "but the reason, um, um, it's called backpropagation is, mathematically it is just the chain rule, right,",
    "start": "4327260",
    "end": "4336320"
  },
  {
    "text": "but the way we go about calculating the gradients, is we start from the end and work our way backwards.",
    "start": "4336320",
    "end": "4343055"
  },
  {
    "text": "So algorithmically, right, uh, we- we follow a particular sequence of steps,",
    "start": "4343055",
    "end": "4349310"
  },
  {
    "text": "which makes it appear that the gradients are being calculated in a backward fashion, right. But mathematically, it is just the chain rule.",
    "start": "4349310",
    "end": "4356825"
  },
  {
    "text": "Right. There's- there's a difference between, um, a mathematical answer and an algorithmic answer, right.",
    "start": "4356825",
    "end": "4362270"
  },
  {
    "text": "For the same mathematical answer there can be multiple algorithmic answers. We saw that with kernels as well. Right. For the same- for the same inner product,",
    "start": "4362270",
    "end": "4369635"
  },
  {
    "text": "we could either calculate the explicit feature representation. We take the inner- the, the dot-product between them,",
    "start": "4369635",
    "end": "4375199"
  },
  {
    "text": "or we can directly calculate the- the kernel function. Right. They're- they're mathematically equivalent but algorithmically different.",
    "start": "4375200",
    "end": "4381845"
  },
  {
    "text": "Similarly, the backpropagation mathematically is just the chain rule, right.",
    "start": "4381845",
    "end": "4388775"
  },
  {
    "text": "But algorithmically, we calculate it in a way such that it is memory efficient and we reuse a lot of the computation while- when,",
    "start": "4388775",
    "end": "4398330"
  },
  {
    "text": "uh, um, when- when, when- we use a lot of the intermediate computations when calculating, right.",
    "start": "4398330",
    "end": "4404210"
  },
  {
    "text": "So that's- that's backpropagation. Uh, and let's- let's see how we go about doing it.",
    "start": "4404210",
    "end": "4409505"
  },
  {
    "text": "Any- any questions so far? [NOISE]",
    "start": "4409505",
    "end": "4418034"
  },
  {
    "text": "Before we- we go into that propagation, there was a- a question asked earlier.",
    "start": "4418035",
    "end": "4424665"
  },
  {
    "text": "Right? Why- why can we not- what would happen- or rather the question was,",
    "start": "4424665",
    "end": "4429960"
  },
  {
    "text": "wouldn't all the neurons in say the first layer, learn the same thing, right?",
    "start": "4429960",
    "end": "4435665"
  },
  {
    "text": "Why- why aren't we just learning multiple copies of the same logistic regression?",
    "start": "4435665",
    "end": "4441160"
  },
  {
    "text": "And the answer of that is, if we perform an initialization where the entire network is initialized to 0s,",
    "start": "4441160",
    "end": "4448545"
  },
  {
    "text": "all the weights and biases are initialized to 0s, then that's exactly what will happen. We start with a 0 initialization.",
    "start": "4448545",
    "end": "4455010"
  },
  {
    "text": "What we will observe is all the neurons in every layer are learning the same thing.",
    "start": "4455010",
    "end": "4461250"
  },
  {
    "text": "So we- we'll just end up having multiple copies of the same neuron within a given layer.",
    "start": "4461250",
    "end": "4466800"
  },
  {
    "text": "And in order to avoid the problem,",
    "start": "4466800",
    "end": "4472739"
  },
  {
    "text": "we perform what is called as a random initialization, which means we gonna initia- initialize all the weights",
    "start": "4472740",
    "end": "4478469"
  },
  {
    "text": "and biases in the network in a random way, by- by calling a random number generator and- and, uh, initializing them.",
    "start": "4478470",
    "end": "4486525"
  },
  {
    "text": "And this is a necessary step, which is, um, also called symmetry breaking.",
    "start": "4486525",
    "end": "4493215"
  },
  {
    "text": "Because the network is- is a- a symmetric within a layer, right? And in order to break that symmetry,",
    "start": "4493215",
    "end": "4499949"
  },
  {
    "text": "um, we initialize them at- at different rand- at, uh, different random initializations, and because of that random initialization,",
    "start": "4499950",
    "end": "4507675"
  },
  {
    "text": "they will end up learning different functions. So each logistic regression will end up learning,",
    "start": "4507675",
    "end": "4513585"
  },
  {
    "text": "uh- wi- within a layer, is going to end up learning different weights and biases just",
    "start": "4513585",
    "end": "4519180"
  },
  {
    "text": "because we have start- we have started from a random initialization. In case of logistic regression,",
    "start": "4519180",
    "end": "4526620"
  },
  {
    "text": "no matter where we initialize from, we always reach the same answer. But over here, uh,",
    "start": "4526620",
    "end": "4533940"
  },
  {
    "text": "in- in logistic regression, we always use the same answer because the problem was convex. But once we add in a neural network setting, we lose convexity.",
    "start": "4533940",
    "end": "4542010"
  },
  {
    "text": "This- this giant composition of functions in general will not be con- convex.",
    "start": "4542010",
    "end": "4550619"
  },
  {
    "text": "Which means depending on the initialization, we are going to reach a different solution, right?",
    "start": "4550620",
    "end": "4557375"
  },
  {
    "text": "And which is why random initialization is- is- is very necessary. And the way we go about",
    "start": "4557375",
    "end": "4563989"
  },
  {
    "text": "initializing is- so W^L.",
    "start": "4563990",
    "end": "4573940"
  },
  {
    "text": "So in- in- in, um, in logistic regression or- or- or in general in our learning algorithms,",
    "start": "4573990",
    "end": "4582585"
  },
  {
    "text": "the way we set about doing it is initialize w's and b's,",
    "start": "4582585",
    "end": "4591780"
  },
  {
    "text": "and then for log of i in 1,",
    "start": "4591780",
    "end": "4597449"
  },
  {
    "text": "through some kind or another t and some time step. Now Theta equals Theta minus Alpha times something, right?",
    "start": "4597450",
    "end": "4607950"
  },
  {
    "text": "In order to do the initialization step, which in logistic regression and linear models we would just initialize them to 0.",
    "start": "4607950",
    "end": "4615765"
  },
  {
    "text": "In that, in- in- in place of zero initialization, we will instead do an initialization like this. So W^l initialize it with a random number generator, a Gaussian random number generator of mean 0,",
    "start": "4615765",
    "end": "4629460"
  },
  {
    "text": "and square root of 2 over n^l plus n^ l minus 1.",
    "start": "4629460",
    "end": "4642900"
  },
  {
    "text": "Or this is one choice. Or we could also do W of l is",
    "start": "4642900",
    "end": "4649469"
  },
  {
    "text": "uniform minus 0.1 plus",
    "start": "4649470",
    "end": "4657090"
  },
  {
    "text": "0.1 There are many different initialization schemes. So this is just a random uniform initialization and- and,",
    "start": "4657090",
    "end": "4665340"
  },
  {
    "text": "um, this is commonly done. This has a particular name, it's called, um, Xavier initialization.",
    "start": "4665340",
    "end": "4672840"
  },
  {
    "text": "[NOISE] And this is",
    "start": "4672840",
    "end": "4682590"
  },
  {
    "text": "another trial of initialization. And what you will observe is the choice of initialization is also",
    "start": "4682590",
    "end": "4688620"
  },
  {
    "text": "a hyper-parameter when you are learning to fit your model. Yes, question?",
    "start": "4688620",
    "end": "4694630"
  },
  {
    "text": "The W^L is just specific of given layer [inaudible]",
    "start": "4696850",
    "end": "4702000"
  },
  {
    "text": "So the superscript identifies what layer each thing belongs to. So W1 and b1,",
    "start": "4702000",
    "end": "4709485"
  },
  {
    "text": "they belong to the layer 1? Yes. So now so the b1 is shared by every Bayes component- shared by every [OVERLAPPING]",
    "start": "4709485",
    "end": "4721370"
  },
  {
    "text": "So the question is, in our- is this shared by every neuron in the layer? So the dimension of b1 is the number of neurons in that layer.",
    "start": "4721370",
    "end": "4731940"
  },
  {
    "text": "And each element of this vector corresponds to one neuron.",
    "start": "4731940",
    "end": "4737199"
  },
  {
    "text": "Okay. Yes, question. What's that small element [inaudible]",
    "start": "4737200",
    "end": "4743790"
  },
  {
    "text": "Yeah, so this over here is the number of neurons in the L clear and the number of neurons in the L minus 1th layer.",
    "start": "4743790",
    "end": "4752260"
  },
  {
    "text": "Yeah, we can- we can discuss the reason for that.",
    "start": "4753530",
    "end": "4759449"
  },
  {
    "text": "Probably if you remind me again, we'll- uh, we can discuss the reason for that. Yes, question?",
    "start": "4759450",
    "end": "4765705"
  },
  {
    "text": "I wonder would this get the same answer as multiple copies with respect with what you taught",
    "start": "4765705",
    "end": "4770760"
  },
  {
    "text": "us because this is like if you see the second layer and the first layer,",
    "start": "4770760",
    "end": "4776739"
  },
  {
    "text": "first layer input like in the second layer note that the [inaudible]",
    "start": "4783590",
    "end": "4794010"
  },
  {
    "text": "So the- the- the question is- is all the nodes be the same?",
    "start": "4794010",
    "end": "4800070"
  },
  {
    "text": "The the- the answer is no because W is a matrix and each row of the W matrix is associated with each neuron.",
    "start": "4800070",
    "end": "4811485"
  },
  {
    "text": "Right? And if W is initialized randomly, then each row of W is going to be different- different vector. Right? So [OVERLAPPING]",
    "start": "4811485",
    "end": "4820740"
  },
  {
    "text": "[inaudible] So, yeah. Over here,",
    "start": "4820740",
    "end": "4825855"
  },
  {
    "text": "you know, I should probably put this as W_ij, right.",
    "start": "4825855",
    "end": "4831660"
  },
  {
    "text": "So each element of your W matrix initialize it by performing a random sample from this question.",
    "start": "4831660",
    "end": "4839699"
  },
  {
    "text": "[inaudible] when updating we take some. Yes, when we are updating, we need to update.",
    "start": "4839700",
    "end": "4846705"
  },
  {
    "text": "So you- we can also see this as, you know, W_ij of lth layer equals W_ij of l minus alpha",
    "start": "4846705",
    "end": "4859725"
  },
  {
    "text": "times partial of the loss with respect to W of ij.",
    "start": "4859725",
    "end": "4868080"
  },
  {
    "text": "And this, and this are equivalent. There's just uh, um, uh, vectorized notation.",
    "start": "4868080",
    "end": "4875440"
  },
  {
    "text": "All right, so the training algorithm, you know, first we initialize w and b with some kind of initialization.",
    "start": "4876470",
    "end": "4884640"
  },
  {
    "text": "And for each- for each iteration we have a nested loop for l In 1,",
    "start": "4884640",
    "end": "4897660"
  },
  {
    "text": "2, l W^l equals",
    "start": "4897660",
    "end": "4906010"
  },
  {
    "text": "Alpha times partial of loss with respect to partial W^t, right?",
    "start": "4908660",
    "end": "4921159"
  },
  {
    "text": "So this is going to be our- our high level algorithm. First, initialize all the W's and b with",
    "start": "4923900",
    "end": "4929790"
  },
  {
    "text": "some kind of a chosen random initialization, okay? Initialize the full network, and then for each iteration,",
    "start": "4929790",
    "end": "4938205"
  },
  {
    "text": "we're going to- we're going to take some examples, maybe one examples or maybe a few examples,",
    "start": "4938205",
    "end": "4944295"
  },
  {
    "text": "and we're going to calculate the gradient of the loss with respect to",
    "start": "4944295",
    "end": "4951179"
  },
  {
    "text": "the weights and biases of each layer and perform a gradient update of each of the weights and bias parameter.",
    "start": "4951180",
    "end": "4959175"
  },
  {
    "text": "Yes. Question. [inaudible]",
    "start": "4959175",
    "end": "4967650"
  },
  {
    "text": "So what do you mean by weight 1 and weight 2? Do you mean the weight- by weight 1, uh, do you mean the first layer or the first iteration?",
    "start": "4967650",
    "end": "4974760"
  },
  {
    "text": "[inaudible] we get a breakthrough from that, we update that weight.",
    "start": "4974760",
    "end": "4982710"
  },
  {
    "text": "Does it have a dependence on someone this weight? [inaudible]",
    "start": "4982710",
    "end": "4988830"
  },
  {
    "text": "So we'll see what the relation between the weights are now next. So now what I'm going to draw is you can call it a computation graph.",
    "start": "4988830",
    "end": "5000505"
  },
  {
    "text": "This- this will allow us to understand how all the different terms are dependent on the loss, all right?",
    "start": "5000505",
    "end": "5007270"
  },
  {
    "text": "So first, I'm gonna have- I'm gonna call this a_0.",
    "start": "5007270",
    "end": "5015995"
  },
  {
    "text": "And this is basically just x. And to this a_0, we're going use",
    "start": "5015995",
    "end": "5022890"
  },
  {
    "text": "matrix w_1 and a bias B_1, right?",
    "start": "5023050",
    "end": "5034460"
  },
  {
    "text": "And here,now we have some outcomes,",
    "start": "5034460",
    "end": "5044820"
  },
  {
    "text": "z_1 and the computation that happens here. So basically, each, um, um,",
    "start": "5048670",
    "end": "5055840"
  },
  {
    "text": "square or a rectangle is some vector, right? And the clouds are basically computations, right?",
    "start": "5055840",
    "end": "5063380"
  },
  {
    "text": "So some, some, some operation happening here. And here, the operation is z, z equals wa plus b, right?",
    "start": "5063380",
    "end": "5076820"
  },
  {
    "text": "And now this goes into a cloud of operation",
    "start": "5076820",
    "end": "5081889"
  },
  {
    "text": "and comes out as a_1.",
    "start": "5081890",
    "end": "5088200"
  },
  {
    "text": "And here the operation is a equals g of z, okay?",
    "start": "5088600",
    "end": "5096470"
  },
  {
    "text": "And similarly, next, we have w_2,",
    "start": "5096470",
    "end": "5107180"
  },
  {
    "text": "b_2 to some operation",
    "start": "5107180",
    "end": "5117620"
  },
  {
    "text": "for which these going as inputs and outcomes,",
    "start": "5117620",
    "end": "5123360"
  },
  {
    "text": "z_2, and again something else,",
    "start": "5126130",
    "end": "5134280"
  },
  {
    "text": "this we'll call it, um, a_2. And again this is just g, right?",
    "start": "5139090",
    "end": "5146585"
  },
  {
    "text": "And here it is z equals wa plus b, okay?",
    "start": "5146585",
    "end": "5155105"
  },
  {
    "text": "And let's just draw one more, um, okay?",
    "start": "5155105",
    "end": "5161735"
  },
  {
    "text": "Now here- let's say we have w_3, b_3.",
    "start": "5161735",
    "end": "5169380"
  },
  {
    "text": "Again, um, z equals wa plus b,",
    "start": "5176710",
    "end": "5183170"
  },
  {
    "text": "and we get z_3",
    "start": "5183170",
    "end": "5188820"
  },
  {
    "text": "and another g. You get- so,",
    "start": "5191980",
    "end": "5197210"
  },
  {
    "text": "uh, for the sake of simplicity, we will assume that the final layer has just one row because we want a scalar.",
    "start": "5197210",
    "end": "5205020"
  },
  {
    "text": "And b_3 will also be just a scalar, okay?",
    "start": "5209350",
    "end": "5220685"
  },
  {
    "text": "And z_3 is also a scalar,",
    "start": "5220685",
    "end": "5223950"
  },
  {
    "text": "and we get a_3, right?",
    "start": "5229270",
    "end": "5236900"
  },
  {
    "text": "And a_3 is also equal to y hat, right?",
    "start": "5236900",
    "end": "5242929"
  },
  {
    "text": "And I just want to continue it here, and we have y hat and y.",
    "start": "5242930",
    "end": "5256820"
  },
  {
    "text": "So one arrow comes into this cloud. This y comes in here, and outcomes our loss, right?",
    "start": "5256820",
    "end": "5266195"
  },
  {
    "text": "And the loss is also a scalar, right?",
    "start": "5266195",
    "end": "5271804"
  },
  {
    "text": "We start with the input vector. And this- the dimension of this is the dimension of our data, right?",
    "start": "5271805",
    "end": "5278765"
  },
  {
    "text": "And the dimension of z was a hyperparameter that we chose, right?",
    "start": "5278765",
    "end": "5284750"
  },
  {
    "text": "Um, and, and the dimension of z, the dimension of a and the dimension of b is equal to the number of layers in the first,",
    "start": "5284750",
    "end": "5293585"
  },
  {
    "text": "first layer- the number of neurons in the first layer, right? And the way we can compute z is wa plus b,",
    "start": "5293585",
    "end": "5301820"
  },
  {
    "text": "when the ws and bs are coming here, um, and so on. We, we, we, we ness this by, you know,",
    "start": "5301820",
    "end": "5308614"
  },
  {
    "text": "we are adding them, and finally we have a loss, right? And what we want to do is take the partial derivative or",
    "start": "5308615",
    "end": "5316100"
  },
  {
    "text": "the gradient of L with respect to each ws,",
    "start": "5316100",
    "end": "5321875"
  },
  {
    "text": "and take the partial derivative of L with respect to each bs, right?",
    "start": "5321875",
    "end": "5329000"
  },
  {
    "text": "And then perform gradient of it. Yes, question? [inaudible] Yes.",
    "start": "5329000",
    "end": "5337080"
  },
  {
    "text": "So, so, uh, the question is, what if y hat was a vector, right?",
    "start": "5347350",
    "end": "5353045"
  },
  {
    "text": "Um, y hat would, would, uh, um, in general, y hat could be anything as long as your loss is a scalar, right?",
    "start": "5353045",
    "end": "5360860"
  },
  {
    "text": "Eventually we want the loss to be a scalar. [BACKGROUND] Yeah,",
    "start": "5360860",
    "end": "5363780"
  },
  {
    "text": "so the question is, uh, could we not, uh, append? Uh, um, yeah, so,",
    "start": "5375070",
    "end": "5381785"
  },
  {
    "text": "uh, uh, you know, have another, another, uh, one- extend the a vector by 1 and extend another row for, uh, w [OVERLAPPING].",
    "start": "5381785",
    "end": "5390560"
  },
  {
    "text": "Yeah, you could, you could do that. Um, in, um, the- you, you, you could do that, but, uh, in practice,",
    "start": "5390560",
    "end": "5396935"
  },
  {
    "text": "ws and bs are kept separate, and there's a good reason for that.",
    "start": "5396935",
    "end": "5402410"
  },
  {
    "text": "Um, we saw the, the reason in SVMs. So if you remember that we were only penalizing the no- norm of w and not penalizing b.",
    "start": "5402410",
    "end": "5411425"
  },
  {
    "text": "And there's gonna be a similar reason why where we- we are gonna, um, uh, perform something called regularization that,",
    "start": "5411425",
    "end": "5418445"
  },
  {
    "text": "that's gonna come in the future, only on ws and not on bs. So it's, it's- fun notation it's, it's- in this case, it's good to keep the bs and ws separate.",
    "start": "5418445",
    "end": "5426020"
  },
  {
    "text": "[inaudible]",
    "start": "5426020",
    "end": "5435000"
  },
  {
    "text": "Yeah. Yeah, yeah, that's right. Yeah, if you- you- you could do that.",
    "start": "5435000",
    "end": "5440690"
  },
  {
    "text": "uh, I mean, that is- mathematically they are the same, right? Uh, if you- if you think of,",
    "start": "5440690",
    "end": "5445880"
  },
  {
    "text": "ah, if you, [NOISE] at every layer, if you extend a,",
    "start": "5445880",
    "end": "5451235"
  },
  {
    "text": "uh, if you extend a by adding a 1.",
    "start": "5451235",
    "end": "5456980"
  },
  {
    "text": "Then you can get rid of b, sure you can do that. Uh, but in practice, you know, uh, it's exactly the same,",
    "start": "5456980",
    "end": "5463700"
  },
  {
    "text": "but in practice, you know, nobody does that. Anyways, so now the question is, how are we gonna compute each of these partial derivatives of the final loss,",
    "start": "5463700",
    "end": "5473705"
  },
  {
    "text": "which is a scalar, right? These is a scalar with respect to this matrix,",
    "start": "5473705",
    "end": "5480010"
  },
  {
    "text": "this bias and, you know, and so on. And- and that's where chain rule comes to help.",
    "start": "5480010",
    "end": "5485390"
  },
  {
    "text": "And that's what back propagation is all about. Okay,  so I'm gonna switch to that board.  [NOISE].",
    "start": "5485390",
    "end": "5496400"
  },
  {
    "text": "So first we are going to make a few observations. For this um, ex- example,",
    "start": "5496400",
    "end": "5503750"
  },
  {
    "text": "we are assuming binary classification. That is, ys are in 0s and 1s and,",
    "start": "5503750",
    "end": "5514550"
  },
  {
    "text": "[NOISE] y's are 0s and 1s. [NOISE]",
    "start": "5514550",
    "end": "5530600"
  },
  {
    "text": "But in general, your y's could be anything and your last function could be something.",
    "start": "5530600",
    "end": "5536210"
  },
  {
    "text": "Here we're just assuming the logistic loss, right? So first we uh will observe that um,",
    "start": "5536210",
    "end": "5542075"
  },
  {
    "text": "so we will work it out for w2 um,",
    "start": "5542075",
    "end": "5548075"
  },
  {
    "text": "and even in the notes we have done it for w2. With the steps are exactly the same for w1 and w3 and b1, b3, right?",
    "start": "5548075",
    "end": "5554240"
  },
  {
    "text": "Once- once you know how to do it for one, you can just apply the same recipe for all of them. So, partial of loss with respect",
    "start": "5554240",
    "end": "5562670"
  },
  {
    "text": "to w2 [NOISE] is equal to,",
    "start": "5562670",
    "end": "5571670"
  },
  {
    "text": "if you remember, from our matrix calculus review is gonna be partial of L with respect to",
    "start": "5571670",
    "end": "5580355"
  },
  {
    "text": "partial of w2 1,1, partial w2.",
    "start": "5580355",
    "end": "5594900"
  },
  {
    "text": "In these case, uh in- in- in the nodes we, uh, the second node has three.",
    "start": "5596100",
    "end": "5604770"
  },
  {
    "text": "So w- so this had three, but- but in general, you know, it's just uh, depending on the dimensions of w, you know,",
    "start": "5604770",
    "end": "5614540"
  },
  {
    "text": "you have uh- in this case it's just 1,3 and,",
    "start": "5614540",
    "end": "5621380"
  },
  {
    "text": "[NOISE] right?",
    "start": "5621380",
    "end": "5631520"
  },
  {
    "text": "So this is a matrix and you take the partial of this loss with respect to every single element and arrange as a matrix.",
    "start": "5631520",
    "end": "5637460"
  },
  {
    "text": "So this is it- you know, th- this is the definition of uh, uh, the gradient of a scalar loss with respect to a matrix. Yes question?",
    "start": "5637460",
    "end": "5647150"
  },
  {
    "text": "[inaudible] w2, is the second layer",
    "start": "5647150",
    "end": "5653750"
  },
  {
    "text": "[inaudible] you- you could,",
    "start": "5653750",
    "end": "5661520"
  },
  {
    "text": "uh, um so first let's work out the math and then we look at the algorithm of how we go about doing it. So this is just math, you know, what's- what's the gradient of- of- [inaudible].",
    "start": "5661520",
    "end": "5670940"
  },
  {
    "text": "Yeah this one is the middle layer, there is the middle layer, right? And now [NOISE] make a few more observations [NOISE].",
    "start": "5670940",
    "end": "5683210"
  },
  {
    "text": "So the partial of L with respect to z3. So what is z here?",
    "start": "5683210",
    "end": "5691430"
  },
  {
    "text": "z3 is- [NOISE] so",
    "start": "5691430",
    "end": "5700850"
  },
  {
    "text": "the partial of L with respect to z3 is [NOISE]",
    "start": "5700850",
    "end": "5718130"
  },
  {
    "text": "equal to partial of z3 times minus",
    "start": "5718130",
    "end": "5725929"
  },
  {
    "text": "y log y hat minus 1 minus y log 1 minus y-hat, right?",
    "start": "5725930",
    "end": "5739180"
  },
  {
    "text": "And these [NOISE] works out to be [NOISE] -",
    "start": "5739180",
    "end": "5750870"
  },
  {
    "text": "So I'm gonna few- skip a few steps here. [NOISE] It's basically the same kind of calculation you did with logistic regression.",
    "start": "5755710",
    "end": "5763655"
  },
  {
    "text": "And you get these is minus y log y hat is basically sigmoid of",
    "start": "5763655",
    "end": "5772684"
  },
  {
    "text": "z3 minus 1 minus y log 1 minus sigmoid of z3, right?",
    "start": "5772685",
    "end": "5786150"
  },
  {
    "text": "And you can do the steps- the- there's basically the same kind of steps we did for homework 1,",
    "start": "5788470",
    "end": "5796400"
  },
  {
    "text": "question 1, Part A. When you're- when you're showing uh- uh the uh positive definiteness of the hessian of logistic regression.",
    "start": "5796400",
    "end": "5804020"
  },
  {
    "text": "You would have calculated, you know, basically the same steps and these simplifies to, [NOISE] a3 minus y.",
    "start": "5804020",
    "end": "5810830"
  },
  {
    "text": "So, partial of L with respect to z3 is equal to a3 minus y.",
    "start": "5810830",
    "end": "5816710"
  },
  {
    "text": "[NOISE] And now- [NOISE]",
    "start": "5816710",
    "end": "5824150"
  },
  {
    "text": "and know we are gonna calculate,",
    "start": "5824150",
    "end": "5830690"
  },
  {
    "text": "the partial of each of these. I'm gonna pick 1 ijth element and see what it turns out to be.",
    "start": "5830690",
    "end": "5838580"
  },
  {
    "text": "So the partial of L with respect to partial of wij,",
    "start": "5838580",
    "end": "5847595"
  },
  {
    "text": "of the second layer. Is equal to partial of L with respect to,",
    "start": "5847595",
    "end": "5856910"
  },
  {
    "text": "partial of a3 times partial of",
    "start": "5856910",
    "end": "5867630"
  },
  {
    "text": "a3 with respect to partial of wij, right?",
    "start": "5867670",
    "end": "5879830"
  },
  {
    "text": "This is a chain rule. Any questions for this? Okay. Then these will further break it down",
    "start": "5879830",
    "end": "5886910"
  },
  {
    "text": "[NOISE]",
    "start": "5886910",
    "end": "5893330"
  },
  {
    "text": "times partial of.",
    "start": "5893330",
    "end": "5898440"
  },
  {
    "text": "Any question from here to here? Again, just the chain rule, and so on.",
    "start": "5915930",
    "end": "5923949"
  },
  {
    "text": "And now, we observe that this- did I miss something?",
    "start": "5923950",
    "end": "5929680"
  },
  {
    "text": "Z, um, so- so one more step.",
    "start": "5929680",
    "end": "5936610"
  },
  {
    "text": "So partial of L with respect to partial of a3,",
    "start": "5936610",
    "end": "5942159"
  },
  {
    "text": "times partial of a3, with respect to partial of z3,",
    "start": "5942160",
    "end": "5952180"
  },
  {
    "text": "times partial of a-",
    "start": "5952180",
    "end": "5957230"
  },
  {
    "text": "a3_ z3 and z3_ a2,",
    "start": "5962490",
    "end": "5967310"
  },
  {
    "text": "times partial of- z2",
    "start": "5980520",
    "end": "5988490"
  },
  {
    "text": "times partial of of z, not z- w_2_ij.",
    "start": "6003890",
    "end": "6010300"
  },
  {
    "text": "So basically apply the same thing and we get this- this long expression, right?",
    "start": "6015770",
    "end": "6022425"
  },
  {
    "text": "And how did we reach this expression? To reach this expression, [NOISE] what we basically did is [NOISE] applying the chain rule, right?",
    "start": "6022425",
    "end": "6034034"
  },
  {
    "text": "So dz- so d_a2 by d_z2 is a Jacobian,",
    "start": "6034035",
    "end": "6043710"
  },
  {
    "text": "because it's a vector valued function by a vector valued function, right?",
    "start": "6043710",
    "end": "6049305"
  },
  {
    "text": "So this is gonna be- so here we're gonna get uh, let me use the red to denote the gradients.",
    "start": "6049305",
    "end": "6057690"
  },
  {
    "text": "So the partial of L with respect to y hat right,",
    "start": "6057690",
    "end": "6063045"
  },
  {
    "text": "over here- so anywhere where there's a computation, we get a gradient or a Jacobian.",
    "start": "6063045",
    "end": "6068280"
  },
  {
    "text": "So L with respect to y hat is gonna be a scalar.",
    "start": "6068280",
    "end": "6073409"
  },
  {
    "text": "So I'm just going to denote it by a dot.",
    "start": "6073410",
    "end": "6078420"
  },
  {
    "text": "There's another computation here, output to input, and this is going to be a scalar because it is a scalar- it is a scalar.",
    "start": "6078560",
    "end": "6085469"
  },
  {
    "text": "The- the derivative is going to be a scalar. From scalar to vector.",
    "start": "6085470",
    "end": "6092849"
  },
  {
    "text": "So the derivative of a scalar valued output times a vector valued input is going to be a- a gradient of vector.",
    "start": "6092850",
    "end": "6099510"
  },
  {
    "text": "So I'm gonna write it as a vector, right? And the derivative of vector valued output by a vector valued input is going to be a?",
    "start": "6099510",
    "end": "6109245"
  },
  {
    "text": "A Jacobian matrix. So I'm going to have a matrix here.",
    "start": "6109245",
    "end": "6114300"
  },
  {
    "text": "Similarly, vector-valued uh, output to vector valued input is going to be a?",
    "start": "6114300",
    "end": "6119880"
  },
  {
    "text": "Jacobian. Right? Now, the-",
    "start": "6119880",
    "end": "6126389"
  },
  {
    "text": "the- the um, the way we go about doing this, uh,",
    "start": "6126390",
    "end": "6133995"
  },
  {
    "text": "the common pattern that you observe is that you- if the deeper your network is, you're going to have this alternating sequence of Zs",
    "start": "6133995",
    "end": "6140670"
  },
  {
    "text": "and A's and Zs and As and Zs and As, right? And from every a to every z, and from every z to every a,",
    "start": "6140670",
    "end": "6147435"
  },
  {
    "text": "you're gonna have- keep getting Jacobians, right? And whenever there is a g, the Jacobian, uh,",
    "start": "6147435",
    "end": "6156210"
  },
  {
    "text": "because this is element-wise operation, the Jacobian is going to be a diagonal matrix, right?",
    "start": "6156210",
    "end": "6161805"
  },
  {
    "text": "And over here also, uh, this is gonna be a diagonal matrix.",
    "start": "6161805",
    "end": "6167595"
  },
  {
    "text": "Whenever the non-linearity comes into picture because it's an element- wise operation. And whenever we- we um,",
    "start": "6167595",
    "end": "6177360"
  },
  {
    "text": "from z to a, because z is w times a, you will observe that the Jacobian from z to a will always",
    "start": "6177360",
    "end": "6187754"
  },
  {
    "text": "be z- will always be just w. There is a question.",
    "start": "6187754",
    "end": "6198000"
  },
  {
    "text": "The weights are all independent of each other, but uh in order to calculate the Chain rule,",
    "start": "6198200",
    "end": "6204750"
  },
  {
    "text": "[NOISE] what we do is we break them down into local derivatives. [inaudible]",
    "start": "6204750",
    "end": "6216870"
  },
  {
    "text": "Yeah, the- the uh, well, what do you mean by independent? [inaudible].",
    "start": "6216870",
    "end": "6226800"
  },
  {
    "text": "So here the- it is- it is diagonal only for the non-linearity, because that's just acting element wise.",
    "start": "6226800",
    "end": "6233190"
  },
  {
    "text": "There is no interaction between the other terms. It- it's just performing element-wise um, uh, operations.",
    "start": "6233190",
    "end": "6241860"
  },
  {
    "text": "So this is going to be uh, a diagonal matrix, all right? And from z to a,",
    "start": "6241860",
    "end": "6247949"
  },
  {
    "text": "it's always gonna be the corresponding weight matrix, and from a to z it's always going to be a diagonal matrix, right?",
    "start": "6247950",
    "end": "6254355"
  },
  {
    "text": "And no matter how deep you go, you- you just gonna keep getting this diagonal matrix,",
    "start": "6254355",
    "end": "6259364"
  },
  {
    "text": "corresponding weight matrix, diagonal matrix, corresponding weight matrix, and so on, right? And now when we want to calculate the gradient",
    "start": "6259365",
    "end": "6266715"
  },
  {
    "text": "of- and the gradients that we want to update uh, for values over here, right?",
    "start": "6266715",
    "end": "6274844"
  },
  {
    "text": "These are the ones we want to- these are the- the- the parameters that we wanna update, right?",
    "start": "6274845",
    "end": "6280710"
  },
  {
    "text": "So we want to calculate the- the- the gradient of the final loss with respect to",
    "start": "6280710",
    "end": "6287910"
  },
  {
    "text": "this blue dot- final loss with respect to this blue dot, and so on. And construct our update matrix, right?",
    "start": "6287910",
    "end": "6300225"
  },
  {
    "text": "Because this matrix we saw is- [NOISE] no, each of this corresponds to a blue dot, right?",
    "start": "6300225",
    "end": "6309930"
  },
  {
    "text": "So we wanna calculate the gradient of the final loss [NOISE] with respect to each blue dot and construct the corresponding update matrix.",
    "start": "6309930",
    "end": "6319125"
  },
  {
    "text": "And in order to calculate the final loss with respect to the blue dot, we just apply the chain rule.",
    "start": "6319125",
    "end": "6324795"
  },
  {
    "text": "And the chain rule says- back to the chain rule [NOISE].",
    "start": "6324795",
    "end": "6334380"
  },
  {
    "text": "The chain rule says, each of these are gonna be- this is gonna be- um,",
    "start": "6334380",
    "end": "6341800"
  },
  {
    "text": "so z to a is always gonna be the matrix w of",
    "start": "6343520",
    "end": "6351210"
  },
  {
    "text": "3. a to z is gonna be a diagonal matrix of g prime,",
    "start": "6351210",
    "end": "6361500"
  },
  {
    "text": "and this thing over here is what we calculated already.",
    "start": "6361500",
    "end": "6369660"
  },
  {
    "text": "That's a3 minus y. All right.",
    "start": "6369660",
    "end": "6377190"
  },
  {
    "text": "So we're gonna get a3 minus y, times w3, times diagonal of g prime of z2, times this one.",
    "start": "6377190",
    "end": "6386625"
  },
  {
    "text": "Now, how do we handle this? So once we bootstrap this, no matter how deep our network is,",
    "start": "6386625",
    "end": "6392460"
  },
  {
    "text": "we're just gonna keep adding on, you know, the w matrix itself and the diagonal of g prime,",
    "start": "6392460",
    "end": "6399344"
  },
  {
    "text": "the next w matrix, the next diagonal of g prime, next w matrix, next diagonal of g prime. And that's gonna be a recurring pattern.",
    "start": "6399345",
    "end": "6406409"
  },
  {
    "text": "And we just need to figure out the starting end, which we- which is common for all. Which is just you know, the a3 minus y.",
    "start": "6406410",
    "end": "6413804"
  },
  {
    "text": "And all the way at the beginning. And for all the way at the beginning, we are trying to calculate the gradient of a vector times a scalar, right?",
    "start": "6413805",
    "end": "6423750"
  },
  {
    "text": "So this is a vector and this is a scalar. z- z2 is a vector,",
    "start": "6423750",
    "end": "6429480"
  },
  {
    "text": "w_ i_ j is a scalar, right? So in- in our picture [NOISE],",
    "start": "6429480",
    "end": "6436010"
  },
  {
    "text": "now we are trying to calculate um, the gradient of the z vector with respect to the scalar.",
    "start": "6436010",
    "end": "6445420"
  },
  {
    "text": "And this is again uh, pretty straightforward. [NOISE]",
    "start": "6445420",
    "end": "6454675"
  },
  {
    "text": "And this comes out to be- so if you are doing it for wij,",
    "start": "6454675",
    "end": "6463433"
  },
  {
    "text": "this will just be a matrix with 0s",
    "start": "6463434",
    "end": "6471010"
  },
  {
    "text": "everywhere except aj of",
    "start": "6471010",
    "end": "6477670"
  },
  {
    "text": "1 in the ith position, right?",
    "start": "6477670",
    "end": "6483054"
  },
  {
    "text": "And the reason is again, pretty, pretty simple because z is just wa plus b.",
    "start": "6483055",
    "end": "6493480"
  },
  {
    "text": "So the- the derivative with respect to w- so wij can only influence the ith element of z,",
    "start": "6493480",
    "end": "6505400"
  },
  {
    "text": "wij cannot, so if- if wij is- is say from- from the second row,",
    "start": "6505830",
    "end": "6513175"
  },
  {
    "text": "so all these parameters of w can only influence the second, second element of z.",
    "start": "6513175",
    "end": "6520130"
  },
  {
    "text": "So the derivative of this vector with respect to this is going to have zeros everywhere else and",
    "start": "6520130",
    "end": "6526900"
  },
  {
    "text": "the influence of ij th element on the ith element is just aj, okay?",
    "start": "6526900",
    "end": "6535554"
  },
  {
    "text": "So this is going to be just aj in the ith position. Any questions on this?",
    "start": "6535555",
    "end": "6544810"
  },
  {
    "text": "And- and- and so if- if we kind of do a dimensional check,",
    "start": "6544810",
    "end": "6550255"
  },
  {
    "text": "this is 1 cross 1,  w3 is,",
    "start": "6550255",
    "end": "6555610"
  },
  {
    "text": "in this case, in- in- in the- in the example in the notes it's 1 cross 2 and this diagonal is going to be 2 cross 2,",
    "start": "6555610",
    "end": "6564594"
  },
  {
    "text": "but with only diagonal entries and this is gonna be 2 cross",
    "start": "6564595",
    "end": "6570220"
  },
  {
    "text": "1 and that example there's- there's- there's only- there's only 2 so",
    "start": "6570220",
    "end": "6578889"
  },
  {
    "text": "the other one will be 0 and aj will be ith element, and now we can combine all of these into, into a single entry.",
    "start": "6578890",
    "end": "6589690"
  },
  {
    "text": "So this is basically a_3 minus y, a scalar times w_3 and when you multiply,",
    "start": "6589690",
    "end": "6600830"
  },
  {
    "text": "a diagonal matrix with another matrix, you get the matrix which is the same as",
    "start": "6601070",
    "end": "6607410"
  },
  {
    "text": "performing element-wise multiplication, right?",
    "start": "6607410",
    "end": "6616300"
  },
  {
    "text": "When you multiply by a diagonal matrix, with the- with the vector, it's essentially the same as performing element-wise multiplication",
    "start": "6616300",
    "end": "6624475"
  },
  {
    "text": "and this whole thing is now a one by two times aj,",
    "start": "6624475",
    "end": "6638050"
  },
  {
    "text": "which is 2 by 1 in the ith position.",
    "start": "6638050",
    "end": "6646760"
  },
  {
    "text": "And further, this can be further simplified as a3 minus y times w_3 element-wise,",
    "start": "6648540",
    "end": "6661705"
  },
  {
    "text": "g prime of z_3, all right?",
    "start": "6661705",
    "end": "6667450"
  },
  {
    "text": "And because it's- this is zeros in everywhere except the ith position, this vector,",
    "start": "6667450",
    "end": "6673390"
  },
  {
    "text": "we only need the ith index times",
    "start": "6673390",
    "end": "6680540"
  },
  {
    "text": "aj_1, okay?",
    "start": "6682290",
    "end": "6691585"
  },
  {
    "text": "And this is for ijth element. So partial l with respect to wij",
    "start": "6691585",
    "end": "6698635"
  },
  {
    "text": "is the ith element of this vector times jth element of this vector, which means partial l with respect to partial",
    "start": "6698635",
    "end": "6709645"
  },
  {
    "text": "w. The full matrix is just the outer product between these two vectors, all right.",
    "start": "6709645",
    "end": "6716430"
  },
  {
    "text": "So that is a_3 minus y, w_3, g prime z_3.",
    "start": "6716430",
    "end": "6729159"
  },
  {
    "text": "So this is the vector from which we took the ith ele ment instead. For the full matrix, it's going to be this vector outer product with",
    "start": "6729160",
    "end": "6737230"
  },
  {
    "text": "aj_1 transpose. Next question.",
    "start": "6737230",
    "end": "6744970"
  },
  {
    "text": "What is [inaudible]? It's aj_1 because we are calculating with respect to w_2,",
    "start": "6744970",
    "end": "6752515"
  },
  {
    "text": "and for w_2, sorry, I should have kept the picture here. For w_2, a_1 is the thing that gets multiplied by.",
    "start": "6752515",
    "end": "6761635"
  },
  {
    "text": "So what- what you see here is- there is a fair amount of  notation,",
    "start": "6761635",
    "end": "6768625"
  },
  {
    "text": "but the main idea to- to take away from this is the loss is- the final loss is always",
    "start": "6768625",
    "end": "6774580"
  },
  {
    "text": "a scalar and we want to take the derivatives with respect to elements of a given matrix, right?",
    "start": "6774580",
    "end": "6781480"
  },
  {
    "text": "And for that, we use the chain rule to,",
    "start": "6781480",
    "end": "6787314"
  },
  {
    "text": "to start taking the gradient all the way from the final loss to the layer below- before to the layer before,",
    "start": "6787314",
    "end": "6796600"
  },
  {
    "text": "to the layer before, to the layer before and each of those gives you a Jacobian,",
    "start": "6796600",
    "end": "6801880"
  },
  {
    "text": "and those Jacobians have a repetitive pattern. The very first time it is something different, you know,",
    "start": "6801880",
    "end": "6809170"
  },
  {
    "text": "a_3 minus y and this comes from logistic regression, and then onwards, all the Jacobians,",
    "start": "6809170",
    "end": "6815230"
  },
  {
    "text": "you're gonna have this daisy chain of Jacobians and they are very easy to compute. They're gonna be the corresponding weight matrix or",
    "start": "6815230",
    "end": "6820870"
  },
  {
    "text": "a diagonal of the derivative of the activation function, right? And you're going to have a daisy chain of this- of these Jacobians,",
    "start": "6820870",
    "end": "6828400"
  },
  {
    "text": "depending on how deep your network is, right? And the dimensions are always gonna be such that the output of this layer",
    "start": "6828400",
    "end": "6835719"
  },
  {
    "text": "is going to be the input of this layer and the output of this is gonna be the input of this and you know,",
    "start": "6835720",
    "end": "6840820"
  },
  {
    "text": "they will always match in dimensions. So we're going to start with a 1-by-1 and gonna end up with a 1 and",
    "start": "6840820",
    "end": "6847375"
  },
  {
    "text": "all the intermediate- intermediate dimensions where the- where they meet will always match and once you- once you multiply this whole thing,",
    "start": "6847375",
    "end": "6857425"
  },
  {
    "text": "you're gonna end up with a scalar, a 1-by-1, 1-by-1, solution and that's gonna",
    "start": "6857425",
    "end": "6863710"
  },
  {
    "text": "be the value you're going to fill in here and then you repeat that. When you repeat that we see that- that 1 by 1 entry was basically",
    "start": "6863710",
    "end": "6871719"
  },
  {
    "text": "the product of the ith element of one vector times the jth element of another vector, which means the full matrix is just the outer product",
    "start": "6871720",
    "end": "6878020"
  },
  {
    "text": "between those two vectors. Yes, question? In the final result will be g prime 2 [inaudible].",
    "start": "6878020",
    "end": "6888670"
  },
  {
    "text": "Yeah, this should be- you're right, so this should be 2. Thank you.",
    "start": "6888670",
    "end": "6892970"
  },
  {
    "text": "Yeah. It's a 2 - 2 over here, I made it a 3 over there.",
    "start": "6898350",
    "end": "6903320"
  },
  {
    "text": "Right? So that's basically- that's basically the- applying the chain rule to get the partial derivatives and basically backprop is now- no,",
    "start": "6903570",
    "end": "6913750"
  },
  {
    "text": "this what- what we see, you know, this daisy chain structure. If we want to compute the partial with respect to w_1 now, right?",
    "start": "6913750",
    "end": "6922705"
  },
  {
    "text": "It's basically you are gonna add two more lin  ks into this daisy chain, all right?",
    "start": "6922705",
    "end": "6928120"
  },
  {
    "text": "Into this daisy chain of Jacobians, you're gonna have two more Jacobians and the final and- and this is gonna be specific to the previous layer, right?",
    "start": "6928120",
    "end": "6937704"
  },
  {
    "text": "And basically, backprop tells you that if we start computing the- the derivatives with respect to matrix all the way from the nth,",
    "start": "6937705",
    "end": "6946045"
  },
  {
    "text": "then we can reuse the terms. We- we don't have to re-compute the- the product of this,",
    "start": "6946045",
    "end": "6955119"
  },
  {
    "text": "which is gonna be a vector for the next- for the previous layer, we're just going to have another w matrix and a diagonal matrix and",
    "start": "6955120",
    "end": "6963850"
  },
  {
    "text": "the- the corresponding a matrix- a- a vector for the previous layer.",
    "start": "6963850",
    "end": "6968920"
  },
  {
    "text": "So we can reuse all this computation when we're calculating the derivatives for the previous layer and that's- that's basically backpropagation.",
    "start": "6968920",
    "end": "6975925"
  },
  {
    "text": "It suggests you that you should start working backwards so that you can reuse computation.",
    "start": "6975925",
    "end": "6981864"
  },
  {
    "text": "Right? Any question, next question. [inaudible].",
    "start": "6981864",
    "end": "6988900"
  },
  {
    "text": "Yeah, so, so here we are, you know uh,. In this daisy chain, you observe that we are moving from vector to vector to vector, right?",
    "start": "6988900",
    "end": "6995920"
  },
  {
    "text": "So the local derivative will be a Jacobian. With respect to vector.",
    "start": "6995920",
    "end": "7001805"
  },
  {
    "text": "With respect to vector exactly. All right, so then we'll break.",
    "start": "7001805",
    "end": "7006389"
  }
]