[
  {
    "text": "So, uh, good.",
    "start": "5034",
    "end": "6495"
  },
  {
    "text": "So far we talked about a single graph neural network layer.",
    "start": "6495",
    "end": "11280"
  },
  {
    "text": "What I'm going to talk about next is to talk about, uh,",
    "start": "11280",
    "end": "14849"
  },
  {
    "text": "how do we stack, uh,",
    "start": "14850",
    "end": "16935"
  },
  {
    "text": "layers into a multi-layer,",
    "start": "16935",
    "end": "18900"
  },
  {
    "text": "uh, graph neural network.",
    "start": "18900",
    "end": "21404"
  },
  {
    "text": "So, we first talked about, uh,",
    "start": "21405",
    "end": "24640"
  },
  {
    "text": "designing and defining a single layer of a graph neural network,",
    "start": "24640",
    "end": "28789"
  },
  {
    "text": "where we said that it comp- it composes of",
    "start": "28790",
    "end": "31130"
  },
  {
    "text": "a message transformation operation and a message aggregation operation.",
    "start": "31130",
    "end": "36215"
  },
  {
    "text": "We also talked about additional,",
    "start": "36215",
    "end": "38375"
  },
  {
    "text": "uh, things you can add to training,",
    "start": "38375",
    "end": "40625"
  },
  {
    "text": "which is like batch normalization,",
    "start": "40625",
    "end": "42680"
  },
  {
    "text": "choice of different activation functions,",
    "start": "42680",
    "end": "45215"
  },
  {
    "text": "choice of L2 normalization,",
    "start": "45215",
    "end": "47600"
  },
  {
    "text": "um, things like that.",
    "start": "47600",
    "end": "49265"
  },
  {
    "text": "What we want to talk next is to talk about how can you stack these, uh,",
    "start": "49265",
    "end": "53875"
  },
  {
    "text": "layers one on top of each other and, uh, for example,",
    "start": "53875",
    "end": "57660"
  },
  {
    "text": "how can you add or skip connections to graph neural networks?",
    "start": "57660",
    "end": "61425"
  },
  {
    "text": "So that's the, uh,",
    "start": "61425",
    "end": "62805"
  },
  {
    "text": "topic I wanna, uh, discuss next.",
    "start": "62805",
    "end": "65860"
  },
  {
    "text": "So the question is,",
    "start": "66200",
    "end": "68575"
  },
  {
    "text": "how do I construct a graph neural network based on",
    "start": "68575",
    "end": "71619"
  },
  {
    "text": "the single layer that I have, already defined.",
    "start": "71620",
    "end": "75315"
  },
  {
    "text": "And, ah, a standard way,",
    "start": "75315",
    "end": "77105"
  },
  {
    "text": "a usual way would be to stack graph neural networks sequentially.",
    "start": "77105",
    "end": "80740"
  },
  {
    "text": "So basically the idea is I- under- as the embedding of node at layer 0,",
    "start": "80740",
    "end": "86290"
  },
  {
    "text": "I simply use the raw node features of the node and then I'm, you know,",
    "start": "86290",
    "end": "90565"
  },
  {
    "text": "transporting them individually layer by layer up to some,",
    "start": "90565",
    "end": "94630"
  },
  {
    "text": "uh, number of layers.",
    "start": "94630",
    "end": "95979"
  },
  {
    "text": "For example, in this case,",
    "start": "95980",
    "end": "97315"
  },
  {
    "text": "we have now a three-layer, uh,",
    "start": "97315",
    "end": "99330"
  },
  {
    "text": "graph neural network where on the input I get the raw node features x.",
    "start": "99330",
    "end": "104365"
  },
  {
    "text": "They get transformed into the embedding at a level 3 of, uh,",
    "start": "104365",
    "end": "110140"
  },
  {
    "text": "node v. That is actually an important issue in,",
    "start": "110140",
    "end": "117220"
  },
  {
    "text": "uh, graph neural networks that prevents us from stacking too many layers, uh, together.",
    "start": "117220",
    "end": "124010"
  },
  {
    "text": "And the important point here that I want to make",
    "start": "124010",
    "end": "127340"
  },
  {
    "text": "is introduce you the notion of over-smoothing,",
    "start": "127340",
    "end": "130914"
  },
  {
    "text": "um, and discuss how to prevent it.",
    "start": "130915",
    "end": "134135"
  },
  {
    "text": "And then one thing I wanna also make, uh,",
    "start": "134135",
    "end": "137659"
  },
  {
    "text": "a point about is that the depth of the graph neural network",
    "start": "137660",
    "end": "141995"
  },
  {
    "text": "is different than the depth in terms of number of layers in,",
    "start": "141995",
    "end": "147260"
  },
  {
    "text": "let's say convolutional neural networks.",
    "start": "147260",
    "end": "149405"
  },
  {
    "text": "So the depth of graph neural networks really tells me how many",
    "start": "149405",
    "end": "153830"
  },
  {
    "text": "hops away in the network do I go to collect the information.",
    "start": "153830",
    "end": "158675"
  },
  {
    "text": "And it doesn't necessarily say how complex or how expressive, uh,",
    "start": "158675",
    "end": "163055"
  },
  {
    "text": "the entire network is,",
    "start": "163055",
    "end": "165409"
  },
  {
    "text": "because that depends on the design of each individual layer of the graph neural network.",
    "start": "165410",
    "end": "171230"
  },
  {
    "text": "So kind of the point I'm trying to make,",
    "start": "171230",
    "end": "173599"
  },
  {
    "text": "the notion of a layer in",
    "start": "173600",
    "end": "174830"
  },
  {
    "text": "a graph neural network is different than the notion of a layer in,",
    "start": "174830",
    "end": "178310"
  },
  {
    "text": "let's say, a convolutional neural network.",
    "start": "178310",
    "end": "180880"
  },
  {
    "text": "So the issue of stacking many GNN layers together is",
    "start": "180880",
    "end": "185270"
  },
  {
    "text": "that GNNs tend to suffer from what is called an over-smoothing problem.",
    "start": "185270",
    "end": "189905"
  },
  {
    "text": "And the over-smoothing problem is that kind of node embeddings,",
    "start": "189905",
    "end": "194880"
  },
  {
    "text": "you know, converge to the same or very similar value.",
    "start": "194880",
    "end": "198740"
  },
  {
    "text": "And the reason why this is happening is because,",
    "start": "198740",
    "end": "202130"
  },
  {
    "text": "uh, if the receptive fields,",
    "start": "202130",
    "end": "204395"
  },
  {
    "text": "as I'm going to define them later of the- of the networks are too big,",
    "start": "204395",
    "end": "208880"
  },
  {
    "text": "then basically all the network- all the neural networks collect the same information,",
    "start": "208880",
    "end": "213635"
  },
  {
    "text": "so at the end, the final output is also the same for all different nodes.",
    "start": "213635",
    "end": "218629"
  },
  {
    "text": "And we don't want to have this problem of over-smoothing",
    "start": "218630",
    "end": "221540"
  },
  {
    "text": "because we want embeddings for different nodes to be different.",
    "start": "221540",
    "end": "225260"
  },
  {
    "text": "So let me tell you more about what is an over-smoothing problem",
    "start": "225260",
    "end": "229430"
  },
  {
    "text": "and why does it happen and how do we, uh, remedy it.",
    "start": "229430",
    "end": "233689"
  },
  {
    "text": "So first, we need to define this notion of a receptive field.",
    "start": "233690",
    "end": "237995"
  },
  {
    "text": "A receptive field, uh,",
    "start": "237995",
    "end": "239720"
  },
  {
    "text": "is a set of nodes that determine the embedding of the node of interest.",
    "start": "239720",
    "end": "244520"
  },
  {
    "text": "So in a K layer GNN, uh,",
    "start": "244520",
    "end": "247495"
  },
  {
    "text": "each node has a receptive field of k-hops- k-hop neighborhood around that node.",
    "start": "247495",
    "end": "253610"
  },
  {
    "text": "And this becomes, uh,",
    "start": "253610",
    "end": "255395"
  },
  {
    "text": "important because for example,",
    "start": "255395",
    "end": "257180"
  },
  {
    "text": "if you say I wanted to link prediction between the two yellow nodes,",
    "start": "257180",
    "end": "260690"
  },
  {
    "text": "uh, in this graph.",
    "start": "260690",
    "end": "262135"
  },
  {
    "text": "Then the question is,",
    "start": "262135",
    "end": "263820"
  },
  {
    "text": "how ma- as I increase the,",
    "start": "263820",
    "end": "265740"
  },
  {
    "text": "uh, depth of the network,",
    "start": "265740",
    "end": "267585"
  },
  {
    "text": "how many, uh, and look at",
    "start": "267585",
    "end": "270210"
  },
  {
    "text": "the corresponding computation graphs of the two, uh, yellow nodes?",
    "start": "270210",
    "end": "274610"
  },
  {
    "text": "uh, the question is, how, um,",
    "start": "274610",
    "end": "277550"
  },
  {
    "text": "how big, uh, how big is the receptive field, right?",
    "start": "277550",
    "end": "281490"
  },
  {
    "text": "So for example, what I'm trying to show here is for a single node, uh,",
    "start": "281490",
    "end": "285910"
  },
  {
    "text": "the receptive field at one layer,",
    "start": "285910",
    "end": "287990"
  },
  {
    "text": "so one hop away is, you know,",
    "start": "287990",
    "end": "289819"
  },
  {
    "text": "this four, uh, five different,",
    "start": "289820",
    "end": "291815"
  },
  {
    "text": "uh, red nodes denoted here.",
    "start": "291815",
    "end": "294065"
  },
  {
    "text": "If I say now, let's do a two-layer neural network.",
    "start": "294065",
    "end": "297050"
  },
  {
    "text": "This is now the receptive field.",
    "start": "297050",
    "end": "298639"
  },
  {
    "text": "It's all neighbors and all the neighbors of neighbors.",
    "start": "298640",
    "end": "301520"
  },
  {
    "text": "It's like one-hop neighborhood and two-hop neighborhood.",
    "start": "301520",
    "end": "304655"
  },
  {
    "text": "And if I go to a three-layer neural network,",
    "start": "304655",
    "end": "306995"
  },
  {
    "text": "in this case of a small graph,",
    "start": "306995",
    "end": "308824"
  },
  {
    "text": "now, notice that basically,",
    "start": "308825",
    "end": "310400"
  },
  {
    "text": "majority or almost every node in the net- in the- in",
    "start": "310400",
    "end": "313580"
  },
  {
    "text": "the underlying network is part of my graph neural network architecture.",
    "start": "313580",
    "end": "318335"
  },
  {
    "text": "So this means that this yellow node is going to collect information from",
    "start": "318335",
    "end": "322490"
  },
  {
    "text": "every other node in the network to combi- to determine its own, uh, embedding.",
    "start": "322490",
    "end": "327880"
  },
  {
    "text": "Now, if you, for example wanna do link prediction, um,",
    "start": "327880",
    "end": "332355"
  },
  {
    "text": "then you, uh, you wanna say whether a pair of nodes is,",
    "start": "332355",
    "end": "336240"
  },
  {
    "text": "uh, connected or not.",
    "start": "336240",
    "end": "337550"
  },
  {
    "text": "And what is interesting in this case is that, um,",
    "start": "337550",
    "end": "340970"
  },
  {
    "text": "the- the number of neighbors that are shared grows very",
    "start": "340970",
    "end": "345110"
  },
  {
    "text": "quickly as we increase the number of hops in the graph neural network.",
    "start": "345110",
    "end": "349564"
  },
  {
    "text": "So now, uh, I have a different visualization here.",
    "start": "349565",
    "end": "352880"
  },
  {
    "text": "I have two nodes denoted by yellow and I",
    "start": "352880",
    "end": "356745"
  },
  {
    "text": "compute one-hop neighborhoods from each and I say what nodes are in the intersection?",
    "start": "356745",
    "end": "361875"
  },
  {
    "text": "What nodes are shared?",
    "start": "361875",
    "end": "362970"
  },
  {
    "text": "And here, one node is shared.",
    "start": "362970",
    "end": "364605"
  },
  {
    "text": "Then if I say, let's compute 2-hop neighborhood,",
    "start": "364605",
    "end": "367650"
  },
  {
    "text": "now, all these neighbors are shared.",
    "start": "367650",
    "end": "370340"
  },
  {
    "text": "And if I say how many neighbors are shared, uh,",
    "start": "370340",
    "end": "374300"
  },
  {
    "text": "between 3-hops, how many nodes are share- shared within three hops?",
    "start": "374300",
    "end": "378335"
  },
  {
    "text": "Again, you see that basically almost all the nodes are shared.",
    "start": "378335",
    "end": "381905"
  },
  {
    "text": "And the problem then becomes that as the network is aggregating all this information",
    "start": "381905",
    "end": "387995"
  },
  {
    "text": "and all the- all the nodes- all the graph neural networks basically get the same inputs,",
    "start": "387995",
    "end": "394260"
  },
  {
    "text": "it will be increasingly hard to differentiate between different nodes, uh, you know,",
    "start": "394260",
    "end": "399710"
  },
  {
    "text": "let's say the nodes that are- that are going to be connected in",
    "start": "399710",
    "end": "402289"
  },
  {
    "text": "the network and the nodes that are not connected in the network.",
    "start": "402290",
    "end": "406120"
  },
  {
    "text": "So, uh, you know,",
    "start": "406120",
    "end": "408169"
  },
  {
    "text": "how do we explain the notion of over-smoothing with this definition of a receptive field?",
    "start": "408170",
    "end": "414275"
  },
  {
    "text": "Uh, you know, we know that the embedding of a node is-",
    "start": "414275",
    "end": "417220"
  },
  {
    "text": "this determined by its receptive field, right?",
    "start": "417220",
    "end": "420060"
  },
  {
    "text": "And if two nodes have highly overlapping receptive fields,",
    "start": "420060",
    "end": "423900"
  },
  {
    "text": "there- there- then their embeddings are also going to be most likely, uh, similar.",
    "start": "423900",
    "end": "429335"
  },
  {
    "text": "So this means that if I stack many GNN layers together,",
    "start": "429335",
    "end": "433490"
  },
  {
    "text": "then it means nodes will have highly overlapping receptive fields, uh,",
    "start": "433490",
    "end": "437720"
  },
  {
    "text": "which means they will collect information from",
    "start": "437720",
    "end": "440255"
  },
  {
    "text": "the same part of the network and they will aggregate it in the same way.",
    "start": "440255",
    "end": "443840"
  },
  {
    "text": "So node embeddings will be highly similar.",
    "start": "443840",
    "end": "446915"
  },
  {
    "text": "So it means it can be very hard for us to distinguish between",
    "start": "446915",
    "end": "449840"
  },
  {
    "text": "different nodes and this is what we call an over-smoothing problem, right?",
    "start": "449840",
    "end": "453680"
  },
  {
    "text": "It's like you collect too much information from the neighborhood and then,",
    "start": "453680",
    "end": "458340"
  },
  {
    "text": "um, if you collect kind of too much,",
    "start": "458340",
    "end": "460580"
  },
  {
    "text": "everyone collects the same information,",
    "start": "460580",
    "end": "463009"
  },
  {
    "text": "so every node kind of has the same information,",
    "start": "463010",
    "end": "466580"
  },
  {
    "text": "computes the same embedding and it is very hard to differentiate between them.",
    "start": "466580",
    "end": "470465"
  },
  {
    "text": "So the question is,",
    "start": "470465",
    "end": "472280"
  },
  {
    "text": "how do we overcome over-smoothing?",
    "start": "472280",
    "end": "473885"
  },
  {
    "text": "Uh, first is that we are cautious about how many,",
    "start": "473885",
    "end": "478580"
  },
  {
    "text": "um, layers, how many GNN layers, uh, do we use.",
    "start": "478580",
    "end": "482764"
  },
  {
    "text": "So what this means that, unlike in, uh,",
    "start": "482765",
    "end": "485925"
  },
  {
    "text": "neural networks in other domains like",
    "start": "485925",
    "end": "487955"
  },
  {
    "text": "convolutional neural networks for image classification,",
    "start": "487955",
    "end": "491030"
  },
  {
    "text": "adding more layers to our graph neural network does not always skip.",
    "start": "491030",
    "end": "495865"
  },
  {
    "text": "So first, what we need to do, uh,",
    "start": "495865",
    "end": "498410"
  },
  {
    "text": "to determine how many layers is good is to analyze then the-",
    "start": "498410",
    "end": "501575"
  },
  {
    "text": "the amount of information we need to make a good prediction.",
    "start": "501575",
    "end": "505760"
  },
  {
    "text": "So basically, analyze different depths,",
    "start": "505760",
    "end": "508400"
  },
  {
    "text": "different receptive fields, and try to get a good, uh,",
    "start": "508400",
    "end": "511850"
  },
  {
    "text": "balance between the diameter of the network and the amount of",
    "start": "511850",
    "end": "515120"
  },
  {
    "text": "information that a single GNN is aggregating goal.",
    "start": "515120",
    "end": "519214"
  },
  {
    "text": "Because if the depth is too big, then basically,",
    "start": "519215",
    "end": "521900"
  },
  {
    "text": "the receptive field of a single node may basically be the entire, uh, network.",
    "start": "521900",
    "end": "526610"
  },
  {
    "text": "The second thing is that we wanna s- setup the number of GNN layers L to be, uh,",
    "start": "526610",
    "end": "533459"
  },
  {
    "text": "a bit, uh, more than the receptive field we, uh,",
    "start": "533460",
    "end": "537530"
  },
  {
    "text": "we like, but we don't wanna make L to be unnecessarily large.",
    "start": "537530",
    "end": "542245"
  },
  {
    "text": "Um, so that's one way,",
    "start": "542245",
    "end": "544560"
  },
  {
    "text": "uh, to deal with this.",
    "start": "544560",
    "end": "546000"
  },
  {
    "text": "Another way to deal with this is to say,",
    "start": "546000",
    "end": "548495"
  },
  {
    "text": "how do we enhance expressive power of a GNN if the number of layers is smaller?",
    "start": "548495",
    "end": "554700"
  },
  {
    "text": "The way we do this is the following.",
    "start": "555140",
    "end": "559335"
  },
  {
    "text": "Um, right, how do we make GNNs more- more",
    "start": "559335",
    "end": "562110"
  },
  {
    "text": "expressive if we cannot make them more expressive but making them deeper.",
    "start": "562110",
    "end": "566144"
  },
  {
    "text": "One option is to- to add more expressive power within a GNN layer.",
    "start": "566145",
    "end": "571500"
  },
  {
    "text": "So what this means is that in our previous examples, uh,",
    "start": "571500",
    "end": "574965"
  },
  {
    "text": "each transformation or aggregation function was only one linear transformation.",
    "start": "574965",
    "end": "580245"
  },
  {
    "text": "But we can make aggregation and transformation become deep neural networks by themselves.",
    "start": "580245",
    "end": "586170"
  },
  {
    "text": "So for example, we could make the aggregation operator and the transformation operator,",
    "start": "586170",
    "end": "591029"
  },
  {
    "text": "let's say a three-layer- um,",
    "start": "591030",
    "end": "592980"
  },
  {
    "text": "uh, multilayer perceptron network, uh,",
    "start": "592980",
    "end": "595949"
  },
  {
    "text": "and not just a simple, uh,",
    "start": "595950",
    "end": "597780"
  },
  {
    "text": "linear, uh, layer in the network.",
    "start": "597780",
    "end": "600390"
  },
  {
    "text": "In this way add, um,",
    "start": "600390",
    "end": "602025"
  },
  {
    "text": "express- ex- expressiveness, uh, to the neural network.",
    "start": "602025",
    "end": "606000"
  },
  {
    "text": "Right, so now our single layer graph neural network is really a three-layer,",
    "start": "606000",
    "end": "611445"
  },
  {
    "text": "uh, deep neural network, right?",
    "start": "611445",
    "end": "613590"
  },
  {
    "text": "So the notion of a layer in a GNN and a notion of",
    "start": "613590",
    "end": "617010"
  },
  {
    "text": "a layer in terms of transformations, uh, is different.",
    "start": "617010",
    "end": "621885"
  },
  {
    "text": "So another way how we can make shallow GNNs more expressive is to add,",
    "start": "621885",
    "end": "628680"
  },
  {
    "text": "uh, layers that do not pass messages.",
    "start": "628680",
    "end": "631980"
  },
  {
    "text": "So what this means is that a GNN does not",
    "start": "631980",
    "end": "635324"
  },
  {
    "text": "necessarily have to contain only GNN layers, right?",
    "start": "635325",
    "end": "639375"
  },
  {
    "text": "We can, for example, have, uh,",
    "start": "639375",
    "end": "641040"
  },
  {
    "text": "multilayer perceptron layers before and after the GNN layers.",
    "start": "641040",
    "end": "645839"
  },
  {
    "text": "And you can think of these as pre-processing and post-processing layers.",
    "start": "645840",
    "end": "649980"
  },
  {
    "text": "To give you the idea, right,",
    "start": "649980",
    "end": "651660"
  },
  {
    "text": "we could take the input- uh,",
    "start": "651660",
    "end": "653310"
  },
  {
    "text": "massive inputs- um, input features,",
    "start": "653310",
    "end": "657120"
  },
  {
    "text": "transform them through the preprocessing step of",
    "start": "657120",
    "end": "660330"
  },
  {
    "text": "multilayer perceptron, apply the graph neural network layers,",
    "start": "660330",
    "end": "664080"
  },
  {
    "text": "and then again have a couple of, um, um, multilayer,",
    "start": "664080",
    "end": "667979"
  },
  {
    "text": "uh, perceptron layers that do the post-processing of embeddings.",
    "start": "667979",
    "end": "672255"
  },
  {
    "text": "So we can think of these as pre-processing layers that are-",
    "start": "672255",
    "end": "675540"
  },
  {
    "text": "that are important when encoding node features.",
    "start": "675540",
    "end": "679289"
  },
  {
    "text": "For example, if node features represent images or text,",
    "start": "679289",
    "end": "683280"
  },
  {
    "text": "we would want to have an entire CNN here, for example.",
    "start": "683280",
    "end": "686610"
  },
  {
    "text": "Um, and then we have our post-processing layers, uh,",
    "start": "686610",
    "end": "690120"
  },
  {
    "text": "which are important when we are reasoning,",
    "start": "690120",
    "end": "692460"
  },
  {
    "text": "or- or transforming over whether the node, uh, embeddings.",
    "start": "692460",
    "end": "695790"
  },
  {
    "text": "Uh, this becomes important if you are doing,",
    "start": "695790",
    "end": "697949"
  },
  {
    "text": "for example, graph classification or knowledge graphs,",
    "start": "697950",
    "end": "700890"
  },
  {
    "text": "where the transformations here add a lot to",
    "start": "700890",
    "end": "703410"
  },
  {
    "text": "the expressive power of the graph neural networks, right?",
    "start": "703410",
    "end": "706290"
  },
  {
    "text": "So in practice, adding these pre-processing and post-processing layers,",
    "start": "706290",
    "end": "710100"
  },
  {
    "text": "uh, works great in practice.",
    "start": "710100",
    "end": "712050"
  },
  {
    "text": "So it means we are combining classical neural network layers with graph,",
    "start": "712050",
    "end": "716565"
  },
  {
    "text": "uh, neural network layers.",
    "start": "716565",
    "end": "719220"
  },
  {
    "text": "So, uh, the- the last way how we can,",
    "start": "719220",
    "end": "724095"
  },
  {
    "text": "um, uh, think about, uh,",
    "start": "724095",
    "end": "726105"
  },
  {
    "text": "shallower graph neural networks,",
    "start": "726105",
    "end": "728519"
  },
  {
    "text": "but being the more expressive is to add this notion of a skip connection, right?",
    "start": "728520",
    "end": "735030"
  },
  {
    "text": "And the observation from Over-Smoothing problem that I discussed was",
    "start": "735030",
    "end": "738870"
  },
  {
    "text": "that node embeddings in earlier GNN layers can sometime,",
    "start": "738870",
    "end": "743654"
  },
  {
    "text": "um, better differentiate between different nodes earlier,",
    "start": "743655",
    "end": "747030"
  },
  {
    "text": "meaning lower layer, uh, embeddings.",
    "start": "747030",
    "end": "749595"
  },
  {
    "text": "And the solution is that we can increase the impact of",
    "start": "749595",
    "end": "752264"
  },
  {
    "text": "earlier layers on the final known embedding to add the shortcuts,",
    "start": "752265",
    "end": "756240"
  },
  {
    "text": "uh, in the neural network,",
    "start": "756240",
    "end": "757920"
  },
  {
    "text": "or what do we mean by shortcuts is skip connections, right?",
    "start": "757920",
    "end": "761610"
  },
  {
    "text": "So if I go now back to my picture from the previous slide,",
    "start": "761610",
    "end": "764519"
  },
  {
    "text": "what I can add is,",
    "start": "764520",
    "end": "765780"
  },
  {
    "text": "when I have the, um- the GNN layers,",
    "start": "765780",
    "end": "769725"
  },
  {
    "text": "I can add this red connections that basically skip a layer and go from,",
    "start": "769725",
    "end": "774660"
  },
  {
    "text": "um connectly- directly connect,",
    "start": "774660",
    "end": "776550"
  },
  {
    "text": "let's say the GNN layer 1 to the GNN layer 3,",
    "start": "776550",
    "end": "780149"
  },
  {
    "text": "and they skip this layer 2, uh, in-between.",
    "start": "780150",
    "end": "783090"
  },
  {
    "text": "So the idea is that this is now a skip connection.",
    "start": "783090",
    "end": "786795"
  },
  {
    "text": "Uh, so the message now gets duplicated.",
    "start": "786795",
    "end": "789540"
  },
  {
    "text": "One message goes into the transformation layer and,",
    "start": "789540",
    "end": "792329"
  },
  {
    "text": "uh, weight update,",
    "start": "792330",
    "end": "793635"
  },
  {
    "text": "while the same message also gets dup- duplicated and just kind of",
    "start": "793635",
    "end": "796590"
  },
  {
    "text": "sent forward and then the two branches are summed up.",
    "start": "796590",
    "end": "800025"
  },
  {
    "text": "So before adding skip connections,",
    "start": "800025",
    "end": "802890"
  },
  {
    "text": "we simply took the message and transformed it.",
    "start": "802890",
    "end": "805125"
  },
  {
    "text": "Now with a skip connection,",
    "start": "805125",
    "end": "806895"
  },
  {
    "text": "we take the message, transform it,",
    "start": "806895",
    "end": "808740"
  },
  {
    "text": "but then sum it up or aggregate it with the untransformed, uh, message itself.",
    "start": "808740",
    "end": "814395"
  },
  {
    "text": "So that is, um,",
    "start": "814395",
    "end": "816180"
  },
  {
    "text": "an interesting, uh, approach, um, as well.",
    "start": "816180",
    "end": "819820"
  },
  {
    "text": "So why do we care about skip connections and why do skip connections work?",
    "start": "819820",
    "end": "825200"
  },
  {
    "text": "Um, intuition is that skip connections create what is called, uh, mixture models.",
    "start": "825200",
    "end": "830830"
  },
  {
    "text": "Mixture model in a sense that now your model is",
    "start": "830830",
    "end": "833490"
  },
  {
    "text": "a weighted combination of a previous layer and the current layer message.",
    "start": "833490",
    "end": "837720"
  },
  {
    "text": "In this way basically means that you- you have now mixing",
    "start": "837720",
    "end": "840720"
  },
  {
    "text": "together two different layers or two different, uh, models.",
    "start": "840720",
    "end": "844769"
  },
  {
    "text": "Um, there is a lot of skip connections to add, right?",
    "start": "844770",
    "end": "849240"
  },
  {
    "text": "If you, um- if you have, let's say,",
    "start": "849240",
    "end": "852060"
  },
  {
    "text": "n skip connections, then there is 2 to the n possible message-passing paths,",
    "start": "852060",
    "end": "857745"
  },
  {
    "text": "which really allows you to increase the expressive power and gives neural network,",
    "start": "857745",
    "end": "863895"
  },
  {
    "text": "uh, more flexibility in terms of how",
    "start": "863895",
    "end": "866070"
  },
  {
    "text": "messages are passed and how messages are, uh, aggregated.",
    "start": "866070",
    "end": "869955"
  },
  {
    "text": "Uh, and, uh, first right, um,",
    "start": "869955",
    "end": "873915"
  },
  {
    "text": "we can also automatically get a mixture of automa- uh, shallow GNNs,",
    "start": "873915",
    "end": "878730"
  },
  {
    "text": "as well as the deep GNNs through the,",
    "start": "878730",
    "end": "881480"
  },
  {
    "text": "uh, message-passing layers, right?",
    "start": "881480",
    "end": "883639"
  },
  {
    "text": "So basically what I mean by this is you could have a three-layer,",
    "start": "883640",
    "end": "886160"
  },
  {
    "text": "uh, neural network and then by adding skip connections,",
    "start": "886160",
    "end": "889504"
  },
  {
    "text": "you can basically now have the final output to",
    "start": "889505",
    "end": "892220"
  },
  {
    "text": "be some combination of a one-layer neural network,",
    "start": "892220",
    "end": "895430"
  },
  {
    "text": "a two-layer neural network,",
    "start": "895430",
    "end": "897095"
  },
  {
    "text": "A one-layer neural network fed into the, uh,",
    "start": "897095",
    "end": "899795"
  },
  {
    "text": "third layer of the neural network and all this aggregated,",
    "start": "899795",
    "end": "902779"
  },
  {
    "text": "um, into the final output.",
    "start": "902780",
    "end": "904520"
  },
  {
    "text": "So now you can think that the final output is a- is a- is a combination of, uh,",
    "start": "904520",
    "end": "909750"
  },
  {
    "text": "in this case, four different, uh,",
    "start": "909750",
    "end": "912225"
  },
  {
    "text": "neural network, uh, architectures, right?",
    "start": "912225",
    "end": "915464"
  },
  {
    "text": "And, you know, the way you can- you can think of it is to say, oh,",
    "start": "915465",
    "end": "918150"
  },
  {
    "text": "I have three layers,",
    "start": "918150",
    "end": "919305"
  },
  {
    "text": "I add these three skip connections.",
    "start": "919305",
    "end": "921600"
  },
  {
    "text": "What this really does is,",
    "start": "921600",
    "end": "923235"
  },
  {
    "text": "you can think of it now that you have",
    "start": "923235",
    "end": "924870"
  },
  {
    "text": "four different neural network architectures that you are mixing or adding together,",
    "start": "924870",
    "end": "930480"
  },
  {
    "text": "uh, for, uh- during learning.",
    "start": "930480",
    "end": "932894"
  },
  {
    "text": "So it's a very efficient representation that really you can think of it in this,",
    "start": "932895",
    "end": "938220"
  },
  {
    "text": "um- in this second way.",
    "start": "938220",
    "end": "940319"
  },
  {
    "text": "So how would you apply skip connections,",
    "start": "940320",
    "end": "943620"
  },
  {
    "text": "uh, to graph neural networks?",
    "start": "943620",
    "end": "945420"
  },
  {
    "text": "For example, if we take a classical, uh,",
    "start": "945420",
    "end": "948690"
  },
  {
    "text": "graph convolutional neural network, uh,",
    "start": "948690",
    "end": "951270"
  },
  {
    "text": "architecture and add skip connections to it,",
    "start": "951270",
    "end": "954120"
  },
  {
    "text": "uh, this is how it would look like.",
    "start": "954120",
    "end": "955980"
  },
  {
    "text": "Before in the standard GNN layer- GCN layer,",
    "start": "955980",
    "end": "959925"
  },
  {
    "text": "we take the messages from the neighbors, uh,",
    "start": "959925",
    "end": "962490"
  },
  {
    "text": "transform them, and, uh,",
    "start": "962490",
    "end": "964470"
  },
  {
    "text": "average them together and we can think of this as our f of x from the previous slide.",
    "start": "964470",
    "end": "969509"
  },
  {
    "text": "So a GCN layer with a skip connection would be the same f of x plus,",
    "start": "969510",
    "end": "975090"
  },
  {
    "text": "uh, the lay- the message, uh,",
    "start": "975090",
    "end": "977250"
  },
  {
    "text": "from the previous, uh, layer, right?",
    "start": "977250",
    "end": "979620"
  },
  {
    "text": "So here we are just adding in the message from the previous layer and then",
    "start": "979620",
    "end": "982950"
  },
  {
    "text": "passing it all through the, um, non-linearity.",
    "start": "982950",
    "end": "986715"
  },
  {
    "text": "So this is- what this means is we added",
    "start": "986715",
    "end": "988830"
  },
  {
    "text": "the skip connection here in a single layer through this,",
    "start": "988830",
    "end": "992280"
  },
  {
    "text": "uh, blue part, uh, here.",
    "start": "992280",
    "end": "994590"
  },
  {
    "text": "And of course, um,",
    "start": "994590",
    "end": "996915"
  },
  {
    "text": "we have many other options for skip connections.",
    "start": "996915",
    "end": "1000035"
  },
  {
    "text": "We can make them skip one layer,",
    "start": "1000035",
    "end": "1002149"
  },
  {
    "text": "we can make them skip, uh, multiple layers.",
    "start": "1002150",
    "end": "1005480"
  },
  {
    "text": "Um, there is a- there is an interesting paper, uh,",
    "start": "1005480",
    "end": "1008750"
  },
  {
    "text": "from ICML called Jumping Knowledge Networks, where for example,",
    "start": "1008750",
    "end": "1013100"
  },
  {
    "text": "the proposal is to add these skips from a given layer",
    "start": "1013100",
    "end": "1017074"
  },
  {
    "text": "all the way to the- to the last layer where you basically now can think,",
    "start": "1017075",
    "end": "1021095"
  },
  {
    "text": "Oh, I have a o- one-layer neural network,",
    "start": "1021095",
    "end": "1023689"
  },
  {
    "text": "I have a two-layer neural network,",
    "start": "1023690",
    "end": "1025339"
  },
  {
    "text": "I have a three-layer neural network.",
    "start": "1025340",
    "end": "1027439"
  },
  {
    "text": "I take their inputs and,",
    "start": "1027440",
    "end": "1029480"
  },
  {
    "text": "uh- and then aggregate them to get the final, uh, output.",
    "start": "1029480",
    "end": "1033140"
  },
  {
    "text": "So by basically directly skipping to the- to the last- to the finals- final layer, um,",
    "start": "1033140",
    "end": "1038584"
  },
  {
    "text": "the final layer can then aggregate all these embeddings from",
    "start": "1038585",
    "end": "1042274"
  },
  {
    "text": "neural networks of different depths and this way basically,",
    "start": "1042275",
    "end": "1046339"
  },
  {
    "text": "uh, determine what information is more important,",
    "start": "1046340",
    "end": "1050315"
  },
  {
    "text": "uh, for, let's say for the prediction task.",
    "start": "1050315",
    "end": "1052684"
  },
  {
    "text": "Is it the information from very close by",
    "start": "1052685",
    "end": "1055550"
  },
  {
    "text": "nearby nodes or is it really about aggregating bigger parts of the network?",
    "start": "1055550",
    "end": "1059840"
  },
  {
    "text": "And by adding these skip connections,",
    "start": "1059840",
    "end": "1062210"
  },
  {
    "text": "basically this allows us to weigh or to determine what information is more important,",
    "start": "1062210",
    "end": "1067190"
  },
  {
    "text": "something that has been aggregated across multiple hops, or something that has been,",
    "start": "1067190",
    "end": "1071059"
  },
  {
    "text": "let say, aggregated over zero hops or over only a single hop?",
    "start": "1071060",
    "end": "1076085"
  },
  {
    "text": "Um, and that is very interesting and again,",
    "start": "1076085",
    "end": "1078455"
  },
  {
    "text": "adds to expressivity, uh, and,",
    "start": "1078455",
    "end": "1080899"
  },
  {
    "text": "uh, improves the performance,",
    "start": "1080900",
    "end": "1083135"
  },
  {
    "text": "uh, of graph neural, uh, networks.",
    "start": "1083135",
    "end": "1086309"
  }
]