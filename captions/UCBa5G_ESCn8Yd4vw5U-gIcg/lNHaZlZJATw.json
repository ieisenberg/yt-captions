[
  {
    "text": "All right. Let's- let's jump right in then. Um, quick recap.",
    "start": "4580",
    "end": "10230"
  },
  {
    "text": "So we basically just reviewed prerequisites for the course. Um, the hope is, um,",
    "start": "10230",
    "end": "17285"
  },
  {
    "text": "ev- all of you are now more or less on the same page in terms of, uh, prerequisites. Um, also, the pre-requi- the- the- the kind of material that",
    "start": "17285",
    "end": "25130"
  },
  {
    "text": "we covered in the pre-requisites gives you a flavor of, you know, the kind of, um, mathematical background you are expected to have and the kind of,",
    "start": "25130",
    "end": "32340"
  },
  {
    "text": "you know, problems that you'll be solving in the homework. So hopefully that's- that's informative for you in terms of,",
    "start": "32340",
    "end": "38080"
  },
  {
    "text": "um, um, expecting what to come- uh, wh- what comes in the rest of the course. So we covered linear algebra, um,",
    "start": "38080",
    "end": "45170"
  },
  {
    "text": "and we- we went over some matrix calculus, uh, matrix calculus, uh, mostly bec- uh, uh,",
    "start": "45170",
    "end": "50660"
  },
  {
    "text": "we- we are- we are interested in- in, you know, for example, uh, taking derivatives of a scalar-valued function with respect to a vector-valued input.",
    "start": "50660",
    "end": "59050"
  },
  {
    "text": "you know, that's the most common thing we'll be doing, um, uh, in this course. Um, either vector-valued input or matrix-valued input.",
    "start": "59050",
    "end": "66360"
  },
  {
    "text": "But mo- most of the times it's a scalar-valued output, which is gonna be your loss function. Uh, we're gonna cover that today.",
    "start": "66360",
    "end": "72650"
  },
  {
    "text": "Um, we also covered, um, probability theory- some- some, uh, parts of probability theory.",
    "start": "72650",
    "end": "79525"
  },
  {
    "text": "Um, and then, um, on Friday we- we touched upon maximum likelihood expectation.",
    "start": "79525",
    "end": "86405"
  },
  {
    "text": "That's, you know, uh, just- just, uh, a small part of mathematical statistics that's kind of, uh,",
    "start": "86405",
    "end": "91850"
  },
  {
    "text": "relevant to, um, o- o- or that part that's most relevant to this course.",
    "start": "91850",
    "end": "97240"
  },
  {
    "text": "And, um, we- we saw the example of how you can- how you can use",
    "start": "97240",
    "end": "102560"
  },
  {
    "text": "maximum likelihood estimation to estimate the parameters of a- a Gaussian- uh, a multivariate Gaussian.",
    "start": "102560",
    "end": "108619"
  },
  {
    "text": "And we saw that, um, what- what- um, the estimators that we obtained for the mean and",
    "start": "108620",
    "end": "115700"
  },
  {
    "text": "covariance happened to be very intuitive ones, right? For example, the mean- the- the mean estimator turned out to be",
    "start": "115700",
    "end": "121820"
  },
  {
    "text": "just the average of the given, you know, uh, x's. And the covariance happens to be the sample covariance of- of the, uh, given inputs.",
    "start": "121820",
    "end": "129399"
  },
  {
    "text": "So, um, it's- it's nice to see that, um, you know, the intuitive definitions happen to,",
    "start": "129400",
    "end": "135360"
  },
  {
    "text": "you know, be well-grounded in some solid theory. So today we're gonna switch gears and start supervised learning.",
    "start": "135360",
    "end": "143540"
  },
  {
    "text": "Supervised learning is- basically, um, deals with problems where we are trying to learn mapping",
    "start": "143540",
    "end": "149920"
  },
  {
    "text": "from some input x to some output y, right? And the input x and y are- could be anything.",
    "start": "149920",
    "end": "158370"
  },
  {
    "text": "Um, most of the times, um, the in- uh, we are gonna be dealing with",
    "start": "158370",
    "end": "164450"
  },
  {
    "text": "problems that are either regression problems or classification problems. Those are, like, the most common kind of,",
    "start": "164450",
    "end": "170030"
  },
  {
    "text": "uh, machine learning, uh, problems. And depending on the problem, for example, if it is regression,",
    "start": "170030",
    "end": "176880"
  },
  {
    "text": "[NOISE] right, and let's say we're trying to predict,",
    "start": "176880",
    "end": "183960"
  },
  {
    "text": "um, the price of, say, um, houses.",
    "start": "183960",
    "end": "187870"
  },
  {
    "text": "And you're given the, um, living area, right?",
    "start": "192500",
    "end": "199650"
  },
  {
    "text": "And this is the price. Okay? So you may have- right,",
    "start": "199650",
    "end": "206055"
  },
  {
    "text": "let's call this in- in- in square feet. [NOISE]",
    "start": "206055",
    "end": "233160"
  },
  {
    "text": "So let's say, um, you're given a dataset that has mappings x and y, or pairs of x and y,",
    "start": "233160",
    "end": "239345"
  },
  {
    "text": "where x is- um, where each example denotes, uh, one- one house.",
    "start": "239345",
    "end": "245580"
  },
  {
    "text": "And the x of that row is the- is the, um, living area in- in some unit.",
    "start": "245580",
    "end": "252095"
  },
  {
    "text": "And the y of that row is the price in some unit, right? And given this dataset,",
    "start": "252095",
    "end": "257890"
  },
  {
    "text": "um, we could, for example, plot it like this.",
    "start": "257890",
    "end": "261070"
  },
  {
    "text": "So a unit here, square feet, and the unit here is $1,000. And you may get, [NOISE] right?",
    "start": "270590",
    "end": "281100"
  },
  {
    "text": "So, uh, what do we have here? So here, uh, we have a plot where each dot represents one row.",
    "start": "281100",
    "end": "289190"
  },
  {
    "text": "Right? And the x coordinate of the dot is the x value, and the y coordinate of the dot is the corresponding y value.",
    "start": "289190",
    "end": "296050"
  },
  {
    "text": "And this i- this is just a scatter plot of the, uh, housing prices dataset, right?",
    "start": "296050",
    "end": "301800"
  },
  {
    "text": "And the goal here is to learn some function, let's call it, uh, a hypothesis.",
    "start": "301800",
    "end": "309350"
  },
  {
    "text": "The reason it's called hypothesis is- is not very crucial. So if you wanna learn a hypothesis,",
    "start": "309450",
    "end": "317375"
  },
  {
    "text": "we'll call it h of x, which is some function of x that is similar to or- or,",
    "start": "317375",
    "end": "324080"
  },
  {
    "text": "um, that's a- that- that gives you an output that is as close as possible to y, right?",
    "start": "324080",
    "end": "329750"
  },
  {
    "text": "That's the, um, um, that's the goal of a regression problem, right? And in- there are some other cases where what you're trying to learn is,",
    "start": "329750",
    "end": "339395"
  },
  {
    "text": "um, a classification problem, where [NOISE],",
    "start": "339395",
    "end": "354259"
  },
  {
    "text": "right, and here, x_1 and x_2.",
    "start": "354260",
    "end": "361100"
  },
  {
    "text": "So here this was x, and this was y; x was scalar value.",
    "start": "361100",
    "end": "366789"
  },
  {
    "text": "But imagine a different problem where your x's have two- have- have two attributes instead of just one attribute.",
    "start": "366790",
    "end": "376340"
  },
  {
    "text": "Let's say there was, um, a second attribute. Uh, you could call it- you could call it, um, um,",
    "start": "376340",
    "end": "383979"
  },
  {
    "text": "for example, number of bedrooms, right?",
    "start": "383980",
    "end": "389235"
  },
  {
    "text": "And in- and- and there's going to be a corresponding- um, maybe, let me just,",
    "start": "389235",
    "end": "395340"
  },
  {
    "text": "um, write this differently. Let's say you have a different dataset where you have x_1, x_2, and y.",
    "start": "395340",
    "end": "407150"
  },
  {
    "text": "And each example, again, is one row. And your y's are 0s and 1s.",
    "start": "407150",
    "end": "413020"
  },
  {
    "text": "And your x would be some values, where the 0 and 1 tell you what class the example belongs to.",
    "start": "413020",
    "end": "421009"
  },
  {
    "text": "And our goal is to learn some kind of a classifier, which is, you know,",
    "start": "421010",
    "end": "428180"
  },
  {
    "text": "some hyperplane that divides your x's into two parts. And the- the, uh, the goal is to learn",
    "start": "428180",
    "end": "435185"
  },
  {
    "text": "a suitable hyperplane such that most of the positive examples are on one side, and most of the negative examples are on the other side, right?",
    "start": "435185",
    "end": "443085"
  },
  {
    "text": "These are two different kinds of supervised learning problems. Um, and we call it supervised because, with each example,",
    "start": "443085",
    "end": "450530"
  },
  {
    "text": "there is a corresponding y that's given to us, which is like- uh, that's the supervision signal where it tells you what",
    "start": "450530",
    "end": "456920"
  },
  {
    "text": "the right answer is for each given example, right? And for the f- probably the first third or first half of the course,",
    "start": "456920",
    "end": "463550"
  },
  {
    "text": "we're gonna be focusing on supervised learning problems, and the two most common problems you're gonna be",
    "start": "463550",
    "end": "468650"
  },
  {
    "text": "looking at are regression and classification. [NOISE] Right?",
    "start": "468650",
    "end": "476430"
  },
  {
    "text": "And to set up the basic terminology, we're gonna stick with this terminology.",
    "start": "476430",
    "end": "481690"
  },
  {
    "text": "In fact, throughout the course, um, in supervised settings,",
    "start": "481690",
    "end": "486895"
  },
  {
    "text": "we're going to call the inputs as x and the outputs as y. N is going to be the number of examples in our training set, right?",
    "start": "486895",
    "end": "496510"
  },
  {
    "text": "You're given a training set from which we want to learn some, uh, uh, hypotheses. And the number of examples, um,",
    "start": "496510",
    "end": "502854"
  },
  {
    "text": "in the training set is- is going to be n in all your homeworks and- and- and all, uh, through the rest of the course.",
    "start": "502855",
    "end": "508765"
  },
  {
    "text": "And we're gonna call the- a given pair x, y as an example in a supervised learning training set.",
    "start": "508765",
    "end": "515065"
  },
  {
    "text": "Now, d is going to be the number of dimensions of our input. In this case d was 1,",
    "start": "515065",
    "end": "521485"
  },
  {
    "text": "in this case d is 2, right? But d could be, you know, an arbitrarily large number,",
    "start": "521485",
    "end": "528080"
  },
  {
    "text": "um, of- of, uh, dimensions of the input. And we're gonna use superscript i with",
    "start": "528080",
    "end": "535585"
  },
  {
    "text": "a parenthesis to indicate that it is the i^th example. So x^i would be the x_1,",
    "start": "535585",
    "end": "544755"
  },
  {
    "text": "you know, uh, uh, um, x_1, x_2 of the i^th example. And y^i will be the corresponding- uh,",
    "start": "544755",
    "end": "556680"
  },
  {
    "text": "uh, uh, y^i will be the corresponding label. So we- we- we generally call the y's as labels or ground truth.",
    "start": "556680",
    "end": "565380"
  },
  {
    "text": "[NOISE] And x will be called, uh, your input.",
    "start": "565380",
    "end": "570840"
  },
  {
    "text": "So y is the example output, or you can call it label or ground truth, right?",
    "start": "570840",
    "end": "582945"
  },
  {
    "text": "And, um, when given a pair, x^i, y^i together, they form the i^th example- i^th super- uh,",
    "start": "582945",
    "end": "588090"
  },
  {
    "text": "uh, supervised, uh, learning example. Any questions about this terminology? Okay. Not to confuse, whenever there is a parenthesis, you know,",
    "start": "588090",
    "end": "595415"
  },
  {
    "text": "it means we are not taking the i^th power of x or the i^th power of y. It means, you know, it's just the i^th example, right?",
    "start": "595415",
    "end": "603210"
  },
  {
    "text": "And in the case of a regression problem, y^i will be a real-value number, right?",
    "start": "603210",
    "end": "611685"
  },
  {
    "text": "And in case of classification, y^i will be- y^i will be in 0 or 1 if it is binary classification.",
    "start": "611685",
    "end": "622510"
  },
  {
    "text": "If it's a multi-class classification, then y^i will be, you know- uh, it can take more number of discrete values.",
    "start": "622510",
    "end": "629735"
  },
  {
    "text": "Right. So this- this is, you know, uh, to set up the, uh, terminology- uh, any- any questions about the terminology?",
    "start": "629735",
    "end": "637654"
  },
  {
    "text": "If not, let's- let's- um, [NOISE] let's jump into supervised learning.",
    "start": "637655",
    "end": "643920"
  },
  {
    "text": "[NOISE]",
    "start": "643920",
    "end": "653690"
  },
  {
    "text": "All right. So the big picture of supervised learning, we saw this picture, uh, is you're given a training set.",
    "start": "653690",
    "end": "662180"
  },
  {
    "text": "Training set is the set of x^i, y^i pairs,",
    "start": "664440",
    "end": "671425"
  },
  {
    "text": "where i is from 1 to n. All right, this  means,",
    "start": "671425",
    "end": "677380"
  },
  {
    "text": "uh, a set of x^i, y^i pairs, n, n of them.",
    "start": "677380",
    "end": "682615"
  },
  {
    "text": "And using this training set, we wanna run it through some learning algorithm.",
    "start": "682615",
    "end": "690860"
  },
  {
    "text": "Now, the, the specific algorithm that we choose will depend on what y^i is.",
    "start": "694380",
    "end": "700450"
  },
  {
    "text": "For example, if it's real value, then it has to be a regression learning algorithm. If it's discrete value,",
    "start": "700450",
    "end": "705850"
  },
  {
    "text": "then it has to be a, uh, um, a classification algorithm. And the output of the learning algorithm is our hypothesis or",
    "start": "705850",
    "end": "714010"
  },
  {
    "text": "the learned hypothesis, right?",
    "start": "714010",
    "end": "722425"
  },
  {
    "text": "So this- this, um, h, uh, h of x is gonna be the output of the learning algorithm.",
    "start": "722425",
    "end": "727765"
  },
  {
    "text": "And this, this output or this model that we call are the learnt model, let's call this the learnt model,",
    "start": "727765",
    "end": "734140"
  },
  {
    "text": "can now take in new x's.",
    "start": "734140",
    "end": "741910"
  },
  {
    "text": "Let's call this our x_test, right, and output the corresponding y hat.",
    "start": "741910",
    "end": "750145"
  },
  {
    "text": "Okay. So, uh, this is the- the, uh, the big picture of supervised learning.",
    "start": "750145",
    "end": "758575"
  },
  {
    "text": "And all the algorithms that we're gonna study are gonna follow this pattern, right?",
    "start": "758575",
    "end": "763780"
  },
  {
    "text": "So we start with the training set. We will run it through the learning algorithm.",
    "start": "763780",
    "end": "768805"
  },
  {
    "text": "We obtain a model. And the model can now take examples from what we call as the test set. Or, you know, these could be examples that,",
    "start": "768805",
    "end": "776259"
  },
  {
    "text": "that are fed into the model when you actually deploy the model in production. And the, the, the goal here is",
    "start": "776260",
    "end": "785130"
  },
  {
    "text": "that even though we are learning from a fixed set of examples, we hope to generalize well to new examples that we've not seen before.",
    "start": "785130",
    "end": "793125"
  },
  {
    "text": "So x_test in general will be an example that your model has never seen before. And we hope to, um,",
    "start": "793125",
    "end": "800410"
  },
  {
    "text": "we hope that the algorithm that we've learned or the model that we've learned will output y's that are,",
    "start": "800410",
    "end": "806815"
  },
  {
    "text": "you know, kind of correct in some way.",
    "start": "806815",
    "end": "809510"
  },
  {
    "text": "So with that, uh, let's start with our fi- very first learning algorithm, linear regression.",
    "start": "813720",
    "end": "822800"
  },
  {
    "text": "So in linear regression, x is an R^d and",
    "start": "827250",
    "end": "834865"
  },
  {
    "text": "y is an R, right? And that's the training set that we are given,",
    "start": "834865",
    "end": "843505"
  },
  {
    "text": "n, n such things, uh, n such examples, right?",
    "start": "843505",
    "end": "853210"
  },
  {
    "text": "And now we wanna learn, uh, a hy- a hypothesis which belongs to a family.",
    "start": "853210",
    "end": "860350"
  },
  {
    "text": "What do we mean by that? So in this, in this setting, we didn't- we did not impose any kind of restriction on what h can be.",
    "start": "860350",
    "end": "869410"
  },
  {
    "text": "It could be any function whatsoever that takes x as an input and produces some real- some,",
    "start": "869410",
    "end": "874975"
  },
  {
    "text": "some, uh, some y. However, that is a very broad class of algorithms,",
    "start": "874975",
    "end": "882565"
  },
  {
    "text": "a very bro- broad class of hypotheses. And we wanna limit the, the family of hypotheses over which we are gonna learn in some way.",
    "start": "882565",
    "end": "892210"
  },
  {
    "text": "And the most simplest form of, uh, such a hypothesis is called a linear hypothesis.",
    "start": "892210",
    "end": "899079"
  },
  {
    "text": "And we're gonna write it in this way, h_Theta of x is equal to x naught plus Theta_1 x_1",
    "start": "899080",
    "end": "911619"
  },
  {
    "text": "plus Theta_2 x_2 plus Theta_d",
    "start": "911619",
    "end": "924115"
  },
  {
    "text": "x_d, right? Uh, what's happening here?",
    "start": "924115",
    "end": "930325"
  },
  {
    "text": "So the hypothesis that we wanna learn has some parameter Theta.",
    "start": "930325",
    "end": "937555"
  },
  {
    "text": "And Theta is in this case. But before I tell what, uh, Theta is,",
    "start": "937555",
    "end": "946150"
  },
  {
    "text": "uh, this should be Theta naught.",
    "start": "946150",
    "end": "952070"
  },
  {
    "text": "So, uh, the hypothesis that we learn comes with,",
    "start": "953310",
    "end": "959755"
  },
  {
    "text": "uh, a Theta vector that has d plus 1 components, right?",
    "start": "959755",
    "end": "965845"
  },
  {
    "text": "1 through d corresponding to the x's and, and, uh, uh, another Theta naught. So Theta naught or Theta is in R^d plus 1.",
    "start": "965845",
    "end": "978009"
  },
  {
    "text": "This is the extra, uh, plus 1 that we get. And the goal of linear regression is to learn a suitable set of parameters Theta that,",
    "start": "978010",
    "end": "990715"
  },
  {
    "text": "that make, uh, y value as close as possible to h_Theta of x, right?",
    "start": "990715",
    "end": "997570"
  },
  {
    "text": "That's the, that's the, uh, that's the goal of, of, uh, learning algorithm.",
    "start": "997570",
    "end": "1002625"
  },
  {
    "text": "And this we can also write h_Theta of x is equal to summation of i",
    "start": "1002625",
    "end": "1011955"
  },
  {
    "text": "equals 1 to d Theta_i x_i",
    "start": "1011955",
    "end": "1021280"
  },
  {
    "text": "plus Theta naught, right? And we will adopt the convention that we will add a new column to our x's.",
    "start": "1024260",
    "end": "1035220"
  },
  {
    "text": "Let's call it x naught. And that will be equal to one for all the examples, right?",
    "start": "1035220",
    "end": "1042704"
  },
  {
    "text": "And which means I can- we can now write h_Theta of x to be",
    "start": "1042705",
    "end": "1050174"
  },
  {
    "text": "Theta naught x naught plus Theta_1 x_1 plus Theta_d x_d.",
    "start": "1050175",
    "end": "1061785"
  },
  {
    "text": "Okay. And this is also easier to the i equals this time from 0",
    "start": "1061785",
    "end": "1067050"
  },
  {
    "text": "to d, theta_i x_i, right?",
    "start": "1067050",
    "end": "1073725"
  },
  {
    "text": "And x naught is greater than theta transpose x, right?",
    "start": "1073725",
    "end": "1080505"
  },
  {
    "text": "This, this additional term that we include in all of our examples where we just set it to 1 is also called the intercept term.",
    "start": "1080505",
    "end": "1091540"
  },
  {
    "text": "And this is mostly just for notational convenience, right? Uh, there's, there's, you know,",
    "start": "1094880",
    "end": "1101220"
  },
  {
    "text": "there is absolutely no difference between this version and this version, um, except for notation.",
    "start": "1101220",
    "end": "1107640"
  },
  {
    "text": "Here we have an annoying extra additive term, here there is no annoying extra additive term.",
    "start": "1107640",
    "end": "1113355"
  },
  {
    "text": "That's about, you know, it's just notational difference, right?",
    "start": "1113355",
    "end": "1123525"
  },
  {
    "text": "Now, given this, this, this family of hypotheses that we want to limit ourselves to,",
    "start": "1123525",
    "end": "1132674"
  },
  {
    "text": "where the specific member of the family is decided by the specific Theta vector that is, that is used,",
    "start": "1132675",
    "end": "1142080"
  },
  {
    "text": "we are going to define something called as a cost function,",
    "start": "1142080",
    "end": "1146260"
  },
  {
    "text": "or it's also sometimes called the loss function, [NOISE] right?",
    "start": "1150770",
    "end": "1155590"
  },
  {
    "text": "And in this cost function or loss function, we want to capture the amount of",
    "start": "1157850",
    "end": "1165299"
  },
  {
    "text": "displeasure a specific hypothesis causes to us in some way, right?",
    "start": "1165300",
    "end": "1170625"
  },
  {
    "text": "And a common, uh, a very commonly used cost function for, uh,",
    "start": "1170625",
    "end": "1176270"
  },
  {
    "text": "regression problems is called the, uh, uh, squared error.",
    "start": "1176270",
    "end": "1183350"
  },
  {
    "text": "And we're gonna define it like this. So all the cost functions or loss functions, uh, in this course will be called J, right?",
    "start": "1183350",
    "end": "1191640"
  },
  {
    "text": "And we call it a cost function when we want it to be small, we- the, the desired output for a cost function should be,",
    "start": "1191640",
    "end": "1200385"
  },
  {
    "text": "you know, small, right? And this is gonna be defined as half of 1 equals 1 to n, right?",
    "start": "1200385",
    "end": "1211240"
  },
  {
    "text": "So what's happening here? We have n training examples, i from one to n, and for each example,",
    "start": "1229580",
    "end": "1238059"
  },
  {
    "text": "we take the hypotheses that- that we obtain by some given Theta,",
    "start": "1238550",
    "end": "1246240"
  },
  {
    "text": "that is an input to the cost function, calculate what that hypothesis will,",
    "start": "1246240",
    "end": "1251789"
  },
  {
    "text": "uh, what output that hypothesis, uh, will output, and calculate the squared error between the correct answer",
    "start": "1251790",
    "end": "1259995"
  },
  {
    "text": "and the output of that hypothesis and square it, right? This is also called the squared error.",
    "start": "1259995",
    "end": "1266940"
  },
  {
    "text": "Um, and this is a very commonly used, uh, loss function.",
    "start": "1266940",
    "end": "1272279"
  },
  {
    "text": "And for different values of Theta, the cost function will evaluate to",
    "start": "1272280",
    "end": "1278055"
  },
  {
    "text": "different- different values for the given, uh, training set. Yes, question. [OVERLAPPING] Why do we have a half here?",
    "start": "1278055",
    "end": "1288195"
  },
  {
    "text": "Good question. I'll- I'll come to that shortly. Um, the- the- the thing to, uh, uh,",
    "start": "1288195",
    "end": "1295710"
  },
  {
    "text": "observe here is that for the- from the cost functions point of view, Theta is the only variable, right?",
    "start": "1295710",
    "end": "1302940"
  },
  {
    "text": "The training set that we have given is- is kind of embedded in this function. It's fixed, right?",
    "start": "1302940",
    "end": "1309270"
  },
  {
    "text": "And if you- if you obtain a different set of training, uh, a different training set of- of having, um, uh,",
    "start": "1309270",
    "end": "1316785"
  },
  {
    "text": "different, uh, features, and- and labels, the cost function is going to be different.",
    "start": "1316785",
    "end": "1321870"
  },
  {
    "text": "So the thing that makes one cost function different from another is one, of course, the functional form,",
    "start": "1321870",
    "end": "1327540"
  },
  {
    "text": "and also the training set it- itself. So the training set- the training data that we have is",
    "start": "1327540",
    "end": "1332730"
  },
  {
    "text": "kind of embedded into the cost function, right? That's- that's, uh, that's something you wanna, um, keep in mind.",
    "start": "1332730",
    "end": "1338940"
  },
  {
    "text": "Okay? And now the goal is to find Theta hat,",
    "start": "1338940",
    "end": "1344924"
  },
  {
    "text": "which is- which somehow minimizes the cost function, right?",
    "start": "1344925",
    "end": "1350830"
  },
  {
    "text": "We have Theta, and now, this is Theta i equals 1 to n,",
    "start": "1355190",
    "end": "1367810"
  },
  {
    "text": "h Theta of x^i, y^i squared.",
    "start": "1368090",
    "end": "1381914"
  },
  {
    "text": "Right. So we want to find that Theta that minimizes the cost function to the smallest value possible, right?",
    "start": "1381915",
    "end": "1392070"
  },
  {
    "text": "Any- any- any questions so far? Yes. Should we be dividing it by the number of examples to normalize this?",
    "start": "1392070",
    "end": "1398294"
  },
  {
    "text": "Sure. So the question is, should we be dividing it by the number of examples to,",
    "start": "1398295",
    "end": "1403650"
  },
  {
    "text": "um, um, to normalize this? So if our goal is to find, uh,",
    "start": "1403650",
    "end": "1409890"
  },
  {
    "text": "the lowest possible cost itself, or the loss value itself, then yes.",
    "start": "1409890",
    "end": "1416160"
  },
  {
    "text": "Um, it- it matters a lot whether we normalize this by n or not, but the goal here is to find the arg min, which is, you know,",
    "start": "1416160",
    "end": "1423870"
  },
  {
    "text": "um, what is the value of Theta that minimizes this the most, right? And the value that minimizes- the value of Theta that",
    "start": "1423870",
    "end": "1430140"
  },
  {
    "text": "minimizes the cost the most is the same, whether- you know, the cost is this value,",
    "start": "1430140",
    "end": "1435450"
  },
  {
    "text": "or the cost is this times 1 over n, right? Er, er, you can take 1 over n, you know, outside,",
    "start": "1435450",
    "end": "1440460"
  },
  {
    "text": "and the value that Theta evaluates to will always be the same. So er, in terms of, um, if the interest is in finding the small Theta,",
    "start": "1440460",
    "end": "1448215"
  },
  {
    "text": "then it- it doesn't matter whether you normalize it by n or not. Good question.",
    "start": "1448215",
    "end": "1453400"
  },
  {
    "text": "Okay. Now, the question is now, how are we going to perform this minimization, right?",
    "start": "1454760",
    "end": "1460995"
  },
  {
    "text": "What's- what's- what, uh, what do we do to actually perform this minimization? Because this is just a mathematical expression.",
    "start": "1460995",
    "end": "1467100"
  },
  {
    "text": "This is not an algorithm, right? The algorithm will tell us how we are going to perform this minimization process, okay?",
    "start": "1467100",
    "end": "1474525"
  },
  {
    "text": "And this brings us to our first algorithm which we call as gradient descent.",
    "start": "1474525",
    "end": "1484090"
  },
  {
    "text": "Right? So to perform gradient descent,",
    "start": "1491690",
    "end": "1497340"
  },
  {
    "text": "um, let me- let me start with some pictures to,",
    "start": "1497340",
    "end": "1501190"
  },
  {
    "text": "um- okay? What I have drawn here are- so the two axis are x_1 to- let's call this x_d.",
    "start": "1530840",
    "end": "1540510"
  },
  {
    "text": "Imagine that our- you know, this is in a- in a d-dimensional space. Uh, it's easy to visualize it in a two-dimensional space, but,",
    "start": "1540510",
    "end": "1546825"
  },
  {
    "text": "you know, uh, it's- it's- it's representing a d-dimensional space, and what I have drawn here is the contour plot of",
    "start": "1546825",
    "end": "1554370"
  },
  {
    "text": "the cost function, right?",
    "start": "1554370",
    "end": "1563610"
  },
  {
    "text": "Now, um, this plot is fundamentally different from the two plots we saw over there,",
    "start": "1563610",
    "end": "1569370"
  },
  {
    "text": "or there the axis was data, and here the axis is the parameters, right?",
    "start": "1569370",
    "end": "1574950"
  },
  {
    "text": "A very different plot, right? We have the- the, uh, the axis, uh,",
    "start": "1574950",
    "end": "1581640"
  },
  {
    "text": "to be the parameters, and this is the contour plot of- of the cost function. What's the contour plot? Yes.",
    "start": "1581640",
    "end": "1588570"
  },
  {
    "text": "Shouldn't the y-axis be j, of Theta, and the Theta 1, and Theta db be be kind of on the x-axis",
    "start": "1588570",
    "end": "1595860"
  },
  {
    "text": "So good question. Good question. So, um, the question is, shouldn't the, uh, y-axis be j of Theta,",
    "start": "1595860",
    "end": "1602250"
  },
  {
    "text": "and, uh, the Theta 1 and- and 2 Theta to be- be- be kind of, uh, uh, on the x-axis.",
    "start": "1602250",
    "end": "1607830"
  },
  {
    "text": "Um, that- so, um, the answer is, this is a contour plot, right?",
    "start": "1607830",
    "end": "1613274"
  },
  {
    "text": "In a contour plot, what we- what we do is, we trace out the set of all- the set of all,",
    "start": "1613275",
    "end": "1622575"
  },
  {
    "text": "uh, uh points in- in Theta, which output a specific value of J Theta.",
    "start": "1622575",
    "end": "1627960"
  },
  {
    "text": "So this corresponds to J of theta equals 1,",
    "start": "1627960",
    "end": "1633465"
  },
  {
    "text": "and this corresponds to J of Theta equals 2,",
    "start": "1633465",
    "end": "1638880"
  },
  {
    "text": "and this corresponds to J of Theta equals 3. Okay? It's- it's a very different way of- of- of looking at- at,",
    "start": "1638880",
    "end": "1648690"
  },
  {
    "text": "um, of- of- of plotting things, right? The- the value of J Theta is not visible here,",
    "start": "1648690",
    "end": "1656025"
  },
  {
    "text": "and it is- it's kind of implicit in the shape of the contours that we draw, right?",
    "start": "1656025",
    "end": "1662355"
  },
  {
    "text": "Now, uh, a few things that you can observe is that, uh, what can we say about the shape of J theta here? Okay.",
    "start": "1662355",
    "end": "1670515"
  },
  {
    "text": "Is it like a boat shape? Yeah, it's like a boat shape where it is minimized at this value.",
    "start": "1670515",
    "end": "1678180"
  },
  {
    "text": "And as you- as you kind of move farther away in the parameter space from this value,",
    "start": "1678180",
    "end": "1684120"
  },
  {
    "text": "you are- the cost function evaluates to larger values, right? Is this clear? Yes, question.",
    "start": "1684120",
    "end": "1692115"
  },
  {
    "text": "Can you also not think of this as a dome shaped",
    "start": "1692115",
    "end": "1697770"
  },
  {
    "text": "So the question is, can you also not think of this as, uh, uh, dome shaped? Right. Um, you c- this,",
    "start": "1697770",
    "end": "1704174"
  },
  {
    "text": "this would also be the contour plot of- of a dome shaped, uh, function.",
    "start": "1704175",
    "end": "1709215"
  },
  {
    "text": "If the values w- we're in the other way. This is 3, this is 2, this is 1.",
    "start": "1709215",
    "end": "1715275"
  },
  {
    "text": "Then, uh, for these values of J Theta, then this would be a dome shape.",
    "start": "1715275",
    "end": "1720375"
  },
  {
    "text": "Right? Now, as we get closer to the center, the value J Theta- the value should become smaller and smaller.",
    "start": "1720375",
    "end": "1726210"
  },
  {
    "text": "So it kind of you know, comes down to, er, er, as- as we move to the center.",
    "start": "1726210",
    "end": "1731500"
  },
  {
    "text": "Good question. Now, the- the cost function that we described over here,",
    "start": "1731600",
    "end": "1741044"
  },
  {
    "text": "where Theta is in some, uh, d plus 1 dimension because of the extra intercept term, right?",
    "start": "1741045",
    "end": "1748019"
  },
  {
    "text": "So perhaps this is Theta d plus 1. Right? Now, the goal is to find the Theta that minimizes this cost function.",
    "start": "1748020",
    "end": "1758715"
  },
  {
    "text": "So assume this cost function takes the shape. It could be, you know, uh, uh, um, you know, it's a- some shape.",
    "start": "1758715",
    "end": "1763905"
  },
  {
    "text": "Um, and now we want to find a Theta that minimizes this cost function,",
    "start": "1763905",
    "end": "1769935"
  },
  {
    "text": "um, to the smallest possible value? Yes, question. How do we know that there's only one value of Theta that minimizes the cost function?",
    "start": "1769935",
    "end": "1778169"
  },
  {
    "text": "Very good question. So how do we know that, um, um, um, there's only one value of Theta that minimizes the cost function?",
    "start": "1778170",
    "end": "1786000"
  },
  {
    "text": "In general, it need not. Uh, if you, uh, the correct way to actually write this is,",
    "start": "1786000",
    "end": "1791985"
  },
  {
    "text": "you know, Theta belongs to argmin where argmin gives you a set of minimizers. There could be multiple values of Theta that minimize your,",
    "start": "1791985",
    "end": "1800580"
  },
  {
    "text": "uh, minimize your function. For example, um, imagine this to be the cost function, and your cost function has this shape.",
    "start": "1800580",
    "end": "1807629"
  },
  {
    "text": "So any Theta along this line is a minimizer. It's not a bowl-shaped, it's just, you know, um,",
    "start": "1807630",
    "end": "1813315"
  },
  {
    "text": "that- it's just that the minimum value can be obtained by multiple values of Theta along the line. In, in such cases argmin is actually a set of Theta values.",
    "start": "1813315",
    "end": "1822990"
  },
  {
    "text": "But for- for the most part we'll be assuming that there is one unique, uh, minimizer.",
    "start": "1822990",
    "end": "1828315"
  },
  {
    "text": "Good question. So, um, so r- our, um, our goal is to find the value of- of- of Theta where J theta is the smallest.",
    "start": "1828315",
    "end": "1839910"
  },
  {
    "text": "And for this, we're gonna use an algorithm called \"Gradient Descent.\" Now, how does gradient descent work?",
    "start": "1839910",
    "end": "1845555"
  },
  {
    "text": "So gradient descent, we start with a random initial value of Theta.",
    "start": "1845555",
    "end": "1853570"
  },
  {
    "text": "So we said, Theta- let's call it Theta-naught [NOISE] equal to,",
    "start": "1853570",
    "end": "1860940"
  },
  {
    "text": "you know, some initialization. A common initialization, is you just start with Theta-naught equal to 0,",
    "start": "1860940",
    "end": "1871155"
  },
  {
    "text": "at the origin, right? Theta-naught. Now, it's, uh,",
    "start": "1871155",
    "end": "1879420"
  },
  {
    "text": "again important not to confuse with the superscript I'm using here. If it is theta, then the superscript means the ith example, right?",
    "start": "1879420",
    "end": "1887490"
  },
  {
    "text": "Now, if you're running gradient descent, um, the superscript over here indicates the time-step.",
    "start": "1887490",
    "end": "1894330"
  },
  {
    "text": "After what iteration we are above the value of Theta that we get at, uh, a particular iteration, right?",
    "start": "1894330",
    "end": "1900825"
  },
  {
    "text": "So, uh, take Theta-naught, I know, initialize it to some value, it could be the origin.",
    "start": "1900825",
    "end": "1906195"
  },
  {
    "text": "And then, um, so first I'm going to write it,",
    "start": "1906195",
    "end": "1915375"
  },
  {
    "text": "um, in the partial, uh, partial derivative notation, and then, uh, in a gradient form.",
    "start": "1915375",
    "end": "1920520"
  },
  {
    "text": "So Theta J of",
    "start": "1920520",
    "end": "1925690"
  },
  {
    "text": "1 equal to Theta 0,",
    "start": "1925690",
    "end": "1932248"
  },
  {
    "text": "minus Alpha times J Theta-naught.",
    "start": "1932249",
    "end": "1943210"
  },
  {
    "text": "What does that mean? So VR at Theta naught equal to 0 or are- are- yeah- are- at Theta naught.",
    "start": "1947530",
    "end": "1958370"
  },
  {
    "text": "So first, what we do is we find the partial derivative of the function J with respect to some",
    "start": "1958370",
    "end": "1965450"
  },
  {
    "text": "coordinate j, right? J can take a value between 1 to d plus 1, right?",
    "start": "1965450",
    "end": "1973880"
  },
  {
    "text": "And we calculate the partial derivative of the loss function with respect to j.",
    "start": "1973880",
    "end": "1979085"
  },
  {
    "text": "And maybe I'll use a different color for here.",
    "start": "1979085",
    "end": "1984335"
  },
  {
    "text": "We have multiplied by some constant Alpha, right?",
    "start": "1984335",
    "end": "1992434"
  },
  {
    "text": "Where Alpha is what we call as a learning rate.",
    "start": "1992435",
    "end": "1996600"
  },
  {
    "text": "And we repeat this for all js,",
    "start": "1999520",
    "end": "2004765"
  },
  {
    "text": "for all j in 1 to d plus 1.",
    "start": "2004765",
    "end": "2013945"
  },
  {
    "text": "Right? And this is, uh, with this rule, we calculate,",
    "start": "2013945",
    "end": "2020215"
  },
  {
    "text": "we- we calculate Theta j for, uh, uh, for the next iteration, right?",
    "start": "2020215",
    "end": "2028425"
  },
  {
    "text": "And that's gonna give us, for example, Theta 1.",
    "start": "2028425",
    "end": "2035230"
  },
  {
    "text": "Now what's happening here. The- an easy way to understand this is to look at this with uh,",
    "start": "2035570",
    "end": "2043660"
  },
  {
    "text": "with a vector notation. So in the vector notation, this will look like Theta 1 is equal to",
    "start": "2043660",
    "end": "2050830"
  },
  {
    "text": "Theta naught minus Alpha times gradient with respect to Theta of-.",
    "start": "2050830",
    "end": "2059720"
  },
  {
    "text": "In this notation, which is the same as this notation.",
    "start": "2064560",
    "end": "2074320"
  },
  {
    "text": "[NOISE] So we have the loss function J over here,",
    "start": "2074320",
    "end": "2084085"
  },
  {
    "text": "and the gradient of a scalar valued function with respect to Theta will give you the direction of steepest ascent.",
    "start": "2084085",
    "end": "2094254"
  },
  {
    "text": "Right? So if we are at this value, the gradient of J of Theta with respect to Theta will",
    "start": "2094255",
    "end": "2103390"
  },
  {
    "text": "tell us that you need to move in this direction to improve Theta- to, to, to increase the value of J Theta.",
    "start": "2103390",
    "end": "2110575"
  },
  {
    "text": "Right? Instead what we do is we flip the sign, make it a negative.",
    "start": "2110575",
    "end": "2118359"
  },
  {
    "text": "Which means now we are looking at this direction and",
    "start": "2118360",
    "end": "2124465"
  },
  {
    "text": "update Theta by moving it by a small amount in the direction of steepest descent.",
    "start": "2124465",
    "end": "2132700"
  },
  {
    "text": "Right? So the step size that we move, right?",
    "start": "2132700",
    "end": "2138790"
  },
  {
    "text": "This difference, is Alpha times negative gradient.",
    "start": "2138790",
    "end": "2147980"
  },
  {
    "text": "We started a Theta naught. Calculate the direction in which J Theta increases the most.",
    "start": "2153060",
    "end": "2161725"
  },
  {
    "text": "Flip the sign, and take a small step in that direction where the step size is decided by Alpha.",
    "start": "2161725",
    "end": "2169010"
  },
  {
    "text": "Any questions here? Yes question. [BACKGROUND] So Theta 0, can it be random?",
    "start": "2169350",
    "end": "2178555"
  },
  {
    "text": "Theta 0 is some kind of an initialization. And uh, a common initialization is to start at 0.",
    "start": "2178555",
    "end": "2185425"
  },
  {
    "text": "You can actually initialize it to uh, any, any value. And a topic that is a slightly more advanced is that if your cost function is convex,",
    "start": "2185425",
    "end": "2194575"
  },
  {
    "text": "which is like a bowl shaped function, then the value that you initialize it two does not matter. You always, you know, end up- when this algorithm ends,",
    "start": "2194575",
    "end": "2202510"
  },
  {
    "text": "you always reach the same value. Yes question. Good question.",
    "start": "2202510",
    "end": "2209170"
  },
  {
    "text": "How do we know we're not trapped in a local minimum? I'm gonna postpone that question for now and assume,",
    "start": "2209170",
    "end": "2214465"
  },
  {
    "text": "you know, we don't have local minimums. We'll deal with local minimums later. Yes. Question. Yes. In practice,",
    "start": "2214465",
    "end": "2223559"
  },
  {
    "text": "what value of Alpha should we take? Great question, and there is no- um, uh,",
    "start": "2223560",
    "end": "2228955"
  },
  {
    "text": "it so happens that well-defined cost functions like,",
    "start": "2228955",
    "end": "2234715"
  },
  {
    "text": "you know, gradient descent on say, the linear regression are tolerant to a whole range of Alphas.",
    "start": "2234715",
    "end": "2240460"
  },
  {
    "text": "And- and you know, but um, in practice, you actually experiment with a few different values of",
    "start": "2240460",
    "end": "2246505"
  },
  {
    "text": "Alphas and- and figure out which one works best for you. Alright? So this is one step,",
    "start": "2246505",
    "end": "2254365"
  },
  {
    "text": "you know, this is, uh, uh, one step in partial derivative notation, and this is one step in vector notation, right?",
    "start": "2254365",
    "end": "2261760"
  },
  {
    "text": "And I personally feel the vector notation is easier to kind of understand. This looks a little cryptic,",
    "start": "2261760",
    "end": "2267175"
  },
  {
    "text": "whereas over here there's uh, uh, a very clear geometric meaning where you are at one particular value,",
    "start": "2267175",
    "end": "2273444"
  },
  {
    "text": "you know, in this- in this, uh, parameter space. You- you. Calculate the gradient of the cost function with respect to the current position.",
    "start": "2273445",
    "end": "2281650"
  },
  {
    "text": "And that gives you the direction of ascent. You flip the sign, take a small step in that direction,",
    "start": "2281650",
    "end": "2287200"
  },
  {
    "text": "and you- you reach a new Theta, Theta 1 such that j of Theta 1,",
    "start": "2287200",
    "end": "2294880"
  },
  {
    "text": "is hopefully less than J of Theta naught.",
    "start": "2294880",
    "end": "2304269"
  },
  {
    "text": "Why do I write hopefully here? Because if you take too large a step,",
    "start": "2304270",
    "end": "2310060"
  },
  {
    "text": "you can overshoot and reach- you know, reach a point that is actually at a higher cost.",
    "start": "2310060",
    "end": "2315625"
  },
  {
    "text": "So, uh, tuning the learning rate is important because you don't wanna overstep, right?",
    "start": "2315625",
    "end": "2320875"
  },
  {
    "text": "And as long as your learning rate is- is well tuned for your cost function, then this hopefully will be true, most of the time.",
    "start": "2320875",
    "end": "2328549"
  },
  {
    "text": "Right? And what we do, we iterate this. So repeat till convergence.",
    "start": "2330750",
    "end": "2343430"
  },
  {
    "text": "Now, we say repeat til convergence. What does that mean?",
    "start": "2343680",
    "end": "2348865"
  },
  {
    "text": "What we mean is um, if- if- if we- if I write this as Theta of t plus 1 equals Theta",
    "start": "2348865",
    "end": "2359230"
  },
  {
    "text": "of t minus Alpha-",
    "start": "2359230",
    "end": "2364160"
  },
  {
    "text": "t. If we repeat this process,",
    "start": "2367980",
    "end": "2373494"
  },
  {
    "text": "we will get a series of parameters, Theta 0,",
    "start": "2373494",
    "end": "2382690"
  },
  {
    "text": "Theta 1, Theta 2,",
    "start": "2382690",
    "end": "2387680"
  },
  {
    "text": "Theta t, Theta t plus 1, right?",
    "start": "2388920",
    "end": "2397240"
  },
  {
    "text": "We get a series of values. Now, for those of you who are- who",
    "start": "2397240",
    "end": "2403299"
  },
  {
    "text": "have a more advanced math background and know what convergence means, then we are talking about the convergence of this series.",
    "start": "2403300",
    "end": "2410140"
  },
  {
    "text": "Right? When the series converges, you can stop, um, um, iterating.",
    "start": "2410140",
    "end": "2415375"
  },
  {
    "text": "Now, However, in practice, defining the convergence has a more practical definition.",
    "start": "2415375",
    "end": "2423205"
  },
  {
    "text": "So there are many ways to check for convergence. One way to check for convergence is to look at the norm of Theta t minus Theta t minus 1.",
    "start": "2423205",
    "end": "2435115"
  },
  {
    "text": "Right? Has- has- have you stopped after taking a step?",
    "start": "2435115",
    "end": "2440230"
  },
  {
    "text": "Have you reached a stage where Theta t to Theta t minus 1 was, you know,",
    "start": "2440230",
    "end": "2445780"
  },
  {
    "text": "was- was- was- was too small or ignorably small. That's one way to check for convergence.",
    "start": "2445780",
    "end": "2450790"
  },
  {
    "text": "Another way to check for convergence is has your loss stopped going down? So J of Theta of t minus J of Theta of  t minus 1.",
    "start": "2450790",
    "end": "2463010"
  },
  {
    "text": "I'm putting it as a norm, but you know this is just a scalar. Right? And that's another way to check for convergence.",
    "start": "2464490",
    "end": "2471550"
  },
  {
    "text": "Yet another way to check for convergence is, was the norm of your gradient too small?",
    "start": "2471550",
    "end": "2477560"
  },
  {
    "text": "Okay, so you can look for the norm of the gradient and- and- and check if the norm of the gradient became too small.",
    "start": "2484290",
    "end": "2490585"
  },
  {
    "text": "Or you can- you can- you can look at the difference between the two, you know, the previous uh, uh,",
    "start": "2490585",
    "end": "2495595"
  },
  {
    "text": "parameter and the current parameter and check if, you know, that- that difference, uh, got smaller than some Epsilon.",
    "start": "2495595",
    "end": "2501595"
  },
  {
    "text": "Or if, you know, the- the cost value itself, if it stopped, um, stopped producing right?",
    "start": "2501595",
    "end": "2507850"
  },
  {
    "text": "The- the- there- there's no right answer for how you want to check for convergence in practice.",
    "start": "2507850",
    "end": "2512920"
  },
  {
    "text": "These are just, you know, a few- few thoughts on how you can, um, implement it. So in your- in your code there would be some kind of an Epsilon that you define,",
    "start": "2512920",
    "end": "2521835"
  },
  {
    "text": "which is, you know, maybe 10 to the minus 5 or some- some such small number.",
    "start": "2521835",
    "end": "2527145"
  },
  {
    "text": "And you keep iterating this inner loop by, you know, wherever, you know, think of t as your iteration number and you keep iterating",
    "start": "2527145",
    "end": "2534595"
  },
  {
    "text": "until one of these values becomes smaller than Epsilon. So that would be like your breaking condition for the loop. Yes, question.",
    "start": "2534595",
    "end": "2544910"
  },
  {
    "text": "Oh no, there's just notation, so, you know, think of it as the absolute value, right?",
    "start": "2546650",
    "end": "2552220"
  },
  {
    "text": "So if- if stops with- Yes. [BACKGROUND]",
    "start": "2552590",
    "end": "2574539"
  },
  {
    "text": "So the question, if I understood right is, can we not have, um, um, um,",
    "start": "2574540",
    "end": "2580385"
  },
  {
    "text": "a way to intelligently set Alpha for each iteration to get- get closer. You can- you can do a whole lot of, you know,",
    "start": "2580385",
    "end": "2586480"
  },
  {
    "text": "such variants and there are a lot of variants of this algorithms that exist in practice, um,",
    "start": "2586480",
    "end": "2592450"
  },
  {
    "text": "but for the most part, um, we kind of exploit the fact that as you get closer to- to,",
    "start": "2592450",
    "end": "2598795"
  },
  {
    "text": "uh, the minimum, the gradient also keeps becoming smaller. Right? So as- as you get closer,",
    "start": "2598795",
    "end": "2605380"
  },
  {
    "text": "the- the- the overall update, even for a constant Alpha, the overall update also keeps becoming smaller because",
    "start": "2605380",
    "end": "2610780"
  },
  {
    "text": "your gradient also becomes, uh, smaller and smaller. I mean the- the, um, intuition there is that,",
    "start": "2610780",
    "end": "2616240"
  },
  {
    "text": "once you reach the absolute minimum, right? So supposing this is J of Theta, right?",
    "start": "2616240",
    "end": "2622540"
  },
  {
    "text": "Once you reach the absolute minimum, the gradient is 0. Right? So- and the gradient keeps getting smaller and smaller as,",
    "start": "2622540",
    "end": "2630940"
  },
  {
    "text": "you know, you approach- approach the minimum. Right? So, uh, most of the times,",
    "start": "2630940",
    "end": "2635965"
  },
  {
    "text": "some kind of a- a- a well-tuned value for Alpha, constant value is- is- is, you know, works- works well enough. Good question.",
    "start": "2635965",
    "end": "2644184"
  },
  {
    "text": "Any other question? All right. So what's- what's happening now, um,",
    "start": "2644185",
    "end": "2650605"
  },
  {
    "text": "we saw this- this algorithm called gradient descent,",
    "start": "2650605",
    "end": "2655675"
  },
  {
    "text": "where we are given some kind of a cost function which we visualize, uh, with contour plots that is defined over the parameter space, right?",
    "start": "2655675",
    "end": "2665125"
  },
  {
    "text": "And the shape of the cost function has the data set kind of embedded in it, right? If you chose a different training set,",
    "start": "2665125",
    "end": "2671980"
  },
  {
    "text": "then maybe your cost function is tilted differently or has a minimum at a- at a- at a different location, but the training set is kind of embedded inside this cost function itself.",
    "start": "2671980",
    "end": "2681025"
  },
  {
    "text": "And now we start with a random initialization of- of Theta and we keep,",
    "start": "2681025",
    "end": "2686140"
  },
  {
    "text": "um, taking small steps in the direction of the negative gradient, and we keep taking steps until we hit some kind of a convergence, um, uh,",
    "start": "2686140",
    "end": "2696430"
  },
  {
    "text": "convergence condition, and there are many choices for defining what the convergence condition is.",
    "start": "2696430",
    "end": "2702069"
  },
  {
    "text": "And we take this algorithm, uh, which we call as gradient descent, and apply it to the linear regression cost function.",
    "start": "2702070",
    "end": "2711590"
  },
  {
    "text": "Right? We take this algorithm and we apply to the linear regression cost function. What does that mean?",
    "start": "2711590",
    "end": "2720290"
  },
  {
    "text": "It means the update rule that we have- update rule that we have over here will take a specific form.",
    "start": "2720810",
    "end": "2733280"
  },
  {
    "text": "So the methodology we are following here is- is common for pretty",
    "start": "2736110",
    "end": "2742330"
  },
  {
    "text": "much all the algorithms that we're gonna be- that we're gonna be,",
    "start": "2742330",
    "end": "2748285"
  },
  {
    "text": "uh, uh, studying, where we define- given the training set,",
    "start": "2748285",
    "end": "2754720"
  },
  {
    "text": "we define some kind of a cost function, defined over a parameter space and use",
    "start": "2754720",
    "end": "2759970"
  },
  {
    "text": "gradient descent to find a parameter that minimizes the cost function, right? This- this is a common template that we repeat over and over for different algorithms.",
    "start": "2759970",
    "end": "2771190"
  },
  {
    "text": "The algorith- uh, for- for different models. The model will give us different cost functions, right?",
    "start": "2771190",
    "end": "2778225"
  },
  {
    "text": "And the corresponding cost function will be plugged into this algorithm until we minimize it.",
    "start": "2778225",
    "end": "2785245"
  },
  {
    "text": "Right? That's gonna be a repeating pattern throughout this class. So in case of- now,",
    "start": "2785245",
    "end": "2790900"
  },
  {
    "text": "gradient descent on linear regression.",
    "start": "2790900",
    "end": "2800240"
  },
  {
    "text": "So gradient descent on linear regression will look like this.",
    "start": "2806100",
    "end": "2810890"
  },
  {
    "text": "Repeat un- until convergence.",
    "start": "2811380",
    "end": "2819769"
  },
  {
    "text": "Right? So first let's set Theta naught equal to some initialization, right?",
    "start": "2819780",
    "end": "2830785"
  },
  {
    "text": "Repeat until convergence, Theta t plus 1",
    "start": "2830785",
    "end": "2836664"
  },
  {
    "text": "equals Theta t minus Alpha.",
    "start": "2836665",
    "end": "2843430"
  },
  {
    "text": "That's the step size or the learning rate times the gradient of this vector jt.",
    "start": "2843430",
    "end": "2853375"
  },
  {
    "text": "Right? Now, this is, um, the standard gradient descent and when you apply to linear regression,",
    "start": "2853375",
    "end": "2859930"
  },
  {
    "text": "we replace the cost function with the cost function of linear regression.ikea So this is Theta t minus half Theta.",
    "start": "2859930",
    "end": "2874029"
  },
  {
    "text": "And what's the cost function? Half equals 1 to n,",
    "start": "2874250",
    "end": "2881859"
  },
  {
    "text": "h Theta of x_i minus y_i squared.",
    "start": "2883700",
    "end": "2894170"
  },
  {
    "text": "Let's simplify this further. Theta of t minus Alpha times gradient half of",
    "start": "2894170",
    "end": "2907820"
  },
  {
    "text": "n. Each Theta of X is Theta transpose X.",
    "start": "2909390",
    "end": "2917420"
  },
  {
    "text": "Theta transpose X_i minus y_i square.",
    "start": "2917700",
    "end": "2928855"
  },
  {
    "text": "Right? So just to make it clear, um, the thing that we are minimizing with respect to is respect to Theta,",
    "start": "2928855",
    "end": "2938530"
  },
  {
    "text": "and this is the only place where Theta appears.",
    "start": "2938530",
    "end": "2944890"
  },
  {
    "text": "Right? The xs and ys are all given, they are just constants, uh, for the purposes of optimizing this cost function,",
    "start": "2944890",
    "end": "2952119"
  },
  {
    "text": "which is why we- we- we- we think of the- the training set kind of being embedded in- in your cost function.",
    "start": "2952120",
    "end": "2959590"
  },
  {
    "text": "Right? They are just different constants as far as, uh, the cost function is concerned. Right? Let's- let's, um,",
    "start": "2959590",
    "end": "2966760"
  },
  {
    "text": "expand this further using our matrix calculus tricks. This is Theta t minus Alpha of",
    "start": "2966760",
    "end": "2978080"
  },
  {
    "text": "n is the square. Um, this will give us 2 times- I'm sorry.",
    "start": "2983640",
    "end": "2992860"
  },
  {
    "text": "Yeah. 2 times Theta transpose X_i",
    "start": "2992860",
    "end": "2999070"
  },
  {
    "text": "minus y_i times x_i, right?",
    "start": "2999070",
    "end": "3010770"
  },
  {
    "text": "Now, the two-and-a-half cancel, so somebody asks, why do we have- have a half in the- in the- in the cost function?",
    "start": "3010770",
    "end": "3018255"
  },
  {
    "text": "The reason is only to just make the gradient update rule look simple.",
    "start": "3018255",
    "end": "3023414"
  },
  {
    "text": "Just- just- just the way multiplying it by 1 over n did not matter. In fact, the- the- you know,",
    "start": "3023415",
    "end": "3029220"
  },
  {
    "text": "half also does not matter except to make our gradient expression look- look simple. This transpose t",
    "start": "3029220",
    "end": "3039045"
  },
  {
    "text": "minus Alpha times sum",
    "start": "3039045",
    "end": "3045645"
  },
  {
    "text": "of i equals 1 to n of Theta transpose x minus x_i,",
    "start": "3045645",
    "end": "3055275"
  },
  {
    "text": "minus y_i times x_i.",
    "start": "3055275",
    "end": "3063069"
  },
  {
    "text": "Right? Notice here that Theta transpose x_i is a scalar, y_i is a scalar,",
    "start": "3063320",
    "end": "3071925"
  },
  {
    "text": "so thet- Theta transpose X_i minus y_i is also a scalar, so it is a scalar times a vector,",
    "start": "3071925",
    "end": "3080565"
  },
  {
    "text": "and the dimension of this is D. Right? So it is- you're summing over",
    "start": "3080565",
    "end": "3086715"
  },
  {
    "text": "a D dimensional vector scaled by some scalar in- in such terms, so this whole expression is D dimensional vector, right?",
    "start": "3086715",
    "end": "3097035"
  },
  {
    "text": "And Theta t was also a D dimensional vector. So we- the next Theta t plus",
    "start": "3097035",
    "end": "3103589"
  },
  {
    "text": "1 is a D dimensional vector minus some scalar times a D dimensional vector.",
    "start": "3103590",
    "end": "3109890"
  },
  {
    "text": "So this is also a D dimensional vector. Right? So when you- when you- when you, um, write out matrix calculus,",
    "start": "3109890",
    "end": "3116039"
  },
  {
    "text": "it's always- always a good- good idea to make sure that the dimensions match. Right. Any questions about- any questions about this?",
    "start": "3116040",
    "end": "3125445"
  },
  {
    "text": "So this is the gradient update, uh, algorithm, where you repeat this over and over until one of the convergence conditions hit,",
    "start": "3125445",
    "end": "3135255"
  },
  {
    "text": "um, with your training set embedded here. And once you- once you,",
    "start": "3135255",
    "end": "3141660"
  },
  {
    "text": "um- once this algorithm converges, you would have solved linear regression. Yes, there was a question.",
    "start": "3141660",
    "end": "3149100"
  },
  {
    "text": "[inaudible]. Yes. So the question is,",
    "start": "3149100",
    "end": "3154515"
  },
  {
    "text": "what's the dimen- what- what's- what's t- what are we talking about, uh, Theta over here? Theta is, uh, the- the set of all,",
    "start": "3154515",
    "end": "3161520"
  },
  {
    "text": "uh, all- all parameters, which is vector-valued. And the superscript over here is basically saying, you know,",
    "start": "3161520",
    "end": "3167760"
  },
  {
    "text": "which iteration in gradient descent are we at. Right? So this is a D dimensional vector,",
    "start": "3167760",
    "end": "3174944"
  },
  {
    "text": "x is a D dimensional vector, or maybe d plus 1 to account for the, uh, uh, intercept. So this is also d plus 1 dimensional and, um,",
    "start": "3174945",
    "end": "3183130"
  },
  {
    "text": "h Theta of x scalar, y scalar, you know, and so on. Any questions? Yes.",
    "start": "3183130",
    "end": "3190609"
  },
  {
    "text": "[inaudible]",
    "start": "3190610",
    "end": "3205975"
  },
  {
    "text": "If you take the log of the sum of the squared errors. Uh, I'm sorry, I missed the question, what's- what's?",
    "start": "3205975",
    "end": "3213040"
  },
  {
    "text": "[inaudible]",
    "start": "3213040",
    "end": "3221890"
  },
  {
    "text": "Um, not really so we go- taking a log wouldn't- wouldn't,",
    "start": "3221890",
    "end": "3227589"
  },
  {
    "text": "uh, um, so the reason the- the 1/2 exist here is only to make this update rule looks simple. There's- there's no other, uh, no other reason.",
    "start": "3227590",
    "end": "3234775"
  },
  {
    "text": "Because, you know, whatever constant you have, uh, you know, half or- or any other constant.",
    "start": "3234775",
    "end": "3239905"
  },
  {
    "text": "You can always kind of counter it by choosing a different alpha, right? So there's- there's, uh- um,",
    "start": "3239905",
    "end": "3246115"
  },
  {
    "text": "no purpose for that. All right. So that's gradient descent. However, um, in practice [NOISE] what we use is a variant of, um,",
    "start": "3246115",
    "end": "3258654"
  },
  {
    "text": "so for- for- a lot of convex functions like this for- for example, for linear regression, uh,",
    "start": "3258655",
    "end": "3264190"
  },
  {
    "text": "the algorithm that, you know, uh, gradient descent works perfectly fine. However, there's a variant of gradient descent or",
    "start": "3264190",
    "end": "3270130"
  },
  {
    "text": "stochastic gradient descent [NOISE] called SGD.",
    "start": "3270130",
    "end": "3283820"
  },
  {
    "text": "So the way SGD, uh, works is- it is- so what- what- let's- let's look at this gradient,",
    "start": "3285060",
    "end": "3296410"
  },
  {
    "text": "uh, descent, um, algorithm on linear regression here. So this is the update rule.",
    "start": "3296410",
    "end": "3302410"
  },
  {
    "text": "How- what- what we have over here, and we repeat this over and over for each- each- um, for each update.",
    "start": "3302410",
    "end": "3309640"
  },
  {
    "text": "So in order to [NOISE] make so much of progress.",
    "start": "3309640",
    "end": "3317005"
  },
  {
    "text": "So the goal is we start from here and we wanna reach here. And these are small steps of progress that you are making.",
    "start": "3317005",
    "end": "3322705"
  },
  {
    "text": "Each step that we're- I know of- of gradient descent is like a small amount of progress that we're making. So in order to make so much of progress,",
    "start": "3322705",
    "end": "3329890"
  },
  {
    "text": "one step worth of progress, [NOISE] we need to compute this step, right?",
    "start": "3329890",
    "end": "3339775"
  },
  {
    "text": "And for those of you who are- who are probably kind of algorithmically minded,",
    "start": "3339775",
    "end": "3344920"
  },
  {
    "text": "you might be wondering, for each small step of progress that we wanna make,",
    "start": "3344920",
    "end": "3350935"
  },
  {
    "text": "we need to iterate over our entire training set, right? And the number of examples in your training set could be",
    "start": "3350935",
    "end": "3358980"
  },
  {
    "text": "a million or it could be a billion, right? In order to- to make a small- uh,",
    "start": "3358980",
    "end": "3364960"
  },
  {
    "text": "in order to make a small step of progress, we need to scan through our entire training set with this algorithm, right.",
    "start": "3364960",
    "end": "3371020"
  },
  {
    "text": "And that can be extremely expensive. If your- if your, um,",
    "start": "3371020",
    "end": "3376600"
  },
  {
    "text": "um, training set is too big or if your- uh, if- if your model is also too big to compute gradients on- on so many examples, right?",
    "start": "3376600",
    "end": "3385720"
  },
  {
    "text": "And this motivates the- the SGD or the stochastic gradient descent algorithm,",
    "start": "3385720",
    "end": "3391330"
  },
  {
    "text": "which is a variant of gradient descent. Where- what we do is,",
    "start": "3391330",
    "end": "3397760"
  },
  {
    "text": "we say Theta of t plus 1 is equal to Theta of t plus",
    "start": "3400200",
    "end": "3410980"
  },
  {
    "text": "or rather minus Alpha",
    "start": "3410980",
    "end": "3417220"
  },
  {
    "text": "times I'm gonna call this J tilde of Theta.",
    "start": "3417220",
    "end": "3425510"
  },
  {
    "text": "Where J tilde of Theta [NOISE] is- in case of linear regression,",
    "start": "3426210",
    "end": "3434020"
  },
  {
    "text": "it's gonna be half of",
    "start": "3434020",
    "end": "3440770"
  },
  {
    "text": "[NOISE].",
    "start": "3440770",
    "end": "3474430"
  },
  {
    "text": "So what do we do? Instead of calculating the gradient of the loss function on the full training set,",
    "start": "3474430",
    "end": "3481765"
  },
  {
    "text": "we instead sample just one example, uniformly at random, right?",
    "start": "3481765",
    "end": "3488035"
  },
  {
    "text": "And pretend that is our entire training set. Pretend that our training set has only one example.",
    "start": "3488035",
    "end": "3495020"
  },
  {
    "text": "Calculate the gradient of that cost function,",
    "start": "3495020",
    "end": "3500040"
  },
  {
    "text": "which has only one random example, right? And take a step in the direction according to that cost function, right?",
    "start": "3500040",
    "end": "3508599"
  },
  {
    "text": "And we take one step and repeat this process by sampling",
    "start": "3508600",
    "end": "3513760"
  },
  {
    "text": "a new training example to construct this temporary or proxy loss function specific to that example,",
    "start": "3513760",
    "end": "3520470"
  },
  {
    "text": "there is no summation here. We use some example K, and that K is sampled uniformly at random from your training set.",
    "start": "3520470",
    "end": "3528135"
  },
  {
    "text": "And for each iteration we sample a different example, right?",
    "start": "3528135",
    "end": "3534835"
  },
  {
    "text": "This might look- this might look, uh, it might be surprising. You might wonder, would this even work?",
    "start": "3534835",
    "end": "3542545"
  },
  {
    "text": "Right. The- the intuition over here is that, you know, um,",
    "start": "3542545",
    "end": "3553940"
  },
  {
    "text": "this is our, um, J Theta- Theta 1 to Theta d.",
    "start": "3556800",
    "end": "3566215"
  },
  {
    "text": "And we start at some random position here.",
    "start": "3566215",
    "end": "3572755"
  },
  {
    "text": "In case of gradient descent- [NOISE] in case of gradient descent the-",
    "start": "3572755",
    "end": "3579055"
  },
  {
    "text": "the trajectory followed by the sequence of Thetas would look something like this, right?",
    "start": "3579055",
    "end": "3587000"
  },
  {
    "text": "You- that we- we are making- making- um,",
    "start": "3591930",
    "end": "3598165"
  },
  {
    "text": "you're making progress in the direction of the final minima and taking smaller,",
    "start": "3598165",
    "end": "3604780"
  },
  {
    "text": "and smaller steps as we go closer and closer, right? This would- this is a trajectory that gradient descent would have taken.",
    "start": "3604780",
    "end": "3612550"
  },
  {
    "text": "Whereas, with stochastic gradient descent, the- the updates might look like this, right?",
    "start": "3612550",
    "end": "3622610"
  },
  {
    "text": "We start with Theta naught, right? Instead of calculating the gradient with respect to the true cost function,",
    "start": "3636910",
    "end": "3646655"
  },
  {
    "text": "we are calculating the gradient with respect to this proxy cost function, which has only one random example in it, right?",
    "start": "3646655",
    "end": "3653510"
  },
  {
    "text": "And we're gonna make a step in the negative gradient of that, you know, um, um, uh,",
    "start": "3653510",
    "end": "3659539"
  },
  {
    "text": "proxy cost function and that's gonna be of Theta_1. And Theta_2 will be with respect to the next random, um, example.",
    "start": "3659540",
    "end": "3667805"
  },
  {
    "text": "And what we notice is that in- while in the case of gradient descent, the direction was consistent and always headed toward the local minima,",
    "start": "3667805",
    "end": "3676865"
  },
  {
    "text": "and stochastic gradient descent, the directions are a little, um, crooked, so to speak, right?",
    "start": "3676865",
    "end": "3683510"
  },
  {
    "text": "And you might even encounter cases where you are kind of actually going in the opposite direction of,",
    "start": "3683510",
    "end": "3688925"
  },
  {
    "text": "you know, where you actually want to go eventually, right? But it so happens that by following this algorithm, eventually,",
    "start": "3688925",
    "end": "3697400"
  },
  {
    "text": "you will reach a region around the true minima, right,",
    "start": "3697400",
    "end": "3706549"
  },
  {
    "text": "and all further updates of- of, uh, uh, stochastic gradient descent is gonna keep you",
    "start": "3706550",
    "end": "3713089"
  },
  {
    "text": "confined to a small ball around the true global minima, and the radius of that ball is gonna be a function of the step size Alpha.",
    "start": "3713090",
    "end": "3724230"
  },
  {
    "text": "Right? There's a lot of theory that- that, um- that kind of precisely characterizes this.",
    "start": "3725230",
    "end": "3732800"
  },
  {
    "text": "Uh, so, for example, if you- you know, if you're interested to go deeper into this theory, you can, um,",
    "start": "3732800",
    "end": "3737839"
  },
  {
    "text": "um- you can- you can study stochastic approximation",
    "start": "3737840",
    "end": "3742980"
  },
  {
    "text": "that give you- that- that- that formally, um- that precisely formulates the con- the- the behavior of SGD.",
    "start": "3749020",
    "end": "3757520"
  },
  {
    "text": "But for- from this course point of view, all we need to, um, know is that SGD works, okay?",
    "start": "3757520",
    "end": "3764330"
  },
  {
    "text": "[NOISE] So, um, SGD is- it makes very noisy updates because we're not taking the full set of training,",
    "start": "3764330",
    "end": "3773750"
  },
  {
    "text": "uh- the full training set at every step. It's gonna make noisy steps, but on average, you're gonna reach the global minimum or you're gonna reach a region that is, you know,",
    "start": "3773750",
    "end": "3783350"
  },
  {
    "text": "sufficiently close to the global minimum, and that region is characterized by the, um- by the step size Alpha that you choose. Yes, question.",
    "start": "3783350",
    "end": "3792260"
  },
  {
    "text": "[inaudible]. Sorry, is- is there,",
    "start": "3792260",
    "end": "3802220"
  },
  {
    "text": "uh- can you take a running sample of, uh, uh- A running sum. -a- a running sum of- of",
    "start": "3802220",
    "end": "3807545"
  },
  {
    "text": "the previous gradients and- and do some kind of an averaging of them, um, um, to- to kind of have less noisy updates?",
    "start": "3807545",
    "end": "3814670"
  },
  {
    "text": "[NOISE] Yeah, there are lots of variants of, uh- of, uh- of- of- of gradient descent,",
    "start": "3814670",
    "end": "3820130"
  },
  {
    "text": "and the, um- the technique that you described is also commonly called as momentum.",
    "start": "3820130",
    "end": "3826859"
  },
  {
    "text": "So when you- when you, uh- when you loo- uh, when you look up, uh, all the different, uh,",
    "start": "3828310",
    "end": "3834170"
  },
  {
    "text": "optimization algorithms that exist for machine learning, you know, some of them are gonna have this parameter called momentum,",
    "start": "3834170",
    "end": "3839569"
  },
  {
    "text": "and that does, uh, something exactly what you described, where it keeps a running average of previous descents to kind of,",
    "start": "3839570",
    "end": "3845015"
  },
  {
    "text": "um, make the updates less noisy. [inaudible].",
    "start": "3845015",
    "end": "3857900"
  },
  {
    "text": "So, uh, for true SGD, um, each time you pick an example,",
    "start": "3857900",
    "end": "3864140"
  },
  {
    "text": "you randomly pick it. The next time, you may pick the same example, but you don't care. You know, just- just pick it randomly, right?",
    "start": "3864140",
    "end": "3870170"
  },
  {
    "text": "Uh, but in a lot of, uh, actual applications, and- and also in the notes, what we see is you kind of scan your training set from top to bottom.",
    "start": "3870170",
    "end": "3879079"
  },
  {
    "text": "Preferably, you want to shuffle your training set once, and- and just loop over your training set from,",
    "start": "3879080",
    "end": "3885275"
  },
  {
    "text": "you know, 1 through n by doing a scan, by using a different example each time, and then you can just repeat again or, you know,",
    "start": "3885275",
    "end": "3892400"
  },
  {
    "text": "at the end of one sweep, you know, shuffle it- reshuffle it and- and, you know, um, repeat again. Yeah? [inaudible].",
    "start": "3892400",
    "end": "3900470"
  },
  {
    "text": "No, there is- there is no restriction on the minimum size of, uh, training- uh, um, training set. The algorithm works for any training set, uh, but,",
    "start": "3900470",
    "end": "3908944"
  },
  {
    "text": "um, if your training set is not too large, then, you know, just regular gradient descent",
    "start": "3908945",
    "end": "3914830"
  },
  {
    "text": "might work well for you. Um, also, another, uh,",
    "start": "3914830",
    "end": "3920065"
  },
  {
    "text": "thing to note is that with stochastic gradient descent, you may need a lot more number of steps to converge as compared to gradient descent,",
    "start": "3920065",
    "end": "3929895"
  },
  {
    "text": "but the cost of taking each step- uh, the computational cost of each step is so small",
    "start": "3929895",
    "end": "3935975"
  },
  {
    "text": "that it's well-worth- well-worth it to have more number of steps, uh, but they are, you know,",
    "start": "3935975",
    "end": "3942230"
  },
  {
    "text": "so much more inexpensive compared to a full gradient descent. Yes, question. [inaudible].",
    "start": "3942230",
    "end": "3954620"
  },
  {
    "text": "Yep, so the question is, uh, can- can- can we do something in between gradient descent and, um, uh,",
    "start": "3954620",
    "end": "3961190"
  },
  {
    "text": "stochastic gradient descent where instead of one example, we- we take a small batch of examples, um,",
    "start": "3961190",
    "end": "3967580"
  },
  {
    "text": "and the J Tilde Theta is then defined over as a summation over that batch of examples. Yes, you can absolutely do that, and that's,",
    "start": "3967580",
    "end": "3974660"
  },
  {
    "text": "uh- that's called mini-batch gradient descent, and in fact, most of, um, uh, you know, deep learning and neural networks do exactly that,",
    "start": "3974660",
    "end": "3981845"
  },
  {
    "text": "where you take, um, uh, a batch of examples, uh, where the batch size is some small number like 64 something, yeah.",
    "start": "3981845",
    "end": "3989210"
  },
  {
    "text": "Is it faster or is it more advantageous to use that?",
    "start": "3989210",
    "end": "3994640"
  },
  {
    "text": "Is it more advantageous to use that thing? It could be more advantageous, um, for- for, um,",
    "start": "3994640",
    "end": "4001780"
  },
  {
    "text": "it so happens that the- the situations where we try- where we need to use, um, uh, uh,",
    "start": "4001780",
    "end": "4010750"
  },
  {
    "text": "SGD or mini-batch SGD happen to be with deep learning or neural networks,",
    "start": "4010750",
    "end": "4016270"
  },
  {
    "text": "where the cost function is not convex, right? And once you have, uh, uh,",
    "start": "4016270",
    "end": "4021460"
  },
  {
    "text": "a nonconvex cost function, it's very hard to, um- it's very hard to analyze and",
    "start": "4021460",
    "end": "4027520"
  },
  {
    "text": "make precise statements of what helps and what doesn't help. And so in- in- in those situations, it- the answer is almost always try and see if it works better.",
    "start": "4027520",
    "end": "4036650"
  },
  {
    "text": "Okay. So that's SGD and gradient descent.",
    "start": "4037200",
    "end": "4043015"
  },
  {
    "text": "Any- any questions before we move on? [NOISE] All right.",
    "start": "4043015",
    "end": "4049900"
  },
  {
    "text": "So [NOISE]",
    "start": "4049900",
    "end": "4062980"
  },
  {
    "text": "now that we've seen SGD and gradient descent, two different iterative algorithms,",
    "start": "4062980",
    "end": "4068740"
  },
  {
    "text": "they are also called numerical algorithms because, um, in order to compute the values,",
    "start": "4068740",
    "end": "4074875"
  },
  {
    "text": "you actually need a computer where you- where you code this algorithm and run it and get a solution.",
    "start": "4074875",
    "end": "4080440"
  },
  {
    "text": "You don't have like a- a mathematical expression for what the final answer is, right? You just describe this algorithm,",
    "start": "4080440",
    "end": "4086395"
  },
  {
    "text": "implement it as code, execute it on a computer, and it's gonna return some numerical values for your Theta, right?",
    "start": "4086395",
    "end": "4093745"
  },
  {
    "text": "And this is- this is gonna be the case for most of our algorithms,",
    "start": "4093745",
    "end": "4099068"
  },
  {
    "text": "where we don't have a precise mathematical expression for the final answer.",
    "start": "4099069",
    "end": "4105849"
  },
  {
    "text": "You're gonna define a numerical solution, an iterative solution, and you then need to code it up and",
    "start": "4105850",
    "end": "4114250"
  },
  {
    "text": "find a numerical answer for that particular problem. Yes, question.",
    "start": "4114250",
    "end": "4120609"
  },
  {
    "text": "[inaudible]. Yes, that's what we're coming to, okay? So, er, the only exception or one of the few exceptions is linear regression,",
    "start": "4120610",
    "end": "4129805"
  },
  {
    "text": "where there is a closed form- a- a close form [NOISE] solution for- for,",
    "start": "4129805",
    "end": "4135355"
  },
  {
    "text": "uh, uh, uh, minimizing the cost function over here. Now, the reason why we first, uh,",
    "start": "4135355",
    "end": "4140980"
  },
  {
    "text": "started with gradient descent for linear regression is because taking gradients is very easy and it's easy to kind",
    "start": "4140980",
    "end": "4147159"
  },
  {
    "text": "of show you how gradient descent works, right? But in practice, as an exception,",
    "start": "4147160",
    "end": "4152290"
  },
  {
    "text": "you know, for linear regression, there's actually, um, a closed-form solution which you can use and that's something we will see now.",
    "start": "4152290",
    "end": "4159490"
  },
  {
    "text": "[NOISE] All right. So first, let's redefine J Theta.",
    "start": "4159490",
    "end": "4165775"
  },
  {
    "text": "So first, we saw J Theta to be, uh- we define it as half i equals 1n, [NOISE]",
    "start": "4165775",
    "end": "4186819"
  },
  {
    "text": "[NOISE] right? This was our cost function. Now, let us first rewrite this as in- in- in a vectorized notation.",
    "start": "4186820",
    "end": "4202540"
  },
  {
    "text": "Let us define the, um- what you call as the design matrix x,",
    "start": "4208280",
    "end": "4223590"
  },
  {
    "text": "which is an n by d vector, okay?",
    "start": "4223590",
    "end": "4229150"
  },
  {
    "text": "And each-",
    "start": "4234290",
    "end": "4237250"
  },
  {
    "text": "each row in this matrix is one input- input x vector right?",
    "start": "4245460",
    "end": "4252640"
  },
  {
    "text": "And we also define y,",
    "start": "4252640",
    "end": "4256790"
  },
  {
    "text": "y_1 through y_n.",
    "start": "4257850",
    "end": "4265720"
  },
  {
    "text": "All right? And now Theta will be a vector,",
    "start": "4265720",
    "end": "4271780"
  },
  {
    "text": "Theta_1 through Theta d, or d plus 1, so the row of each,",
    "start": "4271780",
    "end": "4281515"
  },
  {
    "text": "uh- the row of each design matrix is d plus 1 dimensional, uh, if you include the interceptor and, um,",
    "start": "4281515",
    "end": "4289150"
  },
  {
    "text": "the vector Theta is- is, uh, d plus 1, um, dimensional vector,",
    "start": "4289150",
    "end": "4294265"
  },
  {
    "text": "and y is n dimensional, one for each, um- one for each example.",
    "start": "4294265",
    "end": "4299695"
  },
  {
    "text": "Right? Now- [NOISE]",
    "start": "4299695",
    "end": "4310645"
  },
  {
    "text": "The expression X Theta minus Y,",
    "start": "4310645",
    "end": "4317610"
  },
  {
    "text": "will be the matrix X, multiplied by the vector Theta,",
    "start": "4317610",
    "end": "4324045"
  },
  {
    "text": "minus the vector Y, right? So X as in R^n by",
    "start": "4324045",
    "end": "4331980"
  },
  {
    "text": "d. Theta as in R- nd plus 1.Theta as in R^d plus 1,",
    "start": "4331980",
    "end": "4339735"
  },
  {
    "text": "and Y as in R^n, right?",
    "start": "4339735",
    "end": "4345179"
  },
  {
    "text": "So, uh, multiplying a- a- a matrix of n by d plus 1 times a vector of d plus 1 will",
    "start": "4345180",
    "end": "4351240"
  },
  {
    "text": "give us a vector n- R^n minus R^n, right?",
    "start": "4351240",
    "end": "4358275"
  },
  {
    "text": "And this will be just R^n, right? So what does this look like? Um, X Theta is equal",
    "start": "4358275",
    "end": "4368730"
  },
  {
    "text": "to x^1 transpose Theta,",
    "start": "4368730",
    "end": "4375760"
  },
  {
    "text": "x^i transpose Theta, x^n transpose Theta, right?",
    "start": "4376970",
    "end": "4386295"
  },
  {
    "text": "Minus y, so we can include y right here. [NOISE] Minus y^1,",
    "start": "4386295",
    "end": "4396555"
  },
  {
    "text": "minus y^i, minus y^n.",
    "start": "4396555",
    "end": "4403510"
  },
  {
    "text": "And this is X Theta minus y, this turn. Is this clear? Any questions on this?",
    "start": "4404120",
    "end": "4413020"
  },
  {
    "text": "And we define the J of",
    "start": "4414680",
    "end": "4424920"
  },
  {
    "text": "Theta to be half X Theta minus Y transpose X Theta minus Y.",
    "start": "4424920",
    "end": "4436690"
  },
  {
    "text": "Why is this? So this is X Theta minus y,",
    "start": "4439370",
    "end": "4444945"
  },
  {
    "text": "we transpose it and, you know, take the dot product of this with this. So it's- it's basically you're squaring each term and summing them up,",
    "start": "4444945",
    "end": "4453580"
  },
  {
    "text": "right? Does that make sense. You're squaring each term and summing it up, and you're dividing it by a half,",
    "start": "4454160",
    "end": "4461025"
  },
  {
    "text": "which makes this actually exactly equal to half i equals 1 to n,",
    "start": "4461025",
    "end": "4467670"
  },
  {
    "text": "Theta transpose X^i, minus y^i squared, right?",
    "start": "4467670",
    "end": "4474900"
  },
  {
    "text": "So this is the same as the original cost function that we have, right? Any questions here?",
    "start": "4474900",
    "end": "4481710"
  },
  {
    "text": "Exactly the same here, we are using vector notation. Here we were just, you know, um,",
    "start": "4481710",
    "end": "4487560"
  },
  {
    "text": "iterating and having a loop over every example. This is just a vector rotation, right?",
    "start": "4487560",
    "end": "4493320"
  },
  {
    "text": "And now, um, [NOISE] let's try to solve for",
    "start": "4493320",
    "end": "4499380"
  },
  {
    "text": "Theta from the expression equal to 0, right?",
    "start": "4499380",
    "end": "4505620"
  },
  {
    "text": "We want to- we want to set the gradient of this with respect to Theta to be equal to 0, and solve for Theta from this- from this expression, right?",
    "start": "4505620",
    "end": "4513990"
  },
  {
    "text": "[NOISE]",
    "start": "4513990",
    "end": "4523950"
  },
  {
    "text": "So half X Theta minus y transpose",
    "start": "4523950",
    "end": "4544785"
  },
  {
    "text": "X Theta minus- X Theta minus y.",
    "start": "4544785",
    "end": "4553185"
  },
  {
    "text": "Now the reason why there's no transpose between these two is because this is not one example. It is the full matrix,",
    "start": "4553185",
    "end": "4558600"
  },
  {
    "text": "the design matrix, right? And in the matrix, every example is already in the form of a row vector, right?",
    "start": "4558600",
    "end": "4565995"
  },
  {
    "text": "So, um, every example is already in the form of a row vector, it's kind of pre-transposed for you.",
    "start": "4565995",
    "end": "4571560"
  },
  {
    "text": "So this is, um- this is a vector of n dimension- of scalars of n dimension,",
    "start": "4571560",
    "end": "4579000"
  },
  {
    "text": "and this is equal to just the cost function. And now let's- let's, um- let's check through this.",
    "start": "4579000",
    "end": "4586235"
  },
  {
    "text": "This is gradient of Theta of half. I'm just gonna expand this.",
    "start": "4586235",
    "end": "4592550"
  },
  {
    "text": "So X Theta transpose X Theta,",
    "start": "4592550",
    "end": "4598369"
  },
  {
    "text": "minus X Theta transpose y,",
    "start": "4598370",
    "end": "4604580"
  },
  {
    "text": "minus y transpose X Theta, plus y transpose y.",
    "start": "4604580",
    "end": "4612850"
  },
  {
    "text": "All right? [NOISE] This is in respect to Theta,",
    "start": "4614950",
    "end": "4621870"
  },
  {
    "text": "half of Theta transpose X transpose X Theta,",
    "start": "4622750",
    "end": "4631670"
  },
  {
    "text": "minus Theta transpose X transpose y.",
    "start": "4631670",
    "end": "4646080"
  },
  {
    "text": "And these two are scalars, and these two evaluate to the same thing.",
    "start": "4647390",
    "end": "4654030"
  },
  {
    "text": "And so I'm just going to make it 2 plus y transpose y, right?",
    "start": "4654030",
    "end": "4662835"
  },
  {
    "text": "Any questions on how we went from here to here? There was a question on Piazza of how- of- of a similar- similar, uh- uh,",
    "start": "4662835",
    "end": "4670260"
  },
  {
    "text": "step we did for the Gaussian, uh, MLE of the mean of the Gaussian. This is a scalar.",
    "start": "4670260",
    "end": "4677280"
  },
  {
    "text": "X is a- X is a, uh, a matrix, Theta is a vector, X Theta is a vector,",
    "start": "4677280",
    "end": "4682560"
  },
  {
    "text": "vector- uh, dot vector is a scalar. This is a scalar. So, um, and if you have scalars,",
    "start": "4682560",
    "end": "4689880"
  },
  {
    "text": "the transpose of the scalars is the same thing. So if you transpose this, you get two times,",
    "start": "4689880",
    "end": "4695190"
  },
  {
    "text": "um- um- um, the first expression. So, um, this is just 2 times, um- 2 times that.",
    "start": "4695190",
    "end": "4702340"
  },
  {
    "text": "And- and now, um, let's observe a few things. So this is a vector,",
    "start": "4703580",
    "end": "4710038"
  },
  {
    "text": "and this is some scalar- this is some vector. And vector, er, transpose another vector is a scalar.",
    "start": "4710039",
    "end": "4716880"
  },
  {
    "text": "This is the bi- uh, the, uh- um, quadratic form that we've seen in the past, right?",
    "start": "4716880",
    "end": "4723570"
  },
  {
    "text": "And when we take the, uh- uh- when we take the gradients, we get half of 2 X transpose X Theta,",
    "start": "4723570",
    "end": "4737534"
  },
  {
    "text": "minus 2 X transpose y.",
    "start": "4737534",
    "end": "4742785"
  },
  {
    "text": "And this is just 0, so this is gonna just cancel out, right?",
    "start": "4742785",
    "end": "4747840"
  },
  {
    "text": "And this- this is the gradient, and we want this to be equal to 0, which gives us X transpose X Theta",
    "start": "4747840",
    "end": "4757830"
  },
  {
    "text": "equals X transpose y, right?",
    "start": "4757830",
    "end": "4764835"
  },
  {
    "text": "And this equation that we have here is called the normal equation.",
    "start": "4764835",
    "end": "4770320"
  },
  {
    "text": "And from this you can solve for Theta to be equal to, um,",
    "start": "4773720",
    "end": "4779400"
  },
  {
    "text": "X transpose X inverse X transpose Y,",
    "start": "4779400",
    "end": "4785219"
  },
  {
    "text": "as long as X transpose X is invertible, right? And for now, let's assume X transpose X is invertible.",
    "start": "4785220",
    "end": "4795375"
  },
  {
    "text": "If you have two rows- uh, two columns in your matrix that are duplicates of each other, then X transpose X may not be invertible,",
    "start": "4795375",
    "end": "4803100"
  },
  {
    "text": "but we'll address those kinds of, um- um, oddities later. For now, assume X transpose X is invertible,",
    "start": "4803100",
    "end": "4809744"
  },
  {
    "text": "and this gives you an estimator for Theta hat, right?",
    "start": "4809744",
    "end": "4820080"
  },
  {
    "text": "So this is called the, uh, normal equation. And it is- it is- it is only in the case of,",
    "start": "4820080",
    "end": "4826710"
  },
  {
    "text": "um- so linear regression is one of the few cases where you can come up with an exact solution and",
    "start": "4826710",
    "end": "4832860"
  },
  {
    "text": "not- and not limited to a numerical solution for which we need a computer, right?",
    "start": "4832860",
    "end": "4839235"
  },
  {
    "text": "This is an exact solution for calculating Theta hat, uh, or- or to minimize your cost function because J of",
    "start": "4839235",
    "end": "4847739"
  },
  {
    "text": "this Theta hat is gonna minimize your- is gonna,",
    "start": "4847740",
    "end": "4853920"
  },
  {
    "text": "uh, minimize this to be the smallest, right? Any questions? Yes, question.",
    "start": "4853920",
    "end": "4859995"
  },
  {
    "text": "What is the use of the other approach? Right. So what's the use of the other approach? The- the other approach is, uh,",
    "start": "4859995",
    "end": "4865605"
  },
  {
    "text": "we- we use linear regression to describe gradient descent because it is easy to derive the update rules for the purposes of,",
    "start": "4865605",
    "end": "4873494"
  },
  {
    "text": "you know, education, right? You're gonna use gradient descent-based approaches for pretty much all other machine learning algorithms like,",
    "start": "4873495",
    "end": "4881790"
  },
  {
    "text": "uh- that we're going to come across in this course. Linear regression is one exception,",
    "start": "4881790",
    "end": "4887300"
  },
  {
    "text": "which you can solve it with gradient descent. There's no problem solving it with gradient descent, but you also have a closed form solution for linear regression,",
    "start": "4887300",
    "end": "4895550"
  },
  {
    "text": "which the other algorithms don't have. Any other question? Okay. Now, let's- let's kind of,",
    "start": "4895550",
    "end": "4905955"
  },
  {
    "text": "um, see a few more interpretations of this, right?",
    "start": "4905955",
    "end": "4912790"
  },
  {
    "text": "There are, um- so",
    "start": "4913070",
    "end": "4922050"
  },
  {
    "text": "there is a probabilistic interpretation of this.",
    "start": "4922050",
    "end": "4925540"
  },
  {
    "text": "In the probabilistic interpretation, we make this assumption that the way",
    "start": "4933660",
    "end": "4940045"
  },
  {
    "text": "our Y^is are generated in our training set is through this process.",
    "start": "4940045",
    "end": "4946540"
  },
  {
    "text": "So Y^i is equal to Theta transpose X^i plus Epsilon^i.",
    "start": "4946540",
    "end": "4958225"
  },
  {
    "text": "Where Epsilon^i is some kind of a- a Gaussian noise.",
    "start": "4958225",
    "end": "4969190"
  },
  {
    "text": "It means 0 variance Sigma squared. Right. So what does this mean?",
    "start": "4969190",
    "end": "4976179"
  },
  {
    "text": "It means the way our data set is generated is, we start with some X. For example, if you're talking about the -the,",
    "start": "4976180",
    "end": "4983829"
  },
  {
    "text": "uh, a price of a house. The Xs will describe the features of that house, like the area or- or number of bedrooms, et cetera.",
    "start": "4983830",
    "end": "4990925"
  },
  {
    "text": "Right. And there exists some unknown Theta vector that,",
    "start": "4990925",
    "end": "4996699"
  },
  {
    "text": "you know, that we are interested in calculating. And the way the corresponding Y^i is generated is by taking the dot product of Theta and-",
    "start": "4996700",
    "end": "5005599"
  },
  {
    "text": "Theta and the features of the- of- of the input and adding random Gaussian noise.",
    "start": "5005600",
    "end": "5013260"
  },
  {
    "text": "Right. Now, the noise that gets added to each example is a different noise,",
    "start": "5013880",
    "end": "5020969"
  },
  {
    "text": "but the noises are distributed according to a Gaussian variable, according to a Gaussian distribution.",
    "start": "5020970",
    "end": "5028270"
  },
  {
    "text": "Right, this is- this is the assumption that we're making. Okay. This may or may not hold true in reality, but this,",
    "start": "5028370",
    "end": "5037335"
  },
  {
    "text": "we- we start with the way our -our, uh, uh, we start with this assumption to describe the way our data is generated.",
    "start": "5037335",
    "end": "5044670"
  },
  {
    "text": "You know, then what we- what we, um, what we then do is, swap the terms around.",
    "start": "5044670",
    "end": "5056310"
  },
  {
    "text": "So Epsilon^i is-is- is, um, is a random Gaussian variable,",
    "start": "5056310",
    "end": "5063555"
  },
  {
    "text": "which means Epsilon^i is also equal to Y^i minus Theta transpose X^i.",
    "start": "5063555",
    "end": "5074040"
  },
  {
    "text": "Right. Just move Epsilon over and- and, uh, uh, Theta transpose X to the other side.",
    "start": "5074040",
    "end": "5081570"
  },
  {
    "text": "And this term is, therefore,",
    "start": "5081570",
    "end": "5086699"
  },
  {
    "text": "right, so Y minus Theta transpose X^i is also distributed according to a Gaussian variable,",
    "start": "5091010",
    "end": "5101265"
  },
  {
    "text": "which now means that,",
    "start": "5101265",
    "end": "5103990"
  },
  {
    "text": "all right, this is, um, this is the claim that we make,",
    "start": "5136970",
    "end": "5142440"
  },
  {
    "text": "because Epsilon^i is normally distributed, right?",
    "start": "5142440",
    "end": "5148560"
  },
  {
    "text": "Y minus Theta transpose X^i is also normally distributed with-with mean 0 and variance Sigma squared,",
    "start": "5148560",
    "end": "5158100"
  },
  {
    "text": "which means probability of Y given X has this density function.",
    "start": "5158100",
    "end": "5164080"
  },
  {
    "text": "Is this clear? Any questions of how we went from here to here? Yes?",
    "start": "5165260",
    "end": "5172260"
  },
  {
    "text": "[inaudible] Yeah. So in this case, um,",
    "start": "5172260",
    "end": "5177510"
  },
  {
    "text": "the parameters of this probability density- of this probably density is Theta.",
    "start": "5177510",
    "end": "5183255"
  },
  {
    "text": "Right? Theta is the- is the, um, um,  the probability density that's given to us or-or- the parameter that's given to us.",
    "start": "5183255",
    "end": "5192360"
  },
  {
    "text": "Unlike the usual Gaussians where the parameter is Mu, this is a re-parameterized version where the parameters are Theta.",
    "start": "5192360",
    "end": "5200210"
  },
  {
    "text": "[inaudible]",
    "start": "5200210",
    "end": "5208909"
  },
  {
    "text": "So what this means is Y^i,",
    "start": "5208909",
    "end": "5217355"
  },
  {
    "text": "so that- that's a good question. Let me- let me clarify this a little further.",
    "start": "5217355",
    "end": "5222930"
  },
  {
    "text": "Right? So Gaussian variables have this- this property where,",
    "start": "5222930",
    "end": "5232260"
  },
  {
    "text": "so if- if Y^i minus Theta transpose X^i is- is distributed according to a Gaussian.",
    "start": "5232260",
    "end": "5239164"
  },
  {
    "text": "Now, Y^i minus Theta",
    "start": "5239165",
    "end": "5244230"
  },
  {
    "text": "transpose X^i is distributed as normal, right?",
    "start": "5244230",
    "end": "5253530"
  },
  {
    "text": "This also implies Y^i,",
    "start": "5253530",
    "end": "5258880"
  },
  {
    "text": "because Gaussians have this- have this, um, what's also called as location-scale property,",
    "start": "5265940",
    "end": "5272535"
  },
  {
    "text": "where you can- you can move your Gaussian. If- if you add some constant to your, um, um,",
    "start": "5272535",
    "end": "5278040"
  },
  {
    "text": "Gaussian random variable, then it just, you know, moves the, uh, Gaussian there. So over here, nothing in this is random,",
    "start": "5278040",
    "end": "5285315"
  },
  {
    "text": "it's just some constant. Right? So if Y^i minus Theta transpose X^i is distributed according to this,",
    "start": "5285315",
    "end": "5290760"
  },
  {
    "text": "then it means Y^i is distributed according to, uh, a normal distribution that has mean Theta transpose X^i and variance Sigma squared.",
    "start": "5290760",
    "end": "5298650"
  },
  {
    "text": "Right? And no, this is the mean and this the variance, and, you know, that's what you see. [BACKGROUND].",
    "start": "5298650",
    "end": "5312930"
  },
  {
    "text": "[inaudible] Exactly. So we're pro- we're- we're, uh, we're assuming that Y^i has- is- is distributed as a normal random variable,",
    "start": "5312930",
    "end": "5320520"
  },
  {
    "text": "as- as a standard normal variable, whose mean is different for different examples.",
    "start": "5320520",
    "end": "5325890"
  },
  {
    "text": "So for- for Y^2, the mean is going to be Theta transpose X^2. Okay. For Y^1 the mean is Theta transpose X^ 1.",
    "start": "5325890",
    "end": "5333300"
  },
  {
    "text": "And this statement is exactly the same as this statement.",
    "start": "5333300",
    "end": "5338684"
  },
  {
    "text": "Right? So to- to jump from here to here, so, um, it's easy to kind of,",
    "start": "5338685",
    "end": "5345780"
  },
  {
    "text": "uh, go through these steps. So If Epsilon^i is Y minus Theta transpose X^i, distributed according- according to Gaussian.",
    "start": "5345780",
    "end": "5353370"
  },
  {
    "text": "That means, um, you know, Y^i minus Theta transpose X^i is this distribution according to a mean 0 Gaussian,",
    "start": "5353370",
    "end": "5359235"
  },
  {
    "text": "which also means Y^i is distribution- is distributed according to a normal with mean Theta transpose X^i and variance Sigma squared.",
    "start": "5359235",
    "end": "5368295"
  },
  {
    "text": "And this can be, you know, and therefore the probability density of Y^i is given by this.",
    "start": "5368295",
    "end": "5374895"
  },
  {
    "text": "Right? So Y^i is the- is- is the variable, this is the mean.",
    "start": "5374895",
    "end": "5380625"
  },
  {
    "text": "This is the variance. Yes, question? [inaudible]",
    "start": "5380625",
    "end": "5387810"
  },
  {
    "text": "[BACKGROUND] Yes, I missed the one-half, thank you. Thank you. Is- is this clear?",
    "start": "5387810",
    "end": "5396630"
  },
  {
    "text": "Right. Now, once we have this, once we have, uh, uh, uh,",
    "start": "5396630",
    "end": "5402750"
  },
  {
    "text": "Y^i in this form, we are now going to do, maximum likelihood.",
    "start": "5402750",
    "end": "5409840"
  },
  {
    "text": "This question? [BACKGROUND] Exactly,",
    "start": "5418940",
    "end": "5425910"
  },
  {
    "text": "so here um, you are given the data, x's and y's,",
    "start": "5425910",
    "end": "5431910"
  },
  {
    "text": "which is distributed according to some unknown parameter.",
    "start": "5431910",
    "end": "5437130"
  },
  {
    "text": "And now the data is given and the parameters are unknown. And we are gonna do maximum likelihood to estimate the parameters.",
    "start": "5437130",
    "end": "5447150"
  },
  {
    "text": "[NOISE] Right?",
    "start": "5447150",
    "end": "5455340"
  },
  {
    "text": "So, the data is the x's and y's.",
    "start": "5455340",
    "end": "5460705"
  },
  {
    "text": "Parameters are- generally it is mu and sigma square.",
    "start": "5460705",
    "end": "5468015"
  },
  {
    "text": "But in this case, the parameters are theta and sigma square.",
    "start": "5468015",
    "end": "5473100"
  },
  {
    "text": "But theta is not the m- is not the mean, right? Theta transpose x is the mean for each y, right?",
    "start": "5473100",
    "end": "5481440"
  },
  {
    "text": "So th- these is a, a, a conditional distribution, right? And so we can write the likelihood function as i equals 1 to n,",
    "start": "5481440",
    "end": "5494260"
  },
  {
    "text": "probability of yi given xi with,",
    "start": "5494260",
    "end": "5501989"
  },
  {
    "text": "with a given theta, right? And we are making an IID assumption. That's the case all the time.",
    "start": "5501990",
    "end": "5507449"
  },
  {
    "text": "We assume that the epsilons, the [NOISE] epsilons over here are IID [NOISE].",
    "start": "5507450",
    "end": "5516120"
  },
  {
    "text": "Which means we can break down the [NOISE] likelihood as a product of n different, different terms.",
    "start": "5516120",
    "end": "5524535"
  },
  {
    "text": "How are we doing with respect to time? We have ten more minutes. With respect to n, and, and, and then what we do is in- instead of,",
    "start": "5524535",
    "end": "5533175"
  },
  {
    "text": "of ah, the likelihood, we take the log likelihood, right?",
    "start": "5533175",
    "end": "5538980"
  },
  {
    "text": "And these is gonna give us [NOISE] log, um, log of that PDF.",
    "start": "5538980",
    "end": "5545250"
  },
  {
    "text": "So ah, the sum i equals 1- 1 to n, We'll call this L theta [NOISE] log of 1 over square root of 2i sigma square",
    "start": "5545250",
    "end": "5560010"
  },
  {
    "text": "[NOISE] x minus of minus",
    "start": "5560010",
    "end": "5566399"
  },
  {
    "text": "half yi minus beta transpose xi",
    "start": "5566399",
    "end": "5574980"
  },
  {
    "text": "squared over sigma squared.",
    "start": "5574980",
    "end": "5579550"
  },
  {
    "text": "Let me try to select more clearly, it's just",
    "start": "5580220",
    "end": "5585850"
  },
  {
    "text": "L theta equals- All right,",
    "start": "5586190",
    "end": "5594000"
  },
  {
    "text": "now, let's take a step back and kind of analyze this.",
    "start": "5594000",
    "end": "5600810"
  },
  {
    "text": "The function over which, the, the, the variable over which, we'll define this as theta and that",
    "start": "5600810",
    "end": "5610470"
  },
  {
    "text": "comes up here, all right?",
    "start": "5610470",
    "end": "5617190"
  },
  {
    "text": "And now wi- ah, with this likelihood function.",
    "start": "5617190",
    "end": "5622815"
  },
  {
    "text": "You can kind of, you know, um, zoom out and look at this expression.",
    "start": "5622815",
    "end": "5628005"
  },
  {
    "text": "And you can see that log and x are  something that cancel out, all right?",
    "start": "5628005",
    "end": "5635415"
  },
  {
    "text": "The, the 2pi sigma squared is just some constant. We are assuming sigma squared is some- just some unknown constant.",
    "start": "5635415",
    "end": "5643785"
  },
  {
    "text": "This is some constant again. We have a minus half square. So all this is going to,",
    "start": "5643785",
    "end": "5650880"
  },
  {
    "text": "you know, the, the, the, um, I guess the summary here is that, you know,",
    "start": "5650880",
    "end": "5657315"
  },
  {
    "text": "by looking at this expression here your attention should naturally just focus on these part, right?",
    "start": "5657315",
    "end": "5664545"
  },
  {
    "text": "Everything else just, just, just, just know, goes away, right? [NOISE] The- you can write this as sum over",
    "start": "5664545",
    "end": "5674099"
  },
  {
    "text": "i equals 1 to n. Log of some constant is some k. We don't care.",
    "start": "5674100",
    "end": "5680535"
  },
  {
    "text": "Log and the exponent cancel out. Minus half sum sigma square, all right?",
    "start": "5680535",
    "end": "5690420"
  },
  {
    "text": "Yi minus theta transpose xi square. That's a half, okay?",
    "start": "5690420",
    "end": "5699600"
  },
  {
    "text": "And so is equal to- I'm gonna take this k out.",
    "start": "5699600",
    "end": "5706710"
  },
  {
    "text": "Or some n times k, um, some constant minus 1 over sigma square,",
    "start": "5706710",
    "end": "5715395"
  },
  {
    "text": "half equals 1 to n,",
    "start": "5715395",
    "end": "5719740"
  },
  {
    "text": "right? So, the likelihood function, the log likelihood function.",
    "start": "5729920",
    "end": "5735840"
  },
  {
    "text": "By making a probabilistic assumption about the noise. Is gonna give us the negative of a scaled version of the original cost function, right?",
    "start": "5735840",
    "end": "5748005"
  },
  {
    "text": "Which means by performing maximum likelihood. We are minimizing the squared error. Does that makes sense?",
    "start": "5748005",
    "end": "5758730"
  },
  {
    "text": "So this, this, this is kind of, um, whenever you see a Gaussian come into picture.",
    "start": "5758730",
    "end": "5765989"
  },
  {
    "text": "You know, this is probably the most important thing. It is the exponent of some squared entity.",
    "start": "5765989",
    "end": "5771060"
  },
  {
    "text": "And when you take the log likelihood, you know, the log and the exponent cancel out and you're gonna be left with just the,",
    "start": "5771060",
    "end": "5776130"
  },
  {
    "text": "the squared term, right? And it's gonna come with a negative sign. And because we're doing maximum likelihood,",
    "start": "5776130",
    "end": "5783165"
  },
  {
    "text": "we're trying to maximize the negative of something. Which means we're trying to minimize these.",
    "start": "5783165",
    "end": "5787780"
  },
  {
    "text": "Yes question. [BACKGROUND] Exactly what we've shown here is that um,",
    "start": "5792380",
    "end": "5802485"
  },
  {
    "text": "is that linear regression can be viewed as performing maximum likelihood.",
    "start": "5802485",
    "end": "5807598"
  },
  {
    "text": "Where the noise that comes into each example is assumed to be a Gaussian noise, right?",
    "start": "5807599",
    "end": "5817680"
  },
  {
    "text": "If you make the assumption that the, the x's and y's have a linear relationship and have an additive Gaussian noise.",
    "start": "5817680",
    "end": "5826035"
  },
  {
    "text": "Then the maximum likelihood theory tells you what you need to do is exactly the same as minimizing the,",
    "start": "5826035",
    "end": "5834585"
  },
  {
    "text": "you know, squared error or ordinary least, least squares. The, the, the two are ah, the two ah, approaches of,",
    "start": "5834585",
    "end": "5841514"
  },
  {
    "text": "you know, defining the cost function and minimizing it. Or giving it a probabilistic setting and maximizing the likelihood,",
    "start": "5841515",
    "end": "5847245"
  },
  {
    "text": "are exactly the same, right? So to, you know,",
    "start": "5847245",
    "end": "5853290"
  },
  {
    "text": "just minimize arg max of L theta was equal",
    "start": "5853290",
    "end": "5860370"
  },
  {
    "text": "to arg min of [NOISE] J theta.",
    "start": "5860370",
    "end": "5868440"
  },
  {
    "text": "There's some extra scaled versions, but we're not interested in the values. But we are interested in the arg max and the arg min, right?",
    "start": "5868440",
    "end": "5874875"
  },
  {
    "text": "And they are exactly the same. Next question. [BACKGROUND] Yeah [BACKGROUND].",
    "start": "5874875",
    "end": "5886370"
  },
  {
    "text": "Yeah. So the question is, if we made an assumption that, uh, Epsilon instead of Gaussian was Poisson,",
    "start": "5886370",
    "end": "5892520"
  },
  {
    "text": "what- what would happen? And for that I would say, wait till- well, we're going to cover GLMs, right?",
    "start": "5892520",
    "end": "5900635"
  },
  {
    "text": "Uh, yes, that's a very good question. And what we're going to see is- is, um, the maximum likelihood, um,",
    "start": "5900635",
    "end": "5907699"
  },
  {
    "text": "interpretation is actually more general and you can- you can, um, um, you can do many more things. Yes, question.",
    "start": "5907700",
    "end": "5914630"
  },
  {
    "text": "[inaudible]",
    "start": "5914630",
    "end": "5919670"
  },
  {
    "text": "So the Epsilon over here is actually the difference between, um, kind of the true y-value and the observed y-value, like, uh,",
    "start": "5919670",
    "end": "5927395"
  },
  {
    "text": "Theta transpose x^i is like, you know, we had like the true price of the house, but what you observe has some noise in it.",
    "start": "5927395",
    "end": "5933545"
  },
  {
    "text": "You know, maybe the mood of the buyer was- was bad that day or there's- there's some noise that gets added into the process.",
    "start": "5933545",
    "end": "5940054"
  },
  {
    "text": "And you're making an assumption that the noise is- is- is Gaussian. Now in the last few minutes that we have,",
    "start": "5940055",
    "end": "5949670"
  },
  {
    "text": "um, I want to provide yet another interpretation of linear regression, right?",
    "start": "5949670",
    "end": "5955850"
  },
  {
    "text": "So we saw, um, linear regression can be solved through gradient descent or through the normal equations.",
    "start": "5955850",
    "end": "5962599"
  },
  {
    "text": "We saw that linear regression is- is actually the same as maximum likelihood if you assume that the noise is Gaussian.",
    "start": "5962600",
    "end": "5971375"
  },
  {
    "text": "Now, here's yet another, uh, view of linear regression, right?",
    "start": "5971375",
    "end": "5977010"
  },
  {
    "text": "Now, if- if, um, what we want to solve is X Theta equals Y, right?",
    "start": "5979150",
    "end": "5989615"
  },
  {
    "text": "Now here, X is a matrix, Theta is a vector and Y is another vector.",
    "start": "5989615",
    "end": "5996065"
  },
  {
    "text": "Right? Now remember the- the, um, the functional view of matrices that we spoke about earlier?",
    "start": "5996065",
    "end": "6003310"
  },
  {
    "text": "So you can imagine, um, an input space,",
    "start": "6003310",
    "end": "6007400"
  },
  {
    "text": "Theta_1, Theta_2, Theta_d.",
    "start": "6011520",
    "end": "6017245"
  },
  {
    "text": "You have a matrix X, and you have an output space, y_1,",
    "start": "6017245",
    "end": "6031575"
  },
  {
    "text": "y_2, y_n. Now, let me intentionally keep this two-dimensional.",
    "start": "6031575",
    "end": "6038070"
  },
  {
    "text": "So we have Theta_1 to- through Theta_d.",
    "start": "6038070",
    "end": "6042010"
  },
  {
    "text": "Right? What this means is we have some Theta- some- some parameter space Theta.",
    "start": "6045260",
    "end": "6053329"
  },
  {
    "text": "That's the input space to the matrix X. X is the design matrix, right?",
    "start": "6053730",
    "end": "6059575"
  },
  {
    "text": "Now the rows of your- the data is actually in X not in Theta, right? And y is- is- is the corresponding,",
    "start": "6059575",
    "end": "6067450"
  },
  {
    "text": "um, um, y labels. Now what we want is to find Theta that-",
    "start": "6067450",
    "end": "6072550"
  },
  {
    "text": "that makes our output as close to y as possible, right?",
    "start": "6072550",
    "end": "6078565"
  },
  {
    "text": "We also saw in- when we were lil- uh, reviewing linear algebra, that if X is not full rank,",
    "start": "6078565",
    "end": "6085870"
  },
  {
    "text": "then there exists a subspace in X, a low-dimensional subspace that has a one-to-one mapping between the input and the output space, right?",
    "start": "6085870",
    "end": "6094809"
  },
  {
    "text": "Now- for now, let's assume X is full rank.",
    "start": "6094810",
    "end": "6100030"
  },
  {
    "text": "Which means if X is in- is in d by n and d is the smaller dimension,",
    "start": "6100030",
    "end": "6105820"
  },
  {
    "text": "then assume that X has rank d, right? And Y is in an n-dimensional space.",
    "start": "6105820",
    "end": "6113034"
  },
  {
    "text": "So n is generally much larger, right? N is much larger and the given-",
    "start": "6113035",
    "end": "6119410"
  },
  {
    "text": "so the training set that we are given is represented by this matrix X and some point in this output space of ys.",
    "start": "6119410",
    "end": "6131844"
  },
  {
    "text": "Where this point, remember ys are all scalars. So each dimension is representing a different example, right?",
    "start": "6131845",
    "end": "6141219"
  },
  {
    "text": "It's an n-dimensional space where each dimension represents a different example. And there's gonna be some subspace in- in",
    "start": "6141220",
    "end": "6151360"
  },
  {
    "text": "the output space that passes through the origin,",
    "start": "6151360",
    "end": "6159100"
  },
  {
    "text": "which- for which there is a bijection between Theta and the subspace.",
    "start": "6159100",
    "end": "6164890"
  },
  {
    "text": "And also the set of all points in the output space that can be reached by Theta is limited to that subspace,",
    "start": "6164890",
    "end": "6173199"
  },
  {
    "text": "right? We remember that? Now we also spoke about projections, right?",
    "start": "6173200",
    "end": "6184635"
  },
  {
    "text": "Now, if you have a matrix X whose columns define a set of basis onto which we want to project something.",
    "start": "6184635",
    "end": "6194875"
  },
  {
    "text": "Right? The prediction matrix was anybody remembers? X transpose, X transpose,",
    "start": "6194875",
    "end": "6203380"
  },
  {
    "text": "wait, X inverse, X transpose.",
    "start": "6203380",
    "end": "6209050"
  },
  {
    "text": "This was the projection matrix. So this subspace defines the set of all points that can",
    "start": "6209050",
    "end": "6214690"
  },
  {
    "text": "be reached through linear combinations of X, right? And those columns are, you know,",
    "start": "6214690",
    "end": "6220810"
  },
  {
    "text": "are basically the columns over here, right? And the projection matrix of this output subspace is this, right?",
    "start": "6220810",
    "end": "6230395"
  },
  {
    "text": "Now, which means the given y, supposing we are given some value y?",
    "start": "6230395",
    "end": "6236665"
  },
  {
    "text": "Most likely it's not gonna reside in the subspace. It's because, you know, the- the- the space- the n-dimensional space is so",
    "start": "6236665",
    "end": "6243190"
  },
  {
    "text": "much larger and the d-dimensional space- d is so much smaller than n, that very likely the y that we observe is not gonna",
    "start": "6243190",
    "end": "6250630"
  },
  {
    "text": "lie in the space exactly- in the subspace exactly, right? And what we wanna do [NOISE] is now project this",
    "start": "6250630",
    "end": "6258580"
  },
  {
    "text": "point onto the subspace such that it's perpendicular, right?",
    "start": "6258580",
    "end": "6265510"
  },
  {
    "text": "So this is our observed vector of ys that we want to project onto the subspace that- that is reachable through X, right?",
    "start": "6265510",
    "end": "6275665"
  },
  {
    "text": "And the- the- the projection matrix is gonna be this. And what- the- the point onto which it gets projected will have",
    "start": "6275665",
    "end": "6287364"
  },
  {
    "text": "a one-to-one mapping with some point over here, right?",
    "start": "6287365",
    "end": "6296275"
  },
  {
    "text": "So which means X Theta hat is gonna be equal to x,",
    "start": "6296275",
    "end": "6307030"
  },
  {
    "text": "x transpose, x inverse, x transpose, y.",
    "start": "6307030",
    "end": "6313914"
  },
  {
    "text": "Well, this is the projection matrix, this the vector that we wanna project, right?",
    "start": "6313915",
    "end": "6319014"
  },
  {
    "text": "And this basically kind of is another way of seeing that, you know, Theta hat equals x transpose, x inverse,",
    "start": "6319015",
    "end": "6326364"
  },
  {
    "text": "x transpose, y, right?",
    "start": "6326365",
    "end": "6334900"
  },
  {
    "text": "Does it make sense? So this is the projection matrix that will",
    "start": "6334900",
    "end": "6340659"
  },
  {
    "text": "take any vector in the output space and project it onto the column space of- of X.",
    "start": "6340660",
    "end": "6346495"
  },
  {
    "text": "And we take the vector y and project it onto the column space of X.",
    "start": "6346495",
    "end": "6352660"
  },
  {
    "text": "And once it's in the column space of X, there will exist some- made some- some- some vector in the input space that,",
    "start": "6352660",
    "end": "6361570"
  },
  {
    "text": "[NOISE] you know because there is a bijection between the- the- the- the column space and the row space,",
    "start": "6361570",
    "end": "6366985"
  },
  {
    "text": "which means X Theta hat equals some- some value,",
    "start": "6366985",
    "end": "6373315"
  },
  {
    "text": "will have an inverse because there's a one-to-one bijection that exists. And that inverse is- is,",
    "start": "6373315",
    "end": "6378355"
  },
  {
    "text": "you know, because- because now, you know, X is now kind of invertible because it's in that space where the bijection exists.",
    "start": "6378355",
    "end": "6385420"
  },
  {
    "text": "Theta hat is equal to x transpose- X equals x, x transpose, x inverse, x transpose y.",
    "start": "6385420",
    "end": "6391690"
  },
  {
    "text": "And so basically what linear reg- regression is doing is projecting the y-values onto the subspace reachable through X,",
    "start": "6391690",
    "end": "6400075"
  },
  {
    "text": "and then solving- um, um, finding Theta hat, you know, corresponding to that projection. Yes, question?",
    "start": "6400075",
    "end": "6408895"
  },
  {
    "text": "How do we know it's the optimal? I'm sorry. How do we know it's the optimal Theta? So ho- how do we know it's the optimal Theta?",
    "start": "6408895",
    "end": "6415330"
  },
  {
    "text": "Um- here, optimal in the sense, um, so- so we- we- when we- when we spoke about, uh,",
    "start": "6415330",
    "end": "6424920"
  },
  {
    "text": "projections, we discussed this interpretation that the point to which you get projected is in a way,",
    "start": "6424920",
    "end": "6431715"
  },
  {
    "text": "the closest possible point to the true vector, right? So in that way, you're trying to find the Theta that will- that",
    "start": "6431715",
    "end": "6439060"
  },
  {
    "text": "will take you to a point that is closest to y, right? And basically, Pythagoras's theorem tells you that",
    "start": "6439060",
    "end": "6445900"
  },
  {
    "text": "the square of this distance is equal to the sum of the squares of, you know, the each components and that,",
    "start": "6445900",
    "end": "6451615"
  },
  {
    "text": "you know, um, turns out to be exactly equal this. So Pythagoras theorem tells you that [NOISE] this is equal to the square of",
    "start": "6451615",
    "end": "6457330"
  },
  {
    "text": "this residual, right? All right. With that we will- we will conclude,",
    "start": "6457330",
    "end": "6464020"
  },
  {
    "text": "uh, our, um, um, our survey of, uh, linear regression and next we gonna to start with classification.",
    "start": "6464020",
    "end": "6474980"
  }
]