[
  {
    "text": "Hello everyone. Uh, so my name is Raphael Townshend.",
    "start": "3470",
    "end": "9060"
  },
  {
    "text": "I'm one of the head TAs for this class. This week Andrew is traveling and my advisor is still dealing with medical issues.",
    "start": "9060",
    "end": "15570"
  },
  {
    "text": "So I'm going to be giving today's lecture. Um, you heard from my wonderful co-head TA Anand a couple of weeks ago.",
    "start": "15570",
    "end": "21785"
  },
  {
    "text": "And so today, we're gonna be going over decision trees and various ensemble methods.",
    "start": "21785",
    "end": "27279"
  },
  {
    "text": "Uh, so these might seem a bit like disparate topics at first, but really decision trees are, sort of, a classical example model class to use with various ensembling methods.",
    "start": "27280",
    "end": "36710"
  },
  {
    "text": "We're gonna get into a little bit why in a bit, but just to give you guys an overview of what the outlines can be.",
    "start": "36710",
    "end": "42270"
  },
  {
    "text": "We're first gonna go over decision trees, then we're gonna go over general ensembling methods and then go specifically into bagging random forests and boosting.",
    "start": "42270",
    "end": "49415"
  },
  {
    "text": "Okay. So let's get started. So first let's cover some decision trees.",
    "start": "49415",
    "end": "57070"
  },
  {
    "text": "Okay. So last week, Andrew was covering SVNs that are, sort of, one of the classical linear models and,",
    "start": "64910",
    "end": "72494"
  },
  {
    "text": "sort of, brought to a close a lot of discussion of those linear models. And so today we gonna be getting to decision trees which is really",
    "start": "72495",
    "end": "78125"
  },
  {
    "text": "one of our first examples of a non-linear model. And so to motivate these guys let me give you guys an example.",
    "start": "78125",
    "end": "84520"
  },
  {
    "text": "Okay. So I'm Canadian, I really like to ski. So I'm gonna motivate it using that.",
    "start": "84520",
    "end": "90235"
  },
  {
    "text": "So pretend you have a classifier that given a time and a location tells you whether or not you can ski,",
    "start": "90235",
    "end": "96930"
  },
  {
    "text": "so it's a binary classifier saying yes or no. And so you have, you can imagine, a graph like this,",
    "start": "96930",
    "end": "103560"
  },
  {
    "text": "and on the x-axis we're gonna have time in months, so counting from the start.",
    "start": "103560",
    "end": "110110"
  },
  {
    "text": "So starting at 1 for January to 12 for December, and then on the y-axis we're gonna use latitude in degrees, okay?",
    "start": "112480",
    "end": "122385"
  },
  {
    "text": "And so for those of you who might have forgotten what latitude is, it's basically at positive 90 degrees you're at the North Pole,",
    "start": "122385",
    "end": "129560"
  },
  {
    "text": "at negative 90 degrees you're at the South Pole. So positive 90, negative 90,",
    "start": "129560",
    "end": "137984"
  },
  {
    "text": "0 being the Equator and it's, sort of, your location along the north-south axis.",
    "start": "137984",
    "end": "142985"
  },
  {
    "text": "Okay. So given this, if you might recall, the winter in the Northern Hemisphere generally happens in the early months of the year.",
    "start": "142985",
    "end": "152360"
  },
  {
    "text": "So you might see that you can ski in these early months over here and it has some positive data points, and then in the later months,",
    "start": "152360",
    "end": "158629"
  },
  {
    "text": "right, and then in the middle, you can't really ski.",
    "start": "158630",
    "end": "165255"
  },
  {
    "text": "Versus in the Southern Hemisphere, it's basically flipped, where you can not ski in the early months.",
    "start": "165255",
    "end": "171620"
  },
  {
    "text": "You can ski during the May, June, July, August time period,",
    "start": "171620",
    "end": "177120"
  },
  {
    "text": "and then you can't ski in the earlier months, and then the equator in general is just not great for skiing that's the reason I don't live there,",
    "start": "177120",
    "end": "183995"
  },
  {
    "text": "and so you just have a bunch of negatives here. Okay. And so when you look at a data set like this,",
    "start": "183995",
    "end": "191530"
  },
  {
    "text": "you've sort of got these separate regions that you're looking at, right? And you sort of want to isolate out those regions of positive examples.",
    "start": "191530",
    "end": "197675"
  },
  {
    "text": "If you had a linear classifier, you'd sort of be hard-pressed to come up with any sort of decision boundary that would separate this reasonably.",
    "start": "197675",
    "end": "203870"
  },
  {
    "text": "Now you could think okay, maybe you have an SVM or something, you've come up with a kernel that could perh- perhaps project this into a higher feature space that would make it linearly separable,",
    "start": "203870",
    "end": "212840"
  },
  {
    "text": "but it turns out that with decision trees, you have a very natural way to do this.",
    "start": "212840",
    "end": "218050"
  },
  {
    "text": "So to sort of make clear exactly what we want to do with decision trees, is we wanna sort of partition the space into individual regions.",
    "start": "218050",
    "end": "226200"
  },
  {
    "text": "So we sort of wanna isolate out these, like, positive examples, for example. In general this problem is fairly intractable just coming up with the optimal regions.",
    "start": "226200",
    "end": "234950"
  },
  {
    "text": "But how we do with decision trees is we do it in this basically greedy,",
    "start": "234950",
    "end": "241110"
  },
  {
    "text": "top-down, recursive manner,",
    "start": "241960",
    "end": "250560"
  },
  {
    "text": "and this is going to be recursive partitioning.",
    "start": "252080",
    "end": "256360"
  },
  {
    "text": "Okay? And so it's- basically it's top-down because we're starting with the overall region and we wanna slowly partition it up, okay?",
    "start": "259900",
    "end": "268970"
  },
  {
    "text": "And then it's greedy because at each step we wanna pick the best partition possible. Okay. So let's actually try and work out intuitively what a decision tree would do, okay?",
    "start": "268970",
    "end": "279440"
  },
  {
    "text": "So what we do is we start with the overall space and the tree is basically gonna play 20 Questions with this space.",
    "start": "279440",
    "end": "285970"
  },
  {
    "text": "Okay. So like for example, one question it might ask is, if we have the data coming in like this, is,",
    "start": "285970",
    "end": "292535"
  },
  {
    "text": "is the latitude greater than 30 degrees, okay?",
    "start": "292535",
    "end": "298500"
  },
  {
    "text": "And that would involve, sort of, cutting the space like this, for example, okay?",
    "start": "298510",
    "end": "303545"
  },
  {
    "text": "And then we'd have a yes or a no.",
    "start": "303545",
    "end": "307890"
  },
  {
    "text": "And so starting from, like, the most general space now we have partitioned the overall space into two separate spaces using this, this question.",
    "start": "308780",
    "end": "318200"
  },
  {
    "text": "Okay. And this is where the recursive part comes in now, because now that you've sort of split the space into two,",
    "start": "318200",
    "end": "324270"
  },
  {
    "text": "you can then sort of treat each individual space as a new problem to ask a new question about.",
    "start": "324270",
    "end": "329914"
  },
  {
    "text": "So for example now that you've asked this latitude greater than 30 question, you could then ask something like,",
    "start": "329915",
    "end": "335045"
  },
  {
    "text": "month less than like March or something like that.",
    "start": "335045",
    "end": "340975"
  },
  {
    "text": "All right, and that would give you a yes or no. And what that works out to effectively,",
    "start": "340975",
    "end": "347050"
  },
  {
    "text": "is that now you've taken this upper space here and divided it up into these two separate regions like this.",
    "start": "347050",
    "end": "355595"
  },
  {
    "text": "And so you could imagine how through asking these recursive questions over and over again,",
    "start": "355595",
    "end": "360670"
  },
  {
    "text": "you could start splitting up the entire space into your individual regions like this.",
    "start": "360670",
    "end": "366350"
  },
  {
    "text": "Okay, and so to make this a little bit more formal, what we're looking for is,",
    "start": "368360",
    "end": "374919"
  },
  {
    "text": "we're looking for, sort of, the split function, okay? So you can, sort of, define a region. So you have a region and let's call that region R_p in this case for R parent, okay?",
    "start": "374920",
    "end": "389130"
  },
  {
    "text": "And we're looking for, looking for a split S_p,",
    "start": "389130",
    "end": "403710"
  },
  {
    "text": "such that you have an S_p, you can, sort of, write out this S_p function as a function of j,t.",
    "start": "403710",
    "end": "412035"
  },
  {
    "text": "Okay, where we saw you- j is which feature number and T is the threshold you're using.",
    "start": "412035",
    "end": "418240"
  },
  {
    "text": "And so you can, sort of, write this out formally as, sort of, you're outputting a tuple, where on the one hand you have a set X where you",
    "start": "418240",
    "end": "427290"
  },
  {
    "text": "have the x j- the jth feature of x is less than the threshold,",
    "start": "427290",
    "end": "432675"
  },
  {
    "text": "and you have Xs element of R-p, since we're only partitioning that parent region.",
    "start": "432675",
    "end": "438349"
  },
  {
    "text": "And then the second set is literally the same thing, except it's just those that are greater than t. And so we can refer",
    "start": "439110",
    "end": "453850"
  },
  {
    "text": "to each one of these as R_1 and R_2.",
    "start": "453850",
    "end": "463990"
  },
  {
    "text": "Any questions so far? No? Okay. So we,",
    "start": "463990",
    "end": "470250"
  },
  {
    "text": "sort of, define now how we would, sort of, do this. We're trying to, like, greedily pick these peaks that are partitioning our input space and the splits are,",
    "start": "470250",
    "end": "477690"
  },
  {
    "text": "sort of, defined by which feature you're looking at and the threshold that you're applying to that feature.",
    "start": "477690",
    "end": "483545"
  },
  {
    "text": "Uh, sort of a natural question to ask now is, is how do you choose these splits,",
    "start": "483545",
    "end": "490100"
  },
  {
    "text": "right? And so I sort of gave this intuitive explanation, that really what you're trying to do is you're trying to isolate out",
    "start": "502370",
    "end": "509139"
  },
  {
    "text": "the space of positives and negatives in this case. And so what is useful to define is a loss on a region, okay?",
    "start": "509140",
    "end": "518250"
  },
  {
    "text": "So define your loss L on R,",
    "start": "518250",
    "end": "525125"
  },
  {
    "text": "loss on R. And",
    "start": "525125",
    "end": "533920"
  },
  {
    "text": "so for now let's define our loss as something fairly obvious, is your misclassification loss.",
    "start": "533920",
    "end": "539665"
  },
  {
    "text": "It's how many examples in your region you get wrong. And so assuming that you have, uh,",
    "start": "539665",
    "end": "546645"
  },
  {
    "text": "given C classes total,",
    "start": "546645",
    "end": "552530"
  },
  {
    "text": "you can define P hat c",
    "start": "554370",
    "end": "566410"
  },
  {
    "text": "to be the proportion",
    "start": "566410",
    "end": "568819"
  },
  {
    "text": "of examples in R",
    "start": "575490",
    "end": "584274"
  },
  {
    "text": "that are of class c. [NOISE]",
    "start": "584275",
    "end": "594060"
  },
  {
    "text": "And so now that we've got this definition, where we had this p hat c of telling us the proportion of examples that we've gotten that case,",
    "start": "594060",
    "end": "600645"
  },
  {
    "text": "you can try to define the loss of any region as loss,",
    "start": "600645",
    "end": "606150"
  },
  {
    "text": "let's call it misclassification, it's just 1 minus max over c of p hat c, okay?",
    "start": "606150",
    "end": "620640"
  },
  {
    "text": "And so the reasoning behind this is basically you can say that for any region that you've subdivided generally,",
    "start": "620640",
    "end": "626550"
  },
  {
    "text": "what you'll want to do is predict the most common class there, which is just the maximum p hat c, right?",
    "start": "626550",
    "end": "632875"
  },
  {
    "text": "And so then all the remaining probability just gets thrown onto misclassification errors, okay?",
    "start": "632875",
    "end": "639270"
  },
  {
    "text": "And so then once we do this, we want to basically pick- now that we have a loss defined,",
    "start": "639270",
    "end": "645464"
  },
  {
    "text": "we want to, um, pick a split that decreases the loss as much as possible.",
    "start": "645464",
    "end": "651630"
  },
  {
    "text": "So you recall I've defined this region, R_parent, and then these two children regions R_1 and R_2.",
    "start": "651630",
    "end": "658245"
  },
  {
    "text": "And you basically want to reduce that loss as much as possible. So you want to, um,",
    "start": "658245",
    "end": "664515"
  },
  {
    "text": "basically minimize loss,",
    "start": "664515",
    "end": "671175"
  },
  {
    "text": "R_parent minus loss of R_1 plus loss of R_2.",
    "start": "671175",
    "end": "679690"
  },
  {
    "text": "And so this is sort of your parent loss,",
    "start": "682130",
    "end": "687310"
  },
  {
    "text": "this is your children loss.",
    "start": "691310",
    "end": "694630"
  },
  {
    "text": "Okay. And since you're picking- and basically what you're minimizing over in some case is this j, t that we",
    "start": "697370",
    "end": "704940"
  },
  {
    "text": "defined over here since this split is really what is gonna define our two children regions, right.",
    "start": "704940",
    "end": "711975"
  },
  {
    "text": "And what you'll notice is that the loss of the parent doesn't really matter in this case because that's already defined.",
    "start": "711975",
    "end": "717029"
  },
  {
    "text": "So really all you're trying to do is minimize this negative sum of losses of your children, okay?",
    "start": "717030",
    "end": "724930"
  },
  {
    "text": "So let's move to the next board here. [NOISE]",
    "start": "727150",
    "end": "747180"
  },
  {
    "text": "So I started to find this misclassification loss. Let's get a little bit into actually why misclassification loss isn't actually the right loss to use for this problem, so,",
    "start": "747180",
    "end": "756310"
  },
  {
    "text": "okay? And so for a simple example, let's pretend- So I've sort of drawn out a tree like this,",
    "start": "770780",
    "end": "777075"
  },
  {
    "text": "Let's pretend that instead we have another setup here where we're coming into a decision node.",
    "start": "777075",
    "end": "784650"
  },
  {
    "text": "And at this point we have 900 positives and 100 negatives, okay?",
    "start": "784650",
    "end": "790605"
  },
  {
    "text": "So this is sort of a misclassification loss, of 100 in this case because you'd predict the most common class and end up with 100 misclassified examples.",
    "start": "790605",
    "end": "799335"
  },
  {
    "text": "All right, and so this would be your region R_p right now, right? And so then you can split it into these two other regions, right?",
    "start": "799335",
    "end": "807870"
  },
  {
    "text": "Say R_1 and R_2.",
    "start": "807870",
    "end": "811000"
  },
  {
    "text": "And say that what you've achieved now is you have the 700 positive,",
    "start": "814040",
    "end": "819134"
  },
  {
    "text": "100 negatives on this side versus, uh, 200 positives and 0 negatives on this side, okay?",
    "start": "819135",
    "end": "829390"
  },
  {
    "text": "Now, this seems like a pretty good split since you're getting out some more examples.",
    "start": "829730",
    "end": "834795"
  },
  {
    "text": "But what you can see is that, if you just drew the same thing again, right, R_p with 900 and 100,",
    "start": "834795",
    "end": "842620"
  },
  {
    "text": "split split, and say in this case, instead,",
    "start": "844670",
    "end": "851385"
  },
  {
    "text": "you've got 400 positives over here, 100 negatives, and 500 positives and 0 negatives.",
    "start": "851385",
    "end": "862305"
  },
  {
    "text": "So most people would argue that this bright decision boundary is better than the left one because you're basically isolating out even more positives in this case.",
    "start": "862305",
    "end": "870540"
  },
  {
    "text": "However, if you're just looking at your misclassification loss, it turns out that on this left one here,",
    "start": "870540",
    "end": "875970"
  },
  {
    "text": "let's call this R_1 and R_2 versus this right one, Let's call this R_1 prime, R_2 prime, okay?",
    "start": "875970",
    "end": "881880"
  },
  {
    "text": "So your loss of R_1 plus R_2,",
    "start": "881880",
    "end": "887650"
  },
  {
    "text": "on this left case it's just 100 plus 0. All right, so it's just 100.",
    "start": "887840",
    "end": "893685"
  },
  {
    "text": "And then on the right side here, it's actually still just the same, right?",
    "start": "893685",
    "end": "904335"
  },
  {
    "text": "And in fact, if you'd look at the original loss of your parent it's also just 100, right?",
    "start": "904335",
    "end": "909899"
  },
  {
    "text": "So you haven't really according to this loss metric changed anything at all. And so that sort of brings up one problem with the misclassification loss is that,",
    "start": "909900",
    "end": "917820"
  },
  {
    "text": "it's not really sensitive enough, okay? So like instead what we can do is we can define this cross-entropy loss, okay?",
    "start": "917820",
    "end": "926730"
  },
  {
    "text": "So which we'll define as L_cross.",
    "start": "926730",
    "end": "948459"
  },
  {
    "text": "Let me just write this out here.",
    "start": "951830",
    "end": "955090"
  },
  {
    "text": "And so really what you're doing is you're just summing over the classes and it's the probability- that the proportion of elements in that class",
    "start": "960650",
    "end": "967650"
  },
  {
    "text": "times the log of the proportion in that class. And how you can think of this is, It's sort of this concept that we borrow from information theory,",
    "start": "967650",
    "end": "975180"
  },
  {
    "text": "which is sort of like the number of bits you need to communicate to tell someone who already knows what the probabilities are what class you are looking at.",
    "start": "975180",
    "end": "983010"
  },
  {
    "text": "And so that sounds like a mouthful but really you can sort of think of it intuitively as, if someone already knows the probabilities,",
    "start": "983010",
    "end": "989339"
  },
  {
    "text": "like say it's a 100% chance that it is of one class, then you don't need to communicate anything to tell",
    "start": "989340",
    "end": "995115"
  },
  {
    "text": "them exactly which class it is because it's obvious that it is that one class versus if you have a fairly even split then you'd need to communicate",
    "start": "995115",
    "end": "1001490"
  },
  {
    "text": "a lot more information to tell someone exactly what class you were in.",
    "start": "1001490",
    "end": "1006995"
  },
  {
    "text": "Any questions so far? Yeah? [inaudible].",
    "start": "1006995",
    "end": "1017750"
  },
  {
    "text": "The R_1, R_2 for the parent class? [inaudible].",
    "start": "1017750",
    "end": "1025579"
  },
  {
    "text": "For this case here? Yeah, yeah, so, um, for that case there so you see that, er,",
    "start": "1025580",
    "end": "1031939"
  },
  {
    "text": "I'll try and reach up there, but so it's like say like R_p was your start region, right? You could say it's the overall region, right?",
    "start": "1031940",
    "end": "1039110"
  },
  {
    "text": "And then R_1 would be all the points above this latitude 30 line. And R_2 would be all the points below the latitude 30 line.",
    "start": "1039110",
    "end": "1048329"
  },
  {
    "text": "Yeah, yeah? [inaudible].",
    "start": "1048460",
    "end": "1059630"
  },
  {
    "text": "Yeah. So the question is, when you're trying to minimize this loss here, is it the same as maximizing the,",
    "start": "1059630",
    "end": "1065780"
  },
  {
    "text": "the children loss, and since, er - no, uh, ah, let's see, of maximizing the children loss.",
    "start": "1065780",
    "end": "1071660"
  },
  {
    "text": "And yeah, it turns out it doesn't really matter, which, um, which way you put it. It just- basically, you're trying to either minimize the loss of",
    "start": "1071660",
    "end": "1078919"
  },
  {
    "text": "the children or maximize the gain in information, basically. [inaudible].",
    "start": "1078920",
    "end": "1095529"
  },
  {
    "text": "Yeah. Let's see. Yeah, you're right. That should actually be a max. Let me fix that really quick.",
    "start": "1095530",
    "end": "1103470"
  },
  {
    "text": "Because you start with your parent loss, and then you're subtracting out your children's loss,",
    "start": "1110680",
    "end": "1116149"
  },
  {
    "text": "and so the amount left, let's see, the higher this loss is- yeah. So you really want to maximize this guy.",
    "start": "1116150",
    "end": "1122850"
  },
  {
    "text": "Makes sense, everyone? Thanks for that.",
    "start": "1125050",
    "end": "1130140"
  },
  {
    "text": "Okay, so I've sort of given this like, hand-wavy- Oh, sure, what's up? [inaudible].",
    "start": "1142390",
    "end": "1150680"
  },
  {
    "text": "So that would be log-based. The question is, for the cross-entropy loss, is it log base 2 or log base c? It's log base 2.",
    "start": "1150680",
    "end": "1156649"
  },
  {
    "text": "Okay, here, I can write that out. Yep.",
    "start": "1156650",
    "end": "1164750"
  },
  {
    "text": "[inaudible]. Oh, sorry, I didn't quite hear that.",
    "start": "1164750",
    "end": "1169970"
  },
  {
    "text": "[inaudible].",
    "start": "1169970",
    "end": "1179299"
  },
  {
    "text": "Okay. Um, so the question is can- uh, what is the proportion that are correct versus incorrect",
    "start": "1179300",
    "end": "1184910"
  },
  {
    "text": "for these two examples we've worked through here? Um, and so, yeah- basically, what we're starting with is,",
    "start": "1184910",
    "end": "1191180"
  },
  {
    "text": "we're starting with we have 900/100, 900 positives and 100 negatives. All right, so you can imagine that if you just stopped at this point, right,",
    "start": "1191180",
    "end": "1198259"
  },
  {
    "text": "you would just cla- classify everything as positive, right, and so you get 100 negatives incorrect.",
    "start": "1198260",
    "end": "1204440"
  },
  {
    "text": "Does that make sense? Because this is 900 positives and 100 negatives. So if you just stopped here and just tried to classify,",
    "start": "1204440",
    "end": "1211220"
  },
  {
    "text": "given this whole region R_p, you would end up getting 10% of your examples wrong, right?",
    "start": "1211220",
    "end": "1218570"
  },
  {
    "text": "In this case, we're sort of talking- we're not talking about percentages, we're talking about absolute number of examples that we've gotten wrong,",
    "start": "1218570",
    "end": "1224840"
  },
  {
    "text": "but you can also definitely talk in terms of percentages instead. And then down here, once you've split it,",
    "start": "1224840",
    "end": "1231380"
  },
  {
    "text": "right, now you've got these two subregions, right? And for- on this, on this left one here,",
    "start": "1231380",
    "end": "1237110"
  },
  {
    "text": "you still have more positives than negatives, right? So you're still gonna classify positive in this leaf, right?",
    "start": "1237110",
    "end": "1244025"
  },
  {
    "text": "And you're still gonna classify positive in this leaf, too, because they- they're both majority class,",
    "start": "1244025",
    "end": "1251195"
  },
  {
    "text": "or the positives are still the majority class there. And in this case, since you have 0 negatives, you're not gonna make any errors in your classification,",
    "start": "1251195",
    "end": "1257600"
  },
  {
    "text": "whereas in this case here, it's still going to make 100 errors. And so what I'm saying is that, at this level, so if we just look above this line at R_p,",
    "start": "1257600",
    "end": "1265700"
  },
  {
    "text": "right, you're making 100 mistakes, and then below this line you're still making 100 mistakes.",
    "start": "1265700",
    "end": "1270845"
  },
  {
    "text": "So what I'm saying is that, that the loss in this case is not very informative. [inaudible].",
    "start": "1270845",
    "end": "1278690"
  },
  {
    "text": "Um, so this, this p-hat- okay, I'm being a little bit loose with terminology with the notation here,",
    "start": "1278690",
    "end": "1283850"
  },
  {
    "text": "but the p-hat in this case is a proportion, okay? But you can also easily- basically,",
    "start": "1283850",
    "end": "1288950"
  },
  {
    "text": "it's like whether you're normalizing the whole thing or not. Yeah. Okay. So I've",
    "start": "1288950",
    "end": "1298640"
  },
  {
    "text": "sort of given this a bit handwavy explanation as to why misclassification loss versus cross-entropy loss might be better or worse.",
    "start": "1298640",
    "end": "1304790"
  },
  {
    "text": "Um, we can actually get a fairly good intuition for why this is the case by looking at it from a sort of geometric perspective.",
    "start": "1304790",
    "end": "1312649"
  },
  {
    "text": "So pretend now that you have this, this plot, okay?",
    "start": "1312650",
    "end": "1320345"
  },
  {
    "text": "And what you're plotting here is- pretend you have a binary classification problem, okay? So you have just- is it positive class or negative class, okay?",
    "start": "1320345",
    "end": "1328550"
  },
  {
    "text": "And so you can sort of represent, say p-hat, as like the proportion of positives in your set, okay?",
    "start": "1328550",
    "end": "1335320"
  },
  {
    "text": "And what you've got plotted up here is your loss. Okay. For cross-entropy loss,",
    "start": "1335320",
    "end": "1342380"
  },
  {
    "text": "where your curve is gonna end up looking like is, is gonna end up looking like this strictly concave curve like this, okay?",
    "start": "1342380",
    "end": "1349160"
  },
  {
    "text": "And what you can do is you can sort of look at where your children versus your parent would fall on this curve.",
    "start": "1349160",
    "end": "1356945"
  },
  {
    "text": "So say that you have two children, okay? You have one up here, so like let's call this LR1.",
    "start": "1356945",
    "end": "1363635"
  },
  {
    "text": "And you have one down here, LR2, okay?",
    "start": "1363635",
    "end": "1369680"
  },
  {
    "text": "And say that you have an equal number of examples in both R1 and R2, so they're equally weighted.",
    "start": "1369680",
    "end": "1376010"
  },
  {
    "text": "If you take- when you're looking at the overall loss between the two, right, that's really just the average of the two.",
    "start": "1376010",
    "end": "1382340"
  },
  {
    "text": "So you can draw a line between these two, and the midpoint turns out to be the average of your two losses.",
    "start": "1382340",
    "end": "1389929"
  },
  {
    "text": "So this is LR1 plus LR2 divided by 2.",
    "start": "1389930",
    "end": "1398910"
  },
  {
    "text": "That's for this guy, okay? And what you can notice is that, in fact,",
    "start": "1402850",
    "end": "1409880"
  },
  {
    "text": "the loss of the parent node is actually just this point projected upwards here,",
    "start": "1409880",
    "end": "1415415"
  },
  {
    "text": "so this would be your LR parent. And this difference right here,",
    "start": "1415415",
    "end": "1421865"
  },
  {
    "text": "this difference, is sort of your change in loss.",
    "start": "1421865",
    "end": "1428880"
  },
  {
    "text": "Does this makes sense? Any questions?",
    "start": "1436240",
    "end": "1440970"
  },
  {
    "text": "Okay. So we have this- just to recap, okay. So we have- say, we have two children regions, right?",
    "start": "1443140",
    "end": "1449540"
  },
  {
    "text": "And they have different probabilities of positive examples occurring, right? They sort of would fall- one would fall on this point on the curve, and say,",
    "start": "1449540",
    "end": "1458149"
  },
  {
    "text": "the other one falls on this point on the curve, then the average of the two losses sort of falls on the midpoint between these two original losses.",
    "start": "1458150",
    "end": "1464975"
  },
  {
    "text": "And if you look at the parent, it's really just halfway between on the x-axis,",
    "start": "1464975",
    "end": "1470270"
  },
  {
    "text": "and you can project up towards for that as well, and you end up with the loss of R_parent. What's up? [inaudible].",
    "start": "1470270",
    "end": "1484355"
  },
  {
    "text": "Okay. So what we're looking at here is we're looking at the cross entropy loss. So you've got this function here, this L cross entropy right,",
    "start": "1484355",
    "end": "1491150"
  },
  {
    "text": "and that's in terms of p-hat c's, right? In this case here, we're just assuming that we have two classes, okay?",
    "start": "1491150",
    "end": "1498350"
  },
  {
    "text": "So what we're doing is we're just modifying the p-hat c, we're, we're changing that on the x-axis and then we're looking at what",
    "start": "1498350",
    "end": "1505190"
  },
  {
    "text": "the response of the overall loss function is on the y-axis. And so what I just did here is for any- this curve just represents for any p-hat c,",
    "start": "1505190",
    "end": "1514595"
  },
  {
    "text": "what the cross entropy loss would look like. Okay. And so we can come back to this, for example, right?",
    "start": "1514595",
    "end": "1521000"
  },
  {
    "text": "And if we look at this parent here right, this guy has a 10%, right? It's sort of like p-hat,",
    "start": "1521000",
    "end": "1528275"
  },
  {
    "text": "p-hat for this guy is 0.1, it's 10% basically or,",
    "start": "1528275",
    "end": "1534575"
  },
  {
    "text": "or I guess no, in this case, would be 0.9 sorry. And then versus here, in these two cases, right,",
    "start": "1534575",
    "end": "1540860"
  },
  {
    "text": "your p-hat, in this case, is 1 since you've got them all right, all right, and then, in this case,",
    "start": "1540860",
    "end": "1547445"
  },
  {
    "text": "it's 0.8, okay? So you can sort of see since these are equal, there's the same number of examples in both of these,",
    "start": "1547445",
    "end": "1554389"
  },
  {
    "text": "the p-hat of the parent is just the average of the p-hat's of the children. Okay. And so that's how we can sort of take this LR_parent,",
    "start": "1554390",
    "end": "1562720"
  },
  {
    "text": "this LR_parent is just halfway, if we projected this down, all right. Let me just erase this little bit here.",
    "start": "1562720",
    "end": "1571160"
  },
  {
    "text": "If we projected this down like this,",
    "start": "1573550",
    "end": "1577560"
  },
  {
    "text": "we'd see that this- that this point here is the midpoint.",
    "start": "1580480",
    "end": "1586200"
  },
  {
    "text": "Okay. Um, but then when you're actually averaging the two losses after you've done the split,",
    "start": "1589420",
    "end": "1596570"
  },
  {
    "text": "then you can basically just, you're just taking the average loss right? You're just summing LR1 plus",
    "start": "1596570",
    "end": "1601850"
  },
  {
    "text": "LR2 and if you're taking the average then you're dividing by two, and what you can do is you can just draw the line and take the midpoint of this line instead. Yeah.",
    "start": "1601850",
    "end": "1609260"
  },
  {
    "text": "[inaudible]. Yeah. [inaudible]",
    "start": "1609260",
    "end": "1625909"
  },
  {
    "text": "Yeah. Exactly. So yeah really any- if there- it's a good point. The question was if you have an uneven split,",
    "start": "1625910",
    "end": "1632795"
  },
  {
    "text": "uh what would that look like on this curve, right? And so at this point, I've been making the math easy by",
    "start": "1632795",
    "end": "1638540"
  },
  {
    "text": "just saying there's an even split but really if there was a slightly uneven split you- the average would just be any point along this line that you've drawn.",
    "start": "1638540",
    "end": "1645740"
  },
  {
    "text": "As you can see the whole thing is strictly concave so any point along that line is going to lie below the original loss curve for the parent.",
    "start": "1645740",
    "end": "1654380"
  },
  {
    "text": "So you're basically, as long as you're not picking the exact same points on the probability curve and not making any gain at all in your split,",
    "start": "1654380",
    "end": "1661174"
  },
  {
    "text": "you're gonna gain some amount of information through this split.",
    "start": "1661175",
    "end": "1665700"
  },
  {
    "text": "Okay. Now, this was the cross-entropy loss, right?",
    "start": "1667720",
    "end": "1678720"
  },
  {
    "text": "If instead, we look at the misclassification loss over here,",
    "start": "1682960",
    "end": "1688294"
  },
  {
    "text": "let's draw this one instead.",
    "start": "1688295",
    "end": "1690600"
  },
  {
    "text": "What we can see, in this case, if you draw it is that it's in fact really this pyramid kind of shape where",
    "start": "1712180",
    "end": "1718700"
  },
  {
    "text": "it's just linear and then flips over once you start classifying the other side. And if you did the same argument here where we had LR1 and LR2,",
    "start": "1718700",
    "end": "1730919"
  },
  {
    "text": "and then you drew a line between them, all right, that's basically just still the loss curve, and so,",
    "start": "1730930",
    "end": "1736820"
  },
  {
    "text": "in this case, like your midpoint would be the same point as your parent. So your loss of R_parent, in this case,",
    "start": "1736820",
    "end": "1743930"
  },
  {
    "text": "would equal your loss of R1 plus loss of R2 divided by 2.",
    "start": "1743930",
    "end": "1751920"
  },
  {
    "text": "All right. And so in this case, you can- there's even now according to the cross entropy formulation,",
    "start": "1752590",
    "end": "1758390"
  },
  {
    "text": "you do have a gain in information and intuitively we do see a gain in information over here. For the misclassification loss, since it's not very sensitive,",
    "start": "1758390",
    "end": "1765425"
  },
  {
    "text": "if you end up with points on the same side of the curve, then you actually don't see any sort of information gain based on this kind of representation.",
    "start": "1765425",
    "end": "1773640"
  },
  {
    "text": "And so there's actually a couple, I, I presented the cross entropy loss here. There's also the Gini loss which is another one,",
    "start": "1776380",
    "end": "1784010"
  },
  {
    "text": "which people just write out as, as the sum over your classes p-hat c times one minus p-hat c,",
    "start": "1784010",
    "end": "1794679"
  },
  {
    "text": "okay, and it turns out that this curve also looks very similar to this original cross entropy curve.",
    "start": "1794680",
    "end": "1800990"
  },
  {
    "text": "And what you'll see is that actually most curves that are successfully used for de- decision splits,",
    "start": "1800990",
    "end": "1806390"
  },
  {
    "text": "look basically like the strictly concave function.",
    "start": "1806390",
    "end": "1810150"
  },
  {
    "text": "Okay. So that's where it covers a lot of the criteria we use for splits.",
    "start": "1811840",
    "end": "1817279"
  },
  {
    "text": "Um, let's look at some extensions for decision trees.",
    "start": "1817280",
    "end": "1822000"
  },
  {
    "text": "Actually, I'm going to keep this guy.",
    "start": "1838150",
    "end": "1841350"
  },
  {
    "text": "Okay. So, so far I've been talking about decision trees for classification.",
    "start": "1858100",
    "end": "1863270"
  },
  {
    "text": "You could also imagine having decision trees for regression, and people generally call these regression trees, okay.",
    "start": "1863270",
    "end": "1871355"
  },
  {
    "text": "So taking the ski example again let's pretend that instead of now predicting whether or not you can ski,",
    "start": "1871355",
    "end": "1876965"
  },
  {
    "text": "you're predicting the amount of snowfall you would expect in that area around that time. Um, so like let's- I'm just gonna say it's like inches of snowfall I guess or something,",
    "start": "1876965",
    "end": "1887360"
  },
  {
    "text": "per like day or something and just like maybe have some values up here.",
    "start": "1887360",
    "end": "1893270"
  },
  {
    "text": "Some high value because you're- it's winter over there, it's mostly 0s over here because there's summer,",
    "start": "1893270",
    "end": "1900335"
  },
  {
    "text": "and then you have some more high values over here, and then you have 0s along the equator again.",
    "start": "1900335",
    "end": "1908370"
  },
  {
    "text": "0s, southern hemisphere over our winter,",
    "start": "1911080",
    "end": "1916890"
  },
  {
    "text": "and then more 0s like this. And you can sort of see how you would do just the exact same thing.",
    "start": "1919540",
    "end": "1925370"
  },
  {
    "text": "You still want to isolate out regions and sort of increase like the purity of those regions. So you could still create like your trees like this,",
    "start": "1925370",
    "end": "1933934"
  },
  {
    "text": "all right, and split out like this for example. And what you do when you get to one of your leaves",
    "start": "1933935",
    "end": "1940895"
  },
  {
    "text": "is instead of just predicting a majority class, what you can do is predict the mean of the values left.",
    "start": "1940895",
    "end": "1947615"
  },
  {
    "text": "So you're predicting, predict y hat where, well for Rm.",
    "start": "1947615",
    "end": "1958490"
  },
  {
    "text": "So pretend you have a region Rm, you're predicting y hat of m which is the sum of all the indices in Rm,",
    "start": "1958490",
    "end": "1967590"
  },
  {
    "text": "Y i minus y hat m, and you want the squared loss and then you can sort of I guess,",
    "start": "1967630",
    "end": "1975530"
  },
  {
    "text": "in this case, you want to normalize by the overall cardinality of Rm or how many points you have in Rm.",
    "start": "1975530",
    "end": "1984450"
  },
  {
    "text": "And so in this case, basically all you've done is you've switched your loss function or, no sorry that's wrong.",
    "start": "1985270",
    "end": "1994190"
  },
  {
    "text": "[LAUGHTER] This is actually- I got a little bit ahead of myself. This is actually just- the,",
    "start": "1994190",
    "end": "2000549"
  },
  {
    "text": "the mean value would just be this, in this case, right? It's just your summing all the values within your region.",
    "start": "2000550",
    "end": "2005920"
  },
  {
    "text": "So in this case, 7, 9, 8, 10, and then just taking the average of that. Um, but so then what you do,",
    "start": "2005920",
    "end": "2013450"
  },
  {
    "text": "what I was starting to write out there was actually really the, the loss that you would use, in this case, right, which is your squared loss, okay?",
    "start": "2013450",
    "end": "2021335"
  },
  {
    "text": "So like we'll just call that L squared which, in this case,",
    "start": "2021335",
    "end": "2028289"
  },
  {
    "text": "would be equal to Y i minus",
    "start": "2028290",
    "end": "2035520"
  },
  {
    "text": "y hat m squared over R m. That's what I started to write over there.",
    "start": "2035520",
    "end": "2043980"
  },
  {
    "text": "But in this case, right, you have your mean prediction and then your loss in this case, is how far off your mean prediction is from the overall predictions,",
    "start": "2043980",
    "end": "2052463"
  },
  {
    "text": "in this case. Yep.",
    "start": "2052464",
    "end": "2061740"
  },
  {
    "text": "So in terms of [inaudible].",
    "start": "2061740",
    "end": "2074200"
  },
  {
    "text": "So that's a really good question. The question was uh, how do you actually search for your splits,",
    "start": "2074200",
    "end": "2079210"
  },
  {
    "text": "how do you actually solve the optimization problem of finding these splits? And it turns out that you can actually basically brute force it very efficiently.",
    "start": "2079210",
    "end": "2086215"
  },
  {
    "text": "I'm going to get into sot of the details of how you do that shortly, but it turns out that you can just go through everything",
    "start": "2086215",
    "end": "2091960"
  },
  {
    "text": "fairly quickly. Um, I'll get into that. I think that's in a couple of sections from now,",
    "start": "2091960",
    "end": "2097220"
  },
  {
    "text": "yeah. Any other questions? Okay. So this is,",
    "start": "2097500",
    "end": "2105190"
  },
  {
    "text": "uh, for regression trees, right? It turns out that, um, another useful extension that,",
    "start": "2105190",
    "end": "2111445"
  },
  {
    "text": "that you don't really get for other learning algorithms is that you can also deal with, uh, categorical variables fairly easily.",
    "start": "2111445",
    "end": "2119030"
  },
  {
    "text": "And basically, for this case, you could imagine that instead of having your latitude in degrees,",
    "start": "2129630",
    "end": "2135700"
  },
  {
    "text": "you could just have three categories right? You could have something like, uh,",
    "start": "2135700",
    "end": "2141200"
  },
  {
    "text": "this is the northern hemisphere, this is the equator,",
    "start": "2141240",
    "end": "2147259"
  },
  {
    "text": "and this is the southern hemisphere, okay? And then you could ask questions instead of the sort,",
    "start": "2147570",
    "end": "2156070"
  },
  {
    "text": "like that initial question we had before, where it was latitude greater than 30. Your question could instead be is,",
    "start": "2156070",
    "end": "2163000"
  },
  {
    "text": "is- I guess this would be, is location in northern hemisphere?",
    "start": "2163000",
    "end": "2174550"
  },
  {
    "text": "Right. And you can have basically any sort of subset- you could ask me question about any sort of subset of the categories you're looking at.",
    "start": "2174550",
    "end": "2181285"
  },
  {
    "text": "Right? So in this case Northern, you would still- this question would still split out this top part from these bottom pieces here.",
    "start": "2181285",
    "end": "2187450"
  },
  {
    "text": "One thing to be careful about though is that if you have q categories,",
    "start": "2187450",
    "end": "2193040"
  },
  {
    "text": "then you have- I mean, you basically are considering every single possible subset of these categories.",
    "start": "2196650",
    "end": "2203350"
  },
  {
    "text": "So that's 2 to the q possible splits.",
    "start": "2203350",
    "end": "2207140"
  },
  {
    "text": "And so in general, you don't want to deal with too many categories because this will become quickly intractable to look through that many possible examples.",
    "start": "2212040",
    "end": "2221214"
  },
  {
    "text": "It turns out that in certain very specific cases, you can still deal with a lot of categories.",
    "start": "2221215",
    "end": "2227575"
  },
  {
    "text": "One such case is for binary classification where then you can just- the math is a little",
    "start": "2227575",
    "end": "2233230"
  },
  {
    "text": "bit complicated for this one but you can basically sort your categories by how many positive examples are in each category,",
    "start": "2233230",
    "end": "2238974"
  },
  {
    "text": "and then just take that as like a sorted order then search through that linearly, and it turns out that that yields to an optimal solution.",
    "start": "2238975",
    "end": "2246920"
  },
  {
    "text": "So decision trees, we can use them for regression, we can also use them for categorical variables.",
    "start": "2254040",
    "end": "2259525"
  },
  {
    "text": "Um, one thing that I've not gotten into is that, you can imagine that in the limit if you grew your tree without ever stopping,",
    "start": "2259525",
    "end": "2266950"
  },
  {
    "text": "you could end up just having a separate region for every single data point that you have. Um, so that's really- you could consider that probably",
    "start": "2266950",
    "end": "2275039"
  },
  {
    "text": "over fitting if you ran it all the way to that completion, right? So you can sort of see that decision trees are fairly high variance models.",
    "start": "2275040",
    "end": "2285560"
  },
  {
    "text": "So one thing that we're interested in doing is regularizing these high variance models.",
    "start": "2285690",
    "end": "2291950"
  },
  {
    "text": "And generally, how people have solved this problem is through a number of heuristics, okay? So one such heuristic is that if you hit a certain minimum leaf size,",
    "start": "2304920",
    "end": "2314859"
  },
  {
    "text": "you stop splitting that leaf, okay? So for example in this case if you've hit like you only have",
    "start": "2314860",
    "end": "2323020"
  },
  {
    "text": "four examples left in this leaf, then you just stop. Another one is you can enforce a maximum depth,",
    "start": "2323020",
    "end": "2330740"
  },
  {
    "text": "and sort of a related one in this case is a max number of nodes.",
    "start": "2334380",
    "end": "2340220"
  },
  {
    "text": "And then a fourth very tempting one I've got to say to use is you say,",
    "start": "2347760",
    "end": "2353605"
  },
  {
    "text": "a minimum decrease in loss, right?",
    "start": "2353605",
    "end": "2357140"
  },
  {
    "text": "I say this one's tempting because it's generally not actually a good idea to use this minimum decrease in loss 1.",
    "start": "2364710",
    "end": "2371425"
  },
  {
    "text": "You can think about that, by thinking that if you have any sort of higher-order interactions between your variables, um,",
    "start": "2371425",
    "end": "2377680"
  },
  {
    "text": "you might have to ask one question that is not very optimal, or doesn't give you that much of an increase in loss,",
    "start": "2377680",
    "end": "2383305"
  },
  {
    "text": "and then your follow-up question combined with that first question might give you a much better increase. And you can sort of see that in this case,",
    "start": "2383305",
    "end": "2389230"
  },
  {
    "text": "where the initial latitude questions doesn't really give us that much of a gain. We sort of split some positive and negatives,",
    "start": "2389230",
    "end": "2394750"
  },
  {
    "text": "but the combination of the latitude question plus the time question really nails down what we want.",
    "start": "2394750",
    "end": "2400045"
  },
  {
    "text": "And if we were looking at it purely from the minimum decrease in loss perspective, we might stop too early and miss that entirely.",
    "start": "2400045",
    "end": "2407680"
  },
  {
    "text": "And so a better way to do this kind of loss decrease is instead you grow out your full tree,",
    "start": "2407680",
    "end": "2413700"
  },
  {
    "text": "and then you prune it backwards instead. So you grow out the whole thing and then you check which nodes to prune out.",
    "start": "2413700",
    "end": "2420195"
  },
  {
    "text": "Pruning. And how you generally do this, is you, you take it- you have a validation set that you use this with,",
    "start": "2420195",
    "end": "2427720"
  },
  {
    "text": "and you evaluate what your misclassification error is on your validation set. If for each example that you might remove for each leaf that you might remove.",
    "start": "2427720",
    "end": "2436255"
  },
  {
    "text": "So you would use misclassification in",
    "start": "2436255",
    "end": "2441430"
  },
  {
    "text": "this case with a validation set.",
    "start": "2441430",
    "end": "2450260"
  },
  {
    "text": "Any questions? Yeah? The minimum decrease in loss.",
    "start": "2460620",
    "end": "2466285"
  },
  {
    "text": "The minimum decrease in loss? So, um, yeah of course. Uh, so you'll recall that before I was talking about sort of this RP,",
    "start": "2466285",
    "end": "2474715"
  },
  {
    "text": "this loss of R_parent versus loss of R_1 plus loss of R_2. All right, so when we're- I had written out a maximization basically,",
    "start": "2474715",
    "end": "2482635"
  },
  {
    "text": "um, oh to be clear, the question is, can you explain a little bit more clearly what this minimum decrease in loss means?",
    "start": "2482635",
    "end": "2490550"
  },
  {
    "text": "And so you have your loss of R_1 and R_2 versus your loss of R_parent, right? So the split before the split, right,",
    "start": "2491340",
    "end": "2499089"
  },
  {
    "text": "you have your loss before split.",
    "start": "2499090",
    "end": "2505190"
  },
  {
    "text": "You have the loss of R_parent, and then after split,",
    "start": "2505830",
    "end": "2512090"
  },
  {
    "text": "you have loss of R_1 plus loss of R_2.",
    "start": "2516210",
    "end": "2521244"
  },
  {
    "text": "Yeah. And if, if this decrease between your loss of R_parent to your loss of your children is not great enough,",
    "start": "2521245",
    "end": "2528520"
  },
  {
    "text": "you might be tempted to say, \"Okay, that question didn't really gain us anything, and so therefore we will not actually use that question.\"",
    "start": "2528520",
    "end": "2535809"
  },
  {
    "text": "But what I'm saying is that sometimes you have to ask multiple questions, right? You have to ask sort of sub-optimal questions first to get to the really good questions,",
    "start": "2535810",
    "end": "2543190"
  },
  {
    "text": "especially if you have sort of interaction between your variables, if there is some amount of correlation between your variables.",
    "start": "2543190",
    "end": "2549410"
  },
  {
    "text": "Okay. So we talked about regularization.",
    "start": "2556890",
    "end": "2561920"
  },
  {
    "text": "I said that we would get to run time, let's actually just go up here again.",
    "start": "2562320",
    "end": "2567819"
  },
  {
    "text": "[NOISE] So let's cover that really quickly.",
    "start": "2567820",
    "end": "2574660"
  },
  {
    "text": "[NOISE]",
    "start": "2574660",
    "end": "2608164"
  },
  {
    "text": "Okay. So it'll be useful to define a couple of numbers at this point. So say you have n examples.",
    "start": "2608165",
    "end": "2615349"
  },
  {
    "text": "[NOISE] You have f features,",
    "start": "2615350",
    "end": "2623130"
  },
  {
    "text": "and finally, you have, uh, d- let's say the depth of your tree is d, okay.",
    "start": "2627070",
    "end": "2634770"
  },
  {
    "text": "All right. So you've gra- you, you have n examples that you trained on you- with the each of f features and your resulting tree has depth d. So at test time,",
    "start": "2637630",
    "end": "2646895"
  },
  {
    "text": "your run-time is basically just your depth d, right? [NOISE] It's just o of d,",
    "start": "2646895",
    "end": "2657500"
  },
  {
    "text": "right? Which is your depth. And typically, though not in all cases, um,",
    "start": "2657500",
    "end": "2663125"
  },
  {
    "text": "d is sort of about- is less than the log of your number of examples.",
    "start": "2663125",
    "end": "2670205"
  },
  {
    "text": "And you can sort of think about this as if you have a fairly balanced tree right, you'll end up sort of evenly splitting out all the examples and sort of recursively like",
    "start": "2670205",
    "end": "2679265"
  },
  {
    "text": "doing these binary splits and so you'll be splitting it at the log of that n. Okay. So at test-time you've generally got it pretty quick.",
    "start": "2679265",
    "end": "2686530"
  },
  {
    "text": "Uh, at train time,",
    "start": "2686530",
    "end": "2689540"
  },
  {
    "text": "um, you have each point.",
    "start": "2692520",
    "end": "2697595"
  },
  {
    "text": "So if you return back to this example, you'll see that each point, right, once you've done a split only belongs to",
    "start": "2697595",
    "end": "2704300"
  },
  {
    "text": "the left or right of that split afterwards. All right. So it's sort of like, like this point right here, once you've split here will only ever be part of this region,",
    "start": "2704300",
    "end": "2711290"
  },
  {
    "text": "will never be considered on the other side, on the right-hand side of that split. All right.",
    "start": "2711290",
    "end": "2716690"
  },
  {
    "text": "So if your, if your tree is of depth d, each point,",
    "start": "2716690",
    "end": "2722060"
  },
  {
    "text": "each point is part",
    "start": "2722060",
    "end": "2728135"
  },
  {
    "text": "of Od nodes.",
    "start": "2728135",
    "end": "2735270"
  },
  {
    "text": "Okay. And then at each node, you can actually work out that the cost of evaluating that point",
    "start": "2736990",
    "end": "2745055"
  },
  {
    "text": "for- at training time is actually just proportional to the number of features f.",
    "start": "2745055",
    "end": "2750030"
  },
  {
    "text": "I won't get too much into the details of why this is, but you can consider that if you're doing binary features, for example,",
    "start": "2764650",
    "end": "2771680"
  },
  {
    "text": "where each feature is just yes or no of some sort, then you only have to consider, if you have f features total,",
    "start": "2771680",
    "end": "2777200"
  },
  {
    "text": "you only have to consider, um, f possible splits. So that's why the cost in that case would be f,",
    "start": "2777200",
    "end": "2783200"
  },
  {
    "text": "and then if it was instead a, uh, quantitative feature, I mentioned briefly that you could sort",
    "start": "2783200",
    "end": "2788644"
  },
  {
    "text": "the overall features and then scan through them linearly, um, and that also ends up being asymptotically O of f to do that.",
    "start": "2788645",
    "end": "2796890"
  },
  {
    "text": "Okay. So each point is at most O of d nodes, and then the cost of point at each node is O of f and you have n points total.",
    "start": "2797410",
    "end": "2806240"
  },
  {
    "text": "So the total cost is really just,",
    "start": "2806240",
    "end": "2809340"
  },
  {
    "text": "is just O of nfd, like this.",
    "start": "2814510",
    "end": "2819785"
  },
  {
    "text": "It turns out that this is actually surprisingly fast, uh,",
    "start": "2819785",
    "end": "2825275"
  },
  {
    "text": "especially if you consider that n times f is just the size of your original design matrix,",
    "start": "2825275",
    "end": "2832355"
  },
  {
    "text": "right or your data matrix, all right. Your data matrix is",
    "start": "2832355",
    "end": "2841040"
  },
  {
    "text": "of size n times f,",
    "start": "2841040",
    "end": "2846680"
  },
  {
    "text": "right, and then you're only- your, your runtime is going through the data matrix that most depth times,",
    "start": "2846680",
    "end": "2852530"
  },
  {
    "text": "and since depth is log of n, that turns out to be or generally bounded by log of n, you have generally, a fairly,",
    "start": "2852530",
    "end": "2859250"
  },
  {
    "text": "fast training time as well. Any questions about runtime? [NOISE] Okay.",
    "start": "2859250",
    "end": "2870410"
  },
  {
    "text": "So I've been talking a lot about the good sides of decision trees right, they seem pretty nice so far.",
    "start": "2870410",
    "end": "2875570"
  },
  {
    "text": "However, there are a number of downsides too. Um, and one big one is that it doesn't have additive structure to it.",
    "start": "2875570",
    "end": "2886724"
  },
  {
    "text": "And so let me explain a little bit what that means.",
    "start": "2886725",
    "end": "2890120"
  },
  {
    "text": "Okay. So let's say now we have an example and you have just two features again,",
    "start": "2909610",
    "end": "2915635"
  },
  {
    "text": "so x1 and x2, and you ca- say you define a line, okay,",
    "start": "2915635",
    "end": "2921530"
  },
  {
    "text": "just running through the middle defined by x1 equals x2. And all the points above this line are positive,",
    "start": "2921530",
    "end": "2929910"
  },
  {
    "text": "and all the points below it are negative. Now, if you have a simple linear model like logistic regression,",
    "start": "2930130",
    "end": "2936859"
  },
  {
    "text": "you'll have no issue with this kind of setup. But for a decision tree, [LAUGHTER] basically,",
    "start": "2936860",
    "end": "2942095"
  },
  {
    "text": "you'd have to ask a lot of questions that even somewhat approximate this line. Like, what you can try is you're going to say okay let's split it this way,",
    "start": "2942095",
    "end": "2949609"
  },
  {
    "text": "and maybe we can do a split this way and then now I split here, maybe something like this,",
    "start": "2949610",
    "end": "2955130"
  },
  {
    "text": "and basically something like that, right? Even here you- so you've asked a lot of questions and you've only gotten",
    "start": "2955130",
    "end": "2962390"
  },
  {
    "text": "a very rough approximation of the actual line that you've drawn in this case. And so decision trees do have a lot of issues with these kind of structures",
    "start": "2962390",
    "end": "2970400"
  },
  {
    "text": "where the v- the features are interacting additively with one another.",
    "start": "2970400",
    "end": "2975359"
  },
  {
    "text": "Okay. So to recap so far, since we've covered a number of different things about decision trees,",
    "start": "2976960",
    "end": "2984869"
  },
  {
    "text": "there's a number of pu- pluses and minuses to decision trees. Okay. So on the plus side, they're actually,",
    "start": "2986320",
    "end": "2993125"
  },
  {
    "text": "I think this is an important point is that they're actually pretty easy to explain, right? If you're explaining what a decision tree is to like a non-technical person,",
    "start": "2993125",
    "end": "3001480"
  },
  {
    "text": "it's fairly obvious you're like okay you have this tree, you're just playing 20 Questions with your data and letting it co- come up with one question at a time.",
    "start": "3001480",
    "end": "3008710"
  },
  {
    "text": "There are also interpretable, you can just draw out the tree especially for shorter trees to see exactly what it's doing.",
    "start": "3008710",
    "end": "3018590"
  },
  {
    "text": "It can deal with categorical variables,",
    "start": "3020010",
    "end": "3023869"
  },
  {
    "text": "and it's generally pretty fast.",
    "start": "3029670",
    "end": "3032990"
  },
  {
    "text": "However, on the negative side, one that I alluded to is that they're",
    "start": "3035730",
    "end": "3042010"
  },
  {
    "text": "fairly high variance models and so are oftentimes prone to overfitting your data.",
    "start": "3042010",
    "end": "3048920"
  },
  {
    "text": "They're bad at additive structure.",
    "start": "3051090",
    "end": "3054740"
  },
  {
    "text": "And then finally they have, because in large part because of these first two,",
    "start": "3060750",
    "end": "3066940"
  },
  {
    "text": "they generally have fairly low predictive accuracy. [NOISE] I know what you guys are thinking,",
    "start": "3066940",
    "end": "3076660"
  },
  {
    "text": "I just spent all this time talking about decision trees and then I tell you guys they actually sort of suck. So why did I actually cover decision trees?",
    "start": "3076660",
    "end": "3082420"
  },
  {
    "text": "And the answer is that in fact you can make decision trees a lot better through ensembling.",
    "start": "3082420",
    "end": "3088180"
  },
  {
    "text": "And a lot of the methods, for example at the leading methods in Kaggle these days are actually built on ensembles of decision trees,",
    "start": "3088180",
    "end": "3095395"
  },
  {
    "text": "and they really provide an ideal sort of model framework to look at, through which we can examine a lot of these different ensembling methods.",
    "start": "3095395",
    "end": "3102830"
  },
  {
    "text": "Any questions about decision trees before I move on? [NOISE] Yeah?",
    "start": "3103110",
    "end": "3108460"
  },
  {
    "text": "[inaudible].",
    "start": "3108460",
    "end": "3114609"
  },
  {
    "text": "I don't think that's strictly- Okay. So the question is for the cross-entropy loss,",
    "start": "3114610",
    "end": "3120190"
  },
  {
    "text": "does the log need to be base 2? And the answer is I'm pretty sure that it's not very relevant in this case,",
    "start": "3120190",
    "end": "3126235"
  },
  {
    "text": "I'm not 100% sure about that but I'm pretty sure that the base of the log of that makes, it's cross entropy loss actually initially came out of like information theory,",
    "start": "3126235",
    "end": "3132940"
  },
  {
    "text": "we have like computer bits and you're transmitting bits. So it's useful to think in terms of bits of information that you can transmit,",
    "start": "3132940",
    "end": "3138985"
  },
  {
    "text": "which is why it came up as log base 2 in the initial formulation. [NOISE]",
    "start": "3138985",
    "end": "3174310"
  },
  {
    "text": "Okay. So now let's talk about ensembling.",
    "start": "3174310",
    "end": "3177590"
  },
  {
    "text": "Okay. So why does ensembling help? At some level, you can sort of think back to your basic, uh, statistics.",
    "start": "3186330",
    "end": "3194740"
  },
  {
    "text": "So say you have, um, you have XIs, XIs, which are random variables.",
    "start": "3194740",
    "end": "3211370"
  },
  {
    "text": "I'll sometimes write this as just RV, um,",
    "start": "3216900",
    "end": "3222579"
  },
  {
    "text": "that are independent and identically distributed.",
    "start": "3222580",
    "end": "3238195"
  },
  {
    "text": "And so probably a lot of you are familiar with this already or you can call this IID, okay.",
    "start": "3238195",
    "end": "3249500"
  },
  {
    "text": "Now say that your variance of one of these variables is Sigma squared.",
    "start": "3250170",
    "end": "3259700"
  },
  {
    "text": "Then what you can show is that the variance of the mean of many of these variables.",
    "start": "3261120",
    "end": "3268300"
  },
  {
    "text": "So let's- of many of these random variables or written alternatively,",
    "start": "3268300",
    "end": "3273670"
  },
  {
    "text": "1 over N sum over I to the XI is equal to Sigma",
    "start": "3273670",
    "end": "3282010"
  },
  {
    "text": "squared over N. And so each independent variable you factor in is",
    "start": "3282010",
    "end": "3288130"
  },
  {
    "text": "decreasing the variance of your model, all right? And so the thought is that if you can factor in a number of independent sources,",
    "start": "3288130",
    "end": "3297490"
  },
  {
    "text": "you can slowly decrease your variance. Okay, so, uh, so that- though this is",
    "start": "3297490",
    "end": "3302860"
  },
  {
    "text": "a little bit simplistic of a way of looking at this, because really all these different things are factoring together have some amount of correlation with each other.",
    "start": "3302860",
    "end": "3309595"
  },
  {
    "text": "And so this independence assumption is oftentimes not correct. So if instead,",
    "start": "3309595",
    "end": "3316129"
  },
  {
    "text": "you drop the independence assumption.",
    "start": "3321480",
    "end": "3325340"
  },
  {
    "text": "So now your variables are just ID, right?",
    "start": "3337890",
    "end": "3343670"
  },
  {
    "text": "Okay. And say we can characterize what the correlation between any two XIs is and we can write that down as Rho. So Xi.",
    "start": "3351030",
    "end": "3374170"
  },
  {
    "text": "Then you can actually write out the variance of your mean as",
    "start": "3374170",
    "end": "3382000"
  },
  {
    "text": "Rho Sigma squared- Sigma squared,",
    "start": "3382000",
    "end": "3388795"
  },
  {
    "text": "plus 1 minus Rho over M or- no, N Sigma squared, okay?",
    "start": "3388795",
    "end": "3398065"
  },
  {
    "text": "And so you can sort of see that if your correlation- if they're fully correlated, then your- this term will drop to 0 and",
    "start": "3398065",
    "end": "3403599"
  },
  {
    "text": "that you'll just have Sigma squared again because adding a bunch of fully correlated variables is just gonna give you the original variable's variance versus if they're",
    "start": "3403600",
    "end": "3410980"
  },
  {
    "text": "completely decorrelated then this term drops to 0 and you just end up with Sigma squared over N which gives you the initial,",
    "start": "3410980",
    "end": "3416635"
  },
  {
    "text": "uh, independent identically distributed equation.",
    "start": "3416635",
    "end": "3421735"
  },
  {
    "text": "And so in this case, really what you wanna do- the name of the game is, you wanna have as many different models that you're",
    "start": "3421735",
    "end": "3429040"
  },
  {
    "text": "factoring as possible to increase this N which drives this term down. And then on the other hand, you also want to make sure",
    "start": "3429040",
    "end": "3434770"
  },
  {
    "text": "those models are as decorrelated as possible so that your Rho goes down and this first term goes down as well, okay?",
    "start": "3434770",
    "end": "3442910"
  },
  {
    "text": "And so this gives rise to a number of different ways to ensemble.",
    "start": "3455310",
    "end": "3460550"
  },
  {
    "text": "And one way you could think about doing this is you just use different algorithms, right?",
    "start": "3468090",
    "end": "3474650"
  },
  {
    "text": "This is actually what a lot of people in Kaggle, for example, will do, is they'll just take a neural network or Random Forest, an SVM,",
    "start": "3482510",
    "end": "3490080"
  },
  {
    "text": "average them all together and generally that actually works pretty well but- then you sort of have to",
    "start": "3490080",
    "end": "3495510"
  },
  {
    "text": "spend your time implementing all these separate algorithms which is oftentimes not the most efficient use of your time.",
    "start": "3495510",
    "end": "3501815"
  },
  {
    "text": "Another one that people would like to do is just use different training sets.",
    "start": "3501815",
    "end": "3508490"
  },
  {
    "text": "Okay. And again, in this case, like you probably spent a lot of effort collecting your initial training set,",
    "start": "3517710",
    "end": "3523960"
  },
  {
    "text": "you don't want your- like machine learning person to just come and recommend to you that, just go collect a whole second training set",
    "start": "3523960",
    "end": "3530259"
  },
  {
    "text": "or something like that to improve your performance. Like that's generally not the most helpful recommendation, okay?",
    "start": "3530260",
    "end": "3537040"
  },
  {
    "text": "And so then, what we're gonna cover now are these two other methods that we use to do ensembling. And one of them is called bagging,",
    "start": "3537040",
    "end": "3544550"
  },
  {
    "text": "which is sort of trying to approximate having different training sets.",
    "start": "3544860",
    "end": "3550270"
  },
  {
    "text": "We'll get into that quickly. And then you also have boosting.",
    "start": "3550270",
    "end": "3555200"
  },
  {
    "text": "And just so that you guys will have a little bit of context, we're gonna be using decision trees to talk a lot about these models;",
    "start": "3558510",
    "end": "3564954"
  },
  {
    "text": "and so bagging, you might have heard of random forests, that's a variant of bagging for decision trees.",
    "start": "3564955",
    "end": "3573260"
  },
  {
    "text": "And then for boosting, you might have heard of things like AdaBoost,",
    "start": "3573600",
    "end": "3580000"
  },
  {
    "text": "or XGBoost, which are variants of boosting for decision trees.",
    "start": "3580000",
    "end": "3587960"
  },
  {
    "text": "Okay, so that sort of covers at a high level what we would wanna do. These first two are very nice because they're sort of would give us a much",
    "start": "3590940",
    "end": "3598869"
  },
  {
    "text": "more like independently correlated- or less correlated variables. But generally, we're- we end up doing these latter two",
    "start": "3598870",
    "end": "3606369"
  },
  {
    "text": "because we don't want to collect new training sets or train entirely new algorithms. Okay, so let's cover bagging first.",
    "start": "3606370",
    "end": "3614150"
  },
  {
    "text": "Okay, so bagging really stands for this thing. It's called bootstrap aggregation, okay?",
    "start": "3620850",
    "end": "3627685"
  },
  {
    "text": "Um, and so- first,",
    "start": "3627685",
    "end": "3641410"
  },
  {
    "text": "let's just break down this term. So bootstrap, what that is, is it's typically this method used in statistics to measure",
    "start": "3641410",
    "end": "3647200"
  },
  {
    "text": "the uncertainty of your estimate, okay? And so what- what is useful to define in this case for when you're talking about bagging",
    "start": "3647200",
    "end": "3655840"
  },
  {
    "text": "is you can say that you have a true population P, okay?",
    "start": "3655840",
    "end": "3667810"
  },
  {
    "text": "And your training set- training set S is sampled from P, right?",
    "start": "3667810",
    "end": "3678730"
  },
  {
    "text": "So you just start drawing a bunch of examples from P and that's what forms your training set at some level. And so ideally, like for example,",
    "start": "3678730",
    "end": "3685690"
  },
  {
    "text": "this different training set's approach. What you do is, you just draw S1, S2, S3, S4, and then train your model in each one of those separately.",
    "start": "3685690",
    "end": "3692695"
  },
  {
    "text": "Unfortunately, you generally don't have the time to do that. And so what ba- what Bootstrapping does,",
    "start": "3692695",
    "end": "3699205"
  },
  {
    "text": "is you assume basically that your population is your training sample, okay?",
    "start": "3699205",
    "end": "3708150"
  },
  {
    "text": "So you assume that your population is your training sample. And so now that you have this S is approximating your P,",
    "start": "3708150",
    "end": "3714630"
  },
  {
    "text": "then you can draw new samples from your population by just drawing samples from S instead, okay?",
    "start": "3714630",
    "end": "3721440"
  },
  {
    "text": "So you have bootstrap samples, is what they're called.",
    "start": "3721440",
    "end": "3726550"
  },
  {
    "text": "Z sampled from S. And so how that works is you basically just take your train- your- your training sample, okay?",
    "start": "3731040",
    "end": "3740289"
  },
  {
    "text": "Say it's of like cardinality N or something. And you just sample N times from S and this is important,",
    "start": "3740290",
    "end": "3746320"
  },
  {
    "text": "you do it with replacement. Because they're pretending that this is the population, and so doing it with replacement sort of makes that assumption",
    "start": "3746320",
    "end": "3753280"
  },
  {
    "text": "hold that you're sampling from it as a population.",
    "start": "3753280",
    "end": "3757280"
  },
  {
    "text": "Okay, so that's bootstrapping. So you generate all these different bootstrap samples Z on your- from your training set.",
    "start": "3758810",
    "end": "3767099"
  },
  {
    "text": "And what you can do is you can take your model and train it on all these separate bootstrap samples, and then you can sort of look at the variability in",
    "start": "3767100",
    "end": "3774880"
  },
  {
    "text": "the predictions that your model ends up making based on these different bootstrap samples. And that gives you sort of a measure of uncertainty.",
    "start": "3774880",
    "end": "3781675"
  },
  {
    "text": "I'm not gonna go into too much detail now because that's not actually what we're gonna use Bootstrapping for. What we want to use bootstrapping for is we wanna aggregate these two Bootstrap samples.",
    "start": "3781675",
    "end": "3791050"
  },
  {
    "text": "And so at a very high level, what that means is we're gonna take a bunch of Bootstrap samples, train separate models on each and then average their outputs, okay?",
    "start": "3791050",
    "end": "3801680"
  },
  {
    "text": "So let's make that a little bit more formal. [NOISE]",
    "start": "3802230",
    "end": "3826895"
  },
  {
    "text": "So you have bootstrap samples",
    "start": "3826895",
    "end": "3839270"
  },
  {
    "text": "Z_1 through Z_M say, okay, capital M. That's just say how many bootstrap samples you're going to take.",
    "start": "3839270",
    "end": "3847715"
  },
  {
    "text": "Okay, you train [NOISE] a model,",
    "start": "3847715",
    "end": "3854760"
  },
  {
    "text": "G_M, okay, on Z_M, okay?",
    "start": "3856180",
    "end": "3860400"
  },
  {
    "text": "Then all you're doing is you're just defining this new sort of meta model.",
    "start": "3865540",
    "end": "3870980"
  },
  {
    "text": "I'm not putting a subscript on this one to show that it's a meta model, T of M, which is just the sum of your predictions of your individual models,",
    "start": "3870980",
    "end": "3883160"
  },
  {
    "text": "divided by the total number of models you have, all right? And this is just me writing out what I was sort of talking about right up there for bagging.",
    "start": "3883160",
    "end": "3894200"
  },
  {
    "text": "If you're taking these bootstrap samples and then you're training separate models, and then you're just aggregating them all together to get this bagging approach.",
    "start": "3894200",
    "end": "3902670"
  },
  {
    "text": "So if we just do a little bit of analysis from the bias-variance perspective on this,",
    "start": "3906790",
    "end": "3912125"
  },
  {
    "text": "we can sort of see why this kind of thing might work. [NOISE]",
    "start": "3912125",
    "end": "3928220"
  },
  {
    "text": "And so you recall we had this equation up here, right? The va- variance of the mean is rho sigma squared,",
    "start": "3928220",
    "end": "3933800"
  },
  {
    "text": "plus 1 minus rho, over n of sigma squared. So let me just write that out here. [NOISE]",
    "start": "3933800",
    "end": "3949670"
  },
  {
    "text": "And in this case, our M is actually really uh, just the number of bootstrap samples. So we'll just use big M in this case.",
    "start": "3949670",
    "end": "3956670"
  },
  {
    "text": "And what you're doing is by taking these bootstrap samples, you're sort of decorrelating the models you're training.",
    "start": "3957670",
    "end": "3963380"
  },
  {
    "text": "Your bootstrapping [NOISE] is",
    "start": "3963380",
    "end": "3971059"
  },
  {
    "text": "driving down [NOISE] rho.",
    "start": "3971060",
    "end": "3977150"
  },
  {
    "text": "Okay. And so by driving this down,",
    "start": "3977150",
    "end": "3982990"
  },
  {
    "text": "you're sort of making this term get smaller and smaller. And then your question might be okay, what about this term here? And it turns out that basically you can take as many bootstrap samples as you want,",
    "start": "3982990",
    "end": "3992869"
  },
  {
    "text": "and that will slowly drive down- it increases M and drive down this second term. And it turns out that one nice thing about bootstrapping,",
    "start": "3992870",
    "end": "4000925"
  },
  {
    "text": "is that increasing the number of bootstrap models in your training, doesn't actually cause you to overfit anymore than you were beforehand.",
    "start": "4000925",
    "end": "4009775"
  },
  {
    "text": "Because all you're doing, is you're driving down this term here. So more M [NOISE] and it's just less in variance.",
    "start": "4009775",
    "end": "4019060"
  },
  {
    "text": "[NOISE] All you're doing is driving down the second term as much",
    "start": "4019060",
    "end": "4026109"
  },
  {
    "text": "as possible when you're getting more and more bootstrap samples. So generally, it only improves performance. And so generally what people will do is they'll sample",
    "start": "4026110",
    "end": "4032470"
  },
  {
    "text": "more and more models until they see that their error stops going down. Because that means they basically eliminated this term over here.",
    "start": "4032470",
    "end": "4039700"
  },
  {
    "text": "So this seems kinda nice, right? You're decreasing the variance, where is the trade-off coming in? Oh, there is a question there.",
    "start": "4039700",
    "end": "4047410"
  },
  {
    "text": "[inaudible].",
    "start": "4047410",
    "end": "4053349"
  },
  {
    "text": "Yeah, there's definitely a bound, right? Because um, I'm not going to define one formally right now.",
    "start": "4053350",
    "end": "4059215"
  },
  {
    "text": "Oh, the question is can you define a bound on how much you decrease rho by? Uh, I'm not- yeah,",
    "start": "4059215",
    "end": "4065080"
  },
  {
    "text": "so there's definitely a lower bound [NOISE] or, oh yeah, a lower bound on how far you can decrease rho.",
    "start": "4065080",
    "end": "4071930"
  },
  {
    "text": "Basically it comes down to your bootstrap samples are still fairly highly correlated with one another, all right.",
    "start": "4072420",
    "end": "4078400"
  },
  {
    "text": "Because they're still just drawing it from the same sample set S. Really, your Z is gonna end up containing about two- each Z is going to contain",
    "start": "4078400",
    "end": "4085869"
  },
  {
    "text": "about two thirds of S. And so your Zs are still gonna be fairly highly correlated with each other. And no, I don't have a formal equation to write down as to",
    "start": "4085870",
    "end": "4092860"
  },
  {
    "text": "exactly how much that decreases rho by, or how much that bounds rho by, you can sort of see intuitively that there is a bound there and that you can't just",
    "start": "4092860",
    "end": "4100989"
  },
  {
    "text": "magically decrease rho all the way down to 0 and achieve 0 variance. [NOISE] All right.",
    "start": "4100990",
    "end": "4109839"
  },
  {
    "text": "So saying that you decrease variance, that seems very nice. One issue that comes up with, with uh,",
    "start": "4109840",
    "end": "4116065"
  },
  {
    "text": "bootstrapping is that in fact you're actually slightly increasing the bias of your models when you're doing this.",
    "start": "4116065",
    "end": "4121720"
  },
  {
    "text": "And the reasoning for that [NOISE] is because of",
    "start": "4121720",
    "end": "4132750"
  },
  {
    "text": "this sub-sampling that I was talking about here. Each one of your Zs is now about two-thirds of the original S. So you're training on less data um,",
    "start": "4132750",
    "end": "4140490"
  },
  {
    "text": "and so your models are becoming slightly less uh, you know, complex and so that increases your bias in this case. Yes.",
    "start": "4140490",
    "end": "4147850"
  },
  {
    "text": "[inaudible]",
    "start": "4147850",
    "end": "4154870"
  },
  {
    "text": "Yeah, for sure. Um, so the question is, can you explain the difference between",
    "start": "4154870",
    "end": "4160345"
  },
  {
    "text": "a random variable and an algorithm in this case, right? And so you could sorta- at a, at a very high level,",
    "start": "4160345",
    "end": "4165790"
  },
  {
    "text": "you can think of an algorithm as a classifier- as a function that's taking in some data and making a prediction.",
    "start": "4165790",
    "end": "4171444"
  },
  {
    "text": "Right? And if you sort of see those- that whole setup as sort of like, the probability, the algorithm is",
    "start": "4171445",
    "end": "4177775"
  },
  {
    "text": "giving some sort of output in the probabilistic perspective, you can sort of see the algorithm as like a random variable in the case- in this case.",
    "start": "4177775",
    "end": "4185664"
  },
  {
    "text": "Sort of like, you're basically considering, sort of the space of possible predictions that",
    "start": "4185665",
    "end": "4191199"
  },
  {
    "text": "your algorithm can make and that you can sort of see as a distribution of possible predictions",
    "start": "4191200",
    "end": "4196270"
  },
  {
    "text": "and that you can approximate that as a random variable. I mean it is a random variable at some level, because it's sort of like based on what's training sample you end up with,",
    "start": "4196270",
    "end": "4205375"
  },
  {
    "text": "your predictions of your output model are gonna change. And so since you're sampling sort of these random samples from your population set,",
    "start": "4205375",
    "end": "4213730"
  },
  {
    "text": "you can consider your algorithm as sort of based on that random sample and therefore a random variable itself.",
    "start": "4213730",
    "end": "4220640"
  },
  {
    "text": "Okay. So yeah, your bias is slightly increased because [NOISE] of random subsampling,",
    "start": "4221970",
    "end": "4232150"
  },
  {
    "text": "[NOISE] but generally,",
    "start": "4232150",
    "end": "4241255"
  },
  {
    "text": "the decrease in variance that you get from doing this, is much larger than the slight increase in bias you get from,",
    "start": "4241255",
    "end": "4247780"
  },
  {
    "text": "from doing this randomized subsampling. So in a lot of cases, bagging is quite nice. [NOISE] Okay?",
    "start": "4247780",
    "end": "4254510"
  },
  {
    "text": "So I've talked a bit about ba- about bagging, uh, let's talk about decision trees plus bagging now.",
    "start": "4268230",
    "end": "4276139"
  },
  {
    "text": "Okay. So you recall that decision trees are high [NOISE] variance,",
    "start": "4283290",
    "end": "4291410"
  },
  {
    "text": "low bias okay? And this right here sort of explains why they're a pretty good fit for bagging.",
    "start": "4295830",
    "end": "4303715"
  },
  {
    "text": "Okay? Because bagging what you're doing, is you're decreasing the variance of your models for a slight increase in bias.",
    "start": "4303715",
    "end": "4309505"
  },
  {
    "text": "And since most of your error from your decision trees is coming from the high variance side of things,",
    "start": "4309505",
    "end": "4314740"
  },
  {
    "text": "by sort of driving down that variance, you get a lot more benefit than for a, a model that would be on the reverse high bias and low variance.",
    "start": "4314740",
    "end": "4323350"
  },
  {
    "text": "Okay? So, so this makes this like an ideal fit [NOISE] for bagging.",
    "start": "4323350",
    "end": "4330520"
  },
  {
    "text": "[NOISE]",
    "start": "4330520",
    "end": "4344575"
  },
  {
    "text": "Okay. [NOISE] So now, um, this is sort of decision trees plus bagging.",
    "start": "4344575",
    "end": "4349600"
  },
  {
    "text": "I said that random forests are sort of a version of decision trees plus bagging. And so what I've described here is actually almost a random forest at this point.",
    "start": "4349600",
    "end": "4358825"
  },
  {
    "text": "The one key point we're still missing is that random forests actually introduce even more randomization into each individual decision tree.",
    "start": "4358825",
    "end": "4366745"
  },
  {
    "text": "And the idea behind that is that- as I had a question from before is this Rho,",
    "start": "4366745",
    "end": "4371980"
  },
  {
    "text": "you can only drive it down so far through just pure bootstrapping. But if you can further decorrelate your different random variables,",
    "start": "4371980",
    "end": "4378850"
  },
  {
    "text": "then you can drive down that variance even further, okay? Um, and so the idea there is that basically for- at each split for random forests,",
    "start": "4378850",
    "end": "4390529"
  },
  {
    "text": "at each split, you consider only a fraction",
    "start": "4398460",
    "end": "4409719"
  },
  {
    "text": "of your total features, right?",
    "start": "4409720",
    "end": "4421040"
  },
  {
    "text": "So it's sort of like, for that ski example, maybe like for the first split, I only let it look at latitude, and then for the second split,",
    "start": "4425850",
    "end": "4432429"
  },
  {
    "text": "I only let it look at, uh, the time of the year. [NOISE] And so this might seem a little bit unintuitive at first,",
    "start": "4432430",
    "end": "4439989"
  },
  {
    "text": "but you can sort of get the intuition from two ways. One is that you're decreasing Rho and then the other one",
    "start": "4439990",
    "end": "4451405"
  },
  {
    "text": "is you can think that- say you have a classification example, where you have one very strong predictor that gets you very good performance on its own.",
    "start": "4451405",
    "end": "4459430"
  },
  {
    "text": "And regardless of what bootstrap sample you select, your model is probably gonna use that predictor as its first split.",
    "start": "4459430",
    "end": "4464905"
  },
  {
    "text": "That's gonna cause all your models to be very highly correlated right at that first split, for example, and by instead forcing it to,",
    "start": "4464905",
    "end": "4472150"
  },
  {
    "text": "to sample from different features. Instead, that's going to increase the, uh, or decrease the correlation between your models.",
    "start": "4472150",
    "end": "4479680"
  },
  {
    "text": "And so it's all about decorrelating your models in this case. [NOISE] Okay.",
    "start": "4479680",
    "end": "4492925"
  },
  {
    "text": "And that sort of brings to a close a lot of our discussion of bagging. Are there any questions regarding bagging?",
    "start": "4492925",
    "end": "4498860"
  },
  {
    "text": "Okay. Now, I've covered bagging.",
    "start": "4500700",
    "end": "4506920"
  },
  {
    "text": "Let's get a little bit into boosting. [NOISE] And I'll make this quick.",
    "start": "4506920",
    "end": "4516925"
  },
  {
    "text": "But basically, whereas bagging we sort of saw in the intuition that",
    "start": "4516925",
    "end": "4524020"
  },
  {
    "text": "we were decreasing variance, boosting is sort of actually more of the opposite where you're decreasing the bias of your models, okay?",
    "start": "4524020",
    "end": "4531805"
  },
  {
    "text": "So- [NOISE]",
    "start": "4531805",
    "end": "4543460"
  },
  {
    "text": "and also it- it's basically, um, more additive in, um, in how it's doing things.",
    "start": "4543460",
    "end": "4549760"
  },
  {
    "text": "So versus- [NOISE] you'll recall that for bagging,",
    "start": "4549760",
    "end": "4555639"
  },
  {
    "text": "you were taking the average of a number of variables. In boosting, what happens, you train one model and then you add that prediction into your ensemble.",
    "start": "4555640",
    "end": "4562555"
  },
  {
    "text": "And then when you turn a new model, you just add that in as a prediction. And so- and that's a little bit handwavy right now.",
    "start": "4562555",
    "end": "4567745"
  },
  {
    "text": "So let me actually make that clear through an example. [NOISE] So say you have a dataset, again, X1,",
    "start": "4567745",
    "end": "4577045"
  },
  {
    "text": "X2, X2 and you have some data points, maybe some- that's actually- just call them pluses and minuses.",
    "start": "4577045",
    "end": "4587020"
  },
  {
    "text": "So you have some more pluses here, and then maybe a couple of minuses and some pluses here, okay?",
    "start": "4587020",
    "end": "4595510"
  },
  {
    "text": "And what you- say you're training a size one decision tree. So decision stumps is what we call them.",
    "start": "4595510",
    "end": "4601660"
  },
  {
    "text": "And so you only get to ask one question at a time. And the reason behind this, just really quickly is that because you're decreasing bias",
    "start": "4601660",
    "end": "4608605"
  },
  {
    "text": "by restricting your trees to be only depth 1, you basically are increasing",
    "start": "4608605",
    "end": "4614245"
  },
  {
    "text": "their amount of bias and decreasing their amount of variance, which makes them a better fit for boosting kind of methods.",
    "start": "4614245",
    "end": "4619375"
  },
  {
    "text": "And say that you come up with a, a decision boundary, okay? Say this one here, okay?",
    "start": "4619375",
    "end": "4626650"
  },
  {
    "text": "And what you're gonna do is, on this side you predict positive, right? And on this side you predict negative.",
    "start": "4626650",
    "end": "4631885"
  },
  {
    "text": "There's like a reasonable like line that you could draw here, but it's not perfect, right? You've made some mistakes. And in fact, what you can do is you can sort of identify these mistakes.",
    "start": "4631885",
    "end": "4641355"
  },
  {
    "text": "Now, if we draw this in red, right? You've got- made these guys as mistakes.",
    "start": "4641355",
    "end": "4646915"
  },
  {
    "text": "And what boosting does is basically it increases the weights of the mistakes you've made.",
    "start": "4646915",
    "end": "4653380"
  },
  {
    "text": "And then for the next out- uh, decision stump that you train, it's now trained on this modified set.",
    "start": "4653380",
    "end": "4659050"
  },
  {
    "text": "Which we could, let's just draw it over here.",
    "start": "4659050",
    "end": "4662270"
  },
  {
    "text": "One. [NOISE] And so now you- these positives,",
    "start": "4665340",
    "end": "4670929"
  },
  {
    "text": "I'll just draw them much bigger. You know, you've got big positives here and some small negatives, and some small positives,",
    "start": "4670930",
    "end": "4677275"
  },
  {
    "text": "some big negatives here. And so now your model, to try and get these right,",
    "start": "4677275",
    "end": "4683440"
  },
  {
    "text": "might pick a decision boundary like this, right? And this is also basically recursive in that each step, right?",
    "start": "4683440",
    "end": "4690610"
  },
  {
    "text": "You're gonna be reweighting each of the examples based on how many of your previous ones have gotten it wrong or right in the past.",
    "start": "4690610",
    "end": "4697360"
  },
  {
    "text": "[NOISE] And so basically what you're doing is you can sort of weight each one of these classifiers.",
    "start": "4697360",
    "end": "4705925"
  },
  {
    "text": "You can determine [NOISE]",
    "start": "4705925",
    "end": "4712810"
  },
  {
    "text": "for classifier Gm,",
    "start": "4712810",
    "end": "4719980"
  },
  {
    "text": "a weight Alpha m, which is proportional to how many examples you got wrong or right.",
    "start": "4719980",
    "end": "4729310"
  },
  {
    "text": "So a better classifier, you wanna give it more weight, um, and, uh, a bad classifier you wanna give it less weight proportional.",
    "start": "4729310",
    "end": "4738679"
  },
  {
    "text": "And, uh, I think that the exact equation used in AdaBoost, for example,",
    "start": "4739710",
    "end": "4744820"
  },
  {
    "text": "is just log of 1 minus the error of your nth model divided with basically log odds, okay?",
    "start": "4744820",
    "end": "4754375"
  },
  {
    "text": "And then your total classifier is just F of- or let's just call it G of x again.",
    "start": "4754375",
    "end": "4759520"
  },
  {
    "text": "G of x is just the sum over m of Alpha m,",
    "start": "4759520",
    "end": "4768985"
  },
  {
    "text": "G of m, right? And then each G of m is trained",
    "start": "4768985",
    "end": "4774460"
  },
  {
    "text": "on a weighted- on a reweighted,",
    "start": "4774460",
    "end": "4787690"
  },
  {
    "text": "actually, reweighted training set.",
    "start": "4787690",
    "end": "4793310"
  },
  {
    "text": "And so I've glossed over a lot of the details here in interest of time, but the specifics of an algorithm like this are- will be in the lecture notes.",
    "start": "4796080",
    "end": "4805690"
  },
  {
    "text": "And this algorithm is actually known as AdaBoost. [NOISE] And basically through similar techniques,",
    "start": "4805690",
    "end": "4813715"
  },
  {
    "text": "you can derive algorithms such as XGBoost or gradient boosting machines that also allow you to",
    "start": "4813715",
    "end": "4820240"
  },
  {
    "text": "basically reweight the examples you're getting right or wrong in this sort of dynamic fashion and slowly adding them in this additive fashion to your composite model.",
    "start": "4820240",
    "end": "4829090"
  },
  {
    "text": "[NOISE] And that about finishes it for today. Uh, thanks for coming. Um, yeah, a great rest of your week.",
    "start": "4829090",
    "end": "4837140"
  }
]