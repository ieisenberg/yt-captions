[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5990"
  },
  {
    "text": "Hi, in this module, I'm going to\ntalk about the general strategy",
    "start": "5990",
    "end": "8780"
  },
  {
    "start": "6000",
    "end": "12000"
  },
  {
    "text": "for performing probabilistic\ninference in Bayesian networks.",
    "start": "8780",
    "end": "13110"
  },
  {
    "start": "12000",
    "end": "91000"
  },
  {
    "text": "So recall that a\nBayesian network consists",
    "start": "13110",
    "end": "16350"
  },
  {
    "text": "of a set of random variables,\nfor example cold, allergies,",
    "start": "16350",
    "end": "20700"
  },
  {
    "text": "cough, and itchy eyes.",
    "start": "20700",
    "end": "23468"
  },
  {
    "text": "And then the Bayesian\nnetwork defines",
    "start": "23468",
    "end": "25010"
  },
  {
    "text": "a directed acyclic graph\nover these random variables",
    "start": "25010",
    "end": "28790"
  },
  {
    "text": "that capture the\nqualitative dependencies",
    "start": "28790",
    "end": "31850"
  },
  {
    "text": "between the variables.",
    "start": "31850",
    "end": "33300"
  },
  {
    "text": "For example, cough is\ncaused by cold or allergies.",
    "start": "33300",
    "end": "37190"
  },
  {
    "text": "Itchy eyes is caused\nby allergies alone.",
    "start": "37190",
    "end": "40340"
  },
  {
    "text": "Quantitatively, the\nBayesian network",
    "start": "40340",
    "end": "41900"
  },
  {
    "text": "specifies a set of local\nconditional distributions",
    "start": "41900",
    "end": "45530"
  },
  {
    "text": "of each variable xi\ngiven the parents of i.",
    "start": "45530",
    "end": "52370"
  },
  {
    "text": "And so in this example,\nI would have probability",
    "start": "52370",
    "end": "54620"
  },
  {
    "text": "of c times\nprobability of a times",
    "start": "54620",
    "end": "56730"
  },
  {
    "text": "probability of h given c and a\ntimes probability of i given a.",
    "start": "56730",
    "end": "61906"
  },
  {
    "text": "And then when I multiply all of\nthese probabilities together,",
    "start": "61906",
    "end": "67700"
  },
  {
    "text": "then I get by definition the\njoint probability distribution",
    "start": "67700",
    "end": "74000"
  },
  {
    "text": "over all the rest.",
    "start": "74000",
    "end": "76410"
  },
  {
    "text": "In this case, I have a\njoint distribution over C,",
    "start": "76410",
    "end": "80285"
  },
  {
    "text": "A, H, and I. So you can think\nabout the Bayesian network",
    "start": "80285",
    "end": "88490"
  },
  {
    "text": "as defining this joint\ndistribution, which",
    "start": "88490",
    "end": "90920"
  },
  {
    "text": "is a probabilistic database\nwhere you can answer questions",
    "start": "90920",
    "end": "95119"
  },
  {
    "start": "91000",
    "end": "137000"
  },
  {
    "text": "about this data.",
    "start": "95120",
    "end": "96110"
  },
  {
    "text": "For example, what is\nthe probability of C",
    "start": "96110",
    "end": "99150"
  },
  {
    "text": "given H equals 1 and I equals 1?",
    "start": "99150",
    "end": "102110"
  },
  {
    "text": "Generally, you have\na Bayesian network.",
    "start": "102110",
    "end": "104480"
  },
  {
    "text": "Some of the variables\nyou observe as evidence,",
    "start": "104480",
    "end": "107960"
  },
  {
    "text": "for example H and\nI in this case.",
    "start": "107960",
    "end": "111170"
  },
  {
    "text": "And another set of\nvariables you are",
    "start": "111170",
    "end": "113689"
  },
  {
    "text": "interested in which are\nthe query variables,",
    "start": "113690",
    "end": "117240"
  },
  {
    "text": "so that way Q would be C here.",
    "start": "117240",
    "end": "120360"
  },
  {
    "text": "And what we want to\nproduce is the probability",
    "start": "120360",
    "end": "124100"
  },
  {
    "text": "of the query variables\nconditioned on evidence.",
    "start": "124100",
    "end": "127460"
  },
  {
    "text": "Normally this is a probability\nof Q equals q for each",
    "start": "127460",
    "end": "132950"
  },
  {
    "text": "of the values of little q.",
    "start": "132950",
    "end": "135282"
  },
  {
    "start": "135282",
    "end": "138180"
  },
  {
    "start": "137000",
    "end": "297000"
  },
  {
    "text": "So the overarching\nstrategy that we're",
    "start": "138180",
    "end": "140642"
  },
  {
    "text": "going to take for performing\ninference in Bayesian networks",
    "start": "140642",
    "end": "143099"
  },
  {
    "text": "is to convert them into\nMarkov networks, which",
    "start": "143100",
    "end": "146430"
  },
  {
    "text": "we discussed inference for.",
    "start": "146430",
    "end": "149290"
  },
  {
    "text": "So recall-- we're going to\nwalk through this example.",
    "start": "149290",
    "end": "153579"
  },
  {
    "text": "So recall that the joint\ndistribution over the variables",
    "start": "153580",
    "end": "158830"
  },
  {
    "text": "here is equal to\nsimply the product",
    "start": "158830",
    "end": "162940"
  },
  {
    "text": "of the local conditional\ndistributions by definition",
    "start": "162940",
    "end": "167220"
  },
  {
    "text": "of the Bayesian network, OK?",
    "start": "167220",
    "end": "170250"
  },
  {
    "text": "But these local\nconditional distributions a",
    "start": "170250",
    "end": "173480"
  },
  {
    "text": "are non-negative quantity.",
    "start": "173480",
    "end": "174920"
  },
  {
    "text": "So they can be interpreted as\nfactors in the factor graph.",
    "start": "174920",
    "end": "178670"
  },
  {
    "text": "So let's draw the factor graph.",
    "start": "178670",
    "end": "181349"
  },
  {
    "text": "So here we have the\nsame set of variables.",
    "start": "181350",
    "end": "185690"
  },
  {
    "text": "For every variable, we\nhave a factor corresponding",
    "start": "185690",
    "end": "189110"
  },
  {
    "text": "to this local\nconditional distribution.",
    "start": "189110",
    "end": "190880"
  },
  {
    "text": "We have probability of c,\nprobability of a, probability",
    "start": "190880",
    "end": "195980"
  },
  {
    "text": "of h given c and a, which\nconnects C, A, and H",
    "start": "195980",
    "end": "201200"
  },
  {
    "text": "and then probability\nof i given A.",
    "start": "201200",
    "end": "205040"
  },
  {
    "text": "So in the factor\ngraph representation,",
    "start": "205040",
    "end": "207500"
  },
  {
    "text": "these are simply functions.",
    "start": "207500",
    "end": "209900"
  },
  {
    "text": "This is a function that\ndepends on c and h.",
    "start": "209900",
    "end": "212851"
  },
  {
    "text": "And the factor\ngraph doesn't really",
    "start": "212852",
    "end": "214310"
  },
  {
    "text": "care that it's a\nlocal distribution.",
    "start": "214310",
    "end": "217910"
  },
  {
    "text": "So now remember in a Markov\nnetwork, we take a factor graph",
    "start": "217910",
    "end": "222250"
  },
  {
    "text": "and we multiply all\nthe factors together.",
    "start": "222250",
    "end": "224380"
  },
  {
    "text": "And we divide by\nthe normalization",
    "start": "224380",
    "end": "227770"
  },
  {
    "text": "constant to get this\nproduct to sum to 1.",
    "start": "227770",
    "end": "232830"
  },
  {
    "text": "But notice that in this case\nthat the normalization constant",
    "start": "232830",
    "end": "235560"
  },
  {
    "text": "is exactly 1.",
    "start": "235560",
    "end": "237090"
  },
  {
    "text": "Because we had this\nequality from the definition",
    "start": "237090",
    "end": "241930"
  },
  {
    "text": "of the Bayesian network.",
    "start": "241930",
    "end": "242930"
  },
  {
    "text": "So Z has to be 1 in this case.",
    "start": "242930",
    "end": "246730"
  },
  {
    "text": "So the Bayesian network\nis just a Markov network",
    "start": "246730",
    "end": "249550"
  },
  {
    "text": "with the normalization\nconstant 1.",
    "start": "249550",
    "end": "253090"
  },
  {
    "text": "And that means we can\ntake any Bayesian network",
    "start": "253090",
    "end": "257410"
  },
  {
    "text": "and reinterpret it\nas a Markov network",
    "start": "257410",
    "end": "260769"
  },
  {
    "text": "and answer all sorts\nof marginal queries.",
    "start": "260769",
    "end": "262977"
  },
  {
    "text": "For example, you can ask\nfor the probability of A.",
    "start": "262977",
    "end": "265060"
  },
  {
    "text": "We can ask for the\nprobability of H, and so on.",
    "start": "265060",
    "end": "270200"
  },
  {
    "text": "But I'll just remind you\nthat a single factor connects",
    "start": "270200",
    "end": "274900"
  },
  {
    "text": "all the parents.",
    "start": "274900",
    "end": "276220"
  },
  {
    "text": "So notice that there are two\nedges, C to H, A to H here.",
    "start": "276220",
    "end": "281020"
  },
  {
    "text": "But in the factor\ngraph representation,",
    "start": "281020",
    "end": "283210"
  },
  {
    "text": "you should connect the parents\nand the child into one.",
    "start": "283210",
    "end": "291050"
  },
  {
    "text": "So there's only\none thing missing",
    "start": "291050",
    "end": "294060"
  },
  {
    "text": "from this picture, which is\nthat often in Bayesian networks",
    "start": "294060",
    "end": "297450"
  },
  {
    "start": "297000",
    "end": "491000"
  },
  {
    "text": "you want to condition\non evidence.",
    "start": "297450",
    "end": "300570"
  },
  {
    "text": "So let's condition on\nH and I. To do this,",
    "start": "300570",
    "end": "306390"
  },
  {
    "text": "we're going to define\na Markov network",
    "start": "306390",
    "end": "309210"
  },
  {
    "text": "over the non-conditioned\nvariables.",
    "start": "309210",
    "end": "311979"
  },
  {
    "text": "So in this case, that's going\nto be C equals c, A equals a,",
    "start": "311980",
    "end": "317100"
  },
  {
    "text": "condition on H equals\n1 and I equals 1.",
    "start": "317100",
    "end": "320940"
  },
  {
    "text": "And what I'm going\nto do is we're just",
    "start": "320940",
    "end": "325230"
  },
  {
    "text": "going to substitute the\nvalues of the evidence",
    "start": "325230",
    "end": "329220"
  },
  {
    "text": "into the factors themselves.",
    "start": "329220",
    "end": "332110"
  },
  {
    "text": "So here is a factor graph.",
    "start": "332110",
    "end": "333210"
  },
  {
    "text": "I have only C and A left.",
    "start": "333210",
    "end": "335310"
  },
  {
    "text": "And p of c and p\nof a is the same.",
    "start": "335310",
    "end": "338740"
  },
  {
    "text": "And now we have this factor\nthat depends on C and A,",
    "start": "338740",
    "end": "342960"
  },
  {
    "text": "but h is equal to 1.",
    "start": "342960",
    "end": "344970"
  },
  {
    "text": "So I don't need to\nrepresent h as a variable.",
    "start": "344970",
    "end": "348190"
  },
  {
    "text": "And here the same.",
    "start": "348190",
    "end": "349950"
  },
  {
    "text": "I equals 1.",
    "start": "349950",
    "end": "350730"
  },
  {
    "text": "So I don't need to\nrepresent i as a variable.",
    "start": "350730",
    "end": "353840"
  },
  {
    "text": "But now I take\nthese four factors.",
    "start": "353840",
    "end": "356570"
  },
  {
    "text": "And I multiply\nthem all together.",
    "start": "356570",
    "end": "359660"
  },
  {
    "text": "And I get that as\nthis factor graph.",
    "start": "359660",
    "end": "363140"
  },
  {
    "text": "And now I need to normalize by 1\nover Z. It's a different Z now.",
    "start": "363140",
    "end": "368690"
  },
  {
    "text": "In this case, Z is\nnot 1, because I'm",
    "start": "368690",
    "end": "372260"
  },
  {
    "text": "conditioning on evidence.",
    "start": "372260",
    "end": "374060"
  },
  {
    "text": "And in particular, Z is\ngoing to be the probability",
    "start": "374060",
    "end": "379340"
  },
  {
    "text": "of the evidence.",
    "start": "379340",
    "end": "382180"
  },
  {
    "text": "And you can see this,\nbecause this is a joint--",
    "start": "382180",
    "end": "385810"
  },
  {
    "text": "this is a conditional\ndistribution.",
    "start": "385810",
    "end": "387880"
  },
  {
    "text": "And conditional distribution is\nequal to the joint distribution",
    "start": "387880",
    "end": "392710"
  },
  {
    "text": "divided by the\nmarginal of the thing",
    "start": "392710",
    "end": "395259"
  },
  {
    "text": "that you're conditioning on.",
    "start": "395260",
    "end": "396970"
  },
  {
    "text": "So Z has to be equal to the\nmarginal of the evidence.",
    "start": "396970",
    "end": "401120"
  },
  {
    "text": "But, nonetheless, this\nis a Markov network.",
    "start": "401120",
    "end": "404810"
  },
  {
    "text": "And now, again, we can run\nany inference algorithm",
    "start": "404810",
    "end": "407450"
  },
  {
    "text": "we'd like over this\nMarkov network,",
    "start": "407450",
    "end": "409730"
  },
  {
    "text": "for example, Gibbs sampling.",
    "start": "409730",
    "end": "412030"
  },
  {
    "text": "Well, let me actually do that\nin this little form here.",
    "start": "412030",
    "end": "415940"
  },
  {
    "text": "So here is the\nmedical diagnosis.",
    "start": "415940",
    "end": "418340"
  },
  {
    "text": "We define variables\nC, A, H, and I.",
    "start": "418340",
    "end": "423580"
  },
  {
    "text": "We're going to\ncondition on H equals 1.",
    "start": "423580",
    "end": "425853"
  },
  {
    "text": "I equals 1.",
    "start": "425853",
    "end": "427419"
  },
  {
    "text": "And we're interested in the\nmarginal probability of C.",
    "start": "427420",
    "end": "430780"
  },
  {
    "text": "And we're going to\nrun Gibbs sampling.",
    "start": "430780",
    "end": "434139"
  },
  {
    "text": "So Gibbs sampling,\nremember, is going",
    "start": "434140",
    "end": "438460"
  },
  {
    "text": "to take an arbitrary factor\ngraph or Markov network.",
    "start": "438460",
    "end": "443110"
  },
  {
    "text": "And it's going to go\nthrough an assignment",
    "start": "443110",
    "end": "446050"
  },
  {
    "text": "and reassign each\nvariable one at a time.",
    "start": "446050",
    "end": "448849"
  },
  {
    "text": "And it's going to\naccumulate these counts.",
    "start": "448850",
    "end": "451350"
  },
  {
    "text": "But let me speed\nthis up a little bit",
    "start": "451350",
    "end": "453160"
  },
  {
    "text": "and say I do 1,000\nsteps at a time.",
    "start": "453160",
    "end": "455990"
  },
  {
    "text": "And now you can see\nthat these counts",
    "start": "455990",
    "end": "458500"
  },
  {
    "text": "should converge to the right.",
    "start": "458500",
    "end": "461350"
  },
  {
    "text": "It's about 0.13.",
    "start": "461350",
    "end": "463930"
  },
  {
    "text": "Should converge to the\nright, of probability of C",
    "start": "463930",
    "end": "467830"
  },
  {
    "text": "conditioned on H and I.",
    "start": "467830",
    "end": "474180"
  },
  {
    "text": "So then we're kind of done.",
    "start": "474180",
    "end": "475930"
  },
  {
    "text": "We have a Bayesian network.",
    "start": "475930",
    "end": "478229"
  },
  {
    "text": "We condition on evidence.",
    "start": "478230",
    "end": "480270"
  },
  {
    "text": "We formed this reduced factor\ngraph or Markov network.",
    "start": "480270",
    "end": "484860"
  },
  {
    "text": "And then we just\nrun Gibbs sampling.",
    "start": "484860",
    "end": "489159"
  },
  {
    "text": "So in some sense, we are done.",
    "start": "489160",
    "end": "490810"
  },
  {
    "text": "But I want to push this a\nlittle bit further and show",
    "start": "490810",
    "end": "493950"
  },
  {
    "start": "491000",
    "end": "711000"
  },
  {
    "text": "how we can leverage the\nstructure of Bayesian networks",
    "start": "493950",
    "end": "496320"
  },
  {
    "text": "to optimize things.",
    "start": "496320",
    "end": "497160"
  },
  {
    "start": "497160",
    "end": "498600"
  },
  {
    "text": "So let's take another example\nwhere we're now conditioning",
    "start": "498600",
    "end": "501320"
  },
  {
    "text": "on H. OK, so we're\nconditioning on H.",
    "start": "501320",
    "end": "506140"
  },
  {
    "text": "So let's go through\nthe motions here.",
    "start": "506140",
    "end": "508660"
  },
  {
    "text": "We're going to define a Markov\nnetwork on the variables",
    "start": "508660",
    "end": "512950"
  },
  {
    "text": "that we didn't condition on,\nconditioned on H equals 1.",
    "start": "512950",
    "end": "517330"
  },
  {
    "text": "And that's going to be equal\nto just the product of all",
    "start": "517330",
    "end": "521320"
  },
  {
    "text": "the local conditional\ndistributions",
    "start": "521320",
    "end": "523150"
  },
  {
    "text": "where we've substituted\nnow H equals 1.",
    "start": "523150",
    "end": "527110"
  },
  {
    "text": "And now, the\nnormalization constant",
    "start": "527110",
    "end": "528579"
  },
  {
    "text": "is the probability\nof the evidence.",
    "start": "528580",
    "end": "531790"
  },
  {
    "text": "And now, I can ask\nthe question, what",
    "start": "531790",
    "end": "535579"
  },
  {
    "text": "is the probability of C\nequals 1 given H equals 1?",
    "start": "535580",
    "end": "538730"
  },
  {
    "text": "This is something that I\ncan just go and compute",
    "start": "538730",
    "end": "542630"
  },
  {
    "text": "using Gibbs sampling.",
    "start": "542630",
    "end": "544490"
  },
  {
    "text": "But the question is, can we\nreduce the Markov network",
    "start": "544490",
    "end": "547760"
  },
  {
    "text": "before running inference?",
    "start": "547760",
    "end": "548873"
  },
  {
    "text": "Because if we can get\nthe Markov network to be",
    "start": "548873",
    "end": "550790"
  },
  {
    "text": "a little bit smaller,\nthen hopefully inference",
    "start": "550790",
    "end": "553459"
  },
  {
    "text": "can be a bit faster.",
    "start": "553460",
    "end": "557610"
  },
  {
    "text": "So the answer is yes.",
    "start": "557610",
    "end": "559670"
  },
  {
    "text": "And we're going to show this by\ndoing a little bit of algebra",
    "start": "559670",
    "end": "564190"
  },
  {
    "text": "here.",
    "start": "564190",
    "end": "564690"
  },
  {
    "text": "So here is this Bayesian\nnetwork again where",
    "start": "564690",
    "end": "570630"
  },
  {
    "text": "I've conditioned\non H. So now, let",
    "start": "570630",
    "end": "574020"
  },
  {
    "text": "me compute the\nmarginal distribution",
    "start": "574020",
    "end": "577920"
  },
  {
    "text": "where I marginalize out I. So\nhere, I don't have I anymore.",
    "start": "577920",
    "end": "582410"
  },
  {
    "text": "But I can express this in terms\nof this probability of C, A,",
    "start": "582410",
    "end": "587639"
  },
  {
    "text": "and I given H where\nI simply sum out",
    "start": "587640",
    "end": "591380"
  },
  {
    "text": "all possible values of\n[AUDIO OUT] so this is just",
    "start": "591380",
    "end": "594350"
  },
  {
    "text": "the definition of marginal\nprobability inference.",
    "start": "594350",
    "end": "599130"
  },
  {
    "text": "So now, using the definition\nof the Bayesian network,",
    "start": "599130",
    "end": "603740"
  },
  {
    "text": "I can rewrite the\njoint distribution",
    "start": "603740",
    "end": "607279"
  },
  {
    "text": "in terms of local\nconditional distribution.",
    "start": "607280",
    "end": "610960"
  },
  {
    "text": "OK, so and now, I\nmake an observation,",
    "start": "610960",
    "end": "619440"
  },
  {
    "text": "which is that summing over i.",
    "start": "619440",
    "end": "622310"
  },
  {
    "text": "But none of this\nactually depends on I",
    "start": "622310",
    "end": "624260"
  },
  {
    "text": "except for this last factor.",
    "start": "624260",
    "end": "627500"
  },
  {
    "text": "So what I can do is\npush all of this stuff",
    "start": "627500",
    "end": "630860"
  },
  {
    "text": "out or equivalently push\nthe summation inside.",
    "start": "630860",
    "end": "634640"
  },
  {
    "text": "So now, it's wrapped tightly\naround this p of i given a.",
    "start": "634640",
    "end": "640660"
  },
  {
    "text": "Now, what is this?",
    "start": "640660",
    "end": "643100"
  },
  {
    "text": "By definition of local\nconditional distributions,",
    "start": "643100",
    "end": "646019"
  },
  {
    "text": "this is exactly 1,\nso it gets dropped.",
    "start": "646020",
    "end": "649990"
  },
  {
    "text": "So now, I have this nicer form.",
    "start": "649990",
    "end": "652230"
  },
  {
    "text": "But not only is\nit smaller, let's",
    "start": "652230",
    "end": "655230"
  },
  {
    "text": "try to understand what it is.",
    "start": "655230",
    "end": "657029"
  },
  {
    "text": "This is the probability\nof c, probability of a,",
    "start": "657030",
    "end": "661830"
  },
  {
    "text": "probability of h\nequals 1 given c and a.",
    "start": "661830",
    "end": "666090"
  },
  {
    "text": "So it's as if this variable\nI didn't exist at all.",
    "start": "666090",
    "end": "673030"
  },
  {
    "text": "So this is a general idea\nbehind Bayesian networks, which",
    "start": "673030",
    "end": "677160"
  },
  {
    "text": "is that you can throw\naway any unobserved leaves",
    "start": "677160",
    "end": "680339"
  },
  {
    "text": "before running inference.",
    "start": "680340",
    "end": "682750"
  },
  {
    "text": "So this is very powerful because\nit connects marginalization",
    "start": "682750",
    "end": "687930"
  },
  {
    "text": "over variables,\nwhich is generally",
    "start": "687930",
    "end": "689760"
  },
  {
    "text": "an algebraic operation.",
    "start": "689760",
    "end": "691020"
  },
  {
    "text": "It involves a lot of\nhard work with removal,",
    "start": "691020",
    "end": "695100"
  },
  {
    "text": "which is a graph\noperation, which",
    "start": "695100",
    "end": "697279"
  },
  {
    "text": "is more intuitive and\ntrivial in this case.",
    "start": "697280",
    "end": "701100"
  },
  {
    "text": "So in general,\nmarginalization is hard.",
    "start": "701100",
    "end": "702829"
  },
  {
    "text": "But when they are unobserved\nleaves of a Bayesian network,",
    "start": "702830",
    "end": "706430"
  },
  {
    "text": "it is trivial or just remove it.",
    "start": "706430",
    "end": "708649"
  },
  {
    "start": "708650",
    "end": "712520"
  },
  {
    "start": "711000",
    "end": "848000"
  },
  {
    "text": "So here is another type of\nstructure we can exploit,",
    "start": "712520",
    "end": "715460"
  },
  {
    "text": "which is actually not\nspecific to Bayesian networks.",
    "start": "715460",
    "end": "718540"
  },
  {
    "text": "It shows up more generally\nin Markov networks.",
    "start": "718540",
    "end": "721690"
  },
  {
    "text": "Well, let's take\nanother example here.",
    "start": "721690",
    "end": "723540"
  },
  {
    "text": "We're going to condition\non I at this time.",
    "start": "723540",
    "end": "726250"
  },
  {
    "text": "So here, we're going to define\nthis Markov network, where",
    "start": "726250",
    "end": "730560"
  },
  {
    "text": "let's just write down this\nquery that we're interested in.",
    "start": "730560",
    "end": "734290"
  },
  {
    "text": "We're interested in C equals\nc given I equals 1 here.",
    "start": "734290",
    "end": "739220"
  },
  {
    "text": "And expanding it out\nbased on the definition",
    "start": "739220",
    "end": "742310"
  },
  {
    "text": "of marginal\nprobability, I can put",
    "start": "742310",
    "end": "745190"
  },
  {
    "text": "in probability of C, A,\nand H where I sum over",
    "start": "745190",
    "end": "749240"
  },
  {
    "text": "all possible values\nof A and H. So I'm",
    "start": "749240",
    "end": "751580"
  },
  {
    "text": "marginalizing out A and H here.",
    "start": "751580",
    "end": "754690"
  },
  {
    "text": "And by definition of\nthe Bayesian network,",
    "start": "754690",
    "end": "757780"
  },
  {
    "text": "I can replace this with\nthe local conditional",
    "start": "757780",
    "end": "762100"
  },
  {
    "text": "distributions.",
    "start": "762100",
    "end": "764019"
  },
  {
    "text": "And now, using the\nsame trick as before,",
    "start": "764020",
    "end": "766570"
  },
  {
    "text": "I notice that H is an unobserved\nleaf, so I can actually",
    "start": "766570",
    "end": "770530"
  },
  {
    "text": "marginalize out H. And this\nfactor disappears graphically.",
    "start": "770530",
    "end": "774790"
  },
  {
    "text": "This H disappears.",
    "start": "774790",
    "end": "778220"
  },
  {
    "text": "And now, I am left with\nthis Bayesian network,",
    "start": "778220",
    "end": "784839"
  },
  {
    "text": "where notice that the only\nthing that depends on c",
    "start": "784840",
    "end": "792410"
  },
  {
    "text": "is this p of c.",
    "start": "792410",
    "end": "794750"
  },
  {
    "text": "We're going to try to pull it\nout and rewrite it as follows.",
    "start": "794750",
    "end": "800060"
  },
  {
    "text": "And now, I have p of\nc times some mess.",
    "start": "800060",
    "end": "803890"
  },
  {
    "text": "And the nice thing\nin this case is",
    "start": "803890",
    "end": "805840"
  },
  {
    "text": "that this mess is\njust the constant",
    "start": "805840",
    "end": "809330"
  },
  {
    "text": "because it doesn't depend on c.",
    "start": "809330",
    "end": "812710"
  },
  {
    "text": "And moreover, because p\nof c is a distribution",
    "start": "812710",
    "end": "815710"
  },
  {
    "text": "and this left-hand\nside is a distribution,",
    "start": "815710",
    "end": "817510"
  },
  {
    "text": "this constant is actually 1.",
    "start": "817510",
    "end": "820580"
  },
  {
    "text": "So because C graphically,\nC and this AI sub-graph,",
    "start": "820580",
    "end": "827150"
  },
  {
    "text": "is actually disconnected,\nwhich means that I can simply",
    "start": "827150",
    "end": "831830"
  },
  {
    "text": "remove this part.",
    "start": "831830",
    "end": "835150"
  },
  {
    "text": "So generally, I can throw away\nany disconnected components",
    "start": "835150",
    "end": "840730"
  },
  {
    "text": "before running inference, OK?",
    "start": "840730",
    "end": "844199"
  },
  {
    "text": "So in general, let's\nsummarize here.",
    "start": "844200",
    "end": "849790"
  },
  {
    "start": "848000",
    "end": "916000"
  },
  {
    "text": "We've tackled the\nproblem of how to perform",
    "start": "849790",
    "end": "851620"
  },
  {
    "text": "probabilistic inference\nin Bayesian networks",
    "start": "851620",
    "end": "853570"
  },
  {
    "text": "by reducing the problem to\ninference in a Markov network.",
    "start": "853570",
    "end": "858490"
  },
  {
    "text": "So to prepare the\nMarkov network,",
    "start": "858490",
    "end": "862420"
  },
  {
    "text": "we're going to first\ncondition on the evidence.",
    "start": "862420",
    "end": "865190"
  },
  {
    "text": "So this is tantamount\nto substituting",
    "start": "865190",
    "end": "867550"
  },
  {
    "text": "the values of the\nevidence into the factors.",
    "start": "867550",
    "end": "871459"
  },
  {
    "text": "Then we throw away\nany unobserved leaves,",
    "start": "871460",
    "end": "873770"
  },
  {
    "text": "in this case, H. We throw away\nany disconnected components.",
    "start": "873770",
    "end": "879260"
  },
  {
    "text": "But these two are\njust optimizations,",
    "start": "879260",
    "end": "881090"
  },
  {
    "text": "which are totally\noptional, but it will often",
    "start": "881090",
    "end": "883370"
  },
  {
    "text": "save you some work.",
    "start": "883370",
    "end": "885200"
  },
  {
    "text": "Now, we define a Markov network\nover the remaining factors.",
    "start": "885200",
    "end": "890200"
  },
  {
    "text": "Now, we just have\na factor graph,",
    "start": "890200",
    "end": "894620"
  },
  {
    "text": "where we can now run your\nfavorite inference algorithm.",
    "start": "894620",
    "end": "897810"
  },
  {
    "text": "So in the case,\nit's very simple.",
    "start": "897810",
    "end": "899840"
  },
  {
    "text": "As it is the case here, you\ncan just do it manually.",
    "start": "899840",
    "end": "902750"
  },
  {
    "text": "Or if what's remaining\nis more complicated,",
    "start": "902750",
    "end": "906020"
  },
  {
    "text": "then you can do something\nlike Gibbs sampling.",
    "start": "906020",
    "end": "909030"
  },
  {
    "text": "And that's the end.",
    "start": "909030",
    "end": "911120"
  },
  {
    "start": "911120",
    "end": "915000"
  }
]