[
  {
    "start": "0",
    "end": "5545"
  },
  {
    "text": "Hey, everybody. Welcome back. We're going to\nstart talking more about the state of efficient\nreinforcement learning today.",
    "start": "5545",
    "end": "11240"
  },
  {
    "text": "But before we do that, we're\ngoing to start with a Check Your Understanding. So this asks you to think back\nabout what we were learning",
    "start": "11240",
    "end": "18790"
  },
  {
    "text": "from multi-armed bandits. I would probably do\none and six first because they're warm-ups,\nand then the rest of these.",
    "start": "18790",
    "end": "25689"
  },
  {
    "text": "Just to clarify, in terms of\nnotation, I'm using f of delta here to be a function of delta.",
    "start": "25690",
    "end": "32470"
  },
  {
    "text": "Because I was slightly loose on\nexactly what the dependence is on delta, in terms of whether\nit's like delta over t",
    "start": "32470",
    "end": "39670"
  },
  {
    "text": "or what we're going to\nchoose for that function. So I just wanted to be\nagnostic to that there and put it as a log of\na function of delta.",
    "start": "39670",
    "end": "45840"
  },
  {
    "start": "45840",
    "end": "109550"
  },
  {
    "text": "As usual, feel free to look back\nat your notes from last week, if you want to refresh\nyour brain on the notation.",
    "start": "109550",
    "end": "114655"
  },
  {
    "start": "114655",
    "end": "167680"
  },
  {
    "text": "All right. One more minute to write\ndown your initial answers and then I'll ask you to turn\nto a neighbor and compare. ",
    "start": "167680",
    "end": "226190"
  },
  {
    "text": "All right. Why don't you\ncompare your answers to someone that's nearby you? [SIDE CONVERSATION]",
    "start": "226190",
    "end": "233088"
  },
  {
    "start": "233088",
    "end": "286962"
  },
  {
    "text": "All right, great. Let's come back together. So I think most people\nconverged on the same answer",
    "start": "286962",
    "end": "292100"
  },
  {
    "text": "for the first one, which\nis, yes, algorithms that minimize regret do\nalso maximize reward.",
    "start": "292100",
    "end": "298262"
  },
  {
    "text": "Ooh, hold on. Pen is not working. Let's see if I can\ngrab a different one.",
    "start": "298262",
    "end": "306410"
  },
  {
    "text": "So the first one is true,\nif I can get this up here. OK. So the first one is true. If you minimize regret,\nyou also maximize reward.",
    "start": "306410",
    "end": "316070"
  },
  {
    "text": "For the second one,\nis that one true? Do you want to\nsay why it's true?",
    "start": "316070",
    "end": "321831"
  },
  {
    "text": "Are you saying it's false? I'm saying the\nsecond one is true. ",
    "start": "321831",
    "end": "328550"
  },
  {
    "text": "Hold on. Let's see if I can get\nmy thing to power up. My pen isn't working. So the second one is--",
    "start": "328550",
    "end": "333555"
  },
  {
    "text": " let's double check that I-- I'll keep it back onto here,\nin terms of the answers.",
    "start": "333555",
    "end": "339670"
  },
  {
    "text": "I moved things around a\nlittle bit, last minute, which is always\ndangerous, but I wanted to include a couple\nadditional ones.",
    "start": "339670",
    "end": "346340"
  },
  {
    "text": "OK, let's see if we can make\nthis do the right thing. OK. So the second one\nshould be true.",
    "start": "346340",
    "end": "351870"
  },
  {
    "text": "This is basically\nthe UCB algorithm, which is this is the\nempirical estimate of the performance of each arm.",
    "start": "351870",
    "end": "358030"
  },
  {
    "text": "So in the case where you just\nhave a finite set of arms, which we can also think of as\na finite set of actions,",
    "start": "358030",
    "end": "363810"
  },
  {
    "text": "we just look at what\ntheir average reward was. Nt of a was how many times have\nwe pulled action a after t time",
    "start": "363810",
    "end": "372240"
  },
  {
    "text": "steps. And log of f of delta\nwas just the term that we had to try to express\nthe dependence on delta.",
    "start": "372240",
    "end": "379169"
  },
  {
    "text": "Delta was used to look\nat confidence intervals that we were using for the\nupper confidence bound.",
    "start": "379170",
    "end": "385200"
  },
  {
    "text": "So this is true. The third one is also true.",
    "start": "385200",
    "end": "391050"
  },
  {
    "text": "So, in general, with our\nconfidence intervals, you will be selecting all arms\nan infinite number of times.",
    "start": "391050",
    "end": "398420"
  },
  {
    "text": "But it might be\nreally slow later on. Let's say you have a really\nbig gap between arms.",
    "start": "398420",
    "end": "404440"
  },
  {
    "text": "Then that log term-- and you'll have a t dependence\nin there, in general.",
    "start": "404440",
    "end": "410830"
  },
  {
    "text": "That will continue\nto grow a little bit. So you'll sample another arm\nagain, which helps with the fact that you might have\nbeen really unlucky",
    "start": "410830",
    "end": "417280"
  },
  {
    "text": "and gotten a really weird\nestimate of the arm performance, so far. OK.",
    "start": "417280",
    "end": "422360"
  },
  {
    "text": "This one was a\nlittle bit subtle. And I realize it could\nbe not quite clear here whether I was\nasking you to think",
    "start": "422360",
    "end": "427960"
  },
  {
    "text": "about the T over delta part or\nthe 1 over square root Nt of a. I wanted you to focus\non the first thing.",
    "start": "427960",
    "end": "435520"
  },
  {
    "text": "So what this is saying\nhere is that, instead of shrinking our confidence\nintervals by a rate of 1",
    "start": "435520",
    "end": "443530"
  },
  {
    "text": "over square root Nt, we're\nshrinking them at a rate of n to the minus fourth.",
    "start": "443530",
    "end": "449870"
  },
  {
    "text": "Let me just write that. ",
    "start": "449870",
    "end": "455910"
  },
  {
    "text": "[ERASING WHITEBOARD] ",
    "start": "455910",
    "end": "460960"
  },
  {
    "text": "That's pretty squeaky. OK. All right. So let me just give--\nso we're shrinking it.",
    "start": "460960",
    "end": "466320"
  },
  {
    "text": " So will that mean that\nour confidence intervals",
    "start": "466320",
    "end": "473810"
  },
  {
    "text": "are wider or narrower for\nthe same number of counts? So let's say versus Nt of\na to the minus one half.",
    "start": "473810",
    "end": "486169"
  },
  {
    "text": "So, for example, if Nt\nof a is equal to 100, you've pulled this\narm 100 times.",
    "start": "486170",
    "end": "491900"
  },
  {
    "text": "Which of these two is going to\nbe bigger, the one on the left or the one on the right?",
    "start": "491900",
    "end": "497545"
  },
  {
    "text": " The one on the right. That's right.",
    "start": "497545",
    "end": "502810"
  },
  {
    "text": "So instead of it being-- oh, the other way around. So this is going to be-- so if\nyou have 100 to the minus 1/4",
    "start": "502810",
    "end": "509990"
  },
  {
    "text": "versus 100 to the minus 1/2,\nthis is going to be 1/10. And this is going to be\napproximately 1 over 3.",
    "start": "509990",
    "end": "517760"
  },
  {
    "text": "There's several\ndifferent inverses here. I know. What this means, basically,\nis that we're growing--",
    "start": "517760",
    "end": "523260"
  },
  {
    "text": "we're shrinking our\nconfidence intervals slower. So another thing you might\nsee often, as a bonus term--",
    "start": "523260",
    "end": "531330"
  },
  {
    "text": "DeepMind often uses\nthis, in particular, for some of their algorithms,\nis \"to the minus 1.\"",
    "start": "531330",
    "end": "537670"
  },
  {
    "text": "So that's a faster rate. So then that one\nwould be 1 over 100.",
    "start": "537670",
    "end": "544470"
  },
  {
    "text": "You can think of\nthese as trading off different amounts\nof, essentially, how much exploration\nyou're going to get.",
    "start": "544470",
    "end": "549930"
  },
  {
    "text": "Because this is\nsaying how quickly are you collapsing\nyour confidence interval as you have more data.",
    "start": "549930",
    "end": "556920"
  },
  {
    "text": "Now, as somebody\npointed-- was asking me about-- when I was going\naround, they were like, well, we didn't just\nrandomly pick this.",
    "start": "556920",
    "end": "563290"
  },
  {
    "text": "We picked this because\nof the uncertainty bounds that we derived\nfrom Hoeffding. So Hoeffding said, if you\nhave an empirical estimate",
    "start": "563290",
    "end": "570990"
  },
  {
    "text": "of your mean, how far away could\nthat be from the true mean? Well, under pretty\nmild conditions",
    "start": "570990",
    "end": "576990"
  },
  {
    "text": "about your variable\nbeing bounded, we could get this 1\nover squared N rate.",
    "start": "576990",
    "end": "583170"
  },
  {
    "text": "So someone was asking me,\nvery reasonably, well, why would you pick, say,\nthis or something else?",
    "start": "583170",
    "end": "588840"
  },
  {
    "text": "You might pick this\nbecause you just don't want to explore as much. So even though this\nholds for our theory,",
    "start": "588840",
    "end": "595990"
  },
  {
    "text": "it's often somewhat\nconservative in practice. So you might just pick something\nlike a faster exploration rate",
    "start": "595990",
    "end": "602519"
  },
  {
    "text": "because, empirically, you\nwant to just explore less. And you could think of\nthat as being related back",
    "start": "602520",
    "end": "609300"
  },
  {
    "text": "to what we saw with\nPPO, that a lot of their theoretical\nderivation said, this is what your\nstep size should be,",
    "start": "609300",
    "end": "615400"
  },
  {
    "text": "but it was way too conservative\nfor most realistic applications. So they just\nchanged it, and they introduced the clipping thing.",
    "start": "615400",
    "end": "621960"
  },
  {
    "text": "On the other hand,\nthere might be cases for which you\nmight not be sure you",
    "start": "621960",
    "end": "627149"
  },
  {
    "text": "could get this sort of rate. Or you might have other\nreasons to think you might need more exploration.",
    "start": "627150",
    "end": "633250"
  },
  {
    "text": "So, for example, maybe\nthings are non-stationary. And you think, my\ncustomer preferences are actually changing over time.",
    "start": "633250",
    "end": "639820"
  },
  {
    "text": "And so I want to\nexplore more, over time, than I would if I assumed\nthat I was stationary.",
    "start": "639820",
    "end": "646569"
  },
  {
    "text": "And we'll talk more about\nstationarity in just a second. So given all of that, this\nmeans that this expression",
    "start": "646570",
    "end": "654000"
  },
  {
    "text": "would actually have wider\nconfidence intervals and probably a higher\nupper confidence bound than our original\nalgorithm, which",
    "start": "654000",
    "end": "659970"
  },
  {
    "text": "means that we would still expect\nthat, over time, it will learn-- [SNEEZES] --pull the optimal arm-- bless\nyou-- more than any other arms.",
    "start": "659970",
    "end": "667449"
  },
  {
    "text": "But it probably won't have\nas tight regret bounds because we may be\nexploring too much.",
    "start": "667450",
    "end": "674610"
  },
  {
    "text": "Now, this was an\ninteresting one. Will this, if we add this\nparticular bonus term,",
    "start": "674610",
    "end": "681480"
  },
  {
    "text": "make the algorithm\noptimistic, with respect to the empirical rewards? ",
    "start": "681480",
    "end": "710130"
  },
  {
    "text": "Somebody want to\nsay if it's going to make-- if we add\non a bonus term, will it make it\noptimistic, with respect",
    "start": "710130",
    "end": "715168"
  },
  {
    "text": "to the empirical rewards?  Just the empirical rewards, so\ncompared to your empirical mean.",
    "start": "715168",
    "end": "722515"
  },
  {
    "text": " I'm not trying to make\nit a trick question.",
    "start": "722515",
    "end": "728605"
  },
  {
    "text": " Yes. Yes, exactly. So if you just add 20 to\nyour empirical estimate,",
    "start": "728605",
    "end": "736172"
  },
  {
    "text": "it will be optimistic,\nwith respect to your empirical estimate. But is it guaranteed\nto be optimistic,",
    "start": "736172",
    "end": "741410"
  },
  {
    "text": "with respect to your true mean? ",
    "start": "741410",
    "end": "752880"
  },
  {
    "text": "So imagine if I had\nsaid B was 0.001. It would still make\nit be optimistic,",
    "start": "752880",
    "end": "759390"
  },
  {
    "text": "with respect to your\nempirical rewards. But would it necessarily\nbe optimistic, with respect to your true mean? ",
    "start": "759390",
    "end": "768770"
  },
  {
    "text": "In general, no, right? So if you think back\nto our bandits, which just had binary\nrewards, let's say",
    "start": "768770",
    "end": "775850"
  },
  {
    "text": "you have a coin that actually\nhas a 0.5 probability of getting a heads, which we'll call a 1.",
    "start": "775850",
    "end": "781370"
  },
  {
    "text": "If you flip it once\nand you get a tails, your empirical\nestimate will be 0. If you add a bonus of 0.01, your\nempirical estimate will be 0.01.",
    "start": "781370",
    "end": "791340"
  },
  {
    "text": "The true value of the\nmean is still 0.5. So one of the key ideas\nfrom using Hoeffding",
    "start": "791340",
    "end": "796850"
  },
  {
    "text": "and explicit upper\nconfidence bounds is that, in general, it's\nnot easy to figure out a simple bonus term\nyou can add in order",
    "start": "796850",
    "end": "804080"
  },
  {
    "text": "to make things optimistic. And so that's why you\nmight, in general, want",
    "start": "804080",
    "end": "809540"
  },
  {
    "text": "to be using these Hoeffding\nor other explicitly derived confidence intervals.",
    "start": "809540",
    "end": "816800"
  },
  {
    "text": "OK, great. And then the last one is true. Does anybody have any\nquestions about these? Yeah? Can you explain\nnumber three again?",
    "start": "816800",
    "end": "825649"
  },
  {
    "text": "I'm not sure whether\ndependence on--  It's inside of here.",
    "start": "825650",
    "end": "833070"
  },
  {
    "text": "So if you go back to the\nslides from last time-- let me see if I\njust have them up. ",
    "start": "833070",
    "end": "840760"
  },
  {
    "text": "Yeah, we create these\nupper confidence bounds. ",
    "start": "840760",
    "end": "850260"
  },
  {
    "text": "So, in general, we define\nthese upper confidence bounds. We talked about how\nwe need the bounds",
    "start": "850260",
    "end": "856170"
  },
  {
    "text": "to hold over all-time steps, t. And so, in fact, this was\nnot a perfect expression",
    "start": "856170",
    "end": "862667"
  },
  {
    "text": "that we're going to have\nsome sort of t dependence inside of the log. And in general, we'll have\nsomething like t or t squared",
    "start": "862667",
    "end": "869970"
  },
  {
    "text": "inside of the log. And so that will introduce\na dependence on the time-- either the time step, so far,\nor your total time horizon",
    "start": "869970",
    "end": "877020"
  },
  {
    "text": "inside of your upper\nconfidence bound.  Any other questions\nabout this part?",
    "start": "877020",
    "end": "883270"
  },
  {
    "start": "883270",
    "end": "888410"
  },
  {
    "text": "OK. All right. I'll make sure that the\nsolutions are aligned.",
    "start": "888410",
    "end": "894110"
  },
  {
    "text": "So last time we talked\nabout Bayesians. Sorry. We talked about bandits, which\nwere this single-state version",
    "start": "894110",
    "end": "901100"
  },
  {
    "text": "of Markov decision processes. Your actions didn't make any\ndifference to the next state because you're always\nin a single state.",
    "start": "901100",
    "end": "907759"
  },
  {
    "text": "We talked about how people\noften use the word \"arms\" as an equivalent for actions. And, there, we were\ntrying to be really",
    "start": "907760",
    "end": "913880"
  },
  {
    "text": "explicit about uncertainty\nover the rewards. And we talked about algorithm,\nupper confidence bounds",
    "start": "913880",
    "end": "920330"
  },
  {
    "text": "for trying to be\noptimistic, with respect to that uncertainty. Well, today what we're\ngoing to focus on mostly",
    "start": "920330",
    "end": "925370"
  },
  {
    "text": "is Bayesian bandits. And we'll get there\nin a few minutes. Before we do that, I think\nit's nice to think about--",
    "start": "925370",
    "end": "931577"
  },
  {
    "text": "I think it's exciting to think\nabout all the application areas where these come up. And I wanted to go through\nthis example, which",
    "start": "931577",
    "end": "937820"
  },
  {
    "text": "I think I mentioned\nbriefly in lecture one, just again, to think about all\nthe complexities which come up",
    "start": "937820",
    "end": "943670"
  },
  {
    "text": "when we want to try to\nuse these in practice and where bandit algorithms,\nin particular, might be used.",
    "start": "943670",
    "end": "949080"
  },
  {
    "text": "So this is a really\nbeautiful paper by Hamsa Bastani, which was\nin Nature a few years ago.",
    "start": "949080",
    "end": "955060"
  },
  {
    "text": "And they were trying to tackle a\nreally important problem, which was, at the time, as\neverything was shutting down",
    "start": "955060",
    "end": "960750"
  },
  {
    "text": "with COVID, all\nthese countries had to decide on a quarantine\nprotocol and who to test.",
    "start": "960750",
    "end": "967230"
  },
  {
    "text": "So, before, a number of\ncountries basically almost entirely shut down travel.",
    "start": "967230",
    "end": "972670"
  },
  {
    "text": "But particularly in the\nbeginning, people were letting-- And even then, often,\nthere might be exceptions.",
    "start": "972670",
    "end": "978730"
  },
  {
    "text": "So as people come into\na border crossing, countries had to\ndecide who to test.",
    "start": "978730",
    "end": "985590"
  },
  {
    "text": "Now, they couldn't\nnecessarily test everybody because resources are finite. They're also having\ntesting facilities",
    "start": "985590",
    "end": "992518"
  },
  {
    "text": "they're using for testing\nall of their own individuals. And tests are expensive. In addition, when\nsomeone is tested,",
    "start": "992518",
    "end": "998230"
  },
  {
    "text": "they are going to ask\nthem to quarantine. Depending on where\nyou were in the world, that might actually have been\nfunded by the government.",
    "start": "998230",
    "end": "1004620"
  },
  {
    "text": "So you have to go to a\nquarantine hotel, which also costs the government money. So there are a lot of\nreasons, in this case,",
    "start": "1004620",
    "end": "1010220"
  },
  {
    "text": "that your resources are limited\nand you don't necessarily want to test everyone. Also, in general, it may not\nbe necessary to test everybody,",
    "start": "1010220",
    "end": "1016430"
  },
  {
    "text": "if you're trying to minimize\nthe probability of letting in people that have COVID,\nin terms of limiting spread.",
    "start": "1016430",
    "end": "1023688"
  },
  {
    "text": "So if there's someone\nthat's from somewhere where there's no COVID, then you don't\nnecessarily need to test them. So this is the setting.",
    "start": "1023688",
    "end": "1029760"
  },
  {
    "text": "What happens is that when\npeople were coming into Greece, they would submit\na form in advance,",
    "start": "1029760",
    "end": "1035849"
  },
  {
    "text": "like when you go to the airport\nor before you go, et cetera. And then what they would do is\nthey had this approach called",
    "start": "1035849",
    "end": "1043130"
  },
  {
    "text": "Eva, which tried to\nuse the prior testing results to figure out who to\nactually test when they came.",
    "start": "1043130",
    "end": "1050240"
  },
  {
    "text": "So what would happen is that,\nthen, when somebody comes, like the next day,\neither they would say, we're not going to\ntest you at all--",
    "start": "1050240",
    "end": "1056240"
  },
  {
    "text": "and then you would\nleave the premises. Or, for a subset of\npeople, based on the form, based on where they\nwere coming and based",
    "start": "1056240",
    "end": "1062690"
  },
  {
    "text": "on prior results they had, they\nwould decide to test someone. Then, after you got that test,\nyou would send it to a lab.",
    "start": "1062690",
    "end": "1069770"
  },
  {
    "text": "And it normally would\ntake 24 to 48 hours. I don't remember exactly\nwhat kind of test",
    "start": "1069770",
    "end": "1074990"
  },
  {
    "text": "they were using there. Maybe it was some\nsort of rapid PCR. I don't remember. Those would go to\na central database.",
    "start": "1074990",
    "end": "1081930"
  },
  {
    "text": "And then they would\nuse those results. All these people would\nquarantine for 24 to 96 hours",
    "start": "1081930",
    "end": "1087440"
  },
  {
    "text": "or so, during this time period. They would get the results back. If you're clear, you\ncan go and proceed.",
    "start": "1087440",
    "end": "1093183"
  },
  {
    "text": "Otherwise, you need to\ncontinue to quarantine. And then they're going\nto use this information to go back to Eva and\nupdate their algorithm.",
    "start": "1093183",
    "end": "1100549"
  },
  {
    "text": "So this is really cool\nbecause this is an opportunity to try to be very\ncareful about resources,",
    "start": "1100550",
    "end": "1108269"
  },
  {
    "text": "but really do so in a\nway that still preserves the safety of the\nindividuals in the country, as much as possible,\nand the public health.",
    "start": "1108270",
    "end": "1115130"
  },
  {
    "text": "So I like that, in\nthe Bastani paper, they describe this as a\nnon-stationary contextual batch",
    "start": "1115130",
    "end": "1120259"
  },
  {
    "text": "bandit problem with delayed\nfeedback and constraints. OK. So that's quite a mouthful.",
    "start": "1120260",
    "end": "1125780"
  },
  {
    "text": "But I think it's really\nnice to think about-- as we go from this simple\nsetting of just thinking there are k arms, we can think\nabout all the practical things",
    "start": "1125780",
    "end": "1133640"
  },
  {
    "text": "that we might have to\ndeal with in this setting. So, here, in some ways,\nthe k is very small.",
    "start": "1133640",
    "end": "1138960"
  },
  {
    "text": "It's only two. Either you're going\nto test someone, or you're not\ngoing to test them. So it's a very small action\nspace, which is nice.",
    "start": "1138960",
    "end": "1145850"
  },
  {
    "text": "In this case, compared to\nwhat we've seen, so far-- but we'll see this case later.",
    "start": "1145850",
    "end": "1151350"
  },
  {
    "text": "We're going to have context. Context, you can think of\nas just being like states. So people will have\na feature vector that",
    "start": "1151350",
    "end": "1157610"
  },
  {
    "text": "describes what country\nthey're coming from, a bunch of other details\nabout them, and that gives you",
    "start": "1157610",
    "end": "1162710"
  },
  {
    "text": "a state that we're\ngoing to use to decide whether or not to test someone. So that's why it's contextual.",
    "start": "1162710",
    "end": "1168800"
  },
  {
    "text": "It's non-stationary because\nCOVID was constantly evolving and, often, a lot of the\ninformation we were getting",
    "start": "1168800",
    "end": "1176330"
  },
  {
    "text": "was lagged. So if you're in\nGreece, you might be able to see information\nfrom Sweden and from China and from the US.",
    "start": "1176330",
    "end": "1182600"
  },
  {
    "text": "But all of that information\nis often, likely, probably at a population level. Those people may or may\nnot be the same people",
    "start": "1182600",
    "end": "1189110"
  },
  {
    "text": "that are traveling to Greece. Probably, in general,\nthey're different. And because of the lag, it\nmay or may not be informative.",
    "start": "1189110",
    "end": "1197125"
  },
  {
    "text": "And in fact, in\ntheir paper, they argue a lot of that\ninformation was not as informative as this kind\nof real-time information.",
    "start": "1197125",
    "end": "1203600"
  },
  {
    "text": "It's batched. What I mean by that is that-- and we'll see this more today.",
    "start": "1203600",
    "end": "1208980"
  },
  {
    "text": "You don't get to make a decision\nafter every test or not test. You don't see the\nresult immediately.",
    "start": "1208980",
    "end": "1215120"
  },
  {
    "text": "So what happens\nhere is they say, 200 people fly in on a plane. You have to decide, for\nevery single one of them,",
    "start": "1215120",
    "end": "1221520"
  },
  {
    "text": "whether or not you're\ngoing to test them. And then you wait two days. [LAUGHS] So it's this\ndelayed feedback,",
    "start": "1221520",
    "end": "1228590"
  },
  {
    "text": "and you have to make a decision\nfor everybody before you get to observe that feedback. And so that makes\nit quite tricky.",
    "start": "1228590",
    "end": "1235470"
  },
  {
    "text": "And we'll talk\nmore about why that might be tricky for some of\nthe upper confidence bound algorithms we've seen so far.",
    "start": "1235470",
    "end": "1241610"
  },
  {
    "text": "I think this batching\nis really important for many, many application areas. So if you think back\nto our guest lecture",
    "start": "1241610",
    "end": "1247940"
  },
  {
    "text": "and you think about direct\npreference optimization, this is another area\nwhere, in general, you're",
    "start": "1247940",
    "end": "1253460"
  },
  {
    "text": "going to be able to get a\nbatch of data, label it all and then continue. So in some of the work\nthat my lab is doing",
    "start": "1253460",
    "end": "1260150"
  },
  {
    "text": "and some other\npeople's work, when we're thinking about\ndoing adaptive data collection for\npreference optimization,",
    "start": "1260150",
    "end": "1265800"
  },
  {
    "text": "we, again, need it to be\nable to handle this much more realistic batch setting,\ncompared to getting information",
    "start": "1265800",
    "end": "1271880"
  },
  {
    "text": "after each decision.  So the delayed feedback\nis this 24 to 48 hours.",
    "start": "1271880",
    "end": "1279230"
  },
  {
    "text": "And the final thing\nis constraints. So there are lots of constraints\nin this setting, which",
    "start": "1279230",
    "end": "1285380"
  },
  {
    "text": "also generally changed the\nsetting from a lot of the ones we've thought about, so far. So one is that you might\nhave resource constraints.",
    "start": "1285380",
    "end": "1291769"
  },
  {
    "text": "You might say, at most, we\ncan handle, let's say, 100. I forget exactly what\nit was in the paper. 100 tests a day, so you're going\nto have constraints on that.",
    "start": "1291770",
    "end": "1299820"
  },
  {
    "text": "The second is, politically, you\nmight have constraints, too. It might be tricky\nfor Greece, if they decide that they're not going\nto let in anyone from Sweden.",
    "start": "1299820",
    "end": "1307290"
  },
  {
    "text": "So there might be\ndifferent quotas, or there might be\nother reasons to say, we have to think about\nsome broader types of risks",
    "start": "1307290",
    "end": "1314480"
  },
  {
    "text": "and benefits in these cases. So that's also challenging. One way you can think about\nimplementing this is this",
    "start": "1314480",
    "end": "1320690"
  },
  {
    "text": "could essentially change your\npolicy class that is reasonable. So instead of your\npolicy class saying",
    "start": "1320690",
    "end": "1326720"
  },
  {
    "text": "you can make any decision\nfor any individual, you may now have a\npopulation-level constraint",
    "start": "1326720",
    "end": "1332179"
  },
  {
    "text": "as well. This is something\nthat my lab has thought about some with our\npartner, Sharad Goel, who's",
    "start": "1332180",
    "end": "1338809"
  },
  {
    "text": "at the Harvard Kennedy School. And there, we've\nthought about cases where you might have resource\nconstraints and fairness",
    "start": "1338810",
    "end": "1344420"
  },
  {
    "text": "constraints that mean that\nyou can't just make decisions for people individually. But you need to think\nabout overall trade-offs,",
    "start": "1344420",
    "end": "1351620"
  },
  {
    "text": "in terms of your\npolicy quality, that happen at the population level. The reason that's important\nis because it often introduces",
    "start": "1351620",
    "end": "1358640"
  },
  {
    "text": "a lot of challenges\ncomputationally, when you can't just think of\neach individual separately.",
    "start": "1358640",
    "end": "1363848"
  },
  {
    "text": "All right, so we won't be\nable to cover all of the ways that they handle\nthis algorithmically.",
    "start": "1363848",
    "end": "1369177"
  },
  {
    "text": "But I encourage you\nto read the paper, if you're interested\nin this space. And I think it's a\nreally beautiful example",
    "start": "1369177",
    "end": "1374299"
  },
  {
    "text": "of using reinforcement\nlearning, particularly multi-armed bandits,\nto tackle this problem.",
    "start": "1374300",
    "end": "1379362"
  },
  {
    "text": "One of the things that\nthey had to do-- so this was a real system. They really deployed\nit in Greece. I think, when I\ntalked to Hamsa, she",
    "start": "1379362",
    "end": "1384900"
  },
  {
    "text": "said it came\ntogether in a month. It was a really amazing effort. And then one of the interesting\nthings they also had to do here",
    "start": "1384900",
    "end": "1391820"
  },
  {
    "text": "is to understand how much\nof an impact it made. Because they weren't\ngoing to do a randomized controlled trial in\nCOVID to understand this.",
    "start": "1391820",
    "end": "1399240"
  },
  {
    "text": "So another interesting thing\nthat this paper looks at is using offline methods,\nlike the batch methods",
    "start": "1399240",
    "end": "1405830"
  },
  {
    "text": "you've been seeing\nin the past, to try to estimate the counterfactual\nof how much impact this had.",
    "start": "1405830",
    "end": "1411608"
  },
  {
    "text": "So I think it's a\nreally nice example of a lot of the different\nideas that we've been seeing in this class. ",
    "start": "1411608",
    "end": "1418260"
  },
  {
    "text": "All right. So that's one of the many, many\nways that bandits are useful. Clinical trials is another one.",
    "start": "1418260",
    "end": "1423630"
  },
  {
    "text": "A/B testing, ad placement,\nthere's many, many others as well. But I think this is a really\nnice example in public health.",
    "start": "1423630",
    "end": "1430078"
  },
  {
    "text": "OK. So now let's continue. We're going to talk\nabout, specifically, some of the algorithms that\ncould be relevant to this",
    "start": "1430078",
    "end": "1436000"
  },
  {
    "text": "and, in particular,\nThompson sampling, which is particularly relevant to\nthis kind of batch setting.",
    "start": "1436000",
    "end": "1442610"
  },
  {
    "text": "All right. I'm going to do, very\nquickly, just notation. Remember, regret is the\nopportunity loss per one step.",
    "start": "1442610",
    "end": "1447850"
  },
  {
    "text": "Total regret is the\ntotal opportunity loss. We're using Q to denote\nthe expected reward",
    "start": "1447850",
    "end": "1453360"
  },
  {
    "text": "for a particular arm.  I'm blanking on who\nsuggested this last time.",
    "start": "1453360",
    "end": "1459990"
  },
  {
    "text": "Forgive me. But someone came up\nto me and said, hey, couldn't we have used\njust a smarter, optimistic",
    "start": "1459990",
    "end": "1465899"
  },
  {
    "text": "initialization? Do we have to actually have\nthese upper confidence bounds?",
    "start": "1465900",
    "end": "1472090"
  },
  {
    "text": "And I think that's a very\nreasonable suggestion. And that was a great\nfollow-up to some of the stuff we're going\nto talk about today.",
    "start": "1472090",
    "end": "1479980"
  },
  {
    "text": "So one simple thing\nyou can imagine you could do,\ninstead of worrying about these upper\nconfidence bounds which",
    "start": "1479980",
    "end": "1485108"
  },
  {
    "text": "you have to update all the\ntime, is you just optimize-- you just initialize your\nq hat to some high value.",
    "start": "1485108",
    "end": "1491620"
  },
  {
    "text": "And then you just update\nthat estimate over time. And when you do\nthat, you know that,",
    "start": "1491620",
    "end": "1498100"
  },
  {
    "text": "eventually, you're going to\nconverge to the right thing. Asymptotically, with the\nlaw of large numbers is--",
    "start": "1498100",
    "end": "1504850"
  },
  {
    "text": "you're not changing\nthat initialized value. That initialized value may\nor may not have been right. It'll be an upper bound, and\nit'll just converge to it.",
    "start": "1504850",
    "end": "1512740"
  },
  {
    "text": "So this is an interesting\nthing you can do. ",
    "start": "1512740",
    "end": "1517940"
  },
  {
    "text": "The challenge with that\nis that, in general, you don't know how high to make it.",
    "start": "1517940",
    "end": "1523130"
  },
  {
    "text": "And so if you make\nit really high-- let me just be clear here,\nwhat I mean by \"really high.\"",
    "start": "1523130",
    "end": "1529970"
  },
  {
    "text": "Often, this might\nbe much, much larger than the actual range\nof possible rewards. So maybe your arm rewards\ncan be between 0 and 1.",
    "start": "1529970",
    "end": "1537149"
  },
  {
    "text": "And you initialize this to 70. So sometimes the\ninitialization might be far higher than what\nis actually practical.",
    "start": "1537150",
    "end": "1545179"
  },
  {
    "text": "It does encourage a\nlot of exploration early on, which might\nbe really valuable. But in general, unless you get\nthe value exactly right, which",
    "start": "1545180",
    "end": "1552830"
  },
  {
    "text": "you generally can't know-- because that's why you're trying\nto learn, in the first place.",
    "start": "1552830",
    "end": "1558200"
  },
  {
    "text": "Then you can still lock\non to a suboptimal action. And what do I mean by \"lock\non,\" is that you converge",
    "start": "1558200",
    "end": "1563360"
  },
  {
    "text": "to a suboptimal action and then\nyou never try anything again, which means you'd\nget linear regret.",
    "start": "1563360",
    "end": "1570260"
  },
  {
    "text": "The other thing\nthat's bad is that, if you initialize Q too high,\nthen you're also just not going",
    "start": "1570260",
    "end": "1578690"
  },
  {
    "text": "to benefit from-- you're going to be making\nbad decisions for much longer than you\nactually need to do. ",
    "start": "1578690",
    "end": "1585960"
  },
  {
    "text": "Even though, in theory, this\ncould be a good thing to do or-- sorry. In principle, you might imagine,\nthis is a good thing to do.",
    "start": "1585960",
    "end": "1593510"
  },
  {
    "text": "In reality, it's\nvery hard to set. Now, it's also an\ninteresting question",
    "start": "1593510",
    "end": "1601309"
  },
  {
    "text": "of how you might do this\nwith function approximation. I know you didn't\nimplement deep Q-learning.",
    "start": "1601310",
    "end": "1607890"
  },
  {
    "text": "But if you think back\nto deep Q-learning, where we used a neural network\nto represent the Q function,",
    "start": "1607890",
    "end": "1614870"
  },
  {
    "text": "do you guys think it is\neasy to initialize that, so the values are optimistic?",
    "start": "1614870",
    "end": "1621320"
  },
  {
    "text": "Let's see. At least [MUTED]\nshaking his head. Why not? You're right. It's just [INAUDIBLE]\nfor the network",
    "start": "1621321",
    "end": "1626852"
  },
  {
    "text": "to output specific value. Yeah, it's hard. Right? Maybe you could train\nit on fake data. But then you'd have to\nknow how big the Q is.",
    "start": "1626852",
    "end": "1633075"
  },
  {
    "text": "Yeah, in general, this is\nreally-- if it was a table, it's at least easy\nto write down, like,",
    "start": "1633075",
    "end": "1638900"
  },
  {
    "text": "90 for all of those things. And that's what you initialize. In a deep neural network,\nit's really unclear,",
    "start": "1638900",
    "end": "1644269"
  },
  {
    "text": "how you initialize\nthose parameters, so that, for all the states\nthat you would reach, you would have even a good\nshot of it being optimistic.",
    "start": "1644270",
    "end": "1651450"
  },
  {
    "text": "So I think that's another\nchallenge here, is-- and that's a challenge for a\nlot of the optimism algorithms",
    "start": "1651450",
    "end": "1656780"
  },
  {
    "text": "we'll see, in general,\nis can we do it with function approximation.",
    "start": "1656780",
    "end": "1662030"
  },
  {
    "text": "Now, there's a lot of work on\nthinking about how to do things with function approximation. And we'll get into that soon.",
    "start": "1662030",
    "end": "1668228"
  },
  {
    "text": " So if you do carefully choose\nthe initialization value,",
    "start": "1668228",
    "end": "1674880"
  },
  {
    "text": "you can get good\nperformance under a new way of measuring what good\nperformance actually means.",
    "start": "1674880",
    "end": "1681380"
  },
  {
    "text": "OK. So let's go back to regret. So in regret, we just\ntry to think about",
    "start": "1681380",
    "end": "1688460"
  },
  {
    "text": "how do we quantify\nthe performance as we make lots of decisions. So T, here, is the number\nof decisions we make.",
    "start": "1688460",
    "end": "1695090"
  },
  {
    "text": "And we're just trying\nto think, in this case, about how many decisions\nwe make over time.",
    "start": "1695090",
    "end": "1700465"
  },
  {
    "text": "Let me see if the pen\nis finally charged. Not today.",
    "start": "1700465",
    "end": "1705490"
  },
  {
    "text": "So we could either\nbe making lots of little mistakes or\ninfrequent, large ones. And what you might\nimagine that we want to do",
    "start": "1705490",
    "end": "1716769"
  },
  {
    "text": "is to think about a\ndifferent form of loss. And so we're in, particular,\nanother form of performance.",
    "start": "1716770",
    "end": "1727820"
  },
  {
    "text": "That is going to be PAC. So let's draw what\nthat'll look like. ",
    "start": "1727820",
    "end": "1734530"
  },
  {
    "text": "So I think I drew this last\ntime, but I'll draw it again. So make this times step t.",
    "start": "1734530",
    "end": "1740860"
  },
  {
    "text": "And this is Q of at, Q of the\nactual arm that you pulled.",
    "start": "1740860",
    "end": "1747010"
  },
  {
    "text": "And this is q of a star,",
    "start": "1747010",
    "end": "1752512"
  },
  {
    "text": "So let's imagine that you\nhave an algorithm that is pulling arms, like the\nfollowing, all right, which",
    "start": "1752512",
    "end": "1764470"
  },
  {
    "text": "means that-- then maybe sometimes it's\npulling the right arm, hopefully. So in this case,\nsometimes the algorithm",
    "start": "1764470",
    "end": "1771789"
  },
  {
    "text": "is doing something that's\njust a little bit suboptimal, and sometimes it is making\na really big mistake.",
    "start": "1771790",
    "end": "1777970"
  },
  {
    "text": "So what we can do here\nis we can quantify how big our mistakes are.",
    "start": "1777970",
    "end": "1783580"
  },
  {
    "text": "And you might have\na situation where",
    "start": "1783580",
    "end": "1789010"
  },
  {
    "text": "you say optimal\nperformance is really hard. It's really hard to learn\nwhat everyone's perfect ad",
    "start": "1789010",
    "end": "1797000"
  },
  {
    "text": "preferences are or\nthings like that. Maybe I'm going to\nrelax my criteria. I'm not going to require\noptimal performance,",
    "start": "1797000",
    "end": "1803070"
  },
  {
    "text": "but I want pretty\ngood performance. I want epsilon optimal.",
    "start": "1803070",
    "end": "1808100"
  },
  {
    "text": "So what we do, in this\ncase, is we count every time",
    "start": "1808100",
    "end": "1815419"
  },
  {
    "text": "we make a bad decision. Meaning, something that is\nworse than epsilon optimal. And otherwise, we\nthink of all of those",
    "start": "1815420",
    "end": "1821410"
  },
  {
    "text": "as basically being in an\nequivalence class of optimal. ",
    "start": "1821410",
    "end": "1826460"
  },
  {
    "text": "So that's going to\nbe what we think about when we think about PAC. OK.",
    "start": "1826460",
    "end": "1831580"
  },
  {
    "text": "So I'll define what that is. So a PAC algorithm--\nand raise your hands if you've seen this\nin machine learning.",
    "start": "1831580",
    "end": "1837809"
  },
  {
    "text": "If you've taken\nmachine learning, you might have seen PAC. OK. Yeah, so at least one\nor two people have.",
    "start": "1837810",
    "end": "1843500"
  },
  {
    "text": "So, often, in machine\nlearning, particularly if it's a machine learning\nclass that includes some theory,",
    "start": "1843500",
    "end": "1848510"
  },
  {
    "text": "they'll talk about PAC and\nprobably approximately correct algorithms. And that's where\nthis idea comes from. So it came from the\nmachine learning community,",
    "start": "1848510",
    "end": "1855920"
  },
  {
    "text": "and then reinforcement\nlearning borrowed it. So the idea in a PAC algorithm\nis that on each time step,",
    "start": "1855920",
    "end": "1862190"
  },
  {
    "text": "a PAC algorithm is going to\nchoose an action whose value is epsilon optimal. Meaning, the value of\nthe action that's taken",
    "start": "1862190",
    "end": "1869060"
  },
  {
    "text": "is at least the value of the\noptimal action minus epsilon. So that means that\nwe're in this region,",
    "start": "1869060",
    "end": "1876269"
  },
  {
    "text": "with high probability on\nall but a polynomial number of time steps.",
    "start": "1876270",
    "end": "1882200"
  },
  {
    "text": "So essentially, it's\nsaying that the majority of the time, your algorithm\nis making good decisions.",
    "start": "1882200",
    "end": "1887309"
  },
  {
    "text": "Good, here, being defined\nas epsilon optimal. But sometimes we'll\nmake bad decisions. But we're going to say,\nwith high probability,",
    "start": "1887310",
    "end": "1893700"
  },
  {
    "text": "the total number\nof bad decisions we make is not too many. What we mean by \"not too\nmany\" here is something",
    "start": "1893700",
    "end": "1898730"
  },
  {
    "text": "that's polynomial, in\nyour problem parameters. So that generally means the\nnumber of actions you have,",
    "start": "1898730",
    "end": "1904680"
  },
  {
    "text": "epsilon, delta, et cetera. As you might expect,\nif epsilon is smaller,",
    "start": "1904680",
    "end": "1911730"
  },
  {
    "text": "generally, the number of\nsamples you need will go up. Normally, something like\n1 over epsilon squared.",
    "start": "1911730",
    "end": "1917049"
  },
  {
    "text": "So if you care about\nbeing more optimal, you're going to need more data.",
    "start": "1917050",
    "end": "1922583"
  },
  {
    "text": "Or, in other words,\nyour algorithm might make bad\ndecisions for longer. If delta is smaller,\nmeaning that you",
    "start": "1922583",
    "end": "1928980"
  },
  {
    "text": "want this to hold with\nhigher probability, you'll also need more data.",
    "start": "1928980",
    "end": "1934020"
  },
  {
    "text": "And if there are a lot of\nactions to learn about, in general, you need more data. So it gives us some notion of\nthe complexity of the problem",
    "start": "1934020",
    "end": "1942029"
  },
  {
    "text": "to learning. So this is a different type of-- a lot of algorithms, you can\nget both PAC guarantees for",
    "start": "1942030",
    "end": "1949230"
  },
  {
    "text": "and regret guarantees. But it is just a different\nnotion of optimality. ",
    "start": "1949230",
    "end": "1956092"
  },
  {
    "text": "Most of the PAC algorithms\nfor reinforcement learning are based on either\noptimism, like what we've seen from last lecture,\nor Thompson sampling, which",
    "start": "1956092",
    "end": "1963340"
  },
  {
    "text": "we're going to see\nlater in this lecture. And there do exist PAC\nalgorithms that just",
    "start": "1963340",
    "end": "1969790"
  },
  {
    "text": "initialize everything\nto a really high value. I don't know of any\npractical algorithms that do that, ones that\npeople use in practice.",
    "start": "1969790",
    "end": "1976990"
  },
  {
    "text": "But there is theory and\npapers about that case, so it is possible to do.",
    "start": "1976990",
    "end": "1983090"
  },
  {
    "text": "All right. And we'll see more\nstuff about PAC shortly. Let me just give an example. So remember back\nto our fake trying",
    "start": "1983090",
    "end": "1989770"
  },
  {
    "text": "to learn how to\ntreat broken toes example from last time, where\nwe had surgery and taping, buddy",
    "start": "1989770",
    "end": "1996160"
  },
  {
    "text": "taping the toes together. Again, this is not\nmedical advice. Imagine that this\nis-- epsilon is 0.05.",
    "start": "1996160",
    "end": "2004049"
  },
  {
    "text": "So in this case, before\nwe thought about this is what the optimal sequence\nof actions you should take--",
    "start": "2004050",
    "end": "2009820"
  },
  {
    "text": "but, of course, you don't\nknow that because you don't have data. If you had this sequence\nof actions, [INAUDIBLE]",
    "start": "2009820",
    "end": "2016090"
  },
  {
    "text": "and optimistic algorithm,\nthis would be the regret you would get in each case.",
    "start": "2016090",
    "end": "2022660"
  },
  {
    "text": "But under the PAC case-- let's see if I can\ntype this here.",
    "start": "2022660",
    "end": "2028080"
  },
  {
    "text": "Under the PAC case,\nthis would be epsilon.",
    "start": "2028080",
    "end": "2033210"
  },
  {
    "text": " So the important\nthing to notice here",
    "start": "2033210",
    "end": "2038470"
  },
  {
    "text": "is that, because\nthe reward of a2 is within the epsilon bound of\na1, which is the optimal action,",
    "start": "2038470",
    "end": "2045800"
  },
  {
    "text": "this action would also\nbe considered optimal. So from the perspective of\nthe PAC algorithm definition,",
    "start": "2045800",
    "end": "2052129"
  },
  {
    "text": "this would be not\ndenoted as a mistake. The only mistakes would be\nwhen the algorithm takes a3.",
    "start": "2052130",
    "end": "2059469"
  },
  {
    "text": "So when we talk about\nthis PAC definition here of counting up the\nnumber of time steps,",
    "start": "2059469",
    "end": "2066710"
  },
  {
    "text": "we don't make a\nreally good decision. The only decisions that would\ncount for that, in this setting,",
    "start": "2066710",
    "end": "2071739"
  },
  {
    "text": "is the a3 decisions. In contrast to that, when\nwe talk about regret, anything that's\nsuboptimal counts.",
    "start": "2071739",
    "end": "2078750"
  },
  {
    "text": "So you get penalized for\nall the a2 decisions. I thought we were\nallowed to make mistakes",
    "start": "2078750",
    "end": "2085090"
  },
  {
    "text": "for a polynomial\nnumber of steps. Yes, you are allowed,\nand it still will be PAC.",
    "start": "2085090",
    "end": "2091399"
  },
  {
    "text": "That's exactly right. But I'm just pointing out here\nthat the-- so the only actions you're taking that count towards\nthat polynomial is a3 here.",
    "start": "2091400",
    "end": "2098280"
  },
  {
    "text": "It's not a2. Whereas, a3 and a2 count\ntowards your regret. OK? Yeah,",
    "start": "2098280",
    "end": "2103630"
  },
  {
    "text": "So does screening become easier\nif I gradually reduce epsilon? Good question.",
    "start": "2103630",
    "end": "2109230"
  },
  {
    "text": "So normally, in these cases,\nyou fix epsilon in advance. And it defines the\nnumber of samples",
    "start": "2109230",
    "end": "2119300"
  },
  {
    "text": "you're going to need for each of\nthe actions or states in actions in the MDP case. So it's like an\nexploration term,",
    "start": "2119300",
    "end": "2125720"
  },
  {
    "text": "and you keep track of counts. There are algorithms-- with\nme and my former PhD student,",
    "start": "2125720",
    "end": "2132055"
  },
  {
    "text": "[MUTED], part of\nthe work that we did there was to talk about what\nif you want to have guarantees over many epsilons at once.",
    "start": "2132055",
    "end": "2137809"
  },
  {
    "text": "I'm thinking more\nalong the lines of epsilon greedy algorithms\nwe did in [INAUDIBLE] where,",
    "start": "2137810",
    "end": "2144200"
  },
  {
    "text": "because we gradually\nreduce epsilon [INAUDIBLE] to the [INAUDIBLE].",
    "start": "2144200",
    "end": "2149900"
  },
  {
    "text": "If that's the\nsame, they can be-- do the same thing there. Yeah, it's a little bit subtle. It's a great question.",
    "start": "2149900",
    "end": "2155070"
  },
  {
    "text": "So in general, the\nbounds will depend, something like 1\nover epsilon squared. So if your epsilon\nis going to 0,",
    "start": "2155070",
    "end": "2160453"
  },
  {
    "text": "that will say that you have\nto do an infinite amount of exploration. If you're interested,\nI have this.",
    "start": "2160453",
    "end": "2166160"
  },
  {
    "text": "I mean, one of our papers\nthinks about trying to have simultaneous bounds\nover lots of epsilons.",
    "start": "2166160",
    "end": "2171200"
  },
  {
    "text": "But in general, the\nbasic version of this, you commit to an\nepsilon in advance.",
    "start": "2171200",
    "end": "2176780"
  },
  {
    "text": "Great questions. All right. So going back to where we\nare and reminding ourselves,",
    "start": "2176780",
    "end": "2182869"
  },
  {
    "text": "in terms of algorithms-- this relates to\nyour epsilon greedy. Constant e-greedy, decaying\ne-greedy and optimistic",
    "start": "2182870",
    "end": "2188600"
  },
  {
    "text": "initialization all have\nthe problem, in general, of having sublinear-- of having bad performance.",
    "start": "2188600",
    "end": "2195180"
  },
  {
    "text": "It's, in theory, possible\nto have sublinear regret. But you often need to\nhave stronger knowledge",
    "start": "2195180",
    "end": "2201410"
  },
  {
    "text": "than is known. Optimistic initialization also\ncan have the PAC guarantees that we just talked about.",
    "start": "2201410",
    "end": "2206740"
  },
  {
    "text": " And I guess I'll just\nsay, too, you can",
    "start": "2206740",
    "end": "2212839"
  },
  {
    "text": "convert these results into-- so epsilon greedy is\nnot a PAC algorithm.",
    "start": "2212840",
    "end": "2218810"
  },
  {
    "text": "But you can think\nabout different types of other exploration strategies\nand whether or not they're PAC.",
    "start": "2218810",
    "end": "2224540"
  },
  {
    "text": "And we'll get back\ninto those soon. OK. Let's jump into\nBayesian bandits.",
    "start": "2224540",
    "end": "2230110"
  },
  {
    "text": "They're a pretty elegant idea. So, so far, we've made\nalmost no assumptions",
    "start": "2230110",
    "end": "2236579"
  },
  {
    "text": "about our reward distribution. So we've maybe said\nthey're bounded. It can be between 0 and 1.",
    "start": "2236580",
    "end": "2241750"
  },
  {
    "text": "And that's basically all\nwe needed for Hoeffding. We need them to be bounded. We needed them to be--",
    "start": "2241750",
    "end": "2247560"
  },
  {
    "text": "they're independent and\nidentically distributed. But we haven't made\nany other assumptions.",
    "start": "2247560",
    "end": "2253420"
  },
  {
    "text": "So we haven't said\nit's Gaussian, or it's a Bernoulli or\nsomething else we might know. And when we're being\nBayesian about this,",
    "start": "2253420",
    "end": "2260110"
  },
  {
    "text": "we're actually going\nto leverage knowledge we have about the structure\nof the way the rewards are generated. And what I mean by\nthat is, normally,",
    "start": "2260110",
    "end": "2266610"
  },
  {
    "text": "some particular statistical\nmodel, so it's a Gaussian model. Or it's a Bernoulli\nmodel, things like that.",
    "start": "2266610",
    "end": "2273570"
  },
  {
    "text": "And the reason that\nthat might be helpful is that, often, if\nwe're doing these in a domain like public\nhealth or others,",
    "start": "2273570",
    "end": "2279255"
  },
  {
    "text": "people might know lots\nof information about-- [SNEEZES] --what the reward structure is. Bless you.",
    "start": "2279255",
    "end": "2284580"
  },
  {
    "text": "And could we leverage that to\nget better algorithms and better performance? OK.",
    "start": "2284580",
    "end": "2290470"
  },
  {
    "text": "So before we do\nthis, it's probably helpful to do just a quick\nrefresher on Bayesian inference.",
    "start": "2290470",
    "end": "2295512"
  },
  {
    "text": "Some of you guys might\nhave done a lot of this. Some of you might\nhave done very little. We'll go through just a\nquick reminder about this",
    "start": "2295512",
    "end": "2304921"
  },
  {
    "text": "because this is going to\nbe used a lot for what we're going to see today. So the idea is that\nwe're going to start",
    "start": "2304921",
    "end": "2311890"
  },
  {
    "text": "with a prior over the\nunknown parameters. In our particular\ncase, that's going to be the unknown distribution\nover the rewards for each arm.",
    "start": "2311890",
    "end": "2320500"
  },
  {
    "text": "So it's like if we\nhave a coin flip. Or, if we think about\nthe toes example, what's the probability that\nsomeone's going to heel,",
    "start": "2320500",
    "end": "2327310"
  },
  {
    "text": "if they're given surgery? We don't know what that\nparameter, theta is. And so we're going to\nhave a prior over what",
    "start": "2327310",
    "end": "2333280"
  },
  {
    "text": "that theta could be. Once we're given\nsome observations about that parameter--\nfor example,",
    "start": "2333280",
    "end": "2340010"
  },
  {
    "text": "if we observe, when\nyou do surgery, that someone was healed, that is\ngoing to change your uncertainty",
    "start": "2340010",
    "end": "2345640"
  },
  {
    "text": "over the unknown parameters. ",
    "start": "2345640",
    "end": "2352147"
  },
  {
    "text": "So let's do a\nparticular example. So if the reward of arm I is\nthe probability distribution",
    "start": "2352147",
    "end": "2358259"
  },
  {
    "text": "depends on a\nparameter of phi i, we have initial prior\nover that parameter. Pull on arm, we\nobserve a reward.",
    "start": "2358260",
    "end": "2365590"
  },
  {
    "text": "Then we can use Bayes'-- that\nshould be \"Bayes'--\" Bayes' rule to update that. ",
    "start": "2365590",
    "end": "2372215"
  },
  {
    "text": "And I think it's really\nhelpful to visualize how the priors change over time,\nso we'll see that in an example",
    "start": "2372215",
    "end": "2377250"
  },
  {
    "text": "shortly, just so you can see\nwhat that might look like. So what we're going to\nhave here is Bayes' rule.",
    "start": "2377250",
    "end": "2387500"
  },
  {
    "start": "2387500",
    "end": "2401760"
  },
  {
    "text": "All right. This is our prior probability\nover the parameter governing the reward distribution\nfor this arm.",
    "start": "2401760",
    "end": "2409830"
  },
  {
    "text": "This is the likelihood\nof observing a particular reward given\na specific parameter value.",
    "start": "2409830",
    "end": "2415840"
  },
  {
    "text": "And this is the probability of\nseeing that reward in general. And when we do that,\nthis is Bayes' rule,",
    "start": "2415840",
    "end": "2421780"
  },
  {
    "text": "and then we use it to update\nwhat our new probability is over the parameter that\ngenerates that reward.",
    "start": "2421780",
    "end": "2427420"
  },
  {
    "text": "So in the case of\nsurgery, it would be before we had some\ndistribution over how successful",
    "start": "2427420",
    "end": "2433390"
  },
  {
    "text": "we think surgery is on average,\nwe give surgery to someone, we update it, we observe\nthat they are healed.",
    "start": "2433390",
    "end": "2439660"
  },
  {
    "text": "And then that changes what\nwe think about the underlying parameters. So, bring that out here.",
    "start": "2439660",
    "end": "2447900"
  },
  {
    "text": " This is the prior probability. This is the\nprobability of reward",
    "start": "2447900",
    "end": "2453510"
  },
  {
    "text": "given a particular parameter. This is the probability of\ngetting the reward in general. And we can rewrite this by\nusing the joint distribution",
    "start": "2453510",
    "end": "2461370"
  },
  {
    "text": "of the reward and the parameter\nand then marginalize out",
    "start": "2461370",
    "end": "2466440"
  },
  {
    "text": "the parameter.  All right. ",
    "start": "2466440",
    "end": "2474680"
  },
  {
    "text": "So this is beautiful. Oh, yeah? Can we go back to\nthe previous slide.",
    "start": "2474680",
    "end": "2480080"
  },
  {
    "text": "I'm just kind of\nconfused on the setup. If I imagine that phi as a\nprimer for a Bernoulli variable",
    "start": "2480080",
    "end": "2488690"
  },
  {
    "text": "and using background\nknowledge, I have some prior as\nto what it should be, what does it mean to have\na distribution over that?",
    "start": "2488690",
    "end": "2496520"
  },
  {
    "text": "Yeah, it's a great-- in general, it\nmay not be obvious",
    "start": "2496520",
    "end": "2501650"
  },
  {
    "text": "that we can compute this. So for example, we're\ngoing to see in some cases, this is analytic. You can analytically update\nthis, which is super elegant.",
    "start": "2501650",
    "end": "2508680"
  },
  {
    "text": "What I mean in that\ncase is as a simple-- so let's say-- we'll\nsee an example shortly.",
    "start": "2508680",
    "end": "2516510"
  },
  {
    "text": "But phi i could be as the\nprobability recovery--",
    "start": "2516510",
    "end": "2523400"
  },
  {
    "text": "I'll do this for surgery-- for surgery. So this would be, say, 90% of\nthe time, someone's recovered.",
    "start": "2523400",
    "end": "2531299"
  },
  {
    "text": "And let's say-- or\nsomething like that. And this could be\na particular prior. So I could say, I\nthink my probability",
    "start": "2531300",
    "end": "2540089"
  },
  {
    "text": "that your recovered mostly\nfrom the surgery is 0.9.",
    "start": "2540090",
    "end": "2545140"
  },
  {
    "text": "So I'm pretty confident\nthat the surgery is going to be highly\neffective on average. But I think that there's some\nprobability that the surgery is",
    "start": "2545140",
    "end": "2556500"
  },
  {
    "text": "not so effective. And then I would\nsay, well, I think that maybe it was\n10% probability",
    "start": "2556500",
    "end": "2561580"
  },
  {
    "text": "the surgery isn't as effective. But on average, people are\ngoing to recover at rate 0.4",
    "start": "2561580",
    "end": "2567600"
  },
  {
    "text": "with the surgery. And we'll see some\nspecific examples of this. This is not the priors\nwe're going to use,",
    "start": "2567600",
    "end": "2573055"
  },
  {
    "text": "but this just\nillustrates how you can have distributions over\ndistributions which can get confusing pretty quickly.",
    "start": "2573055",
    "end": "2578970"
  },
  {
    "text": "But on the other hand, it's\nalso super elegant and a place where we can put\nin prior knowledge,",
    "start": "2578970",
    "end": "2584000"
  },
  {
    "text": "just like clinicians\nand others may have information where they\ncan actually directly specify these priors.",
    "start": "2584000",
    "end": "2591910"
  },
  {
    "text": "All right. And so there's many questions\nyou might have in this case of, like, where do these\npriors come from?",
    "start": "2591910",
    "end": "2597690"
  },
  {
    "text": "And even if we\nhave these priors, how do we do this calculation? So in general, this\nis complicated.",
    "start": "2597690",
    "end": "2607050"
  },
  {
    "text": "So you can see here,\nyou've got to have a functional form for this. This, in our case, was\nlike flipping a coin.",
    "start": "2607050",
    "end": "2613170"
  },
  {
    "text": "And so if your coin has a bias\nof 0.9, what's the probability you'd get reward 1?",
    "start": "2613170",
    "end": "2618570"
  },
  {
    "text": "It would be 0.9. So you have to have a\nprobability distribution here, probability distribution here. You have to marginalize\none out over here.",
    "start": "2618570",
    "end": "2625080"
  },
  {
    "text": "And when you do all of that,\nyou get your new posterior, which is after you\nobserve something, now,",
    "start": "2625080",
    "end": "2631540"
  },
  {
    "text": "what is your new distribution? So you might imagine\nthat now I update this, maybe I see that the\nsurgery was successful.",
    "start": "2631540",
    "end": "2636690"
  },
  {
    "text": "And I'm, like, oh, maybe I can\nupdate this to be 0.95 and 0.05.",
    "start": "2636690",
    "end": "2642618"
  },
  {
    "text": "So in general,\nthis is going to be computationally\ntricky to do exactly without additional structure.",
    "start": "2642618",
    "end": "2649480"
  },
  {
    "text": "There's lots of ways\nto approximate it, but the really cool thing\nis that in some cases, you can do this analytically.",
    "start": "2649480",
    "end": "2656400"
  },
  {
    "text": "So this is idea of\nthese conjugate priors. So this is beautiful idea\nof the exponential families.",
    "start": "2656400",
    "end": "2664600"
  },
  {
    "text": "And if you have a\nrepresentation of your prior that is conjugate with-- this\nis often called your likelihood",
    "start": "2664600",
    "end": "2671100"
  },
  {
    "text": "function. Then, after you do\nall of this updating, this new thing is in the\nsame statistical family",
    "start": "2671100",
    "end": "2678210"
  },
  {
    "text": "as what this was in before. And we'll see some specific\nexamples of this in a second.",
    "start": "2678210",
    "end": "2684170"
  },
  {
    "text": "So the high level, really\nbeautiful idea in this case is that it's analytic. When you do all of this,\nlet's say this was initially",
    "start": "2684170",
    "end": "2691740"
  },
  {
    "text": "a Gaussian, this is still\ngoing to be a Gaussian if you use conjugate priors. ",
    "start": "2691740",
    "end": "2698604"
  },
  {
    "text": "So let's see how to\ndo this for Bernoulli. So for Bernoulli, there\nis a conjugate prior, which is really cool.",
    "start": "2698604",
    "end": "2704160"
  },
  {
    "text": "And the conjugate prior is\ncalled a beta distribution. And it's going to have a really\nnice, beautiful interpretation",
    "start": "2704160",
    "end": "2709650"
  },
  {
    "text": "that we'll see in just a second. So the equation looks terrible. The equation says\nthe probability",
    "start": "2709650",
    "end": "2715050"
  },
  {
    "text": "of a particular theta-- remember, this is the\nbias of your coin-- given some alpha and\nbeta-- these are just",
    "start": "2715050",
    "end": "2721860"
  },
  {
    "text": "two other parameters-- is\ntheta to the alpha minus 1, 1 minus theta of the\nbeta minus 1 times",
    "start": "2721860",
    "end": "2727260"
  },
  {
    "text": "the gamma function of alpha\nplus beta divided by gamma of alpha, gamma of beta.",
    "start": "2727260",
    "end": "2732420"
  },
  {
    "text": "So this looks fairly\nterrible, but it is conjugate, which means that\nafter we observe something,",
    "start": "2732420",
    "end": "2738550"
  },
  {
    "text": "our new posterior is also\ngoing to be the same. But it turns out that it has\na really simple explanation,",
    "start": "2738550",
    "end": "2745130"
  },
  {
    "text": "a really simple\nintuition, which is, imagine you start with your\nprior being a beta alpha beta.",
    "start": "2745130",
    "end": "2752200"
  },
  {
    "text": "And then you observe\na reward that's either 0 or 1 because your\nvariable is just 0 or 1.",
    "start": "2752200",
    "end": "2757369"
  },
  {
    "text": "It's a Bernoulli. Then your new beta,\nyour posterior,",
    "start": "2757370",
    "end": "2762580"
  },
  {
    "text": "is just r plus alpha,\ncomma, 1 minus r plus alpha.",
    "start": "2762580",
    "end": "2769860"
  },
  {
    "text": "What does this mean? If you observed a 1, then you\nincrement your first parameter.",
    "start": "2769860",
    "end": "2775130"
  },
  {
    "text": "It's like you increase\nthe number of successes. If you observe a 0, you increase\nthis number, the second number,",
    "start": "2775130",
    "end": "2781680"
  },
  {
    "text": "like you increase the\nnumber of failures. So you can think of what the\nbeta is doing is essentially just keeping track of how\nmany heads did you get,",
    "start": "2781680",
    "end": "2790570"
  },
  {
    "text": "how many tails, or how many ones\ndid you get, and how many zeros? It's just keeping\ntrack of those, and it can use those\nto explicitly update",
    "start": "2790570",
    "end": "2797820"
  },
  {
    "text": "what the probability\nis of your theta. So it's really beautiful\nbecause you don't--",
    "start": "2797820",
    "end": "2803940"
  },
  {
    "text": "computationally, that's\nreally easy to keep track of. You're just going to add one\ndepending on what you see. And what you can think\nof this here is being as,",
    "start": "2803940",
    "end": "2810810"
  },
  {
    "text": "how confident are you in advance\nof how many pseudocounts did you see of success versus failure?",
    "start": "2810810",
    "end": "2818309"
  },
  {
    "text": "So, for example, if I'm really\nconfident that the surgery is going to be successful,\nmaybe I'm like, yeah, I'm so confident.",
    "start": "2818310",
    "end": "2823788"
  },
  {
    "text": "It's as if I've seen 100\nsuccessful surgeries and only two failures.",
    "start": "2823788",
    "end": "2829347"
  },
  {
    "text": "But if I'm really uncertain,\nwhat I would do is I'd say, well, I'm going to treat like\none successful, one failure. I really don't know.",
    "start": "2829347",
    "end": "2835480"
  },
  {
    "text": "And we'll see what this\nlooks like in just a sec. ",
    "start": "2835480",
    "end": "2841930"
  },
  {
    "text": "Excuse me. So now when we have\nthis, this is basically giving us a distribution\nover the reward parameters,",
    "start": "2841930",
    "end": "2848170"
  },
  {
    "text": "and we can use this to\nactually make decisions. All right.",
    "start": "2848170",
    "end": "2853319"
  },
  {
    "text": "So there's a couple of\ndifferent ways to do this. And one of the\nways to do this is by getting a confidence\ninterval, similar to what",
    "start": "2853320",
    "end": "2860099"
  },
  {
    "text": "we've seen before. But the other thing is\ncalled probability matching or Thompson sampling. And let's go through Thompson\nsampling now and see an example.",
    "start": "2860100",
    "end": "2867755"
  },
  {
    "text": " All right. So in probability\nmatching, we're",
    "start": "2867755",
    "end": "2877250"
  },
  {
    "text": "going to assume we're in\nthe Bayesian bandit case. And what probability\nmatching does is says, OK, the way we might\nwant to explore",
    "start": "2877250",
    "end": "2883670"
  },
  {
    "text": "is by sampling actions according\nto the probability that they're optimal, given everything\nI've seen so far.",
    "start": "2883670",
    "end": "2890000"
  },
  {
    "text": "So what it says is given\nsome history, which is like the past\nthings I've tried",
    "start": "2890000",
    "end": "2895190"
  },
  {
    "text": "and whether I've gotten\nones or zeros for them, I want to select a new action\nbased on the probability",
    "start": "2895190",
    "end": "2901310"
  },
  {
    "text": "that its true mean is\nhigher than the mean of all the other arms. ",
    "start": "2901310",
    "end": "2908105"
  },
  {
    "text": "And I'm not going\nto tell you yet that that's formally a good\nthing to do in terms of regret, but you might imagine, that's\na reasonable thing to do,",
    "start": "2908105",
    "end": "2914240"
  },
  {
    "text": "sort of says, oh,\nwell, if I think that arm is likely to be\nthe optimal one with 60% probability, I'll try\nthat with 60% probability.",
    "start": "2914240",
    "end": "2920966"
  },
  {
    "text": "And then if I think\nthere's another arm that might be optimal, I'll try\nthat with 30% probability.",
    "start": "2920967",
    "end": "2927480"
  },
  {
    "text": "Now, in general, it's not clear\nhow you would compute this. It seems kind of an\ninteresting idea. It's not clear you\ncan compute it,",
    "start": "2927480",
    "end": "2933570"
  },
  {
    "text": "but it turns out there's\na really simple algorithm to compute this. So this is called\nThompson sampling.",
    "start": "2933570",
    "end": "2940660"
  },
  {
    "text": "And I think it was first\ninvented by Thompson maybe in 1919. Maybe 1919, maybe 1920.",
    "start": "2940660",
    "end": "2947569"
  },
  {
    "text": "Around 1919, Thompson sampling. So it was around forever. I mean, it's been around\nfor like 100 years.",
    "start": "2947570",
    "end": "2954250"
  },
  {
    "text": "But at least from the\nmachine learning perspective, I think it was forgotten about\nfor the first, I don't know,",
    "start": "2954250",
    "end": "2959684"
  },
  {
    "text": "90 of those? It really came back into\nprominence about 2010, 2011, when some people\ndiscovered that it actually",
    "start": "2959685",
    "end": "2966210"
  },
  {
    "text": "had some really nice empirical\nproperties, unlike Hoeffding, which has been used\nfor a long time.",
    "start": "2966210",
    "end": "2971980"
  },
  {
    "text": "How does Thompson sampling work? We're going to have a\nprior over each arm. Then for each iteration,\nwhat we're going to do",
    "start": "2971980",
    "end": "2978130"
  },
  {
    "text": "is we're going to sample\na reward distribution from the posterior. We'll see an example of\nexactly what I mean by that.",
    "start": "2978130",
    "end": "2983400"
  },
  {
    "text": "We compute the action value\nfunction, given that sample. We take the arm that is\nmaximum, given those Q's.",
    "start": "2983400",
    "end": "2991130"
  },
  {
    "text": "We observe a reward, and\nwe update our posterior. And then we're going to\ndo this many, many times. And again, this will all\nseem much more concrete",
    "start": "2991130",
    "end": "2997570"
  },
  {
    "text": "when we do an example. And it's going to turn out\nthat this exactly implements",
    "start": "2997570",
    "end": "3002700"
  },
  {
    "text": "probability matching. So let's come back\nto this in a second, and let's first do\na specific example,",
    "start": "3002700",
    "end": "3008110"
  },
  {
    "text": "because I think that will\nmake it a lot more concrete. All right. So let's go back to our\nbroken toes example.",
    "start": "3008110",
    "end": "3015630"
  },
  {
    "text": "What we're going to\ndo, so we're going to place a prior over\neach arms parameter, and I'm going to choose beta 1,\n1, What does a beta 1, 1 look",
    "start": "3015630",
    "end": "3024000"
  },
  {
    "text": "like? That looks like the following. I'm sorry, my pen\nisn't working today, otherwise that would\nhave been helpful.",
    "start": "3024000",
    "end": "3029440"
  },
  {
    "text": "But I'll draw it up here. ",
    "start": "3029440",
    "end": "3040230"
  },
  {
    "text": "This is 0, this is\n1, this is theta. We know that for a Bernoulli\nvariable, the value for a theta",
    "start": "3040230",
    "end": "3049180"
  },
  {
    "text": "has to be somewhere\nbetween 0 and 1 because you can either\nalways get 1 or always get",
    "start": "3049180",
    "end": "3054520"
  },
  {
    "text": "0 or somewhere in between. What a beta 1, 1, looks\nlike-- so this is going to be the probability of theta.",
    "start": "3054520",
    "end": "3060410"
  },
  {
    "text": "This is my prior. What a beta 1, 1 looks\nlike is this, which",
    "start": "3060410",
    "end": "3066490"
  },
  {
    "text": "is a uniform distribution. What it says is I have\nno idea what theta is.",
    "start": "3066490",
    "end": "3071960"
  },
  {
    "text": "It could be 0, it could be 1, it\ncould be 0.5, it could be 0.7, it could be 0.9. It just says someone\nis totally agnostic.",
    "start": "3071960",
    "end": "3079040"
  },
  {
    "text": "This is called often like an\nuninformative prior, saying, I have no idea\nwhat my probability",
    "start": "3079040",
    "end": "3084460"
  },
  {
    "text": "is for surgery, et cetera. But this is what\nthat looks like. ",
    "start": "3084460",
    "end": "3091534"
  },
  {
    "text": "So this is our prior. And now, what we're\ngoing to do is we're actually going to\nsample a Bernoulli parameter,",
    "start": "3091534",
    "end": "3098300"
  },
  {
    "text": "given the prior of each\narm for the three arms. So what does that mean?",
    "start": "3098300",
    "end": "3103819"
  },
  {
    "text": "That means I'm going to\nsample something for surgery. I'm going to sample\nsomething for buddy taping,",
    "start": "3103820",
    "end": "3112855"
  },
  {
    "text": "and I'm going to\nsample something for nothing, for do nothing. All of them have this\nparticular prior for now.",
    "start": "3112855",
    "end": "3120290"
  },
  {
    "text": "So for the first one,\nit's like I'm just sampling from a uniform\ndistribution between 0 and 1. So it could be anything\nbetween those 0 and 1.",
    "start": "3120290",
    "end": "3129860"
  },
  {
    "text": "Let me just check\nwhich number I'm-- should I use the numbers I'm\ngoing to use for the next ones?",
    "start": "3129860",
    "end": "3135400"
  },
  {
    "text": "So let's say, for example,\nthat I happen to sample 0.3.",
    "start": "3135400",
    "end": "3141589"
  },
  {
    "text": "That's a totally reasonable\nthing that I could sample, given this uniform\ndistribution between 0 and 1.",
    "start": "3141590",
    "end": "3147350"
  },
  {
    "text": "Then for buddy taping,\nlet's say I sample 0.5.",
    "start": "3147350",
    "end": "3153150"
  },
  {
    "text": "Again, a totally reasonable\nthing I could sample, given this distribution. And for do nothing, I'm\ngoing to sample 0.6.",
    "start": "3153150",
    "end": "3162780"
  },
  {
    "text": "So this is just\nthe distributions that I have over my prior\nover the parameters,",
    "start": "3162780",
    "end": "3168720"
  },
  {
    "text": "and this is a particular set\nof parameters I could sample. Given that, I now--",
    "start": "3168720",
    "end": "3178260"
  },
  {
    "text": "what Thompson sampling\nsays I should do is I should select the\naction that is maximal given",
    "start": "3178260",
    "end": "3184710"
  },
  {
    "text": "the parameters I've sampled. So under these three\nparameters, if you want to maximize the\nprobability that someone",
    "start": "3184710",
    "end": "3191130"
  },
  {
    "text": "will recover from surgery-- or sorry, recover from their-- recover in terms of\ntheir broken toe,",
    "start": "3191130",
    "end": "3196990"
  },
  {
    "text": "should I do surgery,\nbuddy taping, or nothing? Which one has the\nhighest chance?",
    "start": "3196990",
    "end": "3203593"
  },
  {
    "text": "In this case, nothing. In this case, nothing, right. So as soon as you--\nin our case, it's",
    "start": "3203593",
    "end": "3209040"
  },
  {
    "text": "pretty simple once\nyou see the theta, because the theta is exactly\nequal to the expected reward. So what this would\nsay is in this case,",
    "start": "3209040",
    "end": "3215148"
  },
  {
    "text": "you should do nothing.  So this is going to be--",
    "start": "3215148",
    "end": "3221030"
  },
  {
    "start": "3221030",
    "end": "3233270"
  },
  {
    "text": "So this will say do nothing. All right. We're going to observe\nthe patient's outcome.",
    "start": "3233270",
    "end": "3238490"
  },
  {
    "text": "Now, in this case, we're going\nto assume that doing nothing is actually not so effective.",
    "start": "3238490",
    "end": "3243599"
  },
  {
    "text": "And so we're going\nto observe a zero. And now, what we're\ngoing to do is we're going to update the\nposterior over doing nothing,",
    "start": "3243600",
    "end": "3250620"
  },
  {
    "text": "given that observation. Now, the other two haven't--\nthe other two arms,",
    "start": "3250620",
    "end": "3257060"
  },
  {
    "text": "their prior hasn't changed\nbecause we haven't gotten any observations about\nsurgery or buddy taping.",
    "start": "3257060",
    "end": "3262475"
  },
  {
    "text": "The only thing we've\ngot an observation about is doing nothing. So what I said before--",
    "start": "3262475",
    "end": "3269150"
  },
  {
    "text": "so we have alpha. This is our alpha\nbeta parameter. So this is our prior. In particular, it\nwas beta 1, 1 before.",
    "start": "3269150",
    "end": "3277400"
  },
  {
    "text": "And when I pull\nthis arm and I get a reward of 0, what I said we\nwould do here is the first one",
    "start": "3277400",
    "end": "3285980"
  },
  {
    "text": "you can think of as being\nthe number of successes, and the second one is\nthe number of failures. So what this becomes is\nit becomes beta 1, 2.",
    "start": "3285980",
    "end": "3298161"
  },
  {
    "text": "And that is going\nto look different. ",
    "start": "3298161",
    "end": "3305589"
  },
  {
    "text": "It's going to look like this. This is a beta 1, 2.",
    "start": "3305590",
    "end": "3314880"
  },
  {
    "text": "And this is a beta 1, 1. ",
    "start": "3314880",
    "end": "3321544"
  },
  {
    "text": "Does somebody want to\ntell me intuitively why it makes sense\nthat this looks like this for doing nothing?",
    "start": "3321544",
    "end": "3327510"
  },
  {
    "text": "Does this put-- where\ndoes this put weight in terms of parameters? ",
    "start": "3327510",
    "end": "3333599"
  },
  {
    "text": "Since we received a 0, we should\nthink it's more likely at 0. Yeah, we should think of the\nparameter value is likely lower.",
    "start": "3333600",
    "end": "3341170"
  },
  {
    "text": "And so we've shifted\nour probability mass. And so we're, like, OK, for\nthe things that we don't know, we're still totally agnostic\nabout whether they're",
    "start": "3341170",
    "end": "3348120"
  },
  {
    "text": "effective or not. For the thing that we just\ntried, do nothing, we got a 0. So it is more likely that\nour actual theta is lower",
    "start": "3348120",
    "end": "3356670"
  },
  {
    "text": "because a lower theta in\ngeneral, will generate more 0's. And so we've changed\nour distribution.",
    "start": "3356670",
    "end": "3362860"
  },
  {
    "text": " Let's see what that\nlooks like here.",
    "start": "3362860",
    "end": "3368520"
  },
  {
    "text": "So this is our new posterior. We're using again,\nremember, this is conjugate. So this is our new theta,\nand we haven't changed it",
    "start": "3368520",
    "end": "3375810"
  },
  {
    "text": "for the other two. Now, here's the next\nimportant thing. What we're going to\ndo now is we are--",
    "start": "3375810",
    "end": "3382710"
  },
  {
    "text": "so this is what that beta\nlooks like, just for beta 1, 2. Now, we're going to do our\nnext step at Thompson sampling.",
    "start": "3382710",
    "end": "3388950"
  },
  {
    "text": "So what we have to do now\nis we now have to resample.",
    "start": "3388950",
    "end": "3395640"
  },
  {
    "text": "So we are going to resample\nwhere we have two distributions. These two ones are a beta 1, 1.",
    "start": "3395640",
    "end": "3402630"
  },
  {
    "text": "And this one is a beta 1, 2. We're going to throw away\nall of our old parameters",
    "start": "3402630",
    "end": "3408840"
  },
  {
    "text": "from last time. They were just samples. We now have an updated\ndistribution for one",
    "start": "3408840",
    "end": "3414150"
  },
  {
    "text": "of the arms, and we have the\nold one for the other two. So in this case, we might\nresample, and we would get this.",
    "start": "3414150",
    "end": "3421610"
  },
  {
    "text": " It's more likely now that\nwe would sample a theta",
    "start": "3421610",
    "end": "3428140"
  },
  {
    "text": "3, which is lower\nbecause our beta puts more weight on the lower part.",
    "start": "3428140",
    "end": "3434530"
  },
  {
    "text": "So this is what this\nlooks like here. So under this, we're going to\npull arm one because it has",
    "start": "3434530",
    "end": "3439840"
  },
  {
    "text": "the highest expected success.  And that one is going\nto give us a beta 2, 1,",
    "start": "3439840",
    "end": "3447329"
  },
  {
    "text": "because remember, again, that we\nincrement the first one in terms of the number of successes\nand number of failures.",
    "start": "3447330",
    "end": "3453700"
  },
  {
    "text": "So as you might expect, it's\nexactly symmetric to this one. ",
    "start": "3453700",
    "end": "3459720"
  },
  {
    "text": "And we have something\nthat looks like this. I'm not being perfectly\nprecise of the intersection.",
    "start": "3459720",
    "end": "3465520"
  },
  {
    "start": "3465520",
    "end": "3475330"
  },
  {
    "text": "So now, again, we throw\naway all the old parameters that we've sampled so far.",
    "start": "3475330",
    "end": "3480359"
  },
  {
    "text": "So we're going to throw away\nthe 0.7, the 0.5, and the 0.3. And we're going to resample. ",
    "start": "3480360",
    "end": "3488280"
  },
  {
    "text": "So this time, let's imagine\nwe sample 0.71, 0.65, and 0.1. And we again observe that\nthe outcome from surgery",
    "start": "3488280",
    "end": "3495720"
  },
  {
    "text": "is successful. This is what a beta\n3, 1 looks like. So it stops looking\nlike a straight line.",
    "start": "3495720",
    "end": "3501610"
  },
  {
    "text": "It starts having a curve. So I really like these graphs\nbecause I feel like it gives one",
    "start": "3501610",
    "end": "3506790"
  },
  {
    "text": "a much better intuitive sense of\nhow as you get information that translates to your\nposterior over what",
    "start": "3506790",
    "end": "3514770"
  },
  {
    "text": "you think the theta likely is. So as you see more\nsuccesses, it will",
    "start": "3514770",
    "end": "3520317"
  },
  {
    "text": "tend to go weighted to one way. As you see more\nfailures, they're weighted to the other way. And as you might expect-- so we're not seeing\nthat right here.",
    "start": "3520318",
    "end": "3526420"
  },
  {
    "text": "But if you can\nsee cases where it starts to concentrate in the\nmiddle or somewhere in between,",
    "start": "3526420",
    "end": "3534109"
  },
  {
    "text": "just depends on what actual\nobservations you're getting. ",
    "start": "3534110",
    "end": "3541282"
  },
  {
    "text": "So this is how Thompson\nsampling works. And I think-- so\nlet's say we did this, then now we have this something\nthat's even more peaked.",
    "start": "3541282",
    "end": "3547730"
  },
  {
    "text": "We get another one, and you can\nsee it just continues to curve. OK, yeah? Could you use Thompson sampling\nwith random variables that just",
    "start": "3547730",
    "end": "3557740"
  },
  {
    "text": "have many more parameters\nas opposed to just using Bernoulli's? Yes, absolutely.",
    "start": "3557740",
    "end": "3563809"
  },
  {
    "text": "Yeah. And one of the examples\nwe'll see later today, they're using it\nfor advertising, and they have a large\nnumber of features.",
    "start": "3563810",
    "end": "3570090"
  },
  {
    "text": "Yeah. You can extend all of these to\nthe function approximation case. Good question.",
    "start": "3570090",
    "end": "3576300"
  },
  {
    "text": "So I think one of the things-- I mean, obviously this\nis a small example. What we saw in this\nparticular example I just did is that\nwe quickly started",
    "start": "3576300",
    "end": "3585000"
  },
  {
    "text": "to converge to a1 in this case. Now, notice so far, we've\nactually never pulled a2.",
    "start": "3585000",
    "end": "3591750"
  },
  {
    "text": "We had some probability\nof pulling a2 because if we had sampled\na really high value for a2,",
    "start": "3591750",
    "end": "3598140"
  },
  {
    "text": "then we would have pulled it. But we haven't done that yet. And a1, which actually does\nhave generally, a higher",
    "start": "3598140",
    "end": "3607050"
  },
  {
    "text": "probability in this case\nof having good outcomes, is starting to be pulled. So it's quite different\nthan the optimism methods",
    "start": "3607050",
    "end": "3613650"
  },
  {
    "text": "because in optimism, we had to\nat least pull each arm once, so we could even start to\ninitialize our confidence bounds.",
    "start": "3613650",
    "end": "3619170"
  },
  {
    "text": "That's not the case here. We already have a prior\nover what the values are, all of these, and we\ncan immediately start",
    "start": "3619170",
    "end": "3624380"
  },
  {
    "text": "using that to make decisions. ",
    "start": "3624380",
    "end": "3629960"
  },
  {
    "text": "All right. So what is Thompson\nsampling doing when we're doing these\npolls, and what results",
    "start": "3629960",
    "end": "3636430"
  },
  {
    "text": "do we have in this case?  So what it is doing in this\ncase is it's-- well, actually,",
    "start": "3636430",
    "end": "3645680"
  },
  {
    "text": "let me just step back because\nI wanted to get to the example. So I went through that\npart a little bit fast.",
    "start": "3645680",
    "end": "3650930"
  },
  {
    "text": "Let me just go to how\nthe matching is working. So let's just go back\nto here for a second.",
    "start": "3650930",
    "end": "3656680"
  },
  {
    "text": "So what we can see in this case\nis what Thompson sampling is actually doing, is\nthat each time point,",
    "start": "3656680",
    "end": "3662740"
  },
  {
    "text": "it is trying to select actions\naccording to this probability. ",
    "start": "3662740",
    "end": "3668890"
  },
  {
    "text": "And it'll often end\nup being optimistic in the face of\nuncertainty because we're doing an argmax with respect\nto our empirical estimates.",
    "start": "3668890",
    "end": "3679510"
  },
  {
    "text": "But it won't-- in general,\nas you might imagine, uncertain actions have a higher\nprobability of being the max.",
    "start": "3679510",
    "end": "3685420"
  },
  {
    "text": "So if you are really sure\nthat your parameter is at 0.5 and you have another parameter\nyou have very little information",
    "start": "3685420",
    "end": "3692590"
  },
  {
    "text": "about, you have a\nbeta 1, 1, then you're more likely to accidentally\nsample a much higher value",
    "start": "3692590",
    "end": "3697780"
  },
  {
    "text": "for that parameter. So the elegant thing\nhere is that you can",
    "start": "3697780",
    "end": "3707178"
  },
  {
    "text": "think of this as the following. This is really useful\nalso for the theory. So in posterior matching,\nthat's this first line.",
    "start": "3707178",
    "end": "3714890"
  },
  {
    "text": "That's sampling things\naccording to this. What Thompson sampling does is\nit doesn't do that explicitly.",
    "start": "3714890",
    "end": "3721790"
  },
  {
    "text": "It just samples\na reward for each of the-- like a reward parameter\nfor each of the different arms.",
    "start": "3721790",
    "end": "3727559"
  },
  {
    "text": "And then it picks the\none that's argmax. And so it's really\nelegant that that",
    "start": "3727560",
    "end": "3733280"
  },
  {
    "text": "is in fact, the same thing as\ndoing probability matching. ",
    "start": "3733280",
    "end": "3739559"
  },
  {
    "text": "That gives us the\nfact that that ends up working in terms of these. So the key idea in this case is\nthat as you're computing these",
    "start": "3739560",
    "end": "3749099"
  },
  {
    "text": "with respect to the data\nthat you have so far, in fact, the probability that\nThompson sampling picks an arm",
    "start": "3749100",
    "end": "3756990"
  },
  {
    "text": "is exactly equal to\nthis true probability, given all the data\nyou've seen so far.",
    "start": "3756990",
    "end": "3762930"
  },
  {
    "text": "I'll do a pointer. If we have time at\nthe end, maybe I'll to go through the proof briefly\nfor the Bayesian regret case.",
    "start": "3762930",
    "end": "3769260"
  },
  {
    "text": "But there's also a really nice\nexplanation of this inside of Tor Lattimore and\nCsaba Szepesv'ari's book.",
    "start": "3769260",
    "end": "3774607"
  },
  {
    "text": "There's quite a lot there. This is a quite\nmathematical version of it, but it gives you some\nreally nice background.",
    "start": "3774607",
    "end": "3781878"
  },
  {
    "text": "Let's go back to here. ",
    "start": "3781878",
    "end": "3788250"
  },
  {
    "text": "Let's first just talk about,\nhow do we evaluate performance? So what we saw in\nfrequentist regret, like what we saw last\ntime, is that we're",
    "start": "3788250",
    "end": "3795210"
  },
  {
    "text": "assuming a particular\nunknown set of parameters. Our arms are actually\n0.9, 0.7, 0.6.",
    "start": "3795210",
    "end": "3802570"
  },
  {
    "text": "We just don't know\nwhat they are. And then our regret is\nalways evaluated with respect",
    "start": "3802570",
    "end": "3807720"
  },
  {
    "text": "to the optimal arm, given\nthat fixed set of parameters. Bayesian regret assumes there's\nthis prior over the parameters.",
    "start": "3807720",
    "end": "3815860"
  },
  {
    "text": "And so when we\ntalk about regret, we're actually taking an\nexpectation with respect to that prior.",
    "start": "3815860",
    "end": "3821910"
  },
  {
    "text": "So it's still-- this looks\nlike exactly the same as the frequentist\nregret, but now we have this outer\nexpectation over theta.",
    "start": "3821910",
    "end": "3828155"
  },
  {
    "text": " One of the key ideas\nof this, in terms",
    "start": "3828155",
    "end": "3835320"
  },
  {
    "text": "of how one might prove\nthings in this case, is if we think back to how we\nproved some ideas around regret,",
    "start": "3835320",
    "end": "3841543"
  },
  {
    "text": "we didn't do the full proof. I just try to give\nsome sketches. One of the key ideas in the\nproof for frequentist regret",
    "start": "3841543",
    "end": "3847770"
  },
  {
    "text": "in upper confidence\nbounds is that we try to construct these\nupper confidence bounds UT",
    "start": "3847770",
    "end": "3853799"
  },
  {
    "text": "that we thought would be higher\nthan the true value of the arm with high probability.",
    "start": "3853800",
    "end": "3859990"
  },
  {
    "text": "And we use that in order to\nfigure out how many times we would pull suboptimal arms. We leverage this fact.",
    "start": "3859990",
    "end": "3867670"
  },
  {
    "text": "So it turns out that you\ncan do Bayesian regret bounds under a pretty\nsimilar decomposition.",
    "start": "3867670",
    "end": "3873350"
  },
  {
    "text": "You can think about computing\nan upper confidence bound and the likelihood\nthat it'll hold.",
    "start": "3873350",
    "end": "3878595"
  },
  {
    "text": " We might come back\nto that later today, but I want to first get into\nextending these up to higher",
    "start": "3878595",
    "end": "3887270"
  },
  {
    "text": "level settings as well. Before we do that, I\njust want to highlight that if you try to\nget standard bounds,",
    "start": "3887270",
    "end": "3894230"
  },
  {
    "text": "like what we saw last time, for\nstandard Thompson sampling-- and what I mean by that is\nthe type of Thompson sampling",
    "start": "3894230",
    "end": "3899870"
  },
  {
    "text": "I just showed you. To my last check,\nthey don't actually match the best bound for upper\nconfidence bound and frequentist",
    "start": "3899870",
    "end": "3905839"
  },
  {
    "text": "algorithms. However, often\nempirically, they can be really effective algorithms.",
    "start": "3905840",
    "end": "3911328"
  },
  {
    "text": "And I'll just highlight\nhere that in general, you can't compare directly\nbetween Bayesian regret bounds and frequentist\nbecause one of them",
    "start": "3911328",
    "end": "3918050"
  },
  {
    "text": "is with respect to this\nprior over parameters. So let's look at that\nfor a particular domain",
    "start": "3918050",
    "end": "3925760"
  },
  {
    "text": "and why Thompson sampling\nmight be particularly helpful for a lot of real world cases.",
    "start": "3925760",
    "end": "3930920"
  },
  {
    "text": "So this is a really nice paper\nby Olivier Chapelle and Lihong Li, which sort of re-initiated\na huge amount of interest",
    "start": "3930920",
    "end": "3939050"
  },
  {
    "text": "in Thompson sampling a\nlittle over a decade ago. So I think they were both\nat Yahoo at the time,",
    "start": "3939050",
    "end": "3944130"
  },
  {
    "text": "if I remember, right? They were thinking about\na contextual bandit case, so they were thinking\nabout making news article",
    "start": "3944130",
    "end": "3950280"
  },
  {
    "text": "recommendations, et cetera. And so there you\nwould have a context, like you'd have a bunch of\nfeatures about an individual.",
    "start": "3950280",
    "end": "3956820"
  },
  {
    "text": "And also, often you would\nhave a bunch of features about the arms, so explaining\nmaybe news articles, and all the features, or\nads and stuff like that.",
    "start": "3956820",
    "end": "3965973"
  },
  {
    "text": "But we're still going to see\nin the context of sampled iid at each step. So if I give a particular\nad at this time point,",
    "start": "3965973",
    "end": "3973800"
  },
  {
    "text": "it doesn't impact what's\ngoing to happen to [MUTED] So it's still a bandit. There's no sequential\ndependencies there.",
    "start": "3973800",
    "end": "3980640"
  },
  {
    "text": "Arms are articles. Reward is binary. Either you click\non it or you don't.",
    "start": "3980640",
    "end": "3987090"
  },
  {
    "text": "In this case, you can model\nit using logistic regression because you have\nthis binary output.",
    "start": "3987090",
    "end": "3992550"
  },
  {
    "text": "So what are we seeing here? So this is CTR, which means\nit's a clickthrough rate. It's normalized\nbecause they're not going to tell us exactly what\nthey get on their real world",
    "start": "3992550",
    "end": "3999647"
  },
  {
    "text": "data. The important thing\nto look at here is the x-axis, which is delay. So in many cases, just like\nwhat we saw for a public health",
    "start": "3999647",
    "end": "4007553"
  },
  {
    "text": "setting, there will be\nsome form of delay even for online customer cases. So Amazon will\nshow you something,",
    "start": "4007553",
    "end": "4013190"
  },
  {
    "text": "and they don't find\nout for a little bit of whether or not you're\nclicking on it or whether you bought the thing. And so what they're comparing\ntheir algorithms with here",
    "start": "4013190",
    "end": "4020720"
  },
  {
    "text": "is the following. So TS is Thompson sampling. OTS is optimistic\nThompson sampling.",
    "start": "4020720",
    "end": "4026840"
  },
  {
    "text": "You can try to add in a little\nbit of optimism in these. UCB is upper confidence bound.",
    "start": "4026840",
    "end": "4032630"
  },
  {
    "text": "EG I think, is epsilon greedy. And exploit is you\njust do whatever the mean looks like so far.",
    "start": "4032630",
    "end": "4038970"
  },
  {
    "text": "These are all hyperparameters. As often is the case, the\nhyperparameters matter,",
    "start": "4038970",
    "end": "4044350"
  },
  {
    "text": "so it's useful to look at these. I think the really\ninteresting thing to look at in this case is\nto look across the time.",
    "start": "4044350",
    "end": "4051280"
  },
  {
    "text": "So if you-- this is the\nshortest delay, and this",
    "start": "4051280",
    "end": "4056520"
  },
  {
    "text": "is the longest delay. And you can see for\nthe blue algorithm, it varies very little in\nterms of its performance,",
    "start": "4056520",
    "end": "4063490"
  },
  {
    "text": "even if things\nare delayed a lot. But if you look at, say,\nUCB, its performance",
    "start": "4063490",
    "end": "4072180"
  },
  {
    "text": "tends to drop a lot in terms\nof as you have longer delay.",
    "start": "4072180",
    "end": "4077410"
  },
  {
    "text": "And so that is\none of the reasons why you might want\nto do Thompson sampling in these cases.",
    "start": "4077410",
    "end": "4084030"
  },
  {
    "text": "So let's think more\nabout that and do a check our understanding. So let's think about an online\nnews website with lots of people",
    "start": "4084030",
    "end": "4090540"
  },
  {
    "text": "logging in every second. Often, someone will come\nonline before you've seen the outcome of\nthe previous person.",
    "start": "4090540",
    "end": "4095994"
  },
  {
    "text": "It asks you to select\nall of the things that you think are true as we\nthink about Thompson sampling versus upper confidence bounds.",
    "start": "4095995",
    "end": "4101599"
  },
  {
    "start": "4101599",
    "end": "4180490"
  },
  {
    "text": "All right. Why don't you compare your\nanswer to someone nearby? ",
    "start": "4180490",
    "end": "4250070"
  },
  {
    "text": "So let's come back together. So as we were just discussing,\nwe pointed out that Thompson",
    "start": "4250070",
    "end": "4256610"
  },
  {
    "text": "sampling could cause much worse\nperformance-- this one is true-- than optimism if the\nprior is very misleading.",
    "start": "4256610",
    "end": "4263550"
  },
  {
    "text": "So this is true. ",
    "start": "4263550",
    "end": "4283469"
  },
  {
    "text": "Because if for example, maybe\nsurgery is really effective and someone starts off and\nthinks surgery isn't effective",
    "start": "4283470",
    "end": "4289980"
  },
  {
    "text": "at all, and so you put a\nlot of probability mass, you could have a really\nsharp prior on it over here,",
    "start": "4289980",
    "end": "4295619"
  },
  {
    "text": "then it could take a\nlong time essentially for your data to\noverwhelm your prior. So this one can be a problem.",
    "start": "4295620",
    "end": "4302220"
  },
  {
    "text": "The first one is also true. So if you think back\nto the algorithms",
    "start": "4302220",
    "end": "4307620"
  },
  {
    "text": "that we saw last\ntime for optimism, there is no randomness in\nthere, unless you have a tie.",
    "start": "4307620",
    "end": "4314050"
  },
  {
    "text": "So if your upper confidence\nbound for arm one is higher than the\nupper confidence bound for arm two and\narm three, you're",
    "start": "4314050",
    "end": "4320310"
  },
  {
    "text": "just going to take arm one. And that's fine, but\nif you have a delay, that means you can't update\nthose upper confidence bounds.",
    "start": "4320310",
    "end": "4327400"
  },
  {
    "text": "So if the next customer\ncomes, you're, like, oh. Or the next patient comes,\nand you're, like, I still think surgery is best. I still think surgery is best.",
    "start": "4327400",
    "end": "4333360"
  },
  {
    "text": "And you're not going to\ntry anything different. Whereas Thompson sampling just\nhas this prior or posterior.",
    "start": "4333360",
    "end": "4340630"
  },
  {
    "text": "And so if I have\nsomeone come, I can just sample from all of my priors.",
    "start": "4340630",
    "end": "4346480"
  },
  {
    "text": "And then if another\nperson comes, I'll again sample\nfrom my priors. And so because of\nthis distribution",
    "start": "4346480",
    "end": "4353139"
  },
  {
    "text": "over parameters,\nunless it's collapsed to a delta function,\nin which case you know what the right\nthing is to do anyway,",
    "start": "4353140",
    "end": "4358820"
  },
  {
    "text": "you'll get natural exploration. So that's one of the really big\nbenefits of Thompson sampling, is that even if you\ndon't get new data,",
    "start": "4358820",
    "end": "4365450"
  },
  {
    "text": "you naturally will try\nout different things, and that can be really helpful. It is true that optimism\nalgorithms generally",
    "start": "4365450",
    "end": "4372510"
  },
  {
    "text": "are better than Thompson\nsampling in terms of their regret bounds. That may or may not translate\nto empirical benefits.",
    "start": "4372510",
    "end": "4378460"
  },
  {
    "text": "But they don't\nactually necessarily have strong regret\nbounds for this setting. So this is false.",
    "start": "4378460",
    "end": "4384020"
  },
  {
    "text": "And that's because\nall the bounds we've been talking\nabout so far, don't think about that batch setting.",
    "start": "4384020",
    "end": "4390058"
  },
  {
    "text": "They're being derived for the\ncase where you get information, you update your confidence\nbounds, you continue.",
    "start": "4390058",
    "end": "4396940"
  },
  {
    "text": "So this highlights some\nof the particular benefits and the potential weaknesses\nof Thompson sampling.",
    "start": "4396940",
    "end": "4402920"
  },
  {
    "text": "If your prior is reasonable and\nyou've got this delay or batch setting, it can be very helpful.",
    "start": "4402920",
    "end": "4408940"
  },
  {
    "text": "If your prior is really\nbad, it can take a long time to get past that. ",
    "start": "4408940",
    "end": "4417960"
  },
  {
    "text": "So before we end today, I\nthink an interesting question to consider is whether or not\nThompson sampling is optimal.",
    "start": "4417960",
    "end": "4426630"
  },
  {
    "text": "Now, we can get nice regret\nbounds for this case. I know I didn't have a chance to\ngo through that particular proof today, but it's not\noptimal in general.",
    "start": "4426630",
    "end": "4435280"
  },
  {
    "text": "So it would be really cool\nif we could get something that was basically perfect. You might imagine that\nif you have a prior",
    "start": "4435280",
    "end": "4442230"
  },
  {
    "text": "and you have a known horizon\nthat you could compute a decision policy that would\nmaximize your expected rewards,",
    "start": "4442230",
    "end": "4447570"
  },
  {
    "text": "given that prior\nand the horizon. So I haven't, at\nleast in this class, taught you all the tools\nyou need to do that.",
    "start": "4447570",
    "end": "4453760"
  },
  {
    "text": "But at a high level,\nyou could think of it as a Markov decision\nprocess over parameters,",
    "start": "4453760",
    "end": "4458889"
  },
  {
    "text": "which is kind of wild. So if any of you guys have taken\nMykel Kochenderfer's class-- actually, who's taken\nMykel's class, anybody here?",
    "start": "4458890",
    "end": "4465360"
  },
  {
    "text": "So you can think\nof like a POMDP. Your state is your parameters,\nyour actions are pulling things,",
    "start": "4465360",
    "end": "4470790"
  },
  {
    "text": "and then your belief state\nis your new probability of your parameters. So it's really elegant.",
    "start": "4470790",
    "end": "4476460"
  },
  {
    "text": "In theory, you can\ncompute something that will exactly maximize\nyour expected reward",
    "start": "4476460",
    "end": "4481630"
  },
  {
    "text": "by doing POMDP planning. The problem, also for those of\nyou who've taken Mykel's class, is that often, POMDP planning\nis really intractable.",
    "start": "4481630",
    "end": "4488530"
  },
  {
    "text": "So it's often not\nclear that we could do this in a computationally\nreasonable way.",
    "start": "4488530",
    "end": "4493750"
  },
  {
    "text": "In general, one\nof the challenges here is that if you\nwanted to do this, it would have a\ndecision policy that's",
    "start": "4493750",
    "end": "4500050"
  },
  {
    "text": "a function of the\nhistory, which means all the prior\nactions you've taken and all of the rewards\nyou've observed.",
    "start": "4500050",
    "end": "4505889"
  },
  {
    "text": "And that's going to\nincrease exponentially with the number of\ndecisions you made. ",
    "start": "4505890",
    "end": "4513010"
  },
  {
    "text": "So there's this idea\nof an index policy. And an index policy\nsays we don't want to have to think about this\nexponential history or state.",
    "start": "4513010",
    "end": "4522190"
  },
  {
    "text": "An index policy is, one,\na decision policy that computes a real valued\nindex for each arm, and it plays the arm\nwith the highest index,",
    "start": "4522190",
    "end": "4528760"
  },
  {
    "text": "using statistics only from\nthat arm and the horizon. So that means I don't\nhave to pay attention",
    "start": "4528760",
    "end": "4534310"
  },
  {
    "text": "to this combinatorial\nexponential thing. I can just say for\nthis particular arm,",
    "start": "4534310",
    "end": "4540380"
  },
  {
    "text": "maybe what were my rewards\nthat I've observed so far, and then I can use that\ninformation to make decisions.",
    "start": "4540380",
    "end": "4546200"
  },
  {
    "text": "So, for example,\na greedy algorithm which just relies on\nyour empirical average of the performance for each\narm, is an index policy.",
    "start": "4546200",
    "end": "4554200"
  },
  {
    "text": "So it's an upper\nconfidence bound algorithm because it just relies on\nthe upper confidence bound for the rewards you've\nseen for each arm.",
    "start": "4554200",
    "end": "4560060"
  },
  {
    "text": "So there are a lot\nof index policies. Surprisingly, there is an\nindex policy that is optimal.",
    "start": "4560060",
    "end": "4567710"
  },
  {
    "text": "So Gittins proved that there\nexists an optimal policy for maximizing the expected\ndiscounted reward in a Bayesian",
    "start": "4567710",
    "end": "4574340"
  },
  {
    "text": "multi-armed bandit,\nthat you can compute, that only depends\non these statistics separately for each arms.",
    "start": "4574340",
    "end": "4580580"
  },
  {
    "text": "So that's really cool. It means that it is\npossible in some settings to actually exactly optimize\nyour expected sum of discounted",
    "start": "4580580",
    "end": "4587960"
  },
  {
    "text": "rewards for these type\nof Bayesian bandits. ",
    "start": "4587960",
    "end": "4593389"
  },
  {
    "text": "Thompson sampling will\nnot do this in general. So Thompson sampling\nis generally not equal to what the\nGittins index would be,",
    "start": "4593390",
    "end": "4599360"
  },
  {
    "text": "but it can still be a\nvery good thing to do. All right. So just to summarize\nsome of the things that",
    "start": "4599360",
    "end": "4605270"
  },
  {
    "text": "are useful to understand from\nthis part of the section. And next time,\nwe're going to start talking about these ideas for\nsequential decision processes,",
    "start": "4605270",
    "end": "4613340"
  },
  {
    "text": "like Markov decision process. You should be able to\ndefine regret and PAC. You should be able to prove\nor know why the UCB bandit",
    "start": "4613340",
    "end": "4619940"
  },
  {
    "text": "algorithm has sublinear regret,\nlike up to the proof sketch we did in class. You should be able to give\nan example of why e-greedy,",
    "start": "4619940",
    "end": "4626180"
  },
  {
    "text": "and greedy and pessimism\ncan result in linear regret. I don't think you\nneed to be able to do",
    "start": "4626180",
    "end": "4632360"
  },
  {
    "text": "this for Gaussian rewards. But you should be able to do\nThompson's sampling for the case",
    "start": "4632360",
    "end": "4639320"
  },
  {
    "text": "that we've just talked about,\nat least in pseudocode land. So if someone said, you\nobserve another count,",
    "start": "4639320",
    "end": "4644519"
  },
  {
    "text": "what would your\nbeta parameter be? And then also that you should\nbe able to understand the UCB bandit algorithm as\nwe've covered in class.",
    "start": "4644520",
    "end": "4651650"
  },
  {
    "text": "So we've been building\nup all of these things to think about now how we can do\nexploration and data efficient",
    "start": "4651650",
    "end": "4657290"
  },
  {
    "text": "learning for\nsequential processes. So next time, we'll\nthink about how to do this in a standard\ndecision process",
    "start": "4657290",
    "end": "4663013"
  },
  {
    "text": "as well as thinking\nabout, what do we do when we're in really\nlarge state spaces or really large action spaces, and\nhow do we lift this all up",
    "start": "4663013",
    "end": "4668595"
  },
  {
    "text": "for function approximation? I'll see you on Wednesday. ",
    "start": "4668595",
    "end": "4677000"
  }
]