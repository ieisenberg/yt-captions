[
  {
    "start": "0",
    "end": "5120"
  },
  {
    "text": "So today, I'm to introduce our\nfirst invited speaker who's",
    "start": "5120",
    "end": "10880"
  },
  {
    "text": "Douwe Kiela. Douwe has also been-- as well as being invited and\nI'll tell his background,",
    "start": "10880",
    "end": "17810"
  },
  {
    "text": "he's also in the\nsymbolic systems program, has been an adjunct\nprofessor, and has",
    "start": "17810",
    "end": "23930"
  },
  {
    "text": "been involved with some\nstudents in that role as well. But in his invited role, he's\noriginally from the Netherlands",
    "start": "23930",
    "end": "30500"
  },
  {
    "text": "where he even learned some\nlogic among other things back in the old days. But in more recent\ntimes, he's been",
    "start": "30500",
    "end": "37880"
  },
  {
    "text": "a prominent\ndeep-learning researcher. For a number of years,\nhe worked at Facebook,",
    "start": "37880",
    "end": "44870"
  },
  {
    "text": "now Meta in the FAIR unit and\nwas involved in various ideas, including retrieval\naugmented generation.",
    "start": "44870",
    "end": "53780"
  },
  {
    "text": "After that, he then spent\nsome time at Hugging Face. He's become\ninterested in looking",
    "start": "53780",
    "end": "59750"
  },
  {
    "text": "at multimodal models,\nwhich is what he's going to be talking about today. And welcome, Douwe.",
    "start": "59750",
    "end": "66210"
  },
  {
    "text": "It's great to have you. Thank you very much. [APPLAUSE]",
    "start": "66210",
    "end": "71832"
  },
  {
    "text": " All right. That works, right?",
    "start": "71832",
    "end": "77230"
  },
  {
    "text": "Yes. Thanks, everyone, for coming. I understand that you get\npoints for being here,",
    "start": "77230",
    "end": "82430"
  },
  {
    "text": "so you're not\nreally here for me. [LAUGHTER] But thanks for coming anyway. So I'm going to talk about\nmultimodal deep learning.",
    "start": "82430",
    "end": "89439"
  },
  {
    "text": "It's going to have an NLP focus. Of course, that's\nfor this course. But it's also\nbecause, otherwise, I",
    "start": "89440",
    "end": "94570"
  },
  {
    "text": "would really be talking\nfor many more hours than I have time for here. So I'll try to really keep\nit focused on the things",
    "start": "94570",
    "end": "101650"
  },
  {
    "text": "that I think will be most\nuseful for you to learn. And so the first thing\nyou should understand",
    "start": "101650",
    "end": "107380"
  },
  {
    "text": "is that this whole\nconcept of multimodality is kind of\nill-defined, actually. So if you go to the\ndictionary, you'll",
    "start": "107380",
    "end": "114370"
  },
  {
    "text": "see that it means having\nor involving several modes or modalities or maxima.",
    "start": "114370",
    "end": "121135"
  },
  {
    "text": "And so what mode\nhere really means is-- so it could be mode\nin the very generic sense or it could be a very\nprecise sense of the mode",
    "start": "121135",
    "end": "129129"
  },
  {
    "text": "of a statistical distribution. And so depending on the paper\nyou're reading, in some cases,",
    "start": "129130",
    "end": "134382"
  },
  {
    "text": "people really mean\nthe statistical sense, in other cases, people\nreally mean this sort of very vague\nconcept of a modality",
    "start": "134382",
    "end": "140830"
  },
  {
    "text": "where it really means\nthe type of information that you're getting. So an example of\nmodality, in that case, is an image or speech\nsignal or audio,",
    "start": "140830",
    "end": "148930"
  },
  {
    "text": "in general, or even olfaction,\nso smell or things like that. So in this lecture,\nwe're just going",
    "start": "148930",
    "end": "156129"
  },
  {
    "text": "to focus mostly on text\nbecause this is an NLP course, and we're going to\nfocus on images mostly",
    "start": "156130",
    "end": "162430"
  },
  {
    "text": "as the other modality\nto keep it simple. All right, so why\ndoes it matter?",
    "start": "162430",
    "end": "168490"
  },
  {
    "text": "Why do we care\nabout multimodality? And so there are a couple\nof really good reasons",
    "start": "168490",
    "end": "173670"
  },
  {
    "text": "in general for this. The first one is\nabout faithfulness. So if you look at how we\nhumans understand the world,",
    "start": "173670",
    "end": "181140"
  },
  {
    "text": "how we make sense of what\nhappens in the world, that is very multimodal, right?",
    "start": "181140",
    "end": "186630"
  },
  {
    "text": "So we perceive the world,\nnot just using vision or just audio, but we\nsynthesize information",
    "start": "186630",
    "end": "192750"
  },
  {
    "text": "across all of these\ndifferent modalities and that's how we understand\nthe world and each other. There's also a very practical\nargument for doing it.",
    "start": "192750",
    "end": "200739"
  },
  {
    "text": "It's because the internet\nis multimodal, right? So if you go to, I don't\nknow, Facebook or something",
    "start": "200740",
    "end": "206010"
  },
  {
    "text": "like that, it rarely happens\nthat it's just text or just an image, it's\nusually a combination of multiple modalities.",
    "start": "206010",
    "end": "212890"
  },
  {
    "text": "And then the final good\nreason that we're just starting to hit now if\nyou're really following",
    "start": "212890",
    "end": "219269"
  },
  {
    "text": "where the field is\ngoing, we're kind of running out of text data for\nthese large language models. So one interesting way to\nkeep scaling on the data side",
    "start": "219270",
    "end": "228130"
  },
  {
    "text": "is to make use of all of\nthese other modalities. So if you can have your\nlanguage model also watch",
    "start": "228130",
    "end": "233620"
  },
  {
    "text": "all of the videos of\ncats in the world, it's going to understand the\nconcept of cat much better.",
    "start": "233620",
    "end": "238790"
  },
  {
    "text": "And that's what we want\nto have in these models. We want them to understand\nthe world in the same way",
    "start": "238790",
    "end": "244000"
  },
  {
    "text": "that humans understand it. So right now\nmultimodality is really",
    "start": "244000",
    "end": "249040"
  },
  {
    "text": "one of the main frontiers of\nthis new foundation model drive that we're all in right now.",
    "start": "249040",
    "end": "255500"
  },
  {
    "text": "There's a thing called\nthe McGurk effect. Let's see if it loads up. So what we'll see when this\nloads is this guy over here.",
    "start": "255500",
    "end": "265160"
  },
  {
    "text": "And we'll have the same\naudio effect being played. So the audio is\nexactly the same.",
    "start": "265160",
    "end": "270970"
  },
  {
    "text": "And this man is going to say\nsomething like ba, ba, ba.",
    "start": "270970",
    "end": "276190"
  },
  {
    "text": "And so you're hearing a B there\nI think if you look at my mouth because that's what I said.",
    "start": "276190",
    "end": "281200"
  },
  {
    "text": "But if you then change\nthe video to where he says fa, fa, fa, with\nexactly the same audio,",
    "start": "281200",
    "end": "287800"
  },
  {
    "text": "you're going to hear\nthe other version. So unfortunately, I can't really\nswap in the different audio",
    "start": "287800",
    "end": "293830"
  },
  {
    "text": "here, so you have\nto trust me for it. We might suddenly start hearing\na guy saying fa, fa, fa, and then--",
    "start": "293830",
    "end": "299050"
  },
  {
    "text": "[LAUGHTER]  All right. So multimodal applications.",
    "start": "299050",
    "end": "305720"
  },
  {
    "text": "So when we have\nmultiple modalities, we can do all kinds\nof interesting things.",
    "start": "305720",
    "end": "311780"
  },
  {
    "text": "And as I said, most of the use\ncases we have on the internet, they're all multimodal. And there are some really\nkind of obvious things",
    "start": "311780",
    "end": "318970"
  },
  {
    "text": "we would be interested\nin if we have information from these different\ndata sources, right, from different modalities.",
    "start": "318970",
    "end": "325490"
  },
  {
    "text": "So obviously, we might\nwant to do retrieval. So maybe given a\nbit of text we want to find the right\nimage or maybe given",
    "start": "325490",
    "end": "332620"
  },
  {
    "text": "some image we want to find\nthe right text for it so we can match them up. Obviously, we can also do\nthis in a generative setting.",
    "start": "332620",
    "end": "338750"
  },
  {
    "text": "So then we have image\ncaptioning, which you probably heard of, we can do\ntext-to-image generation. So that's image synthesis,\nso stable diffusion.",
    "start": "338750",
    "end": "346300"
  },
  {
    "text": "Everybody in the audience\nhere has probably seen that. Then we can do visual\nquestion answering",
    "start": "346300",
    "end": "351563"
  },
  {
    "text": "where we have an image\nand text and then we need to generate\nsome new text. We have multimodal\nclassification where we have image\nand text and we",
    "start": "351563",
    "end": "357730"
  },
  {
    "text": "need to have a\nlabel, for example, whether something is\nhate speech or not. And then, in general,\nwe want to be",
    "start": "357730",
    "end": "363400"
  },
  {
    "text": "able to have a richer\nunderstanding of information, which means that we\ncombine images and text",
    "start": "363400",
    "end": "368650"
  },
  {
    "text": "and then use it for\ndownstream applications that require better understanding\nor better generation.",
    "start": "368650",
    "end": "374260"
  },
  {
    "text": "So this field really\nis super hot right now. So there's this\nnice paper title.",
    "start": "374260",
    "end": "380110"
  },
  {
    "text": "I predict that\nthis paper is going to do really well in\nterms of citations just because it has\nsuch a citable title.",
    "start": "380110",
    "end": "385930"
  },
  {
    "text": "I think a lot of people are\nnot actually going to read it. And so, I mean, I've been\nin this field for quite a",
    "start": "385930",
    "end": "391258"
  },
  {
    "text": "while now and people\nhave been saying that for a really long time. I think Chris would\nagree, though. So for decades, people\nhave been saying",
    "start": "391258",
    "end": "397060"
  },
  {
    "text": "that multimodal is\nthe next big thing, but now it's really\ntrue, I think. [LAUGHTER]",
    "start": "397060",
    "end": "402140"
  },
  {
    "text": " All right. So the outline for what we're\ngoing to be talking about. So first, I'm going to\ntell you a little bit",
    "start": "402140",
    "end": "408220"
  },
  {
    "text": "about early models,\nthen we're going to do a bit of a deep dive\non some of the specifics, then we're going to go over\na particular type of fusion,",
    "start": "408220",
    "end": "416530"
  },
  {
    "text": "contrastive models,\nor late fusion. Then we're going to go\nthrough a little bit of the history of multimodal\nfoundation models.",
    "start": "416530",
    "end": "423628"
  },
  {
    "text": "Then we're going to talk a\nlittle bit about evaluation, a little bit about\nother modalities, and then I'll make some\npredictions for the future,",
    "start": "423628",
    "end": "429610"
  },
  {
    "text": "and hopefully, maybe give\nyou some cool research ideas or things to\ntalk or think about.",
    "start": "429610",
    "end": "434810"
  },
  {
    "text": "All right. So, obviously,\nthere's a lot of work that happened before\ndeep learning. But I think if you want to\nstart from the deep learning",
    "start": "434810",
    "end": "442780"
  },
  {
    "text": "revolution and what was\nhappening in images and text, then a good starting point is,\nfor example, WSABI or DeVise",
    "start": "442780",
    "end": "452510"
  },
  {
    "text": "or Richard Socher, who\nyou've probably heard of, has done some really\ncool early work in this that really pioneered\na lot of these ideas.",
    "start": "452510",
    "end": "460699"
  },
  {
    "text": "And the basic gist of this is\nthat we have a vision model. On the one hand, we\nhave a language model.",
    "start": "460700",
    "end": "467811"
  },
  {
    "text": "So this really, I\nmean, the first lecture of this course I think was\nabout word embeddings, right? So that's just your basic\nword embedding model.",
    "start": "467812",
    "end": "474080"
  },
  {
    "text": "And now we need to figure\nout how to align them in the same multimodal space. So the way you do that is you\nget some sort of similarity",
    "start": "474080",
    "end": "481070"
  },
  {
    "text": "metric, right,\nthis score function or like a kernel function if\nyou're thinking about this from a support vector machine\nliterature perspective",
    "start": "481070",
    "end": "487340"
  },
  {
    "text": "and now you need to figure\nout in a max margin or margin loss, how you want\nto align these two",
    "start": "487340",
    "end": "494810"
  },
  {
    "text": "points in your embedding space. So things that are similar,\nyou want to bring them closer together,\nthings that are not, you want bring\nthem further apart.",
    "start": "494810",
    "end": "501690"
  },
  {
    "text": "And if you do that in this\nmultimodal embedding space that means that you can\ndo interesting cross-modal",
    "start": "501690",
    "end": "508280"
  },
  {
    "text": "transfer where you can take the\nword embedding for something like auto or like\nhorse, and then you can find close images in the\nembedding space to that thing",
    "start": "508280",
    "end": "516900"
  },
  {
    "text": "and now you've solved\nthe retrieval problem. So this is a really\nnice early application.",
    "start": "516900",
    "end": "522419"
  },
  {
    "text": "And I think a lot of\nthe stuff that I'm going to talk about in\nthe early slides you're going to see this thing\ncome over and over again.",
    "start": "522419",
    "end": "529040"
  },
  {
    "text": "You're going to see it\nget kind of reinvented with fancier models, but it's\nbasically all the same stuff.",
    "start": "529040",
    "end": "535460"
  },
  {
    "text": "So you can do\ncross-modal transfer where you have images and\ntext, but you can also combine them together so that\nyou get a multimodal word",
    "start": "535460",
    "end": "543020"
  },
  {
    "text": "embedding. And so this just gives you a\nmore accurate representation",
    "start": "543020",
    "end": "548089"
  },
  {
    "text": "of how humans understand word\nmeaning because when we think about the word moon\nor cat or something,",
    "start": "548090",
    "end": "554269"
  },
  {
    "text": "we can go to Wikipedia\nand read that a cat is a small carnivorous mammal that\npeople like to keep as pets,",
    "start": "554270",
    "end": "560150"
  },
  {
    "text": "or we can just go and\nlook at pictures of cats and now we understand\nwhat a cat is, right? And I would argue actually\nthat for a lot of people,",
    "start": "560150",
    "end": "566779"
  },
  {
    "text": "the picture of the cat is\nmuch closer to the meaning of the concept of cat. So some early work\nwhere people were",
    "start": "566780",
    "end": "574270"
  },
  {
    "text": "trying to do this\nis from Bruni et al. where they did multimodal\ndistributional semantics using",
    "start": "574270",
    "end": "580300"
  },
  {
    "text": "this very elegant approach\ncalled bag of visual words. So who has heard of\nbag of visual words?",
    "start": "580300",
    "end": "588450"
  },
  {
    "text": "Very few people. OK. So it's surprisingly simple. So I like it. It's nicely elegant.",
    "start": "588450",
    "end": "594450"
  },
  {
    "text": "So you take a picture of\nthe moon, in this case. I think you can see it\nin the back, too, right? So we use an algorithm like SIFT\nto find interesting key points,",
    "start": "594450",
    "end": "603750"
  },
  {
    "text": "so sort of where the\ndifference between the pixels and the pixels next to it,\nwhere the difference is big,",
    "start": "603750",
    "end": "609930"
  },
  {
    "text": "those are the spots you\nwant to be looking at. And for each of\nthese key points,",
    "start": "609930",
    "end": "615360"
  },
  {
    "text": "you get feature descriptors. So relatively small vectors\nlike 32 dimensional. It depends kind of on the\nimplementation of this.",
    "start": "615360",
    "end": "622950"
  },
  {
    "text": "And what you can do\nnow with these feature descriptors is you can\ncluster them using k means and then you assign\nevery one of these points",
    "start": "622950",
    "end": "631250"
  },
  {
    "text": "so you can count how\noften they occur, right? So in this picture\nof the moon, we have actually the\ncount is-- oh yeah,",
    "start": "631250",
    "end": "636860"
  },
  {
    "text": "so there are three\nred dots, right? So that's why the\nred dot one is three. So what that gives you is\nan idea of the visual words,",
    "start": "636860",
    "end": "645890"
  },
  {
    "text": "very similar to the\noriginal bag of words model that you hopefully\nhave heard about maybe in the first lecture.",
    "start": "645890",
    "end": "652320"
  },
  {
    "text": "So that's the visual equivalent\nof the textual thing. And so if you do this\nand you then concatenate",
    "start": "652320",
    "end": "658820"
  },
  {
    "text": "or you apply SVD to fuse the\ninformation, what you get is a word embedding that\nis much more representative",
    "start": "658820",
    "end": "665029"
  },
  {
    "text": "of human meaning, as\nreflected in the data sets that people use to\ncare about at the time.",
    "start": "665030",
    "end": "672450"
  },
  {
    "text": "So after that, there were a\ncouple of people, me included, who tried to take these ideas\nand then really apply deep",
    "start": "672450",
    "end": "678750"
  },
  {
    "text": "learning to them. So some of the very early\nversions of this use convolutional neural\nnetworks, and then",
    "start": "678750",
    "end": "685080"
  },
  {
    "text": "you can transfer the\nfeatures from your ConvNet and you take your\nword embeddings",
    "start": "685080",
    "end": "690480"
  },
  {
    "text": "which you've seen in\nthe first lecture, and then you can\nconcatenate them now you have a multimodal\nword vector,",
    "start": "690480",
    "end": "696690"
  },
  {
    "text": "or you can do something\nslightly fancier. So you've seen the\nskip-gram model. You can also try to do\nskip-gram predictions",
    "start": "696690",
    "end": "703800"
  },
  {
    "text": "onto image features, right? So when you see a word\nlike cat in some context like the cute little cat sat on\nthe mat, then when you see cat",
    "start": "703800",
    "end": "711690"
  },
  {
    "text": "you also want to\npredict cat pictures. So super easy ideas,\nbut it turned out that this gives you much\nricher word representations.",
    "start": "711690",
    "end": "719440"
  },
  {
    "text": "So that's kind of cool. But obviously, words\nare very limited. What we really care about\nis not words but sentences.",
    "start": "719440",
    "end": "726089"
  },
  {
    "text": "So then people started\nreally looking into sentence representations and\nhow can we figure out",
    "start": "726090",
    "end": "732269"
  },
  {
    "text": "how to get compositional\nunderstanding in these sentence representations and how do\nwe align that with images.",
    "start": "732270",
    "end": "739800"
  },
  {
    "text": "So the loss here is\nvery similar to what we saw with words\nand pictures but now we just have a sentence\nencoder, right?",
    "start": "739800",
    "end": "747060"
  },
  {
    "text": "And so there's some\nreally cool early papers from Andrej Karpathy,\nand Richard Socher also",
    "start": "747060",
    "end": "752220"
  },
  {
    "text": "had some work here.  So the basic idea is just that\ninstead of having these word",
    "start": "752220",
    "end": "758940"
  },
  {
    "text": "embeddings we now have\nan LSTM in these papers or some other kind of\nrecurrent neural network,",
    "start": "758940",
    "end": "764220"
  },
  {
    "text": "or in the case of this one,\nrecursive neural network, and then we try to align\nthe features together.",
    "start": "764220",
    "end": "771070"
  },
  {
    "text": "And so these three\nor four papers are actually very important. This one by me is less\nimportant but it's still",
    "start": "771070",
    "end": "776800"
  },
  {
    "text": "kind of interesting\nbecause we showed here that grounded sentence\nrepresentation.",
    "start": "776800",
    "end": "782230"
  },
  {
    "text": "So if you actually\njust use this part here as a sentence\nencoder for NLP tasks,",
    "start": "782230",
    "end": "787260"
  },
  {
    "text": "the ability to just\npredict pictures from it already gives you a really good\nsentence representation, right?",
    "start": "787260",
    "end": "793260"
  },
  {
    "text": "So just by predicting\npictures you can imagine what\nthings look like and that gives you a really good\nmeaning representation which",
    "start": "793260",
    "end": "800040"
  },
  {
    "text": "you can then transfer\nto, I don't know, sentiment classification\nor something else.",
    "start": "800040",
    "end": "806410"
  },
  {
    "text": "And then of course, once\nwe have sentence encoders then we also have decoders.",
    "start": "806410",
    "end": "813100"
  },
  {
    "text": "And so when the\nsequence-to-sequence architecture came out, which\nyou've probably also heard about in this course, what you\ncan do instead of having a text",
    "start": "813100",
    "end": "821620"
  },
  {
    "text": "encoder for your source\nlanguage if you're doing machine translation is you can plug in\na ConvNet instead of an LSTM",
    "start": "821620",
    "end": "828850"
  },
  {
    "text": "encoder, and now you\ncan generate captions. So that's exactly\nwhat people did. We used to have all of these\nfancy diagrams in our papers",
    "start": "828850",
    "end": "836155"
  },
  {
    "text": "then where we explained the\nLSTM and how that works. Probably people don't learn\nthat anymore these days.",
    "start": "836155",
    "end": "841510"
  },
  {
    "text": "They do? Yeah. [INAUDIBLE] Very good. They might make a comeback,\nI think, at some point.",
    "start": "841510",
    "end": "847570"
  },
  {
    "text": "Transformers are\ngoing to go away. We'll see. [LAUGHTER]  And so one of the things\nthat people figured out",
    "start": "847570",
    "end": "854650"
  },
  {
    "text": "in machine translation\nvery early on is that you can do\nalignment of words between your source language\nand your target language.",
    "start": "854650",
    "end": "861610"
  },
  {
    "text": "And you can do the same\nthing actually with images. So if you want to align a word\nin your generated sequence",
    "start": "861610",
    "end": "868700"
  },
  {
    "text": "with something in\nyour picture, then you can use the same\napproach for that. And that approach, of\ncourse, is called attention.",
    "start": "868700",
    "end": "876350"
  },
  {
    "text": "So you've learned a\nlot about attention probably in this course. And so yeah, that was one\nof the building blocks",
    "start": "876350",
    "end": "882403"
  },
  {
    "text": "of these systems\nas well where you can do very interesting\nthings and really see that when it has to generate\nstop for the stop sign,",
    "start": "882403",
    "end": "890210"
  },
  {
    "text": "that it's really actually\nlooking at the stop sign. So there's a really\ncool alignment going on there in these models.",
    "start": "890210",
    "end": "898230"
  },
  {
    "text": "And so the final early model we\nshould talk about a little bit is GANs.",
    "start": "898230",
    "end": "904040"
  },
  {
    "text": "Who here has heard of GANs? OK. That's a lot more than\nbag of visual words.",
    "start": "904040",
    "end": "909190"
  },
  {
    "text": "I guess that makes sense. So yeah, the basic idea of a\nGAN is really that you have this",
    "start": "909190",
    "end": "915070"
  },
  {
    "text": "generator and discriminator and\nyou want to have the generator generate images that\nthe discriminator cannot",
    "start": "915070",
    "end": "920710"
  },
  {
    "text": "distinguish. So it cannot distinguish\nfake and real images, right? And if you do that, you\ncan actually condition that",
    "start": "920710",
    "end": "928120"
  },
  {
    "text": "on a piece of text, and\nthen you can generate images using some text prompts.",
    "start": "928120",
    "end": "934959"
  },
  {
    "text": "So that's what the first\nversions of stable diffusion were doing things like this and\nit's all a natural progression",
    "start": "934960",
    "end": "942940"
  },
  {
    "text": "to that model. So those were the early models.",
    "start": "942940",
    "end": "949220"
  },
  {
    "text": "Do people have any burning\nquestions about this or does this all make sense? ",
    "start": "949220",
    "end": "955150"
  },
  {
    "text": "All right. So let's do a bit\nof a deeper dive then, in particular,\non features and fusion.",
    "start": "955150",
    "end": "961960"
  },
  {
    "text": "So those are really the\ncore building blocks for all of this\nmultimodal stuff. But before we go\nthere, maybe very",
    "start": "961960",
    "end": "967870"
  },
  {
    "text": "briefly if all of\nthis multimodal stuff is cool and sort of useful and\ndoesn't look that difficult,",
    "start": "967870",
    "end": "975700"
  },
  {
    "text": "why aren't we all doing\nmultimodal things? So why do we focus on\nspecific modalities?",
    "start": "975700",
    "end": "981570"
  },
  {
    "text": "And I think there are\na couple of problems just to be aware of. So one is modalities\ncan sometimes dominate,",
    "start": "981570",
    "end": "988060"
  },
  {
    "text": "especially text is much more\ndominant than vision or audio in many use cases.",
    "start": "988060",
    "end": "993070"
  },
  {
    "text": "So you can already\njust have a model that picks up on the\ntext signal and basically learns to ignore the\nimage completely,",
    "start": "993070",
    "end": "999320"
  },
  {
    "text": "which actually\nhappened embarrassingly for visual question answering. We'll get to that. So visual question\nanswering you could do that",
    "start": "999320",
    "end": "1005520"
  },
  {
    "text": "without actually\nlooking at the picture. The additional modalities\ncan add a lot of noise,",
    "start": "1005520",
    "end": "1011522"
  },
  {
    "text": "so it makes your\nmachine-learning problem more difficult. You don't\nalways have full coverage.",
    "start": "1011522",
    "end": "1016529"
  },
  {
    "text": "So as I said if you\nlook at Facebook posts, sometimes you have text,\nsometimes you have pictures, sometimes you have both, but\nyou don't have a guarantee",
    "start": "1016530",
    "end": "1022200"
  },
  {
    "text": "that you always have both. So how do you deal with that? In many cases, we just\nreally weren't ready,",
    "start": "1022200",
    "end": "1027390"
  },
  {
    "text": "it was too complicated\nto implement stuff. And also just in general how\nto design your model really",
    "start": "1027390",
    "end": "1034290"
  },
  {
    "text": "to combine all the information\nis actually quite complicated. So in order to maybe drive\nthe point home a little bit,",
    "start": "1034290",
    "end": "1044619"
  },
  {
    "text": "so featurizing text. I guess we all know how to do\nthat by now, especially sort of in the age of\ntransformers and before",
    "start": "1044619",
    "end": "1051180"
  },
  {
    "text": "in LSTMs where we just have\nyour batch by your sequence. So batch size by sequence\nlength by embedding size, right?",
    "start": "1051180",
    "end": "1058050"
  },
  {
    "text": "So it's always like a\n3D tensor, and that's how you encode your textual\ninformation when you pump it",
    "start": "1058050",
    "end": "1064440"
  },
  {
    "text": "through your neural net. And so with images,\nit's slightly trickier because you can just kind\nof look at the patches.",
    "start": "1064440",
    "end": "1071640"
  },
  {
    "text": "But then if you do convolutions,\nyou're shifting over the image and then you're\naggregating, right?",
    "start": "1071640",
    "end": "1077590"
  },
  {
    "text": "And in many cases, you don't\nreally want to be this uniform. You want to have\nsomething that actually",
    "start": "1077590",
    "end": "1082750"
  },
  {
    "text": "looks at the things\nin the picture, right? So this is called\nregion features where you would use an object\ndetector as a first step",
    "start": "1082750",
    "end": "1089620"
  },
  {
    "text": "for processing your\nimage, and then you would have a\nConvNet backbone that encodes the features for\nthat particular sub-image",
    "start": "1089620",
    "end": "1096549"
  },
  {
    "text": "like these guys, like\nskateboard or something, it has its own vector\nrepresentation, right?",
    "start": "1096550",
    "end": "1101890"
  },
  {
    "text": "And then in terms\nof dense features, we now also have\nVision Transformers. So we'll just very\nquickly go over",
    "start": "1101890",
    "end": "1107860"
  },
  {
    "text": "that to make sure\nwe're on the same page. So there are all these models,\nlike YOLO is a really good one if you haven't\nheard of that yet.",
    "start": "1107860",
    "end": "1114160"
  },
  {
    "text": "So we're at YOLOv7 now, I\nthink, or 8, I don't know. So there's a new one coming out\nevery other year or something.",
    "start": "1114160",
    "end": "1122620"
  },
  {
    "text": "But the basic idea\nis that we get these bounding boxes\nfor things in the images",
    "start": "1122620",
    "end": "1127809"
  },
  {
    "text": "or actually segmentations\nwith the bounding boxes is what people tend to use\nand they have labels, right? So this is labeled like\nbackpack or something.",
    "start": "1127810",
    "end": "1134810"
  },
  {
    "text": "And so you can do this\nas a pre-processing step on your image to get a much\nricher representation of what",
    "start": "1134810",
    "end": "1141198"
  },
  {
    "text": "is really in that image,\nwhich you can then pump into your system\nas we'll see later. So then how you\nencode the information",
    "start": "1141198",
    "end": "1147970"
  },
  {
    "text": "that is in these little\nbounding boxes or actually in the image itself in general? We just use a standard\nConvNet for that.",
    "start": "1147970",
    "end": "1155000"
  },
  {
    "text": "And so this probably feels super\nobvious now, but in 2014 when",
    "start": "1155000",
    "end": "1160300"
  },
  {
    "text": "people were starting\nto discover this, it was really very surprising\nthat you could just use off-the-shelf\nConvNet features",
    "start": "1160300",
    "end": "1167049"
  },
  {
    "text": "to really replace the entire\ncomputer vision pipeline. So people used to do all of this\nvery fancy sophisticated stuff",
    "start": "1167050",
    "end": "1173930"
  },
  {
    "text": "and people spent decades\non trying to refine this and then it was all\nthrown away and replaced by a ConvNet that does all\nof that stuff for free.",
    "start": "1173930",
    "end": "1181910"
  },
  {
    "text": "And so the cool\nthing you get there is that you can transfer very\neasily across different tasks.",
    "start": "1181910",
    "end": "1186950"
  },
  {
    "text": "So you can have a\nvery generic ConvNet and then use it to all\nkinds of very specialized",
    "start": "1186950",
    "end": "1192009"
  },
  {
    "text": "things like spotting buildings\nin Paris, for example, or flowers, or other stuff.",
    "start": "1192010",
    "end": "1198700"
  },
  {
    "text": "And then of course in the age\nof transformers, how far are we? We're already quite a while.",
    "start": "1198700",
    "end": "1204190"
  },
  {
    "text": "And this is only the\nfirst transformer actually in the slide deck. So we're making good progress.",
    "start": "1204190",
    "end": "1210610"
  },
  {
    "text": "So Vision Transformers\nare what we would use these days to\nencode the images where",
    "start": "1210610",
    "end": "1215680"
  },
  {
    "text": "you have these flattened\npatches and then you would do kind of the\nstandard BERT architecture",
    "start": "1215680",
    "end": "1221650"
  },
  {
    "text": "maybe as you would know\nit from this course, and then you do\nclassification, right? So this is all a\nstandard transformer.",
    "start": "1221650",
    "end": "1227247"
  },
  {
    "text": "Everything is standard,\nexcept now your input here is not words or tokens,\nit's patches of an image.",
    "start": "1227247",
    "end": "1232840"
  },
  {
    "text": "And then you classify that. All right. So then we have a bunch\nof features and now",
    "start": "1232840",
    "end": "1238279"
  },
  {
    "text": "how do we combine the\ninformation, right? So let's say we have\ntwo vectors u and v. So it sounds easy how\nwe could combine them.",
    "start": "1238280",
    "end": "1247370"
  },
  {
    "text": "It turns out that\nthere are actually very many ways to combine them. So I don't think\nit's really useful",
    "start": "1247370",
    "end": "1252380"
  },
  {
    "text": "to go over all the\ndifferent ways here, but you can do\nvery simple things. So obviously, inner\nproduct or similarity",
    "start": "1252380",
    "end": "1259201"
  },
  {
    "text": "is what you would use if you\nwant to do cross-modal things. So if you want to embed things\nin the same vector space.",
    "start": "1259202",
    "end": "1264690"
  },
  {
    "text": "But you can do sort\nof fancier projections on top or different\ncombinations that are linear,",
    "start": "1264690",
    "end": "1271429"
  },
  {
    "text": "or you can do\nmultiplicative things where you multiply the\ncomponents element-wise or you do some sort of gating\nover the different features.",
    "start": "1271430",
    "end": "1278660"
  },
  {
    "text": "You can do attention. You can do fancier\nbilinear things. You can do very fancy\ncompact bilinear things.",
    "start": "1278660",
    "end": "1285080"
  },
  {
    "text": "So there's really a\nwealth of literature on all the different ways\nyou can combine two vectors.",
    "start": "1285080",
    "end": "1290580"
  },
  {
    "text": "And so this is called\nmultimodal fusion. And most of the literature\non multimodality",
    "start": "1290580",
    "end": "1296100"
  },
  {
    "text": "is essentially about\nthis question-- what is the best way to do fusion? And that's it.",
    "start": "1296100",
    "end": "1302510"
  },
  {
    "text": "So I think within\nthat discussion, it's maybe useful to distinguish\nbetween different levels of fusion.",
    "start": "1302510",
    "end": "1307830"
  },
  {
    "text": "So you can do it very\nearly where basically you make sure you have the different\nfeatures and then you just",
    "start": "1307830",
    "end": "1314240"
  },
  {
    "text": "in the modern sense\nof attention you would attend to everything\nin all the features from the beginning.",
    "start": "1314240",
    "end": "1319730"
  },
  {
    "text": "You can first treat them\nseparately and then combine them, or you can treat\nthem as completely separate",
    "start": "1319730",
    "end": "1325160"
  },
  {
    "text": "and then you only combine\nthe final scores, right? So that's what we would\ncall early fusion.",
    "start": "1325160",
    "end": "1331370"
  },
  {
    "text": "And then my invention for\ncalling the middle part would be sort of\nmiddle fusion, and then you have late fusion where\nyou really just combine",
    "start": "1331370",
    "end": "1338630"
  },
  {
    "text": "the scores or the logits,\nbut you don't really have any interaction\nbetween the information",
    "start": "1338630",
    "end": "1343640"
  },
  {
    "text": "from the different modalities. So you can do really fun\nstuff with multimodal fusion.",
    "start": "1343640",
    "end": "1350420"
  },
  {
    "text": "So this is a paper I\nreally like, FiLM, where you have this very\nspecial feature",
    "start": "1350420",
    "end": "1359510"
  },
  {
    "text": "map, this sort of F here\nand it gets modulated by a multiplicative factor.",
    "start": "1359510",
    "end": "1364830"
  },
  {
    "text": "So this gamma and an additive\nsort of bias vector, this beta, and you have a different one\nfor every layer of a ResNet",
    "start": "1364830",
    "end": "1372360"
  },
  {
    "text": "that is conditioned on some\nencoding of the thing you're after. So in this case, are there\nmore cubes than yellow things?",
    "start": "1372360",
    "end": "1378700"
  },
  {
    "text": "So we have some vector\nrepresentation for that. And we use that\nvector representation to modulate the ResNet blocks\nat every layer of the ConvNet.",
    "start": "1378700",
    "end": "1388290"
  },
  {
    "text": "So you can really\ndo very fun things where you're sort of modulating\none network with the other one",
    "start": "1388290",
    "end": "1393720"
  },
  {
    "text": "and really try to have them\nlearn as much as possible from that. All right.",
    "start": "1393720",
    "end": "1398870"
  },
  {
    "text": "So let's talk about\nlate fusion then. So late fusion is what we would\nnow call contrastive models.",
    "start": "1398870",
    "end": "1406130"
  },
  {
    "text": "But the basic idea is that we\nhave this similarity score. So we have these to kind of--\nwe process the modalities",
    "start": "1406130",
    "end": "1412190"
  },
  {
    "text": "completely independently\nand then at the very end, we do some combination. And the most famous instance\nof that these days is CLIP.",
    "start": "1412190",
    "end": "1420169"
  },
  {
    "text": "So who's heard of CLIP? OK. So CLIP from OpenAI.",
    "start": "1420170",
    "end": "1427610"
  },
  {
    "text": "So it's, again, exactly\nthe same contrastive loss that we've seen. And all these early\napproaches, it",
    "start": "1427610",
    "end": "1434450"
  },
  {
    "text": "does kind of negative\nsampling but then in batch. So you just have a batch. You have two things\nthat are aligned.",
    "start": "1434450",
    "end": "1441260"
  },
  {
    "text": "So like this is the first piece\nof text and the first image, they are aligned. So this is the right answer.",
    "start": "1441260",
    "end": "1446669"
  },
  {
    "text": "And I just want to make sure\nthat I rank this thing higher than all the\nalternatives, right,",
    "start": "1446670",
    "end": "1451820"
  },
  {
    "text": "and I want to make sure I rank\nthis thing higher than all the alternatives. So it's a very,\nvery simple idea.",
    "start": "1451820",
    "end": "1458760"
  },
  {
    "text": "Really nothing special\nabout this architecture that was invented here, but\nwhat made this thing so cool",
    "start": "1458760",
    "end": "1465080"
  },
  {
    "text": "was first of all,\nit was transformers and it was transformers\nall the way. So your text encoder\nwould be a transformer",
    "start": "1465080",
    "end": "1470210"
  },
  {
    "text": "and your image encoder would\nbe a ViT image encoder, so also a transformer.",
    "start": "1470210",
    "end": "1475520"
  },
  {
    "text": "And it was trained on\nlots and lots of web data. So Alec Radford\nis really a genius",
    "start": "1475520",
    "end": "1481490"
  },
  {
    "text": "at creating very\nhigh-quality data sets. And he created I think\n300 million image text",
    "start": "1481490",
    "end": "1487380"
  },
  {
    "text": "pairs for this data set,\ntrained a bigger model on it than people used to do, and then\nwe got this amazing model out",
    "start": "1487380",
    "end": "1494850"
  },
  {
    "text": "of it. And so moving away from the\nwords there to the sort of text",
    "start": "1494850",
    "end": "1501540"
  },
  {
    "text": "that you would see\non the internet. So the caption for\nan image on the web is not going to say\ndog or cat, it's",
    "start": "1501540",
    "end": "1507809"
  },
  {
    "text": "going to say a photo of a cat\ndoing something, something. So that means that you can do\nzero-shot label predictions",
    "start": "1507810",
    "end": "1516029"
  },
  {
    "text": "where you have a photo\nof the-- and then you need to figure out\nwhat the right label is",
    "start": "1516030",
    "end": "1521700"
  },
  {
    "text": "for a given image using\nthis kind of prompt. So you probably all know\nabout prompting large language",
    "start": "1521700",
    "end": "1528179"
  },
  {
    "text": "models, so you can prompt\nvision and language models in very much\nthe same way and do zero-shot generalization.",
    "start": "1528180",
    "end": "1535690"
  },
  {
    "text": "So if you want to read\na really good paper, I would recommend that\nyou read this paper. This is really one that's\ngoing to teach you how",
    "start": "1535690",
    "end": "1541530"
  },
  {
    "text": "to write really good papers. It's thorough. It's really worth\na very close read I think if you're\ninterested in this field.",
    "start": "1541530",
    "end": "1548050"
  },
  {
    "text": "And so I think when it came out,\nactually, on ImageNet itself,",
    "start": "1548050",
    "end": "1553470"
  },
  {
    "text": "it didn't really\noutperform ResNet, right? So you might think,\noh yeah, actually, it's not all that special.",
    "start": "1553470",
    "end": "1559680"
  },
  {
    "text": "But what really\nmade it special was that it generalized much better\nto these other data sets. So this ResNet thing here\nis pretty terrible at some",
    "start": "1559680",
    "end": "1568350"
  },
  {
    "text": "of these kind of adversarial\nversions of ImageNet, and CLIP is super\nrobust to that. So it's just a way better\nimage encoder in general.",
    "start": "1568350",
    "end": "1577460"
  },
  {
    "text": "So very, very\nquickly after CLIP, there was this paper from Google\nusing ALIGN which was basically",
    "start": "1577460",
    "end": "1585400"
  },
  {
    "text": "exactly the same idea. The field is not really\nthat creative at all. It's the same idea, but then\nyou just keep throwing more data",
    "start": "1585400",
    "end": "1592720"
  },
  {
    "text": "and more compute at it and\nthat often works much better. So that's what they\nfound here, too.",
    "start": "1592720",
    "end": "1597790"
  },
  {
    "text": "1.8 billion image text\npairs instead of 300 million gives you a better model.",
    "start": "1597790",
    "end": "1603030"
  },
  {
    "text": "Surprise. So still very cool. And what is really\ncool, I think,",
    "start": "1603030",
    "end": "1609389"
  },
  {
    "text": "is that there's this\norganization called LAION, where they've started this\nopen-source collective",
    "start": "1609390",
    "end": "1616620"
  },
  {
    "text": "to create really\nhigh-quality data sets. And so the LAION, the\ninitial data set, was--",
    "start": "1616620",
    "end": "1624300"
  },
  {
    "text": "how many examples in\nthe initial LAION? 400 million. 400 million. He knows. I know that he knows.",
    "start": "1624300",
    "end": "1632280"
  },
  {
    "text": "So now there's a much\nbigger version of LAION that's even multilingual and\nit has 5 billion examples.",
    "start": "1632280",
    "end": "1638010"
  },
  {
    "text": "So Stable Diffusion was trained\non sort of the English subset of this thing.",
    "start": "1638010",
    "end": "1643960"
  },
  {
    "text": "And that's one of the\nreasons that it's so awesome is because it's just\nseen a ton of data and that really makes\nyour system a lot better.",
    "start": "1643960",
    "end": "1651280"
  },
  {
    "text": "So if you're looking for\nthe ultimate data set to play around with\nyour own ideas,",
    "start": "1651280",
    "end": "1657112"
  },
  {
    "text": "if you have enough compute,\nobviously, then you should really look\nat this data set.",
    "start": "1657112",
    "end": "1662460"
  },
  {
    "text": "All right. Any questions about\nup until this point? ",
    "start": "1662460",
    "end": "1670650"
  },
  {
    "text": "No? All right. So then we'll move\non from late fusion",
    "start": "1670650",
    "end": "1675950"
  },
  {
    "text": "to kind of middle\nfusion, early fusion. And this really is\nthe core of what",
    "start": "1675950",
    "end": "1682553"
  },
  {
    "text": "I think a lot of people\nin the field right now or if you're interested\nin getting in this field or if you're going\nto go into industry",
    "start": "1682553",
    "end": "1687770"
  },
  {
    "text": "and you're going to\nbe using this stuff, this is what you should\nreally understand. And again, the ideas sort\nof stack onto each other.",
    "start": "1687770",
    "end": "1696480"
  },
  {
    "text": "So I've kind of\nsequenced the slides to give you an idea of\nhow the scientists came up",
    "start": "1696480",
    "end": "1701870"
  },
  {
    "text": "with the next step. And you can really see\nthe architecture just get slightly more\nand more advanced",
    "start": "1701870",
    "end": "1707057"
  },
  {
    "text": "but basically, a lot\nof it is just more data and more compute again. So who knows how BERT works?",
    "start": "1707057",
    "end": "1716168"
  },
  {
    "text": "[LAUGHTER] Everybody should raise\ntheir hands in this.",
    "start": "1716168",
    "end": "1721690"
  },
  {
    "text": "So yeah. So BERT is so canonical, I think\neverybody gets how BERT works.",
    "start": "1721690",
    "end": "1728670"
  },
  {
    "text": "So I don't think we\nneed a real refresher, but I think you can think-- and\nso the reason I have this slide",
    "start": "1728670",
    "end": "1735570"
  },
  {
    "text": "is because I want you to think\nabout if you have a BERT model and you have a\nbunch of images, how",
    "start": "1735570",
    "end": "1741900"
  },
  {
    "text": "are you going to turn that\nBERT model into something multimodal? So there are a bunch\nof obvious things",
    "start": "1741900",
    "end": "1749340"
  },
  {
    "text": "you could do given the kind\nof features I told you about and the fusion process. So how are you going to do that?",
    "start": "1749340",
    "end": "1756490"
  },
  {
    "text": "Does anybody want\nto say something? ",
    "start": "1756490",
    "end": "1764419"
  },
  {
    "text": "If you're doing\nclassification, you can take the models from there\nand then just concatenate it",
    "start": "1764420",
    "end": "1769950"
  },
  {
    "text": "to whatever encoder,\nmaybe an ANN or whatever you're training on the data\nconcatenating and training.",
    "start": "1769950",
    "end": "1777240"
  },
  {
    "text": "Yeah, exactly. So you can take the ConvNet\nfeatures and the classifier token from BERT, concatenate\nthem, and then classify",
    "start": "1777240",
    "end": "1785400"
  },
  {
    "text": "for cat or something like that\nor whatever the thing is you're interested in. So that's one thing.",
    "start": "1785400",
    "end": "1791160"
  },
  {
    "text": "You could also take\nthe ConvNet features and give them to the BERT model\nin lots of different ways.",
    "start": "1791160",
    "end": "1797820"
  },
  {
    "text": "We can use the region features. So I think a lot of\npeople when BERT came out",
    "start": "1797820",
    "end": "1804030"
  },
  {
    "text": "who were working in vision\nand language processing were thinking exactly about OK, so\ndo we do middle fusion, late",
    "start": "1804030",
    "end": "1809560"
  },
  {
    "text": "fusion? Do we do early fusion? How do we do the fusion? And so there were\na lot of papers",
    "start": "1809560",
    "end": "1815700"
  },
  {
    "text": "all coming out basically at\naround the same time where people were doing\nversions of this.",
    "start": "1815700",
    "end": "1821040"
  },
  {
    "text": "So BERT was really\nthe innovation and then everybody just\nplugged it into their own thing because of Hugging Face\ntransformers and things",
    "start": "1821040",
    "end": "1827800"
  },
  {
    "text": "like that. So the first thing\nis visual BERT.",
    "start": "1827800",
    "end": "1833320"
  },
  {
    "text": "This was one of the very early\nones where you have this image and people would do\nobject detection on this.",
    "start": "1833320",
    "end": "1839620"
  },
  {
    "text": "So you get like a hat and a\nracket and a shirt and things like that. So you can just really\ntake these features",
    "start": "1839620",
    "end": "1845170"
  },
  {
    "text": "and then plug them into\nyour transformer model and then you try to\nrecover the features.",
    "start": "1845170",
    "end": "1852620"
  },
  {
    "text": "And so this really is probably\nthe simplest way to do it. And so this is what we call\na single-stream architecture",
    "start": "1852620",
    "end": "1860889"
  },
  {
    "text": "where you have all of\nthese concatenating the original input features\nand then putting them",
    "start": "1860890",
    "end": "1865900"
  },
  {
    "text": "through the same transformer. What you can also do\nand that's something that this model\ncalled ViLBERT did",
    "start": "1865900",
    "end": "1872350"
  },
  {
    "text": "is where you have two\ndifferent streams. So you essentially have these\ntwo parallel transformers,",
    "start": "1872350",
    "end": "1878110"
  },
  {
    "text": "but at every layer,\nyou give them cross-attention, or\nco-attention as they call it.",
    "start": "1878110",
    "end": "1885080"
  },
  {
    "text": "But it's basically\nlike-- so you just make sure you have\nan attention map that spans both and\nthen you just do your full normal\ntransformer layer again.",
    "start": "1885080",
    "end": "1893830"
  },
  {
    "text": "So this, you can train just\nlike your regular BERT. So you have your masked\nlanguage model here",
    "start": "1893830",
    "end": "1902170"
  },
  {
    "text": "and here you do some\nequivalent of that. And then you also have your\nnext sentence prediction,",
    "start": "1902170",
    "end": "1907299"
  },
  {
    "text": "which you probably remember\nfrom your BERT lecture, but instead, here,\nwe're saying, OK,",
    "start": "1907300",
    "end": "1912710"
  },
  {
    "text": "is this image aligned with\nthis piece of text or not? There's also LXMERT.",
    "start": "1912710",
    "end": "1918920"
  },
  {
    "text": "I could go on forever. There are like 100 papers\nthat came out that did this all at the same time. So LXMERT had a different\ncross-modal output encoder,",
    "start": "1918920",
    "end": "1927010"
  },
  {
    "text": "a bunch of different\nways of encoding the positional information. So you could say,\nOK, I just have",
    "start": "1927010",
    "end": "1932200"
  },
  {
    "text": "a bunch of bounding\nboxes that are featurized but I don't care about\nwhere they are in the image. So it's just a bag\nof bounding boxes.",
    "start": "1932200",
    "end": "1940539"
  },
  {
    "text": "Or you could say,\nI found it here like this is the particular\ntop left and bottom right coordinate and that's what you\nfeaturize into your network.",
    "start": "1940540",
    "end": "1949100"
  },
  {
    "text": "You can also do\nsomething even dumber. And I can say that\nbecause this is my paper--",
    "start": "1949100",
    "end": "1954950"
  },
  {
    "text": "[LAUGHTER]  --where you just take\nthe image itself, you put it through a\nResNet, and then you",
    "start": "1954950",
    "end": "1961640"
  },
  {
    "text": "do a little bit of pooling\non the final feature maps and you just give those\nfeature maps to BERT.",
    "start": "1961640",
    "end": "1968150"
  },
  {
    "text": "And so you then need to\ndistinguish between your text segment embeddings, right, and\nyour vision segment embeddings.",
    "start": "1968150",
    "end": "1976730"
  },
  {
    "text": "So this actually works\nsurprisingly well. You don't have to do\nany additional training.",
    "start": "1976730",
    "end": "1982197"
  },
  {
    "text": "You can just take\nBERT out of the box. Initially, you freeze it. You learn to project\ninto BERT token space,",
    "start": "1982197",
    "end": "1987500"
  },
  {
    "text": "then you unfreeze\nyour ResNet, and then finally you unfreeze\nyour BERT, and now you have a very good\nmultimodal classifier",
    "start": "1987500",
    "end": "1993830"
  },
  {
    "text": "on the problem you care about. So a lot of these\nother papers, they're doing what they call multimodal\npretraining where first, you",
    "start": "1993830",
    "end": "2000550"
  },
  {
    "text": "have a BERT model and a ResNet. So they're unimodal pretrained. And then you couple\nthem together",
    "start": "2000550",
    "end": "2005770"
  },
  {
    "text": "and then you have\na multimodal sort of intermediary pretraining\nstep before you fine-tune it",
    "start": "2005770",
    "end": "2010840"
  },
  {
    "text": "on the problem you care about. And what we showed here is\nthat you don't really need that actually in many cases.",
    "start": "2010840",
    "end": "2016400"
  },
  {
    "text": "So that's a very\nstrong baseline. You can also go to the\npixel level completely.",
    "start": "2016400",
    "end": "2023080"
  },
  {
    "text": "So that's what they did\nin this other paper called PixelBERT where they-- it's\nbasically exactly MMBT.",
    "start": "2023080",
    "end": "2029770"
  },
  {
    "text": "So the previous supervised\none, but here they do the multimodal\npretraining step",
    "start": "2029770",
    "end": "2035110"
  },
  {
    "text": "and show that for VQA\nit helps a little bit. So there are many of these BERTs\ndoing sort of visual things.",
    "start": "2035110",
    "end": "2043929"
  },
  {
    "text": "People really tried everything. Here's another\none called UNITER, where they added a bunch\nof different losses.",
    "start": "2043930",
    "end": "2050679"
  },
  {
    "text": "We can really talk about\nthis for a very long time. We're not going to do that. I'm just going to\ntalk you through some",
    "start": "2050679",
    "end": "2056919"
  },
  {
    "text": "of the more interesting ones. So this one I think is\nquite interesting, ViLT, because here this is really\nthe first instance where",
    "start": "2056920",
    "end": "2063550"
  },
  {
    "text": "we are completely gone\nfrom ConvNet features. So we don't do\nany pre-processing",
    "start": "2063550",
    "end": "2069408"
  },
  {
    "text": "on the image, no region\nfeatures, no backbone, then it featurizes the parts\nof the image we care about.",
    "start": "2069409",
    "end": "2075559"
  },
  {
    "text": "We just have these\npatches of the image. So really in a grid. We flatten those patches. We just pumped them into the\ntransformer straight away.",
    "start": "2075560",
    "end": "2083060"
  },
  {
    "text": "So this really is sort of BERT\nand ViT together in one model and this worked\nreally very well.",
    "start": "2083060",
    "end": "2089699"
  },
  {
    "text": "So that's been the trend. So here's a nice\nvery long list of all",
    "start": "2089699",
    "end": "2095617"
  },
  {
    "text": "of these different\nmodels and what they do. And so really the\ndistinctions are just in what is the text\nencoder that you use.",
    "start": "2095618",
    "end": "2102110"
  },
  {
    "text": "So do you use BERT or something\nfancier or better, RoBERTa? What is your vision encoder?",
    "start": "2102110",
    "end": "2108350"
  },
  {
    "text": "So in many cases, you have\nthese region features. So you would do an\nR-CNN style thing,",
    "start": "2108350",
    "end": "2113450"
  },
  {
    "text": "or you could just do\na ResNet or a ViT. You have different\nkinds of fusion. So either single or dual\nstream, as we talked about.",
    "start": "2113450",
    "end": "2120500"
  },
  {
    "text": "So visual BERT or ViLBERT. Different pretraining tasks. So masked language modeling,\nimage text matching.",
    "start": "2120500",
    "end": "2128240"
  },
  {
    "text": "There's a bunch of\nfunkier ones you can do. And then finally, you can\ndo multimodal pretraining",
    "start": "2128240",
    "end": "2135320"
  },
  {
    "text": "on all of these different data\nsets that have aligned data. So you are probably\nwondering like, OK,",
    "start": "2135320",
    "end": "2141890"
  },
  {
    "text": "so what is really the\ninteresting difference between a lot of these? And so I have another\nrecommended paper",
    "start": "2141890",
    "end": "2149420"
  },
  {
    "text": "that if you're\ninterested in this space you should really\ntake a look at. This is also a really\nwell-done paper, where they unmasked\nmultimodal pretraining.",
    "start": "2149420",
    "end": "2159320"
  },
  {
    "text": "So basically, they say if you\ntake all of these little model inventions and you train these\ndifferent models on exactly",
    "start": "2159320",
    "end": "2167060"
  },
  {
    "text": "the same data in\nexactly the same way, it turns out that they're\nall basically the same. [LAUGHTER]",
    "start": "2167060",
    "end": "2172660"
  },
  {
    "text": " So that's a lot of wasted\neffort on the part of the field because everybody is saying,\noh, my model is better,",
    "start": "2172660",
    "end": "2179450"
  },
  {
    "text": "but it's actually just\nbecause you trained it on different data and there's\nno real model innovation going",
    "start": "2179450",
    "end": "2186260"
  },
  {
    "text": "on in a lot of these things. So I don't mean to sound\ndiscouraging or anything like that, but I think\nthat's why this paper is",
    "start": "2186260",
    "end": "2193880"
  },
  {
    "text": "really nice and really\nimportant is because it just shows us what really matters.",
    "start": "2193880",
    "end": "2199750"
  },
  {
    "text": "So this is also work that\nI did myself called FLAVA",
    "start": "2199750",
    "end": "2205070"
  },
  {
    "text": "with my team, where we wanted\nto take these ideas really to the limit. So a lot of the things\nthat you've seen now,",
    "start": "2205070",
    "end": "2212700"
  },
  {
    "text": "so the visual BERTs and\nthe FILBERTs and things like that, they're all\nabout multimodal questions. So how can we do visual\nquestion answering,",
    "start": "2212700",
    "end": "2219540"
  },
  {
    "text": "something like that, where we\njust have these two modalities. We only care about problems\nthat always involve these two",
    "start": "2219540",
    "end": "2224670"
  },
  {
    "text": "modalities. And where we want\nto go, and this is the basic premise I think of\nfoundation models in general,",
    "start": "2224670",
    "end": "2231510"
  },
  {
    "text": "is that we have one\nmodel to rule them all. So this one model\ncan consume data from all of these\ndifferent modalities",
    "start": "2231510",
    "end": "2237730"
  },
  {
    "text": "and it can synthesize across all\nof these different modalities and then do useful things\nwith that information.",
    "start": "2237730",
    "end": "2244560"
  },
  {
    "text": "So with FLAVA, that's\nexactly what we try to build. So we wanted to have one\nfoundation model that",
    "start": "2244560",
    "end": "2249630"
  },
  {
    "text": "is good at vision and\nlanguage, and computer vision and natural language\nprocessing, is jointly pretrained on all of these\ndifferent data sources.",
    "start": "2249630",
    "end": "2256810"
  },
  {
    "text": "So it's also trained on\njust CCNews, Common Crawl, and BookCorpus. So it's very good\nat things you would",
    "start": "2256810",
    "end": "2263490"
  },
  {
    "text": "expect BERT to be good at. It's trained on\nImageNet for image data, so it's good at the things that\nyou would expect a basic image",
    "start": "2263490",
    "end": "2270600"
  },
  {
    "text": "model to be good at. And then you have this PMD\ndata set that we created out of publicly available image text\npairs that we also trained it",
    "start": "2270600",
    "end": "2279410"
  },
  {
    "text": "on. So this PMD data\nset is really just-- if you take all the\ndata sets that were ever created that have\nimage text pairs that",
    "start": "2279410",
    "end": "2285630"
  },
  {
    "text": "are publicly available. So unfortunately, the CLIP\ndata and the Google align data and all of these data sets,\nthey haven't been open source.",
    "start": "2285630",
    "end": "2292200"
  },
  {
    "text": "So this is before LAION. So now there's a good\nalternative to this.",
    "start": "2292200",
    "end": "2297480"
  },
  {
    "text": "But so this PMD data set, if you\ncombine all of these image text",
    "start": "2297480",
    "end": "2302580"
  },
  {
    "text": "pairs, you get 70\nmillion of them. So that's still a\npretty decent size. And then you can take all\nof this data basically",
    "start": "2302580",
    "end": "2309053"
  },
  {
    "text": "to solve all of these\nproblems that we know and we care about in\nthese different fields. So you can do\nmultimodal reasoning,",
    "start": "2309053",
    "end": "2314160"
  },
  {
    "text": "you can do language\nunderstanding, you can do visual\nrecognition all with exactly the same model.",
    "start": "2314160",
    "end": "2319170"
  },
  {
    "text": "And that's a very powerful idea. I think if you work at\na company like Facebook you don't want to have\ndifferent models for all kinds",
    "start": "2319170",
    "end": "2326320"
  },
  {
    "text": "of different things, you\nwant to have one model that you can really\nuse for everything that's going to really make\nyour life a lot easier.",
    "start": "2326320",
    "end": "2333980"
  },
  {
    "text": "So the exact architecture\nhere is that on the one hand, we have this image encoder\nwhere we take the image,",
    "start": "2333980",
    "end": "2340450"
  },
  {
    "text": "we encode it as\npatches, and we just do what we call\nmasked image modeling, but it's basically masked\nlanguage modeling just",
    "start": "2340450",
    "end": "2347380"
  },
  {
    "text": "on the image tokens. And then on the other side,\nwe have the masked language",
    "start": "2347380",
    "end": "2353830"
  },
  {
    "text": "modeling on the language. So your regular\nsort of BERT thing. And then we have\na multimodal part",
    "start": "2353830",
    "end": "2360310"
  },
  {
    "text": "where all of this\ninformation gets combined. So we have a masked\nmultimodal modeling loss",
    "start": "2360310",
    "end": "2366370"
  },
  {
    "text": "term where you can also\ndo image text matching. So this is like your BERT next\nsentence prediction thing.",
    "start": "2366370",
    "end": "2371540"
  },
  {
    "text": "And then we also have a\nglobal contrastive loss, which is exactly like a CLIP. So if you do all\nof this stuff, it's",
    "start": "2371540",
    "end": "2377350"
  },
  {
    "text": "just all transformers\nall the way down. It's sort of a very elegant\nway I think to combine",
    "start": "2377350",
    "end": "2382869"
  },
  {
    "text": "a lot of this information. And when you do that, you\nget something that can really do a lot of things very well.",
    "start": "2382870",
    "end": "2389180"
  },
  {
    "text": "So we're not going to\ntalk about that table, it's just way too many numbers. So just trust me we were\npretty thorough in generating",
    "start": "2389180",
    "end": "2396940"
  },
  {
    "text": "the table here. [LAUGHTER]  So over 35 different\ntasks if you compare FLAVA",
    "start": "2396940",
    "end": "2402670"
  },
  {
    "text": "to all kinds of different\nablations in terms of CLIP models, then this is\njust a much better way",
    "start": "2402670",
    "end": "2408099"
  },
  {
    "text": "to get to this information. So I think this is a nice\nexample of where we're probably going to go with the\nfield in the near future.",
    "start": "2408100",
    "end": "2417290"
  },
  {
    "text": "So the other trend that we see\nvery obviously in the field right now is that everybody\ncares about generative models,",
    "start": "2417290",
    "end": "2423032"
  },
  {
    "text": "right? So language models and\nimage generative models. There's just a trend where\nwe want to be generative.",
    "start": "2423032",
    "end": "2430030"
  },
  {
    "text": "We want to move away from this\ncontrastive discriminative stuff to the more interesting,\nmore richer representations",
    "start": "2430030",
    "end": "2436270"
  },
  {
    "text": "maybe that you get out of\ngenerating sequences or images. So this SimVLM paper was\none of the first ones where",
    "start": "2436270",
    "end": "2444370"
  },
  {
    "text": "they really had this\nseparate decoder that was trying to\ngenerate or complete captions, which they\nshowed gives you",
    "start": "2444370",
    "end": "2450910"
  },
  {
    "text": "a lot richer representations. I think this is actually the\ncurrent state of the art now. It's called CoCa.",
    "start": "2450910",
    "end": "2457810"
  },
  {
    "text": "So a lot of these models. They all again look very\nsimilar, but in this case,",
    "start": "2457810",
    "end": "2462880"
  },
  {
    "text": "now we're starting to really\nsee these text decoders. So initially, with CLIP,\nI think that's also what they were trying to\ngo for, like OpenAI being",
    "start": "2462880",
    "end": "2470380"
  },
  {
    "text": "a company that really\nlikes generative models, but they couldn't\nreally get it to work. And I think it took\nus a while as a field",
    "start": "2470380",
    "end": "2476559"
  },
  {
    "text": "to really figure out how\nto do this the right way. And so right now we're really\nin the age of language models,",
    "start": "2476560",
    "end": "2484637"
  },
  {
    "text": "right? So one of the interesting things\nyou can do with language models",
    "start": "2484637",
    "end": "2490090"
  },
  {
    "text": "is just keep them\nfrozen and then learn how to project into\nthe language models. So the MMBT\narchitecture I talked",
    "start": "2490090",
    "end": "2497859"
  },
  {
    "text": "about where we had this BERT\nmodel, we kept it frozen, and we learn to project\ninto the BERT token space.",
    "start": "2497860",
    "end": "2504580"
  },
  {
    "text": "You can do exactly\nthe same thing but then with a much fancier\nmodel or something like T5",
    "start": "2504580",
    "end": "2510220"
  },
  {
    "text": "even where you just have an\nencoder-decoder or some kind of generative part of this. You keep that thing\nfrozen, and then you",
    "start": "2510220",
    "end": "2517330"
  },
  {
    "text": "learn to project into the token\nspace of that frozen language model, and then you can do\nlots of fun stuff it turns out.",
    "start": "2517330",
    "end": "2524930"
  },
  {
    "text": "So what they show in this\npaper is that you then get few-shot learners. So all of the things\nyou see with GPT-3",
    "start": "2524930",
    "end": "2531040"
  },
  {
    "text": "where you can just give it\nsome in-context examples and it's going to figure\nout binding on the fly.",
    "start": "2531040",
    "end": "2538240"
  },
  {
    "text": "So it says like this is a\ndax and this is a blicket. So what is this? And then it gives you the\nanswer that it's a dax.",
    "start": "2538240",
    "end": "2545330"
  },
  {
    "text": "So it really learns\nin context how you decide the feature\nmappings, which is really",
    "start": "2545330",
    "end": "2550610"
  },
  {
    "text": "solving the grounding\nproblem that a lot of this multimodal\nstuff started with.",
    "start": "2550610",
    "end": "2555930"
  },
  {
    "text": "So I think that's very cool. Then probably one of the coolest\npapers right now or models",
    "start": "2555930",
    "end": "2562232"
  },
  {
    "text": "right now that you\nmight have heard of if you follow the field\nis Flamingo, out of DeepMind,",
    "start": "2562232",
    "end": "2567290"
  },
  {
    "text": "where they take a\nChinchilla language model. So this is really an\noptimal language model.",
    "start": "2567290",
    "end": "2573650"
  },
  {
    "text": "And now you have\nthis vision encoder that encodes multiple\ndifferent images",
    "start": "2573650",
    "end": "2579260"
  },
  {
    "text": "that you can then do reasoning\nover and then autocomplete. So what this gets you is just\na much more powerful model",
    "start": "2579260",
    "end": "2586849"
  },
  {
    "text": "because you can\ndo your generative over lots of different images. So it's really like step-wise.",
    "start": "2586850",
    "end": "2592700"
  },
  {
    "text": "You can see it. We started off with\nvery simple transformers and now we're\nactually at something that is starting to get pretty\ncomplicated because we have",
    "start": "2592700",
    "end": "2600320"
  },
  {
    "text": "these building blocks\nlike a perceiver resampler where we have a bunch\nof different images",
    "start": "2600320",
    "end": "2605900"
  },
  {
    "text": "that we featurized\nand now we need to compress the information\nbecause sometimes we have three images, sometimes\nwe have five images",
    "start": "2605900",
    "end": "2612329"
  },
  {
    "text": "so we want to make sure that\nwe can compress it so that it's always ready for consumption by\nthe next layer of the language",
    "start": "2612330",
    "end": "2619160"
  },
  {
    "text": "model. So this paper again\nis a really good paper to read because they\nactually-- so this is not me.",
    "start": "2619160",
    "end": "2626007"
  },
  {
    "text": "This is not my code. This comes from\nthe actual paper. So they just have the diagram\ntogether with the code so that you can really\nunderstand what it's doing,",
    "start": "2626007",
    "end": "2632910"
  },
  {
    "text": "which I think is really great. And so once you have\nyour perceiver resampling",
    "start": "2632910",
    "end": "2640130"
  },
  {
    "text": "step, what you then do is\nyou do gated cross-attention. This is how you implement it.",
    "start": "2640130",
    "end": "2646109"
  },
  {
    "text": "And so this gated\ncross-attention you do that before your\nfrozen language model layer.",
    "start": "2646110",
    "end": "2652100"
  },
  {
    "text": "So you really just have a\nfrozen Chinchilla language model and you learn to modulate\nthe information that",
    "start": "2652100",
    "end": "2658039"
  },
  {
    "text": "goes into that language model. You propagate the\ngradients all the way back, you just don't update\nthe language model.",
    "start": "2658040",
    "end": "2664100"
  },
  {
    "text": "So you're really trying\nto figure out like, how am I going to\ndesign my signal so that my language model can\ndo the most with it, right?",
    "start": "2664100",
    "end": "2671060"
  },
  {
    "text": "How am I going to\ncombine the information? So you'll notice that now\nwe do it before the layer. And a lot of this\nother stuff you",
    "start": "2671060",
    "end": "2676670"
  },
  {
    "text": "would do the attention\nafter the layer, but here you do it before. So Karpathy, I think, more than\n10 years ago had this image.",
    "start": "2676670",
    "end": "2685820"
  },
  {
    "text": "It's Barack Obama\nsetting his foot here on the scale to\nmake somebody think",
    "start": "2685820",
    "end": "2691720"
  },
  {
    "text": "they're a lot heavier\nthan they really are. So this is obviously funny to\nus, but not to an AI system,",
    "start": "2691720",
    "end": "2699040"
  },
  {
    "text": "I think, unless it really\nunderstands the scene. And so that's why\nKarpathy at the time",
    "start": "2699040",
    "end": "2704400"
  },
  {
    "text": "said this would be a really\ngood visual Turing test. If a system can figure\nthis out, then it's actually really smart.",
    "start": "2704400",
    "end": "2710700"
  },
  {
    "text": "And so obviously,\nit's been a bit of a challenge for everybody\nworking in the field than to get something that\nactually works on this.",
    "start": "2710700",
    "end": "2716380"
  },
  {
    "text": "And so Flamingo, as it turns\nout, kind of gets the joke. But yeah, so it's a bit\nunclear if it really",
    "start": "2716380",
    "end": "2723930"
  },
  {
    "text": "gets the joke because if\nyou read this conversation, it's sort of getting steered\nin the right direction, right? But at least we're\nmaking progress,",
    "start": "2723930",
    "end": "2731130"
  },
  {
    "text": "let's put it that way. And then so in\nFlamingo, you still",
    "start": "2731130",
    "end": "2736383"
  },
  {
    "text": "have a lot of moving\nparts, but you can really take this almost\nto the full extreme where you try to freeze\nalmost everything.",
    "start": "2736383",
    "end": "2742150"
  },
  {
    "text": "And you just want to\nlearn this kind of mapping between your image encoder\nand your language model,",
    "start": "2742150",
    "end": "2747250"
  },
  {
    "text": "or your image encoder and your\nencoder-decoder architecture, and all you really do is just\nthe projection between the two,",
    "start": "2747250",
    "end": "2753505"
  },
  {
    "text": "right? So there's this\nnice model called BLIP2, where they experiment\nwith OPT for the language",
    "start": "2753505",
    "end": "2759760"
  },
  {
    "text": "model and FlanT5 for the\nencoder-decoder architecture. And this just gives\nyou amazing results.",
    "start": "2759760",
    "end": "2765010"
  },
  {
    "text": "It gives you really complex\ncaptions and things like that without any real direct\nsupervision on the captions",
    "start": "2765010",
    "end": "2771880"
  },
  {
    "text": "itself, which is pretty\nimpressive, I think. So that just shows you the power\nof language models in general.",
    "start": "2771880",
    "end": "2779279"
  },
  {
    "text": "So here are some examples. So it can really\ndo different things from captioning to reasoning\nto visual question answering",
    "start": "2779280",
    "end": "2786630"
  },
  {
    "text": "to location detection. So you can have a long\nconversation with this system.",
    "start": "2786630",
    "end": "2792570"
  },
  {
    "text": "This really is the\nfuture of where we're going, where we're\ngoing to have a ChatGPT but it's also going to be able\nto see the world in a way.",
    "start": "2792570",
    "end": "2800820"
  },
  {
    "text": "And so I think an\ninteresting thing. So you've probably heard of\nchain of thought prompting",
    "start": "2800820",
    "end": "2805853"
  },
  {
    "text": "and things like\nthat where you ask the language model like\nlet's think step by step. And you can tell a vision\nand language model,",
    "start": "2805853",
    "end": "2813870"
  },
  {
    "text": "generate a rationale for why\nsomething might be the case. So you generate a\npotential explanation",
    "start": "2813870",
    "end": "2821130"
  },
  {
    "text": "for what your answer might be. And then after that, you ask\nit to answer the question. And it turns out that if\nyou do that multimodal chain",
    "start": "2821130",
    "end": "2828990"
  },
  {
    "text": "of thought prompting, then\nthe system gets much better. And so this is the new state\nof the art on ScienceQA",
    "start": "2828990",
    "end": "2835980"
  },
  {
    "text": "or benchmarks like that just\nbecause it learns to unpack the information, right? And so I think we're really\nas a field just starting",
    "start": "2835980",
    "end": "2844119"
  },
  {
    "text": "to figure out what the\npotential is of this. And I think this paper\nis where they also show that multimodal chain\nof thought prompting really",
    "start": "2844120",
    "end": "2851440"
  },
  {
    "text": "gets you pretty amazing results. And they show very nice\nresults on Raven matrices",
    "start": "2851440",
    "end": "2857109"
  },
  {
    "text": "and very complicated\nIQ tests sort of things that humans are\nsupposed to be really",
    "start": "2857110",
    "end": "2862982"
  },
  {
    "text": "good at but you have to\nbe a pretty smart human to really be good at this\nand the system just nails it.",
    "start": "2862982",
    "end": "2869290"
  },
  {
    "text": "So we're making\nsuper fast progress. We started off from\na very simple BERT model that was able to\nlook at some pictures",
    "start": "2869290",
    "end": "2876470"
  },
  {
    "text": "and now we're getting to these\nvery sophisticated foundation models. So that was my short history of\nmultimodal foundation models.",
    "start": "2876470",
    "end": "2886080"
  },
  {
    "text": "So how much time do I have left? So after 5:50. 25 minutes.",
    "start": "2886080",
    "end": "2891190"
  },
  {
    "text": "All right. OK. Plenty of time. We have some questions. Yeah, please questions.",
    "start": "2891190",
    "end": "2896980"
  },
  {
    "text": " Do we really do\nmuch pre-processing of images to these\nmodels anymore?",
    "start": "2896980",
    "end": "2903928"
  },
  {
    "text": "So I noticed a lot\nof the images that just looked like\nthey were boxes, like square images\npassed through, kind",
    "start": "2903928",
    "end": "2910599"
  },
  {
    "text": "of no sense of shape in them. Yeah. So I think the history\nof computer vision",
    "start": "2910600",
    "end": "2918250"
  },
  {
    "text": "has been very similar to the\nhistory of natural language processing where we thought we\nneeded all of this structure and all of these\ndifferent things.",
    "start": "2918250",
    "end": "2924650"
  },
  {
    "text": "And it turns out you can\njust throw it all away and just have a big\ntransformer over the patches.",
    "start": "2924650",
    "end": "2930730"
  },
  {
    "text": "Sorry, yes. [LAUGHTER] It's CS231 in 1 minute. Save you time.",
    "start": "2930730",
    "end": "2935830"
  },
  {
    "text": "[LAUGHTER]  You mentioned a couple of\ntimes like models being frozen.",
    "start": "2935830",
    "end": "2942550"
  },
  {
    "text": "What does that mean? Yeah. Sorry, I should have\nexplained that better, maybe. So it just means that we are\nnot updating the weights.",
    "start": "2942550",
    "end": "2951070"
  },
  {
    "text": "So if we go to this area\nI think is a nice example. So we have frozen\nself-attention.",
    "start": "2951070",
    "end": "2959349"
  },
  {
    "text": "So that just means that\nwhen we do a forward pass, we go all the way to\nwhatever we want to predict.",
    "start": "2959350",
    "end": "2964579"
  },
  {
    "text": "We get some gradients, we\ntake them all the way down, but we only update the\nnon-frozen layers, right?",
    "start": "2964580",
    "end": "2970760"
  },
  {
    "text": "So here the\ngradients actually do get updated but these\njust never change. And so the reason you\nwant to do that is",
    "start": "2970760",
    "end": "2976430"
  },
  {
    "text": "because, otherwise, you're\ngoing to drift way too far. So then you're\ngoing to destroy all",
    "start": "2976430",
    "end": "2981859"
  },
  {
    "text": "of the cool stuff your\nlanguage model has learned because you're just going to\nfocus on this small data set that you're training it on.",
    "start": "2981860",
    "end": "2988230"
  },
  {
    "text": "So you want to preserve the\nabilities of the language model, but you want\nit to become good at the thing you care about.",
    "start": "2988230",
    "end": "2993440"
  },
  {
    "start": "2993440",
    "end": "2998869"
  },
  {
    "text": "Other questions. In terms of multimodal\nfusion, is there a benefit to doing that earlier\nmiddle fusion as opposed",
    "start": "2998870",
    "end": "3005829"
  },
  {
    "text": "to only doing the late fusion? Yeah. So we're going to talk\nabout evaluation next.",
    "start": "3005830",
    "end": "3012070"
  },
  {
    "text": "So it really depends on the\ntask that you care about. And so I would say the\nearlier is always the better",
    "start": "3012070",
    "end": "3019150"
  },
  {
    "text": "if you can afford it. And so CLIP is very\nefficient to train. It's very late fusion,\nright, at the very end.",
    "start": "3019150",
    "end": "3025820"
  },
  {
    "text": "So there's no\ninteraction between the different modalities. So that's really good if you\nwant to be very efficient",
    "start": "3025820",
    "end": "3033259"
  },
  {
    "text": "and if you want to be-- for\ntraining, it's much nicer. But if you want to have\na richer understanding",
    "start": "3033260",
    "end": "3039579"
  },
  {
    "text": "of the multimodal signal, then\nyou want to do earlier fusion. So yeah, there's\nalways a trade-off.",
    "start": "3039580",
    "end": "3046713"
  },
  {
    "text": " So it seems like images are\njust a lot more data than text.",
    "start": "3046713",
    "end": "3054780"
  },
  {
    "text": "So how much more difficult\nare these to train, and how much bigger does\nthe image processing",
    "start": "3054780",
    "end": "3062369"
  },
  {
    "text": "have to be compared\nto the language model? Yeah. So images are more\ncomplex in a way,",
    "start": "3062370",
    "end": "3070890"
  },
  {
    "text": "but they're also higher\nbandwidth representations. So there's a lot of just\npixels that our brains just",
    "start": "3070890",
    "end": "3078540"
  },
  {
    "text": "abstract away. It's really about the\nscene that you're seeing and you're not really\nthinking too much",
    "start": "3078540",
    "end": "3083970"
  },
  {
    "text": "about the pixels themselves. So like Yann LeCun likes to\nsay that language is just",
    "start": "3083970",
    "end": "3090630"
  },
  {
    "text": "a low bandwidth, a proxy\nfor a language of thought,",
    "start": "3090630",
    "end": "3095710"
  },
  {
    "text": "which is much richer and\nmuch higher bandwidth and he thinks probably\nvisual, I'm not so sure.",
    "start": "3095710",
    "end": "3101910"
  },
  {
    "text": "But so, yeah. I don't think that\nthere's necessarily a difference between\nthe scaling laws",
    "start": "3101910",
    "end": "3108690"
  },
  {
    "text": "that you see in these\nsystems, or at least we still have to\nfigure that out.",
    "start": "3108690",
    "end": "3114160"
  },
  {
    "text": "We'll talk about that\ntowards the end as well. Can these models also have\ncertain social and cultural",
    "start": "3114160",
    "end": "3122680"
  },
  {
    "text": "biases just like the\nnatural language inference? Oh yeah, they have\nterrible biases. Yeah.",
    "start": "3122680",
    "end": "3127710"
  },
  {
    "text": "[LAUGHTER]  So yeah. So some people are\nactually working on this who are in this very room.",
    "start": "3127710",
    "end": "3133900"
  },
  {
    "text": "So these models can\nbe very racist also in what they generate or the\nkind of predictions they make.",
    "start": "3133900",
    "end": "3140210"
  },
  {
    "text": "So if you have an Asian\nbasketball player standing like this with a basketball\nvery obviously there,",
    "start": "3140210",
    "end": "3147202"
  },
  {
    "text": "then the model will think\nthat he's playing ping pong because he's Asian. [LAUGHTER] I'm not joking.",
    "start": "3147202",
    "end": "3152800"
  },
  {
    "text": "[LAUGHTER]  So these models-- yeah, just\nlike all neural networks,",
    "start": "3152800",
    "end": "3158770"
  },
  {
    "text": "right, this is\nreally a big problem. And one of the most\ninteresting problems that you should be working\non if you're a student",
    "start": "3158770",
    "end": "3164650"
  },
  {
    "text": "and you want to\nmake a difference is how do we get these\nsystems to be much better at these sorts of things.",
    "start": "3164650",
    "end": "3169930"
  },
  {
    "text": " So in one of the examples you\nshowed that the model interpret",
    "start": "3169930",
    "end": "3176440"
  },
  {
    "text": "from the content of an image. So if we want to understand\nthe content of a video, so what are challenges you\nmight see along this path,",
    "start": "3176440",
    "end": "3184720"
  },
  {
    "text": "and what improvements we\ncan make towards this goal? Yeah. So you're asking about the\nattention mask sort of, right?",
    "start": "3184720",
    "end": "3193630"
  },
  {
    "text": "So you can use the\nsame idea for videos and you just look at the video. And so these systems\nare so good now,",
    "start": "3193630",
    "end": "3200320"
  },
  {
    "text": "the object detectors\nare so good, you can really track\nobjects kind of real-time as they go through\nyour video, and so you",
    "start": "3200320",
    "end": "3207640"
  },
  {
    "text": "can try to check how that\naligns with your attention mask in your model. ",
    "start": "3207640",
    "end": "3215140"
  },
  {
    "text": "So videos I think\nare interesting, but they're also not really\ninteresting because you can very often just\nsubsample images and solve",
    "start": "3215140",
    "end": "3222640"
  },
  {
    "text": "the images rather than having\nto deal with the complex video. But yeah.",
    "start": "3222640",
    "end": "3228770"
  },
  {
    "text": "All right. Maybe one more question and then\nwe'll go do some evaluation. So these multimodal\nmodels when you only",
    "start": "3228770",
    "end": "3236710"
  },
  {
    "text": "provide-- let's say\nyou only provide a single source of media,\nso say only text or vision, how does it perform\nin that case?",
    "start": "3236710",
    "end": "3243220"
  },
  {
    "text": "Because obviously, it's more\ngeared for multimodal cases. Yeah. So that's one of the\ngiant shortcomings",
    "start": "3243220",
    "end": "3249220"
  },
  {
    "text": "of a lot of these models is\nthat they're really just built for multimodal stuff,\nand so what if I",
    "start": "3249220",
    "end": "3254680"
  },
  {
    "text": "don't have an image, right? And so that's why we\ndid FLAVA because we",
    "start": "3254680",
    "end": "3260710"
  },
  {
    "text": "want to have one model that\ncan do all of that stuff. And that's why in MMBT, so\nthe supervised multimodal",
    "start": "3260710",
    "end": "3267460"
  },
  {
    "text": "by transformer, we\nactually have an analysis of how robust is this model to\nmissing images or missing text.",
    "start": "3267460",
    "end": "3274869"
  },
  {
    "text": "So I think a lot\nof folks working on these early visual BERT\nmodels that were myopically",
    "start": "3274870",
    "end": "3280510"
  },
  {
    "text": "focused on VQA,\nwhich is actually a great segue to what I\nwant to talk about next.",
    "start": "3280510",
    "end": "3286480"
  },
  {
    "text": "So it really depends on the task\nthat you care about as I said. And so I think if I'm going to\ntell you about multimodality,",
    "start": "3286480",
    "end": "3295423"
  },
  {
    "text": "I also have to\ntell you how you're going to check that the\nmultimodal system is actually good at multimodal things.",
    "start": "3295423",
    "end": "3300690"
  },
  {
    "text": "And so that's the topic of\nevaluation, which actually is a super important topic.",
    "start": "3300690",
    "end": "3307160"
  },
  {
    "text": "And a lot of people they want\nto be cool and build big models, but I think it\nshould be way cooler",
    "start": "3307160",
    "end": "3312349"
  },
  {
    "text": "to do a proper evaluation\nof these models, especially if you're in\nacademia because you only have limited GPUs anyway.",
    "start": "3312350",
    "end": "3318770"
  },
  {
    "text": "So what can you do? [LAUGHTER] Sorry. I don't want to rub it in, but--",
    "start": "3318770",
    "end": "3324260"
  },
  {
    "text": "[LAUGHTER]  So how do you check?",
    "start": "3324260",
    "end": "3329300"
  },
  {
    "text": "Well, there's this\namazing project. So ImageNet really changed\nthe history of deep learning,",
    "start": "3329300",
    "end": "3336329"
  },
  {
    "text": "I think. And this other data set CoCo,\nI think, also really changed, especially vision and language,\nbut also I think vision,",
    "start": "3336330",
    "end": "3344089"
  },
  {
    "text": "in general, where they have\njust a bunch of main sort",
    "start": "3344090",
    "end": "3349121"
  },
  {
    "text": "of multimodal tasks. So these images are\nvery richly annotated with all kinds of\ndifferent things.",
    "start": "3349121",
    "end": "3354840"
  },
  {
    "text": "So like the segmentation of the\nobjects, the bounding boxes, the labels of the\nbounding boxes they",
    "start": "3354840",
    "end": "3360380"
  },
  {
    "text": "come at different\npixel granularities. It's a huge data set.",
    "start": "3360380",
    "end": "3365720"
  },
  {
    "text": "It's very fine-grained annotated\nin terms of the categories that it has, and then\nyou have five captions",
    "start": "3365720",
    "end": "3372170"
  },
  {
    "text": "for each of these images. And so this really\nwas the first data set that unlocked a lot of\nsort of vision and language",
    "start": "3372170",
    "end": "3379400"
  },
  {
    "text": "processing at scale because\nyou had your picture and you had your\ncaption and now you need to figure out, OK, how\ndo I give the right caption",
    "start": "3379400",
    "end": "3386180"
  },
  {
    "text": "for this image? So that's image captioning. Or can I retrieve given\nsome piece of text the right image or the\nimage for the piece of text?",
    "start": "3386180",
    "end": "3394890"
  },
  {
    "text": "So there's a bunch of\nvery impactful data sets that do this stuff. We already talked about\nLAION, with CoCo really",
    "start": "3394890",
    "end": "3401450"
  },
  {
    "text": "is the main one still I\nthink that a lot of people use as the canonical instance\nof this data set category.",
    "start": "3401450",
    "end": "3408920"
  },
  {
    "text": "And then the other thing\nthat people really care about in vision and\nlanguage processing is visual question answering.",
    "start": "3408920",
    "end": "3416420"
  },
  {
    "text": "And so there really are a\nbunch of academic groups who are or have been\nso focused on this task",
    "start": "3416420",
    "end": "3423769"
  },
  {
    "text": "that they didn't really\ncare about anything else. And that's why you\nsee a lot of models that are really optimized just\nfor multimodal and nothing",
    "start": "3423770",
    "end": "3430970"
  },
  {
    "text": "else. And you can see that\nreflected in the citation counts as of last\nnight at 3:00 AM.",
    "start": "3430970",
    "end": "3438560"
  },
  {
    "text": "So VQA just has way more\ncitations than image captioning data sets even, right?",
    "start": "3438560",
    "end": "3444320"
  },
  {
    "text": "And so what you do here\nis you just have an image and then people ask\nvery simple questions.",
    "start": "3444320",
    "end": "3450210"
  },
  {
    "text": "So annotators, they ask\nthese simple questions, they give the\nanswers, and now we",
    "start": "3450210",
    "end": "3455240"
  },
  {
    "text": "want to be able to answer\nthese questions with machines. And as I alluded to earlier, one\nof the embarrassing backstories",
    "start": "3455240",
    "end": "3462350"
  },
  {
    "text": "of this data set was that the\ninitial version of the data set was actually\nfound to have images",
    "start": "3462350",
    "end": "3468920"
  },
  {
    "text": "not really matter at all. So you could just\nlook at the question and it could have something\nlike, how many slices of pizza",
    "start": "3468920",
    "end": "3475520"
  },
  {
    "text": "are there? Well, not in that\nparticular case, but in almost all\nof the data set",
    "start": "3475520",
    "end": "3481250"
  },
  {
    "text": "the right answer for how much\nor how many question was 2. So if you just predicted\n2 to every how much",
    "start": "3481250",
    "end": "3487790"
  },
  {
    "text": "or how many questions,\nyou got 70% accuracy on the counting category. So careful data set or\nevaluation benchmark design",
    "start": "3487790",
    "end": "3496609"
  },
  {
    "text": "is also really a\nskill and you really need to think about\nwhat you're doing. You can't just set some data\naside and evaluate it on,",
    "start": "3496610",
    "end": "3503180"
  },
  {
    "text": "you have to really think\nabout what you're doing. And so there's VQA\nby Chris actually, which is also just I think\na better-designed version",
    "start": "3503180",
    "end": "3511580"
  },
  {
    "text": "of this data set maybe. So you might want to\nuse that these days. There are also kind of\nvery targeted data sets",
    "start": "3511580",
    "end": "3520700"
  },
  {
    "text": "that really try to measure\none particular thing. And I think one of\nthe things we really want to get at with\nthese models is what",
    "start": "3520700",
    "end": "3527690"
  },
  {
    "text": "we would call compositionality. So we want to be\nable to really take the parts and reason\nabout the whole",
    "start": "3527690",
    "end": "3533460"
  },
  {
    "text": "and understand the relationships\nbetween the different concepts. So CLEVR was a very\nclever data set",
    "start": "3533460",
    "end": "3539040"
  },
  {
    "text": "that was designed really to\nmeasure the compositionality, both on the language side\nand on the vision side.",
    "start": "3539040",
    "end": "3544958"
  },
  {
    "text": "So you have to understand\nthe relationships between all of these different\nobjects in the images. So that's been a\npretty impactful data",
    "start": "3544958",
    "end": "3551385"
  },
  {
    "text": "set I think for really\nforcing people to think about compositionality. But a lot of these data sets\nreally had big problems.",
    "start": "3551385",
    "end": "3561510"
  },
  {
    "text": "So one of the problems\nis they were too easy. So VQA is plateauing out.",
    "start": "3561510",
    "end": "3566911"
  },
  {
    "text": "We can talk about that\na little bit, too. It wasn't really realistic,\nso you could solve VQA and that's probably going\nto make some people's",
    "start": "3566912",
    "end": "3573480"
  },
  {
    "text": "lives better. You're all trying to\nprocess the means. I can see everybody. [LAUGHTER]",
    "start": "3573480",
    "end": "3578770"
  },
  {
    "text": "OK. Let's get to the\nmemes first then. So obviously, these memes are\nnot actually in the data set.",
    "start": "3578770",
    "end": "3586350"
  },
  {
    "text": "So I could put some\nreally hateful memes about sort of Hitler or\nsomething which are in the data",
    "start": "3586350",
    "end": "3591745"
  },
  {
    "text": "set but that would be less fun. So these are mean meme\nexamples to demonstrate how",
    "start": "3591745",
    "end": "3600430"
  },
  {
    "text": "the data set was constructed. And so one of the problems\nwe had as I said like VQA,",
    "start": "3600430",
    "end": "3605730"
  },
  {
    "text": "the V didn't really matter. What we want to have\nis the data set. If we care about multimodality\nspecifically, it's like,",
    "start": "3605730",
    "end": "3612280"
  },
  {
    "text": "how do we get a data\nset that you can only get right if you are good\nat multimodal reasoning?",
    "start": "3612280",
    "end": "3617619"
  },
  {
    "text": "And otherwise, you're\njust going to screw it up. And so this is what\nwe came up with. If you have a meme like\nthis one, love the way",
    "start": "3617620",
    "end": "3624400"
  },
  {
    "text": "you smell today, I mean,\nthat's not very nice if you send this to your friend. [LAUGHTER] ",
    "start": "3624400",
    "end": "3631360"
  },
  {
    "text": "So it turns out that if you just\nswap out the background, now it's a very nice thing to say.",
    "start": "3631360",
    "end": "3637510"
  },
  {
    "text": "And this one is, I don't\nknow, maybe a bit weird if you like this, but-- [LAUGHTER]",
    "start": "3637510",
    "end": "3642640"
  },
  {
    "text": "--there's nothing\nwrong with it, right? And so it's the same\nfor this one here,",
    "start": "3642640",
    "end": "3648250"
  },
  {
    "text": "like, look, how many people love\nyou with the tumbleweed that's really sad. If you change just\none word suddenly",
    "start": "3648250",
    "end": "3654640"
  },
  {
    "text": "it's like a really\nnice thing to say. [LAUGHTER]  So if you want to\nsolve this, if you",
    "start": "3654640",
    "end": "3660730"
  },
  {
    "text": "want to classify this\ncorrectly for the meanness, then you have to\nreally understand",
    "start": "3660730",
    "end": "3666400"
  },
  {
    "text": "multimodal reasoning. You have to understand\nthe relationship between the image\nand the text in order to get to the right label.",
    "start": "3666400",
    "end": "3672460"
  },
  {
    "text": "And so it was really constructed\nby design to do that. So how we did it exactly is we\nused some really highly trained",
    "start": "3672460",
    "end": "3681940"
  },
  {
    "text": "annotators. And then one of the big problems\nwith a lot of these data sets is that nobody really knows\nwho owns the meme, for example.",
    "start": "3681940",
    "end": "3691220"
  },
  {
    "text": "So somebody makes this\nmeme now they technically own the copyright. And so when I made this data\nset, I was working at Facebook",
    "start": "3691220",
    "end": "3698110"
  },
  {
    "text": "and they were very afraid\nof copyright things. So what we actually had to\ndo is we had to pay people",
    "start": "3698110",
    "end": "3704109"
  },
  {
    "text": "to make new memes. [LAUGHTER]  So not from scratch.",
    "start": "3704110",
    "end": "3709730"
  },
  {
    "text": "So we could show them\nthe actual examples and then they had to try to find\nimages that were corresponding",
    "start": "3709730",
    "end": "3717400"
  },
  {
    "text": "to the original\nsource image and try to recreate the meme\nbut now with an image that we could buy from Getty.",
    "start": "3717400",
    "end": "3724555"
  },
  {
    "text": "And so we gave a lot\nof money to Getty so that we could then release\nthe data set to the public so",
    "start": "3724555",
    "end": "3731200"
  },
  {
    "text": "that people could do\nactually research on this and understand for\ntheir multimodal models whether they're good or not.",
    "start": "3731200",
    "end": "3736970"
  },
  {
    "text": "And so we really\ntried to make it so that we had these\nbenign confounders.",
    "start": "3736970",
    "end": "3743470"
  },
  {
    "text": "Sorry, I start the\nword with co-founders. So the confounder\nhere is obviously",
    "start": "3743470",
    "end": "3750025"
  },
  {
    "text": "that you have your\noriginal meme and then you have your confounder where you\nswap out one of the modalities and here you have\nthe other one, right?",
    "start": "3750025",
    "end": "3756580"
  },
  {
    "text": "So we had our annotators\ndo that as well. And so this led to a\nreally nice data set,",
    "start": "3756580",
    "end": "3763120"
  },
  {
    "text": "I think, because it showed\nsome of the intuitions that I think a lot of\npeople in the field had,",
    "start": "3763120",
    "end": "3768289"
  },
  {
    "text": "which is that multimodal\npretraining doesn't really work. Is that an alarm?",
    "start": "3768290",
    "end": "3773790"
  },
  {
    "text": "[LAUGHTER] So multimodal pretraining\ndoesn't really work. And so all of this\nstuff that people",
    "start": "3773790",
    "end": "3779900"
  },
  {
    "text": "have been doing\nwith all their fancy visual BERT models\nactually turned out maybe to not really be\nthat useful anyway.",
    "start": "3779900",
    "end": "3786769"
  },
  {
    "text": "So maybe it got you one\npoint extra from visual BERT to a different visual BERT,\nlike less than a point",
    "start": "3786770",
    "end": "3793940"
  },
  {
    "text": "just by doing that\nmultimodal pretraining. So that means we still have\nto figure this stuff out.",
    "start": "3793940",
    "end": "3800960"
  },
  {
    "text": "This data set is far\nfrom solved and we still have a long way\nto go despite all of these fancy models\nand a new paper",
    "start": "3800960",
    "end": "3808880"
  },
  {
    "text": "coming out every week\nthat does something new like we're not there yet. And I think that's encouraging,\nespecially for you.",
    "start": "3808880",
    "end": "3816410"
  },
  {
    "text": "You can go out and solve it. So what we did\nwith this data set",
    "start": "3816410",
    "end": "3821960"
  },
  {
    "text": "is we organized a competition. We had 100K in prize\nmoney to try to see what people could come up with.",
    "start": "3821960",
    "end": "3828720"
  },
  {
    "text": "And so there was a lot of\nnice work coming out of that and we really managed to crank\nthe numbers up by quite a lot.",
    "start": "3828720",
    "end": "3837300"
  },
  {
    "text": "But the solutions were\nslightly disappointing. So I don't know if\nyou've ever used Kaggle, but if you want to\nreally win on Kaggle",
    "start": "3837300",
    "end": "3843990"
  },
  {
    "text": "you just have to\nensemble the hell out of all of the\ndifferent models that are the current state of\nthe art and then you're",
    "start": "3843990",
    "end": "3849030"
  },
  {
    "text": "very likely to win, right? And so that's what\nhappened here.",
    "start": "3849030",
    "end": "3854820"
  },
  {
    "text": "There wasn't really the\nfundamental breakthrough we had maybe been hoping for. So that still needs\nto be built, I think.",
    "start": "3854820",
    "end": "3862470"
  },
  {
    "text": "So this other data set I just\nwant to briefly talk about. So the theme of this\nsection is like if you",
    "start": "3862470",
    "end": "3868290"
  },
  {
    "text": "make a data set, think about it\nvery carefully, because you can really be very\ncreative with this",
    "start": "3868290",
    "end": "3873540"
  },
  {
    "text": "and really, really measure the\nthings you're trying to get at. So this data set Winoground, we\nwere trying to figure out, OK,",
    "start": "3873540",
    "end": "3881140"
  },
  {
    "text": "how good is CLIP actually? So it looks really\namazing and it's way better than things\nthat were previously there,",
    "start": "3881140",
    "end": "3887010"
  },
  {
    "text": "but does it understand\ncompositional relationships in the same way that\nhumans would understand it,",
    "start": "3887010",
    "end": "3892230"
  },
  {
    "text": "or is it just sort of fitting\nonto the data distribution and it can be very good at\nthe head of the distribution",
    "start": "3892230",
    "end": "3898320"
  },
  {
    "text": "but is terrible at the tail? And you can probably already\nguess where this is going.",
    "start": "3898320",
    "end": "3904319"
  },
  {
    "text": "So just to give you an\nillustration of what is in this data set, you would\nhave some plants surrounding",
    "start": "3904320",
    "end": "3909540"
  },
  {
    "text": "a light bulb or you would\nhave a light bulb surrounding some plants. So notice that the words here\nare exactly the same words but",
    "start": "3909540",
    "end": "3917910"
  },
  {
    "text": "in a different order. So the visual depiction of these\nwords is very, very different.",
    "start": "3917910",
    "end": "3924890"
  },
  {
    "text": "So if your contrastive\nmodel is actually good at understanding\nthe visual semantic",
    "start": "3924890",
    "end": "3930910"
  },
  {
    "text": "or the visual linguistic\ncompositionality of these examples, then\nit can get it right.",
    "start": "3930910",
    "end": "3939490"
  },
  {
    "text": "But again, if it's\nactually just overfitting on the data distribution\nthat is seen and is biased toward\nwhat it sees often,",
    "start": "3939490",
    "end": "3947020"
  },
  {
    "text": "then it doesn't really get it. And so one paper that we use\nas a source of inspiration",
    "start": "3947020",
    "end": "3953140"
  },
  {
    "text": "for this work is\nthis paper here, \"Order Word Matters\nPre-Training for Little.\"",
    "start": "3953140",
    "end": "3959260"
  },
  {
    "text": "So we actually found\nthat the order of words doesn't even matter that\nmuch for general pretraining",
    "start": "3959260",
    "end": "3964450"
  },
  {
    "text": "very often, which is also\nkind of a scary thing, right? So this is deep\nlearning for NLP. We think that language\nis really important,",
    "start": "3964450",
    "end": "3971380"
  },
  {
    "text": "but these models can\nreason about language even if you shuffle all the words.",
    "start": "3971380",
    "end": "3976700"
  },
  {
    "text": "And so that's probably\nnot what we want to have. And so that doesn't tell you\nsomething about how great",
    "start": "3976700",
    "end": "3983500"
  },
  {
    "text": "we are as researchers,\nit tells you something about how terrible\nour evaluation benchmarks are.",
    "start": "3983500",
    "end": "3989079"
  },
  {
    "text": "And that's what we need to fix. So what we did with\nthis data set, here are some other nice examples.",
    "start": "3989080",
    "end": "3994900"
  },
  {
    "text": "There's a mug in some grass or\nthere's some grass in a mug. These are very\ndifferent pictures. And so for us,\nthese are trivial.",
    "start": "3994900",
    "end": "4001559"
  },
  {
    "text": "So what's the difference between\na truck fire and a fire truck? They're pretty\nimportant I think also",
    "start": "4001560",
    "end": "4008740"
  },
  {
    "text": "to get that distinction right. So guess what?",
    "start": "4008740",
    "end": "4014770"
  },
  {
    "text": "State-of-the-art models often\nperform below random chance. [LAUGHTER]",
    "start": "4014770",
    "end": "4019864"
  },
  {
    "text": " So as I said, we still\nhave a lot of work to do,",
    "start": "4019864",
    "end": "4024890"
  },
  {
    "text": "which is good. And so when this\npaper came out, I think the reaction\nwas really nice.",
    "start": "4024890",
    "end": "4032350"
  },
  {
    "text": "So when DALL-E2 came out-- so you've probably\nheard of DALL-E2, right?",
    "start": "4032350",
    "end": "4037540"
  },
  {
    "text": "So it's like Stable\nDiffusion but then before Stable Diffusion. And so this was\nreally the first model",
    "start": "4037540",
    "end": "4044560"
  },
  {
    "text": "that really showed\njust how impressive these generative models can be\nwhen they're creating images.",
    "start": "4044560",
    "end": "4051260"
  },
  {
    "text": "So there's a mug in some grass. You do have to kind of cheat\na little bit because you",
    "start": "4051260",
    "end": "4056500"
  },
  {
    "text": "have to add digital art here. If you don't add that then\nit breaks down completely.",
    "start": "4056500",
    "end": "4062080"
  },
  {
    "text": "[LAUGHTER]  So it's sort of prompt\nhacking, I think, or sort of tuning on\nthe test set, but OK.",
    "start": "4062080",
    "end": "4069370"
  },
  {
    "text": "So this is pretty good. So it's definitely is better\nthan I think a lot of people",
    "start": "4069370",
    "end": "4074680"
  },
  {
    "text": "would have expected even\na couple of years ago. But it's not perfect because\npeople on the internet",
    "start": "4074680",
    "end": "4082039"
  },
  {
    "text": "like to take more pictures\nof spoons than forks. So if you say there are\nfewer spoons and forks",
    "start": "4082040",
    "end": "4090050"
  },
  {
    "text": "or there are fewer\nforks and spoons, it just really like spoons more. [LAUGHTER]",
    "start": "4090050",
    "end": "4096138"
  },
  {
    "text": " And so maybe it's like\nThe Matrix or something, I don't know.",
    "start": "4096138",
    "end": "4103100"
  },
  {
    "text": "Spoons are just nicer. So again, what you can see here\nis that these models really",
    "start": "4103100",
    "end": "4108380"
  },
  {
    "text": "are just reflections of the\ndata that they're trained on. So models are getting\nbetter, but if you've",
    "start": "4108380",
    "end": "4115880"
  },
  {
    "text": "looked at Stable\nDiffusion, it still can't count fingers\nand things like that. So again, there's still a\nlot of cool work to be done.",
    "start": "4115880",
    "end": "4124380"
  },
  {
    "text": "Any questions on evaluation? ",
    "start": "4124380",
    "end": "4132620"
  },
  {
    "text": "No? OK. So let's talk about other\nmodalities then because--",
    "start": "4132620",
    "end": "4137809"
  },
  {
    "text": "so we've really just been\nfocused on images and images are great. There are lots of\nimages on the internet.",
    "start": "4137810",
    "end": "4144839"
  },
  {
    "text": "And so that makes it an\nobvious thing to focus on. It's also I think if\nyou look at our brain,",
    "start": "4144840",
    "end": "4151009"
  },
  {
    "text": "vision is a very\ndominant modality, right? So how we understand the\nworld is very vision-driven.",
    "start": "4151010",
    "end": "4156410"
  },
  {
    "text": "But it doesn't have\nto be the case. So there's all these other\ninteresting problems that involve different modalities.",
    "start": "4156410",
    "end": "4162778"
  },
  {
    "text": "And so the most obvious one\nis just speech or audio. So after seeing comes hearing.",
    "start": "4162779",
    "end": "4169714"
  },
  {
    "text": "And really we could do\nanother lecture just like this just on\nspeech and audio and there's lots of interesting\nstuff to talk about.",
    "start": "4169715",
    "end": "4176479"
  },
  {
    "text": "Obviously, we don't\nhave time, but I'll give you another nice example\nof how amazing Alec Radford is",
    "start": "4176479",
    "end": "4182929"
  },
  {
    "text": "at creating data sets. So there's this\nWhisper model that came out of OpenAI not\ntoo long ago, which",
    "start": "4182930",
    "end": "4189770"
  },
  {
    "text": "was trained on 680,000 hours of\nmultilingual multitask speech data.",
    "start": "4189770",
    "end": "4195329"
  },
  {
    "text": "So speech with transcriptions. And they trained this\nvery fancy thing on there,",
    "start": "4195330",
    "end": "4201300"
  },
  {
    "text": "which actually is not\nvery fancy at all, it's just the log\nmel spectrogram. So how you represent\nthe audio signal.",
    "start": "4201300",
    "end": "4206580"
  },
  {
    "text": "And then you feed that\ninto a big transformer. So this is your encoder\nself-attention here,",
    "start": "4206580",
    "end": "4211800"
  },
  {
    "text": "and then you have\nyour decoder where you have your\ncross-attention, and then you just generate the sequence.",
    "start": "4211800",
    "end": "4217210"
  },
  {
    "text": "So this is encoder-decoder\nbasic transformer model but your input is one\ndimensional convolutions",
    "start": "4217210",
    "end": "4224640"
  },
  {
    "text": "over the log mel spectrogram. And so there's lots of papers\nthat do very similar things.",
    "start": "4224640",
    "end": "4230475"
  },
  {
    "text": "There's models like Wav2Vec\nthat try to turn the wave signal into vectors or\nyou can discretize it",
    "start": "4230475",
    "end": "4235890"
  },
  {
    "text": "in lots of different ways. So there's a wealth\nof literature. Then I think one of\nthe funny observations",
    "start": "4235890",
    "end": "4242460"
  },
  {
    "text": "actually is that you can just\nreduce audio to vision anyway, right? So that's what you could argue\nthis log mel spectrogram does.",
    "start": "4242460",
    "end": "4250770"
  },
  {
    "text": "So not to toot my own\nhorn, but in 2017, I did this paper where we\nshowed that you can just",
    "start": "4250770",
    "end": "4255989"
  },
  {
    "text": "take a real audio sample, turn\nit into a spectrogram, really",
    "start": "4255990",
    "end": "4263490"
  },
  {
    "text": "just a spectrogram. So what does the spectrum\nof the audio file look like, feed that to a\nregular ConvNet like an AlexNet",
    "start": "4263490",
    "end": "4270870"
  },
  {
    "text": "even, and then that gives you\namazing auditory features. So now you can use\nthis to distinguish between violins or guitars\nand things like that.",
    "start": "4270870",
    "end": "4278520"
  },
  {
    "text": "So maybe you can just reduce\nall of this to vision. So one question\nmaybe you could ask is can we also reduce\nlanguage to vision,",
    "start": "4278520",
    "end": "4285810"
  },
  {
    "text": "or vision to language? So that's what people\nare thinking about.",
    "start": "4285810",
    "end": "4292320"
  },
  {
    "text": "So we talked about video. There was a question\nabout video. So a lot of these ideas also\nextend pretty directly to video",
    "start": "4292320",
    "end": "4299310"
  },
  {
    "text": "but now you just have more data. So like Flamingo already had\na bunch of different images in it. You can do Flamingo over videos.",
    "start": "4299310",
    "end": "4306750"
  },
  {
    "text": "Probably, a lot\nof the images are pretty useless for\nwhat you're trying to do with this video model.",
    "start": "4306750",
    "end": "4312540"
  },
  {
    "text": "So they're too similar. It doesn't really add all\nthat much information. So you want to\nsubsample the frames",
    "start": "4312540",
    "end": "4318280"
  },
  {
    "text": "so that you get the most useful\ninformation out of your video. And so there's a\nbunch of approaches",
    "start": "4318280",
    "end": "4323670"
  },
  {
    "text": "that take the keyframes\nand then you just do a standard joint vision and\nlanguage transformer encoder",
    "start": "4323670",
    "end": "4330210"
  },
  {
    "text": "thing on top of that. So this is becoming hopefully\nby now a very familiar recipe.",
    "start": "4330210",
    "end": "4336179"
  },
  {
    "text": "And so there's this-- so MERLOT is a nice\narchitecture that does this and then they came up\nwith MERLOT Reserve,",
    "start": "4336180",
    "end": "4343410"
  },
  {
    "text": "kind of a silly name, where they\nalso added audio to this model. So this is now a trimodal model.",
    "start": "4343410",
    "end": "4350220"
  },
  {
    "text": "And so we're going towards\nthis foundation model that can consume all of these\ndifferent modalities all in one",
    "start": "4350220",
    "end": "4356910"
  },
  {
    "text": "go and that's really like\na clear trend in the field. Another very interesting\ndirection I think where--",
    "start": "4356910",
    "end": "4365100"
  },
  {
    "text": "in the field, we were very\nexcited about this for a while, but I think it's\ngone now because it's",
    "start": "4365100",
    "end": "4371880"
  },
  {
    "text": "too difficult to create\nlots of high-quality data in this setting. But what you can do is you can\nhave simulated environments.",
    "start": "4371880",
    "end": "4378940"
  },
  {
    "text": "So this is a paper\nfrom DeepMind from 2017 where they had this agent\nwalk around in the maze and then it could have\nnatural language instructions.",
    "start": "4378940",
    "end": "4386190"
  },
  {
    "text": "It could also generalize to dax\nand blicks and different sort of groundings and\nassignments that you",
    "start": "4386190",
    "end": "4391560"
  },
  {
    "text": "could do in that environment. So this is a super\ninteresting direction I think in the long\nterm because this is how",
    "start": "4391560",
    "end": "4397469"
  },
  {
    "text": "humans learn language, right? We walk around in the world. We interact with\nour environments. We have all of these different\nperceptual observations.",
    "start": "4397470",
    "end": "4404280"
  },
  {
    "text": "We synthesize them in our brain. We manipulate objects. We change our own\nviewpoint and that's",
    "start": "4404280",
    "end": "4409530"
  },
  {
    "text": "how we learn everything\nwe know about the world. And so our language\nis very intricately",
    "start": "4409530",
    "end": "4414930"
  },
  {
    "text": "connected to that world\nand how we observe it. So I think that\nmight make a comeback",
    "start": "4414930",
    "end": "4421230"
  },
  {
    "text": "at some point in the future. You can also do other stuff. So especially with this\nkind of conditioning on text",
    "start": "4421230",
    "end": "4428835"
  },
  {
    "text": "that we're seeing a lot of. So DALL-E2 and Stable\nDiffusion and all of these different\nthings, and the original",
    "start": "4428835",
    "end": "4435260"
  },
  {
    "text": "GAN we talked about\nat the beginning. You can do the\nsame thing but now you're generating\n3D point clouds.",
    "start": "4435260",
    "end": "4442160"
  },
  {
    "text": "So this is a 3D\ncorgi, using a corgi. And so this prompt can probably\nbecome much more complex",
    "start": "4442160",
    "end": "4448850"
  },
  {
    "text": "over time and you\ncan do AutoCAD design and just say give me\na house and it's just",
    "start": "4448850",
    "end": "4454160"
  },
  {
    "text": "going to design the\nwhole house for you. So you can just tweak the\nprompt and things like that.",
    "start": "4454160",
    "end": "4459950"
  },
  {
    "text": "That's all coming or even\nalready here in many cases. So the final modality I just\nbriefly wanted to talk about",
    "start": "4459950",
    "end": "4467930"
  },
  {
    "text": "is olfactory embeddings. [LAUGHTER] ",
    "start": "4467930",
    "end": "4474090"
  },
  {
    "text": "So olfaction means smell,\nif you didn't know. So it turns out--\nso my PhD thesis",
    "start": "4474090",
    "end": "4481440"
  },
  {
    "text": "was about grounding semantics\nin different perceptual modalities.",
    "start": "4481440",
    "end": "4487090"
  },
  {
    "text": "So a lot of my work started in\nvision, and then it's like, OK, now audio is the\nobvious next one, right?",
    "start": "4487090",
    "end": "4492270"
  },
  {
    "text": "So you can learn the\nmeaning of violin and then maybe you can learn\nwhat a violin looks like",
    "start": "4492270",
    "end": "4497917"
  },
  {
    "text": "and what it is and\nwhat it sounds like and that's going to give\nyou a richer representation. But for a lot of these\nwords, what's actually",
    "start": "4497917",
    "end": "4504120"
  },
  {
    "text": "very primitive to\ntheir meaning is what they smell like because\nin our brains that's really",
    "start": "4504120",
    "end": "4509700"
  },
  {
    "text": "one of the core areas and one of\nthe oldest areas in your brain. So what you can try\nto do if you want",
    "start": "4509700",
    "end": "4516120"
  },
  {
    "text": "to complete all of your\nperceptual modalities is you can try to build\nolfactory embeddings.",
    "start": "4516120",
    "end": "4521500"
  },
  {
    "text": "So that was kind of a joke\npaper I did, but the funny thing",
    "start": "4521500",
    "end": "4526770"
  },
  {
    "text": "is it actually worked. [LAUGHTER] So there's a catalog, the\nSigma-Aldrich fine flavors",
    "start": "4526770",
    "end": "4535000"
  },
  {
    "text": "and fragrances catalog,\nwhere you can look up words like melon and\npineapple, and then it's",
    "start": "4535000",
    "end": "4540610"
  },
  {
    "text": "going to give you all of\nthe chemical compounds that produce this smell or taste.",
    "start": "4540610",
    "end": "4545980"
  },
  {
    "text": "And so if you do that, then\nyou can count the occurrences and then you can do SVD or\nsomething like that on it",
    "start": "4545980",
    "end": "4552670"
  },
  {
    "text": "to get it to be a bit more\nof a real embedding model. So now you get smell\nembeddings, smell vectors.",
    "start": "4552670",
    "end": "4559540"
  },
  {
    "text": "And then you can compute\nsimilarity judgments between these smells. So it turns out apple\nsmells like pear,",
    "start": "4559540",
    "end": "4567280"
  },
  {
    "text": "and the chocolate and\ncocoa and sweet and coffee are sort of related. So you get these clusters of\ndifferent smells just based off",
    "start": "4567280",
    "end": "4575620"
  },
  {
    "text": "of their chemical compounds. So this bag of chemical\ncompounds model gives you a very\nrich representation.",
    "start": "4575620",
    "end": "4582470"
  },
  {
    "text": "And so if you look at all of the\nwords that are concrete enough to have smell, so if you have\na word democracy in there,",
    "start": "4582470",
    "end": "4590410"
  },
  {
    "text": "that doesn't really smell\nlike anything, right? [LAUGHTER] So you ignore democracy,\nyou just focus on the things",
    "start": "4590410",
    "end": "4599640"
  },
  {
    "text": "that smell or that\ncould smell, I guess. So the really\ninteresting thing to me",
    "start": "4599640",
    "end": "4605790"
  },
  {
    "text": "is that this is\nmuch more correlated with human similarity judgments\nthan the linguistic vectors",
    "start": "4605790",
    "end": "4613140"
  },
  {
    "text": "we had at the time. So for a word like\napple, you can just get a word vector you've\nlearned in your first lecture.",
    "start": "4613140",
    "end": "4621515"
  },
  {
    "text": "And so you can do skip-gram\nand things like that. But that thing is\nnot going to be as correlated with human\nsimilarity judgments",
    "start": "4621515",
    "end": "4629130"
  },
  {
    "text": "as this bag of chemical\ncompounds model. So that's pretty interesting.",
    "start": "4629130",
    "end": "4634260"
  },
  {
    "text": "So even something like smell\nwhere maybe we think this doesn't really matter. If you really want to understand\nhow humans understand language,",
    "start": "4634260",
    "end": "4641520"
  },
  {
    "text": "then maybe you want to include\nthis in your foundation model, too.",
    "start": "4641520",
    "end": "4647330"
  },
  {
    "text": "But I would start\nwith other modalities. [LAUGHTER]  All right.",
    "start": "4647330",
    "end": "4652400"
  },
  {
    "text": "About time. OK. Yeah, sorry. So where to next?",
    "start": "4652400",
    "end": "4657570"
  },
  {
    "text": "I think I've already said\nmost of this actually. So one foundation model\nis going to rule them all.",
    "start": "4657570",
    "end": "4663949"
  },
  {
    "text": "So, I mean, there will be many\nof these but a lot of them are going to have very\nsimilar traits, I think.",
    "start": "4663950",
    "end": "4669470"
  },
  {
    "text": "We're going to be\nlooking at scaling laws and trying to\nunderstand really what is the relationship between\nthe different modalities, which",
    "start": "4669470",
    "end": "4675740"
  },
  {
    "text": "one do we want more\nof, that sort of stuff. We're going to have\nretrieval augmentation. This thing is going to be really\nhuge if you've heard of RAG,",
    "start": "4675740",
    "end": "4683630"
  },
  {
    "text": "or if you haven't,\nyou should look it up. So all of these parts of these\nmodels can also be multimodal.",
    "start": "4683630",
    "end": "4689179"
  },
  {
    "text": "We need way better evaluation\nand better measurement. We already talked\nabout that, too. And that's all I had.",
    "start": "4689180",
    "end": "4694770"
  },
  {
    "text": "Thank you. [APPLAUSE] ",
    "start": "4694770",
    "end": "4703000"
  }
]