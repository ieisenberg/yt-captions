[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "All right, let's get started. So we're gonna continue talking about Bayesian networks which we started on Monday.",
    "start": "4670",
    "end": "12420"
  },
  {
    "text": "Um, and just a kind of quick recap, uh, we've been talking about Bayesian networks which is a new paradigm for defining models.",
    "start": "12420",
    "end": "22090"
  },
  {
    "text": "Um, and what is a Bayesian network? Uh, you have a set of variables which are nodes in a graph.",
    "start": "22090",
    "end": "27840"
  },
  {
    "start": "23000",
    "end": "118000"
  },
  {
    "text": "For example, uh, whether you have a cold, whether you have allergies, whether you're coughing, whether you have itchy eyes.",
    "start": "27840",
    "end": "33345"
  },
  {
    "text": "These nodes are related by a set of directed edges which capture various dependencies.",
    "start": "33345",
    "end": "39170"
  },
  {
    "text": "For example, itchy eyes is caused by allergies but not cold or by cough.",
    "start": "39170",
    "end": "44539"
  },
  {
    "text": "[NOISE] And then formally for every variable in the Bayesian network,",
    "start": "44540",
    "end": "50590"
  },
  {
    "text": "you have a local conditional distribution which specifies the distribution over that variable given the parents.",
    "start": "50590",
    "end": "57650"
  },
  {
    "text": "So the parents of cough are cold and allergies. So what, you would have a local conditional distribution of p of h given c and a.",
    "start": "57650",
    "end": "66955"
  },
  {
    "text": "You do that for all the variables, and finally you take all the factors or",
    "start": "66955",
    "end": "72050"
  },
  {
    "text": "local conditional distributions and you multiply them together and you get one glorious joint distribution over all the possible variables in your distribution.",
    "start": "72050",
    "end": "83485"
  },
  {
    "text": "Okay? So in other words, to sum it up you can think about Bayesian networks as factor graphs plus probability.",
    "start": "83485",
    "end": "91220"
  },
  {
    "text": "They allow you to define ginormous joint distributions over lots of random variables,",
    "start": "91220",
    "end": "96845"
  },
  {
    "text": "uh, using factor graphs which allow you to specify things very compactly. And moreover, we saw glimpses of how we can",
    "start": "96845",
    "end": "105875"
  },
  {
    "text": "use the structure factor graphs to permit efficient inference. So probabilistic inference in Bayesian networks",
    "start": "105875",
    "end": "114575"
  },
  {
    "text": "is the task of- given a Bayesian network, which is, you know, this oracle about, uh,",
    "start": "114575",
    "end": "121680"
  },
  {
    "start": "118000",
    "end": "528000"
  },
  {
    "text": "what you know about the world, um, and you look at some evidence that you've found.",
    "start": "121680",
    "end": "127955"
  },
  {
    "text": "So it's, you know, it's raining or not raining, or, [NOISE] or you have itchy eyes or so on.",
    "start": "127955",
    "end": "134540"
  },
  {
    "text": "And you condition on that evidence, and you also have a set of query variables that you're interested in asking about.",
    "start": "134540",
    "end": "143180"
  },
  {
    "text": "And the goal is to compute the probability of the query variables conditioned on the evidence that you see,",
    "start": "143180",
    "end": "150410"
  },
  {
    "text": "big E equals little e. Remember lower- upper cases, random variables, lowercase, um, is actual values.",
    "start": "150410",
    "end": "159835"
  },
  {
    "text": "Um, and so for example, um, in the coughing case there's, um,",
    "start": "159835",
    "end": "167445"
  },
  {
    "text": "the probability of a cold given the fact that you're coughing but don't have itchy eyes, okay?",
    "start": "167445",
    "end": "175695"
  },
  {
    "text": "And this probabi- this probability is defined by just the laws of probability which we went over,",
    "start": "175695",
    "end": "181970"
  },
  {
    "text": "uh, the first, uh, slide of last lecture. Um, and the challenge is how to do this efficiently, okay?",
    "start": "181970",
    "end": "191840"
  },
  {
    "text": "And that's going to be the topic of, uh, this, this class. So any questions about the basic setup of what Bayesian networks are and,",
    "start": "191840",
    "end": "199155"
  },
  {
    "text": "um, how do you- what does it mean to do probabilistic inference?",
    "start": "199155",
    "end": "203770"
  },
  {
    "text": "Okay. One maybe kind of a high-level note about Bayesian networks is that I,",
    "start": "206570",
    "end": "211705"
  },
  {
    "text": "I think they are really, um, powerful as a way to describe kind of knowledge.",
    "start": "211705",
    "end": "217435"
  },
  {
    "text": "I think a lot of, uh, AI today is focused on particular tasks where you define some outpu- inputs,",
    "start": "217435",
    "end": "223300"
  },
  {
    "text": "and you define some outputs, and you train their classifier. And the classifier that you train can only do this one thing, input, output.",
    "start": "223300",
    "end": "230075"
  },
  {
    "text": "But the paradigm behind Bayesian networks and, you know, databases in general is that you have to develop a kind",
    "start": "230075",
    "end": "237520"
  },
  {
    "text": "of a knowledge source which can be probabilistic, it's captured by this joint distribution. And once you have it,",
    "start": "237520",
    "end": "243845"
  },
  {
    "text": "you use those tools of probability to allow you to answer arbitrary questions about it. So you can give me any pieces of evidence",
    "start": "243845",
    "end": "251360"
  },
  {
    "text": "and any query and it's clear what I meant to do, I'm supposed to compute these values.",
    "start": "251360",
    "end": "256690"
  },
  {
    "text": "So this is- it's kind of a more flexible and powerful, uh, paradigm than just, you know, converting inputs and outputs.",
    "start": "256690",
    "end": "264155"
  },
  {
    "text": "And so that's why I think it's so interesting. Okay. So uh, today we're gonna focus on how to",
    "start": "264155",
    "end": "270530"
  },
  {
    "text": "compute these arbitrary inference queries efficiently. I'm gonna start with forward-backward and particle filtering,",
    "start": "270530",
    "end": "277160"
  },
  {
    "text": "these are going to specialize to the specific, uh, Bayesian networks, uh, called HMMs or Hidden Markov Models.",
    "start": "277160",
    "end": "283825"
  },
  {
    "text": "And then we're gonna look at Gibbs sampling which is a much, uh, you know, more general way of doing things.",
    "start": "283825",
    "end": "288995"
  },
  {
    "text": "Okay. So um, a Hidden Markov Model which I talked about, uh, last time, um,",
    "start": "288995",
    "end": "295690"
  },
  {
    "text": "which we're going to go in more detail at this time is, uh, a Bayesian network where there exists,",
    "start": "295690",
    "end": "302389"
  },
  {
    "text": "uh, a sequence of hidden variables and a corresponding sequence of observed variables.",
    "start": "302389",
    "end": "309395"
  },
  {
    "text": "So as a kind of motivating example, imagine you're tracking some sort of object, um, in your homework you'll be tracking cars.",
    "start": "309395",
    "end": "317000"
  },
  {
    "text": "So uh, Hi is going to be the location of the object or car at a particular time step i.",
    "start": "317000",
    "end": "323855"
  },
  {
    "text": "And Ei is going to be some sort of sensor reading that you get at that particular time step.",
    "start": "323855",
    "end": "328880"
  },
  {
    "text": "It could be the location plus some noise, or some sort of distance to that, um, the true object, um, and so on.",
    "start": "328880",
    "end": "336230"
  },
  {
    "text": "Okay, so these are the variables and, uh,",
    "start": "336230",
    "end": "341550"
  },
  {
    "text": "it goes well as saying that the hidden variables are hidden and the observed variables are observed. Um, so the distributions over, uh,",
    "start": "341550",
    "end": "351870"
  },
  {
    "text": "this- all the variables are specified by three types of local conditional distributions.",
    "start": "351870",
    "end": "357305"
  },
  {
    "text": "The first one is just the starting distribution. What is the probability of H_1?",
    "start": "357305",
    "end": "362830"
  },
  {
    "text": "Um, this could be uniform over all possible locations just as an example.",
    "start": "362830",
    "end": "368689"
  },
  {
    "text": "Um, and then we have the transition distributions which specify what is the distribution over a particular hidden variable,",
    "start": "368690",
    "end": "377024"
  },
  {
    "text": "the location of the true location of object H_i given H_i minus 1.",
    "start": "377024",
    "end": "382400"
  },
  {
    "text": "So this captures dynamics of how this object or car might move over time.",
    "start": "382400",
    "end": "388990"
  },
  {
    "text": "Um, for example, it could just be uniform over adjacent locations, right?",
    "start": "388990",
    "end": "394069"
  },
  {
    "text": "So cars can't teleport, they can only move to adjacent locations over uh, one timestep.",
    "start": "394070",
    "end": "399250"
  },
  {
    "text": "And finally, we have emission distributions which, uh, govern how the sensor reading is,",
    "start": "399250",
    "end": "407795"
  },
  {
    "text": "uh, computed as a function of the location. Okay? Um, so this, again,",
    "start": "407795",
    "end": "413870"
  },
  {
    "text": "could be something as simple as uniform over adjacent locations, um, if you expect to see some noise in your sensor.",
    "start": "413870",
    "end": "419950"
  },
  {
    "text": "The sensor doesn't tell you where exactly the car is, but it tells you approximately where the car is.",
    "start": "419950",
    "end": "425760"
  },
  {
    "text": "And the joint distribution over all these random variables is",
    "start": "426170",
    "end": "431210"
  },
  {
    "text": "going to be given by simply the product of everything you see on the board. I'm just gonna write this up on the board just for, you know, reference.",
    "start": "431210",
    "end": "438395"
  },
  {
    "text": "Um, so we have the probability of H equals h. So this- when I write H equals h that means,",
    "start": "438395",
    "end": "444915"
  },
  {
    "text": "uh, the H_1 through H_n. So all the random variables, uh, all the hidden random variables,",
    "start": "444915",
    "end": "450030"
  },
  {
    "text": "all the observed random variables, and this is by definition equal to, um, the, the start distribution.",
    "start": "450030",
    "end": "459135"
  },
  {
    "text": "Um, let's see just to make sure I have- okay, good notation here. So start distribution over,",
    "start": "459135",
    "end": "466694"
  },
  {
    "text": "um, h_1, and then I have the transitions, i equals, um, 1 to- uh,",
    "start": "466695",
    "end": "473669"
  },
  {
    "text": "I guess, 2 to n, just to make sure, okay.",
    "start": "473670",
    "end": "479220"
  },
  {
    "text": "So this is the probability of, um, h_i given h_i minus 1. And then finally I have for every time step I through 1 through n. I",
    "start": "479220",
    "end": "488960"
  },
  {
    "text": "have the probability of an observation given, um, h_i.",
    "start": "488960",
    "end": "494270"
  },
  {
    "text": "Okay? So multiply all these factors together, that gives me a single number that is, uh,",
    "start": "494270",
    "end": "500539"
  },
  {
    "text": "the probability of all the observed and all the hidden variables.",
    "start": "500540",
    "end": "505910"
  },
  {
    "text": "Okay? Any questions about the definition of a Hidden Markov Model? [NOISE] Okay.",
    "start": "505910",
    "end": "517594"
  },
  {
    "text": "So given we have one of these models, remember with a Bayesian network I can answer any sort of queries.",
    "start": "517595",
    "end": "523880"
  },
  {
    "text": "I can ask what is the probability of, um, H_3 given H_2 and E_5,",
    "start": "523880",
    "end": "529220"
  },
  {
    "start": "528000",
    "end": "1872000"
  },
  {
    "text": "and it can do, do all sorts of crazy things. And all of these things are possible and efficient to, um, you know,",
    "start": "529220",
    "end": "535340"
  },
  {
    "text": "compute but we're going to focus on two main types of questions. Uh, motivated by the,",
    "start": "535340",
    "end": "541760"
  },
  {
    "text": "uh, let's say object tracking example. The first question is filtering. Filtering says, \"You're at a particular time step,",
    "start": "541760",
    "end": "548480"
  },
  {
    "text": "let's say time step 3. What, what do I know about the true object location",
    "start": "548480",
    "end": "553910"
  },
  {
    "text": "H_3 given all the evidence I've seen up until, uh, now?\"",
    "start": "553910",
    "end": "559000"
  },
  {
    "text": "So this is kinda real-time object tracking, at each point in time you look at all the evidence and you want to know where,",
    "start": "559000",
    "end": "565415"
  },
  {
    "text": "um, the object is. Uh, a similar question is smoothing and you're still looking at a particular time step,",
    "start": "565415",
    "end": "576035"
  },
  {
    "text": "um, um, three, let's say. Um, but you're conditioning on all the evidence.",
    "start": "576035",
    "end": "581465"
  },
  {
    "text": "So you're looking at all the observations, and you're looking at- you're kind of thinking about, uh, this more retrospectively.",
    "start": "581465",
    "end": "588335"
  },
  {
    "text": "Where was object- where was the object at time step 3? So think about if you're trying to reconstruct the trajectory or something.",
    "start": "588335",
    "end": "596250"
  },
  {
    "text": "Okay? So this is called filtering and smoothing. Um, so let's now try to develop, um,",
    "start": "596420",
    "end": "606899"
  },
  {
    "text": "an algorithm for answering these type of queries, and, um, without loss of generality,",
    "start": "606900",
    "end": "613715"
  },
  {
    "text": "I'm going to focus on answering smoothing queries. Um, so my- why is it the case that if I tell you I can solve all smoothing questions,",
    "start": "613715",
    "end": "623690"
  },
  {
    "text": "I can also solve all filtering questions? [NOISE] [inaudible]",
    "start": "623690",
    "end": "634985"
  },
  {
    "text": "So it is true so that this- in filtering this is- the evidence is a subset.",
    "start": "634985",
    "end": "642204"
  },
  {
    "text": "But the answers are going to be different depending on what evidence you compute on. So you can't literally just use one as the answer for the other. Yeah.",
    "start": "642205",
    "end": "651610"
  },
  {
    "text": "You marginalize over the things that you, uh, like E_45 to get to- Yeah, yeah, so you marginalize.",
    "start": "651610",
    "end": "658150"
  },
  {
    "text": "That's- that's the key idea. Is that suppose I had a smoother and I wanted to answer this filtering query, right?",
    "start": "658150",
    "end": "664120"
  },
  {
    "text": "So this is H_3 given E_1 to E_2, E_3, right? Remember last time we talked about how you can take leaves of",
    "start": "664120",
    "end": "671260"
  },
  {
    "text": "Bayesian networks which are not observed and just essentially wiped them away. So if you don't observe E_4, E_5, H_4,",
    "start": "671260",
    "end": "678399"
  },
  {
    "text": "H_5, you can just pretend those things, you know, don't exist. Right. And now you're back",
    "start": "678400",
    "end": "685690"
  },
  {
    "text": "to a smoothing query where you're conditioning on all the evidence. [NOISE].",
    "start": "685690",
    "end": "690699"
  },
  {
    "text": "Okay, so we're gonna focus on smoothing and to make progress on this problem,",
    "start": "690700",
    "end": "697135"
  },
  {
    "text": "I'm going to introduce a representation that's going to help us think about the possible assignments, right?",
    "start": "697135",
    "end": "706285"
  },
  {
    "text": "And just to be- be clear, right. There's- the reason why this is not completely trivial is that",
    "start": "706285",
    "end": "712570"
  },
  {
    "text": "there are for- if you have N hidden variables, there's 2 to the N or exponential N number of possible assignments.",
    "start": "712570",
    "end": "721660"
  },
  {
    "text": "And you can't just enumerate all of them. So you're going to have to come up with some algorithm that can compute it more efficiently.",
    "start": "721660",
    "end": "727360"
  },
  {
    "text": "Okay, so what we're gonna do is introduce this lattice representation which is gonna give us a compact way of representing those assignments.",
    "start": "727360",
    "end": "734230"
  },
  {
    "text": "And then we can see how we can operate on that representation, okay?",
    "start": "734230",
    "end": "739435"
  },
  {
    "text": "So this is gonna smell a lot like a state-based model. So we're kind of going backwards, uh, but hopefully it'll make sense.",
    "start": "739435",
    "end": "746530"
  },
  {
    "text": "So the idea behind a lattice representation is that I'm going to have, um, a set of rows and columns.",
    "start": "746530",
    "end": "754404"
  },
  {
    "text": "So each column is going to correspond to a particular variable. So the first column is going to correspond to H_1.",
    "start": "754405",
    "end": "762280"
  },
  {
    "text": "And each row is going to correspond to some setting of that variable.",
    "start": "762280",
    "end": "768850"
  },
  {
    "text": "So there's two possible things I can do. I can either set H_1 equals one or I can set H_1 equal to 2.",
    "start": "768850",
    "end": "775959"
  },
  {
    "text": "I'm- the version I'm drawing on the board is going to be a simplification of what I have on the slides just,",
    "start": "775960",
    "end": "781089"
  },
  {
    "text": "uh, in the interest of space. And the second column is going to be either H_2 equals 1 or H_2 equals 2.",
    "start": "781090",
    "end": "793390"
  },
  {
    "text": "So by going through these, uh, lattice nodes which are drawn as boxes, I'm kind of assigning random variables to a particular value.",
    "start": "793390",
    "end": "802060"
  },
  {
    "text": "Okay, so I'm going to connect these up. So from this state I can either set H_2 equals 1 or 2.",
    "start": "802060",
    "end": "807955"
  },
  {
    "text": "Here I can also go from to 1 or 2, and finally let's just do H_3 equals 1,",
    "start": "807955",
    "end": "814045"
  },
  {
    "text": "H_3 equals 2, and similarly I can choose either one of them from no matter where I am.",
    "start": "814045",
    "end": "820675"
  },
  {
    "text": "And finally I have an end state, okay? So first notice that the size of the lattice is reasonably well controlled.",
    "start": "820675",
    "end": "830290"
  },
  {
    "text": "It's simply the number of timesteps times the number of values that a variable can take on.",
    "start": "830290",
    "end": "835824"
  },
  {
    "text": "So let's suppose that there's N timesteps and K possible let's say locations,",
    "start": "835825",
    "end": "847480"
  },
  {
    "text": "so values of H_I. So how many nodes are here?",
    "start": "847480",
    "end": "853130"
  },
  {
    "text": "K times N. K times N, right? Okay, so that means we can- essentially,",
    "start": "854670",
    "end": "862630"
  },
  {
    "text": "this doesn't blow up exponentially. Okay, so now let us interpret a path from start to end.",
    "start": "862630",
    "end": "869454"
  },
  {
    "text": "What does a path from start to end tell us? So let's take- let's take this path.",
    "start": "869455",
    "end": "877225"
  },
  {
    "text": "What does this tell us? Yeah.",
    "start": "877225",
    "end": "883569"
  },
  {
    "text": "It's like a particular assignment of arranged variables. Yeah, it's a particular assignment of the variables.",
    "start": "883570",
    "end": "888910"
  },
  {
    "text": "So this one says, set H_1 to 1, H_2 to 1, H_3 to 1. This path says set H_1 to 2,",
    "start": "888910",
    "end": "896274"
  },
  {
    "text": "H_1 to 1, and H_3 to 2, and so on. Okay, so every path from start to end is",
    "start": "896275",
    "end": "902800"
  },
  {
    "text": "an assignment to all the unobserved or hidden variables. Okay, so now remember each assignment comes with some sort of probability.",
    "start": "902800",
    "end": "912415"
  },
  {
    "text": "So we're going to try to represent those probabilities, um, juxtaposed on this graph. Okay. So I'm gonna go through each of these edges.",
    "start": "912415",
    "end": "921880"
  },
  {
    "text": "So remember these are the probabilities. So every assignment has- is a product of the factors.",
    "start": "921880",
    "end": "927475"
  },
  {
    "text": "And I'm going to basically take these factors and just sprinkle them on the edges at the points where I can compute the factors,",
    "start": "927475",
    "end": "937839"
  },
  {
    "text": "and I'll explain more what I mean by this. Okay, uh, so- so maybe one- one kind of preliminary thing I should mention is",
    "start": "937840",
    "end": "950829"
  },
  {
    "text": "suppose for this example we have- we are conditioning on",
    "start": "950830",
    "end": "956305"
  },
  {
    "text": "E_1 equals 1, E_2 equals 2.",
    "start": "956305",
    "end": "963339"
  },
  {
    "text": "Let's say E_1 equals- sorry E_3 equals 1, okay? So I'm conditioning on these things.",
    "start": "963340",
    "end": "970630"
  },
  {
    "text": "Notice I'm not drawing them in here because these are observed variables. I don't have to reason about what values they take on.",
    "start": "970630",
    "end": "977620"
  },
  {
    "text": "I'm only going to consider the hidden variables which I don't know. But this is just going to be some sort of reference.",
    "start": "977620",
    "end": "983709"
  },
  {
    "text": "Okay. So let's start with- start H_1 equals 1, right?",
    "start": "983710",
    "end": "990415"
  },
  {
    "text": "So if you remember, uh, backtracking in CSPs, right? We basically took factors,",
    "start": "990415",
    "end": "996445"
  },
  {
    "text": "and then whenever we could evaluate the factor of, we just put it down on that edge in the backtracking tree.",
    "start": "996445",
    "end": "1005070"
  },
  {
    "text": "So here what- what can we do we have the probability of H_1 equals 1, okay?",
    "start": "1005070",
    "end": "1013035"
  },
  {
    "text": "And then we also have the probability of the- the first emission probability I can compute, right?",
    "start": "1013035",
    "end": "1018720"
  },
  {
    "text": "So that's the probability of E_1 equals the evidence I saw which is 1,",
    "start": "1018720",
    "end": "1024164"
  },
  {
    "text": "given H_1 equals 1, which is the value that I've committed to here. So this is a number that is essentially the weight,",
    "start": "1024165",
    "end": "1032819"
  },
  {
    "text": "or cost, or score, or whatever you wanna call it that I, um, am going to incur when I traverse that edge.",
    "start": "1032820",
    "end": "1041174"
  },
  {
    "text": "Okay, so what about this one? So I have the transition from P- let's say,",
    "start": "1041175",
    "end": "1050490"
  },
  {
    "text": "uh, H_2 equals 1, given H_1 equals 1,",
    "start": "1050490",
    "end": "1056730"
  },
  {
    "text": "and times the probability of E_2 equals whatever I observe which is 2,",
    "start": "1056730",
    "end": "1062610"
  },
  {
    "text": "given H_2 equals 1. And similarly over here I have probability of H_3 equals 1 given",
    "start": "1062610",
    "end": "1069450"
  },
  {
    "text": "H_2 equals 1 times the probability of E_3 equals 1,",
    "start": "1069450",
    "end": "1074745"
  },
  {
    "text": "which is whatever I observed, given H_3 equals 1. And then over here there's no more factors, uh, left,",
    "start": "1074745",
    "end": "1081540"
  },
  {
    "text": "so I'm just going to put 1 there, okay? And you can check that when I traverse this path and I",
    "start": "1081540",
    "end": "1087630"
  },
  {
    "text": "multiply all these probabilities together, that's exactly this expression for H_1 equals 1,",
    "start": "1087630",
    "end": "1095040"
  },
  {
    "text": "H_2 equals 1, H_3 equals 1. E_1 equals 1, E_2 equals 2, and E_3 equals 1.",
    "start": "1095040",
    "end": "1101710"
  },
  {
    "text": "And for each of these edges, I have an analogous quantity depending on the values that I'm dealing with, okay?",
    "start": "1102410",
    "end": "1113040"
  },
  {
    "text": "Any questions about this basic idea? So in the slides, this is basically what I just said.",
    "start": "1113040",
    "end": "1121090"
  },
  {
    "text": "Okay. So- so now, um, now what I am trying to do now is to- let's say I'm interested in,",
    "start": "1129740",
    "end": "1139875"
  },
  {
    "text": "um, you know, smoothing. So I'm interested in what is the probability of H_3 equals 2, right?",
    "start": "1139875",
    "end": "1148515"
  },
  {
    "text": "Actually, let's- let's do the example on the board just- just, uh, because that's the one I'll actually do.",
    "start": "1148515",
    "end": "1154680"
  },
  {
    "text": "So suppose I'm interested in the probability of H_2 equals,",
    "start": "1154680",
    "end": "1159795"
  },
  {
    "text": "uh, let's say 2 given the evidence. E_2 equals 2, E_3 equals 1.",
    "start": "1159795",
    "end": "1167170"
  },
  {
    "text": "Okay, so this is the query I'm interested in, you know, computing.",
    "start": "1167170",
    "end": "1172770"
  },
  {
    "text": "Okay. So how can I interpret this quantity in terms of this lattice, right?",
    "start": "1172770",
    "end": "1182520"
  },
  {
    "text": "So this is- there is this H_2 equals 2 here, right?",
    "start": "1182520",
    "end": "1188530"
  },
  {
    "text": "That's somehow privileged and then asking, you know what is the probability of this given the evidence? Yeah.",
    "start": "1188630",
    "end": "1200480"
  },
  {
    "text": "Sum over all the probabilities [inaudible]. Yeah, so sum over all the probabilities of the paths, right?",
    "start": "1200480",
    "end": "1211125"
  },
  {
    "text": "So remember every path through from start to end is an assignment.",
    "start": "1211125",
    "end": "1217125"
  },
  {
    "text": "Some of those paths go through this node which means that H_2 equals 2, and some of them don't,",
    "start": "1217125",
    "end": "1222315"
  },
  {
    "text": "which means that it's not true, right? So if you look at all those paths and you sum up",
    "start": "1222315",
    "end": "1229650"
  },
  {
    "text": "their weights of this node and divide by the sum over all paths,",
    "start": "1229650",
    "end": "1235530"
  },
  {
    "text": "then you get the probability of H_2 equals to 2, given the evidence.",
    "start": "1235530",
    "end": "1241290"
  },
  {
    "text": "Okay. So let me just write this. This is going to be, uh,",
    "start": "1242840",
    "end": "1247935"
  },
  {
    "text": "sum over, um, colloquially, sum over paths through, um,",
    "start": "1247935",
    "end": "1256530"
  },
  {
    "text": "each two equals 2 divided by sum of over all paths.",
    "start": "1256530",
    "end": "1263535"
  },
  {
    "text": "Okay. So now, the problem is to compute the sum over all paths going through,",
    "start": "1263535",
    "end": "1270360"
  },
  {
    "text": "um, h_2 or not going through h_2. Okay, again we don't want to sum over all the paths literally",
    "start": "1270360",
    "end": "1276960"
  },
  {
    "text": "because that's going to be, um, exponential time. So how can we do this? Yeah.",
    "start": "1276960",
    "end": "1282360"
  },
  {
    "text": "When you say sum, do you mean sum of the weights or some of the counts like how many [inaudible]? Right. So what I mean by sum,",
    "start": "1282360",
    "end": "1288195"
  },
  {
    "text": "I mean sum of the weights. So every path has a weight which is the product of the weights on the edges and you sum of the- those weights, yeah.",
    "start": "1288195",
    "end": "1297570"
  },
  {
    "text": "Okay, so what's an idea that we can use to compute the,",
    "start": "1297570",
    "end": "1302804"
  },
  {
    "text": "the sum efficiently? Sum, key word.",
    "start": "1302805",
    "end": "1308085"
  },
  {
    "text": "Dynamic programming. Yeah, dynamic programming. Okay, so this, um, we're gonna do this kind of recursively.",
    "start": "1308085",
    "end": "1315030"
  },
  {
    "text": "Um, let me just show this slide. So, er, it's gonna be a little bit different from the diamond programming that we saw,",
    "start": "1315220",
    "end": "1322555"
  },
  {
    "text": "er, before it's gonna be, um, more general because we're not computing and let's say, ah,",
    "start": "1322555",
    "end": "1328730"
  },
  {
    "text": "one particular query but I'm going to compute a bunch of quantities are gonna allow us to compute all queries essentially.",
    "start": "1328730",
    "end": "1335779"
  },
  {
    "text": "Um, okay, so let's- there's going to be three quantities I'm gonna look at,",
    "start": "1335780",
    "end": "1343800"
  },
  {
    "text": "um, and hopefully I can [NOISE] kind of out of colors but,",
    "start": "1343800",
    "end": "1349740"
  },
  {
    "text": "um, let's not use green for this then. So there is going to be, um,",
    "start": "1349740",
    "end": "1355575"
  },
  {
    "text": "you know, forward messages, um, F which I'll explain a bit.",
    "start": "1355575",
    "end": "1361500"
  },
  {
    "text": "There's going to be backward, um, messages B. Okay, so what I want to do is,",
    "start": "1361500",
    "end": "1371039"
  },
  {
    "text": "ah, for every node I'm gonna compute two numbers. Yeah, question? No.",
    "start": "1371040",
    "end": "1376410"
  },
  {
    "text": "Um, I have- I think I'm okay. What color is that? Um, sure I'll take a blue marker, yeah.",
    "start": "1376410",
    "end": "1382155"
  },
  {
    "text": "Great. Thanks. Okay great.",
    "start": "1382155",
    "end": "1388425"
  },
  {
    "text": "So, um, let's call this S, okay. Um, okay so, um,",
    "start": "1388425",
    "end": "1396105"
  },
  {
    "text": "so for every node I'm gonna compute two numbers. One number we'll call the forward number is- or the forward message is going to",
    "start": "1396105",
    "end": "1404240"
  },
  {
    "text": "be the sum over all the weights of the partial paths going into that node. And the orange number is going to",
    "start": "1404240",
    "end": "1412040"
  },
  {
    "text": "be the sum of all the partial paths from that node to the end. Okay. So let's, let's,",
    "start": "1412040",
    "end": "1419265"
  },
  {
    "text": "um, so these are all meant to be probability. So the number should be less than 1 but just to keep things simple,",
    "start": "1419265",
    "end": "1428160"
  },
  {
    "text": "I'm gonna put actual numbers on these just two, um, uh, just two integers on them, so they don't have to carry around decimal points.",
    "start": "1428160",
    "end": "1439664"
  },
  {
    "text": "Okay, so this is one, let's say, one, two, one, um, one, two, one,",
    "start": "1439665",
    "end": "1446820"
  },
  {
    "text": "uh, one, two and one. Okay, so remember every edge has a weight associated with it.",
    "start": "1446820",
    "end": "1454890"
  },
  {
    "text": "And so now let me compute, um, the forward probability. So what is the sum of all paths going into that node?",
    "start": "1454890",
    "end": "1462570"
  },
  {
    "text": "It's just 1, right? Okay, so, ah, sorry forward is green.",
    "start": "1462570",
    "end": "1467955"
  },
  {
    "text": "So one and this is two, okay just copying this.",
    "start": "1467955",
    "end": "1473085"
  },
  {
    "text": "And now recursively what is the p- um, sum of all the weights going into this?",
    "start": "1473085",
    "end": "1480075"
  },
  {
    "text": "Okay, So, um, I could have come from here or I could have come from here.",
    "start": "1480075",
    "end": "1485325"
  },
  {
    "text": "Right, so if I come from here it's one times, um, whatever the sum was there.",
    "start": "1485325",
    "end": "1491130"
  },
  {
    "text": "So that's 1 times 1, that's 1, ah, plus 2 times 2.",
    "start": "1491130",
    "end": "1496395"
  },
  {
    "text": "Okay, so I'm gonna get a 5. 1 plus 4. And here, I'm going to have, ah,",
    "start": "1496395",
    "end": "1503580"
  },
  {
    "text": "1 times 1 so that's 1, 1 times 2, that's 2. So that's going to be 3.",
    "start": "1503580",
    "end": "1509205"
  },
  {
    "text": "Hopefully, you guys are checking my math here. Um, so what about this node? So now recursively this could have-",
    "start": "1509205",
    "end": "1516149"
  },
  {
    "text": "the paths going in here could have come from this node or that node. So that's 2 times 5, so that's 10.",
    "start": "1516150",
    "end": "1523950"
  },
  {
    "text": "1 times 3 and that's, ah, 3. So that's 13.",
    "start": "1523950",
    "end": "1529429"
  },
  {
    "text": "And this is, uh, 1 times 5 plus, uh, 3 times 2.",
    "start": "1529430",
    "end": "1535760"
  },
  {
    "text": "So that's, um, 11. Right, okay. So 13 represents the sum over all paths going into H_3 equals 1, okay?",
    "start": "1535760",
    "end": "1547380"
  },
  {
    "text": "So I can do the backward direction [NOISE]. So, um, so this is going to be, ah, in orange,",
    "start": "1547380",
    "end": "1556020"
  },
  {
    "text": "the backward messages which are, um, paths going to the n. So this is gonna be 1.",
    "start": "1556020",
    "end": "1562559"
  },
  {
    "text": "So here, I'm going to have, ah- so 2 times 1 plus 1 times 1.",
    "start": "1562560",
    "end": "1571065"
  },
  {
    "text": "So that's a 3. This is 1 times 1, 2 times 1.",
    "start": "1571065",
    "end": "1576105"
  },
  {
    "text": "So that's a 3 as well. Ah, this is 1 times 3,",
    "start": "1576105",
    "end": "1581730"
  },
  {
    "text": "ah, 1 times 3. So that's a 6. This is 2 times,",
    "start": "1581730",
    "end": "1587054"
  },
  {
    "text": "that's a 6 and a 3. So that's a 9, ah, and then I'm, you know, done.",
    "start": "1587055",
    "end": "1592755"
  },
  {
    "text": "Okay. Okay. Does that make all sense?",
    "start": "1592755",
    "end": "1598275"
  },
  {
    "text": "So these are kind of compact representations over the essentially the flow in and out of,",
    "start": "1598275",
    "end": "1604470"
  },
  {
    "text": "ah, these lattice nodes. Okay, so the, the kind of the s- kind of magic happens,",
    "start": "1604470",
    "end": "1610965"
  },
  {
    "text": "um, when I have, um, these axes. So now for every node,",
    "start": "1610965",
    "end": "1616440"
  },
  {
    "text": "I'm going to also just multiply them together, okay? So that's gonna be 6,",
    "start": "1616440",
    "end": "1622109"
  },
  {
    "text": "uh, 18, um, 18, as 9, 13, 11 and that's it, okay?",
    "start": "1622109",
    "end": "1632220"
  },
  {
    "text": "So what happens when I multiply them together? Let's take another look at this node, right? So what does 9 represent?",
    "start": "1632220",
    "end": "1640170"
  },
  {
    "text": "9 represents the sum over all the paths going through here,",
    "start": "1640170",
    "end": "1646740"
  },
  {
    "text": "right because I can take whatever paths I have coming in and I can take whatever paths I have",
    "start": "1646740",
    "end": "1652290"
  },
  {
    "text": "going out and any sort of combination of them will be a valid end to end path.",
    "start": "1652290",
    "end": "1658710"
  },
  {
    "text": "Okay, and so this total weight is, you know, 9 there. Yeah?",
    "start": "1658710",
    "end": "1663975"
  },
  {
    "text": "Why instead of sum we multiply for this case? So why do we multiply instead of sum here?",
    "start": "1663975",
    "end": "1669660"
  },
  {
    "text": "Um, because we're multiplying, the weight of a path is the product, okay?",
    "start": "1669660",
    "end": "1675640"
  },
  {
    "text": "Mathematically, what's going on is, um, exactly factoring.",
    "start": "1675740",
    "end": "1681750"
  },
  {
    "text": "Right, so I suppose I had numbers, let's say a, um,",
    "start": "1681750",
    "end": "1687015"
  },
  {
    "text": "b and c and d and I could choose a and b and c and",
    "start": "1687015",
    "end": "1692070"
  },
  {
    "text": "d. So what are the possible- so then I can do a plus b times c plus d, right?",
    "start": "1692070",
    "end": "1698820"
  },
  {
    "text": "Which is the sum over all possible paths and, uh, you can- thus paths are either ac,",
    "start": "1698820",
    "end": "1705404"
  },
  {
    "text": "uh, ad, bc and bd. Right, so I'm basically doing this th-",
    "start": "1705404",
    "end": "1712424"
  },
  {
    "text": "computing it in a factorized way rather than expanding out. That's mathematically what's going on when",
    "start": "1712425",
    "end": "1717809"
  },
  {
    "text": "I multiply the forward and the backward messages. And why are these called messages?",
    "start": "1717810",
    "end": "1725100"
  },
  {
    "text": "So the idea of messages, uh, comes from the fact that you can intuitively think about",
    "start": "1725100",
    "end": "1730515"
  },
  {
    "text": "the forward messages as being kind of sent across the graph, right? Because the message here depends only on the neighbors here.",
    "start": "1730515",
    "end": "1737460"
  },
  {
    "text": "And once I get these messages, I can compute the f- the- my messages are next time-step based on that.",
    "start": "1737460",
    "end": "1743655"
  },
  {
    "text": "So it's kind of a summary of what's going on and I ca- I can send the messages forward and same in the backward direction.",
    "start": "1743655",
    "end": "1752019"
  },
  {
    "text": "Okay, so now once I have these values, uh, how do I go back and compute my query?",
    "start": "1752360",
    "end": "1760150"
  },
  {
    "text": "Sum over all paths through h_2 equals 2. What is that?",
    "start": "1764060",
    "end": "1770925"
  },
  {
    "text": "9, right. And over the sum of all our paths,",
    "start": "1770925",
    "end": "1777645"
  },
  {
    "text": "what's the sum over all paths?",
    "start": "1777645",
    "end": "1780040"
  },
  {
    "text": "Sorry, this should be 15. I was wondering, did I screw anything else up? I think that's right.",
    "start": "1792920",
    "end": "1802395"
  },
  {
    "text": "I was checking because you know, when if you sum these two numbers you get 24 which is all of the sum of all paths",
    "start": "1802395",
    "end": "1809070"
  },
  {
    "text": "going through here and that better be the same number here and also here would be the same number there, right? Okay. Someone that should have caught that.",
    "start": "1809070",
    "end": "1816300"
  },
  {
    "text": "Okay, um, all right, so these are all the paths going through nine,",
    "start": "1816300",
    "end": "1821385"
  },
  {
    "text": "er- oh, sorry, going through this node. And, um, if you look at all paths,",
    "start": "1821385",
    "end": "1828110"
  },
  {
    "text": "that's going to be 15 plus 9, and that's going to be 24. Okay, so final answer is a probability of h_2 equals 2 given these made-up,",
    "start": "1828110",
    "end": "1839580"
  },
  {
    "text": "uh, weights is going to be 9 over 24.",
    "start": "1839580",
    "end": "1843370"
  },
  {
    "text": "Okay, any questions about that? [NOISE]",
    "start": "1845000",
    "end": "1859590"
  },
  {
    "text": "Okay, so le- let me just quickly go over the slides which is gonna be a more mathematical treatment of what I did on the board.",
    "start": "1859590",
    "end": "1866925"
  },
  {
    "text": "Hopefully, one of the ways will resonate with you. So define the forward messages for every node is going to be a sum over all of",
    "start": "1866925",
    "end": "1876300"
  },
  {
    "start": "1872000",
    "end": "2056000"
  },
  {
    "text": "the values at the previous time steps of the forward message at our previous timesteps times the weight on the edge from,",
    "start": "1876300",
    "end": "1883715"
  },
  {
    "text": "uh, the previous value to the, the current value? Um, the backward is gonna be defined similarly for every node.",
    "start": "1883715",
    "end": "1891230"
  },
  {
    "text": "Sum over all the values assigned to the- at the next time step, all outgoing edges, um,",
    "start": "1891230",
    "end": "1897020"
  },
  {
    "text": "of the backward message at the next time step times the weight into that next time step.",
    "start": "1897020",
    "end": "1902330"
  },
  {
    "text": "And then define S as simply just the product of F and B.",
    "start": "1902330",
    "end": "1907610"
  },
  {
    "text": "Okay, so that's what I did on the board. And then finally, if you normalize the sum at each point in time,",
    "start": "1907610",
    "end": "1915120"
  },
  {
    "text": "you can get the, uh, distribution over the hidden variable or given all the evidence.",
    "start": "1915120",
    "end": "1920830"
  },
  {
    "text": "And to summarize the algorithm, the forward-backward algorithm, this is actually a very old algorithm, um,",
    "start": "1922640",
    "end": "1930960"
  },
  {
    "text": "developed actually for, ah, for speech recognition, a while back.",
    "start": "1930960",
    "end": "1936420"
  },
  {
    "text": "I think it's, you know, probably in the '60s or so. Um, so you sweep forward, you compute all the forward messages,",
    "start": "1936420",
    "end": "1944115"
  },
  {
    "text": "and then you sweep backwards and compute all the backward messages, and then for every position,",
    "start": "1944115",
    "end": "1950610"
  },
  {
    "text": "you compute S_i for each i and you normalize. So the output of this algorithm is not just the answer to one query,",
    "start": "1950610",
    "end": "1957299"
  },
  {
    "text": "but all the smoothing queries you want. Because at every position,",
    "start": "1957300",
    "end": "1962550"
  },
  {
    "text": "you have the distribution over the, um, the hidden variable h_i.",
    "start": "1962550",
    "end": "1968520"
  },
  {
    "text": "And the running time is n times, ah, k squared, ah,",
    "start": "1968520",
    "end": "1974640"
  },
  {
    "text": "because there's n time steps and every time step you have to compute the sum.",
    "start": "1974640",
    "end": "1980460"
  },
  {
    "text": "So for a k possible values here, you look at k possible values there. So that's a k squared, and it's n times k squared.",
    "start": "1980460",
    "end": "1988140"
  },
  {
    "text": "Interestingly if you ask, okay, what's the cost of computing a single query?",
    "start": "1988140",
    "end": "1994755"
  },
  {
    "text": "It would also be n times k squared. So it's kind of cool that you compute all the queries in the same time that it takes to compute",
    "start": "1994755",
    "end": "2001820"
  },
  {
    "text": "a single query. Okay. Question?",
    "start": "2001820",
    "end": "2007070"
  },
  {
    "text": "[inaudible]. So, uh, question is does this only work for Hidden Markov Models or is it more general?",
    "start": "2007070",
    "end": "2015799"
  },
  {
    "text": "There's certainly adaptations of this which work, uh, very naturally for other types of networks.",
    "start": "2015800",
    "end": "2021965"
  },
  {
    "text": "And one immediate generalization is if you have not just a chain structure, but you have a tree structure then the idea of passing messages along that tree, um,",
    "start": "2021965",
    "end": "2030500"
  },
  {
    "text": "it's called belief propagation, um, is, uh, just works pretty much out of the box.",
    "start": "2030500",
    "end": "2036245"
  },
  {
    "text": "For arbitrary Bayesian networks this won't work because, ah, once you have cycles,",
    "start": "2036245",
    "end": "2041705"
  },
  {
    "text": "then you can't represent it as a lattice anymore.",
    "start": "2041705",
    "end": "2045450"
  },
  {
    "text": "Any other questions? Okay. So to summarize, this lattice representation,",
    "start": "2047950",
    "end": "2058115"
  },
  {
    "start": "2056000",
    "end": "2138000"
  },
  {
    "text": "uh, allows us to think of paths as assignments, which is a familiar idea if we're thinking about state-based models.",
    "start": "2058115",
    "end": "2065500"
  },
  {
    "text": "Um, we can use the idea of dynamic programming to compute the sums efficiently but we're doing",
    "start": "2065500",
    "end": "2071089"
  },
  {
    "text": "this extra thing where we're computing all of the sums, um, for all the queries at once. And the forward-backward algorithm, uh,",
    "start": "2071090",
    "end": "2077869"
  },
  {
    "text": "allows you to share intermediate computation across the different queries. Yeah.",
    "start": "2077870",
    "end": "2083120"
  },
  {
    "text": "[inaudible]",
    "start": "2083120",
    "end": "2088460"
  },
  {
    "text": "So the output of this algorithm is, uh, basically the probability of h_i given all the evidence, for every i.",
    "start": "2088460",
    "end": "2096260"
  },
  {
    "text": "[inaudible]. [NOISE].",
    "start": "2096260",
    "end": "2105050"
  },
  {
    "text": "Oh, so how would you actually use this, do you sample from it? Um, depends on what you want to do with it. So the output of this you can think about it as a distribution at each time step.",
    "start": "2105050",
    "end": "2115280"
  },
  {
    "text": "So it's like n by k matrix of probabilities, right? From that you can sample if you want, uh,",
    "start": "2115280",
    "end": "2121805"
  },
  {
    "text": "you're gonna- you might be only interested in only, uh, various points in time.",
    "start": "2121805",
    "end": "2127655"
  },
  {
    "text": "Um, it's yeah. Okay. So, uh,",
    "start": "2127655",
    "end": "2136805"
  },
  {
    "text": "let's move on to the second algorithm which is called particle filtering. Um, so we're interested still in Hidden Markov Models,",
    "start": "2136805",
    "end": "2146119"
  },
  {
    "start": "2138000",
    "end": "2464000"
  },
  {
    "text": "um, or the particle filtering again is something actually much more general than that.",
    "start": "2146120",
    "end": "2151280"
  },
  {
    "text": "Um, and we're going to only focus on query- filtering questions. So we're doing our filtering,",
    "start": "2151280",
    "end": "2157115"
  },
  {
    "text": "we're at a particular time step, we're only interested in the probability of the hidden variable at that timestep computation on the past.",
    "start": "2157115",
    "end": "2165890"
  },
  {
    "text": "And why might, um, we not be satisfied with,",
    "start": "2165890",
    "end": "2170990"
  },
  {
    "text": "um, Hidden Markov or the forward-backward algorithm?",
    "start": "2170990",
    "end": "2176345"
  },
  {
    "text": "So here's the motivating picture. So imagine, um, you're doing, uh,",
    "start": "2176345",
    "end": "2182735"
  },
  {
    "text": "the car assignment, um, let's say, and you're, so you're tracking cars. Okay? So cars let's say live on a huge grid, um,",
    "start": "2182735",
    "end": "2193519"
  },
  {
    "text": "so at each position h_i, um, the value of h_i is some point on this grid.",
    "start": "2193520",
    "end": "2201289"
  },
  {
    "text": "But you don't know where it is, you want to track it. Okay. So if this is like 100 by 100, you know,",
    "start": "2201290",
    "end": "2208100"
  },
  {
    "text": "that's 10,000, um, if this were a thing where it continuously would be even, you know, worse.",
    "start": "2208100",
    "end": "2213395"
  },
  {
    "text": "Uh, so this, um, this k squared where k is the number of values",
    "start": "2213395",
    "end": "2219200"
  },
  {
    "text": "could be like 10,000 squared and that's a large number. Right? So, um, even though hidden Markov model with backward,",
    "start": "2219200",
    "end": "2228755"
  },
  {
    "text": "forward-backward is not exponential time, even the quadratic can be pretty expensive.",
    "start": "2228755",
    "end": "2235770"
  },
  {
    "text": "And in the further the, motivation is, you know, you- and you really shouldn't have to pay that much, right?",
    "start": "2235810",
    "end": "2243860"
  },
  {
    "text": "Because let's say your sensor tells you that, oh the car is up here somewhere. And you know cars can't move all the way across here.",
    "start": "2243860",
    "end": "2251975"
  },
  {
    "text": "So then, you know, but the algorithm is going to consider each of these possibilities and most of",
    "start": "2251975",
    "end": "2259355"
  },
  {
    "text": "all these probabilities are gonna have pretty much 0 probability. So that's really wasteful to consider all of them.",
    "start": "2259355",
    "end": "2265310"
  },
  {
    "text": "So can we somehow focus our energies on the region that, um, have actual high probability? Yeah, question?",
    "start": "2265310",
    "end": "2272630"
  },
  {
    "text": "Is that way of like saying do you think of backwards like for the later timesteps you can't have 0 in one of those positions in the original table?",
    "start": "2272630",
    "end": "2281345"
  },
  {
    "text": "The question is can you go backwards. Like if you can, like if you're continuing to do it one way and",
    "start": "2281345",
    "end": "2287810"
  },
  {
    "text": "you say like it's very unlikely that I'm gonna go back to the starting position, do- do each of those variables happen in the same domain, have to say?",
    "start": "2287810",
    "end": "2294125"
  },
  {
    "text": "Oh so each of these variables, they don't have to be from the same, um, domain.",
    "start": "2294125",
    "end": "2300125"
  },
  {
    "text": "For this presentation, they're in the same domain just for simplicity. Um, but I think what you're asking is you know that maybe,",
    "start": "2300125",
    "end": "2308540"
  },
  {
    "text": "um, a car only moves let's say forward or something. Then there is some restriction on the domain.",
    "start": "2308540",
    "end": "2315020"
  },
  {
    "text": "Um, it's not gonna be that significant because you still don't know where the car is.",
    "start": "2315020",
    "end": "2320930"
  },
  {
    "text": "So, uh, it doesn't really cut out that many possibilities.",
    "start": "2320930",
    "end": "2326315"
  },
  {
    "text": "Yeah, maybe by a factor of 2 or something but that's not, um, that significant. Yeah. Okay, so how do we go about making this a little bit more efficient?",
    "start": "2326315",
    "end": "2338045"
  },
  {
    "text": "Um, so let's look at beam search. So our final algorithm is not gonna be beam search,",
    "start": "2338045",
    "end": "2345050"
  },
  {
    "text": "it's gonna be particle filtering but beam search is gonna give us some inspiration. So remember in beam search we keep a set of k candidates of partial assignments,",
    "start": "2345050",
    "end": "2356285"
  },
  {
    "text": "um, and algorithm as follows. You start with a, a single empty assignment, and then for every, um,",
    "start": "2356285",
    "end": "2362540"
  },
  {
    "text": "position time step, um, I'm going to consider all the candidates which are",
    "start": "2362540",
    "end": "2368570"
  },
  {
    "text": "assignments to the fi- first i minus 1 variables. I'm gonna extend it.",
    "start": "2368570",
    "end": "2373850"
  },
  {
    "text": "There's possible ways of extending in our setting h_i to v from any v in the domain of i. Um,",
    "start": "2373850",
    "end": "2382820"
  },
  {
    "text": "so now I'm going to amass this, uh, set of, um,",
    "start": "2382820",
    "end": "2388205"
  },
  {
    "text": "extended assignments, um, now have k times as many because each previous assignment got expanded by k. So I'm gonna prune down.",
    "start": "2388205",
    "end": "2397655"
  },
  {
    "text": "I'm just going to take all of them, sort them by weight and take the- the top k. Okay?",
    "start": "2397655",
    "end": "2404360"
  },
  {
    "text": "So visually, um, remember from last time it looks like this. So here is, uh,",
    "start": "2404360",
    "end": "2411425"
  },
  {
    "text": "this object tracking, um, where we have five variables and you start with beam search which is, um,",
    "start": "2411425",
    "end": "2419345"
  },
  {
    "text": "assigning X_1 to 0, um, or 1 and then, um, you extend it.",
    "start": "2419345",
    "end": "2425870"
  },
  {
    "text": "So you extend the assignments, you prune down, you extend assignments, you prune down, you extend the assignments and prune down, and so on.",
    "start": "2425870",
    "end": "2434255"
  },
  {
    "text": "And at the end of the day, you get k candidates. Each candidate is, uh, full assignment to all the variables and it has a particular weight,",
    "start": "2434255",
    "end": "2442835"
  },
  {
    "text": "which is its actual weight. And at each intermediate time,",
    "start": "2442835",
    "end": "2448370"
  },
  {
    "text": "it's a partial assignment to only the prefix of, uh, i random variables.",
    "start": "2448370",
    "end": "2453920"
  },
  {
    "text": "Okay? And remember that beam search doesn't have any guarantees. It's just a heuristic, but it, uh, often it works well in practice.",
    "start": "2453920",
    "end": "2460915"
  },
  {
    "text": "And the pict- picture you should have in your head is that you have the exponentially sized tree of",
    "start": "2460915",
    "end": "2466360"
  },
  {
    "start": "2464000",
    "end": "2689000"
  },
  {
    "text": "all possible assignments and beam search is kind of this pruned, uh, breadth-first search along this tree which",
    "start": "2466360",
    "end": "2473020"
  },
  {
    "text": "only looks at promising directions and continues, um, so you don't have to keep track of all of them.",
    "start": "2473020",
    "end": "2479270"
  },
  {
    "text": "Okay. So at the end, um, you can use beam search,",
    "start": "2480030",
    "end": "2485755"
  },
  {
    "text": "you get a set of candidates which are full assignments to all the random variables and you can compute,",
    "start": "2485755",
    "end": "2495175"
  },
  {
    "text": "uh, any quantities you want. Um, uh so the problem with this is that it's slow.",
    "start": "2495175",
    "end": "2505730"
  },
  {
    "text": "Um, for the same reasons as I described, uh, before it requires considering every possible value of, um, H_i.",
    "start": "2505730",
    "end": "2515395"
  },
  {
    "text": "So it's a little bit better than forward-backward, right? So for forward-backward, um, you have to have the domain size times the domain size and now for beam search,",
    "start": "2515395",
    "end": "2525430"
  },
  {
    "text": "um, it's the size of the beam times the domain size, you know, which is better.",
    "start": "2525430",
    "end": "2530515"
  },
  {
    "text": "But- but still I think we can do a little bit better than that. Um, and finally there's this kind of more subtle point is that, um,",
    "start": "2530515",
    "end": "2538265"
  },
  {
    "text": "as we'll see later really taking the best k might not be the best thing to do because you want some- maintain some diversity.",
    "start": "2538265",
    "end": "2546005"
  },
  {
    "text": "Right. Just a kind of a, uh, quick visual. So suppose you, um,",
    "start": "2546005",
    "end": "2551850"
  },
  {
    "text": "your beam consists of only cars over here. It's kind of a little bit redundant but you might want,",
    "start": "2551850",
    "end": "2557995"
  },
  {
    "text": "uh, kind of a broader representation. Okay. So the idea with particle filtering is to just tweak beam search a little bit,",
    "start": "2557995",
    "end": "2568954"
  },
  {
    "text": "um, and this is going to be expanded into three steps which I'll, um, talk about.",
    "start": "2568955",
    "end": "2574610"
  },
  {
    "text": "Okay, so let me, um.",
    "start": "2574610",
    "end": "2580000"
  },
  {
    "text": "Does anyone need this on the board? Can I erase it? Okay. We're good. [NOISE] Anyway.",
    "start": "2580000",
    "end": "2586005"
  },
  {
    "text": "You can look at video if you ah, don't remember. [NOISE] Alright.",
    "start": "2586005",
    "end": "2591240"
  },
  {
    "text": "So there's three steps here. Okay. And we're gonna try to do this ah, pictorially over here.",
    "start": "2591240",
    "end": "2597210"
  },
  {
    "text": "Um, [NOISE] so, so the idea behind particle f- filtering is,",
    "start": "2597210",
    "end": "2602400"
  },
  {
    "text": "I'm going to maintain a set of particles that kind of represent where I think the object, ah is.",
    "start": "2602400",
    "end": "2608475"
  },
  {
    "text": "So imagine [NOISE] um, [NOISE] you know the object starts over here somewhere. So you have a set of particles,",
    "start": "2608475",
    "end": "2614519"
  },
  {
    "text": "and I'm gonna iteratively go through these three steps. So propose, [NOISE] um,",
    "start": "2614519",
    "end": "2621790"
  },
  {
    "text": "w- w- w- weight,",
    "start": "2621790",
    "end": "2627950"
  },
  {
    "text": "[NOISE] and um, ah re-sample [NOISE]. So this is meant to be kind of a replacement of the extend-prune uh,",
    "start": "2627950",
    "end": "2636420"
  },
  {
    "text": "strategy for Beam search. Okay, so the first, step is to propose.",
    "start": "2636420",
    "end": "2643020"
  },
  {
    "text": "So at any point and time particle filtering maintains a set of partial assignments,",
    "start": "2643020",
    "end": "2649620"
  },
  {
    "text": "known as particles that kinda tries to mimic a particular distribution.",
    "start": "2649620",
    "end": "2654960"
  },
  {
    "text": "So um, to kind of jumping to the second time step. We can think about this,",
    "start": "2654960",
    "end": "2660270"
  },
  {
    "text": "set of particles as representing the probability of H1 and H2 given the evidence, you know, so far.",
    "start": "2660270",
    "end": "2667244"
  },
  {
    "text": "Okay. Um on the board, I'm only gonna draw the, ah the particle representing the value H2,",
    "start": "2667245",
    "end": "2675225"
  },
  {
    "text": "um because it- it's hard to draw trajectories but you can think about um, ah really particle filtering maintains this lineage as well.",
    "start": "2675225",
    "end": "2684570"
  },
  {
    "text": "Okay, so the key idea of the proposal distribution is that, okay, we want to advance time, now.",
    "start": "2684570",
    "end": "2691230"
  },
  {
    "start": "2689000",
    "end": "2764000"
  },
  {
    "text": "So um, we're interested in H3 but we only have H2,",
    "start": "2691230",
    "end": "2696825"
  },
  {
    "text": "so how do we figure out where H3 is? So we, propose possible ah,",
    "start": "2696825",
    "end": "2704280"
  },
  {
    "text": "values of H3 based on H2. So this is idea of proposal. We just simply draw H3 from this transition distribution.",
    "start": "2704280",
    "end": "2712335"
  },
  {
    "text": "So this- remember this distribution is- comes from an HMM. This is d- you're given HMM, so you can do this.",
    "start": "2712335",
    "end": "2718695"
  },
  {
    "text": "Um, and this gives us a set of new particles which are now extended by 1. And this represents the ah,",
    "start": "2718695",
    "end": "2725280"
  },
  {
    "text": "distribution H1, H2, H3, given the same evidence. Okay? So pictorially, wh- ah,",
    "start": "2725280",
    "end": "2732619"
  },
  {
    "text": "you should think about propose as um, [NOISE] So propose is kind of taking",
    "start": "2732620",
    "end": "2739830"
  },
  {
    "text": "each of these particles and sampling according to th- the transition. So think about the particles as um,",
    "start": "2739830",
    "end": "2747510"
  },
  {
    "text": "you know ah, just moving in some direction. It's almost like simulating where,",
    "start": "2747510",
    "end": "2753960"
  },
  {
    "text": "you know cars are, are, are going. And this is done in kind of stochastically and randomly.",
    "start": "2753960",
    "end": "2760080"
  },
  {
    "text": "Okay. Um, step two is to wait.",
    "start": "2760080",
    "end": "2766875"
  },
  {
    "start": "2764000",
    "end": "3013000"
  },
  {
    "text": "So, so far the new locations really don't represent ah, reality.",
    "start": "2766875",
    "end": "2772980"
  },
  {
    "text": "Right? Because we also see E3. At timestep three we get a new observation that",
    "start": "2772980",
    "end": "2778710"
  },
  {
    "text": "hasn't been incorporated somehow, into this. We're just kind of s- simulating what ah, might happen um,",
    "start": "2778710",
    "end": "2786720"
  },
  {
    "text": "and so the idea here behind a weighting is for each of those particles,",
    "start": "2786720",
    "end": "2792630"
  },
  {
    "text": "we're gonna assign a weight now, which is equal to, the mission distribution over E3 given, you know H3.",
    "start": "2792630",
    "end": "2800234"
  },
  {
    "text": "Again, this is emission distribution which is given by our HMM so we can just evaluate ah, it whenever we feel like it.",
    "start": "2800235",
    "end": "2806730"
  },
  {
    "text": "And the set of new particles w- which are weighted, can be thought of representing this distribution where now we",
    "start": "2806730",
    "end": "2813960"
  },
  {
    "text": "have condition on um, E3 equals 1. Okay, so now each of these particles has some, you know weight.",
    "start": "2813960",
    "end": "2821985"
  },
  {
    "text": "So on, on this picture [NOISE] it kind of looks like this, um,",
    "start": "2821985",
    "end": "2828015"
  },
  {
    "text": "so maybe let's say the ah, emission distribution is kind of let's say um,",
    "start": "2828015",
    "end": "2834285"
  },
  {
    "text": "ah, let's say a Gaussian distribution around th- the observation. So suppose, the observation tells you,",
    "start": "2834285",
    "end": "2840510"
  },
  {
    "text": "well, it's over here somewhere, um, which means that these particles are gonna ge- get higher weight and these particles are gonna get lower weight.",
    "start": "2840510",
    "end": "2848984"
  },
  {
    "text": "Um, and if they're far away enough then maybe they get like almost zero weight. So um, I, I'm going to kind of softly [NOISE] x [NOISE] these out.",
    "start": "2848985",
    "end": "2858510"
  },
  {
    "text": "So think about these [NOISE] as um, okay. Mayb- maybe I'll do this. So I'll upweight these, um,",
    "start": "2858510",
    "end": "2863730"
  },
  {
    "text": "[NOISE] and kind of start downweighting them. [NOISE] So nothing really gets ah, zeroed out.",
    "start": "2863730",
    "end": "2872670"
  },
  {
    "text": "But you can think about these as downweighting and these as upweighting provided [NOISE] you have let's say some evidence um,",
    "start": "2872670",
    "end": "2879285"
  },
  {
    "text": "E3 that tells you, your- you're going in that direction. Okay. Okay so the final step is,",
    "start": "2879285",
    "end": "2890340"
  },
  {
    "text": "is really about a resource distribution question. Um, so now we have weighted particles,",
    "start": "2890340",
    "end": "2896845"
  },
  {
    "text": "we need to somehow get back to unweighted particles. Okay, so which one to choose?",
    "start": "2896845",
    "end": "2903180"
  },
  {
    "text": "Um, [NOISE] so imagine ah, you have this situation where you have particles which are kind of distributed,",
    "start": "2903370",
    "end": "2911470"
  },
  {
    "text": "the weights of the particles are fairly uniform. Um, then you could imagine let's take just the particles with the highest weight.",
    "start": "2911470",
    "end": "2921960"
  },
  {
    "text": "Right. This is very similar to what Beam Search would do. You just take all the particles with high weight and ah,",
    "start": "2921960",
    "end": "2928935"
  },
  {
    "text": "just keep those and nothing else. Um, so this is not a crazy thing to do but um,",
    "start": "2928935",
    "end": "2937410"
  },
  {
    "text": "it, it might give you an impression that you are more confident than you actually are.",
    "start": "2937410",
    "end": "2942885"
  },
  {
    "text": "Right, because imagine the, the weights are fairly uniform. So maybe this one is like, you know,",
    "start": "2942885",
    "end": "2947940"
  },
  {
    "text": "0.5 and this one is like 0.48. Um, so you're kind of just b- breaking ties in a very biased way.",
    "start": "2947940",
    "end": "2955320"
  },
  {
    "text": "Um, so the idea is that if you sample it and instead sample from this distribution, you're gonna get something a lot more representative rath- rather",
    "start": "2955320",
    "end": "2962640"
  },
  {
    "text": "than just taking um, kind of the best. Okay, so how do you sample from this distribution?",
    "start": "2962640",
    "end": "2969300"
  },
  {
    "text": "Um, so the general idea is, if you have- if I- this is just kind of a useful module to have here having.",
    "start": "2969300",
    "end": "2974985"
  },
  {
    "text": "If I give you a distribution, um, over n possible values,",
    "start": "2974985",
    "end": "2979995"
  },
  {
    "text": "then I'm gonna draw, ah, let's say K samples from this. So if I have the distribution over four possible locations with",
    "start": "2979995",
    "end": "2987030"
  },
  {
    "text": "these probabilities or you know, weights, then um, I might draw off ah, four samples,",
    "start": "2987030",
    "end": "2994275"
  },
  {
    "text": "and I might pick a1 ah, and then a2, and then I may pick a1 and a1.",
    "start": "2994275",
    "end": "3001250"
  },
  {
    "text": "So some of these things are not gonna be chosen ah, if they have sufficiently low probability.",
    "start": "3001250",
    "end": "3007350"
  },
  {
    "text": "Okay. So going back to the particle filtering setting, uh, we have these old particles remembering which are weighted.",
    "start": "3009400",
    "end": "3019220"
  },
  {
    "start": "3013000",
    "end": "3361000"
  },
  {
    "text": "Uh, and first, I'm going to normalize these weights to get a distribution.",
    "start": "3019220",
    "end": "3024740"
  },
  {
    "text": "So add these numbers up and divide by that. And then, I'm going to sample",
    "start": "3024740",
    "end": "3031235"
  },
  {
    "text": "according to the distri- distribution given weights on the previous slide. So I might draw in this case 0, 1, 1 once,",
    "start": "3031235",
    "end": "3039440"
  },
  {
    "text": "and I might draw it again and might not, uh, even keep this particle, right?",
    "start": "3039440",
    "end": "3045589"
  },
  {
    "text": "Um, and the idea here is that suppose a particle has really,",
    "start": "3045590",
    "end": "3051350"
  },
  {
    "text": "really low weight, it has like 0.00001. Then, um, I shouldn't, kind of, keep,",
    "start": "3051350",
    "end": "3059059"
  },
  {
    "text": "um, it around and because it continues to occupy memory and I have to keep track of it. It's basically, you know, gone right.",
    "start": "3059060",
    "end": "3066800"
  },
  {
    "text": "So this re-sampling, kind of, regenerates the pool by focusing their efforts, um, on the higher weight particles.",
    "start": "3066800",
    "end": "3074000"
  },
  {
    "text": "It might even have re-sample a hi- hi- higher weight particle multiple times. Um, um, and then not sample the low weight particles, uh, zero times.",
    "start": "3074000",
    "end": "3083970"
  },
  {
    "text": "Okay. So in this, in this picture here, so re-sampling, uh, might- let's see,",
    "start": "3084880",
    "end": "3092509"
  },
  {
    "text": "uh, how do I, how do I draw this. So maybe, now I have, um, maybe I sample this twice, maybe I sample this twice,",
    "start": "3092510",
    "end": "3099035"
  },
  {
    "text": "and maybe these don't get sampled. Maybe I sample this once, this once, uh, right.",
    "start": "3099035",
    "end": "3106430"
  },
  {
    "text": "So, so the blue represent the particles after this one round of particle filtering.",
    "start": "3106430",
    "end": "3112880"
  },
  {
    "text": "Where I've, kinda, moved the particles over here a little bit and lots of weight from there.",
    "start": "3112880",
    "end": "3119914"
  },
  {
    "text": "So that's why it's kinda called particle tracking because you can think about the swarm of particles representing where the object might be.",
    "start": "3119915",
    "end": "3126905"
  },
  {
    "text": "And over time as, as I follow the transition dynamics and,",
    "start": "3126905",
    "end": "3131915"
  },
  {
    "text": "um, hit it with the observations, I can, uh, move this swarm over time.",
    "start": "3131915",
    "end": "3137315"
  },
  {
    "text": "So that's the picture you should have in your head. Okay. So let's go through the formal algorithm.",
    "start": "3137315",
    "end": "3145565"
  },
  {
    "text": "It's gonna be very similar to beam search. So you start with empty assignment, and then you propose s- where you",
    "start": "3145565",
    "end": "3153760"
  },
  {
    "text": "take your partial assignments to the previous i minus 1 variables. And then I'm going to consider for each one of them just sampling",
    "start": "3153760",
    "end": "3161410"
  },
  {
    "text": "once from this transition distribution and augmenting that assignment.",
    "start": "3161410",
    "end": "3167204"
  },
  {
    "text": "So unlike beam search, where the size of C prime was K times larger than C,",
    "start": "3167205",
    "end": "3172505"
  },
  {
    "text": "the size of C prime is equal to the size of C in this case. Second, I relate so looking at the evidence,",
    "start": "3172505",
    "end": "3180500"
  },
  {
    "text": "and applying the evidence- probability of evidence given the particle H_i,",
    "start": "3180500",
    "end": "3185900"
  },
  {
    "text": "um, gives me a weight for every particle. Um, and then I'm going to normalize this distribution sample",
    "start": "3185900",
    "end": "3193130"
  },
  {
    "text": "K elements independently from that distribution and that redistributes, um, the, the particles to where I think they're more promising.",
    "start": "3193130",
    "end": "3202830"
  },
  {
    "text": "Okay. So let's go through this, um, quick demo.",
    "start": "3203920",
    "end": "3209569"
  },
  {
    "text": "Um, so the same problem, uh, as before, uh, I'm gonna set the number of particles to a 100.",
    "start": "3209570",
    "end": "3217040"
  },
  {
    "text": "So I start with all the particles, um, [NOISE] assigning X1, you know,",
    "start": "3217040",
    "end": "3223910"
  },
  {
    "text": "to 0 and there is 100 copies of them. Um, I extend and notice that some of the particles go to 0,",
    "start": "3223910",
    "end": "3231950"
  },
  {
    "text": "and some of the particles go to 1 with, uh, approximate probability proportional to whatever the transitions are.",
    "start": "3231950",
    "end": "3239075"
  },
  {
    "text": "Uh, and then going to redistribute which changes the, uh, balance a little bit.",
    "start": "3239075",
    "end": "3245045"
  },
  {
    "text": "And gonna extend prune, uh, extend. Uh, by prune, I really mean,",
    "start": "3245045",
    "end": "3252500"
  },
  {
    "text": "uh, ris- re-weight and re-sample. Um, and notice that, uh,",
    "start": "3252500",
    "end": "3258665"
  },
  {
    "text": "the particles kind of get more diver- This is more diverse than, um. Well, it's more diverse than beam search because I'm using K",
    "start": "3258665",
    "end": "3266060"
  },
  {
    "text": "equals 100 rather than like 3. But, uh, but you can see that, um, some of these particles might, like, my, like,",
    "start": "3266060",
    "end": "3273650"
  },
  {
    "text": "this one has 0 weight, so that when I re-sample and they just go away. Do you have a question?",
    "start": "3273650",
    "end": "3280670"
  },
  {
    "text": "Yeah. Why don't we aggregate all the ones into a single category and all the zeros into single category?",
    "start": "3280670",
    "end": "3286505"
  },
  {
    "text": "So just to show the branching pattern, or is it actually relevant? Yeah. So that's a good question.",
    "start": "3286505",
    "end": "3291560"
  },
  {
    "text": "So notice that, um, all of these are, all of these zero for the purposes of,",
    "start": "3291560",
    "end": "3299480"
  },
  {
    "text": "uh, X4 are just pretty the same. So if you only care about the marginals then you can collapse them. You're absolutely right.",
    "start": "3299480",
    "end": "3306290"
  },
  {
    "text": "In this demo, it's- I'm maintaining the entire history so that, yeah, you can show the- see the branching.",
    "start": "3306290",
    "end": "3314370"
  },
  {
    "text": "Okay. So this is that point [LAUGHTER]. If you only care about the last, uh,",
    "start": "3318010",
    "end": "3323750"
  },
  {
    "text": "position and not about the possible trajectories, then you can",
    "start": "3323750",
    "end": "3329210"
  },
  {
    "text": "actually collapse all the particles with the same H_i into, uh, one.",
    "start": "3329210",
    "end": "3335270"
  },
  {
    "text": "And then furthermore, if there's repeats then you can just keep track of the count, right. And this is actually what you would do in your assignment.",
    "start": "3335270",
    "end": "3341720"
  },
  {
    "text": "I am giving you kind of the more general picture in case. Because particle filtering is more general than just, uh,",
    "start": "3341720",
    "end": "3347765"
  },
  {
    "text": "looking at the last timestep, but most of the time you're just interested in the last timestep.",
    "start": "3347765",
    "end": "3353730"
  },
  {
    "text": "Okay. So just a quick, kinda quick, um, visual illustration of this.",
    "start": "3356770",
    "end": "3362285"
  },
  {
    "start": "3361000",
    "end": "3444000"
  },
  {
    "text": "Um, let's define this factor graph where, oh, you have, um,",
    "start": "3362285",
    "end": "3369095"
  },
  {
    "text": "transitions that are basically 1 or 0 depending on whether h_y- i and H_i 1- minus 1 are close to each other.",
    "start": "3369095",
    "end": "3378485"
  },
  {
    "text": "And O_i is some sensor reading. Um, one thing I've been a little bit, uh,",
    "start": "3378485",
    "end": "3383765"
  },
  {
    "text": "sliding under the rug is sometimes I've talked about local conditional distributions, sometimes I've been talking about factors.",
    "start": "3383765",
    "end": "3390185"
  },
  {
    "text": "Remember from the point of view of inference, um, it really doesn't matter. They're all just, you know, factors, right?",
    "start": "3390185",
    "end": "3397370"
  },
  {
    "text": "So in particular, if I give you a factor graph which- right remember which is not necessarily,",
    "start": "3397370",
    "end": "3404088"
  },
  {
    "text": "uh, uh, a Bayesian network. I can nonetheless still define a distribution by simply just, uh, normalizing.",
    "start": "3404089",
    "end": "3412310"
  },
  {
    "text": "Pick all the weights a mulpti- uh, and normalize and divide by that.",
    "start": "3412310",
    "end": "3417350"
  },
  {
    "text": "And these objects are actually called, uh, Markov networks or Markov random fields, which is another object of study that we're not gonna talk about in this class, but, um,",
    "start": "3417350",
    "end": "3426425"
  },
  {
    "text": "this is actually a more, you know, general way to think about the relationship between factor graphs and distributions.",
    "start": "3426425",
    "end": "3432470"
  },
  {
    "text": "Um, we're only mostly focusing on, on Bayesian networks in this class.",
    "start": "3432470",
    "end": "3438019"
  },
  {
    "text": "But some examples would be more general than that. Okay. So you have this distribution and,",
    "start": "3438020",
    "end": "3444380"
  },
  {
    "start": "3444000",
    "end": "3576000"
  },
  {
    "text": "um, so you can play with this demo in the slides. If you click then, this yellow dot shows you,",
    "start": "3444380",
    "end": "3451415"
  },
  {
    "text": "uh, the observation at a particular point in time. And the noise, uh,",
    "start": "3451415",
    "end": "3457385"
  },
  {
    "text": "the observation is related to the true position of some particle by based on the noise that you define here.",
    "start": "3457385",
    "end": "3463910"
  },
  {
    "text": "So here I've defined box noise which means that it's gonna be a uniform distribution over a box of,",
    "start": "3463910",
    "end": "3470285"
  },
  {
    "text": "uh, 3 by, uh, 3 by 3. Or I guess a 6 by 6, uh, box.",
    "start": "3470285",
    "end": "3476645"
  },
  {
    "text": "Um, and so if I increase the number of particles to,",
    "start": "3476645",
    "end": "3483680"
  },
  {
    "text": "uh, let's say 10,000, then what I'm gonna show you is a, a red blob,",
    "start": "3483680",
    "end": "3490625"
  },
  {
    "text": "um, that looks like it's trying to eat the [LAUGHTER] uh,",
    "start": "3490625",
    "end": "3496310"
  },
  {
    "text": "uh, the yellow dot. Uh, and this red blob shows you, uh, the set of particles where the intensity is",
    "start": "3496310",
    "end": "3503960"
  },
  {
    "text": "the count of that number of particles in a particular cell. So this swarm, kind of,",
    "start": "3503960",
    "end": "3509720"
  },
  {
    "text": "corresponds to on the board, it's the set of particles. Uh, but since this is discretized, you can, kind of,",
    "start": "3509720",
    "end": "3515750"
  },
  {
    "text": "see the pile of particles, piling up on each other. Uh, and just to see how well this is doing,",
    "start": "3515750",
    "end": "3522470"
  },
  {
    "text": "show true position, er, you can see the blue dot is the actual object position,",
    "start": "3522470",
    "end": "3527825"
  },
  {
    "text": "the yellow dot is the noisy observation, and it's trying to do its best to track where the blue is.",
    "start": "3527825",
    "end": "3533900"
  },
  {
    "text": "It's not perfect, uh, because this is kind of an approximate algorithm, but it kind of gets most of the way there.",
    "start": "3533900",
    "end": "3541050"
  },
  {
    "text": "Okay. Any questions about particle filtering? So to summarize, you can do forward backward if you",
    "start": "3545550",
    "end": "3555214"
  },
  {
    "text": "can swallow computing on a number of domain values times number of domain values.",
    "start": "3555215",
    "end": "3560285"
  },
  {
    "text": "If you have large domains, but you really think that none- most of them don't matter, then particle filtering is,",
    "start": "3560285",
    "end": "3566060"
  },
  {
    "text": "is a good tool because it allows you to focus your energies on the relevant part of the space.",
    "start": "3566060",
    "end": "3572510"
  },
  {
    "text": "Okay. So now, let's revisit Gibbs sampling from a probabilistic inference point of view.",
    "start": "3573160",
    "end": "3578855"
  },
  {
    "start": "3576000",
    "end": "3579000"
  },
  {
    "text": "So remember, Gibbs sampling we talked about, uh, last week as a way to compute the maximum weight assignment in an arbitrary factor graph,",
    "start": "3578855",
    "end": "3587345"
  },
  {
    "start": "3579000",
    "end": "4309000"
  },
  {
    "text": "where the main purpose is to get out, uh, of local minimum. Uh, so remember how Gibbs sampling works,",
    "start": "3587345",
    "end": "3593840"
  },
  {
    "text": "you have a weight which is defined for, uh, complete assignments. So unlike particle filtering or beam search,",
    "start": "3593840",
    "end": "3601085"
  },
  {
    "text": "we're starting with complete assignments and trying to modify the complete assignments rather trying to extend partial assignments.",
    "start": "3601085",
    "end": "3606620"
  },
  {
    "text": "Uh, so you loop and you compute. You pick up a variable X_i and then you consider all the possible, uh, um,",
    "start": "3606620",
    "end": "3618320"
  },
  {
    "text": "possible values you can take on, and you choose, uh, the value with probability proportional to its weight.",
    "start": "3618320",
    "end": "3625670"
  },
  {
    "text": "Okay. Let me show you this, um, example now, that uh, we saw last week.",
    "start": "3625670",
    "end": "3630920"
  },
  {
    "text": "Uh, so, so same graph here. Uh, so we start with this complete assignment.",
    "start": "3630920",
    "end": "3638660"
  },
  {
    "text": "Uh, and then we're gonna examine X1. X1 can take on two possible values.",
    "start": "3638660",
    "end": "3645609"
  },
  {
    "text": "For each of these values, I'm gonna compute its weight. And remember in Gibbs sampling, I only need to consider the Markov blanket of that variable.",
    "start": "3645610",
    "end": "3653830"
  },
  {
    "text": "Uh, the factors here are o1 and t1 because that's only thing that changes.",
    "start": "3653830",
    "end": "3660320"
  },
  {
    "text": "Everything else is a constant. Uh, and then I'll normalize and sample from that.",
    "start": "3660320",
    "end": "3666185"
  },
  {
    "text": "Okay. So then, uh, I go onto the next variable and so on. So I sweep across all the variables, and, um,",
    "start": "3666185",
    "end": "3674870"
  },
  {
    "text": "eventually the weight hopefully goes up, but not always up because sometimes I might sample a value that has lower probability.",
    "start": "3674870",
    "end": "3682800"
  },
  {
    "text": "Okay. And at the same time, I can do various things like, um,",
    "start": "3684370",
    "end": "3690755"
  },
  {
    "text": "computing the, the marginal distribution over a, a particular variable.",
    "start": "3690755",
    "end": "3696980"
  },
  {
    "text": "So, or if two variables. So I can, um, basically count the number of times I see particular patterns",
    "start": "3696980",
    "end": "3704359"
  },
  {
    "text": "and I can normalize that to get a distribution over that particular pattern.",
    "start": "3704360",
    "end": "3709670"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "3709670",
    "end": "3714059"
  },
  {
    "text": "So now let's try to interpret Gibbs sampling from a probabilistic, ah, point of view.",
    "start": "3715510",
    "end": "3721610"
  },
  {
    "text": "So instead of just thinking about, ah, a weight as just a function, we can actually think about the probability distribution",
    "start": "3721610",
    "end": "3729545"
  },
  {
    "text": "induced by that factor graph by again summing all the weights over all possible assignments. Normalizing, there you have a distribution.",
    "start": "3729545",
    "end": "3737825"
  },
  {
    "text": "Um, so the way to think about Gibbs sampling is",
    "start": "3737825",
    "end": "3743420"
  },
  {
    "text": "now more succinctly and more actually traditionally written as the following,",
    "start": "3743420",
    "end": "3748655"
  },
  {
    "text": "which is you loop through all the variables, and for every variable you're going to look at",
    "start": "3748655",
    "end": "3754235"
  },
  {
    "text": "the probability of that variable taking on a particular value condition on everything else.",
    "start": "3754235",
    "end": "3759920"
  },
  {
    "text": "So now I can give like, write down this probability which is, you know, a nice way to think about what Gibbs sampling is actually, you know, doing.",
    "start": "3759920",
    "end": "3768395"
  },
  {
    "text": "Um, and the, the guarantee with Gibbs sampling,",
    "start": "3768395",
    "end": "3773704"
  },
  {
    "text": "um, under, you know, some conditions, which I won't get into is that, as you run this for long enough, you, ah,",
    "start": "3773704",
    "end": "3781474"
  },
  {
    "text": "the sample that you get is actually a true sample from this distribution,",
    "start": "3781475",
    "end": "3786680"
  },
  {
    "text": "as if you had sampled from this distribution. And if you did a multiple times, now you can actually compute any sort of marginal,",
    "start": "3786680",
    "end": "3793910"
  },
  {
    "text": "ah, distribution, you know, you like. So now, while there's, that guarantee sounds really nice,",
    "start": "3793910",
    "end": "3801305"
  },
  {
    "text": "there are situations where Gibbs sampling could take exponential time to get there, so caveats.",
    "start": "3801305",
    "end": "3808410"
  },
  {
    "text": "Okay. So let's look at a possible application of Gibbs sampling, image denoising. So suppose you have some sort of noisy image and you want to clean it up.",
    "start": "3808960",
    "end": "3817655"
  },
  {
    "text": "So how can this be, um, helpful? So we can model this image denoising problem as,",
    "start": "3817655",
    "end": "3824160"
  },
  {
    "text": "um, this factor graph where, um, you have a grid of, ah, pixel values,",
    "start": "3824160",
    "end": "3830569"
  },
  {
    "text": "um, and, ah, they're connected, um, in this kind of grid like way.",
    "start": "3830570",
    "end": "3836825"
  },
  {
    "text": "So every value of X_i is, where i is a location, um, two numbers,",
    "start": "3836825",
    "end": "3843365"
  },
  {
    "text": "is going to be either, ah, 0 or 1, um, and we're gonna assume that some subset of the pixels are observed.",
    "start": "3843365",
    "end": "3852665"
  },
  {
    "text": "Um, and in case, ah, we observe it then we're actually just going to have, ah,",
    "start": "3852665",
    "end": "3858020"
  },
  {
    "text": "a factor that is actually a constraint that says that value of X_i has to be whatever we observed.",
    "start": "3858020",
    "end": "3863990"
  },
  {
    "text": "And these- we have these, um, transition potentials that say",
    "start": "3863990",
    "end": "3870215"
  },
  {
    "text": "neighboring pixels are more likely to be the same than different. So it assigns value 2 to pixels which are the same and 1 to pixels which are different.",
    "start": "3870215",
    "end": "3880290"
  },
  {
    "text": "Okay. So is the model clear? So now let's try to do Gibbs sampling in this, in this model [NOISE].",
    "start": "3881230",
    "end": "3892070"
  },
  {
    "text": "Just to give you, um, a concrete idea of what this looks like. Um, so we now look at- I'm not gonna draw the entire, ah,",
    "start": "3892070",
    "end": "3901070"
  },
  {
    "text": "[NOISE] the grid but I'm gonna center around a particular node that we're interested in, um, sampling.",
    "start": "3901070",
    "end": "3909755"
  },
  {
    "text": "So there's more stuff over here [NOISE]. Okay. So, um, and remember in,",
    "start": "3909755",
    "end": "3915964"
  },
  {
    "text": "in Gibbs sampling, um, at any point in time, all the variables have some sort of preliminary as- assignments.",
    "start": "3915965",
    "end": "3922880"
  },
  {
    "text": "So this might be 1, um, ah, 1, 1, and 0 and this might be 1.",
    "start": "3922880",
    "end": "3929660"
  },
  {
    "text": "Okay. Um, so now I sweep through and I'm gonna pick up this variable,",
    "start": "3929660",
    "end": "3935180"
  },
  {
    "text": "and you're gonna say, [NOISE] shall I try to change its value? First of all, you ignore the old value because it doesn't,",
    "start": "3935180",
    "end": "3942910"
  },
  {
    "text": "[NOISE] ah, factor into the algorithm. And now you're going to consider, um, ah, let's say, this is X_i.",
    "start": "3942910",
    "end": "3950665"
  },
  {
    "text": "So X_i, there's two possible, um, ah, values here.",
    "start": "3950665",
    "end": "3956630"
  },
  {
    "text": "So 0 and 1, right? So, um, and I'm gonna look at the weight.",
    "start": "3956630",
    "end": "3964610"
  },
  {
    "text": "So if it's 0, then I'm gonna evaluate each of these,",
    "start": "3964610",
    "end": "3969920"
  },
  {
    "text": "ah, factors based on that. So remember, the transition potential is,",
    "start": "3969920",
    "end": "3976625"
  },
  {
    "text": "um, um, well, I'm not gonna write it down.",
    "start": "3976625",
    "end": "3982370"
  },
  {
    "text": "But if- let's consider 0 here. So these are different. That means I'm gonna get a 1,",
    "start": "3982370",
    "end": "3989474"
  },
  {
    "text": "ah, these are different, that's gonna be a 1. These are different, that's gonna be a 1,",
    "start": "3989474",
    "end": "3995045"
  },
  {
    "text": "um, these are the same and I'm gonna get a 2. Okay. Now I try 1.",
    "start": "3995045",
    "end": "4000370"
  },
  {
    "text": "Um, these are the same. These are the same. These are the same,",
    "start": "4000370",
    "end": "4006610"
  },
  {
    "text": "and, ah, these two are different. Um, so for every assignment I have this weight, so this is 2,",
    "start": "4006610",
    "end": "4013270"
  },
  {
    "text": "this is 8x, um, and then I, ah, normalize.",
    "start": "4013270",
    "end": "4018640"
  },
  {
    "text": "So this is gonna be 0.8 and this is gonna be 0.2, [NOISE] and I draw, um,",
    "start": "4018640",
    "end": "4025330"
  },
  {
    "text": "ah, flip a coin with heads, ah, 0.8 and whatever I get I put down.",
    "start": "4025330",
    "end": "4032560"
  },
  {
    "text": "So I might have with point a probability I put that 1 back, and so on.",
    "start": "4032560",
    "end": "4037790"
  },
  {
    "text": "Okay. And here's another example which I'm not gonna go through.",
    "start": "4038430",
    "end": "4043040"
  },
  {
    "text": "Okay. So Gibbs sampling is gonna do that. So now let's look at this, ah, um, concrete.",
    "start": "4043650",
    "end": "4050875"
  },
  {
    "text": "Oops. Okay. So. All right. Okay. So what you're looking at here is this grid of pixels.",
    "start": "4050875",
    "end": "4059560"
  },
  {
    "text": "Um, white means that the pixel is unobserved.",
    "start": "4059560",
    "end": "4064720"
  },
  {
    "text": "Black or red means that it's observed to be whatever color, ah, you see on the screen.",
    "start": "4064720",
    "end": "4070975"
  },
  {
    "text": "And, ah, so this is some- somewhat of a noisy image and the goal is to fill in the white pixels so the picture makes sense.",
    "start": "4070975",
    "end": "4079600"
  },
  {
    "text": "And visually, you guys can probably look at this and see the hidden, ah, text.",
    "start": "4079600",
    "end": "4086905"
  },
  {
    "text": "No? [NOISE] Okay. [LAUGHTER] Okay. So your  denoising system is pretty good.",
    "start": "4086905",
    "end": "4092560"
  },
  {
    "text": "Um, okay. So I'm gonna run Gibbs sampling, so we click. And what you're seeing here, each iteration,",
    "start": "4092560",
    "end": "4098903"
  },
  {
    "text": "I'm gonna go through every pixel and apply exactly the algorithm of whatever I did on the board.",
    "start": "4098904",
    "end": "4104484"
  },
  {
    "text": "Okay? So you can see that, um, these are just samples from,",
    "start": "4104485",
    "end": "4110650"
  },
  {
    "text": "ah, the set of unobserved random variables. Okay. So to turn this into something more, um, useful,",
    "start": "4110650",
    "end": "4119980"
  },
  {
    "text": "you can instead of looking at a particular sample, you look at, um,",
    "start": "4119980",
    "end": "4125605"
  },
  {
    "text": "the marginal, which is for every location, what is the average pixel value that I've seen so far?",
    "start": "4125605",
    "end": "4131454"
  },
  {
    "text": "Ah, and if you do that, then you can see a little bit clearer picture of, um, 221,",
    "start": "4131455",
    "end": "4139404"
  },
  {
    "text": "and it's not gonna be perfect because the model that we have is fairly simplistic.",
    "start": "4139405",
    "end": "4144460"
  },
  {
    "text": "I always says is similar pixels- neighboring pixels have or tend to have the same,",
    "start": "4144460",
    "end": "4150489"
  },
  {
    "text": "um, ah, color and it has no notion of, you know, letters or something.",
    "start": "4150490",
    "end": "4157734"
  },
  {
    "text": "Okay. But you can see kind of a simple example of, ah, you know, Gibbs sampling our work.",
    "start": "4157735",
    "end": "4162895"
  },
  {
    "text": "Um, there's a bunch of parameters you can play with. Um, you can- here's, ah,",
    "start": "4162895",
    "end": "4167980"
  },
  {
    "text": "another picture of a cat and you can try to-, um,",
    "start": "4167980",
    "end": "4173410"
  },
  {
    "text": "you can play around with the coherence, which is how sticky the, the, ah,",
    "start": "4173410",
    "end": "4178690"
  },
  {
    "text": "transition constraints are, you could try to use ICM, which won't work, um, at all, and so on.",
    "start": "4178690",
    "end": "4185300"
  },
  {
    "text": "Okay. Any questions?",
    "start": "4186630",
    "end": "4190969"
  },
  {
    "text": "I think we might actually end early today. Okay. So, um, just to kind of sum up,",
    "start": "4192420",
    "end": "4200770"
  },
  {
    "text": "we've, last week, Mondays we define new models. So we defined a Bayesian network or a factor graph.",
    "start": "4200770",
    "end": "4207805"
  },
  {
    "text": "And today, we're focusing on the question of how do we do probabilistic inference, and for some set of models I've shown you how to do this.",
    "start": "4207805",
    "end": "4215320"
  },
  {
    "text": "Um, there's a number of algorithms here, um, and a, a forward backward, which, ah,",
    "start": "4215320",
    "end": "4222520"
  },
  {
    "text": "work for HMMs and are exact, particle filtering which works for HMMs although they can be generalized,",
    "start": "4222520",
    "end": "4229030"
  },
  {
    "text": "or which is approximate, and Gibbs sampling which works for general factor graphs which is also approximate.",
    "start": "4229030",
    "end": "4234385"
  },
  {
    "text": "Each of these algorithms we've seen kind of these ideas in previous incarnations.",
    "start": "4234385",
    "end": "4240789"
  },
  {
    "text": "So forward backward is very similar to variable elimination.",
    "start": "4240790",
    "end": "4246760"
  },
  {
    "text": "Um, because it- variable elimination also, ah, computes things exactly.",
    "start": "4246760",
    "end": "4254199"
  },
  {
    "text": "Um, particle filtering is like Beam Search, um, to compute things approximately,",
    "start": "4254200",
    "end": "4260725"
  },
  {
    "text": "and Gibbs sampling is like, um, iterative conditional modes or Gibbs sampling,",
    "start": "4260725",
    "end": "4266790"
  },
  {
    "text": "um, which, ah, we saw from last week. Okay. So, ah, next Monday,",
    "start": "4266790",
    "end": "4273390"
  },
  {
    "text": "we're gonna look at how we do learning. So up until now, the Bayesian network,",
    "start": "4273390",
    "end": "4278790"
  },
  {
    "text": "all the probabilities are fixed and now we're going to actually start to do learning.",
    "start": "4278790",
    "end": "4284545"
  },
  {
    "text": "Um, I should say that maybe this learning sounds a bit scary because there's already a lot of machinery behind",
    "start": "4284545",
    "end": "4291070"
  },
  {
    "text": "inference and factors and all this stuff but you'll be pleasantly surprised that learning is actually much simpler than inference.",
    "start": "4291070",
    "end": "4298690"
  },
  {
    "text": "So stay tuned.",
    "start": "4298690",
    "end": "4299949"
  }
]