[
  {
    "start": "0",
    "end": "5652"
  },
  {
    "text": "Hi, everyone. Welcome to the last\nlecture of CS 330.",
    "start": "5652",
    "end": "11090"
  },
  {
    "text": "Today it's a very\nexciting lecture. We'll be talking about\nfrontiers and open-challenges",
    "start": "11090",
    "end": "16209"
  },
  {
    "text": "in multi-task and meta-learning. And we'll have eight\nspeakers who'll",
    "start": "16210",
    "end": "22180"
  },
  {
    "text": "tell us about some\nof the work that is at the frontier of\nmulti-task, meta-learning,",
    "start": "22180",
    "end": "29439"
  },
  {
    "text": "and actually other\nsub-fields as well. So it should be really exciting. It will give you\nthe opportunity not",
    "start": "29440",
    "end": "35530"
  },
  {
    "text": "only to learn about their\nwork, but also to actually meet the people who are behind it.",
    "start": "35530",
    "end": "41230"
  },
  {
    "text": "Before we get started,\na few logistics. So again, this is the\nlast lecture today.",
    "start": "41230",
    "end": "49390"
  },
  {
    "text": "We will have the instructor\noffice hours right after this, which will be also held on\nZoom, and the link to join that",
    "start": "49390",
    "end": "55089"
  },
  {
    "text": "is on Canvas. And then on Tuesday we'll have\nthe project poster session, and then Wednesday,\nthe projects are due.",
    "start": "55090",
    "end": "64209"
  },
  {
    "text": "One more thing is that\nwe'll have the final course survey at some point. I'm not exactly sure when you\nwill get an email about this,",
    "start": "64209",
    "end": "71560"
  },
  {
    "text": "but you will. And we really appreciate\nyour feedback. That really helps us\nimprove the course, and we incorporated a lot of\nthe feedback from previous years",
    "start": "71560",
    "end": "79780"
  },
  {
    "text": "to make the course better. So please take the\ntime to fill those out. These are really,\nreally helpful, and we really appreciate\nyour feedback.",
    "start": "79780",
    "end": "86355"
  },
  {
    "text": " So today we'll be talking about\nfrontiers and open challenges.",
    "start": "86355",
    "end": "91590"
  },
  {
    "text": "And I split them\ninto two big topics. One will be about utilizing\nzero-shot generalization",
    "start": "91590",
    "end": "99270"
  },
  {
    "text": "of large language models. And we talk quite a bit\nabout large language models. We had one of the\nlectures on it.",
    "start": "99270",
    "end": "105090"
  },
  {
    "text": "One of the guest lecturers\nfrom Colin on it. And we'll explore\nsome of the questions",
    "start": "105090",
    "end": "110680"
  },
  {
    "text": "when it comes to using\nthese large language models. We'll talk about how to\nfine tune these large models without negative transfer.",
    "start": "110680",
    "end": "118585"
  },
  {
    "text": "And we'll also talk about how we\ncan apply these large language models to other domains, such\nas robotics, for instance.",
    "start": "118585",
    "end": "125440"
  },
  {
    "text": "And then in the second\npart of the lecture, we'll talk about how we can\nmake reinforcement learning and better data sponge.",
    "start": "125440",
    "end": "131230"
  },
  {
    "text": "And this might be a little\nenigmatic at this point. I will explain what I mean\nby this when we get there.",
    "start": "131230",
    "end": "137599"
  },
  {
    "text": "But some of the topics\nthat we'll discuss will include how to\nscale up data collection. How to allow--\nhow to have robots",
    "start": "137600",
    "end": "144700"
  },
  {
    "text": "to collect a lot more data than\nthey're collecting currently. And how we can utilize broad\noffline data sources, including",
    "start": "144700",
    "end": "151790"
  },
  {
    "text": "non-robotic data sources. And then how we can apply\nreinforcement learning to lifelong settings as well.",
    "start": "151790",
    "end": "157542"
  },
  {
    "text": "And then at the end we'll also\ndiscuss the open challenges.  So let's start with utilizing\nzero-shot generalization",
    "start": "157542",
    "end": "164780"
  },
  {
    "text": "of language models. So just a few words\nof introduction. Well, what is special about\nlarge language models?",
    "start": "164780",
    "end": "171799"
  },
  {
    "text": "There's a lot of\nhype around them and maybe the hype is\na little bit too much.",
    "start": "171800",
    "end": "177360"
  },
  {
    "text": "But the one thing that is\nextremely exciting about them is their ability to perform\nzero-shot generalization.",
    "start": "177360",
    "end": "185120"
  },
  {
    "text": "And Colin in his lecture\ntalked a little bit about how they could be viewed\nas models that do implicitly",
    "start": "185120",
    "end": "191870"
  },
  {
    "text": "meta-learning. But the zero-shot\ngeneralization is something that we really\nhaven't seen at that scale,",
    "start": "191870",
    "end": "198433"
  },
  {
    "text": "and that makes them extremely\nexciting and extremely useful for other\ndomains as well.",
    "start": "198433",
    "end": "204780"
  },
  {
    "text": "So with that, I would like\nto welcome Eric Mitchell, who will tell us a little bit\nabout how we can fine tune",
    "start": "204780",
    "end": "212108"
  },
  {
    "text": "these large models. What are the difficulties\nthat come with that. And how we can do this\nbetter without having",
    "start": "212108",
    "end": "218840"
  },
  {
    "text": "any negative transfer. So Eric Mitchell\nis a PhD student with Chris Manning\nand Chelsea Finn.",
    "start": "218840",
    "end": "225110"
  },
  {
    "text": "So Eric, please take it away. Yeah, thanks for the\nlovely intro, Karol.",
    "start": "225110",
    "end": "230530"
  },
  {
    "text": "Great to be here and\ntalk for just a couple of minutes about our work. Maybe if you could stop sharing\nyour screen, I can show--",
    "start": "230530",
    "end": "240294"
  },
  {
    "text": "I've got some slides. Perfect. Thank you. So as Karol mentioned.",
    "start": "240294",
    "end": "245379"
  },
  {
    "text": "I'm going to say a couple words\nabout fine tuning on basically one example without\nnegative transfer, which",
    "start": "245380",
    "end": "251769"
  },
  {
    "text": "we can think about as editing\na pre-trained neural network.",
    "start": "251770",
    "end": "257838"
  },
  {
    "text": "So I'll get right into it. Basically, what do I mean\nby editing neural network, and why do we want to do this?",
    "start": "257839",
    "end": "263090"
  },
  {
    "text": "Well, I mean, you're probably\nfamiliar with the fact that when we train a neural\nnetwork on some big data set like ImageNet or\nWikiText, they end up",
    "start": "263090",
    "end": "271150"
  },
  {
    "text": "soaking up a lot of\nthe knowledge one way or another in these data sets. But obviously as Sam Altman\npointed out in his tweet,",
    "start": "271150",
    "end": "277780"
  },
  {
    "text": "even a big model like\nGPT-3 is still wrong. And even when they're correct\nat the moment that training",
    "start": "277780",
    "end": "283810"
  },
  {
    "text": "completes, over time\ntheir predictions may become obsolete. So as an example, if we\ntake some more or less state",
    "start": "283810",
    "end": "291610"
  },
  {
    "text": "of the art question\nanswering model like T5, and we ask it who's the\nprime minister of the UK is.",
    "start": "291610",
    "end": "297639"
  },
  {
    "text": "It's going to say Theresa May. And depending on how familiar\nyou are with UK politics,",
    "start": "297640",
    "end": "304000"
  },
  {
    "text": "you'll know that this is not\nthe prime minister of the UK, though it used to be. And so this just\nreflects the fact",
    "start": "304000",
    "end": "309412"
  },
  {
    "text": "that this model\nwas trained on data that was gathered in the past. And so ideally, what we'd\nlike to be able to do",
    "start": "309412",
    "end": "315100"
  },
  {
    "text": "is locally tweak our models\nbehavior on this one example without destroying\nits representations",
    "start": "315100",
    "end": "321670"
  },
  {
    "text": "and its predictions on other\nparts of the input domain. And this problem isn't really\nspecific to language models,",
    "start": "321670",
    "end": "328090"
  },
  {
    "text": "although they provide\na convenient modality to highlight how\nthis failure occurs. We can come up with examples\nwith image classifiers",
    "start": "328090",
    "end": "335470"
  },
  {
    "text": "screwing up when they're exposed\nto one particular background, but not otherwise. Policies that make mistakes in\none particular configuration",
    "start": "335470",
    "end": "343060"
  },
  {
    "text": "of the environment, not others. Translation systems\nmistranslating particular phrases between\nparticular languages,",
    "start": "343060",
    "end": "349150"
  },
  {
    "text": "and so on, and so forth. So it's a fairly\ngeneral problem that crops up when we start\nto deploy these models",
    "start": "349150",
    "end": "355090"
  },
  {
    "text": "and then time goes by. So in other words, the\nmodel is mostly right, but we want to figure\nout how to change it",
    "start": "355090",
    "end": "360670"
  },
  {
    "text": "for just this example\nand the related examples. So how do we actually do this?",
    "start": "360670",
    "end": "367580"
  },
  {
    "text": "Well, if you squint at it,\nit's sort of like one-shot learning in a way. I mean, we get one\nexample, and we",
    "start": "367580",
    "end": "373478"
  },
  {
    "text": "get the label for that example. And we want to correctly change\nour model for that one example. So in our normal few\nshot learning setup,",
    "start": "373478",
    "end": "381130"
  },
  {
    "text": "we're going to have some\nmeta-data set, where here our meta-data set is a data\nset of individual questions,",
    "start": "381130",
    "end": "387160"
  },
  {
    "text": "and each question is sort of\na different one-shot learning task. So we have x edit, the\ninput, the question, y edit,",
    "start": "387160",
    "end": "393910"
  },
  {
    "text": "the true label\nfor that question. And then x prime is\nsort of our query input, like a rephrasing\nof the question",
    "start": "393910",
    "end": "399790"
  },
  {
    "text": "to see if our one-shot\nadaptation actually generalizes. So as an example\nof this, we would",
    "start": "399790",
    "end": "405370"
  },
  {
    "text": "have x edit as like\nwho's the PM of the UK. We would put this through\nour pre-trained model. And we would say, well, it\nsays Theresa May instead",
    "start": "405370",
    "end": "411850"
  },
  {
    "text": "of Boris Johnson. That's not so good. And so that's our\nsupport example. We would give it a\nsupport label essentially,",
    "start": "411850",
    "end": "419240"
  },
  {
    "text": "which is the edit\ntarget, Boris Johnson. And then we have a\nquery input, x prime, which checks if\nthe model correctly",
    "start": "419240",
    "end": "425320"
  },
  {
    "text": "has changed its predictions,\nnot just for that exact input, but for related ones. And so this is basically\none-shot learning.",
    "start": "425320",
    "end": "431949"
  },
  {
    "text": "But this doesn't really fully\nget at the editing problem because we have an\nadditional issue here,",
    "start": "431950",
    "end": "437120"
  },
  {
    "text": "which is, after we've\nperformed this edit, we want to make sure\nthat our model is making the same predictions\non unrelated data",
    "start": "437120",
    "end": "443980"
  },
  {
    "text": "as it did before\nwe made the edit. So if we have this\ncontrol input, x lock, this sort of\nlocality constraint.",
    "start": "443980",
    "end": "450220"
  },
  {
    "text": "Like who is the\npresident of France? If we change the prime\nminister of the UK, we don't want to change\nthe president of France.",
    "start": "450220",
    "end": "455800"
  },
  {
    "text": "And so we want to have an\nadditional constraint, that's not only saying\nafter adaptation, what is my model doing.",
    "start": "455800",
    "end": "461650"
  },
  {
    "text": "But it should also\nbe constraining the difference in the\nbehavior of the model before and after adaptation.",
    "start": "461650",
    "end": "466670"
  },
  {
    "text": "So this is a little different\nthan normal two-shot learning. And so at this point,\nyou might think, well, OK, we have this\nsort of slightly different",
    "start": "466670",
    "end": "472680"
  },
  {
    "text": "last function. Let's throw a MAML at it\nor some other algorithm. But there's sort of one\nother consideration, which",
    "start": "472680",
    "end": "478620"
  },
  {
    "text": "is that as opposed to\nmaybe normal meta-learning where we start out\nwith a random model, and then we meta-train it so\nthat it's good at adapting.",
    "start": "478620",
    "end": "486090"
  },
  {
    "text": "In this setting, we're actually\ngiven a pre-trained model up front. And so we don't actually\nwant to change that model",
    "start": "486090",
    "end": "493229"
  },
  {
    "text": "initialization because\nthe pre-trained model, we are assuming sort of\nperforms well most of the time.",
    "start": "493230",
    "end": "498599"
  },
  {
    "text": "So somehow we need to do the\nmeta-learning without changing our initialization,\nand this makes",
    "start": "498600",
    "end": "503760"
  },
  {
    "text": "it difficult to\nstraightforwardly apply an algorithm like MAML,\nwhere the representation power of meta-learning comes\nfrom this really big parameter",
    "start": "503760",
    "end": "511710"
  },
  {
    "text": "space in the model\ninitialization. So instead what\nwe're going to do is, we need the meta-learner\nto be separate from theta.",
    "start": "511710",
    "end": "517825"
  },
  {
    "text": "And so one way or\nanother, we need to parameterize some\ntransform of the update, which",
    "start": "517825",
    "end": "522870"
  },
  {
    "text": "are these-- one way or another these\nlittle networks that basically take the\nfine-tuned gradient and map it to a new\npseudo gradient,",
    "start": "522870",
    "end": "530199"
  },
  {
    "text": "which provides the properties\nthat we want for edits. And so this is the framework\nthat I've been thinking about,",
    "start": "530200",
    "end": "535980"
  },
  {
    "text": "we've been thinking\nabout for editing. And you can do some\npretty interesting things",
    "start": "535980",
    "end": "541388"
  },
  {
    "text": "that basically give the behavior\nthat's shown in this diagram here. So to recap, the\nediting models lets",
    "start": "541388",
    "end": "546480"
  },
  {
    "text": "us locally adjust their\nbehavior after training, which can be useful in sort of\na wide variety of circumstances. And it's sort of like\none-shot learning,",
    "start": "546480",
    "end": "553330"
  },
  {
    "text": "except we have these\ncouple of different tweaks to the setup, including\nthe adaptation should only",
    "start": "553330",
    "end": "559770"
  },
  {
    "text": "be a local change. And then we also\ndon't really want to change the initialization\nduring the course of this meta-training.",
    "start": "559770",
    "end": "565860"
  },
  {
    "text": "So hopefully that was\npresented in a way that was somewhat passable.",
    "start": "565860",
    "end": "570870"
  },
  {
    "text": "If you're interested,\nthere's a paper that we've recently\nput on archive called Fast Model Editing at Scale.",
    "start": "570870",
    "end": "576550"
  },
  {
    "text": "Thank you so much, Eric. Are there any questions? We have a little bit of time. So if there are any questions\nfrom the audience, please,",
    "start": "576550",
    "end": "583380"
  },
  {
    "text": "just speak up or\nraise your hand. ",
    "start": "583380",
    "end": "594234"
  },
  {
    "text": "Actually, I have one question. Do you think that changing the-- or modifying the\ngradient itself,",
    "start": "594234",
    "end": "600399"
  },
  {
    "text": "the way you were doing\nit in this paper, could be applied beyond\nediting large language models,",
    "start": "600400",
    "end": "606093"
  },
  {
    "text": "but could be also\na different form of meta-learning in general? Yeah, for sure. I mean, there are a handful\nof different algorithms",
    "start": "606093",
    "end": "613380"
  },
  {
    "text": "like Meta-Curvature, or\nWarpGrad, or Meta-SGD, et cetera, that are basically\nlearning some transformation",
    "start": "613380",
    "end": "621893"
  },
  {
    "text": "of the gradient. Maybe they also learn\nthe initialization. But they add this expressiveness\nby transforming the gradient.",
    "start": "621893",
    "end": "628020"
  },
  {
    "text": "And yeah, I mean,\nthis seems like it could be pretty directly\napplied to these settings. And we came to it motivated\nby the editing problem,",
    "start": "628020",
    "end": "635340"
  },
  {
    "text": "but realized sort\nof the solution is actually probably\nquite a bit more general than just the specific editing.",
    "start": "635340",
    "end": "640840"
  },
  {
    "text": "So yeah. Awesome. Cool.",
    "start": "640840",
    "end": "646656"
  },
  {
    "text": "Are there any questions\nfrom the audience. ",
    "start": "646656",
    "end": "653140"
  },
  {
    "text": "We have one.  Yeah, I see a raised hand. ",
    "start": "653140",
    "end": "659940"
  },
  {
    "text": "Yeah, this is pretty\ndarn fascinating. I was wondering a little\nbit down-the-road kind",
    "start": "659940",
    "end": "665685"
  },
  {
    "text": "of applications if\nyou could speculate a little bit about that. Is it more of like\nyou have a language",
    "start": "665685",
    "end": "670890"
  },
  {
    "text": "model that you tuned,\nand it's mostly right, but there's some cases\nthat it gets wrong that you want to fix?",
    "start": "670890",
    "end": "677070"
  },
  {
    "text": "Or maybe could it be like\nwith the UK prime minister example, something that's\nchanging constantly,",
    "start": "677070",
    "end": "685660"
  },
  {
    "text": "and so more of a, oh, this\nneeds to be updated because of changing times?",
    "start": "685660",
    "end": "692620"
  },
  {
    "text": "Yeah. Oh, sorry. Yeah, go ahead. Just any thoughts about that. Yeah, sure. So I think the\nimmediate application",
    "start": "692620",
    "end": "698890"
  },
  {
    "text": "is more like what I\npresented in terms of-- maybe Google deploys\na translation model",
    "start": "698890",
    "end": "704110"
  },
  {
    "text": "and they get like a handful\nof user reports every day. And ideally you'd have\nsome sort of pretty quick automated process to be able\nto take these user reports",
    "start": "704110",
    "end": "711490"
  },
  {
    "text": "and inject them into\nthe model quickly, so you don't have to retrain\nor go through some long process to fix the model.",
    "start": "711490",
    "end": "717352"
  },
  {
    "text": "And these might be due to\njust errors during training or temporal shift. I think longer term, it\nwould be interesting to start",
    "start": "717352",
    "end": "724600"
  },
  {
    "text": "tying this editings\ndirection into more like continual\nlearning settings where we don't want to just sort\nof do one edit or a big batch",
    "start": "724600",
    "end": "731620"
  },
  {
    "text": "of edits once right up front. But ideally we'd have\nsome way to either take a pre-trained model or tweak\na pre-trained model such",
    "start": "731620",
    "end": "738010"
  },
  {
    "text": "that we can sort of\nsuccessfully-- successively patch it in this\nway and not compound",
    "start": "738010",
    "end": "743740"
  },
  {
    "text": "this drawdown that's incurred\nevery darn time we do an edit. So I would say\nlonger down the road, you want to tie it with\ncontinual learning.",
    "start": "743740",
    "end": "750856"
  },
  {
    "text": "Thank you. Sure. All right, great. Thank you, Eric, so much.",
    "start": "750856",
    "end": "756633"
  },
  {
    "text": "Feel free to drop\nout of the call. I know that you're busy. Thanks so much. ",
    "start": "756633",
    "end": "763620"
  },
  {
    "text": "All right. So we talked a little bit about\nhow we can fine-tune or edit",
    "start": "763620",
    "end": "768840"
  },
  {
    "text": "these large language models. Let's see. Does this work? Let's talk a little\nbit about how",
    "start": "768840",
    "end": "774450"
  },
  {
    "text": "we can apply these\nlarge language models to other domains. So there is a little bit of\nwork on applying large language",
    "start": "774450",
    "end": "781560"
  },
  {
    "text": "models to other domains\nthat still involve language. And there is some\ninteresting work coming from OpenAI that\ntalks about solving math word",
    "start": "781560",
    "end": "791100"
  },
  {
    "text": "problems. For instance, we're given\na math question like this. Ali is the Dean of\na private school",
    "start": "791100",
    "end": "797040"
  },
  {
    "text": "where he teaches one class. John is also a Dean\nof a public school. John has two classes\nin his school. Each class 1 over--",
    "start": "797040",
    "end": "802690"
  },
  {
    "text": "one eighth the capacity\nof Ali's class, which has the capacity of 120 students.",
    "start": "802690",
    "end": "807930"
  },
  {
    "text": "What is the combined\ncapacity of both schools? It turns out that they\ncan train a model that",
    "start": "807930",
    "end": "812970"
  },
  {
    "text": "is pre-trained on\nGPT-3 that can actually answer that question correctly. So they take that\nquestion, put it",
    "start": "812970",
    "end": "819690"
  },
  {
    "text": "in a prompt for a\nlarge language model, and then the model is able\nto output the correct answer.",
    "start": "819690",
    "end": "826080"
  },
  {
    "text": "But it turns out that\nactually just fine-tuning that model to these\nkind of questions doesn't work very well.",
    "start": "826080",
    "end": "831130"
  },
  {
    "text": "So instead what they do is\nthey train an additional model that does verification. So given a set of\ndifferent answers",
    "start": "831130",
    "end": "837340"
  },
  {
    "text": "and the question that\nwas originally asked, it can just discriminative\nsay which answer is correct,",
    "start": "837340",
    "end": "843767"
  },
  {
    "text": "or if there is no correct\nanswer to say that there is no correct answer. And it turns out\nthat this model is--",
    "start": "843767",
    "end": "851010"
  },
  {
    "text": "actually provides quite a\nbit of boost of performance. And it's much easier\nto train the model",
    "start": "851010",
    "end": "856950"
  },
  {
    "text": "to verify the\nsolution than to train it to fine-tune the\nmodel to always propose the correct solution.",
    "start": "856950",
    "end": "862180"
  },
  {
    "text": "So then our test time it\nwould have the large language model produce a set of\ndifferent solutions, and then use the\nverification model",
    "start": "862180",
    "end": "867540"
  },
  {
    "text": "to verify which one\nis correct, and that achieves a fairly high\nresult. Alternatively,",
    "start": "867540",
    "end": "874540"
  },
  {
    "text": "they've also shown some\napplications of large language models to something like code\ncompletion where you can--",
    "start": "874540",
    "end": "881390"
  },
  {
    "text": "where they introduce\nthis tool called Codex where you can specify\nthe instruction that you",
    "start": "881390",
    "end": "887050"
  },
  {
    "text": "would want the model to help you\nwith to specify the code for. Then that instruction is then\nchanged to look like a comment.",
    "start": "887050",
    "end": "895930"
  },
  {
    "text": "So they take the\nprompt, they put it as the comment for a Python\ncode that will be generated.",
    "start": "895930",
    "end": "902470"
  },
  {
    "text": "That model was then\nfine-tuned on a lot of Python instructions\nassociated with the comments as well.",
    "start": "902470",
    "end": "908060"
  },
  {
    "text": "And then the large\nlanguage model outputs the Python code\nthat is being executed, and you see the results of that.",
    "start": "908060",
    "end": "914540"
  },
  {
    "text": "And there is obviously\nmany, many applications of that as well. Starting from just\nhelping us be better",
    "start": "914540",
    "end": "921190"
  },
  {
    "text": "coders, but also a very\nreally interactive tool that would allow maybe more\npeople get into coding.",
    "start": "921190",
    "end": "929050"
  },
  {
    "text": "So these are some applications\nof large language models that we see today that's still\ninvolve a lot of language",
    "start": "929050",
    "end": "936339"
  },
  {
    "text": "itself. But I thought that\none interesting topic would be to talk about how large\nlanguage models can be applied",
    "start": "936340",
    "end": "943360"
  },
  {
    "text": "to other domains that maybe\nare not necessarily connected to language, such as robotics.",
    "start": "943360",
    "end": "949780"
  },
  {
    "text": "So with that-- next\nwe'll talk about how to apply large language\nmodels to other domains.",
    "start": "949780",
    "end": "956170"
  },
  {
    "text": "And I would like to invite Eric\nJang, who is a senior research",
    "start": "956170",
    "end": "962589"
  },
  {
    "text": "scientist working\nat Google Brain, at Brain Robotics\nwhere we work together on a lot of these problems.",
    "start": "962590",
    "end": "969550"
  },
  {
    "text": "And Eric will talk\nto us a little bit about his work in the topic. ",
    "start": "969550",
    "end": "975795"
  },
  {
    "text": "Thanks for the intro,\nKarol, and for inviting me to give a quick talk on this\nproject that we did at Google.",
    "start": "975795",
    "end": "982630"
  },
  {
    "text": "So yeah, my name is Eric Jang. I'm a researcher at\nGoogle working on robots.",
    "start": "982630",
    "end": "987720"
  },
  {
    "text": "So I'll be talking about BC-Z,\nzero-shot task generalization with robotic imitation learning. Here's a list of\nthe collaborators.",
    "start": "987720",
    "end": "993420"
  },
  {
    "text": "The usual suspects in large\nscale robotic learning.",
    "start": "993420",
    "end": "998459"
  },
  {
    "text": "So as a quick\npreliminary, can we learn tasks in the real world\nwith deep learning on robots?",
    "start": "998460",
    "end": "1005480"
  },
  {
    "text": "And the last few years,\nand many years before that have suggested that\nthe answer is yes.",
    "start": "1005480",
    "end": "1010550"
  },
  {
    "text": "So you can do a single task\nlearning with pick-and-place and it generalizes to\na variety of objects.",
    "start": "1010550",
    "end": "1016880"
  },
  {
    "text": "You can do cable\ninsertion, and it also generalizes to\ndifferent situations. Block stacking also generalizes.",
    "start": "1016880",
    "end": "1022400"
  },
  {
    "text": "In this work, we also study a\nlittle bit of pick-and-place as well. But as I'm sure many of\nthe students in this class",
    "start": "1022400",
    "end": "1028819"
  },
  {
    "text": "are interested in, can we\nget one network or one policy to learn multiple tasks\nin the real world?",
    "start": "1028819",
    "end": "1035300"
  },
  {
    "text": "And again, the\nanswer is also yes. So some of the work by\nKarol and collaborators have done this on MT-Opt.",
    "start": "1035300",
    "end": "1041420"
  },
  {
    "text": "There's some other work\nwith Transporter Nets and Task-Embedded\nControl Networks, all suggesting that\nunsurprisingly, you can get",
    "start": "1041420",
    "end": "1049970"
  },
  {
    "text": "robots to do multiple things. And there's some sort of\ngeneralization and knowledge transfer between there as well.",
    "start": "1049970",
    "end": "1056670"
  },
  {
    "text": "So the goal of this\nwork was to look at whether robots can learn\ntasks that they haven't been trained on in the real world.",
    "start": "1056670",
    "end": "1062310"
  },
  {
    "text": "So not only are we\ndoing multi-task, where we want to generalize\na test time to new tasks that it hasn't seen demos for.",
    "start": "1062310",
    "end": "1069470"
  },
  {
    "text": "The setup is very simple. So we collect a lot of data with\nexpert teleoperation on a 100",
    "start": "1069470",
    "end": "1076070"
  },
  {
    "text": "plus manipulation tasks. We use pretty much the\nsimplest learning algorithm",
    "start": "1076070",
    "end": "1081080"
  },
  {
    "text": "possible for learning\nvisual motor control, which is imitation learning. So image in, action out.",
    "start": "1081080",
    "end": "1087770"
  },
  {
    "text": "We progress this to\nexpert demonstrations. And then at test time we\ncommand it with a new task,",
    "start": "1087770",
    "end": "1094190"
  },
  {
    "text": "and it'll just generalize\nto New instructions. So in the blue squares here you\nsee some of the training tasks",
    "start": "1094190",
    "end": "1099828"
  },
  {
    "text": "sentences, like place the\nbottle in ceramic bowl. Place the bowl-- bottle in. Knock the racer\nover, and so forth.",
    "start": "1099828",
    "end": "1105810"
  },
  {
    "text": "And then the green\nboxes contain test tasks for which the instructions we've\nnever trained the policy on.",
    "start": "1105810",
    "end": "1112440"
  },
  {
    "text": "The model architecture\nis very simple. It's just a modification\nof a resonant. And the resonant takes in a\nfrozen pre-trained sentence",
    "start": "1112440",
    "end": "1120090"
  },
  {
    "text": "encoder embedding or a\nvideo encoder embedding. I'm just going to talk about the\nsentence encoder aspect of this in this talk, but you can think\nof them as interchangeable.",
    "start": "1120090",
    "end": "1129730"
  },
  {
    "text": "So we have the\ntraining task divided into two sets of objects. The object set on the\ntop contains tasks",
    "start": "1129730",
    "end": "1135660"
  },
  {
    "text": "like place the sponge\nand the ceramic bowl. And the objects on\nthe bottom, which has mostly different\nobjects, has",
    "start": "1135660",
    "end": "1141127"
  },
  {
    "text": "things like wipe the\ntray with sponge, or put this cup in that\nother cup, and so forth. So the cool thing about\ntraining all of this tasks",
    "start": "1141127",
    "end": "1149400"
  },
  {
    "text": "is that we get generalization\nthrough the magic of deep learning, which\nis that at test time",
    "start": "1149400",
    "end": "1155370"
  },
  {
    "text": "you can mix together objects\nin these different object sets and command it with a\nnatural language instruction",
    "start": "1155370",
    "end": "1160645"
  },
  {
    "text": "that it's never been\ntrained on before. So it's never seen like the\ngrapes and the ceramic bowl in the same image in\nthe training data.",
    "start": "1160645",
    "end": "1167520"
  },
  {
    "text": "And it hasn't been\ntrained on the sentence, place the grapes in\nthe ceramic bowl. But it'll still generalize\nand do the correct behavior.",
    "start": "1167520",
    "end": "1175740"
  },
  {
    "text": "Here are some other videos\nof the held-out tasks that it can do.",
    "start": "1175740",
    "end": "1180880"
  },
  {
    "text": "And so, what are\nthe takeaways here? Well, so we've seen\nthat the system, which we call BC-Z\ngeneralizes to unseen",
    "start": "1180880",
    "end": "1187470"
  },
  {
    "text": "points in some pre-trained\nfrozen task embedded space. And if you'll permit me\nsome wild speculation here,",
    "start": "1187470",
    "end": "1195840"
  },
  {
    "text": "I think that large\nlanguage models are much more than a way to\nprovide a policy with a task. So I mean, that's what\nwe do in this paper.",
    "start": "1195840",
    "end": "1202809"
  },
  {
    "text": "But as you can see\nfrom our results, language allows you to\ncompose semantics together, like unseen subject\nobject-verb combinations.",
    "start": "1202810",
    "end": "1209820"
  },
  {
    "text": "But beyond that, language\nhas a lot more kind of power than that. So there's this idea of\nChomsky and recursion, which",
    "start": "1209820",
    "end": "1217380"
  },
  {
    "text": "is that you can embed clauses\ninside of other clauses, and you can do this\nover and over again. You can perform logical\nreasoning within language",
    "start": "1217380",
    "end": "1225120"
  },
  {
    "text": "as you've seen with these\nOpenAI Codex examples. And sometimes even\nlike fuzzy logic is OK.",
    "start": "1225120",
    "end": "1231419"
  },
  {
    "text": "So when you write like\na programming docstring, it's kind of like\nlogical but not really, and still language models\ncan understand that.",
    "start": "1231420",
    "end": "1238600"
  },
  {
    "text": "So if you believe that large\nlanguage models can do this, then it might be a\nreally nice substrate",
    "start": "1238600",
    "end": "1244290"
  },
  {
    "text": "for improving generalization. And in machine learning,\nwe have this perspective that generalization is like\nthis notion of invariance.",
    "start": "1244290",
    "end": "1250560"
  },
  {
    "text": "So you see different images,\nbut ultimately, the things",
    "start": "1250560",
    "end": "1256410"
  },
  {
    "text": "that you see at test time, even\nthough you haven't seen them before, they mean the same\nthing as something you saw at training time, and\nthat's what allows robots",
    "start": "1256410",
    "end": "1263580"
  },
  {
    "text": "to generalize the new scenes. The linguist view\non generalization is that words are some sort\nof discrete unit of meaning",
    "start": "1263580",
    "end": "1270190"
  },
  {
    "text": "and language is about composing\ndiscrete tokens into more complex meanings. So as a recipe for making\nmore powerful, more general",
    "start": "1270190",
    "end": "1278800"
  },
  {
    "text": "systems, maybe what we\nshould do is take our data, like robot episodes\nor something.",
    "start": "1278800",
    "end": "1283900"
  },
  {
    "text": "We pair that with a bunch\nof rich human language, for example, by\njust describing it.",
    "start": "1283900",
    "end": "1290059"
  },
  {
    "text": "And then we train some\nsort of data model jointly with the language model. And we use the language\naspects of compositionality,",
    "start": "1290060",
    "end": "1296210"
  },
  {
    "text": "and recursion, and\nlogical reasoning to then now compose and\ncontrol the data in any way",
    "start": "1296210",
    "end": "1302522"
  },
  {
    "text": "that the language permits,\nwhich is pretty much everything. So I have a blog post where\nI talk about this called Just",
    "start": "1302522",
    "end": "1307910"
  },
  {
    "text": "Ask for Generalization. You want to take a look. And I think that's my time. So thank you very much.",
    "start": "1307910",
    "end": "1313732"
  },
  {
    "text": "Thank you, Eric. Are there any questions? ",
    "start": "1313733",
    "end": "1322789"
  },
  {
    "text": "I had a quick question. Super interesting work. I'm curious to what you--",
    "start": "1322790",
    "end": "1329330"
  },
  {
    "text": "if you see any sort of limits\nin the pre-training procedures we use for these\nlarge language models. And basically, if you could\npre-train that model yourself",
    "start": "1329330",
    "end": "1338043"
  },
  {
    "text": "or if you could make\na little adjustment to how that model\nwas pre-trained, do you think there's\na lot of headroom",
    "start": "1338043",
    "end": "1344240"
  },
  {
    "text": "still to squeeze a lot\nmore compositionally or recursion from those models?",
    "start": "1344240",
    "end": "1349549"
  },
  {
    "text": "Or do you think the pre-training\nwe do now is basically, the vast majority of the benefit\nwe're going to get from these?",
    "start": "1349550",
    "end": "1354815"
  },
  {
    "text": " I don't have a good\nanswer for that. If I had to take a guess,\nprobably some fine tuning",
    "start": "1354815",
    "end": "1362940"
  },
  {
    "text": "would be necessary. Because aligning the\nlanguage composition",
    "start": "1362940",
    "end": "1368005"
  },
  {
    "text": "and generalization\nto what you actually want at deployment\ntime seems tricky and maybe additional\nkind of feedback",
    "start": "1368005",
    "end": "1374670"
  },
  {
    "text": "via fine-tuning and customized\ndata collection would help. ",
    "start": "1374670",
    "end": "1382890"
  },
  {
    "text": "Excellent. So do we have any\nother questions from the participants.",
    "start": "1382890",
    "end": "1389540"
  },
  {
    "text": "I also actually have one\nquestion to you, Eric, as well. Do you think that-- it's really interesting\nthe perspective [INAUDIBLE]",
    "start": "1389540",
    "end": "1397220"
  },
  {
    "text": "that maybe we could just\ncollect robotic data and label it with language, and\nthen just ask for what we want.",
    "start": "1397220",
    "end": "1404660"
  },
  {
    "text": "Do you think there's\nan equivalent to-- there always exist a language\nprompt that you can specify,",
    "start": "1404660",
    "end": "1411950"
  },
  {
    "text": "that would be equivalent\nto an existing algorithm that we already have?",
    "start": "1411950",
    "end": "1419110"
  },
  {
    "text": "Basically, does this mapping\nelement exist, do you think? ",
    "start": "1419110",
    "end": "1424230"
  },
  {
    "text": "Are you saying, is\nit possible to get the behavior of any algorithm\nby conditioning on language?",
    "start": "1424230",
    "end": "1432180"
  },
  {
    "text": "That's right, yeah. I would argue, yes,\nbecause language also supports-- not only does it\nsupport fuzzy logic as we",
    "start": "1432180",
    "end": "1437850"
  },
  {
    "text": "see in the Codex, but it\nalso supports concrete logic. You can just put in the\ntokens of an actual program",
    "start": "1437850",
    "end": "1443280"
  },
  {
    "text": "into the language string. So if your neural network learns\nto do program interpretation, then it could conceptually\ndo that as well.",
    "start": "1443280",
    "end": "1450940"
  },
  {
    "text": "Cool. Very interesting. All right, I think we\nhave one raised hand. ",
    "start": "1450940",
    "end": "1458920"
  },
  {
    "text": "Hi, thanks for the great talk. My question relates to\nlonger horizon tasks.",
    "start": "1458920",
    "end": "1464840"
  },
  {
    "text": "And like I said longer\ncommands language sequences and how the actual performance\nvaries from longer horizon",
    "start": "1464840",
    "end": "1471630"
  },
  {
    "text": "tasks? And if you have any ideas on\novercoming some of the issues that [INAUDIBLE] [? face ?]\n[? with ?] learning a policy",
    "start": "1471630",
    "end": "1477583"
  },
  {
    "text": "for a long-horizon\ntask in this setting? Sure. So I'm just going\nto kind of focus",
    "start": "1477583",
    "end": "1482690"
  },
  {
    "text": "on the concrete robotic results\nand not any of this kind of speculative language stuff.",
    "start": "1482690",
    "end": "1487760"
  },
  {
    "text": "So in general, learning\nlonger horizon tasks is much harder than learning\nshort horizon tasks. And we see that there's a\nstrong negative correlation",
    "start": "1487760",
    "end": "1494059"
  },
  {
    "text": "between the length of a demo\nand our learn policies success rate. So we're controlling\nat 10 Hertz,",
    "start": "1494060",
    "end": "1500390"
  },
  {
    "text": "which is pretty high frequency\nfor robotic deep learning. And as a result, our performance\nnumbers aren't super good.",
    "start": "1500390",
    "end": "1506250"
  },
  {
    "text": "But yeah, they're in the 100\nto 200 number of decisions.",
    "start": "1506250",
    "end": "1511620"
  },
  {
    "text": "I think this is more of like\nhow do you get reliable learning in these kind of hard MDPs.",
    "start": "1511620",
    "end": "1517690"
  },
  {
    "text": "Yeah, so it's an open\nproblem in the field. Great. Thank you.",
    "start": "1517690",
    "end": "1524230"
  },
  {
    "text": "Great. Thank you. Thanks, Eric. Next, we'll have Corey Lynch\nwho is also a senior research",
    "start": "1524230",
    "end": "1532020"
  },
  {
    "text": "scientist at Google Brain\nwhere we work together on some of these things.",
    "start": "1532020",
    "end": "1537510"
  },
  {
    "text": "And she will talk to us a little\nbit about Language Conditioned Imitation Learning as well. But it will be\nslightly different.",
    "start": "1537510",
    "end": "1543995"
  },
  {
    "text": "And I think there may be\nsome different conclusions also about using\nlarge language models.",
    "start": "1543995",
    "end": "1549690"
  },
  {
    "text": "So Corey, please, take it away. Great. Thank you, Karol. Hi, I'm Corey.",
    "start": "1549690",
    "end": "1554940"
  },
  {
    "text": "And like Karol said, I'll be\npresenting some joint work with colleagues at Google Brain\non combining robot learning",
    "start": "1554940",
    "end": "1561210"
  },
  {
    "text": "with large language models. So much like the last\ntalk, the setting that we're interested\nin is a single robot",
    "start": "1561210",
    "end": "1567600"
  },
  {
    "text": "that can learn a very\nlarge number of tasks. And furthermore,\nwe'd like users to be able to specify any of\nthese tasks in free form",
    "start": "1567600",
    "end": "1575250"
  },
  {
    "text": "natural language. And also like the\nlast work, we're restricting our\nattention to just",
    "start": "1575250",
    "end": "1580350"
  },
  {
    "text": "simple supervised\nimitation learning. So a typical multi-task\nimitation learning setup",
    "start": "1580350",
    "end": "1586365"
  },
  {
    "text": "might look like this. Where you collect a large\nnumber of demonstration data sets for each task.",
    "start": "1586365",
    "end": "1592700"
  },
  {
    "text": "You perform supervised learning,\nand then at test time you might condition on things\nlike one-hot task id's.",
    "start": "1592700",
    "end": "1598279"
  },
  {
    "text": "But if you'd like to scale to\na very large number of text commands for imagine like push\nthe button or even something",
    "start": "1598280",
    "end": "1605210"
  },
  {
    "text": "like move your\nhand back slightly, something more interactive. You can imagine that these\ncollection requirements might",
    "start": "1605210",
    "end": "1610340"
  },
  {
    "text": "actually get quite\nexpensive, and this mode of behavior conditioning could\nbecome pretty inflexible.",
    "start": "1610340",
    "end": "1616299"
  },
  {
    "text": "So the main questions we\nasked ourselves in this work were how can we lower the\ncollectional requirements?",
    "start": "1616300",
    "end": "1621340"
  },
  {
    "text": "Can we instead\nlearn a large number of skills self-supervised\nfrom unlabeled data that's much less expensive to collect?",
    "start": "1621340",
    "end": "1627820"
  },
  {
    "text": "And second, if you've learned\na lot of these behaviors, how can you actually\ncondition them with things free-form text?",
    "start": "1627820",
    "end": "1633615"
  },
  {
    "text": "And then, finally, this one's\nkind of subtle and specific to text conditioning. Language can almost\nbe too flexible.",
    "start": "1633615",
    "end": "1639345"
  },
  {
    "text": "There's so many ways\nthat you can think of describing the same task. It's unlikely that you actually\ncover them all in training.",
    "start": "1639345",
    "end": "1644790"
  },
  {
    "text": "So how can you solve for\nnew synonym instructions at test time without having\nto collect new robot data?",
    "start": "1644790",
    "end": "1651920"
  },
  {
    "text": "So I'll quickly\ndescribe our approach. The first ingredient is a large\nunstructured play data set.",
    "start": "1651920",
    "end": "1657470"
  },
  {
    "text": "Here, a human\ntele-operates the robot and plays with the objects\nin the scene in as many ways that they can think of.",
    "start": "1657470",
    "end": "1663049"
  },
  {
    "text": "So you can see there's\nno upfront task definition and no resets. And this is important\nbecause it means",
    "start": "1663050",
    "end": "1668540"
  },
  {
    "text": "humans can quickly collect\nlots of demonstration data covering many tasks.",
    "start": "1668540",
    "end": "1673669"
  },
  {
    "text": "And so we see a lot of\nbehaviors on display here. How do you actually\ntrain a robot to execute specific ones\nfrom text descriptions?",
    "start": "1673670",
    "end": "1681529"
  },
  {
    "text": "So say you've collected a\nnumber of hours of play, you can now consider all 1-\nto 2-second random windows.",
    "start": "1681530",
    "end": "1688280"
  },
  {
    "text": "And one thing to note here\nis that each window contains the exact information for how\nthe robot got from an arbitrary",
    "start": "1688280",
    "end": "1694460"
  },
  {
    "text": "start state to an\narbitrary end state. And nothing stops\nyou from treating each of these as\ndemonstrations for how to reach",
    "start": "1694460",
    "end": "1701540"
  },
  {
    "text": "goal images with the robot. At the same time, we can\nsend less than 1% of these",
    "start": "1701540",
    "end": "1706700"
  },
  {
    "text": "to crowd-sourced annotators. And after watching a\nvideo, we can ask them, what text instruction\nwould you have",
    "start": "1706700",
    "end": "1712370"
  },
  {
    "text": "given the robot to get it to go\nfrom start frame to end frame? And so in this way, we\ncan efficiently mine two",
    "start": "1712370",
    "end": "1717890"
  },
  {
    "text": "demonstration data sets\nfrom play, one big one, solving for image goals,\nand a smaller one,",
    "start": "1717890",
    "end": "1722960"
  },
  {
    "text": "solving for text goals. So say you have these data sets. Rather than train a\nseparate policy on each,",
    "start": "1722960",
    "end": "1729200"
  },
  {
    "text": "we're now instead going to\ntrain one big policy to solve for either image\nor language goals.",
    "start": "1729200",
    "end": "1734840"
  },
  {
    "text": "And you can also\nthink of this as just doing language condition\nlearning with an auxiliary self supervised loss to\nimprove sample efficiency.",
    "start": "1734840",
    "end": "1742190"
  },
  {
    "text": "And we found that this sort\nof shared training scheme is actually fairly important. It allows us to reduce the\ncost of language annotation",
    "start": "1742190",
    "end": "1748880"
  },
  {
    "text": "to less than 1% of the\ncollected robot experience. And so at test time, we're only\ngoing to use text conditioning.",
    "start": "1748880",
    "end": "1756310"
  },
  {
    "text": "Here's an example of a\nhuman typing free form natural language\ncommands to the agent.",
    "start": "1756310",
    "end": "1762040"
  },
  {
    "text": "You can see the robot executing\nmultiple of these in a row. And the agent is receiving\nonboard RGB camera",
    "start": "1762040",
    "end": "1768160"
  },
  {
    "text": "input, and free free-form\ntext instructions, and then producing actions that\ncan close the loop at 30 Hertz.",
    "start": "1768160",
    "end": "1773230"
  },
  {
    "text": "So in the interest of\ntime, I'll refer you to more videos at\nthe link above. But then we've now described\na system that can, one,",
    "start": "1773230",
    "end": "1780370"
  },
  {
    "text": "learn behaviors from\nunstructured data, and two, allow people to command\nthese with natural language.",
    "start": "1780370",
    "end": "1786830"
  },
  {
    "text": "What about number three. So in our experiments, we found\na simple and what we think is-- actually, like a fairly\ngeneric solution to this.",
    "start": "1786830",
    "end": "1793059"
  },
  {
    "text": "Instead of learning on top\nof a raw text instructions, first embed them in the\nspace of a large language",
    "start": "1793060",
    "end": "1799659"
  },
  {
    "text": "model that's been pre-trained\nin web scale text data. So why do this? Pre-trained language\nmodels have been",
    "start": "1799660",
    "end": "1805780"
  },
  {
    "text": "shown to become\nsynonym aware, meaning they can map at\nthe sentence level",
    "start": "1805780",
    "end": "1811540"
  },
  {
    "text": "to disjoint, but\nsemantically similar sentences to the same\nregion of embedding space.",
    "start": "1811540",
    "end": "1816710"
  },
  {
    "text": "And so by doing\nrobot learning on top of pre-trained embeddings,\nthe language model is responsible for mapping\nnew instructions at test time",
    "start": "1816710",
    "end": "1824950"
  },
  {
    "text": "to ones that the robot has\nalready been trained to follow. And so we think this is a pretty\nnice separation of concerns.",
    "start": "1824950",
    "end": "1830590"
  },
  {
    "text": "This means that the\npolicy itself doesn't need to know how to generalize. It's just the language\nmodel that needs",
    "start": "1830590",
    "end": "1836110"
  },
  {
    "text": "to know how to generalize. And so practically, this\nmeans that your agent can now follow thousands of new\nsynonym instructions",
    "start": "1836110",
    "end": "1843850"
  },
  {
    "text": "at test time in zero-shot\nwithout requiring new robot demonstrations. Which is good because robot\ndemonstrations are expensive.",
    "start": "1843850",
    "end": "1851620"
  },
  {
    "text": "And another nice thing is that\nif your pre-training process happens to be multilingual,\nthis means that you can even",
    "start": "1851620",
    "end": "1857530"
  },
  {
    "text": "follow instructions\nin new languages that you've never collected. So this is what you see here.",
    "start": "1857530",
    "end": "1863659"
  },
  {
    "text": "This is our agent\nfollowing zero-shot French, even though it's only\nbeen trained in English. And I think this really\nillustrates the power of this.",
    "start": "1863660",
    "end": "1872020"
  },
  {
    "text": "Both me and the\nagent have only ever been trained to follow\ninstructions in English. But by just applying\nthis trick my agent",
    "start": "1872020",
    "end": "1878620"
  },
  {
    "text": "suddenly knows how to\ndemonstrate physical knowledge in French. So I think this sort\nof trick highlights",
    "start": "1878620",
    "end": "1884470"
  },
  {
    "text": "we're really only scratching the\nsurface of the knowledge that can be transferred from\nlarge language models",
    "start": "1884470",
    "end": "1889809"
  },
  {
    "text": "to robot learning. And so I think there's\na lot of exciting ways that we can think\nof transferring",
    "start": "1889810",
    "end": "1895120"
  },
  {
    "text": "even more of this knowledge. Thanks for your time. If you'd like to learn\nmore about this work, feel free to check out our paper\nor more videos on the project",
    "start": "1895120",
    "end": "1903970"
  },
  {
    "text": "page at language-play.github.io. Thanks. Thank you, Corey.",
    "start": "1903970",
    "end": "1909659"
  },
  {
    "text": "Are there any questions? ",
    "start": "1909660",
    "end": "1917419"
  },
  {
    "text": "I also have one question. Have you noticed-- I guess it's a little\ndifficult to evaluate this.",
    "start": "1917420",
    "end": "1926260"
  },
  {
    "text": "But can you tell whether you\ncan also generate new motions that you haven't seen before.",
    "start": "1926260",
    "end": "1931570"
  },
  {
    "text": "Like conditioning in\nlanguage in different ways? So it's not just\nnecessarily indexing",
    "start": "1931570",
    "end": "1937570"
  },
  {
    "text": "what it's already\nseen, but maybe it can also generalize to something\nthat it's never seen before.",
    "start": "1937570",
    "end": "1943450"
  },
  {
    "text": "Yeah, I think given\nthe fact that the-- I think this might be easier to\nstudy in a controlled setting",
    "start": "1943450",
    "end": "1950910"
  },
  {
    "text": "where you didn't have the\nunstructured demonstration data, because it's fairly hard\nto guarantee that you haven't",
    "start": "1950910",
    "end": "1956700"
  },
  {
    "text": "seen this motion before. But I think it's a very\ninteresting question. And it's probably\nworth some study.",
    "start": "1956700",
    "end": "1962970"
  },
  {
    "text": "Can you actually by conditioning\non a new region of language space get some sort of\ninterpolate of nice action",
    "start": "1962970",
    "end": "1972030"
  },
  {
    "text": "behavior that you can prove has\nnever been seen in training? It maybe hard to study\nin the exact setting",
    "start": "1972030",
    "end": "1977460"
  },
  {
    "text": "that we described here. But I definitely\nthink it's worth studying in a new setting. Awesome.",
    "start": "1977460",
    "end": "1982600"
  },
  {
    "text": "Thank you. Any other questions\nfrom the participants. We have a raised hand.",
    "start": "1982600",
    "end": "1987825"
  },
  {
    "text": " Yeah. Hi. Yeah, I guess my\nquestion is [INAUDIBLE]..",
    "start": "1987825",
    "end": "1997035"
  },
  {
    "text": "Is there a reason most\nlanguage [INAUDIBLE] nowadays tend to focus on\nimitation learning and not reinforcement learning directly?",
    "start": "1997035",
    "end": "2002950"
  },
  {
    "text": "So I can kind of get broader\nthan just the play data, and maybe start learning\nnewer things that's",
    "start": "2002950",
    "end": "2008259"
  },
  {
    "text": "not in the play data as well. And I guess-- Oh, sorry. And also, I guess, I don't know\nif you have any distribution",
    "start": "2008260",
    "end": "2016120"
  },
  {
    "text": "shift issues, if you just\ndo imitation learning, and yeah, usual issues with\n[INAUDIBLE] learning, I guess.",
    "start": "2016120",
    "end": "2024919"
  },
  {
    "text": "[INAUDIBLE] reinforcement\nlearning language? Yeah. I mean, I think\nthere's a lot of really interesting\nreinforcement learning",
    "start": "2024920",
    "end": "2032560"
  },
  {
    "text": "language plus language work. And I think you\nmight even be hearing about some really interesting\napplications of that",
    "start": "2032560",
    "end": "2039730"
  },
  {
    "text": "in the following speakers. But at least for us, the\nreason that we restricted",
    "start": "2039730",
    "end": "2045352"
  },
  {
    "text": "our attention to\nimitation learning was to avoid questions like\nhow do you specify rewards",
    "start": "2045352",
    "end": "2051730"
  },
  {
    "text": "in the real world? How do you do\nmeaningful exploration, targeted exploration?",
    "start": "2051730",
    "end": "2057969"
  },
  {
    "text": "These are like thorny\nquestions when you try to scale up to robotics. And so at least for\nus, the question",
    "start": "2057969",
    "end": "2064238"
  },
  {
    "text": "was, how far can we push\nsimple imitation learning kind of until it breaks. And as for things like\ndistribution shift,",
    "start": "2064239",
    "end": "2072010"
  },
  {
    "text": "if you check out the\npaper, where we actually have some interesting\nresults about the robustness",
    "start": "2072010",
    "end": "2077529"
  },
  {
    "text": "of policies when they're\ntrained over unstructured data, as opposed to kind of\nmore rigid curated data",
    "start": "2077530",
    "end": "2083469"
  },
  {
    "text": "sets, precisely to address these\nthings like distribution shift. So just by training\non more diverse data,",
    "start": "2083469",
    "end": "2092138"
  },
  {
    "text": "you can show that these\npolicies at test time are a little bit more--",
    "start": "2092139",
    "end": "2097990"
  },
  {
    "text": "like a little bit more robust\nto perturbation and distribution shift than if you\nhad trained them",
    "start": "2097990",
    "end": "2103990"
  },
  {
    "text": "on kind of more rigid and\nkind of curated data sets. But that being said, it is\nstill imitation learning.",
    "start": "2103990",
    "end": "2111530"
  },
  {
    "text": "So the problem is you can,\nof course, get off of-- you're still the subject to\nall of the compounding error,",
    "start": "2111530",
    "end": "2118660"
  },
  {
    "text": "you don't have like\na generic solution. Got it. And I guess-- sorry. Just one more\nfollow-up question.",
    "start": "2118660",
    "end": "2125670"
  },
  {
    "text": "I guess, if you're labeling\nplay data and then [INAUDIBLE],, I guess, also, there might be\nan issue of, if the play data--",
    "start": "2125670",
    "end": "2131940"
  },
  {
    "text": "like lets say you move\na cup from the top shelf to the bottom shelf-- isn't exactly optimal because\nmaybe the human is moving it",
    "start": "2131940",
    "end": "2137737"
  },
  {
    "text": "in a suboptimal way. And maybe [INAUDIBLE] language\ninstruction [INAUDIBLE].. Then I guess it\nwould be learning",
    "start": "2137737",
    "end": "2143580"
  },
  {
    "text": "a suboptimal trajectory for\nthat language instruction. Unless, you're-- I guess if the\nplay data is large enough this",
    "start": "2143580",
    "end": "2149130"
  },
  {
    "text": "won't be an issue. Yes, the key thing here\nis that we're restricting to short horizon behaviors.",
    "start": "2149130",
    "end": "2155590"
  },
  {
    "text": "And so certainly for like\na long enough horizon you might do a lot\nof suboptimal things",
    "start": "2155590",
    "end": "2161010"
  },
  {
    "text": "in between getting from the\nstart state to the end state. But at least by looking\nat 1- to 2-second windows,",
    "start": "2161010",
    "end": "2167970"
  },
  {
    "text": "empirically just by watching\na lot of these videos, you can convince yourself\nthat a lot of these",
    "start": "2167970",
    "end": "2173460"
  },
  {
    "text": "are fairly optimal\nfor the short horizon of straight line reaching\nstart state to end state.",
    "start": "2173460",
    "end": "2181769"
  },
  {
    "text": "And you also get--\nyou also get kind of text instructions\npaired to Windows that are like move\nyour arm to the left,",
    "start": "2181770",
    "end": "2188730"
  },
  {
    "text": "and then move it back. So not necessarily tasks\nyou would ever command, but more just descriptions\nof what's happening.",
    "start": "2188730",
    "end": "2194359"
  },
  {
    "text": "Got it. Thank you. All right. Thank you, Corey. And since we just--",
    "start": "2194360",
    "end": "2201569"
  },
  {
    "text": "there was a question about\nusing language and reinforcement learning, our next speaker\nwill talk exactly about this.",
    "start": "2201570",
    "end": "2207640"
  },
  {
    "text": "So next we have Suraj Nair, who\nis a student of Chelsea Finn, will tell us a little bit about\nhow we can apply large language",
    "start": "2207640",
    "end": "2215069"
  },
  {
    "text": "models or language in general\nin reinforcement learning. Take it away.",
    "start": "2215070",
    "end": "2220220"
  },
  {
    "text": "Great. Thanks, Karol. So hopefully everyone\ncan see my screen. I'm Suraj. I'm a PhD student with Chelsea.",
    "start": "2220220",
    "end": "2226500"
  },
  {
    "text": "And this is some work\nwe did in our lab and with some collaborators\nat Google as well,",
    "start": "2226500",
    "end": "2231930"
  },
  {
    "text": "called Learning\nLanguage-Conditioned Robot Behavior from Offline Data\nand Crowd-Sourced Annotation.",
    "start": "2231930",
    "end": "2237510"
  },
  {
    "text": "And the broader\ngoal in this work is similar to a few prior\npresentations as well, is we",
    "start": "2237510",
    "end": "2245010"
  },
  {
    "text": "want to have our robots be able\nto learn from large offline data sets to be able to--",
    "start": "2245010",
    "end": "2250920"
  },
  {
    "text": "many different downstream tasks. And particularly we'd\nlike to learn from kind",
    "start": "2250920",
    "end": "2256920"
  },
  {
    "text": "of non-expert offline data. And in this paradigm humans\nneed easy and effective ways",
    "start": "2256920",
    "end": "2263970"
  },
  {
    "text": "of actually specifying these\ntasks, and the rewards, and success criteria.",
    "start": "2263970",
    "end": "2269530"
  },
  {
    "text": "And so one common approach\nthat's used is goal images. So you could have an\nimage that specifies",
    "start": "2269530",
    "end": "2275310"
  },
  {
    "text": "the task and many prior\nworks have done this. And it's a natural choice,\nbecause these goal images",
    "start": "2275310",
    "end": "2281040"
  },
  {
    "text": "exist in the data set already. But there's a\nnumber of drawbacks, like the human action\nneeds to go in at test time",
    "start": "2281040",
    "end": "2286412"
  },
  {
    "text": "and create this still\nimage to specify the task. And it can often over\nspecify the task,",
    "start": "2286412",
    "end": "2291630"
  },
  {
    "text": "like different objects\nthat are not task relevant. Can't specify\npersistent behavior.",
    "start": "2291630",
    "end": "2296895"
  },
  {
    "text": "So in this work the\nquestion we're looking at is, well, can we actually do\nthis with natural language. Like just say, in this case,\npush the pink stapler forward.",
    "start": "2296895",
    "end": "2305910"
  },
  {
    "text": "It's much easier for\nhumans to provide and can flexibly\nrepresent many tasks.",
    "start": "2305910",
    "end": "2312700"
  },
  {
    "text": "So a little more concretely. In this work we're trying\nto learn language condition, vision motor manipulation\nskills on robots",
    "start": "2312700",
    "end": "2319710"
  },
  {
    "text": "from offline robot data and\ncrowd-sourced annotation. And critically, we want\nto learn from potentially",
    "start": "2319710",
    "end": "2327180"
  },
  {
    "text": "a highly suboptimal offline\ndata that's not expert data. And so this might be, for\nexample, data from some agents",
    "start": "2327180",
    "end": "2335070"
  },
  {
    "text": "that it's exploring\nautonomously, or maybe reusing the replay buffer of a\npreviously trained RL agent.",
    "start": "2335070",
    "end": "2342190"
  },
  {
    "text": "And then we want to have\ncrowd-sourced annotation tasks. What tasks, if any, are\nhappening in this data set?",
    "start": "2342190",
    "end": "2348630"
  },
  {
    "text": "And the key idea in our work\nis that we can't necessarily treat the actions in\nthis data as optimal.",
    "start": "2348630",
    "end": "2355810"
  },
  {
    "text": "But what we do know is\neven if the agent took some suboptimal trajectory,\ngoing from the beginning to the end of an\nepisode it's going",
    "start": "2355810",
    "end": "2362070"
  },
  {
    "text": "to complete whatever instruction\nthe human annotated it with. And we can then use this to\nlearn a language condition",
    "start": "2362070",
    "end": "2369900"
  },
  {
    "text": "reward function that can be\nused for offline reinforcement learning. So at a high level,\nour approach is",
    "start": "2369900",
    "end": "2376770"
  },
  {
    "text": "going to look like-- we'll\nhave some offline data. We're going to get\nit crowd-sourced.",
    "start": "2376770",
    "end": "2381780"
  },
  {
    "text": "Have annotators kind\nof annotate what is happening in each episode. So some episodes it\nmight be do nothing.",
    "start": "2381780",
    "end": "2388060"
  },
  {
    "text": "Some episodes it might be\ntry to insert the marker into the hole. And we're going to use this\nto learn a language condition",
    "start": "2388060",
    "end": "2394110"
  },
  {
    "text": "reward function. In the interest of time, I\nwon't go over the details on the exact offline\nRL method we used.",
    "start": "2394110",
    "end": "2401970"
  },
  {
    "text": "Like in our case we used\na model-based approach. But the key component\nhere is really",
    "start": "2401970",
    "end": "2407040"
  },
  {
    "text": "a learning-reward\nfunction that you can use with whatever\nyour favorite offline RL algorithm is.",
    "start": "2407040",
    "end": "2413490"
  },
  {
    "text": "And how do we actually learn it? Well, the reward\nfunction we train is basically just a binary\nclassifier of instruction",
    "start": "2413490",
    "end": "2420220"
  },
  {
    "text": "completion. So it's going to take an\ninitial state, a final state, and some instruction, and\nit's going to predict--",
    "start": "2420220",
    "end": "2426569"
  },
  {
    "text": "does transitioning from that\ninitial state to final state complete that instruction.",
    "start": "2426570",
    "end": "2432630"
  },
  {
    "text": "Collecting positive\nexamples for this is straightforward\nbecause we have episodes in our offline data and\ntheir corresponding annotations",
    "start": "2432630",
    "end": "2438450"
  },
  {
    "text": "that we can treat as positives. And you can actually\ngenerate, basically, as many negatives as you want\nby randomly combining episodes",
    "start": "2438450",
    "end": "2447323"
  },
  {
    "text": "with different instructions,\nand take an instruction from a different episode\nand put it in place. Or reversing the episode to\nbasically get these negatives.",
    "start": "2447323",
    "end": "2457770"
  },
  {
    "text": "And this classifier is going\nto use a pre-trained language model, and we'll see a bit\nmore about this in a few slides",
    "start": "2457770",
    "end": "2466410"
  },
  {
    "text": "in the experiments. But basically, using this\npre-trained language model enables some\ngeneralization even when",
    "start": "2466410",
    "end": "2471930"
  },
  {
    "text": "you have limited grounded\ndata of episodes, and their corresponding\nannotations.",
    "start": "2471930",
    "end": "2478920"
  },
  {
    "text": "So I'll skip over the\ncontrol part for now and then move on to just\nsome of the-- highlighting",
    "start": "2478920",
    "end": "2484735"
  },
  {
    "text": "the experiments. In the first one, we were\nbasically just looking at how does this\ncompare to using",
    "start": "2484735",
    "end": "2490470"
  },
  {
    "text": "goal images as specification\nor imitation learning. And the key takeaways here are\nthat, because the data we're",
    "start": "2490470",
    "end": "2497160"
  },
  {
    "text": "trying to learn from\nis really suboptimal, if you were trying to imitate\nit, or do language conditioning",
    "start": "2497160",
    "end": "2502260"
  },
  {
    "text": "imitation on this data it's\ngoing to perform poorly. Whereas using our\nmethod, you can actually improve over the behavior\npolicy that collected the data.",
    "start": "2502260",
    "end": "2510630"
  },
  {
    "text": "And it also outperforms say,\nusing goal images as your task specification, because those\nare prone to over specifying",
    "start": "2510630",
    "end": "2517470"
  },
  {
    "text": "the task. The next thing we look at\nis generalization to unseen",
    "start": "2517470",
    "end": "2523050"
  },
  {
    "text": "phrasings of your\ndifferent commands. So for example,\nwe might normally",
    "start": "2523050",
    "end": "2528539"
  },
  {
    "text": "be evaluating on tasks\nlike close the drawer, or turn the faucet left. And we look at, OK, what if\nwe swap out the verb, or noun,",
    "start": "2528540",
    "end": "2535470"
  },
  {
    "text": "or both with an unseen one. Or we actually sent\nour survey to the lab",
    "start": "2535470",
    "end": "2541260"
  },
  {
    "text": "and said phrase this instruction\nin some creative way. So instead of saying turn\nthe faucet left, some of you might say spin the nozzle left.",
    "start": "2541260",
    "end": "2547960"
  },
  {
    "text": "And what we see is\nthat with our method, we're actually pretty\nrobust to these rephrasing.",
    "start": "2547960",
    "end": "2554520"
  },
  {
    "text": "Looks like at most a\n10% drop in performance. And using a pre-trained language\nmodel is critical for this.",
    "start": "2554520",
    "end": "2560550"
  },
  {
    "text": "If we don't use the pre-trained\nlanguage model performance across the board is worse even\non the scene instructions.",
    "start": "2560550",
    "end": "2566220"
  },
  {
    "text": "And it's performance degrades\na lot more when you're giving unseen instructions.",
    "start": "2566220",
    "end": "2572460"
  },
  {
    "text": "So the pre-trained model is\npretty important for this. And then the last\nthing we looked at",
    "start": "2572460",
    "end": "2577620"
  },
  {
    "text": "was trying to actually\nuse this on a real robot. So we took basically,\na replay buffer",
    "start": "2577620",
    "end": "2583110"
  },
  {
    "text": "from a totally\ndifferent project, which was just 3,000 episodes of\nthe replay buffer trying to do",
    "start": "2583110",
    "end": "2588930"
  },
  {
    "text": "various skills on this desk-- of this robot\ninteracting on this desk, consisting of many unsuccessful\nand unsuccessful attempts.",
    "start": "2588930",
    "end": "2596490"
  },
  {
    "text": "And then we would crowd-source\nannotations for this, and then use our method. So this is the interface we gave\nto the annotators where we say,",
    "start": "2596490",
    "end": "2605319"
  },
  {
    "text": "OK, describe what the\nrobot's doing as a command. And then if you're not sure like\ndo nothing or you can't tell.",
    "start": "2605320",
    "end": "2614315"
  },
  {
    "text": "And so we got a lot of\nvery creative annotations from the crowd-sourced workers.",
    "start": "2614315",
    "end": "2622320"
  },
  {
    "text": "Things like rub the door handle. Hit the rack with the marker. But despite how noisy\nthey were, we're",
    "start": "2622320",
    "end": "2629340"
  },
  {
    "text": "still able to learn\nto then be able specify the robot to do tasks\nlike open the left drawer.",
    "start": "2629340",
    "end": "2634619"
  },
  {
    "text": "Open the right drawer,\nmoving the stapler and reaching the markers,\nor reaching the cabinet,",
    "start": "2634620",
    "end": "2645150"
  },
  {
    "text": "and also being robust. Again, just different ways\nof rephrasing the task. So instead of saying move\nthe stapler we could say,",
    "start": "2645150",
    "end": "2652569"
  },
  {
    "text": "push the small gray stapler\naround on top of the desk. And again, I would say largely\ndue to the pre-trained language",
    "start": "2652570",
    "end": "2657630"
  },
  {
    "text": "model, it's still able\nto complete these tasks. So just to sum up.",
    "start": "2657630",
    "end": "2663310"
  },
  {
    "text": "I think the key takeaways are\nto have multi-factor or useful multi-task robot policies.",
    "start": "2663310",
    "end": "2669090"
  },
  {
    "text": "Humans need easy ways\nof specifying tasks like natural language. And then using\nour method, we can",
    "start": "2669090",
    "end": "2675150"
  },
  {
    "text": "learn these language\nconditioned policies from very suboptimal\noffline data.",
    "start": "2675150",
    "end": "2680230"
  },
  {
    "text": "And then lastly, the\npre-trained language models are important to actually\nbeing able to generalize",
    "start": "2680230",
    "end": "2685650"
  },
  {
    "text": "unseen tasks. And while we did not\nhave-- we don't actually have an experiment\nof testing this, I suspect that it's\ncritical to being",
    "start": "2685650",
    "end": "2692850"
  },
  {
    "text": "able to handle the diversity\nand crowdsource annotations when you have annotators\ngiving you all sorts of funny ways of phrasing\nwhat the robot is doing.",
    "start": "2692850",
    "end": "2700401"
  },
  {
    "text": "I think without the\npre-trained language model, it would be probably be very\ndifficult to learn from that.",
    "start": "2700402",
    "end": "2706410"
  },
  {
    "text": "Yeah. That's it. Thanks. Thank you, Suraj.",
    "start": "2706410",
    "end": "2712320"
  },
  {
    "text": "Are there any questions? In the interest of time, I won't\nbe asking a question myself.",
    "start": "2712320",
    "end": "2717580"
  },
  {
    "text": "Let's see if there's anybody. ",
    "start": "2717580",
    "end": "2723000"
  },
  {
    "text": "Right, I don't\nsee any questions. Now, if you have any\nquestions, please just write them in the Chat. And I think maybe, Suraj,\nif you can stick around",
    "start": "2723000",
    "end": "2730200"
  },
  {
    "text": "for another five minutes\nor so that'll be great. Sure. Thank you, Suraj. Bye.",
    "start": "2730200",
    "end": "2737390"
  },
  {
    "text": "So yeah, we talked about\nhow we can fine tune these large models and then\nhow we can apply large language",
    "start": "2737390",
    "end": "2744380"
  },
  {
    "text": "models to other domains, such\nas robotics with reinforcement learning or imitation learning. A few takeaways.",
    "start": "2744380",
    "end": "2750740"
  },
  {
    "text": "We can transform\nthe gradient updates that can allow us to\nedit models sufficiently. And then we can also use\nlarge language models",
    "start": "2750740",
    "end": "2757970"
  },
  {
    "text": "to boost generalization. And we've found a few-- we've seen a few\nexamples in robotics",
    "start": "2757970",
    "end": "2763700"
  },
  {
    "text": "that show how we can generalize\nto new text commands. How we can potentially\ngeneralize to new languages",
    "start": "2763700",
    "end": "2769940"
  },
  {
    "text": "even. Potentially doing new\ntasks, and maybe even more. As Eric was saying there\nis a potential that--",
    "start": "2769940",
    "end": "2777839"
  },
  {
    "text": "there's a lot of potential in\nthese large language models and how they can be used. So next we'll talk about\nhow we can make reinforced",
    "start": "2777840",
    "end": "2785750"
  },
  {
    "text": "learning a better data sponge. So what I mean by this is that,\nif we look at large language",
    "start": "2785750",
    "end": "2791390"
  },
  {
    "text": "models, for instance, or any\nsupervised learning recipes right now, they\nrequire a lot of data,",
    "start": "2791390",
    "end": "2796550"
  },
  {
    "text": "and very expressive\ncapable models that's going to digest\nall of the data. So in other words,\nthey're able to absorb",
    "start": "2796550",
    "end": "2804020"
  },
  {
    "text": "large amounts of data. They're really good data sponge. And large language models\nis a perfect example of that, where they\ncan just-- they'll",
    "start": "2804020",
    "end": "2810403"
  },
  {
    "text": "go over the entire Wikipedia\nand take all of the data in. And as a result, we get\nreally good generalization",
    "start": "2810403",
    "end": "2817470"
  },
  {
    "text": "out of these models. So we can prompt\na model that was trained like that with\nsomething that is pretty clear.",
    "start": "2817470",
    "end": "2825090"
  },
  {
    "text": "That it shouldn't\nhave seen before, such as creating an image\nof a snail [INAUDIBLE] out of corkscrew.",
    "start": "2825090",
    "end": "2830520"
  },
  {
    "text": "And it actually does a\npretty good job doing that. Now, if we compare it to\nreinforcement learning.",
    "start": "2830520",
    "end": "2836070"
  },
  {
    "text": "In reinforcement\nlearning, when we usually talk about data we digest, it's\nusually something like this.",
    "start": "2836070",
    "end": "2842100"
  },
  {
    "text": "We just have a\n[INAUDIBLE] Cheetah, and that's the only\nenvironment we train on. And we evaluate in\nthe same environment.",
    "start": "2842100",
    "end": "2847620"
  },
  {
    "text": "Even if we expand on that and\nuse something like Meta-World that we discussed before,\nif we look at performance",
    "start": "2847620",
    "end": "2854490"
  },
  {
    "text": "of these multi-task\nreinforcement learning algorithms, they\ndon't seem to work very well across many different\ntasks with different reward",
    "start": "2854490",
    "end": "2862158"
  },
  {
    "text": "functions, different\ndifficulties of tasks, and so on. So if we were to draw a data\nsponge analogy for this.",
    "start": "2862158",
    "end": "2870059"
  },
  {
    "text": "Our data sponge is not\nvery, very absorbent for reinforcement learning.",
    "start": "2870060",
    "end": "2875619"
  },
  {
    "text": "So I think one set of really\ninteresting directions and interesting\n[INAUDIBLE] is how",
    "start": "2875620",
    "end": "2880740"
  },
  {
    "text": "we can make this data\nsponge for reinforcement learning much more absorbent.",
    "start": "2880740",
    "end": "2885750"
  },
  {
    "text": "So rather than working maybe\na better sample complexity. So how to use less data. How we could work on algorithms\nthat can utilize and digest",
    "start": "2885750",
    "end": "2892920"
  },
  {
    "text": "more data. Be a better data sponge. And in that vein, we'll\nhave four presentations.",
    "start": "2892920",
    "end": "2899809"
  },
  {
    "text": "And first, we'll start with\nhow we can, in the first place, just scale up data\ncollection itself.",
    "start": "2899810",
    "end": "2905690"
  },
  {
    "text": "And to talk about\nthis, we have with us Archit Sharma, who is a PhD\nstudent with Chelsea Finn as well.",
    "start": "2905690",
    "end": "2912076"
  },
  {
    "text": "Archit, please, take it away. Great. Hi, everyone.",
    "start": "2912076",
    "end": "2918300"
  },
  {
    "text": "I'm your favorite TA, Archit. And I'm going to be\ntalking about what I do outside holding office\nhours and making assignments.",
    "start": "2918300",
    "end": "2927030"
  },
  {
    "text": "More specifically, I'm\ngoing to talk about RL. And as I think, Karol\nhas already said,",
    "start": "2927030",
    "end": "2932380"
  },
  {
    "text": "we need to make--\nlike get more data. And I'm going to talk about\nwhy RL is not able to scale up",
    "start": "2932380",
    "end": "2939309"
  },
  {
    "text": "data collection at this point. And I would suggest like\nthis just boils down to making RL more autonomous.",
    "start": "2939310",
    "end": "2948030"
  },
  {
    "text": "The motivation\nbegins with comparing with how humans learn. And in this specific\ncase, we are",
    "start": "2948030",
    "end": "2953670"
  },
  {
    "text": "looking at example of\nhow a human juggles balls and how they're learning. And this is obviously\na very difficult task.",
    "start": "2953670",
    "end": "2960790"
  },
  {
    "text": "So the human needs to practice\nthis task several times. But the important point that\nI would like to emphasize here",
    "start": "2960790",
    "end": "2966270"
  },
  {
    "text": "is that the human doesn't\nneed external help",
    "start": "2966270",
    "end": "2971520"
  },
  {
    "text": "while practicing this task. So the balls fall down. They pick it up. They start again, and they\nimprove by themselves.",
    "start": "2971520",
    "end": "2977620"
  },
  {
    "text": "And this is a very\nimportant point, because to me, this is the\ngoal standard of autonomous learning. Now, let's contrast this with\nhow robots actually learn.",
    "start": "2977620",
    "end": "2986220"
  },
  {
    "text": "And again, in these\nspecific examples. The first robot is trying\nto hit the puck into a goal.",
    "start": "2986220",
    "end": "2992370"
  },
  {
    "text": "And in the second\nexample, the robot is trying to open the door. ",
    "start": "2992370",
    "end": "2997690"
  },
  {
    "text": "So in this case, whenever the\nrobot needs to practice again, a human first needs to intervene\ninto the data collection.",
    "start": "2997690",
    "end": "3004070"
  },
  {
    "text": "And then it needs to reset the\nenvironment so that the robot can start practicing again.",
    "start": "3004070",
    "end": "3009470"
  },
  {
    "text": "I hope at this point,\nit's very clear that the way robot\nlearning proceeds is far from autonomous.",
    "start": "3009470",
    "end": "3014660"
  },
  {
    "text": "Even though RL is such a\ngreat paradigm for learning, it still relies on\nhumans fairly heavily.",
    "start": "3014660",
    "end": "3022220"
  },
  {
    "text": "Now, learning autonomously isn't\njust about mimicking humans.",
    "start": "3022220",
    "end": "3029330"
  },
  {
    "text": "This leads to practical\nproblems as well as some of the previous results--",
    "start": "3029330",
    "end": "3035030"
  },
  {
    "text": "previous video suggested. So more specifically,\nlet's look at how RL works.",
    "start": "3035030",
    "end": "3042180"
  },
  {
    "text": "It starts off with the state s0. And then RL relies on collecting\ntrajectories in the environment world and it collects\na sequence of action--",
    "start": "3042180",
    "end": "3049263"
  },
  {
    "text": "takes a sequence of\nactions and collects a trajectory like this. From s0, ao, s1, a1,\nand reaches a state sH.",
    "start": "3049263",
    "end": "3055369"
  },
  {
    "text": " After a point we\nterminate this trajectory.",
    "start": "3055370",
    "end": "3061020"
  },
  {
    "text": "We re-initialize the\nenvironment at s0 prime and it collects a new\ntrajectory in the environment.",
    "start": "3061020",
    "end": "3066840"
  },
  {
    "text": "Now, reinforcement\nlearning, obviously, relies on a lot of\npracticing in the environment",
    "start": "3066840",
    "end": "3072829"
  },
  {
    "text": "and collecting a lot of\ntrajectories like this. But the big question is,\nwho takes the environment",
    "start": "3072830",
    "end": "3079309"
  },
  {
    "text": "from the state sH to s0 prime. And this might not seem\nimmediately obvious, but based on the examples\nthat you've seen thus far,",
    "start": "3079310",
    "end": "3086150"
  },
  {
    "text": "this often ends up being humans. And this ends up being a\nmajor problem because you're almost always reliant on humans\nbeing there for the robots",
    "start": "3086150",
    "end": "3093620"
  },
  {
    "text": "to continue learning. Now, assuming you\nbuy this premise.",
    "start": "3093620",
    "end": "3098820"
  },
  {
    "text": "We decided-- wait, we should-- we should first look at,\nI mean, it's very easy",
    "start": "3098820",
    "end": "3105540"
  },
  {
    "text": "to ask the question. That what if we just\nrun our algorithms for longer without resetting\nthe environment.",
    "start": "3105540",
    "end": "3112770"
  },
  {
    "text": "And this is kind of the question\nwe investigated as well. Which is what if we just\ntake our basic algorithms and just run them longer.",
    "start": "3112770",
    "end": "3119860"
  },
  {
    "text": "So we started with giving\nresets every 1,000 steps, and we increase this\nat 200,000 steps. And as you can see\nthe performance",
    "start": "3119860",
    "end": "3126000"
  },
  {
    "text": "depreciates quite a lot, which\nsuggests that the algorithms that we have right now are not\nexactly capable of operating",
    "start": "3126000",
    "end": "3132240"
  },
  {
    "text": "autonomously, and they're very\nheavily reliant on humans. So motivated by this, we\ndecided that we should maybe",
    "start": "3132240",
    "end": "3141000"
  },
  {
    "text": "change the problem itself. We should realize the constraint\nthat humans are not there to intervene for the robots\nto continue learning.",
    "start": "3141000",
    "end": "3150319"
  },
  {
    "text": "And so skip a few details\nhere, but the broad goal here is that the robot\nshould continue",
    "start": "3150320",
    "end": "3156590"
  },
  {
    "text": "interacting and learning\nwith the environment. And it's initialized\nat that environment once in the beginning, but after\nthat the interaction continues",
    "start": "3156590",
    "end": "3164059"
  },
  {
    "text": "for posterity. In formal terms\nthis will continue for an infinite\nhorizon, but there wouldn't be any resets to\ngenerate new trajectories",
    "start": "3164060",
    "end": "3170630"
  },
  {
    "text": "in the environment. And there are a few metrics\nthat we can optimize for.",
    "start": "3170630",
    "end": "3175730"
  },
  {
    "text": "But broadly, we want to learn\ngood behaviors that robots would want to execute\nor be used for,",
    "start": "3175730",
    "end": "3181910"
  },
  {
    "text": "and this is what we want\nto learn autonomously. Now, this is a\nreally hard problem.",
    "start": "3181910",
    "end": "3188850"
  },
  {
    "text": "And the way we do\nreinforcement learning clearly is not sufficient for it. So this motivated us to\ncreate a nice benchmark",
    "start": "3188850",
    "end": "3197000"
  },
  {
    "text": "where we have a diverse\ncollection of tasks which are representative\nof realistic scenarios.",
    "start": "3197000",
    "end": "3203000"
  },
  {
    "text": "And we wanted to\nencourage the community to come up with algorithms\nthat can actually learn without humans.",
    "start": "3203000",
    "end": "3208435"
  },
  {
    "text": "Because that would enable us\nto, a, run robots for longer, collect a lot more data.",
    "start": "3208435",
    "end": "3214200"
  },
  {
    "text": "And I would point out\na surprising result towards the end as well.",
    "start": "3214200",
    "end": "3220500"
  },
  {
    "text": "And some of the\ntasks we consider are manipulation related tasks. We have a multi-task\nkitchen environment as well where we have a\ndiverse set of tasks as well.",
    "start": "3220500",
    "end": "3227660"
  },
  {
    "text": "We have locomotion and we\nhave some high dimensional dexterous hand\nmanipulation tasks as well. And we strongly\nwant the community",
    "start": "3227660",
    "end": "3234859"
  },
  {
    "text": "to participate in\nimproving the algorithms and developing better ways\nto scale up data collection",
    "start": "3234860",
    "end": "3242180"
  },
  {
    "text": "and do RL in general. Unfortunately, I do\nnot have the time to go into the details of how\nto build these algorithms.",
    "start": "3242180",
    "end": "3249269"
  },
  {
    "text": "So I'm going to draw up\nsome references here. And always feel free\nto reach out to me.",
    "start": "3249270",
    "end": "3254460"
  },
  {
    "text": "But one thing I do\nwant to point out to keep the excitement is that,\nwhen agents learn autonomously,",
    "start": "3254460",
    "end": "3264120"
  },
  {
    "text": "it naturally leads\nto robustness. As you saw in the example\nof how human is learning, they learn to correct\ntheir own mistakes,",
    "start": "3264120",
    "end": "3269990"
  },
  {
    "text": "and this is a big part of\nreinforcement learning. But learning\nautonomously encourages robustness even more.",
    "start": "3269990",
    "end": "3277380"
  },
  {
    "text": "And this is really desirable\nfor practical robot learning or practical embodied agents.",
    "start": "3277380",
    "end": "3282470"
  },
  {
    "text": "And to quickly explain\nthe bar graph here. The oracle is the policy\nthat learns with resets.",
    "start": "3282470",
    "end": "3288740"
  },
  {
    "text": "And we see a much larger\ndrop in performance from the gray bar to the\ngreen bar when the conditions",
    "start": "3288740",
    "end": "3295190"
  },
  {
    "text": "it trained on changes. Whereas the FBRL and VaPRL\nare two different autonomous",
    "start": "3295190",
    "end": "3301700"
  },
  {
    "text": "algorithms. And basically have no drop\nin performance when the--",
    "start": "3301700",
    "end": "3307474"
  },
  {
    "text": "especially VaPRL doesn't\nhave a drop in performance when the initial conditions\nof the environment changes. Which suggest that if\nyou learn autonomously,",
    "start": "3307475",
    "end": "3313890"
  },
  {
    "text": "you would end up learning\nrobustly as well. So yeah, that's\nmy talk, and feel",
    "start": "3313890",
    "end": "3319320"
  },
  {
    "text": "free to reach out to\nme on Ed or whatever. So yeah, thanks.",
    "start": "3319320",
    "end": "3326190"
  },
  {
    "text": "Thank you so much. In the interest of time, I will\nask people to ask questions",
    "start": "3326190",
    "end": "3332130"
  },
  {
    "text": "in the Chat for now. Actually, we have\none quick question. Let's do it.",
    "start": "3332130",
    "end": "3338910"
  },
  {
    "text": "Hi. It's [INAUDIBLE]. So a quick question is it\nstill considered autonomous",
    "start": "3338910",
    "end": "3344120"
  },
  {
    "text": "learning if you have a\nsecondary robot reset the arm in the environment for you?",
    "start": "3344120",
    "end": "3349820"
  },
  {
    "text": "I noticed in one of your\nexamples, video examples, a door was opening and closing.",
    "start": "3349820",
    "end": "3355550"
  },
  {
    "text": "And a human had\nto close the door. Couldn't that have easily been\nsolved by having a circle?",
    "start": "3355550",
    "end": "3362810"
  },
  {
    "text": "That's exactly one of the\ntasks we have in the benchmark. And how you should do it. Because the same\nrobot can actually",
    "start": "3362810",
    "end": "3369470"
  },
  {
    "text": "learn how to open and\nclose the door, right? It doesn't need to\nbe done by the human. So if the robot--",
    "start": "3369470",
    "end": "3376460"
  },
  {
    "text": "there are ways to script\nthese things as well. But the idea is that we don't\nwant to spend a lot of time",
    "start": "3376460",
    "end": "3382820"
  },
  {
    "text": "with us doing the effort. But we want robots\nto learn without us expanding the effort.",
    "start": "3382820",
    "end": "3388400"
  },
  {
    "text": "So this is exactly one of the\ntasks we have in the benchmark as well. I see. Thank you. Yeah.",
    "start": "3388400",
    "end": "3394440"
  },
  {
    "text": "Thank you. Thanks, Archit. And yeah, if you\nhave more questions, please ask them in the Chat.",
    "start": "3394440",
    "end": "3399750"
  },
  {
    "text": "All right. So we talked a little\nbit about how we can scale up data collection. Let's now talk a\nlittle bit about how we",
    "start": "3399750",
    "end": "3405230"
  },
  {
    "text": "can use broader data sources. How we can actually\nmake reinforcement learning a better data sponge. So next step, we\nhave Annie Chen,",
    "start": "3405230",
    "end": "3413900"
  },
  {
    "text": "who is a PhD student with\nChelsea Finn, who will tell us a little bit about how we can\nuse other data sources as well.",
    "start": "3413900",
    "end": "3420290"
  },
  {
    "text": "Annie, take it away. Great, yeah. So as Karol mentioned, I--",
    "start": "3420290",
    "end": "3428570"
  },
  {
    "text": "hi, everyone. My name is Annie Chen. And I'm a PhD student\nworking with Chelsea,",
    "start": "3428570",
    "end": "3433760"
  },
  {
    "text": "and I'll be sharing a little\nbit from our recent work on learning generalizes-- Learning Generalizable\nRobotic Reward",
    "start": "3433760",
    "end": "3440390"
  },
  {
    "text": "Functions from\nIn-The-Wild Human Videos. And yeah this is a joint work\nwith Suraj Nair and Chelsea.",
    "start": "3440390",
    "end": "3450130"
  },
  {
    "text": "So we're motivated by\nthe overarching goal of a general\npurpose robot that's capable of completing\na wide range of tasks",
    "start": "3450130",
    "end": "3458079"
  },
  {
    "text": "across many different\nenvironments. Towards this goal,\nan agent must be able to acquire some metric of\ntask success or reward signal.",
    "start": "3458080",
    "end": "3467260"
  },
  {
    "text": "And this is necessary\nfor reinforcement learning, planning, knowing\nwhen to ask for help.",
    "start": "3467260",
    "end": "3472730"
  },
  {
    "text": "So here we're focused on\nlearning reward functions, which allow a robot to\ndetermine its own proficiency",
    "start": "3472730",
    "end": "3478840"
  },
  {
    "text": "at a specified task. We're particularly\ninterested in how in--",
    "start": "3478840",
    "end": "3486640"
  },
  {
    "text": "reward functions that\ncan generalize across varying environments and\ntasks, because otherwise we can't hope for our agents to\nbe able to use them to learn",
    "start": "3486640",
    "end": "3494920"
  },
  {
    "text": "general purpose policies. So a promising path\ntowards this goal",
    "start": "3494920",
    "end": "3499940"
  },
  {
    "text": "is to leverage large\nand diverse data sets, which we've seen have been\nimportant in achieving broader",
    "start": "3499940",
    "end": "3506810"
  },
  {
    "text": "generalization capabilities\nin computer vision and NLP.",
    "start": "3506810",
    "end": "3511820"
  },
  {
    "text": "However, collecting\nlarge amounts of diverse interaction\ndata on real robots",
    "start": "3511820",
    "end": "3517220"
  },
  {
    "text": "can be pretty challenging\nand time consuming. So alternatively in the\nwild, videos of humans",
    "start": "3517220",
    "end": "3525400"
  },
  {
    "text": "is this huge source\nof very diverse data that already exists. And so if we can teach robots\nusing this sort of data,",
    "start": "3525400",
    "end": "3533140"
  },
  {
    "text": "we can greatly increase\nthe diversity of the data that our agents can\nlearn from without",
    "start": "3533140",
    "end": "3538330"
  },
  {
    "text": "much additional overhead.  However, learning\nfrom this sort of data",
    "start": "3538330",
    "end": "3546210"
  },
  {
    "text": "is very challenging for\na number of reasons. So first, there exists\na large visual domain",
    "start": "3546210",
    "end": "3551759"
  },
  {
    "text": "shift in the observation space. Second, there is no clear\nmapping between a humans action",
    "start": "3551760",
    "end": "3556830"
  },
  {
    "text": "space and a robots. And lastly, in the\nwild human videos may often be low\nquality and noisy.",
    "start": "3556830",
    "end": "3564600"
  },
  {
    "text": "So how do we leverage this\ndata for a reward function? The key idea to\nour approach, which we call a domain agnostic\nvideo discriminator or DVD,",
    "start": "3564600",
    "end": "3573120"
  },
  {
    "text": "is to train a\ndiscriminator that looks at two videos and outputs,\nwhether they're completing the same task or not.",
    "start": "3573120",
    "end": "3579300"
  },
  {
    "text": "So we train the discriminator\nto predict, one, if two videos are\ndoing the same task,",
    "start": "3579300",
    "end": "3584700"
  },
  {
    "text": "and zero, if they are\ncompleting different tasks. We can then use this\ndiscriminator in conjunction",
    "start": "3584700",
    "end": "3591329"
  },
  {
    "text": "with any human\nvideo demonstration as a reward function to\nsolve the task that's",
    "start": "3591330",
    "end": "3596609"
  },
  {
    "text": "specified by that video. So unfortunately,\nI think I'm going to have to skip the\nspecifics of that for time.",
    "start": "3596610",
    "end": "3604510"
  },
  {
    "text": "But what this\nactually looks like is given a human\nvideo demonstration for closing a drawer.",
    "start": "3604510",
    "end": "3611250"
  },
  {
    "text": "Our method enables the agent to\nclose the door in these unseen test environments.",
    "start": "3611250",
    "end": "3617980"
  },
  {
    "text": "And here's another example with\nturning a faucet to the right. So even though as\nyou can see there's",
    "start": "3617980",
    "end": "3623740"
  },
  {
    "text": "a large visual domain shift\nbetween the given video, and the simulation\nenvironments, DVD",
    "start": "3623740",
    "end": "3629740"
  },
  {
    "text": "allows the agent to complete\nthe task in these unseen environments.",
    "start": "3629740",
    "end": "3635790"
  },
  {
    "text": "The important takeaway\nis that training using human videos shown\nin the purple and red bars",
    "start": "3635790",
    "end": "3641280"
  },
  {
    "text": "improves environment\ngeneralization significantly compared to only using\nrobot data, which is shown in the green bars.",
    "start": "3641280",
    "end": "3650090"
  },
  {
    "text": "And here we also find that\nDVD works on a real robot. So the first video here is\nthe agent closing the door,",
    "start": "3650090",
    "end": "3658460"
  },
  {
    "text": "but without having ever\nseen this toy kitchen set before during training.",
    "start": "3658460",
    "end": "3664100"
  },
  {
    "text": "And the second video, which\nis pushing the tissue box to the left, is\nwithout having seen",
    "start": "3664100",
    "end": "3670190"
  },
  {
    "text": "any examples of this task of\npushing things to the left during training. ",
    "start": "3670190",
    "end": "3677410"
  },
  {
    "text": "And so in summary, DVD is a\nsimple and scalable approach to task specification\nthat can leverage",
    "start": "3677410",
    "end": "3683820"
  },
  {
    "text": "the diversity of these\nin-the-wild human videos to generalize to new\nenvironments and tasks.",
    "start": "3683820",
    "end": "3690760"
  },
  {
    "text": "So the hope is that DVD\ninspires a future work in applying broad data sets\nfor more robust generalization",
    "start": "3690760",
    "end": "3698550"
  },
  {
    "text": "capabilities. And there's a lot of\nreally exciting directions for future work along this line.",
    "start": "3698550",
    "end": "3705640"
  },
  {
    "text": "So first our method focuses only\non learning reward functions that generalize and\ndoesn't actually",
    "start": "3705640",
    "end": "3711690"
  },
  {
    "text": "learn a generalizable\npolicy directly. Also so far, we've\nonly tested DVD on course tasks\nthat don't require",
    "start": "3711690",
    "end": "3717809"
  },
  {
    "text": "fine grained manipulations. And lastly, the reward\nis non-Markovian.",
    "start": "3717810",
    "end": "3723810"
  },
  {
    "text": "But it'd be\ninteresting to explore using the reward to learn model\nfree policies with memory.",
    "start": "3723810",
    "end": "3732173"
  },
  {
    "text": "And that's all I have. So thank you for\nlistening and feel free to reach out if you're\ncurious to learn more",
    "start": "3732173",
    "end": "3737520"
  },
  {
    "text": "about this. Thank you, Annie. Thank you for the\ngreat presentation. And it's really\ncool to see having",
    "start": "3737520",
    "end": "3743880"
  },
  {
    "text": "some works that apply data that\ngoes beyond just robotic data. Really cool.",
    "start": "3743880",
    "end": "3750630"
  },
  {
    "text": "In the interest of time, I will\nask students to ask questions in the Chat.",
    "start": "3750630",
    "end": "3755730"
  },
  {
    "text": "And we'll move to\nour next speaker, who is Igor Mordatch, who's a\nresearch scientist at Google",
    "start": "3755730",
    "end": "3762599"
  },
  {
    "text": "Brain as well. And he will tell us\na little bit more about how we can\nactually simplify",
    "start": "3762600",
    "end": "3768420"
  },
  {
    "text": "reinforce learning formulation. And maybe use some\nof the ideas that we know from large language\nmodels and supervised",
    "start": "3768420",
    "end": "3774510"
  },
  {
    "text": "learning to do reinforcement\nlearning better-- reinforcement learning a little bit simpler.",
    "start": "3774510",
    "end": "3780280"
  },
  {
    "text": "Take it away, Igor. Yeah. Yeah, thank you. Thank you. So yes, Karol was\nsaying, I'll be",
    "start": "3780280",
    "end": "3787440"
  },
  {
    "text": "talking about\ntreating reinforcement learning as sequence modeling. And by sequence\nmodeling colloquially,",
    "start": "3787440",
    "end": "3794160"
  },
  {
    "text": "I mean, combining say\ntransformer-based architectures with large-scale pre-training.",
    "start": "3794160",
    "end": "3799740"
  },
  {
    "text": "And I believe Colin a few weeks\nback already talked quite a bit about such models.",
    "start": "3799740",
    "end": "3806490"
  },
  {
    "text": "So we are seeing a lot\nof research computing-- infrastructure\ninvestment into this. And so it's worth asking how\ncan we as practitioners in AI",
    "start": "3806490",
    "end": "3815250"
  },
  {
    "text": "and robotics get the most\nout of this investment. And so what we've\ntried to do is to frame",
    "start": "3815250",
    "end": "3823650"
  },
  {
    "text": "the problem of making\ndecisions, which would normally associate with reinforcement\nlearning as a sequence modeling",
    "start": "3823650",
    "end": "3830040"
  },
  {
    "text": "problem. And by that, I mean,\nnot just using, say transformer networks to\nmodel your policy or value",
    "start": "3830040",
    "end": "3839250"
  },
  {
    "text": "functions. But actually replace\nreinforcement learning algorithms involving\na lot of moving pieces",
    "start": "3839250",
    "end": "3846119"
  },
  {
    "text": "such as recursive development\nupdates, choices of discounts, factors, and so on, replace\nall of that complexity",
    "start": "3846120",
    "end": "3852900"
  },
  {
    "text": "with just a single sequence\nmodeling objective. So this is a very drastic\nproblem reduction.",
    "start": "3852900",
    "end": "3859260"
  },
  {
    "text": "But I think it's worth\nasking how far can just such a reduction take us.",
    "start": "3859260",
    "end": "3865230"
  },
  {
    "text": "And this is the\nhypothesis that we try to explore in their recent\ndecision transformer paper",
    "start": "3865230",
    "end": "3872250"
  },
  {
    "text": "with colleagues at UC Berkeley. And so we're going to be\nplaying around with here is actually just the sequences,\nthe nature of the sequences",
    "start": "3872250",
    "end": "3881050"
  },
  {
    "text": "that we feed into these models. The choice that we've\nexplored in the work",
    "start": "3881050",
    "end": "3886540"
  },
  {
    "text": "is actually sequences of\nreturns, followed by states,",
    "start": "3886540",
    "end": "3892630"
  },
  {
    "text": "followed by actions. So these triples just\nrepeated all over",
    "start": "3892630",
    "end": "3897700"
  },
  {
    "text": "for across different time steps. And by returns here,\nI mean, actually,",
    "start": "3897700",
    "end": "3904540"
  },
  {
    "text": "the sum of all the\nfuture rewards. So what this in essence\ncan actually do, is it can incorporate some\nhindsight information you have",
    "start": "3904540",
    "end": "3913090"
  },
  {
    "text": "about the outcomes of\nthis episode earlier on into the sequence, which\nends up being very useful",
    "start": "3913090",
    "end": "3920680"
  },
  {
    "text": "as we'll see. So we take these tokens of\nreturn, states, and actions,",
    "start": "3920680",
    "end": "3927339"
  },
  {
    "text": "heap them into a causal\ntransformer model, such as, for example, what\nGPT models use.",
    "start": "3927340",
    "end": "3933160"
  },
  {
    "text": "And we ask it to predict\nsequences of actions.",
    "start": "3933160",
    "end": "3939230"
  },
  {
    "text": "And at a deployment\ntime, we just ought to aggressively predict\nthese action sequences",
    "start": "3939230",
    "end": "3945740"
  },
  {
    "text": "conditioned on some target\nreturn, or maybe more broadly,",
    "start": "3945740",
    "end": "3951560"
  },
  {
    "text": "some notion of optimality. So for example, we may\nask that we've seen --",
    "start": "3951560",
    "end": "3958460"
  },
  {
    "text": "there's some target\nscore in a game that we know that we can\nget to, and we just-- and we've seen that some\ndemonstration data-- this is",
    "start": "3958460",
    "end": "3966707"
  },
  {
    "text": "in the offline RL setting. It was able to achieve it. So we put that as target, and\nthis idea actually goes back.",
    "start": "3966707",
    "end": "3973280"
  },
  {
    "text": "Has really interesting deep\nroots in optimal control and inferences.",
    "start": "3973280",
    "end": "3981500"
  },
  {
    "text": "So after doing all\nof this, the results are actually very promising\nin the offline RL regime where",
    "start": "3981500",
    "end": "3990510"
  },
  {
    "text": "we're seeing we've tested\nit on discrete control benchmarks such as a\nnumber of Atari games.",
    "start": "3990510",
    "end": "3997550"
  },
  {
    "text": "Continuous control, like\nthe openAI Gym environments. And these somewhat unorthodox\nkey-to-door environment",
    "start": "3997550",
    "end": "4008290"
  },
  {
    "text": "from DeepMind. Which basically is really\ntargeting long-term credit assignment where I can only\nat the end of the episode",
    "start": "4008290",
    "end": "4017740"
  },
  {
    "text": "open the door and\nget a reward if I have picked up the key much,\nmuch earlier on in the episode.",
    "start": "4017740",
    "end": "4024650"
  },
  {
    "text": "So you really have to do the\nright thing in the beginning. And we're seeing that\nthese models actually",
    "start": "4024650",
    "end": "4030670"
  },
  {
    "text": "do in cases significantly\nbetter than behavioral cloning. And in some cases,\nkind of about the same,",
    "start": "4030670",
    "end": "4037420"
  },
  {
    "text": "or sometimes better, sometimes\nworse than temporal difference learning approaches\nto offline RL.",
    "start": "4037420",
    "end": "4044950"
  },
  {
    "text": "And there's actually\nsome interesting things where we have these key\nto door environments. They tend to various\nvery significant actions",
    "start": "4044950",
    "end": "4053829"
  },
  {
    "text": "like the attention\nweights in these models. They tend to significant\nevents in the past.",
    "start": "4053830",
    "end": "4060650"
  },
  {
    "text": "So I think I'll skip some\nof the investigations.",
    "start": "4060650",
    "end": "4065720"
  },
  {
    "text": "But the idea here is not to\ntry to introduce new algorithms or models, but it's really\ncareful analysis, and problem",
    "start": "4065720",
    "end": "4074110"
  },
  {
    "text": "reduction to\nactually whittle down the space of possible methods\nand strive for simplicity.",
    "start": "4074110",
    "end": "4079640"
  },
  {
    "text": "And the ways to build on\nour community's efforts more effectively. So these are the\nlinks to the paper,",
    "start": "4079640",
    "end": "4086440"
  },
  {
    "text": "and the code if\nyou're interested. And I think that's it for me.",
    "start": "4086440",
    "end": "4092860"
  },
  {
    "text": "Thank you so much, Igor. Thanks for the great talk. I'll also ask students to\nask questions on the Chat",
    "start": "4092860",
    "end": "4100979"
  },
  {
    "text": "in the interest of time. And next we have\nwith us Annie Xie,",
    "start": "4100979",
    "end": "4107520"
  },
  {
    "text": "who will talk to us a\nlittle bit about how we can apply reinforcement\nlearning to lifelong settings.",
    "start": "4107520",
    "end": "4113130"
  },
  {
    "text": "Where we can extend the\nperiod of an experiment or where we can extend\nthe lifetime of a RL",
    "start": "4113130",
    "end": "4120330"
  },
  {
    "text": "beyond a single experiment. Annie is a PhD student\nwith Chelsea Finn.",
    "start": "4120330",
    "end": "4127161"
  },
  {
    "text": "Annie, please take it away. So I am Annie and\ntoday I'll be talking",
    "start": "4127161",
    "end": "4134318"
  },
  {
    "text": "about some very recent work\non life long robot learning.",
    "start": "4134319",
    "end": "4139409"
  },
  {
    "text": "So some of the current ways\nto learn multiple tasks are to learn each\none from scratch or to learn them together\nunder the multi-task learning",
    "start": "4139410",
    "end": "4147350"
  },
  {
    "text": "framework. And in standard multi-task\nlearning the agent needs to collect data\nin each new task--",
    "start": "4147350",
    "end": "4154818"
  },
  {
    "text": "sorry-- needs to collect data\nin each new task around Robin style. But this for my data\ncollection isn't",
    "start": "4154819",
    "end": "4160640"
  },
  {
    "text": "very practical for a lot\nof real robot setups. And so for example, if\nyou consider these two",
    "start": "4160640",
    "end": "4168318"
  },
  {
    "text": "tasks repeatedly switching\nbetween the lock insertion and this bottle capping\nsetup isn't very realistic.",
    "start": "4168319",
    "end": "4173899"
  },
  {
    "text": "And what would be\nmore natural instead are robots that can\nlearn tasks in sequence.",
    "start": "4173899",
    "end": "4179839"
  },
  {
    "text": "And that's just the\nlifelong learning problem. So a lot of the work\nin lifelong learning",
    "start": "4179840",
    "end": "4185450"
  },
  {
    "text": "has been concerned with\ncatastrophic forgetting, which evaluates how well\nan agent that's",
    "start": "4185450",
    "end": "4190609"
  },
  {
    "text": "given a finite amount\nof memory and parameters can perform on\npreviously solved tasks.",
    "start": "4190609",
    "end": "4196730"
  },
  {
    "text": "And in this setup,\nwe're kind of concerned about a different aspect\nof lifelong learning, which",
    "start": "4196730",
    "end": "4201890"
  },
  {
    "text": "is forward transfer. Specifically, we allow\nthe agent to retain all of its previous experiences,\nwhich isn't unreasonable",
    "start": "4201890",
    "end": "4210199"
  },
  {
    "text": "if we consider the\ncost of storing data compared to the cost of\ncollecting significantly",
    "start": "4210200",
    "end": "4215330"
  },
  {
    "text": "more data. And so we also simplify\nthe problem a bit by assuming that the\nsame action spaces are",
    "start": "4215330",
    "end": "4222199"
  },
  {
    "text": "shared between [INAUDIBLE] vary\nacross the different tasks. And the agent in our setup also\nonly advances to the next task",
    "start": "4222200",
    "end": "4230600"
  },
  {
    "text": "once it finishes\nlearning the current one. So, on this slide, we\noutline our solution",
    "start": "4230600",
    "end": "4236239"
  },
  {
    "text": "to this problem, which\nbasically alternates between an offline\npre-training phase, and an online improvement\nphase for every new task",
    "start": "4236240",
    "end": "4243560"
  },
  {
    "text": "that comes in. And during the offline\nphase, we are essentially restoring the replay buffers\nfrom the earlier tasks",
    "start": "4243560",
    "end": "4250430"
  },
  {
    "text": "and pre-training the\nagent on this data. Then during the online phase, we\nwill improve the agent's policy",
    "start": "4250430",
    "end": "4259050"
  },
  {
    "text": "with data that's\ncollected online. And instead of\njust throwing away the prior experiences\nat this point,",
    "start": "4259050",
    "end": "4264910"
  },
  {
    "text": "we can actually\ntry to re-use them. But to account for the\ndifferences in the data",
    "start": "4264910",
    "end": "4270510"
  },
  {
    "text": "distribution, we\nre-weigh the samples based on how likely they\nare under the current task",
    "start": "4270510",
    "end": "4276690"
  },
  {
    "text": "versus an earlier one. And this ratio can be\ndecomposed into these to highlight terms here.",
    "start": "4276690",
    "end": "4283050"
  },
  {
    "text": "The first term basically\nsays given a transition, how likely is this sample\nfrom the current task,",
    "start": "4283050",
    "end": "4288840"
  },
  {
    "text": "versus a previous one? And we can estimate this ratio\nwith a binary classifier that's",
    "start": "4288840",
    "end": "4295139"
  },
  {
    "text": "trained on the transitions. And we also transform this\nratio by thresholding it.",
    "start": "4295140",
    "end": "4300660"
  },
  {
    "text": "So we're essentially filtering\nout the irrelevant samples. And we don't-- And basically, we\ndon't have to actively use all",
    "start": "4300660",
    "end": "4307469"
  },
  {
    "text": "the samples in the data set. And the second term, that's\nunderlined in yellow,",
    "start": "4307470",
    "end": "4313020"
  },
  {
    "text": "can be estimated with the\nsize of the data sets. And what this term\nis basically saying is we're going to decrease\nthe weight of prior samples",
    "start": "4313020",
    "end": "4320460"
  },
  {
    "text": "as we collect more new\nsamples in the current task.",
    "start": "4320460",
    "end": "4325810"
  },
  {
    "text": "So next we actually wanted\nto leverage the prior data even more specifically\nfor exploration.",
    "start": "4325810",
    "end": "4332739"
  },
  {
    "text": "Essentially, the\nintuition carries that prior experience\non related tasks",
    "start": "4332740",
    "end": "4337750"
  },
  {
    "text": "can inform the agent\nto which states to explore in future ones. For example, a robot that's\ndoing tabletop manipulation",
    "start": "4337750",
    "end": "4345760"
  },
  {
    "text": "generally needs to move\nits arm towards objects. And we build in this\nheuristic by uploading samples",
    "start": "4345760",
    "end": "4351820"
  },
  {
    "text": "that states that were\nmore likely to earlier tasks than the current ones. And by doing so the agent is\nincentivized to visit them more",
    "start": "4351820",
    "end": "4359230"
  },
  {
    "text": "if they are useful.  And then to evaluate\nour approach,",
    "start": "4359230",
    "end": "4365110"
  },
  {
    "text": "we designed a family\nof key insertion tasks, where we vary the\nplacement of the box,",
    "start": "4365110",
    "end": "4370730"
  },
  {
    "text": "as well as whether they're\nplaced flat or vertically. And we also varied the size\nand the orientation of the key.",
    "start": "4370730",
    "end": "4377560"
  },
  {
    "text": "And for comparison, we looked\nat a few prior approaches that transferred learned weights\nlike progressive networks",
    "start": "4377560",
    "end": "4384970"
  },
  {
    "text": "and continuous fine tuning. We also looked at approaches\nthat transfer prior data",
    "start": "4384970",
    "end": "4390850"
  },
  {
    "text": "like this method called DARC\nand standard off-policy RL. And overall, our method achieves\nthe highest success rate",
    "start": "4390850",
    "end": "4399520"
  },
  {
    "text": "across three different\ntask sequences.",
    "start": "4399520",
    "end": "4404590"
  },
  {
    "text": "And it even does\nbetter than if we were to learn each\ntask from scratch.",
    "start": "4404590",
    "end": "4410410"
  },
  {
    "text": "And then finally, we evaluated\nour algorithm on a Franka robot on a sequence of tasks that\ninclude setups like bottle",
    "start": "4410410",
    "end": "4419020"
  },
  {
    "text": "capping and block insertion. ",
    "start": "4419020",
    "end": "4426220"
  },
  {
    "text": "And to summarize,\nour method gets within [INAUDIBLE]\ncentimeters of the goal, which",
    "start": "4426220",
    "end": "4431559"
  },
  {
    "text": "is less than half compared\nto learning from scratch. Yeah, that's it. ",
    "start": "4431560",
    "end": "4438940"
  },
  {
    "text": "I guess there's no\ntime for questions, but feel free to contact me.",
    "start": "4438940",
    "end": "4444780"
  },
  {
    "text": "Yeah. Thank you so much, Annie. We'll have questions\non the Chat, so if you could stick\naround a little bit,",
    "start": "4444780",
    "end": "4451449"
  },
  {
    "text": "that would be great. Thank you. It's really good\nto see that we're going to go beyond removing\nall the or running--",
    "start": "4451450",
    "end": "4458920"
  },
  {
    "text": "collecting data\nonly for one paper, and then not getting\nback to this ever again. Very cool work.",
    "start": "4458920",
    "end": "4464530"
  },
  {
    "text": "Thank you. And yes, I'll ask the students\nto ask the questions over Chat",
    "start": "4464530",
    "end": "4470020"
  },
  {
    "text": "in the interest of time. And we'll switch\nover to presenting.",
    "start": "4470020",
    "end": "4478010"
  },
  {
    "text": " All right, so we\ntalked a little bit about how we can scale\nup data collection.",
    "start": "4478010",
    "end": "4485570"
  },
  {
    "text": "How we can utilize broader\noffline data sources, including non-robotic data sources. And then how we can\nchange the algorithms,",
    "start": "4485570",
    "end": "4491750"
  },
  {
    "text": "reinforced learning\nalgorithms to look more like supervised\nlearning algorithms, so that maybe they\ncan utilize more data.",
    "start": "4491750",
    "end": "4498330"
  },
  {
    "text": "And then at the end,\nAnnie also told us a little bit how we\ncan apply reinforced learning to lifelong settings.",
    "start": "4498330",
    "end": "4506090"
  },
  {
    "text": "So there are a few takeaways\nthat we could draw from this.",
    "start": "4506090",
    "end": "4511900"
  },
  {
    "text": "We can scale up data collection\nby eliminating resets. And it's important also to keep\nin mind as Archit pointed out",
    "start": "4511900",
    "end": "4518270"
  },
  {
    "text": "in the Chat, that actually by\nmaking reinforcement learning more autonomous, there's many\nmore advantages than just",
    "start": "4518270",
    "end": "4525909"
  },
  {
    "text": "scaling of data collection. And then there are multiple\nways to broaden data sources by for instance\nincorporating robotic data",
    "start": "4525910",
    "end": "4532929"
  },
  {
    "text": "such as human videos in-the-wild\nor casting reinforcement learning into a simpler\nproblem formulation.",
    "start": "4532930",
    "end": "4539480"
  },
  {
    "text": "And then we can also\nextend reinforcement learning to lifelong settings\nby retaining experiences.",
    "start": "4539480",
    "end": "4546159"
  },
  {
    "text": "All right, so in\nthe remaining-- oh, we don't have that much time. So I wanted to briefly tell you\nin the remaining two minutes,",
    "start": "4546160",
    "end": "4553665"
  },
  {
    "text": "let's say, I wanted\nto briefly tell you about some other open challenges\nthat we haven't previously covered.",
    "start": "4553665",
    "end": "4560540"
  },
  {
    "text": "And there is a few, although\nwe covered a lot of ground. Generalization,\nwhere we assume that our meta-test distribution,\nmeta-training distributions",
    "start": "4560540",
    "end": "4568670"
  },
  {
    "text": "are the same. And it's very\noften not the case. So we also need to\nbe able to generalize out-of-distribution\ntasks, which is often",
    "start": "4568670",
    "end": "4574940"
  },
  {
    "text": "the case with long-tailed\ndistributions. And I'll skip the\ndetails on that.",
    "start": "4574940",
    "end": "4580610"
  },
  {
    "text": "We also need to look a little\nbit more into multimodality. So how can we learn prior from\nmultiple modalities of data.",
    "start": "4580610",
    "end": "4587878"
  },
  {
    "text": "So far, we've mostly\nlooked at the problems where the predominant\nmodality was vision. There are many other\nmodalities that--",
    "start": "4587878",
    "end": "4595400"
  },
  {
    "text": "such as, touch for instance,\nthat often are sound, that often have some\nadditional complimentary forms",
    "start": "4595400",
    "end": "4603370"
  },
  {
    "text": "of information that we\nshould be looking at as well. And then there are\nquestions regarding the algorithm, model selection.",
    "start": "4603370",
    "end": "4609880"
  },
  {
    "text": "So answering questions\nsuch as, when will we know whether multi-task\nor mental learning is going to help?",
    "start": "4609880",
    "end": "4615489"
  },
  {
    "text": "Or how to select the best\nmodel based on the algorithm that we run?",
    "start": "4615490",
    "end": "4620790"
  },
  {
    "text": "Then there is more work to be\ndone in terms of benchmarks, although we are getting a\nlittle bit better at this.",
    "start": "4620790",
    "end": "4626260"
  },
  {
    "text": "We want to have benchmarks that\nhave sufficient breadth, that challenge current algorithms\nto find common structure.",
    "start": "4626260",
    "end": "4632070"
  },
  {
    "text": "And then also benchmarks\nthat are realistic so that they can reflect real-world\nproblems that we actually",
    "start": "4632070",
    "end": "4638460"
  },
  {
    "text": "care about. And we've seen some examples\nof that throughout the course but I think there is\nmore to be done there.",
    "start": "4638460",
    "end": "4645520"
  },
  {
    "text": "And then there is some work to\nbe done on improving the core algorithms themselves. So I think the theory\nfor most of these methods",
    "start": "4645520",
    "end": "4653640"
  },
  {
    "text": "is actually lagging\nbehind quite a bit. So we need some\nadditional work to develop",
    "start": "4653640",
    "end": "4659190"
  },
  {
    "text": "a theoretical understanding\nof the performance of these algorithms. We can also look\na little bit more into multi-step problems, or\nlifelong learning problems",
    "start": "4659190",
    "end": "4667290"
  },
  {
    "text": "for performing tasks in\nsequence present challenges. And as we've seen in the\nprevious lecture on Monday,",
    "start": "4667290",
    "end": "4673560"
  },
  {
    "text": "performing lifelong\nlearning is actually quite-- can be quite difficult\nand very often",
    "start": "4673560",
    "end": "4680040"
  },
  {
    "text": "is understood in\nmany different ways. So I think we could make\na lot of progress here. And then there's probably\nmany more challenges",
    "start": "4680040",
    "end": "4686220"
  },
  {
    "text": "that you discovered\nin your homework and in the final project. And since this is\nthe last lecture,",
    "start": "4686220",
    "end": "4694380"
  },
  {
    "text": "I wanted to briefly go\nback to the bigger picture. So at the beginning\nof the course, we talked about how machines\nare pretty good specialists.",
    "start": "4694380",
    "end": "4703470"
  },
  {
    "text": "They can solve things such\nas Atari games or playing Go, or even doing\nhelicopter acrobatics.",
    "start": "4703470",
    "end": "4710580"
  },
  {
    "text": "But there are pretty good\nin these narrow domains. However, when we look at\nhumans, we are pretty good--",
    "start": "4710580",
    "end": "4718389"
  },
  {
    "text": "what I would call, generalist. And what I mean\nby generalists is that we have this ability to\nlearn many different skills,",
    "start": "4718390",
    "end": "4725310"
  },
  {
    "text": "build on prior knowledge to\nadapt to new environments, and have some sort\nof common sense.",
    "start": "4725310",
    "end": "4732040"
  },
  {
    "text": "The ML systems that I showed\npreviously on the other hand do not. So they are specialists that\ntry to master a single task,",
    "start": "4732040",
    "end": "4738789"
  },
  {
    "text": "starting from scratch,\ndisregarding a greater understanding of\nhow the world works.",
    "start": "4738790",
    "end": "4744670"
  },
  {
    "text": "And in this class, we took a\nstep towards building machines that are generalists.",
    "start": "4744670",
    "end": "4751450"
  },
  {
    "text": "There's a lot of\ntopics that we covered that go in that direction. Things such as how to\nlearn multiple tasks",
    "start": "4751450",
    "end": "4756760"
  },
  {
    "text": "or multi-task learning. How to leverage\nprior experience when learning new things, for\ninstance, with meta-learning.",
    "start": "4756760",
    "end": "4762639"
  },
  {
    "text": "How to solve sequential\ndecision problems with RL. How to learn\ngeneral-purpose models with model-based\nreinforcement learning.",
    "start": "4762640",
    "end": "4769000"
  },
  {
    "text": "How to prepare for tasks before\nwe even know what they are, which is what we did\nin skill discovery.",
    "start": "4769000",
    "end": "4774850"
  },
  {
    "text": "How to perform tasks in\nsequence with hierarchical RL. And how to learn continuously.",
    "start": "4774850",
    "end": "4781050"
  },
  {
    "text": "I think a lot of\nthese things cover-- we covered a number\nof things that can bring us closer to this\ngoal of building generalist",
    "start": "4781050",
    "end": "4788910"
  },
  {
    "text": "learning systems. But there's always a\nquestion, what else?",
    "start": "4788910",
    "end": "4794010"
  },
  {
    "text": "What is missing? And I think that now you should\nbe well equipped with the tools",
    "start": "4794010",
    "end": "4800340"
  },
  {
    "text": "that you get to know in\nthis class to figure it out.",
    "start": "4800340",
    "end": "4805460"
  },
  {
    "text": "Thank you so much for being\nsuch an engaging group. It was a lot of fun teaching\nthis quarter to all of you.",
    "start": "4805460",
    "end": "4812750"
  },
  {
    "text": "Thank you also from the\nentire course staff, from our course coordinator, all\nthe TAs, Chelsea, and myself.",
    "start": "4812750",
    "end": "4820070"
  },
  {
    "text": "Thank you. We'll see you. ",
    "start": "4820070",
    "end": "4827094"
  }
]