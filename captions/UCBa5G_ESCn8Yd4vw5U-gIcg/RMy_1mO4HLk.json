[
  {
    "text": "From today I guess,\nyou're going to see me for at least a few weeks.",
    "start": "5540",
    "end": "11320"
  },
  {
    "text": "We're going to cover some\nsupervised learning algorithms, and we're going to talk\nabout deep learning,",
    "start": "11320",
    "end": "18029"
  },
  {
    "text": "and then I'll pass\non to Chris to talk about unsupervised learning.",
    "start": "18029",
    "end": "23650"
  },
  {
    "text": "So I think I'm going to be\nin charge in the next three or four weeks.",
    "start": "23650",
    "end": "29179"
  },
  {
    "text": "And I'm going to use the board. Part of it because I think there\nis a little bit more memory on the board, right?",
    "start": "29180",
    "end": "36390"
  },
  {
    "text": "You can review things that I\nwrote like 10 minutes ago even.",
    "start": "36390",
    "end": "42379"
  },
  {
    "text": "I don't know whether that's\nthe best for everyone. I think in the past\nI've surveyed students and someone prefer\nthe board, someone",
    "start": "42380",
    "end": "47610"
  },
  {
    "text": "prefers the Zoom, the iPad so\nI'll give it a try with this. But any comments or any kind\nof suggestions are welcome",
    "start": "47610",
    "end": "56010"
  },
  {
    "text": "and we are open to change\nthe format as well. But for today at least I'm\ngoing to use the board.",
    "start": "56010",
    "end": "63730"
  },
  {
    "text": "And I think the video is able\nto capture the board almost the same as the iPad I hope.",
    "start": "63730",
    "end": "71390"
  },
  {
    "text": "OK. So I'm going to talk about the\nso-called generative learning",
    "start": "71390",
    "end": "81710"
  },
  {
    "text": "algorithms. So the next two lectures\nwill be about this.",
    "start": "81710",
    "end": "87719"
  },
  {
    "text": "I'm going to define what does\nit mean by generative learning algorithms.",
    "start": "87720",
    "end": "93180"
  },
  {
    "text": "And there are two types of\ngenerating learning algorithms that we are going to cover. One is called Gaussian\ndiscriminative analysis.",
    "start": "93180",
    "end": "100990"
  },
  {
    "text": "I guess these are\nall new words that I have to define as I'm\nintroducing these things.",
    "start": "100990",
    "end": "110719"
  },
  {
    "text": "GDA. And another type of algorithms\nis called Naive Bayes.",
    "start": "110720",
    "end": "122840"
  },
  {
    "text": "OK. So I guess let me get started. So I will start by\ndefining what do",
    "start": "122840",
    "end": "130190"
  },
  {
    "text": "I mean by generative\nlearning algorithms. To kind of define\nthese terms I think is useful to compare with\nwhat Chris has introduced",
    "start": "130190",
    "end": "138560"
  },
  {
    "text": "in the last two weeks. So in the last two weeks,\nthe type of algorithms Chris",
    "start": "138560",
    "end": "143980"
  },
  {
    "text": "introduced, we call\nthem discriminative learning algorithms.",
    "start": "143980",
    "end": "149140"
  },
  {
    "text": "So, discriminative\nlearning algorithms.",
    "start": "149140",
    "end": "165980"
  },
  {
    "text": "So the reason we call them\ndiscriminative learning algorithms is the following. So in some sense,\nthe definition is",
    "start": "165980",
    "end": "172120"
  },
  {
    "text": "that if you model\nor you parameterize the relationship, the\nconditional relationship,",
    "start": "172120",
    "end": "182790"
  },
  {
    "text": "of y given x, then we call\nthis discriminative learning algorithms.",
    "start": "182790",
    "end": "187870"
  },
  {
    "text": "And I think if you\nrecall, this is the type of models we consider\nin the last two weeks.",
    "start": "187870",
    "end": "194450"
  },
  {
    "text": "So we model y as a\nlinear function for x. Maybe y is linear function\nof x plus Gaussian noise.",
    "start": "194450",
    "end": "200739"
  },
  {
    "text": "Or maybe y is the function\nof x plus exponential family or something like that.",
    "start": "200739",
    "end": "206459"
  },
  {
    "text": "So for example, in the most\ngeneral format in the last two ways can be summarized as you\nthink of the x, the y given",
    "start": "206459",
    "end": "213799"
  },
  {
    "text": "x parametized by theta. This is a distribution of y\ngiven x parameterized by theta",
    "start": "213799",
    "end": "219730"
  },
  {
    "text": "You write this as some\nexponential family. Exponential with\nsome distributions",
    "start": "219730",
    "end": "226100"
  },
  {
    "text": "in exponential family. With some parameter.",
    "start": "226100",
    "end": "232220"
  },
  {
    "text": "With some input eta. And this eta is a\nlinear function of x. For example, you can say this\nis a Gaussian distribution",
    "start": "232220",
    "end": "240440"
  },
  {
    "text": "with mean eta. And that's just the standard\nlinear regression, right?",
    "start": "240440",
    "end": "245799"
  },
  {
    "text": "So this is why we call\nthem discriminative learning algorithms. And today, we're going to talk\nabout a so-called generative",
    "start": "245799",
    "end": "255290"
  },
  {
    "text": "version. Generative learning algorithms.",
    "start": "255290",
    "end": "268130"
  },
  {
    "text": "The basic idea is that you are\ngoing to model or parameterize. Your model means is a word.",
    "start": "268130",
    "end": "276130"
  },
  {
    "text": "Basically means parameterized\nor you have a mathematical model for the conditional for\nthe joint distribution.",
    "start": "276130",
    "end": "282330"
  },
  {
    "text": "p of x comma y. The joint distribution of x and\ny, using a simple chain rule,",
    "start": "282330",
    "end": "289680"
  },
  {
    "text": "you can write this as\nx given y times p of y.",
    "start": "289680",
    "end": "295919"
  },
  {
    "text": "So you model both\nthese two quantities. Is this the-- there's\nsome light is flashing.",
    "start": "295920",
    "end": "301759"
  },
  {
    "text": "Some-- should I-- are you bothered by it or not? I'm fine with it.",
    "start": "301759",
    "end": "307539"
  },
  {
    "text": "Just-- OK. No worries. OK. I think it flashes every\nminute or something.",
    "start": "307539",
    "end": "313770"
  },
  {
    "text": "Anyway. So you model the\njoint description by modeling each\npart of this tool.",
    "start": "313770",
    "end": "318860"
  },
  {
    "text": "And particularly, you model\nthe joint distribution by modeling the\ndistribution of y and the distribution\nof x condition y.",
    "start": "318860",
    "end": "325090"
  },
  {
    "text": "Here, x and y are not symmetric.\ny is the label. x is the input. And typically, they have\nvery different meanings.",
    "start": "325090",
    "end": "330479"
  },
  {
    "text": "So y could be something\nlike the price of the house and x is the features.",
    "start": "330479",
    "end": "336710"
  },
  {
    "text": "Like what you know\nabout the houses. So like the square feet, the\nlot size, so and so forth.",
    "start": "336710",
    "end": "345020"
  },
  {
    "text": "And recall that\nthe x is the input. And this is maybe something like\na label or some kind of class.",
    "start": "345020",
    "end": "353479"
  },
  {
    "text": "If you have classification,\nthis is about class. Like maybe positive\nsentiment, negative sentiment.",
    "start": "353479",
    "end": "360050"
  },
  {
    "text": "And so basically, this\nis the distribution of the input given the label\nand this is the distribution",
    "start": "360050",
    "end": "366230"
  },
  {
    "text": "of the label itself. And sometimes we also call\nthis a prior for the label.",
    "start": "366230",
    "end": "372350"
  },
  {
    "text": "A prior for the\nclass or the label. Because this is\nwhat you believe.",
    "start": "372350",
    "end": "378689"
  },
  {
    "text": "Like for example, suppose y\nis 1 means positive sentiment.",
    "start": "378690",
    "end": "385060"
  },
  {
    "text": "Suppose you are\nclassifying the text. Then this is a distribution\nover two labels. Positive and negative.",
    "start": "385060",
    "end": "392069"
  },
  {
    "text": "And this is the\nprior that you have for how many positive\nexamples or negative samples",
    "start": "392069",
    "end": "397509"
  },
  {
    "text": "are in your data set.",
    "start": "397509",
    "end": "403300"
  },
  {
    "text": "So after you model\nand parameterize this, you can learn these\ntwo distributions.",
    "start": "403300",
    "end": "408610"
  },
  {
    "text": "You can learn the distribution. Learn p of x and y. And p of y, we're going to\nsay how do we learn them.",
    "start": "408610",
    "end": "415000"
  },
  {
    "text": "And after you\nlearn both of this, you are going to still solve\nthe classification problem. Your goal is still the same.",
    "start": "415000",
    "end": "420300"
  },
  {
    "text": "You are trying to classify. You are still trying to compute.",
    "start": "420300",
    "end": "425659"
  },
  {
    "text": "So this is the test time. You are still trying to compute,\nfor example, p of y given x.",
    "start": "425660",
    "end": "433610"
  },
  {
    "text": "You're still trying\nto classify what's the chance of each of the\nlabel or you probably want",
    "start": "433610",
    "end": "439780"
  },
  {
    "text": "to get the max of y given x. We are going to\ntalk about exactly-- you care about.",
    "start": "439780",
    "end": "444819"
  },
  {
    "text": "But essentially, you still\ncare about the relationship of y conditional x.",
    "start": "444820",
    "end": "449860"
  },
  {
    "text": "And how do you get this? You got this by Bayes' rule.",
    "start": "449860",
    "end": "455730"
  },
  {
    "text": "Meaning, so recall that p\nof, for example, y given",
    "start": "455730",
    "end": "462060"
  },
  {
    "text": "x is equal to p of x given\ny times p of y over x.",
    "start": "462060",
    "end": "469740"
  },
  {
    "text": "So you know this quantity. You know this quantity.",
    "start": "469740",
    "end": "476060"
  },
  {
    "text": "I'm assuming you already\nlearned this too, right? And how do you know\nthe denominator? Then you can just\nwrite this as--",
    "start": "476060",
    "end": "484150"
  },
  {
    "text": "the denominator is really\njust, you take sum over y, p of x given y times p of y.",
    "start": "484150",
    "end": "492199"
  },
  {
    "text": "Maybe just for the\nsake of notation, let's call this y prime, right?",
    "start": "492199",
    "end": "501310"
  },
  {
    "text": "So this is the\nstandard total law of probabilities\nwhere you compute",
    "start": "501310",
    "end": "507689"
  },
  {
    "text": "the marginal\nprobability of p of x. You use that as the\ndenominator and then you also use these two\nquantities on the top.",
    "start": "507690",
    "end": "516490"
  },
  {
    "text": "Actually in many cases,\nyou don't necessarily have to compute the p of x. We'll see exactly how it works. But roughly speaking, after\nyou know these two things,",
    "start": "516490",
    "end": "523570"
  },
  {
    "text": "you can know-- after you know x\ngiven y and y, you can know y given x by\ndoing some Bayes' rule.",
    "start": "523570",
    "end": "530240"
  },
  {
    "text": "By the way, feel free\nto stop me at any point.",
    "start": "530240",
    "end": "536510"
  },
  {
    "text": "Just to raise hand\nor just speak. Maybe.",
    "start": "536510",
    "end": "542020"
  },
  {
    "text": "A few first. [INAUDIBLE] So let me repeat the question.",
    "start": "542020",
    "end": "548620"
  },
  {
    "text": "Is it true that the\ndiscriminative learning algorithms cannot work on\nnon-discriminative in this",
    "start": "548620",
    "end": "555279"
  },
  {
    "text": "family. No. I think the discriminant\nlearning algorithms can also work with other\npossible distributions here.",
    "start": "555280",
    "end": "562310"
  },
  {
    "text": "So as long as you\nspecify y given x and you parameterized that\nby some parameter theta,",
    "start": "562310",
    "end": "567860"
  },
  {
    "text": "you can in theory. You can still learn them using\nsimilar type of methodology. I'm going to discuss\nthe methodology as well.",
    "start": "567860",
    "end": "577019"
  },
  {
    "text": "But if you have a\nexponential family, then there are several benefits. For example, the\nquiz discussed this.",
    "start": "577019",
    "end": "584319"
  },
  {
    "text": "Many properties, nice properties\nof exponential family. If you don't have them then you\ncannot use those properties.",
    "start": "584320",
    "end": "590930"
  },
  {
    "text": "You have to use\nsomething else or you have to rely on optimization. Or sometimes it's challenging\ndepending on the cases.",
    "start": "590930",
    "end": "596800"
  },
  {
    "text": "But in principle, you can\nhave other distributions here.",
    "start": "596800",
    "end": "602709"
  },
  {
    "text": "Maybe let's just do this other. Yeah. [INAUDIBLE]",
    "start": "602709",
    "end": "608579"
  },
  {
    "text": "Yeah. Yeah. I'm going to talk about that. This is just the\ngeneral framework.",
    "start": "608579",
    "end": "613750"
  },
  {
    "text": "So the difference between\nthe two that I can guess",
    "start": "613750",
    "end": "620280"
  },
  {
    "text": "is that in discriminative\nlearning algorithm, [INAUDIBLE] trying to [INAUDIBLE].",
    "start": "620280",
    "end": "626680"
  },
  {
    "text": "I'm sure you won't have\nany parameters [INAUDIBLE].. Actually, you will\nhave parameters because I'm going to\nparameterize these two",
    "start": "626680",
    "end": "632610"
  },
  {
    "text": "distributions and I learn them. So, yeah. I'm going to talk\nabout that as well. [INAUDIBLE] the same\nkind of p of y given x.",
    "start": "632610",
    "end": "644070"
  },
  {
    "text": "So this is a-- yes. So I'm also going to\ndiscuss the differences. Like, what's the high level\ndifferences, why don't we",
    "start": "644070",
    "end": "650950"
  },
  {
    "text": "do this. But I think it's\neasier to discuss those once I tell you a little\nmore about concretely",
    "start": "650950",
    "end": "656459"
  },
  {
    "text": "how this works. But so far, you are right. Basically, this is a somewhat\nseemingly circuitous way",
    "start": "656459",
    "end": "663010"
  },
  {
    "text": "to get p of y given x. It's not direct, right?",
    "start": "663010",
    "end": "668370"
  },
  {
    "text": "Yeah. I think I'm going to discuss\nthe differences probably",
    "start": "668370",
    "end": "675000"
  },
  {
    "text": "later in the lecture\njust because it's easier when I have some examples. Thank you.",
    "start": "675000",
    "end": "681630"
  },
  {
    "text": "OK. Any other questions? Would it be possible to\nwrite a little bigger please?",
    "start": "681630",
    "end": "688520"
  },
  {
    "text": "Sure. Yeah. That's a great suggestion. I think that's probably also\nuseful for the recording as well.",
    "start": "688520",
    "end": "693889"
  },
  {
    "text": "And also, feel free to remind me\nagain because this happens too in the past as well.",
    "start": "693889",
    "end": "699730"
  },
  {
    "text": "Like, every time\nafter a few lectures, I stopped writing big. Even after a few\nminutes sometimes.",
    "start": "699730",
    "end": "707329"
  },
  {
    "text": "OK. So this is just a very\nhigh level introduction. So we're going to talk\nabout two instantiations",
    "start": "707329",
    "end": "718480"
  },
  {
    "text": "of \nthis general idea.",
    "start": "718480",
    "end": "727130"
  },
  {
    "text": "So the difference is really just\nthat one case is a continuous x",
    "start": "727130",
    "end": "733420"
  },
  {
    "text": "and the other case\nis discrete x. And this continuous--\nit's called the Gaussian",
    "start": "733420",
    "end": "740510"
  },
  {
    "text": "discriminant analysis. And for the discrete\nx, we are going to focus on our application\nwhich is the spam filtering.",
    "start": "740510",
    "end": "749750"
  },
  {
    "text": "And today, I think\nwe're going mostly",
    "start": "749750",
    "end": "755470"
  },
  {
    "text": "talk about a continuous case. And next lecture, I'm going\nto talk about the spam filter.",
    "start": "755470",
    "end": "762220"
  },
  {
    "text": "All right, so. So now, one example, how do\nwe instantiate this plan?",
    "start": "762220",
    "end": "771700"
  },
  {
    "text": "So for GDA, what\nyou do is you say, I'm going to suppose x in r d.",
    "start": "771700",
    "end": "781880"
  },
  {
    "text": "I'm going to jump\nthe convention. Just because here I'm not\ngoing to use the bias.",
    "start": "781880",
    "end": "790320"
  },
  {
    "text": "At least in the modeling part,\nI'm not going to use the bias. Don't worry too much about it.",
    "start": "790320",
    "end": "796310"
  },
  {
    "text": "It's just, we don't\nhave the x 0 as well. I know it doesn't\nreally matter that much. So the main thing is\nI'm going to assume.",
    "start": "796310",
    "end": "805519"
  },
  {
    "text": "You can say this\nis assumption, you can say this is a\nmodeling assumption. I'm going to assume p of\nx given y is a Gaussian.",
    "start": "805520",
    "end": "819170"
  },
  {
    "text": "Gaussian distribution. So what does that really mean?",
    "start": "819170",
    "end": "825440"
  },
  {
    "text": "That really means\nthat you write this. You say, x given y is following\nsome Gaussian distribution",
    "start": "825440",
    "end": "834980"
  },
  {
    "text": "with some mean and covariance. And here, note that x is\na high dimensional vector",
    "start": "834980",
    "end": "840980"
  },
  {
    "text": "so I'm going to have a high\ndimensional multivariate Gaussian distribution. With a mean and be\nsome covariant sigma.",
    "start": "840980",
    "end": "848769"
  },
  {
    "text": "So it's probably useful for\nme to briefly kind of digress",
    "start": "848769",
    "end": "856209"
  },
  {
    "text": "a little bit to briefly\ntalk about some basics about multivariate\nGaussian distribution. These are just some\nvery quick review",
    "start": "856209",
    "end": "862670"
  },
  {
    "text": "if you haven't seen this. But I'm assuming that\nyou know something about one-dimensional\nGaussian distribution.",
    "start": "862670",
    "end": "869040"
  },
  {
    "text": "So just a very quick digression. So if you have a\nmulti-dimensional Gaussian",
    "start": "869040",
    "end": "877759"
  },
  {
    "text": "random variable.",
    "start": "877759",
    "end": "882860"
  },
  {
    "text": "So what happens is that, suppose\nyou have some random variable z sample from this\nGaussian distribution.",
    "start": "882860",
    "end": "888680"
  },
  {
    "text": "Which means, mu and\nsigma, covariance sigma. So here, mu is a\nthree-dimensional vector",
    "start": "888680",
    "end": "895760"
  },
  {
    "text": "and sigma is a matrix. Is the so-called\ncovariance matrix.",
    "start": "895760",
    "end": "901899"
  },
  {
    "text": "So the property you need to know\nis that, as the name suggests,",
    "start": "901899",
    "end": "908139"
  },
  {
    "text": "the expectation of the\nz is supposed to be the min of the mean parameter.",
    "start": "908139",
    "end": "916579"
  },
  {
    "text": "And the covariance of\nthe random variable z, which is defined to\nbe the expectation of z",
    "start": "916579",
    "end": "924940"
  },
  {
    "text": "minus expectation z times z\nminus expectation z transpose.",
    "start": "924940",
    "end": "932990"
  },
  {
    "text": "This is the covariance matrix.",
    "start": "932990",
    "end": "939050"
  },
  {
    "text": "So this is how\nyou generate the z from this Gaussian distribution\nparameterized by these two",
    "start": "939050",
    "end": "945680"
  },
  {
    "text": "parameters, mu and sigma. And the resulting\nrandom variable z would have these two properties.",
    "start": "945680",
    "end": "951259"
  },
  {
    "text": "The mean is mu and\ncovariance is sigma. And you only need\ntwo set of parameters",
    "start": "951259",
    "end": "957440"
  },
  {
    "text": "to describe uniquely a\nGaussian distribution.",
    "start": "957440",
    "end": "965430"
  },
  {
    "text": "You also know the density of\nthis Gaussian distribution. So the density of the\nGaussian distribution",
    "start": "965430",
    "end": "971440"
  },
  {
    "text": "is something like this. I don't expect you to\nremember the formula because I know I remember it\nafter I teach it so many times.",
    "start": "971440",
    "end": "980410"
  },
  {
    "text": "But before I taught it, I\ndon't think I remember it in my graduate school. But the formula is that-- not\nsure whether you can see this.",
    "start": "980410",
    "end": "992250"
  },
  {
    "text": "Something like this.",
    "start": "992250",
    "end": "1004139"
  },
  {
    "text": "And here, this is\nthe determinant. OK. Great.",
    "start": "1004139",
    "end": "1009410"
  },
  {
    "text": "I see some question. Sure. Maybe I'll start with the one.",
    "start": "1009410",
    "end": "1015490"
  },
  {
    "text": "What does that denominator say?",
    "start": "1015490",
    "end": "1023220"
  },
  {
    "text": "What is the deno-- The denominator. Yeah. There. So this is 2 pi to\nthe power of d over 2.",
    "start": "1023220",
    "end": "1028990"
  },
  {
    "text": "And the determinant of\nsigma is the power of 1/2. I'll write even bigger.",
    "start": "1028990",
    "end": "1034418"
  },
  {
    "text": "All right. Thank you. And then, what does the little\ntriangle over the equal sign? Oh. This is just the-- oh.",
    "start": "1034419",
    "end": "1040319"
  },
  {
    "text": "Why I'm doing this. Oh, this is just a definition\nof the covariance in case you don't know the definition. So.",
    "start": "1040319",
    "end": "1046130"
  },
  {
    "text": "OK. Thank you. Yeah. Yeah. I'm just using that to\nindicate this definition.",
    "start": "1046130",
    "end": "1052670"
  },
  {
    "text": "And by the way, this formula\nI don't think we really have to remember it. The most important\nthing is you have a constant times\nsome exponential",
    "start": "1052670",
    "end": "1060360"
  },
  {
    "text": "of some quadratic form of Z. So is z [INAUDIBLE]?",
    "start": "1060360",
    "end": "1065700"
  },
  {
    "text": "z is a vector. Oh. Yeah. That's a good question.",
    "start": "1065700",
    "end": "1071470"
  },
  {
    "text": "So this is why this is\na little more complex than one-dimensional case. If you are familiar with\na one-dimensional case,",
    "start": "1071470",
    "end": "1077380"
  },
  {
    "text": "then this will be\n[INAUDIBLE] of mu, and this will be a [INAUDIBLE].",
    "start": "1077380",
    "end": "1083200"
  },
  {
    "text": "Sometimes people write\nit sigma squared. And the sigma will be just the-- so for one-dimensional case,\nthis is just a variance.",
    "start": "1083200",
    "end": "1093559"
  },
  {
    "text": "And now it's the\nso-called covariance.",
    "start": "1093559",
    "end": "1098779"
  },
  {
    "text": "So in some sense, if\nyou at sigma of ij, this is really just the\ncorrelation between--",
    "start": "1098780",
    "end": "1105210"
  },
  {
    "text": "this will be the\nexpectation of z i.",
    "start": "1105210",
    "end": "1110610"
  },
  {
    "text": "z the i is squared minus\nu i times z j minus mu j.",
    "start": "1110610",
    "end": "1115840"
  },
  {
    "text": "In some sense, the entry\nof the covariance matrix is capturing the correlation\nbetween two coordinates.",
    "start": "1115840",
    "end": "1124860"
  },
  {
    "text": "Of course you have\nto remove the mean to match the correlation\nin the right way. But you match the correlation\nof the two coordinates",
    "start": "1124860",
    "end": "1130500"
  },
  {
    "text": "of this random variable.",
    "start": "1130500",
    "end": "1135820"
  },
  {
    "text": "I saw some other questions. [INAUDIBLE] Yes. So x and z are the\nsame thing here. I use z because I\nwant to be abstract.",
    "start": "1135820",
    "end": "1142669"
  },
  {
    "text": "So later, I'm going to\nhave a little more-- I'm going to have to change\nthis a little bit too.",
    "start": "1142670",
    "end": "1150070"
  },
  {
    "text": "But this is just\nfor abstraction. I use a different variable.",
    "start": "1150070",
    "end": "1155380"
  },
  {
    "text": "[INAUDIBLE] The second term in sigma. On the right side.",
    "start": "1155380",
    "end": "1162990"
  },
  {
    "text": "Here. So here because\nthey are scalars, so that's why I didn't\nhave to transpose. So this is a scalar.",
    "start": "1162990",
    "end": "1169260"
  },
  {
    "text": "This is a scalar. Right? And here, the reason why I have\ntransposed is because then-- and you make it matrix.",
    "start": "1169260",
    "end": "1177450"
  },
  {
    "text": "So, I don't know. I think some of you probably\nare familiar with this so I don't want to spend\na lot of time on this.",
    "start": "1177450",
    "end": "1184120"
  },
  {
    "text": "But some of you are\nprobably not very familiar. So I think I used to have some-- let me show you\nsome other pictures",
    "start": "1184120",
    "end": "1191100"
  },
  {
    "text": "to get a little bit more\nsense on the covariance. Let me see. How do I connect to this?",
    "start": "1191100",
    "end": "1200630"
  },
  {
    "text": "Do they know this?",
    "start": "1200630",
    "end": "1213850"
  },
  {
    "text": "How do I signal\nanything to them?",
    "start": "1213850",
    "end": "1224030"
  },
  {
    "text": "I don't see they are\ncapturing the video. The screen.",
    "start": "1224030",
    "end": "1230780"
  },
  {
    "text": "Anyway. Anyway, these are the slides.",
    "start": "1230780",
    "end": "1237289"
  },
  {
    "text": "So it should be-- oh. Yeah. OK. Great.",
    "start": "1237289",
    "end": "1242450"
  },
  {
    "text": "So it used to be the\ncase that I make slides for this part of the talk. I just make three slides. But then I realize that\nmaybe it's just easier for me to show you the lecture notes.",
    "start": "1242450",
    "end": "1247639"
  },
  {
    "text": "Because then you know\nwhere to find them again. So I'm not being lazy here. It's just-- it's also\neasier for me of course.",
    "start": "1247640",
    "end": "1258059"
  },
  {
    "text": "OK. So these are some visualizations\nof the density function here. So the first set of the--",
    "start": "1258059",
    "end": "1264220"
  },
  {
    "text": "just look at the figures. This is a two-dimensional case,\nright, we have for these two.",
    "start": "1264220",
    "end": "1270580"
  },
  {
    "text": "And you visualize the density\nof the Gaussian distribution. So density always looks\nlike something like this.",
    "start": "1270580",
    "end": "1278549"
  },
  {
    "text": "And these are the cases where\nthe covariance is identity. It means that when the\ncovariance is identity,",
    "start": "1278549",
    "end": "1284840"
  },
  {
    "text": "it means that-- so that when the covariance\nis additive matrix.",
    "start": "1284840",
    "end": "1292860"
  },
  {
    "text": "It means that there is no\ncorrelation between any pairs of coordinates.",
    "start": "1292860",
    "end": "1298080"
  },
  {
    "text": "Like i and j, when i\nand j are not the same, they have no correlation. And the results-- the\nshape of this Gaussian",
    "start": "1298080",
    "end": "1309159"
  },
  {
    "text": "is always spherical. So basically, when\nit's identity, it means that you have\nequal strength in all the--",
    "start": "1309160",
    "end": "1315429"
  },
  {
    "text": "basically, you just\nhave the same strength in all the directions.",
    "start": "1315429",
    "end": "1322570"
  },
  {
    "text": "Because every dimension\nlooks the same in some sense. So basically,\nthat's when you see",
    "start": "1322570",
    "end": "1331960"
  },
  {
    "text": "this kind of very spherical\nshape of the density function.",
    "start": "1331960",
    "end": "1337100"
  },
  {
    "text": "The one thing is that the\nsize of this density function depends on the scalar in\nterms of the identity, right?",
    "start": "1337100",
    "end": "1343539"
  },
  {
    "text": "So if you have identity, then I\nthink this is the leftmost one.",
    "start": "1343539",
    "end": "1349269"
  },
  {
    "text": "But if you make the\ncovariance two times bigger than your density function will\nbe supported, in some sense,",
    "start": "1349270",
    "end": "1356039"
  },
  {
    "text": "like more supported\non a larger region. So I think that's the last one.",
    "start": "1356039",
    "end": "1362250"
  },
  {
    "text": "And then in the middle one,\nyou have a smaller covariance. So in some sense, the\ncovariance is describing how",
    "start": "1362250",
    "end": "1370980"
  },
  {
    "text": "at least how large this-- like, how large the shape\nof this density function is.",
    "start": "1370980",
    "end": "1378110"
  },
  {
    "text": "And also, it's\ndefining two things. One is the size\nand the other thing is that what's the correlation.",
    "start": "1378110",
    "end": "1383230"
  },
  {
    "text": "What's the kind of\norientation of the shape. So maybe one way\nto think about this is that if you look at the\nsecond rows of the figures,",
    "start": "1383230",
    "end": "1392750"
  },
  {
    "text": "so these are cases where the\ncovariance matrix are no longer identity. And they are for example\nin the third figure here.",
    "start": "1392750",
    "end": "1400980"
  },
  {
    "text": "So you have some correlations\nbetween two directions and then you see this\nellipsoid kind of shape",
    "start": "1400980",
    "end": "1408600"
  },
  {
    "text": "is rotated into that direction. Just because in that\ndirections, the two quadrants are more correlated.",
    "start": "1408600",
    "end": "1413610"
  },
  {
    "text": "So it's more likely\nthat these two quadrants are simultaneously\nbigger or smaller.",
    "start": "1413610",
    "end": "1418730"
  },
  {
    "text": "So that's why in\nthat direction you have for you have more kind of\nlike a mass in that direction. And you can see that the\ndifference between these three",
    "start": "1418730",
    "end": "1427299"
  },
  {
    "text": "figures is that the correlation\nalong that direction is bigger and bigger. In the first one,\nthere's no correlation",
    "start": "1427299",
    "end": "1432649"
  },
  {
    "text": "that spatial direction, x\nis equals to y direction. And the second one, you\nhave a little correlation so that's why it goes\ntowards that direction.",
    "start": "1432650",
    "end": "1439540"
  },
  {
    "text": "And then in the third one,\nyou have more correlation so it's even more\nskewed in some sense.",
    "start": "1439540",
    "end": "1447620"
  },
  {
    "text": "Any questions? So I guess maybe\nanother way to look",
    "start": "1447620",
    "end": "1454620"
  },
  {
    "text": "at this is that you can look\nat the contours of this thing.",
    "start": "1454620",
    "end": "1460970"
  },
  {
    "text": "So basically, you\nlook at the level set",
    "start": "1460970",
    "end": "1466450"
  },
  {
    "text": "of this density function. Level set means that\nthe set of basically the",
    "start": "1466450",
    "end": "1472460"
  },
  {
    "text": "set of points with\nthe equal density.",
    "start": "1472460",
    "end": "1477490"
  },
  {
    "text": "And then you can see that you\nget this kind of ellipsoid. And the same thing, right? So if you have more\ncorrelations then this ellipsoid",
    "start": "1477490",
    "end": "1485940"
  },
  {
    "text": "will become more tilt towards\none spatial direction. So for example here. I guess if you can\nsee my pointer.",
    "start": "1485940",
    "end": "1492419"
  },
  {
    "text": "So this means that in\nthis direction like-- so I think if this is\nx1, this is x2, right?",
    "start": "1492419",
    "end": "1498679"
  },
  {
    "text": "So if you see these\nkind of contours then it means that\nx1, x2 are equal are likely to be simultaneously\nbigger or smaller.",
    "start": "1498679",
    "end": "1506450"
  },
  {
    "text": "So that's why they\nhave correlation. And if you see in\nthis one, then you're going to have some\nreverse type of things.",
    "start": "1506450",
    "end": "1513460"
  },
  {
    "text": "So if x1 is bigger, then\nx2 is likely to be smaller. And that's when you\nsee this kind of shape.",
    "start": "1513460",
    "end": "1520230"
  },
  {
    "text": "There's no need to\nexactly understand this. It's just some kind\nof rough intuition.",
    "start": "1520230",
    "end": "1528100"
  },
  {
    "text": "In reality, you\ndon't necessarily have to exactly\nvisualize all of this. But these are-- any questions?",
    "start": "1528100",
    "end": "1534799"
  },
  {
    "text": "I know sometimes this\ncould be confusing, sometimes this could\nbe very enlightening. I don't know, like--",
    "start": "1534799",
    "end": "1542520"
  },
  {
    "text": "depending on-- feel free\nto ask any questions.",
    "start": "1542520",
    "end": "1549770"
  },
  {
    "text": "OK. OK. So I guess we'll move on\nback to the more messy stuff.",
    "start": "1549770",
    "end": "1555250"
  },
  {
    "text": "OK, cool. So I have introduced the\nmultivariate Gaussian distribution.",
    "start": "1555250",
    "end": "1561070"
  },
  {
    "text": "And now, I'm going to go back\nto the Gaussian discriminative analysis. So we are going to parameterize\nx given y as a Gaussian",
    "start": "1561070",
    "end": "1577200"
  },
  {
    "text": "distribution. By the way, I think I\nforgot to mention that here.",
    "start": "1577200",
    "end": "1582820"
  },
  {
    "text": "In both of these two cases,\nthe y is always discrete.",
    "start": "1582820",
    "end": "1588940"
  },
  {
    "text": "You can make y\nnon-discrete as well. But here, we're only\nlooking at a case where y the label\nis always discreet.",
    "start": "1588940",
    "end": "1595539"
  },
  {
    "text": "So now, let me\ncontinue with the GDA.",
    "start": "1595539",
    "end": "1603000"
  },
  {
    "text": "So as I said, y is\ndiscrete and we are only going to assume that\nthere are two labels.",
    "start": "1603000",
    "end": "1611410"
  },
  {
    "text": "Say, 0 and 1. I guess-- I don't know why.",
    "start": "1611410",
    "end": "1620630"
  },
  {
    "text": "I think I missed the\none small thing here. But let me just.",
    "start": "1620630",
    "end": "1628570"
  },
  {
    "text": "After the running examples\nwe used to have for this GD",
    "start": "1628570",
    "end": "1637700"
  },
  {
    "text": "application is\nthat you can think of like you have some kind of-- I guess for example,\ncancer classification.",
    "start": "1637700",
    "end": "1648250"
  },
  {
    "text": "So like a benign-- a malignant classification.",
    "start": "1648250",
    "end": "1654700"
  },
  {
    "text": "So you have some maybe x1,\nx2 two-dimension of inputs",
    "start": "1654700",
    "end": "1659990"
  },
  {
    "text": "and you see a bunch\nof data like this.",
    "start": "1659990",
    "end": "1666730"
  },
  {
    "text": "So these are cases where\nyou have benign cancer. Think of maybe an x1 and x2 as\na measurement of the patients.",
    "start": "1666730",
    "end": "1675220"
  },
  {
    "text": "So maybe blood pressure or some\nsize of certain kind of tumors. And for every patient\nor every case,",
    "start": "1675220",
    "end": "1683130"
  },
  {
    "text": "you know whether this is\na benign cancer or not. These are the bad case. The malignant cancers.",
    "start": "1683130",
    "end": "1691090"
  },
  {
    "text": "And the question is that you\nwant to classify these examples into two classes. So that in the future if you\nsee one more example, one more",
    "start": "1691090",
    "end": "1700148"
  },
  {
    "text": "example here maybe, you\nwant to know whether this is a benign one or not. And the label is basically here.",
    "start": "1700149",
    "end": "1706000"
  },
  {
    "text": "Let's call this label So that's kind of the\ntarget applications",
    "start": "1706000",
    "end": "1711620"
  },
  {
    "text": "we are thinking about. So now I'm going to\nparameterize what is x given y.",
    "start": "1711620",
    "end": "1718830"
  },
  {
    "text": "X given y. And I need to specify\ntwo cases where I want-- one case is that what\nis the x description of x given",
    "start": "1718830",
    "end": "1725672"
  },
  {
    "text": "y 0. And the other case was the\ndescription x give y as well.",
    "start": "1725672",
    "end": "1730860"
  },
  {
    "text": "I'm going to make both of\nthese Gaussian distributions. So I'm going to assume\nthat x given y 0",
    "start": "1730860",
    "end": "1737059"
  },
  {
    "text": "is a Gaussian distribution. And the Gaussian distribution\nhas mean mu 0 and covariance",
    "start": "1737059",
    "end": "1743429"
  },
  {
    "text": "sigma. So here, I'm in the\nhigh dimensional case. So mu 0 is Rd, In\nthis case, d is 2.",
    "start": "1743429",
    "end": "1750299"
  },
  {
    "text": "And sigma is in Rd by d.",
    "start": "1750299",
    "end": "1755370"
  },
  {
    "text": "And for the other one, my\nmodel assumption is the same. I'm going to assume\nthis as mu 1 and sigma.",
    "start": "1755370",
    "end": "1762440"
  },
  {
    "text": "So the same covariance,\nthat's just for convenience, I can use different\ncovariance matrix.",
    "start": "1762440",
    "end": "1767500"
  },
  {
    "text": "But here, I have to\nuse a different mean. Because clearly if you fit\nthe Gaussian distribution",
    "start": "1767500",
    "end": "1772640"
  },
  {
    "text": "to this bunch of points and you\nhave a Gaussian distribution for this kind of points, you're\ngoing to have different mean.",
    "start": "1772640",
    "end": "1778299"
  },
  {
    "text": "So that's why I'm going to\nhave mu 1 here and mu 0 here. So mu 1, mu 0 sigma,\nthese are the parameters.",
    "start": "1778300",
    "end": "1783778"
  },
  {
    "text": "Oh, yeah. Go ahead. [INAUDIBLE] is that the\nsame covariant distribution? Yeah.",
    "start": "1783779",
    "end": "1789740"
  },
  {
    "text": "So this is mostly\njust for convenience. You can make them not the\nsame covariance matrix.",
    "start": "1789740",
    "end": "1796210"
  },
  {
    "text": "In terms of the\noptimization, in terms of learning this\nthese things, it's",
    "start": "1796210",
    "end": "1801539"
  },
  {
    "text": "going to be more complicated. So it's still learnable at least\nwith some advanced techniques.",
    "start": "1801540",
    "end": "1806710"
  },
  {
    "text": "But it's going to\nbe more complicated. So here, it's really, in\nsome sense, a simplification. A simplified assumption.",
    "start": "1806710",
    "end": "1813980"
  },
  {
    "text": "[INAUDIBLE] given [INAUDIBLE]\nx given y equal to--",
    "start": "1813980",
    "end": "1821419"
  },
  {
    "text": "Oh, yeah. So that's a good question. So this is like this. So you give me this event y, 0,\nwhat's the distribution of x?",
    "start": "1821419",
    "end": "1831059"
  },
  {
    "text": "I'll assume they're providing\nseparate [INAUDIBLE]",
    "start": "1831059",
    "end": "1836390"
  },
  {
    "text": "distribution. Is it because they are\ndifferent in the [INAUDIBLE]??",
    "start": "1836390",
    "end": "1844210"
  },
  {
    "text": "That's a good question. So how do you model the x and y? So in many cases, you have\nmany, many different choices.",
    "start": "1844210",
    "end": "1850549"
  },
  {
    "text": "But you have\ndifferent covariance. You can have different means. Or you can even model\nthem in different ways. So here, I think--",
    "start": "1850550",
    "end": "1858200"
  },
  {
    "text": "at least-- if you\nlook at the data, you see that the distribution\nof x given y is 1 and x given y",
    "start": "1858200",
    "end": "1866769"
  },
  {
    "text": "So probably it makes sense to\nmodel them separately, right? If you model the whole\nthing as a joint Gaussian,",
    "start": "1866769",
    "end": "1871940"
  },
  {
    "text": "I think it doesn't\nlook like a Gaussian. That's pretty much the reason.",
    "start": "1871940",
    "end": "1879190"
  },
  {
    "text": "OK. Cool. So are we done?",
    "start": "1879190",
    "end": "1885710"
  },
  {
    "text": "So we haven't done\nwith the model yet because we only\nmodel x given y, right?",
    "start": "1885710",
    "end": "1890809"
  },
  {
    "text": "Remember that we also\nhave to model p of y. You need the p of\ny and p of x and y",
    "start": "1890809",
    "end": "1896560"
  },
  {
    "text": "to know the joint\nprobability distribution and then you can\nuse the Bayes rule. So how do we model the p of y?",
    "start": "1896561",
    "end": "1902039"
  },
  {
    "text": "This is relatively\neasier because p of y is only a distribution\nover two possible choices. Y only have two\nchoices, 0 and 1.",
    "start": "1902039",
    "end": "1909090"
  },
  {
    "text": "So basically, you just have\nto have two parameters, right? So one parameter is supposed\nto model p of y as one,",
    "start": "1909090",
    "end": "1916299"
  },
  {
    "text": "let's call this phi. And then you have p\nof y is easier, zero. Let's call this one minus v.\nBecause the sum of these two",
    "start": "1916299",
    "end": "1924000"
  },
  {
    "text": "has to be 0. Plus, the sum of\nthese two has to be 1. So in some sense, you say\ny is from this Bernoulli",
    "start": "1924000",
    "end": "1932270"
  },
  {
    "text": "distribution. With parameter phi.",
    "start": "1932270",
    "end": "1939580"
  },
  {
    "text": "This is just another\nway to say this. OK? So basically in summarize,\nwhat are the parameters?",
    "start": "1939580",
    "end": "1945669"
  },
  {
    "text": "So the parameters-- I guess this goes\nback to the question. Someone asked this question.",
    "start": "1945669",
    "end": "1951640"
  },
  {
    "text": "So we do have to\nhave parameters even for the generative\nlearning algorithm.",
    "start": "1951640",
    "end": "1957480"
  },
  {
    "text": "And the parameters are mu And our goal, our\nnext step, would",
    "start": "1957480",
    "end": "1966049"
  },
  {
    "text": "be, we want to learn these\nparameters so that we know p of x and y and p of y so that\nwe can compute p of y the next.",
    "start": "1966049",
    "end": "1981908"
  },
  {
    "text": "So the next part is\nabout fitting parameters. OK?",
    "start": "1981909",
    "end": "1987610"
  },
  {
    "text": "So how do we learn the\nparameters from data?",
    "start": "1987610",
    "end": "1996400"
  },
  {
    "text": "So we learn the parameters. The general principle\nis maximum likelihood.",
    "start": "1996400",
    "end": "2012860"
  },
  {
    "text": "I think probably Chris has\ntalked about this word maximum likelihood probably once or\ntwice in the previous lectures.",
    "start": "2012860",
    "end": "2019200"
  },
  {
    "text": "But here, the maximum likelihood\nis a little bit different. I'm going to compare\nwhat's the difference between this likelihood from the\none that we discussed before.",
    "start": "2019200",
    "end": "2027340"
  },
  {
    "text": "So first let me define what\nmaximum likelihood here means in this setting. So by maximum likelihood, I\nfirst have defined likelihood.",
    "start": "2027340",
    "end": "2035230"
  },
  {
    "text": "So likelihood is this\nbasically the chance of seeing a data\ngiven the parameters.",
    "start": "2035230",
    "end": "2041260"
  },
  {
    "text": "So it's a function\nof the parameter. So if you have this\nparameters, phi, mu 1, mu 0.",
    "start": "2041260",
    "end": "2047050"
  },
  {
    "text": "Mu 0, mu 1, and sigma. You can define the likelihood\nof these parameters. This is the chance of seeing all\nyour data ",
    "start": "2047050",
    "end": "2066669"
  },
  {
    "text": "given the parameters. So you hypothetically think\nthat all the data generated",
    "start": "2066669",
    "end": "2072050"
  },
  {
    "text": "from this distribution. And you look at what's\nthe density of your data",
    "start": "2072050",
    "end": "2079510"
  },
  {
    "text": "under these parameters. Sorry. [INAUDIBLE] Oh, even bigger?",
    "start": "2079510",
    "end": "2085000"
  },
  {
    "text": "OK. Cool. Yeah, sure. And let me also clarify\nthe notation here.",
    "start": "2085000",
    "end": "2090789"
  },
  {
    "text": "So x superscript i, I think\nprobably Chris defined this,",
    "start": "2090790",
    "end": "2096000"
  },
  {
    "text": "right? So we're going to use this\nthroughout the lectures.",
    "start": "2096000",
    "end": "2102040"
  },
  {
    "text": "So xi, yi, this is\nthe i-th example.",
    "start": "2102040",
    "end": "2116230"
  },
  {
    "text": "So we have this data\nset with n examples, and I'm looking\nat the likelihood of all of these examples under\nthe parameter phi, mu0, mu1,",
    "start": "2116230",
    "end": "2124880"
  },
  {
    "text": "and sigma. So for every parameter,\nyou have a likelihood. And this likelihood can have--",
    "start": "2124880",
    "end": "2133750"
  },
  {
    "text": "This sounds kind of complicated. But actually, you can\nsomewhat simplify it a little bit because these\nexamples are independent.",
    "start": "2133750",
    "end": "2142369"
  },
  {
    "text": "So you are assumed that your\ndata are drawn independently from-- each examples are\ndrawn independently.",
    "start": "2142370",
    "end": "2148099"
  },
  {
    "text": "So then you can factorize this. So this is the product of the\nlikelihood of all the examples.",
    "start": "2148099",
    "end": "2155410"
  },
  {
    "text": "Because you used\nthe independence. The p of xi, yi given.",
    "start": "2155410",
    "end": "2172549"
  },
  {
    "text": "And you can even factorize\nthis a little bit more to say that you\ncan use the table",
    "start": "2172549",
    "end": "2181181"
  },
  {
    "text": "to get p of xi given yi and\nthe parameter the parameters.",
    "start": "2181181",
    "end": "2190849"
  },
  {
    "text": "And times p of yi\ngiven the parameters,",
    "start": "2190849",
    "end": "2201230"
  },
  {
    "text": "No, that's not all. Everything depends\non everything else.",
    "start": "2201230",
    "end": "2212539"
  },
  {
    "text": "I don't think I have\na different color. So here, you can have\nsome simplification.",
    "start": "2212540",
    "end": "2218609"
  },
  {
    "text": "Because yi, the distribution\nof y, only depends on phi. The mu1, mu2, mu0,\nmu1, and sigma",
    "start": "2218609",
    "end": "2226359"
  },
  {
    "text": "are describing the\nconditional probability. So y only depends on phi.",
    "start": "2226359",
    "end": "2231500"
  },
  {
    "text": "So you don't have to\nwrite here these things. But because there is\nno such dependencies. And also the same thing,\nxi condition yi only",
    "start": "2231500",
    "end": "2240710"
  },
  {
    "text": "depends on mu0, mu1, sigma. It doesn't depend\non phi, so you don't have to necessarily phi here. Even though you write\nit, is the same.",
    "start": "2240710",
    "end": "2247838"
  },
  {
    "text": "OK. So this is the so-called\nmaximum likelihood.",
    "start": "2247839",
    "end": "2253339"
  },
  {
    "text": "And what you do is\nyou want to say, I'm going to maximize this\nL, phi, mu0, mu1, and sigma.",
    "start": "2253339",
    "end": "2264390"
  },
  {
    "text": "So basically, you say\nI'm going to maximize. So the learned parameters\nwill be the maximizer",
    "start": "2264390",
    "end": "2272740"
  },
  {
    "text": "of this likelihood function. You want to find\na parameter such that the likelihood\nis the largest.",
    "start": "2272740",
    "end": "2280130"
  },
  {
    "text": "So this is the so-called maximum\nlikelihood in our context. Why are you making thes\ndependencies like--",
    "start": "2280130",
    "end": "2287890"
  },
  {
    "text": "is it possible that\nthere is some kind of dependence between the data\nand in that case what do we do?",
    "start": "2287890",
    "end": "2295010"
  },
  {
    "text": "All right. So, why-- the question\nis, why are you making these independent assumptions? I think in short, if I\nhave a very short answer,",
    "start": "2295010",
    "end": "2302410"
  },
  {
    "text": "I think this is\nalmost like always assumed that in almost\ncertain settings, even in the most\nadvanced settings.",
    "start": "2302410",
    "end": "2308020"
  },
  {
    "text": "And the reason is\nthat there are I think there are multiple reasons. You can say this is-- in some sense-- one thing\nthat you can imagine",
    "start": "2308020",
    "end": "2315630"
  },
  {
    "text": "is that you do collect\ndata somewhat independently from a very large pool. That's probably the\nsimplest way to say it.",
    "start": "2315630",
    "end": "2322019"
  },
  {
    "text": "Of course, there are cases that\nthis independence is not true. For example, if you\nhave interactions. For example, suppose I first\nget some data from you,",
    "start": "2322020",
    "end": "2329480"
  },
  {
    "text": "and then I do\nsomething, and then I get some other data from\nyou, and maybe these data are no longer independent. Or maybe the second time you\nprovide me data, you also",
    "start": "2329480",
    "end": "2337980"
  },
  {
    "text": "look at the first time. Sometimes there is something\nabout this dependencies. Especially in\nreinforcement learning",
    "start": "2337980",
    "end": "2343630"
  },
  {
    "text": "like where you\nhave interactions. So in those cases, we\nwill drop this kind of independence assumption. But in most of the\ncases, we do assume",
    "start": "2343630",
    "end": "2349569"
  },
  {
    "text": "the data are sampled from\na large pool independently. [INAUDIBLE] Phi is a scalar.",
    "start": "2349570",
    "end": "2354860"
  },
  {
    "text": "Yes, you are right. And the phi is a scalar and\nit's also a scalar in 0.",
    "start": "2354860",
    "end": "2362849"
  },
  {
    "text": "That's a good question. [INAUDIBLE] Do you have the\nfreedom to change phi?",
    "start": "2362850",
    "end": "2370740"
  },
  {
    "text": "Yeah. We are-- also phi is\na parameter, right? So you are going to learn phi. You're going to learn what\nis the right phi from data.",
    "start": "2370740",
    "end": "2378630"
  },
  {
    "text": "So how do you learn phi? So you are going to find\nout the maximizer of this. And the maximizer will be like--",
    "start": "2378630",
    "end": "2384180"
  },
  {
    "text": "When we talk, for example,\nsome training that data. If you could just phi equal to\ncases where y is equal to 1.",
    "start": "2384180",
    "end": "2394740"
  },
  {
    "text": "But both [INAUDIBLE]. You are exactly right. You are ahead of it. But we are going to prove that.",
    "start": "2394740",
    "end": "2399799"
  },
  {
    "text": "We are going to show that\nthat's actually the solution. So there's a reason for that. You have the very\ngood intuition. The phi is pretty\nmuch the proportion",
    "start": "2399800",
    "end": "2407869"
  },
  {
    "text": "of the positive examples. The proportion of\npositive examples.",
    "start": "2407869",
    "end": "2413020"
  },
  {
    "text": "But we are going\nto actually show that's actually the case if\nyou use this methodology. Yeah. Can you just repeat\nin maximum likelihood",
    "start": "2413020",
    "end": "2418560"
  },
  {
    "text": "or in maximizing the chance\nof the data being represented?",
    "start": "2418560",
    "end": "2426609"
  },
  {
    "text": "So the maximum\nlikelihood is you are maximizing the chance of the\ndata given the parameters.",
    "start": "2426609",
    "end": "2433109"
  },
  {
    "text": "So the parameters\nare just the sum. You look at all\npossible parameters and see which one\nwhich parameter",
    "start": "2433109",
    "end": "2438869"
  },
  {
    "text": "can give the most likelihood.",
    "start": "2438869",
    "end": "2446549"
  },
  {
    "text": "So this is the\nmaximum likelihood. This is the\nmethodology we're going to use for generative\nlearning algorithms. Not only today's\nalgorithm, but also",
    "start": "2446550",
    "end": "2453040"
  },
  {
    "text": "for next lecture where\nwe have other settings. We still maximize\nthe likelihood. And just to compare this with\nthe discriminative learning",
    "start": "2453040",
    "end": "2460609"
  },
  {
    "text": "algorithm, there we also\nuse maximum likelihood. And the meaning of the\nphrase maximum likelihood",
    "start": "2460609",
    "end": "2466349"
  },
  {
    "text": "could be a little bit different. So here, you are maximizing\nthe so-called the joint density",
    "start": "2466349",
    "end": "2474549"
  },
  {
    "text": "of both x and y. You are maximizing\nthe probability to see the pairs of x and y.",
    "start": "2474550",
    "end": "2480779"
  },
  {
    "text": "But for discriminative\nalgorithms.",
    "start": "2480779",
    "end": "2487079"
  },
  {
    "text": "So for discriminative algorithm.",
    "start": "2487079",
    "end": "2494359"
  },
  {
    "text": "So what you do that\nis that you are maximizing the so-called\nconditional likelihood.",
    "start": "2494359",
    "end": "2502338"
  },
  {
    "text": "Even though many\ncases, people just drop the word conditional when\nit's clear from the context.",
    "start": "2502339",
    "end": "2509230"
  },
  {
    "text": "The conditional likelihood\nis this probability",
    "start": "2509230",
    "end": "2516130"
  },
  {
    "text": "of seeing the family\nof labels conditioned",
    "start": "2516130",
    "end": "2522250"
  },
  {
    "text": "on the inputs and\nthe parameter theta.",
    "start": "2522250",
    "end": "2532770"
  },
  {
    "text": "And you can also factorize this. You can factorize this as-- so here, I'm using theta\nas a generic parameter",
    "start": "2532770",
    "end": "2537890"
  },
  {
    "text": "just to be abstract\nbecause I'm talking about the abstract setting. So you can think of this data\nas the linear model family.",
    "start": "2537890",
    "end": "2546599"
  },
  {
    "text": "And you can still\nfactorize this. You can factorize this\ninto using independence.",
    "start": "2546600",
    "end": "2557460"
  },
  {
    "text": "But whatever you do,\nyou always condition x. So x is considered\nto be, in some sense, a deterministic\nquality you observe.",
    "start": "2557460",
    "end": "2564220"
  },
  {
    "text": "You don't know how\nx is generated. We don't care about\nhow x is generated. You just care about how y is\ngenerated conditioned on you",
    "start": "2564220",
    "end": "2571480"
  },
  {
    "text": "see x. And part of the reason why\nyou do this is that you only",
    "start": "2571480",
    "end": "2577290"
  },
  {
    "text": "model y given x. You didn't model what's\nthe distribution of x. So there's actually\nno way you can do this maximum likelihood\nabove in a discriminative sense.",
    "start": "2577290",
    "end": "2587880"
  },
  {
    "text": "Because the only thing\nyou model is y given x. That's the only quantity that\nyou have for parameterized form",
    "start": "2587880",
    "end": "2594369"
  },
  {
    "text": "for. So you just go with\nwhatever you have subsets.",
    "start": "2594369",
    "end": "2600460"
  },
  {
    "text": "And it turns out that these\ntwo are indeed different. There are some\nrelationship and there are differences which I'll\ndiscuss after I introduce",
    "start": "2600460",
    "end": "2607859"
  },
  {
    "text": "some more examples I think. Any questions so far? So this is what we've\nbeen doing so far.",
    "start": "2607859",
    "end": "2616480"
  },
  {
    "text": "This is what we-- yes. In the last weeks. Exactly.",
    "start": "2616480",
    "end": "2622060"
  },
  {
    "text": "I think this is probably\nthe best discussed after I give some examples\nof the concrete examples.",
    "start": "2622060",
    "end": "2628930"
  },
  {
    "text": "But in some sense, you\ncan see the differences between these two\nkind of algorithms. And the differences between\nthese two type of assumptions",
    "start": "2628930",
    "end": "2635260"
  },
  {
    "text": "is that here you have more\nassumptions on all the data.",
    "start": "2635260",
    "end": "2640760"
  },
  {
    "text": "You are making some\nassumptions on both x and y. And here, you're only making\nassumptions on y given x.",
    "start": "2640760",
    "end": "2646140"
  },
  {
    "text": "So it's really about how-- the differences will be about\nhow many assumptions you impose",
    "start": "2646140",
    "end": "2652430"
  },
  {
    "text": "on the structure of your data. How much-- like in some sense,\nthere is a whole universe.",
    "start": "2652430",
    "end": "2657750"
  },
  {
    "text": "So you cannot model everything. So you choose some part of\nthe quantities you can model.",
    "start": "2657750",
    "end": "2664500"
  },
  {
    "text": "And the decision here is\nyou model both x and y. And decision here is you\nonly model part of it. And that will cause some\ndifferences in certain cases.",
    "start": "2664500",
    "end": "2674150"
  },
  {
    "text": "So can we learn\n[INAUDIBLE] assumptions about distributions\nof our features?",
    "start": "2674150",
    "end": "2679690"
  },
  {
    "text": "So with that [INAUDIBLE]? That's a great question.",
    "start": "2679690",
    "end": "2687420"
  },
  {
    "text": "So the question was that, here,\nfor the generative algorithms, we have the generative\nlearning algorithms.",
    "start": "2687420",
    "end": "2693150"
  },
  {
    "text": "We have assumptions on\nthe features, right? Would that be a problem if you\ngeneralize to other examples.",
    "start": "2693150",
    "end": "2699460"
  },
  {
    "text": "So it depends. It depends on whether your\nassumptions are correct or not in some sense. But if your assumptions on the\nfeatures is kind of Gaussian,",
    "start": "2699460",
    "end": "2707980"
  },
  {
    "text": "then actually it\nwould provide you more generalization because\nyour assumptions are correct and they impose structures.",
    "start": "2707980",
    "end": "2714640"
  },
  {
    "text": "And when your\nassumption is wrong, then it would cause problems. So actually, this is some--",
    "start": "2714640",
    "end": "2721710"
  },
  {
    "text": "yeah. This is basically like the\nmain difference, right?",
    "start": "2721710",
    "end": "2726740"
  },
  {
    "text": "For different algorithms,\nsometimes you can have-- as I said, you have a lot of\nvariables in this world, right?",
    "start": "2726740",
    "end": "2732849"
  },
  {
    "text": "So you probably can even\npick some other like--",
    "start": "2732850",
    "end": "2738000"
  },
  {
    "text": "so basically, you have-- you can try to model\na lot of mechanisms or you can try to only\nmodel a part of it.",
    "start": "2738000",
    "end": "2743880"
  },
  {
    "text": "And what's the decision\noften is a tradeoff. If you model too much,\nthen you are risking",
    "start": "2743880",
    "end": "2749319"
  },
  {
    "text": "to model them incorrectly. And if you model too less,\nthen you don't have enough-- like, you don't leverage enough\nprior knowledge in subsets,",
    "start": "2749319",
    "end": "2757920"
  },
  {
    "text": "right? Like, if for example, if you\nreally know this is a Gaussian, you probably should just\nleverage the prior knowledge.",
    "start": "2757920",
    "end": "2764140"
  },
  {
    "text": "But you may be wrong so\nit depends on the cases. Yeah, this is a great question.",
    "start": "2764140",
    "end": "2769270"
  },
  {
    "text": "And I'll discuss\na little bit more about this in a more\nmathematical level. Will we use the Gaussian?",
    "start": "2769270",
    "end": "2775349"
  },
  {
    "text": "Because given the mean\nand variance and you don't know anything,\nis this the best way",
    "start": "2775349",
    "end": "2780400"
  },
  {
    "text": "to model something in real life? Yeah. I hear the question is, given\nthe mean and covariance, if you",
    "start": "2780400",
    "end": "2788839"
  },
  {
    "text": "don't know anything else,\nis the Gaussian the best? Is that question? Yes. I think that's a great question.",
    "start": "2788839",
    "end": "2793930"
  },
  {
    "text": "So typically, you\nare basically right. If you don't know anything\nelse, then you probably",
    "start": "2793930",
    "end": "2798960"
  },
  {
    "text": "should just model\nthem like Gaussian if you only know\nmean and covariance. But on the other\nhand, to be fair,",
    "start": "2798960",
    "end": "2805980"
  },
  {
    "text": "you know more than the\nmean and covariance. So for example, if\nyou really want, you can compute the third other\ncorrelations between these data",
    "start": "2805980",
    "end": "2812940"
  },
  {
    "text": "points, between\nthese coordinates. So in theory, you can also-- because you have so many data--\nif you have a lot of data,",
    "start": "2812940",
    "end": "2818749"
  },
  {
    "text": "you probably can model other\nhigher moments of the data. I'm not sure of the\ndefinition of moments.",
    "start": "2818749",
    "end": "2826180"
  },
  {
    "text": "You look at the\nhigher correlations between the coordinates of the\ndata if you have a lot of data. But Gaussian is a pretty\nreasonable assumption",
    "start": "2826180",
    "end": "2832559"
  },
  {
    "text": "and it's still used very often. Sometimes people use\ntransformations of Gaussian.",
    "start": "2832559",
    "end": "2838119"
  },
  {
    "text": "So here, we assume\nthey are Gaussian. Sometimes people use\nlike, you can transform the Gaussian in certain ways. But Gaussian is a pretty\nreasonable assumption.",
    "start": "2838119",
    "end": "2845240"
  },
  {
    "text": "[INAUDIBLE] the y is a\ncontinuous value then [INAUDIBLE]?",
    "start": "2845240",
    "end": "2851410"
  },
  {
    "text": "So how do I do the case\nwhen y is continuous?",
    "start": "2851410",
    "end": "2860900"
  },
  {
    "text": "Is that the question? Yes. So we don't cover that. But pretty much, you follow\nthe same methodology. You are going to have a\ndifferent prior or different",
    "start": "2860900",
    "end": "2868309"
  },
  {
    "text": "distribution for\ny, p of y, right? So maybe you can model p\nof y by say Gaussian again,",
    "start": "2868309",
    "end": "2873960"
  },
  {
    "text": "if you want or-- [INAUDIBLE] You're going to\nhave more variables to describe the distribution.",
    "start": "2873960",
    "end": "2880270"
  },
  {
    "text": "The accuracy will\nbe [INAUDIBLE]?? The accuracy?",
    "start": "2880270",
    "end": "2885609"
  },
  {
    "text": "Yes, the accuracy [INAUDIBLE]? Oh. So I guess the question--",
    "start": "2885609",
    "end": "2890900"
  },
  {
    "text": "So-- in short, whether y\nis discrete or continuous in some sense is mostly\ndecided by our data set, right?",
    "start": "2890900",
    "end": "2900819"
  },
  {
    "text": "So I think typically if your\ndata set is really discrete. So if you really just\nhave benign cancer or not,",
    "start": "2900819",
    "end": "2907559"
  },
  {
    "text": "you probably don't\nwant to make it continuous for the same\nreason as you said. Why do you want to make it\nmore complicated, right? So you have more\nparameters to learn, right?",
    "start": "2907560",
    "end": "2914080"
  },
  {
    "text": "But sometimes, it's just like\nyour y is really continuous. There's no way you\ncan discreteize them",
    "start": "2914080",
    "end": "2919420"
  },
  {
    "text": "in a meaningful way. And also, to be fair,\nthe parameter to model y",
    "start": "2919420",
    "end": "2926829"
  },
  {
    "text": "is often much smaller. You have a much smaller\nnumber of parameters to model y than the number\nof parameters to model x.",
    "start": "2926829",
    "end": "2934880"
  },
  {
    "text": "For example here if\nyou count, so you only need one parameter model y.",
    "start": "2934880",
    "end": "2940220"
  },
  {
    "text": "So even if this is continuous,\nyou only need one real number to model y. Problem, if you have\nsay for example y",
    "start": "2940220",
    "end": "2945391"
  },
  {
    "text": "is a Gaussian distribution, but\nit's one dimensional, right? So you only need one parameter. But for x, it's a\nhigh dimensional thing",
    "start": "2945391",
    "end": "2951279"
  },
  {
    "text": "so you have to always\nuse more parameters. So typically, it's\nnot a big issue. [INAUDIBLE] you began\n[INAUDIBLE] two distributions,",
    "start": "2951279",
    "end": "2959190"
  },
  {
    "text": "one for when y equals 2,\nanother one for [INAUDIBLE]..",
    "start": "2959190",
    "end": "2966869"
  },
  {
    "text": "But let's say you\nhave more features, then it's kind of harder\nto visualize the data.",
    "start": "2966869",
    "end": "2972280"
  },
  {
    "text": "How do you make this judgment\nof how many distributions to have on your parameter?",
    "start": "2972280",
    "end": "2980410"
  },
  {
    "text": "Yeah, I think that's\na great question. How do you make this judgment? How many kind of\ndistributions, right?",
    "start": "2980410",
    "end": "2985960"
  },
  {
    "text": "So the easiest to answer\nis that you always use two as long as you have two labels.",
    "start": "2985960",
    "end": "2991310"
  },
  {
    "text": "If you just have two types of\nthings you want to classify, you just always have two\ndifferent distributions, two Gaussians.",
    "start": "2991310",
    "end": "2997140"
  },
  {
    "text": "Of course, you may want\nto go more advanced. To say, even for the\nbenign cancer, right?",
    "start": "2997140",
    "end": "3002590"
  },
  {
    "text": "It's not like really-- really like all the benign\ncancers are the same, right? Maybe there are two\nsub-populations, right?",
    "start": "3002590",
    "end": "3008609"
  },
  {
    "text": "So it does probably\nrequire a little bit of domain knowledge, or\nmaybe your trials and errors. You could try to\ngeneralize this.",
    "start": "3008609",
    "end": "3020090"
  },
  {
    "text": "OK. So let me proceed with the-- OK.",
    "start": "3020090",
    "end": "3025119"
  },
  {
    "text": "So I discussed a lot of\nmethodologies so far. So now my next goal is\nhow do I maximize this?",
    "start": "3025119",
    "end": "3032329"
  },
  {
    "text": "How do I maximize\nthe likelihood? So you need to be able\nto do this empirically. So to get the parameters.",
    "start": "3032329",
    "end": "3039339"
  },
  {
    "text": "So this is about\ncomputation, partly. So what do we do is we-- one choice is that\nyou just write down",
    "start": "3039339",
    "end": "3044720"
  },
  {
    "text": "this function in your computer\nand you write some optimization algorithm. But you only do a little\nmore than that in math",
    "start": "3044720",
    "end": "3051609"
  },
  {
    "text": "because that would simplify\nyour implementation, right? So we're going to\nsimplify this formula",
    "start": "3051609",
    "end": "3057250"
  },
  {
    "text": "so that we can-- and actually\nwe're going to do a lot of math so that you don't really need\na laptop or your computer",
    "start": "3057250",
    "end": "3063849"
  },
  {
    "text": "to compute the parameters\nto run optimization algorithm for this, right? So what I'm going\nto do is that--",
    "start": "3063849",
    "end": "3072510"
  },
  {
    "text": "so the first thing is that we\nknow that if you do a argmax.",
    "start": "3072510",
    "end": "3081329"
  },
  {
    "text": "So we care about\nthe argmax, right? The maximizer of this thing.",
    "start": "3081329",
    "end": "3087579"
  },
  {
    "text": "So the maximizer is the same\nif you transform your loss function with any continuous\nmultiple functions.",
    "start": "3087579",
    "end": "3096539"
  },
  {
    "text": "So even if you add a log, the\nmaximizer would be the same.",
    "start": "3096540",
    "end": "3106520"
  },
  {
    "text": "And for the purpose\nof this course, we define this using a little l.",
    "start": "3106520",
    "end": "3114000"
  },
  {
    "text": "This is the log likelihood. OK.",
    "start": "3114000",
    "end": "3120329"
  },
  {
    "text": "While we are doing this,\nit sounds like we just introduce something even more--",
    "start": "3120329",
    "end": "3127319"
  },
  {
    "text": "more symbols. The reason is that this will\nmake the product to itself.",
    "start": "3127319",
    "end": "3133200"
  },
  {
    "text": "So log of this product. Log of a bunch of\nproductive bunch of terms",
    "start": "3133200",
    "end": "3140020"
  },
  {
    "text": "will be equal to the sum of\nthe log of each of the terms.",
    "start": "3140020",
    "end": "3145290"
  },
  {
    "text": "So this will be sum of the\nlog of these two terms.",
    "start": "3145290",
    "end": "3151840"
  },
  {
    "text": "And the log of the\nproduct of these two terms will be the sum of the\nlog of each of them. So the log of p of xi given\nyi plus the log of pyi given--",
    "start": "3151840",
    "end": "3174319"
  },
  {
    "text": "I guess I don't have the right\nphi here for this purpose.",
    "start": "3174320",
    "end": "3181230"
  },
  {
    "text": "Right. So everything becomes a sum and\nthat's very important actually. Because even you do\nthis numerically,",
    "start": "3181230",
    "end": "3188450"
  },
  {
    "text": "it's very important\nto take the log. Because all of these numbers,\nif you do it empirically, you will see like they are\neither very small or very big.",
    "start": "3188450",
    "end": "3196450"
  },
  {
    "text": "Just because-- recall that we\nhave Gaussian distribution. There's an exponential here.",
    "start": "3196450",
    "end": "3201849"
  },
  {
    "text": "So it's pretty easy to\nsee the density function. It's pretty easy to be either\nvery big or very small.",
    "start": "3201850",
    "end": "3206920"
  },
  {
    "text": "They're not on the best\nscale like you can imagine. But if you take a log of it,\nthe scale will be much nicer.",
    "start": "3206920",
    "end": "3214650"
  },
  {
    "text": "So the log of the\ndensity will be like some reasonable\nkind of scaling so that you can numerically use.",
    "start": "3214650",
    "end": "3221299"
  },
  {
    "text": "And also it becomes a sum so you\ndon't have to do the product. And then, how do we proceed?",
    "start": "3221300",
    "end": "3227490"
  },
  {
    "text": "One option is that,\nagain, you can still do the numerical stuff. You can do a optimization\nalgorithm to get the minimizer.",
    "start": "3227490",
    "end": "3235829"
  },
  {
    "text": "But here, we can actually\nanalytically compute the maximizer.",
    "start": "3235829",
    "end": "3240920"
  },
  {
    "text": "So the maximizer here,\nhow do you compute it? What you do is you--",
    "start": "3240920",
    "end": "3246530"
  },
  {
    "text": "I'm going to continue here. So how do you find\nthe maximizer?",
    "start": "3246530",
    "end": "3252290"
  },
  {
    "text": "So there's a small fact. I guess, probably you learn\nfrom the calculus class.",
    "start": "3252290",
    "end": "3258770"
  },
  {
    "text": "So if theta is a\nmaximizer, then that",
    "start": "3258770",
    "end": "3264920"
  },
  {
    "text": "means that of some\nfunction of theta-- I'm being abstract here.",
    "start": "3264920",
    "end": "3269950"
  },
  {
    "text": "So then it means\nthat the gradient of the function at theta\nevaluate theta is equal to 0.",
    "start": "3269950",
    "end": "3279369"
  },
  {
    "text": "So if you are on\nthe maximizer, you have to satisfy the gradient 0. And actually in many\ncases, if this f is convex, then this is if and only if.",
    "start": "3279369",
    "end": "3286600"
  },
  {
    "text": "If f is not convex, this is\nstill a necessary condition.",
    "start": "3286600",
    "end": "3291740"
  },
  {
    "text": "So for us, it's\nactually convex so it's a necessary and\nsufficient condition. But for other cases, this\nmight be just only a necessary",
    "start": "3291740",
    "end": "3300869"
  },
  {
    "text": "condition. So because you have this, then\nyou can solve the equation. So you can try to\nsolve the equation.",
    "start": "3300869",
    "end": "3308829"
  },
  {
    "text": "I'm going back to-- this\nis a small abstract fact. Now I'm going back to this case. So basically you\nsay, the gradient",
    "start": "3308829",
    "end": "3316150"
  },
  {
    "text": "of the loss with back to all the\nparameters should be 0, right? Like, with respect to phi, mu1,\nmu2, and sigma should all be 0.",
    "start": "3316150",
    "end": "3326230"
  },
  {
    "text": "So basically you just have\nto say, the gradient is zero.",
    "start": "3326230",
    "end": "3334369"
  },
  {
    "text": "And this really just means that\nthe partial derivative is back to each of the parameter is 0.",
    "start": "3334369",
    "end": "3351538"
  },
  {
    "text": "So now you have four equations. And you can try to find the\nsolutions for these equations.",
    "start": "3351539",
    "end": "3357250"
  },
  {
    "text": "And this is I think\nhomework 1 q1d.",
    "start": "3357250",
    "end": "3365440"
  },
  {
    "text": "So in that homework,\nwe're asking you to-- first of all, you\nhave to compute what is each of these is, right?",
    "start": "3365440",
    "end": "3371059"
  },
  {
    "text": "So you have to have\nan analytical formula for each of this, right? So what is the derivative\nof l with respect to phi.",
    "start": "3371059",
    "end": "3377260"
  },
  {
    "text": "You have to do some calculation\nto see what's the derivative. You have to plug in all of\nour definition of this p,",
    "start": "3377260",
    "end": "3383109"
  },
  {
    "text": "these two p's. And you get the loss function\nas a function of phi, and then you derive the\nderivative with respect",
    "start": "3383109",
    "end": "3390420"
  },
  {
    "text": "to phi, and then write\nout that this is 0, and you solve the equations. So that's homework q1d.",
    "start": "3390420",
    "end": "3396799"
  },
  {
    "text": "And this is sort of\na complicated to some extent but not-- it's still manageable.",
    "start": "3396799",
    "end": "3402500"
  },
  {
    "text": "There are even more\ncomplicated things than this in machine learning. But at the first time, it would\nbe a little bit complicated",
    "start": "3402500",
    "end": "3410020"
  },
  {
    "text": "because all of this has a\nlittle bit of complex formulas. And what I'm going to\ndo next is that I'm",
    "start": "3410020",
    "end": "3419710"
  },
  {
    "text": "going to tell you what's the\nsolution of this directly. So you know the answer\nof the homework question.",
    "start": "3419710",
    "end": "3425130"
  },
  {
    "text": "And I'm going to proceed-- I'm going to interpret why\nthe solution makes some sense. You will see the\nsolutions actually make",
    "start": "3425130",
    "end": "3431309"
  },
  {
    "text": "a lot of sense intuitively. And I'm going to\nproceed with that. [INAUDIBLE] is it, like\nwouldn't mu 0 be 1,",
    "start": "3431309",
    "end": "3439578"
  },
  {
    "text": "[INAUDIBLE] switch to mu1, mu2. Oh, my bad. Mu1. Did I switch to that?",
    "start": "3439579",
    "end": "3444690"
  },
  {
    "text": "Oh, my bad. This is mu0. This is mu1. My bad. Thank you. And only the first one\nis a scalar, right?",
    "start": "3444690",
    "end": "3452789"
  },
  {
    "text": "The other one is already\nthe vector [INAUDIBLE]??",
    "start": "3452789",
    "end": "3457970"
  },
  {
    "text": "So what's the\ndimensions of this? Yeah, that's a\nwonderful question. So I think often this is a\nconfusion that is pretty often.",
    "start": "3457970",
    "end": "3465319"
  },
  {
    "text": "Like, just because\nsometimes in math they have different things. So in this class, the\nderivative with respect",
    "start": "3465320",
    "end": "3474400"
  },
  {
    "text": "to the parameter,\nany parameter, we have the same shape\nas the parameter. So this is a\none-dimensional parameter.",
    "start": "3474400",
    "end": "3481849"
  },
  {
    "text": "So that means that\nthis is a scalar. This derivative is a scalar. And this is the d-dimensional\nvector, meaning 0.",
    "start": "3481849",
    "end": "3487890"
  },
  {
    "text": "So that means that\nderivative is rd. And this is rd by d. [INAUDIBLE] scalar.",
    "start": "3487890",
    "end": "3493588"
  },
  {
    "text": "The other 0's are like\n[INAUDIBLE] vector",
    "start": "3493589",
    "end": "3499559"
  },
  {
    "text": "are the ones [INAUDIBLE]. Right. So this is a 0 as a vector.",
    "start": "3499559",
    "end": "3510190"
  },
  {
    "text": "Thank you. OK. Cool. So I need to erase\nsomething I guess.",
    "start": "3510190",
    "end": "3534240"
  },
  {
    "text": "So what other solutions.",
    "start": "3534240",
    "end": "3539990"
  },
  {
    "text": "So the solutions are-- OK.",
    "start": "3539990",
    "end": "3545240"
  },
  {
    "text": "I'm going to first\ndefine some notations. So \nlet U0 to be all the",
    "start": "3545240",
    "end": "3564880"
  },
  {
    "text": "examples that are positive. We said that index for\nthe positive examples.",
    "start": "3564880",
    "end": "3571230"
  },
  {
    "text": "Indices positive. And-- wait, my bad.",
    "start": "3571230",
    "end": "3582289"
  },
  {
    "text": "This is U1. U0 is the indices of\nnegative examples.",
    "start": "3582289",
    "end": "3597390"
  },
  {
    "text": "So under the MLE solution, the\nsolution will be the following. So phi is equal to U1 over n.",
    "start": "3597390",
    "end": "3606869"
  },
  {
    "text": "Where n is the total\nnumber of examples.",
    "start": "3606869",
    "end": "3621519"
  },
  {
    "text": "Which is equal to U0 plus U1. So, what is this?",
    "start": "3621520",
    "end": "3627068"
  },
  {
    "text": "This is really just the\nfraction of positive examples.",
    "start": "3627069",
    "end": "3637130"
  },
  {
    "text": "So this is the phi you learn. So phi is supposed to be the\nprobability of y is 1, right?",
    "start": "3637130",
    "end": "3642910"
  },
  {
    "text": "That's your modeling choice. And it turns out that if\nyou learn it from data, it will be exactly the\nfraction of positive examples",
    "start": "3642910",
    "end": "3650230"
  },
  {
    "text": "in the data. So this is the most likely phi\nthat can join with your data. Which is exactly\nthe same fraction",
    "start": "3650230",
    "end": "3656369"
  },
  {
    "text": "as in the empirical data. And of course, 1 minus\nv will be the fraction",
    "start": "3656369",
    "end": "3667369"
  },
  {
    "text": "of negative examples\nin the data. Question.",
    "start": "3667369",
    "end": "3673099"
  },
  {
    "text": "A little bit bigger. OK. Thanks. How do I remember this?",
    "start": "3673099",
    "end": "3679710"
  },
  {
    "text": "I have to burn this in my head. So 1 minus phi is equal to--",
    "start": "3679710",
    "end": "3684920"
  },
  {
    "text": "this is the fraction\nof active examples. [INAUDIBLE],, it's just,\nI can't see it from here.",
    "start": "3684920",
    "end": "3694400"
  },
  {
    "text": "You said 1 is equal to what? Mu sub 1. Mu sub 1 is a set.",
    "start": "3694400",
    "end": "3700270"
  },
  {
    "text": "So the set contains\nall the indices. Such that y1 is equal to 1.",
    "start": "3700270",
    "end": "3707460"
  },
  {
    "text": "So this is a indices\nof positive examples.",
    "start": "3707460",
    "end": "3713890"
  },
  {
    "text": "So for example, suppose\nyou have here, right?",
    "start": "3713890",
    "end": "3720660"
  },
  {
    "text": "So this will be the set U1. This will be the set U0. And how do you\ndecide what is phi?",
    "start": "3720660",
    "end": "3727549"
  },
  {
    "text": "The maximum\nlikelihood of phi will be just you count how\nmany examples in total.",
    "start": "3727549",
    "end": "3733720"
  },
  {
    "text": "maybe 10 examples. And you say, four of\nthem are positive.",
    "start": "3733720",
    "end": "3740279"
  },
  {
    "text": "So that's why phi\nis going to be--",
    "start": "3740279",
    "end": "3745450"
  },
  {
    "text": "so in this case phi will\nbe 4 over the total number",
    "start": "3745450",
    "end": "3753220"
  },
  {
    "text": "of examples, which is 10.4. For some reason, I'm going to\nwrite this phi as the follows.",
    "start": "3753220",
    "end": "3763440"
  },
  {
    "text": "I'm going to write phi also\njust to-- this is mostly just for mathematical cosmetically. You want it to make a look a\nlittle bit nicer in some sense or more consistent with\nthe other equations I'm",
    "start": "3763440",
    "end": "3768990"
  },
  {
    "text": "going to write next. So you can also write\nthis as the following.",
    "start": "3768990",
    "end": "3778940"
  },
  {
    "text": "Let me expand this notation. So this is so-called\nan indicator function. So indicator function.",
    "start": "3778940",
    "end": "3784180"
  },
  {
    "text": "I'm going to write it as 1 of\nE. I think in the homework,",
    "start": "3784180",
    "end": "3789470"
  },
  {
    "text": "we write it like this. Different people have\ndifferent types of brackets but it's the same\nas honestly defined.",
    "start": "3789470",
    "end": "3796770"
  },
  {
    "text": "So 1 E, this is equals to-- this is the so-called indicator\nfunction for the event E.",
    "start": "3796770",
    "end": "3806288"
  },
  {
    "text": "So it's equal to 1 if E happens.",
    "start": "3806289",
    "end": "3811700"
  },
  {
    "text": "And it equals to 0 otherwise.",
    "start": "3811700",
    "end": "3817650"
  },
  {
    "text": "So let me check whether this-- so in this case, where so this\nindicator of y goes to 1 just",
    "start": "3817650",
    "end": "3823740"
  },
  {
    "text": "means that if y is equal\nto 1, is equal to 1. This indicator of\nthat is equal to 1. Otherwise, it's going\nto be equal to 0.",
    "start": "3823740",
    "end": "3830500"
  },
  {
    "text": "So basically, this\nindicator is only 1 when the label is positive.",
    "start": "3830500",
    "end": "3835910"
  },
  {
    "text": "And I'm taking the\nsum over all examples. So that's why I'm basically\ncounting how many examples",
    "start": "3835910",
    "end": "3841980"
  },
  {
    "text": "satisfy yi is equal to 1. So that's why this whole\nthing is just equals to U1.",
    "start": "3841980",
    "end": "3848190"
  },
  {
    "text": "It's probably useful to\nunderstand this notation because I'm going\nto have a little bit",
    "start": "3848190",
    "end": "3853501"
  },
  {
    "text": "more complicated\nformulas than this. So this is a warmup\nin some sense.",
    "start": "3853501",
    "end": "3860039"
  },
  {
    "text": "Go. [INAUDIBLE] This is just a-- I guess-- in my mind,\nthis is capital U. Which",
    "start": "3860039",
    "end": "3869730"
  },
  {
    "text": "is not the Greek letter mu. I guess the way we do it.",
    "start": "3869730",
    "end": "3877450"
  },
  {
    "text": "The handwriting is\nnot that obvious. Yeah. But they are they're\ncomplete differences. This is a set,\nanother parameter.",
    "start": "3877450",
    "end": "3886460"
  },
  {
    "text": "No relationship at all. [INAUDIBLE]",
    "start": "3886460",
    "end": "3891890"
  },
  {
    "text": "Why do I? Sorry, I didn't hear.",
    "start": "3891890",
    "end": "3899500"
  },
  {
    "text": "[INAUDIBLE] Oh. Let's-- oh, I see. I see.",
    "start": "3899500",
    "end": "3905260"
  },
  {
    "text": "So that's a good question. Yeah. Thanks for that. So the absolute value is-- maybe I should define this.",
    "start": "3905260",
    "end": "3910850"
  },
  {
    "text": "So this is the site. So when you have a set,\nI'm using this as the size",
    "start": "3910850",
    "end": "3917950"
  },
  {
    "text": "or the cardinality of the site. Like, how many items\nare in the set.",
    "start": "3917950",
    "end": "3929230"
  },
  {
    "text": "That's just a notation. Yeah. Yeah, that's a good catch. Maybe I should take\na note on this.",
    "start": "3929230",
    "end": "3934920"
  },
  {
    "text": "I was asked of this\nthe last time as well.",
    "start": "3934920",
    "end": "3946760"
  },
  {
    "text": "OK. Cool. So I'm going to continue\nwith the telling you the solution of this MLE.",
    "start": "3946760",
    "end": "3963930"
  },
  {
    "text": "Mu0, this is mu.",
    "start": "3963930",
    "end": "3976480"
  },
  {
    "text": "Not the U. This is the\nMLE for the parameter mu0. This is equals to 1 over.",
    "start": "3976480",
    "end": "3986609"
  },
  {
    "text": "This is U0, the number\nof negative examples, times the sum of all\nthe xi's in the set U0.",
    "start": "3986609",
    "end": "4005829"
  },
  {
    "text": "So this is the sum of\nall the positive examples and I'm taking some\nof the input vectors,",
    "start": "4005830",
    "end": "4012390"
  },
  {
    "text": "the feature vectors xy. So basically, this is just the\naverage, the empirical average,",
    "start": "4012390",
    "end": "4024578"
  },
  {
    "text": "of xi's off on inactive xi's.",
    "start": "4024579",
    "end": "4030609"
  },
  {
    "text": "So I'm looking at all\nthe negative examples. I'm going to take the\nempirical average of the xi's. And that turns out to\nbe the best estimate",
    "start": "4030609",
    "end": "4037059"
  },
  {
    "text": "for the means of that class. Empirical average?",
    "start": "4037059",
    "end": "4044079"
  },
  {
    "text": "Does the word empirical mean\ndifferent types of average?",
    "start": "4044080",
    "end": "4050380"
  },
  {
    "text": "Oh. I see. I see. Yeah. Good question. So it doesn't mean anything.",
    "start": "4050380",
    "end": "4056380"
  },
  {
    "text": "Empirical average is\nthe same as average. Yeah. There is-- just think\nof it as the average.",
    "start": "4056380",
    "end": "4063328"
  },
  {
    "text": "There is some reason why\nI use that just because in some other cases,\nsometimes you--",
    "start": "4063329",
    "end": "4070359"
  },
  {
    "text": "don't worry about it. Sorry. OK. So I guess these are the\nresults you would get off",
    "start": "4070359",
    "end": "4080450"
  },
  {
    "text": "of solving [INAUDIBLE]. Right. Right.",
    "start": "4080450",
    "end": "4086230"
  },
  {
    "text": "Exactly. Exactly. And now, I'm just\ntelling you the answer.",
    "start": "4086230",
    "end": "4094569"
  },
  {
    "text": "But this sounds very\nintuitive, right? So what would you\nguess what's the best",
    "start": "4094569",
    "end": "4101420"
  },
  {
    "text": "meaning for this class? Probably you should just use\nthe average of all the examples. I would.",
    "start": "4101420",
    "end": "4107370"
  },
  {
    "text": "At least you know. If you see it, it sounds\nsomewhat reasonable.",
    "start": "4107370",
    "end": "4112698"
  },
  {
    "text": "And you can guess,\njust because these are symmetric from the mu1.",
    "start": "4112699",
    "end": "4119240"
  },
  {
    "text": "It's the same thing. You're going to have 1 over the\nsize of the positive examples",
    "start": "4119240",
    "end": "4125199"
  },
  {
    "text": "times xi times the\nsum of the-- right.",
    "start": "4125199",
    "end": "4133130"
  },
  {
    "text": "So this is the average\nof positive xi's.",
    "start": "4133130",
    "end": "4146758"
  },
  {
    "text": "We're going to write this as-- so, I'm going to use\nthis indicator function to write them in a\nslightly different way.",
    "start": "4146759",
    "end": "4152940"
  },
  {
    "text": "So I'm continuing here. So if you look at this formula,\nyou can write this as U0.",
    "start": "4152940",
    "end": "4159940"
  },
  {
    "text": "The size is equal to, as we\nargued, the indicator of yi is equal to 0.",
    "start": "4159940",
    "end": "4165428"
  },
  {
    "text": "This is the number of examples\nwhere yi is equal to 0.",
    "start": "4165429",
    "end": "4172210"
  },
  {
    "text": "Because y is equal 0 means\nthe indicator is equal to 1. That's what the\nindicator is saying.",
    "start": "4172210",
    "end": "4177639"
  },
  {
    "text": "Indicator saying is,\nif the indicator is y only if the event is happening. So that's why this is\none where y is zero.",
    "start": "4177640",
    "end": "4185450"
  },
  {
    "text": "So that's why this\ndenominator is the same as the size of\nthe negative examples.",
    "start": "4185450",
    "end": "4191380"
  },
  {
    "text": "And then, the numerator can\nbe written as is equal zero.",
    "start": "4191380",
    "end": "4202469"
  },
  {
    "text": "So you first have this\nindicator only selecting those examples that are negative\nand then you take multiple xi.",
    "start": "4202469",
    "end": "4213370"
  },
  {
    "text": "And for the second part. For the mu1, it's the same. You just replace 0 by 1.",
    "start": "4213370",
    "end": "4221660"
  },
  {
    "text": "So you have. And you have. You select all the positive\nexamples and you multiply xi.",
    "start": "4221660",
    "end": "4232340"
  },
  {
    "text": "So why I'm writing it like this?",
    "start": "4232340",
    "end": "4243050"
  },
  {
    "text": "One reason is that it looks\na little bit more systematic.",
    "start": "4243050",
    "end": "4248770"
  },
  {
    "text": "I'm not sure whether you agree\nwith that and maybe you don't. Another thing is\nthat I think you",
    "start": "4248770",
    "end": "4254619"
  },
  {
    "text": "see this kind of formulas in\nprint often for other cases as well. So it's probably\ngood to unify them in some ways in some sense.",
    "start": "4254620",
    "end": "4261180"
  },
  {
    "text": "But you don't have to\nremember any of this. I think this way is the best\nway to remember them and kind",
    "start": "4261180",
    "end": "4267140"
  },
  {
    "text": "of interpret them. So just treat this\nas cosmetic changes. Some cosmetic changes\nof the former.",
    "start": "4267140",
    "end": "4276860"
  },
  {
    "text": "Next, I'm going to have sigma.",
    "start": "4276860",
    "end": "4296678"
  },
  {
    "text": "So the solution for the\nMLE for sigma is like this.",
    "start": "4296679",
    "end": "4303090"
  },
  {
    "text": "It may sound a little\nbit complicated.",
    "start": "4303090",
    "end": "4313940"
  },
  {
    "text": "OK. So let me try to--",
    "start": "4313940",
    "end": "4324020"
  },
  {
    "text": "What is this mu? This mu is the mu we\nhave completed above. So you have to use\nthe mu you complete above to complete the sigma.",
    "start": "4324020",
    "end": "4330829"
  },
  {
    "text": "So these are the mus\nyou computed above. A mu yi could be mu1 or\nmu0 depending on what's y and what's y.",
    "start": "4330830",
    "end": "4337969"
  },
  {
    "text": "And one way to interpret\nthis is that you just look at",
    "start": "4337969",
    "end": "4343310"
  },
  {
    "text": "can expand the sum\ninto two cases. One case is that y is 0. And the other case is y is 1.",
    "start": "4343310",
    "end": "4350150"
  },
  {
    "text": "So when y is 0, so you have--",
    "start": "4350150",
    "end": "4356760"
  },
  {
    "text": "so those are the i's\nthat are in the set U0. And this is xi minus mu0\ntimes xi minus mu0 transpose.",
    "start": "4356760",
    "end": "4371680"
  },
  {
    "text": "And then you have.",
    "start": "4371680",
    "end": "4384690"
  },
  {
    "text": "You look at those cases\nwhere yi is equal to 1. And then this mu yi becomes\nmu1 and you've got this.",
    "start": "4384690",
    "end": "4393519"
  },
  {
    "text": "So this makes it a little\nbit easier to interpret. Because this-- if you\ncompare this with--",
    "start": "4393519",
    "end": "4399510"
  },
  {
    "text": "this is kind of the covariance. But the covariance evaluated\non the empirical data. So this is the-- oh, sorry\non the data set you have.",
    "start": "4399510",
    "end": "4408070"
  },
  {
    "text": "Empirical. Empirical is a word that's used\nto stress that you are seeing the data as a sample data.",
    "start": "4408070",
    "end": "4414449"
  },
  {
    "text": "So that's-- I kept using that. But we don't have\nto use that word. So this is the covariance.",
    "start": "4414450",
    "end": "4420460"
  },
  {
    "text": "Covariance of xi's for\nthose xi's in the site U0",
    "start": "4420460",
    "end": "4425910"
  },
  {
    "text": "for those negative examples. So this is the covariance\noften active examples.",
    "start": "4425910",
    "end": "4436010"
  },
  {
    "text": "And this is the covariance\nof the positive examples.",
    "start": "4436010",
    "end": "4442079"
  },
  {
    "text": "And it turns out that the\naverage of them, in this sense,",
    "start": "4442080",
    "end": "4448640"
  },
  {
    "text": "is the best gas for sigma. It's the sigma that gives\nyou the maximum likelihood. So I've got all of this\nparameters so far, right?",
    "start": "4448640",
    "end": "4465550"
  },
  {
    "text": "And now, you're going to ask\nto prove all of this are true. But suppose we already got\nall of these parameters,",
    "start": "4465550",
    "end": "4470810"
  },
  {
    "text": "we can compute them in\nnumerically by plugging this formula because you just\nplug in all the xi's. You have all of the data.",
    "start": "4470810",
    "end": "4476949"
  },
  {
    "text": "You plug in. You get all of these parameters. So you've got all\nthe parameters. So that's the so-called\nlearning process.",
    "start": "4476949",
    "end": "4482620"
  },
  {
    "text": "You learn the parameters. And now, the next question is,\nhow do you make predictions on a new example, right?",
    "start": "4482620",
    "end": "4488330"
  },
  {
    "text": "You got the parameters, how\ndo you make predictions? [INAUDIBLE] did you assume\nsigma is different for y plus 0",
    "start": "4488330",
    "end": "4495100"
  },
  {
    "text": "and y plus 1? Do you think the average\nof covariants [INAUDIBLE]??",
    "start": "4495100",
    "end": "4501349"
  },
  {
    "text": "So if you assume\nthey are different, I don't think the\nformula will be this. And actually, if\nthey are different,",
    "start": "4501350",
    "end": "4506949"
  },
  {
    "text": "you don't even have\nan analytical form for the solution of the analogy.",
    "start": "4506949",
    "end": "4514330"
  },
  {
    "text": "You cannot solve\nit analytically. So here, it's kind of\nlike, for some reason,",
    "start": "4514330",
    "end": "4519469"
  },
  {
    "text": "because we are making all of\nthis simplifying assumption, you can solve the\nmaximizer of the MLE.",
    "start": "4519469",
    "end": "4524670"
  },
  {
    "text": "But actually, it's\nnot always the case. You can write analytically. And when sigma are different\nfor the two sub-population,",
    "start": "4524670",
    "end": "4532559"
  },
  {
    "text": "you don't have that\nanalytical solution.",
    "start": "4532560",
    "end": "4537870"
  },
  {
    "text": "So now we are talking\nabout prediction.",
    "start": "4537870",
    "end": "4560770"
  },
  {
    "text": "Given x, you want\nto output some y.",
    "start": "4560770",
    "end": "4572469"
  },
  {
    "text": "You want to understand what\nit is that benign cancer does. That's your final goal. That's the final goal\nof the [INAUDIBLE]..",
    "start": "4572469",
    "end": "4579490"
  },
  {
    "text": "And the way that we\ndo it is that you say, I'm going to output\nthe most likely one.",
    "start": "4579490",
    "end": "4586870"
  },
  {
    "text": "So I'm going to output argmax.",
    "start": "4586870",
    "end": "4592470"
  },
  {
    "text": "The maximizer of p of y given x. And the parameter phi,\nmu0, mu1, and sigma.",
    "start": "4592470",
    "end": "4602739"
  },
  {
    "text": "And note here that these are\nthe solutions of the MLE.",
    "start": "4602739",
    "end": "4614230"
  },
  {
    "text": "So these are not\narbitrary parameters. So in some sense,\nyou can even say I'm abusing notation a little\nbit just for simplicity.",
    "start": "4614230",
    "end": "4620350"
  },
  {
    "text": "So here, this phi,\nmu1, mu0, sigma are those solutions that are\ncomputed from this formula.",
    "start": "4620350",
    "end": "4627310"
  },
  {
    "text": "Go ahead. [INAUDIBLE] In where?",
    "start": "4627310",
    "end": "4635080"
  },
  {
    "text": "In which case? In here?",
    "start": "4635080",
    "end": "4642849"
  },
  {
    "text": "[INAUDIBLE] This is a matrix. This is a vector. This is a vector.",
    "start": "4642850",
    "end": "4648570"
  },
  {
    "text": "Mu0 is a vector. Mu0 is the mean of the Gaussian. It's a D dimensional vector.",
    "start": "4648570",
    "end": "4656990"
  },
  {
    "text": "So did I say something here? I guess that you\nraised it, right? So you assume y given x\nand y is from y maybe zero.",
    "start": "4656990",
    "end": "4668040"
  },
  {
    "text": "This is from Gaussian\nwith mu0 and sigma. So mu 0 is a D\ndimensional vector.",
    "start": "4668040",
    "end": "4674640"
  },
  {
    "text": "Sigma is a matrix. [INAUDIBLE] No, that's phi.",
    "start": "4674640",
    "end": "4684909"
  },
  {
    "text": "Phi is a scalar. It's the probability\nof y is equal to 1.",
    "start": "4684909",
    "end": "4690700"
  },
  {
    "text": "And mu0 is the mean of the-- mu0 is the mean of x given y 0.",
    "start": "4690700",
    "end": "4698030"
  },
  {
    "text": "And mu1 is the mean\nof x give y is 1.",
    "start": "4698030",
    "end": "4704539"
  },
  {
    "text": "[INAUDIBLE] Yep. OK. Cool. Back to here.",
    "start": "4704540",
    "end": "4712340"
  },
  {
    "text": "So this is my methodology. I'm going to take the MLE. So how do I compute this? So it turns out that this--",
    "start": "4712340",
    "end": "4718659"
  },
  {
    "text": "of course, I have to use the\nBayes' rule to get y given x. Because I only know x given y. I only know y.",
    "start": "4718659",
    "end": "4723678"
  },
  {
    "text": "But I don't know\nwhat is y given x. So one thing is I have\nto use Bayes' rule. So let me do the\nBayes' rule for you",
    "start": "4723679",
    "end": "4730310"
  },
  {
    "text": "and it's actually simpler\nthan you may think. So because here you are\nmaximizing over y, right?",
    "start": "4730310",
    "end": "4737199"
  },
  {
    "text": "You are trying to output\nwhich y is more likely, right? Well, it's more likely to\nbe benign cancer or not.",
    "start": "4737200",
    "end": "4742760"
  },
  {
    "text": "So basically, this\nmaximization problem we just have two choices. We are just maximizing\nover 2 possible choices.",
    "start": "4742760",
    "end": "4748599"
  },
  {
    "text": "So you are just taking the\nargmax of the two quantities.",
    "start": "4748600",
    "end": "4761719"
  },
  {
    "text": "The two scalars. Yeah, both possibilities.",
    "start": "4761719",
    "end": "4769678"
  },
  {
    "text": "And this one. So you just care about\nthese two scalars",
    "start": "4769679",
    "end": "4779699"
  },
  {
    "text": "and which one is bigger. And it turns out that these\ntwo scalars, their sum is 1.",
    "start": "4779699",
    "end": "4785250"
  },
  {
    "text": "Because given x, y\ncan only be 0 or 1.",
    "start": "4785250",
    "end": "4790730"
  },
  {
    "text": "So maybe let's suppose just\nfor the sake of simplicity. So suppose you call this\na and you call this p.",
    "start": "4790730",
    "end": "4796239"
  },
  {
    "text": "Then a plus b is equal to 1. And you care about\nwhich one is bigger.",
    "start": "4796239",
    "end": "4803290"
  },
  {
    "text": "Whether the A's\nbigger or B is bigger. So if you have A plus\nB equals 1 and you",
    "start": "4803290",
    "end": "4810350"
  },
  {
    "text": "are taking max of A and B, then\nwhat does this really mean? It really means\nthat you are asking",
    "start": "4810350",
    "end": "4816350"
  },
  {
    "text": "whether A is bigger than Because you're going to choose\nA if A is bigger than 0.5.",
    "start": "4816350",
    "end": "4827150"
  },
  {
    "text": "Because if A is bigger than 0.5,\nthat means B is less than 0.5. So that's why you choose\nA. And you only choose B--",
    "start": "4827150",
    "end": "4838270"
  },
  {
    "text": "maybe let's write this again.",
    "start": "4838270",
    "end": "4844510"
  },
  {
    "text": "Sorry. If A is bigger than 0.5, because\nthat means B is less than 0.5.",
    "start": "4844510",
    "end": "4852510"
  },
  {
    "text": "And it's going to be equals\nB if A is less than 0.5. Because that also implies\nB is bigger than 0.5.",
    "start": "4852510",
    "end": "4861420"
  },
  {
    "text": "So basically, the question\nis that you just care about whether A is bigger 0.5 or not.",
    "start": "4861420",
    "end": "4866620"
  },
  {
    "text": "So going back to this. I'm doing an abstract thing. So if you're going back\nto this, then it really just means that this\nargmax is equals to 1.",
    "start": "4866620",
    "end": "4878260"
  },
  {
    "text": "Y is equal to 1 if\nthe probability of y is equal to 1 given x and the\nparameters is bigger than 0.5.",
    "start": "4878260",
    "end": "4886370"
  },
  {
    "text": "Sorry, my-- there's\na little bit--",
    "start": "4886370",
    "end": "4891409"
  },
  {
    "text": "and limited space. And it's 0 if p of y\ngiven y is 1 given x,",
    "start": "4891409",
    "end": "4899970"
  },
  {
    "text": "the parameter is less than 0.5.",
    "start": "4899970",
    "end": "4922330"
  },
  {
    "text": "Which also makes\nsense because y-- Oh, my bad. Right.",
    "start": "4922330",
    "end": "4927380"
  },
  {
    "text": "So this also makes sense. Because basically this is saying\nthat if the probability of y is 1 is larger,\nthen you choose y.",
    "start": "4927380",
    "end": "4935110"
  },
  {
    "text": "You choose 1. Otherwise, you choose 0. That's it. I just mathematically\nderived that for you.",
    "start": "4935110",
    "end": "4942369"
  },
  {
    "text": "That's it.",
    "start": "4942370",
    "end": "4949560"
  },
  {
    "text": "And if you look at this figure,\nso what's the final decision? What's the final kind of\nboundary between these two",
    "start": "4949560",
    "end": "4956179"
  },
  {
    "text": "cases? This one will be the family\nof x such that this p of y",
    "start": "4956179",
    "end": "4965900"
  },
  {
    "text": "equals to y given x is\nequal to exactly 0.5. So if you define\nthis family of x's.",
    "start": "4965900",
    "end": "4972600"
  },
  {
    "text": "This is a family of x's\nsuch that the y given x",
    "start": "4972600",
    "end": "4979560"
  },
  {
    "text": "is equal to exactly 0.5. And this is called\nthe decision boundary.",
    "start": "4979560",
    "end": "4994409"
  },
  {
    "text": "On one side of the\nboundary, y1 is more likely. On the other side of the\nboundary, y0 is more likely.",
    "start": "4994409",
    "end": "4999730"
  },
  {
    "text": "And the boundary, you just do\nsome arbitrary type grouping or you just randomly output.",
    "start": "4999730",
    "end": "5006440"
  },
  {
    "text": "The boundary wouldn't be\nvery likely to show up. It's very unlikely that\nyour point will be exactly on the boundary.",
    "start": "5006440",
    "end": "5011910"
  },
  {
    "text": "So it doesn't matter that much.",
    "start": "5011910",
    "end": "5023330"
  },
  {
    "text": "So maybe let me just-- maybe let me just-- quickly. Because we only have\ntwo minutes left,",
    "start": "5023330",
    "end": "5028659"
  },
  {
    "text": "let me just quickly\nsay what this decision boundary is for the Gaussian\ndiscriminant analysis.",
    "start": "5028659",
    "end": "5035389"
  },
  {
    "text": "Because here, what\nI'm doing here is pretty general in sometimes. You can see, right? I didn't really talk about what\nexactly the parameters are.",
    "start": "5035389",
    "end": "5041980"
  },
  {
    "text": "And if you really want to know\nit know what this p of y given x is but you have to\nplug in the parameters,",
    "start": "5041980",
    "end": "5048070"
  },
  {
    "text": "you have to use the\nmodeling, right? For Gaussian discriminant\nanalysis, if you plug in,",
    "start": "5048070",
    "end": "5058620"
  },
  {
    "text": "so what you do is\nyou're following. So you do p of y is\nequal to y given x. This is the thing you\nreally care about.",
    "start": "5058620",
    "end": "5071350"
  },
  {
    "text": "So you use Bayes' rule. So you say that this\nis equal to p of x.",
    "start": "5071350",
    "end": "5079160"
  },
  {
    "text": "y is 1. Here, you only depend on mu1\nsigma and you'd have p of y",
    "start": "5079160",
    "end": "5087880"
  },
  {
    "text": "is one given phi. And you divide this by p of x.",
    "start": "5087880",
    "end": "5097630"
  },
  {
    "text": "The probability of x. This is the Bayes' rule\nthat we kind of alluded to",
    "start": "5097630",
    "end": "5104450"
  },
  {
    "text": "in the beginning of the lecture. And then you do a\nlot of calculation.",
    "start": "5104450",
    "end": "5109658"
  },
  {
    "text": "Which I think this is homework. You do a lot of calculation.",
    "start": "5109659",
    "end": "5117040"
  },
  {
    "text": "And what you'll find out\nis that actually this has a relatively simple form. So the form looks like 1 over",
    "start": "5117040",
    "end": "5129698"
  },
  {
    "text": "transpose x plus theta 0.",
    "start": "5129699",
    "end": "5135210"
  },
  {
    "text": "Where theta is something of rd. Says 0 is a scalar.",
    "start": "5135210",
    "end": "5142930"
  },
  {
    "text": "So they are functions. They depend on-- let\nme just simply say,",
    "start": "5142930",
    "end": "5151659"
  },
  {
    "text": "these are depend on the\nphi, mu0, mu1, and sigma.",
    "start": "5151659",
    "end": "5164440"
  },
  {
    "text": "So basically,\neventually, you get all of these\nparameters and then you use this parameter to\ncompute theta and theta 0",
    "start": "5164440",
    "end": "5171449"
  },
  {
    "text": "and then you get y given x. And that's your probability. And then, you're on the\ncomputer decision boundary.",
    "start": "5171449",
    "end": "5177980"
  },
  {
    "text": "Let me also do that real quick. Sorry, we're running\na little bit late.",
    "start": "5177980",
    "end": "5186520"
  },
  {
    "text": "So then, you got a\ndecision boundary. So what's the decision boundary? The decision boundary is\nwhen this is equal to 0.5.",
    "start": "5186520",
    "end": "5202030"
  },
  {
    "text": "And when this is\nequal to 0.5 is when this exponential is equal to 1. Then you have 1 over 2 is 0.5.",
    "start": "5202030",
    "end": "5208520"
  },
  {
    "text": "So when the exponential\nis equal to 1,",
    "start": "5208520",
    "end": "5215159"
  },
  {
    "text": "this means theta transpose x-- I think. Sorry. Let me see. I think I need to have\na parenthesis here.",
    "start": "5215159",
    "end": "5225020"
  },
  {
    "text": "But only because\nI'm abstract here, it doesn't really\nmatter that much. It means this is equal to 0.",
    "start": "5225020",
    "end": "5237070"
  },
  {
    "text": "And you will see that\np of y is equal to 1 given x is larger than 0.5 is\nthe same as theta transpose x",
    "start": "5237070",
    "end": "5248460"
  },
  {
    "text": "plus theta 0 is larger than\nmaybe you say larger than 0.",
    "start": "5248460",
    "end": "5257120"
  },
  {
    "text": "So basically, you have\na linear function of x. That's our decision boundary. That's why I keep\ndrawing a line here.",
    "start": "5257120",
    "end": "5264410"
  },
  {
    "text": "From here, from the\nprinciple, you never know why this is a line, right? The principle says that maybe\nthis is some formula of x.",
    "start": "5264410",
    "end": "5272138"
  },
  {
    "text": "Let's separate this. But the derivation tells us\nthat at least for this case,",
    "start": "5272139",
    "end": "5277400"
  },
  {
    "text": "the decision boundary is\na linear function of x. [INAUDIBLE] vector [INAUDIBLE]?",
    "start": "5277400",
    "end": "5284179"
  },
  {
    "text": "So I think I'm\ngoing to have this. Sorry. This is a typo here. I should have the parentheses. Yeah.",
    "start": "5284179",
    "end": "5289719"
  },
  {
    "text": "I know with the signs\nare not that important.",
    "start": "5289719",
    "end": "5298060"
  },
  {
    "text": "I think-- yeah, we are-- maybe you can see the\nbest way you can come to me to ask the questions.",
    "start": "5298060",
    "end": "5303989"
  },
  {
    "text": "I can stay here. Is that the best way? Maybe. Yeah. OK.",
    "start": "5303989",
    "end": "5308641"
  }
]