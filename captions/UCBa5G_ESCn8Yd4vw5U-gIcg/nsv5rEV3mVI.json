[
  {
    "start": "0",
    "end": "340000"
  },
  {
    "text": "So welcome back.",
    "start": "0",
    "end": "1820"
  },
  {
    "text": "Today, we're going to\ncover model selection",
    "start": "1820",
    "end": "3860"
  },
  {
    "text": "and regularization.",
    "start": "3860",
    "end": "4797"
  },
  {
    "text": "But we have a special guest,\nmy former graduate student,",
    "start": "4797",
    "end": "7130"
  },
  {
    "text": "Daniella Witten.",
    "start": "7130",
    "end": "8130"
  },
  {
    "text": "Hi.",
    "start": "8130",
    "end": "8630"
  },
  {
    "text": "Welcome, Daniella.",
    "start": "8630",
    "end": "9380"
  },
  {
    "text": "Thank you.",
    "start": "9380",
    "end": "10317"
  },
  {
    "text": "Daniella is now at\nUniversity of Washington.",
    "start": "10317",
    "end": "12150"
  },
  {
    "text": "But maybe you want to tell\nstudents a bit about yourself",
    "start": "12150",
    "end": "14900"
  },
  {
    "text": "and how you got to be here.",
    "start": "14900",
    "end": "16049"
  },
  {
    "text": "Yeah.",
    "start": "16050",
    "end": "16550"
  },
  {
    "text": "Well, in college, I\nstudied math and biology.",
    "start": "16550",
    "end": "20119"
  },
  {
    "text": "And when I was\ngraduating, I knew",
    "start": "20120",
    "end": "21860"
  },
  {
    "text": "I wanted to go to grad\nschool in something.",
    "start": "21860",
    "end": "23930"
  },
  {
    "text": "But I couldn't really decide\non one particular thing",
    "start": "23930",
    "end": "26600"
  },
  {
    "text": "that I wanted to study\nfor the rest of my life.",
    "start": "26600",
    "end": "28920"
  },
  {
    "text": "And so I ended up doing\na PhD in statistics.",
    "start": "28920",
    "end": "31340"
  },
  {
    "text": "And I was lucky enough to\ndo it at Stanford with Rob.",
    "start": "31340",
    "end": "34580"
  },
  {
    "text": "And I graduated in 2010,\nmoved up to Seattle,",
    "start": "34580",
    "end": "38210"
  },
  {
    "text": "and I'm now an\nassistant professor",
    "start": "38210",
    "end": "39710"
  },
  {
    "text": "at the University of Washington\nin the Biostat Department there.",
    "start": "39710",
    "end": "43790"
  },
  {
    "text": "And I didn't invite\nDaniella here",
    "start": "43790",
    "end": "46280"
  },
  {
    "text": "just because she's a\ngreat student and friend.",
    "start": "46280",
    "end": "48500"
  },
  {
    "text": "But also she's a\nco-author of the textbook",
    "start": "48500",
    "end": "50310"
  },
  {
    "text": "on this course is based.",
    "start": "50310",
    "end": "52080"
  },
  {
    "text": "So Danielle and I are going\nto give today's talk together.",
    "start": "52080",
    "end": "55940"
  },
  {
    "text": "So I'm going to talk about model\nselection and regularization.",
    "start": "55940",
    "end": "59670"
  },
  {
    "text": "So let's recall the\nlinear model we've talked",
    "start": "59670",
    "end": "62773"
  },
  {
    "text": "about already in the course.",
    "start": "62773",
    "end": "63940"
  },
  {
    "text": "We have a response variable y.",
    "start": "63940",
    "end": "65239"
  },
  {
    "text": "And we're going to model it as\na function of some predictors",
    "start": "65240",
    "end": "67740"
  },
  {
    "text": "or features, X1 through Xp.",
    "start": "67740",
    "end": "70049"
  },
  {
    "text": "And we've talked\nabout least squares",
    "start": "70050",
    "end": "71760"
  },
  {
    "text": "for doing that\nearlier in the course.",
    "start": "71760",
    "end": "74310"
  },
  {
    "text": "And later on in\nthe course, we'll",
    "start": "74310",
    "end": "76487"
  },
  {
    "text": "talk about ways of making\nthat model more general,",
    "start": "76487",
    "end": "78570"
  },
  {
    "text": "making it nonlinear,\nfor example.",
    "start": "78570",
    "end": "81000"
  },
  {
    "text": "We'll have additive\nbut non-linear models",
    "start": "81000",
    "end": "85850"
  },
  {
    "text": "in the lectures that\ncover chapter 7.",
    "start": "85850",
    "end": "88700"
  },
  {
    "text": "We'll consider nonlinear\nmodels in chapter 8,",
    "start": "88700",
    "end": "90930"
  },
  {
    "text": "things like trees and boosting.",
    "start": "90930",
    "end": "92440"
  },
  {
    "text": "But today, actually we're going\nto stick to the linear model",
    "start": "92440",
    "end": "94940"
  },
  {
    "text": "and talk about a different way\nof fitting the linear model.",
    "start": "94940",
    "end": "97398"
  },
  {
    "text": "Why?",
    "start": "97398",
    "end": "98000"
  },
  {
    "text": "Well, because the model,\nalthough it's very simple,",
    "start": "98000",
    "end": "100130"
  },
  {
    "text": "and it's actually been around\nsince probably the 1920s",
    "start": "100130",
    "end": "102380"
  },
  {
    "text": "and 1930s, it's a very important\nmodel because it's simple.",
    "start": "102380",
    "end": "106970"
  },
  {
    "text": "Which means it can\nbe interpreted.",
    "start": "106970",
    "end": "108757"
  },
  {
    "text": "There's a small\nnumber of coefficients",
    "start": "108757",
    "end": "110340"
  },
  {
    "text": "typically, if we\nhave a small number",
    "start": "110340",
    "end": "111840"
  },
  {
    "text": "of features that are important.",
    "start": "111840",
    "end": "113250"
  },
  {
    "text": "And also, it predicts\nfuture data quite well",
    "start": "113250",
    "end": "116840"
  },
  {
    "text": "in a lot of cases, despite\nthe fact it's simple.",
    "start": "116840",
    "end": "119270"
  },
  {
    "text": "So we want to talk today about\nways of fitting the linear model",
    "start": "119270",
    "end": "123109"
  },
  {
    "text": "that improving on least squares\nby selecting or shrinking",
    "start": "123110",
    "end": "126530"
  },
  {
    "text": "the coefficients\nof features to make",
    "start": "126530",
    "end": "128750"
  },
  {
    "text": "the model more interpretable,\nand in some cases,",
    "start": "128750",
    "end": "131210"
  },
  {
    "text": "to predict better.",
    "start": "131210",
    "end": "132360"
  },
  {
    "text": "So we'll talk about a number\nof methods of doing that today.",
    "start": "132360",
    "end": "136670"
  },
  {
    "text": "And we'll just say a little\nmore about the two objectives.",
    "start": "136670",
    "end": "140569"
  },
  {
    "text": "When the number of\nfeatures is bigger",
    "start": "140570",
    "end": "142370"
  },
  {
    "text": "than the number of\nsamples-- and we've",
    "start": "142370",
    "end": "143610"
  },
  {
    "text": "talked about that in\nthe course already.",
    "start": "143610",
    "end": "144910"
  },
  {
    "text": "This is a situation that\ncomes up more and more often",
    "start": "144910",
    "end": "146730"
  },
  {
    "text": "these days, where we\nhave a lot of features",
    "start": "146730",
    "end": "148522"
  },
  {
    "text": "you measure on patients or\nin business, maybe on a stock",
    "start": "148522",
    "end": "151680"
  },
  {
    "text": "or in other situations.",
    "start": "151680",
    "end": "155849"
  },
  {
    "text": "It's cheap to\nmeasure things now.",
    "start": "155850",
    "end": "157560"
  },
  {
    "text": "And it's often the\ncase when that p",
    "start": "157560",
    "end": "160050"
  },
  {
    "text": "might be much bigger than\nthe n, the number of samples.",
    "start": "160050",
    "end": "162412"
  },
  {
    "text": "So in that situation, of course,\nwe can't use full least squares.",
    "start": "162412",
    "end": "165120"
  },
  {
    "text": "Because the solutions\nare not even defined.",
    "start": "165120",
    "end": "167400"
  },
  {
    "text": "So somehow, we have to reduce\nthe number of features.",
    "start": "167400",
    "end": "169890"
  },
  {
    "text": "And that becomes more\nand more important,",
    "start": "169890",
    "end": "173190"
  },
  {
    "text": "not just to obtain a\nsolution, but to avoid",
    "start": "173190",
    "end": "176760"
  },
  {
    "text": "fitting the data too hard.",
    "start": "176760",
    "end": "177939"
  },
  {
    "text": "So when we want\nto predict better,",
    "start": "177940",
    "end": "182380"
  },
  {
    "text": "we'll shrink or regularize\nor select features in order",
    "start": "182380",
    "end": "185680"
  },
  {
    "text": "to improve the prediction.",
    "start": "185680",
    "end": "187209"
  },
  {
    "text": "And along the same lines, we\nhave a small number of features,",
    "start": "187210",
    "end": "190130"
  },
  {
    "text": "the model becomes\nmore interpretable.",
    "start": "190130",
    "end": "192040"
  },
  {
    "text": "If we hand our collaborator a\nfew hundred features and say,",
    "start": "192040",
    "end": "195849"
  },
  {
    "text": "these are important, that\nmight be hard to interpret.",
    "start": "195850",
    "end": "198190"
  },
  {
    "text": "If we compare them down to\nthe most important 5 or 10,",
    "start": "198190",
    "end": "201450"
  },
  {
    "text": "it becomes easier to interpret,\nand from a scientific point",
    "start": "201450",
    "end": "206739"
  },
  {
    "text": "of view, more useful.",
    "start": "206740",
    "end": "208290"
  },
  {
    "start": "208290",
    "end": "211329"
  },
  {
    "text": "Feature selection is a way\nof choosing among features",
    "start": "211330",
    "end": "214360"
  },
  {
    "text": "to find the ones that\nare most informative.",
    "start": "214360",
    "end": "217190"
  },
  {
    "text": "So we'll talk about three\nclasses of techniques",
    "start": "217190",
    "end": "219350"
  },
  {
    "text": "in today's lecture.",
    "start": "219350",
    "end": "220650"
  },
  {
    "text": "Subset selection, where\nwe try to find among the p",
    "start": "220650",
    "end": "224269"
  },
  {
    "text": "predictors the ones that are the\nmost related to the response.",
    "start": "224270",
    "end": "227660"
  },
  {
    "text": "And we'll see different\nflavors of subset selection.",
    "start": "227660",
    "end": "231320"
  },
  {
    "text": "Best subset selection.",
    "start": "231320",
    "end": "233718"
  },
  {
    "text": "We'll try to look among\nall possible combinations",
    "start": "233718",
    "end": "235760"
  },
  {
    "text": "of features to find the ones\nthat are the most predictive.",
    "start": "235760",
    "end": "238310"
  },
  {
    "text": "And then we'll talk about\nforward and backward",
    "start": "238310",
    "end": "241370"
  },
  {
    "text": "stepwise methods,\nwhich don't try",
    "start": "241370",
    "end": "242930"
  },
  {
    "text": "to find the best among\nall possible combinations,",
    "start": "242930",
    "end": "245450"
  },
  {
    "text": "but try to do an\nintelligent search",
    "start": "245450",
    "end": "247010"
  },
  {
    "text": "through the space of models.",
    "start": "247010",
    "end": "248430"
  },
  {
    "text": "So forward stepwise, backward\nstepwise and all subsets.",
    "start": "248430",
    "end": "251299"
  },
  {
    "text": "And then some more\nmodern techniques",
    "start": "251300",
    "end": "252950"
  },
  {
    "text": "known as shrinkage methods,\nin which we don't actually",
    "start": "252950",
    "end": "255890"
  },
  {
    "text": "select variables explicitly,\nbut rather we put on a penalty",
    "start": "255890",
    "end": "259790"
  },
  {
    "text": "to penalize the model for\nthe number of coefficients,",
    "start": "259790",
    "end": "264200"
  },
  {
    "text": "or the size of coefficients\nin various ways.",
    "start": "264200",
    "end": "266450"
  },
  {
    "text": "So we'll talk about ridge\nregression and the Lasso",
    "start": "266450",
    "end": "269300"
  },
  {
    "text": "in the shrinkage section.",
    "start": "269300",
    "end": "271030"
  },
  {
    "text": "And then finally in the third\nsection, dimension reduction.",
    "start": "271030",
    "end": "274940"
  },
  {
    "text": "We'll talk about ways of finding\ncombinations of variables,",
    "start": "274940",
    "end": "279710"
  },
  {
    "text": "extracting important\ncombinations of variables,",
    "start": "279710",
    "end": "282050"
  },
  {
    "text": "and then using\nthose combinations",
    "start": "282050",
    "end": "283550"
  },
  {
    "text": "as the features in regression.",
    "start": "283550",
    "end": "286789"
  },
  {
    "text": "We'll talk about PCR, Principal\nComponents Regression,",
    "start": "286790",
    "end": "289130"
  },
  {
    "text": "and partial least squares\nin those settings.",
    "start": "289130",
    "end": "291930"
  },
  {
    "text": "So three classes of methods\nwe'll talk about today.",
    "start": "291930",
    "end": "295400"
  },
  {
    "text": "And one of the things\nabout today's lecture",
    "start": "295400",
    "end": "297800"
  },
  {
    "text": "is that we're going to be\nlooking at all of these ideas",
    "start": "297800",
    "end": "300289"
  },
  {
    "text": "within the context\nof linear regression.",
    "start": "300290",
    "end": "302250"
  },
  {
    "text": "So if you're trying to predict\nsome quantitative response,",
    "start": "302250",
    "end": "304730"
  },
  {
    "text": "and you want to fit a less\nflexible but perhaps more",
    "start": "304730",
    "end": "309320"
  },
  {
    "text": "predictive and also more\ninterpretable model,",
    "start": "309320",
    "end": "311330"
  },
  {
    "text": "these are ways that\nyou can shrink,",
    "start": "311330",
    "end": "314120"
  },
  {
    "text": "in a sense, your usual\nleast squares solution",
    "start": "314120",
    "end": "316820"
  },
  {
    "text": "in order to get better results.",
    "start": "316820",
    "end": "318830"
  },
  {
    "text": "But these concepts\ncan just as well",
    "start": "318830",
    "end": "321259"
  },
  {
    "text": "be applied in the context of\nlogistic regression or, really,",
    "start": "321260",
    "end": "324380"
  },
  {
    "text": "your favorite model,\ndepending on the data set",
    "start": "324380",
    "end": "326630"
  },
  {
    "text": "that you have at hand\nand the type of response",
    "start": "326630",
    "end": "328970"
  },
  {
    "text": "that you're trying to predict.",
    "start": "328970",
    "end": "330240"
  },
  {
    "text": "And so even though linear\nregression is really",
    "start": "330240",
    "end": "332365"
  },
  {
    "text": "what we'll be\ntalking about here,",
    "start": "332365",
    "end": "333740"
  },
  {
    "text": "these really apply to logistic\nand other types of models.",
    "start": "333740",
    "end": "337017"
  },
  {
    "text": "So Daniella is going\nto, first of all,",
    "start": "337017",
    "end": "338600"
  },
  {
    "text": "tell us about subset selection.",
    "start": "338600",
    "end": "341220"
  },
  {
    "start": "340000",
    "end": "538000"
  },
  {
    "text": "So best subset selection\nis a really simple idea.",
    "start": "341220",
    "end": "344110"
  },
  {
    "text": "And the idea here is\nsuppose that we have access",
    "start": "344110",
    "end": "346319"
  },
  {
    "text": "to p predictors.",
    "start": "346320",
    "end": "347280"
  },
  {
    "text": "But we want to actually have a\nsimpler model that involves only",
    "start": "347280",
    "end": "351780"
  },
  {
    "text": "a subset of those p predictors.",
    "start": "351780",
    "end": "353590"
  },
  {
    "text": "Well, the natural\nway to do it is",
    "start": "353590",
    "end": "355440"
  },
  {
    "text": "to consider every possible\nsubset of p predictors.",
    "start": "355440",
    "end": "359070"
  },
  {
    "text": "And to choose the best model\nout of every single model",
    "start": "359070",
    "end": "362370"
  },
  {
    "text": "that just contains\nsome of the predictors.",
    "start": "362370",
    "end": "364260"
  },
  {
    "text": "And so the way that we do this\nis in a very organized way.",
    "start": "364260",
    "end": "367080"
  },
  {
    "text": "We first create a model\nthat we're going to call M0.",
    "start": "367080",
    "end": "369900"
  },
  {
    "text": "And M0 is the null model\nthat contains no predictors.",
    "start": "369900",
    "end": "373960"
  },
  {
    "text": "It just contains an intercept.",
    "start": "373960",
    "end": "375699"
  },
  {
    "text": "And we're just going to\npredict the sample mean",
    "start": "375700",
    "end": "377730"
  },
  {
    "text": "for each observation.",
    "start": "377730",
    "end": "379650"
  },
  {
    "text": "So that's going to give us M0.",
    "start": "379650",
    "end": "381780"
  },
  {
    "text": "And now we're going to try\nto create a model called M1.",
    "start": "381780",
    "end": "385770"
  },
  {
    "text": "And M1 one is going to\nbe the best model that",
    "start": "385770",
    "end": "388020"
  },
  {
    "text": "contains exactly one predictor.",
    "start": "388020",
    "end": "389819"
  },
  {
    "text": "So in order to get M1, we need\nto look at all p models that",
    "start": "389820",
    "end": "394410"
  },
  {
    "text": "contain exactly one predictor.",
    "start": "394410",
    "end": "396280"
  },
  {
    "text": "And we have to find the\nbest among those p models.",
    "start": "396280",
    "end": "400130"
  },
  {
    "text": "Next, we want to find\na model called M2.",
    "start": "400130",
    "end": "402237"
  },
  {
    "text": "That's going to be\nthe best model that",
    "start": "402237",
    "end": "403820"
  },
  {
    "text": "contains two predictors.",
    "start": "403820",
    "end": "405650"
  },
  {
    "text": "So how many models\nare there that",
    "start": "405650",
    "end": "407750"
  },
  {
    "text": "contain two predictors if we\nhave p predictors in total?",
    "start": "407750",
    "end": "411610"
  },
  {
    "text": "And the answer is p choose two.",
    "start": "411610",
    "end": "414909"
  },
  {
    "text": "So if you haven't seen\nthis notation before,",
    "start": "414910",
    "end": "417340"
  },
  {
    "text": "this notation is\nwritten like this.",
    "start": "417340",
    "end": "420290"
  },
  {
    "text": "It's pronounced choose.",
    "start": "420290",
    "end": "421990"
  },
  {
    "text": "So this means p choose two.",
    "start": "421990",
    "end": "424860"
  },
  {
    "text": "And it's equal to p factorial\ndivided by 2 factorial times p",
    "start": "424860",
    "end": "431189"
  },
  {
    "text": "minus 2 factorial.",
    "start": "431190",
    "end": "433730"
  },
  {
    "text": "And that is actually the\nnumber of possible models",
    "start": "433730",
    "end": "438020"
  },
  {
    "text": "that I can get that contain\nexactly two predictors out of p",
    "start": "438020",
    "end": "441020"
  },
  {
    "text": "predictors total.",
    "start": "441020",
    "end": "443759"
  },
  {
    "text": "And so I can consider all p\nchoose two models containing",
    "start": "443760",
    "end": "447360"
  },
  {
    "text": "two predictors.",
    "start": "447360",
    "end": "448272"
  },
  {
    "text": "I'm going to choose\nthe best one and I'm",
    "start": "448273",
    "end": "449940"
  },
  {
    "text": "going to call it M2, and so on.",
    "start": "449940",
    "end": "452380"
  },
  {
    "text": "I can keep on getting\nthe best model",
    "start": "452380",
    "end": "454265"
  },
  {
    "text": "with three predictors,\nfour predictors,",
    "start": "454265",
    "end": "455890"
  },
  {
    "text": "and so on, up to the best\nmodel with p predictors.",
    "start": "455890",
    "end": "459400"
  },
  {
    "text": "So if I'm choosing the best\nmodel out of all models",
    "start": "459400",
    "end": "462370"
  },
  {
    "text": "containing three\npredictors in order to get,",
    "start": "462370",
    "end": "464590"
  },
  {
    "text": "let's say, M3, I can do that in\na pretty straightforward way.",
    "start": "464590",
    "end": "468130"
  },
  {
    "text": "Because I can just say that out\nof all models containing three",
    "start": "468130",
    "end": "471010"
  },
  {
    "text": "predictors, the\nbest one is the one",
    "start": "471010",
    "end": "472870"
  },
  {
    "text": "with the smallest\nresidual sum of squares,",
    "start": "472870",
    "end": "474760"
  },
  {
    "text": "or equivalently, the\nlargest R squared.",
    "start": "474760",
    "end": "479110"
  },
  {
    "text": "And so in this way,\nI get a best model",
    "start": "479110",
    "end": "481330"
  },
  {
    "text": "containing 0, 1, 2, all the\nway through p predictors.",
    "start": "481330",
    "end": "484639"
  },
  {
    "text": "I've called them M0, M1,\nM2, all the way through Mp.",
    "start": "484640",
    "end": "488940"
  },
  {
    "text": "And now I'm on to step three.",
    "start": "488940",
    "end": "490395"
  },
  {
    "text": "And in this final step,\nall that I need to do",
    "start": "490395",
    "end": "492270"
  },
  {
    "text": "is choose the best model\nout of M0 through Mp.",
    "start": "492270",
    "end": "496289"
  },
  {
    "text": "And in order to do this,\nactually, this step 3",
    "start": "496290",
    "end": "499320"
  },
  {
    "text": "is a little bit subtle.",
    "start": "499320",
    "end": "500400"
  },
  {
    "text": "Because we need to be very\ncareful that we choose a model",
    "start": "500400",
    "end": "503430"
  },
  {
    "text": "that really has the smallest\ntest error, rather than",
    "start": "503430",
    "end": "506520"
  },
  {
    "text": "the model that has the\nsmallest training error.",
    "start": "506520",
    "end": "509729"
  },
  {
    "text": "And so there are a\nnumber of techniques",
    "start": "509730",
    "end": "511590"
  },
  {
    "text": "that we can use to choose a\nsingle best model from among M0",
    "start": "511590",
    "end": "514620"
  },
  {
    "text": "to Mp.",
    "start": "514620",
    "end": "515940"
  },
  {
    "text": "And these include\nprediction error estimated",
    "start": "515940",
    "end": "518520"
  },
  {
    "text": "through cross-validation,\nas well as",
    "start": "518520",
    "end": "520968"
  },
  {
    "text": "some methods that\nwe're going to talk",
    "start": "520968",
    "end": "522510"
  },
  {
    "text": "about later in this\nlecture, which you might not",
    "start": "522510",
    "end": "524520"
  },
  {
    "text": "have seen before.",
    "start": "524520",
    "end": "525540"
  },
  {
    "text": "And these include Mallows's CP,\nBayesian Information Criterion,",
    "start": "525540",
    "end": "529980"
  },
  {
    "text": "and adjusted R squared.",
    "start": "529980",
    "end": "532500"
  },
  {
    "text": "So we'll come back to some of\nthose topics in a few minutes.",
    "start": "532500",
    "end": "535550"
  },
  {
    "start": "535550",
    "end": "538529"
  },
  {
    "start": "538000",
    "end": "824000"
  },
  {
    "text": "So here's an example\non the credit data set.",
    "start": "538530",
    "end": "540790"
  },
  {
    "text": "And we saw this data set in\none of the earlier chapters.",
    "start": "540790",
    "end": "544649"
  },
  {
    "text": "And this is a data set that\ncontains 10 predictors involving",
    "start": "544650",
    "end": "548910"
  },
  {
    "text": "things like number of credit\ncards and credit rating",
    "start": "548910",
    "end": "553319"
  },
  {
    "text": "and credit limit.",
    "start": "553320",
    "end": "554410"
  },
  {
    "text": "And the goal is to predict a\nquantitative response, which",
    "start": "554410",
    "end": "557430"
  },
  {
    "text": "is credit card balance.",
    "start": "557430",
    "end": "559470"
  },
  {
    "text": "And so what we can\ndo is we can look",
    "start": "559470",
    "end": "562470"
  },
  {
    "text": "at every single model that\ncontains a subset of these 10",
    "start": "562470",
    "end": "565920"
  },
  {
    "text": "predictors.",
    "start": "565920",
    "end": "567320"
  },
  {
    "text": "And these models are\nactually plotted here",
    "start": "567320",
    "end": "569600"
  },
  {
    "text": "on the left-hand side.",
    "start": "569600",
    "end": "571759"
  },
  {
    "text": "So here, this x-axis is\nthe number of predictors.",
    "start": "571760",
    "end": "575340"
  },
  {
    "text": "It actually goes from 1 to 11\nbecause one of these predictors",
    "start": "575340",
    "end": "578390"
  },
  {
    "text": "is categorical\nwith three levels.",
    "start": "578390",
    "end": "580950"
  },
  {
    "text": "And so we used a couple of\ndummy variables to encode that.",
    "start": "580950",
    "end": "585410"
  },
  {
    "text": "And on the y-axis\nis the residual sum",
    "start": "585410",
    "end": "587569"
  },
  {
    "text": "of squares for each of\nthe possible models.",
    "start": "587570",
    "end": "589660"
  },
  {
    "text": "So for instance,\nlike this dot right",
    "start": "589660",
    "end": "591410"
  },
  {
    "text": "here indicates a\nmodel that contains",
    "start": "591410",
    "end": "593779"
  },
  {
    "text": "one predictor with a pretty\nterrible residual sum",
    "start": "593780",
    "end": "596390"
  },
  {
    "text": "of squares.",
    "start": "596390",
    "end": "597290"
  },
  {
    "text": "And all the way down\nhere, we've got a model",
    "start": "597290",
    "end": "599420"
  },
  {
    "text": "with 11 predictors that has\na pretty decent residual sum",
    "start": "599420",
    "end": "601940"
  },
  {
    "text": "of squares.",
    "start": "601940",
    "end": "602850"
  },
  {
    "text": "So the reason that there's a\nlot of dots in this picture",
    "start": "602850",
    "end": "605480"
  },
  {
    "text": "is because there's a lot of\npossible submodels, given",
    "start": "605480",
    "end": "609740"
  },
  {
    "text": "10 total predictors.",
    "start": "609740",
    "end": "611693"
  },
  {
    "text": "And in fact, as we're going\nto see in a couple of minutes,",
    "start": "611693",
    "end": "614110"
  },
  {
    "text": "there's 2 to the 10 submodels.",
    "start": "614110",
    "end": "616519"
  },
  {
    "text": "So there's actually 2 to\nthe 10 dots in this picture.",
    "start": "616520",
    "end": "619220"
  },
  {
    "text": "Although some of them are\nsort of on top of each other.",
    "start": "619220",
    "end": "621980"
  },
  {
    "text": "And so this red line here\nindicates the best model",
    "start": "621980",
    "end": "625130"
  },
  {
    "text": "of each size.",
    "start": "625130",
    "end": "626090"
  },
  {
    "text": "So this red dot\nright here is M0.",
    "start": "626090",
    "end": "629140"
  },
  {
    "start": "629140",
    "end": "632000"
  },
  {
    "text": "Excuse me, this is M1.",
    "start": "632000",
    "end": "633570"
  },
  {
    "text": "What I just showed you is M1.",
    "start": "633570",
    "end": "635317"
  },
  {
    "text": "So that's the best model\ncontaining one predictor.",
    "start": "635317",
    "end": "637400"
  },
  {
    "text": "Because that is the smallest\nresidual sum of squares.",
    "start": "637400",
    "end": "639780"
  },
  {
    "text": "This is the best model\ncontaining two predictors.",
    "start": "639780",
    "end": "642070"
  },
  {
    "text": "This is M3, best model\ncontaining three predictors.",
    "start": "642070",
    "end": "644610"
  },
  {
    "text": "And so on.",
    "start": "644610",
    "end": "645540"
  },
  {
    "text": "So when we perform best subset\nselection, what we really do",
    "start": "645540",
    "end": "648560"
  },
  {
    "text": "is we trace out this curve\nwhen we get M0 through Mp.",
    "start": "648560",
    "end": "652854"
  },
  {
    "text": "And now, we're just going to\nneed to find a way to choose.",
    "start": "652854",
    "end": "655760"
  },
  {
    "text": "Is M10 better than or\nworse than M4, and so on.",
    "start": "655760",
    "end": "658555"
  },
  {
    "text": "We're just going to have\nto choose among that lower",
    "start": "658555",
    "end": "660680"
  },
  {
    "text": "frontier.",
    "start": "660680",
    "end": "662550"
  },
  {
    "text": "So on the left-hand\nside here, we",
    "start": "662550",
    "end": "664860"
  },
  {
    "text": "see number of predictors\nagainst residual sum of squares.",
    "start": "664860",
    "end": "668279"
  },
  {
    "text": "And on the right-hand\nside, we've",
    "start": "668280",
    "end": "669750"
  },
  {
    "text": "got number of predictors\nagainst R squared.",
    "start": "669750",
    "end": "672300"
  },
  {
    "text": "And as we saw in\nchapter 3, R squared",
    "start": "672300",
    "end": "675180"
  },
  {
    "text": "just tells you the\nproportion of variance",
    "start": "675180",
    "end": "676890"
  },
  {
    "text": "explained by a linear regression\nmodel, by a least squares model.",
    "start": "676890",
    "end": "680930"
  },
  {
    "text": "And so once again,\nwe see a whole lot",
    "start": "680930",
    "end": "683070"
  },
  {
    "text": "of gray dots indicating every\npossible model containing",
    "start": "683070",
    "end": "687180"
  },
  {
    "text": "a subset of these 10 predictors.",
    "start": "687180",
    "end": "689260"
  },
  {
    "text": "And then the red line shows\nthe best model of each size.",
    "start": "689260",
    "end": "694820"
  },
  {
    "text": "So again, this is the\nM1 that we saw earlier.",
    "start": "694820",
    "end": "697270"
  },
  {
    "text": "This is M2.",
    "start": "697270",
    "end": "699170"
  },
  {
    "text": "Over here is M10.",
    "start": "699170",
    "end": "701440"
  },
  {
    "text": "And what we noticed is that\nas the models get bigger,",
    "start": "701440",
    "end": "704830"
  },
  {
    "text": "as we look at larger\nand larger models,",
    "start": "704830",
    "end": "706570"
  },
  {
    "text": "the residual sum of\nsquares decreases",
    "start": "706570",
    "end": "708790"
  },
  {
    "text": "and the R squared increases.",
    "start": "708790",
    "end": "710680"
  },
  {
    "text": "So do we have any idea\nof why that's happening?",
    "start": "710680",
    "end": "715540"
  },
  {
    "text": "Maybe Rob can tell us.",
    "start": "715540",
    "end": "717610"
  },
  {
    "text": "I can tell you.",
    "start": "717610",
    "end": "718750"
  },
  {
    "text": "It's because as you\nadd in variables,",
    "start": "718750",
    "end": "720640"
  },
  {
    "text": "things cannot get worse, right?",
    "start": "720640",
    "end": "722420"
  },
  {
    "text": "If you have a subset of\nsize three, for example,",
    "start": "722420",
    "end": "724630"
  },
  {
    "text": "and you look for the best subset\nof size four, at the very worst,",
    "start": "724630",
    "end": "727840"
  },
  {
    "text": "you could set the coefficient\nfor the fourth variable to be 0.",
    "start": "727840",
    "end": "730780"
  },
  {
    "text": "And you'll have the same error\nas for the three variable model.",
    "start": "730780",
    "end": "735440"
  },
  {
    "text": "So the curve can\nnever get worse.",
    "start": "735440",
    "end": "737810"
  },
  {
    "text": "It can be flat.",
    "start": "737810",
    "end": "738580"
  },
  {
    "text": "If you clear the\nslide here, we can",
    "start": "738580",
    "end": "740038"
  },
  {
    "text": "see it looks like it's\nactually flat from about",
    "start": "740038",
    "end": "742090"
  },
  {
    "text": "three predictors, from\nthree about up to 11.",
    "start": "742090",
    "end": "746830"
  },
  {
    "text": "But it's certainly\nnot going to go up.",
    "start": "746830",
    "end": "750920"
  },
  {
    "text": "It can be flat, as\nit is in this case.",
    "start": "750920",
    "end": "753019"
  },
  {
    "text": "But we can't do any worse\nby adding a predictor.",
    "start": "753020",
    "end": "755620"
  },
  {
    "text": "And actually, what Rob just\nsaid relates to the idea that--",
    "start": "755620",
    "end": "759160"
  },
  {
    "text": "remember on the previous\nslide here in step three,",
    "start": "759160",
    "end": "762139"
  },
  {
    "text": "when we were talking about\nbest subset selection,",
    "start": "762140",
    "end": "764210"
  },
  {
    "text": "we had this step.",
    "start": "764210",
    "end": "764977"
  },
  {
    "text": "And I said that, in order\nto choose among the best",
    "start": "764977",
    "end": "767060"
  },
  {
    "text": "model of each size, we're going\nto need to be really careful.",
    "start": "767060",
    "end": "769680"
  },
  {
    "text": "We're going to have to use\ncross validation or CP, BIC,",
    "start": "769680",
    "end": "772940"
  },
  {
    "text": "adjusted R squared.",
    "start": "772940",
    "end": "774360"
  },
  {
    "text": "And that really relates\nto what's going on here.",
    "start": "774360",
    "end": "776940"
  },
  {
    "text": "Because if I asked\nyou, hey, what's",
    "start": "776940",
    "end": "780170"
  },
  {
    "text": "the best model with\neight predictors?",
    "start": "780170",
    "end": "781970"
  },
  {
    "text": "You'd say, OK, here\nare all of the models",
    "start": "781970",
    "end": "785029"
  },
  {
    "text": "with eight predictors.",
    "start": "785030",
    "end": "786140"
  },
  {
    "text": "And clearly, this\nis the best one.",
    "start": "786140",
    "end": "787953"
  },
  {
    "text": "It's got the smallest\nresidual sum of squares.",
    "start": "787953",
    "end": "789870"
  },
  {
    "text": "There's no argument.",
    "start": "789870",
    "end": "790980"
  },
  {
    "text": "But if I ask you which\nis better, this model",
    "start": "790980",
    "end": "794449"
  },
  {
    "text": "here or this model,\nthere, suddenly it's",
    "start": "794450",
    "end": "798710"
  },
  {
    "text": "not so straightforward.",
    "start": "798710",
    "end": "799820"
  },
  {
    "text": "Because you're kind of\ncomparing apples and oranges.",
    "start": "799820",
    "end": "801990"
  },
  {
    "text": "You're comparing a model\nwith four predictors",
    "start": "801990",
    "end": "803899"
  },
  {
    "text": "to a model with\neight predictors.",
    "start": "803900",
    "end": "805373"
  },
  {
    "text": "You can't just look at\nwhich one has a smaller",
    "start": "805373",
    "end": "807290"
  },
  {
    "text": "residual sum of squares.",
    "start": "807290",
    "end": "808579"
  },
  {
    "text": "Because, of course, the\none with eight predictors",
    "start": "808580",
    "end": "811160"
  },
  {
    "text": "is going to have a smaller\nresidual sum of squares.",
    "start": "811160",
    "end": "813959"
  },
  {
    "text": "So in order to really,\nin a meaningful way,",
    "start": "813960",
    "end": "816548"
  },
  {
    "text": "choose among a set of models\nwith different predictors,",
    "start": "816548",
    "end": "818839"
  },
  {
    "text": "we're going to\nhave to be careful.",
    "start": "818840",
    "end": "819950"
  },
  {
    "text": "And we'll talk about\nthat in a few minutes.",
    "start": "819950",
    "end": "821880"
  },
  {
    "text": "Good",
    "start": "821880",
    "end": "823130"
  },
  {
    "start": "823130",
    "end": "824000"
  }
]