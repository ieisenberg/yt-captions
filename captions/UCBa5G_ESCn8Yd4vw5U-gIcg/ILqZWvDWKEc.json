[
  {
    "start": "0",
    "end": "5720"
  },
  {
    "text": "Good afternoon, CS109. How are you guys doing? [CHEERING] Fantastic. Hey, welcome back\nfrom the break.",
    "start": "5720",
    "end": "12830"
  },
  {
    "text": "I hope you guys had a\nwonderful Thanksgiving. I hope you had a chance for some\nrest, some relaxation, possibly",
    "start": "12830",
    "end": "19070"
  },
  {
    "text": "some good food and\nsome family time. And it's really\nwonderful to see you. I, of course, in\nthe holiday spirit",
    "start": "19070",
    "end": "26384"
  },
  {
    "text": "took some time to think about\nthe things I'm grateful for and how lucky am I to\nbe here doing the job",
    "start": "26385",
    "end": "32445"
  },
  {
    "text": "that I love the most,\nwhich is teaching with such a wonderful group. I am getting a\nlittle sad that we're getting close to the\nend of the quarter.",
    "start": "32445",
    "end": "38977"
  },
  {
    "text": "I'm like, oh, I\nwill miss you guys. Anyways, that's not what's\nimportant right now.",
    "start": "38977",
    "end": "45540"
  },
  {
    "text": "It's been a while. So I thought it'd be nice\nto just take a moment and recap where we\nleft off before we",
    "start": "45540",
    "end": "50970"
  },
  {
    "text": "jump into today's great topic. In CS109, we're on this\nfinal part of the class",
    "start": "50970",
    "end": "56253"
  },
  {
    "text": "where we were learning\nabout machine learning. It is going to take\nus to the point where we're going to\nlearn how deep learning,",
    "start": "56253",
    "end": "61455"
  },
  {
    "text": "a.k.a. neural networks work. But in order to\nunderstand that, you need to know the most core\nclassification algorithms Naive",
    "start": "61455",
    "end": "67800"
  },
  {
    "text": "Bayes and logistic regression. And those two algorithms,\nlike all of deep learning, rest upon a foundation\nof parameter estimation.",
    "start": "67800",
    "end": "75690"
  },
  {
    "text": "If you have any\nprobabilistic model, how could you\nestimate the numbers that could make the\nprobabilistic model",
    "start": "75690",
    "end": "81840"
  },
  {
    "text": "accurate or inaccurate.  Machine learning, when we\ntalk about it in CS109,",
    "start": "81840",
    "end": "89960"
  },
  {
    "text": "it's generally a\npretty simple process. This is the process\nof you're going to be building a model\nwhich we sometimes",
    "start": "89960",
    "end": "95780"
  },
  {
    "text": "think about as these black\nbox models where somebody could give you inputs, features\nfor a particular individual.",
    "start": "95780",
    "end": "102380"
  },
  {
    "text": "Your black box module\nwill do some work and it'll come up\nwith a prediction. And the work that the\nblack box module does",
    "start": "102380",
    "end": "109670"
  },
  {
    "text": "will be based on some\nnumbers, some key numbers that we call parameters and a\nlot of work in machine learning",
    "start": "109670",
    "end": "116300"
  },
  {
    "text": "is setting these parameters\nso that your black box is able to make good predictions. Particularly in\nCS109, we've been",
    "start": "116300",
    "end": "122680"
  },
  {
    "text": "talking about a\nspecific prediction type called classification. And classification is\nwhere you're not just",
    "start": "122680",
    "end": "129280"
  },
  {
    "text": "predicting anything. You're predicting\ndiscrete class labels. Like for example, 1 or 0, will\nsomebody have a healthy heart.",
    "start": "129280",
    "end": "136780"
  },
  {
    "text": "1 or 0 will somebody\nlike a movie. Or 1 or 0, somebody coming\nfrom particular ancestry. ",
    "start": "136780",
    "end": "143830"
  },
  {
    "text": "The most interesting\npart of machine learning comes in what we call training.",
    "start": "143830",
    "end": "149290"
  },
  {
    "text": "So in the first step\nyou formalize a problem. You take a real world problem. You make a model. You come up with a formal\nmodel that has parameters.",
    "start": "149290",
    "end": "156040"
  },
  {
    "text": "But the interesting\nthing is when you take data and use those\ndata to learn the parameters.",
    "start": "156040",
    "end": "161709"
  },
  {
    "text": "Once you've done that\nthough, it is worth noting that it is beholden\nupon you to try and estimate",
    "start": "161710",
    "end": "166930"
  },
  {
    "text": "how good your algorithm is. So we will reserve\nsome data for testing. And then we can say,\nwe've got an algorithm.",
    "start": "166930",
    "end": "173140"
  },
  {
    "text": "It's trained. And this is how\naccurate we think it is. Training data often has\nthis crazy notation.",
    "start": "173140",
    "end": "181120"
  },
  {
    "text": "In order to gain intelligence\nfor a classification task, I'm going to give\nyou a bunch of data. We call that data training data.",
    "start": "181120",
    "end": "186603"
  },
  {
    "text": "And training data is going to\nbe a combination of features and you're going to\nbe told the label",
    "start": "186603",
    "end": "192580"
  },
  {
    "text": "for this particular individual. So you say, in my training\ndata, here's one individual. They've got these features and\na 1 or 0 for their class label.",
    "start": "192580",
    "end": "201110"
  },
  {
    "text": "This is generic. It works regardless of what\nproblem you're talking about. But it does have some notation\nI don't want to lose people on.",
    "start": "201110",
    "end": "208720"
  },
  {
    "text": "This superscript is saying which\nindividual we're talking about. The x are the inputs,\na.k.a. the features.",
    "start": "208720",
    "end": "214269"
  },
  {
    "text": "And the y's are the labels. m is the size of the feature,\nso how many inputs do",
    "start": "214270",
    "end": "219850"
  },
  {
    "text": "I have per individual. I found that notation quite\nhard so I spent a little time",
    "start": "219850",
    "end": "225099"
  },
  {
    "text": "in class making sure\nthat we understood what that notation was. And particularly, if\nyou have an individual,",
    "start": "225100",
    "end": "233470"
  },
  {
    "text": "we think of the x's as being the\nlist of numbers for the inputs and there will be m of those.",
    "start": "233470",
    "end": "238580"
  },
  {
    "text": "And the y being the prediction\nof healthy or not healthy. In training data, you're\ntold both x and y.",
    "start": "238580",
    "end": "244400"
  },
  {
    "text": "And the real\ninteresting thing is to build a machine where\nyou could just put in inputs and it would predict the y.",
    "start": "244400",
    "end": "252720"
  },
  {
    "text": "Maybe questions about this\ngeneral idea of classification before we jump into the one\nalgorithm we've got so far,",
    "start": "252720",
    "end": "258838"
  },
  {
    "text": "Naive Bayes?  Well, just before the break,\nwe talked about an algorithm",
    "start": "258839",
    "end": "266530"
  },
  {
    "text": "how you could do classification. We said, OK, if you want\nto build this black box, we're just going to take some\ninputs and make a prediction.",
    "start": "266530",
    "end": "272468"
  },
  {
    "text": "One way you could do\nthat is you could make a little probabilistic model.",
    "start": "272468",
    "end": "277600"
  },
  {
    "text": "And that probabilistic\nmodel is going to be able to compute what's\nthe probability that y equals 1 given x and the probability\nthat y equals 0 given x.",
    "start": "277600",
    "end": "284590"
  },
  {
    "text": "And then we're either going\nto choose our prediction to be 1 or 0. Classification has\nto give back 1 or 0",
    "start": "284590",
    "end": "290140"
  },
  {
    "text": "based on this probability. Very reasonable thing to do. And particularly, imagine this,\nif you had this probability,",
    "start": "290140",
    "end": "297310"
  },
  {
    "text": "we'll try y equals 0,\nand we'll try y equals 1, and whichever one is\nthe larger value, that's",
    "start": "297310",
    "end": "302680"
  },
  {
    "text": "the prediction we'll make. That's why this\nargmax exists just so we can turn a probability\ninto either a 0 or a 1.",
    "start": "302680",
    "end": "311400"
  },
  {
    "text": "Naive Bayes though, while this\nwas a nice idea, didn't work. We couldn't make a\nprobabilistic model",
    "start": "311400",
    "end": "316920"
  },
  {
    "text": "that would generally work. So we made this awfully\nincorrect but very helpful assumption that the\nprobability for any individual",
    "start": "316920",
    "end": "325020"
  },
  {
    "text": "of their inputs\ngiven their output is equal to the probability\nof each input on its own",
    "start": "325020",
    "end": "331440"
  },
  {
    "text": "given the output. So this is a whole\nvector of numbers. So it's the probability of x1,\nand x2, and x3, and x4 given y.",
    "start": "331440",
    "end": "338688"
  },
  {
    "text": "We're going to say\nthat's going to be the product of each feature. We assume the features are\nindependent of each other given",
    "start": "338688",
    "end": "345540"
  },
  {
    "text": "the class label. Now this was the derivation\nand the Naive Bayes assumption",
    "start": "345540",
    "end": "350790"
  },
  {
    "text": "shows up right here. But importantly, if you\nmake this assumption, then you end up with a way\nto make your prediction.",
    "start": "350790",
    "end": "358380"
  },
  {
    "text": "We use log probabilities\nbecause making sure it works on a\ncomputer is very important.",
    "start": "358380",
    "end": "363990"
  },
  {
    "text": "And the product of a\nbunch of probabilities could become quite small. And we do this derivation. It leads to this very\nsimple way of how",
    "start": "363990",
    "end": "371020"
  },
  {
    "text": "we can make our predictions. All these things are learnable. You are now tasked with,\nbased on your training data,",
    "start": "371020",
    "end": "378040"
  },
  {
    "text": "learning this probability. The probability that y\nequals 0 and the probability that y equals 1. You also have to learn each\nof these probabilities.",
    "start": "378040",
    "end": "384790"
  },
  {
    "text": "For every feature,\nwhat's the probability that each feature takes\non its value given y equals either 0 or 1.",
    "start": "384790",
    "end": "391450"
  },
  {
    "text": "Those are all very,\nvery doable things. Particularly if you wanted to\nlearn each of these things,",
    "start": "391450",
    "end": "398020"
  },
  {
    "text": "it's just going to be counting. MLE says these are\nBernoullis so we",
    "start": "398020",
    "end": "403270"
  },
  {
    "text": "can estimate the probability\np of these Bernoullis just counting.",
    "start": "403270",
    "end": "409120"
  },
  {
    "text": "Then we went really deep into\nthis MAP, this Bayesian way of parameter estimation.",
    "start": "409120",
    "end": "414639"
  },
  {
    "text": "And it just ended up, if\nyou had a Laplace prior, of bean counting with a\nlittle bit of addition.",
    "start": "414640",
    "end": "421509"
  },
  {
    "text": "So long story short,\nwe have an algorithm. It's called Naive Bayes. It uses this awful assumption\nbut it turns things",
    "start": "421510",
    "end": "429940"
  },
  {
    "text": "into just counting. So training just\nbecomes counting and then predictions\njust require you to put your counting\nthrough the particular function",
    "start": "429940",
    "end": "439419"
  },
  {
    "text": "that we derived. So at this point, we have an\nalgorithm, which is fantastic.",
    "start": "439420",
    "end": "448100"
  },
  {
    "text": "But there are other algorithms. And it turns out there are other\nalgorithms that you absolutely should know. Because some of these\nother algorithms",
    "start": "448100",
    "end": "454810"
  },
  {
    "text": "have changed the way that\nthe whole world works of artificial intelligence.",
    "start": "454810",
    "end": "459880"
  },
  {
    "text": "And we're going to\nlearn about that today but a little bit more background\nthat's worth recalling",
    "start": "459880",
    "end": "465580"
  },
  {
    "text": "is optimization. When we were talking about\nparameter estimation,",
    "start": "465580",
    "end": "470800"
  },
  {
    "text": "one of the great ideas\nthat we came up with was, if you want to\nfind parameters that",
    "start": "470800",
    "end": "476620"
  },
  {
    "text": "maximize likelihood,\nyou're actually doing an optimization task. You're saying I want you to\nchoose parameters for me.",
    "start": "476620",
    "end": "483557"
  },
  {
    "text": "And I want those\nparameters to be such that the likelihood is\nas large as possible. And we figured out that this\nwas kind of like an optimization",
    "start": "483557",
    "end": "492200"
  },
  {
    "text": "task. And in order to do that, we\nsaid, we can use hill climbing, particularly we could\nuse gradient ascent.",
    "start": "492200",
    "end": "497949"
  },
  {
    "text": "And gradient ascent tells\nyou that if you can give me the derivative of the thing\nyou care about with respect",
    "start": "497950",
    "end": "505540"
  },
  {
    "text": "to each of your movable\ndials, if you give me those derivatives, then I've got\na very simple algorithm to come up with very good parameters.",
    "start": "505540",
    "end": "512080"
  },
  {
    "text": "So a little bit of\nrecall, gradient ascent. Gradient ascent,\ngradient descent are brother and\nsister algorithms.",
    "start": "512080",
    "end": "518380"
  },
  {
    "text": "One could just be optimizing\nthe negative of the other. And just to recall,\nyou have a way",
    "start": "518380",
    "end": "524200"
  },
  {
    "text": "of optimizing parameters\nfor a likelihood function and as gradient ascent. ",
    "start": "524200",
    "end": "530930"
  },
  {
    "text": "So this wasn't that important. And review. [SIGHS] That was a lot.",
    "start": "530930",
    "end": "537540"
  },
  {
    "text": "But feel free to ask questions\nif you forgot something and you want me to recall it. This is supposed to\nbe a conversation. In fact, what you're\nlearning today,",
    "start": "537540",
    "end": "544019"
  },
  {
    "text": "you're going to have to\nprogram in your problem sets and it's something crazy\nimportant for the world",
    "start": "544020",
    "end": "549248"
  },
  {
    "text": "so I want you guys to\nunderstand it deeply. I would like to invite you\nto ask the questions you're curious about because\nother people in class",
    "start": "549248",
    "end": "555120"
  },
  {
    "text": "will be curious\nabout it as well. It's time. One of my favorite things to\nteach, logistic regression,",
    "start": "555120",
    "end": "562520"
  },
  {
    "text": "a very simple algorithm\nfor making a classification prediction that is going to\nbe very impactful because it",
    "start": "562520",
    "end": "570860"
  },
  {
    "text": "is, in fact, the heart\nand soul of deep learning. If you've heard of neural\nnetworks or deep learning, it is a bunch of\nlogistic regressions",
    "start": "570860",
    "end": "577010"
  },
  {
    "text": "put on top of each other. So learn logistic regression,\nit is critically important.",
    "start": "577010",
    "end": "582290"
  },
  {
    "text": "In order to understand\nlogistic regression, we are going to be\ntaking advantage of some of the parameter\nestimation we've seen so far.",
    "start": "582290",
    "end": "588860"
  },
  {
    "text": " I know I just gave\nyou a lot of review.",
    "start": "588860",
    "end": "594529"
  },
  {
    "text": "I'm just going to give\nyou a tiny bit more mathematical background because\nI don't want to lose people on some prerequisites that you\nmight not have seen before.",
    "start": "594530",
    "end": "601320"
  },
  {
    "text": "Well, this is less\nof a prerequisite but it's just more of\na cool thing to know. There is this function\nthat people in AI love,",
    "start": "601320",
    "end": "609070"
  },
  {
    "text": "it's called the\nsigmoid function, which is so confusing. Why is it so confusing?",
    "start": "609070",
    "end": "614100"
  },
  {
    "text": "Because we've seen\nsigmoids before. When have we seen sigmoids? ",
    "start": "614100",
    "end": "620130"
  },
  {
    "text": "Variance. Yeah, the variance of say\na normal distribution. Hey, you guys, it's\nan interesting day.",
    "start": "620130",
    "end": "626920"
  },
  {
    "text": "Before pandemic,\nI had this policy of bringing fruit to class. And if people ask\nquestions or interacted,",
    "start": "626920",
    "end": "633160"
  },
  {
    "text": "I would give out fruits. And then the whole world\nstopped with COVID, and you couldn't throw fruits\nthrough Zoom, so I stopped.",
    "start": "633160",
    "end": "638680"
  },
  {
    "text": "And then we came back but just\ngiving fruits didn't feel right when we were all so worried.",
    "start": "638680",
    "end": "644080"
  },
  {
    "text": "But I have fruits again for\nthe first time in three years. This is a special day.",
    "start": "644080",
    "end": "649587"
  },
  {
    "text": "So anyways, ask a question,\nget a little mandarin. You can eat after class,\nshare it with your friends, as you like. Anyways, as we last left off,\nsigmoid is a confusing name",
    "start": "649587",
    "end": "658890"
  },
  {
    "text": "because it means a\nvery different thing. This symbol is also\nused for variance. The sigmoid function is a\ntotally different thing.",
    "start": "658890",
    "end": "665700"
  },
  {
    "text": "The sigmoid function\nis just a way of writing this whole piece\nof math in a condensed way.",
    "start": "665700",
    "end": "671460"
  },
  {
    "text": "And this whole piece of math\nis saying, take some input, and take 1 divided by\n1 plus e to the power",
    "start": "671460",
    "end": "677730"
  },
  {
    "text": "of negative that input. If you were to graph it for all\nthe different possible inputs, it looks like this.",
    "start": "677730",
    "end": "684360"
  },
  {
    "text": "And people in AI\nand probability love this function for two reasons.",
    "start": "684360",
    "end": "690100"
  },
  {
    "text": "One reason is it's a\nsquashing function. No matter what number\nyou put into this,",
    "start": "690100",
    "end": "695160"
  },
  {
    "text": "your output will\nbe between 0 and 1. I don't want to give things\naway but people in probability love that because if you can\nguarantee that your output is",
    "start": "695160",
    "end": "702960"
  },
  {
    "text": "between 0 and 1, then\nit starts to look like something that maybe\nyou could call a probability.",
    "start": "702960",
    "end": "708840"
  },
  {
    "text": "Anyways, this is just a\nfunction you should know. We haven't used it\nfor anything too deep. I have written the function\nitself over here on the board.",
    "start": "708840",
    "end": "717810"
  },
  {
    "text": "Caution, not the\nsame sigma that you learned about when\nyou were a wee kid in the first half of CS109.",
    "start": "717810",
    "end": "725415"
  },
  {
    "text": "Another thing I want to\nput in our background before we jump into things\nis some key notation. You've probably seen this\nin like Math51 or something",
    "start": "725415",
    "end": "733050"
  },
  {
    "text": "like that before, which is\nthe transpose of two vectors. If theta is a vector\nand x is a vector",
    "start": "733050",
    "end": "739709"
  },
  {
    "text": "and they're of the\nsame length, theta transpose x is another\nway of writing this sum.",
    "start": "739710",
    "end": "745380"
  },
  {
    "text": "It's saying take the\nfirst element of both of these vectors, multiply\nthem together, then add that",
    "start": "745380",
    "end": "751290"
  },
  {
    "text": "to the product of\nthe second element, and add that to the product\nof the third element, and the product of\nthe nth element.",
    "start": "751290",
    "end": "757570"
  },
  {
    "text": "So this is really\nwhat that is saying.",
    "start": "757570",
    "end": "762580"
  },
  {
    "text": "But you can imagine\nthese three are three different pieces\nof notation for writing the exact same thing.",
    "start": "762580",
    "end": "767980"
  },
  {
    "text": "It is saying an element-wise\nproduct and then the sum over-- the sum over the\nelement-wise product.",
    "start": "767980",
    "end": "775089"
  },
  {
    "text": "Again, I wrote that over here. Theta transpose x is this sum. You'll notice on\nthe board I actually",
    "start": "775090",
    "end": "781930"
  },
  {
    "text": "had this starting at 0, not 1. It depends how you\nindex your lists. But I'm going to start indexing\nby 0 in a little bit which",
    "start": "781930",
    "end": "790510"
  },
  {
    "text": "you'll see when we get there. Any questions on this notation? Don't get lost on notation. Why would you want to\nget lost in background?",
    "start": "790510",
    "end": "796930"
  },
  {
    "text": "Anything confusing or what\nis confusing about this? ",
    "start": "796930",
    "end": "802990"
  },
  {
    "text": "Just want a mandarin? Anybody just want a mandarin? Yeah? I just want to make\nsure I clarify,",
    "start": "802990",
    "end": "808420"
  },
  {
    "text": "is theta matrix or a vector? It's going to be a vector. So if theta is a vector\nand x is a vector.",
    "start": "808420",
    "end": "814490"
  },
  {
    "text": "So x could be for example,\nthis is generally true if x is a vector but maybe it\ncould be a list of features.",
    "start": "814490",
    "end": "820386"
  },
  {
    "text": "And we're going to try\nsomething because I'm afraid of breaking a camera. We're going to pass this back. OK, fantastic.",
    "start": "820387",
    "end": "827410"
  },
  {
    "text": "Now, if you want to be super\nfancy, when you do this, you end up with this\nbig calculation.",
    "start": "827410",
    "end": "834700"
  },
  {
    "text": "But if you were to actually\nget a computer out and evaluate this, this would be a number. And because this\nis a number, you",
    "start": "834700",
    "end": "841120"
  },
  {
    "text": "can put it into that function. So you could take\nthis calculation, take the number that results,\nand put it through the sigmoid.",
    "start": "841120",
    "end": "847360"
  },
  {
    "text": "Notationally, if\nyou did that, you could say that's\nthe sigmoid of theta transpose x, which would be\nthe same as a sigmoid as that",
    "start": "847360",
    "end": "855940"
  },
  {
    "text": "or the sigmoid of that. These are three different ways\nof writing the same thing. Or you could just put that\nnumber into the sigmoid itself.",
    "start": "855940",
    "end": "863852"
  },
  {
    "text": "A little bit\n[INAUDIBLE] notation. But that's just abstract. We haven't put this into any\npoint of our derivations.",
    "start": "863853",
    "end": "871390"
  },
  {
    "text": "Some background I\nput on the board. Keep it in mind. And if you have\nquestions later, ask me.",
    "start": "871390",
    "end": "878220"
  },
  {
    "text": "Oh, the chain rule. Man, I remember when I\nlearned this in math. And it wasn't motivated at all.",
    "start": "878220",
    "end": "883410"
  },
  {
    "text": "We're like, we're just\ngoing to learn calculus and because we're\nlearning calculus, you need to know the chain rule\nso you can do the things that were on the final.",
    "start": "883410",
    "end": "888810"
  },
  {
    "text": "I wish my math\nteacher had told me this was the idea that\nwould mathematically change the world.",
    "start": "888810",
    "end": "894690"
  },
  {
    "text": "The chain rule is\nthe reason that we can stack things that are\nderivable on top of each other",
    "start": "894690",
    "end": "900149"
  },
  {
    "text": "and have neural networks\nlearn through the whole thing because computers can\ndo the chain rule. It's crazy important.",
    "start": "900150",
    "end": "905820"
  },
  {
    "text": "But anyways, the chain rule\nsays, if you have some function and you can decompose\nit, you could say,",
    "start": "905820",
    "end": "913980"
  },
  {
    "text": "if f of x you want to derive it\nwith respect to x, if you can decompose this into\nsaying like f is actually",
    "start": "913980",
    "end": "921090"
  },
  {
    "text": "some other function z\nof x, you can do f of z with respect to z.",
    "start": "921090",
    "end": "926790"
  },
  {
    "text": "So reference how this\nfunction changes with respect to its input and then derive f\nof z-- or z with respect to x.",
    "start": "926790",
    "end": "935490"
  },
  {
    "text": "Just want to recall this. But I think if you\nhaven't seen chain rule, A, please do watch your\nfavorite Khan Academy",
    "start": "935490",
    "end": "943329"
  },
  {
    "text": "video on the chain rule. And second, I will\nshow you it in action and you should particularly\nmake sure you understand",
    "start": "943330",
    "end": "949510"
  },
  {
    "text": "chain rule in the context I\ntalk about it, today's class. OK, I'm so excited.",
    "start": "949510",
    "end": "955218"
  },
  {
    "text": "We've done our hard work. We've done our review. We've done our background. It's time to have a\nbig party and learn",
    "start": "955218",
    "end": "960520"
  },
  {
    "text": "about logistic regression. Chapter 1, the big picture.",
    "start": "960520",
    "end": "966380"
  },
  {
    "text": "We want to invent the next\nclassification algorithm. And classification,\nyou're going to be",
    "start": "966380",
    "end": "973400"
  },
  {
    "text": "building a machine\nwhere you put in inputs and it makes a prediction. And we decided that a really\ngood way to build this machine",
    "start": "973400",
    "end": "981380"
  },
  {
    "text": "would be if we could calculate\nthe probability of the output taking on the value 0\nor 1 given the inputs.",
    "start": "981380",
    "end": "987170"
  },
  {
    "text": "If we could know that\nconditional probability, we could make a\nreally great machine. Naive Bayes tried to do this.",
    "start": "987170",
    "end": "994139"
  },
  {
    "text": "And in order to do this, it\nmade this really big assumption, the probability of\neach feature given",
    "start": "994140",
    "end": "1000009"
  },
  {
    "text": "y being the product of each\nfeature on its own given y. It's helpful. It made the math really simple.",
    "start": "1000010",
    "end": "1006560"
  },
  {
    "text": "But it's definitely\nwrong in some cases. It's not always true\nthat the features would be independent of\neach other given the output.",
    "start": "1006560",
    "end": "1014550"
  },
  {
    "text": "But the simple idea that is\ngoing to drive all of today is a bit of a light bulb moment.",
    "start": "1014550",
    "end": "1021800"
  },
  {
    "text": "What if we just allow this\nto be a bit more of a machine that we construct? What if we just build a machine\nthat can directly figure out",
    "start": "1021800",
    "end": "1029000"
  },
  {
    "text": "the probability of y given x? And that was the simple\nlight bulb idea that led to logistic regression.",
    "start": "1029000",
    "end": "1035959"
  },
  {
    "text": "The simple idea was, we've got\nour x's, that's going to be a list of 1's and 0's.",
    "start": "1035960",
    "end": "1041209"
  },
  {
    "text": "We have to predict our y that's\neither going to be a 1 or a 0. And the simple idea\nof logistic regression",
    "start": "1041210",
    "end": "1047359"
  },
  {
    "text": "is, well, what if we just\nconstructed a machine that took those x's and predicted y?",
    "start": "1047359",
    "end": "1052910"
  },
  {
    "text": "And we allowed that machine\nto be a little bit more flexible than say,\nsomething that",
    "start": "1052910",
    "end": "1058790"
  },
  {
    "text": "had to be a Naive Bayes model. And particularly, people\nstarted using this one",
    "start": "1058790",
    "end": "1066590"
  },
  {
    "text": "specific machine. They said, OK, I'm going to\nbuild a machine where I'm going",
    "start": "1066590",
    "end": "1072049"
  },
  {
    "text": "to take each of these numbers. My machine is going to have\na weight for each number.",
    "start": "1072050",
    "end": "1077390"
  },
  {
    "text": "I'm going to weight each\nnumber, sum them all up-- sum up all those\nweighted things,",
    "start": "1077390",
    "end": "1083250"
  },
  {
    "text": "and then I'm going to get\na number which I'll squash, and I'm going to call the\nsquashed thing the probability",
    "start": "1083250",
    "end": "1088470"
  },
  {
    "text": "that y equals 1. [VOCALIZES EXPLOSION] Craziness. Let me give you that picture\nin a little more detail.",
    "start": "1088470",
    "end": "1095550"
  },
  {
    "text": "Logistic regression\nassumption is, I'm just going to build\nyou a machine that's got some parameters in\nit that are movable.",
    "start": "1095550",
    "end": "1102539"
  },
  {
    "text": "You're going to choose\ngreat values of parameters such that your machine\nwhen it takes in x's, just",
    "start": "1102540",
    "end": "1109200"
  },
  {
    "text": "outputs things that are\nclose to the probability that y equals 1 given\nthose particular inputs.",
    "start": "1109200",
    "end": "1116200"
  },
  {
    "text": "Here, I think this machine is\nso important to understand. It's one of the most\nimportant machineries to get your head around.",
    "start": "1116200",
    "end": "1121971"
  },
  {
    "text": "So I spent some\ntime trying to make this machine look\na little prettier than just that equation. People said, OK, you're\ngoing to take your inputs",
    "start": "1121972",
    "end": "1129280"
  },
  {
    "text": "and you're going to predict\nwhether or not y equals 1. So you want the property that\ny equals 1 given your inputs. We're going to have a\nprobability producing",
    "start": "1129280",
    "end": "1135512"
  },
  {
    "text": "machine take all your\ninputs and every input is going to go through this\nchannel where it gets weighted.",
    "start": "1135512",
    "end": "1141250"
  },
  {
    "text": "It's going to be\nweighted by parameters. And then every channel,\nonce it gets weighted, it's going to come into\nbig summation unit.",
    "start": "1141250",
    "end": "1148053"
  },
  {
    "text": "We're going to\nweight each channel and then sum them together. You could call this\nsum z if you wanted.",
    "start": "1148053",
    "end": "1153160"
  },
  {
    "text": "Now, when you take an\ninput and you weight them, and you sum them\ntogether, there's no guarantee that this looks\nanything like a probability.",
    "start": "1153160",
    "end": "1161299"
  },
  {
    "text": "So then what we're\ngoing to do is we're going to put it\nthrough a squashing function. And this squashing function\nwill take this number",
    "start": "1161300",
    "end": "1168429"
  },
  {
    "text": "and make it look\nlike a probability. And then the craziest\nthing is, we're going to interpret that as the\nprobability that y equals 1.",
    "start": "1168430",
    "end": "1176520"
  },
  {
    "text": "Craziness. I use this metaphor\nof a sound board where each of your\nparameters are movable dials",
    "start": "1176520",
    "end": "1183240"
  },
  {
    "text": "and you can move them\nto change the thetas and that would change\nhow your machine works.",
    "start": "1183240",
    "end": "1189770"
  },
  {
    "text": "In my cartoon just to be\nclear, these are the inputs. You can have every thing you're\nmaking prediction of will",
    "start": "1189770",
    "end": "1197060"
  },
  {
    "text": "come with a list of inputs. And we're just going\nto set the machine to start out with\nwhatever those inputs are",
    "start": "1197060",
    "end": "1203000"
  },
  {
    "text": "if we're making a prediction. For example, if you're\npredicting whether or not somebody likes a movie, the\ninputs could be yes or no,",
    "start": "1203000",
    "end": "1210270"
  },
  {
    "text": "did they like other movies. The output is also in this\ncase going to be a 1 or a 0.",
    "start": "1210270",
    "end": "1217044"
  },
  {
    "text": "And it's going to be yes or no,\ndid they like the target movie. And we would like it so\nthat this machine, when you take an individual,\nand you set whether or not",
    "start": "1217045",
    "end": "1224380"
  },
  {
    "text": "they like these three different\nmovies, once it's weighted those 1's and 0's,\nsummed them up, put it through its\nsquashing function,",
    "start": "1224380",
    "end": "1230950"
  },
  {
    "text": "we would like the\nprobability that it comes out to be as close as possible\nto the true probability",
    "start": "1230950",
    "end": "1237220"
  },
  {
    "text": "that this individual\nwould like that movie. Can I be clear about something? Is this how the\nworld really works?",
    "start": "1237220",
    "end": "1244450"
  },
  {
    "text": "Is there something true\nabout the universe that when probabilities are being\nproduced, actually, underneath the hood the universe\nis making this little machine",
    "start": "1244450",
    "end": "1251799"
  },
  {
    "text": "and pushing these inputs\nthrough these weights and then squashing it?",
    "start": "1251800",
    "end": "1257460"
  },
  {
    "text": "No, there is nothing\nrealistic about this. It's wrong. Why would we use a wrong model?",
    "start": "1257460",
    "end": "1264900"
  },
  {
    "text": "Because it turns out this\nwrong model is very useful. So we're going to make\nthis little machine.",
    "start": "1264900",
    "end": "1271170"
  },
  {
    "text": "And the most beautiful\nthing about this machine is it's going to end\nup being a LEGO block and we'll be able to stack\nthem on top of each other.",
    "start": "1271170",
    "end": "1276780"
  },
  {
    "text": "But I'm getting ahead of myself. Yes, question? Where did the thetas come from? Good question.",
    "start": "1276780",
    "end": "1282100"
  },
  {
    "text": "I haven't told you yet, but it's\ngoing to be really important. If I put random thetas,\nI've got a random machine.",
    "start": "1282100",
    "end": "1288360"
  },
  {
    "text": "But my claim for you is, if\nsome oracle came and gave you the best setting of\nthetas, then maybe this",
    "start": "1288360",
    "end": "1294960"
  },
  {
    "text": "could be pretty reasonable. But your question is,\nwhere did they come from. Oh, my god, I'm gonna\ntell you about that.",
    "start": "1294960",
    "end": "1300870"
  },
  {
    "text": "It's going to come from\nparameter estimation. But I haven't told you that.",
    "start": "1300870",
    "end": "1305894"
  },
  {
    "text": "Here you go. I never heard it. OK, question and then question. Does it return 1--",
    "start": "1305895",
    "end": "1311382"
  },
  {
    "text": "the squashing\nfunction just returns a value of more than 0.5? So the squashing function\nis going to give you",
    "start": "1311382",
    "end": "1316800"
  },
  {
    "text": "a number between 0 and 1. And yeah, we are going to\nsay like, if it gives you a value of 0.8,\nwe're going to assume",
    "start": "1316800",
    "end": "1322650"
  },
  {
    "text": "that's the property\nthat y equals 1. And if the probability that y\nequals 1 is greater than 0.5, we're just going to predict a 1.",
    "start": "1322650",
    "end": "1329950"
  },
  {
    "text": "OK, yeah? So at the moment, it seems like\nour sigmoid function is kind",
    "start": "1329950",
    "end": "1336519"
  },
  {
    "text": "of just an arbitrary choice. If we wanted to get a\nnumber between 0 and 1, I might have just\ntaken the average of--",
    "start": "1336520",
    "end": "1343780"
  },
  {
    "text": "so isn't our result heavily\ndependent on the arbitrary choice of the\nsquashing function?",
    "start": "1343780",
    "end": "1349750"
  },
  {
    "text": "Yes. OK, next question. No, I'm just joking. [LAUGHS] You say like,\nhey, I could have come up",
    "start": "1349750",
    "end": "1354893"
  },
  {
    "text": "with a different\nsquashing function that could make this thing\nthat's real valued and make it look\nlike a probability. And people have.",
    "start": "1354893",
    "end": "1361434"
  },
  {
    "text": "Sigmoid, actually,\nit's not old school. But there's other different\nsquashing functions that people now consider\nusing and some neural networks",
    "start": "1361435",
    "end": "1368010"
  },
  {
    "text": "will use different squashing\nfunctions other than a sigmoid. So you're right. It was a bit arbitrary\nbut boy, does it work.",
    "start": "1368010",
    "end": "1373930"
  },
  {
    "text": " Yes, question? Do we consider different\nxi's for whether y is 1 or 0?",
    "start": "1373930",
    "end": "1382860"
  },
  {
    "text": " The meaning of xi's are\ngoing to stay the same.",
    "start": "1382860",
    "end": "1389642"
  },
  {
    "text": "x3 will always be whether\nor not somebody likes Pulp Fiction in this case. And the values of those xi's\nwill depend on the individual",
    "start": "1389642",
    "end": "1396797"
  },
  {
    "text": "that you're making\nthe prediction for. So if an individual comes,\nI will put all their values of what they like\nfor this movie.",
    "start": "1396797",
    "end": "1403390"
  },
  {
    "text": "And then I will predict the y. So I wouldn't say that the\nx's depend on the y-- sorry,",
    "start": "1403390",
    "end": "1408640"
  },
  {
    "text": "the x's depend on the y's. Rather, I would\nsay in our machine, you set the x's and\nwe predict the y's.",
    "start": "1408640",
    "end": "1414492"
  },
  {
    "text": "And I'm afraid of\nthrowing an orange that far so please come after\nclass and grab your mandarin.",
    "start": "1414492",
    "end": "1420330"
  },
  {
    "text": "So we've got inputs. We've got weights,\na.k.a. parameters.",
    "start": "1420330",
    "end": "1426340"
  },
  {
    "text": "We have the weighted sum. This will be a number where\nevery input has been weighted and we add them together\nto get the weighted sum.",
    "start": "1426340",
    "end": "1432039"
  },
  {
    "text": "At this point, we've\ncalculated the sum of all the inputs weighted by\ntheir particular weight added",
    "start": "1432040",
    "end": "1440360"
  },
  {
    "text": "together. This is a number at this point. And then we squash that number\nand we call the resulting",
    "start": "1440360",
    "end": "1446139"
  },
  {
    "text": "of the squash a probability. We then make our prediction. And this is it. This is the logistic regression\nmodel, a.k.a. assumption.",
    "start": "1446140",
    "end": "1455710"
  },
  {
    "text": "Now, a great question\nwas, where did you get those parameters from? The parameters really matter.",
    "start": "1455710",
    "end": "1462190"
  },
  {
    "text": "If I gave you\ndifferent parameters, you would probably end up\nwith different predictions.",
    "start": "1462190",
    "end": "1468779"
  },
  {
    "text": "Similarly, if you\nchange the inputs, if you have the model, if\ndifferent people show up and they have different\ntastes, like somebody",
    "start": "1468780",
    "end": "1475590"
  },
  {
    "text": "doesn't like The Patriot,\nthat can also change the output of this model. So this model is\nbased on two things",
    "start": "1475590",
    "end": "1480953"
  },
  {
    "text": "the input of the\nindividual you're making the prediction for\nand the parameters that have been set. The parameters are\ngenerally set for everybody,",
    "start": "1480953",
    "end": "1488320"
  },
  {
    "text": "whereas each individual\ninput will have different, what we call, features,\ndifferent x's.",
    "start": "1488320",
    "end": "1495570"
  },
  {
    "text": "So putting that back into\nthe language of mathematics, logistic regression makes this\nassumption the probability",
    "start": "1495570",
    "end": "1503510"
  },
  {
    "text": "that y equals 1 given\nall the features is going to be sigmoid of z.",
    "start": "1503510",
    "end": "1509270"
  },
  {
    "text": "And in that sigmoid\nof z, we're going to add up the weighted\nsum of all the features.",
    "start": "1509270",
    "end": "1515390"
  },
  {
    "text": "Oh, wait, but there's\nthis one extra number. Oh, man, let's go back.",
    "start": "1515390",
    "end": "1522170"
  },
  {
    "text": "If a user likes\nthese three movies, I say, OK, Independence\nDay, we set that to 0.",
    "start": "1522170",
    "end": "1528500"
  },
  {
    "text": "Patriot, they like\nit, so we set it to 1. Pulp Fiction, they like\nit so we set it to 1. What is going on with that x 0?",
    "start": "1528500",
    "end": "1535519"
  },
  {
    "text": "It doesn't correspond to a\nmovie and it's always set to 1.",
    "start": "1535520",
    "end": "1540560"
  },
  {
    "text": "That's a bit of a mystery. Here's what that\nmystery is about. People realized this\nmodel does way better when",
    "start": "1540560",
    "end": "1549140"
  },
  {
    "text": "it's allowed to have a\nparticular parameter which can offset the result\nof this equation.",
    "start": "1549140",
    "end": "1558180"
  },
  {
    "text": "So if you have\nthis weighted sum, people like to have a parameter\nthat you could just add to it.",
    "start": "1558180",
    "end": "1564920"
  },
  {
    "text": "For simplicity,\nwhat we would do is we would take the\ninputs, the features",
    "start": "1564920",
    "end": "1571040"
  },
  {
    "text": "and we would always\nset a 0th value and we would always set it to 1. Because if you set\na 0th value to 1,",
    "start": "1571040",
    "end": "1578480"
  },
  {
    "text": "then your weighted sum\njust becomes a weighted sum where the first input will\nbe a 1 times by theta 0.",
    "start": "1578480",
    "end": "1587080"
  },
  {
    "text": "And 1 times theta 0 is just\ngoing to give you theta 0. It just allows you to\nadd in this intercept.",
    "start": "1587080",
    "end": "1593150"
  },
  {
    "text": "This is a detail. Obviously, it's not that\ncritical to the whole plot line. But this detail is\ngoing to be critical.",
    "start": "1593150",
    "end": "1599080"
  },
  {
    "text": "When you actually\nhave to go code this, it makes a big, big\ndifference in whether or not this model works. So the detail here\nis, yes, we're",
    "start": "1599080",
    "end": "1605680"
  },
  {
    "text": "going to make this\nassumption and it's going to allow us to\ncalculate this number which",
    "start": "1605680",
    "end": "1611380"
  },
  {
    "text": "is the weighted sum of each\ninput multiplied by a feature. We're going to squash that. But before we squash\nit, we're going",
    "start": "1611380",
    "end": "1617650"
  },
  {
    "text": "to add in some other theta. So there's a bunch of\nparameters in this model",
    "start": "1617650",
    "end": "1622990"
  },
  {
    "text": "and one of them just corresponds\nto what we call the offset. To make this easy\nin your code, you're",
    "start": "1622990",
    "end": "1629370"
  },
  {
    "text": "just going to take every input\nand you're going to add in a 1 before it.",
    "start": "1629370",
    "end": "1635220"
  },
  {
    "text": "And then that 1 will be\nmultiplied by a theta. And that will make\nthis the offset.",
    "start": "1635220",
    "end": "1640587"
  },
  {
    "text": "I think that's a\nlittle bit confusing so if somebody could\nask me a question, that would be wonderful. Yes? Do we also consider\nthat the base case?",
    "start": "1640588",
    "end": "1647799"
  },
  {
    "text": "So if all the other things\nwere 0, that would be true? Yeah, it is a bit\nof a base case.",
    "start": "1647800",
    "end": "1654830"
  },
  {
    "text": "Another terminology for\nexactly the same intuition is it biases the machine\nto either predicting",
    "start": "1654830",
    "end": "1662039"
  },
  {
    "text": "more people liking the movie\nor not liking the movie. Let's say your output is\na super popular movie.",
    "start": "1662040",
    "end": "1669240"
  },
  {
    "text": "You might want to\nbias it so that you predict that people like this\nmovie more often than not. And so you can think of\nit a bit like a base case",
    "start": "1669240",
    "end": "1675540"
  },
  {
    "text": "if everything else\nis 0, this is going to be influencing\nthe movie a lot. And generally, this\ncan either make",
    "start": "1675540",
    "end": "1680640"
  },
  {
    "text": "you more likely to\npredict 1 or less likely. Come back and get your\nmandarin after class.",
    "start": "1680640",
    "end": "1686795"
  },
  {
    "text": "[CHUCKLES] It's been a while. I'm not so confident in\nmy throwing long distance.",
    "start": "1686795",
    "end": "1692090"
  },
  {
    "text": "So anyways, one thing\nI haven't told you, this model predicts the\nprobability that y equals 1.",
    "start": "1692090",
    "end": "1699450"
  },
  {
    "text": "Where is the model that\npredicts the probability that y equals 0? You guys are far\nenough into probability",
    "start": "1699450",
    "end": "1704600"
  },
  {
    "text": "that you won't be surprised\nfor me to tell you, you don't need another\nmodel if you've got a model that\npredicts y equals 1",
    "start": "1704600",
    "end": "1711140"
  },
  {
    "text": "and if you want to know what's\nthe probability that the output class is a 0, you can just\ntake advantage of the class",
    "start": "1711140",
    "end": "1718429"
  },
  {
    "text": "that probably that y equals\n1 given x plus probability that y equals 1\ngiven x should be 1. So if you want the\nprobability that y equals 0,",
    "start": "1718430",
    "end": "1726058"
  },
  {
    "text": "if that was your assumption for\nwhat's the probability that y equals 1, probability\ny equals 0 is just going to be 1 minus that.",
    "start": "1726058",
    "end": "1734080"
  },
  {
    "text": "Moving and grooving. And it all just\ncomes down to this. This is the plot line so far.",
    "start": "1734080",
    "end": "1739840"
  },
  {
    "text": "Logistic regression\ngives us this assumption that you can figure\nout the probability that your output being\ntaken on the value 1",
    "start": "1739840",
    "end": "1747490"
  },
  {
    "text": "can be calculated using\nthis simple mathematical or computer-programmed function,\nthe sigmoid of the weighted sum",
    "start": "1747490",
    "end": "1756160"
  },
  {
    "text": "of all of our inputs. Sigmoid function\nlooks like this. And I think some people\nstarted to intuit this",
    "start": "1756160",
    "end": "1762250"
  },
  {
    "text": "but I want to be very clear. In this model, we're\ngoing to do the sigmoid",
    "start": "1762250",
    "end": "1768080"
  },
  {
    "text": "of this weighted sum. So it's worth understanding\nthe sigmoid function. And 1 thing knowing about\nthe sigmoid function",
    "start": "1768080",
    "end": "1774020"
  },
  {
    "text": "is its input's always a number. If that input is positive,\nso the thing that",
    "start": "1774020",
    "end": "1779780"
  },
  {
    "text": "goes into the sigmoid\nis greater than 0, then the output is going\nto be greater than 0.5.",
    "start": "1779780",
    "end": "1786870"
  },
  {
    "text": "So if z is greater\nthan 0, sigmoid of z is greater than 0.5.",
    "start": "1786870",
    "end": "1793090"
  },
  {
    "text": "If sigmoid of z is\ninterpreted as the probability that y equals 1, as soon\nas the input to the sigmoid",
    "start": "1793090",
    "end": "1799270"
  },
  {
    "text": "gets positive, we'll\npredict that this is a 1. As soon as the input to\nthe sigmoid is negative,",
    "start": "1799270",
    "end": "1805550"
  },
  {
    "text": "we're going to predict 0. Why? Because if the input\nto the sigmoid, the thing that goes\nin here is negative,",
    "start": "1805550",
    "end": "1813470"
  },
  {
    "text": "then the result of the\nsigmoid will be less than 0.5. And if the probability that\ny equals 1 is less than 0.5,",
    "start": "1813470",
    "end": "1820070"
  },
  {
    "text": "we're going to be predicting the\nother class label which is 0. Again, questions,\ncomments, concerns?",
    "start": "1820070",
    "end": "1826805"
  },
  {
    "text": " As I said, this is how\nwe're going to define",
    "start": "1826805",
    "end": "1833100"
  },
  {
    "text": "one neuron in a neural network. So it's worth getting\nyour head around. So if you have the\nweighted sum, why",
    "start": "1833100",
    "end": "1841970"
  },
  {
    "text": "do you even need to put it\ninto the sigmoid function if you already know\nwhether or not it's going to be above or below 0.5?",
    "start": "1841970",
    "end": "1848700"
  },
  {
    "text": "Such a good question. So just to say that back\nto you, you're like, hey, if I wanted to actually\nprogram this thing up,",
    "start": "1848700",
    "end": "1854030"
  },
  {
    "text": "could I just get\nthis weighted sum and then just check\nif it's positive? Forget the rest and if it's\npositive, I predict a 1.",
    "start": "1854030",
    "end": "1860059"
  },
  {
    "text": "If it's negative, I predict a 0? You're absolutely right\nfor the prediction.",
    "start": "1860060",
    "end": "1866419"
  },
  {
    "text": "But when it comes to the\nother part, training, this theory is going to be very\nhelpful because it will tell us how we can update.",
    "start": "1866420",
    "end": "1872210"
  },
  {
    "text": "And even though\nyou're right, you could also still\nuse this mechanism if you want to tell somebody\nnot just a 1 or a 1.",
    "start": "1872210",
    "end": "1879380"
  },
  {
    "text": "But you say, this is a 1 with\nprobability 0.99 versus this is a 1 with probability 0.55.",
    "start": "1879380",
    "end": "1885440"
  },
  {
    "text": "So you might want to give\nmore information to the user where this is still useful. And also, for the other theory,\nwe're going to need this.",
    "start": "1885440",
    "end": "1894059"
  },
  {
    "text": "So close. No one was hurt for\npeople watching online. Yes, question? ",
    "start": "1894060",
    "end": "1900510"
  },
  {
    "text": "--function on each theta\ntimes xi and some [? data? ?]",
    "start": "1900510",
    "end": "1906660"
  },
  {
    "text": "No, I don't think that would\ngive you the same answer. It's not linear? Yeah, the sigmoid function,\nto get really mathy,",
    "start": "1906660",
    "end": "1913440"
  },
  {
    "text": "is this thing we call nonlinear. So you can't do a sigmoid\nof each theta j times xj",
    "start": "1913440",
    "end": "1920023"
  },
  {
    "text": "and then add those up. That would give you\na different number. Yeah, good question. That turns out to be a really,\nreally useful thing when you",
    "start": "1920023",
    "end": "1925200"
  },
  {
    "text": "start putting these together. So what is now annoying\nwill later be useful. Question. So just zooming\nout a little bit,",
    "start": "1925200",
    "end": "1930960"
  },
  {
    "text": "it seems like this method\nis for the most part better than Naive Bayes because\nit makes the assumption that",
    "start": "1930960",
    "end": "1937950"
  },
  {
    "text": "doesn't sound as crazy. Is that usually\nthe case or does it really depend on the situation?",
    "start": "1937950",
    "end": "1943882"
  },
  {
    "text": "And if so, how can we\nknow which one is better? OK, good. So the question is, hey, Naive\nBayes made this assumption",
    "start": "1943882",
    "end": "1950250"
  },
  {
    "text": "that we thought\nwas crazy and this feels better because the\nassumption isn't as crazy, which one is better.",
    "start": "1950250",
    "end": "1955300"
  },
  {
    "text": "It turns out in most\ncases where you're either using logistic regression\nor Naive Bayes, they're often pretty similar.",
    "start": "1955300",
    "end": "1961930"
  },
  {
    "text": "And it turns out the decision\nis more often defined by, do you have things\nlike continuous inputs.",
    "start": "1961930",
    "end": "1967480"
  },
  {
    "text": "When you start to break\nout of binary values, then one tends to be more\nuseful than the other. But right now, if everything is\nbinary, they look very similar.",
    "start": "1967480",
    "end": "1976380"
  },
  {
    "text": "The Naive Bayes assumption\ngenerally is pretty good. Why wouldn't this be as good?",
    "start": "1976380",
    "end": "1982120"
  },
  {
    "text": "Why wouldn't this just blow\nthe pants off of Naive Bayes? It's still making assumption. This isn't the Naive\nBayes assumption,",
    "start": "1982120",
    "end": "1988560"
  },
  {
    "text": "but it's a pretty\nsimplistic assumption. Probabilities in the\nworld are nuanced. Maybe there's much\nmore complexity",
    "start": "1988560",
    "end": "1994590"
  },
  {
    "text": "that goes on to how\ny ends up being 1. And it's not just this\nsimple linear sum thrown",
    "start": "1994590",
    "end": "2001159"
  },
  {
    "text": "through a squashing function. Good question. Yes? ",
    "start": "2001160",
    "end": "2010440"
  },
  {
    "text": "That's right. Yeah. ",
    "start": "2010440",
    "end": "2017149"
  },
  {
    "text": "--be able to be\nnegative [INAUDIBLE] weights [INAUDIBLE]? The weights can be negative.",
    "start": "2017149",
    "end": "2022320"
  },
  {
    "text": "So the question is, how could\nyou get a negative value. If these thetas are\nnegative, then you can end up with\na negative value.",
    "start": "2022320",
    "end": "2027530"
  },
  {
    "text": "And the thetas absolutely\ncan be negative. They're not\nconstrained in any way. They could be anything\nfrom negative infinity to positive infinity.",
    "start": "2027530",
    "end": "2033080"
  },
  {
    "text": "What a good question. Let's actually write that. Let's say our\nthetas, so theta j is",
    "start": "2033080",
    "end": "2039080"
  },
  {
    "text": "going to be an element\nof any real number. So it could be negative\ninfinity to positive infinity.",
    "start": "2039080",
    "end": "2044480"
  },
  {
    "text": " A little bit more exposition. We didn't talk too much\nabout linear regression",
    "start": "2044480",
    "end": "2051210"
  },
  {
    "text": "but linear regression is\ndoing a different thing than classification. Right now we're\npredicting 1's and 0's.",
    "start": "2051210",
    "end": "2056888"
  },
  {
    "text": "Regression is generally\nwhat we call it when you predict real numbers. Classification is when you\npredict something like y",
    "start": "2056889",
    "end": "2062739"
  },
  {
    "text": "is a 1, or y is a 0, or\ny is a discrete value. Linear regression is a\nname for an algorithm that",
    "start": "2062739",
    "end": "2069060"
  },
  {
    "text": "does regression, I love it. Naive Bayes is the algorithm\nfor the first classification",
    "start": "2069060",
    "end": "2074190"
  },
  {
    "text": "that we've seen so far. And I love this question. It's using Bayes theorem. It's making this really\nnaive assumption.",
    "start": "2074190",
    "end": "2080940"
  },
  {
    "text": "It's all beautifully\nencapsulated in this name. The name of this algorithm\nis logistic regression.",
    "start": "2080940",
    "end": "2088469"
  },
  {
    "text": "And I think it's an\nawesome algorithm. I think it is an awful name.",
    "start": "2088469",
    "end": "2093620"
  },
  {
    "text": "Why is it an awful name? First of all, it's\nnot doing regression, it's doing classification.",
    "start": "2093620",
    "end": "2099500"
  },
  {
    "text": "People call it a regression\nbecause technically, it's regressing into the real\nvalue that is a probability.",
    "start": "2099500",
    "end": "2105200"
  },
  {
    "text": "But I think that's\nvery confusing. And instead of using\nthe term logistic, logistic is technically a\nparent name for a sigmoid.",
    "start": "2105200",
    "end": "2112680"
  },
  {
    "text": "I would have called it-- Chris would have called it\nthe sigmoid classification algorithm. So I just want to point this out\nthat it's not a very good name.",
    "start": "2112680",
    "end": "2120020"
  },
  {
    "text": "It's really not\ndoing regression. It is doing classification.",
    "start": "2120020",
    "end": "2125710"
  },
  {
    "text": "But of course, it's not\nthat important right now. What is important is\nthe final mystery.",
    "start": "2125710",
    "end": "2131800"
  },
  {
    "text": "If you understand this model,\nyou're most of the way there. You understand what logistic\nregression is doing,",
    "start": "2131800",
    "end": "2138010"
  },
  {
    "text": "but now you need to peel back\nto the next level of detail. You need to understand\nunder the hood this next most complicated\nmechanism, which",
    "start": "2138010",
    "end": "2145840"
  },
  {
    "text": "is where does the\nintelligence come from and how could you make\nyour logistic regression algorithm as smart as possible?",
    "start": "2145840",
    "end": "2152980"
  },
  {
    "text": "What a mystery, let's dive in. The intelligence of a\nlogistic regression model",
    "start": "2152980",
    "end": "2161130"
  },
  {
    "text": "comes from its thetas. If you took a logistic\nregression model and you gave me\nrandom thetas, it",
    "start": "2161130",
    "end": "2166980"
  },
  {
    "text": "would be awful at\nmaking predictions. I'd have no reason to trust it. But if you gave me\nperfect thetas, then",
    "start": "2166980",
    "end": "2173670"
  },
  {
    "text": "maybe you could start\nto do a good job. The intelligence\nlives in those thetas",
    "start": "2173670",
    "end": "2178680"
  },
  {
    "text": "which begs the question,\nhow are we going to get good values of theta.",
    "start": "2178680",
    "end": "2184900"
  },
  {
    "text": "And I mentioned this\nearlier, but the answer is we have a technique\nfor choosing parameters.",
    "start": "2184900",
    "end": "2191599"
  },
  {
    "text": "This is a probabilistic model. It's producing a probability. We can think about the\nlikelihood of a training data",
    "start": "2191600",
    "end": "2199150"
  },
  {
    "text": "set. And we could ask\nthe question, which values of these parameters\nwould maximize likelihood.",
    "start": "2199150",
    "end": "2205920"
  },
  {
    "text": "That's a long way of\nsaying, we can just use the parameter\nestimation techniques we've learned so far.",
    "start": "2205920",
    "end": "2211680"
  },
  {
    "text": "We can take training data\nand choose these parameters based on training data.",
    "start": "2211680",
    "end": "2217110"
  },
  {
    "text": "So we're going to do this. We're going to\ntake training data and we're going to\ntry and learn them using this idea of\nmaximum likelihood.",
    "start": "2217110",
    "end": "2223369"
  },
  {
    "text": "Remember when we first\nlearned maximum likelihood? I said, imagine we're trying\nto estimate the parameters",
    "start": "2223370",
    "end": "2229280"
  },
  {
    "text": "of a normal distribution. And I give you data points. And you can try and\nchoose the parameters that maximize the likelihood\nof those data points.",
    "start": "2229280",
    "end": "2238190"
  },
  {
    "text": "We came up with a\nnice way of doing that and that's exactly\nthe idea we're going to use for\nchoosing the parameters.",
    "start": "2238190",
    "end": "2244680"
  },
  {
    "text": "I'm going to do something a\nlittle bit different though. Today I'm going\nto focus on what's",
    "start": "2244680",
    "end": "2249790"
  },
  {
    "text": "most important for you for your\nproblem set, which is what are the results of the mathematics. Once you understand how you\ncould use those results,",
    "start": "2249790",
    "end": "2256480"
  },
  {
    "text": "then we're going to derive them. Does that sound good? So we're going to start\nwith the endpoint.",
    "start": "2256480",
    "end": "2262920"
  },
  {
    "text": "We are going to use maximum\nlikelihood estimation. We're going to start with\nthe logistic assumption",
    "start": "2262920",
    "end": "2268200"
  },
  {
    "text": "that for any data in a database,\nfor example, or a training data set, the probability that the\nlabel takes on the value 1",
    "start": "2268200",
    "end": "2276079"
  },
  {
    "text": "given the features is going to\nbe sigma a theta transpose x. That's a logistic\nregression assumption.",
    "start": "2276080",
    "end": "2283370"
  },
  {
    "text": "And of course, that\nalso means that we assume that the probability\ny equals 0 is just going to be 1 minus that.",
    "start": "2283370",
    "end": "2289010"
  },
  {
    "text": "Those are our assumptions. Based on those assumptions\nwe could call this",
    "start": "2289010",
    "end": "2295799"
  },
  {
    "text": "some measure of the\nprobability that-- or this is the probability\nthat y equals 1. Then we can take a\ntraining data set",
    "start": "2295800",
    "end": "2302490"
  },
  {
    "text": "and we can figure\nout the likelihood based on this assumption. And once you get the likelihood,\nyou can get the log likelihood.",
    "start": "2302490",
    "end": "2309589"
  },
  {
    "text": "And then what we're\ngoing to do is we're going to be\nable to figure out how we get the derivative\nof log likelihood.",
    "start": "2309590",
    "end": "2315470"
  },
  {
    "text": "Now, you're going\nto later in class understand the mathematics\nfor how we got this equation.",
    "start": "2315470",
    "end": "2320785"
  },
  {
    "text": "You'll understand\nthe mathematics for how we got this equation. But before we drive into\nany of that mathematics,",
    "start": "2320785",
    "end": "2326210"
  },
  {
    "text": "you need to understand\nthe plot line. And the plot line is, we're\ngoing to make this assumption.",
    "start": "2326210",
    "end": "2333220"
  },
  {
    "text": "Then we're going to\nsay based on data, I can say how likely\ndoes that data look under a particular\nset of parameters,",
    "start": "2333220",
    "end": "2341420"
  },
  {
    "text": "and that's this function. So if you gave me any\ndata set and you gave me a particular setting\nof parameters,",
    "start": "2341420",
    "end": "2348020"
  },
  {
    "text": "I could put those two things\ntogether through that equation number two and it will score it.",
    "start": "2348020",
    "end": "2353360"
  },
  {
    "text": "Will score your\nparameters and say, these parameters are making\nthe data look really likely or these parameters\nlook pretty bad.",
    "start": "2353360",
    "end": "2359060"
  },
  {
    "text": "I can now tell the difference\nbetween random parameters and smart parameters. Because smart parameters\nwould make training data",
    "start": "2359060",
    "end": "2365780"
  },
  {
    "text": "look really, really likely. Again, we'll\nunderstand this later. I want to make sure people\nunderstand the plot line.",
    "start": "2365780",
    "end": "2371790"
  },
  {
    "text": "This in itself is\na scoring function. It doesn't tell you\nhow to get good thetas.",
    "start": "2371790",
    "end": "2378380"
  },
  {
    "text": "So that's where the\nderivative comes in. I make the assumption. I figure out a way to\nscore your parameters.",
    "start": "2378380",
    "end": "2384380"
  },
  {
    "text": "And I say you want to\nchoose good parameters, pal? You give me the derivative. And not just any derivative,\nI want the derivative",
    "start": "2384380",
    "end": "2390830"
  },
  {
    "text": "of your score with\nrespect to every parameter you might be changing. We'll be able to calculate\nthat numerically.",
    "start": "2390830",
    "end": "2398100"
  },
  {
    "text": "We'll come up with\nan equation for it. And this equation is the thing\nwe give to our optimization",
    "start": "2398100",
    "end": "2403829"
  },
  {
    "text": "algorithm. At this point, we're\nhappy to hand this over to gradient ascent. And we say, choose\nthe values of theta",
    "start": "2403830",
    "end": "2410790"
  },
  {
    "text": "that maximize this\nscoring function. And hey, I know gradient\nascent, you really",
    "start": "2410790",
    "end": "2415980"
  },
  {
    "text": "want derivatives so I'm going\nto give you some derivatives. So at this point, we're\nready to give things over",
    "start": "2415980",
    "end": "2421380"
  },
  {
    "text": "to optimization. And optimization will start\nchanging values of thetas. It'll start coming up\nwith good values of thetas",
    "start": "2421380",
    "end": "2427020"
  },
  {
    "text": "that will eventually make\nthe score really high. A question though? Is log likelihood convex?",
    "start": "2427020",
    "end": "2432480"
  },
  {
    "text": "Is it convex? Yes, it is. It is convex so that is a very,\nvery nice property of our log likelihood function.",
    "start": "2432480",
    "end": "2439920"
  },
  {
    "text": "OK, plot line questions. [INAUDIBLE] I was\nwondering, are we",
    "start": "2439920",
    "end": "2445680"
  },
  {
    "text": "doing this because the logistic\nis a sigmoid so it only has one maximization\nbut [INAUDIBLE]??",
    "start": "2445680",
    "end": "2452829"
  },
  {
    "text": " Yeah, I mean just\nto go back, we're",
    "start": "2452830",
    "end": "2459030"
  },
  {
    "text": "trying to choose\nthese parameters. And the idea of convexity would\nsuggest that maybe there's",
    "start": "2459030",
    "end": "2464130"
  },
  {
    "text": "really easy ways to choose these\nparameters that didn't require going to gradient ascent.",
    "start": "2464130",
    "end": "2469560"
  },
  {
    "text": "And it also has\nother implications. If you end up with\ngood parameters that",
    "start": "2469560",
    "end": "2474570"
  },
  {
    "text": "are a local maxima,\ndo you believe that it could be a global maxima? And it turns out all these\nthings just end up being true.",
    "start": "2474570",
    "end": "2481060"
  },
  {
    "text": "It is a very nice\nmaximization thing. If you end up running gradient\nascent, if you get to a peak,",
    "start": "2481060",
    "end": "2488430"
  },
  {
    "text": "I can guarantee you\nit's the global peak. It's not just a peak, it will\nbe the top of the mountain.",
    "start": "2488430",
    "end": "2496530"
  },
  {
    "text": "Just wondering, say\nfor example, you have a problem where it\nmaximizes and [INAUDIBLE]??",
    "start": "2496530",
    "end": "2502980"
  },
  {
    "text": "So if it maximizes\nthe score, which we use the log\nlikelihood function for, if you find some thetas\nthat maximize the score,",
    "start": "2502980",
    "end": "2510300"
  },
  {
    "text": "they'll be good. And they'll be the best thetas. The optimization algorithm won't\nactually return you bad thetas.",
    "start": "2510300",
    "end": "2518490"
  },
  {
    "text": "Good question. So at this point, I've\ntalked a little bit about passing this equation\noff to an algorithm.",
    "start": "2518490",
    "end": "2526470"
  },
  {
    "text": "Before we get into driving\neach of these steps, I want you to see\nthe whole plot line. And the whole plot line really\nends with passing this off",
    "start": "2526470",
    "end": "2533340"
  },
  {
    "text": "to an optimization algorithm. What does that look like? Well, gradient ascent, come on\nand play, our old friend you.",
    "start": "2533340",
    "end": "2541870"
  },
  {
    "text": "Remember gradient ascent? It says, hey, if\nyou want to end up with really awesome\nparameters, we're",
    "start": "2541870",
    "end": "2548230"
  },
  {
    "text": "going to repeatedly hill climb. And by hill climb I mean\ntake your old parameter",
    "start": "2548230",
    "end": "2554320"
  },
  {
    "text": "and then we're\ngoing to figure out the derivative of your score\nwith respect to that parameter.",
    "start": "2554320",
    "end": "2559600"
  },
  {
    "text": "And we're going to multiply that\nby a step size, and add that, and that will become\nour new parameter. So we're always updating it.",
    "start": "2559600",
    "end": "2565930"
  },
  {
    "text": "If there's just\none parameter, you can imagine this is\nlike taking small steps either making the\nparameter larger",
    "start": "2565930",
    "end": "2571780"
  },
  {
    "text": "or maybe making the\nparameter smaller. If this is negative, step\nsize is always positive.",
    "start": "2571780",
    "end": "2577270"
  },
  {
    "text": "And then just slowly making\nyour way towards the ideal value of the parameter.",
    "start": "2577270",
    "end": "2582340"
  },
  {
    "text": "If there is multiple\nparameters, not a problem. We're just going to be-- each of them is going to be\nmaking its progress in sync.",
    "start": "2582340",
    "end": "2592500"
  },
  {
    "text": "I think this is nice\nto show as mathematics but I find this algorithm much\neasier to look at-- sorry,",
    "start": "2592500",
    "end": "2598790"
  },
  {
    "text": "I'm going to skip\nthat a little bit. And I'm going to\ntake this focus where",
    "start": "2598790",
    "end": "2604010"
  },
  {
    "text": "we say we want to\nupdate each parameter. We're going to use the\nidea of gradient ascent.",
    "start": "2604010",
    "end": "2611750"
  },
  {
    "text": "And gradient ascent\nrequires a derivative but I gave you what\nthe derivative was. We didn't prove\nit but I told you",
    "start": "2611750",
    "end": "2618525"
  },
  {
    "text": "that this was the\nderivative and I want to show you how we'll use it. If somebody gives\nyou the derivative",
    "start": "2618525",
    "end": "2623570"
  },
  {
    "text": "for any particular theta\nj, So the derivative of your score with\nrespect to theta j, you would plug this into here.",
    "start": "2623570",
    "end": "2630559"
  },
  {
    "text": "And this is the\nequation that tells you how you should change each of\nyour theta j's on every step.",
    "start": "2630560",
    "end": "2636720"
  },
  {
    "text": "It says, as you're trying\nto get up this mountain, on every step,\nupdate every theta.",
    "start": "2636720",
    "end": "2643320"
  },
  {
    "text": "Take the old value, calculate\nthis complicated thing. You can have Python\ndo that for you.",
    "start": "2643320",
    "end": "2649109"
  },
  {
    "text": "It'll come back as a number. And that number, when you\nmultiply by step size, we're going to use that\nto change the value.",
    "start": "2649110",
    "end": "2655320"
  },
  {
    "text": "The old value will get modified\nby whatever this derivative is. And we'll put that\nin a whole loop.",
    "start": "2655320",
    "end": "2661500"
  },
  {
    "text": "We'll loop this many\ntimes and the parameters get better, and\nbetter, and better. That's the promise\nof hill climbing.",
    "start": "2661500",
    "end": "2668530"
  },
  {
    "text": "But while it's nice to\ngive you the mathematics, I think it's a little bit\nnicer to give you pseudocode.",
    "start": "2668530",
    "end": "2674110"
  },
  {
    "text": "The very simple idea\nof logistic regression is a two-step algorithm. First you initialize\nyour parameters",
    "start": "2674110",
    "end": "2679570"
  },
  {
    "text": "and then you can optimize them. But that's too high level. Let's dive a little bit deeper.",
    "start": "2679570",
    "end": "2685560"
  },
  {
    "text": "When you're optimizing\nthem, you're going to repeat many\ntimes taking a step.",
    "start": "2685560",
    "end": "2691440"
  },
  {
    "text": "To take a step, you're\ngoing to calculate all the gradients and then every\nsingle theta is going to be--",
    "start": "2691440",
    "end": "2697920"
  },
  {
    "text": "it's going to be changed\nby the step size. This n is not the\nnumber of data points.",
    "start": "2697920",
    "end": "2703020"
  },
  {
    "text": "It's the step size\ntimes the gradient of j. We're going to take\nall those gradients",
    "start": "2703020",
    "end": "2708490"
  },
  {
    "text": "and we're going to\nstore them into numbers. The gradients will\neventually become numbers and we want to calculate\nthose actual numbers.",
    "start": "2708490",
    "end": "2715560"
  },
  {
    "text": "That's kind of a wild thing. We think about derivatives\nas being equations. But if you actually were to\nsubstitute all of your data",
    "start": "2715560",
    "end": "2723360"
  },
  {
    "text": "points and all of\nyour thetas, this wouldn't just be an equation. It would be something that\nwould turn into a single number.",
    "start": "2723360",
    "end": "2729630"
  },
  {
    "text": "The derivative would\nactually be a real value. And then we're going\nto take that real value",
    "start": "2729630",
    "end": "2735450"
  },
  {
    "text": "and store it into a variable. And we're going to use\nthat to update theta j.",
    "start": "2735450",
    "end": "2740690"
  },
  {
    "text": "Of course, this requires that\nwe can calculate that gradient. And calculate the\ngradient, you're",
    "start": "2740690",
    "end": "2746780"
  },
  {
    "text": "going to have to loop\nfor every parameter. And for every\nparameter, you're going to have to loop over\nall training examples.",
    "start": "2746780",
    "end": "2752855"
  },
  {
    "text": "And then you're going to have to\nactually update your gradient. And using the equation we\ngave before, this is a pretty",
    "start": "2752855",
    "end": "2759050"
  },
  {
    "text": "reasonable thing to do. For every training\nexample, you could calculate this term,\nsum them all together,",
    "start": "2759050",
    "end": "2765260"
  },
  {
    "text": "that's your gradient. At this point, I don't want\nyou to have memorized this. This will make much more\nsense when you sit down",
    "start": "2765260",
    "end": "2771800"
  },
  {
    "text": "to actually code it and you'll\nlook at this and be like, OK, yes, I need to\nget my gradients. We've got an equation\nfor gradients.",
    "start": "2771800",
    "end": "2777320"
  },
  {
    "text": "I can calculate the\ngradient each step point. And then I'm going to\nrepeat many times improving.",
    "start": "2777320",
    "end": "2783890"
  },
  {
    "text": "But hopefully, now I've shown\nyou the whole plot line. ",
    "start": "2783890",
    "end": "2791289"
  },
  {
    "text": "Well actually, maybe\nI'll stop here. We want to become\nsmart by getting",
    "start": "2791290",
    "end": "2798079"
  },
  {
    "text": "great parameters, that's where\nthe intelligence comes from. First, we derive a log\nlikelihood function.",
    "start": "2798080",
    "end": "2803620"
  },
  {
    "text": "That's a score. It tells you if your\nparameters are good or bad. Then we want to\nchoose parameters that make that score\nas large as possible,",
    "start": "2803620",
    "end": "2810230"
  },
  {
    "text": "so then we get the derivative. We calculate the\nderivative and then we use gradient ascent to choose\nthe values of the parameters",
    "start": "2810230",
    "end": "2818560"
  },
  {
    "text": "that maximize our score. And that uses hill\nclimbing, a.k.a. gradient ascent which requires you to be\nable to calculate derivatives.",
    "start": "2818560",
    "end": "2826160"
  },
  {
    "text": "So we needed to have figured out\nthe derivative, which we did. And then this ends\nup being Python. When you run this whole thing,\nit will repeat many times.",
    "start": "2826160",
    "end": "2833650"
  },
  {
    "text": "And at the end, it will give\nyou back good values of thetas, not just the initial\nassignment to thetas.",
    "start": "2833650",
    "end": "2839200"
  },
  {
    "text": " That's it. That's the next hardest thing.",
    "start": "2839200",
    "end": "2844740"
  },
  {
    "text": "We'll talk about those\nderivations in a second. But just to make sure we're\nfollowing on the plot line,",
    "start": "2844740",
    "end": "2850670"
  },
  {
    "text": "I want to show you this\nvery important chart. It's both meant to give\nyou insight into what",
    "start": "2850670",
    "end": "2856040"
  },
  {
    "text": "I've been talking about, but\nit's also a great debugging technique. The promise of hill climbing\nis that you might start out",
    "start": "2856040",
    "end": "2863900"
  },
  {
    "text": "with bad thetas, but over time,\nas you do this gradient ascent,",
    "start": "2863900",
    "end": "2869210"
  },
  {
    "text": "your thetas will get\nbetter and better. And better and\nbetter particularly in terms of the score\nthat we call the log",
    "start": "2869210",
    "end": "2874670"
  },
  {
    "text": "likelihood or the likelihood. So when you start\nout, you start out with random values of\nthetas and your score",
    "start": "2874670",
    "end": "2881860"
  },
  {
    "text": "is going to be pretty low. But if you took one\nstep of gradient ascent,",
    "start": "2881860",
    "end": "2887050"
  },
  {
    "text": "it will end up moving\nall of your thetas. It will depend on the\ngradient for each theta",
    "start": "2887050",
    "end": "2892250"
  },
  {
    "text": "so they could all\nmove differently. And once you get\nthose gradients, we're going take\none step and they'll",
    "start": "2892250",
    "end": "2898150"
  },
  {
    "text": "change ever so slightly. We've now gone from the\nfirst step of our algorithm to the next time\nthrough the loop.",
    "start": "2898150",
    "end": "2904840"
  },
  {
    "text": "And an important thing\nthat's going to happen is because we changed all\nthese parameters with respect",
    "start": "2904840",
    "end": "2911460"
  },
  {
    "text": "to the derivative of the score\nwith respect to each parameter, we're guaranteed that\nthe score should go up.",
    "start": "2911460",
    "end": "2918640"
  },
  {
    "text": "So the next time we calculate\nour score, it should be higher. It's not the final\nversion of our parameters,",
    "start": "2918640",
    "end": "2924610"
  },
  {
    "text": "we'd like to repeat this\ngradient ascent many steps. So again, we get the derivative\nof the score with respect",
    "start": "2924610",
    "end": "2930640"
  },
  {
    "text": "to every parameter. And they'll all\nchange once again. So now they've\nchanged twice and we get to the next iteration\nof our algorithm.",
    "start": "2930640",
    "end": "2937520"
  },
  {
    "text": "And again, we're guaranteed\nthat these changes should make the score go up. So at this point, we've\ngot a third incarnation",
    "start": "2937520",
    "end": "2946180"
  },
  {
    "text": "of our parameter\nvalues and our score is starting to look pretty good. Gradient ascent, if\nyou kept doing it,",
    "start": "2946180",
    "end": "2952390"
  },
  {
    "text": "your score should just keep\ngoing up, and up, and up. And that's what gradient\nascent is doing.",
    "start": "2952390",
    "end": "2958488"
  },
  {
    "text": "It's just giving you a way\nto change your parameters so that your score goes up. And an important debugging\nidea you can get from this",
    "start": "2958488",
    "end": "2964950"
  },
  {
    "text": "is, let's say your\nalgorithm is not working. It could happen. You could go and code this up.",
    "start": "2964950",
    "end": "2969990"
  },
  {
    "text": "You could run it. And it could not be giving\nyou the results you wanted. One thing is you might have\ngotten your derivatives wrong.",
    "start": "2969990",
    "end": "2976360"
  },
  {
    "text": "And if you get your\nderivatives wrong, it might not be the case that\nlikelihood is actually going up",
    "start": "2976360",
    "end": "2981390"
  },
  {
    "text": "over different iterations. So this is something\nthat I often used when I was implementing\nlogistic regression or neural",
    "start": "2981390",
    "end": "2987420"
  },
  {
    "text": "networks to make sure that I had\nactually coded them correctly. ",
    "start": "2987420",
    "end": "2993480"
  },
  {
    "text": "And then finally, I just\nwanted to take this moment to remind you of this one\ndetail which was, don't forget,",
    "start": "2993480",
    "end": "3000450"
  },
  {
    "text": "we still have the\nintercept term. Remember we had a theta 0? And that theta 0 was going\nto be added into our sum",
    "start": "3000450",
    "end": "3008450"
  },
  {
    "text": "before we threw it into\nthe squashing function? So before we came up with\nthe squashing function, we're going to add\nit into theta 0.",
    "start": "3008450",
    "end": "3014850"
  },
  {
    "text": "And don't forget that\nthe way we actually implement that is by making\nx0 always equal to one.",
    "start": "3014850",
    "end": "3021240"
  },
  {
    "text": "If you make x0\nalways equal to one no matter who the user is, then\nthat will be the same as adding an intercept.",
    "start": "3021240",
    "end": "3029350"
  },
  {
    "text": "So classification\nwe've already talked about so I'm going to actually\ngo pretty quickly through that. You get the probability\nthat y equals one.",
    "start": "3029350",
    "end": "3036160"
  },
  {
    "text": "If that's greater than\n0.5, you predict a 1. Otherwise, you predict a 0. For the actual\nprediction we make",
    "start": "3036160",
    "end": "3042369"
  },
  {
    "text": "we call that y hat, how cute. It's our guess at y. And that hat is making\nus look a little silly",
    "start": "3042370",
    "end": "3048155"
  },
  {
    "text": "so that you can tell\nthat we're a guess and we're not the actual value. Anyways, I don't think that's\nso surprising at this point.",
    "start": "3048155",
    "end": "3055900"
  },
  {
    "text": "So you get this value. If it's greater than\n0.5, you predict a 1.",
    "start": "3055900",
    "end": "3062120"
  },
  {
    "text": "But just to be clear. Once you've got good\nthetas, you should be able to take a new user,\nset their inputs to 0, 1,",
    "start": "3062120",
    "end": "3069755"
  },
  {
    "text": "1 for this particular user. Then you should be able\nto do your forward pass, get a value of z, squash\nit, get the probability.",
    "start": "3069755",
    "end": "3077210"
  },
  {
    "text": "If that probability is greater\nthan 0.5, then you predict a 1. Oh, you guys worked really\nhard to follow that plot line.",
    "start": "3077210",
    "end": "3084510"
  },
  {
    "text": "It's a pretty\ncomplicated plot line. There's a lot of moving pieces. ",
    "start": "3084510",
    "end": "3090150"
  },
  {
    "text": "But at this point,\nhopefully, you believe me when I say, OK,\nyou can make this model. OK, we can score it.",
    "start": "3090150",
    "end": "3097150"
  },
  {
    "text": "And if you can\nscore it, then you can choose parameters that\nmaximize that score using gradient ascent.",
    "start": "3097150",
    "end": "3102450"
  },
  {
    "text": "But what I haven't told you,\nthe missing part of the story was where did those\nequations come from.",
    "start": "3102450",
    "end": "3108480"
  },
  {
    "text": "And maybe you\ndon't need to know. If you actually have to just\ncode this up for your homework, you could skip this\nfinal part of where",
    "start": "3108480",
    "end": "3114030"
  },
  {
    "text": "do those equations come from. You can just look up the\nslides, get those equations, code it into Python, run\nit, get the right answer,",
    "start": "3114030",
    "end": "3119415"
  },
  {
    "text": "have your good times, go\nplay with your friends. But I'm going to\ngive you a reason for why you should care about\nwhere those equations come",
    "start": "3119415",
    "end": "3127150"
  },
  {
    "text": "from. Because logistic regression is\njust the start of the story. This is a good idea that\nyou can imagine is a seed.",
    "start": "3127150",
    "end": "3134530"
  },
  {
    "text": "And this seed has blossomed in\nwonderful and complicated ways. So to the extent that\nyou can understand",
    "start": "3134530",
    "end": "3139600"
  },
  {
    "text": "the seed is the\nextent to which you could understand deep learning. So we should really know all\nthe different parts, especially",
    "start": "3139600",
    "end": "3145599"
  },
  {
    "text": "if you want to either be the\nones to make this flourish or as you guys know, AI is going\nto require us to understand it.",
    "start": "3145600",
    "end": "3153520"
  },
  {
    "text": "It's having big\nimpacts on society and we need to really, really\nget our heads around what",
    "start": "3153520",
    "end": "3159220"
  },
  {
    "text": "these powerful tools are doing. For both those\nreasons, we really want to know how this black\nbox is going to be working.",
    "start": "3159220",
    "end": "3166280"
  },
  {
    "text": "So you guys ready for\njumping into this math? OK, let's do it.",
    "start": "3166280",
    "end": "3171520"
  },
  {
    "text": "So I gave you these equations. I said, this is our assumption. Based on the assumption, you\ncan get that this is the score.",
    "start": "3171520",
    "end": "3177580"
  },
  {
    "text": "And then we can derive a score\nwith respect to each parameter. But I didn't tell\nyou step by step",
    "start": "3177580",
    "end": "3183579"
  },
  {
    "text": "how you get these two values. So let's do it. How do we get that log\nlikelihood function?",
    "start": "3183580",
    "end": "3191309"
  },
  {
    "text": "As you recall, a\nvery funny thing happened when we wanted\nto do MLE on a Bernoulli.",
    "start": "3191310",
    "end": "3198660"
  },
  {
    "text": "When we wanted to do\nMLE on a Bernoulli, the first step is you've\ngot training data,",
    "start": "3198660",
    "end": "3204060"
  },
  {
    "text": "that'll be a bunch\nof 0's and 1's. Can you write how likely\nthat training data looks based on the parameters.",
    "start": "3204060",
    "end": "3210380"
  },
  {
    "text": "In the case of a Bernoulli,\nthere's only one parameter. That's the probability\nparameter p.",
    "start": "3210380",
    "end": "3215730"
  },
  {
    "text": "Do you remember Bernoulli,\nit's either 1 or 0. And it's one with probability p. If you were doing MLE\nto choose a Bernoulli,",
    "start": "3215730",
    "end": "3222060"
  },
  {
    "text": "your job was to choose a p. And MLE step by step formula,\nthe first phase of MLE",
    "start": "3222060",
    "end": "3229190"
  },
  {
    "text": "says, write down a\nlikelihood function, which often would require you to\nwrite down the probability mass",
    "start": "3229190",
    "end": "3235520"
  },
  {
    "text": "function. A Bernoulli probability mass\nfunction looks like this. But we have this issue that\nthe Bernoulli probability",
    "start": "3235520",
    "end": "3244100"
  },
  {
    "text": "mass function which says,\nthe probability of a 1 should be whatever\nyour parameter p is.",
    "start": "3244100",
    "end": "3249589"
  },
  {
    "text": "The probability of z\nshould be 1 minus p. This really simple\nthing is not derivable.",
    "start": "3249590",
    "end": "3255070"
  },
  {
    "text": "Those bar charts\nweren't derivable. So even though the Bernoulli\nis a simple concept and has a simple\nprobability mass function,",
    "start": "3255070",
    "end": "3260552"
  },
  {
    "text": "we couldn't write a likelihood\nfunction for Bernoullis that was derivable. So instead, we did\nthis super cheeky thing",
    "start": "3260552",
    "end": "3268180"
  },
  {
    "text": "where we came up with a\nderivable version of this bar chart. That derivable version of this\nbar chart looks like this.",
    "start": "3268180",
    "end": "3277130"
  },
  {
    "text": "It takes your parameter\nand raises it to y. Remember for Bernoulli,\ny can only be 0 or 1.",
    "start": "3277130",
    "end": "3282850"
  },
  {
    "text": "And then it multiplies\nit by this other term, which is 1 minus\nthe parameter raised to the power of 1 minus y.",
    "start": "3282850",
    "end": "3289300"
  },
  {
    "text": "Insanity. But insanity that we\nneed to understand. This is what people use\nin logistic regression,",
    "start": "3289300",
    "end": "3296950"
  },
  {
    "text": "and deep learning, and beyond\nfor likelihoods of a Bernoulli because this is going\nto be derivable.",
    "start": "3296950",
    "end": "3302619"
  },
  {
    "text": "This is equivalent to the bar\nchart when y equals 0 or 1. And let's just do one example\nof that to remind ourselves.",
    "start": "3302620",
    "end": "3310579"
  },
  {
    "text": "Let's take this equation\nand plug-in 1 for y. If you plug-in 1 for y, you\nget p to the power of 1.",
    "start": "3310580",
    "end": "3317410"
  },
  {
    "text": "Everybody, what's\np to the power 1? P. Oh, OK, good. So we have p, that's\ngood, multiplied",
    "start": "3317410",
    "end": "3323380"
  },
  {
    "text": "by this other scary term. Ooh. Well if y was 1,\nwhat's 1 minus y? 0.",
    "start": "3323380",
    "end": "3329230"
  },
  {
    "text": "And what's whatever\nraised to the power of 0? 1. So we have p times\n1, which gives us--",
    "start": "3329230",
    "end": "3334930"
  },
  {
    "text": "p. Oh, yeah. So this is really nice. When you put it in a 1 for\ny, it gives you back p.",
    "start": "3334930",
    "end": "3340720"
  },
  {
    "text": "So if you put it in a 1\nfor y, you get back p. And it turns out\nif you put in a 0 for y into this crazy equation,\nyou'll just get back 1 minus p.",
    "start": "3340720",
    "end": "3348370"
  },
  {
    "text": "It's a really, really\ncomplicated way of writing that bar chart. But we needed to do this\nbecause it made it derivable.",
    "start": "3348370",
    "end": "3356250"
  },
  {
    "text": "Recalling that, the\nfirst step of MLE means take this\nassumption, and now",
    "start": "3356250",
    "end": "3362640"
  },
  {
    "text": "imagine you had a\ntraining data set, and write how likely does\nthat training data set look",
    "start": "3362640",
    "end": "3368460"
  },
  {
    "text": "in the face of the current\n[? setting ?] of parameters. Score the parameters by coming\nup with a likelihood function.",
    "start": "3368460",
    "end": "3376800"
  },
  {
    "text": "Imagine that you just had one\nthing in your training data point.",
    "start": "3376800",
    "end": "3383010"
  },
  {
    "text": "The way we could score it is\nwe could use the Bernoulli. We could say, hey, take your\nsingle training data point.",
    "start": "3383010",
    "end": "3389770"
  },
  {
    "text": "And if you put that x, the\ninputs of the training data point, if you did\ntranspose of theta",
    "start": "3389770",
    "end": "3396150"
  },
  {
    "text": "and put it through a sigmoid,\nthis whole thing here, that is the probability\nthat y equals 1.",
    "start": "3396150",
    "end": "3401890"
  },
  {
    "text": "It's the parameter p of\ny, which is a Bernoulli.",
    "start": "3401890",
    "end": "3407549"
  },
  {
    "text": "This thing here, that's\nthe p of your Bernoulli. The whole thing that comes\nout of the logistic regression",
    "start": "3407550",
    "end": "3412950"
  },
  {
    "text": "equation is the probability\nthat y equals 1. And if we use that\ncontinuous derivable",
    "start": "3412950",
    "end": "3419110"
  },
  {
    "text": "version of a Bernoulli,\nprobability mass function says, the likelihood that y\ntakes on its value, which",
    "start": "3419110",
    "end": "3428170"
  },
  {
    "text": "could be a 0 or 1 for a\ndata point in your data set, given the inputs for\nthat single data point",
    "start": "3428170",
    "end": "3434800"
  },
  {
    "text": "is going to be p to the\npower of either the 1 or 0, that is y, times 1 minus p\nto the power of 1 minus the 1",
    "start": "3434800",
    "end": "3441640"
  },
  {
    "text": "or 0, that is y. So to be clear, your data\npoint would have an x.",
    "start": "3441640",
    "end": "3447220"
  },
  {
    "text": "And the x could\nbe something like, hey, remember x0 is\nalways a 1, always a 1.",
    "start": "3447220",
    "end": "3454420"
  },
  {
    "text": "But maybe there's\nthree other features and they represent whether\npeople like different movies and it could be 0, 0, 1.",
    "start": "3454420",
    "end": "3461890"
  },
  {
    "text": "And in this case, y is\ngoing to be a number 0 or 1. And theta is going\nto be a list which",
    "start": "3461890",
    "end": "3469990"
  },
  {
    "text": "could be like negative\n7, 2, 0, 3, which will be the ways that you\nwait x before you squash it",
    "start": "3469990",
    "end": "3477839"
  },
  {
    "text": "and get the probability\nthat y equals 1. And we're going to\nscore this combination. You have your data--",
    "start": "3477840",
    "end": "3483880"
  },
  {
    "text": "you have your training\ndata, which at this point is hilariously just one. How funny would it be to\ntrain a whole machine learning",
    "start": "3483880",
    "end": "3490020"
  },
  {
    "text": "algorithm with one data point? Now we want a lot\nof data points. Why am I starting with one? Because the math is easier.",
    "start": "3490020",
    "end": "3495910"
  },
  {
    "text": "So let's start with one. So if you have one\ndata point and you want to score this data, you\nwould calculate the probability",
    "start": "3495910",
    "end": "3504190"
  },
  {
    "text": "that y equals one\nbased on these thetas. So you take this theta. Transpose it with x that\ngives you a weighted sum.",
    "start": "3504190",
    "end": "3510370"
  },
  {
    "text": "Squash it. And that squash will\ngive you the probability that y equals 1.",
    "start": "3510370",
    "end": "3516099"
  },
  {
    "text": "And if you put that into\nthis equation, for y we're going to substitute\n1 because that's what the data point says.",
    "start": "3516100",
    "end": "3522070"
  },
  {
    "text": "And that means we're going\nto ignore this whole thing and we'll just be\nleft with this term. So it'll just be the\nprobability that y equals 1.",
    "start": "3522070",
    "end": "3529330"
  },
  {
    "text": "So if our data point had a\n1, this likelihood function will just return you\nback the number, which",
    "start": "3529330",
    "end": "3536170"
  },
  {
    "text": "is the probability\nyour algorithm has said that the data point would be 1.",
    "start": "3536170",
    "end": "3541600"
  },
  {
    "text": "Take a moment and talk\nto the person next to you and see if you can ask a\nquestion about why that's true. Because I do want to stop here.",
    "start": "3541600",
    "end": "3547210"
  },
  {
    "text": "This seems like if\nyou can follow this, the rest will be just math. So talk to the\nperson next to you. See if you can come\nup with a question.",
    "start": "3547210",
    "end": "3553200"
  },
  {
    "text": "What's confusing about\nthis or what could I understand more deeply. Take a minute and a half\njust to have a chat.",
    "start": "3553200",
    "end": "3558869"
  },
  {
    "start": "3558870",
    "end": "3566250"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "3566250",
    "end": "3607870"
  },
  {
    "text": "Take 20 more seconds. [SIDE CONVERSATIONS] ",
    "start": "3607870",
    "end": "3635380"
  },
  {
    "text": "OK, what are we doing here? We want to choose good thetas. MLE tells us how.",
    "start": "3635380",
    "end": "3640908"
  },
  {
    "text": "But the first thing\nyou have to do with MLE is you have to come up\nwith a scoring function and say, how good do my\nthetas look like based",
    "start": "3640908",
    "end": "3647109"
  },
  {
    "text": "on some real training data. We thought about\nthis in the case where you have just\na single data point.",
    "start": "3647110",
    "end": "3652450"
  },
  {
    "text": "And my claim for you is, if you\nhave just a single data point, you can calculate\nthis equation, which will return you back\na single number, which",
    "start": "3652450",
    "end": "3659140"
  },
  {
    "text": "will be the score of\nhow good your thetas are for that single data point. But it's a very confusing thing.",
    "start": "3659140",
    "end": "3664820"
  },
  {
    "text": "There's a lot going on here. You have logistic regression. You have interpreting the\noutput as a probability",
    "start": "3664820",
    "end": "3670840"
  },
  {
    "text": "of a Bernoulli. We have the continuous\nderivable probability mass function of the Bernoulli.",
    "start": "3670840",
    "end": "3676120"
  },
  {
    "text": "And all three of those things\nare happening at the same time. It's just so much that we\nshould ask some good questions.",
    "start": "3676120",
    "end": "3682880"
  },
  {
    "text": "Any questions come up? Yes? If we have inputs\nthat aren't binary, how do we transform\nthis [INAUDIBLE]??",
    "start": "3682880",
    "end": "3690720"
  },
  {
    "text": "Good question. So the question is if we have\ninputs that are not binary, how will this get transformed.",
    "start": "3690720",
    "end": "3695970"
  },
  {
    "text": "I have good news for you. If your inputs are\nnon-binary, you could use the exact same thing.",
    "start": "3695970",
    "end": "3701650"
  },
  {
    "text": "It's just the x's will\nhappen to be not 0's and 1's. If the output happens\nto be non-binary,",
    "start": "3701650",
    "end": "3707890"
  },
  {
    "text": "it becomes much more\ncomplicated because we're going to be\ninterpreting the output of this logistic regression as\na probability of a Bernoulli.",
    "start": "3707890",
    "end": "3715560"
  },
  {
    "text": "And so if you can have your y\ntake on non-Bernoulli values, it's going to not work.",
    "start": "3715560",
    "end": "3720810"
  },
  {
    "text": "So inputs can change to be\nanything but output we're going to say has to be 0 and 1's. Good questions.",
    "start": "3720810",
    "end": "3726360"
  },
  {
    "text": "And I owe you mandarins. I am going to keep aside the\nthree mandarins that I owe. Yes?",
    "start": "3726360",
    "end": "3732980"
  },
  {
    "text": "For Naive Bayes--\nsorry wait, actually-- For Naive Bayes,\nwe were able to do",
    "start": "3732980",
    "end": "3742390"
  },
  {
    "text": "joint distribution,\njoint probabilities of multiple random variables. Are we going to see\nan example of this",
    "start": "3742390",
    "end": "3747620"
  },
  {
    "text": "here or do we just assume\nthe y's of the vector? So in Naive Bayes, we\ncould have x be a vector.",
    "start": "3747620",
    "end": "3755780"
  },
  {
    "text": "And here you can also\nhave x be a vector. In both cases, x can\nhave lots of features.",
    "start": "3755780",
    "end": "3761300"
  },
  {
    "text": "And in both cases, the\nthing you're predicting is always just a 0 or a 1. So both of them are going to\nhave the exact same format.",
    "start": "3761300",
    "end": "3767900"
  },
  {
    "text": "x can be large, y will be\neither a single 0 or 1. And this is a little\nbit hard to see but my x is bolded\njust to try and show",
    "start": "3767900",
    "end": "3775040"
  },
  {
    "text": "that it could be a whole list. My x can be a whole\nlist of numbers, whereas my y will\neither be a 0 or 1,",
    "start": "3775040",
    "end": "3780780"
  },
  {
    "text": "which is the exact same\nformat as Naive Bayes. You are right that\nin Naive Bayes there's much more\ninterpretable way",
    "start": "3780780",
    "end": "3787339"
  },
  {
    "text": "of talking about the joint\nlikelihood of x and y, whereas this one is not\ntrying to do a joint. It's just trying to talk about\nthe probability of y given x.",
    "start": "3787340",
    "end": "3794970"
  },
  {
    "text": "So Naive Bayes is\ntrying to do something a little more complicated. That's why I asked to\nmake this assumption.",
    "start": "3794970",
    "end": "3800280"
  },
  {
    "text": "Good question. Yes? Why do we need to be concerned\nabout multicollinearity?",
    "start": "3800280",
    "end": "3806670"
  },
  {
    "text": "Why don't you need to\nbe concerned about it? Well, the idea being\ngradient ascent will choose good\nthetas even if things",
    "start": "3806670",
    "end": "3813540"
  },
  {
    "text": "are collinear or not collinear. It will still be able\nto just optimize. Let's talk about\nthat after class.",
    "start": "3813540",
    "end": "3820400"
  },
  {
    "text": "OK, yes? So basically saying that the\noutput of our machine learning",
    "start": "3820400",
    "end": "3827780"
  },
  {
    "text": "is a Bernoulli and\nit's got some p value. And we're basically\nassuming what p is.",
    "start": "3827780",
    "end": "3833299"
  },
  {
    "text": "But say in our case, we're\nsaying it's a signal function. But you could use\nsomething else? Yeah, that's exactly it.",
    "start": "3833300",
    "end": "3839120"
  },
  {
    "text": "It's just saying, OK, I'm\nassuming this y is a Bernoulli. I need a little mechanism to get\nthe probability of it being 1.",
    "start": "3839120",
    "end": "3845630"
  },
  {
    "text": "I've got logistic regression. You could have made\na different machine but logistic machine is\na machine that we chose.",
    "start": "3845630",
    "end": "3852080"
  },
  {
    "text": "And that's what we've got. And because that's what we've\ngot, because this is really a Bernoulli, we can just\nwrite a likelihood function.",
    "start": "3852080",
    "end": "3858350"
  },
  {
    "text": "Does a sigmoid just happen to\nwork really well or is there a reason that it\nworked really well?",
    "start": "3858350",
    "end": "3863510"
  },
  {
    "text": "No, it squashes nicely. [CHUCKLES] There's\nnot a deeper reason. It's a squashing function. Think of it being\npractical, not being",
    "start": "3863510",
    "end": "3870890"
  },
  {
    "text": "driven by deep neuroscience\nor something like that. Yes, question? So here, the probability\nis the transpose",
    "start": "3870890",
    "end": "3878690"
  },
  {
    "text": "of theta multiplied with x\npassed through the sigmoid.",
    "start": "3878690",
    "end": "3884540"
  },
  {
    "text": "And so that whole thing\nis to the power y, right? Yes, this is a probability. This is the parameter\np of our Bernoulli.",
    "start": "3884540",
    "end": "3893690"
  },
  {
    "text": "And why do the thetas change? That's one thing that I\nhad trouble understanding in the previous explanation\nwith each iteration?",
    "start": "3893690",
    "end": "3900570"
  },
  {
    "text": "We want the thetas to change. So first thing, we're\njust going to come up with a scoring function.",
    "start": "3900570",
    "end": "3905840"
  },
  {
    "text": "But then we're going to\nuse a scoring function to change our thetas. And we want to change the thetas\nto make the scoring function go",
    "start": "3905840",
    "end": "3911240"
  },
  {
    "text": "as large as possible. So you can think\nabout the score-- the thetas are movables. They're things that we\ncan move and your job",
    "start": "3911240",
    "end": "3917510"
  },
  {
    "text": "is to move them, and move\nthem in a good direction. So that's why they change\nbecause it will end up making this thing smarter.",
    "start": "3917510",
    "end": "3924410"
  },
  {
    "text": "But we still need to\nderive this in order to figure out exactly how\nwe should make them change. So your first claim though,\nI think was very helpful.",
    "start": "3924410",
    "end": "3931873"
  },
  {
    "text": "I think that's something I\nreally want to highlight. This whole thing,\nwe could have just called the p to our parameter.",
    "start": "3931873",
    "end": "3937490"
  },
  {
    "text": "And in fact, you\nwill sometimes see people give this whole\nthing a different symbol",
    "start": "3937490",
    "end": "3944690"
  },
  {
    "text": "to represent it because\nit's just used so much. And this equation\nwould have been a lot easier if we said\nsomething like, we call this p",
    "start": "3944690",
    "end": "3951650"
  },
  {
    "text": "hat and we just started\nusing p hat symbol any time you saw this. It would make this maybe a\nlittle bit more readable.",
    "start": "3951650",
    "end": "3958407"
  },
  {
    "text": "And some people do do that. You guys rock. Those are very good questions.",
    "start": "3958407",
    "end": "3965280"
  },
  {
    "text": "The great thing\nabout likelihood is that if you assume all\nyour data points are IID, so each data point is\nindependent of other data",
    "start": "3965280",
    "end": "3972310"
  },
  {
    "text": "points. Different from the\nNaive Bayes assumption that's assuming the\nfeatures are independent. If you assume the data\npoints are independent,",
    "start": "3972310",
    "end": "3977697"
  },
  {
    "text": "which is very reasonable,\nthen the likelihood of more than one\npoint is just going to be the product\nover many, many points",
    "start": "3977697",
    "end": "3984240"
  },
  {
    "text": "of the exact same\nfunction that we had here. So take that and just\nput it through a loop",
    "start": "3984240",
    "end": "3990270"
  },
  {
    "text": "of multiplication. So it says for every data\npoint in my data set,",
    "start": "3990270",
    "end": "3995770"
  },
  {
    "text": "we're going to calculate\nits likelihood. So this was the likelihood\nof one data point.",
    "start": "3995770",
    "end": "4001410"
  },
  {
    "text": "And we want the product of the\nlikelihood of every data point in our data set. So then we can just\nsubstitute in here.",
    "start": "4001410",
    "end": "4008290"
  },
  {
    "text": "Notice that if y was always\n1, we wouldn't need this term. But for some of our data\npoints, y will be 1.",
    "start": "4008290",
    "end": "4015569"
  },
  {
    "text": "And for other data\npoints, y will be a 0. And don't forget that\nthis is just picking out",
    "start": "4015570",
    "end": "4021150"
  },
  {
    "text": "either p or 1 minus p.",
    "start": "4021150",
    "end": "4027099"
  },
  {
    "text": "And that's the scoring function. Yes? So the little i's are\nnot actually exponents, indicating which x or\ny we're looking at?",
    "start": "4027100",
    "end": "4034030"
  },
  {
    "text": "Yes, exactly. And particularly,\nit's which data point. So here this is saying--",
    "start": "4034030",
    "end": "4039490"
  },
  {
    "text": "oops. Pen. This is not an exponent, it's\njust saying the i'th data",
    "start": "4039490",
    "end": "4045750"
  },
  {
    "text": "point. And this is the same notation\nthat they use in 221 and 229.",
    "start": "4045750",
    "end": "4051000"
  },
  {
    "text": "So I did want to start\nusing that notation here to set you guys up if you\never take those classes.",
    "start": "4051000",
    "end": "4057240"
  },
  {
    "text": "Which you don't have to,\nbut they're kind of fun. I digress. So yeah, this is saying\nyour data point has n values",
    "start": "4057240",
    "end": "4063420"
  },
  {
    "text": "and we're going to loop i being\nthe index of the curtain data point you're on. And we're going to calculate\nthis for the current data",
    "start": "4063420",
    "end": "4069570"
  },
  {
    "text": "point. This was one data point. But in the next data point,\nthis could be superscript 0",
    "start": "4069570",
    "end": "4075119"
  },
  {
    "text": "or superscript 5, let's say.  And then the next\ndata point may be",
    "start": "4075120",
    "end": "4081800"
  },
  {
    "text": "you have different\ninputs and a different y.  Now, I skipped ahead to this.",
    "start": "4081800",
    "end": "4088310"
  },
  {
    "text": "This is a scoring function,\nit would have been just fine. But if you try and program\nthis for a large data set, you'll have underflow problems.",
    "start": "4088310",
    "end": "4095130"
  },
  {
    "text": "So instead of just using\nthis scoring function, we're going to use the log of\nit because log is monotonic.",
    "start": "4095130",
    "end": "4101200"
  },
  {
    "text": "If this is a good scoring\nfunction, the log of it will also be a good\nscoring function. So let's use the log.",
    "start": "4101200",
    "end": "4108028"
  },
  {
    "text": "So at this point, I've\ngiven you the mathematics for the first\nequation that you saw, which was this is the\nscoring function for how good",
    "start": "4108029",
    "end": "4115649"
  },
  {
    "text": "your parameters are. And we're not there yet. The last thing we\nneed to do is be",
    "start": "4115649",
    "end": "4121239"
  },
  {
    "text": "able to say, OK, that's\na scoring function, give me a way to make\nmy parameters better.",
    "start": "4121240",
    "end": "4128770"
  },
  {
    "text": "Which means we need a gradient. We have to be able to derive\nthat scoring function,",
    "start": "4128770",
    "end": "4134770"
  },
  {
    "text": "that thing that can take a theta\nand say this is good or bad. And we want the derivative\nof that score with respect",
    "start": "4134770",
    "end": "4140049"
  },
  {
    "text": "to each parameter. We're going to do\nsomething brave.",
    "start": "4140050",
    "end": "4145180"
  },
  {
    "text": "Before we do it though, know\nthis thing about sigmoids. Sigmoids are not just\na squashing function.",
    "start": "4145180",
    "end": "4152920"
  },
  {
    "text": "They're a squashing function\nwith the most beautiful derivative. If you wanted to know the\nderivative of a sigmoid,",
    "start": "4152920",
    "end": "4160799"
  },
  {
    "text": "it is equal to the\nsigmoid itself times 1 minus the sigmoid. If you notice the sigmoid\nfunction has this natural",
    "start": "4160800",
    "end": "4168990"
  },
  {
    "text": "base e, and as you\nknow, natural base e is the very special\nnumber where if you",
    "start": "4168990",
    "end": "4175410"
  },
  {
    "text": "had to take its derivative\nof e to the power of x, it has that nice\nproperty of not changing.",
    "start": "4175410",
    "end": "4181469"
  },
  {
    "text": "For similar reasons,\nbecause sigmoid has the e natural base\nin it, its derivative",
    "start": "4181470",
    "end": "4187528"
  },
  {
    "text": "ends up being very, very nice. This is gorgeous.",
    "start": "4187529",
    "end": "4192750"
  },
  {
    "text": "That is another reason we\nuse sigmoid by the way. It's a squashing function with\na beautiful, easy derivative.",
    "start": "4192750",
    "end": "4199020"
  },
  {
    "text": " You now have a likelihood--\noh, before we do that, I just",
    "start": "4199020",
    "end": "4208630"
  },
  {
    "text": "want to show you guys what\nthis could look like using the sigmoid derivative before\nwe do any hard derivatives",
    "start": "4208630",
    "end": "4214810"
  },
  {
    "text": "or hard likelihood derivatives. So this is an aside. Welcome to my little aside.",
    "start": "4214810",
    "end": "4220600"
  },
  {
    "text": "I just told you sigmoid has a\nbeautiful derivative, that's in the top right. And I want us to practice\nusing the sigmoid's derivative.",
    "start": "4220600",
    "end": "4227469"
  },
  {
    "text": "And particularly, I\nwant us to calculate the derivative of this\nexpression with respect",
    "start": "4227470",
    "end": "4233500"
  },
  {
    "text": "to one of the thetas. Just as practice,\njust as warm-up.",
    "start": "4233500",
    "end": "4238790"
  },
  {
    "text": "What you should do is you\nshould use chain rule. You should say, this is a\ncomposition of doing step one.",
    "start": "4238790",
    "end": "4246680"
  },
  {
    "text": "This is step one. And then once you\ndo step two, you-- step two is to put this\nthrough the sigmoid function.",
    "start": "4246680",
    "end": "4252539"
  },
  {
    "text": "If you have a\ntwo-step process, it's a good candidate for\nsomething you can break apart using the chain rule.",
    "start": "4252540",
    "end": "4258050"
  },
  {
    "text": "Particularly, I'm\ngoing to say that z is going to be equal\nto theta transpose x.",
    "start": "4258050",
    "end": "4264650"
  },
  {
    "text": "z is just the input\nto the sigmoid. And so this is going to be\nthe same as the derivative",
    "start": "4264650",
    "end": "4271070"
  },
  {
    "text": "of sigmoid z with respect\nto z and the derivative of z with respect to theta j.",
    "start": "4271070",
    "end": "4276199"
  },
  {
    "text": "As I said, you should\npractice chain rule. You should make\nsure you know it. Watch the Khan Academy. And make sure you can\nfollow exactly this example.",
    "start": "4276200",
    "end": "4283820"
  },
  {
    "text": "Because this is the\napplication that is most important of chain rule. Now, if you want\nto plug and chug,",
    "start": "4283820",
    "end": "4293020"
  },
  {
    "text": "this derivative of sigmoid\nz with respect to z is just going to be sigmoid of\nz times 1 minus sigmoid of z.",
    "start": "4293020",
    "end": "4301240"
  },
  {
    "text": "So sigmoid of z times\n1 minus sigmoid of z. Remember, z is just theta\ntranspose x so sigmoid of z",
    "start": "4301240",
    "end": "4309010"
  },
  {
    "text": "is that. And 1 minus sigmoid\nof z is that. So that is just that\nfirst expression.",
    "start": "4309010",
    "end": "4314920"
  },
  {
    "text": "The derivative of sigmoid\nof z with respect to z is just sigmoid z times\n1 minus sigmoid z, done.",
    "start": "4314920",
    "end": "4321159"
  },
  {
    "text": "And then we need\nthis expression. What's the derivative of z with\nrespect to a particular theta",
    "start": "4321160",
    "end": "4326660"
  },
  {
    "text": "j? And I'll tell you\nthe answer right now but let's understand why.",
    "start": "4326660",
    "end": "4331750"
  },
  {
    "text": "If you're deriving this\nexpression with respect to a particular theta, let's\nsay you're deriving this.",
    "start": "4331750",
    "end": "4338720"
  },
  {
    "text": "So the derivative of theta\ntranspose x with respect to theta 5.",
    "start": "4338720",
    "end": "4345630"
  },
  {
    "text": "Recall what this is. It's theta 0 times x0,\nplus theta 1 times x1,",
    "start": "4345630",
    "end": "4351840"
  },
  {
    "text": "plus theta 2 times x2,\nwhere does theta 5 show up? Only times x5.",
    "start": "4351840",
    "end": "4358170"
  },
  {
    "text": "The only part of this term\nwhere theta 5 shows up is when it's multiplied by\nx5, that's the only place.",
    "start": "4358170",
    "end": "4364540"
  },
  {
    "text": "So if you derive this whole\nthing with respect to theta 5, it would be the same as\nderiving theta 5 times",
    "start": "4364540",
    "end": "4372750"
  },
  {
    "text": "x5 with respect to theta 5. And if you're driving\nthis simple multiplication",
    "start": "4372750",
    "end": "4378930"
  },
  {
    "text": "with respect to\ntheta 5, what is it? [INAUDIBLE] Oh, you guys said it so timidly.",
    "start": "4378930",
    "end": "4384340"
  },
  {
    "text": "Say it loud. I want to hear it. People on recording want\nto hear your voices. X5. Yeah, that's just x5.",
    "start": "4384340",
    "end": "4391329"
  },
  {
    "text": "More generally, if you're\nderiving this with respect to theta j, it will be xj.",
    "start": "4391330",
    "end": "4396630"
  },
  {
    "text": "So if j was 5, it would be x5. If j was 3, it would be x3.",
    "start": "4396630",
    "end": "4402210"
  },
  {
    "text": "So a simpler way of writing\nthe exact same thing would be",
    "start": "4402210",
    "end": "4409060"
  },
  {
    "text": "is you could have\ndeclassified this to be y hat and then this is just sigmoid\nof z, times 1 minus z,",
    "start": "4409060",
    "end": "4416559"
  },
  {
    "text": "times xj, which is the same\nas this expression times 1 minus this expression times xj.",
    "start": "4416560",
    "end": "4422890"
  },
  {
    "text": "This is just a\nreally cleaner way of writing what we derived\nbefore, where y hat is",
    "start": "4422890",
    "end": "4428020"
  },
  {
    "text": "equal to this sigmoid of z. So sigmoid of z, times\n1 minus sigmoid of z,",
    "start": "4428020",
    "end": "4433900"
  },
  {
    "text": "times xj, that is\nthe derivative. OK, we don't have time\nfor pedagogical pause.",
    "start": "4433900",
    "end": "4439480"
  },
  {
    "text": "We took the question before. Are you guys ready?",
    "start": "4439480",
    "end": "4444619"
  },
  {
    "text": "This is Stanford,\nwe can do this. That is the thing\nthat I want you",
    "start": "4444620",
    "end": "4450770"
  },
  {
    "text": "to derive in terms of theta j. [VOCALIZING DRAMATIC BEAT]",
    "start": "4450770",
    "end": "4456270"
  },
  {
    "text": " OK, just go and derive it. [EXCITED SQUEAL] We can do this.",
    "start": "4456270",
    "end": "4463014"
  },
  {
    "text": "[CHUCKLES] Here are\na couple pro tips. This is going to\nbe the derivative",
    "start": "4463015",
    "end": "4469370"
  },
  {
    "text": "of a sum of the logs, right? So this is the\nthing we're deriving and it's got a big sum here.",
    "start": "4469370",
    "end": "4474690"
  },
  {
    "text": "Let's start by just doing\nthe derivative of one of these inner things. Because the derivative of sum\nis just a sum of derivatives.",
    "start": "4474690",
    "end": "4481080"
  },
  {
    "text": "So if you can do the\nderivative of the inner part, your life will\nbecome really easy. The derivative of sums is\nthe sum of derivatives.",
    "start": "4481080",
    "end": "4486902"
  },
  {
    "text": "Just tell me the derivative\nof this inner part. So then my question becomes\na little bit easier, can you derive this?",
    "start": "4486902",
    "end": "4492050"
  },
  {
    "text": "And particularly, any time\nI see a sigmoid of theta",
    "start": "4492050",
    "end": "4497270"
  },
  {
    "text": "transpose x, every\ntime I see this thing, I'm going to call it y hat\nbecause that notation makes",
    "start": "4497270",
    "end": "4503989"
  },
  {
    "text": "things so much easier to read. And if you call that\ny hat, then this",
    "start": "4503990",
    "end": "4510239"
  },
  {
    "text": "is going to be just derive\nthis with respect to a theta j. You're like, where\nare all my thetas?",
    "start": "4510240",
    "end": "4516600"
  },
  {
    "text": "Oh, don't forget the thetas\nare actually in the y hats because the y hats are\nactually this expression. So they're still there\nbut that's the derivative",
    "start": "4516600",
    "end": "4523410"
  },
  {
    "text": "we're trying to do. Now, derive this inner thing\nwith respect to theta j",
    "start": "4523410",
    "end": "4531180"
  },
  {
    "text": "where I'm using this\nshorthand for y hat, which is sigmoid of theta transpose x.",
    "start": "4531180",
    "end": "4539790"
  },
  {
    "text": "Idea of chain rule is this. There are no thetas here.",
    "start": "4539790",
    "end": "4545190"
  },
  {
    "text": "All the thetas are\nhidden in these y hats because the y hats are\nmy shorthand for sigmoid",
    "start": "4545190",
    "end": "4550770"
  },
  {
    "text": "of theta transpose x. You should not try and do\nthis derivative straight. Instead, you should\nuse chain rule.",
    "start": "4550770",
    "end": "4557730"
  },
  {
    "text": "You should say, all of\nmy thetas are in y hat. So I should derive that\nwhole expression with respect",
    "start": "4557730",
    "end": "4565679"
  },
  {
    "text": "to y hat. And then I should derive y hat\nwith respect to the theta j. Chain rule is decomposition\nfor derivatives.",
    "start": "4565680",
    "end": "4573780"
  },
  {
    "text": "It allows you to take a big\nderivative and do it in parts. It says, OK, just derive\nthings with respect",
    "start": "4573780",
    "end": "4579210"
  },
  {
    "text": "to y hat, then y hat\nwith respect to theta j. Do those separately\nand multiply them.",
    "start": "4579210",
    "end": "4584650"
  },
  {
    "text": "So let's start out with\nwhat is the derivative of y hat with respect to theta j.",
    "start": "4584650",
    "end": "4591570"
  },
  {
    "text": "I have good news for you. If y hat is this expression,\nderiving it with respect",
    "start": "4591570",
    "end": "4597880"
  },
  {
    "text": "to theta j is the thing\nwe did in our warm-up. We actually figured this out it\nwas sigmoid of theta transpose",
    "start": "4597880",
    "end": "4604389"
  },
  {
    "text": "x, times 1 minus sigmoid\ntheta transpose x, times xj. They just wrote the\nexact same thing here.",
    "start": "4604390",
    "end": "4610210"
  },
  {
    "text": "Don't forget the y hat was\nmy new shorthand for writing sigmoid of theta transpose j. Hey, we're halfway there.",
    "start": "4610210",
    "end": "4616460"
  },
  {
    "text": "Now all you have to do is derive\nthat expression with respect to y hat.",
    "start": "4616460",
    "end": "4622650"
  },
  {
    "text": "And remember, you're\nusing y hat as a variable. At this point you can forget\nthat y hat equals this at all",
    "start": "4622650",
    "end": "4631570"
  },
  {
    "text": "and just say what\nis the derivative of this expression with respect\nto some variable called y hat.",
    "start": "4631570",
    "end": "4637300"
  },
  {
    "text": "Well, if you take\nthis first term, what's the derivative of y log\ny hat with respect to y hat?",
    "start": "4637300",
    "end": "4644780"
  },
  {
    "text": "Well, this doesn't have y hat\nso it'll just come in as a term. And log of y hat derive with\nrespect to y hat is just 1",
    "start": "4644780",
    "end": "4652610"
  },
  {
    "text": "over y hat. And same thing here. Log of 1 minus y hat will just\nbecome 1 over 1 minus y hat.",
    "start": "4652610",
    "end": "4659660"
  },
  {
    "text": "But because there's a little\nthing going on inside the log, you have to derive\nthat respect to y hat",
    "start": "4659660",
    "end": "4664700"
  },
  {
    "text": "and you get a negative sign. So this positive became a\nnegative sign and this term just came out here.",
    "start": "4664700",
    "end": "4669739"
  },
  {
    "text": "This whole derivative becomes\nthis term, which is not so bad.",
    "start": "4669740",
    "end": "4675800"
  },
  {
    "text": "And we're done. We just did this\ncrazy hard derivative. It turns out by using\nchain rule and breaking",
    "start": "4675800",
    "end": "4681460"
  },
  {
    "text": "into the derivative\nof loss with respect to y hat and derivative y\nhat with respect to theta, it becomes not so bad.",
    "start": "4681460",
    "end": "4687829"
  },
  {
    "text": "You could simplify\nthis a little bit if you actually did the math. But that's not as\nimportant as knowing",
    "start": "4687830",
    "end": "4692980"
  },
  {
    "text": "how to do the derivatives. Now if you were to\ndo this as the sum--",
    "start": "4692980",
    "end": "4699630"
  },
  {
    "text": "we skipped this part. We just did the\nderivative of the inside. But this will just\nbecome the sum of each of those derivatives.",
    "start": "4699630",
    "end": "4705460"
  },
  {
    "text": "So we did this derivative. And then you're just going to\nsum that over for all your data points, which leaves us\nat [VOCALIZING] you get",
    "start": "4705460",
    "end": "4714120"
  },
  {
    "text": "the derivative of your\nscoring function with respect to each of the thetas,\nwhich brings us to the end of our story.",
    "start": "4714120",
    "end": "4720570"
  },
  {
    "text": "You guys know\nlogistic regression. You guys understand\nthe assumptions. You guys understand\nhow we score it.",
    "start": "4720570",
    "end": "4725640"
  },
  {
    "text": "And now you understand\nhow you could derive the score with respect\nto each theta j which you then put into gradient ascent.",
    "start": "4725640",
    "end": "4731699"
  },
  {
    "text": "Wow, you work so hard and\nI have a treat for you. Come back on Wednesday\nand I will tell you",
    "start": "4731700",
    "end": "4737490"
  },
  {
    "text": "how this becomes the blossom\nthat is deep learning. We will understand deep\nlearning and the extent that we understand\nlogistic regression.",
    "start": "4737490",
    "end": "4743915"
  },
  {
    "text": "Have a fantastic day. Get started early\non problem set 6. I appreciate you guys so much. Come back on Wednesday. Cheers, CS109.",
    "start": "4743915",
    "end": "4750610"
  },
  {
    "start": "4750610",
    "end": "4756000"
  }
]