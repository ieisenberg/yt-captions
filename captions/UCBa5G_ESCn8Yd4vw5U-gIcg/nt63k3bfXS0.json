[
  {
    "text": "Hey, morning everyone. Welcome back. Um, so last week you heard about uh,",
    "start": "3470",
    "end": "11360"
  },
  {
    "text": "logistic regression and um, uh, generalized linear models.",
    "start": "11360",
    "end": "16470"
  },
  {
    "text": "And it turns out all of the learning algorithms we've been learning about so far are called discriminative learning algorithms,",
    "start": "16470",
    "end": "22770"
  },
  {
    "text": "which is one big bucket of learning algorithms. And today um, what I'd like to do is share with you how generative learning algorithms work.",
    "start": "22770",
    "end": "30600"
  },
  {
    "text": "Um, and in particular you learned about Gaussian discriminant analysis so by the end of the day, you will know how to implement this.",
    "start": "30600",
    "end": "35910"
  },
  {
    "text": "And it turns out that uh, compared to say logistic regression for classification, GDA is actually a um,",
    "start": "35910",
    "end": "43350"
  },
  {
    "text": "simpler and maybe more computationally efficient algorithm to implement ah, in some cases. So um, and it sometimes works better if you have uh,",
    "start": "43350",
    "end": "51650"
  },
  {
    "text": "very small data sets sometimes with some caveats. Um, and there was a helpful comparison between generative learning algorithms,",
    "start": "51650",
    "end": "58250"
  },
  {
    "text": "which is a new class of algorithms you hear about today, versus discriminative learn- learning algorithms. And then we'll talk about naive Bayes and how you can use that to uh,",
    "start": "58250",
    "end": "66610"
  },
  {
    "text": "build a spam filter, for example. Okay? So um, we'll use binary classification as the motivating example for today.",
    "start": "66610",
    "end": "77345"
  },
  {
    "text": "And um, if you have a data set that looks like this with two classes,",
    "start": "77345",
    "end": "84330"
  },
  {
    "text": "then what a discriminative learning algorithm, like logistic regression would do, is use gradient descent to search for",
    "start": "84330",
    "end": "91759"
  },
  {
    "text": "a line that separates the positive-negative examples, right? So if you randomish - randomly initialize parameters,",
    "start": "91760",
    "end": "98150"
  },
  {
    "text": "maybe starts with some digital boundary like that and over the course of gradient descent, you know,",
    "start": "98150",
    "end": "103880"
  },
  {
    "text": "the line migrates or evolves until you get maybe a line like that, that separates the positive and negative examples.",
    "start": "103880",
    "end": "109700"
  },
  {
    "text": "And um, logistic regression is really searching for a line, searching for a decision boundary that separates the positive and negative examples.",
    "start": "109700",
    "end": "118160"
  },
  {
    "text": "Um, and so if this was the uh, malignant tumors [NOISE] and the benign tumors example,",
    "start": "118160",
    "end": "125810"
  },
  {
    "text": "right, that's - that's what logistic regression would do. Now, there's a different class of algorithm which isn't searching for this separation,",
    "start": "125810",
    "end": "134840"
  },
  {
    "text": "which isn't trying to maximize the likelihood that you - the way you saw last week, which is um,",
    "start": "134840",
    "end": "140500"
  },
  {
    "text": "here's an alternative, just call it generative learning algorithm; which is rather than looking at two classes and trying to find the separation.",
    "start": "140500",
    "end": "147780"
  },
  {
    "text": "Instead, the algorithm is going to look at the classes one at a time. First, we'll look at all of the malignant tumors, right?",
    "start": "147780",
    "end": "154215"
  },
  {
    "text": "In the cancer example and try to build a model for what malignant tumors look like.",
    "start": "154215",
    "end": "159239"
  },
  {
    "text": "So you might say, ah, it looks like all the malignant tumors um, roughly [NOISE] all the malignant tumors roughly live in that ellipse.",
    "start": "159240",
    "end": "169925"
  },
  {
    "text": "And then you look at all the benign tumors in isolation and say, ah, it looks like all the benign tumors roughly live in that ellipse.",
    "start": "169925",
    "end": "178115"
  },
  {
    "text": "And then at classification time, if there's a new patient in your office with those features, uh,",
    "start": "178115",
    "end": "186170"
  },
  {
    "text": "it would then look at this new patient and compare it to the malignant tumor model compared to the benign tumor model and then say,",
    "start": "186170",
    "end": "195320"
  },
  {
    "text": "in this case, ah, it looks like this one. Looks a lot more like the benign tumors I had previously seen, so we're gonna classify that as a benign tumor.",
    "start": "195320",
    "end": "202400"
  },
  {
    "text": "Okay? So um, rather than looking at both classes simultaneously and searching for a way to separate them,",
    "start": "202400",
    "end": "211910"
  },
  {
    "text": "a generative learning algorithm, uh, instead builds a model of what each of the classes looks like,",
    "start": "211910",
    "end": "218360"
  },
  {
    "text": "kind of almost in isolation, with some details we'll learn about later. And then at test time uh,",
    "start": "218360",
    "end": "224150"
  },
  {
    "text": "it evaluates a new example against the benign model, evaluates against the malignant model and tries to see which of",
    "start": "224150",
    "end": "230690"
  },
  {
    "text": "the two models it matches more closely against. So let's formalize this.",
    "start": "230690",
    "end": "236450"
  },
  {
    "text": "Um, a discriminative learning [NOISE] algorithm",
    "start": "236450",
    "end": "241760"
  },
  {
    "text": "learns P of y given x, right?",
    "start": "241760",
    "end": "250385"
  },
  {
    "text": "Um, or uh, what it learns um,",
    "start": "250385",
    "end": "256669"
  },
  {
    "text": "[NOISE] right?",
    "start": "256670",
    "end": "262880"
  },
  {
    "text": "Some mapping [NOISE] from x to y directly. You know, as I learn- Or it can learn,",
    "start": "262880",
    "end": "268785"
  },
  {
    "text": "I think Annan briefly talked about the Perceptron algorithm, it's helpful to support vector machines later. But learns a function mapping from x to the labels directly.",
    "start": "268785",
    "end": "276700"
  },
  {
    "text": "So that's a discriminative learning algorithm. We're trying to discriminate between positive and negative classes. [NOISE] In contrast, a generative learning algorithm,",
    "start": "276700",
    "end": "285320"
  },
  {
    "text": "[NOISE] it learns P",
    "start": "285320",
    "end": "294680"
  },
  {
    "text": "of um, x given y.",
    "start": "294680",
    "end": "301050"
  },
  {
    "text": "So this says, what are the features like, [NOISE] given the class, right?",
    "start": "301050",
    "end": "311015"
  },
  {
    "text": "So um, instead P of y given x, we're gonna learn p of x given y.",
    "start": "311015",
    "end": "316520"
  },
  {
    "text": "So in other words, given that a tumor is malignant, what are the features likely gonna be like? Or given the tumor's benign,",
    "start": "316520",
    "end": "322280"
  },
  {
    "text": "what are the features x gonna be like? Okay? And then as- and then they'll also- generative learning algorithm,",
    "start": "322280",
    "end": "329775"
  },
  {
    "text": "will also learn P of y. So this is a- this is also called the class prior to be this probability, I guess.",
    "start": "329775",
    "end": "337485"
  },
  {
    "text": "It's called a class prior. It's just- when the patient walks into your office, before you've even examined them,",
    "start": "337485",
    "end": "342800"
  },
  {
    "text": "before you've even seen them, what are the odds that their tumor is malignant versus benign, right? Before you see any features, okay?",
    "start": "342800",
    "end": "350254"
  },
  {
    "text": "And so using Bayes' Rule, [NOISE] if you can build a model for P of x given y and for P of y,",
    "start": "350255",
    "end": "361680"
  },
  {
    "text": "um, if- you know, if you can calculate numbers for both of these quantities then using Bayes' rule,",
    "start": "361680",
    "end": "367100"
  },
  {
    "text": "when you have a new test example [NOISE] with features x, you can then calculate the chance of y being equal to",
    "start": "367100",
    "end": "374120"
  },
  {
    "text": "1 as this, [NOISE] right?",
    "start": "374120",
    "end": "385970"
  },
  {
    "text": "Where P of x by the - [NOISE] okay?",
    "start": "385970",
    "end": "398510"
  },
  {
    "text": "[NOISE] Um, and so",
    "start": "398510",
    "end": "408000"
  },
  {
    "text": "if you learn this term, P of x given y, then you can plug that in here, right?",
    "start": "408000",
    "end": "418050"
  },
  {
    "text": "And if you've also learned this term P of y, you can plug that in here.",
    "start": "418050",
    "end": "423794"
  },
  {
    "text": "Right. Um, and so P of x in the denominators,",
    "start": "423795",
    "end": "430860"
  },
  {
    "text": "goes into denominator, okay? So if you've learned both- both of those terms in the red square and in the orange square,",
    "start": "430860",
    "end": "437845"
  },
  {
    "text": "you could plug it into all of those terms and therefore use Bayes' rule to calculate P of y equals 1, given x.",
    "start": "437845",
    "end": "445350"
  },
  {
    "text": "So given the new patient with features x, you could use this formula to calculate what's the chance that a tumor is malignant.",
    "start": "445350",
    "end": "451115"
  },
  {
    "text": "If you've estimated you know these - these two quantities in the red and in the orange circles.",
    "start": "451115",
    "end": "458180"
  },
  {
    "text": "Okay? So um, [NOISE] that's the framework we'll use to build generative learning algorithms.",
    "start": "458180",
    "end": "464990"
  },
  {
    "text": "And in fact, today you see two examples of generative learning algorithms. One for continuous value features,",
    "start": "464990",
    "end": "471034"
  },
  {
    "text": "which is used for things like the tumor classification and one for discrete features, which uh, you can use for building,",
    "start": "471035",
    "end": "477780"
  },
  {
    "text": "like in email spam, for example, right? Or - or I don't know. Or If you want to download Twitter things and see how positive or negative a sentiment on Twitter is or something.",
    "start": "477780",
    "end": "486620"
  },
  {
    "text": "right? Well we'll have a natural language processing example later. So um, let's talk about Gaussian discriminant analysis.",
    "start": "486620",
    "end": "497460"
  },
  {
    "text": "[NOISE] GDA.",
    "start": "497460",
    "end": "504210"
  },
  {
    "text": "Um, so uh,",
    "start": "504210",
    "end": "510190"
  },
  {
    "text": "let's develop this model, assuming that the features x are continuous values.",
    "start": "514190",
    "end": "519210"
  },
  {
    "text": "And when we develop um, generative learning algorithms, I'm gonna use x and Rn.",
    "start": "519210",
    "end": "525450"
  },
  {
    "text": "So you know, I'm gonna drop the x 0 equals 1, convention.",
    "start": "525450",
    "end": "530670"
  },
  {
    "text": "So I'm not gonna- we're not gonna need that extra x equals 1. So x is now Rn rather than Rn plus 1.",
    "start": "530670",
    "end": "538985"
  },
  {
    "text": "And the key assumption in Gaussian discriminant analysis is, we're going to assume that P of x given y",
    "start": "538985",
    "end": "547970"
  },
  {
    "text": "[NOISE] is distributed Gaussian, right?",
    "start": "547970",
    "end": "553589"
  },
  {
    "text": "In other words conditioned on the tumors being malignant, the distribution of the features is Gaussian.",
    "start": "553590",
    "end": "559385"
  },
  {
    "text": "The other features like uh, size of the- size of the tumor, the- the cell adhesion or whatever features you use to measure a tumor um,",
    "start": "559385",
    "end": "566750"
  },
  {
    "text": "and condition on it being benign, the distribution is also Gaussian. So um, actually, how many of you are familiar with the multivariate Gaussian?",
    "start": "566750",
    "end": "575405"
  },
  {
    "text": "Raise your hand if you are. Like half of you? One-third? No. Two-fifths? Okay. Cool. Alright. How many",
    "start": "575405",
    "end": "581150"
  },
  {
    "text": "of you are familiar about a uni-variate, like a single dimensional Gaussian? Okay. Cool. Almost everyone. All right.",
    "start": "581150",
    "end": "586595"
  },
  {
    "text": "Cool. So let me- let me just go through what is a multivariate Gaussian distribution. So the Gaussian is this familiar bell-shaped curve.",
    "start": "586595",
    "end": "594170"
  },
  {
    "text": "A multivariate Gaussian is the generalization of this familiar bell-shaped curve over a 1-dimensional random",
    "start": "594170",
    "end": "600410"
  },
  {
    "text": "variable to multiple random variables at the same time to- to- to vector value random variables rather than a uni-variate random variable.",
    "start": "600410",
    "end": "607655"
  },
  {
    "text": "So um, if z, [NOISE] this is due to Gaussian, with some mean vector mu and some covariance matrix sigma um,",
    "start": "607655",
    "end": "618615"
  },
  {
    "text": "so if z is in Rn then mu would be Rn as well.",
    "start": "618615",
    "end": "626815"
  },
  {
    "text": "And sigma, the covariance matrix, will be n by n. So z is two-dimensional, mu is two-dimensional and sigma is two-dimensional.",
    "start": "626815",
    "end": "633365"
  },
  {
    "text": "And the expected value of z is equal to um, the mean.",
    "start": "633365",
    "end": "639520"
  },
  {
    "text": "And the um, covariance of z, [NOISE] if you're familiar with multivariate co-variances,",
    "start": "639520",
    "end": "648410"
  },
  {
    "text": "uh, this is the formula. Right. Um, and this simplifies,",
    "start": "648410",
    "end": "654735"
  },
  {
    "text": "we show in the lecture notes. You can get this in the lecture notes. [NOISE] So you- and uh,",
    "start": "654735",
    "end": "662805"
  },
  {
    "text": "following sometimes semi-standard convention, I'm sometimes gonna omit the square brackets.",
    "start": "662805",
    "end": "667900"
  },
  {
    "text": "So instead of writing the expected value of z, meaning the mean of z, sometimes I just write to this e, z right?",
    "start": "667900",
    "end": "673840"
  },
  {
    "text": "And omit- omit the square brackets to simplify the notation a bit. Okay? And the derivation from this step to this step is given in the lecture notes.",
    "start": "673840",
    "end": "682645"
  },
  {
    "text": "Um, and so, well, [NOISE] the probability density function for a Gaussian looks like this.",
    "start": "682645",
    "end": "690760"
  },
  {
    "text": "[NOISE]",
    "start": "690760",
    "end": "696450"
  },
  {
    "text": "And this is one of those formulas that, I don't know. When you're implementing these algorithms you use it over and over.",
    "start": "696450",
    "end": "704145"
  },
  {
    "text": "But what I've seen for a lot of people is al- almost no one- well, very few people start their machine learning and memorize this formula.",
    "start": "704145",
    "end": "710910"
  },
  {
    "text": "Just look at it every time you need it. I've used it so many times I seem to have it seared in my brain by now, but most people don't even- when you've used it enough,",
    "start": "710910",
    "end": "718620"
  },
  {
    "text": "you- you- you end up memorizing it. But let me show you some pictures of what this looks like since I think that would,",
    "start": "718620",
    "end": "724770"
  },
  {
    "text": "um, that might be more useful. So the Multivariate Gaussian density has two parameters; Mu and Sigma.",
    "start": "724770",
    "end": "733395"
  },
  {
    "text": "They control the mean and the variance of this density.",
    "start": "733395",
    "end": "738780"
  },
  {
    "text": "Okay? So this is a picture of the Gaussian density. Um, this is a two-dimensional Gaussian bump.",
    "start": "738780",
    "end": "746115"
  },
  {
    "text": "And for now, I've set the mean parameter to 0. So Mu is a two dimensional parameter,",
    "start": "746115",
    "end": "752190"
  },
  {
    "text": "it's uh, it's 0, 0, which is why this Gaussian bump is centered at 0.",
    "start": "752190",
    "end": "758580"
  },
  {
    "text": "Um, and the Co-variance matrix Sigma is the identity,",
    "start": "758580",
    "end": "765325"
  },
  {
    "text": "um, i- i- i - is the identity matrix. So uh, so you know, well,",
    "start": "765325",
    "end": "770820"
  },
  {
    "text": "so- so you've have this standard- this is also called the standard Gaussian distribution which means 0 and covariance equals to the identity.",
    "start": "770820",
    "end": "778290"
  },
  {
    "text": "Now, I'm gonna take the covariance matrix and shrink it, right? So take a covariance matrix and multiply it by a number less than 1.",
    "start": "778290",
    "end": "784904"
  },
  {
    "text": "That should shrink the variance- reduce the variability of distributions. If I do that, the density um,",
    "start": "784905",
    "end": "792390"
  },
  {
    "text": "the p- probability density function becomes taller. Uh, this- this is a probability density function.",
    "start": "792390",
    "end": "798090"
  },
  {
    "text": "So it always integrates to 1, right? The area under the curve, you know, is- is 1. And so by reducing the covariance from the identity to 0.6 times the identity,",
    "start": "798090",
    "end": "808275"
  },
  {
    "text": "it reduces the spread of the Gaussian density, um, but it also makes it tall as a result, because,",
    "start": "808275",
    "end": "813975"
  },
  {
    "text": "you know, the area under the curve must integrate to 1. Now let's make it fatter.",
    "start": "813975",
    "end": "819135"
  },
  {
    "text": "Let's make the covariance two times the identity. Then you end up with a wider distribution where the values of",
    "start": "819135",
    "end": "826395"
  },
  {
    "text": "um- I guess the axes here, this would be the z1 and the z2 axis; the two-dimensional Gaussian density, right?",
    "start": "826395",
    "end": "833070"
  },
  {
    "text": "Increases the variance of the density. So let's go back to a standard Gaussian, uh, covariance equal 1, 1.",
    "start": "833070",
    "end": "839565"
  },
  {
    "text": "Now, let's try fooling around with the off-diagonal entries. Um, I'm gonna- So right now,",
    "start": "839565",
    "end": "845040"
  },
  {
    "text": "the off diagonal entries are 0, right? So in this Gaussian density, the off-diagonal elements are 0, 0.",
    "start": "845040",
    "end": "851780"
  },
  {
    "text": "Let's increase that to 0.5 and see what happens. So if you do that, then the Gaussian density,",
    "start": "851780",
    "end": "858110"
  },
  {
    "text": "uh, hope you can see see the change, right? It goes from this round shape to this slightly narrower thing. Let's increase that further to 0.8, 0.8.",
    "start": "858110",
    "end": "865320"
  },
  {
    "text": "Then the density ends up looking like that, um, where now, it's more likely that z1 now- z1 and z2 are positively correlated.",
    "start": "865320",
    "end": "875100"
  },
  {
    "text": "Okay? So let's go through all of these plots. But now looking at contours of these Gaussian densities instead of these 3-D bumps.",
    "start": "875100",
    "end": "882120"
  },
  {
    "text": "So uh, this is the contours of the Gaussian density when the covariance matrix",
    "start": "882120",
    "end": "888675"
  },
  {
    "text": "is the identity matrix and I apologize the aspect ratio. These are supposed to be perfectly round circles",
    "start": "888675",
    "end": "893850"
  },
  {
    "text": "but the aspect ratio makes this look a little bit fatter, but this is supposed to be perfectly round circles.",
    "start": "893850",
    "end": "899115"
  },
  {
    "text": "Um, and so, uh, when, uh, the covariance matrix is the identity matrix,",
    "start": "899115",
    "end": "904425"
  },
  {
    "text": "you know, z1 and z2 are uncorrelated. Um, uh, and the contours of the Gaussian bump,",
    "start": "904425",
    "end": "910335"
  },
  {
    "text": "of the Gaussian density look like brown circles. And if you increase the off-diagonal, excuse me, then it looks like that.",
    "start": "910335",
    "end": "917205"
  },
  {
    "text": "If you increase it further to 0.8, 0.8, it looks like that, okay? Uh, where now, most of",
    "start": "917205",
    "end": "922950"
  },
  {
    "text": "the probability mass- probability ma- most probably density function places value on, um, z1 and z2 being positively correlated.",
    "start": "922950",
    "end": "931530"
  },
  {
    "text": "Um, next, let's look at, uh, what happens if we set the off-diagonal elements to negative values, right?",
    "start": "931530",
    "end": "939690"
  },
  {
    "text": "So, um, actually what do you think will happen? Let's set the off-diagonals to negative 0.5, 0.5.",
    "start": "939690",
    "end": "946300"
  },
  {
    "text": "Right. Oh well. People are seeing, fewer making that hand gesture. Okay, cool. Right. [LAUGHTER] Right.",
    "start": "947240",
    "end": "952890"
  },
  {
    "text": "So- so- so as you- you endow the two random variables with negative correlation, so you end up with, um,",
    "start": "952890",
    "end": "958545"
  },
  {
    "text": "this type of probability density function, right? Uh, and the contours, it looks like this.",
    "start": "958545",
    "end": "965030"
  },
  {
    "text": "Okay? Whe- whereas now slanted the other way. So now z1 and z2 have a negative correlation.",
    "start": "965030",
    "end": "970490"
  },
  {
    "text": "And that's 0.8, 0.8. Okay? All right. So- so far we've been keeping the mean vector as",
    "start": "970490",
    "end": "976470"
  },
  {
    "text": "0 and just varying the covariance matrix. Um, oh good. Yeah? [inaudible].",
    "start": "976470",
    "end": "984360"
  },
  {
    "text": "Uh, yes. Every covariance matrix is symmetric. Yeah. [inaudible] Uh, the true thing about",
    "start": "984360",
    "end": "997230"
  },
  {
    "text": "the covariance matrix has interesting column vectors, that point in interesting directions. Not really.",
    "start": "997230",
    "end": "1002585"
  },
  {
    "text": "Um, let me think. Maybe you should- yeah- yeah- uh,",
    "start": "1002585",
    "end": "1008930"
  },
  {
    "text": "no I- I- I think the covariance matrix is always symmetric. And so I would usually not look at single columns of the covariance matrix in isolation.",
    "start": "1008930",
    "end": "1019070"
  },
  {
    "text": "Uh, when we talk about Principal components analysis, we talk about the Eigenvectors of the covariance matrix, which are the principle directions in which it points but,",
    "start": "1019070",
    "end": "1026270"
  },
  {
    "text": "uh, yeah we- we- we- we'll get to that later. [inaudible]",
    "start": "1026270",
    "end": "1032920"
  },
  {
    "text": "Uh, yeah. So the Eigenvectors are a covariance matrix, points in the principal axes of the ellipse. That's defined by the contents.",
    "start": "1032920",
    "end": "1038089"
  },
  {
    "text": "Yeah. Cool. Okay. Um, so this standard Gaussian would mean 0.",
    "start": "1038090",
    "end": "1045169"
  },
  {
    "text": "So the Gaussian bump is centered at 0, 0 because mu is 0, 0. Uh, let's move Mu around.",
    "start": "1045170",
    "end": "1051110"
  },
  {
    "text": "So I'm going to move, you know, Mu to 0, 1.5. So that moves the Gaussian, uh,",
    "start": "1051110",
    "end": "1057095"
  },
  {
    "text": "the position of the Gaussian density right. Now let's move it to a different location. Move it to minus 1.5, minus 1.",
    "start": "1057095",
    "end": "1064535"
  },
  {
    "text": "And so by varying the value of Mu, you could also shift the center of the Gaussian density around.",
    "start": "1064535",
    "end": "1070280"
  },
  {
    "text": "Okay? So I hope this gives you a sense of, um, as you vary the parameters,",
    "start": "1070280",
    "end": "1075619"
  },
  {
    "text": "the mean and the covariance matrix of the 2D Gaussian density, um,",
    "start": "1075619",
    "end": "1080690"
  },
  {
    "text": "those are probably- probably density functions you can get as a result of changing Mu and Sigma.",
    "start": "1080690",
    "end": "1086210"
  },
  {
    "text": "Okay? Um, any other questions about this?",
    "start": "1086210",
    "end": "1090419"
  },
  {
    "text": "Raise the screen. [NOISE] All right, cool.",
    "start": "1092650",
    "end": "1097650"
  },
  {
    "text": "Here is a GDA, right, model.",
    "start": "1129070",
    "end": "1136804"
  },
  {
    "text": "Um, and- and, uh, let's see.",
    "start": "1136805",
    "end": "1142850"
  },
  {
    "text": "So, um, remember for GDA,",
    "start": "1142850",
    "end": "1149090"
  },
  {
    "text": "we need to model P of x given y, right? It's up here, y given x. So I'm gonna write this separately in two separate equations P of x given y equals 0.",
    "start": "1149090",
    "end": "1158075"
  },
  {
    "text": "So what's the chance- what's the, uh, probability density of the features if it is a benign tumor?",
    "start": "1158075",
    "end": "1164225"
  },
  {
    "text": "Um, I'm going to assume it's Gaussian. So I'm just going to write down the formula for Gaussian.",
    "start": "1164225",
    "end": "1170330"
  },
  {
    "text": "[NOISE]",
    "start": "1170330",
    "end": "1195215"
  },
  {
    "text": "And then similarly, I'm going to assume that if is a malignant tumor as if y is equal to 1,",
    "start": "1195215",
    "end": "1201995"
  },
  {
    "text": "that the density of the features is also Gaussian, okay?",
    "start": "1201995",
    "end": "1211190"
  },
  {
    "text": "And, um, I wanna point out a couple of things, so the parameters of the GDA model are",
    "start": "1211190",
    "end": "1218600"
  },
  {
    "text": "mu0, mu1, and sigma.",
    "start": "1218600",
    "end": "1224630"
  },
  {
    "text": "Um, and for reasons, we'll go into a little bit, we'll use the same sigma for both class.",
    "start": "1224630",
    "end": "1231840"
  },
  {
    "text": "Um, but we use different means, 0 and 1, okay?",
    "start": "1231940",
    "end": "1237320"
  },
  {
    "text": "Uh, and we can come back to this later. If you want, you could use separate parameters, you know,",
    "start": "1237320",
    "end": "1242690"
  },
  {
    "text": "sigma 0 and sigma 1, but that's not usually done. So we're going to assume that the two Gaussians,",
    "start": "1242690",
    "end": "1248180"
  },
  {
    "text": "for the positive and negative classes, have the same covariance matrix but they, they have different means. Uh, you don't have to make this assumption,",
    "start": "1248180",
    "end": "1254480"
  },
  {
    "text": "but this is the way it's most commonly done. And then we can talk about the reason why we tend to do that in a second.",
    "start": "1254480",
    "end": "1260400"
  },
  {
    "text": "Um, so this is a model for P of y given x. The other thing we need to do is model P of y.",
    "start": "1260400",
    "end": "1269430"
  },
  {
    "text": "Uh, so y is just a Bernoulli random variable, right. It takes on, you know, the values 0 or 1.",
    "start": "1269430",
    "end": "1275335"
  },
  {
    "text": "And so, I'm going to write it like this, phi to the y times 1 minus phi to the 1 minus y, okay?",
    "start": "1275335",
    "end": "1286940"
  },
  {
    "text": "Um, and you saw this kind of notation when we talked about logistic regression,",
    "start": "1286940",
    "end": "1292654"
  },
  {
    "text": "but all this means is that, um, you know, probability of y being equal to 1 is equal to phi, right.",
    "start": "1292655",
    "end": "1300575"
  },
  {
    "text": "Because y is either 0 or 1. And so, um, this is the way of writing, uh, uh, probability of y equals 1 is equal to phi, okay?",
    "start": "1300575",
    "end": "1309005"
  },
  {
    "text": "And, uh, you saw a similar explanation, it's a notation when we're talking about, um, logistic regression, right,",
    "start": "1309005",
    "end": "1315500"
  },
  {
    "text": "one week ago, last Monday. And so, the last parameter is phi.",
    "start": "1315500",
    "end": "1321080"
  },
  {
    "text": "So this is Rn, this is also Rn,",
    "start": "1321080",
    "end": "1326140"
  },
  {
    "text": "this is Rn by n and that's just a real number between 0 and 1, okay?",
    "start": "1326140",
    "end": "1334850"
  },
  {
    "text": "So, um, for any- let's see. So if you can fit mu0,",
    "start": "1345220",
    "end": "1351125"
  },
  {
    "text": "mu1, sigma, and phi to your data, then these parameters will define P of x given y and P of y.",
    "start": "1351125",
    "end": "1360965"
  },
  {
    "text": "And so, if at test time you have a new patient walk into your office, and you need to compute this,",
    "start": "1360965",
    "end": "1367429"
  },
  {
    "text": "then you can compute, right, these things in the red and the orange boxes. Each of these is a number,",
    "start": "1367430",
    "end": "1372950"
  },
  {
    "text": "and by plugging all these numbers in the formula, you get a number alpha P of y equals 1 given x and you can then predict,",
    "start": "1372950",
    "end": "1378575"
  },
  {
    "text": "you know, malignant or benign tumor. Right. So let's talk about how to fit the parameters.",
    "start": "1378575",
    "end": "1385460"
  },
  {
    "text": "So you have a training set, um, as usual,",
    "start": "1385460",
    "end": "1390500"
  },
  {
    "text": "I'm gonna write the tre- well, I'm go- let me write the training set like this xi, yi, for i equals 1 through m, right?",
    "start": "1390500",
    "end": "1397430"
  },
  {
    "text": "This is a usual training set. Um, and what we're going to do,",
    "start": "1397430",
    "end": "1404075"
  },
  {
    "text": "in order to fit these parameters is maximize the joint likelihood.",
    "start": "1404075",
    "end": "1410010"
  },
  {
    "text": "And in particular, um, let me define the likelihood of",
    "start": "1412600",
    "end": "1418445"
  },
  {
    "text": "the parameters to be",
    "start": "1418445",
    "end": "1426860"
  },
  {
    "text": "equal to the product from i equals 1 through m, up here, xi, yi, you know,",
    "start": "1426860",
    "end": "1435245"
  },
  {
    "text": "parameterized by the, um, the parameters, okay?",
    "start": "1435245",
    "end": "1445710"
  },
  {
    "text": "Um, and I'm, I'm just like dropped the parameters here, right?",
    "start": "1453790",
    "end": "1459230"
  },
  {
    "text": "To simplify the notation a little bit, okay? And the big difference between, um,",
    "start": "1459230",
    "end": "1467135"
  },
  {
    "text": "a generative learning algorithm like this, compared to a discriminative learning algorithm, is that the cost function you maximize is this joint likelihood which is p of x, y.",
    "start": "1467135",
    "end": "1481760"
  },
  {
    "text": "Whereas for a discriminative learning algorithm,",
    "start": "1481760",
    "end": "1485490"
  },
  {
    "text": "we were maximizing, um, this other thing, right.",
    "start": "1486790",
    "end": "1493791"
  },
  {
    "text": "Uh, which is sometimes also called the conditional likelihood, okay?",
    "start": "1500160",
    "end": "1510575"
  },
  {
    "text": "So the big difference between the- these two cost functions, is that for logistic regression or linear regression and generalized linear models,",
    "start": "1510575",
    "end": "1518990"
  },
  {
    "text": "you were trying to choose parameters theta, that maximize p of y given x.",
    "start": "1518990",
    "end": "1524480"
  },
  {
    "text": "But for generative learning algorithms, we're gonna try to choose parameters that maximize p of x and y or p of x, y, right.",
    "start": "1524480",
    "end": "1532445"
  },
  {
    "text": "Okay? So all right.",
    "start": "1532445",
    "end": "1545160"
  },
  {
    "text": "So if you use, um, maximum likelihood estimation.",
    "start": "1563080",
    "end": "1568769"
  },
  {
    "text": "Um, so you choose the parameters phi, mu0, mu1,",
    "start": "1578650",
    "end": "1585290"
  },
  {
    "text": "and sigma they maximize the log likelihood, right.",
    "start": "1585290",
    "end": "1591650"
  },
  {
    "text": "Where this you define as, you know, log of the likelihood that we defined up there.",
    "start": "1591650",
    "end": "1598039"
  },
  {
    "text": "Um, and so, uh, th- we, we actually ask you to do this as a problem set in the next homework.",
    "start": "1598040",
    "end": "1603410"
  },
  {
    "text": "But so the way you maximize this is, um, look at that formula for the likelihood,",
    "start": "1603410",
    "end": "1609005"
  },
  {
    "text": "take logs, take derivatives of this thing, set the derivative equal to 0 and then solve for the values of the parameters that maximize this whole thing.",
    "start": "1609005",
    "end": "1616115"
  },
  {
    "text": "And I'll, I'll, I'll just tell you the answers you are supposed to get. [LAUGHTER]. But you still have to do the derivation.",
    "start": "1616115",
    "end": "1622950"
  },
  {
    "text": "Right. um, the value of phi that maximizes this is,",
    "start": "1625150",
    "end": "1631100"
  },
  {
    "text": "you know, not that surprisingly. So, so phi is the estimate of probability of y being equal to 1, right?",
    "start": "1631100",
    "end": "1638990"
  },
  {
    "text": "So what's the chance when the next patient walks into your, uh, doctor's office that they have a, a malignant tumor?",
    "start": "1638990",
    "end": "1645755"
  },
  {
    "text": "And so the maximum likelihood estimate for phi is, um, it's just of all of your training examples,",
    "start": "1645755",
    "end": "1651320"
  },
  {
    "text": "what's the fraction with label y equals 1, right. So it's the, the maximum likelihood of the, uh,",
    "start": "1651320",
    "end": "1656465"
  },
  {
    "text": "bias of a coin toss is just, well, count up the fraction of heads you got, okay? So this, this is it.",
    "start": "1656465",
    "end": "1662120"
  },
  {
    "text": "um, and one other way to write this is, um, sum from i equals 1 through m indicator.",
    "start": "1662120",
    "end": "1671340"
  },
  {
    "text": "Okay. Right. Um, let's see.",
    "start": "1674110",
    "end": "1684605"
  },
  {
    "text": "So as you saw the indicator notation on Wednesday, did you? No. Uh, did you so- do, did we talk about the indicator notation on Wednesday?",
    "start": "1684605",
    "end": "1692480"
  },
  {
    "text": "No. Okay. Um, so, um, uh, this notation is an indicator function, uh, where,",
    "start": "1692480",
    "end": "1698960"
  },
  {
    "text": "um, indicator yi, equals 1 is, uh, uh, return 0 or 1 depending on whether the thing inside is true, right?",
    "start": "1698960",
    "end": "1705920"
  },
  {
    "text": "So there's an indicator notation in which an indicator of a true statement is equal to 1 and indicator of a false statement is equal to 0.",
    "start": "1705920",
    "end": "1715130"
  },
  {
    "text": "So that's another way of writing, writing this formula, right. Um, and then the maximum likelihood estimate for mu0 is this, um,",
    "start": "1715130",
    "end": "1726010"
  },
  {
    "text": "I'll just write out.",
    "start": "1726010",
    "end": "1727760"
  },
  {
    "text": "Okay. Ah, so, well, it- it actually if you,",
    "start": "1742860",
    "end": "1748400"
  },
  {
    "text": "ah, put aside the math for now, what do you think is the maximum likelihood estimate of the mean of all of the,",
    "start": "1748400",
    "end": "1753559"
  },
  {
    "text": "ah, features for the benign tumors, right? Well, what you do is you take all the benign tumors in your training set and just take the average,",
    "start": "1753560",
    "end": "1760385"
  },
  {
    "text": "that seems like a very reasonable way. Just look- look at your training set. Look at all of the- look at all of the benign tumors, all the Os,",
    "start": "1760385",
    "end": "1768799"
  },
  {
    "text": "I guess, and you just take the mean of these, and that, you know, seems like a pretty reasonable way to estimate Mu 0, right?",
    "start": "1768800",
    "end": "1775325"
  },
  {
    "text": "Look all of your negative examples and average their features. So this is a way of writing out that intuition.",
    "start": "1775325",
    "end": "1780890"
  },
  {
    "text": "Um, So the denominator is sum from i equals 1 through m indicates a y_i equals, 0,",
    "start": "1780890",
    "end": "1786560"
  },
  {
    "text": "and so the denominator will count up the number of examples that have benign tumors, right?",
    "start": "1786560",
    "end": "1793160"
  },
  {
    "text": "Because every time y_i equals 0, you get an extra 1 in this sum,",
    "start": "1793160",
    "end": "1799025"
  },
  {
    "text": "um, ah, and so the denominator ends up being the total number of benign tumors in your training set.",
    "start": "1799025",
    "end": "1806345"
  },
  {
    "text": "Okay? Um, and the numerator, ah, sum for m equals 1 through m indicator is a benign tumor times x_i.",
    "start": "1806345",
    "end": "1815375"
  },
  {
    "text": "So the effect of that is, um, whenever, a tumor is benign is 1 times the features,",
    "start": "1815375",
    "end": "1823654"
  },
  {
    "text": "whenever an example is malignant is 0 times the features and so the numerator is summing up all the features,",
    "start": "1823654",
    "end": "1832850"
  },
  {
    "text": "all the feature vectors for all of the examples that are benign. Does that make sense?",
    "start": "1832850",
    "end": "1838309"
  },
  {
    "text": "I- I just write this out, so this is the sum of feature vectors for,",
    "start": "1838310",
    "end": "1847445"
  },
  {
    "text": "um, for all the examples with y equals 0 and the denominator is a number of the examples,",
    "start": "1847445",
    "end": "1858480"
  },
  {
    "text": "where y equals 0, okay? And then if you take this ratio,",
    "start": "1861130",
    "end": "1867665"
  },
  {
    "text": "if you take this fraction, then you're summing up all of the feature vectors for the benign tumors divide by the total number of benign tumors in the training set,",
    "start": "1867665",
    "end": "1875660"
  },
  {
    "text": "and so that's just the mean of the feature vectors of all of the benign examples.",
    "start": "1875660",
    "end": "1881675"
  },
  {
    "text": "Okay? Um, and then,",
    "start": "1881675",
    "end": "1890520"
  },
  {
    "text": "right, maximum likelihood for Mu 1, no surprises, is sort of kind of what you'd expect,",
    "start": "1898630",
    "end": "1903875"
  },
  {
    "text": "sum up all of the positive examples and divide by the total number of positive examples and get the means.",
    "start": "1903875",
    "end": "1909140"
  },
  {
    "text": "So that's maximum likelihood for Mu_1, um, and then I just write this out.",
    "start": "1909140",
    "end": "1916860"
  },
  {
    "text": "If you are familiar with covariance matrices, this formula may not surprise you.",
    "start": "1917800",
    "end": "1924710"
  },
  {
    "text": "But if you're less familiar, then I guess you can see the details in the homework.",
    "start": "1924710",
    "end": "1933480"
  },
  {
    "text": "Okay. Don't worry too much about that. Ah, you can unpack the details in the lecture notes.",
    "start": "1939880",
    "end": "1945530"
  },
  {
    "text": "So we'll know how it works, okay? But the covariance matrix, basically tries to,",
    "start": "1945530",
    "end": "1951845"
  },
  {
    "text": "you know, fit contours to the ellipse, right? Like we saw, ah, so- so try to fill the Gaussian to both of these with",
    "start": "1951845",
    "end": "1958940"
  },
  {
    "text": "these corresponding means but you want one covariance matrix to both of these. Okay? Um, So these are the- so- so- so the way- so the way I motivated this was,",
    "start": "1958940",
    "end": "1969995"
  },
  {
    "text": "you know, I said, well, if you want to estimate the mean of a coin toss, just count up the fraction of coin tosses, they came up heads,",
    "start": "1969995",
    "end": "1975500"
  },
  {
    "text": "ah, and then it seems that the mean for Mu_0 and Mu_1, you just look at these examples and pick the mean, right?",
    "start": "1975500",
    "end": "1980929"
  },
  {
    "text": "So that- that was the intuitive explanation for how you get these formulas. But the mathematically sound way to get",
    "start": "1980930",
    "end": "1986779"
  },
  {
    "text": "these formulas is not by this intuitive argument that I just gave, it's instead to look at the likelihood, ah,",
    "start": "1986780",
    "end": "1993980"
  },
  {
    "text": "take logs, get the log likelihood, take derivatives, set derivatives equal to 0, solve for all these values and prove more formally that these",
    "start": "1993980",
    "end": "2001405"
  },
  {
    "text": "are the actual values that maximize this thing, right? By- by the same theories as you solved,",
    "start": "2001405",
    "end": "2006654"
  },
  {
    "text": "so you can see that for yourself, um, in the problem sets. Okay? So- All right.",
    "start": "2006655",
    "end": "2020049"
  },
  {
    "text": "Um, finally, having fit these parameters,",
    "start": "2020050",
    "end": "2026545"
  },
  {
    "text": "um, if you want to make a prediction, right?",
    "start": "2026545",
    "end": "2034810"
  },
  {
    "text": "So given the new patient, ah, how do you make a prediction for whether their tumor is malignant or benign?",
    "start": "2034810",
    "end": "2041934"
  },
  {
    "text": "Um, so if you want to predict the most likely class label,",
    "start": "2041935",
    "end": "2049780"
  },
  {
    "text": "ah, you choose max over y, of p of y, given x, right?",
    "start": "2049780",
    "end": "2057220"
  },
  {
    "text": "Um, and by Bayes' rule, this is max over y of p of x given y,",
    "start": "2057220",
    "end": "2063730"
  },
  {
    "text": "p of y divided by p of x.",
    "start": "2063730",
    "end": "2069205"
  },
  {
    "text": "Okay? Now, um, I wanna introduce one esh- well, one- one more piece of notation which is,",
    "start": "2069205",
    "end": "2076315"
  },
  {
    "text": "ah, I wanna introduce, actually, how- how many of you are familiar with the arg max notation?",
    "start": "2076315",
    "end": "2083800"
  },
  {
    "text": "Most of you? Like two- two-thirds? Okay, cool. I- I- I'll go over this quickly.",
    "start": "2083800",
    "end": "2089155"
  },
  {
    "text": "So, um, this is just an example. So the, um, let's see.",
    "start": "2089155",
    "end": "2097885"
  },
  {
    "text": "Ah, boy. All right.",
    "start": "2097885",
    "end": "2103075"
  },
  {
    "text": "So, you know, the Min over z of, uh, z minus 5 squared is equal to 0",
    "start": "2103075",
    "end": "2111190"
  },
  {
    "text": "because the smallest possible value of z by a 5 squared is 0, right? and the arg min over z of z minus 5 squared is equal to 5.",
    "start": "2111190",
    "end": "2123100"
  },
  {
    "text": "Okay? So the min is the smallest possible value attained by the thing inside",
    "start": "2123100",
    "end": "2128440"
  },
  {
    "text": "and the arg min is the value you need to plug in to achieve that smallest possible value, right?",
    "start": "2128440",
    "end": "2134170"
  },
  {
    "text": "So ah, the prediction you actually want to make, if you want to output a value for y, you don't wanna output a probability, right?",
    "start": "2134170",
    "end": "2140619"
  },
  {
    "text": "You know what I'm saying? Well, what do I think is the value of y? So you might choose a value of y that maximizes this, and- so- so there's the arg max of this and this would be either 0 or 1, right?",
    "start": "2140620",
    "end": "2149170"
  },
  {
    "text": "Um, so that's equal to arg max of that, and you notice that, ah, this denominator is just a constant, right?",
    "start": "2149170",
    "end": "2156580"
  },
  {
    "text": "It doesn't- it doesn't- it's a p of x, it's- y doesn't even appear in there? It's just some positive number.",
    "start": "2156580",
    "end": "2162100"
  },
  {
    "text": "And so this is equal to, just arg max over y,",
    "start": "2162100",
    "end": "2167200"
  },
  {
    "text": "p of x given y times p of y, okay? So when implementing, um, ah,",
    "start": "2167200",
    "end": "2175900"
  },
  {
    "text": "when- when making predictions with Gaussian disc- in a- with the generative learning algorithms,",
    "start": "2175900",
    "end": "2181750"
  },
  {
    "text": "sometimes to save on computation, you don't bother to calculate the denominator, if all you care about is to make a prediction,",
    "start": "2181750",
    "end": "2188500"
  },
  {
    "text": "but if you'd actually need a probability, then you'd have to normalize the probability, okay?",
    "start": "2188500",
    "end": "2193970"
  },
  {
    "text": "Okay. So let's examine",
    "start": "2196950",
    "end": "2207369"
  },
  {
    "text": "what the algorithm is doing. [NOISE].",
    "start": "2207370",
    "end": "2216970"
  },
  {
    "text": "All right. So let's look at the same dataset and compare and contrast what",
    "start": "2216970",
    "end": "2222130"
  },
  {
    "text": "a discriminative learning algorithm versus a generative learning algorithm will do on this dataset.",
    "start": "2222130",
    "end": "2227680"
  },
  {
    "text": "Right. Um, here's example with two features X1 and X2 and positive and negative examples.",
    "start": "2227680",
    "end": "2235780"
  },
  {
    "text": "So let's start with a discriminative learning algorithm. Um, let say you initialize the parameters randomly.",
    "start": "2235780",
    "end": "2241870"
  },
  {
    "text": "Typically, when you run a logistic regression, I almost always initialize the parameters as 0 but- but this just, you know,",
    "start": "2241870",
    "end": "2247600"
  },
  {
    "text": "it's more interesting to start off for the purposes of visualization, with a random line I guess. And then if you run one iteration of gradient descent on the conditional likelihood,",
    "start": "2247600",
    "end": "2257325"
  },
  {
    "text": "um, one iteration of logistic regression moves the line there. There's two iterations, three iterations, um,",
    "start": "2257325",
    "end": "2264435"
  },
  {
    "text": "four iterations and so on and after about 20 iterations it will converge to that pretty decent discriminative boundary.",
    "start": "2264435",
    "end": "2273565"
  },
  {
    "text": "So that's logistic regression, really searching for a line that separates positive and negative examples.",
    "start": "2273565",
    "end": "2278695"
  },
  {
    "text": "How about the generative learning algorithm? What it does is the following,",
    "start": "2278695",
    "end": "2283915"
  },
  {
    "text": "which is fit with Gaussian discriminant analysis. What we'll do, is fit Gaussians to the positive and negative examples.",
    "start": "2283915",
    "end": "2293109"
  },
  {
    "text": "Right, and just one- one technical detail, um, I described this as if we look at the two classes separately",
    "start": "2293110",
    "end": "2300010"
  },
  {
    "text": "because we use the same covariance matrix sigma for the positive and negative classes. We actually don't quite look at them totally separately but we do",
    "start": "2300010",
    "end": "2306910"
  },
  {
    "text": "fit two Gaussian densities to the positive and negative examples. And then what we do is,",
    "start": "2306910",
    "end": "2313255"
  },
  {
    "text": "for each point try to decide whether this is class label using Bayes' rule,",
    "start": "2313255",
    "end": "2318579"
  },
  {
    "text": "using that formula and it turns out that this implies the following decision boundary.",
    "start": "2318580",
    "end": "2323815"
  },
  {
    "text": "Right. So points to the upper right of this decision boundary, to that straight line I just drew,",
    "start": "2323815",
    "end": "2329815"
  },
  {
    "text": "you are closer to the negative class. You end up classifying them as negative examples and points to the lower left of that line,",
    "start": "2329815",
    "end": "2336895"
  },
  {
    "text": "you end there classifying as- as a positive examples. And I've- I've also drawn in green here the decision boundary for logistic regression.",
    "start": "2336895",
    "end": "2346030"
  },
  {
    "text": "So- so- so these two algorithms actually come up with slightly different decision boundaries.",
    "start": "2346030",
    "end": "2351714"
  },
  {
    "text": "Okay, but the way you arrive at these two decision boundaries are a little bit different.",
    "start": "2351715",
    "end": "2356320"
  },
  {
    "text": "So, um. All right,",
    "start": "2356860",
    "end": "2363365"
  },
  {
    "text": "let's go back to the- Any questions about this? Yeah. [NOISE] [inaudible].",
    "start": "2363365",
    "end": "2381671"
  },
  {
    "text": "Oh, sure yes, good question. So why- why- why do we use two separate means, mu 0 and mu 1 and a single covariance matrix sigma?",
    "start": "2381671",
    "end": "2389605"
  },
  {
    "text": "It turns out that, um-. It turns out that if you choose to build the model this way,",
    "start": "2389605",
    "end": "2396430"
  },
  {
    "text": "the decision boundary ends up being linear and so for a lot of problems if you want to linear decision boundary,",
    "start": "2396430",
    "end": "2401769"
  },
  {
    "text": "um, uh, um, yeah. And it turns out you could choose to use two separate,",
    "start": "2401770",
    "end": "2407020"
  },
  {
    "text": "um, covariance matrix sigma 0 and sigma 1, and they'll actually work okay. Right. There's- it is actually very reasonable to do so as well,",
    "start": "2407020",
    "end": "2414340"
  },
  {
    "text": "but you double the number of parameters roughly and you end up with a decision boundary that isn't linear anymore.",
    "start": "2414340",
    "end": "2421345"
  },
  {
    "text": "But it is actually not an unreasonable algorithm to do that as well. Um, now, there's one-",
    "start": "2421345",
    "end": "2430330"
  },
  {
    "text": "[BACKGROUND].",
    "start": "2430330",
    "end": "2451180"
  },
  {
    "text": "Now, there's one very interesting property, um, about Gaussian discriminant analysis and it turns out that's- ah.",
    "start": "2451180",
    "end": "2462520"
  },
  {
    "text": "Well, let's- let's compare",
    "start": "2462520",
    "end": "2466460"
  },
  {
    "text": "GDA to logistic regression and,",
    "start": "2469530",
    "end": "2476005"
  },
  {
    "text": "um, for a fixed set of parameters.",
    "start": "2476005",
    "end": "2481519"
  },
  {
    "text": "Right. So let's say you've learned some set of parameters.",
    "start": "2484230",
    "end": "2489655"
  },
  {
    "text": "Um, I'm going to do an exercise where we're going to plot,",
    "start": "2489655",
    "end": "2495560"
  },
  {
    "text": "P of Y equals 1 given X, you're parameterized by all these things,",
    "start": "2498240",
    "end": "2506660"
  },
  {
    "text": "right, as a function of x.",
    "start": "2507320",
    "end": "2510910"
  },
  {
    "text": "So I'm gonna do this little exercise in a second, but what this means is,",
    "start": "2514410",
    "end": "2519685"
  },
  {
    "text": "um, well, this formula, this is equal to P of X given Y equals 1, you know,",
    "start": "2519685",
    "end": "2528414"
  },
  {
    "text": "which is parameterized by- right well, the various parameters times p of y equals 1,",
    "start": "2528415",
    "end": "2535675"
  },
  {
    "text": "is parameterized by phi divided by P of X which depends on all the parameters, I guess.",
    "start": "2535675",
    "end": "2543020"
  },
  {
    "text": "Right. So by Bayes rule,",
    "start": "2546840",
    "end": "2551964"
  },
  {
    "text": "you know this formula is equal to this little thing and just as we saw earlier, I guess right.",
    "start": "2551964",
    "end": "2560260"
  },
  {
    "text": "Once you have fixed all the parameters that's just a number you compute by evaluating the Gaussian density.",
    "start": "2560260",
    "end": "2566079"
  },
  {
    "text": "Um, this is the Bernoulli probability, so actually P of Y equals 1 parameterized by phi is just equal to phi",
    "start": "2566080",
    "end": "2573760"
  },
  {
    "text": "is that second term and you similarly calculate the denominator. But so for every value of x,",
    "start": "2573760",
    "end": "2579265"
  },
  {
    "text": "you can compute this ratio and thus get a number for the chance of Y being 1 given X. So I'm gonna go",
    "start": "2579265",
    "end": "2588730"
  },
  {
    "text": "through one example of",
    "start": "2588730",
    "end": "2594385"
  },
  {
    "text": "what function you'd get for P of Y equals 1 given X, for what function you get for this if you actually plot this for,",
    "start": "2594385",
    "end": "2602740"
  },
  {
    "text": "um, different values of X. Okay. So, um, let's see.",
    "start": "2602740",
    "end": "2611620"
  },
  {
    "text": "Let's say you have just one feature X, so X is a- a- and let's say that you have",
    "start": "2611620",
    "end": "2619030"
  },
  {
    "text": "a few negative examples there and a few positive examples there.",
    "start": "2619030",
    "end": "2625345"
  },
  {
    "text": "Right. So it's a simple dataset. Okay, and let's see what Gaussian discriminant analysis will do on this dataset.",
    "start": "2625345",
    "end": "2635530"
  },
  {
    "text": "Um, with just one feature so that's why all the data is parsing on 1D. So let me map all this data to an x-axis.",
    "start": "2635530",
    "end": "2650060"
  },
  {
    "text": "I just filled this data and mapped it down. And if you fit a Gaussian to each of these two data sets then you end up with, you know,",
    "start": "2652320",
    "end": "2663625"
  },
  {
    "text": "Gaussians as follows where this bump on the left is P of X given Y equals 0 and",
    "start": "2663625",
    "end": "2670420"
  },
  {
    "text": "this bump on the right is P of X given Y equals 1.",
    "start": "2670420",
    "end": "2677049"
  },
  {
    "text": "Right, and- and again just to check on all details that we set the same variance to the two Gaussians,",
    "start": "2677050",
    "end": "2682390"
  },
  {
    "text": "but you know, you kinda model the Gaussian densities of what does this class 0 look like? What does class 1 look like with two Gaussian bumps like this?",
    "start": "2682390",
    "end": "2690234"
  },
  {
    "text": "Then because the dataset is split 50-50 P of Y equals 1 is 0.5. Right, so one half prior.",
    "start": "2690235",
    "end": "2697285"
  },
  {
    "text": "Okay. Now, let's go through that exercise I described on the left of trying to",
    "start": "2697285",
    "end": "2703210"
  },
  {
    "text": "plot P of Y equals 1 given X for different values of X.",
    "start": "2703210",
    "end": "2709599"
  },
  {
    "text": "So the vertical axis here as P of Y equals 1 given different values of X.",
    "start": "2709600",
    "end": "2714820"
  },
  {
    "text": "So, um, let's pick a point far to the left here. Right. With this model you- if you actually",
    "start": "2714820",
    "end": "2723370"
  },
  {
    "text": "calculate this ratio you find that if you have a point here, it almost certainly came from this Gaussian on the left.",
    "start": "2723370",
    "end": "2732145"
  },
  {
    "text": "If- if you have an unlabeled example here, you're almost certain it came from the class 0 Gaussian",
    "start": "2732145",
    "end": "2738460"
  },
  {
    "text": "because the chance of this Gaussian generating example all the way to left is almost 0.",
    "start": "2738460",
    "end": "2743530"
  },
  {
    "text": "Right, and so chance of P- P of Y equals 1 given X is very small. So for a point-like that,",
    "start": "2743530",
    "end": "2748869"
  },
  {
    "text": "you end up with a point you know, very close to 0, right. Um, let's pick another point.",
    "start": "2748870",
    "end": "2755200"
  },
  {
    "text": "Right, how about this point, the midpoint. Well, if you're getting example right at the midpoint, you- you really have no idea. You really can't tell.",
    "start": "2755200",
    "end": "2761575"
  },
  {
    "text": "Did this come from the negative or the positive Gaussian? Can't tell. Right. So this is really 50-50.",
    "start": "2761575",
    "end": "2767050"
  },
  {
    "text": "So I guess if this is 0.5 for that midpoint you would have P of Y equals 1 given X is 0.5.",
    "start": "2767050",
    "end": "2776214"
  },
  {
    "text": "Um, then if you go to a point away to the variance, if you get an example way here, then you'd be pretty sure this came from the positive examples and so,",
    "start": "2776215",
    "end": "2783865"
  },
  {
    "text": "you know, you get a point like that. Right. Now, it turns out that if you repeat this exercise",
    "start": "2783865",
    "end": "2791830"
  },
  {
    "text": "sweeping from left to right for many many points on the X axis you find that,",
    "start": "2791830",
    "end": "2797335"
  },
  {
    "text": "for points far to the left, the chance of this coming from, um,",
    "start": "2797335",
    "end": "2803095"
  },
  {
    "text": "the Y equals 1 class is very small and as you approach this midpoint,",
    "start": "2803095",
    "end": "2808105"
  },
  {
    "text": "it increases to 0.5 and it surpasses 0.5. And then beyond a certain point,",
    "start": "2808105",
    "end": "2814450"
  },
  {
    "text": "it becomes very very close to 1. Right, and you do this exercise and actually just for every point, you know,",
    "start": "2814450",
    "end": "2821349"
  },
  {
    "text": "for a dense grid on the x-axis evaluate this formula which will give you a number between 0 and 1.",
    "start": "2821350",
    "end": "2828145"
  },
  {
    "text": "Is the probability and go ahead and plot, you know, the values you get a curve like this.",
    "start": "2828145",
    "end": "2833290"
  },
  {
    "text": "It turns out that if you connect up the dots, um, then this is exactly a sigmoid function.",
    "start": "2833290",
    "end": "2840490"
  },
  {
    "text": "The shape of that turns out to be exactly a shaped sigmoid function and you prove this in the problem sets as well.",
    "start": "2840490",
    "end": "2847060"
  },
  {
    "text": "Right. Um, so, um,",
    "start": "2847060",
    "end": "2858160"
  },
  {
    "text": "both logistic regression and Gaussian discriminant analysis actually end up using",
    "start": "2858160",
    "end": "2863724"
  },
  {
    "text": "a sigmoid function to calculate P of Y equals 1 given X or- or the,",
    "start": "2863725",
    "end": "2870895"
  },
  {
    "text": "the outcome ends up being a sigmoid function. I guess the mechanics is, you actually use this calculation rather than compute a sigmoid function.",
    "start": "2870895",
    "end": "2877480"
  },
  {
    "text": "Right. But, um, the specific choice of the parameters they end up",
    "start": "2877480",
    "end": "2882609"
  },
  {
    "text": "choosing are quite different and you saw when I was projecting the results on the display just now in PowerPoint,",
    "start": "2882610",
    "end": "2888670"
  },
  {
    "text": "that the two algorithms actually come up with two different decision boundaries. Right. So, um, let's discuss when",
    "start": "2888670",
    "end": "2898075"
  },
  {
    "text": "a genitive algorithm like GDA is superior and when a distributed algorithm like logistic regression is superior.",
    "start": "2898075",
    "end": "2905290"
  },
  {
    "text": "Um, let's see if I can get rid of this.",
    "start": "2905290",
    "end": "2912880"
  },
  {
    "text": "[BACKGROUND]",
    "start": "2912880",
    "end": "2928170"
  },
  {
    "text": "All right. So GDA, Gaussian Distributed Analysis.",
    "start": "2928170",
    "end": "2933960"
  },
  {
    "text": "So the generative approach. This assumes that x given y equals 0,",
    "start": "2933960",
    "end": "2942900"
  },
  {
    "text": "this is Gaussian, with mean Mu_0 and co variance Sigma. It assumes x given y equals 1,",
    "start": "2942900",
    "end": "2949170"
  },
  {
    "text": "this is Gaussian with mean Mu_1 and covariance Sigma, and y is Bernoulli with, um, parenthesis Phi.",
    "start": "2949170",
    "end": "2960765"
  },
  {
    "text": "Right. And what logistic regression does. [NOISE] This is a discriminative algorithm,",
    "start": "2960765",
    "end": "2970380"
  },
  {
    "text": "uh, there is some [LAUGHTER] strange wind at the back, is it?",
    "start": "2970380",
    "end": "2979545"
  },
  {
    "text": "Yeah. I see. Okay. Cool. All right. Yeah. Why? You know",
    "start": "2979545",
    "end": "2986130"
  },
  {
    "text": "the-there's just a scary UN report on global [LAUGHTER] warming over the weekend. I hope we don't already have storms here, um.",
    "start": "2986130",
    "end": "2992474"
  },
  {
    "text": "Okay. It's okay. Did you guys see the UN report? It's slightly scary actually wa- the-",
    "start": "2992475",
    "end": "2998549"
  },
  {
    "text": "the UN report on global warming but hopefully- all right. Good. Hurricane stopped.",
    "start": "2998550",
    "end": "3003590"
  },
  {
    "text": "[LAUGHTER] Um, let's see.",
    "start": "3003590",
    "end": "3009950"
  },
  {
    "text": "Uh, so what logistic regression assumes is p of y equals 1 given x.",
    "start": "3009950",
    "end": "3020105"
  },
  {
    "text": "You know, that this is, uh, governed by logistic function. Right. So this is really 1 over 1 plus e is a negative Theta transpose x.",
    "start": "3020105",
    "end": "3026720"
  },
  {
    "text": "We-where some details about x_0 equals 1 and so on.",
    "start": "3026720",
    "end": "3032690"
  },
  {
    "text": "Right. So just- just- okay. So- so in other words, uh, it's assumed that this is,",
    "start": "3032690",
    "end": "3038210"
  },
  {
    "text": "um, p of y equals 1 given x is logistic.",
    "start": "3038210",
    "end": "3044849"
  },
  {
    "text": "Okay. And the argument that I just described just now, uh,",
    "start": "3045310",
    "end": "3050630"
  },
  {
    "text": "plotting you know p of y equals 1 given x point-by-point to really the sigmoid curve I drew on the other board.",
    "start": "3050630",
    "end": "3057660"
  },
  {
    "text": "What that illustrates. Um, it doesn't prove it. You prove it yourself in a homework problem.",
    "start": "3057660",
    "end": "3062980"
  },
  {
    "text": "But what that illustrates is that, this set of assumptions implies that p of y equals 1 given x is governed by a logistic function.",
    "start": "3062980",
    "end": "3074065"
  },
  {
    "text": "Right. But it turns out that the implication in the opposite direction is not true.",
    "start": "3074065",
    "end": "3081265"
  },
  {
    "text": "Right. So if you assume p of y equals 1 given x is governed by logistic function by- by this shape,",
    "start": "3081265",
    "end": "3088625"
  },
  {
    "text": "this does not in any way shape or form assume that x given y is Gaussian,",
    "start": "3088625",
    "end": "3093695"
  },
  {
    "text": "uh, uh, x given y equals 0 is Gaussian x given y equals 1 is Gaussian. Right. So what this means is that GDA,",
    "start": "3093695",
    "end": "3103204"
  },
  {
    "text": "the generative learning algorithm in this case, this makes a stronger set of assumptions and which this regression makes",
    "start": "3103205",
    "end": "3115010"
  },
  {
    "text": "a weaker set of",
    "start": "3115010",
    "end": "3120740"
  },
  {
    "text": "assumptions because you can prove these assumptions from these assumptions. Okay. Um, and by the way as- as- uh,",
    "start": "3120740",
    "end": "3131870"
  },
  {
    "text": "as- as- uh, let's see. And so what you see in a lot of learning algorithms is that, um,",
    "start": "3131870",
    "end": "3140135"
  },
  {
    "text": "if you make strongly modeling assumptions and if your modeling assumptions are roughly correct,",
    "start": "3140135",
    "end": "3145204"
  },
  {
    "text": "then your model will do better because you're telling more information to the algorithm.",
    "start": "3145205",
    "end": "3150410"
  },
  {
    "text": "So if indeed x given y is Gaussian, then GDA will do better because you're telling",
    "start": "3150410",
    "end": "3158000"
  },
  {
    "text": "the algorithm x given y is Gaussian and so it can be more efficient. And so even if a very small dataset, um,",
    "start": "3158000",
    "end": "3164839"
  },
  {
    "text": "if these assumptions are roughly correct, then GA will do better. And the problem with GDA is,",
    "start": "3164840",
    "end": "3170809"
  },
  {
    "text": "if these assumptions turn out to be wrong. So if x given y is not at all Gaussian, then this might be a very bad set of assumptions to make.",
    "start": "3170810",
    "end": "3177860"
  },
  {
    "text": "You might be trying to fit a Gaussian density to data that is not at all Gaussian and then GDA would do more poorly.",
    "start": "3177860",
    "end": "3184865"
  },
  {
    "text": "Okay. So here's one fun fact. Here's another example, get to your question in a second,",
    "start": "3184865",
    "end": "3191224"
  },
  {
    "text": "which is let's say the following are true; let's say that x given y equals 1 is Poisson with, uh,",
    "start": "3191225",
    "end": "3202070"
  },
  {
    "text": "parameter Lambda_1 and x given y equals 0 is Poisson with mean,",
    "start": "3202070",
    "end": "3211490"
  },
  {
    "text": "uh, Lambda_0, or lambda_1 not 0 and y, as before, is Bernoulli 5x.",
    "start": "3211490",
    "end": "3219085"
  },
  {
    "text": "Right. It turns out that this set of assumptions also imply that p of y equals 1 given x.",
    "start": "3219085",
    "end": "3227599"
  },
  {
    "text": "This is logistic, okay, and you can prove this.",
    "start": "3227600",
    "end": "3232970"
  },
  {
    "text": "And this is actually true for, um, any generalized linear model, actually where, uh, where- where, uh, the difference between",
    "start": "3232970",
    "end": "3239930"
  },
  {
    "text": "these two distributions varies only according to the natural parameter as a generalized name. Excuse me, of the exponential family distribution.",
    "start": "3239930",
    "end": "3246829"
  },
  {
    "text": "Right. And so what this means is that, um, if you don't know if your data is Gaussian or Poisson,",
    "start": "3246830",
    "end": "3254980"
  },
  {
    "text": "um, if you're using logistic regression you don't need to worry about it. It'll work fine either way.",
    "start": "3254980",
    "end": "3260340"
  },
  {
    "text": "Right. So- so, you know, maybe, um, you are fitting data to s- maybe a fitting, uh,",
    "start": "3260340",
    "end": "3265369"
  },
  {
    "text": "uh, a model, binary classification model to some data. And you don't know, is a data Gaussian?",
    "start": "3265370",
    "end": "3270575"
  },
  {
    "text": "Is it Poisson? Is this some other exponential family model? Maybe you just don't know. But if you're fitting logistic regression,",
    "start": "3270575",
    "end": "3276560"
  },
  {
    "text": "it- it'll do fine under all of those scenarios. Right. But if your data was actually Poisson but you assumed it was Gaussian,",
    "start": "3276560",
    "end": "3284119"
  },
  {
    "text": "then your model might do quite poorly. Okay. So the key high level principles when you take away from this is, um, uh, uh,",
    "start": "3284119",
    "end": "3294180"
  },
  {
    "text": "if you make weaker assumptions as in logistic regression, then your algorithm will be more robust to modeling",
    "start": "3295720",
    "end": "3303200"
  },
  {
    "text": "assumptions such as accidentally assuming the data is Gaussian and it is not. Uh, but on the flip side,",
    "start": "3303200",
    "end": "3308315"
  },
  {
    "text": "if you have a very small dataset, then, um, using a model that makes more assumptions will actually allow you to do better",
    "start": "3308315",
    "end": "3316415"
  },
  {
    "text": "because by making more assumptions you're just telling the algorithm more truth about the world which is,",
    "start": "3316415",
    "end": "3321635"
  },
  {
    "text": "you know, \"Hey, algorithm, the world is Gaussian,\" and if it is Gaussian, then it will actually do- do- do better.",
    "start": "3321635",
    "end": "3327170"
  },
  {
    "text": "Okay. Your question at the back or a few questions. Go ahead. Just from that, is there a point do you know like what sort",
    "start": "3327170",
    "end": "3335359"
  },
  {
    "text": "of data it usually has a Gaussian problem?",
    "start": "3335360",
    "end": "3340600"
  },
  {
    "text": "Oh, oh, yeah. Practical sample without data is a Gaussian probably, you know, it's, uh, uh- yeah, you know,",
    "start": "3340600",
    "end": "3347555"
  },
  {
    "text": "it's a matter of degree. Right. Most data on this universe is Gaussian [LAUGHTER] uh, uh, uh, except at this feed data, I guess.",
    "start": "3347555",
    "end": "3355535"
  },
  {
    "text": "Yeah, but- but, um- I think it's actually a- a matter of degree. Right. If- if you plot- actually if you take",
    "start": "3355535",
    "end": "3361220"
  },
  {
    "text": "continuous value data- no, ther- ther- there are exceptions. You could plot it and most data that you plot, you know,",
    "start": "3361220",
    "end": "3366994"
  },
  {
    "text": "will not really be Gaussian but a lot of it you can convince yourself is vaguely Gaussian. So I think a lot of it is amount of degree.",
    "start": "3366995",
    "end": "3373115"
  },
  {
    "text": "I- I- I actually tell you the way I choose to use, um, these two algorithms. So I think that the whole world has moved toward using bigger than three datasets.",
    "start": "3373115",
    "end": "3381350"
  },
  {
    "text": "Right. Digital Civil Society which is a lot of data and so for a lot of problems we have a lot of data,",
    "start": "3381350",
    "end": "3386780"
  },
  {
    "text": "I would probably use logistic regression. Because with more data, you could overcome telling the algorithm less about the world.",
    "start": "3386780",
    "end": "3394730"
  },
  {
    "text": "Right. So- so the algorithm has two sources of knowledge. Uh, one source of knowledge is what did you tell it,",
    "start": "3394730",
    "end": "3400625"
  },
  {
    "text": "what are the assumptions you told it to make? And the second source of knowledge is learned from the data and in this era of big data,",
    "start": "3400625",
    "end": "3407210"
  },
  {
    "text": "we have a lot of data, you know, there is a strong trend to use logistic regression which makes less assumptions and just lets the algorithm",
    "start": "3407210",
    "end": "3413990"
  },
  {
    "text": "figure out whether it wants to figure out from the data. Right. Now, one practical reason why I still use algorithms like the GDA,",
    "start": "3413990",
    "end": "3421160"
  },
  {
    "text": "general discriminant analysis, so algorithms like this, um, uh, is that, it's actually quite computationally efficient and so",
    "start": "3421160",
    "end": "3427880"
  },
  {
    "text": "the- there's actually one use case at Landing. AI that we're working on where we just need to fit a ton of models and don't have the patience to run the GC progression over and over.",
    "start": "3427880",
    "end": "3435755"
  },
  {
    "text": "And it turns out computing mean and variances of, um, covariance matrices is very efficient and so",
    "start": "3435755",
    "end": "3442220"
  },
  {
    "text": "there's actually apart from the assumptions type of benefit, uh, which is a general philosophical point.",
    "start": "3442220",
    "end": "3447665"
  },
  {
    "text": "We'll see again later in this course. Right. Th- this idea about do you make strong or weak assumptions? This is a general principle in machine learning that we'll see again in other places.",
    "start": "3447665",
    "end": "3455810"
  },
  {
    "text": "But the very concrete- the other reason I tend to use GDA these days is less that I think I perform better from",
    "start": "3455810",
    "end": "3461690"
  },
  {
    "text": "an accuracy point of view but there's actually a very efficient algorithm. We just compute the mean covar- covariance and we are done and there's no iterative process needed.",
    "start": "3461690",
    "end": "3468500"
  },
  {
    "text": "So these days when I use these models, um, is more motivated by computation and less by performance.",
    "start": "3468500",
    "end": "3476450"
  },
  {
    "text": "But this general principle is one that we'll come back to again later when we develop more sophisticated learning algorithms. Yeah.",
    "start": "3476450",
    "end": "3482810"
  },
  {
    "text": "Uh, if the data is generated from a Gaussian but my program synthesis are different with the assumption",
    "start": "3482810",
    "end": "3490010"
  },
  {
    "text": "that we just use the same program for performance-? Oh, right, ah, so what happens when the co-variance matrices are different?",
    "start": "3490010",
    "end": "3497055"
  },
  {
    "text": "It turns out that, uh, uh, trying to remember, it still ends up being a logistic function but with a bunch of quadratic terms in the logistic function.",
    "start": "3497055",
    "end": "3503490"
  },
  {
    "text": "So it's not a linear decision boundary anymore. You can end up with a decision boundary, you know, that- that- that looks like this, right?",
    "start": "3503490",
    "end": "3509790"
  },
  {
    "text": "With positive and negative examples separated by some- by some other shape from a linear decision boundary. Uh, you- you could- you could- you could fig- actually,",
    "start": "3509790",
    "end": "3516750"
  },
  {
    "text": "I- if you're curious, I encourage you to, you know, uh, uh, fire up Python NumPy and- and",
    "start": "3516750",
    "end": "3522075"
  },
  {
    "text": "play around their parameters and plot this for yourself, uh, questions? Is it recommended that we use some kind of statistical global test to make",
    "start": "3522075",
    "end": "3530210"
  },
  {
    "text": "sure that the plot distribution results have a equal variance before we do GDA?",
    "start": "3530210",
    "end": "3535305"
  },
  {
    "text": "Yeah. It's recommended that you do some cyclical tests to see if it's Gaussian, um, I can tell you what's done in practice.",
    "start": "3535305",
    "end": "3542115"
  },
  {
    "text": "I think in practice, if you have enough data to do a cyclical test and gain conviction, you probably have enough data to just use logistic regression,",
    "start": "3542115",
    "end": "3550530"
  },
  {
    "text": "um, uh, the- the- I- I don't know. [LAUGHTER] Well no, that's not really fair. I don't know. If they're very high dimensional data,",
    "start": "3550530",
    "end": "3556905"
  },
  {
    "text": "I- I think what often happens more, is people just plot the data, and if it looks clearly non-Gaussian,",
    "start": "3556905",
    "end": "3562800"
  },
  {
    "text": "then, you know, there will be reasons not to use GDA. But what happens often is that um,",
    "start": "3562800",
    "end": "3569055"
  },
  {
    "text": "uh, uh, yeah sometimes you just have a very small training set and it's just a matter of judgment, right?",
    "start": "3569055",
    "end": "3574650"
  },
  {
    "text": "Like if you have, you- if you have, uh, uh, uh, you know, I don't know, 50 examples of healthcare records,",
    "start": "3574650",
    "end": "3580155"
  },
  {
    "text": "then you just have to ask some doctors and ask, \"Well, do you think the distribution is rath- rath- relatively Gaussian,\" and use domain knowledge like that.",
    "start": "3580155",
    "end": "3587880"
  },
  {
    "text": "Right? I think- by the way a- another philosophical point, um, I think that, uh, the machine learning world has,",
    "start": "3587880",
    "end": "3595500"
  },
  {
    "text": "frank- you know, a little bit overhyped big data, right? And- and yes it's true that when we have more data,",
    "start": "3595500",
    "end": "3601020"
  },
  {
    "text": "it's great and I love data and a- um, having more data pretty much never hurts and usually the more data the better, so all that is true.",
    "start": "3601020",
    "end": "3607680"
  },
  {
    "text": "And I think we did a good job telling people that high-level message, you know, more data almost always helps. But, um, uh, I think a lot of the skill in machine learning",
    "start": "3607680",
    "end": "3616260"
  },
  {
    "text": "these days is getting your algorithms to work even when you don't have a million in examples, even you don't have a hundred million examples.",
    "start": "3616260",
    "end": "3622440"
  },
  {
    "text": "So there are lots of machine learning applications where you just don't have a million examples, uh, you have a hundred examples and, um,",
    "start": "3622440",
    "end": "3629625"
  },
  {
    "text": "it's then the skill in designing your learning algorithm matters much more. Um, so if you take something like ImageNet,",
    "start": "3629625",
    "end": "3635415"
  },
  {
    "text": "mi- million in- in- images, there are now dozens of teams, maybe hundreds of teams, I don't know. They can get great results.",
    "start": "3635415",
    "end": "3640740"
  },
  {
    "text": "They give a million examples, right? and so the performance difference between teams, you know, there are now dozens of teams that get great performance,",
    "start": "3640740",
    "end": "3647820"
  },
  {
    "text": "if a million examples, uh, for- for- for image classification, for ImageNet. But if you have only a hundred examples,",
    "start": "3647820",
    "end": "3654000"
  },
  {
    "text": "then the high-skilled teams will actually do much, much, much, much better than the low skilled teams,",
    "start": "3654000",
    "end": "3659280"
  },
  {
    "text": "whereas the performance gap is smaller when you have giant data sets I think, so and I think that it's these types of intuitions,",
    "start": "3659280",
    "end": "3665984"
  },
  {
    "text": "you know, what assumptions you use, generative or discriminative, that actually distinguishes the high-skilled teams and, uh, and, uh,",
    "start": "3665985",
    "end": "3671670"
  },
  {
    "text": "and the less experienced teams and drives a lot of the performance differences when you have small data.",
    "start": "3671670",
    "end": "3676800"
  },
  {
    "text": "Oh, and if someone goes to you and says, \"Oh you only have a hundred examples, you'll never do anything.\"",
    "start": "3676800",
    "end": "3682425"
  },
  {
    "text": "Uh, then I don't know, if- if there's a competitor saying that, then I'll say, \"Great, you know, don't do it because I can make it work.\"",
    "start": "3682425",
    "end": "3688380"
  },
  {
    "text": "Uh, well, I don't know. Uh, but- but I think there are a lot of applications where your skill at designing a machine learning system,",
    "start": "3688380",
    "end": "3694065"
  },
  {
    "text": "really makes a bigger difference when you have a- make- makes a- it makes a difference from big data and small data, but it just- this is a very clear where you don't have much data,",
    "start": "3694065",
    "end": "3701970"
  },
  {
    "text": "is the assumptions you code into the algorithm like, is it Gaussian, is it Poisson? That- that skill allows you to drive",
    "start": "3701970",
    "end": "3708315"
  },
  {
    "text": "much bigger performance than a lower-skill team would be able to. All right. This is- uh- uh-",
    "start": "3708315",
    "end": "3715005"
  },
  {
    "text": "coul- could- I should still take questions from all of you. Yeah, go ahead.",
    "start": "3715005",
    "end": "3718000"
  },
  {
    "text": "Um, what's the implication when [inaudible]. Oh, sure. So does this, uh, yes, so what's the general statement of this?",
    "start": "3723680",
    "end": "3730530"
  },
  {
    "text": "Yes, so if, uh, x given y equals 1, uh, it comes from an exponential family distribution,",
    "start": "3730530",
    "end": "3736095"
  },
  {
    "text": "x given y equals 0 comes to an exponential family distribution, it's the same exponential family distribution and if they",
    "start": "3736095",
    "end": "3741390"
  },
  {
    "text": "vary only by the natural parameter of the exponential family distribution, then this will be logistic.",
    "start": "3741390",
    "end": "3747180"
  },
  {
    "text": "Yeah. Um, I think this was once a midterm homework problem to prove this actually?",
    "start": "3747180",
    "end": "3752265"
  },
  {
    "text": "But, yeah. All right, uh, actually let's take one last question then we move on, go ahead.",
    "start": "3752265",
    "end": "3758020"
  },
  {
    "text": "Uh, if performance [inaudible] Oh, uh, does performance improvement happen even as you increase the number of classes?",
    "start": "3762290",
    "end": "3769800"
  },
  {
    "text": "Uh, ye- I think so yes,",
    "start": "3769800",
    "end": "3775140"
  },
  {
    "text": "uh, and the generalization of this would be the Softmax Regression which I didn't talk about. But yes. I think it's a similar thing holds true for, um,",
    "start": "3775140",
    "end": "3782339"
  },
  {
    "text": "GDA for multiple- and we have so far we're going to talk about Binary Classification, whether you have more than two classes. But, uh, but yes, similar- similar things holds true for,",
    "start": "3782340",
    "end": "3790590"
  },
  {
    "text": "uh, like a GDA with three classes and Softmax. Yeah. Oh yes, right. You saw Softmax the other day.",
    "start": "3790590",
    "end": "3797684"
  },
  {
    "text": "Cool. Um, and this- this theme",
    "start": "3797685",
    "end": "3803370"
  },
  {
    "text": "that when you have less data the algorithm needs to rely more on assumptions you code in.",
    "start": "3803370",
    "end": "3808740"
  },
  {
    "text": "This is a recurring theme that we'll come back to it as well. This is one of the important principles of machine learning,",
    "start": "3808740",
    "end": "3814140"
  },
  {
    "text": "that when you have less data your skill at coding and your knowledge matters much more.",
    "start": "3814140",
    "end": "3819404"
  },
  {
    "text": "Uh, this is a theme we'll come back to you when we talk about much more complicated learning algorithms as well. All right.",
    "start": "3819405",
    "end": "3828960"
  },
  {
    "text": "So, uh, I want a fresh board for this.",
    "start": "3828960",
    "end": "3833560"
  },
  {
    "text": "So you've seen GDA in the context of, um, continuous valued, uh, features x.",
    "start": "3839390",
    "end": "3848775"
  },
  {
    "text": "The last thing I want to do today, um, is talk about one more generative learning algorithm called Naive Bayes,",
    "start": "3848775",
    "end": "3858765"
  },
  {
    "text": "um, and I'm gonna use as most of the example; e-mail spam classification, but this- this is- this-",
    "start": "3858765",
    "end": "3866010"
  },
  {
    "text": "I guess this is our first foray into natural language processing, right? But given in a piece of text, like given a piece of email,",
    "start": "3866010",
    "end": "3871185"
  },
  {
    "text": "can you classify this as spam or not spam? Or, uh, other examples, uh, uh, actually several years ago, Ebay,",
    "start": "3871185",
    "end": "3877740"
  },
  {
    "text": "used to have a problem of, you know, the- if someone's trying to sell something and you write a text description, right? \"Hey, I have a secondhand, you know,",
    "start": "3877740",
    "end": "3884610"
  },
  {
    "text": "Roomba, I'm trying to sell it on Ebay.\" How do you take that text that someone wrote over the description and categorize it, is it an electronic thing or are they trying to sell a TV?",
    "start": "3884610",
    "end": "3892170"
  },
  {
    "text": "Are they trying to sell clothing? Uh, but these- these examples of text classification problems, we have a piece of text and you want to classify into one of",
    "start": "3892170",
    "end": "3899955"
  },
  {
    "text": "two categories for spam or not spam or one of maybe thousands of categories, and they're trying to take a product description and classify it into one of the classes.",
    "start": "3899955",
    "end": "3908070"
  },
  {
    "text": "Um, and so the first question we will have is, um, uh,",
    "start": "3908070",
    "end": "3914670"
  },
  {
    "text": "given the e-mail problem, uh, given the e-mail classification problem, how do you represent it as a feature vector?",
    "start": "3914670",
    "end": "3923560"
  },
  {
    "text": "And so, um, in Naive Bayes what we're going to do is take your e-mail,",
    "start": "3927410",
    "end": "3934545"
  },
  {
    "text": "take a piece of e-mail and first map it to a feature vector X.",
    "start": "3934545",
    "end": "3940365"
  },
  {
    "text": "And we'll do so as follows, which is first, um, let's start with a- let's start with",
    "start": "3940365",
    "end": "3947010"
  },
  {
    "text": "the English dictionary and make a list of all the words in the English dictionary, right?",
    "start": "3947010",
    "end": "3952200"
  },
  {
    "text": "So first of all there's the English dictionary as A, second word in the English edition is aardvark. Third word is aardwolf.",
    "start": "3952200",
    "end": "3959220"
  },
  {
    "text": "[BACKGROUND] No, it's easy, look it up. [NOISE] [LAUGHTER]",
    "start": "3959220",
    "end": "3964890"
  },
  {
    "text": "Um, and then, you know, uh, uh, e- e-mail spam lot of people asking to buy stuff so that they would buy, right?",
    "start": "3964890",
    "end": "3971565"
  },
  {
    "text": "And then, um, uh, and then the last word in my dictionary is zymurgy,",
    "start": "3971565",
    "end": "3976664"
  },
  {
    "text": "which is the technological chemistry that refers to the fermentation process in brewing.",
    "start": "3976664",
    "end": "3983370"
  },
  {
    "text": "Um, So- so- I think it is a useful way to think about it, in- in- in- practice, what you do is not, uh, uh,",
    "start": "3983370",
    "end": "3988950"
  },
  {
    "text": "actually look at the dictionary but look at the top 10,000 words, you know, in your training set. Right? So maybe you have 10,000,",
    "start": "3988950",
    "end": "3994920"
  },
  {
    "text": "it's easier to think about it as if it was a dictionary but, you know, in practice, well, you- the other thing that's- dictionary has too many words,",
    "start": "3994920",
    "end": "4001280"
  },
  {
    "text": "but where- the other way to do this is to look through your own e-mail co-pairs and just find the top 10,000 occurring words and use that as a feature set,",
    "start": "4001280",
    "end": "4009110"
  },
  {
    "text": "and so I don't know. Right? And your e-mails, I guess you're getting a bunch of e-mail about- from us or maybe others about CS229.",
    "start": "4009110",
    "end": "4015340"
  },
  {
    "text": "So CS229 might appear in your dictionary of building your e-mail spam filter for yourself, even if it doesn't appear in the- in the official, uh,",
    "start": "4015340",
    "end": "4022240"
  },
  {
    "text": "was it like the Oxford dictionary, just yet just- just- just- you wait, we'll- we- we'll get CS229 there someday.",
    "start": "4022240",
    "end": "4028245"
  },
  {
    "text": "All right. Um, and so given an e-mail,",
    "start": "4028245",
    "end": "4034640"
  },
  {
    "text": "what we would like to do is then, um, take this piece of text and represent it as a feature vector.",
    "start": "4034640",
    "end": "4040460"
  },
  {
    "text": "And so one way to do this is, um, you can create a binary feature vector,",
    "start": "4040460",
    "end": "4045485"
  },
  {
    "text": "that puts a 1, if a word appears in the e-mail and puts a 0 if it doesn't.",
    "start": "4045485",
    "end": "4051755"
  },
  {
    "text": "Right? So if you've gotten an e-mail, um, uh, that asks you to, you know, buy some stuff and then the word A appears in e-mail, you put a 1 there.",
    "start": "4051755",
    "end": "4059480"
  },
  {
    "text": "Did not try to sell aardvark or aardwolf, so 0 there, buy and so on.",
    "start": "4059480",
    "end": "4066605"
  },
  {
    "text": "Right? So you take a- take an e-mail and turn it into a binary feature vector.",
    "start": "4066605",
    "end": "4073670"
  },
  {
    "text": "Um, and so here the feature vector is 0, 1 to the n,",
    "start": "4073670",
    "end": "4081260"
  },
  {
    "text": "because there's a n-dimensional binary feature vector, where- where for the purpose of illustration, let's say,",
    "start": "4081260",
    "end": "4087305"
  },
  {
    "text": "n is 10,000 because you're using, you know, take the top 10,000 words, uh, that appear in your e-mail training set as the dictionary that you will use.",
    "start": "4087305",
    "end": "4096920"
  },
  {
    "text": "So, um.",
    "start": "4096920",
    "end": "4109759"
  },
  {
    "text": "So in other words, X_i is indicator word i appears in the e-mail, right?",
    "start": "4109760",
    "end": "4122614"
  },
  {
    "text": "So it's either 0 or 1 depending on whether or not that word i from this list appears in your e-mail.",
    "start": "4122615",
    "end": "4129710"
  },
  {
    "text": "Now, um, in the Naive Bayes algorithm, we're going to build a generative learning algorithm.",
    "start": "4129710",
    "end": "4138245"
  },
  {
    "text": "Um, and so we want to model P of x given y, right?",
    "start": "4138245",
    "end": "4149000"
  },
  {
    "text": "As well as P of y, okay? But there are, uh,",
    "start": "4149000",
    "end": "4155119"
  },
  {
    "text": "2 to the 10,000 possible values of x, right?",
    "start": "4155120",
    "end": "4165005"
  },
  {
    "text": "Because x is a binary vector of this 10,000 dimensional. So we try to model P of x in the straightforward way as a multinomial distribution over,",
    "start": "4165005",
    "end": "4175325"
  },
  {
    "text": "you know, 2 to the 10,000 possible outcomes. Then you need, right, uh, uh, you need,",
    "start": "4175325",
    "end": "4181474"
  },
  {
    "text": "you know 2 to the 10,000 parameters, right? Which is a lot, or technically,",
    "start": "4181475",
    "end": "4187430"
  },
  {
    "text": "you need 2 to 10,000 minus 1 parameter because that adds up to 1, and you can see one parameter.",
    "start": "4187430",
    "end": "4192560"
  },
  {
    "text": "But so, modeling this without additional assumptions won't- won't work, right, because of the excessive number of parameters.",
    "start": "4192560",
    "end": "4200480"
  },
  {
    "text": "So in the Naive Bayes algorithm, we're going to assume that X_i's are",
    "start": "4200480",
    "end": "4208470"
  },
  {
    "text": "conditionally independent given y, okay?",
    "start": "4208870",
    "end": "4221150"
  },
  {
    "text": "Uh, let me just write out what this means, but so P of x_1 up to x_10,000 given y by the chain rule of probability,",
    "start": "4221150",
    "end": "4232925"
  },
  {
    "text": "this is equal to P of x_1 given y times P of x_2 given,",
    "start": "4232925",
    "end": "4243949"
  },
  {
    "text": "um, x_1 and y times p of x_3 given x_1,",
    "start": "4243950",
    "end": "4250330"
  },
  {
    "text": "x_2 Y up to your p of x_10,000 given, and so on, right?",
    "start": "4250330",
    "end": "4257770"
  },
  {
    "text": "So I haven't made any assumptions yet. This is just a true statement of fact as always true by the- by the chain rule of probability.",
    "start": "4257770",
    "end": "4264580"
  },
  {
    "text": "Um, and what we're going to assume which is what this assumption is,",
    "start": "4264580",
    "end": "4270050"
  },
  {
    "text": "is that this is equal to this first term no change the",
    "start": "4275860",
    "end": "4280969"
  },
  {
    "text": "x_2 given y p of x_3 given y and so on,",
    "start": "4280970",
    "end": "4286040"
  },
  {
    "text": "p of X_10,000 given y, okay?",
    "start": "4286040",
    "end": "4291440"
  },
  {
    "text": "So this assumption is called a conditional independence assumption it's also sometimes called the Naive Bayes assumption.",
    "start": "4291440",
    "end": "4299420"
  },
  {
    "text": "But you're assuming that, um, so long as you know why the chance of seeing the words,",
    "start": "4299420",
    "end": "4306215"
  },
  {
    "text": "um, aardvark in your e-mail does not depend on whether the word \"A\" appears in your e-mail, right?",
    "start": "4306215",
    "end": "4313250"
  },
  {
    "text": "Um, and this is one of those assumptions that is definitely not a true assumption and that is just not mathematically true assumption.",
    "start": "4313250",
    "end": "4319520"
  },
  {
    "text": "Just that sometimes your data isn't perfectly Gaussian, but if it was Gaussian you can kind of get away with it. So this assumption is not true,",
    "start": "4319520",
    "end": "4327140"
  },
  {
    "text": "um, in a mathematical sense, but it may be not so horrible that you can't get away with it, right?",
    "start": "4327140",
    "end": "4333199"
  },
  {
    "text": "Um, and so- so- so it's like, if you- if any of you are familiar with probabilistic graphical models,",
    "start": "4333200",
    "end": "4339050"
  },
  {
    "text": "if you've taken CS-228, uh, this assumption is summarizing this picture,",
    "start": "4339050",
    "end": "4344240"
  },
  {
    "text": "and if you haven't taken CS-228 this picture won't make sense, but don't worry about it. Um, right, that, uh,",
    "start": "4344240",
    "end": "4352940"
  },
  {
    "text": "once you know the class label is a spam or not spam whether or not each word appears or does not appear is independent, okay?",
    "start": "4352940",
    "end": "4360574"
  },
  {
    "text": "So this is called conditional. So the mechanics of this assumption is really just captured by this equation,",
    "start": "4360575",
    "end": "4366905"
  },
  {
    "text": "um, and you just use this equation, that's all you need to derive Naive Bayes. But intuition is that if I tell you whether",
    "start": "4366905",
    "end": "4374780"
  },
  {
    "text": "this piece- if I tell you that this piece of e-mail is spam then whether the word by appears in it doesn't affect you believes that what- whether the word",
    "start": "4374780",
    "end": "4381950"
  },
  {
    "text": "mortgage or discount or whatever spammy words appear, right?",
    "start": "4381950",
    "end": "4386850"
  },
  {
    "text": "So just to summarize, this is product from i equals 1 through n of p of X_i given y.",
    "start": "4387730",
    "end": "4397020"
  },
  {
    "text": "All right, so the parameters of this model,",
    "start": "4426670",
    "end": "4431945"
  },
  {
    "text": "um, are, I'm going to write it, Phi subscript,",
    "start": "4431945",
    "end": "4437750"
  },
  {
    "text": "um, j given y equals",
    "start": "4437750",
    "end": "4443330"
  },
  {
    "text": "1 as the probability that x_j equals 1 given y equals 1,",
    "start": "4443330",
    "end": "4449795"
  },
  {
    "text": "phi subscript j given y equals 0, and then Phi.",
    "start": "4449795",
    "end": "4462539"
  },
  {
    "text": "And just to distinguish all these Phi's from each other, we can just call this Phi subscript y, okay?",
    "start": "4464890",
    "end": "4472010"
  },
  {
    "text": "So this parameter says, if a spam e-mail, if y equals 1 is spam and y equals 0 is not spam.",
    "start": "4472010",
    "end": "4478820"
  },
  {
    "text": "If it's a spam e-mail, what's the chance of word j appearing in the e-mail? If it's not spam e-mail what's the chance of word j appearing in the e-mail.",
    "start": "4478820",
    "end": "4487325"
  },
  {
    "text": "Then also, what's the cost prior, what's the prior probability that the next e-mail you receive in your, uh, in your- in your inbox is spam e-mail?",
    "start": "4487325",
    "end": "4496860"
  },
  {
    "text": "And so to fit the parameters of this model,",
    "start": "4500620",
    "end": "4505760"
  },
  {
    "text": "you would s- similar to Gaussian discriminant analysis,",
    "start": "4505760",
    "end": "4511380"
  },
  {
    "text": "write down the John- joint likelihood. So the joint likelihood of these parameters, right?",
    "start": "4512530",
    "end": "4522890"
  },
  {
    "text": "Is a product, you know,",
    "start": "4522890",
    "end": "4528800"
  },
  {
    "text": "given these parameters, right?",
    "start": "4528800",
    "end": "4534770"
  },
  {
    "text": "Similar to what we had for Gaussian discriminant analysis. And the maximum likelihood estimates, um, if you take this,",
    "start": "4534770",
    "end": "4543110"
  },
  {
    "text": "take logs, take derivatives, set derivatives to 0, solve for the values that maximize this, you find that the maximum likelihood estimates of the parameters are, Phi_y,",
    "start": "4543110",
    "end": "4552200"
  },
  {
    "text": "this is pretty much what you'd expect, right?",
    "start": "4552200",
    "end": "4560840"
  },
  {
    "text": "It's just a fraction of spam e-mails and, uh, Phi of j given y equals 1 is, um, well,",
    "start": "4560840",
    "end": "4569570"
  },
  {
    "text": "I'll write this out in indicator function notation.",
    "start": "4569570",
    "end": "4573060"
  },
  {
    "text": "Oh, shoot, sorry.",
    "start": "4579970",
    "end": "4582930"
  },
  {
    "text": "Okay. So that's the indicator function notation of writing notes.",
    "start": "4603880",
    "end": "4609094"
  },
  {
    "text": "Look through your, uh, training set, find all the spam e-mails and of all the spam e-mails,",
    "start": "4609095",
    "end": "4614765"
  },
  {
    "text": "i.e., examples of y equals 1 count up what fraction of them had word j in it, right?",
    "start": "4614765",
    "end": "4620690"
  },
  {
    "text": "So you estimate that the chance of word j appearing- you estimate the chance of the word by appearing in a spam e-mail is",
    "start": "4620690",
    "end": "4627070"
  },
  {
    "text": "just we have all the spam e-mails in your training set, what fraction of them contain the word by?",
    "start": "4627070",
    "end": "4632665"
  },
  {
    "text": "What- what fraction of them had, you know, x_j equals 1 for say, the word by, okay?",
    "start": "4632665",
    "end": "4637970"
  },
  {
    "text": "Um, and so it turns out that if you implement this algorithm,",
    "start": "4637970",
    "end": "4643055"
  },
  {
    "text": "it will- it will nearly work, I guess, uh, uh, but this is Naive Bayes for,",
    "start": "4643055",
    "end": "4648770"
  },
  {
    "text": "um, for e-mail spam classification, right? And I mentioned, uh, one reason this,",
    "start": "4648770",
    "end": "4655100"
  },
  {
    "text": "uh, and it turns out that what one fixed to this algorithm, which we'll talk about on Wednesday, um,",
    "start": "4655100",
    "end": "4661205"
  },
  {
    "text": "this is actually, it's actually a not too horrible spam classifier. It turns out that if you used logistic regression",
    "start": "4661205",
    "end": "4668570"
  },
  {
    "text": "for spam classification you do better than this almost all the time. But this is a very efficient algorithm,",
    "start": "4668570",
    "end": "4674360"
  },
  {
    "text": "because estimating these parameters is just counting, and then computing probabilities is just multiplying a bunch of numbers.",
    "start": "4674360",
    "end": "4680270"
  },
  {
    "text": "So there's nothing iterative about this. So you can fit this model very efficiently and also keep on updating this model even as you get new data,",
    "start": "4680270",
    "end": "4687245"
  },
  {
    "text": "even as you get new- new- new uses hits mark or spam or whatever, even as you get new data, you can update this model very efficiently.",
    "start": "4687245",
    "end": "4694489"
  },
  {
    "text": "Um, but it turns out that, uh, actually, the biggest problem with this algorithm is,",
    "start": "4694490",
    "end": "4700205"
  },
  {
    "text": "what happens if, uh, this is zero or if- if you get zeros in some of these equations, right?",
    "start": "4700205",
    "end": "4705830"
  },
  {
    "text": "But we'll come back to that when we talk about Laplace moving on Wednesday, okay?",
    "start": "4705830",
    "end": "4711980"
  },
  {
    "text": "All right, any quick questions before we wrap up? Okay, okay good.",
    "start": "4711980",
    "end": "4718135"
  },
  {
    "text": "So now you've learned about generative learning algorithms, um, we'll come back on Wednesday and learn about some more fine details how to make this work even better.",
    "start": "4718135",
    "end": "4725770"
  },
  {
    "text": "So let's break, I'll see you on Wednesday.",
    "start": "4725770",
    "end": "4728210"
  }
]