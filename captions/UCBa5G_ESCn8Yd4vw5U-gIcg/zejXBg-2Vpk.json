[
  {
    "start": "0",
    "end": "5120"
  },
  {
    "text": "Thanks so much. Great to be here. And Happy Halloween--\nbelated Halloween, everyone.",
    "start": "5120",
    "end": "10770"
  },
  {
    "text": "So I think the talk is going\nto be split into two sections. So I'll start by spending like\n10 minutes, 15 minutes chatting",
    "start": "10770",
    "end": "18020"
  },
  {
    "text": "about transformers in general. But I'm assuming most of\nyou are familiar with them and we can move on to NPTs,\nwhich Jannik and Neil will",
    "start": "18020",
    "end": "26510"
  },
  {
    "text": "be presenting. So let's see, I'm going to try\nto fly through the transformer",
    "start": "26510",
    "end": "35750"
  },
  {
    "text": "overview and maybe spend\na little bit extra time on the history of\ntransformers and maybe just",
    "start": "35750",
    "end": "42110"
  },
  {
    "text": "tell the story a little bit. I think that might\nbe more interesting.",
    "start": "42110",
    "end": "47880"
  },
  {
    "text": "So just in terms of the\ntransformer architecture, the two kind of things that it\nintroduced for the first time",
    "start": "47880",
    "end": "54740"
  },
  {
    "text": "were multi-headed attention\nand self-attention. And then, it combined those with\nfast autoregressive decoding.",
    "start": "54740",
    "end": "61400"
  },
  {
    "text": "So before the transformer,\npretty much everyone was using LSTMs.",
    "start": "61400",
    "end": "66410"
  },
  {
    "text": "And LSTMs with attention. But I'll try to get\ninto the difference of self-attention,\nmulti-headed attention.",
    "start": "66410",
    "end": "75770"
  },
  {
    "text": "So originally, you\nhave two sequences and you would have a attention\nmodule, which would attend",
    "start": "75770",
    "end": "83570"
  },
  {
    "text": "from the source to the target. And so, each token or each\nword in the source sequence",
    "start": "83570",
    "end": "89240"
  },
  {
    "text": "would get associated with\na soft approximation of one element in the target sequence.",
    "start": "89240",
    "end": "97340"
  },
  {
    "text": "And so, you'd end up\nwith something like this. But with self-attention,\nwe did away with the two separate sequences.",
    "start": "97340",
    "end": "103729"
  },
  {
    "text": "We make them both the same. And so, you're relating each\nelement within the sequence",
    "start": "103730",
    "end": "109160"
  },
  {
    "text": "to another element\nin the sequence. And so, the idea\nhere is that you're",
    "start": "109160",
    "end": "117140"
  },
  {
    "text": "learning a relationship\nof the words within a sentence\nto the other words. So you can imagine something\nlike an adjective, which",
    "start": "117140",
    "end": "124670"
  },
  {
    "text": "is being applied to a noun. And so you want to relate that\nadjective-- like the blue ball, you want to relate blue\nas referring to ball.",
    "start": "124670",
    "end": "132860"
  },
  {
    "text": "So we're learning patterns\nwithin the sequence-- intra-sequence patterns. ",
    "start": "132860",
    "end": "139910"
  },
  {
    "text": "Sorry, I gave this\ntalk in Kenya. So I'm using Swahili here.",
    "start": "139910",
    "end": "145640"
  },
  {
    "text": "But with multi-head\nattention, the idea is you have each\nword represented by an embedding, which is\nin the depth dimension here.",
    "start": "145640",
    "end": "153599"
  },
  {
    "text": "And then, you have\nyour sentence of words. You split that up into a\nbunch of different groups.",
    "start": "153600",
    "end": "159690"
  },
  {
    "text": "So here, I've chopped it\ndepth-wise into four groups. You apply attention to each one\nof these groups independently.",
    "start": "159690",
    "end": "167330"
  },
  {
    "text": "And then, when you\nget the result back, you concatenate them together\nand you're back to your model",
    "start": "167330",
    "end": "173900"
  },
  {
    "text": "dimension representation. So what this lets you do is, if\neach attention-- each attention",
    "start": "173900",
    "end": "181415"
  },
  {
    "text": "head can now focus on\nlearning one pattern. So maybe attention head one\nis learning the relationship",
    "start": "181415",
    "end": "188420"
  },
  {
    "text": "of adjectives to nouns. And the second attention head\ncan learn something different.",
    "start": "188420",
    "end": "194430"
  },
  {
    "text": "So this lets us learn\na hierarchy or a list of different relationships.",
    "start": "194430",
    "end": "200960"
  },
  {
    "text": "OK, so that was self-attention. The other piece is fast\nautoregressive decoding.",
    "start": "200960",
    "end": "207725"
  },
  {
    "text": " And do I really want\nto go into this?",
    "start": "207725",
    "end": "213590"
  },
  {
    "text": "OK, I will. So the important\nthing about this is that if you're doing normal\nautoregressive decoding, what",
    "start": "213590",
    "end": "220920"
  },
  {
    "text": "you do is you generate\nyour first token. And now, conditioned\non that first token, you generate the second. And conditioned on the first\ntwo, you generate the third",
    "start": "220920",
    "end": "228112"
  },
  {
    "text": "and so on and so forth. That's super slow, right? It's a loop applying this\nthing again and again.",
    "start": "228113",
    "end": "233950"
  },
  {
    "text": "And so, what we\ncan do instead is we make an assumption\nin the code that our model always\ngenerates the right thing.",
    "start": "233950",
    "end": "242849"
  },
  {
    "text": "And we generate-- and then,\nwe generate a prediction only one token ahead.",
    "start": "242850",
    "end": "248640"
  },
  {
    "text": "And so, the way that\nthis looks is you-- OK, so y hat here--",
    "start": "248640",
    "end": "257430"
  },
  {
    "text": " sorry. One second. Input to output. So you have your\noutputs, which are y.",
    "start": "257430",
    "end": "264380"
  },
  {
    "text": "You have your targets,\nwhich are y hat. And what you do is you\nfeed in those gold targets",
    "start": "264380",
    "end": "272449"
  },
  {
    "text": "so that you don't need\nto actually do this loop. So instead of assuming--\ninstead of having",
    "start": "272450",
    "end": "277580"
  },
  {
    "text": "to generate the first\ntoken, feed it back into your architecture,\ngenerate a second token, you feed in the entire\ntarget sequence.",
    "start": "277580",
    "end": "285830"
  },
  {
    "text": "And you just pretend that you\ngenerate all the right tokens up to position k. And then, you predict\nthe k plus first.",
    "start": "285830",
    "end": "292759"
  },
  {
    "text": "And you compute\nyour loss on that. And so in reality,\nyour model might have generated at the\nbeginning of training junk.",
    "start": "292760",
    "end": "300380"
  },
  {
    "text": "But you're getting a\nloss as if your model had seen all the correct\ntokens and is now just",
    "start": "300380",
    "end": "305870"
  },
  {
    "text": "predicting the next one. This is a little\nbit subtle, but it's hugely impactful for training\nspeed because all of this",
    "start": "305870",
    "end": "313730"
  },
  {
    "text": "can be done in parallel. And so, it's actually what\nmakes transformers so scalable.",
    "start": "313730",
    "end": "318935"
  },
  {
    "text": " OK, so in order to\ndo this successfully,",
    "start": "318935",
    "end": "324250"
  },
  {
    "text": "if you were just feeding in\nall of the correct tokens",
    "start": "324250",
    "end": "329560"
  },
  {
    "text": "naively, what would\nhappen is your model would just be able to look\nforward in time and cheat.",
    "start": "329560",
    "end": "338229"
  },
  {
    "text": "So you've put in all of your\ntrue targets, the things that you're trying to get\nyour model to predict.",
    "start": "338230",
    "end": "343552"
  },
  {
    "text": "And so, if that's what you're\ncomputing your loss on, you could just look\nforward in time and say, OK, I'm just\ngoing to grab that",
    "start": "343552",
    "end": "349330"
  },
  {
    "text": "and would get 0 error\ntrivially, right? Because you've given it\nall the right answers. So what we have to do\ninside the architecture",
    "start": "349330",
    "end": "356169"
  },
  {
    "text": "is we need to actually prevent\nthe attention mechanism from being able to look at\ntokens that it shouldn't",
    "start": "356170",
    "end": "362890"
  },
  {
    "text": "have been able to see already. So the way that this\nlooks is you create a mask",
    "start": "362890",
    "end": "370150"
  },
  {
    "text": "on your attention. And so, sorry,\nthis is the example",
    "start": "370150",
    "end": "375160"
  },
  {
    "text": "of doing a trivial attention. If you don't mask your attention\nproperly, what it's going to do",
    "start": "375160",
    "end": "382090"
  },
  {
    "text": "is it's just going to\nlook into the future, just grab the token that\nyou're telling it to predict,",
    "start": "382090",
    "end": "387670"
  },
  {
    "text": "and copy it over. And so, it learns\nsomething trivial. Something it doesn't\nactually generalize. And so, what we\ndo is we actually",
    "start": "387670",
    "end": "393729"
  },
  {
    "text": "prevent it from attending\nto those tokens. We prevent it from\nattending into the future. For each position in\nthe source sequence,",
    "start": "393730",
    "end": "401350"
  },
  {
    "text": "we block out everything that\nit shouldn't be able to see. Everything into the future.",
    "start": "401350",
    "end": "407410"
  },
  {
    "text": "And then, as we move\ndown, we gradually unblock so it can start\nto see into the past.",
    "start": "407410",
    "end": "414110"
  },
  {
    "text": "So those are kind of the two-- the three major components\nof transformers.",
    "start": "414110",
    "end": "420640"
  },
  {
    "text": "The self-attention, the\nmulti-head attention, and then, deploying this gold targets--",
    "start": "420640",
    "end": "427120"
  },
  {
    "text": "decoding this fast\nautoregressive decoding. ",
    "start": "427120",
    "end": "432220"
  },
  {
    "text": "In terms of the\nstory, which might be a little bit more interesting.",
    "start": "432220",
    "end": "438600"
  },
  {
    "text": "So transformers--\nI was an intern with Lukasz Kaiser at\nGoogle back in 2017.",
    "start": "438600",
    "end": "446210"
  },
  {
    "text": "And I was sitting next\nto Noam and she was a couple of seats down from us.",
    "start": "446210",
    "end": "453110"
  },
  {
    "text": "And what's really incredible\nis that, essentially, this entire project came\ntogether in like three months",
    "start": "453110",
    "end": "459290"
  },
  {
    "text": "and it was done. So I showed up at Google. Noam had been working on\nautoregressive models.",
    "start": "459290",
    "end": "467419"
  },
  {
    "text": "Same thing with Ashish\nand Jakab and Nikki. And they had just been kind\nof like exploring the space,",
    "start": "467420",
    "end": "474080"
  },
  {
    "text": "figuring it out. And Lukasz and I,\nat the same time, we had been working on\nthis framework called",
    "start": "474080",
    "end": "479363"
  },
  {
    "text": "Tensor2Tensor,\nwhich was explicitly made for multimodal learning,\nautoregressive learning.",
    "start": "479363",
    "end": "488810"
  },
  {
    "text": "And Lukasz is kind of\na master of keeping",
    "start": "488810",
    "end": "494190"
  },
  {
    "text": "track of everything that's\nhappening in the field and adopting it. And so, within\nTensor2Tensor there",
    "start": "494190",
    "end": "500009"
  },
  {
    "text": "were these emerging little\nthings that maybe one paper had",
    "start": "500010",
    "end": "506620"
  },
  {
    "text": "been written about and\npeople were interested in, like LayerNorm but\nit hadn't actually taken off yet, the warm up in\nthe learning rate schedule.",
    "start": "506620",
    "end": "517330"
  },
  {
    "text": "All of these little\npieces were just default-- like on by default. And so,\nwhen Noam and Ashish and Nikki",
    "start": "517330",
    "end": "525910"
  },
  {
    "text": "and Jakab came over and\nadopted Tensor2Tensor, all of these things were\njust on by default. And so,",
    "start": "525910",
    "end": "534610"
  },
  {
    "text": "a lot of people, when they\nlook at the transformer paper, it just seems like\nthere's so many arbitrary little\nthings thrown in.",
    "start": "534610",
    "end": "542350"
  },
  {
    "text": "And now, in present day, these\nhave become standard for a lot of different training\nalgorithms, like the learning",
    "start": "542350",
    "end": "549070"
  },
  {
    "text": "rate warm up, the\nway that we did initialization,\nall of these pieces",
    "start": "549070",
    "end": "554860"
  },
  {
    "text": "have just become the norm. But back then, they had kind\nof just been introduced.",
    "start": "554860",
    "end": "561040"
  },
  {
    "text": "And so, we spent a lot of\ntime running ablations trying to figure out which were\nthe necessary pieces",
    "start": "561040",
    "end": "567930"
  },
  {
    "text": "and what made it work. And if any of you have actually\ntried training transformers",
    "start": "567930",
    "end": "573030"
  },
  {
    "text": "and tried pulling\nout the learning rate warm up or changing any\nof these little pieces,",
    "start": "573030",
    "end": "579360"
  },
  {
    "text": "you'll see that it really\ndoes break down optimization. It actually really\ndoes hurt performance.",
    "start": "579360",
    "end": "586005"
  },
  {
    "text": "For instance, like removing the\nLayerNorms, that type of thing. ",
    "start": "586005",
    "end": "591260"
  },
  {
    "text": "I always thought it\nwas kind of funny how all of these random additions\nthat Lukasz had just",
    "start": "591260",
    "end": "597578"
  },
  {
    "text": "thrown in because he was\nplaying around with them turned out to be crucial, and\nthey were just on by default.",
    "start": "597578",
    "end": "605810"
  },
  {
    "text": "So anyway, it was\nlike three months. I remember it all really\nstarted coming together",
    "start": "605810",
    "end": "612880"
  },
  {
    "text": "towards the end, like just\nbefore the NeurIPS deadline. ",
    "start": "612880",
    "end": "618000"
  },
  {
    "text": "And I can still remember sitting\nin the micro kitchen and Ashish telling me-- like\nit was just like-- I was a little intern.",
    "start": "618000",
    "end": "623790"
  },
  {
    "text": "Telling me like, this is\ngoing to be such a big deal. And I was like, yeah, sure, OK. I have no idea what's happening.",
    "start": "623790",
    "end": "629850"
  },
  {
    "text": "I just showed up. And he was like, no, dude,\nlike this actually matters.",
    "start": "629850",
    "end": "636060"
  },
  {
    "text": "We bumped up BLEU three points. And I was like,\nsick, great, anyway. ",
    "start": "636060",
    "end": "644480"
  },
  {
    "text": "And then, I can\nremember on the night of the deadline for NeurIPS. It was like 2:00 AM.",
    "start": "644480",
    "end": "651800"
  },
  {
    "text": "Ashish was the only\none left at the office and we were still like\nmoving around figures",
    "start": "651800",
    "end": "658700"
  },
  {
    "text": "and adjusting things. And then, I went to bed\nbut Ashish stayed up. And I slept in like this\ntiny little phone booth.",
    "start": "658700",
    "end": "667543"
  },
  {
    "text": "And then, for the other\npaper that I was submitting, I forgot to press Submit. But luckily, some lady opened\nthe door to the phone booth",
    "start": "667543",
    "end": "675650"
  },
  {
    "text": "and hit me in the head while\nI was sleeping in the morning. And just before the\ndeadline I got the paper in.",
    "start": "675650",
    "end": "681830"
  },
  {
    "text": "And so, I owe it to\nthat lady for submitting",
    "start": "681830",
    "end": "687350"
  },
  {
    "text": "to NeurIPS that year. But yeah, anyway, I think the\ncrazy thing about transformers was that it all came together\nin like three months.",
    "start": "687350",
    "end": "693740"
  },
  {
    "text": "Most of the ideas\nhappened in that span. And it was just like this sprint\ntowards the NeurIPS deadline.",
    "start": "693740",
    "end": "700565"
  },
  {
    "text": "I think a lot of the other\nmembers on the team, Jakab, Lukasz, Noam, Ashish, they\nknew how important it was.",
    "start": "700565",
    "end": "705710"
  },
  {
    "text": "But for me, I was\nlike, I don't know. I really did not\nappreciate the impact.",
    "start": "705710",
    "end": "714050"
  },
  {
    "text": "But in retrospect,\nit's been amazing how the community has kind of\ncome together and adopted it.",
    "start": "714050",
    "end": "721110"
  },
  {
    "text": "And I think most of\nthat can be ascribed to the ease of optimization. It seems like very robust\nto hyperparameter choices.",
    "start": "721110",
    "end": "728630"
  },
  {
    "text": "So you don't need to tune\nthe hell out of it, spend a lot of time tweaking\nlittle things.",
    "start": "728630",
    "end": "734078"
  },
  {
    "text": "And the other side\nis that it's super tailored to the\naccelerators that we run on.",
    "start": "734078",
    "end": "739700"
  },
  {
    "text": "So it's very paralyzable-- hyper-efficient.",
    "start": "739700",
    "end": "745130"
  },
  {
    "text": "And so, it lends\nitself to that kind of scaling law effort that's\nreally taken off in popularity.",
    "start": "745130",
    "end": "752340"
  },
  {
    "text": "OK, unless there\nare any questions-- Yeah, that's a\nfascinating story. That was such a cool story. Oh my God.",
    "start": "752340",
    "end": "759579"
  },
  {
    "text": "We're both excited, so we\njust muted at the same time. It's so cool.",
    "start": "759580",
    "end": "766760"
  },
  {
    "text": "Yeah, so that's my section. If there's any questions,\nhappy to answer them. Otherwise, let's get into NPTs.",
    "start": "766760",
    "end": "772400"
  },
  {
    "text": "NPTs are like, I think, they\nare such a nice next level",
    "start": "772400",
    "end": "778700"
  },
  {
    "text": "abstraction of the architecture. So you've probably seen\nthe trend of transformers",
    "start": "778700",
    "end": "784250"
  },
  {
    "text": "getting applied to new domains. First into vision\nand video and audio.",
    "start": "784250",
    "end": "792838"
  },
  {
    "text": "But this is kind of\nlike cutting back to an even more abstract level.",
    "start": "792838",
    "end": "798430"
  },
  {
    "text": "I think tabular data. I don't know. I'll let Jannik and Neil\ntake over from here. But I think NPT is a\npretty sick project.",
    "start": "798430",
    "end": "808760"
  },
  {
    "text": "Thanks, Aidan, for\nthe introduction. Thanks all for the invitation. We're very happy to be here.",
    "start": "808760",
    "end": "814430"
  },
  {
    "text": "And Neil and I are\nnow going to tell you about our self-attention\nbetween datapoints paper",
    "start": "814430",
    "end": "821510"
  },
  {
    "text": "where we just introduced the\nnon-parametric transformer architecture.",
    "start": "821510",
    "end": "826580"
  },
  {
    "text": "We'll start with a\nlittle bit of motivation, move on to explaining the\narchitecture in detail, then show you the experiments.",
    "start": "826580",
    "end": "832320"
  },
  {
    "text": "This is more or less a\nstep through of the paper, but maybe, with a little bit of\nextra insight here and there.",
    "start": "832320",
    "end": "837980"
  },
  {
    "text": " All right, as promised, the\nmotivation, a brief summary.",
    "start": "837980",
    "end": "845130"
  },
  {
    "text": "So we'll start by thinking about\nsomething that we don't often think about, and that is that\nfrom CNNs to transformers,",
    "start": "845130",
    "end": "852950"
  },
  {
    "text": "most of supervised\ndeep learning relies on parametric prediction. So what that means is that we\nhave some set of training data.",
    "start": "852950",
    "end": "860959"
  },
  {
    "text": "And we want to learn to predict\nthe outcomes y from the inputs x. And first, we set up some model\nwith tunable parameters theta.",
    "start": "860960",
    "end": "869600"
  },
  {
    "text": "Then, we optimized\nthese parameters to maximize predictive\nlikelihoods on a training set",
    "start": "869600",
    "end": "875089"
  },
  {
    "text": "or, equivalently, we\nminimize the loss. And then, after training,\nwe have this optimized set",
    "start": "875090",
    "end": "882200"
  },
  {
    "text": "of parameters theta. And then, at test time, we\njust put these into the model and use these parameters to\npredict on novel test data.",
    "start": "882200",
    "end": "890540"
  },
  {
    "text": "And so, crucially here,\nour prediction at test time only depends on these\nparameters, right?",
    "start": "890540",
    "end": "896240"
  },
  {
    "text": "It's parametric. Also, that means that\ngiven these parameters, the prediction is entirely\nindependent of the training",
    "start": "896240",
    "end": "903230"
  },
  {
    "text": "data. And so, why would we want\nto do parametric prediction?",
    "start": "903230",
    "end": "908862"
  },
  {
    "text": "Well, it's really convenient. Because all that we've\nlearned from the training data can be summarized\nin the parameters.",
    "start": "908862",
    "end": "914810"
  },
  {
    "text": "And so, at prediction\ntime, we only need these final\nparameters and we do not need to store the\ntraining data, which",
    "start": "914810",
    "end": "920053"
  },
  {
    "text": "might be really, really large. On the other hand,\nwe usually have models that already\npredict for a bunch of data",
    "start": "920053",
    "end": "926930"
  },
  {
    "text": "in parallel, right? Think of mini batching\nand modern architectures. And actually, things\nlike batch norm",
    "start": "926930",
    "end": "932840"
  },
  {
    "text": "already make these\ndata interact. And so, our thinking\nhere was that if we've",
    "start": "932840",
    "end": "938540"
  },
  {
    "text": "got all of this data\nin parallel anyways, there's no reason not\nto make use of it. And so, more a bit grander, we\nchallenge parametric prediction",
    "start": "938540",
    "end": "948500"
  },
  {
    "text": "as the dominant paradigm\nin deep learning. And so, we want to give models\nthe additional flexibility",
    "start": "948500",
    "end": "955130"
  },
  {
    "text": "of using the training\ndata directly when making predictions. And so, a bit more\nconcretely, we",
    "start": "955130",
    "end": "962060"
  },
  {
    "text": "introduced the non-parametric\ntransformer architecture. And this is going to be\na general deep learning",
    "start": "962060",
    "end": "968060"
  },
  {
    "text": "architecture, meaning\nwe can apply it to a variety of scenarios. NPTs will take the entire data\nset as input whenever possible.",
    "start": "968060",
    "end": "977730"
  },
  {
    "text": "And NPTs then, crucially, learn\nto predict from interactions between data points.",
    "start": "977730",
    "end": "984080"
  },
  {
    "text": "And to achieve this, we use\nmulti-head self-attention that, as Aidan has\nintroduced us to,",
    "start": "984080",
    "end": "990770"
  },
  {
    "text": "has just really established\nitself as the general purpose layer for reasoning.",
    "start": "990770",
    "end": "996060"
  },
  {
    "text": "We also take another thing\nfrom the NLP community and we use a stochastic\nmasking mechanism.",
    "start": "996060",
    "end": "1003110"
  },
  {
    "text": "And we use that to tell NPTs\nwhere to predict, and also to regularize the\nlearning task of it.",
    "start": "1003110",
    "end": "1009240"
  },
  {
    "text": "And lastly, of course,\nwe hope to convince you that this ends up\nworking really well and that this simple\nidea of learning",
    "start": "1009240",
    "end": "1016280"
  },
  {
    "text": "to predict from the other\ndatapoints of the input, from the training\npoints of the input, and supporting them as well.",
    "start": "1016280",
    "end": "1022190"
  },
  {
    "text": " Sorry. And so, very briefly summarizing\nwhat we've heard already.",
    "start": "1022190",
    "end": "1029530"
  },
  {
    "text": "A, we input into NPTs\nthe entire data set. And then, B, let's say, for\nthe purpose of this slide here,",
    "start": "1029530",
    "end": "1037030"
  },
  {
    "text": "we only care about predicting\nthe orange question mark in that green row.",
    "start": "1037030",
    "end": "1042220"
  },
  {
    "text": "And then, we can compare NPTs\nto parametric prediction, right? So a classical\ndeep learning model",
    "start": "1042220",
    "end": "1048280"
  },
  {
    "text": "would predict this\ntarget value only from the features of\nthat single green input.",
    "start": "1048280",
    "end": "1053410"
  },
  {
    "text": "To do that, it would use\nthe parameters theta. Those would depend\non whatever training data we've seen and so on.",
    "start": "1053410",
    "end": "1058820"
  },
  {
    "text": "But at test time, we only look\nat that single row for which we care about the prediction.",
    "start": "1058820",
    "end": "1064450"
  },
  {
    "text": "In contrast, NPTs predict\nan explicit dependence on all samples in the input.",
    "start": "1064450",
    "end": "1070180"
  },
  {
    "text": "They can look beyond that\nsingle gradation of interest. And look at all other\n[INAUDIBLE] that are there",
    "start": "1070180",
    "end": "1075340"
  },
  {
    "text": "and consider their\nvalues for prediction. So this presents an\nentirely different way",
    "start": "1075340",
    "end": "1080710"
  },
  {
    "text": "of thinking about how we\nlearn predictive mechanisms. And somebody on Twitter\ncalled this k-NN 2.0,",
    "start": "1080710",
    "end": "1086890"
  },
  {
    "text": "which we would have not\nwritten in the paper. But maybe it is\nkind of a nice way of thinking about how\nNPTs can learn to predict.",
    "start": "1086890",
    "end": "1095620"
  },
  {
    "text": "So of course, non-parametric\nmodels are a thing already. We didn't invent them at all. And I define them\nhere as prediction",
    "start": "1095620",
    "end": "1103780"
  },
  {
    "text": "in explicit dependence on\nthe training data, which is certainly what NPTs do. Classical examples,\nlike Gaussian processes,",
    "start": "1103780",
    "end": "1111100"
  },
  {
    "text": "k nearest neighbor,\nkernel methods, those might be familiar to you. And there exists, also,\nefforts to combine",
    "start": "1111100",
    "end": "1117670"
  },
  {
    "text": "the benefits of non-parametric\nmetrics and representation learning in a similar fashion\nto how we did it in NPTs.",
    "start": "1117670",
    "end": "1124880"
  },
  {
    "text": "However, these approaches are\nusually limited, in some sense, in comparison to NPTs, right?",
    "start": "1124880",
    "end": "1130000"
  },
  {
    "text": "They're often kind of motivated\nfrom the statistics community a bit more. They often requires some\nmore finicky approaches,",
    "start": "1130000",
    "end": "1135912"
  },
  {
    "text": "such as inference\nschemes or are limited in the interactions they can\nlearn or things like that. And so, we really think\nNPTs present maybe",
    "start": "1135912",
    "end": "1144820"
  },
  {
    "text": "the most versatile\nand most widely applicable of these\nnon-parametric prediction approaches.",
    "start": "1144820",
    "end": "1150020"
  },
  {
    "text": "That's something we\nexplicitly wanted to have. We wanted to have something\nthat's really easy to use, plug and play, works in\na ton of scenarios,",
    "start": "1150020",
    "end": "1156700"
  },
  {
    "text": "and works really well. And so, with that,\nI'll hand over",
    "start": "1156700",
    "end": "1161990"
  },
  {
    "text": "to Neil, who's going\nto tell you about the non-parametric transformer\narchitecture in all",
    "start": "1161990",
    "end": "1167240"
  },
  {
    "text": "of its details. We also have one question. [INTERPOSING VOICES] Go ahead.",
    "start": "1167240",
    "end": "1174470"
  },
  {
    "text": "Hi, Jannik. Hi. So, could you please go\nback to the previous slide?",
    "start": "1174470",
    "end": "1180650"
  },
  {
    "text": "The very previous slide?? Yes, yes. This slide, yeah. So in terms of the\nproblem definition,",
    "start": "1180650",
    "end": "1187250"
  },
  {
    "text": "I think it is quite similar\nto some meta learning problem, which basically\nlearns a mapping from a data",
    "start": "1187250",
    "end": "1194480"
  },
  {
    "text": "point on the data set\nto some predictions. So could you please\nsuggest any differences",
    "start": "1194480",
    "end": "1200510"
  },
  {
    "text": "between your problem setting and\nmeta learning problem settings.",
    "start": "1200510",
    "end": "1205850"
  },
  {
    "text": "I can't really figure out any\ndifferences between these two problems.",
    "start": "1205850",
    "end": "1212100"
  },
  {
    "text": "Well, I think it really\ndepends on the framing that you want to have, right?",
    "start": "1212100",
    "end": "1217789"
  },
  {
    "text": "So I would say\nmeta learning would be when I try to predict\nover multiple data sets.",
    "start": "1217790",
    "end": "1222980"
  },
  {
    "text": "So when I try to\npredict some-- when I try to learn some\nsort of prediction model where I can just plug\nin a different data",
    "start": "1222980",
    "end": "1229110"
  },
  {
    "text": "set and it will automatically--\nor almost automatically give me new predictions on this\ndifferent data distribution.",
    "start": "1229110",
    "end": "1235293"
  },
  {
    "text": "But that's not what\nwe do at all, right? We are training a single\nmodel for a fixed data set.",
    "start": "1235293",
    "end": "1240380"
  },
  {
    "text": "And so, this is why\nI wouldn't really call that meta learning\nbecause we're doing-- we're trying to print\non the same tasks",
    "start": "1240380",
    "end": "1246440"
  },
  {
    "text": "that all the supervised\ndeep learning or any supervised\nmachine learning method",
    "start": "1246440",
    "end": "1251929"
  },
  {
    "text": "is trying to predict well on. [INTERPOSING VOICES]",
    "start": "1251930",
    "end": "1257630"
  },
  {
    "text": "Okay, so assuming you use\nkind of same test site to test your training\nmodel, right?",
    "start": "1257630",
    "end": "1265250"
  },
  {
    "text": "[INTERPOSING VOICES] I mean like-- so basically,\nin meta learning,",
    "start": "1265250",
    "end": "1275470"
  },
  {
    "text": "we're going to test on different\nkind of meta test sets. But in your case,\nyou just want to use",
    "start": "1275470",
    "end": "1282265"
  },
  {
    "text": "a test set which is\nsimilar to the distribution",
    "start": "1282265",
    "end": "1287270"
  },
  {
    "text": "of your training set. Is that right? Yeah, absolutely. So we explore data set\ndistribution shift a bit.",
    "start": "1287270",
    "end": "1293260"
  },
  {
    "text": "I think it's a really\ninteresting scenario. I think meta learning\ndifferent data sets",
    "start": "1293260",
    "end": "1298450"
  },
  {
    "text": "is also an interesting\nscenario, right? When you have this model\nwhere you could just push in different data sets. But for the scope of this\npaper, it's very much",
    "start": "1298450",
    "end": "1307000"
  },
  {
    "text": "training set, test\nset, they come from the same distribution. And we're just trying to\ndo supervised learning",
    "start": "1307000",
    "end": "1314170"
  },
  {
    "text": "in a standard setting. I see. Cool. Thank you. Thank you for the question.",
    "start": "1314170",
    "end": "1320950"
  },
  {
    "text": "Yeah, and I would chime in a\ncouple of additional things, I guess. So at least from what I\nunderstand from the problem",
    "start": "1320950",
    "end": "1327040"
  },
  {
    "text": "definition of meta learning, I\nthink the aim is more, perhaps, being able to perform well on a\nnew data set with a relatively",
    "start": "1327040",
    "end": "1334540"
  },
  {
    "text": "small number of\nadditional gradient steps on that data set. So I think there are some\ninteresting ways that you could",
    "start": "1334540",
    "end": "1340300"
  },
  {
    "text": "actually consider applying\nNPTs in a meta learning type setting. And so we'll get into\nthis a little bit more.",
    "start": "1340300",
    "end": "1346010"
  },
  {
    "text": "But for example, there might\nbe ways to, essentially, add in a new data set.",
    "start": "1346010",
    "end": "1351071"
  },
  {
    "text": "So let's suppose we've trained\non a bunch of different data sets. We now add in a new data set. We can, perhaps, do some sorts\nof zero shot meta learning,",
    "start": "1351072",
    "end": "1361927"
  },
  {
    "text": "basically, where there's no need\nfor additional gradient steps because we're\nbasically predicting kind of similar to how you might\ndo prompting nowadays in NLP",
    "start": "1361927",
    "end": "1369520"
  },
  {
    "text": "literature. Anyways, yeah, I think we'll\nget into some more details. ",
    "start": "1369520",
    "end": "1377710"
  },
  {
    "text": "Just to chime in on that. I don't think that every\nmeta learning algorithm--",
    "start": "1377710",
    "end": "1383840"
  },
  {
    "text": "I think the one that\nyou're describing right now are optimization based, but\nthey're also black box ones.",
    "start": "1383840",
    "end": "1389529"
  },
  {
    "text": "You don't need to further-- I think the main\ndifference seems to be that there is one task\nversus multiple tasks for meta",
    "start": "1389530",
    "end": "1397270"
  },
  {
    "text": "learning. Yeah, I think so, too. I think that the main-- yeah,\nthe main framing question",
    "start": "1397270",
    "end": "1403575"
  },
  {
    "text": "is whether or not there's\nmultiple data sets. ",
    "start": "1403575",
    "end": "1409559"
  },
  {
    "text": "Cool. OK, awesome. If there's no other\nquestions, I'll dive a bit more into\nthe architecture.",
    "start": "1409560",
    "end": "1416100"
  },
  {
    "text": " Awesome. So there's three key\ncomponents to NPTs.",
    "start": "1416100",
    "end": "1423423"
  },
  {
    "text": "I'm going to first state\nthem at a high level. And then, we'll go through\neach of them in more detail.",
    "start": "1423423",
    "end": "1428590"
  },
  {
    "text": "So first of all, we take\nthe entire data set. All data points as input. So for example, at\ntest time, the model",
    "start": "1428590",
    "end": "1435150"
  },
  {
    "text": "is going to take as input\nboth training and test data. And we approximate this with\nmini batches for large data.",
    "start": "1435150",
    "end": "1443870"
  },
  {
    "text": "We apply self-attention\nbetween data points. So for example, at test\ntime, we model relationships",
    "start": "1443870",
    "end": "1449690"
  },
  {
    "text": "amongst training points,\namongst test points, and between the two sets.",
    "start": "1449690",
    "end": "1455040"
  },
  {
    "text": "And then, finally, we have\nthis masking-based training objective. It's a BERT-like\nstochastic masking.",
    "start": "1455040",
    "end": "1460580"
  },
  {
    "text": "And the key point\nis that we actually use it on both features as\nwell as on training targets.",
    "start": "1460580",
    "end": "1465690"
  },
  {
    "text": "And we'll get into\nwhy that leads to an interesting\npredictive mechanism later.",
    "start": "1465690",
    "end": "1472010"
  },
  {
    "text": "So to start with, this\nidea of data sets as input. There's two things that\ncompose the input to NPT.",
    "start": "1472010",
    "end": "1478930"
  },
  {
    "text": "It's a full data set\nin the form of a matrix X and a masking matrix\nM. And so, Jannik",
    "start": "1478930",
    "end": "1484780"
  },
  {
    "text": "has described this data\nset matrix a little bit. We basically have\ndata points as rows. The columns are attributes.",
    "start": "1484780",
    "end": "1490900"
  },
  {
    "text": "And each attribute shares\nsome kind of semantic meaning among all of its data points.",
    "start": "1490900",
    "end": "1496173"
  },
  {
    "text": "So say, for example, you're\njust doing single target classification or regression. The last column\nwould be the target,",
    "start": "1496173",
    "end": "1502460"
  },
  {
    "text": "and the rest of the matrix\nwould be input features. So for example, the\npixels of an image.",
    "start": "1502460",
    "end": "1508529"
  },
  {
    "text": "We also have a masking matrix. So let's say, we're thinking\nabout mask language modeling.",
    "start": "1508530",
    "end": "1513710"
  },
  {
    "text": "The mask tokens\nwill just tell us where we're going\nto conceal words and where we're going\nto backpropagate a loss.",
    "start": "1513710",
    "end": "1519799"
  },
  {
    "text": "We do a similar type\nof thing here where we use this binary\nmask matrix to specify which entries are masks.",
    "start": "1519800",
    "end": "1526150"
  },
  {
    "text": "And the goal is to\npredict masked values from observed values.",
    "start": "1526150",
    "end": "1532029"
  },
  {
    "text": "I see that there was a\nquestion about handling inputs with different lengths.",
    "start": "1532030",
    "end": "1537940"
  },
  {
    "text": "In the data sets\nwe've considered, we'll get into it in\nthe results section, but it's mostly been sort\nof tabular and image data",
    "start": "1537940",
    "end": "1545380"
  },
  {
    "text": "where the lengths for each of\nthe data points is the same, but it would work\njust like padding. That would be a reasonable\nway to go about that.",
    "start": "1545380",
    "end": "1553052"
  },
  {
    "text": "And there's also kind of\nan interesting-- yeah. Go for it, Jannik. Just to add to that. I'm not sure if length refers\nto columns or to rows, right?",
    "start": "1553052",
    "end": "1562270"
  },
  {
    "text": "Rows we don't care\nabout how many rows. Length, padding or something\nwould be an option.",
    "start": "1562270",
    "end": "1567670"
  },
  {
    "text": "Yeah, my question was\nabout columns exactly. So that makes sense. Thanks.",
    "start": "1567670",
    "end": "1573684"
  },
  {
    "text": "Yeah, I mean, that goes along\nwith the whole meta earning discussion is, I\nthink, if we wanted to adapt two data sets that\nhave a different number of data",
    "start": "1573685",
    "end": "1580559"
  },
  {
    "text": "points per data set, we can\ntake advantage of the fact that self-attention\nis OK with that.",
    "start": "1580560",
    "end": "1585840"
  },
  {
    "text": " Cool. ",
    "start": "1585840",
    "end": "1591419"
  },
  {
    "text": "So continuing on. Left to discuss\nhere is, basically,",
    "start": "1591420",
    "end": "1597390"
  },
  {
    "text": "how we do the embedding. So to put this\nmore explicitly, we have this data matrix that has\nn data points, it's called X.",
    "start": "1597390",
    "end": "1604289"
  },
  {
    "text": "And it also has d attributes. And we have this\nbinary mask matrix M. We're going to stack them.",
    "start": "1604290",
    "end": "1609690"
  },
  {
    "text": "And then, we're going to\ndo a linear embedding. So specifically, we're doing\nthe same linear embedding",
    "start": "1609690",
    "end": "1615840"
  },
  {
    "text": "independently for\neach data point. We're learning a different\nembedding for each attribute. We have a positional encoding\non the index of the attributes",
    "start": "1615840",
    "end": "1623503"
  },
  {
    "text": "because we don't\nreally care about, say, being equivariant\nover the columns. If it's tabular\ndata, you, of course, want to treat all these kind\nof heterogeneous columns",
    "start": "1623503",
    "end": "1630558"
  },
  {
    "text": "differently. And then, finally, we have an\nencoding on the type of column, so whether or not it's\ncontinuous or categorical.",
    "start": "1630558",
    "end": "1637720"
  },
  {
    "text": "And that ends up giving us this\ninput data set representation that is dimensions n by d by e.",
    "start": "1637720",
    "end": "1643780"
  },
  {
    "text": " The second key component\nof NPTs is the attention",
    "start": "1643780",
    "end": "1649480"
  },
  {
    "text": "between data points. So to do that, we first take\nthis representation we have",
    "start": "1649480",
    "end": "1655440"
  },
  {
    "text": "and flatten to an n by d\ntimes e representation. So basically, we're treating\neach of these d times",
    "start": "1655440",
    "end": "1662130"
  },
  {
    "text": "e size rows as if it's\na token representation. We're actually going to just\naccomplish this operation using",
    "start": "1662130",
    "end": "1668700"
  },
  {
    "text": "multi and self-attention. We've reviewed this a lot. But the nice thing is that we\nknow from language modeling,",
    "start": "1668700",
    "end": "1674562"
  },
  {
    "text": "if we stack this\nmultiple times, we can model these higher\norder dependencies. And here, they're\nbetween data points.",
    "start": "1674562",
    "end": "1679870"
  },
  {
    "text": "And that's really the key\ndraw of this architecture. There's been other instances of\npeople using attention verse--",
    "start": "1679870",
    "end": "1685740"
  },
  {
    "text": "for similar sorts of things. So for example, like\nattentive neural processes. A lot of times they've sort\nof use just a single layer",
    "start": "1685740",
    "end": "1693269"
  },
  {
    "text": "as kind of a\nrepresentational lookup. And we believe that\nthis, actually, ends up limiting expressivity.",
    "start": "1693270",
    "end": "1698760"
  },
  {
    "text": "And that by stacking\nthis many times, you can learn more complex\nrelationships between the data points.",
    "start": "1698760",
    "end": "1704382"
  },
  {
    "text": "Neil, you also have\nsome questions.  You can go ahead first.",
    "start": "1704382",
    "end": "1709850"
  },
  {
    "text": "Oh, cool. Thanks. I have a question about how\nyou guys do the embedding. Is it always like-- or do\nwe use convolutional filters",
    "start": "1709850",
    "end": "1717620"
  },
  {
    "text": "or linear layers? What is the type of\nembedding that you guys use? Yeah, so I'm attempting\nto go back to the slide.",
    "start": "1717620",
    "end": "1725480"
  },
  {
    "text": "I think it's not very\nhappy with me right now. But yeah, so for tabular data,\nwe did just linear embeddings,",
    "start": "1725480",
    "end": "1732600"
  },
  {
    "text": "actually. So we could get into, I guess,\ndetails of featurization",
    "start": "1732600",
    "end": "1738168"
  },
  {
    "text": "for categorical and continuous. But it's literally like,\nsay, for categorical, you do a one hot encoding. And then, you learn\nthis embedding that",
    "start": "1738168",
    "end": "1744710"
  },
  {
    "text": "is specific to that attribute. And then, for\nnumerical, I believe, we were just\nstandard normalizing.",
    "start": "1744710",
    "end": "1750800"
  },
  {
    "text": "For the image\ndata, we did end up using a ResNet 18\nencoder for CIFAR-10.",
    "start": "1750800",
    "end": "1757880"
  },
  {
    "text": "However, I think that-- I mean, we'll discuss that\na bit later in results. But that embedding\nis a bit arbitrary.",
    "start": "1757880",
    "end": "1765053"
  },
  {
    "text": "You can sort of do whatever. The key part of the\narchitecture is the attention between data points. In terms of how you actually\nwant to embed each attribute,",
    "start": "1765053",
    "end": "1772270"
  },
  {
    "text": "it's kind of up to you. Thanks. ",
    "start": "1772270",
    "end": "1779350"
  },
  {
    "text": "I think [AUDIO OUT]\nanother question. Same question as Victor's. ",
    "start": "1779350",
    "end": "1786260"
  },
  {
    "text": "Awesome. Cool. So here we have attention\nbetween data points done.",
    "start": "1786260",
    "end": "1793200"
  },
  {
    "text": "So we can also do this\nattention between attributes. So we reshape back to this\nn by d by e representation.",
    "start": "1793200",
    "end": "1801450"
  },
  {
    "text": "And then, we can just apply\nself-attention independently to each row. In other words, to\na single data point.",
    "start": "1801450",
    "end": "1807090"
  },
  {
    "text": "And the intuition for why\nwe would do this nested type idea where we switch between\nattention between data points",
    "start": "1807090",
    "end": "1813030"
  },
  {
    "text": "and attention\nbetween attributes is just we're trying to learn\nbetter per data point representations for the between\ndata point interactions.",
    "start": "1813030",
    "end": "1821380"
  },
  {
    "text": "This is literally just\nnormal self-attention as you'd see in language\nmodeling or image classification.",
    "start": "1821380",
    "end": "1826600"
  },
  {
    "text": "The attributes are\nthe tokens here. And finally, we just\nrinse and repeat.",
    "start": "1826600",
    "end": "1834070"
  },
  {
    "text": "So what are we actually\ngetting out of this? To summarize, we're learning\nhigher order relationships between data points.",
    "start": "1834070",
    "end": "1840480"
  },
  {
    "text": "We're learning transformations\nof individual data points. And then, importantly, NPT is\nequivariant to a permutation",
    "start": "1840480",
    "end": "1847380"
  },
  {
    "text": "of the data points. This basically just\nreflects the intuition that the learned relationships\nbetween the data points",
    "start": "1847380",
    "end": "1853653"
  },
  {
    "text": "should not depend on the\nordering in which you receive them or in which\nyou observe your data set. ",
    "start": "1853653",
    "end": "1861150"
  },
  {
    "text": "The third key component of NPT\nis a masking-based training objective.",
    "start": "1861150",
    "end": "1866340"
  },
  {
    "text": "So recall that what\nwe're trying to do is we're trying to\npredict missing entries from observed entries.",
    "start": "1866340",
    "end": "1872220"
  },
  {
    "text": "And those mass values can\nbe both features or targets. So again, the classic use, say,\nof masked language modeling",
    "start": "1872220",
    "end": "1878850"
  },
  {
    "text": "is to do\nself-supervised learning on a sequence of\ntokens, which you could think of as just having\nfeatures in our setting.",
    "start": "1878850",
    "end": "1885840"
  },
  {
    "text": "Ours is a bit\ndifferent in that we do stochastic feature masking\nto mask feature values",
    "start": "1885840",
    "end": "1890910"
  },
  {
    "text": "with a probability\np sub feature. And then, we also do this\nmasking of training targets",
    "start": "1890910",
    "end": "1896970"
  },
  {
    "text": "with this probability\np sub target. So if we write out the\ntraining objective,",
    "start": "1896970",
    "end": "1902250"
  },
  {
    "text": "we are just taking a weighted\nsum of the negative log likelihood lost from targets\nas well as from features.",
    "start": "1902250",
    "end": "1908429"
  },
  {
    "text": "And of course, at\ntest time, we're only going to mask and compute\na loss over the targets of test",
    "start": "1908430",
    "end": "1913799"
  },
  {
    "text": "points. So to break this down a\nbit further and point out some of the cool\nparts of it here,",
    "start": "1913800",
    "end": "1920237"
  },
  {
    "text": "the thing that's highlighted\nright now on the far right is the term relating\nto the features. It's the feature masking.",
    "start": "1920238",
    "end": "1927240"
  },
  {
    "text": "Basically, we find that this\nhas a nice regularizing effect. More or less, the model\ncan now predict anywhere.",
    "start": "1927240",
    "end": "1933570"
  },
  {
    "text": "It makes the task a bit harder\nand introduces some more supervision. And we found in an ablation for\nthe tabular data sets that it",
    "start": "1933570",
    "end": "1939150"
  },
  {
    "text": "helped for 8 of 10 of those. And then, there's\nthis other term, which is kind of interesting.",
    "start": "1939150",
    "end": "1944460"
  },
  {
    "text": "It's the stochastic\ntarget masking. And the idea is\nthat you're actually",
    "start": "1944460",
    "end": "1949950"
  },
  {
    "text": "going to have some\ntraining targets unmasked to the model at input\nat training time, which",
    "start": "1949950",
    "end": "1957030"
  },
  {
    "text": "means that the NPT\ncan learn to predict the masked targets of\ncertain training data points",
    "start": "1957030",
    "end": "1962460"
  },
  {
    "text": "using the targets of\nother training data points as well as all of the\ntraining features.",
    "start": "1962460",
    "end": "1967500"
  },
  {
    "text": "And so, that means\nyou don't actually need to memorize a mapping\nbetween training inputs and outputs in your parameters.",
    "start": "1967500",
    "end": "1973590"
  },
  {
    "text": "You can, instead, devote the\nrepresentational capacity of the model to learn functions\nthat use other training",
    "start": "1973590",
    "end": "1979230"
  },
  {
    "text": "features and targets as input. So this is kind of getting\ninto the idea of this sort of learn k-NN idea.",
    "start": "1979230",
    "end": "1985980"
  },
  {
    "text": "Obviously, we can learn more\ncomplex relational lookups and those sorts of\nthings from this.",
    "start": "1985980",
    "end": "1991800"
  },
  {
    "text": "But you can imagine\none such case being-- we have a bunch of test\ndata points coming in",
    "start": "1991800",
    "end": "1997770"
  },
  {
    "text": "and we're going to\nlook at their features and use that to assign them\nto clusters of training data",
    "start": "1997770",
    "end": "2002930"
  },
  {
    "text": "points. And then, our prediction\nfor those points is just going to be an\ninterpolation of the training",
    "start": "2002930",
    "end": "2007970"
  },
  {
    "text": "targets in that\nrespective cluster. That's an example of\nsomething that this mechanism lets NPTs learn.",
    "start": "2007970",
    "end": "2013910"
  },
  {
    "text": " All right, so if\nthere's any questions,",
    "start": "2013910",
    "end": "2020095"
  },
  {
    "text": "we can take them now. Otherwise, happy to take them\nin the discussion or something. ",
    "start": "2020095",
    "end": "2028350"
  },
  {
    "text": "All right, so let's discuss-- Yeah, go for it.",
    "start": "2028350",
    "end": "2033790"
  },
  {
    "text": "I'm curious, when you are\nusing the entire data set, is that limited type\nof data sets you",
    "start": "2033790",
    "end": "2040250"
  },
  {
    "text": "can use because of the size? Yeah, so in practice,\nwe do random mini",
    "start": "2040250",
    "end": "2046529"
  },
  {
    "text": "batching as an approximation. So the idea is just-- if you have a reasonably\nlarge mini batch,",
    "start": "2046530",
    "end": "2053460"
  },
  {
    "text": "you're going to benefit\na bit from still having this look up ability. Because in a reasonable\nnumber of classes,",
    "start": "2053460",
    "end": "2059149"
  },
  {
    "text": "probably, you're going\nto be able to learn some interesting mappings\nbased on features and targets",
    "start": "2059150",
    "end": "2064872"
  },
  {
    "text": "amongst those classes. We found, in practice, that-- and we'll get into\nthis a little bit. But we do actually,\nindeed, learn",
    "start": "2064873",
    "end": "2073949"
  },
  {
    "text": "to use relationships between\ndata points on prediction for data sets where we're\ndoing mini batching.",
    "start": "2073949",
    "end": "2080310"
  },
  {
    "text": "And we also didn't,\nnecessarily, find that you need like a ludicrously\nlarge batch size for this to be a thing.",
    "start": "2080310",
    "end": "2086309"
  },
  {
    "text": "But I do think\nit's just this is, in general, an important point. And it's one that points\nus towards looking",
    "start": "2086310",
    "end": "2091770"
  },
  {
    "text": "into, say, sparse\ntransformers literature if we're trying to expand\nto some larger data sets",
    "start": "2091770",
    "end": "2097620"
  },
  {
    "text": "without having the mini\nbatching assumption. Great, thank you.",
    "start": "2097620",
    "end": "2102730"
  },
  {
    "text": "If I can add a number to that. We can, without mini batching,\naccommodate data sets",
    "start": "2102730",
    "end": "2109390"
  },
  {
    "text": "of around 8,000 points or so. So that already accounts for a\nfair proportion, I would say,",
    "start": "2109390",
    "end": "2117250"
  },
  {
    "text": "of the tabular data\nsets out there. But we also do data sets\nwith 11 million points where, obviously, we then\nresort to mini batching.",
    "start": "2117250",
    "end": "2125050"
  },
  {
    "text": "So that's very good to\nhave an idea of the sizes that we're talking about. I'm curious on that.",
    "start": "2125050",
    "end": "2131460"
  },
  {
    "text": "I mean, it is pretty exciting. I feel like you don't normally\nhear about transformers being applied to it as a size 8,000.",
    "start": "2131460",
    "end": "2140870"
  },
  {
    "text": "I'm curious-- we can talk\nabout this sort of later once we've covered the\nnew material-- if you found that sample efficiency\nis one of the key gains here?",
    "start": "2140870",
    "end": "2148790"
  },
  {
    "text": "Or just experience\nworking on small data sets of transformers, generally? And yeah, but I'll happy\nto punt the answer to that",
    "start": "2148790",
    "end": "2154910"
  },
  {
    "text": "until after this part\nof the discussion. Yeah, I think that'd be really\nnice to talk about a bit.",
    "start": "2154910",
    "end": "2161174"
  },
  {
    "text": "And it was something that,\nin general, I guess, I'd say it was surprising to us in\nterms of how robust NPTs were",
    "start": "2161175",
    "end": "2167320"
  },
  {
    "text": "on small data sets and\nhow we surprisingly didn't have to tune a\nterrible number of parameters. But we can get into\ndetails in a bit.",
    "start": "2167320",
    "end": "2174040"
  },
  {
    "text": " Awesome. So to get into the\nexperiments, we",
    "start": "2174040",
    "end": "2182820"
  },
  {
    "text": "focused a lot on tabular\ndata because it's a very general setting.",
    "start": "2182820",
    "end": "2188340"
  },
  {
    "text": "And it's also notoriously\nchallenging for deep learning. So we know tree-based boosting\nmethods, stuff like XGBoost,",
    "start": "2188340",
    "end": "2194790"
  },
  {
    "text": "is very dominant. And this is also a very\nrelevant domain to, I think,",
    "start": "2194790",
    "end": "2199860"
  },
  {
    "text": "people in industry and\nthat sort of thing. So we were excited\nabout the idea of trying to do better on this. So we chose a broad\nselection of data",
    "start": "2199860",
    "end": "2206880"
  },
  {
    "text": "sets varying across a\nfew different dimensions. As we mentioned, on the order\nof hundreds to tens of millions",
    "start": "2206880",
    "end": "2214590"
  },
  {
    "text": "of instances. Broad range of the number of\nfeatures and the composition of features in terms of being\ncategorical or continuous.",
    "start": "2214590",
    "end": "2222450"
  },
  {
    "text": "Various types of tasks,\nbinary or multi-class classification as\nwell as regression. And like I said,\nthe baselines were",
    "start": "2222450",
    "end": "2227760"
  },
  {
    "text": "kind of the usual\nsuspects for tabular data. XGBoost, CatBoost, LightGMB,\ntune MLPs, and TabNet,",
    "start": "2227760",
    "end": "2234570"
  },
  {
    "text": "which is a transformer\narchitecture for tabular data. So to get into the\nresults, here, I'm",
    "start": "2234570",
    "end": "2241910"
  },
  {
    "text": "showing the average rank\nfor the various subtasks. We did well in terms of\nrank-wise performance",
    "start": "2241910",
    "end": "2248570"
  },
  {
    "text": "against methods like\nCatBoost and XGBoost, which are designed\nspecifically for tabular data. And in fact, we find that\nNPT is the top performer on 4",
    "start": "2248570",
    "end": "2256490"
  },
  {
    "text": "of the 10 of these data sets. On image data, I mentioned\nthat we used a CNN encoder.",
    "start": "2256490",
    "end": "2261950"
  },
  {
    "text": "And with that, we were\nperforming well on CIFAR-10. And we also think that,\nin general, like with,",
    "start": "2261950",
    "end": "2268400"
  },
  {
    "text": "let's say new work on image\ntransformers and small data, this can probably just be\ndone with linear patching.",
    "start": "2268400",
    "end": "2273633"
  },
  {
    "text": "And so, this-- the manner in\nwhich you're embedding things is probably not the key.",
    "start": "2273633",
    "end": "2280590"
  },
  {
    "text": "Neil, if I can jump\nin with two questions. Can you go back\ntwo slides first?",
    "start": "2280590",
    "end": "2287670"
  },
  {
    "text": "One is just a small minor point. Back one more, please. Thank you. Here for the features, 50 plus.",
    "start": "2287670",
    "end": "2294030"
  },
  {
    "text": "What does plus mean here?  I'll have to double check\nwhat the exact number is.",
    "start": "2294030",
    "end": "2300090"
  },
  {
    "text": "I'm pretty sure it's\nprobably around 50. I would guess like-- So the 50 is really an order of.",
    "start": "2300090",
    "end": "2306210"
  },
  {
    "text": "It's not like 150 or 5,000. Yeah, I mean, I'll double\ncheck for you or you",
    "start": "2306210",
    "end": "2313650"
  },
  {
    "text": "can check with the\nmetadata statistics at the end of the paper. But no, it wasn't\narbitrarily large.",
    "start": "2313650",
    "end": "2320680"
  },
  {
    "text": "I would say, though, we\ndid these ablations on whether or not we actually need\nattention between attributes.",
    "start": "2320680",
    "end": "2328710"
  },
  {
    "text": "We did find that this\nended up benefiting us. But you could, perhaps, do\nsay just an MLP embedding",
    "start": "2328710",
    "end": "2337950"
  },
  {
    "text": "in that dimension and go to\nlike a relatively small number of hidden dimensions and\nfit kind of an arbitrary",
    "start": "2337950",
    "end": "2343380"
  },
  {
    "text": "number of features. So I think that-- yeah, if you kind of relax\nthe necessity of attention",
    "start": "2343380",
    "end": "2351029"
  },
  {
    "text": "between attributes,\nyou can probably scale out, at least at that\ndimension, quite a lot. OK, and then, my\nsecond question, if you",
    "start": "2351030",
    "end": "2357300"
  },
  {
    "text": "could go forward one slide.  Oh, thank you. Here, I'm not sure\nI quite caught.",
    "start": "2357300",
    "end": "2363790"
  },
  {
    "text": "What is 4 of 10 data sets, 2 of\n10 data sets, and 4 of 10 mean?",
    "start": "2363790",
    "end": "2369080"
  },
  {
    "text": "This is of all the tabular\ndata sets that we had. Oh, I see, so you're saying\nbinary classification is 4.",
    "start": "2369080",
    "end": "2374750"
  },
  {
    "text": "I see. OK. Yeah, exactly.  Awesome.",
    "start": "2374750",
    "end": "2381230"
  },
  {
    "text": "Any other questions? The standard errors here. Because I mean, there's\njust 10 data sets, right?",
    "start": "2381230",
    "end": "2387310"
  },
  {
    "text": "Yeah, correct. 10 total tabular data sets. Yeah. But these are-- [INTERPOSING VOICES]",
    "start": "2387310",
    "end": "2392650"
  },
  {
    "text": "Yeah, these are\nrank-wise performance. Correct. OK, I'm just missing how the--\nwhere the uncertainty comes",
    "start": "2392650",
    "end": "2402180"
  },
  {
    "text": "from in this case. Yeah, averaged over 4 of\n10 data sets, the rank.",
    "start": "2402180",
    "end": "2408240"
  },
  {
    "text": "So for each particular\ndata set, we have a rank of all\nthe different methods. Then, we take the average\nand the variant answer",
    "start": "2408240",
    "end": "2415079"
  },
  {
    "text": "of the rankings within\neach of the types of task--",
    "start": "2415080",
    "end": "2420330"
  },
  {
    "text": "within binary classification,\nwithin multiclass, et cetera. Got it. We also, if you're curious, have\nthe full results in the paper.",
    "start": "2420330",
    "end": "2430010"
  },
  {
    "text": "Yeah, thank you. We also have a couple\nof questions more. And some more\nquestions [INAUDIBLE]..",
    "start": "2430010",
    "end": "2435474"
  },
  {
    "text": " Hey, yeah, thanks. I guess, I just found\nit a little surprising",
    "start": "2435474",
    "end": "2442020"
  },
  {
    "text": "that the worst\nperformer was k-NN given that it's also non-parametric.",
    "start": "2442020",
    "end": "2448500"
  },
  {
    "text": "I guess, could you\ncomment on that? Is it that there's something\nintrinsic to the NPT that",
    "start": "2448500",
    "end": "2455519"
  },
  {
    "text": "makes it just exceptional far\nbeyond other non-parametric methods?",
    "start": "2455520",
    "end": "2461250"
  },
  {
    "text": "Yeah, why is it that k-NN\nperforms the worse here?",
    "start": "2461250",
    "end": "2466770"
  },
  {
    "text": "Well, I suppose,\nultimately, k-NN is still a relatively\nnaive predictive method",
    "start": "2466770",
    "end": "2471780"
  },
  {
    "text": "in that it might\njust be predicting based on cluster means. So for example, I think this\nis probably universally true",
    "start": "2471780",
    "end": "2480008"
  },
  {
    "text": "for all the data sets,\nbut there's probably some amount of kind of\nadditional reasoning that needs to occur over\nthe features, at least,",
    "start": "2480008",
    "end": "2486130"
  },
  {
    "text": "at a basic level. So for example, one\nof the data sets is this poker hand\ndata set where it's like a mapping between\nall of the different hands",
    "start": "2486130",
    "end": "2492819"
  },
  {
    "text": "you have in poker and\nwhat like they're commonly known to people, like\nfull houses or whatever. So this requires some amount\nof reasoning over the features",
    "start": "2492820",
    "end": "2500070"
  },
  {
    "text": "to be able to group\nthings together. So just taking the cluster\nmeans of the featurization",
    "start": "2500070",
    "end": "2505530"
  },
  {
    "text": "of those different\nhands is likely not going to give you a great\npredictive function.",
    "start": "2505530",
    "end": "2511500"
  },
  {
    "text": "Whereas, NPTs can kind of do\nthe classic thing where, say, you have an MLP type of\nthing over the features",
    "start": "2511500",
    "end": "2517740"
  },
  {
    "text": "or a tree type of thing\nover the features. You can learn some sort\nof complex embedding. But then, you also can do\nsome non-parametric sort",
    "start": "2517740",
    "end": "2525600"
  },
  {
    "text": "of prediction based on, say,\nlike clusters of embeddings. Yeah, that makes sense.",
    "start": "2525600",
    "end": "2530880"
  },
  {
    "text": "I guess, what if you used\npre-trained embeddings",
    "start": "2530880",
    "end": "2536339"
  },
  {
    "text": "from a stack of encoders as\nyour vector representation for the k-NN?",
    "start": "2536340",
    "end": "2542277"
  },
  {
    "text": "How do you think that\nwould perform compared to the rest of the crowd? Yeah, yeah, so this is like--",
    "start": "2542277",
    "end": "2548280"
  },
  {
    "text": "I mean, this idea is\nkind of like deep kernel learning where like-- Yeah, I believe it is\ndeep kernel learning.",
    "start": "2548280",
    "end": "2553950"
  },
  {
    "text": "It is, basically, you\nuse an MLP independently. So you learn an MLP on\neach input data point.",
    "start": "2553950",
    "end": "2561810"
  },
  {
    "text": "And then, you apply a GP\nover all the representations of those. So you get this complex\nembedding and then the lookups.",
    "start": "2561810",
    "end": "2568380"
  },
  {
    "text": "The key difference between\nthat type of idea and NPTs is that we also learn the\nrelationships between the data",
    "start": "2568380",
    "end": "2574380"
  },
  {
    "text": "points themselves because we\nuse this parametric attention mechanism to learn\nthe relationship.",
    "start": "2574380",
    "end": "2579390"
  },
  {
    "text": "So we're not just learning like\nan embedding independently, we're, basically,\nbackpropagating",
    "start": "2579390",
    "end": "2584790"
  },
  {
    "text": "through the entire process\nlearning the ways in which we would try to embed this. But also, the ways that\nsay the lookup would occur",
    "start": "2584790",
    "end": "2591869"
  },
  {
    "text": "and, essentially, the\nrelationships that can potentially be kind\nof higher order as well.",
    "start": "2591870",
    "end": "2597990"
  },
  {
    "text": "OK, cool. Wait. Might I ask one\nmore follow-up or--",
    "start": "2597990",
    "end": "2604660"
  },
  {
    "text": "Yeah, go for it. Cool. Yeah, thanks. So I guess, then, if\nthe advantage of NPT",
    "start": "2604660",
    "end": "2610420"
  },
  {
    "text": "has to do with sort of the\nrelationships between data points, then what if you\ntook the-- let's say encoder",
    "start": "2610420",
    "end": "2622390"
  },
  {
    "text": "representations and then\nyou pass that as input, say, for the ten nearest neighbors\nalong with some other input",
    "start": "2622390",
    "end": "2635230"
  },
  {
    "text": "representation and sort of\nhad this weighted average like attention style\nwhere you weighted",
    "start": "2635230",
    "end": "2642280"
  },
  {
    "text": "the vectors of the\nnearest neighbors based",
    "start": "2642280",
    "end": "2648460"
  },
  {
    "text": "on the attention weights\nbetween those input data points and then the supplied\ninput data point.",
    "start": "2648460",
    "end": "2656109"
  },
  {
    "text": "And then, like pass that as the\nvector to the final prediction",
    "start": "2656110",
    "end": "2662590"
  },
  {
    "text": "later. Do you think that captures some\namount of the relationship? Or is that off base?",
    "start": "2662590",
    "end": "2670810"
  },
  {
    "text": "So I think the nice part-- and really, what\nour idea is behind this whole thing is just\nthese sorts of instances",
    "start": "2670810",
    "end": "2677859"
  },
  {
    "text": "where certain\nfixed kernels would perform particularly well on\ntasks is kind of an annoyance.",
    "start": "2677860",
    "end": "2683380"
  },
  {
    "text": "And ultimately, tuning a\nlot of these types of things or trying to derive the\npredictive methods that might make a lot of sense for a\ngiven situation kind of stinks.",
    "start": "2683380",
    "end": "2691090"
  },
  {
    "text": "And ideally, you'd want to just\nback propagate on a data set and kind of learn these\nrelationships yourself.",
    "start": "2691090",
    "end": "2696310"
  },
  {
    "text": "So I, actually, would be\nreally interested to see if we can come up with some\nsynthetic experiments that",
    "start": "2696310",
    "end": "2701470"
  },
  {
    "text": "have these very particular\nk-NN like predictive mechanisms and just see if we can\nlearn precisely those",
    "start": "2701470",
    "end": "2707350"
  },
  {
    "text": "and get 0 error with NPTs. And in fact, we'll get\ninto this a little bit",
    "start": "2707350",
    "end": "2712600"
  },
  {
    "text": "with some of the interventional\nexperiments we do. We have precise lookup\nfunctions that NPTs end up",
    "start": "2712600",
    "end": "2718720"
  },
  {
    "text": "being able to learn. So we can learn interesting\nrelational functions. Cool. Yeah.",
    "start": "2718720",
    "end": "2724050"
  },
  {
    "text": "Thanks a lot. Appreciate it. ",
    "start": "2724050",
    "end": "2729120"
  },
  {
    "text": "All right. One more question from-- Yeah, sure.",
    "start": "2729120",
    "end": "2734310"
  },
  {
    "text": "I just want to clarify\nsomething about-- basically, so at\ntest time, you just",
    "start": "2734310",
    "end": "2740130"
  },
  {
    "text": "take the exact same\ndata set and you just add your test examples, right?",
    "start": "2740130",
    "end": "2745650"
  },
  {
    "text": "And then, you do the\nsame type of masking. Is that how it works? Yeah, correct.",
    "start": "2745650",
    "end": "2751830"
  },
  {
    "text": "OK, got it. And I do have one more question. That is just because I\nthink I misunderstood how",
    "start": "2751830",
    "end": "2759839"
  },
  {
    "text": "the effects of\nyour NPT objective. Do you mind going\nback to that slide?",
    "start": "2759840",
    "end": "2764940"
  },
  {
    "text": "Sure.  Yeah, can you\nrepeat one more time",
    "start": "2764940",
    "end": "2773010"
  },
  {
    "text": "what makes this so special? Yeah, so the regularizer on\nthe right over the features",
    "start": "2773010",
    "end": "2779990"
  },
  {
    "text": "I would think of very similarly\nto self-supervised learning with just a standard\ntransformer.",
    "start": "2779990",
    "end": "2785540"
  },
  {
    "text": "You're basically\njust introducing a lot more supervision. Even if, say, you're just\ndoing a supervised objective,",
    "start": "2785540",
    "end": "2792079"
  },
  {
    "text": "this is kind of some amount of\nreconstruction over the feature as you learn a more\ninteresting representation and what a regularizing effect,\nwhich we think is interesting.",
    "start": "2792080",
    "end": "2800630"
  },
  {
    "text": "But perhaps, not as interesting\nas this stochastic target masking. This one is unique\nbecause in kind",
    "start": "2800630",
    "end": "2806540"
  },
  {
    "text": "of standard parametric\ndeep learning, you're not going to have an\ninstance in your training",
    "start": "2806540",
    "end": "2812210"
  },
  {
    "text": "process where you're\ntaking targets as input. And so, basically,\nwhat happens is",
    "start": "2812210",
    "end": "2819980"
  },
  {
    "text": "if you have your training data\nset as input, whatever, you're going to have some stochastic\nfeature masking stuff happening",
    "start": "2819980",
    "end": "2825312"
  },
  {
    "text": "on the features. Amongst the training\ntargets, you're randomly going to have some of those\nunmasked and some of them",
    "start": "2825312",
    "end": "2831740"
  },
  {
    "text": "will indeed be masks. You're going to be\nback propagating a loss on the ones that are\nmasked, of course, because you don't want your model to\nhave those available at input",
    "start": "2831740",
    "end": "2839120"
  },
  {
    "text": "if you're going to actually try\nto back propagate a loss on it. But you can use the\nother ones as input.",
    "start": "2839120",
    "end": "2844190"
  },
  {
    "text": "And that means you\ncan learn these kind of like interpolative functions. So that was this\nwhole idea of being",
    "start": "2844190",
    "end": "2849230"
  },
  {
    "text": "able to kind of learn k-NN. But doesn't that allow\nthe model to cheat again?",
    "start": "2849230",
    "end": "2857450"
  },
  {
    "text": "Yeah, so this is an\ninteresting point. And actually, subtle. So I think it's really\nworthwhile to bring up.",
    "start": "2857450",
    "end": "2864620"
  },
  {
    "text": "So first of all, we never\nactually back propagate a loss on something that was visible\nto the model at input.",
    "start": "2864620",
    "end": "2872480"
  },
  {
    "text": "And so, if, for example, the\nmodel did actually end up, basically, overfitting\non training labels,",
    "start": "2872480",
    "end": "2877910"
  },
  {
    "text": "we would not observe\nthe model's ability to generalize to test data. We don't observe this.",
    "start": "2877910",
    "end": "2883460"
  },
  {
    "text": "So obviously, it\nseems like this kind of blocking of back\npropagation on labels",
    "start": "2883460",
    "end": "2889040"
  },
  {
    "text": "that are visible at input\nto the NPT is helping. It could also be possible\nthat in BERT style",
    "start": "2889040",
    "end": "2896630"
  },
  {
    "text": "stochastic masking,\nyou also randomly will flip some labels to be\nin a different category.",
    "start": "2896630",
    "end": "2903260"
  },
  {
    "text": "So this is kind of\njust like a random fine print that was introduced\nin the BERT masking text.",
    "start": "2903260",
    "end": "2908510"
  },
  {
    "text": "We also do that. So it's possible that\nsomehow contributes to that. But it's probably pretty\nlikely to just be the fact",
    "start": "2908510",
    "end": "2915873"
  },
  {
    "text": "that we're not back propagating\na loss on something that's visible. Great. Thanks.",
    "start": "2915873",
    "end": "2920920"
  },
  {
    "text": "Makes sense.  I have two more questions\nif I can jump in.",
    "start": "2920920",
    "end": "2927950"
  },
  {
    "text": "Yeah, sure. Sorry, can we go to the\nmetrics, the performance, the results slide.",
    "start": "2927950",
    "end": "2933500"
  },
  {
    "text": "Sure. I feel like I missed\nsomething else. I'm sorry about this. So looking on the binary\nclassification, AUROC.",
    "start": "2933500",
    "end": "2940270"
  },
  {
    "text": " Can you clarify what\nthese numbers mean? Are they the AUROC?",
    "start": "2940270",
    "end": "2948330"
  },
  {
    "text": "So this is the-- so on each of the data sets--",
    "start": "2948330",
    "end": "2953520"
  },
  {
    "text": "so say, for a particular\nbinary classification data set, we're going to get a\nranking of the methods.",
    "start": "2953520",
    "end": "2960060"
  },
  {
    "text": "[INTERPOSING VOICES] Repeat this. Yeah. Go for it. So these numbers here are\nthe relative ranking across,",
    "start": "2960060",
    "end": "2966840"
  },
  {
    "text": "in this particular case,\nthe four data sets. Correct. Yeah. Oh, I see. So these values\nare not the AUROCs",
    "start": "2966840",
    "end": "2974250"
  },
  {
    "text": "on average across the data sets. No. Yeah they're not-- [INTERPOSING VOICES]",
    "start": "2974250",
    "end": "2980760"
  },
  {
    "text": "Averaging AUROC\nmight make sense, but averaging things\nlike accuracy and RMSE seems like a bad idea, right?",
    "start": "2980760",
    "end": "2986820"
  },
  {
    "text": "Because you might\nhave some data sets where everything\nis high accuracy or where RMSE needs something\ndrastically different.",
    "start": "2986820",
    "end": "2993790"
  },
  {
    "text": "I see. So these numbers here only\ntell us the relative ranking between the different methods,\nnot how well they actually",
    "start": "2993790",
    "end": "2999030"
  },
  {
    "text": "perform. I mean, it's also probably\nhow they perform relative to one another, but not how\nwill they perform [INAUDIBLE]..",
    "start": "2999030",
    "end": "3004778"
  },
  {
    "text": "I see. That's all in the appendix. We have that information. I see. OK, I was sitting here confused\ngoing like, why is AUROC?",
    "start": "3004778",
    "end": "3011890"
  },
  {
    "text": "Why is the best\none the smallest? And accuracy? What is an accuracy of 2.5? Anyways. OK, that makes much more sense.",
    "start": "3011890",
    "end": "3017570"
  },
  {
    "text": "Thank you both.  Awesome.",
    "start": "3017570",
    "end": "3023220"
  },
  {
    "text": "Great. So I'll try to\nspeed through this just in the interest of time.",
    "start": "3023220",
    "end": "3028530"
  },
  {
    "text": "But basically,\nthe thing that you might be thinking after\nall of these results is, are we even learning\nany data point interactions",
    "start": "3028530",
    "end": "3036119"
  },
  {
    "text": "on these real data sets? And so, basically, we\ndesigned an experiment to figure this out. And the idea is that we're\ngoing to disallow NPT",
    "start": "3036120",
    "end": "3044099"
  },
  {
    "text": "from using other data points\nwhen predicting on one of them. If we do that and we\nobserve that NPT actually",
    "start": "3044100",
    "end": "3050670"
  },
  {
    "text": "predicts or performed\nsignificantly worse, it is, indeed, using\nthese interactions between data points.",
    "start": "3050670",
    "end": "3057840"
  },
  {
    "text": "A subtle challenge or\njust an added bonus we can get from this is that,\nideally, we wouldn't actually",
    "start": "3057840",
    "end": "3063660"
  },
  {
    "text": "break batch statistics. So let's say the mean of\neach particular attribute, if we can find a way to\ndo this experiment such",
    "start": "3063660",
    "end": "3070890"
  },
  {
    "text": "that we don't\nbreak these things, we can rule out the possibility\nthat we learn something that's a bit similar to batch norm.",
    "start": "3070890",
    "end": "3078190"
  },
  {
    "text": "And so, the way that we\ndo this is we basically look at the predictions\nfor each one of the data points in sequence.",
    "start": "3078190",
    "end": "3083740"
  },
  {
    "text": "So let's say, in\nthis case, we're looking at the\nprediction of the model for this particular green row.",
    "start": "3083740",
    "end": "3088776"
  },
  {
    "text": "And it's going to be predicting\nin this last column that has this question\nmark, which is masked. What we're going to\ndo is we're going",
    "start": "3088777",
    "end": "3094318"
  },
  {
    "text": "to permute each\nof the attributes independently amongst\nall other data points except for that one.",
    "start": "3094318",
    "end": "3099370"
  },
  {
    "text": "So the information for\nthat row, if it was just predicting a classic parametric\ndeep model, is still intact.",
    "start": "3099370",
    "end": "3105730"
  },
  {
    "text": "But the information from all\nof the other rows is gone. So this is why we call this sort\nof the corruption experiment.",
    "start": "3105730",
    "end": "3112330"
  },
  {
    "text": "And so, we find,\nin general, when we perform this experiment,\nperformance kind of falls off a cliff for the vast\nmajority of these methods.",
    "start": "3112330",
    "end": "3119280"
  },
  {
    "text": "And I'll note that\nthe performances between the methods on a lot\nof these were fairly close.",
    "start": "3119280",
    "end": "3125320"
  },
  {
    "text": "And so, this is, actually,\nindeed, pretty significant. So for example, on\nprotein, we went from being the top performer\namongst all the methods",
    "start": "3125320",
    "end": "3131460"
  },
  {
    "text": "to the worst performer. Worse than even like k-NN\nor something like that. I'll also note that there's kind\nof this interesting behavior",
    "start": "3131460",
    "end": "3139259"
  },
  {
    "text": "where on these data sets like\nForest and Kick and Breast Cancer. We actually observed\nthat there is, basically,",
    "start": "3139260",
    "end": "3144927"
  },
  {
    "text": "no drop in performance. And we, basically, see this as\nkind of an interesting feature and not necessarily a\nbug of the model, which",
    "start": "3144927",
    "end": "3151380"
  },
  {
    "text": "is that if we're backpropagating\non a given data set, the model can sort of just\nfind that it's actually not",
    "start": "3151380",
    "end": "3158070"
  },
  {
    "text": "that worthwhile to\nattempt to predict using some kind of relational\npredictive mechanism amongst data points.",
    "start": "3158070",
    "end": "3163890"
  },
  {
    "text": "And can instead just learn\nto predict parametrically. And, basically,\nignore other data points when it's predicting\non any given one of them.",
    "start": "3163890",
    "end": "3171549"
  },
  {
    "text": "And so, this probably leads\nto some interesting ideas where perhaps you could\ndo, like, post doc pruning",
    "start": "3171550",
    "end": "3176580"
  },
  {
    "text": "or something like that. Taking away the attention\nbetween data points and doing fine\ntuning, let's say.",
    "start": "3176580",
    "end": "3181829"
  },
  {
    "text": " All right, so now I'll\nhand over to Jannik",
    "start": "3181830",
    "end": "3186900"
  },
  {
    "text": "talk a bit about learning some\ninteresting relationships. Yeah, will you, though?",
    "start": "3186900",
    "end": "3194000"
  },
  {
    "text": "I see that we're at the\nend of what the time is. But I know there's a buffer\nplanned in or something.",
    "start": "3194000",
    "end": "3200780"
  },
  {
    "text": "Can you-- I can go\nthrough this experiment. We can have a bit of discussion. What do you guys prefer?",
    "start": "3200780",
    "end": "3207950"
  },
  {
    "text": "Yeah, I think,\nnormally, what we do is we would sort of stop\nthe recording at this point",
    "start": "3207950",
    "end": "3213530"
  },
  {
    "text": "and have an off the\nrecord discussion. And I guess the question\nto ask is, does anyone have",
    "start": "3213530",
    "end": "3220340"
  },
  {
    "text": "any questions at this point? ",
    "start": "3220340",
    "end": "3227599"
  },
  {
    "text": "But I think we've basically\nbeen watching questions as they come. So I personally feel fine\njust considering the sorts",
    "start": "3227600",
    "end": "3236195"
  },
  {
    "text": "of questions throughout. Yeah, I guess that\nsounds good, Jannik.",
    "start": "3236195",
    "end": "3242180"
  },
  {
    "text": "You can go forward\nwith it as planned. And yeah. Later we can see\nabout the time thing.",
    "start": "3242180",
    "end": "3250920"
  },
  {
    "text": "I think this will only be like\nanother four or five minutes. Yeah, that's for sure. Then go for it.",
    "start": "3250920",
    "end": "3256125"
  },
  {
    "text": "Yeah, for sure. All right, so Neil has\nnow told us how well",
    "start": "3256125",
    "end": "3263520"
  },
  {
    "text": "NPTs perform in real data. And that they do make use of\ninformation from other samples to the input.",
    "start": "3263520",
    "end": "3269467"
  },
  {
    "text": "But we're now going\nto take this a bit further and come up with some\ntoy experiments that test a bit",
    "start": "3269467",
    "end": "3274980"
  },
  {
    "text": "the extent to which NPTs can\nlearn to look up information from other rows. The extent to which\nthey can learn",
    "start": "3274980",
    "end": "3281160"
  },
  {
    "text": "this non-parametric\nprediction mechanism. And so, specifically,\nwhat we'll do is we'll create the following\nsemi-synthetic data set.",
    "start": "3281160",
    "end": "3289440"
  },
  {
    "text": "So I want you to focus on A now. So we take one of\nthe tabular data sets that we've used previously,\nspecifically, the protein data",
    "start": "3289440",
    "end": "3297122"
  },
  {
    "text": "set, but it doesn't\nreally matter. What matters is that it's\na regression data set. And so, now, what we do is we--",
    "start": "3297123",
    "end": "3303720"
  },
  {
    "text": "the top half here is\nthe original data set. But the bottom half is a\ncopy of the original data",
    "start": "3303720",
    "end": "3309900"
  },
  {
    "text": "set where we have unveiled\nthe true target value.",
    "start": "3309900",
    "end": "3314960"
  },
  {
    "text": "And so, now, NPTs\ncould learn to use attention between data\npoints to achieve arbitrarily good performance.",
    "start": "3314960",
    "end": "3320790"
  },
  {
    "text": "They could learn to\nlook up the target values in these\nmatching duplicate rows. And then, paste them back into\nthat masked out target value.",
    "start": "3320790",
    "end": "3330905"
  },
  {
    "text": "Then, at test\ntime, of course, we put in a novel test data input\nwhere this mechanism is also",
    "start": "3330905",
    "end": "3337200"
  },
  {
    "text": "possible just to make\nsure that it hasn't learned to memorize anything. But it has, actually, learned\nthis correct relational",
    "start": "3337200",
    "end": "3343740"
  },
  {
    "text": "mechanism. And so, what we see\nis that, indeed, NPTs do successfully learn\nto perform this lookup.",
    "start": "3343740",
    "end": "3350099"
  },
  {
    "text": "So what I'm visualizing\nhere is attention maps. And they very clearly\nshow that, let's say,",
    "start": "3350100",
    "end": "3355170"
  },
  {
    "text": "when predicting for\nthis green row here, this first green row,\nwhat NPTs look at is exactly only that\nother green row here.",
    "start": "3355170",
    "end": "3363550"
  },
  {
    "text": "And so, this is really nice. We can further look at the\nkind of the Pearson correlation",
    "start": "3363550",
    "end": "3372090"
  },
  {
    "text": "between what NPTs should\npredict and what they actually do predict. And so, this is 99.9%.",
    "start": "3372090",
    "end": "3377890"
  },
  {
    "text": "This is much better\nthan anything you could achieve with\nparametric prediction. And so, it seems that\nNPTs here can actually",
    "start": "3377890",
    "end": "3384030"
  },
  {
    "text": "discover this mechanism. And discover, here, I feel\nlike is the right word. Because NPTs could\nhave, as we've seen,",
    "start": "3384030",
    "end": "3391710"
  },
  {
    "text": "just also continue to predict\nin parametric fashion, right, from each\nrow independently.",
    "start": "3391710",
    "end": "3398130"
  },
  {
    "text": "This is really kind\nof showing to us that there is this bias\nin the model to learn",
    "start": "3398130",
    "end": "3403559"
  },
  {
    "text": "to predict from other rows. And of course, that is also\nvery attractive in this setting because it allows you to\nachieve arbitrary low loss",
    "start": "3403560",
    "end": "3411390"
  },
  {
    "text": "in this setting, or as low\nas you can optimize for it. And so, we kind of\ntake that to mean",
    "start": "3411390",
    "end": "3418810"
  },
  {
    "text": "that our gradient-based\ndiscovery non-parametric philosophy\nseems to make some sense.",
    "start": "3418810",
    "end": "3425630"
  },
  {
    "text": "And so, we can take this a bit\nfurther by performing somewhat of an interventional\nexperiment that investigates",
    "start": "3425630",
    "end": "3431950"
  },
  {
    "text": "the extent to which NPTs\nhave actually learned a robust causal mechanism\nthat's underlying",
    "start": "3431950",
    "end": "3438760"
  },
  {
    "text": "this semi-synthetic data set. And so, just appending this\nextra column of test data.",
    "start": "3438760",
    "end": "3449670"
  },
  {
    "text": "That's already kind of cool. But I think we can\ntake it a bit further and actually study if this\ngeneralizes beyond the data",
    "start": "3449670",
    "end": "3456790"
  },
  {
    "text": "that we see in the\ntraining set or beyond data coming from this\nspecific distribution. And so, what we now\ndo is we intervene",
    "start": "3456790",
    "end": "3462940"
  },
  {
    "text": "on individual duplicate\ndata points at test time by varying their target value.",
    "start": "3462940",
    "end": "3469520"
  },
  {
    "text": "So now, we only care about the\nprediction in a specific row. We do this across all rows. But at each time, we just\ncare about a single row.",
    "start": "3469520",
    "end": "3476859"
  },
  {
    "text": "What we do is we change\nthe target value here. What we're hoping to see is\nthat the NPT just adjusts",
    "start": "3476860",
    "end": "3483230"
  },
  {
    "text": "the prediction as well, right? There's a very simple\nintervention experiment for us to test if NPTs have actually\nlearned this mechanism.",
    "start": "3483230",
    "end": "3489849"
  },
  {
    "text": "And to some extent, it\nalso tests robustness. Because now, we're\nassociating target values",
    "start": "3489850",
    "end": "3495070"
  },
  {
    "text": "with features that are not part\nof the training distribution here. And so, what we see is that as\nwe adjust these values here,",
    "start": "3495070",
    "end": "3505299"
  },
  {
    "text": "this is the duplicate value. And then, we here see\nthat the target value-- as we adjust them, we can\nsee that the correlation",
    "start": "3505300",
    "end": "3512215"
  },
  {
    "text": "stays really, really good. It's not quite 99.9%,\nlike on average. We're now at 99.6%.",
    "start": "3512215",
    "end": "3518500"
  },
  {
    "text": "But it's still very, very good. And at this point, you might\nbe slightly annoyed with me",
    "start": "3518500",
    "end": "3524920"
  },
  {
    "text": "because standard\nnon-parametric models can also solve this task, right? But this is a task that I could\nsolve by nearest neighbors.",
    "start": "3524920",
    "end": "3532330"
  },
  {
    "text": "Sure, maybe, I would have to\nchange the input format a bit because this is kind of\nlike in a batch setting",
    "start": "3532330",
    "end": "3537550"
  },
  {
    "text": "and I can just use masks. But most generally,\nnearest neighbor can also-- it also looks up different input\npoints based on their features.",
    "start": "3537550",
    "end": "3547410"
  },
  {
    "text": "Nearest neighbor doesn't\nlearn to do this. I still think it's cool\nthat we need to learn this because it does\nrequire a decent amount",
    "start": "3547410",
    "end": "3554650"
  },
  {
    "text": "of computational sequences\nthat we have to learn, like match all the features,\nlook up target value, copy it back, and so on.",
    "start": "3554650",
    "end": "3561140"
  },
  {
    "text": "But it is, in fact, very easy\nfor us to complicate this task to a degree such\nthat, essentially,",
    "start": "3561140",
    "end": "3568390"
  },
  {
    "text": "no other model that we know\nof can solve this very easily. And so, a really\nsimple thing to do",
    "start": "3568390",
    "end": "3574990"
  },
  {
    "text": "is just to add a plus 1 to\nall of the duplicate values.",
    "start": "3574990",
    "end": "3581030"
  },
  {
    "text": "So now, our nearest\nneighbor would look up the right row, of course,\nbut it would always",
    "start": "3581030",
    "end": "3586809"
  },
  {
    "text": "predict the wrong target\nwith a plus 1 on it. And in fact, many of the\nmodels that we're aware of,",
    "start": "3586810",
    "end": "3592480"
  },
  {
    "text": "they're not modeling\nthe joint distribution over features and targets. What they're modeling is\nthe conditional distribution",
    "start": "3592480",
    "end": "3599710"
  },
  {
    "text": "of the targets given\nthe input features. So they also cannot do this.",
    "start": "3599710",
    "end": "3605089"
  },
  {
    "text": "And so, for us, it's\nreally no problem at all. NPTs will just learn to subtract\nanother one and no problems.",
    "start": "3605090",
    "end": "3611860"
  },
  {
    "text": "And sure, this is also still\na very synthetic setting. But I do think--",
    "start": "3611860",
    "end": "3618070"
  },
  {
    "text": "I mean, I challenge you\nto come up with something that NPTs can't solve but\nthe other models can solve.",
    "start": "3618070",
    "end": "3623890"
  },
  {
    "text": "I think this-- in general,\nthis masking mechanism and the non-parametricity\nof the approach",
    "start": "3623890",
    "end": "3629820"
  },
  {
    "text": "is really nice in general and\nleads to lots of nice behavior in a variety of settings.",
    "start": "3629820",
    "end": "3635060"
  },
  {
    "text": "And so, with that,\nI think we can go to the conclusions, which\nNeil is going to give you.",
    "start": "3635060",
    "end": "3642349"
  },
  {
    "text": "Yeah, I think-- I mean, we can cut out\nthe main part here. I'll just fast forward.",
    "start": "3642350",
    "end": "3648320"
  },
  {
    "text": "Just look at them. Yeah, I was going\nto say, I think you got you all get the gist.",
    "start": "3648320",
    "end": "3654050"
  },
  {
    "text": "NPTs take the entire\ndata set as input and they use self-attention\nto model complex relationships between data points.",
    "start": "3654050",
    "end": "3661010"
  },
  {
    "text": "They do well in\nexperiments on tabular data as well as image data.",
    "start": "3661010",
    "end": "3666110"
  },
  {
    "text": "We present some of these\ninterventional experiments to show that they can solve\ncomplex reasoning tasks. There are some more\nexperiments in the paper.",
    "start": "3666110",
    "end": "3672905"
  },
  {
    "text": "I'd say that the interesting\ntype of future work is scaling type things.",
    "start": "3672905",
    "end": "3678140"
  },
  {
    "text": "So we can-- not having this\nmini batching approximation. And then, also just\ntrying to expand this to some more interesting\napplication demands.",
    "start": "3678140",
    "end": "3684540"
  },
  {
    "text": "So we talked a little\nbit about meta learning, but it could also be things\nlike few shot generalization in general, domain adaptation,\nsemi-supervised learning, et",
    "start": "3684540",
    "end": "3691119"
  },
  {
    "text": "cetera. So I think if there's\nsome more questions, maybe we can do some more discussion.",
    "start": "3691120",
    "end": "3698980"
  },
  {
    "text": "Yeah, sounds good. Great. Thanks for the talk. I think everyone had a fun time. So good. [CLAPPING]",
    "start": "3698980",
    "end": "3705430"
  },
  {
    "text": "I will just also ask\nsome general questions. And then, we can have\na discussion session with everyone after that.",
    "start": "3705430",
    "end": "3712220"
  },
  {
    "text": "So I think one thing\nthat I noticed is this-- you said this is\nsimilar to k-NNs.",
    "start": "3712220",
    "end": "3717280"
  },
  {
    "text": "And I thought this seems\nsimilar to like graph neural networks where you can think\neach data point is like a node.",
    "start": "3717280",
    "end": "3722638"
  },
  {
    "text": "And then, you can\nthink of everything as a fully connected graph. And you're learning some sort\nof attention rate in this graph.",
    "start": "3722638",
    "end": "3728055"
  },
  {
    "text": "So this is like\na node prediction task you are kind of doing\non this graph structure. So any comment on that?",
    "start": "3728055",
    "end": "3734440"
  },
  {
    "text": "Is it similar to\ngraph neural networks or is it like other differences? Yeah, that's a very\ngood observation.",
    "start": "3734440",
    "end": "3741550"
  },
  {
    "text": "Yeah, I think there are a\nlot of similarities to work done on graph neural networks. If we want to talk\nabout differences,",
    "start": "3741550",
    "end": "3747730"
  },
  {
    "text": "the differences\nmight be that we're kind of assuming a fully\nconnected graph, right?",
    "start": "3747730",
    "end": "3753170"
  },
  {
    "text": "And so, you could\nmaybe also phrase that as we're discovering\nthe relational structure. Whereas, graph neural networks\nusually assume that it's given.",
    "start": "3753170",
    "end": "3760630"
  },
  {
    "text": "But that's also not always true. And so, there are a\nlot of similarities. I don't know, Neil, if there\nis something specific you would",
    "start": "3760630",
    "end": "3766960"
  },
  {
    "text": "like to mention, go ahead. But it's a very\ngood observation. And we also do feel\nthat that's the case.",
    "start": "3766960",
    "end": "3772539"
  },
  {
    "text": "And we've added an extra\nsection on related work on geographical networks in the\nupdated version of the paper",
    "start": "3772540",
    "end": "3778180"
  },
  {
    "text": "that will be online soon. Got it. Yeah, I agree with\neverything you've said.",
    "start": "3778180",
    "end": "3784570"
  },
  {
    "text": "I think that the closest work\nfrom the GNN literature that we were looking at a little bit\nwas this neural relational",
    "start": "3784570",
    "end": "3789880"
  },
  {
    "text": "inference paper, which uses\nmessage passing neural networks to try to learn edges that\nmay or may not exist and help",
    "start": "3789880",
    "end": "3798310"
  },
  {
    "text": "for extrapolating,\nI think, positions of particles in a multi-particle\nsystem or something, which",
    "start": "3798310",
    "end": "3805540"
  },
  {
    "text": "is a similar idea to us. If you don't have\nthese edges as given, the attention\nmechanism can kind of",
    "start": "3805540",
    "end": "3810970"
  },
  {
    "text": "approximate an\ninteresting relationship amongst some interacting things. I see.",
    "start": "3810970",
    "end": "3816610"
  },
  {
    "text": "Got it. Yeah, that's really cool. Another thing is-- so you\nmostly look on tabular data.",
    "start": "3816610",
    "end": "3821619"
  },
  {
    "text": "But can you also\nhave other modalities like if you want to do\nlanguage or something? Can you still use\nnon-parametric transformers?",
    "start": "3821620",
    "end": "3830640"
  },
  {
    "text": "Yeah, so I think part of our\nmotivation for doing tabular was because we felt like\ntabular data is, in a sense,",
    "start": "3830640",
    "end": "3836850"
  },
  {
    "text": "a generalization of, let's say,\nthe language data, for example. I mean, I guess there's these\nother notions that people",
    "start": "3836850",
    "end": "3843990"
  },
  {
    "text": "have brought up like padding. But ultimately,\nyou can think of it as a bunch of\ncategorical attributes.",
    "start": "3843990",
    "end": "3850829"
  },
  {
    "text": "So it is definitely\ngeneralizable to things like sentences. And we do images.",
    "start": "3850830",
    "end": "3856619"
  },
  {
    "text": "So yeah. I think, actually, like--",
    "start": "3856620",
    "end": "3861850"
  },
  {
    "text": "I always go back and\nforth on whether or not I think smaller or larger data\nis more interesting for us.",
    "start": "3861850",
    "end": "3867668"
  },
  {
    "text": "So I think small data\nis really interesting because we can just fit the\nentire data set into it. And all of this just\nworks out of the box",
    "start": "3867668",
    "end": "3875460"
  },
  {
    "text": "without any extra thought. But large data is actually\nalso really interesting because, sure, you\nmight have to introduce",
    "start": "3875460",
    "end": "3882780"
  },
  {
    "text": "some approximative mechanism\nor some lookup mechanism because you can't always\nhave the entire data set in.",
    "start": "3882780",
    "end": "3889140"
  },
  {
    "text": "But, at the same time, you\nare very explicitly kind of trading off the\ncompute that you",
    "start": "3889140",
    "end": "3894540"
  },
  {
    "text": "use to look up with the\ncompute that you need to store. How many parameters in GPT\nare used for storing data?",
    "start": "3894540",
    "end": "3903358"
  },
  {
    "text": "There's lots of memorization\nhappening in these models, and we know that. And so, maybe we can use the\nparameters more efficiently",
    "start": "3903358",
    "end": "3910080"
  },
  {
    "text": "to learn lookup\ntype behavior that is more close to this\nneural k-NN or whatever.",
    "start": "3910080",
    "end": "3916250"
  },
  {
    "text": "So I think these are\nvery exciting questions. Yeah. I'll also be looking\nforward to the future",
    "start": "3916250",
    "end": "3922650"
  },
  {
    "text": "because this seems\nlike a very good way to do a one shot kind\nof learning situations. So really interesting\nto see that.",
    "start": "3922650",
    "end": "3929190"
  },
  {
    "text": " OK, so I will stop\nthe recording and we",
    "start": "3929190",
    "end": "3935130"
  },
  {
    "text": "can have any other questions. ",
    "start": "3935130",
    "end": "3943000"
  }
]