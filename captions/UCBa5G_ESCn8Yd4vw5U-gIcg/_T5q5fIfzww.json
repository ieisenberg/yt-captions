[
  {
    "start": "0",
    "end": "4862"
  },
  {
    "text": "CHRISTOPHER POTTS:\nHello, everyone.",
    "start": "4862",
    "end": "6319"
  },
  {
    "text": "Welcome to part 5 in our\nseries on supervised sentiment",
    "start": "6320",
    "end": "8690"
  },
  {
    "text": "analysis.",
    "start": "8690",
    "end": "9320"
  },
  {
    "text": "The focus of this screencast\nis on the module sst.py,",
    "start": "9320",
    "end": "12590"
  },
  {
    "text": "which is included in the\ncourse code distribution.",
    "start": "12590",
    "end": "15890"
  },
  {
    "text": "It contains a\nbunch of tools that",
    "start": "15890",
    "end": "17300"
  },
  {
    "text": "will let you work\nfluidly, I hope,",
    "start": "17300",
    "end": "19130"
  },
  {
    "text": "with the Stanford\nSentiment Treebank",
    "start": "19130",
    "end": "20960"
  },
  {
    "text": "and conduct a lot of\nexperiments in service",
    "start": "20960",
    "end": "24140"
  },
  {
    "text": "of completing the homework and\nalso doing an original system",
    "start": "24140",
    "end": "26960"
  },
  {
    "text": "entry for the bake off.",
    "start": "26960",
    "end": "28519"
  },
  {
    "text": "Let's say that my goals for\nthe screencast are two-fold.",
    "start": "28520",
    "end": "31063"
  },
  {
    "text": "First, I do just want to get\nyou acquainted with this code",
    "start": "31063",
    "end": "33480"
  },
  {
    "text": "so that you can work with it on\nthe assignment in the bake off.",
    "start": "33480",
    "end": "36737"
  },
  {
    "text": "And in addition, I'd guess I'd\nlike to convey to you some best",
    "start": "36737",
    "end": "39320"
  },
  {
    "text": "practices around\nsetting up a code",
    "start": "39320",
    "end": "41810"
  },
  {
    "text": "infrastructure for\na project, say,",
    "start": "41810",
    "end": "44270"
  },
  {
    "text": "that will let you run\na lot of experiments",
    "start": "44270",
    "end": "46370"
  },
  {
    "text": "and really explore\nthe space of ideas",
    "start": "46370",
    "end": "48289"
  },
  {
    "text": "that you have without\nintroducing a lot of bugs",
    "start": "48290",
    "end": "50900"
  },
  {
    "text": "or writing a lot of extra code.",
    "start": "50900",
    "end": "53420"
  },
  {
    "text": "So let's begin.",
    "start": "53420",
    "end": "54149"
  },
  {
    "text": "We'll start with these\nreader functions.",
    "start": "54150",
    "end": "55940"
  },
  {
    "text": "At the top in the\nfirst cell here,",
    "start": "55940",
    "end": "57410"
  },
  {
    "text": "I just load in not\nonly OS so that we",
    "start": "57410",
    "end": "59300"
  },
  {
    "text": "can find our files,\nbut also sst which",
    "start": "59300",
    "end": "61399"
  },
  {
    "text": "is the module of interest.",
    "start": "61400",
    "end": "63320"
  },
  {
    "text": "We set up this variable here\nthat's a pointer to where",
    "start": "63320",
    "end": "66200"
  },
  {
    "text": "the data set itself lives.",
    "start": "66200",
    "end": "68210"
  },
  {
    "text": "And then this function,\nsst.train_reader,",
    "start": "68210",
    "end": "70760"
  },
  {
    "text": "will let you load in a Pandas\ndata frame that contains",
    "start": "70760",
    "end": "74270"
  },
  {
    "text": "the train set for the sst.",
    "start": "74270",
    "end": "76100"
  },
  {
    "text": "You'll notice that there\nare two optional keywords,",
    "start": "76100",
    "end": "78409"
  },
  {
    "text": "include_subtrees and dedup.",
    "start": "78410",
    "end": "80900"
  },
  {
    "text": "Dedup will remove\nrepeated examples,",
    "start": "80900",
    "end": "82790"
  },
  {
    "text": "and include_subtrees\nis a flag that",
    "start": "82790",
    "end": "84620"
  },
  {
    "text": "will let you include\nor exclude all",
    "start": "84620",
    "end": "86930"
  },
  {
    "text": "of the subtrees that\nthe sst contains.",
    "start": "86930",
    "end": "89330"
  },
  {
    "text": "By default, we'll include\njust the full examples.",
    "start": "89330",
    "end": "92300"
  },
  {
    "text": "But if you said, include\nsubtrees equals true,",
    "start": "92300",
    "end": "95150"
  },
  {
    "text": "you get a much\nlarger data set as we",
    "start": "95150",
    "end": "97220"
  },
  {
    "text": "discussed in the screencast\non the sst itself.",
    "start": "97220",
    "end": "101030"
  },
  {
    "text": "In cell 4 here, I'm just giving\nyou a look at one random record",
    "start": "101030",
    "end": "104220"
  },
  {
    "text": "from this.",
    "start": "104220",
    "end": "104720"
  },
  {
    "text": "So remember, it is\na PANDAS data frame.",
    "start": "104720",
    "end": "107107"
  },
  {
    "text": "But we can get it as a\ndictionary for a little bit",
    "start": "107107",
    "end": "109190"
  },
  {
    "text": "of an easier look.",
    "start": "109190",
    "end": "110480"
  },
  {
    "text": "We've got an example ID.",
    "start": "110480",
    "end": "112070"
  },
  {
    "text": "We have the text of the\nsentence, the label,",
    "start": "112070",
    "end": "114470"
  },
  {
    "text": "which is either negative,\npositive, or neutral.",
    "start": "114470",
    "end": "116780"
  },
  {
    "text": "And then is_subtree is\na flag on whether or not",
    "start": "116780",
    "end": "119299"
  },
  {
    "text": "it's a full root level example\nor a subconstituent of such",
    "start": "119300",
    "end": "123050"
  },
  {
    "text": "an example.",
    "start": "123050",
    "end": "124732"
  },
  {
    "text": "Since we have loaded\nthis in with include",
    "start": "124732",
    "end": "126440"
  },
  {
    "text": "subtrees equals false, we get\nthis distribution of labels",
    "start": "126440",
    "end": "129539"
  },
  {
    "text": "here.",
    "start": "129539",
    "end": "130039"
  },
  {
    "text": "This is just a distribution of\nlabels on the full examples.",
    "start": "130039",
    "end": "133670"
  },
  {
    "text": "But of course, as we\nchange these flags,",
    "start": "133670",
    "end": "135360"
  },
  {
    "text": "we get very different\ncounts down here.",
    "start": "135360",
    "end": "138050"
  },
  {
    "text": "And then something comparable\nhappens with the dev reader--",
    "start": "138050",
    "end": "140930"
  },
  {
    "text": "dev_df from sst.dev_reader, with\na pointer to the home directory",
    "start": "140930",
    "end": "145159"
  },
  {
    "text": "for the data as before.",
    "start": "145160",
    "end": "146930"
  },
  {
    "text": "And here, the\nsubtree distinction",
    "start": "146930",
    "end": "148489"
  },
  {
    "text": "and the dedup distinction,\nthose are much less important",
    "start": "148490",
    "end": "151160"
  },
  {
    "text": "because these data sets consist\njust of root level examples.",
    "start": "151160",
    "end": "154460"
  },
  {
    "text": "And there are very few,\nif any, duplicate examples",
    "start": "154460",
    "end": "157190"
  },
  {
    "text": "in those data sets.",
    "start": "157190",
    "end": "158120"
  },
  {
    "start": "158120",
    "end": "161030"
  },
  {
    "text": "Now let's turn to\nfeature functions.",
    "start": "161030",
    "end": "162620"
  },
  {
    "text": "We'll begin to build up\na framework for doing",
    "start": "162620",
    "end": "164690"
  },
  {
    "text": "supervised sentiment analysis.",
    "start": "164690",
    "end": "166950"
  },
  {
    "text": "And the starting point\nhere is what I've",
    "start": "166950",
    "end": "168950"
  },
  {
    "text": "called the feature function.",
    "start": "168950",
    "end": "170450"
  },
  {
    "text": "It's given in two, unigrams_phi.",
    "start": "170450",
    "end": "172069"
  },
  {
    "text": "It takes in a text\nthat is a string.",
    "start": "172070",
    "end": "174860"
  },
  {
    "text": "And what it does is return a\ndictionary that is essentially",
    "start": "174860",
    "end": "178340"
  },
  {
    "text": "a count dictionary over\nthe unigrams in that string",
    "start": "178340",
    "end": "181910"
  },
  {
    "text": "as given by this very simple\ntokenization scheme, which",
    "start": "181910",
    "end": "185210"
  },
  {
    "text": "just down cases all\nof the tokens and then",
    "start": "185210",
    "end": "188060"
  },
  {
    "text": "splits on whitespace.",
    "start": "188060",
    "end": "189950"
  },
  {
    "text": "So as an example text, if I\nhave \"NLU is enlightening,\"",
    "start": "189950",
    "end": "192739"
  },
  {
    "text": "space, and then an\nexclamation mark,",
    "start": "192740",
    "end": "194870"
  },
  {
    "text": "and I call the feature\nfunction on that string,",
    "start": "194870",
    "end": "198349"
  },
  {
    "text": "I get this count\ndictionary here,",
    "start": "198350",
    "end": "200133"
  },
  {
    "text": "which is just giving\nthe number of times",
    "start": "200133",
    "end": "201800"
  },
  {
    "text": "each token appears\nin that string",
    "start": "201800",
    "end": "204260"
  },
  {
    "text": "according to the\nfeature function.",
    "start": "204260",
    "end": "206241"
  },
  {
    "text": "I'd say it's really\nimportant when you're",
    "start": "206242",
    "end": "207950"
  },
  {
    "text": "working with the standard\nversion of this framework",
    "start": "207950",
    "end": "210349"
  },
  {
    "text": "doing handbuilt feature\nfunctions, that you just",
    "start": "210350",
    "end": "212840"
  },
  {
    "text": "abide by the contract that\nall of these feature functions",
    "start": "212840",
    "end": "215750"
  },
  {
    "text": "take in strings and\nreturn dictionaries,",
    "start": "215750",
    "end": "219260"
  },
  {
    "text": "mappings strings\nto their counts.",
    "start": "219260",
    "end": "221150"
  },
  {
    "text": "Or if you want to, they're\nBools or floats or something",
    "start": "221150",
    "end": "224299"
  },
  {
    "text": "that we can make use of when\nwe're doing featurization.",
    "start": "224300",
    "end": "228840"
  },
  {
    "text": "The next step here is what\nI've called a model wrapper.",
    "start": "228840",
    "end": "231180"
  },
  {
    "text": "And this is going to look\na little bit trivial here.",
    "start": "231180",
    "end": "233230"
  },
  {
    "text": "But as you'll see as we move\nthrough more advanced methods",
    "start": "233230",
    "end": "235647"
  },
  {
    "text": "in this unit, especially\nthe next screencast,",
    "start": "235647",
    "end": "237840"
  },
  {
    "text": "it's really nice to\nhave these wrappers",
    "start": "237840",
    "end": "240150"
  },
  {
    "text": "around the normal, essentially\nthe fit function down here.",
    "start": "240150",
    "end": "244267"
  },
  {
    "text": "So I'm going to make use of\na scikit linear model called",
    "start": "244267",
    "end": "246599"
  },
  {
    "text": "LogisticRegression,\na very standard sort",
    "start": "246600",
    "end": "248267"
  },
  {
    "text": "of cross-entropy classifier.",
    "start": "248267",
    "end": "250800"
  },
  {
    "text": "I've called my function\nfit_softmax_classifier.",
    "start": "250800",
    "end": "253350"
  },
  {
    "text": "And it takes in a supervised\ndata set, so a feature matrix",
    "start": "253350",
    "end": "257190"
  },
  {
    "text": "and a list of labels.",
    "start": "257190",
    "end": "259350"
  },
  {
    "text": "And I set up my model down here.",
    "start": "259350",
    "end": "261165"
  },
  {
    "text": "And I've used some of\nthe keyword parameters.",
    "start": "261165",
    "end": "263040"
  },
  {
    "text": "There are many more\nfor the scikit model.",
    "start": "263040",
    "end": "265500"
  },
  {
    "text": "And then the crucial thing\nis that I call fit and return",
    "start": "265500",
    "end": "268320"
  },
  {
    "text": "the model, which is now\na trained model, trained",
    "start": "268320",
    "end": "270900"
  },
  {
    "text": "on this data set xy.",
    "start": "270900",
    "end": "273030"
  },
  {
    "text": "It might look like all I've\ndone is called fit on a model",
    "start": "273030",
    "end": "275610"
  },
  {
    "text": "that I set up.",
    "start": "275610",
    "end": "276270"
  },
  {
    "text": "But as you'll see, it's nice\nto have a wrapper function so",
    "start": "276270",
    "end": "279270"
  },
  {
    "text": "that we can potentially\ndo a lot more as part",
    "start": "279270",
    "end": "282090"
  },
  {
    "text": "of this particular step in\nour experimental workflow.",
    "start": "282090",
    "end": "285837"
  },
  {
    "text": "So now let's just bring\nall those things together",
    "start": "285838",
    "end": "287880"
  },
  {
    "text": "into what is called\nsst.experiment,",
    "start": "287880",
    "end": "290500"
  },
  {
    "text": "which is like one-stop shopping\nfor a complete experiment",
    "start": "290500",
    "end": "293310"
  },
  {
    "text": "in supervised\nsentiment analysis.",
    "start": "293310",
    "end": "295620"
  },
  {
    "text": "So we load in these\ntwo libraries.",
    "start": "295620",
    "end": "297419"
  },
  {
    "text": "We get a pointer to our dataset,\nand then call sst.experiment.",
    "start": "297420",
    "end": "302710"
  },
  {
    "text": "The first argument\nis the dataset",
    "start": "302710",
    "end": "305185"
  },
  {
    "text": "that it will be trained on.",
    "start": "305185",
    "end": "306310"
  },
  {
    "text": "So that's like\ntrain_df from before.",
    "start": "306310",
    "end": "308730"
  },
  {
    "text": "We have a feature function\nand a model wrapper.",
    "start": "308730",
    "end": "311580"
  },
  {
    "text": "And then these other\nthings are optional.",
    "start": "311580",
    "end": "313389"
  },
  {
    "text": "So if I leave\nassess_dataframes as none,",
    "start": "313390",
    "end": "316140"
  },
  {
    "text": "then it will do a random\nsplit on this train reader",
    "start": "316140",
    "end": "318810"
  },
  {
    "text": "according to train size.",
    "start": "318810",
    "end": "320490"
  },
  {
    "text": "If you do specify some data\nframes here, a list of them,",
    "start": "320490",
    "end": "323289"
  },
  {
    "text": "then each one will be used\nas a separate evaluation",
    "start": "323290",
    "end": "326280"
  },
  {
    "text": "against the model that you\ntrain on this original data.",
    "start": "326280",
    "end": "329740"
  },
  {
    "text": "You can set the score\nfunction if you want.",
    "start": "329740",
    "end": "331979"
  },
  {
    "text": "Our default is macro F1.",
    "start": "331980",
    "end": "334048"
  },
  {
    "text": "And then we'll return to\nthese two options later.",
    "start": "334048",
    "end": "336090"
  },
  {
    "text": "Verbose is just whether you\nwant to print some information.",
    "start": "336090",
    "end": "338548"
  },
  {
    "text": "And Vectorize is an option\nthat you can turn on and off.",
    "start": "338548",
    "end": "341520"
  },
  {
    "text": "And you'll probably\nturn it off when",
    "start": "341520",
    "end": "343590"
  },
  {
    "text": "you do deep learning\nexperiments, which we'll",
    "start": "343590",
    "end": "345600"
  },
  {
    "text": "talk about later in the unit.",
    "start": "345600",
    "end": "347880"
  },
  {
    "text": "The result of all that\nis a bunch of information",
    "start": "347880",
    "end": "350070"
  },
  {
    "text": "about your experiments\nstored in this variable.",
    "start": "350070",
    "end": "352350"
  },
  {
    "text": "And because we had\nverbose equals true,",
    "start": "352350",
    "end": "354510"
  },
  {
    "text": "you're going to report here.",
    "start": "354510",
    "end": "356580"
  },
  {
    "text": "And this is just a\nfirst chance to call out",
    "start": "356580",
    "end": "358590"
  },
  {
    "text": "that throughout this\ncourse, essentially, when",
    "start": "358590",
    "end": "361260"
  },
  {
    "text": "we do classifier experiments,\nour primary metric is",
    "start": "361260",
    "end": "364620"
  },
  {
    "text": "going to be the macro\naverage F1 score.",
    "start": "364620",
    "end": "367440"
  },
  {
    "text": "This is useful for us\nbecause it gives equal weight",
    "start": "367440",
    "end": "369750"
  },
  {
    "text": "to all the classes in our data,\nregardless of their size, which",
    "start": "369750",
    "end": "374160"
  },
  {
    "text": "is typically reflecting\nour value that we care even",
    "start": "374160",
    "end": "376500"
  },
  {
    "text": "about small classes.",
    "start": "376500",
    "end": "377610"
  },
  {
    "text": "We want to do well even on\nthe rare events in our space.",
    "start": "377610",
    "end": "380819"
  },
  {
    "text": "And it's also perfectly\nbalancing precision and recall",
    "start": "380820",
    "end": "383220"
  },
  {
    "text": "which is like a\ngood null hypothesis",
    "start": "383220",
    "end": "385440"
  },
  {
    "text": "if we're not told ahead of\ntime based on some other goal",
    "start": "385440",
    "end": "388500"
  },
  {
    "text": "whether we should favor\nprecision or recall.",
    "start": "388500",
    "end": "391410"
  },
  {
    "text": "So that all leads us to\nkind of favor as a default",
    "start": "391410",
    "end": "393750"
  },
  {
    "text": "this macro average F1\nscore as an assessment",
    "start": "393750",
    "end": "396210"
  },
  {
    "text": "of how the model fit.",
    "start": "396210",
    "end": "397680"
  },
  {
    "text": "And here we've gotten 51.3.",
    "start": "397680",
    "end": "401360"
  },
  {
    "text": "The return value of\nsst.experiment, as I said,",
    "start": "401360",
    "end": "404539"
  },
  {
    "text": "is a dictionary.",
    "start": "404540",
    "end": "405410"
  },
  {
    "text": "And it should package up for you\nall the objects and information",
    "start": "405410",
    "end": "408710"
  },
  {
    "text": "you would need to test the\nmodel, assess the model,",
    "start": "408710",
    "end": "411740"
  },
  {
    "text": "and do all kinds of\ndeep error analysis.",
    "start": "411740",
    "end": "413840"
  },
  {
    "text": "That is the philosophy here\nthat you should, if possible,",
    "start": "413840",
    "end": "416540"
  },
  {
    "text": "capture as much information as\nyou can about the experiment",
    "start": "416540",
    "end": "419270"
  },
  {
    "text": "that you ran in the\nservice of being",
    "start": "419270",
    "end": "421370"
  },
  {
    "text": "able to do subsequent downstream\nanalysis of what happened.",
    "start": "421370",
    "end": "424767"
  },
  {
    "text": "And so here I'm just\ngiving an example",
    "start": "424767",
    "end": "426350"
  },
  {
    "text": "that we've got the model, the\nfeature function, the train",
    "start": "426350",
    "end": "429470"
  },
  {
    "text": "dataset, whenever our\nassess datasets were used.",
    "start": "429470",
    "end": "431690"
  },
  {
    "text": "And if that was a random\nsplit of the train data,",
    "start": "431690",
    "end": "433820"
  },
  {
    "text": "that will be reflected\nin these two variables.",
    "start": "433820",
    "end": "436460"
  },
  {
    "text": "The set of predictions\nthat you made",
    "start": "436460",
    "end": "437960"
  },
  {
    "text": "about each one of\nthe assess datasets,",
    "start": "437960",
    "end": "440150"
  },
  {
    "text": "the metrics you chose, and\nthe scores that you got.",
    "start": "440150",
    "end": "443389"
  },
  {
    "text": "And then if you do dive in,\nlike if you look at train set,",
    "start": "443390",
    "end": "446120"
  },
  {
    "text": "it's a standard data set.",
    "start": "446120",
    "end": "447720"
  },
  {
    "text": "x is your feature space.",
    "start": "447720",
    "end": "449780"
  },
  {
    "text": "y is your labels.",
    "start": "449780",
    "end": "451100"
  },
  {
    "text": "Vectorizer is something\nthat I'll return to.",
    "start": "451100",
    "end": "453060"
  },
  {
    "text": "That's an important part about\nhow the internal workings",
    "start": "453060",
    "end": "455780"
  },
  {
    "text": "of sst.experiment function.",
    "start": "455780",
    "end": "457790"
  },
  {
    "text": "And then you have the\nraw examples in case",
    "start": "457790",
    "end": "459680"
  },
  {
    "text": "you need to do some really\nserious human level error",
    "start": "459680",
    "end": "462470"
  },
  {
    "text": "analysis of the examples as\ndistinct from how they're",
    "start": "462470",
    "end": "465950"
  },
  {
    "text": "represented in\nthis feature space.",
    "start": "465950",
    "end": "470120"
  },
  {
    "text": "So here is just a slide that\nbrings all of those pieces",
    "start": "470120",
    "end": "472479"
  },
  {
    "text": "together.",
    "start": "472480",
    "end": "473030"
  },
  {
    "text": "This is one-stop shopping\nfor an entire experiment.",
    "start": "473030",
    "end": "476170"
  },
  {
    "text": "We loaded all our libraries.",
    "start": "476170",
    "end": "477490"
  },
  {
    "text": "We have our pointer to the data.",
    "start": "477490",
    "end": "479380"
  },
  {
    "text": "And then the ingredients are\nreally a feature function",
    "start": "479380",
    "end": "482200"
  },
  {
    "text": "and a model wrapper.",
    "start": "482200",
    "end": "483760"
  },
  {
    "text": "And that's all you need\nin our default setting.",
    "start": "483760",
    "end": "486370"
  },
  {
    "text": "Point it to the train data\nand it will do its job",
    "start": "486370",
    "end": "489070"
  },
  {
    "text": "and record all you would\nwant for this experiment,",
    "start": "489070",
    "end": "491750"
  },
  {
    "text": "I hope, in this\nexperiment variable here.",
    "start": "491750",
    "end": "496060"
  },
  {
    "text": "There's a final piece.",
    "start": "496060",
    "end": "496990"
  },
  {
    "text": "I want to return to\nthat vectorizer variable",
    "start": "496990",
    "end": "499210"
  },
  {
    "text": "that you saw in the return\nvalues for sst.experiment.",
    "start": "499210",
    "end": "502778"
  },
  {
    "text": "And that is making use of\nwhat in scikit-learn is",
    "start": "502778",
    "end": "504820"
  },
  {
    "text": "called the DictVectorizer.",
    "start": "504820",
    "end": "507100"
  },
  {
    "text": "And this is really nice\nconvenience function",
    "start": "507100",
    "end": "509480"
  },
  {
    "text": "for translating from human\nrepresentations of your data",
    "start": "509480",
    "end": "513250"
  },
  {
    "text": "into representations that\nmachine learning models like",
    "start": "513250",
    "end": "516159"
  },
  {
    "text": "to consume.",
    "start": "516159",
    "end": "517360"
  },
  {
    "text": "So let me just walk\nthrough this example here.",
    "start": "517360",
    "end": "519320"
  },
  {
    "text": "I've loaded the DictVectorizer,\nand I've got my train features",
    "start": "519320",
    "end": "523090"
  },
  {
    "text": "here in the mode that\nI just showed you",
    "start": "523090",
    "end": "524950"
  },
  {
    "text": "where here we have two examples\nand each one is represented",
    "start": "524950",
    "end": "528610"
  },
  {
    "text": "by our feature function\nas a dictionary",
    "start": "528610",
    "end": "531130"
  },
  {
    "text": "that maps like words\ninto their counts.",
    "start": "531130",
    "end": "534520"
  },
  {
    "text": "It could be more\nflexible than that,",
    "start": "534520",
    "end": "536020"
  },
  {
    "text": "but that's like the most\nbasic case that we consider.",
    "start": "536020",
    "end": "539290"
  },
  {
    "text": "And I set up my vectorizer in\n3, and then I call fit_transform",
    "start": "539290",
    "end": "544720"
  },
  {
    "text": "on this list of dictionaries.",
    "start": "544720",
    "end": "546970"
  },
  {
    "text": "And the result here,\nx_train, is a matrix,",
    "start": "546970",
    "end": "550420"
  },
  {
    "text": "where each of the\ncolumns corresponds",
    "start": "550420",
    "end": "552700"
  },
  {
    "text": "to the keys in the dictionary\nrepresenting a unique feature.",
    "start": "552700",
    "end": "556930"
  },
  {
    "text": "And the values are, of\ncourse, stored in that column.",
    "start": "556930",
    "end": "560240"
  },
  {
    "text": "So this feature\nspace here has been",
    "start": "560240",
    "end": "562240"
  },
  {
    "text": "turned into a matrix that\nhas two examples, 0 and 1.",
    "start": "562240",
    "end": "567190"
  },
  {
    "text": "There are a total of\nthree features represented",
    "start": "567190",
    "end": "569470"
  },
  {
    "text": "across our two\nexamples, A, B, and C.",
    "start": "569470",
    "end": "572709"
  },
  {
    "text": "And you can see that the\ncounts are stored here.",
    "start": "572710",
    "end": "574710"
  },
  {
    "text": "So example 0 has 1 for A\nand 1 for B and 0 for C.",
    "start": "574710",
    "end": "581170"
  },
  {
    "text": "And example 1 has 0 for\nA, 1 for B, and 2 for C.",
    "start": "581170",
    "end": "587170"
  },
  {
    "text": "So that's recorded\nin the columns here.",
    "start": "587170",
    "end": "589180"
  },
  {
    "text": "You can, of course,\nundertake this step by hand.",
    "start": "589180",
    "end": "592420"
  },
  {
    "text": "But it's a kind of\nerror-prone step.",
    "start": "592420",
    "end": "594519"
  },
  {
    "text": "And I'm just encouraging\nyou to use DictVectorizer",
    "start": "594520",
    "end": "596830"
  },
  {
    "text": "to handle it all and\nessentially map you",
    "start": "596830",
    "end": "599350"
  },
  {
    "text": "from this, which is pretty\nhuman interpretable, into this,",
    "start": "599350",
    "end": "602600"
  },
  {
    "text": "which is something your\nmodels like to consume.",
    "start": "602600",
    "end": "606100"
  },
  {
    "text": "There's a second\nadvantage here, which",
    "start": "606100",
    "end": "608860"
  },
  {
    "text": "is that if you use\na DictVectorizer",
    "start": "608860",
    "end": "611140"
  },
  {
    "text": "and you need to now do\nsomething at test time,",
    "start": "611140",
    "end": "614260"
  },
  {
    "text": "you can easily use\nyour vectorizer",
    "start": "614260",
    "end": "616000"
  },
  {
    "text": "to create feature spaces\nthat are harmonized",
    "start": "616000",
    "end": "618250"
  },
  {
    "text": "with what you saw in training.",
    "start": "618250",
    "end": "619540"
  },
  {
    "text": "So as an example,\nif my test features",
    "start": "619540",
    "end": "622149"
  },
  {
    "text": "are another pair of examples\nwith a different character,",
    "start": "622150",
    "end": "626050"
  },
  {
    "text": "then I can call transform\non the original trained",
    "start": "626050",
    "end": "629589"
  },
  {
    "text": "vectorizer from up here.",
    "start": "629590",
    "end": "631090"
  },
  {
    "text": "And it will translate that\nlist of features into a matrix.",
    "start": "631090",
    "end": "634983"
  },
  {
    "text": "Now, the important thing\nabout what's happening here",
    "start": "634983",
    "end": "637149"
  },
  {
    "text": "is that it's going\nto package the test",
    "start": "637150",
    "end": "638980"
  },
  {
    "text": "features into the\noriginal training space",
    "start": "638980",
    "end": "641370"
  },
  {
    "text": "because, of course,\nthose are the features",
    "start": "641370",
    "end": "643120"
  },
  {
    "text": "that your model recognizes.",
    "start": "643120",
    "end": "644737"
  },
  {
    "text": "Those are the features\nthat you have weights",
    "start": "644737",
    "end": "646570"
  },
  {
    "text": "for if you've trained a model.",
    "start": "646570",
    "end": "648190"
  },
  {
    "text": "So it's important to call\ntransform at this space.",
    "start": "648190",
    "end": "650920"
  },
  {
    "text": "And as an indication of\none of the things that's",
    "start": "650920",
    "end": "652959"
  },
  {
    "text": "going to happen here is notice\nthat in the test features,",
    "start": "652960",
    "end": "655930"
  },
  {
    "text": "my second example has\na brand new feature D.",
    "start": "655930",
    "end": "659080"
  },
  {
    "text": "But D is not represented\nin the training space.",
    "start": "659080",
    "end": "661870"
  },
  {
    "text": "We have no weights for it.",
    "start": "661870",
    "end": "663130"
  },
  {
    "text": "It's simply not part of that\noriginal training data set.",
    "start": "663130",
    "end": "667040"
  },
  {
    "text": "And so the result is that\nwhen we call transform,",
    "start": "667040",
    "end": "669459"
  },
  {
    "text": "that feature is\nsimply elided, which",
    "start": "669460",
    "end": "672310"
  },
  {
    "text": "is the desired behavior\nas we're translating",
    "start": "672310",
    "end": "674470"
  },
  {
    "text": "from training into testing.",
    "start": "674470",
    "end": "676480"
  },
  {
    "text": "And notice that the\nDictVectorizer has simply",
    "start": "676480",
    "end": "678970"
  },
  {
    "text": "handled that seamlessly\nfor you, provided",
    "start": "678970",
    "end": "681850"
  },
  {
    "text": "that you remember\nat the second stage",
    "start": "681850",
    "end": "683889"
  },
  {
    "text": "not to call fit_transform.",
    "start": "683890",
    "end": "685570"
  },
  {
    "text": "That's the number one\ngotcha for this interface",
    "start": "685570",
    "end": "688360"
  },
  {
    "text": "is that if you call\nfit_transform a second time,",
    "start": "688360",
    "end": "691180"
  },
  {
    "text": "it will simply\nchange the feature",
    "start": "691180",
    "end": "692800"
  },
  {
    "text": "space into the one that is\nrepresented in your test",
    "start": "692800",
    "end": "695649"
  },
  {
    "text": "features.",
    "start": "695650",
    "end": "696430"
  },
  {
    "text": "And then everything\nwill fall apart.",
    "start": "696430",
    "end": "698300"
  },
  {
    "text": "And your model, as\ntrained from before,",
    "start": "698300",
    "end": "700600"
  },
  {
    "text": "will be unable to consume\nthese new matrices",
    "start": "700600",
    "end": "703449"
  },
  {
    "text": "that you've created.",
    "start": "703450",
    "end": "704590"
  },
  {
    "text": "But provided you remember that\nthe rhythm is fit_transform",
    "start": "704590",
    "end": "707410"
  },
  {
    "text": "and then transform, this\nshould be really a nice set",
    "start": "707410",
    "end": "710829"
  },
  {
    "text": "of interfaces.",
    "start": "710830",
    "end": "711590"
  },
  {
    "text": "And of course, this\nis what sst.experiment",
    "start": "711590",
    "end": "714100"
  },
  {
    "text": "is doing by default\nunder the hood for you.",
    "start": "714100",
    "end": "718290"
  },
  {
    "start": "718290",
    "end": "722000"
  }
]