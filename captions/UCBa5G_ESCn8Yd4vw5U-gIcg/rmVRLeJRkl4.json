[
  {
    "start": "0",
    "end": "103000"
  },
  {
    "text": "Hi, everybody. Welcome to Stanford's\nCS224N, also known",
    "start": "5300",
    "end": "11540"
  },
  {
    "text": "as Ling284, Natural Language\nProcessing with Deep Learning.",
    "start": "11540",
    "end": "16610"
  },
  {
    "text": "I'm Christopher Manning,\nand I'm the main instructor for this class.",
    "start": "16610",
    "end": "22140"
  },
  {
    "text": "So what we hope to do\ntoday is to dive right in.",
    "start": "22140",
    "end": "27300"
  },
  {
    "text": "So I'm going to spend\nabout 10 minutes talking about the course,\nand then we're going",
    "start": "27300",
    "end": "32390"
  },
  {
    "text": "to get straight into\ncontent for reasons I'll explain in a minute. So we'll talk about human\nlanguage and word meaning,",
    "start": "32390",
    "end": "39739"
  },
  {
    "text": "I'll then introduce the ideas\nof the word2vec algorithm for learning word meaning.",
    "start": "39740",
    "end": "45170"
  },
  {
    "text": "And then going from there\nwe'll kind of concretely work through how you can\nwork out objective function",
    "start": "45170",
    "end": "51200"
  },
  {
    "text": "gradients with respect to\nthe word2vec algorithm, and say a teeny bit about\nhow optimization works.",
    "start": "51200",
    "end": "58129"
  },
  {
    "text": "And then right at\nthe end of the class I then want to spend\na little bit of time giving you a sense of how\nthese word vectors work,",
    "start": "58130",
    "end": "66290"
  },
  {
    "text": "and what you can do with them. So really the key\nlearning for today",
    "start": "66290",
    "end": "71329"
  },
  {
    "text": "is, I want to give you a sense\nof how amazing deep learning",
    "start": "71330",
    "end": "76370"
  },
  {
    "text": "word vectors are. So we have this really\nsurprising result that word meaning\ncan be represented,",
    "start": "76370",
    "end": "82850"
  },
  {
    "text": "not perfectly but\nreally rather well by a large vector\nof real numbers.",
    "start": "82850",
    "end": "89000"
  },
  {
    "text": "And that's sort of in a way, a\ncommonplace of the last decade of deep learning, but it\nflies in the face of thousands",
    "start": "89000",
    "end": "96470"
  },
  {
    "text": "of years of tradition. And it's really rather\nan unexpected result to start focusing on.",
    "start": "96470",
    "end": "103119"
  },
  {
    "start": "103000",
    "end": "190000"
  },
  {
    "text": "OK, so quickly what do we\nhope to teach in this course? So we've got three\nprimary goals.",
    "start": "103120",
    "end": "110520"
  },
  {
    "text": "The first is to teach\nyou the foundation, say a good deep understanding\nof the effect of modern methods",
    "start": "110520",
    "end": "118010"
  },
  {
    "text": "for deep learning\napplied to NLP. So we are going to start with\nand go through the basics,",
    "start": "118010",
    "end": "124010"
  },
  {
    "text": "and then go on to\nkey methods that are used in NLP, recurrent\nnetworks, attention",
    "start": "124010",
    "end": "129259"
  },
  {
    "text": "transformers, and\nthings like that. We want to do something\nmore than just that.",
    "start": "129259",
    "end": "134969"
  },
  {
    "text": "We'd also like to give you\nsome sense of a big picture understanding of human\nlanguages and what",
    "start": "134970",
    "end": "141260"
  },
  {
    "text": "are the reasons for\nwhy they're actually quite difficult to\nunderstand and produce even though humans\nseem to do it easily.",
    "start": "141260",
    "end": "149640"
  },
  {
    "text": "Now obviously if you\nreally want to learn a lot about this topic,\nyou should enroll in and go and start\ndoing some classes",
    "start": "149640",
    "end": "155330"
  },
  {
    "text": "in the linguistics department. But nevertheless\nfor a lot of you, this is the only\nhuman language content",
    "start": "155330",
    "end": "162170"
  },
  {
    "text": "you'll see during your\nmaster's degree or whatever. And so we do hope to spend a bit\nof time on that starting today.",
    "start": "162170",
    "end": "169890"
  },
  {
    "text": "And then finally,\nwe want to give you an understanding of an ability\nto build systems in PyTorch,",
    "start": "169890",
    "end": "176660"
  },
  {
    "text": "that's some of the\nmajor problems in NLP. So we'll look at learning word\nmeanings, dependency parsing,",
    "start": "176660",
    "end": "182600"
  },
  {
    "text": "machine translation,\nquestion answering. Let's dive in to human language.",
    "start": "182600",
    "end": "190460"
  },
  {
    "start": "190000",
    "end": "607000"
  },
  {
    "text": "Once upon a time, I had a\nlot longer introduction that gave lots of examples about\nhow human languages can",
    "start": "190460",
    "end": "198140"
  },
  {
    "text": "be misunderstood\nand complex, I'll show a few of those\nexamples in later lectures.",
    "start": "198140",
    "end": "205920"
  },
  {
    "text": "But since right for\ntoday, we're going to be focused on word meaning. I thought I'd just give\none example, which comes",
    "start": "205920",
    "end": "214790"
  },
  {
    "text": "from a very nice xkcd cartoon. And that isn't\nsort of about some",
    "start": "214790",
    "end": "222200"
  },
  {
    "text": "of the syntactic\nambiguities of sentences, but instead it's really\nemphasizing the important point",
    "start": "222200",
    "end": "229849"
  },
  {
    "text": "that language is a social\nsystem, constructed and interpreted by people.",
    "start": "229850",
    "end": "234980"
  },
  {
    "text": "And that's part of how-- and it changes as people decide\nto adapt its construction,",
    "start": "234980",
    "end": "242220"
  },
  {
    "text": "and that's part\nof the reason why human languages, great\nas an adaptive system for human beings but difficult\nas a system for our computers",
    "start": "242220",
    "end": "253460"
  },
  {
    "text": "to understand to this day. So in this conversation between\nthe two women, one says,",
    "start": "253460",
    "end": "259459"
  },
  {
    "text": "anyway I could care less. And the other says, I think you\nmean you couldn't care less,",
    "start": "259459",
    "end": "265170"
  },
  {
    "text": "saying you could\ncare less implies you care at least some amount. And the other one\nsays, I don't know",
    "start": "265170",
    "end": "272600"
  },
  {
    "text": "where these unbelievably\ncomplicated brains drifting through a void trying in vain\nto connect with one another,",
    "start": "272600",
    "end": "279650"
  },
  {
    "text": "by plainly fleeing words\nout into the darkness. Every choice of phrasing,\nspelling, and tone, and timing",
    "start": "279650",
    "end": "287330"
  },
  {
    "text": "carries countless sync signals\nand contexts and subtext, and more and every\nlistener interprets",
    "start": "287330",
    "end": "294650"
  },
  {
    "text": "those signals in their own way. Language isn't a formal system,\nlanguage is glorious chaos.",
    "start": "294650",
    "end": "301880"
  },
  {
    "text": "You can never know for sure what\nany words will mean to anyone, all you can do is try to\nget better at guessing",
    "start": "301880",
    "end": "309020"
  },
  {
    "text": "how your words affect people. So you can have a chance\nof finding the ones that will make them feel\nsomething like what",
    "start": "309020",
    "end": "315710"
  },
  {
    "text": "you want them to feel. Everything else is pointless. I assume you're giving me tips\non how you interpret words",
    "start": "315710",
    "end": "322930"
  },
  {
    "text": "because you want me\nto feel less alone. If so, then thank\nyou, that means a lot.",
    "start": "322930",
    "end": "329740"
  },
  {
    "text": "But if you're just\nrunning my sentences to pass some mental\nchecklist so you can show off how well you know\nit, then I could care less.",
    "start": "329740",
    "end": "338580"
  },
  {
    "text": "OK, so that's\nultimately what our goal is, how to do a better job at\nbuilding computational systems",
    "start": "338580",
    "end": "348720"
  },
  {
    "text": "that try to get better at\nguessing how their words will affect other people and what\nother people are meaning",
    "start": "348720",
    "end": "356700"
  },
  {
    "text": "by the words that\nthey choose to say. So an interesting thing\nabout human language",
    "start": "356700",
    "end": "365590"
  },
  {
    "text": "is, it is a system that was\nconstructed by human beings.",
    "start": "365590",
    "end": "372020"
  },
  {
    "text": "And it's a system that was\nconstructed relatively recently",
    "start": "372020",
    "end": "377979"
  },
  {
    "text": "in some sense. So in discussions of\nartificial intelligence,",
    "start": "377980",
    "end": "383110"
  },
  {
    "text": "a lot of the time people\nfocus a lot on human brains",
    "start": "383110",
    "end": "388389"
  },
  {
    "text": "and the neurons passing\nby, and this intelligence that's meant to be\ninside people's heads.",
    "start": "388390",
    "end": "395300"
  },
  {
    "text": "But I just wanted to focus for a\nmoment on the role of language, there's actually--",
    "start": "395300",
    "end": "401320"
  },
  {
    "text": "this is kind of\ncontroversial but it's not necessarily the case\nthat humans are much more",
    "start": "401320",
    "end": "407979"
  },
  {
    "text": "intelligent than some of the\nhigher apes like chimpanzees or bonobos.",
    "start": "407980",
    "end": "413630"
  },
  {
    "text": "So chimpanzees and\nbonobos have been shown to be able to use\ntools, to make plans,",
    "start": "413630",
    "end": "419259"
  },
  {
    "text": "and in fact chimps have much\nbetter short term memory than human beings do.",
    "start": "419260",
    "end": "425380"
  },
  {
    "text": "So relative to that, if you\nlook through the history of life on Earth, human beings develop\nlanguage really recently.",
    "start": "425380",
    "end": "433810"
  },
  {
    "text": "How recently, we\nkind of actually don't know because there's\nno fossils that say, OK here's a language speaker.",
    "start": "433810",
    "end": "441039"
  },
  {
    "text": "But most people estimate that\nlanguage arose for human beings",
    "start": "441040",
    "end": "447010"
  },
  {
    "text": "sort of somewhere in\nthe range of 100,000",
    "start": "447010",
    "end": "452110"
  },
  {
    "text": "to a million years ago. OK, that's the way I\nlet go but compared to the process of the\nevolution of life on Earth,",
    "start": "452110",
    "end": "458530"
  },
  {
    "text": "that's kind of\nblinking an eyelid. But that powerful communication\nbetween human beings",
    "start": "458530",
    "end": "466750"
  },
  {
    "text": "quickly set off our ascendancy\nover other creatures. So it's kind of interesting that\nthe ultimate power turned out",
    "start": "466750",
    "end": "474910"
  },
  {
    "text": "not to have been poisonous\nfangs or being super fast or super big, that having\nthe ability to communicate",
    "start": "474910",
    "end": "482349"
  },
  {
    "text": "with other members\nof your tribe. It was much more recently again\nthat humans developed writing,",
    "start": "482350",
    "end": "488960"
  },
  {
    "text": "which allowed knowledge to be\ncommunicated across distances of time and space.",
    "start": "488960",
    "end": "494389"
  },
  {
    "text": "And so that's only\nabout 5,000 years old, the power of writing. So in just a few thousand\nyears the ability",
    "start": "494390",
    "end": "502840"
  },
  {
    "text": "to preserve and share knowledge\ntook us from the Bronze Age to the smartphones\nand tablets of today.",
    "start": "502840",
    "end": "510819"
  },
  {
    "text": "So a key question for artificial\nintelligence and human-computer interaction is how\nto get computers",
    "start": "510820",
    "end": "516849"
  },
  {
    "text": "to be able to understand\nthe information conveyed in human languages. Simultaneously,\nartificial intelligence",
    "start": "516850",
    "end": "523929"
  },
  {
    "text": "requires computers with\nthe knowledge of people. Fortunately now,\nAI systems might",
    "start": "523929",
    "end": "529779"
  },
  {
    "text": "be able to benefit\nfrom a virtuous cycle. We need knowledge to understand\nlanguage and people well,",
    "start": "529780",
    "end": "536050"
  },
  {
    "text": "but it's also the case that\na lot of that knowledge is contained in language spread\nout across the books and web",
    "start": "536050",
    "end": "543010"
  },
  {
    "text": "pages of the world. And that's one of\nthe things we're going to look at in\nthis course is, how that we can sort of build\non that virtuous cycle.",
    "start": "543010",
    "end": "551720"
  },
  {
    "text": "A lot of progress\nhas already been made and I just want to very\nquickly give a sense of that.",
    "start": "551720",
    "end": "558810"
  },
  {
    "text": "So in the last decade\nor so and especially in the last few years, with\nnewer methods of machine",
    "start": "558810",
    "end": "566389"
  },
  {
    "text": "translation we're now in a\nspace where machine translation really works moderately well.",
    "start": "566390",
    "end": "573750"
  },
  {
    "text": "So again, from the\nhistory of the world, this is just amazing, right\nfor thousands of years",
    "start": "573750",
    "end": "578959"
  },
  {
    "text": "learning other\npeople's languages was a human task which\nrequired a lot of effort",
    "start": "578960",
    "end": "585680"
  },
  {
    "text": "and concentration. But now we're in a world\nwhere you could just hop on your web\nbrowser and think,",
    "start": "585680",
    "end": "591800"
  },
  {
    "text": "oh, I wonder what the\nnews is in Kenya today, and you can head off\nover to a Kenyan website",
    "start": "591800",
    "end": "597780"
  },
  {
    "text": "and you can see\nsomething like this. And you can go, huh,\nand you can then ask Google to translate\nit for you from Swahili,",
    "start": "597780",
    "end": "606380"
  },
  {
    "text": "and the translation\nisn't quite perfect but it's reasonably good.",
    "start": "606380",
    "end": "612050"
  },
  {
    "start": "607000",
    "end": "643000"
  },
  {
    "text": "So the newspaper, Tuko, has been\ninformed that local government minister links on\n[NON-ENGLISH SPEECH] and his",
    "start": "612050",
    "end": "619460"
  },
  {
    "text": "transport counterparts\n[INAUDIBLE] died within two separate hours. So within two separate hours,\nthis is kind of awkward,",
    "start": "619460",
    "end": "626570"
  },
  {
    "text": "but essentially we're\ndoing pretty well at getting the information\nout of this page,",
    "start": "626570",
    "end": "631800"
  },
  {
    "text": "and so that's quite amazing. The single biggest\ndevelopment in NLP",
    "start": "631800",
    "end": "639019"
  },
  {
    "text": "for the last year certainly\nin the popular media was GPT-3, which was\na huge new model that",
    "start": "639020",
    "end": "648019"
  },
  {
    "start": "643000",
    "end": "853000"
  },
  {
    "text": "was released by OpenAI. What GPT-3 is about and why it's\ngreat is actually a bit subtle,",
    "start": "648020",
    "end": "655710"
  },
  {
    "text": "and so I can't really go through\nall the details of this here but it's exciting\nbecause it seems",
    "start": "655710",
    "end": "662570"
  },
  {
    "text": "like it's the first step on\nthe path to what we might call universal models,\nwhere you can train up",
    "start": "662570",
    "end": "668570"
  },
  {
    "text": "one extremely large\nmodel on something like, that library picture\nI showed before,",
    "start": "668570",
    "end": "674900"
  },
  {
    "text": "and it just has\nknowledge of the world knowledge of human languages\nknowledge, of how to do tasks.",
    "start": "674900",
    "end": "682050"
  },
  {
    "text": "And then you can apply it\nto do all sorts of things. So no longer are\nwe building a model",
    "start": "682050",
    "end": "687949"
  },
  {
    "text": "to detect spam and then a\nmodel to detect pornography, and then a model to detect\nwhatever foreign language",
    "start": "687950",
    "end": "696290"
  },
  {
    "text": "content, and just building\nall these separate supervised classifiers for\nevery different task, we've now just built up\na model that understands.",
    "start": "696290",
    "end": "704660"
  },
  {
    "text": "So exactly what it does is it\njust predicts following words.",
    "start": "704660",
    "end": "710360"
  },
  {
    "text": "So on the left it's\nbeen told to write",
    "start": "710360",
    "end": "716810"
  },
  {
    "text": "about Elon Musk in the\nstyle of Doctor Seuss,",
    "start": "716810",
    "end": "722120"
  },
  {
    "text": "and it started\noff with some text and then it's\ngenerating more text. And the way it generates more\ntext is literally by just",
    "start": "722120",
    "end": "730970"
  },
  {
    "text": "predicting one word\nat a time, following words to complete its text.",
    "start": "730970",
    "end": "737000"
  },
  {
    "text": "But this has a very\npowerful facility because what you\ncan do with GPT-3",
    "start": "737000",
    "end": "744380"
  },
  {
    "text": "is you can give a\ncouple of examples of what you'd like it to do. So I can give it some text\nand say, I broke the window,",
    "start": "744380",
    "end": "752990"
  },
  {
    "text": "change it into a question,\nwhat did I break? I gracefully saved the day I\nchanged it into a question.",
    "start": "752990",
    "end": "759350"
  },
  {
    "text": "What did I gracefully save? So this prompt tells GPT-3\nwhat I'm wanting it to do.",
    "start": "759350",
    "end": "767280"
  },
  {
    "text": "And so then if I give it\nanother statement like, I gave John flowers. I can then say three,\npredict what words come next.",
    "start": "767280",
    "end": "775220"
  },
  {
    "text": "And it'll follow my\nprompt and produce who did I give flowers to? Or I can say I gave her\na rose and a guitar,",
    "start": "775220",
    "end": "782780"
  },
  {
    "text": "and it will follow the\nidea of the pattern and to who did I give\na rose and a guitar to?",
    "start": "782780",
    "end": "788720"
  },
  {
    "text": "And actually this\none model can then do an amazing range\nof things, including",
    "start": "788720",
    "end": "794000"
  },
  {
    "text": "many that's [AUDIO OUT]\nat all, to give just one example of that. Another thing that\nyou can do is get",
    "start": "794000",
    "end": "802040"
  },
  {
    "text": "it to translate human\nlanguage sentences into SQL. So this can make it\nmuch easier to do CS145.",
    "start": "802040",
    "end": "810560"
  },
  {
    "text": "So having given that a couple\nof examples of SQL translation",
    "start": "810560",
    "end": "816410"
  },
  {
    "text": "of human language text which\nI'm in this time not showing because it won't\nfit on my slide,",
    "start": "816410",
    "end": "821510"
  },
  {
    "text": "I can then give it a\nsentence like how many users have signed up since\nthe start of 2020,",
    "start": "821510",
    "end": "827180"
  },
  {
    "text": "and it turns it into SQL. Or I can give it\nanother query what is the average number\nof influences each user",
    "start": "827180",
    "end": "833900"
  },
  {
    "text": "subscribed to, and again it\nthen converts that into SQL.",
    "start": "833900",
    "end": "840110"
  },
  {
    "text": "So GPT-3 knows a lot about\nthe meaning of language",
    "start": "840110",
    "end": "845180"
  },
  {
    "text": "and the meaning of\nother things like SQL, and can fluently manipulate it.",
    "start": "845180",
    "end": "849920"
  },
  {
    "start": "853000",
    "end": "979000"
  },
  {
    "text": "OK, so that leads us trying\nto this [INAUDIBLE] meaning, and how do we represent\nthe meaning of a word?",
    "start": "853110",
    "end": "860910"
  },
  {
    "text": "Well, what is meaning? Well, we can look up something\nlike the Webster's dictionary and say, OK, the idea that is\nrepresented by a word, the idea",
    "start": "860910",
    "end": "870960"
  },
  {
    "text": "that a person wants\nto express by using words signs, et cetera. That Webster's\ndictionary definition",
    "start": "870960",
    "end": "877740"
  },
  {
    "text": "is really focused on\nthe word idea somehow, but this is pretty close\nto the commonest way that linguists\nthink about meaning.",
    "start": "877740",
    "end": "885180"
  },
  {
    "text": "So do they think of\nword meaning as being a pairing between a word\nwhich is a signifier",
    "start": "885180",
    "end": "892290"
  },
  {
    "text": "or symbol, and the\nthing that it signifies, the signified thing which\nis an idea or thing.",
    "start": "892290",
    "end": "898210"
  },
  {
    "text": "So that the meaning of the\nword chair is the set of things that are chairs.",
    "start": "898210",
    "end": "904529"
  },
  {
    "text": "And that's referred to as\ndenotational semantics. A term that's also used\nand similarly applied",
    "start": "904530",
    "end": "911310"
  },
  {
    "text": "for the semantics of\nprogramming languages. This model isn't very\ndeeply implementable,",
    "start": "911310",
    "end": "919440"
  },
  {
    "text": "like how do I go from\nthe idea that chair means the set of chairs in\nthe world to something",
    "start": "919440",
    "end": "925920"
  },
  {
    "text": "I can manipulate meaning\nwith my computers. So traditionally the way\nthat meaning has normally",
    "start": "925920",
    "end": "934139"
  },
  {
    "text": "been handled in natural\nlanguage processing systems is to make use of\nresources like dictionary,",
    "start": "934140",
    "end": "940620"
  },
  {
    "text": "and thesaurus in particular. Popular ones, WordNet, which\norganized words and terms",
    "start": "940620",
    "end": "947490"
  },
  {
    "text": "into both synonyms sets of words\nthat can mean the same thing,",
    "start": "947490",
    "end": "952680"
  },
  {
    "text": "and hypernyms which correspond\nto ISA relationships. And so for the ISA\nrelationships we",
    "start": "952680",
    "end": "959490"
  },
  {
    "text": "can kind of look at\nthe hypernyms of panda, and panda as a kind\nof procyonidae,",
    "start": "959490",
    "end": "964930"
  },
  {
    "text": "whatever those are, probably\nwith red pandas, which is a kind of carnivore, which\nis a kind of placental, which",
    "start": "964930",
    "end": "973020"
  },
  {
    "text": "is kind a mammal. And you sort of head up\nthis hypernyms hierarchy.",
    "start": "973020",
    "end": "979390"
  },
  {
    "start": "979000",
    "end": "1151000"
  },
  {
    "text": "So word that has been a great\nresource for NLP, that's also been highly deficient.",
    "start": "979390",
    "end": "985480"
  },
  {
    "text": "So it lacks a lot of nuance,\nso for example in WordNet,",
    "start": "985480",
    "end": "990940"
  },
  {
    "text": "proficient is listed\nas a synonym for good. But maybe that's\nsometimes true but it",
    "start": "990940",
    "end": "996580"
  },
  {
    "text": "seems like in a lot of\ncontexts it's not true. And you mean something\nrather different when you say\nproficient versus good,",
    "start": "996580",
    "end": "1003000"
  },
  {
    "text": "it's limited as a human\nconstructed thesaurus. So in particular there's\nlots of words and lots",
    "start": "1003000",
    "end": "1009600"
  },
  {
    "text": "of uses of words that\njust aren't there, including anything that is\nmore current terminology.",
    "start": "1009600",
    "end": "1019770"
  },
  {
    "text": "Like wicked is there\nfor the wicked witch, but not for more\nmodern colloquial uses.",
    "start": "1019770",
    "end": "1026520"
  },
  {
    "text": "Ninja's certainly isn't there\nfor the kind of description some people make of\nprogrammers, and it's",
    "start": "1026520",
    "end": "1033240"
  },
  {
    "text": "impossible to keep up to date. So it requires a\nlot of human labor, but even when you have that,\nit has a set of synonyms",
    "start": "1033240",
    "end": "1043829"
  },
  {
    "text": "but doesn't really have\na good sense of words that means something similar. So fantastic and\ngreat means something",
    "start": "1043829",
    "end": "1053100"
  },
  {
    "text": "similar without\nreally being synonyms. And so this idea of\nmeaning similarity",
    "start": "1053100",
    "end": "1059250"
  },
  {
    "text": "is something that would be\nreally useful to make progress on, and where deep\nlearning models excel.",
    "start": "1059250",
    "end": "1066300"
  },
  {
    "text": "OK, so what's the problem\nwith a lot of traditional NLP? Well the problem with a\nlot of traditional NLP",
    "start": "1066300",
    "end": "1074280"
  },
  {
    "text": "is that words are regarded\nas discrete symbols. So we have symbols\nlike hotel, conference,",
    "start": "1074280",
    "end": "1080730"
  },
  {
    "text": "motel are words,\nwhich in deep learning speak we refer to as a\nlocalized representation.",
    "start": "1080730",
    "end": "1088350"
  },
  {
    "text": "And that's because if you are in\na statistical machine learning",
    "start": "1088350",
    "end": "1094679"
  },
  {
    "text": "systems, want to represent\nthese symbols that each of them is a separate thing so the\nstandard way of representing",
    "start": "1094680",
    "end": "1102600"
  },
  {
    "text": "them and this is what\nyou do in something like a statistical\nmodel if you're building",
    "start": "1102600",
    "end": "1108059"
  },
  {
    "text": "a logistic regression model\nwith words as features is that you represent\nthem as one word vector.",
    "start": "1108060",
    "end": "1115160"
  },
  {
    "text": "So you have a dimension\nfor each different word. So maybe like my example,\nhere are my representations",
    "start": "1115160",
    "end": "1121679"
  },
  {
    "text": "as vectors for motel and hotel. And so that means\nthat we have to have",
    "start": "1121680",
    "end": "1127950"
  },
  {
    "text": "huge vectors corresponding\nto the number of words in our vocabulary. So the kind of if you had a high\nschool English dictionary it",
    "start": "1127950",
    "end": "1135990"
  },
  {
    "text": "probably have about 250,000\nwords in it, but there are many, many more words\nin the language really.",
    "start": "1135990",
    "end": "1142530"
  },
  {
    "text": "So maybe we at least want to\nhave a 500,000 dimensional vector to be able\nto cope with that.",
    "start": "1142530",
    "end": "1150410"
  },
  {
    "text": "OK, but the bigger the\neven bigger problem with discrete symbols is that we\ndon't have this notion of word",
    "start": "1150410",
    "end": "1157970"
  },
  {
    "start": "1151000",
    "end": "1227000"
  },
  {
    "text": "relationships and similarity. So for example in\nweb search, if a user searches for Seattle\nmotel, we'd also",
    "start": "1157970",
    "end": "1165470"
  },
  {
    "text": "like to match on documents\ncontaining Seattle hotel. But our problem is we've\ngot this one word vectors",
    "start": "1165470",
    "end": "1172580"
  },
  {
    "text": "for the different words. And so in a formal\nmathematical sense, these two vectors\nare orthogonal,",
    "start": "1172580",
    "end": "1178710"
  },
  {
    "text": "that there's no natural notion\nof similarity between them whatsoever. Well, there are some things\nthat we could do but try and do",
    "start": "1178710",
    "end": "1186620"
  },
  {
    "text": "about that, and people did\ndo about that in before 2010.",
    "start": "1186620",
    "end": "1192950"
  },
  {
    "text": "We could say, hey we could\nuse the WordNet synonyms and we count things that lists\nof synonyms are similar anyway.",
    "start": "1192950",
    "end": "1199010"
  },
  {
    "text": "Or hey, maybe we\ncould somehow build up our representations of\nwords that have meaning,",
    "start": "1199010",
    "end": "1205040"
  },
  {
    "text": "overlap and people did\nall of those things. But they tended to fail\nbadly from incompleteness,",
    "start": "1205040",
    "end": "1211580"
  },
  {
    "text": "so instead what I\nwant to introduce today is the modern deep\nlearning method of doing that,",
    "start": "1211580",
    "end": "1218090"
  },
  {
    "text": "where we encode similarity in\nreal value vector themselves.",
    "start": "1218090",
    "end": "1224549"
  },
  {
    "text": "So how do we go\nabout doing that? And the way we do that is\nby exploiting this idea",
    "start": "1224550",
    "end": "1231740"
  },
  {
    "start": "1227000",
    "end": "1413000"
  },
  {
    "text": "called distributional semantics. So the idea of distributional\nsemantics is again something",
    "start": "1231740",
    "end": "1239720"
  },
  {
    "text": "that when you first see it,\nmaybe feels a little bit crazy because rather\nthan having something",
    "start": "1239720",
    "end": "1246350"
  },
  {
    "text": "like denotational semantics,\nwhat we're now going to do is say that a word's\nmeaning is going",
    "start": "1246350",
    "end": "1253279"
  },
  {
    "text": "to be given by the words that\nfrequently appear close to it.",
    "start": "1253280",
    "end": "1259010"
  },
  {
    "text": "J. R Firth was a\nBritish linguist from the middle of last century,\nand one of his pithy slogans",
    "start": "1259010",
    "end": "1266630"
  },
  {
    "text": "that everyone quotes\nat this moment is, you shall know a word\nby the company it keeps.",
    "start": "1266630",
    "end": "1273050"
  },
  {
    "text": "And so this idea that you can\nrepresent a sense for words, meaning as a notion of what\ncontext that appears in",
    "start": "1273050",
    "end": "1282950"
  },
  {
    "text": "has been a very successful idea. One of the most\nsuccessful ideas that's",
    "start": "1282950",
    "end": "1289220"
  },
  {
    "text": "used throughout statistical\nand deep learning NLP. It's actually an interesting\nidea more philosophically,",
    "start": "1289220",
    "end": "1297800"
  },
  {
    "text": "so that there are kind of\ninteresting connections. For example in Wittgenstein's\nlater writings,",
    "start": "1297800",
    "end": "1304309"
  },
  {
    "text": "he became enamored of a\nused theory of meaning. And this is a sin in\nsome sense, a used theory",
    "start": "1304310",
    "end": "1309980"
  },
  {
    "text": "of meaning but whether it's the\nultimate theory of semantics, it's actually still\npretty controversial.",
    "start": "1309980",
    "end": "1316040"
  },
  {
    "text": "But it proves to be an\nextremely computational sense of semantics, which\nhas just led to it",
    "start": "1316040",
    "end": "1322160"
  },
  {
    "text": "being used everywhere very\nsuccessfully in deep learning systems.",
    "start": "1322160",
    "end": "1327930"
  },
  {
    "text": "So when a word\nappears in a text, it has a context which\nare a set of words",
    "start": "1327930",
    "end": "1333860"
  },
  {
    "text": "that appear in the eye. And so for a particular word,\nmy example here is banking.",
    "start": "1333860",
    "end": "1340310"
  },
  {
    "text": "We'll find a bunch of places\nwhere banking occurs in text,",
    "start": "1340310",
    "end": "1345890"
  },
  {
    "text": "and will collect the\nsort of nearby words that context words\nand we'll see and say",
    "start": "1345890",
    "end": "1351290"
  },
  {
    "text": "that those words\nthat are appearing in that kind of\nmuddy brown color around banking, that\nthose contexts words",
    "start": "1351290",
    "end": "1358010"
  },
  {
    "text": "will in some sense represent\nthe meaning of the word banking.",
    "start": "1358010",
    "end": "1363740"
  },
  {
    "text": "While I'm here let me just\nmention one distinction that will come up regularly. When we're talking about a\nword in our natural language",
    "start": "1363740",
    "end": "1372260"
  },
  {
    "text": "processing class, we sort\nof have two senses of word",
    "start": "1372260",
    "end": "1377270"
  },
  {
    "text": "which are referred to\nas types and tokens. So there's a particular\ninstance for words.",
    "start": "1377270",
    "end": "1383910"
  },
  {
    "text": "So there's in the first example,\ngovernment debt problems turning into banking crises.",
    "start": "1383910",
    "end": "1389330"
  },
  {
    "text": "There's banking\nthere, and that's a token of the word banking. But then I've collected a\nbunch of instances of quote",
    "start": "1389330",
    "end": "1397760"
  },
  {
    "text": "unquote, the word banking. And when I say the word banking\nand a bunch of examples of it,",
    "start": "1397760",
    "end": "1403340"
  },
  {
    "text": "I'm then treating banking as a\ntype which refers to the uses and meaning the word banking\nhas across instances.",
    "start": "1403340",
    "end": "1413380"
  },
  {
    "start": "1413000",
    "end": "1651000"
  },
  {
    "text": "OK, so what are we going to do\nwith this distributional models",
    "start": "1413380",
    "end": "1421300"
  },
  {
    "text": "of language? Well, what we want\nto do is based",
    "start": "1421300",
    "end": "1426790"
  },
  {
    "text": "on looking at the words that\noccur in context as vectors that we want to build\nup dense real valued",
    "start": "1426790",
    "end": "1436360"
  },
  {
    "text": "vector for each word,\nthat in some sense represents the\nmeaning of that word.",
    "start": "1436360",
    "end": "1443590"
  },
  {
    "text": "And the way it will represent\nthe meaning of that word is that this vector will\nbe useful for predicting",
    "start": "1443590",
    "end": "1452049"
  },
  {
    "text": "other words that\noccur in the context.",
    "start": "1452050",
    "end": "1454670"
  },
  {
    "text": "So in this example, to keep it\nmanageable on the slide vectors are only eight dimensional.",
    "start": "1457460",
    "end": "1464450"
  },
  {
    "text": "But in reality we use\nconsiderably bigger vectors, so a very common\nsize is actually",
    "start": "1464450",
    "end": "1469690"
  },
  {
    "text": "300 dimensional vectors. OK, so for each word\nthat's a word type,",
    "start": "1469690",
    "end": "1475990"
  },
  {
    "text": "we don't have a word vector. These are also used\nwith other names,",
    "start": "1475990",
    "end": "1482590"
  },
  {
    "text": "they refer to as neural\nword representations or for a reason, they'll become\nclearer on the next slide,",
    "start": "1482590",
    "end": "1488830"
  },
  {
    "text": "they're referred to\nas word embeddings. So these are now a distributed\nrepresentation, not a localized",
    "start": "1488830",
    "end": "1494890"
  },
  {
    "text": "representation because the\nmeaning of the word banking is spread over all 300\ndimensions of the vector.",
    "start": "1494890",
    "end": "1504780"
  },
  {
    "text": "These are called word embeddings\nbecause effectively when we have a whole bunch of words,\nthese representations place",
    "start": "1504780",
    "end": "1513390"
  },
  {
    "text": "them all in a high\ndimensional vector space, and so they're embedded\ninto that space.",
    "start": "1513390",
    "end": "1519690"
  },
  {
    "text": "Now unfortunately\nhuman beings are very bad at looking at\n300 dimensional vector",
    "start": "1519690",
    "end": "1526800"
  },
  {
    "text": "spaces or even eight\ndimensional vector spaces, so the only thing that I\ncan really display to you",
    "start": "1526800",
    "end": "1532710"
  },
  {
    "text": "here is a two dimensional\nprojection of that space. Now even that's\nuseful, but it's also",
    "start": "1532710",
    "end": "1539538"
  },
  {
    "text": "important to realize\nthat when you're making it two dimensional\nprojection of a three dimensional space,\nyou're losing almost all",
    "start": "1539538",
    "end": "1547260"
  },
  {
    "text": "the information in that space. And a lot of things will\nbe crushed together, that don't actually\ndeserve to be better.",
    "start": "1547260",
    "end": "1554080"
  },
  {
    "text": "So here's my word\nembeddings, of course you can't see any of those at all.",
    "start": "1554080",
    "end": "1561280"
  },
  {
    "text": "But if I zoom in and\nthen I zoom in further, what you'll already see is\nthat the representations",
    "start": "1561280",
    "end": "1568860"
  },
  {
    "text": "we've learnt distributionally\nto just a good job at grouping together similar words.",
    "start": "1568860",
    "end": "1577570"
  },
  {
    "text": "So in this sort of\noverall picture, I can zoom into one\npart of the space, is actually the part that's\nup here and this view of it.",
    "start": "1577570",
    "end": "1586650"
  },
  {
    "text": "And it's got words\nfor countries, so not only are countries\ngenerally grouped together,",
    "start": "1586650",
    "end": "1593760"
  },
  {
    "text": "even the sort of particular\nsub groupings of countries make a certain amount of sense.",
    "start": "1593760",
    "end": "1600060"
  },
  {
    "text": "And down here we then\nhave nationality words, if we go to another\npart of the space we can",
    "start": "1600060",
    "end": "1605550"
  },
  {
    "text": "see different kind of words. So here are verbs and we\nhave ones like come and go,",
    "start": "1605550",
    "end": "1611250"
  },
  {
    "text": "a very similar saying\nand thinking words, say think expect, they\nkind of similar and nearby.",
    "start": "1611250",
    "end": "1620700"
  },
  {
    "text": "Over on the bottom right, we\nhave sort of verbal exhilarates and corpus, so have, had,\nhas, forms of the verb to be.",
    "start": "1620700",
    "end": "1628710"
  },
  {
    "text": "And certain content for verbs\nare similar to corpus verbs, because they describe states.",
    "start": "1628710",
    "end": "1635490"
  },
  {
    "text": "He remained angry,\nhe became angry, and so they're actually\nthen grouped close together",
    "start": "1635490",
    "end": "1641130"
  },
  {
    "text": "to the word, the verb to be. So there's a lot of\ninteresting structure",
    "start": "1641130",
    "end": "1646320"
  },
  {
    "text": "in this space that\nthen represents the meaning of words. So the algorithm I'm\ngoing to introduce now",
    "start": "1646320",
    "end": "1654720"
  },
  {
    "start": "1651000",
    "end": "2275000"
  },
  {
    "text": "is one that's called\nword2vec, which was introduced by Tomas Mikolov and\ncolleagues in 2013",
    "start": "1654720",
    "end": "1662940"
  },
  {
    "text": "as a framework for\nlearning word vectors, and it's sort of\na simple and easy to understand place to start.",
    "start": "1662940",
    "end": "1668580"
  },
  {
    "text": "So the idea is we\nhave a lot of text from somewhere, which we\ncommonly refer to as a corpus.",
    "start": "1668580",
    "end": "1676500"
  },
  {
    "text": "Of text corpus as just\nthe Latin word for body, so it's a body of text.",
    "start": "1676500",
    "end": "1682300"
  },
  {
    "text": "And so then we choose\na fixed vocabulary which will typically be large\nbut nevertheless truncated,",
    "start": "1682300",
    "end": "1688930"
  },
  {
    "text": "so we get rid of some of\nthe really rare words, so we might say\nvocabulary size of 400,000",
    "start": "1688930",
    "end": "1695190"
  },
  {
    "text": "and we then create for\nourselves vector for each word.",
    "start": "1695190",
    "end": "1701730"
  },
  {
    "text": "OK, so then what\nwe do is we want to work out what's a good\nvector for each word,",
    "start": "1701730",
    "end": "1710130"
  },
  {
    "text": "and the really\ninteresting thing is that we can learn these word\nvectors from just a big pile",
    "start": "1710130",
    "end": "1717090"
  },
  {
    "text": "of text by doing this\ndistributional similarity task of being able to predict,\nwell, what words occur",
    "start": "1717090",
    "end": "1725460"
  },
  {
    "text": "in the context of other words. So in particular,\nwe're going [AUDIO OUT]",
    "start": "1725460",
    "end": "1731130"
  },
  {
    "text": "through in the texts,\nand so at any moment we have a center word C, and\ncontext words outside of it,",
    "start": "1731130",
    "end": "1739930"
  },
  {
    "text": "which we'll call O. And then\nbased on the current word vectors, we're\ngoing to calculate",
    "start": "1739930",
    "end": "1747390"
  },
  {
    "text": "the probability of a\ncontext word occurring, given the center word\naccording to our current model.",
    "start": "1747390",
    "end": "1756269"
  },
  {
    "text": "But then we know that\ncertain words did actually occur in the context\nof that center word,",
    "start": "1756270",
    "end": "1762360"
  },
  {
    "text": "and so what we\nwant to do is then keep adjusting the word\nvectors to maximize",
    "start": "1762360",
    "end": "1767669"
  },
  {
    "text": "the probability that's assigned\nto words that actually occur in the context of\nthe center word",
    "start": "1767670",
    "end": "1774660"
  },
  {
    "text": "as we proceed\nthrough these texts. So to start to make that\na bit more concrete,",
    "start": "1774660",
    "end": "1779700"
  },
  {
    "text": "this is what we're doing. So we have a piece of text, we\nchoose our center word which",
    "start": "1779700",
    "end": "1786900"
  },
  {
    "text": "is here in two and\nthen we say, well,",
    "start": "1786900",
    "end": "1792690"
  },
  {
    "text": "if a model of predicting the\nprobability of context words given the center\nword and this model,",
    "start": "1792690",
    "end": "1799120"
  },
  {
    "text": "we'll come to in a\nminute, but it's defined in terms of our word vectors. So let's see what\nprobability it gives",
    "start": "1799120",
    "end": "1806529"
  },
  {
    "text": "to the words that\nactually occurred in the context of this word.",
    "start": "1806530",
    "end": "1812470"
  },
  {
    "text": "It gives them some\nprobability, but maybe it'd be nice if the probability\nit assigned was higher.",
    "start": "1812470",
    "end": "1818150"
  },
  {
    "text": "So then how can we\nchange our word vectors to raise those probabilities?",
    "start": "1818150",
    "end": "1823419"
  },
  {
    "text": "And so we'll do\nsome calculations with into being the\ncenter word, and then",
    "start": "1823420",
    "end": "1828520"
  },
  {
    "text": "we'll just go on\nto the next word and then we'll do the\nsame kind of calculations,",
    "start": "1828520",
    "end": "1833590"
  },
  {
    "text": "and keep on chunking. So the big question\nthen is, well what are we doing\nfor working out",
    "start": "1833590",
    "end": "1840220"
  },
  {
    "text": "the probability of\na word occurring in the context of\nthe center word?",
    "start": "1840220",
    "end": "1846100"
  },
  {
    "text": "And so that's the\ncentral part of what we develop as the word2vec object.",
    "start": "1846100",
    "end": "1854380"
  },
  {
    "text": "So this is the overall\nmodel that we want to use. So for each position in our\ncorpus, our body of text,",
    "start": "1854380",
    "end": "1862559"
  },
  {
    "text": "we want to predict context words\nwithin a window of fixed size M, given the center word WJ.",
    "start": "1862560",
    "end": "1870029"
  },
  {
    "text": "And we want to become\ngood at doing that, so we want to give high\nprobability to words",
    "start": "1870030",
    "end": "1875460"
  },
  {
    "text": "that occur in the context. And so what we're\ngoing to do is we're going to work out\nwhat's formerly",
    "start": "1875460",
    "end": "1881490"
  },
  {
    "text": "the data likelihood\nas to how good a job we do at predicting words in\nthe context of other words.",
    "start": "1881490",
    "end": "1889090"
  },
  {
    "text": "And so formally\nthat likelihood is going to be defined in\nterms of our word vectors.",
    "start": "1889090",
    "end": "1894880"
  },
  {
    "text": "So they're the\nparameters of our model, and it's going to be calculated\nas taking the product of using",
    "start": "1894880",
    "end": "1901830"
  },
  {
    "text": "each word as the\ncenter word, and then the product of each\nword and a window around that of the probability\nof predicting that context",
    "start": "1901830",
    "end": "1910350"
  },
  {
    "text": "word in the center word. And so to learn\nthis model, we're",
    "start": "1910350",
    "end": "1916080"
  },
  {
    "text": "going to have an objective\nfunction, sometimes also called a cost or a loss\nthat we want to optimize.",
    "start": "1916080",
    "end": "1922110"
  },
  {
    "text": "And essentially\nwhat we want to do is we want to maximize the\nlikelihood of the context we",
    "start": "1922110",
    "end": "1928650"
  },
  {
    "text": "see around center words. But following standard\npractice, we slightly",
    "start": "1928650",
    "end": "1933870"
  },
  {
    "text": "fiddle that because rather\nthan dealing with products, it's easier to deal\nwith sums and so we",
    "start": "1933870",
    "end": "1941220"
  },
  {
    "text": "work with log likelihood. And once we take log\nlikelihood, all of our products turn into sums.",
    "start": "1941220",
    "end": "1947700"
  },
  {
    "text": "We also work with the\naverage log likelihood, so we've got one on, t term\nhere for the number of words",
    "start": "1947700",
    "end": "1954929"
  },
  {
    "text": "in the corpus, and finally\nfor no particular reason we like to minimize our\nobjective function rather",
    "start": "1954930",
    "end": "1962039"
  },
  {
    "text": "than maximizing it. So we stick a minus\nsign in there, and so then by minimizing this\nobjective function, J of theta,",
    "start": "1962040",
    "end": "1970470"
  },
  {
    "text": "that becomes maximizing\nour predictive accuracy.",
    "start": "1970470",
    "end": "1976630"
  },
  {
    "text": "OK, so that's the\nset up, but we still haven't made any\nprogress in how do we",
    "start": "1976630",
    "end": "1984100"
  },
  {
    "text": "calculate the probability of a\nword occurring in the context, given the center word.",
    "start": "1984100",
    "end": "1989650"
  },
  {
    "text": "And so the way\nwe're actually going to do that is we have vector\nrepresentations for each word,",
    "start": "1989650",
    "end": "1998470"
  },
  {
    "text": "and we're going to work\nout the probability simply in terms of the word vectors.",
    "start": "1998470",
    "end": "2004470"
  },
  {
    "text": "Now at this point there's\na little technical point, we're actually going to give\nto each word, two word vectors.",
    "start": "2004470",
    "end": "2011580"
  },
  {
    "text": "One word vector for when\nit's used as the center word, and a\ndifferent word vector",
    "start": "2011580",
    "end": "2017250"
  },
  {
    "text": "when that's used\nas a context word. This is done because\nit just simplifies",
    "start": "2017250",
    "end": "2022950"
  },
  {
    "text": "the math and the optimization,\nthat seems a little bit ugly that actually makes building\nword vectors a lot easier,",
    "start": "2022950",
    "end": "2031110"
  },
  {
    "text": "and really we can come back\nto that and discuss it later. But that's what it is.",
    "start": "2031110",
    "end": "2036929"
  },
  {
    "text": "And so then once we\nhave these word vectors, the equation that\nwe're going to use",
    "start": "2036930",
    "end": "2044549"
  },
  {
    "text": "for giving the probability of\na context word appearing given the center word is\nthat we're going",
    "start": "2044550",
    "end": "2050730"
  },
  {
    "text": "to calculate it using the\nexpression, the middle bottom of my slide.",
    "start": "2050730",
    "end": "2055869"
  },
  {
    "text": "So let's sort of pull that\napart just a little bit more.",
    "start": "2055870",
    "end": "2063850"
  },
  {
    "text": "So what we have here\nwith this expression is so for a particular center word\nand a particular context word",
    "start": "2063850",
    "end": "2073699"
  },
  {
    "text": "O, we're going to\nlook up the vector representation of each word. So there U of O and V\nof C, and so then we're",
    "start": "2073699",
    "end": "2081840"
  },
  {
    "text": "simply going to take the dot\nproduct of those two vectors. So dot product is\na natural measure",
    "start": "2081840",
    "end": "2088469"
  },
  {
    "text": "for similarity\nbetween words because in any particular\nmention of opposite,",
    "start": "2088469",
    "end": "2095669"
  },
  {
    "text": "you'll get a component that\nadds to that dot product sum. If both are negative, it'll add\na lot to the dot product sum.",
    "start": "2095670",
    "end": "2102690"
  },
  {
    "text": "If one's positive\nand one's negative, it'll subtract from\nthe similarity measure.",
    "start": "2102690",
    "end": "2109440"
  },
  {
    "text": "Both of them are zero,\nwon't change the similarity. So it sort of seems\nsort of plausible idea",
    "start": "2109440",
    "end": "2115170"
  },
  {
    "text": "to just take a dot product\nand thinking, well, if two words have a\nlarger dot product, that",
    "start": "2115170",
    "end": "2121079"
  },
  {
    "text": "means they're more similar. And so then after that,\nwe sort of really doing",
    "start": "2121080",
    "end": "2127890"
  },
  {
    "text": "nothing more than OK, we\nwant to use dot products to represent words similarity.",
    "start": "2127890",
    "end": "2133619"
  },
  {
    "text": "And now let's do the\ndumbest thing that we know, how to turn this into a\nprobability distribution.",
    "start": "2133620",
    "end": "2140830"
  },
  {
    "text": "Well, what do we do? Well firstly, well\ntaking a dot product of two vectors\nthat might come out",
    "start": "2140830",
    "end": "2147750"
  },
  {
    "text": "as positive or\nnegative, but well we want to have probabilities. We can't have negative\nprobabilities.",
    "start": "2147750",
    "end": "2154330"
  },
  {
    "text": "So a simple way to avoid\nnegative probabilities is to exponentiate\nthem, because then we",
    "start": "2154330",
    "end": "2159660"
  },
  {
    "text": "know everything is positive. And so then we are always\ngetting a positive number in the numerator, but\nfor probabilities we",
    "start": "2159660",
    "end": "2168420"
  },
  {
    "text": "also want to have the\nnumbers add up to 1. So we have a probability\ndistribution.",
    "start": "2168420",
    "end": "2173740"
  },
  {
    "text": "So we just normalizing\nthe obvious way, where we divide through by the\nsum of the numerator quantity",
    "start": "2173740",
    "end": "2181350"
  },
  {
    "text": "for each different\nword in the vocabulary, and so then necessarily\nthat gives us a probability distribution.",
    "start": "2181350",
    "end": "2188799"
  },
  {
    "text": "So all the rest\nof that I was just talking through what\nwe're using there is what's called the\nsoftmax function.",
    "start": "2188800",
    "end": "2195550"
  },
  {
    "text": "So the softmax function\nwill take any R in vector",
    "start": "2195550",
    "end": "2200580"
  },
  {
    "text": "and turn it into\nthings between 0 to 1.",
    "start": "2200580",
    "end": "2206080"
  },
  {
    "text": "And so we can take numbers and\nput them through the softmax, and turn them into\nthe redistribution.",
    "start": "2206080",
    "end": "2213290"
  },
  {
    "text": "So the name comes from the fact\nthat it's sort of like a max. So because of the fact\nthat we exponentiate,",
    "start": "2213290",
    "end": "2220420"
  },
  {
    "text": "that really emphasizes\nthe big contents in the different dimensions\nof calculating similarity.",
    "start": "2220420",
    "end": "2229360"
  },
  {
    "text": "So most of the probability goes\nto the most similar things,",
    "start": "2229360",
    "end": "2234670"
  },
  {
    "text": "and it's called soft\nbecause well, it doesn't do that absolutely. It'll still give\nsome probability",
    "start": "2234670",
    "end": "2241720"
  },
  {
    "text": "to everything that's in\nthe slightest bit similar-- I mean, on the other hand it's a\nslightly weird name because max",
    "start": "2241720",
    "end": "2249849"
  },
  {
    "text": "normally takes a set of\nthings and just returns one the biggest of them,\nwhereas the softmax is",
    "start": "2249850",
    "end": "2257470"
  },
  {
    "text": "taking a set of numbers\nand scaling them, that is returning the whole\nprobability distribution.",
    "start": "2257470",
    "end": "2265609"
  },
  {
    "text": "OK, so now we have all\nthe pieces of our model. And so how do we make\nour word vectors?",
    "start": "2265610",
    "end": "2274400"
  },
  {
    "text": "Well, the idea of what\nwe want to do is we want to fiddle our word vectors\nin such a way that we minimize",
    "start": "2274400",
    "end": "2284029"
  },
  {
    "start": "2275000",
    "end": "2395000"
  },
  {
    "text": "our loss, i.e. that we maximize\nthe probability of the words we actually saw in the\ncontext of the center word.",
    "start": "2284030",
    "end": "2292040"
  },
  {
    "text": "And so the theta represents all\nof our model parameters in one",
    "start": "2292040",
    "end": "2298700"
  },
  {
    "text": "very long vector. So for our model here,\nthe only parameters are our word vectors.",
    "start": "2298700",
    "end": "2305400"
  },
  {
    "text": "So we have for each\nword two vectors, its context vector\nand its center vector.",
    "start": "2305400",
    "end": "2313160"
  },
  {
    "text": "And each of those is a\nD dimensional vector, where D might be 300 and\nwe have V many words.",
    "start": "2313160",
    "end": "2320720"
  },
  {
    "text": "So we end up with this big\nhuge vector which is 2DV long,",
    "start": "2320720",
    "end": "2326660"
  },
  {
    "text": "which if you have a 500,000\nvocab times the 300 dimensional",
    "start": "2326660",
    "end": "2331700"
  },
  {
    "text": "at the time, it's more math\nthan I can do in my head, but it's got millions of\nmillions of parameters,",
    "start": "2331700",
    "end": "2337720"
  },
  {
    "text": "it's got millions and\nmillions of parameters. And we somehow want\nto fiddle them all",
    "start": "2337720",
    "end": "2343550"
  },
  {
    "text": "to maximize the prediction\nof context words. And so the way we kind of do\nthat then is we use calculus.",
    "start": "2343550",
    "end": "2353609"
  },
  {
    "text": "So if what we want to do is\ntake that math that we've seen previously and say, well\nwith this objective function,",
    "start": "2353610",
    "end": "2363050"
  },
  {
    "text": "we can work out\nderivatives and so we can work out where\nthe gradient is.",
    "start": "2363050",
    "end": "2370230"
  },
  {
    "text": "So how we can walk\ndownhill to minimize loss. So we're at some point\nand we can figure out",
    "start": "2370230",
    "end": "2376970"
  },
  {
    "text": "what is downhill, and we\ncan then progressively walk downhill and\nimprove our model.",
    "start": "2376970",
    "end": "2385890"
  },
  {
    "text": "And so what our\njob is going to be is to compute all of\nthose vector gradients.",
    "start": "2385890",
    "end": "2392550"
  },
  {
    "text": "OK, so at this point I\nthen want to kind of show",
    "start": "2392550",
    "end": "2398570"
  },
  {
    "start": "2395000",
    "end": "2470000"
  },
  {
    "text": "a little bit more as to how\nwe can actually do that.",
    "start": "2398570",
    "end": "2405300"
  },
  {
    "text": "And then a couple\nmore slides here but maybe I'll just try and\njigger things again and move",
    "start": "2405300",
    "end": "2413480"
  },
  {
    "text": "to my interactive whiteboard. What we wanted to do,\nso we had our overall--",
    "start": "2413480",
    "end": "2421550"
  },
  {
    "text": "we had our overall\nJ theta that we were wanting to minimize our\naverage neg log likelihood,",
    "start": "2424100",
    "end": "2431900"
  },
  {
    "text": "so that was the minus 1 on T of\nthe sum of T equals 1 to big T,",
    "start": "2431900",
    "end": "2439099"
  },
  {
    "text": "which was our text length. And then we were going through\nthe words in each context, so we were doing\nJ between M words",
    "start": "2439100",
    "end": "2448160"
  },
  {
    "text": "on each side except itself. And then what we wanted to\ndo was in the side there,",
    "start": "2448160",
    "end": "2456080"
  },
  {
    "text": "we were then working out the log\nprobability of the context word",
    "start": "2456080",
    "end": "2462260"
  },
  {
    "text": "at that position given the word\nthat's in the center position T",
    "start": "2462260",
    "end": "2468485"
  },
  {
    "text": ". And so then we converted\nthat into our word vectors",
    "start": "2468485",
    "end": "2477060"
  },
  {
    "start": "2470000",
    "end": "2930000"
  },
  {
    "text": "by saying that the\nprobability of O given C",
    "start": "2477060",
    "end": "2482550"
  },
  {
    "text": "is going to be expressed as the\nnoun, this softmax of the dot",
    "start": "2482550",
    "end": "2487870"
  },
  {
    "text": "product.",
    "start": "2487870",
    "end": "2488370"
  },
  {
    "text": "And so now what we\nwant to do is work out",
    "start": "2504700",
    "end": "2511240"
  },
  {
    "text": "the gradient, the direction of\ndownhill for this [INAUDIBLE]..",
    "start": "2511240",
    "end": "2518990"
  },
  {
    "text": "And so the way\nwe're doing that is we're working out the partial\nderivative of this expression",
    "start": "2518990",
    "end": "2525820"
  },
  {
    "text": "with respect to every\nparameter in the model,",
    "start": "2525820",
    "end": "2531750"
  },
  {
    "text": "and all the parameters\nin the model are the components, the\ndimensions of the word",
    "start": "2531750",
    "end": "2537780"
  },
  {
    "text": "vectors of every word. And so we have the center word\nvectors and the outside word",
    "start": "2537780",
    "end": "2545040"
  },
  {
    "text": "vectors. So here I'm just going to\ndo the center word vectors",
    "start": "2545040",
    "end": "2553320"
  },
  {
    "text": "but on homework-- on a future\nhomework assignment two, the outside word\nvectors will show up",
    "start": "2553320",
    "end": "2559620"
  },
  {
    "text": "and they're kind of similar. So what we're doing\nis we're working out the partial derivative\nwith respect",
    "start": "2559620",
    "end": "2568020"
  },
  {
    "text": "to our center word vector, which\nis maybe a 300 dimensional word vector of this\nprobability of O give C.",
    "start": "2568020",
    "end": "2580140"
  },
  {
    "text": "And since we're using\nlog probabilities of the log of this probability\nof O given C of this exp",
    "start": "2580140",
    "end": "2586290"
  },
  {
    "text": "of your U of OTVC, over-- my writing will get\nworse and worse, sorry.",
    "start": "2586290",
    "end": "2594115"
  },
  {
    "text": "I've already made a\nmistake, haven't I? Sum, the sum of W equals\n1 to the vocabulary",
    "start": "2594115",
    "end": "2600310"
  },
  {
    "text": "of the exp of UWTVC. OK, well at this point\nthings start off pretty easy.",
    "start": "2600310",
    "end": "2611170"
  },
  {
    "text": "So what we have\nhere is something that's log of A over\nB, so that's easy.",
    "start": "2611170",
    "end": "2617370"
  },
  {
    "text": "We can turn this into\nlog A minus log B. But before I go\nfurther I'll just",
    "start": "2617370",
    "end": "2622859"
  },
  {
    "text": "make a comment at this point. So at this point my\naudience divides on in two,",
    "start": "2622860",
    "end": "2632510"
  },
  {
    "text": "there are some people\nin the audience for which maybe a lot of\npeople in the audience--",
    "start": "2632510",
    "end": "2639349"
  },
  {
    "text": "this really elementary math,\nI've seen this a million times before and he isn't even\nexplaining it very well.",
    "start": "2639350",
    "end": "2646650"
  },
  {
    "text": "And if you're in\nthat group, well feel free to look at your\nemail or the newspaper",
    "start": "2646650",
    "end": "2652099"
  },
  {
    "text": "or whatever else is\nbest suited to you, but I think there are also\nother people in the class who",
    "start": "2652100",
    "end": "2659359"
  },
  {
    "text": "are, the last time\nI saw calculus was when I was in high school,\nfor which that's not the case.",
    "start": "2659360",
    "end": "2665430"
  },
  {
    "text": "And so I wanted to spend a\nfew minutes going through this a bit concretely so that to\ntry and get over the idea",
    "start": "2665430",
    "end": "2673490"
  },
  {
    "text": "that even though most of deep\nlearning and even word vector",
    "start": "2673490",
    "end": "2679410"
  },
  {
    "text": "learning seems like magic,\nthat it's not really magic.",
    "start": "2679410",
    "end": "2685190"
  },
  {
    "text": "It's really just doing\nmath and one of the things we hope is that you\ndo actually understand",
    "start": "2685190",
    "end": "2690740"
  },
  {
    "text": "this math that's being done. So I'll keep along and\ndo a bit more of it,",
    "start": "2690740",
    "end": "2696170"
  },
  {
    "text": "so then what we have it's use\nthis way of writing the log.",
    "start": "2696170",
    "end": "2703170"
  },
  {
    "text": "And so then we can say\nthat that expression above equals the partial\nderivatives of VC",
    "start": "2703170",
    "end": "2712349"
  },
  {
    "text": "of the log of the\nnumerator log XUOTVC",
    "start": "2712350",
    "end": "2720960"
  },
  {
    "text": "minus the partial derivative\nof the log of the denominator.",
    "start": "2720960",
    "end": "2734099"
  },
  {
    "text": "So that's then the\nsum of W equals 1 to V of the X of UWTVC.",
    "start": "2734100",
    "end": "2740890"
  },
  {
    "text": "So at that point, I\nhave my numerator here",
    "start": "2746000",
    "end": "2753140"
  },
  {
    "text": "and my former denominator there. So at that point there is that's\nthe first part is the numerator",
    "start": "2753140",
    "end": "2762950"
  },
  {
    "text": "part, so the numerator part\nis really, really easy, so we have here that log on X\njust inverses of each other.",
    "start": "2762950",
    "end": "2774160"
  },
  {
    "text": "So they'd just go away. So that becomes the\nderivative with respect",
    "start": "2774160",
    "end": "2782130"
  },
  {
    "text": "to VC of just\nwhat's left behind,",
    "start": "2782130",
    "end": "2788890"
  },
  {
    "text": "which is U0 dot product with VC. OK, and so the\nthing to be aware of",
    "start": "2788890",
    "end": "2797400"
  },
  {
    "text": "is we're still doing this\nmultivariate calculus. So what we have here is calculus\nwith respect to a vector,",
    "start": "2797400",
    "end": "2805559"
  },
  {
    "text": "like hopefully you saw\nsome of the math 51 or some other place,\nnot high school.",
    "start": "2805560",
    "end": "2812130"
  },
  {
    "text": "Single variable calculus. On the other hand\nto the extent, you",
    "start": "2812130",
    "end": "2819075"
  },
  {
    "text": "can't remember\nsome of this stuff. Most of the time you can\njustify it perfectly well",
    "start": "2819075",
    "end": "2824550"
  },
  {
    "text": "by thinking about what happens\nwith one dimension at a time,",
    "start": "2824550",
    "end": "2830400"
  },
  {
    "text": "and it generalizes to\nmultivariable calculus. So if about all that\nyou remember of calculus",
    "start": "2830400",
    "end": "2836970"
  },
  {
    "text": "is that d dx of ax\nequals a, really it's",
    "start": "2836970",
    "end": "2843300"
  },
  {
    "text": "the same thing that we're\ngoing to be using here. That here we have the outside\nword dot producted with the VC.",
    "start": "2843300",
    "end": "2858550"
  },
  {
    "text": "Well at the end\nof the day, that's going to have terms of sort\nof U0 component 1 times",
    "start": "2858550",
    "end": "2867370"
  },
  {
    "text": "the center word component\n1 plus U0 component 2 plus.",
    "start": "2867370",
    "end": "2874900"
  },
  {
    "text": "There were component\n2, and so we're sort of using this bit over here.",
    "start": "2878420",
    "end": "2883710"
  },
  {
    "text": "And so what we're\ngoing to be getting out is the U0, and U01,\nand the U02, so this",
    "start": "2883710",
    "end": "2892070"
  },
  {
    "text": "will be all that is left\nwith respect to VC1. When we take its derivative\nwith respect to VC1",
    "start": "2892070",
    "end": "2898339"
  },
  {
    "text": "and this term will\nbe the only thing left when we take the\nderivative with respect to the variable VC2.",
    "start": "2898340",
    "end": "2905570"
  },
  {
    "text": "So the end result of taking\nthe vector antiderivative",
    "start": "2905570",
    "end": "2911760"
  },
  {
    "text": "of U0 dot producted with VC\nis simply going to be U0.",
    "start": "2911760",
    "end": "2920670"
  },
  {
    "text": "OK great, so that's progress. So then at that point we go on\nand we say oh damn, we still",
    "start": "2920670",
    "end": "2931680"
  },
  {
    "start": "2930000",
    "end": "5067000"
  },
  {
    "text": "have the denominator [AUDIO OUT]\nslightly more complex, but not",
    "start": "2931680",
    "end": "2941670"
  },
  {
    "text": "so bad. So then we want to take\nthe partial derivatives with respect to VC of the\nlog of the denominator.",
    "start": "2941670",
    "end": "2949530"
  },
  {
    "text": "OK and so then at this\npoint, the one tool",
    "start": "2958590",
    "end": "2963600"
  },
  {
    "text": "that we need to\nknow and remember is how to use the chain rule.",
    "start": "2963600",
    "end": "2968650"
  },
  {
    "text": "So the chain rule is\nwhen you're wanting to work out having derivatives\nof compositions of function.",
    "start": "2968650",
    "end": "2979309"
  },
  {
    "text": "So we have F of G of whatever\nX, but here it's going to be VC.",
    "start": "2979310",
    "end": "2986470"
  },
  {
    "text": "And so we want to say,\nOK what we have here is we're working out a\ncomposition of functions.",
    "start": "2986470",
    "end": "2993920"
  },
  {
    "text": "So here's our F and here\nis our X, which is G of VC.",
    "start": "2993920",
    "end": "3005430"
  },
  {
    "text": "Maybe I shouldn't call it X,\nmaybe it's probably better",
    "start": "3005430",
    "end": "3013630"
  },
  {
    "text": "to call it Z or something. OK, so when we then want\nto work out the chain rule,",
    "start": "3013630",
    "end": "3023680"
  },
  {
    "text": "well what do we do? We take the derivative\nof F at the point Z.",
    "start": "3023680",
    "end": "3031660"
  },
  {
    "text": "And so at that point we have\nto actually remember something, we have to remember that\nthe derivative of the log",
    "start": "3031660",
    "end": "3037420"
  },
  {
    "text": "is the 1 on X function. So this is going to be\nequal to the 1 on X for Z.",
    "start": "3037420",
    "end": "3048070"
  },
  {
    "text": "So that's then going to be 1\nover the sum of W equals 1 to V",
    "start": "3048070",
    "end": "3053500"
  },
  {
    "text": "of exp of UTVC multiplied\nby the derivative",
    "start": "3053500",
    "end": "3063760"
  },
  {
    "text": "of the inner function. So the derivative of the\npart that is remaining,",
    "start": "3063760",
    "end": "3075930"
  },
  {
    "text": "I'm getting this, the sum of-- and this one trick\nhere at this point",
    "start": "3075930",
    "end": "3081780"
  },
  {
    "text": "we do want to have\na change of index. So we want to say the\nsum of X equals 1 to V",
    "start": "3081780",
    "end": "3087560"
  },
  {
    "text": "of exp of U of X VC.",
    "start": "3087560",
    "end": "3092940"
  },
  {
    "text": "Since we can get into trouble\nif we don't change that",
    "start": "3092940",
    "end": "3098329"
  },
  {
    "text": "variable to be using\na different one. OK, so at that point we're\nmaking some progress,",
    "start": "3098330",
    "end": "3107960"
  },
  {
    "text": "but we still want to work\nout the derivative of this. And so what we want to do is\napply the chain rule once more.",
    "start": "3107960",
    "end": "3114859"
  },
  {
    "text": "So now here is our F and in here\nis our new Z equals G of VC.",
    "start": "3114860",
    "end": "3121120"
  },
  {
    "text": "And so, we then\nsort of repeat over,",
    "start": "3123750",
    "end": "3128910"
  },
  {
    "text": "so we can move the derivative\ninside a sum always.",
    "start": "3128910",
    "end": "3135869"
  },
  {
    "text": "So we then taking the\nderivative of this,",
    "start": "3135870",
    "end": "3146470"
  },
  {
    "text": "and so then the derivative\nof exp is itself,",
    "start": "3146470",
    "end": "3152980"
  },
  {
    "text": "so we're going to just\nhave exp of the UXTVC times",
    "start": "3152980",
    "end": "3160388"
  },
  {
    "text": "there's is a sum of\nX equals 1 to V times the derivative of UX TVC.",
    "start": "3160388",
    "end": "3172299"
  },
  {
    "text": "OK, and so then this is what\nwe've worked out before,",
    "start": "3172300",
    "end": "3178990"
  },
  {
    "text": "we can just rewrite as UX. OK, so now we're\nmaking progress.",
    "start": "3178990",
    "end": "3186220"
  },
  {
    "text": "So if we start putting\nall of that together, what we have is the derivative,\nwell the partial derivatives",
    "start": "3186220",
    "end": "3197690"
  },
  {
    "text": "with VC of this log probability.",
    "start": "3197690",
    "end": "3202819"
  },
  {
    "text": "All right, we have\nthe numerator, which was just U0 minus--",
    "start": "3202820",
    "end": "3209500"
  },
  {
    "text": "we then had the sum of\nthe numerator, sum over X equals 1 to V of exp\nUXTVC times U of X, then",
    "start": "3209500",
    "end": "3223650"
  },
  {
    "text": "that was multiplied by our\nfirst term that came from the 1 on X, which gives you the\nsum of W equals 1 to V",
    "start": "3223650",
    "end": "3233309"
  },
  {
    "text": "of the exp of UWTVC.",
    "start": "3233310",
    "end": "3235740"
  },
  {
    "text": "And this, the fact that\nwe changed the variables became important. And so by just sort of\nrewriting that a little,",
    "start": "3240131",
    "end": "3249740"
  },
  {
    "text": "we can get that equals U0\nminus the sum V equals sorry--",
    "start": "3249740",
    "end": "3256670"
  },
  {
    "text": "X equals 1 to V of this X,\nV of XTVC over the sum of W",
    "start": "3264090",
    "end": "3274810"
  },
  {
    "text": "equals to V of exp\nUWTVC times U of X.",
    "start": "3274810",
    "end": "3282990"
  },
  {
    "text": "And so at that point it's\nsort of interesting thing has happened that we've ended\nup getting straight back exactly",
    "start": "3282990",
    "end": "3290830"
  },
  {
    "text": "the softmax formula probability\nthat we saw when we started.",
    "start": "3290830",
    "end": "3297570"
  },
  {
    "text": "And we can just rewrite\nthat more conveniently as saying this equals U0 minus\nthe sum over X equals 1 to V",
    "start": "3297570",
    "end": "3308130"
  },
  {
    "text": "of the probability of\nX given C times UX.",
    "start": "3308130",
    "end": "3314099"
  },
  {
    "text": "And so what we have\nat that moment is this thing here\nis an expectation.",
    "start": "3314100",
    "end": "3321460"
  },
  {
    "text": "And so this isn't an\naverage over all the context vectors waited by\ntheir probability",
    "start": "3321460",
    "end": "3328920"
  },
  {
    "text": "according to the model. And so it's always the case\nwith these softmax style models,",
    "start": "3328920",
    "end": "3334540"
  },
  {
    "text": "that what you get out\nfor the derivatives is you get the observed\nminus the expected.",
    "start": "3334540",
    "end": "3343509"
  },
  {
    "text": "So our model is good\nif our model on average predicts exactly the word\nvector that we actually see.",
    "start": "3343510",
    "end": "3353650"
  },
  {
    "text": "And so we're going\nto try and adjust the parameters of our\nmodel, so [INAUDIBLE]..",
    "start": "3353650",
    "end": "3361950"
  },
  {
    "text": "Now we try and make it do\nit as much as possible, I mean of course as you'll\nfind you can never get close.",
    "start": "3361950",
    "end": "3371440"
  },
  {
    "text": "If I just say to you OK\nthe word is croissant, which words are going to occur\nin the context of croissant?",
    "start": "3371440",
    "end": "3379970"
  },
  {
    "text": "I mean, you can't answer\nthat, there are all sorts of sentences that you\ncould say that involve the word croissant.",
    "start": "3379970",
    "end": "3385480"
  },
  {
    "text": "So actually our particular\nprobability estimates are going to be kind of\nsmall, but nevertheless we",
    "start": "3385480",
    "end": "3393610"
  },
  {
    "text": "want to sort of fiddle our\nword vectors to try and make those estimates as high\nas we possibly can.",
    "start": "3393610",
    "end": "3401530"
  },
  {
    "text": "So I've gone on about\nthis stuff a bit,",
    "start": "3401530",
    "end": "3407560"
  },
  {
    "text": "but haven't actually sort\nof shown you any of what actually happened. So I just want to\nquickly show you",
    "start": "3407560",
    "end": "3416800"
  },
  {
    "text": "a bit of that as\nto what actually happens with word vectors.",
    "start": "3416800",
    "end": "3422180"
  },
  {
    "text": "So here's a simple\nlittle IPython notebook which is also what you'll be\nusing for assignment one only.",
    "start": "3422180",
    "end": "3429410"
  },
  {
    "text": "So in the first cell I\nimport a bunch of stuff. So we've got NumPy\nfor our vectors.",
    "start": "3429410",
    "end": "3436900"
  },
  {
    "text": "[AUDIO OUT] learns kind\nof your machine learning,",
    "start": "3436900",
    "end": "3443380"
  },
  {
    "text": "Swiss Army Knife\nGENESIM is a package that you may well\nnot have seen before. It's a package that's often\nused for word vectors,",
    "start": "3443380",
    "end": "3450790"
  },
  {
    "text": "it's not really used\nfor deep learning. So this is the only time\nyou'll see it in the class,",
    "start": "3450790",
    "end": "3456172"
  },
  {
    "text": "but if you just\nwant a good package for working with word vectors\nand some other application it's a good one to know about.",
    "start": "3456172",
    "end": "3463810"
  },
  {
    "text": "OK so then in my\nsecond cell here I'm loading a particular\nset of word vectors.",
    "start": "3463810",
    "end": "3471589"
  },
  {
    "text": "So these are glove word vectors\nthat we made at Stanford in 2014, and I'm loading\na hundred dimensional word",
    "start": "3471590",
    "end": "3479770"
  },
  {
    "text": "vectors so that things are\na little bit quicker for me while I'm doing\nthings here, sort",
    "start": "3479770",
    "end": "3486490"
  },
  {
    "text": "of do this model of bread\nand croissant, what I've just",
    "start": "3486490",
    "end": "3491860"
  },
  {
    "text": "got here is word vectors. So I just wanted\nto sort of show you",
    "start": "3491860",
    "end": "3497890"
  },
  {
    "text": "that there are word vectors.",
    "start": "3497890",
    "end": "3500380"
  },
  {
    "text": "Well maybe I should have loaded\nthose word vectors in advance.",
    "start": "3505230",
    "end": "3510033"
  },
  {
    "text": "Let's see.",
    "start": "3512950",
    "end": "3513535"
  },
  {
    "text": "Oh, OK, I'm in business. OK, so here are my word vectors\nfor bread and croissant,",
    "start": "3522480",
    "end": "3533640"
  },
  {
    "text": "and well you can see that\nmaybe these two words are a bit similar, so\nboth of them are negative in the first dimension,\npositive in the second,",
    "start": "3533640",
    "end": "3540509"
  },
  {
    "text": "negative in the third,\npositive in the fourth, negative in the fifth. So it sort of looks\nlike they might",
    "start": "3540510",
    "end": "3546510"
  },
  {
    "text": "have a fair bit of\ndot product which is kind of what we want\nbecause bread and croissant are kind of similar.",
    "start": "3546510",
    "end": "3552360"
  },
  {
    "text": "But what we can do is\nactually ask the model and these are Jensen\nfunctions, now",
    "start": "3552360",
    "end": "3557490"
  },
  {
    "text": "know what are the\nmost similar words so I can ask for\ncroissant, what are",
    "start": "3557490",
    "end": "3564480"
  },
  {
    "text": "the most similar words to that. And it will tell me it's\nthings like brioche, baguette, focaccia. So that's pretty good.",
    "start": "3564480",
    "end": "3571859"
  },
  {
    "text": "Pudding is perhaps a little\nbit more questionable. We can say most\nsimilar to the USA",
    "start": "3571860",
    "end": "3579030"
  },
  {
    "text": "and it says Canada, America,\nUSA with periods, United States that's pretty good.",
    "start": "3579030",
    "end": "3584400"
  },
  {
    "text": "And most similar to banana-- take it out-- coconut, mangoes,\nbananas, sort of fairly",
    "start": "3584400",
    "end": "3591960"
  },
  {
    "text": "tropical fruit grade. Before finishing though, I want\nto show you something slightly",
    "start": "3591960",
    "end": "3599130"
  },
  {
    "text": "more than just similarity, which\nis one of the amazing things that people observed\nwith these word vectors,",
    "start": "3599130",
    "end": "3606089"
  },
  {
    "text": "and that was to say you\ncan actually sort of do arithmetic in this vector\nspace, that makes sense.",
    "start": "3606090",
    "end": "3613180"
  },
  {
    "text": "And so in particular, people\nsuggested this analogy task. And so the idea of\nthe analogy task",
    "start": "3613180",
    "end": "3619410"
  },
  {
    "text": "is you should be able to\nstart with a word like king, and you should be able to\nsubtract out a male component",
    "start": "3619410",
    "end": "3625650"
  },
  {
    "text": "from that, add back\nin a woman component, and then you should\nbe able to ask, well,",
    "start": "3625650",
    "end": "3631570"
  },
  {
    "text": "what word is over here? And what should like is that\nthe word over there is queen.",
    "start": "3631570",
    "end": "3638505"
  },
  {
    "text": "And so this sort\nof little bit of-- so we're going to do that,\nwith this sort of same most",
    "start": "3641040",
    "end": "3651240"
  },
  {
    "text": "similar function which\nis actually more-- so as well as having\npositive words,",
    "start": "3651240",
    "end": "3656790"
  },
  {
    "text": "you can ask for most similar\nnegative words and you might wonder what's most negatively\nsimilar to a banana,",
    "start": "3656790",
    "end": "3663930"
  },
  {
    "text": "and you might be\nthinking, oh it's-- I don't know, some kind\nof meat or something.",
    "start": "3663930",
    "end": "3670230"
  },
  {
    "text": "Actually that by itself\nisn't very useful because when you just ask\nfor most negatively similar",
    "start": "3670230",
    "end": "3676440"
  },
  {
    "text": "to things, you tend to\nget crazy strings that were found in the data\nset that you don't know",
    "start": "3676440",
    "end": "3681630"
  },
  {
    "text": "what they mean if anything. But if we put the\ntwo together, we can use the most similar\nfunction with positives",
    "start": "3681630",
    "end": "3688860"
  },
  {
    "text": "and negatives to do analogies. So we're going to say\nwe want a positive king,",
    "start": "3688860",
    "end": "3695250"
  },
  {
    "text": "we want to subtract\nout negatively man, we want to then add\nin positively woman",
    "start": "3695250",
    "end": "3701070"
  },
  {
    "text": "and find out what's most similar\nto this point in the space. So my analogy function\ndoes that, precisely that",
    "start": "3701070",
    "end": "3708660"
  },
  {
    "text": "by taking a couple\nof most similar ones and then subtracting\nout the negative one.",
    "start": "3708660",
    "end": "3716670"
  },
  {
    "text": "And so we can try out\nthis analogy function. So I can do the analogy, I\nshow in the picture with man",
    "start": "3716670",
    "end": "3724800"
  },
  {
    "text": "is the king as woman is-- sorry I'm not\nsaying that's right.",
    "start": "3724800",
    "end": "3730619"
  },
  {
    "text": "Yeah, man is the\nking as woman is to-- well, sorry I haven't\ndone my cells.",
    "start": "3730620",
    "end": "3736140"
  },
  {
    "text": "OK, man is to king\nas woman is to queen. So that's great,\nand that works well.",
    "start": "3741610",
    "end": "3749740"
  },
  {
    "text": "I mean-- and you can do it\nthe sort of other way around, king is to man as\nqueen is to woman.",
    "start": "3749740",
    "end": "3755829"
  },
  {
    "text": "If this only worked for\nthat one freakish example, you maybe wouldn't\nbe very impressed",
    "start": "3755830",
    "end": "3763250"
  },
  {
    "text": "but you know it actually turns\nout like it's not perfect but you can do all sorts\nof fun analogies with this,",
    "start": "3763250",
    "end": "3768970"
  },
  {
    "text": "and they actually work. So I could ask for something\nlike analogy, here's",
    "start": "3768970",
    "end": "3778360"
  },
  {
    "text": "a good one. Australia is to beer\nas France is to want?",
    "start": "3778360",
    "end": "3788838"
  },
  {
    "text": "And you can think about what\nyou think the answer to that one should be, and it\ncomes out as champagne,",
    "start": "3788838",
    "end": "3795549"
  },
  {
    "text": "which is pretty good. Or I could ask for something\nlike analogy pencil",
    "start": "3795550",
    "end": "3802270"
  },
  {
    "text": "is to sketching as\ncamera is to what?",
    "start": "3802270",
    "end": "3811930"
  },
  {
    "text": "And it says photographing. You can also do the analogies\nwith people, at this point,",
    "start": "3811930",
    "end": "3818390"
  },
  {
    "text": "I have to point out\nthat this data was and the model was built in\n2014, so you can't ask anything",
    "start": "3818390",
    "end": "3826210"
  },
  {
    "text": "about Donald Trump in there. Well you can, Trump is in\nthere but not as president, but I could ask something like\nanalogy a bomb is to Clinton",
    "start": "3826210",
    "end": "3838080"
  },
  {
    "text": "as Reagan is to what?",
    "start": "3838080",
    "end": "3845910"
  },
  {
    "text": "And you can think\nof what you think is the right analogy there, the\nanalogy it returns with Nixon.",
    "start": "3845910",
    "end": "3854400"
  },
  {
    "text": "So I guess that depends on\nwhat you think of Bill Clinton as to whether you think that\nwas a good analogy or not.",
    "start": "3854400",
    "end": "3859730"
  },
  {
    "text": "You can also do sort of\nlinguistic analogies with it, so you can do something\nlike analogy tall is",
    "start": "3859730",
    "end": "3868920"
  },
  {
    "text": "to tallest as long is to what?",
    "start": "3868920",
    "end": "3874740"
  },
  {
    "text": "And it does longest. So it really just sort of\nknows a lot about the meaning behavior of words, and I think\nwhen these methods were first",
    "start": "3874740",
    "end": "3885750"
  },
  {
    "text": "developed and hopefully still\nfor you, that people were just gobsmacked about how\nwell this actually worked",
    "start": "3885750",
    "end": "3893010"
  },
  {
    "text": "at capturing enough words. And so these word vectors\nthen went everywhere",
    "start": "3893010",
    "end": "3898920"
  },
  {
    "text": "as a new representation that\nwas so powerful for working out word meaning.",
    "start": "3898920",
    "end": "3904710"
  },
  {
    "text": "And so that's our starting\npoint for this class, and we'll say a bit more\nabout them next time.",
    "start": "3904710",
    "end": "3910140"
  },
  {
    "text": "And they're also the basis\nof what you're looking at for the first assignment. Can I ask a quick question about\nthe distinction between the two",
    "start": "3910140",
    "end": "3917970"
  },
  {
    "text": "vectors per word? Yes. So my understanding is that\nthere can be several context",
    "start": "3917970",
    "end": "3924720"
  },
  {
    "text": "words per word in\nthe vocabulary,",
    "start": "3924720",
    "end": "3929940"
  },
  {
    "text": "but then there's\nonly two vectors I thought the distinction\nbetween the two is that one it's like\nthe actual word and one",
    "start": "3929940",
    "end": "3935295"
  },
  {
    "text": "is the context word. But in multiple contexts words,\nhow do you pick just two then?",
    "start": "3935295",
    "end": "3941100"
  },
  {
    "text": "Well, so we're doing\nevery one of them, so like maybe I won't turn\nback on the screen share",
    "start": "3941100",
    "end": "3949350"
  },
  {
    "text": "but we are doing in the\nobjective function there was a sum over--",
    "start": "3949350",
    "end": "3956230"
  },
  {
    "text": "so you've got this big\ncorpus of text right, so you're taking a sum\nover every word which",
    "start": "3956230",
    "end": "3962010"
  },
  {
    "text": "is it appearing as\nthe center word, and then inside that there's\na second sum which is",
    "start": "3962010",
    "end": "3968430"
  },
  {
    "text": "for each word in the context. So you are going to count\neach word as a context word,",
    "start": "3968430",
    "end": "3973890"
  },
  {
    "text": "and so then for one particular\nterm of that objective function you've got a particular context\nword and a particular center",
    "start": "3973890",
    "end": "3982440"
  },
  {
    "text": "word that you're then sort of\nsumming over different context words for each\ncenter word, and then",
    "start": "3982440",
    "end": "3989520"
  },
  {
    "text": "you're summing over all of the\ndecisions of different center words. And to say just a sentence\nmore about having two vectors,",
    "start": "3989520",
    "end": "3999840"
  },
  {
    "text": "I mean in some sense\nit's an ugly detail that was done to make things\nsort of simple and fast.",
    "start": "3999840",
    "end": "4007039"
  },
  {
    "text": "So if you look at\nthe math carefully,",
    "start": "4007040",
    "end": "4013460"
  },
  {
    "text": "if you sort of treated the\ntwo vectors as the same,",
    "start": "4013460",
    "end": "4018550"
  },
  {
    "text": "so if you use the same\nvectors for center and context and you say, OK, let's\nwork out the derivatives,",
    "start": "4018550",
    "end": "4026600"
  },
  {
    "text": "things get uglier. And the reason that\nthey get uglier is it's",
    "start": "4026600",
    "end": "4032750"
  },
  {
    "text": "when I'm iterating over all\nthe choices of context word, oh my God sometimes\nthe context word",
    "start": "4032750",
    "end": "4039770"
  },
  {
    "text": "is going to be the same\nas the center word, and so that messes with\nworking out my derivatives.",
    "start": "4039770",
    "end": "4047720"
  },
  {
    "text": "Whereas by taking them\nas separate vectors that never happens, so it's easy.",
    "start": "4047720",
    "end": "4053750"
  },
  {
    "text": "But the kind of\ninteresting thing is saying that you have these\ntwo different representations",
    "start": "4053750",
    "end": "4060140"
  },
  {
    "text": "sort of just ends up really\nsort of doing no harm, and my wave my hands\nargument for that",
    "start": "4060140",
    "end": "4067340"
  },
  {
    "text": "is since we're kind of\nmoving through each position, the corpus one by\none something--",
    "start": "4067340",
    "end": "4075619"
  },
  {
    "text": "a word that is the\ncenter word at one moment is going to be the context\nword at the next moment,",
    "start": "4075620",
    "end": "4081620"
  },
  {
    "text": "and the word that\nwas the context word is going to have\nbecome the center word. So you're sort of\ndoing the computation",
    "start": "4081620",
    "end": "4090260"
  },
  {
    "text": "both ways in each case. And so you should be\nable to convince yourself",
    "start": "4090260",
    "end": "4095510"
  },
  {
    "text": "that the two\nrepresentations for the word end up being very similar,\nand they are not identical",
    "start": "4095510",
    "end": "4102270"
  },
  {
    "text": "both for technical\nreasons of the ends of documents and things like\nthat, but very, very similar.",
    "start": "4102270",
    "end": "4109080"
  },
  {
    "text": "And so effectively\nyou tend to get two very similar\nrepresentations for each word,",
    "start": "4109080",
    "end": "4114140"
  },
  {
    "text": "and we just average them\nand collect the word vector. And when we use\nword vectors we just have one vector for each word.",
    "start": "4114140",
    "end": "4121850"
  },
  {
    "text": "That makes sense thank you. I have a question\npurely out of curiosity. So we don't know when we\nprojected the vectors,",
    "start": "4121850",
    "end": "4130049"
  },
  {
    "text": "the word vectors onto\nto the 2D surface we saw like little\nclusters of squares are similar to each\nother, and then",
    "start": "4130050",
    "end": "4135450"
  },
  {
    "text": "later on, we saw that\nwith the analogies thing, we kind of see that there's\nthese directional vectors that",
    "start": "4135450",
    "end": "4141330"
  },
  {
    "text": "sort of get like the rule of\nor the C of O or something like that. And so I'm wondering, is there--",
    "start": "4141330",
    "end": "4146900"
  },
  {
    "text": "are there relationships\nbetween those relational vectors\nthemselves such as, like is the rule of vector\nsort of similar to the C of O",
    "start": "4146900",
    "end": "4156179"
  },
  {
    "text": "of vector which is very\ndifferent from like-- makes a good\nsandwich with vector.",
    "start": "4156180",
    "end": "4162990"
  },
  {
    "text": "Is there any research on that? That's a good question.",
    "start": "4162990",
    "end": "4166969"
  },
  {
    "text": "You've stumped me already\nin the first lecture.",
    "start": "4169850",
    "end": "4172010"
  },
  {
    "text": "Yeah, I can't actually think\nof a piece of research. And so I'm not sure I\nhave a confident answer,",
    "start": "4178710",
    "end": "4185250"
  },
  {
    "text": "I mean it seems like\nthat's a really easy thing to check with once you have one\nof these sets of word vectors",
    "start": "4185250",
    "end": "4194250"
  },
  {
    "text": "that it seems like for\nany relationship that",
    "start": "4194250",
    "end": "4199440"
  },
  {
    "text": "is represented well\nenough by a word. You should be able to see if\nit comes out kind of similar.",
    "start": "4199440",
    "end": "4204810"
  },
  {
    "text": "I mean, I'm not sure,\nwe can look and see. That's totally OK, just curious.",
    "start": "4207840",
    "end": "4213315"
  },
  {
    "text": "I'm sorry, I missed the last\nlittle bit to your answer to your first question. So when you want to collapse\ntwo vectors for the same work,",
    "start": "4216990",
    "end": "4223630"
  },
  {
    "text": "did you usually\ntake the average? Different people have\ndone different things. But the most common\npractice is after you've--",
    "start": "4223630",
    "end": "4232719"
  },
  {
    "text": "there's still a bit more I have\nto cover about running word2vec that we didn't really\nget through today.",
    "start": "4232720",
    "end": "4238310"
  },
  {
    "text": "So I still got a bit more\nwork to do on Thursday, but once you've run\nyour word2vec algorithm",
    "start": "4238310",
    "end": "4244420"
  },
  {
    "text": "and you sort of your output\nis two vectors for each word and kind of when it's center\nand when its context, and so",
    "start": "4244420",
    "end": "4254230"
  },
  {
    "text": "typically people just average\nthose two vectors and say OK, that's the representation\nof the word croissant,",
    "start": "4254230",
    "end": "4261730"
  },
  {
    "text": "and that's what appears in\nthe sort of word vectors file, like the one I loaded.",
    "start": "4261730",
    "end": "4268870"
  },
  {
    "text": "Makes sense, thank you. I think-- so my question\nis, if a word to have",
    "start": "4268870",
    "end": "4274740"
  },
  {
    "text": "two different meanings or\nmultiple different meanings, can we still represent it\nas the same single vector?",
    "start": "4274740",
    "end": "4281330"
  },
  {
    "text": "Yes, that's a very\ngood question. And actually there\nis some content on that in Thursday's lecture,\nso I can say more about that.",
    "start": "4281330",
    "end": "4291010"
  },
  {
    "text": "But yeah, the first reaction\nis you kind of should be scared because something\nI've said nothing about at all",
    "start": "4291010",
    "end": "4299310"
  },
  {
    "text": "is most words, especially\nshort common words have lots of meanings.",
    "start": "4299310",
    "end": "4305160"
  },
  {
    "text": "So if you have a word like star,\nthat can be astronomical object",
    "start": "4305160",
    "end": "4311040"
  },
  {
    "text": "or it can be a film\nstar, a Hollywood star, or it can be something\nlike the gold stars",
    "start": "4311040",
    "end": "4316530"
  },
  {
    "text": "that you got in\nelementary school. And just taking all those\nuses of the word star",
    "start": "4316530",
    "end": "4324210"
  },
  {
    "text": "and collapsing them together\ninto one word vector.",
    "start": "4324210",
    "end": "4329610"
  },
  {
    "text": "And you might think that's\nreally crazy and bad, but actually turns out\nto work rather well.",
    "start": "4329610",
    "end": "4337739"
  },
  {
    "text": "Maybe I won't go\nthrough all of that right now because\nthere is actually stuff",
    "start": "4337740",
    "end": "4342900"
  },
  {
    "text": "on that on Thursday's lecture. Oh, I see, thanks. You can look ahead of the\nslides for next time, oh wait.",
    "start": "4342900",
    "end": "4352260"
  },
  {
    "text": "I have a quick question, I know\nthis might seem kind of strange to ask, but I guess\na lot of us were also",
    "start": "4352260",
    "end": "4358350"
  },
  {
    "text": "taking this course because of\nthe hype between AI and speech recognition.",
    "start": "4358350",
    "end": "4364890"
  },
  {
    "text": "And my basic question is, do we\nwe look the stack of something",
    "start": "4364890",
    "end": "4376567"
  },
  {
    "text": "like Alexa or something\nto provide speech to context or actions\nin this course,",
    "start": "4376567",
    "end": "4382220"
  },
  {
    "text": "or is it just primarily\nunderstanding? So this is an unusual\nand unusual quarter,",
    "start": "4382220",
    "end": "4391520"
  },
  {
    "text": "but for this quarter there's\na very clear answer which is this quarter there's also\na speech class being taught,",
    "start": "4391520",
    "end": "4402239"
  },
  {
    "text": "which is CS224S, a speech class\nbeing taught by Andrew Maas,",
    "start": "4402240",
    "end": "4408450"
  },
  {
    "text": "and you know this\nis a class that's been more regularly\noffered, sometimes it's only been offered\nevery third year,",
    "start": "4408450",
    "end": "4415130"
  },
  {
    "text": "but it's being\noffered right now. So if what you want to do is\nlearn about speech recognition",
    "start": "4415130",
    "end": "4421520"
  },
  {
    "text": "and learn about sort of methods\nfor building dialogue systems,",
    "start": "4421520",
    "end": "4427400"
  },
  {
    "text": "you should do CS224S. So for this class in general\nthe vast bulk of this class",
    "start": "4427400",
    "end": "4437510"
  },
  {
    "text": "is working with text and doing\nvarious kinds of text analysis",
    "start": "4437510",
    "end": "4444019"
  },
  {
    "text": "and understanding. So we do tasks like some\nof the ones I've mentioned, we do machine translation,\nwe do question answering,",
    "start": "4444020",
    "end": "4455000"
  },
  {
    "text": "we look at how to parse\nthe structure of sentences and things like that. In other years I sometimes\nsay a little bit about speech,",
    "start": "4455000",
    "end": "4463640"
  },
  {
    "text": "but since this quarter there's\na whole different class that's focused on speech\nthat seem a little bit silly.",
    "start": "4463640",
    "end": "4469994"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "4469994",
    "end": "4472760"
  },
  {
    "text": "--focus more and more\non speech [INAUDIBLE]..",
    "start": "4479490",
    "end": "4482540"
  },
  {
    "text": "I'm now getting a\nbad echo, I'm not sure if that's my fault or\nyour fault but anyway, answer,",
    "start": "4487760",
    "end": "4495560"
  },
  {
    "text": "so the speech class\ndoes a mix of stuff. So I mean the sort of\npure speech problems",
    "start": "4495560",
    "end": "4502760"
  },
  {
    "text": "classically have been\ndoing speech recognition. So going from a speech signal to\ntext and doing text to speech,",
    "start": "4502760",
    "end": "4512210"
  },
  {
    "text": "going from text to a speech\nsignal and both of those",
    "start": "4512210",
    "end": "4517310"
  },
  {
    "text": "are problems which\nare now normally done, including by the cell phone\nthat sits in your pocket using",
    "start": "4517310",
    "end": "4524120"
  },
  {
    "text": "neural networks. And so it covers both of\nthose, but then between that,",
    "start": "4524120",
    "end": "4530120"
  },
  {
    "text": "the class covers quite a\nbit and in particular it starts off with looking at\nbuilding dialogue systems.",
    "start": "4530120",
    "end": "4537580"
  },
  {
    "text": "So this is sort of\nsomething like Alexa, Google Assistant, Siri,\nas to well assuming you",
    "start": "4537580",
    "end": "4544820"
  },
  {
    "text": "have a speech recognition,\na text to speech system, then you do have\ntext-in and text-out.",
    "start": "4544820",
    "end": "4552180"
  },
  {
    "text": "What are the kind\nof ways that people go about building dialogue\nsystems like the ones",
    "start": "4552180",
    "end": "4561190"
  },
  {
    "text": "that I just mentioned. I actually had a question.",
    "start": "4561190",
    "end": "4566280"
  },
  {
    "text": "So I think there is\nsome people are noticing that the opposites were really\nnear to each other, which",
    "start": "4566280",
    "end": "4572450"
  },
  {
    "text": "was kind of odd but\nI was also wondering, what about positive and\nnegative balance or lack of it?",
    "start": "4572450",
    "end": "4580699"
  },
  {
    "text": "Is that captured well\nin this type of model or is it not captured well\nwith the opposite sides where",
    "start": "4580700",
    "end": "4587930"
  },
  {
    "text": "it really-- So the short answer\nis both of those. So this is a good question,\na good observation.",
    "start": "4587930",
    "end": "4594770"
  },
  {
    "text": "And the short answer\nis no, both of those are captured really,\nreally badly.",
    "start": "4594770",
    "end": "4599960"
  },
  {
    "text": "I mean there's a definition of-- when I say really,\nreally badly I",
    "start": "4599960",
    "end": "4607160"
  },
  {
    "text": "mean what I mean is, if that's\nwhat you want to focus on,",
    "start": "4607160",
    "end": "4613850"
  },
  {
    "text": "you've got problems. I mean it's not that the\nalgorithm doesn't work so precisely, what you find\nis that antonyms generally",
    "start": "4613850",
    "end": "4624140"
  },
  {
    "text": "occur in very similar topics\nbecause whether it's saying",
    "start": "4624140",
    "end": "4629580"
  },
  {
    "text": "John is really tall or\nJohn is really short, or that movie was fantastic,\nor that movie was terrible.",
    "start": "4629580",
    "end": "4637400"
  },
  {
    "text": "You get antonyms occurring\nin the same context. So because of that, their\nvectors are very similar",
    "start": "4637400",
    "end": "4644630"
  },
  {
    "text": "and similarly sort of effect\nand sentiment based words. Well, like Mike creating\nterrible example,",
    "start": "4644630",
    "end": "4652160"
  },
  {
    "text": "the context is similar. That if you're just learning\nthis kind of predict words",
    "start": "4652160",
    "end": "4659510"
  },
  {
    "text": "and context models that no,\nthat's not captured now.",
    "start": "4659510",
    "end": "4665329"
  },
  {
    "text": "That's not the end\nof the story, I mean absolutely people\nwanted to use neural networks",
    "start": "4665330",
    "end": "4671690"
  },
  {
    "text": "for sentiment and other kinds\nof sort of connotation effect. And there are very good ways\nof doing that, but somehow you",
    "start": "4671690",
    "end": "4679550"
  },
  {
    "text": "have to do something more\nthan simply predicting words and context because that's\nnot sufficient to capture",
    "start": "4679550",
    "end": "4686750"
  },
  {
    "text": "that dimension,\nmore on that later. I just happen to\nlike adjectives too,",
    "start": "4686750",
    "end": "4693639"
  },
  {
    "text": "like very basic adjectives,\nlike so and like not. Because they're sort\nof like appearing like some context here.",
    "start": "4693640",
    "end": "4701190"
  },
  {
    "text": "What was your first\nexample before not? Like so. This is so cool--",
    "start": "4701190",
    "end": "4706270"
  },
  {
    "text": "So that's actually a\ngood question as well. So yeah, so there are these very\ncommon words that are commonly",
    "start": "4706270",
    "end": "4712710"
  },
  {
    "text": "referred to as function words\nby linguists, which now includes ones like so and not,\nbut other ones like",
    "start": "4712710",
    "end": "4720989"
  },
  {
    "text": "and prepositions, like\nto, on, you sort of",
    "start": "4720990",
    "end": "4727350"
  },
  {
    "text": "might suspect that\nthe word vectors for those don't work out\nvery well because they",
    "start": "4727350",
    "end": "4733770"
  },
  {
    "text": "occur in all kinds of\ndifferent contexts. And they're not very distinct\nfrom each other in many cases,",
    "start": "4733770",
    "end": "4740520"
  },
  {
    "text": "and to a first approximation\nI think that's true. And part of why I didn't use\nthose as examples in my slides,",
    "start": "4740520",
    "end": "4750070"
  },
  {
    "text": "but at the end of the day, we do\nbuild up vector representations of those words too.",
    "start": "4750070",
    "end": "4755650"
  },
  {
    "text": "And you'll see in\na few lectures time when we start building what\nwe call language models.",
    "start": "4755650",
    "end": "4762280"
  },
  {
    "text": "That actually they do a great\njob in those words as well, I mean to explain what\nI'm meaning there.",
    "start": "4762280",
    "end": "4767949"
  },
  {
    "text": "I mean another feature\nof the word2vec model",
    "start": "4767950",
    "end": "4774130"
  },
  {
    "text": "is that actually ignore\nthe position of words, so it's said I'm\ngoing to predict",
    "start": "4774130",
    "end": "4780370"
  },
  {
    "text": "every word around\nthe center word but I'm predicting\nit in the same way.",
    "start": "4780370",
    "end": "4786610"
  },
  {
    "text": "I'm not predicting\ndifferently the word before me versus the word after\nme, or the word two",
    "start": "4786610",
    "end": "4792370"
  },
  {
    "text": "away in either\ndirection, they all just predicted the same by that\none probability function.",
    "start": "4792370",
    "end": "4799700"
  },
  {
    "text": "And so if that's all\nyou've got, that's sort of destroys your ability\nto do a good job at capturing",
    "start": "4799700",
    "end": "4807550"
  },
  {
    "text": "these sort of common\nmore grammatical words like so not an end. But we build slightly\ndifferent models",
    "start": "4807550",
    "end": "4814750"
  },
  {
    "text": "that are more sensitive to\nthe structure of sentences, and then we start doing\na good job on those two.",
    "start": "4814750",
    "end": "4821200"
  },
  {
    "text": "OK, thank you. I had a question about the\ncharacterization of word2vec.",
    "start": "4821200",
    "end": "4829540"
  },
  {
    "text": "Because I read [INAUDIBLE],,\nand it seems the character architecture [INAUDIBLE].",
    "start": "4829540",
    "end": "4836373"
  },
  {
    "text": "It was slightly\ndifferent from how it's presented in\nthe lecture, so are like two complimentary\nways together or--",
    "start": "4836373",
    "end": "4843099"
  },
  {
    "text": "Yeah, so I've still\ngot more to say,",
    "start": "4843100",
    "end": "4848200"
  },
  {
    "text": "so stay tuned Thursday for\nmore staff on word vectors.",
    "start": "4848200",
    "end": "4854920"
  },
  {
    "text": "So word2vec is\nkind of a framework for building word\nvectors, and that there",
    "start": "4854920",
    "end": "4861940"
  },
  {
    "text": "are sort of several\nvariant precise algorithms within the framework.",
    "start": "4861940",
    "end": "4867280"
  },
  {
    "text": "And one of them is whether\nyou're predicting the context",
    "start": "4867280",
    "end": "4873639"
  },
  {
    "text": "words or whether you're\npredicting the center word. So the model I showed was\npredicting the context words,",
    "start": "4873640",
    "end": "4881560"
  },
  {
    "text": "so it was the skip grand model. But then there's sort of a\ndetail of how in particular",
    "start": "4881560",
    "end": "4889360"
  },
  {
    "text": "to do the optimization\nand what I presented was the sort of easiest way\nto do it, which is naive",
    "start": "4889360",
    "end": "4898540"
  },
  {
    "text": "optimization with the equation,\nthe softmax equation for word",
    "start": "4898540",
    "end": "4903790"
  },
  {
    "text": "vectors. It turns out that that\nnaive optimization is sort of needlessly expensive,\nand people have come up",
    "start": "4903790",
    "end": "4913360"
  },
  {
    "text": "with faster ways of doing it,\nin particular the commerce thing",
    "start": "4913360",
    "end": "4919179"
  },
  {
    "text": "you see is what's\ncalled skip grand with negative sound playing,\nand the negative sampling is then sort of a much more\nefficient way to estimate.",
    "start": "4919180",
    "end": "4927430"
  },
  {
    "text": "Things and I'll mention\nthat on Thursday. Right, OK, thank you.",
    "start": "4927430",
    "end": "4933040"
  },
  {
    "text": "I was asking for\nmore information about how word vectors are\nconstructed beyond the summary",
    "start": "4933040",
    "end": "4940870"
  },
  {
    "text": "of random initialization. And then gradient based\nadditive [INAUDIBLE]..",
    "start": "4940870",
    "end": "4947410"
  },
  {
    "text": "Yeah, so I sort of will do a bit\nmore connecting this together",
    "start": "4947410",
    "end": "4952930"
  },
  {
    "text": "in the Thursday lecture,\nI guess to sort of-- I mean so much one can\nfit in the first class,",
    "start": "4952930",
    "end": "4959140"
  },
  {
    "text": "but the picture is\nessentially the picture I showed the pieces of.",
    "start": "4959140",
    "end": "4964750"
  },
  {
    "text": "So to learn word\nvectors you start off",
    "start": "4964750",
    "end": "4970660"
  },
  {
    "text": "by having a vector\nfor each word type both for context and outside\nand those vectors you initialize",
    "start": "4970660",
    "end": "4981250"
  },
  {
    "text": "randomly, so that you just\nput small little numbers that",
    "start": "4981250",
    "end": "4988000"
  },
  {
    "text": "are randomly generated\nin each vector component. And that's just\nyour starting point,",
    "start": "4988000",
    "end": "4993590"
  },
  {
    "text": "and so from there on you're\nusing an iterative algorithm where you're progressively\nupdating those word vectors,",
    "start": "4993590",
    "end": "5002110"
  },
  {
    "text": "so they do a better job at\npredicting which words appear in the context of other words.",
    "start": "5002110",
    "end": "5008250"
  },
  {
    "text": "And the way that\nwe're going to do that is by using the gradients,\nthat I was sort of starting",
    "start": "5008250",
    "end": "5017280"
  },
  {
    "text": "to show how to calculate and\nthen once you have a gradient, you can walk in the opposite\ndirection of the gradient",
    "start": "5017280",
    "end": "5024539"
  },
  {
    "text": "and you're then\nwalking downhill, i.e. you're minimizing\nyour loss and we're",
    "start": "5024540",
    "end": "5030330"
  },
  {
    "text": "going to sort of do lots of\nthat until our word vectors get as good as possible.",
    "start": "5030330",
    "end": "5035650"
  },
  {
    "text": "So it's really all math,\nbut in some sense word",
    "start": "5035650",
    "end": "5042270"
  },
  {
    "text": "vector learning is\nsort of miraculous since you do literally\njust start off",
    "start": "5042270",
    "end": "5048150"
  },
  {
    "text": "with completely\nrandom word vectors, and run this algorithm\nof predicting words",
    "start": "5048150",
    "end": "5055260"
  },
  {
    "text": "for a long time, and out of\nnothing emergences word vectors",
    "start": "5055260",
    "end": "5060360"
  },
  {
    "text": "that represent meaning well.",
    "start": "5060360",
    "end": "5062900"
  }
]