[
  {
    "start": "0",
    "end": "4374"
  },
  {
    "text": "SPEAKER: Hello, everyone.",
    "start": "4374",
    "end": "5860"
  },
  {
    "text": "Welcome back.",
    "start": "5860",
    "end": "6670"
  },
  {
    "text": "This is the seventh\nand final screencast",
    "start": "6670",
    "end": "8940"
  },
  {
    "text": "in our series on advanced\nbehavioral evaluation for NLU.",
    "start": "8940",
    "end": "12389"
  },
  {
    "text": "In the previous\nscreencast, I introduced",
    "start": "12390",
    "end": "14640"
  },
  {
    "text": "the idea of having adversarial\ntrain sets in the mix.",
    "start": "14640",
    "end": "17580"
  },
  {
    "text": "And I did that in the\ncontext of adversarial NLI.",
    "start": "17580",
    "end": "20760"
  },
  {
    "text": "And I talked briefly about\nthe Dynabench platform.",
    "start": "20760",
    "end": "23850"
  },
  {
    "text": "For this screencast, we're\ngoing to build on those ideas.",
    "start": "23850",
    "end": "26650"
  },
  {
    "text": "This is going to be a deep dive\non the DynaSent dataset, which",
    "start": "26650",
    "end": "29789"
  },
  {
    "text": "I was involved in creating.",
    "start": "29790",
    "end": "31350"
  },
  {
    "text": "You've actually worked\nalready with DynaSent",
    "start": "31350",
    "end": "33600"
  },
  {
    "text": "in the context of assignment\n1 and the associated bake off.",
    "start": "33600",
    "end": "36989"
  },
  {
    "text": "I'm describing it\nnow because I think",
    "start": "36990",
    "end": "38670"
  },
  {
    "text": "it introduces some new\nconcepts and also because it",
    "start": "38670",
    "end": "41789"
  },
  {
    "text": "might be a useful\nresource for you",
    "start": "41790",
    "end": "43530"
  },
  {
    "text": "as you think about projects.",
    "start": "43530",
    "end": "45210"
  },
  {
    "text": "All our data code and models\nare available on GitHub.",
    "start": "45210",
    "end": "48899"
  },
  {
    "text": "DynaSent is a\nsubstantial resource",
    "start": "48900",
    "end": "51060"
  },
  {
    "text": "with over 120,000\nsentences across 2 rounds.",
    "start": "51060",
    "end": "54690"
  },
  {
    "text": "And each one of those\nexamples has five gold labels",
    "start": "54690",
    "end": "57690"
  },
  {
    "text": "from crowdworkers.",
    "start": "57690",
    "end": "59850"
  },
  {
    "text": "There's the associated paper.",
    "start": "59850",
    "end": "61649"
  },
  {
    "text": "And as I said, round 2 for\nthis was created on Dynabench",
    "start": "61650",
    "end": "65580"
  },
  {
    "text": "with an interesting adversarial\ndynamic that I'll talk about.",
    "start": "65580",
    "end": "69780"
  },
  {
    "text": "This is a complete\nproject overview.",
    "start": "69780",
    "end": "71880"
  },
  {
    "text": "We're going to walk through\nthis diagram in some detail.",
    "start": "71880",
    "end": "74399"
  },
  {
    "text": "At a high level,\nthough, I think you",
    "start": "74400",
    "end": "75900"
  },
  {
    "text": "can see there are two rounds.",
    "start": "75900",
    "end": "77250"
  },
  {
    "text": "And there are also\ntwo models in the mix.",
    "start": "77250",
    "end": "79320"
  },
  {
    "text": "Those are the red boxes.",
    "start": "79320",
    "end": "80980"
  },
  {
    "text": "And at each round,\nwe're going to do",
    "start": "80980",
    "end": "82500"
  },
  {
    "text": "extensive human validation.",
    "start": "82500",
    "end": "84930"
  },
  {
    "text": "Let's dive into round 1.",
    "start": "84930",
    "end": "86970"
  },
  {
    "text": "The starting point\nfor this is model 0,",
    "start": "86970",
    "end": "89340"
  },
  {
    "text": "which we use as a device for\nfinding interesting cases",
    "start": "89340",
    "end": "94049"
  },
  {
    "text": "naturally occurring on the web.",
    "start": "94050",
    "end": "96390"
  },
  {
    "text": "Those are human validated.",
    "start": "96390",
    "end": "97800"
  },
  {
    "text": "And that gives us\nour round 1 dataset.",
    "start": "97800",
    "end": "100980"
  },
  {
    "text": "In a bit more detail, model\n0 is a RoBERTa-based model",
    "start": "100980",
    "end": "104790"
  },
  {
    "text": "that was fine tuned on a whole\nlot of sentiment examples.",
    "start": "104790",
    "end": "109090"
  },
  {
    "text": "These are the five\nbenchmarks that we",
    "start": "109090",
    "end": "110939"
  },
  {
    "text": "used to develop this model.",
    "start": "110940",
    "end": "112680"
  },
  {
    "text": "All of these datasets\nfor us are cast",
    "start": "112680",
    "end": "114720"
  },
  {
    "text": "as ternary sentiment problems.",
    "start": "114720",
    "end": "116760"
  },
  {
    "text": "That is positive,\nnegative, and neutral.",
    "start": "116760",
    "end": "119290"
  },
  {
    "text": "And you can see from\nthis slide, that we",
    "start": "119290",
    "end": "121090"
  },
  {
    "text": "are training on a really\nsubstantial number of sentiment",
    "start": "121090",
    "end": "124600"
  },
  {
    "text": "examples.",
    "start": "124600",
    "end": "126320"
  },
  {
    "text": "We're going to benchmark\nthese models against three",
    "start": "126320",
    "end": "128869"
  },
  {
    "text": "external datasets--",
    "start": "128870",
    "end": "130369"
  },
  {
    "text": "SST-3, Yelp, and Amazon.",
    "start": "130370",
    "end": "133310"
  },
  {
    "text": "And the table here is showing\nthat the model does well",
    "start": "133310",
    "end": "136430"
  },
  {
    "text": "on all three of\nthose benchmarks.",
    "start": "136430",
    "end": "137840"
  },
  {
    "text": "The results are not stellar.",
    "start": "137840",
    "end": "139280"
  },
  {
    "text": "I think this is a pretty\nhard multi-domain problem",
    "start": "139280",
    "end": "142760"
  },
  {
    "text": "for sentiment.",
    "start": "142760",
    "end": "143780"
  },
  {
    "text": "But in general, this\nlooks like a solid device",
    "start": "143780",
    "end": "147020"
  },
  {
    "text": "for finding interesting cases.",
    "start": "147020",
    "end": "148970"
  },
  {
    "text": "And that's the role\nthat this will play.",
    "start": "148970",
    "end": "150830"
  },
  {
    "text": "We are primarily thinking\nof using model 0 as a device",
    "start": "150830",
    "end": "153800"
  },
  {
    "text": "for harvesting\nexamples from the wild.",
    "start": "153800",
    "end": "157340"
  },
  {
    "text": "The space we explore is\nthe Yelp Open Dataset.",
    "start": "157340",
    "end": "160580"
  },
  {
    "text": "And we use the\nfollowing heuristic.",
    "start": "160580",
    "end": "162440"
  },
  {
    "text": "We favor sentences where the\nreview is one star and model",
    "start": "162440",
    "end": "166760"
  },
  {
    "text": "0 predicted positive.",
    "start": "166760",
    "end": "168920"
  },
  {
    "text": "And conversely, where the\nreview is 5 star and model 0",
    "start": "168920",
    "end": "172700"
  },
  {
    "text": "predicted negative.",
    "start": "172700",
    "end": "173930"
  },
  {
    "text": "This is a heuristic\nthat we think,",
    "start": "173930",
    "end": "175849"
  },
  {
    "text": "on average, will\nlead us to examples",
    "start": "175850",
    "end": "178250"
  },
  {
    "text": "that model 0 is\ngetting incorrect.",
    "start": "178250",
    "end": "180890"
  },
  {
    "text": "But it is just a heuristic.",
    "start": "180890",
    "end": "183050"
  },
  {
    "text": "The only labels we\nuse are ones that",
    "start": "183050",
    "end": "185180"
  },
  {
    "text": "are derived from a\nhuman validation phase.",
    "start": "185180",
    "end": "187920"
  },
  {
    "text": "This slide is showing you\nthe interface that we used.",
    "start": "187920",
    "end": "190470"
  },
  {
    "text": "The code is actually available\nin the project repository.",
    "start": "190470",
    "end": "193713"
  },
  {
    "text": "And you can see at a\nhigh level that reviewers",
    "start": "193713",
    "end": "195630"
  },
  {
    "text": "were making a choice about\nwhether a sentiment had",
    "start": "195630",
    "end": "198240"
  },
  {
    "text": "positive, negative, no\nsentiment, or mixed sentiment",
    "start": "198240",
    "end": "201090"
  },
  {
    "text": "labels.",
    "start": "201090",
    "end": "202140"
  },
  {
    "text": "Each example was\nvalidated by five workers.",
    "start": "202140",
    "end": "206400"
  },
  {
    "text": "And the resulting dataset\nis quite substantial.",
    "start": "206400",
    "end": "209079"
  },
  {
    "text": "This is a kind of\nsummary of the numbers.",
    "start": "209080",
    "end": "210960"
  },
  {
    "text": "First, I would point out\nthat 47% of the examples",
    "start": "210960",
    "end": "214980"
  },
  {
    "text": "are adversarial, which\nseems to me a high rate.",
    "start": "214980",
    "end": "217769"
  },
  {
    "text": "But the dataset includes both\nadversarial and non-adversarial",
    "start": "217770",
    "end": "221260"
  },
  {
    "text": "cases.",
    "start": "221260",
    "end": "221760"
  },
  {
    "text": "I think that's important\nto making a high quality",
    "start": "221760",
    "end": "224129"
  },
  {
    "text": "benchmark.",
    "start": "224130",
    "end": "225750"
  },
  {
    "text": "There are two ways that you\ncan think about training",
    "start": "225750",
    "end": "228240"
  },
  {
    "text": "on this resource.",
    "start": "228240",
    "end": "229500"
  },
  {
    "text": "The standard one would be\nwhat we call majority label",
    "start": "229500",
    "end": "232450"
  },
  {
    "text": "training.",
    "start": "232450",
    "end": "232950"
  },
  {
    "text": "And this is the\ncase where you infer",
    "start": "232950",
    "end": "235530"
  },
  {
    "text": "that the label for an example\nis the label that was chosen by,",
    "start": "235530",
    "end": "238860"
  },
  {
    "text": "at least three of the five\npeople who labeled it.",
    "start": "238860",
    "end": "241650"
  },
  {
    "text": "And if there is no\nsuch majority label,",
    "start": "241650",
    "end": "243930"
  },
  {
    "text": "you put that in that\nSeparate Elsewhere category.",
    "start": "243930",
    "end": "246719"
  },
  {
    "text": "That leads to a\nsubstantial resource.",
    "start": "246720",
    "end": "249430"
  },
  {
    "text": "However, we find that it is\nmore powerful to do what we",
    "start": "249430",
    "end": "252480"
  },
  {
    "text": "call distributional training.",
    "start": "252480",
    "end": "254280"
  },
  {
    "text": "In distributional training,\nyou repeat each example",
    "start": "254280",
    "end": "256859"
  },
  {
    "text": "five times with\neach of the labels",
    "start": "256860",
    "end": "259319"
  },
  {
    "text": "that it got from\nthe crowdworkers",
    "start": "259320",
    "end": "261120"
  },
  {
    "text": "and train on that entire set.",
    "start": "261120",
    "end": "263490"
  },
  {
    "text": "The result is that you don't\nhave to worry about the no",
    "start": "263490",
    "end": "266310"
  },
  {
    "text": "majority category anymore.",
    "start": "266310",
    "end": "267810"
  },
  {
    "text": "So you keep all your examples.",
    "start": "267810",
    "end": "269700"
  },
  {
    "text": "And you also intuitively get a\nmuch more nuanced perspective",
    "start": "269700",
    "end": "273660"
  },
  {
    "text": "on the sentiment judgments\nthat people offered.",
    "start": "273660",
    "end": "276270"
  },
  {
    "text": "Some are clear cases\nwith five out of five.",
    "start": "276270",
    "end": "278740"
  },
  {
    "text": "And some, actually, have\npretty mixed distributions",
    "start": "278740",
    "end": "281069"
  },
  {
    "text": "across the labels.",
    "start": "281070",
    "end": "282240"
  },
  {
    "text": "And you're training models\non all of that information.",
    "start": "282240",
    "end": "286009"
  },
  {
    "text": "And we find, in practice, that\nleads to more robust models.",
    "start": "286010",
    "end": "290240"
  },
  {
    "text": "For the Dev and Test,\nwe restrict attention",
    "start": "290240",
    "end": "292550"
  },
  {
    "text": "to positive,\nnegative, and neutral",
    "start": "292550",
    "end": "294020"
  },
  {
    "text": "to have a clean three-class\nternary sentiment problem.",
    "start": "294020",
    "end": "297620"
  },
  {
    "text": "And we balanced across\nthose three labels",
    "start": "297620",
    "end": "300110"
  },
  {
    "text": "for both Dev and Test.",
    "start": "300110",
    "end": "303300"
  },
  {
    "text": "So how do we do?",
    "start": "303300",
    "end": "304680"
  },
  {
    "text": "Well, let's think first about\nmodel 0 and its performance",
    "start": "304680",
    "end": "307460"
  },
  {
    "text": "on this benchmark.",
    "start": "307460",
    "end": "308430"
  },
  {
    "text": "This is a kind of summary.",
    "start": "308430",
    "end": "309600"
  },
  {
    "text": "We set things up so that model\n0 performs at chance on round 1.",
    "start": "309600",
    "end": "314900"
  },
  {
    "text": "No information coming from\nmodel 0 about the labels.",
    "start": "314900",
    "end": "318020"
  },
  {
    "text": "And then you have the summary\nnumbers here from before",
    "start": "318020",
    "end": "321410"
  },
  {
    "text": "on how it does on all of\nthose external benchmarks.",
    "start": "321410",
    "end": "324830"
  },
  {
    "text": "Humans, by contrast, do\nextremely well on round 1.",
    "start": "324830",
    "end": "328430"
  },
  {
    "text": "We estimate that the kind of\nF1 for humans is around 88%.",
    "start": "328430",
    "end": "333350"
  },
  {
    "text": "And that's a high number.",
    "start": "333350",
    "end": "335030"
  },
  {
    "text": "And it also arguably understates\nthe level of agreement.",
    "start": "335030",
    "end": "337820"
  },
  {
    "text": "So we note that 614\nof our 1,200 workers",
    "start": "337820",
    "end": "341330"
  },
  {
    "text": "never disagreed with\nthe majority label.",
    "start": "341330",
    "end": "343979"
  },
  {
    "text": "So this looks to us\nlike a very high rate",
    "start": "343980",
    "end": "346880"
  },
  {
    "text": "of agreement and consistency\nfor humans on this resource.",
    "start": "346880",
    "end": "353120"
  },
  {
    "text": "And here, just to round out\nthe discussion of round 1",
    "start": "353120",
    "end": "355880"
  },
  {
    "text": "are some randomly\nsampled short examples",
    "start": "355880",
    "end": "358220"
  },
  {
    "text": "showing you every combination\nof model prediction",
    "start": "358220",
    "end": "361310"
  },
  {
    "text": "and distribution across the\nlabels focused on the majority",
    "start": "361310",
    "end": "365030"
  },
  {
    "text": "label in this case.",
    "start": "365030",
    "end": "366020"
  },
  {
    "text": "And you see a lot of interesting\nnuanced linguistic things,",
    "start": "366020",
    "end": "369419"
  },
  {
    "text": "and I think a lot of use\nof non-literal language.",
    "start": "369420",
    "end": "374550"
  },
  {
    "text": "Let's move now to round 2.",
    "start": "374550",
    "end": "376229"
  },
  {
    "text": "This is substantially different.",
    "start": "376230",
    "end": "378120"
  },
  {
    "text": "We begin from model 1.",
    "start": "378120",
    "end": "379800"
  },
  {
    "text": "And this is a RoBERTa\nmodel that was fine",
    "start": "379800",
    "end": "381870"
  },
  {
    "text": "tuned on those external\nsentiment benchmarks,",
    "start": "381870",
    "end": "385320"
  },
  {
    "text": "as well as all of\nour round 1 data.",
    "start": "385320",
    "end": "387750"
  },
  {
    "text": "The intuition here, coming\nfrom the ANLI project,",
    "start": "387750",
    "end": "390510"
  },
  {
    "text": "is that we should train\nmodels on previous rounds",
    "start": "390510",
    "end": "393420"
  },
  {
    "text": "of our own dynamic\ndataset collection.",
    "start": "393420",
    "end": "396800"
  },
  {
    "text": "Instead of harvesting examples\nfrom the wild in this phase,",
    "start": "396800",
    "end": "399770"
  },
  {
    "text": "we're going to use\nDynabench to crowdsource",
    "start": "399770",
    "end": "401840"
  },
  {
    "text": "sentences that fool model 1\nwhile human validate those.",
    "start": "401840",
    "end": "405720"
  },
  {
    "text": "And that will lead us\nto our round 2 dataset.",
    "start": "405720",
    "end": "409000"
  },
  {
    "text": "Let's think a little\nbit about model 1.",
    "start": "409000",
    "end": "411250"
  },
  {
    "text": "Again, this is a\nRoBERTa-based classifier.",
    "start": "411250",
    "end": "413680"
  },
  {
    "text": "And it is trained on those\nsame external benchmarks.",
    "start": "413680",
    "end": "416830"
  },
  {
    "text": "But now, down\nsampled somewhat so",
    "start": "416830",
    "end": "419020"
  },
  {
    "text": "that we can give a\nlot of weight to round",
    "start": "419020",
    "end": "421300"
  },
  {
    "text": "1, which is now in the mix.",
    "start": "421300",
    "end": "423310"
  },
  {
    "text": "These models are still trained\non a substantial amount",
    "start": "423310",
    "end": "426669"
  },
  {
    "text": "of data.",
    "start": "426670",
    "end": "427600"
  },
  {
    "text": "And we're trying to\noffer some evidence",
    "start": "427600",
    "end": "430900"
  },
  {
    "text": "that round 1 is the important\nthing to actually focus on",
    "start": "430900",
    "end": "433690"
  },
  {
    "text": "for this model.",
    "start": "433690",
    "end": "435500"
  },
  {
    "text": "How do we do?",
    "start": "435500",
    "end": "436640"
  },
  {
    "text": "This is a summary of performance\non the external datasets,",
    "start": "436640",
    "end": "439940"
  },
  {
    "text": "as well as round 1.",
    "start": "439940",
    "end": "441770"
  },
  {
    "text": "And you can see down here that\nthis model is getting around",
    "start": "441770",
    "end": "444680"
  },
  {
    "text": "80% on our round 1\ndata with essentially",
    "start": "444680",
    "end": "448250"
  },
  {
    "text": "no loss in performance on\nthose external benchmarks.",
    "start": "448250",
    "end": "451130"
  },
  {
    "text": "There is a bit of a drop.",
    "start": "451130",
    "end": "452480"
  },
  {
    "text": "I think we are performing\nsome kind of domain shift",
    "start": "452480",
    "end": "455210"
  },
  {
    "text": "by emphasizing round\n1, as I described.",
    "start": "455210",
    "end": "458580"
  },
  {
    "text": "But overall, we're maintaining\npretty good performance",
    "start": "458580",
    "end": "461030"
  },
  {
    "text": "while doing quite well\non the round 1 dataset.",
    "start": "461030",
    "end": "464690"
  },
  {
    "start": "464690",
    "end": "467300"
  },
  {
    "text": "I want to do a deep\ndive a little bit on how",
    "start": "467300",
    "end": "469940"
  },
  {
    "text": "the examples were crowdsourced.",
    "start": "469940",
    "end": "471260"
  },
  {
    "text": "Because I think this is\nan interesting nuance",
    "start": "471260",
    "end": "473210"
  },
  {
    "text": "around how to get people\nto write productively",
    "start": "473210",
    "end": "475910"
  },
  {
    "text": "in a crowdsourcing context.",
    "start": "475910",
    "end": "477800"
  },
  {
    "text": "In the original\ninterface, we simply",
    "start": "477800",
    "end": "480080"
  },
  {
    "text": "did more or less what\nwas done for ANLI, which",
    "start": "480080",
    "end": "483289"
  },
  {
    "text": "is that we asked people to\nwrite a sentence from scratch",
    "start": "483290",
    "end": "486170"
  },
  {
    "text": "that would fool the model\nin a particular way.",
    "start": "486170",
    "end": "489080"
  },
  {
    "text": "We found though that that's a\nvery difficult creative writing",
    "start": "489080",
    "end": "492860"
  },
  {
    "text": "task.",
    "start": "492860",
    "end": "493520"
  },
  {
    "text": "And it leads people\nto do similar things",
    "start": "493520",
    "end": "495680"
  },
  {
    "text": "over multiple examples,\nwhich we intuited",
    "start": "495680",
    "end": "498259"
  },
  {
    "text": "would lead to artifacts\nin the resulting dataset.",
    "start": "498260",
    "end": "501600"
  },
  {
    "text": "And so we switched\nto emphasizing what",
    "start": "501600",
    "end": "503630"
  },
  {
    "text": "we call the prompt condition.",
    "start": "503630",
    "end": "505520"
  },
  {
    "text": "In the prompt\ncondition, we actually",
    "start": "505520",
    "end": "507470"
  },
  {
    "text": "offer crowdworkers a\nnaturally occurring sentence",
    "start": "507470",
    "end": "510860"
  },
  {
    "text": "that comes from the\nYelp Open Dataset.",
    "start": "510860",
    "end": "513440"
  },
  {
    "text": "And their task is to edit\nthat sentence in order",
    "start": "513440",
    "end": "516530"
  },
  {
    "text": "to achieve this task\nof fooling the model.",
    "start": "516530",
    "end": "519229"
  },
  {
    "text": "The result is a dataset\nthat's much more high quality",
    "start": "519230",
    "end": "522229"
  },
  {
    "text": "and has much more\nnaturalistic examples in it.",
    "start": "522230",
    "end": "525740"
  },
  {
    "text": "For validation, we did\nthe same thing as round 1.",
    "start": "525740",
    "end": "528750"
  },
  {
    "text": "And that leads to a dataset\nthat looks like this.",
    "start": "528750",
    "end": "531250"
  },
  {
    "text": "There are only 19%\nadversarial examples in this.",
    "start": "531250",
    "end": "534480"
  },
  {
    "text": "I think this shows that\nby now, in the process,",
    "start": "534480",
    "end": "537100"
  },
  {
    "text": "we have a very strong\nsentiment model that",
    "start": "537100",
    "end": "539160"
  },
  {
    "text": "is very difficult to fool.",
    "start": "539160",
    "end": "541170"
  },
  {
    "text": "But 19% is still a substantial\nnumber numerically.",
    "start": "541170",
    "end": "544410"
  },
  {
    "text": "And so we feel like\nwe're in good shape.",
    "start": "544410",
    "end": "546690"
  },
  {
    "text": "Overall, it's a somewhat\nsmaller benchmark.",
    "start": "546690",
    "end": "549160"
  },
  {
    "text": "But it has similar structure.",
    "start": "549160",
    "end": "551079"
  },
  {
    "text": "We can do majority label\ntraining, as well as",
    "start": "551080",
    "end": "553740"
  },
  {
    "text": "distributional training.",
    "start": "553740",
    "end": "555060"
  },
  {
    "text": "And we have balanced\nDev and Test.",
    "start": "555060",
    "end": "556830"
  },
  {
    "text": "They just happen to be a\nlittle smaller than round 1.",
    "start": "556830",
    "end": "561310"
  },
  {
    "text": "How does model 1\ndo versus humans?",
    "start": "561310",
    "end": "563920"
  },
  {
    "text": "Well, again, we set things up\nso that model 1 would perform",
    "start": "563920",
    "end": "567070"
  },
  {
    "text": "at chance on our round 2 data.",
    "start": "567070",
    "end": "568840"
  },
  {
    "text": "And you saw that model 1 does\npretty well on the round 1",
    "start": "568840",
    "end": "571810"
  },
  {
    "text": "data.",
    "start": "571810",
    "end": "572500"
  },
  {
    "text": "For humans, though, this\nround is extremely intuitive.",
    "start": "572500",
    "end": "576070"
  },
  {
    "text": "Our estimate of F1\nfor humans is actually",
    "start": "576070",
    "end": "578230"
  },
  {
    "text": "higher than for round 1.",
    "start": "578230",
    "end": "579399"
  },
  {
    "text": "We're now at around 90%.",
    "start": "579400",
    "end": "581560"
  },
  {
    "text": "And here, 116 of our\n244 workers never",
    "start": "581560",
    "end": "585640"
  },
  {
    "text": "disagreed with the\nmajority label.",
    "start": "585640",
    "end": "587410"
  },
  {
    "text": "Again, a substantial\nlevel of agreement on what",
    "start": "587410",
    "end": "591250"
  },
  {
    "text": "are clearly very difficult\nsentiment problems.",
    "start": "591250",
    "end": "594778"
  },
  {
    "text": "And just to round\nthis out, I thought",
    "start": "594778",
    "end": "596320"
  },
  {
    "text": "I'd show another sample of\nexamples from this round,",
    "start": "596320",
    "end": "599830"
  },
  {
    "text": "again, showing model 1\npredictions and every way",
    "start": "599830",
    "end": "602950"
  },
  {
    "text": "that the majority label\ncould have played out.",
    "start": "602950",
    "end": "605240"
  },
  {
    "text": "And I think even more\nthan in round 1, what",
    "start": "605240",
    "end": "607660"
  },
  {
    "text": "we start to see\nare examples that",
    "start": "607660",
    "end": "609759"
  },
  {
    "text": "make extensive use of\nintricate syntactic structures",
    "start": "609760",
    "end": "613750"
  },
  {
    "text": "and also intricate use\nof non-literal language",
    "start": "613750",
    "end": "617560"
  },
  {
    "text": "like metaphor and\nsarcasm and irony,",
    "start": "617560",
    "end": "620930"
  },
  {
    "text": "as techniques for\ncoming up with examples",
    "start": "620930",
    "end": "622820"
  },
  {
    "text": "that are intuitive\nfor us as humans",
    "start": "622820",
    "end": "624680"
  },
  {
    "text": "but are routinely very\nchallenging for even",
    "start": "624680",
    "end": "628310"
  },
  {
    "text": "our best models.",
    "start": "628310",
    "end": "630710"
  },
  {
    "text": "So that is DynaSent.",
    "start": "630710",
    "end": "632180"
  },
  {
    "text": "Let me use this opportunity\nto just wrap things up",
    "start": "632180",
    "end": "635300"
  },
  {
    "text": "with a few conclusions.",
    "start": "635300",
    "end": "636560"
  },
  {
    "text": "These are all meant to be\nopen questions designed",
    "start": "636560",
    "end": "639230"
  },
  {
    "text": "to have us looking ahead to the\nfuture of adversarial training",
    "start": "639230",
    "end": "642949"
  },
  {
    "text": "and testing.",
    "start": "642950",
    "end": "644120"
  },
  {
    "text": "Core question here.",
    "start": "644120",
    "end": "645770"
  },
  {
    "text": "Can adversarial training\nimprove systems?",
    "start": "645770",
    "end": "649100"
  },
  {
    "text": "I think overall,\nwe're seeing evidence",
    "start": "649100",
    "end": "651380"
  },
  {
    "text": "that the answer is yes.",
    "start": "651380",
    "end": "652430"
  },
  {
    "text": "But there is some nuance there.",
    "start": "652430",
    "end": "653990"
  },
  {
    "text": "And I think it's going\nto take some calibration",
    "start": "653990",
    "end": "656029"
  },
  {
    "text": "to get this exactly right.",
    "start": "656030",
    "end": "658850"
  },
  {
    "text": "\"What constitutes a fair\nnon-IID generalization test?\"",
    "start": "658850",
    "end": "663529"
  },
  {
    "text": "I introduced this\nnotion of fairness",
    "start": "663530",
    "end": "665420"
  },
  {
    "text": "when we discussed the analytic\nconsiderations around all",
    "start": "665420",
    "end": "668120"
  },
  {
    "text": "these behavioral evaluations.",
    "start": "668120",
    "end": "669830"
  },
  {
    "text": "And then this\nbecame very pressing",
    "start": "669830",
    "end": "671750"
  },
  {
    "text": "when we talked about why some\nof the COGs and re-COGs splits",
    "start": "671750",
    "end": "675380"
  },
  {
    "text": "are so difficult.\nThe question arises,",
    "start": "675380",
    "end": "677810"
  },
  {
    "text": "whether it's even fair to be\nasking our machine learning",
    "start": "677810",
    "end": "680870"
  },
  {
    "text": "systems to generalize,\nin particular,",
    "start": "680870",
    "end": "683390"
  },
  {
    "text": "ways that might, nonetheless,\nseem pretty intuitive for us",
    "start": "683390",
    "end": "686690"
  },
  {
    "text": "as humans.",
    "start": "686690",
    "end": "688710"
  },
  {
    "text": "\"Can hard behavioral\ntesting provide us",
    "start": "688710",
    "end": "691200"
  },
  {
    "text": "with the insights we need when\nit comes to certifying systems",
    "start": "691200",
    "end": "694740"
  },
  {
    "text": "as trustworthy?",
    "start": "694740",
    "end": "695880"
  },
  {
    "text": "If so, which tests?",
    "start": "695880",
    "end": "697410"
  },
  {
    "text": "And if not, what\nshould we do instead?\"",
    "start": "697410",
    "end": "699690"
  },
  {
    "text": "I think this is a\ncrucial question.",
    "start": "699690",
    "end": "701460"
  },
  {
    "text": "I think in a way, we know\nthat the answer is no.",
    "start": "701460",
    "end": "704600"
  },
  {
    "text": "No amount of behavioral\ntesting can offer us",
    "start": "704600",
    "end": "707310"
  },
  {
    "text": "the kind of guarantees\nthat we're seeking.",
    "start": "707310",
    "end": "709470"
  },
  {
    "text": "But it is a powerful\ncomponent in getting",
    "start": "709470",
    "end": "713279"
  },
  {
    "text": "closer to deeply understanding\nwhat these systems are like.",
    "start": "713280",
    "end": "716220"
  },
  {
    "text": "And certainly, we can\nuse behavioral testing",
    "start": "716220",
    "end": "718319"
  },
  {
    "text": "to find cases where they\ndefinitely fall down.",
    "start": "718320",
    "end": "721110"
  },
  {
    "text": "But for actual certification\nof safety and trustworthiness,",
    "start": "721110",
    "end": "724709"
  },
  {
    "text": "I believe we will\nneed to go deeper.",
    "start": "724710",
    "end": "726600"
  },
  {
    "text": "And that is the topic of\nthe next unit of the course.",
    "start": "726600",
    "end": "730949"
  },
  {
    "text": "Fundamentally, are\nour best systems",
    "start": "730950",
    "end": "733950"
  },
  {
    "text": "finding systematic solutions?",
    "start": "733950",
    "end": "735510"
  },
  {
    "text": "If the answer is yes,\nwe will feel, as humans,",
    "start": "735510",
    "end": "738450"
  },
  {
    "text": "that we can trust them.",
    "start": "738450",
    "end": "739710"
  },
  {
    "text": "And if the answer is no, even\nif they seem to behave well",
    "start": "739710",
    "end": "742620"
  },
  {
    "text": "in some scenarios,\nwe might always",
    "start": "742620",
    "end": "744529"
  },
  {
    "text": "worry that they're\ngoing to do things that",
    "start": "744530",
    "end": "746280"
  },
  {
    "text": "are totally baffling to us.",
    "start": "746280",
    "end": "748480"
  },
  {
    "text": "And then finally, the big juicy\ncognitive and philosophical",
    "start": "748480",
    "end": "752040"
  },
  {
    "text": "question.",
    "start": "752040",
    "end": "752880"
  },
  {
    "text": "\"Where humans\ngeneralize in ways that",
    "start": "752880",
    "end": "755130"
  },
  {
    "text": "are unsupported by\ndirect experience,",
    "start": "755130",
    "end": "757530"
  },
  {
    "text": "how should AI respond in\nterms of system design?\"",
    "start": "757530",
    "end": "760770"
  },
  {
    "text": "What should we do\nin order to achieve",
    "start": "760770",
    "end": "763230"
  },
  {
    "text": "these very unusual\nquasi cognitive,",
    "start": "763230",
    "end": "766470"
  },
  {
    "text": "quasi behavioral\nlearning targets?",
    "start": "766470",
    "end": "768600"
  },
  {
    "text": "I don't have a way to\nresolve this question.",
    "start": "768600",
    "end": "770920"
  },
  {
    "text": "But I think it's\nreally pressing when",
    "start": "770920",
    "end": "772529"
  },
  {
    "text": "we think about really\nchallenging our systems to do",
    "start": "772530",
    "end": "775710"
  },
  {
    "text": "complex things with language.",
    "start": "775710",
    "end": "778340"
  },
  {
    "start": "778340",
    "end": "783000"
  }
]