[
  {
    "text": "All right. So the plan for today is to \ncontinue our discussion of score-based models,  ",
    "start": "5640",
    "end": "11799"
  },
  {
    "text": "and we'll see how they are connected to \ndiffusion models. And we'll see some of  ",
    "start": "11800",
    "end": "18680"
  },
  {
    "text": "the state-of-the-art stuff that currently \nhas been used to generate images, videos,   some of the things we've seen in \nthe very first introductory lecture.",
    "start": "18680",
    "end": "28600"
  },
  {
    "text": "So brief reminder-- this is the usual \nroadmap slide for the course. Today,  ",
    "start": "28600",
    "end": "35440"
  },
  {
    "text": "we're talking about diffusion models or \nscore-based models. You can think of them   as one way of defining this model family by \nparameterizing the score and then learning  ",
    "start": "35440",
    "end": "46360"
  },
  {
    "text": "the data distribution by essentially \nusing some kind of score-matching loss.",
    "start": "46360",
    "end": "52160"
  },
  {
    "text": "And we've seen that-- yeah, that's \nthe key underlying idea is that to  ",
    "start": "52160",
    "end": "60040"
  },
  {
    "text": "represent the probability distribution, \nwe're going to use a neural network,  ",
    "start": "60040",
    "end": "65199"
  },
  {
    "text": "which is vector-valued. So for every point, \nit gives you a vector. And that vector is  ",
    "start": "65200",
    "end": "72479"
  },
  {
    "text": "supposed to represent the gradient of the \nlog likelihood at that point. And so can  ",
    "start": "72480",
    "end": "78560"
  },
  {
    "text": "think of it as a vector field like the one \nyou see here that is parameterized by some  ",
    "start": "78560",
    "end": "84600"
  },
  {
    "text": "neural network. And so as you change the \nweights, you get different vector fields.",
    "start": "84600",
    "end": "90280"
  },
  {
    "text": "And we've seen that it's possible to fit these \nmodels to data by doing score matching. So we've  ",
    "start": "90280",
    "end": "97400"
  },
  {
    "text": "seen that the machinery that we talked about \nin the context of energy-based models can be  ",
    "start": "97400",
    "end": "103600"
  },
  {
    "text": "applied very naturally to these settings. And so \nthere is a way to fit the estimated gradients to  ",
    "start": "103600",
    "end": "112159"
  },
  {
    "text": "the true gradients by minimizing this kind \nof loss, which only depends on the model.",
    "start": "112160",
    "end": "119120"
  },
  {
    "text": "This is the thing we derived by doing integration \nby parts. And it's a principled way of fitting the  ",
    "start": "119120",
    "end": "126560"
  },
  {
    "text": "model. The issue is that it's not going to work in \npractice if you're dealing with high-dimensional   settings because of this trace of the Jacobian \nterm that basically would require a lot of back  ",
    "start": "126560",
    "end": "137840"
  },
  {
    "text": "propagation steps, and so it's not going to \nwork if you're trying to, say, model images.",
    "start": "137840",
    "end": "144200"
  },
  {
    "text": "And so in the last lecture, we talked about two \nways of making score matching, more scalable.   The first one is denoising score matching, where \nthe idea is that instead of trying to model the  ",
    "start": "144200",
    "end": "154840"
  },
  {
    "text": "score of the data distribution, we're going to \ntry to model the score of this noise-perturbed  ",
    "start": "154840",
    "end": "160160"
  },
  {
    "text": "data distribution. And typically, the way we \nobtain this noise-perturbed data distribution is  ",
    "start": "160160",
    "end": "167400"
  },
  {
    "text": "by starting from a data point and then applying \nthis perturbation kernel, which gives you the  ",
    "start": "167400",
    "end": "173360"
  },
  {
    "text": "probability of error. Given that you have a clean \nimage x, what is the distribution over noisy  ",
    "start": "173360",
    "end": "180000"
  },
  {
    "text": "images x tilde? And it could be something \nas simple as let's add Gaussian noise to x.",
    "start": "180000",
    "end": "186600"
  },
  {
    "text": "And it turns out that estimating the score \nof this noise-perturbed data distribution is   actually much more efficient computationally. \nAnd so the usual kind of score matching loss  ",
    "start": "186600",
    "end": "198040"
  },
  {
    "text": "where you do regression, some kind of \nl2 loss between the estimated score,  ",
    "start": "198040",
    "end": "203560"
  },
  {
    "text": "and the true score of the noise-perturbed \ndata density-- that's the key difference   here. We're no longer estimating the score of \npdata. We're estimating the score of q sigma.",
    "start": "203560",
    "end": "213360"
  },
  {
    "text": "It turns out that it can be rewritten in terms of \nthe score of this transition kernel, perturbation  ",
    "start": "213360",
    "end": "220040"
  },
  {
    "text": "kernel, q sigma of x tilde given x, which is just, \nlet's say, a Gaussian. And so in the case of a  ",
    "start": "220040",
    "end": "228560"
  },
  {
    "text": "Gaussian distribution, this objective function \nbasically corresponds to denoising because the--  ",
    "start": "228560",
    "end": "236560"
  },
  {
    "text": "yeah, basically the score of a Gaussian is just \nlike the difference from the mean, essentially.",
    "start": "236560",
    "end": "242120"
  },
  {
    "text": "And so you can equivalently think of denoising \nscore matching as solving a denoising problem,  ",
    "start": "242120",
    "end": "248879"
  },
  {
    "text": "where what you're doing is you're sampling a \ndata point, you're sampling a noise vector,  ",
    "start": "248880",
    "end": "254280"
  },
  {
    "text": "and then you're feeding data plus noise to the \nscore model as data. And the goal of the score  ",
    "start": "254280",
    "end": "260400"
  },
  {
    "text": "model is to try to estimate z, essentially, which \nis the amount of noise that you've added to the,  ",
    "start": "260400",
    "end": "265919"
  },
  {
    "text": "to the clean data x. And so there's \nthis equivalence between learning the  ",
    "start": "265920",
    "end": "271720"
  },
  {
    "text": "score of the noise-perturbed data \ndensity and performing denoising.",
    "start": "271720",
    "end": "277960"
  },
  {
    "text": "And as you see, this is much more \nefficient because we no longer have   to deal with traces of Jacobians. It's a \nloss that you can efficiently optimize as  ",
    "start": "277960",
    "end": "288040"
  },
  {
    "text": "a function of theta. And so the pros is, yeah, \nit's much more scalable. It has this intuitive  ",
    "start": "288040",
    "end": "295680"
  },
  {
    "text": "correspondence to denoising, meaning that \nprobably architectures that work well for   denoising are going to work well for \nthis kind of score estimation task.",
    "start": "296320",
    "end": "305640"
  },
  {
    "text": "The negative side of this approach is \nthat we're no longer estimating the   score of the clean data distribution. We're now \nestimating the score of this noise-perturbed  ",
    "start": "306840",
    "end": "317160"
  },
  {
    "text": "data density. And so we're shifting the goal \npost here because we're no longer estimating  ",
    "start": "317160",
    "end": "324800"
  },
  {
    "text": "the score of the true data density, but \nwe're estimating the score. Even if we're   doing very well at solving this problem, \neven if we can drive the loss to zero,  ",
    "start": "324800",
    "end": "332280"
  },
  {
    "text": "we don't overfit, everything works well, \nwe're no longer estimating what we started   out with, but we're estimating this \nnoise-perturbed data density score.",
    "start": "332280",
    "end": "344000"
  },
  {
    "text": "And then we've seen the alternative is \nto do some kind of random projection,   and that's the sliced score matching approach, \nwhere essentially instead of trying to match the  ",
    "start": "344000",
    "end": "353800"
  },
  {
    "text": "true gradient with the estimated gradient \nat every point, we try to just match their  ",
    "start": "353800",
    "end": "360639"
  },
  {
    "text": "projections along some random direction \nv. And so at every point, we sample a  ",
    "start": "360640",
    "end": "366520"
  },
  {
    "text": "direction vector v based on some distribution, \nwe project the true score, the estimated score  ",
    "start": "366520",
    "end": "373639"
  },
  {
    "text": "at every point. After the projection, you get \nscalars, and then you compare the projections.",
    "start": "373640",
    "end": "379880"
  },
  {
    "text": "And if the vector fields are indeed the \nsame, then the projections should also be   the same. And it turns out that, again, this \nobjective function can be rewritten into one  ",
    "start": "379880",
    "end": "389960"
  },
  {
    "text": "that only depends on your model-- kind of \nthe same integration by parts trick. And  ",
    "start": "389960",
    "end": "395199"
  },
  {
    "text": "now this is something that can be evaluated \nefficiently, you can optimize efficiently  ",
    "start": "395200",
    "end": "402160"
  },
  {
    "text": "as a function of theta because essentially \nit only involves directional derivatives.",
    "start": "402160",
    "end": "408600"
  },
  {
    "text": "And so it's much more scalable than vanilla \nscore matching. It also estimates the score  ",
    "start": "408600",
    "end": "414720"
  },
  {
    "text": "of the true data density as opposed to the \ndata density plus noise. But it's a little  ",
    "start": "414720",
    "end": "420520"
  },
  {
    "text": "bit slower than denoising score matching because \nyou still have to take derivatives, basically.",
    "start": "420520",
    "end": "429280"
  },
  {
    "text": "So that's where we ended our last lecture. \nAnd then the other thing we talked about is  ",
    "start": "429280",
    "end": "437200"
  },
  {
    "text": "how to do inference. And we said, well, \nif you somehow are able to estimate the  ",
    "start": "439240",
    "end": "445840"
  },
  {
    "text": "underlying vector field of gradients by doing \nsome kind of score matching, then there are   ways of generating samples by using some kind of \nLangevin dynamics procedure where these scores  ",
    "start": "445840",
    "end": "458240"
  },
  {
    "text": "are telling you in which direction you should \ngo if you want to increase the probability of   your data point. And so you just follow these \narrows, and you can generate samples, basically.",
    "start": "458240",
    "end": "468760"
  },
  {
    "text": "And what we've seen is that this didn't actually \nwork in practice. This variant of the approach,  ",
    "start": "468760",
    "end": "474240"
  },
  {
    "text": "it makes sense, but it doesn't work \nfor several reasons. One is that,   at least for images, we expect the data to lie \non a low-dimensional manifold, meaning that the  ",
    "start": "474240",
    "end": "484880"
  },
  {
    "text": "score is not really a well-defined object. We \nhave this intuition that we're not expecting  ",
    "start": "484880",
    "end": "492920"
  },
  {
    "text": "to be able to learn accurate scores when we're \nfar away from the high data density regions.",
    "start": "492920",
    "end": "498720"
  },
  {
    "text": "If you think about the loss, it depends \non samples that you draw from the data   distribution. Most of the samples are \ngoing to come from high-probability  ",
    "start": "499600",
    "end": "507600"
  },
  {
    "text": "regions. When you're far away, you \nhave an object that looks nothing,   let's say, like an image. You've never \nseen these things during training.",
    "start": "507600",
    "end": "516360"
  },
  {
    "text": "It's unlikely that you're going to be able to \nestimate the score very accurately. And that's  ",
    "start": "516360",
    "end": "521560"
  },
  {
    "text": "a problem because then Langevin dynamics depends \non this information to find high-probability   regions. And so you might get lost, and you \nmight not be able to generate good samples.",
    "start": "521560",
    "end": "532960"
  },
  {
    "text": "And then we've seen that there are issues with the \nconvergence speed of Langevin dynamics. It might   not even converge. If you have zero probability \nregions somewhere, it might not be able to go  ",
    "start": "532960",
    "end": "543760"
  },
  {
    "text": "from one region of the space of possible images \nto another one. And so that's also an issue.",
    "start": "543760",
    "end": "552200"
  },
  {
    "text": "And so what we are going to see today is that \nthere is actually a very simple solution to all   of these three issues that we just talked about. \nAnd that basically involves adding noise-- adding,  ",
    "start": "552200",
    "end": "565160"
  },
  {
    "text": "let's say, Gaussian noise to the data. \nAnd to see this, we notice that, well,  ",
    "start": "565160",
    "end": "571600"
  },
  {
    "text": "one issue is that if the data lies on a \nmanifold, then the score is not really defined.",
    "start": "571600",
    "end": "577560"
  },
  {
    "text": "But the moment you add the noise to the data,   then it becomes supported over the whole \nspace. Noisy data, you are adding noise,  ",
    "start": "577560",
    "end": "588120"
  },
  {
    "text": "so any possible combination of pixel values \nhas some probability under this noise-perturbed  ",
    "start": "588120",
    "end": "594640"
  },
  {
    "text": "distribution. And so even though the \noriginal data lies on a manifold, the   moment you add noise, you fall off the manifold, \nand it becomes supported over the whole space.",
    "start": "594640",
    "end": "606000"
  },
  {
    "text": "Score matching on noisy data will allow us to \nbasically estimate the score much more accurately.  ",
    "start": "606960",
    "end": "614320"
  },
  {
    "text": "This is some empirical evidence showing if you try \nto do score matching on CIFAR-10 on clean images,  ",
    "start": "614320",
    "end": "621080"
  },
  {
    "text": "the loss is very, very bumpy. You're \nnot learning very well. But the moment   you add noise to the data, a tiny little \namount of noise to the data with some tiny,  ",
    "start": "621080",
    "end": "630120"
  },
  {
    "text": "little standard deviation, then the \nloss converges much more nicely.",
    "start": "630120",
    "end": "636600"
  },
  {
    "text": "And it solves the issue of the fact that score \nmatching is not accurate in low data density  ",
    "start": "637760",
    "end": "646240"
  },
  {
    "text": "regions. But remember the intuition was that most \nof your data points are going to come from-- let's  ",
    "start": "646240",
    "end": "654399"
  },
  {
    "text": "say, if your data is a mixture of two Gaussians-- \none here, and one here-- most of the data will   be-- the samples that you see during training are \ngoing to come from this region or this region,  ",
    "start": "654400",
    "end": "664640"
  },
  {
    "text": "the two corners where the data is distributed. \nAnd as a result, if you try to use data,  ",
    "start": "664640",
    "end": "671720"
  },
  {
    "text": "fit a score model, there is a true score in the \nmiddle, there is an estimated score on the right.",
    "start": "671720",
    "end": "678639"
  },
  {
    "text": "It's going to be accurate around the \nhigh data density regions. It's going   to be inaccurate the moment you go far away. \nBut if you think about adding noise, again,  ",
    "start": "678640",
    "end": "689400"
  },
  {
    "text": "it's a good thing for us because if you add \na noise to the data, then the samples of the  ",
    "start": "689400",
    "end": "697640"
  },
  {
    "text": "noise-perturbed data densities are going to \nbe, again, spread out all over the space.",
    "start": "697640",
    "end": "703640"
  },
  {
    "text": "And so what happens is that now if \nyou think about where you're going   to see your samples during training, if you \nadd a sufficiently large amount of noise,  ",
    "start": "703640",
    "end": "711520"
  },
  {
    "text": "the samples are going to be all over the space. \nThey're going to be spread around the whole space.  ",
    "start": "711520",
    "end": "719600"
  },
  {
    "text": "And what this means is that if you are willing to \nadd noise to your data and you add a sufficiently   large amount of noise, then we might be able to \nestimate the score accurately all over the space.",
    "start": "719600",
    "end": "733680"
  },
  {
    "text": "And now, of course, this is good because \nit means that we might be able to get  ",
    "start": "733680",
    "end": "739960"
  },
  {
    "text": "good information from our Langevin dynamics \nsampler. If we are relying on these arrows to  ",
    "start": "739960",
    "end": "745560"
  },
  {
    "text": "go towards high probability regions, \nLangevin dynamics will probably work   if we do this. The problem is that \nwe're no longer approximating.",
    "start": "745560",
    "end": "755520"
  },
  {
    "text": "We're no longer-- if you do Langevin \ndynamics over these estimated scores,  ",
    "start": "755520",
    "end": "761640"
  },
  {
    "text": "you're going to be producing samples from a \nnoisy data distribution. So you're going to be  ",
    "start": "761640",
    "end": "766720"
  },
  {
    "text": "generating images that look like this instead \nof generating images that look like this. So  ",
    "start": "766720",
    "end": "773560"
  },
  {
    "text": "that's the trade off here. Yes, we're going to \nbe able to estimate the score more accurately,  ",
    "start": "773560",
    "end": "779360"
  },
  {
    "text": "but we're estimating the score of the wrong thing. Basically, you talked about the \nscore estimation being the same  ",
    "start": "779360",
    "end": "788120"
  },
  {
    "text": "as denoising. So I guess, how is this \ndifferent than that concept? Because we   showed that score matching is \nequal to denoising. So to me,  ",
    "start": "788120",
    "end": "795800"
  },
  {
    "text": "I'm a bit confused that we're just adding noise \nto the data. So what are we doing differently? So before, what we were doing is we were \nestimating the score of the noisy data  ",
    "start": "795800",
    "end": "805520"
  },
  {
    "text": "distribution. And so here, if you were to \ndo this, yeah, you would be using the other   score matching. You would solve a denoising \nproblem. You would learn the score of the  ",
    "start": "805520",
    "end": "813680"
  },
  {
    "text": "noisy data distribution. Now you follow that \nscore, and you are producing noisy samples.",
    "start": "813680",
    "end": "819080"
  },
  {
    "text": "I see. So if we followed what we \ndid, we would get noisy images. We would get noisy images. That's \nthe problem. That's the trick.",
    "start": "819080",
    "end": "825160"
  },
  {
    "text": "So on the one hand, you would like sigma, \nthe amount of noise that you add, to be as   small as possible because then you're learning \nthe score of the clean data. So presumably,  ",
    "start": "826000",
    "end": "835320"
  },
  {
    "text": "if you follow those scores, you're going \nto generate clean samples. On the other   hand, if you do that, we're not \ngoing to be expected to learn  ",
    "start": "835320",
    "end": "843480"
  },
  {
    "text": "the score very accurately. And so that's \nthe dilemma that we have here, basically.",
    "start": "843480",
    "end": "849000"
  },
  {
    "text": "So it's essentially the difference between this \nand denoising score matching that denoising  ",
    "start": "849000",
    "end": "854040"
  },
  {
    "text": "score matching, we are putting noise on the \nreal data samples and estimating that noise  ",
    "start": "854040",
    "end": "859079"
  },
  {
    "text": "sample. But eventually, we're going to learn the \nnew data sample if the noise is small. But here,  ",
    "start": "859080",
    "end": "864560"
  },
  {
    "text": "we are estimating-- we're using the noise data \nsamples from the first place. And if we're using  ",
    "start": "864560",
    "end": "871880"
  },
  {
    "text": "denoising score matching, we're going to be adding \nnoise to the noise samples and then [INAUDIBLE] No, no, no. Actually, you could use denoising \nscore matching to estimate the score. In fact,  ",
    "start": "871880",
    "end": "880280"
  },
  {
    "text": "that's what we would end up doing. So if \nyou were to use denoising score matching,  ",
    "start": "880280",
    "end": "886200"
  },
  {
    "text": "you would take data, you would add noise, \nyou would solve the denoising problem. What you end up learning is the score of the  ",
    "start": "886200",
    "end": "893080"
  },
  {
    "text": "perturbed data density. So you end up \nlearning this, but that's not this,  ",
    "start": "893080",
    "end": "900880"
  },
  {
    "text": "which is what you wanted. It's not \nthe score of the clean data density. So in particular, if you were to then \nfollow those scores that you have here,  ",
    "start": "900880",
    "end": "912040"
  },
  {
    "text": "you would produce samples according to \ntheir noise-perturbed data density. And so  ",
    "start": "912040",
    "end": "917160"
  },
  {
    "text": "in particular, the images would \nlook like this, not like this. Are you saying this is equivalent to \njust doing denoising score matching?",
    "start": "917160",
    "end": "925640"
  },
  {
    "text": "So I understand how you do this. Or you could \neven do sliced score matching here or vanilla  ",
    "start": "925640",
    "end": "931440"
  },
  {
    "text": "score-- you could do sliced score matching, for \nexample, not vanilla, but you could do sliced   score matching here to estimate this. Denoising \nscore matching would be a much more natural choice  ",
    "start": "931440",
    "end": "941840"
  },
  {
    "text": "because it's faster, and it automatically gives \nyou the score of a noise-perturbed data density.",
    "start": "941840",
    "end": "947560"
  },
  {
    "text": "So here, I'm just saying, even if \nyou were able to estimate the scores,   they are not what you want. Using denoising \nscore matching would be a very natural way  ",
    "start": "947560",
    "end": "956240"
  },
  {
    "text": "of estimating these scores, and \nthat's what we're actually to do. So you were saying how real data logs \non low-dimensional manifolds. Can you  ",
    "start": "956240",
    "end": "966600"
  },
  {
    "text": "say that different words? What does it \nmean about in real-life data, the stuff  ",
    "start": "966600",
    "end": "973800"
  },
  {
    "text": "this looks at. What does that mean? \nHow does it even work? What is it? The manifold?",
    "start": "973800",
    "end": "979519"
  },
  {
    "text": "Yeah. Yeah. It basically means that-- if you \nrecall from the last slide, we were saying,  ",
    "start": "979520",
    "end": "986200"
  },
  {
    "text": "OK, if you were to do a PCA of the data and you \nkeep a sufficiently large number of components,  ",
    "start": "986200",
    "end": "992080"
  },
  {
    "text": "you would reconstruct the data almost perfectly, \nwhich basically means that the different  ",
    "start": "992080",
    "end": "997120"
  },
  {
    "text": "pixels in an image-- they are not linearly \nindependent. Once you know, a subset of them,  ",
    "start": "997120",
    "end": "1002720"
  },
  {
    "text": "you get all the others automatically, \nwhich basically means that the images   lie on some kind of plane, essentially, which \nis what I'm visualizing here with this shape.",
    "start": "1002720",
    "end": "1015079"
  },
  {
    "text": "So not all possible pixel values are actually \nvalid in the data distribution, essentially. There  ",
    "start": "1015080",
    "end": "1022520"
  },
  {
    "text": "is some kind of constraints, which you can think \nof it as encoding this kind of curve. And all the  ",
    "start": "1022520",
    "end": "1028079"
  },
  {
    "text": "images that we have in the data, they lie on this \nsurface or this curve in a high-dimensional space.  ",
    "start": "1028080",
    "end": "1034799"
  },
  {
    "text": "And so the score is not quite well defined because \nwhat does it mean to go off the curve? Then,  ",
    "start": "1034800",
    "end": "1041480"
  },
  {
    "text": "the probability is zero the moment you go off \nthe curve, and so it can explode, basically.",
    "start": "1041480",
    "end": "1048840"
  },
  {
    "text": "How does that work? Well, the moment you add noise, than basically \nany combination of pixel values is valid because  ",
    "start": "1053160",
    "end": "1059080"
  },
  {
    "text": "there's always some probability of adding the \nright amount of noise such that that combination   was possible. So if you imagine data that lies on \na plane or that kind of surface and then you add  ",
    "start": "1059080",
    "end": "1070320"
  },
  {
    "text": "noise, you're moving the value by a little bit, \nand then it's no longer lies on that plane or  ",
    "start": "1070320",
    "end": "1077480"
  },
  {
    "text": "no longer lies on the surface. So you're breaking \nthat. that constraint that held for the real data  ",
    "start": "1077480",
    "end": "1084559"
  },
  {
    "text": "no longer holds for noise-perturbed data, and that \nhelps estimating the gradient more accurately.",
    "start": "1084560",
    "end": "1093320"
  },
  {
    "text": "Real data, if it lies on a low-dimensional \nmanifold, then, there are points that are just   never occurred. But then here, when you're adding \nsuch small amounts of noise, it still feels like  ",
    "start": "1093320",
    "end": "1103480"
  },
  {
    "text": "there would be a type of low data problem where \nany really far-out, random assortment of pixels  ",
    "start": "1103480",
    "end": "1110240"
  },
  {
    "text": "is basically never going to occur even if it \nhas some very minimal probability. So basically,  ",
    "start": "1110240",
    "end": "1117720"
  },
  {
    "text": "a too small amount of noise, you do \nthis amount, and nothing really changes.",
    "start": "1117720",
    "end": "1122480"
  },
  {
    "text": "So yeah. So noise does become more stable, but \nyou do have this problem that, as you said,  ",
    "start": "1123480",
    "end": "1129040"
  },
  {
    "text": "if you add sufficiently small amount \nof noise, it's going to be more or   less. There is not really a discontinuity. So \nyeah, you had a very small amount of noise,  ",
    "start": "1129040",
    "end": "1138960"
  },
  {
    "text": "your noise-perturbed data distribution \nis very close to what you wanted,   so that's great. But you're not really solving \nthe problems that we have here, basically.",
    "start": "1138960",
    "end": "1146960"
  },
  {
    "text": "That's what I have in the next \nslide here. That's the question,   how much noise do we want to add? Do you want to \nadd a very little small amount of noise? Do you  ",
    "start": "1146960",
    "end": "1155280"
  },
  {
    "text": "want to add a of noise? If you think about the \ndifferent amount of noise that you can add,  ",
    "start": "1155280",
    "end": "1163920"
  },
  {
    "text": "you're going to get different trade offs. \nSo you can imagine that there is the real  ",
    "start": "1163920",
    "end": "1169960"
  },
  {
    "text": "data density. There is the real scores. And if \nyou try to estimate them using score matching,  ",
    "start": "1169960",
    "end": "1175559"
  },
  {
    "text": "there's going to be a lot of error \nin this region, as we discussed.",
    "start": "1175560",
    "end": "1181240"
  },
  {
    "text": "Then you could say, OK, now I'm going to add a \nlittle bit of noise. So I'm no longer estimating  ",
    "start": "1181240",
    "end": "1186640"
  },
  {
    "text": "the right thing, so there's going to be \na little bit of error everywhere because   I'm estimating noise-perturbed scores instead \nof true scores. But my estimation starts to  ",
    "start": "1186640",
    "end": "1195840"
  },
  {
    "text": "become a little bit better. And then you can \nadd even more noise. And then, at some point,  ",
    "start": "1195840",
    "end": "1202679"
  },
  {
    "text": "you are doing a great job at estimating the \nscores, but you're estimating the scores   of something completely wrong because \nyou added too much noise to your data.",
    "start": "1202680",
    "end": "1212320"
  },
  {
    "text": "And maybe that's the extreme where you add a \nton of noise. You've completely destroyed the  ",
    "start": "1212320",
    "end": "1217639"
  },
  {
    "text": "structure that was there in the data. So what \nyou're estimating has nothing to do with the  ",
    "start": "1217640",
    "end": "1223080"
  },
  {
    "text": "clean images that you started from, but you're \ndoing a very good job at estimating the score   because it becomes very easy. So those \nare the things that you need to balance.",
    "start": "1223080",
    "end": "1233120"
  },
  {
    "text": "We want to be able to estimate the score \naccurately, and so we would like to add as   much noise as possible to do that. But at the \nsame time, adding noise reduces the quality of  ",
    "start": "1233120",
    "end": "1242960"
  },
  {
    "text": "the things we generate because we're estimating \nthe score of a noise-perturbed data density. So  ",
    "start": "1242960",
    "end": "1248720"
  },
  {
    "text": "we're no longer estimating what we wanted, \nwhich is the thing that we have up here,   but we're estimating the score of a data \ndistribution with a lot of noise added.",
    "start": "1248720",
    "end": "1258600"
  },
  {
    "text": "So the denoising score matching thing-- so that is   just the first example and that \nis what it was restricted to?",
    "start": "1259440",
    "end": "1266760"
  },
  {
    "text": "You can estimate any of these. You \ncan use the noisy score matching to   estimate the score of any of these \nslices. But it's going to perform--  ",
    "start": "1266760",
    "end": "1275279"
  },
  {
    "text": "it might become very bad if the amount \nof noise that you add is very small. And so that's what you see here. \nWell, this is maybe the clean score,  ",
    "start": "1275280",
    "end": "1284440"
  },
  {
    "text": "or maybe this is a little bit of noise. Denoising \nscore matching is not going to work very well.",
    "start": "1284440",
    "end": "1290360"
  },
  {
    "text": "So denoising score matching is a technique that   you can use to estimate the \nscores for any given thing.",
    "start": "1290360",
    "end": "1296575"
  },
  {
    "text": "For any amount of noise. We're actually changing the points, or by \nadding noise, we're getting much more points. No. So the noisy score matching is a way of \nestimating the score of a noise-perturbed  ",
    "start": "1296575",
    "end": "1306679"
  },
  {
    "text": "data density with any amount of noise that you \nwant. The question is, how much noise do you   want to add? You'd like to add as little noise as \npossible because you want to estimate something  ",
    "start": "1306680",
    "end": "1315800"
  },
  {
    "text": "close to the real data. But the more noise you \nadd, the better estimation becomes. And so that's  ",
    "start": "1315800",
    "end": "1323960"
  },
  {
    "text": "the problem that you want to trade off these \ntwo things, and it's not clear how to do that.",
    "start": "1323960",
    "end": "1331024"
  },
  {
    "text": "Don't you feel like when we add a little \nnoise, it's working with the function that   has discontinuity features that if you do a \nperturbation figure out the gradient, it's  ",
    "start": "1331025",
    "end": "1341440"
  },
  {
    "text": "very easy to overshoot and undershoot, and then we \nmake it function smoother? So that we easy to find  ",
    "start": "1341440",
    "end": "1347200"
  },
  {
    "text": "a gradient, but it's not accurate anymore because \nyou're estimating the gradient on the wrong thing.",
    "start": "1347200",
    "end": "1353399"
  },
  {
    "text": "Yeah. So this is perhaps another way to \nthink about it. Imagine that somehow the  ",
    "start": "1353400",
    "end": "1358720"
  },
  {
    "text": "data lies on this curve, and this is \njust a curve in a 2D space. You know,  ",
    "start": "1358720",
    "end": "1364880"
  },
  {
    "text": "most of your samples are going to be close to that \nthick line that we have here. What's happening?",
    "start": "1364880",
    "end": "1376320"
  },
  {
    "text": "And so if you were to estimate the \nscore far away from the black curve,  ",
    "start": "1376320",
    "end": "1381840"
  },
  {
    "text": "it's going to be fairly inaccurate. Then you \ncan imagine, OK, let's add a lot of noise,  ",
    "start": "1381840",
    "end": "1387039"
  },
  {
    "text": "sigma 3. Then, most of the samples are going to \nbe pretty far away from the black curve. And so  ",
    "start": "1387040",
    "end": "1395120"
  },
  {
    "text": "we're going to get pretty good directional \ninformation when you're far away from a  ",
    "start": "1395120",
    "end": "1400320"
  },
  {
    "text": "clean sample. But it's going to be inaccurate the \nmoment you get closer to where the real data lies.",
    "start": "1400320",
    "end": "1408440"
  },
  {
    "text": "And then you can imagine a setting where you \nhave a ensemble of different noise levels.  ",
    "start": "1409360",
    "end": "1416240"
  },
  {
    "text": "You're not just considering a single noise \nlevel, but you are considering many of them.   And so that you are able to get good directional \ninformation, both when you're far away and when  ",
    "start": "1416240",
    "end": "1427040"
  },
  {
    "text": "we are a little bit closer and a little \nbit closer to the real data distribution.",
    "start": "1427040",
    "end": "1432840"
  },
  {
    "text": "And that's the main underlying idea of \na diffusion model or score-based model.  ",
    "start": "1432840",
    "end": "1438440"
  },
  {
    "text": "The key idea is that we're not just going to \nlearn the score of the data, or we're not just  ",
    "start": "1438960",
    "end": "1444480"
  },
  {
    "text": "going to learn the score of the data plus a single \namount of noise. But we're going to try to learn   the score of the data perturbed with different \nkinds of amounts of noise. That's the intuition.",
    "start": "1444480",
    "end": "1457840"
  },
  {
    "text": "And so specifically, we're going to consider \ndifferent amounts of noise-- sigma 1,   sigma 2, all the way to sigma L. And \nwe're going to use something called  ",
    "start": "1457840",
    "end": "1468680"
  },
  {
    "text": "annealed Langevin dynamics to basically \ngenerate samples. And the basic idea is  ",
    "start": "1468680",
    "end": "1474920"
  },
  {
    "text": "that when we initialize our Langevin dynamics \nprocedure, there's probably going to be very  ",
    "start": "1474920",
    "end": "1482400"
  },
  {
    "text": "little structure in the samples. They don't look \nlike natural images. And so what we can do is we  ",
    "start": "1482400",
    "end": "1488800"
  },
  {
    "text": "can follow the scores that were estimated for \nthe data distribution plus a lot of noise.",
    "start": "1488800",
    "end": "1497480"
  },
  {
    "text": "And for a little bit, if you \nwere to keep running this thing,   then you would be able to generate samples \nfrom the data distribution plus a lot of noise,  ",
    "start": "1497480",
    "end": "1506800"
  },
  {
    "text": "which is not what we want. But what we can \ndo is we can use these samples to initialize  ",
    "start": "1506800",
    "end": "1513000"
  },
  {
    "text": "another Langevin dynamics procedure where \nwe've decreased the amount of noise by a   little bit. And then you basically keep \nrunning your Langevin dynamics procedure,  ",
    "start": "1513000",
    "end": "1524120"
  },
  {
    "text": "following the scores corresponding to the data \ndensity plus a smaller amount of noise at sigma 2.",
    "start": "1524120",
    "end": "1531600"
  },
  {
    "text": "Then you decrease it even more, and you initialize \nbecause you got closer and closer to the high data  ",
    "start": "1531600",
    "end": "1538240"
  },
  {
    "text": "density regions, then we know that now we are \nstarting to see more structure in the data. And  ",
    "start": "1538240",
    "end": "1543960"
  },
  {
    "text": "so we should follow the score for the data density \nplus, let's say, a very small amount of noise. And  ",
    "start": "1543960",
    "end": "1551799"
  },
  {
    "text": "then again, you follow the arrows, and then you're \ngenerating samples like the ones we actually want.",
    "start": "1551800",
    "end": "1559400"
  },
  {
    "text": "But at this point, we can get the best of both \nworlds because at the end of this procedure,   we're generating samples from data plus a very \nsmall amount of noise. But throughout the sampling  ",
    "start": "1559400",
    "end": "1571840"
  },
  {
    "text": "procedure, we are always getting relatively \naccurate estimates of the scores because we  ",
    "start": "1571840",
    "end": "1577559"
  },
  {
    "text": "are considering multiple noise scales. So at \nthe very beginning where there was no structure   in the data, we were following the score \ncorresponding to data plus a lot of noise.",
    "start": "1577560",
    "end": "1587440"
  },
  {
    "text": "And then as we add more and more structure to \nthe data, because we are moving towards higher   probability regions by following these arrows, \nthen we can afford to reduce the-- basically  ",
    "start": "1587440",
    "end": "1598279"
  },
  {
    "text": "consider the gradients of data that was perturbed \nwith smaller amount of noise. And this procedure  ",
    "start": "1598280",
    "end": "1606760"
  },
  {
    "text": "will get us the best of both worlds because \nLangevin dynamics is never lost. We're always   following a pretty accurate estimates of the \ngradient. But at the same time, at the end,  ",
    "start": "1606760",
    "end": "1618440"
  },
  {
    "text": "we're able to generate samples for a distribution \nof data plus noise where this noise level, sigma  ",
    "start": "1618440",
    "end": "1624559"
  },
  {
    "text": "3, can be very, very small. So this final samples \nthat you produce are going to be almost clean.",
    "start": "1624560",
    "end": "1631858"
  },
  {
    "text": "Are there any studies on how many different noises \nand what the sequence decay should look like? Yeah. So typically, people use \n1,000. That's the magic number.",
    "start": "1631858",
    "end": "1641159"
  },
  {
    "text": "But in the second part of the lecture, \nwe'll talk about an infinite number   of noise levels. So the natural way to do \nthings is to actually consider continuous  ",
    "start": "1641160",
    "end": "1650320"
  },
  {
    "text": "numbers. That's what gets you the best of-- a \nlot of structure, a lot of interesting things.",
    "start": "1652160",
    "end": "1661400"
  },
  {
    "text": "So that's the intuition. And you can see here \nanother example of what happens if you were to  ",
    "start": "1661400",
    "end": "1668520"
  },
  {
    "text": "just run Langevin dynamics. It has this problem \nwhere you're seeing too many particles down here  ",
    "start": "1668520",
    "end": "1678120"
  },
  {
    "text": "because it doesn't mix sufficiently rapidly. And even though there should be more probability \nmass up here, meaning that more particles should  ",
    "start": "1679160",
    "end": "1686480"
  },
  {
    "text": "end up up here, there's too many down here \nbecause the arrows are basically down. You're not  ",
    "start": "1686480",
    "end": "1692880"
  },
  {
    "text": "estimating things accurately. And if you do anneal \nLangevin of dynamics, so you use this procedure  ",
    "start": "1692880",
    "end": "1698640"
  },
  {
    "text": "where you run multiple longevity dynamics chains \ncorresponding to different amounts of noise,  ",
    "start": "1698640",
    "end": "1704680"
  },
  {
    "text": "then it ends up giving you the right distribution \nwhere you see there is many fewer particles  ",
    "start": "1704680",
    "end": "1709760"
  },
  {
    "text": "down here representing the fact that there \nshould be less probability mass down there.",
    "start": "1709760",
    "end": "1718080"
  },
  {
    "text": "And yeah, here is another example showing this, \nbut let me skip. So what does it mean in practice?  ",
    "start": "1718080",
    "end": "1725760"
  },
  {
    "text": "What it means in practice is that in order to do \nthis, you need to be able to estimate the score,  ",
    "start": "1725760",
    "end": "1733320"
  },
  {
    "text": "not just of the data density, not just \nof the data density plus a certain   fixed amount of noise, but you need to be \nable to jointly estimate the score of the  ",
    "start": "1733320",
    "end": "1743640"
  },
  {
    "text": "data plus different amounts of noise levels-- \nvarious amounts of noise levels. So you need  ",
    "start": "1743640",
    "end": "1749840"
  },
  {
    "text": "to be able to know what a z-score of data \nplus a lot of noise. You need to be able   to know what a score of data plus a little \nbit less noise, all the way down to a very,  ",
    "start": "1749840",
    "end": "1759600"
  },
  {
    "text": "very small amount of noise added to the data, \nwhere it's almost the true data density.",
    "start": "1759600",
    "end": "1768160"
  },
  {
    "text": "And that's fine because if you do anneal Langevin \ndynamics, even though this score is only ever  ",
    "start": "1768160",
    "end": "1773280"
  },
  {
    "text": "going to be estimated accurately close to the high \ndata density regions, we still have that problem.  ",
    "start": "1773280",
    "end": "1779080"
  },
  {
    "text": "This score here is not going to be estimated \naccurately everywhere. It's only ever going to   be estimated accurately when you're very close to, \nlet's say, a real image. But that's fine because  ",
    "start": "1779080",
    "end": "1789520"
  },
  {
    "text": "we're using the Langevin dynamics procedure, and \nwe're only going to use this score model towards   the end of the sampling, where we already have a \npretty good guess of the kind of images we want.",
    "start": "1789520",
    "end": "1800000"
  },
  {
    "text": "While this score here, which \nis data plus a ton of noise,   is going to be estimated pretty accurately \neverywhere, it's going to be good at the  ",
    "start": "1800000",
    "end": "1808080"
  },
  {
    "text": "beginning of the sampling. But we don't want \nto just keep following that because we want   to be able to sample from something \nclose to the clean data distribution.",
    "start": "1808080",
    "end": "1818960"
  },
  {
    "text": "And so to make things efficient, what \nwe would do is we would have-- you could  ",
    "start": "1818960",
    "end": "1825520"
  },
  {
    "text": "potentially train separate score networks, \none for every noise level. If you have,  ",
    "start": "1825520",
    "end": "1831320"
  },
  {
    "text": "let's say, 1,000 noise levels, that would \nmean 1,000 different neural networks,   kind of like training, each one being \ntrained on a different kind of vector field.",
    "start": "1831320",
    "end": "1843320"
  },
  {
    "text": "To make things more efficient in practice, what \nyou can do is you can have a single neural network  ",
    "start": "1843320",
    "end": "1849360"
  },
  {
    "text": "that takes an additional input parameter sigma, \nwhich is basically just the amount of noise that  ",
    "start": "1849360",
    "end": "1855799"
  },
  {
    "text": "we're considering. And the single neural network \nwill jointly estimate all these different vector  ",
    "start": "1855800",
    "end": "1861760"
  },
  {
    "text": "fields. So when you fit in a large value \nof sigma here as an input, then the network  ",
    "start": "1861760",
    "end": "1868000"
  },
  {
    "text": "knows that it should be estimating the vector \nfield for, let's say, data plus a lot of noise.",
    "start": "1868000",
    "end": "1873520"
  },
  {
    "text": "While when you fit in as an input, a small \nvalue of sigma, then the network knows that   it needs to estimate the vector field for \ndata plus a small amount of noise. And so  ",
    "start": "1873520",
    "end": "1885040"
  },
  {
    "text": "this is just basically a way to make the \ncomputation a lot more efficient because   we have now a single model that is trained to \nsolve all these different estimation problems.  ",
    "start": "1885040",
    "end": "1895520"
  },
  {
    "text": "It's going to be worse than just training 1,000 \nseparate models, but it's going to be much more   efficient because we're just training \na single neural network at that point.",
    "start": "1895520",
    "end": "1905460"
  },
  {
    "text": "So in this case, we're learning a vector field, \nnot a probability distribution, right? But  ",
    "start": "1905460",
    "end": "1912600"
  },
  {
    "text": "when you do the score matching for energy-based \nmodels, in that case, are we learning the solution  ",
    "start": "1912600",
    "end": "1918160"
  },
  {
    "text": "to an ODE that has a closed form solution? Or are \nwe implicitly assuming that because we're taking  ",
    "start": "1918160",
    "end": "1923840"
  },
  {
    "text": "a gradient of a probability distribution \ninstead of just learning a vector field?",
    "start": "1923840",
    "end": "1929159"
  },
  {
    "text": "So yeah. So what we're learning here-- so these \nvector fields are not necessarily conservative  ",
    "start": "1929160",
    "end": "1935280"
  },
  {
    "text": "unless you parameterize the network in a certain \nway. You could potentially parameterize the  ",
    "start": "1935280",
    "end": "1940800"
  },
  {
    "text": "network such that it's the gradient of an energy \nfunction. Actually, it doesn't hurt performance   too much if you do that, but it doesn't actually \nseem to help. So in practice, you can just use a  ",
    "start": "1940800",
    "end": "1950440"
  },
  {
    "text": "free-form neural network that goes from, say, \nimages to images, and that's not a problem.",
    "start": "1950440",
    "end": "1957840"
  },
  {
    "text": "But you're right that it's not necessarily the \ngradient of a potential of an energy function,   and so weird things can happen, \nwhere if you follow a loop,  ",
    "start": "1957840",
    "end": "1966159"
  },
  {
    "text": "the probability can go up or down, even though \nif there was really an underlying energy,  ",
    "start": "1966160",
    "end": "1971760"
  },
  {
    "text": "it shouldn't change the probability. So that could \nbe a problem. But in practice, it works. Yeah?",
    "start": "1971760",
    "end": "1980907"
  },
  {
    "text": "For a bigger noise level, you can \ngenerate multiple perturbed data   points with one real data point. So \nwe got a hyperparameter based on--",
    "start": "1980907",
    "end": "1991080"
  },
  {
    "text": "The number of noise levels? Is \nthat the question, whether that's-- Given the noise level, you can still generate \nmultiple data points with one real data point.",
    "start": "1991080",
    "end": "2000120"
  },
  {
    "text": "So you mean when we run the Langevin procedure? When you generate the training data for the score.",
    "start": "2000960",
    "end": "2007880"
  },
  {
    "text": "Yeah. Yeah, you can generate-- so we haven't \ntalked about how we actually learn it. Right now,  ",
    "start": "2007880",
    "end": "2013360"
  },
  {
    "text": "I'm just saying, what does the model \nlook like? So the model is going to be   a single neural network that will \ntry to jointly estimate all these  ",
    "start": "2013360",
    "end": "2020640"
  },
  {
    "text": "scores. How do we actually learn it? \nWe're going to learn it by denoising  ",
    "start": "2020640",
    "end": "2025920"
  },
  {
    "text": "score matching. So there's going to be \nthis noise conditional score network,   which is going to be a network that jointly \nestimates all these vector field of scores.",
    "start": "2025920",
    "end": "2035960"
  },
  {
    "text": "And how should we train this? You could \ndo slight score matching. It's much more  ",
    "start": "2035960",
    "end": "2042120"
  },
  {
    "text": "natural to just use denoising score matching. \nSince denoising score matching already gives   you the score of a noise-perturbed data \ndensity, you might as well directly use  ",
    "start": "2042120",
    "end": "2051480"
  },
  {
    "text": "that. So since we were trying to model \nscores of data plus noise, we might as  ",
    "start": "2051480",
    "end": "2059240"
  },
  {
    "text": "well just directly use denoising score matching \nbecause that's a little bit more efficient.",
    "start": "2059240",
    "end": "2064399"
  },
  {
    "text": "And then the loss function is going to be \na weighted combination of denoising score  ",
    "start": "2064400",
    "end": "2070760"
  },
  {
    "text": "matching losses because we want to jointly solve, \nlet's say, 1,000 different tasks. And so the loss  ",
    "start": "2070760",
    "end": "2078960"
  },
  {
    "text": "function might look something like this, where \nwe have this noise-conditional score network  ",
    "start": "2078960",
    "end": "2085119"
  },
  {
    "text": "that takes as input a data point and a noise \nlevel and tries to estimate the score of the  ",
    "start": "2085120",
    "end": "2090200"
  },
  {
    "text": "data distribution perturbed with that noise level \nat that point x. And we want to train this score  ",
    "start": "2090200",
    "end": "2097880"
  },
  {
    "text": "network to perform well across different \nnoise levels. So if you have L noise levels,  ",
    "start": "2097880",
    "end": "2103839"
  },
  {
    "text": "this noise-conditional score network should be \nable to solve all these different regression   problems as well as possible. And so there is \na lambda sigma i parameter here that basically  ",
    "start": "2105000",
    "end": "2115720"
  },
  {
    "text": "controls how much you care about estimating \naccurately the scores at different noise levels.",
    "start": "2115720",
    "end": "2124560"
  },
  {
    "text": "Since the data is already perturbed now, \nwe don't add an additional perturbations?",
    "start": "2124560",
    "end": "2129840"
  },
  {
    "text": "Yeah. So the log would look like this. \nSo the data is clean, as you said. And then you add noise corresponding to the \nsigma that you care about, and then you try  ",
    "start": "2129840",
    "end": "2139520"
  },
  {
    "text": "to denoise that data point. And so if you \nthink of it from the denoising perspective,   we're not just learning to denoise data that has \nbeen perturbed with a fixed amount of noise, but  ",
    "start": "2139520",
    "end": "2149240"
  },
  {
    "text": "we're basically learning a family of denoisers, \neach one working with different amounts of noise.  ",
    "start": "2149240",
    "end": "2155720"
  },
  {
    "text": "So there is a denoiser that works when the data \nis corrupted with a very large amount of noise,  ",
    "start": "2155720",
    "end": "2161080"
  },
  {
    "text": "and there's going to be a denoiser that \nworks when the data is corrupted with   a smaller amount of noise, all the way down \nto almost no noise being added to the data.",
    "start": "2161080",
    "end": "2170319"
  },
  {
    "text": "And these are different denoising \nproblems and equivalently corresponding   to estimating the scores of different \nnoise-perturbed data distributions.",
    "start": "2170320",
    "end": "2183680"
  },
  {
    "text": "[INAUDIBLE] longer than asked to the real samples. Yeah. So when the noise is very large, \nbasically denoising is very hard. If I  ",
    "start": "2183680",
    "end": "2192880"
  },
  {
    "text": "add an infinite amount of noise, basically \nall the structure and the original data is   lost. And the best you can do is to \nbasically output the average image,  ",
    "start": "2192880",
    "end": "2202320"
  },
  {
    "text": "essentially. That's the only thing you can do. \nThere is no information in x tilde about x.",
    "start": "2202320",
    "end": "2208560"
  },
  {
    "text": "So the best thing you can do is to basically-- \nif you're trying to minimize an L2 loss to   predict the mean. And you can imagine \nthat if that's the only thing you have,  ",
    "start": "2208560",
    "end": "2217560"
  },
  {
    "text": "you're not going to be able to generate good \nimages. But because you know also how to denoise   images with less amounts of noise, then if you \nknow all these score models, then you can do that  ",
    "start": "2217560",
    "end": "2228440"
  },
  {
    "text": "annealed Langevin dynamics procedure, and you \ncan actually generate clean images at the end.",
    "start": "2228440",
    "end": "2234960"
  },
  {
    "text": "You said we can customize the Lambda \nfunction to assign weights. But from  ",
    "start": "2234960",
    "end": "2241200"
  },
  {
    "text": "a practical standpoint, what sort \nof a lambda are we looking for it? So yeah, we'll get to what lambdas make sense. \nIn theory, if you had infinite capacity,  ",
    "start": "2241200",
    "end": "2251119"
  },
  {
    "text": "this model is arbitrarily powerful. It doesn't \neven matter because it could solve perfectly   each task. In practice, it matters how you \nweight this different score matching losses.",
    "start": "2251120",
    "end": "2263480"
  },
  {
    "text": "We'll get to that. And yeah, but the loss function \nbasically looks like this. It's a mixture of  ",
    "start": "2263480",
    "end": "2270640"
  },
  {
    "text": "denoising score-matching objectives across all the \ndifferent noise levels that we are considering.",
    "start": "2271640",
    "end": "2277440"
  },
  {
    "text": "Would I be correct in saying that \nif you have different noise levels,   then would the gradients be in some way \nrelated by some scaling factor? Because  ",
    "start": "2278040",
    "end": "2285520"
  },
  {
    "text": "the general direction is the same. So to \nme, I'm a bit-- I guess, why can't we just  ",
    "start": "2285520",
    "end": "2290600"
  },
  {
    "text": "learn one model and just scale it? Because \nlike learning rate-- so if you're far away,  ",
    "start": "2290600",
    "end": "2296680"
  },
  {
    "text": "try to estimate it from-- try to get an \nestimate of the gradient, just scale it  ",
    "start": "2296680",
    "end": "2301480"
  },
  {
    "text": "by a big amount and move in a direction. And \nas you get closer, just taper it down so that  ",
    "start": "2302480",
    "end": "2307720"
  },
  {
    "text": "you'll converge. So why not do something like \nthat? Because if you assume a Gaussian shape,  ",
    "start": "2307720",
    "end": "2313800"
  },
  {
    "text": "then I assume that we should be able to scale \ngradients also-- or scale the score, I mean.",
    "start": "2313800",
    "end": "2317960"
  },
  {
    "text": "So it's true that they are all related to each \nother. So in theory, if you know the score at  ",
    "start": "2319000",
    "end": "2325200"
  },
  {
    "text": "a particular noise level, you can, in theory, \nrecover the score of different noise levels. But  ",
    "start": "2325200",
    "end": "2331400"
  },
  {
    "text": "it's not just the scaling. So there is something \ncalled the Fokker-Planck equation. And if you were  ",
    "start": "2331400",
    "end": "2337000"
  },
  {
    "text": "to solve of that equation, which is just the PDE \nat the end of the day, but if you were able to  ",
    "start": "2337000",
    "end": "2342880"
  },
  {
    "text": "solve that PDE, that tells you basically \nhow the scores are related to each other.",
    "start": "2342880",
    "end": "2348059"
  },
  {
    "text": "So that's deterministic but intractable, I see. Yeah. You can try to enforce it. There are \npapers. We have a number of papers trying  ",
    "start": "2348060",
    "end": "2358200"
  },
  {
    "text": "to enforce that condition because, in some \nsense, this is just treating all these tasks   as being independent. But we know that they are \nrelated to each other, as you were suggesting.",
    "start": "2358200",
    "end": "2368680"
  },
  {
    "text": "And so you might be able to do better if you \ntie together the losses because you know how  ",
    "start": "2368680",
    "end": "2375599"
  },
  {
    "text": "to go from one solution to the other solution. \nIn practice, it doesn't seem to help a lot. But  ",
    "start": "2375600",
    "end": "2380800"
  },
  {
    "text": "you're right. There is something called-- yeah, \nif you could solve the Fokker-Planck equation,   you could go from-- yeah, go from any score to \nany other score, at least in continuous time.",
    "start": "2380800",
    "end": "2394440"
  },
  {
    "text": "Cool. So now we have several choices   to make. We need to choose what kind of \nnoise scales are we going to consider.",
    "start": "2394440",
    "end": "2403080"
  },
  {
    "text": "So we need to decide on what is the maximum \namount of noise that we're going to add. We   need to decide the minimum amount of noise that \nwe're going to add, and we need to decide how to  ",
    "start": "2403080",
    "end": "2414280"
  },
  {
    "text": "step in between these two extremes, essentially. \nAnd for the maximum noise scale, you probably  ",
    "start": "2414280",
    "end": "2422520"
  },
  {
    "text": "want to choose it to be roughly the maximum \npairwise distance between any two data points.",
    "start": "2422520",
    "end": "2429680"
  },
  {
    "text": "And the idea is like if you have two images in \nthe data, x1 and x2, you want the amount of noise  ",
    "start": "2429680",
    "end": "2438280"
  },
  {
    "text": "to be sufficiently large so that basically it's \npossible to go from one data point to the other  ",
    "start": "2438280",
    "end": "2445320"
  },
  {
    "text": "data point if you were to add noise, essentially. \nSo if you start from data point 1 and you were to  ",
    "start": "2445320",
    "end": "2451640"
  },
  {
    "text": "add a sufficiently large amount of noise, there \nshould be a reasonable probability of generating   a data point 2-- an equivalently of going back on \nthe other direction. And this basically ensures  ",
    "start": "2451640",
    "end": "2465680"
  },
  {
    "text": "that-- like at the beginning when you start out \nyour Langevin dynamics procedure with a lot of  ",
    "start": "2465680",
    "end": "2472040"
  },
  {
    "text": "noise, that's going to mix. It's going to explore \nthe space pretty efficiently because there is a  ",
    "start": "2472040",
    "end": "2479200"
  },
  {
    "text": "way to go basically from any point to any other \npoint. That's the intuition for this choice.",
    "start": "2479200",
    "end": "2487240"
  },
  {
    "text": "The minimum noise scale, you probably want it \nto be sufficiently small so that the image plus  ",
    "start": "2489040",
    "end": "2495600"
  },
  {
    "text": "noise is hard to distinguish from just a clean \nimage. So the minimum noise scale should be very,  ",
    "start": "2495600",
    "end": "2504040"
  },
  {
    "text": "very small. And the other thing to decide is \nhow you go from the maximum to the minimum.  ",
    "start": "2504040",
    "end": "2516160"
  },
  {
    "text": "So how do you interpolate between these two \nextremes? And again, the idea is that if you  ",
    "start": "2516160",
    "end": "2521240"
  },
  {
    "text": "think about that Langevin dynamics procedure, \nwe want to make sure that these different noise  ",
    "start": "2521240",
    "end": "2529119"
  },
  {
    "text": "scales have sufficiently overlap so that \nwhen you initialize the Langevin dynamics  ",
    "start": "2529120",
    "end": "2534560"
  },
  {
    "text": "chain corresponding to the next noise level, \nyou're starting with something that makes sense.",
    "start": "2534560",
    "end": "2540280"
  },
  {
    "text": "So if you imagine that you have these kind of \nspheres where you are increasingly corresponding  ",
    "start": "2540280",
    "end": "2547760"
  },
  {
    "text": "to what you would get or the probability as \nyou increase the amount of noise that you add,  ",
    "start": "2547760",
    "end": "2554560"
  },
  {
    "text": "and you go sigma 2, sigma 1, sigma 3, essentially \nwhat you want to make sure is that when you have,  ",
    "start": "2555640",
    "end": "2562200"
  },
  {
    "text": "let's say, data plus noise level sigma 2, there \nshould be sufficiently overlap with the kind of  ",
    "start": "2562200",
    "end": "2569839"
  },
  {
    "text": "data points you expect when you have data plus \nnoise level sigma 3, so that when you use the  ",
    "start": "2569840",
    "end": "2576600"
  },
  {
    "text": "samples that were obtained by running dynamics \nwith sigma 2 noise levels and you use them to  ",
    "start": "2576600",
    "end": "2582440"
  },
  {
    "text": "initialize the Langevin chain corresponding to \nnoise level sigma 3, you have something that  ",
    "start": "2582440",
    "end": "2588480"
  },
  {
    "text": "makes sense. Because if there is no overlap, so \nyou have a drastic reduction in noise level. So  ",
    "start": "2588480",
    "end": "2596080"
  },
  {
    "text": "you go from a lot of noise to very little noise. \nAfter running your Langevin chain for the large  ",
    "start": "2596080",
    "end": "2601720"
  },
  {
    "text": "amount of noise, you're not going to get a \ngood initialization for the next noise level.  ",
    "start": "2601720",
    "end": "2607119"
  },
  {
    "text": "But if there is a decent amount of \noverlap between these noise levels,  ",
    "start": "2607840",
    "end": "2613080"
  },
  {
    "text": "then you expect this annealed Langevin dynamics \nprocedure to actually work pretty well. Yeah?",
    "start": "2613080",
    "end": "2624080"
  },
  {
    "text": "How can we estimate that other than just guessing? Yeah. So we know we are deciding the sigmas, \nright? And so what you can do is you can actually  ",
    "start": "2624080",
    "end": "2634600"
  },
  {
    "text": "work out a little bit of math about what makes \nsense. And it makes sense, according to this  ",
    "start": "2634600",
    "end": "2641720"
  },
  {
    "text": "heuristic, to basically use some kind of geometric \nprogression between the different noise levels.",
    "start": "2641720",
    "end": "2649800"
  },
  {
    "text": "This ensures that there is sufficiently \noverlap between the different shells  ",
    "start": "2649800",
    "end": "2655160"
  },
  {
    "text": "that you get as you increase the-- or \nyou decrease the amount of noise that   you add. And this is a heuristic. It's \nnot necessarily the only valid choice,  ",
    "start": "2655160",
    "end": "2663920"
  },
  {
    "text": "but this is like the first one \nthat we did that seemed to work.",
    "start": "2663920",
    "end": "2669799"
  },
  {
    "text": "The other kind of thing we can decide \nis the weighting factor. Remember,  ",
    "start": "2669800",
    "end": "2675680"
  },
  {
    "text": "we're jointly solving all these estimation \nproblems, corresponding to different noise   levels. How much should we care about the \ndifferent noise levels? Which is controlled  ",
    "start": "2675680",
    "end": "2686840"
  },
  {
    "text": "by this lambda sigma i hyperparameter that \ndecides how much weight do you put on the  ",
    "start": "2686840",
    "end": "2692360"
  },
  {
    "text": "different components of the loss. And so \nhow do we choose this weighting function?",
    "start": "2692360",
    "end": "2699600"
  },
  {
    "text": "The idea is that you want to balance the \nimportance of the different components   in the loss. And a reasonable heuristic \nthat, again, works well in practice is to  ",
    "start": "2699600",
    "end": "2710720"
  },
  {
    "text": "choose that to be equal to the amount of \nnoise that you're adding at that level.",
    "start": "2710720",
    "end": "2716560"
  },
  {
    "text": "Doesn't this encourage us to ignore \nthe more fine-tuned noise levels?",
    "start": "2716560",
    "end": "2725800"
  },
  {
    "text": "It's because the size of the arrow changes \nalso the way it's scaled. Do I have it here?  ",
    "start": "2725800",
    "end": "2733760"
  },
  {
    "text": "So the loss would look something like \nthis. And basically-- yeah. Essentially,  ",
    "start": "2733760",
    "end": "2741560"
  },
  {
    "text": "you actually end up carrying the same \nabout the different noise levels if  ",
    "start": "2741560",
    "end": "2746680"
  },
  {
    "text": "you do that choice because of \nthe various scaling factors.",
    "start": "2746680",
    "end": "2752119"
  },
  {
    "text": "So remember, there is a sigma i here. So again,  ",
    "start": "2752120",
    "end": "2760120"
  },
  {
    "text": "it's a choice. Other choices can work as well, \nbut this is the, the thing that we did. Yeah?",
    "start": "2760120",
    "end": "2768160"
  },
  {
    "text": "I noticed this is similar to what I \nthink someone asked about before that   the epsilon theta is defined \nas basically a scaled version  ",
    "start": "2768160",
    "end": "2773920"
  },
  {
    "text": "of the score. Is there some meaning \nbehind what epsilon theta represents? Yeah So epsilon theta here is basically \na noise prediction because it's basically  ",
    "start": "2773920",
    "end": "2783880"
  },
  {
    "text": "literally just estimating the noise \nthat was added. There is a different   parameterization where you might \nwant to predict-- as we discussed,  ",
    "start": "2783880",
    "end": "2790480"
  },
  {
    "text": "when the noise level is very high, then you \nmight want to predict the mean. So there are   different ways of parameterizing what the \nnetwork is supposed to output. This is the  ",
    "start": "2793560",
    "end": "2803920"
  },
  {
    "text": "simplest one where you're just predicting \nthe it's a noise prediction kind of task.",
    "start": "2803920",
    "end": "2811760"
  },
  {
    "text": "And so the final loss looks something like this. \nYou basically sample a mini-batch of data points.  ",
    "start": "2811760",
    "end": "2819240"
  },
  {
    "text": "Then, you sample a mini-batch of noise indices. \nSo you basically equivalently choose a bunch of  ",
    "start": "2819240",
    "end": "2826840"
  },
  {
    "text": "noise levels, one per data point, let's say, \nuniformly across the different noise scales  ",
    "start": "2826840",
    "end": "2835480"
  },
  {
    "text": "that we're willing to consider. Maybe L here could \nbe 1,000 if you have 1,000 different noise levels.",
    "start": "2835480",
    "end": "2843560"
  },
  {
    "text": "And then what you do is you sample noise,   IID one noise vector per data point. And then, \nyou basically train the score network to solve  ",
    "start": "2843560",
    "end": "2859000"
  },
  {
    "text": "the denoising problem for each data point, \nessentially, with the weighting function that  ",
    "start": "2859000",
    "end": "2865160"
  },
  {
    "text": "we had before. And then you basically just \ndo stochastic gradient descent on this loss,  ",
    "start": "2865160",
    "end": "2873640"
  },
  {
    "text": "trying to essentially find parameters \ntheta that minimize this denoising loss,  ",
    "start": "2873640",
    "end": "2880279"
  },
  {
    "text": "which is equivalent to essentially estimating \nthe scores of the data density perturbed with  ",
    "start": "2880280",
    "end": "2887880"
  },
  {
    "text": "these various noise levels as well as you can. And \nso basically, everything is just as efficient as  ",
    "start": "2887880",
    "end": "2898680"
  },
  {
    "text": "training a single score model because everything \nis amortized, and there's a single core network  ",
    "start": "2898680",
    "end": "2905599"
  },
  {
    "text": "that is jointly trained to estimate the score of \ndata plus noise at different noise intensities.",
    "start": "2905600",
    "end": "2913600"
  },
  {
    "text": "And so the final thing looks like \nthis. You have data, clean data,  ",
    "start": "2913600",
    "end": "2920240"
  },
  {
    "text": "and then you have noise at the end. And so there \nis going to be correspondingly different versions  ",
    "start": "2920240",
    "end": "2931080"
  },
  {
    "text": "of the data distribution that has been perturbed \nwith increasingly large amounts of noise going  ",
    "start": "2931080",
    "end": "2936920"
  },
  {
    "text": "from clean data, mediumly perturbed \ndata, all the way to data plus a ton   of noise where basically the structure \nis completely destroyed. So visually,  ",
    "start": "2936920",
    "end": "2947160"
  },
  {
    "text": "you can think of clean data, data plus a little \nbit of noise, more noise, more noise, more noise,   all the way to huge amount of noise, where you \ndon't even recognize what you started from.",
    "start": "2947160",
    "end": "2960920"
  },
  {
    "text": "In terms of noise scheduling, you \nalways have go from the large to small.  ",
    "start": "2961440",
    "end": "2966720"
  },
  {
    "text": "The geometry sequence will always \nhave or have different noise?",
    "start": "2968080",
    "end": "2973960"
  },
  {
    "text": "So you're thinking of increasing and \ndecreasing and doing other things? I think,  ",
    "start": "2973960",
    "end": "2983119"
  },
  {
    "text": "in principle, you could do this kind \nof. Actually, yeah, people-- I mean,   you can. So during learning, I guess there is \nalways going to be-- I mean, it's a scalar,  ",
    "start": "2983120",
    "end": "2993200"
  },
  {
    "text": "and so there's always going to be an ordering. And \nso you're always going to be considering. You're   always going to go from the smallest \namount to the largest amount of noise.",
    "start": "2993200",
    "end": "3000079"
  },
  {
    "text": "During inference, it might make sense to do \ndifferent things than what I described is  ",
    "start": "3000080",
    "end": "3007040"
  },
  {
    "text": "annealed Langevin dynamics, where you would \nstart from a lot of noise and you go all the   way to clean data. There are versions \nwhere you might not want to do this,  ",
    "start": "3007040",
    "end": "3016040"
  },
  {
    "text": "and so you might want to go from noise to \ndata, and then maybe a little bit of noise,  ",
    "start": "3016040",
    "end": "3021320"
  },
  {
    "text": "and then go back. So yeah, there is a \nlot of flexibility then at the inference.",
    "start": "3021320",
    "end": "3028680"
  },
  {
    "text": "A training that you need to estimate all of \nthem. And there's always going to be a mean,   and there's going to be a max. And then you \ncan think about how you space them. Say again?",
    "start": "3028680",
    "end": "3040900"
  },
  {
    "text": "A parallel process training-- \ntrain them in parallel.",
    "start": "3040900",
    "end": "3046039"
  },
  {
    "text": "We train all of them in parallel, yeah. \nYeah, yeah, yeah. So all of them is--   there is a single model that is jointly trained \nto solve all these denoising tasks. And in fact,  ",
    "start": "3046040",
    "end": "3058000"
  },
  {
    "text": "you can get better performance if you're \nwilling to consider multiple-- like,   some of the state-of-the-art models that \nare out there, they don't have a single one.",
    "start": "3058000",
    "end": "3065360"
  },
  {
    "text": "They might have a few of them. Because they can \nafford to train a bunch of different noise models,  ",
    "start": "3065360",
    "end": "3071240"
  },
  {
    "text": "it might make sense to do it because this \nis purely like a computational trick. If  ",
    "start": "3071240",
    "end": "3076760"
  },
  {
    "text": "you could, it would be better to separately \nestimate the scores for every noise level.",
    "start": "3076760",
    "end": "3082920"
  },
  {
    "text": "And so then there's going to be different \nnoise levels. For every noise level,   there's going to be a corresponding data \ndensity plus noise, which we're going to  ",
    "start": "3082920",
    "end": "3091600"
  },
  {
    "text": "denote sigma 1. And this is the same as the q \nsigma that we had at the beginning. So you can   think of it as data becoming increasingly \ncorrupted as you go from left to right.",
    "start": "3091600",
    "end": "3103160"
  },
  {
    "text": "For each one of these noise levels, there is \ngoing to be a corresponding score model. So  ",
    "start": "3103160",
    "end": "3109319"
  },
  {
    "text": "all these scores are going to be different \nbecause we are adding increasingly large   amounts of noise. And then there's going to \nbe a single neural network that is jointly  ",
    "start": "3109320",
    "end": "3117800"
  },
  {
    "text": "trained to estimate all these vector fields. \nAnd the network takes the data x where you are  ",
    "start": "3117800",
    "end": "3123520"
  },
  {
    "text": "in this plot and the sigma, and it will \nestimate the score for that noise level.",
    "start": "3123520",
    "end": "3131680"
  },
  {
    "text": "And we jointly train them by this mixture \nof denoising score-matching objectives,  ",
    "start": "3132880",
    "end": "3139759"
  },
  {
    "text": "Which is just a mixture of the usual score \nmatching loss. And then we do, again,  ",
    "start": "3140840",
    "end": "3146600"
  },
  {
    "text": "Langevin dynamics to sample. So what you would \ndo is you would initialize your particles at   random. Presumably, that is pretty close to just \npure noise. So we initialize our particles here,  ",
    "start": "3146600",
    "end": "3160000"
  },
  {
    "text": "and then we follow the scores for p sigma \n3, which is data plus a lot of noise.",
    "start": "3160000",
    "end": "3167120"
  },
  {
    "text": "We're getting a little bit closer \nto the higher data density regions,   and then we use these samples to initialize a \nnew Langevin dynamics chain for the data density  ",
    "start": "3167120",
    "end": "3176840"
  },
  {
    "text": "plus a little bit less noise. \nWe follow these arrows again,  ",
    "start": "3176840",
    "end": "3182760"
  },
  {
    "text": "and you see that the particles are moving \ntowards the high probability regions of the   original data density. So they're becoming \nmore and more structured in some sense.",
    "start": "3182760",
    "end": "3191320"
  },
  {
    "text": "And then we use these particles to \ninitialize another Langevin chain,   corresponding to an even smaller amount of noise. \nAnd at the end, we get samples that are very close  ",
    "start": "3191320",
    "end": "3203599"
  },
  {
    "text": "to the original data density because sigma 1 is \nvery, very small. So this is almost the same as  ",
    "start": "3203600",
    "end": "3210680"
  },
  {
    "text": "clean data. And throughout the process, we \nare getting good directional information  ",
    "start": "3210680",
    "end": "3216280"
  },
  {
    "text": "from our score models because we're using the \ncorresponding noise level in the data density.",
    "start": "3216280",
    "end": "3227480"
  },
  {
    "text": "It seems like more of a deterministic process. \nFor a given noise level, you always get a   particular mode in a distribution. In a sense, if \nit were truly generative, you would expect like a  ",
    "start": "3227480",
    "end": "3236960"
  },
  {
    "text": "more multimodal, I guess, score function in the \nsense that you should have a slight probability   to go to any of the modes or any of the, I \nguess, the clusters over here. And I guess,  ",
    "start": "3236960",
    "end": "3247400"
  },
  {
    "text": "that's something which I can see over here. \nI was just curious what your perspective is. Yeah. So that's a good question. And although \nthe arrow is pointing you in one direction  ",
    "start": "3247400",
    "end": "3258799"
  },
  {
    "text": "because it's deterministic, recall the \nLangevin dynamics is following the arrow,   but it's also adding noise at every step. And \nso regardless of where you are, if you run  ",
    "start": "3258800",
    "end": "3268240"
  },
  {
    "text": "it for a sufficiently long amount of time, you \nmight end up somewhere completely differently.  ",
    "start": "3268240",
    "end": "3275360"
  },
  {
    "text": "So even though you initialize the particle in \nthe same point, maybe you initialize it here,  ",
    "start": "3275360",
    "end": "3280720"
  },
  {
    "text": "because of the randomness in the Langevin \ndynamics, you might end up in completely   different places, meaning that you're going \nto generate completely different images.",
    "start": "3280720",
    "end": "3292800"
  },
  {
    "text": "And so that's the procedure, annealed Langevin \ndynamics. And here, you can see how it works.  ",
    "start": "3292800",
    "end": "3300840"
  },
  {
    "text": "This is what happens if you follow that exact \nprocedure on real data sets. Let me play it again.",
    "start": "3301840",
    "end": "3309640"
  },
  {
    "text": "So you see how you start from pure noise, \nand then you're following these arrows,  ",
    "start": "3309640",
    "end": "3315400"
  },
  {
    "text": "you're following these gradients. And you're \nslowly basically turning noise into data.",
    "start": "3315400",
    "end": "3324000"
  },
  {
    "text": "And here, you can see examples on some image data \nsets-- MNIST, CIFAR-10, and so forth. And you can  ",
    "start": "3324000",
    "end": "3330240"
  },
  {
    "text": "see that it has this flavor of going from noise \nto data by following these score models. Yeah?",
    "start": "3330240",
    "end": "3339640"
  },
  {
    "text": "How many times do we update the tilt converter? Yeah, great question. How many steps? \nIdeally, you would want to do as many  ",
    "start": "3339640",
    "end": "3346840"
  },
  {
    "text": "as possible. In practice, that's expensive, so \nyou might want to do one, or two, or maybe 10.",
    "start": "3346840",
    "end": "3353960"
  },
  {
    "text": "Each step is expensive because it requires a \nfull neural network evaluation. And so that's,  ",
    "start": "3353960",
    "end": "3360400"
  },
  {
    "text": "again, a hyperparameter. You should do \nas many as you can. But the more you do,  ",
    "start": "3360400",
    "end": "3365799"
  },
  {
    "text": "the more expensive it is to generate a sample. All the pictures are generated?",
    "start": "3365800",
    "end": "3373200"
  },
  {
    "text": "These are all generated, yeah, yeah, through \nthe annealed Langevin dynamics procedure.",
    "start": "3373200",
    "end": "3381060"
  },
  {
    "text": "[INAUDIBLE] Oh, yeah. So MNIST is basically a data set of \nhandwritten digits. They kind look like that,  ",
    "start": "3381060",
    "end": "3387280"
  },
  {
    "text": "and then you have people's faces, and \nyou have CIFAR-10 samples. So they are   pretty good samples that were generated by \nthis model. They have the right structure,  ",
    "start": "3387280",
    "end": "3395560"
  },
  {
    "text": "and the model is able to generate \nreasonable looking images. Yeah?",
    "start": "3395560",
    "end": "3401760"
  },
  {
    "text": "In training, we pick a few discrete \nsigma values and fit into the model.  ",
    "start": "3401760",
    "end": "3407360"
  },
  {
    "text": "And during the inversion we anneal, can \nwe pick from the continuous [INAUDIBLE]?",
    "start": "3407360",
    "end": "3413160"
  },
  {
    "text": "So we will come to continuous soon, \nyeah. For now, yeah, It's all discrete,   And you would anneal down following \nthe same kind of schedule. Yeah?",
    "start": "3413160",
    "end": "3422960"
  },
  {
    "text": "I guess, these days, you see mostly \nthese type of models being used. I guess,   is there an objective benefit in using \ndiffusion models compared to GANs? I guess,  ",
    "start": "3422960",
    "end": "3431319"
  },
  {
    "text": "more from a theoretical perspective, because \nwe see they get better results in practice,   but is that theoretically \ngrounded for some reason?",
    "start": "3431320",
    "end": "3438040"
  },
  {
    "text": "It's not theoretically grounded. I think one \nkey advantage is that if you think about it,   they can be much more expensive at the \ninference time. Imagine you're running a  ",
    "start": "3438040",
    "end": "3447080"
  },
  {
    "text": "Langevin dynamics chain, which might involve \nevaluating a neural network 1,000 times,  ",
    "start": "3447080",
    "end": "3452160"
  },
  {
    "text": "10,000 times. So if you think \nof it from that perspective,   you have a very deep computation graph that \nyou're allowed to use at inference time.",
    "start": "3452160",
    "end": "3461920"
  },
  {
    "text": "But at training time, you never have \nto actually unroll it. So that's the   key insight that you're allowed to use \na lot of computation at inference time,  ",
    "start": "3461920",
    "end": "3471359"
  },
  {
    "text": "but it's not very expensive to train because \nit's not trained by, again, generating a full  ",
    "start": "3471360",
    "end": "3476520"
  },
  {
    "text": "sample and then checking how should I change my \nparameters to make the sample better. It's trained  ",
    "start": "3476520",
    "end": "3483480"
  },
  {
    "text": "very incrementally to slightly improve the samples \nby a little bit. And then you keep repeating that  ",
    "start": "3483480",
    "end": "3491160"
  },
  {
    "text": "procedure at inference time, and you get great \nsamples. So I think that's a key insight for why.",
    "start": "3491160",
    "end": "3498440"
  },
  {
    "text": "They would both maximize likelihood. But-- This is not maximizing. Oh, sorry. Neither is. I guess, they would both be equivalent, \nat least from some perspective in terms  ",
    "start": "3498440",
    "end": "3508200"
  },
  {
    "text": "of data generation or some-- you know, matching \nsome distribution or something. They would be   equivalent from that perspective. It's just that \nthe efficient models are at a finer scale then.",
    "start": "3508200",
    "end": "3518200"
  },
  {
    "text": "Yeah. I guess they are trained in a very different   way. There is no two-sample test. \nThey are trained by score matching.",
    "start": "3518200",
    "end": "3524119"
  },
  {
    "text": "The architectures are different. The \namount of compute that you use during   training and inference time are different. \nSo there's a lot of things that change.",
    "start": "3524120",
    "end": "3533160"
  },
  {
    "text": "It's hard to say. There's \nno theoretical argument for   why this is better. But in practice, \nyeah, it seems to be dominating. It's  ",
    "start": "3533160",
    "end": "3539839"
  },
  {
    "text": "also much more stable to train because it's \nall just score matching, so no minimax. Yeah?",
    "start": "3539840",
    "end": "3546629"
  },
  {
    "text": "So I think when you were discussing the model,   there is an analogy that diffusion models \nare like stacking of these. How is that?",
    "start": "3546629",
    "end": "3554280"
  },
  {
    "text": "We're not yet at diffusion models yet, so \nwe'll hopefully talk about that later. But  ",
    "start": "3554280",
    "end": "3559760"
  },
  {
    "text": "we'll see the flow model soon, yeah. So yeah, \nif you look at certain kind of metrics that  ",
    "start": "3559760",
    "end": "3565040"
  },
  {
    "text": "we'll talk about more in a few lectures, but \nit was kind of like the first model that was  ",
    "start": "3565040",
    "end": "3570920"
  },
  {
    "text": "actually able to beat GANs back then, or \nthe state of the art on image generation,  ",
    "start": "3570920",
    "end": "3576960"
  },
  {
    "text": "that was kind of like the first hint that these \nkind of models could actually outperform GANs   despite a lot of years and years and lots of \nresources that were spent in optimizing GANs.",
    "start": "3576960",
    "end": "3587640"
  },
  {
    "text": "This thing was actually able to improve sample \nquality according to some metrics. And indeed,  ",
    "start": "3587640",
    "end": "3594279"
  },
  {
    "text": "these are different kinds of data sets. \nScaling up the model a little bit, it can  ",
    "start": "3594280",
    "end": "3599400"
  },
  {
    "text": "generate pretty reasonable faces of people, \nand monuments, and things like that. Yeah?",
    "start": "3599400",
    "end": "3607640"
  },
  {
    "text": "That's metric inception like metrics? Yeah. We'll talk about the metrics more in \nfuture lectures, but there are metrics that  ",
    "start": "3607640",
    "end": "3615000"
  },
  {
    "text": "try to quantify how good the samples are, how \nclosely they match the-- they relate to how  ",
    "start": "3615000",
    "end": "3621320"
  },
  {
    "text": "visually appealing the samples are. And they are \nautomated, so there's no human in the loop. And   they correlate with how good the samples are-- \nnot super important what they are exactly. But  ",
    "start": "3621960",
    "end": "3633480"
  },
  {
    "text": "the important bit was that there was the first \nmodel that was actually competitive with GANs,   and that's what prompted a lot of the follow-up \nwork on really scaling these models up.",
    "start": "3633480",
    "end": "3650681"
  },
  {
    "text": "So the noise level you used in sampling \nas we sample mass most often used for? Yeah. And yeah, so that was very \npromising. And you might also wonder,  ",
    "start": "3650681",
    "end": "3662240"
  },
  {
    "text": "which I think was also brought up here, do the \nnoise levels that you use during inference have  ",
    "start": "3662240",
    "end": "3669400"
  },
  {
    "text": "to match the ones that you see during training? \nCan we use more noise levels, less? And so it's  ",
    "start": "3669400",
    "end": "3675480"
  },
  {
    "text": "pretty natural to think about what happens if \nyou have an infinite number of noise levels. Right now, we have the clean data distribution, \nwhich, let's say, is just a mixture of two  ",
    "start": "3675480",
    "end": "3685600"
  },
  {
    "text": "Gaussians. So here, yellow denotes high \nprobability density, and blue denotes low  ",
    "start": "3685600",
    "end": "3691160"
  },
  {
    "text": "probability density. And then so far, what we \nsaid is that we're going to consider multiple  ",
    "start": "3691160",
    "end": "3697039"
  },
  {
    "text": "versions of this data distribution perturbed with \nincreasingly large amounts of noise-- so sigma 1,  ",
    "start": "3697040",
    "end": "3702920"
  },
  {
    "text": "sigma, 2, sigma 3, where sigma 3 is a \nvery large amounts of Gaussian noise.",
    "start": "3702920",
    "end": "3708559"
  },
  {
    "text": "So that's the structure in the data \nis completely lost. So if you start   out with a distribution, it's just a \nmixture of two. Gaussians. After you  ",
    "start": "3708560",
    "end": "3716520"
  },
  {
    "text": "add a sufficiently large amount of noise, you \nare left with just pure noise, essentially.",
    "start": "3716520",
    "end": "3723120"
  },
  {
    "text": "And you could imagine using-- maybe here, I \nhave three different noise levels. You could  ",
    "start": "3723120",
    "end": "3728760"
  },
  {
    "text": "imagine-- well, and you can always plot these \ndensities. So you have most of the probability  ",
    "start": "3728760",
    "end": "3736760"
  },
  {
    "text": "mass is here and here because it's a mixture \nof two Gaussians. Then, you can see that the   probability mass spreads out as you add \nmore and more Gaussian noise to the data.",
    "start": "3736760",
    "end": "3747440"
  },
  {
    "text": "And now you might wonder, well, what happens if \nwe were to consider multiple noise levels that  ",
    "start": "3747440",
    "end": "3753200"
  },
  {
    "text": "are in between? What happens if we add a noise \nlevel that is in between 0 and sigma 1? Then,  ",
    "start": "3753200",
    "end": "3761119"
  },
  {
    "text": "you're going to get a density that is in between \nthese two. And in the limit, you can think about  ",
    "start": "3761120",
    "end": "3767200"
  },
  {
    "text": "what happens if you were to consider an infinite \nnumber of noise levels going from 0 to whatever  ",
    "start": "3767200",
    "end": "3773640"
  },
  {
    "text": "was the maximum amount. And what you're going \nto get is an infinite number of data densities  ",
    "start": "3773640",
    "end": "3782079"
  },
  {
    "text": "perturbed with increasingly large amounts of noise \nthat are now indexed by t, where t is a continuous  ",
    "start": "3782080",
    "end": "3788960"
  },
  {
    "text": "random variable, a continuous variable from 0 to \nthe maximum that you have right at the other end.",
    "start": "3788960",
    "end": "3797640"
  },
  {
    "text": "So each slice here, each vertical slice is \nbasically the density of data where we've  ",
    "start": "3797640",
    "end": "3808680"
  },
  {
    "text": "added noise corresponding to this continuous index \nt. And so you can see how I got here. We started  ",
    "start": "3808680",
    "end": "3819400"
  },
  {
    "text": "out with a finite number of noise levels where \nall the probability mass was here, and then it  ",
    "start": "3819400",
    "end": "3825720"
  },
  {
    "text": "spreads out. And then you can think about a finer \ninterpolation, and finer, and finer until you have  ",
    "start": "3825720",
    "end": "3832960"
  },
  {
    "text": "something continuous. And so you go from pure data \nat time 0 here on the left to pure noise on the  ",
    "start": "3832960",
    "end": "3841400"
  },
  {
    "text": "other extreme, where corresponding to the maximum \namount of noise that you're adding to the data.",
    "start": "3841400",
    "end": "3851119"
  },
  {
    "text": "So now instead of having 1,000 different versions \nof the data perturbed with increasingly large  ",
    "start": "3851120",
    "end": "3856920"
  },
  {
    "text": "amounts of noise, you have an infinite number \nof data densities that have been perturbed with  ",
    "start": "3856920",
    "end": "3864920"
  },
  {
    "text": "increasingly large amounts of noise. And so \nyou can think of what we were doing before as  ",
    "start": "3864920",
    "end": "3872319"
  },
  {
    "text": "selecting 1,000 different slices here and modeling \nthose data distributions. But really, there is an  ",
    "start": "3872840",
    "end": "3881400"
  },
  {
    "text": "infinite number of them. And that perspective \nis actually quite, quite useful, as we'll see.",
    "start": "3881400",
    "end": "3889680"
  },
  {
    "text": "And so you have this sequence of distributions. \nP0 is just the clean data, and pt at the other  ",
    "start": "3889680",
    "end": "3900200"
  },
  {
    "text": "extreme is what you get if you add the \nmaximum amount of noise, which you can   think of as some kind of noise distribution where \nthe structure in the data is completely lost, and  ",
    "start": "3900200",
    "end": "3910960"
  },
  {
    "text": "that corresponds to basically pure noise. So as \nyou go from left to right, you are increasing the  ",
    "start": "3910960",
    "end": "3917880"
  },
  {
    "text": "amount of noise that you add to the data as you go \nfrom pure data to pure noise at the other extreme.",
    "start": "3917880",
    "end": "3925640"
  },
  {
    "text": "And now you can imagine what happens if you \nperturb data with increasingly large amounts  ",
    "start": "3926200",
    "end": "3933800"
  },
  {
    "text": "of noise. What happens is that you start with \npoints that are distributed according to p0, that  ",
    "start": "3933800",
    "end": "3940680"
  },
  {
    "text": "are distributed according to the data density. \nFor example, you start with these four images. And then as you go from left to right, you are \nincreasingly adding noise to these data samples  ",
    "start": "3940680",
    "end": "3952800"
  },
  {
    "text": "until you are left with pure noise. And so you can \nthink of having a collection of random variables,  ",
    "start": "3952800",
    "end": "3964160"
  },
  {
    "text": "xt, one for each time step, which is basically \ndescribing what happens as you go from left to  ",
    "start": "3964160",
    "end": "3971079"
  },
  {
    "text": "right, as you go from pure data to pure \nnoise. And all these random variables,  ",
    "start": "3971080",
    "end": "3977360"
  },
  {
    "text": "which you can think about as a \nstochastic process is just a collection,   an infinite of an infinite number of random \nvariables, They. All have densities, pts,  ",
    "start": "3977360",
    "end": "3984440"
  },
  {
    "text": "which are just these data densities plus noise \nthat we've been talking about for a while.",
    "start": "3984440",
    "end": "3990640"
  },
  {
    "text": "And we can describe the evolution of this \nor how these random variables change over  ",
    "start": "3990640",
    "end": "3996480"
  },
  {
    "text": "time if you think of this axis as a time-slide \ndimension, then all these random variables are  ",
    "start": "3996480",
    "end": "4003680"
  },
  {
    "text": "related to each other. And we can describe \ntheir relationship using something called   a stochastic differential equation. \nIt's not super important what it is,  ",
    "start": "4003680",
    "end": "4012320"
  },
  {
    "text": "but it's basically a simple formula that relates \nthe values of these random variables take. And  ",
    "start": "4012320",
    "end": "4019040"
  },
  {
    "text": "it's similar to an ordinary differential \nequation, which would just be some kind of   deterministic evolution where the difference \nis that we add basically noise at every step.",
    "start": "4019040",
    "end": "4030800"
  },
  {
    "text": "So you can imagine particles that evolve \nfrom left to right, following some kind  ",
    "start": "4030800",
    "end": "4036000"
  },
  {
    "text": "of deterministic dynamics where we add a little \nbit of noise at every step. And in particular,  ",
    "start": "4036000",
    "end": "4045880"
  },
  {
    "text": "it turns out that if all you want to \ndo is to go from data to pure noise,  ",
    "start": "4045880",
    "end": "4051720"
  },
  {
    "text": "all you have to do is you can describe this \nprocess very simply with a stochastic differential  ",
    "start": "4051720",
    "end": "4057440"
  },
  {
    "text": "equation where you're basically just-- the way \nxd changes infinitesimally is by adding a little  ",
    "start": "4057440",
    "end": "4064760"
  },
  {
    "text": "bit of noise at every step. You can think \nof this as some kind of random walk where,   at every step, you add a little bit of noise. \nAnd if you keep running this random walk for a  ",
    "start": "4064760",
    "end": "4073840"
  },
  {
    "text": "sufficiently large amount of time, you \nend up with a pure noise distribution.",
    "start": "4073840",
    "end": "4081800"
  },
  {
    "text": "And not super important, but what's interesting \nis that we can start thinking about what happens  ",
    "start": "4081800",
    "end": "4087680"
  },
  {
    "text": "if we reverse the direction of time. \nNow, we have this stochastic process   that evolves over time. Going from left to \nright here, you go from data to noise. Now  ",
    "start": "4087680",
    "end": "4097920"
  },
  {
    "text": "you can start thinking about what happens if \nyou were to reverse the direction of time,   and you go from capital T to 0. And \nso you go from pure noise to data.",
    "start": "4097920",
    "end": "4112880"
  },
  {
    "text": "That's what we want to do if we want to generate \nsamples. If you want to generate samples,   you want to basically invert this process. \nYou want to change the direction of time.",
    "start": "4112880",
    "end": "4123000"
  },
  {
    "text": "And it turns out that there is a simple, \nstochastic differential equation that  ",
    "start": "4123000",
    "end": "4128880"
  },
  {
    "text": "describes the process in reverse time. And the \ninteresting thing is that you can describe this  ",
    "start": "4128880",
    "end": "4138520"
  },
  {
    "text": "process with a stochastic differential equation, \nwhich is relatively simple. And really, the only  ",
    "start": "4138520",
    "end": "4143640"
  },
  {
    "text": "thing that you need is the score functions \nof these noise-perturbed data densities.",
    "start": "4143640",
    "end": "4151600"
  },
  {
    "text": "So if you somehow have access to the score \nfunctions of these densities pts corresponding  ",
    "start": "4151600",
    "end": "4157799"
  },
  {
    "text": "to data plus noise corresponding to time t, \nthen there is a simple, stochastic differential  ",
    "start": "4157800",
    "end": "4163839"
  },
  {
    "text": "equation that you can use to describe \nthis process of going from noise to data.",
    "start": "4163840",
    "end": "4171199"
  },
  {
    "text": "And so if you somehow knew this score \nfunction, which, to some extent,  ",
    "start": "4171200",
    "end": "4177440"
  },
  {
    "text": "we can approximate with a score-based model, \nwe can build a generative model out of this  ",
    "start": "4177440",
    "end": "4185680"
  },
  {
    "text": "interpretation. And so the idea is that \nwe're going to train a neural network,  ",
    "start": "4185680",
    "end": "4190759"
  },
  {
    "text": "just like before, to estimate all these score \nmodels or these ground truth scores. These are  ",
    "start": "4190760",
    "end": "4200599"
  },
  {
    "text": "scores of data plus noise where there is \nan infinite number of noise levels now. So this is exactly what we're doing \nbefore, except that now t doesn't  ",
    "start": "4200600",
    "end": "4208760"
  },
  {
    "text": "take 1,000 different possible values. T can \ntake an infinite number of different values,  ",
    "start": "4208760",
    "end": "4214199"
  },
  {
    "text": "but it's exactly the same thing. Before, we \nwere just estimating the scores of pt for  ",
    "start": "4214200",
    "end": "4220520"
  },
  {
    "text": "1,000 different chosen noise levels. Now \nwe do it for an infinite number of them.",
    "start": "4220520",
    "end": "4228480"
  },
  {
    "text": "And we do that by doing the usual mixture of \ndenoising score-matching objectives. So we  ",
    "start": "4228480",
    "end": "4235920"
  },
  {
    "text": "want to be able to train a single neural \nnetwork as theta that jointly estimates  ",
    "start": "4235920",
    "end": "4241560"
  },
  {
    "text": "all these infinite number of scores. And so \nit's exactly what we had before. Except that   instead of being a sum over 1,000 different \nnoise levels, now it's an integral over  ",
    "start": "4241560",
    "end": "4253239"
  },
  {
    "text": "an infinite number of different ts-- all \nthe time steps that we have in that plot.",
    "start": "4253240",
    "end": "4261200"
  },
  {
    "text": "And if you can somehow train this model well, \nyou can derive this loss to a small number,  ",
    "start": "4261200",
    "end": "4270680"
  },
  {
    "text": "which means that this score model approximates \nthe true score accurately. Then, you can basically  ",
    "start": "4270680",
    "end": "4279960"
  },
  {
    "text": "plug in your score model in that reverse time \nstochastic differential equation. So recall we  ",
    "start": "4279960",
    "end": "4287520"
  },
  {
    "text": "had this SDE here such that-- so if you knew this \nscore, then you could just solve this stochastic  ",
    "start": "4287520",
    "end": "4294120"
  },
  {
    "text": "differential equation, and you would go from \nnoise to data. You take this exact equation,   you replace the true score with the \nestimated score, and you get that.",
    "start": "4294120",
    "end": "4304840"
  },
  {
    "text": "And the advantage of this is that now this \nis a well-studied problem. You just have a   stochastic differential equation. You \njust need to solve it. And there is a  ",
    "start": "4304840",
    "end": "4312480"
  },
  {
    "text": "lot of numerical methods that you can \nuse to solve a stochastic differential   equation. The simplest one is basically \nsomething similar to Euler method for ODEs,  ",
    "start": "4312480",
    "end": "4322520"
  },
  {
    "text": "where you basically just discretize time, \nand you just step through this equation.",
    "start": "4322520",
    "end": "4328040"
  },
  {
    "text": "So this is a continuous time kind of evolution. \nYou can just take increments of delta t,  ",
    "start": "4328040",
    "end": "4335960"
  },
  {
    "text": "and you just basically discretize this. So \nI guess, delta t here is negative. So you  ",
    "start": "4335960",
    "end": "4342280"
  },
  {
    "text": "decrease time, and then you take a step \nhere following the deterministic part,  ",
    "start": "4342280",
    "end": "4348400"
  },
  {
    "text": "which is given by the score. And then you \nadd a little bit of noise at every step.",
    "start": "4348400",
    "end": "4353520"
  },
  {
    "text": "So you see how this is basically the same as \nLangevin dynamics. It's always some kind of  ",
    "start": "4353520",
    "end": "4358560"
  },
  {
    "text": "follow the gradient and add a little bit of \nnoise at every step. And so you can interpret  ",
    "start": "4358560",
    "end": "4364840"
  },
  {
    "text": "that as being just a discretization of \nthis stochastic differential equation   that tells you exactly what you should \ndo if you want to go from noise to data.",
    "start": "4364840",
    "end": "4376920"
  },
  {
    "text": "You're sampling t samples here? \nHow is it continuous? We're no  ",
    "start": "4376920",
    "end": "4385760"
  },
  {
    "text": "longer doing this noise level by noise levels. So there is a continuous number of them. So \nt is a uniform continuous random variable  ",
    "start": "4385760",
    "end": "4395400"
  },
  {
    "text": "between 0 and t. So there is an infinite \nnumber of-- let me see if I have it here. So there is an infinite number of noise \nlevels. And what we do is we're basically  ",
    "start": "4395400",
    "end": "4407760"
  },
  {
    "text": "numerically integrating these stochastic \ndifferential equation that goes from noise   to data. So you start out here by basically \nsampling from a pure noise distribution.",
    "start": "4407760",
    "end": "4422164"
  },
  {
    "text": "Yeah. How was it happening for you? So then you take small steps. You still need to, \nbut you have a freedom to choose. At that point,  ",
    "start": "4422164",
    "end": "4428720"
  },
  {
    "text": "you don't necessarily have to \ntake always 1,000 of the length. You can apply whatever. And there are many \nmore advanced numerical schemes for solving  ",
    "start": "4428720",
    "end": "4439239"
  },
  {
    "text": "stochastic differential equations. At \nthe moment you've managed to formulate   the problem of sampling to solving a \nstochastic differential equation, then  ",
    "start": "4439240",
    "end": "4446760"
  },
  {
    "text": "you can use a lot of advanced numerical methods \nfor solving stochastic differential equations.",
    "start": "4446760",
    "end": "4456280"
  },
  {
    "text": "And so you step through time, and you discretize-- \nyou try to find a solution to this trajectory  ",
    "start": "4456280",
    "end": "4461760"
  },
  {
    "text": "that goes from noise to data, basically. And \nthat's the main idea of score-based diffusion  ",
    "start": "4461760",
    "end": "4473760"
  },
  {
    "text": "models. And there is actually a connection \nwith what we were seeing before. Before,  ",
    "start": "4473760",
    "end": "4480000"
  },
  {
    "text": "we were doing Langevin dynamics. How is \nthat related to this numerical SDE solver?",
    "start": "4480000",
    "end": "4486720"
  },
  {
    "text": "You can think of it as kind of like the numerical   solver will take a step. It's trying to \napproximate the true solution of the SDE,  ",
    "start": "4486720",
    "end": "4497000"
  },
  {
    "text": "which is like this red trajectory that \nI'm showing here. You can use a numerical  ",
    "start": "4497000",
    "end": "4503000"
  },
  {
    "text": "SDE solver, and you can help it basically \nat every step by doing Langevin dynamics.",
    "start": "4503000",
    "end": "4509080"
  },
  {
    "text": "So Langevin dynamics is a procedure that would \nallow you to sample from this slice. And so what  ",
    "start": "4509080",
    "end": "4517840"
  },
  {
    "text": "you can do is you can combine or you can either \njust use character steps, in which case basically  ",
    "start": "4517840",
    "end": "4526080"
  },
  {
    "text": "you get the procedure that I talked about before, \nwhere you do annealed Langevin dynamics. You   just follow Langevin for a little bit, and \nthen you follow the Langevin corresponding  ",
    "start": "4526080",
    "end": "4535159"
  },
  {
    "text": "to the next slice, and you follow the Langevin \ncorresponding to the next slide, and so forth. Or you can apply these numerical methods \nto try to jump across time. And you can  ",
    "start": "4535160",
    "end": "4545920"
  },
  {
    "text": "combine the two of them to eventually end up \nwith something that can generate samples. So  ",
    "start": "4545920",
    "end": "4553080"
  },
  {
    "text": "you can think of it as, once you view \nit from this perspective, there is,   again, many different ways of solving the \nSDE, including using Langevin dynamics to  ",
    "start": "4553080",
    "end": "4564160"
  },
  {
    "text": "sample from these intermediate densities that \nyou get as you change the time dimension.",
    "start": "4564160",
    "end": "4573600"
  },
  {
    "text": "And yeah, this is the kind of thing that \nreally works extremely well. This kind of  ",
    "start": "4573600",
    "end": "4581120"
  },
  {
    "text": "model-- again, I guess we haven't \ntalked exactly about the metrics,   but it achieves state-of-the-art results on \na variety of image benchmarks. And you can  ",
    "start": "4581120",
    "end": "4592440"
  },
  {
    "text": "see some of the high-resolution images \nthat can be generated by this model.",
    "start": "4592440",
    "end": "4598239"
  },
  {
    "text": "These are fake samples. These people don't exist,   but you can see that the model is able \nto generate very high-quality data by  ",
    "start": "4599200",
    "end": "4607720"
  },
  {
    "text": "basically solving this stochastic differential \nequation and mapping pure noise to images that  ",
    "start": "4607720",
    "end": "4614840"
  },
  {
    "text": "have the right structure. And they're \nalmost indistinguishable from real samples.",
    "start": "4615360",
    "end": "4623119"
  },
  {
    "text": "Possibly, this question has been answered before,   but I see most of the images, when they have \nfingers in them, that's where the problem is.",
    "start": "4623120",
    "end": "4631760"
  },
  {
    "text": "They have-- Fingers. Oh, yeah. Fingers are a hard one \nto do. It's a hard problem for  ",
    "start": "4631760",
    "end": "4637840"
  },
  {
    "text": "these models to learn how to make \nhands. I guess in this data sets,   it's typically just the face, so \nyou don't have to worry about it.",
    "start": "4637840",
    "end": "4644840"
  },
  {
    "text": "But I think people have made \nprogress on that as well with   more training data. I think people \nhave been able to specialize. If you  ",
    "start": "4645680",
    "end": "4654000"
  },
  {
    "text": "show a lot of hands in the training \nset, the model learns how to do them.",
    "start": "4654000",
    "end": "4667560"
  },
  {
    "text": "Where does the [INAUDIBLE]? There is a bunch of data sets \nwhere we have celebrities' faces,  ",
    "start": "4667560",
    "end": "4673240"
  },
  {
    "text": "and then you can train a model. Pictures? Yeah, those are real pictures.",
    "start": "4673240",
    "end": "4679040"
  },
  {
    "text": "Oh, they use real pictures \nto create a fake picture. You train a model, and then you \ngenerate new people that like the  ",
    "start": "4679040",
    "end": "4685400"
  },
  {
    "text": "ones you have in your training \nset, but don't really exist. Is that from the real picture, which also \nfrom people's face or from other categories?",
    "start": "4685400",
    "end": "4694360"
  },
  {
    "text": "It was always faces. So you train on faces, \nand then you generate other people's face.",
    "start": "4694360",
    "end": "4698639"
  },
  {
    "text": "So how do you validate that \nthe image that's generated   isn't just a sample from your training set?",
    "start": "4700680",
    "end": "4708760"
  },
  {
    "text": "Yeah, that's a great question. How do \nyou prevent overfitting? You can look   at it from the perspective of the loss. \nIf you believe the score matching loss,  ",
    "start": "4708760",
    "end": "4716280"
  },
  {
    "text": "you can see how well it generalizes to \nvalidation or test data. We also did  ",
    "start": "4716280",
    "end": "4723000"
  },
  {
    "text": "extensive tests on trying to find our nearest \nneighbor in the data set. And we're pretty  ",
    "start": "4723000",
    "end": "4728520"
  },
  {
    "text": "confident that it's often able to generate \nnew images that you haven't seen before.",
    "start": "4728520",
    "end": "4736320"
  },
  {
    "text": "There are certainly cases where especially \ntext-to-image diffusion models actually memorize,  ",
    "start": "4736880",
    "end": "4742280"
  },
  {
    "text": "which might be OK. I mean, I don't think it's \nnecessarily wrong behavior to memorize some of  ",
    "start": "4744000",
    "end": "4749200"
  },
  {
    "text": "the data points. But yeah, people have been able \nto craft captions such that if you ask the model  ",
    "start": "4749200",
    "end": "4757520"
  },
  {
    "text": "to generate you an image with that caption, it \nproduces exactly an image from the training set.",
    "start": "4757520",
    "end": "4763480"
  },
  {
    "text": "So memorization does happen, but \nnot to the extent that it's only   replicating images in the training set. \nIt's certainly able to generate new images,  ",
    "start": "4763480",
    "end": "4772680"
  },
  {
    "text": "including composing things in interesting \nways that possibly have been seen, I think,  ",
    "start": "4772680",
    "end": "4778840"
  },
  {
    "text": "even on the internet. So it's certainly \nshowing some generalization capabilities.",
    "start": "4778840",
    "end": "4784159"
  },
  {
    "text": "And I think looking at the loss is a pretty good \nway to see that, indeed, it's not overfitting. The  ",
    "start": "4784160",
    "end": "4789480"
  },
  {
    "text": "score matching loss that you see in a training \nset is pretty close to the one you see on the   validation on seen data. So it's not overfitting, \nat least. We're not yet at that level.",
    "start": "4789480",
    "end": "4804079"
  },
  {
    "text": "So the reason why the samples are \nmore realistic-- is that because   a diffusion model can estimate the \ntrue data distribution more accurate  ",
    "start": "4804080",
    "end": "4812400"
  },
  {
    "text": "than other generative models again or \nsomething? Or is there another reason? Yeah, it's a mix. I think it goes back to what \nwe were saying before. The models are trained  ",
    "start": "4812400",
    "end": "4820760"
  },
  {
    "text": "by score matching, so it's a much more stable \ntraining objective. From the perspective of  ",
    "start": "4820760",
    "end": "4827280"
  },
  {
    "text": "the computation graph, like if you think \nabout what happens if you solve an SDE,  ",
    "start": "4827280",
    "end": "4832360"
  },
  {
    "text": "that's an infinitely deep \ncomputation graph because,   at this point, you're discretizing \nit, of course. But in principle,  ",
    "start": "4832360",
    "end": "4840840"
  },
  {
    "text": "you can make it as deep as you want because \nyou're choosing increasingly small time steps.",
    "start": "4840840",
    "end": "4847119"
  },
  {
    "text": "This can become a very deep computation graph \nthat you can use at inference time. So again,  ",
    "start": "4847120",
    "end": "4853360"
  },
  {
    "text": "that's the key advantage that you can \nhave a very deep. You can use a lot of   computation at inference time to generate \nimages without having to pay a huge price  ",
    "start": "4853360",
    "end": "4861440"
  },
  {
    "text": "at training time because the models are \ntrained through this score-matching kind   of-- kind of like working at the level \nof small changes, figuring out how to  ",
    "start": "4861440",
    "end": "4872719"
  },
  {
    "text": "improve an image by a little bit. And then \nyou stack all these little improvements,  ",
    "start": "4872720",
    "end": "4878320"
  },
  {
    "text": "and you get a very deep computation graph \nthat can generate very high-quality data sets.",
    "start": "4878320",
    "end": "4884560"
  },
  {
    "text": "So it's a lot bigger tree, right? So if do  ",
    "start": "4884560",
    "end": "4889600"
  },
  {
    "text": "this for some latent variables because \nyou have your breakdown is smaller?",
    "start": "4889600",
    "end": "4895640"
  },
  {
    "text": "Yeah. So latent variables-- I guess, I \ndon't have time to talk about it today,   unfortunately. But there is a way to think \nof it from the perspective of-- it turns out  ",
    "start": "4895640",
    "end": "4905680"
  },
  {
    "text": "that there is a way to convert this model \ninto a normalizing flow, at which point,   you would have latent variables. And \nthe machinery looks something like this.",
    "start": "4905680",
    "end": "4915040"
  },
  {
    "text": "We have this stochastic differential equation \nthat goes from data to noise. It turns out  ",
    "start": "4915040",
    "end": "4921840"
  },
  {
    "text": "that it's possible to describe a stochastic \nprocess that has exactly the same marginals,  ",
    "start": "4921840",
    "end": "4927840"
  },
  {
    "text": "but it's purely deterministic. . So there is \nan ordinary differential equation-- the kind of  ",
    "start": "4927840",
    "end": "4934080"
  },
  {
    "text": "things you probably have seen in other classes-- \nthat has exactly the same marginals over time.",
    "start": "4934080",
    "end": "4940920"
  },
  {
    "text": "And again, this ordinary differential equation \ndepends on the score. So if you are able to  ",
    "start": "4940920",
    "end": "4947560"
  },
  {
    "text": "estimate the score, you can actually generate \nsamples. You can go from noise to data by solving  ",
    "start": "4947560",
    "end": "4955400"
  },
  {
    "text": "an ordinary differential equation instead of \nsolving a stochastic differential equation.",
    "start": "4955400",
    "end": "4960880"
  },
  {
    "text": "And at that point, because you can think \nof the ordinary differential equation as  ",
    "start": "4960880",
    "end": "4968000"
  },
  {
    "text": "basically defining a normalizing flow because the \nmapping from the initial condition to the final  ",
    "start": "4968000",
    "end": "4975720"
  },
  {
    "text": "condition of the ordinary differential equation is \ninvertible. So you can go from left to right along  ",
    "start": "4975720",
    "end": "4984200"
  },
  {
    "text": "these white trajectories or from left to right or \nright to left. And that's an invertible mapping.",
    "start": "4984840",
    "end": "4993679"
  },
  {
    "text": "So essentially, this machinery defines \na continuous time normalizing flow  ",
    "start": "4993680",
    "end": "5000440"
  },
  {
    "text": "where the invertible mapping is given by \nsolving an ordinary differential equation.   These white trajectories that are the solutions \nof that same ordinary differential equation with  ",
    "start": "5000440",
    "end": "5011880"
  },
  {
    "text": "different initial conditions-- they cannot cross \nbecause that's how ordinary differential equation.  ",
    "start": "5011880",
    "end": "5017639"
  },
  {
    "text": "So the paths corresponding to different initial \nconditions, they can never cross, which basically  ",
    "start": "5017640",
    "end": "5027720"
  },
  {
    "text": "means that the mapping is invertible, which \nbasically means that this is a normalizing flow.",
    "start": "5027720",
    "end": "5034240"
  },
  {
    "text": "And so that-- I guess, I don't \nhave time to talk about it,   unfortunately. But if you're willing to \ntake the normalizing flow perspective,  ",
    "start": "5034240",
    "end": "5042880"
  },
  {
    "text": "then you can go from data to noise. And the noise \nis a latent vector that is encoding the data.",
    "start": "5042880",
    "end": "5052120"
  },
  {
    "text": "And the latent vector has the \nsame dimension because here,   it's clean image. Image plus noise has the same \ndimension. It's really just a normalizing flow.",
    "start": "5052120",
    "end": "5061960"
  },
  {
    "text": "The latent variables indeed have a simple \ndistribution because it's pure noise,   and it's just like the mapping from \nnoise to data is given by a solving  ",
    "start": "5061960",
    "end": "5071320"
  },
  {
    "text": "an ordinary differential equation, which is \ndefined by the score model. So it's a flow   model that is not being trained by maximum \nlikelihood. It's trained by score matching.",
    "start": "5071320",
    "end": "5081400"
  },
  {
    "text": "And you can think of it as a flow with \nan infinite depth. That's another way to  ",
    "start": "5081400",
    "end": "5087840"
  },
  {
    "text": "think about it, which means that you \ncan also get likelihoods. That's the   other interesting bit that you can \nget if you take that perspective.",
    "start": "5087840",
    "end": "5102000"
  }
]