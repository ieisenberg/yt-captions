[
  {
    "text": "Okay. So let's, uh, get started with the actual, ah, technical content.",
    "start": "5030",
    "end": "10290"
  },
  {
    "text": "So remember from last time, we gave an overview of the class. We talked about different types of models that we're gonna explore: reflex models,",
    "start": "10290",
    "end": "19170"
  },
  {
    "text": "state-based models, variable based models, and logic models which we'll see throughout the course. But underlying all of this is, is, you know, machine learning.",
    "start": "19170",
    "end": "26625"
  },
  {
    "text": "Because machine learning is what allows you to take data and, um, tune the parameters of the model,",
    "start": "26625",
    "end": "32099"
  },
  {
    "text": "so you don't have to, ah, work as hard designing the model. Um, so in this lecture,",
    "start": "32100",
    "end": "37829"
  },
  {
    "text": "I'm gonna start with the simplest of the models, the reflex based models, um, and show how machine learning can be applied to these type of models.",
    "start": "37830",
    "end": "45949"
  },
  {
    "text": "And throughout the class, ah, we're going to talk about different types of models and how learning will help with those as well.",
    "start": "45950",
    "end": "53005"
  },
  {
    "text": "So there's gonna be three parts, we're gonna talk about linear predictors, um, which includes classification regression, um,",
    "start": "53005",
    "end": "59510"
  },
  {
    "text": "loss minimization which is basically stating an objective function of how you, ah, want to train your machine learning model,",
    "start": "59510",
    "end": "65420"
  },
  {
    "text": "and then stochastic gradient descent, which is an algorithm that allows you to actually, ah, do the work.",
    "start": "65420",
    "end": "71425"
  },
  {
    "text": "So let's start with, ah, perhaps the most, um, cliched example of,",
    "start": "71425",
    "end": "76470"
  },
  {
    "text": "uh, you know, machine learning. So you have- we wanted to do spam classification. So the input is x,",
    "start": "76470",
    "end": "83335"
  },
  {
    "text": "um, an email message. Um, and you wanna know whether an email message is spam or not spam.",
    "start": "83335",
    "end": "89660"
  },
  {
    "text": "Um, so we're gonna denote the output of the classifier to be Y which is in this case,",
    "start": "89660",
    "end": "95060"
  },
  {
    "text": "either spam or not spam. And our goal is to, ah, produce a predictor F, right?",
    "start": "95060",
    "end": "101460"
  },
  {
    "text": "So a predictor in general is going to be a- a function that maps some input x to some output y.",
    "start": "101460",
    "end": "107705"
  },
  {
    "text": "In this case, it's gonna take an email message and map it to whether the email message is spam or not.",
    "start": "107705",
    "end": "113960"
  },
  {
    "text": "Okay. So there- there's many types of prediction problems, um, binary classification is the simplest one where the output is one of two,",
    "start": "113960",
    "end": "123775"
  },
  {
    "text": "um, possibilities either yes or no. And we're gonna usually denote this as plus 1 or minus 1,",
    "start": "123775",
    "end": "130399"
  },
  {
    "text": "sometimes you'll also see 1 and 0. Um, there's regression where you're trying to predict a numerical value,",
    "start": "130400",
    "end": "136269"
  },
  {
    "text": "for example, let's say housing price. Um, there's a multi-class classification where Y is, ah,",
    "start": "136270",
    "end": "143170"
  },
  {
    "text": "not just two items but possibly, um, 100 items, maybe cat, dog, truck,",
    "start": "143170",
    "end": "148665"
  },
  {
    "text": "tree, and different kind of image categories. Um, there's ranking where the output,",
    "start": "148665",
    "end": "154255"
  },
  {
    "text": "um, is a permutation of the input, this can be useful. For example, if the input is a set of, um, articles,",
    "start": "154255",
    "end": "160310"
  },
  {
    "text": "or products, or webpages, and you want to rank them in some order to show to a user. Um, structured prediction is where Y,",
    "start": "160310",
    "end": "166760"
  },
  {
    "text": "ah, the output is an object that is much more complicated. Um, perhaps, it's a whole sentence or even an image.",
    "start": "166760",
    "end": "173510"
  },
  {
    "text": "And it's something that you have to kind of construct, you have to build this thing from scratch, it's not just a labeling.",
    "start": "173510",
    "end": "179675"
  },
  {
    "text": "Um, and there's many more types of prediction problems. Um, but underlying all of this,",
    "start": "179675",
    "end": "185519"
  },
  {
    "text": "you know, whenever someone says I'm gonna do machine learning. The first question you should ask is, okay what's the data? Because without data, there's no learning.",
    "start": "185520",
    "end": "192730"
  },
  {
    "text": "So we're gonna call an example. Um, x, y pair is something that specifies",
    "start": "192730",
    "end": "200000"
  },
  {
    "text": "what the output should be when the input is x, okay? And a training data or a set of examples,",
    "start": "200000",
    "end": "207710"
  },
  {
    "text": "the training set is going to be simply a list or a multiset of, er, examples.",
    "start": "207710",
    "end": "212990"
  },
  {
    "text": "So you can think about this as a partial specification of behavior. So remember, we're trying to design a system",
    "start": "212990",
    "end": "218480"
  },
  {
    "text": "that has certain- certain types of behaviors, and we're gonna show you examples of what that sum should do.",
    "start": "218480",
    "end": "223910"
  },
  {
    "text": "If I have some email message that has CS221 then it's not spam but if it has, um, lots of, ah, dollar signs then there might,",
    "start": "223910",
    "end": "231310"
  },
  {
    "text": "um, um, be spam. Um, and, ah- so remember this is not a false specification behavior.",
    "start": "231310",
    "end": "238095"
  },
  {
    "text": "These, ah, ten examples or even a million examples might not tell you what exactly this function is supposed to do.",
    "start": "238095",
    "end": "243800"
  },
  {
    "text": "It's just examples of, ah, what the function could do on those particular examples.",
    "start": "243800",
    "end": "249335"
  },
  {
    "text": "Okay. So once you have this data, so we're gonna use D_train to denote, ah, the data set.",
    "start": "249335",
    "end": "255170"
  },
  {
    "text": "Remember, it's a set of input output pairs. Um, we're going to,",
    "start": "255170",
    "end": "260340"
  },
  {
    "text": "ah, push this into a learning algorithm or a learner. And what is the learning algorithm is gonna produce?",
    "start": "260340",
    "end": "265940"
  },
  {
    "text": "It's gonna produce a predictor. So predictors are F and the predictor remember is what?",
    "start": "265940",
    "end": "272060"
  },
  {
    "text": "It's actually itself a function that, um, takes an input x and maps it to an output y.",
    "start": "272060",
    "end": "278750"
  },
  {
    "text": "Okay? So there's kind of two levels here. And you can understand this in terms of the,",
    "start": "278750",
    "end": "284615"
  },
  {
    "text": "uh, modeling inferences of a learning paradigm. So modeling is about the question of what",
    "start": "284615",
    "end": "290330"
  },
  {
    "text": "should the types of predictors after you should consider are. Ah, inference is about how do you compute y given x?",
    "start": "290330",
    "end": "298470"
  },
  {
    "text": "And learning is about how you take data and produce a predictor so that you can do inference?",
    "start": "298470",
    "end": "304880"
  },
  {
    "text": "Okay. Any questions about this so far? [NOISE]",
    "start": "304880",
    "end": "313590"
  },
  {
    "text": "So this is pretty high level and abstract and generic right now, and this is kinda, kind of on purpose because I wanna highlight how, um,",
    "start": "313590",
    "end": "320520"
  },
  {
    "text": "general machine learning is before going into the specifics of, uh, linear predictors, right?",
    "start": "320520",
    "end": "326355"
  },
  {
    "text": "So this is an abstract framework. Okay. So let's dig in a little bit to this actual,",
    "start": "326355",
    "end": "333799"
  },
  {
    "text": "um, an actual problem. Um, so just to simplify, ah, the email problem,",
    "start": "333799",
    "end": "340574"
  },
  {
    "text": "let's, eh, consider the task of, um, predicting whether a string is an email address or not.",
    "start": "340575",
    "end": "346445"
  },
  {
    "text": "Okay. Um, so the input is an em-, ah, is a string and, ah,",
    "start": "346445",
    "end": "351840"
  },
  {
    "text": "the output is- it's a binary classification problem, it's either 1 if it's an email or minus 1 if it's not, that's what you want.",
    "start": "351840",
    "end": "358820"
  },
  {
    "text": "Um, um, so the first step of, um, doing linear prediction is,",
    "start": "358820",
    "end": "365625"
  },
  {
    "text": "um, known as feature extraction. And the question you should ask yourself is, what properties of the input x might be relevant for predicting the output y?",
    "start": "365625",
    "end": "375200"
  },
  {
    "text": "Right, so I say, I really highlighted might be, right? At this point, you're not trying to encode the actual set of rules that solves a problem,",
    "start": "375200",
    "end": "383480"
  },
  {
    "text": "that would involve no learning, and that would just be trying to do it directly. But instead of- for learning you're kind of taking a,",
    "start": "383480",
    "end": "390050"
  },
  {
    "text": "um, you know, a more of a backseat and you're saying, \"Well, here are some hints that could help you.\"",
    "start": "390050",
    "end": "395135"
  },
  {
    "text": "Okay. Ah, so formally, a feature extractor takes an input and outputs a set of feature name,",
    "start": "395135",
    "end": "403759"
  },
  {
    "text": "feature value pairs, right? So I'll go through an example here. So if I have abc@gmail.com,",
    "start": "403760",
    "end": "410030"
  },
  {
    "text": "what are the properties that might be useful for determining whether a string is an email address or not?",
    "start": "410030",
    "end": "416420"
  },
  {
    "text": "Well, you might consider the length of the string, if it's greater than 10,",
    "start": "416420",
    "end": "421490"
  },
  {
    "text": "maybe long strings are less likely to be email addresses than shorter ones. Um, and here, the feature name is length greater than 10.",
    "start": "421490",
    "end": "428479"
  },
  {
    "text": "So that's just kind of a label of that feature, and the value of that feature is 1, ah, representing it's true.",
    "start": "428480",
    "end": "434970"
  },
  {
    "text": "So it will be 0, if it's false. Here's another feature, the fraction of alphanumeric characters, right?",
    "start": "434970",
    "end": "440760"
  },
  {
    "text": "So that happens to be 0.85 which is the number. Um, there might be features that test for a particular,",
    "start": "440760",
    "end": "448195"
  },
  {
    "text": "um, you know, letters for example, that it doesn't contain an \"at\" sign or that has a, you know,",
    "start": "448195",
    "end": "454130"
  },
  {
    "text": "feature value of 1 because there is an \"at\" sign, endsWith.com is one, endsWith.org is a 0 because that's not true.",
    "start": "454130",
    "end": "462400"
  },
  {
    "text": "So, um, and there you could have many, many more features, ah, and we'll talk more about features on next time.",
    "start": "462400",
    "end": "468995"
  },
  {
    "text": "But the point is that you have a set of properties, you're kind of distilling down this input which is could be a string,",
    "start": "468995",
    "end": "476760"
  },
  {
    "text": "or could be an image, or could be something more complicated into kind of a um, you know, ground-up fashion that later,",
    "start": "476760",
    "end": "483439"
  },
  {
    "text": "we'll see how a machine learning algorithm can take advantage of. Okay. So you have this, ah,",
    "start": "483440",
    "end": "488805"
  },
  {
    "text": "feature vector which has- is a list of feature values and their associated names or labels.",
    "start": "488805",
    "end": "497120"
  },
  {
    "text": "Okay. But later, we'll see that the- the names don't matter to the learning algorithm.",
    "start": "497120",
    "end": "502910"
  },
  {
    "text": "So actually, what you should also think about the feature vector is simply a list of numbers,",
    "start": "502910",
    "end": "508115"
  },
  {
    "text": "and just kind of on the side make a note that all this, you know. position number three corresponds to contains \"@\" and so on.",
    "start": "508115",
    "end": "514865"
  },
  {
    "text": "Right, so I've distilled the- the email address abc@gmail.com into the list of numbers 0- or 1,",
    "start": "514865",
    "end": "524204"
  },
  {
    "text": "0.85, 1, 1, 0. Okay. So that's feature extraction. It's kind of distilling complex objects into lists of numbers which we'll",
    "start": "524205",
    "end": "532640"
  },
  {
    "text": "see is what the kind of the lingua franca of these machine learning algorithms is.",
    "start": "532640",
    "end": "538190"
  },
  {
    "text": "Okay. So I'm gonna write some concepts on a board. There's gonna be a bunch of, um,",
    "start": "538190",
    "end": "543785"
  },
  {
    "text": "concepts I'm going to introduce, and I'll just keep them up on the board for reference. So feature vector is again an important notion and it's denoted Phi,",
    "start": "543785",
    "end": "552950"
  },
  {
    "text": "um, of x on input. So Phi itself- sometimes, you think about it, er,",
    "start": "552950",
    "end": "558065"
  },
  {
    "text": "you call it the feature map which takes an input and returns, um, a vector, and this notation means that returns in general,",
    "start": "558065",
    "end": "564800"
  },
  {
    "text": "ah, d-dimensional vector, so a list of d numbers. And, um, the components of this feature vector we can write down as Phi_1,",
    "start": "564800",
    "end": "573290"
  },
  {
    "text": "Phi_2, all the way to Phi_d of x. Okay. So this notation is,",
    "start": "573290",
    "end": "582240"
  },
  {
    "text": "eh, you know, convenient, um, because we're gonna start shifting our focus from thinking about",
    "start": "582240",
    "end": "588379"
  },
  {
    "text": "the features as properties of input to features as kind of mathematical objects. So in particular, Phi of x is a point in a high-dimensional space.",
    "start": "588380",
    "end": "597320"
  },
  {
    "text": "So if you had two features, that would be a point in two-dimensional space, but in general, you might have a million features,",
    "start": "597320",
    "end": "602570"
  },
  {
    "text": "so that's a feature, ah, it's a point enough, a hundred- ah, uh, million dimensional space.",
    "start": "602570",
    "end": "608125"
  },
  {
    "text": "So, you know, it might be hard to think about that space, but well, we'll see how we can, you know, deal with that in a later in a, in a bit.",
    "start": "608125",
    "end": "616800"
  },
  {
    "text": "Okay. So- so that's a feature vector, you take an input and return a list of numbers.",
    "start": "617080",
    "end": "622870"
  },
  {
    "text": "Okay. Um, and now, the second piece is a weight vector.",
    "start": "622870",
    "end": "628430"
  },
  {
    "text": "So let me write down a weight vector. [NOISE]",
    "start": "628430",
    "end": "633470"
  },
  {
    "text": "So a weight vector is going to be noted W. Um, and this is also,",
    "start": "633470",
    "end": "639670"
  },
  {
    "text": "uh, a list of D numbers. It's a point in a D-dimensional space but we're gonna interpret it differently, as we'll see later.",
    "start": "639670",
    "end": "646825"
  },
  {
    "text": "Okay. So- so a way to think about a weight vector is that, for each feature J. So for example, frac of Alpha, um,",
    "start": "646825",
    "end": "655535"
  },
  {
    "text": "we're gonna have a real number WJ, that represents the contribution of that feature to the prediction.",
    "start": "655535",
    "end": "660940"
  },
  {
    "text": "So this contribution is 0.6. So what does this 0.6 mean?",
    "start": "660940",
    "end": "666220"
  },
  {
    "text": "So, so the way to think about this is that you have your weight vector and you have a feature vector of a particular input,",
    "start": "666220",
    "end": "674904"
  },
  {
    "text": "and you want- the score of, uh, your prediction",
    "start": "674905",
    "end": "680410"
  },
  {
    "text": "is going to be, uh, the dot product between the weight vector and the feature vector.",
    "start": "680410",
    "end": "686230"
  },
  {
    "text": "Okay. So um, that's written W dot a phi of X um, which is um,",
    "start": "686230",
    "end": "693579"
  },
  {
    "text": "written out as basically, looking at all the features and multiplying the feature of the value",
    "start": "693580",
    "end": "698740"
  },
  {
    "text": "times the weight of that feature and summing up all those numbers. So for this example,",
    "start": "698740",
    "end": "703960"
  },
  {
    "text": "it will be minus 1.2, that's the weight of the first feature, times 1, that's the feature value,",
    "start": "703960",
    "end": "710410"
  },
  {
    "text": "plus 0.6 times 0.85 and so on. And then, you get this number of 4.51 which is- happens to",
    "start": "710410",
    "end": "718300"
  },
  {
    "text": "be the score for this example. Question? So the feature extraction which is phi of X, is that, uh,",
    "start": "718300",
    "end": "726265"
  },
  {
    "text": "supposed to be like an automated process or is it a part of manual extraction classification procedures?",
    "start": "726265",
    "end": "731980"
  },
  {
    "text": "Yeah. So the question is, is the feature extraction manual or automatic? So uh, phi is going to be implemented as a function like encode, right.",
    "start": "731980",
    "end": "744130"
  },
  {
    "text": "Um, you're going to write this function manually. But you know, the function itself is run automatically on examples.",
    "start": "744130",
    "end": "752785"
  },
  {
    "text": "Um, later we'll see how you can actually learn features as well. So you can slowly start to do less",
    "start": "752785",
    "end": "758470"
  },
  {
    "text": "of a manual effort but uh, we're going to hold off until, next time for that. Question? So we're talking about weight gaining",
    "start": "758470",
    "end": "764260"
  },
  {
    "text": "and I know that in certain tests of regressions like, uh, the weights being, uh, a percentage change,",
    "start": "764260",
    "end": "769750"
  },
  {
    "text": "[inaudible] weights to percentage change of the outcome it doesn't, it doesn't mean the sphere? Yeah. So the question is about interpretation of weights.",
    "start": "769750",
    "end": "777640"
  },
  {
    "text": "Sometimes weights can have a more precise meaning. In general, um, you can,",
    "start": "777640",
    "end": "783940"
  },
  {
    "text": "you can try to read the tea leaves but it I don't think there is maybe, uh, in general a mathematically precise thing",
    "start": "783940",
    "end": "791890"
  },
  {
    "text": "you can say about the meaning of individual weights. But intuitively, and the intuition is important,",
    "start": "791890",
    "end": "797320"
  },
  {
    "text": "is that you should think about each feature as you know, a little person that's going to make a vote on this prediction, right?",
    "start": "797320",
    "end": "803380"
  },
  {
    "text": "So you're voting either plus, yay or nay? And the weight of a particular feature is-",
    "start": "803380",
    "end": "809785"
  },
  {
    "text": "specifies both the direction level whether- if positive weight means that, um, that little person, um,",
    "start": "809785",
    "end": "815380"
  },
  {
    "text": "is voting positive and negative weight means, that it's voting negative.",
    "start": "815380",
    "end": "820435"
  },
  {
    "text": "The magnitude of that weight, is how strongly that little person feels about the prediction, right?",
    "start": "820435",
    "end": "827680"
  },
  {
    "text": "So, you know, contains add as three because maybe like \"@\" signs generally do occur in",
    "start": "827680",
    "end": "833020"
  },
  {
    "text": "email addresses but you know the fraction of alphanumeric characters, it's you know, less. So at that level,",
    "start": "833020",
    "end": "838510"
  },
  {
    "text": "you can have some intuition but the precise numbers and y is 0.6 versus 0.5. Um, that's, um, you can't really say much about that. Yeah. Another question?",
    "start": "838510",
    "end": "847524"
  },
  {
    "text": "Does, uh, [inaudible] [NOISE] is it the same dot product for deeper networks. They can feel like more weight vectors afterwards.",
    "start": "847525",
    "end": "854590"
  },
  {
    "text": "It's still like it's, like just more than one products. [NOISE] So right now we're focusing on linear classifier.",
    "start": "854590",
    "end": "862390"
  },
  {
    "text": "So the question is what happens if you have a neural net with more layers? Um, there's gonna be more dot products but there's",
    "start": "862390",
    "end": "868150"
  },
  {
    "text": "also goin- it's not just adding more features. There's gonna be other uh, components which we'll get to in a later lecture.",
    "start": "868150",
    "end": "875270"
  },
  {
    "text": "Yeah? Do the weights have to add up to a certain number or how do you normalize it, so the weights, like you have to change the score value",
    "start": "875270",
    "end": "883000"
  },
  {
    "text": "[inaudible] . Yeah. So the question is, do the weights have to add up to something? Short answer is.",
    "start": "883000",
    "end": "888685"
  },
  {
    "text": "No. There's obviously restricted settings, where you might want to normalize the weights or something but we're not gonna,",
    "start": "888685",
    "end": "895750"
  },
  {
    "text": "you know, uh, consider that right now. Later, we'll see that the magnitude of weight does tell you, you know, something.",
    "start": "895750",
    "end": "903399"
  },
  {
    "text": "Okay, so, so just to summarize it's important to note that the weight vectors,",
    "start": "903830",
    "end": "910290"
  },
  {
    "text": "there's only one weight vector, right, you have to find one set of parameters for every- everybody.",
    "start": "910290",
    "end": "916195"
  },
  {
    "text": "But the feature vector is per example. So for every input, you get a new feature vector and",
    "start": "916195",
    "end": "921700"
  },
  {
    "text": "the dot product of those two weighted combination of features is the uh, is the score.",
    "start": "921700",
    "end": "927500"
  },
  {
    "text": "Okay, so, so now let's try to put the pieces together and define,",
    "start": "927540",
    "end": "934945"
  },
  {
    "text": "um, uh, of the actual predictor. All right, so remember we had this box with f in it,",
    "start": "934945",
    "end": "941380"
  },
  {
    "text": "which takes x and returns y. So what is inside that box? Um, and I'm hopefully giving you some intuition.",
    "start": "941380",
    "end": "948370"
  },
  {
    "text": "Let me go to a board and write, uh, a few more things. So the score, uh, remember is w dot phi of x.",
    "start": "948370",
    "end": "954190"
  },
  {
    "text": "And this is just gonna be a number, um, and uh, the predictor.",
    "start": "954190",
    "end": "959785"
  },
  {
    "text": "So linear predictor actually let me call this linear.",
    "start": "959785",
    "end": "965305"
  },
  {
    "text": "To be more precise, it's a linear classifier not just a predictor. Classifier is just a predictor that does classification.",
    "start": "965305",
    "end": "973060"
  },
  {
    "text": "Um, so a linear classifier",
    "start": "973060",
    "end": "977810"
  },
  {
    "text": "um, denoted f of w. So f is where we're going to use, you know, predictors.",
    "start": "978210",
    "end": "983650"
  },
  {
    "text": "W just means that this predictor depends on a particular set of weights. And this predictor is, uh,",
    "start": "983650",
    "end": "990894"
  },
  {
    "text": "going to look at the score and return the sign of that score. So what is the sign? The sign looks at the score and says, is it a positive of a number?",
    "start": "990895",
    "end": "999100"
  },
  {
    "text": "if it's positive then we're gonna return plus 1. If it's a negative number, I'm gonna return minus 1.",
    "start": "999100",
    "end": "1004440"
  },
  {
    "text": "And if it's 0 then you know, I don't care. You can return plus 1 if you want, it doesn't matter. Um, so what this is doing the remember the score is either, is a real number.",
    "start": "1004440",
    "end": "1015329"
  },
  {
    "text": "So it's either gonna be kind of leaning towards um, you know, large value, large positive values or leaning towards,",
    "start": "1015330",
    "end": "1022560"
  },
  {
    "text": "uh, s- large small- negative values. And the sign basically says, okay you gotta commit are you- which side are you on?",
    "start": "1022560",
    "end": "1030630"
  },
  {
    "text": "Are you on the positive side or you on the negative side? And just kind of discretizes it. That's what the sign does.",
    "start": "1030630",
    "end": "1036819"
  },
  {
    "text": "Okay. Okay, so, so let's look at a simple example",
    "start": "1037520",
    "end": "1045120"
  },
  {
    "text": "because I think a lot of what I've seen before is kind of more the, uh, formal machinery behind and the math behind how it works but it's",
    "start": "1045120",
    "end": "1054059"
  },
  {
    "text": "really useful to have some geometric intuition because then you can draw some pictures.",
    "start": "1054060",
    "end": "1059190"
  },
  {
    "text": "Okay, so let's consider this, uh, case. So we have a weight vector which is 2, 1,",
    "start": "1059190",
    "end": "1064410"
  },
  {
    "text": "2 minus 1, and a feature vector which is a 2, 0, and another feature vector which is x0, 2 and 2, 4.",
    "start": "1064410",
    "end": "1071565"
  },
  {
    "text": "Okay. So there's only two dimensions so I can try to draw them on a board. So let's try to do that.",
    "start": "1071565",
    "end": "1077490"
  },
  {
    "text": "Okay, so here is a two-dimensional plot. Um, and let's draw the fea- the weight vector first.",
    "start": "1077490",
    "end": "1085725"
  },
  {
    "text": "Okay so the weight vector is going to be at 2 minus 1. Okay. So that's this point.",
    "start": "1085725",
    "end": "1092445"
  },
  {
    "text": "And the way to think about the weight vector is not the point. Um, but actually um,",
    "start": "1092445",
    "end": "1098460"
  },
  {
    "text": "the, the, the vector going from the origin to that point for reasons that will become clear later.",
    "start": "1098460",
    "end": "1104205"
  },
  {
    "text": "Okay so that's the, that's the weight. Okay. Um and then what about the other points so we have 2, 0, 0, 2.",
    "start": "1104205",
    "end": "1111600"
  },
  {
    "text": "So 2, 0 is here, 0, 2 is here and 2, 4 is, uh, here.",
    "start": "1111600",
    "end": "1123135"
  },
  {
    "text": "Right? Okay, so we have three points here. Okay, so, um, how do I think about what this weight vectors is, is doing?",
    "start": "1123135",
    "end": "1132779"
  },
  {
    "text": "So just for just for reference remember the classifier is looking at the sign of W dot, uh, phi of x.",
    "start": "1132780",
    "end": "1142120"
  },
  {
    "text": "Okay. Um, so let's try to do uh, classification on these three points.",
    "start": "1142280",
    "end": "1148410"
  },
  {
    "text": "Okay so w is um, let me write it out formally, so 2, 1.",
    "start": "1148410",
    "end": "1153960"
  },
  {
    "text": "Um, and this is 0, 2. So what's the score when I do W dot phi of x here? It's 4, right?",
    "start": "1153960",
    "end": "1165255"
  },
  {
    "text": "Because this is um, uh, 2, 0, 0, 2 um, 2, 4.",
    "start": "1165255",
    "end": "1175980"
  },
  {
    "text": "So this is just a dot product that's 4, um, and take the sign what's the sign of 4?",
    "start": "1175980",
    "end": "1181500"
  },
  {
    "text": "One. Okay. So that means I'm going to label this point as a positive, right?",
    "start": "1181500",
    "end": "1189705"
  },
  {
    "text": "Positive point, okay what about 0, 2? Actually, sorry, this is just be a minus 1, right?",
    "start": "1189705",
    "end": "1195690"
  },
  {
    "text": "Okay. This is 2, minus 1. Okay, so if I take the dot product between this,",
    "start": "1195690",
    "end": "1200715"
  },
  {
    "text": "I get minus 2 and then the sign of minus 2 is,",
    "start": "1200715",
    "end": "1206380"
  },
  {
    "text": "is minus 1, okay, so that's a minus. Um, and what about this one?",
    "start": "1206660",
    "end": "1212580"
  },
  {
    "text": "So what's the dot product there? It's gonna be 0. Okay. So, um, so this classifier will classify this point as a positive.",
    "start": "1212580",
    "end": "1224010"
  },
  {
    "text": "This is a negative and this one I don't know. Okay. So we can fill in more points. Um, but, but, you know,",
    "start": "1224010",
    "end": "1231510"
  },
  {
    "text": "does anyone see kind of um, maybe a more general pattern? I don't wanna have to fill in the entire board with classifications.",
    "start": "1231510",
    "end": "1237990"
  },
  {
    "text": "Yeah? Orthogonal, everything to the right of it is positive and to the left of it is negative.",
    "start": "1237990",
    "end": "1245295"
  },
  {
    "text": "Yeah so so let's try to draw the orthogonal. Uh, this needs to go through that line.",
    "start": "1245295",
    "end": "1251985"
  },
  {
    "text": "Okay, [NOISE] okay, so let's draw the orthogonal.",
    "start": "1251985",
    "end": "1260520"
  },
  {
    "text": "So this is a right angle. Okay. And, ah, what that gentleman said is that,",
    "start": "1260520",
    "end": "1267990"
  },
  {
    "text": "the points- any point over here because it has acute angle width w,",
    "start": "1267990",
    "end": "1274410"
  },
  {
    "text": "is going to be classified as positive. So all of this stuff is um, you know, positive, positive, positive, positive, positive,",
    "start": "1274410",
    "end": "1282270"
  },
  {
    "text": "and everything over here because it's an obtuse angle with w is going to be negative,",
    "start": "1282270",
    "end": "1288450"
  },
  {
    "text": "so everything over here is negative. And then, everything on this line is going to be 0.",
    "start": "1288450",
    "end": "1297164"
  },
  {
    "text": "Okay? So, so I don't know. Okay, and this line is called, um,",
    "start": "1297165",
    "end": "1304490"
  },
  {
    "text": "the decision boundary, which is the concept not just for linear classifiers,",
    "start": "1304490",
    "end": "1310190"
  },
  {
    "text": "but whenever you have any sort of classifier the decision boundary is the separation between the regions of the space where the classification is positive versus negative.",
    "start": "1310190",
    "end": "1320625"
  },
  {
    "text": "Okay? And in this case, um, it's, it's separate because uh, we have linear classifiers,",
    "start": "1320625",
    "end": "1329685"
  },
  {
    "text": "the decision boundary is straight, and we're just separating the, the space into two halves.",
    "start": "1329685",
    "end": "1335865"
  },
  {
    "text": "Um, if you were in three-dimensions, um, this vector would still be just a you know vector,",
    "start": "1335865",
    "end": "1341835"
  },
  {
    "text": "but this decision, um, boundary would be a plane. So you can think about it as you know coming out of the board if you want,",
    "start": "1341835",
    "end": "1349680"
  },
  {
    "text": "but I'm not gonna try to draw that. Um, and that's, that's kind of the geometric interpretation of how linear classifiers,",
    "start": "1349680",
    "end": "1358049"
  },
  {
    "text": "ah, you know, work here. Question, yeah? It seems like your weight could be any values here.",
    "start": "1358050",
    "end": "1363419"
  },
  {
    "text": "Right? Yeah. So we have one last [inaudible]. Yeah. [inaudible] .",
    "start": "1363420",
    "end": "1369940"
  },
  {
    "text": "Yeah. So that's a good point. So the, the observation is that, no matter, if you scale this weight by 2,",
    "start": "1369940",
    "end": "1377730"
  },
  {
    "text": "it's actually gonna still have the same decision boundaries. So the magnitude of the weight doesn't matter it's the direction that matters.",
    "start": "1377730",
    "end": "1384450"
  },
  {
    "text": "Um, so this is true for just making a prediction. Um, when we look at learning, ah,",
    "start": "1384450",
    "end": "1390960"
  },
  {
    "text": "the magnitude of the weight will matter because we're going to, you know, consider other more nuanced loss functions.",
    "start": "1390960",
    "end": "1398770"
  },
  {
    "text": "Yeah. Okay. So let's move on.",
    "start": "1399170",
    "end": "1405150"
  },
  {
    "text": "Any questions about linear predictors? So, so, far what we've done is, we haven't done any learning.",
    "start": "1405150",
    "end": "1410235"
  },
  {
    "text": "Right. If you've ah, you know, noticed, we've just simply defined the set of predictors that we're interested in.",
    "start": "1410235",
    "end": "1417585"
  },
  {
    "text": "So we have a feature vector, we have weight vectors, multiply them together, get a score and",
    "start": "1417585",
    "end": "1423450"
  },
  {
    "text": "then you can send them through a sign function and you get these linear classifiers. Right. There, there's no specification of data yet.",
    "start": "1423450",
    "end": "1432420"
  },
  {
    "text": "Okay. So now, let's actually turn to do some learning.",
    "start": "1432420",
    "end": "1437970"
  },
  {
    "text": "So remember this framework, learning needs to take some data and return a predictor and our predictors are ah,",
    "start": "1437970",
    "end": "1446775"
  },
  {
    "text": "specified by a weight vector. So you can equivalently think about the learning algorithm as outputting a weight vector if you want for linear classifiers.",
    "start": "1446775",
    "end": "1456750"
  },
  {
    "text": "Um, and let's unpack the learner. So the learning algorithm is going to be based on optimization which we started ah,",
    "start": "1456750",
    "end": "1465390"
  },
  {
    "text": "reviewing last lecture um, which separates ah, what you want to compute from how you want to compute it.",
    "start": "1465390",
    "end": "1473400"
  },
  {
    "text": "So we're going to first define an optimization problem which specifies",
    "start": "1473400",
    "end": "1478530"
  },
  {
    "text": "what properties we want a- a classifier to have in terms of the data,",
    "start": "1478530",
    "end": "1483930"
  },
  {
    "text": "and then we're going to figure out how to actually optimize this. [NOISE] And this module is actually really really powerful um,",
    "start": "1483930",
    "end": "1491895"
  },
  {
    "text": "and it allows people to go ahead and work on different types of",
    "start": "1491895",
    "end": "1497835"
  },
  {
    "text": "criteria for and different types of models separately from the people who actually develop general purpose algorithms.",
    "start": "1497835",
    "end": "1504960"
  },
  {
    "text": "Um, and this has served kind of the field of machinery quite well. Okay. So let's start with an optimization problem.",
    "start": "1504960",
    "end": "1513015"
  },
  {
    "text": "So this is an important concept um, called a loss function and",
    "start": "1513015",
    "end": "1520830"
  },
  {
    "text": "this is a super general idea that's using the machine learning and statistics.",
    "start": "1520830",
    "end": "1525914"
  },
  {
    "text": "So a loss function takes a particular example x, y and a weight vector, um,",
    "start": "1525915",
    "end": "1533265"
  },
  {
    "text": "and returns a number and this number represents how unhappy we would be if we used the predictor",
    "start": "1533265",
    "end": "1539940"
  },
  {
    "text": "given by W to make a prediction on x when the correct output is y.",
    "start": "1539940",
    "end": "1547254"
  },
  {
    "text": "Okay. So it's a little bit of a mouthful but, um, this basically is trying to characterize, you know,",
    "start": "1547255",
    "end": "1552785"
  },
  {
    "text": "if you handed me a classifier, and I go on to this example and try to classify it, is it gonna get it right or is it gonna get it wrong?",
    "start": "1552785",
    "end": "1560200"
  },
  {
    "text": "So high loss is bad ah, you don't wanna lose and low loss is good. So normally, zero loss is the- the best you can then hope for.",
    "start": "1560200",
    "end": "1569920"
  },
  {
    "text": "Okay. So let's do figure out the loss function for binary classification here.",
    "start": "1570050",
    "end": "1576765"
  },
  {
    "text": "Um, so just some notation, the correct label is, ah, denoted y and, um,",
    "start": "1576765",
    "end": "1583710"
  },
  {
    "text": "the predicted label remember is um, the score, ah, sent through the sign function and that's going to give you some particular label.",
    "start": "1583710",
    "end": "1592620"
  },
  {
    "text": "Um, and let's look at this example. So w equals 2 minus 1 phi of x equals ah,",
    "start": "1592620",
    "end": "1599265"
  },
  {
    "text": "2, 0 and y equals minus 1. Okay. So we already defined the score as, um,",
    "start": "1599265",
    "end": "1606210"
  },
  {
    "text": "one example is a w dot phi of x which is, um, how co- confident we're predicting minu- plus 1.",
    "start": "1606210",
    "end": "1613950"
  },
  {
    "text": "That's the way to, uh, you know, interpret this. Okay. So um, what's the score of this,",
    "start": "1613950",
    "end": "1620040"
  },
  {
    "text": "for this particular example again? It's 4. Right. Um, which means I'm kind of,",
    "start": "1620040",
    "end": "1626985"
  },
  {
    "text": "kinda positive that it's ah, you know, a plus 1. Yeah. Question? Ah, I was wondering, is the loss function generally 1-dimensional or,",
    "start": "1626985",
    "end": "1635100"
  },
  {
    "text": "or the output of the loss function? Yeah. So the- the question is whether the output of loss function is usually a single number or not.",
    "start": "1635100",
    "end": "1643110"
  },
  {
    "text": "Um, in most cases it is for basically all practical cases you should think about the loss functions outputting a single number.",
    "start": "1643110",
    "end": "1650309"
  },
  {
    "text": "The inputs can be, you know, a crazy high-dimensional. Yeah.",
    "start": "1650310",
    "end": "1656460"
  },
  {
    "text": "Why is it not 1-dimension? [NOISE] Um, there are cases where you might have multiple objectives that you're trying to optimize at once ah,",
    "start": "1656460",
    "end": "1664785"
  },
  {
    "text": "but in this class it's always gonna be, you know, 1-dimensional. Like maybe you care about, you know,",
    "start": "1664785",
    "end": "1669915"
  },
  {
    "text": "both time and space or accuracy but robustness or something. Sometimes you have multi-objective optimization.",
    "start": "1669915",
    "end": "1677055"
  },
  {
    "text": "But that's way beyond the scope of this class. Okay. So we have a score.",
    "start": "1677055",
    "end": "1684299"
  },
  {
    "text": "Um, and now we're gonna define a margin. So let me, um.",
    "start": "1684300",
    "end": "1689790"
  },
  {
    "text": "Okay. So let's, let's actually do this. So we're talking about classification.",
    "start": "1689790",
    "end": "1696825"
  },
  {
    "text": "I'm gonna sneak regression in a bit. So score is w dot phi of x.",
    "start": "1696825",
    "end": "1702975"
  },
  {
    "text": "This is how confident we are about plus 1, um, and the margin is the score ah, times y.",
    "start": "1702975",
    "end": "1712650"
  },
  {
    "text": "Um, and this relies on y being plus 1 or minus 1. So this might seem a little bit mysterious but let's try to,",
    "start": "1712650",
    "end": "1719985"
  },
  {
    "text": "you know, decipher that, um here. Um, so in this example,",
    "start": "1719985",
    "end": "1726554"
  },
  {
    "text": "the score is 4. So what's the margin?",
    "start": "1726555",
    "end": "1730630"
  },
  {
    "text": "You multiply by minus 1. So the margin is, ah, minus 4.",
    "start": "1732530",
    "end": "1737835"
  },
  {
    "text": "Right. And the margins interpretation is how correct we are.",
    "start": "1737835",
    "end": "1743130"
  },
  {
    "text": "Right. So imagine the correct answer is ah, if, if the score in the margin had the same sign,",
    "start": "1743130",
    "end": "1749909"
  },
  {
    "text": "then you're gonna get positive numbers and then the, the confident, the more confident you are then the more correct you are.",
    "start": "1749910",
    "end": "1756225"
  },
  {
    "text": "Um, but if y is minus 1 and the score is positive,",
    "start": "1756225",
    "end": "1761895"
  },
  {
    "text": "then the margin is gonna be negative which means that you're gonna be confidently wrong um, which is bad.",
    "start": "1761895",
    "end": "1770400"
  },
  {
    "text": "[LAUGHTER] Okay. So just to to see if we kind of understand what's going on.",
    "start": "1770400",
    "end": "1776475"
  },
  {
    "text": "Um, so when is a binary classifier making a mistake on a given example.",
    "start": "1776475",
    "end": "1781919"
  },
  {
    "text": "Um, so I'm gonna ask for a kind of a show of hands. How many people think it's, it's when the margin is, uh, less than 0.",
    "start": "1781920",
    "end": "1791340"
  },
  {
    "text": "Okay. I guess we can kind of stop there. [LAUGHTER]",
    "start": "1791340",
    "end": "1797309"
  },
  {
    "text": "I used to do these online quizzes where it was anonymous but we're not doing that this year.",
    "start": "1797310",
    "end": "1802320"
  },
  {
    "text": "Okay. So yes, the margin is less than 0. Um, when the margin is less than 0 that means y and",
    "start": "1802320",
    "end": "1808620"
  },
  {
    "text": "the score are different signs which means that you're making a mistake. [NOISE]",
    "start": "1808620",
    "end": "1815490"
  },
  {
    "text": "Okay. So now we have the notion of a margin. Let's define ah, something called the zero-one loss and it's called zero-one because it returns either a 0 or a 1.",
    "start": "1815490",
    "end": "1824370"
  },
  {
    "text": "Okay. Very creative name. Um, so the loss function is simply,",
    "start": "1824370",
    "end": "1834210"
  },
  {
    "text": "did you make a mistake or not? Okay. So this notation let's try to decipher a bit. So if f of x here is the prediction when the input is x,",
    "start": "1834210",
    "end": "1844410"
  },
  {
    "text": "um, and not equal y is saying, did you make a mistake? So that's, think about it as a Boolean,",
    "start": "1844410",
    "end": "1850039"
  },
  {
    "text": "and this one bracket is um, just notation. It's called an indicator function that takes a condition and returns either a 1 or 0.",
    "start": "1850040",
    "end": "1858665"
  },
  {
    "text": "So if ah, if the, the condition is true, then it's gonna return a 1 and if the condition is false, it returns a 0.",
    "start": "1858665",
    "end": "1867015"
  },
  {
    "text": "Okay. So all this is doing is basically returning a 1, if you made a mistake and 0,",
    "start": "1867015",
    "end": "1872670"
  },
  {
    "text": "if you didn't make a mistake. Okay. And we can write that as follows.",
    "start": "1872670",
    "end": "1879419"
  },
  {
    "text": "We can write that as um, the margin less or equal to 0.",
    "start": "1879420",
    "end": "1884940"
  },
  {
    "text": "Right. Because pre- on the previous side of the margin is less than or equal to 0, then we've made a mistake and we should incur ah,",
    "start": "1884940",
    "end": "1892025"
  },
  {
    "text": "a loss of 1 and if the margin is greater than 0, then we didn't make a mistake and we should incur a loss of 0.",
    "start": "1892025",
    "end": "1900299"
  },
  {
    "text": "Okay. All right so, um, it will be useful to draw these loss functions,",
    "start": "1902750",
    "end": "1910830"
  },
  {
    "text": "um, pictorially like this. Okay, so on the axi- x-axis here,",
    "start": "1910830",
    "end": "1915870"
  },
  {
    "text": "we're going to show the margin, right? Remember the margin is how, uh, correct you are.",
    "start": "1915870",
    "end": "1922080"
  },
  {
    "text": "And on the, uh, y-axis we're gonna show the- the loss function which is how much you're gonna suffer for it.",
    "start": "1922080",
    "end": "1929415"
  },
  {
    "text": "Okay, so remember the margin, if the margin is positive, that means you're getting it right which means that the loss is 0.",
    "start": "1929415",
    "end": "1936360"
  },
  {
    "text": "But if the margin is less than 0, that means you are getting it wrong and the loss is 1.",
    "start": "1936360",
    "end": "1942315"
  },
  {
    "text": "Okay, so this is a 0-1 loss. That's, uh, thi- this thing- the visual that you should have in mind when you think about zero-one loss. Yeah.",
    "start": "1942315",
    "end": "1949409"
  },
  {
    "text": "[NOISE] Like less than 0 because we are not defining the event actually 0 [inaudible] classified as correct.",
    "start": "1949410",
    "end": "1957690"
  },
  {
    "text": "Yeah, so there is this kind of boundary condition of when ex- what happens exactly at 0 that I'm trying to sweep under the rug because it's not, um, terribly important.",
    "start": "1957690",
    "end": "1967335"
  },
  {
    "text": "Um, here, it's less we go to 0 to be kind of on the safe side. So if you don't know you're also, uh, gonna get it wrong.",
    "start": "1967335",
    "end": "1974580"
  },
  {
    "text": "Um, otherwise you could always just return 0 and then you, that, you don't want that.",
    "start": "1974580",
    "end": "1982960"
  },
  {
    "text": "Okay. So is it- uh, any questions about, uh, kind of binary classification so far.",
    "start": "1985400",
    "end": "1991260"
  },
  {
    "text": "So we've set up these linear predictors and I've defined the 0-1 loss as a way to capture, um,",
    "start": "1991260",
    "end": "1997815"
  },
  {
    "text": "how unhappy we would be if we had a classifier that was, ah, operating on a particular data point x, y.",
    "start": "1997815",
    "end": "2006180"
  },
  {
    "text": "So, um, just to- I'm gonna go on a little bit of a digression and talk about linear regression.",
    "start": "2007720",
    "end": "2018659"
  },
  {
    "text": "Um, uh, um, [NOISE]",
    "start": "2018910",
    "end": "2025820"
  },
  {
    "text": "and, and the reason I'm doing this is that loss minimization is such a powerful and general framework,",
    "start": "2025820",
    "end": "2031940"
  },
  {
    "text": "and it go- transcends, you know, all of these, you know, linear classifiers, regression, setups.",
    "start": "2031940",
    "end": "2037085"
  },
  {
    "text": "So I want to kind of emphasize over- the overall story. So I'm gonna give you a bunch of different examples, um, classification,",
    "start": "2037085",
    "end": "2045230"
  },
  {
    "text": "linear regression side-by-side so we can actually see how they compare and hopefully, their- the common denominator will kind of emerge more, um, clearly from that.",
    "start": "2045230",
    "end": "2054319"
  },
  {
    "text": "Okay, so we talked a little bit about linear regression in the last lecture, right? So linear regression in some sense is simpler than",
    "start": "2054320",
    "end": "2060919"
  },
  {
    "text": "classification because if you have a linear, uh, uh, predictor, um,",
    "start": "2060920",
    "end": "2067460"
  },
  {
    "text": "and you get the score w dot phi of x, it's already a real number. So in linear regression,",
    "start": "2067460",
    "end": "2072530"
  },
  {
    "text": "you simply return that real number and you call that your prediction. Okay? Okay so now we- let's move towards defining our loss function.",
    "start": "2072530",
    "end": "2083090"
  },
  {
    "text": "Um, so there's gonna be, uh, a concept that's gonna be useful, it's called the residual, um, which is,",
    "start": "2083090",
    "end": "2090679"
  },
  {
    "text": "as- against kind of trying to capture how, uh, wrong you are. Um, so here is a particular linear, uh, predictor, um,",
    "start": "2090680",
    "end": "2100670"
  },
  {
    "text": "linear regresser, um, and it's making predictions all along,",
    "start": "2100670",
    "end": "2105950"
  },
  {
    "text": "you know, for different values of x. Um, and here's a data point of Phi of xy. Okay? So the residual is the difference between,",
    "start": "2105950",
    "end": "2114005"
  },
  {
    "text": "um, the true value y and the predictor value y. Okay, um, and in particular it's the amount by which,",
    "start": "2114005",
    "end": "2123065"
  },
  {
    "text": "um, the prediction is overshooting the, you know, target.",
    "start": "2123065",
    "end": "2128160"
  },
  {
    "text": "Okay, so this is- this is a difference. Um, and if you square the [NOISE] difference you get something called,",
    "start": "2128170",
    "end": "2137540"
  },
  {
    "text": "uh, the squared loss. [NOISE] So this is something we mentioned last lecture.",
    "start": "2137540",
    "end": "2144829"
  },
  {
    "text": "Um, residual can be either negative or [NOISE] positive. Um, but errors, either,",
    "start": "2144830",
    "end": "2150845"
  },
  {
    "text": "if you're very positive or very negative, that's bad and squaring them makes it so that you're gonna, you know,",
    "start": "2150845",
    "end": "2156260"
  },
  {
    "text": "suffer equally for, um, errors in both, you know, directions.",
    "start": "2156260",
    "end": "2161340"
  },
  {
    "text": "Okay, so the square loss is the residual squared. So let's do this kind of simple example.",
    "start": "2161950",
    "end": "2168140"
  },
  {
    "text": "So here we have our weight vector 2 minus 1. The feature vector is 2, 0. What's the score?",
    "start": "2168140",
    "end": "2174530"
  },
  {
    "text": "It's 4, y is minus 1. So, uh, the residual is 4 minus minus 1 which is 5 and,",
    "start": "2174530",
    "end": "2185119"
  },
  {
    "text": "uh, 5 squared is 25. So the squared loss on this particular example is 25.",
    "start": "2185120",
    "end": "2191190"
  },
  {
    "text": "Okay, so let's plot this. So just like we did it for a 0-1 loss.",
    "start": "2193000",
    "end": "2199789"
  },
  {
    "text": "Let's see what this loss function looks like. So the, the horizontal axis here instead of being the margin is going to be this quantity,",
    "start": "2199790",
    "end": "2207480"
  },
  {
    "text": "uh, for regression called the residual. Um, it's going to be the difference between the prediction and the, the true target.",
    "start": "2207480",
    "end": "2214075"
  },
  {
    "text": "And I'm gonna plot the loss function. Um, and this loss function is just,",
    "start": "2214075",
    "end": "2219700"
  },
  {
    "text": "you know, the squared function, right? So with- if the residual is 0, then the loss is 0.",
    "start": "2219700",
    "end": "2225484"
  },
  {
    "text": "If as a residual grows in either direction, then I'm going to pay, uh, something for it.",
    "start": "2225485",
    "end": "2232505"
  },
  {
    "text": "And it's a quadratic penalty which means that, um, it actually grows, you know, uh, pretty fast.",
    "start": "2232505",
    "end": "2239270"
  },
  {
    "text": "So if I'm, you know, the residual is 10 then I'm paying 100.",
    "start": "2239270",
    "end": "2243990"
  },
  {
    "text": "Okay, so, so that's the squared loss. Um, there's also another loss.",
    "start": "2244930",
    "end": "2250910"
  },
  {
    "text": "I'll throw in here, um, called the absolute deviation loss. And this might actually be the last thought, if you didn't know about regression you might,",
    "start": "2250910",
    "end": "2259865"
  },
  {
    "text": "uh, immediately come to. It's basically the absolute difference between the prediction and,",
    "start": "2259865",
    "end": "2265790"
  },
  {
    "text": "um, the, the actual true target. [NOISE] Um, turns out the squared loss.",
    "start": "2265790",
    "end": "2272075"
  },
  {
    "text": "The- there's a kind of a longer discussion about, you know, which loss function, um, you know, makes sense.",
    "start": "2272075",
    "end": "2277775"
  },
  {
    "text": "The- the salient points here are that the absolute deviation loss is kind it has this kink here.",
    "start": "2277775",
    "end": "2284720"
  },
  {
    "text": "Um, and so it's not smooth. Sometimes it makes it harder to optimize, um, but the squared loss also has this kind of thing that blows up,",
    "start": "2284720",
    "end": "2294545"
  },
  {
    "text": "which means that it's, uh, uh, it really doesn't like having outliers or, uh,",
    "start": "2294545",
    "end": "2300320"
  },
  {
    "text": "really large values because it's gonna, you- you're gonna pay a lot for it. Um, but at this level,",
    "start": "2300320",
    "end": "2308180"
  },
  {
    "text": "just think about this as, you know, different losses. There's also something called a Huber loss which kind of, uh, um, combines both of these, is smooth,",
    "start": "2308180",
    "end": "2315410"
  },
  {
    "text": "and also grows linearly instead of quadratically. Um, okay, so we have both classification and regression.",
    "start": "2315410",
    "end": "2327050"
  },
  {
    "text": "We can define margins and residuals. We get either, uh, different loss functions out of it.",
    "start": "2327050",
    "end": "2333619"
  },
  {
    "text": "Right? Um, and now we want to minimize the loss. Okay? Um, so it turns out that for one example and this is really easy, right?",
    "start": "2333620",
    "end": "2344404"
  },
  {
    "text": "So if I- if I told you, okay, how do I minimize the loss here? Well, okay, it's 0. Done. [NOISE] So that- that's not super interesting.",
    "start": "2344405",
    "end": "2353345"
  },
  {
    "text": "And this corresponds to the fact that, you know, if you have a classifier, you're just trying to fit one point, um, it's really not that hard.",
    "start": "2353345",
    "end": "2360349"
  },
  {
    "text": "So that's kind of not the point. [NOISE] The point of machine learning is that you have to fit all of them.",
    "start": "2360350",
    "end": "2366005"
  },
  {
    "text": "Remember, you only get one weight vector, you have all of these examples, you have a million examples. And you want to find one weight vector that kind of balances,",
    "start": "2366005",
    "end": "2373460"
  },
  {
    "text": "uh, errors across all of them. And in general, you might not be able to achieve loss of 0, right?",
    "start": "2373460",
    "end": "2380990"
  },
  {
    "text": "So tough luck . Life is hard. Ah, so you have to make trade-offs, you know, which examples are you going to kind of sacrifice for the good of other examples.",
    "start": "2380990",
    "end": "2391535"
  },
  {
    "text": "And this is kind of actually a lot of where, you know, issues around fairness of machine learning actually come in",
    "start": "2391535",
    "end": "2397310"
  },
  {
    "text": "because in cases where you can't actually make a prediction that's, you know, equally good for everyone.",
    "start": "2397310",
    "end": "2402770"
  },
  {
    "text": "You know, how do you actually, you know, responsibly make these trade-offs. Um, but, you know,",
    "start": "2402770",
    "end": "2407780"
  },
  {
    "text": "that's a- that's a broader topic. Let's just focus on trade-off defined by the simple sum over all the loss examples.",
    "start": "2407780",
    "end": "2415910"
  },
  {
    "text": "So lets just say we want to minimize the average loss over all the examples.",
    "start": "2415910",
    "end": "2420964"
  },
  {
    "text": "Okay, so once we have these loss functions, if you average [NOISE] over the training set, you get something which we're gonna call a train loss.",
    "start": "2420965",
    "end": "2428135"
  },
  {
    "text": "Um, and that's a function of W. Right? So loss is on a particular example.",
    "start": "2428135",
    "end": "2433670"
  },
  {
    "text": "Train loss is on the entire data set. [NOISE]",
    "start": "2433670",
    "end": "2441309"
  },
  {
    "text": "Okay. So any questions about this, uh, so far?",
    "start": "2441310",
    "end": "2446060"
  },
  {
    "text": "Okay. So there is this, uh, discussion about which regression loss to use, which I'm gonna skip.",
    "start": "2451530",
    "end": "2457810"
  },
  {
    "text": "Um, you can feel free to read it in the notes if you're interested. The punchline is that if you want things that look like the mean square loss,",
    "start": "2457810",
    "end": "2465145"
  },
  {
    "text": "if you want things that look like the median, use the absolute deviation loss. Um, but I'll skip that for now. Yeah?",
    "start": "2465145",
    "end": "2471850"
  },
  {
    "text": "[inaudible] regression like this. Uh, when do people start thinking of regressions like in terms of loss minimization?",
    "start": "2471850",
    "end": "2481030"
  },
  {
    "text": "Yeah. Uh, so regression has, Least Squares Regression is from like the early 1800s.",
    "start": "2481030",
    "end": "2487585"
  },
  {
    "text": "Um, so it's been around for is- you know, kind of, you can call it the first machine learning that was ever done, um, if you- if you want, um,",
    "start": "2487585",
    "end": "2495865"
  },
  {
    "text": "I guess the loss minimization framework is, um, it's hard to kind of pinpoint a particular point in time, you know,",
    "start": "2495865",
    "end": "2505345"
  },
  {
    "text": "it's kind of not a terribly, uh, um, er, er, you know, it's not like,",
    "start": "2505345",
    "end": "2511600"
  },
  {
    "text": "uh, um, you know, innovation in some sense. It's just more of a- at least right now it's kind of a pedagogical tool to organize,",
    "start": "2511600",
    "end": "2518619"
  },
  {
    "text": "um, all the different methods that exist. Yeah. Say I'm training on mean and median.",
    "start": "2518620",
    "end": "2526494"
  },
  {
    "text": "Do you mean that like, uh, in that particular training, training set, the median would be the [NOISE] highest accuracy and the most confident,",
    "start": "2526495",
    "end": "2533785"
  },
  {
    "text": "whereas like with, uh, loss [inaudible] deviation would be the median instead of the mean?",
    "start": "2533785",
    "end": "2539000"
  },
  {
    "text": "Yeah. So, um, I don't wanna get into these examples but, uh, bri- briefly, if you have three points that you- you can't exactly f- fit perfectly,",
    "start": "2539000",
    "end": "2550989"
  },
  {
    "text": "um, you- if you use absolute deviation, then you're gonna find the median value.",
    "start": "2550989",
    "end": "2556930"
  },
  {
    "text": "You're gonna basically predict the median value. And if you use the square loss, you're gonna predict the mean value.",
    "start": "2556930",
    "end": "2562675"
  },
  {
    "text": "But, um, I'm happy to talk offline [NOISE] if- if you want. [NOISE]",
    "start": "2562675",
    "end": "2571065"
  },
  {
    "text": "Okay. So what we've talked about so far is we have these wonderful linear predictors which are driven by feature vectors and weight vectors,",
    "start": "2571065",
    "end": "2578220"
  },
  {
    "text": "and now we can define a bunch of different loss functions that capture, you know, how we care about,",
    "start": "2578220",
    "end": "2585675"
  },
  {
    "text": "um, you know, regression and classification. And now let's try to actually do some real, uh, machine learning.",
    "start": "2585675",
    "end": "2592825"
  },
  {
    "text": "How, how do you actually optimize these objectives? So remember the learner is going, uh,",
    "start": "2592825",
    "end": "2598435"
  },
  {
    "text": "so now we've talked about the optimization problem which is minimizing the training loss. Um, we'll come back to that next lecture.",
    "start": "2598435",
    "end": "2605289"
  },
  {
    "text": "Um, and then now we're gonna talk about optimization algorithm. Okay? So what is a optimization problem?",
    "start": "2605290",
    "end": "2613359"
  },
  {
    "text": "Now, remember last time we said, okay, let's just abstract away from the details a little bit. Let's not worry about if it's,",
    "start": "2613360",
    "end": "2619555"
  },
  {
    "text": "uh, the square loss or s- you know, some other loss. [NOISE] Um, let's just think about as a kind of abstract function.",
    "start": "2619555",
    "end": "2626724"
  },
  {
    "text": "So one-dimension, the training loss might look something like this. You have a single weight and for",
    "start": "2626725",
    "end": "2633430"
  },
  {
    "text": "each weight you have a number which is your loss on your training samples. [NOISE] Okay? And you want to find this point.",
    "start": "2633430",
    "end": "2640030"
  },
  {
    "text": "So in two dimensions, um, it looks something like this. Yeah. Let me try and actually draw this because I think it'll,",
    "start": "2640030",
    "end": "2646930"
  },
  {
    "text": "[NOISE] uh, be, um, useful in a bit to solve, let me pull this up. [NOISE]",
    "start": "2646930",
    "end": "2653290"
  },
  {
    "text": "Okay. So in two dimensions, um, what optimization looks like is as follows. So I'm gonna- I'm now plotting,",
    "start": "2653290",
    "end": "2660865"
  },
  {
    "text": "um, W_1 and W_2 which are the two components of this two-dimensional weight vector.",
    "start": "2660865",
    "end": "2667375"
  },
  {
    "text": "For every point I have a weight vector and that value is gonna be the loss, the training loss.",
    "start": "2667375",
    "end": "2673150"
  },
  {
    "text": "Um, and it's, er, you know, [NOISE] it's pretty standard in these settings to draw what are called level curves.",
    "start": "2673150",
    "end": "2679855"
  },
  {
    "text": "Um, so let's do this. So each curve here is a ring of points where,",
    "start": "2679855",
    "end": "2689035"
  },
  {
    "text": "uh, the function value is identical. So if you, uh, look at terrain maps, those are level curves.",
    "start": "2689035",
    "end": "2694900"
  },
  {
    "text": "So you know, kind of what I'm talking about. So this is the minimum and as you kind of grow out you get larger and larger, um.",
    "start": "2694900",
    "end": "2702285"
  },
  {
    "text": "Okay. I'll keep on doing this for a little bit. Okay. [NOISE] All right.",
    "start": "2702285",
    "end": "2707390"
  },
  {
    "text": "Um. [NOISE] And, uh, the goal is to find the minimum. Okay. All right.",
    "start": "2707390",
    "end": "2714119"
  },
  {
    "text": "So how are we gonna do this? So yeah, question. Assuming that there is a single minimum.",
    "start": "2714120",
    "end": "2720180"
  },
  {
    "text": "Yeah, why am I assuming, uh, there is a single minimum. [NOISE] in general for arbitrary loss functions,",
    "start": "2720180",
    "end": "2725405"
  },
  {
    "text": "there is not necessary a single minimum, I'm just doing this for simplicity. It turns out to be true for, um,",
    "start": "2725405",
    "end": "2731950"
  },
  {
    "text": "you know, uh, many of these linear classifiers. [NOISE]",
    "start": "2731950",
    "end": "2739960"
  },
  {
    "text": "Okay. So last time we talked about gradient descent, right? And the idea behind gradient descent is that well, I don't know where this is.",
    "start": "2739960",
    "end": "2748690"
  },
  {
    "text": "So let's just start at 0, [NOISE] as good as any place. And what I'm gonna do at 0 is I'm gonna compute the gradient.",
    "start": "2748690",
    "end": "2755425"
  },
  {
    "text": "So the gradient is this vector that's, uh, perpendicular to the level curves.",
    "start": "2755425",
    "end": "2761200"
  },
  {
    "text": "So the gradient is gonna point in this direction. That says, hey, in this direction is where",
    "start": "2761200",
    "end": "2767140"
  },
  {
    "text": "the function is increasing the most dramatically. Um, and gradient descent says,",
    "start": "2767140",
    "end": "2773485"
  },
  {
    "text": "um, takes- goes in the opposite direction, right? Because remember we wanna minimize loss.",
    "start": "2773485",
    "end": "2779035"
  },
  {
    "text": "Um, so I'm gonna go here. And, um, now I'll hopefully reduce my, uh,",
    "start": "2779035",
    "end": "2785845"
  },
  {
    "text": "function value, not necessarily but, um, we hope that's- that's the case. Now, we compute, uh, the gradient [NOISE] again.",
    "start": "2785845",
    "end": "2792760"
  },
  {
    "text": "The gradient says, um, you know, maybe it's pointing this way. So I go in that direction and maybe now it's,",
    "start": "2792760",
    "end": "2799359"
  },
  {
    "text": "uh, pointing this way. And I keep on going. Um, this is a little bit made up.",
    "start": "2799360",
    "end": "2806185"
  },
  {
    "text": "Um, but hopefully, eventually I get to the, um, the [NOISE] origin.",
    "start": "2806185",
    "end": "2812140"
  },
  {
    "text": "And you know, I'm, I'm kind of simplifying things quite a bit here. So in- there's a whole field of optimization that studies exactly what kind of",
    "start": "2812140",
    "end": "2819580"
  },
  {
    "text": "functions you can optimize and how gradient descent when it works and when it doesn't. Um, I'm just gonna kind of go through the mechanics now and defer",
    "start": "2819580",
    "end": "2827635"
  },
  {
    "text": "the kind of the formal proofs of when this actually works until, um, later.",
    "start": "2827635",
    "end": "2832850"
  },
  {
    "text": "Okay. So that's kind of the- the schema of how gradient descent works.",
    "start": "2833490",
    "end": "2838540"
  },
  {
    "text": "So in code this looks like this. So initialize at 0 and then loop in some number of iterations,",
    "start": "2838540",
    "end": "2844855"
  },
  {
    "text": "um, which let's- for simplicity just think there's a fixed number of iterations. And then, I'm gonna pick up my weights,",
    "start": "2844855",
    "end": "2851500"
  },
  {
    "text": "compute the gradient, move in the opposite direction, and then there's gonna be a step size that, uh, tells me how fast I want to,",
    "start": "2851500",
    "end": "2858280"
  },
  {
    "text": "you know, make progress. Okay? And we'll come back to, you know, uh, what,",
    "start": "2858280",
    "end": "2863710"
  },
  {
    "text": "uh, the step size, uh, does later.",
    "start": "2863710",
    "end": "2868160"
  },
  {
    "text": "Okay. So let's specialize it to a least squares, uh, regression. So we kind of did this last week,",
    "start": "2869140",
    "end": "2875930"
  },
  {
    "text": "but, um, just to kind of review, um. So the training loss for least squares regression is this.",
    "start": "2875930",
    "end": "2884240"
  },
  {
    "text": "So remember it's an average over the loss of individual examples,",
    "start": "2884240",
    "end": "2889340"
  },
  {
    "text": "and the loss of a particular example is the residual squared. So that's this expression.",
    "start": "2889340",
    "end": "2894755"
  },
  {
    "text": "Um, and then all we have to do is compute the gradient. And you know, if you remember your calculus,",
    "start": "2894755",
    "end": "2901204"
  },
  {
    "text": "it's just I've used the chain rule. So this two comes down here. You have the, um, you know,",
    "start": "2901205",
    "end": "2908030"
  },
  {
    "text": "the residual times the derivative of what's inside here and the gradient with respect to W is, uh, phi of x.",
    "start": "2908030",
    "end": "2916595"
  },
  {
    "text": "Okay. So last time we did this in Python in 1-dimension.",
    "start": "2916595",
    "end": "2922085"
  },
  {
    "text": "So 1-dimension, and hopefully all of you should feel comfortable doing this because this is just kind of basic, um, calculus.",
    "start": "2922085",
    "end": "2928145"
  },
  {
    "text": "Um, here we have w is a vector. So, uh, we're not taking derivatives but we're taking gradients.",
    "start": "2928145",
    "end": "2935975"
  },
  {
    "text": "Um, so there's, you know, some things to be, uh, wary of but in this case it's often kind of useful to double-check that.",
    "start": "2935975",
    "end": "2944560"
  },
  {
    "text": "Well, um, the gradient version actually matches, uh, the, the single-dimensional version",
    "start": "2944560",
    "end": "2951075"
  },
  {
    "text": "as well because last time remember we have the x out here. Um, and one thing to note here is that,",
    "start": "2951075",
    "end": "2960275"
  },
  {
    "text": "um, there's a prediction minus target, and that's the residual.",
    "start": "2960275",
    "end": "2965585"
  },
  {
    "text": "So the gradient is driven by, um, you know, kind of this quantity.",
    "start": "2965585",
    "end": "2970865"
  },
  {
    "text": "So if the prediction equals the target, uh, what's the gradient?",
    "start": "2970865",
    "end": "2975935"
  },
  {
    "text": "It's going to be 0 which is kind of what you want. If you're already getting the answer correct,",
    "start": "2975935",
    "end": "2981964"
  },
  {
    "text": "then you shouldn't want to move your, uh, your weights, right? So often you know we can do things in the abstract and everything will work.",
    "start": "2981965",
    "end": "2991849"
  },
  {
    "text": "But you know it's, it's often a good idea to write down some objective functions, take the gradient and see if gradient descent on using",
    "start": "2991850",
    "end": "3000100"
  },
  {
    "text": "these gradients that you computed is kind of a sensible thing because there's kind of many layers you can understand and get intuition for this stuff at",
    "start": "3000100",
    "end": "3008099"
  },
  {
    "text": "the kind of abstract level optimization or kind of at the algorithmic level. Like you pick up an example is it sensible to update when",
    "start": "3008100",
    "end": "3015720"
  },
  {
    "text": "the gradient other than when the prediction equals the target. Okay, so so let's take the code that we have from our, from last time,",
    "start": "3015720",
    "end": "3026010"
  },
  {
    "text": "and I'm going to expand on it a little bit, and hopefully set the stage for doing stochastic gradient.",
    "start": "3026010",
    "end": "3033825"
  },
  {
    "text": "Um, okay. So, so last time we had gradient descent.",
    "start": "3033825",
    "end": "3040570"
  },
  {
    "text": "Okay, so remember last time we defined a set of points, we defined the function which is the train loss here.",
    "start": "3040940",
    "end": "3047575"
  },
  {
    "text": "Um, we defined the derivative of the function, and then we have gradient descent. Okay, um, so I'm gonna do a little bit of housecleaning and I'm just,",
    "start": "3047575",
    "end": "3056619"
  },
  {
    "text": "uh, um, don't mind me. Um, okay so I'm gonna make this a little bit more explicit,",
    "start": "3056620",
    "end": "3064510"
  },
  {
    "text": "what this algorithm is. Gradient descent depends on, um, a function, a derivative of a function and let say, um, you know,",
    "start": "3064510",
    "end": "3073750"
  },
  {
    "text": "the dimensionality, um, and I can call this gradient FDF",
    "start": "3073750",
    "end": "3079780"
  },
  {
    "text": "and in this case it's, uh, D where D equals 2.",
    "start": "3079780",
    "end": "3084790"
  },
  {
    "text": "Okay, and I want to kind of separate. This is the kind of algorithms and this is, you know, modeling.",
    "start": "3084790",
    "end": "3093985"
  },
  {
    "text": "So this is what we want to compute and this is, you know, how we compute it. [NOISE]",
    "start": "3093985",
    "end": "3101050"
  },
  {
    "text": "Okay and this code should still work. Okay, um. All right, so what I'm gonna do now is,",
    "start": "3101050",
    "end": "3106630"
  },
  {
    "text": "um, upgrade this to vector. So remember the x here is just a number, right?",
    "start": "3106630",
    "end": "3112329"
  },
  {
    "text": "But we want to support vectors. Um, so in Python,",
    "start": "3112330",
    "end": "3118000"
  },
  {
    "text": "um, we're going to import NumPy so which is this, uh, nice vector and matrix library um, and,",
    "start": "3118000",
    "end": "3126710"
  },
  {
    "text": "um, I'm gonna make some, you know, arrays here, um, which this is just going to be a one-dimensional array.",
    "start": "3126900",
    "end": "3135970"
  },
  {
    "text": "So it's not that exciting. So this, this w dot x becomes,",
    "start": "3135970",
    "end": "3143000"
  },
  {
    "text": "uh, the actual dot I need to call. And I think w needs to be np.zeros(d).",
    "start": "3144540",
    "end": "3153950"
  },
  {
    "text": "Okay. All right. So that's just- should still run actually,",
    "start": "3154530",
    "end": "3160990"
  },
  {
    "text": "sorry, this is 1-dimensional. Okay. So remember last time we ran this,",
    "start": "3160990",
    "end": "3166720"
  },
  {
    "text": "uh, this program and, um, it starts out with some weights and then it",
    "start": "3166720",
    "end": "3172480"
  },
  {
    "text": "converges to 0.8 and the function value kind of keeps on going on. Okay. All right, so let's,",
    "start": "3172480",
    "end": "3178600"
  },
  {
    "text": "let's try to, um, you know it's really hard to kind of see you whether",
    "start": "3178600",
    "end": "3185040"
  },
  {
    "text": "this algorithm is any, doing anything interesting because we only have two points, it's kind of trivial.",
    "start": "3185040",
    "end": "3190235"
  },
  {
    "text": "So how do we go about, um, you know, because I'm going to also implement stochastic gradient descent.",
    "start": "3190235",
    "end": "3196810"
  },
  {
    "text": "How do we have kind of a test case to see if this algorithm is, you know, working? Um, so there's kind of this technique which I,",
    "start": "3196810",
    "end": "3204205"
  },
  {
    "text": "I really like [NOISE] which is to call, generate artificial data and ideas that, you know, what is learning.",
    "start": "3204205",
    "end": "3212950"
  },
  {
    "text": "You're learning as you're taking a dataset and you're trying to fit- find the, the weights that best fit our dataset.",
    "start": "3212950",
    "end": "3219250"
  },
  {
    "text": "Uh, but in general if I generate some arbitrary, if I downloaded a dataset I have no idea what the right kind of quote unquote right answer is.",
    "start": "3219250",
    "end": "3226840"
  },
  {
    "text": "So there's a technique where I go backwards and say, okay let's let's decide what the right answer is.",
    "start": "3226840",
    "end": "3232375"
  },
  {
    "text": "So let's say the right answer is, um, 1, 2, 3, 4, 5. So it's a 5-dimensional problem.",
    "start": "3232375",
    "end": "3238255"
  },
  {
    "text": "Okay. Um, and I'm going to generate some data based on that so that this,",
    "start": "3238255",
    "end": "3247329"
  },
  {
    "text": "uh, weight vector is kind of good for that data. Um, I'm going to skip all my breaks in this lecture.",
    "start": "3247330",
    "end": "3257275"
  },
  {
    "text": "Um, so I'm going to generate a bunch of points. So let's generate 10,000 point.",
    "start": "3257275",
    "end": "3263140"
  },
  {
    "text": "The nice thing about artificial data is you can generate as much as you'd want. Um, there's a question, yeah? A true w?",
    "start": "3263140",
    "end": "3270340"
  },
  {
    "text": "So true w just means like the, the correct, the ground truth, the w.",
    "start": "3270340",
    "end": "3275380"
  },
  {
    "text": "The true y, true output or? So w is a weight vector.",
    "start": "3275380",
    "end": "3281260"
  },
  {
    "text": "So this is kind of going backwards. Remember, I want to fit the weight vector but um, I'm just kind of saying this is the right answer.",
    "start": "3281260",
    "end": "3288700"
  },
  {
    "text": "So I want to make sure that the algorithm actually recovers this later. Okay, so I'm going to generate some random data.",
    "start": "3288700",
    "end": "3294850"
  },
  {
    "text": "So there's a nice function, random.randn which generates a random d-dimensional vector and y.",
    "start": "3294850",
    "end": "3303130"
  },
  {
    "text": "I'm gonna set- what should I set y to?",
    "start": "3303130",
    "end": "3311043"
  },
  {
    "text": "Which side of w you want? Yeah. So I'm gonna do regressions. So I want to do, uh,",
    "start": "3311043",
    "end": "3317394"
  },
  {
    "text": "true_w dot uh, x, right?",
    "start": "3317395",
    "end": "3322825"
  },
  {
    "text": "So I mean if you think about it, if I took this data and I",
    "start": "3322825",
    "end": "3329965"
  },
  {
    "text": "found the, the like true one- w is the right thing that we'll get 0 loss here.",
    "start": "3329965",
    "end": "3335724"
  },
  {
    "text": "Okay. But I'm going to make your life a little bit more interesting and we're gonna add some noise.",
    "start": "3335725",
    "end": "3343690"
  },
  {
    "text": "Okay, so let's print out what that looks like.",
    "start": "3343690",
    "end": "3349615"
  },
  {
    "text": "Also I should add it to my dataset. So okay, so this is my dataset.",
    "start": "3349615",
    "end": "3360010"
  },
  {
    "text": "Okay, I mean, I can't really tell what's going on but, but you can look at the code and you, you can assure yourself that,",
    "start": "3360010",
    "end": "3367660"
  },
  {
    "text": "uh, this data has structure in it. [NOISE] Okay, so let's get rid of this print statement and let's train and see what happens.",
    "start": "3367660",
    "end": "3377320"
  },
  {
    "text": "So let's. Okay. Oh, one thing I forgot to do.",
    "start": "3377320",
    "end": "3384069"
  },
  {
    "text": "Um, so if you notice that the objective functions that I've, uh, written down they haven't divided by the number of data points.",
    "start": "3384070",
    "end": "3392530"
  },
  {
    "text": "I want the average loss, not the, the sum. Um, it turns out that, you know if you have the sum,",
    "start": "3392530",
    "end": "3398380"
  },
  {
    "text": "then things get really big and you know, blow up. So let me just normalize that. Okay. So let me lock it.",
    "start": "3398380",
    "end": "3408965"
  },
  {
    "text": "Okay, so it's training, it's training. Um, actually so let me, uh, do more iterations.",
    "start": "3408965",
    "end": "3414930"
  },
  {
    "text": "So I did 100 iterations, let's do 1000 iterations. Okay. So when the function value is going down,",
    "start": "3414930",
    "end": "3422875"
  },
  {
    "text": "that's always something to- you know, good to check. Um, and you can see the weights are kind of slowly getting to,",
    "start": "3422875",
    "end": "3429565"
  },
  {
    "text": "you know, what appears to be 1, 2, 3, 4, 5, right? Okay. So this is a hard proof but it's kind of",
    "start": "3429565",
    "end": "3437800"
  },
  {
    "text": "evidence that this learning algorithm is actually kind of doing the right thing.",
    "start": "3437800",
    "end": "3442310"
  },
  {
    "text": "Um, okay so now let's see if I add, you know more points.",
    "start": "3443010",
    "end": "3449620"
  },
  {
    "text": "So I now have 100,000 points. Now, you know, obviously it gets slower,",
    "start": "3449620",
    "end": "3456400"
  },
  {
    "text": "um, and you'll, you know, hopefully get there you know, one day but I'm just gonna kill it.",
    "start": "3456400",
    "end": "3461359"
  },
  {
    "text": "Okay, any questions about, uh, oops, my terminal got screwed up.",
    "start": "3462060",
    "end": "3467575"
  },
  {
    "text": "Okay. So what did I do here, I defined loss functions, took their derivatives.",
    "start": "3467575",
    "end": "3473620"
  },
  {
    "text": "Um, the gradient descent is what we implemented last time and the only thing different",
    "start": "3473620",
    "end": "3479140"
  },
  {
    "text": "I did, this time is generated data sets so I can kind of check whether gradient descent is working. Yeah question.",
    "start": "3479140",
    "end": "3484750"
  },
  {
    "text": "So the fact that the gradient is just the residual [inaudible]",
    "start": "3484750",
    "end": "3491000"
  },
  {
    "text": "a algorithm to learn from overpredictions versus like underpredictions?",
    "start": "3491000",
    "end": "3496600"
  },
  {
    "text": "The question is whether the fact that the gradient is residual allows the algorithm to learn from under or over predictions.",
    "start": "3496600",
    "end": "3503109"
  },
  {
    "text": "Um, yeah. So the gradient is if you think about it, yeah that's good intuition.",
    "start": "3503110",
    "end": "3508540"
  },
  {
    "text": "So if you look at, um, if you're over-predicting, right?",
    "start": "3508540",
    "end": "3513595"
  },
  {
    "text": "That means the gradient is kind of- assume that this is like 1. So that means this is going to be positive which means that, hey if you opt that way,",
    "start": "3513595",
    "end": "3521425"
  },
  {
    "text": "you're going to over-predict more and more and incur more loss. So, um, by subtracting a gradient,",
    "start": "3521425",
    "end": "3526555"
  },
  {
    "text": "you're kind of pushing the weights out in the other direction and same for when you're, um, you're under-predicting.",
    "start": "3526555",
    "end": "3532585"
  },
  {
    "text": "Yeah, so that's good intuition to have. Yeah. What is the effect of the noise when you generate [inaudible]",
    "start": "3532585",
    "end": "3543425"
  },
  {
    "text": "What is the effect of the noise? Um, the effect of the noise, it makes the problem a little bit, you know,",
    "start": "3543425",
    "end": "3549590"
  },
  {
    "text": "harder so that it takes more examples to learn. Um, if you shut off the noise then it will- you know, we can try that.",
    "start": "3549590",
    "end": "3557750"
  },
  {
    "text": "Um, I've never done this before, but presumably you'll learn, you know, f- faster, but maybe not.",
    "start": "3557750",
    "end": "3564065"
  },
  {
    "text": "Um, the noise isn't, you know, that much. But, um, okay.",
    "start": "3564065",
    "end": "3573200"
  },
  {
    "text": "So, so let's say you have, you know, like 500 examp- 1000 examples.",
    "start": "3573200",
    "end": "3579244"
  },
  {
    "text": "You know, that's quite a few examples. As in now, you know, this algorithm runs, you know, pretty slowly, right?",
    "start": "3579245",
    "end": "3584350"
  },
  {
    "text": "And in- in modern machine learning you have, you know, millions or hundreds of millions of examples. So gradient descent is gonna be, you know, pretty slow.",
    "start": "3584350",
    "end": "3593204"
  },
  {
    "text": "So how can we speed things up a little bit, and what's the problem here?",
    "start": "3593205",
    "end": "3600214"
  },
  {
    "text": "Well, if you look at the- the- what the algorithm is doing, it's iterating.",
    "start": "3600215",
    "end": "3606110"
  },
  {
    "text": "And each iteration it's computing the gradient of the training loss. And the training loss is,",
    "start": "3606110",
    "end": "3611585"
  },
  {
    "text": "um, average of all the points, which means that you have to go through all the points and you compute the lo- gradient of the loss and you add everything up.",
    "start": "3611585",
    "end": "3618800"
  },
  {
    "text": "And that's what is expensive and, you know, it takes time. So, you know, you might wonder,",
    "start": "3618800",
    "end": "3626030"
  },
  {
    "text": "well, how, how can you avoid this? I mean, you- if you wanted to do gradient descent you have to go through all your points.",
    "start": "3626030",
    "end": "3631625"
  },
  {
    "text": "Um, and the, the key insight behind stochastic gradient descent is that,",
    "start": "3631625",
    "end": "3637430"
  },
  {
    "text": "well maybe- maybe you don't have to do that. So, um, maybe- you know,",
    "start": "3637430",
    "end": "3643325"
  },
  {
    "text": "here- here's some intuition, right? So what is- what is this gradient?",
    "start": "3643325",
    "end": "3648575"
  },
  {
    "text": "So this gradient is actually the sum of all the gradients from all the examples in your training set.",
    "start": "3648575",
    "end": "3655025"
  },
  {
    "text": "Right? So we have 500,000 points adding to that. So actually what this gradient is- is, um,",
    "start": "3655025",
    "end": "3661925"
  },
  {
    "text": "it's actually kind of a sum of different things which are maybe pointing in slightly different directions which all average out to this direction.",
    "start": "3661925",
    "end": "3669755"
  },
  {
    "text": "Okay. So maybe you can actually not average all of them,",
    "start": "3669755",
    "end": "3677390"
  },
  {
    "text": "but you can, um, average just a couple or maybe even in an extreme case you can just like take one of them and just,",
    "start": "3677390",
    "end": "3684724"
  },
  {
    "text": "you know, march in that direction. So, so here's the idea behind stochastic gradient descent. So instead of doing gradient descent,",
    "start": "3684725",
    "end": "3691145"
  },
  {
    "text": "we are going to change the algorithm to say for each example in the training set,",
    "start": "3691145",
    "end": "3696755"
  },
  {
    "text": "I'm just going to pick it up and just update, you know. It's- instead of like sitting down and",
    "start": "3696755",
    "end": "3703460"
  },
  {
    "text": "looking at all of the training examples and thinking really hard, I'm just gonna pick up one training example and update right away.",
    "start": "3703460",
    "end": "3708785"
  },
  {
    "text": "So again, the key idea here is, it's not about quality it's about, uh, quantity.",
    "start": "3708785",
    "end": "3713795"
  },
  {
    "text": "May be not the world's best life lesson, but it seems to work in- it works in here.",
    "start": "3713795",
    "end": "3720300"
  },
  {
    "text": "Um, and then, there's also this question of what should the step size be?",
    "start": "3720850",
    "end": "3726140"
  },
  {
    "text": "And in- generally, in stochastic gradient descent, it's actually even a bit more important because,",
    "start": "3726140",
    "end": "3732605"
  },
  {
    "text": "um, when you're updating on each- each individual example, you're getting kind of noisy estimates of the actual gradient.",
    "start": "3732605",
    "end": "3739310"
  },
  {
    "text": "And, uh, and people often ask me like, \"Oh, how should I set my step size and all.\" And the answer is like there is no formula.",
    "start": "3739310",
    "end": "3747425"
  },
  {
    "text": "I mean, there are formulas, but there's no kind of definitive answer. Here's some general guidance.",
    "start": "3747425",
    "end": "3753575"
  },
  {
    "text": "Um, so if step size is small, so really close to 0, that means you are taking tiny steps, right?",
    "start": "3753575",
    "end": "3760355"
  },
  {
    "text": "That means that it'll take longer to get where you want to go, but you're kind of proceeding cautiously.",
    "start": "3760355",
    "end": "3766400"
  },
  {
    "text": "so it's less likely you're gonna, you know- uh, if you mess up and go in",
    "start": "3766400",
    "end": "3771440"
  },
  {
    "text": "the wrong direction you're not gonna go too far in the wrong direction. Um, conversely, if you have it to be really,",
    "start": "3771440",
    "end": "3777800"
  },
  {
    "text": "really, large then, you know, it's like a race car. You, kind of, drive really fast,",
    "start": "3777800",
    "end": "3783050"
  },
  {
    "text": "but you might just kind of bounce around a lot. So, pictorially what this looks like is that, you know,",
    "start": "3783050",
    "end": "3790145"
  },
  {
    "text": "here's maybe a moderate step size, but if you're taking steps, really big steps, um,",
    "start": "3790145",
    "end": "3795785"
  },
  {
    "text": "you might go over here and then you jump around and then maybe, maybe you'll end up in the right place but maybe",
    "start": "3795785",
    "end": "3801155"
  },
  {
    "text": "sometimes you can actually get flung off out of orbit and diverge to infinity which is a bad situation.",
    "start": "3801155",
    "end": "3809119"
  },
  {
    "text": "Um, so there's many ways to set the step size. You can set it to a, you know, constant.",
    "start": "3809120",
    "end": "3815270"
  },
  {
    "text": "You can- usually, you have to, um, you know, tune it. Or you can set it to be decreasing the intuition",
    "start": "3815270",
    "end": "3822320"
  },
  {
    "text": "being that as you optimize and get closer to the optimum, you kind of want to slow down, right? Like if you- you're coming on the freeway, you're driving really fast,",
    "start": "3822320",
    "end": "3829970"
  },
  {
    "text": "but once you get to your house you probably don't want to be like driving 60 miles an hour.",
    "start": "3829970",
    "end": "3834720"
  },
  {
    "text": "Okay. So- actually I didn't implement stochastic gradient.",
    "start": "3835000",
    "end": "3840380"
  },
  {
    "text": "So let me do that. So let's, let's try to get stochastic gradient up and going here.",
    "start": "3840380",
    "end": "3846059"
  },
  {
    "text": "Okay. So, so the interface to stochastic gradient changes.",
    "start": "3846880",
    "end": "3852349"
  },
  {
    "text": "So- right? So the- in gradients then all you need is a function. And it just kind of computes the sum over all the training examples.",
    "start": "3852350",
    "end": "3861180"
  },
  {
    "text": "Um, so in stochastic gradient, I'm just going to denote S as for stochastic gradient.",
    "start": "3861180",
    "end": "3867140"
  },
  {
    "text": "I'm gonna take an index I, and I'm going to update on the Ith point only.",
    "start": "3867140",
    "end": "3872840"
  },
  {
    "text": "So I'm going to only compute the loss on the Ith point. And same for its derivative. Um, you can look at the Ith point,",
    "start": "3872840",
    "end": "3881435"
  },
  {
    "text": "um, and just compute the gradient on that Ith point.",
    "start": "3881435",
    "end": "3886910"
  },
  {
    "text": "Okay? And this should be called SDF. Okay. So now instead of doing gradient descent,",
    "start": "3886910",
    "end": "3895174"
  },
  {
    "text": "let's do stochastic gradient descent. And I'm going to pass in sf, sdf, d, and,",
    "start": "3895175",
    "end": "3903140"
  },
  {
    "text": "um, the number of points because I need to know how many points there are now. Um, copy gradient descent,",
    "start": "3903140",
    "end": "3910099"
  },
  {
    "text": "and it's basically kind of the same function. I'm just going to stick another for loop there. So stochastic gradient descent,",
    "start": "3910100",
    "end": "3915829"
  },
  {
    "text": "it's going to take the stochastic functions, stochastic gradient, the dimensionality and- Okay?",
    "start": "3915829",
    "end": "3923180"
  },
  {
    "text": "So now, before I was just going through, um, number of iterations and now, right,",
    "start": "3923180",
    "end": "3930245"
  },
  {
    "text": "I'm not going to try to compute the value of the- all the training examples. I'm going to, um,",
    "start": "3930245",
    "end": "3935990"
  },
  {
    "text": "loop over all the points and I'm going to call just evaluate the function at that point I,",
    "start": "3935990",
    "end": "3945680"
  },
  {
    "text": "and compute the gradient at that point I instead of the entire, you know, dataset. And then everything else is the same.",
    "start": "3945680",
    "end": "3953015"
  },
  {
    "text": "I mean, one other thing I'll do here is that I'll use a different step size schedule. So um, 1 divided by number of updates.",
    "start": "3953015",
    "end": "3963060"
  },
  {
    "text": "So I want it so that the number of, uh, the step size is gonna decrease over time.",
    "start": "3963370",
    "end": "3970950"
  },
  {
    "text": "Okay, so I start with a equals 1 and then it's half, and then it's a third, and it's a fourth, and it keeps on going down.",
    "start": "3973150",
    "end": "3980570"
  },
  {
    "text": "Um, sometimes you can put a square root and that's more typical in some cases, but, um, I'm not going to worry about the details too much. Uh, question?",
    "start": "3980570",
    "end": "3989580"
  },
  {
    "text": "The point I is the chosen randomly but here we just [inaudible].",
    "start": "3990190",
    "end": "3995569"
  },
  {
    "text": "Yes. The question is- the word stochastic means that there should be some randomness here.",
    "start": "3995570",
    "end": "4000655"
  },
  {
    "text": "And, you know, technically speaking, the- the stochastic gradient descent is where",
    "start": "4000655",
    "end": "4006220"
  },
  {
    "text": "you're sampling a random point and then you're updating on it. I'm cheating a little bit, um, uh, because I'm iterating over all the points.",
    "start": "4006220",
    "end": "4015700"
  },
  {
    "text": "You know, in practice if you have a lot of points and you randomize the order it's kind of- it's- it's",
    "start": "4015700",
    "end": "4020770"
  },
  {
    "text": "similar but it's- there is a kind of a technical difference that I'm trying to hide.",
    "start": "4020770",
    "end": "4026900"
  },
  {
    "text": "Okay. So- so this is stochastic gradient descent. Um, to iterate, you know,",
    "start": "4027720",
    "end": "4034555"
  },
  {
    "text": "go over all the points and just, you know update. Okay? Um, so let's see if this works.",
    "start": "4034555",
    "end": "4042580"
  },
  {
    "text": "Um, okay.",
    "start": "4042580",
    "end": "4045620"
  },
  {
    "text": "I don't think that worked. [LAUGHTER] Maybe- let's see what happened here?",
    "start": "4049470",
    "end": "4058825"
  },
  {
    "text": "I did try it on 100,000 points. Maybe that works. And, nope, doesn't work either.",
    "start": "4058825",
    "end": "4065120"
  },
  {
    "text": "Um, anyone see the problem?",
    "start": "4069570",
    "end": "4077620"
  },
  {
    "text": "[inaudible]",
    "start": "4077620",
    "end": "4085150"
  },
  {
    "text": "So I'm printing this, um, out, uh, at the- at the end,",
    "start": "4085150",
    "end": "4092200"
  },
  {
    "text": "um, of each iteration. So that should be fine, um.",
    "start": "4092200",
    "end": "4098000"
  },
  {
    "text": "Really, this should work.",
    "start": "4098000",
    "end": "4099001"
  },
  {
    "text": "So gradient descent was working, right? Maybe I'll, I'll try- It's probably not the best idea to be debugging this live.",
    "start": "4108420",
    "end": "4117730"
  },
  {
    "text": "Okay. Let's, let's make sure gradient descent works. Um, okay, so that was working right.",
    "start": "4117730",
    "end": "4125500"
  },
  {
    "text": "Okay. So stochastic gradient descent. I mean, it's really fast and converges,",
    "start": "4125500",
    "end": "4133180"
  },
  {
    "text": "[LAUGHTER] but it doesn't converge to the right answer.",
    "start": "4133180",
    "end": "4136940"
  },
  {
    "text": "I think [inaudible]. Yeah, but that should get incremented to 1.",
    "start": "4138750",
    "end": "4145330"
  },
  {
    "text": "So that-",
    "start": "4145330",
    "end": "4150759"
  },
  {
    "text": "It might be true.",
    "start": "4150760",
    "end": "4157469"
  },
  {
    "text": "Okay, so I do have a version of this code that does work. [LAUGHTER] So what am I doing here, that's different.",
    "start": "4157470",
    "end": "4163310"
  },
  {
    "text": "Okay, I'll have some water. Maybe I need some water. [LAUGHTER]",
    "start": "4163310",
    "end": "4169599"
  },
  {
    "text": "Okay, so this version works. Yeah.",
    "start": "4169600",
    "end": "4175000"
  },
  {
    "text": "[inaudible] Yeah, that's- that's probably good.",
    "start": "4175000",
    "end": "4181674"
  },
  {
    "text": "That's a good call. Yeah. okay.",
    "start": "4181675",
    "end": "4186170"
  },
  {
    "text": "All right. Now, it works. Thank you. [LAUGHTER]",
    "start": "4187260",
    "end": "4192000"
  },
  {
    "text": "Um, so yeah. Yeah, this is a good lesson. Um, it's that when you're dividing, um,",
    "start": "4193710",
    "end": "4201655"
  },
  {
    "text": "these needs to be one- actually in Python 3, this is not a problem but I'm so- on Python 2 for some reason.",
    "start": "4201655",
    "end": "4207175"
  },
  {
    "text": "But this should be, uh, 1.0 divided by numUpdates. Otherwise, I was getting- So how is it faster?",
    "start": "4207175",
    "end": "4213340"
  },
  {
    "text": "Okay. So why is it faster? [LAUGHTER]. Yeah, okay.",
    "start": "4213340",
    "end": "4218890"
  },
  {
    "text": "Okay. Let's- let's, uh, go back to 500,000, okay.",
    "start": "4218890",
    "end": "4224930"
  },
  {
    "text": "Okay. So one full sweep over the data is the same amount of time.",
    "start": "4226130",
    "end": "4231929"
  },
  {
    "text": "But you notice that immediately, it already converges to 1, 2, 3, 4, 5, right?",
    "start": "4231930",
    "end": "4239435"
  },
  {
    "text": "So this is like way, way faster than gradient descent. Remember, I just, uh, kind of compare it.",
    "start": "4239435",
    "end": "4244480"
  },
  {
    "text": "Um, gradient descent is, um, you run it.",
    "start": "4244480",
    "end": "4250540"
  },
  {
    "text": "And after one stop, it's, like, not even close. Right. Yeah?",
    "start": "4250540",
    "end": "4257020"
  },
  {
    "text": "What noise levels you have to have until gradient descent becomes better? What noise levels you have to have until gradient descent becomes better?",
    "start": "4257020",
    "end": "4265165"
  },
  {
    "text": "Um, so it is true that if you have more noise, then gradient descent might be, uh,",
    "start": "4265165",
    "end": "4271480"
  },
  {
    "text": "stochastic gradient descent can be unstable. Um, there might be ways to mitigate that with step size choices.",
    "start": "4271480",
    "end": "4277030"
  },
  {
    "text": "But, um, yeah, probably, you have to add a lot of noise for stochastic gradient to be, um, really bad.",
    "start": "4277030",
    "end": "4283750"
  },
  {
    "text": "Um, I mean, this is in some sense, you know, if you take a step back and think about what's going on in this problem,",
    "start": "4283750",
    "end": "4289165"
  },
  {
    "text": "it's a 5-dimensional problem. There's only five numbers and I'm feeding it half a million data points, right?",
    "start": "4289165",
    "end": "4296755"
  },
  {
    "text": "There, there aren't- there's not that much to learn here. And so there's a lot of redundancy in the dataset.",
    "start": "4296755",
    "end": "4303160"
  },
  {
    "text": "And generally, actually, this is true. I go into a large dataset, there's gonna be a lot of, you know, redundancy. So, uh, going through all of the data and then try to make an informed decision is,",
    "start": "4303160",
    "end": "4312940"
  },
  {
    "text": "you know, pretty wasteful, where sometimes you can just kind of get a representative sample from, um, one example or more as common to do the",
    "start": "4312940",
    "end": "4320460"
  },
  {
    "text": "like of kind of mini-batches where you maybe grab a hundred examples and you update on that which is- so there's",
    "start": "4320460",
    "end": "4325830"
  },
  {
    "text": "a way to be somewhere in between stochastic gradient and gradient descent. Okay, let me move on. Um.",
    "start": "4325830",
    "end": "4331160"
  },
  {
    "text": "Okay. Summary so far, we have linear predictors, um, which are based on scores.",
    "start": "4331160",
    "end": "4338469"
  },
  {
    "text": "So linear predictors we include both classifiers and regressors, um, we can do loss minimization,",
    "start": "4338470",
    "end": "4343900"
  },
  {
    "text": "and we can, uh, if we implement it correctly, we can do, uh, SGD. Okay. So that was- I'm kind of switching things.",
    "start": "4343900",
    "end": "4353650"
  },
  {
    "text": "I hope you are kind of following along. I'll introduced binary classification and then, I did all the optimization for linear regression.",
    "start": "4353650",
    "end": "4360969"
  },
  {
    "text": "So now, let's go back to classification and see if we could do stochastic gradient descent here.",
    "start": "4360970",
    "end": "4366820"
  },
  {
    "text": "Okay. So for classification, remember, we decided that the zero-one loss is the thing we want.",
    "start": "4366820",
    "end": "4372565"
  },
  {
    "text": "We want to minimize the number of mistakes. You know, who can argue with that? Um, so rem- remember, what is zero-one loss look like? It looks like this.",
    "start": "4372565",
    "end": "4379765"
  },
  {
    "text": "Okay? So what happens if I try to run stochastic gradient descent on this?",
    "start": "4379765",
    "end": "4384440"
  },
  {
    "text": "Um, I mean, I can run the code, but [OVERLAPPING] yeah, it's- it won't work,",
    "start": "4386340",
    "end": "4393985"
  },
  {
    "text": "right? And why won't it work? [inaudible]. Yeah. So two popular answers are it's not differentiable,",
    "start": "4393985",
    "end": "4401695"
  },
  {
    "text": "that's- it's one problem. Um, but I think that the- the bigger problem and kind of deeper problem is that,",
    "start": "4401695",
    "end": "4407530"
  },
  {
    "text": "what is the- what is the gradient? Zero. Zero. It's like zero, basically everywhere except for this point,",
    "start": "4407530",
    "end": "4413170"
  },
  {
    "text": "which are, you know, it doesn't really matter. So, um, so as- as we learned that if you try to update with a gradient of 0,",
    "start": "4413170",
    "end": "4420639"
  },
  {
    "text": "um, then you, you won't move your weights, right? So gradient descent will not work on the zero-one, uh, loss.",
    "start": "4420640",
    "end": "4427855"
  },
  {
    "text": "Um, so that's- that's kind of unfortunate. So how should we fix this problem? Yeah?",
    "start": "4427855",
    "end": "4437590"
  },
  {
    "text": "[inaudible] Yeah, let's, let's make the gradient non-zero. Let's skew things. Um, so there's one loss,",
    "start": "4437590",
    "end": "4446680"
  },
  {
    "text": "which I'm gonna introduce called the hinge loss, which, uh, does exactly that. Um, so let me write the hinge loss down.",
    "start": "4446680",
    "end": "4454015"
  },
  {
    "text": "And the hinge loss, um, is basically, uh, is zero here when the margin is greater than or equal to 1 and rises linearly.",
    "start": "4454015",
    "end": "4464290"
  },
  {
    "text": "So if you've gotten it correct by a margin of 1 so you're kind of pretty safely on the err side of,",
    "start": "4464290",
    "end": "4470665"
  },
  {
    "text": "um, getting it correct, then we won't charge you anything. But as soon as you start, you know, dip into this area,",
    "start": "4470665",
    "end": "4477160"
  },
  {
    "text": "we're gonna charge you a kind of a linear amount and your loss is gonna grow linearly. Um, so there's some reasons why this is a good idea.",
    "start": "4477160",
    "end": "4484780"
  },
  {
    "text": "So it upper bounds the zero-one loss, um, it's, uh, it has a property called- known as convexity,",
    "start": "4484780",
    "end": "4491500"
  },
  {
    "text": "which means that if you actually run the gradient descent, you're actually gonna converge to the global optimum. Um, I'm not gonna get into that.",
    "start": "4491500",
    "end": "4498670"
  },
  {
    "text": "And so that's, you know, that's a hinge loss. Um, so what remains to be done is to compute the gradient of this,",
    "start": "4498670",
    "end": "4508554"
  },
  {
    "text": "you know, hinge loss, okay? So how do you compute this gradient? So in some sense, it's a trick question because",
    "start": "4508555",
    "end": "4516640"
  },
  {
    "text": "the gradient doesn't exist because it's not, um, you know, differentiable everywhere,",
    "start": "4516640",
    "end": "4521710"
  },
  {
    "text": "but we're gonna pre- pretend that little point doesn't exist, okay? So, so what is this hinge loss?",
    "start": "4521710",
    "end": "4529255"
  },
  {
    "text": "The hinge loss is actually two functions, right? There is a zero function here and then there's like this,",
    "start": "4529255",
    "end": "4536139"
  },
  {
    "text": "uh, 1 minus x function. So what am I plotting here? I'm plotting the- the margin and, uh, the loss.",
    "start": "4536140",
    "end": "4544195"
  },
  {
    "text": "Okay? So this is, uh, the zero function, and this is, uh, 1 minus, uh, w dot phi of xy.",
    "start": "4544195",
    "end": "4551920"
  },
  {
    "text": "And the hinge loss is just the maxima of these two functions. So at every point,",
    "start": "4551920",
    "end": "4556930"
  },
  {
    "text": "I'm just taking the top function. So um, that's how I am able to trace out, uh, this- this curve.",
    "start": "4556930",
    "end": "4563380"
  },
  {
    "text": "Okay? All right. So if I want to take the gradient of this function,",
    "start": "4563380",
    "end": "4569500"
  },
  {
    "text": "you know, you, you can try to do the math. Well, let's think through it. You know, what- what should the gradient be?",
    "start": "4569500",
    "end": "4575500"
  },
  {
    "text": "Um, we're, we're here, what should the gradient be? It's zero. And if I'm here, what should the gradient be?",
    "start": "4575500",
    "end": "4582640"
  },
  {
    "text": "It should be the- whatever the gradient of this function is, right? So in general, when you have a gradient of this- of this kind of max,",
    "start": "4582640",
    "end": "4589960"
  },
  {
    "text": "uh, you have to kind of break it up into cases. Um, and depending on where you are,",
    "start": "4589960",
    "end": "4595255"
  },
  {
    "text": "um, you, you have a different case. So loss is equal to- if I'm over here,",
    "start": "4595255",
    "end": "4603159"
  },
  {
    "text": "and what's the condition for being over here? If the margin is greater than 1, right?",
    "start": "4603160",
    "end": "4616495"
  },
  {
    "text": "And then otherwise, I'm going to take the gradient of this with respect to w,",
    "start": "4616495",
    "end": "4621925"
  },
  {
    "text": "which is gonna be minus phi of x y, you know, otherwise.",
    "start": "4621925",
    "end": "4626994"
  },
  {
    "text": "Okay? Um, so again, we can try to interpret the, the gradient of the hinge loss.",
    "start": "4626995",
    "end": "4635290"
  },
  {
    "text": "So remember your stochastic gradient descent, you have a weight vector, and you're gonna pick up an example and you say,",
    "start": "4635290",
    "end": "4640555"
  },
  {
    "text": "Oh, let's compute the gradient move away from it. So if you're getting the example right, then the gradient zero don't move, which is the right thing to do.",
    "start": "4640555",
    "end": "4649179"
  },
  {
    "text": "And otherwise, you're going to move in that direction because you're minus, minus of phi of x y,",
    "start": "4649180",
    "end": "4656230"
  },
  {
    "text": "which kind of imprints this example into your weight vector. So- and you can formally show that it actually increases your, uh,",
    "start": "4656230",
    "end": "4663550"
  },
  {
    "text": "margin after you do this. Okay? Yeah?",
    "start": "4663550",
    "end": "4669595"
  },
  {
    "text": "What's the significance of the margin being 1? What's the significance of the margin being 1?",
    "start": "4669595",
    "end": "4675025"
  },
  {
    "text": "Um, this is a little bit arbitrary, you're just kind of sending a non-zero value.",
    "start": "4675025",
    "end": "4680440"
  },
  {
    "text": "Um, and, and, you know, in support vector machines, you set it to 1, and then you have regularization on the weights and",
    "start": "4680440",
    "end": "4687850"
  },
  {
    "text": "that gives you, uh, some interpretation. So I don't have time to go over that right now, but, uh, feel free to ask me later.",
    "start": "4687850",
    "end": "4695830"
  },
  {
    "text": "There's another loss function. Uh, do you have a question? Yeah. Why is the or why do we choose the margin if it's",
    "start": "4695830",
    "end": "4703989"
  },
  {
    "text": "a loss function that's supposed on the square or another loop? Yeah. So why do you choose the margin?",
    "start": "4703990",
    "end": "4709510"
  },
  {
    "text": "So in classification, we're gonna look at the margin because that tells you how comfortable when you're predicting,",
    "start": "4709510",
    "end": "4715630"
  },
  {
    "text": "uh, co- you know, correctly. In regression, you're gonna look at residuals and square losses.",
    "start": "4715630",
    "end": "4720820"
  },
  {
    "text": "So it depends on what kind of- what problem you're trying to solve. Um, just really quickly,",
    "start": "4720820",
    "end": "4726009"
  },
  {
    "text": "some of you might have heard of logistic regression. Logistic regression is this, uh,",
    "start": "4726009",
    "end": "4731380"
  },
  {
    "text": "yellow loss function, right? So the point of this is saying that this loss minimization framework is, you know,",
    "start": "4731380",
    "end": "4739600"
  },
  {
    "text": "really general and a lot of things that you might have heard of least squares logistic regression are a kind of a special case of this.",
    "start": "4739600",
    "end": "4745270"
  },
  {
    "text": "So if you kind of master how to do loss minimization, you kind of, uh, can do it all.",
    "start": "4745270",
    "end": "4751165"
  },
  {
    "text": "Okay. So summary, um, basically, what's on the board here? If you're doing classification,",
    "start": "4751165",
    "end": "4757870"
  },
  {
    "text": "you take the score which comes from the, uh, w dot phi of x and you drive it into the sign,",
    "start": "4757870",
    "end": "4764860"
  },
  {
    "text": "and then you get either plus 1 or minus 1. Regression, you just use a score. Now to train, you have to assess how well you're doing.",
    "start": "4764860",
    "end": "4772465"
  },
  {
    "text": "In classification, there's a notion of a margin. Res- uh, in regression, it's the residual, and then you can define loss functions.",
    "start": "4772465",
    "end": "4779800"
  },
  {
    "text": "And here is we only talking about five loss functions but there's many others, um, especially for a kind of structure prediction or ranking problems,",
    "start": "4779800",
    "end": "4786429"
  },
  {
    "text": "there's all sorts of different loss functions. But they're kind of based on these simple ideas of,",
    "start": "4786430",
    "end": "4791440"
  },
  {
    "text": "you know, you have a hinge, the upper balance is zero-one if you're doing classification and, [NOISE] um, some sort of square-like error for, you know, regression.",
    "start": "4791440",
    "end": "4800415"
  },
  {
    "text": "And then, once you have your loss function, provided it's not zero-one, you can optimize it using, um, SGD,",
    "start": "4800415",
    "end": "4806430"
  },
  {
    "text": "which turns out to be a lot faster than, you know, gradient descent. Okay. So next time, we're gonna talk about, uh,",
    "start": "4806430",
    "end": "4813055"
  },
  {
    "text": "Phi of x, which we've kind of left as, you know, someone just hands it to you. And then we're also gonna talk about what is",
    "start": "4813055",
    "end": "4819490"
  },
  {
    "text": "the really true objective of machine learning? Is it really to optimize the training loss?",
    "start": "4819490",
    "end": "4824500"
  },
  {
    "text": "Okay, until next time.",
    "start": "4824500",
    "end": "4827510"
  }
]