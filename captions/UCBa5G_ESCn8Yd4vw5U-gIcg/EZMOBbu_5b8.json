[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "start": "0",
    "end": "4360"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome back, everyone.",
    "start": "4360",
    "end": "6110"
  },
  {
    "text": "This is part four in our\nseries on Contextual Word",
    "start": "6110",
    "end": "8193"
  },
  {
    "text": "Representations.",
    "start": "8193",
    "end": "9160"
  },
  {
    "text": "We are going to be talking\nabout a robustly optimized BERT",
    "start": "9160",
    "end": "11920"
  },
  {
    "text": "approach a.k.a.",
    "start": "11920",
    "end": "13330"
  },
  {
    "text": "RoBERTa.",
    "start": "13330",
    "end": "15160"
  },
  {
    "text": "So recall that I finished the\nBERT screencast by listing out",
    "start": "15160",
    "end": "17740"
  },
  {
    "text": "some known limitations of\nthe BERT model, most of which",
    "start": "17740",
    "end": "20680"
  },
  {
    "text": "were identified by the original\nBERT authors themselves.",
    "start": "20680",
    "end": "23780"
  },
  {
    "text": "And top of the list\nwas simply that,",
    "start": "23780",
    "end": "26140"
  },
  {
    "text": "although the original\nBERT paper does",
    "start": "26140",
    "end": "27730"
  },
  {
    "text": "a good job of exploring\nablations of their system",
    "start": "27730",
    "end": "30760"
  },
  {
    "text": "and different\noptimization choices.",
    "start": "30760",
    "end": "32770"
  },
  {
    "text": "There's a very large landscape\nof ideas here, and most of it",
    "start": "32770",
    "end": "35920"
  },
  {
    "text": "was left unexplored\nin the original paper.",
    "start": "35920",
    "end": "38890"
  },
  {
    "text": "Essentially, what\nthe RoBERTa team did",
    "start": "38890",
    "end": "40660"
  },
  {
    "text": "is explore more\nwidely in this space.",
    "start": "40660",
    "end": "42880"
  },
  {
    "text": "That is the robustly\noptimized part of RoBERTa.",
    "start": "42880",
    "end": "46848"
  },
  {
    "start": "46000",
    "end": "228000"
  },
  {
    "text": "So what I've done\nfor this slide here",
    "start": "46848",
    "end": "48390"
  },
  {
    "text": "is list out what I take to\nbe the central differences",
    "start": "48390",
    "end": "50730"
  },
  {
    "text": "between BERT and RoBERTa.",
    "start": "50730",
    "end": "52052"
  },
  {
    "text": "And I'll follow this up with\nsome evidence from the RoBERTa",
    "start": "52052",
    "end": "54510"
  },
  {
    "text": "paper in a second.",
    "start": "54510",
    "end": "55620"
  },
  {
    "text": "But first let's go through the\ncentral differences beginning",
    "start": "55620",
    "end": "58739"
  },
  {
    "text": "with this question of static\nversus dynamic masking.",
    "start": "58740",
    "end": "62100"
  },
  {
    "text": "So for the original\nBERT paper what they did",
    "start": "62100",
    "end": "64439"
  },
  {
    "text": "is create four copies\nof their dataset,",
    "start": "64440",
    "end": "66354"
  },
  {
    "text": "each with different masking.",
    "start": "66354",
    "end": "68880"
  },
  {
    "text": "And then those four copies\nwere used repeatedly",
    "start": "68880",
    "end": "71579"
  },
  {
    "text": "through epochs of training.",
    "start": "71580",
    "end": "73860"
  },
  {
    "text": "The RoBERTa team\nhad the intuition",
    "start": "73860",
    "end": "75360"
  },
  {
    "text": "that it would be useful\nto inject some diversity",
    "start": "75360",
    "end": "77400"
  },
  {
    "text": "into this training process.",
    "start": "77400",
    "end": "78970"
  },
  {
    "text": "So they went to the other\nextreme, dynamic masking.",
    "start": "78970",
    "end": "81990"
  },
  {
    "text": "Every single example when\nit's presented to the model",
    "start": "81990",
    "end": "84450"
  },
  {
    "text": "is masked a potentially\ndifferent way,",
    "start": "84450",
    "end": "86799"
  },
  {
    "text": "via some random function.",
    "start": "86800",
    "end": "89980"
  },
  {
    "text": "There are also differences\nin how examples",
    "start": "89980",
    "end": "91730"
  },
  {
    "text": "are presented to the models.",
    "start": "91730",
    "end": "93050"
  },
  {
    "text": "So BERT presented two\nconcatenated document segments.",
    "start": "93050",
    "end": "96380"
  },
  {
    "text": "This was crucial to its next\nsentence prediction task.",
    "start": "96380",
    "end": "99290"
  },
  {
    "text": "Whereas for RoBERTa\nwe're just going",
    "start": "99290",
    "end": "100790"
  },
  {
    "text": "to have sentence\nsequences, that is",
    "start": "100790",
    "end": "102710"
  },
  {
    "text": "pairs, that may even\nspan document boundaries.",
    "start": "102710",
    "end": "106850"
  },
  {
    "text": "Relatedly whereas BERT had,\nas one of its central pieces,",
    "start": "106850",
    "end": "110000"
  },
  {
    "text": "this next sentence\nprediction task.",
    "start": "110000",
    "end": "112970"
  },
  {
    "text": "RoBERTa simply drops that as\npart of the objective here.",
    "start": "112970",
    "end": "116000"
  },
  {
    "text": "That simplifies the\npresentation of examples",
    "start": "116000",
    "end": "118460"
  },
  {
    "text": "and also simplifies\nthe modeling objective.",
    "start": "118460",
    "end": "120890"
  },
  {
    "text": "Now, RoBERTa is simply using\na masked language modeling",
    "start": "120890",
    "end": "124280"
  },
  {
    "text": "objective.",
    "start": "124280",
    "end": "124909"
  },
  {
    "start": "124910",
    "end": "127750"
  },
  {
    "text": "There are also changes to the\nsize of the training batches.",
    "start": "127750",
    "end": "130240"
  },
  {
    "text": "So for BERT that batch\nsize was 256 examples.",
    "start": "130240",
    "end": "133510"
  },
  {
    "text": "RoBERTa cranked out\nall the way up to 2000.",
    "start": "133510",
    "end": "136803"
  },
  {
    "text": "There are differences when\nit comes to tokenization.",
    "start": "136803",
    "end": "138970"
  },
  {
    "text": "So as we've seen BERT used this\nvery interesting word piece",
    "start": "138970",
    "end": "141910"
  },
  {
    "text": "tokenization approach which\nmixes some sub-word pieces",
    "start": "141910",
    "end": "145150"
  },
  {
    "text": "with some whole words.",
    "start": "145150",
    "end": "146980"
  },
  {
    "text": "RoBERTa simplified that down to\njust character-level byte-pair",
    "start": "146980",
    "end": "150310"
  },
  {
    "text": "encoding which I think leads\nto many more word pieces",
    "start": "150310",
    "end": "154000"
  },
  {
    "text": "intuitively.",
    "start": "154000",
    "end": "156780"
  },
  {
    "text": "There are also differences\nin how the model was trained.",
    "start": "156780",
    "end": "159840"
  },
  {
    "text": "So BERT trained on a substantial\ncorpus, the BooksCorpus",
    "start": "159840",
    "end": "163510"
  },
  {
    "text": "plus English Wikipedia,\nis a lot of data indeed.",
    "start": "163510",
    "end": "166349"
  },
  {
    "text": "RoBERTa again cranked\nthat up even further,",
    "start": "166350",
    "end": "168540"
  },
  {
    "text": "they trained on the\nBooksCorpus, the CC-News corpus,",
    "start": "168540",
    "end": "171900"
  },
  {
    "text": "the OpenWebText\ncorpus and the Stories",
    "start": "171900",
    "end": "174390"
  },
  {
    "text": "corpus, a substantial increase\nin the amount of training data.",
    "start": "174390",
    "end": "179612"
  },
  {
    "text": "There are also differences in\nthe number of training steps,",
    "start": "179612",
    "end": "182069"
  },
  {
    "text": "and there's a subtlety here.",
    "start": "182070",
    "end": "183237"
  },
  {
    "text": "So for the BERT model,\nit was originally",
    "start": "183237",
    "end": "185340"
  },
  {
    "text": "trained on 1 million steps.",
    "start": "185340",
    "end": "187500"
  },
  {
    "text": "The RoBERTa model was\ntrained on 500,000 steps.",
    "start": "187500",
    "end": "190590"
  },
  {
    "text": "Which sounds like fewer\nsteps, but overall this",
    "start": "190590",
    "end": "193500"
  },
  {
    "text": "is substantially more\ntraining, in virtue of the fact",
    "start": "193500",
    "end": "196290"
  },
  {
    "text": "that the training batch sizes\nare so much larger for RoBERTa",
    "start": "196290",
    "end": "200400"
  },
  {
    "text": "than they are for BERT.",
    "start": "200400",
    "end": "203250"
  },
  {
    "text": "And finally, the\noriginal BERT authors",
    "start": "203250",
    "end": "204940"
  },
  {
    "text": "had an intuition that\nwould be useful in getting",
    "start": "204940",
    "end": "206940"
  },
  {
    "text": "the optimization process\ngoing, to train just",
    "start": "206940",
    "end": "209580"
  },
  {
    "text": "on short sequences first.",
    "start": "209580",
    "end": "211500"
  },
  {
    "text": "The RoBERTa team\ndropped that idea,",
    "start": "211500",
    "end": "213235"
  },
  {
    "text": "and they train on\nfull length sequences",
    "start": "213235",
    "end": "214860"
  },
  {
    "text": "throughout the life\ncycle of optimization.",
    "start": "214860",
    "end": "217818"
  },
  {
    "text": "There are some\nadditional differences",
    "start": "217818",
    "end": "219360"
  },
  {
    "text": "related to the optimizer\nand the data presentation.",
    "start": "219360",
    "end": "221910"
  },
  {
    "text": "I'm going to set\nthose aside, if you",
    "start": "221910",
    "end": "223410"
  },
  {
    "text": "want the details I refer to\nsection 3.1 of the RoBERTa",
    "start": "223410",
    "end": "226590"
  },
  {
    "text": "paper.",
    "start": "226590",
    "end": "227672"
  },
  {
    "text": "So let's look at a\nlittle bit of evidence",
    "start": "227672",
    "end": "229379"
  },
  {
    "start": "228000",
    "end": "391000"
  },
  {
    "text": "for these various\nchoices, starting",
    "start": "229380",
    "end": "231300"
  },
  {
    "text": "with that question of dynamic\nversus static masking.",
    "start": "231300",
    "end": "234253"
  },
  {
    "text": "So this is the primary evidence,\nthey're using three benchmarks.",
    "start": "234253",
    "end": "236920"
  },
  {
    "text": "SQuAD, MNLI and\nSST-2, and you can",
    "start": "236920",
    "end": "240450"
  },
  {
    "text": "see that more or less across\nthe board dynamic masking",
    "start": "240450",
    "end": "243360"
  },
  {
    "text": "is better.",
    "start": "243360",
    "end": "244110"
  },
  {
    "text": "Not by a lot, but\ndynamic masking also",
    "start": "244110",
    "end": "247200"
  },
  {
    "text": "has going for this intuition\nthat BERT is kind of data",
    "start": "247200",
    "end": "250770"
  },
  {
    "text": "inefficient, we can only mask\nout a small number of tokens.",
    "start": "250770",
    "end": "253990"
  },
  {
    "text": "And it seems like it ought\nto be useful to inject",
    "start": "253990",
    "end": "256170"
  },
  {
    "text": "a lot of diversity into that, so\nthat a lot of different tokens",
    "start": "256170",
    "end": "258959"
  },
  {
    "text": "get masked, as we go through\nthe training process.",
    "start": "258959",
    "end": "261756"
  },
  {
    "text": "But the choice is of course,\nsupported numerically here,",
    "start": "261757",
    "end": "264090"
  },
  {
    "text": "I think pretty substantially.",
    "start": "264090",
    "end": "265892"
  },
  {
    "start": "265892",
    "end": "268630"
  },
  {
    "text": "This slide table here\nsummarizes the choice",
    "start": "268630",
    "end": "270670"
  },
  {
    "text": "about how to present the\nexamples to the model.",
    "start": "270670",
    "end": "272800"
  },
  {
    "text": "And this is also a little\nbit subtle, so numerically",
    "start": "272800",
    "end": "276310"
  },
  {
    "text": "the DOC-SENTENCES\napproach was best.",
    "start": "276310",
    "end": "278290"
  },
  {
    "text": "And this was an approach where\nthey just took contiguous",
    "start": "278290",
    "end": "281230"
  },
  {
    "text": "sentences from within documents\nand treated a document boundary",
    "start": "281230",
    "end": "284590"
  },
  {
    "text": "as a kind of hard boundary.",
    "start": "284590",
    "end": "286419"
  },
  {
    "text": "That's numerically\nbetter, according",
    "start": "286420",
    "end": "288040"
  },
  {
    "text": "to the benchmark results.",
    "start": "288040",
    "end": "289450"
  },
  {
    "text": "But they actually decided to\ngo with the FULL-SENTENCES",
    "start": "289450",
    "end": "292150"
  },
  {
    "text": "approach.",
    "start": "292150",
    "end": "292699"
  },
  {
    "text": "And the reason for that is,\nin not respecting document",
    "start": "292700",
    "end": "295660"
  },
  {
    "text": "boundaries, it is easier\nto create lots of batches",
    "start": "295660",
    "end": "298810"
  },
  {
    "text": "of exactly the same size.",
    "start": "298810",
    "end": "300650"
  },
  {
    "text": "Which leads to\nall sorts of gains",
    "start": "300650",
    "end": "302169"
  },
  {
    "text": "when you think about optimizing\na large model like this.",
    "start": "302170",
    "end": "305180"
  },
  {
    "text": "So basically, they decided\nthat those gains offset",
    "start": "305180",
    "end": "308139"
  },
  {
    "text": "the slightly lower performance\nof FULL-SENTENCES as",
    "start": "308140",
    "end": "311560"
  },
  {
    "text": "compared to\nDOC-SENTENCES, and that's",
    "start": "311560",
    "end": "313419"
  },
  {
    "text": "why this became their\ncentral approach.",
    "start": "313420",
    "end": "317510"
  },
  {
    "text": "Here's the summary of\nevidence for choosing 2K",
    "start": "317510",
    "end": "320410"
  },
  {
    "text": "as the batch size.",
    "start": "320410",
    "end": "321280"
  },
  {
    "text": "You can see that they chose 256,\nwhich was the BERT original.",
    "start": "321280",
    "end": "324639"
  },
  {
    "text": "2K and 8K and 2K looks\nlike the sweet spot",
    "start": "324640",
    "end": "328120"
  },
  {
    "text": "according to MLNI, SST-2.",
    "start": "328120",
    "end": "330490"
  },
  {
    "text": "And this kind of\npseudo perplexity",
    "start": "330490",
    "end": "332349"
  },
  {
    "text": "value that you get out of\nbidirectional models like BERT",
    "start": "332350",
    "end": "335060"
  },
  {
    "text": "and RoBERTa.",
    "start": "335060",
    "end": "335910"
  },
  {
    "text": "So that's a clear argument.",
    "start": "335910",
    "end": "337690"
  },
  {
    "text": "And then finally, when we come\nto just the amount of training",
    "start": "337690",
    "end": "340300"
  },
  {
    "text": "that we do.",
    "start": "340300",
    "end": "341650"
  },
  {
    "text": "The lesson here apparently\nis more is better.",
    "start": "341650",
    "end": "344290"
  },
  {
    "text": "On the top of this table\nhere we have some comparisons",
    "start": "344290",
    "end": "346540"
  },
  {
    "text": "within the RoBERTa model,\npointing to 500K as the best.",
    "start": "346540",
    "end": "350027"
  },
  {
    "text": "And I would just\nremind you that, that",
    "start": "350027",
    "end": "351610"
  },
  {
    "text": "is overall substantially\nmore training.",
    "start": "351610",
    "end": "354159"
  },
  {
    "text": "Than was done in 1\nmillion steps with BERT,",
    "start": "354160",
    "end": "356110"
  },
  {
    "text": "in virtue of the fact that\nour batch sizes for RoBERTa",
    "start": "356110",
    "end": "359469"
  },
  {
    "text": "are so much larger.",
    "start": "359470",
    "end": "362560"
  },
  {
    "text": "In closing, I just want to say\nthat RoBERTa too only explored",
    "start": "362560",
    "end": "366010"
  },
  {
    "text": "a small part of the\npotential design choices",
    "start": "366010",
    "end": "368620"
  },
  {
    "text": "that we could make in\nthis large landscape.",
    "start": "368620",
    "end": "371620"
  },
  {
    "text": "If you would like to hear\neven more about what we know",
    "start": "371620",
    "end": "373960"
  },
  {
    "text": "and what we think we know about\nmodels like BERT and RoBERTa,",
    "start": "373960",
    "end": "376870"
  },
  {
    "text": "I highly recommend this paper\ncalled The Primer in BERTology,",
    "start": "376870",
    "end": "380510"
  },
  {
    "text": "which has lots of\nadditional wisdom,",
    "start": "380510",
    "end": "382630"
  },
  {
    "text": "and insights, and ideas\nabout these models.",
    "start": "382630",
    "end": "386340"
  },
  {
    "start": "386340",
    "end": "391000"
  }
]