[
  {
    "start": "0",
    "end": "5290"
  },
  {
    "text": "Welcome back, everyone. This is lecture 20 of CS 229. And the main topic\nfor today will",
    "start": "5290",
    "end": "11710"
  },
  {
    "text": "be variational autoencoders. So variational\nautoencoders is probably one of the simplest\ndeep generative models.",
    "start": "11710",
    "end": "22400"
  },
  {
    "text": "So deep generative models\nis a very hot topic in machine learning\nright now, where",
    "start": "22400",
    "end": "27550"
  },
  {
    "text": "we try to build generative\nmodels of our data using neural networks. And variational\nautoencoders was one",
    "start": "27550",
    "end": "35320"
  },
  {
    "text": "of the early models which made\ngood progress in this field. And it's also probably\nthe model that one",
    "start": "35320",
    "end": "42879"
  },
  {
    "text": "should start studying\nfirst because it has the key components that\nare kind of necessary that are",
    "start": "42880",
    "end": "48879"
  },
  {
    "text": "used in more fancier models. So for studying\nvariational autoencoders,",
    "start": "48880",
    "end": "56230"
  },
  {
    "text": "first, we will look at\nsimple autoencoders. Autoencoders have\na long history,",
    "start": "56230",
    "end": "61930"
  },
  {
    "text": "and we look at what\nsimple autoencoders are.",
    "start": "61930",
    "end": "67870"
  },
  {
    "text": "And then we'll kind\nof switch gears back into expectation\nmaximization and look",
    "start": "67870",
    "end": "74140"
  },
  {
    "text": "at a few variants of\nexpectation maximization because that kind of\ngives good motivation",
    "start": "74140",
    "end": "80620"
  },
  {
    "text": "into variational autoencoders. We'll first study something\ncalled as MCMC expectation",
    "start": "80620",
    "end": "85660"
  },
  {
    "text": "maximizer, MCMC EM, where MCMC\nstands for Markov Chain Monte Carlo. And we will also have a quick\nlook at variational inference,",
    "start": "85660",
    "end": "96920"
  },
  {
    "text": "which is kind of like a\ncounterpart to Monte Carlo techniques.",
    "start": "96920",
    "end": "102550"
  },
  {
    "text": "And then we'll look at\nyet another variant of EM called variational EM,\nand then switch gears",
    "start": "102550",
    "end": "109000"
  },
  {
    "text": "into the variational\nautoencoder itself. So that's the plan for today.",
    "start": "109000",
    "end": "114320"
  },
  {
    "text": "And so if, in terms\nof the overall course, if we are able to\ncover this today,",
    "start": "114320",
    "end": "120490"
  },
  {
    "text": "then probably today will be\nkind of the last math-intensive",
    "start": "120490",
    "end": "125659"
  },
  {
    "text": "class. We'll have one more\nclass on Friday, where we'll be covering more\ngeneral topics like evaluation",
    "start": "125660",
    "end": "136630"
  },
  {
    "text": "metrics and general tips\nfor executing machine learning projects that\nwill not be as math heavy.",
    "start": "136630",
    "end": "142795"
  },
  {
    "text": "So probably this is going to be\nthe last math math-heavy class. And next week, we'll be just\ndoing review of all the topics",
    "start": "142795",
    "end": "149620"
  },
  {
    "text": "that we have done in the course. And also the review\nthat we do next week will be suggestive of the\nkind of topics that are",
    "start": "149620",
    "end": "158350"
  },
  {
    "text": "important for your final exam. So we'll be stressing\nmore on topics that are important\nfor the final exam",
    "start": "158350",
    "end": "164480"
  },
  {
    "text": "so that you can focus on that. Yes, question? Is the final\ncumulative, or is it the last of the chapter\nthird assignments?",
    "start": "164480",
    "end": "171580"
  },
  {
    "text": "So the final is cumulative. Since we did not have a\nmidterm, it's cumulative.",
    "start": "171580",
    "end": "178220"
  },
  {
    "text": "It covers everything.  So a quick recap of what we\ncovered in the last class--",
    "start": "178220",
    "end": "185580"
  },
  {
    "text": "so in the last class, we\nmostly dealt with the principle called the maximum\nentropy principle, where entropy of a\nprobability distribution",
    "start": "185580",
    "end": "193200"
  },
  {
    "text": "is defined as the expectation\nof the negative log of the probability value itself.",
    "start": "193200",
    "end": "201030"
  },
  {
    "text": "And the maximum\nentropy principle suggests that we should maximize\nthe entropy of a probability",
    "start": "201030",
    "end": "208497"
  },
  {
    "text": "distribution that we\nare trying to estimate, subject to some constraints,\nwhere most of the times the constraints are\nexpectation of some function",
    "start": "208497",
    "end": "217020"
  },
  {
    "text": "of the variable-- of the space\nover which the distribution is defined, and we want\nthe expectations",
    "start": "217020",
    "end": "223980"
  },
  {
    "text": "to be equal to the\nempirical expectations,",
    "start": "223980",
    "end": "229920"
  },
  {
    "text": "and these generally\ncome from data. Right? This is where data comes in into\nthe maximum entropy principle",
    "start": "229920",
    "end": "235890"
  },
  {
    "text": "because they serve as\nthe values to which we want the constraints\nto satisfy, right?",
    "start": "235890",
    "end": "240930"
  },
  {
    "text": "So subject to these\nconstraints, we get an entire family\nor an entire class",
    "start": "240930",
    "end": "247980"
  },
  {
    "text": "of probability distributions\nthat satisfy this constraint, right? So supposing these T1 and T2\nare like x and x squared--",
    "start": "247980",
    "end": "258549"
  },
  {
    "text": "so if T1 of x equals x,\nT2 of x equals x squared,",
    "start": "258550",
    "end": "264389"
  },
  {
    "text": "then basically the constraints\nthat we are trying to express is the first moment of\nthe data should be some--",
    "start": "264390",
    "end": "269849"
  },
  {
    "text": "the first moment of the data. The second moment\nof the distribution should be the second moment\nof the data, and so on.",
    "start": "269850",
    "end": "276930"
  },
  {
    "text": "And subject to\nthese constraints, we basically have\nan infinite number of probability\ndistributions in general",
    "start": "276930",
    "end": "282569"
  },
  {
    "text": "because satisfying\njust two or three constraints is pretty easy. And then the maximum\nentropy principle",
    "start": "282570",
    "end": "288569"
  },
  {
    "text": "suggests that, among the\nclass of all these candidate probability distributions\nthat satisfy",
    "start": "288570",
    "end": "294630"
  },
  {
    "text": "these moments, the one\nthat we want to choose is the one that\nmaximizes entropy. All right?",
    "start": "294630",
    "end": "300719"
  },
  {
    "text": "Yes, question? Can you clarify what\nyou mean by \"moments\"? So moments are--\nso expectation of x",
    "start": "300720",
    "end": "308010"
  },
  {
    "text": "is called the first moment. Expectation of x squared\nis the second moment. Expectation of x cubed is the\nthird moment, and so on, right?",
    "start": "308010",
    "end": "316497"
  },
  {
    "text": "So the objective that we want\nto maximize is the entropy.",
    "start": "316497",
    "end": "325160"
  },
  {
    "text": "I think there was also\na question somebody asked where, why are we\ntrying to maximize entropy and why not maximize variance?",
    "start": "325160",
    "end": "333000"
  },
  {
    "text": "Maximizing variance,\nfor example, will-- so supposing our\nprobability distribution",
    "start": "333000",
    "end": "338039"
  },
  {
    "text": "is defined on the range a to\nb, a, right, finite support. Now, if we want a\ndistribution that",
    "start": "338040",
    "end": "345750"
  },
  {
    "text": "has high uncertainty\nacross, then we need the probability,\nPDF, to look",
    "start": "345750",
    "end": "353130"
  },
  {
    "text": "like this, which\nuniformly assigns 1 over b minus a density\nto all the areas.",
    "start": "353130",
    "end": "359699"
  },
  {
    "text": "Now, if you want\nto just maximize variance instead of\nmaximizing entropy, then the distribution that\nwe have will be something",
    "start": "359700",
    "end": "367050"
  },
  {
    "text": "like this-- a and b, where\nhalf the mass is here, half the mass is here, right? So this maximizes variance,\nbut it does not necessarily",
    "start": "367050",
    "end": "376170"
  },
  {
    "text": "mean that you are maximizing\nuncertainty, right? So maximizing variance\nis not something",
    "start": "376170",
    "end": "381537"
  },
  {
    "text": "that we want to do if we want\nto just increase our uncertainty and kind of stay unbiased\nabout our estimate of what",
    "start": "381537",
    "end": "388889"
  },
  {
    "text": "the probability\ndistribution is, right? So maximum entropy\nprinciples tells us",
    "start": "388890",
    "end": "394290"
  },
  {
    "text": "that satisfy this constraint,\nand among all the distributions that satisfy it, choose the one\nthat has the highest entropy.",
    "start": "394290",
    "end": "400710"
  },
  {
    "text": "And we saw that this is equal\nto this kind of dual problem,",
    "start": "400710",
    "end": "405840"
  },
  {
    "text": "where the dual problem is\nmaximum likelihood, where if we were to start from\nanother direction, where we want",
    "start": "405840",
    "end": "413160"
  },
  {
    "text": "to perform maximum likelihood--\nand the probability",
    "start": "413160",
    "end": "418680"
  },
  {
    "text": "distribution, if\nwe assume, is part of the exponential family,\nwhere the sufficient statistics",
    "start": "418680",
    "end": "426030"
  },
  {
    "text": "of the exponential\nfamily are basically the constraints that\nwe want to satisfy, then these two problems\nare equivalent.",
    "start": "426030",
    "end": "433210"
  },
  {
    "text": "In other words, maximum\nentropy naturally gives rise to the exponential\nfamily of probability",
    "start": "433210",
    "end": "441960"
  },
  {
    "text": "distributions, right? And then we kind of saw\nthis somewhat related topic",
    "start": "441960",
    "end": "447919"
  },
  {
    "text": "called calibration, which\nis super important if you're building real-world predictive\nmodels, real-world forecasting",
    "start": "447920",
    "end": "455280"
  },
  {
    "text": "models. So calibration is\nthis property where the predicted probabilities\nmatch the observed frequencies,",
    "start": "455280",
    "end": "462360"
  },
  {
    "text": "where you say that\nsome outcome has a probability of, say, 80%--",
    "start": "462360",
    "end": "468150"
  },
  {
    "text": "let's say it's going to rain\ntomorrow with probability 80%. Then if you collect the\nset of all predictions",
    "start": "468150",
    "end": "473940"
  },
  {
    "text": "where the prediction\nwas 80%, then approximately 0.8 fraction\nof those two outcomes,",
    "start": "473940",
    "end": "480217"
  },
  {
    "text": "it actually should\nhave rained, right? Not more, not less. The fraction of\nobserved outcomes",
    "start": "480217",
    "end": "486630"
  },
  {
    "text": "should match the\npredicted probability. Then that model is then\nsaid to be well calibrated",
    "start": "486630",
    "end": "492930"
  },
  {
    "text": "against that distribution. And we also saw that\ncalibration and accuracy",
    "start": "492930",
    "end": "500400"
  },
  {
    "text": "are kind of orthogonal. One does not necessarily\nimply the other. You can have\nwell-calibrated models",
    "start": "500400",
    "end": "506100"
  },
  {
    "text": "that have very poor\naccuracy, and vice versa. And related to this\nconcept of calibration,",
    "start": "506100",
    "end": "515308"
  },
  {
    "text": "there is this concept called\na proper scoring rule. So you can think\nof loss functions as being a proper scoring\nrule if they take a forecast",
    "start": "515309",
    "end": "524610"
  },
  {
    "text": "distribution and some actual\noutcome that was observed and kind of basically give\na score to the forecaster",
    "start": "524610",
    "end": "532440"
  },
  {
    "text": "based on what the actual\noutcome was, right? And you can think of\nit as a loss function where the smaller the score,\nthe better the forecaster was,",
    "start": "532440",
    "end": "539980"
  },
  {
    "text": "right? And a proper scoring\nrule is one that satisfies this property, where,\ngiven any two probability",
    "start": "539980",
    "end": "546060"
  },
  {
    "text": "distributions P and Q,\nwhere if you assumed Q is the real-world occurrence\nor the real-world probability",
    "start": "546060",
    "end": "554280"
  },
  {
    "text": "distribution, the true\nprobability distributions. And if x's are being sampled\nfrom this real world,",
    "start": "554280",
    "end": "560880"
  },
  {
    "text": "and these real-world samples are\nused to score the forecaster's",
    "start": "560880",
    "end": "569820"
  },
  {
    "text": "probability distribution, where\nthese are the predictions,",
    "start": "569820",
    "end": "574890"
  },
  {
    "text": "then only when the prediction\nis the true probability,",
    "start": "574890",
    "end": "581670"
  },
  {
    "text": "the loss will be the lowest. For any other\nprobability distribution being predicted, if the true\nevents are sampled from Q,",
    "start": "581670",
    "end": "589240"
  },
  {
    "text": "then the expected score\nfor the other forecasts",
    "start": "589240",
    "end": "595170"
  },
  {
    "text": "will always be higher\nthan the expected score for the true distribution\nbeing forecasted, right?",
    "start": "595170",
    "end": "603570"
  },
  {
    "text": "So this is called a\nproper scoring rule. And if we build models that\noptimize for proper scoring",
    "start": "603570",
    "end": "609810"
  },
  {
    "text": "rules-- that is, if our\nloss function is penalizing our prediction P using\nsome proper scoring rule f,",
    "start": "609810",
    "end": "617760"
  },
  {
    "text": "and subject to few\nother constraints-- it's easy to see that proper\nscoring rules encourages",
    "start": "617760",
    "end": "627360"
  },
  {
    "text": "our model to predict\nprobabilities that are forecasted, because\nthey are minimized",
    "start": "627360",
    "end": "634980"
  },
  {
    "text": "when the predicted\nprobability equal to the real-world\noccurrence of data, right?",
    "start": "634980",
    "end": "640079"
  },
  {
    "text": "And we saw the connection\nto maximum entropy of this proper scoring\nrule that if we",
    "start": "640080",
    "end": "647670"
  },
  {
    "text": "strive to maximize the entropy\nor use the maximum entropy principle, then,\nas a consequence,",
    "start": "647670",
    "end": "653410"
  },
  {
    "text": "the loss function\nthat we get is going to be the negative log P of\nx, and that comes directly",
    "start": "653410",
    "end": "658870"
  },
  {
    "text": "from this. I know we are trying\nto maximize this. Instead, we can minimize\nnegative log P of x, right?",
    "start": "658870",
    "end": "665527"
  },
  {
    "text": "So the negative log\nlikelihood is just the loss function of the\nmaximum likelihood objective. And we saw that f of (P,\nx) equals negative log",
    "start": "665527",
    "end": "675480"
  },
  {
    "text": "P of x is a proper scoring rule\nbecause if we just plug minus log P of x here and take\nit to the other side",
    "start": "675480",
    "end": "682440"
  },
  {
    "text": "or bring that to the other side,\nwe get that KL divergence is always greater than or\nequal to 0, which is true,",
    "start": "682440",
    "end": "691520"
  },
  {
    "text": "that we've also seen\nthat in the homework. So essentially,\nthe big picture is",
    "start": "691520",
    "end": "697440"
  },
  {
    "text": "that maximum entropy principle\nencourages us to make",
    "start": "697440",
    "end": "703170"
  },
  {
    "text": "calibrated predictions, right? That's the big story\nfrom last class. Any questions on this before\nwe move on to today's topics?",
    "start": "703170",
    "end": "711890"
  },
  {
    "text": "OK, cool. So today, we'll\nswitch gears and talk about variational autoencoders.",
    "start": "711890",
    "end": "717980"
  },
  {
    "text": "And the first step\nin this journey will be to have a look at what\nautoencoders are in general.",
    "start": "717980",
    "end": "725970"
  },
  {
    "text": "So to study\nautoencoders, we again go back to neural\nnetworks, right? So in neural networks--",
    "start": "725970",
    "end": "732230"
  },
  {
    "start": "732230",
    "end": "740149"
  },
  {
    "text": "So the only kind\nof neural networks that you've studied\nso far are those that can be used in a\nsupervised setting, where",
    "start": "740150",
    "end": "748449"
  },
  {
    "text": "we start with some input layer,\nor we started with some input layer.",
    "start": "748450",
    "end": "753880"
  },
  {
    "text": "And then we had all these hidden\nlayers fully connected, right?",
    "start": "753880",
    "end": "763170"
  },
  {
    "text": "Fully connected layers. And we ended up with\na single scalar y hat.",
    "start": "763170",
    "end": "771420"
  },
  {
    "text": "And we compared it\nagainst the ground truth y, or the true label y. And out of these two, we\nconstructed a loss, right?",
    "start": "771420",
    "end": "780870"
  },
  {
    "text": "And this was a scalar, and\nthen we minimize the loss.",
    "start": "780870",
    "end": "787040"
  },
  {
    "text": "That gave us a scalar\nvalued function as a function of\nall the parameters.",
    "start": "787040",
    "end": "792230"
  },
  {
    "text": "And then we basically\noptimized the loss by performing gradient\ndescent on our loss function.",
    "start": "792230",
    "end": "800639"
  },
  {
    "text": "And in order to calculate\ngradient descent with respect to all the parameters, we use\nthe multivariate calculus chain",
    "start": "800640",
    "end": "807769"
  },
  {
    "text": "rule, which was essentially the\nsame as backpropagation, right? That's what we did in\nsupervised learning setting.",
    "start": "807770",
    "end": "816230"
  },
  {
    "text": "Instead, now, what we're going\nto do is we are only given x's.",
    "start": "816230",
    "end": "821660"
  },
  {
    "text": "So our training set is now just\na set of x's, x1 through xn.",
    "start": "821660",
    "end": "827540"
  },
  {
    "text": "There is no y.  And the goal with\nautoencoders is",
    "start": "827540",
    "end": "839940"
  },
  {
    "text": "to learn a way in which\nwe introduce something",
    "start": "839940",
    "end": "845910"
  },
  {
    "text": "called as a bottleneck and\nreconstruct the original data. What I mean by that is we\nstart with the original--",
    "start": "845910",
    "end": "853260"
  },
  {
    "text": "with the input layer being\nx, right, and this used to be d-dimensional, right?",
    "start": "853260",
    "end": "858690"
  },
  {
    "text": "And this will still\nbe d-dimensional. So this is the input layer. And then we have a few fully\nconnected layers, right,",
    "start": "858690",
    "end": "871590"
  },
  {
    "text": "and we bring it down to some\nk-dimensional hidden layer.",
    "start": "871590",
    "end": "877350"
  },
  {
    "text": "Let's call it z, right? And from this\nk-dimensional hidden layer,",
    "start": "877350",
    "end": "882840"
  },
  {
    "text": "we start where typically\nk is smaller than d. From here, we start increasing\nthe dimensions of the hidden",
    "start": "882840",
    "end": "893089"
  },
  {
    "text": "layers until we are back\nto d-dimensional layer.",
    "start": "893090",
    "end": "899630"
  },
  {
    "text": "And this layer,\nwe'll call it x hat. And our loss will now be to\nminimize x hat minus x, right?",
    "start": "899630",
    "end": "913340"
  },
  {
    "text": "So we want the output of our\nnetwork to be the input itself.",
    "start": "913340",
    "end": "918400"
  },
  {
    "text": "This may sound trivial\nbecause all the network has to do is to take\nthe input and give",
    "start": "918400",
    "end": "924662"
  },
  {
    "text": "the same thing as the output. But the challenge\nfor the network is that it has to basically\ntake this data transformation",
    "start": "924662",
    "end": "933520"
  },
  {
    "text": "through this bottleneck\nlayer z, which has dimension k, which\nis much smaller than d.",
    "start": "933520",
    "end": "939430"
  },
  {
    "text": "What this encourages\nthe model to do is to learn some kind of a\nlow-dimensional representation",
    "start": "939430",
    "end": "946389"
  },
  {
    "text": "of our high-dimensional\ninput data. And then starting with this\nlow-dimensional representation,",
    "start": "946390",
    "end": "953410"
  },
  {
    "text": "we map it back into the\nhigh-dimensional data itself,",
    "start": "953410",
    "end": "959180"
  },
  {
    "text": "right? And if the model is able\nto successfully minimize this loss to a\nsatisfactory level,",
    "start": "959180",
    "end": "964750"
  },
  {
    "text": "then essentially the model\nhas learned to compress data into some kind of a latent\nstate or a hidden state, right?",
    "start": "964750",
    "end": "973270"
  },
  {
    "text": "So here, we typically\ncall the parameters",
    "start": "973270",
    "end": "978730"
  },
  {
    "text": "of the first half\nof the network-- let's call these\nparameters as phi. So these are all the weights\nand biases of all the layers",
    "start": "978730",
    "end": "985420"
  },
  {
    "text": "until the hidden layer\nthat we are interested in. And let's call the\nweights and biases",
    "start": "985420",
    "end": "992620"
  },
  {
    "text": "of all the layers starting from\nthis hidden layer z all the way until x hat, which is the\nreconstruction, to be--",
    "start": "992620",
    "end": "1000510"
  },
  {
    "text": "let's call it theta, right? And so the loss is now\nessentially sum over i",
    "start": "1000510",
    "end": "1008189"
  },
  {
    "text": "equals 1 to n, where small\nn is the number of examples. We want to minimize the\nnorm between xi minus--",
    "start": "1008190",
    "end": "1019690"
  },
  {
    "text": "let's give them names. So this part of the\nnetwork, which takes as input x and outputs z--",
    "start": "1019690",
    "end": "1026638"
  },
  {
    "text": "let's call it the\nencoder, right?  So it encodes data into some\nhidden representation, z.",
    "start": "1026639",
    "end": "1035040"
  },
  {
    "text": "And we'll call the second\nhalf of the network that takes z as the input and\noutputs x hat as the output--",
    "start": "1035040",
    "end": "1041970"
  },
  {
    "text": "we'll call it the\ndecoder, right?",
    "start": "1041970",
    "end": "1047109"
  },
  {
    "text": "And the encoder is\nparameterized by phi, and the decoder is\nparameterized by theta, right?",
    "start": "1047109",
    "end": "1054130"
  },
  {
    "text": "So now the loss is x\nminus x hat minus--",
    "start": "1054130",
    "end": "1060750"
  },
  {
    "text": "so first, we want to encode xi,\nand the parameter here is phi.",
    "start": "1060750",
    "end": "1069390"
  },
  {
    "text": "And we want to take this-- this is essentially equal-- you can call this zi, right?",
    "start": "1069390",
    "end": "1076130"
  },
  {
    "text": "And then we want to\ntake the zi and feed it as input to the decoder, right?",
    "start": "1076130",
    "end": "1081290"
  },
  {
    "text": "So feed this into the decoder. ",
    "start": "1081290",
    "end": "1086730"
  },
  {
    "text": "And this decoder is\nparameterized by theta, right? And the version that comes\nout of the decoder-- that is,",
    "start": "1086730",
    "end": "1093900"
  },
  {
    "text": "the original x encoded\nthrough the encoder, and decode that encoding\nback from the decoder--",
    "start": "1093900",
    "end": "1100440"
  },
  {
    "text": "this whole thing is x hat i. ",
    "start": "1100440",
    "end": "1105990"
  },
  {
    "text": "So we want to minimize the\nnorm between xi and x hat i, where the loss is a function\nof theta and phi, right?",
    "start": "1105990",
    "end": "1118730"
  },
  {
    "text": "So this objective\nis basically the-- gives you what is called\nas the autoencoder.",
    "start": "1118730",
    "end": "1124670"
  },
  {
    "text": "We call it an\nautoencoder because we want to encode\nsomething and decode it back to the same thing, which is\nwhere the word \"auto\" comes in.",
    "start": "1124670",
    "end": "1132560"
  },
  {
    "text": "And the way we go\nabout training this is through backpropagation, right?",
    "start": "1132560",
    "end": "1140299"
  },
  {
    "text": "This is going to\nbe a scalar loss because it's a norm of a\nvector, of the norm squared of a vector. So we are starting with\nsome kind of a scalar loss.",
    "start": "1140300",
    "end": "1148610"
  },
  {
    "text": "So you can think of this as-- you come to an encoded state\nand decode it back into x hat--",
    "start": "1148610",
    "end": "1159919"
  },
  {
    "text": "and decode it back into x hat.",
    "start": "1159920",
    "end": "1165420"
  },
  {
    "text": " And we are provided a\nsecond copy of x itself.",
    "start": "1165420",
    "end": "1173310"
  },
  {
    "text": "So this is d-dimensional\nthis is d-dimensional. So start from the input,\nencode it, decode it.",
    "start": "1173310",
    "end": "1180800"
  },
  {
    "text": "You get x hat. Take the original--\nanother copy of x.",
    "start": "1180800",
    "end": "1186830"
  },
  {
    "text": "And using these two, construct\nthe loss to be x minus x hat",
    "start": "1186830",
    "end": "1192799"
  },
  {
    "text": "squared. And this is a scalar loss. This will be in R, right?",
    "start": "1192800",
    "end": "1198049"
  },
  {
    "text": "And this gives us\na loss function against which we can now perform\nbackpropagation and train",
    "start": "1198050",
    "end": "1203539"
  },
  {
    "text": "all the model parameters\nphi and theta. These are basically the weights\nand biases in the first half",
    "start": "1203540",
    "end": "1208940"
  },
  {
    "text": "and the second half\nof the network, right? And the way you go about\ndoing backpropagation",
    "start": "1208940",
    "end": "1214770"
  },
  {
    "text": "is exactly how we saw in\nthe neural network lectures. Yes, question? What is the benefit\nof doing this?",
    "start": "1214770",
    "end": "1219942"
  },
  {
    "text": "Because we can do the\nsame thing with PCA, where you can find\nthe dimension and then use the data to\nthose dimensions.",
    "start": "1219942",
    "end": "1226120"
  },
  {
    "text": "This would be a lossy encoding. Good question. So the question is, how is\nthis different from PCA, right?",
    "start": "1226120",
    "end": "1232420"
  },
  {
    "text": "In PCA, we are doing\nsomething very similar. We start with a\nd-dimensional data and project it down into a\nlow-- a k-dimensional hidden",
    "start": "1232420",
    "end": "1242080"
  },
  {
    "text": "representation. And then in PCA,\nthe objective there",
    "start": "1242080",
    "end": "1247360"
  },
  {
    "text": "was to minimize the\ndistance between every point",
    "start": "1247360",
    "end": "1253299"
  },
  {
    "text": "in this projection, right? The main difference between\nPCA and an autoencoder",
    "start": "1253300",
    "end": "1258520"
  },
  {
    "text": "is that, in PCA, the\ntransformation from x to z is strictly linear, right?",
    "start": "1258520",
    "end": "1263740"
  },
  {
    "text": "Here, you can have multiple\nhidden layers, which have multiple nonlinearities.",
    "start": "1263740",
    "end": "1269278"
  },
  {
    "text": "So you can actually\nperform PCA, interesting. So, yes, so technically,\nyou could perform a PCA here",
    "start": "1269278",
    "end": "1279940"
  },
  {
    "text": "with this thing, except in PCA,\nyou might also require this--",
    "start": "1279940",
    "end": "1286629"
  },
  {
    "text": "you need the exact mapping. Should this be the inverse? You can do something\nvery similar to PCA with this\nwithout nonlinearities.",
    "start": "1286630",
    "end": "1292840"
  },
  {
    "text": "Yeah, you can do it. OK, so what is the\nbenefit of using it? Is that the same as PCA? So this is not the same as\nPCA because, here, things",
    "start": "1292840",
    "end": "1300170"
  },
  {
    "text": "can be nonlinear. But why would\nsomeone actually want to do this with\ndata in real life? The reason why you would want to\ndo this with data in real life",
    "start": "1300170",
    "end": "1309139"
  },
  {
    "text": "is because you want\nto learn some kind of a useful hidden\nrepresentation that has some kind of a\nlatent meaning, right?",
    "start": "1309140",
    "end": "1316880"
  },
  {
    "text": "And we'll see with\nvariational autoencoders of how this z\nrepresentation is kind of--",
    "start": "1316880",
    "end": "1324150"
  },
  {
    "text": "where the hidden representation\nwith variational autoencoders will end up having\nsome kind of a meaning.",
    "start": "1324150",
    "end": "1330500"
  },
  {
    "text": "We'll see later today. All right, so this is\nvariational autoencoders,",
    "start": "1330500",
    "end": "1336320"
  },
  {
    "text": "and think of this-- in a simplistic term, think\nof this as dimensionality reduction, or a way\nin which you are just",
    "start": "1336320",
    "end": "1343880"
  },
  {
    "text": "learning some kind of a compact\nrepresentation for your data. There are variations\nof autoencoders",
    "start": "1343880",
    "end": "1350000"
  },
  {
    "text": "called denoising\nautoencoders, where, in denoising\nautoencoders, you learn",
    "start": "1350000",
    "end": "1356090"
  },
  {
    "text": "how to denoise your\ndata by training, by feeding some kind of a\nnoisy version of the input.",
    "start": "1356090",
    "end": "1363620"
  },
  {
    "text": "And in the loss-- so you feed a noisy\nversion of the input",
    "start": "1363620",
    "end": "1368929"
  },
  {
    "text": "but try to recover\nthe original x. And that gives you something\ncalled as the denoising autoencoder, where the network\nlearns to denoise the training",
    "start": "1368930",
    "end": "1379250"
  },
  {
    "text": "data and, hopefully, generalizes\nto denoising unseen data as well, right?> So that's\na-- there are many variants",
    "start": "1379250",
    "end": "1385550"
  },
  {
    "text": "to the autoencoder.  Any questions on this?",
    "start": "1385550",
    "end": "1390595"
  },
  {
    "text": "This is all we're going to\ntalk about autoencoders, and you're going to switch\nto a different topic. Yes? So this flow model, can you\nbring up the predict label?",
    "start": "1390595",
    "end": "1398060"
  },
  {
    "text": "So there are no labels here. So the task here is to\nnot predict a label,",
    "start": "1398060",
    "end": "1404030"
  },
  {
    "text": "but the task here is to\ngo through this bottleneck and reconstruct the\noriginal data itself. That's the task.",
    "start": "1404030",
    "end": "1411760"
  },
  {
    "text": "So if I consider z as\nthe last [INAUDIBLE] working off the\nfirst important part,",
    "start": "1411760",
    "end": "1418340"
  },
  {
    "text": "what will be the--\nwhat will z output? So z. So the question\nis, if we consider",
    "start": "1418340",
    "end": "1423929"
  },
  {
    "text": "z to be the output of the\nfirst half of the network, what",
    "start": "1423930",
    "end": "1429090"
  },
  {
    "text": "should z output? And the answer is\nwhatever the network realizes is the optimum z that\nhelps it recover back x hat,",
    "start": "1429090",
    "end": "1436670"
  },
  {
    "text": "right? We are not providing\nany kind of supervision of what z should be.",
    "start": "1436670",
    "end": "1442980"
  },
  {
    "text": "All we are saying is,\nwhatever z comes out of the first half\nshould be the z",
    "start": "1442980",
    "end": "1448560"
  },
  {
    "text": "that you feed into the\nsecond half that performs good reconstruction of x. ",
    "start": "1448560",
    "end": "1456010"
  },
  {
    "text": "Yes, another question? Yes, so two questions. Are phi and theta\nrelated in any aspect?",
    "start": "1456010",
    "end": "1462183"
  },
  {
    "text": "No, phi and theta are just\ntwo different parameters. They're not necessarily alike. Should they be\ninverses in some way? No, we don't presume phi\nand theta to be inverses.",
    "start": "1462183",
    "end": "1469690"
  },
  {
    "text": "OK, and in the end, is your\ngoal going to be somehow to detach the second half of it\nand use that as a compression?",
    "start": "1469690",
    "end": "1477280"
  },
  {
    "text": "Yeah, so the eventual goal\nis to use only the first half",
    "start": "1477280",
    "end": "1482680"
  },
  {
    "text": "on unseen data to get their\nhidden representations or their compact\nrepresentations. So you chop your\nnetwork at this point",
    "start": "1482680",
    "end": "1489977"
  },
  {
    "text": "and use it as an\nencoder, where you're getting some kind of a\ncompressed representation. Yeah. So then you replan this with\ndifferent number of layers",
    "start": "1489977",
    "end": "1500000"
  },
  {
    "text": "and then you see\nwhat is optimal? Yeah, so the question is,\nwhat's the dimension of z?",
    "start": "1500000",
    "end": "1506440"
  },
  {
    "text": "How many layers do we have? Those are all hyperparameters\nthat you want to tune, where you can train\nwith your training data",
    "start": "1506440",
    "end": "1513919"
  },
  {
    "text": "and see how well the\nreconstruction is happening on a test data. And you can do your bias\nvariance analysis there.",
    "start": "1513920",
    "end": "1520909"
  },
  {
    "text": "Yes, question? Why do we not see\nthe decoder as vector and factor analysis\nwith no conditions? Like, then it should\nbe even less than d?",
    "start": "1520910",
    "end": "1528260"
  },
  {
    "text": "Right, so a relation\nto factor analysis-- let's take that offline. That's kind of\ntangential to our topic",
    "start": "1528260",
    "end": "1535340"
  },
  {
    "text": "right now, but happy to\nanswer that after the lecture. All right, so this\nis the autoencoder,",
    "start": "1535340",
    "end": "1543470"
  },
  {
    "text": "and we'll come back to\nautoencoder in a few minutes. In the meantime, a few more\nrelated topics for our buildup.",
    "start": "1543470",
    "end": "1550400"
  },
  {
    "text": " So next, so this\nwas autoencoders.",
    "start": "1550400",
    "end": "1556370"
  },
  {
    "start": "1556370",
    "end": "1566160"
  },
  {
    "text": "2, MCMC EM, right?",
    "start": "1566160",
    "end": "1572820"
  },
  {
    "text": "So a quick recall of EM. So in EM, we performed\nthis iterative procedure",
    "start": "1572820",
    "end": "1581730"
  },
  {
    "text": "of going through two steps,\nthe E step and the M step, over and over again until we\nconverge, where the E step was",
    "start": "1581730",
    "end": "1592770"
  },
  {
    "text": "for all i, where i is index in\nyour data elements, set Qi of z",
    "start": "1592770",
    "end": "1602700"
  },
  {
    "text": "to be equal to P of z given xi\nunder the current parameter,",
    "start": "1602700",
    "end": "1611710"
  },
  {
    "text": "theta. ",
    "start": "1611710",
    "end": "1619450"
  },
  {
    "text": "And then to make\nthis more clear, let's also give time indexes.",
    "start": "1619450",
    "end": "1624760"
  },
  {
    "text": "So at the t-th iteration,\nwe use theta t. ",
    "start": "1624760",
    "end": "1631840"
  },
  {
    "text": "And in the M step, we perform\ntheta t plus 1 to be equal to--",
    "start": "1631840",
    "end": "1642659"
  },
  {
    "text": "I'm just going to write\nit for one example, but it's basically just\nthe sum over all examples--",
    "start": "1642660",
    "end": "1650490"
  },
  {
    "text": "arg max theta of the ELBO.",
    "start": "1650490",
    "end": "1656290"
  },
  {
    "text": "I'm going to write out\nthe ELBO in full form-- sum over z Qt of z\ntimes log P of x,",
    "start": "1656290",
    "end": "1675660"
  },
  {
    "text": "z, parameterized by theta,\nover Qi t of z, right?",
    "start": "1675660",
    "end": "1688640"
  },
  {
    "text": " This, in fact-- I'm just writing\nit in terms of one example.",
    "start": "1688640",
    "end": "1695549"
  },
  {
    "text": "Let me just write\nin all the examples. So i equals 1 to n. You have xi, zi, and zi here.",
    "start": "1695550",
    "end": "1706110"
  },
  {
    "text": " And in this, we can\nmake a few comments.",
    "start": "1706110",
    "end": "1713629"
  },
  {
    "text": "First of all, we assume that\nwe could perform this posterior calculation, P of z given x.",
    "start": "1713630",
    "end": "1719049"
  },
  {
    "text": "We could calculate\nthis somehow, right? This is kind of an\nimplicit assumption in EM,",
    "start": "1719050",
    "end": "1724059"
  },
  {
    "text": "that we can calculate this\nposterior distribution P of z given x, right? That may or may not\nhold true in practice.",
    "start": "1724060",
    "end": "1730900"
  },
  {
    "text": "So that was a big assumption\nthat we quietly swept",
    "start": "1730900",
    "end": "1737350"
  },
  {
    "text": "under the rug and assumed we\ncould calculate the posterior distribution somehow. Now, the challenge\ncomes when you",
    "start": "1737350",
    "end": "1744130"
  },
  {
    "text": "are considering complex models. For example, suppose our\nmodel is something like this--",
    "start": "1744130",
    "end": "1749320"
  },
  {
    "text": "so z comes from some\nnormal distribution with mean 0 and I k times k, a\ncontinuous, latent variable z.",
    "start": "1749320",
    "end": "1760240"
  },
  {
    "text": "And let's say x given z is some\nkind of a-- that's a neural",
    "start": "1760240",
    "end": "1768280"
  },
  {
    "text": "network, right-- neural network with\nparameter theta that",
    "start": "1768280",
    "end": "1774309"
  },
  {
    "text": "takes in as input z, a right?",
    "start": "1774310",
    "end": "1779410"
  },
  {
    "text": "So z is a sample from some\nGaussian distribution, and we feed that Gaussian\ndistribution-- so",
    "start": "1779410",
    "end": "1786940"
  },
  {
    "text": "imagine the second\nhalf of this network. We feed that z into\na neural network,",
    "start": "1786940",
    "end": "1793090"
  },
  {
    "text": "and out will come our data, the\ndata that we observe, right? In this kind of a setting,\nwhere our model is so complex",
    "start": "1793090",
    "end": "1803230"
  },
  {
    "text": "that calculating P of z given\nx is pretty much impossible.",
    "start": "1803230",
    "end": "1809240"
  },
  {
    "text": "There's no way we can take\nsome arbitrary neural network and calculate the\nposterior of z given",
    "start": "1809240",
    "end": "1814945"
  },
  {
    "text": "x in a closed-form estimate. It may not be a neural network. It could be any kind of--",
    "start": "1814945",
    "end": "1820840"
  },
  {
    "text": "some complex model, some complex\nzi parameterized by theta.",
    "start": "1820840",
    "end": "1831595"
  },
  {
    "text": "This could be hierarchical. It could be any kind\nof a complex model. Now the question is,\nhow do we perform",
    "start": "1831595",
    "end": "1838180"
  },
  {
    "text": "EM in this setting to estimate\nour parameter's theta? Because we are no\nlonger able to come up",
    "start": "1838180",
    "end": "1844029"
  },
  {
    "text": "with a closed-form estimate\nfor P of z given x, right? And this is where we can\nmake a few observations.",
    "start": "1844030",
    "end": "1854510"
  },
  {
    "text": "So first of all, it's EM. In the M step, we are\nholding Q fixed, right?",
    "start": "1854510",
    "end": "1862830"
  },
  {
    "text": "We saw this previously as well. The only place where\nthe variable that we",
    "start": "1862830",
    "end": "1868590"
  },
  {
    "text": "are optimizing, this theta--\nthe only place that shows up is over here, right?",
    "start": "1868590",
    "end": "1876920"
  },
  {
    "text": "Why are you performing\nthis with arg max? This is the only theta\nthat we are adjusting to perform the arg max, right? Everything else here\nare constants, right?",
    "start": "1876920",
    "end": "1885040"
  },
  {
    "text": "And so we can write this as-- I think we mentioned\nthis earlier as well,",
    "start": "1885040",
    "end": "1891580"
  },
  {
    "text": "that we can all-- in EM, the M\nstep can always be written as--",
    "start": "1891580",
    "end": "1897496"
  },
  {
    "text": "equals 1 to n sum\nover z Qi t of zi",
    "start": "1897496",
    "end": "1907690"
  },
  {
    "text": "log P of xi, zi,\nparameterized by theta.",
    "start": "1907690",
    "end": "1915639"
  },
  {
    "start": "1915640",
    "end": "1923160"
  },
  {
    "text": "Right? These two are always\nthe same, will always-- are equivalent because log of\na by b can be written as log",
    "start": "1923160",
    "end": "1931799"
  },
  {
    "text": "a minus log b. And log b is essentially a\nconstant with respect to theta.",
    "start": "1931800",
    "end": "1937600"
  },
  {
    "text": "So we can just get rid of the\ndenominator inside the log, right? And now, if you look\nat this in this form,",
    "start": "1937600",
    "end": "1948480"
  },
  {
    "text": "we get another insight. The insight is that,\neven though in the E step",
    "start": "1948480",
    "end": "1956700"
  },
  {
    "text": "we say we want to calculate\nQ, the probability distribution of\nthe posterior of z,",
    "start": "1956700",
    "end": "1965270"
  },
  {
    "text": "we don't really want to\ncalculate the density values itself, right?",
    "start": "1965270",
    "end": "1971360"
  },
  {
    "text": "The only way Q gets\nused in the M step is to construct this\nexpectation, all right?",
    "start": "1971360",
    "end": "1979700"
  },
  {
    "text": "So this is i equals 1 to\nn, expectation of zi coming",
    "start": "1979700",
    "end": "1987080"
  },
  {
    "text": "from Qi of t of log P\nxi, zi, theta, arg max.",
    "start": "1987080",
    "end": "2004490"
  },
  {
    "text": " So even though Qi appears\nin this expression,",
    "start": "2004490",
    "end": "2013960"
  },
  {
    "text": "it is only used to take an\nexpectation of this function, right? We don't really need to\ncalculate the density",
    "start": "2013960",
    "end": "2021340"
  },
  {
    "text": "by itself. We are only interested in\nthe density values of Q only to perform this\nexpectation, right?",
    "start": "2021340",
    "end": "2029230"
  },
  {
    "text": "And using this insight,\nthat the purpose of Q is only to take the\nexpectation, we can instead--",
    "start": "2029230",
    "end": "2037240"
  },
  {
    "text": "what we can do is to approximate\nthis arg max theta sum over i",
    "start": "2037240",
    "end": "2046610"
  },
  {
    "text": "equals 1 to n. And over here, what\nwe are going to do is to replace the expectation\nwith the Monte Carlo",
    "start": "2046610",
    "end": "2055969"
  },
  {
    "text": "estimate of the expectation. And what that means is,\ninstead of integrating over",
    "start": "2055969",
    "end": "2062239"
  },
  {
    "text": "this function using\nthe density of Q, instead, we will take\nmany, many samples of z",
    "start": "2062239",
    "end": "2068239"
  },
  {
    "text": "from Q, right, and take\nthe average of this",
    "start": "2068239",
    "end": "2074320"
  },
  {
    "text": "across all those\nsamples of z, right? So that would look\nsomething like this. Let's assume we take\ncapital T number",
    "start": "2074320",
    "end": "2081429"
  },
  {
    "text": "of samples for each example. And this would be\n1 over capital T.",
    "start": "2081429",
    "end": "2089790"
  },
  {
    "text": "T is probably a bad choice,\nso let's call it capital M.",
    "start": "2089790",
    "end": "2094820"
  },
  {
    "text": "So we're going to use\ncapital M number of samples for each Monte Carlo\nexpectation estimate.",
    "start": "2094820",
    "end": "2102590"
  },
  {
    "text": "Small m equals 1 to\ncapital M log p of xi.",
    "start": "2102590",
    "end": "2114150"
  },
  {
    "text": "And in place of zi, I'm going\nto write zm, where zm is zm.",
    "start": "2114150",
    "end": "2125450"
  },
  {
    "text": " i is sampled from\nQi of t, right?",
    "start": "2125450",
    "end": "2138010"
  },
  {
    "text": "So we rewrite this expectation\nas a Monte Carlo estimate, where the z's are\nsampled from Q's.",
    "start": "2138010",
    "end": "2145210"
  },
  {
    "text": "The Q's, however, is\nstill this posterior. But there are many\ntechniques to sample",
    "start": "2145210",
    "end": "2150640"
  },
  {
    "text": "from a posterior of a complex\nprobability distribution, even though we don't\nknow how to evaluate it.",
    "start": "2150640",
    "end": "2156430"
  },
  {
    "text": "There are techniques\nsuch as Gibbs sampling",
    "start": "2156430",
    "end": "2163700"
  },
  {
    "text": "or Metropolis-Hastings. So MCMC is a vast,\nvast, vast field,",
    "start": "2163700",
    "end": "2171349"
  },
  {
    "text": "where there are many techniques\nfor sampling from posteriors, even though we don't know\nhow to exactly calculate",
    "start": "2171350",
    "end": "2177350"
  },
  {
    "text": "the density of a given\npoint in the posterior. But we can still\nget samples, right? And basically, the\nlaw of large numbers",
    "start": "2177350",
    "end": "2184100"
  },
  {
    "text": "tells us that as M\ngoes to infinity, this Monte Carlo\nestimate will tend",
    "start": "2184100",
    "end": "2190069"
  },
  {
    "text": "to-- will converge towards\nthe true expectation, right? And so, with this technique--",
    "start": "2190070",
    "end": "2196760"
  },
  {
    "text": " this is a variant of EM\nwhere, even though we",
    "start": "2196760",
    "end": "2203559"
  },
  {
    "text": "don't know how to calculate\nthe posterior distribution, we can approximate the posterior\nusing Monte Carlo techniques",
    "start": "2203560",
    "end": "2210790"
  },
  {
    "text": "or sampling techniques. So this is one way to\nwork around complex or",
    "start": "2210790",
    "end": "2218350"
  },
  {
    "text": "hard-to-compute posteriors. Any questions on this? Yes, question? Can you explain the\nlast step again?",
    "start": "2218350",
    "end": "2225150"
  },
  {
    "text": "The last step? Yeah, so in the last\nstep, what we are doing is we are replacing\nthis expectation",
    "start": "2225150",
    "end": "2233740"
  },
  {
    "text": "with the Monte Carlo\nestimate of the expectation. ",
    "start": "2233740",
    "end": "2239329"
  },
  {
    "text": "So we replace this expectation\nwith the Monte Carlo estimate of the expectation. Where did the Q go?",
    "start": "2239330",
    "end": "2245080"
  },
  {
    "text": "Where did the Q go? Q is a distribution from\nwhich the zi's are sampled.",
    "start": "2245080",
    "end": "2251510"
  },
  {
    "text": "Oh. Right? So over here, zi\nwas sampled from Q, and this was an analytical\nexpression for the expectation.",
    "start": "2251510",
    "end": "2259099"
  },
  {
    "text": "Instead, we replace it with\nthe Monte Carlo estimate of this expectation, where\nzi's are sampled from Q.",
    "start": "2259100",
    "end": "2264575"
  },
  {
    "text": "And we sample capital M of\nnumber of such samples from Qi",
    "start": "2264575",
    "end": "2270290"
  },
  {
    "text": "and construct this\naverage of this function, using those samples\nas the Monte Carlo",
    "start": "2270290",
    "end": "2275903"
  },
  {
    "text": "estimate of the expectation. So expressing the expectation\nin both those ways",
    "start": "2275903",
    "end": "2281300"
  },
  {
    "text": "is equivalent as\nif M is infinity? Yes, as you take M to\ninfinity, then these two will evaluate to the\nsame value, right,",
    "start": "2281300",
    "end": "2287089"
  },
  {
    "text": "and that's the law of\nlarge numbers, all right? So the original\nEM, we constructed",
    "start": "2287090",
    "end": "2295640"
  },
  {
    "text": "a convergence proof, where,\nwith every E and M step, the likelihood was\nonly increasing, right?",
    "start": "2295640",
    "end": "2302599"
  },
  {
    "text": "We saw that proof\nwith likelihood-- construct a lower bound, and\nthen construct a lower bound",
    "start": "2302600",
    "end": "2308300"
  },
  {
    "text": "and another lower bound. And at every step, we were\nguaranteed that the theta t's",
    "start": "2308300",
    "end": "2314780"
  },
  {
    "text": "were increasing the likelihood. But over here, that guarantee\ndoes not hold anymore",
    "start": "2314780",
    "end": "2320770"
  },
  {
    "text": "because this is an\napproximation and not-- an approximation\nof the lower bound",
    "start": "2320770",
    "end": "2326430"
  },
  {
    "text": "and not the exact lower bound. Yes, question? So with Gibbs sampling, do\nwe sample the conditional?",
    "start": "2326430",
    "end": "2332009"
  },
  {
    "text": "Can we sample the joint-- So with Gibbs sampling, you\ncan sample the posterior,",
    "start": "2332009",
    "end": "2337650"
  },
  {
    "text": "samples from the posterior. And then the [INAUDIBLE]? So we get samples of z's, right?",
    "start": "2337650",
    "end": "2345000"
  },
  {
    "text": "And using the sample\nvalues of z's, we can take the average\nof the joint.",
    "start": "2345000",
    "end": "2351269"
  },
  {
    "text": "Good question, right? So this is one\napproach to addressing",
    "start": "2351270",
    "end": "2356820"
  },
  {
    "text": "intractable posteriors\nthrough this technique called sampling, right? And there is this other\nkind of counterpart approach",
    "start": "2356820",
    "end": "2366570"
  },
  {
    "text": "for approximating\ncomplex posteriors, which is called variational\ninference, all right?",
    "start": "2366570",
    "end": "2380340"
  },
  {
    "text": "So when you want to calculate\nposterior distributions, you basically have\nthree choices.",
    "start": "2380340",
    "end": "2385599"
  },
  {
    "text": "Choice number 1 is\nthe exact posterior. You use math, algebra,\nuse Bayes' rule,",
    "start": "2385600",
    "end": "2391500"
  },
  {
    "text": "and calculate the\nexact posterior. If that is not possible, you\nare left with two other choices.",
    "start": "2391500",
    "end": "2397960"
  },
  {
    "text": "Choice number 2 is to\napproximate it using Gibbs sampling or through some kind\nof sampling approach, a Monte",
    "start": "2397960",
    "end": "2404460"
  },
  {
    "text": "Carlo approach, where\nyou take samples and take the expectation of\nthe thing you want",
    "start": "2404460",
    "end": "2410299"
  },
  {
    "text": "to take the expectation of\nwith respect to the posterior.",
    "start": "2410300",
    "end": "2415680"
  },
  {
    "text": "Option number 3 is\nvariational inference. So variational inference\nis another technique",
    "start": "2415680",
    "end": "2421700"
  },
  {
    "text": "to go to work around these\nintractable posteriors. And in order to understand\nvariational inference,",
    "start": "2421700",
    "end": "2430010"
  },
  {
    "text": "we go back to the\nexpression we derived",
    "start": "2430010",
    "end": "2436070"
  },
  {
    "text": "in EM, which was basically\nlog P of x was greater than",
    "start": "2436070",
    "end": "2442820"
  },
  {
    "text": "or equal to ELBO of\nx with some Q, right?",
    "start": "2442820",
    "end": "2452270"
  },
  {
    "text": "We derived this using\nJensen's inequality, and we left it at\nthis form, saying",
    "start": "2452270",
    "end": "2457610"
  },
  {
    "text": "the ELBO is a lower bound\nof the log probability",
    "start": "2457610",
    "end": "2463702"
  },
  {
    "text": "of the evidence, right? It's the lower bound\nof the evidence. But how much is it lower by?",
    "start": "2463702",
    "end": "2468980"
  },
  {
    "text": " So the question is, log\nP of x is equal to ELBO",
    "start": "2468980",
    "end": "2477990"
  },
  {
    "text": "of x plus what, right? ",
    "start": "2477990",
    "end": "2483950"
  },
  {
    "text": "And, in fact, the answer\nis pretty straightforward. All what you want to do is\ntake this to the other side,",
    "start": "2483950",
    "end": "2489830"
  },
  {
    "text": "and you'll get-- this is equal to log\nP of x minus ELBO.",
    "start": "2489830",
    "end": "2497030"
  },
  {
    "text": " And the ELBO is\nbasically this form.",
    "start": "2497030",
    "end": "2504680"
  },
  {
    "text": "And you plug in\nthis form over here. I'm not going to do the algebra. It's pretty straightforward,\nvery simple algebra.",
    "start": "2504680",
    "end": "2511370"
  },
  {
    "text": "What you get over here is\nbasically the KL divergence between Q and P of z given x.",
    "start": "2511370",
    "end": "2518630"
  },
  {
    "text": " And this should\nnot be surprising because when Q is equal\nto P of z given x,",
    "start": "2518630",
    "end": "2527280"
  },
  {
    "text": "we saw that the ELBO is\ntight and will be exactly equal to log P of x, right?",
    "start": "2527280",
    "end": "2535061"
  },
  {
    "text": "So it could have been the KL\ndivergence from P to Q or Q to P. One of those two\nare the natural choices.",
    "start": "2535061",
    "end": "2541782"
  },
  {
    "text": "But if you work\nout the algebra, it turns out that it is\nthe KL divergence from Q to P, P of z given x, right?",
    "start": "2541782",
    "end": "2549030"
  },
  {
    "text": "So our goal is to estimate\nlog P of z given x, right?",
    "start": "2549030",
    "end": "2559380"
  },
  {
    "text": "And here, you're\nmaking this observation that the KL divergence\nbetween Q and P z given x",
    "start": "2559380",
    "end": "2568540"
  },
  {
    "text": "is equal to this\nvalue over here. ",
    "start": "2568540",
    "end": "2574090"
  },
  {
    "text": "So this is basically kind of\nthe motivation for how we derive",
    "start": "2574090",
    "end": "2580110"
  },
  {
    "text": "the variational inference part. So for variational\ninference, what we see is log P of x does not\nhave any Q term in it.",
    "start": "2580110",
    "end": "2591050"
  },
  {
    "text": "So this is effectively\na constant. So log P of x is\nbasically a constant.",
    "start": "2591050",
    "end": "2599700"
  },
  {
    "text": "It has no Q term in it.",
    "start": "2599700",
    "end": "2605270"
  },
  {
    "start": "2605270",
    "end": "2614480"
  },
  {
    "text": "And then we have the\nELBO of x Q plus DKL of Q",
    "start": "2614480",
    "end": "2625280"
  },
  {
    "text": "to P of z given x, right? So this is a constant with\nrespect to Q, with respect",
    "start": "2625280",
    "end": "2634750"
  },
  {
    "text": "to Q. And this has a Q\nterm, and this has a Q term.",
    "start": "2634750",
    "end": "2640610"
  },
  {
    "text": "And we want our Q to be exactly\nequal to P of z given x, right?",
    "start": "2640610",
    "end": "2649230"
  },
  {
    "text": "And that's our eventual\ngoal, or the ideal goal,",
    "start": "2649230",
    "end": "2654650"
  },
  {
    "text": "that z is equal\nto P of z given x. But instead, with variational\ninference, what we do is we--",
    "start": "2654650",
    "end": "2661305"
  },
  {
    "text": " in order to make it as close\nto p of z given x as possible,",
    "start": "2661305",
    "end": "2668920"
  },
  {
    "text": "we maximize this\nwith respect to Q.",
    "start": "2668920",
    "end": "2676255"
  },
  {
    "text": "Now, if we maximize\nthis with respect to Q, make this observation that this\nis greater than or equal to 0.",
    "start": "2676255",
    "end": "2683800"
  },
  {
    "text": "So for no value of Q will\nthis become negative, right-- no matter\nwhat Q we choose",
    "start": "2683800",
    "end": "2689577"
  },
  {
    "text": "will that become negative. If you choose the\nbest possible Q, then this term just becomes 0.",
    "start": "2689577",
    "end": "2697280"
  },
  {
    "text": "So instead of calculating\nP of z given x somehow,",
    "start": "2697280",
    "end": "2703580"
  },
  {
    "text": "we take this variational\napproach where we say, Q of z",
    "start": "2703580",
    "end": "2708590"
  },
  {
    "text": "given-- or Q of-- or, rather,\nP of z given x-- ",
    "start": "2708590",
    "end": "2721310"
  },
  {
    "text": "we approximate it\nby the arg max of q",
    "start": "2721310",
    "end": "2728800"
  },
  {
    "text": "and some family Q of the\nELBO of x and q, right?",
    "start": "2728800",
    "end": "2743180"
  },
  {
    "text": "If we maximize the ELBO as much\nas possible with respect to Q,",
    "start": "2743180",
    "end": "2749290"
  },
  {
    "text": "we are bounded by log\nP of x anyways, right? So this provides a ceiling.",
    "start": "2749290",
    "end": "2754590"
  },
  {
    "text": "And if we maximize the\nELBO as much as possible, because this is non-negative,\nthe highest possible we can",
    "start": "2754590",
    "end": "2761750"
  },
  {
    "text": "take this-- when we maximize this\ncompletely with respect to Q, we would have naturally\nobtained the Q",
    "start": "2761750",
    "end": "2769730"
  },
  {
    "text": "that minimized the KL divergence\nbetween Q and P of z given x. Does that make sense?",
    "start": "2769730",
    "end": "2776569"
  },
  {
    "text": "So performing arg max\nof this ELBO over Q",
    "start": "2776570",
    "end": "2782720"
  },
  {
    "text": "will give us a Q that is\nvery close to P of z given x.",
    "start": "2782720",
    "end": "2787970"
  },
  {
    "text": "And this kind of\nan approach where we maximize this lower\nbound with respect to Q",
    "start": "2787970",
    "end": "2795950"
  },
  {
    "text": "and obtain a distribution,\nwhich we approximated to be P of z given x, is called\nvariational inference, right?",
    "start": "2795950",
    "end": "2801860"
  },
  {
    "start": "2801860",
    "end": "2809300"
  },
  {
    "text": "And this is a kind of\ncomplementary approach",
    "start": "2809300",
    "end": "2815650"
  },
  {
    "text": "to the sampling-based\napproaches, because in sampling-based\napproaches, you could take an infinite\nnumber of samples.",
    "start": "2815650",
    "end": "2824589"
  },
  {
    "text": "And eventually,\nyou would recover the exact evidence lower bound.",
    "start": "2824590",
    "end": "2831760"
  },
  {
    "text": "You would recover the\nexact ELBO as you took M to infinity, all right? Whereas over here,\nmost of the times,",
    "start": "2831760",
    "end": "2839690"
  },
  {
    "text": "we will not recover\nthe exact posterior, but we will end up having some\nkind of an approximation of P",
    "start": "2839690",
    "end": "2847430"
  },
  {
    "text": "of z given x depending on how\nflexible this family of Q is. ",
    "start": "2847430",
    "end": "2854990"
  },
  {
    "text": "And over here, the technique\nwe used for sampling, whereas with\nvariational inference,",
    "start": "2854990",
    "end": "2861300"
  },
  {
    "text": "the technique uses\noptimization, right? In sampling, we do--",
    "start": "2861300",
    "end": "2867160"
  },
  {
    "text": "sampling, in Monte Carlo\ntechniques, we do sampling. With variational inference,\nwe do optimization. Here in the limit, we\nrecovered the exact solution,",
    "start": "2867160",
    "end": "2875590"
  },
  {
    "text": "whereas over here, we always\nget an approximate solution",
    "start": "2875590",
    "end": "2880870"
  },
  {
    "text": "depending on how\nflexible the family Q is. Over here, if you're familiar\nwith Monte Carlo techniques,",
    "start": "2880870",
    "end": "2888490"
  },
  {
    "text": "you never know how\ngood your estimate is. You need to keep taking\nmore and more samples. All you know is, eventually, you\nwill reach the exact solution.",
    "start": "2888490",
    "end": "2897880"
  },
  {
    "text": "But at any given time-- say\nyou've taken 100 samples or have taken 10\nmillion samples-- that you don't know how\ngood or how close you",
    "start": "2897880",
    "end": "2905140"
  },
  {
    "text": "are to the true expectation. You're just told that\neventually you will be. But this technique\nwill converge, right?",
    "start": "2905140",
    "end": "2912670"
  },
  {
    "text": "You know when to stop. ",
    "start": "2912670",
    "end": "2917758"
  },
  {
    "text": "When you complete this\noptimization problem, you know when to stop. You know that the\nsolution is approximate,",
    "start": "2917758",
    "end": "2924938"
  },
  {
    "text": "but you know when to stop,\nwhereas over here, you don't know when to stop. So those are the trade-offs\nbetween Monte Carlo techniques",
    "start": "2924938",
    "end": "2934692"
  },
  {
    "text": "and variational\ninference techniques, or the sampling techniques\nversus variational inference techniques. Yes, question?",
    "start": "2934693",
    "end": "2940120"
  },
  {
    "text": "Is this kind of doing\na different thing? So the Gibbs sampling\none is for the M step,",
    "start": "2940120",
    "end": "2945730"
  },
  {
    "text": "and this guy is for\nthe E step because you are getting Q's, right?",
    "start": "2945730",
    "end": "2951250"
  },
  {
    "text": "Yeah, so the Q that\nyou end up here, you're going to plug\nit in over here.",
    "start": "2951250",
    "end": "2956790"
  },
  {
    "text": "Is it the first part and not-- Yeah, you can think of this\nas the first part, where you recover Q using\nvariational inference",
    "start": "2956790",
    "end": "2964090"
  },
  {
    "text": "and use that Q to construct\nyour M step objective, right? And, whereas over here,\nyou constructed the M step",
    "start": "2964090",
    "end": "2974890"
  },
  {
    "text": "objective, or the proxy\nto the M step objective, through sampling. Whereas here, the proxy was\nthrough this approximation of--",
    "start": "2974890",
    "end": "2982930"
  },
  {
    "text": "from the variational\ninference step, all right? And this technique of\nconstructing the M step",
    "start": "2982930",
    "end": "2991279"
  },
  {
    "text": "objective using a proxy Q\ninstead of the exact posterior",
    "start": "2991280",
    "end": "2996430"
  },
  {
    "text": "Q will give rise to something\nthat's called variational EM. ",
    "start": "2996430",
    "end": "3006030"
  },
  {
    "text": "In both MCMC EM\nand variational EM, the goal is to work\naround the construction",
    "start": "3006030",
    "end": "3012840"
  },
  {
    "text": "of the exact posterior, right? In MCMC EM, we worked around\nit using sampling, Monte Carlo",
    "start": "3012840",
    "end": "3019319"
  },
  {
    "text": "sampling, or Gibbs sampling. In variational EM, we worked\naround the exact posterior",
    "start": "3019320",
    "end": "3024510"
  },
  {
    "text": "using variational inference. Yes, question? You said the Monte\nCarlo [INAUDIBLE]..",
    "start": "3024510",
    "end": "3031354"
  },
  {
    "text": "Can you not, one by one,\nkeep adding some samples and then keep also [INAUDIBLE]?",
    "start": "3031354",
    "end": "3037186"
  },
  {
    "text": " So I guess the\nquestion is suggesting",
    "start": "3037186",
    "end": "3043930"
  },
  {
    "text": "a technique of how we can check\nwhether MCMC has converged.",
    "start": "3043930",
    "end": "3049359"
  },
  {
    "text": "I'll not go deeper into that. We can discuss that offline. But in general,\nit's a hard problem",
    "start": "3049360",
    "end": "3054670"
  },
  {
    "text": "when you're running Monte\nCarlo techniques to know whether you--",
    "start": "3054670",
    "end": "3060280"
  },
  {
    "text": "so the technical term\nthere is something called as a burn-in\nphase, where, first, you",
    "start": "3060280",
    "end": "3069480"
  },
  {
    "text": "start with some kind\nof an initialization. And you don't know\nhow good it is, so you want to first\ndiscard some samples.",
    "start": "3069480",
    "end": "3075070"
  },
  {
    "text": "And then, hopefully,\nafter the burn-in phase, which is hard to calculate\nwhether the burn-in phase is over or not,\nhopefully, from then,",
    "start": "3075070",
    "end": "3080890"
  },
  {
    "text": "you are getting samples\nfrom the true posterior. And then it becomes a\nquestion of how many you want. But in general, calculating\nwhether the burn-in phase",
    "start": "3080890",
    "end": "3087700"
  },
  {
    "text": "is over or not is kind\nof a hard problem. All right, so the two approach--\nso in the standard EM,",
    "start": "3087700",
    "end": "3098360"
  },
  {
    "text": "you calculate the\nexact posterior using math and\nanalytical expressions.",
    "start": "3098360",
    "end": "3104270"
  },
  {
    "text": "If that is hard, you're\nleft with two options. One option is MCMC EM, where you\napproximate the M step with--",
    "start": "3104270",
    "end": "3112355"
  },
  {
    "text": " the expectation in the M\nstep with the Monte Carlo",
    "start": "3112355",
    "end": "3119869"
  },
  {
    "text": "expectation. And in variational EM,\nwhich is the other approach, you construct an\napproximation of Q",
    "start": "3119870",
    "end": "3129440"
  },
  {
    "text": "from some family,\ncapital Q. And you use that, the recovered\nQ from optimization,",
    "start": "3129440",
    "end": "3138470"
  },
  {
    "text": "and construct the M step\nproxy by using the Q that",
    "start": "3138470",
    "end": "3144290"
  },
  {
    "text": "came from the variational step. Yes, question? Why is it called MCMC? ",
    "start": "3144290",
    "end": "3151759"
  },
  {
    "text": "Monte Carlo, Markov\nChain Monte Carlo. That's the techniques\nyou use are called MCMC",
    "start": "3151760",
    "end": "3157430"
  },
  {
    "text": "techniques to obtain samples.  All right, so now, a few more\ndetails about variational",
    "start": "3157430",
    "end": "3168410"
  },
  {
    "text": "inference, and then\nwe can start talking about the variational\nautoencoder.",
    "start": "3168410",
    "end": "3174050"
  },
  {
    "text": " So in variational inference,\nmost of the time, the question",
    "start": "3174050",
    "end": "3181970"
  },
  {
    "text": "is, how do we\nchoose the family Q from which we want to\nperform optimization? ",
    "start": "3181970",
    "end": "3190080"
  },
  {
    "text": "And a common family of\nprobability distributions",
    "start": "3190080",
    "end": "3198750"
  },
  {
    "text": "is to-- so remember, z-- so\nthe Q distribution that we are trying to recover\nin variational inference is",
    "start": "3198750",
    "end": "3206670"
  },
  {
    "text": "over z's, and z is a\nvector in Rk, right? So k-dimensional vector.",
    "start": "3206670",
    "end": "3213480"
  },
  {
    "text": "So our probability\ndistribution must be a distribution\nover k vectors,",
    "start": "3213480",
    "end": "3219600"
  },
  {
    "text": "or vectors of\ndimension k, right? And high-dimensional probability\ndistributions are quite few.",
    "start": "3219600",
    "end": "3232740"
  },
  {
    "text": "You don't have as many\nprobability distributions as distributions\nover scalar values.",
    "start": "3232740",
    "end": "3241440"
  },
  {
    "text": "And a common assumption made\nin variational inference",
    "start": "3241440",
    "end": "3247380"
  },
  {
    "text": "is to assume that Q of z are\nthe family of distributions",
    "start": "3247380",
    "end": "3255779"
  },
  {
    "text": "from which you want to perform\nthe optimization, Q of z, where z is in--",
    "start": "3255780",
    "end": "3262440"
  },
  {
    "text": "think of it as in Rk-- can be factored into\ncomponents, Q1 of z1 times",
    "start": "3262440",
    "end": "3272910"
  },
  {
    "text": "Q2 of z2 times Qk of zk, right?",
    "start": "3272910",
    "end": "3281010"
  },
  {
    "text": "You're going to assume-- if we assume that the\ncomponents of the z vector",
    "start": "3281010",
    "end": "3288780"
  },
  {
    "text": "can be factored into\nindependent scalar probability distributions-- so if we\nmake this assumption, which",
    "start": "3288780",
    "end": "3294780"
  },
  {
    "text": "is a common assumption\nthat is made, then there is a name\ngiven to this assumption.",
    "start": "3294780",
    "end": "3302340"
  },
  {
    "text": "It is called\nmean-field assumption. ",
    "start": "3302340",
    "end": "3309380"
  },
  {
    "text": "Mean-field assumption--\nand variational inference that uses this kind\nof a factorization",
    "start": "3309380",
    "end": "3315790"
  },
  {
    "text": "is called mean-field\nvariational inference. And the roots into why\nwe make this assumption",
    "start": "3315790",
    "end": "3326440"
  },
  {
    "text": "is beyond the scope. For those who are interested-- OK, so the obvious first\nand the most simple reason",
    "start": "3326440",
    "end": "3333850"
  },
  {
    "text": "is that it makes\ncomputation easier. You're making strong,\nindependent assumptions,",
    "start": "3333850",
    "end": "3339520"
  },
  {
    "text": "but you get the\ncomputational ease in return. And there are other\ngood reasons why this",
    "start": "3339520",
    "end": "3349430"
  },
  {
    "text": "can be done in certain cases. For example, so the mean-field\nvariational inference actually comes from statistical physics,\nwhere variational inference was",
    "start": "3349430",
    "end": "3356720"
  },
  {
    "text": "kind of-- I guess it was invented in\nstatistical physics, where",
    "start": "3356720",
    "end": "3363140"
  },
  {
    "text": "you make this\nmean-field assumption, and that holds well\nin statistical physics for whatever reasons.",
    "start": "3363140",
    "end": "3369380"
  },
  {
    "text": "But it may or may not\nhold on your data, but it's still commonly\ndone because it",
    "start": "3369380",
    "end": "3375170"
  },
  {
    "text": "makes computation easy. And, in fact, the\nword \"mean-field\" comes from statistical\nphysics, where--",
    "start": "3375170",
    "end": "3382580"
  },
  {
    "text": "from field theory and whatnot\nin statistical physics, where they make\nthese approximations that things are independent.",
    "start": "3382580",
    "end": "3389817"
  },
  {
    "text": "However, you're going to call\nall the techniques that make",
    "start": "3389817",
    "end": "3394849"
  },
  {
    "text": "this assumption as\nmean-field techniques, where the Q that\nwe are trying to--",
    "start": "3394850",
    "end": "3401420"
  },
  {
    "text": "the family Q,\nwhere we are trying to perform the\nmaximization, can be",
    "start": "3401420",
    "end": "3407010"
  },
  {
    "text": "factored into these\nindividual components. ",
    "start": "3407010",
    "end": "3413050"
  },
  {
    "text": "And this mean-field technique,\nmean-field assumption, is something that\nyou're going to use",
    "start": "3413050",
    "end": "3419170"
  },
  {
    "text": "in variational autoencoders. So that's just a name,\nvariational autoencoders.",
    "start": "3419170",
    "end": "3430320"
  },
  {
    "start": "3430320",
    "end": "3442160"
  },
  {
    "text": "So if you remember, in EM, we\nconstructed this ELBO, right?",
    "start": "3442160",
    "end": "3453079"
  },
  {
    "text": "And the ELBO had a Q and\nparameter theta, right?",
    "start": "3453080",
    "end": "3458480"
  },
  {
    "text": "In the E step, we would\nfind the best possible Q, which is the posterior.",
    "start": "3458480",
    "end": "3464390"
  },
  {
    "text": "And in the M step, we would\nupdate theta and the error",
    "start": "3464390",
    "end": "3470359"
  },
  {
    "text": "to recover the next\nestimate of theta. And while performing each of\nthese steps, in the E step,",
    "start": "3470360",
    "end": "3476960"
  },
  {
    "text": "while we were\ncalculating the next Q, we would hold theta fixed. And in the M step, when we were\ncalculating the next theta,",
    "start": "3476960",
    "end": "3484170"
  },
  {
    "text": "we would hold Q fixed, right? And that is a technique that's\nalso called coordinate ascent,",
    "start": "3484170",
    "end": "3493790"
  },
  {
    "text": "right? And coordinate ascent\nor coordinate descent-- ",
    "start": "3493790",
    "end": "3499000"
  },
  {
    "text": "our objective has\nmultiple variables. And the way we go\nabout optimizing",
    "start": "3499000",
    "end": "3504190"
  },
  {
    "text": "it is to start with\nsome initialization, hold most of them fixed,\nor all except one fixed,",
    "start": "3504190",
    "end": "3509860"
  },
  {
    "text": "or all except a subset fixed. And by holding them\nfixed, optimize the ones",
    "start": "3509860",
    "end": "3515170"
  },
  {
    "text": "that we have not held fixed. And once you obtain\nthat updated estimate,",
    "start": "3515170",
    "end": "3520480"
  },
  {
    "text": "then hold the updated\nestimates fixed and optimize the\nother ones and so on.",
    "start": "3520480",
    "end": "3527360"
  },
  {
    "text": "So one way to think of it\nis, if this is Q and theta,",
    "start": "3527360",
    "end": "3538550"
  },
  {
    "text": "and supposing this is the\ncontour plot of the ELBO,",
    "start": "3538550",
    "end": "3546770"
  },
  {
    "text": "we start with some-- let's say we start at\nsome point over here. And we hold Q fixed and optimize\nit with respect to theta,",
    "start": "3546770",
    "end": "3557510"
  },
  {
    "text": "and we need some\npoint over here. And then we hold theta fixed and\noptimize it with respect to Q,",
    "start": "3557510",
    "end": "3566860"
  },
  {
    "text": "and we need some point here. And then we hold Q fixed\nand optimize it with respect to theta, and then\nhold theta fixed,",
    "start": "3566860",
    "end": "3571960"
  },
  {
    "text": "optimize it with\nrespect to Q, and so on, where we are updating only\none of the axes at a time.",
    "start": "3571960",
    "end": "3579339"
  },
  {
    "text": "And this is called\nbasically coordinate ascent. ",
    "start": "3579340",
    "end": "3588849"
  },
  {
    "text": "We are trying to climb\nthis hill of this contour plot of the ELBO by moving in\nonly one direction at a time.",
    "start": "3588850",
    "end": "3597470"
  },
  {
    "text": "So we are either\nmoving north/south or moving east/west, right? And keep moving until\nyou've kind of reached",
    "start": "3597470",
    "end": "3605060"
  },
  {
    "text": "the local optima\nalong that direction and stop there, and then\nstart moving north/south",
    "start": "3605060",
    "end": "3611269"
  },
  {
    "text": "until you reach a local optima,\nand then again, east/west until you reach a local\noptima, and so on. So that's coordinate ascent.",
    "start": "3611270",
    "end": "3617277"
  },
  {
    "text": " And this coordinate\nascent kind of",
    "start": "3617277",
    "end": "3622880"
  },
  {
    "text": "worked well with\nclassical EM where,",
    "start": "3622880",
    "end": "3628279"
  },
  {
    "text": "the optimization along the Q\nstep was not gradient ascent, but we were just calculating\nthe posterior, right?",
    "start": "3628280",
    "end": "3635060"
  },
  {
    "text": "So calculating the posterior--\nso starting from this and just calculating\nthe new Q, which",
    "start": "3635060",
    "end": "3640220"
  },
  {
    "text": "is the posterior corresponding\nto this value of theta, was the E step, right?",
    "start": "3640220",
    "end": "3646370"
  },
  {
    "text": "So the E step moves this way,\nand the M step moves this way. ",
    "start": "3646370",
    "end": "3655260"
  },
  {
    "text": "Now, using this kind of\ncoordinate ascent, if we",
    "start": "3655260",
    "end": "3661230"
  },
  {
    "text": "ask the question, can we do\ngradient descent instead, what does that even mean?",
    "start": "3661230",
    "end": "3666860"
  },
  {
    "text": "Instead of doing\ncoordinate ascent, can you do gradient descent? And that basically gives rise\nto the variational autoencoder.",
    "start": "3666860",
    "end": "3680070"
  },
  {
    "text": "So in the variational\nautoencoder, we are going to maximize the\nELBO using gradient descent.",
    "start": "3680070",
    "end": "3686490"
  },
  {
    "text": "And the way we go\nabout doing it, we're going to first assume that\nthe distribution from the model",
    "start": "3686490",
    "end": "3694980"
  },
  {
    "text": "is something like this. We're going to assume z comes\nfrom some normal distribution",
    "start": "3694980",
    "end": "3703074"
  },
  {
    "text": "k times k. So that's where z comes from.",
    "start": "3703074",
    "end": "3709910"
  },
  {
    "text": "And then z comes from that.",
    "start": "3709910",
    "end": "3716910"
  },
  {
    "text": "And x given z comes from\nsome normal distribution,",
    "start": "3716910",
    "end": "3728329"
  },
  {
    "text": "whose mean is given by a\nfunction g of z parameterized",
    "start": "3728330",
    "end": "3735650"
  },
  {
    "text": "by theta. And we're going to assume some\nkind of a fixed variance, OK?",
    "start": "3735650",
    "end": "3746800"
  },
  {
    "text": "So z is like the\nlatent variable.",
    "start": "3746800",
    "end": "3751820"
  },
  {
    "text": "So z is sampled from\nsome normal distribution. There's the prior.",
    "start": "3751820",
    "end": "3758859"
  },
  {
    "text": "And x given z, think of it\nas the likelihood, right?",
    "start": "3758860",
    "end": "3764970"
  },
  {
    "start": "3764970",
    "end": "3773030"
  },
  {
    "text": "And now, in order to\nperform the E step,",
    "start": "3773030",
    "end": "3785000"
  },
  {
    "text": "we need to get the\nposterior, right? The posterior is-- if\nwe have access to it,",
    "start": "3785000",
    "end": "3793470"
  },
  {
    "text": "it would be P of z given x, OK? But with this kind\nof a model, where",
    "start": "3793470",
    "end": "3803600"
  },
  {
    "text": "g is a neural network\nwith parameter theta,",
    "start": "3803600",
    "end": "3812920"
  },
  {
    "text": "it's very hard to\ncalculate p of z given x. You're probably never going\nto be able to obtain a P of z given x in a\nclosed-from estimate.",
    "start": "3812920",
    "end": "3820240"
  },
  {
    "text": "So when we don't have\nan exact solution, basically, we have\ntwo other options-- sampling or\nvariational inference.",
    "start": "3820240",
    "end": "3825970"
  },
  {
    "text": "And the techniques that's used\nin the variational autoencoder is, not surprisingly,\nvariational inference,",
    "start": "3825970",
    "end": "3832293"
  },
  {
    "text": "which is why we call it the\nvariational autoencoder. So we want to estimate or\napproximate P of z given",
    "start": "3832293",
    "end": "3837974"
  },
  {
    "text": "x using variational inference. And the way we go about\ndoing variational inference",
    "start": "3837975",
    "end": "3843940"
  },
  {
    "text": "is that the family Q, Q family\nfor variational inference--",
    "start": "3843940",
    "end": "3854109"
  },
  {
    "text": " we need to choose a family for\ndoing variational inference.",
    "start": "3854110",
    "end": "3861080"
  },
  {
    "text": "And that family\nis basically Qi is",
    "start": "3861080",
    "end": "3868760"
  },
  {
    "text": "equal to normal distribution\nwith mean small q xi, phi",
    "start": "3868760",
    "end": "3883410"
  },
  {
    "text": "and a diagonal of v\nof xi, psi, right?",
    "start": "3883410",
    "end": "3893640"
  },
  {
    "start": "3893640",
    "end": "3900079"
  },
  {
    "text": "And what does this mean, right? So first, we spoke\nabout having families",
    "start": "3900080",
    "end": "3905330"
  },
  {
    "text": "of distributions for performing\nvariational inference.",
    "start": "3905330",
    "end": "3911210"
  },
  {
    "text": "But in the distribution, of\nthe family of distributions",
    "start": "3911210",
    "end": "3918109"
  },
  {
    "text": "on which we perform\nvariational inference, we would get a different Q\ndistribution, per example,",
    "start": "3918110",
    "end": "3923892"
  },
  {
    "text": "right? In EM, the E step is performed\nseparately for each example.",
    "start": "3923892",
    "end": "3934260"
  },
  {
    "text": "However, here, what\nwe are doing is we are recognizing the fact that\nQi depends on x because Qi is",
    "start": "3934260",
    "end": "3943609"
  },
  {
    "text": "supposed to be P of z given x. And we are going to\napproximate the Q distribution",
    "start": "3943610",
    "end": "3949970"
  },
  {
    "text": "across all examples using\na neural network, right? So the neural\nnetwork will take x",
    "start": "3949970",
    "end": "3955760"
  },
  {
    "text": "as the input and output\nthe mean and variance of a normal distribution for\nthe corresponding example Qi,",
    "start": "3955760",
    "end": "3964070"
  },
  {
    "text": "for the corresponding example i. And this is sometimes\ncalled amortized inference",
    "start": "3964070",
    "end": "3976400"
  },
  {
    "text": "because the\ndifference between EM and the variational\nautoencoder here",
    "start": "3976400",
    "end": "3983630"
  },
  {
    "text": "is that, in EM, we were\nseparately calculating the parameters of the Q\ndistribution for each example",
    "start": "3983630",
    "end": "3990530"
  },
  {
    "text": "separately and independently. We would calculate Q-- ",
    "start": "3990530",
    "end": "3997090"
  },
  {
    "text": "I'm just going to pull\nthis here too, right?",
    "start": "3997090",
    "end": "4005160"
  },
  {
    "text": "In EM, we will loop over each\nexample, and for each example, separately, we would calculate\nP of z given xi, right?",
    "start": "4005160",
    "end": "4012779"
  },
  {
    "text": "And once you calculate\nthat, keep it aside, pick the next example,\nand repeat this and calculate the Q\ndistribution for that example.",
    "start": "4012780",
    "end": "4021000"
  },
  {
    "text": "And we will do that\nindependently for each example. Whereas with amortized\ninference, what we instead do",
    "start": "4021000",
    "end": "4027540"
  },
  {
    "text": "is we're not going to\ncalculate a separate posterior distribution for each example.",
    "start": "4027540",
    "end": "4033430"
  },
  {
    "text": "Instead, what we\nare going to do is we're going to assume\nthat all Q's belong",
    "start": "4033430",
    "end": "4040950"
  },
  {
    "text": "to a normal distribution. And each of these--",
    "start": "4040950",
    "end": "4046770"
  },
  {
    "text": "the mean and variance\nof each Qi is going to be a function\nof your x's, right?",
    "start": "4046770",
    "end": "4054368"
  },
  {
    "text": "They're going to be a\nfunction of your x's. The mean and variance of each\nQi is going to be a function of the x's.",
    "start": "4054368",
    "end": "4059730"
  },
  {
    "text": "And those functions are\nessentially neural networks. So feed x as the input,\nand the neural network",
    "start": "4059730",
    "end": "4066750"
  },
  {
    "text": "will output two\nscalars per input. ",
    "start": "4066750",
    "end": "4072300"
  },
  {
    "text": "One vector will be the mean. The other vector\nwill be the variance.",
    "start": "4072300",
    "end": "4078010"
  },
  {
    "text": "So your Q network would\nlook something like this. You take x as the input, and\nyou will have a few layers.",
    "start": "4078010",
    "end": "4088780"
  },
  {
    "text": " And as the final layer, you\nwill get mu, so the mean.",
    "start": "4088780",
    "end": "4104370"
  },
  {
    "text": "We call this q\nparameterized by phi. So phi, q phi, this\nentire network--",
    "start": "4104370",
    "end": "4113180"
  },
  {
    "text": "this is q parameterized by phi. So phi represents\nall the weights and bias of this network.",
    "start": "4113180",
    "end": "4118210"
  },
  {
    "text": "So the input to this\nnetwork is an example, xi. ",
    "start": "4118210",
    "end": "4124240"
  },
  {
    "text": "And the output of this\nnetwork is some vector mu i.",
    "start": "4124240",
    "end": "4131290"
  },
  {
    "text": "xi was in Rd.  Mu i is in Rk.",
    "start": "4131290",
    "end": "4139439"
  },
  {
    "text": "And this Mu i will\nbe used as the mean of a normal distribution,\nwhere this normal distribution",
    "start": "4139439",
    "end": "4147950"
  },
  {
    "text": "represents Qi. Yes, question? Why do we need a bottleneck\nfor the distribution?",
    "start": "4147950",
    "end": "4153299"
  },
  {
    "text": "So there's no bottleneck here. We are just mapping it\nfrom x to mu, right? There's no bottleneck here.",
    "start": "4153300",
    "end": "4158960"
  },
  {
    "text": "These could be any\ndimensions, right? We just need to go from x to z. This is not an autoencoder?",
    "start": "4158960",
    "end": "4165229"
  },
  {
    "text": "This is not an autoencoder yet. It's just some network that\ntakes you from x to z's.",
    "start": "4165229",
    "end": "4170359"
  },
  {
    "text": "There is no bottleneck here. Yes, question? Why don't we just assume\n[INAUDIBLE] is k less than d",
    "start": "4170359",
    "end": "4175770"
  },
  {
    "text": "here? Yeah, k is generally\nless than d. Yes, that's the\ngeneral assumption because, in general, the latent\nrepresentations will always",
    "start": "4175770",
    "end": "4184040"
  },
  {
    "text": "have a compact representation. So z's will be of\ndimension k by k, and x's--",
    "start": "4184040",
    "end": "4190700"
  },
  {
    "text": "and this will be in Rd. So when using that mu\nfor [INAUDIBLE] Q--",
    "start": "4190700",
    "end": "4197030"
  },
  {
    "text": "like, mu is the mean for the new\nfamily for the distribution Q, right? Yes. So why does it need to be\nin the lower dimension?",
    "start": "4197030",
    "end": "4203400"
  },
  {
    "text": "Can it be in the\nsame dimension, then? So Q is a distribution over z. So the question is, why\nis mu a smaller dimension?",
    "start": "4203400",
    "end": "4211930"
  },
  {
    "text": "That's because Q is a\ndistribution over z.",
    "start": "4211930",
    "end": "4216970"
  },
  {
    "text": "And z is k-dimensional,\nso the mean has to be k-dimensional right?",
    "start": "4216970",
    "end": "4222170"
  },
  {
    "text": "So this should be in Rk. The mean should be in\nRk because z is in Rk.",
    "start": "4222170",
    "end": "4229090"
  },
  {
    "text": " And the covariance should\nbe in R of k times k.",
    "start": "4229090",
    "end": "4237460"
  },
  {
    "text": " Yes, question? OK, so the formula\nthat you wrote",
    "start": "4237460",
    "end": "4242929"
  },
  {
    "text": "there that is about referring\nto the variational autoencoder if you just [INAUDIBLE]?",
    "start": "4242929",
    "end": "4248650"
  },
  {
    "text": "So the thing we discussed\nin the beginning was just autoencoders all the\nway at the beginning.",
    "start": "4248650",
    "end": "4254050"
  },
  {
    "text": "That was not variational\nautoencoders. So, OK, I was going\nto ask you, why is the z and x given z\njustified to be that?",
    "start": "4254050",
    "end": "4261928"
  },
  {
    "text": "Like, why is that-- So this is the model. So why is this justified? This is the model we\nare starting with.",
    "start": "4261928",
    "end": "4267220"
  },
  {
    "text": "We are not proving anything. We are assuming if\nthis is the model and seeing what the\nconsequences are.",
    "start": "4267220",
    "end": "4273429"
  },
  {
    "text": "So that z is not the compressed\nversion, not the hidden latent version of it?",
    "start": "4273430",
    "end": "4278560"
  },
  {
    "text": "Yeah, so I would say, hold\noff making connections to autoencoders for the moment. We'll put them all together\nand see a connection soon,",
    "start": "4278560",
    "end": "4285740"
  },
  {
    "text": "all right? So for now, z is\nsome hidden layer. You're right. Eventually, this will\nbe the bottleneck layer.",
    "start": "4285740",
    "end": "4291260"
  },
  {
    "text": "But for now, assume-- just\nlike in factor analysis, in factor analysis, z was\nsome k-dimensional vector,",
    "start": "4291260",
    "end": "4298750"
  },
  {
    "text": "and x given z was\nsomething else. So in place of that, assume you\nhave a model like this, right?",
    "start": "4298750",
    "end": "4306110"
  },
  {
    "text": "So we go from x, which\nis in Rd, down to Rk,",
    "start": "4306110",
    "end": "4312199"
  },
  {
    "text": "and that will be\nthe mean, right? Yes, question? I don't understand how you\ngot to the covariance matrix.",
    "start": "4312200",
    "end": "4318781"
  },
  {
    "text": "I'm going to come\nto the covariance now right here, right? So that gave us the mean. For the covariance, instead,\nwhat we are going to do",
    "start": "4318782",
    "end": "4326390"
  },
  {
    "text": "is have something very similar. Instead of q, we are\ngoing to call it v, right?",
    "start": "4326390",
    "end": "4332520"
  },
  {
    "text": " So again, we're\nstarting with x in Rd",
    "start": "4332520",
    "end": "4344540"
  },
  {
    "text": "and have some kind\nof a network, right?",
    "start": "4344540",
    "end": "4349700"
  },
  {
    "text": " I know over here, you're going\nto get v, again, in Rk, right?",
    "start": "4349700",
    "end": "4363210"
  },
  {
    "text": "And this v, are then\ngoing to make it positive because standard deviations and\nvariances are always positive.",
    "start": "4363210",
    "end": "4370600"
  },
  {
    "text": "And there are many techniques\nyou can use to take any number and make it positive. One approach is to\njust square them.",
    "start": "4370600",
    "end": "4377250"
  },
  {
    "text": "Another approach is to\nexponentiate each element, and both are commonly done.",
    "start": "4377250",
    "end": "4383110"
  },
  {
    "text": "So this v is generally\ntaken to be--",
    "start": "4383110",
    "end": "4388409"
  },
  {
    "text": "let's call the last layer as-- I don't know-- u,\nnot v, u right?",
    "start": "4388410",
    "end": "4395960"
  },
  {
    "text": "Generally, the last layer\nof the variance network is taken to be e to\nthe u element-wise",
    "start": "4395960",
    "end": "4405380"
  },
  {
    "text": "to get positive standard\ndeviations, right? And this vector of\nlength k is then",
    "start": "4405380",
    "end": "4413480"
  },
  {
    "text": "converted into a\ndiagonal matrix, where we have v1, v2, and vk.",
    "start": "4413480",
    "end": "4423960"
  },
  {
    "text": "And this will be k by k. And this matrix is used as\nthe covariance matrix of Qi.",
    "start": "4423960",
    "end": "4431340"
  },
  {
    "text": " Why, though? It seems like you have two\ndifferent models there.",
    "start": "4431340",
    "end": "4438360"
  },
  {
    "text": "We have two different\nmodels there. Two different networks.",
    "start": "4438360",
    "end": "4443460"
  },
  {
    "text": "Two different\nnetworks to take us from x to one parameter\nand x to another parameter.",
    "start": "4443460",
    "end": "4448650"
  },
  {
    "text": "In practice, what's\ncommonly done is to have a single network, where\neverything until the last layer",
    "start": "4448650",
    "end": "4455190"
  },
  {
    "text": "is shared. And at the last layer, you\nhave a separate branch, so one branch for the mean,\none branch for the variance",
    "start": "4455190",
    "end": "4463290"
  },
  {
    "text": "because we need two parameters\nfor a normal distribution.",
    "start": "4463290",
    "end": "4469860"
  },
  {
    "text": "We need a mu and we\nneed a covariance. And you can get all of them as\nthe output from the last layer.",
    "start": "4469860",
    "end": "4478200"
  },
  {
    "text": "That's also totally\nfine that there's not much difference between having\ndistinct networks versus having",
    "start": "4478200",
    "end": "4485730"
  },
  {
    "text": "one network, where you kind\nof split the last layer into different parameters. Both work totally fine.",
    "start": "4485730",
    "end": "4492026"
  },
  {
    "text": "I don't want to say\nthis because Q-- I could understand how\nthe Q function describes",
    "start": "4492026",
    "end": "4498240"
  },
  {
    "text": "this family of Q because\nthe model is the same. But I don't understand\nhow you derive this from a separate model\nwith the variance of this.",
    "start": "4498240",
    "end": "4504690"
  },
  {
    "text": "Why is the-- So the variance of this? Yeah. So, essentially, Qi--\nso the question is,",
    "start": "4504690",
    "end": "4512370"
  },
  {
    "text": "I guess, why do we\nhave two networks? I guess that's at the\nheart of the question. So the Qi, we define it to\nbe a normal distribution.",
    "start": "4512370",
    "end": "4521670"
  },
  {
    "text": "And in order to define\na normal distribution, you need two parameters, right? So for each example, we\nneed two parameters, a mean",
    "start": "4521670",
    "end": "4527699"
  },
  {
    "text": "and variance. For another example,\nanother mean and a variance. And because they are in\nhigh-dimensional space,",
    "start": "4527700",
    "end": "4533910"
  },
  {
    "text": "the mean and variance will be\na vector and a matrix, right?",
    "start": "4533910",
    "end": "4539750"
  },
  {
    "text": "Take a new example-- you need a new mean vector and\na new covariance matrix, right? The mean-covariance\npair is per example.",
    "start": "4539750",
    "end": "4548150"
  },
  {
    "text": "And the way, instead of\nseparately estimating them for each example, we are going\nto perform amortized inference,",
    "start": "4548150",
    "end": "4556130"
  },
  {
    "text": "which means recognizing\nthe fact that they depend on x, we try to capture\nthe relation using a shared",
    "start": "4556130",
    "end": "4563239"
  },
  {
    "text": "network, where you feed\nin x, and the output is the mean vector.",
    "start": "4563240",
    "end": "4569119"
  },
  {
    "text": "Feed in x, and the output\nis a covariance matrix.",
    "start": "4569120",
    "end": "4574520"
  },
  {
    "text": "Instead of calculating\nthem separately, this technique is called\namortized inference, right?",
    "start": "4574520",
    "end": "4581660"
  },
  {
    "text": "And instead of estimating\nall the means and variances",
    "start": "4581660",
    "end": "4587990"
  },
  {
    "text": "separately, we instead learn\nthe parameters phi and psi. ",
    "start": "4587990",
    "end": "4594800"
  },
  {
    "text": "Yes, question? Actually, as you mentioned, in\nthe case of the q, we use a--",
    "start": "4594800",
    "end": "4601210"
  },
  {
    "text": "parameterize by the phi? Yes. And then in case\nof the v, we are",
    "start": "4601210",
    "end": "4606462"
  },
  {
    "text": "parameterizing under the psi? Yeah. How can we use the\nnetwork to get them?",
    "start": "4606462",
    "end": "4612130"
  },
  {
    "text": "Yeah, so the question is,\nhere, we use the notation that q is parameterized\nby phi, and here, it's",
    "start": "4612130",
    "end": "4618610"
  },
  {
    "text": "parameterized by psi. And I also mentioned that\nyou can share all the layers",
    "start": "4618610",
    "end": "4623710"
  },
  {
    "text": "and have just the\nlast layer separate. If we do the shared\nlayer, then this notation",
    "start": "4623710",
    "end": "4628719"
  },
  {
    "text": "does not work, right? So according to this\nnotation, we just",
    "start": "4628720",
    "end": "4635889"
  },
  {
    "text": "have two different\nnetworks, right? And these parameters are\npsi, and these parameters",
    "start": "4635890",
    "end": "4641650"
  },
  {
    "text": "are phi, right? But you can-- in\npractice, what's commonly",
    "start": "4641650",
    "end": "4647740"
  },
  {
    "text": "done is you don't have two\ncompletely separate networks because these two will\nhave shared parameters.",
    "start": "4647740",
    "end": "4655857"
  },
  {
    "text": "Yes, question? You're kind of just assuming\nthat you guys have--",
    "start": "4655857",
    "end": "4661390"
  },
  {
    "text": "you completely made\nup q and v, and we said that we get the\nvariance from them.",
    "start": "4661390",
    "end": "4669280"
  },
  {
    "text": "But how are you even going\nto verify what those two networks without [INAUDIBLE]? We'll come to that.",
    "start": "4669280",
    "end": "4674620"
  },
  {
    "text": "We'll come to that. We're going to come\nto that, right? So we're going to have\nthese two networks, which,",
    "start": "4674620",
    "end": "4682540"
  },
  {
    "text": "when you start, they're\nuntrained, randomly initialized. You feed in an x. What comes out at\nthe other end, you",
    "start": "4682540",
    "end": "4690489"
  },
  {
    "text": "use it as the mean of the Q\ndistribution for the E step. And at the other\nnetwork, feed in.",
    "start": "4690490",
    "end": "4697449"
  },
  {
    "text": "You get a vector, convert\nit into a diagonal matrix, and use it as the\ncovariance matrix",
    "start": "4697450",
    "end": "4704830"
  },
  {
    "text": "for the E step of that example. So for each example, you feed\nit once through this and once",
    "start": "4704830",
    "end": "4710230"
  },
  {
    "text": "through this, take\nthe two parameters, and plug it, and construct the\nE step of that example, right?",
    "start": "4710230",
    "end": "4717550"
  },
  {
    "text": "And now, because we are assuming\na Gaussian distribution,",
    "start": "4717550",
    "end": "4723320"
  },
  {
    "text": "and because we are having a\ndiagonal covariance matrix--",
    "start": "4723320",
    "end": "4729050"
  },
  {
    "text": "so Gaussian distributions\nhave this property that if there is no correlation\nbetween two components",
    "start": "4729050",
    "end": "4736930"
  },
  {
    "text": "of a joint Gaussian,\nthen they necessarily must be independent, right? The Gaussian distribution\nis probably the only one.",
    "start": "4736930",
    "end": "4743980"
  },
  {
    "text": "Maybe there are\nothers, but I think it's only the Gaussian that had\nthis property that if you take a joint Gaussian vector and\ntwo components of the joint",
    "start": "4743980",
    "end": "4756040"
  },
  {
    "text": "are uncorrelated,\nhave zero covariance, then the two are necessarily\nindependent, right?",
    "start": "4756040",
    "end": "4762460"
  },
  {
    "text": "So this diagonal\ncovariance matrix means we are making the\nmean-field assumption.",
    "start": "4762460",
    "end": "4774044"
  },
  {
    "start": "4774044",
    "end": "4780340"
  },
  {
    "text": "So because we are constructing\na diagonal matrix from a vector,",
    "start": "4780340",
    "end": "4786010"
  },
  {
    "text": "and we are using a\nnormal distribution, we are effectively doing\nmean-field variational inference, where we are assuming\nthat each of the zi's are",
    "start": "4786010",
    "end": "4794650"
  },
  {
    "text": "independent. So this is the mean of the zi's,\nand these are the variances",
    "start": "4794650",
    "end": "4801130"
  },
  {
    "text": "of the zi's. Yes, question? That squaring for the diagonal--\nwhat element are you squaring?",
    "start": "4801130",
    "end": "4806200"
  },
  {
    "text": "Yeah, so you can think-- I would say, don't\nworry too much about this squaring because\nwhat you get out of here",
    "start": "4806200",
    "end": "4813670"
  },
  {
    "text": "is generally the\nstandard deviation, and the variance is\nbasically the square",
    "start": "4813670",
    "end": "4818710"
  },
  {
    "text": "of the standard deviation. That's why I use\nthe notation here, just the way in we write\nmean and sigma squared.",
    "start": "4818710",
    "end": "4825385"
  },
  {
    "text": " But what if it is diagonal?",
    "start": "4825385",
    "end": "4831470"
  },
  {
    "text": "Because it's a\ndiagonal matrix, you can think of it as\nelement-wise, yes.",
    "start": "4831470",
    "end": "4836747"
  },
  {
    "text": "Yes, question? Can we just train\nthe model to such that the output is\nset to the variance for the standard deviation?",
    "start": "4836747",
    "end": "4843382"
  },
  {
    "text": "Can you please\nrepeat the question? Should we train the model such\nthat the output is on variance",
    "start": "4843382",
    "end": "4848530"
  },
  {
    "text": "instead of standard deviation? Variance instead of\nstandard deviation? The output is-- Yeah, you could.",
    "start": "4848530",
    "end": "4854572"
  },
  {
    "text": "You can treat this\nas the variance. You can treat this as\na standard deviation. It doesn't make a\nlot of difference.",
    "start": "4854572",
    "end": "4860650"
  },
  {
    "text": "Both approaches work fine. Yes, question? Just to confirm,\nphi and psi, they're both shared across\nall i examples, right?",
    "start": "4860650",
    "end": "4867190"
  },
  {
    "text": "Yes, good point. So phis and psis are shared\nacross examples, right?",
    "start": "4867190",
    "end": "4873250"
  },
  {
    "text": "You feed different examples\nthrough this network. The network is the same. The network stays\nfixed for all examples.",
    "start": "4873250",
    "end": "4879790"
  },
  {
    "text": "The output of the network\nwill be the mean and variance of the q distribution.",
    "start": "4879790",
    "end": "4885020"
  },
  {
    "text": "Good point.  So now, we basically have\nall the missing pieces,",
    "start": "4885020",
    "end": "4891940"
  },
  {
    "text": "or all the pieces to\nwrite out our objective. And there's going to\nbe one final trick that",
    "start": "4891940",
    "end": "4898720"
  },
  {
    "text": "is required to finish this up. ",
    "start": "4898720",
    "end": "4917890"
  },
  {
    "text": "So remember, you first observed\nthat EM was coordinate ascent.",
    "start": "4917890",
    "end": "4923310"
  },
  {
    "text": "And now, instead, we want\nto do gradient descent. And the Q's here are coming\nthrough this amortizing",
    "start": "4923310",
    "end": "4931094"
  },
  {
    "text": "inference, where we have\na single network that outputs the parameters\nfor each example rather than having different\nparameters for each example.",
    "start": "4931095",
    "end": "4943050"
  },
  {
    "text": "In standard EM, we calculate\ndifferent parameters for each example in the E step. But with amortized\ninference, we just",
    "start": "4943050",
    "end": "4949110"
  },
  {
    "text": "have one network\nhaving shared weights that outputs the Q parameters\nfor all examples, right?",
    "start": "4949110",
    "end": "4958010"
  },
  {
    "text": "So now the ELBO-- we can\nwrite the ELBO like this.",
    "start": "4958010",
    "end": "4964090"
  },
  {
    "start": "4964090",
    "end": "4969860"
  },
  {
    "text": "So the ELBO here\nwill be over phi,",
    "start": "4969860",
    "end": "4975850"
  },
  {
    "text": "psi, theta equal to\nsum over i equals 1",
    "start": "4975850",
    "end": "4981820"
  },
  {
    "text": "to n expectation of zi coming\nfrom Qi log P of xi, zi,",
    "start": "4981820",
    "end": "4999290"
  },
  {
    "text": "parameterized by theta\nover Qi of zi, right?",
    "start": "4999290",
    "end": "5009000"
  },
  {
    "text": "Where Qi is a normal\ndistribution with mean q xi",
    "start": "5009000",
    "end": "5021450"
  },
  {
    "text": "parameterized by\nphi, and covariance being a diagonal matrix,\nthe v network, which takes",
    "start": "5021450",
    "end": "5031080"
  },
  {
    "text": "input xi parameters psi, right?",
    "start": "5031080",
    "end": "5038480"
  },
  {
    "text": "So this is our ELBO now. The original ELBO--\nso for reference,",
    "start": "5038480",
    "end": "5045739"
  },
  {
    "text": "the ELBO with standard EM was--",
    "start": "5045740",
    "end": "5050870"
  },
  {
    "text": "over there, the ELBO\nhad just Q and theta, and that was sum over\ni equals 1 to n--",
    "start": "5050870",
    "end": "5061760"
  },
  {
    "text": "let me just make it nice--  As expectation zi\nfrom Qi log P of x,",
    "start": "5061760",
    "end": "5079960"
  },
  {
    "text": "z parameterized by\ntheta and Qi of zi.",
    "start": "5079960",
    "end": "5088840"
  },
  {
    "text": "So this was the\nELBO in case of EM,",
    "start": "5088840",
    "end": "5094800"
  },
  {
    "text": "where each Q was\nseparately calculated for each example in parallel.",
    "start": "5094800",
    "end": "5103840"
  },
  {
    "text": "However, when we move over\nto amortized inference,",
    "start": "5103840",
    "end": "5109320"
  },
  {
    "text": "we don't have Q\nand theta anymore. We had Q because we would\ncalculate Q separately",
    "start": "5109320",
    "end": "5116940"
  },
  {
    "text": "for each distribution. Instead of having separate\nQ's for each distribution, we instead have\nshared phi and psi",
    "start": "5116940",
    "end": "5126390"
  },
  {
    "text": "that are shared across\nexamples, right? And the rest of the\nexpression stays the same,",
    "start": "5126390",
    "end": "5138460"
  },
  {
    "text": "except Qi is now not something\nthat was independently calculated for each example.",
    "start": "5138460",
    "end": "5144610"
  },
  {
    "text": "But instead, we use this\namortized inference technique, where we feed each\nexample to these networks,",
    "start": "5144610",
    "end": "5151000"
  },
  {
    "text": "and the output of\nthe network will become the parameters of the\ncorresponding Q distribution,",
    "start": "5151000",
    "end": "5157293"
  },
  {
    "text": "right? So now we are pretty much ready. In case of EM, we would\noptimize this ELBO",
    "start": "5157293",
    "end": "5165610"
  },
  {
    "text": "with coordinate ascent. Now we're going to\noptimize this objective",
    "start": "5165610",
    "end": "5171160"
  },
  {
    "text": "with gradient descent, right? And the parameters\nthat we want optimize are the phi, psi, and theta.",
    "start": "5171160",
    "end": "5178659"
  },
  {
    "text": "So this means you want to now\nmaximize this ELBO with respect",
    "start": "5178660",
    "end": "5186300"
  },
  {
    "text": "to these parameters, which\nmeans we want our update rules to look like this. So we want theta to be\ntheta plus some learning",
    "start": "5186300",
    "end": "5197010"
  },
  {
    "text": "rate-- let's call it eta. That's just the learning rate--",
    "start": "5197010",
    "end": "5202184"
  },
  {
    "text": "times the gradient\nwith respect to theta of ELBO of psi, phi, theta.",
    "start": "5202185",
    "end": "5212580"
  },
  {
    "text": "And phi is equal to phi plus\neta gradient with respect",
    "start": "5212580",
    "end": "5219720"
  },
  {
    "text": "to phi of basically\nthe same thing. Similarly, psi equal\nto psi plus eta",
    "start": "5219720",
    "end": "5229260"
  },
  {
    "text": "times the gradient with\nrespect to psi of the ELBO.",
    "start": "5229260",
    "end": "5234940"
  },
  {
    "text": "You want to do this\nuntil convergence, right? In EM, we would do E step\nand M step until convergence.",
    "start": "5234940",
    "end": "5241110"
  },
  {
    "text": "In case of the\nvariational autoencoder, we want to perform\nthese gradient updates",
    "start": "5241110",
    "end": "5246330"
  },
  {
    "text": "until convergence, which\nmeans, in case of EM, we had Q's and thetas.",
    "start": "5246330",
    "end": "5251370"
  },
  {
    "text": "Now, we will have--  so thetas, and here, we\nwill have phi and psi.",
    "start": "5251370",
    "end": "5261780"
  },
  {
    "text": "I'm just writing them together. And you have-- I'm going to think you start\nwith some random location,",
    "start": "5261780",
    "end": "5267750"
  },
  {
    "text": "and we want to perform\ngradient update, right?",
    "start": "5267750",
    "end": "5274620"
  },
  {
    "text": "And most commonly, we\ndon't do gradient ascent. We do stochastic\ngradient descent.",
    "start": "5274620",
    "end": "5282270"
  },
  {
    "text": "And now the challenge is, how\ndo we calculate these gradients? And once we are able to\ncalculate these gradients,",
    "start": "5282270",
    "end": "5288420"
  },
  {
    "text": "we are effectively done. So the way we go about\ncalculating these gradients is--",
    "start": "5288420",
    "end": "5294220"
  },
  {
    "text": "so the first thing, the\ngradient with respect to theta. So gradient with respect to\ntheta of ELBO of phi, psi,",
    "start": "5294220",
    "end": "5304530"
  },
  {
    "text": "theta is equal to gradient\nwith respect to theta of i",
    "start": "5304530",
    "end": "5312110"
  },
  {
    "text": "equals 1 to n expectation\nof z with respect",
    "start": "5312110",
    "end": "5317310"
  },
  {
    "text": "to Qi of log P of\nx, z parameterized",
    "start": "5317310",
    "end": "5327830"
  },
  {
    "text": "by theta over Qi zi.",
    "start": "5327830",
    "end": "5335680"
  },
  {
    "text": "Now, how do we take the\ngradient of some term",
    "start": "5335680",
    "end": "5343720"
  },
  {
    "text": "under an expectation? We just take the gradient\ninside the expectation. And we can take the gradient\ninside the expectation",
    "start": "5343720",
    "end": "5350409"
  },
  {
    "text": "because the distribution\nwith which we are taking the expectation does\nnot depend on theta, right?",
    "start": "5350410",
    "end": "5358200"
  },
  {
    "text": "So this is equal to\nsum over i equals 1 to n expectation zi from\nQi of gradient with respect",
    "start": "5358200",
    "end": "5370235"
  },
  {
    "text": "to theta of log P of xi--",
    "start": "5370235",
    "end": "5377614"
  },
  {
    "text": "so xi and zi-- xi, zi, theta.",
    "start": "5377614",
    "end": "5384070"
  },
  {
    "text": " And the denominator\nwill just cancel, just",
    "start": "5384070",
    "end": "5390740"
  },
  {
    "text": "like in the case of EM. In case of EM, when we were\noptimizing with respect to theta, the denominator\ncould be removed, right?",
    "start": "5390740",
    "end": "5401830"
  },
  {
    "text": "And this, we will factor\nit out into P of x.",
    "start": "5401830",
    "end": "5408140"
  },
  {
    "text": "P of x, z is basically P of x\ngiven z times P of z, right?",
    "start": "5408140",
    "end": "5413335"
  },
  {
    "text": "And this is equal\nto i equal to 1 to n expectation zi from\nQi gradient with respect",
    "start": "5413335",
    "end": "5423280"
  },
  {
    "text": "to theta log P of xi\ngiven zi parameterized",
    "start": "5423280",
    "end": "5433699"
  },
  {
    "text": "by theta plus gradient with\nrespect to theta log P of zi,",
    "start": "5433700",
    "end": "5441960"
  },
  {
    "text": "right? And this just goes to 0.",
    "start": "5441960",
    "end": "5447120"
  },
  {
    "text": "And this, P of x given z-- if you remember P of x given\nz is our encoder, right?",
    "start": "5447120",
    "end": "5458360"
  },
  {
    "text": "So P of x given z, we\nassume takes this encoder.",
    "start": "5458360",
    "end": "5463489"
  },
  {
    "text": "And this is basically\nthe log likelihood",
    "start": "5463490",
    "end": "5471450"
  },
  {
    "text": "using g, the encoder. ",
    "start": "5471450",
    "end": "5477030"
  },
  {
    "text": "And we use the\nbackpropagation over here. Now, the challenge comes.",
    "start": "5477030",
    "end": "5483469"
  },
  {
    "text": "This was pretty straightforward. We could just take the gradient\ninside the expectation, and everything else\ninside was pretty",
    "start": "5483470",
    "end": "5490610"
  },
  {
    "text": "simple and straightforward. But now comes the challenge\nfor the other parameters. And this is where the\nVAE's innovation comes in.",
    "start": "5490610",
    "end": "5501150"
  },
  {
    "text": "So now, if you want to\ndo with respect to-- the gradient with respect\nto phi, ELBO of phi, psi,",
    "start": "5501150",
    "end": "5520270"
  },
  {
    "text": "theta equal to gradient\nwith respect to phi",
    "start": "5520270",
    "end": "5526560"
  },
  {
    "text": "sum i equals 1 to\nn expectation of zi",
    "start": "5526560",
    "end": "5534450"
  },
  {
    "text": "from Qi of log P of x,\nz theta and Q of zi.",
    "start": "5534450",
    "end": "5548110"
  },
  {
    "text": " But now the Q is\nbasically what we",
    "start": "5548110",
    "end": "5554700"
  },
  {
    "text": "saw is parameterized\nby phi and psi. And now, if you want to take\nthe gradient of this objective",
    "start": "5554700",
    "end": "5560670"
  },
  {
    "text": "with respect to phi,\nthen the distribution with respect to which we\nare taking the expectation",
    "start": "5560670",
    "end": "5566219"
  },
  {
    "text": "depends on phi, right? In these cases, we can just swap\nthe gradient and expectation.",
    "start": "5566220",
    "end": "5574330"
  },
  {
    "text": "And this is where,\nreally, the kind of key innovation of the\nvariational autoencoder",
    "start": "5574330",
    "end": "5579550"
  },
  {
    "text": "comes into picture, which\nis called reparameterization trick, right? So the reparameterization\ntrick is",
    "start": "5579550",
    "end": "5586150"
  },
  {
    "text": "something we've seen already in\nthe past, which is basically,",
    "start": "5586150",
    "end": "5591460"
  },
  {
    "text": "if z comes from some normal\ndistribution of mean mu",
    "start": "5591460",
    "end": "5599430"
  },
  {
    "text": "and standard\ndeviation psi, we can rewrite z to be\nequal to some epsilon",
    "start": "5599430",
    "end": "5608710"
  },
  {
    "text": "times standard deviation\nplus mu, where epsilon comes",
    "start": "5608710",
    "end": "5614230"
  },
  {
    "text": "from a normal distribution, a\nstandard normal distribution,",
    "start": "5614230",
    "end": "5620580"
  },
  {
    "text": "right? You're going to make use\nof this special property of Gaussian distributions.",
    "start": "5620580",
    "end": "5626369"
  },
  {
    "text": "This is also called the\nlocation-scale property, where you can decouple a\nGaussian random variable",
    "start": "5626370",
    "end": "5631670"
  },
  {
    "text": "from its parameters, where\nthe randomness is completely contained in the\nseparate variable",
    "start": "5631670",
    "end": "5637880"
  },
  {
    "text": "called epsilon, which has\nmean 0, standard deviation 1. And we can take these parameters\nand scale the standard normal",
    "start": "5637880",
    "end": "5650449"
  },
  {
    "text": "and move its location\nto mu, right? And now, we're going to rewrite\nthis objective as the gradient",
    "start": "5650450",
    "end": "5659730"
  },
  {
    "text": "with respect to phi. Sum of i equals 1 to n--",
    "start": "5659730",
    "end": "5665910"
  },
  {
    "text": "I have a question. Yes, question? So are you saying that\nthe distribution of Qi is parameterized by phi?",
    "start": "5665910",
    "end": "5672210"
  },
  {
    "text": "The distribution Qi is\nparameterized by phi precisely because Qi\nmean depends on phi.",
    "start": "5672210",
    "end": "5680179"
  },
  {
    "text": "OK. Right? And this, we're going\nto now rewrite to test.",
    "start": "5680180",
    "end": "5686050"
  },
  {
    "text": "i equals 1 to n,\nexpectation of epsilon",
    "start": "5686050",
    "end": "5694340"
  },
  {
    "text": "coming from some normal 0, 1. So instead of taking\nexpectation with respect to z,",
    "start": "5694340",
    "end": "5702437"
  },
  {
    "text": "we instead take the\nexpectation with respect to epsilon, which\nhas no parameters. And I'm going to write log of\nP of x comma-- in case of z,",
    "start": "5702437",
    "end": "5715020"
  },
  {
    "text": "we take epsilon. And the epsilon times\nthe standard deviation",
    "start": "5715020",
    "end": "5725550"
  },
  {
    "text": "plus the mean. So this might look\na little complex,",
    "start": "5725550",
    "end": "5732030"
  },
  {
    "text": "but let me use some\nsimpler notation here. So this, we will write\nit as epsilon i times--",
    "start": "5732030",
    "end": "5744120"
  },
  {
    "text": "I'm going to write it as psi\ni plus mu i, parameterized",
    "start": "5744120",
    "end": "5756740"
  },
  {
    "text": "by theta, divided by Q of\nepsilon i times plus mu i,",
    "start": "5756740",
    "end": "5775180"
  },
  {
    "text": "where mu i is equal\nto q of xi, phi.",
    "start": "5775180",
    "end": "5783840"
  },
  {
    "text": "And i is diagonal of v\nof xi parameterized by--",
    "start": "5783840",
    "end": "5796099"
  },
  {
    "text": " Right? And once we are able to rewrite\nthe expectation with respect",
    "start": "5796100",
    "end": "5804540"
  },
  {
    "text": "to z in terms of expectation\nwith respect to some epsilon, and in place of z, we're going\nto take epsilon and scale it",
    "start": "5804540",
    "end": "5813420"
  },
  {
    "text": "by the covariance matrix\nand add the mean vector that",
    "start": "5813420",
    "end": "5818460"
  },
  {
    "text": "comes from q and v,\nwhich is basically z. So this whole thing\nover here is zi.",
    "start": "5818460",
    "end": "5831060"
  },
  {
    "text": "And similarly, this\nwhole thing over here is also zi, where we\nmake use of this property",
    "start": "5831060",
    "end": "5839760"
  },
  {
    "text": "to do these replacements. And now, the\nexpectation does not",
    "start": "5839760",
    "end": "5845970"
  },
  {
    "text": "have this parameter anymore. And this will now\nallow us to swap it. And again, once we\nswap it, the gradients",
    "start": "5845970",
    "end": "5851880"
  },
  {
    "text": "can be taken in a\nstraightforward way. Yes, question? So [INAUDIBLE] our loss\nfunction-- shouldn't we",
    "start": "5851880",
    "end": "5860584"
  },
  {
    "text": "make mu predicted minus\nexactly squared [INAUDIBLE]??",
    "start": "5860584",
    "end": "5867060"
  },
  {
    "text": "So the question is, where is\nthe loss for the correct mu, I guess? I don't know. [INAUDIBLE] becomes ELBO\ninstead of that loss function.",
    "start": "5867060",
    "end": "5876700"
  },
  {
    "text": "Yes. Yeah, so let's piece\nit all together. That hopefully will\nanswer it, right? So this gives us\na way in which we",
    "start": "5876700",
    "end": "5883119"
  },
  {
    "text": "can take the gradient\nof the ELBO with respect to phi, right?",
    "start": "5883120",
    "end": "5888280"
  },
  {
    "text": "Taking the gradient of the\nELBO with respect to theta was pretty\nstraightforward because we could swap the gradient\nand expectation",
    "start": "5888280",
    "end": "5894190"
  },
  {
    "text": "because the expectation did not\ndepend on theta in any ways. But for the other parameters,\nwhere the expectation",
    "start": "5894190",
    "end": "5901420"
  },
  {
    "text": "depends on phi itself, we make\nuse of this reparameterization trick, where, with the\nreparameterization trick,",
    "start": "5901420",
    "end": "5908949"
  },
  {
    "text": "we can rewrite the\nexpectation from in terms of z to in terms of epsilon\nthat has no parameters.",
    "start": "5908950",
    "end": "5914687"
  },
  {
    "text": "Yes, question? You sample epsilon, or-- Yes, I'm going to\ncome to that, right? So, now, we've replaced the\nexpectation with respect",
    "start": "5914687",
    "end": "5925210"
  },
  {
    "text": "to z with expectation with\nrespect to epsilon, right?",
    "start": "5925210",
    "end": "5930790"
  },
  {
    "text": "So the question is now,\npiecing this all together-- ",
    "start": "5930790",
    "end": "5937730"
  },
  {
    "text": "over here, what we can\nsee is we still have,",
    "start": "5937730",
    "end": "5947400"
  },
  {
    "text": "even though we were able\nto swap the gradient and the expectation,\nwe are still",
    "start": "5947400",
    "end": "5952500"
  },
  {
    "text": "left with the\nexpectations, right? So the gradients still have an\nexpectation from left integer",
    "start": "5952500",
    "end": "5963060"
  },
  {
    "text": "which, in general,\ncan be a problem.",
    "start": "5963060",
    "end": "5968870"
  },
  {
    "text": "But what is done in practice\nis that this expectation is approximated with\nMonte Carlo, which",
    "start": "5968870",
    "end": "5976010"
  },
  {
    "text": "means we take a\nsample zi from Qi and construct and\ncalculate the--",
    "start": "5976010",
    "end": "5983550"
  },
  {
    "text": " and use that as the input for\nthis encoder neural network",
    "start": "5983550",
    "end": "5991790"
  },
  {
    "text": "to perform backpropagation. And backpropagation is\nperformed with respect to theta. And the output of\nthe neural network",
    "start": "5991790",
    "end": "5999969"
  },
  {
    "text": "is trained to be xi's, right?",
    "start": "5999970",
    "end": "6005220"
  },
  {
    "text": "So you can think\nof this as the--",
    "start": "6005220",
    "end": "6010590"
  },
  {
    "text": "sorry, this should\nbe the decoder. I'm sorry about that.",
    "start": "6010590",
    "end": "6016050"
  },
  {
    "text": "This is the P of z given x,\nso this should be the decoder.",
    "start": "6016050",
    "end": "6022385"
  },
  {
    "text": "And then the decoder,\nwe are fed the input zi, and the output should be xi.",
    "start": "6022385",
    "end": "6028670"
  },
  {
    "text": "And the xi over here--",
    "start": "6028670",
    "end": "6037690"
  },
  {
    "text": "instead of g, let's give\nit a different name. ",
    "start": "6037690",
    "end": "6044580"
  },
  {
    "text": "So what does the\nnotes use for g? So, all right, so the\nnote uses g for decoder. So, OK, so g is fine.",
    "start": "6044580",
    "end": "6051719"
  },
  {
    "text": "So this has-- the loss\nfunction over here",
    "start": "6051720",
    "end": "6057990"
  },
  {
    "text": "is the maximum\nlikelihood objective. And the maximum\nlikelihood objective is just the\nmultivariate Gaussian,",
    "start": "6057990",
    "end": "6064390"
  },
  {
    "text": "and that gives us the loss. And this is basically\nthe gradient",
    "start": "6064390",
    "end": "6070620"
  },
  {
    "text": "of the loss of this multivariate\nGaussian, whose parameters are",
    "start": "6070620",
    "end": "6078090"
  },
  {
    "text": "theta-- whose parameters are the-- so let me just\nwrite this clearly.",
    "start": "6078090",
    "end": "6084670"
  },
  {
    "text": "So you can think of this\nas a neural network, where the inputs are z's, and\nthe output is x hat.",
    "start": "6084670",
    "end": "6096570"
  },
  {
    "text": "And this x hat is going to be\ncompared against x's, right?",
    "start": "6096570",
    "end": "6107699"
  },
  {
    "text": "So this becomes the mean\nof the distribution, and x is going to\nbe the observation.",
    "start": "6107700",
    "end": "6112739"
  },
  {
    "text": "And we assume that the variance\nis some sigma squared I. And this, you plug it in\ninto the log probability",
    "start": "6112740",
    "end": "6120030"
  },
  {
    "text": "of xi parameterized by the\nmu that comes out of g of zi",
    "start": "6120030",
    "end": "6130800"
  },
  {
    "text": "and the variance of\nsigma squared I, right? So this log likelihood, or\nthe negative log likelihood,",
    "start": "6130800",
    "end": "6138150"
  },
  {
    "text": "becomes the loss. And the input becomes the\nzi that is sampled from Q.",
    "start": "6138150",
    "end": "6145134"
  },
  {
    "text": "And you feed that zi. You get a corresponding mean.",
    "start": "6145134",
    "end": "6150390"
  },
  {
    "text": "And you assume a\nconstant variance, and you plug it in into\na multivariate Gaussian",
    "start": "6150390",
    "end": "6159300"
  },
  {
    "text": "likelihood objective. And here, P will be 1\nover 2 pi to the d by 2",
    "start": "6159300",
    "end": "6165870"
  },
  {
    "text": "and the whole thing, right? So that's how things\nwork on the decoder side,",
    "start": "6165870",
    "end": "6173910"
  },
  {
    "text": "where zi's are sampled from Qi. But what are Qi here?",
    "start": "6173910",
    "end": "6180720"
  },
  {
    "text": "Qi is basically the output\nof our encoder network.",
    "start": "6180720",
    "end": "6189280"
  },
  {
    "text": "So this was the decoder network. The gradient with\nrespect to theta was for the decoder\npart of the network.",
    "start": "6189280",
    "end": "6197579"
  },
  {
    "text": "And the picture\nof the two working together is something like this. So let me do it.",
    "start": "6197580",
    "end": "6203610"
  },
  {
    "start": "6203610",
    "end": "6213449"
  },
  {
    "text": "Right? ",
    "start": "6213450",
    "end": "6219730"
  },
  {
    "text": "z is latent, right?",
    "start": "6219730",
    "end": "6224850"
  },
  {
    "text": "x is observed.  Now, the encoder\ntakes us from x to z.",
    "start": "6224850",
    "end": "6232020"
  },
  {
    "text": " And the decoder takes\nus from z to x, right?",
    "start": "6232020",
    "end": "6242150"
  },
  {
    "text": "So in the variational\nautoencoder, we are using neural\nnetworks here and here.",
    "start": "6242150",
    "end": "6248650"
  },
  {
    "text": "They are two different\nneural networks. This neural network has\nparameter phi and psi.",
    "start": "6248650",
    "end": "6256390"
  },
  {
    "text": "And this is being used as a\nreplacement for the E step.",
    "start": "6256390",
    "end": "6262002"
  },
  {
    "text": "And over here, the\ndecoder neural network has parameters theta. And this is being used kind\nof like the M step, right?",
    "start": "6262002",
    "end": "6270820"
  },
  {
    "text": "But we we're going to jointly\noptimize both these objectives rather than one at a time.",
    "start": "6270820",
    "end": "6277659"
  },
  {
    "text": "And in order to optimize them\nwith respect to-- and the way",
    "start": "6277660",
    "end": "6283270"
  },
  {
    "text": "we optimize them is by\nmaximizing the ELBO. So the ELBO has\nphi, psi, and theta.",
    "start": "6283270",
    "end": "6294850"
  },
  {
    "text": "And we want to calculate the\ngradient of this with respect to phi, psi, and\ntheta separately",
    "start": "6294850",
    "end": "6300610"
  },
  {
    "text": "and perform gradient descent. And in order to perform the-- in order to calculate\nthe gradients,",
    "start": "6300610",
    "end": "6307540"
  },
  {
    "text": "we saw this problem where the\ngradient with respect to theta was pretty easy\nbecause we could just",
    "start": "6307540",
    "end": "6313780"
  },
  {
    "text": "swap the expectation and\nthe gradient operator. So for the decoder,\nthis was easy.",
    "start": "6313780",
    "end": "6323590"
  },
  {
    "text": "We can just swap it. But for the encoder,\nwhere the expectation depended on phi and psi, we made\nuse of the reparameterization",
    "start": "6323590",
    "end": "6331480"
  },
  {
    "text": "trick, OK?",
    "start": "6331480",
    "end": "6339650"
  },
  {
    "text": "And after doing the\nreparameterization trick for both the gradients, we are\nstill left with an expectation,",
    "start": "6339650",
    "end": "6345960"
  },
  {
    "text": "though, right? We are still left\nwith an expectation. We were able to swap\nit, but we are still left with an expectation.",
    "start": "6345960",
    "end": "6351320"
  },
  {
    "text": "We do not eliminate\nthe expectation, right? And what is done in\npractice is those gradients",
    "start": "6351320",
    "end": "6359179"
  },
  {
    "text": "are approximated using\nMonte Carlo estimates. Those expectations\nof the gradients",
    "start": "6359180",
    "end": "6364370"
  },
  {
    "text": "are approximated with\nMonte Carlo expectations of those gradients.",
    "start": "6364370",
    "end": "6370220"
  },
  {
    "text": "And the way you think of it is--",
    "start": "6370220",
    "end": "6375560"
  },
  {
    "text": "so we have the x's, and\nyou have an encoder, which takes you down to z.",
    "start": "6375560",
    "end": "6384470"
  },
  {
    "text": "And from z, you have\nsomething that takes you back to x, right?",
    "start": "6384470",
    "end": "6391050"
  },
  {
    "text": "And in the encoder, we saw that\nthe encoder takes x as input.",
    "start": "6391050",
    "end": "6400829"
  },
  {
    "text": "And, in fact, it outputs two\ncomponents-- a mu component--",
    "start": "6400830",
    "end": "6406600"
  },
  {
    "text": "so it outputs a mu component\nand the sigma component as a diagonal matrix.",
    "start": "6406600",
    "end": "6413190"
  },
  {
    "text": "So the encoder takes\nx as input and outputs two sets of parameters. Now, these two sets\nof parameters are--",
    "start": "6413190",
    "end": "6423210"
  },
  {
    "text": "so this is xi. These two sets of parameters\ntogether define Qi, right?",
    "start": "6423210",
    "end": "6431340"
  },
  {
    "text": "That's the posterior. And from this, we are\nnow going to sample",
    "start": "6431340",
    "end": "6438300"
  },
  {
    "text": "a zi because the gradient\nhas this expectation.",
    "start": "6438300",
    "end": "6446590"
  },
  {
    "text": "And for that, we're\ngoing to sample the zi's from these parameters. And we're going to use the\nzi's as input for the decoder",
    "start": "6446590",
    "end": "6454500"
  },
  {
    "text": "network. So the encoder\nhas parameter phi. The decoder has parameter theta.",
    "start": "6454500",
    "end": "6460889"
  },
  {
    "text": "And we're going to take\nthese sample zi's and feed it as input to the decoder\nto get the recovered",
    "start": "6460890",
    "end": "6468540"
  },
  {
    "text": "or the reconstructed x's. And together, with both these\nencoder and decoder networks,",
    "start": "6468540",
    "end": "6477660"
  },
  {
    "text": "we are able to construct\nthe ELBO, right? So the ELBO was--",
    "start": "6477660",
    "end": "6484980"
  },
  {
    "text": "so the ELBO was over here. ",
    "start": "6484980",
    "end": "6493179"
  },
  {
    "text": "So the ELBO is over here. And this is P of x given\nz times P of z, right?",
    "start": "6493180",
    "end": "6500650"
  },
  {
    "text": "So this is the decoder\nneural network, right?",
    "start": "6500650",
    "end": "6507020"
  },
  {
    "text": "And everywhere\nwhere there is Qi, we have the encoder\nneural networks, right?",
    "start": "6507020",
    "end": "6512260"
  },
  {
    "text": "And the way we go about training\nthe variational autoencoder",
    "start": "6512260",
    "end": "6520750"
  },
  {
    "text": "is to minimize,\nmaximize that loss and-- maximize the ELBO, and we\ngo about maximizing the ELBO",
    "start": "6520750",
    "end": "6528340"
  },
  {
    "text": "by calculating the gradients and\ntaking gradient descent steps.",
    "start": "6528340",
    "end": "6533820"
  },
  {
    "text": "Yes, question? Can you use any\nkind of autoencoder to do expectation maximization\nwhere you calculate",
    "start": "6533820",
    "end": "6540893"
  },
  {
    "text": "the gradients properly? So I would say this is-- so the question is, can we\nuse any kind of neural network",
    "start": "6540894",
    "end": "6547160"
  },
  {
    "text": "to do expectation maximization? I would say, don't\nthink of this as doing expectation maximization.",
    "start": "6547160",
    "end": "6553400"
  },
  {
    "text": "This is an alternative to\nexpectation maximization. And expectation\nmaximization is defined as the coordinate\nascent approach, right?",
    "start": "6553400",
    "end": "6560720"
  },
  {
    "text": "So what we are\ninstead doing here is we are just trying to fit\na latent variable model where",
    "start": "6560720",
    "end": "6567230"
  },
  {
    "text": "the relation to EM is through\nthis coordinate ascent",
    "start": "6567230",
    "end": "6572300"
  },
  {
    "text": "versus gradient\ndescent interpretation. Also, is coordinate\nascent susceptible",
    "start": "6572300",
    "end": "6577810"
  },
  {
    "text": "to saddle points\nor local optima? So the question is\ncoordinate ascent susceptible to saddle\npoints or local optima?",
    "start": "6577810",
    "end": "6584469"
  },
  {
    "text": "Yes, they are. VAE is also susceptible to\nlocal optima, absolutely.",
    "start": "6584470",
    "end": "6589930"
  },
  {
    "text": "They are not convex problems. All right, so that's pretty much\nthe variational autoencoder.",
    "start": "6589930",
    "end": "6597670"
  },
  {
    "text": "Generally, because we are\nassuming a constant variance",
    "start": "6597670",
    "end": "6603330"
  },
  {
    "text": "over here, it\ngenerally works out that the encoder\nloss works out to be",
    "start": "6603330",
    "end": "6609480"
  },
  {
    "text": "just the squared error, the\nsquared norm between the two networks. So here, you used x\nhat minus xy, right?",
    "start": "6609480",
    "end": "6618690"
  },
  {
    "text": "If you work out\nthe math, this part works out to be just\nthe squared error. And these two-- the\nencoder does not",
    "start": "6618690",
    "end": "6626219"
  },
  {
    "text": "have a direct loss\non its own, right? So there is no supervision\nfor x's and sigmas.",
    "start": "6626220",
    "end": "6635699"
  },
  {
    "text": "Instead, the mu and sigma\nwill push these z values",
    "start": "6635700",
    "end": "6642540"
  },
  {
    "text": "to one particular\nlocation and concentration because we are sampling z's\nfrom this distribution, right?",
    "start": "6642540",
    "end": "6649680"
  },
  {
    "text": "And in case of-- you can think of the\nvariational autoencoder",
    "start": "6649680",
    "end": "6654990"
  },
  {
    "text": "to be exactly equal to a\nsimple autoencoder in the case",
    "start": "6654990",
    "end": "6660270"
  },
  {
    "text": "where the covariances\nare 0, where we use the mean itself as the z.",
    "start": "6660270",
    "end": "6668100"
  },
  {
    "text": "Whereas in the\nvariational autoencoder, we are outputting two sets\nof parameters-- a mean, which will tell you\napproximately where z will be,",
    "start": "6668100",
    "end": "6675750"
  },
  {
    "text": "and also a covariance,\nsuch that we sample-- we are effectively adding\nsome noise to the z",
    "start": "6675750",
    "end": "6683250"
  },
  {
    "text": "before we take it through\nthe decoder process. And it's motivated through\nthis ELBO maximization",
    "start": "6683250",
    "end": "6693960"
  },
  {
    "text": "kind of theory. But in practice,\nthe way we actually implement this is to implement\nit as a simple autoencoder,",
    "start": "6693960",
    "end": "6703980"
  },
  {
    "text": "except at the z layer,\nwe add some noise in the form of the epsilons\nthat we sample, right?",
    "start": "6703980",
    "end": "6712670"
  },
  {
    "text": "So that's variational\nautoencoders. And that pretty much\nwraps up our study",
    "start": "6712670",
    "end": "6721370"
  },
  {
    "text": "of unsupervised learning. And in the Friday\nlecture, we'll be kind of switching\ngears and looking",
    "start": "6721370",
    "end": "6728570"
  },
  {
    "text": "at evaluation metrics\nand other general tips on how to implement\nmachine learning projects.",
    "start": "6728570",
    "end": "6734580"
  },
  {
    "text": "And on Monday, we're\ngoing to just start the review of the full course,\nfocusing on parts that are",
    "start": "6734580",
    "end": "6739850"
  },
  {
    "text": "important for the final exam. All right, if there are any\nquestions regarding this,",
    "start": "6739850",
    "end": "6745190"
  },
  {
    "text": "feel free to walk up, and\nI'll be happy to answer them. ",
    "start": "6745190",
    "end": "6755000"
  }
]