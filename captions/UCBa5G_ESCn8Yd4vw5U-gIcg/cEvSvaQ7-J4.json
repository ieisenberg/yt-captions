[
  {
    "start": "0",
    "end": "9960"
  },
  {
    "text": "So thank you for\nbeing here today. I'm really excited to be\nable to share with you some of the work I've been\ndoing this past year",
    "start": "9960",
    "end": "17369"
  },
  {
    "text": "since I graduated at Stanford. And I want to begin and\nsay that we are living in a really exciting time.",
    "start": "17370",
    "end": "25289"
  },
  {
    "text": "Right now, we're just witnessing\nhow autonomous systems are really revolutionizing\nthe world we live in",
    "start": "25290",
    "end": "30779"
  },
  {
    "text": "as we speak today. And so as we see more\nrobots or autonomous systems",
    "start": "30780",
    "end": "36480"
  },
  {
    "text": "becoming more pervasive\nin our everyday life, we also find ourselves in\nsituations where, we as humans,",
    "start": "36480",
    "end": "42630"
  },
  {
    "text": "need to interact with robots. And we see this, of course,\nin autonomous driving.",
    "start": "42630",
    "end": "49050"
  },
  {
    "text": "We also see this in warehouse\nsettings and also at our homes-- inside our houses.",
    "start": "49050",
    "end": "54989"
  },
  {
    "text": "We have mobile robots that\nneed to navigate around people. We also see this in\nassistive technologies,",
    "start": "54990",
    "end": "60600"
  },
  {
    "text": "such as in wheelchairs. And also in search and rescue. If you ever get\nlost on a hike, you",
    "start": "60600",
    "end": "67800"
  },
  {
    "text": "might see that you\nwill have a UAV and flyover to try and search\nthe area and look for you.",
    "start": "67800",
    "end": "73170"
  },
  {
    "text": "And not only is some\nof those operations are done autonomously, but\nthey're also oftentimes",
    "start": "73170",
    "end": "79020"
  },
  {
    "text": "controlled by humans. And so you have humans also\ninteracting with autonomy through some interface.",
    "start": "79020",
    "end": "86790"
  },
  {
    "text": "So I just want to\nsay a few words about my research lab, which\nis the Control and Trustworthy",
    "start": "86790",
    "end": "92760"
  },
  {
    "text": "Robotics Lab. And we work on building\ntrustworthy autonomous systems that can operate\nseamlessly with, alongside,",
    "start": "92760",
    "end": "102119"
  },
  {
    "text": "and around humans. OK. And so this ranges all the\nway from assistive control,",
    "start": "102120",
    "end": "111720"
  },
  {
    "text": "also where you have\nsystems where a human might want to interact\nwith some interface",
    "start": "111720",
    "end": "118770"
  },
  {
    "text": "to then command an autonomous\nsystem to do a task remotely. And also includes navigation.",
    "start": "118770",
    "end": "124679"
  },
  {
    "text": "So this could be in\nterms of pedestrians, or even in autonomous driving.",
    "start": "124680",
    "end": "129990"
  },
  {
    "text": "And I'll talk a little\nbit more about that today. But just kind of zooming\nin a little bit more into the navigation setting.",
    "start": "129990",
    "end": "137730"
  },
  {
    "text": "I wanted to highlight some\nof the challenges that needs to be addressed when it\ncomes to building trustworthy",
    "start": "137730",
    "end": "143430"
  },
  {
    "text": "autonomous systems that can\noperate with, alongside, and around humans.",
    "start": "143430",
    "end": "148590"
  },
  {
    "text": "One of the big challenges is\nhuman behavior prediction.",
    "start": "148590",
    "end": "153629"
  },
  {
    "text": "So if there are humans\naround, an autonomous system, such as this mobile\nrobot here, needs to be able to predict\nhow humans behave.",
    "start": "153630",
    "end": "161970"
  },
  {
    "text": "So that's a pretty\nactive problem right now, and I have some work on\nthat I did during my PhD.",
    "start": "161970",
    "end": "171120"
  },
  {
    "text": "And then given that we have a\nprediction of how humans behave, then the robot needs\nto then make a plan,",
    "start": "171120",
    "end": "180150"
  },
  {
    "text": "figure out what a good\ntrajectory is that counts--",
    "start": "180150",
    "end": "185400"
  },
  {
    "text": "takes into account\nhow humans behave. So this is what I will refer to\nas interaction aware planning. Because what the robot does\nwill affect what the human does,",
    "start": "185400",
    "end": "194010"
  },
  {
    "text": "what the human does will\naffect what the robot does and so forth. So there's these kind of\ncomplex interaction dynamics",
    "start": "194010",
    "end": "199050"
  },
  {
    "text": "that needs to be accounted for. And then kind of as\nthis is all happening, we also need to make sure\nat the end of the day,",
    "start": "199050",
    "end": "206520"
  },
  {
    "text": "this needs to be done safely. And so there's this question\nof safe control, what",
    "start": "206520",
    "end": "212070"
  },
  {
    "text": "are methods and\ntechniques we can apply to ensure that the controls\nthat your system is making is safe and is not going\nto collide with humans.",
    "start": "212070",
    "end": "221370"
  },
  {
    "text": "And all this is assuming that\nthe robot knows what to do. It understands what task\nit needs to accomplish.",
    "start": "221370",
    "end": "229013"
  },
  {
    "text": "But then there's\nalso the question of how do you even formulate\nor specify that task.",
    "start": "229013",
    "end": "234299"
  },
  {
    "text": "And then finally, in these\nkind of real-world settings, there are certain\ncontextual information",
    "start": "234300",
    "end": "241769"
  },
  {
    "text": "that we should\nleverage, and that should provide some structure\ninto how we actually design",
    "start": "241770",
    "end": "249090"
  },
  {
    "text": "a lot of these algorithms. And so I just\nwanted to highlight some of the key\nchallenges that exist",
    "start": "249090",
    "end": "255340"
  },
  {
    "text": "and certainly some\nof the challenges that I am and my lab is\ninterested in looking at.",
    "start": "255340",
    "end": "261130"
  },
  {
    "text": "And so for today's talk, I want\nto share a perspective, which",
    "start": "261130",
    "end": "267610"
  },
  {
    "text": "is moving from a\ntask-centric perspective to a human-centric perspective.",
    "start": "267610",
    "end": "273460"
  },
  {
    "text": "And the idea is basically\nrather than treating humans, us, as a nuisance\nto the robot, we",
    "start": "273460",
    "end": "280630"
  },
  {
    "text": "want to start thinking\nabout how we the robot could start treating\nhumans as partners and actually work together\nto achieve their task,",
    "start": "280630",
    "end": "289930"
  },
  {
    "text": "whether they be the same task\nor their own respective task. And so this will be a common\ntheme throughout today's talk.",
    "start": "289930",
    "end": "297700"
  },
  {
    "text": "So we're treating humans not\nas nuisance, but as partners.",
    "start": "297700",
    "end": "303100"
  },
  {
    "text": "And I won't have time to talk\nabout all these challenges, and I'm really going\nto be just focusing",
    "start": "303100",
    "end": "308949"
  },
  {
    "text": "on these two, which is on\ninteraction aware planning and safe control.",
    "start": "308950",
    "end": "314449"
  },
  {
    "text": "OK. So that's the plan for today. And so I've\nstructured my talk so",
    "start": "314450",
    "end": "321760"
  },
  {
    "text": "that I want to start\nanswering these two questions. I have some ideas\nand present some work",
    "start": "321760",
    "end": "329229"
  },
  {
    "text": "where that provides potential\nsolutions to answering them, but I would say these are still\npretty much open problems that",
    "start": "329230",
    "end": "337600"
  },
  {
    "text": "is worthy of attention. And so the first\nhalf of my talk, I'll be talking about safe control.",
    "start": "337600",
    "end": "344620"
  },
  {
    "text": "And we're trying to\nanswer the question of, how close is too close?",
    "start": "344620",
    "end": "349750"
  },
  {
    "text": "When it comes to humans\nand robots interacting with each other, at what\npoint should your robot",
    "start": "349750",
    "end": "355210"
  },
  {
    "text": "decide that this is\nwhere I draw the line and I need to stop and\ntake some evasive control?",
    "start": "355210",
    "end": "361180"
  },
  {
    "text": "OK. If you go back to\nthe image over here,",
    "start": "361180",
    "end": "366400"
  },
  {
    "text": "I'll mention that this is\ntaken from the U-Dub campus. This is in the middle of campus. It's a very high-traffic area.",
    "start": "366400",
    "end": "375040"
  },
  {
    "text": "If we were to tell your robot\nkeep a 2-meter buffer away from",
    "start": "375040",
    "end": "380650"
  },
  {
    "text": "everyone, it's not going to be\nable to get across the square. So it needs to be able\nto understand how humans",
    "start": "380650",
    "end": "387849"
  },
  {
    "text": "are behaving and understand\nmaybe the human isn't out to get it, to chase it\ndown and kick it or something.",
    "start": "387850",
    "end": "396130"
  },
  {
    "text": "And so the question is,\nwhere do we draw the line? How close is too close?",
    "start": "396130",
    "end": "401290"
  },
  {
    "text": "And then the second\nquestion, which is the second part of my\ntalk about interaction aware planning, how do we\nmake interactions",
    "start": "401290",
    "end": "409330"
  },
  {
    "text": "natural and seamless without\nexplicit communication? And again, I'll talk\nmore about this later,",
    "start": "409330",
    "end": "415820"
  },
  {
    "text": "but it's quite amazing\nhow we as humans are able to navigate really busy\nintersections without colliding",
    "start": "415820",
    "end": "421960"
  },
  {
    "text": "into one another, yet we're\nnot announcing what we're doing to everyone around us. So how is that possible?",
    "start": "421960",
    "end": "428810"
  },
  {
    "text": "OK. So to start off, let's\ntalk about safe control.",
    "start": "428810",
    "end": "434169"
  },
  {
    "text": "So in this driving\nscenario, imagine you are the control engineer\nfor this autonomous car.",
    "start": "434170",
    "end": "441880"
  },
  {
    "text": "And the cars in this\nsituation they're going to merge\ninto the same lane.",
    "start": "441880",
    "end": "447310"
  },
  {
    "text": "So should the autonomous car\nbrake, to brake or not to brake? ",
    "start": "447310",
    "end": "454070"
  },
  {
    "text": "I'm going to give the\nunsatisfying answer, and say that it depends.",
    "start": "454070",
    "end": "459710"
  },
  {
    "text": "It depends on what you consider\nas a reasonable assumption",
    "start": "459710",
    "end": "465289"
  },
  {
    "text": "on how the other driver,\nthe human driver, is going to behave. So we can make an\nassumption and say,",
    "start": "465290",
    "end": "471050"
  },
  {
    "text": "let's assume both cars can do\nabsolutely anything possible. And if you project those\nassumptions forward in time",
    "start": "471050",
    "end": "478910"
  },
  {
    "text": "and see if these sets overlap,\nthen we can say it is unsafe, and therefore, the autonomous\ncar should slow down.",
    "start": "478910",
    "end": "487090"
  },
  {
    "text": "But if you make a different\nset of assumptions, say, if we assume that in such\nan emergency scenario,",
    "start": "487090",
    "end": "492280"
  },
  {
    "text": "both cars will\nbrake and slow down, and you see that if\nyou simulate forward and that these sets\ndo not intersect,",
    "start": "492280",
    "end": "499780"
  },
  {
    "text": "then we say the\nsituation is safe and the autonomous car should\ncontinue doing its thing.",
    "start": "499780",
    "end": "506349"
  },
  {
    "text": "But then if you make\ndifferent other assumptions, the answer might be yes\nor no, or the answer",
    "start": "506350",
    "end": "512140"
  },
  {
    "text": "might be brake or not brake. But this really boils\ndown to what constitutes as reasonable behavior.",
    "start": "512140",
    "end": "519490"
  },
  {
    "text": "Unfortunately,\ndifferent stakeholders make very different assumptions.",
    "start": "519490",
    "end": "524710"
  },
  {
    "text": "Depending on who\nyou talk to, they choose different assumptions,\nand therefore everyone",
    "start": "524710",
    "end": "531580"
  },
  {
    "text": "has different definition\nof safety in that sense. And so I actually\nkind of looked back",
    "start": "531580",
    "end": "538269"
  },
  {
    "text": "on kind of the safe\ncontrol literature and saw that there were\nmany different approaches",
    "start": "538270",
    "end": "543400"
  },
  {
    "text": "to coming up with safe\ncontrol strategies. And you might actually\nhave seen some of them.",
    "start": "543400",
    "end": "548720"
  },
  {
    "text": "There's velocity obstacle. Something known as contingency\nplanning and making sure",
    "start": "548720",
    "end": "553880"
  },
  {
    "text": "you have a backup maneuver\nin case of an emergency. Some of my own work on\nHamilton-Jacobi reachability.",
    "start": "553880",
    "end": "561920"
  },
  {
    "text": "You might have seen-- heard of forward reachable sets. And there's also some other\nconcepts from industry.",
    "start": "561920",
    "end": "568130"
  },
  {
    "text": "So from NVIDIA, they have this\nthing called safety force field, and another one from MobileEye\ncalled responsibility",
    "start": "568130",
    "end": "574610"
  },
  {
    "text": "sensitive safety. So there's all these different\nnames for different safe control strategies.",
    "start": "574610",
    "end": "579770"
  },
  {
    "text": "And I was a little overwhelmed. I was like, well, there's all\nthese different methods, which one is the right one to pick?",
    "start": "579770",
    "end": "585470"
  },
  {
    "text": "So I took a step back\nand tried to think what was common to all of them,\nand I came up with a term--",
    "start": "585470",
    "end": "593360"
  },
  {
    "text": "something I made up-- it's a very creative name and\nit's just called safety concept. And a safety concept\nI decided was--",
    "start": "593360",
    "end": "601640"
  },
  {
    "text": "it would tell us two things. The first was it would tell\nus how safe a situation was.",
    "start": "601640",
    "end": "607759"
  },
  {
    "text": "And the second, it\nwould also tell us what is safe or unsafe controls\nyour system should take.",
    "start": "607760",
    "end": "615370"
  },
  {
    "text": "So the input to-- so you can think of\nthese as two functions. So the input to these\nfunctions is the world state.",
    "start": "615370",
    "end": "622149"
  },
  {
    "text": "So just some information about\nthe environment and the world. And then it should give\nus a measure of safety,",
    "start": "622150",
    "end": "627940"
  },
  {
    "text": "so potentially just\na single number. If it's positive,\nit means we're safe. If it's negative, it\nmeans we're unsafe.",
    "start": "627940",
    "end": "633700"
  },
  {
    "text": "And the magnitude\nideally would tell us the severity or the magnitude of\nthat safe or unsafe situation.",
    "start": "633700",
    "end": "640720"
  },
  {
    "text": "And then it should also tell\nus, is accelerating bad, or is turning to the right good?",
    "start": "640720",
    "end": "647860"
  },
  {
    "text": "And kind of show us what a\nsafe and unsafe things to do. OK.",
    "start": "647860",
    "end": "652890"
  },
  {
    "text": "And it turns out all these\ndifferent control strategies more or less fit under this\nsafety concept definition.",
    "start": "652890",
    "end": "660699"
  },
  {
    "text": "And then I also then\nlooked back onto what I did with Hamilton-Jacobi\nreachability.",
    "start": "660700",
    "end": "667660"
  },
  {
    "text": "So just as a quick\nintro to this, Hamilton-Jacobi reachability\nis this really nice",
    "start": "667660",
    "end": "675850"
  },
  {
    "text": "mathematical framework\nthat captures closed-loop interactions\nbetween a human and a robot or two agents.",
    "start": "675850",
    "end": "681760"
  },
  {
    "text": "And so if you are familiar\nwith optimal control or taking Professor Pavone's\ntwo or three course,",
    "start": "681760",
    "end": "689950"
  },
  {
    "text": "or I don't know if\nhe's teaching it, but there's a two\nor three course. You will also learn\nabout optimal control",
    "start": "689950",
    "end": "698500"
  },
  {
    "text": "and Hamilton-Jacobi-Bellman\nequation. And this equation here, the\nHamilton-Jacobi-Isaacs equation",
    "start": "698500",
    "end": "707020"
  },
  {
    "text": "is really similar,\nexcept there's an additional term which is\nthe disturbance term, which we treat as the human.",
    "start": "707020",
    "end": "713200"
  },
  {
    "text": "So we treat a human as a\ndisturbance to your system and you want to make\nsure that your robot is",
    "start": "713200",
    "end": "720230"
  },
  {
    "text": "robust against what\nthe human does.",
    "start": "720230",
    "end": "725690"
  },
  {
    "text": "So we have this really\nnice mathematical framework to describe these\nclosed-loop interactions.",
    "start": "725690",
    "end": "732140"
  },
  {
    "text": "And so it turns out\nthat the solution to this partial\ndifferential equation V,",
    "start": "732140",
    "end": "739580"
  },
  {
    "text": "we can treat this as\na measure of safety. And then we have\nthis min operator.",
    "start": "739580",
    "end": "745610"
  },
  {
    "text": "So as I mentioned, d\nis for disturbance. So we treat the human\nas a disturbance.",
    "start": "745610",
    "end": "750800"
  },
  {
    "text": "And to be robust\nagainst the worst case, we treat the human as someone\nwho was seeking collision.",
    "start": "750800",
    "end": "758570"
  },
  {
    "text": "And then we also want the robot\nto maximize safety and avoid collision.",
    "start": "758570",
    "end": "763820"
  },
  {
    "text": "So that's how you\ninterpret this equation, and the idea is that if\nwe can solve this pde",
    "start": "763820",
    "end": "768890"
  },
  {
    "text": "and get the solution V, then we\nhave this function that tells us",
    "start": "768890",
    "end": "774320"
  },
  {
    "text": "how safe or unsafe\nthe situation is. OK. And this is actually a\nvery general framework",
    "start": "774320",
    "end": "782230"
  },
  {
    "text": "and it turns out, depending\non the assumptions you make about what the\nrobot and human does,",
    "start": "782230",
    "end": "788720"
  },
  {
    "text": "so what these sets\ncapital U and D are, we can actually describe\nnot just one safety concept,",
    "start": "788720",
    "end": "795190"
  },
  {
    "text": "but a family of safety concept. By changing our choice of\ncapital U and capital D,",
    "start": "795190",
    "end": "801070"
  },
  {
    "text": "we can actually describe many\ndifferent safety concepts, even the ones I showed on\nthe previous slide.",
    "start": "801070",
    "end": "807310"
  },
  {
    "text": "So we can actually recover\nvelocity obstacles, contingency planning, and\nforward reachable sets.",
    "start": "807310",
    "end": "814990"
  },
  {
    "text": "So that's really-- so\nthat's really nice. So now we have some\nmathematical framework to synthesize a safety concept.",
    "start": "814990",
    "end": "822550"
  },
  {
    "text": "But this still doesn't\nanswer the question of what are reasonable\nassumptions to make about how other agents behave.",
    "start": "822550",
    "end": "829370"
  },
  {
    "text": "OK. And this was something\nI saw in my own work in the past where if we\nassume that both the human",
    "start": "829370",
    "end": "835900"
  },
  {
    "text": "and the robot could do\nabsolutely anything possible, they could accelerate\nor slam on the brakes",
    "start": "835900",
    "end": "841149"
  },
  {
    "text": "if we're talking about a car\nor and swerve really hard left or right, then these\nsets that we get",
    "start": "841150",
    "end": "848709"
  },
  {
    "text": "or what we consider as\nan unsafe state turns out to be impractical.",
    "start": "848710",
    "end": "854710"
  },
  {
    "text": "And so the way you would\ninterpret this picture here is that suppose the\nautonomous car is red",
    "start": "854710",
    "end": "860470"
  },
  {
    "text": "and the human-driven car is in\ngreen, if the green car enters this bubble, the red\ncar thinks it's unsafe.",
    "start": "860470",
    "end": "867700"
  },
  {
    "text": "But you can see this\nbubble is quite wide. It's so wide that it\nthinks the car is unsafe",
    "start": "867700",
    "end": "873730"
  },
  {
    "text": "if it's just in the next lane. And we know from how we drive on\nthe road that that's not true.",
    "start": "873730",
    "end": "879500"
  },
  {
    "text": "Oftentimes when\nwe're driving, we're totally OK with having\na car right next to us in the next lane\nbecause there is",
    "start": "879500",
    "end": "885649"
  },
  {
    "text": "some assumption that\nthis other car is not going to just really swerve into\nyou, maybe 99.99% of the time.",
    "start": "885650",
    "end": "896210"
  },
  {
    "text": "OK. And so yeah. So really the\nquestion then becomes, how do we pick a reasonable\nchoice for U and D?",
    "start": "896210",
    "end": "905070"
  },
  {
    "text": "And surprise, surprise, we\ncan learn this from data. All right.",
    "start": "905070",
    "end": "910560"
  },
  {
    "text": "So if we have some data\nset of how humans interact with each other,\nhopefully we can start to recover what these\nsets U and D should be.",
    "start": "910560",
    "end": "919380"
  },
  {
    "text": "And so that's precisely\nwhat was done here. So the idea is that suppose\nwe have a data set of all",
    "start": "919380",
    "end": "926730"
  },
  {
    "text": "these states and controls,\nthen we want to learn a set U that's a function of x.",
    "start": "926730",
    "end": "934410"
  },
  {
    "text": "So basically, we want to learn\na state-dependent control set. And so here I'm going to\nvisualize what I mean.",
    "start": "934410",
    "end": "941279"
  },
  {
    "text": "So suppose for a particular\nstate or a situation,",
    "start": "941280",
    "end": "946560"
  },
  {
    "text": "we observed what\nhumans did, and this is the type of controls they\nwould do in that situation.",
    "start": "946560",
    "end": "953130"
  },
  {
    "text": "And then what we want\nto do is learn a set that contains these points.",
    "start": "953130",
    "end": "959190"
  },
  {
    "text": "So we want to learn U of x. ",
    "start": "959190",
    "end": "964920"
  },
  {
    "text": "But then there's also\na question of, well, here I've chosen an\nellipse or an oval,",
    "start": "964920",
    "end": "970290"
  },
  {
    "text": "but I could have\nalso chosen a bigger set that also contained these\npoints, or even just set the--",
    "start": "970290",
    "end": "976740"
  },
  {
    "text": "I should choose everything. If I chose a rectangle that\ncovered the whole U1 and U2",
    "start": "976740",
    "end": "983160"
  },
  {
    "text": "space, it would still\ncontain those points. So that's still a set that\ncontains those crosses.",
    "start": "983160",
    "end": "989670"
  },
  {
    "text": "But one of the\ninsights we made here was that humans tend to take\ncontrols that keep them safe.",
    "start": "989670",
    "end": "997410"
  },
  {
    "text": "So these crosses here, we can\nassume these are safe controls. We're not going to deliberately\ncause a collision, hopefully.",
    "start": "997410",
    "end": "1008570"
  },
  {
    "text": "But this also meant that the\nthings that we don't observe, things that don't have crosses,\nimply that they are unsafe.",
    "start": "1008570",
    "end": "1017060"
  },
  {
    "text": "So there's a reason why we\nobserve what we observe, but also a reason why\nwe don't observe what we",
    "start": "1017060",
    "end": "1022670"
  },
  {
    "text": "don't observe in the data set. And so this is just\na fancy way to say that assuming our data set\ncontains safe interactions,",
    "start": "1022670",
    "end": "1031910"
  },
  {
    "text": "so there are no collisions,\nall the points in there live inside a control\ninvariant set.",
    "start": "1031910",
    "end": "1039109"
  },
  {
    "text": "And so this learning\nproblem becomes learning a control\ninvariant set from data.",
    "start": "1039109",
    "end": "1045679"
  },
  {
    "text": "So a control\ninvariant set is a set of states where your dynamical\nsystem can stay inside of it",
    "start": "1045680",
    "end": "1052190"
  },
  {
    "text": "indefinitely. And it turns out we can\nrepresent control invariant",
    "start": "1052190",
    "end": "1058220"
  },
  {
    "text": "sets using something known as\na control barrier function. And I'm going to use this slide\nto quickly kind of introduce",
    "start": "1058220",
    "end": "1066601"
  },
  {
    "text": "what a control\nbarrier function is, and conceptually\nit's quite elegant.",
    "start": "1066602",
    "end": "1073070"
  },
  {
    "text": "So what we can do is start\noff with an unsafe region that we want to stay out of. And then we can come\nup with a function b.",
    "start": "1073070",
    "end": "1082440"
  },
  {
    "text": "It's a scalar-valued\nfunction that takes in state and spits out a single number. And b should be\nchosen such that it",
    "start": "1082440",
    "end": "1089010"
  },
  {
    "text": "is negative inside the\nregion and positive outside. ",
    "start": "1089010",
    "end": "1095190"
  },
  {
    "text": "And then the control\nbarrier function kind of framework formulation\nsays we want this inequality",
    "start": "1095190",
    "end": "1103330"
  },
  {
    "text": "to be true for all our states. And this is just\nsaying that we are",
    "start": "1103330",
    "end": "1108910"
  },
  {
    "text": "bounding the rate at which the\nsystem approaches the boundary. So if we were to\nstart outside, we",
    "start": "1108910",
    "end": "1117519"
  },
  {
    "text": "can move however\nwe want, as long as the rate at which\nb decreases does not",
    "start": "1117520",
    "end": "1123640"
  },
  {
    "text": "exceed some certain threshold. So it's saying if we're far\naway from the unsafe region,",
    "start": "1123640",
    "end": "1129700"
  },
  {
    "text": "we have a lot of freedom\nto do what we want. If I'm far away from a wall,\nI could run towards the wall",
    "start": "1129700",
    "end": "1135700"
  },
  {
    "text": "if I wanted to. But the closer I get to the\nunsafe region, say this wall, I need to start slowing\ndown to the point",
    "start": "1135700",
    "end": "1142360"
  },
  {
    "text": "where when I'm at the wall, I\nneed to either stop, move along the wall, or start moving\naway from the wall.",
    "start": "1142360",
    "end": "1150340"
  },
  {
    "text": "So that's the idea of a\ncontrol barrier function, and they are used to\ndescribe or represent",
    "start": "1150340",
    "end": "1155890"
  },
  {
    "text": "control invariant sets. And so this becomes, a CBF,\nor a Control Barrier Function,",
    "start": "1155890",
    "end": "1161150"
  },
  {
    "text": "learning problem. And what is also\nreally nice about CBFs is that we can also\nfind a control set, u,",
    "start": "1161150",
    "end": "1170120"
  },
  {
    "text": "that satisfy this inequality. And so this is how\nwe parameterize this state-dependent\ncontrol set.",
    "start": "1170120",
    "end": "1178450"
  },
  {
    "text": "Yep, state-dependent\ncontrol set.  And here in the work\nI have cited here",
    "start": "1178450",
    "end": "1188170"
  },
  {
    "text": "we are trying to\nlearn parameters of this alpha function. And this actually turns out\nto be a very simple learning",
    "start": "1188170",
    "end": "1195070"
  },
  {
    "text": "problem and we can just do\nthis through gradient descent. So nothing really fancy is\nhappening under the hood here.",
    "start": "1195070",
    "end": "1202370"
  },
  {
    "text": "OK. So suppose we do this. We have data. We learn this\nstate-dependent control",
    "start": "1202370",
    "end": "1207850"
  },
  {
    "text": "set by using control barrier\nfunctions as an inductive bias, or a way to parameterize it.",
    "start": "1207850",
    "end": "1215830"
  },
  {
    "text": "We can then plug it back into\nthis Hamilton-Jacobi-Isaacs partial differential\nequation and solve that pde.",
    "start": "1215830",
    "end": "1223059"
  },
  {
    "text": "Here I'm skipping\nover some details. It's not as simple as just\nkind of slapping it on. It does make the problem\na little bit more",
    "start": "1223060",
    "end": "1231250"
  },
  {
    "text": "difficult to solve. But this becomes-- if you're\ninterested in game theory, this becomes a constraint\nto play a game.",
    "start": "1231250",
    "end": "1241090"
  },
  {
    "text": "OK. So we got some data, actually\nfrom some of my previous work",
    "start": "1241090",
    "end": "1246539"
  },
  {
    "text": "as well, where this is a\nreally nice data set that contains interesting\ninteractions because what we did",
    "start": "1246540",
    "end": "1253620"
  },
  {
    "text": "was we had two people\non a driving simulator and we asked them to swap\nlanes in a short amount of time",
    "start": "1253620",
    "end": "1260610"
  },
  {
    "text": "and distance. And so this is\nsomething I think we all have experienced as we're trying\nto merge onto the highway.",
    "start": "1260610",
    "end": "1266160"
  },
  {
    "text": "I think the 101\nexit in and out has a very short distance for you\nto actually merge onto the 101.",
    "start": "1266160",
    "end": "1275970"
  },
  {
    "text": "So this is exactly what\nwe're trying to emulate here. And these are the\nsets that we get",
    "start": "1275970",
    "end": "1282690"
  },
  {
    "text": "from applying this framework. So in the gray is the one\nwhere we assume the worst case,",
    "start": "1282690",
    "end": "1290310"
  },
  {
    "text": "so we assume both agents\ncan do anything possible, but you can see that this\nleads to very wide sets.",
    "start": "1290310",
    "end": "1296850"
  },
  {
    "text": "In the red, we assume that both\ncars are just going to break. So this is a pretty\noptimistic assumption,",
    "start": "1296850",
    "end": "1303820"
  },
  {
    "text": "and this leads into these sets\nthat are very long and skinny. But applying this learning\napproach, the set we get",
    "start": "1303820",
    "end": "1310690"
  },
  {
    "text": "is somewhere in between. It still has a little bit of\nwidth because cars can steer,",
    "start": "1310690",
    "end": "1320980"
  },
  {
    "text": "but maybe they won't\nsee a maximally when they're driving on the road.",
    "start": "1320980",
    "end": "1326770"
  },
  {
    "text": "And so here we can see\nthat by leveraging data, we can synthesize these\nnovel safety concepts.",
    "start": "1326770",
    "end": "1335830"
  },
  {
    "text": "And that is maybe not as extreme\nas the worst-case setting",
    "start": "1335830",
    "end": "1342580"
  },
  {
    "text": "or the optimistic setting\nshown in red there. OK.",
    "start": "1342580",
    "end": "1348500"
  },
  {
    "text": "So there's that and\nthen I also wanted to share some other\nrecent work where we're using control barrier\nfunctions to learn safety",
    "start": "1348500",
    "end": "1357139"
  },
  {
    "text": "for multiple agents. So we extended this to\nthe multi-agent setting.",
    "start": "1357140",
    "end": "1365490"
  },
  {
    "text": "But again, in a\ndriving context, it's really interesting\nbecause even though there",
    "start": "1365490",
    "end": "1371640"
  },
  {
    "text": "are more agents, more\ncars on the road, depending on where\nyou are on the road,",
    "start": "1371640",
    "end": "1377309"
  },
  {
    "text": "you are actually more or less\nresponsible for maintaining safety. And so what I mean by\nthat is depending on maybe",
    "start": "1377310",
    "end": "1385260"
  },
  {
    "text": "you're on a lane where\nyou have to merge onto fast-moving traffic, so you\nas the person trying to merge",
    "start": "1385260",
    "end": "1391500"
  },
  {
    "text": "are actually more\nresponsible to avoid cutting in front of someone,\nwhereas if you're already",
    "start": "1391500",
    "end": "1399000"
  },
  {
    "text": "on the main road you\njust continue moving and you're not really expected\nto stop and let someone in.",
    "start": "1399000",
    "end": "1406995"
  },
  {
    "text": " So yeah. And so also when you're driving,\nyou are expected to not crash",
    "start": "1406995",
    "end": "1415380"
  },
  {
    "text": "into the person in front\nof you, but you also expect the person behind\nyou to not crash into you,",
    "start": "1415380",
    "end": "1421060"
  },
  {
    "text": "but you're not fully responsible\nfor avoiding collision with the car behind. But you should brake\ncheck in general.",
    "start": "1421060",
    "end": "1429960"
  },
  {
    "text": "OK. So again, we wanted\nto capture this-- ",
    "start": "1429960",
    "end": "1436350"
  },
  {
    "text": "we wanted to understand how\nresponsibility is allocated across multiple agents.",
    "start": "1436350",
    "end": "1443610"
  },
  {
    "text": "And this should be a\nfunction for how many people are on the road, what the road\ngeometry is like, and also",
    "start": "1443610",
    "end": "1450990"
  },
  {
    "text": "traffic rules. And long story short, we used\nto control barrier function",
    "start": "1450990",
    "end": "1458350"
  },
  {
    "text": "to represent safety here, but\nwhat we did was we added this extra term \"gamma,\" and we\ncalled it the responsibility",
    "start": "1458350",
    "end": "1465820"
  },
  {
    "text": "allocation function. And you can think of this\nas basically an offset to this inequality to make it\neasier or harder to satisfy.",
    "start": "1465820",
    "end": "1473980"
  },
  {
    "text": " And we were able to\nlearn gamma from data.",
    "start": "1473980",
    "end": "1480059"
  },
  {
    "text": "So we had all these data\nsets that were available, and we were able to\nlearn what the gamma was.",
    "start": "1480060",
    "end": "1486820"
  },
  {
    "text": "And this helped us to become-- do some forensic analysis\nafter an accident happens.",
    "start": "1486820",
    "end": "1494460"
  },
  {
    "text": "This probably might not\nhold in a courtroom, but this at least\ngives us some insight",
    "start": "1494460",
    "end": "1499980"
  },
  {
    "text": "into how we provide some\ninsight into how an accident--",
    "start": "1499980",
    "end": "1508360"
  },
  {
    "text": "it will explain how an\naccident could have happened. So we can see in\nthis situation, we have an orange and purple car.",
    "start": "1508360",
    "end": "1515429"
  },
  {
    "text": "And in this case, the\norange car pulled out, cut in front of the purple\ncar, and a collision happened.",
    "start": "1515430",
    "end": "1522539"
  },
  {
    "text": "And so you can see the\nacceleration and steering profile over time. And then based on\nour learned gamma,",
    "start": "1522540",
    "end": "1530130"
  },
  {
    "text": "we saw that earlier\non in the interaction, the orange car is responsible\nfor not cutting off",
    "start": "1530130",
    "end": "1536220"
  },
  {
    "text": "the purple car, which is\naligned with our intuition. If you were the orange\ncar, you probably",
    "start": "1536220",
    "end": "1542110"
  },
  {
    "text": "should have waited\nfor the purple car to pass before you turned. But that didn't\nhappen, the orange car",
    "start": "1542110",
    "end": "1548560"
  },
  {
    "text": "cut in front of the purple car. And then once the\norange car got in front,",
    "start": "1548560",
    "end": "1554680"
  },
  {
    "text": "the purple car should have\nslowed down, but it didn't. And you can see that for\na brief period of time,",
    "start": "1554680",
    "end": "1560200"
  },
  {
    "text": "the purple car was\nactually more responsible for maintaining safety.",
    "start": "1560200",
    "end": "1567039"
  },
  {
    "text": "And then here is a plot\nthat shows how much they violate that inequality. So you can see\naround this region,",
    "start": "1567040",
    "end": "1576760"
  },
  {
    "text": "the purple car violates\nthe safety constraint,",
    "start": "1576760",
    "end": "1582010"
  },
  {
    "text": "meaning they should have\nslowed down more than they did, and perhaps this was why\na collision happened.",
    "start": "1582010",
    "end": "1590380"
  },
  {
    "text": "OK. So yeah. So I want to wrap up the\nfirst half of my talk",
    "start": "1590380",
    "end": "1596240"
  },
  {
    "text": "and just kind of mention\nsome of ongoing challenges that we're still working on\nand maybe you should also start",
    "start": "1596240",
    "end": "1605750"
  },
  {
    "text": "thinking about. So the first one\nis data collection. So although in the context\nof autonomous driving",
    "start": "1605750",
    "end": "1613490"
  },
  {
    "text": "there's a lot of publicly\navailable data sets right now, I would argue that a lot of it\nis actually not that interesting",
    "start": "1613490",
    "end": "1621200"
  },
  {
    "text": "and that just nothing\nsafety-critical is happening in those situations. And so I think there can be a\nlot of effort and investigation",
    "start": "1621200",
    "end": "1630770"
  },
  {
    "text": "done to think about how\nwe can be more selective or how we can design\nexperiments to elicit",
    "start": "1630770",
    "end": "1637460"
  },
  {
    "text": "more interesting and\nsafety-critical scenarios, and use that to learn more\nnovel safety concepts.",
    "start": "1637460",
    "end": "1644600"
  },
  {
    "text": "And then we also have to do\nsome closed-loop testing, and actually put\nthem on a real car, have it interact with\nother humans, and see,",
    "start": "1644600",
    "end": "1653039"
  },
  {
    "text": "is there a noticeable\ndifference? And something I didn't really\ngo into but I briefly mentioned",
    "start": "1653040",
    "end": "1658080"
  },
  {
    "text": "was that solving this\nHamilton-Jacobi reachability problem suffers from the\ncurse of dimensionality.",
    "start": "1658080",
    "end": "1665250"
  },
  {
    "text": "I was able to do\nit for this work, just because the\nsystem I was looking at was relatively low dimensional.",
    "start": "1665250",
    "end": "1670920"
  },
  {
    "text": "But if we were to extend\nthis to higher dimensions, to more complicated\nsettings, we are quickly",
    "start": "1670920",
    "end": "1677550"
  },
  {
    "text": "faced by a brick wall in terms\nof making this tractable. So I think there could\nbe also some work done",
    "start": "1677550",
    "end": "1684600"
  },
  {
    "text": "to address those issues. OK. So now let's move on to the\nsecond half of the talk.",
    "start": "1684600",
    "end": "1692450"
  },
  {
    "text": "So this was on\ninteraction-aware planning. We want to answer\nthe question of how",
    "start": "1692450",
    "end": "1699530"
  },
  {
    "text": "to make interactions\nnatural and seamless without explicit communication. This is a picture in Japan,\nthe really famous crosswalk.",
    "start": "1699530",
    "end": "1709760"
  },
  {
    "text": "There's hundreds,\nthousands of people. And I bring this up because\nit is quite astounding",
    "start": "1709760",
    "end": "1718220"
  },
  {
    "text": "that people are able to\nnavigate past each other without colliding.",
    "start": "1718220",
    "end": "1723940"
  },
  {
    "text": "And I wanted to\nknow, why is that? Why is it that humans\nare able to do this",
    "start": "1723940",
    "end": "1729530"
  },
  {
    "text": "and what can we do to make\nsure that robots are also able to do this? Imagine trying to get\nyour food delivered",
    "start": "1729530",
    "end": "1736580"
  },
  {
    "text": "and having the robot navigate\nthrough this crosswalk to deliver your pizza?",
    "start": "1736580",
    "end": "1744290"
  },
  {
    "text": "And my hypothesis is that\nhumans are self-preserving,",
    "start": "1744290",
    "end": "1749910"
  },
  {
    "text": "so we don't actually\nwant to run into people. And if you are told\nhow to avoid collision,",
    "start": "1749910",
    "end": "1756240"
  },
  {
    "text": "then we're going to do\nit to avoid colliding. So humans are self-preserving\nand will engage",
    "start": "1756240",
    "end": "1763440"
  },
  {
    "text": "in joint collision avoidance. And so the natural question as\nsomeone working in robotics is,",
    "start": "1763440",
    "end": "1772120"
  },
  {
    "text": "can we achieve safe and fluent\nhuman-robot interactions by leveraging the fact that\nhumans are self-preserving?",
    "start": "1772120",
    "end": "1779409"
  },
  {
    "text": "And this is going back to\nthe slide I had earlier on, where we want to move from\na task-centric perspective",
    "start": "1779410",
    "end": "1787299"
  },
  {
    "text": "to a human-centric perspective. We want to stop treating\nhumans as a nuisance and start treating them\nas partners and as someone",
    "start": "1787300",
    "end": "1794410"
  },
  {
    "text": "who will actually help the\nrobot achieve their task. ",
    "start": "1794410",
    "end": "1799470"
  },
  {
    "text": "OK. And so in this work I'm going\nto present, the answer is yes, and we can actually do this\nthrough a very simple changes",
    "start": "1799470",
    "end": "1807740"
  },
  {
    "text": "to a somewhat standard\ntrajectory optimization problem.",
    "start": "1807740",
    "end": "1813260"
  },
  {
    "text": "And I want to also just\nmention trajectory optimization is a widely used method for\nrobot planning and control.",
    "start": "1813260",
    "end": "1822350"
  },
  {
    "text": "People have used this for many,\nmany different applications, from off-road,\naggressive driving,",
    "start": "1822350",
    "end": "1827840"
  },
  {
    "text": "to quadrotor control,\nto also human robot interaction, and even\nlanding and rocket landings.",
    "start": "1827840",
    "end": "1833760"
  },
  {
    "text": "OK. So trajectory optimization is\nvery commonly, commonly used.",
    "start": "1833760",
    "end": "1840180"
  },
  {
    "text": "OK. And so my claim is that\nthrough a standard trajectory optimization framework,\nwe make a few tweaks to it",
    "start": "1840180",
    "end": "1847610"
  },
  {
    "text": "and we're able to achieve\nsafe and fluent interactions. And these tweaks are\nguided by two observations.",
    "start": "1847610",
    "end": "1855320"
  },
  {
    "text": "And the first observation\nis that humans will cooperate if told how. So as I mentioned before,\nif someone is approaching me",
    "start": "1855320",
    "end": "1863660"
  },
  {
    "text": "and someone told me, hey,\nKaren, move to the right and you will avoid collision\nwith this other person,",
    "start": "1863660",
    "end": "1869600"
  },
  {
    "text": "I'm going to do it. There's no reason why\nI wouldn't do that. So if someone told me\nhow to avoid collision",
    "start": "1869600",
    "end": "1876350"
  },
  {
    "text": "and it made sense and was\nlogical, then I will do it. So how do we-- like, what\ndoes that mean for the robot?",
    "start": "1876350",
    "end": "1884820"
  },
  {
    "text": "So what that means for\nthe robot is two things. The first is legibility.",
    "start": "1884820",
    "end": "1890400"
  },
  {
    "text": "So the robot should\nmove in a legible way, and legibility means\nmotion that conveys intent.",
    "start": "1890400",
    "end": "1897120"
  },
  {
    "text": "So if the robot is moving in\na way that is quite intuitive and its motion corresponds\nto what it's trying to do,",
    "start": "1897120",
    "end": "1904200"
  },
  {
    "text": "then this will inform a human\nwho's watching this robot how to avoid collision.",
    "start": "1904200",
    "end": "1909630"
  },
  {
    "text": "So we want to incorporate\nsome legibility into the robot's behavior.",
    "start": "1909630",
    "end": "1915690"
  },
  {
    "text": "The second is proactivity. So not only do we\nneed to be legible, but we need to be proactive\nin conveying intent.",
    "start": "1915690",
    "end": "1924390"
  },
  {
    "text": "So we all like to be\nproactive in our lives in many, many aspects,\nbut this is just",
    "start": "1924390",
    "end": "1930809"
  },
  {
    "text": "saying we want to do things\nearly before a problem has actually occurred.",
    "start": "1930810",
    "end": "1936120"
  },
  {
    "text": "And so by the robot\nbeing proactive, this is essentially providing\na human the opportunity",
    "start": "1936120",
    "end": "1942150"
  },
  {
    "text": "to avoid collision. OK. So yeah. So the question we\nwant to tackle here",
    "start": "1942150",
    "end": "1948870"
  },
  {
    "text": "is, how do we encode legibility\nand proactivity into a robot planning algorithm?",
    "start": "1948870",
    "end": "1955360"
  },
  {
    "text": "The second observation\nwe make is humans are not completely selfless.",
    "start": "1955360",
    "end": "1961620"
  },
  {
    "text": "OK. So if you told me, hey, Karen,\nif you want to avoid colliding",
    "start": "1961620",
    "end": "1967590"
  },
  {
    "text": "with everyone in the room,\njust stand in the corner, wait till 1:30 until\neveryone's left,",
    "start": "1967590",
    "end": "1974830"
  },
  {
    "text": "and then walk out the room. OK. I'm not going to do that. I mean, I'm not\ncompletely selfless.",
    "start": "1974830",
    "end": "1980664"
  },
  {
    "text": " So we want to also leverage\nthe fact that we don't--",
    "start": "1980665",
    "end": "1988990"
  },
  {
    "text": "we want to be courteous,\nbut not too courteous. And the way we\nincorporate this idea",
    "start": "1988990",
    "end": "1995480"
  },
  {
    "text": "is to introduce this\nidea of inconvenience. So we can measure some\nlevel of inconvenience",
    "start": "1995480",
    "end": "2001810"
  },
  {
    "text": "that the robot is accumulating\nor also the human is accumulating, then\nwhat we can do",
    "start": "2001810",
    "end": "2007240"
  },
  {
    "text": "is apply a budget constraint\nto it, and say, as a robot,",
    "start": "2007240",
    "end": "2012760"
  },
  {
    "text": "or as a human, I understand\nyou're moving a little bit out of your way to avoid\ncollision, but we",
    "start": "2012760",
    "end": "2018400"
  },
  {
    "text": "want to make sure this isn't too\nmuch or beyond some threshold. OK.",
    "start": "2018400",
    "end": "2023460"
  },
  {
    "text": "And so this then leads\nto the second question, how do we limit the\namount of inconvenience",
    "start": "2023460",
    "end": "2029039"
  },
  {
    "text": "an individual experiences? OK. So here is a very general setup\nfor a trajectory optimization",
    "start": "2029040",
    "end": "2037780"
  },
  {
    "text": "problem. And what we propose\nin this work, and this is led by my\nstudent and undergraduate",
    "start": "2037780",
    "end": "2044470"
  },
  {
    "text": "at U-Dub, who we introduced\nkind of to two-ish things.",
    "start": "2044470",
    "end": "2050349"
  },
  {
    "text": "The first is this idea\nof a markup factor, and we add this term\nto the running cost.",
    "start": "2050350",
    "end": "2057070"
  },
  {
    "text": "And I'll talk a little\nbit more about that in a couple of slides. And then we also add\nthis inconvenience budget",
    "start": "2057070",
    "end": "2063550"
  },
  {
    "text": "as a constraint. And we say we do not want to\ninconvenience others too much. And we also don't want to\ninconvenience ourselves",
    "start": "2063550",
    "end": "2070510"
  },
  {
    "text": "as a robot too much either. But because we are adding\nan extra constraint,",
    "start": "2070510",
    "end": "2076509"
  },
  {
    "text": "this could make the\nproblem infeasible. And so we actually\nadd some slack",
    "start": "2076510",
    "end": "2081789"
  },
  {
    "text": "to the collision avoidance\nconstraint with other humans. And this might sound a little\ncounterproductive because we",
    "start": "2081790",
    "end": "2089469"
  },
  {
    "text": "want to make sure we don't hit\nhumans and by relaxing that constraint might\ndo the opposite,",
    "start": "2089469",
    "end": "2096290"
  },
  {
    "text": "but we'll show in some results\nis that this actually doesn't-- through simulation\ndoesn't actually",
    "start": "2096290",
    "end": "2101870"
  },
  {
    "text": "reduce the safety of our system. And we believe it's because of\nthis markup and inconvenience",
    "start": "2101870",
    "end": "2107450"
  },
  {
    "text": "term that we've added\nto the formulation. OK. So let me talk a little bit\nmore about the markup factor.",
    "start": "2107450",
    "end": "2114980"
  },
  {
    "text": "So the markup factor\nis chosen to-- mu here is greater than 1.",
    "start": "2114980",
    "end": "2121490"
  },
  {
    "text": "And it's basically selected\nto encourage the robot to take non-trivial\ncontrols earlier.",
    "start": "2121490",
    "end": "2128160"
  },
  {
    "text": "And so the idea is\nif I'm the robot and I want to\navoid this table, I",
    "start": "2128160",
    "end": "2133730"
  },
  {
    "text": "could avoid it by\ngoing up to the table, then turning at the last moment.",
    "start": "2133730",
    "end": "2139670"
  },
  {
    "text": "Alternatively, I\ncould have said, well, I want to\navoid this table, so let me start\nmoving away from it",
    "start": "2139670",
    "end": "2145370"
  },
  {
    "text": "early, and so that if someone\nelse is watching me do this, they'll understand that I\nam trying to avoid the table",
    "start": "2145370",
    "end": "2152330"
  },
  {
    "text": "and they will plan accordingly. And therefore, providing\nthem an opportunity to figure out what\nthey should do.",
    "start": "2152330",
    "end": "2159420"
  },
  {
    "text": "OK. So maybe the general lesson\nis like don't leave things to the last minute. ",
    "start": "2159420",
    "end": "2166640"
  },
  {
    "text": "OK. So just to visualize\nthe effect of this,",
    "start": "2166640",
    "end": "2174500"
  },
  {
    "text": "we looked at this\nhead-on scenario, where we have the robot and the human\napproaching each other, kind",
    "start": "2174500",
    "end": "2179540"
  },
  {
    "text": "of like a corridor problem. And then in this\nplot, we see that",
    "start": "2179540",
    "end": "2185809"
  },
  {
    "text": "over time, what\nwe're plotting here is the heading\nrate of the robot.",
    "start": "2185810",
    "end": "2191599"
  },
  {
    "text": "And you can see that for mu, if\nwe have a markup greater than 1, the robot turns\nearlier and faster.",
    "start": "2191600",
    "end": "2199760"
  },
  {
    "text": "Whereas if we choose a markup\nof 0.9, so basically a discount, the robot starts\nturning later and less.",
    "start": "2199760",
    "end": "2207560"
  },
  {
    "text": "And so this is indicating\nthat by the simple addition of this markup factor,\nwhich mu is greater than 1,",
    "start": "2207560",
    "end": "2214730"
  },
  {
    "text": "we have introduced proactivity. And the fact that you're\nturning earlier and faster",
    "start": "2214730",
    "end": "2222210"
  },
  {
    "text": "could mean that\nit is more legible and the motion is more legible.",
    "start": "2222210",
    "end": "2227460"
  },
  {
    "text": "It's showing, hey, I want\nto turn and move to the side to pass an oncoming agent.",
    "start": "2227460",
    "end": "2234819"
  },
  {
    "text": "OK. So yeah, basically,\nwith a higher markup,",
    "start": "2234820",
    "end": "2239860"
  },
  {
    "text": "we encourage faster\nand earlier rotation. And then here's another plot\nto show the effect of markup.",
    "start": "2239860",
    "end": "2248470"
  },
  {
    "text": "So again, this human and robot\nare approaching each other, say in a narrow corridor.",
    "start": "2248470",
    "end": "2253690"
  },
  {
    "text": "In the green here-- sorry, not green,\nin the blue, we are showing the\nheading of the robot.",
    "start": "2253690",
    "end": "2260560"
  },
  {
    "text": "And you can see in blue, which\nis our proposed framework, the heading is quite\nsmooth and consistent.",
    "start": "2260560",
    "end": "2267760"
  },
  {
    "text": "It's always turning left\nand passing the human.",
    "start": "2267760",
    "end": "2273250"
  },
  {
    "text": "But without this\nmarkup term, we see that the heading over\na number of simulations",
    "start": "2273250",
    "end": "2279910"
  },
  {
    "text": "oscillates back and\nforth and so this is corresponding to the case,\nthat kind of awkward situation",
    "start": "2279910",
    "end": "2286167"
  },
  {
    "text": "you come across when you're\ntrying to pass someone on a narrow corridor. And so yeah, just through this\nsimple addition of this markup,",
    "start": "2286167",
    "end": "2294790"
  },
  {
    "text": "we're able to get some really\nnice and smooth and confident behaviors, as opposed to\nthis oscillatory behavior.",
    "start": "2294790",
    "end": "2305390"
  },
  {
    "text": "OK. Now moving on to inconvenience. So as I said, inconvenience\nis the cost incurred--",
    "start": "2305390",
    "end": "2313210"
  },
  {
    "text": "so we define inconvenience\nas the cost incurred compared to an ideal trajectory. So here suppose there was\nno one in the environment",
    "start": "2313210",
    "end": "2321010"
  },
  {
    "text": "and we can compute what\nthe path the human-- the robot would have taken. But then there is\na human in the way,",
    "start": "2321010",
    "end": "2328090"
  },
  {
    "text": "and so the robot decides\nto do something else. And we want to figure\nout, or compute,",
    "start": "2328090",
    "end": "2333339"
  },
  {
    "text": "what is the difference in\nthe cost between these two trajectories, and this is what\nwe define as inconvenience.",
    "start": "2333340",
    "end": "2341200"
  },
  {
    "text": "And you're free-- you\ncan choose whatever function you want to choose.",
    "start": "2341200",
    "end": "2347050"
  },
  {
    "text": "At least in our work, we\nlooked at things like distance to the goal, total acceleration,\nand total distance.",
    "start": "2347050",
    "end": "2354220"
  },
  {
    "text": "OK. You can see I'm\nrunning out of time. So I'm going to-- let me just quickly go\nthrough these results.",
    "start": "2354220",
    "end": "2362529"
  },
  {
    "text": "And so here the first four\nmethods are baseline methods,",
    "start": "2362530",
    "end": "2368410"
  },
  {
    "text": "and these are\ncommon methods used for multi-agent interactions.",
    "start": "2368410",
    "end": "2375160"
  },
  {
    "text": "And this last one on the right\nis the one we're proposing. But you can see through all\nthese other baseline methods,",
    "start": "2375160",
    "end": "2381790"
  },
  {
    "text": "the results we get are either\nthe robot turns too little too late, or the robot becomes\ntoo timid and just stops",
    "start": "2381790",
    "end": "2391690"
  },
  {
    "text": "and freaks out. Another baseline says that our\nrobot is a little indecisive",
    "start": "2391690",
    "end": "2398110"
  },
  {
    "text": "and oscillates a little. This other method, which\nis actually our framework,",
    "start": "2398110",
    "end": "2403540"
  },
  {
    "text": "but without the markup\nand inconvenience term, oscillates a little, and\nit's unfair in the sense",
    "start": "2403540",
    "end": "2410260"
  },
  {
    "text": "that the human has deviated\nmuch more than the robot.",
    "start": "2410260",
    "end": "2416390"
  },
  {
    "text": "So we want to make sure that\nthe interaction is more fair. And so you can see our\nmethod is fluent and fair,",
    "start": "2416390",
    "end": "2423680"
  },
  {
    "text": "so the interaction\nis quite smooth. And both the human and the\nrobot deviates the same amount.",
    "start": "2423680",
    "end": "2428720"
  },
  {
    "text": " And so here's just\nanother plot to show",
    "start": "2428720",
    "end": "2435349"
  },
  {
    "text": "the yaw rate between\nthe human and the robot. So on the left is,\nagain, a baseline method",
    "start": "2435350",
    "end": "2441770"
  },
  {
    "text": "where there is no markup\nand no inconvenience budget, and on the right is\nwhat we're proposing.",
    "start": "2441770",
    "end": "2446960"
  },
  {
    "text": "But again, you can see\nthat without the markup term and the\ninconvenience budget, the robot's yaw rate\noscillates a whole lot.",
    "start": "2446960",
    "end": "2455840"
  },
  {
    "text": "So again, it's very indecisive,\nwhereas in our method, it's much smoother. ",
    "start": "2455840",
    "end": "2463010"
  },
  {
    "text": "OK. And then we also tested this\nwith a human in the loop. So we're still\nworking on getting",
    "start": "2463010",
    "end": "2469450"
  },
  {
    "text": "this done on like a\nphysical platform, testbed, but we did the next\nbest thing which",
    "start": "2469450",
    "end": "2476530"
  },
  {
    "text": "was hook a Xbox\ncontroller to a simulator and told a human to control\nthis little red blob, which",
    "start": "2476530",
    "end": "2483640"
  },
  {
    "text": "is the human. But this was done in\na first-person view. And then the robot there is\nrunning the algorithm we just",
    "start": "2483640",
    "end": "2490090"
  },
  {
    "text": "talked about. And this was-- we could\ndo this all in real time. So you can see in this\nsimulation, I'll play it again,",
    "start": "2490090",
    "end": "2499120"
  },
  {
    "text": "but the robot actually\nwas the first to start moving to the side\nto avoid a collision.",
    "start": "2499120",
    "end": "2505390"
  },
  {
    "text": "And so this is, again,\nhighlighting the proactivity",
    "start": "2505390",
    "end": "2511480"
  },
  {
    "text": "that we get by the simple\naddition of a markup term and also this\ninconvenience budget.",
    "start": "2511480",
    "end": "2520559"
  },
  {
    "text": "OK. And so we ran a\nbunch of experiments with different initial\nconditions and headings.",
    "start": "2520560",
    "end": "2526410"
  },
  {
    "text": "And then we can\nsee that in terms of total acceleration, so you\ncan think of this as control",
    "start": "2526410",
    "end": "2533160"
  },
  {
    "text": "effort, and this other metric,\nwhich we call path irregularity index, this is a--",
    "start": "2533160",
    "end": "2540600"
  },
  {
    "text": "you can think of this as the\namount of turning an agent does.",
    "start": "2540600",
    "end": "2546340"
  },
  {
    "text": "So if the goal is\nover there, you want to be facing the\ngoal most of the time. You don't want to be facing\nperpendicular to the goal.",
    "start": "2546340",
    "end": "2553530"
  },
  {
    "text": "And so for both these\nmetrics, lower is better. And then you can\nsee for our proposed",
    "start": "2553530",
    "end": "2559170"
  },
  {
    "text": "method in the shaded columns,\nour methods are maybe not always",
    "start": "2559170",
    "end": "2564809"
  },
  {
    "text": "the lowest, but\nthey are both low and between the\nhuman and the robot, they're both rather equal.",
    "start": "2564810",
    "end": "2572580"
  },
  {
    "text": "And so this is showing\nthat through our framework, we're able to achieve\npro-social interactions.",
    "start": "2572580",
    "end": "2579360"
  },
  {
    "text": "So pro-social here means\nthat it is behavior that benefits everyone. So we don't have anyone\nthat's particularly",
    "start": "2579360",
    "end": "2586150"
  },
  {
    "text": "selfish or selfless. Everyone kind of\nhas an equal share in benefiting from\nthe interaction.",
    "start": "2586150",
    "end": "2593960"
  },
  {
    "text": "OK. So we can see for our\nmethod, it's low, but also the magnitude between the blue\nand the red is roughly equal.",
    "start": "2593960",
    "end": "2604030"
  },
  {
    "text": "And then in terms\nof safety, so as I mentioned, before we actually\nrelaxed the safety constraint.",
    "start": "2604030",
    "end": "2611260"
  },
  {
    "text": "But you can see\nthat in our method,",
    "start": "2611260",
    "end": "2616720"
  },
  {
    "text": "there's a tiny bit of\nviolation, but it's pretty much we are right at\nthe limit of safety here.",
    "start": "2616720",
    "end": "2624250"
  },
  {
    "text": "So we don't actually do much\nworse, say like in this method,",
    "start": "2624250",
    "end": "2630850"
  },
  {
    "text": "this is pretty bad. There's pretty much a collision\nthat has happened there.",
    "start": "2630850",
    "end": "2635940"
  },
  {
    "text": "OK. And again, because we're using\ntrajectory optimization, which is a very flexible\nframework, we're",
    "start": "2635940",
    "end": "2641990"
  },
  {
    "text": "able to incorporate more agents. But in this case, these\nother agents shown in gray,",
    "start": "2641990",
    "end": "2648710"
  },
  {
    "text": "we call them\nnon-interactive agents, so we just treat them as\nregular moving obstacles.",
    "start": "2648710",
    "end": "2655160"
  },
  {
    "text": "And the idea is that when\nyou're interacting in a crowd,",
    "start": "2655160",
    "end": "2660289"
  },
  {
    "text": "you're not looking\nat 100 people-- 100 people at a time when\ntrying to avoid them.",
    "start": "2660290",
    "end": "2665630"
  },
  {
    "text": "You're only focusing\non maybe three people and everyone else is\njust in your peripheral.",
    "start": "2665630",
    "end": "2673130"
  },
  {
    "text": "And so that's the idea\nbehind these simulations. And on the right,\nit's also very easy",
    "start": "2673130",
    "end": "2679070"
  },
  {
    "text": "to incorporate walls, wall\nconstraints to the formulation so we can do this for\nindoor navigation,",
    "start": "2679070",
    "end": "2686030"
  },
  {
    "text": "in addition to\noutdoor navigation. And so that's actually what\nwe're working on right now.",
    "start": "2686030",
    "end": "2691760"
  },
  {
    "text": "This is a picture\noutside my office. And so this is a\nreally narrow corridor.",
    "start": "2691760",
    "end": "2696890"
  },
  {
    "text": "So it'll be interesting to see\nhow your robot would navigate through this or just in\nthe building in general",
    "start": "2696890",
    "end": "2702230"
  },
  {
    "text": "and outdoors again on campus. OK. So I know I have three minutes.",
    "start": "2702230",
    "end": "2708860"
  },
  {
    "text": "So before I wrap up, I just\nwanted to give you guys a sneak-peek into some\nwork that I'm working on",
    "start": "2708860",
    "end": "2716569"
  },
  {
    "text": "with my PhD student,\nKazuki here, and I'm really excited about it. And so we have some work on\ncombining control barrier",
    "start": "2716570",
    "end": "2726020"
  },
  {
    "text": "functions and control Lyapunov\nfunctions with diffusion models. So diffusion models,\nmaybe as you know,",
    "start": "2726020",
    "end": "2733210"
  },
  {
    "text": "have gotten a lot of interest\nand this is really useful to generate robot trajectories.",
    "start": "2733210",
    "end": "2739400"
  },
  {
    "text": "But we want these\ntrajectories to be safe. And so we are working\non a method that",
    "start": "2739400",
    "end": "2744500"
  },
  {
    "text": "integrates or incorporates\ncontrol barrier functions and control Lyapunov\nfunctions, and influences",
    "start": "2744500",
    "end": "2752420"
  },
  {
    "text": "the denoising process to make\nsure that the output is safe. And so you can see\non the left here",
    "start": "2752420",
    "end": "2759410"
  },
  {
    "text": "is the method that\nKazuki has designed,",
    "start": "2759410",
    "end": "2765230"
  },
  {
    "text": "and it's able to move\naround these multiple-- these are pedestrians.",
    "start": "2765230",
    "end": "2771200"
  },
  {
    "text": "But then if we just\napplied something known as a control barrier\nfunction QP, a Quadratic Programming, it freaks out.",
    "start": "2771200",
    "end": "2780020"
  },
  {
    "text": "It actually starts running away\nfrom these pedestrians and then actually collides.",
    "start": "2780020",
    "end": "2785030"
  },
  {
    "text": "But because it needed\nto reach the goal, it kind of panics and\nzooms right to the goal.",
    "start": "2785030",
    "end": "2790730"
  },
  {
    "text": "But this is still-- we're still working\non this, but I wanted to give you guys a sneak-peek. And I'm happy to chat\nmore if you have--",
    "start": "2790730",
    "end": "2800240"
  },
  {
    "text": "if you're interested\nin this work. OK. So I'm going to wrap up and\nkind of end on this last slide",
    "start": "2800240",
    "end": "2806810"
  },
  {
    "text": "and say, again, the\ntitle of my talk is towards trusted\nhuman-centric autonomy,",
    "start": "2806810",
    "end": "2812540"
  },
  {
    "text": "and looking at ways how we\ncan start treating humans as partners as\nopposed to a nuisance.",
    "start": "2812540",
    "end": "2819020"
  },
  {
    "text": "And I will just kind of talk\nabout four key points that were common throughout my talk.",
    "start": "2819020",
    "end": "2825290"
  },
  {
    "text": "The first is data. So we need to be more\nintentional about data collection and really start\nfocusing collecting data that",
    "start": "2825290",
    "end": "2833660"
  },
  {
    "text": "encompasses\nsafety-critical scenarios and seeing how humans respond\nor react in these scenarios.",
    "start": "2833660",
    "end": "2841970"
  },
  {
    "text": "The second is in\nalgorithmic development. So what I propose especially\nin the first half of my talk",
    "start": "2841970",
    "end": "2847250"
  },
  {
    "text": "is looking at how\nwe can use control theory to provide inductive\nbias for learning methods.",
    "start": "2847250",
    "end": "2853910"
  },
  {
    "text": "And this is opposed to\napplying learning methods for control theory. So yeah, so looking at how\nwe can use control theory",
    "start": "2853910",
    "end": "2860600"
  },
  {
    "text": "for structure in learning. And then computational methods. A lot of reachability\nmethods I talk about",
    "start": "2860600",
    "end": "2867770"
  },
  {
    "text": "suffer from the curse\nof dimensionality. So we need some ways\nto address that. And finally, at\nthe end of the day,",
    "start": "2867770",
    "end": "2873830"
  },
  {
    "text": "we're looking at algorithms\nwith humans involved, humans in the loop. So we need to do testing\nwith humans in the loop,",
    "start": "2873830",
    "end": "2881720"
  },
  {
    "text": "but that is not only expensive,\nbut coming up with performance metrics is really hard.",
    "start": "2881720",
    "end": "2889220"
  },
  {
    "text": "Actually, the experiments\nthat look like nothing has really happened actually\nas the most successful results",
    "start": "2889220",
    "end": "2896569"
  },
  {
    "text": "because it means no\ncollision has happened and everything seemed\nreally natural.",
    "start": "2896570",
    "end": "2901680"
  },
  {
    "text": "So yeah. So I'll just leave with that\nand happy to take questions, or we can take them outside as I\nthink we'll get kicked out soon",
    "start": "2901680",
    "end": "2908940"
  },
  {
    "text": "for the next class. So thank you for your attention. [APPLAUSE]",
    "start": "2908940",
    "end": "2914240"
  },
  {
    "start": "2914240",
    "end": "2918000"
  }
]