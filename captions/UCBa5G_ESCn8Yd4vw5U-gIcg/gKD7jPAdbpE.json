[
  {
    "start": "0",
    "end": "5510"
  },
  {
    "text": "OK, so now we'd better get\ngoing with the lecture. OK so we're now at week\n8, second half of week 4.",
    "start": "5510",
    "end": "16760"
  },
  {
    "text": "So the agenda today is\nthat I'm first of all going to finish off the last\npieces of the attention",
    "start": "16760",
    "end": "23240"
  },
  {
    "text": "that I didn't talk\nabout last time. But the main topic today is\nto talk about final projects.",
    "start": "23240",
    "end": "29990"
  },
  {
    "text": "And that will consist of sort of\na grab bag of different things. I'll talk about\nthe final projects",
    "start": "29990",
    "end": "36120"
  },
  {
    "text": "and finding research\ntopics, and finding data, and doing research. I'll give a very\nbrief introduction",
    "start": "36120",
    "end": "42950"
  },
  {
    "text": "to the reading comprehension\nand question answering, which is our default final project.",
    "start": "42950",
    "end": "48080"
  },
  {
    "text": "But we get a whole\nlecture on that at the beginning of week 6. So this is just to give you a\nlittle bit of what it's about,",
    "start": "48080",
    "end": "55843"
  },
  {
    "text": "and if you're\nthinking about doing the default final project. In some sense, this\nlecture is also",
    "start": "55843",
    "end": "62809"
  },
  {
    "text": "a rare chance in this\ncourse to pause for breath, because we are really,\nwe've had up until now",
    "start": "62810",
    "end": "70470"
  },
  {
    "text": "been having the fire hose going\nfull stream ahead, spraying you",
    "start": "70470",
    "end": "75830"
  },
  {
    "text": "with new facts, and approaches,\nand algorithms, and models, and linguistic things.",
    "start": "75830",
    "end": "82020"
  },
  {
    "text": "So this is a brief\nrespite from that. So if you have any\nquestions that you've",
    "start": "82020",
    "end": "87110"
  },
  {
    "text": "been wondering about for\nweeks, today's lecture could be a good\ntime to ask them. Because after today's\nlecture, in week 5,",
    "start": "87110",
    "end": "95540"
  },
  {
    "text": "we'll turn the fire\nhose right back on, and we'll have a lot of new\ninformation about transformers",
    "start": "95540",
    "end": "102109"
  },
  {
    "text": "and large pre-trained\nlanguage models that have become a huge\npart of modern neural NLP,",
    "start": "102110",
    "end": "108080"
  },
  {
    "text": "as I already mentioned a little\nbit of, later into this class.",
    "start": "108080",
    "end": "113970"
  },
  {
    "text": "OK, so this is where we left\nthings last time, basically. So I'd sort of talked\nthrough the rough idea",
    "start": "113970",
    "end": "123380"
  },
  {
    "text": "that what we're going to do\nfor this new attention model is that we are going to use\nthe encoder just as before.",
    "start": "123380",
    "end": "130710"
  },
  {
    "text": "And then once we're running\nthe decoder, at each time step,",
    "start": "130710",
    "end": "136750"
  },
  {
    "text": "we're going to compute a new\nhidden representation using the same kind of sequence\nmodel as we had before.",
    "start": "136750",
    "end": "144200"
  },
  {
    "text": "But now we're going to use\nthat hidden representation of the decoder to look\nback at the encoder.",
    "start": "144200",
    "end": "153209"
  },
  {
    "text": "And it's going to then work\nout respectively some kind",
    "start": "153210",
    "end": "158370"
  },
  {
    "text": "of function of similarity\nbetween encoder hidden states and decoder hidden states.",
    "start": "158370",
    "end": "164940"
  },
  {
    "text": "And based on those, it's\ngoing to work out what are called attention scores. And attention scores are\nactually probability weights,",
    "start": "164940",
    "end": "173640"
  },
  {
    "text": "as to how much it likes\ndifferent elements. And based on those\nattention scores,",
    "start": "173640",
    "end": "179280"
  },
  {
    "text": "we're going to compute an\nattention distribution.",
    "start": "179280",
    "end": "184630"
  },
  {
    "text": "So this is our\nprobability distribution. And then based on\nthat, what we do",
    "start": "184630",
    "end": "189760"
  },
  {
    "text": "is compute a weighted average\nof the encoder RNN hidden states",
    "start": "189760",
    "end": "195129"
  },
  {
    "text": "weighted by the\nattention distribution. And so that's going to give\nus a new attention output",
    "start": "195130",
    "end": "200360"
  },
  {
    "text": "vector, which is\nlike the hidden state vector of the encoder that\nis an extra hidden vector.",
    "start": "200360",
    "end": "207819"
  },
  {
    "text": "And so we're gonna\nuse both of them to then generate our\nnext output, which",
    "start": "207820",
    "end": "215110"
  },
  {
    "text": "is going to be here, the word\npie at the end of the sequence. So let's start off now by\ndoing that with some equations.",
    "start": "215110",
    "end": "225090"
  },
  {
    "text": "So-- Chris, before we move on,\nthere's a good question here, sorry, which is why is the\nencoder and decoder both",
    "start": "225090",
    "end": "233230"
  },
  {
    "text": "required, as opposed to\njust the same RNN for both? I think it-- yeah. OK, I'll address that.",
    "start": "233230",
    "end": "239740"
  },
  {
    "text": "So, well, maybe\nthere are a couple of still possible\ninterpretations for that.",
    "start": "239740",
    "end": "245110"
  },
  {
    "text": "But I'll say something about it. So notice that the basic case\nthat we've been doing here",
    "start": "245110",
    "end": "251590"
  },
  {
    "text": "is this case of machine\ntranslation, where we've got a source encoder, which\nis in the source language,",
    "start": "251590",
    "end": "261039"
  },
  {
    "text": "and we've got a\ntarget decoder which is in the target language. So since these two [INAUDIBLE]\ndifferent languages.",
    "start": "261040",
    "end": "272949"
  },
  {
    "text": "It makes sense to have\nseparate sequence models with different RNN\nparameters for each one.",
    "start": "272950",
    "end": "279550"
  },
  {
    "text": "And at that point,\nit's just a fact about what we want to do with\nmachine translation, which",
    "start": "279550",
    "end": "287680"
  },
  {
    "text": "is that, well, we actually\nwant to look back at the source",
    "start": "287680",
    "end": "292690"
  },
  {
    "text": "to try and decide what\nextra words to put into the translation. So it makes sense to\nattend back from the source",
    "start": "292690",
    "end": "300410"
  },
  {
    "text": "to the translation. But what you might be asking\nis, why do we only do that?",
    "start": "300410",
    "end": "307240"
  },
  {
    "text": "Why don't we also consider\ndoing attention from here back into the decoder RNN?",
    "start": "307240",
    "end": "314170"
  },
  {
    "text": "And if that's what you\nare thinking about, that's a great suggestion.",
    "start": "314170",
    "end": "320289"
  },
  {
    "text": "And actually, very quickly\nafter these attention models were developed, that's exactly\nwhat people started doing.",
    "start": "320290",
    "end": "328530"
  },
  {
    "text": "They decided that,\nwell, actually we could start using more\nforms of attention. And we could also use\nattention that looks back",
    "start": "328530",
    "end": "336190"
  },
  {
    "text": "in the decoder sequence. And that often gets referred\nto as self attention. And self attention\nhas proven to be",
    "start": "336190",
    "end": "343360"
  },
  {
    "text": "an extremely powerful concept. And, indeed, that then leads\ninto the transformer models",
    "start": "343360",
    "end": "351460"
  },
  {
    "text": "that we're going\nto see next week. Going in on things, I\nthink self attention",
    "start": "351460",
    "end": "359180"
  },
  {
    "text": "wasn't quite as much an\nobviously needed idea.",
    "start": "359180",
    "end": "364770"
  },
  {
    "text": "From this initial\ntranslation motivation, which was where\nattention was developed,",
    "start": "364770",
    "end": "371470"
  },
  {
    "text": "it seemed fairly clear that\nwhen you're running the decoder",
    "start": "371470",
    "end": "377310"
  },
  {
    "text": "RNN that as a conditional\nlanguage model, it will get some information\nabout the source fed",
    "start": "377310",
    "end": "383789"
  },
  {
    "text": "into its initial state. And it seemed pretty\nclear you were losing a lot of information\nabout the details of what",
    "start": "383790",
    "end": "391080"
  },
  {
    "text": "was in the source\nsentence, and therefore it would be really, really useful\nhave this idea of attention,",
    "start": "391080",
    "end": "397030"
  },
  {
    "text": "so you could directly\nlook at it as you perceive it to translate. In a way, it's a\nlittle bit less obvious",
    "start": "397030",
    "end": "404069"
  },
  {
    "text": "that you need that\nfor the decoder RNN. Because after all, last week, we\nintroduced those really clever",
    "start": "404070",
    "end": "412120"
  },
  {
    "text": "LSTMs. And the whole argument\nof the LSTM was actually they're pretty\ngood at maintaining",
    "start": "412120",
    "end": "418410"
  },
  {
    "text": "history of a sequence through\nquite a bunch of time periods.",
    "start": "418410",
    "end": "423760"
  },
  {
    "text": "So to the extent that the\nLSTM is doing a perfect job, maybe you shouldn't really need\nself attention in your decoder.",
    "start": "423760",
    "end": "431610"
  },
  {
    "text": "But actually, precisely\nwhat's being shown is that this\nmechanism of attention",
    "start": "431610",
    "end": "437070"
  },
  {
    "text": "is a much more\neffective method, again, of selectively addressing\nelements of your past state.",
    "start": "437070",
    "end": "444150"
  },
  {
    "text": "And it's sort of lighter weight. Rather than having\nto kind of cook up the parameters for\nyour LSTM so it's",
    "start": "444150",
    "end": "450150"
  },
  {
    "text": "carrying just the right\ninformation forward all the time, provided you\ncarry only enough information",
    "start": "450150",
    "end": "457740"
  },
  {
    "text": "that the model knows\nwhere to look back to. You can then kind of grab more\ninformation from past states",
    "start": "457740",
    "end": "464759"
  },
  {
    "text": "when you want to. So that's actually\na great approach. But I won't talk\nabout it more now,",
    "start": "464760",
    "end": "471330"
  },
  {
    "text": "and that will come\nout more next week. ",
    "start": "471330",
    "end": "477840"
  },
  {
    "text": "OK, the equations, hopefully\nthese won't actually seem difficult.\nSo what we have is",
    "start": "477840",
    "end": "484470"
  },
  {
    "text": "we have our encoded hidden\nstates which are vectors,",
    "start": "484470",
    "end": "490620"
  },
  {
    "text": "on time step t, we have\nthe decoder hidden state, which is also a vector of\nthe hidden state dimension.",
    "start": "490620",
    "end": "497280"
  },
  {
    "text": "And then what we want to\ndo is get attention scores as to for st, how much attention\nit pays to each of the hidden",
    "start": "497280",
    "end": "509729"
  },
  {
    "text": "states of the encoder. And the easiest way\nto do that is just",
    "start": "509730",
    "end": "515700"
  },
  {
    "text": "use dot products between\nthe source and the-- sorry, not the source--\nthe dot products",
    "start": "515700",
    "end": "522240"
  },
  {
    "text": "between the decoder hidden state\ns and the encoder hidden state",
    "start": "522240",
    "end": "527279"
  },
  {
    "text": "h. So these give us\na bunch of numbers that might be\nnegative or positive, but they're attention scores.",
    "start": "527280",
    "end": "534040"
  },
  {
    "text": "And so have, just like we did\nright from the first lecture with word vectors,\nwe then put those",
    "start": "534040",
    "end": "540000"
  },
  {
    "text": "through a softmax distribution. And then we get a\nprobability distribution",
    "start": "540000",
    "end": "545100"
  },
  {
    "text": "over the time steps\nof the encoder. Okay, so and now we've got\nthat probability distribution.",
    "start": "545100",
    "end": "552819"
  },
  {
    "text": "We can construct a\nnew vector by creating a weighted sum of the\nencoder hidden states",
    "start": "552820",
    "end": "561760"
  },
  {
    "text": "based on these attention\ndistribution probabilities.",
    "start": "561760",
    "end": "567130"
  },
  {
    "text": "And that [INAUDIBLE]\ngoing to make use of that in\ngenerating the output.",
    "start": "567130",
    "end": "574210"
  },
  {
    "text": "We're going to concatenate\nthe attention output with the decoder\nhidden state st.",
    "start": "574210",
    "end": "580550"
  },
  {
    "text": "So now that's\nsomething of size 2h. And then we're\ngoing to proceed as",
    "start": "580550",
    "end": "585610"
  },
  {
    "text": "with the non-attention model. We then put that\nthrough another softmax",
    "start": "585610",
    "end": "590890"
  },
  {
    "text": "to generate a probability\ndistribution of output words, and then we'd sample a word.",
    "start": "590890",
    "end": "596770"
  },
  {
    "text": "And hopefully that's a\nfairly obvious implementation",
    "start": "596770",
    "end": "601930"
  },
  {
    "text": "of what we have here. So we've got our vectors\nwith encoder and decoder.",
    "start": "601930",
    "end": "607050"
  },
  {
    "text": "We're getting dot products\nof st with each one. Softmax turns that\ninto a probability.",
    "start": "607050",
    "end": "614410"
  },
  {
    "text": "We take the weighted\naverage of the ones in red to get the attention output. We combine that with the\nst decoder's hidden state.",
    "start": "614410",
    "end": "626680"
  },
  {
    "text": "And then we put it\nthrough another softmax, and we can sample pie. ",
    "start": "626680",
    "end": "637259"
  },
  {
    "text": "Okay, so I sort almost\ncan't stress enough",
    "start": "637260",
    "end": "642430"
  },
  {
    "text": "that attention is great. So the very first modern neural\nmachine translation program",
    "start": "642430",
    "end": "651790"
  },
  {
    "text": "was done in 2014 at\nGoogle by [INAUDIBLE]..",
    "start": "651790",
    "end": "658000"
  },
  {
    "text": "And they had a straightforward\nencoder, decoder to LSTMs.",
    "start": "658000",
    "end": "663550"
  },
  {
    "text": "And by a bunch of\ntricks of having very deep LSTMs,\nhuge amount of data,",
    "start": "663550",
    "end": "670029"
  },
  {
    "text": "huge amount of training,\nother tricks that I don't want to go into now,\nthey were actually",
    "start": "670030",
    "end": "675100"
  },
  {
    "text": "able to get good results\nby just putting together a straight seq2seq neural\nmachine translation system.",
    "start": "675100",
    "end": "685040"
  },
  {
    "text": "But very soon after that, in\nfact, later the same year, a group from Montreal, Dzmitry\nBahdanau, Kyunghyun Cho,",
    "start": "685040",
    "end": "693769"
  },
  {
    "text": "and Yoshua Bengio introduced\nsequence to sequence model with attention.",
    "start": "693770",
    "end": "699829"
  },
  {
    "text": "And it was just\nobviously better. So attention significantly\nimproved neural machine",
    "start": "699830",
    "end": "705020"
  },
  {
    "text": "translation performance. And that sort of makes sense. It allows the decoder to\nfocus on parts of the source",
    "start": "705020",
    "end": "713220"
  },
  {
    "text": "sentence. So I think that is giving you\na much more human-like model",
    "start": "713220",
    "end": "718640"
  },
  {
    "text": "of doing machine translation. Because you know exactly\nwhat a human translator would",
    "start": "718640",
    "end": "723740"
  },
  {
    "text": "do is you read the\nsource sentence, you've got an idea\nof what it's about, you start writing the first\ncouple of words of translation.",
    "start": "723740",
    "end": "731120"
  },
  {
    "text": "And then what you\ndo is you look back to see what exactly it said,\nas that sort of modifiers",
    "start": "731120",
    "end": "739399"
  },
  {
    "text": "the noun to translate\nthe next few words. Technically people think of\nit as solving the bottleneck",
    "start": "739400",
    "end": "745700"
  },
  {
    "text": "problem, because attention\nnow allows us full access to the entire\nsource hidden state,",
    "start": "745700",
    "end": "752810"
  },
  {
    "text": "and we can get any\ninformation that we need. It's not the case\nthat all information has to be encoded in\nthe final hidden state.",
    "start": "752810",
    "end": "760860"
  },
  {
    "text": "It also helps with the\nvanishing gradient problem. So effectively, now,\nwe have shortcuts back",
    "start": "760860",
    "end": "767200"
  },
  {
    "text": "to every hidden\nstate of the encoder. And so therefore there's\nalways a short path",
    "start": "767200",
    "end": "773360"
  },
  {
    "text": "with gradient flow, and that\ngreatly mitigates the vanishing gradient problem.",
    "start": "773360",
    "end": "778776"
  },
  {
    "text": "A final neat thing\nis that attention provides some effective\ninterpretability to sequence",
    "start": "778776",
    "end": "786290"
  },
  {
    "text": "to sequence models. Because by looking at the\nattention distribution,",
    "start": "786290",
    "end": "791690"
  },
  {
    "text": "we can see what the\ndecoder was focusing on. So in a stark\nprobabilistic way, we",
    "start": "791690",
    "end": "798020"
  },
  {
    "text": "get the [INAUDIBLE]\noperation of the model a soft alignment as to which\nwords translate which words.",
    "start": "798020",
    "end": "805490"
  },
  {
    "text": "So in this example of the\nFrench sentence being translated is he hit me with a pie, where\nthere's sort of a single verb",
    "start": "805490",
    "end": "813800"
  },
  {
    "text": "here in the French. Which is kind of like\nas in English sometimes people imply they\nuse pied as a verb.",
    "start": "813800",
    "end": "820100"
  },
  {
    "text": "So it's sort of like\nsaying he, me, pied. That the model is getting\nthat \"il\" is translated as he,",
    "start": "820100",
    "end": "828890"
  },
  {
    "text": "the \"m'\" is being translated as\nme, and essentially \"entarte\" is being translated\nas hit with a pie.",
    "start": "828890",
    "end": "838150"
  },
  {
    "text": "So the amazing thing is,\nthe model was never told about any of these alignments.",
    "start": "838150",
    "end": "844839"
  },
  {
    "text": "There was no explicit\nseparate model which was trying to\nlearn these alignments, as in the earlier statistical\nphrase based systems.",
    "start": "844840",
    "end": "852820"
  },
  {
    "text": "We just built a sequence to\nsequence model with attention, and said, here are a lot\nof translated sentences.",
    "start": "852820",
    "end": "860800"
  },
  {
    "text": "Start running back and\nforth and try and get [INAUDIBLE] sentences.",
    "start": "860800",
    "end": "868200"
  },
  {
    "text": "And it just learns by\nitself in deciding where it's best to pay attention.",
    "start": "868200",
    "end": "873780"
  },
  {
    "text": "What is a good alignment between\nthe source and the target languages? ",
    "start": "873780",
    "end": "883800"
  },
  {
    "text": "Okay, so that's the\nbasic idea of attention.",
    "start": "883800",
    "end": "889868"
  },
  {
    "text": "I want to go a bit more into\nthe complexity of attention",
    "start": "889868",
    "end": "895250"
  },
  {
    "text": "since it's such an\nimportant idea that we'll see a lot as we continue\nnow with the course, right?",
    "start": "895250",
    "end": "903210"
  },
  {
    "text": "So there are several\nattention variants. But first of all,\nwhat's the common part?",
    "start": "903210",
    "end": "910160"
  },
  {
    "text": "So we have some\nvalues, some vectors that we're going to be sort\nof using as our memory,",
    "start": "910160",
    "end": "917630"
  },
  {
    "text": "and we have a query vector. So attention always involves\nthat we calculate the attention",
    "start": "917630",
    "end": "924529"
  },
  {
    "text": "scores and turn those\ninto a probability distribution of the softmax.",
    "start": "924530",
    "end": "932180"
  },
  {
    "text": "And we use the\nattention distribution to calculate a weighted sum\nof the elements in our memory,",
    "start": "932180",
    "end": "941000"
  },
  {
    "text": "giving us an attention output. So the main place where you'll\nimmediately see variation",
    "start": "941000",
    "end": "948360"
  },
  {
    "text": "in the attention is, how do you\ncompute these attention scores?",
    "start": "948360",
    "end": "954440"
  },
  {
    "text": "And so let's go through some\nof the ways that that's done. So the simplest, most obvious\nway to do it is to say, let's",
    "start": "954440",
    "end": "962660"
  },
  {
    "text": "just take the dot product\nbetween the given-- the current hidden state of the\ndecoder, and all of the vectors",
    "start": "962660",
    "end": "971592"
  },
  {
    "text": "here, the source encoder vectors\nthat we are putting attention",
    "start": "971592",
    "end": "977149"
  },
  {
    "text": "over. And that sort of\nmakes sense, right? This is a dot product, it's our\nmost basic similarity score.",
    "start": "977150",
    "end": "984990"
  },
  {
    "text": "But it seems like there's\nsomething wrong with this. And that is, it\nseems wrong for you",
    "start": "984990",
    "end": "992839"
  },
  {
    "text": "to want to think\nthat the entirety of the source hidden states,\nand the entirety of the target",
    "start": "992840",
    "end": "999300"
  },
  {
    "text": "hidden states is all\nhaving information about where to attend to.",
    "start": "999300",
    "end": "1005500"
  },
  {
    "text": "Because really, these LSTMs\nare doing multiple things. So the LSTMs are carrying\nforward information",
    "start": "1005500",
    "end": "1013300"
  },
  {
    "text": "along their own sequence\nto help record information about the past so it can\nbe used in the future.",
    "start": "1013300",
    "end": "1020050"
  },
  {
    "text": "They have information in\nthe hidden state to tell you which output you\nshould generate next.",
    "start": "1020050",
    "end": "1026980"
  },
  {
    "text": "And perhaps, they're\nencoding some information that will serve as a kind of\na query key for getting out",
    "start": "1026980",
    "end": "1034270"
  },
  {
    "text": "information by attention from\nthe source hidden states. So it seems like\nprobably we only want",
    "start": "1034270",
    "end": "1040990"
  },
  {
    "text": "to use some of the\ninformation in them to calculate our\nattention score.",
    "start": "1040990",
    "end": "1046599"
  },
  {
    "text": "And so that's the\nkind of approach that was taken very\nquickly in subsequent work.",
    "start": "1046599",
    "end": "1054320"
  },
  {
    "text": "So the next year, Thang\nLuong, working with me,",
    "start": "1054320",
    "end": "1059769"
  },
  {
    "text": "explored this idea that\nis now normally called multiplicative attention. So in a multiplicative\nattention,",
    "start": "1059770",
    "end": "1066520"
  },
  {
    "text": "we put an extra matrix in the\nmiddle of our dot product.",
    "start": "1066520",
    "end": "1071580"
  },
  {
    "text": "So this gives us a matrix\nof learnable parameters. And so effectively,\ninside this matrix,",
    "start": "1071580",
    "end": "1079750"
  },
  {
    "text": "we can learn what parts\nof s to pay attention to, and what parts of\nh to pay attention",
    "start": "1079750",
    "end": "1085540"
  },
  {
    "text": "to when calculating\nthe similarity, and hence the attention score\nbetween the source hidden",
    "start": "1085540",
    "end": "1094330"
  },
  {
    "text": "states and the\ndecoder's hidden state. So that is a good thing, which\nin general works much better.",
    "start": "1094330",
    "end": "1103840"
  },
  {
    "text": "But there's perhaps\na problem with that, which is, maybe this W matrix\nhas too many parameters.",
    "start": "1103840",
    "end": "1111970"
  },
  {
    "text": "Because we've got these two\nvectors, which in the simplest case are both in\nthe same dimension d, but they don't\nhave to be the same.",
    "start": "1111970",
    "end": "1119080"
  },
  {
    "text": "And we're now\nputting in d squared, new parameters for the\nmatrix W. And that's sort of",
    "start": "1119080",
    "end": "1125289"
  },
  {
    "text": "feels like it's too many. Because arguably, it\nseems like we should only have about two d parameters,\none d saying how much attention",
    "start": "1125290",
    "end": "1133440"
  },
  {
    "text": "to pay to different parts of\ns, and the other one how much attention to pay to parts of h.",
    "start": "1133440",
    "end": "1140050"
  },
  {
    "text": "There's a reason for more. By having a whole matrix\nhere, you're not only doing the scoring\nelement-wise, right, you",
    "start": "1140050",
    "end": "1147820"
  },
  {
    "text": "can have any element\nof s being combined with any element\nof the h vector,",
    "start": "1147820",
    "end": "1154390"
  },
  {
    "text": "and see that as a useful part\nof your similarity score. But there's still a\nlot of parameters.",
    "start": "1154390",
    "end": "1161750"
  },
  {
    "text": "So a bit after that,\n[INAUDIBLE] parameters.",
    "start": "1161750",
    "end": "1170390"
  },
  {
    "text": "So if you have a\nmatrix W and you'd like it to have less parameters,\nthe obvious linear algebra",
    "start": "1170390",
    "end": "1176380"
  },
  {
    "text": "thing to do is to say,\nOK, we can model W as U transpose V. Where U and\nV are low rank skinny matrices.",
    "start": "1176380",
    "end": "1187910"
  },
  {
    "text": "So we can choose some number k\nfor how skinny these matrices are going to be.",
    "start": "1187910",
    "end": "1193570"
  },
  {
    "text": "And they can be k\ntimes d matrices. And then we're getting a\nreduced rank matrix here.",
    "start": "1193570",
    "end": "1200690"
  },
  {
    "text": "And so we have a lot less-- we\nend up with a d by d matrix, but it has a lot less\nparameters in it.",
    "start": "1200690",
    "end": "1208360"
  },
  {
    "text": "And so people explored that. And at that point,\nif you just do",
    "start": "1208360",
    "end": "1214270"
  },
  {
    "text": "a little bit of linear\nalgebra, what we have here is exactly the\nsame, the source hidden vector",
    "start": "1214270",
    "end": "1222070"
  },
  {
    "text": "and the decoder hidden vector,\nand projecting each of them",
    "start": "1222070",
    "end": "1228370"
  },
  {
    "text": "with a low rank\nlinear projection, and then taking the dot\nproduct of the projections.",
    "start": "1228370",
    "end": "1235030"
  },
  {
    "text": "And if you remember\nthis equation here at all, next\nTuesday's lecture,",
    "start": "1235030",
    "end": "1240710"
  },
  {
    "text": "you'll see that that's actually\nexactly what's happening in transformer models. ",
    "start": "1240710",
    "end": "1247750"
  },
  {
    "text": "But none of these were actually\nthe original form of attention that was suggested\nby Bahdanau et al.",
    "start": "1247750",
    "end": "1255190"
  },
  {
    "text": "But Bahdanau et al. suggested is the way we\ncould calculate the attention",
    "start": "1255190",
    "end": "1261830"
  },
  {
    "text": "score is by taking\nthe two vector, multiply each by a matrix,\nadding them, putting them",
    "start": "1261830",
    "end": "1270760"
  },
  {
    "text": "through a tanh function, giving\nus another vector which we then dot product with\nyet another vector,",
    "start": "1270760",
    "end": "1277360"
  },
  {
    "text": "and we get out a weight. So in the literature that\ncompares attention variance,",
    "start": "1277360",
    "end": "1284529"
  },
  {
    "text": "this one is normally referred\nto additive attention. I've always thought that\nthat's a really lousy name,",
    "start": "1284530",
    "end": "1292389"
  },
  {
    "text": "or at least it never\nmade sense to me. Because really what\nyou're doing here is that you're using a\nneural net layer to calculate",
    "start": "1292390",
    "end": "1300560"
  },
  {
    "text": "the attention score, right? This looks just like the\nkind of neural net layers that we use when we wanted\nto calculate scores,",
    "start": "1300560",
    "end": "1308237"
  },
  {
    "text": "such as when we were\ndoing simple sort of feed-forward networks\nat the beginning when wanted to calculate scores.",
    "start": "1308237",
    "end": "1314275"
  },
  {
    "start": "1314275",
    "end": "1320270"
  },
  {
    "text": "OK, in assignment 4, which\nyou should look at really soon if you haven't looked\nat it yet, actually one",
    "start": "1320270",
    "end": "1326690"
  },
  {
    "text": "of the things in the written\nproblems of assignment 4 is thinking about these\ndifferent attention variants.",
    "start": "1326690",
    "end": "1333350"
  },
  {
    "text": " OK, yeah, so I've only\npresented this idea of attention",
    "start": "1333350",
    "end": "1341360"
  },
  {
    "text": "as something good for\nmachine translation to use between the source and\nthe target sequence models,",
    "start": "1341360",
    "end": "1350840"
  },
  {
    "text": "except for that when I tried\nto answer the question. But, really, this is a general\ndeep learning technique, right?",
    "start": "1350840",
    "end": "1358220"
  },
  {
    "text": "So it's not only great\nfor that application. You can use it in many\narchitectures, not just",
    "start": "1358220",
    "end": "1364610"
  },
  {
    "text": "sequence to sequence. And you can use\nit for many tasks, not just for\nmachine translation.",
    "start": "1364610",
    "end": "1370230"
  },
  {
    "text": "So any time that you have\na bunch of vector values",
    "start": "1370230",
    "end": "1375919"
  },
  {
    "text": "and you have sum of a vector\nthat you can regard as a query, attention is a technique to\ncompute a value from them,",
    "start": "1375920",
    "end": "1385280"
  },
  {
    "text": "that you can have the\nquery attend to the values. ",
    "start": "1385280",
    "end": "1392260"
  },
  {
    "text": "And so once you think\nabout it like that, you can think about attention\nas a kind of memory access",
    "start": "1392260",
    "end": "1400460"
  },
  {
    "text": "mechanism, that the weighted\nsum that attention calculates gives you a kind of\nselective summary of some",
    "start": "1400460",
    "end": "1408580"
  },
  {
    "text": "of the information\ncontained in the values. And the query tells you which\nvalues to pay attention to.",
    "start": "1408580",
    "end": "1417290"
  },
  {
    "text": "So, on the one\nhand, you could say this is a good general\ntechnique any time you",
    "start": "1417290",
    "end": "1423280"
  },
  {
    "text": "have a whole bunch of\nvectors and you want to just get one vector out.",
    "start": "1423280",
    "end": "1429130"
  },
  {
    "text": "Well, the dumbest thing you can\ndo is just average them all, or do max pooling, take\nthe element-wise max",
    "start": "1429130",
    "end": "1436570"
  },
  {
    "text": "of each element. But this gives you a\nmuch more flexible way to combine them together\ninto a single vector.",
    "start": "1436570",
    "end": "1444340"
  },
  {
    "text": "And so you can think of\nthat as actually giving us an operation that's much more\nlike a conventional computer",
    "start": "1444340",
    "end": "1452470"
  },
  {
    "text": "and its memory access. So the values we can\nthink of as like our RAM.",
    "start": "1452470",
    "end": "1457900"
  },
  {
    "text": " And [INAUDIBLE] memory, right?",
    "start": "1457900",
    "end": "1464250"
  },
  {
    "text": "We have a query vector,\nand the query vector acts as a sort of associative\nmemory pointer that",
    "start": "1464250",
    "end": "1470640"
  },
  {
    "text": "says how much weight to put\non different parts of RAM. And then we sort\nof proportionately",
    "start": "1470640",
    "end": "1476370"
  },
  {
    "text": "retrieve those bits of\nRAM to get a new value. Another cool thing\nabout attention",
    "start": "1476370",
    "end": "1483780"
  },
  {
    "text": "that I'll just mention here\nis that attention is actually",
    "start": "1483780",
    "end": "1490720"
  },
  {
    "text": "a genuinely new deep learning\nidea from the last decade, developed in the last decade.",
    "start": "1490720",
    "end": "1497140"
  },
  {
    "text": "It's sort of a slightly\ndepressing fact about deep learning\nthat a lot of what's",
    "start": "1497140",
    "end": "1502419"
  },
  {
    "text": "being done in deep\nlearning in recent years wasn't actually new ideas.",
    "start": "1502420",
    "end": "1507429"
  },
  {
    "text": "They were ideas that were\ndeveloped in the 80s and 90s, it's just people aren't\nable to get much use of them",
    "start": "1507430",
    "end": "1513370"
  },
  {
    "text": "then, because their\ncomputers were too small, and the amount of data\nthey had was too small.",
    "start": "1513370",
    "end": "1518620"
  },
  {
    "text": "And they were sort\nof reinvented-- well, not reinvented, but\nthey were given new life",
    "start": "1518620",
    "end": "1524590"
  },
  {
    "text": "in the 2010s. But attention is a genuinely\nnew idea from the 2010s.",
    "start": "1524590",
    "end": "1532600"
  },
  {
    "text": "And we'll see sort of next\nweek how it sort of then",
    "start": "1532600",
    "end": "1537730"
  },
  {
    "text": "turns into this huge\nidea that transforms NLP. ",
    "start": "1537730",
    "end": "1543769"
  },
  {
    "text": "OK, so that's all I wanted\nto tell you about attention, and now I'm going to\nget on and go and talk",
    "start": "1543770",
    "end": "1549830"
  },
  {
    "text": "about final projects and\nall this stuff about that. So am I good to start\ndoing other stuff?",
    "start": "1549830",
    "end": "1555679"
  },
  {
    "text": " Yes.",
    "start": "1555680",
    "end": "1561310"
  },
  {
    "text": "Yeah, OK, so this is a quick\nreminder so you don't forget. This is the coursework.",
    "start": "1561310",
    "end": "1568380"
  },
  {
    "text": "So you're right in the middle. You're past half way almost of\ngoing through the assignments.",
    "start": "1568380",
    "end": "1577140"
  },
  {
    "text": "And they're worth\nabout half the grade. But the other half\nof the grade comes from the final project, which\nI'm going to talk about today.",
    "start": "1577140",
    "end": "1585570"
  },
  {
    "text": "And that gets split up. So there's a project proposal,\nwhich we've handed out",
    "start": "1585570",
    "end": "1590789"
  },
  {
    "text": "instructions for today. It's worth 5%. There's a milestone that's\nworth 5% in the middle.",
    "start": "1590790",
    "end": "1597690"
  },
  {
    "text": "And there's a report at the end,\nwhich is the big part, which is worth 30%. And then we also want to have a\nrepresentation of your project",
    "start": "1597690",
    "end": "1605610"
  },
  {
    "text": "that people can easily browse. So in normal years when\nthere were people on campus,",
    "start": "1605610",
    "end": "1611610"
  },
  {
    "text": "we had a poster session\nfor the class, which just seems like we\njust can't usefully do in the current world.",
    "start": "1611610",
    "end": "1617370"
  },
  {
    "text": "We want to put up a\nwebsite where there's some brief descriptions\nof each project,",
    "start": "1617370",
    "end": "1623040"
  },
  {
    "text": "so a nice picture\nof what you've done. And then we can link to\nyour project write up.",
    "start": "1623040",
    "end": "1628530"
  },
  {
    "text": "And so you will also\nthen get 3% for that. Just make sure you do it, when\nit comes down to it, right?",
    "start": "1628530",
    "end": "1634020"
  },
  {
    "text": " And yeah, so I should just\nmention again, as we go along.",
    "start": "1634020",
    "end": "1641230"
  },
  {
    "text": "The honor code, that's important\nalso for the final projects.",
    "start": "1641230",
    "end": "1647720"
  },
  {
    "text": "So just to be clear on that,\nso for the final projects, in a lot of cases, you'll get to\nuse a lot of stuff that already",
    "start": "1647720",
    "end": "1654970"
  },
  {
    "text": "exists, that you may well\nbe using some model that you can just download\nfrom the GitHub",
    "start": "1654970",
    "end": "1660820"
  },
  {
    "text": "repository or similar places. You may well be\ntaking various ideas.",
    "start": "1660820",
    "end": "1668590"
  },
  {
    "text": "It's fine for the\nfinal project to use any amount of existing stuff.",
    "start": "1668590",
    "end": "1673840"
  },
  {
    "text": "But you have to make sure\nthat you acknowledge what you're using and document it.",
    "start": "1673840",
    "end": "1679820"
  },
  {
    "text": "And in terms of evaluating\nyour projects, what we'll be interested in is the value\nadd in terms of what you did,",
    "start": "1679820",
    "end": "1689650"
  },
  {
    "text": "not what you're able to\ndownload from other people. So for the final project,\nyou can have teams of 1 to 3.",
    "start": "1689650",
    "end": "1699100"
  },
  {
    "text": "In almost every case,\nevery member of the team gets the same score. But we do ask for a brief\nstatement of the work done",
    "start": "1699100",
    "end": "1706390"
  },
  {
    "text": "by each teammate. And if it's clear\nthat there was sort of some egregious imbalance,\nin sort of 1 in 100 cases,",
    "start": "1706390",
    "end": "1714970"
  },
  {
    "text": "we do do something about that. So for the final project, you've\nessentially got two choices.",
    "start": "1714970",
    "end": "1722600"
  },
  {
    "text": "You can either do our default\nfinal project where we give you the scaffolding and send you\npointed in some direction,",
    "start": "1722600",
    "end": "1731140"
  },
  {
    "text": "or you can propose a custom\nfinal project, which we then need to approve as a suitable\nfinal project for the course.",
    "start": "1731140",
    "end": "1739150"
  },
  {
    "text": "And I'm going to\ntalk a bit about both of those in this class. So you can work in\nteams of 1 to 3.",
    "start": "1739150",
    "end": "1746889"
  },
  {
    "text": "If you have a bigger team\nwe expect you to do more. Sometimes people use the same\nproject for multiple classes,",
    "start": "1746890",
    "end": "1755260"
  },
  {
    "text": "so that it might also use it\nas [INAUDIBLE] allow that.",
    "start": "1755260",
    "end": "1763360"
  },
  {
    "text": "But it's sort of the\nsame general rule, right? So that there are two\nof you, and you're using the project\nfor two classes each,",
    "start": "1763360",
    "end": "1769280"
  },
  {
    "text": "that's sort of like it should\nbe four persons worth of work that is being done.",
    "start": "1769280",
    "end": "1774380"
  },
  {
    "text": "And so we expect projects\nthat are bigger, that--",
    "start": "1774380",
    "end": "1779980"
  },
  {
    "text": "if it's just one\nof you, we can be totally satisfied with\nsomething compact and small.",
    "start": "1779980",
    "end": "1785840"
  },
  {
    "text": "It sort of has to be done, but\nit can be compact and small. Whereas if there are three\nof you, we sort of feel like,",
    "start": "1785840",
    "end": "1793600"
  },
  {
    "text": "well, you really should\nhave had enough time to actually implement\na variant model and see if that works better.",
    "start": "1793600",
    "end": "1802390"
  },
  {
    "text": "You can use any\nframework or language you want for the final project,\nbut in practice, basically,",
    "start": "1802390",
    "end": "1808510"
  },
  {
    "text": "people go on using PyTorch. And then, finally,\nI've got to mention on the default final project.",
    "start": "1808510",
    "end": "1815080"
  },
  {
    "text": "I'll get back to\nthis in a minute. But actually this\nyear, new thing, we've got two sub-variants of\nthe default final project,",
    "start": "1815080",
    "end": "1823480"
  },
  {
    "text": "and you can choose\none or the other. So for custom\nfinal projects, I'm",
    "start": "1823480",
    "end": "1829360"
  },
  {
    "text": "really happy to talk to\npeople about final projects. But there's this problem.",
    "start": "1829360",
    "end": "1834490"
  },
  {
    "text": "And you'll be encouraged to\nsign up for an office hour slot. But there's this problem\nthat there's only one of me.",
    "start": "1834490",
    "end": "1842080"
  },
  {
    "text": "So also encourage you to talk\nto the TAs about final projects.",
    "start": "1842080",
    "end": "1849059"
  },
  {
    "text": "Many of the TAs have experience\nfor all sorts of things. And we've tried to\nsort of summarize",
    "start": "1849060",
    "end": "1854080"
  },
  {
    "text": "some of what they\nhave experience with on the office hours page.",
    "start": "1854080",
    "end": "1859580"
  },
  {
    "text": "So look at that and try\nand get some ideas of who might be good to talk about.",
    "start": "1859580",
    "end": "1865100"
  },
  {
    "text": "So for the final projects,\nwe make sure everyone",
    "start": "1865100",
    "end": "1870580"
  },
  {
    "text": "has some kind of mentor. And the mentor can either be a\nTA or instructor in this class,",
    "start": "1870580",
    "end": "1877030"
  },
  {
    "text": "or it could be somebody else. And for the somebody\nelse option, well, if there's someone\nyou know at Stanford,",
    "start": "1877030",
    "end": "1884530"
  },
  {
    "text": "and there's some cool\nproject they have, whatever it is,\nif it's something",
    "start": "1884530",
    "end": "1890230"
  },
  {
    "text": "on the political science\nof understanding documents, or in the med school\ndoing something,",
    "start": "1890230",
    "end": "1897910"
  },
  {
    "text": "you can have them\nas your mentor. Or we then also collected\nsome projects from people",
    "start": "1897910",
    "end": "1904060"
  },
  {
    "text": "in the Stanford NLP\ngroup and community, and we're going to be\ndistributing a list of those.",
    "start": "1904060",
    "end": "1909680"
  },
  {
    "text": "So you can try and\nsort of sign up to be doing one\nof those projects, and they can be a mentor.",
    "start": "1909680",
    "end": "1915160"
  },
  {
    "text": "So in that case,\nthe other person we expect to be\nthe mentor that's keeping an eye your project\nand telling you to do",
    "start": "1915160",
    "end": "1922210"
  },
  {
    "text": "something sort of sensible. But it's one of the\nTAs in the class who",
    "start": "1922210",
    "end": "1927384"
  },
  {
    "text": "will be grading your various\npieces of work for the class. OK, so these are the details of\nthe two default final projects,",
    "start": "1927385",
    "end": "1936779"
  },
  {
    "text": "and the handouts for them. There are about 10\nor so page handouts for each one are\nout on the web now.",
    "start": "1936780",
    "end": "1944200"
  },
  {
    "text": "So both of them involve\nquestion answering, which I'll mention a bit at the end.",
    "start": "1944200",
    "end": "1950160"
  },
  {
    "text": "And there are sort\nof two variants. One is for you to actually\nbuild the question answering",
    "start": "1950160",
    "end": "1957840"
  },
  {
    "text": "architecture by yourself. And I've described\nthis as from scratch,",
    "start": "1957840",
    "end": "1964320"
  },
  {
    "text": "but it's not really from\nscratch, because we give you a baseline question answering\nsystem in the starter code.",
    "start": "1964320",
    "end": "1970600"
  },
  {
    "text": "But you're working,\ndoing all the work of what else could I\nadd to the model, how could I add extra layers for\nattention and other things",
    "start": "1970600",
    "end": "1978330"
  },
  {
    "text": "to make that better,\nusing the SQuAD data set.",
    "start": "1978330",
    "end": "1984279"
  },
  {
    "text": "But one of the things\nthat's happened in NLP, which has really been the\ntopic of next week's class,",
    "start": "1984280",
    "end": "1991650"
  },
  {
    "text": "is in the last few\nyears in NLP, there's been this revolution in using\nlarge, pre-trained language",
    "start": "1991650",
    "end": "2000110"
  },
  {
    "text": "models with names like\nBERT, roBERTa, others,",
    "start": "2000110",
    "end": "2006110"
  },
  {
    "text": "which have just been\nfantastically good for doing natural language problems. So the other choice is to\nmake use of those models.",
    "start": "2006110",
    "end": "2016730"
  },
  {
    "text": "And so then,\neffectively, the model and the model architecture as a\nstarting point is given to you.",
    "start": "2016730",
    "end": "2023360"
  },
  {
    "text": "And so then what we're hoping\nto have people focus on is how to build a robust\nquestion answering",
    "start": "2023360",
    "end": "2028940"
  },
  {
    "text": "system, which works\nacross different data sets and domains. So a huge problem with\na lot of NLP models",
    "start": "2028940",
    "end": "2036290"
  },
  {
    "text": "is if you train them on\nsay, Wikipedia data, they work great on Wikipedia data.",
    "start": "2036290",
    "end": "2043040"
  },
  {
    "text": "But then as soon as you try\nand use them on something else, whether it's customer\nsupport questions or web",
    "start": "2043040",
    "end": "2050449"
  },
  {
    "text": "questions, that their\nperformance degrades greatly. Despite the fact that\nthe human being it",
    "start": "2050449",
    "end": "2057830"
  },
  {
    "text": "seems like they're doing\n[INAUDIBLE] and so the building",
    "start": "2057830",
    "end": "2065080"
  },
  {
    "text": "a robust QA system\ntrack with them, going to be sort\nof training or fine",
    "start": "2065080",
    "end": "2070100"
  },
  {
    "text": "tuning pre-trained language\nmodels on simple data sets. And your goal is to produce\nsomething that then performs",
    "start": "2070100",
    "end": "2078440"
  },
  {
    "text": "well on different data sets.  OK, so for this topic\nof question answering,",
    "start": "2078440",
    "end": "2086399"
  },
  {
    "text": "I'm going to set a few\nminutes on it at the end. But on Tuesday of week\n6, the entire lecture",
    "start": "2086400",
    "end": "2093210"
  },
  {
    "text": "is on question answering. So there'll be a\nlot of content there on the different kinds of models\nfor really getting up to speed.",
    "start": "2093210",
    "end": "2100080"
  },
  {
    "text": "Good stuff to know\neven if you're not doing the default final\nproject, because it's a major application of NLP.",
    "start": "2100080",
    "end": "2105600"
  },
  {
    "text": "But even better\nstuff to know if you are doing the final project. So look out for that.",
    "start": "2105600",
    "end": "2110660"
  },
  {
    "text": "But just to give\nyou one example, this is the kind\nof thing we have in question answering problems.",
    "start": "2110660",
    "end": "2115710"
  },
  {
    "text": "So question answering is\ntaking a paragraph of text. But I only put one sentence\nto keep my slide short.",
    "start": "2115710",
    "end": "2124500"
  },
  {
    "text": "Bill Aiken, adopted by Mexican\nmovie actress Lupe Mayorga, grew up in the\nneighboring town of Madera",
    "start": "2124500",
    "end": "2131980"
  },
  {
    "text": "and his song\nchronicled the what? The question is, in what\ntown did Bill Aiken grow up?",
    "start": "2131980",
    "end": "2137910"
  },
  {
    "text": "This stuff doesn't\nactually seen so hard. I presume all of\nyou could have done this when you were in\nsixth grade in school,",
    "start": "2137910",
    "end": "2144300"
  },
  {
    "text": "or something like that. The answer should be Madera. But somehow, Google's BERT\nmodel fails to find that answer.",
    "start": "2144300",
    "end": "2152850"
  },
  {
    "text": "Perhaps it's partly\nbecause there's the extra stuff in the middle\nthere, adopted by Mexican movie",
    "start": "2152850",
    "end": "2157859"
  },
  {
    "text": "actress. And so it says,\nno, this sentence doesn't answer the question.",
    "start": "2157860",
    "end": "2164150"
  },
  {
    "text": "So you can hope to\ndo better than that.  OK, so there's this choice\nabout doing either the default",
    "start": "2164150",
    "end": "2171910"
  },
  {
    "text": "or custom final project. And the overall steps have\nbeen that about half the people",
    "start": "2171910",
    "end": "2180610"
  },
  {
    "text": "do each. So there's no sort of\nclear winner of a choice.",
    "start": "2180610",
    "end": "2185920"
  },
  {
    "text": "In terms of thinking about\nwhat you should choose, I mean, I think the default\nfinal project is great",
    "start": "2185920",
    "end": "2194650"
  },
  {
    "text": "if you have limited experience\nwith doing research, if you don't have any\nclear idea of what",
    "start": "2194650",
    "end": "2200020"
  },
  {
    "text": "you would want to do with\na custom final project, if you think it would be good to\nhave guidance and a clear goal",
    "start": "2200020",
    "end": "2206890"
  },
  {
    "text": "to work towards-- indeed, we give\nyou a leaderboard so you can compete against\nthe other people doing",
    "start": "2206890",
    "end": "2212049"
  },
  {
    "text": "the default final project-- then you should do the\ndefault final project.",
    "start": "2212050",
    "end": "2217060"
  },
  {
    "text": "It gives you guidance,\nscaffolding, clear goal posts. I mean, and in particular,\njust to sort of try and give",
    "start": "2217060",
    "end": "2226330"
  },
  {
    "text": "a sharper edge to this, I\nmean, the fact of the matter is every year there\nare few people who",
    "start": "2226330",
    "end": "2234580"
  },
  {
    "text": "do a custom final project. And when we're grading\nthem, we look at this custom",
    "start": "2234580",
    "end": "2240730"
  },
  {
    "text": "final project, and we\nsay, huh, this just looks pretty lame compared\nto what people are doing",
    "start": "2240730",
    "end": "2248740"
  },
  {
    "text": "in the default final project. And that's a bad state to be in. But if you're doing the\ncustom final project,",
    "start": "2248740",
    "end": "2256269"
  },
  {
    "text": "you want to have a\nsort of a clear thing that you thought out\nthat is interesting,",
    "start": "2256270",
    "end": "2261850"
  },
  {
    "text": "so it'll seem as or more\ninteresting than the default final project.",
    "start": "2261850",
    "end": "2267250"
  },
  {
    "text": "And if you don't think\nyou've got such a thing, you're actually better off\ndoing the default final project. ",
    "start": "2267250",
    "end": "2274820"
  },
  {
    "text": "Why should you do the\ncustom final project? Well, if you have some\nresearch projects, possibly",
    "start": "2274820",
    "end": "2280849"
  },
  {
    "text": "something you've\nalready been working on, or any rate, something\nthat you think, oh, this would be\ngreat to do, which",
    "start": "2280850",
    "end": "2287540"
  },
  {
    "text": "there are two requirements\nfor our project. It has to substantively\ninvolve human language,",
    "start": "2287540",
    "end": "2293060"
  },
  {
    "text": "and it has to substantively\ninvolve neural networks. So that doesn't mean it has\nto be fully about those.",
    "start": "2293060",
    "end": "2298880"
  },
  {
    "text": "If you want to do a language\nand vision project, that's fine. Or if you want to compare neural\nnetworks versus other machine",
    "start": "2298880",
    "end": "2307100"
  },
  {
    "text": "learning methods on some\nproblem, that's fine. But you do have to, in your\nproject, substantively use",
    "start": "2307100",
    "end": "2314119"
  },
  {
    "text": "both of those things. Yeah, so if you'd like to\nsort of design your own thing and come up with something\ndifferent on your own,",
    "start": "2314120",
    "end": "2321440"
  },
  {
    "text": "or just you don't like\nquestion answering, or if you basically want\nmore experience of going",
    "start": "2321440",
    "end": "2328010"
  },
  {
    "text": "through the whole process of\ntrying to find a good research goal, finding data and\ntools to explore it,",
    "start": "2328010",
    "end": "2333620"
  },
  {
    "text": "work it out on your own,\nthen doing the custom final project is a great choice.",
    "start": "2333620",
    "end": "2338870"
  },
  {
    "text": " OK, either way, the\nsteps that you go through",
    "start": "2338870",
    "end": "2347600"
  },
  {
    "text": "are the following. So the first thing, which\nyou've got until Feb 16 for,",
    "start": "2347600",
    "end": "2354260"
  },
  {
    "text": "is to write a project proposal,\nwhich is 3 to [INAUDIBLE]",
    "start": "2354260",
    "end": "2363170"
  },
  {
    "text": "So some of the details vary\non the kind of project.",
    "start": "2363170",
    "end": "2368690"
  },
  {
    "text": "So you need to decide\non the research topic for your project, which is\nkind of easy if you're doing",
    "start": "2368690",
    "end": "2376250"
  },
  {
    "text": "the default final project. It's either A or\nB. But we then want",
    "start": "2376250",
    "end": "2381380"
  },
  {
    "text": "you to choose one\nresearch paper that's relevant to your final project,\nread it, and then learn",
    "start": "2381380",
    "end": "2389900"
  },
  {
    "text": "some stuff from it that\nyou can write about. And we want you to\nwrite about what your plan is as to\nwhat you're going",
    "start": "2389900",
    "end": "2396380"
  },
  {
    "text": "to do about the final project. And that will include\ndescribing things as needed,",
    "start": "2396380",
    "end": "2401580"
  },
  {
    "text": "such as the data\nand the evaluation. And, again, this is especially\nimportant for custom",
    "start": "2401580",
    "end": "2407340"
  },
  {
    "text": "final projects. Might be kind of\nobvious if you're doing a default final project.",
    "start": "2407340",
    "end": "2415430"
  },
  {
    "text": "And so typically, if you're\ndoing a default final project, this should be three pages.",
    "start": "2415430",
    "end": "2421220"
  },
  {
    "text": "And if you're doing a\ncustom final project, this should be four pages. So there are two parts of that.",
    "start": "2421220",
    "end": "2428080"
  },
  {
    "text": "The first part of it is writing\nabout the paper you read. And so that part is two pages.",
    "start": "2428080",
    "end": "2435220"
  },
  {
    "text": "And so there are longer\nform of the instructions. But we want you to read\nand think about this paper.",
    "start": "2435220",
    "end": "2442359"
  },
  {
    "text": "What are its novel\ncontributions? Is that an idea that could\nbe employed in other ways?",
    "start": "2442360",
    "end": "2449109"
  },
  {
    "text": "Are there things that\ndidn't really do well? How does it compare\nto other ways",
    "start": "2449110",
    "end": "2454690"
  },
  {
    "text": "that people have\napproached this problem? Does it sort of-- even though it did\nthings one way,",
    "start": "2454690",
    "end": "2461110"
  },
  {
    "text": "does it actually\nsuggest ideas that really could do things a\ndifferent way that might even be better?",
    "start": "2461110",
    "end": "2466880"
  },
  {
    "text": "And so we want you to write\nthis 2 page paper summary,",
    "start": "2466880",
    "end": "2472670"
  },
  {
    "text": "which we will grade. And, respectively, we're going\nto grade that on how good a job",
    "start": "2472670",
    "end": "2478180"
  },
  {
    "text": "you do at thinking about\nanalyzing and having critical comments on this paper.",
    "start": "2478180",
    "end": "2485060"
  },
  {
    "text": "So then the other half, which\nmight be longer or shorter, depending on whether\nit's custom or default,",
    "start": "2485060",
    "end": "2491410"
  },
  {
    "text": "is to propose what\nyou're going to do. And, really, this\npart is formative.",
    "start": "2491410",
    "end": "2499810"
  },
  {
    "text": "So formative means we're not\ngoing to grade it harshly. We want you to write\nit so we can help you.",
    "start": "2499810",
    "end": "2506180"
  },
  {
    "text": "So we want you to sort of\nbullet through what you're going to do, and us\nhave an opportunity",
    "start": "2506180",
    "end": "2512170"
  },
  {
    "text": "to say, no, that's\nunrealistically large, or that sounds\ninsufficiently ambitious,",
    "start": "2512170",
    "end": "2517720"
  },
  {
    "text": "or this isn't going\nto work unless you can get more data than\nyou actually have, and things like that.",
    "start": "2517720",
    "end": "2524210"
  },
  {
    "text": "So this is mainly\njust the feedback. But on the things\nto think about,",
    "start": "2524210",
    "end": "2531849"
  },
  {
    "text": "project plans that\nare lacking, they're generally just lacking\nconcreteness in nuts and bolts",
    "start": "2531850",
    "end": "2538300"
  },
  {
    "text": "ways, which are\nessential to being able to do a good final project\nin a short amount of time.",
    "start": "2538300",
    "end": "2545120"
  },
  {
    "text": "So you need to have\nfound good data, or have a realistic\nplan, so that",
    "start": "2545120",
    "end": "2551480"
  },
  {
    "text": "shouldn't be plant,\na realistic plan to be able to collect that. You need to have a\nrealistic way that you",
    "start": "2551480",
    "end": "2557950"
  },
  {
    "text": "can evaluate your work. You need to have thought about\nwhat kinds of experiments",
    "start": "2557950",
    "end": "2563320"
  },
  {
    "text": "you could run so you could show\nwhether your model is working well or badly. And we'll be sort of looking\nto see whether you've",
    "start": "2563320",
    "end": "2570160"
  },
  {
    "text": "done these things. OK, so then a couple\nof weeks after that",
    "start": "2570160",
    "end": "2576720"
  },
  {
    "text": "is the project milestone,\nwhich is, again, from everyone.",
    "start": "2576720",
    "end": "2581920"
  },
  {
    "text": "This is a progress report. And this, again, just meant\nto help keep you on track. So you should be more\nthan halfway done.",
    "start": "2581920",
    "end": "2589230"
  },
  {
    "text": "You should have been able-- in nearly all cases,\nI'll talk more",
    "start": "2589230",
    "end": "2594450"
  },
  {
    "text": "about this in a minute,\nnearly all cases, you should already have\nbeen able to implement some baseline system and have\nsome initial experimental",
    "start": "2594450",
    "end": "2603269"
  },
  {
    "text": "results to show. You might still be\nworking on your main model and have nothing to show\nfor it, but hopefully you",
    "start": "2603270",
    "end": "2609900"
  },
  {
    "text": "have at least set an\nobvious simple baseline to know how well it works. It's this thing, and I built\nthat, and I have some numbers.",
    "start": "2609900",
    "end": "2617070"
  },
  {
    "text": "And then we want an\nupdate on how you plan to spend the rest of your time. And, again, a lot of this\nis about us giving you",
    "start": "2617070",
    "end": "2624000"
  },
  {
    "text": "more feedback as to\nwhat are the best things you can do for the\nfinal two weeks of the class.",
    "start": "2624000",
    "end": "2631570"
  },
  {
    "text": "And then at the end, there's\nwriting a final project.",
    "start": "2631570",
    "end": "2637900"
  },
  {
    "text": "And for this, the\nquality of your writeup",
    "start": "2637900",
    "end": "2644809"
  },
  {
    "text": "is really, really important\nto your grade, right? By and large, we're going to\nevaluate how good your project",
    "start": "2644810",
    "end": "2650960"
  },
  {
    "text": "is by reading the writeup. So make sure you\nbudget sufficient time",
    "start": "2650960",
    "end": "2657020"
  },
  {
    "text": "to actually do a good\njob of [INAUDIBLE]",
    "start": "2657020",
    "end": "2664040"
  },
  {
    "text": "You can look at good projects\nfrom past years that are all up on the CS224N website.",
    "start": "2664040",
    "end": "2670250"
  },
  {
    "text": "2020 was kind of a mess because\nof the start of the pandemic. So, of course, we\nshould look at 2019.",
    "start": "2670250",
    "end": "2676790"
  },
  {
    "text": "So there's even better\nmodels of what you should do. So the details vary, but\nthis is sort of a picture",
    "start": "2676790",
    "end": "2684830"
  },
  {
    "text": "to have in mind, and normally\nwhat the writeup looks like. So eight pages, you want to have\nan abstract and introduction",
    "start": "2684830",
    "end": "2692870"
  },
  {
    "text": "to the paper, you want to\ntalk about related prior work, you want to present\nthe model you're",
    "start": "2692870",
    "end": "2698750"
  },
  {
    "text": "using, you want to talk\nabout the data you're using, talk about your experiments,\nwhat the results are,",
    "start": "2698750",
    "end": "2705589"
  },
  {
    "text": "what you learned about it. The details may vary. Some papers there's less\nto say about the model",
    "start": "2705590",
    "end": "2712595"
  },
  {
    "text": "and there's more to say\nabout the experiment. So you can sort of move\nthings around a bit, but roughly something like this.",
    "start": "2712595",
    "end": "2718550"
  },
  {
    "text": " Okay, so I now want\nto go on and say",
    "start": "2718550",
    "end": "2725030"
  },
  {
    "text": "a bit about research\nand practical things that we need to do.",
    "start": "2725030",
    "end": "2730230"
  },
  {
    "text": "A lot of these things are\nrelevant to everybody. At any rate, there\nare things that you",
    "start": "2730230",
    "end": "2736070"
  },
  {
    "text": "should know a little bit about. So the very first one is\nfinding research topics,",
    "start": "2736070",
    "end": "2742290"
  },
  {
    "text": "which is sort of especially\nvital to custom final projects.",
    "start": "2742290",
    "end": "2748170"
  },
  {
    "text": "Yeah, so really\nfor all of science, there's sort of only\ntwo ways that you",
    "start": "2748170",
    "end": "2753740"
  },
  {
    "text": "can have a research project. That one way is doing\ndomain science, where",
    "start": "2753740",
    "end": "2760520"
  },
  {
    "text": "you start with a problem\nof interest in the domain, such as, how can I\nbuild a decent Cherokee",
    "start": "2760520",
    "end": "2767240"
  },
  {
    "text": "to English machine\ntranslation system? And you work on\nfinding ways to be able to do it better\nthan people currently",
    "start": "2767240",
    "end": "2774680"
  },
  {
    "text": "know how to do, or to understand\nthe problem better than people currently understand it.",
    "start": "2774680",
    "end": "2781010"
  },
  {
    "text": "And the other way is to take a\nmethodological approach where you start off with some method\nor approach of interest,",
    "start": "2781010",
    "end": "2788750"
  },
  {
    "text": "and then you work out good\nways to extend or improve it, or new ways to apply it.",
    "start": "2788750",
    "end": "2794010"
  },
  {
    "text": "And so basically you're\ndoing one of those. So there are effectively\ndifferent kinds",
    "start": "2794010",
    "end": "2801350"
  },
  {
    "text": "of projects you can do. This is not an exhaustive\nlist, but most projects sort of",
    "start": "2801350",
    "end": "2807440"
  },
  {
    "text": "fall into one of these\nbuckets, very non-uniformly.",
    "start": "2807440",
    "end": "2812569"
  },
  {
    "text": "So the most common type is you\nfind some application or task",
    "start": "2812570",
    "end": "2818480"
  },
  {
    "text": "of interest, and you explore\nhow to approach and solve it as well as possible.",
    "start": "2818480",
    "end": "2824660"
  },
  {
    "text": "Often, that's using\nexisting models and trying out different\noptions, and things like that.",
    "start": "2824660",
    "end": "2830420"
  },
  {
    "text": "A second kind is you can\ntake some relatively complex neural architecture, i.e.\nit has to be something more",
    "start": "2830420",
    "end": "2838190"
  },
  {
    "text": "complex than we've built\nfor assignments 1 through 5. You implement it,\nand get something",
    "start": "2838190",
    "end": "2846440"
  },
  {
    "text": "that works on some data. And while you are doing\nsomething fairly complex,",
    "start": "2846440",
    "end": "2852980"
  },
  {
    "text": "it's fine just to implement\nit and get it to work.",
    "start": "2852980",
    "end": "2857990"
  },
  {
    "text": "But if there's some way that\nyou can tweak it, and try and do something different, and\nsee if that makes that even",
    "start": "2857990",
    "end": "2863359"
  },
  {
    "text": "better, or maybe\nit makes it worse, and you can do\nexperiments either way, that's sort of even better.",
    "start": "2863360",
    "end": "2870560"
  },
  {
    "text": "There are then other kinds\nof projects that you can do. So another kind of project\nis an analysis project.",
    "start": "2870560",
    "end": "2877580"
  },
  {
    "text": "So you can take\nsome model that's an existing model and\nyou can poke at it",
    "start": "2877580",
    "end": "2882770"
  },
  {
    "text": "and find out things\nabout what it knows. So you could take anything,\neven as simple as word vectors,",
    "start": "2882770",
    "end": "2889910"
  },
  {
    "text": "and poke at them. And you can find out\nthings like, well, how much do word vectors\nknow about word sensors?",
    "start": "2889910",
    "end": "2896670"
  },
  {
    "text": "Well, sometimes the same word\nis both a noun and a verb. Can you tell-- can\nyou sort of get",
    "start": "2896670",
    "end": "2902540"
  },
  {
    "text": "that difference similarities\nout of the word vectors? So analysis projects\nare perfectly fine,",
    "start": "2902540",
    "end": "2909529"
  },
  {
    "text": "often interesting. And then of these five kinds,\nthe rarest kind of project,",
    "start": "2909530",
    "end": "2914930"
  },
  {
    "text": "we've had a couple, is\na theoretical project. So there's a lot of\ninteresting deep learning",
    "start": "2914930",
    "end": "2920900"
  },
  {
    "text": "theory is to how do these\nthings work and why, and what would it take\nto make it work better?",
    "start": "2920900",
    "end": "2927930"
  },
  {
    "text": "And so you can work\non sort of getting any kind of non-trivial\nproperty or understanding",
    "start": "2927930",
    "end": "2935390"
  },
  {
    "text": "of a deep learning model. Here are just quickly a couple\nof examples of some projects",
    "start": "2935390",
    "end": "2943190"
  },
  {
    "text": "from the past years of\n224N just to give you a couple of examples\nof things like this.",
    "start": "2943190",
    "end": "2949724"
  },
  {
    "text": "So deep poetry, this\nwas doing generation",
    "start": "2949725",
    "end": "2955040"
  },
  {
    "text": "of Shakespearean sonnets. So the way it was\nbeing generated was with [INAUDIBLE] word with\nnow for machine translation.",
    "start": "2955040",
    "end": "2967910"
  },
  {
    "text": "But it had interesting\ndifferences, because if you wanted\nto generate poetry, you need to know about\nmetrical structure, and rhyme,",
    "start": "2967910",
    "end": "2974780"
  },
  {
    "text": "and so they're working\nout how to add components to the model that could do that.",
    "start": "2974780",
    "end": "2980310"
  },
  {
    "text": "Here is someone who\nwas implementing a complex neural model. So there's been a line\nof work at DeepMind",
    "start": "2980310",
    "end": "2988840"
  },
  {
    "text": "on trying to build\ngeneral purpose computers with\nneural architectures,",
    "start": "2988840",
    "end": "2994200"
  },
  {
    "text": "so there's first of all\nneural Turing machines, and then a subsequent\nmodel called",
    "start": "2994200",
    "end": "2999490"
  },
  {
    "text": "the differential\n[INAUDIBLE] neural computer. And they haven't released the\ncode of those open source.",
    "start": "2999490",
    "end": "3006720"
  },
  {
    "text": "And so Carol decided\nthat she was going to-- she was going to implement\ndifferential neural computers",
    "start": "3006720",
    "end": "3013740"
  },
  {
    "text": "and get them to work. This was a very dangerous\nidea, because we were in week 10 of the\nclass, and she still",
    "start": "3013740",
    "end": "3021090"
  },
  {
    "text": "hadn't gotten them\nto work at all. But luckily, she got it\ntogether at the last moment",
    "start": "3021090",
    "end": "3026430"
  },
  {
    "text": "and actually got\nher model working, and was able to run it and get\nresults on some of the problems",
    "start": "3026430",
    "end": "3031619"
  },
  {
    "text": "that DeepMind was also\nshowing results on. And so she pulled the\nrabbit out of the hat,",
    "start": "3031620",
    "end": "3037380"
  },
  {
    "text": "and had an enormous success. And we were very impressed\nthat she managed to do that.",
    "start": "3037380",
    "end": "3043880"
  },
  {
    "text": "Here's-- so sometimes final\nprojects can become papers. Here is a final\nproject that became",
    "start": "3043880",
    "end": "3052700"
  },
  {
    "text": "paper published at the top\nmachine learning conference. This is from a few years\nago, this one from 2017.",
    "start": "3052700",
    "end": "3059150"
  },
  {
    "text": "It actually has a couple\nof fairly simple ideas. There are ideas that, at that\ntime, people weren't using,",
    "start": "3059150",
    "end": "3067430"
  },
  {
    "text": "and these two people proved\nworked and improved things. And they got a conference\npaper out of it.",
    "start": "3067430",
    "end": "3074569"
  },
  {
    "text": "So I'll just mention\none of them now. So if you think about our\ncurrent neural network language",
    "start": "3074570",
    "end": "3080420"
  },
  {
    "text": "models, they have both-- for words and encoding into\ndistributed vectors, where",
    "start": "3080420",
    "end": "3087755"
  },
  {
    "text": "at the other end, the\nsoftmax matrix basically hides inside it, just\nlike for our word",
    "start": "3087755",
    "end": "3096230"
  },
  {
    "text": "vectors, our word\nvector for each word. And then you are sort of\ndeciding the probabilities",
    "start": "3096230",
    "end": "3102440"
  },
  {
    "text": "of generating\ndifferent words based on similarities between\nthe query vector",
    "start": "3102440",
    "end": "3110750"
  },
  {
    "text": "and each of those words. So their idea was, look,\nmaybe we'd actually be able to build better language\nmodels if we tied together",
    "start": "3110750",
    "end": "3119000"
  },
  {
    "text": "the word-embedding\nmatrix and the matrix used to project the RNN output.",
    "start": "3119000",
    "end": "3126260"
  },
  {
    "text": "And, actually, they showed that\nyou can get significant gains by doing that.",
    "start": "3126260",
    "end": "3131760"
  },
  {
    "text": "And so now, it's basically\nsort of now become standard,",
    "start": "3131760",
    "end": "3137150"
  },
  {
    "text": "if you're wanting to build\nstrong neural language models, that you want to tie those two\nsets of parameters together.",
    "start": "3137150",
    "end": "3143270"
  },
  {
    "text": "So that was pretty cool. Here's some more\nsystems of the project.",
    "start": "3143270",
    "end": "3149150"
  },
  {
    "text": "So I'll mention\nthis again later, but something people have been\ninterested in is how can you",
    "start": "3149150",
    "end": "3155030"
  },
  {
    "text": "squish neural nets\nand make them small so you can run them on a\nregular laptop, or a smaller",
    "start": "3155030",
    "end": "3164300"
  },
  {
    "text": "device like a mobile phone. And so these people\nworked on, how",
    "start": "3164300",
    "end": "3171590"
  },
  {
    "text": "could you do quantization\nof neural networks down to sort of one or\ntwo bits per parameter",
    "start": "3171590",
    "end": "3178190"
  },
  {
    "text": "and still get good results. So this is sort\nof an example, has",
    "start": "3178190",
    "end": "3184339"
  },
  {
    "text": "to involve neural nets\nand human language, it's sort of a way in\nwhich you could say, this didn't involve\nhuman language at all,",
    "start": "3184340",
    "end": "3191930"
  },
  {
    "text": "because this was really\nabout quantizing neural nets. But effectively, where\nwe draw the line is",
    "start": "3191930",
    "end": "3197780"
  },
  {
    "text": "we say you will be\nallowed to do this, providing the task\nand the model you",
    "start": "3197780",
    "end": "3204049"
  },
  {
    "text": "use to demonstrate success or\nfailure as a natural language task.",
    "start": "3204050",
    "end": "3209420"
  },
  {
    "text": "And so they did both\nword similarity tasks,",
    "start": "3209420",
    "end": "3214520"
  },
  {
    "text": "and question answering\ntasks of seeing how well the system performed\nafter doing the compression",
    "start": "3214520",
    "end": "3220820"
  },
  {
    "text": "by quantization.  How can you find a good\nplace for a project",
    "start": "3220820",
    "end": "3227720"
  },
  {
    "text": "if you don't have a good idea? One place is to sort of\nlook at recent papers.",
    "start": "3227720",
    "end": "3234510"
  },
  {
    "text": "So for most NLP papers, they\nappear in the ACL anthology. Also look at major machine\nlearning conferences.",
    "start": "3234510",
    "end": "3241760"
  },
  {
    "text": "Most past 224N projects are\nup on the class website. You can look through them.",
    "start": "3241760",
    "end": "3247290"
  },
  {
    "text": "It's just general\narXiv.org preprint servers. But perhaps even\nbetter is looking",
    "start": "3247290",
    "end": "3254330"
  },
  {
    "text": "for an interesting\nproblem in the world. So Hal Varian is\nan an economist.",
    "start": "3254330",
    "end": "3259640"
  },
  {
    "text": "Was it [INAUDIBLE] And he wrote\nthis cool paper that I often",
    "start": "3259640",
    "end": "3266180"
  },
  {
    "text": "recommend to students,\nHow to Build an Economic Model in Your Spare Time.",
    "start": "3266180",
    "end": "3272720"
  },
  {
    "text": "And the paper isn't really\nabout economic models. It's about how to do research.",
    "start": "3272720",
    "end": "3279230"
  },
  {
    "text": "And what he says right\nat the beginning, section 1 of the paper\nis called Getting Ideas.",
    "start": "3279230",
    "end": "3286370"
  },
  {
    "text": "And he writes, \"The way to get\nideas, that's the question. Most graduate\nstudents are convinced",
    "start": "3286370",
    "end": "3293030"
  },
  {
    "text": "that the way you get ideas\nis to read journal papers. In my experience,\njournals really",
    "start": "3293030",
    "end": "3298700"
  },
  {
    "text": "have a very good source\nof original ideas. You can get lots of things\nfrom journal papers, technique,",
    "start": "3298700",
    "end": "3305030"
  },
  {
    "text": "insight, even truth. But most of the\ntime, you will only get somebody else's ideas.\"",
    "start": "3305030",
    "end": "3311420"
  },
  {
    "text": "And so he talks further\nabout better ways of getting ideas by thinking\nabout problems in the world.",
    "start": "3311420",
    "end": "3318454"
  },
  {
    "text": " So Arxiv is this huge\nrepository of papers.",
    "start": "3318455",
    "end": "3324770"
  },
  {
    "text": "It's hard to navigate. There are various tools that\ncan make it easier to navigate.",
    "start": "3324770",
    "end": "3330110"
  },
  {
    "text": "One of them is Arxiv\nSanity Preserver, which is a website that was\nwritten by Andrej Karpahty, who",
    "start": "3330110",
    "end": "3337380"
  },
  {
    "text": "was the original person\nwho constructed and taught the CS231N course, still\na good thing to use.",
    "start": "3337380",
    "end": "3346880"
  },
  {
    "text": "There are lots of leaderboards\nfor different tasks in NLP. So a place you can find\ntasks and work on tasks",
    "start": "3346880",
    "end": "3355670"
  },
  {
    "text": "is looking at leaderboards. So Papers With Code\nand NLPProgress",
    "start": "3355670",
    "end": "3360829"
  },
  {
    "text": "are two good general\nsources of leaderboards. There are also then lots\nof more specialized ones",
    "start": "3360830",
    "end": "3367369"
  },
  {
    "text": "for particular tasks. I wanted to then,\nfor research topics--",
    "start": "3367370",
    "end": "3373460"
  },
  {
    "text": "this material, these\nnext four slides, are brand new for this year.",
    "start": "3373460",
    "end": "3378650"
  },
  {
    "text": "I wanted to sort of\njust say a few words about the funny,\nsomewhat different time",
    "start": "3378650",
    "end": "3387440"
  },
  {
    "text": "that we're in, where there's\nsort of old Deep Learning NLP,",
    "start": "3387440",
    "end": "3392540"
  },
  {
    "text": "and new Deep Learning NLP. So in the early days of\nthe Deep Learning revival--",
    "start": "3392540",
    "end": "3398960"
  },
  {
    "text": "which I'll call 2010 to 2018,\nbecause 2010 is the year I started doing Deep\nLearning for NLP.",
    "start": "3398960",
    "end": "3407090"
  },
  {
    "text": "Most of the work was in\ndefining and exploring better Deep Learning architectures.",
    "start": "3407090",
    "end": "3414300"
  },
  {
    "text": "So the typical paper was, I can\nimprove a summarization system",
    "start": "3414300",
    "end": "3420170"
  },
  {
    "text": "by not only using\nthe kind of attention I just explained,\nbut I could add an extra kind of\nattention, which I'll",
    "start": "3420170",
    "end": "3426230"
  },
  {
    "text": "use as a copying mechanism. So I'll do additional\nattention calculations to work out source words\nwhich I could copy verbatim",
    "start": "3426230",
    "end": "3436820"
  },
  {
    "text": "to the output, rather than\nhaving to regenerate them through the standard\nkind of softmax. That's just one\nexample of millions.",
    "start": "3436820",
    "end": "3443680"
  },
  {
    "text": "But you were changing the\narchitecture of the model, and make things better.",
    "start": "3443680",
    "end": "3450090"
  },
  {
    "text": "That's what a lot of good\nCS224N projects did too.",
    "start": "3450090",
    "end": "3455360"
  },
  {
    "text": "And, in particular,\nfor one branch of the default final\nproject, the one where",
    "start": "3455360",
    "end": "3461329"
  },
  {
    "text": "you start with the baseline\nSQuAD question answering system and work out ways\nto make it better",
    "start": "3461330",
    "end": "3468109"
  },
  {
    "text": "by adding more\nneural architecture, it's essentially doing this.",
    "start": "3468110",
    "end": "3474860"
  },
  {
    "text": "But, actually, if you\nlook out in the research",
    "start": "3474860",
    "end": "3480110"
  },
  {
    "text": "world of the last\ncouple of years, the sad truth is that\napproach is dead.",
    "start": "3480110",
    "end": "3486680"
  },
  {
    "text": "It's not actually dead. I mean, that's too strong.",
    "start": "3486680",
    "end": "3492290"
  },
  {
    "text": "But that's now much more\ndifficult and rarer to do.",
    "start": "3492290",
    "end": "3498620"
  },
  {
    "text": "If you look at the sort of\nlast three years of NLP-- and this is true both for people\nwho are implementing production",
    "start": "3498620",
    "end": "3508910"
  },
  {
    "text": "NLP systems at\ncompanies and for people who are doing research in NLP--",
    "start": "3508910",
    "end": "3516530"
  },
  {
    "text": "that most work is\nmore like this. There are these enormously\ngood big pre-trained language",
    "start": "3516530",
    "end": "3525890"
  },
  {
    "text": "models, BERT, GPT-2,\nroBERTa, XLNet, T5, they exist on the web,\nand you can download them",
    "start": "3525890",
    "end": "3537710"
  },
  {
    "text": "with one Python command, and\nthey give you a great basis for doing most NLP tasks.",
    "start": "3537710",
    "end": "3544759"
  },
  {
    "text": "We'll learn all\nabout them next week. So you're not actually\nstarting from scratch",
    "start": "3544760",
    "end": "3550310"
  },
  {
    "text": "defining your own\nmodel architecture and playing with variants. You're saying, I'm going\nto be using RoBERTa",
    "start": "3550310",
    "end": "3556299"
  },
  {
    "text": "[INAUDIBLE] further fine\ntuning it for your task,",
    "start": "3556300",
    "end": "3564290"
  },
  {
    "text": "doing domain adaptation,\nand things like that.",
    "start": "3564290",
    "end": "3570160"
  },
  {
    "text": "So this is still very\nquickly of our 2021 NLP for all of your practical\nprojects and industry needs,",
    "start": "3570160",
    "end": "3580109"
  },
  {
    "text": "this is basically the formula\nthat which you should probably use. There's this enormously\ngreat library",
    "start": "3580110",
    "end": "3587400"
  },
  {
    "text": "by the company Huggingface. You install it, pip\ninstall transformers.",
    "start": "3587400",
    "end": "3593370"
  },
  {
    "text": "And then that gives you\na great implementation of the transformers we\nlearn about next week.",
    "start": "3593370",
    "end": "3598380"
  },
  {
    "text": "And then, effectively,\nwhat you're doing is something like my code below. This code doesn't quite run.",
    "start": "3598380",
    "end": "3604170"
  },
  {
    "text": "It's missing some\npieces if you try it. But the sort of pieces are, you\nload a big pre-trained language",
    "start": "3604170",
    "end": "3612150"
  },
  {
    "text": "model. So here I'm loading the\nbert-base-uncased model. It turns out these models all\nhave very special tokenizers.",
    "start": "3612150",
    "end": "3620609"
  },
  {
    "text": "You'll hear about those\nnext week, as well. So we grab the tokenizer for it. Then we are going to fine\ntune it for our task.",
    "start": "3620610",
    "end": "3628920"
  },
  {
    "text": "Like maybe I'm going to be\nworking with legal data, so I want to take\nthe general model and fine tune it to\nunderstand legal data better.",
    "start": "3628920",
    "end": "3637410"
  },
  {
    "text": "And then I've got\nsomething I want to do, like question answering,\nor perhaps here I'm",
    "start": "3637410",
    "end": "3643740"
  },
  {
    "text": "using\nBertForSequenceClassification. Maybe the task I want\nto do is label mentions",
    "start": "3643740",
    "end": "3650789"
  },
  {
    "text": "of sections of legal code. So at that point, I'm\nthen going to sort",
    "start": "3650790",
    "end": "3657000"
  },
  {
    "text": "of fine tune for\nthat particular task, and then run on that task.",
    "start": "3657000",
    "end": "3664450"
  },
  {
    "text": "And so this is kind\nof what we're doing, and we're just\nsort of doing stuff",
    "start": "3664450",
    "end": "3669540"
  },
  {
    "text": "on top of a pre-existing model. So a lot of what\nis exciting now is",
    "start": "3669540",
    "end": "3675510"
  },
  {
    "text": "problems that work within\nor around that world.",
    "start": "3675510",
    "end": "3681980"
  },
  {
    "text": "So and you might\nthink about that, and look at recent\npapers and things that",
    "start": "3681980",
    "end": "3688690"
  },
  {
    "text": "could be done about that world. Well, one of them-- and this is\nthe other half of the default final project--",
    "start": "3688690",
    "end": "3694900"
  },
  {
    "text": "is we still have this problem\nof robustness to domain shift, that these models are trained\non normally one kind of text,",
    "start": "3694900",
    "end": "3703790"
  },
  {
    "text": "but we often want to use it\nfor different kinds of text, including often when you're\ntalking about problem",
    "start": "3703790",
    "end": "3709990"
  },
  {
    "text": "or application\nspecific questions, there are domains for which\nyou don't have much data.",
    "start": "3709990",
    "end": "3715780"
  },
  {
    "text": "How can you do that well? If you've got one of these big\nmodels, well, in some sense",
    "start": "3715780",
    "end": "3722420"
  },
  {
    "text": "they're good. But are they actually\nrobust that they",
    "start": "3722420",
    "end": "3727430"
  },
  {
    "text": "work for all of the different\nthings in the space you'd like them to work on? So you're not only interested in\nbasic accuracy, but robustness.",
    "start": "3727430",
    "end": "3735980"
  },
  {
    "text": "So this RobustnessGym was a\nvery recently announced project",
    "start": "3735980",
    "end": "3741190"
  },
  {
    "text": "by a PhD student at\nStanford, Karan Goel. It's actually testing the\nrobustness of NLP models.",
    "start": "3741190",
    "end": "3747710"
  },
  {
    "text": "And they've got some\nstuff built into it, but there's lots of stuff\nthat isn't built into it. If someone would like to sort\nof choose some NLP problem,",
    "start": "3747710",
    "end": "3756110"
  },
  {
    "text": "whether it's dependency\nparsing or something else like summarization, and build\nout robustness tests for it,",
    "start": "3756110",
    "end": "3763040"
  },
  {
    "text": "that could be kind\nof interesting to do. A bunch of other things,\nyou can look at issues",
    "start": "3763040",
    "end": "3770960"
  },
  {
    "text": "whether there's bias\nembedded in these models. How can you explain\nwhat they're doing?",
    "start": "3770960",
    "end": "3776960"
  },
  {
    "text": "How can you use\ndata augmentation? There's been an\nenormous amount of work on using data augmentation to\nimprove model resources, lots",
    "start": "3776960",
    "end": "3785780"
  },
  {
    "text": "of different areas. I'll just mention two other\nthings and then I'll go on.",
    "start": "3785780",
    "end": "3793849"
  },
  {
    "text": "So there's been a ton of work\non scaling models up and down.",
    "start": "3793850",
    "end": "3799100"
  },
  {
    "text": "So these big, pre-trained\nlanguage models are already big.",
    "start": "3799100",
    "end": "3805579"
  },
  {
    "text": "So the fact of the\nmatter is, it's just not possible for the\ntime and resources",
    "start": "3805580",
    "end": "3811430"
  },
  {
    "text": "that you have in\n224N to be thinking, I'm going to be building by\nmyself big pre-trained models.",
    "start": "3811430",
    "end": "3820549"
  },
  {
    "text": "All you can do is use\nones that already exist. But on the other hand, people\nare actually interested in,",
    "start": "3820550",
    "end": "3829369"
  },
  {
    "text": "can you build very small models\nthat still work pretty well? And there are lots of examples.",
    "start": "3829370",
    "end": "3836660"
  },
  {
    "text": "But actually one that was done\nrecently for question answering was that there was a bake off\ncompetition at the last NeurIPS",
    "start": "3836660",
    "end": "3844250"
  },
  {
    "text": "which was called Efficient QA. And one of the\ndivisions of this bake",
    "start": "3844250",
    "end": "3849320"
  },
  {
    "text": "off was, can you build a\nperformant question answering",
    "start": "3849320",
    "end": "3854990"
  },
  {
    "text": "system that will\nrun [INAUDIBLE].. ",
    "start": "3854990",
    "end": "3861095"
  },
  {
    "text": "And, well, that's actually\na reasonable thing that you could attempt\nfor a final project.",
    "start": "3861095",
    "end": "3866900"
  },
  {
    "text": "Another thing people have\nbeen very interested in is wanting to explore more\nadvanced learning capabilities",
    "start": "3866900",
    "end": "3875900"
  },
  {
    "text": "in neural networks, ideas like\ncompositionality, systematic generalization, fast learning,\nsuch as meta learning work.",
    "start": "3875900",
    "end": "3884480"
  },
  {
    "text": "And a lot of the time,\npeople have investigated these in small domains.",
    "start": "3884480",
    "end": "3890030"
  },
  {
    "text": "So here's a couple of examples\nthat you could go look at, BabyAI and gSCAN.",
    "start": "3890030",
    "end": "3895070"
  },
  {
    "text": "So those could be\ninteresting places you could look for a final project. ",
    "start": "3895070",
    "end": "3902750"
  },
  {
    "text": "OK, other stuff to know\nquickly, probably I can't quite go through all of\nthese slides, and some of them",
    "start": "3902750",
    "end": "3909440"
  },
  {
    "text": "you could just take\nhome and look at. You need data.",
    "start": "3909440",
    "end": "3915040"
  },
  {
    "text": "Now, we actually love it\nif people feel like they can collect their own data. And sometimes there are good\nways to collect your own data.",
    "start": "3915040",
    "end": "3923390"
  },
  {
    "text": "But the reality is a lot of\nthe time, the easiest way",
    "start": "3923390",
    "end": "3928400"
  },
  {
    "text": "to get a fast start\non a final project when you only have a few weeks\nis to make use of an existing",
    "start": "3928400",
    "end": "3933680"
  },
  {
    "text": "data set. And there are lots of\npre-existing data sets. So this data from the\nLinguistic Data Consortium,",
    "start": "3933680",
    "end": "3941780"
  },
  {
    "text": "which is a licensed data. We have licenses at Stanford. You can look through\ntheir catalog and find all the\nthings they have.",
    "start": "3941780",
    "end": "3948470"
  },
  {
    "text": "There are websites\nthat have lots of data. So if you want to do\nmachine translation tasks, you can find lots of\nmachine translation data",
    "start": "3948470",
    "end": "3956450"
  },
  {
    "text": "on this website. Dependency parsing, if you're\nkeen on assignment 3, lots",
    "start": "3956450",
    "end": "3962360"
  },
  {
    "text": "of data on the Universal\nDependencies website. There are now several\nwebsites that are",
    "start": "3962360",
    "end": "3969109"
  },
  {
    "text": "collecting a lot of data sets. So Huggingface has\nalso just recently actually announced\nHuggingface Datasets,",
    "start": "3969110",
    "end": "3975240"
  },
  {
    "text": "which is sort of an\nindex of datasets. And the Paperswithcode\npeople also have Paperswithcode Datasets,\nso you can look at those.",
    "start": "3975240",
    "end": "3984132"
  },
  {
    "text": "There are many more.  Yeah, here's just\na quick summary",
    "start": "3984132",
    "end": "3994360"
  },
  {
    "text": "of thinking about\na research project. So this is just\none example, right?",
    "start": "3994360",
    "end": "4001890"
  },
  {
    "text": "That suppose you\nthink summarization. So that's going from\na longer piece of text to a short summary of it.",
    "start": "4001890",
    "end": "4008440"
  },
  {
    "text": "So what you have to do, you\nneed to find the data set. And, well, probably\nthe easiest thing to do",
    "start": "4008440",
    "end": "4015630"
  },
  {
    "text": "is to use an existing tech\nsummarization data set. You can find several online.",
    "start": "4015630",
    "end": "4021390"
  },
  {
    "text": "But this is why you can\nthink of interesting ways to create your own data. So something that you might have\nnoticed if you look at Twitter",
    "start": "4021390",
    "end": "4028500"
  },
  {
    "text": "is that journalists often\npromote their own stories by, or their newspaper or TV station\ndoes by putting out a tweet.",
    "start": "4028500",
    "end": "4038220"
  },
  {
    "text": "And so that's sort of\nlike found data, which is sort of self summaries. Could you collect\nsome of those and try",
    "start": "4038220",
    "end": "4044250"
  },
  {
    "text": "and learn to generate\ntweets for a news story? ",
    "start": "4044250",
    "end": "4049300"
  },
  {
    "text": "So I'll say a bit\nabout data set hygiene. You want to be careful\nabout working things out",
    "start": "4049300",
    "end": "4054850"
  },
  {
    "text": "so you have train\nsets and test sets. I'll say that in a minute. You want some way to\nevaluate whether you're",
    "start": "4054850",
    "end": "4061540"
  },
  {
    "text": "doing well as you build models. And you pretty much need to have\nan automatic evaluation metric,",
    "start": "4061540",
    "end": "4068380"
  },
  {
    "text": "even though human\nevaluation's great, since you want to\ntrain a bunch of models and see whether they're\nbetter or worse.",
    "start": "4068380",
    "end": "4074050"
  },
  {
    "text": "You normally want an automatic\nmetric that's semi-decent. You should make sure you\nhave some kind of baseline",
    "start": "4074050",
    "end": "4082300"
  },
  {
    "text": "that you want to have some sense\nof whether you're doing well. And so commonly,\nyou, first off, want",
    "start": "4082300",
    "end": "4087760"
  },
  {
    "text": "to implement some simple model\nlike a logistic regression, or just averaging word\nvectors or something,",
    "start": "4087760",
    "end": "4094510"
  },
  {
    "text": "and see how well that works. Because then if you're not\ndoing better than that, that you're actually making\nno progress whatsoever.",
    "start": "4094510",
    "end": "4102068"
  },
  {
    "text": "Then you should implement\nsome neural net model that you think might\nbe good, and see",
    "start": "4102069",
    "end": "4107199"
  },
  {
    "text": "if you can get that to work,\nand have that work well. Make sure you keep\nlooking at your data",
    "start": "4107200",
    "end": "4114759"
  },
  {
    "text": "to see what kind of\nerrors you're making, and think about how you could\nchange the model to avoid them.",
    "start": "4114760",
    "end": "4120520"
  },
  {
    "text": "And then, hopefully,\nyou've got time to try out some model\nvariants and see if they're better or worse.",
    "start": "4120520",
    "end": "4127159"
  },
  {
    "text": "And then that will help\nin having a good project. ",
    "start": "4127160",
    "end": "4132210"
  },
  {
    "text": "Yeah, so quickly, for what\nI call here pots of data.",
    "start": "4132210",
    "end": "4137460"
  },
  {
    "text": "So many public data sets\ncome with a structure where they have\ntrain, dev, and test.",
    "start": "4137460",
    "end": "4144960"
  },
  {
    "text": "And the idea is that you keep\nyour test data until the end,",
    "start": "4144960",
    "end": "4150060"
  },
  {
    "text": "and you only sort\nof do test runs when development is complete,\nor at least almost complete.",
    "start": "4150060",
    "end": "4158370"
  },
  {
    "text": "[INAUDIBLE] Train on the train data, and\nyou evaluate on the dev data.",
    "start": "4158370",
    "end": "4166060"
  },
  {
    "text": "Sometimes you need\neven more data sets. Sometimes you want to do\ntuning of hyperparameters, and you might need a train\nset, a tune set, a dev set,",
    "start": "4166060",
    "end": "4173790"
  },
  {
    "text": "and a test set. If the data set, if it\ndoesn't come pre-split or it doesn't have\nenough pieces,",
    "start": "4173790",
    "end": "4180689"
  },
  {
    "text": "it's sort of your job to work\nout how to cut it into pieces",
    "start": "4180689",
    "end": "4185910"
  },
  {
    "text": "and to get things working. And the reason why\nyou need to do this",
    "start": "4185910",
    "end": "4192909"
  },
  {
    "text": "is that it's necessary to have\nthese different sets to get",
    "start": "4192910",
    "end": "4200580"
  },
  {
    "text": "realistic measures\nof performance. In fact, ideally, you're\nonly actually testing",
    "start": "4200580",
    "end": "4208409"
  },
  {
    "text": "on the test set once and,\nat any rate, very few times.",
    "start": "4208410",
    "end": "4213705"
  },
  {
    "text": "So if you're doing the\ndefault final project, we limit you to three\nruns on the test set. So remember that, and\nsave your three runs.",
    "start": "4213705",
    "end": "4221730"
  },
  {
    "text": "And the reason\nthat we do that is that if you mix up and\noveruse these data sets,",
    "start": "4221730",
    "end": "4228720"
  },
  {
    "text": "the results just become invalid. So when you train on\ndata, well, you'll",
    "start": "4228720",
    "end": "4234690"
  },
  {
    "text": "always build a model that does\nwell on the training data. So that's not exciting.",
    "start": "4234690",
    "end": "4239890"
  },
  {
    "text": "And, well, if you want\nto tune hyperparameters, if you tune them on\nthe training data set,",
    "start": "4239890",
    "end": "4245460"
  },
  {
    "text": "you won't get good\nvalid values for them, because they're not tuned to\ngood values for something that",
    "start": "4245460",
    "end": "4251760"
  },
  {
    "text": "would work on\ndifferent test data. But the surplus part\nof it is I think",
    "start": "4251760",
    "end": "4258659"
  },
  {
    "text": "a lot of people starting\noff think, oh, there's no harm in just keeping on\ntesting on the test set.",
    "start": "4258660",
    "end": "4264630"
  },
  {
    "text": "Every time I try\nout some variant, I'll just see how it's\ngoing on the test set.",
    "start": "4264630",
    "end": "4271180"
  },
  {
    "text": "It's a really compelling thing\nthat you want to do, right? You changed your\nmodel, and you'd like to know whether it made\nthe score on the test set",
    "start": "4271180",
    "end": "4278550"
  },
  {
    "text": "go up or down. But the truth is that if you\ndo that, you're cheating.",
    "start": "4278550",
    "end": "4285870"
  },
  {
    "text": "Because what you do is\nyou're sort of slowly training on the test set.",
    "start": "4285870",
    "end": "4291180"
  },
  {
    "text": "Because you keep every\nchange that helps on the test set, and you throw\naway every change",
    "start": "4291180",
    "end": "4297239"
  },
  {
    "text": "that doesn't help on the\ntest set, or makes it worse. And so you're learning\nabout the test set,",
    "start": "4297240",
    "end": "4302579"
  },
  {
    "text": "and what things happen by chance\nto work on that particular test set. So you effectively slowly\ntraining on the test",
    "start": "4302580",
    "end": "4310260"
  },
  {
    "text": "set, and you get biased,\nunrealistically high levels of performance",
    "start": "4310260",
    "end": "4320780"
  },
  {
    "text": "OK, let's see. I think there are\nthen sort of a bunch more slides on\nneural nets that I",
    "start": "4320780",
    "end": "4328909"
  },
  {
    "text": "think I'm going to\njust skip for a moment.",
    "start": "4328910",
    "end": "4334190"
  },
  {
    "text": "But I think in general\nthey're useful to have seen. Maybe I'll just sort\nof mention this one.",
    "start": "4334190",
    "end": "4345580"
  },
  {
    "text": "So for the final projects,\nyou're much more on your own. And you have to work\nout for yourself",
    "start": "4345580",
    "end": "4353890"
  },
  {
    "text": "how to get your neural\nnetwork to work. So the first thing\nis you want to start",
    "start": "4353890",
    "end": "4361900"
  },
  {
    "text": "with a positive attitude. These neural nets are amazing. They really want to learn. They want to find any pattern\nthey can anywhere in data.",
    "start": "4361900",
    "end": "4370640"
  },
  {
    "text": "They really just do that. It's in their DNA. So if your neural\nnetwork isn't learning,",
    "start": "4370640",
    "end": "4376900"
  },
  {
    "text": "it means you're doing something\nwrong that's preventing it from learning successfully.",
    "start": "4376900",
    "end": "4382719"
  },
  {
    "text": "But at that point,\nthere's a grim reality. There are all sorts of things\nthat cause neural nets to not",
    "start": "4382720",
    "end": "4390970"
  },
  {
    "text": "learn at all, or a more\ncommon case, actually, is they sort of learn\na bit, that they",
    "start": "4390970",
    "end": "4396280"
  },
  {
    "text": "don't learn very well. There are bugs in the code. You're dividing through\nby the wrong thing.",
    "start": "4396280",
    "end": "4403810"
  },
  {
    "text": "You're missing some\nconnections in your network, so no information is\nflowing from one place",
    "start": "4403810",
    "end": "4409540"
  },
  {
    "text": "to another place. You're calculating the\ngradients wrong in some layer, there are all sorts of\nthings that can go wrong.",
    "start": "4409540",
    "end": "4418610"
  },
  {
    "text": "So you need to then work out\nhow to find and fix them. And the truth is that this\ndebugging and tuning can often",
    "start": "4418610",
    "end": "4425770"
  },
  {
    "text": "take way more time than I'm\ngoing to implement the model. So in terms of thinking about\nhow much you could get through,",
    "start": "4425770",
    "end": "4433719"
  },
  {
    "text": "you should be thinking,\nOK, I've coded up my model. That doesn't mean\nthat you're 75% done.",
    "start": "4433720",
    "end": "4439810"
  },
  {
    "text": "It's pretty frequent, it means\nthat you're only 20% done, and there's still a lot of work\nto go to get things working.",
    "start": "4439810",
    "end": "4447185"
  },
  {
    "text": " OK, so for the last\nminutes I just want",
    "start": "4447185",
    "end": "4454230"
  },
  {
    "text": "to say a few minutes about\n[INAUDIBLE] final projects,",
    "start": "4454230",
    "end": "4461220"
  },
  {
    "text": "and I've got some\nidea about what this is about as an NLP problem. ",
    "start": "4461220",
    "end": "4469210"
  },
  {
    "text": "OK, so the problem\nis most commonly called question\nanswering of a document.",
    "start": "4469210",
    "end": "4475450"
  },
  {
    "text": "But really the part\nthat we're doing is perhaps better called\nreading comprehension.",
    "start": "4475450",
    "end": "4482020"
  },
  {
    "text": "And so the idea of\nthis is that you want to actually be\nable to answer questions",
    "start": "4482020",
    "end": "4489310"
  },
  {
    "text": "based on documents. So here's an example\nof a question,",
    "start": "4489310",
    "end": "4494980"
  },
  {
    "text": "who was Australia's\nthird prime minister? And once upon a time, if you\ntype the question into Google,",
    "start": "4494980",
    "end": "4503440"
  },
  {
    "text": "all you got was web search. And it returned to you a list of\npages with the implied promise",
    "start": "4503440",
    "end": "4510940"
  },
  {
    "text": "that some of the ones\nright near the top probably had the\nanswer to the question. But if you do this now, it\ngives you back an answer.",
    "start": "4510940",
    "end": "4520330"
  },
  {
    "text": "And so here is the answer,\nJohn Christian Watson. And the important\nthing to realize",
    "start": "4520330",
    "end": "4526360"
  },
  {
    "text": "is that this kind of\nfeatured snippet, Google has 101, or maybe 1,001\ndifferent pieces inside it.",
    "start": "4526360",
    "end": "4534250"
  },
  {
    "text": "But this featured\nsnippet isn't coming from the Google knowledge\ngraph structured data.",
    "start": "4534250",
    "end": "4541300"
  },
  {
    "text": "It's coming straight\nfrom a webpage where some part of the\nGoogle search system",
    "start": "4541300",
    "end": "4548860"
  },
  {
    "text": "has actually read this web page\nand decided what the answer is.",
    "start": "4548860",
    "end": "4554570"
  },
  {
    "text": "So for this kind of system,\nthe most straightforward way to do it is you have two parts.",
    "start": "4554570",
    "end": "4560810"
  },
  {
    "text": "First you have a\nweb search system that finds the page that\nprobably has the answer.",
    "start": "4560810",
    "end": "4566200"
  },
  {
    "text": "And then you have a reading\ncomprehension system that actually looks\ninside the text",
    "start": "4566200",
    "end": "4571720"
  },
  {
    "text": "and works to extract the answer. And so if you look through\nthis piece of text,",
    "start": "4571720",
    "end": "4578830"
  },
  {
    "text": "and this sentence says, was\nan Australian politician who served as the third prime\nminister of Australia, that's",
    "start": "4578830",
    "end": "4585190"
  },
  {
    "text": "what I'm asking for in slightly\ndifferent wording, Australia's third prime minister.",
    "start": "4585190",
    "end": "4590680"
  },
  {
    "text": "This answers the question. And so it correctly says that\nthe answer is John Christian",
    "start": "4590680",
    "end": "4595690"
  },
  {
    "text": "Watson. And so what we want to build\nin the default final project is",
    "start": "4595690",
    "end": "4601300"
  },
  {
    "text": "systems that do that\nsecond part, that given a piece of\ntext and a question,",
    "start": "4601300",
    "end": "4608380"
  },
  {
    "text": "they can give the answer. And so the general\nmotivation for why this is important\nis once we have",
    "start": "4608380",
    "end": "4614800"
  },
  {
    "text": "massive, full-text document\ncollections that simply we're",
    "start": "4614800",
    "end": "4621309"
  },
  {
    "text": "saying, here's a list of\nmaybe relevant documents, is of limited use. But we'd really much prefer to\nget answers to our questions.",
    "start": "4621310",
    "end": "4629889"
  },
  {
    "text": "And you know, that's\ntrue in general. But it's especially true\nif you're using your phone",
    "start": "4629890",
    "end": "4635440"
  },
  {
    "text": "to try and look for\ninformation, rather than sitting in front of a 27-inch monitor. It's especially\ntrue if you're using",
    "start": "4635440",
    "end": "4643150"
  },
  {
    "text": "a virtual assistant device\nlike Alexa or Google Assistant.",
    "start": "4643150",
    "end": "4648340"
  },
  {
    "text": "So that's the problem\nthat's being worked on,",
    "start": "4648340",
    "end": "4654280"
  },
  {
    "text": "reading comprehension\nor question answering. And so the SQuAD\ndata set, which was built by Pranav\nRajpurkar and Percy Liang",
    "start": "4654280",
    "end": "4663250"
  },
  {
    "text": "consists of passages\ntaken from Wikipedia, and questions, which\nteam won Super Bowl 50.",
    "start": "4663250",
    "end": "4671200"
  },
  {
    "text": "And what you're meant\nto be able to do is read through this passage,\nblah, blah, blah, blah, blah,",
    "start": "4671200",
    "end": "4677050"
  },
  {
    "text": "and say, aha, the answer\nis the Denver Broncos. ",
    "start": "4677050",
    "end": "4683170"
  },
  {
    "text": "And so there are\n100,000 such examples. And so the answer\nis always just taken",
    "start": "4683170",
    "end": "4690310"
  },
  {
    "text": "to be a span of the passage. And that's referred to as\nextractive question answering.",
    "start": "4690310",
    "end": "4696389"
  },
  {
    "text": "So to collect this\ndata, what was done was that people\nwere shown passages,",
    "start": "4696390",
    "end": "4703720"
  },
  {
    "text": "asked several questions, just\nlike reading comprehension in school, or perhaps\nslightly simpler questions.",
    "start": "4703720",
    "end": "4710290"
  },
  {
    "text": "And they were asked to\nchoose a standard answer. And as in these\nexamples show, they",
    "start": "4710290",
    "end": "4718510"
  },
  {
    "text": "showed it to three human beings. And they didn't always\nchoose exactly the same span.",
    "start": "4718510",
    "end": "4723850"
  },
  {
    "text": "Because there's some\nuncertainty as to how many words to include. But roughly they're answering\nthe question in that way.",
    "start": "4723850",
    "end": "4733849"
  },
  {
    "text": "And so then we have\nevaluation measures. And so there are two\nevaluation measures.",
    "start": "4733850",
    "end": "4739909"
  },
  {
    "text": "One is exact match, whether\nyou return exactly what one of the humans returned.",
    "start": "4739910",
    "end": "4745300"
  },
  {
    "text": "And the other one\nis the F1 measure, which is the overlapping\nwords of your span to one",
    "start": "4745300",
    "end": "4752739"
  },
  {
    "text": "of the humans, roughly. So for SQuAD, the\n[INAUDIBLE] We're",
    "start": "4752740",
    "end": "4762730"
  },
  {
    "text": "going to do SQuAD 2.0, which\nis just a little bit more",
    "start": "4762730",
    "end": "4768370"
  },
  {
    "text": "complex, because they make it\na little bit trickier that some of the questions have\nno answers in the text.",
    "start": "4768370",
    "end": "4775789"
  },
  {
    "text": "So here's a piece of\ntext about Genghis Khan. And the question is, when did\nGenghis Khan kill Great Khan?",
    "start": "4775790",
    "end": "4784360"
  },
  {
    "text": "And, well, if we read\nthrough this text,",
    "start": "4784360",
    "end": "4790510"
  },
  {
    "text": "there's Genghis Khan\nat the beginning. And there's talk\nabout different Khans.",
    "start": "4790510",
    "end": "4796690"
  },
  {
    "text": "And there's the person down here\nwho became Great Khan in 1251. Genghis Khan didn't\nkill Great Khan.",
    "start": "4796690",
    "end": "4803830"
  },
  {
    "text": "It doesn't say that at all. But actually, here's\nMicrosoft nlnet,",
    "start": "4803830",
    "end": "4809769"
  },
  {
    "text": "which is another strong\nquestion answering system. And if you ask it this\nquestion, it says 1234.",
    "start": "4809770",
    "end": "4818620"
  },
  {
    "text": "So the reality is that\na lot of these models have effectively, heuristically\nbehaved like, OK, well this",
    "start": "4818620",
    "end": "4825370"
  },
  {
    "text": "is asking for a year. Let me look for a\nyear in this passage. So this is near discussion\nof Genghis Khan.",
    "start": "4825370",
    "end": "4832600"
  },
  {
    "text": "And maybe weakening is\nsomehow similar to killing. I'm going to guess 1234.",
    "start": "4832600",
    "end": "4837610"
  },
  {
    "text": "And that's sort of the wrong\nthing to have done here. So this is a good\nreliability test for question answering models.",
    "start": "4837610",
    "end": "4843530"
  },
  {
    "text": "So that's an interesting\nadd-on problem to look at. OK, I'm going to\nstop there for today.",
    "start": "4843530",
    "end": "4850460"
  },
  {
    "text": "Good luck with your projects. ",
    "start": "4850460",
    "end": "4858000"
  }
]