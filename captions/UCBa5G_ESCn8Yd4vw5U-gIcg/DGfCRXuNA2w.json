[
  {
    "start": "0",
    "end": "5220"
  },
  {
    "text": "Hello, welcome to CS224N. Today we'll be talking\nabout pretraining,",
    "start": "5220",
    "end": "12180"
  },
  {
    "text": "which is another exciting\ntopic on the road to modern natural\nlanguage processing.",
    "start": "12180",
    "end": "18340"
  },
  {
    "text": "OK.  How is everyone doing?",
    "start": "18340",
    "end": "23430"
  },
  {
    "text": "Thumbs up, thumbs\nside, thumbs down. Wow! No response bias there.",
    "start": "23430",
    "end": "29760"
  },
  {
    "text": "All thumbs up. Oh, the side. Nice. I like that honesty. That's good. Well, OK, so we're now--",
    "start": "29760",
    "end": "37017"
  },
  {
    "text": "what is this? Week 5? Yes, it's week 5. And we have a couple--",
    "start": "37018",
    "end": "42809"
  },
  {
    "text": "so this lecture, the\nTransformers lecture, and then to a lesser extent, Thursday's\nlecture on natural language",
    "start": "42810",
    "end": "49739"
  },
  {
    "text": "generation will\nbe the sort of sum of lectures for the assignments\nyou have to do, right?",
    "start": "49740",
    "end": "56190"
  },
  {
    "text": "So assignment 5 is\ncoming out on Thursday. And the topics covered\nin this lecture",
    "start": "56190",
    "end": "64500"
  },
  {
    "text": "and the self-attention\ntransformers and, again, a little bit of\nnatural language generation will be tested in assignment 5.",
    "start": "64500",
    "end": "70278"
  },
  {
    "text": "And then, for the\nrest of the course, we will go through some\nreally fascinating topics in sort of modern natural\nlanguage processing that",
    "start": "70278",
    "end": "77940"
  },
  {
    "text": "should be useful for\nyour final projects and future jobs and interviews\nand intellectual curiosity.",
    "start": "77940",
    "end": "84930"
  },
  {
    "text": "But I think that this-- today's\nlecture is significantly less",
    "start": "84930",
    "end": "90390"
  },
  {
    "text": "technical in detail than last\nThursday's on self-attention and transformers, but\nit should give you",
    "start": "90390",
    "end": "96540"
  },
  {
    "text": "an idea of the sort of world\nof pretraining and sort of how it helps define natural\nlanguage processing today.",
    "start": "96540",
    "end": "106450"
  },
  {
    "text": "So a reminder\nabout assignment 5. Your project proposals also are\ndue on Tuesday, next Tuesday.",
    "start": "106450",
    "end": "113650"
  },
  {
    "text": "Please do get those in. Try to get them in\non time so that we can give you prompt feedback\nabout your project proposals.",
    "start": "113650",
    "end": "121240"
  },
  {
    "text": "And yeah, so let's jump into it. ",
    "start": "121240",
    "end": "126497"
  },
  {
    "text": "OK. So what we're going\nto start with today is a bit of a technical detail\non word structure and sort",
    "start": "126497",
    "end": "136170"
  },
  {
    "text": "of how we model the input\nsequence of words that we get. So when we were teaching\nword2vec and all the methods",
    "start": "136170",
    "end": "146280"
  },
  {
    "text": "that we've talked about so far,\nwe assumed a finite vocabulary, right? So you had a vocabulary V\nthat you define via whatever.",
    "start": "146280",
    "end": "152504"
  },
  {
    "text": "You've looked at some data. You've decided what the\nwords are in that data. And so you have some\nwords like hat and learn.",
    "start": "152505",
    "end": "162540"
  },
  {
    "text": "And you have this embedding. It's in red because you've\nlearned it properly. Actually, let's replace\n\"hat\" and \"learn\"",
    "start": "162540",
    "end": "168270"
  },
  {
    "text": "with \"pizza\" and \"tasty.\" Those are better. And so that's all well and good.",
    "start": "168270",
    "end": "173730"
  },
  {
    "text": "You see these words\nin your model, and you have an embedding\nthat's been learned on your data",
    "start": "173730",
    "end": "180600"
  },
  {
    "text": "to sort of know what to do\nwhen you see those words. But when you see some\nsort of variations,",
    "start": "180600",
    "end": "186070"
  },
  {
    "text": "maybe you see like\n\"taaaasty\" and maybe a typo like \"laern,\" or\nmaybe novel items",
    "start": "186070",
    "end": "193300"
  },
  {
    "text": "where it's a word\nthat you as a human can understand as\nthis combination-- this is called\nderivational morphology.",
    "start": "193300",
    "end": "201280"
  },
  {
    "text": "--of like this word,\ntransformer, that and -ify, which means take\nthis noun and give me back",
    "start": "201280",
    "end": "208420"
  },
  {
    "text": "a verb that means to\nmake more like that noun, to \"transformerify\"\nNLP might mean",
    "start": "208420",
    "end": "213460"
  },
  {
    "text": "to make NLP more like using\ntransformers and such.",
    "start": "213460",
    "end": "218830"
  },
  {
    "text": "And for each of these-- this maybe didn't show up\nin your training corpus. And language is\nalways doing this.",
    "start": "218830",
    "end": "225790"
  },
  {
    "text": "People are always coming\nup with new words, and there's new\ndomains, and there's-- young people are always\nmaking new words.",
    "start": "225790",
    "end": "232300"
  },
  {
    "text": "It's great. And so it's a problem\nfor your model, though,q because you've\ndefined this finite vocabulary.",
    "start": "232300",
    "end": "237430"
  },
  {
    "text": "And there's no mapping\nin that vocabulary for each of these things,\neven though their meanings",
    "start": "237430",
    "end": "244209"
  },
  {
    "text": "should be relatively\nwell defined based on the data you've seen so far. It's just that the string of\ncharacters that define them",
    "start": "244210",
    "end": "251290"
  },
  {
    "text": "aren't quite what you've seen. And so what do you do? Well, maybe you map them to\nthis universal unknown token,",
    "start": "251290",
    "end": "258518"
  },
  {
    "text": "this UNK, right? So it's like, oh,\nI see something, I don't know what, I've\nnever seen it before. I'm going to say it's\nalways represented",
    "start": "258519",
    "end": "264699"
  },
  {
    "text": "by the same token, UNK. And so that's been\ndone in the past. And that's sort of bad,\nright, because it's totally",
    "start": "264700",
    "end": "271870"
  },
  {
    "text": "losing tons of information. But you need to map\nit to something.",
    "start": "271870",
    "end": "278550"
  },
  {
    "text": "And so this is a clear problem,\nespecially-- in English, it's a problem.",
    "start": "278550",
    "end": "284060"
  },
  {
    "text": "In many of the\nworld's languages, it's a substantially\nlarger problem, right? So English has a relatively\nsimple word structure.",
    "start": "284060",
    "end": "293300"
  },
  {
    "text": "There's a couple of conjugations\nfor each verb, like eat, eats, eaten, ate.",
    "start": "293300",
    "end": "300110"
  },
  {
    "text": "But in a language with much\nmore complex morphology or word structure, you'll have a\nconsiderably more complex set",
    "start": "300110",
    "end": "310490"
  },
  {
    "text": "of things that you\ncould see in the world. So here is a conjugation\ntable for a Swahili verb,",
    "start": "310490",
    "end": "317270"
  },
  {
    "text": "and it has over\n300 conjugations. And if I define a vocabulary\nto be every unique string",
    "start": "317270",
    "end": "323270"
  },
  {
    "text": "of characters maps\nto its own word, then every one of\nthe 300 conjugations",
    "start": "323270",
    "end": "328280"
  },
  {
    "text": "would get an independent\nvector under my model, which makes no sense because the\n300 conjugations obviously",
    "start": "328280",
    "end": "335180"
  },
  {
    "text": "have a lot in common and differ\nby sort of meaningful extents. So you don't want to do this.",
    "start": "335180",
    "end": "341190"
  },
  {
    "text": "I'd have to have a huge\nvocabulary if I wanted all conjugations to show up.",
    "start": "341190",
    "end": "346259"
  },
  {
    "text": "And that's a mistake\nfor efficiency reasons and for learning reasons. Any questions so far?",
    "start": "346260",
    "end": "352110"
  },
  {
    "text": " Cool. OK. And so what we end up doing is--",
    "start": "352110",
    "end": "362289"
  },
  {
    "text": "we'll look at subword\nstructure, subword modeling. So what we're going to\ndo is we're going to say,",
    "start": "362290",
    "end": "367930"
  },
  {
    "text": "I'm not going to\neven try to define what the set of all words is. I'm going to define\nmy vocabulary",
    "start": "367930",
    "end": "375220"
  },
  {
    "text": "to include parts of words. Where am I? Right? ",
    "start": "375220",
    "end": "386080"
  },
  {
    "text": "So I'm going to split words into\nsequences of known subwords. And so there's a\nsimple algorithm",
    "start": "386080",
    "end": "392290"
  },
  {
    "text": "for this where you start\nwith all characters, right? So if I only had a\nvocabulary of all characters,",
    "start": "392290",
    "end": "398270"
  },
  {
    "text": "and maybe like an \"end-of-word\"\nsymbol, for a finite data set,",
    "start": "398270",
    "end": "404139"
  },
  {
    "text": "then I could-- no matter what\nword I saw in the future, as long as I had seen\nall possible characters, I could take the word and say,\nI don't know what this word is.",
    "start": "404140",
    "end": "410950"
  },
  {
    "text": "I'm going to split it into all\nof its individual characters. So you won't have\nthis UNK problem. You can sort of\nrepresent any word.",
    "start": "410950",
    "end": "417199"
  },
  {
    "text": "And then you're going to find\ncommon adjacent characters and say, OK, \"a\"\nand \"b\" co-occur",
    "start": "417200",
    "end": "422319"
  },
  {
    "text": "next to each other quite a bit. So I'm going to add a new\nword to my vocabulary. Now it's all characters\nplus this new word \"a, b,\"",
    "start": "422320",
    "end": "430510"
  },
  {
    "text": "which is a subword. And likewise, I'm\ngoing-- so now I'm going to replace the character\npair with a new subword",
    "start": "430510",
    "end": "436940"
  },
  {
    "text": "and repeat until you\nadd a lot, a lot, a lot of vocabulary items\nthrough this process of what things tend to\nco-occur next to each other.",
    "start": "436940",
    "end": "444210"
  },
  {
    "text": "And so what you'll end\nup with is a vocabulary of very commonly\nco-occurring substrings",
    "start": "444210",
    "end": "451400"
  },
  {
    "text": "by which you can build up words. And this was\noriginally developed for machine translation\nbut then has",
    "start": "451400",
    "end": "456530"
  },
  {
    "text": "been used considerably in\npretty much all modern language models. So now we have\n\"hat\" and \"learn.\"",
    "start": "456530",
    "end": "464030"
  },
  {
    "text": "So in our subword\nvocabulary, \"hat\" and \"learn\" showed up enough that they are\ntheir own individual words.",
    "start": "464030",
    "end": "469920"
  },
  {
    "text": "So that's sort of good, ri ght? So simple common words show up\nas a word in your vocabulary",
    "start": "469920",
    "end": "476125"
  },
  {
    "text": "just like you\nwould like them to. But now \"tasty\" maybe\ngets split into T-A-A. And then maybe, in some\ncases, this ## means",
    "start": "476125",
    "end": "484745"
  },
  {
    "text": "don't add a space next. So T-A-A, and then A-A-A,\nand then S-T-Y, right?",
    "start": "484745",
    "end": "492050"
  },
  {
    "text": "So I've actually taken one\nthing that seems like a word. And in my vocabulary,\nit's now split into three subword tokens.",
    "start": "492050",
    "end": "499860"
  },
  {
    "text": "So when I pass this\nto my transformer or to my recurrent\nneural network, the recurrent\nneural network would",
    "start": "499860",
    "end": "506190"
  },
  {
    "text": "take T-A-A as just\na single element, do the RNN update, and then\ntake A-A-A, do the RNN update,",
    "start": "506190",
    "end": "514080"
  },
  {
    "text": "and then S-T-Y. So it could learn to process\nconstructions like this.",
    "start": "514080",
    "end": "519569"
  },
  {
    "text": "And maybe I can even add\nmore A-A-As in the middle and have it do something\nsimilar instead of just seeing the\nentire word tasty",
    "start": "519570",
    "end": "528240"
  },
  {
    "text": "and not knowing what it means. Is that-- that's feedback, yeah.",
    "start": "528240",
    "end": "534220"
  },
  {
    "text": " How loud is that feedback?",
    "start": "534220",
    "end": "541160"
  },
  {
    "text": "Are we good? OK, I think we're fixed. Great. And so same with\n\"transformerify.\"",
    "start": "541160",
    "end": "547980"
  },
  {
    "text": "Maybe \"transformer\"\nis its own word. And then \"-ify\" And so you can\nsee that you have three learned",
    "start": "547980",
    "end": "553529"
  },
  {
    "text": "embeddings instead of one\nuseless UNK embedding. So this is just wildly useful\nand is used pretty much",
    "start": "553530",
    "end": "560770"
  },
  {
    "text": "everywhere. Variants of this algorithm are\nused pretty much everywhere in modern NLP.",
    "start": "560770",
    "end": "566370"
  },
  {
    "text": "Questions? Yes. If we have three\nembeddings for \"tasty,\" do we just add them together?",
    "start": "566370",
    "end": "572660"
  },
  {
    "text": "So the question is, if we have\nthree embeddings for \"tasty,\" do we just add them together?",
    "start": "572660",
    "end": "577889"
  },
  {
    "text": "If we want to represent-- so when we're actually\nprocessing the sequence, I'd see something like \"I\nlearned about the T-A-A, A-A-A,",
    "start": "577890",
    "end": "589600"
  },
  {
    "text": "S-T-Y.\" So it'd actually be\ntotally separate tokens. But if I wanted\nto then say what's my representation\nof this thing, it",
    "start": "589600",
    "end": "597238"
  },
  {
    "text": "depends on what you want to do. Sometimes you average the\ncontextual representations of the three or look\nat the last one, maybe.",
    "start": "597238",
    "end": "606334"
  },
  {
    "text": "At that point, it's\nunclear what to do, but everything sort of works OK. How do you know where to split?",
    "start": "606335",
    "end": "612648"
  },
  {
    "text": "How do you what? How do you know where\nto split the phrases? Yeah, so you know where to\nsplit based on the algorithm",
    "start": "612648",
    "end": "618670"
  },
  {
    "text": "that I specified earlier\nfor learning the vocabulary. So you've learned\nthis vocabulary",
    "start": "618670",
    "end": "624490"
  },
  {
    "text": "by just combining commonly\nco-occurring adjacent strings of letters. So \"a, b\" co-occurred a lot.",
    "start": "624490",
    "end": "630850"
  },
  {
    "text": "So now I've got a new\nword, that's \"a, b.\" And then, when I'm actually\nwalking through and tokenizing,",
    "start": "630850",
    "end": "636370"
  },
  {
    "text": "I try to split as\nlittle as possible. So I split words into\nthe maximal subword that",
    "start": "636370",
    "end": "641822"
  },
  {
    "text": "takes up the most characters. There are algorithms for this. Yeah, so I'm like, OK, if\nI want to split this up,",
    "start": "641822",
    "end": "648970"
  },
  {
    "text": "there's many ways\nI can split it up. And you try to find\nsome approximate what the best way to split it\ninto the fewest words is.",
    "start": "648970",
    "end": "655060"
  },
  {
    "text": "Yeah. [INAUDIBLE] of punctuation in\nthe character set [INAUDIBLE]??",
    "start": "655060",
    "end": "660459"
  },
  {
    "text": "Yes. The question is, do people\nmake use of punctuation in the character set? Do people do it? Yes, absolutely.",
    "start": "660460",
    "end": "666240"
  },
  {
    "text": "So from this point\non, just assume",
    "start": "666240",
    "end": "672750"
  },
  {
    "text": "that what text is\ngiven to these models is as unprocessed as possible. You take it.",
    "start": "672750",
    "end": "678120"
  },
  {
    "text": "You try to make it sort\nof clean-looking text where you've removed HTML tags,\nmaybe if it's from the internet",
    "start": "678120",
    "end": "684240"
  },
  {
    "text": "or whatever. But then, beyond\nthat, you process it as little as possible\nso that it reflects",
    "start": "684240",
    "end": "690570"
  },
  {
    "text": "as well as possible what\npeople might actually be using this for. So maybe earlier\nin the course, when",
    "start": "690570",
    "end": "696600"
  },
  {
    "text": "we were looking at\nword2vec, maybe we might we have thought\nabout, oh, we don't want word2vec vectors of\npunctuation or something",
    "start": "696600",
    "end": "703529"
  },
  {
    "text": "like that. Now everything is just\nas close as possible to what the text you'd\nget with people trying",
    "start": "703530",
    "end": "710760"
  },
  {
    "text": "to use your system would be. So yes. In practice punctuation,\nand dot, dot, dot might be its own word, and\nmaybe a sequence of hyphens",
    "start": "710760",
    "end": "720339"
  },
  {
    "text": "because people make\nbig bars across tables. Yeah. ",
    "start": "720340",
    "end": "727850"
  },
  {
    "text": "How does it impact one word or\nit could be multiple embeddings",
    "start": "727850",
    "end": "734149"
  },
  {
    "text": "versus a single embedding? Does the system treat\nthose any differently?",
    "start": "734150",
    "end": "741620"
  },
  {
    "text": "The question is, does the\nsystem treat any differently words that are really\nthemselves, the whole word, versus words that are pieces?",
    "start": "741620",
    "end": "747995"
  },
  {
    "text": "No, the system has no idea. They're all just indices into\nyour embedding vocabulary",
    "start": "747995",
    "end": "754820"
  },
  {
    "text": "matrix. So they're all treated equally. ",
    "start": "754820",
    "end": "761230"
  },
  {
    "text": "What about really long\nwords that are, I guess, relatively common? Because if you're\nbuilding up from, say, character all the way up--",
    "start": "761230",
    "end": "767589"
  },
  {
    "text": "what happens then? Yeah, the question is, what\nhappens to very long words, if you're building up\nfrom character pairs",
    "start": "767590",
    "end": "774399"
  },
  {
    "text": "and portions of characters? In practice, the\nstatistics speak really",
    "start": "774400",
    "end": "779800"
  },
  {
    "text": "well for themselves. So if a long word\nis very common, it will end up in\nthe vocabulary. And if it's not very\ncommon, it won't.",
    "start": "779800",
    "end": "787750"
  },
  {
    "text": "There are algorithms\nthat aren't this that do slightly\nbetter in various ways,",
    "start": "787750",
    "end": "792760"
  },
  {
    "text": "but the intuition that\nyou sort of figure out what the common\nco-occurring substrings are",
    "start": "792760",
    "end": "799120"
  },
  {
    "text": "independent of length almost\nis the right intuition to have. And so, yeah, you\ncan actually just look at the learned vocabularies\nof a lot of these models.",
    "start": "799120",
    "end": "806350"
  },
  {
    "text": "And you see some long\nwords just because they-- if they showed up a lot.",
    "start": "806350",
    "end": "812440"
  },
  {
    "text": " I'm curious, how does\nit weigh the frequency?",
    "start": "812440",
    "end": "821240"
  },
  {
    "text": "So let's say there's If\nwe-- or in your next slide, it was like -ify at\nthe very last one.",
    "start": "821240",
    "end": "828079"
  },
  {
    "text": "So it if could be really common. So how does a weigh the\nfrequency of a subword versus the length of it?",
    "start": "828080",
    "end": "834250"
  },
  {
    "text": "It tries to split it up\ninto the smallest number. But what if it could split it\nup into three but one of them",
    "start": "834250",
    "end": "839380"
  },
  {
    "text": "was super common? Yeah. So the question is,\nif a \"transformer\" is a subword in my vocabulary,\nand \"if\" is a subword,",
    "start": "839380",
    "end": "847540"
  },
  {
    "text": "and \"y\" is a subword, and -ify,\nas a three-use letter tuple,",
    "start": "847540",
    "end": "852759"
  },
  {
    "text": "is also a subword, how does\nit choose to take the -ify, maybe it's not very common,\nas opposed to splitting it",
    "start": "852760",
    "end": "860380"
  },
  {
    "text": "into more subwords? It's just a choice. We choose to try to take the\nsmallest number of subwords",
    "start": "860380",
    "end": "866440"
  },
  {
    "text": "because that tends to be\nmore of the bottleneck as opposed to having a bunch\nof very common, very",
    "start": "866440",
    "end": "872170"
  },
  {
    "text": "short subwords. Sequence length is a big\nproblem in transformers. And this seems to be what works.",
    "start": "872170",
    "end": "879410"
  },
  {
    "text": "Although, trying to split\nthings into multiple options of a sequence and running the\ntransformer on all of them is the thing that\npeople have done to see",
    "start": "879410",
    "end": "886100"
  },
  {
    "text": "which one will work better. But yeah, having\nfewer bigger subwords tends to be the\nbest sort of idea.",
    "start": "886100",
    "end": "891720"
  },
  {
    "text": "I'm going to start\nmoving on, though. Feel free to ask\nme more questions about this afterwards.",
    "start": "891720",
    "end": "896750"
  },
  {
    "text": "OK, so let's talk about\npretraining from the context of the course so far.",
    "start": "896750",
    "end": "903030"
  },
  {
    "text": "So at the very\nbeginning of the course, we gave you this\nquote, which was, \"You shall know a word\nby the company it keeps.\"",
    "start": "903030",
    "end": "909680"
  },
  {
    "text": "This was the thesis of the\ndistributional hypothesis, right, that the meaning\nof the word is defined by,",
    "start": "909680",
    "end": "916460"
  },
  {
    "text": "or at least reflected\nby, what words it tends to co-occur around. And we implemented\nthis via word2vec.",
    "start": "916460",
    "end": "923770"
  },
  {
    "text": "The same person\nwho made that quote had a separate quote, actually\nearlier, that continues",
    "start": "923770",
    "end": "930670"
  },
  {
    "text": "this notion of meaning as\ndefined by context, which says something along the lines of,\nwell, since the word shows up",
    "start": "930670",
    "end": "938860"
  },
  {
    "text": "in context when we\nactually use it, when we speak to each other,\nthe meaning of the word should be defined in the context\nthat it actually shows up in.",
    "start": "938860",
    "end": "947200"
  },
  {
    "text": "And so, the complete meaning\nof a word is always contextual, and no study of meaning\napart from a complete context",
    "start": "947200",
    "end": "954190"
  },
  {
    "text": "can be taken seriously. So right? The big difference here is at\nword2vec training time if I",
    "start": "954190",
    "end": "961389"
  },
  {
    "text": "have the word\nrecord, R-E-C-O-R-D,",
    "start": "961390",
    "end": "966490"
  },
  {
    "text": "when I'm training word2vec, I\nget one vector or two, but-- one vector, meaning\nrecord the string.",
    "start": "966490",
    "end": "975790"
  },
  {
    "text": "And it has to learn by\nwhat context it shows up in that sometimes this can\nmean I \"record,\" i.e. the verb",
    "start": "975790",
    "end": "985129"
  },
  {
    "text": "or \"record,\" i.e. the noun. But I only have one\nvector to represent it.",
    "start": "985130",
    "end": "990240"
  },
  {
    "text": "And so when I use the word2vec\nembedding of \"record,\" it has this mixture of\nmeaning of both of its senses.",
    "start": "990240",
    "end": "998870"
  },
  {
    "text": "It doesn't get to\nspecialize and say, oh, this part means record,\nand this part means record.",
    "start": "998870",
    "end": "1004880"
  },
  {
    "text": "And so word2vec is going\nto just sort of fail. And so I can build better\nrepresentations of language",
    "start": "1004880",
    "end": "1011230"
  },
  {
    "text": "through these contextual\nrepresentations that are going to take things like\nrecurrent neural networks or transformers that we\nused before to build up",
    "start": "1011230",
    "end": "1018390"
  },
  {
    "text": "sort of contextual meaning. ",
    "start": "1018390",
    "end": "1023709"
  },
  {
    "text": "So what we had before were\npretrained word embeddings. And then we had a\nbig box on top of it",
    "start": "1023710",
    "end": "1030880"
  },
  {
    "text": "like a transformer or an LSTM\nthat was not pretrained, right? So you learn via context\nyour word embeddings here.",
    "start": "1030880",
    "end": "1039069"
  },
  {
    "text": "And then you have a task, like\nsentiment analysis or machine translation or\nparsing or whatever.",
    "start": "1039069",
    "end": "1045520"
  },
  {
    "text": "And you initialize all the\nparameters of this randomly, and then you train to\npredict your label.",
    "start": "1045520",
    "end": "1052750"
  },
  {
    "text": "And the big difference\nin today's work is that we're going to try to\npre-train all the parameters.",
    "start": "1052750",
    "end": "1059510"
  },
  {
    "text": "So I have my big transformer,\nand instead of just pretraining my word embeddings\nwith word2vec,",
    "start": "1059510",
    "end": "1065289"
  },
  {
    "text": "I'm going to train all of the\nparameters of the network,",
    "start": "1065290",
    "end": "1070510"
  },
  {
    "text": "trying to teach it much\nmore about language that I could use in\nmy downstream tasks.",
    "start": "1070510",
    "end": "1077480"
  },
  {
    "text": "So now I'm-- the labeled\ndata that I have for, say, machine translation\nmight need to be smaller.",
    "start": "1077480",
    "end": "1085669"
  },
  {
    "text": "I might not need as much\nof it because I've already trained much more of the\nnetwork than I otherwise",
    "start": "1085670",
    "end": "1090700"
  },
  {
    "text": "would have if I had just\ngotten word2vec embeddings. OK. ",
    "start": "1090700",
    "end": "1096910"
  },
  {
    "text": "So here, I've pretrained\nthis entire sort of structure, the word\nembeddings, the transformer",
    "start": "1096910",
    "end": "1102520"
  },
  {
    "text": "on top. Everything's been\ntrained via methods that we'll talk about today. And so, what does this give you?",
    "start": "1102520",
    "end": "1108790"
  },
  {
    "text": "It gives you very strong\nrepresentations of language. So the meaning of\n\"record\" and \"record\"",
    "start": "1108790",
    "end": "1114700"
  },
  {
    "text": "will be different in the sort\nof contextual representations that know where in\nthe sequence it is",
    "start": "1114700",
    "end": "1121389"
  },
  {
    "text": "and what words are co-occurring\nwith it in this specific input than word2vec, which only has\none representation for record",
    "start": "1121390",
    "end": "1127450"
  },
  {
    "text": "independent of\nwhere it shows up. It'll also be used as strong\nparameter initializations",
    "start": "1127450",
    "end": "1133570"
  },
  {
    "text": "for NLP models. So in all of your\nhomework so far, you've worked with building out\na natural language processing",
    "start": "1133570",
    "end": "1140350"
  },
  {
    "text": "system from scratch,\nlike how do I initialize this weight matrix? And we always say, oh, small,\nnormally distributed noise,",
    "start": "1140350",
    "end": "1147970"
  },
  {
    "text": "like little values\nclose to zero. And here we're\ngoing to say, well,",
    "start": "1147970",
    "end": "1154300"
  },
  {
    "text": "just like we were going to\nuse the word2vec embeddings, and those sort of\nencoded structure, I'm going to start maybe my\nmachine translation system",
    "start": "1154300",
    "end": "1162110"
  },
  {
    "text": "from a parameter\ninitialization that's given to me via pretraining.",
    "start": "1162110",
    "end": "1167442"
  },
  {
    "text": "And then also it's\ngoing to give us probability distributions\nover language that we can use to\ngenerate and otherwise.",
    "start": "1167442",
    "end": "1173022"
  },
  {
    "text": "And we'll talk about this. OK? So whole models are\ngoing to be pretrained. So all of the pretraining\nis effectively",
    "start": "1173022",
    "end": "1181660"
  },
  {
    "text": "going to be centered around\nthis idea of reconstructing the input. So you have an input,\nit's a sequence of texts",
    "start": "1181660",
    "end": "1187960"
  },
  {
    "text": "that some human has\ngenerated, and the hypothesis is that by masking\nout part of it",
    "start": "1187960",
    "end": "1195730"
  },
  {
    "text": "and tasking a neural\nnetwork with reconstructing the original input,\nthat neural network",
    "start": "1195730",
    "end": "1201460"
  },
  {
    "text": "has to learn a lot\nabout language, about the world in order to do\na good job of reconstructing",
    "start": "1201460",
    "end": "1207190"
  },
  {
    "text": "the input, right? So this is now a supervised\nlearning problem, just like machine translation.",
    "start": "1207190",
    "end": "1213340"
  },
  {
    "text": "I've taken this sentence\nthat just existed, \"Stanford University\nis located in--\" say, Palo Alto, California, or\nStanford, California, I guess.",
    "start": "1213340",
    "end": "1222850"
  },
  {
    "text": "And I have, by removing\nthis part of the sentence,",
    "start": "1222850",
    "end": "1227860"
  },
  {
    "text": "made a label for myself. The input is this\nbroken, masked sentence,",
    "start": "1227860",
    "end": "1233590"
  },
  {
    "text": "and the label is\n\"Stanford\" or \"Palo Alto.\" ",
    "start": "1233590",
    "end": "1239040"
  },
  {
    "text": "So if I give this\nexample to a network and ask it to predict\nthe center thing, as it's doing its gradient\nstep on this input,",
    "start": "1239040",
    "end": "1246120"
  },
  {
    "text": "it's going to encode information\nabout the co-occurrence between this context and for\n\"University is located in--\"",
    "start": "1246120",
    "end": "1251940"
  },
  {
    "text": "and \"Palo Alto.\" So by tasking it with\nthis, it might learn, say, where Stanford is.",
    "start": "1251940",
    "end": "1258030"
  },
  {
    "text": "What else might it learn? Well, it can learn things\nabout maybe syntax. So \"I put--\" blank \"--fork\ndown on the table.\"",
    "start": "1258030",
    "end": "1265568"
  },
  {
    "text": "Here, there's only a certain\nset of words that could go here. \"I put the fork\ndown on the table.\" \"I put a fork down\non the table.\"",
    "start": "1265568",
    "end": "1271900"
  },
  {
    "text": "These are syntactic constraints. So the context shows\nme what kinds of words",
    "start": "1271900",
    "end": "1277920"
  },
  {
    "text": "can appear in what\nkinds of contexts.  \"The woman walked\nacross the street,",
    "start": "1277920",
    "end": "1283590"
  },
  {
    "text": "checking for traffic\nover--\" blank \"--shoulder.\" Any idea is that\nwhat could go here?",
    "start": "1283590",
    "end": "1289276"
  },
  {
    "text": "\"Her,\" right? So this co-reference between\nthis entity who is being",
    "start": "1289276",
    "end": "1295830"
  },
  {
    "text": "discussed in the world, this\nwoman, and her shoulder-- now when I discuss-- this is sort of a\nlinguistic concept.",
    "start": "1295830",
    "end": "1302170"
  },
  {
    "text": "The word \"her\" here is a\nco-referent to a woman, right? It's referring to the same\nentity in the discourse. And so the network might\nbe able to learn things",
    "start": "1302170",
    "end": "1309400"
  },
  {
    "text": "about what kind of entities\nare doing, what, where. ",
    "start": "1309400",
    "end": "1316617"
  },
  {
    "text": "It can learn things\nabout sort of semantics. So if I have, \"I\nwent to the ocean to see the fish, turtles,\nseals, and--\" blank.",
    "start": "1316617",
    "end": "1322590"
  },
  {
    "text": "Then, the word\nthat's in the blank should be sort of a\nmember of the class that I'm thinking of as a person\nwriting this sentence, of stuff",
    "start": "1322590",
    "end": "1329789"
  },
  {
    "text": "that I see when\nI go to the ocean and see these other\nthings as well, right? So in order to do\nthis prediction task,",
    "start": "1329790",
    "end": "1335550"
  },
  {
    "text": "maybe I'll learn about the\nsemantics of aquatic creatures.",
    "start": "1335550",
    "end": "1342940"
  },
  {
    "text": "OK. So what else could I learn? I've got, \"Overall, the value\nI got from the two hours watching it was the sum total\nof the popcorn and drink.",
    "start": "1342940",
    "end": "1349650"
  },
  {
    "text": "The movie was--\" blank. What kind of task\ncould I be learning from doing this sort\nof prediction problem?",
    "start": "1349650",
    "end": "1357000"
  },
  {
    "text": "[INAUDIBLE] Sentiment. Exactly. So this is just a\nnaturalistic text that I naturally wrote myself.",
    "start": "1357000",
    "end": "1365510"
  },
  {
    "text": "But by saying, oh,\nthe movie was bad, I'm learning about the latent\nsentiment of the person who",
    "start": "1365510",
    "end": "1373183"
  },
  {
    "text": "wrote this, what\nthey were feeling about the movie at the time. So maybe if I see a\nnew review later on,",
    "start": "1373183",
    "end": "1379370"
  },
  {
    "text": "I can just paste in the\nreview, say the movie was-- blank. And if the model\ngenerates \"bad\" or \"good,\"",
    "start": "1379370",
    "end": "1386779"
  },
  {
    "text": "that could be implicitly solving\nthe task of sentiment analysis. ",
    "start": "1386780",
    "end": "1393610"
  },
  {
    "text": "So here's another one,\n\"Iroh went to the kitchen to make some tea. Standing next to Iroh,\nZuko pondered his destiny.",
    "start": "1393610",
    "end": "1399539"
  },
  {
    "text": "Zuko left the--\" blank. OK.  So in this scenario, we've\ngot a world that implicitly",
    "start": "1399540",
    "end": "1407010"
  },
  {
    "text": "has been designed by the person\nwho is creating this text, right? I've got physical\nlocations in the discourse,",
    "start": "1407010",
    "end": "1413610"
  },
  {
    "text": "like the kitchen. And I've got Zuko. We've got Iroh is\nin the kitchen. Zuko is next to Iroh.",
    "start": "1413610",
    "end": "1420389"
  },
  {
    "text": "So Zuko must be in the kitchen. So what could Zuko leave\nbut the kitchen, right?",
    "start": "1420390",
    "end": "1427050"
  },
  {
    "text": "And so in terms\nof latent notions of embodiment and\nphysical location, the way that people talk about\npeople being next to something",
    "start": "1427050",
    "end": "1434669"
  },
  {
    "text": "and then leaving something\ncould tell you stuff about-- yeah, a little bit about\nhow the world works even.",
    "start": "1434670",
    "end": "1441495"
  },
  {
    "text": " So here's the sequence,\n\"I was thinking",
    "start": "1441495",
    "end": "1446809"
  },
  {
    "text": "about the sequence that goes 1,\n1, 2, 3, 5, 8, 13, 21--\" blank.",
    "start": "1446810",
    "end": "1452270"
  },
  {
    "text": "And this is a pretty\ntough one, right?",
    "start": "1452270",
    "end": "1457730"
  },
  {
    "text": "This is the Fibonacci sequence. If you have a model by\nlooking at a bunch of numbers from the Fibonacci sequence,\nlearn to, in general,",
    "start": "1457730",
    "end": "1465470"
  },
  {
    "text": "predict the next one-- that's a question you\nshould be thinking about throughout the lecture.",
    "start": "1465470",
    "end": "1471769"
  },
  {
    "text": "OK, any questions\non these examples of what you might learn\nfrom predicting the context? ",
    "start": "1471770",
    "end": "1482290"
  },
  {
    "text": "OK, cool. So a very simple way to\nthink about pretraining",
    "start": "1482290",
    "end": "1487660"
  },
  {
    "text": "is pretraining is\nlanguage modeling. So we saw language modeling\nearlier in the course. And now, we're just going to\nsay instead of using my language",
    "start": "1487660",
    "end": "1494559"
  },
  {
    "text": "model just to\nprovide probabilities over the next word, I am going\nto train it on that task. I'm going to actually\nmodel the distribution p",
    "start": "1494560",
    "end": "1504890"
  },
  {
    "text": "theta of the word t given\nall the words previous. And there's a ton of\ndata for this, ri ght?",
    "start": "1504890",
    "end": "1511460"
  },
  {
    "text": "There's just an\namazing amount of data for this in a lot of\nlanguages, especially English.",
    "start": "1511460",
    "end": "1516769"
  },
  {
    "text": "There's very little\ndata for this and actually most of\nthe world's languages, which is a separate problem. But you can pre-train just\nthrough language modeling,",
    "start": "1516770",
    "end": "1523835"
  },
  {
    "text": "right? So I'm going to sort of do\nthe teacher-forcing thing. So I have \"Iroh.\" I predict \"goes,\" I have\n\"goes,\" I predict \"to.\"",
    "start": "1523835",
    "end": "1530360"
  },
  {
    "text": "And I'm going to\ntrain my sort of LSTM or my transformer\nto do this task.",
    "start": "1530360",
    "end": "1535650"
  },
  {
    "text": "And then I'm just going\nto keep all the weights. OK, I'm going to save all\nthe network parameters.",
    "start": "1535650",
    "end": "1540740"
  },
  {
    "text": " And then, once I have\nthese parameters, ri ght, instead of generating\nthem from my language model,",
    "start": "1540740",
    "end": "1547887"
  },
  {
    "text": "I'm just going to use\nthem as an initialization for my parameters. So I have this pretraining\nfine-tuning paradigm,",
    "start": "1547887",
    "end": "1555150"
  },
  {
    "text": "two steps. Most of you, I think, in your--\nwell, maybe not this year. Let's say a large\nportion of you this year",
    "start": "1555150",
    "end": "1561480"
  },
  {
    "text": "in your final projects will\nbe doing the pretraining, fine-tuning sort of\nparadigm where someone has done the pretraining for you.",
    "start": "1561480",
    "end": "1566970"
  },
  {
    "text": "So you have a ton of text. You learn very general things\nabout the distribution of words",
    "start": "1566970",
    "end": "1572970"
  },
  {
    "text": "and the latent things that tell\nyou about the world and about language. And then in step two,\nyou've got some tasks, maybe",
    "start": "1572970",
    "end": "1580320"
  },
  {
    "text": "sentiment analysis. And you have maybe\nnot very many labels. You have a little\nbit of labeled data.",
    "start": "1580320",
    "end": "1586380"
  },
  {
    "text": "And you adapt the\npretrained model to the task that you care about\nby further doing",
    "start": "1586380",
    "end": "1591690"
  },
  {
    "text": "gradient steps on this task. So you give it \"the movie was--\"\nyou predict \"happy\" or \"sad.\"",
    "start": "1591690",
    "end": "1597780"
  },
  {
    "text": "And then you sort of continue\nto update the parameters based on the initialization\nfrom the pretraining.",
    "start": "1597780",
    "end": "1606060"
  },
  {
    "text": "And this just works\nexceptionally well, unbelievably well compared\nto training from scratch,",
    "start": "1606060",
    "end": "1611730"
  },
  {
    "text": "intuitively,\nbecause you've taken a lot of the burden of\nlearning about language, learning about the\nworld off of the data",
    "start": "1611730",
    "end": "1618210"
  },
  {
    "text": "that you've labeled\nfor sentiment analysis. And you're giving\nthat task of learning all this very general stuff\nto the much more general task",
    "start": "1618210",
    "end": "1625500"
  },
  {
    "text": "of language modeling. Yes. You said we didn't have much\ndata in other languages.",
    "start": "1625500",
    "end": "1630660"
  },
  {
    "text": "What do you mean by data? Is it just text\nin that language-- Yeah --labeled in some way?",
    "start": "1630660",
    "end": "1636610"
  },
  {
    "text": "So the question is, you\nsaid we have a lot of data in English but not\nin other languages.",
    "start": "1636610",
    "end": "1642030"
  },
  {
    "text": "What do you mean by\ndata that we don't have a lot of in other languages? Is it just text? It's literally just\ntext, no annotations",
    "start": "1642030",
    "end": "1649800"
  },
  {
    "text": "because you don't\nneed annotations to do language model pretraining. The existence of that\nsequence of words",
    "start": "1649800",
    "end": "1655020"
  },
  {
    "text": "that someone has\nwritten provides you with all these pairs\nof input and output.",
    "start": "1655020",
    "end": "1660950"
  },
  {
    "text": "Input \"Iroh,\" output \"goes.\" Input \"Iroh goes--\" output \"to.\" Those are all labels\nthat you've constructed",
    "start": "1660950",
    "end": "1667840"
  },
  {
    "text": "from the input just existing. But in most languages, even\non the entire internet--",
    "start": "1667840",
    "end": "1672940"
  },
  {
    "text": "there's about 7,000-ish\nlanguages on Earth, and most of them don't have the\nsort of billions of words that",
    "start": "1672940",
    "end": "1681220"
  },
  {
    "text": "you might want to\ntrain these systems on. Yeah.",
    "start": "1681220",
    "end": "1686789"
  },
  {
    "text": "If you're pretraining\non the entire thing, are you still learning one\nvector representation per word? The question is, if you're\npretraining the entire thing,",
    "start": "1686790",
    "end": "1693640"
  },
  {
    "text": "do you still learn one vector\nrepresentation per word? You learn one vector\nrepresentation, that is the noncontextual\ninput vector.",
    "start": "1693640",
    "end": "1701452"
  },
  {
    "text": "So you have your\nvocabulary matrix, you've got your\nembedding matrix that is vocabulary size by\nmodel dimensionality.",
    "start": "1701452",
    "end": "1708640"
  },
  {
    "text": "And so yeah, \"Iroh\" has\none vector. \"--goes\" has one vector. But then the transformer that\nyou're learning on top of it",
    "start": "1708640",
    "end": "1715520"
  },
  {
    "text": "takes in the sequence\nso far and sort of gives a vector to\neach of them that's dependent on the\ncontext in that case.",
    "start": "1715520",
    "end": "1721640"
  },
  {
    "text": "But still, at the\ninput, you only have one embedding per word. ",
    "start": "1721640",
    "end": "1728320"
  },
  {
    "text": "So what metric would you use to\nevaluate the pretrained model? It's supposed to be general.",
    "start": "1728320",
    "end": "1733705"
  },
  {
    "text": "But there's\napplication-specific metrics. So which one do you use? Yeah, so the question\nis, what metric do you use to evaluate\npretrained models",
    "start": "1733705",
    "end": "1740200"
  },
  {
    "text": "since it's supposed\nto be so general? --but there are lots of\nvery specific evaluations you could use.",
    "start": "1740200",
    "end": "1747190"
  },
  {
    "text": "We'll get into a lot of that\nin the rest of the lecture. While you're training it,\nyou can use simple metrics that sort of correlate\nwith what you want but",
    "start": "1747190",
    "end": "1753970"
  },
  {
    "text": "aren't actually what you want,\njust like the probability quality, right? So you can evaluate the\nperplexity of your language",
    "start": "1753970",
    "end": "1760403"
  },
  {
    "text": "model, just like you\nwould have when you cared about language modeling. And it turns out to be the\ncase that better perplexity",
    "start": "1760403",
    "end": "1766630"
  },
  {
    "text": "correlates with all the stuff\nthat's much harder to evaluate, like lots and lots\nof different tasks.",
    "start": "1766630",
    "end": "1772210"
  },
  {
    "text": "But also, the natural\nlanguage processing community has built very large\nof benchmark suites",
    "start": "1772210",
    "end": "1777700"
  },
  {
    "text": "of varying tasks to\ntry to get at sort of a notion of generality. Although that's\nvery, very difficult.",
    "start": "1777700",
    "end": "1783400"
  },
  {
    "text": "It's ill-defined, even. And so when you develop\nnew pretraining methods, what you often do\nis you try to pick",
    "start": "1783400",
    "end": "1789770"
  },
  {
    "text": "a whole bunch of evaluations\nand show that you do better on all of them. And that's your\nargument for generality.",
    "start": "1789770",
    "end": "1795530"
  },
  {
    "text": "OK So why should this sort of\npretraining, fine-tuning,",
    "start": "1795530",
    "end": "1802940"
  },
  {
    "text": "two-part paradigm help? This is still an open\narea of research,",
    "start": "1802940",
    "end": "1808980"
  },
  {
    "text": "but the intuitions\nare all you're going to take from this course. So right? So pretraining provides some\nstarting parameters, L theta.",
    "start": "1808980",
    "end": "1817500"
  },
  {
    "text": "So this is all the parameters\nin your network, right, from trying to do this minimum\nover all possible settings",
    "start": "1817500",
    "end": "1823549"
  },
  {
    "text": "of your parameters of\nthe pretraining loss. And then, the\nfine-tuning process takes your data for fine-tuning.",
    "start": "1823550",
    "end": "1831110"
  },
  {
    "text": "You've got some labels. And it tries to\napproximate the minimum through gradient descent of\nthe loss of the fine-tuning",
    "start": "1831110",
    "end": "1837800"
  },
  {
    "text": "task of theta. But you start at\ntheta hat, right? So you start gradient\ndescent at theta hat,",
    "start": "1837800",
    "end": "1843840"
  },
  {
    "text": "which your pretraining\nprocess gave you. And then, if you could actually\nsolve this min and wanted to,",
    "start": "1843840",
    "end": "1851809"
  },
  {
    "text": "it sort of feels like\nthe starting point shouldn't matter. But it really,\nreally, really does.",
    "start": "1851810",
    "end": "1857809"
  },
  {
    "text": "It really does. So that's-- and we'll talk\na bit more about this later.",
    "start": "1857810",
    "end": "1863840"
  },
  {
    "text": "But the process of\ngradient descent, maybe it stick relatively\nclose to the theta hat",
    "start": "1863840",
    "end": "1869870"
  },
  {
    "text": "during fine-tuning, ri ght? So you start at theta\nhat, and then you",
    "start": "1869870",
    "end": "1875180"
  },
  {
    "text": "walk downhill with gradient\ndescent until you hit a valley. And that valley ends up being\nreally good because it's",
    "start": "1875180",
    "end": "1882230"
  },
  {
    "text": "close to the pretraining\nparameters, which were really good for a lot of things. This is a cool place where\nsort of practice and theory",
    "start": "1882230",
    "end": "1889260"
  },
  {
    "text": "are sort of meeting, where\noptimization people want to understand why\nthis is so useful.",
    "start": "1889260",
    "end": "1894730"
  },
  {
    "text": "NLP people just want to\nbuild better systems. So yeah, maybe the\nstuff around theta hat",
    "start": "1894730",
    "end": "1903210"
  },
  {
    "text": "tends to generalize well. If you want to work\non this kind of thing, you should talk about it. Yeah.",
    "start": "1903210",
    "end": "1908280"
  },
  {
    "text": "So if stochastic\ngradient descent sticks relatively close. But what if we were to\nuse a different optimizer?",
    "start": "1908280",
    "end": "1913769"
  },
  {
    "text": "How would that\nchange our results? The question is, if\nstochastic gradient descent",
    "start": "1913770",
    "end": "1919380"
  },
  {
    "text": "sticks relatively close, what\nif we use a different optimizer? If we use any common\nvariant or gradient descent,",
    "start": "1919380",
    "end": "1925530"
  },
  {
    "text": "like any first-order\nmethod like, Adam, which we use in\nthis course, or AdaGrad, or-- they all have these\nvery, very similar properties.",
    "start": "1925530",
    "end": "1934710"
  },
  {
    "text": "Other types of optimization,\nwe just tend to not use. So who knows? Yeah.",
    "start": "1934710",
    "end": "1939750"
  },
  {
    "text": "[INAUDIBLE] still unclear\non why the pretraining plus fine-tuning works better\nthan just fine-tuning",
    "start": "1939750",
    "end": "1945809"
  },
  {
    "text": "but making the [INAUDIBLE]\nadding more layers, more data, et cetera? Yeah, the question is, why\ndoes the pretrained, fine-tune",
    "start": "1945810",
    "end": "1952610"
  },
  {
    "text": "paradigm work better than just\nmaking the model more powerful, adding more layers, adding more\ndata to just the fine-tuning?",
    "start": "1952610",
    "end": "1961715"
  },
  {
    "text": "The simple answer is that you\nhave orders of magnitude more data that's unlabeled.",
    "start": "1961715",
    "end": "1968440"
  },
  {
    "text": "That's just text that you found. Then you do carefully\nlabeled data and the task",
    "start": "1968440",
    "end": "1974420"
  },
  {
    "text": "that you care about because\nthat's expensive to get. It has to be examples of your\nmovie reviews or whatever",
    "start": "1974420",
    "end": "1980060"
  },
  {
    "text": "that you've had someone\nlabel carefully. So you have something\nlike, on the internet,",
    "start": "1980060",
    "end": "1987710"
  },
  {
    "text": "at least 5 trillion, maybe\n10 trillion words of this,",
    "start": "1987710",
    "end": "1992840"
  },
  {
    "text": "and you have maybe a million\nwords of your labeled data or whatever over here. So it's just the\nscale is way off.",
    "start": "1992840",
    "end": "2001179"
  },
  {
    "text": "But there's also an\nintuition that learning to do a very, very simple\nthing, like sentiment analysis,",
    "start": "2001180",
    "end": "2007929"
  },
  {
    "text": "is not going to get you a\nvery generally able agent",
    "start": "2007930",
    "end": "2014860"
  },
  {
    "text": "in a wide range of settings\ncompared to language modeling. So it's hard to get--",
    "start": "2014860",
    "end": "2020679"
  },
  {
    "text": "how do I put it? Even if you have a lot of\nlabeled data of movie reviews of the kind that people\nare writing today,",
    "start": "2020680",
    "end": "2028400"
  },
  {
    "text": "maybe tomorrow\nthey'll start writing slightly different\nkinds of movie reviews, and your system doesn't\nperform as well.",
    "start": "2028400",
    "end": "2033410"
  },
  {
    "text": "Whereas if you pretrained on\na really diverse set of texts from a wide range of\nsources and people,",
    "start": "2033410",
    "end": "2038690"
  },
  {
    "text": "it might be more adaptable to\nseeing stuff that doesn't quite look like the training\ndata you showed it,",
    "start": "2038690",
    "end": "2044990"
  },
  {
    "text": "even if you showed it\na ton of training data. So one of the big\ntakeaways of pretraining",
    "start": "2044990",
    "end": "2050210"
  },
  {
    "text": "is that you get this huge\namount of variety of sort of text on the internet.",
    "start": "2050210",
    "end": "2055579"
  },
  {
    "text": "And you have to be very careful. Yeah, you should be very\ncareful about what kind of text you're showing it and what\nkind of text you're not",
    "start": "2055580",
    "end": "2062330"
  },
  {
    "text": "because the internet is\nfull of awful text as well.",
    "start": "2062330",
    "end": "2067550"
  },
  {
    "text": "But some of that\ngenerality just comes from how hard this problem\nis and how much data you can show it.",
    "start": "2067550",
    "end": "2074210"
  },
  {
    "text": "Is the pretrained model just\nlike trained on so much data? How do you then train it so\nthat it considers the stuff",
    "start": "2074210",
    "end": "2082109"
  },
  {
    "text": "that you're fine-tuning it with\nas more important, more salient to the task it's\ntrying to do rather than just one in a billion\narticles of data or something?",
    "start": "2082110",
    "end": "2090399"
  },
  {
    "text": "Yeah, so the question is,\ngiven that the amount of data on the pretraining side\nis orders of magnitude",
    "start": "2090400",
    "end": "2096270"
  },
  {
    "text": "more than the amount of data\non the fine-tuning side, how do you sort of get\nacross to the model that, well, OK,\nactually, the fine-tuning",
    "start": "2096270",
    "end": "2101936"
  },
  {
    "text": "task is what I care about. So focus on that. It's about the fact\nthat I did this first,",
    "start": "2101936",
    "end": "2107160"
  },
  {
    "text": "the pretraining first, and then\nI do the fine-tuning second. So I've done-- I've gotten\nmy parameter initialization",
    "start": "2107160",
    "end": "2114220"
  },
  {
    "text": "from this. I've set it somewhere. And then I fine-tune it. I move to where the parameters\nare doing well for this task",
    "start": "2114220",
    "end": "2120900"
  },
  {
    "text": "afterwards. And so, well, it might\njust forget a lot about how to do this\nbecause now I'm just asking",
    "start": "2120900",
    "end": "2127470"
  },
  {
    "text": "it to do this at this point. I should move on, I think.",
    "start": "2127470",
    "end": "2132630"
  },
  {
    "text": "But we're going to keep talking\nabout this in much more detail with more concrete elements.",
    "start": "2132630",
    "end": "2138130"
  },
  {
    "text": "So OK. So let's talk about\nmodel pretraining.",
    "start": "2138130",
    "end": "2144880"
  },
  {
    "text": "Oh, wait. That did not advance the slides. ",
    "start": "2144880",
    "end": "2153760"
  },
  {
    "text": "Nice, OK. Let's talk about model\npretraining three ways. In our Transformers\nlecture, Tuesday, we",
    "start": "2153760",
    "end": "2161680"
  },
  {
    "text": "talked about encoders,\nencoder-decoders, and decoders. And we'll do decoders\nlast because, actually,",
    "start": "2161680",
    "end": "2168520"
  },
  {
    "text": "many of the largest models\nthat are being used today are all decoders.",
    "start": "2168520",
    "end": "2174070"
  },
  {
    "text": "And so we'll have a bit\nmore to say about them. So let's recall these three.",
    "start": "2174070",
    "end": "2179230"
  },
  {
    "text": "So encoders get\nbidirectional context. You have a single\nsequence, and you're able to see the whole thing,\nlike an encoder in machine",
    "start": "2179230",
    "end": "2186339"
  },
  {
    "text": "translation. Encoder-decoders\nhave one portion",
    "start": "2186340",
    "end": "2191670"
  },
  {
    "text": "of the network that gets\nbidirectional context. So that's like the source\nsentence of my machine translation system.",
    "start": "2191670",
    "end": "2197640"
  },
  {
    "text": "And then they're paired\nwith a decoder that gets unidirectional\ncontext so that I have this informational masking\nwhere I can't see the future so",
    "start": "2197640",
    "end": "2206517"
  },
  {
    "text": "that I can do things\nlike language modeling, I can generate the next token\nof my translation, whatever. So you could think\nof it as, I've",
    "start": "2206517",
    "end": "2212460"
  },
  {
    "text": "got my source sentence here and\nmy partial translation here, and I'm decoding\nout the translation.",
    "start": "2212460",
    "end": "2218789"
  },
  {
    "text": "And then decoders only are\nthings like language models where-- we've seen a\nlot of this so far. And there's pretraining for all\nthree large classes of models.",
    "start": "2218790",
    "end": "2228930"
  },
  {
    "text": "And how you pre-train\nthem, and then how you use them depends on the properties\nand the proclivities",
    "start": "2228930",
    "end": "2234120"
  },
  {
    "text": "of the specific architecture. So let's look at encoders first. So we've looked at language\nmodeling quite a bit,",
    "start": "2234120",
    "end": "2241270"
  },
  {
    "text": "but we can't do language\nmodeling with an encoder because they get\nbidirectional context.",
    "start": "2241270",
    "end": "2246510"
  },
  {
    "text": "So if I'm down here at \"I\"\nand I want to present-- I want to predict\nthe next word, it's",
    "start": "2246510",
    "end": "2253450"
  },
  {
    "text": "a trivial task at\nthis level here to predict the next word\nbecause, in the middle,",
    "start": "2253450",
    "end": "2259359"
  },
  {
    "text": "I was able to look\nat the next word. And so I should just know. There's nothing\nhard about learning",
    "start": "2259360",
    "end": "2264460"
  },
  {
    "text": "to predict the next word\nhere because I could just look at it, see what it\nis and then copy it over. So when I'm training an encoder\nin something for pretraining,",
    "start": "2264460",
    "end": "2274690"
  },
  {
    "text": "I have to be a little\nbit more clever. In practice, what I do\nis something like this.",
    "start": "2274690",
    "end": "2279730"
  },
  {
    "text": "I take the input, and\nI modify it somewhat. I mask out words like\nI did in the examples I gave at the\nbeginning of class.",
    "start": "2279730",
    "end": "2286000"
  },
  {
    "text": "So I blank to the blank. And then I have the network\npredict with its whole--",
    "start": "2286000",
    "end": "2293020"
  },
  {
    "text": "I have it build contextual\nrepresentations. So now this vector\nrepresentation of the blank sees the entire\ncontext around it here.",
    "start": "2293020",
    "end": "2302050"
  },
  {
    "text": "And then I predict\nthe word went, and then here the word store. ",
    "start": "2302050",
    "end": "2309210"
  },
  {
    "text": "Any questions? ",
    "start": "2309210",
    "end": "2314380"
  },
  {
    "text": "And you can see how\nthis is doing something quite a bit like\nlanguage modeling but with bidirectional context.",
    "start": "2314380",
    "end": "2321099"
  },
  {
    "text": "I've removed the\nnetwork's information about the words that\ngo in the blanks. And I'm training it\nto reconstruct that.",
    "start": "2321100",
    "end": "2327369"
  },
  {
    "text": "So I only have loss terms. So I only ask it to\nactually do the prediction, compute the loss, back-propagate\nthe gradients for the words",
    "start": "2327370",
    "end": "2334810"
  },
  {
    "text": "that I've masked out. And you can think of this as\ninstead of learning probability",
    "start": "2334810",
    "end": "2339940"
  },
  {
    "text": "of x, where x is like a\nsentence or a document, this is learning the probability\nof x, the real document given",
    "start": "2339940",
    "end": "2346540"
  },
  {
    "text": "x tilde, which is sort of this\ncorrupted document with some",
    "start": "2346540",
    "end": "2351640"
  },
  {
    "text": "of the information missing. And so we get this\nsequence of vectors here,",
    "start": "2351640",
    "end": "2357640"
  },
  {
    "text": "one per word, which is the\noutput of my encoder in blue. And then I'd say\nthat for the words",
    "start": "2357640",
    "end": "2362980"
  },
  {
    "text": "that I want to predict,\nyi, I draw them. This sim means\nthe probability is",
    "start": "2362980",
    "end": "2368890"
  },
  {
    "text": "proportional to my\nembedding matrix times my representation of it.",
    "start": "2368890",
    "end": "2376300"
  },
  {
    "text": "So it's just a\nlinear transformation of that last thing here. So this A plus b is\nthis red portion here,",
    "start": "2376300",
    "end": "2381805"
  },
  {
    "text": "and then do the prediction. And I train the entire\nnetwork to do this. Yes.",
    "start": "2381805",
    "end": "2387040"
  },
  {
    "text": "So the words that we\nmask out-- or do we just select them randomly, or\nis there something into it?",
    "start": "2387040",
    "end": "2394180"
  },
  {
    "text": "The question is, do we\njust choose words randomly to mask out, or\nis there a scheme? Mostly randomly.",
    "start": "2394180",
    "end": "2399420"
  },
  {
    "text": "We'll talk about a\nslightly smarter scheme in a couple of slides. But yeah, just mostly randomly.",
    "start": "2399420",
    "end": "2405400"
  },
  {
    "text": "Yeah. All right. Just what was that last\npart on the bottom, x of the max version of?",
    "start": "2405400",
    "end": "2411480"
  },
  {
    "text": "Like if it's the first or\nthe very last sentence? Yeah, so I'm saying\nthat I'm defining",
    "start": "2411480",
    "end": "2419100"
  },
  {
    "text": "X tilde to be this\ninput part where I've got the masked version\nof the sentence",
    "start": "2419100",
    "end": "2424830"
  },
  {
    "text": "with these sort\nof words missing. And then, I'm defining a\nprobability distribution that's the probability\nof a sequence conditioned",
    "start": "2424830",
    "end": "2432300"
  },
  {
    "text": "on the input being the\nsort of corrupted sequence, the masked sequence. ",
    "start": "2432300",
    "end": "2439570"
  },
  {
    "text": "OK. So this brings us to a\nvery, very popular NLP model",
    "start": "2439570",
    "end": "2447220"
  },
  {
    "text": "that you need to know about. It's called BERT. And it was the first one to\npopularize this masked language",
    "start": "2447220",
    "end": "2452619"
  },
  {
    "text": "modeling objective. And they released the weights\nof this pretrained transformer",
    "start": "2452620",
    "end": "2458343"
  },
  {
    "text": "that they pretrained\nvia something that looks a lot like\nmasked language modeling. And so these you\ncan download, you",
    "start": "2458343",
    "end": "2463810"
  },
  {
    "text": "can use them via\ncode that's released by the company Hugging Face that\nwe have continued to bring up.",
    "start": "2463810",
    "end": "2470170"
  },
  {
    "text": "Many of you will use a model\nlike BERT in your final project because it's such a useful\nbuilder of representations",
    "start": "2470170",
    "end": "2476410"
  },
  {
    "text": "of language and context. So let's talk a little bit about\nthe details of masked language modeling in BERT.",
    "start": "2476410",
    "end": "2483200"
  },
  {
    "text": "First, we take 15% of\nthe subword tokens. So remember, all of our\ninputs now are subword tokens.",
    "start": "2483200",
    "end": "2490370"
  },
  {
    "text": "I've made them all\nlook like words, but just like we saw at the\nvery beginning of class, each of these tokens could just\nbe some portion, some subword.",
    "start": "2490370",
    "end": "2498492"
  },
  {
    "text": "And I'm going to do a\ncouple of things with it. Sometimes I am going to\njust mask out the word",
    "start": "2498492",
    "end": "2505680"
  },
  {
    "text": "and then predict the true word. Sometimes I'm going\nto replace the word",
    "start": "2505680",
    "end": "2511200"
  },
  {
    "text": "with some random\nsample of another word from my distribution--\nfrom my vocabulary and predict the real word\nthat was supposed to go there.",
    "start": "2511200",
    "end": "2518430"
  },
  {
    "text": "And sometimes, I'm going to\nnot change the word at all and still predict it.",
    "start": "2518430",
    "end": "2523950"
  },
  {
    "text": "The intuition of this\nis the following. If I just had to build\ngood representations",
    "start": "2523950",
    "end": "2531630"
  },
  {
    "text": "in the middle of this network\nfor words that are masked out, then when I actually\nuse the model at test",
    "start": "2531630",
    "end": "2538620"
  },
  {
    "text": "time on some real-- review to do\nsentiment analysis on, well, there are never going\nto be any tokens like this.",
    "start": "2538620",
    "end": "2545230"
  },
  {
    "text": "So maybe the model\nwon't do a very good job because it's like,\noh, I have no job to do here because I only need\nto deal with the masked tokens.",
    "start": "2545230",
    "end": "2553240"
  },
  {
    "text": "By giving it sequences of\nwords, or sometimes it's the real word that needs to\nbe predicted-- sometimes,",
    "start": "2553240",
    "end": "2558940"
  },
  {
    "text": "you have to detect\nif the word is wrong. The idea is that\nnow when I give it a sentence that\ndoesn't have any masks,",
    "start": "2558940",
    "end": "2566530"
  },
  {
    "text": "it actually does a good job\nof representing all the words in context because it\nhas this chance that it",
    "start": "2566530",
    "end": "2571652"
  },
  {
    "text": "could be asked to predict\nanything at any time. ",
    "start": "2571652",
    "end": "2579280"
  },
  {
    "text": "So the folks at Google\nwho were defining this had a separate\nadditional task that",
    "start": "2579280",
    "end": "2588130"
  },
  {
    "text": "is sort of interesting\nto think about. So this was their BERT\nmodel from their paper.",
    "start": "2588130",
    "end": "2593313"
  },
  {
    "text": "They had their\nposition embeddings just like we saw from\nour transformers lecture, token embeddings\njust like we saw",
    "start": "2593313",
    "end": "2599590"
  },
  {
    "text": "from the transformers lecture. But then also they\nhad this thing called a segment embedding where\nthey had two possible segments,",
    "start": "2599590",
    "end": "2605680"
  },
  {
    "text": "segment A and segment B. And they had this\nadditional task",
    "start": "2605680",
    "end": "2611710"
  },
  {
    "text": "where they would get a big\nchunk of text for segment A and a big chunk of\ntext for segment B.",
    "start": "2611710",
    "end": "2616860"
  },
  {
    "text": "And then, they\nwould ask the model, is segment B real\ncontinuation of segment A?",
    "start": "2616860",
    "end": "2622930"
  },
  {
    "text": "Was it the text that\nactually came next, or did I just pick this\nbig segment randomly",
    "start": "2622930",
    "end": "2628000"
  },
  {
    "text": "from somewhere else? And the idea was that this\nshould teach the network something-- some notion of\nlong-distance coherence,",
    "start": "2628000",
    "end": "2634220"
  },
  {
    "text": "right, about the connection\nbetween a bunch of text over here and a bunch\nof text over there.",
    "start": "2634220",
    "end": "2640010"
  },
  {
    "text": "It turns out it's\nnot really necessary, but it's an interesting idea. And sort of similar\nthings have continued",
    "start": "2640010",
    "end": "2646870"
  },
  {
    "text": "to have some\ninfluence since then. But again, you should\nget this intuition",
    "start": "2646870",
    "end": "2651910"
  },
  {
    "text": "that we're trying to come\nup with hard problems for the network to solve\nsuch that by solving them, it has to learn a\nlot about language.",
    "start": "2651910",
    "end": "2659110"
  },
  {
    "text": "And we're defining\nthose problems by making simple transformations\nor removing information",
    "start": "2659110",
    "end": "2664900"
  },
  {
    "text": "from text that just\nhappened to occur.  Questions?",
    "start": "2664900",
    "end": "2672470"
  },
  {
    "text": "Yeah, the plus signs. Do we concatenate\nthe vectors, or do we do an element-wise addition?",
    "start": "2672470",
    "end": "2678387"
  },
  {
    "text": "The question is, for\nthese plus signs, do we concatenate the vectors\nor do element-wise addition? We do element-wise addition.",
    "start": "2678387",
    "end": "2685770"
  },
  {
    "text": "You could have\nconcatenated them. However, one of\nthe big conventions of all of these networks\nis that you always",
    "start": "2685770",
    "end": "2692330"
  },
  {
    "text": "have exactly the same\nnumber of dimensions everywhere at every\nlayer of the network. It just makes\neverything very simple.",
    "start": "2692330",
    "end": "2698303"
  },
  {
    "text": "So just saying everything's\nthe same dimension and then doing addition\njust ends up being simpler.",
    "start": "2698303",
    "end": "2704672"
  },
  {
    "text": "Yeah. So why is the\nnext-sentence prediction not necessary with an\nintuition for that?",
    "start": "2704673",
    "end": "2710770"
  },
  {
    "text": "Yeah, why was the next sentence\nprediction not necessary? One thing that it\ndoes that's a negative",
    "start": "2710770",
    "end": "2716380"
  },
  {
    "text": "is that now the effect of\ncontext length for a lot",
    "start": "2716380",
    "end": "2724240"
  },
  {
    "text": "of your examples is halved. So one of the things that's\nuseful about pretraining seemingly is that you get\nto build representations",
    "start": "2724240",
    "end": "2730930"
  },
  {
    "text": "of very long sequences of text. So this is very short. But in practice,\nsegment A was going",
    "start": "2730930",
    "end": "2736900"
  },
  {
    "text": "to be something like\n250 words and segment B was going to be 250 words. And in the paper\nthat let us know",
    "start": "2736900",
    "end": "2743800"
  },
  {
    "text": "that this wasn't\nnecessary they always had a long segment of 500 words. And it seemed to\nbe useful to always",
    "start": "2743800",
    "end": "2750370"
  },
  {
    "text": "have this very long context\nbecause longer contexts help give you more information\nabout the role",
    "start": "2750370",
    "end": "2757000"
  },
  {
    "text": "that each word is playing in\nthat specific context, right? If I see one word,\nit's hard to know.",
    "start": "2757000",
    "end": "2762040"
  },
  {
    "text": "If I just see a \"record,\"\nit's hard to know what it's supposed to mean. But if I see a thousand\nwords around it, it's much clearer what its\nrole is in that context is?",
    "start": "2762040",
    "end": "2769480"
  },
  {
    "text": "So yeah, it cuts the effect\nof context size is one answer. ",
    "start": "2769480",
    "end": "2776910"
  },
  {
    "text": "Well, another thing is that\nthis is actually much more difficult. This is a\nmuch more recent paper that I don't have in\nthe slides, but it's",
    "start": "2776910",
    "end": "2783313"
  },
  {
    "text": "been shown since then that\nthese models are really, really bad at the next-sentence\nprediction task.",
    "start": "2783313",
    "end": "2788770"
  },
  {
    "text": "So it could be that maybe it\njust was too hard at the time.",
    "start": "2788770",
    "end": "2794700"
  },
  {
    "text": "And so it just wasn't\nuseful because the model was failing to do it at all. So I'll give the link\nfor that paper later.",
    "start": "2794700",
    "end": "2803650"
  },
  {
    "text": "Can you explain\nagain why we need to do next-sentence prediction? What about just masking\nand predicting the next?",
    "start": "2803650",
    "end": "2809109"
  },
  {
    "text": "I missed that jump, so the\nnext-sentence prediction. Yeah, so the question\nis, why do we need to do next sentence prediction?",
    "start": "2809110",
    "end": "2814902"
  },
  {
    "text": "Why not just do the\nmasking we saw before? That's the thing. You seem to not need to\ndo next since prediction. But as like history\nof the research,",
    "start": "2814902",
    "end": "2822760"
  },
  {
    "text": "it was thought that\nthis was useful. And the idea was\nthat it required you to develop this\nsort of pairwise,",
    "start": "2822760",
    "end": "2830260"
  },
  {
    "text": "like do these two\nsegments of text interact? How do they interact? Are they related? This sort of the\nlonger distance notion.",
    "start": "2830260",
    "end": "2836890"
  },
  {
    "text": "And many NLP tasks are\ndefined on pairs of things. And they thought\nthat might be useful.",
    "start": "2836890",
    "end": "2842780"
  },
  {
    "text": "And so they published\nit with this. And then someone\nelse came through and published a new model\nthat didn't do that.",
    "start": "2842780",
    "end": "2848099"
  },
  {
    "text": "And it sort of did better. So this is just-- so yeah,\nthere are intuitions as",
    "start": "2848100",
    "end": "2854680"
  },
  {
    "text": "to why it could work. It just didn't. So BERT wasn't doing masking\nor it was doing both? It was doing both.",
    "start": "2854680",
    "end": "2860140"
  },
  {
    "text": "It was doing both\nthis next-sentence-- so BERT was doing both this\nnext-sentence prediction",
    "start": "2860140",
    "end": "2865930"
  },
  {
    "text": "training as well as this masking\ntraining all at the same time.",
    "start": "2865930",
    "end": "2871900"
  },
  {
    "text": "And so you had to have\na separate predictor head on top of BERT, a separate\npredictor classification thing.",
    "start": "2871900",
    "end": "2879430"
  },
  {
    "text": "And so one detail\nthere is that there's this special word at\nthe beginning of BERT in every sequence, that CLS.",
    "start": "2879430",
    "end": "2886859"
  },
  {
    "text": "And you can define\na predictor on top of that fake word embedding\nthat was going to say",
    "start": "2886860",
    "end": "2893280"
  },
  {
    "text": "is the next sentence\nis real or fake or not. OK, I'm going to move on.",
    "start": "2893280",
    "end": "2900390"
  },
  {
    "text": "And so this gets at\nthe question that we had earlier about how do\nyou evaluate these things.",
    "start": "2900390",
    "end": "2905400"
  },
  {
    "text": "There's a lot of different\nNLP tasks out there. Gosh. And when people were\ndefining these papers,",
    "start": "2905400",
    "end": "2912050"
  },
  {
    "text": "they would look at a ton\nof different evaluations that had been sort of compiled\nas a set of things that are still hard for today's systems.",
    "start": "2912050",
    "end": "2918620"
  },
  {
    "text": "So are you detecting paraphrases\nbetween questions or two Quora questions, actually,\nthe same question?",
    "start": "2918620",
    "end": "2924230"
  },
  {
    "text": "That turns out to be hard. Can you do sentiment analysis\non this hard data set?",
    "start": "2924230",
    "end": "2931400"
  },
  {
    "text": "Can you tell if sentences are\nlinguistically acceptable, are they grammatical or not? Are two sequences\nsimilar semantically?",
    "start": "2931400",
    "end": "2938960"
  },
  {
    "text": "Do they mean vaguely\na similar thing? And we'll talk a bit about\nnatural language inference",
    "start": "2938960",
    "end": "2944060"
  },
  {
    "text": "later. But that's the\ntask of defining-- if I say I saw the dog,\nthat does not necessarily",
    "start": "2944060",
    "end": "2951380"
  },
  {
    "text": "mean I saw the little dog. But saying I saw the little\ndog does mean I saw the dog.",
    "start": "2951380",
    "end": "2957859"
  },
  {
    "text": "So that's this natural\nlanguage inference task. And the striking--\nthe difference between pretraining days\nwhere you had this row here,",
    "start": "2957860",
    "end": "2969390"
  },
  {
    "text": "before you had substantial\namounts of pretraining, and BERT was just like the field\nwas taken aback in a way that's",
    "start": "2969390",
    "end": "2977610"
  },
  {
    "text": "hard to describe, very\ncarefully crafted architectures for each individual task\nwhere everyone was designing",
    "start": "2977610",
    "end": "2984512"
  },
  {
    "text": "their own neural network and\ndoing things that they thought were clever as how to define all\nthe connections and the weights",
    "start": "2984512",
    "end": "2990330"
  },
  {
    "text": "and whatever to do their\ntasks independently. So everyone was doing\na different thing for each one of\nthese tasks roughly.",
    "start": "2990330",
    "end": "2997170"
  },
  {
    "text": "All of that was blown\nout of the water by just building\na big transformer and just teach it to\npredict the missing",
    "start": "2997170",
    "end": "3002960"
  },
  {
    "text": "words a whole bunch\nand then fine-tune it on each of these tasks. So this was just a sea\nchange in the field.",
    "start": "3002960",
    "end": "3009740"
  },
  {
    "text": "People were amazed. It's a little bit less flashy\nthan ChatGPT, I'll admit. But it's really part of the\nstory that gets us to it.",
    "start": "3009740",
    "end": "3018920"
  },
  {
    "text": "OK, questions.  So to get stuff out of the--",
    "start": "3018920",
    "end": "3028021"
  },
  {
    "text": "during the encoder\npretraining stage, encoder usually outputs\nsome hidden values.",
    "start": "3028021",
    "end": "3036520"
  },
  {
    "text": "How do we correlate\nthose to words that we are trying\nto test against? So the question is,\nthe encoder output",
    "start": "3036520",
    "end": "3044710"
  },
  {
    "text": "is a bunch of hidden values? And how do we actually\ncorrelate those values to stuff",
    "start": "3044710",
    "end": "3051279"
  },
  {
    "text": "that we want to predict? I'm going to move\non to the next slide here to bring up this\nexample here, right? So the encoder gives us,\nfor each input word token,",
    "start": "3051280",
    "end": "3060069"
  },
  {
    "text": "a vector of that\ntoken that represents the token in context. And the question is, how do\nwe get these representations",
    "start": "3060070",
    "end": "3067480"
  },
  {
    "text": "and turn them into answers for\nthe tasks that we care about?",
    "start": "3067480",
    "end": "3072830"
  },
  {
    "text": "And the answer comes back to\nsomething like this maybe.",
    "start": "3072830",
    "end": "3092325"
  },
  {
    "start": "3092325",
    "end": "3098099"
  },
  {
    "text": "I'm not sure. So when we were doing\nthe pretraining, we had the transformer that was\ngiving us our representations.",
    "start": "3098100",
    "end": "3103710"
  },
  {
    "text": "And we had this little last\nlayer here, this little affine transformation that moved us\nfrom the encoder's hidden state",
    "start": "3103710",
    "end": "3111839"
  },
  {
    "text": "size to the vocabulary\nto do our prediction. And we just remove this\nlast prediction layer here.",
    "start": "3111840",
    "end": "3118050"
  },
  {
    "text": "And let's say we want to do\nsomething that is classifying",
    "start": "3118050",
    "end": "3123160"
  },
  {
    "text": "the sentiment of the sentence. We just pick, arbitrarily, maybe\nthe last word in the sentence. And we stick a linear\nclassifier on top",
    "start": "3123160",
    "end": "3131280"
  },
  {
    "text": "and map it to\npositive or negative, and then fine-tune\nthe whole thing. OK.",
    "start": "3131280",
    "end": "3137100"
  },
  {
    "text": "So yeah, the BERT model\nhad two different models. One was 110 million parameters.",
    "start": "3137100",
    "end": "3142859"
  },
  {
    "text": "One was 340 million. Keep that sort of in the\nback of your head sort of percolating as we talk\nabout models with many, many",
    "start": "3142860",
    "end": "3149369"
  },
  {
    "text": "more parameters later on. It was trained on 800\nbillion words plus",
    "start": "3149370",
    "end": "3158310"
  },
  {
    "text": "that is definitely wrong. Maybe 25 million words. But on the order of less\nthan a billion words of text.",
    "start": "3158310",
    "end": "3165540"
  },
  {
    "text": "Quite a bit still. And it was trained on what\nwas considered at the time",
    "start": "3165540",
    "end": "3170609"
  },
  {
    "text": "to be a whole lot of compute. It was Google doing this,\nand they released it, and we were like, oh, who\nhas that kind of compute",
    "start": "3170610",
    "end": "3177442"
  },
  {
    "text": "but Google? Although, nowadays, it's not\nconsidered to be very much. But fine-tuning is practical\nand common on a single GPU.",
    "start": "3177442",
    "end": "3184550"
  },
  {
    "text": "So you could take the\nBERT model that they spent a lot of time training\nand fine-tune it yourself on your task on even\na very small GPU.",
    "start": "3184550",
    "end": "3194160"
  },
  {
    "text": " OK. ",
    "start": "3194160",
    "end": "3201210"
  },
  {
    "text": "So one question is, well,\nthis seems really great. Why don't we just use\nthis for everything?",
    "start": "3201210",
    "end": "3206950"
  },
  {
    "text": " Yeah. And the answer is, well, what\nis the pretraining objective?",
    "start": "3206950",
    "end": "3214950"
  },
  {
    "text": "What's the structure of the\npretrained model good for? BERT is really good for\nfilling in the blanks,",
    "start": "3214950",
    "end": "3221610"
  },
  {
    "text": "but it's much less\nnaturally used for actually generating text.",
    "start": "3221610",
    "end": "3226710"
  },
  {
    "text": "So I wouldn't want to\nuse BERT to generate a summary of something because\nit's not really built for it.",
    "start": "3226710",
    "end": "3232320"
  },
  {
    "text": "It's not. It doesn't have a natural notion\nof predicting the next word, given all the words\nthat came before it.",
    "start": "3232320",
    "end": "3238180"
  },
  {
    "text": "So maybe I want to use BERT if\nI want a good representation of, say, a document,\nto classify it,",
    "start": "3238180",
    "end": "3243270"
  },
  {
    "text": "give it one of a\nset of topic labels, just say it's toxic or\nnon-toxic or whatever. But I wouldn't want to use it\nto generate a whole sequence.",
    "start": "3243270",
    "end": "3253060"
  },
  {
    "text": "OK, some extensions of BERT--\nso we had a question earlier of whether you just mask\nthings out randomly.",
    "start": "3253060",
    "end": "3258730"
  },
  {
    "text": "One thing that\nseems to work better is you mask out whole\ncontiguous spans",
    "start": "3258730",
    "end": "3266049"
  },
  {
    "text": "because the difficulty\nof this problem",
    "start": "3266050",
    "end": "3271260"
  },
  {
    "text": "is much easier than\nit would otherwise be because this is\npart of irresistibly,",
    "start": "3271260",
    "end": "3276930"
  },
  {
    "text": "and you can tell very easily\nbased on the subwords that came before it. Whereas if I have a much longer\nsequence, it's a trade-off.",
    "start": "3276930",
    "end": "3285390"
  },
  {
    "text": "But this might be\na harder problem, and it ends up being better\nto do this span-based masking",
    "start": "3285390",
    "end": "3291420"
  },
  {
    "text": "than random masking,\nand that might be because subwords make very\nsimple prediction problems when",
    "start": "3291420",
    "end": "3296609"
  },
  {
    "text": "you mask out just\none subword of a word versus all the\nsubwords of a word.",
    "start": "3296610",
    "end": "3302400"
  },
  {
    "text": "So this ends up\ndoing much better. There's also a paper\ncalled the RoBERTa paper which showed that the next\nsentence prediction wasn't",
    "start": "3302400",
    "end": "3310440"
  },
  {
    "text": "necessarily. They also showed that they\nreally should have trained it on a lot more text.",
    "start": "3310440",
    "end": "3316530"
  },
  {
    "text": "So RoBERTa is a drop-in\nreplacement for BERT. So if you're thinking of\nusing BERT, just use RoBERTa.",
    "start": "3316530",
    "end": "3321660"
  },
  {
    "text": "It's better. And it gave us this\nintuition that we really don't know a whole lot about\nthe best practices for training",
    "start": "3321660",
    "end": "3326758"
  },
  {
    "text": "these things. You sort of train it for as\nlong as you're willing to, and things do good\nstuff and whatever.",
    "start": "3326758",
    "end": "3333590"
  },
  {
    "text": "So this is very-- but it's very difficult\nto do sort of iteration on these models\nbecause they're big.",
    "start": "3333590",
    "end": "3339220"
  },
  {
    "text": "It's expensive to train them. Another thing that you should\nknow for your final projects",
    "start": "3339220",
    "end": "3344980"
  },
  {
    "text": "in the world ahead\nis this notion of fine-tuning all\nparameters of the network versus just a couple of them.",
    "start": "3344980",
    "end": "3350900"
  },
  {
    "text": "So what we've\ntalked about so far is you pre-train\nall the parameters, and then you fine-tune\nall of them as well.",
    "start": "3350900",
    "end": "3356522"
  },
  {
    "text": "So all the parameter\nvalues change. An alternative, which you\ncall parameter efficient or",
    "start": "3356522",
    "end": "3361839"
  },
  {
    "text": "lightweight fine-tuning,\nyou choose little bits of parameters, or you\nchoose some very smart way",
    "start": "3361840",
    "end": "3367840"
  },
  {
    "text": "of keeping most of the\nparameters fixed and only fine-tuning others. And the intuition is that\nthese pretrained parameters",
    "start": "3367840",
    "end": "3374319"
  },
  {
    "text": "were really good. And you want to make\nthe minimal change from the pretrained\nmodel to the model that",
    "start": "3374320",
    "end": "3380680"
  },
  {
    "text": "does what you want\nso that you keep some of the generality,\nsome of the goodness of the pretraining.",
    "start": "3380680",
    "end": "3385930"
  },
  {
    "text": "So one way that this is done\nis called prefix tuning. Prompt tuning is very similar. --where you actually freeze all\nthe parameters of the network.",
    "start": "3385930",
    "end": "3393320"
  },
  {
    "text": "So I've pretrained\nmy network here. And I never change any\nof the parameter values.",
    "start": "3393320",
    "end": "3399610"
  },
  {
    "text": "Instead, I make a bunch of fake\nsort of pseudo-word vectors that I prepend to the very\nbeginning of the sequence.",
    "start": "3399610",
    "end": "3407180"
  },
  {
    "text": "And I train just them,\nit's sort of unintuitive. It's like these would have been\nlike inputs to the network,",
    "start": "3407180",
    "end": "3413360"
  },
  {
    "text": "but I'm specifying\nthem as parameters, and I'm training everything\nto do my sentiment analysis task just by changing the values\nof these sort of fake words.",
    "start": "3413360",
    "end": "3422920"
  },
  {
    "text": "And this is nice because I get\nto keep all the good pretrained parameters and then just\nspecify the sort of diff that",
    "start": "3422920",
    "end": "3432339"
  },
  {
    "text": "ends up generalizing better. This is a very open\nfield of research.",
    "start": "3432340",
    "end": "3437349"
  },
  {
    "text": "But this is also\ncheaper because I don't have to compute\nthe gradients, or I don't have to store the\ngradients and all the optimizer",
    "start": "3437350",
    "end": "3444550"
  },
  {
    "text": "states with respect to\nall these parameters. I'm only training a very\nsmall number of parameters.",
    "start": "3444550",
    "end": "3450700"
  },
  {
    "text": "Yeah. Does it it make any difference\nto put these state parameters [INAUDIBLE] as if [INAUDIBLE]?",
    "start": "3450700",
    "end": "3457998"
  },
  {
    "text": "It doesn't make any\ndifference to put these at the end or the beginning. In a decoder, you have to\nput them at the beginning",
    "start": "3457998",
    "end": "3463510"
  },
  {
    "text": "because otherwise, you\ndon't see them before you process the whole sequence.",
    "start": "3463510",
    "end": "3468670"
  },
  {
    "text": "Yes. Can we just attach new\nlayers and only train the new layers [INAUDIBLE]? The question is, can we\njust attach the new layers",
    "start": "3468670",
    "end": "3475540"
  },
  {
    "text": "at the sort of top of\nthis and only train those? Absolutely. This works a bit better. Another thing that\nworks well, sorry we're",
    "start": "3475540",
    "end": "3482620"
  },
  {
    "text": "running out of time, is\ntaking each weight matrix. So I have a bunch of weight\nmatrices in my transformer,",
    "start": "3482620",
    "end": "3489460"
  },
  {
    "text": "and I freeze the\nweight matrix and learn a very low-rank little diff.",
    "start": "3489460",
    "end": "3495130"
  },
  {
    "text": "And I set the weight\nmatrix's value to be the original value\nplus my very low-rank diff",
    "start": "3495130",
    "end": "3503559"
  },
  {
    "text": "from the original one. And this ends up being a very\nsimilarly useful technique.",
    "start": "3503560",
    "end": "3509450"
  },
  {
    "text": "And the overall idea\nhere is that, again, I'm learning way fewer\nparameters than I did via pretraining and\nfreezing most of the pretraining",
    "start": "3509450",
    "end": "3517029"
  },
  {
    "text": "parameters. OK, encoder-decoders. So for encoder-decoders,\nwe could do something",
    "start": "3517030",
    "end": "3523770"
  },
  {
    "text": "like language modeling. I've got my input sequence here,\nencoder output sequence here.",
    "start": "3523770",
    "end": "3529560"
  },
  {
    "text": "And I could say this part\nis my prefix for having bidirectional context.",
    "start": "3529560",
    "end": "3534840"
  },
  {
    "text": "And I could then predict\nall the words that are in the latter\nhalf of the sequence,",
    "start": "3534840",
    "end": "3540510"
  },
  {
    "text": "just like a language model,\nand that would work fine. And so this is something\nthat you could do, right?",
    "start": "3540510",
    "end": "3547080"
  },
  {
    "text": "You take a long text, split\nit into two, give half of it to the encoder, and then\ngenerate the second half",
    "start": "3547080",
    "end": "3552690"
  },
  {
    "text": "with the decoder.  But in practice, what\nworks much better",
    "start": "3552690",
    "end": "3558440"
  },
  {
    "text": "is this notion of\nspan corruption. Span corruption is going to\nshow up in your assignment 5. And the idea here\nis a lot like BERT",
    "start": "3558440",
    "end": "3566450"
  },
  {
    "text": "but in a sort of\ngenerative sense where I'm going to mask\nout a bunch of words",
    "start": "3566450",
    "end": "3572450"
  },
  {
    "text": "in the input \"Thank you--\" mask\ntoken one, \"me to your party--\"",
    "start": "3572450",
    "end": "3577579"
  },
  {
    "text": "mask token two, \"week.\" And then, at the output,\nI generate the mask token,",
    "start": "3577580",
    "end": "3584370"
  },
  {
    "text": "and then what was\nsupposed to be there, but the mask token replaced it. So \"Thank you,\" then\npredict \"--for inviting--\"",
    "start": "3584370",
    "end": "3591210"
  },
  {
    "text": "at the output, \"--me to\nyour party last week.\" And what this does\nis that it allows you",
    "start": "3591210",
    "end": "3597900"
  },
  {
    "text": "to have bidirectional context. I get to see the whole\nsequence, except I can generate",
    "start": "3597900",
    "end": "3605160"
  },
  {
    "text": "the parts that were missing. So this feels a\nlittle bit like BERT. You mask out parts of the\ninput, but you actually",
    "start": "3605160",
    "end": "3610590"
  },
  {
    "text": "generate the output\nas a sequence like you would in\nlanguage modeling. So this might be\ngood for something",
    "start": "3610590",
    "end": "3615930"
  },
  {
    "text": "like machine translation,\nwhere I have an input that I want\nbidirectional context in, but then I want to\ngenerate an output",
    "start": "3615930",
    "end": "3622079"
  },
  {
    "text": "and I want to pre-train\nthe whole thing. So this was shown to work\nbetter than language modeling at the scales that\nthese folks at Google",
    "start": "3622080",
    "end": "3629490"
  },
  {
    "text": "were able to test back in 2018. This is still quite popular. ",
    "start": "3629490",
    "end": "3635580"
  },
  {
    "text": "Yeah, there's a lot of numbers. It works better than\nthe other stuff. I'm not going to worry about it.",
    "start": "3635580",
    "end": "3643020"
  },
  {
    "text": "There's a fascinating\nproperty of these models also. So T5 was the model that\nwas originally introduced",
    "start": "3643020",
    "end": "3651000"
  },
  {
    "text": "with Salient Span Masking. And you can think of-- at pretraining time,\nyou saw a bunch",
    "start": "3651000",
    "end": "3656369"
  },
  {
    "text": "of things like \"Franklin D.\nRoosevelt was born in--\" blank, and you generated out the blank.",
    "start": "3656370",
    "end": "3662280"
  },
  {
    "text": "And there's this task called\nOpen-Domain Question Answering,",
    "start": "3662280",
    "end": "3668280"
  },
  {
    "text": "which has a bunch\nof trivia questions, like \"When was Franklin\nD. Roosevelt born?\" And you're supposed to generate\nout the answer as a string,",
    "start": "3668280",
    "end": "3676380"
  },
  {
    "text": "just from your parameters. So you did a bunch\nof pretraining, you saw a bunch\nof text, and then you're supposed to\ngenerate these answers.",
    "start": "3676380",
    "end": "3682589"
  },
  {
    "text": "And what's fascinating is\nthat this sort of Salient Span Masking method allowed\nyou to pre-train and then",
    "start": "3682590",
    "end": "3691820"
  },
  {
    "text": "fine-tune on some examples of\nquestions, trivia questions. And then when you test it on\nnew trivia questions, it would--",
    "start": "3691820",
    "end": "3700045"
  },
  {
    "text": "the model would\nimplicitly extract from its pretraining\ndata somehow",
    "start": "3700045",
    "end": "3705460"
  },
  {
    "text": "the answer to that new question\nthat it never saw explicitly at fine-tuning time. So it learned this sort of\nimplicit retrieval sometimes,",
    "start": "3705460",
    "end": "3713559"
  },
  {
    "text": "less than 50% of the\ntime or whatever, but much more than\nrandom chance, yeah.",
    "start": "3713560",
    "end": "3719710"
  },
  {
    "text": "And that's sort of fascinating. So you've learned to access\nthis sort of latent knowledge",
    "start": "3719710",
    "end": "3725140"
  },
  {
    "text": "that you've stored\nup by pretraining. And so, yeah, you\njust pass it the text \"When was Roosevelt born?\"",
    "start": "3725140",
    "end": "3730630"
  },
  {
    "text": "And it would pass out an answer. And one thing to know is\nthat the answers always look very fluent.",
    "start": "3730630",
    "end": "3735790"
  },
  {
    "text": "They always look\nvery reasonable, but they're frequently wrong. And that's still true\nof things like ChatGPT.",
    "start": "3735790",
    "end": "3741535"
  },
  {
    "text": " Yeah, OK. So that's like\nencoder-decoder models.",
    "start": "3741535",
    "end": "3749920"
  },
  {
    "text": "Next up, we've got decoders. And we'll spend a\nlong time on decoders. So this is just our\nnormal language model.",
    "start": "3749920",
    "end": "3755870"
  },
  {
    "text": "So we get a sequence of hidden\nstates from our decoder. The models-- the words can\nonly look at themselves",
    "start": "3755870",
    "end": "3761900"
  },
  {
    "text": "and not the future. And then, I predict the\nnext word in the sentence. And then, here again, I can\ndo sentiment, analysis maybe",
    "start": "3761900",
    "end": "3770089"
  },
  {
    "text": "take the last\nstate, the last word and then predict\n\"happy\" or \"sad\" based on that last embedding,\nback-propagate the gradients",
    "start": "3770090",
    "end": "3777348"
  },
  {
    "text": "through the whole network,\ntrain the whole thing, or do some kind of\nlightweight or parameter",
    "start": "3777348",
    "end": "3782569"
  },
  {
    "text": "efficient fine-tuning\nlike we mentioned earlier. So this is our\npretraining a decoder.",
    "start": "3782570",
    "end": "3787760"
  },
  {
    "text": "And I can just pre-train\nit on language modeling.",
    "start": "3787760",
    "end": "3793220"
  },
  {
    "text": "So again, you might\nwant to do this if you are wanting to generate\ntext, generate things.",
    "start": "3793220",
    "end": "3802359"
  },
  {
    "text": "You can use this like you\nuse an encoder-decoder. But in practice,\nas we'll see, a lot",
    "start": "3802360",
    "end": "3807400"
  },
  {
    "text": "of the sort of biggest, most\npowerful pretrained models tend to be decoder only.",
    "start": "3807400",
    "end": "3813550"
  },
  {
    "text": "It's not really\nclear exactly why, except they seem a little bit\nsimpler than encoder-decoders.",
    "start": "3813550",
    "end": "3820810"
  },
  {
    "text": "And you get to share all the\nparameters in one big network for the decoder, whereas\nwith an encoder-decoder,",
    "start": "3820810",
    "end": "3826390"
  },
  {
    "text": "you have to split them, sort\nof some into the encoder, some into the decoder. So for the rest of this lecture,\nwe'll talk only about decoders.",
    "start": "3826390",
    "end": "3835420"
  },
  {
    "text": "And in modern things\nthe biggest networks do tend to be decoders.",
    "start": "3835420",
    "end": "3840430"
  },
  {
    "text": "So we're coming all the\nway back again to 2018, and the GPT model from\nOpenAI was a big success.",
    "start": "3840430",
    "end": "3849140"
  },
  {
    "text": "It had 117 million parameters. It had 768-dimensional\nhidden states,",
    "start": "3849140",
    "end": "3856390"
  },
  {
    "text": "and it had this vocabulary that\nwas 40,000-ish words that was",
    "start": "3856390",
    "end": "3863359"
  },
  {
    "text": "defined via a method like what\nwe showed at the beginning of class trained on BooksCorpus.",
    "start": "3863360",
    "end": "3868430"
  },
  {
    "text": "And actually, GPT never\nactually showed up in the original paper. ",
    "start": "3868430",
    "end": "3875780"
  },
  {
    "text": "It's unclear what exactly\nit's supposed to refer to. But this model was a\nprecursor to all the things",
    "start": "3875780",
    "end": "3883580"
  },
  {
    "text": "that you're hearing\nabout nowadays. If you move forward--",
    "start": "3883580",
    "end": "3888810"
  },
  {
    "text": "oh, yeah, so if you-- it hm, let's see here.",
    "start": "3888810",
    "end": "3895490"
  },
  {
    "text": "So if we wanted to do\nsomething like natural language inference, right, which says\ntake these pairs of sentences,",
    "start": "3895490",
    "end": "3902180"
  },
  {
    "text": "\"The man is in the doorway,\"\n\"The person is near the door,\" and say that these\nmean-- that one entails",
    "start": "3902180",
    "end": "3908300"
  },
  {
    "text": "the other, the sort of premise\nentails the hypothesis that I can believe the hypothesis\nif I believe the premise,",
    "start": "3908300",
    "end": "3914030"
  },
  {
    "text": "I just sort of\nconcatenate them together. So give it maybe a start\ntoken, pass in one sentence,",
    "start": "3914030",
    "end": "3921109"
  },
  {
    "text": "pass in some delimiter\ntoken, pass in the other. And then, predict, yes/no\nentailment-- not entailment.",
    "start": "3921110",
    "end": "3928820"
  },
  {
    "text": "Fine-tuning GPT on this. It worked really well. And then BERT came after GPT.",
    "start": "3928820",
    "end": "3935480"
  },
  {
    "text": "BERT did a bit better. It had a bidirectional context. But it did sort of\nan excellent job.",
    "start": "3935480",
    "end": "3943880"
  },
  {
    "text": "And then came GPT 2,\nwhere they focused more on the generative\nabilities of the network, right?",
    "start": "3943880",
    "end": "3949559"
  },
  {
    "text": "So we looked at now a\nmuch larger network.",
    "start": "3949560",
    "end": "3954590"
  },
  {
    "text": "We've gone from 117\nmillion to 1.5 billion. And given some\nsort of prompt, it",
    "start": "3954590",
    "end": "3959780"
  },
  {
    "text": "could generate, at the\ntime, a quite surprisingly coherent continuation\nto the prompt. So it's telling this sort\nof story about scientists",
    "start": "3959780",
    "end": "3969050"
  },
  {
    "text": "and unicorns here. And this size of model\nis still small enough",
    "start": "3969050",
    "end": "3975230"
  },
  {
    "text": "that you can use on it a\nsmall GPU and fine-tune it and whatever. And its capabilities of\ngenerating long coherent texts",
    "start": "3975230",
    "end": "3983000"
  },
  {
    "text": "was just sort of\nexceptional at the time.",
    "start": "3983000",
    "end": "3988070"
  },
  {
    "text": "And it was also trained on\nmore data, although I don't-- something like 9\nbillion words of text.",
    "start": "3988070",
    "end": "3995270"
  },
  {
    "text": "And then so, after\nGPT-2, we come to GPT-3. We're walking\nthrough these models.",
    "start": "3995270",
    "end": "4001992"
  },
  {
    "text": "And then, we come with\nup a different way of interacting with the models. So we've interacted with\npretrained models in two ways",
    "start": "4001992",
    "end": "4008180"
  },
  {
    "text": "so far. We've sampled from the\ndistribution that they define. We generated text via a machine\ntranslation system or whatever,",
    "start": "4008180",
    "end": "4017230"
  },
  {
    "text": "or you fine-tune them on\na task that we care about, and then we take\ntheir predictions.",
    "start": "4017230",
    "end": "4023440"
  },
  {
    "text": "But GPT-3 seems to have an\ninteresting new ability.",
    "start": "4023440",
    "end": "4030040"
  },
  {
    "text": "It's much larger, and\nit can do some tasks without any sort of\nfine-tuning whatsoever.",
    "start": "4030040",
    "end": "4037780"
  },
  {
    "text": "GPT-3 is much larger than GPT-2. So we went from GPT\n100-ish million parameters.",
    "start": "4037780",
    "end": "4043300"
  },
  {
    "text": "GPT-2, 1.5 billion. GPT-3, 175 billion,\nmuch larger, trained",
    "start": "4043300",
    "end": "4049250"
  },
  {
    "text": "on 300 billion words of text. And this notion of\nin-context learning",
    "start": "4049250",
    "end": "4054560"
  },
  {
    "text": "that it could define or figure\nout patterns in the training or in the example that\nit's currently seeing",
    "start": "4054560",
    "end": "4060080"
  },
  {
    "text": "and continue the pattern is\ncalled in-context learning. So you've got the word \"thanks.\"",
    "start": "4060080",
    "end": "4066410"
  },
  {
    "text": "And I pass in this\nlittle arrow and say, OK, \"thanks\" goes to \"marci,\" and\nthen \"hello\" goes to \"bonjour.\"",
    "start": "4066410",
    "end": "4071630"
  },
  {
    "text": "And then, give they it\nall of these examples and ask it what\n\"otter\" should go to.",
    "start": "4071630",
    "end": "4077210"
  },
  {
    "text": "And it's learned to\ncontinue the pattern and say that this is the\ntranslation of \"otter.\"",
    "start": "4077210",
    "end": "4084589"
  },
  {
    "text": "So now, remember,\nthis is a single input that I've given to my model.",
    "start": "4084590",
    "end": "4089690"
  },
  {
    "text": "And I haven't said, oh, do\ntranslation or fine-tune it on translation or whatever. I've just passed in the\ninput, given it some examples.",
    "start": "4089690",
    "end": "4096830"
  },
  {
    "text": "And then it is able,\nto some extent, do this seemingly complex task.",
    "start": "4096830",
    "end": "4102318"
  },
  {
    "text": "That's in-context learning. And here are more examples. Maybe you give it\nexamples of addition,",
    "start": "4102319",
    "end": "4109630"
  },
  {
    "text": "and then it can do some\nsimple addition afterward. You give it-- in this case,\nthis is sort of rewriting typos.",
    "start": "4109630",
    "end": "4116830"
  },
  {
    "text": "It can figure out how to rewrite\ntypos in context learning for machine translation. And this was the\nstart of this idea",
    "start": "4116830",
    "end": "4123520"
  },
  {
    "text": "that there were these emergent\nproperties that showed up in much larger models. And it wasn't clear when\nlooking at the smaller models",
    "start": "4123520",
    "end": "4130930"
  },
  {
    "text": "that you'd get this new-- this qualitatively new\nbehavior out of them.",
    "start": "4130930",
    "end": "4137930"
  },
  {
    "text": "It's not obvious from just\nthe language modeling signal. GPT-3 is just trained\non that decoder only,",
    "start": "4137930",
    "end": "4144170"
  },
  {
    "text": "just next-- predict\nthe next word, that it would, as a\nresult of that training,",
    "start": "4144170",
    "end": "4149509"
  },
  {
    "text": "learn to perform seemingly\nquite complex things as a function of its context. ",
    "start": "4149510",
    "end": "4156350"
  },
  {
    "text": "Yeah, OK. One or two questions about that. ",
    "start": "4156350",
    "end": "4166950"
  },
  {
    "text": "This should be quite\nsurprising, I think. So far, we said-- we've talked\nabout good representations, contextual representations,\nmeanings of words, and context.",
    "start": "4166950",
    "end": "4174870"
  },
  {
    "text": "This is some very, very\nhigh-level pattern matching. It's coming up with patterns\nin just the input data",
    "start": "4174870",
    "end": "4180479"
  },
  {
    "text": "and that one sequence of text\nthat you've passed it so far and it's able, to identify\nhow to complete the pattern.",
    "start": "4180479",
    "end": "4187859"
  },
  {
    "text": "And you should think, what\nkinds of things can this solve? What are its capabilities? What are its limitations?",
    "start": "4187859",
    "end": "4194130"
  },
  {
    "text": "This ends up being an\nopen area of research. What are the kinds of\nproblems that you maybe saw in the training data log?",
    "start": "4194130",
    "end": "4199830"
  },
  {
    "text": "Maybe GPT-3 saw a ton of\npairs of words, right? It saw a bunch of dictionaries,\nbilingual dictionaries,",
    "start": "4199830",
    "end": "4206945"
  },
  {
    "text": "in its training data. So it learned to do\nsomething like this. Or is it doing something\nmuch more general, where it's really learning\nthe task in context?",
    "start": "4206945",
    "end": "4214230"
  },
  {
    "text": "The actual story,\nwe're not totally sure. It's something in the middle. It seems like it has to be\ntied to your training data",
    "start": "4214230",
    "end": "4222150"
  },
  {
    "text": "in ways that we don't\nquite understand. But there's also a\nnon-trivial ability to learn new, sort of at\nleast, types of patterns",
    "start": "4222150",
    "end": "4230199"
  },
  {
    "text": "just from the context. So this is a very\ninteresting thing to work on. Now, we've talked a lot about\nthe size of these models",
    "start": "4230200",
    "end": "4236989"
  },
  {
    "text": "so far. And as models have\ngotten larger, they've always gotten better. We trained them on\nmore data, right?",
    "start": "4236990",
    "end": "4243280"
  },
  {
    "text": "So GPT-3 was trained on\n300 billion words of text, and it was 175\nbillion parameters.",
    "start": "4243280",
    "end": "4250660"
  },
  {
    "text": "And at that scale, it\ncosts a lot of money to build these things.",
    "start": "4250660",
    "end": "4256270"
  },
  {
    "text": "And it's very unclear\nwhether you're getting the best use\nout of your money, is bigger really what you\nshould have been doing in terms",
    "start": "4256270",
    "end": "4261645"
  },
  {
    "text": "of the number of parameters? So the cost of\ntraining one of these is roughly you take the\nnumber of parameters,",
    "start": "4261645",
    "end": "4268270"
  },
  {
    "text": "you multiply it by\nthe number of tokens that you're going to train\nit on, the number of words. And some folks at\nDeepMind, I forgot",
    "start": "4268270",
    "end": "4275599"
  },
  {
    "text": "the citation on this--\nsome folks at DeepMind realized through\nsome experimentation",
    "start": "4275600",
    "end": "4280790"
  },
  {
    "text": "that actually GPT-3 was\njust comically oversized. So Chinchilla, the\nmodel they trained,",
    "start": "4280790",
    "end": "4287480"
  },
  {
    "text": "is less than half the\nsize and works better, but they just trained\nit on way more data.",
    "start": "4287480",
    "end": "4294420"
  },
  {
    "text": "And this is an interesting\nof trade-off about how do you best spend your compute. You can't do this more\nthan a handful of times,",
    "start": "4294420",
    "end": "4301140"
  },
  {
    "text": "even if you're Google. So open questions there as well.",
    "start": "4301140",
    "end": "4308160"
  },
  {
    "text": "Another sort of\nway of interacting with these networks that\nhas come out recently is called Chain of Thought.",
    "start": "4308160",
    "end": "4315950"
  },
  {
    "text": "So the prefix, right, we saw in\nthe in-context learning slide that the prefix can help sort\nof specify what task you're",
    "start": "4315950",
    "end": "4322570"
  },
  {
    "text": "trying to solve right now. And it can do even more. So here's standard\nsort of prompting.",
    "start": "4322570",
    "end": "4327610"
  },
  {
    "text": "We have a prefix of examples\nof questions and answers. So you have a question and\nthen an example answer.",
    "start": "4327610",
    "end": "4334700"
  },
  {
    "text": "So that's your prompt\nthat's specifying the task. And then you have\na new question, and you're having the\nmodel generate an answer,",
    "start": "4334700",
    "end": "4340668"
  },
  {
    "text": "and it generates it wrong. And Chain-of-Thought\nPrompting says, well, how",
    "start": "4340668",
    "end": "4346690"
  },
  {
    "text": "about in the example? In the demonstration we\ngive, we give the question, and then we give this\nsort of decomposition",
    "start": "4346690",
    "end": "4353110"
  },
  {
    "text": "of steps towards how to\nget an answer, right? So I'm actually writing this\nout as part of the input.",
    "start": "4353110",
    "end": "4358300"
  },
  {
    "text": "I'm giving\nannotations as a human to say, oh, to solve this\nsort of word problem,",
    "start": "4358300",
    "end": "4364120"
  },
  {
    "text": "here's how you could\nthink it through-ish. And then I give\nit a new question.",
    "start": "4364120",
    "end": "4369367"
  },
  {
    "text": "And the model says, oh, I\nknow what I'm supposed to do. I'm supposed to first\ngenerate a sequence of steps,",
    "start": "4369368",
    "end": "4375790"
  },
  {
    "text": "of intermediate steps, and\nthen next say the answer is-- and then say what the answer is.",
    "start": "4375790",
    "end": "4381650"
  },
  {
    "text": "And it turns out,\nand this should again be very surprising,\nthat the model can",
    "start": "4381650",
    "end": "4387190"
  },
  {
    "text": "tend to generate plausible\nsequences of steps and then much more frequently\ngenerates the correct answer",
    "start": "4387190",
    "end": "4393580"
  },
  {
    "text": "after doing so relative\nto trying to generate the answer by itself. So you can think of\nthis as a scratchpad.",
    "start": "4393580",
    "end": "4400329"
  },
  {
    "text": "You can think of\nthis as increasing the amount of computation that\nyou're putting into trying to solve the problem.",
    "start": "4400330",
    "end": "4406910"
  },
  {
    "text": "You writing out your thoughts. As I generate each word\nof this continuation here,",
    "start": "4406910",
    "end": "4412780"
  },
  {
    "text": "I'm able to condition on\nall the past words so far. And so maybe it\njust, yeah, allows",
    "start": "4412780",
    "end": "4419800"
  },
  {
    "text": "the network to decompose the\nproblem into smaller, simpler problems, which is more\nable to solve each.",
    "start": "4419800",
    "end": "4427570"
  },
  {
    "text": "No one's really sure why\nthis works exactly, either. At this point, with networks\nthat are this large,",
    "start": "4427570",
    "end": "4433960"
  },
  {
    "text": "their emergent properties\nare both very powerful and exceptionally\nhard to understand",
    "start": "4433960",
    "end": "4439420"
  },
  {
    "text": "and very hard you should think\nto trust because it's unclear what its capabilities are,\nand what its limitations are,",
    "start": "4439420",
    "end": "4446680"
  },
  {
    "text": "where it will fail. So why do we think\npretraining is teaching? Gosh, a wide range\nof things, even",
    "start": "4446680",
    "end": "4454470"
  },
  {
    "text": "beyond what I've written in\nthis slide, which I mostly wrote two years ago.",
    "start": "4454470",
    "end": "4459540"
  },
  {
    "text": "So it can teach you\ntrivia, and syntax, and co-reference, and maybe\nsome lexical semantics, and sentiment and\nsome reasoning,",
    "start": "4459540",
    "end": "4466080"
  },
  {
    "text": "way more reasoning\nthan we would have thought even three years ago. And yet they also learn and\nexacerbate racism and sexism,",
    "start": "4466080",
    "end": "4474750"
  },
  {
    "text": "all manner of biases. There will be more\non this later.",
    "start": "4474750",
    "end": "4480720"
  },
  {
    "text": "The generality of this\nis really, I think, what's taken many people aback. And so, increasingly,\nthese objects",
    "start": "4480720",
    "end": "4487380"
  },
  {
    "text": "are not just studied for\nthe sake of using them but studied for the sake\nof understanding anything",
    "start": "4487380",
    "end": "4493680"
  },
  {
    "text": "about how they work\nand how they fail. Yeah. Any questions?",
    "start": "4493680",
    "end": "4499199"
  },
  {
    "start": "4499200",
    "end": "4505330"
  },
  {
    "text": "Has anyone tried benchmarking\nGPT for programming tasks",
    "start": "4505330",
    "end": "4511240"
  },
  {
    "text": "like how accurately\nit does, et cetera? Yeah, the question\nis, has anyone tried benchmarking GPT\nfor programming tasks?",
    "start": "4511240",
    "end": "4518590"
  },
  {
    "text": "Has anyone seen\nhow well it does? Yes, so there's definitely\nexamples of people using GPT-3",
    "start": "4518590",
    "end": "4525489"
  },
  {
    "text": "for simple programming\nthings and then the modern state-of-the-art competitive\nprogramming bots are all based",
    "start": "4525490",
    "end": "4534190"
  },
  {
    "text": "on ideas from language modeling. And I think they're all also\nbased on pretrained language",
    "start": "4534190",
    "end": "4540130"
  },
  {
    "text": "models themselves. If you just take all of these\nideas and apply it to GitHub,",
    "start": "4540130",
    "end": "4546310"
  },
  {
    "text": "then you get some very\ninteresting emergent behaviors relating to code fallout. And so yeah, I think\nall of the best systems",
    "start": "4546310",
    "end": "4553900"
  },
  {
    "text": "use this more or less. So lots of benchmarking\nthere for sure. Is this the basis for what\nGitHub Copilot is trying to do?",
    "start": "4553900",
    "end": "4562582"
  },
  {
    "text": "The question is,\nis this the basis? Is that what we just mentioned,\nthe basis for the GitHub Copilot system? Yes, absolutely.",
    "start": "4562582",
    "end": "4570219"
  },
  {
    "text": "We don't know exactly what\nit is in terms of details, but it's all these ideas.",
    "start": "4570220",
    "end": "4575903"
  },
  {
    "text": "What if you have\na situation where you have still a large amount\nof data for general data,",
    "start": "4575903",
    "end": "4580920"
  },
  {
    "text": "and then you have also\na large amount of data for your fine-tuning task? At what point is it better\nto train a new model",
    "start": "4580920",
    "end": "4587489"
  },
  {
    "text": "for that fine-tuning\nversus get data from both? Yeah, the question\nis, what if you have a large amount of\ndata for pretraining",
    "start": "4587490",
    "end": "4593670"
  },
  {
    "text": "and a large amount of\ndata for fine-tuning? When is it better to\ndo a separate training",
    "start": "4593670",
    "end": "4598860"
  },
  {
    "text": "on just the fine-tuning data? Almost never. If you have a bunch\nof data for the task",
    "start": "4598860",
    "end": "4607110"
  },
  {
    "text": "that you care about, what's\nfrequently done instead is three-part training\nwhere you pre-train",
    "start": "4607110",
    "end": "4612540"
  },
  {
    "text": "on a very broad corpus. Then, you sort of continue\nto pre-train using something like language modeling\non an unlabeled",
    "start": "4612540",
    "end": "4620280"
  },
  {
    "text": "version of the labeled\ndata that you have. You just strip the labels off\nand just treat it all as text,",
    "start": "4620280",
    "end": "4625560"
  },
  {
    "text": "and do language\nmodeling on that, adapt the parameters\na little bit, and then do the final stage\nof fine-tuning with the labels",
    "start": "4625560",
    "end": "4632277"
  },
  {
    "text": "that you want. And that works even better. There's an interesting paper\ncalled Don't Stop Pretraining.",
    "start": "4632277",
    "end": "4639340"
  },
  {
    "text": "Nice. Final question. Just one question, sorry. Anyone new?",
    "start": "4639340",
    "end": "4645250"
  },
  {
    "text": "Someone new with a question? Yes. ",
    "start": "4645250",
    "end": "4650869"
  },
  {
    "text": "Yeah, I was\nwondering, do you know if there's a lot of instances\nwhere a pretrained model can",
    "start": "4650870",
    "end": "4656780"
  },
  {
    "text": "do some task that it has\nnot seen before, even without fine-tuning? Yeah, so are there any instances\nof where a pretrained model can",
    "start": "4656780",
    "end": "4663500"
  },
  {
    "text": "do a task that it hasn't seen\nbefore without fine-tuning? The question is, what does\n\"it hasn't seen before\" mean,",
    "start": "4663500",
    "end": "4669072"
  },
  {
    "text": "right? These models, especially GPT-3\nand similar very large models,",
    "start": "4669072",
    "end": "4675200"
  },
  {
    "text": "during pretraining did it\never see something exactly like this sort of word\nproblem arithmetic?",
    "start": "4675200",
    "end": "4681590"
  },
  {
    "text": "Maybe. Maybe not, it's actually\nsort of unclear. It's clearly able\nto recombine sort of bits and pieces of tasks\nthat it saw implicitly",
    "start": "4681590",
    "end": "4690140"
  },
  {
    "text": "during pretraining. We saw the same thing\nwith trivia, right? Language modeling looks a\nlot like trivia sometimes,",
    "start": "4690140",
    "end": "4695450"
  },
  {
    "text": "where you just read the first\nparagraph of a Wikipedia page, and it's kind of answering\na bunch of little trivia",
    "start": "4695450",
    "end": "4701330"
  },
  {
    "text": "questions about where\nsomeone was born and when. But it's never seen\nsomething quite like this.",
    "start": "4701330",
    "end": "4706400"
  },
  {
    "text": "And it's actually still\nkind of astounding how much it's able\nto do things that don't seem like they should\nhave shown up all that directly",
    "start": "4706400",
    "end": "4712440"
  },
  {
    "text": "in the pretraining data. Quantifying that extent is\nan open research problem. OK, that's it.",
    "start": "4712440",
    "end": "4718010"
  },
  {
    "text": "Let's call it. ",
    "start": "4718010",
    "end": "4726000"
  }
]