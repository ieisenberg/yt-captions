[
  {
    "text": "All right, welcome back everyone. Hope you had a good weekend. So this is Lecture 16 of CS229.",
    "start": "4340",
    "end": "11910"
  },
  {
    "text": "And today we're gonna start a new chapter on unsupervised learning. All right?",
    "start": "11910",
    "end": "17220"
  },
  {
    "text": "Uh, so uh, unsupervised learning will be the broad topic for the rest of this week and parts of next week.",
    "start": "17220",
    "end": "25035"
  },
  {
    "text": "And the specific topics for today are the K-means algorithm, the mixture of Gaussians,",
    "start": "25035",
    "end": "31395"
  },
  {
    "text": "which is also called the GMM or Gaussian- Gaussian mixture models and, uh, the expectation-maximization, um, algorithm.",
    "start": "31395",
    "end": "38430"
  },
  {
    "text": "And, um, to- jumping into today's topics,",
    "start": "38430",
    "end": "43680"
  },
  {
    "text": "uh, so what you've seen so far is, uh, the first few weeks, first uh,",
    "start": "43680",
    "end": "48920"
  },
  {
    "text": "three four weeks or so we covered supervised learning where we were trying to learn a function that maps X to Y.",
    "start": "48920",
    "end": "55400"
  },
  {
    "text": "And we're given pairs of X, Y as our training set or examples. And then after supervised learning,",
    "start": "55400",
    "end": "62149"
  },
  {
    "text": "we went into some learning theory. We studied bias-variance, uh, trade off and the bias-variance analysis and saw a little bit into generalization.",
    "start": "62150",
    "end": "70565"
  },
  {
    "text": "And then we, um, last week we saw reinforcement learning, where the goal in reinforcement learning is,",
    "start": "70565",
    "end": "77495"
  },
  {
    "text": "rather than minimizing some kind of a loss function, we want to maximize the value by choosing a suitable policy, right?",
    "start": "77495",
    "end": "83735"
  },
  {
    "text": "And here value is the long-term cumulative sum of discounted rewards. And we want to maximize the long-term reward by choosing a policy.",
    "start": "83735",
    "end": "92180"
  },
  {
    "text": "[NOISE] Now, the new chapter we're going to start today, uh, starting today is unsupervised learning.",
    "start": "92180",
    "end": "97985"
  },
  {
    "text": "And in the unsupervised learning case, um, we are given a dataset generally, um,",
    "start": "97985",
    "end": "103610"
  },
  {
    "text": "a collection of X1,",
    "start": "103610",
    "end": "110360"
  },
  {
    "text": "X2, et cetera, Xn. Right? And we do not have a corresponding Y variable associated with each X variable.",
    "start": "110360",
    "end": "122450"
  },
  {
    "text": "You're just given a- a set of collect, um, um, examples, um, a set of X's where each Xi is in Rd,",
    "start": "122450",
    "end": "134005"
  },
  {
    "text": "in d-dimensional real space for example. [NOISE] And our goal is to learn some kind of a structure in these X's.",
    "start": "134005",
    "end": "141200"
  },
  {
    "text": "Right? We don't have a corresponding correct answer. We don't have what's otherwise called supervision of what the correct answer is for each X.",
    "start": "141200",
    "end": "149090"
  },
  {
    "text": "But in general, we're just given some- some collection of X's and our- our- the goal is to find some kind of an interesting structure in these",
    "start": "149090",
    "end": "155840"
  },
  {
    "text": "X's that hopefully gives us some new insight. Right? So, [NOISE] um,",
    "start": "155840",
    "end": "161055"
  },
  {
    "text": "we've seen logistic regression before. And in the case of Logix- logistic regression norm, um,",
    "start": "161055",
    "end": "168430"
  },
  {
    "text": "X1, Xd, let me use a few colors here.",
    "start": "168500",
    "end": "175570"
  },
  {
    "text": "We had some- [NOISE]",
    "start": "177620",
    "end": "192750"
  },
  {
    "text": "You're given a dataset like this and our goal was to find a separating hyper plane.",
    "start": "192750",
    "end": "198500"
  },
  {
    "text": "Right? That was- this is- is supervised learning because we're given the correct answer.",
    "start": "198500",
    "end": "204520"
  },
  {
    "text": "That is the color of each point along with the point itself. All right? Whereas in the unsupervised learning,",
    "start": "204520",
    "end": "210265"
  },
  {
    "text": "assume the- the- the problem would translate to something like this. Right? We are given some points, just the X's.",
    "start": "210265",
    "end": "222790"
  },
  {
    "text": "And our goal now is to learn some kind of an interesting structure here. Right? Previously we had- we had the correct answer for each, uh, for each input.",
    "start": "229550",
    "end": "238275"
  },
  {
    "text": "Now we're just given a collection of X's and a reasonable, uh, structure to find in this are these two clusters, right?",
    "start": "238275",
    "end": "246480"
  },
  {
    "text": "So loosely speaking, uh, we want to look for- for such structures with- when we are given just X's.",
    "start": "247340",
    "end": "256880"
  },
  {
    "text": "However, this problem is generally not very well-defined. Right? In the first case,",
    "start": "256880",
    "end": "262640"
  },
  {
    "text": "for each point we- we were told what the correct answer is but consider a problem like this.",
    "start": "262640",
    "end": "268980"
  },
  {
    "text": "Right? And if- if you are asked to find an interesting stuct- structure in this kind of a data,",
    "start": "277910",
    "end": "284199"
  },
  {
    "text": "it would be totally reasonable to say this is one cluster, this is one cluster,",
    "start": "284200",
    "end": "289960"
  },
  {
    "text": "and this is another cluster. Right? And another totally reasonable thing would be to say, this is one cluster and this is one cluster.",
    "start": "289960",
    "end": "297275"
  },
  {
    "text": "Right? So in a way, there is no correct answer, so to speak. And our goal is to learn some kind of an interesting,",
    "start": "297275",
    "end": "303580"
  },
  {
    "text": "um, uh, structure in the presence of such kind of ambiguities. Right? The, um, the way to- you- you want to think of this is",
    "start": "303580",
    "end": "313415"
  },
  {
    "text": "classification problems in the supervised setting are somewhat related to clustering problems in the unsupervised setting, right?",
    "start": "313415",
    "end": "322235"
  },
  {
    "text": "When the cluster identity is like the class label. And we want to,",
    "start": "322235",
    "end": "327560"
  },
  {
    "text": "looking at just the X's, find out both how many number of classes there are and also to which class each example belongs to.",
    "start": "327560",
    "end": "335920"
  },
  {
    "text": "Right? And, um, why would this be interesting? So for example, this would be interesting,",
    "start": "335920",
    "end": "343009"
  },
  {
    "text": "for example, for, uh, in examples where supposing you have, um, uh, you're working at a marketing department and you have information about your customers.",
    "start": "343010",
    "end": "351995"
  },
  {
    "text": "Right? And the information about your customers can be represented in- in, you know, in some kind of a vector space where, you know,",
    "start": "351995",
    "end": "359135"
  },
  {
    "text": "you have the age of the customer here, you have, you know, their, um, uh,",
    "start": "359135",
    "end": "364550"
  },
  {
    "text": "annual income and on another axis you might have their,",
    "start": "364550",
    "end": "370909"
  },
  {
    "text": "you know, geographic location.",
    "start": "370910",
    "end": "373680"
  },
  {
    "text": "Okay? And each customer would be a point in the space.",
    "start": "377530",
    "end": "382050"
  },
  {
    "text": "Right? Now as a- as a- as a- as a person who is working in marketing,",
    "start": "383060",
    "end": "388880"
  },
  {
    "text": "you might be interested to perform some kind of a market segmentation to identify,",
    "start": "388880",
    "end": "394505"
  },
  {
    "text": "you know, groups of customers so that you can do some kind of a targeted, you know, advertising or marketing campaign or some such thing.",
    "start": "394505",
    "end": "401330"
  },
  {
    "text": "Right? So that's just one example of how- why unsupervised learning might be, you know, interesting.",
    "start": "401330",
    "end": "407045"
  },
  {
    "text": "So the first unsupervised learning algorithm that we'll be seeing is something called as the,",
    "start": "407045",
    "end": "412639"
  },
  {
    "text": "um, the K-means clustering algorithm. [NOISE] The K-means clustering algorithm is- is, uh, pretty straightforward.",
    "start": "412640",
    "end": "423865"
  },
  {
    "text": "This is probably one of the simplest algorithms of unsupervised learning, K-means algorithm.",
    "start": "423865",
    "end": "436620"
  },
  {
    "text": "So we are given a data set of n examples, x_1 through x_n,",
    "start": "436620",
    "end": "447139"
  },
  {
    "text": "where each x_i is in R_d, right?",
    "start": "447140",
    "end": "453390"
  },
  {
    "text": "And our goal is to group them into k clusters.",
    "start": "453390",
    "end": "458829"
  },
  {
    "text": "right? And for the purpose of the algorithm, we will- we will assume that k is given to us.",
    "start": "459770",
    "end": "465930"
  },
  {
    "text": "And the algorithm goes like this, initialize cluster centroids,",
    "start": "465930",
    "end": "476319"
  },
  {
    "text": "mu_1, mu_2, mu_k.",
    "start": "480410",
    "end": "485985"
  },
  {
    "text": "So you have one, uh, centroid per cluster where each of them are",
    "start": "485985",
    "end": "492750"
  },
  {
    "text": "in R_d randomly, right?",
    "start": "492750",
    "end": "499320"
  },
  {
    "text": "So each of these mu_1, mu_2, mu_k, uh, is a vector. Previously in- in our notations generally, uh,",
    "start": "499320",
    "end": "506700"
  },
  {
    "text": "having a suffix to a variable generally meant it was a scalar, right?",
    "start": "506700",
    "end": "512099"
  },
  {
    "text": "But in this case, mu_1 is a full vector, right?",
    "start": "512100",
    "end": "517935"
  },
  {
    "text": "And there are k such full vectors, uh, mu_1 from mu_k and we initialized them",
    "start": "517935",
    "end": "522990"
  },
  {
    "text": "randomly and then we repeat this until convergence. Repeat until",
    "start": "522990",
    "end": "531399"
  },
  {
    "text": "convergence for every i,",
    "start": "532280",
    "end": "539565"
  },
  {
    "text": "where i denotes the, uh, example number set C_i equal",
    "start": "539565",
    "end": "550800"
  },
  {
    "text": "to arg min j x_i",
    "start": "550800",
    "end": "560595"
  },
  {
    "text": "minus mu_ j squared. And then that's step one and step two,",
    "start": "560595",
    "end": "569825"
  },
  {
    "text": "for every j, where j is now,",
    "start": "569825",
    "end": "575640"
  },
  {
    "text": "uh- where j indicates the cluster right entity set mu_j is equal to",
    "start": "575640",
    "end": "586840"
  },
  {
    "text": "sum over i equals 1 to n, indicator of C_i equals j x_i,",
    "start": "587720",
    "end": "602649"
  },
  {
    "text": "and sum over i equals 1 to n indicator C_i equals j.",
    "start": "602870",
    "end": "614380"
  },
  {
    "text": "So what are we doing here? K-means is an iterative algorithm where we are",
    "start": "615530",
    "end": "620880"
  },
  {
    "text": "given a set of n examples which we index by i. And we want to identify k-clusters,",
    "start": "620880",
    "end": "628920"
  },
  {
    "text": "where the k-clusters are indexed by j. We initialize the cluster centroids randomly,",
    "start": "628920",
    "end": "634964"
  },
  {
    "text": "where mu_1 through mu_k are- are each a vector, uh, in R_d and we repeat until convergence where for every- for every i,",
    "start": "634964",
    "end": "646110"
  },
  {
    "text": "we set C_i here, C, uh, C- you can think of C as an array of length n,",
    "start": "646110",
    "end": "652829"
  },
  {
    "text": "where for each example that is a corresponding C_i, for every x_i there's a corresponding C_i and we set C_i to be the identity,",
    "start": "652830",
    "end": "660660"
  },
  {
    "text": "that's the arg min of j, identity of the nearest mean, right? And based on the set C_i vector or C_i array,",
    "start": "660660",
    "end": "670740"
  },
  {
    "text": "we then recalculate mu_j, where mu_j is calculated as the mean of",
    "start": "670740",
    "end": "678180"
  },
  {
    "text": "all xi's for which C_i equals j. Yes, question?",
    "start": "678180",
    "end": "683580"
  },
  {
    "text": "[inaudible]. So, uh, as I said already for now,",
    "start": "683580",
    "end": "688695"
  },
  {
    "text": "let's assume k is told to us. You know, we are given what k is and this is the algorithm, right?",
    "start": "688695",
    "end": "694665"
  },
  {
    "text": "And the way, uh, it's- it's a pretty straightforward algorithm where we, uh, where we alternate between one step,",
    "start": "694665",
    "end": "703214"
  },
  {
    "text": "where we are- where we are calculating the cluster identities for each example and in the other step where we are recalculating the cluster centroids.",
    "start": "703215",
    "end": "713430"
  },
  {
    "text": "And this is probably seen through a simple, uh, visualization, which- let me have a quick look at.",
    "start": "713430",
    "end": "721550"
  },
  {
    "text": "[NOISE]",
    "start": "721550",
    "end": "731248"
  },
  {
    "text": "Any questions on this so far? Yes, question. [inaudible].",
    "start": "731249",
    "end": "748470"
  },
  {
    "text": "So- so the question is, uh, what happens, uh, can we use, uh,",
    "start": "748470",
    "end": "754170"
  },
  {
    "text": "an unsupervised learning setting to learn the different cluster centers and use that as a classification algorithm.",
    "start": "754170",
    "end": "759450"
  },
  {
    "text": "[inaudible].",
    "start": "759450",
    "end": "764640"
  },
  {
    "text": "Yeah, it might or it might not have the same as a supervised learning algorithm.",
    "start": "764640",
    "end": "769030"
  },
  {
    "text": "Yeah, so supposing this is, you know, uh, think of this as a collection of points that are given to us where each green point is,",
    "start": "770030",
    "end": "778529"
  },
  {
    "text": "uh, you know, is- is, uh, a data point in, uh, x_i in R_d and the way,",
    "start": "778530",
    "end": "784680"
  },
  {
    "text": "uh, the algorithm, uh, works like this. We- here we assume k equals 2 and the red X and the blue X,",
    "start": "784680",
    "end": "792720"
  },
  {
    "text": "you can think of them as mu_1 and mu _2, which are randomly initialized.",
    "start": "792720",
    "end": "798570"
  },
  {
    "text": "And in the first step, what we do is for each point- for each point X,",
    "start": "798570",
    "end": "805545"
  },
  {
    "text": "we identify the nearest cluster, right? We- we- we set C_i to be the identity of that cluster,",
    "start": "805545",
    "end": "816795"
  },
  {
    "text": "which has the smallest L2 distance between that point and the cluster centroid. So over here, the red dots are those for which the red X is closer.",
    "start": "816795",
    "end": "826395"
  },
  {
    "text": "And the blue dots are those, uh, X's for which the blue X is closer, right?",
    "start": "826395",
    "end": "831630"
  },
  {
    "text": "So this is like setting the- the C_i's in the first iteration. And once we set the C_i's in the next step,",
    "start": "831630",
    "end": "838290"
  },
  {
    "text": "what we do is recalculate the mu, uh, mu_j's. So what happened here? I'm going to go back one slide.",
    "start": "838290",
    "end": "845400"
  },
  {
    "text": "So, uh, at this stage we set all the cluster identities. And in the next stage,",
    "start": "845400",
    "end": "850935"
  },
  {
    "text": "what we want to do is recalculate the red X, that is, uh, the mu to be at the middle of the red points,",
    "start": "850935",
    "end": "859680"
  },
  {
    "text": "at the red circles, and similarly recalculate the- the blue mu to",
    "start": "859680",
    "end": "865620"
  },
  {
    "text": "be the center of all the blue circles and that's what happens in the next slide over here, right?",
    "start": "865620",
    "end": "871260"
  },
  {
    "text": "So this is- this is just taking the mean of all the points of, uh, the corresponding color. And then what we do, we repeat and assign new cluster identities,",
    "start": "871260",
    "end": "881810"
  },
  {
    "text": "re-evaluate all the points to see which new cluster centroid they're closer to.",
    "start": "881810",
    "end": "887700"
  },
  {
    "text": "So what happened? Uh, let me go back one slide just to see the difference. So these two points which previously belong to the-",
    "start": "887720",
    "end": "895280"
  },
  {
    "text": "the old blue centroid now got mapped to the new,",
    "start": "895280",
    "end": "901920"
  },
  {
    "text": "uh, to the red one. And then we re-evaluate the centroids again. And the centroids will now move to the center, right?",
    "start": "901920",
    "end": "909360"
  },
  {
    "text": "And once we reach here in the next iteration, I actually moved a slide, you know, nothing changes and we declare that the algorithm has converged, right?",
    "start": "909360",
    "end": "919410"
  },
  {
    "text": "It's a-it's a pretty- pretty simple and straightforward algorithm. And now the- the, uh, a few natural questions to ask is,",
    "start": "919410",
    "end": "927450"
  },
  {
    "text": "you know, will this algorithm always converge? And will it always give us the same- same answers all the time?",
    "start": "927450",
    "end": "935440"
  },
  {
    "text": "So it can be shown that the algorithm does always converge.",
    "start": "936260",
    "end": "944600"
  },
  {
    "text": "And what we mean by convergence in this algorithm has a special meaning.",
    "start": "944600",
    "end": "950050"
  },
  {
    "text": "So if we consider this loss function called J of C,",
    "start": "950050",
    "end": "957745"
  },
  {
    "text": "mu to be equal to i equals 1 through n,",
    "start": "957745",
    "end": "967345"
  },
  {
    "text": "x_i minus mu C_i",
    "start": "967345",
    "end": "975120"
  },
  {
    "text": "squared and this is also called the distortion function.",
    "start": "975130",
    "end": "983030"
  },
  {
    "text": "And the K-means algorithm is basically an algorithm to",
    "start": "990770",
    "end": "996060"
  },
  {
    "text": "minimizing this particular loss of the distortion function in the form of coordinate descent.",
    "start": "996060",
    "end": "1001610"
  },
  {
    "text": "So what's coordinate descent? Coordinate descent, you can think of coordinate descent as a variant of gradient descent,",
    "start": "1001610",
    "end": "1006935"
  },
  {
    "text": "where- what we're doing is at each step, instead of minimizing the loss with respect to all the input variables,",
    "start": "1006935",
    "end": "1016565"
  },
  {
    "text": "we only minimize the loss with respect to a few variables by holding the others fixed, right?",
    "start": "1016565",
    "end": "1022280"
  },
  {
    "text": "So the- the, uh, step number one corresponds to",
    "start": "1022280",
    "end": "1028370"
  },
  {
    "text": "minimizing the distortion function by holding Mu fixed and optimizing C,",
    "start": "1028370",
    "end": "1033574"
  },
  {
    "text": "where- where we calculate new Cs. And step number two corresponds to then minimizing J",
    "start": "1033575",
    "end": "1041120"
  },
  {
    "text": "again by holding the Cs fixed and optimizing it with respect to Mu, right? So K-means",
    "start": "1041120",
    "end": "1048720"
  },
  {
    "text": "is coordinate descent",
    "start": "1049840",
    "end": "1059070"
  },
  {
    "text": "of the distortion function J,",
    "start": "1060340",
    "end": "1071434"
  },
  {
    "text": "where in- in one step we optimize it with respect to C and the other step we optimize it with respect to Mu,",
    "start": "1071435",
    "end": "1077255"
  },
  {
    "text": "and the result of the optimization results in these- in these, uh, closed form rules for recalculating the C's and mu's.",
    "start": "1077255",
    "end": "1086549"
  },
  {
    "text": "And we say that K-means algorithm converges in the sense",
    "start": "1086560",
    "end": "1092300"
  },
  {
    "text": "that eventually we are going to read some kind of a local minima of this J function.",
    "start": "1092300",
    "end": "1099210"
  },
  {
    "text": "It may so happen that we may have minimized J,",
    "start": "1099220",
    "end": "1105695"
  },
  {
    "text": "but we may end up toggling between two sets of mu- two sets of mus and Cs alternating,",
    "start": "1105695",
    "end": "1114215"
  },
  {
    "text": "uh, once we reach a local minima, though that happens extremely rarely in practice.",
    "start": "1114215",
    "end": "1119630"
  },
  {
    "text": "But we will eventually reach a state where J is no longer minimized further,",
    "start": "1119630",
    "end": "1125270"
  },
  {
    "text": "we're gonna flatten out in J and most of the times, pretty much all the time in practice, that's going to result in some kind of a mu and C that does not change anymore.",
    "start": "1125270",
    "end": "1134855"
  },
  {
    "text": "This J is non-convex, which means the- the Mus and the Cs that we end up in can change from run to run.",
    "start": "1134855",
    "end": "1144170"
  },
  {
    "text": "If you start with a different initialization, you may end up with a different set of Mus and Cs.",
    "start": "1144170",
    "end": "1149735"
  },
  {
    "text": "Which- which kind of, uh, ties back to a question that was asked before, you know, uh, why do we, you know,",
    "start": "1149735",
    "end": "1157235"
  },
  {
    "text": "ever need to use the label identities, uh, and- and not just perform- perform clustering,",
    "start": "1157235",
    "end": "1164540"
  },
  {
    "text": "and the answer is- is that this is basically a non-convex pro- problem, and we can end up with different,",
    "start": "1164540",
    "end": "1171095"
  },
  {
    "text": "uh, different cluster identities depending on the initialization.",
    "start": "1171095",
    "end": "1176280"
  },
  {
    "text": "Any questions on this is? Yes, question.",
    "start": "1176800",
    "end": "1180690"
  },
  {
    "text": "So the question is by looking at a function, how do we determine whether it is, uh, convex or not?",
    "start": "1185920",
    "end": "1191660"
  },
  {
    "text": "Uh, in general, it is, uh, the answer is not always straightforward.",
    "start": "1191660",
    "end": "1197465"
  },
  {
    "text": "So it is easy to show that something is convex by showing it as, you know, a composition of convex sub functions.",
    "start": "1197465",
    "end": "1205925"
  },
  {
    "text": "Uh, however, showing something is not convex, uh, is not always that straightforward.",
    "start": "1205925",
    "end": "1213455"
  },
  {
    "text": "Uh, something could, uh, which may not appear to be convex at first, can sometimes with some kind of reparameterization may end up being convex, etc.",
    "start": "1213455",
    "end": "1222035"
  },
  {
    "text": "Um, in this case, it happens to be non-convex.",
    "start": "1222035",
    "end": "1226590"
  },
  {
    "text": "Any questions on this? [NOISE] Cool.",
    "start": "1230710",
    "end": "1235880"
  },
  {
    "text": "Uh, so given this clustering pro- uh,",
    "start": "1235880",
    "end": "1242960"
  },
  {
    "text": "clustering approach that we've seen, let's move on to something that is slightly different and also somewhat related.",
    "start": "1242960",
    "end": "1251255"
  },
  {
    "text": "What is the problem of density estimation? [NOISE]",
    "start": "1251255",
    "end": "1262760"
  },
  {
    "text": "So density estimation generally refers to the problem where we are given some number of data points.",
    "start": "1262760",
    "end": "1270750"
  },
  {
    "text": "We're given some number of data points, and this is in R,",
    "start": "1285600",
    "end": "1292780"
  },
  {
    "text": "you can think of it as the x-axis. And these points are residing in a continuous space.",
    "start": "1292780",
    "end": "1298774"
  },
  {
    "text": "Uh, and the goal is to now, uh, we assume that these points are sampled from some kind of a probability distribution,",
    "start": "1298775",
    "end": "1308870"
  },
  {
    "text": "and because this- this, um, these points are- are- are coming on a- on a- on a continuous distribution,",
    "start": "1308870",
    "end": "1317315"
  },
  {
    "text": "the corresponding probability distribution has some kind of a density. It's not a probability mass function,",
    "start": "1317315",
    "end": "1322970"
  },
  {
    "text": "but it's a probability density function. Given these points, points that look like this,",
    "start": "1322970",
    "end": "1329299"
  },
  {
    "text": "the question is, you know, what is the density function from which these points were sampled?",
    "start": "1329300",
    "end": "1334414"
  },
  {
    "text": "In general, it's a very hard problem. Because, uh, if you wanna fit this data really, really well,",
    "start": "1334415",
    "end": "1341540"
  },
  {
    "text": "then the best possible fit would be a density that looks like this, where it has, you know, like direct delta function over every data point.",
    "start": "1341540",
    "end": "1352260"
  },
  {
    "text": "You know, a peak that, you know,",
    "start": "1352630",
    "end": "1360890"
  },
  {
    "text": "that- that's really, really peaked out each data point, and you can, you know, this- this- this, uh,",
    "start": "1360890",
    "end": "1366455"
  },
  {
    "text": "this is a valid- valid density. But at the same time it doesn't feel natural.",
    "start": "1366455",
    "end": "1372995"
  },
  {
    "text": "Another, uh, another equally valid density would be, you know, something that looks like this.",
    "start": "1372995",
    "end": "1379710"
  },
  {
    "text": "Note this is also a valid density, right, from which it could have been sampling because there's nothing to the left, nothing to the right,",
    "start": "1389350",
    "end": "1396139"
  },
  {
    "text": "and there's some values over here, and on there- there something where- where there's some data,",
    "start": "1396140",
    "end": "1401420"
  },
  {
    "text": "so you might- you might have some kind of a density. And also, this is also valid.",
    "start": "1401420",
    "end": "1406320"
  },
  {
    "text": "All right? So all these are different possible answers for what the underlying density,",
    "start": "1414530",
    "end": "1422414"
  },
  {
    "text": "uh, is from which these points are, are calculated. And kind of the, the fundamental problem of density estimation is that,",
    "start": "1422414",
    "end": "1428700"
  },
  {
    "text": "the density function has to be a continuous function. In the case if, if these were, you know, um,",
    "start": "1428700",
    "end": "1436875"
  },
  {
    "text": "the outcomes of coin tosses where the support was discrete, then maximum likelihood was, was pretty straightforward.",
    "start": "1436875",
    "end": "1443040"
  },
  {
    "text": "You could, you know, treat them as a multinomial and just count them. But whereas in density estimation, we need to come up with the smooth function or discrete observations, right?",
    "start": "1443040",
    "end": "1454905"
  },
  {
    "text": "I, I said discrete observations because we have a fixed number of observations. And we want to come up with a, with a, with a, uh, smooth estimate.",
    "start": "1454905",
    "end": "1461805"
  },
  {
    "text": "So a common approach in uh, density estimation is to use this model called the Gaussian mixture model,",
    "start": "1461805",
    "end": "1469810"
  },
  {
    "text": "or it's also called the mixture of Gaussians.",
    "start": "1470810",
    "end": "1474640"
  },
  {
    "text": "A Gaussian mixture model where given some, given some, um, uh,",
    "start": "1479510",
    "end": "1485310"
  },
  {
    "text": "data points that look like this, right?",
    "start": "1485310",
    "end": "1496025"
  },
  {
    "text": "We want to- we made this hypothesis that there are these two underlying different- two different distinct Gaussian distributions.",
    "start": "1496025",
    "end": "1506289"
  },
  {
    "text": "And there is one Gaussian distribution from which these were sampling, another Gaussian distribution from which these were sampling.",
    "start": "1507050",
    "end": "1514200"
  },
  {
    "text": "And together, you can take the sum of these two Gaussians, uh, a Gaussian probability distributions, and say,",
    "start": "1514200",
    "end": "1520515"
  },
  {
    "text": "this entire dataset is sampled from these two different Gaussian, um, uh, these two mixture of these two Gaussians, right?",
    "start": "1520515",
    "end": "1528165"
  },
  {
    "text": "And the choice of K is something again, uh, is, is, uh, similar to, you know, in k-means it,",
    "start": "1528165",
    "end": "1535005"
  },
  {
    "text": "it is something that we choose by, by, um, you know, visual inspection or, or,",
    "start": "1535005",
    "end": "1540975"
  },
  {
    "text": "um, or, or, or in general, seeing how well the data fit the, um, um,",
    "start": "1540975",
    "end": "1546840"
  },
  {
    "text": "fit the number of k. And the, um, the problem we have now is to, is to,",
    "start": "1546840",
    "end": "1555000"
  },
  {
    "text": "given this dataset, estimate the two Gaussians from which the dataset might have come, right?",
    "start": "1555000",
    "end": "1561560"
  },
  {
    "text": "We are not told what the, uh, identity of the two, uh, Gaussians are.",
    "start": "1561560",
    "end": "1567290"
  },
  {
    "text": "If this were to be a supervised setting,",
    "start": "1567290",
    "end": "1571050"
  },
  {
    "text": "then we would have, you know, they would",
    "start": "1572510",
    "end": "1585030"
  },
  {
    "text": "come with some kind of an identity.",
    "start": "1585030",
    "end": "1587740"
  },
  {
    "text": "And we could have fit one Gaussian here and the other here, right?",
    "start": "1591680",
    "end": "1601410"
  },
  {
    "text": "And this is exactly what we did in, if you remember, when we did something like this,.",
    "start": "1601410",
    "end": "1607830"
  },
  {
    "text": "GDA. GDA exactly. So in GDA, we were, we were told that X's come from are,",
    "start": "1607830",
    "end": "1613500"
  },
  {
    "text": "are sampled from Gaussians. And there are these two different classes, Class 1 and Class 2, right?",
    "start": "1613500",
    "end": "1619544"
  },
  {
    "text": "And our goal was to take these X's along with their cluster identities, the corresponding y's, and estimate the mus and sigmas for the two classes, right?",
    "start": "1619545",
    "end": "1629895"
  },
  {
    "text": "Now in Gaussian mixture model, we are essentially generalizing GDA in a way where we're not given what the Y labels are,",
    "start": "1629895",
    "end": "1639885"
  },
  {
    "text": "we are just given the x's. And you also relax the constraint which we had in GDA that the covariates have to be the same.",
    "start": "1639885",
    "end": "1646125"
  },
  {
    "text": "In this case, the covariates can be different. And our goal is to now come up with some kind of density p of x.",
    "start": "1646125",
    "end": "1657105"
  },
  {
    "text": "That allows us to assign probability density to the observed values.",
    "start": "1657105",
    "end": "1663044"
  },
  {
    "text": "Okay? So that's, that's the, uh, uh, that's the setting in which, uh, the Gaussian mixture model comes into picture.",
    "start": "1663045",
    "end": "1669910"
  },
  {
    "text": "And the reason why, um, why we are interested to even calculate this p of x.",
    "start": "1671090",
    "end": "1679185"
  },
  {
    "text": "There are many reasons why calculating p of x could be interesting. So here's one example.",
    "start": "1679185",
    "end": "1686019"
  },
  {
    "text": "So here's, a, a completely made up example, supposing, uh, supposing you are, um,",
    "start": "1696890",
    "end": "1702765"
  },
  {
    "text": "an aircraft manufacturer where let's assume the,",
    "start": "1702765",
    "end": "1708990"
  },
  {
    "text": "the, um, the parts that we manufacture have two kind of attributes.",
    "start": "1708990",
    "end": "1714360"
  },
  {
    "text": "Let's say, you know, um, heat tolerance.",
    "start": "1714360",
    "end": "1719740"
  },
  {
    "text": "And if any of you are, you know, are in aeronautics, what, what might be another,",
    "start": "1721460",
    "end": "1727255"
  },
  {
    "text": "let's say- let's call it, um, um, um, heat tolerance and,",
    "start": "1727255",
    "end": "1734220"
  },
  {
    "text": "and, um, power output, whatever that means.",
    "start": "1734220",
    "end": "1739539"
  },
  {
    "text": "So let's assume there are, there are, there are these two kinds of, um, um,",
    "start": "1739670",
    "end": "1745215"
  },
  {
    "text": "attributes that are, that are, um, therefore some part that we manufacture.",
    "start": "1745215",
    "end": "1751049"
  },
  {
    "text": "And in general, what we observe that if we were to plot the, um, the,",
    "start": "1751050",
    "end": "1757905"
  },
  {
    "text": "the, every manufactured part as a point here you might observe",
    "start": "1757905",
    "end": "1763260"
  },
  {
    "text": "that most",
    "start": "1763260",
    "end": "1772980"
  },
  {
    "text": "of the normal parts fall along some kind of,",
    "start": "1772980",
    "end": "1778919"
  },
  {
    "text": "um, uh, a distribution like this. Maybe there are, you know, two different kinds of, uh,",
    "start": "1778920",
    "end": "1784830"
  },
  {
    "text": "sub-types of parts may be based on the material or something where some of them, uh, uh, fall in distribution,",
    "start": "1784830",
    "end": "1790770"
  },
  {
    "text": "some of them fall in this kind of a distribution. You know, uh, whatever be the reason. But generally let's assume that, you know,",
    "start": "1790770",
    "end": "1797790"
  },
  {
    "text": "normal looking parts fall in this kind of, uh, um, belong to this kind of a p robability distribution.",
    "start": "1797790",
    "end": "1804870"
  },
  {
    "text": "Now suppose we want to, we want to, uh, have some kind of an automated anomaly detection where we want to detect that,",
    "start": "1804870",
    "end": "1813525"
  },
  {
    "text": "you know, some part is, is, is faulty for some reason, right? For example, a part that may- that has this attribute, right?",
    "start": "1813525",
    "end": "1822465"
  },
  {
    "text": "We want to identify that this point over here is, is, uh, is faulty.",
    "start": "1822465",
    "end": "1828660"
  },
  {
    "text": "And at first, it, it might be, you know, um, even though this looks visually away from this kind of a heatmap,",
    "start": "1828660",
    "end": "1838408"
  },
  {
    "text": "if you were to look at any one of the axis alone, it looks pretty normal, right?",
    "start": "1838409",
    "end": "1843900"
  },
  {
    "text": "Just from the heat tolerance point of view, it's kind of, you know, close to the mean.",
    "start": "1843900",
    "end": "1849299"
  },
  {
    "text": "And if you just look at the power output, it's also kind of near the mean, but it's this combination that makes it kind of,",
    "start": "1849300",
    "end": "1854790"
  },
  {
    "text": "you know, an anomalous example, right? And the, the way, uh uh,",
    "start": "1854790",
    "end": "1860670"
  },
  {
    "text": "this kind of an anomaly detection is, um, can be carried out in practice is to construct,",
    "start": "1860670",
    "end": "1866309"
  },
  {
    "text": "you know, a density estimate p of x for these points. Where the- where this p of x assigns",
    "start": "1866309",
    "end": "1874410"
  },
  {
    "text": "high probability for anything that falls in this region. And p of x assigns low probability for anything outside this region, right?",
    "start": "1874410",
    "end": "1884415"
  },
  {
    "text": "And a common approach to doing this kind of a density estimation is to use mixture of Gaussians, right?",
    "start": "1884415",
    "end": "1892275"
  },
  {
    "text": "And the way we go about, uh, doing mixture of Gaussians is so first what we're",
    "start": "1892275",
    "end": "1900360"
  },
  {
    "text": "going to do is to provide you an algorithm to do mixture of Gaussians.",
    "start": "1900360",
    "end": "1906015"
  },
  {
    "text": "And you're going to provide, uh, construct this first algorithm based purely on intuition.",
    "start": "1906015",
    "end": "1913815"
  },
  {
    "text": "And then what we're gonna do is describe this general framework called expectation maximization,",
    "start": "1913815",
    "end": "1920835"
  },
  {
    "text": "and then re-derive Gaussian mixture- the Gaussian mixture model using this framework and see that we end up with",
    "start": "1920835",
    "end": "1927540"
  },
  {
    "text": "the same algorithm that we got using intuition. The expectation, uh,",
    "start": "1927540",
    "end": "1934245"
  },
  {
    "text": "maximization that we're going to see next is a more general framework that works for a broad class of- um- of- of- of- um, generative models.",
    "start": "1934245",
    "end": "1944835"
  },
  {
    "text": "These are examples of generative models. And being a Gaussian mixture model is just one such,",
    "start": "1944835",
    "end": "1951990"
  },
  {
    "text": "one such model which can be solved through expectation-maximization. Yes question?",
    "start": "1951990",
    "end": "1957809"
  },
  {
    "text": "So even given that you know [inaudible] difficulty to scale it based on, you know, this triangle,",
    "start": "1959380",
    "end": "1966090"
  },
  {
    "text": "the [inaudible] shouldn't we also mix- doesn't someone need to tell us what",
    "start": "1966090",
    "end": "1973590"
  },
  {
    "text": "is the highest restrict,",
    "start": "1973590",
    "end": "1983819"
  },
  {
    "text": "like, what is the upper bound [inaudible] at the number of data points, right? Yeah. Don't we need to be told what the frequency [inaudible].",
    "start": "1983819",
    "end": "1990930"
  },
  {
    "text": "So the, the the question is, uh, I guess, um, to- to kinda summarize this,",
    "start": "1990930",
    "end": "1996990"
  },
  {
    "text": "how do we find out k? And, uh, it is true that as we increase the number k,",
    "start": "1996990",
    "end": "2002990"
  },
  {
    "text": "uh, we kind of fit the data better and better. And, um, in order to kind of, uh,",
    "start": "2002990",
    "end": "2009565"
  },
  {
    "text": "think of what's the best value of k, uh, I will leave this as a thought exercise for",
    "start": "2009565",
    "end": "2014779"
  },
  {
    "text": "now and we'll come back to this probably next week. Um, try to see how you can apply, you know,",
    "start": "2014780",
    "end": "2021200"
  },
  {
    "text": "uh, what we learned in, um, um, in learning theory bias and variance. You know, how, how can you, you know, um, uh,",
    "start": "2021200",
    "end": "2028595"
  },
  {
    "text": "give some thought on how you can apply bias-variance analysis on this kind of a problem, right? And we will come back to this later.",
    "start": "2028595",
    "end": "2035150"
  },
  {
    "text": "For now we'll, you know, for, for, for today and for, uh, the rest of this week, we're going to just cover more algorithms and we'll, you know,",
    "start": "2035150",
    "end": "2041975"
  },
  {
    "text": "come back to it later with, you know, and see how we can kind of approach it in a more principled way.",
    "start": "2041975",
    "end": "2047375"
  },
  {
    "text": "You know, for now, you know, as, as a mental exercise, see how you can apply bias-variance analysis in this kind of a setting.",
    "start": "2047375",
    "end": "2053780"
  },
  {
    "text": "[NOISE]",
    "start": "2053780",
    "end": "2067129"
  },
  {
    "text": "So in the mixture of Gaussian, our Gaussian mixture model.",
    "start": "2067130",
    "end": "2073800"
  },
  {
    "text": "So we are, um, you're given attaining the set of just xs.",
    "start": "2076300",
    "end": "2084060"
  },
  {
    "text": "And then we're going to assume that is this, um,",
    "start": "2091090",
    "end": "2098435"
  },
  {
    "text": "z^i which belongs to a multinomial distribution with",
    "start": "2098435",
    "end": "2109490"
  },
  {
    "text": "parameter- with parameter Phi where Phi_j is greater than equal to 0,",
    "start": "2109490",
    "end": "2116270"
  },
  {
    "text": "and the sum over all j equals 1 through k, Phi_j equals 1.",
    "start": "2116270",
    "end": "2123300"
  },
  {
    "text": "And phi_j is basically, um, um,",
    "start": "2124780",
    "end": "2131315"
  },
  {
    "text": "Phi_j is the probability that z^i equals J, right?",
    "start": "2131315",
    "end": "2139730"
  },
  {
    "text": "And then we have x^i given z^i equals",
    "start": "2139730",
    "end": "2146390"
  },
  {
    "text": "j to be distributed from a normal distribution of mean Mu j and covariance Sigma j.",
    "start": "2146390",
    "end": "2157835"
  },
  {
    "text": "Right? So this is describing the model. So the- the way we assume the model works is that first we",
    "start": "2157835",
    "end": "2166580"
  },
  {
    "text": "sample the class identity z from some kind of a multinomial distribution, right?",
    "start": "2166580",
    "end": "2173900"
  },
  {
    "text": "And then wha-, you know, once we, once we have a sample, the identity, we generate an obs- an observation x condition on",
    "start": "2173900",
    "end": "2183455"
  },
  {
    "text": "the z value that we sample from what- some particular Gaussian distribution with mean Mu j and,",
    "start": "2183455",
    "end": "2189215"
  },
  {
    "text": "and covariance Sigma j. This is very similar to GDA, right? In GDA, uh, the- the- the- the differences between this and GDA is that in GDA,",
    "start": "2189215",
    "end": "2200180"
  },
  {
    "text": "we call z as y, here we're just calling it z. And that is a common pattern that you will see when in- in- in the algorithms,",
    "start": "2200180",
    "end": "2209450"
  },
  {
    "text": "where we're doing full observation, which becomes a supervised setting, the variables that we call y,",
    "start": "2209450",
    "end": "2214970"
  },
  {
    "text": "we end up calling them as z in unsupervised setting, because they are not observed, okay?",
    "start": "2214970",
    "end": "2220355"
  },
  {
    "text": "In this case, there is this underlying z that we do not observe, that are sampled from some multinomial distribution.",
    "start": "2220355",
    "end": "2227479"
  },
  {
    "text": "And depending on the identity of the cluster that we, uh, that we sample,",
    "start": "2227479",
    "end": "2233330"
  },
  {
    "text": "the observation is then sampled from a corresponding Gaussian distribution that has a mean and covariance specific to that cluster.",
    "start": "2233330",
    "end": "2241550"
  },
  {
    "text": "In these cases, z^'i's, because they're not observed, are called latent variables.",
    "start": "2241550",
    "end": "2249420"
  },
  {
    "text": "So latent variable is a fancy name for a random variable that you've not observed.",
    "start": "2252910",
    "end": "2258230"
  },
  {
    "text": "You've just not seen what- what um, what its value is in, and that's why we call it latent.",
    "start": "2258230",
    "end": "2263840"
  },
  {
    "text": "Have question. Yes, question. So is there Phi in class prior? So Phi over here is the,",
    "start": "2263840",
    "end": "2270559"
  },
  {
    "text": "you - you can think of it as the class prior. The- the- the class prior that we had in GDA.",
    "start": "2270560",
    "end": "2277550"
  },
  {
    "text": "Uh, this just tells us of all the examples x that we have, what fraction of them belong to cluster j? Good question.",
    "start": "2277550",
    "end": "2287820"
  },
  {
    "text": "And now in GDA,",
    "start": "2290890",
    "end": "2300200"
  },
  {
    "text": "we perform maximum likelihood estimation in GDA. And our maximum likelihood objective",
    "start": "2300200",
    "end": "2306470"
  },
  {
    "text": "was GDA it was",
    "start": "2306470",
    "end": "2314315"
  },
  {
    "text": "log p of x, y Mu Sigma and Phi over the parameters, right?",
    "start": "2314315",
    "end": "2325655"
  },
  {
    "text": "So this was the log-likelihood objective in GDA, right?",
    "start": "2325655",
    "end": "2334160"
  },
  {
    "text": "Whereas in Gaussian mixture model, we do not observe y, right?",
    "start": "2334160",
    "end": "2342230"
  },
  {
    "text": "And so in the- in the Gaussian mixture model, our objective will be to maximize log p of x",
    "start": "2342230",
    "end": "2354200"
  },
  {
    "text": "given for Phi Mu Sigma.",
    "start": "2354200",
    "end": "2359869"
  },
  {
    "text": "Like this will be our likelihood function. We test the only difference.",
    "start": "2359870",
    "end": "2367400"
  },
  {
    "text": "Over here, your objective was the full joint distribution. Over here, we would have liked to do the same,",
    "start": "2367400",
    "end": "2375845"
  },
  {
    "text": "but we haven't observed the y- the corresponding ys, which you call as zs here, right? They-, they- they're not observed.",
    "start": "2375845",
    "end": "2381950"
  },
  {
    "text": "So we cannot construct a likelihood function because we won't know what value of z to put in this expression.",
    "start": "2381950",
    "end": "2388760"
  },
  {
    "text": "If you had observed them, it was pretty straightforward, we would have been just doing GDA. And so instead what we do is maximize log p of x,",
    "start": "2388760",
    "end": "2398585"
  },
  {
    "text": "and log p of x can- can also be written as log sum over z,",
    "start": "2398585",
    "end": "2407690"
  },
  {
    "text": "p of x,z Phi Mu.",
    "start": "2407690",
    "end": "2416819"
  },
  {
    "text": "Right? So we write out the full joint distribution and marginalize out the latent variable.",
    "start": "2419020",
    "end": "2426270"
  },
  {
    "text": "Any- any questions how we went from this to this? Yes, question. [inaudible]",
    "start": "2427120",
    "end": "2440690"
  },
  {
    "text": "So the question is, shouldn't z also contribute to our likelihood objective?",
    "start": "2440690",
    "end": "2446450"
  },
  {
    "text": "And the answer is, if we had observed z, then yes, it should have. But we don't know what z is.",
    "start": "2446450",
    "end": "2452840"
  },
  {
    "text": "So it we know it cannot.",
    "start": "2452840",
    "end": "2455730"
  },
  {
    "text": "Assuming [inaudible] So the- so the- so the question is, we are assuming there are k clusters and given a setting,",
    "start": "2465280",
    "end": "2472520"
  },
  {
    "text": "ah, you know, some value for k shouldn't be therefore account for- for the cluster identity.",
    "start": "2472520",
    "end": "2478550"
  },
  {
    "text": "Uh, we are making an assumption about k, but we don't know which of those k clusters each one belongs to.",
    "start": "2478550",
    "end": "2487070"
  },
  {
    "text": "Okay? All right. So our objective is to now maximize this expression,",
    "start": "2487070",
    "end": "2495665"
  },
  {
    "text": "which is the same as this expression. Right? And for the rest of- of today's lecture and throughout,",
    "start": "2495665",
    "end": "2505610"
  },
  {
    "text": "uh, it- it can be useful to set up some kind of terminology.",
    "start": "2505610",
    "end": "2511250"
  },
  {
    "text": "So for example, p of z, we will call it class prior,",
    "start": "2511250",
    "end": "2516720"
  },
  {
    "text": "or, in cases where z is not discrete but it's continuous, we'll just call it prior.",
    "start": "2517930",
    "end": "2525140"
  },
  {
    "text": "Right. P of x,z,",
    "start": "2525140",
    "end": "2531320"
  },
  {
    "text": "we will call it the model, because it- it describes the full data generating process.",
    "start": "2531320",
    "end": "2537740"
  },
  {
    "text": "The joint distribution always gen- describes the full data generating process and that's always called the model, right?",
    "start": "2537740",
    "end": "2544234"
  },
  {
    "text": "And z in unsupervised settings is called latent because we don't observe it.",
    "start": "2544235",
    "end": "2553760"
  },
  {
    "text": "Latent is just a fancy word for unobserved. Okay? And P of z given x,",
    "start": "2553760",
    "end": "2559940"
  },
  {
    "text": "we would call it the posterior. Okay? And finally, P of x.",
    "start": "2559940",
    "end": "2567695"
  },
  {
    "text": "So P of z is called the prior and P of x will be called the evidence.",
    "start": "2567695",
    "end": "2573060"
  },
  {
    "text": "Evidence because x is what we observe, That's the evidence-based on which we are performing inference.",
    "start": "2573550",
    "end": "2578990"
  },
  {
    "text": "This is just terminology and this terminology is- is pretty standard and used in many papers, many- many books.",
    "start": "2578990",
    "end": "2587195"
  },
  {
    "text": "So our goal is to maximize the likelihood using the evidence.",
    "start": "2587195",
    "end": "2595590"
  },
  {
    "text": "Right? And this-",
    "start": "2600070",
    "end": "2603360"
  },
  {
    "text": "the way we go about doing that is- right?",
    "start": "2613540",
    "end": "2625715"
  },
  {
    "text": "So, to directly maximize this evidence, uh, um, directly, uh, maximize this log, uh, likelihood.",
    "start": "2625715",
    "end": "2634385"
  },
  {
    "text": "If we were to attempt it the way we did it with GDA of taking derivative, setting it equal to 0, and solving for the parameters,",
    "start": "2634385",
    "end": "2641270"
  },
  {
    "text": "you will observe that you ju- you won't get a closed-form expression for this. You can try it out, and then you won't get",
    "start": "2641270",
    "end": "2646430"
  },
  {
    "text": "a closed-form expression for the way we got it with GDA. In GDA, we got a closed-form expression because we had",
    "start": "2646430",
    "end": "2652430"
  },
  {
    "text": "observed both xs and zs, we called it y's. And if we had observed both xs and ys here,",
    "start": "2652430",
    "end": "2657830"
  },
  {
    "text": "we would have gotten a closed-form expression. But because, um, the zs are not observed and we are taking this,",
    "start": "2657830",
    "end": "2665015"
  },
  {
    "text": "ah, ah, marginalizing them. If- if we work it out, we will not get closed-form expressions. So instead, what we will do is just like taking inspiration from k-means,",
    "start": "2665015",
    "end": "2676735"
  },
  {
    "text": "we are going to first, um, imagine, or, or, or come up with some kind of an estimate for zi's first.",
    "start": "2676735",
    "end": "2686569"
  },
  {
    "text": "So the algorithm that we're going to do is repeat until convergence,",
    "start": "2686570",
    "end": "2699170"
  },
  {
    "text": "but this is just taken inspired by k-means. [NOISE]",
    "start": "2699170",
    "end": "2714380"
  },
  {
    "text": "We'll call it the E step. For each i, j set w_",
    "start": "2714380",
    "end": "2729580"
  },
  {
    "text": "i j [NOISE] to be equal to P of z_i",
    "start": "2729580",
    "end": "2738530"
  },
  {
    "text": "equals j given x_i",
    "start": "2738530",
    "end": "2746540"
  },
  {
    "text": "parameters Phi, Mu, Sigma. [NOISE] And the M step,",
    "start": "2746540",
    "end": "2753619"
  },
  {
    "text": "where we update the parameters. P_ j equals 1 over n,",
    "start": "2756930",
    "end": "2774650"
  },
  {
    "text": "equals 1 to n w_i j. Mu _j is sum of",
    "start": "2774690",
    "end": "2785964"
  },
  {
    "text": "i equals 1 to n w_i j x_i over",
    "start": "2785965",
    "end": "2796105"
  },
  {
    "text": "i equals 1 to n w_i j,",
    "start": "2796105",
    "end": "2802705"
  },
  {
    "text": "and Sigma_j is equal to sum",
    "start": "2802705",
    "end": "2812860"
  },
  {
    "text": "over i equals 1 to n w_ i j x_i minus Mu_j,",
    "start": "2812860",
    "end": "2822850"
  },
  {
    "text": "x_i minus Mu_j transpose,",
    "start": "2822850",
    "end": "2829970"
  },
  {
    "text": "divided by i equals 1 to n w_i j.",
    "start": "2829970",
    "end": "2839119"
  },
  {
    "text": "So what we did is we start with some random initialization,",
    "start": "2839120",
    "end": "2846155"
  },
  {
    "text": "so uh, randomly initialized parameters.",
    "start": "2846155",
    "end": "2851375"
  },
  {
    "text": "And so the repeat will start after we, uh, randomly initialize it, randomly initialize Mu, Phi, and Sigma.",
    "start": "2851375",
    "end": "2863670"
  },
  {
    "text": "Right? Start with some random initialization, and think of this as the way we randomly initialize the cluster centroids and k-means.",
    "start": "2863670",
    "end": "2871855"
  },
  {
    "text": "Right? And based on the random initialization in k-means for each point,",
    "start": "2871855",
    "end": "2878755"
  },
  {
    "text": "we associated it to the nearest cluster centroid, right? And the- the- the kind of similar operation that we're gonna do here is for each point,",
    "start": "2878755",
    "end": "2888520"
  },
  {
    "text": "we're gonna assign a weight to each cluster centroid specific to that point,",
    "start": "2888520",
    "end": "2894455"
  },
  {
    "text": "where the weight is the posterior distribution of P of z given x.",
    "start": "2894455",
    "end": "2900080"
  },
  {
    "text": "Right? So given a point, we calculate a posterior distribution of the probability that this point belongs to a particular centroid.",
    "start": "2900080",
    "end": "2909665"
  },
  {
    "text": "And this is just the posterior, uh, um, the posterior distribution.",
    "start": "2909665",
    "end": "2914930"
  },
  {
    "text": "We-We're gonna call it a weight. And once we calculate this weight,",
    "start": "2914930",
    "end": "2920510"
  },
  {
    "text": "we're gonna re-weight all our data points to calculate the corresponding Mus and Sigmas.",
    "start": "2920510",
    "end": "2926315"
  },
  {
    "text": "Right? So for example, x_i will-may belong to- will have a weight of,",
    "start": "2926315",
    "end": "2933545"
  },
  {
    "text": "let's say, um, uh, if there are- if there are, uh, say, three, uh, centroids where k equals 3,",
    "start": "2933545",
    "end": "2939830"
  },
  {
    "text": "then P of z_i equals,",
    "start": "2939830",
    "end": "2947180"
  },
  {
    "text": "uh, uh, j given x_i, right? This could be some kind of, um,",
    "start": "2947180",
    "end": "2954530"
  },
  {
    "text": "multinomial distribution like 0.1, 0.7, 0.2, right?",
    "start": "2954530",
    "end": "2961265"
  },
  {
    "text": "Where this belongs to k equals 1, k equals, uh, 2, k equals 3.",
    "start": "2961265",
    "end": "2966829"
  },
  {
    "text": "Where if the centroid Mu was- was close to- um, uh, Mu 2 was close to,",
    "start": "2966830",
    "end": "2972155"
  },
  {
    "text": "uh, x_i, then it would have a higher weight, and, you know, these two are farther away, so it has a lower weight.",
    "start": "2972155",
    "end": "2978230"
  },
  {
    "text": "In - in case of k-means, we would do a hard assignment of every point to one cluster only.",
    "start": "2978230",
    "end": "2986765"
  },
  {
    "text": "But over here, you're doing a soft assignment, where every point has- has a soft assignment in",
    "start": "2986765",
    "end": "2993980"
  },
  {
    "text": "the form of this probability distribution for all the cluster centroids. And the probability assigned to the centroid that is closer to the point.",
    "start": "2993980",
    "end": "3004795"
  },
  {
    "text": "By closer here, we mean in a probabilistic sense where it is- uh, it has a high likelihood in that- in- in- in that, um,",
    "start": "3004795",
    "end": "3012745"
  },
  {
    "text": "clusters Gaussian distribution, then it will have a high posterior, uh, uh, probability.",
    "start": "3012745",
    "end": "3020335"
  },
  {
    "text": "And we do this kind of a soft assignment of every point to- to the set of all clusters.",
    "start": "3020335",
    "end": "3025885"
  },
  {
    "text": "And using the- the correct uh, calculated weights, we recalculate the Mus and Sigmas,",
    "start": "3025885",
    "end": "3032695"
  },
  {
    "text": "using the weighted dataset. Here, every- every, uh, point i,",
    "start": "3032695",
    "end": "3038485"
  },
  {
    "text": "contributes to every centroid j, and the contribution is weighted by the corresponding w_i j. Questions?",
    "start": "3038485",
    "end": "3048590"
  },
  {
    "text": "So that first step that you, yeah. So that will have- also will that be- will that have like",
    "start": "3049170",
    "end": "3054640"
  },
  {
    "text": "a closed form of expression where you calculate all the differences and then divide by the sum over all of them?",
    "start": "3054640",
    "end": "3059740"
  },
  {
    "text": "Yes. So the question is, will this have a closed-form expression? Um, [OVERLAPPING] It'll be 0.7 over?",
    "start": "3059740",
    "end": "3068950"
  },
  {
    "text": "Yeah. Yeah. So the question is- so for this, I would- I would, uh, remind you, uh, in case of GDA,",
    "start": "3068950",
    "end": "3075535"
  },
  {
    "text": "we had calculated a posterior that was very similar. And if you remember, the posterior had a form of?",
    "start": "3075535",
    "end": "3081280"
  },
  {
    "text": "A logistic. Yeah, it had the form of a logistic, uh, uh, uh, uh, regression.",
    "start": "3081280",
    "end": "3087040"
  },
  {
    "text": "In that case, however, we limited ourselves to two, uh, two points- to two classes,",
    "start": "3087040",
    "end": "3092200"
  },
  {
    "text": "and we also had a constraint that Sigma was common to all of them. But in this case, when we relax that, um, uh, constraint,",
    "start": "3092200",
    "end": "3099670"
  },
  {
    "text": "what we will observe is that the posterior takes the form of a soft max, right?",
    "start": "3099670",
    "end": "3104770"
  },
  {
    "text": "It- it takes the form of a soft max that uses quadratic features, quadratic features of Xs.",
    "start": "3104770",
    "end": "3111279"
  },
  {
    "text": "Right? But, uh, for-for a fixed value of, you know, for- for a small case, you can, you know, come up with- with- with an expression for this,",
    "start": "3111280",
    "end": "3118780"
  },
  {
    "text": "and in fact, you'll be doing this in your homework. So that'll be clarified in your homework as well. Right? So, um- so inspired by k-means,",
    "start": "3118780",
    "end": "3128395"
  },
  {
    "text": "here is a version of- you can think of- of the Gaussian mixture model as soft k-means, you know, people will call it the soft k-means as well.",
    "start": "3128395",
    "end": "3135400"
  },
  {
    "text": "Uh, we call it soft because, um, in the assignment phase, in k-means, right?",
    "start": "3135400",
    "end": "3141849"
  },
  {
    "text": "In k-means it was a hard assignment, right? This one- this corresponds to the E-step.",
    "start": "3141850",
    "end": "3148360"
  },
  {
    "text": "The, uh-in k-means, uh, the cluster identity was a hard assignment.",
    "start": "3148360",
    "end": "3154089"
  },
  {
    "text": "You can think of, uh, k-means as a way in which we calculate a posterior distribution,",
    "start": "3154090",
    "end": "3159790"
  },
  {
    "text": "which is always one-hot, right? If- if- uh, if you- if we calculate these kind of posterior distributions,",
    "start": "3159790",
    "end": "3167200"
  },
  {
    "text": "then we effectively get k-means out of, uh, GMMs, right? And in the equivalent of the M-step,",
    "start": "3167200",
    "end": "3175269"
  },
  {
    "text": "was the way in which we were recalculating Mus. Uh, the Mu_js, we use only those- those, uh,",
    "start": "3175269",
    "end": "3182515"
  },
  {
    "text": "xs for which the cluster identity matched the, uh, uh, the corresponding cluster.",
    "start": "3182515",
    "end": "3188575"
  },
  {
    "text": "And over here, instead of- instead of having, um, an indicator function, we're gonna use this soft assignment weight.",
    "start": "3188575",
    "end": "3197350"
  },
  {
    "text": "Okay? Any questions? So you just claimed that if Ws [inaudible] becomes k mean?",
    "start": "3197350",
    "end": "3203455"
  },
  {
    "text": "Yeah. So if- if Ws are one hat, then this- this will essentially be,",
    "start": "3203455",
    "end": "3210595"
  },
  {
    "text": "you know, uh, k-means. Yes. Question?",
    "start": "3210595",
    "end": "3214670"
  },
  {
    "text": "The w_i j, how do we calculate it if we haven't observed-observed Z_i? So the question is, how do we calculate w_i j if we- if we",
    "start": "3216270",
    "end": "3224020"
  },
  {
    "text": "have not observed Z_is- the- the Zs? Is that the question? Yeah. So here, uh,",
    "start": "3224020",
    "end": "3229045"
  },
  {
    "text": "in order to calculate this, we don't need to know Z_is. And so we don't need to- so we've just, uh, cons- uh,",
    "start": "3229045",
    "end": "3236710"
  },
  {
    "text": "uh, [NOISE] we're just constructing the probability that z_i could be equal to j.",
    "start": "3236710",
    "end": "3242305"
  },
  {
    "text": "Right? And the way we go about doing this, is to use the Bayes rule. Right? And the way you use the Bayes rule,",
    "start": "3242305",
    "end": "3250030"
  },
  {
    "text": "[NOISE] um, [NOISE] right?",
    "start": "3250030",
    "end": "3255660"
  },
  {
    "text": "So- and this can be calculated as P of- you know, so P of z given x is equal to P of x given z times P of z,",
    "start": "3255660",
    "end": "3270675"
  },
  {
    "text": "or sum overall of z,",
    "start": "3270675",
    "end": "3276115"
  },
  {
    "text": "P of x given z times P of z.",
    "start": "3276115",
    "end": "3281305"
  },
  {
    "text": "Over here P of x given z is normal distribution,",
    "start": "3281305",
    "end": "3286704"
  },
  {
    "text": "so this is Gaussian [NOISE] Right?",
    "start": "3286705",
    "end": "3293350"
  },
  {
    "text": "And P of z is just multinomial.",
    "start": "3293350",
    "end": "3296930"
  },
  {
    "text": "And in the denominator, uh, you know, it is same- the, uh, the- the same terms, Gaussian and the multinomial,",
    "start": "3299420",
    "end": "3306180"
  },
  {
    "text": "but you sum it over all the classes. [NOISE] And you will see that, you know, um,",
    "start": "3306180",
    "end": "3312220"
  },
  {
    "text": "just the way you showed it in Homework 1 for GDA, where in GDA this took the form of a logistic, uh, regression.",
    "start": "3312220",
    "end": "3319450"
  },
  {
    "text": "Uh, here, you can actually show that it takes the form of a soft max. It's- it's very similar calculation.",
    "start": "3319450",
    "end": "3325780"
  },
  {
    "text": "[NOISE] Any questions? Right? Yeah. So this is basically,",
    "start": "3325780",
    "end": "3333325"
  },
  {
    "text": "um, the Gaussian mixture module, where we derive the steps to be- by taking inspiration from k-means, right?",
    "start": "3333325",
    "end": "3340105"
  },
  {
    "text": "And we are intentionally giving it the names, the E-step and the M-step, because next, we're gonna, uh, talk about the EM algorithm and, uh,",
    "start": "3340105",
    "end": "3347890"
  },
  {
    "text": "we're gonna derive it in a more principled way, and we're gonna end up with the same update rules. Right? And this is- uh,",
    "start": "3347890",
    "end": "3355060"
  },
  {
    "text": "you know, think of this as soft k-means. [NOISE] Soft k-means. [NOISE] Right.",
    "start": "3355060",
    "end": "3364040"
  },
  {
    "text": "Now, we're gonna switch gears and talk about the EM algorithm.",
    "start": "3381710",
    "end": "3387820"
  },
  {
    "text": "So the EM algorithm is also called the expectation-maximization algorithm.",
    "start": "3388670",
    "end": "3394770"
  },
  {
    "text": "That's an algorithm where it gives us a framework of",
    "start": "3394770",
    "end": "3401895"
  },
  {
    "text": "maximize - of - of performing maximum likelihood estimation when some variables are unobserved, all right?",
    "start": "3401895",
    "end": "3408990"
  },
  {
    "text": "It is - it - it - it is used in cases where we have a functional form for the joint P of x comma Z,",
    "start": "3408990",
    "end": "3417195"
  },
  {
    "text": "and x and z could be anything but Z's are not observed, so expectation-maximization.",
    "start": "3417195",
    "end": "3427000"
  },
  {
    "text": "All right.",
    "start": "3427640",
    "end": "3436109"
  },
  {
    "text": "Expectation-maximization is an algorithm where we perform MLE in the presence of latent variables.",
    "start": "3436110",
    "end": "3446890"
  },
  {
    "text": "All right. So where the true model [NOISE] is P of",
    "start": "3455840",
    "end": "3465345"
  },
  {
    "text": "x comma z with some parameters Theta. And if we observe x and z jointly,",
    "start": "3465345",
    "end": "3474555"
  },
  {
    "text": "if everything is observed, then the problem is very simple, we perform simple maximum likelihood estimation.",
    "start": "3474555",
    "end": "3481289"
  },
  {
    "text": "But when z is unobserved,",
    "start": "3481290",
    "end": "3489070"
  },
  {
    "text": "we want to instead perform maximization of the objective l",
    "start": "3490160",
    "end": "3501000"
  },
  {
    "text": "Theta equals log P of x,",
    "start": "3501000",
    "end": "3507240"
  },
  {
    "text": "where z is marginalized, and the EM algorithm, or the EM framework,",
    "start": "3507240",
    "end": "3512760"
  },
  {
    "text": "gives us a framework for achieving this. Where we maximize log P of x in an indirect way rather than directly,",
    "start": "3512760",
    "end": "3521760"
  },
  {
    "text": "uh, taking the derivatives and setting them to 0 and so on. Now, before we go into the EM algorithm,",
    "start": "3521760",
    "end": "3527700"
  },
  {
    "text": "some kind of a - it - it - it can be useful to have some kind of a context here. So the EM algorithm was - was, uh,",
    "start": "3527700",
    "end": "3534015"
  },
  {
    "text": "discovered some - sometime in the - I think early 1970s where, uh, you know, where - where people were trying to, uh,",
    "start": "3534015",
    "end": "3541920"
  },
  {
    "text": "perform MLE in the presence of unobserved, uh, uh, unobserved data. And it so happens that this - this framework is so general and so",
    "start": "3541920",
    "end": "3551730"
  },
  {
    "text": "powerful that there are - it has been adapted in so many different ways,",
    "start": "3551730",
    "end": "3557025"
  },
  {
    "text": "and there have been minor, uh, uh, you know, exceptions to the EM algorithms are - are there in so many different forms.",
    "start": "3557025",
    "end": "3563505"
  },
  {
    "text": "But this framework is - is somewhat central and understanding EM",
    "start": "3563505",
    "end": "3569369"
  },
  {
    "text": "in a deep way will be extremely useful if you are interested in things like deep generative models.",
    "start": "3569370",
    "end": "3575670"
  },
  {
    "text": "All right. So in - in, um, over the last few years, there has been, uh, tremendous growth in deep generative models, uh,",
    "start": "3575670",
    "end": "3583275"
  },
  {
    "text": "where, um, you might have heard of, you know, variational autoencoders, generative adversarial networks or GANs, or flow-based models, glow based models.",
    "start": "3583275",
    "end": "3592515"
  },
  {
    "text": "And all of them, uh, understanding all of them would be a lot - much easier if you really understand the EM algorithm well,",
    "start": "3592515",
    "end": "3599670"
  },
  {
    "text": "because the EM framework gives you a - a kind of a mind-map where you can place all these different algorithms and kind of",
    "start": "3599670",
    "end": "3606195"
  },
  {
    "text": "understand their strengths and weaknesses and - and you know, what's common between them, what's different between them, and so on.",
    "start": "3606195",
    "end": "3611790"
  },
  {
    "text": "So EM algorithm is - is, um, one of the key algorithms to,",
    "start": "3611790",
    "end": "3617565"
  },
  {
    "text": "um, for - for even modern, um, um, um, deep learning or deep generative,",
    "start": "3617565",
    "end": "3622740"
  },
  {
    "text": "um, um, um, deep generative models. All Right? So before we jump into EM algorithm,",
    "start": "3622740",
    "end": "3629280"
  },
  {
    "text": "we're gonna first look at something called as Jensen's inequality.",
    "start": "3629280",
    "end": "3636090"
  },
  {
    "text": "[NOISE]",
    "start": "3636090",
    "end": "3649980"
  },
  {
    "text": "So Jensen's inequality is a - is a very general,",
    "start": "3649980",
    "end": "3658994"
  },
  {
    "text": "uh, probabilistic inequality that's used, you know, in - in very many places,",
    "start": "3658995",
    "end": "3664140"
  },
  {
    "text": "uh, in probability theory and applied probability theory. And it will show up in - in our derivation of, uh, EM algorithm as well.",
    "start": "3664140",
    "end": "3670800"
  },
  {
    "text": "So you can think of Jensen's inequality as a probabilistic tool that we will use in deriving, uh, EM.",
    "start": "3670800",
    "end": "3676214"
  },
  {
    "text": "But Jensen's inequality by itself is - is - is a very generic and you know,",
    "start": "3676215",
    "end": "3681375"
  },
  {
    "text": "commonly used inequality in - in, uh, probability theory. So let's - let's assume a function f to be convex, right?",
    "start": "3681375",
    "end": "3693810"
  },
  {
    "text": "Assume f to be a convex function, which means, um, f double prime of x is greater than equal to 0 for all X.",
    "start": "3693810",
    "end": "3703440"
  },
  {
    "text": "[NOISE] All right. And we say that f is strictly convex,",
    "start": "3703440",
    "end": "3713590"
  },
  {
    "text": "if f double prime of x is greater than zero for all x, okay?",
    "start": "3716090",
    "end": "3725070"
  },
  {
    "text": "So the, uh, the mental picture to have is f of x,",
    "start": "3725070",
    "end": "3738240"
  },
  {
    "text": "and this is x, f",
    "start": "3738240",
    "end": "3746940"
  },
  {
    "text": "of x and x. So this is an example of a convex function.",
    "start": "3746940",
    "end": "3752625"
  },
  {
    "text": "Convex functions are bowl-shaped functions, right, and they can have a ze- zero second derivative in a few places where f prime of x,",
    "start": "3752625",
    "end": "3762420"
  },
  {
    "text": "um, is greater than or equal to 0. But in a strictly convex function, the second derivative is never exactly equal to 0,",
    "start": "3762420",
    "end": "3769980"
  },
  {
    "text": "it's always greater than 0. So if there are straight lines for - for certain input ranges in your functions,",
    "start": "3769980",
    "end": "3777734"
  },
  {
    "text": "then it can still be convex. But for a strictly convex function that cannot be straight lines, all right?",
    "start": "3777735",
    "end": "3784200"
  },
  {
    "text": "It should always be curving upwards. Now, the Jensen's inequality tells us",
    "start": "3784200",
    "end": "3792300"
  },
  {
    "text": "that expectation of f of x,",
    "start": "3792300",
    "end": "3800745"
  },
  {
    "text": "where x is some random variable, is always greater than or equal to f of expectation of x,",
    "start": "3800745",
    "end": "3813400"
  },
  {
    "text": "where f is convex.",
    "start": "3813410",
    "end": "3824565"
  },
  {
    "text": "So this is Jensen's inequality.",
    "start": "3824565",
    "end": "3828849"
  },
  {
    "text": "So the Jensen's inequality tells us that, the expectation of f of x,",
    "start": "3836510",
    "end": "3842369"
  },
  {
    "text": "where f is a convex function and x is a random variable, and the expectation is taken with respect to the randomness in x,",
    "start": "3842370",
    "end": "3849779"
  },
  {
    "text": "will always be greater than equal to f of the expectation of x, right?",
    "start": "3849780",
    "end": "3856605"
  },
  {
    "text": "And moreover, if f is strictly convex- [NOISE] strictly convex,",
    "start": "3856605",
    "end": "3868930"
  },
  {
    "text": "then, [NOISE] expectation of",
    "start": "3873290",
    "end": "3879345"
  },
  {
    "text": "f of x equals f if,",
    "start": "3879345",
    "end": "3886605"
  },
  {
    "text": "then- then- [NOISE] if expectation f of x equals f of E of x.",
    "start": "3886605",
    "end": "3897580"
  },
  {
    "text": "Then x equals expectation of x, with probability 1.",
    "start": "3898970",
    "end": "3909660"
  },
  {
    "text": "There's a lot of jargon here, we'll- we'll uh, dissect it in a moment. So to, uh, just to restate it again, Jensen's inequality says,",
    "start": "3909660",
    "end": "3918585"
  },
  {
    "text": "if f is a convex function and x is any random variable, uh, then expectation of f of x is greater than equal to f of expectation of x.",
    "start": "3918585",
    "end": "3929595"
  },
  {
    "text": "And moreover, if f is strictly convex, then this statements holds true.",
    "start": "3929595",
    "end": "3936075"
  },
  {
    "text": "And this whole statement holds true, only if f is strictly convex.",
    "start": "3936075",
    "end": "3941115"
  },
  {
    "text": "Now, if expect- the- the statement is, if f of x equals, uh,",
    "start": "3941115",
    "end": "3947505"
  },
  {
    "text": "expectation of f of x equals f of expectation of x, then it must be the case that x equals expectation of x with probability 1.",
    "start": "3947505",
    "end": "3956070"
  },
  {
    "text": "Which means essentially x is a constant, right? Now, what does this mean?",
    "start": "3956070",
    "end": "3961410"
  },
  {
    "text": "Um, to- to, kind of, understand this [NOISE] more intuitively, uh, this- this picture can help.",
    "start": "3961410",
    "end": "3967530"
  },
  {
    "text": "[NOISE]",
    "start": "3967530",
    "end": "3978900"
  },
  {
    "text": "Right, let this be some function, um, f of x.",
    "start": "3978900",
    "end": "3984090"
  },
  {
    "text": "[NOISE] Okay? And this is basically x, all right?",
    "start": "3984090",
    "end": "3992089"
  },
  {
    "text": "And I will use a different color here. [NOISE] Let's also assume that x is- has a probability distribution associated with it.",
    "start": "3992090",
    "end": "4004799"
  },
  {
    "text": "Right? So the green dotted line represents the probability density of- of the random variable x,",
    "start": "4007060",
    "end": "4014734"
  },
  {
    "text": "and f is some function of x. Right? Now, expectation of x or E,",
    "start": "4014735",
    "end": "4024170"
  },
  {
    "text": "ah, would be somewhere here. So let's call this [NOISE] expectation of x.",
    "start": "4024170",
    "end": "4031910"
  },
  {
    "text": "Right? So expectation of x is- is, um, so think of it as like the mu if this is a Gaussian, right?",
    "start": "4031910",
    "end": "4038494"
  },
  {
    "text": "That's the expectation of a random variable. All right? And now, um, f of expectation of x.",
    "start": "4038495",
    "end": "4046055"
  },
  {
    "text": "So, um, in- in- in the case where x,",
    "start": "4046055",
    "end": "4051470"
  },
  {
    "text": "um, let's assume x takes only two possible values. So let's assume, um,",
    "start": "4051470",
    "end": "4059790"
  },
  {
    "text": "let's draw another picture here. So this is x,",
    "start": "4059830",
    "end": "4068960"
  },
  {
    "text": "this is f of x. Let's assume x takes only two possible values.",
    "start": "4068960",
    "end": "4074720"
  },
  {
    "text": "Let's assume it's a discrete distribution here with- we- we- we, uh, here x was continuous, but to understand Jensen's inequality,",
    "start": "4074720",
    "end": "4081110"
  },
  {
    "text": "let's, uh, assume, uh, a discrete, uh, setting, where x takes only two possible values.",
    "start": "4081110",
    "end": "4088170"
  },
  {
    "text": "This value and this value. Maybe they are, you know, 0, 1, 2, 3, 4, and- and 10.",
    "start": "4089470",
    "end": "4096694"
  },
  {
    "text": "Let's assume take- x takes any one of these values. And the mean of x, if it takes, uh- uh,",
    "start": "4096695",
    "end": "4104185"
  },
  {
    "text": "with probability half the value 1, and its probability half the value 10, then expectation of x will be 5.5?",
    "start": "4104185",
    "end": "4113884"
  },
  {
    "text": "All right? So expectation of x equals 5.5,",
    "start": "4113885",
    "end": "4120119"
  },
  {
    "text": "is 5.5? Yeah, 5.5. And this over here",
    "start": "4120120",
    "end": "4129829"
  },
  {
    "text": "is f of expectation of x.",
    "start": "4129830",
    "end": "4135515"
  },
  {
    "text": "Okay? Does that make sense? Does the expectation of x, you evaluate f at expectation of x and you get f of expectation of x.",
    "start": "4135515",
    "end": "4142159"
  },
  {
    "text": "That's the, uh, right hand side. Right? Now similarly, um, with probability half,",
    "start": "4142160",
    "end": "4148640"
  },
  {
    "text": "f of x can take this value, and with probability half, another half f of x takes this value.",
    "start": "4148640",
    "end": "4155850"
  },
  {
    "text": "Right? So this is- let's call this a and b.",
    "start": "4156090",
    "end": "4162574"
  },
  {
    "text": "So this is f of a, and this is f of b.",
    "start": "4162575",
    "end": "4172190"
  },
  {
    "text": "Right? And expectation of f of a, f of b, is basically the midpoint between f of a and f of b. All right?",
    "start": "4175870",
    "end": "4185509"
  },
  {
    "text": "Because with the probability half it takes this value, with the probability half it takes this value, so the expectation of- of f of x is this one.",
    "start": "4185510",
    "end": "4196985"
  },
  {
    "text": "Right? And this is, expectation of f of x.",
    "start": "4196985",
    "end": "4207905"
  },
  {
    "text": "Right? And it- it so happens that this point will always be the midpoint of- will",
    "start": "4207905",
    "end": "4219710"
  },
  {
    "text": "always be the midpoint of the cord connecting f of a and f of b. Right? And what Jensen's inequality is telling us is that this point,",
    "start": "4219710",
    "end": "4230465"
  },
  {
    "text": "the point that- that's the- the midpoint of the cord connecting two points on f,",
    "start": "4230465",
    "end": "4235639"
  },
  {
    "text": "will always be higher than this point.",
    "start": "4235640",
    "end": "4241085"
  },
  {
    "text": "Right? So f of expectation of x is always less than the expectation of f of x.",
    "start": "4241085",
    "end": "4250260"
  },
  {
    "text": "Okay? Is this clear? Can you raise your hand if you understood this?",
    "start": "4251320",
    "end": "4259820"
  },
  {
    "text": "Some of you have not. Okay, can anybody tell me what-",
    "start": "4259820",
    "end": "4265145"
  },
  {
    "text": "what's- what's- what's still confusing here? I can just go over it again. So f is a convex function,",
    "start": "4265145",
    "end": "4271520"
  },
  {
    "text": "which is, kind of, bending upwards. Right? And the x axis is- is- is,",
    "start": "4271520",
    "end": "4278255"
  },
  {
    "text": "uh, denote some random variable. Right? And in this case, just for the purpose of understanding, um, Jensen's inequality,",
    "start": "4278255",
    "end": "4285800"
  },
  {
    "text": "let's assume x is- is- is, uh, takes on two values- one of two values,",
    "start": "4285800",
    "end": "4291034"
  },
  {
    "text": "either 1 or 10, with equal probability. So the expectation of x is therefore 5.5.",
    "start": "4291035",
    "end": "4297290"
  },
  {
    "text": "Right? That's- that's over here. Now, f of 1, you know, let's call it a, um,",
    "start": "4297290",
    "end": "4303365"
  },
  {
    "text": "f of a is- is, um, this point. So this is f of a. So the height from the x axis to this point, this is f of a.",
    "start": "4303365",
    "end": "4311615"
  },
  {
    "text": "And similarly, if this is b, this is f of b. And the expectation of f of x is therefore the midpoint connecting these two points.",
    "start": "4311615",
    "end": "4320630"
  },
  {
    "text": "Right? And that comes over here. And the expe- f of expectation of x.",
    "start": "4320630",
    "end": "4327335"
  },
  {
    "text": "Where expectation of x was- was, uh, 5.5, is this point, comes over here.",
    "start": "4327335",
    "end": "4334055"
  },
  {
    "text": "Right? And Jensen's inequality is- is therefore essentially saying that the cord connecting any two points of a convex function,",
    "start": "4334055",
    "end": "4342739"
  },
  {
    "text": "always lies above the cord itself.",
    "start": "4342740",
    "end": "4345810"
  },
  {
    "text": "Right? The expectation, uh, expectation of f of x is- is higher than f of expectation of x.",
    "start": "4348280",
    "end": "4357090"
  },
  {
    "text": "Okay? Kind of, under- understood. All right? Okay? Let's- let's, uh, move on.",
    "start": "4358570",
    "end": "4366425"
  },
  {
    "text": "And it also tells us that if f is strictly convex, right? F is strictly convex, then,",
    "start": "4366425",
    "end": "4373310"
  },
  {
    "text": "um, f of x equals, ah, if we do f is strictly convex,",
    "start": "4373310",
    "end": "4378680"
  },
  {
    "text": "and if expectation of f of x equals f of expectation of x, then x equals the expectation of x, uh- uh, itself.",
    "start": "4378680",
    "end": "4386330"
  },
  {
    "text": "Which means x is essentially, uh, a constant. What does that mean? So here's an example of f of x that is strictly convex.",
    "start": "4386330",
    "end": "4396320"
  },
  {
    "text": "Right? Now, um, if expectation of f of x equals f of expectation of x,",
    "start": "4396320",
    "end": "4405579"
  },
  {
    "text": "when can that be possible? Right? Expectation of f of x equals,",
    "start": "4405580",
    "end": "4411910"
  },
  {
    "text": "uh- uh, f of expectation of x? When can, um, let's assume,",
    "start": "4411910",
    "end": "4419165"
  },
  {
    "text": "you know, a and b are here. Right? This is expectation of f of x.",
    "start": "4419165",
    "end": "4427595"
  },
  {
    "text": "And let's say this is f of expectation of x, right? If f is strictly convex,",
    "start": "4427595",
    "end": "4434900"
  },
  {
    "text": "and if the two are equal, f of x equals expectation of x,",
    "start": "4434900",
    "end": "4440990"
  },
  {
    "text": "then the only way that it's possible, that, you know, f of x equals expectation of f, uh, f of x,",
    "start": "4440990",
    "end": "4448130"
  },
  {
    "text": "is if [NOISE] x has a probability density.",
    "start": "4448130",
    "end": "4455900"
  },
  {
    "text": "[NOISE] Okay?",
    "start": "4455900",
    "end": "4463025"
  },
  {
    "text": "Now over here, in this dotted line, essentially, I'm drawing, uh- uh,",
    "start": "4463025",
    "end": "4468425"
  },
  {
    "text": "the probability density of x, which is like a direct Delta function where it has, all its mass concentrated at just one point.",
    "start": "4468425",
    "end": "4476150"
  },
  {
    "text": "Right? And in this case, this is expectation of x and f of expectation of x is here.",
    "start": "4476150",
    "end": "4483290"
  },
  {
    "text": "And also, because x always takes on this value with probability 1,",
    "start": "4483290",
    "end": "4488660"
  },
  {
    "text": "f of x also always takes on this proba- this, uh, um, uh, value with probability 1.",
    "start": "4488660",
    "end": "4494810"
  },
  {
    "text": "And therefore, f of x equals, um, f of expectation of x equals expectation of f of x.",
    "start": "4494810",
    "end": "4500869"
  },
  {
    "text": "Because essentially, you know, the equivalent of the cord connecting two points, has length 0 here, right?",
    "start": "4500870",
    "end": "4507785"
  },
  {
    "text": "All the values of f of x are always here, and x always evaluates to the same value. Yes question?",
    "start": "4507785",
    "end": "4515060"
  },
  {
    "text": "[inaudible] When x is",
    "start": "4515060",
    "end": "4520775"
  },
  {
    "text": "continuous- continuous [inaudible] what is the expectation of [inaudible]",
    "start": "4520775",
    "end": "4526730"
  },
  {
    "text": "So what's the, ah, expectation of, ah, a continuous random variable, if x is a continuous random variable and it has a PDF,",
    "start": "4526730",
    "end": "4535594"
  },
  {
    "text": "let's call it, um, small p of x. Right? Then, expectation of x is equal to the integral of x times p of x dx.",
    "start": "4535595",
    "end": "4548775"
  },
  {
    "text": "So you have p of x in between f of x? So p, in this case is some probability.",
    "start": "4548775",
    "end": "4554570"
  },
  {
    "text": "So the green line- the green doted line is p of x here. My question is what is e of f of x.",
    "start": "4554570",
    "end": "4560855"
  },
  {
    "text": "So what's e of f of x? So e of f of x- so if e",
    "start": "4560855",
    "end": "4568264"
  },
  {
    "text": "of f of x equal to the integral of f of x,",
    "start": "4568265",
    "end": "4575630"
  },
  {
    "text": "p of x dx. Good question. Okay? So this is- this is Jensen's inequality.",
    "start": "4575630",
    "end": "4588740"
  },
  {
    "text": "And- and- and the reason why we require f to be strictly convex",
    "start": "4588740",
    "end": "4593795"
  },
  {
    "text": "is because if f were not strictly convex then,",
    "start": "4593795",
    "end": "4598880"
  },
  {
    "text": "you could have a case where x- x is- f of x is flat in someplace.",
    "start": "4598880",
    "end": "4604895"
  },
  {
    "text": "And x has this density",
    "start": "4604895",
    "end": "4610835"
  },
  {
    "text": "and expectation of f of x would be here and f of expectation would be here.",
    "start": "4610835",
    "end": "4617975"
  },
  {
    "text": "And also f- expectation of f of x would also be the average f of x in this region,",
    "start": "4617975",
    "end": "4626510"
  },
  {
    "text": "which is also a constant. So expectation of f of x would be equal to f of expectation of x even",
    "start": "4626510",
    "end": "4634370"
  },
  {
    "text": "though x is not constant because f has a flat region somewhere. Yes,  question.",
    "start": "4634370",
    "end": "4640940"
  },
  {
    "text": "[inaudible]",
    "start": "4640940",
    "end": "4648770"
  },
  {
    "text": "So the question is of- in the- in the convex case can we assume that-",
    "start": "4648770",
    "end": "4654200"
  },
  {
    "text": "[inaudible]",
    "start": "4654200",
    "end": "4661100"
  },
  {
    "text": "So for- for- for- I mean for this- for this case, yeah,",
    "start": "4661100",
    "end": "4666200"
  },
  {
    "text": "So if- if- for this to hold without x being a constant for x_2 and f of x should be the same.",
    "start": "4666200",
    "end": "4677135"
  },
  {
    "text": "Then, all of x should be distributed in a region where f is flat. Yeah, that's right.",
    "start": "4677135",
    "end": "4683639"
  },
  {
    "text": "Okay. So what are some examples of convex functions [NOISE]?",
    "start": "4683760",
    "end": "4701920"
  },
  {
    "text": "Anybody, example of a convex function?",
    "start": "4701920",
    "end": "4708630"
  },
  {
    "text": "Y equals x squared. Y equals x squared.",
    "start": "4710680",
    "end": "4714780"
  },
  {
    "text": "Examples for concave function? Minus x squared, y equals minus x squared.",
    "start": "4716920",
    "end": "4723740"
  },
  {
    "text": "Yep- yep, that's good. So examples",
    "start": "4723740",
    "end": "4733025"
  },
  {
    "text": "of convex,",
    "start": "4733025",
    "end": "4739199"
  },
  {
    "text": "concave and strict, yes or no, right?",
    "start": "4739480",
    "end": "4748250"
  },
  {
    "text": "So convex function we saw x squared is convex,",
    "start": "4748250",
    "end": "4753500"
  },
  {
    "text": "minus x squared is, therefore, concave, right? And is this strictly convex?",
    "start": "4753500",
    "end": "4761645"
  },
  {
    "text": "And therefore also strictly concave. Right? Now, another function,",
    "start": "4761645",
    "end": "4768570"
  },
  {
    "text": "y equals- or- or f of x equals- [NOISE] equals mx plus c straight line.",
    "start": "4768910",
    "end": "4782060"
  },
  {
    "text": "It's convex. By definition it is convex. A straight line is convex and it is also concave.",
    "start": "4782060",
    "end": "4789769"
  },
  {
    "text": "[NOISE] But is it strict? No, okay, it's not, right.",
    "start": "4789770",
    "end": "4796250"
  },
  {
    "text": "Now, what about e to the x?",
    "start": "4796250",
    "end": "4801720"
  },
  {
    "text": "Convex? Minus e to the x is therefore concave. Is it strict?",
    "start": "4802900",
    "end": "4810155"
  },
  {
    "text": "Yes. What about log x? No [inaudible]",
    "start": "4810155",
    "end": "4817910"
  },
  {
    "text": "So log x is concave. And therefore minus log x is convex.",
    "start": "4817910",
    "end": "4824525"
  },
  {
    "text": "right? And it is strict. Okay? Cool. Now, how does this-",
    "start": "4824525",
    "end": "4833210"
  },
  {
    "text": "how is this useful for expectation maximization? Yes. Question.",
    "start": "4833210",
    "end": "4839020"
  },
  {
    "text": "[inaudible] Yeah, a straight line is always convex and concave because-",
    "start": "4839020",
    "end": "4846280"
  },
  {
    "text": "[inaudible] Is- is- so f double prime is equal to 0.",
    "start": "4846280",
    "end": "4853640"
  },
  {
    "text": "And the definition of convex is that f double-prime should be greater than or equal to zero. So- and it's equal to 0,",
    "start": "4853640",
    "end": "4860105"
  },
  {
    "text": "so it satisfies greater than or equal to 0. And similarly, for concave it is less than or equal to 0 and it's equal to 0,",
    "start": "4860105",
    "end": "4866570"
  },
  {
    "text": "so it satisfies less than or equal to 0. So- okay? Now- so using Jensen's inequality and with these observations,",
    "start": "4866570",
    "end": "4876665"
  },
  {
    "text": "that expectation of f of x is greater than equal to f of expectation of x and- and so on.",
    "start": "4876665",
    "end": "4885090"
  },
  {
    "text": "We can adapt Jensen's inequality to the concave case. Where basically it says,",
    "start": "4885760",
    "end": "4893220"
  },
  {
    "text": "if f is concave, right?",
    "start": "4893800",
    "end": "4899105"
  },
  {
    "text": "Example f of x equals log x. right?",
    "start": "4899105",
    "end": "4905525"
  },
  {
    "text": "Then, the inequality will switch. So the expectation of log x instead of greater than or equal to,",
    "start": "4905525",
    "end": "4915650"
  },
  {
    "text": "will be less than or equal to log expectation of x.",
    "start": "4915650",
    "end": "4922340"
  },
  {
    "text": "Okay? This is also Jensen's inequality.",
    "start": "4922340",
    "end": "4925889"
  },
  {
    "text": "So now, let's derive the EM algorithm. Right? So in the EM algorithm,",
    "start": "4927700",
    "end": "4937130"
  },
  {
    "text": "our goal is to",
    "start": "4937130",
    "end": "4942389"
  },
  {
    "text": "maximize log p of xi theta,",
    "start": "4942730",
    "end": "4952850"
  },
  {
    "text": "where by theta mean, you know, all the parameters i equals 1 to n. Okay?",
    "start": "4952850",
    "end": "4959075"
  },
  {
    "text": "We want to maximize this. That's the goal. What- what we're trying to achieve, this is our end goal. We want to maximize log p of x.",
    "start": "4959075",
    "end": "4965195"
  },
  {
    "text": "But however, maximizing log p of x can be hard because the z's are unobserved.",
    "start": "4965195",
    "end": "4971585"
  },
  {
    "text": "If z's were observed, this was very easy, but z's are unobserved, so it's hard. That's the case where- that's the setting we're in.",
    "start": "4971585",
    "end": "4979160"
  },
  {
    "text": "And now for- for the derivation,",
    "start": "4979160",
    "end": "4984665"
  },
  {
    "text": "I'm going to assume one example. So I'm just going to write it as log p of x comma theta.",
    "start": "4984665",
    "end": "4993440"
  },
  {
    "text": "And I'm going to leave the summation out. But basically the whole- the entire derivation that we're going to do,",
    "start": "4993440",
    "end": "4998630"
  },
  {
    "text": "you can improve the summation and everything will hold, right? It's just to simplify notation, right? So log p of x,",
    "start": "4998630",
    "end": "5005335"
  },
  {
    "text": "we want to maximize this. That's our goal, right?",
    "start": "5005335",
    "end": "5010405"
  },
  {
    "text": "So the first thing we're gonna do is write log p of x,",
    "start": "5010405",
    "end": "5017949"
  },
  {
    "text": "theta is equal to log of the sum of z,",
    "start": "5017950",
    "end": "5025105"
  },
  {
    "text": "p of x, z theta, right?",
    "start": "5025105",
    "end": "5030489"
  },
  {
    "text": "First, we're going to marginalize out z. Okay? And then, once we do that,",
    "start": "5030490",
    "end": "5037700"
  },
  {
    "text": "we will define an- you know some arbitrary probability distribution called q or z's.",
    "start": "5039090",
    "end": "5047409"
  },
  {
    "text": "And write this as log sum over z,",
    "start": "5047410",
    "end": "5053830"
  },
  {
    "text": "q of z times p of x,",
    "start": "5053830",
    "end": "5062140"
  },
  {
    "text": "z theta divided by q of z.",
    "start": "5062140",
    "end": "5069880"
  },
  {
    "text": "Where q of z is greater than 0 for all z.",
    "start": "5069880",
    "end": "5076995"
  },
  {
    "text": "Some arbitrary probability distribution of z, it could be anything whatsoever,",
    "start": "5076995",
    "end": "5082705"
  },
  {
    "text": "any kind of probability distribution or z such that q of z is greater than 0 everywhere. Yes, question.",
    "start": "5082705",
    "end": "5088720"
  },
  {
    "text": "[inaudible] So-",
    "start": "5088720",
    "end": "5093949"
  },
  {
    "text": "so- so the question is it's, you know why is this a hard problem?",
    "start": "5105380",
    "end": "5110730"
  },
  {
    "text": "[inaudible]",
    "start": "5110730",
    "end": "5118030"
  },
  {
    "text": "So it is- it is hard because we are having a summation over here, right? And in general, when in- in the cases where z is continuous, this would be an integral.",
    "start": "5118030",
    "end": "5126895"
  },
  {
    "text": "Right? And that integral can be, you know, arbitrarily co- complex. Sir, do you mean comp- computationally expensive?",
    "start": "5126895",
    "end": "5132850"
  },
  {
    "text": "It can be computationally expensive, it can be analytically not possible in cases when we want an analytical solution.",
    "start": "5132850",
    "end": "5139255"
  },
  {
    "text": "Okay. Good question. So we come up,",
    "start": "5139255",
    "end": "5147145"
  },
  {
    "text": "you know, q can be any distribution whatsoever, as long as q of z is greater than 0 for all z. And- and now,",
    "start": "5147145",
    "end": "5155665"
  },
  {
    "text": "we can see that this can be written, as log expect.",
    "start": "5155665",
    "end": "5164180"
  },
  {
    "text": "All right, what did I do here? Nothing, basically.",
    "start": "5181560",
    "end": "5186820"
  },
  {
    "text": "So this is the definition of expectation, right? So this is a function of,",
    "start": "5186820",
    "end": "5192655"
  },
  {
    "text": "you know, Z- some function of Z, all right? And um, think of this as the probability,",
    "start": "5192655",
    "end": "5199689"
  },
  {
    "text": "and this is some function, and therefore this is just the expectation. Is this clear?",
    "start": "5199689",
    "end": "5206605"
  },
  {
    "text": "Yeah. Yeah, okay. So this is- this just, um, um, the expectation.",
    "start": "5206605",
    "end": "5212110"
  },
  {
    "text": "And now, we make use of Jensen's Inequality and note that log of the expectation of something is greater",
    "start": "5212110",
    "end": "5218889"
  },
  {
    "text": "than the expectation of the log of the same thing, right? And so this is gonna be greater than or equal to expectation of Z,",
    "start": "5218890",
    "end": "5228535"
  },
  {
    "text": "Q log P of X, Z theta, over Q of Z.",
    "start": "5228535",
    "end": "5240520"
  },
  {
    "text": "Yes question? [BACKGROUND] F here is log, and log is concave. Log is concave, right?",
    "start": "5240520",
    "end": "5250449"
  },
  {
    "text": "And this is our random variable x, right?",
    "start": "5250450",
    "end": "5256810"
  },
  {
    "text": "Any- any- any- any questions on how we apply it? How we went from here to here, this is probably the most crucial step, all good?",
    "start": "5256810",
    "end": "5265750"
  },
  {
    "text": "Okay. And this, what we see here,",
    "start": "5265750",
    "end": "5272710"
  },
  {
    "text": "we will call this- and I'll give it a name. We'll call it ELBO, evidence lower bound.",
    "start": "5272710",
    "end": "5280330"
  },
  {
    "text": "All right, so it's the ELBO of Q",
    "start": "5280330",
    "end": "5286210"
  },
  {
    "text": "and- ELBO of X cubed theta, right?",
    "start": "5286210",
    "end": "5298540"
  },
  {
    "text": "And Jensen's inequality tells us that the ELBO, you know, we just defined to be this term is always less than or equal",
    "start": "5298540",
    "end": "5306520"
  },
  {
    "text": "to our objective that we want to maximize, right?",
    "start": "5306520",
    "end": "5311920"
  },
  {
    "text": "Which- which means now if we find thetas and Q's such that we are maximizing the ELBO,",
    "start": "5311920",
    "end": "5319060"
  },
  {
    "text": "then implicitly for the same values of theta log P of X is also going up.",
    "start": "5319060",
    "end": "5324530"
  },
  {
    "text": "Does it make sense? ELBO is defined to be- i- is by Jensen's inequality,",
    "start": "5325860",
    "end": "5333010"
  },
  {
    "text": "ELBO is always a lower bound for log P of X, our likelihood.",
    "start": "5333010",
    "end": "5338275"
  },
  {
    "text": "Both of them have theta in them, right? Now, if we find values of theta such that we are maximizing the ELBO,",
    "start": "5338275",
    "end": "5347320"
  },
  {
    "text": "then it necessarily means that log P of X at that value of theta is higher.",
    "start": "5347320",
    "end": "5352540"
  },
  {
    "text": "And Jensen's inequality gives us that inequality. Yes, question.",
    "start": "5352540",
    "end": "5362530"
  },
  {
    "text": "[BACKGROUND]",
    "start": "5362530",
    "end": "5372760"
  },
  {
    "text": "We'll- we'll- we will to come to that. All right, so this is the, um,",
    "start": "5372760",
    "end": "5377845"
  },
  {
    "text": "so this is the ELBO and this term ELBO is something you will very commonly encounter if you're reading- reading research papers about you know,",
    "start": "5377845",
    "end": "5386605"
  },
  {
    "text": "generative models or deep generative models, um, you know this is- this is a widely used term and ELBO means, you know,",
    "start": "5386605",
    "end": "5394090"
  },
  {
    "text": "the- the- the lower, the lessor side of the Jensen's inequality of log,",
    "start": "5394090",
    "end": "5400675"
  },
  {
    "text": "uh, log P of X. All right, and our goal is to now,",
    "start": "5400675",
    "end": "5407480"
  },
  {
    "text": "before we go into our goal, let's- let's make a few more observations. Now, log P of X is greater than equal to ELBO at all times.",
    "start": "5408180",
    "end": "5421195"
  },
  {
    "text": "That's, uh, what Jensen's inequality says, but other cases when log P of X is exactly equal to the ELBO, right?",
    "start": "5421195",
    "end": "5429639"
  },
  {
    "text": "Are there cases- are there cases when",
    "start": "5429640",
    "end": "5439315"
  },
  {
    "text": "log P of X theta is equal",
    "start": "5439315",
    "end": "5444820"
  },
  {
    "text": "to ELBO of X and Q theta.",
    "start": "5444820",
    "end": "5451480"
  },
  {
    "text": "[NOISE]",
    "start": "5451480",
    "end": "5462280"
  },
  {
    "text": "And the answer is yes, it is yes, because of the second part of Jensen's inequality that we saw, right?",
    "start": "5462280",
    "end": "5470080"
  },
  {
    "text": "So, Jensen's inequality, uh, we also saw that if F is strictly convex,",
    "start": "5470080",
    "end": "5476425"
  },
  {
    "text": "log X is strictly convex, right? If uh, F is strictly convex,",
    "start": "5476425",
    "end": "5482949"
  },
  {
    "text": "then expectation of F of X- expectation of F of X equals uh,",
    "start": "5482950",
    "end": "5490555"
  },
  {
    "text": "F of expectation of X if and only if X is a constant, right?",
    "start": "5490555",
    "end": "5496920"
  },
  {
    "text": "So this is one side of Jensen's inequality. This is the other side of Jensen's inequality, right?",
    "start": "5496920",
    "end": "5502440"
  },
  {
    "text": "And the two will be equal if and only if the term inside is a constant.",
    "start": "5502440",
    "end": "5512440"
  },
  {
    "text": "That's what uh, Jensen's inequality uh, told us, because log is strictly concave. Yes, question.",
    "start": "5512440",
    "end": "5521830"
  },
  {
    "text": "[BACKGROUND] In this case we want- we want this entire term over here to be a constant.",
    "start": "5521830",
    "end": "5531145"
  },
  {
    "text": "Now, are there cases- so the next uh, question is, are there you know, under what circumstances is this entire term over here always a constant?",
    "start": "5531145",
    "end": "5539425"
  },
  {
    "text": "That's- that's- that's what we're going to answer next.[BACKGROUND] It has to be- it has to be independent of Z,",
    "start": "5539425",
    "end": "5549730"
  },
  {
    "text": "so it is constant with respect to Z. Right? So the question now is,",
    "start": "5549730",
    "end": "5558550"
  },
  {
    "text": "in order to make this inequality an equality,",
    "start": "5558550",
    "end": "5563860"
  },
  {
    "text": "because- because log is- because log is strictly concave,",
    "start": "5563860",
    "end": "5570145"
  },
  {
    "text": "the inequality becomes an equality if and only if P of X,",
    "start": "5570145",
    "end": "5576250"
  },
  {
    "text": "Z theta over Q of",
    "start": "5576250",
    "end": "5581320"
  },
  {
    "text": "Z equals some constant C, all right?",
    "start": "5581320",
    "end": "5588864"
  },
  {
    "text": "And now this implies that P, Q of Z over there,",
    "start": "5588865",
    "end": "5594190"
  },
  {
    "text": "Q of Z is equal to 1 over C times P of X, Z.",
    "start": "5594190",
    "end": "5601690"
  },
  {
    "text": "[NOISE] All right?",
    "start": "5601690",
    "end": "5612350"
  },
  {
    "text": "And we also know that,",
    "start": "5612350",
    "end": "5616210"
  },
  {
    "text": "because this is just a proportionality constant. We can write these as Q of z is proportional to P of x, z.",
    "start": "5618620",
    "end": "5627015"
  },
  {
    "text": "So, Q of z is just proportional to P of ah, x, z.",
    "start": "5627015",
    "end": "5633570"
  },
  {
    "text": "And in order to make this equal to, [NOISE] we just use",
    "start": "5633570",
    "end": "5644370"
  },
  {
    "text": "the- calculate the proportional normalizing constant. That you're summing over z, P of x, z.",
    "start": "5644370",
    "end": "5649889"
  },
  {
    "text": "[NOISE] Right?",
    "start": "5649890",
    "end": "5656220"
  },
  {
    "text": "And this is basically P of x, z theta.",
    "start": "5656220",
    "end": "5661485"
  },
  {
    "text": "Divided by, when you marginalize out z, you just get P of x theta.",
    "start": "5661485",
    "end": "5667020"
  },
  {
    "text": "[NOISE] And this is equal to P of z given z theta. [inaudible]",
    "start": "5667020",
    "end": "5675929"
  },
  {
    "text": "So the question is, why did we normalize it with P of x? Yeah.",
    "start": "5675930",
    "end": "5681260"
  },
  {
    "text": "So, because Q of z is proportional to this. And in order to- and we know that this is a probability ah, distribution.",
    "start": "5681260",
    "end": "5689159"
  },
  {
    "text": "Which means it has to sum up to 1. So this must- the normalizing constant necessarily. Must necessarily be the,",
    "start": "5689160",
    "end": "5695940"
  },
  {
    "text": "uh, uh, sum of -sum of this. Was there another question? Yeah.",
    "start": "5695940",
    "end": "5701250"
  },
  {
    "text": "The denominator is a probability distribution over x and z?",
    "start": "5701250",
    "end": "5707130"
  },
  {
    "text": "So the denominator is this term that is summed over all possible values of z.",
    "start": "5707130",
    "end": "5712360"
  },
  {
    "text": "But if you are summing, [inaudible] over x? So this is a distribution over Z. P of x, z could be anything.",
    "start": "5715340",
    "end": "5722985"
  },
  {
    "text": "X could be continuous, right? Q of z is a distribution over Z that must sum up to 1, right?",
    "start": "5722985",
    "end": "5731550"
  },
  {
    "text": "And that's proportional to ah, ah, p of ah, x, z. And ah, the- the- it is proportional and the corresponding normalizing constant,",
    "start": "5731550",
    "end": "5742275"
  },
  {
    "text": "must necessarily be the- um, um, the-the- the- ah, ah, the sum over the numerators for all possible values of z.",
    "start": "5742275",
    "end": "5750060"
  },
  {
    "text": "Because it must- it must ah, sum up to 1, right? So, when Q of z equals P of z given x,",
    "start": "5750060",
    "end": "5759855"
  },
  {
    "text": "Then Jensen's inequality, will change",
    "start": "5759855",
    "end": "5764880"
  },
  {
    "text": "into an equality, right?",
    "start": "5764880",
    "end": "5770145"
  },
  {
    "text": "Lots of moving parts. yes. So, we started with- we started with log P of x, right?",
    "start": "5770145",
    "end": "5776550"
  },
  {
    "text": "And wrote it out as -as, you know, the sum over z of the joint. You know, and there's, there's nothing fancy going on here, right?",
    "start": "5776550",
    "end": "5783699"
  },
  {
    "text": "We're just marginalizing out the um, um, um, z. And then we multiply and divide it by some arbitrary distribution q.",
    "start": "5783700",
    "end": "5793679"
  },
  {
    "text": "And by multiplying it and dividing it. These, the numerator allowed us to write it in the form of an expectation.",
    "start": "5793680",
    "end": "5801585"
  },
  {
    "text": "All right, and the- and this state in the denominator. And, once we wrote it as an expectation.",
    "start": "5801585",
    "end": "5809130"
  },
  {
    "text": "We had log  in the expectation, right? So initially we started with the log likelihood. And therefore we use the concave version of Jensen's inequality.",
    "start": "5809130",
    "end": "5816855"
  },
  {
    "text": "So this log and this expectation are then swap, right? And we get a greater than equal to expectation of log, right?",
    "start": "5816855",
    "end": "5825945"
  },
  {
    "text": "And once we get these, you know, this is basically Jensen's inequality. The one side and the other side of Jensen's inequality.",
    "start": "5825945",
    "end": "5832589"
  },
  {
    "text": "We call the lower- the lower end of the Jensen's inequality. We just call it ELBO, we just give it a name, right?",
    "start": "5832589",
    "end": "5838995"
  },
  {
    "text": "And then we use the- the uh, the corollary of Jensen's inequality.",
    "start": "5838995",
    "end": "5844095"
  },
  {
    "text": "To look for conditions when this inequality will be exactly equal to, right?",
    "start": "5844095",
    "end": "5849345"
  },
  {
    "text": "And because log- because log is strictly convex.",
    "start": "5849345",
    "end": "5854790"
  },
  {
    "text": "The corollary of Jensen's inequality gives us a condition when this inequality becomes an equality.",
    "start": "5854790",
    "end": "5860970"
  },
  {
    "text": "And the condition is that this term must be a constant. And then in order for this to be a constant,",
    "start": "5860970",
    "end": "5867060"
  },
  {
    "text": "with respect to z. It is necessarily the case that Q of z must be equal",
    "start": "5867060",
    "end": "5872090"
  },
  {
    "text": "to the posterior distribution of z given x, right? Whenever Q equals z given x at that value of beta.",
    "start": "5872090",
    "end": "5880290"
  },
  {
    "text": "Then, Jensen's inequality becomes a strict, ah, becomes an equality, right?",
    "start": "5880290",
    "end": "5885960"
  },
  {
    "text": "[NOISE] And now, given these two- [NOISE] yes question.",
    "start": "5885960",
    "end": "5895020"
  },
  {
    "text": "Can you show once again why q of z equals [inaudible]. So, why is ah,",
    "start": "5895020",
    "end": "5900165"
  },
  {
    "text": "ah, ah, yeah, this, all right so, we want to find the condition that P of x,",
    "start": "5900165",
    "end": "5907935"
  },
  {
    "text": "z divided by Q. This is equal to some constant, right? So now, let it be equal to some unknown constant c, right?",
    "start": "5907935",
    "end": "5915810"
  },
  {
    "text": "So, you can take Q of z over here, right? And once you take it over here, this becomes proportional because of some constant times, -.",
    "start": "5915810",
    "end": "5922659"
  },
  {
    "text": "Right. - right? So, now what we- we also know that, sum over Q um,",
    "start": "5922660",
    "end": "5931005"
  },
  {
    "text": "of z equals 1, right? Which means um, the sum over all of -all of the right hand ah, ah, terms.",
    "start": "5931005",
    "end": "5941400"
  },
  {
    "text": "Must be equal to, C itself,- Yeah, that's C. - right?",
    "start": "5941400",
    "end": "5947130"
  },
  {
    "text": "And we divide it by C and we get, exactly. So Q of z is equal to the posterior of z given x at that value of",
    "start": "5947130",
    "end": "5956100"
  },
  {
    "text": "theta. Thank you, thanks for asking. Um, okay, so, based on this,",
    "start": "5956100",
    "end": "5970575"
  },
  {
    "text": "we- we, we write the ah, [NOISE] the EM algorithm,",
    "start": "5970575",
    "end": "5977114"
  },
  {
    "text": "or the more general form of the EM algorithm. [NOISE] We call it the more general",
    "start": "5977114",
    "end": "5982409"
  },
  {
    "text": "form because throughout this- throughout this ah, setting. We have not assumed any specific form for P of x and z, right?",
    "start": "5982410",
    "end": "5989579"
  },
  {
    "text": "It could be the mixture of Gaussian. If it could be any algorithm. The- this derivation holds for any such latent variable model.",
    "start": "5989580",
    "end": "5997410"
  },
  {
    "text": "[NOISE] All right,",
    "start": "5997410",
    "end": "6005615"
  },
  {
    "text": "so that gives us the ah, the algorithm. So the EM algorithm.",
    "start": "6005615",
    "end": "6012780"
  },
  {
    "text": "The EM algorithm basically tells [NOISE] for- so we have the E step [NOISE] for",
    "start": "6013870",
    "end": "6024245"
  },
  {
    "text": "each i [NOISE] set Qi of Zi,",
    "start": "6024245",
    "end": "6034250"
  },
  {
    "text": "[NOISE] is equal to P of Zi given [NOISE] xi, theta.",
    "start": "6034250",
    "end": "6044000"
  },
  {
    "text": "And the M-step [NOISE] set",
    "start": "6044000",
    "end": "6052170"
  },
  {
    "text": "theta [NOISE] equal to arg max theta of i",
    "start": "6053500",
    "end": "6064280"
  },
  {
    "text": "equal to 1 to n ELBO",
    "start": "6064280",
    "end": "6069635"
  },
  {
    "text": "of xi, Qi, beta",
    "start": "6069635",
    "end": "6076829"
  },
  {
    "text": "Right? So what did we do? We basically, um- we basically get this EM algorithm where",
    "start": "6079130",
    "end": "6092190"
  },
  {
    "text": "the corresponding E-Step is to set Q to be the posterior distribution of P of z given x,",
    "start": "6092190",
    "end": "6100949"
  },
  {
    "text": "and in the M-Step, we set Theta to be equal to the arg max of the EL BO.",
    "start": "6100950",
    "end": "6106635"
  },
  {
    "text": "Now why will this- why will this work? To see why this works, let's see this diagram.",
    "start": "6106635",
    "end": "6113429"
  },
  {
    "text": "[NOISE]",
    "start": "6113430",
    "end": "6126180"
  },
  {
    "text": "Now, let's suppose this is Theta. This is not x, this is theta. [NOISE] Right?",
    "start": "6126180",
    "end": "6131700"
  },
  {
    "text": "And this is- is- is- um, so let's us assume this is our-",
    "start": "6131700",
    "end": "6137940"
  },
  {
    "text": "[NOISE] so this is",
    "start": "6137940",
    "end": "6145255"
  },
  {
    "text": "log P of x Theta, right? As we vary Theta,",
    "start": "6145255",
    "end": "6151220"
  },
  {
    "text": "log P of x gives us different values. This is not the density, there's a likelihood because the x-axis is Theta,",
    "start": "6151220",
    "end": "6157085"
  },
  {
    "text": "and not x, right? And there's a dotted line because we don't know it, right?",
    "start": "6157085",
    "end": "6162345"
  },
  {
    "text": "It- it is hard to calculate. We will have to modularize our x, which may be an- an integral. And what we got from this ELBO is that for any given value of Q,",
    "start": "6162345",
    "end": "6173940"
  },
  {
    "text": "the ELBO of, uh, ELBO of, uh, uh, uh, of x for Q and Theta will always be less than or equal to log P of x.",
    "start": "6173940",
    "end": "6183270"
  },
  {
    "text": "That's what Jensen's inequality, uh, gave us. So Jensen's inequality tells us that, you know, this is- all right?",
    "start": "6183270",
    "end": "6190800"
  },
  {
    "text": "This is, right? This is one possible ELBO x,",
    "start": "6190800",
    "end": "6201150"
  },
  {
    "text": "Q and Theta, and let's consider another ELBO, right?",
    "start": "6201150",
    "end": "6211140"
  },
  {
    "text": "So for example. [NOISE] This is another ELBO,",
    "start": "6211140",
    "end": "6217460"
  },
  {
    "text": "[NOISE] x, Q, Theta, right?",
    "start": "6217460",
    "end": "6225900"
  },
  {
    "text": "So for different choices of Q, we get different lower bounds of P of x, right?",
    "start": "6225900",
    "end": "6234720"
  },
  {
    "text": "And what Jensen's inequality tells us is that, um, or the other corollary tells us is that for a given value of Theta,",
    "start": "6234720",
    "end": "6244395"
  },
  {
    "text": "let's- let's assume this is our randomly she- randomly initialize Theta of -, right?",
    "start": "6244395",
    "end": "6250545"
  },
  {
    "text": "For this value of- of- uh, uh, Theta, if we choose Q to be P of z given z Theta 0,",
    "start": "6250545",
    "end": "6260790"
  },
  {
    "text": "for this choice of Q, let's call it Q_0, right?",
    "start": "6260790",
    "end": "6266310"
  },
  {
    "text": "For this choice, if supposing this is, uh, the EL BO for Q_0,",
    "start": "6266310",
    "end": "6273525"
  },
  {
    "text": "and then it basically tells us that the ELBO value equals log P of z at this value of- of which means,",
    "start": "6273525",
    "end": "6282915"
  },
  {
    "text": "the EL BO touches log P of z at this value of- of- of Theta naught, right?",
    "start": "6282915",
    "end": "6291150"
  },
  {
    "text": "When we are at Theta naught, if we choose the Q of, uh, if we chose to construct an EL BO,",
    "start": "6291150",
    "end": "6296880"
  },
  {
    "text": "using Q as the posterior distribution with that parameter value, then the EL BO will be tight,",
    "start": "6296880",
    "end": "6302250"
  },
  {
    "text": "um, with respect to the invisible goal that we're trying to maximize at that value of Theta.",
    "start": "6302250",
    "end": "6308850"
  },
  {
    "text": "And now, if we maximize in the M-Step, if you choose a new Theta,",
    "start": "6308850",
    "end": "6314430"
  },
  {
    "text": "such that we maximize, [NOISE] okay, if this is Theta 1,",
    "start": "6314430",
    "end": "6324329"
  },
  {
    "text": "that is- that- the- the- the Theta of for the next iteration. And now, we construct yet another EL BO, right?",
    "start": "6324330",
    "end": "6333449"
  },
  {
    "text": "Now the new EL BO will be constructing- constructed using, you know,",
    "start": "6333450",
    "end": "6338865"
  },
  {
    "text": "this Q- Q^1, which uses state, uh, uh, Q^1.",
    "start": "6338865",
    "end": "6346020"
  },
  {
    "text": "Uh, um, then this ELBO will be- [NOISE] will be tight at log P of x at Theta ^1, right?",
    "start": "6346020",
    "end": "6356730"
  },
  {
    "text": "Depending on the, uh, um, the choice of the Q to be equal to the a posterior using the corresponding,",
    "start": "6356730",
    "end": "6364935"
  },
  {
    "text": "uh, Theta value, the EL BO that will get will be tight. At- at, uh, uh, will be touching the- the log P of x.",
    "start": "6364935",
    "end": "6371700"
  },
  {
    "text": "That's- that's what Jensen's inequality is coronally- corollary tells us. Now, if we start at Theta naught maximize the- the EL BO,",
    "start": "6371700",
    "end": "6379710"
  },
  {
    "text": "that is the M-Step, we get the new Theta^1, construct the new EL BO, and maximize that one,",
    "start": "6379710",
    "end": "6387285"
  },
  {
    "text": "and we reach here so this will be Theta^2, and here, we construct yet another EL BO,",
    "start": "6387285",
    "end": "6395745"
  },
  {
    "text": "such that it is tight at- at this value of- of- of Theta^2 and so on.",
    "start": "6395745",
    "end": "6401825"
  },
  {
    "text": "If we repeat this over and over by constructing different lower bounds at each value of Theta,",
    "start": "6401825",
    "end": "6408955"
  },
  {
    "text": "and we are maximizing the lower bound, and so on, then we would eventually reach a local optima where the algorithm converges which means,",
    "start": "6408955",
    "end": "6419775"
  },
  {
    "text": "you know, Theta stops changing. And that's essentially what the EM algorithm uh, um, does.",
    "start": "6419775",
    "end": "6426840"
  },
  {
    "text": "So this is the, uh, the rough intuition of- or- or the- or the visual in, uh, intuition of how the EM algorithm works.",
    "start": "6426840",
    "end": "6433665"
  },
  {
    "text": "And in the next class, we will, uh, we will go through a proof to show that it actually converges, you know,",
    "start": "6433665",
    "end": "6439080"
  },
  {
    "text": "more than just, you know, just simply drawing some pictures. Yes, questions? How do you, you know,",
    "start": "6439080",
    "end": "6447570"
  },
  {
    "text": "compute the EL BO P of x if you just take P of x [inaudible] Yes, exactly. So how do we comp- compute the ELBO?",
    "start": "6447570",
    "end": "6453690"
  },
  {
    "text": "The ELBO is- is, uh, exactly this. Um, exactly. Yeah.",
    "start": "6453690",
    "end": "6459224"
  },
  {
    "text": "But you don't have a [inaudible] So in- in the next class, we'll see an example of how we apply it to",
    "start": "6459225",
    "end": "6465780"
  },
  {
    "text": "Gaussian mixture models where it- it might be more, uh, clear. But for now, you know, for the purpose of this lecture, uh,",
    "start": "6465780",
    "end": "6472065"
  },
  {
    "text": "it's enough to have this abstract view of how the EM algorithm works in general, and in the next class, we'll apply it to",
    "start": "6472065",
    "end": "6477585"
  },
  {
    "text": "Gaussian mixture models and see exactly how we took the step forward. [NOISE] Is there any other question? Yes, question?",
    "start": "6477585",
    "end": "6487305"
  },
  {
    "text": "Is Theta z equals Theta transpose x? Is Theta, uh, given as z equals Theta transpose x?",
    "start": "6487305",
    "end": "6496170"
  },
  {
    "text": "That's how do we do algorithm? No, Theta- Theta here, this is- is some unknown, um- um- um- parameter of the module.",
    "start": "6496170",
    "end": "6502830"
  },
  {
    "text": "[inaudible]. No, no, that we- we don't make any linearity of course.",
    "start": "6502830",
    "end": "6507400"
  }
]