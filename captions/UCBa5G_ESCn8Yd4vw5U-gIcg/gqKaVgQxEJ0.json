[
  {
    "text": "So hello. Welcome to 229. So we're starting\na block of three",
    "start": "4800",
    "end": "10080"
  },
  {
    "text": "lectures that I get the\nprivilege of spending some time with you and\nkind of walking you through the building\nblocks and basics.",
    "start": "10080",
    "end": "16200"
  },
  {
    "text": "Before I get into the plan\nfor those three lectures, I want to make\nsure we understand a couple of logistics. So I posted something\non Ed that kind of",
    "start": "16200",
    "end": "23340"
  },
  {
    "text": "explained why I was setting\nup lecture in the way I am. You are not obligated\nto read that. But if you're interested,\ngo ahead and read it,",
    "start": "23340",
    "end": "29340"
  },
  {
    "text": "super happy to take feedback\nand discuss any of that. One of the things that I\nliked about the pandemic",
    "start": "29340",
    "end": "34500"
  },
  {
    "text": "was that more people were\nasking questions during class. And I think part of that was\nbecause people were using",
    "start": "34500",
    "end": "41160"
  },
  {
    "text": "the anonymous feature\non Zoom quite a bit, and I wish we still had that. We don't in this class\nfor various reasons.",
    "start": "41160",
    "end": "46980"
  },
  {
    "text": "So what we're\ngoing to do instead is we're going to have\nthis Ed thread that I just set up that says lecture 2,",
    "start": "46980",
    "end": "52200"
  },
  {
    "text": "And feel free to fire\naway questions on there. I may not take all of them. I reserve the\nright to skip them.",
    "start": "53880",
    "end": "59160"
  },
  {
    "text": "TAs may jump in and\nanswer some and I'll try to follow up on\nanything that's there. But it's really helpful to\nme that you ask questions",
    "start": "59160",
    "end": "66120"
  },
  {
    "text": "and happy to talk about\nwhatever you want, really. Maybe relevant to the class\nis helpful, but pretty much",
    "start": "66120",
    "end": "71400"
  },
  {
    "text": "whatever you want. Second thing, there are a couple\nof downloads that I put up",
    "start": "71400",
    "end": "76500"
  },
  {
    "text": "before my lectures. I put up two things. One is a handwritten note of\nwhat I'm going to talk about, which are the same\nnotes that I use.",
    "start": "76500",
    "end": "82080"
  },
  {
    "text": "I modified them a little bit\nand then also a template in case you want to follow along. Again, you don't need\nany of this stuff.",
    "start": "82080",
    "end": "87600"
  },
  {
    "text": "You can just sit,\nwatch it on video, watch it here, ask questions,\ndo whatever you want. But it's just so that you\nknow the material that's there",
    "start": "87600",
    "end": "93540"
  },
  {
    "text": "and that things like data that I\nwant to show you and look real. I can cut and paste that in, and\nyou can have it in front of you",
    "start": "93540",
    "end": "100320"
  },
  {
    "text": "while I go through it, OK? All right, so that's the\nlogistics I will use. I'm going to try\nand use the iPad.",
    "start": "100320",
    "end": "105420"
  },
  {
    "text": "I like using the\nwhiteboard feel. So this is a good compromise\nbecause it slows me down. If I get excited, I'll start\ntalking all kinds of nonsense.",
    "start": "105420",
    "end": "112920"
  },
  {
    "text": "So this will focus me a\nlittle bit more on the class, and you'll see how\nlong I last, all right?",
    "start": "112920",
    "end": "118200"
  },
  {
    "text": "So what we're going to\ndo in this first three sections of the class,\nfirst three lectures is kind of build up increasingly\nsophisticated machine learning",
    "start": "118200",
    "end": "126420"
  },
  {
    "text": "models. And what you're going to see\nis that they are very, very similar to a model that you\nprobably already know and love,",
    "start": "126420",
    "end": "132360"
  },
  {
    "text": "which is linear regression. If you don't know linear\nregression, don't worry. Today's lecture is\neffectively going to be talking about linear\nregression with slightly",
    "start": "132360",
    "end": "139379"
  },
  {
    "text": "fancier notation and some little\nbits around the algorithm, but it's basically just\nfitting a line, OK?",
    "start": "139380",
    "end": "145319"
  },
  {
    "text": "It's really hopefully going to\nbe something that you've seen and you can grab on to. And then what we'll\ndo in the next lecture",
    "start": "145320",
    "end": "150540"
  },
  {
    "text": "is we'll generalize this\nfrom regression, which is the kind of\ntraditional fitting a line to classification, and that'll\nhave a couple of twists.",
    "start": "150540",
    "end": "156360"
  },
  {
    "text": "We choose our notation\na little bit carefully. And what that allows us to\ndo is show that that way, that we're looking\nat classification.",
    "start": "157080",
    "end": "162900"
  },
  {
    "text": "And we'll talk about what\nclassification really is, allows us to do a much\nlarger class of models, which are called these\nexponential family of models.",
    "start": "162900",
    "end": "169860"
  },
  {
    "text": "And they're going to\nkind of rear their head throughout the course. So we're going to see a\nprecise definition that",
    "start": "169860",
    "end": "175020"
  },
  {
    "text": "allows us to have a huge\nnumber of statistical models and kind of treat\nthem in one way. So we don't have to\nunderstand the details",
    "start": "175020",
    "end": "180540"
  },
  {
    "text": "of every little model. We have an abstraction of\nhow to find its parameters, how to do inference on it,\nlet's get a prediction out of it",
    "start": "180540",
    "end": "186300"
  },
  {
    "text": "and kind of understand it and\nunderstand these algorithms. I'll try to highlight for you\nas we go through there which",
    "start": "186300",
    "end": "192180"
  },
  {
    "text": "of these pieces\nactually carry over to what I would call kind of\nmodern and industrial machine learning. Feel free to ask questions.",
    "start": "192180",
    "end": "197640"
  },
  {
    "text": "Effectively, the way we\nsolve these algorithms or we solve these underlying\noptimization problems",
    "start": "197640",
    "end": "203220"
  },
  {
    "text": "is exactly the way\nwe run everything from how images are\ndetected to how search works",
    "start": "203220",
    "end": "209219"
  },
  {
    "text": "in various different\ncorners of it, to natural language\nprocessing to translation. Weirdly enough, this\nabstraction kind of carries over",
    "start": "209220",
    "end": "215520"
  },
  {
    "text": "for all of that. And the underlying workhorse\nalgorithm, which we'll see is called stochastic\ngradient descent. And so we'll try\nand introduce it",
    "start": "215520",
    "end": "221700"
  },
  {
    "text": "in that absolutely\nsimplest setting, OK? And so that's the idea. It's going to be building\nparallel structure for the next kind of three,\nso linear regression,",
    "start": "221700",
    "end": "228720"
  },
  {
    "text": "classification, and\nthen we're going to go through this generalized\nexponential family. And they will have a\nvery parallel structure.",
    "start": "228720",
    "end": "234660"
  },
  {
    "text": "If you go back to the notes,\nyou'll be able to pull out oh, this is the solving part. This is the model part and\nwhat we're going to do there.",
    "start": "234660",
    "end": "239760"
  },
  {
    "text": "All right, then\nTengyu takes over, teaches you a bunch of\nawesome stuff, neural nets, all the rest of\nthat stuff, kernels. Then I come back and\nteach you unsupervised,",
    "start": "240840",
    "end": "247680"
  },
  {
    "text": "and there again, is a\ndifferent structure there. But it's very, very similar and\ngraphical models and the rest",
    "start": "247680",
    "end": "253200"
  },
  {
    "text": "make an appearance there, OK? So today our plan is\nto get through first to some very basic definitions.",
    "start": "253200",
    "end": "259560"
  },
  {
    "text": "We'll be a little\nbit pedantic there. But that doesn't mean you\nshouldn't ask questions, means if you don't\nunderstand something, you should, and I\nhaven't done my job.",
    "start": "260220",
    "end": "266400"
  },
  {
    "text": "So just fire off a question\nin any form you like. Then we're going to talk\nabout linear regression, which",
    "start": "266400",
    "end": "271620"
  },
  {
    "text": "as I said, is fitting\na line, except where we fitting high dimensional\nlines eventually. So we're going to want\nto abstract that away.",
    "start": "271620",
    "end": "276840"
  },
  {
    "text": "We'll talk about batch\nand stochastic gradient descent, which are two\nalgorithms in machine learning",
    "start": "276840",
    "end": "282300"
  },
  {
    "text": "as Tengyu talked about. We're not great\nwith terminology. This algorithm was called\nincremental gradient descent in the '60s.",
    "start": "282300",
    "end": "288240"
  },
  {
    "text": "It's been around forever. Our incremental gradient methods\nactually wasn't-- even it's not even a descent method\nformally, doesn't matter.",
    "start": "288240",
    "end": "295020"
  },
  {
    "text": "The point is these are old\nthings that people have been using for a long time. And weirdly enough, it's\nwhat we use every day.",
    "start": "295020",
    "end": "302280"
  },
  {
    "text": "It's as I said, this is\nlike a workhorse algorithm that you're going to see. And then I'll very briefly\ncover the normal equation",
    "start": "302280",
    "end": "308160"
  },
  {
    "text": "because I think it's a\ncurse on your homeworks and also give you some practice\nwith vector derivatives.",
    "start": "308160",
    "end": "314760"
  },
  {
    "text": "So you do need to know\nthe vector derivatives stuff to make your life\neasier in this class. You'll have to compute,\noccasionally compute a gradient",
    "start": "314760",
    "end": "321600"
  },
  {
    "text": "or computer derivative. And this is a place\nwhere you kind of know what the right\nanswer is, so when you compute these derivatives,\nit's an easy place",
    "start": "321600",
    "end": "327840"
  },
  {
    "text": "to check yourself. But I wouldn't say\nthat normal equations are the most important thing\nyou'll learn in this class.",
    "start": "327840",
    "end": "333120"
  },
  {
    "text": "It's just solid. You should know what they are. It's not hard. OK, all right, great, so let's\ntalk about supervised learning.",
    "start": "333120",
    "end": "341340"
  },
  {
    "text": "All right so this next\nsection as I mentioned, is going to be all\nsupervised learning. And it'll all follow kind of\nthe same general schema, right?",
    "start": "342360",
    "end": "350400"
  },
  {
    "text": "And what I mean is we're going\nto try and have some what we call prediction function.",
    "start": "350400",
    "end": "354960"
  },
  {
    "text": "And basically, all\nthat's going to be is a function h, which we'll\nuse this notation consistently that goes from some set\nx to some set y, OK?",
    "start": "359580",
    "end": "366780"
  },
  {
    "text": "Before defining this\nformula, let me just give you a couple of examples. So one idea is that x could be\nthe set, some set of images,",
    "start": "366780",
    "end": "373020"
  },
  {
    "text": "all right? So we could look at images,\nat a bunch of images, and we could ask does\nit contain a cat, right?",
    "start": "373020",
    "end": "378660"
  },
  {
    "text": "That was actually a very\nimportant machine learning problem at one point in time. People still work on it, right?",
    "start": "378660",
    "end": "383940"
  },
  {
    "text": "What's the object\nthat's in this image? That would be a\nprediction, right? Where your y's\nhere would be a set of labels that say things like\ncat, dog, things like that, OK?",
    "start": "384660",
    "end": "391979"
  },
  {
    "text": "It could also be text, right? So we could look at text here,\nand we could ask questions",
    "start": "393180",
    "end": "398280"
  },
  {
    "text": "that maybe we arguably should\ndo better on in machine learning like is it hate speech, right?",
    "start": "398280",
    "end": "405960"
  },
  {
    "text": "And so we ask here,\nthis x here, these are all examples of data\ntypes that we want to work on.",
    "start": "405960",
    "end": "411479"
  },
  {
    "text": "And these are all labels or y's\nthat we're talking about, OK? Now, we'll look at as Tengyu\nshowed in his lecture,",
    "start": "412560",
    "end": "418620"
  },
  {
    "text": "we'll look at house data. Now historically,\nhouse data has been one of the most common machine\nlearning and statistical tasks.",
    "start": "418620",
    "end": "425400"
  },
  {
    "text": "It's in every stats 101 course. So you may have\nseen this before. I kind of hope you have.",
    "start": "425400",
    "end": "429600"
  },
  {
    "text": "And when we look\nthrough it, I'm going to point out the real data that\nyou can use to try this out",
    "start": "430620",
    "end": "435660"
  },
  {
    "text": "in a competition like Kaggle. There's a Kaggle where you\ncan download house prices from Ames, Iowa and try and\nguess how much they should",
    "start": "435660",
    "end": "442560"
  },
  {
    "text": "sell for, things like that. People actually\nmake money on that, by the way, not\neverybody, sometimes hard if you follow the news, right?",
    "start": "442560",
    "end": "448919"
  },
  {
    "text": "Zillow tried to sell houses and\nestimate them and flip them, and they lost a bunch of money. Blackstone, if you care\nabout private equity,",
    "start": "448920",
    "end": "455340"
  },
  {
    "text": "managed to make money\ndoing that, right? They bought houses,\nand they were able to predict how much they\nwere going to sell them at. So maybe trivial\nas it seems, these",
    "start": "455940",
    "end": "463020"
  },
  {
    "text": "are actually problems\nthat people care about. OK. Anyway, so we need an\nabstraction, so we have this x",
    "start": "463020",
    "end": "468240"
  },
  {
    "text": "and we have this y. We need something else to make\nthis a supervised problem. And we talked\nabout it yesterday.",
    "start": "468240",
    "end": "473220"
  },
  {
    "text": "We're given a training set, OK? So what is the training set?",
    "start": "474120",
    "end": "478320"
  },
  {
    "text": "Well formally, it's just\ngoing to be a set of pairs. This is just\nintroducing notation. You have an x1 and y1.",
    "start": "479700",
    "end": "487380"
  },
  {
    "text": "OK now, comma all\nthe way xn to y.",
    "start": "488100",
    "end": "493980"
  },
  {
    "text": "Now, xi here is\ngoing to live in x. It's some encoding of an image.",
    "start": "497700",
    "end": "503820"
  },
  {
    "text": "Maybe it's the bits\nthat are in the image. That would be a\nreasonable encoding. Maybe it's RGB values\nthat's in there. If it's text, maybe it's\nthe ASCII characters",
    "start": "503820",
    "end": "510480"
  },
  {
    "text": "or Unicode characters\nthat are in there. It's some bag of bit, OK? Now, we're later going to\nabstract this away and almost",
    "start": "510480",
    "end": "517200"
  },
  {
    "text": "always work in a vector space. We'll talk about where those\nvector spaces come from. But that's kind of\nwhere the data actually",
    "start": "517200",
    "end": "523500"
  },
  {
    "text": "lives and y i is going\nto live in some set, and those are going\nto be our labels. Oops. All right, all right, so now\nour do, given that information,",
    "start": "523500",
    "end": "535020"
  },
  {
    "text": "is we have to find\na good h, x OK?",
    "start": "536040",
    "end": "545459"
  },
  {
    "text": "Often, we call it h\nbecause it's a hypothesis. All right now,\nthat notion of good",
    "start": "547800",
    "end": "555420"
  },
  {
    "text": "is going to occupy a\nfair amount of what we worry about over the\nnext couple of lectures. What does it mean\nto be good, right?",
    "start": "555420",
    "end": "560520"
  },
  {
    "text": "In some intuitive\nsense, because I have these examples of x's\nand y's, one reasonable thing I should expect is\nI kind of get them",
    "start": "560520",
    "end": "567120"
  },
  {
    "text": "more often right than\nrandom chance, right? That's kind of a very basic\nidea of what would be good. You show me an image.",
    "start": "567120",
    "end": "572280"
  },
  {
    "text": "It has a cat in it. I get most of the cats right. Now, you've used\nenough machine learning to know we don't get it\nright all the time, right?",
    "start": "572280",
    "end": "578579"
  },
  {
    "text": "And it's still useful. So we'll have\nstatistical notions. We'll try to get it\nright kind of on average.",
    "start": "578580",
    "end": "584040"
  },
  {
    "text": "Now, more advanced things,\nlike just recency bias because Tatsu was\ntalking about it in the class before\non the board,",
    "start": "584040",
    "end": "589860"
  },
  {
    "text": "you could also\nworry about how well you do on some groups\nversus other groups. Some groups, you know you're\npredicting really well on.",
    "start": "589860",
    "end": "596400"
  },
  {
    "text": "But other groups have\nqualities, and you're not predicting as well on that. You could worry about\nthat and say I want to do, my prediction I only\ncare about as being",
    "start": "596400",
    "end": "603900"
  },
  {
    "text": "as well as I do on any one\nof these predefined groups. So you can have multiple\nnotions of good. We're going to stick with\nthe simplest in basic,",
    "start": "603900",
    "end": "610740"
  },
  {
    "text": "which is like how accurate\nam I at the task in this? But this mathematical framework\ncan accommodate all of those.",
    "start": "610740",
    "end": "616440"
  },
  {
    "text": "When you actually write\nit down, the tweaks that I just mentioned to come\nup with those different, what",
    "start": "616440",
    "end": "621960"
  },
  {
    "text": "they're called loss functions\nis really, really kind of straightforward\nmathematically. They'll kind of go\nthrough the same thing.",
    "start": "621960",
    "end": "626580"
  },
  {
    "text": "So all I want you to\ntake away from this is we have a training set. That's what's provided to us. These yi's are going\nto be supervision.",
    "start": "627120",
    "end": "633480"
  },
  {
    "text": "They're in some set. Our goal will be to\nfind a good h among all the possible functions.",
    "start": "633480",
    "end": "638760"
  },
  {
    "text": "And by the way, the class\nof functions from one space to another is enormous, right? So we're going to have to\nrestrict that in some way.",
    "start": "638760",
    "end": "644160"
  },
  {
    "text": "And that's kind of the setup\nfor supervised learning. OK? All right now, this\nhere, we will often",
    "start": "645600",
    "end": "651540"
  },
  {
    "text": "refer to as the training set or\nthe training data that's there. And what we're really interested\nby the way in which is probably",
    "start": "651540",
    "end": "658080"
  },
  {
    "text": "a little bit counterintuitive\nthe first time you hear it is, we're not doing strictly\nmachine learners. We're not doing strictly\nwhat's called interpolation.",
    "start": "658080",
    "end": "664620"
  },
  {
    "text": "We're not just trying to predict\nback on the x and y pairs that we have, we're\ngoing to try and worry",
    "start": "664620",
    "end": "669960"
  },
  {
    "text": "about how well we're going\nto do on a new x and a new y. So why does that make sense? Imagine someone shows\nup with an image.",
    "start": "669960",
    "end": "675360"
  },
  {
    "text": "Odds of that, they just took\nit with their, phone right? My phone is just littered\nwith pictures of my daughters. So if I take a new\npicture of my daughter,",
    "start": "675360",
    "end": "681660"
  },
  {
    "text": "and probably the label should\nbe the same as the last 1,000 pictures I took. But it's going to look a\nlittle different, right.",
    "start": "681660",
    "end": "687420"
  },
  {
    "text": "So when I show that\npicture, I don't care how well I did on the last\npicture that I took of her. I care how well I\ndid on this picture,",
    "start": "687420",
    "end": "693420"
  },
  {
    "text": "right, on those x and y pairs. And that's a little bit weird. And that means that implicitly\nwhat we're going to assume here",
    "start": "693420",
    "end": "698940"
  },
  {
    "text": "is that these x's and y's\nyou should think about is drawn from a large population\nof images that are out there.",
    "start": "698940",
    "end": "704759"
  },
  {
    "text": "And we want to do not-- we're\nsampling some piece of it. And we want to do well\non those images that are going to come in the future.",
    "start": "704760",
    "end": "710279"
  },
  {
    "text": "That's why we think\nabout it as a prediction. So it may not be great to just\nreturn the label of every x",
    "start": "710280",
    "end": "716220"
  },
  {
    "text": "and y we've ever seen, right? We have to in some way\nkind of generalize, is the technical term\nto those new images.",
    "start": "716220",
    "end": "722340"
  },
  {
    "text": "OK? All right, so the\nreason we call this a prediction is we\ncare about new x's",
    "start": "722340",
    "end": "727260"
  },
  {
    "text": "that are not in\nour training set.",
    "start": "727980",
    "end": "729660"
  },
  {
    "text": "Right, now, if you look at\nthat, and you're mathematically minded, you're\nlike, how the heck",
    "start": "734280",
    "end": "739500"
  },
  {
    "text": "do you say anything about that? And hopefully, you\ngot a clue there. If it doesn't make\nsense yet, don't worry. We're going to make some\nassumption like we randomly",
    "start": "739500",
    "end": "746040"
  },
  {
    "text": "sampled from all of\nthe images and how well do I do on another\nrandomly chosen image. OK? That's what we're going to do.",
    "start": "746040",
    "end": "750960"
  },
  {
    "text": "In some way, the\nset you train on, though better be like the\nset that you evaluate on, that you take your predictions\non, or you're out of luck.",
    "start": "751500",
    "end": "758160"
  },
  {
    "text": "If you train your model\non pictures of my daughter and ask to know about\ncars, I don't know how well",
    "start": "758160",
    "end": "763500"
  },
  {
    "text": "it's going to do, right? So there's clearly\nsome link here. Now weirdly enough,\nalthough I say that, one of the big trends\nin machine learning that's",
    "start": "763500",
    "end": "769620"
  },
  {
    "text": "going on right now. And in fact the course that I\nco-taught with Tatsu and Percy last quarter was about\nthese large models that we just trained to predict\nkind of everything that's",
    "start": "769620",
    "end": "776279"
  },
  {
    "text": "on the web. And they seem to do\npretty well on things, so just want to highlight\nthere's a really",
    "start": "776280",
    "end": "782220"
  },
  {
    "text": "strange notion of good. You spend your whole life\ntrying to think what good is if you're a machine learner. OK, a couple more\nthings, as I said,",
    "start": "782220",
    "end": "787920"
  },
  {
    "text": "I'm just going to go off on\ntangents if no one stops me. All right, so if y is discrete,\nthis is just terminology.",
    "start": "787920",
    "end": "793500"
  },
  {
    "text": "So it's a discrete space. We think about this\nas classification. OK, that's the terminology.",
    "start": "796140",
    "end": "801900"
  },
  {
    "text": "You can think the simplest\nversion is yes or no. Does it contain a cat, yes\nor no, binary classification.",
    "start": "801900",
    "end": "806460"
  },
  {
    "text": "You could also have a\nbunch of different classes. Is it a car, a plane, a truck? What model of car is it?",
    "start": "807000",
    "end": "812520"
  },
  {
    "text": "Those are classifications. They are enumerated sets. The other thing\nwhich you're probably familiar with from\ncalculus, and we'll",
    "start": "812520",
    "end": "819480"
  },
  {
    "text": "talk a little bit about today\nis when y is continuous.",
    "start": "819480",
    "end": "821459"
  },
  {
    "text": "And this is called\nregression, regression, OK. All right, so this is\nan example of something",
    "start": "824640",
    "end": "832140"
  },
  {
    "text": "that's discrete, this cat. And the house\nprice, this is going to be an example of regression.",
    "start": "832140",
    "end": "837240"
  },
  {
    "text": "And that's what we're\ngoing to look at today. In lecture three,\nwe switch, and we start to look at\nclassification, which has some subtle differences.",
    "start": "837240",
    "end": "842340"
  },
  {
    "text": "OK, awesome. All right, let's\nlook at some data. Any questions about the setup or\nkind of higher level questions",
    "start": "843600",
    "end": "852000"
  },
  {
    "text": "about what it is,\nwhat goes on here? All right, sounds good, OK.",
    "start": "852000",
    "end": "860040"
  },
  {
    "text": "so let's look at\nsome real data here. I'll try and get it\nall on the screen. So I'm going to look at\nthis house price data. As I mentioned, this\nis the Ames data set,",
    "start": "860040",
    "end": "866279"
  },
  {
    "text": "which follows a very\nfamous data set just for historical reasons\nof Boston house prices that you can go look\nat and download.",
    "start": "866280",
    "end": "871860"
  },
  {
    "text": "You can download it in one\nline into Pandas if you want, happy to put information\nonline about how to do that.",
    "start": "871860",
    "end": "878040"
  },
  {
    "text": "This is real data of\nreal houses and Ames. And so what I'm showing here\nis these are their real IDs.",
    "start": "878040",
    "end": "884700"
  },
  {
    "text": "I just randomly selected some to\nkind of make the picture pretty just be honest. And then here's their\nsale price, right?",
    "start": "884700",
    "end": "891180"
  },
  {
    "text": "So this is their actual\nsale price in the data. And this is their lot area. This is kind of like some\nnotion of square feet",
    "start": "891180",
    "end": "896399"
  },
  {
    "text": "that's actually present. This data set, I\nthink, has something like 93 columns inside of it.",
    "start": "896400",
    "end": "902819"
  },
  {
    "text": "I've just selected\na small set of them. We'll come back\nto that a second. Now, one of the things that I\ndid here is the first thing you",
    "start": "902820",
    "end": "908760"
  },
  {
    "text": "should do when you're\nencountering a new set of data, and I cannot emphasize\nthis enough, is look at it.",
    "start": "908760",
    "end": "914100"
  },
  {
    "text": "The number of times that\npeople, especially engineers and industry take their data and\nstart running fancy stuff on it I'm like, well, did you look?",
    "start": "914100",
    "end": "919980"
  },
  {
    "text": "I still remember when I was\nrunning a machine learning team at an unnamed large company. And they were like,\nwhy are you sitting in the cafe just\nlabeling data, just",
    "start": "919980",
    "end": "926880"
  },
  {
    "text": "looking at data sets for days. It's like, I don't\nknow what's going on. I want to figure\nout what's actually what people are actually\ndoing on this data set,",
    "start": "926880",
    "end": "932459"
  },
  {
    "text": "and it's really important, OK? So when you're doing your\nprojects, first plot it. So here's a plot, right, x-axis\nsquare feet, y-axis price.",
    "start": "932460",
    "end": "940560"
  },
  {
    "text": "And clearly, there's some\ngeneral upward trajectory trend here. We're going to be more precise\nabout that in the next slide,",
    "start": "940560",
    "end": "946019"
  },
  {
    "text": "right? You get bigger houses,\nthey cost more. Maybe as you can think about\nit, that's not quite true. If it's in a really desirable\nneighborhood, it costs more,",
    "start": "946020",
    "end": "954300"
  },
  {
    "text": "and if it's in a less\ndesirable neighborhood, maybe it cost less. So there are clearly\nother factors, those are going to be\ncalled features in a minute.",
    "start": "954300",
    "end": "959459"
  },
  {
    "text": "But this is our first model, OK? So let's look at\none other feature. So you can also look at the\nnumber of bedrooms, right?",
    "start": "959460",
    "end": "964860"
  },
  {
    "text": "So you see here a plot. These are categorical values. That's why I put them in there. I mean, they're kind of\ncontinuous in some way.",
    "start": "964860",
    "end": "970080"
  },
  {
    "text": "You can still treat them\nas numbers, so that's fine. And you see there's some\nspread among three bedrooms and among four bedrooms, and\nthe price is the y-axis, right?",
    "start": "970080",
    "end": "977096"
  },
  {
    "text": "OK, awesome, all\nright, so what would",
    "start": "977096",
    "end": "982620"
  },
  {
    "text": "we want here, going\nback up for a second, what do we want, actually? We want to get a function. What's our hypothesis go from?",
    "start": "982620",
    "end": "987960"
  },
  {
    "text": "It goes from lot area,\nand it predicts price. That's just notation.",
    "start": "987960",
    "end": "993180"
  },
  {
    "text": "OK? This is what we're after. So you show me this data, and my\ngoal is to produce some h, OK?",
    "start": "993180",
    "end": "1000260"
  },
  {
    "text": "Now, as I talked\nabout, there are lots of functions that\ncan take in a lot of areas and return sale prices.",
    "start": "1000260",
    "end": "1005420"
  },
  {
    "text": "It could scramble it. It could do whatever it wanted. It could go look up\nfrom an oracle, whatever it wanted to do. There are tons and\ntons of functions.",
    "start": "1005420",
    "end": "1011840"
  },
  {
    "text": "We're going to look at a simple\nrestricted class of functions in just a second. But I just want to\nput that in your head.",
    "start": "1011840",
    "end": "1017000"
  },
  {
    "text": "This is actually a\npretty hard problem. So we need some\nrepresentation for h, OK?",
    "start": "1017000",
    "end": "1022040"
  },
  {
    "text": "So how do we represent that h? Now, we're going to look\nat a class of models, which is called linear, although\nif you're a stickler,",
    "start": "1022040",
    "end": "1028280"
  },
  {
    "text": "you'll realize right\naway that they're affine. I'll explain why I allow myself\nto cheat like that in a second. OK, so here's a model\nthat we could use.",
    "start": "1028280",
    "end": "1036500"
  },
  {
    "text": "OK, x1, OK. so the idea here is you give me\nthe variable, right, x1, which",
    "start": "1038120",
    "end": "1046100"
  },
  {
    "text": "in this case would be the square\nfootage of whatever you have. And then I will multiply\nit by some theta.",
    "start": "1046100",
    "end": "1051740"
  },
  {
    "text": "And this theta is going to\nbe a weight, we'll call it, or a parameter of the model. And this is how I'm going\nto form my regression, looks",
    "start": "1051740",
    "end": "1058100"
  },
  {
    "text": "like a line, right? So far, so good, right?",
    "start": "1058100",
    "end": "1063080"
  },
  {
    "text": "Now let me see if I\ncan show you a line. There's a line that does it. OK? That's basically that\nline through the data",
    "start": "1063980",
    "end": "1070820"
  },
  {
    "text": "that we just looked at, OK? Now, I want to actually\ncome one more second.",
    "start": "1070820",
    "end": "1076760"
  },
  {
    "text": "How does this actually\nmap on to this? Oops, scroll down. Sorry for the bad scrolling. Here, I'm going to go to 0.",
    "start": "1076760",
    "end": "1082640"
  },
  {
    "text": "Remember my h is\ngoing to look like x equals theta 0 plus theta 1 x1.",
    "start": "1082640",
    "end": "1089240"
  },
  {
    "text": "Well, what does it\nlook like just so you make sure the picture is clear. This here is theta 0, right? It's where I am.",
    "start": "1089240",
    "end": "1094700"
  },
  {
    "text": "It's the response at 0. And then this gives\nme the slope, right? This is of slope theta 1.",
    "start": "1094700",
    "end": "1100640"
  },
  {
    "text": "And then when I go to\npredict, what do I do? I grab a point. Let's grab this one. I project its value onto the x.",
    "start": "1101420",
    "end": "1107840"
  },
  {
    "text": "And this is where I predict\nits price would be, right? This is the price of this one. Does that make sense?",
    "start": "1107840",
    "end": "1112760"
  },
  {
    "text": "All right, awesome. OK, so this looks like a\nrelatively simple model. But if you look at it at this\nscale, not so bad, honestly,",
    "start": "1113660",
    "end": "1122540"
  },
  {
    "text": "right? There's some kind of\nlinear trend there. There are some errors, or\nwhat we call residuals. In a second we'll try and\nminimize those, these errors.",
    "start": "1122540",
    "end": "1129020"
  },
  {
    "text": "But this is like our first\npredictive model, OK? And as I said, it's something\nthat you're hopefully quite familiar with\njust in fancier notation",
    "start": "1130100",
    "end": "1137720"
  },
  {
    "text": "for the moment. All right, awesome.",
    "start": "1137720",
    "end": "1141800"
  },
  {
    "text": "So now, I'm going to go,\nsorry for the skipping, I'm going to go and say, OK, how\ndo we generalize this, right?",
    "start": "1143780",
    "end": "1149000"
  },
  {
    "text": "So imagine we had our data set. We had x1, x2, so on.",
    "start": "1149000",
    "end": "1155120"
  },
  {
    "text": "And we have a bunch of features. And I'm going to use my\nfeatures from my notes, but hopefully this doesn't\ncause you any panic. I have size.",
    "start": "1155120",
    "end": "1161059"
  },
  {
    "text": "I have bedroom. We have a lot size. And as I mentioned, in\nthe actual real data set,",
    "start": "1161060",
    "end": "1167660"
  },
  {
    "text": "there's like 80, 90 of these\nthings, and I have price. And remember,\nprice is my target.",
    "start": "1167660",
    "end": "1175100"
  },
  {
    "text": "This is my y. And these are my x's. So this is, I'm just\ngoing to put numbers here. Don't worry about them.",
    "start": "1175100",
    "end": "1180800"
  },
  {
    "text": "I don't know why I\nwrote these in my notes, but these are the\nones I used just for the sake of consistency. So write this, 45k, 30k, 400,",
    "start": "1180800",
    "end": "1191600"
  },
  {
    "text": "The thing that I care\nabout is that this is my notation for the first\ndata point and the second data",
    "start": "1194060",
    "end": "1199640"
  },
  {
    "text": "point. And this is x1 1. This is x1 2. This is the second feature, OK?",
    "start": "1199640",
    "end": "1205360"
  },
  {
    "text": "All right, now, I called\nthis a linear model, right? But if you're a stickler, and\nyou took a bunch of things,",
    "start": "1205360",
    "end": "1212480"
  },
  {
    "text": "you're like, no,\nthat's an affine model. You have this theta The way that we get\naround that is we're",
    "start": "1212480",
    "end": "1217700"
  },
  {
    "text": "going to assume that theta\nis 0 for every model, x0 for every model\nis identically 1, OK?",
    "start": "1217700",
    "end": "1222980"
  },
  {
    "text": "So that's just a convention. Don't stub your toe on\nit, that is xi 0 equals 1.",
    "start": "1222980",
    "end": "1229159"
  },
  {
    "text": "And I claim you should\nconvince yourself for one second that\nmeans that what is linear in this\nnew set of features",
    "start": "1230300",
    "end": "1236600"
  },
  {
    "text": "is my old affine models, right? I'm just putting a OK?",
    "start": "1237440",
    "end": "1242480"
  },
  {
    "text": "All right, that allows me to\njust simplify my notation, OK? So what's the model\nthe class of models",
    "start": "1243380",
    "end": "1251240"
  },
  {
    "text": "that I'm looking at here? Well, they're linear models\nagain with that terminology. And they're going to\nlook at theta 0 times",
    "start": "1251240",
    "end": "1257000"
  },
  {
    "text": "x 0, which we know is 1 plus\ntheta 1 times x1 plus dot,",
    "start": "1257000",
    "end": "1263120"
  },
  {
    "text": "dot, dot dot dot theta,\ngoing to call it d times xd. OK, and this equals\nsum j goes from 0",
    "start": "1263120",
    "end": "1270980"
  },
  {
    "text": "to d theta j times\nxj, all right.",
    "start": "1270980",
    "end": "1276500"
  },
  {
    "text": "And remember, I'm just going\nto write it again, x0 equals 1.",
    "start": "1276500",
    "end": "1280280"
  },
  {
    "text": "And nb means know well. All right, now,\nthis allows me, now",
    "start": "1282080",
    "end": "1290420"
  },
  {
    "text": "I have a very high\ndimensional problem. Now, high dimensions don't\nwork like low dimensions. I won't go into a\nwhole thing about it.",
    "start": "1290420",
    "end": "1296720"
  },
  {
    "text": "But high dimensions are very\nfun and interesting spaces. You can build really\ninteresting machine",
    "start": "1296720",
    "end": "1301759"
  },
  {
    "text": "learning models by\ntaking your data, doing what's called embedding\nit and then training a linear model on top. And that actually,\nin some areas,",
    "start": "1301760",
    "end": "1307519"
  },
  {
    "text": "is actually state of the art\nof what we know how to do. So those models have\npotentially hundreds of features that are underneath the covers.",
    "start": "1307520",
    "end": "1313700"
  },
  {
    "text": "For us, these features\nright now are going to be all human interpretable. They're going to\ncome from the table. So when you give me the row\nx1, I fill in this value",
    "start": "1313700",
    "end": "1322100"
  },
  {
    "text": "here with 2104. I fill in this the x2 value\nand so on and as I go.",
    "start": "1322100",
    "end": "1329840"
  },
  {
    "text": "So I just fill in\nthe values as I go. That's how I form my prediction,\na little bit more notation.",
    "start": "1329840",
    "end": "1334880"
  },
  {
    "text": "All right, now, if\nyou don't remember, I'm just going to introduce\nvector notation here. These are column vectors.",
    "start": "1336400",
    "end": "1342560"
  },
  {
    "text": "They're going to look like this. And this just going to\nsave me time and space and fill with things.",
    "start": "1342560",
    "end": "1348080"
  },
  {
    "text": "OK, x1 is going to be a vector",
    "start": "1348080",
    "end": "1352880"
  },
  {
    "text": "Oop, sorry, sorry about that. I wanted to start at 0.",
    "start": "1360020",
    "end": "1365540"
  },
  {
    "text": "x1 0, x1 1 and so on. And remember,  ",
    "start": "1365540",
    "end": "1369320"
  },
  {
    "text": "this thing is 1. We've said many times,\nand this is whatever the value is up there 2104, OK?",
    "start": "1375620",
    "end": "1381380"
  },
  {
    "text": "In general, this is\ngoing to be the size feature, the bedrooms feature,\nand so on, clear enough, right?",
    "start": "1381380",
    "end": "1390500"
  },
  {
    "text": "These are the parameters.",
    "start": "1390500",
    "end": "1392240"
  },
  {
    "text": "And these are the features. Right, so why be so\npedantic about this piece?",
    "start": "1395840",
    "end": "1401539"
  },
  {
    "text": "It's because we're\ngoing to use this in several different guises. These parameters are going\nto mean different things as we change the hypothesis\nfunction over time.",
    "start": "1401540",
    "end": "1408440"
  },
  {
    "text": "And we just want to make\nsure the mapping is clear. So just make sure the\nmapping is super crystal clear in your head of how\nI take a data point that looks like this and map it\ninto a feature vector that",
    "start": "1409820",
    "end": "1417380"
  },
  {
    "text": "looks like that. That's all that I care\nthat you get out of this. And then we have some\ndifferent vectors. And yi is going to be the price\nin our example, his price.",
    "start": "1417380",
    "end": "1426680"
  },
  {
    "text": "Now, recall this\nnotion wasn't, we didn't pick this by accident. This was a training example.",
    "start": "1430700",
    "end": "1436640"
  },
  {
    "text": "This pair, xi yi is a\ntraining example, all right?",
    "start": "1436640",
    "end": "1443120"
  },
  {
    "text": "This is the i-th\ntraining example, just the i-th one in the set.",
    "start": "1443120",
    "end": "1447140"
  },
  {
    "text": "OK. So far so good? Now, I'm going to\ncreate a matrix here",
    "start": "1450680",
    "end": "1455720"
  },
  {
    "text": "capital X that's going to have\none row for every example.",
    "start": "1455720",
    "end": "1463580"
  },
  {
    "text": "So on X, so there are n of\nthose characters in my notation.",
    "start": "1469340",
    "end": "1475820"
  },
  {
    "text": "And so where does\nthis matrix live? Well, they're n\nrows, and I recall because of my convention that I\nadded a extra dimension, which",
    "start": "1478400",
    "end": "1488059"
  },
  {
    "text": "I always made 1, it's d plus 1. And I'm just highlighting\nthis and being pedantic because I don't want\nit to bite you when you realize",
    "start": "1488060",
    "end": "1493820"
  },
  {
    "text": "why they have d plus 1? Where did it come from? It's the 1. And this is someone\nwho said this you've taught this\ncourse many times,",
    "start": "1493820",
    "end": "1499460"
  },
  {
    "text": "someone's going to\nget bitten by it. I'll say it many times, OK? It's uncomfortable\nwhen it happens.",
    "start": "1499460",
    "end": "1504259"
  },
  {
    "text": "So this is now I can think about\nmy training data as a matrix, awesome.",
    "start": "1505340",
    "end": "1509299"
  },
  {
    "text": "OK, so now, we have\na bunch of notation. I have basically bored you to\ndeath with 100 different ways",
    "start": "1511400",
    "end": "1516620"
  },
  {
    "text": "to write down your data set. But I haven't\nanswered the question that we actually\ncared about, which is, how do I find something\nthat's good, right?",
    "start": "1516620",
    "end": "1524059"
  },
  {
    "text": "How do I find an example\nof something that's good? All right, so now\nlet's look at here.",
    "start": "1524060",
    "end": "1530240"
  },
  {
    "text": "So why do we think\nthis line is good? You remember this\nfrom how you fit it. You think it's good because\nit makes small errors, right?",
    "start": "1530240",
    "end": "1537200"
  },
  {
    "text": "If it were all lying on the\nline, right on top of the line, the distance from any point\nto the line would be 0.",
    "start": "1537200",
    "end": "1542360"
  },
  {
    "text": "And we think the\nline was pretty good if we could kind of\nminimize those errors, OK? And this is the error. This is the residual.",
    "start": "1542360",
    "end": "1547640"
  },
  {
    "text": "Now, for computational reasons\nand historical reasons, we'll look at the squares\nof those residuals in just",
    "start": "1547640",
    "end": "1553519"
  },
  {
    "text": "a second. Don't worry too much about that. You can do everything\nI'm telling you with the absolute value\nof the things, right?",
    "start": "1553520",
    "end": "1558980"
  },
  {
    "text": "You don't want to do\nthe signed value of them because what is a negative\nerror mean, right? You should pay a penalty\nis the intuition whenever",
    "start": "1558980",
    "end": "1565400"
  },
  {
    "text": "you make an error, OK? All right, so\nlet's look at this. All right, so we're\ngoing to look at our h.",
    "start": "1565400",
    "end": "1572120"
  },
  {
    "text": "And I'm now going to\nwrite it sub theta. j goes from 0 to d\ntheta j of xj, OK?",
    "start": "1572120",
    "end": "1581420"
  },
  {
    "text": "So now, picking a good\nmodel, I can actually make some sense for. What do I want? Well, I want somehow\nthat h of theta x",
    "start": "1581420",
    "end": "1588980"
  },
  {
    "text": "is approximately equal to the y\nwhen x and y are paired, right? If x and y come\nfrom a new example,",
    "start": "1588980",
    "end": "1594860"
  },
  {
    "text": "you show me a new image. It has a cat or not that\nlabel may be opaque to me, but it exists. I want my prediction to be\nclose to that y on average.",
    "start": "1594860",
    "end": "1602420"
  },
  {
    "text": "Or for house prices,\nyou give me a new house, I predict its price\nas close as possible.",
    "start": "1603020",
    "end": "1608240"
  },
  {
    "text": "I may not get the\nexact dollar, but it should be penalized a lot if\nI'm off by $1 million maybe, but not if I'm\noff by $10, right?",
    "start": "1608240",
    "end": "1615380"
  },
  {
    "text": "That's kind of the\nintuition here, right? So how do I write that down?",
    "start": "1615380",
    "end": "1619220"
  },
  {
    "text": "The idea is I'm going to\nlook at this function, J, which we're going to come to\na couple of different times.",
    "start": "1620900",
    "end": "1624980"
  },
  {
    "text": "And that one half is\njust normalization. I'm going to look at my data.",
    "start": "1626540",
    "end": "1631820"
  },
  {
    "text": "And I'm going to say take my\nprediction on the i-th element, yi and square it, OK?",
    "start": "1632900",
    "end": "1640460"
  },
  {
    "text": "Now, this is our first\nexample of a cost function. And I wrote it in\na really weird way.",
    "start": "1640460",
    "end": "1644960"
  },
  {
    "text": "But I want to come back to\nwhy I'm doing it this way. This is also called\nleast squares. You've probably seen this a\nbunch of times, and that's OK.",
    "start": "1645860",
    "end": "1653480"
  },
  {
    "text": "And if not, don't worry. We'll go through it. There's nothing mysterious, OK? So let's unpack it. So this thing here is\nthe prediction, says,",
    "start": "1653480",
    "end": "1660140"
  },
  {
    "text": "you give me a point xi,\nwhat's my prediction on xi? Some y, and that says it\nshould be close to whatever",
    "start": "1660140",
    "end": "1665900"
  },
  {
    "text": "the training set said yi was. Remember what we're given? We're given xi and yi\npairs that are together.",
    "start": "1665900",
    "end": "1671840"
  },
  {
    "text": "Image, cat, house,\ninformation, all of its description and the\nprice, we should be close, OK?",
    "start": "1671840",
    "end": "1678679"
  },
  {
    "text": "We're penalized more for\nerrors that are far away. I could give you a\nbig song and dance about why this is appropriate. And indeed there are\nlots of statistical song",
    "start": "1679520",
    "end": "1685880"
  },
  {
    "text": "and dances about it. But really, we're\ndoing it because it's easy to compute everything\nthat I'm going to do. You just want something that's\nkind of sensible, right?",
    "start": "1685880",
    "end": "1692180"
  },
  {
    "text": "You should be penalized\nmore the more wrong your guess is, right, roughly\nspeaking in this example.",
    "start": "1692180",
    "end": "1697580"
  },
  {
    "text": "Now, what does it mean\nto pick a good model? Well, our model is\nnow determined solely",
    "start": "1698540",
    "end": "1705380"
  },
  {
    "text": "by those theta j's, right? If we knew the\ntheta j's, our model would be completely determined.",
    "start": "1705380",
    "end": "1710899"
  },
  {
    "text": "That was the trick\nI pulled on you when I said, oh,\nhow are we going to represent our hypothesis? We're going to represent\nit in this class.",
    "start": "1710900",
    "end": "1716000"
  },
  {
    "text": "That means now, we reduced\nfrom all the crazy functions that you could have\never dreamed up, any computer program\nthat you could ever",
    "start": "1716000",
    "end": "1721279"
  },
  {
    "text": "have written that was functional\nto the class of functions that are represented\nby these weights.",
    "start": "1721280",
    "end": "1726560"
  },
  {
    "text": "The wild thing is there's\na lot of functions you can represent that way, OK? And we'll see that over the\ncourse of the class, especially",
    "start": "1727700",
    "end": "1734960"
  },
  {
    "text": "when you start to get really\nhigh dimensions OK, cool. So which one am I going to pick? Yeah, please. So [INAUDIBLE] this least\nsquared cost function,",
    "start": "1734960",
    "end": "1741325"
  },
  {
    "text": "I'm understanding that\nwe're cost function what is important to us. That's the gradient. So why do we need [INAUDIBLE]\nto constantly [INAUDIBLE]",
    "start": "1741326",
    "end": "1754400"
  },
  {
    "text": "Awesome question. Yeah, very advanced question. So the question is hey, you\nwrote this one half there. It seems unnecessarily\nand potentially confusing.",
    "start": "1754400",
    "end": "1761120"
  },
  {
    "text": "Why would you pay\nthe cost to do it? And the reason is when I take\nthe derivative in a minute, it will cancel out and\nmake my life easier.",
    "start": "1761120",
    "end": "1767000"
  },
  {
    "text": "But there's no-- the other\npoint that you made is, and I love the way you said it. This is exactly right. We don't care.",
    "start": "1767000",
    "end": "1773060"
  },
  {
    "text": "I wouldn't call it that we\ncare only about the gradient, but we only care about\nthe minimizer for the loss function. So if your loss function\ncosts 10 or costs 100,",
    "start": "1773060",
    "end": "1780260"
  },
  {
    "text": "doesn't matter. What you care about is\nwhat theta minimizes it, and you got that\nconcept exactly right. So I hope that makes sense.",
    "start": "1780260",
    "end": "1786080"
  },
  {
    "text": "When we're setting up the\ncost function in some ways, sometimes we give\nit an interpretation almost to debug it to\nunderstand what it's doing.",
    "start": "1786080",
    "end": "1792440"
  },
  {
    "text": "But really, all we\ncare about is what is the theta when we minimize\nover all the thetas of j theta,",
    "start": "1792440",
    "end": "1798860"
  },
  {
    "text": "this is what we're\nsolving for, right? So we basically want\nto solve this j theta. Now, as we'll see in a\nsecond, for linear functions,",
    "start": "1798860",
    "end": "1807080"
  },
  {
    "text": "we can do this. For more complicated\nsets of functions, it's not always\nclear that there even exists a minimizer that we\ncan reasonably find, right?",
    "start": "1807080",
    "end": "1816019"
  },
  {
    "text": "So there could be these\nwild functions that take bumps and other things. I'll draw one for\nyou in a minute when we talk about solving it.",
    "start": "1816020",
    "end": "1821720"
  },
  {
    "text": "But for linear\nfunctions, what's amazing and why we teach\nthe normal things, you can prove what h\ntheta is in this example.",
    "start": "1821720",
    "end": "1827960"
  },
  {
    "text": "Wonderful point, OK, but\nthat's the central thing. We're going to\nset up these costs so that we get a model out. We've restricted\nthe class of what",
    "start": "1828920",
    "end": "1836120"
  },
  {
    "text": "we're looking at to something\nthat's relatively small where we can fit the parameters. Then we just have to minimize.",
    "start": "1836120",
    "end": "1841040"
  },
  {
    "text": "OK, awesome, right,\nthis is what I mean by optimization, by the\nway, is solving this equation.",
    "start": "1842720",
    "end": "1849320"
  },
  {
    "text": "I haven't told you how\nwe're going to solve it yet, but hopefully this is good. Now, just for leading a\nlittle bit ahead for and also",
    "start": "1851240",
    "end": "1857600"
  },
  {
    "text": "to kind of stall in\ncase anyone wants to ask a question, what\nwe're eventually going to do is we're going to replace this\nj with increasingly complicated",
    "start": "1857600",
    "end": "1864140"
  },
  {
    "text": "potentially functions that\nwe're going to look at, one for classification, one\nfor other statistical models. But we're going to do\nalmost everything that",
    "start": "1864140",
    "end": "1871159"
  },
  {
    "text": "comes after this part\nto all of those models. So once we kind of get\nit in this form where it's like a prediction\nand some penalty",
    "start": "1871160",
    "end": "1877220"
  },
  {
    "text": "for how poorly\nit's doing, we may use different cost functions. Everything that comes next we'll\nbe able to do for all of them.",
    "start": "1877220",
    "end": "1883820"
  },
  {
    "text": "That's why we set up all this\nkind of elaborate notation for fitting a line, right?",
    "start": "1883820",
    "end": "1889140"
  },
  {
    "text": "It is still, by the way boggles\nmy mind how much machine learning you can do by just\nfitting lines, just higher",
    "start": "1889640",
    "end": "1895280"
  },
  {
    "text": "and higher dimensional lines. But we can talk about\nthat some other time. OK, awesome.",
    "start": "1895280",
    "end": "1899780"
  },
  {
    "text": "All right, so how are\nwe going to solve this? Now, there are many\nways to solve this. If you've taken a\nlinear algebra course,",
    "start": "1902840",
    "end": "1908360"
  },
  {
    "text": "you're like oh, I compute\nthe normal equations, and then I'm done, least\nsquares, or MATLAB or NumPy, and you're like oh, I\ndo least squares solve,",
    "start": "1908360",
    "end": "1915080"
  },
  {
    "text": "or whatever it is, backslash,\nwhatever you want to do. We're going to solve it\nin a way that sets us up for the rest of\nmachine learning.",
    "start": "1915080",
    "end": "1922100"
  },
  {
    "text": "Because machine learning\nwill deal in functions that aren't quite as nice as\nlinear regression quite a bit. And in fact, the trend\nhas been when I first",
    "start": "1922100",
    "end": "1928460"
  },
  {
    "text": "got into machine\nlearning in antiquity, we were all about what are\ncalled convex or bull shaped functions just roughly.",
    "start": "1928460",
    "end": "1933800"
  },
  {
    "text": "And we were really\nobsessed, were we getting the right theta, right? We're like statisticians. At large scale, can we\nget the right theta?",
    "start": "1933800",
    "end": "1939080"
  },
  {
    "text": "Is there one individual theta? Modern machine\nlearning doesn't care. We don't even know if\nwe get the right answer.",
    "start": "1939080",
    "end": "1944420"
  },
  {
    "text": "We don't even know how. There was a paper I was\nreading from DeepMind thisz morning that\nwas like, oh you should run your models longer. No one noticed, right?",
    "start": "1944420",
    "end": "1949880"
  },
  {
    "text": "How do we not know when\nto run the models longer? We don't. That's the world we live in. So how does this work?",
    "start": "1949880",
    "end": "1954920"
  },
  {
    "text": "So imagine to this\nis our cost function. OK? Now just as an aside, I want to\nsay the linear function doesn't",
    "start": "1954920",
    "end": "1961880"
  },
  {
    "text": "look like that. So don't think about. The linear function looks\nnice and bowl shaped, OK? The reason that's important,\nas I was just saying",
    "start": "1961880",
    "end": "1967460"
  },
  {
    "text": "is a local minimum,\nthis is a local minimum. So is this. So is this, roughly speaking,\nis global when you're convex.",
    "start": "1967460",
    "end": "1974540"
  },
  {
    "text": "If that doesn't make sense to\nyou, don't worry about it, OK? For convex, we'll come back to\nthat point later in the course.",
    "start": "1974540",
    "end": "1981020"
  },
  {
    "text": "But I just want\nto say don't think of this function I'm\ndrawing here as what happens with least squares. We're just optimizing\na J for right now, OK?",
    "start": "1981020",
    "end": "1988279"
  },
  {
    "text": "All right, so how are\nwe going to do it? We're going to use a\nvery simple algorithm. We're going to start\nwith a guess, which",
    "start": "1988880",
    "end": "1996140"
  },
  {
    "text": "is going to be theta 0. How did we pick this guess? Felt good, randomly, set it to",
    "start": "1996140",
    "end": "2002380"
  },
  {
    "text": "there are entire\nmachine learning papers, by the way written, I've even\nwritten something which I'm not sure if I should be\nembarrassed or proud of,",
    "start": "2003940",
    "end": "2009399"
  },
  {
    "text": "that talk about\nhow you initialize various different\nparts of the model, OK? For us, though, it won't matter\nfor least squares and some",
    "start": "2009400",
    "end": "2016780"
  },
  {
    "text": "of the other models\nwe're studying because we'll be able to\nget to the right solution. All right, so now\nimagine for the moment,",
    "start": "2016780",
    "end": "2022540"
  },
  {
    "text": "I found you a model. I found you an initial model. Well, it's clearly from looking\naround, imagine I'm just",
    "start": "2022540",
    "end": "2028960"
  },
  {
    "text": "looking, I'm the point. And I'm looking clearly, I,\ncan go down from here, right? So the natural greedy heuristic\nis compute the gradient.",
    "start": "2028960",
    "end": "2035919"
  },
  {
    "text": "What does the gradient\nlook like here? It looks like this. Oops, I can make\nand do this fancier.",
    "start": "2035920",
    "end": "2039580"
  },
  {
    "text": "You see that? Good. I compute the\ngradient, and then I walk downhill, sound\ngood, all right,",
    "start": "2042100",
    "end": "2051460"
  },
  {
    "text": "tells me to go downhill\nfrom here, right? Whatever shape I'm\nat, this gradient will also tell me what to do. Now, there are some\nproblems, right, just as",
    "start": "2051460",
    "end": "2057280"
  },
  {
    "text": "an aside, what if I\nwere right here, oh, doesn't tell me what to do. Don't worry about that. It's a local maximum.",
    "start": "2057280",
    "end": "2062800"
  },
  {
    "text": "I'd be toast. But here, it tells\nme to go downhill. Now, once I go downhill,\nhow far do I go?",
    "start": "2062800",
    "end": "2069580"
  },
  {
    "text": "Again, feels good,\nI pick a value. It's called a step size. So my next value is\ngoing to look like this.",
    "start": "2070300",
    "end": "2077080"
  },
  {
    "text": "t plus 1 is going to be defined\nto be 5t minus sub alpha theta",
    "start": "2077980",
    "end": "2088540"
  },
  {
    "text": "J theta t. Now, my notation is a\nlittle bit weird here. Imagine it's one\ndimensional for the second.",
    "start": "2089500",
    "end": "2094120"
  },
  {
    "text": "OK, compute the gradient, go\nin the opposite direction. That's all that's going on.",
    "start": "2095980",
    "end": "2101860"
  },
  {
    "text": "This thing here is\ncalled a learning rate. Embarrassingly, I think I\nhave won awards for papers",
    "start": "2101860",
    "end": "2107260"
  },
  {
    "text": "that are about learning rates. But they are not very well set. So you just kind\nof pick a value. For deep learning people\nnow have all kinds",
    "start": "2107260",
    "end": "2113680"
  },
  {
    "text": "of what they call\nadaptive optimizers if you look in the\nliterature about how to set these values for you.",
    "start": "2113680",
    "end": "2117580"
  },
  {
    "text": "You don't want to set\nit too big or too small. There is a theory about how\nto do it for linear things, but don't worry. For you, you just\nkind of pick a value.",
    "start": "2118780",
    "end": "2124000"
  },
  {
    "text": "Just imagine what\ncould go wrong? What happens if you\npick it too big? Well then you kind of\nshoot off over here, right?",
    "start": "2124000",
    "end": "2129640"
  },
  {
    "text": "You pick it too small, then you\nmake little bumps like this, right? You don't make enough progress. It's not too hard to think\nabout what should happen here.",
    "start": "2129640",
    "end": "2136599"
  },
  {
    "text": "And then what happens? Well, I get a new point. This is my theta 1. And as suggestively\ndone here, I iterate.",
    "start": "2137440",
    "end": "2144160"
  },
  {
    "text": "I compute the gradient,\nand I bounce down, and then hopefully I get closer. Please. What's the denominator? Oh sorry.",
    "start": "2144160",
    "end": "2151780"
  },
  {
    "text": "That is just-- this is my\nnotation for the gradient with respect to theta.",
    "start": "2151780",
    "end": "2156160"
  },
  {
    "text": "This is a partial derivative\nwith respect to theta. So imagine it's one\ndimensional, and I'm just setting up for the\nfact that I'm going",
    "start": "2157420",
    "end": "2162880"
  },
  {
    "text": "to use multiple dimensions. It's literally just the\ngradient with respect to theta, the\nderivative in this case.",
    "start": "2162880",
    "end": "2168880"
  },
  {
    "text": "Now, what I'll do is I'll\ncompute that J for all 0 to d characters.",
    "start": "2171520",
    "end": "2176859"
  },
  {
    "text": "And that gives me my\nhigh dimensional role. OK.",
    "start": "2178660",
    "end": "2182680"
  },
  {
    "text": "Please. Is this [INAUDIBLE] Yeah, so right now,\nI've just shown it,",
    "start": "2184600",
    "end": "2191440"
  },
  {
    "text": "I've just shown J as\nan abstract function. I haven't decomposed\nit as a sum. That's a great point. Let's come back to that in one\nminute exactly what happens",
    "start": "2191440",
    "end": "2199180"
  },
  {
    "text": "when we have a data point. It's going to be my next line. Other questions?",
    "start": "2199180",
    "end": "2204520"
  },
  {
    "text": "Is it clear? So I did actually a fair\namount of work there and tricked you just\nso you're clear.",
    "start": "2205300",
    "end": "2210940"
  },
  {
    "text": "I went from one dimension\nto d plus 1 dimensions by just changing\nthis the subindex and did them all\nby themselves so",
    "start": "2210940",
    "end": "2217780"
  },
  {
    "text": "make sure that that\nsits OK with you, right? Please. [INAUDIBLE] gradient\nof chasing on graph. Yeah, so how can we\nunderstand it on a graph?",
    "start": "2217780",
    "end": "2231220"
  },
  {
    "text": "What do you mean by on a graph? Like on this graph\nin particular? Awesome. Yeah, yeah. So just imagine the\none-dimensional case",
    "start": "2231220",
    "end": "2238360"
  },
  {
    "text": "carries what you\nneed to deal with. So you're in a\nparticular basis, right? Meaning you have\ntheta1, theta 2.",
    "start": "2238360",
    "end": "2244240"
  },
  {
    "text": "So imagine I'm standing\nin two-dimensional space. I can look down one\naxis, and then I have a one-dimensional function.",
    "start": "2244240",
    "end": "2249520"
  },
  {
    "text": "Then I have a gradient there. That gives me the vector\nin this direction. Then imagine I turn 90\ndegrees orthogonally.",
    "start": "2249520",
    "end": "2254799"
  },
  {
    "text": "I look 90 degrees there. I get another\none-dimensional function. I compute its gradient. Now the gradient is actually,\nif you look at the derivative,",
    "start": "2254800",
    "end": "2261640"
  },
  {
    "text": "it's actually all those\nvectors put together, one after the other in component.",
    "start": "2261640",
    "end": "2265960"
  },
  {
    "text": "But that's exactly right. Yeah. So yeah, but you're asking\nexactly the right questions. So just picture it as\nthe tangent to the curve,",
    "start": "2266860",
    "end": "2273520"
  },
  {
    "text": "if that helps you\nin high dimensions. If not, don't. Yeah. Cool.",
    "start": "2273520",
    "end": "2279819"
  },
  {
    "text": "Wonderful questions. OK, so what do I hope\nthat you understand? Here's some rule. You have the intuition\nthat what it's going to do",
    "start": "2279820",
    "end": "2285580"
  },
  {
    "text": "is it's going to\nbounce slowly downhill. OK. Now if you start to think\nabout high dimensions, and I think this is\nwhy the question came,",
    "start": "2285580",
    "end": "2291220"
  },
  {
    "text": "starts to get a little weird. What does it mean\nin high dimensions? You can imagine something\nthat looks like a saddle. If you know a saddle. Then you're like,\noh gosh, what's",
    "start": "2291220",
    "end": "2297339"
  },
  {
    "text": "going to happen when I get\nto the top of the saddle? Clearly I can go off the sides\nand get a little bit smaller. Right, that would be good.",
    "start": "2297340",
    "end": "2302859"
  },
  {
    "text": "Maybe it goes down and stops. But I can get stuck on the\ntop of the saddle, too. And weirdly enough, it's\ncalled a saddle point.",
    "start": "2302860",
    "end": "2308500"
  },
  {
    "text": "Don't worry. OK? Sound good? All right. We're not worrying\nabout convergence.",
    "start": "2308500",
    "end": "2314080"
  },
  {
    "text": "Right, notice this algorithm\nhas a very clear error mode. Here, we found what looks\nlike the global minimum.",
    "start": "2314080",
    "end": "2319660"
  },
  {
    "text": "But what if we started here? We go bounce, bounce, bounce,\nand we'd find this one.",
    "start": "2319660",
    "end": "2323859"
  },
  {
    "text": "Now, how do you\nstop this algorithm? You stop the algorithm when this\nupdate becomes too small, OK?",
    "start": "2325900",
    "end": "2332019"
  },
  {
    "text": "And you set that tolerance. Maybe you set it to what's\ncalled the machine precision, like 10 to the minus 6, 16.",
    "start": "2332020",
    "end": "2337420"
  },
  {
    "text": "Or you set it to Or you want a quick solution. But the point is no\nmatter what you do,",
    "start": "2337420",
    "end": "2345760"
  },
  {
    "text": "you're going to get stuck\nhere with a descent method. Because it's going to go\ndownhill and get stuck here. And you're going to miss\nthis much better solution.",
    "start": "2345760",
    "end": "2352540"
  },
  {
    "text": "That won't happen for\nlinear regression. We won't talk about why\nat this exact moment. We can prove it in a little bit.",
    "start": "2353620",
    "end": "2358720"
  },
  {
    "text": "But for things that\nare bowl shaped, every local minimum\nis a global minimum.",
    "start": "2358720",
    "end": "2363940"
  },
  {
    "text": "Then we're in good shape. That's why we cared so\nmuch about these things 10, We care about them\noccasionally now.",
    "start": "2363940",
    "end": "2371020"
  },
  {
    "text": "Less than we used to. OK. All right, so let's compute\nsome of those derivatives.",
    "start": "2371800",
    "end": "2379480"
  },
  {
    "text": "Getting back to the earlier\nasked question, which was, hey, what does this mean for a sum? OK.",
    "start": "2379480",
    "end": "2384520"
  },
  {
    "text": "All right. So remember RJ had a\nvery particular form. So we're going to compute the\npartial derivative with respect",
    "start": "2386140",
    "end": "2392140"
  },
  {
    "text": "to some sub j of j theta. OK, so this is the\nderivative here.",
    "start": "2392140",
    "end": "2398200"
  },
  {
    "text": "Whoops. The derivative with respect\nto the j-th component.",
    "start": "2398200",
    "end": "2402460"
  },
  {
    "text": "OK. Now, we take the sum. i goes from 1 to n.",
    "start": "2403840",
    "end": "2408160"
  },
  {
    "text": "I'm going to put the 1/2\ninside, because I can. And then this is linear.",
    "start": "2409060",
    "end": "2415060"
  },
  {
    "text": "And we'll come back to what\nthat means in one second. I just did a little bit\nof work here, not much.",
    "start": "2416140",
    "end": "2421360"
  },
  {
    "text": "I just rewrote the definition\nof j, which is this sum. And then I took the\npartial derivative",
    "start": "2421360",
    "end": "2427480"
  },
  {
    "text": "and I pushed it inside\nbecause it's linear. OK. And we should know that\ngradients are linear. OK.",
    "start": "2427480",
    "end": "2432700"
  },
  {
    "text": "Now, when I do that, I\nget something actually fairly intuitive.",
    "start": "2432700",
    "end": "2436059"
  },
  {
    "text": "And this makes my heart sing.",
    "start": "2438520",
    "end": "2443020"
  },
  {
    "text": "Times partial derivative\ntheta j h theta of x.",
    "start": "2446260",
    "end": "2451420"
  },
  {
    "text": "OK? I canceled the 2 with the about the cooking\nshow preparation.",
    "start": "2451420",
    "end": "2456940"
  },
  {
    "text": "And that is\nstandard, by the way. Now look what I have here,\nwhich is kind of nice.",
    "start": "2456940",
    "end": "2460299"
  },
  {
    "text": "This thing is\nbasically the error. But it's signed. Tells me which way\nI'm making a mistake.",
    "start": "2462820",
    "end": "2467980"
  },
  {
    "text": "Kind of too high\nor too low, right? That's all that thing is. This is the misprediction.",
    "start": "2468820",
    "end": "2472960"
  },
  {
    "text": "Or the error. OK? Now I have the\nderivative with respect",
    "start": "2476140",
    "end": "2481960"
  },
  {
    "text": "to the underlying\nfunction class. Now why did I bother to\nwrite it out this way? Clearly I could have\nskipped a step of doing this and jumped right to the end.",
    "start": "2481960",
    "end": "2488140"
  },
  {
    "text": "But this is going to be general\nfor almost all the models we care about. That's why I did this.",
    "start": "2488140",
    "end": "2493539"
  },
  {
    "text": "OK? So what is it in this\nspecific situation? We'll recall h of theta of x was\nequal to theta0x0 plus theta1x1",
    "start": "2493540",
    "end": "2503380"
  },
  {
    "text": "plus theta2x2 plus-- computing the derivative\nof this is pretty easy.",
    "start": "2503380",
    "end": "2508480"
  },
  {
    "text": "It's just-- oops.",
    "start": "2510820",
    "end": "2514180"
  },
  {
    "text": "Theta j h theta of x is xj.",
    "start": "2517060",
    "end": "2521440"
  },
  {
    "text": "Right? Please. The second line, while you have\nthose scripts on the right. On the right. Superscript over x on the right.",
    "start": "2522820",
    "end": "2533500"
  },
  {
    "text": "On the second line. Here? On the right. Regular.",
    "start": "2533500",
    "end": "2538659"
  },
  {
    "text": "On the right here? Yes. Oh, this should\nhave a superscript. Oh, I'm so sorry.",
    "start": "2538660",
    "end": "2544359"
  },
  {
    "text": "Great catch. This is at that data point. Yeah. Wonderful catch.",
    "start": "2544360",
    "end": "2550120"
  },
  {
    "text": "Thank you. Sir, I seem to generalize\nwhat would cause your hitch. Either would you like\nreporting on the equation",
    "start": "2550120",
    "end": "2555819"
  },
  {
    "text": "or some trigonometry. Could be whatever you want. All I care about is\nthis is the error times the derivative with\nrespect to that underlying",
    "start": "2555820",
    "end": "2562120"
  },
  {
    "text": "model. This is a very basic\nversion of what looks like a chain kind of rule. And we're going to use that\nlike nobody's business.",
    "start": "2562120",
    "end": "2567940"
  },
  {
    "text": "So if you didn't know the\nchain rule before this class, you will definitely\nknow it by the end, because we use it non-stop.",
    "start": "2567940",
    "end": "2573220"
  },
  {
    "text": "But yeah, this is\njust set up for that. That's why it's generalizable. It's the error, which is totally\ngeneralizable for any model",
    "start": "2573220",
    "end": "2578680"
  },
  {
    "text": "that has to do with\nprediction times how you compute the derivative. What's the change of\nthe underlying model? We'll be able to\ngeneralize that.",
    "start": "2578680",
    "end": "2584380"
  },
  {
    "text": "And in this case, it's just xj. All right? So now, right, getting back to\nthis, what is our whole rule?",
    "start": "2584380",
    "end": "2594760"
  },
  {
    "text": "It looks like this. Theta j, theta j t minus\nalpha sum over all the data.",
    "start": "2594760",
    "end": "2604599"
  },
  {
    "text": "Answering the earlier question. At this point,\nwe're doing what's called batch gradient,\nwhich we'll come back to in one second.",
    "start": "2604600",
    "end": "2608860"
  },
  {
    "text": "Minus yi times xi j. Now notice I'm going to try\nand do some highlighting here.",
    "start": "2610240",
    "end": "2617440"
  },
  {
    "text": "I hope this is OK\nfor people to see. And I apologize if\nyou're colorblind and this doesn't\nhelp you too much.",
    "start": "2617440",
    "end": "2623380"
  },
  {
    "text": "But these are the same. OK, hopefully these are\ndistinguishable colors,",
    "start": "2623380",
    "end": "2629380"
  },
  {
    "text": "these j's. And then the i's are the\nother index that's going on. And these are the data\npoints themselves.",
    "start": "2629380",
    "end": "2634420"
  },
  {
    "text": "OK? So I look at every data\npoint, and I'm doing the j-th component of each one. Right?",
    "start": "2635140",
    "end": "2640660"
  },
  {
    "text": "Now by the magic of vector\nnotation, here's what I can do.",
    "start": "2640660",
    "end": "2647619"
  },
  {
    "text": "I just write this as this.",
    "start": "2649120",
    "end": "2651040"
  },
  {
    "text": "h theta xi. This doesn't change.",
    "start": "2655060",
    "end": "2658660"
  },
  {
    "text": "This is a vector equation.",
    "start": "2664660",
    "end": "2665859"
  },
  {
    "text": "OK. OK. So this is basically looping\nover all the j indices at once. OK.",
    "start": "2670120",
    "end": "2675220"
  },
  {
    "text": "If you're unfamiliar\nwith vector notations, one of the reasons\nI'm doing this quickly is I will do it secondhand\nthroughout the course.",
    "start": "2675220",
    "end": "2680799"
  },
  {
    "text": "It's not deep. It's not like it\nrequires a lot of stuff. Just requires a\nlittle bit of reps. Kind of repeat on them.",
    "start": "2680800",
    "end": "2686619"
  },
  {
    "text": "Please. [INAUDIBLE] It's the same rate for\nevery theta [INAUDIBLE]",
    "start": "2686620",
    "end": "2693339"
  },
  {
    "text": "Wonderful question. So alpha u will typically\nset for an iteration, right? When you take a step, typically\nyou can change it across steps.",
    "start": "2693340",
    "end": "2701740"
  },
  {
    "text": "So one thing is here I've\nsaid alpha does not depend on t, the iteration step.",
    "start": "2701740",
    "end": "2707380"
  },
  {
    "text": "But in general, it usually does. You usually decay the\nlearning rate over time. So that's just what's\ndone in practice.",
    "start": "2707380",
    "end": "2712480"
  },
  {
    "text": "And that's done for\nreally good things. What you don't typically\ndo is have alpha depend on the data\npoints itself,",
    "start": "2712480",
    "end": "2717820"
  },
  {
    "text": "because then it's\nalmost functioning like a free parameter, at least\nin classical machine learning. But in both optimizers,\none of which",
    "start": "2717820",
    "end": "2725200"
  },
  {
    "text": "was invented by our own\nJohn Duchi and other folks, you actually do\nchange the alphas for every different\ncoordinate, which",
    "start": "2725200",
    "end": "2731860"
  },
  {
    "text": "was I think his first paper\nwas out of grad and then out of delta. So people do things like that\nthat are a little bit more",
    "start": "2731860",
    "end": "2737200"
  },
  {
    "text": "sophisticated. And why they do those, I'm\nhappy to explain offline. But right now, just think\nof alpha as a constant,",
    "start": "2737200",
    "end": "2743080"
  },
  {
    "text": "like it's small enough\nthat it's not going to lead you too far astray. Like if it were too\nbig, you'd jump too far. And maybe you could do\na little bit better.",
    "start": "2743080",
    "end": "2749500"
  },
  {
    "text": "But maybe not too much. In fact, there's a\nvery basic rule, which is called gradient descent rule,\nis actually very widely used.",
    "start": "2749500",
    "end": "2756880"
  },
  {
    "text": "Very, very widely used. With just one alpha. Wonderful question. And those are the\nright questions to ask. Like, how does this parameter\ndepend on what's around it?",
    "start": "2756880",
    "end": "2764140"
  },
  {
    "text": "Start thinking like that as\nyou go through the course. That's really, really\nhelpful to understand.",
    "start": "2764140",
    "end": "2767080"
  },
  {
    "text": "OK. So far, so good. So at this point, we\nknow how to fit a line. Which doesn't feel like a\nhuge accomplishment maybe,",
    "start": "2770020",
    "end": "2775300"
  },
  {
    "text": "but I think it's pretty cool. And we fit it in this\nobfuscated general way that's going to allow us to\ndo more models I claim,",
    "start": "2775300",
    "end": "2782079"
  },
  {
    "text": "but I'll verify\nthat in two classes. This vector equation\nhere is just showing you like all the\nthings that we computed.",
    "start": "2782080",
    "end": "2788079"
  },
  {
    "text": "This is specific to the earlier\npoint to the line, right? This gradient here is this guy.",
    "start": "2788080",
    "end": "2796060"
  },
  {
    "text": "Those are the same. That's why this\nmodel popped out, OK? Awesome. And we'll come back\nto that in a minute.",
    "start": "2796060",
    "end": "2801099"
  },
  {
    "text": "OK. Now, a topic that is practically\nquite important for machine learning is, and it was\nhinted at earlier, is--",
    "start": "2803680",
    "end": "2811300"
  },
  {
    "text": "and I'll copy this equation--",
    "start": "2811300",
    "end": "2812980"
  },
  {
    "text": "is, what do we do in practice? So one thing that we may\nnot like about this equation",
    "start": "2816880",
    "end": "2822400"
  },
  {
    "text": "is this thing is huge. In modern machine\nlearning, we'll often look at data\nsets that have millions",
    "start": "2822400",
    "end": "2828160"
  },
  {
    "text": "or billions of points, right? Well, it's not uncommon to\nrun models where you're like, every sentence that\nhas been emitted",
    "start": "2828160",
    "end": "2833380"
  },
  {
    "text": "on the web in the last 10\nyears is a training example. Or every token,\nright, every word.",
    "start": "2833380",
    "end": "2838480"
  },
  {
    "text": "And it would be just\nenormous right at that point. It'd just be a huge thing. So even doing one pass over your\ndata is potentially too much.",
    "start": "2839320",
    "end": "2846819"
  },
  {
    "text": "OK, now that's a really extreme\nand crazy version of that. That's a really extreme\nand crazy version of that.",
    "start": "2846820",
    "end": "2852280"
  },
  {
    "text": "But you can also\nimagine situations where you're looking at\nhundreds or thousands of images, and you potentially\nwant to look at fewer.",
    "start": "2852280",
    "end": "2857740"
  },
  {
    "text": "So we'll come to how\nwe do that in a second. Sorry, yeah? Is superscript above\nthe first data set? Oh, it's t and t plus 1.",
    "start": "2857740",
    "end": "2864400"
  },
  {
    "text": "These are the steps. Remember we started at\ntheta 0 superscript.",
    "start": "2865300",
    "end": "2868599"
  },
  {
    "text": "And then we moved\nfrom 1 to 2 to 3 to 4. And so this is just the\nrecursive rule that takes you",
    "start": "2871960",
    "end": "2877960"
  },
  {
    "text": "from theta t to theta t plus 1. So theta t is just whatever\ncurrent theta we're on.",
    "start": "2877960",
    "end": "2883360"
  },
  {
    "text": "Exactly. So you just imagine it as a-- it's a recursive way to\nspecify we're at particular t.",
    "start": "2883360",
    "end": "2890980"
  },
  {
    "text": "And here's how we\nevolve to t plus 1. Exactly right. You got it perfectly. [INAUDIBLE] Exactly right.",
    "start": "2890980",
    "end": "2897700"
  },
  {
    "text": "So theta t, when we go\nback to here-- oops, sorry. I hope that's not dizzying. I wish there were a way to\nskip without making you sick.",
    "start": "2897700",
    "end": "2902860"
  },
  {
    "text": "Is this vector. It's just a particular\ninstantiation of those vectors, one for every\nof the d plus 1 components.",
    "start": "2902860",
    "end": "2908740"
  },
  {
    "text": "Yeah, please. [INAUDIBLE] Yeah. So we will take\nsteps, as I said,",
    "start": "2909300",
    "end": "2915160"
  },
  {
    "text": "until we converge typically. Or we can take a\nfixed number of steps. I'm eliding that because\nfor this particular problem,",
    "start": "2915160",
    "end": "2921400"
  },
  {
    "text": "I can kind of give\nyou a rule of thumb. I can point you at a paper that\ntells you how to set alpha. In general for machine\nlearning, as I was kind of very",
    "start": "2921400",
    "end": "2927700"
  },
  {
    "text": "obliquely referring\nto, we don't actually know how to tell\nthat we've converged. And part of the\nreason is if you knew",
    "start": "2927700",
    "end": "2934600"
  },
  {
    "text": "your model was this\nnice bowl shape, then you can actually\nprove that the closer you get to the optimum, the smaller\nyour gradient is getting.",
    "start": "2934600",
    "end": "2941020"
  },
  {
    "text": "And you can predict kind of how\nfar away you're going to be. For a nice class of functions.",
    "start": "2941020",
    "end": "2946120"
  },
  {
    "text": "For nastier functions\nand the ones that we're going to care\nabout more, you can't do that. So it doesn't make\nsense to say that you",
    "start": "2946120",
    "end": "2952119"
  },
  {
    "text": "found the right answer. And so I don't emphasize that. For these models, I can\ngive you a beautiful story. Happy to type it up\nonline and tell you.",
    "start": "2952120",
    "end": "2958540"
  },
  {
    "text": "But in general for\nmachine learning, honestly we just run\nit till it feels good. Like, oh, the curve stopped.",
    "start": "2958540",
    "end": "2963880"
  },
  {
    "text": "It stopped getting better. And that was this\nDeepMind paper that said, hey, for these\nreally large 280 billion parameter models.",
    "start": "2963880",
    "end": "2969580"
  },
  {
    "text": "So their theta has 280\nbillion parameters in it. They're like, we didn't\nrun it long enough. If we kept running\nit and it was better.",
    "start": "2969580",
    "end": "2975160"
  },
  {
    "text": "And everyone who works\nin machine learning for long enough in\nthe last five years has a situation\nwhere they forgot",
    "start": "2975160",
    "end": "2980440"
  },
  {
    "text": "they were training a model. Hopefully you're not paying for\nit on AWS or GCP or something. And then you come\nback a week later,",
    "start": "2980440",
    "end": "2986680"
  },
  {
    "text": "and it's doing better\nthan you thought. And that is a very\nstrange situation. So I don't have a\ngreat rule for this.",
    "start": "2986680",
    "end": "2992200"
  },
  {
    "text": "For your projects,\nit will be clearer. I'm telling you the\nreal stuff, though. Awesome. Please. This equation [INAUDIBLE]",
    "start": "2992200",
    "end": "2997540"
  },
  {
    "text": "So we will only use it\nin the forward direction",
    "start": "2997540",
    "end": "3004140"
  },
  {
    "text": "of going t to t plus 1. But you could imagine that\nit's reversible if you wanted. [INAUDIBLE] Oh, wonderful question.",
    "start": "3004140",
    "end": "3011160"
  },
  {
    "text": "Yeah, yeah. So in the sense that\nif you shoot past-- let's go back here. So if you're here\nand you shoot past--",
    "start": "3011160",
    "end": "3017640"
  },
  {
    "text": "your step is kind of too\nbig for the gradient, you kind of trust\nit too much, then the next iteration, the gradient\nwill point in this direction,",
    "start": "3017640",
    "end": "3023580"
  },
  {
    "text": "right. And so you'll step back. So it will actually\nhave this ping pong.",
    "start": "3023580",
    "end": "3027480"
  },
  {
    "text": "You actually want\nthat to happen. It turns out the optimal rate-- I mean, I can bore you with this\nfor days-- the optimal rate is",
    "start": "3028740",
    "end": "3034620"
  },
  {
    "text": "actually when you're doing that\nskipping, for whatever reason. Yeah. But it's more intuitive for\npeople to roll down the hill. Yeah. Wonderful point.",
    "start": "3034620",
    "end": "3040080"
  },
  {
    "text": "You got it exactly right. Please. So is it possible for the update\nto be 0 even if h theta of xi",
    "start": "3040080",
    "end": "3047160"
  },
  {
    "text": "is not necessarily\nyi three times? Yeah. So it's not possible for it\nto be exactly 0 everywhere.",
    "start": "3047160",
    "end": "3055260"
  },
  {
    "text": "But it's possible to have\ngradients that are not giving you any information. Yeah, wonderful question. Absolutely wonderful question.",
    "start": "3055260",
    "end": "3060660"
  },
  {
    "text": "And it's because\nit's a linear system. Right, so it's not full rank\nfor the linear algebra nerds. Yeah. Wonderful question.",
    "start": "3060660",
    "end": "3065940"
  },
  {
    "text": "Please. So let's say you have this\nfunctional thing, right. But you flip it so\ntheta 0 is equal to 0,",
    "start": "3065940",
    "end": "3074040"
  },
  {
    "text": "but on the other side. Would you only get\nthe local minimum over there and not the actual--",
    "start": "3074040",
    "end": "3080580"
  },
  {
    "text": "Exactly right. Yeah. And that's what I'm saying. We used to worry about\nthat quite a bit.",
    "start": "3080580",
    "end": "3085920"
  },
  {
    "text": "Now we just say it's good. I wish I could tell you\nsomething better than that. But we'll get into\nwhy that's true. But yeah, when your function\nis in a good class--",
    "start": "3085920",
    "end": "3092880"
  },
  {
    "text": "and good here formally\nmeans convex and bounded in some way-- then\nyou will provably get to the right solution.",
    "start": "3092880",
    "end": "3098520"
  },
  {
    "text": "We'll talk about those\nconditions later. The reason I\nde-emphasize them now is because modern\nmachine learning actually",
    "start": "3098520",
    "end": "3104640"
  },
  {
    "text": "works on functions that\nlook like this, not on the other class of functions. And so that's less\nimportant for students.",
    "start": "3104640",
    "end": "3110820"
  },
  {
    "text": "And then you would rightly say,\nyou told me all this stuff. I memorized all\nthese conditions, and then I got\ninto the workforce. I'm like, none of them\nworked and no one uses them.",
    "start": "3110820",
    "end": "3117300"
  },
  {
    "text": "Like, yeah, that's true. And you're exactly right. And so people worry\nabout initialization. Where do you start\nso that you are",
    "start": "3117300",
    "end": "3124200"
  },
  {
    "text": "guaranteed to get a good model. In fact, there are a couple\nof awesome theory results. I'll take one from my group,\none from Tengyu's, that",
    "start": "3124200",
    "end": "3129660"
  },
  {
    "text": "said for certain class of\nthese nasty non-convex models, if you initialize\nin a particular way, you would be guaranteed\nto get the right answer.",
    "start": "3129660",
    "end": "3136080"
  },
  {
    "text": "Actually, I'll show\nyou one in week 11, a simple version of that. Where if you\ninitialize cleverly, there's not a unique\nanswer, but you'll",
    "start": "3136080",
    "end": "3142140"
  },
  {
    "text": "get the right one every time. Or sorry, class 11, not week 11. Yeah.",
    "start": "3142140",
    "end": "3145920"
  },
  {
    "text": "[INAUDIBLE] Yeah, people will try\nrandom initialization. The problem is the trend is for\nmodels to be really expensive.",
    "start": "3147980",
    "end": "3153960"
  },
  {
    "text": "So you run huge models. So any one run could cost\na couple million dollars. I was looking at\nAmazon's GPT-3 service.",
    "start": "3153960",
    "end": "3161549"
  },
  {
    "text": "I think it costs $6\nmillion a month to run. So do you want to try to\nrun it multiple times?",
    "start": "3161550",
    "end": "3167460"
  },
  {
    "text": "If you got money, go ahead. But you want to try\nand do other tricks. People used to do a lot\nmore random restarting.",
    "start": "3167460",
    "end": "3173280"
  },
  {
    "text": "Now it's really sad to\nsay this is the state, but we've evolved folksonomies.",
    "start": "3173280",
    "end": "3178200"
  },
  {
    "text": "If you train these\nmodels, you know, what are the right parameters and\nwhat is everybody else using?",
    "start": "3178920",
    "end": "3184200"
  },
  {
    "text": "And not everyone\ntries and explores everything, let alone\nhow long you tune it,",
    "start": "3184200",
    "end": "3188580"
  },
  {
    "text": "what optimizers you use. We all use the same stuff. But we don't have great\nformal justification for it.",
    "start": "3189300",
    "end": "3196140"
  },
  {
    "text": "Maybe I'm exposing too much. It's not as bad as it sounds. There actually are\nprinciples in this area. I'm just telling\nyou the plates that",
    "start": "3196140",
    "end": "3202079"
  },
  {
    "text": "are broken, because they're\nmore interesting to me. Yeah. [INAUDIBLE] Oh, wonderful question.",
    "start": "3202080",
    "end": "3211080"
  },
  {
    "text": "We're going to\ncome back to that. So the solution is,\ndo I want to-- there's a phenomenon that\na lot of people",
    "start": "3211080",
    "end": "3216420"
  },
  {
    "text": "know about in machine learning,\nwhich is, if I take my model",
    "start": "3216420",
    "end": "3221460"
  },
  {
    "text": "and I exactly fit\nmy training data, maybe it won't generalize well. It'll fit to some error\nor some noise in the data,",
    "start": "3221460",
    "end": "3227820"
  },
  {
    "text": "and this is roughly overfitting. We cover that in lecture 10. In lecture 10, at least\nwhen I taught it last, I also taught about\nsomething which",
    "start": "3227820",
    "end": "3234180"
  },
  {
    "text": "is in modern machine learning. We realize that actually\nsometimes that concern is overstated for some models. And there's a wonderful\npaper by Misha Belkin that",
    "start": "3234180",
    "end": "3241020"
  },
  {
    "text": "said you can actually\ninterpolate, perfectly fit your data and\noptimally generalize for some classes of models. So that tradeoff isn't as\nclear for modern models",
    "start": "3241020",
    "end": "3248339"
  },
  {
    "text": "as it was for old models. Maybe I should stop telling\nyou about this stuff. But yes, in general,\noverfitting is a problem. You can overfit a model and\nbelieve your training data",
    "start": "3248340",
    "end": "3255960"
  },
  {
    "text": "too much. Yeah. But this area is fascinating. I can obviously rant\nabout it for weeks, so.",
    "start": "3255960",
    "end": "3260640"
  },
  {
    "text": "Wonderful questions. Yeah, yeah. This is absolutely great. OK, so what do I\nwant to tell you?",
    "start": "3261600",
    "end": "3266940"
  },
  {
    "text": "So I don't want to tell\nyou normal equations. I thought that was pretty\nclear from the beginning. So you read about those. If you want, I'll type up notes.",
    "start": "3266940",
    "end": "3272460"
  },
  {
    "text": "Andrew's notes are\ngreat on this point. But I do want to\ntell you this one little bit with my last\ncouple of minutes about batch",
    "start": "3272460",
    "end": "3277740"
  },
  {
    "text": "versus stochastic mini batch. Because it actually is\nrelevant and useful.",
    "start": "3277740",
    "end": "3282600"
  },
  {
    "text": "OK. So when we last left off, we\nwere looking at this equation.",
    "start": "3283620",
    "end": "3290220"
  },
  {
    "text": "And we noticed this problem,\nthat n was really big. And I just hopefully told you,\nn is really big, and so is d. The number of parameters\nis really big.",
    "start": "3290220",
    "end": "3296760"
  },
  {
    "text": "So this is expensive. I wouldn't want to look\nat all of my training data before I took my first\nstep, because probably",
    "start": "3296760",
    "end": "3302640"
  },
  {
    "text": "my initial guess\nis not that good. That's why I'm training a model. If randomly\ninitializing the model, which is something people try\nto do, gave me good predictions,",
    "start": "3302640",
    "end": "3309480"
  },
  {
    "text": "I just use that. So obviously, I want to take\nout as many steps as I can. So here's what I'll do. I'll use mini batches.",
    "start": "3309480",
    "end": "3315599"
  },
  {
    "text": "So what does mini batching do? OK. I won't get too formal. But basically what I'll\ndo is I'll select some B,",
    "start": "3315600",
    "end": "3322140"
  },
  {
    "text": "let's say at random. OK, I'm being vague\nhere what random means. I wrote a bunch of\npapers about this.",
    "start": "3322140",
    "end": "3327900"
  },
  {
    "text": "You can either pick-- randomly select them, or\nyou can shuffle the order.",
    "start": "3327900",
    "end": "3332940"
  },
  {
    "text": "And in conventional\nmachine learning, we shuffle the order for\na variety of reasons. And then I pick B items.",
    "start": "3332940",
    "end": "3339000"
  },
  {
    "text": "So B is going to be\nmuch smaller than n. OK?",
    "start": "3339000",
    "end": "3343200"
  },
  {
    "text": "All right. And then I update.",
    "start": "3344520",
    "end": "3347099"
  },
  {
    "text": "I'm going to call it\nthis, because this made me make it more clear. i equals 1.",
    "start": "3354300",
    "end": "3359220"
  },
  {
    "text": "Or actually, I'm going to write\nit a little bit strangely. I apologize for this notation. Notation's better in my\nnotes, but I want to write it",
    "start": "3359940",
    "end": "3365160"
  },
  {
    "text": "this way, because\nit's easier to say. xi.",
    "start": "3365160",
    "end": "3370200"
  },
  {
    "text": "yi. It makes it more clear\nwhat's going on, I hope. OK, what's going on?",
    "start": "3370200",
    "end": "3375720"
  },
  {
    "text": "So I select a bunch\nof indexes, B. And then I just\ncompute my estimate",
    "start": "3375720",
    "end": "3381119"
  },
  {
    "text": "of the gradient over them. OK.",
    "start": "3381120",
    "end": "3385200"
  },
  {
    "text": "I could even pick\nB to be size 1. Just pick a single point,\nas someone was alluding to earlier, and take a step.",
    "start": "3386220",
    "end": "3391740"
  },
  {
    "text": "Now, what are the\nobvious tradeoffs here? On one hand, if I pick a step,\nthat step is really fast.",
    "start": "3391740",
    "end": "3398160"
  },
  {
    "text": "Right, if I pick\na single element. It's super fast to compute\nrelative to looking at the entire data set. But it's going to be noisy.",
    "start": "3398160",
    "end": "3404279"
  },
  {
    "text": "It's going to have low quality. I may not have\nenough information to step in the\ndirection I want to go. On the other hand, if I\nlook at the whole data set,",
    "start": "3404280",
    "end": "3410700"
  },
  {
    "text": "it's going to be super accurate\nabout what the gradient is. In fact, I'll compute it\nexactly up to numerical issues.",
    "start": "3410700",
    "end": "3415200"
  },
  {
    "text": "But it's super slow. Now, what people do is\nthey tend to pick batches that are on the smaller side.",
    "start": "3415980",
    "end": "3421920"
  },
  {
    "text": "Right, and you pick them\nas big as you can tolerate. And I won't go into the reasons\nfor this underlying hardware. Happy to answer\nquestions about it.",
    "start": "3421920",
    "end": "3428460"
  },
  {
    "text": "But basically you pick\nbatches that are kind of as many as you can get for free. Modern hardware works\nkind of in parallel.",
    "start": "3428460",
    "end": "3434460"
  },
  {
    "text": "So you'll grab and look at-- expensive than looking at one,",
    "start": "3434460",
    "end": "3439920"
  },
  {
    "text": "OK, on a modern platform. Now I'm using these\nnoisy proxies.",
    "start": "3439920",
    "end": "3446040"
  },
  {
    "text": "And you may think, am I\nstill guaranteed to converge? And then the answer\nis effectively yes. And under really,\nreally harsh conditions.",
    "start": "3446040",
    "end": "3455040"
  },
  {
    "text": "In fact, I'm very\nproud of something that my first PhD student and\ncollaborators, that Ben Recht and Steve Wright wrote\nabout, this paper called",
    "start": "3455040",
    "end": "3461700"
  },
  {
    "text": "Hogwild!, which is very stupid,\nhas an exclamation point. But also got a 10 year\nTest of Time Award for saying that\nyou can basically",
    "start": "3461700",
    "end": "3467340"
  },
  {
    "text": "run these things in the\ncraziest possible ways, and they still converge. These stochastic\nsampling regimes.",
    "start": "3467340",
    "end": "3472500"
  },
  {
    "text": "OK, I won't go into\ndetails about that. My point is this thing is\nactually fairly robust.",
    "start": "3472500",
    "end": "3477840"
  },
  {
    "text": "This take a bunch of error\nestimates and step them. And in fact, almost all\nmodern machine learning is geared towards what's\ncalled mini batching.",
    "start": "3477840",
    "end": "3484440"
  },
  {
    "text": "If you download PyTorch\nor JAX or TensorFlow or whatever you're\nusing, odds are",
    "start": "3484440",
    "end": "3489900"
  },
  {
    "text": "it has native support to\ngive you mini batches. OK. And that is basically\njust taking an-- oops, taking an estimate\nof this piece here,",
    "start": "3489900",
    "end": "3497339"
  },
  {
    "text": "and using that noisy estimate. And why might that make sense? Well, imagine your\ndata set contains a bunch of near copies.",
    "start": "3498180",
    "end": "3504180"
  },
  {
    "text": "If your data set\ncontained all copies, then you would just be\nreading the same example and getting no\ninformation, right?",
    "start": "3504180",
    "end": "3510420"
  },
  {
    "text": "If instead you were\nsampling that same example, you would go potentially\nunboundedly faster.",
    "start": "3510420",
    "end": "3515640"
  },
  {
    "text": "And if you think about\nwhat we're looking at, when I told you\nimages, like the images on my phone for\nmy daughter, there",
    "start": "3515640",
    "end": "3520980"
  },
  {
    "text": "are a lot of pictures\nof my daughters. A lot, OK? I'm a regular dad. I take lots of pictures. So that means there's\na lot of density.",
    "start": "3520980",
    "end": "3528060"
  },
  {
    "text": "And so machine learning\noperates in these regimes where you have huge, dense,\nrepeated amounts of data.",
    "start": "3528060",
    "end": "3533100"
  },
  {
    "text": "OK? All right. So this is going to come back. We're going to see\nthis next time.",
    "start": "3533100",
    "end": "3539700"
  },
  {
    "text": "We're going to see\nit in particular when we start to look at various\ndifferent loss functions.",
    "start": "3539700",
    "end": "3546240"
  },
  {
    "text": "We're going to generalize how we\ndo prediction to classification next time. And then to a huge class\nof statistical models",
    "start": "3546240",
    "end": "3551580"
  },
  {
    "text": "called exponential\nfamily models. To go back to the top. I skipped just to make\nsure you know what's here and what I skipped.",
    "start": "3551580",
    "end": "3557100"
  },
  {
    "text": "We went through the\nbasic definitions. We saw how to fit a line. We went through batch\nand stochastic gradient",
    "start": "3557100",
    "end": "3562320"
  },
  {
    "text": "descent of how to solve\nthe underlying model. We set up a bunch of notation. This is going to be one\nof the dryer classes",
    "start": "3562320",
    "end": "3567660"
  },
  {
    "text": "where I'm just writing out\nall the bits of notation. And we saw how to solve them. Those will all carry over\nto our next brand of models.",
    "start": "3567660",
    "end": "3574920"
  },
  {
    "text": "The normal equations, if you\nrun into problems, blame me. I'm happy to take a\nlook through them. They're relatively\nstraightforward",
    "start": "3574920",
    "end": "3580320"
  },
  {
    "text": "and the notes are pretty good. But I'll look at Ed if\nyou run into any problems there, and happy to\nanswer questions. With that, thank\nyou so much time",
    "start": "3580320",
    "end": "3586559"
  },
  {
    "text": "for your time and attention,\nand I hope to see some of you on Monday.",
    "start": "3586560",
    "end": "3588480"
  }
]