[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "So in terms of inherently\ninterpretable models, we started our discussion\nabout things like shallow trees",
    "start": "5490",
    "end": "12780"
  },
  {
    "start": "6000",
    "end": "281000"
  },
  {
    "text": "or simple linear models or\nsimple rule-based models and so on.",
    "start": "12780",
    "end": "17880"
  },
  {
    "text": "In this module, for the\nnext 40 minutes or so, we'll do a quick overview\nof some of these approaches.",
    "start": "17880",
    "end": "25900"
  },
  {
    "text": "So let me start by\nsaying that I am not going to touch on the very\nbasic sort of linear models",
    "start": "25900",
    "end": "33120"
  },
  {
    "text": "or regularized linear\nmodels with, like, L1- or L0- or\nL2-style regressions,",
    "start": "33120",
    "end": "38289"
  },
  {
    "text": "or regularized regressions. But instead I'm going\nto start from something, like, around the\nyear 2014, 2015.",
    "start": "38290",
    "end": "45900"
  },
  {
    "text": "Since then, there has\nbeen more renewed interest in sort of building inherently\ninterpretable models,",
    "start": "45900",
    "end": "54030"
  },
  {
    "text": "exploring newer\nways than what we had with respect to previous\nwork in '70s and '80s, right?",
    "start": "54030",
    "end": "60810"
  },
  {
    "text": "So what I'm not covering\nis a few things, so just linear models, linear\nlogistic regression,",
    "start": "60810",
    "end": "67650"
  },
  {
    "text": "regularizations on\ntop of them, and then decision tree construction\nusing greedy approaches.",
    "start": "67650",
    "end": "73800"
  },
  {
    "text": "So those are the\nthings that I'm going to sort of put a pin on and\nassume some familiarity for all",
    "start": "73800",
    "end": "79140"
  },
  {
    "text": "of you. But I'll start\nfrom the year 2015 and go over some of\nthe approaches that",
    "start": "79140",
    "end": "84330"
  },
  {
    "text": "came since then. OK? Does that sound good? All right. OK. So again, as I was saying,\nso with the renewed interest",
    "start": "84330",
    "end": "92340"
  },
  {
    "text": "recently, a lot of approaches,\nagain, the constructs is probably what I would\nencourage all of you",
    "start": "92340",
    "end": "98550"
  },
  {
    "text": "to take away from some\nof these discussions. So the people have\ntried to build several kinds of inherently\ninterpretable models.",
    "start": "98550",
    "end": "106440"
  },
  {
    "text": "So one is rule-based models. The other is risk scores. And I'll explain what\nthey are in a little bit.",
    "start": "106440",
    "end": "113250"
  },
  {
    "text": "And then there is\ngeneralized relative models, which had gained a lot\nof popularity recently.",
    "start": "113250",
    "end": "118380"
  },
  {
    "text": "And then there are\nprototype-based models, OK? So let's go over each of these.",
    "start": "118380",
    "end": "123588"
  },
  {
    "text": "Oh, and also,\nattention-based models. All right, OK. So let's go to the\nfirst set of approaches.",
    "start": "123588",
    "end": "130800"
  },
  {
    "text": "That is the\nrule-based approaches. And I'm going to first\ntalk about a couple",
    "start": "130800",
    "end": "136110"
  },
  {
    "text": "of approaches in\nthese categories before I move on to another one. This approach called\nBayesian rule lists",
    "start": "136110",
    "end": "142980"
  },
  {
    "text": "was introduced in 2016 by\nBen Letham and Cynthia Rudin.",
    "start": "142980",
    "end": "148349"
  },
  {
    "text": "And this was basically\nan approach that is-- the output of this approach\nlooks something like this,",
    "start": "148350",
    "end": "155497"
  },
  {
    "text": "right? So it's a sequence of\nif, else, if rules. And this is actually the output\nthat they get with their method",
    "start": "155497",
    "end": "163920"
  },
  {
    "text": "as a classifier for\nstroke prediction, right? So this is basically they\nwere working with some doctors",
    "start": "163920",
    "end": "170340"
  },
  {
    "text": "and health care experts\nto design models that can be used to\npredict strokes in advance,",
    "start": "170340",
    "end": "176519"
  },
  {
    "text": "or stroke risk in advance. And the requirement\nin those settings is that doctors want\nto precisely see",
    "start": "176520",
    "end": "182819"
  },
  {
    "text": "the rules that can be\nused to sort of employ as they go in their day-to-day\ndecision making, right?",
    "start": "182820",
    "end": "189360"
  },
  {
    "text": "So they didn't want any\nkind of complex models. They wanted something\nas simple as this. But also they wanted\nsomething accurate, right?",
    "start": "189360",
    "end": "196590"
  },
  {
    "text": "So this approach is kind of\nmotivated by that scenario. So what exactly is\ntheir approach, right?",
    "start": "196590",
    "end": "204000"
  },
  {
    "text": "So their approach is basically\nproposing a generative model, which is designed to\nproduce these kinds of if,",
    "start": "204000",
    "end": "211545"
  },
  {
    "text": "else, if lists. And the way they designed\nthis generative model is by striking a balance between\naccuracy, interpretability,",
    "start": "211545",
    "end": "220440"
  },
  {
    "text": "and computation. And I'll go into some more\ndetails about this approach. But you could look at, or anyone\ncould look at that particular",
    "start": "220440",
    "end": "228780"
  },
  {
    "text": "if, else, if list\nthat we saw and say, wait, why are they just not\nusing other similar models?",
    "start": "228780",
    "end": "234690"
  },
  {
    "text": "For example, what about decision\ntrees, like CART, C5.0, C4.5?",
    "start": "234690",
    "end": "239850"
  },
  {
    "text": "Why are they not using these\nkinds of models, right? The reason, or at least\nwhat the paper argues",
    "start": "239850",
    "end": "245550"
  },
  {
    "text": "is that these kinds of models\nemploy greedy-based approaches.",
    "start": "245550",
    "end": "250770"
  },
  {
    "text": "So while greedy methods, as\nwe all know, are efficient, they can often get stuck\nin local minima, right?",
    "start": "250770",
    "end": "257040"
  },
  {
    "text": "So the argument that\nthis paper makes is that it is\ncomputationally great.",
    "start": "257040",
    "end": "262049"
  },
  {
    "text": "But the resulting\nsolution is not going to be perfect in terms of\naccuracy or interpretability.",
    "start": "262050",
    "end": "268590"
  },
  {
    "text": "So that's why they\nput forth that there is a need for new methods\nto even generate things",
    "start": "268590",
    "end": "275387"
  },
  {
    "text": "like rules lists from scratch. OK?  So for those of you who are\nfamiliar with Bayesian models,",
    "start": "275387",
    "end": "284020"
  },
  {
    "start": "281000",
    "end": "519000"
  },
  {
    "text": "here is what their generative\nmodel or the generative process looks like.",
    "start": "284020",
    "end": "289067"
  },
  {
    "text": "If you think about it-- and\nfor those of you who are not familiar, there's no\nneed to worry too much-- all that goes on in generative\nmodels or these kinds",
    "start": "289067",
    "end": "296800"
  },
  {
    "text": "of Bayesian topic\nmodeling style approaches is that you sort of assume\na particular process that",
    "start": "296800",
    "end": "304780"
  },
  {
    "text": "generates the data. And then you have a bunch\nof unknown parameters as you assume this process.",
    "start": "304780",
    "end": "310840"
  },
  {
    "text": "And then you work\nbackwards and then try to estimate or learn\nthose unknown parameters using",
    "start": "310840",
    "end": "316540"
  },
  {
    "text": "different techniques, right? So that's the basic\nintuition or concept behind generative models.",
    "start": "316540",
    "end": "322940"
  },
  {
    "text": "So what is this particular\ngenerative model trying to do? So essentially, the generation\nprocess says, first of all,",
    "start": "322940",
    "end": "329770"
  },
  {
    "text": "pick a list length. So how many rules are\nthere in the list? We sample that number.",
    "start": "329770",
    "end": "335650"
  },
  {
    "text": "And then we basically\nsample a default rule, which will apply for things\nthat no other rule is",
    "start": "335650",
    "end": "341710"
  },
  {
    "text": "going to apply for. And then we are going to sample\nthe M different other rules",
    "start": "341710",
    "end": "347140"
  },
  {
    "text": "in our failsafe set of list. And for each rule,\nwhat we do, we",
    "start": "347140",
    "end": "352539"
  },
  {
    "text": "basically sample the cardinality\nof the antecedent, which is the left-hand\nside of the rule,",
    "start": "352540",
    "end": "357850"
  },
  {
    "text": "how many predicates are in the\nleft-hand side of the rule. And then we basically\nsample the predicates",
    "start": "357850",
    "end": "365530"
  },
  {
    "text": "on the left-hand side\nof the rule, right? So you sample the number. And then you sample\nthe predicates.",
    "start": "365530",
    "end": "370900"
  },
  {
    "text": "And then of course,\nyou sample what should be on the right-hand\nside of the rule, right? And then the observations will\nbe generated from these sample",
    "start": "370900",
    "end": "379729"
  },
  {
    "text": "rules as find the rule\nthat actually applies",
    "start": "379730",
    "end": "384880"
  },
  {
    "text": "to this instance in the data. And then if no antecedents\napply or no rules apply,",
    "start": "384880",
    "end": "390760"
  },
  {
    "text": "then you basically give\nit the default rule. Otherwise you sort of\nsample a class label",
    "start": "390760",
    "end": "396880"
  },
  {
    "text": "based on that rule that applies\nto this instance, right? So that's the high level. If we were to think about\nconstructing these rules,",
    "start": "396880",
    "end": "404770"
  },
  {
    "text": "that's intuitively\nhow you would think about this kind of a process. So I want to note\na few things here.",
    "start": "404770",
    "end": "412190"
  },
  {
    "text": "So one is the sort of\nsampling the predicates or the conditions.",
    "start": "412190",
    "end": "417770"
  },
  {
    "text": "And when I use the\nword predicate, I basically just mean these\nconditions are the predicates,",
    "start": "417770",
    "end": "422840"
  },
  {
    "text": "right? So it's like, if\ntransient is basic, or if transient ischemic\nattack, that's the condition,",
    "start": "422840",
    "end": "429290"
  },
  {
    "text": "or that's the predicate. All right. So A here is the set of\npremined antecedents.",
    "start": "429290",
    "end": "437830"
  },
  {
    "text": "So what do I mean by\nthat is whenever they're trying to sort of pick the\nleft-hand side of the rules,",
    "start": "437830",
    "end": "445060"
  },
  {
    "text": "they're actually selecting\nfrom a set of candidate left-hand sides that\nare already available.",
    "start": "445060",
    "end": "450790"
  },
  {
    "text": "OK? So they use some sort of\nfrequent item set mining, for those of you who are\nfamiliar with a priori style",
    "start": "450790",
    "end": "458080"
  },
  {
    "text": "algorithms. So they kind of use\nthat to determine what are some of the frequently\noccurring patterns in the data",
    "start": "458080",
    "end": "466390"
  },
  {
    "text": "and then try to pick\nleft-hand sides from those. OK? So there's a candidate set. And they try to pick from it.",
    "start": "466390",
    "end": "472700"
  },
  {
    "text": "So now, just at a\nvery high level, for this kind of a\ngenerative process,",
    "start": "472700",
    "end": "478030"
  },
  {
    "text": "we use something called as\nMetropolis-Hastings algorithm. It's essentially like\nfor those of you who",
    "start": "478030",
    "end": "483220"
  },
  {
    "text": "are familiar with MCMC\nor Gibbs sampling method, so this is essentially one of\nthose classes of algorithms.",
    "start": "483220",
    "end": "490120"
  },
  {
    "text": "And using this, they go back\nand estimate all the parameters in that generative process\nthat they don't know, right?",
    "start": "490120",
    "end": "497500"
  },
  {
    "text": "So for example, before\nconstructing the list, you don't know what will be the\ncardinality of the antecedent",
    "start": "497500",
    "end": "504310"
  },
  {
    "text": "or the number of predicates\nof antecedent of some rules. Or you don't know how many\nrules you're picking, right?",
    "start": "504310",
    "end": "510580"
  },
  {
    "text": "But using this process, they go\nback and learn those and then build the list, OK?",
    "start": "510580",
    "end": "516340"
  },
  {
    "text": "So that's the high-level idea. All right. So one of the major sources\nof practical feasibility",
    "start": "516340",
    "end": "523820"
  },
  {
    "start": "519000",
    "end": "669000"
  },
  {
    "text": "here is the set of\npremined antecedents or premined left-hand\nsides of these rules.",
    "start": "523820",
    "end": "530225"
  },
  {
    "text": " And that basically helps\nthem reduce the model space",
    "start": "530225",
    "end": "536360"
  },
  {
    "text": "because you're essentially\nsearching over those rules only in order to find your\nfinal list, right?",
    "start": "536360",
    "end": "542779"
  },
  {
    "text": "So if your candidate\nrule set is small, then you basically have smaller\nsearch space to look at, right?",
    "start": "542780",
    "end": "548510"
  },
  {
    "text": "So they can control the\ncomplexity of the problem through that, right? And as long as the set of\npremined antecedent list",
    "start": "548510",
    "end": "556700"
  },
  {
    "text": "is expressive, you\nwill be able to find an accurate-enough\ndecision list.",
    "start": "556700",
    "end": "562610"
  },
  {
    "text": "Plus, having that\nset to be small can also help with better\ngeneralization as well, right?",
    "start": "562610",
    "end": "569029"
  },
  {
    "text": "So you need to choose that\nset carefully in order to find a good or an accurate\ndecision list with this method.",
    "start": "569030",
    "end": "577310"
  },
  {
    "text": "Any thoughts, questions?  All right.",
    "start": "577310",
    "end": "583360"
  },
  {
    "text": "OK, so the next approach\nwhich kind of tries to simplify this process\na bit more further",
    "start": "583360",
    "end": "589450"
  },
  {
    "text": "and also makes it more\nefficient and also add some tweaks on interpretability,\nwhich I'll also",
    "start": "589450",
    "end": "596440"
  },
  {
    "text": "discuss a bit more in\nour analysis section, is interpretable decision sets. And this is like an example\ndecision set classifier",
    "start": "596440",
    "end": "606190"
  },
  {
    "text": "for disease diagnosis task on\na particular data set, right? So what immediately\nkind of strikes to you",
    "start": "606190",
    "end": "612430"
  },
  {
    "text": "is instead of if,\nelse, if rules, there is a set of unordered\nif, then rules here.",
    "start": "612430",
    "end": "617870"
  },
  {
    "text": "So that's the main\ndifference in terms of how the rules look like. You would be surprised to know\nthat even just a change in the",
    "start": "617870",
    "end": "625389"
  },
  {
    "text": "construct from if, else,\nif to if, then already changes significantly how\npeople perform certain tasks.",
    "start": "625390",
    "end": "633370"
  },
  {
    "text": "For example-- and I'm going\nto talk about this a lot more in our analysis section to show\nhow even simple changes in how",
    "start": "633370",
    "end": "641110"
  },
  {
    "text": "an interpretation looks,\nright, can have big differences in terms of-- let's say, if a doctor\nis looking at these rules",
    "start": "641110",
    "end": "647830"
  },
  {
    "text": "and trying to make\nsome decisions and so on, the\ncognitive load that",
    "start": "647830",
    "end": "652840"
  },
  {
    "text": "is incurred with small\nchanges can actually be pretty big, right? So while computationally\nwe would be like,",
    "start": "652840",
    "end": "659260"
  },
  {
    "text": "oh, that's a list. There is ordering. There's a set. There is no ordering. That's the difference.",
    "start": "659260",
    "end": "664390"
  },
  {
    "text": "But these things\nhave a lot of impact. And we'll talk about\nthat a bit more later. So this approach will\nalso hopefully make",
    "start": "664390",
    "end": "672010"
  },
  {
    "start": "669000",
    "end": "736000"
  },
  {
    "text": "it clear to you\nas to what exactly is the desiderata behind\nthese kinds of rule approaches",
    "start": "672010",
    "end": "678279"
  },
  {
    "text": "that are being used to construct\nthese rule-based models, again, in a renewed fashion since 2015.",
    "start": "678280",
    "end": "685300"
  },
  {
    "text": "So this is what is\nthe set of criteria that this approach\nis focusing on, which",
    "start": "685300",
    "end": "690910"
  },
  {
    "text": "is you want to account for\nrecall, precision, distinctness of the rules,\nsparsity in the rules,",
    "start": "690910",
    "end": "697870"
  },
  {
    "text": "and then covering all\nthe classes, right? And recall and precision\nis important for accurate",
    "start": "697870",
    "end": "703870"
  },
  {
    "text": "predictions. And distinctness, parsimony,\nand covering all the classes",
    "start": "703870",
    "end": "708970"
  },
  {
    "text": "is important for\ninterpretability, right? So just to go over\nsome of the terms here-- and just bear\nwith me as we do this",
    "start": "708970",
    "end": "715750"
  },
  {
    "text": "because these are the building\nblocks of a lot of the things that we'll talk about later. So then we are going to just\nskim through these details",
    "start": "715750",
    "end": "722890"
  },
  {
    "text": "and say, oh, we\nrefer to them there. OK? All right.  So the object of function, in\nthis case, as we were saying,",
    "start": "722890",
    "end": "731110"
  },
  {
    "text": "it's trying to sort of encompass\nfive different criteria, right? So parsimony, or the\nsparsity, is basically",
    "start": "731110",
    "end": "739480"
  },
  {
    "start": "736000",
    "end": "947000"
  },
  {
    "text": "trying to ensure that there are\nfewer rules in the rule set.",
    "start": "739480",
    "end": "744589"
  },
  {
    "text": "So now our goal is we have\nto pick from a candidate set of rules that we have. We need to pick a subset of some\nk if there are if, then rules.",
    "start": "744590",
    "end": "753970"
  },
  {
    "text": "And then that will become\nour classifier, right? And the first criterion in\nthat is we favor fewer rules.",
    "start": "753970",
    "end": "760720"
  },
  {
    "text": "So we want to either\nminimize the number of rules,",
    "start": "760720",
    "end": "765769"
  },
  {
    "text": "which is denoted by size of R or\nmaximize the negative of that. So that's essentially what you\nsee is that expression there.",
    "start": "765770",
    "end": "772825"
  },
  {
    "text": "OK? And also fewer predicates,\nwhich is basically the number of\nconditions in the rules,",
    "start": "772825",
    "end": "778810"
  },
  {
    "text": "that should also be smaller. So we want to minimize that or\nmaximize its sort of complement",
    "start": "778810",
    "end": "784270"
  },
  {
    "text": "or its negative, right? Now, in terms of\ndistinctness, because what we are looking at here is\ntrying to find a set of if, then",
    "start": "784270",
    "end": "792700"
  },
  {
    "text": "rules, we want to ensure\nthat the points being covered by the set of rules\nis minimal, or covered",
    "start": "792700",
    "end": "801220"
  },
  {
    "text": "by multiple rules\nin this set of rules is minimal because otherwise you\nare essentially just creating",
    "start": "801220",
    "end": "806590"
  },
  {
    "text": "duplicate rules, which\nyou don't need, right? So you are trying to\nbasically minimize",
    "start": "806590",
    "end": "812079"
  },
  {
    "text": "two notions of overlap,\nwhich is intraclass overlap and interclass overlap.",
    "start": "812080",
    "end": "817720"
  },
  {
    "text": "So basically the point\nis that the number of points that satisfy more than\none rule should be minimized.",
    "start": "817720",
    "end": "824920"
  },
  {
    "text": "OK? So that's what this expression\nis trying to capture. And then the next one\nis class coverage,",
    "start": "824920",
    "end": "831200"
  },
  {
    "text": "which is you are\nbasically checking to see that there is\nat least some rule",
    "start": "831200",
    "end": "836440"
  },
  {
    "text": "corresponding to a given\nclass or any given class C so that minority classes, for\nexample, in the kinds of data",
    "start": "836440",
    "end": "843610"
  },
  {
    "text": "sets we are looking at, like\nthe health care data sets, some rare cancers, they are\npresent in such tiny number",
    "start": "843610",
    "end": "850720"
  },
  {
    "text": "of samples in the data that\nclassifiers might often just ignore them\nbecause it doesn't matter to their overall\naccuracy, right?",
    "start": "850720",
    "end": "857709"
  },
  {
    "text": "But here we are sort of\nforcing that to happen. And we want every class to\nbe represented in some rule.",
    "start": "857710",
    "end": "863649"
  },
  {
    "text": "And then, of course, there\nis the precision piece, where you're trying to minimize\nincorrect covers, right?",
    "start": "863650",
    "end": "870610"
  },
  {
    "text": "So basically, we want to\nminimize for each rule that we pick in\nthis set of rules",
    "start": "870610",
    "end": "877840"
  },
  {
    "text": "the number of points\nwhich satisfy the rule but do not belong to the\nclass that the rule is",
    "start": "877840",
    "end": "883810"
  },
  {
    "text": "assigning the point to, right? So that ensures precision,\nwhich is important for accuracy.",
    "start": "883810",
    "end": "890020"
  },
  {
    "text": "And then recall, and the\nway we are sort of trying to enforce recall is that\nyou encourage each point",
    "start": "890020",
    "end": "898600"
  },
  {
    "text": "to be covered by at\nleast one rule in the set that you produce, right?",
    "start": "898600",
    "end": "904300"
  },
  {
    "text": "So for example,\nthe correct cover that you see there,\nthat's basically capturing",
    "start": "904300",
    "end": "909730"
  },
  {
    "text": "the number of data\npoints which satisfy s and also which is the rule s\nand also belong to the class c,",
    "start": "909730",
    "end": "917750"
  },
  {
    "text": "right? So you are basically trying to\nensure that no point is left out from this covering process.",
    "start": "917750",
    "end": "925180"
  },
  {
    "text": "So putting it all\ntogether, what you see is you can basically\ncombine these, put it",
    "start": "925180",
    "end": "930279"
  },
  {
    "text": "into an optimization\nproblem, essentially you'll get some kind of argmax problem\nlike this and then a summation",
    "start": "930280",
    "end": "937240"
  },
  {
    "text": "over all these factors. So the property of this\nkind of an objective--",
    "start": "937240",
    "end": "943790"
  },
  {
    "text": "or the set of properties of\nthis kind of an objective is that this turns out\nto be something that's",
    "start": "943790",
    "end": "949660"
  },
  {
    "start": "947000",
    "end": "1006000"
  },
  {
    "text": "called a non-normal nonmonotone\nand submodular optimization problem, OK?",
    "start": "949660",
    "end": "955370"
  },
  {
    "text": "So without going into too much\ndetails, because that's not the point of this conversation,\nmaximizing such a problem",
    "start": "955370",
    "end": "963340"
  },
  {
    "text": "turns out to be NP hard. But the good news is there\nare some approaches which can provide reasonable\napproximations,",
    "start": "963340",
    "end": "970360"
  },
  {
    "text": "and we can use those\nto efficiently solve this objective, right, in fact,\nfaster than what the Bayesian",
    "start": "970360",
    "end": "977065"
  },
  {
    "text": "rule list was trying to do. OK? So with that, I think we are\nkind of pretty much covering",
    "start": "977065",
    "end": "983770"
  },
  {
    "text": "some of the key approaches\nthat were proposed as the interests in\nrule lists and rule sets",
    "start": "983770",
    "end": "989530"
  },
  {
    "text": "got rejuvenated\nsomewhere around 2015. There are also\nseveral other classes",
    "start": "989530",
    "end": "995620"
  },
  {
    "text": "of interpretable models. So next one is called\nas risk scores.",
    "start": "995620",
    "end": "1001630"
  },
  {
    "text": "So risk scores are basically\nthings that look like this. So I'm going to just\nlet you guys see this",
    "start": "1001630",
    "end": "1008910"
  },
  {
    "start": "1006000",
    "end": "1139000"
  },
  {
    "text": "as I pause for a minute. ",
    "start": "1008910",
    "end": "1022180"
  },
  {
    "text": "So what you see here in these\nkinds of risk score lists is that for each condition,\nyou're assigning points.",
    "start": "1022180",
    "end": "1031490"
  },
  {
    "text": "For example, if the\nnumber of prior arrests is greater than or equal to 2,\nthat's worth 1 point, right?",
    "start": "1031490",
    "end": "1038349"
  },
  {
    "text": "And if your age at release is\ngreater than or equal to 40, that's actually a\nnegative 1 point.",
    "start": "1038349",
    "end": "1044349"
  },
  {
    "text": "So you're trying to sort of\ncreate this kind of scores associated with conditions.",
    "start": "1044349",
    "end": "1050750"
  },
  {
    "text": "And at the end of the\nday, you will just sum up all those scores. And the resulting\nscore will tell you",
    "start": "1050750",
    "end": "1056649"
  },
  {
    "text": "how risky it is, for example,\nfor a defendant to be released or how much of a risk there\nis in a person defaulting",
    "start": "1056650",
    "end": "1063850"
  },
  {
    "text": "on a loan and things\nlike that, right? So it's almost like saying\nif this condition holds at a point.",
    "start": "1063850",
    "end": "1070240"
  },
  {
    "text": "And as these conditions\nhold, keep adding points. What is the total? And the total will tell you\nhow risky the person is.",
    "start": "1070240",
    "end": "1076825"
  },
  {
    "text": "OK? So this kind of a construct\nis actually pretty popular,",
    "start": "1076825",
    "end": "1083950"
  },
  {
    "text": "both in medicine and\ncriminal justice. Like, I myself collaborate\nwith doctors and folks",
    "start": "1083950",
    "end": "1090220"
  },
  {
    "text": "in criminal justice. And they are very comfortable\nwith this kind of risk scores because all they\nhave to do is just",
    "start": "1090220",
    "end": "1096270"
  },
  {
    "text": "kind of go through that\nchecklist and assign points. And then they get a\nrisk rate, risk score.",
    "start": "1096270",
    "end": "1102200"
  },
  {
    "text": "So that's why they're\npretty popular. In fact, the existing\nmodels of risk scores",
    "start": "1102200",
    "end": "1108100"
  },
  {
    "text": "that are out there\nquite a bit in practice in domains like medicine\nand criminal justice",
    "start": "1108100",
    "end": "1113350"
  },
  {
    "text": "are handwritten or hand-drawn\nrisk score checklist, which have been sort of made\nup by domain experts or doctors",
    "start": "1113350",
    "end": "1122410"
  },
  {
    "text": "or lawyers and judges. Of course, they're using\ntheir domain expertise in coming up with those lists.",
    "start": "1122410",
    "end": "1128510"
  },
  {
    "text": "But essentially,\nthey sort of prefer these kinds of checklists\nwith scores quite a bit.",
    "start": "1128510",
    "end": "1133837"
  },
  {
    "text": "So there's something that\nnaturally comes to them in terms of utility, OK? So how do we now construct\nthese kinds of risk score",
    "start": "1133838",
    "end": "1143320"
  },
  {
    "start": "1139000",
    "end": "1314000"
  },
  {
    "text": "checklists using data, right? So that's the problem, or\nthat's the question that",
    "start": "1143320",
    "end": "1148510"
  },
  {
    "text": "was dealt with by Ustun and\nRudin in the year 2016, OK?",
    "start": "1148510",
    "end": "1154360"
  },
  {
    "text": "So this is roughly the kind of\nproblem that they formulate. It turns out to be a pretty\ncomplicated problem to actually",
    "start": "1154360",
    "end": "1161889"
  },
  {
    "text": "solve correctly, accurately. So we'll get to\nthat a bit later. But essentially, if you see what\nthis is trying to do, is that--",
    "start": "1161890",
    "end": "1171280"
  },
  {
    "text": "so you're trying to basically\nformulate or come up with this kind of\na set of conditions",
    "start": "1171280",
    "end": "1178990"
  },
  {
    "text": "and corresponding scores\nsuch that you are basically",
    "start": "1178990",
    "end": "1184210"
  },
  {
    "text": "minimizing two terms. So the first one is\nbasically ensuring",
    "start": "1184210",
    "end": "1190090"
  },
  {
    "text": "that your scores\nwork out, so those scores that you are applying. And then you will sort of use\nthose scores to then convert",
    "start": "1190090",
    "end": "1197770"
  },
  {
    "text": "them into the\nprobability of risk associated with someone,\nlet's say, right? So the scoring mechanism,\nthe first term will ensure",
    "start": "1197770",
    "end": "1206620"
  },
  {
    "text": "that scoring mechanism\nor the scores you are assigning to each of the\npredicates in your risk score",
    "start": "1206620",
    "end": "1214480"
  },
  {
    "text": "list actually map to the\nprecise risk in practice",
    "start": "1214480",
    "end": "1220210"
  },
  {
    "text": "or the real estimates\nof the risk in the data. So that's what that last term\nis doing in the objective.",
    "start": "1220210",
    "end": "1226269"
  },
  {
    "text": "And then the second\nterm, you are essentially trying to sort of minimize\nthe number of the predicates",
    "start": "1226270",
    "end": "1235900"
  },
  {
    "text": "or the terms that will get on\nto this risk score checklist, right? So you don't want the\nchecklist to be having, like,",
    "start": "1235900",
    "end": "1241960"
  },
  {
    "text": "100 attributes there, right? So you want to minimize that. So that's what that second term,\nwhich is the regularization",
    "start": "1241960",
    "end": "1248200"
  },
  {
    "text": "term, is doing. And then, of course,\nthere could be a bunch of other\nconstraints on how you will pick these conditions\nthat you see in the risk score",
    "start": "1248200",
    "end": "1257050"
  },
  {
    "text": "checklist, and also the scores\nthat you assign to them. For example, there might\nbe a condition that don't",
    "start": "1257050",
    "end": "1262690"
  },
  {
    "text": "assign continuous value scores. The scores that you\nassign should be integers because those are more\ninterpretable, right?",
    "start": "1262690",
    "end": "1268929"
  },
  {
    "text": "So you can think of them\nas 1 point, 2 point, but not as 1.73 or\nsomething, right?",
    "start": "1268930",
    "end": "1274690"
  },
  {
    "text": "So there could be a bunch\nof such constraints, both on what features\nyou're picking",
    "start": "1274690",
    "end": "1279820"
  },
  {
    "text": "and also what scores you\nare assigning to them. That is encoded by the\ncondition there, right?",
    "start": "1279820",
    "end": "1285400"
  },
  {
    "text": "So the above, when\nwe put together some of the more sort\nof practical conditions",
    "start": "1285400",
    "end": "1290620"
  },
  {
    "text": "into this problem\nsetup, turns out to be a mixed-integer program. And it is optimized in\nthis particular work",
    "start": "1290620",
    "end": "1297850"
  },
  {
    "text": "using a cutting plane method and\na branch-and-bound technique. So it does not have\nan easy solution.",
    "start": "1297850",
    "end": "1303730"
  },
  {
    "text": "But I think we'll leave\nit at that, right? So the next class of models\nis the generalized additive",
    "start": "1303730",
    "end": "1309970"
  },
  {
    "text": "models. And so this is the class\nof models which basically",
    "start": "1309970",
    "end": "1317440"
  },
  {
    "start": "1314000",
    "end": "1380000"
  },
  {
    "text": "produces outputs like this. So what you're\nessentially seeing is that you kind of\nsee what are called",
    "start": "1317440",
    "end": "1325280"
  },
  {
    "text": "as shape functions associated\nwith input variables. So what this is showing\nis demand is your outcome",
    "start": "1325280",
    "end": "1331730"
  },
  {
    "text": "variable, OK? And each of the variables\nyou see on the x-axis",
    "start": "1331730",
    "end": "1336830"
  },
  {
    "text": "are your input features. So this is kind of showing\nhow the outcome variable will",
    "start": "1336830",
    "end": "1342800"
  },
  {
    "text": "change as a function of each\nof the input variables, right? So as hour increases,\nwhat happens to demand?",
    "start": "1342800",
    "end": "1349730"
  },
  {
    "text": "And as temperature\nincreases, what happens to demand and so on? And by the way, these are\nnot just plots from the data.",
    "start": "1349730",
    "end": "1357720"
  },
  {
    "text": "This is actually the output of\nthe method that is generalized additive models, OK?",
    "start": "1357720",
    "end": "1363110"
  },
  {
    "text": "So it shows you\nhow the output can be modeled as different\nfunctions of input variables.",
    "start": "1363110",
    "end": "1369480"
  },
  {
    "text": "So the output, in this\ncase, is essentially a summation of all the curves\nthat you're seeing here.",
    "start": "1369480",
    "end": "1375140"
  },
  {
    "text": "OK? It's like an\nadditive combination. So just to show you a precise\nmathematical form of this,",
    "start": "1375140",
    "end": "1385220"
  },
  {
    "start": "1380000",
    "end": "1804000"
  },
  {
    "text": "so, for example,\nthe first row here is essentially like a linear\nregression model, right? So your y is basically\nthis kind of a additive",
    "start": "1385220",
    "end": "1393639"
  },
  {
    "text": "or like a linear combination\nof different features that you have. And then we go one step up to\na generalized linear model,",
    "start": "1393640",
    "end": "1401860"
  },
  {
    "text": "where you can take\na function of y and then model it as a linear\ncombination of different input",
    "start": "1401860",
    "end": "1408220"
  },
  {
    "text": "features, right? Then we can think of an additive\nmodel, where your y will just",
    "start": "1408220",
    "end": "1414040"
  },
  {
    "text": "be an additive combination\nbut of different functions",
    "start": "1414040",
    "end": "1419770"
  },
  {
    "text": "of the input variable, right? So each input variable you\ncan learn a separate function",
    "start": "1419770",
    "end": "1425680"
  },
  {
    "text": "over it. And then the additive\ncombination of that will give you y. Now, a generalized\nversion of that",
    "start": "1425680",
    "end": "1432429"
  },
  {
    "text": "is basically a generalized\nadditive model, where you can take a function\nof the output variable, that",
    "start": "1432430",
    "end": "1438519"
  },
  {
    "text": "is g of y, and\nthen think of that or consider that to be this\nkind of an additive combination",
    "start": "1438520",
    "end": "1445750"
  },
  {
    "text": "of functions of different\ninput variables.",
    "start": "1445750",
    "end": "1452080"
  },
  {
    "text": "And of course, if you want to\nmake the model more complex, your y is basically\njust a function",
    "start": "1452080",
    "end": "1457780"
  },
  {
    "text": "of all the input variables. And that could be an\narbitrarily complex function. So think of deep neural\nnets and so on, which",
    "start": "1457780",
    "end": "1464740"
  },
  {
    "text": "fall into the last category. So this table also kind\nof tries to summarize",
    "start": "1464740",
    "end": "1470049"
  },
  {
    "text": "some of the knowledge that\nwe have been thinking about. So if you think\nof a linear model,",
    "start": "1470050",
    "end": "1475190"
  },
  {
    "text": "It's just a bunch\nof coefficients associated with each feature. We multiply the feature by the\ncoefficient, add everything up.",
    "start": "1475190",
    "end": "1483140"
  },
  {
    "text": "We get the output, right? So the interpretability or the\nintelligibility factor is high.",
    "start": "1483140",
    "end": "1489399"
  },
  {
    "text": "Accuracy may not always be high. Of course, depending\non the data, it might. But it may not be as high.",
    "start": "1489400",
    "end": "1495260"
  },
  {
    "text": "Now, if you think of the\ngeneralized linear model, which is something like a logistic\nregression, in that case also,",
    "start": "1495260",
    "end": "1502840"
  },
  {
    "text": "your intelligibility or\nability to see five weights and then see what they're\ndoing, that is good.",
    "start": "1502840",
    "end": "1509300"
  },
  {
    "text": "But again, accuracy may\nnot always be good, right? And then the third row\nis the additive model,",
    "start": "1509300",
    "end": "1514780"
  },
  {
    "text": "where you are just thinking\nof additive combination of different functions. Your intelligibility\nis probably a bit worse",
    "start": "1514780",
    "end": "1522220"
  },
  {
    "text": "off than just seeing the\nactual variable itself because now we are\nseeing a function.",
    "start": "1522220",
    "end": "1527530"
  },
  {
    "text": "Your accuracy might improve\na bit, though, right? And the same is the case with\ngeneralized additive model.",
    "start": "1527530",
    "end": "1533649"
  },
  {
    "text": "So you are seeing\ncomplex functions now. So your interpretability\nis probably not as high as just thinking of it\nas one variable and its weight.",
    "start": "1533650",
    "end": "1542170"
  },
  {
    "text": "But at the same time, it helps\nwith accuracy a bit, right? OK. So GAMs and GAM squares--",
    "start": "1542170",
    "end": "1550420"
  },
  {
    "text": "I guess that's how\nthey're called. So these have again become\npopular starting 2015, 2016.",
    "start": "1550420",
    "end": "1556929"
  },
  {
    "text": "So GAMs are essentially\nwhat we just saw. So they sort of model\nthese first-order terms.",
    "start": "1556930",
    "end": "1563140"
  },
  {
    "text": "And they're like\nadditive combinations of the functions of the\ndifferent input variables.",
    "start": "1563140",
    "end": "1569019"
  },
  {
    "text": "GAM squared is basically\ngoing one step further. And it is also considering\npairwise interactions",
    "start": "1569020",
    "end": "1575409"
  },
  {
    "text": "between features, right? So the claim is that these kinds\nof shape functions that you",
    "start": "1575410",
    "end": "1581050"
  },
  {
    "text": "saw earlier that are\nproduced by these approaches are interpretable. And we'll talk about that as\nwell in just a bit, right?",
    "start": "1581050",
    "end": "1588500"
  },
  {
    "text": "So how to learn these\nkinds of models. So the idea here is\nthat given that you're",
    "start": "1588500",
    "end": "1593530"
  },
  {
    "text": "trying to fit a function\non each input variable and then do an additive\ncombination of it,",
    "start": "1593530",
    "end": "1598960"
  },
  {
    "text": "so you basically represent\neach feature or each component as a spline.",
    "start": "1598960",
    "end": "1604600"
  },
  {
    "text": "And then you basically use\na least-squares formulation and then run an\noptimization procedure",
    "start": "1604600",
    "end": "1610210"
  },
  {
    "text": "to sort of balance\nsmoothness of the spline, as well as the\nempirical error, right? So that's how you go about it.",
    "start": "1610210",
    "end": "1616660"
  },
  {
    "text": "And then with GAM\nsquared, you also have the higher-order\ninteraction terms to consider.",
    "start": "1616660",
    "end": "1622040"
  },
  {
    "text": "So in this case, you\nfirst build a GAM which is without the interaction\nterm, take the residual error,",
    "start": "1622040",
    "end": "1629140"
  },
  {
    "text": "and then rank all possible pairs\nof interactions in the residual",
    "start": "1629140",
    "end": "1635080"
  },
  {
    "text": "and basically try to pick some\ntop k interaction pairs that are contributing\nto the residual,",
    "start": "1635080",
    "end": "1641290"
  },
  {
    "text": "using some sort of feature\nselection type approaches, like, even\ncross-validation, right?",
    "start": "1641290",
    "end": "1647770"
  },
  {
    "text": "And then just pick\nthose top k pairs, and now try to again repeat\nthis learning procedure, where",
    "start": "1647770",
    "end": "1655120"
  },
  {
    "text": "you are basically representing\neach the pairwise interaction term as a spline.",
    "start": "1655120",
    "end": "1660250"
  },
  {
    "text": "And you are just sort of using\na least-squares formulation to fit it. So, yeah, if some of this\nfeels a bit fuzzy, that's fine.",
    "start": "1660250",
    "end": "1668140"
  },
  {
    "text": "These are details of\nhow exactly people are trying to learn such models. But the important piece is\nthe construct that you saw,",
    "start": "1668140",
    "end": "1675309"
  },
  {
    "text": "which basically looks\nsomething like this. And the claim is\nthese kinds of things",
    "start": "1675310",
    "end": "1680800"
  },
  {
    "text": "are actually interpretable. I think I'll just\nsort of do a poll here",
    "start": "1680800",
    "end": "1686730"
  },
  {
    "text": "to see how many of you\nthink this is interpretable? Show of hands.",
    "start": "1686730",
    "end": "1692070"
  },
  {
    "text": " OK. That is some of you, right?",
    "start": "1692070",
    "end": "1697355"
  },
  {
    "text": " What about the rest? How many of you think\nit's not interpretable?",
    "start": "1697355",
    "end": "1703210"
  },
  {
    "text": "Let me see if there are\npeople who are undecided. Oh, there are some\nundecided folks.",
    "start": "1703210",
    "end": "1708580"
  },
  {
    "text": "So why do you think\nit's not interpretable?",
    "start": "1708580",
    "end": "1714120"
  },
  {
    "text": "Maybe [INAUDIBLE]\nlooking at these terms. But then beyond\n[INAUDIBLE],, I think",
    "start": "1714120",
    "end": "1720870"
  },
  {
    "text": "it's too much [INAUDIBLE]. Right. And what do you feel\nabout thinking about this",
    "start": "1720870",
    "end": "1726450"
  },
  {
    "text": "as the whole outcome\nvariable is some kind of additive combination\nof all these shapes?",
    "start": "1726450",
    "end": "1734980"
  },
  {
    "text": "Right. So I think I understand the\nsensitivity of each variable, but not the combined\neffect of the difference",
    "start": "1734980",
    "end": "1741720"
  },
  {
    "text": "across [INAUDIBLE]. Right. So I think the sort\nof intelligibility",
    "start": "1741720",
    "end": "1747218"
  },
  {
    "text": "or the interpretability-- I know we are using these\nterms somewhat loosely, but-- of GAMs has been\ndebated quite a bit.",
    "start": "1747218",
    "end": "1754690"
  },
  {
    "text": "So again, there are pieces of\nit that make it interpretable, which is you can kind of\nsee how the output would",
    "start": "1754690",
    "end": "1761580"
  },
  {
    "text": "vary as a function of individual\ninput variables, right? But at the same time, thinking\nabout all the variables in one",
    "start": "1761580",
    "end": "1769590"
  },
  {
    "text": "shot or in conjunction\nand then thinking what their effect\nwould be on the output",
    "start": "1769590",
    "end": "1774720"
  },
  {
    "text": "is a little hard to fathom from\nsomething like this, right? So there have been\ndebates about whether this",
    "start": "1774720",
    "end": "1780930"
  },
  {
    "text": "should be considered\ninterpretable or not. In fact, I think there is\na dearth of doing more user",
    "start": "1780930",
    "end": "1786570"
  },
  {
    "text": "studies to actually\ncheck if some of this is considered interpretable. If so, by what kinds\nof users and so on.",
    "start": "1786570",
    "end": "1792787"
  },
  {
    "text": "OK? All right.  So with that, I'm going\nto keep us moving.",
    "start": "1792787",
    "end": "1798480"
  },
  {
    "text": "And let's talk a little bit\nabout prototype-based models.",
    "start": "1798480",
    "end": "1803910"
  },
  {
    "text": "So first, from here on, we\nare actually also jumping into some more complex aspects.",
    "start": "1803910",
    "end": "1809700"
  },
  {
    "start": "1804000",
    "end": "2040000"
  },
  {
    "text": "But I'm first going to describe\na very simple approach for what would be prototype-based models.",
    "start": "1809700",
    "end": "1815340"
  },
  {
    "text": "And then we'll see how to\nsort of make deep learning models interpretable by\nincorporating prototypes",
    "start": "1815340",
    "end": "1822510"
  },
  {
    "text": "or some other constructs. OK? All right. ",
    "start": "1822510",
    "end": "1827580"
  },
  {
    "text": "So the goal with this\nparticular the basic problem we are considering here is\nto identify k prototypes",
    "start": "1827580",
    "end": "1836100"
  },
  {
    "text": "or instances from a\ngiven data set such that if there is\na new instance, we",
    "start": "1836100",
    "end": "1842700"
  },
  {
    "text": "can use these prototypes\nto assign labels for that instance, right? So you want to pick some\nrepresentative points",
    "start": "1842700",
    "end": "1850289"
  },
  {
    "text": "in the data set, such that if\nyou give me a new data point, I should be able to\ncompute the nearest",
    "start": "1850290",
    "end": "1856050"
  },
  {
    "text": "neighbors among these\nrepresentative points and assign the label of\nthat nearest neighbor.",
    "start": "1856050",
    "end": "1861539"
  },
  {
    "text": "And that should be the correct\nlabel for that new data point. So in some sense, I'm\ntrying to build a classifier",
    "start": "1861540",
    "end": "1869040"
  },
  {
    "text": "through these\nrepresentative data points. Is that clear, right?",
    "start": "1869040",
    "end": "1874320"
  },
  {
    "text": "So the way it is this paper\ngoes about this problem is that you consider each\ninstance to cover some epsilon",
    "start": "1874320",
    "end": "1885059"
  },
  {
    "text": "neighborhood around it, right,\nlike an epsilon ball around it. We consider that each\ninstance sort of covers",
    "start": "1885060",
    "end": "1892410"
  },
  {
    "text": "that neighborhood, OK, which\nmeans-- and cover means, if a point lies within that\nepsilon distance from any given",
    "start": "1892410",
    "end": "1901590"
  },
  {
    "text": "instance, then you say this\npoint belongs to the epsilon neighborhood of that instance. And I'll show you pictorially\nin just a little bit, right?",
    "start": "1901590",
    "end": "1909300"
  },
  {
    "text": "So once you associate this\nkind of a neighborhood with each instance,\nnow what you have done",
    "start": "1909300",
    "end": "1916530"
  },
  {
    "text": "is you can sort of see\nthe analogy between this and a rule, right? So earlier, we were\ntrying to come up",
    "start": "1916530",
    "end": "1923100"
  },
  {
    "text": "with a subset of\nrules in the rule set that we discussed about are\nthe interpretable decision sets",
    "start": "1923100",
    "end": "1929100"
  },
  {
    "text": "that we discussed about. We were coming up\nwith a subset of rules that can cover all\nthe points in the data",
    "start": "1929100",
    "end": "1936420"
  },
  {
    "text": "and also be accurate\nclassifiers, right? So similarly, we\nare just replacing",
    "start": "1936420",
    "end": "1942390"
  },
  {
    "text": "the construct of the\nrule now with a prototype or an instance. And an instance is set\nto cover all the points",
    "start": "1942390",
    "end": "1950370"
  },
  {
    "text": "within the epsilon\nball neighborhood around that instance, right? So for example--\nand then once we",
    "start": "1950370",
    "end": "1957510"
  },
  {
    "text": "see this sort of\ndeduction or reduction to this problem we talked about\nearlier, as soon as you define",
    "start": "1957510",
    "end": "1963779"
  },
  {
    "text": "this neighborhood, you can\napply all the optimization problem that we talked about\nearlier in the context of rule",
    "start": "1963780",
    "end": "1969840"
  },
  {
    "text": "sets and then find a\nsubset of prototypes that maximally cover the\ndata, but is also accurately",
    "start": "1969840",
    "end": "1977310"
  },
  {
    "text": "assigning labels to the points\nthat are getting covered, right? So just to show some pictorial\nrepresentation of this--",
    "start": "1977310",
    "end": "1984480"
  },
  {
    "text": "and the value of\nthis epsilon that defines the neighborhood can\nbe given by an end user, right?",
    "start": "1984480",
    "end": "1991500"
  },
  {
    "text": "So this is basically showing\nfor different values of epsilon, if we pick certain prototypes,\nwhat are the neighborhoods",
    "start": "1991500",
    "end": "1999510"
  },
  {
    "text": "that they're covering, right? So if epsilon is\nsmall, you'll end up picking a lot of prototypes\nbecause it's only covering",
    "start": "1999510",
    "end": "2006740"
  },
  {
    "text": "a small portion of the data. Each prototype is only covering\na small portion of the data. But as you increase\nthe value of epsilon,",
    "start": "2006740",
    "end": "2014340"
  },
  {
    "text": "you'll end up picking\nfewer prototypes because each prototype covers\na larger chunk of the data,",
    "start": "2014340",
    "end": "2019650"
  },
  {
    "text": "right? And so essentially,\nwhat you have built is some kind of a\nnearest-neighbor classifier.",
    "start": "2019650",
    "end": "2026610"
  },
  {
    "text": "And you have sort of\ntook some prototypes out for building such a classifier. All right. ",
    "start": "2026610",
    "end": "2034110"
  },
  {
    "text": "So this is another\npopular class. And this kind of nicely\nfeeds into our discussion",
    "start": "2034110",
    "end": "2041010"
  },
  {
    "start": "2040000",
    "end": "2401000"
  },
  {
    "text": "on-- so, so far, we\nhave been talking about we want simpler\nmodels, simpler constructs.",
    "start": "2041010",
    "end": "2046950"
  },
  {
    "text": "How do I learn\nrule lists and rule sets and GAMs and simple\nprototype-based models?",
    "start": "2046950",
    "end": "2054669"
  },
  {
    "text": "So I'm sure some of\nyou are wondering, OK, when will we get to\ndeep learning, right? So all these seem like\nnice and simple models.",
    "start": "2054670",
    "end": "2062609"
  },
  {
    "text": "But these are not going\nto help with my accuracy. I want to run deep\nlearning models. So what about\ninterpretability there, right?",
    "start": "2062610",
    "end": "2069270"
  },
  {
    "text": "So here we are jumping\ninto that territory. OK. So this is basically-- and\nthere are few approaches which",
    "start": "2069270",
    "end": "2078489"
  },
  {
    "text": "try to do something like this. The idea here is that you\ncan make deep learning",
    "start": "2078489",
    "end": "2083949"
  },
  {
    "text": "architectures interpretable\nby essentially introducing these kinds of layers, right?",
    "start": "2083949",
    "end": "2090669"
  },
  {
    "text": "So there is a prototype\nlayer, followed by a fully connected layer. So you can augment\nthese two layers",
    "start": "2090670",
    "end": "2097450"
  },
  {
    "text": "almost towards the end\nof a given neural network architecture and try to get\nsome interpretability in terms",
    "start": "2097450",
    "end": "2106180"
  },
  {
    "text": "of the predictions being made. So the specific\narchitecture that you're",
    "start": "2106180",
    "end": "2112000"
  },
  {
    "text": "seeing, what it has\nis this first part is basically what is like\nan autoencoder, right?",
    "start": "2112000",
    "end": "2118180"
  },
  {
    "text": "And then you take the\noutput of the encoder. And then you pass it\nto the prototype layer",
    "start": "2118180",
    "end": "2124810"
  },
  {
    "text": "and the fully connected layer. And then you get the output. And we'll go into the details\nof what each layer is doing.",
    "start": "2124810",
    "end": "2130810"
  },
  {
    "start": "2130810",
    "end": "2136640"
  },
  {
    "text": "Yeah. So let's say our input has the\ndimensionality of p, right? So it's all real numbers.",
    "start": "2136640",
    "end": "2143210"
  },
  {
    "text": "So there's a vector of real\nnumbers and of dimension p. And then that passes\nthrough the encoder.",
    "start": "2143210",
    "end": "2150400"
  },
  {
    "text": "And then it gets\ntransformed into Rq, which is basically a\nq-dimensional vector",
    "start": "2150400",
    "end": "2155800"
  },
  {
    "text": "of real numbers. And q is typically\nless than p, right? So for those of you\nwho are wondering",
    "start": "2155800",
    "end": "2161680"
  },
  {
    "text": "what exactly this\ncomponent is, think of it as essentially it's creating a\nlow-dimensional representation",
    "start": "2161680",
    "end": "2168130"
  },
  {
    "text": "of your input, right? So in some sense,\nyou're trying to reduce the dimensions in your\ndata while preserving",
    "start": "2168130",
    "end": "2174490"
  },
  {
    "text": "the most important information. So you're doing that\nin the first component.",
    "start": "2174490",
    "end": "2180550"
  },
  {
    "text": "And then you are inputting\na q-dimensional vector of real numbers to\nthe prototype layer.",
    "start": "2180550",
    "end": "2187930"
  },
  {
    "text": "And what is the\nprototype layer doing? The prototype layer is actually\nresponsible for computing",
    "start": "2187930",
    "end": "2194980"
  },
  {
    "text": "or learning the prototypes. And more specifically, each\nnode in the prototype layer",
    "start": "2194980",
    "end": "2201579"
  },
  {
    "text": "is computing this value, right? So for example, p1 is computing.",
    "start": "2201580",
    "end": "2207490"
  },
  {
    "text": "So given an input\ndata point x, we got this z, which is the\ntransformed version of x.",
    "start": "2207490",
    "end": "2215890"
  },
  {
    "text": "And then each of those\nnodes in the prototype layer is computing the-- sort of\nthis L2 norm difference,",
    "start": "2215890",
    "end": "2223780"
  },
  {
    "text": "or you can think of it as the\ndistance between z and p1, OK, so between the transformed\ninput point and the prototype",
    "start": "2223780",
    "end": "2233170"
  },
  {
    "text": "that it's learning or it has\ncurrently learned, right? So in some sense,\neach layer is just",
    "start": "2233170",
    "end": "2239740"
  },
  {
    "text": "outputting the distance\nbetween the input point or some transformation\nof it and then",
    "start": "2239740",
    "end": "2246100"
  },
  {
    "text": "the prototype chosen so far. OK? All right.  So now that that's the case, the\ndimensionality of that vector",
    "start": "2246100",
    "end": "2255730"
  },
  {
    "text": "will be m. So that is basically you get\none distance value with respect",
    "start": "2255730",
    "end": "2261369"
  },
  {
    "text": "to each prototype, right? So you get-- first,\nnotice computing",
    "start": "2261370",
    "end": "2267280"
  },
  {
    "text": "the distance between z and p1. Second, notice computing the\ndistance between z and p2",
    "start": "2267280",
    "end": "2273039"
  },
  {
    "text": "and so on. You get one scalar value\nfrom each of those nodes. So the entire\noutput of that layer",
    "start": "2273040",
    "end": "2279550"
  },
  {
    "text": "will be m different values.  So then the fully\nconnected layer",
    "start": "2279550",
    "end": "2286660"
  },
  {
    "text": "is essentially\ncomputing a weighted sum of these distances, right? So you got the distances.",
    "start": "2286660",
    "end": "2293020"
  },
  {
    "text": "Now you can make\nthese distances-- the weighted combination\nof these distances",
    "start": "2293020",
    "end": "2298510"
  },
  {
    "text": "using this fully\nconnected layer here. And then of course, so that will\nsort of take the dimensionality",
    "start": "2298510",
    "end": "2306460"
  },
  {
    "text": "from Rm, and you're sort of\nmultiplying that before with w.",
    "start": "2306460",
    "end": "2312250"
  },
  {
    "text": "And so basically\nthat will end up being a k-dimensional vector. And that, in turn, can be\npassed on to the softmax layer",
    "start": "2312250",
    "end": "2319540"
  },
  {
    "text": "in order to output probabilities\nover k classes, right? So basically, what you're doing\nin sort of all these details is",
    "start": "2319540",
    "end": "2327849"
  },
  {
    "text": "that you're essentially\ntrying to-- so normally, deep neural networks\nwould basically have, let's say, this component,\nnone of these two layers,",
    "start": "2327850",
    "end": "2336310"
  },
  {
    "text": "and then you go\nstraight there, right? But you're introducing a couple\nof interpretability layers.",
    "start": "2336310",
    "end": "2342910"
  },
  {
    "text": "There are mainly the\nprototype layer there. So that for every\ninstance, you also",
    "start": "2342910",
    "end": "2349030"
  },
  {
    "text": "have a prototype that sort\nof says something like, oh, I made this prediction\non this instance",
    "start": "2349030",
    "end": "2355310"
  },
  {
    "text": "because it looks\nlike this prototype. Does that make sense?",
    "start": "2355310",
    "end": "2361000"
  },
  {
    "text": "So that's the thing\nthat we are trying to get to here by adding\nthese kinds of layers. OK, all right? ",
    "start": "2361000",
    "end": "2369975"
  },
  {
    "text": "OK. So I think we have a little\nbit more time before the break. And I'm going to\ncover this as well.",
    "start": "2369976",
    "end": "2376569"
  },
  {
    "text": "So the last is\nattention-based models. Again, this has gained a lot\nof popularity in recent times.",
    "start": "2376570",
    "end": "2384580"
  },
  {
    "text": "So the idea is just\nlike we were thinking about adding these prototype\nlayers to deep neural network",
    "start": "2384580",
    "end": "2391800"
  },
  {
    "text": "architectures, here\nwe are thinking of adding another kind\nof layer, which is called",
    "start": "2391800",
    "end": "2397260"
  },
  {
    "text": "as attention layer, right? OK. So just to illustrate\nthis, let me first",
    "start": "2397260",
    "end": "2407120"
  },
  {
    "start": "2401000",
    "end": "2698000"
  },
  {
    "text": "show you this example of a\nmachine translation task. So there is nothing\ninterpretable here.",
    "start": "2407120",
    "end": "2414560"
  },
  {
    "text": "We have not yet\nintroduced anything. But this is an example of\nan architecture of a model",
    "start": "2414560",
    "end": "2420500"
  },
  {
    "text": "that people use to do\nmachine translation. OK? So the idea here is that--",
    "start": "2420500",
    "end": "2425779"
  },
  {
    "text": "so there is your input. You are trying to translate\nthe sentence from English to French, OK?",
    "start": "2425780",
    "end": "2431280"
  },
  {
    "text": "So there are three\ntokens in your input. And each token is passed\nthrough an encoder, which",
    "start": "2431280",
    "end": "2438770"
  },
  {
    "text": "will basically generate\nsome kind of a hidden state representation for that token.",
    "start": "2438770",
    "end": "2444990"
  },
  {
    "text": "So that's what the\nside is doing, right? And as you can see, the\nhidden state representation",
    "start": "2444990",
    "end": "2451130"
  },
  {
    "text": "for the second token depends\non that, as well as this input, right? So both are the inputs here.",
    "start": "2451130",
    "end": "2457620"
  },
  {
    "text": "So in some sense, you're trying\nto basically take each word and then get some vector\nrepresentation of that word",
    "start": "2457620",
    "end": "2464750"
  },
  {
    "text": "through some process. Just think of it in\nthat abstract way, OK? ",
    "start": "2464750",
    "end": "2470810"
  },
  {
    "text": "So now, once this\nencoder is sort of built, the next part is that\nthere is a decoder",
    "start": "2470810",
    "end": "2477760"
  },
  {
    "text": "on the other side, which\nis basically-- so there is a context vector in between.",
    "start": "2477760",
    "end": "2482950"
  },
  {
    "text": "I think for the purpose\nof this architecture, you could assume that C\nis basically just taking",
    "start": "2482950",
    "end": "2489490"
  },
  {
    "text": "the value of h3, which is\nthe encoded representation of the last word there, OK?",
    "start": "2489490",
    "end": "2496160"
  },
  {
    "text": "So I think there\nare architectures which do use that and some\nfunctions of h3, and so on.",
    "start": "2496160",
    "end": "2502010"
  },
  {
    "text": "But let's just assume C is h3. OK? And C is often referred\nto as the context vector",
    "start": "2502010",
    "end": "2508390"
  },
  {
    "text": "because it is trying to capture\nsome context about the input. OK? All right.",
    "start": "2508390",
    "end": "2513640"
  },
  {
    "text": "So now, the next set of layers\nthat you have is the decoder. And that takes C\nas input and then",
    "start": "2513640",
    "end": "2521619"
  },
  {
    "text": "tries to generate a hidden\nstate representation for each of the output words.",
    "start": "2521620",
    "end": "2527800"
  },
  {
    "text": "And then once you generate that\nhidden state representation for, say, the first word in\nthe output, then conditioned",
    "start": "2527800",
    "end": "2535300"
  },
  {
    "text": "on that hidden state\nrepresentation, you can generate the\nactual word, right? So that's how\nmachine translation",
    "start": "2535300",
    "end": "2541119"
  },
  {
    "text": "happens with models like these. OK? ",
    "start": "2541120",
    "end": "2546530"
  },
  {
    "text": "So now attention\nlayers, how to add it in this kind of an\narchitecture, right?",
    "start": "2546530",
    "end": "2552920"
  },
  {
    "text": "So before I describe exactly\nwhat is attention here, the change that you hopefully\nnotice from the previous slide",
    "start": "2552920",
    "end": "2559280"
  },
  {
    "text": "to this one is that we kind\nof added some complexity to the context vector, right?",
    "start": "2559280",
    "end": "2565010"
  },
  {
    "text": "So what we are doing is now the\nencoder has remained the same. The decoder has\nremained the same.",
    "start": "2565010",
    "end": "2570660"
  },
  {
    "text": "The context vector\nis no longer just C. So we now have separate\ncontext vectors",
    "start": "2570660",
    "end": "2577370"
  },
  {
    "text": "corresponding to each\ntoken of the output, right? So the way we introduce\ninterpretability into this",
    "start": "2577370",
    "end": "2585230"
  },
  {
    "text": "is as follows. So the context\nvector corresponding",
    "start": "2585230",
    "end": "2591260"
  },
  {
    "text": "to each hidden representation\nof an output token, which",
    "start": "2591260",
    "end": "2597020"
  },
  {
    "text": "is si is basically the hidden\nrepresentation of the output token i. So the context vector\nwill be modeled",
    "start": "2597020",
    "end": "2604549"
  },
  {
    "text": "as this kind of a\ncombination which is aij and then hj, right?",
    "start": "2604550",
    "end": "2610579"
  },
  {
    "text": "So in some sense,\nwhat you're doing is you're representing\neach context vector",
    "start": "2610580",
    "end": "2615950"
  },
  {
    "text": "as some kind of a\ncombination of each of the hidden representations\nfrom the encoder side.",
    "start": "2615950",
    "end": "2622970"
  },
  {
    "text": "So you are no longer saying,\nI'm just going to take h3, and then all the decoder\npieces will depend on just h3.",
    "start": "2622970",
    "end": "2631280"
  },
  {
    "text": "What you're saying\nis, my context will be some combination\nof these three.",
    "start": "2631280",
    "end": "2637550"
  },
  {
    "text": "And I will learn what\nthat combination is. And that combination\nwill tell me",
    "start": "2637550",
    "end": "2643400"
  },
  {
    "text": "the importance of each of the\ninput words in generating each of the output words, right?",
    "start": "2643400",
    "end": "2649790"
  },
  {
    "text": "So that's exactly what you're\ndoing when you basically add this kind of a term there.",
    "start": "2649790",
    "end": "2655100"
  },
  {
    "text": " So in some sense, the aij\nthat we have introduced there,",
    "start": "2655100",
    "end": "2662230"
  },
  {
    "text": "it captures the attention\nplaced on input token j when determining the decoder\nhidden state si, right?",
    "start": "2662230",
    "end": "2670670"
  },
  {
    "text": "So in some sense, how much\ndoes the output word i depend on each of the\ninput words j, that's",
    "start": "2670670",
    "end": "2678490"
  },
  {
    "text": "what is captured by aij, right? And that is basically the\nnotion of attention weight.",
    "start": "2678490",
    "end": "2685690"
  },
  {
    "text": "And that basically\ndescribes the importance of each of the input features\nin these kinds of networks.",
    "start": "2685690",
    "end": "2692430"
  },
  {
    "start": "2692430",
    "end": "2697000"
  }
]