[
  {
    "start": "0",
    "end": "5530"
  },
  {
    "text": "All right. They should be up now. ",
    "start": "5530",
    "end": "65742"
  },
  {
    "text": "All right, just take a second\nand then compare your answers to someone near you. The reason I'm asking you about\nthese particular algorithms",
    "start": "65742",
    "end": "71740"
  },
  {
    "text": "is because some of\nthe ideas today, even though we're going to be\ntalking about AlphaGo and Monte Carlo Tree Search,\nwill be related",
    "start": "71740",
    "end": "77500"
  },
  {
    "text": "to some of the\nthings that helped make those advances possible. So just check. Good chance to refresh\nyour understanding",
    "start": "77500",
    "end": "83410"
  },
  {
    "text": "of how upper confidence\nbound algorithms work. ",
    "start": "83410",
    "end": "126505"
  },
  {
    "text": "And the one I thought\nmight be somewhat controversial in particular\nis the third one of whether or not if you have a reward\nmodel and it's known,",
    "start": "126505",
    "end": "133980"
  },
  {
    "text": "whether there's\nstill any benefit to using an upper\nconfidence bound algorithm. ",
    "start": "133980",
    "end": "148210"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "148210",
    "end": "184400"
  },
  {
    "text": "All right, let's\ncome back together. So, it looks like there was good\nagreement on the first couple.",
    "start": "184400",
    "end": "189409"
  },
  {
    "text": "So the first one\nis true, which is, you can think of upper\nconfidence bounds as being a way to balance\nbetween our uncertainty",
    "start": "189410",
    "end": "196989"
  },
  {
    "text": "over outcomes when we have\nlimited amounts of data and yet use that\ninformation to still try",
    "start": "196990",
    "end": "202180"
  },
  {
    "text": "to acquire high reward. And these algorithms can\nbe used both in bandits and Markov decision processes.",
    "start": "202180",
    "end": "209680"
  },
  {
    "text": "The third one is a\nlittle bit tricky. Actually either\nanswer would be fine depending on which\nsetting you're looking at.",
    "start": "209680",
    "end": "215930"
  },
  {
    "text": "So does somebody\nwant to argue why, if the reward model\nis known, there is no benefit to using upper\nconfidence bound algorithms.",
    "start": "215930",
    "end": "224150"
  },
  {
    "text": " So in some settings\nthere would not be.",
    "start": "224150",
    "end": "230060"
  },
  {
    "text": "Someone tell me a setting where\nif you knew the reward model, you should not use an upper\nconfidence bound algorithm,",
    "start": "230060",
    "end": "237295"
  },
  {
    "text": "something that we saw over\nthe last couple of weeks that was different than\nthe reinforcement learning framework. ",
    "start": "237295",
    "end": "245180"
  },
  {
    "text": "The multi-armed bandits. That's right. Yeah, so in the\nmulti-armed bandit case, where there's no state\nand there's no dynamics,",
    "start": "245180",
    "end": "251680"
  },
  {
    "text": "the decisions that you make\ndon't influence the next state at all. Then exactly what you said. If you knew what the reward\nmodel is, you'd know how to act.",
    "start": "251680",
    "end": "259260"
  },
  {
    "text": "Like if I knew whether a\ncustomer liked ad A or ad B better, I would just show them\nad A So in a multi-armed bandit",
    "start": "259260",
    "end": "267120"
  },
  {
    "text": "setting/ so in a MAB\nsetting, this is true.",
    "start": "267120",
    "end": "276860"
  },
  {
    "text": "In general, it's not true in\nRl that in generally false.",
    "start": "276860",
    "end": "284960"
  },
  {
    "text": "Somebody want to tell me\nwhy in general, it's false? Even if the reward model\nin reinforcement learning, you might still want to use\nan upper confidence bound",
    "start": "284960",
    "end": "292430"
  },
  {
    "text": "based algorithm. ",
    "start": "292430",
    "end": "298270"
  },
  {
    "text": "Because we want to know is\nthe value function rather than just the [INAUDIBLE] reward.",
    "start": "298270",
    "end": "304068"
  },
  {
    "text": "That's right. So assuming that you don't\nknow what [MUTED] which is, I'm assuming you don't\nknow the dynamics model,",
    "start": "304068",
    "end": "309280"
  },
  {
    "text": "so you don't know how to\ncompute what your optimal value function is. It's still often helpful to\nuse upper confidence bounds.",
    "start": "309280",
    "end": "316297"
  },
  {
    "text": "And in fact, in many cases, you\nmight know the reward function for when you reach a state, like\nyou know when a customer clicks",
    "start": "316297",
    "end": "321640"
  },
  {
    "text": "on something that's good. But the hard thing is maybe\nto drive them into states where they are going\nto click on something",
    "start": "321640",
    "end": "327370"
  },
  {
    "text": "or they are going\nto make a purchase. So in Rl this is\ngenerally false and we'll see some other\nexamples today where",
    "start": "327370",
    "end": "334360"
  },
  {
    "text": "it's helpful to use an upper\nconfidence bound algorithm, even though we know quite\na lot about the world. ",
    "start": "334360",
    "end": "342340"
  },
  {
    "text": "So what we're going to\nbe talking about today is Monte Carlo Tree\nSearch and AlphaGo.",
    "start": "342340",
    "end": "348675"
  },
  {
    "text": "And before we get\ninto that, I'll just remind us a\nlittle bit about where we are in the course. So we have just a\nfew more weeks left.",
    "start": "348675",
    "end": "355520"
  },
  {
    "text": "You should have all gotten\nfeedback on your projects. I encourage you to come\ntalk to me or anybody else",
    "start": "355520",
    "end": "360740"
  },
  {
    "text": "about any questions you have. I have two office\nhours this week because I was traveling late\nlast week for a conference,",
    "start": "360740",
    "end": "367067"
  },
  {
    "text": "so you're welcome to\ncome to my office hours today, which are right after\nthis class or on Thursday. There's also a lot of\nother office hours.",
    "start": "367067",
    "end": "373730"
  },
  {
    "text": "In addition to that,\na week from this Wednesday, we're\ngoing to have a quiz. The quiz will send more\ndetails out about On Ed.",
    "start": "373730",
    "end": "381650"
  },
  {
    "text": "But the main idea is that it's\ngoing to be multiple choice. It is designed to be\neasier than the midterm,",
    "start": "381650",
    "end": "387030"
  },
  {
    "text": "but we'll give you the\nfull amount of time. People generally take\nthe full amount of time just to check their answers.",
    "start": "387030",
    "end": "392100"
  },
  {
    "text": "And it will cover the\nentire course, so everything up through the day before.",
    "start": "392100",
    "end": "398242"
  },
  {
    "text": "Does anybody have any logistics\nquestions before we get going? ",
    "start": "398242",
    "end": "404750"
  },
  {
    "text": "All right. So we're going to talk\nabout Monte Carlo Tree Search and AlphaZero. So as many of you may know\nthere was this amazing series",
    "start": "404750",
    "end": "413460"
  },
  {
    "text": "of results from DeepMind\nand kind of like the 2016 to 2019 time period\naround showing",
    "start": "413460",
    "end": "420600"
  },
  {
    "text": "how you could use\nreinforcement learning and AI to conquer\nthe board game Go.",
    "start": "420600",
    "end": "425669"
  },
  {
    "text": "And this happened about a decade\nearlier than people expected. And this was really considered\none of the huge achievements",
    "start": "425670",
    "end": "432330"
  },
  {
    "text": "in AI. So people really thought\nit was going to take a lot longer to do this. Chess had already been mastered,\ncheckers longer before that.",
    "start": "432330",
    "end": "441570"
  },
  {
    "text": "But there was a lot of\ndifferent innovations that came out of a\nlong history of work that DeepMind used to\nmake this possible.",
    "start": "441570",
    "end": "448610"
  },
  {
    "text": "And I also think\nthat it incorporates a lot of interesting different\nideas that one might think",
    "start": "448610",
    "end": "453630"
  },
  {
    "text": "could be helpful to try\nto solve other problems. The other thing that\nI think is interesting about this is it's quite a\ndifferent form of reinforcement",
    "start": "453630",
    "end": "459720"
  },
  {
    "text": "learning than we've seen before. It's really reinforcement\nlearning for computation. And we'll see a lot\nmore about that.",
    "start": "459720",
    "end": "466690"
  },
  {
    "text": "So, what we're\ngoing to start with is thinking about\nsimulation-based search.",
    "start": "466690",
    "end": "471850"
  },
  {
    "text": "And simulation-based\nsearch is going to sound quite familiar\nbecause we've been seeing ideas around this with Monte Carlo\nsearch and Monte Carlo methods.",
    "start": "471850",
    "end": "480525"
  },
  {
    "text": "But then we're going to\nthink about combining these with using different parts of\nthe stochastic decision making",
    "start": "480525",
    "end": "485680"
  },
  {
    "text": "process. All right. So in particular, one\nof the major ideas that we're going to\nbe looking at today",
    "start": "485680",
    "end": "491965"
  },
  {
    "text": "is the idea that we're\ngoing to be mostly focusing on how to make-- figure\nout what we should",
    "start": "491965",
    "end": "497150"
  },
  {
    "text": "do in the current state only. So in general, in\nclass, whenever",
    "start": "497150",
    "end": "503090"
  },
  {
    "text": "we've been computing a\npolicy or a value function, we've been computing it\nfor the entire state space. So we might have a policy, and\nif anybody gave us a state,",
    "start": "503090",
    "end": "510892"
  },
  {
    "text": "we could immediately\ntell you what action or action distribution\nwe should use or we compute a Q function\nfor the whole space.",
    "start": "510892",
    "end": "518510"
  },
  {
    "text": "One of the key ideas today\nis to say, well, maybe, particularly if we've got an\nenormous space, that we don't",
    "start": "518510",
    "end": "524720"
  },
  {
    "text": "really care about trying to\ncompute an optimal policy for everything in the space. Maybe we just want to\nuse our computation",
    "start": "524720",
    "end": "532220"
  },
  {
    "text": "to really focus on a\ngood decision for right now or for whatever state\nthat you might end up in. And there are lots\nof reasons to think",
    "start": "532220",
    "end": "538940"
  },
  {
    "text": "that that might be\nimportant, particularly in really large domains. So you could imagine,\nif you're the Fed",
    "start": "538940",
    "end": "545150"
  },
  {
    "text": "and you're trying to make some\nsort of federal monetary policy, you probably don't care about\ndoing this for all the scenarios",
    "start": "545150",
    "end": "550880"
  },
  {
    "text": "which the US is not in. You really want to figure it\nout for the current scenario. In the case of the board\ngame, Go as we'll see,",
    "start": "550880",
    "end": "557790"
  },
  {
    "text": "there's just an enormous\nspace of potential states you could end up in. And it may not be important to\nhave a perfect way of acting",
    "start": "557790",
    "end": "565080"
  },
  {
    "text": "in all of those. So one big idea\nhere is that we're going to be mostly focusing\non computation to figure out",
    "start": "565080",
    "end": "571500"
  },
  {
    "text": "what's the right thing to\ndo in the current space. So one thing you\nmight do in this case,",
    "start": "571500",
    "end": "578470"
  },
  {
    "text": "given all the ideas we've seen\nin class, is you might simulate. So imagine that someone\ngives you a policy",
    "start": "578470",
    "end": "586200"
  },
  {
    "text": "and what you want\nto do is try to do at least as good as that policy\nand maybe a little bit better.",
    "start": "586200",
    "end": "591938"
  },
  {
    "text": "So one thing you could\ndo is say, well, I'm in a current state, like a\ncurrent real state, say, ST,",
    "start": "591938",
    "end": "597149"
  },
  {
    "text": "and what I'm going to\ndo is I'm going to say, think about all the different\nactions I could take next.",
    "start": "597150",
    "end": "602230"
  },
  {
    "text": "And then I'm going to\nroll out using my default policy from those states.",
    "start": "602230",
    "end": "607440"
  },
  {
    "text": "So this is just\nlike for K episodes from the current real state.",
    "start": "607440",
    "end": "612519"
  },
  {
    "text": "I'm going to roll out in\nmy brain what might happen. Now this means that I need some\naccess to a dynamics model.",
    "start": "612520",
    "end": "618910"
  },
  {
    "text": "So I can only do this if\nI have access to a model. And what I'm going\nto mean here by model",
    "start": "618910",
    "end": "623970"
  },
  {
    "text": "is dynamics and reward model.",
    "start": "623970",
    "end": "631487"
  },
  {
    "text": "So you might imagine\nthat you actually know how the world\nworks or that someone that you've learned some sort\nof model from the past, this",
    "start": "631487",
    "end": "637839"
  },
  {
    "text": "could be estimated or true. And so then what we can\ndo is we can just do sort of Monte Carlo evaluation.",
    "start": "637840",
    "end": "644330"
  },
  {
    "text": "And what we're getting here is\nan estimate of the Q function that says, if I start in this\nstate, and I take this action",
    "start": "644330",
    "end": "649420"
  },
  {
    "text": "and I roll out under my\nsimulation policy, what is my expected return?",
    "start": "649420",
    "end": "654510"
  },
  {
    "text": "So that just gives me an\nestimate of the Q function. We've seen this before\nwith Monte Carlo methods.",
    "start": "654510",
    "end": "660220"
  },
  {
    "text": "And then you can just pick\nwhatever the real action is, like this is what\nyou're going to take in the real world with\nthe maximum value.",
    "start": "660220",
    "end": "667420"
  },
  {
    "text": "And what you can think\nof what this is doing, and I'm just going to\naugment this with pi to make that even more clear,\nis what I'm essentially",
    "start": "667420",
    "end": "673840"
  },
  {
    "text": "computing is from the\ncurrent state, what is the Q value of my simulation policy.",
    "start": "673840",
    "end": "679720"
  },
  {
    "text": "And so I could roll out\nunder those policies. And then I'm going to\ndo one step of policy improvement, given that.",
    "start": "679720",
    "end": "686829"
  },
  {
    "text": "And so if someone gave you\na budget of computation, this would be one reasonable\nthing you could do with it. And then we're going\nto see a lot of things",
    "start": "686830",
    "end": "693339"
  },
  {
    "text": "that you can do that are\nmuch better than this. But this is one\nthing you could do that would be viewed sort of as\nsimulation-based search, which",
    "start": "693340",
    "end": "699040"
  },
  {
    "text": "would allow you to do better\nthan the current policy you have. All right.",
    "start": "699040",
    "end": "704170"
  },
  {
    "text": "I think it's helpful to\nthink about this in terms of what the tree structure is. So the idea in this setting\nis that we start in some state",
    "start": "704170",
    "end": "712649"
  },
  {
    "text": "and we're going\nto roll out under and we're going to\ntake a certain action. So that's going to\nbe our action A here.",
    "start": "712650",
    "end": "721130"
  },
  {
    "text": "That's our action A and\nthen after that, we're going to get to a\nnext state, S prime.",
    "start": "721130",
    "end": "728930"
  },
  {
    "text": "And then from then\nonwards, we're going to follow our policy pi. So this here is\npolicy of S prime.",
    "start": "728930",
    "end": "736400"
  },
  {
    "text": "We're going to sample an\naction according to that. So at the root for\nmy current state,",
    "start": "736400",
    "end": "742560"
  },
  {
    "text": "I'm going to consider\nall possible actions. But then after I take\nthat action and transition to a next state, I'm\njust going to roll out",
    "start": "742560",
    "end": "748572"
  },
  {
    "text": "by following my policy pi. And then I do that all the\nway out till I hit a terminal.",
    "start": "748572",
    "end": "754980"
  },
  {
    "text": "So T here equals\na terminal state. ",
    "start": "754980",
    "end": "762740"
  },
  {
    "text": "And so one thing you could do is\njust do the simulation and then average over the root nodes. But you also now have\na whole bunch of data.",
    "start": "762740",
    "end": "769100"
  },
  {
    "text": "So you could do other forms of\nreinforcement learning given that data. Does the pi have to be fairly\noptimal for this [INAUDIBLE]?",
    "start": "769100",
    "end": "778617"
  },
  {
    "text": "Good question. So it depends how we think\nof what this is computing. So, if this is just\ncomputing Q pi of SA,",
    "start": "778617",
    "end": "789650"
  },
  {
    "text": "it's just doing one step\nof policy evaluation. So this will work, whether pi is\na good policy or a bad policy.",
    "start": "789650",
    "end": "796709"
  },
  {
    "text": "But exactly, I think, to your\npoint, if pi isn't very good, then you probably aren't\ngoing to get a very good--",
    "start": "796710",
    "end": "802430"
  },
  {
    "text": "you're just doing one step\nof policy improvement here. You're not necessarily going\nto get Q star unless pi",
    "start": "802430",
    "end": "808790"
  },
  {
    "text": "is really close to optimal. OK, so this is one\nthing you could do, but I think the nice\nthing of visualizing",
    "start": "808790",
    "end": "814490"
  },
  {
    "text": "the tree in this case is it\nstarts to make it really obvious that you could do other things\nthat could be better than just",
    "start": "814490",
    "end": "819740"
  },
  {
    "text": "following whatever\nyour current policy is or whatever policy you\nmight have access to.",
    "start": "819740",
    "end": "826509"
  },
  {
    "text": "And I'll just make this clear\nwith the model and a default",
    "start": "826510",
    "end": "837830"
  },
  {
    "text": "So we might instead, if we have\nlimited amounts of computation, instead of just\ndoing rollouts, we",
    "start": "837830",
    "end": "843529"
  },
  {
    "text": "might want to try\nto get something that's closer to Q star. And one way we\ncould try to do this",
    "start": "843530",
    "end": "848690"
  },
  {
    "text": "is by trying to construct\nan expectimax tree. So raise your hand if you've\nseen either minimax trees",
    "start": "848690",
    "end": "855320"
  },
  {
    "text": "or expectimax trees before. OK, like a few people,\nbut not everybody.",
    "start": "855320",
    "end": "860760"
  },
  {
    "text": "So this will be a quick\nintroduction to those. But the idea is if\nwe think about what",
    "start": "860760",
    "end": "866480"
  },
  {
    "text": "this forward search is,\nreally what we are doing when we construct this tree--",
    "start": "866480",
    "end": "872100"
  },
  {
    "text": "so this is the action. So this could be like,\nsay, A2 and this is a A1m and this is a next state.",
    "start": "872100",
    "end": "878930"
  },
  {
    "text": "Imagine that you just have a\nfew states in these examples. So the black nodes\nare all actions",
    "start": "878930",
    "end": "884420"
  },
  {
    "text": "and the white nodes\nare all the states. You can think of this, and\nwe've seen similar graphs",
    "start": "884420",
    "end": "890160"
  },
  {
    "text": "to this a while ago, think\nof this as just rolling out your Bellman backups. So you could think\nof what happens",
    "start": "890160",
    "end": "896460"
  },
  {
    "text": "in the world is, I\ntake an action and I transition to some state, and\nthen I take another action and I transition to some states\nand sometimes I terminate.",
    "start": "896460",
    "end": "904290"
  },
  {
    "text": "And what I would do\nnormally in this case is, then I would back\nup along this tree.",
    "start": "904290",
    "end": "910600"
  },
  {
    "text": "So whenever I have\nstates, I would take like an average\nor an expectation.",
    "start": "910600",
    "end": "917596"
  },
  {
    "text": " And this is really\njust representing the probability of\nS prime given SA,",
    "start": "917596",
    "end": "925700"
  },
  {
    "text": "So it's representing that sum. And then every time I have\nactions, I would take a max.",
    "start": "925700",
    "end": "933610"
  },
  {
    "text": "And this is just representing\ninside of the Bellman backup that I take the max\nover the actions. So you could think of\nthis as just approximating",
    "start": "933610",
    "end": "940510"
  },
  {
    "text": "the max over, max over ARSA\nplus gamma sum over S prime,",
    "start": "940510",
    "end": "950410"
  },
  {
    "text": "probability of S prime given\nSA B of S prime, except for instead of having\nB of S prime, then",
    "start": "950410",
    "end": "956470"
  },
  {
    "text": "you would just expand\nthis out all the way until you hit the\nterminal state. And this would require\nus also to keep",
    "start": "956470",
    "end": "963020"
  },
  {
    "text": "track of the rewards we obtain\nas we go down this tree. So, for example, here you\nmight get reward of S prime A.",
    "start": "963020",
    "end": "974410"
  },
  {
    "text": "So does that make sense? So if you have access to\na Markov decision process and its dynamics model\nand its reward model,",
    "start": "974410",
    "end": "980270"
  },
  {
    "text": "one way you could use\nthat to figure out what's the optimal thing\nto do in your current state",
    "start": "980270",
    "end": "985480"
  },
  {
    "text": "is you build this tree. Build this tree\nuntil at a leaf you",
    "start": "985480",
    "end": "990580"
  },
  {
    "text": "reach a terminal node or\nfor a fixed horizon H, and then you back\nup by doing wherever",
    "start": "990580",
    "end": "996940"
  },
  {
    "text": "you see a branching\naccording to states, you take an average weighted by\nthe probability of each state.",
    "start": "996940",
    "end": "1002590"
  },
  {
    "text": "And whenever you get to a set of\naction nodes, you take the max. Anybody have any\nquestions about that?",
    "start": "1002590",
    "end": "1009235"
  },
  {
    "text": "We might get to this later. But if we're considering\ncomplex games, like go,",
    "start": "1009235",
    "end": "1014760"
  },
  {
    "text": "the state space is\nlike massive, right? It's very unlikely\nthat you're going to run into the same\ncomposition of the board twice.",
    "start": "1014760",
    "end": "1021597"
  },
  {
    "text": "Like, how do you deal with that? Great question. Hold on to that. We'll get to it. Yes, yeah, absolutely. So right now, well, and in\nfact, on the next slide,",
    "start": "1021597",
    "end": "1027886"
  },
  {
    "text": "we'll talk about how\nbig this tree is. But this, at least conceptually,\nshould be something that you think, yeah,\nwe could do this. I could imagine doing this.",
    "start": "1027887",
    "end": "1034140"
  },
  {
    "text": "So why might this be\nbetter than before? Well, this might be better\nthan before because you're not actually solving the whole MDP.",
    "start": "1034140",
    "end": "1039810"
  },
  {
    "text": "You're only doing sort\nof Bellman backups starting from the\ncurrent state you're in. And so you might imagine that\nif the space is enormous,",
    "start": "1039810",
    "end": "1046988"
  },
  {
    "text": "even though you're sort\nof rolling this out in terms of this kind of\nexponentially growing tree, it still might be smaller\nthan your whole state space.",
    "start": "1046988",
    "end": "1053950"
  },
  {
    "text": "But as I was saying,\nthis is huge in general.",
    "start": "1053950",
    "end": "1060250"
  },
  {
    "text": "So if you want to\nactually expand the whole tree in general,\nthe size of the tree is going to scale by the\nsize of your state space",
    "start": "1060250",
    "end": "1066390"
  },
  {
    "text": "times the size of your\naction space to the H. H here would be her horizon.",
    "start": "1066390",
    "end": "1071950"
  },
  {
    "text": "And so as you\ncould imagine, this is going to be terrible\nreally quickly. We don't want to-- if you think\nabout Go or you think about",
    "start": "1071950",
    "end": "1079480"
  },
  {
    "text": "Mountain Car or other\ngames where you might be-- or other environments where\nyou might be having sort of 100",
    "start": "1079480",
    "end": "1084640"
  },
  {
    "text": "or to 1,000 steps, this is going\nto be completely intractable. But as you might notice\nwhen we're looking at this,",
    "start": "1084640",
    "end": "1093309"
  },
  {
    "text": "here, when we wrote it out,\nwe thought about all the next states we could reach.",
    "start": "1093310",
    "end": "1098780"
  },
  {
    "text": "But if that's a\nreally large set, we know that we don't\nnecessarily actually have to sample all of them\nand compute that exactly",
    "start": "1098780",
    "end": "1105679"
  },
  {
    "text": "in order to get a good\nestimate of the expectation. We know that, in fact,\nwe can just sample. So if you sample, what's\nthe next state 100 times",
    "start": "1105680",
    "end": "1113570"
  },
  {
    "text": "and average over\nall of their values. That's a pretty\ngood approximation of what the average\nvalue is, even",
    "start": "1113570",
    "end": "1118910"
  },
  {
    "text": "if there are 10 billion states. Because you can\napproximate an expectation",
    "start": "1118910",
    "end": "1124400"
  },
  {
    "text": "by an average and that tends\nto concentrate really quickly. So that's going to be one of\nthe really big ideas of using",
    "start": "1124400",
    "end": "1131450"
  },
  {
    "text": "Monte Carlo Tree Search is\nthat we're not going to have to expand all the next states. We're just going to sample them.",
    "start": "1131450",
    "end": "1137299"
  },
  {
    "text": "So let's see how\nthat might work. So this is where we get into\nMonte Carlo tree search.",
    "start": "1137300",
    "end": "1142590"
  },
  {
    "text": "And note I highlighted\na tree here because we're not doing\nMonte Carlo search anymore. We're not just rolling\nout with a policy.",
    "start": "1142590",
    "end": "1148650"
  },
  {
    "text": "We're essentially going to try\nto sample parts of that tree. But we're not going to\njust do single pi rollouts.",
    "start": "1148650",
    "end": "1156690"
  },
  {
    "text": "So we're going to build\na search tree rooted at the current state. We're going to sample\nactions the next states,",
    "start": "1156690",
    "end": "1163320"
  },
  {
    "text": "and we're going to explore\ndifferent parts of that tree. We're not going to always follow\nthe same simulation policy pi.",
    "start": "1163320",
    "end": "1170700"
  },
  {
    "text": "OK. And then after the\nsearch is finished, we're going to take an\naction in the real world",
    "start": "1170700",
    "end": "1176040"
  },
  {
    "text": "by whatever has the\nhighest value, as we estimate at the root. At least that's one\nway we could do things.",
    "start": "1176040",
    "end": "1181690"
  },
  {
    "text": "We'll see some\nother ways to do it.  And, well, let me just give\na little bit of intuition",
    "start": "1181690",
    "end": "1190620"
  },
  {
    "text": "of why does this work. This works because what\nwe're doing in this case is we are approximating.",
    "start": "1190620",
    "end": "1197355"
  },
  {
    "text": " expectations with averages.",
    "start": "1197355",
    "end": "1209220"
  },
  {
    "text": "OK. So we're not actually trying\nto expand all the next state. We're just going to\napproximate it with averages. And that will turn out to\nconcentrate pretty quickly.",
    "start": "1209220",
    "end": "1216059"
  },
  {
    "text": "And that's going to\nbe really helpful. So let's do a quick\ncheck your understanding. So oops.",
    "start": "1216060",
    "end": "1222180"
  },
  {
    "text": "Well, there you go. That's OK. You can think about whether\nor not you agree with this. Monte Carlo Tree Search\ninvolves deciding on an action",
    "start": "1222180",
    "end": "1228620"
  },
  {
    "text": "to take by doing tree search. So think about whether it's a\ngood choice for short horizon",
    "start": "1228620",
    "end": "1234500"
  },
  {
    "text": "problems and why, long horizon,\nand large state and action space.",
    "start": "1234500",
    "end": "1239770"
  },
  {
    "text": " And actually the middle of\nthis is slightly debatable.",
    "start": "1239770",
    "end": "1246130"
  },
  {
    "text": "So take a second and\nthink about this. ",
    "start": "1246130",
    "end": "1256259"
  },
  {
    "text": "Uncover this so that when I\nupload it later, people can-- ",
    "start": "1256260",
    "end": "1281520"
  },
  {
    "text": "So why might the\nfirst part be false? Why would we not want\nto do this further? Well, first of all, does anybody\nhave any questions on what",
    "start": "1281520",
    "end": "1288838"
  },
  {
    "text": "Monte Carlo Tree Search\nis doing in terms of how it's different than the\nother things that we could do.",
    "start": "1288838",
    "end": "1294875"
  },
  {
    "start": "1294875",
    "end": "1300940"
  },
  {
    "text": "So then tell me why\nit's not probably a good choice for\na short horizon problems with small\nstate and action spaces.",
    "start": "1300940",
    "end": "1307610"
  },
  {
    "text": "What would you do\ninstead in those cases? Yeah, [INAUDIBLE]",
    "start": "1307610",
    "end": "1313549"
  },
  {
    "text": "We better do what? Monte Carlo [INAUDIBLE]. So maybe. I guess what I was thinking\nmore is in that case,",
    "start": "1313550",
    "end": "1319792"
  },
  {
    "text": "maybe you should just\ndo dynamic programming. Yeah if the state space and the\naction space is really small, you can just do value iteration.",
    "start": "1319792",
    "end": "1326740"
  },
  {
    "text": "Yeah. Monte Carlo search\ncould work too. But in particular, if things are\nreally small, if you think back,",
    "start": "1326740",
    "end": "1333740"
  },
  {
    "text": "it's been a long time\nI know, but in Monte in standard dynamic\nprogramming, it's",
    "start": "1333740",
    "end": "1340570"
  },
  {
    "text": "only like S squared\ntimes A for each backup. And then you're just doing that. If you're only just doing\nthe H times, that's nice.",
    "start": "1340570",
    "end": "1347505"
  },
  {
    "text": "You don't have any exponential\ndependence in that case. So if it's really small. ",
    "start": "1347505",
    "end": "1356950"
  },
  {
    "text": "Just do Bellman backups. ",
    "start": "1356950",
    "end": "1364340"
  },
  {
    "text": "And the order of that is\nroughly A squared A times the horizon H, roughly.",
    "start": "1364340",
    "end": "1370790"
  },
  {
    "text": "So at least it avoids\nthe exponential. It will be a good choice\nfor long horizon problems",
    "start": "1370790",
    "end": "1376340"
  },
  {
    "text": "with a large state space and\naction, a small action space.",
    "start": "1376340",
    "end": "1381953"
  },
  {
    "text": "Because what we're doing\nin this case is we're approximating that\nexpectation by samples.",
    "start": "1381953",
    "end": "1386960"
  },
  {
    "text": "So we approximate-- so this\nis true and this is false. Approximating an\nexpectation by samples.",
    "start": "1386960",
    "end": "1401270"
  },
  {
    "text": "And so that means\ninstead of us having to get that like\nenormous state space that we're multiplying by,\nwhether S squared or such,",
    "start": "1401270",
    "end": "1407640"
  },
  {
    "text": "we're just sampling from that. And so we can have\nsomething that's more like a constant\nwith respect to how much we're sampling.",
    "start": "1407640",
    "end": "1413600"
  },
  {
    "text": "Now the middle one is actually\na little bit controversial. And we're going to see\ndifferent ways to tackle this.",
    "start": "1413600",
    "end": "1418980"
  },
  {
    "text": "Why should this be\nsomewhat controversial? Well, in Monte\nCarlo Tree Search, the initial way we're\ngetting the big gain",
    "start": "1418980",
    "end": "1424610"
  },
  {
    "text": "is we're sampling next states\ninstead of enumerating them. But it shouldn't be obvious that\nfor actions we want to maximize.",
    "start": "1424610",
    "end": "1432210"
  },
  {
    "text": "For actions, we want to take\nthe best over all the actions. And so Monte Carlo Tree\nSearch a priori still",
    "start": "1432210",
    "end": "1437870"
  },
  {
    "text": "has to just sample the\nwhole action space. And so it's not clear yet that\nunless we do something special,",
    "start": "1437870",
    "end": "1443878"
  },
  {
    "text": "that Monte Carlo tree\nsearch is necessarily going to help us when we've\ngot really big action spaces. Because in general we've\nreplaced the expectation",
    "start": "1443878",
    "end": "1451340"
  },
  {
    "text": "by a set of samples. But it hasn't told us yet\nhow to do anything smart in terms of the action space.",
    "start": "1451340",
    "end": "1457510"
  },
  {
    "text": "So this one is sort of\ndebatable, may be false.",
    "start": "1457510",
    "end": "1464448"
  },
  {
    "text": "Depends how you think about it. But of course, there\nare a lot of algorithms that combine with\nMonte Carlo Tree Search to show us how we might be\nable to tackle this problem.",
    "start": "1464448",
    "end": "1472180"
  },
  {
    "text": "So what we really\nwant to be able to do is solve long horizon problems\nwith enormous action spaces and enormous state spaces.",
    "start": "1472180",
    "end": "1478580"
  },
  {
    "text": "So we're going to need ideas\nbeyond Monte Carlo Tree Search to tackle that.",
    "start": "1478580",
    "end": "1483730"
  },
  {
    "text": "An Upper Confidence Tree Search\nis one idea for how to do this. And I think UCT came out in\naround maybe like 2007, 2008.",
    "start": "1483730",
    "end": "1493240"
  },
  {
    "text": "People started using\nit for Go around then. And the idea in this\ncase is that in addition to doing the sampling\nover next states,",
    "start": "1493240",
    "end": "1500690"
  },
  {
    "text": "let's be strategic over\nwhat action we take when we're expanding in our tree.",
    "start": "1500690",
    "end": "1506590"
  },
  {
    "text": "So when we decide to\nsample, the next action doesn't have to be from\na default policy pi.",
    "start": "1506590",
    "end": "1512380"
  },
  {
    "text": "Let's think carefully\nabout essentially where do we want to\nfill in our search tree.",
    "start": "1512380",
    "end": "1517600"
  },
  {
    "text": "And this is one of those\nother really big ideas. Because this is\nreally where we're going to start to think about\nideas from reinforcement",
    "start": "1517600",
    "end": "1523090"
  },
  {
    "text": "learning essentially to\noptimize computation. Because right now\nwe're still assuming",
    "start": "1523090",
    "end": "1529150"
  },
  {
    "text": "that we know the MDP, that we\nknow what the dynamics model is and we know what\nthe reward model is.",
    "start": "1529150",
    "end": "1534590"
  },
  {
    "text": "So in theory, if\ncomputation was no issue, we could just do value backups. The challenge is this is going\nto be completely enormous",
    "start": "1534590",
    "end": "1541540"
  },
  {
    "text": "and thus totally intractable. So the idea here is to say,\nwell, maybe if we have access",
    "start": "1541540",
    "end": "1547330"
  },
  {
    "text": "to those, we can\nstill think of trying to approximate sort of\nlike Bellman backups or approximate maxes.",
    "start": "1547330",
    "end": "1553100"
  },
  {
    "text": "But we don't\nactually have to want to enumerate all\nthe actions as much, and we want to\nreally focus where",
    "start": "1553100",
    "end": "1558160"
  },
  {
    "text": "we're using our computation. And DeepMind has\nbeen really a pioneer in thinking about using\nreinforcement learning",
    "start": "1558160",
    "end": "1564790"
  },
  {
    "text": "to prioritize computation,\nto solve a lot of really important problems. And I'll try to come\nback to that at the end.",
    "start": "1564790",
    "end": "1571549"
  },
  {
    "text": "OK, so how does UCT work? The idea is, and this is why\nI asked you guys about this",
    "start": "1571550",
    "end": "1577720"
  },
  {
    "text": "and refresh your\nunderstanding is, we are going to treat each\nnode, where each node that was sort of like a state node inside\nof our tree search as a bandit.",
    "start": "1577720",
    "end": "1587274"
  },
  {
    "text": "And so it's like we have many,\nmany, many, many bandit problems inside of our search tree.",
    "start": "1587275",
    "end": "1592450"
  },
  {
    "text": "And we're going to then maintain\nan upper confidence bound of the reward of each\narm inside of a node.",
    "start": "1592450",
    "end": "1599110"
  },
  {
    "text": "So the first node, you would\nhave is your root node.",
    "start": "1599110",
    "end": "1604620"
  },
  {
    "text": " All right. And so it would have,\nsay, A1, A2, A3,",
    "start": "1604620",
    "end": "1611880"
  },
  {
    "text": "and we would think of that as\na MAB, as a multi-armed bandit. And then when you get\nfurther down in the tree,",
    "start": "1611880",
    "end": "1618720"
  },
  {
    "text": "so let's say we this\ngoes to next S prime. This would be another.",
    "start": "1618720",
    "end": "1623830"
  },
  {
    "text": "A1, A2, A3.  And this would be another\nmulti-armed bandit.",
    "start": "1623830",
    "end": "1631000"
  },
  {
    "text": "And you would have you'd have\nto store in memory lots and lots and lots of different\nmulti-armed bandits.",
    "start": "1631000",
    "end": "1636490"
  },
  {
    "text": "So you're maintaining huge\nnumbers of multi-armed bandits. And just like what we normally\ndo in upper confidence bound,",
    "start": "1636490",
    "end": "1642530"
  },
  {
    "text": "we're going to maintain an upper\nconfidence bound over each arm. But we're going to be thinking\nof that as essentially what",
    "start": "1642530",
    "end": "1648429"
  },
  {
    "text": "would happen if I\ntake this action and then act optimally\ntill the end.",
    "start": "1648430",
    "end": "1655363"
  },
  {
    "text": "Now, one big challenge\nis, of course, we don't know what the reward\nwould be of acting optimally. So there's going to be a\nlot of different policies",
    "start": "1655363",
    "end": "1663280"
  },
  {
    "text": "that are moving at once. But let's see what\nthat might look like. So here's the idea.",
    "start": "1663280",
    "end": "1669120"
  },
  {
    "text": "So let's say what\nwe're going to call. We're going to say\nwe have a node I. So this could be our root node\nor it could be any other node.",
    "start": "1669120",
    "end": "1676030"
  },
  {
    "text": "The way we are going to I'm\njust going to call this AI. We're going to try to maintain\nan upper confidence bound",
    "start": "1676030",
    "end": "1684200"
  },
  {
    "text": "over what is the potential\nexpected discounted sum of rewards we'd get\nstarting in this node",
    "start": "1684200",
    "end": "1690230"
  },
  {
    "text": "and taking this action\nas the following. Let's say that we've been\nin that particular node",
    "start": "1690230",
    "end": "1696530"
  },
  {
    "text": "and we have rolled\nout from it using some strategy that\nwe haven't really talked about yet, NIA times.",
    "start": "1696530",
    "end": "1702778"
  },
  {
    "text": "So this is the number of times\nwe've been to this node before and we've happened to\nexpand the A action.",
    "start": "1702778",
    "end": "1709250"
  },
  {
    "text": "What we do is we look\nat all the returns we've gotten under those cases. So what's a return again?",
    "start": "1709250",
    "end": "1714930"
  },
  {
    "text": "So a return would be,\nyou go back to here. ",
    "start": "1714930",
    "end": "1721200"
  },
  {
    "text": "So let's say you've\ndone this, OK. What you would do, what your\nreturn in this case would be is",
    "start": "1721200",
    "end": "1726390"
  },
  {
    "text": "it would be a sum of\nall of these rewards you've gotten along the way.",
    "start": "1726390",
    "end": "1732330"
  },
  {
    "text": "So G we're going to use\nto denote the return. So this would be, this reward\nfrom starting in state A,",
    "start": "1732330",
    "end": "1739850"
  },
  {
    "text": "taking A1, and getting\nthe rewards out to the terminal state. And maybe next time you\ngo down this action,",
    "start": "1739850",
    "end": "1747180"
  },
  {
    "text": "you actually get to here and\nyou get a different return. So those are just like your\nMonte Carlo returns from before.",
    "start": "1747180",
    "end": "1753620"
  },
  {
    "text": "And it's just for\nall the other times you went through that action. So that's part of it.",
    "start": "1753620",
    "end": "1760400"
  },
  {
    "text": "So that's just an average and\nit's kind of a weird average because it might be that\nyour which nodes you visited",
    "start": "1760400",
    "end": "1765970"
  },
  {
    "text": "and which actions you\ntook have changed. So we're not committing it to\nit to be a particular policy. It's just like we've\ntaken some action",
    "start": "1765970",
    "end": "1773170"
  },
  {
    "text": "and we've followed-- we've\nmade a series of decisions until we got to\na terminal state. We added up the rewards and\nwe keep track of that here.",
    "start": "1773170",
    "end": "1780530"
  },
  {
    "text": "So that's one thing and that's\nsort of a we probably look at it and think this is a very\nloose approximation of what",
    "start": "1780530",
    "end": "1785620"
  },
  {
    "text": "the optimal Q value is\nfor that state in action. The second term looks like upper\nconfidence bound, which is,",
    "start": "1785620",
    "end": "1794034"
  },
  {
    "text": "you have some constant C/\nyou have some log term, which depends on the number\nof times you visited this node",
    "start": "1794035",
    "end": "1800500"
  },
  {
    "text": "divided by NIA, number of\ntimes we've been in that node and taken that\nparticular action.",
    "start": "1800500",
    "end": "1806770"
  },
  {
    "text": "So this just looks\nlike a bandit term. It's an upper confidence bound\nover the reward that we can get.",
    "start": "1806770",
    "end": "1812909"
  },
  {
    "text": " And so what Upper\nConfidence Tree does",
    "start": "1812910",
    "end": "1818400"
  },
  {
    "text": "is that the way it picks\nthe action, the next action to take from the current node is\nit picks whichever one of these",
    "start": "1818400",
    "end": "1824820"
  },
  {
    "text": "has this higher upper\nconfidence bound. Now, it should seem\nslightly suspicious",
    "start": "1824820",
    "end": "1830690"
  },
  {
    "text": "that this works because in\nbandits, when we took an action, we knew that this really was an\nunbiased estimate of the reward",
    "start": "1830690",
    "end": "1839030"
  },
  {
    "text": "of that action because we just\nwould see that one action. And then we knew from\nHoeffding that this really was an upper confidence bound\non the true value of that arm.",
    "start": "1839030",
    "end": "1848900"
  },
  {
    "text": "But now we're in a\nmuch more weird case, where we are thinking of this\nfor a sequence of actions we're going to take.",
    "start": "1848900",
    "end": "1854640"
  },
  {
    "text": "We're trying to do\nexpectations over states, and the actual actions we're\ntaking from this node onwards",
    "start": "1854640",
    "end": "1860570"
  },
  {
    "text": "may not be optimal. One time we might go through-- I'll draw it on the board.",
    "start": "1860570",
    "end": "1868290"
  },
  {
    "text": "Like one time we might\ngo through this zig zag. Another time we might\ngo through this zig zag.",
    "start": "1868290",
    "end": "1874150"
  },
  {
    "text": "Another time we\nmight come back here and then we take a\ndifferent action. So it's not like we're not doing\none step of policy improvement",
    "start": "1874150",
    "end": "1881250"
  },
  {
    "text": "here. We just have lots of different\nthings that we're trying and we're averaging over them.",
    "start": "1881250",
    "end": "1886920"
  },
  {
    "text": "So you should be slightly\nsuspicious whether or not this is going to be\ndoing a reasonable thing.",
    "start": "1886920",
    "end": "1892770"
  },
  {
    "text": "But it's certainly\nsomething you could do, something you could\nimagine coding. And then we'll do\nthis many, many times.",
    "start": "1892770",
    "end": "1899860"
  },
  {
    "text": "And then at the very end-- so this will expand essentially\ndifferent parts of your tree. And when you're following\nthis, in particular,",
    "start": "1899860",
    "end": "1907029"
  },
  {
    "text": "you're going to start to\nexpand parts of the tree which look promising more.",
    "start": "1907030",
    "end": "1912670"
  },
  {
    "text": "So if this one happens\nto have been getting-- this one gets like\nplus 100 and this gets",
    "start": "1912670",
    "end": "1917970"
  },
  {
    "text": "plus 100 and this gets plus 90. Whereas let's say one other\ntime when you took this action,",
    "start": "1917970",
    "end": "1924550"
  },
  {
    "text": "you went down here\nand you got minus 10, well, then when the next time\nyou get to your root node,",
    "start": "1924550",
    "end": "1929570"
  },
  {
    "text": "you're probably going to be\nmore likely to keep going down this path. So it's going to selectively\nexpand parts of your tree.",
    "start": "1929570",
    "end": "1938440"
  },
  {
    "text": "It's not going to hit and you'll\nhave these unbalanced trees where you'll often see parts of\nthings are getting filled in.",
    "start": "1938440",
    "end": "1944997"
  },
  {
    "text": "And then maybe if something\nelse becomes more promising, you'll switch to\nanother part of the tree and fill in things there.",
    "start": "1944997",
    "end": "1951159"
  },
  {
    "text": "So it's sort of this\nunbalanced construction of your forward search tree. And the way that\nit's unbalanced is",
    "start": "1951160",
    "end": "1956710"
  },
  {
    "text": "that using the Monte Carlo\naspect to approximate all the expectations. And you're using\nthis upper confidence",
    "start": "1956710",
    "end": "1963160"
  },
  {
    "text": "bound to of selectively\nprioritize across your actions. And that's what's going to help\nwith our enormous action space.",
    "start": "1963160",
    "end": "1971830"
  },
  {
    "text": "Now, you still\nmight be concerned that when there's a really\nenormous number of actions, like we're going to see\nin Go in other cases,",
    "start": "1971830",
    "end": "1978290"
  },
  {
    "text": "that this still isn't\ngoing to be enough, because if the number of\nactions you have is a million,",
    "start": "1978290",
    "end": "1987429"
  },
  {
    "text": "these things generally-- you'll have 0 for\nthis part right before you have\ntaken any actions",
    "start": "1987430",
    "end": "1993250"
  },
  {
    "text": "and your counts will\nall be the same. So, it should still be\nconcerning because what should",
    "start": "1993250",
    "end": "1999039"
  },
  {
    "text": "you do if you have these\n1,000 different actions, and like you might not be able\nto do anything essentially until",
    "start": "1999040",
    "end": "2005340"
  },
  {
    "text": "you've visited everything once. Because before then,\nas long as you've defined something that's a\nreasonable upper confidence",
    "start": "2005340",
    "end": "2012300"
  },
  {
    "text": "bound, everything is\ngoing to look awesome. It's like action 99 will be\nawesome/ action 100 will be",
    "start": "2012300",
    "end": "2017320"
  },
  {
    "text": "awesome. And so you'll have to sample\nall of them at least once, and that generally will\nbe completely intractable.",
    "start": "2017320",
    "end": "2023039"
  },
  {
    "text": "So we'll see ways to\nfurther reduce this. But what you can think of\nthis part is doing is saying,",
    "start": "2023040",
    "end": "2028600"
  },
  {
    "text": "well, if you can at least\nsample every action once, you can at least\nmean that you're not going to have to focus on\nunpromising actions later",
    "start": "2028600",
    "end": "2035678"
  },
  {
    "text": "because you're going to quickly\nuse this upper confidence bound. ",
    "start": "2035678",
    "end": "2043830"
  },
  {
    "text": "So these sort of Monte\nCarlo Tree Searches are starting to look\nreally promising. A lot of people have\nused tree search",
    "start": "2043830",
    "end": "2050129"
  },
  {
    "text": "based algorithms,\nas some of you might have seen in other machine\nlearning algorithms or other AI classes probably in particular.",
    "start": "2050130",
    "end": "2057030"
  },
  {
    "text": "But what this Monte Carlo\nand UCT based approaches is, there's this highly\nselective best first search,",
    "start": "2057030",
    "end": "2063158"
  },
  {
    "text": "but with simulations as well. And they're using\nsampling to break",
    "start": "2063159",
    "end": "2068969"
  },
  {
    "text": "the curse of dimensionality. On UCT and UCT to help\nwith large action spaces.",
    "start": "2068969",
    "end": "2083713"
  },
  {
    "start": "2083714",
    "end": "2089270"
  },
  {
    "text": "And the other really nice\nbenefit of these ones is they're parallelisable So\nwhen you're sampling things,",
    "start": "2089270",
    "end": "2097863"
  },
  {
    "text": "you could certainly\nimagine trying to expand, do these sort of\nrollouts many, many times",
    "start": "2097863",
    "end": "2103480"
  },
  {
    "text": "and then collect the results. So you can start to parallelize\nthese methods as well. And that's going to\nbe really helpful.",
    "start": "2103480",
    "end": "2110670"
  },
  {
    "text": "So that's the background\nbetween Monte Carlo Tree Search. But now, of course, the\nreally big breakthrough",
    "start": "2110670",
    "end": "2117430"
  },
  {
    "text": "that this allowed or\npeople built on these ideas is to achieve AlphaGo, AlphaGo\nand then AlphaZero and then",
    "start": "2117430",
    "end": "2124599"
  },
  {
    "text": "MuZero. So there's a whole\nsequence of them. And let's just get up a\nmovie for that for a second.",
    "start": "2124600",
    "end": "2131162"
  },
  {
    "start": "2131163",
    "end": "2137120"
  },
  {
    "text": "And who here has played Go. OK a few people. I was thinking it could be\nfun to have us all play it",
    "start": "2137120",
    "end": "2143495"
  },
  {
    "text": "so we could see that\nit's really quite hard. But another time.",
    "start": "2143495",
    "end": "2149650"
  },
  {
    "text": "Go is the world's oldest\ncontinuously played board game. It is one of the simplest\nand also most abstract.",
    "start": "2149650",
    "end": "2158650"
  },
  {
    "text": "Beating a professional\nplayer at Go is a long standing challenge\nof artificial intelligence.",
    "start": "2158650",
    "end": "2163790"
  },
  {
    "text": " Everything we've\never tried in AI just falls over when\nyou try the game of Go.",
    "start": "2163790",
    "end": "2170380"
  },
  {
    "text": "The number of possible\nconfigurations of the board is more than the number\nof atoms in the universe.",
    "start": "2170380",
    "end": "2175810"
  },
  {
    "text": "AlphaGo found a way to\nlearn how to play Go. ",
    "start": "2175810",
    "end": "2181350"
  },
  {
    "text": "Building suspense. We'll see how the network goes. This is a documentary\nof DeepMind's efforts",
    "start": "2181350",
    "end": "2187200"
  },
  {
    "text": "to try to beat the world\nclass people in Go. Let me see if I\ncan make it work. I think it's probably\ndecided it doesn't",
    "start": "2187200",
    "end": "2193320"
  },
  {
    "text": "like the internet right now. Let's double check if\nI can get that to work.",
    "start": "2193320",
    "end": "2199220"
  },
  {
    "text": "So what they ended\nup doing is they are going to use\nreinforcement learning to help solve this problem.",
    "start": "2199220",
    "end": "2204349"
  },
  {
    "text": "We'll see whether or not the\ntechnical difficulties resolve. And then what they\ndid is they started playing against grandmasters\nand they tried to-- then",
    "start": "2204350",
    "end": "2211310"
  },
  {
    "text": "they played against Lee\nSedol, who was one of the best people in the world at Go. And I think one of the really\ninteresting things about this",
    "start": "2211310",
    "end": "2217970"
  },
  {
    "text": "is that it really\nshows that A, it's now possible to use\nAI to beat the best people in the world at Go, but\nalso the types of strategies",
    "start": "2217970",
    "end": "2227000"
  },
  {
    "text": "that it built were very\ndifferent than what people were doing before. And so I think this is a\npretty important aspect for AI",
    "start": "2227000",
    "end": "2234470"
  },
  {
    "text": "because we've often thought of\nAI as sort of automating things that people already\nknow how to do.",
    "start": "2234470",
    "end": "2240500"
  },
  {
    "text": "And I think this illustrated\nthat they're really starting to be places\nwhere computers go beyond even the best humans\nand what we know how to do.",
    "start": "2240500",
    "end": "2249030"
  },
  {
    "text": "And since then, there's\nbeen a recent paper, I think maybe a year\nor two ago by Been Kim trying to look whether or not\nyou can teach grandmasters",
    "start": "2249030",
    "end": "2256910"
  },
  {
    "text": "using the strategies that\nAlphaGo and its descendants",
    "start": "2256910",
    "end": "2261920"
  },
  {
    "text": "invented. And so then there's this\nreally interesting opportunity and question to think\nabout, can we actually",
    "start": "2261920",
    "end": "2267740"
  },
  {
    "text": "learn from computers\nin these new ways and try to exceed both human\nlevel performance and computer",
    "start": "2267740",
    "end": "2272780"
  },
  {
    "text": "level performance. So I will post this later. You guys can look at it.",
    "start": "2272780",
    "end": "2278510"
  },
  {
    "text": "Let's go back to there. OK. All right. So how does Go work?",
    "start": "2278510",
    "end": "2284750"
  },
  {
    "text": "Well, it's a really,\nreally old game. It's considered one of the\nclassic, hardest board games.",
    "start": "2284750",
    "end": "2290600"
  },
  {
    "text": "And it was considered a\ngrand challenge for AI for many, many decades. This sort of game tree\nsearch that we saw before,",
    "start": "2290600",
    "end": "2297890"
  },
  {
    "text": "something like a Ford search. Now, it couldn't be\nexpectimax in this case because it's a two player game.",
    "start": "2297890",
    "end": "2303590"
  },
  {
    "text": "Go is what's considered\na zero sum game, meaning that someone\neither wins and loses.",
    "start": "2303590",
    "end": "2308750"
  },
  {
    "text": "And in this case, whenever\nwe think of a next state, rather than it being\nan expectation, it's really a minimax\nproblem because each opponent",
    "start": "2308750",
    "end": "2316450"
  },
  {
    "text": "is playing to win. So in this case, it's good to\nthink about what is actually",
    "start": "2316450",
    "end": "2323320"
  },
  {
    "text": "uncertain in this case. When we're playing Go, the\nrules of the game are known.",
    "start": "2323320",
    "end": "2328962"
  },
  {
    "text": "They actually have\nanother descendant now where you didn't have to\nknow the rules of the game. But certainly for the first\nfew, the rules of the game",
    "start": "2328962",
    "end": "2334970"
  },
  {
    "text": "were known. So what's unknown\nin Go, if we wanted to think about building a tree\nor trying to learn in this case,",
    "start": "2334970",
    "end": "2345410"
  },
  {
    "text": "if we know the rules-- Yeah. Well, you might expect your\nadversary to play the best move.",
    "start": "2345410",
    "end": "2352579"
  },
  {
    "text": "That might not always be true. They might be seeking\ndifferent strategies, so you wouldn't know that.",
    "start": "2352580",
    "end": "2357829"
  },
  {
    "text": "It's a good point. So it might be as\n[MUTED] saying, that you might not know exactly\nwhat the best strategy is,",
    "start": "2357830",
    "end": "2363580"
  },
  {
    "text": "or you might not know\nwhether someone's going to play the best strategy. I think the other\nthing that I think of is that we don't always know\nwhat the best strategy is.",
    "start": "2363580",
    "end": "2370822"
  },
  {
    "text": "It's just incredibly hard to\ncompute this in this case. And so in this case,\nthat's sort of next state,",
    "start": "2370822",
    "end": "2376460"
  },
  {
    "text": "if that next state is\nreally from an adversary, it's not clear you've\ngot stochasticity in that because you don't know\nwhat the optimal game is.",
    "start": "2376460",
    "end": "2383980"
  },
  {
    "text": "Now, of course, once\nsomeone picks a move, everything is deterministic. So in some ways it's all\ndeterministic, it's all known.",
    "start": "2383980",
    "end": "2390080"
  },
  {
    "text": "The key thing is that because\nit's this adversarial game, it's not clear what the\noptimal strategy is.",
    "start": "2390080",
    "end": "2395680"
  },
  {
    "text": "So that's kind of one of\nthe really hard parts. All right. Just a couple basics\nof the rules of Go.",
    "start": "2395680",
    "end": "2401690"
  },
  {
    "text": "So normally it's played\non a 19 by 19 board. But when people first\nstarted to-- well, kids and also when researchers\nstarted tackling this game",
    "start": "2401690",
    "end": "2409760"
  },
  {
    "text": "in earnest, starting like the\nlate 2000s or 2000, David Silva, who's one of the authors of\nthis and an amazing researcher,",
    "start": "2409760",
    "end": "2416579"
  },
  {
    "text": "I think as part of his PhD\nin the late 2008, 2009, he was doing things\non a 9 by 9 board.",
    "start": "2416580",
    "end": "2423890"
  },
  {
    "text": "And just as a couple of basics,\nthere's two different players, either someone playing the black\nstones or the white stones,",
    "start": "2423890",
    "end": "2430530"
  },
  {
    "text": "and you're trying to surround\nstones that are captured and then you win. It's as I said, it's a\nzero-one game, zero-one game,",
    "start": "2430530",
    "end": "2442349"
  },
  {
    "text": "which means a winner takes all. One of the interesting\nthings about Go is that in general, there's\nno intermediate reward.",
    "start": "2442350",
    "end": "2448519"
  },
  {
    "text": "So you have to play\ntill the end of the game to see who's actually winning. And so there's just\na single reward at the end, which also makes\nit very hard for credit",
    "start": "2448520",
    "end": "2455599"
  },
  {
    "text": "assignment and to understand\nwhat moves caused the resulting game. ",
    "start": "2455600",
    "end": "2462300"
  },
  {
    "text": "Yeah, so AlphaGo and\nAlphaZero, AlphaGo was the first one that was used.",
    "start": "2462300",
    "end": "2468040"
  },
  {
    "text": "Then they developed\na number of variants. They then played\nagainst Lee Sedol, and then there was AlphaZero.",
    "start": "2468040",
    "end": "2474390"
  },
  {
    "text": "And what they\nexhibit in this case is a number of different\nreally interesting features. So they have self-play,\nstrategic computation, highly",
    "start": "2474390",
    "end": "2482640"
  },
  {
    "text": "selective best first search. They use the power of averaging. They leverage local computation.",
    "start": "2482640",
    "end": "2488280"
  },
  {
    "text": "And then they learn\nand update heuristics. For those of you that have\nseen tree search based methods before, you've often\nprobably seen ideas",
    "start": "2488280",
    "end": "2495150"
  },
  {
    "text": "around heuristics, which are\nother ways to think about how do you expand the tree. One of the interesting\nideas in these papers",
    "start": "2495150",
    "end": "2502522"
  },
  {
    "text": "is that they're going to\nlearn those heuristics and update them over time. It's another important aspect.",
    "start": "2502522",
    "end": "2508210"
  },
  {
    "text": "So let's see how it works. So how does self-play work?",
    "start": "2508210",
    "end": "2513930"
  },
  {
    "text": "So the key idea in this\ncase is that we're going to have the agent play itself. So there's going\nto-- you can think",
    "start": "2513930",
    "end": "2520030"
  },
  {
    "text": "of it as there being two copies\nof this same agent right now. And what will happen when\nthey're playing a game",
    "start": "2520030",
    "end": "2526660"
  },
  {
    "text": "is they compute the best\nmove of the current state and then the opponent\ndoes the same. And they have access\nessentially to the same policy",
    "start": "2526660",
    "end": "2533320"
  },
  {
    "text": "or the same sort of algorithm,\nbut they're both just using it in an adversarial way.",
    "start": "2533320",
    "end": "2539020"
  },
  {
    "text": "And so that means the only\nbottleneck in this case is computation. We have no humans involved.",
    "start": "2539020",
    "end": "2544359"
  },
  {
    "text": "And self-play also provides\na well-matched player. So take a second\nand think about,",
    "start": "2544360",
    "end": "2550440"
  },
  {
    "text": "what are the benefits that\nwill happen with self-play and what's going to be\nlike the reward density.",
    "start": "2550440",
    "end": "2557840"
  },
  {
    "text": "Are there going to be lots of\nrewards when you do self-play? Are there going to be\nvery little rewards? Let's just take a second\nand I'll check and see",
    "start": "2557840",
    "end": "2563400"
  },
  {
    "text": "whether I can make\nthe networks work. ",
    "start": "2563400",
    "end": "2573940"
  },
  {
    "text": "Maybe talk to a neighbor and\nsee if you guys both have the same idea of whether\nself-play will be helpful or not. ",
    "start": "2573940",
    "end": "2592750"
  },
  {
    "text": "All right. What does this do\nto policy training? What happens when\nyou do self-play? ",
    "start": "2592750",
    "end": "2601170"
  },
  {
    "text": "Do you have higher\nreward density. Do you have low reward density? What happens? ",
    "start": "2601170",
    "end": "2609953"
  },
  {
    "text": "Here, raise your\nhand if you think you have high reward density. If you think you have\nlow reward density.",
    "start": "2609953",
    "end": "2616770"
  },
  {
    "text": "So, all right. Was somebody who think we\nhave high reward density you want to explain why.",
    "start": "2616770",
    "end": "2622118"
  },
  {
    "text": "That's right. We do a pretty high\nreward density. Why do we get that\nwhen we do self-play? What happens?",
    "start": "2622118",
    "end": "2629577"
  },
  {
    "text": "Or I think it's easy,\nmaybe easiest to think of like if you play against\nsomeone that's much, much",
    "start": "2629577",
    "end": "2634770"
  },
  {
    "text": "better than you, what happens? ",
    "start": "2634770",
    "end": "2640260"
  },
  {
    "text": "Just kind of lose-lose. Lose all the time, right? I mean, everyone's\nprobably done this before. You play against like a friend\nof yours that maybe much",
    "start": "2640260",
    "end": "2646320"
  },
  {
    "text": "better at a board game than\nyou or something like that. Or you're a better friend of\nyours is much better than you at tennis or something, and\nyou go and play with them.",
    "start": "2646320",
    "end": "2652150"
  },
  {
    "text": "And it's not normally\nthat fun because you just lose all the time. And when you lose\nall the time, you",
    "start": "2652150",
    "end": "2657595"
  },
  {
    "text": "may not get very much\nsignal about what things are even\ndoing better or worse at because you always lose. And so that would be a\ncase where the reward",
    "start": "2657595",
    "end": "2664260"
  },
  {
    "text": "density is very low because the\nplayers are really mismatched. And it means that most of the\ntime the agent is not winning.",
    "start": "2664260",
    "end": "2671010"
  },
  {
    "text": "Now the same thing is true\nif the agent is much better than the other agent. But self-play means you're\nsort of matched cheaters,",
    "start": "2671010",
    "end": "2677773"
  },
  {
    "text": "like matched at the same level\nas someone who plays tennis the same level as\nyou, or you're matched with someone who has the\nsame Helo score as you",
    "start": "2677773",
    "end": "2684269"
  },
  {
    "text": "and in chess or Go,\nwhich is sort of a way to quantify the player's skill.",
    "start": "2684270",
    "end": "2689522"
  },
  {
    "text": "And the nice thing about that\nis that you would expect that roughly, if you play someone\nthat's exactly the same level--",
    "start": "2689522",
    "end": "2694530"
  },
  {
    "text": "now here you're an\nRl agent, so you're going to play someone that's\nactually just on the other side. So they're exactly\nthe same level as you,",
    "start": "2694530",
    "end": "2701119"
  },
  {
    "text": "and that means you'd expect\nyou'd win about half the time. I think, on average. So that's really good density\nfor something that is a zero-one",
    "start": "2701120",
    "end": "2710650"
  },
  {
    "text": "game because you're not just\nlike every 3,000 games getting a zero or a one here.",
    "start": "2710650",
    "end": "2717005"
  },
  {
    "text": "About half the time\nyou'd expect to get a one and half the time you'd\nexpect to get a zero. And the reason that\nmight be beneficial",
    "start": "2717005",
    "end": "2722980"
  },
  {
    "text": "is hopefully that's\ngoing to give you a lot more signal\nof how you should change your policy in order to\nfigure out how to get better.",
    "start": "2722980",
    "end": "2730510"
  },
  {
    "text": "So I think that self-play\nis a really interesting one because you could think\nof it in some ways as kind of providing an\nautomatic curriculum.",
    "start": "2730510",
    "end": "2738160"
  },
  {
    "text": "Go to the next one. The rewards are going\nto be pretty dense. And for those of you that have\nseen curriculum learning before",
    "start": "2738160",
    "end": "2744030"
  },
  {
    "text": "in other machine learning stuff,\njust like in classes where you often build up\nwith math over time",
    "start": "2744030",
    "end": "2749490"
  },
  {
    "text": "and you don't start\nwith calculus. You start with addition\nor what a number is, and then you slowly build up.",
    "start": "2749490",
    "end": "2754930"
  },
  {
    "text": "So you're always trying to be\non roughly the right level. Similarly here, the agent\nshould do that automatically",
    "start": "2754930",
    "end": "2761092"
  },
  {
    "text": "because they're\ngoing to start off and they're going to\nboth be terrible at Go, but they're still going to get\npretty high density of reward",
    "start": "2761093",
    "end": "2766960"
  },
  {
    "text": "because they're\nboth terrible at Go. And then over time, the\nagents are going to get better and then now they're\nautomatically",
    "start": "2766960",
    "end": "2773220"
  },
  {
    "text": "always playing an agent that's\nroughly the same level as them. Now, we'll have to see\nwhy the algorithm will",
    "start": "2773220",
    "end": "2779220"
  },
  {
    "text": "help them get better. But intuitively, as we saw, even\nwith the Monte Carlo simulation, not even tree\nsearch, there, it was",
    "start": "2779220",
    "end": "2786089"
  },
  {
    "text": "doing like one step\nof policy improvement. So you can imagine that\neven if each round were just doing of one step of policy\nimprovement, over time,",
    "start": "2786090",
    "end": "2793990"
  },
  {
    "text": "we would hope that we're going\nto get better and better. So this idea of\nself-play I think is a really interesting one.",
    "start": "2793990",
    "end": "2800550"
  },
  {
    "text": "It works really well in games\nand it's been exploited a lot. I've often thought like it would\nbe really interesting to see",
    "start": "2800550",
    "end": "2807130"
  },
  {
    "text": "are there other\nplaces you can set up to essentially be like a game. Because what you\ncould think of here",
    "start": "2807130",
    "end": "2812140"
  },
  {
    "text": "is what self-play is leveraging\nis that sort for the dynamics",
    "start": "2812140",
    "end": "2817660"
  },
  {
    "text": "part of your\nenvironment, you now have a simulator you can plug\nin, which is the agent itself.",
    "start": "2817660",
    "end": "2823900"
  },
  {
    "text": "Now, in general cases,\nyou can't do that. Like if I'm going to\nsimulate patient dynamics,",
    "start": "2823900",
    "end": "2829460"
  },
  {
    "text": "I can't just plug in, like, I\ncan't do self-play for that. An action is how the patient\nresponds to some treatment.",
    "start": "2829460",
    "end": "2837830"
  },
  {
    "text": "And I can't like play\nagainst two patients. That doesn't make sense. But I think in some\ncases here, it's really",
    "start": "2837830",
    "end": "2845500"
  },
  {
    "text": "a very reasonable thing\nto do to use self-play and it can be really efficient. Because you can\nthink of it in a way",
    "start": "2845500",
    "end": "2851590"
  },
  {
    "text": "as like it's changing the\nstrategies in a way that sort of iteratively updating the\ncomplexity of the environment",
    "start": "2851590",
    "end": "2857770"
  },
  {
    "text": "you're trying to solve. OK, so [MUTED]? I think I have a good idea. But what is the exact\ndefinition of reward density.",
    "start": "2857770",
    "end": "2865580"
  },
  {
    "text": "What is it with respect to? A good question. So here I just mean about\nlots of things here.",
    "start": "2865580",
    "end": "2870888"
  },
  {
    "text": "What I mean by reward\ndensity is how often you're going to get a-- you're going to win. And here rewards only\nhappen at the end.",
    "start": "2870888",
    "end": "2879750"
  },
  {
    "text": "So it would just be,\nof the games you play, are you going to\nget a lot of reward? Are you going to get zero?",
    "start": "2879750",
    "end": "2885200"
  },
  {
    "text": "If the agents are really\nmismatched, in general, the reward density is going\nto be either saturated, which means you always\nwin or near zero",
    "start": "2885200",
    "end": "2892733"
  },
  {
    "text": "because you're\nnever going to win. And neither of those\nare very informative. And the idea is that if\nyou're getting reward",
    "start": "2892733",
    "end": "2897765"
  },
  {
    "text": "about half the time,\nthat might be really informative, because you\nget lots of signal of like, that thing worked.",
    "start": "2897765",
    "end": "2903180"
  },
  {
    "text": "That didn't work. That thing worked. That didn't work. And so you're going\nto have a lot of stuff to estimate kind of a\ngradient or an improvement",
    "start": "2903180",
    "end": "2909740"
  },
  {
    "text": "for your decision policy. Yeah. With self-play, can we say that\nif you play against someone",
    "start": "2909740",
    "end": "2918900"
  },
  {
    "text": "who has a completely\nnew strategy, might not be able to generalize\nwell enough, because I",
    "start": "2918900",
    "end": "2924960"
  },
  {
    "text": "was always playing\nagainst myself and always using the\nsame kind of strategies. Great question. So yeah, so that's a really\ngreat question, which is, OK,",
    "start": "2924960",
    "end": "2932230"
  },
  {
    "text": "well, so self-play\nmight be good, but then what if you suddenly\nplay against someone really different. So, what we're going to\nhave to see in this case",
    "start": "2932230",
    "end": "2939420"
  },
  {
    "text": "is whether or not\nover time, you do get to something\nthat's essentially like a minimax policy.",
    "start": "2939420",
    "end": "2946290"
  },
  {
    "text": "So if you get to\nthe optimal policy, you could hope that you\nreally are at like grandmaster",
    "start": "2946290",
    "end": "2952020"
  },
  {
    "text": "or beyond. And one of the\nexciting things here is that they will get to that. So as this ratchets\nup and ratchets up",
    "start": "2952020",
    "end": "2958680"
  },
  {
    "text": "after lots and lots of\ntrading and after using very, very complicated networks,\nyou can get to that level.",
    "start": "2958680",
    "end": "2963920"
  },
  {
    "text": " Does that work-- well, Does\nthat work for games where moves",
    "start": "2963920",
    "end": "2972570"
  },
  {
    "text": "are not deterministic, like, I\ndon't know, like gambling games, like poker or\nsomething where there",
    "start": "2972570",
    "end": "2978900"
  },
  {
    "text": "is some sort of\nprobability associated? Yeah, interesting. So there's also\nbeen a lot of work. There are really, really\ngood AI agents for poker now.",
    "start": "2978900",
    "end": "2986080"
  },
  {
    "text": "I think it was 2019 that Noam\nBrown, had a paper in Science showing that you could beat--",
    "start": "2986080",
    "end": "2992843"
  },
  {
    "text": "I don't know if you could\nbeat, but it was certainly sort of competitive with\ntop humans, I believe.",
    "start": "2992843",
    "end": "2997980"
  },
  {
    "text": "So. Thomas Sandholm and Noam\nBrown, who did his PhD at CMU, had have got an agent\nto do well at poker.",
    "start": "2997980",
    "end": "3004410"
  },
  {
    "text": "The algorithms are slightly\ndifferent, but Yes, you can. It's a good question here.",
    "start": "3004410",
    "end": "3010020"
  },
  {
    "text": "We're assuming that it's\nalso going to leverage the deterministic nature. Yeah, all right.",
    "start": "3010020",
    "end": "3017450"
  },
  {
    "text": "So how does this work? Let's go through what it's doing\nbecause it relates to Upper",
    "start": "3017450",
    "end": "3023420"
  },
  {
    "text": "Confidence Tree Search. But there are many changes. So there are many\nimprovements that were needed for it\nto get much better.",
    "start": "3023420",
    "end": "3029600"
  },
  {
    "text": "But it is going to be\nsimilar in the sense that it's going\nto try to compute. First, we're going\nto start with,",
    "start": "3029600",
    "end": "3034730"
  },
  {
    "text": "it's going to simulate\nmany, many, many games, and it's going to iteratively\ntry to learn better strategies.",
    "start": "3034730",
    "end": "3040572"
  },
  {
    "text": "One of the things that\nis going to be different compared to naive\nUpper Confidence Tree",
    "start": "3040572",
    "end": "3047330"
  },
  {
    "text": "is that we're going to actually\nmaintain a neural network. So let me just\nget back to there. ",
    "start": "3047330",
    "end": "3055500"
  },
  {
    "text": "So what we're going\nto do in this case is we're going to have a neural\nnetwork that, given a state",
    "start": "3055500",
    "end": "3065640"
  },
  {
    "text": "can produce both an\nestimate of V of S and a policy distribution\nfor that state over actions.",
    "start": "3065640",
    "end": "3073950"
  },
  {
    "text": "So we're going to maintain\na single neural network. This is what AlphaZero does. It maintains a\nsingle neural network",
    "start": "3073950",
    "end": "3079260"
  },
  {
    "text": "that, given an input\nstate, will output both an estimate of\nthe value of that state and a policy for that state,\na distribution over actions.",
    "start": "3079260",
    "end": "3088290"
  },
  {
    "text": "And we're going to talk about\nhow we train that shortly. But for now, just assume\nto start that we've already",
    "start": "3088290",
    "end": "3093750"
  },
  {
    "text": "trained that or that\nwe have access to that and we're going to use that now,\nwhen we're going to do a number,",
    "start": "3093750",
    "end": "3100120"
  },
  {
    "text": "we're going to play\na number of games. And in particular,\nlet's first think about how we're going to compute\nthe first move in a single game.",
    "start": "3100120",
    "end": "3106837"
  },
  {
    "text": "So we're going to\ndo some self-- we're going to do self-play in\nthis case between two agents. Value's the same.",
    "start": "3106837",
    "end": "3112308"
  },
  {
    "text": "What we're going to do is we're\ngoing to do an Upper Confidence Bound based thing. OK, what does these\nupper confidence",
    "start": "3112308",
    "end": "3117780"
  },
  {
    "text": "bounds going to be based on? And then we're and\nthen we're going to decide the max between them. So this is going to look like\nUCT, but slightly different.",
    "start": "3117780",
    "end": "3126920"
  },
  {
    "text": "So what U is going to be equal\nto, in this case, so U of IA, so let's say this\nis node I. It's",
    "start": "3126920",
    "end": "3135010"
  },
  {
    "text": "going to be proportional\nto the following. It's going to be\nproportional to PSA.",
    "start": "3135010",
    "end": "3141250"
  },
  {
    "text": "It is divided by one plus FSA.",
    "start": "3141250",
    "end": "3150350"
  },
  {
    "text": "This is from our policy.  Look, I'll write it as this.",
    "start": "3150350",
    "end": "3156090"
  },
  {
    "text": " So, this is from\nour policy network.",
    "start": "3156090",
    "end": "3162690"
  },
  {
    "text": "This means that our\nupper confidence bound is going to include in it\na bias towards some actions",
    "start": "3162690",
    "end": "3168200"
  },
  {
    "text": "versus others. So our policy network is going\nto say, if you give me a state, I will give you a\ndistribution over actions.",
    "start": "3168200",
    "end": "3175190"
  },
  {
    "text": "And that's this. And that is going to\nhelp us with the fact that we have an enormous\nnumber of actions. And so this is going to\nprioritize some actions that we",
    "start": "3175190",
    "end": "3182570"
  },
  {
    "text": "think in general might be better\nfor these types of states. So this will be a deep neural--\nlike that neural network up.",
    "start": "3182570",
    "end": "3188160"
  },
  {
    "text": "There is going to be a huge\ncrazy deep neural network and it's going to try to\nleverage similar types of states",
    "start": "3188160",
    "end": "3193520"
  },
  {
    "text": "to suggest which\nactions might be useful in this particular state. The other thing\nyou can see here is",
    "start": "3193520",
    "end": "3199790"
  },
  {
    "text": "that this is going to decay as\nwe visit a state in action more.",
    "start": "3199790",
    "end": "3206090"
  },
  {
    "text": "Here in this case-- so I'll\njust be a little careful. this is all going to be\noperating, I believe this part,",
    "start": "3206090",
    "end": "3214650"
  },
  {
    "text": "is really I. So this is, I think\nat the node level. I'll double check that. But the PSA has to be\nat the state level.",
    "start": "3214650",
    "end": "3224746"
  },
  {
    "text": "So remember, you'll be in\nsome state at this point and you could feed this into a\nconvolutional neural network.",
    "start": "3224747",
    "end": "3229830"
  },
  {
    "text": "It's an image of the board or\nsome other deep neural network. So that part has to generalize. But I'm pretty sure--",
    "start": "3229830",
    "end": "3235510"
  },
  {
    "text": "I'll double check this,\nthat the count here is actually specific to\nthis particular node.",
    "start": "3235510",
    "end": "3240810"
  },
  {
    "text": "Now, why is this U interesting? It's interesting both because it\nincorporates sort of a priority function over actions.",
    "start": "3240810",
    "end": "3247000"
  },
  {
    "text": "You might say some actions\nare better or worse and that's going to change\nwhich ones we expand. The other is that we\nare decaying faster",
    "start": "3247000",
    "end": "3253829"
  },
  {
    "text": "than normal upper\nconfidence bounds. So recall the UCT, U\nwas proportional to one",
    "start": "3253830",
    "end": "3264510"
  },
  {
    "text": "over square root. Yeah. So this is going to\ndecay a lot faster.",
    "start": "3264510",
    "end": "3270850"
  },
  {
    "text": "This means we're being\na lot more aggressive in our upper confidence bound. We're shrinking fast. And so what that\nmeans is that we're",
    "start": "3270850",
    "end": "3277060"
  },
  {
    "text": "going to do a lot less\nexploration of things that we think are not so good. So that's one really\nimportant part",
    "start": "3277060",
    "end": "3282970"
  },
  {
    "text": "of how we're going to\npick what to expand. The other part is this\nnotion of Q. So how are we",
    "start": "3282970",
    "end": "3288849"
  },
  {
    "text": "defining Q for this node? Q is going to be\nequal to one over NIA.",
    "start": "3288850",
    "end": "3295155"
  },
  {
    "text": "And again, I'll\ndouble check this is nodes rather than states. Sum over S prime, V of S prime.",
    "start": "3295155",
    "end": "3303779"
  },
  {
    "text": "So what this means\nhere is that this is going to be an empirical\nestimate of what the value is",
    "start": "3303780",
    "end": "3309720"
  },
  {
    "text": "over the states\nthat we've reached by following this particular\naction in this node.",
    "start": "3309720",
    "end": "3315210"
  },
  {
    "text": "And we're going to see where\nthat comes from shortly. It's going to be a little\nbit different than what",
    "start": "3315210",
    "end": "3320820"
  },
  {
    "text": "we saw before. But these are the two\ncomponents that we're going to use to decide\nwhich action to expand.",
    "start": "3320820",
    "end": "3328440"
  },
  {
    "text": "So yeah. By referring to the\nnode here, we're talking about the identity\nof the node in the matrix",
    "start": "3328440",
    "end": "3335250"
  },
  {
    "text": "or the state of the\nnode in the state? The state. So you can think\nof what a node here",
    "start": "3335250",
    "end": "3340980"
  },
  {
    "text": "is, in his case is it is\na particular board game configuration. So it's like saying the\nwhite pieces are here",
    "start": "3340980",
    "end": "3349050"
  },
  {
    "text": "and the black pieces are here. So it's like you could think\nof it as just like an image, an image of the board. ",
    "start": "3349050",
    "end": "3356175"
  },
  {
    "text": "And the earlier\nwork, in fact, was using convolutional\nneural networks to take in essentially\nimages and features.",
    "start": "3356175",
    "end": "3361630"
  },
  {
    "text": "Yeah. Is there a meaningful difference\nbetween nodes and states? Yes, so it's a great question.",
    "start": "3361630",
    "end": "3366760"
  },
  {
    "text": "So in general, there\nmay be a difference between nodes and states\nbecause, well, this is I'm not",
    "start": "3366760",
    "end": "3372332"
  },
  {
    "text": "a Go expert, so I don't know. But in general for these\ntype of algorithms, you could reach the same state\nat different parts of the tree.",
    "start": "3372332",
    "end": "3378369"
  },
  {
    "text": "And if you can do that, then\nyou would have different bonuses there. Now, I don't know\nenough about Go",
    "start": "3378370",
    "end": "3384240"
  },
  {
    "text": "to know whether\nthat's always possible and it's certainly\npossible in some cases that it would be isomorphic,\nthat the nodes and states would",
    "start": "3384240",
    "end": "3390300"
  },
  {
    "text": "be identical. But in general, these\nsorts of algorithms can work in cases\nwhere you can certainly imagine for checkers\nor chess and stuff,",
    "start": "3390300",
    "end": "3397150"
  },
  {
    "text": "you could end up in the same\nboard game state later on, but it would be a\ndifferent part of the tree.",
    "start": "3397150",
    "end": "3404030"
  },
  {
    "text": "OK. All right. So this is just the start. This is just\nstarting at the root, trying to figure\nout which action",
    "start": "3404030",
    "end": "3409705"
  },
  {
    "text": "we're going to\ntake from the root. And then what we do is\nwe repeatedly expand.",
    "start": "3409705",
    "end": "3415160"
  },
  {
    "text": "So in this case, we would\nfollow the right hand side. Now, what we would do\nat this case, which",
    "start": "3415160",
    "end": "3421070"
  },
  {
    "text": "is pretty interesting, is so\nthis would deterministically-- I would put down, say,\na piece on the board. In this case, I decided\nto put down this piece.",
    "start": "3421070",
    "end": "3430740"
  },
  {
    "text": "And then what I would do is\nI would flip over and pretend to be the opponent and it would\ndo the same thing using its Q",
    "start": "3430740",
    "end": "3439310"
  },
  {
    "text": "and U. Now it's\nQ and U are going to use the same neural\nnetwork approximation.",
    "start": "3439310",
    "end": "3445490"
  },
  {
    "text": "So this is just self-play. But it's just useful\nto in this case that they are going to be\noptimizing for the opposite.",
    "start": "3445490",
    "end": "3451743"
  },
  {
    "text": "One is trying to\noptimize that the Black pieces are going to dominate. The other one is going\nto try to optimize so the white pieces dominate.",
    "start": "3451743",
    "end": "3456990"
  },
  {
    "text": "So now we're going to have that\nthe opponent selects the max Q",
    "start": "3456990",
    "end": "3467650"
  },
  {
    "text": "plus U. So it's\njust useful to think of you're sort of\nrepeatedly flipping back",
    "start": "3467650",
    "end": "3472839"
  },
  {
    "text": "and forth between these two. But you're using exactly\nthe same neural network parameters when you do that.",
    "start": "3472840",
    "end": "3479869"
  },
  {
    "text": "So this is going to continue\ngoing all the way down until we hit a leaf node. So this is again, we haven't\neven selected a single action",
    "start": "3479870",
    "end": "3487000"
  },
  {
    "text": "to take. All of this is going\nto help us finally take a real action in one game. So right now we're just going to\ndo a whole bunch of computation",
    "start": "3487000",
    "end": "3493820"
  },
  {
    "text": "to figure out what\nthat action is. And just to note\nagain here, so we're",
    "start": "3493820",
    "end": "3499040"
  },
  {
    "text": "assuming that we have access to\nthis parameterized deep neural network. And whenever we\ndo this expansion,",
    "start": "3499040",
    "end": "3505500"
  },
  {
    "text": "we are using our P\nfunction because that's what was going into our\nupper confidence bound.",
    "start": "3505500",
    "end": "3510619"
  },
  {
    "text": "So our U was a function\nof P. So it's a function",
    "start": "3510620",
    "end": "3518030"
  },
  {
    "text": "of these probabilities. And so we could weight\ndifferent actions more. So we keep going all the way\ndown until we hit a leaf node.",
    "start": "3518030",
    "end": "3526640"
  },
  {
    "text": "And at that point\nwe plug in V of S, so when we hit a leaf node.",
    "start": "3526640",
    "end": "3532440"
  },
  {
    "text": "So if this is terminal. We're going to do\nV of S. We're going to use our neural network\nto plug in V of S.",
    "start": "3532440",
    "end": "3545780"
  },
  {
    "text": "So this is different\nthan what we saw before. Because before we\nwere thinking we could actually get the\nrewards along our trajectory",
    "start": "3545780",
    "end": "3552559"
  },
  {
    "text": "until we get to the final end. Or if we didn't\nhave any rewards, we just get whether\nwe sort of thought we were in a winning or\nlosing state at that point.",
    "start": "3552560",
    "end": "3559480"
  },
  {
    "text": "We're not doing that anymore. We are plugging in an estimate\nof the value of the final state",
    "start": "3559480",
    "end": "3564990"
  },
  {
    "text": "according to our value network. And that means also that we\ncan either go all the way out till we win or lose a\ngame, or we can terminate.",
    "start": "3564990",
    "end": "3572079"
  },
  {
    "text": "We can say after 700\nsteps, plug in our V of S, which would give us an\nestimate of how likely we were",
    "start": "3572080",
    "end": "3577470"
  },
  {
    "text": "to win the game at that point. So once you have\nthat, we're going to propagate all of\nthis stuff back up.",
    "start": "3577470",
    "end": "3584350"
  },
  {
    "text": "So if we're going to\nselect that, once we go all the way down\nand we get to some V,",
    "start": "3584350",
    "end": "3591560"
  },
  {
    "text": "this is going to go back up.  And remember, what\nthis is going to do",
    "start": "3591560",
    "end": "3597800"
  },
  {
    "text": "is we're going to\nupdate our Q function. So our Q was equal\nto one over NIA,",
    "start": "3597800",
    "end": "3605690"
  },
  {
    "text": "sum over all of our\ntimes V of S prime. So we're going to update our\nvalue all the way back up.",
    "start": "3605690",
    "end": "3612770"
  },
  {
    "text": "So we used our P function\nwhen we were expanding out to figure out which\nactions to take as well as our upper\nconfidence bound.",
    "start": "3612770",
    "end": "3619470"
  },
  {
    "text": "And then we use our V\nprediction to do the backups. So the way that it would\nwork is we go all the way out",
    "start": "3619470",
    "end": "3625910"
  },
  {
    "text": "to a leaf node, and then\nwe go all the way back up along the ancestors\nto the root node.",
    "start": "3625910",
    "end": "3631770"
  },
  {
    "text": "And then we do the\nwhole thing again. We do that many, many,\nmany, many times. I'd have to remind myself,\nI think it's like, say,",
    "start": "3631770",
    "end": "3637700"
  },
  {
    "text": "for example, it might be\n160,000 times, for example, just to give you a\nsense of the scale.",
    "start": "3637700",
    "end": "3642800"
  },
  {
    "text": "So it could be something\nlike 160,000 times, and that means you're going\nto fill in parts of the tree.",
    "start": "3642800",
    "end": "3648200"
  },
  {
    "text": "And then after all\nof that, we have to decide what actually to do. So that's just to compute a\ntree to decide the current move.",
    "start": "3648200",
    "end": "3657029"
  },
  {
    "text": "So we do this many,\nmany, many times.",
    "start": "3657030",
    "end": "3662650"
  },
  {
    "start": "3662650",
    "end": "3668170"
  },
  {
    "text": "So we do this many times\nand then at the end, we are going to decide what\nto do with our root node",
    "start": "3668170",
    "end": "3674260"
  },
  {
    "text": "by the following. And this, again, is a little\nbit different than what we've seen before. We are going to compute a\npolicy for the root node",
    "start": "3674260",
    "end": "3681340"
  },
  {
    "text": "by figuring out\nwhich actions did we mostly visit underneath it.",
    "start": "3681340",
    "end": "3688000"
  },
  {
    "text": "So we're going to look at NSA. So sort of which\nactions, how many times do we take each of the\nactions, from the root",
    "start": "3688000",
    "end": "3693850"
  },
  {
    "text": "node to one over tau.  I think this should be minus.",
    "start": "3693850",
    "end": "3699980"
  },
  {
    "text": "Let me just double check. Yeah, I guess it just\ndepends how you set tau. So tau is just going to be\na temperature parameter.",
    "start": "3699980",
    "end": "3706270"
  },
  {
    "text": " So if tau for example, was\nminus one in this case,",
    "start": "3706270",
    "end": "3715530"
  },
  {
    "text": "then it would be one over NSA. Would be proportional to that. ",
    "start": "3715530",
    "end": "3725130"
  },
  {
    "text": "Or if N was one, you would\nbe sort of proportional. You would take things according\nto that divided by the total.",
    "start": "3725130",
    "end": "3731250"
  },
  {
    "text": "So if n is one, sorry, if tau\nis one, tau is equal to one, then it would be NSA\ndivided by N of S.",
    "start": "3731250",
    "end": "3741230"
  },
  {
    "text": "And as you increase\nor decrease this, then you get things closer to\ntaking a max or just averaging.",
    "start": "3741230",
    "end": "3750290"
  },
  {
    "text": "So this allows you to\nhave a stochastic policy at the root node instead\nof necessarily just taking",
    "start": "3750290",
    "end": "3755630"
  },
  {
    "text": "the argmax.  So this is quite interesting. So this is what they're\ngoing to end up doing.",
    "start": "3755630",
    "end": "3761480"
  },
  {
    "text": "After you do all of\nthis, you're going to actually take an action\nand then you're going to-- so that gives you\na policy and then",
    "start": "3761480",
    "end": "3767422"
  },
  {
    "text": "you are going to\nsample from that policy to actually make a decision. OK, so this is how a game works.",
    "start": "3767423",
    "end": "3773390"
  },
  {
    "text": "You do an enormous\namount of computation. At the end, you get\nthis policy according to the number of times you've\ntaken each action from the root",
    "start": "3773390",
    "end": "3780930"
  },
  {
    "text": "node. And then you sample\nfrom that policy. You reach a new state. So like, let's say you\nput down that thing.",
    "start": "3780930",
    "end": "3788080"
  },
  {
    "text": "Then the opponent does\nexactly the same thing and they put down something. And you repeat this all the\nway out until the game ends.",
    "start": "3788080",
    "end": "3795540"
  },
  {
    "text": "Now, even if your DeepMind,\nyou care about computation. And so in some cases,\nthey will truncate games if they think there's\ndefinitely going",
    "start": "3795540",
    "end": "3802500"
  },
  {
    "text": "to be one outcome or the other. But in general,\nyou would just keep going this all this\nway and Z here would be who won or lost the game.",
    "start": "3802500",
    "end": "3809105"
  },
  {
    "start": "3809105",
    "end": "3815850"
  },
  {
    "text": "Yeah. You said they will\ntruncate games, but do they actually\nsit behind the computer",
    "start": "3815850",
    "end": "3822990"
  },
  {
    "text": "and watch the games\nbeing played or like-- No, no, it's absolutely\nall automated. So this is going on\nbillions of times.",
    "start": "3822990",
    "end": "3830740"
  },
  {
    "text": "And what they will do is, if\nI think it's after 700 moves, if they're not-- if\nthey think either it's",
    "start": "3830740",
    "end": "3836010"
  },
  {
    "text": "going to end in a draw or it's\ndefinitely going to be a lose, and then they try to bound\nlike false positives and stuff",
    "start": "3836010",
    "end": "3841410"
  },
  {
    "text": "like that. But it was interesting to\nme that they included that. Just indicates that\nit probably saved them a substantial amount\nof computation time.",
    "start": "3841410",
    "end": "3848020"
  },
  {
    "text": "Yeah, no, everything\nis totally automated. So what they do is\nnow at this point, so this is like a single\ngame-- as you could imagine,",
    "start": "3848020",
    "end": "3854530"
  },
  {
    "text": "this is an enormous\namount of computation. After a single game, but a lot\nof this could be parallelized.",
    "start": "3854530",
    "end": "3859710"
  },
  {
    "text": "You're now going to train\nour neural networks. So remember that we\nused a neural network to both give us an estimate\nof the probabilities,",
    "start": "3859710",
    "end": "3865930"
  },
  {
    "text": "like give us a policy for\neach state as well as a value. And what we do is now we have--",
    "start": "3865930",
    "end": "3871790"
  },
  {
    "text": "so from that game, that one\ngame, we have one observation. So this is our Z, you know,\nwho won, who won the game.",
    "start": "3871790",
    "end": "3881140"
  },
  {
    "text": "And we had, from each\nstep we had these policies that we computed.",
    "start": "3881140",
    "end": "3886355"
  },
  {
    "text": "And we're going to\nuse those as targets to train our neural network. So what we do is we go\nback and we say, OK, well,",
    "start": "3886355",
    "end": "3891410"
  },
  {
    "text": "in that time when\nyou were in state S and you computed a policy, and\neventually you got a value of Z,",
    "start": "3891410",
    "end": "3899180"
  },
  {
    "text": "you either won or\nlost the game, we are now going to train our\ncrazy big, deep neural network",
    "start": "3899180",
    "end": "3904360"
  },
  {
    "text": "to predict for this state. This is the policy. And for this state\nthat is the value.",
    "start": "3904360",
    "end": "3910700"
  },
  {
    "text": "And this is just a\nsupervised learning problem. And then they do the same\nthing for every state that",
    "start": "3910700",
    "end": "3916890"
  },
  {
    "text": "was reached in that\nparticular game, all using the same final state,\nwhich is either you won or lost.",
    "start": "3916890",
    "end": "3924299"
  },
  {
    "text": "And so this is just\nan enormous network. I can't remember. I think it's maybe like,\nlet's say, 40 layers.",
    "start": "3924300",
    "end": "3931200"
  },
  {
    "text": "And they try, and\nwe'll see shortly, the influence of\narchitecture too. The architecture matters. ",
    "start": "3931200",
    "end": "3938819"
  },
  {
    "text": "And so again, just\nthis neural network goes directly from\nstates to both predict. It's got two output heads, both\npredict policies and values.",
    "start": "3938820",
    "end": "3947100"
  },
  {
    "text": "In their earlier work, they\nhad separate neural networks, but one for policy,\none for values.",
    "start": "3947100",
    "end": "3952590"
  },
  {
    "text": "Here they just combined it. All right. So that is how it works\nin a nutshell in terms",
    "start": "3952590",
    "end": "3959160"
  },
  {
    "text": "of what they're doing. And then they do this for an\nabsolutely enormous amount of time. The final thing I\nthink was trained",
    "start": "3959160",
    "end": "3965340"
  },
  {
    "text": "for 40 days over, like\nmany TPUs, et cetera. Yeah. Does this mean like,\nif you think about it, is this kind of\nlike a loss function",
    "start": "3965340",
    "end": "3971820"
  },
  {
    "text": "with respect to the value? The policy is not actually a\ncomponent of that loss function? Is that what--",
    "start": "3971820",
    "end": "3977360"
  },
  {
    "text": "Yeah, that's a great point. So it is a really good point. So these are just\ntwo different heads.",
    "start": "3977360",
    "end": "3984025"
  },
  {
    "text": "And you can think of\nit as what they're sort of assuming in this case is\nthat the representation you're learning is going to\nbe helpful for both,",
    "start": "3984025",
    "end": "3989820"
  },
  {
    "text": "but this value may or may\nnot relate to this policy.",
    "start": "3989820",
    "end": "3995830"
  },
  {
    "text": "And this is just\nsaying, we think that the features we're\ngoing to learn about this, like the sort of way that\nwe're encoding the game states.",
    "start": "3995830",
    "end": "4002680"
  },
  {
    "text": "And also just to note here,\nit's not just the current board that they're using. The states they use tend\nto use history as well,",
    "start": "4002680",
    "end": "4009150"
  },
  {
    "text": "because again I'm\nnot an expert in Go, but there are various\nrules in Go which mean like I think you can't\nrepeat a move and stuff.",
    "start": "4009150",
    "end": "4014890"
  },
  {
    "text": "So because of that, they have\nto maintain a short history of the previous game states.",
    "start": "4014890",
    "end": "4020490"
  },
  {
    "text": "So you can think of S really as\nbeing like multiple game board states of the past.",
    "start": "4020490",
    "end": "4025890"
  },
  {
    "text": "And I think the\nintuition for this is that you're going\nto learn feature representations from that.",
    "start": "4025890",
    "end": "4031030"
  },
  {
    "text": "They're going to be helpful\nfor predicting both of these. Now ultimately you would\nhope that this sort of, there is some relationship\nbetween these two,",
    "start": "4031030",
    "end": "4037509"
  },
  {
    "text": "but they're not constraining it. ",
    "start": "4037510",
    "end": "4042960"
  },
  {
    "text": "So just to recap, what are the\nkey features that they're using? So I guess also to\nspecify in this case,",
    "start": "4042960",
    "end": "4050680"
  },
  {
    "text": "they're going to do\nthis across many TPUs, over many, many, many days.",
    "start": "4050680",
    "end": "4056412"
  },
  {
    "text": "And what they're\ndoing when they do this is that they're\nconstantly retraining these neural networks.",
    "start": "4056412",
    "end": "4062760"
  },
  {
    "text": "And at the end of all of\nthis, when the actual play kind of test games, say,\nagainst other human players",
    "start": "4062760",
    "end": "4068700"
  },
  {
    "text": "or against other AI\nagents, is they're going to still do the\nMonte Carlo Tree Search.",
    "start": "4068700",
    "end": "4075250"
  },
  {
    "text": "So they're going to take\ntheir final neural networks and then they're still\ngoing to do the Monte Carlo",
    "start": "4075250",
    "end": "4080700"
  },
  {
    "text": "Tree Search method\nthat we've just seen before they make decisions.",
    "start": "4080700",
    "end": "4087073"
  },
  {
    "text": "And so what we'll see in a\nsecond whether that's important or not. So in particular, some of\nthe important questions",
    "start": "4087073",
    "end": "4093660"
  },
  {
    "text": "that they consider in\nthis paper is what is the influence of architecture? Does it matter\nwhich architecture",
    "start": "4093660",
    "end": "4099240"
  },
  {
    "text": "you use in these cases? What is the impact\nof using MCTS? Obviously, they're\nstill learning a policy",
    "start": "4099240",
    "end": "4105839"
  },
  {
    "text": "and they're learning\na value function. And the question is how\nmuch additional gain do you get even after\n40 days of training this",
    "start": "4105840",
    "end": "4113549"
  },
  {
    "text": "by doing Monte Carlo Tree\nSearch and how does it compare to human play\nor using human players?",
    "start": "4113550",
    "end": "4118609"
  },
  {
    "text": "So the first way\nthat they did this is they, instead of having\nthis neural network that was predicting a policy and\na value, is they actually did",
    "start": "4118609",
    "end": "4126299"
  },
  {
    "text": "supervised learning\non human play. And that gave you a way\nto prioritize actions.",
    "start": "4126300",
    "end": "4132051"
  },
  {
    "text": "So that's what they've done\nwhen they did AlphaGo to start. And I think that's what\nthey did also for when they won against Lee Sedol.",
    "start": "4132052",
    "end": "4137684"
  },
  {
    "text": "And then what\nthey've been trying to do in this\npaper and others is to remove some of\nthose assumptions",
    "start": "4137685",
    "end": "4142778"
  },
  {
    "text": "to see if you could learn\nwithout even human knowledge. Now here, I'll just specify that\nthey still know the game rules.",
    "start": "4142779",
    "end": "4153731"
  },
  {
    "text": "And then they have\nlater paper where they want to not even need that. But here the algorithm moves.",
    "start": "4153731",
    "end": "4160060"
  },
  {
    "text": "So the first thing to note\nis that higher is better. This is talking\nabout the performance of the resulting approach\nunder different architectures.",
    "start": "4160060",
    "end": "4171147"
  },
  {
    "text": "So what they do is they\nactually have the same training data that they use and they just\nuse different architectures.",
    "start": "4171147",
    "end": "4176850"
  },
  {
    "text": "They use data in this case from\nsome of the runs of AlphaZero, which is the algorithm\nwe've been talking about.",
    "start": "4176850",
    "end": "4182870"
  },
  {
    "text": "So all of these\nhave the same data and then they look at\nwhat the performance is if you train the neural\nnetworks with that data.",
    "start": "4182870",
    "end": "4189710"
  },
  {
    "text": "So same data, just\ndifferences architecture. And there is a huge difference. This is like from\n3,000 to 4,500.",
    "start": "4189710",
    "end": "4197110"
  },
  {
    "text": "So their current\none-- so this is a convolutional neural\nnetwork, which is separate, meaning that you have\na different policy",
    "start": "4197110",
    "end": "4203080"
  },
  {
    "text": "network from a value network. Whereas this is a\nResNet and they're using a dual representation.",
    "start": "4203080",
    "end": "4209510"
  },
  {
    "text": "So you can see that you\nget a significant benefit by leveraging representational\nstrength across both",
    "start": "4209510",
    "end": "4215110"
  },
  {
    "text": "of these targets. And also that this\nis better than using convolutional neural networks.",
    "start": "4215110",
    "end": "4221760"
  },
  {
    "text": "So I think this is\na good reminder that like when we're doing\nreinforcement learning or we're doing decision\nmaking, we still want to build on all the amazing\nadvances that are happening",
    "start": "4221760",
    "end": "4228890"
  },
  {
    "text": "in deep learning in general. And the complexity of the\nneural networks that we use and the functions they can\nrepresent really matters.",
    "start": "4228890",
    "end": "4235969"
  },
  {
    "text": "So that's the take\nhome from this part. This is a huge difference\nin performance.",
    "start": "4235970",
    "end": "4241250"
  },
  {
    "text": "The second is the impact\nof Monte Carlo Tree Search. So I think this is\nimportant to know. This is if you use\nthe raw network.",
    "start": "4241250",
    "end": "4247250"
  },
  {
    "text": "So you take the network. This is after those 40 days\nof these crazy numbers of TPUs and you don't do Monte\nCarlo Tree Search on top",
    "start": "4247250",
    "end": "4253699"
  },
  {
    "text": "in your evaluation games. And again, this is\nmuch, much, much worse. So this is AlphaGo\nZero, the algorithm",
    "start": "4253700",
    "end": "4259880"
  },
  {
    "text": "we've been talking\nabout, AlphaGo Master was another one they\ndeveloped shortly before this. This is the one\nthat beat Lee Sedol.",
    "start": "4259880",
    "end": "4266330"
  },
  {
    "text": "AlphaGo, what they call Fan is\nthe first big AlphaGo paper. And these are some of\nthe other approaches that",
    "start": "4266330",
    "end": "4272270"
  },
  {
    "text": "happen before their methods. And again, you can see\nthat even though they now have all this beautiful,\ndifferent architecture,",
    "start": "4272270",
    "end": "4278310"
  },
  {
    "text": "et cetera, if you don't do\nMonte Carlo Tree Search on top of that, you miss a lot. So it really is important\nto do this last mile",
    "start": "4278310",
    "end": "4285290"
  },
  {
    "text": "of additional computation\neven after you have these really, really good\nneural networks, this kind of local computation matters.",
    "start": "4285290",
    "end": "4293330"
  },
  {
    "text": "This gives you a sense of\nthe training times involved. So this is the Lee Sedol\npaper or Lee Sedol Method.",
    "start": "4293330",
    "end": "4299429"
  },
  {
    "text": "I don't think they\npublished this before. This is one of their\nmaster methods they had. And this is showing, for a\nparticular size approach,",
    "start": "4299430",
    "end": "4307220"
  },
  {
    "text": "how long it took of training\nbefore you got something that exceeded all of those.",
    "start": "4307220",
    "end": "4313520"
  },
  {
    "text": "So it gets there, but\nit also just highlights the enormous amount\nof computation needed",
    "start": "4313520",
    "end": "4318950"
  },
  {
    "text": "and the importance\nof the architecture. So I know we're\nalmost out of time,",
    "start": "4318950",
    "end": "4324150"
  },
  {
    "text": "but I just want to\nhighlight two things. So again, in this case, it\ndidn't need any human data, no supervised learning.",
    "start": "4324150",
    "end": "4329550"
  },
  {
    "text": "And they noted,\nthough, that it was less good at predicting\nhuman play than some of the other prior methods.",
    "start": "4329550",
    "end": "4335685"
  },
  {
    "text": "So that, again, just highlights\nthat these methods really are helping agents to\ndiscover strategies",
    "start": "4335685",
    "end": "4341150"
  },
  {
    "text": "that are not necessarily the\nones that are used by humans. They're discovering\nvery different ways of solving these sort of\nincredibly complex optimization",
    "start": "4341150",
    "end": "4348620"
  },
  {
    "text": "tasks. And I think that's really\ninteresting in terms of the future of human\nAI collaboration.",
    "start": "4348620",
    "end": "4355610"
  },
  {
    "text": "We're almost out\nof time for today. I'll just highlight as well\nthat these sorts of ideas of how to use Rl to optimize\ncomputation and solve really,",
    "start": "4355610",
    "end": "4363900"
  },
  {
    "text": "really big search\nproblems have also been used by DeepMind to\nsolve things like Alphatensor",
    "start": "4363900",
    "end": "4369050"
  },
  {
    "text": "and these other ways of\ntrying to start automatically searching for new\nalgorithms, which I think",
    "start": "4369050",
    "end": "4374520"
  },
  {
    "text": "is really, really exciting\nbecause you can think of the space of algorithms or\nthe space of different search",
    "start": "4374520",
    "end": "4379850"
  },
  {
    "text": "algorithms, et cetera. And those are enormous. And so you could think of\nusing these types of strategies",
    "start": "4379850",
    "end": "4385430"
  },
  {
    "text": "to prioritize which\nthings are most effective. All right. So I'll leave this here\nbecause we're out of time.",
    "start": "4385430",
    "end": "4391350"
  },
  {
    "text": "You're welcome to look at this,\nto think a little bit more about the aspects of UCT search. And then on\nWednesday, we're going",
    "start": "4391350",
    "end": "4397250"
  },
  {
    "text": "to think more\nabout rewards in Rl and what are the implications\nof which ones we're choosing. I'll see you then.",
    "start": "4397250",
    "end": "4403840"
  },
  {
    "start": "4403840",
    "end": "4408000"
  }
]