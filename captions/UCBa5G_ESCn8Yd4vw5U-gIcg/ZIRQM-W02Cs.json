[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "start": "0",
    "end": "4797"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4797",
    "end": "6130"
  },
  {
    "text": "This is part 6 in our series\non contextual representation.",
    "start": "6130",
    "end": "9450"
  },
  {
    "text": "We're going to focus on RoBERTa.",
    "start": "9450",
    "end": "10980"
  },
  {
    "text": "RoBERTa stands for Robustly\nOptimized BERT Approach.",
    "start": "10980",
    "end": "14610"
  },
  {
    "text": "You might recall that I\nfinished the BERT screencast",
    "start": "14610",
    "end": "17550"
  },
  {
    "start": "15000",
    "end": "15000"
  },
  {
    "text": "by listing out some key known\nlimitations of the BERT model.",
    "start": "17550",
    "end": "21509"
  },
  {
    "text": "And the top item on that\nlist was just an observation",
    "start": "21510",
    "end": "24750"
  },
  {
    "text": "that the BERT team\noriginally did",
    "start": "24750",
    "end": "27240"
  },
  {
    "text": "an admirably detailed,\nbut still very partial",
    "start": "27240",
    "end": "30360"
  },
  {
    "text": "set of ablation studies\nand optimization studies.",
    "start": "30360",
    "end": "33750"
  },
  {
    "text": "So that gave us\nsome glimpses of how",
    "start": "33750",
    "end": "36480"
  },
  {
    "text": "to best optimize BERT\nmodels, but it was hardly",
    "start": "36480",
    "end": "39900"
  },
  {
    "text": "a thorough exploration.",
    "start": "39900",
    "end": "41370"
  },
  {
    "text": "And that's where\nthe RoBERTa team",
    "start": "41370",
    "end": "43350"
  },
  {
    "text": "is going to take over and try to\ndo a more thorough exploration",
    "start": "43350",
    "end": "46859"
  },
  {
    "text": "of this design space.",
    "start": "46860",
    "end": "48690"
  },
  {
    "text": "I think this is a really\ninteresting development",
    "start": "48690",
    "end": "51120"
  },
  {
    "text": "because at a kind of\nmeta level, it points",
    "start": "51120",
    "end": "53670"
  },
  {
    "text": "to a shift in methodologies.",
    "start": "53670",
    "end": "55620"
  },
  {
    "text": "The RoBERTa team does do\na much fuller exploration",
    "start": "55620",
    "end": "59010"
  },
  {
    "text": "of the design space,\nbut it's nowhere near",
    "start": "59010",
    "end": "62010"
  },
  {
    "text": "the kind of exhaustive\nexploration of hyperparameters",
    "start": "62010",
    "end": "65080"
  },
  {
    "text": "that we used to see, especially\nin the pre-deep learning era.",
    "start": "65080",
    "end": "68860"
  },
  {
    "text": "And I think what we're\nseeing with RoBERTa",
    "start": "68860",
    "end": "71110"
  },
  {
    "text": "is that it is simply\ntoo expensive in terms",
    "start": "71110",
    "end": "73660"
  },
  {
    "text": "of money or compute or time\nto be completely thorough.",
    "start": "73660",
    "end": "77800"
  },
  {
    "text": "And so even RoBERTa\nis a kind of very",
    "start": "77800",
    "end": "80500"
  },
  {
    "text": "heuristic and\npartial exploration",
    "start": "80500",
    "end": "82660"
  },
  {
    "text": "of the design space.",
    "start": "82660",
    "end": "84010"
  },
  {
    "text": "But nonetheless, I think it\nwas extremely instructive.",
    "start": "84010",
    "end": "87910"
  },
  {
    "text": "For this slide, I'm\ngoing to list out",
    "start": "87910",
    "end": "89590"
  },
  {
    "start": "88000",
    "end": "88000"
  },
  {
    "text": "kind of key differences\nbetween BERT and RoBERTa.",
    "start": "89590",
    "end": "92590"
  },
  {
    "text": "And then we'll explore\nsome of the evidence",
    "start": "92590",
    "end": "94810"
  },
  {
    "text": "in favor of these\ndecisions just after that.",
    "start": "94810",
    "end": "98740"
  },
  {
    "text": "First item on the list--",
    "start": "98740",
    "end": "100150"
  },
  {
    "text": "BERT used a static\nmasking approach.",
    "start": "100150",
    "end": "103040"
  },
  {
    "text": "And what that means is that\nthey copied their training data",
    "start": "103040",
    "end": "106180"
  },
  {
    "text": "some number of times and applied\ndifferent masks to each copy.",
    "start": "106180",
    "end": "110720"
  },
  {
    "text": "But then that set of copies of\nthe data set with its masking",
    "start": "110720",
    "end": "114700"
  },
  {
    "text": "was used repeatedly\nduring epochs of training.",
    "start": "114700",
    "end": "118030"
  },
  {
    "text": "And what that means is\nthat the same masking was",
    "start": "118030",
    "end": "120490"
  },
  {
    "text": "seen repeatedly by the model.",
    "start": "120490",
    "end": "122930"
  },
  {
    "text": "You might have an\nintuition that we'll",
    "start": "122930",
    "end": "124630"
  },
  {
    "text": "get more and better diversity\ninto this training regime",
    "start": "124630",
    "end": "128259"
  },
  {
    "text": "if we dynamically mask\nexamples, which would just",
    "start": "128259",
    "end": "130750"
  },
  {
    "text": "mean that as we load\nindividual batches,",
    "start": "130750",
    "end": "133270"
  },
  {
    "text": "we apply some random\ndynamic masking to those",
    "start": "133270",
    "end": "137020"
  },
  {
    "text": "so that subsequent batches\ncontaining the same examples",
    "start": "137020",
    "end": "140920"
  },
  {
    "text": "have different masking\napplied to them.",
    "start": "140920",
    "end": "143410"
  },
  {
    "text": "Clearly, that's going to\nintroduce some diversity",
    "start": "143410",
    "end": "145660"
  },
  {
    "text": "into the training regime,\nand that could be useful.",
    "start": "145660",
    "end": "149410"
  },
  {
    "text": "For BERT, the\ninputs to the model",
    "start": "149410",
    "end": "152800"
  },
  {
    "text": "were two concatenated\ndocument segments,",
    "start": "152800",
    "end": "155710"
  },
  {
    "text": "and that's actually crucial to\ntheir next sentence prediction",
    "start": "155710",
    "end": "158500"
  },
  {
    "text": "task.",
    "start": "158500",
    "end": "159220"
  },
  {
    "text": "Whereas for RoBERTa, inputs\nare sentence sequences",
    "start": "159220",
    "end": "162100"
  },
  {
    "text": "that may even span\ndocument boundaries.",
    "start": "162100",
    "end": "165580"
  },
  {
    "text": "Obviously, that's going to be\ndisruptive to the next sentence",
    "start": "165580",
    "end": "168670"
  },
  {
    "text": "prediction objective, but\ncorrespondingly, whereas BERT",
    "start": "168670",
    "end": "172060"
  },
  {
    "text": "had that NSP objective,\nRoBERTa simply",
    "start": "172060",
    "end": "174790"
  },
  {
    "text": "dropped it on the grounds that\nit was not earning its keep.",
    "start": "174790",
    "end": "178930"
  },
  {
    "text": "For BERT, the training\nbatches contain 256 examples.",
    "start": "178930",
    "end": "183519"
  },
  {
    "text": "RoBERTa upped that to\n2,000 examples per batch,",
    "start": "183520",
    "end": "187150"
  },
  {
    "text": "a substantial increase.",
    "start": "187150",
    "end": "189799"
  },
  {
    "text": "BERT used a wordpiece\ntokenizer, whereas RoBERTa",
    "start": "189800",
    "end": "192890"
  },
  {
    "text": "used a character-level\nbyte-pair encoding algorithm.",
    "start": "192890",
    "end": "197600"
  },
  {
    "text": "BERT was trained\non a lot of data--",
    "start": "197600",
    "end": "199550"
  },
  {
    "text": "BooksCorpus and\nEnglish Wikipedia.",
    "start": "199550",
    "end": "201890"
  },
  {
    "text": "RoBERTa leveled up\non the amount of data",
    "start": "201890",
    "end": "204590"
  },
  {
    "text": "by training on BooksCorpus,\nWikipedia, CC-News,",
    "start": "204590",
    "end": "208040"
  },
  {
    "text": "OpenWebText, and Stories.",
    "start": "208040",
    "end": "209689"
  },
  {
    "text": "And the result of that\nis a substantial increase",
    "start": "209690",
    "end": "212630"
  },
  {
    "text": "in the amount of data\nthat the model saw.",
    "start": "212630",
    "end": "215930"
  },
  {
    "text": "BERT was trained\nfor 1 million steps,",
    "start": "215930",
    "end": "218239"
  },
  {
    "text": "whereas RoBERTa was\ntrained for 500,000 steps.",
    "start": "218240",
    "end": "221870"
  },
  {
    "text": "Pause there-- you might think\nthat means RoBERTa was trained",
    "start": "221870",
    "end": "225560"
  },
  {
    "text": "for less time, but\nremember, the batch sizes",
    "start": "225560",
    "end": "228530"
  },
  {
    "text": "are substantially larger.",
    "start": "228530",
    "end": "229940"
  },
  {
    "text": "And so the net effect\nof these two choices",
    "start": "229940",
    "end": "232280"
  },
  {
    "text": "is that RoBERTa was trained\nfor a lot more instances.",
    "start": "232280",
    "end": "237350"
  },
  {
    "text": "And then finally,\nfor the BERT team,",
    "start": "237350",
    "end": "239120"
  },
  {
    "text": "there was an intuition\nthat it would",
    "start": "239120",
    "end": "240620"
  },
  {
    "text": "be useful for optimization to\ntrain on short sequences first.",
    "start": "240620",
    "end": "245060"
  },
  {
    "text": "The RoBERTa team\nsimply dropped that",
    "start": "245060",
    "end": "247099"
  },
  {
    "text": "and trained on\nfull-length sequences",
    "start": "247100",
    "end": "249480"
  },
  {
    "text": "throughout the training regime.",
    "start": "249480",
    "end": "251790"
  },
  {
    "text": "I think those are the\nhigh-level changes",
    "start": "251790",
    "end": "254189"
  },
  {
    "text": "between BERT and RoBERTa.",
    "start": "254190",
    "end": "255570"
  },
  {
    "text": "There are some\nadditional differences,",
    "start": "255570",
    "end": "257489"
  },
  {
    "text": "and I refer to section\n3.1 of the paper",
    "start": "257490",
    "end": "260609"
  },
  {
    "text": "for the details on those.",
    "start": "260610",
    "end": "263550"
  },
  {
    "start": "263000",
    "end": "263000"
  },
  {
    "text": "Let's dive into\nsome of the evidence",
    "start": "263550",
    "end": "265740"
  },
  {
    "text": "that they used\nfor these choices,",
    "start": "265740",
    "end": "267569"
  },
  {
    "text": "beginning with that first\nshift from static masking",
    "start": "267570",
    "end": "271500"
  },
  {
    "text": "to dynamic masking.",
    "start": "271500",
    "end": "273060"
  },
  {
    "text": "This table summarizes their\nevidence for this choice.",
    "start": "273060",
    "end": "276370"
  },
  {
    "text": "They're using SQuAD, MultiNLI,\nand Binary Stanford Sentiment",
    "start": "276370",
    "end": "280770"
  },
  {
    "text": "Treebank as their\nkind of benchmarks",
    "start": "280770",
    "end": "282990"
  },
  {
    "text": "to make this decision.",
    "start": "282990",
    "end": "284370"
  },
  {
    "text": "And you can see that\nfor SQuAD and SST,",
    "start": "284370",
    "end": "286919"
  },
  {
    "text": "there's a pretty clear win.",
    "start": "286920",
    "end": "288240"
  },
  {
    "text": "Dynamic masking is better.",
    "start": "288240",
    "end": "289919"
  },
  {
    "text": "For MultiNLI, it looks like\nthere was a small regression,",
    "start": "289920",
    "end": "292980"
  },
  {
    "text": "but on average, the results\nlook better for dynamic masking.",
    "start": "292980",
    "end": "296880"
  },
  {
    "text": "And I will say that to kind\nof augment these results,",
    "start": "296880",
    "end": "299820"
  },
  {
    "text": "there is a clear intuition\nthat dynamic masking",
    "start": "299820",
    "end": "302520"
  },
  {
    "text": "is going to be useful.",
    "start": "302520",
    "end": "303610"
  },
  {
    "text": "And so even if it's not\nreflected in these benchmarks,",
    "start": "303610",
    "end": "306270"
  },
  {
    "text": "we might still think that\nit's a wise choice if we can",
    "start": "306270",
    "end": "309539"
  },
  {
    "text": "afford to train it that way.",
    "start": "309540",
    "end": "313410"
  },
  {
    "text": "We talked briefly\nabout how examples",
    "start": "313410",
    "end": "315750"
  },
  {
    "text": "are presented to these models.",
    "start": "315750",
    "end": "317340"
  },
  {
    "text": "I would say the two\ncompetitors that RoBERTa",
    "start": "317340",
    "end": "320400"
  },
  {
    "text": "thoroughly evaluated were full\nsentences and doc sentences.",
    "start": "320400",
    "end": "324840"
  },
  {
    "text": "So doc sentences will be where\nwe limit training instances",
    "start": "324840",
    "end": "328350"
  },
  {
    "text": "to pairs of sentences that come\nfrom the same document, which",
    "start": "328350",
    "end": "331493"
  },
  {
    "text": "you would think would give us\na clear intuition about kind",
    "start": "331493",
    "end": "333910"
  },
  {
    "text": "of something like discourse\ncoherence for those instances.",
    "start": "333910",
    "end": "338100"
  },
  {
    "text": "We can also compare that\nagainst full sentences",
    "start": "338100",
    "end": "340710"
  },
  {
    "text": "in which we present\nexamples, even though they",
    "start": "340710",
    "end": "343860"
  },
  {
    "text": "might span document boundaries.",
    "start": "343860",
    "end": "346319"
  },
  {
    "text": "And so we have\nless of a guarantee",
    "start": "346320",
    "end": "347880"
  },
  {
    "text": "of discourse coherence.",
    "start": "347880",
    "end": "349860"
  },
  {
    "text": "Although doc sentences\ncomes out a little bit",
    "start": "349860",
    "end": "352500"
  },
  {
    "text": "ahead in this benchmark\nthat they have set up",
    "start": "352500",
    "end": "354690"
  },
  {
    "text": "across SQuAD, MultiNLI,\nSST-2, and RACE,",
    "start": "354690",
    "end": "358740"
  },
  {
    "text": "they chose full\nsentences on the grounds",
    "start": "358740",
    "end": "361169"
  },
  {
    "text": "that there is more at play\nhere than just accuracy.",
    "start": "361170",
    "end": "365010"
  },
  {
    "text": "We should also think about\nthe efficiency of the training",
    "start": "365010",
    "end": "368310"
  },
  {
    "text": "regime.",
    "start": "368310",
    "end": "369060"
  },
  {
    "text": "And since full sentences\nmakes it much easier",
    "start": "369060",
    "end": "371610"
  },
  {
    "text": "to create efficient\nbatches of examples,",
    "start": "371610",
    "end": "374379"
  },
  {
    "text": "they opted for that instead.",
    "start": "374380",
    "end": "376160"
  },
  {
    "text": "And so that's also\nvery welcome to my mind",
    "start": "376160",
    "end": "378280"
  },
  {
    "text": "because it's showing,\nagain, that there's",
    "start": "378280",
    "end": "380080"
  },
  {
    "text": "more at stake in this new\nera than just accuracy.",
    "start": "380080",
    "end": "383680"
  },
  {
    "text": "We should also\nconsider our resources.",
    "start": "383680",
    "end": "387590"
  },
  {
    "text": "This table summarizes\ntheir evidence",
    "start": "387590",
    "end": "390350"
  },
  {
    "text": "for the larger batch sizes.",
    "start": "390350",
    "end": "392030"
  },
  {
    "text": "Their using various\nmetrics here perplexity,",
    "start": "392030",
    "end": "394610"
  },
  {
    "text": "which is a kind of\npseudo perplexity,",
    "start": "394610",
    "end": "396319"
  },
  {
    "text": "given that BERT uses\nbidirectional context.",
    "start": "396320",
    "end": "399470"
  },
  {
    "text": "And they're also benchmarking\nagainst MultiNLI and SST-2.",
    "start": "399470",
    "end": "403220"
  },
  {
    "text": "And what they find is\nthat, clearly, there's",
    "start": "403220",
    "end": "405770"
  },
  {
    "text": "a win for having this very large\nbatch size at 2,000 examples.",
    "start": "405770",
    "end": "412410"
  },
  {
    "text": "And then finally,\njust the raw amount",
    "start": "412410",
    "end": "414540"
  },
  {
    "text": "of data that these\nmodels are trained on",
    "start": "414540",
    "end": "416460"
  },
  {
    "text": "is interesting, and also\nthe amount of training time",
    "start": "416460",
    "end": "418800"
  },
  {
    "text": "that they get.",
    "start": "418800",
    "end": "419590"
  },
  {
    "text": "And so what they\nfound is that they",
    "start": "419590",
    "end": "421350"
  },
  {
    "text": "got the best results for\nRoBERTa by training for",
    "start": "421350",
    "end": "424590"
  },
  {
    "text": "as long as they could possibly\nafford to on as much data",
    "start": "424590",
    "end": "428610"
  },
  {
    "text": "as they could include.",
    "start": "428610",
    "end": "430379"
  },
  {
    "text": "So you can see the amount of\ndata going up to 160 gigabytes",
    "start": "430380",
    "end": "433860"
  },
  {
    "text": "here versus the largest\nBERT model at 13,",
    "start": "433860",
    "end": "436800"
  },
  {
    "text": "a substantial increase.",
    "start": "436800",
    "end": "438509"
  },
  {
    "text": "The step size going all\nthe way up to 500,000,",
    "start": "438510",
    "end": "441570"
  },
  {
    "text": "whereas for BERT,\nit was a million.",
    "start": "441570",
    "end": "443320"
  },
  {
    "text": "But remember, overall,\nthere are many more examples",
    "start": "443320",
    "end": "445980"
  },
  {
    "text": "being presented as a\nresult of the batch size",
    "start": "445980",
    "end": "448530"
  },
  {
    "text": "being so much larger\nfor the RoBERTa models.",
    "start": "448530",
    "end": "452280"
  },
  {
    "text": "And so again, another\nfamiliar lesson",
    "start": "452280",
    "end": "454830"
  },
  {
    "text": "from the deep learning era--",
    "start": "454830",
    "end": "456210"
  },
  {
    "text": "more is better in terms of data\nand training time, especially",
    "start": "456210",
    "end": "460500"
  },
  {
    "text": "when our goal is to create\nthese pre-trained artifacts that",
    "start": "460500",
    "end": "464610"
  },
  {
    "text": "are useful for fine tuning.",
    "start": "464610",
    "end": "468185"
  },
  {
    "start": "467000",
    "end": "467000"
  },
  {
    "text": "To round this out, I thought\nI'd mention that the RoBERTa",
    "start": "468185",
    "end": "470560"
  },
  {
    "text": "team released two models,\nbase and large, which",
    "start": "470560",
    "end": "473560"
  },
  {
    "text": "are kind of directly comparable\nto the corresponding BERT",
    "start": "473560",
    "end": "476770"
  },
  {
    "text": "artifacts.",
    "start": "476770",
    "end": "477400"
  },
  {
    "text": "The base model has 12 layers,\ndimensionality of 768,",
    "start": "477400",
    "end": "482290"
  },
  {
    "text": "and a feedforward\nlayer of 3,072,",
    "start": "482290",
    "end": "484600"
  },
  {
    "text": "for a total of 125\nmillion parameters, which",
    "start": "484600",
    "end": "487810"
  },
  {
    "text": "is more or less the\nsame as BERT base.",
    "start": "487810",
    "end": "490030"
  },
  {
    "text": "And then correspondingly,\nBERT-large",
    "start": "490030",
    "end": "491920"
  },
  {
    "text": "has all the same basic\nsettings as BERT-base",
    "start": "491920",
    "end": "495610"
  },
  {
    "text": "and correspondingly\nessentially the same number",
    "start": "495610",
    "end": "498129"
  },
  {
    "text": "of parameters at 355 million.",
    "start": "498130",
    "end": "502270"
  },
  {
    "text": "As I said at the start\nof this screencast,",
    "start": "502270",
    "end": "504490"
  },
  {
    "text": "RoBERTa was thorough,\nbut even that",
    "start": "504490",
    "end": "506349"
  },
  {
    "text": "is only a very\npartial exploration",
    "start": "506350",
    "end": "508210"
  },
  {
    "text": "of the full design space\nsuggested by the BERT model.",
    "start": "508210",
    "end": "511300"
  },
  {
    "text": "For many more results,\nI highly recommend",
    "start": "511300",
    "end": "514330"
  },
  {
    "text": "this paper, \"A Primer in\nBERTalogy\" from Rogers et al.",
    "start": "514330",
    "end": "517630"
  },
  {
    "text": "It's a little bit of an\nold paper at this point.",
    "start": "517630",
    "end": "520309"
  },
  {
    "text": "So lots has happened\nsince it was released.",
    "start": "520309",
    "end": "522159"
  },
  {
    "text": "But nonetheless,\nit's very thorough",
    "start": "522159",
    "end": "524380"
  },
  {
    "text": "and contains lots of\ninsights about how",
    "start": "524380",
    "end": "526780"
  },
  {
    "text": "best to set up these BERT\nstyle models for doing",
    "start": "526780",
    "end": "529670"
  },
  {
    "text": "various things in NLP.",
    "start": "529670",
    "end": "531060"
  },
  {
    "text": "So highly recommended\nas a companion",
    "start": "531060",
    "end": "533240"
  },
  {
    "text": "to this little screencast.",
    "start": "533240",
    "end": "536140"
  },
  {
    "start": "536140",
    "end": "540000"
  }
]