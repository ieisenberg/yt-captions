[
  {
    "start": "0",
    "end": "5480"
  },
  {
    "text": "Hi, everybody. Welcome back. This is lecture 2 from\nReinforcement Learning. We're going to start with a\nRefresh Your Understanding.",
    "start": "5480",
    "end": "12950"
  },
  {
    "text": "Again, these are just\na sort of a quick way to check your conceptual\nunderstanding from the most recent lectures, or occasionally\nwe'll go back a little bit.",
    "start": "12950",
    "end": "20280"
  },
  {
    "text": "To do this, you just\nneed to log into Ed. Everybody should be added to Ed. If you're not, just send us\nan email to our mailing list.",
    "start": "20280",
    "end": "27800"
  },
  {
    "text": "So if you go to\nEd, please follow the steps given to log in first\nbefore you click the links. So if you follow those\nsteps and then you're",
    "start": "27800",
    "end": "33800"
  },
  {
    "text": "logged in with your\nSUN ID, then when you click on the poll\nlinks, it should just take you right there,\nand it will just",
    "start": "33800",
    "end": "39283"
  },
  {
    "text": "log all your responses. If you're curious about how\nwe use these for participation points, you can just\ngo to the website",
    "start": "39283",
    "end": "45320"
  },
  {
    "text": "to see how we calculate it. I think we use just a\npercentage of these. If you do a\nsufficient percentage,",
    "start": "45320",
    "end": "51510"
  },
  {
    "text": "then you get full\nparticipation points. It's optional. ",
    "start": "51510",
    "end": "56950"
  },
  {
    "text": "All right. So we're going to\nstart with this today. The question is, in\nMarkov decision processes, a large discount\nfactor gamma means",
    "start": "56950",
    "end": "62208"
  },
  {
    "text": "that short-term rewards\nare much more influential than long-term rewards? And then a second question\nto start thinking about is,",
    "start": "62208",
    "end": "69380"
  },
  {
    "text": "in general-- so last time we started talking\nabout sequential decision making under uncertainty, and\none of the things we often",
    "start": "69380",
    "end": "75650"
  },
  {
    "text": "would like in real-world systems\nis monotonic improvement, meaning that if we get more\ndata or we get more computation,",
    "start": "75650",
    "end": "82890"
  },
  {
    "text": "we know that the system is going\nto be better, make, in our case, better decisions than\nit could if it had",
    "start": "82890",
    "end": "88040"
  },
  {
    "text": "less computation or less data. And so the question that\nI'm posing to you now",
    "start": "88040",
    "end": "93488"
  },
  {
    "text": "and that we're going\nto discuss today is, is it possible to\nconstruct algorithms for computing decision policies\nso that we can guarantee",
    "start": "93488",
    "end": "100700"
  },
  {
    "text": "with additional computation-- we can also think of\noften as iteration-- that we're going to\nmonotonically improve",
    "start": "100700",
    "end": "107090"
  },
  {
    "text": "the decision policy? And you can start to think\nabout if you're already aware of any algorithms that\nmight have that property, if you",
    "start": "107090",
    "end": "113270"
  },
  {
    "text": "think it's impossible, or\nif you think-- if it's true, do you think that all\nalgorithms would satisfy that?",
    "start": "113270",
    "end": "119650"
  },
  {
    "text": "That's not for the poll. That's just to start\nthinking about, and we'll come back to it later.",
    "start": "119650",
    "end": "125600"
  },
  {
    "text": "All right. So I'll just give you\nanother minute or two to do this Refresh\nYour Understanding. It's just a quick one,\nand then we'll go.",
    "start": "125600",
    "end": "130960"
  },
  {
    "start": "130960",
    "end": "145013"
  },
  {
    "text": "And again, these are not\nassessment questions, so you're welcome to look\nback on lecture slides from last time. You're also welcome to talk\nto anybody right next to you.",
    "start": "145013",
    "end": "152175"
  },
  {
    "start": "152175",
    "end": "187940"
  },
  {
    "text": "All right. It looks like we actually have\nmaybe a 2/3-1/3 split on this",
    "start": "187940",
    "end": "193250"
  },
  {
    "text": "question. The correct answer is false. Does somebody want to\nsay why it's false?",
    "start": "193250",
    "end": "198340"
  },
  {
    "text": " Yeah? And remind me your name.",
    "start": "198340",
    "end": "203710"
  },
  {
    "text": "Yeah, I think because you\nmultiply the longer-term rewards by the gamma. So a large gamma means that\nthe long-term rewards are",
    "start": "203710",
    "end": "210370"
  },
  {
    "text": "weighted decently [INAUDIBLE]. That's right. So if you have-- exactly\nwhat [AUDIO OUT] said. So if gamma was\n1, you would care",
    "start": "210370",
    "end": "215830"
  },
  {
    "text": "about short-term rewards exactly\nthe same as long-term rewards. In general, if gamma\nwas 0, you would not",
    "start": "215830",
    "end": "221200"
  },
  {
    "text": "care about long\nterm rewards at all. You'd be entirely myopic. But as gamma gets\ncloser to 1, it's",
    "start": "221200",
    "end": "226480"
  },
  {
    "text": "sort of a relatively weighting\nmore of longer rewards than you would otherwise. ",
    "start": "226480",
    "end": "232780"
  },
  {
    "text": "Great. All right. And yes, as I said,\nwe'll get more into the conceptual\nquestion later.",
    "start": "232780",
    "end": "239800"
  },
  {
    "text": "The other thing that\nI wanted to clarify-- I saw there was some\nquestions on this last time as well as after class\nas well as on Ed--",
    "start": "239800",
    "end": "245770"
  },
  {
    "text": "is I had mentioned when I\nwas making distinguishments between reinforcement learning\nand other forms of AI machine",
    "start": "245770",
    "end": "252430"
  },
  {
    "text": "learning this notion\nof optimization. But I think that that\nwas a little bit--",
    "start": "252430",
    "end": "257677"
  },
  {
    "text": "I think it was more confusing\nthan it was helpful, and depending on\nhow you think of it,",
    "start": "257677",
    "end": "262840"
  },
  {
    "text": "in machine learning\nor AI, we always have some form of\nmetric or optimization. So you can think\nof a loss as also",
    "start": "262840",
    "end": "268870"
  },
  {
    "text": "being-- we're trying\nto minimize the loss, and so that also sounds like\nan optimization problem. So you can just ignore\nthat distinction for now.",
    "start": "268870",
    "end": "275660"
  },
  {
    "text": "I do think in\ngeneral, when we're thinking about\ndecision-making, it's going to be very important what\nwe think of as that metric.",
    "start": "275660",
    "end": "282320"
  },
  {
    "text": "And so it won't necessarily\njust be loss functions. We can have lots of different\nscalar values or even multiple objectives.",
    "start": "282320",
    "end": "288530"
  },
  {
    "text": "But the distinction of whether\nor not supervised learning is using optimization is\nperhaps not so helpful.",
    "start": "288530",
    "end": "294360"
  },
  {
    "text": " OK, great. So let's go ahead\nand get started.",
    "start": "294360",
    "end": "299660"
  },
  {
    "text": "So I do also just want to\nhighlight that for some of you-- and I got a question about this.",
    "start": "299660",
    "end": "305370"
  },
  {
    "text": "We've also got a couple\nof questions about this. This first week or two\nwill overlap a little bit with some of the other\nclasses you might have taken.",
    "start": "305370",
    "end": "311667"
  },
  {
    "text": "So particularly if you've taken\n238 with Mykel Kochenderfer, the beginning may overlap. The things that\nwill probably still",
    "start": "311667",
    "end": "317540"
  },
  {
    "text": "be different in the\nfirst couple of weeks is I expect there's going to\nbe a higher level of theory",
    "start": "317540",
    "end": "322669"
  },
  {
    "text": "in the first week or two\nabout the properties of some of these algorithms and what\nsort of guarantees we have.",
    "start": "322670",
    "end": "328699"
  },
  {
    "text": "And then afterwards,\nI suspect after that, most of the content in\nthe rest of the class will be quite different.",
    "start": "328700",
    "end": "334008"
  },
  {
    "text": "If you have any questions\nabout how this compares to a lot of the other\ndecision-making classes that are offered\nat Stanford, don't",
    "start": "334008",
    "end": "339650"
  },
  {
    "text": "hesitate to reach out to\nme in office hours on Ed or after class. All right.",
    "start": "339650",
    "end": "345120"
  },
  {
    "text": "Now, why do we do this? Because also, you\nmight be thinking, we want to get to\nAlphaGo, or I want to get to controlling\nrobots, or I",
    "start": "345120",
    "end": "350720"
  },
  {
    "text": "want to get to optimizing LLMs. Why are we starting with systems\nlike the seven-state Mars rover",
    "start": "350720",
    "end": "356000"
  },
  {
    "text": "that we're going to look at. And the reason is\nbecause actually a lot of the ideas that enabled people\nto solve AlphaGo and do things",
    "start": "356000",
    "end": "364860"
  },
  {
    "text": "like RLHF, or reinforcement\nlearning from human feedback, really builds on this\nfundamental notion of decision",
    "start": "364860",
    "end": "370800"
  },
  {
    "text": "processes. And I think it's much\neasier to really cleanly see how these ideas come up\nwhen you can actually see",
    "start": "370800",
    "end": "377130"
  },
  {
    "text": "these in the world as tabular. You can just write\ndown all the states. So that's why I\nthink it's helpful,",
    "start": "377130",
    "end": "382630"
  },
  {
    "text": "but even today we're going\nto start to see where those ideas might be applied. So we're going to start to do\nthings like policy search, which",
    "start": "382630",
    "end": "388680"
  },
  {
    "text": "is the foundations\ntowards things like policy gradients, which\nare extremely widely used. So you can think of all of these\nas just being building blocks",
    "start": "388680",
    "end": "395790"
  },
  {
    "text": "that we're going to use to build\nup to get to the point where we're later going to\nbe-- and very soon, within a couple of weeks-- tackling things that are\nstate-of-the-art algorithms.",
    "start": "395790",
    "end": "403860"
  },
  {
    "text": "All right. So what we're going\nto be doing today is really focusing on making\ngood decisions given a Markov decision process,\nand so that means",
    "start": "403860",
    "end": "410460"
  },
  {
    "text": "both being able to understand\nhow good a particular decision policy is as well as what is\nan optimal decision policy.",
    "start": "410460",
    "end": "418610"
  },
  {
    "text": "And when I say we're given a\nmodel of the world, what I mean is that we are given that\ndynamics model, which tells us",
    "start": "418610",
    "end": "426590"
  },
  {
    "text": "how the world evolves\nwhen we make decisions,",
    "start": "426590",
    "end": "432350"
  },
  {
    "text": "and we are given a reward\nmodel, which tells us how good decisions are.",
    "start": "432350",
    "end": "437660"
  },
  {
    "text": "And last time we talked\nabout Markov processes, and we were starting to talk\nabout Markov reward processes",
    "start": "437660",
    "end": "443480"
  },
  {
    "text": "because they can end up being\nreally useful when we're trying to evaluate how good a\nparticular decision policy is.",
    "start": "443480",
    "end": "450610"
  },
  {
    "text": "And we'll see a lot\nof the same ideas from Markov reward\nprocesses to MDPs. ",
    "start": "450610",
    "end": "457060"
  },
  {
    "text": "All right. So let's just\nrefresh our memory. So this is the question\nthat we had before of, how do we think of the\ninfluence of discount factors?",
    "start": "457060",
    "end": "465690"
  },
  {
    "text": "As was said, what happens is\nwe multiply the next reward by the discount factor two\nrewards away by the discount",
    "start": "465690",
    "end": "471950"
  },
  {
    "text": "factor squared, et cetera. And so as you can see there,\nif the horizon is really long",
    "start": "471950",
    "end": "477020"
  },
  {
    "text": "or as it goes to\ninfinity, rewards will have 0 value eventually\nbecause gamma is less than 1.",
    "start": "477020",
    "end": "483470"
  },
  {
    "text": "So the idea of the value\nfunction was to say-- remember, this is a\nMarkov reward process. We don't have decisions yet.",
    "start": "483470",
    "end": "488996"
  },
  {
    "text": "It just says, how much is\nthe expected discounted sum of rewards we will\nget starting in this state",
    "start": "488997",
    "end": "494930"
  },
  {
    "text": "and acting, most of the\ntime today, forever? So most of the time\nwe'll think of today of just getting to act forever\nand how much reward would",
    "start": "494930",
    "end": "502040"
  },
  {
    "text": "you get. And because this gamma-- as\nlong as the gamma is less than 1 here, that will be\na finite number.",
    "start": "502040",
    "end": "509585"
  },
  {
    "text": "So we're just starting to talk\nabout how could we compute this. So again, remember,\nthe return is",
    "start": "509585",
    "end": "514599"
  },
  {
    "text": "going to be a particular\nseries of rewards you might get if you start in\nthis state and act forever, and V is going to\nbe, on average,",
    "start": "514600",
    "end": "521267"
  },
  {
    "text": "how much reward would you get\nif you start in this state and act forever?",
    "start": "521267",
    "end": "526730"
  },
  {
    "text": "All right. So one of the key ideas\nhere is that computing the value of an infinite\nhorizon Markov reward process",
    "start": "526730",
    "end": "534080"
  },
  {
    "text": "leverages the Markov\nproperty, which was this idea that the future\nis independent of the past given",
    "start": "534080",
    "end": "539839"
  },
  {
    "text": "the present. So given your current\nstate, you don't have to think more\nabout the history. So what that implies\nwhen we try to compute",
    "start": "539840",
    "end": "547070"
  },
  {
    "text": "what the expected reward is,\nfuture reward is from a state is we can think of, well, what\nis the immediate reward we get",
    "start": "547070",
    "end": "552380"
  },
  {
    "text": "in that state plus all\nthe different states we could get to next\nunder our dynamics model",
    "start": "552380",
    "end": "557510"
  },
  {
    "text": "and then the value\nof their reward? How much do we\nweigh each of those? Well, we weigh\neach of those just",
    "start": "557510",
    "end": "563470"
  },
  {
    "text": "according to what is the\nprobability I could get to each of those next states. And if you're familiar with\nthings like tree search,",
    "start": "563470",
    "end": "570450"
  },
  {
    "text": "you can think of it as just,\nI'm in my starting state. I think of all the next\nstates I could go to. Each of them have some weight,\ndepending on the probability",
    "start": "570450",
    "end": "576688"
  },
  {
    "text": "I get there, and then\nI sum all of those up according to their values. ",
    "start": "576688",
    "end": "583305"
  },
  {
    "text": "And this is going\nto be the basis of the Bellman equation, which\nwe're going to see lots about.",
    "start": "583305",
    "end": "588880"
  },
  {
    "text": "So if we wanted to think\nabout how we could solve this, one way we could think of it\nas if we have a tabular world,",
    "start": "588880",
    "end": "595840"
  },
  {
    "text": "meaning that we can maintain a\nscalar value for every single",
    "start": "595840",
    "end": "601060"
  },
  {
    "text": "state separately-- so this\nis like our Mars rover case-- then we could just\nexpress the value function",
    "start": "601060",
    "end": "606310"
  },
  {
    "text": "in a matrix equation. So we say the value\nof each of the states is exactly equal to\nthe immediate reward",
    "start": "606310",
    "end": "611649"
  },
  {
    "text": "plus gamma times the\ntransition probability to all the next states.",
    "start": "611650",
    "end": "618740"
  },
  {
    "text": "And so that's nice because\nnow we can just directly solve for what the value is. So we know that\nthis has to halt,",
    "start": "618740",
    "end": "626063"
  },
  {
    "text": "so now what we're going to do\nis just invert that to solve for V. So what we\nwould say in this case",
    "start": "626063",
    "end": "632089"
  },
  {
    "text": "is we would say v minus gamma\ntimes P of V-- this is P.",
    "start": "632090",
    "end": "638060"
  },
  {
    "text": "And again, I'll apologize\nthat in the different things you see online or the textbook,\net cetera people sometimes",
    "start": "638060",
    "end": "644510"
  },
  {
    "text": "use T for transition matrix. They sometimes use\nP for probabilities, going to the next state.",
    "start": "644510",
    "end": "649940"
  },
  {
    "text": "If it's ever confusing what\nnotation is being used, don't hesitate to reach out. OK, so we just rewrite it like\nthis, as equal to R, and then",
    "start": "649940",
    "end": "658260"
  },
  {
    "text": "we move this. So we have V of I\nminus gamma P is equal to RI equals the\nidentity matrix, which",
    "start": "658260",
    "end": "670879"
  },
  {
    "text": "means V is equal to I minus\ngamma P inverse times R.",
    "start": "670880",
    "end": "679560"
  },
  {
    "text": "So why do I show this? I show this because if you\nknow how the world works, you have the dynamics model,\nyou know what the reward",
    "start": "679560",
    "end": "685258"
  },
  {
    "text": "function is, and the\nworld is small enough, you can just directly\nsolve for this. This isn't for decision yet.",
    "start": "685258",
    "end": "691230"
  },
  {
    "text": "This is just showing\nus what the value would be of each of the states.",
    "start": "691230",
    "end": "696630"
  },
  {
    "text": "So this is one way to solve it. We would call this\nthe analytic solution.",
    "start": "696630",
    "end": "702360"
  },
  {
    "text": "And one thing to note here is\nthis requires a matrix inverse, and so there are faster\nalgorithms than N cubed,",
    "start": "702360",
    "end": "708930"
  },
  {
    "text": "N being the number of states. But in general, matrix\ninverses are fairly expensive. So this is being done once,\nbut this is a fairly--",
    "start": "708930",
    "end": "716910"
  },
  {
    "text": "if your state space, the number\nof states you have N, is large, this can be expensive. And it also requires\nthat the identity matrix",
    "start": "716910",
    "end": "724500"
  },
  {
    "text": "minus gamma times the\ndynamics, the dynamics model is invertible.",
    "start": "724500",
    "end": "729828"
  },
  {
    "text": "OK, so this is one way\nwe could solve this. Yeah? And remind me your name. In practice, what\nusually happens?",
    "start": "729828",
    "end": "737380"
  },
  {
    "text": "Do people just go ahead and\ntake the matrix inverse? Let me reword the question.",
    "start": "737380",
    "end": "742710"
  },
  {
    "text": "In practice, do you usually find\nthat these kinds of matrices are invertible? And if yes, do people just\ngo ahead and take the matrix",
    "start": "742710",
    "end": "749940"
  },
  {
    "text": "inverse, or do they\n[INAUDIBLE] something? It's a good question. So in practice,\nis it invertible, and what do people do?",
    "start": "749940",
    "end": "755130"
  },
  {
    "text": "In practice, normally,\nwe're dealing with state spaces that are far\ntoo large, so we can't do this. Yeah, good question.",
    "start": "755130",
    "end": "762089"
  },
  {
    "text": "There might be cases where it's\nsmall enough, but in general, no. So that's a great motivation\nfor a second approach, which",
    "start": "762090",
    "end": "768240"
  },
  {
    "text": "is instead of doing it\ndirectly analytically, we're going to use\ndynamic programming, and we're going to design\nan iterative algorithm.",
    "start": "768240",
    "end": "774147"
  },
  {
    "text": "And this is going to be very,\nvery similar to what we're going to see for decision processes. So the idea in this\ncase is we're not",
    "start": "774147",
    "end": "780090"
  },
  {
    "text": "going to do this in\none step, but we're going to avoid that\nmatrix inverse, which might be pretty expensive. So we're going to initialize the\nvalue of a state to 0 for all s,",
    "start": "780090",
    "end": "788820"
  },
  {
    "text": "and you can think about\nwhether or not it actually matters what we initialize to. But just imagine we do that.",
    "start": "788820",
    "end": "794260"
  },
  {
    "text": "And then for a\nseries of iterations, k is our iteration variable. For all the states in s,\nwhat we do is we say--",
    "start": "794260",
    "end": "800920"
  },
  {
    "text": "we're going to make a new\ncopy of our value function, and we say Vk of s\nis equal to R of s",
    "start": "800920",
    "end": "806680"
  },
  {
    "text": "plus gamma sum over s\nprime probability of going to s prime given\ns times the value",
    "start": "806680",
    "end": "812470"
  },
  {
    "text": "that we already have for\nk minus 1 of s prime. And we just do this over\nand over and over again",
    "start": "812470",
    "end": "818620"
  },
  {
    "text": "until our value\nfunction stops changing, and we'll talk soon about\nwhether it will stop changing.",
    "start": "818620",
    "end": "825220"
  },
  {
    "text": "The nice thing about\nthis is that it's only s squared for each\niteration, so this",
    "start": "825220",
    "end": "830460"
  },
  {
    "text": "would be an iteration,\ninstead of a matrix inverse.",
    "start": "830460",
    "end": "835770"
  },
  {
    "text": " All right.",
    "start": "835770",
    "end": "840850"
  },
  {
    "text": "So this is how you could\ncompute the value of an MRP. Now we're going to see how\nwe could do that for an MDP.",
    "start": "840850",
    "end": "846090"
  },
  {
    "text": "So a Markov decision process is\nvery similar to a Markov reward process, but now we\nget to add in actions.",
    "start": "846090",
    "end": "851560"
  },
  {
    "text": "So now we're actually going to\nbe starting to make decisions. And the idea now is that\nthe dynamics transition",
    "start": "851560",
    "end": "857490"
  },
  {
    "text": "model will probably depend\non the action you take. So you're going to get to\ndifferent distributions",
    "start": "857490",
    "end": "862570"
  },
  {
    "text": "of next states. And so it could be\nsomething like you think of, depending on the ad\nyou show a customer,",
    "start": "862570",
    "end": "868670"
  },
  {
    "text": "they might do different things. Depending on the\ncontrols of your robot, it's going to move or manipulate\nits hand in a different way.",
    "start": "868670",
    "end": "874520"
  },
  {
    "text": "Generally, these\ndynamics are going to be a function of the\naction, and we are going to, for right now,\nassume the reward is",
    "start": "874520",
    "end": "881290"
  },
  {
    "text": "a function of the state\nand the action you take. So you often say that an\nMDP is defined by a tuple--",
    "start": "881290",
    "end": "886959"
  },
  {
    "text": "S, A, dynamics model,\nreward model, and gamma. So we could think\nof that for here.",
    "start": "886960",
    "end": "893540"
  },
  {
    "text": "So now we have our\nsame little Mars rover, but now we actually have two\ndifferent dynamics models,",
    "start": "893540",
    "end": "898580"
  },
  {
    "text": "one for if we take a1\nand one if we take a2. This is just an example. In these cases, these\nare deterministic.",
    "start": "898580",
    "end": "905470"
  },
  {
    "text": "In general, we can have\nthem be stochastic. And we would also need to\nspecify what the reward is, so maybe we have 0 reward\nin all of these states",
    "start": "905470",
    "end": "913540"
  },
  {
    "text": "and plus 1 here and\nplus 10 at the end. And this would just define. So once you've defined the\nstate space, the action space,",
    "start": "913540",
    "end": "920690"
  },
  {
    "text": "the reward function, the\ndynamics model, and the gamma, then you've defined your MDP. ",
    "start": "920690",
    "end": "927990"
  },
  {
    "text": "All right. So now we actually get to start\nto think about policies, which is what we'll be talking about\nthroughout the course, which is,",
    "start": "927990",
    "end": "934240"
  },
  {
    "text": "how do we make decisions\ndepending on the state we're in? And the policy is going to\nspecify the action to take,",
    "start": "934240",
    "end": "942590"
  },
  {
    "text": "which can be deterministic\nor stochastic, and often we're going to think\nof it as being stochastic.",
    "start": "942590",
    "end": "949399"
  },
  {
    "text": "And we'll talk about the\nproperties of stochastic versus deterministic\nones and why you might want one or the\nother quite a bit in the class,",
    "start": "949400",
    "end": "956610"
  },
  {
    "text": "but we can generally\ndo everything we're doing in each case. All right. So an MDP plus a policy is\njust a Markov reward process.",
    "start": "956610",
    "end": "966850"
  },
  {
    "text": "Why is that? Because once you specify\nhow you're going to act, you've removed the\npolicy part, and so",
    "start": "966850",
    "end": "972910"
  },
  {
    "text": "if you want to know how\ngood that policy is-- so let's say someone says--\nagain, your boss says, hey, how good is this thing at\nadvertising to customers,",
    "start": "972910",
    "end": "980240"
  },
  {
    "text": "for example? Then once you've decided\nwhat the policy is, we can think of\nthe reward as just",
    "start": "980240",
    "end": "986320"
  },
  {
    "text": "being a weighted sum over\nthe probability that's taking that action in that state\ntimes the reward for that state",
    "start": "986320",
    "end": "991900"
  },
  {
    "text": "in action and then your\ndynamics model, which is a little more subtle,\nwhich is now you're taking a weighted sum over\nall of the transition dynamics",
    "start": "991900",
    "end": "1000390"
  },
  {
    "text": "according to the action you take\nweighted by the probability you take that action. ",
    "start": "1000390",
    "end": "1007670"
  },
  {
    "text": "So it just defines\na Markov process because now you just have this\ntransformed dynamics model where",
    "start": "1007670",
    "end": "1014960"
  },
  {
    "text": "you've merged in the policy. So why is this helpful? And this is something\nthat you may or may not",
    "start": "1014960",
    "end": "1020870"
  },
  {
    "text": "have seen in previous classes. One of the reasons\nwhy this is helpful is because now we\ncan just say, oh, any techniques we have for\nMarkov reward processes we could",
    "start": "1020870",
    "end": "1027529"
  },
  {
    "text": "also apply to evaluating the\nvalue of a particular policy in a Markov decision\nprocess because we've just",
    "start": "1027530",
    "end": "1034140"
  },
  {
    "text": "reduced an MDP and policy\nevaluation back to an MRP. ",
    "start": "1034140",
    "end": "1042010"
  },
  {
    "text": "All right, so if we think\nabout doing policy evaluation with an MDP, we can just\nplug in the actual policy",
    "start": "1042010",
    "end": "1052100"
  },
  {
    "text": "that we would be using. So what we have in this case\nis that instead of-- now we",
    "start": "1052100",
    "end": "1059350"
  },
  {
    "text": "actually get to make decisions,\nand so then we get to say, what is the\nprobability of picking the action in this state\ntimes the expected discounted",
    "start": "1059350",
    "end": "1066632"
  },
  {
    "text": "sum of rewards at that point?  So this looks very\nsimilar to an MRP,",
    "start": "1066632",
    "end": "1072640"
  },
  {
    "text": "except for we're saying,\nbased on the probability for each action, what\nwould we get next?",
    "start": "1072640",
    "end": "1077890"
  },
  {
    "text": "And we call this a Bellman\nbackup for a particular policy",
    "start": "1077890",
    "end": "1083250"
  },
  {
    "text": "because this is going to specify\nwhat is our expected discounted sum of future rewards if\nwe start in this state",
    "start": "1083250",
    "end": "1090450"
  },
  {
    "text": "and follow the policy? And just notice that if\nthe policy is actually deterministic, we can\nreduce it back to a case",
    "start": "1090450",
    "end": "1098140"
  },
  {
    "text": "where we've sort of\naveraged over these rewards. So remember, this\nwas just going to be",
    "start": "1098140",
    "end": "1106929"
  },
  {
    "text": "if you have a\nparticular action, then you're just going to index\ninto what the reward is for that particular action.",
    "start": "1106930",
    "end": "1112250"
  },
  {
    "text": "So we can see that here. And just raise your hand\nif you've seen this before,",
    "start": "1112250",
    "end": "1118120"
  },
  {
    "text": "if you've seen the [INAUDIBLE]. OK, good, so probably\nat least 2/3 of people.",
    "start": "1118120",
    "end": "1123790"
  },
  {
    "text": "All right.  OK. So if you want to check your\nanswers, if some of this",
    "start": "1123790",
    "end": "1130260"
  },
  {
    "text": "is new for you,\nthen one thing to do is to try to check that you can\ndo this sort of value iteration",
    "start": "1130260",
    "end": "1136370"
  },
  {
    "text": "or this policy evaluation\nfor the Mars rover example. We won't go through it in class,\nbut you can check the answers.",
    "start": "1136370",
    "end": "1141760"
  },
  {
    "text": "I'll release them at the\nend of the slide just to check that you know\nhow to apply this. All right.",
    "start": "1141760",
    "end": "1147335"
  },
  {
    "text": "So of course,\nshortly we're going to be interested in\nnot just evaluating the value of a single policy\nbut finding an optimal policy.",
    "start": "1147335",
    "end": "1153240"
  },
  {
    "text": "So one question is, how\nmany policies are there? And is the optimal\npolicy value unique?",
    "start": "1153240",
    "end": "1161313"
  },
  {
    "text": "So we'll just take a second. You can go to the polls\nand enter in your answer. ",
    "start": "1161313",
    "end": "1175420"
  },
  {
    "text": "OK, great. So it looks like\nmost people got-- the vast majority of\npeople got the right answer for the first one, which\nis it's 2 to the 7.",
    "start": "1175420",
    "end": "1182420"
  },
  {
    "text": "In general, the number\nof policies we have is going to be A to the S\nbecause for every single state",
    "start": "1182420",
    "end": "1190270"
  },
  {
    "text": "we could choose\nany of the actions. And also, most people got the\nnext one right, which is great,",
    "start": "1190270",
    "end": "1195800"
  },
  {
    "text": "which is the optimal policy,\nthe one with the highest value, is not always unique.",
    "start": "1195800",
    "end": "1201280"
  },
  {
    "text": "It can be unique-- it\ndepends on the problem-- but it's not going\nto be unique whenever more than one action has\nthe same identical value,",
    "start": "1201280",
    "end": "1209613"
  },
  {
    "text": "so when you have ties. Yeah? How do we generally deal\nwith invalid actions?",
    "start": "1209613",
    "end": "1216360"
  },
  {
    "text": "Because for example, if we're\nin S1 and we choose left,",
    "start": "1216360",
    "end": "1221450"
  },
  {
    "text": "I would imagine-- to me,\nthat's an invalid action. I'm not sure how we\nreally deal with that.",
    "start": "1221450",
    "end": "1226940"
  },
  {
    "text": "Yeah, so the question was\nif we have invalid actions. So in general, you can have\na different action space",
    "start": "1226940",
    "end": "1232790"
  },
  {
    "text": "be possible in every state. That's also very common\nin recommendation engines that you'd only-- it's\nonly a subset of articles",
    "start": "1232790",
    "end": "1239240"
  },
  {
    "text": "you might show to some\npeople based on their state. In this particular\nexample, we're going to assume that it's\nnot actually go left.",
    "start": "1239240",
    "end": "1245550"
  },
  {
    "text": "It's try left. And so if you try to\ngo left and there's nothing in the\nrest of the world, you just fail, and you\nstay in the same place. But in general, most of\nthe time in the class,",
    "start": "1245550",
    "end": "1252780"
  },
  {
    "text": "we're going to assume\nthe action space is the same for all states,\nbut in some cases, it might be different. Good question. ",
    "start": "1252780",
    "end": "1261490"
  },
  {
    "text": "OK. So in MDP control,\nwe're going to want to not just have the policy-- evaluate a particular\npolicy, but we're going",
    "start": "1261490",
    "end": "1268160"
  },
  {
    "text": "to compute the optimal policy. So we want to take the arg max\nover the policy space, which in general is that\nA to the S space,",
    "start": "1268160",
    "end": "1276080"
  },
  {
    "text": "and there is going to exist a\nunique optimal value function. And the optimal policy\ninside of a tabular MDP",
    "start": "1276080",
    "end": "1284029"
  },
  {
    "text": "in an infinite horizon is\nunique and deterministic. So those are two properties that\nare good to be familiar with.",
    "start": "1284030",
    "end": "1292898"
  },
  {
    "text": "So now we're going\nto think about how do we actually compute this and\nwhat its other properties are.",
    "start": "1292898",
    "end": "1298080"
  },
  {
    "text": "So one is that it's stationary. What I mean by that here is that\nin infinite horizon problem,",
    "start": "1298080",
    "end": "1304650"
  },
  {
    "text": "you always have an infinite\nnumber of additional time steps, and so the optimal thing to\ndo just depends on your state.",
    "start": "1304650",
    "end": "1312270"
  },
  {
    "text": "It doesn't depend\non the time step. We'll think more about\nwhat happens when you only have a finite number,\nlike where H is finite",
    "start": "1312270",
    "end": "1319730"
  },
  {
    "text": "and what might happen there,\nbut for most of today, we're just going to focus on\nthe infinite horizon problem.",
    "start": "1319730",
    "end": "1325309"
  },
  {
    "text": "And as I said-- and most of you guys already\nknew that this, in general, is not unique. ",
    "start": "1325310",
    "end": "1332960"
  },
  {
    "text": "So one option is\npolicy search, and this is where we are going\nto get into-- oh,",
    "start": "1332960",
    "end": "1337970"
  },
  {
    "text": "yeah, and remind me your name. Is the optimality conditioned\non the initial state?",
    "start": "1337970",
    "end": "1343580"
  },
  {
    "text": "It's the optimality\nconditional on the-- what do you mean by that? The state of the [INAUDIBLE].",
    "start": "1343580",
    "end": "1350659"
  },
  {
    "text": "Yes, the optimality, yes,\nit will be per state. Yeah, so the optimal policy\nwill be defined per state.",
    "start": "1350660",
    "end": "1357683"
  },
  {
    "text": "The idea is that you can take a\ndifferent action in every state, and you want to know\nwhat the optimal thing is to do to maximize your\nexpected discounted",
    "start": "1357683",
    "end": "1363740"
  },
  {
    "text": "sum of rewards from every state\nindividually, like pointwise. Good question. Yeah? Yeah, two interconnected\nquestions--",
    "start": "1363740",
    "end": "1370520"
  },
  {
    "text": "why is there a unique\noptimal value function? And second is, can\nyou remind me again",
    "start": "1370520",
    "end": "1377210"
  },
  {
    "text": "of what was the reason why it\nmay not necessarily be unique? You mentioned a specific\ncase related to this.",
    "start": "1377210",
    "end": "1383210"
  },
  {
    "text": "So the optimal policy is\nnot necessarily unique because there could be more than\none action with the same value,",
    "start": "1383210",
    "end": "1388919"
  },
  {
    "text": "and the optimal\nvalue function is unique for reasons we'll\nsee later in this class, like later today.",
    "start": "1388920",
    "end": "1394400"
  },
  {
    "text": "We'll prove it. OK. So one of the things-- and\nthis is going to go back",
    "start": "1394400",
    "end": "1399679"
  },
  {
    "text": "to the conceptual question I\nput at the beginning of class-- is we would like to ideally\nhave methods and algorithms that",
    "start": "1399680",
    "end": "1405980"
  },
  {
    "text": "have monotonic\nimprovement capabilities, and so policy search is\ngoing to be one of those.",
    "start": "1405980",
    "end": "1411930"
  },
  {
    "text": "So what we're\ngoing to do here is we're going to try to search\nto compute the optimal policy. There's A to the S\ndeterministic policies.",
    "start": "1411930",
    "end": "1418980"
  },
  {
    "text": "In general, you could imagine\njust enumerating all of them and evaluating them all, but we\ncan often do better than that.",
    "start": "1418980",
    "end": "1425169"
  },
  {
    "text": "And when I say, \"better,\" what\nI mean here is we can reduce the computation needed to try\nto identify the optimal policy,",
    "start": "1425170",
    "end": "1431399"
  },
  {
    "text": "so we shouldn't have to iterate\nthrough all eight of the S policies. ",
    "start": "1431400",
    "end": "1437159"
  },
  {
    "text": "So how does policy\niteration work? The idea is that we're\ngoing to alternate between having a\ncandidate decision",
    "start": "1437160",
    "end": "1442679"
  },
  {
    "text": "policy that might be optimal. We're going to evaluate\nit, and then we're going to see if\nwe can improve it.",
    "start": "1442680",
    "end": "1447690"
  },
  {
    "text": "And then if we can improve it,\nwe will, and otherwise, we're going to halt. So what we do--\nhow we do this is",
    "start": "1447690",
    "end": "1453350"
  },
  {
    "text": "we're just going to initialize\nit randomly, which just means we're going to start off,\nand we're going to say, for every single state, we're\ngoing to pick an action.",
    "start": "1453350",
    "end": "1460230"
  },
  {
    "text": "And then while our policy\nis still changing-- so this is the L1 norm.",
    "start": "1460230",
    "end": "1465408"
  },
  {
    "text": "It measures if the policy\nchanged for any state just as a refresher. What we're going to\nfirst do is we're going to evaluate the\npolicy, and then we're",
    "start": "1465408",
    "end": "1472804"
  },
  {
    "text": "going to try to improve it. ",
    "start": "1472805",
    "end": "1478910"
  },
  {
    "text": "So in order to do that sort\nof policy improvement step, it's going to be helpful\nto define the Q-function.",
    "start": "1478910",
    "end": "1484135"
  },
  {
    "text": "Again, I know for many of you,\nthis is probably a review. The Q-function of\na particular policy is just, what is the reward of\nthe immediate state and action",
    "start": "1484135",
    "end": "1492170"
  },
  {
    "text": "plus the discounted\nsum of future rewards if we were to,\nafter that action, act according to the policy?",
    "start": "1492170",
    "end": "1499670"
  },
  {
    "text": "So it's sort of like\nsaying, OK, first, when you're in\nthis state, you're going to take this action,\nand then from then on, you're going to follow whatever\nyour policy tells you to do.",
    "start": "1499670",
    "end": "1508070"
  },
  {
    "text": "And for any of you\nwho've seen Q-learning, you've seen this\nsort of idea lot.",
    "start": "1508070",
    "end": "1513210"
  },
  {
    "text": "So what we're going to\ntry to do in this case-- why would we want a Q-function? It turns out it's going to\nmake the policy improvement",
    "start": "1513210",
    "end": "1519122"
  },
  {
    "text": "step really easy. So what we're going to first\ndo is we're going to say, I'm going to take my\nparticular policy. I'm going to compute the Q-value\nfor that particular policy, pi",
    "start": "1519122",
    "end": "1527685"
  },
  {
    "text": "i, because we're\ngoing to be iterating. And then after that,\nwe're going to compute a new policy, pi i plus 1, by\njust taking the arg max of Q.",
    "start": "1527685",
    "end": "1535690"
  },
  {
    "text": "So for our Q-function,\nwe're just going to say, according to this\nQ-function, which says, What is the expected\ndiscounted sum of rewards?",
    "start": "1535690",
    "end": "1543170"
  },
  {
    "text": "if I start in this\nstate, take this action that follows pi, which of\nthose actions is the best?",
    "start": "1543170",
    "end": "1549460"
  },
  {
    "text": "And we can define\nthat per state. Yeah? Is there any relationship\nbetween the Q-function",
    "start": "1549460",
    "end": "1555970"
  },
  {
    "text": "and the value function? Because it kind\nof looks similar. Yeah, yeah. So we often call\nit the Q-function the state-action value function.",
    "start": "1555970",
    "end": "1562170"
  },
  {
    "text": " All right.",
    "start": "1562170",
    "end": "1567220"
  },
  {
    "text": "So this is sort of\njust what we do now. Now we're going to\nhave this Q-function. We're generally going to\ndo this by having this Q.",
    "start": "1567220",
    "end": "1574760"
  },
  {
    "text": "And then we will\ndo pi i plus 1 of s is equal to arg max over of\na of Q of s, a per state.",
    "start": "1574760",
    "end": "1586590"
  },
  {
    "text": "And then we just repeat\nthis over and over again. OK, so there's a\nnumber of questions",
    "start": "1586590",
    "end": "1591917"
  },
  {
    "text": "you might have about this. You might say, OK, this seems\nlike a vaguely reasonable thing to do, but does it have\nany formal properties?",
    "start": "1591917",
    "end": "1597530"
  },
  {
    "text": "Are we guaranteed to improve? What can we say about this? So to do that, I think\nit's useful to delve",
    "start": "1597530",
    "end": "1603700"
  },
  {
    "text": "into what the policy improvement\nstep is actually doing. So what the policy improvement--",
    "start": "1603700",
    "end": "1610750"
  },
  {
    "text": "when we compute the\nQ-function, this is the equation\nfor the Q-function. So we take our old\npolicy pi i, and then",
    "start": "1610750",
    "end": "1618760"
  },
  {
    "text": "we compute the\nQ-function of this. And we can do this iteratively.",
    "start": "1618760",
    "end": "1624700"
  },
  {
    "text": "And now what we want\nto do in this case is think about what is the\nperformance going to be",
    "start": "1624700",
    "end": "1630130"
  },
  {
    "text": "of the new policy we extract.  All right. So what the Q-function\nsays is we're",
    "start": "1630130",
    "end": "1639520"
  },
  {
    "text": "going to be able to show that\nthe Q-function-- the best thing of the\nQ-function is better",
    "start": "1639520",
    "end": "1644800"
  },
  {
    "text": "than the value of\nthe old policy. So what does this say? So the first thing is\njust how we've compute it.",
    "start": "1644800",
    "end": "1652000"
  },
  {
    "text": "This is just the\npolicy evaluation step. ",
    "start": "1652000",
    "end": "1658260"
  },
  {
    "text": "And we know that if we have\na Q-function over s and a,",
    "start": "1658260",
    "end": "1663510"
  },
  {
    "text": "for a particular s max\nover a of Q pi of s, a, has to be at least as good\nas the Q-function for any",
    "start": "1663510",
    "end": "1671760"
  },
  {
    "text": "of the actions. So we know that this has to be-- this thing is always greater\nthan or equal to Q pi i of s, a",
    "start": "1671760",
    "end": "1680070"
  },
  {
    "text": "for all a.  And then this is\njust that equation.",
    "start": "1680070",
    "end": "1686580"
  },
  {
    "text": "This is just what\nexactly is Q pi i of s, a, just the definition, almost,\nexcept for it's particularly",
    "start": "1686580",
    "end": "1695660"
  },
  {
    "text": "for the actions that-- this\nis for specifically if we were",
    "start": "1695660",
    "end": "1704760"
  },
  {
    "text": "to follow the previous policy. So remember, this is the\nequation for Q pi i of s, a.",
    "start": "1704760",
    "end": "1711930"
  },
  {
    "text": "Think about one of those\nactions that you could have done is exactly what the old policy\nwould have told you to do.",
    "start": "1711930",
    "end": "1717130"
  },
  {
    "text": "That is what this equation is. You just take a here, and\nyou plug in pi i of a.",
    "start": "1717130",
    "end": "1722620"
  },
  {
    "start": "1722620",
    "end": "1732700"
  },
  {
    "text": "So that's just\nexactly what this is, and that is just the\ndefinition of V pi i of s. ",
    "start": "1732700",
    "end": "1739799"
  },
  {
    "text": "OK, so what is this saying? What this is saying is if you\nwere to take your new policy,",
    "start": "1739800",
    "end": "1746880"
  },
  {
    "text": "pi i plus 1-- so\nremember, pi i plus 1 is defined as the arg\nmax of this Q-function.",
    "start": "1746880",
    "end": "1752220"
  },
  {
    "text": "It's whatever maximizes\nyour new Q-function. So what this says is if you\nwere to take pi i plus 1 of s",
    "start": "1752220",
    "end": "1757950"
  },
  {
    "text": "for one action and then\nfollow pi i forever-- so that's what the\nQ-function represents--",
    "start": "1757950",
    "end": "1764490"
  },
  {
    "text": "then our expected sum of\nrewards is at least as good as if we'd always followed pi i.",
    "start": "1764490",
    "end": "1770100"
  },
  {
    "text": "So that's what this\nequation is telling us. It's like, if I get to make one\ndecision differently and then from then on, I follow my old\npolicy, the value I can expect",
    "start": "1770100",
    "end": "1778350"
  },
  {
    "text": "is at least as good as the value\nI could expect if I had always followed the old policy. ",
    "start": "1778350",
    "end": "1785328"
  },
  {
    "text": "Does anybody have any\nquestions about that? Because then the next step\nis going to build on that. ",
    "start": "1785328",
    "end": "1793390"
  },
  {
    "text": "Yeah? Can you go back to the\nalgorithm [INAUDIBLE]? Sure. For the policy improvement? Yeah, yeah. ",
    "start": "1793390",
    "end": "1802090"
  },
  {
    "text": "The next slide,\nactually, on [INAUDIBLE]. So the step that we\nare talking about",
    "start": "1802090",
    "end": "1808640"
  },
  {
    "text": "is this one, right, the\npolicy improvements? Yeah. We're trying to see, when we\ndo the policy improvement step",
    "start": "1808640",
    "end": "1815270"
  },
  {
    "text": "and we extract out,\ninstead of max here, arg max to get out\nthe new policy, how does the value\nof that relate",
    "start": "1815270",
    "end": "1820895"
  },
  {
    "text": "to the value of the\nthing you could have done before in that state? And so this is just trying to\nsay, what really is Q pi i of s,",
    "start": "1820895",
    "end": "1829230"
  },
  {
    "text": "a? It is the value you\nget if you first take a and then you follow pi\ni from then onwards?",
    "start": "1829230",
    "end": "1835770"
  },
  {
    "text": "So it's saying if you were to\ndo that, then this new action you've computed,\nthis arg max policy,",
    "start": "1835770",
    "end": "1841310"
  },
  {
    "text": "is actually better than what\nyou would have gotten before or at least as good.",
    "start": "1841310",
    "end": "1846680"
  },
  {
    "text": "But the thing that should\nseem slightly strange to you is I am not creating this\nsort of hybrid policy",
    "start": "1846680",
    "end": "1852740"
  },
  {
    "text": "where I take one new action\nand then I follow pi i forever. I'm creating an\nentirely new policy",
    "start": "1852740",
    "end": "1858710"
  },
  {
    "text": "where I'm not just going to\nfollow pi i plus 1 for one step. I'm going to follow it\nfor all remaining steps.",
    "start": "1858710",
    "end": "1864750"
  },
  {
    "text": "So this should not yet\nconvince you that doing that is actually any better\nthan my old policy.",
    "start": "1864750",
    "end": "1870840"
  },
  {
    "text": "This would say, if you\ntake one new action and then follow your\nold policy, it's going to be better\nthan your old policy.",
    "start": "1870840",
    "end": "1876059"
  },
  {
    "text": "So that's why we have to\ndo additional work to show that we're actually going to get\na monotonic improvement if we",
    "start": "1876060",
    "end": "1881310"
  },
  {
    "text": "just follow this new policy. always. All right. So let's go through that.",
    "start": "1881310",
    "end": "1887169"
  },
  {
    "text": "So what we're going to\nprove is we're going to say, actually, that's true. The new policy we construct\nthrough this policy improvement",
    "start": "1887170",
    "end": "1893700"
  },
  {
    "text": "step is somewhat\nremarkably going to be strictly a monotonic\nimprovement compared",
    "start": "1893700",
    "end": "1899580"
  },
  {
    "text": "to the old policy\nunless it's identical. So that means at every\nstep of policy improvement",
    "start": "1899580",
    "end": "1905390"
  },
  {
    "text": "we're going to get a better and\nbetter policy for every state. ",
    "start": "1905390",
    "end": "1912926"
  },
  {
    "text": "And the only time we're not\nis if we've already converged. So let's go through that.",
    "start": "1912926",
    "end": "1919510"
  },
  {
    "text": "So this is going to prove to us\nthat the value of the old policy is less than or equal to\nthe value of the new policy,",
    "start": "1919510",
    "end": "1926390"
  },
  {
    "text": "meaning we're going to get\nthis monotonic improvement. So what we're going\nto do in this case",
    "start": "1926390",
    "end": "1932500"
  },
  {
    "text": "is we are going to\nfirst write out-- so this is just the definition. This is the definition of\nmax over a over Q pi i.",
    "start": "1932500",
    "end": "1946580"
  },
  {
    "text": "All right. So let's just write\nout what this is. This is going to be equal to--",
    "start": "1946580",
    "end": "1951930"
  },
  {
    "text": "and it will be written out more\nneatly on the next page too. ",
    "start": "1951930",
    "end": "1971010"
  },
  {
    "text": "OK, so what did I do here? I noticed that the\ndefinition of pi i plus 1",
    "start": "1971010",
    "end": "1977580"
  },
  {
    "text": "is exactly the arg max of this\nexpression instead of max. So when we did the\npolicy improvement,",
    "start": "1977580",
    "end": "1983667"
  },
  {
    "text": "the way we did the\npolicy improvement was we took the arg max\nof the Q-function. So instead of having\nthis max out here,",
    "start": "1983667",
    "end": "1990159"
  },
  {
    "text": "I'm just going to\nplug in pi i plus 1 because that's going\nto give me something that's exactly equal to the max\na for that whole expression.",
    "start": "1990160",
    "end": "1999880"
  },
  {
    "text": "All right. And so this is exactly equal to\nthat, but what we can show next or what we can do next is that\nwe can just add the same terms",
    "start": "1999880",
    "end": "2008860"
  },
  {
    "text": "and notice that\nthis is the same. This is less than or equal\nto Q pi i of s prime,",
    "start": "2008860",
    "end": "2015380"
  },
  {
    "text": "a prime because the value\nof pi i for s prime,",
    "start": "2015380",
    "end": "2021940"
  },
  {
    "text": "so following a\nparticular policy, always has to be less\nthan or equal to taking",
    "start": "2021940",
    "end": "2026980"
  },
  {
    "text": "the max over the\nQ-function for that policy. Why is that true? Because either the max is\nthe same as the pi i action",
    "start": "2026980",
    "end": "2035205"
  },
  {
    "text": "or there's a better action. ",
    "start": "2035205",
    "end": "2042250"
  },
  {
    "text": "All right, so that's\nthe less than or equals. And then we can just\nexpand this expression out, and this is going to start to\nget a little bit messy, which",
    "start": "2042250",
    "end": "2049148"
  },
  {
    "text": "is why it'll be nice to have\nit on the next slide too. But what will happen here is you\ncan see how the expansion works.",
    "start": "2049148",
    "end": "2055649"
  },
  {
    "start": "2055650",
    "end": "2060730"
  },
  {
    "text": "And why is this important? This is important\nbecause this is going to allow us to\nthink about not if we just take this new action\non the first step",
    "start": "2060730",
    "end": "2067250"
  },
  {
    "text": "but on all future steps. So what we had here is\nwe had this thing which was max a over Q pi i\nof s prime, a prime.",
    "start": "2067250",
    "end": "2074779"
  },
  {
    "text": "We're going to expand out\nwhat that expression is. Because notice,\nthis thing here is",
    "start": "2074780",
    "end": "2082388"
  },
  {
    "text": "exactly equal to this thing,\nwhich we know is here. So we're just going\nto substitute it.",
    "start": "2082389",
    "end": "2088879"
  },
  {
    "text": "So we can put that in here. So this is R of s prime, pi i\nplus 1 of s prime plus gamma sum",
    "start": "2088880",
    "end": "2098530"
  },
  {
    "text": "over s double prime,\nmeaning s double prime here I'm just using to be\ntwo time steps away.",
    "start": "2098530",
    "end": "2103964"
  },
  {
    "start": "2103965",
    "end": "2112820"
  },
  {
    "text": "Why was that useful? Well, what we've just said\nis that the value of pi of s",
    "start": "2112820",
    "end": "2117980"
  },
  {
    "text": "is less than or equal to\ntaking this new, better action for one time step and\nthen following the old policy.",
    "start": "2117980",
    "end": "2124245"
  },
  {
    "text": "I've now done that\nrecursively, so I've said, well, now that's also\nless than or equal to if I take that new action\nonce and then I take it again",
    "start": "2124245",
    "end": "2131600"
  },
  {
    "text": "and then I follow\nthe old policy. And then you just repeat this. So you just keep nesting this.",
    "start": "2131600",
    "end": "2138120"
  },
  {
    "text": "And what you can\nsee here is that you have these less\nthan or equal that happen when instead of plugging\nin the value of the old policy",
    "start": "2138120",
    "end": "2145049"
  },
  {
    "text": "you allow yourself to take\na max over that Q-function. And what happens if you\ndo this all the way out?",
    "start": "2145050",
    "end": "2151530"
  },
  {
    "text": "This will exactly become equal\nto V of pi i plus 1 of s,",
    "start": "2151530",
    "end": "2156830"
  },
  {
    "text": "so dot, dot, dot, dot. So I have that here.",
    "start": "2156830",
    "end": "2162030"
  },
  {
    "text": "So what this has shown\nhere is that the value you had under the old\npolicy of the state",
    "start": "2162030",
    "end": "2167070"
  },
  {
    "text": "is less than or equal to\nthe value of that state under the new policy,\nso this proves",
    "start": "2167070",
    "end": "2172260"
  },
  {
    "text": "the monotonic improvement,\nwhich is super cool. So this now says if we do\npolicy evaluation where you just",
    "start": "2172260",
    "end": "2178470"
  },
  {
    "text": "keep computing the\nQ-function and taking a max, you will always\nmonotonically improve unless you stay the same.",
    "start": "2178470",
    "end": "2184355"
  },
  {
    "start": "2184355",
    "end": "2190240"
  },
  {
    "text": "All right, so now let's do our\nnext Check Your Understanding, which is, given\neverything I've just said,",
    "start": "2190240",
    "end": "2195970"
  },
  {
    "text": "if the policy doesn't change,\ncan it ever change again? And is there a maximum number of\niterations of policy iteration?",
    "start": "2195970",
    "end": "2205270"
  },
  {
    "text": "Yeah? On the previous slide,\nis the dot, dot, dot supposed to represent just\nalgebraic manipulation or--",
    "start": "2205270",
    "end": "2212410"
  },
  {
    "text": "Yeah, you just keep expanding\nthis all the way out, yeah. Good question. ",
    "start": "2212410",
    "end": "2220682"
  },
  {
    "text": "All right, let's take a\nsecond and do the poll. ",
    "start": "2220682",
    "end": "2287589"
  },
  {
    "text": "Yeah? What's your name?  At what point did we show\nthat this is actually",
    "start": "2287590",
    "end": "2294110"
  },
  {
    "text": "leading to an improvement? Can we just like stay\nin the same value level?",
    "start": "2294110",
    "end": "2301020"
  },
  {
    "text": "Because the inequality was\ngreater than or equal to, so is it possible\nthat you're always",
    "start": "2301020",
    "end": "2306350"
  },
  {
    "text": "equal to where you started? Yeah, it's a great\nquestion, and, in fact, that really-- so I've just\nshown less than or equal to.",
    "start": "2306350",
    "end": "2316700"
  },
  {
    "text": "What we can-- well, I guess we\ncan discuss this in a second, but it will be a monotonic\nimprovement unless you're",
    "start": "2316700",
    "end": "2323600"
  },
  {
    "text": "the optimal policy. So if there's any state\nat which you can improve, you will, and if\nyou stay the same--",
    "start": "2323600",
    "end": "2330840"
  },
  {
    "text": "we'll, actually, we'll\ntalk about this now because it's nicely split\nbetween the answers for both",
    "start": "2330840",
    "end": "2335930"
  },
  {
    "text": "of these questions. So maybe everybody turn to\nsomebody nearby you, and discuss",
    "start": "2335930",
    "end": "2341420"
  },
  {
    "text": "whether you think the\npolicy can ever change if it didn't change initially. And is there a maximum\nnumber of iterations?",
    "start": "2341420",
    "end": "2348140"
  },
  {
    "text": "Because it's pretty evenly\nsplit amongst people who voted. [INTERPOSING VOICES]",
    "start": "2348140",
    "end": "2355646"
  },
  {
    "start": "2355646",
    "end": "2403623"
  },
  {
    "text": "I think that's the maximum-- [INTERPOSING VOICES] ",
    "start": "2403623",
    "end": "2428980"
  },
  {
    "text": "I guess in this example,\nthere's only one option-- [INTERPOSING VOICES] ",
    "start": "2428980",
    "end": "2450160"
  },
  {
    "text": "Yeah, I agree, totally. [INTERPOSING VOICES] I don't see how\npeople saying that-- I want to make sure\nto clarify something",
    "start": "2450160",
    "end": "2456837"
  },
  {
    "text": "because that came up in a\ngood conversation, which is let's assume for the\nmoment there are no ties. I know I said that, in general,\nthe optimistic policies can",
    "start": "2456837",
    "end": "2464500"
  },
  {
    "text": "have ties, and that's true. But for the point of\nview of this question, it is easiest to think\nabout if there is only",
    "start": "2464500",
    "end": "2469630"
  },
  {
    "text": "a single unique optimal policy. So why don't we do that? Again, none of these\nare for assessment.",
    "start": "2469630",
    "end": "2476388"
  },
  {
    "text": "They're only for your learning. But just in terms of what\nyou're thinking through, my intention was to think about\nthe simpler case where there",
    "start": "2476388",
    "end": "2482780"
  },
  {
    "text": "is a single optimal policy\nand then under that case, whether the policy\ncan ever change",
    "start": "2482780",
    "end": "2488930"
  },
  {
    "text": "once it hasn't changed once. What I mean by\nthe policy doesn't change-- meaning when\nwe have had a policy",
    "start": "2488930",
    "end": "2494550"
  },
  {
    "text": "and we do policy improvement\nand our new, improved policy is the same as the old policy.",
    "start": "2494550",
    "end": "2501080"
  },
  {
    "text": "So under the case\nthat I just said, which is that it's\ndeterministic and that there is a single optimal\npolicy, raise your hand",
    "start": "2501080",
    "end": "2509930"
  },
  {
    "text": "if you said the policy,\nonce it doesn't change, it can never change again? That's the correct answer. Does somebody want\nto explain why?",
    "start": "2509930",
    "end": "2516595"
  },
  {
    "text": " You're all correct.",
    "start": "2516595",
    "end": "2523440"
  },
  {
    "text": "Yeah? Remind me your name. It kind of intuitively made\nsense in the sense of you're",
    "start": "2523440",
    "end": "2530200"
  },
  {
    "text": "doing the expected value. So you're summing over all-- or you're summing over\nall of the actions.",
    "start": "2530200",
    "end": "2536380"
  },
  {
    "text": "Even if there's\nstochasticity on the system, you're still taking\nthe average value. So like if it didn't change\nbefore, it won't change change.",
    "start": "2536380",
    "end": "2544880"
  },
  {
    "text": "Yeah, you are taking those and\nso definitely along those lines. So if we look at what was\nthe definition of the policy",
    "start": "2544880",
    "end": "2551373"
  },
  {
    "text": "improvement step-- let me just\ngo a couple of slides back. So what we said is we\ncomputed the Q-function,",
    "start": "2551373",
    "end": "2557360"
  },
  {
    "text": "and then we extracted\na new policy. If pi i plus 1 is\nthe same as pi i,",
    "start": "2557360",
    "end": "2565040"
  },
  {
    "text": "is Q of pi i plus 1 equal\nto Q of pi i plus 1?",
    "start": "2565040",
    "end": "2570050"
  },
  {
    "text": "All right. I probably said that wrong. There's too many i's. Let me just write it out. So the question is, if pi\ni is equal to pi i plus 1,",
    "start": "2570050",
    "end": "2579880"
  },
  {
    "text": "is Q pi i equal\nto Q pi i plus 1?",
    "start": "2579880",
    "end": "2584960"
  },
  {
    "text": " So if it's the same policy, do\nthey have the same Q-function?",
    "start": "2584960",
    "end": "2591220"
  },
  {
    "text": "Yeah, I'm seeing\na bunch of nods. OK, so if your policy\nhasn't changed, meaning that your old policy\nis the same as your new policy,",
    "start": "2591220",
    "end": "2598340"
  },
  {
    "text": "then Q pi i is equal\nto Q pi i plus 1, which means that when you\ndo this for Q pi i plus 1",
    "start": "2598340",
    "end": "2604480"
  },
  {
    "text": "and then try to\nextract a policy, it'll be exactly the same. So once you're stuck there,\nit'll be stuck forever.",
    "start": "2604480",
    "end": "2610900"
  },
  {
    "text": "Now, if you have ties,\nit's more complicated. So if you have\nmultiple actions that can achieve the same Q-function,\nit depends how you break them.",
    "start": "2610900",
    "end": "2618380"
  },
  {
    "text": "If you break them\ndeterministically, you'll stay in the same place. If not, you may oscillate\nbetween all the policies",
    "start": "2618380",
    "end": "2623740"
  },
  {
    "text": "which are optimal, otherwise\nknown as all the policies for which they have\nthe same Q pi i.",
    "start": "2623740",
    "end": "2630370"
  },
  {
    "text": "But in the simpler\ncase that I mentioned, once you've got to that single\npolicy, you won't ever change.",
    "start": "2630370",
    "end": "2636970"
  },
  {
    "text": "And what that means is, given\nthat we also only have a finite number of policies if\nit's deterministic--",
    "start": "2636970",
    "end": "2643579"
  },
  {
    "text": "so we're assuming\nif we stick to-- so I'll just say, no if pi\nstar is unique for all s.",
    "start": "2643580",
    "end": "2655480"
  },
  {
    "text": "So that means for every state\nthere's a unique optimal action. Is there a maximum number of\niterations for policy iteration?",
    "start": "2655480",
    "end": "2661480"
  },
  {
    "text": "If you have\ndeterministic policies, there's only A to\nthe S policies, as everyone was saying\nbefore, which is great,",
    "start": "2661480",
    "end": "2667630"
  },
  {
    "text": "and so since the policy\nimprovement step either improves the value of\nyour policy or halts, that",
    "start": "2667630",
    "end": "2673660"
  },
  {
    "text": "means you only go through each\npolicy once, at most once.",
    "start": "2673660",
    "end": "2679829"
  },
  {
    "text": "There might be some never\nbother to go through. And so that means that\npolicy iteration will halt, and it will take, at\nmost, A to the S policies.",
    "start": "2679830",
    "end": "2686538"
  },
  {
    "text": "If it takes A to the\nS, that means that you evaluated every single policy. In general, you won't.",
    "start": "2686538",
    "end": "2691575"
  },
  {
    "text": " So this is what shows\nthat we actually do",
    "start": "2691575",
    "end": "2697180"
  },
  {
    "text": "get this monotonic improvement. This is really nice. With every single-- because\nyou could imagine in cases where there's a-- oh, question?",
    "start": "2697180",
    "end": "2704410"
  },
  {
    "text": "Yeah, sure. [INAUDIBLE] that we're going to\ndo better than random, right?",
    "start": "2704410",
    "end": "2711700"
  },
  {
    "text": "We haven't guaranteed that\nwhatever we converge to is better than random.",
    "start": "2711700",
    "end": "2717089"
  },
  {
    "text": "We've proven that we're going\nto get to the optimal policy. And the optimal policy\nmay be just random, right?",
    "start": "2717090",
    "end": "2724000"
  },
  {
    "text": "Because depending on the\nenvironment, you might just-- there is no-- you can't\ndo better than random.",
    "start": "2724000",
    "end": "2730960"
  },
  {
    "text": "You mean in terms of\nhow you design actions? Yeah, so for example,\nif it is the case that all of your actions\nhave exactly the same reward,",
    "start": "2730960",
    "end": "2737970"
  },
  {
    "text": "it doesn't matter\nwhether you act randomly or you follow a policy. The value you would get would\nbe exactly the same as random.",
    "start": "2737970",
    "end": "2743770"
  },
  {
    "text": "Whether or not you can\ndo better than random will depend on the domain. The hope is, in general,\nwe can do a lot better.",
    "start": "2743770",
    "end": "2749220"
  },
  {
    "text": " OK, so we've shown now that\nhere is an algorithm where,",
    "start": "2749220",
    "end": "2755190"
  },
  {
    "text": "as we do more and\nmore computation, we get better and\nbetter policies. And this is great because you\nmay not actually want to go--",
    "start": "2755190",
    "end": "2761470"
  },
  {
    "text": "particularly if the state\nspace is very large, you may not want to go until\nwhere your policy entirely stops changing.",
    "start": "2761470",
    "end": "2767240"
  },
  {
    "text": "So if you have an\nenergy time requirement, you can still\nguarantee that, hey, I'm getting better and better,\nand maybe I stop after 100",
    "start": "2767240",
    "end": "2773925"
  },
  {
    "text": "iterations or 1,000 iterations\nand just use that policy. So this is one which has that\nnice monotonicity guarantee.",
    "start": "2773925",
    "end": "2782240"
  },
  {
    "text": "Sorry. Can you say what\nthat is, for example? Oh, sure, yes. And what's your name?",
    "start": "2782240",
    "end": "2788050"
  },
  {
    "text": "Yeah, so A here is\nthe number of actions, and S here is the\nnumber of states. So the decision policy\nspace is for every state,",
    "start": "2788050",
    "end": "2795020"
  },
  {
    "text": "you could pick one\nof the actions, so you multiply all of those. ",
    "start": "2795020",
    "end": "2804690"
  },
  {
    "text": "So this also shows\nhere about how, yeah, exactly what I\nsaid on the previous one, that if your policy\ndoesn't change,",
    "start": "2804690",
    "end": "2810579"
  },
  {
    "text": "it'll never change again,\nagain, assuming pi star",
    "start": "2810580",
    "end": "2818410"
  },
  {
    "text": "is unique per S.",
    "start": "2818410",
    "end": "2824680"
  },
  {
    "text": "OK, so that's one way to go. One way is that we can\ndo policy iteration. And the interesting thing\nabout policy iteration",
    "start": "2824680",
    "end": "2830410"
  },
  {
    "text": "is that every time point\nyou have an explicit policy, and what that policy tells\nyou is how to act forever",
    "start": "2830410",
    "end": "2837910"
  },
  {
    "text": "using that policy. And when you\ncompute the Q-value, it says, how much\nreward do you get if you",
    "start": "2837910",
    "end": "2842978"
  },
  {
    "text": "take this action in this state? And then follow that\nother policy forever. So again, remember, today we're\nin the infinite horizon case",
    "start": "2842978",
    "end": "2850720"
  },
  {
    "text": "unless I specify otherwise. But along the way, a lot of\nthose actions and the decisions",
    "start": "2850720",
    "end": "2857289"
  },
  {
    "text": "we make may not be very good. So your early policies\nmight be pretty bad. We know we're\nmonotonically improving, but the early\npolicies might be bad.",
    "start": "2857290",
    "end": "2864160"
  },
  {
    "text": "Value iteration is different. The idea is that\nat every iteration, we're going to maintain the\noptimal value of starting",
    "start": "2864160",
    "end": "2871240"
  },
  {
    "text": "in a state but as if\nwe only get to make a finite number of decisions. So remember, in\npolicy iteration,",
    "start": "2871240",
    "end": "2877970"
  },
  {
    "text": "we always have a policy, and we\nhave the value of acting in it forever. It just might not be very good.",
    "start": "2877970",
    "end": "2883410"
  },
  {
    "text": "Value iteration is, what\nis the optimal thing for me to do if I can just make one\ndecision, I can take one step?",
    "start": "2883410",
    "end": "2890132"
  },
  {
    "text": "OK, I'm going to figure out\nwhat the optimal thing is to do for one step. Now I'm going to imagine\nI get to take two steps,",
    "start": "2890132",
    "end": "2895640"
  },
  {
    "text": "and I'm going to build on what\nI know I can do for one step. And so now I'll build\nthe optimal thing to do for two steps.",
    "start": "2895640",
    "end": "2901940"
  },
  {
    "text": "So the interesting thing\nwith value iteration is you always have an optimal\nvalue but for the wrong horizon.",
    "start": "2901940",
    "end": "2907850"
  },
  {
    "text": "So one has a value for\nthe infinite horizon. It might be a bad policy.",
    "start": "2907850",
    "end": "2913770"
  },
  {
    "text": "The other one has the\noptimal value and thing to do but for the wrong horizon.",
    "start": "2913770",
    "end": "2920010"
  },
  {
    "text": "And the idea in\nvalue iteration is you just keep going to longer\nand longer and longer episodes, thinking of getting to do H\nplus 1 steps or H plus 2 steps,",
    "start": "2920010",
    "end": "2927930"
  },
  {
    "text": "and then you build upon\nyour previous solutions using dynamic programming. So let's see how to do that.",
    "start": "2927930",
    "end": "2933920"
  },
  {
    "text": "OK, so this is where we get\ninto the Bellman equation. This is the seminal\nwork of Richard Bellman.",
    "start": "2933920",
    "end": "2940070"
  },
  {
    "text": "And the idea here\nis, as we've said, is that for a particular policy,\nwe satisfy the Bellman equation,",
    "start": "2940070",
    "end": "2946550"
  },
  {
    "text": "and we can turn that\ninto an algorithm. So in particular, there's a\nthing called the Bellman backup",
    "start": "2946550",
    "end": "2953599"
  },
  {
    "text": "operator, and what it\nsays is if you give me a value function, which\nright now we can think",
    "start": "2953600",
    "end": "2958625"
  },
  {
    "text": "of as just being a vector-- later we'll get into\nfunction approximation-- and we do a Bellman\nbackup, essentially, it's",
    "start": "2958625",
    "end": "2964339"
  },
  {
    "text": "like saying, I had\na value function, and I want to think\nabout, what should I do if I get to do the\nbest thing that maximizes",
    "start": "2964340",
    "end": "2970430"
  },
  {
    "text": "my immediate reward plus my\nexpected future reward given that value function?",
    "start": "2970430",
    "end": "2975800"
  },
  {
    "text": "So it says, I'm\ngoing to figure out, if I take a max over\nall the actions, what's the reward of that\nstate in the action",
    "start": "2975800",
    "end": "2982230"
  },
  {
    "text": "plus the discounted sum\nof rewards using the value function you've given me?",
    "start": "2982230",
    "end": "2988119"
  },
  {
    "text": "And what that does is\nit yields a new vector of values over all your states.",
    "start": "2988120",
    "end": "2994570"
  },
  {
    "text": "So this is being done\nper state, and this is called the Bellman operator. It comes up all the time.",
    "start": "2994570",
    "end": "3000302"
  },
  {
    "text": " All right. So how do we do value iteration?",
    "start": "3000302",
    "end": "3006670"
  },
  {
    "text": "Well, we're just going\nto do this recursively, so we're just going to loop\nuntil we hit convergence.",
    "start": "3006670",
    "end": "3013690"
  },
  {
    "text": "Just as a refresher, this\nis the L-infinity norm, and what that\nmeans is that it is",
    "start": "3013690",
    "end": "3020290"
  },
  {
    "text": "equal to the max over S V of s.",
    "start": "3020290",
    "end": "3025560"
  },
  {
    "text": "Maybe I'll write it out more\ncarefully, equal to max over",
    "start": "3025560",
    "end": "3034430"
  },
  {
    "text": "S Vk plus 1 of s minus Vk of s.",
    "start": "3034430",
    "end": "3040069"
  },
  {
    "text": "What that just means is that\nif you have two vectors, you look for every\nsingle entry, and you",
    "start": "3040070",
    "end": "3045560"
  },
  {
    "text": "find the entry in which those\ntwo vectors are most different. And that's the\nL-infinity norm just as",
    "start": "3045560",
    "end": "3051003"
  },
  {
    "text": "a refresher for some of you\nwho might not have seen it or not seen it recently. So what value iteration does is\nwe're just going to have a loop.",
    "start": "3051003",
    "end": "3058368"
  },
  {
    "text": "It's going to look very similar\nto what we saw for the Markov reward process. We're going to initialize\nour value function,",
    "start": "3058368",
    "end": "3064210"
  },
  {
    "text": "and then for each\nstate, we're just going to do this Bellman backup. And so it's like we took\nour previous value function.",
    "start": "3064210",
    "end": "3070550"
  },
  {
    "text": "We do our Bellman backup, and\nwe get a new value function. And we do this over and over\nand over again until our value",
    "start": "3070550",
    "end": "3076480"
  },
  {
    "text": "function stops changing. So for policy iteration, we kept\ngoing until our policy stops",
    "start": "3076480",
    "end": "3082099"
  },
  {
    "text": "changing. Here we keep going until our\nvalue function stops changing. And what that condition\nmeans is it says,",
    "start": "3082100",
    "end": "3089609"
  },
  {
    "text": "I keep going until\nthe difference between my old value of estate\nand my new value of estate is really small\nfor all the states.",
    "start": "3089610",
    "end": "3096590"
  },
  {
    "text": "Yeah? And remind me your name. I have a question on how\nto connect this value",
    "start": "3096590",
    "end": "3102840"
  },
  {
    "text": "iteration to-- you just said it works\nwith finite horizon",
    "start": "3102840",
    "end": "3108300"
  },
  {
    "text": "and why policy iteration\nworks if it's infinite. Yeah, good question. So what you could think of\nthis as-- so great question.",
    "start": "3108300",
    "end": "3114750"
  },
  {
    "text": "At the beginning, you don't\nget to make any decisions. The expected discounted sum of\nrewards you get from many states",
    "start": "3114750",
    "end": "3119880"
  },
  {
    "text": "is 0. You don't get to\nmake any decisions. You never got any reward. The first round of this,\nit's like you're saying, OK,",
    "start": "3119880",
    "end": "3127210"
  },
  {
    "text": "I get 0 reward if I\ndon't make any decisions. ",
    "start": "3127210",
    "end": "3132750"
  },
  {
    "text": "k would be 1 here for the\nnext round, so we'd say, if I get to make\none decision, then",
    "start": "3132750",
    "end": "3138150"
  },
  {
    "text": "I would take a max over a,\nmy reward, times discount factor times 0.",
    "start": "3138150",
    "end": "3143590"
  },
  {
    "text": "So it's now saying,\nwhat is the best thing I can get to do if I\nget to make one decision?",
    "start": "3143590",
    "end": "3150050"
  },
  {
    "text": "So what this will be\nis on the first round, this will just be equal to-- so\nif V is equal to 0 for all s,",
    "start": "3150050",
    "end": "3158480"
  },
  {
    "text": "then what we would get\nwhen we do this backup-- we'd get Vk plus 1\nis just equal to--",
    "start": "3158480",
    "end": "3165050"
  },
  {
    "text": "let me put it over s-- is equal to max over a r\nof s, a because this part",
    "start": "3165050",
    "end": "3173240"
  },
  {
    "text": "will be 0 if your value is 0. So now this is like saying, OK,\nbefore I get no reward because I",
    "start": "3173240",
    "end": "3179330"
  },
  {
    "text": "make no decisions. Now what's the best\nthing I should do if I get to make one decision? The next round,\nI'll say, what if I",
    "start": "3179330",
    "end": "3185300"
  },
  {
    "text": "get to make one decision now\nand then plus this will get plugged in as your value?",
    "start": "3185300",
    "end": "3191040"
  },
  {
    "text": "Yeah?  So the expression that we're\nplugging into the max function,",
    "start": "3191040",
    "end": "3197900"
  },
  {
    "text": "is that the same as Q of s, a? How does that [INAUDIBLE]? Good question. So in general, that's going\nto be max over a Q of s, a,",
    "start": "3197900",
    "end": "3207029"
  },
  {
    "text": "yeah, because here we're\nrequiring ourselves to take the max over the\nactions we're taking.",
    "start": "3207030",
    "end": "3213080"
  },
  {
    "text": "Yeah, great question. Yeah? In policy iteration,\nwe were also initializing the values of V.",
    "start": "3213080",
    "end": "3220550"
  },
  {
    "text": "In the policy iteration, we\nwere just randomly initializing our policy. So we're saying, in this state,\nyou go left, and in this state,",
    "start": "3220550",
    "end": "3226820"
  },
  {
    "text": "you go right, et cetera. And then we were evaluating\nthe value of that. We could when we do that\nevaluation-- yes, in that part,",
    "start": "3226820",
    "end": "3233160"
  },
  {
    "text": "we were setting V equal to 0\nand then doing this iteratively. ",
    "start": "3233160",
    "end": "3238862"
  },
  {
    "text": "If we [INAUDIBLE] policy\niteration, then we would be able to detect cycles.",
    "start": "3238862",
    "end": "3246740"
  },
  {
    "text": "If we had done this\nin-- well, we can do-- we can do this as part of the\npolicy evaluation for policy iteration, but\nwhat do you mean we",
    "start": "3246740",
    "end": "3253380"
  },
  {
    "text": "would be able to detect cycles? So states having [INAUDIBLE]. So there we are comparing\nsuccessive policies,",
    "start": "3253380",
    "end": "3261910"
  },
  {
    "text": "and so for that, we are\ncomparing successive value functions. [INAUDIBLE]",
    "start": "3261910",
    "end": "3267329"
  },
  {
    "text": "That's right. So it's a good point to say,\ninside of the policy iteration one, instead of just halting\nwhen your policy has stopped",
    "start": "3267330",
    "end": "3273813"
  },
  {
    "text": "changing, you could also halt if\nyour value function has stopped changing, very few functions.",
    "start": "3273813",
    "end": "3279780"
  },
  {
    "text": "Yeah, [INAUDIBLE]. Yeah? And what's your name? Do you have any guarantees\non [INAUDIBLE], based",
    "start": "3279780",
    "end": "3287160"
  },
  {
    "text": "on value iteration and\npolicy iteration, which one converges faster [INAUDIBLE]?",
    "start": "3287160",
    "end": "3292259"
  },
  {
    "text": "Yeah, it's a great question. I'll look it up for next week. To my knowledge, there isn't\nover that in one that would not",
    "start": "3292260",
    "end": "3300809"
  },
  {
    "text": "be instance-dependent. In practice, policy search\nis very, very popular.",
    "start": "3300810",
    "end": "3308890"
  },
  {
    "text": "Clearly there's a good\nreason why we [INAUDIBLE] policy search [INAUDIBLE].",
    "start": "3308890",
    "end": "3315520"
  },
  {
    "text": "I think part of it may be-- I think part of it is\nprobably that it also often",
    "start": "3315520",
    "end": "3322360"
  },
  {
    "text": "has this nice\nmonotonic improvement, so value iteration\ndoes not necessarily have a monotonic improvement\nrequirement here.",
    "start": "3322360",
    "end": "3330220"
  },
  {
    "text": "So it is always\nthe optimal thing to do for the wrong horizon,\nwhereas the other one says,",
    "start": "3330220",
    "end": "3335740"
  },
  {
    "text": "it may not be optimal for\nages, but it will always be monotonically improving. ",
    "start": "3335740",
    "end": "3342630"
  },
  {
    "text": "Great questions. OK, let's see what\nthe properties are for value iteration because\nthese are really useful,",
    "start": "3342630",
    "end": "3348092"
  },
  {
    "text": "great questions, and we'll see\nwhy this whole thing ends up working. So just I want to\nhighlight here, you could think of\npolicy iteration",
    "start": "3348092",
    "end": "3354360"
  },
  {
    "text": "also as Bellman\noperations, and I think this gets to what\nyour question was about too. So the Bellman backup operator\nfor a particular policy pi",
    "start": "3354360",
    "end": "3361260"
  },
  {
    "text": "is defined as follows. You see you don't\nsee the max anymore. You just are committing to\ndoing a particular policy.",
    "start": "3361260",
    "end": "3367150"
  },
  {
    "text": "And then policy\nevaluations amounts to computing the fixed point,\nand I'll define that more formally in a second.",
    "start": "3367150",
    "end": "3373080"
  },
  {
    "text": "And so to do policy\nevaluation, you just repeatedly apply this operator\nuntil v stops changing.",
    "start": "3373080",
    "end": "3378820"
  },
  {
    "text": "That was the iterative\nalgorithm we saw before, just with different notation.",
    "start": "3378820",
    "end": "3384160"
  },
  {
    "text": "All right. Let's start to talk soon in\na second about fixed point. And then what we'd say\nhere is when we do policy--",
    "start": "3384160",
    "end": "3391280"
  },
  {
    "text": "another way to do\npolicy improvement is to explicitly\ndo another backup but take the arg max\ninstead of the max.",
    "start": "3391280",
    "end": "3397480"
  },
  {
    "text": "So that's the only difference. This is the same as what we're\ndoing for the Q-function, so this is Q pi k of s, a.",
    "start": "3397480",
    "end": "3408641"
  },
  {
    "text": "I'm just showing you different\nnotations for the same thing and also how people sometimes\ntalk about the Bellman backup for a particular policy.",
    "start": "3408642",
    "end": "3415320"
  },
  {
    "text": "But normally, when people\nsay, \"Bellman backups,\" they mean for the optimal. All right. So let's just go back\nto value iteration",
    "start": "3415320",
    "end": "3420660"
  },
  {
    "text": "because while I've told you how\nto compute a value function, I haven't told you how to\nget out a policy from it.",
    "start": "3420660",
    "end": "3426839"
  },
  {
    "text": "So the standard way to\ndo this would be you would go through this\nprocess, and then you would do it, say, one\nmore time and extract",
    "start": "3426840",
    "end": "3432630"
  },
  {
    "text": "the arg max instead of the max\nto actually get your policy. So normally in this\ncase, you don't",
    "start": "3432630",
    "end": "3438000"
  },
  {
    "text": "bother to compute a\npolicy along the way. You just do value\niteration a bunch of times, and then at some\npoint you extract a policy.",
    "start": "3438000",
    "end": "3443460"
  },
  {
    "text": " All right, let's see about\nsome properties of this.",
    "start": "3443460",
    "end": "3449150"
  },
  {
    "text": "So why this is a good thing-- I've already told--\nwe've already seen that policy\niteration is guaranteed to converge because there's only\na finite number of policies.",
    "start": "3449150",
    "end": "3456289"
  },
  {
    "text": "You never repeat a policy,\nand so either you're at the optimal policy already\nor you keep improving.",
    "start": "3456290",
    "end": "3461890"
  },
  {
    "text": "For value iteration,\nit may not be clear yet that this should converge. So I'm first going to define\na contraction operator.",
    "start": "3461890",
    "end": "3468560"
  },
  {
    "text": "So let's let O be an operator,\nlike the Bellman backup, so you can just think of\nit as an algebraic equation",
    "start": "3468560",
    "end": "3474170"
  },
  {
    "text": "if this is something-- if you\nhaven't seen operators before, which is totally fine,\nand then this is just going to denote any norm,\nso like the L-infinity norm",
    "start": "3474170",
    "end": "3482079"
  },
  {
    "text": "or others. If when you apply the operator\nto two different, say,",
    "start": "3482080",
    "end": "3488480"
  },
  {
    "text": "value functions-- we can just\nthink of these here as vectors-- and that distance after\nyou apply the operator",
    "start": "3488480",
    "end": "3494500"
  },
  {
    "text": "is smaller than the distance\nbefore, then it's a contraction. So just give it a\nbit of intuition",
    "start": "3494500",
    "end": "3499835"
  },
  {
    "text": "for this in case the contraction\noperator isn't something you've seen before. If you think about having\ntwo value functions and then",
    "start": "3499835",
    "end": "3506202"
  },
  {
    "text": "there's some states on\nwhich they really differ in their value, what this\nsays is that if you then apply an operator to them-- and we're\ngoing to prove that the Bellman",
    "start": "3506202",
    "end": "3513680"
  },
  {
    "text": "operator is one of them-- that afterwards they\nget closer together. So that max difference\nbetween the states",
    "start": "3513680",
    "end": "3518930"
  },
  {
    "text": "is smaller afterwards. Yeah? Yeah, is this like\na iff or just if?",
    "start": "3518930",
    "end": "3527934"
  },
  {
    "text": "Can there be\ncontraction operators that don't satisfy this-- We're going to look\nat this specifically--",
    "start": "3527935",
    "end": "3535367"
  },
  {
    "text": "you mean are there other-- so I'd have to check. I'm not an expert in all\nof contraction operators,",
    "start": "3535367",
    "end": "3540800"
  },
  {
    "text": "so I'll hesitate. What I will show is that the\nBellman operator satisfies this statement, and\ntherefore, then we can show that we are going\nto converge to a fixed point.",
    "start": "3540800",
    "end": "3547640"
  },
  {
    "text": " All right. So particularly under a couple\nminor assumptions, your discount",
    "start": "3547640",
    "end": "3556542"
  },
  {
    "text": "factor being less than 1,\nor you end up in a terminal state with probability 1,\nessentially, both of these make sure that your expected\nsum of discounted rewards",
    "start": "3556542",
    "end": "3563550"
  },
  {
    "text": "is bounded. Then the Bellman backup\nwhere you do this max",
    "start": "3563550",
    "end": "3568720"
  },
  {
    "text": "is a contraction, which\nmeans that your distance is going to shrink and shrink\nbetween two value functions.",
    "start": "3568720",
    "end": "3575410"
  },
  {
    "text": "So your Vk plus 1 versus\nits difference to Vk, that distance in terms\nof the max difference",
    "start": "3575410",
    "end": "3581880"
  },
  {
    "text": "in any of the states is\ngoing to get smaller. And we'll go through that now.",
    "start": "3581880",
    "end": "3587200"
  },
  {
    "text": "So this is proving that\nwe end up getting the-- this is a contraction,\nso the Bellman backup",
    "start": "3587200",
    "end": "3594630"
  },
  {
    "text": "is a contraction operator\non V for gamma less than 1. Let me just make this large.",
    "start": "3594630",
    "end": "3600240"
  },
  {
    "text": "So we're going to use the\ninfinity norm, which, again, is just saying where is the\nmax difference in the values for any two states, and\nwhat I'm defining here",
    "start": "3600240",
    "end": "3607950"
  },
  {
    "text": "is two different\nvalue functions. So this could be anything. And what I'm going\nto try to show is that after you do the\nBellman operator, that",
    "start": "3607950",
    "end": "3614498"
  },
  {
    "text": "can be no larger than\nthe max difference before you did the\nBellman backup.",
    "start": "3614498",
    "end": "3620400"
  },
  {
    "text": "All right. So what we have here, this\nis the first inequality, so this is important.",
    "start": "3620400",
    "end": "3625569"
  },
  {
    "text": "What I'm going to\nsay is right now, so this is just the definition\nof the Bellman backup operator. What you can see here is I have\ntwo different maxes because I'm",
    "start": "3625570",
    "end": "3635010"
  },
  {
    "text": "going to do the max over a\nfor the first value function and a max over a prime for\nthe other value function.",
    "start": "3635010",
    "end": "3641190"
  },
  {
    "text": "What I'm going to\nsay now is if you do that, instead, this has to\nbe less than or equal to if you",
    "start": "3641190",
    "end": "3647710"
  },
  {
    "text": "pulled the max a out and\nyou required both of them to use the same action.",
    "start": "3647710",
    "end": "3654119"
  },
  {
    "start": "3654120",
    "end": "3660610"
  },
  {
    "text": "And why is this true? Because essentially, what\nwe're allowing here is we're allowing us--",
    "start": "3660610",
    "end": "3665680"
  },
  {
    "text": "before, we could pick\ndifferent actions for both Bellman backups,\nand now we can pick one.",
    "start": "3665680",
    "end": "3674100"
  },
  {
    "start": "3674100",
    "end": "3682495"
  },
  {
    "text": "So that means that\ninstead of getting to maximize the second\nthing separately, we're just going to try to\nmaximize the difference.",
    "start": "3682495",
    "end": "3687980"
  },
  {
    "text": "So that's the first place\nthis less than or equal is going to come in. Once we do that,\nnow everything's taking the same action\non the same time step,",
    "start": "3687980",
    "end": "3694400"
  },
  {
    "text": "so we can get rid of these\nbecause they're identical.",
    "start": "3694400",
    "end": "3699500"
  },
  {
    "text": " So we can just say this is\njust exactly equal to max a.",
    "start": "3699500",
    "end": "3705130"
  },
  {
    "text": "I'm going to pull out the\ndiscount factor of sum over s prime probability\nof s prime given s,",
    "start": "3705130",
    "end": "3712120"
  },
  {
    "text": "a of Vk of s prime\nminus Vj of s prime.",
    "start": "3712120",
    "end": "3717800"
  },
  {
    "text": " Now, again, what I'm going to\ndo is I'm going to bound this,",
    "start": "3717800",
    "end": "3723843"
  },
  {
    "text": "and I'm going to say, the\ndifference between the two value functions that any state\nis always less than or equal to their max difference\nacross all the states.",
    "start": "3723843",
    "end": "3733050"
  },
  {
    "text": "So this is less than or\nequal to max over a gamma sum over s prime probability of\ns prime given s, a Vk minus Vj",
    "start": "3733050",
    "end": "3748396"
  },
  {
    "text": "because the difference\nbetween any particular states is always less than the\nmax difference between any of the states, so\nI upper-bounded it",
    "start": "3748396",
    "end": "3755110"
  },
  {
    "text": "using this expression. But now this term now\ndoes not depend on states,",
    "start": "3755110",
    "end": "3762400"
  },
  {
    "text": "and so I can take\nit out of the sum. This is just some constant. It's, like, 7.",
    "start": "3762400",
    "end": "3767500"
  },
  {
    "text": "So this is equal to\nmax over a gamma. ",
    "start": "3767500",
    "end": "3783109"
  },
  {
    "text": "But this is just a\ntransition model, and the probability\nthat we go to some state has to sum up to 1 if we\nsum over all next states",
    "start": "3783110",
    "end": "3791180"
  },
  {
    "text": "because for any state\nin action you're in, you always have to go\nto some next state. So this is just equal to 1.",
    "start": "3791180",
    "end": "3797640"
  },
  {
    "text": "So we get that this is\njust equal to max over a. And now there's no\nmore dependence on a,",
    "start": "3797640",
    "end": "3804539"
  },
  {
    "text": "so it just disappears.  So what that said is that\nthe distance, the max",
    "start": "3804540",
    "end": "3812040"
  },
  {
    "text": "difference between the Bellman\nbackup value functions we get",
    "start": "3812040",
    "end": "3817500"
  },
  {
    "text": "from starting with two\ndifferent value functions, has to be no larger than the\nmax difference between the value",
    "start": "3817500",
    "end": "3824250"
  },
  {
    "text": "functions before times gamma. ",
    "start": "3824250",
    "end": "3830059"
  },
  {
    "text": "And if your gamma\nis less than 1, that means you're\nstrictly contracting because it means that that max\ndifference has to be smaller",
    "start": "3830060",
    "end": "3837050"
  },
  {
    "text": "than it was before. It would be, like, 0.9 times\nwhatever the distance-- at most, 0.9 times whatever the\ndistance was before.",
    "start": "3837050",
    "end": "3843990"
  },
  {
    "text": "So that's really cool\nbecause that means now that if we apply value\niteration where we're repeatedly",
    "start": "3843990",
    "end": "3849770"
  },
  {
    "text": "doing the Bellman backup,\nwe're shrinking the distance-- so if you think of having a\nseries of value functions, so you've got, like,\nV0 and V1 and V2",
    "start": "3849770",
    "end": "3858080"
  },
  {
    "text": "and V3 and V4, dot,\ndot, dot, you can think of what this distance is.",
    "start": "3858080",
    "end": "3864110"
  },
  {
    "text": "And what this is saying is\nthat these distances are going to be shrinking over time.",
    "start": "3864110",
    "end": "3872460"
  },
  {
    "text": "And I've told you before that\nthe value function is unique, so that means as you shrink and\nshrink and shrink and shrink,",
    "start": "3872460",
    "end": "3878340"
  },
  {
    "text": "this is eventually going\nto become a unique value function because if there were--\nyou can think about it too.",
    "start": "3878340",
    "end": "3886457"
  },
  {
    "text": "If there were two\ndifferent value functions, you can think about what would\nhappen after you do a Bellman backup operator.",
    "start": "3886457",
    "end": "3893380"
  },
  {
    "text": "They are different. So this proves\nit's a contraction,",
    "start": "3893380",
    "end": "3900540"
  },
  {
    "text": "and just to note this here,\neven if all the inequalities are equalities, this is\nstill a contraction if gamma is less than 1.",
    "start": "3900540",
    "end": "3906130"
  },
  {
    "text": "It's still making progress. All right. So here's some\nthoughts in case you want to think about this more.",
    "start": "3906130",
    "end": "3911700"
  },
  {
    "text": "To prove the value iteration\nconverges to a unique solution for discrete state\naction spaces, whether initialization\nmatters at all,",
    "start": "3911700",
    "end": "3918670"
  },
  {
    "text": "and is the value of the policy\nextracted from value iteration at each round guaranteed\nto monotonically improve?",
    "start": "3918670",
    "end": "3926470"
  },
  {
    "text": "So these are all great\nthings to think about. So let's go back to\nmore practically.",
    "start": "3926470",
    "end": "3932740"
  },
  {
    "text": "This is then value iteration\nfor finite-- well, actually, I'll pause here in case anybody\nhas a question about the proof.",
    "start": "3932740",
    "end": "3938460"
  },
  {
    "text": "Yes? And remind me your name. Can you go back to the proof? Yeah.",
    "start": "3938460",
    "end": "3943930"
  },
  {
    "text": "I understand all the steps\nexcept the first one, where we take the max out of the\nnorm and max over as an action.",
    "start": "3943930",
    "end": "3950590"
  },
  {
    "text": "Why is that greater\nthan [INAUDIBLE] having a separate\nmax on the inside? Yeah, good question.",
    "start": "3950590",
    "end": "3956360"
  },
  {
    "text": "So what this is\nsaying here is we have-- you can think of\nthis as a Q-function.",
    "start": "3956360",
    "end": "3961760"
  },
  {
    "text": "So we get to pick a max\nover that Q-function and then we subtract off the\nmax over another Q-function.",
    "start": "3961760",
    "end": "3969220"
  },
  {
    "text": "And you could imagine\nthat you could think of there being lots of\ndifferent pairs of actions in this case, and either this\nmax is the same as this one,",
    "start": "3969220",
    "end": "3978529"
  },
  {
    "text": "so it's actually-- so\nlet's say in particular, concretely, that\nthis gives you a1.",
    "start": "3978530",
    "end": "3983600"
  },
  {
    "text": "That's this one. So this one is either\na1 or another a.",
    "start": "3983600",
    "end": "3989870"
  },
  {
    "text": "If it's another a, that's\njust because this is actually larger than what Q s of--",
    "start": "3989870",
    "end": "3996578"
  },
  {
    "text": "so let me just write\nthis out just in case. So we can think\nof there as either being Q s, a1 under\nthis or Qj of s, a",
    "start": "3996578",
    "end": "4009920"
  },
  {
    "text": "where a is not equal to a1. So either this max\nis exactly the same",
    "start": "4009920",
    "end": "4015440"
  },
  {
    "text": "as this one, in which case this\nis equal, or this is different, and the only time it\nwould be different is if that value\nwas actually larger",
    "start": "4015440",
    "end": "4022160"
  },
  {
    "text": "than the value of Q s, a1. And if this was\nlarger, this difference would be smaller because\nyou'd be subtracting off",
    "start": "4022160",
    "end": "4029029"
  },
  {
    "text": "a larger value.  So that's why we can turn\nthis into an inequality.",
    "start": "4029030",
    "end": "4035838"
  },
  {
    "text": "It's either the same if they\nhappen to have both picked the same action,\nor it would have picked another action, for which\nthat whole difference would",
    "start": "4035838",
    "end": "4042070"
  },
  {
    "text": "have been smaller. Good question. ",
    "start": "4042070",
    "end": "4047470"
  },
  {
    "text": "Yeah? Can you go back to the\nquestions you were posing? Yeah.",
    "start": "4047470",
    "end": "4053310"
  },
  {
    "text": "Is the value of-- the third question out there\nthat, is the value of the policy extracted from value\niteration [INAUDIBLE]?",
    "start": "4053310",
    "end": "4062790"
  },
  {
    "text": "Isn't that implicit within\nvalue iteration that with each--",
    "start": "4062790",
    "end": "4068778"
  },
  {
    "text": "each new value function is\nbetter than the previous one and therefore the policy\nwill also be better? That's a good question, yeah,\nthe question of whether it",
    "start": "4068778",
    "end": "4077040"
  },
  {
    "text": "has guaranteed that. We have not proven\nanything about that yet. We prove that for\npolicy iteration, but this is just to think\nabout it in this case.",
    "start": "4077040",
    "end": "4085870"
  },
  {
    "text": "OK, so let's go now-- anybody else have a\nquestion on the proof? ",
    "start": "4085870",
    "end": "4092720"
  },
  {
    "text": "All right. So one thing I just\nwant to mention briefly and it'll come up\non the homework is thinking about this\nfor finite horizons.",
    "start": "4092720",
    "end": "4098140"
  },
  {
    "text": "So most of today,\nalmost all of today, we've assumed that we\nget to act forever, so we have an infinite horizon.",
    "start": "4098140",
    "end": "4103264"
  },
  {
    "text": "But there will\ncertainly be cases where there's a finite horizon. And in this case, this\ngoes back to the thinking",
    "start": "4103265",
    "end": "4110430"
  },
  {
    "text": "of value iteration as just\ncomputing the optimal value for each horizon.",
    "start": "4110430",
    "end": "4115680"
  },
  {
    "text": "So if we have k equals 1 to H,\nH being the max horizon you want to compute for, then for\neach state at each round",
    "start": "4115680",
    "end": "4123299"
  },
  {
    "text": "you would have a value function,\nk plus 1, which tells you how many decisions you\nmake, what your horizon is,",
    "start": "4123300",
    "end": "4130049"
  },
  {
    "text": "and we would just\ndo this backup. So this looks exactly the\nsame as what we saw before, but now you could also\nget a policy here--",
    "start": "4130050",
    "end": "4137227"
  },
  {
    "text": "so this would be the\npolicy associated with that value function-- of what is the arg max action.",
    "start": "4137227",
    "end": "4143568"
  },
  {
    "text": "So it would compute a series\nof policies, one for each step. ",
    "start": "4143569",
    "end": "4153286"
  },
  {
    "text": "One other thing I\nwant to mention here-- and we'll talk about\nthis on the homework too.",
    "start": "4153286",
    "end": "4158403"
  },
  {
    "text": "One of the other thing\nI want to mention here is that you can\nalso just simulate the value of a\nparticular policy.",
    "start": "4158404",
    "end": "4167127"
  },
  {
    "text": "So this is also\nreally popular once we start to get into\nreally big state spaces. So if we think of the fact that\nin a lot of these algorithms",
    "start": "4167127",
    "end": "4173670"
  },
  {
    "text": "we're doing some sort\nof policy evaluation, one thing you could do is\nyou just take your policy. And you know what your\ndynamics model is,",
    "start": "4173670",
    "end": "4180229"
  },
  {
    "text": "and you know what\nyour reward is. And you just roll it out. So if you're in a\nstate, you simulate what the next state might be.",
    "start": "4180229",
    "end": "4186390"
  },
  {
    "text": "Then you get a reward. So you can just generate a\nreally large number of episodes and then just average them.",
    "start": "4186390",
    "end": "4193189"
  },
  {
    "text": "So I'm just like, oh,\nhow good is this policy? If your boss asks you, you\njust run it on 100 people. You average their rewards,\nand then you're done.",
    "start": "4193189",
    "end": "4200870"
  },
  {
    "text": "And this is something that\nbecomes really popular when it starts to be, say,\nhard to write down what that dynamics\nmodel is explicitly",
    "start": "4200870",
    "end": "4207290"
  },
  {
    "text": "or do that sum over s prime. But it's really easy to sample. And I'll note for that you can\nuse concentration inequalities,",
    "start": "4207290",
    "end": "4215700"
  },
  {
    "text": "like Hoeffding inequality and\nBernstein, for those of you familiar with them,\nto bound how much data",
    "start": "4215700",
    "end": "4222030"
  },
  {
    "text": "do you need for this estimate\nto be close to the true one. And the great thing is\nthat it's not that many,",
    "start": "4222030",
    "end": "4229270"
  },
  {
    "text": "so if you have an enormous\nstate space, like, I don't know, your Amazon or\nsomething like that, or you've got patient data,\nand it's incredibly high",
    "start": "4229270",
    "end": "4235980"
  },
  {
    "text": "dimensional, you don't have to\ndo that huge sum over S prime. You can just sample, and\nyour accuracy generally",
    "start": "4235980",
    "end": "4243000"
  },
  {
    "text": "improves by 1 over\nsquare root n, the number of samples you're doing. And the nice thing\nis that this also requires no assumption\nabout the Markov structure.",
    "start": "4243000",
    "end": "4250567"
  },
  {
    "text": "So you might have a partially\nobservable scenario, which also comes up a lot in\nthings like healthcare, and then you can just\nroll out your policy",
    "start": "4250567",
    "end": "4256590"
  },
  {
    "text": "and just see how well it works. Well, in healthcare,\nyou probably wouldn't just randomly\nroll out any policy,",
    "start": "4256590",
    "end": "4261752"
  },
  {
    "text": "but pricing might be. Yeah? Remind me your name. [INAUDIBLE]",
    "start": "4261752",
    "end": "4269705"
  },
  {
    "start": "4269705",
    "end": "4276078"
  },
  {
    "text": "That's right. So here this is just the policy\nevaluation stage, exactly, and you could either do it to\ncompute the value of a policy",
    "start": "4276078",
    "end": "4282280"
  },
  {
    "text": "or, as you just suggested,\ndo it for the Q-value. So you start off in a state for\neach of the different actions.",
    "start": "4282280",
    "end": "4288560"
  },
  {
    "text": "Then you roll out the\npolicy until the end. And this is just a really\npopular other technique, and it'll come up\nin other places.",
    "start": "4288560",
    "end": "4295130"
  },
  {
    "text": "So I wanted to start saying,\nwe can do that here too. So you can also think\nabout doing all of these",
    "start": "4295130",
    "end": "4301870"
  },
  {
    "text": "in the case of the Mars rover. And I won't go through\nit now, but you can use these as\nexamples to step",
    "start": "4301870",
    "end": "4307810"
  },
  {
    "text": "through these\ndifferent algorithms and think about how you would\ncompute these type of policies.",
    "start": "4307810",
    "end": "4314470"
  },
  {
    "text": "All right. So I will-- maybe I'll\nget to the end of this,",
    "start": "4314470",
    "end": "4320719"
  },
  {
    "text": "but I'll leave you\nwith two things. One is a thought\nquestion, which is, is the optimal\npolicy stationary?",
    "start": "4320720",
    "end": "4326616"
  },
  {
    "text": "What that means is\nindependent of time steps in the finite horizon tasks. And we'll explore this\nissue, too, on the homework.",
    "start": "4326617",
    "end": "4334360"
  },
  {
    "text": "And I also just want to\nrefresh some terminology. So in the context of\nMarkov decision processes",
    "start": "4334360",
    "end": "4341320"
  },
  {
    "text": "and reinforcement learning,\nwhen we say, \"models,\" what we normally mean\nis a mathematical model of the dynamics and reward.",
    "start": "4341320",
    "end": "4347320"
  },
  {
    "text": "Policy is a function\nmapping from states to actions that can be\ndeterministic or stochastic, and the value function is\nthis expected discounted",
    "start": "4347320",
    "end": "4354429"
  },
  {
    "text": "sum of rewards from starting in\na state and following a policy. The things that should be clear\nto you is you should be able",
    "start": "4354430",
    "end": "4361230"
  },
  {
    "text": "to understand what a\nMarkov process is-- a Markov reward\nprocess is an MDP-- the Bellman operator,\ncontraction, model, Q-value,",
    "start": "4361230",
    "end": "4368160"
  },
  {
    "text": "and policy. And you should be able to\nimplement both value iteration and policy iteration,\nand you'll have practice doing that on the homework.",
    "start": "4368160",
    "end": "4373960"
  },
  {
    "text": "You should also understand some\nof the strengths and weaknesses in terms of what we've\ndiscussed in terms of the computational\ncomplexity of some",
    "start": "4373960",
    "end": "4379530"
  },
  {
    "text": "of these different\noperations and be able to prove contraction\nproperties about these as well as understand which\nof these are really leveraging",
    "start": "4379530",
    "end": "4387360"
  },
  {
    "text": "the Markov assumption\nversus which of them don't require that. So next week we'll continue\nto talk about this,",
    "start": "4387360",
    "end": "4393929"
  },
  {
    "text": "and we'll start to talk\nabout function approximation and how we can also\nlearn when we don't know what these models are.",
    "start": "4393930",
    "end": "4399610"
  },
  {
    "text": "I'll see you then. Thanks ",
    "start": "4399610",
    "end": "4407000"
  }
]