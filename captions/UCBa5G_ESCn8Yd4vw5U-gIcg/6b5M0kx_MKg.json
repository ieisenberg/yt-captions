[
  {
    "text": "thanks for the introduction Mark and nice to be here um so I'm chatting I I started actually",
    "start": "10740",
    "end": "15960"
  },
  {
    "text": "at Stanford assistant professor computer science at Stanford I started 2020 um you know so technically it's not that",
    "start": "15960",
    "end": "21840"
  },
  {
    "text": "new but because most of the time you know we had pandemic and we had a beauty renovation so it has to be remote so I",
    "start": "21840",
    "end": "27539"
  },
  {
    "text": "do feel pretty new uh in person that's Stanford um okay so I mostly work on computer",
    "start": "27539",
    "end": "33000"
  },
  {
    "text": "vision uh AI with some applications in graphics and also robotics so I don't",
    "start": "33000",
    "end": "39000"
  },
  {
    "text": "really feel like um you know um you many people here probably know more robotics than me so you know I",
    "start": "39000",
    "end": "45540"
  },
  {
    "text": "would love to learn more from you and please um your any suggestions are very welcome",
    "start": "45540",
    "end": "51360"
  },
  {
    "text": "so I'm going to talk about multi-sensory neural objects um so multi-sem is very easy to",
    "start": "51360",
    "end": "57899"
  },
  {
    "text": "understand neural objects well so what does that mean um I think people have been aware that",
    "start": "57899",
    "end": "63180"
  },
  {
    "text": "there has been a in some states you can call it like a big change or arguably a revolution in computer vision and",
    "start": "63180",
    "end": "69720"
  },
  {
    "text": "Graphics that people have realized oh neuro networks are powerful to representing shapes or representing",
    "start": "69720",
    "end": "75420"
  },
  {
    "text": "scenes in general but if you put them historically there has been mostly prioritized in explicit representations",
    "start": "75420",
    "end": "82200"
  },
  {
    "text": "like matches Point clouds and then people are like okay what if we parametrize them using implicit",
    "start": "82200",
    "end": "87659"
  },
  {
    "text": "representations which has to be another big family of object repetitions and Graphics then",
    "start": "87659",
    "end": "93840"
  },
  {
    "text": "it turned out that for a long time Noah has really done that with neural networks so people are like what if we",
    "start": "93840",
    "end": "99360"
  },
  {
    "text": "do neural networks for implicit repetitions and that lead to a lot of advancements in Vision Graphics",
    "start": "99360",
    "end": "105060"
  },
  {
    "text": "um notable originally in Geometry reputation like deep SDF and later for",
    "start": "105060",
    "end": "110460"
  },
  {
    "text": "appearance representations which you know I think the most famous work is Nerf right so it allows you to synthesize objects in different views",
    "start": "110460",
    "end": "119040"
  },
  {
    "text": "um and then people have thought okay now that works so well in Vision Graphics can it be applied in robotics now we try to do a little bit of work that as well",
    "start": "119040",
    "end": "125880"
  },
  {
    "text": "um so here today I'm going to talk about in sometimes how we have been building these newer representations but in",
    "start": "125880",
    "end": "132300"
  },
  {
    "text": "object-centric way and I try to argue that why this object-centric way seems more right than just representing the",
    "start": "132300",
    "end": "138480"
  },
  {
    "text": "scene as a Nerf and see how that can be used in robotics okay so we want to model objects of course",
    "start": "138480",
    "end": "145620"
  },
  {
    "text": "that's very important right manipulation navigation you want to recognize find objects interacting with them and when",
    "start": "145620",
    "end": "152520"
  },
  {
    "text": "you see an object like this you want to model multiple aspects of it you know you care about how it looks like of",
    "start": "152520",
    "end": "158099"
  },
  {
    "text": "course the appearance and you know by understanding how it looks like that means you should be able to understand",
    "start": "158099",
    "end": "163440"
  },
  {
    "text": "obvious geometry and textures and you will be able to imagine how it should look like you know from a different",
    "start": "163440",
    "end": "168540"
  },
  {
    "text": "Viewpoint right so even just from very limited oppositions you should be able to imagine the shape and textures of the",
    "start": "168540",
    "end": "175200"
  },
  {
    "text": "objects as far as if you really understand the materials about how it reflects light",
    "start": "175200",
    "end": "180900"
  },
  {
    "text": "you know you should be able to synthesize the objects under different lighting conditions",
    "start": "180900",
    "end": "186660"
  },
  {
    "text": "so by the way this is a the output of one of our model which is going from a single image they can do all these but",
    "start": "186660",
    "end": "192239"
  },
  {
    "text": "I'm not going to talk about it this in this talk just because it's not related to robotics but I have to talk more offline",
    "start": "192239",
    "end": "197879"
  },
  {
    "text": "also at the same time you'll care about how it sounds right so no this looks like pulse line so if I there's an",
    "start": "197879",
    "end": "203879"
  },
  {
    "text": "impact on the object or if I hit it then you expect to hear something like that and the sound itself also gives you a",
    "start": "203879",
    "end": "209400"
  },
  {
    "text": "lot of information about object materials and um if it's a dynamic scene about object motion as well",
    "start": "209400",
    "end": "216000"
  },
  {
    "text": "as well as all how it feels throughout the Tactical feedbacks and how it behaves if I interact with it if I push",
    "start": "216000",
    "end": "221700"
  },
  {
    "text": "it what's going to happen right depends on the geometry physical properties materials a lot of things so that seems very important and you",
    "start": "221700",
    "end": "228780"
  },
  {
    "text": "know for a lot of applications clearly uh in in computer vision but um I feel like you know for after",
    "start": "228780",
    "end": "235739"
  },
  {
    "text": "manipulation it seems very important to model these objects in robotics as well so if we look at existing object data",
    "start": "235739",
    "end": "242519"
  },
  {
    "text": "sets what are data is there you know there are data tests from the vision and Graphics Community like shape that which",
    "start": "242519",
    "end": "247739"
  },
  {
    "text": "was also developed by a Stanford and you know it has all those geometries of objects with some texture variations the",
    "start": "247739",
    "end": "254519"
  },
  {
    "text": "nice thing is it does have a lot of variation geometry but the texture because these are all like you know",
    "start": "254519",
    "end": "259799"
  },
  {
    "text": "provided by users the textures here are pretty simple so it doesn't really capture the distribution of textures of",
    "start": "259799",
    "end": "265500"
  },
  {
    "text": "objects in real life and it only has object visual appearance it didn't have anything like physics or",
    "start": "265500",
    "end": "270780"
  },
  {
    "text": "materials and stuff like that has a little bit but it's so little and so inaccurate that people don't really use them",
    "start": "270780",
    "end": "276419"
  },
  {
    "text": "um so it's mostly about visual appearance that means okay it's good for 3D reconstruction but it's not good for",
    "start": "276419",
    "end": "281820"
  },
  {
    "text": "you know manipulation because there's no fittings so alternatively people like you know robotics we're like oh we have real",
    "start": "281820",
    "end": "287400"
  },
  {
    "text": "objects right they try to you know if we want to really standalize the benchmarking then you know we should at",
    "start": "287400",
    "end": "293040"
  },
  {
    "text": "least unify the set of objects that we care about so there are you know benchmarks like ycb that you can purchase and they ship you a set of",
    "start": "293040",
    "end": "299220"
  },
  {
    "text": "objects that are supposed to be all the same um so that's nice it's real it's multimodal but a lot of challenges",
    "start": "299220",
    "end": "305720"
  },
  {
    "text": "including you know every object is still kind of slightly different you know your box and my box are kind of different and",
    "start": "305720",
    "end": "312240"
  },
  {
    "text": "especially I think exemplified by this pandemic that turned out to be really hard you know sometimes like you just",
    "start": "312240",
    "end": "317820"
  },
  {
    "text": "cannot really shift them there's like shipping delays and there's there's a short in inventory a lot of objects are",
    "start": "317820",
    "end": "324120"
  },
  {
    "text": "missing and they cannot be fun anymore um so now you have to replace objects so so real objects because they're real you",
    "start": "324120",
    "end": "330660"
  },
  {
    "text": "know they come with these real challenges as well um so we thought is that possible for us",
    "start": "330660",
    "end": "336120"
  },
  {
    "text": "to try to really think about what is the underlying physical models for these objects and try to build models for",
    "start": "336120",
    "end": "342240"
  },
  {
    "text": "themselves so that we can virtualize them right so that we can actually build virtualized set of real objects that",
    "start": "342240",
    "end": "348060"
  },
  {
    "text": "they're so good and realistic enough that they capture they're from real objects so they does have non-visual",
    "start": "348060",
    "end": "354060"
  },
  {
    "text": "appearance but also physics and they're realistic enough that we can only use it for vision Graphics purposes but also",
    "start": "354060",
    "end": "359940"
  },
  {
    "text": "for robotics purposes so here you know a very simplified version of the physical opt-in model is",
    "start": "359940",
    "end": "366720"
  },
  {
    "text": "you know if you see an image like this and you see there are four blocks and you know that you know we see the image",
    "start": "366720",
    "end": "372240"
  },
  {
    "text": "up here the way it is is because you know they're underlying their their underlying object physical States physical states include objects and",
    "start": "372240",
    "end": "379139"
  },
  {
    "text": "objects have their intrinsics geometry appearance the materials of objects massive frictions",
    "start": "379139",
    "end": "384960"
  },
  {
    "text": "their extrinsics the positions and velocities of objects and the scene descriptions right so seeing if there's",
    "start": "384960",
    "end": "390720"
  },
  {
    "text": "a lot of lightings and Camera parameters and all the things got put together you know and because based on these seeing",
    "start": "390720",
    "end": "396180"
  },
  {
    "text": "States you know they interact with each other eventually it produces the image like this right so in computer Graphics",
    "start": "396180",
    "end": "401819"
  },
  {
    "text": "the process is called rendering right kind of important subjecting graphics and more generally you can call it",
    "start": "401819",
    "end": "406860"
  },
  {
    "text": "simulation because such kind of effect not only exists for a single image you know it can apply for a video and there",
    "start": "406860",
    "end": "412680"
  },
  {
    "text": "could be Dynamics going on as well so",
    "start": "412680",
    "end": "417960"
  },
  {
    "text": "now the question is if we want to you know go from real objects to virtualize them then essentially what you're having",
    "start": "417960",
    "end": "424440"
  },
  {
    "text": "is images or videos of basically observations you know mostly visual can",
    "start": "424440",
    "end": "429720"
  },
  {
    "text": "be multi-sensory observations of these objects now you want to think about okay then how can we get the underlying",
    "start": "429720",
    "end": "435300"
  },
  {
    "text": "physical states of object right so that's like you know you want simulated objects but you don't want to create it",
    "start": "435300",
    "end": "440940"
  },
  {
    "text": "from scratch like okay I sit in front of a computer imagine what a chair would be like I want them to be derived from real",
    "start": "440940",
    "end": "447180"
  },
  {
    "text": "observations like a real to sim process so I want to invert the simulation and how could I do that",
    "start": "447180",
    "end": "454759"
  },
  {
    "text": "um you know so at a higher level you can think about this yeah we have object models and simulation provides you and gives you sensory operations but how can",
    "start": "455220",
    "end": "461400"
  },
  {
    "text": "you how can you invert this process um so most recently we have seen there has",
    "start": "461400",
    "end": "468599"
  },
  {
    "text": "been a lot of efforts in in making things differential right to make this simulation process differential right",
    "start": "468599",
    "end": "474360"
  },
  {
    "text": "including in as a nerve example that you try to make volume render differentials it's actually making the rendering",
    "start": "474360",
    "end": "480300"
  },
  {
    "text": "process differential that as indeed that is you know actually differentiable because that's based on how light",
    "start": "480300",
    "end": "485340"
  },
  {
    "text": "travels and that is something that you can write it down and they are linear algebra so that's differentiable and",
    "start": "485340",
    "end": "490500"
  },
  {
    "text": "there's a little bit of 3D t2d perspective projection as well and of course there are also other efforts in",
    "start": "490500",
    "end": "496139"
  },
  {
    "text": "difference for simulation in making Dynamics differential right but there is of course it's much harder because things like contact it's not that",
    "start": "496139",
    "end": "502680"
  },
  {
    "text": "differentiable so you have to make approximations to it and of course people come up with all these different simulators making different types of",
    "start": "502680",
    "end": "508620"
  },
  {
    "text": "approximations to make them differentiable okay so but why do people spend so much effort in making these",
    "start": "508620",
    "end": "514620"
  },
  {
    "text": "things differential you know one one of the reason is you know once it's differentiable then we have seen neural",
    "start": "514620",
    "end": "520560"
  },
  {
    "text": "networks that they can be updated or you can do inference with back propagation or gradient-based optimizations right so",
    "start": "520560",
    "end": "527100"
  },
  {
    "text": "you can actually use gradient-based off foundations based on these differential simulators to invert the process and you",
    "start": "527100",
    "end": "532740"
  },
  {
    "text": "can get option models from these sensor operations so that is like the The Hope",
    "start": "532740",
    "end": "537959"
  },
  {
    "text": "or the idea behind this kind of work so at a high level you can think about",
    "start": "537959",
    "end": "543420"
  },
  {
    "text": "nerve as one of this one of the example as well so for people who are not",
    "start": "543420",
    "end": "549000"
  },
  {
    "text": "familiar with Nerf it is a method that takes in XYZ which is the positions and",
    "start": "549000",
    "end": "554040"
  },
  {
    "text": "the viewing angles uh which is the Theta and Phi and try to gives you okay what would be the",
    "start": "554040",
    "end": "561600"
  },
  {
    "text": "particular color or they call it Radiance which is RGB and the density which is you can think about it as",
    "start": "561600",
    "end": "567300"
  },
  {
    "text": "approximation of geometry but not really the sigma um so you can get uh the uh the radiance color",
    "start": "567300",
    "end": "576000"
  },
  {
    "text": "and the density for any particular position and the viewing angle right so",
    "start": "576000",
    "end": "581760"
  },
  {
    "text": "this is essentially a newer Network that learned to parametrize the scene so when you can query the neural narrow can say",
    "start": "581760",
    "end": "587880"
  },
  {
    "text": "okay please tell me what would be the radiance in density at this particular position if I see it from this",
    "start": "587880",
    "end": "593760"
  },
  {
    "text": "particular Viewpoint and it can give you that information and then when you can query multiple times and you can put it",
    "start": "593760",
    "end": "598920"
  },
  {
    "text": "together with volume rendering but by making a differential so you have three on your network to",
    "start": "598920",
    "end": "605040"
  },
  {
    "text": "overfit the C's which usually means a lot of observations like 100 images but fitting a neural network by filling your",
    "start": "605040",
    "end": "612060"
  },
  {
    "text": "network to these 100 images the neural networks the density and the radiance of",
    "start": "612060",
    "end": "617100"
  },
  {
    "text": "every point and Viewpoint in the scene what it allows you to do is you can now",
    "start": "617100",
    "end": "622380"
  },
  {
    "text": "see in objects from different viewpoints so here are some examples about what it can",
    "start": "622380",
    "end": "629040"
  },
  {
    "text": "do right so here is like the output right so you have a lot of images and you can see them from different views",
    "start": "629040",
    "end": "634320"
  },
  {
    "text": "and you know these results are pretty impressive especially given the scenes are complex and um",
    "start": "634320",
    "end": "641040"
  },
  {
    "text": "also they're all like in the wild real images and again this is back in 2020 so in the past few years many of you may",
    "start": "641040",
    "end": "647040"
  },
  {
    "text": "have seen a lot of advances since then as well okay but the problem with Nerf especially if",
    "start": "647040",
    "end": "653459"
  },
  {
    "text": "we think about want to apply to robotics I think a straightforward application is not that good or generalizable mostly",
    "start": "653459",
    "end": "660060"
  },
  {
    "text": "because I would say Nerf is not really object Centric because if you look at what it's really doing and it's learning",
    "start": "660060",
    "end": "665279"
  },
  {
    "text": "to encode uh you know taking input XYZ positions and viewpoints and trying to Output the density and Radiance at that",
    "start": "665279",
    "end": "671880"
  },
  {
    "text": "particular point from that particular viewing Direction so it actually learns to encode scene",
    "start": "671880",
    "end": "677040"
  },
  {
    "text": "parameters that are not really belonging to objects like the particular lighting conditions right so a nerve",
    "start": "677040",
    "end": "682079"
  },
  {
    "text": "representation of a scene is specific and tied to the particular lighting conditions at",
    "start": "682079",
    "end": "688440"
  },
  {
    "text": "that moment right so it will now be able to tell you how the object look like if I move the light or if it's in the",
    "start": "688440",
    "end": "694140"
  },
  {
    "text": "morning and how it actually looks like in the afternoon because all these other you know scene descriptions like lightings they're baked in into these",
    "start": "694140",
    "end": "701100"
  },
  {
    "text": "object repetitions well if you really want to care about building a physical object model then you should model things that only belong to the object",
    "start": "701100",
    "end": "707459"
  },
  {
    "text": "right not modeling things that don't belong to the object like lighting um yeah so the parameters that nerve has",
    "start": "707459",
    "end": "714000"
  },
  {
    "text": "learned they're not intrinsic objective properties and they didn't allow you to do things like relighting",
    "start": "714000",
    "end": "720500"
  },
  {
    "text": "[Music] so more broadly if you look at the type",
    "start": "722560",
    "end": "728040"
  },
  {
    "text": "of nerve related approaches for object appearance modeling then you know they can broadly be divided into two categories one is these nerve providing",
    "start": "728040",
    "end": "734579"
  },
  {
    "text": "methods they have very high fidelity view synthesis results but they cannot capture um the object intrinsics including their",
    "start": "734579",
    "end": "741420"
  },
  {
    "text": "materials and reflectance so they cannot rely the objects or do you see composition there's another category of approaches",
    "start": "741420",
    "end": "748079"
  },
  {
    "text": "which can be broadly refined to as inverse rendering methods then they do encode a sum of your",
    "start": "748079",
    "end": "755060"
  },
  {
    "text": "intrinsics allows free lighting but so far the results are mostly assuming the object has very simple brdf which is",
    "start": "755060",
    "end": "762180"
  },
  {
    "text": "um think about it as a reflectance function that is you know how object reflects the light which is associated typically",
    "start": "762180",
    "end": "768180"
  },
  {
    "text": "associated with the materials of the object so they actually make very simple assumptions like objects lambertions",
    "start": "768180",
    "end": "773700"
  },
  {
    "text": "things like that so it cannot model things that are shiny or cannot model",
    "start": "773700",
    "end": "778800"
  },
  {
    "text": "things that are translucent so one thing that we did is okay can we",
    "start": "778800",
    "end": "785279"
  },
  {
    "text": "try to think about learning this neural implicit representations that are object-centric um so what we learned is something we",
    "start": "785279",
    "end": "791700"
  },
  {
    "text": "called an object Centric neural scattering function so it learns the accumulative radiance",
    "start": "791700",
    "end": "797639"
  },
  {
    "text": "transfer from an unobstructive distant light in addition to the volume density",
    "start": "797639",
    "end": "804240"
  },
  {
    "text": "so the function actually you know so what it's really learning is you're not only having this output Direction okay",
    "start": "804240",
    "end": "809639"
  },
  {
    "text": "where which point do I care about and what is the viewing angle I have at this particular point but you also think",
    "start": "809639",
    "end": "815760"
  },
  {
    "text": "about it also cares about okay what would be the incoming like direction right so the function now is higher",
    "start": "815760",
    "end": "820800"
  },
  {
    "text": "dimensional which means it's harder to learn but it does capture you know what is really going on inside the object right because now it has the lighting",
    "start": "820800",
    "end": "827339"
  },
  {
    "text": "input or the incoming light direction as the input so it's condition on it it's no longer assuming oh the light is",
    "start": "827339",
    "end": "833160"
  },
  {
    "text": "something that is you know within the scene and it should just be baking human object representations but it only tried",
    "start": "833160",
    "end": "839399"
  },
  {
    "text": "to learn and focus on object intrinsics by making your learn function condition on input line Direction and output you",
    "start": "839399",
    "end": "844920"
  },
  {
    "text": "know viewing angles as well as the particular positions on on object and sometimes objects can be you know",
    "start": "844920",
    "end": "850800"
  },
  {
    "text": "complex when I think about a soap which is translucent right then there no the fact that it's translucent means there's",
    "start": "850800",
    "end": "856740"
  },
  {
    "text": "a lot of you know um a lot of things going on inside the object that lights are reflecting inside objects in very complex ways and these",
    "start": "856740",
    "end": "863700"
  },
  {
    "text": "things are really hard to model so our goal is you know we want to learn this transfer function that is at this",
    "start": "863700",
    "end": "869100"
  },
  {
    "text": "particular Point given that particular income in light Direction and your viewing angle what would be",
    "start": "869100",
    "end": "874260"
  },
  {
    "text": "the amount of light or the percentage percentage of light that got transferred or I would say to that particular are",
    "start": "874260",
    "end": "881940"
  },
  {
    "text": "going Direction so hopefully the function will capture you know without modeling what's really going on inside",
    "start": "881940",
    "end": "886980"
  },
  {
    "text": "it will learn because these are really hard to model so we'll say we're seeing hopefully it will learn to capture you",
    "start": "886980",
    "end": "892800"
  },
  {
    "text": "know complex lighting effects going on inside objects but that's specific to the object that's the object intrinsic",
    "start": "892800",
    "end": "898019"
  },
  {
    "text": "property and we want the newer numbers to or the neural network functions to overfit you that intrinsic properties instead of the general lighting",
    "start": "898019",
    "end": "904740"
  },
  {
    "text": "conditions or scene descriptions so specifically OSF looks like you know",
    "start": "904740",
    "end": "909839"
  },
  {
    "text": "you have special locations just like Nerf you have a distant light Direction which is where the light is coming from",
    "start": "909839",
    "end": "915540"
  },
  {
    "text": "and you have an outgoing light direction or your viewing angle so the first and second are same as nerve but now you",
    "start": "915540",
    "end": "920760"
  },
  {
    "text": "have this incoming light Direction and the output is this Radiance cumulative radius transfer function and the density",
    "start": "920760",
    "end": "927959"
  },
  {
    "text": "function so density is the same as nerve as well okay so you know look at what's going on",
    "start": "927959",
    "end": "934320"
  },
  {
    "text": "is yeah you have the light and then you have this particular positions and you know you want to compute okay how to",
    "start": "934320",
    "end": "940860"
  },
  {
    "text": "what is 10 is the light in this particular uh income incoming Direction get",
    "start": "940860",
    "end": "946440"
  },
  {
    "text": "reflected to that particular outgoing Direction so you um",
    "start": "946440",
    "end": "952019"
  },
  {
    "text": "so essentially you're having the amount of light that got multiplied by this transfer function which is essentially",
    "start": "952019",
    "end": "957240"
  },
  {
    "text": "like a coefficient and that should be the outgoing line so",
    "start": "957240",
    "end": "963000"
  },
  {
    "text": "um so because the output format in something you can see it's kind of very similar to The Standard volume rendering",
    "start": "963000",
    "end": "970019"
  },
  {
    "text": "framework as used by Nerf as well so it still allows you to use volume rendering to learn from images so that's the same",
    "start": "970019",
    "end": "975540"
  },
  {
    "text": "as nerve but because you are overfitting to this object intrinsic functions you",
    "start": "975540",
    "end": "980639"
  },
  {
    "text": "can now approximate appearance of both you know complex objects their Reflections properties and for both translucent and opaque objects",
    "start": "980639",
    "end": "989100"
  },
  {
    "text": "so here are some examples about um how we were able to rely on opaque objects and the first is Grand choose",
    "start": "989100",
    "end": "995760"
  },
  {
    "text": "the second is our reconstruction I can rotate it and you can see the Reconstruction if you compare with",
    "start": "995760",
    "end": "1001579"
  },
  {
    "text": "you know standard nerve based methods or some extensions to it like the inverse rendering methods I talked about earlier",
    "start": "1001579",
    "end": "1006980"
  },
  {
    "text": "then um you know you can see our results look much better",
    "start": "1006980",
    "end": "1012019"
  },
  {
    "text": "and here are some more results as well",
    "start": "1012019",
    "end": "1015759"
  },
  {
    "text": "you can see I can do normal view synthesis it can do real lighting so you can rotate objects you can realize",
    "start": "1022160",
    "end": "1027798"
  },
  {
    "text": "objects and for both views now we've seen this in real lighting it does much better than the bass lines and here is",
    "start": "1027799",
    "end": "1034760"
  },
  {
    "text": "an example of a translucent object which is a Stanford bunny um",
    "start": "1034760",
    "end": "1040540"
  },
  {
    "text": "right so you can read out the object but also capturing the complex interactive reflections",
    "start": "1043100",
    "end": "1048740"
  },
  {
    "text": "and again that's better than uh that's better than good baselines",
    "start": "1048740",
    "end": "1054519"
  },
  {
    "text": "and while these objects are you know synthetic um you know here are some results on relating a real objects we'll actually",
    "start": "1054559",
    "end": "1060799"
  },
  {
    "text": "purchase some soap and then we try to think about how the model performs if you have this real objects which is soap",
    "start": "1060799",
    "end": "1068539"
  },
  {
    "text": "and you can synthesizing it from different views as well as relied objects",
    "start": "1068539",
    "end": "1074380"
  },
  {
    "text": "yeah the real lighting is kind of a little bit subtle but how do you see but because the soap itself is translucent",
    "start": "1078260",
    "end": "1084020"
  },
  {
    "text": "but you know mostly opaque and by learning things that are object-centric what you can do is you",
    "start": "1084020",
    "end": "1090080"
  },
  {
    "text": "can think about if I have one OSF for that particular object and the other always have for a different object and",
    "start": "1090080",
    "end": "1095299"
  },
  {
    "text": "they're all modeling how objects reflect light so you can put them together with standard Ray tracing and you will be",
    "start": "1095299",
    "end": "1100460"
  },
  {
    "text": "able to um you know seeing objects uh seeing scenes composed of multiple objects",
    "start": "1100460",
    "end": "1105679"
  },
  {
    "text": "learned by our osfs and realign them well if you just stand up put multiple nerves there at first it didn't support",
    "start": "1105679",
    "end": "1111500"
  },
  {
    "text": "the lighting right so you cannot really see the Shadows moving as the light move around and second the card is not as",
    "start": "1111500",
    "end": "1116840"
  },
  {
    "text": "good okay um so this is kind of a high level about",
    "start": "1116840",
    "end": "1124039"
  },
  {
    "text": "you know how it looks like our efforts in building these newer representations for objects and try to think about you",
    "start": "1124039",
    "end": "1129559"
  },
  {
    "text": "know how it looks like so you can do normal business in real lighting and but what about sound you know can we do",
    "start": "1129559",
    "end": "1135679"
  },
  {
    "text": "similar things for sale so we said we want to we want to think about and leverage what is there for",
    "start": "1135679",
    "end": "1141500"
  },
  {
    "text": "what is the physical object models for these objects and for objects visual appearance it's their",
    "start": "1141500",
    "end": "1147440"
  },
  {
    "text": "geometry and it's how they reflect the light the brdfs you know so we try to model that right so that we can",
    "start": "1147440",
    "end": "1153380"
  },
  {
    "text": "disentangle the lighting conditions but what about object sounds what are the",
    "start": "1153380",
    "end": "1158419"
  },
  {
    "text": "physical model for the sound that the object makes so if you're thinking about entrances for sound that object makes the objects",
    "start": "1158419",
    "end": "1164780"
  },
  {
    "text": "make sound because they vibrate right so if you think about you know why that sound this is like a metal",
    "start": "1164780",
    "end": "1171559"
  },
  {
    "text": "I don't know object and for the sound that makes it's making that sound because it's vibrating these different",
    "start": "1171559",
    "end": "1177140"
  },
  {
    "text": "modes you can sort of you can actually visualize them so the sound of every",
    "start": "1177140",
    "end": "1182179"
  },
  {
    "text": "object can actually you can you can write them down as a modal model where the sound of object is you know composed",
    "start": "1182179",
    "end": "1188900"
  },
  {
    "text": "of a number of different frequency modes with different gains and damping and",
    "start": "1188900",
    "end": "1194900"
  },
  {
    "text": "they got put together and of course this is kind of approximate but essentially that's how an object is making those",
    "start": "1194900",
    "end": "1200179"
  },
  {
    "text": "sound so similarly this is like a simulation process for the sound that object makes",
    "start": "1200179",
    "end": "1207500"
  },
  {
    "text": "just like rendering is you can think about it as a simulation process to give you the appearance of object then now",
    "start": "1207500",
    "end": "1213860"
  },
  {
    "text": "the question is yeah is it possible for us to also make that differentiable well turn out this is differential two and",
    "start": "1213860",
    "end": "1219919"
  },
  {
    "text": "also this is during work with Dr James and Jeanette boric at Stanford at last",
    "start": "1219919",
    "end": "1225080"
  },
  {
    "text": "year's Coral so what we did is you know once we",
    "start": "1225080",
    "end": "1230539"
  },
  {
    "text": "assume we have the frequency modes of objects and how they vibrate and for each vibration model you have the",
    "start": "1230539",
    "end": "1235640"
  },
  {
    "text": "corresponding damping coefficient as well as the gains then you can write it down and make the whole process",
    "start": "1235640",
    "end": "1241160"
  },
  {
    "text": "differentiable so what's going on here is if you have a ceramic mark oh",
    "start": "1241160",
    "end": "1248080"
  },
  {
    "text": "you can have an impact hammer you can collect the sound that it makes and because now you know right why the",
    "start": "1248600",
    "end": "1254120"
  },
  {
    "text": "object is making the sound the way it is and you can write down this forward model or simulation model and you can make a differential",
    "start": "1254120",
    "end": "1261200"
  },
  {
    "text": "then what you can do is now you can actually optimize or search for the impact forces and as well as the impulse",
    "start": "1261200",
    "end": "1267980"
  },
  {
    "text": "responses then you know in a forward model you know okay if I put an Impulse if I put an impact Force here and then I",
    "start": "1267980",
    "end": "1275000"
  },
  {
    "text": "will get a particular impulse response so I can convert the two and then they'll be good okay this is my synthesized waveform assuming you know",
    "start": "1275000",
    "end": "1281960"
  },
  {
    "text": "that will be the top is my uh is my impact forces the pro I would say the",
    "start": "1281960",
    "end": "1287059"
  },
  {
    "text": "interaction profile and you can render that into in all in a",
    "start": "1287059",
    "end": "1292460"
  },
  {
    "text": "differential way to the spectrogram under this particular condition if you compare that with what you actually",
    "start": "1292460",
    "end": "1298100"
  },
  {
    "text": "observe right think about it you know in nerve you're like you have a number of images and you want to invert this kind",
    "start": "1298100",
    "end": "1303500"
  },
  {
    "text": "of this differential rendering process you get geometry and Radiance right OSF you have a number of images and you want",
    "start": "1303500",
    "end": "1309320"
  },
  {
    "text": "to invert that now you condition lighting so you want to invert that and get the reflectance functions or the",
    "start": "1309320",
    "end": "1315140"
  },
  {
    "text": "radius transfer functions and here you're you're not seeing the objects but you're hearing the objects what you've",
    "start": "1315140",
    "end": "1320419"
  },
  {
    "text": "got you'll observe it is you know ground shoes or the actual observation of the spectrogram of the object and you want",
    "start": "1320419",
    "end": "1325820"
  },
  {
    "text": "to invert this process by searching for the forces and the way you intact interact with the objects by convolving",
    "start": "1325820",
    "end": "1332960"
  },
  {
    "text": "the interaction profile with the import responses then you get this synthesized or simulated waveform and a spectrogram",
    "start": "1332960",
    "end": "1339860"
  },
  {
    "text": "and the goal is to make the make the two match and because the whole differential the whole simulation process is not",
    "start": "1339860",
    "end": "1346820"
  },
  {
    "text": "differentiable as I said before you can now do a gradient-based update right so you can actually just do gradient-based",
    "start": "1346820",
    "end": "1352460"
  },
  {
    "text": "optimization to search for the impact forces which is shown on the top left that once involved with the impulse",
    "start": "1352460",
    "end": "1359539"
  },
  {
    "text": "responses can give you almost the same you know hopefully the spectrum that is as close as possible to your actual",
    "start": "1359539",
    "end": "1366080"
  },
  {
    "text": "observation so here now you can just by doing gradient space update by hearing the sound you can actually infer okay",
    "start": "1366080",
    "end": "1371840"
  },
  {
    "text": "what are the moment I hit the object and how how you know what is the what is the",
    "start": "1371840",
    "end": "1376940"
  },
  {
    "text": "magnitude of the of the impact as well",
    "start": "1376940",
    "end": "1380620"
  },
  {
    "text": "so this is about vision and sound and so but they are all like for",
    "start": "1382100",
    "end": "1387200"
  },
  {
    "text": "specific you know single object now we have the simulation process we showed how you can use it for inverse rendering of both visual data and and auditory",
    "start": "1387200",
    "end": "1394580"
  },
  {
    "text": "data um so this is only for again a single object or we showed how this can be done",
    "start": "1394580",
    "end": "1399620"
  },
  {
    "text": "for a single object then now the question is okay we have shiftnet which is synthetic but you're not as good but",
    "start": "1399620",
    "end": "1405799"
  },
  {
    "text": "it's large we have ycb which is smaller which is real which is great but you know it has all those real object",
    "start": "1405799",
    "end": "1410960"
  },
  {
    "text": "challenges come with the real objects so if we want to build a virtualized object set data sets by leveraging all these",
    "start": "1410960",
    "end": "1418400"
  },
  {
    "text": "differential simulation techniques that we've just talked about by doing inverse or gradient-based update on the real",
    "start": "1418400",
    "end": "1424159"
  },
  {
    "text": "observations of real objects then we should try to have this virtualized object a virtualized data set of neural",
    "start": "1424159",
    "end": "1430520"
  },
  {
    "text": "objects so that's the idea behind the recent effort we had also with generic",
    "start": "1430520",
    "end": "1436520"
  },
  {
    "text": "and Faithfully at Stanford and Wayne who is just moved from CMU to UIUC so what",
    "start": "1436520",
    "end": "1443780"
  },
  {
    "text": "we try to do is we'll try to do the data set of multi-sensory neural objects",
    "start": "1443780",
    "end": "1449720"
  },
  {
    "text": "so the main idea is the same you know for every real object it has some intrinsics as textures materials and and shape so",
    "start": "1449720",
    "end": "1458000"
  },
  {
    "text": "we want to use newer networks or new implicit repetitions to overfit and to model these properties",
    "start": "1458000",
    "end": "1464659"
  },
  {
    "text": "and you know you can query the neural network with action 6. so what would be the x36 for visual data and that would",
    "start": "1464659",
    "end": "1471320"
  },
  {
    "text": "be the positions you care about the viewing angle the lighting conditions will be the x36 for auditory data that",
    "start": "1471320",
    "end": "1476419"
  },
  {
    "text": "will be the position of the impact and the magnitude of the impact and you know by querying these things uh",
    "start": "1476419",
    "end": "1482360"
  },
  {
    "text": "by carrying these uh yeah the neural network which learns the intrinsics of objects and conditional or the input as",
    "start": "1482360",
    "end": "1489500"
  },
  {
    "text": "the extreme signal objects then hopefully they learn our neural network will give you the visual appearance of",
    "start": "1489500",
    "end": "1494900"
  },
  {
    "text": "the objects the auditory profiled objects and the tactile feedbacks of the objects so the idea behind this object",
    "start": "1494900",
    "end": "1500659"
  },
  {
    "text": "photo data set is a uniform you know because it's all sharing the same you know representation which is implicit",
    "start": "1500659",
    "end": "1505880"
  },
  {
    "text": "and your you are now parametrized it's a uniform object-centric inclusive references for each object and hopefully",
    "start": "1505880",
    "end": "1511940"
  },
  {
    "text": "that will be easy accessible to the community as a standard Benchmark for multi-sensory Learning and you know it",
    "start": "1511940",
    "end": "1517760"
  },
  {
    "text": "will be a platform agnostic so compatible to different robotic virtual environments",
    "start": "1517760",
    "end": "1523460"
  },
  {
    "text": "it's looking a little bit deeper into what object okay so we call every object every neural network is an object file",
    "start": "1523460",
    "end": "1530840"
  },
  {
    "text": "because that's some name from the early AI research um so essentially every object is",
    "start": "1530840",
    "end": "1536539"
  },
  {
    "text": "parameters by new one year Network or you can think about it as three neural networks for three different modalities so the data set has 1000 objects so",
    "start": "1536539",
    "end": "1543080"
  },
  {
    "text": "essentially it's a data set of three thousand neural networks um so what's for every object what we have is you know we have this Vision at",
    "start": "1543080",
    "end": "1550159"
  },
  {
    "text": "which is basically OSF that I just talked about and the conditional 3D coordinates in lighting conditions and",
    "start": "1550159",
    "end": "1555860"
  },
  {
    "text": "try to learn to parameterize what would be the density and color of the object at particular positions and viewpoints and you can combine this OSF",
    "start": "1555860",
    "end": "1562760"
  },
  {
    "text": "representation with model rendering to render images for you for sound we also talked about it right",
    "start": "1562760",
    "end": "1568940"
  },
  {
    "text": "it's based on different impact and using uh modal synthesis but now the extrinsics are you know in addition to",
    "start": "1568940",
    "end": "1575299"
  },
  {
    "text": "the 3D coordinates where the impact is happening and they also conditional the frequencies and damping coefficient",
    "start": "1575299",
    "end": "1582260"
  },
  {
    "text": "for touch which I didn't have to talk about you know we actually built on top of wind changes",
    "start": "1582260",
    "end": "1587559"
  },
  {
    "text": "tactile Simulator for geocytes so on the text similar is called taxium but but",
    "start": "1587559",
    "end": "1594500"
  },
  {
    "text": "what would be the extrinsics for for tactile signal and it will still be the positions where I'm touching objects as well as the content orientations right",
    "start": "1594500",
    "end": "1600860"
  },
  {
    "text": "to which direction I'm trying to fill the object and geopenetration and and the Dual penetrations which is",
    "start": "1600860",
    "end": "1607940"
  },
  {
    "text": "um to think about it as the magnitude of how hard I'm pushing this object",
    "start": "1607940",
    "end": "1614120"
  },
  {
    "text": "so yeah so that's object folder 2.0 because earlier we had a version 1.0 which is smaller I know it's good so 2.0",
    "start": "1614120",
    "end": "1620960"
  },
  {
    "text": "has one ton of objects and so these are neural networks and for all these objects you know they're prioritizing",
    "start": "1620960",
    "end": "1626000"
  },
  {
    "text": "your network you can re-render them from different viewpoints re-light them and you can use cloud in your network to get",
    "start": "1626000",
    "end": "1631580"
  },
  {
    "text": "you okay what happened how would the objects sound like how would it feel like",
    "start": "1631580",
    "end": "1636880"
  },
  {
    "text": "um One Challenge with this neural implicit representation is especially for the vanilla nerve because it",
    "start": "1641360",
    "end": "1647840"
  },
  {
    "text": "actually volume rendering has to query a lot of times and do some Integrations kind of slow um so if you you know in robotic",
    "start": "1647840",
    "end": "1653360"
  },
  {
    "text": "applications if you're doing like you know even just like especially in reinforcement learning you probably",
    "start": "1653360",
    "end": "1658520"
  },
  {
    "text": "don't want to wait for a minute to tell for the system to tell you okay what would be an object you know how it would object looks like either a particular",
    "start": "1658520",
    "end": "1664640"
  },
  {
    "text": "particular position so one thing we did was we also incorporate the recent advances uh called basically you just",
    "start": "1664640",
    "end": "1671659"
  },
  {
    "text": "probably parametrize the neural network with a thousand but smaller much smaller neural network so that the rendering",
    "start": "1671659",
    "end": "1678799"
  },
  {
    "text": "process become much faster so every object in our data set can be rendered",
    "start": "1678799",
    "end": "1684380"
  },
  {
    "text": "from any particular Viewpoint under arbitrary Lighting in real time right so you don't have to wait for a minute to get appearance to object",
    "start": "1684380",
    "end": "1692020"
  },
  {
    "text": "so here's an example of you know one object in our data set you have a ycb mug and you know so you have some",
    "start": "1693200",
    "end": "1699860"
  },
  {
    "text": "representation for the materials and the scales and hear the sound that the new",
    "start": "1699860",
    "end": "1705260"
  },
  {
    "text": "level has learned to encode and this output of what is going on there foreign",
    "start": "1705260",
    "end": "1710559"
  },
  {
    "text": "make a slightly different sound if the impacts at different positions",
    "start": "1710559",
    "end": "1715640"
  },
  {
    "text": "and just to make sure that we're not too far off to go and choose so we actually go to a recording studio a Karma and we",
    "start": "1715640",
    "end": "1722059"
  },
  {
    "text": "hand object there and then we hit it with the impact sound oh sorry with the impact hammer to record the actual sound the object makes so you can compare that",
    "start": "1722059",
    "end": "1728720"
  },
  {
    "text": "with the real impact sound recording and then we have some quantitative benchmarking and Analysis in the paper",
    "start": "1728720",
    "end": "1736460"
  },
  {
    "text": "so it is reasonably close of course it's still not identical and we're trying to improve it and we're thinking about ways to actually do it to to close the gap",
    "start": "1736460",
    "end": "1743539"
  },
  {
    "text": "but I think in some sense it's really really close and here's a different example of a porcelain picture it's a ceramic has a",
    "start": "1743539",
    "end": "1751220"
  },
  {
    "text": "different scale and the sound that it makes",
    "start": "1751220",
    "end": "1755080"
  },
  {
    "text": "as well as the impact sound from the real recording",
    "start": "1760039",
    "end": "1764919"
  },
  {
    "text": "all right so it's not terrible and for tactile here are some examples",
    "start": "1765260",
    "end": "1770779"
  },
  {
    "text": "of you know the technical responses if you have a because again we use a taxim which is similar for geosite right so of",
    "start": "1770779",
    "end": "1777140"
  },
  {
    "text": "course these images are into outside format that essentially you translate the responses into some images but",
    "start": "1777140",
    "end": "1783740"
  },
  {
    "text": "colored with lights from different directions to essentially capture the surface novels so you feel it you can",
    "start": "1783740",
    "end": "1789679"
  },
  {
    "text": "see that you know by touching object even at the same position just but just by having slightly different",
    "start": "1789679",
    "end": "1794899"
  },
  {
    "text": "orientations or rotations and different Geon penetrating depth penetration depth you can see the differences in these",
    "start": "1794899",
    "end": "1801679"
  },
  {
    "text": "tactile responses and again these are all prior to the neural network so you can just query the neural networks and input your neural network okay I want P",
    "start": "1801679",
    "end": "1808700"
  },
  {
    "text": "equals two millimeters and the Phi is zero degrees and it will just output its",
    "start": "1808700",
    "end": "1814279"
  },
  {
    "text": "image free and here's a different example of you know uh basically you're touching the",
    "start": "1814279",
    "end": "1821240"
  },
  {
    "text": "picture at a different position so once you have that right what will be",
    "start": "1821240",
    "end": "1826460"
  },
  {
    "text": "the applications what are the things you can do with it you know so we demonstrated a few uh kind of simple",
    "start": "1826460",
    "end": "1832159"
  },
  {
    "text": "applications um although we're still working on it and try to help explore how it can really be used and benefit robotics but",
    "start": "1832159",
    "end": "1838760"
  },
  {
    "text": "some simple applications are you know you can based on tactile and audio data you know without seeing objects you can",
    "start": "1838760",
    "end": "1844880"
  },
  {
    "text": "try to feel it and then hit it and based on the sound you can try to localize okay which part am I touching object so",
    "start": "1844880",
    "end": "1850640"
  },
  {
    "text": "think about if you're in the dark room and you try to you know figure out okay where the object is and to try to grasp them so it's like tactile audio content",
    "start": "1850640",
    "end": "1858020"
  },
  {
    "text": "localization I'm not going to play a sound because they're kind of similar but essentially through a few iterations you know you feel the objects you hear",
    "start": "1858020",
    "end": "1864080"
  },
  {
    "text": "the sound the object makes and then you do a few times then based on a particle filtering framework you will be able to",
    "start": "1864080",
    "end": "1870020"
  },
  {
    "text": "eventually locate okay what are the positions I'm touching objects so I can you know in something okay myself the",
    "start": "1870020",
    "end": "1876559"
  },
  {
    "text": "finger relatively to the position of the objects and shape reconstruction multi-sensory",
    "start": "1876559",
    "end": "1885260"
  },
  {
    "text": "so going from not only a single image but you know if you feel the object multiple times using geosa sensor you",
    "start": "1885260",
    "end": "1890840"
  },
  {
    "text": "collect all these you know tactile images and that gives you kind of some Spar signals but pretty accurate high I",
    "start": "1890840",
    "end": "1896480"
  },
  {
    "text": "would say high Precision but very local right so signals about object geometry and that can be integrated with visual",
    "start": "1896480",
    "end": "1902960"
  },
  {
    "text": "image you know there are a lot of work on going from visual image to 3D shapes so you can put them together and give you a shape reconstruction but you know",
    "start": "1902960",
    "end": "1910039"
  },
  {
    "text": "these are like the typical applications that you know these data sets can enable so we're not trying to you know we're",
    "start": "1910039",
    "end": "1916460"
  },
  {
    "text": "going to show that our data sets is supposed to be supporting all these applications although we're not trying to make Innovations here on how we can",
    "start": "1916460",
    "end": "1923299"
  },
  {
    "text": "do better on these tasks and of course you can also you know say if you're trying to grasp the object can",
    "start": "1923299",
    "end": "1928880"
  },
  {
    "text": "you based on the tactile images to aim for whether such a graph is successful or not to guide your policy learning and",
    "start": "1928880",
    "end": "1933919"
  },
  {
    "text": "stuff like that so these are like the tasks that we care about and we say okay the data seconds",
    "start": "1933919",
    "end": "1939980"
  },
  {
    "text": "for all these and most recently just that this year's Coral uh we also made uh try to explore a little more you know",
    "start": "1939980",
    "end": "1946640"
  },
  {
    "text": "once you have the objects and you have all these sensory observations okay that's the data set part then how you're",
    "start": "1946640",
    "end": "1952100"
  },
  {
    "text": "going to better use it right can you come up with the model to better use and integrate all these informations due to",
    "start": "1952100",
    "end": "1957679"
  },
  {
    "text": "time constraint I'm not going to talk about it in detail but at a high level you know we try to integrate all these",
    "start": "1957679",
    "end": "1962980"
  },
  {
    "text": "multi-sensory information visual audio and touch using a multi-sensory Transformer based on attention so that",
    "start": "1962980",
    "end": "1970100"
  },
  {
    "text": "and we use it to solve kind of pretty complex tasks that is it's a bit contrived just to be completely honest",
    "start": "1970100",
    "end": "1975799"
  },
  {
    "text": "because we have to come up with tasks that really require three sensory modalities just to really show the",
    "start": "1975799",
    "end": "1981380"
  },
  {
    "text": "benefit of this framework and it turned out right in most cases in our life you know we may rely on one or two so it's",
    "start": "1981380",
    "end": "1986480"
  },
  {
    "text": "not that common that we'll use all three but we try to come up with task setups that could be a big contrived but really",
    "start": "1986480",
    "end": "1992840"
  },
  {
    "text": "demonstrate that where there are cases you may want all three and Advanced packing and pouring I'm going to show",
    "start": "1992840",
    "end": "1998059"
  },
  {
    "text": "this one demo on pluring so because Pro is something like when I try to make coffees myself I Rely Along on actually",
    "start": "1998059",
    "end": "2005980"
  },
  {
    "text": "the sound because sometimes I'm just like watching my phone or something so I just place on the sound I can say okay",
    "start": "2005980",
    "end": "2011140"
  },
  {
    "text": "that's about all right so this is like real what's going in the real world",
    "start": "2011140",
    "end": "2017279"
  },
  {
    "text": "change and the frequencies and based on that usually other guys the Mark is full",
    "start": "2018000",
    "end": "2023740"
  },
  {
    "text": "I have to stop so now if we want robot to do similar things for you and especially in the",
    "start": "2023740",
    "end": "2029080"
  },
  {
    "text": "cases where the the container is not it's opaque so you cannot see the level of water there then you may want to rely",
    "start": "2029080",
    "end": "2035559"
  },
  {
    "text": "on this information as well you know we don't want robot to play with water so so it's kind of small beads",
    "start": "2035559",
    "end": "2041380"
  },
  {
    "text": "but but actually this is a bit hard to hear",
    "start": "2041380",
    "end": "2048058"
  },
  {
    "text": "some people hear it probably not be like this somehow this video has a",
    "start": "2050440",
    "end": "2056338"
  },
  {
    "text": "H if I can do this",
    "start": "2058060",
    "end": "2062159"
  },
  {
    "text": "yeah um yeah first time showing this video so",
    "start": "2063520",
    "end": "2070200"
  },
  {
    "text": "we're gonna fix it for the future because it's first time I'm showing this video this is very recent car 2022 but",
    "start": "2074859",
    "end": "2081520"
  },
  {
    "text": "uh you know actually if you can hear it sound better then you can actually tell even those are not water they're liquid",
    "start": "2081520",
    "end": "2088960"
  },
  {
    "text": "they're just bees or but you can actually hear the difference in the pitch and the goal here the robot is",
    "start": "2088960",
    "end": "2094118"
  },
  {
    "text": "trying to complete a task that is trying to pour exactly 40 grams of the bees into that container",
    "start": "2094119",
    "end": "2099580"
  },
  {
    "text": "um so it turns out that you know by adding the sound information especially in the cases where durable cannot see what's going on in the container they",
    "start": "2099580",
    "end": "2105640"
  },
  {
    "text": "actually really helped and the robot was able to do it much more precisely",
    "start": "2105640",
    "end": "2110400"
  },
  {
    "text": "okay um so finally you know we talk about a small test sensory object we say okay we",
    "start": "2111760",
    "end": "2117520"
  },
  {
    "text": "try to model them right by making the simulation process differential and using inverse process to capture their",
    "start": "2117520",
    "end": "2123099"
  },
  {
    "text": "appearance and the sound they make and how we can scale that up into a data set and you know some of the applications",
    "start": "2123099",
    "end": "2128980"
  },
  {
    "text": "the days that we enable and the method that we can use to leverage all these sensory modalities",
    "start": "2128980",
    "end": "2135400"
  },
  {
    "text": "um so one thing I was missing what is that",
    "start": "2135400",
    "end": "2142240"
  },
  {
    "text": "is here in many cases we're assume you know we have you know all these this amazing video",
    "start": "2142240",
    "end": "2149260"
  },
  {
    "text": "which is like miracle that we can capture whatever data we want right in the case of OSF we're like okay we can put an object there and then we can take",
    "start": "2149260",
    "end": "2155380"
  },
  {
    "text": "a lot of pictures of it and condition different lighting conditions in the case of the sound you know we can say I have a per there's a silent room and",
    "start": "2155380",
    "end": "2161440"
  },
  {
    "text": "then I can hit it with an impact hammer and control everything and there's no noise and everything but",
    "start": "2161440",
    "end": "2167140"
  },
  {
    "text": "if you really want the system the inverse rendering process invert if you want to invert this differential",
    "start": "2167140",
    "end": "2172359"
  },
  {
    "text": "simulation process but you want to make make it to work in the real world and to get as many data as possible as release",
    "start": "2172359",
    "end": "2178359"
  },
  {
    "text": "the data as diverse data as possible then you have to deal with noisy data because you cannot assume you know a lot",
    "start": "2178359",
    "end": "2184180"
  },
  {
    "text": "of things we did we have a dark room everything is control we control light it's anechoic we control the sound but",
    "start": "2184180",
    "end": "2189700"
  },
  {
    "text": "we cannot assume that we have this kind of you know dark rooms everywhere so if",
    "start": "2189700",
    "end": "2194740"
  },
  {
    "text": "we really want to virtualize and build a data set of real objects then in many cases we have to deal with Messy data",
    "start": "2194740",
    "end": "2199900"
  },
  {
    "text": "like this this is not even that mess it's pretty clean actually it was still pretty reasonably messy in the sense that you want to go from noisy massive",
    "start": "2199900",
    "end": "2207220"
  },
  {
    "text": "observations of your multiple objects in terms of visual data there could be occlusions in terms of auditory data",
    "start": "2207220",
    "end": "2212619"
  },
  {
    "text": "there could be noises and you know objects including with each other objects reflect light on top of each",
    "start": "2212619",
    "end": "2218800"
  },
  {
    "text": "other so we want to deal with all these things without assuming you know have control",
    "start": "2218800",
    "end": "2225880"
  },
  {
    "text": "over everything but still be able to infer or derive these neural object representations from these kind of more",
    "start": "2225880",
    "end": "2231880"
  },
  {
    "text": "complex observations um so we tried a little bit in exploring this is pretty much ongoing work and",
    "start": "2231880",
    "end": "2237760"
  },
  {
    "text": "very preliminary but we tried a little bit in exploring how we can go from this noisy visual observations this this part",
    "start": "2237760",
    "end": "2244240"
  },
  {
    "text": "will only focus on visuals so far helping go from visual observations to get the underlying you know newer object",
    "start": "2244240",
    "end": "2249579"
  },
  {
    "text": "models so we feel like you know in order to work with real scenes we want these",
    "start": "2249579",
    "end": "2254800"
  },
  {
    "text": "newer object models in addition or any methods that infer on your object representations you know the method that",
    "start": "2254800",
    "end": "2260320"
  },
  {
    "text": "the inference method should have three properties the first one is you know of course we wanted to learn without supervision right because there are a",
    "start": "2260320",
    "end": "2267040"
  },
  {
    "text": "lot of cases where you can say okay if I know this is a chair um and then I can annotate a lot of",
    "start": "2267040",
    "end": "2272260"
  },
  {
    "text": "chairs I can do kind of 3D reconstruction or stuff like that that's working reasonably well especially if you have a lot of data",
    "start": "2272260",
    "end": "2279099"
  },
  {
    "text": "um but especially especially in the applications of Robotics then we often deal with objects we've never seen before you know and you know sometimes",
    "start": "2279099",
    "end": "2286000"
  },
  {
    "text": "you don't even know is that an object category for it it's just like a random piece of thing um so we wanted to learn without",
    "start": "2286000",
    "end": "2292599"
  },
  {
    "text": "supervision or prior knowledge or assumptions about object categories I don't I'm not only working with cars I'm",
    "start": "2292599",
    "end": "2298480"
  },
  {
    "text": "not only working with a mug but I can the method the inference method can derive these object representations",
    "start": "2298480",
    "end": "2304480"
  },
  {
    "text": "without assuming object categories hey you wanted to explain the image formation process because I know so what",
    "start": "2304480",
    "end": "2311619"
  },
  {
    "text": "does that mean it's you should really capture this generative model or the simulation process there are a lot of work on like oh going from a single",
    "start": "2311619",
    "end": "2317320"
  },
  {
    "text": "image I decompose it and try to infer if this is a segment that is a segment right so I get a lot of you know these",
    "start": "2317320",
    "end": "2323440"
  },
  {
    "text": "image patches or segments but they don't really understand the world is 3D and capturing the 3D geometry or the",
    "start": "2323440",
    "end": "2329320"
  },
  {
    "text": "simulation process in terms of visual data the rendering process now what happened is yeah you get these segments",
    "start": "2329320",
    "end": "2335380"
  },
  {
    "text": "but you don't really know especially in the case of occlusion or if I remove an object what it's going to what's going",
    "start": "2335380",
    "end": "2340480"
  },
  {
    "text": "to what am I going to see because you don't capture image foreign formation process and every object to you it's",
    "start": "2340480",
    "end": "2346180"
  },
  {
    "text": "just like a piece of paper and also included a partial piece of paper so you really want to understand that",
    "start": "2346180",
    "end": "2352480"
  },
  {
    "text": "and of course you know you want to also be 3D aware so you capture the geometric and physical properties of objects for",
    "start": "2352480",
    "end": "2359020"
  },
  {
    "text": "in 3D so if you look at what existing methods can do right so this is in computer",
    "start": "2359020",
    "end": "2364599"
  },
  {
    "text": "vision back in 2013 without deep learning people can like okay give me a collection of images I can try to give",
    "start": "2364599",
    "end": "2369760"
  },
  {
    "text": "you the segments of these objects so they're only simple as they don't assume object categories but they're not really",
    "start": "2369760",
    "end": "2375099"
  },
  {
    "text": "3D aware again these are like piece of papers and they don't capture image formation process so how the image is",
    "start": "2375099",
    "end": "2380619"
  },
  {
    "text": "made you know um so more recently there is some work especially from the mind and uh Google",
    "start": "2380619",
    "end": "2387940"
  },
  {
    "text": "brain where they try to use most typically they call it slaw base methods",
    "start": "2387940",
    "end": "2393880"
  },
  {
    "text": "which is essentially trying to use neural network combine them with probability inference framework to aim",
    "start": "2393880",
    "end": "2400240"
  },
  {
    "text": "for going from a single image okay what would be the objects that can belong to these different slots with the hope that",
    "start": "2400240",
    "end": "2407619"
  },
  {
    "text": "different styles will correspond to different objects so these models are also and supervised and in some sense",
    "start": "2407619",
    "end": "2413859"
  },
  {
    "text": "they do you know sort of have a little bit knowledge of the formation process because they're like okay the image is",
    "start": "2413859",
    "end": "2420339"
  },
  {
    "text": "made because they're objects and they're like you know um there's like a probability distribute actually they have a property you can",
    "start": "2420339",
    "end": "2426400"
  },
  {
    "text": "call it like a probabilistic image generation model about how this image can be synthesized",
    "start": "2426400",
    "end": "2431560"
  },
  {
    "text": "um but they're not speedy aware so you know to them these are like still like 2D 2D parts and you cannot imagine how",
    "start": "2431560",
    "end": "2437440"
  },
  {
    "text": "the object look like from different viewpoints they don't capture anything like 3D geometry of the objects",
    "start": "2437440",
    "end": "2443140"
  },
  {
    "text": "and there are also other works that are trying to uh you know directly",
    "start": "2443140",
    "end": "2450119"
  },
  {
    "text": "reconstruct the 3D geometry and pose of the object so that you can you know change of the positions give you an",
    "start": "2450119",
    "end": "2456160"
  },
  {
    "text": "image you can get the positions object you can move object around so these kind of work they do understand yeah it's a",
    "start": "2456160",
    "end": "2463060"
  },
  {
    "text": "3D and it does capture everything like rendering and everything but most typically these kind of work they assume",
    "start": "2463060",
    "end": "2469240"
  },
  {
    "text": "and I think that the most typical one is they always assume they work with street views data so because they know the only",
    "start": "2469240",
    "end": "2475660"
  },
  {
    "text": "object category that they can work with is cars this is because they require a",
    "start": "2475660",
    "end": "2480700"
  },
  {
    "text": "lot of annotations of the object categories they're like okay for this object will be the geometry or for all the cars what would be the mean shape of",
    "start": "2480700",
    "end": "2487480"
  },
  {
    "text": "the possible cars so they assume uh you know you have knowledge of object categories and they're not like uncertified and they cannot really",
    "start": "2487480",
    "end": "2493660"
  },
  {
    "text": "generalize to you know new objects that the monk is knowledge of new objects but you know these kind of new objects here",
    "start": "2493660",
    "end": "2499000"
  },
  {
    "text": "and there you know which I don't have that many 3D CAD models for so we thought is it possible for us to",
    "start": "2499000",
    "end": "2504579"
  },
  {
    "text": "do this and supervised up to Discovery but discovered these neural object",
    "start": "2504579",
    "end": "2509619"
  },
  {
    "text": "representations that hopefully captures all three aspects where I wanted to end supervised wanted to explain image",
    "start": "2509619",
    "end": "2515320"
  },
  {
    "text": "formation process we also wanted to be 3D aware so that you know you can remove an object seeing what's behind but you",
    "start": "2515320",
    "end": "2520960"
  },
  {
    "text": "can also rotate an object seeing you know the other side of the object so here's what we can do is going from my",
    "start": "2520960",
    "end": "2527140"
  },
  {
    "text": "input image again this is all without supervision and doing testing the only input is single image you can actually",
    "start": "2527140",
    "end": "2532839"
  },
  {
    "text": "get different segments are actually different you know not segment different like slots or different",
    "start": "2532839",
    "end": "2539020"
  },
  {
    "text": "entities that you capture okay these objects and as well as the background and you capture them in 3D in the sense",
    "start": "2539020",
    "end": "2544960"
  },
  {
    "text": "that you can rotate objects see what's behind and you can reconstruct the scene you can remove an object you can insert",
    "start": "2544960",
    "end": "2552460"
  },
  {
    "text": "an object and you can rearrange the objects",
    "start": "2552460",
    "end": "2557460"
  },
  {
    "text": "so the inference process goes this way given an image you know we first we do use this kind of slow based approaches",
    "start": "2562720",
    "end": "2569200"
  },
  {
    "text": "which is something I talked about earlier but they work with 2D so what we",
    "start": "2569200",
    "end": "2574300"
  },
  {
    "text": "took we took their inference framework about decomposing object into a number of different slots but then instead of",
    "start": "2574300",
    "end": "2579819"
  },
  {
    "text": "directly reconstructing the image we incorporate this kind of object-centric neural representations so for every",
    "start": "2579819",
    "end": "2587260"
  },
  {
    "text": "object think about it as a conditional vector and you have a neural network that is conditioned on it and you decode",
    "start": "2587260",
    "end": "2592540"
  },
  {
    "text": "okay what does that Vector really represent right so you're able to Decon I would say decode the geometry and",
    "start": "2592540",
    "end": "2599380"
  },
  {
    "text": "reflectance of objects based on but now you have a conditional decoding model so which is not overfitting to a single",
    "start": "2599380",
    "end": "2605200"
  },
  {
    "text": "object before we say okay we'll offer to a single object object folder is a data set of one certain objects we have one",
    "start": "2605200",
    "end": "2610480"
  },
  {
    "text": "thousand or three thousand neural networks but here it is kind of a bit more General in the sense that okay we have different neural networks but",
    "start": "2610480",
    "end": "2617020"
  },
  {
    "text": "actually we have two different units one for background one foreground but for this unit I will stay conditional in the",
    "start": "2617020",
    "end": "2623020"
  },
  {
    "text": "sense that they take the latent vectors that hopefully captures the geometry and material it reflectance of objects and",
    "start": "2623020",
    "end": "2628480"
  },
  {
    "text": "you're able to decline you're able to decode it once you condition on this latent vector and then you still query",
    "start": "2628480",
    "end": "2633940"
  },
  {
    "text": "you know the XYZ the positions and viewpoints and everything and you will be able to get different objects with",
    "start": "2633940",
    "end": "2639579"
  },
  {
    "text": "this conditional neural network so once you can decompose the scene into different slots or different entities",
    "start": "2639579",
    "end": "2646359"
  },
  {
    "text": "and you can you know decode their appearance in 3D then you can put it back with all these you know standard",
    "start": "2646359",
    "end": "2653140"
  },
  {
    "text": "image rendering framework to reconstruct the scene and your tuning is unsupervised and so which means you're",
    "start": "2653140",
    "end": "2660400"
  },
  {
    "text": "on the under supervision you have is how well your reconstruction is doing you try to reconstruct the scene you compare",
    "start": "2660400",
    "end": "2666099"
  },
  {
    "text": "a reconstructed image with the input image and in training we do assume you have multiple views just like Nerf so",
    "start": "2666099",
    "end": "2672520"
  },
  {
    "text": "you can reconstruct image from different viewpoints and see how the Reconstruction look like in different",
    "start": "2672520",
    "end": "2677800"
  },
  {
    "text": "viewpoints and during testing you're only given a single image and you can aim for the",
    "start": "2677800",
    "end": "2683740"
  },
  {
    "text": "segments and do normal synthesis and editing as well",
    "start": "2683740",
    "end": "2688140"
  },
  {
    "text": "so here's a bit more results input View and this is the ground truth and the",
    "start": "2689319",
    "end": "2694720"
  },
  {
    "text": "second column is the ground two segmentation and this is you know purely using slot tension which doesn't have",
    "start": "2694720",
    "end": "2700000"
  },
  {
    "text": "this kind of object-centric modeling doesn't use neural object representations that does not capture things in 3D so first it didn't work",
    "start": "2700000",
    "end": "2706180"
  },
  {
    "text": "that well and second you know because it's not 3D aware it cannot synthesize what's going on from a different Viewpoint and this is uh our results",
    "start": "2706180",
    "end": "2713740"
  },
  {
    "text": "this is like comparison with the baselines",
    "start": "2713740",
    "end": "2718260"
  },
  {
    "text": "foreign how well it works in real images so",
    "start": "2719260",
    "end": "2727060"
  },
  {
    "text": "clearly the model is limited in how the complexity of data it can deal with so",
    "start": "2727060",
    "end": "2732099"
  },
  {
    "text": "before it was all synthetic so we were like can we capture some real images so again we purchased some objects and we",
    "start": "2732099",
    "end": "2737140"
  },
  {
    "text": "capture these kind of images of real toy toy chairs uh and you know it it does",
    "start": "2737140",
    "end": "2742180"
  },
  {
    "text": "work on generalizing to do these Road holy chairs but that's basically this paper can do that was with Leo give us",
    "start": "2742180",
    "end": "2748720"
  },
  {
    "text": "in this year's iClear um so it's um",
    "start": "2748720",
    "end": "2754599"
  },
  {
    "text": "I think it's a it's okay but you know there's still a long way to go about going from this",
    "start": "2754599",
    "end": "2760660"
  },
  {
    "text": "synthetic toy-ish data uh neither in actually this is real data but it's contrived I was contrived a synthetic or",
    "start": "2760660",
    "end": "2767079"
  },
  {
    "text": "real data to the actual you know complex scenes that we care about",
    "start": "2767079",
    "end": "2772020"
  },
  {
    "text": "but the benefit of this kind of representation is you know once you can do that you can you know uh I would say",
    "start": "2773619",
    "end": "2779020"
  },
  {
    "text": "moving object and switch from before this like round shoes you know if you compare with the bass lines they can't really move an object they have they",
    "start": "2779020",
    "end": "2784480"
  },
  {
    "text": "don't have the knowledge of the 3D representations and if this is you purely using just Nerf with an auto",
    "start": "2784480",
    "end": "2790720"
  },
  {
    "text": "encoder and the last column is ours so you can actually remove the magic you",
    "start": "2790720",
    "end": "2795760"
  },
  {
    "text": "can see an object from different views you can change the background so you say okay I have an image like this if I want",
    "start": "2795760",
    "end": "2801880"
  },
  {
    "text": "to change the background so that you know I want everything else to be the same but only the background of this",
    "start": "2801880",
    "end": "2807700"
  },
  {
    "text": "input image should be background of that image so these are you can also do much better",
    "start": "2807700",
    "end": "2813520"
  },
  {
    "text": "than the bass lines",
    "start": "2813520",
    "end": "2816960"
  },
  {
    "text": "okay so we do care about generalizing to compact scenes so finally let me wrap up with our very reason this is mostly",
    "start": "2822400",
    "end": "2829660"
  },
  {
    "text": "ongoing effort about how we can generalize for this inference method to infer these newer Optics not only from",
    "start": "2829660",
    "end": "2835000"
  },
  {
    "text": "images of you know synthetic you know Pure Color chairs and in wheelchairs but still you know very simple chairs to",
    "start": "2835000",
    "end": "2840760"
  },
  {
    "text": "slightly more complex scenes and I think we observed that the biggest challenge of scaling this method up to more",
    "start": "2840760",
    "end": "2847720"
  },
  {
    "text": "complex scenes is when you're trying to infer the objects or decomposed into different slots or entities you know",
    "start": "2847720",
    "end": "2854560"
  },
  {
    "text": "because we're purely based on earlier workhouse style attention it doesn't really generalize that well to more complex scenes",
    "start": "2854560",
    "end": "2861040"
  },
  {
    "text": "so we need methods that and the reason I didn't generalize that well is it is trying to infer okay what is an object",
    "start": "2861040",
    "end": "2866980"
  },
  {
    "text": "purely by reconstructing the scene so it's purely based on appearance cues therefore it works well or relatively",
    "start": "2866980",
    "end": "2873339"
  },
  {
    "text": "well for objects with pure color but for objects with more complex textures and it didn't work that well and also this",
    "start": "2873339",
    "end": "2879579"
  },
  {
    "text": "is not really how objects are made because if we think about you know what is an object it's not like you know of",
    "start": "2879579",
    "end": "2885220"
  },
  {
    "text": "course it's likely that you know objects a single objects you know different parts of a single object may have",
    "start": "2885220",
    "end": "2890380"
  },
  {
    "text": "similar color or appearance and that's why these methods work to some extent more importantly you know what makes an",
    "start": "2890380",
    "end": "2895839"
  },
  {
    "text": "opt-in object also you know if you're looking to Classic Finance quality of science they're like it's more about",
    "start": "2895839",
    "end": "2900940"
  },
  {
    "text": "their emotion right the way they move together and enter the same interaction these different pixels they move together that really makes them an",
    "start": "2900940",
    "end": "2907000"
  },
  {
    "text": "object so if we care about the problem of now we know that the problem we want to",
    "start": "2907000",
    "end": "2912099"
  },
  {
    "text": "solve of skating these you know infer and your objective representation work to complex scenes is to actually address",
    "start": "2912099",
    "end": "2917740"
  },
  {
    "text": "the problem of the first step that is unsupplies category agnostic segmentation of studying real world",
    "start": "2917740",
    "end": "2923980"
  },
  {
    "text": "images into objects because once you can do a segmentation you can do a conditional neural object repetition",
    "start": "2923980",
    "end": "2929020"
  },
  {
    "text": "nerve to infer the objects in 3D and re-render so if you care about this",
    "start": "2929020",
    "end": "2934480"
  },
  {
    "text": "problem and this is a demo of the another demo this",
    "start": "2934480",
    "end": "2939819"
  },
  {
    "text": "is a data set from this is a bridge data set right by collected by people at Stanford Berkeley",
    "start": "2939819",
    "end": "2946359"
  },
  {
    "text": "um so you can see that our goal is you know based on option motion you know you'll be able to see okay what are the",
    "start": "2946359",
    "end": "2951640"
  },
  {
    "text": "things that actually often move together and you can tie this object motion to their appearance so that based on a",
    "start": "2951640",
    "end": "2957760"
  },
  {
    "text": "short video of you know what object has been doing you can infer um and to do segmentations you can get the",
    "start": "2957760",
    "end": "2964960"
  },
  {
    "text": "segments of these different objects again you don't assume supervision you don't assume annotations are object categories so the goal is this to purely",
    "start": "2964960",
    "end": "2971920"
  },
  {
    "text": "unsupialized and this is category agnostic so this is the main idea behind Eisen so",
    "start": "2971920",
    "end": "2977859"
  },
  {
    "text": "our joint work with Stan yamis and actually also Josh Turner mom as well as Mark mentioned with also against",
    "start": "2977859",
    "end": "2983859"
  },
  {
    "text": "students and postdocs holding Chen and Dan bear so the idea is a collection of",
    "start": "2983859",
    "end": "2990040"
  },
  {
    "text": "physical stuff will always move together under the same application of date everyday actions so can we actually",
    "start": "2990040",
    "end": "2995200"
  },
  {
    "text": "leverage the fact that things that often move together based on their emotion in addition to appearance cues to discover",
    "start": "2995200",
    "end": "3000960"
  },
  {
    "text": "what makes an object an object so that we can get in supervised character agnostic object segmentation",
    "start": "3000960",
    "end": "3007859"
  },
  {
    "text": "unfortunately I don't have time and there's only four minutes left and people are leaving but so uh so I'm very",
    "start": "3007859",
    "end": "3014220"
  },
  {
    "text": "happy to talk about it offline but here I'm just going to show uh one slide of the results where here the input images",
    "start": "3014220",
    "end": "3020819"
  },
  {
    "text": "again oh actually one thing I forgot to mention is during training the framework which we call Eisen leverage motion to",
    "start": "3020819",
    "end": "3027720"
  },
  {
    "text": "learn what makes an opt-in object and try to tie it up the emotional information to the appearance during testing it actually because it has",
    "start": "3027720",
    "end": "3034380"
  },
  {
    "text": "learned what are the textures objects often move together during testing only requires a single image so it's learning",
    "start": "3034380",
    "end": "3039720"
  },
  {
    "text": "unsupialized category agnostic of the segmentation from a single image and hit on the left is the input image and the",
    "start": "3039720",
    "end": "3046740"
  },
  {
    "text": "second and third column on the baselines and the fourth column is the iso and output you can see that it does pretty",
    "start": "3046740",
    "end": "3052020"
  },
  {
    "text": "well on unsupialized category elastic after segmentation for these kind of pretty complex scenes",
    "start": "3052020",
    "end": "3057780"
  },
  {
    "text": "and there's still a gap between that and Grand shoes but I think it's actually pretty close",
    "start": "3057780",
    "end": "3062819"
  },
  {
    "text": "I was very impressed by the results of this paper then we thought okay of course now you",
    "start": "3062819",
    "end": "3068339"
  },
  {
    "text": "address the challenge not fully but to some extent address the challenge of how you can get in supervised category and",
    "start": "3068339",
    "end": "3074099"
  },
  {
    "text": "Gnostic segments of objects even from a single image then you should replace slot tension with icing right so now",
    "start": "3074099",
    "end": "3080280"
  },
  {
    "text": "give an image you do the same thing and try to get the after segmentation based on ice and now instead of slow attention",
    "start": "3080280",
    "end": "3086640"
  },
  {
    "text": "with the hope that generalizes to these more complex objects Beyond this Pure Color shares and after that you can do",
    "start": "3086640",
    "end": "3093359"
  },
  {
    "text": "the same thing as we did before you know decompose every object segment into a conditional latent vector and decode it",
    "start": "3093359",
    "end": "3099420"
  },
  {
    "text": "with you know conditional I would say a neural object representation to get object rep object nerves and backbone",
    "start": "3099420",
    "end": "3104940"
  },
  {
    "text": "nerves and you can put them together to reconstruct the scene and again you can",
    "start": "3104940",
    "end": "3110520"
  },
  {
    "text": "train to reconstruct the scene by matching the Reconstruction loss right so here the same pipelines before but",
    "start": "3110520",
    "end": "3116940"
  },
  {
    "text": "now the inference the influence of newer objects relies on Ison instead of a instead of just star tension",
    "start": "3116940",
    "end": "3124740"
  },
  {
    "text": "and finally you know we were able to make a little bit more progress on how you can get this kind of 3D aware",
    "start": "3124740",
    "end": "3130040"
  },
  {
    "text": "character Gnostic uh generative representations for slightly more",
    "start": "3130040",
    "end": "3135180"
  },
  {
    "text": "complex scenes so these are Beyond you know Pure Color chairs and you can go from images and this is the method we",
    "start": "3135180",
    "end": "3141300"
  },
  {
    "text": "had before we call unsupize of your Radiance field so it's like uorf uh you know for these more complex scenes",
    "start": "3141300",
    "end": "3146940"
  },
  {
    "text": "because it's purely based on Textures it didn't work that well especially for objects of multiple colors uh",
    "start": "3146940",
    "end": "3153300"
  },
  {
    "text": "but with what we call this motion or moving object radius Fields morph then it does much better and it's reasonably",
    "start": "3153300",
    "end": "3160619"
  },
  {
    "text": "close to the ground shoes again this is going from a single image and they're discovering objects imagining how it looks like from different viewpoints",
    "start": "3160619",
    "end": "3166559"
  },
  {
    "text": "guiding that 3D neural object representations and re-render it so this is input image and output is a",
    "start": "3166559",
    "end": "3171960"
  },
  {
    "text": "re-rendering the image from different viewpoints you can compare that with the one true",
    "start": "3171960",
    "end": "3177920"
  },
  {
    "text": "so it does I would say reconstruct uh fine-grained photometric details with higher Fidelity and you know it also",
    "start": "3180300",
    "end": "3187559"
  },
  {
    "text": "captures object geometry again this is all from a single image during testing these are novel objects the scene that",
    "start": "3187559",
    "end": "3193319"
  },
  {
    "text": "the system has never seen these objects before and and also there's chiragognostic you can see these are objects of different categories again",
    "start": "3193319",
    "end": "3199740"
  },
  {
    "text": "from all a single image it can do reasonably well in getting the geometry objects perform now available since this",
    "start": "3199740",
    "end": "3205579"
  },
  {
    "text": "reasonably close to the ground to use a much better database lines so I'll say it also allows phase four",
    "start": "3205579",
    "end": "3212280"
  },
  {
    "text": "mesh reconstructions from object Radiance fields",
    "start": "3212280",
    "end": "3216920"
  },
  {
    "text": "okay um so to summarize I have one minute left now we talked about modeling and",
    "start": "3221940",
    "end": "3228540"
  },
  {
    "text": "how we are making the simulation process differential and inverting the process so that you can get object-centric",
    "start": "3228540",
    "end": "3234420"
  },
  {
    "text": "representations often parameterized neural networks with visual appearance in the sounds the data sets how you can",
    "start": "3234420",
    "end": "3240540"
  },
  {
    "text": "scale it up so you can have a data set between 1000 newer objects or switch our neural networks how it can be possibly",
    "start": "3240540",
    "end": "3246900"
  },
  {
    "text": "applied into these robotic applications as well as the inference problems how you can afford these new object repetitions in the case of you know",
    "start": "3246900",
    "end": "3253140"
  },
  {
    "text": "clutter Mass CCS now this is pretty much ongoing work as you can see but we're making progress",
    "start": "3253140",
    "end": "3258839"
  },
  {
    "text": "so if I want to summarize the key message I will deliver in this talk I think it's mostly about you know sure",
    "start": "3258839",
    "end": "3264420"
  },
  {
    "text": "it's very popular to have implicit or newer representations these days for modeling nerve and scenes and everything",
    "start": "3264420",
    "end": "3269700"
  },
  {
    "text": "and people say okay that's great so we can look into how they can be applied on Envision Graphics boxing robotics but I",
    "start": "3269700",
    "end": "3275819"
  },
  {
    "text": "think when you're doing that you may want to think about you know what are the things that should really be learned of private trust by neural networks and",
    "start": "3275819",
    "end": "3281520"
  },
  {
    "text": "what are the things that are already there and there's no need to you know either relearn it or you know learning",
    "start": "3281520",
    "end": "3286980"
  },
  {
    "text": "it may not help you but actually hurt socialization now I would say what we're doing here is we're thinking about the",
    "start": "3286980",
    "end": "3292079"
  },
  {
    "text": "physical object models right so what is the what really makes an opt-in object what really belongs to the object what",
    "start": "3292079",
    "end": "3297540"
  },
  {
    "text": "are the object intrinsics and these are often the things that are complex hard to model I would say analytically so",
    "start": "3297540",
    "end": "3304859"
  },
  {
    "text": "probably should better be learned by neural network and by having this dry level of abstraction and disentanglement",
    "start": "3304859",
    "end": "3310740"
  },
  {
    "text": "then your allows you to do flexible and compositional generalization in terms of",
    "start": "3310740",
    "end": "3316200"
  },
  {
    "text": "you know seeing scenes with multiple objects for with different rearrangement configurations or in the different",
    "start": "3316200",
    "end": "3321540"
  },
  {
    "text": "lighting conditions and you know once you can aim for these object repetitions you can move the objects you can imagine",
    "start": "3321540",
    "end": "3327300"
  },
  {
    "text": "what's behind you can see it from different views and stuff like that",
    "start": "3327300",
    "end": "3332780"
  },
  {
    "text": "so I think fundamentally we want to address the questions why that is you know when and why is the reason to use",
    "start": "3332780",
    "end": "3339300"
  },
  {
    "text": "the implicit reputations on your representations and when to use them when not to use it right so I think",
    "start": "3339300",
    "end": "3344819"
  },
  {
    "text": "these are like a summary of our approach um you know leveraging these kind of",
    "start": "3344819",
    "end": "3349859"
  },
  {
    "text": "powerful neural object references but to deploy in that physical object-centric way to get these multi-sensory neural",
    "start": "3349859",
    "end": "3356880"
  },
  {
    "text": "Optics thank you",
    "start": "3356880",
    "end": "3359900"
  },
  {
    "text": "foreign I enjoyed your slide combining tactile",
    "start": "3362040",
    "end": "3369420"
  },
  {
    "text": "and vision of course because I do tactical stuff but but a question I had is do you think there was any",
    "start": "3369420",
    "end": "3376380"
  },
  {
    "text": "information you were getting from the tactile sensor that was",
    "start": "3376380",
    "end": "3382040"
  },
  {
    "text": "not visible more useful than just having like a camera on the figure looking in and having another camera that would",
    "start": "3382200",
    "end": "3388440"
  },
  {
    "text": "also provide more information would that be just as good yeah that is a very good point uh so",
    "start": "3388440",
    "end": "3396140"
  },
  {
    "text": "I think you know if you're looking at this like the Reconstruction part that you know this is like a sweet shape",
    "start": "3397859",
    "end": "3403079"
  },
  {
    "text": "reconstruction I think you know probably not because again this is a demonstration of like oh we have three modalities if we support these tasks but",
    "start": "3403079",
    "end": "3409559"
  },
  {
    "text": "this particular example I don't feel like you actually maybe you know touch really giving you more but",
    "start": "3409559",
    "end": "3414720"
  },
  {
    "text": "if you look at later work where we try to really show you know there are cases where tactile information can be useful",
    "start": "3414720",
    "end": "3420300"
  },
  {
    "text": "and I feel like this this is the task that I didn't talk about called Dance packing so in sometimes you're trying to",
    "start": "3420300",
    "end": "3425819"
  },
  {
    "text": "insert objects and put them together and then you know but in a very compact way and I think these are cases where we do",
    "start": "3425819",
    "end": "3431520"
  },
  {
    "text": "have a lot of ablation studies as well as the visualization of the attention maps and you and I were to show that there are cases where the neural network",
    "start": "3431520",
    "end": "3437700"
  },
  {
    "text": "really around the tactile information to really get object to the right positions thank you",
    "start": "3437700",
    "end": "3444800"
  },
  {
    "text": "thank you very much for the talk I had a question about how is your",
    "start": "3451260",
    "end": "3457380"
  },
  {
    "text": "difficult it would be to update your object data set 2.0",
    "start": "3457380",
    "end": "3462980"
  },
  {
    "text": "as you refine your techniques for learning different types of",
    "start": "3462980",
    "end": "3469619"
  },
  {
    "text": "sensory interactions with the objects so you're saying uh sorry just try to make",
    "start": "3469619",
    "end": "3475020"
  },
  {
    "text": "sure I understand the question are you saying once we have a better simulator like you know taxi M 2.0 or something",
    "start": "3475020",
    "end": "3480540"
  },
  {
    "text": "then we have better tactile sensing or or better audio simulation as well then",
    "start": "3480540",
    "end": "3485940"
  },
  {
    "text": "is it possible to update the data set so that the representations can leverage these Advanced simulation techniques so",
    "start": "3485940",
    "end": "3492059"
  },
  {
    "text": "that they can they're more realistic is that your question yes yeah yeah uh I think yes I think that is",
    "start": "3492059",
    "end": "3498660"
  },
  {
    "text": "actually sort of the benefit of these virtualized data sets because if you have a ycb set of ycb objects and one of",
    "start": "3498660",
    "end": "3504780"
  },
  {
    "text": "them is broken what can you do about it we try to get a replacement that seems hard and that update may be out of stock or something but you know for these",
    "start": "3504780",
    "end": "3512040"
  },
  {
    "text": "virtualized objects I think the benefit is once you have better simulation techniques here's you know you can get better representations of the objects in",
    "start": "3512040",
    "end": "3518760"
  },
  {
    "text": "acoustic or tactile properties and all you need to do is to retrain this neural network so you can just release and",
    "start": "3518760",
    "end": "3525660"
  },
  {
    "text": "update your neural network the network can use in the neural network output is more realistic so",
    "start": "3525660",
    "end": "3531299"
  },
  {
    "text": "if that is your question yeah yeah no that was it I was you don't need to do more sort of ground truth testing",
    "start": "3531299",
    "end": "3538500"
  },
  {
    "text": "I think no yeah because yeah",
    "start": "3538500",
    "end": "3542420"
  },
  {
    "text": "um I guess there's been many recent work on just using enough to learn representations for uh robotication",
    "start": "3547740",
    "end": "3553859"
  },
  {
    "text": "um so I guess um also some learning for like particularly objects using a nerve so I guess do you see Nerf as the uh",
    "start": "3553859",
    "end": "3559500"
  },
  {
    "text": "future representation that can be super helpful for uh like multiplication tasks what do you think like other representations like a key Point like a",
    "start": "3559500",
    "end": "3566940"
  },
  {
    "text": "descriptive set could be more useful for your musician tasks in particular I think it's very interesting that in your",
    "start": "3566940",
    "end": "3573299"
  },
  {
    "text": "question is a typical example that because nerve give this amazing visual quality then people use it to refer to",
    "start": "3573299",
    "end": "3579420"
  },
  {
    "text": "something else because when you say Nerf I think you're not really referring to nerfing in particular because that is a",
    "start": "3579420",
    "end": "3585000"
  },
  {
    "text": "method that for normal synthesis and you know allow you to see an object from different views and the most impressive",
    "start": "3585000",
    "end": "3591180"
  },
  {
    "text": "part of nerve is the objects that look so realistic details that you need you have all these in textual details but is",
    "start": "3591180",
    "end": "3597119"
  },
  {
    "text": "that something you really care about your robotics you know that is maybe not and I think what you're maybe what",
    "start": "3597119",
    "end": "3602819"
  },
  {
    "text": "you're really seeing is what about neural objective reference not the radiance part not the nerve R part but",
    "start": "3602819",
    "end": "3609660"
  },
  {
    "text": "what about you know if you look at you know what is really more critical to robotics is you know in particular I",
    "start": "3609660",
    "end": "3615240"
  },
  {
    "text": "would say geometry so the implicit the idea of using you and our implicit representations is way there you know",
    "start": "3615240",
    "end": "3621420"
  },
  {
    "text": "like two years ago then there's deep SDF and all those work that they have been showing the part the power of these",
    "start": "3621420",
    "end": "3626640"
  },
  {
    "text": "simplifications it's nerve that they say Okay instead of using it for geometry we use it for radians and that gives you",
    "start": "3626640",
    "end": "3632520"
  },
  {
    "text": "this fantastic visual quality just like daoi in these days or earlier cycle games and suddenly it got so popular",
    "start": "3632520",
    "end": "3637680"
  },
  {
    "text": "everyone's talking about it but the underlying I think the most important idea is to use think about it as is to",
    "start": "3637680",
    "end": "3644400"
  },
  {
    "text": "use neural networks to parametrize some P properties that we care about and I would say it's less about nerve it's",
    "start": "3644400",
    "end": "3651359"
  },
  {
    "text": "less about Radiance and maybe more about geometry and maybe more about physics you know it could be the newer representation for object physics and",
    "start": "3651359",
    "end": "3658319"
  },
  {
    "text": "now if your question is not about nerve but in general what neural representations play an important role to capture these important properties uh",
    "start": "3658319",
    "end": "3664980"
  },
  {
    "text": "and how would I compare with other explicit representations like key points and matches then I'll say yes you know",
    "start": "3664980",
    "end": "3670260"
  },
  {
    "text": "probably at least I can imagine a combination of these uh in expliciting places what we needed because there are",
    "start": "3670260",
    "end": "3676200"
  },
  {
    "text": "a lot of things that we don't really know how to parametrize explicitly and all we have is a lot of observations so it's not more like a data driven uh you",
    "start": "3676200",
    "end": "3683280"
  },
  {
    "text": "know approaches that you can overfit your data by a neural network for things you care about um and you cannot really write",
    "start": "3683280",
    "end": "3689040"
  },
  {
    "text": "analytical equations for but probably that's not Radiance or Radiance is very secondary",
    "start": "3689040",
    "end": "3694260"
  },
  {
    "text": "thank you yep",
    "start": "3694260",
    "end": "3700980"
  },
  {
    "text": "do you think it might be useful to simulate the sound of two objects interacting with each other and if you",
    "start": "3700980",
    "end": "3709079"
  },
  {
    "text": "have the impact response of two different objects the auditory impact response can can you go from that to what it",
    "start": "3709079",
    "end": "3716400"
  },
  {
    "text": "would sound like if they hit each other or they're up against each other uh I'm sorry I didn't get the questions",
    "start": "3716400",
    "end": "3722640"
  },
  {
    "text": "like if you have two objects you have the impulse responses of the two objects so you have the separate responses of",
    "start": "3722640",
    "end": "3729420"
  },
  {
    "text": "the two objects when you hit them with an impact hammer right but if you want to simulate what happens if you rub them",
    "start": "3729420",
    "end": "3735000"
  },
  {
    "text": "against each other for example a cup on a table yes do you think that might be useful and how difficult would it be to",
    "start": "3735000",
    "end": "3741960"
  },
  {
    "text": "go from the individual responses to also being able to simulate interactions",
    "start": "3741960",
    "end": "3747359"
  },
  {
    "text": "between them because essentially what is going on is you know if you have two object cladding if you drop something on",
    "start": "3747359",
    "end": "3753420"
  },
  {
    "text": "the table that it's basically you think about is if there are things that we cannot simulate but in most cases there",
    "start": "3753420",
    "end": "3758460"
  },
  {
    "text": "are a lot of things you care about it can be decomposed into a number of impacts at different or impulses at",
    "start": "3758460",
    "end": "3763680"
  },
  {
    "text": "different times at different time steps with different magnitudes um so in the case of Q object cladding",
    "start": "3763680",
    "end": "3769680"
  },
  {
    "text": "as well you know it's probably one Collision happening a lot of collision happen later subsequently over time they're not as large so I don't see why",
    "start": "3769680",
    "end": "3777720"
  },
  {
    "text": "and the method cannot be extended there there are things that are really hard to simulate like the sound of scratch I",
    "start": "3777720",
    "end": "3784260"
  },
  {
    "text": "don't want to scratch it sounds bad but if you scratch Blackboard those are kind of slightly different and that's very hard to simulate and I don't think",
    "start": "3784260",
    "end": "3790920"
  },
  {
    "text": "um you know because no all these things we build our simulation techniques that are mostly developing computer Graphics",
    "start": "3790920",
    "end": "3796440"
  },
  {
    "text": "that are in ongoing work and Doc James Stanford is a meme you know he's a very major person who created all these",
    "start": "3796440",
    "end": "3803220"
  },
  {
    "text": "techniques and I think a lot of sound like scratches we still cannot simulate very well but for for impact sounds we",
    "start": "3803220",
    "end": "3809520"
  },
  {
    "text": "can simulate pretty well and it's nice that for a lot of manufusion problems especially about rigid values and",
    "start": "3809520",
    "end": "3814680"
  },
  {
    "text": "deformable objects not different articulate articulated radio bodies um you can actually decompose a lot of",
    "start": "3814680",
    "end": "3820559"
  },
  {
    "text": "these into this impact sound so that's what we can do thank you",
    "start": "3820559",
    "end": "3825440"
  },
  {
    "text": "thank you yeah",
    "start": "3830400",
    "end": "3832880"
  },
  {
    "text": "um I was just wondering so when you were showing that example for like audio when",
    "start": "3838799",
    "end": "3843900"
  },
  {
    "text": "you were pouring the water right how that would work if you were to unify that framework with a vision capability",
    "start": "3843900",
    "end": "3850440"
  },
  {
    "text": "right like I see you have this last slide but we weren't able to get to it due to like time constraints because",
    "start": "3850440",
    "end": "3855960"
  },
  {
    "text": "like here the amount of sound you have like largely depends on how much higher",
    "start": "3855960",
    "end": "3861000"
  },
  {
    "text": "above the like cup you pour it from so then that could depend also on like",
    "start": "3861000",
    "end": "3866460"
  },
  {
    "text": "Vision you realizing like what position is best to hold it at to then like I think the sound you make is actually",
    "start": "3866460",
    "end": "3871920"
  },
  {
    "text": "based on the container of the tube because you can think about it as there's there's air here and when you",
    "start": "3871920",
    "end": "3877500"
  },
  {
    "text": "feel it is up just like throwing water I won't fill this up then uh the the amount of air becomes shorter than the",
    "start": "3877500",
    "end": "3883200"
  },
  {
    "text": "pitches will change so I think that's probably mostly independent of where the",
    "start": "3883200",
    "end": "3888359"
  },
  {
    "text": "arm is but it does matter you know where the microphone is if you put a microphone on an arm then if you're very",
    "start": "3888359",
    "end": "3894059"
  },
  {
    "text": "high up the microphone could get very little sound but I think in this case we probably put a microphone here someday",
    "start": "3894059",
    "end": "3899099"
  },
  {
    "text": "yeah and then how does it work when you unify the three Frameworks like at the end",
    "start": "3899099",
    "end": "3905520"
  },
  {
    "text": "when you unify all the senses how does it end up working out is there actually",
    "start": "3905520",
    "end": "3911099"
  },
  {
    "text": "that's very interesting there's a slide you know it's already too long so I removed a lot of slides but we do have some analysis in the paper to show you",
    "start": "3911099",
    "end": "3918660"
  },
  {
    "text": "know that you try to you know at different stage of the programming process you know the system May at the very beginning it may attend mostly to",
    "start": "3918660",
    "end": "3925140"
  },
  {
    "text": "Vision because there's no sound and you want to make sure that you're at the right position you're right above the tube and when it started pouring you",
    "start": "3925140",
    "end": "3931440"
  },
  {
    "text": "know it's actually more important to looking into the other two directions because uh you know we there are some",
    "start": "3931440",
    "end": "3938400"
  },
  {
    "text": "you know actually it's now here we try to there's a visualization but in practice the camera cannot really see what is inside the tube I think there's",
    "start": "3938400",
    "end": "3944220"
  },
  {
    "text": "some tape so it's kind of opaque so you can't really get much from Vision you have to rely on audio and touch to sense",
    "start": "3944220",
    "end": "3951059"
  },
  {
    "text": "and to hear you know okay how much have I filled uh the tube and then especially",
    "start": "3951059",
    "end": "3957000"
  },
  {
    "text": "at the end I think the system put a lot of attention on the sound when it's close to the moment that it should stop",
    "start": "3957000",
    "end": "3962520"
  },
  {
    "text": "but there's a there's a plot but I don't have it here sorry about that yeah but it is in the paper",
    "start": "3962520",
    "end": "3969200"
  },
  {
    "text": "all right probably we should we should close this session for now um thank you again thank you",
    "start": "3969480",
    "end": "3975780"
  },
  {
    "text": "[Applause]",
    "start": "3975780",
    "end": "3979309"
  }
]