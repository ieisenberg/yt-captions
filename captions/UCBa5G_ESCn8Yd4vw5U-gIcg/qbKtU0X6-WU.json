[
  {
    "start": "0",
    "end": "5462"
  },
  {
    "text": "All right, folks. So there's a lot going\non I wanted to check in. So the course is speeding up a\nlittle bit in terms of the work.",
    "start": "5462",
    "end": "13879"
  },
  {
    "text": "I think the course is\nactually maybe slowing down a little bit in terms of\nthe number of new concepts",
    "start": "13880",
    "end": "18940"
  },
  {
    "text": "that you're being exposed\nto in every lecture. So that's my thought. So this is the phase where it's\nabout banging out assignment",
    "start": "18940",
    "end": "25540"
  },
  {
    "text": "3, all these\npractice assignments, as well as just starting\nto shore up understanding",
    "start": "25540",
    "end": "30550"
  },
  {
    "text": "of actual material. For those of you that haven't\nlooked at assignment 3,",
    "start": "30550",
    "end": "36440"
  },
  {
    "text": "there are two parts-- three parts, technically. But the first two\nparts of the assignment are what I would\nconsider to be warm up.",
    "start": "36440",
    "end": "43600"
  },
  {
    "text": "We give you an algorithm. You're supposed to\nwrite it in CUDA, just to start learning\nsome mechanics,",
    "start": "43600",
    "end": "50590"
  },
  {
    "text": "how do I get this\ncode to compile, how do I create some threads. The real part of the\nassignment is the last part,",
    "start": "50590",
    "end": "56960"
  },
  {
    "text": "which is the render. And unlike most other\nsystems classes, this is about coming up with a\nperformant implementation, sure.",
    "start": "56960",
    "end": "65590"
  },
  {
    "text": "But it's also actually\nabout coming up with some algorithms\nthat will make this work, that\nwill actually allow",
    "start": "65590",
    "end": "71860"
  },
  {
    "text": "you to paralyze this thing. And I figured I'd spend\nfive minutes, actually, just taking everybody\nthrough the problem that you're having to solve.",
    "start": "71860",
    "end": "78170"
  },
  {
    "text": "So the problem that\nyou have to solve is actually to render a picture. Here are some pictures. These are low resolution, but\nyou render a 1,000 by 1,000",
    "start": "78170",
    "end": "86020"
  },
  {
    "text": "image that looks like this. And to keep things simple, since\nthis isn't a computer graphics class, the only thing you\ncan draw in your pictures",
    "start": "86020",
    "end": "93160"
  },
  {
    "text": "are a bunch of circles. So I basically give you the\ncenter xy position, the radius,",
    "start": "93160",
    "end": "101020"
  },
  {
    "text": "and the color of large\nnumbers of circles. And so all of these\nimages, if you look at,",
    "start": "101020",
    "end": "108490"
  },
  {
    "text": "are just images of circles. And the main loop of how to do\nthis is actually pretty simple.",
    "start": "108490",
    "end": "114105"
  },
  {
    "text": " Let me go through\nall the other parts. It's actually pretty simple.",
    "start": "114105",
    "end": "119510"
  },
  {
    "text": "If you look at it-- let's ignore the front part\nwhere every frame, the circles can move. So ignore that.",
    "start": "119510",
    "end": "124729"
  },
  {
    "text": "But here, it's quite simple. For every circle\nthat I give you, compute the bounding\nbox of the circle.",
    "start": "124730",
    "end": "132360"
  },
  {
    "text": "So if I gave you the\nposition and the radius, you can compute\nthe bounding box. And then given the bounding\nbox, iterate through every pixel",
    "start": "132360",
    "end": "140569"
  },
  {
    "text": "in that bounding box. And if the center of the\npixel is inside the circle,",
    "start": "140570",
    "end": "146000"
  },
  {
    "text": "you update the\ncolor of the pixel accordingly, according to\nthe color of the circle. That's the world's simplest\ndrawing algorithm for samples.",
    "start": "146000",
    "end": "154700"
  },
  {
    "text": "Now, the challenge is that\nit's not-- these circles, if you look carefully\nin my picture,",
    "start": "154700",
    "end": "164490"
  },
  {
    "text": "are actually\nsomewhat transparent. So it's kind of a piece of\nstained glass or something",
    "start": "164490",
    "end": "170210"
  },
  {
    "text": "like that. So a 50% transparent red\ncircle on top of a blue circle",
    "start": "170210",
    "end": "177680"
  },
  {
    "text": "looks different than\nthat blue circle on top of the red circle\nbecause of transparency.",
    "start": "177680",
    "end": "183720"
  },
  {
    "text": "So the rule, the thing\nthat makes this problem goes from incredibly trivial\nto incredibly interesting,",
    "start": "183720",
    "end": "191360"
  },
  {
    "text": "is that-- I'm going to give you\nthe circles in an order. I'm just going to give\nthem to you in an array.",
    "start": "191360",
    "end": "197099"
  },
  {
    "text": "And I want you to\nassume that they're ordered in back-to-front order. So in other words, you have\nto add in the first circle",
    "start": "197100",
    "end": "206780"
  },
  {
    "text": "before you add in the next\ncircle and so on and so on. So that's the\nordering constraint",
    "start": "206780",
    "end": "212570"
  },
  {
    "text": "that actually makes this\na tough little problem. And just to give\nyou a sense of that,",
    "start": "212570",
    "end": "218900"
  },
  {
    "text": "here's an example of\nblue over green over red. So if you look\ncarefully, the blue",
    "start": "218900",
    "end": "226590"
  },
  {
    "text": "is the first piece of a filter. Here is if I combine them\ntogether in some other order",
    "start": "226590",
    "end": "234820"
  },
  {
    "text": "and you just get a\ndifferent result. So we're going to\ncall that wrong. So the name of the\ngame is, if you just",
    "start": "234820",
    "end": "241500"
  },
  {
    "text": "look at this nested\nsequence of loops, sequentially, this\nis a valid algorithm",
    "start": "241500",
    "end": "246840"
  },
  {
    "text": "for each circle in the order\nthat they are in the array. The thing that you\nwould probably do",
    "start": "246840",
    "end": "253590"
  },
  {
    "text": "and the code that we give you\nto start parallelizes this loop.",
    "start": "253590",
    "end": "259648"
  },
  {
    "text": "For every circle in parallel\non a different thread, just carry this out. And that will compute\nthe wrong answer, OK?",
    "start": "259649",
    "end": "266800"
  },
  {
    "text": "So step one is to\nconfirm to yourself that that computes the wrong\nanswer, and then make a change.",
    "start": "266800",
    "end": "272760"
  },
  {
    "text": "And when we talked in the\ndata-parallel thinking class, we talked through some\nvarious possibilities.",
    "start": "272760",
    "end": "278590"
  },
  {
    "text": "This is very similar for every\nparticle, what bin is it in. And the key thing\nto keep in mind",
    "start": "278590",
    "end": "284130"
  },
  {
    "text": "is that even though\nyou're supposed to process all these circles\nin the order that they appear,",
    "start": "284130",
    "end": "291060"
  },
  {
    "text": "the only way that you\ncan get the wrong answer is if you process in the\nwrong order and they overlap.",
    "start": "291060",
    "end": "299199"
  },
  {
    "text": "Because if they\ndon't overlap, it doesn't matter what\norder you process it in. So that's all I'm going to do\nto set up the problem right now.",
    "start": "299200",
    "end": "306720"
  },
  {
    "text": "But step one should\nalmost certainly be, change the code to\nget a correct answer.",
    "start": "306720",
    "end": "313660"
  },
  {
    "text": "And that correct answer will\nbe parallel but very slow. And then you're going to\nchip away at correct parallel",
    "start": "313660",
    "end": "320580"
  },
  {
    "text": "and fast. So that's the sequence\nof the assignment, OK? Or In other words, if\nyou knew for every pixel",
    "start": "320580",
    "end": "330780"
  },
  {
    "text": "what circles possibly overlapped\nit, you'd basically be done.",
    "start": "330780",
    "end": "337110"
  },
  {
    "text": "If you had this\nmagical data structure for every pixel\non screen, here's a list of circles\nthat might overlap it.",
    "start": "337110",
    "end": "343750"
  },
  {
    "text": "You would basically\nbe done because you know you can process every pixel\nin parallel and do that safely.",
    "start": "343750",
    "end": "349860"
  },
  {
    "text": "That's all I'll say\non the assignment. It's a fun one. I think most people\nreally, really like this.",
    "start": "349860",
    "end": "355190"
  },
  {
    "text": " Now, it's either a sarcastic\nlaugh or a laugh in agreement.",
    "start": "355190",
    "end": "363660"
  },
  {
    "text": "OK. So since [? Kunal ?] is\naway on some business travel today, the reason\nwhy efficiently",
    "start": "363660",
    "end": "371010"
  },
  {
    "text": "evaluating DNNs at this point\nin the lecture schedule, we're not continuing on\nwith some other stuff,",
    "start": "371010",
    "end": "376020"
  },
  {
    "text": "is largely just for scheduling. So this is a lecture that\nwill be very useful to you in assignment 4.",
    "start": "376020",
    "end": "381220"
  },
  {
    "text": "So it's coming a little bit\nearlier than it needs to be. But in general,\nI thought that it",
    "start": "381220",
    "end": "387240"
  },
  {
    "text": "might be fairly interesting\nto folks given that everybody is messing with various forms\nof deep neural networks today.",
    "start": "387240",
    "end": "393040"
  },
  {
    "text": "I'm going to talk a little\nbit about how we efficiently",
    "start": "393040",
    "end": "398520"
  },
  {
    "text": "optimize these things in\nthe context of running deep neural networks on\nmodern GPUs and CPUs.",
    "start": "398520",
    "end": "407130"
  },
  {
    "text": "I will hint at the very\nend of the day about all of the interesting specialized\nhardware for DNN acceleration",
    "start": "407130",
    "end": "416039"
  },
  {
    "text": "that is starting to emerge. But I'm going to save that\nto the lecture on specialized hardware, which I think is\nright before Thanksgiving,",
    "start": "416040",
    "end": "422797"
  },
  {
    "text": "if I remember. So today is going to be about\nmapping DNNs to GPUs and CPUs,",
    "start": "422797",
    "end": "429940"
  },
  {
    "text": "OK? Quick survey of what folks know. How many people have\ntaken a class, where",
    "start": "429940",
    "end": "435910"
  },
  {
    "text": "you have had to execute\nor implement or use modern deep networks?",
    "start": "435910",
    "end": "442160"
  },
  {
    "text": "That number keeps\ngoing up every year. So we're getting\ncloser and closer to almost everybody\nin the class.",
    "start": "442160",
    "end": "448840"
  },
  {
    "text": "How many people? There's actually another class\nthis quarter by a new faculty member here.",
    "start": "448840",
    "end": "453860"
  },
  {
    "text": "She's teaching this great\nclass called 229S, right? Do we have any\npeople in that class? Because there might\nbe some overlap here.",
    "start": "453860",
    "end": "460570"
  },
  {
    "text": "And so either this is\ngoing to be review, or this is going\nto be very helpful. We'll see, actually.",
    "start": "460570",
    "end": "467800"
  },
  {
    "text": "And so I probably need to\ncoordinate a little bit more next year, but I didn't know\nshe was teaching that class.",
    "start": "467800",
    "end": "473420"
  },
  {
    "text": "So anyways-- So I want to just start\nwith-- for those-- the small fraction\nof you that have not",
    "start": "473420",
    "end": "479530"
  },
  {
    "text": "seen any deep\nneural networks, I'm going to get into the\nworkload a little bit because I need to\nmake sure everybody's",
    "start": "479530",
    "end": "484725"
  },
  {
    "text": "familiar with the workload. And I want to do\nthat for two reasons. One is, I think I'd like to\ngive everybody a basis of how",
    "start": "484725",
    "end": "493270"
  },
  {
    "text": "to think about things. And really, we can think about\nthings in a systems class without much math at all.",
    "start": "493270",
    "end": "498289"
  },
  {
    "text": "And that's going\nto be very helpful. So here's a question. You've seen dependency\ngraphs before in this class.",
    "start": "498290",
    "end": "505090"
  },
  {
    "text": "We saw this when we talked\nabout superscalar execution. Or to be honest, if I\ngave you this task graph,",
    "start": "505090",
    "end": "510650"
  },
  {
    "text": "you would know how to\nexecute it right now. So what does this code do?",
    "start": "510650",
    "end": "516892"
  },
  {
    "text": "I mean, it's just an\nexpression, but let's look at it a little bit more carefully. It maps to basically\nwhat I've done here.",
    "start": "516892",
    "end": "524200"
  },
  {
    "text": "I'm basically computing\nthe product of some values",
    "start": "524200",
    "end": "530800"
  },
  {
    "text": "and I'm summing them. And if you look\ncarefully, that's actually just a dot\nproduct followed",
    "start": "530800",
    "end": "537160"
  },
  {
    "text": "by a max on the value\nof that dot product. And if you took an early\ndeep neural networks class,",
    "start": "537160",
    "end": "546920"
  },
  {
    "text": "you would see a diagram that\nlooks a little bit like this. They would call this a neuron\nin a modern deep neural network.",
    "start": "546920",
    "end": "552610"
  },
  {
    "text": "And at the end of\nthe day, it's just a circuit that performs\nan operation, like what I put on the previous slide,\nwhere inside this neuron,",
    "start": "552610",
    "end": "561470"
  },
  {
    "text": "there's a set of weights,\nwhich is just a vector. We don't need to think about\nthe meaning of those weights",
    "start": "561470",
    "end": "566770"
  },
  {
    "text": "at all at the moment. And there are some input,\nan input vector here, which is in this case a four vector.",
    "start": "566770",
    "end": "573589"
  },
  {
    "text": "And the output of this\nneuron, this neuron, you should just think as a\nfunction, which performs a dot product between two vectors.",
    "start": "573590",
    "end": "580300"
  },
  {
    "text": "Optionally, actually, it\nhas a bias or an addition of another vector. And then takes that through\nsome non-linear function,",
    "start": "580300",
    "end": "586630"
  },
  {
    "text": "and that doesn't really\nmatter at all either. And the function that I'm\ngoing to use in this class is called a max.",
    "start": "586630",
    "end": "592485"
  },
  {
    "text": "If you took a deep\nlearning class and wanted to feel\nbetter about yourself, you'd call it something like\na rectified linear unit.",
    "start": "592485",
    "end": "597920"
  },
  {
    "text": "But it's a max in this case, OK? So for the sake of\nthis conversation,",
    "start": "597920",
    "end": "603279"
  },
  {
    "text": "we can think about-- what we're\ninterested today is taking this overall computation, which\nis this nonlinear function f",
    "start": "603280",
    "end": "612870"
  },
  {
    "text": "on essentially a\nbig matrix algebra operation, in this\ncase, a dot product. And repeating that\nover and over and over.",
    "start": "612870",
    "end": "620380"
  },
  {
    "text": "So you can think,\nactually, about this neuron as actually being-- you\ncan think about every one of these little neurons as being\na little binary classifier,",
    "start": "620380",
    "end": "627550"
  },
  {
    "text": "if you want to take a machine\nlearning interpretation of that. If it's above 0,\nit's saying, yep. If it's below 0, saying no.",
    "start": "627550",
    "end": "634350"
  },
  {
    "text": "And now let's just\nstart wiring them up, because we're\ncomputer scientists. I have a little function, and\nI can wire a simple function up",
    "start": "634350",
    "end": "641010"
  },
  {
    "text": "into larger functions. And often, these\nnetworks are arranged",
    "start": "641010",
    "end": "648480"
  },
  {
    "text": "in a fairly regular\nway, and that I'll have some number of\nthese at every layer.",
    "start": "648480",
    "end": "654010"
  },
  {
    "text": "And the layers are\ndefined by saying that the output of\nthis layer turns into the inputs\nof the next layer.",
    "start": "654010",
    "end": "660240"
  },
  {
    "text": "And if you look carefully\nin my diagrams on the left, you see an example of a\nfully connected layer,",
    "start": "660240",
    "end": "665430"
  },
  {
    "text": "meaning that every single\noutput from layer i is an input to layer i plus 1.",
    "start": "665430",
    "end": "671840"
  },
  {
    "text": "On the diagram on\nthe right, you'll see something that's not\na fully connected layer. It's actually a\nconvolutional layer in 1D,",
    "start": "671840",
    "end": "679250"
  },
  {
    "text": "where there's this sliding\nwindow of every three inputs in this case.",
    "start": "679250",
    "end": "684780"
  },
  {
    "text": "Go to the next neuron. OK. All right. So let's write this a\nlittle bit differently.",
    "start": "684780",
    "end": "691290"
  },
  {
    "text": "Let's write this instead\nof these visual diagrams. Let's write this as some more\nin a linear algebra notation.",
    "start": "691290",
    "end": "699170"
  },
  {
    "text": "And just keep in mind that--\nsee here I have four neurons. So I have four sets of--\nor I guess, in this case,",
    "start": "699170",
    "end": "707490"
  },
  {
    "text": "I have three--  yeah, I have four neurons. And so I have these different\nsets of three weights.",
    "start": "707490",
    "end": "714270"
  },
  {
    "text": "So in this case, my\nneuron has actually got three weights in it. Excuse me, my original\ndiagram had four, but this is a neuron\nwith three weights in it.",
    "start": "714270",
    "end": "720820"
  },
  {
    "text": "So for every one of\nthese four gray boxes, there are three weights. The input-- notice that\nthere are three input values.",
    "start": "720820",
    "end": "727029"
  },
  {
    "text": "That's the right-hand\nside vector x. And then the execution\nof computing the output",
    "start": "727030",
    "end": "732210"
  },
  {
    "text": "is just a matrix-vector\nproduct here. And of course, that goes\nthrough that non-linearity f",
    "start": "732210",
    "end": "739199"
  },
  {
    "text": "in this case. So all of these fancy\ndiagrams start boiling down",
    "start": "739200",
    "end": "744360"
  },
  {
    "text": "to dense matrix\nalgebra pretty quickly, if you want to think\nabout it like that.",
    "start": "744360",
    "end": "750029"
  },
  {
    "text": "Another way you can think\nabout the exact same thing is more like this.",
    "start": "750030",
    "end": "755310"
  },
  {
    "text": "So this is a convolution. So if you look carefully\nat this C code,",
    "start": "755310",
    "end": "761440"
  },
  {
    "text": "there's an input image,\nwhich is width by height. It's actually width by\nheight, width plus 2,",
    "start": "761440",
    "end": "766510"
  },
  {
    "text": "just for some\nboundary conditions. The output is a width\nby height image. I have these weights.",
    "start": "766510",
    "end": "772110"
  },
  {
    "text": "And for every pixel\nof the output, I do something that involves\nsurrounding pixels of the input.",
    "start": "772110",
    "end": "778500"
  },
  {
    "text": "And if you take a\nlook at this code and I asked you,\nWhat does it do? some of you might say, oh,\nit performs a 2D convolution.",
    "start": "778500",
    "end": "786020"
  },
  {
    "text": "And then I turn\nright back around and you go, well, what\nthe heck does that mean? And so if this was, let's\nsay, an input image,",
    "start": "786020",
    "end": "793220"
  },
  {
    "text": "what is the output\nimage look like? Blurry? It's blurry, right?",
    "start": "793220",
    "end": "798480"
  },
  {
    "text": "Because every output pixel\nis essentially the average of the surrounding pixels.",
    "start": "798480",
    "end": "804450"
  },
  {
    "text": "So imagine a situation\nwhere I had a white input pixel and really dark-colored\nsurrounding pixels.",
    "start": "804450",
    "end": "810780"
  },
  {
    "text": "The convolution is going to\nbring that white pixel down to the average. And around that\npixel, it's going",
    "start": "810780",
    "end": "816110"
  },
  {
    "text": "to bring those black pixels\nup to the average, right? So if I run this code on an\nimage that looks like this,",
    "start": "816110",
    "end": "822960"
  },
  {
    "text": "you'll get an image\nthat looks like that. That's one example\nof a convolution, OK?",
    "start": "822960",
    "end": "828980"
  },
  {
    "text": "All right. So that was our 3D\nimage convolution. And I just want you to, in your\nmind, think through this image.",
    "start": "828980",
    "end": "837390"
  },
  {
    "text": "If we think about every one\nof these outputs, pixels as a neuron, its output\nis a weighted combination",
    "start": "837390",
    "end": "845430"
  },
  {
    "text": "of nine inputs here. And those nine\ninputs, in this case, are weighted by these fixed\nweights, one-ninth, one-ninth,",
    "start": "845430",
    "end": "852940"
  },
  {
    "text": "one-ninth. And that's why I drew this\ndiagram that looks like this. Now, just keep in mind that\nwhen I drew it like this,",
    "start": "852940",
    "end": "860710"
  },
  {
    "text": "I'm showing you three\ninputs into every one. In 2D, it would be nine\ninputs into every one.",
    "start": "860710",
    "end": "867540"
  },
  {
    "text": "So if this was\nactually an image, there would be nine inputs. And so I didn't draw a 2D\nversion of a convolution",
    "start": "867540",
    "end": "873120"
  },
  {
    "text": "in my little [? DAGs. ?]\nI drew 1D, OK? And If you took a computer\ngraphics or computer vision",
    "start": "873120",
    "end": "879570"
  },
  {
    "text": "class, you would\nlearn that there are-- if I change these\nweights, I actually",
    "start": "879570",
    "end": "885420"
  },
  {
    "text": "get very different outputs. And so averaging\nmade a lot of sense, but imagine I changed\nsome of these weights",
    "start": "885420",
    "end": "891089"
  },
  {
    "text": "and make some of them negative. Now, all of a sudden, what\nis this computation doing? Instead of averaging\nevery pixel, it's saying,",
    "start": "891090",
    "end": "899500"
  },
  {
    "text": "I want some positive\nscaling times. These pixels over here minus the\nvalue of these pixels over here.",
    "start": "899500",
    "end": "907420"
  },
  {
    "text": "And now, all of a sudden, that's\na finite difference computation. That's a gradient computation. So these different weights,\nall of a sudden, my convolution",
    "start": "907420",
    "end": "916920"
  },
  {
    "text": "is not blurring the image. It's, in fact,\ndoing things like, huh, it's detecting\nhorizontal gradients,",
    "start": "916920",
    "end": "922889"
  },
  {
    "text": "or huh, it's detecting\nvertical gradients. And so all of these, at\nleast in image processing--",
    "start": "922890",
    "end": "929633"
  },
  {
    "text": "In the front half\nof the talk, I'm going to focus on these\nconv layers, which are common in image processing. In the back half\nof the talk, I'm",
    "start": "929633",
    "end": "936030"
  },
  {
    "text": "going to bring up\nattention and transformers. But all these ConvNets are a\nbunch of these convolutions,",
    "start": "936030",
    "end": "943620"
  },
  {
    "text": "largely, except the weights\nare not given by me. They are learned.",
    "start": "943620",
    "end": "949620"
  },
  {
    "text": "So here's an example\nof the classic ImageNet weights from a long time ago.",
    "start": "949620",
    "end": "955470"
  },
  {
    "text": "Here are the weights of a whole\nbunch of different filters visualized.",
    "start": "955470",
    "end": "961430"
  },
  {
    "text": "So this is an 11 by\n11 convolution now. And so this is a\nconvolution that",
    "start": "961430",
    "end": "967610"
  },
  {
    "text": "takes 121 elements of the\ninput for every output. And there's 96 different\nversions of these convolutions.",
    "start": "967610",
    "end": "974750"
  },
  {
    "text": "And so if I take these\nfilters by this input image, you get these different outputs.",
    "start": "974750",
    "end": "980495"
  },
  {
    "text": "You can just say that\nthese different filters are lighting up on different\nparts of the image. They're like detecting\ndifferent features.",
    "start": "980495",
    "end": "988209"
  },
  {
    "text": "And so I want you to think\nabout this [? computation ?] as taking this code here\nand then repeating it",
    "start": "988210",
    "end": "994670"
  },
  {
    "text": "for a whole bunch of different\nfilters with different weights. That's the essence of what\nwe're going to do here",
    "start": "994670",
    "end": "1002380"
  },
  {
    "text": "on something like this. And a common way to\nvisualize this stuff is, think about these\nn-dimensional tensors.",
    "start": "1002380",
    "end": "1008480"
  },
  {
    "text": "So imagine you had an input\nimage that's width by height. So it's width by height by 1.",
    "start": "1008480",
    "end": "1014710"
  },
  {
    "text": "And then I imagine I have all\nof these different filters that are just like--",
    "start": "1014710",
    "end": "1019730"
  },
  {
    "text": "I just showed you 3 by 3 matrix\nof values and stuff like that. So I have these 3 by 3 filters.",
    "start": "1019730",
    "end": "1025530"
  },
  {
    "text": "And if I'm going to do\nnum_filters of them, just for convenience, I'm\ngoing to stack them together in a tensor.",
    "start": "1025530",
    "end": "1030810"
  },
  {
    "text": "So it's 3 by 3 by\nnumber of these filters. And so if I do num_filters\nconvolutions of this,",
    "start": "1030810",
    "end": "1037829"
  },
  {
    "text": "I should get width by height by\nnum_filters amount of output. ",
    "start": "1037829",
    "end": "1043970"
  },
  {
    "text": "OK? And then we're just going\nto repeat this process over and over and over, producing\na bunch of filters,",
    "start": "1043970",
    "end": "1052159"
  },
  {
    "text": "taking it through\nthat function f, which is setting everything to-- clamping at 0, and then\nmaybe actually doing",
    "start": "1052160",
    "end": "1059600"
  },
  {
    "text": "a few other calculations. Like, in this case, a max pool\nis largely just a downsample",
    "start": "1059600",
    "end": "1065090"
  },
  {
    "text": "and so on and so on. And when you see all these fancy\ndiagrams like this, this is--",
    "start": "1065090",
    "end": "1073435"
  },
  {
    "text": "all these is-- just\neach one of these blocks is largely this sequence\nof a bunch of convolutions,",
    "start": "1073436",
    "end": "1081029"
  },
  {
    "text": "or that's the computational\nmeat of it, wired up in very different ways.",
    "start": "1081030",
    "end": "1086570"
  },
  {
    "text": "So here I have ResNet. Here I have a common\nU-Net architecture where everything decimates,\nand then it comes back.",
    "start": "1086570",
    "end": "1093860"
  },
  {
    "text": "That's Inception from Google. OK. So at this point, all I\nreally want you to know",
    "start": "1093860",
    "end": "1102620"
  },
  {
    "text": "is that it's important\nto be able to perform a bunch of convolutions\non a bunch of--",
    "start": "1102620",
    "end": "1109400"
  },
  {
    "text": "on an input image,\nand that produces a bunch of output images. And then we're going to\nperform convolutions on those.",
    "start": "1109400",
    "end": "1117020"
  },
  {
    "text": "And that's the workload that\nI want you to know about. This is the point where I'd be\nhappy to take a question or two,",
    "start": "1117020",
    "end": "1123080"
  },
  {
    "text": "because a lot of you probably\nknow a lot of machine learning, and you maybe have\nsomething-- you may take issue",
    "start": "1123080",
    "end": "1129269"
  },
  {
    "text": "with any of my simplifications\nor something like that. OK? Is it OK to keep going?",
    "start": "1129270",
    "end": "1136050"
  },
  {
    "text": "All right. So first of all, now let's\nthink about how we are going",
    "start": "1136050",
    "end": "1141270"
  },
  {
    "text": "to make this stuff faster. And this is a good point\nin this course where",
    "start": "1141270",
    "end": "1146760"
  },
  {
    "text": "I'd like to talk through,\nwhat are our avenues? And there's three avenues\nof making this faster.",
    "start": "1146760",
    "end": "1153090"
  },
  {
    "text": "One way is, we take\nmachine-learning classes, and we think about\nthese architectures,",
    "start": "1153090",
    "end": "1159120"
  },
  {
    "text": "and we design better ones. For example, there's a lot fewer\nflops in ResNet and Inception",
    "start": "1159120",
    "end": "1165210"
  },
  {
    "text": "than in some of the original\nconv layers that made it big or the [? ConvNets ?]\nthat made it",
    "start": "1165210",
    "end": "1170519"
  },
  {
    "text": "big early in this modern\ndeep learning wave. So significant algorithmic\ntopology design improvements",
    "start": "1170520",
    "end": "1178410"
  },
  {
    "text": "reduce the amount of memory\nneeded, amount of flops needed, and so on and so on.",
    "start": "1178410",
    "end": "1184650"
  },
  {
    "text": "So that's one way\nwe can go, and that has nothing to do\nwith the skills",
    "start": "1184650",
    "end": "1190100"
  },
  {
    "text": "that I teach you in this class. That would be, you go take\na machine-learning class and you say, you know\nwhat, the way SGD performs,",
    "start": "1190100",
    "end": "1198000"
  },
  {
    "text": "I feel like there's some value\nin transferring information around a couple of layers\nwith [? Skip ?] connections,",
    "start": "1198000",
    "end": "1203820"
  },
  {
    "text": "which was one of the big\ninnovations of ResNet. Or I actually want-- I don't really want a\nlot of convolutions.",
    "start": "1203820",
    "end": "1211350"
  },
  {
    "text": "I only need a couple, but\nI have to wire them up in this particular way. So I'm showing you\nan example from--",
    "start": "1211350",
    "end": "1221150"
  },
  {
    "text": "yeah, that's from the\nInception-ResNet block. They're like-- if\nyou think about it, you say, well, what\ndo I need here?",
    "start": "1221150",
    "end": "1228600"
  },
  {
    "text": "I feel like I want a\ncouple of paths of 3 by 3 convolutions to do this amount,\nfeatures of this spatial extent.",
    "start": "1228600",
    "end": "1235799"
  },
  {
    "text": "I need a few cascades of them. Maybe I might have one path that\nhas features of a wider extent.",
    "start": "1235800",
    "end": "1241440"
  },
  {
    "text": "If you're a designer\nof these networks, you think about those\nsorts of things. And if you're a designer\nof those networks at Google",
    "start": "1241440",
    "end": "1247580"
  },
  {
    "text": "and they say, make these\nthings fit on a mobile phone, you hang around for two years\ntrying various combinations",
    "start": "1247580",
    "end": "1253470"
  },
  {
    "text": "of this stuff and come up with\nan architecture that looks a little-- this is a MobileNet.",
    "start": "1253470",
    "end": "1258840"
  },
  {
    "text": "And this is getting-- we're back in 2017 and stuff. So this is starting\nto get old now.",
    "start": "1258840",
    "end": "1265059"
  },
  {
    "text": "But the point was that there\nwas a small team, a person at Google, that tried\ndifferent combinations of,",
    "start": "1265060",
    "end": "1270630"
  },
  {
    "text": "I need this many filters,\nand I need this many layers, and so on and so on. And they came up\nwith this design.",
    "start": "1270630",
    "end": "1276760"
  },
  {
    "text": "If you look at it, here's the\nsize of the filters, 3 by 3 versus 1 by 1. Here's the number\nof those filters.",
    "start": "1276760",
    "end": "1283840"
  },
  {
    "text": "And if every single time\nyou do a downsample, this is the output tensor\nsizes of all of those layers.",
    "start": "1283840",
    "end": "1291617"
  },
  {
    "text": "And first of all, actually,\nif you take a look at this, is there anything alarming? ",
    "start": "1291618",
    "end": "1298650"
  },
  {
    "text": "Given what you\nknow in this class? Yeah. That's a lot of work\nfor a small phone.",
    "start": "1298650",
    "end": "1305919"
  },
  {
    "text": "Like, just-- Oh, yeah. I thought you meant figuring\nthis out with a lot of work. And I'm like, yeah, it was. And they tried to\nautomate this guy later.",
    "start": "1305920",
    "end": "1312070"
  },
  {
    "text": "But that's a different story. Well, I mean, the couple\nof things that I see is, first of all, if you look\nat every one of these layers",
    "start": "1312070",
    "end": "1317600"
  },
  {
    "text": "and you look at the output size,\nat first, you might look at, well, it's just\na 28 by 28 image.",
    "start": "1317600",
    "end": "1323059"
  },
  {
    "text": "That's pretty tiny. It should be nothing. And then if you think about--\ninstead of red, green, and blue at every point, you think\nabout there actually being",
    "start": "1323060",
    "end": "1329917"
  },
  {
    "text": "256 channels or 512 channels. You're like, there's\nquite a bit of output between each of these layers.",
    "start": "1329917",
    "end": "1336620"
  },
  {
    "text": "That's not fitting\nin cash anymore. The other thing that should\nbe a little surprising to you is just like, crap, 7 by 7.",
    "start": "1336620",
    "end": "1344767"
  },
  {
    "text": "How the heck are you going\nto [? syndeize ?] this thing, right? These are some\nweird numbers here to actually make fit in\na modern 70 processor.",
    "start": "1344767",
    "end": "1352563"
  },
  {
    "text": "So it very well could\nhave been the case that you might as well made\nthat 8 by 8 because you would have gotten a\nbunch of stuff for free.",
    "start": "1352563",
    "end": "1358163"
  },
  {
    "text": "But anyways-- OK. So the point of this is, first\nof all, a lot of systems classes",
    "start": "1358163",
    "end": "1364680"
  },
  {
    "text": "take the architecture that\nis the best architecture of the day as a given.",
    "start": "1364680",
    "end": "1370289"
  },
  {
    "text": "And say, let's try\nand optimize that. And I just want\nyou to keep in mind that on the algorithm\nside of the fence,",
    "start": "1370290",
    "end": "1377440"
  },
  {
    "text": "there's constantly massive\ninnovation and iteration. So here are some examples.",
    "start": "1377440",
    "end": "1383260"
  },
  {
    "text": "This is again from\nimage classification, the task that got a lot\nof this started, at least.",
    "start": "1383260",
    "end": "1388990"
  },
  {
    "text": "And these are different\nnetworks that people designed in the research. And you can think about--",
    "start": "1388990",
    "end": "1395730"
  },
  {
    "text": "so basically, they're getting\nmore accurate over time. So the y-axis here is accuracy,\nhow well they classify objects",
    "start": "1395730",
    "end": "1402990"
  },
  {
    "text": "in an image or\nwhat's in an image. This graph down\nhere is accuracy,",
    "start": "1402990",
    "end": "1408809"
  },
  {
    "text": "like 90%, 91% per\nflop, per expense.",
    "start": "1408810",
    "end": "1414930"
  },
  {
    "text": "And this is where we\nwere in the early days. And these networks\nover here may not",
    "start": "1414930",
    "end": "1421110"
  },
  {
    "text": "perform all that much better. Some of them perform better,\nbut this VGG-19 is already",
    "start": "1421110",
    "end": "1426210"
  },
  {
    "text": "all the way over here. These better topologies are\nactually efficiency, right?",
    "start": "1426210",
    "end": "1432480"
  },
  {
    "text": "And that plot over there is\nactually just like a 2D diagram of accuracy per unit cost.",
    "start": "1432480",
    "end": "1438010"
  },
  {
    "text": "So you want to be on the\ntop left of that diagram. ",
    "start": "1438010",
    "end": "1443328"
  },
  {
    "text": "And so if I look at just\na couple of these things, I pull them out and\nI pulled them out from basically a\nchange every year,",
    "start": "1443328",
    "end": "1451830"
  },
  {
    "text": "the accuracy was largely, at\nthis time, about all the same.",
    "start": "1451830",
    "end": "1457390"
  },
  {
    "text": "But if you look in both\nthe number of weights, so the number of\nfilters more or less",
    "start": "1457390",
    "end": "1463720"
  },
  {
    "text": "and the size of those\nfilters, there's a dramatic drop in\nthe amount of memory",
    "start": "1463720",
    "end": "1470010"
  },
  {
    "text": "that's needed to\nstore this model. And there's almost\na commensurate drop in the amount of computation\nmath needed to perform it.",
    "start": "1470010",
    "end": "1478080"
  },
  {
    "text": "So that's about 25x from\nalgorithms in a span of about four years.",
    "start": "1478080",
    "end": "1483700"
  },
  {
    "text": "Hardware is not going to get\n25 times faster in those four years.",
    "start": "1483700",
    "end": "1488820"
  },
  {
    "text": "So if you had designed an\nalgorithm for this network,",
    "start": "1488820",
    "end": "1494259"
  },
  {
    "text": "you would have been-- you\nmight have been pretty far off what is the right\nalgorithm for this network.",
    "start": "1494260",
    "end": "1499710"
  },
  {
    "text": "OK. So I just want to\nstart by making sure everybody understands\nthat that's out there. And so if you are a good machine\nlearning systems engineer,",
    "start": "1499710",
    "end": "1508030"
  },
  {
    "text": "you have to be\ntracking the state of the art and these things. With these big, large language\nmodels and transformers,",
    "start": "1508030",
    "end": "1515590"
  },
  {
    "text": "there's this huge push to\nbuild bigger and bigger because of the scaling laws, right?",
    "start": "1515590",
    "end": "1520840"
  },
  {
    "text": "And there's definitely\na reason for that. But at some level\nof quality, there's all these chip\ncompanies out there",
    "start": "1520840",
    "end": "1526710"
  },
  {
    "text": "that were building these big\nchips because they're like, well, today's LLM is huge. And what's been going on in the\nlast six months or the last 12",
    "start": "1526710",
    "end": "1534390"
  },
  {
    "text": "months with [? Lama ?] and\nall these other things, it's about once\nyou get something that works to your satisfaction,\nthe engineers can come in",
    "start": "1534390",
    "end": "1541500"
  },
  {
    "text": "or the ML engineers can come in\nand they can shrink the hell out of these things. So it makes a complicated design\nproblem of what should you",
    "start": "1541500",
    "end": "1549030"
  },
  {
    "text": "be designing your systems for.  Now, approach two is highly\nrelevant to us in this class.",
    "start": "1549030",
    "end": "1559010"
  },
  {
    "text": "At some point, you say,\nthis is the architecture, this is the topology\nthat we're going with, and we need to make it execute\nwell on the machine that I have.",
    "start": "1559010",
    "end": "1568577"
  },
  {
    "text": "And in this class, the\nmachine that you have would be a multi-core\nCPU or GPU.",
    "start": "1568577",
    "end": "1574698"
  },
  {
    "text": "And I don't mean in this class. I mean, 149 in today's lecture. I mean, a multi-core CPU.",
    "start": "1574698",
    "end": "1580140"
  },
  {
    "text": "So here's me taking my 3\nby 3 convolution slide. I'm just changing the\nC code a little bit",
    "start": "1580140",
    "end": "1587330"
  },
  {
    "text": "to talk a little bit more\nabout what's going on. So first of all, it\nused to be four loops.",
    "start": "1587330",
    "end": "1592530"
  },
  {
    "text": "Now it's seven. So it's four--\nlet's look at the i and j first for every output\npixel, for every filter.",
    "start": "1592530",
    "end": "1604940"
  },
  {
    "text": "For every filter,\nwe need to iterate over the input pixels i and j.",
    "start": "1604940",
    "end": "1610490"
  },
  {
    "text": "And also-- OK, so\none thing I haven't said that its input\nfilter-- we're iterating over all input pixels, i and j.",
    "start": "1610490",
    "end": "1616950"
  },
  {
    "text": "But that input image, the input\nhas multiple channels of depth.",
    "start": "1616950",
    "end": "1622539"
  },
  {
    "text": "So you're actually\niterating over all pixels and all RGB or all 512\nchannels and so on and so on.",
    "start": "1622540",
    "end": "1631390"
  },
  {
    "text": "So this is the convolution. It's a 3D convolution, largely,\nor it's a 2D convolution on a 3D",
    "start": "1631390",
    "end": "1637230"
  },
  {
    "text": "input for every filter,\nfor every output pixel. And then the outermost loop is\nthe loop over batches of images.",
    "start": "1637230",
    "end": "1644830"
  },
  {
    "text": "So often, we'll actually\ndo multiple things through the network at once. That's your batch size.",
    "start": "1644830",
    "end": "1649840"
  },
  {
    "text": "OK, so seven loops here. 1, 2, 3, 4, 5, 6, 7. And this wouldn't be\nhard to implement at all.",
    "start": "1649840",
    "end": "1656580"
  },
  {
    "text": "You could just go\ncode this thing up. Code it up and see. It probably wouldn't\nperform that well,",
    "start": "1656580",
    "end": "1661630"
  },
  {
    "text": "but it would be a\ncorrect implementation. OK. So now it comes into the\nassociation between convolutions",
    "start": "1661630",
    "end": "1673670"
  },
  {
    "text": "and vector algebra. So let me show you a quick hack.",
    "start": "1673670",
    "end": "1678890"
  },
  {
    "text": "So you can go to NumPy, you can\ngo to your favorite library, and you can call matrix\nmatrix multiplication.",
    "start": "1678890",
    "end": "1688610"
  },
  {
    "text": "And you assume that, whether\nit be MATLAB or Torch or NumPy. If you call matrix\nmatrix multiplication,",
    "start": "1688610",
    "end": "1695880"
  },
  {
    "text": "someone has taken the\ntime to implement it well. So there's this reduction.",
    "start": "1695880",
    "end": "1701300"
  },
  {
    "text": "If you can boil something\ndown to matrix operations, maybe you're in\nreasonable shape.",
    "start": "1701300",
    "end": "1706700"
  },
  {
    "text": "So it turns out that\nit's really easy to think about a convolution\nas a matrix-multiply.",
    "start": "1706700",
    "end": "1712842"
  },
  {
    "text": "And let me tell\nyou how to do that. And this is actually how some\nof the earliest conv layer implementations were implemented\nback in the days of system",
    "start": "1712842",
    "end": "1720650"
  },
  {
    "text": "of [? pre ?] [? coding ?] and\nend systems and stuff like that. So imagine that I have\nmy input image here.",
    "start": "1720650",
    "end": "1727890"
  },
  {
    "text": "This is my input\nimage, x and y-- every x and y. And imagine I have\nmy convolution.",
    "start": "1727890",
    "end": "1734580"
  },
  {
    "text": "And let's say it's a\n3 by 3 convolution. So there should be nine weights.",
    "start": "1734580",
    "end": "1739830"
  },
  {
    "text": "Makes sense? And then what I'm going\nto do is I'm going to say, well, every output pixel is just\nthe dot product of these weights",
    "start": "1739830",
    "end": "1748230"
  },
  {
    "text": "and some subset of\nthe input pixels. So I'm going to copy\nthe appropriate input",
    "start": "1748230",
    "end": "1753990"
  },
  {
    "text": "pixels into a matrix here\nthat is width by height rows",
    "start": "1753990",
    "end": "1760020"
  },
  {
    "text": "and nine elements across. OK, so this is nine times\nbigger than the original image.",
    "start": "1760020",
    "end": "1767280"
  },
  {
    "text": "Can you see that? So I'm just going\nto copy data in so that the output pixel,\nthe output row, or the dot",
    "start": "1767280",
    "end": "1775080"
  },
  {
    "text": "product of this row and\nthis row is the first pixel of the convolution. And so what I'm showing\nyou here on the slide",
    "start": "1775080",
    "end": "1781350"
  },
  {
    "text": "is what values, how I\nwould copy values in. And then let's go ahead and\nthink about the next row.",
    "start": "1781350",
    "end": "1787860"
  },
  {
    "text": "And I'd copy these values. And it might be the most clear,\nif I get way down here, where there's no boundary conditions.",
    "start": "1787860",
    "end": "1794090"
  },
  {
    "text": "And so if I'm going to produce\npixel at [? 1/1 ?] coordinate of output pixel, I'm going to\nneed those nine pixels from",
    "start": "1794090",
    "end": "1803030"
  },
  {
    "text": "the input. So I just copied\nthem in right here. And the output of\nthe convolution is just going to be\nthe dot product there.",
    "start": "1803030",
    "end": "1812390"
  },
  {
    "text": "So I hope you can\nsee at this point that with the appropriate\npadding and data duplication,",
    "start": "1812390",
    "end": "1818250"
  },
  {
    "text": "I can implement convolution\nas a matrix-vector product.",
    "start": "1818250",
    "end": "1823970"
  },
  {
    "text": "Right? So check your understanding. Now imagine I want to do a\nconvolution with not just",
    "start": "1823970",
    "end": "1829820"
  },
  {
    "text": "one filter, but I want to do a\nconvolution with many filters.",
    "start": "1829820",
    "end": "1834889"
  },
  {
    "text": "How does my setup change? Yeah. So this vector, these\ndifferent-- this column vector,",
    "start": "1834890",
    "end": "1843000"
  },
  {
    "text": "which is the weights\nof one convolution, I would just stack more\nand more columns there, and it becomes a matrix.",
    "start": "1843000",
    "end": "1849340"
  },
  {
    "text": "So if I wanted to do multiple\nconvolutions against the input image at once, I just copy the\nimage into this left-hand side",
    "start": "1849340",
    "end": "1857580"
  },
  {
    "text": "matrix. My weights are\njust sitting there as a number of filters columns.",
    "start": "1857580",
    "end": "1862840"
  },
  {
    "text": "And now I have matrix\nmatrix products. So if you give me a matrix\nmatrix multiplication library,",
    "start": "1862840",
    "end": "1870640"
  },
  {
    "text": "I know how to populate\nthis matrix in order to produce the output.",
    "start": "1870640",
    "end": "1876850"
  },
  {
    "text": "OK, one more check\nyour understanding. Imagine that the input tensor is\nnot just a single channel image.",
    "start": "1876850",
    "end": "1885070"
  },
  {
    "text": "So here at every xy,\nthere's only one value. Imagine that it has\nmultiple values.",
    "start": "1885070",
    "end": "1890590"
  },
  {
    "text": "It's a multi-dimensional tensor. Let's say it has\n512 values here.",
    "start": "1890590",
    "end": "1896520"
  },
  {
    "text": "How do I change my computation? Because now remember,\nevery convolution",
    "start": "1896520",
    "end": "1902940"
  },
  {
    "text": "is going to be 3 by 3 by 512. How does this set up change?",
    "start": "1902940",
    "end": "1909266"
  },
  {
    "text": "Yeah. [INAUDIBLE] towards\nlooking at [INAUDIBLE]. OK. So this is going to\nbe multiplied by 512,",
    "start": "1909266",
    "end": "1916190"
  },
  {
    "text": "and I need weights for 3 by\n3 by 512 to go down here. So I'm going to get much,\nmuch bigger matrices.",
    "start": "1916190",
    "end": "1922970"
  },
  {
    "text": "It'll look a little bit\nlike this, if there were three channels instead of 512.",
    "start": "1922970",
    "end": "1928679"
  },
  {
    "text": "This is RGB, let's say. It would look like this. OK? ",
    "start": "1928680",
    "end": "1934571"
  },
  {
    "text": "So now we have-- if you give me the fastest\nmatrix multiplication on the planet, you give me--",
    "start": "1934571",
    "end": "1944330"
  },
  {
    "text": "I can produce inputs for you,\nor I use that fastest matrix",
    "start": "1944330",
    "end": "1949460"
  },
  {
    "text": "multiplication on\nthe planet to execute one of these conv layers. And so here's a\nslide or a figure",
    "start": "1949460",
    "end": "1956120"
  },
  {
    "text": "that I stole from\nNVIDIA, where they talk about this transformation.",
    "start": "1956120",
    "end": "1961470"
  },
  {
    "text": "So on the left-hand side are\nthe values in your tensors,",
    "start": "1961470",
    "end": "1966492"
  },
  {
    "text": "like what you'd think of\nif you were using Torch. So I have an input\ntensor X, which",
    "start": "1966492",
    "end": "1971669"
  },
  {
    "text": "is width by height with C\nchannels and batch size N. Well, that means I need a\nweight matrix of C channels.",
    "start": "1971670",
    "end": "1980980"
  },
  {
    "text": "R and S is the 3 by 3,\nthe size of the thing. And then I have\nK filters, right?",
    "start": "1980980",
    "end": "1986910"
  },
  {
    "text": "Yeah, exactly. So this is like common\nparameters for your conv layer.",
    "start": "1986910",
    "end": "1993040"
  },
  {
    "text": "And just notice that this\nconverts, using what I told you on the last slide, the\nmatrices, A, B, and C,",
    "start": "1993040",
    "end": "1999450"
  },
  {
    "text": "that have dimensions that\nare the products of some of those parameters.",
    "start": "1999450",
    "end": "2005120"
  },
  {
    "text": "So this is what you would do,\nif I gave you a fast matrix multiplication library.",
    "start": "2005120",
    "end": "2011929"
  },
  {
    "text": "And so that matrix product,\nA times B equals C, C contains the\nresult of convolving",
    "start": "2011930",
    "end": "2019850"
  },
  {
    "text": "with K filters, an input that is\nwidth by height by C to produce",
    "start": "2019850",
    "end": "2026929"
  },
  {
    "text": "an output that is P by Q by\nN. Pausing for questions.",
    "start": "2026930",
    "end": "2039725"
  },
  {
    "text": "OK. And as you can imagine, pick\nyour favorite computer-- Intel, NVIDIA GPUs,\nincreasingly AMD GPUs.",
    "start": "2039725",
    "end": "2046400"
  },
  {
    "text": "And you can go download a\nlibrary that does fast matrix multiplication. And you're not going to\nwrite it any faster than them",
    "start": "2046400",
    "end": "2052658"
  },
  {
    "text": "because they have their\ncrazy hackers working on this at all times. So that's one way to do this.",
    "start": "2052659",
    "end": "2060290"
  },
  {
    "text": "But maybe we should talk very\nbriefly about how the heck would you even begin to implement\nthis matrix multiplication",
    "start": "2060290",
    "end": "2066310"
  },
  {
    "text": "if you wanted to be one of those\ncrazy hackers that worked all the time on these libraries. So here's matrix multiplication.",
    "start": "2066310",
    "end": "2073512"
  },
  {
    "text": "It's actually much\nsimpler, right? I showed you a seven-loop nest. Matrix multiplication is just\na simple three-loop nest.",
    "start": "2073512",
    "end": "2080149"
  },
  {
    "text": "And some things to\nthink about are like, in this case, I have an M by K\nmatrix by a K by N matrix.",
    "start": "2080150",
    "end": "2088750"
  },
  {
    "text": "For simplicity, let's just say\nall the dimensions are N for now so we can think through this. So if all the dimensions\nare N, how much data",
    "start": "2088750",
    "end": "2094989"
  },
  {
    "text": "do we end up touching? 3 times N squared.",
    "start": "2094989",
    "end": "2100890"
  },
  {
    "text": "And how much work do we do? N cubed. N cubed. Correct, because there's\nthree loops over N.",
    "start": "2100890",
    "end": "2106339"
  },
  {
    "text": "So the problem with\nthis implementation-- let's think about it for large\nN because we set this thing up.",
    "start": "2106340",
    "end": "2113690"
  },
  {
    "text": "N can be quite large because\nN might be 512 times 7 by 7 or something like that.",
    "start": "2113690",
    "end": "2119000"
  },
  {
    "text": "These might be\ngigabyte-sized matrices. Knowing what you know\nabout how computers work,",
    "start": "2119000",
    "end": "2125609"
  },
  {
    "text": "what do you see as\nthe problem if I think about this innermost\nloop of matrix-multiply?",
    "start": "2125610",
    "end": "2132550"
  },
  {
    "text": "It's reading that row of A and\nthat column of B to output one, element of C. Yeah.",
    "start": "2132550",
    "end": "2138549"
  },
  {
    "text": "[INAUDIBLE] but not\nbeing able to reuse that.",
    "start": "2138550",
    "end": "2145960"
  },
  {
    "text": "OK. Well, and the story is\na little bit different. So the statement,\nwhich I agree with, is we're going to get\nvery bad cache locality.",
    "start": "2145960",
    "end": "2153407"
  },
  {
    "text": "Well, let's think about\nthat in more detail. So we are reading across\nA across that row.",
    "start": "2153407",
    "end": "2159760"
  },
  {
    "text": "Are we particularly\nupset about that? Not really, because it's like\nthe most coherent memory access",
    "start": "2159760",
    "end": "2165820"
  },
  {
    "text": "you can make. B, we're jumping\nall around memory because B is-- if this\nis row major data,",
    "start": "2165820",
    "end": "2172910"
  },
  {
    "text": "every element you\ntouch is going to be far away in the address space\nand on a unique cache line.",
    "start": "2172910",
    "end": "2179050"
  },
  {
    "text": "And C is actually as\nbest as it possibly could be because I'm\njust reading and writing the same value that's\ngoing to sit in cache.",
    "start": "2179050",
    "end": "2185830"
  },
  {
    "text": "But if you take a step back and\nthink about your bandwidth bound problem on your homework,\nthis is actually not",
    "start": "2185830",
    "end": "2192130"
  },
  {
    "text": "much better than that bandwidth\nbound problem on the homework, because we're\ngoing to read an A, we're going to read a B. The\naccess to B is pretty terrible.",
    "start": "2192130",
    "end": "2200390"
  },
  {
    "text": "And then we're\njust adding into C. So you're really only doing\none math [? op. ?] Multiply--",
    "start": "2200390",
    "end": "2206390"
  },
  {
    "text": "or two math [? ops ?]\nand multiply, add, yet you're reading two values. So this is your\nclassic bandwidth",
    "start": "2206390",
    "end": "2213020"
  },
  {
    "text": "bound setup, which is a\nlittle bit weird, right? Because matrix\nmultiplication, you just",
    "start": "2213020",
    "end": "2219410"
  },
  {
    "text": "told me, is N cubed work\nfor N squared data access.",
    "start": "2219410",
    "end": "2225450"
  },
  {
    "text": "So fundamentally, it should be\nO of N arithmetic intensity,",
    "start": "2225450",
    "end": "2230760"
  },
  {
    "text": "and you just gave me\nsomething that's O of 1. So you gave me a\nbandwidth-- or I wrote. You didn't give it to me.",
    "start": "2230760",
    "end": "2235860"
  },
  {
    "text": "I wrote a bandwidth\nbound problem. Yet, if I think about\nhow to do this properly,",
    "start": "2235860",
    "end": "2241200"
  },
  {
    "text": "you would tell me, oh, just put\nmatrix A, B, and C in cache. And then the\narithmetic intensity",
    "start": "2241200",
    "end": "2247190"
  },
  {
    "text": "should be O of N. N\ncubed work for every-- But the reason why I can't\nput A and B in cache is what?",
    "start": "2247190",
    "end": "2256010"
  },
  {
    "text": "They're big. This could be gigabyte-sized\nmatrices, right? So this low arithmetic intensity\ndoesn't seem fundamental,",
    "start": "2256010",
    "end": "2265980"
  },
  {
    "text": "but at least the way I wrote the\ncode right now is I'm screwed. I'm running problem 5,\nessentially, from assignment 1.",
    "start": "2265980",
    "end": "2273680"
  },
  {
    "text": "How can I do better?  [INAUDIBLE] Yeah, what?",
    "start": "2273680",
    "end": "2279390"
  },
  {
    "text": "Why the matrices into blocks\nso that each single block can fit into the memory?",
    "start": "2279390",
    "end": "2285040"
  },
  {
    "text": "OK. So the idea which\nwas just given is, the first thing is to\nobserve that I can express",
    "start": "2285040",
    "end": "2292860"
  },
  {
    "text": "matrix multiplication\nnot as operations on individual elements\nof the matrix,",
    "start": "2292860",
    "end": "2298780"
  },
  {
    "text": "but I can express\nmatrix multiplication hierarchically in terms\nof a bunch of submatrix",
    "start": "2298780",
    "end": "2304860"
  },
  {
    "text": "multiplications on blocks. That's the real key insight. So I'm going to\nrewrite the code,",
    "start": "2304860",
    "end": "2312150"
  },
  {
    "text": "and here's how I'm going\nto rewrite the code. Think about everything\nfrom this for loop down as just a\nmatrix multiplication",
    "start": "2312150",
    "end": "2320730"
  },
  {
    "text": "of two sub-blocks. And so the outermost loop\nis matrix multiplication",
    "start": "2320730",
    "end": "2327270"
  },
  {
    "text": "over blocks. So let's think about it. Let's take one block of A,\none block of B. Let's load",
    "start": "2327270",
    "end": "2335520"
  },
  {
    "text": "them into cache, and let's\nmultiply those matrices together. And I'm going to get an output\nmatrix, which is a piece of C.",
    "start": "2335520",
    "end": "2346670"
  },
  {
    "text": "And then I'm going to move one\nblock over an A and one block over a B. And I'm going to\nmultiply those matrices.",
    "start": "2346670",
    "end": "2354140"
  },
  {
    "text": "And in the same way that I\nadded in an element to see here, I'm going to add in the\nresulting sub-block result",
    "start": "2354140",
    "end": "2361700"
  },
  {
    "text": "into the block of C\nthat I'm computing.",
    "start": "2361700",
    "end": "2366936"
  },
  {
    "text": "So what's my arithmetic\nintensity now?",
    "start": "2366936",
    "end": "2374370"
  },
  {
    "text": "For every one of these\nsteps, if I think about-- I should have highlighted this.",
    "start": "2374370",
    "end": "2379960"
  },
  {
    "text": "If I think about\nnow the computation being a sequence of\nthese steps, what is the arithmetic\nintensity of this?",
    "start": "2379960",
    "end": "2386704"
  },
  {
    "text": " It's just matrix multiplication\non blocksizes of B or blocksize",
    "start": "2386705",
    "end": "2392839"
  },
  {
    "text": "here. So the data that I\nhave to load is what? B squared, on the\norder of B squared.",
    "start": "2392840",
    "end": "2399650"
  },
  {
    "text": "And the amount of work\nthat I do is B cubed. So my arithmetic\nintensity is this. It's not N, but it's\nB. So the greater",
    "start": "2399650",
    "end": "2409430"
  },
  {
    "text": "I make my blocksizes,\nas they go to N-- I know. As the blocksizes go to N, I\nget arithmetic intensity of N.",
    "start": "2409430",
    "end": "2415970"
  },
  {
    "text": "As the blocksizes go down to 1,\nI get arithmetic intensity of 1. So my thinking of how\nto choose the blocksize",
    "start": "2415970",
    "end": "2426380"
  },
  {
    "text": "should be, I want as big\nof a blocksize as possible. And what do I actually mean by\npossible here on this statement?",
    "start": "2426380",
    "end": "2434008"
  },
  {
    "text": "It's like the\nmaximum [INAUDIBLE]. Yeah, I cannot make my block\nsizes so large that I start",
    "start": "2434008",
    "end": "2439790"
  },
  {
    "text": "getting capacity\nmisses on my cache. So you would know\nyour cache size.",
    "start": "2439790",
    "end": "2444832"
  },
  {
    "text": "You would pick your\nblocksize so that you could fit a block of\nA, B, and C in cache.",
    "start": "2444832",
    "end": "2450720"
  },
  {
    "text": "And then you would structure\nyour code this way. And I think we focused on this\nexample in the CUDA tutorial",
    "start": "2450720",
    "end": "2459270"
  },
  {
    "text": "last night. But if you don't-- I mean, the code on the\nprevious slide and this slide will take you all of 15 minutes\nto write, like in C. Pop",
    "start": "2459270",
    "end": "2467079"
  },
  {
    "text": "open a C compiler and do it. Make your matrices on the order\nof many megabytes or a gigabyte,",
    "start": "2467080",
    "end": "2475080"
  },
  {
    "text": "you'll be on the order\nof 1000x different speed. It'll be insane how much\nfaster the difference is.",
    "start": "2475080",
    "end": "2482710"
  },
  {
    "text": "So if you ever implement\nmatrix multiply like this on big\nmatrices, you are running at your bandwidth limit.",
    "start": "2482710",
    "end": "2488490"
  },
  {
    "text": "And if you block, you'll\nbe going much, much faster. Yeah. Yeah. So how are those blocks\nstored in the cache lines?",
    "start": "2488490",
    "end": "2497000"
  },
  {
    "text": "Are they stored by row\nparts or column parts? It's a great question. Let's answer the question first,\nand let's think about how that",
    "start": "2497000",
    "end": "2503425"
  },
  {
    "text": "would work on a normal CPU. On a normal CPU, a\ncache, all it does",
    "start": "2503425",
    "end": "2508800"
  },
  {
    "text": "is store lines from\nthe address space. So even though-- let me\ngo back to my figure here.",
    "start": "2508800",
    "end": "2515380"
  },
  {
    "text": "Think about all of\nthe-- let's just say that I choose\na blocksize that's like cache line size\nby cache line size,",
    "start": "2515380",
    "end": "2522750"
  },
  {
    "text": "just to make this\ndescription easier. The cache lines that I\nneed for that block of A",
    "start": "2522750",
    "end": "2530520"
  },
  {
    "text": "are not contiguous in the\naddress space because of the way they're stored.",
    "start": "2530520",
    "end": "2535680"
  },
  {
    "text": "So I'm going to just\nload those cache lines, and the cache is going to figure\nout a place to put them, right?",
    "start": "2535680",
    "end": "2542320"
  },
  {
    "text": "And when the processor\nissues a load instruction to whatever address\nthat this is, the cache will map that to\nthe location in the cache",
    "start": "2542320",
    "end": "2551220"
  },
  {
    "text": "for that cache line stored. So on a CPU, the whole\npoint of the cache is you don't manage it\nas a software programmer.",
    "start": "2551220",
    "end": "2558510"
  },
  {
    "text": "And you could get in trouble if,\nfor example, Intel's algorithm for mapping addresses to\ncache lines doesn't allow--",
    "start": "2558510",
    "end": "2567860"
  },
  {
    "text": "if there are collisions. Two very different\ncache lines end up in the same spot of the cash,\nthat could cause some problems,",
    "start": "2567860",
    "end": "2573622"
  },
  {
    "text": "and you might not get the\nperformance you expect. You go, oh, shoot,\nhow did this happen? And then you might have\nto change your data",
    "start": "2573622",
    "end": "2579210"
  },
  {
    "text": "layout or something like that. But you could also consider like\nthat shared memory on a GPU.",
    "start": "2579210",
    "end": "2584710"
  },
  {
    "text": "We would do things a\nlittle bit differently. We would literally have our\nCUDA threads say, please load that block from\nthe address space",
    "start": "2584710",
    "end": "2591690"
  },
  {
    "text": "and put it in a contiguous\nallocation in CUDA shared memory. And that's the difference\nbetween a cache and what a--",
    "start": "2591690",
    "end": "2599430"
  },
  {
    "text": "CUDA shared memory is more of\nwhat's called a scratch pad. A scratchpad is a\ndifferent address space.",
    "start": "2599430",
    "end": "2605500"
  },
  {
    "text": "A cache is just an\nimplementation detail on the same address space. But either way,\nlet's assume that we",
    "start": "2605500",
    "end": "2613109"
  },
  {
    "text": "can fit those blocks in cache. Yeah. You mentioned the number\nof memory accesses",
    "start": "2613110",
    "end": "2619230"
  },
  {
    "text": "here is B squared? B squared for every sub-step. ",
    "start": "2619230",
    "end": "2627550"
  },
  {
    "text": "That sub-step just\ncorresponds to loading in each of the\nsquared B elements and the A block and the B?",
    "start": "2627550",
    "end": "2633180"
  },
  {
    "text": "That's right. We're going to load B\nsquared elements here. We're going to load B\nsquared elements here. And let's just assume that--\nlet's ignore the reuse,",
    "start": "2633180",
    "end": "2640980"
  },
  {
    "text": "and we're going to load\nB squared elements here. Then we're going to\ndo a matrix-multiply with those matrices, and\nwe'll do B cubed work.",
    "start": "2640980",
    "end": "2648780"
  },
  {
    "text": "So we're doing B cubed work\nfor every B squared I/O. Makes",
    "start": "2648780",
    "end": "2657900"
  },
  {
    "text": "sense? I thought I saw\none more question. ",
    "start": "2657900",
    "end": "2662910"
  },
  {
    "text": "And by the way, remember,\nany modern system, always, the CPU is going\nto have an L1 cache. It's going to have an L2 cache.",
    "start": "2662910",
    "end": "2667930"
  },
  {
    "text": "It's going to have an L3 cache. You might even block\ninto your registers. So my code can start\nadding loops pretty quick.",
    "start": "2667930",
    "end": "2676259"
  },
  {
    "text": "All I'm doing is now\nchoosing different blocksizes so that blocksize-- I can say, give me some blocks\nthat fit in the L3 cache.",
    "start": "2676260",
    "end": "2682830"
  },
  {
    "text": "And then let me partition that\ninto a submatrix multiplication of blocks that fit in the L2\ncache and so on and so on.",
    "start": "2682830",
    "end": "2688942"
  },
  {
    "text": "You almost certainly\nprobably want to block for the O [? one. ?]\nOne level of blocking",
    "start": "2688943",
    "end": "2694425"
  },
  {
    "text": "will get you most of the work,\nbut maybe an extra factor or two or something like that could\ncome from more blocking.",
    "start": "2694425",
    "end": "2701430"
  },
  {
    "text": "Now, of course, this is\nan algorithm that's just plus equals blah, blah, blah.",
    "start": "2701430",
    "end": "2707570"
  },
  {
    "text": "I haven't even talked\nabout SIMD yet. So the multithreading\nseems pretty simple. I'd probably just paralyze over\nthe outermost loop or something",
    "start": "2707570",
    "end": "2715140"
  },
  {
    "text": "like that. But the SIMD could be a\nlittle bit gnarly, if I-- again, there's professional\ngrade hackers hacking",
    "start": "2715140",
    "end": "2721630"
  },
  {
    "text": "on this all the time\nand stuff like that. So let's just think a\nlittle bit about the SIMD. So I decided to write\nthis in intrinsics.",
    "start": "2721630",
    "end": "2728450"
  },
  {
    "text": "Let's say that we are-- and now, by the way, my figures\nare not the entire matrices. I'm thinking about how do\nI multiply these blocks.",
    "start": "2728450",
    "end": "2735620"
  },
  {
    "text": "So notice the dimensions\nare blocksize now. So I could think\nabout it as well. I should take a vector\nfrom B, an element",
    "start": "2735620",
    "end": "2745480"
  },
  {
    "text": "from A. Copy of the element\nof A. That's the splat.",
    "start": "2745480",
    "end": "2752410"
  },
  {
    "text": "Copy that element\nof A four times. And now my dot\nproduct is actually",
    "start": "2752410",
    "end": "2758290"
  },
  {
    "text": "producing multiple outputs\nof C at the same time. So that's kind of interesting.",
    "start": "2758290",
    "end": "2764440"
  },
  {
    "text": "Now I have a SIMD\nversion of the thing, but I have to waste\nthis splat instruction. I don't like the fact that I'm\nstill walking through B, not",
    "start": "2764440",
    "end": "2773400"
  },
  {
    "text": "in row major order. I don't like the fact that\nevery instruction here is dependent on the\nprevious one because I'm",
    "start": "2773400",
    "end": "2779133"
  },
  {
    "text": "doing a dot product,\nwhich actually might hurt [? ILPE ?]\nand some other things. So the design space here gets\ninteresting and complex fast.",
    "start": "2779133",
    "end": "2788970"
  },
  {
    "text": "For example, I\ncould do something where I pre-transpose\nthe block of B.",
    "start": "2788970",
    "end": "2796650"
  },
  {
    "text": "And if I pre-transpose\nthe block of B, now I'm actually just doing\na straight SIMD dot product",
    "start": "2796650",
    "end": "2802920"
  },
  {
    "text": "the whole time. Or there's other versions\nof it where I actually",
    "start": "2802920",
    "end": "2808530"
  },
  {
    "text": "pre-transpose A and not B and\nproduce transpose C. And then re-transpose the\nthing at the end.",
    "start": "2808530",
    "end": "2814329"
  },
  {
    "text": "So I'm not going to get\ninto any of the details. It kind of depends on your SIMD\ninstruction set and your machine",
    "start": "2814330",
    "end": "2819540"
  },
  {
    "text": "and stuff like that. My point being is,\nthis could easily be a two-week\nprogramming assignment,",
    "start": "2819540",
    "end": "2825790"
  },
  {
    "text": "if I wanted it to be for you. And you could try all\nthese various things. And keep in mind that the\nstrategy-- let me go back here.",
    "start": "2825790",
    "end": "2834410"
  },
  {
    "text": "The strategy might be a\nfunction of my dimensions of the blocksize or the matrix.",
    "start": "2834410",
    "end": "2839840"
  },
  {
    "text": "Different blocksizes\nmight lend themselves to different strategies. And if we go back and look\nat this particular network,",
    "start": "2839840",
    "end": "2847120"
  },
  {
    "text": "here are your\ninput matrix sizes, and they're all\ndifferent on every layer.",
    "start": "2847120",
    "end": "2854170"
  },
  {
    "text": "So if you really want\na good implementation, you might use a different matrix\nmultiplication implementation",
    "start": "2854170",
    "end": "2859210"
  },
  {
    "text": "for the various layers. And if you implement\nonly one, you're probably going to be\nsuboptimal, like in this really",
    "start": "2859210",
    "end": "2867010"
  },
  {
    "text": "weird long matrices as\n[? opposed ?] to up there. So this stuff, it gets\ngnarly, and people",
    "start": "2867010",
    "end": "2872740"
  },
  {
    "text": "have put a lot of work into\ncuBLAS and a lot of work into these matrix\nmultiplication routines.",
    "start": "2872740",
    "end": "2879100"
  },
  {
    "text": "Now, there's also another big--\nif I go back a little bit, there's a big problem\nwith this also.",
    "start": "2879100",
    "end": "2888190"
  },
  {
    "text": "And I'm surprised nobody's\nraised their hand yet. Because I had an input image\nor a couple of input images,",
    "start": "2888190",
    "end": "2895059"
  },
  {
    "text": "and what did I do? Yeah. [INAUDIBLE] I basically took every element\nand duplicated it a ton of times",
    "start": "2895060",
    "end": "2903130"
  },
  {
    "text": "in order to make this matrix. So I took a data set that might\nhave been a few hundred of megs and actually made it a matrix\nand did all this data copying",
    "start": "2903130",
    "end": "2911620"
  },
  {
    "text": "to generate maybe multiple\ngigabytes of matrices, especially with a\ncertain batch size.",
    "start": "2911620",
    "end": "2918350"
  },
  {
    "text": "And actually, if you think\nabout-- for those of you that have taken a machine\nlearning class, having to keep extra information\naround during backpropagation",
    "start": "2918350",
    "end": "2924370"
  },
  {
    "text": "and stuff like that, you're out\nof memory very, very quickly. Like with these little\nmatrices, all of a sudden,",
    "start": "2924370",
    "end": "2929680"
  },
  {
    "text": "your footprint is like 16 gig or\nthese little intrinsically small matrices that just\nblow out to 16 gig.",
    "start": "2929680",
    "end": "2936700"
  },
  {
    "text": "So one thing to keep\nin mind is that-- where are we? Yeah.",
    "start": "2936700",
    "end": "2942880"
  },
  {
    "text": "One very modern way\nto do this is to say, we're going to think about it\nlike matrix multiplication,",
    "start": "2942880",
    "end": "2950809"
  },
  {
    "text": "but we're going to go get the\ndata from the original sources on demand when I'm constructing\nthese blocks, right?",
    "start": "2950810",
    "end": "2959870"
  },
  {
    "text": "So the actual data will be-- so if you look at this, this\nis a matrix multiplication.",
    "start": "2959870",
    "end": "2967630"
  },
  {
    "text": "For simplicity, I\ndidn't block it here. For simplicity,\nit's not blocked. It's just normal\nmatrix multiplication.",
    "start": "2967630",
    "end": "2974330"
  },
  {
    "text": "But instead of the\nelement A coming from X, I times width\nplus J, it comes",
    "start": "2974330",
    "end": "2981400"
  },
  {
    "text": "from this accessor, which\ndoes all of the math to figure out what is the\nlocation in the original input",
    "start": "2981400",
    "end": "2988420"
  },
  {
    "text": "tensor, not the\nmatrix, that would have stored the value that\nI need at this matrix.",
    "start": "2988420",
    "end": "2995039"
  },
  {
    "text": "So it's matrix multiplication\nbut not accessing the matrices",
    "start": "2995040",
    "end": "3000410"
  },
  {
    "text": "that I would have created. It's accessing the\noriginal tensors. So in other words, they're\nburning math in this inner loop",
    "start": "3000410",
    "end": "3009500"
  },
  {
    "text": "to compute that address as\na function of the tensor dimensions in exchange for\nnot decompressing the data",
    "start": "3009500",
    "end": "3018440"
  },
  {
    "text": "into these big matrices. So this is something called\nimplicit matrix-multiply in that",
    "start": "3018440",
    "end": "3026630"
  },
  {
    "text": "it's not actually operating on\nmatrices that are this size, even though the\nloop structure is--",
    "start": "3026630",
    "end": "3032300"
  },
  {
    "text": "if it's operating on\nmatrices of that size. Yes? [INAUDIBLE] cache\ninefficiency is not right",
    "start": "3032300",
    "end": "3039619"
  },
  {
    "text": "because we have not put\nup a contiguous block. You're correct. So this dot product\nthat feels like it's",
    "start": "3039620",
    "end": "3046069"
  },
  {
    "text": "iterating over a line of\nthe-- a row of the matrix",
    "start": "3046070",
    "end": "3051620"
  },
  {
    "text": "is actually iterating all\nover that original image. But don't be too worried\nabout that because remember,",
    "start": "3051620",
    "end": "3058660"
  },
  {
    "text": "I'm going to-- that would\nhave sucked anyways. This is going to be blocked. So you should think\nabout these accessors,",
    "start": "3058660",
    "end": "3065099"
  },
  {
    "text": "or you bounce all over memory\nwhen you're bringing that data into shared memory or to cache.",
    "start": "3065100",
    "end": "3070780"
  },
  {
    "text": "Once it's in shared\nmemory or a cache, you flatten it out\nand make it dense.",
    "start": "3070780",
    "end": "3077760"
  },
  {
    "text": "And then you run your\nsub-block matrix multiplication on the dense stuff. In other words, you have to\ncopy data from DRAM into cache.",
    "start": "3077760",
    "end": "3087050"
  },
  {
    "text": "That's the point at\nwhich you do your copy. You don't copy it into\na new location in DRAM",
    "start": "3087050",
    "end": "3092310"
  },
  {
    "text": "and then bring it in. There's a big difference. OK.",
    "start": "3092310",
    "end": "3099160"
  },
  {
    "text": "So that's what you'll see if\nyou just see this implicit GEMM.",
    "start": "3099160",
    "end": "3104230"
  },
  {
    "text": "And by the way,\njust one more thing is-- the other thing that\ngood implementations do",
    "start": "3104230",
    "end": "3110290"
  },
  {
    "text": "is all of this math that goes\nfrom width, height, number of tensors, and batch size\nto an address, if you want,",
    "start": "3110290",
    "end": "3117609"
  },
  {
    "text": "the fastest implementation\nis you pre-compute that math. And then you actually\nmake this a lookup table.",
    "start": "3117610",
    "end": "3124890"
  },
  {
    "text": "So you actually burn\na little bit of memory to get back the address\nand calculations, which",
    "start": "3124890",
    "end": "3131740"
  },
  {
    "text": "actually can be non-trivial\nfor larger tensors. So NVIDIA has this library\ncalled CUTLASS, which,",
    "start": "3131740",
    "end": "3138790"
  },
  {
    "text": "if you want to be a\npretty leet hacker and not go all the\nway to implementing",
    "start": "3138790",
    "end": "3144010"
  },
  {
    "text": "matrix multiplication by\nyourself in [? TTX ?] and GPUs. You might use\nCUTLASS these days.",
    "start": "3144010",
    "end": "3150920"
  },
  {
    "text": "So CUTLASS gives you access to\nvery fast matrix multiplication libraries that fit in shared\nmemory, that have the ability",
    "start": "3150920",
    "end": "3159349"
  },
  {
    "text": "to go pull the data from various\nlocations and things like that. So this is like short of\nwriting your own good CUDA",
    "start": "3159350",
    "end": "3167839"
  },
  {
    "text": "assembly on the\nlow end and using TensorFlow on the high end.",
    "start": "3167840",
    "end": "3174589"
  },
  {
    "text": "CUTLASS is something in between. You're like, I want to\nimplement my own deep network because I want a really\nsmall network running as fast",
    "start": "3174590",
    "end": "3181130"
  },
  {
    "text": "as possible on an NVIDIA GPU. You'd use something\nlike this these days. This is not a very\nuser-friendly library.",
    "start": "3181130",
    "end": "3190250"
  },
  {
    "text": "OK. Now just keep in mind that all\nof this, at least on a big CPU or on a big GPUs\nrunning on this thing,",
    "start": "3190250",
    "end": "3196740"
  },
  {
    "text": "that's quite large, right? So each of these\nlittle SM cores might be kind of cranking on a\nsub-block matrix multiplication.",
    "start": "3196740",
    "end": "3205050"
  },
  {
    "text": "You still need pretty large\nmatrices to fill up this GPU.",
    "start": "3205050",
    "end": "3210290"
  },
  {
    "text": "And that's why when batch size\nis one in machine learning, your performance can go down. There's just not enough work.",
    "start": "3210290",
    "end": "3217760"
  },
  {
    "text": "And so here are\nsome plots, where I'm varying N. And remember,\nP and Q were the output size.",
    "start": "3217760",
    "end": "3225700"
  },
  {
    "text": "R and S was the filter size. So 1 by 1, 3 by 3, 5 by 5.",
    "start": "3225700",
    "end": "3231109"
  },
  {
    "text": "And just notice that you've got\nto make four different batch",
    "start": "3231110",
    "end": "3237430"
  },
  {
    "text": "sizes. You've got to make those output\nimage sizes somewhat large.",
    "start": "3237430",
    "end": "3242590"
  },
  {
    "text": "Before the performance\nof the GPU, the GPU has enough work to\nactually keep itself busy,",
    "start": "3242590",
    "end": "3249190"
  },
  {
    "text": "right? So when people are saying, gosh,\nwith these big neural networks,",
    "start": "3249190",
    "end": "3255020"
  },
  {
    "text": "I can only run batch size\ntwo or three or something like that because I'm going to\nrun out of memory, otherwise.",
    "start": "3255020",
    "end": "3260890"
  },
  {
    "text": "They pay for that in performance\nbecause as these things get a little bit smaller,\nyou're not going to be able to run this big,\nbig processor at peak rate.",
    "start": "3260890",
    "end": "3269470"
  },
  {
    "text": "The y-axis here is throughput? Is throughput, yeah. Floating point [? tera ?]\noperations per second.",
    "start": "3269470",
    "end": "3274880"
  },
  {
    "text": "So higher is better. These three different lines are\nthree different model sizes,",
    "start": "3274880",
    "end": "3280410"
  },
  {
    "text": "different output tensor sizes. P and Q is the x and y\ndimension of the output tensor.",
    "start": "3280410",
    "end": "3285720"
  },
  {
    "text": "And then here, this is\nnot relevant these days because these wide convolutions\ndon't really exist. But this is just saying, if\nyou're using wider convolutions,",
    "start": "3285720",
    "end": "3293370"
  },
  {
    "text": "you do more work. And that work fills up\nthe GPU more quickly. So the graph on the\nright, I think, is--",
    "start": "3293370",
    "end": "3299660"
  },
  {
    "text": "on the left is probably\nmore applicable today. Now, of course, I\ncan also just go back",
    "start": "3299660",
    "end": "3305720"
  },
  {
    "text": "to my original\nblocked conv layer. Here's my C code and my\nsix loops, seven loops.",
    "start": "3305720",
    "end": "3312920"
  },
  {
    "text": "And I just did 15\nminutes on how hard it is to optimize a three-loop\nversion of matrix-multiply.",
    "start": "3312920",
    "end": "3320130"
  },
  {
    "text": "You could also just say, go at\nit, start blocking these loops. And so if you want to just\ndo a direct implementation",
    "start": "3320130",
    "end": "3327980"
  },
  {
    "text": "and do it well, you can\ndefinitely do that on your own as well. And these days, and\nI'll get into this",
    "start": "3327980",
    "end": "3334400"
  },
  {
    "text": "in a second, there's a lot\nof interest in just giving this information at a\ngood compiler and saying,",
    "start": "3334400",
    "end": "3340980"
  },
  {
    "text": "hey, good compiler, you come\nup with the blocking strategy for me. And that's things\nlike XLA, Google,",
    "start": "3340980",
    "end": "3348440"
  },
  {
    "text": "or this new Triton\nthing from OpenAI. They're trying to do that.",
    "start": "3348440",
    "end": "3354119"
  },
  {
    "text": "But in general, I still think\nthese core inner loops are still kind of best done by\nhand, by humans, by hand.",
    "start": "3354120",
    "end": "3364920"
  },
  {
    "text": "Let me just show you a few\nother tricks which are actually pretty interesting. And so keep in mind that--",
    "start": "3364920",
    "end": "3370970"
  },
  {
    "text": "OK. So remember, I said that\na convolution-- here, ignore all this. We can think about it\nas a set of weights,",
    "start": "3370970",
    "end": "3377760"
  },
  {
    "text": "as a right-hand side\nin a matrix vector product against a\nmatrix that comes from the elements\nof the input tensor.",
    "start": "3377760",
    "end": "3384390"
  },
  {
    "text": "And if you look\ncarefully, you're like, you know what-- so\nlet's think about this as M1 plus M2 plus M3.",
    "start": "3384390",
    "end": "3391050"
  },
  {
    "text": "So M1 is just this product,\nM2 is this product, M3 is that product.",
    "start": "3391050",
    "end": "3396350"
  },
  {
    "text": "If we look carefully, there\nare some common subexpressions in here. And we can exploit those\ncommon subexpressions",
    "start": "3396350",
    "end": "3404300"
  },
  {
    "text": "to do some early math,\nand then actually compute",
    "start": "3404300",
    "end": "3409940"
  },
  {
    "text": "the outputs as a function of\nthose common subexpressions. So in this example,\nthis is actually",
    "start": "3409940",
    "end": "3415640"
  },
  {
    "text": "the essence of what's\ncalled a Winograd filter. So you might have heard\nof a Winograd convolution.",
    "start": "3415640",
    "end": "3422030"
  },
  {
    "text": "If I've taken\ndirect convolution-- I used to do six\nmultiplies and four adds.",
    "start": "3422030",
    "end": "3427260"
  },
  {
    "text": "Now I'm actually doing far fewer\nmultiplies and a lot more adds. Now, whether or not\nthis is a good trade",
    "start": "3427260",
    "end": "3433400"
  },
  {
    "text": "off is going to depend on your\nmachine and stuff like that. But there are ways that\nI can algorithmically start moving symbols around and\ndo different amounts of math.",
    "start": "3433400",
    "end": "3441950"
  },
  {
    "text": "Probably the most\ncommon one would be for those of you\nthat are from EE or know anything about\nsignal processing. You know that a convolution\ncan be expressed as a Fourier",
    "start": "3441950",
    "end": "3450830"
  },
  {
    "text": "transform, a basic\npointwise multiplication, and a Fourier transform back. And that fast Fourier\ntransform is basically",
    "start": "3450830",
    "end": "3457988"
  },
  {
    "text": "employing techniques\nlike this, where we're exploiting common\nsubexpressions to remove work. So you can think about\nthis as applying Fourier",
    "start": "3457988",
    "end": "3464120"
  },
  {
    "text": "transform kind of ideas, OK? OK. So all of the big vendors\nhave their own deep learning",
    "start": "3464120",
    "end": "3471980"
  },
  {
    "text": "libraries like cuDNN or\nIntel's oneAPI these days.",
    "start": "3471980",
    "end": "3476990"
  },
  {
    "text": "And if you move up the stack\nand use PyTorch or TensorFlow, all of you or many of\nyou are probably familiar with the library of\ndifferent layer types",
    "start": "3476990",
    "end": "3484340"
  },
  {
    "text": "that you have available to you. So we've talked about conv 2D.",
    "start": "3484340",
    "end": "3489470"
  },
  {
    "text": "That's the one we've\ntalked about here today. But other ones are a\nlittle bit simpler. And so you get this\nlibrary of optimizations.",
    "start": "3489470",
    "end": "3496160"
  },
  {
    "text": "And if you use this in\nTorch or TensorFlow, these operations under\nthe hood get compiled down",
    "start": "3496160",
    "end": "3503420"
  },
  {
    "text": "to cuDNN and these lower level\nvendor-specific libraries. And if we pop open\nthe API of cuDNN",
    "start": "3503420",
    "end": "3511410"
  },
  {
    "text": "which is NVIDIA's core library,\nyou just look at the parameters",
    "start": "3511410",
    "end": "3517760"
  },
  {
    "text": "to-- this is cuDNN\nconvolution forward. This is the forward pass\nof a convolution layer,",
    "start": "3517760",
    "end": "3523369"
  },
  {
    "text": "like we just talked about. And you see a bunch of\ninteresting algorithms or parameters to that thing.",
    "start": "3523370",
    "end": "3530040"
  },
  {
    "text": "It's not just tensor,\nX, weights, weights, W, and output Y. You get this\ndescriptor to the input tensor.",
    "start": "3530040",
    "end": "3539240"
  },
  {
    "text": "You get choices for what\nalgorithms you want to use. And if we look at\nthe algorithms, this should actually make a\nlittle bit more sense now.",
    "start": "3539240",
    "end": "3547589"
  },
  {
    "text": "So let's go look for-- here's implicit GEMM.",
    "start": "3547590",
    "end": "3554010"
  },
  {
    "text": "So GEMM is General\nMatrix Multiplication. The default algorithm\nis implicit GEMM",
    "start": "3554010",
    "end": "3561260"
  },
  {
    "text": "which says, take my tensors,\ntreat the conv layer as a big matrix-multiply,\nbut don't ever actually make",
    "start": "3561260",
    "end": "3567860"
  },
  {
    "text": "these big matrices. Just do the loop indexing as\nyou're doing a matrix-multiply to look up the values.",
    "start": "3567860",
    "end": "3573980"
  },
  {
    "text": "Here's a forward\nalgorithm direct. This is my-- this would\nbe like, oh, just use",
    "start": "3573980",
    "end": "3580730"
  },
  {
    "text": "your blocked version\nof my seven-loop nest to actually directly\ndo the convolution.",
    "start": "3580730",
    "end": "3585800"
  },
  {
    "text": "Here are various\nother ones, like-- here's GEMM, but not implicit.",
    "start": "3585800",
    "end": "3591119"
  },
  {
    "text": "So this actually says, copy the\ndata into these big matrices, and please do it as a normal\nmatrix multiplication.",
    "start": "3591120",
    "end": "3597269"
  },
  {
    "text": "So you have a lot\nof these options. These Winograd options or\nthese FFT options are saying, oh, we want you to transform\nwith an FFT and so on and so on.",
    "start": "3597270",
    "end": "3605250"
  },
  {
    "text": "So you basically\njust tell the API, given what I know about this,\nhere's how I want you to do it.",
    "start": "3605250",
    "end": "3611900"
  },
  {
    "text": "OK. OK. So what we've\ntalked about so far",
    "start": "3611900",
    "end": "3617450"
  },
  {
    "text": "is how to implement matrix\nmatrix multiplication, which is basically how to\nimplement a conv layer.",
    "start": "3617450",
    "end": "3624290"
  },
  {
    "text": "By the way, like a blocked MLP\nor a fully connected layer, it's also a matrix\nmatrix multiplication.",
    "start": "3624290",
    "end": "3630569"
  },
  {
    "text": "So it applies to that as well. What we have not\ntalked about at all",
    "start": "3630570",
    "end": "3637070"
  },
  {
    "text": "is the fact that there\nare multiple layers back to back to back. And there can be\nhundreds of these layers",
    "start": "3637070",
    "end": "3642680"
  },
  {
    "text": "back to back to back. Now keep in mind\nthat I just told you that the output of\none of these layers",
    "start": "3642680",
    "end": "3648289"
  },
  {
    "text": "is a big freaking\nmatrix, a big freaking tensor that could\nbe tens of megabytes",
    "start": "3648290",
    "end": "3653569"
  },
  {
    "text": "or hundreds of megabytes. And let's just think about the\nbasic sequence of a conv layer.",
    "start": "3653570",
    "end": "3659520"
  },
  {
    "text": "We might do this big\nmatrix multiplication and output a width by height\nby N by K output tensor.",
    "start": "3659520",
    "end": "3668220"
  },
  {
    "text": "We're going to store\nthat in memory. And then we're going to bring it\nright back in and do something",
    "start": "3668220",
    "end": "3677120"
  },
  {
    "text": "like add in the bias. And then we're going to\nstore that in memory. And we're going to bring it\nright back in and do a max pool.",
    "start": "3677120",
    "end": "3685790"
  },
  {
    "text": "So this tensor is going in and\nout of memory over and over and over again.",
    "start": "3685790",
    "end": "3692210"
  },
  {
    "text": "And for these-- conv\nlayer might be blockable, but scale and bias is not.",
    "start": "3692210",
    "end": "3697817"
  },
  {
    "text": "The only thing you're doing\nis multiplying every element by a number. And max pool, there's not\na lot of math in there.",
    "start": "3697817",
    "end": "3703550"
  },
  {
    "text": "I'm just taking every 2\nby 2 region of the tensor, computing the max\nand outputting that.",
    "start": "3703550",
    "end": "3708849"
  },
  {
    "text": "So these things here are\nseverely bandwidth bound, severely bandwidth bound.",
    "start": "3708850",
    "end": "3716110"
  },
  {
    "text": "So we would love to be able to\njust kind do all this in one shot and then send the tensor\nout to the rest of the memory.",
    "start": "3716110",
    "end": "3725630"
  },
  {
    "text": "So here's an example of a-- it's very easy for me. If I knew the scale and\nbias that's happening",
    "start": "3725630",
    "end": "3732790"
  },
  {
    "text": "is once I get done with\nthe matrix-multiply, I should go ahead and\nscale it and bias it.",
    "start": "3732790",
    "end": "3740230"
  },
  {
    "text": "And that just saved me hundreds\nof hundreds of megabytes out to memory and back.",
    "start": "3740230",
    "end": "3745480"
  },
  {
    "text": "Here's an interesting\nthing is, how would you rewrite this code if\nyou wanted to do the max pool all in line here?",
    "start": "3745480",
    "end": "3753789"
  },
  {
    "text": "It's not that hard. It makes for some gnarly code. But conceptually,\nit's not that hard. How would you do the max\npool right on the spot?",
    "start": "3753790",
    "end": "3760450"
  },
  {
    "text": " Yeah. You just need to show your\nblocksizes, the multiple of",
    "start": "3760450",
    "end": "3768110"
  },
  {
    "text": "[INAUDIBLE]. Yeah. So imagine this was blocked. If it was blocked, I would\nproduced a block of output",
    "start": "3768110",
    "end": "3773240"
  },
  {
    "text": "sitting there in cache. I should do my max pool right\nthen before I send it out.",
    "start": "3773240",
    "end": "3778320"
  },
  {
    "text": "And then I not only save\nthe store and the load.",
    "start": "3778320",
    "end": "3784460"
  },
  {
    "text": "Actually, what I store\nis four times smaller than what I produced anyways. So there's a huge win there.",
    "start": "3784460",
    "end": "3791540"
  },
  {
    "text": "So it turns out that this\nstuff really matters, and that's when your\nlayer types get gnarly.",
    "start": "3791540",
    "end": "3799730"
  },
  {
    "text": "And without any\ncompiler support, that's when TensorFlow start\nhaving API entry points that",
    "start": "3799730",
    "end": "3807680"
  },
  {
    "text": "were called\nConv2DFusedBatchNorm power of 10 or something like that.",
    "start": "3807680",
    "end": "3813320"
  },
  {
    "text": "You just start seeing this\nAPI that just gets huge because they're like,\nwell, this one's like a really slow\nsequence, and we",
    "start": "3813320",
    "end": "3819710"
  },
  {
    "text": "need to make a\nspecial case for that. Nowadays, people are hoping\nwith things like JAX and Triton,",
    "start": "3819710",
    "end": "3826380"
  },
  {
    "text": "that if the compilers, given\nknowledge of these operations, can do some of this\nfusion for you.",
    "start": "3826380",
    "end": "3832780"
  },
  {
    "text": "But it's still not great. It's still not that great. And let me tell you why\nit's not super great.",
    "start": "3832780",
    "end": "3840369"
  },
  {
    "text": "I want to tell you about\na cool little trick that was invented here at Stanford\nabout a year and a half ago now that\nsignificantly improved",
    "start": "3840370",
    "end": "3847980"
  },
  {
    "text": "the performance of\nthese transformer layers in large language models.",
    "start": "3847980",
    "end": "3853000"
  },
  {
    "text": "And if you take a step back,\nthis trick is basically something that\nany of you in 149,",
    "start": "3853000",
    "end": "3859660"
  },
  {
    "text": "had I given you a\ntransformer layer, you would have looked at it,\nand you would have gone, yeah, you should definitely do this.",
    "start": "3859660",
    "end": "3865110"
  },
  {
    "text": "And had you have\ndone this a year and a half ago, you would have\nbeen in the talk of the town in the machine\nlearning Twittersphere.",
    "start": "3865110",
    "end": "3870803"
  },
  {
    "text": "So I want to talk very quickly. I'm going to move away from\nthese convolutional models. I'm going to talk about\nthese transformers,",
    "start": "3870803",
    "end": "3877740"
  },
  {
    "text": "and I want to talk about\nsequence-to-sequence transformers. So for example, the input might\nbe a sequence of tokens, maybe",
    "start": "3877740",
    "end": "3884340"
  },
  {
    "text": "a sequence of words. And the output of the model is\nto produce the next word, just",
    "start": "3884340",
    "end": "3889440"
  },
  {
    "text": "autoregressively, OK? The application\ndoesn't matter at all. What does matter is\nthe workload when",
    "start": "3889440",
    "end": "3895590"
  },
  {
    "text": "we look into the key part of\nthis neural network, which is this box, which\nis called attention.",
    "start": "3895590",
    "end": "3902190"
  },
  {
    "text": "OK? And I'm not going to get into\nthe algorithms of attention at all right now, but I'm\njust going to tell you",
    "start": "3902190",
    "end": "3908160"
  },
  {
    "text": "what the workload looks like. The input are a\nbunch of tensors.",
    "start": "3908160",
    "end": "3913349"
  },
  {
    "text": "Three tensors, Q, K,\nand V. There are some-- I could give you\nan interpretation of this as a query.",
    "start": "3913350",
    "end": "3919690"
  },
  {
    "text": "This is the key that the queries\nmatch, and this is the value. So you could say, hey, for every\nelement in my input sequence,",
    "start": "3919690",
    "end": "3926490"
  },
  {
    "text": "I'm going to think about that as\nlooking up in a database of what other tokens match.",
    "start": "3926490",
    "end": "3932520"
  },
  {
    "text": "And based on what\nmatch is, that's going to influence the output. But just think about it for\nnow, as I have three embeddings,",
    "start": "3932520",
    "end": "3940420"
  },
  {
    "text": "I have three vectors, Q, K, and\nV, there are N-dimension arrays.",
    "start": "3940420",
    "end": "3945700"
  },
  {
    "text": "And at every point\nin that array, think about there being a\nD-dimensional embedding, OK?",
    "start": "3945700",
    "end": "3951030"
  },
  {
    "text": "So I didn't give\nyou the dimensions here somehow, but this is\nN by D, this is N by D,",
    "start": "3951030",
    "end": "3958200"
  },
  {
    "text": "this is N by D. And this\nattention layer basically is just going to\ncompute interactions",
    "start": "3958200",
    "end": "3964170"
  },
  {
    "text": "between queries and keys. So in other words,\nwe're going to take an outer product of the query\nvector Q against the key vector",
    "start": "3964170",
    "end": "3973920"
  },
  {
    "text": "K. And if these are\nN by D, my output is going to be an M by N matrix.",
    "start": "3973920",
    "end": "3980620"
  },
  {
    "text": "So I'm going to do a matrix-- I'm going to do an outer\nproduct to produce a matrix. Then for every row\nof that matrix,",
    "start": "3980620",
    "end": "3988090"
  },
  {
    "text": "I'm going to perform an\noperation called a softmax. If you don't know what a\nsoftmax is, it does not matter.",
    "start": "3988090",
    "end": "3994450"
  },
  {
    "text": "Let me tell you what\nmatters about the softmax. The softmax of X,\nwhich I want you",
    "start": "3994450",
    "end": "3999480"
  },
  {
    "text": "to think of X as the\nrow of the vector, is just a scaling\nof all the elements,",
    "start": "3999480",
    "end": "4005100"
  },
  {
    "text": "but that scaling depends on the\nmaximum element in the vector.",
    "start": "4005100",
    "end": "4011480"
  },
  {
    "text": "OK, why does that matter? That matters\nbecause I don't know what the maximum\nelement of a vector",
    "start": "4011480",
    "end": "4017990"
  },
  {
    "text": "is until I've computed\nthe entire vector.",
    "start": "4017990",
    "end": "4023010"
  },
  {
    "text": "So this would be\nlike saying, compute the sum of all the\nelements in the array and normalize\neverything by the sum.",
    "start": "4023010",
    "end": "4029930"
  },
  {
    "text": "Computationally, it\nwould be equivalent. OK? So the problem here is\nI do a matrix-multiply.",
    "start": "4029930",
    "end": "4037490"
  },
  {
    "text": "I produce an M by N matrix. Then for every\nrow in the matrix, I've got to compute\nthe max element.",
    "start": "4037490",
    "end": "4043520"
  },
  {
    "text": "And then I use that max\nelement in a new computation",
    "start": "4043520",
    "end": "4049970"
  },
  {
    "text": "to do another matrix multiply-- sorry, a matrix vector product.",
    "start": "4049970",
    "end": "4055020"
  },
  {
    "text": "So let me diagram\nthis out for you. I have Q and my K\nvectors that are N by D.",
    "start": "4055020",
    "end": "4062589"
  },
  {
    "text": "I multiply them together to get\na matrix here that's N by N. And by the way, N is large,\nbecause if we think about",
    "start": "4062590",
    "end": "4069460"
  },
  {
    "text": "sequences of tens of\nthousands of tokens, we're talking about N\nsquared on N equals 10,000.",
    "start": "4069460",
    "end": "4075740"
  },
  {
    "text": "This is a huge\nmulti-gigabyte matrix. Then for every row\nof that matrix,",
    "start": "4075740",
    "end": "4081950"
  },
  {
    "text": "I'm going to compute the\nsoftmax which-- again, if you don't care\nabout softmax, just think about I'm going to\ncompute the sum of all",
    "start": "4081950",
    "end": "4088063"
  },
  {
    "text": "the elements in the\nvector-- or the max of all the elements in the vector.",
    "start": "4088063",
    "end": "4093085"
  },
  {
    "text": "So I normalize all\nthe rows by that sum, and then I do a matrix\nvector product in order",
    "start": "4093085",
    "end": "4099250"
  },
  {
    "text": "to compute the final\nresult. The problem is that this matrix is huge.",
    "start": "4099250",
    "end": "4105649"
  },
  {
    "text": "And so you could do\nblocked matrix multiply so you can compute that\nmatrix very efficiently.",
    "start": "4105649",
    "end": "4111020"
  },
  {
    "text": "But you still have to\nstore it out to memory. You have to bring it back in row\nby row to compute the softmax.",
    "start": "4111020",
    "end": "4117910"
  },
  {
    "text": "You have to bring\nit back in again to compute this\nmatrix vector product. And I'm not showing\nyou a few other steps.",
    "start": "4117910",
    "end": "4123568"
  },
  {
    "text": "There are some\nthings that people do with masking this\nthing and stuff like that. You got to bring it\nin a bunch of times.",
    "start": "4123569",
    "end": "4130100"
  },
  {
    "text": "That's where the compute is in\nthese transformers-- or at least the cost is in the transformers.",
    "start": "4130100",
    "end": "4135599"
  },
  {
    "text": "There's no compute, actually,\nwhich is the problem. So here's the trick.",
    "start": "4135600",
    "end": "4140759"
  },
  {
    "text": "The trick was, is there any way\nwe can do this whole sequence block by block? You just told me\nhow to do a max pool",
    "start": "4140760",
    "end": "4147500"
  },
  {
    "text": "in the middle of a conv layer. The question is the same here. Is there any way that I can\nfuse through this softmax, which",
    "start": "4147500",
    "end": "4154818"
  },
  {
    "text": "looks like something that needs\nevery element of the matrix before I can go any further?",
    "start": "4154819",
    "end": "4160729"
  },
  {
    "text": "And some folks just\nlooked at it and said, OK, so here's again the\nmath of the softmax, which",
    "start": "4160729",
    "end": "4167330"
  },
  {
    "text": "is for every element\nof the vector x, we're just going to\nraise E to the power of x",
    "start": "4167330",
    "end": "4173149"
  },
  {
    "text": "scaled by the maximum\nelement in the vector. So I have to compute\nthat max, and I",
    "start": "4173149",
    "end": "4178790"
  },
  {
    "text": "have to compute the sum in\norder to know how to scale. That's basically [? OK. ?] And it turns out that you\ncan factor this and compute",
    "start": "4178790",
    "end": "4187220"
  },
  {
    "text": "it block by block. And the details of this, I\nthink, we should take offline.",
    "start": "4187220",
    "end": "4192660"
  },
  {
    "text": "But the general\ngist of it is let's think about x as a first\nhalf and a second half. So here's the first half.",
    "start": "4192660",
    "end": "4197820"
  },
  {
    "text": "Here's the second half. And if I think about\nwhat is the max of vector",
    "start": "4197820",
    "end": "4203659"
  },
  {
    "text": "x in terms of maxes of x1\nand x2, well, the max of x",
    "start": "4203660",
    "end": "4208790"
  },
  {
    "text": "is clearly the max of\nx1 and the max of x2. So this can\ndefinitely break down.",
    "start": "4208790",
    "end": "4216200"
  },
  {
    "text": "And it also turns out that\ncomputing this f of x-- well, all I need to know,\nif I knew the max value,",
    "start": "4216200",
    "end": "4223500"
  },
  {
    "text": "I can compute f\non the first half. And then I just scale it back\nup by something we already know.",
    "start": "4223500",
    "end": "4231330"
  },
  {
    "text": "Now, I don't expect this\nto be followed in real time but in 10 minutes of math. And you just notice that\ninside of this term,",
    "start": "4231330",
    "end": "4239330"
  },
  {
    "text": "there's an e to the x1. So these are going\nto cancel, and you're going to end up with-- it'll all work out.",
    "start": "4239330",
    "end": "4245690"
  },
  {
    "text": "The point being is\nthat softmax can be computed in chunks, if you\njust keep a running sum of max.",
    "start": "4245690",
    "end": "4253810"
  },
  {
    "text": "And as a result of that, I can\nfuse through this whole thing.",
    "start": "4253810",
    "end": "4258860"
  },
  {
    "text": "So I can compute the\nmatrix multiplication, I can compute the softmax, and\nI can compute the final matrix",
    "start": "4258860",
    "end": "4264130"
  },
  {
    "text": "product by loading a block\nof Q, loading a block of K, loading a block of V, computing\nthe submatrix multiplication,",
    "start": "4264130",
    "end": "4272620"
  },
  {
    "text": "computing the softmax on a\nchunk of the whole row, which is",
    "start": "4272620",
    "end": "4277670"
  },
  {
    "text": "a row of the submatrix. Then doing the matrix multiply\nand then accumulating into O.",
    "start": "4277670",
    "end": "4283060"
  },
  {
    "text": "So all of a sudden, my memory\nrequirements shrunk from N squared to blocksize\nsquared, which",
    "start": "4283060",
    "end": "4290470"
  },
  {
    "text": "means I can fit much more\ndata on chip in a GPU, which means I can run on\nmuch longer sequences.",
    "start": "4290470",
    "end": "4297620"
  },
  {
    "text": "And because I'm not bandwidth\nbound in some situations, I can run faster. So the speed\nimprovement was modest.",
    "start": "4297620",
    "end": "4304590"
  },
  {
    "text": "I think a few small\nconstant factors. The memory footprint\nwas enormous,",
    "start": "4304590",
    "end": "4309650"
  },
  {
    "text": "and that moved us from sequences\nof length 8,000 to sequences of 32,000 and stuff like that.",
    "start": "4309650",
    "end": "4315120"
  },
  {
    "text": "I believe GPT-4 is enabled\nby an optimization like this. So this is your basic\nproducer-consumer locality",
    "start": "4315120",
    "end": "4321590"
  },
  {
    "text": "optimization. You can imagine it would\nhave been hard for a compiler to do the mathematical\nanalysis to figure out",
    "start": "4321590",
    "end": "4327740"
  },
  {
    "text": "that this was possible, but\nsome folks in group did that.",
    "start": "4327740",
    "end": "4332750"
  },
  {
    "text": "And once you know how\nto chunk that softmax, then you can chunk\nthrough the whole thing.",
    "start": "4332750",
    "end": "4338390"
  },
  {
    "text": "And all of a sudden,\nthe equivalent of just reordering some\nloops here or there.",
    "start": "4338390",
    "end": "4343880"
  },
  {
    "text": "And it's a very big change, OK? So these are the\ntypes of stuff that",
    "start": "4343880",
    "end": "4349460"
  },
  {
    "text": "matter the most, once you have\na good matrix multiplication implementation.",
    "start": "4349460",
    "end": "4355163"
  },
  {
    "text": "These are the types of\nthings that are pretty funny. I actually alluded\nto this before. Eight or nine years ago,\npeople were like, well,",
    "start": "4355163",
    "end": "4361050"
  },
  {
    "text": "you've got a fuse, but we\ndon't really know how to do it. so we're just going to have\nthose people that implemented a good matrix multiplication.",
    "start": "4361050",
    "end": "4366820"
  },
  {
    "text": "We're going to go\nahead and implement you a good FusedBatchNorm into\nthe matrix multiplication or FuseResizeAndPadConv2D and\nget a coffee at the end of it.",
    "start": "4366820",
    "end": "4375909"
  },
  {
    "text": "That's basically\nwhat was popping up. CUDA has had this\nvery template wood--",
    "start": "4375910",
    "end": "4383080"
  },
  {
    "text": "templated fusion capability,\nlike any convolution operator,",
    "start": "4383080",
    "end": "4388130"
  },
  {
    "text": "followed by what's called\na pointwise operator. A pointwise operator\nis something that just is like\na map, followed",
    "start": "4388130",
    "end": "4394600"
  },
  {
    "text": "by another pointwise operator. Anything that fit that pattern. They said, OK, we'll try\nand fuse the pointwise",
    "start": "4394600",
    "end": "4399700"
  },
  {
    "text": "into the matrix multiplication. Given the fact that\na great solution for max pool and other stuff\nwas proposed in five seconds,",
    "start": "4399700",
    "end": "4406060"
  },
  {
    "text": "this is not\nintellectually that hard. What is hard is to do it on\narbitrary tensor programs.",
    "start": "4406060",
    "end": "4412969"
  },
  {
    "text": "And so when you see these\nframeworks out there-- JAX is actually a pretty nice,\nsophisticated one right now.",
    "start": "4412970",
    "end": "4419900"
  },
  {
    "text": "They're now starting to\nanalyze your tensor loop nests and stuff like that\nand go, yeah, we",
    "start": "4419900",
    "end": "4425150"
  },
  {
    "text": "see how we can\nfuse this right in, and we can generate\nthat code for you. So stuff's getting better.",
    "start": "4425150",
    "end": "4431010"
  },
  {
    "text": "It turns out that you need both. You need an excellent\nimplementation of conv layer and matrix-multiply.",
    "start": "4431010",
    "end": "4436700"
  },
  {
    "text": "And then you need\nexcellent smarts so that everything else\non the deep network, it being bandwidth bound,\ndoesn't slow everything down.",
    "start": "4436700",
    "end": "4443969"
  },
  {
    "text": "You need both optimizations\nto have a good network implementation.",
    "start": "4443970",
    "end": "4449450"
  },
  {
    "text": "Just as I finish up,\nI want to talk about-- there's just a whole bunch\nof other stuff out there. The next thing you\ndo to speed things up",
    "start": "4449450",
    "end": "4455660"
  },
  {
    "text": "is you stop using\nregular precision math. You go to lower precision math. People are pushing\nthis to the extreme.",
    "start": "4455660",
    "end": "4462949"
  },
  {
    "text": "I think NVIDIA GPUs\nare now starting to muck around with about\nfour-bit precision math these days.",
    "start": "4462950",
    "end": "4468980"
  },
  {
    "text": "So the space of techniques\nis, first of all, you want to start\nwith good algorithms.",
    "start": "4468980",
    "end": "4475430"
  },
  {
    "text": "If you're a systems person and\nyou ignore algorithm innovation, you will get left behind. And there's a huge\nnumber of startups",
    "start": "4475430",
    "end": "4481760"
  },
  {
    "text": "that are being left\nbehind because they saw this curve of everything\nbigger, everything bigger.",
    "start": "4481760",
    "end": "4487040"
  },
  {
    "text": "And I think that was dumb. Then you need to take 149-like\nprinciples and good compilers",
    "start": "4487040",
    "end": "4493610"
  },
  {
    "text": "principles and optimize these\nthings so that they run well on modern hardware.",
    "start": "4493610",
    "end": "4500400"
  },
  {
    "text": "We didn't talk too much\ntoday about approximations, like low precision and sparsity,\nbut that's out there as well.",
    "start": "4500400",
    "end": "4506489"
  },
  {
    "text": "And then the last\npiece of the puzzle is what hardware you\nrun this thing on. So let's close up in the last\nminute and a half on this.",
    "start": "4506490",
    "end": "4513440"
  },
  {
    "text": "Given what you\nknow, why would we say that a GPU is\na good platform",
    "start": "4513440",
    "end": "4519530"
  },
  {
    "text": "to run these deep neural\nnetwork computations on? What are some properties? ",
    "start": "4519530",
    "end": "4526870"
  },
  {
    "text": "Parallelization. Tons of parallelism. These are huge matrix\nmultiplication operations. What else?",
    "start": "4526870",
    "end": "4532918"
  },
  {
    "text": "There's a lot of compute. Has a lot of\narithmetic intensity if you do it correctly, yep. And so there's a lot of good\nalgorithmic-- good ordering",
    "start": "4532918",
    "end": "4541043"
  },
  {
    "text": "optimizations to get the\narithmetic intensity. So we can use all that compute. All of the weights\napplied to each input.",
    "start": "4541043",
    "end": "4548420"
  },
  {
    "text": "So it's very SIMD-- So we've got a lot\nof SIMD capability, and we already have\nthese processors",
    "start": "4548420",
    "end": "4553930"
  },
  {
    "text": "that had a crap ton of\nALUs on them for graphics. So they happen to be\nright in the right place for the right time to do machine\nlearning about a decade ago,",
    "start": "4553930",
    "end": "4561500"
  },
  {
    "text": "right? So that's why\neverybody loves GPUs. Now, the flip side\nis, why might GPUs",
    "start": "4561500",
    "end": "4570160"
  },
  {
    "text": "be a suboptimal platform\nfor DNN evaluation, given that a GPU is an arbitrary\ngeneral purpose processor?",
    "start": "4570160",
    "end": "4577730"
  },
  {
    "text": "And most of this work are\nthese very simple matrix multiplication, maybe\nmatrix vector multiplication",
    "start": "4577730",
    "end": "4584810"
  },
  {
    "text": "operations. So we're going to have\na whole class on this. But I want to hint\nat it right now so",
    "start": "4584810",
    "end": "4590090"
  },
  {
    "text": "that those of you taking 229\nor maybe when you get around to your assignment is, what\nwas the motivation of a SIMD",
    "start": "4590090",
    "end": "4598460"
  },
  {
    "text": "instruction? Why do architects put SIMD\ninstructions in a processor?",
    "start": "4598460",
    "end": "4605094"
  },
  {
    "text": "[INAUDIBLE] instructions\n[INAUDIBLE]. That's correct. But so what? Why not just build a whole\nbunch of little processors?",
    "start": "4605094",
    "end": "4613190"
  },
  {
    "text": "[INAUDIBLE] strategic\ninstructions. The idea is to amortize non-math\nwork, instruction stream",
    "start": "4613190",
    "end": "4620090"
  },
  {
    "text": "control, data access, whatever\nacross that same operation.",
    "start": "4620090",
    "end": "4626389"
  },
  {
    "text": "And we don't have to stop there\nwith the SIMD instruction.",
    "start": "4626390",
    "end": "4631770"
  },
  {
    "text": "We could add an instruction\nlike a dot product. It'd be super helpful for\na matrix multiplication",
    "start": "4631770",
    "end": "4636800"
  },
  {
    "text": "computation. Like a 4 by 4 dot product,\ndo [? four ?] math ops and add them together. Or we could actually say,\nwell, how about an instruction",
    "start": "4636800",
    "end": "4644210"
  },
  {
    "text": "that does a 4 by\n4 matrix-multiply, like a little, little\nmatrix-multiply? I know how to use\nthat because I know",
    "start": "4644210",
    "end": "4650120"
  },
  {
    "text": "how to break down big matrix\nmultiplications into that. So the key principle here,\nif you're an architect,",
    "start": "4650120",
    "end": "4656030"
  },
  {
    "text": "is you want to amortize all\nof the overhead of running a program over the\nbiggest math operations",
    "start": "4656030",
    "end": "4663890"
  },
  {
    "text": "that you can think of. And here are some examples of\nthe magnitude of that overhead",
    "start": "4663890",
    "end": "4669380"
  },
  {
    "text": "in energy efficiency. So compared to-- and these\nare NVIDIA's estimates.",
    "start": "4669380",
    "end": "4675630"
  },
  {
    "text": "So they have a vested interest\nin making deep learning accelerators. TPUs seem very inefficient\nor not so efficient.",
    "start": "4675630",
    "end": "4683700"
  },
  {
    "text": "But they said, if you just run\nadds and multiplies on an NVIDIA processor and you do your\nmatrix multiplication with that,",
    "start": "4683700",
    "end": "4690650"
  },
  {
    "text": "you're 2,000 times less\nefficient than if you built custom silicon to do\nthat matrix-multiply.",
    "start": "4690650",
    "end": "4697880"
  },
  {
    "text": "If you make a couple\nof bigger operations, like just for\ncomponent dot product,",
    "start": "4697880",
    "end": "4704070"
  },
  {
    "text": "you're all the way down to\n500, only 500x more efficient.",
    "start": "4704070",
    "end": "4709829"
  },
  {
    "text": "If you give yourself an\ninstruction that's a 4 by 4 matrix-multiply-- because\nthat does a lot of work. That's 64 ops in\none instruction.",
    "start": "4709830",
    "end": "4718909"
  },
  {
    "text": "You're all the way\ndown to maybe only 30% more inefficient\nthan these things. So these NVIDIA, their reaction\nwas, in addition to math units",
    "start": "4718910",
    "end": "4727730"
  },
  {
    "text": "for adds and loads and\nall your standard math, they have this thing called a\ntensor core off to the side.",
    "start": "4727730",
    "end": "4733770"
  },
  {
    "text": "What that tensor core is,\nthat's a fancy marketing name for all it is. It's support for a special\ninstruction that does a 4 by--",
    "start": "4733770",
    "end": "4741840"
  },
  {
    "text": "I guess, nowadays,\nit's like a 4 by 8 by 8 by 4 matrix multiplication. It's like 128 ops.",
    "start": "4741840",
    "end": "4747740"
  },
  {
    "text": "So if you write your code\nin terms of your submatrix multiply, you're just\ngoing, oh, do your 8 by 4",
    "start": "4747740",
    "end": "4753710"
  },
  {
    "text": "matrix multiplication. You have access to-- your CUDA cores give you 19.5\nteraflops of floating point math",
    "start": "4753710",
    "end": "4763070"
  },
  {
    "text": "at 32-bit precision. If that's math that can be\nphrased in terms of matrix multiplies and you can do\nit in 16-bit floating point,",
    "start": "4763070",
    "end": "4772660"
  },
  {
    "text": "you get 300 teraflops\nof compute capability. It's an order of magnitude--\nmore than an order of magnitude,",
    "start": "4772660",
    "end": "4779530"
  },
  {
    "text": "more peak compute on that GPU,\njust in-- basically, ALUs,",
    "start": "4779530",
    "end": "4784599"
  },
  {
    "text": "they can only do\na matrix vector-- do a matrix matrix-multiply. That's what a tensor core is. It's just a fancy instruction.",
    "start": "4784600",
    "end": "4791110"
  },
  {
    "text": "And that's NVIDIA's reaction\nto what we'll talk about later, like TPUs and other\naccelerators, things like this.",
    "start": "4791110",
    "end": "4798079"
  },
  {
    "text": "So that's your little crash\ncourse on modern optimization of these networks and hopefully\nallows you to situate yourself",
    "start": "4798080",
    "end": "4805360"
  },
  {
    "text": "in the space of all these\ndifferent solutions that are out there spanning\ndifferent fields. The algorithms people\nare contributing,",
    "start": "4805360",
    "end": "4812060"
  },
  {
    "text": "the software compiler, and\n149 people are contributing, and the hardware people are\nnow contributing as well.",
    "start": "4812060",
    "end": "4817700"
  },
  {
    "text": "So it's a very, very\ninteresting cross-cutting space. Cool. All right. ",
    "start": "4817700",
    "end": "4826000"
  }
]