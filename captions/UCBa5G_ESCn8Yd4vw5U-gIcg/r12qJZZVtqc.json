[
  {
    "text": "So this now concludes,",
    "start": "3980",
    "end": "6900"
  },
  {
    "text": "uh, the first two parts of the lecture.",
    "start": "6900",
    "end": "8640"
  },
  {
    "text": "And what I wanna do in the, uh,",
    "start": "8640",
    "end": "10305"
  },
  {
    "text": "remaining, uh, few minutes,",
    "start": "10305",
    "end": "12615"
  },
  {
    "text": "10 minutes or so, I wanna talk about",
    "start": "12615",
    "end": "14940"
  },
  {
    "text": "connection to know the embeddings and matrix factorization.",
    "start": "14940",
    "end": "18494"
  },
  {
    "text": "So, uh, let me refresh you what we talked,",
    "start": "18495",
    "end": "22320"
  },
  {
    "text": "uh, what we talked, uh, on Tuesday.",
    "start": "22320",
    "end": "24735"
  },
  {
    "text": "So we talked about embeddings and we talked",
    "start": "24735",
    "end": "27690"
  },
  {
    "text": "about how we have this embedding matrix z that, you know,",
    "start": "27690",
    "end": "30930"
  },
  {
    "text": "the number of rows is the e- embedding dimension,",
    "start": "30930",
    "end": "34439"
  },
  {
    "text": "and the number of columns is the number of nodes,",
    "start": "34439",
    "end": "37204"
  },
  {
    "text": "uh, in the graph.",
    "start": "37205",
    "end": "38465"
  },
  {
    "text": "And this means that every column of this matrix z",
    "start": "38465",
    "end": "41735"
  },
  {
    "text": "will store an embedding for that given node.",
    "start": "41735",
    "end": "45080"
  },
  {
    "text": "And our objective was in this encoder-decoder framework is that we wanna maximize,",
    "start": "45080",
    "end": "50615"
  },
  {
    "text": "um, the- the- the dot product between pairs of nodes,",
    "start": "50615",
    "end": "54310"
  },
  {
    "text": "uh, that are similar.",
    "start": "54310",
    "end": "55460"
  },
  {
    "text": "So if two nodes are similar, uh,",
    "start": "55460",
    "end": "57410"
  },
  {
    "text": "then their dot product of their embeddings of",
    "start": "57410",
    "end": "60260"
  },
  {
    "text": "their columns in this matrix z, uh, has to be high.",
    "start": "60260",
    "end": "63835"
  },
  {
    "text": "And that was the- that was the idea.",
    "start": "63835",
    "end": "66315"
  },
  {
    "text": "Then of course, how did you define this notion of similarity?",
    "start": "66315",
    "end": "69140"
  },
  {
    "text": "We said two nodes are similar if they co-appear in the same random walk,",
    "start": "69140",
    "end": "74495"
  },
  {
    "text": "uh, starting at the given node.",
    "start": "74495",
    "end": "76600"
  },
  {
    "text": "So now, you know,",
    "start": "76600",
    "end": "78930"
  },
  {
    "text": "how- how can we,",
    "start": "78930",
    "end": "80910"
  },
  {
    "text": "uh, think about this more broadly, right?",
    "start": "80910",
    "end": "82985"
  },
  {
    "text": "You could say we define the notion of",
    "start": "82985",
    "end": "85130"
  },
  {
    "text": "similarity through random walks in the previous lecture.",
    "start": "85130",
    "end": "87860"
  },
  {
    "text": "What if we define an even simpler notion of, uh, similarity?",
    "start": "87860",
    "end": "91460"
  },
  {
    "text": "What if we say, two nodes are similar if they are connected by an edge.",
    "start": "91460",
    "end": "95440"
  },
  {
    "text": "Then what this means is we say, \"Oh,",
    "start": "95440",
    "end": "97720"
  },
  {
    "text": "I want to approximate this matrix,",
    "start": "97720",
    "end": "99950"
  },
  {
    "text": "say like entry in the mai- UV in the matrix,",
    "start": "99950",
    "end": "102920"
  },
  {
    "text": "say this is either 0 or 1 by this dot product?\"",
    "start": "102920",
    "end": "106155"
  },
  {
    "text": "Like I'm saying, if two nodes are connect- u and v are connected,",
    "start": "106155",
    "end": "109830"
  },
  {
    "text": "then I want their dot product to be 1.",
    "start": "109830",
    "end": "111980"
  },
  {
    "text": "And if they are not connected,",
    "start": "111980",
    "end": "113525"
  },
  {
    "text": "I want their dot product to be 0, right?",
    "start": "113525",
    "end": "116940"
  },
  {
    "text": "Um, of course, like if I write it this way at the level of the single entry,",
    "start": "116940",
    "end": "121760"
  },
  {
    "text": "if I write it in the- in the matrix form,",
    "start": "121760",
    "end": "124715"
  },
  {
    "text": "this- I'm writing basically saying,",
    "start": "124715",
    "end": "126200"
  },
  {
    "text": "this is Z trans- Z transposed times Z equals A.",
    "start": "126200",
    "end": "130024"
  },
  {
    "text": "So this is now caused matrix factorization because I take my matrix A and I factorize it,",
    "start": "130025",
    "end": "137375"
  },
  {
    "text": "I represent it as a product of,",
    "start": "137375",
    "end": "140380"
  },
  {
    "text": "um- of, uh, two matrices.",
    "start": "140380",
    "end": "142760"
  },
  {
    "text": "Um, Z- Z transposed and Z.",
    "start": "142760",
    "end": "145334"
  },
  {
    "text": "So essentially I'm saying I take my adjacency matrix, uh, A.",
    "start": "145335",
    "end": "148655"
  },
  {
    "text": "Here is, for example, one entry of it.",
    "start": "148655",
    "end": "150709"
  },
  {
    "text": "I want- because there is an edge, I want, you know,",
    "start": "150710",
    "end": "153140"
  },
  {
    "text": "the- the- the dot product of this row and that column to be value 1.",
    "start": "153140",
    "end": "157880"
  },
  {
    "text": "For example, for, uh, for this entry,",
    "start": "157880",
    "end": "160280"
  },
  {
    "text": "I would want the product of this,",
    "start": "160280",
    "end": "163235"
  },
  {
    "text": "uh, row and this particular column to be 0.",
    "start": "163235",
    "end": "166565"
  },
  {
    "text": "So this means that we take matrix A and factorize it as a product of Z transpose times Z.",
    "start": "166565",
    "end": "173940"
  },
  {
    "text": "Of course, because embedding dimension D number of rows in Z,",
    "start": "173950",
    "end": "179520"
  },
  {
    "text": "Z is much, much smaller than the number of, uh- uh, nodes, right?",
    "start": "179520",
    "end": "183470"
  },
  {
    "text": "So the matrix is very wide,",
    "start": "183470",
    "end": "185240"
  },
  {
    "text": "but not too- too deep.",
    "start": "185240",
    "end": "186995"
  },
  {
    "text": "This means that this exact approximation saying",
    "start": "186995",
    "end": "190400"
  },
  {
    "text": "A equals Z transpose times Z is generally not possible, right?",
    "start": "190400",
    "end": "195269"
  },
  {
    "text": "We don't have enough representation power",
    "start": "195270",
    "end": "197265"
  },
  {
    "text": "to really capture each edge but perfectly.",
    "start": "197265",
    "end": "199740"
  },
  {
    "text": "So we can learn this martix Z approximately.",
    "start": "199740",
    "end": "203645"
  },
  {
    "text": "So what we can see is,",
    "start": "203645",
    "end": "205235"
  },
  {
    "text": "let's find the matrix Z such that, you know, the, uh,",
    "start": "205235",
    "end": "208995"
  },
  {
    "text": "Z transpose times Z, uh,",
    "start": "208995",
    "end": "211319"
  },
  {
    "text": "the values of it are as similar to the values of A as possible.",
    "start": "211320",
    "end": "215520"
  },
  {
    "text": "How do I- how do I measure similarity now between",
    "start": "215520",
    "end": "218600"
  },
  {
    "text": "values is to use what is called Frobenius norm,",
    "start": "218600",
    "end": "221735"
  },
  {
    "text": "which is simply take an entry in A,",
    "start": "221735",
    "end": "224485"
  },
  {
    "text": "subtract an entry in this,",
    "start": "224485",
    "end": "226680"
  },
  {
    "text": "uh- in this matrix Z transpose times Z,",
    "start": "226680",
    "end": "229709"
  },
  {
    "text": "um, and take the square value of it and sum it up.",
    "start": "229710",
    "end": "232245"
  },
  {
    "text": "So Frobenius norm is simply, uh,",
    "start": "232245",
    "end": "235349"
  },
  {
    "text": "a sum of the square differences, uh,",
    "start": "235350",
    "end": "237405"
  },
  {
    "text": "between corresponding entries into, uh- into matrices.",
    "start": "237405",
    "end": "241935"
  },
  {
    "text": "Right? So this is now very similar to what we were",
    "start": "241935",
    "end": "245540"
  },
  {
    "text": "also doing in the previous, uh, lecture, right?",
    "start": "245540",
    "end": "249079"
  },
  {
    "text": "In the node embeddings lecture.",
    "start": "249080",
    "end": "250350"
  },
  {
    "text": "In the node embeddings lecture,",
    "start": "250350",
    "end": "251960"
  },
  {
    "text": "we were not using the L2 norm to- to define",
    "start": "251960",
    "end": "255710"
  },
  {
    "text": "the discrepancy between A and its factorization,",
    "start": "255710",
    "end": "260105"
  },
  {
    "text": "we used the softmax,",
    "start": "260105",
    "end": "263180"
  },
  {
    "text": "uh, uh, function instead of the L2 norm.",
    "start": "263180",
    "end": "266074"
  },
  {
    "text": "But the goal of approximating A with Z transpose times Z was the same.",
    "start": "266075",
    "end": "271710"
  },
  {
    "text": "So the conclusion is that the inner product decoder with note similarity defined by",
    "start": "271710",
    "end": "277729"
  },
  {
    "text": "edge- edge connectivity is equivalent to",
    "start": "277730",
    "end": "280850"
  },
  {
    "text": "the factorization of adjacency matrix, uh, A, right?",
    "start": "280850",
    "end": "284570"
  },
  {
    "text": "So we say we wanna directly approximate A by",
    "start": "284570",
    "end": "287510"
  },
  {
    "text": "the embeddings of the nodes such that if two nodes are linked,",
    "start": "287510",
    "end": "290780"
  },
  {
    "text": "then I want their dot product to be equal to 1.",
    "start": "290780",
    "end": "293825"
  },
  {
    "text": "And if they are not linked,",
    "start": "293825",
    "end": "295100"
  },
  {
    "text": "I want their dot product to be equal to 0.",
    "start": "295100",
    "end": "297775"
  },
  {
    "text": "So that's the, uh- the simplest way to define node similarity.",
    "start": "297775",
    "end": "302095"
  },
  {
    "text": "Now, in random walk-based similarity,",
    "start": "302095",
    "end": "305240"
  },
  {
    "text": "it turns out that when we have this more,",
    "start": "305240",
    "end": "308380"
  },
  {
    "text": "um, nuanced, more complex definition of, uh,",
    "start": "308380",
    "end": "311485"
  },
  {
    "text": "similarity, then, um, it is still- the entire process is still equivalent to",
    "start": "311485",
    "end": "317300"
  },
  {
    "text": "matrix factorization of just- of a more complex or transformed, uh, adjacency matrix.",
    "start": "317300",
    "end": "324060"
  },
  {
    "text": "So, um, here's the equation,",
    "start": "324060",
    "end": "326130"
  },
  {
    "text": "let me explain it, uh, on the next slide, what does it mean.",
    "start": "326130",
    "end": "329235"
  },
  {
    "text": "So it means that what we are really trying to do is",
    "start": "329235",
    "end": "332360"
  },
  {
    "text": "we are trying to factorize this particular,",
    "start": "332360",
    "end": "335659"
  },
  {
    "text": "um, transformed graph adjacency matrix.",
    "start": "335660",
    "end": "339495"
  },
  {
    "text": "So how is the the- uh,",
    "start": "339495",
    "end": "341775"
  },
  {
    "text": "what- what is the transformation?",
    "start": "341775",
    "end": "343440"
  },
  {
    "text": "The transformation is- here is the, um, adjacency matrix.",
    "start": "343440",
    "end": "348044"
  },
  {
    "text": "Here is the, um, one over the- the diagonal matrix,",
    "start": "348044",
    "end": "351720"
  },
  {
    "text": "these are the node degrees.",
    "start": "351720",
    "end": "353685"
  },
  {
    "text": "Um, this is now, uh,",
    "start": "353685",
    "end": "355350"
  },
  {
    "text": "raised to the power r,",
    "start": "355350",
    "end": "357120"
  },
  {
    "text": "where this- r goes between 1 and T where, uh,",
    "start": "357120",
    "end": "360585"
  },
  {
    "text": "capital T is the context window length is actually the length of the random walks that,",
    "start": "360585",
    "end": "367009"
  },
  {
    "text": "uh, we are simulating in,",
    "start": "367010",
    "end": "368960"
  },
  {
    "text": "uh- in DeepWalk or, uh, node2vec.",
    "start": "368960",
    "end": "371740"
  },
  {
    "text": "Um, volume of G is simply the sum of the entries of the adjacency matrix,",
    "start": "371740",
    "end": "376280"
  },
  {
    "text": "we see- which is twice the number of edges.",
    "start": "376280",
    "end": "378790"
  },
  {
    "text": "And the log b is a factor that corresponds to",
    "start": "378790",
    "end": "382040"
  },
  {
    "text": "the number of negative samples we are using in the optimization problem.",
    "start": "382040",
    "end": "386195"
  },
  {
    "text": "So basically, what this means is that you can compute,",
    "start": "386195",
    "end": "389585"
  },
  {
    "text": "um- in this case,",
    "start": "389585",
    "end": "391919"
  },
  {
    "text": "deep walk either by what we talked last time by simulating random walks",
    "start": "391920",
    "end": "396860"
  },
  {
    "text": "and defining the gradients and doing gradient descent or taking the adjacency matrix,",
    "start": "396860",
    "end": "402719"
  },
  {
    "text": "A, of your graph,",
    "start": "402720",
    "end": "404180"
  },
  {
    "text": "transforming it according to this equation where",
    "start": "404180",
    "end": "407585"
  },
  {
    "text": "we both take into account the lands of the random walks,",
    "start": "407585",
    "end": "410930"
  },
  {
    "text": "as well as the number of negative samples we use.",
    "start": "410930",
    "end": "413514"
  },
  {
    "text": "And if we factorize this, uh,",
    "start": "413515",
    "end": "415815"
  },
  {
    "text": "transform matrix, what I mean by this is if we go and, uh,",
    "start": "415815",
    "end": "419900"
  },
  {
    "text": "replace this A here with the transform matrix and then we try to solve,",
    "start": "419900",
    "end": "424294"
  },
  {
    "text": "uh, this- this equation,",
    "start": "424295",
    "end": "426054"
  },
  {
    "text": "uh, then the, uh, the solution to it,",
    "start": "426054",
    "end": "427875"
  },
  {
    "text": "this matrix Z, will be exactly the same as what,",
    "start": "427875",
    "end": "431955"
  },
  {
    "text": "uh- what we- what, uh,",
    "start": "431955",
    "end": "433485"
  },
  {
    "text": "our approach that we discussed in the previous lecture arrived to.",
    "start": "433485",
    "end": "437400"
  },
  {
    "text": "So, um, basically that is a very- very nice paper.",
    "start": "437400",
    "end": "441030"
  },
  {
    "text": "Um, if you wanna know more about this called network embedding as",
    "start": "441030",
    "end": "444450"
  },
  {
    "text": "matrix factorization that basically unifies DeepWalk LINE,",
    "start": "444450",
    "end": "448410"
  },
  {
    "text": "um, and a lot of other algorithms including node2vec in this,",
    "start": "448410",
    "end": "452405"
  },
  {
    "text": "uh, one mathematical, uh, framework.",
    "start": "452405",
    "end": "455675"
  },
  {
    "text": "So, um, as I said,",
    "start": "455675",
    "end": "458330"
  },
  {
    "text": "this random walk-based similarity can also be thought as, um, matrix factorization.",
    "start": "458330",
    "end": "464150"
  },
  {
    "text": "The equation I show here is for DeepWalk.",
    "start": "464150",
    "end": "466490"
  },
  {
    "text": "Um, you can derive similar type of matrix transformation for, uh, node2vec.",
    "start": "466490",
    "end": "473035"
  },
  {
    "text": "Just the matrix is even more complex because the random walk process of node2vec is,",
    "start": "473035",
    "end": "478325"
  },
  {
    "text": "uh- is more complex,",
    "start": "478325",
    "end": "479675"
  },
  {
    "text": "is more, uh- more nuanced.",
    "start": "479675",
    "end": "482254"
  },
  {
    "text": "So to conclude the lecture today,",
    "start": "482255",
    "end": "485045"
  },
  {
    "text": "I wanna talk a bit about the limitations and kind of",
    "start": "485045",
    "end": "487790"
  },
  {
    "text": "motivate what are we going to talk about next week.",
    "start": "487790",
    "end": "491160"
  },
  {
    "text": "So the limitations of node embeddings via this matrix factorization or this random walks,",
    "start": "491160",
    "end": "496490"
  },
  {
    "text": "uh, like- like I discussed in terms of, um,",
    "start": "496490",
    "end": "501030"
  },
  {
    "text": "node2vec, DeepWalk for multidimensional embeddings,",
    "start": "501030",
    "end": "504370"
  },
  {
    "text": "you can even think of PageRank as a single-dimensional embedding is that,",
    "start": "504370",
    "end": "509060"
  },
  {
    "text": "um, we cannot obtain embeddings for nodes not in the training set.",
    "start": "509060",
    "end": "513020"
  },
  {
    "text": "So this means if our graph is evolving,",
    "start": "513020",
    "end": "515180"
  },
  {
    "text": "if new nodes are appearing over time, then, uh,",
    "start": "515180",
    "end": "518115"
  },
  {
    "text": "the nodes that are not in the graph, uh,",
    "start": "518115",
    "end": "520770"
  },
  {
    "text": "at the time when we are computing embeddings won't have an embedding.",
    "start": "520770",
    "end": "524260"
  },
  {
    "text": "So if a newly added node 5,",
    "start": "524260",
    "end": "526100"
  },
  {
    "text": "for example, arrives, let's say, at the test time,",
    "start": "526100",
    "end": "528755"
  },
  {
    "text": "lets say- or is a new user in the social network,",
    "start": "528755",
    "end": "531290"
  },
  {
    "text": "we can- we cannot compute its embedding,",
    "start": "531290",
    "end": "533389"
  },
  {
    "text": "we have to redo everything from scratch.",
    "start": "533390",
    "end": "536090"
  },
  {
    "text": "We have to recompute all embeddings of all nodes in the network.",
    "start": "536090",
    "end": "539570"
  },
  {
    "text": "So this is very limiting.",
    "start": "539570",
    "end": "542065"
  },
  {
    "text": "The second important thing is that this, uh,",
    "start": "542065",
    "end": "544890"
  },
  {
    "text": "embeddings cannot capture structural similarity.",
    "start": "544890",
    "end": "549450"
  },
  {
    "text": "Uh, the reason being is, for example,",
    "start": "549450",
    "end": "551390"
  },
  {
    "text": "if I have the graph like I show here and I consider nodes,",
    "start": "551390",
    "end": "554795"
  },
  {
    "text": "uh, 1 and 11.",
    "start": "554795",
    "end": "556300"
  },
  {
    "text": "Even though they are in very different parts of the graph,",
    "start": "556300",
    "end": "559260"
  },
  {
    "text": "their local network structure, uh,",
    "start": "559260",
    "end": "561405"
  },
  {
    "text": "looks- looks quite similar.",
    "start": "561405",
    "end": "564210"
  },
  {
    "text": "Um, and DeepWalk and node2vec will come up with",
    "start": "564210",
    "end": "567410"
  },
  {
    "text": "very different embeddings for 11 and- and 1 because,",
    "start": "567410",
    "end": "571894"
  },
  {
    "text": "you know, 11 is neighbor with 12 and 13,",
    "start": "571895",
    "end": "574555"
  },
  {
    "text": "while 1 is neighbor with 2 and 3.",
    "start": "574555",
    "end": "577265"
  },
  {
    "text": "So this means that they- these types of embeddings",
    "start": "577265",
    "end": "581540"
  },
  {
    "text": "won't be able to capture this notion of local structural similarity,",
    "start": "581540",
    "end": "585454"
  },
  {
    "text": "but it will- more be able to capture who- who- what are the identities of the neighbors,",
    "start": "585455",
    "end": "590570"
  },
  {
    "text": "uh, next, uh- next to a given starting node.",
    "start": "590570",
    "end": "593525"
  },
  {
    "text": "Of course, if you were to define these over anonymous walks,",
    "start": "593525",
    "end": "597520"
  },
  {
    "text": "then, um, that would cap- allow us to capture the structure because, uh,",
    "start": "597520",
    "end": "601890"
  },
  {
    "text": "2 and 3, um, would- would- basically the identities of the nodes,",
    "start": "601890",
    "end": "606605"
  },
  {
    "text": "um, uh, are forgotten, and then we would be able to,",
    "start": "606605",
    "end": "609305"
  },
  {
    "text": "uh, solve this, uh, problem.",
    "start": "609305",
    "end": "611920"
  },
  {
    "text": "And then the last limitation I wanna talk about is- is that",
    "start": "611920",
    "end": "616264"
  },
  {
    "text": "this approaches cannot utilize node edge in graph level features,",
    "start": "616265",
    "end": "620465"
  },
  {
    "text": "meaning, you know, feature vectors attached to nodes,",
    "start": "620465",
    "end": "624370"
  },
  {
    "text": "uh, edges and graphs cannot naturally be incorporated in this framework.",
    "start": "624370",
    "end": "628810"
  },
  {
    "text": "Right? We basically create node embedding separately",
    "start": "628810",
    "end": "631475"
  },
  {
    "text": "from the features that these nodes might have.",
    "start": "631475",
    "end": "634440"
  },
  {
    "text": "So for example, you know, I use it in",
    "start": "634440",
    "end": "635870"
  },
  {
    "text": "a social network may have a set of properties, features,",
    "start": "635870",
    "end": "638495"
  },
  {
    "text": "attributes where are protein has a set of properties,",
    "start": "638495",
    "end": "641690"
  },
  {
    "text": "features that you would want to use them in creating, uh, embeddings.",
    "start": "641690",
    "end": "645600"
  },
  {
    "text": "And really, what are we going to talk next is,",
    "start": "645600",
    "end": "648480"
  },
  {
    "text": "um, you know, what are the solutions to these limitations?",
    "start": "648480",
    "end": "651695"
  },
  {
    "text": "The solution for- to these limitations is deep representation learning and,",
    "start": "651695",
    "end": "656330"
  },
  {
    "text": "uh, graph neural networks.",
    "start": "656330",
    "end": "657710"
  },
  {
    "text": "And in the next week,",
    "start": "657710",
    "end": "658940"
  },
  {
    "text": "we are going to move, uh,",
    "start": "658940",
    "end": "660330"
  },
  {
    "text": "to the topic of, uh,",
    "start": "660330",
    "end": "661815"
  },
  {
    "text": "graph neural networks that will allow us to",
    "start": "661815",
    "end": "664430"
  },
  {
    "text": "resolve these limitations that I have just, uh,",
    "start": "664430",
    "end": "667529"
  },
  {
    "text": "discussed, and will allow us to fuse the feature",
    "start": "667530",
    "end": "670330"
  },
  {
    "text": "information together with the structured information.",
    "start": "670330",
    "end": "673880"
  },
  {
    "text": "So to summarize today's lecture,",
    "start": "673880",
    "end": "676295"
  },
  {
    "text": "we talked about PageRank that measures important soft nodes in a graph.",
    "start": "676295",
    "end": "680425"
  },
  {
    "text": "Uh, we talked about it in three different ways,",
    "start": "680425",
    "end": "683720"
  },
  {
    "text": "we talked about it in terms of flow formulation,",
    "start": "683720",
    "end": "687545"
  },
  {
    "text": "in terms of links, and nodes.",
    "start": "687545",
    "end": "689329"
  },
  {
    "text": "We talked about it in terms of",
    "start": "689330",
    "end": "691355"
  },
  {
    "text": "random walk and stationary distribution of a random walk process.",
    "start": "691355",
    "end": "695079"
  },
  {
    "text": "And we also talked about it from the linear algebra point of view, uh,",
    "start": "695080",
    "end": "698495"
  },
  {
    "text": "by basically computing the eigenvector, uh,",
    "start": "698495",
    "end": "701690"
  },
  {
    "text": "to the particularly transformed,",
    "start": "701690",
    "end": "703880"
  },
  {
    "text": "uh, graph adjacency matrix.",
    "start": "703880",
    "end": "705590"
  },
  {
    "text": "Then we talked about, um,",
    "start": "705590",
    "end": "708180"
  },
  {
    "text": "extensions of PageRank particular random walk",
    "start": "708180",
    "end": "711290"
  },
  {
    "text": "with restarts and personalized PageRank where",
    "start": "711290",
    "end": "713449"
  },
  {
    "text": "basically the difference is in terms of changing the teleportation set.",
    "start": "713450",
    "end": "719790"
  },
  {
    "text": "Um, and then the last part of the lecture,",
    "start": "719790",
    "end": "722644"
  },
  {
    "text": "we talked about node embeddings based on random walks and",
    "start": "722645",
    "end": "726200"
  },
  {
    "text": "how they can be rep- expressed as a form of matrix factorization.",
    "start": "726200",
    "end": "730210"
  },
  {
    "text": "So this means that viewing graphs as matrices is a-plays",
    "start": "730210",
    "end": "735740"
  },
  {
    "text": "a key role in all of the above algorithms",
    "start": "735740",
    "end": "738695"
  },
  {
    "text": "where we can think of this in many different ways.",
    "start": "738695",
    "end": "741890"
  },
  {
    "text": "But mathematically at the end,",
    "start": "741890",
    "end": "743270"
  },
  {
    "text": "it's all about mat- matrix representation of the graph, factorizing this matrix,",
    "start": "743270",
    "end": "747920"
  },
  {
    "text": "computing eigenvectors, eigenvalues, and extracting connectivity information,",
    "start": "747920",
    "end": "752300"
  },
  {
    "text": "uh, out of it with the tools of, uh, linear algebra.",
    "start": "752300",
    "end": "756850"
  },
  {
    "text": "So, um, thank you very much,",
    "start": "756850",
    "end": "759540"
  },
  {
    "text": "everyone, for the- for the lecture.",
    "start": "759540",
    "end": "763089"
  }
]