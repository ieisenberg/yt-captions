[
  {
    "start": "0",
    "end": "5390"
  },
  {
    "text": "Hi, everyone. Welcome to CS224N we're\nabout two minutes in.",
    "start": "5390",
    "end": "10560"
  },
  {
    "text": "So let's get started. So today, we've got what I think\nis quite an exciting lecture",
    "start": "10560",
    "end": "16580"
  },
  {
    "text": "topic. We're going to talk\nabout self-attention and transformers.",
    "start": "16580",
    "end": "21630"
  },
  {
    "text": "So these are some ideas that\nare the foundation of most",
    "start": "21630",
    "end": "26689"
  },
  {
    "text": "of the modern advances in\nnatural language processing and actually AI systems in\na broad range of fields.",
    "start": "26690",
    "end": "34649"
  },
  {
    "text": "So it's a very, very fun topic\nBefore we get into that-- ",
    "start": "34650",
    "end": "45180"
  },
  {
    "text": "OK, before we get\ninto that, we're going to have a\ncouple of reminders. So there are brand\nnew lecture notes.",
    "start": "45180",
    "end": "51330"
  },
  {
    "text": "Woo, thanks, thank you. Yeah, I'm very\nexcited about them.",
    "start": "51330",
    "end": "59100"
  },
  {
    "text": "They go into-- they\npretty much follow along with what I'll be\ntalking about today",
    "start": "59100",
    "end": "65010"
  },
  {
    "text": "but go into considerably\nmore detail. Assignment four is\ndue a week from today.",
    "start": "65010",
    "end": "71430"
  },
  {
    "text": "Yeah, so the issues\nwith Azure continue. Thankfully, thankfully,\nour TAs especially",
    "start": "71430",
    "end": "82200"
  },
  {
    "text": "has tested that\nthis works on Colab, and the amount of\ntraining is such that a Colab session will\nallow you to train your machine",
    "start": "82200",
    "end": "91740"
  },
  {
    "text": "translation system. So if you don't have\na GPU, use Colab. We're continuing to\nwork on getting access",
    "start": "91740",
    "end": "97020"
  },
  {
    "text": "to more GPUs for assignment\nfive and the final project. We'll continue to update\nyou as we're able to,",
    "start": "97020",
    "end": "104440"
  },
  {
    "text": "but the usual systems\nthis year are no longer holding because\ncompanies are changing",
    "start": "104440",
    "end": "111078"
  },
  {
    "text": "their minds about things. OK, so our final\nproject proposal, you have a proposal of\nwhat you want to work on",
    "start": "111078",
    "end": "118420"
  },
  {
    "text": "for your final project. We will give you\nfeedback on whether we think it's a feasible\nidea or how to change it.",
    "start": "118420",
    "end": "125299"
  },
  {
    "text": "So this is very\nimportant because we want you to work on\nsomething that we think has a good\nchance of success for the rest of the quarter.",
    "start": "125300",
    "end": "131020"
  },
  {
    "text": "That's going to be out tonight. We'll have Ed announcement\nwhen it is out, and we want to get\nyou feedback on that",
    "start": "131020",
    "end": "137020"
  },
  {
    "text": "pretty quickly because\nyou'll be working on this. After assignment five is\ndone, really the major core",
    "start": "137020",
    "end": "143860"
  },
  {
    "text": "component of the course after\nthat is the final project.",
    "start": "143860",
    "end": "149230"
  },
  {
    "text": "OK, any questions? Cool, OK, OK, so\nlet's take a look",
    "start": "149230",
    "end": "158829"
  },
  {
    "text": "back into what we've done\nso far in this course and see what we were doing in\nnatural language processing.",
    "start": "158830",
    "end": "167092"
  },
  {
    "text": "What was our strategy? If you had a natural\nlanguage processing problem, and you wanted to say, take\nyour best effort attempt at it",
    "start": "167093",
    "end": "172960"
  },
  {
    "text": "without doing anything too\nfancy, you would have said, OK, I'm going to have a\nbidirectional LSTM instead",
    "start": "172960",
    "end": "179560"
  },
  {
    "text": "of a simple RNN. I'm going to use an LSTM\nto encode my sentences. I get bidirectional\ncontext, and if I",
    "start": "179560",
    "end": "187120"
  },
  {
    "text": "have an output that\nI'm trying to generate, I'll have a unidirectional\nLSTM that I was",
    "start": "187120",
    "end": "192760"
  },
  {
    "text": "going to generate one by one. So you have a translation\nor a parse or whatever. And so maybe I've encoded in a\nbidirectional LSTM, the source",
    "start": "192760",
    "end": "199900"
  },
  {
    "text": "sentence, and I'm one by\none decoding out the target with my uni-directional LSTM.",
    "start": "199900",
    "end": "206110"
  },
  {
    "text": "And then, also, I was going to\nuse something like attention to give flexible access to\nmemory if I felt like I needed",
    "start": "206110",
    "end": "215230"
  },
  {
    "text": "to do this look back and see\nwhere I want to translate from. OK, and this was just\nworking exceptionally well,",
    "start": "215230",
    "end": "221799"
  },
  {
    "text": "and we motivated\nattention through wanting to do machine translation. And you have this\nbottleneck where",
    "start": "221800",
    "end": "227830"
  },
  {
    "text": "you don't want to have to\nencode the whole source sentence in a single vector. OK, and in this lecture,\nwe have the same goal.",
    "start": "227830",
    "end": "234543"
  },
  {
    "text": "So we're to be looking at\na lot of the same problems that we did\npreviously, but we're going to use different\nbuilding blocks.",
    "start": "234543",
    "end": "240480"
  },
  {
    "text": "So we're going to\nsay if 2014 to 2017 ish, I was using recurrence\nthrough lots of trial",
    "start": "240480",
    "end": "248410"
  },
  {
    "text": "and error, years later, we\nhad these brand new building blocks that we could plug-in\ndirect replacement for LSTMs,",
    "start": "248410",
    "end": "257049"
  },
  {
    "text": "and they're going to allow for\njust a huge range of much more successful applications.",
    "start": "257050",
    "end": "264490"
  },
  {
    "text": "And so what are the issues\nwith recurrent neural networks we used to use, and\nwhat are the new systems",
    "start": "264490",
    "end": "270580"
  },
  {
    "text": "that we're going to use from\nthis point moving forward. OK, so one of the issues with\na recurrent neural network",
    "start": "270580",
    "end": "278620"
  },
  {
    "text": "is what we're going to call\nlinear interaction distance. So as we know, RNNs are unrolled\nleft to right or right to left",
    "start": "278620",
    "end": "287140"
  },
  {
    "text": "depending on the language\nand the direction. OK, but it encodes\nthis sort of notion of linear locality,\nwhich is useful",
    "start": "287140",
    "end": "293020"
  },
  {
    "text": "because if two words occur\nright next to each other, sometimes they're\nactually quite related. So tasty pizza.",
    "start": "293020",
    "end": "298480"
  },
  {
    "text": "They're nearby, and in the\nrecurrent neural network, you sort of encode tasty, and\nthen you sort of walk one step,",
    "start": "298480",
    "end": "305770"
  },
  {
    "text": "and you encode pizza. So nearby words do often\naffect each other's meanings,",
    "start": "305770",
    "end": "312310"
  },
  {
    "text": "but you have this problem where\nvery long distance dependencies can take a very long\ntime to interact.",
    "start": "312310",
    "end": "318889"
  },
  {
    "text": "So if I have this sentence,\nthe chef, so those are nearby. Those interact with\neach other, and then who",
    "start": "318890",
    "end": "327430"
  },
  {
    "text": "and then a bunch of stuff. Like, the chef who\nwent to the stores and picked up the ingredients\nand loves garlic and then was--",
    "start": "327430",
    "end": "337090"
  },
  {
    "text": "I actually have an RNN step. This application of the\nrecurrent weight matrix",
    "start": "337090",
    "end": "342970"
  },
  {
    "text": "and some element wise\nnon-linearities once, twice, three times, right,\nas many times",
    "start": "342970",
    "end": "348310"
  },
  {
    "text": "as there is potentially\nthe length of the sequence between chef and what's, right?",
    "start": "348310",
    "end": "353680"
  },
  {
    "text": "And it's the chef\nwho was so this is a long-distance\ndependency, should feel kind of\nrelated to the stuff",
    "start": "353680",
    "end": "359140"
  },
  {
    "text": "that we did in\ndependency syntax. But it's quite difficult\nto learn potentially",
    "start": "359140",
    "end": "366310"
  },
  {
    "text": "that these words\nshould be related. So if you have a lot\nof steps between words,",
    "start": "366310",
    "end": "379419"
  },
  {
    "text": "it can be difficult to learn\nthe dependencies between them. We talked about all\nthese gradient problems. LSTMs do a lot better at\nmodeling the gradients",
    "start": "379420",
    "end": "388160"
  },
  {
    "text": "across long distances than\nsimple recurrent neural networks, but it's not perfect.",
    "start": "388160",
    "end": "393889"
  },
  {
    "text": "And we already know\nthat this linear order isn't the right way to\nthink about sentences.",
    "start": "393890",
    "end": "400620"
  },
  {
    "text": "So if I wanted to learn\nthat it's the chef who was,",
    "start": "400620",
    "end": "406760"
  },
  {
    "text": "then I might have a\nhard time doing it because the gradients have to\npropagate from was to chef,",
    "start": "406760",
    "end": "413180"
  },
  {
    "text": "and really, I'd like more\ndirect connection between words that might be related in the\nsentence or in a document,",
    "start": "413180",
    "end": "419705"
  },
  {
    "text": "right, even if these are\ngoing to get much longer. So this linear interaction\ndistance problem.",
    "start": "419705",
    "end": "426170"
  },
  {
    "text": "We would like words\nthat might be related to be able to interact with each\nother in the neural networks computation sort of graph\nmore easily than sort of being",
    "start": "426170",
    "end": "437000"
  },
  {
    "text": "linearly far away\nso that we can learn these long-distance\ndependencies better.",
    "start": "437000",
    "end": "442830"
  },
  {
    "text": "And there's a\nrelated problem too that, again, comes back to\nthe recurrent neural networks dependence on the\nindex, on the index",
    "start": "442830",
    "end": "449360"
  },
  {
    "text": "into the sequence, often\ncalled a dependence on time. So in a recurrent\nneural network,",
    "start": "449360",
    "end": "455150"
  },
  {
    "text": "the forward and backward passes\nhave O of sequence length many. So that means just\nroughly sequence--",
    "start": "455150",
    "end": "461210"
  },
  {
    "text": "in this case, just\nsequence length many unparallelizable\noperations. So we know GPUs are great.",
    "start": "461210",
    "end": "467280"
  },
  {
    "text": "They can do a lot\nof operations at once as long as\nthere's no dependency between the operations\nin terms of time,",
    "start": "467280",
    "end": "474260"
  },
  {
    "text": "but you have to compute one\nand then compute the other. But in a recurrent\nneural network,",
    "start": "474260",
    "end": "479570"
  },
  {
    "text": "you can't actually compute the\nRNN hidden state for time step five before you compute the RNN\nhidden state for time step four",
    "start": "479570",
    "end": "486770"
  },
  {
    "text": "or time step three, right? And so you get this graph\nthat looks very similar where if I want to compute this hidden\nstate, so I've got some word,",
    "start": "486770",
    "end": "494330"
  },
  {
    "text": "I have zero operations\nI need to do before I can compute this state. I have one operation\nI can do before I",
    "start": "494330",
    "end": "500330"
  },
  {
    "text": "can compute this state. And as my sequence length\ngrows, I've got, OK, here,",
    "start": "500330",
    "end": "506000"
  },
  {
    "text": "I've got three operations I\nneed to do before I can compute the state with the number\nthree because I need to compute",
    "start": "506000",
    "end": "511100"
  },
  {
    "text": "this and this and that. So there's three\nunparallelizable operations",
    "start": "511100",
    "end": "517039"
  },
  {
    "text": "that I'm sort of glomming\nall the matrix multiplies and stuff into a single one. So one, two, three,\nand of course,",
    "start": "517039",
    "end": "523219"
  },
  {
    "text": "this grows with the\nsequence length as well. So down over here, so as\nthe sequence length grows,",
    "start": "523220",
    "end": "528740"
  },
  {
    "text": "I can't parallelize-- I can't just have a big GPU\njust kachunk with the matrix",
    "start": "528740",
    "end": "534680"
  },
  {
    "text": "multiply to compute this state\nbecause I need to compute all the previous states beforehand. ",
    "start": "534680",
    "end": "542209"
  },
  {
    "text": "OK, any questions about that? So those are these\ntwo related problems both with the\ndependence on time.",
    "start": "542210",
    "end": "548010"
  },
  {
    "text": "Yeah. Yeah, so I have a question on\nthe linear interaction issues. I thought that was the whole\npoint of the attention network",
    "start": "548010",
    "end": "553880"
  },
  {
    "text": "and then how maybe you want-- during the training\nof the actual cells",
    "start": "553880",
    "end": "559550"
  },
  {
    "text": "that depend more on each other. Can't we do something\nlike the attention and then work our\nway around that?",
    "start": "559550",
    "end": "566069"
  },
  {
    "text": "So the question is with the\nlinear interaction distance, wasn't this the\npoint of attention that it gets around that?",
    "start": "566070",
    "end": "571717"
  },
  {
    "text": "Can't we use something\nwith attention to help or does that just help? So it won't solve the\nparallelizability problem.",
    "start": "571718",
    "end": "577455"
  },
  {
    "text": "And in fact, everything we do\nin the rest of this lecture will be attention based, but\nwe'll get rid of the recurrence and just do attention\nmore or less.",
    "start": "577455",
    "end": "584120"
  },
  {
    "text": "So, yeah, it's a\ngreat intuition. Any other questions?",
    "start": "584120",
    "end": "589760"
  },
  {
    "text": " OK, cool, so if not recurrence,\nwhat about attention,",
    "start": "589760",
    "end": "598130"
  },
  {
    "text": "just a slide a slide back. And so we're going to get\ndeep into attention today,",
    "start": "598130",
    "end": "604470"
  },
  {
    "text": "but just for the\nsecond, attention treats each word's\nrepresentation as a query to access and\nincorporate information",
    "start": "604470",
    "end": "611459"
  },
  {
    "text": "from a set of values. So previously, we\nwere in a decoder. We were decoding out a\ntranslation of a sentence,",
    "start": "611460",
    "end": "617399"
  },
  {
    "text": "and we attended\nto the encoder so that we didn't have to store\nthe entire representation of the source sentence\ninto a single vector.",
    "start": "617400",
    "end": "624029"
  },
  {
    "text": "And here, today, we'll\nthink about attention within a single sentence. So I've got this sentence\nwritten out here with word one",
    "start": "624030",
    "end": "631110"
  },
  {
    "text": "through word T, in\nthis case, and right on these integers\nin the boxes, I'm",
    "start": "631110",
    "end": "636120"
  },
  {
    "text": "writing out the number of\nunparallelizable operations that you need to do before\nyou can compute these.",
    "start": "636120",
    "end": "641880"
  },
  {
    "text": "So for each word,\nyou can independently compute its embedding without\ndoing anything else previously",
    "start": "641880",
    "end": "646950"
  },
  {
    "text": "because the embedding just\ndepends on the word identity. And then with\nattention, if I wanted",
    "start": "646950",
    "end": "653550"
  },
  {
    "text": "to build an attention\nrepresentation of this word by looking at all the other\nwords in the sequence, that's one big operation.",
    "start": "653550",
    "end": "659700"
  },
  {
    "text": "And I can do them in\nparallel for all the words. So the attention\nfor this word I can do for the attention\nfor this word.",
    "start": "659700",
    "end": "666000"
  },
  {
    "text": "I don't need to walk left to\nright like I did for an RNN. Again, we'll get much\ndeeper into this, but this, you should\nhave the intuition",
    "start": "666000",
    "end": "673980"
  },
  {
    "text": "that it solves the\nlinear interaction problem and the\nnon-parallelizability problem because now, no matter how far\naway words are from each other,",
    "start": "673980",
    "end": "682110"
  },
  {
    "text": "I am potentially interacting. I might just attend to you even\nif you're very, very far away",
    "start": "682110",
    "end": "688020"
  },
  {
    "text": "independent of how\nfar away you are, and I also don't\nneed to walk along the sequence linearly long.",
    "start": "688020",
    "end": "694350"
  },
  {
    "text": "So I'm treating the\nwhole sequence at once. So the intuition\nis that attention",
    "start": "694350",
    "end": "700560"
  },
  {
    "text": "allows you to look\nvery far away at once, and it doesn't have this\ndependence on the sequence index that keeps us from\nparallelizing operations.",
    "start": "700560",
    "end": "707040"
  },
  {
    "text": "And so now the rest\nof the lecture, we'll talk in great\ndepth about attention. So maybe let's just move on.",
    "start": "707040",
    "end": "715230"
  },
  {
    "text": "OK, so let's think more\ndeeply about attention. One thing that you might\nthink of with attention",
    "start": "715230",
    "end": "722700"
  },
  {
    "text": "is that it's performing a fuzzy\nlookup in a key value store. So you have a bunch of\nkeys, a bunch of values,",
    "start": "722700",
    "end": "729420"
  },
  {
    "text": "and it's going to\nhelp you access that. So in an actual\nlookup table, just like a dictionary in Python,\nfor example, right, very simple.",
    "start": "729420",
    "end": "738279"
  },
  {
    "text": "You have a table of keys that\neach key maps to a value, and then you give it a query.",
    "start": "738280",
    "end": "743519"
  },
  {
    "text": "And the query matches\none of the keys, and then you return\nthe value, right? So I've got a\nbunch of keys here,",
    "start": "743520",
    "end": "751170"
  },
  {
    "text": "and my query matches the key. So I return the value. Simple, fair, easy.",
    "start": "751170",
    "end": "756960"
  },
  {
    "text": "OK, good. And in attention, so\njust like we saw before,",
    "start": "756960",
    "end": "764100"
  },
  {
    "text": "the query matches\nall keys softly. There's no exact match. You compute some\nsimilarity between the key",
    "start": "764100",
    "end": "771780"
  },
  {
    "text": "and all of the-- sorry, the\nquery and all of the keys, and then you weight the results. So you've got a query again.",
    "start": "771780",
    "end": "777690"
  },
  {
    "text": "You've got a bunch of keys. The query to different extents\nis similar to each of the keys,",
    "start": "777690",
    "end": "784149"
  },
  {
    "text": "and you will sort of measure\nthat similarity between 0 and 1 through a softmax, and then\nyou get the values out.",
    "start": "784150",
    "end": "792370"
  },
  {
    "text": "You average them via the\nweights of the similarity between the key and\nthe query and the keys.",
    "start": "792370",
    "end": "798430"
  },
  {
    "text": "You do a weighted sum\nwith those weights, and you get an output, right? So it really is\nquite a lookup table",
    "start": "798430",
    "end": "804650"
  },
  {
    "text": "but in this soft vector\nspace mushy sort of sense.",
    "start": "804650",
    "end": "809960"
  },
  {
    "text": "So I'm really doing\nsome kind of accessing into this information that's\nstored in the key value store,",
    "start": "809960",
    "end": "815560"
  },
  {
    "text": "but I'm sort of softly\nlooking at all of the results. ",
    "start": "815560",
    "end": "821070"
  },
  {
    "text": "OK, any questions there?  Cool, so what might\nthis look like?",
    "start": "821070",
    "end": "828510"
  },
  {
    "text": "So if I was trying to\nrepresent this sentence, I went to Stanford\nCS224N and learned--",
    "start": "828510",
    "end": "834030"
  },
  {
    "text": "so I'm trying to build a\nrepresentation of learned. I have a key for each word.",
    "start": "834030",
    "end": "841680"
  },
  {
    "text": "So this is this self-attention\nthing that we'll get into. I have a key for each word. A value for each word. I've got the query\nfor learned, and I've",
    "start": "841680",
    "end": "848520"
  },
  {
    "text": "got these sort of\nteal-ish bars up top, which might say how\nmuch you're going to try",
    "start": "848520",
    "end": "854190"
  },
  {
    "text": "to access each of the word. Like, oh, maybe 224N\nis not that important. CS, maybe that determines\nwhat I learned.",
    "start": "854190",
    "end": "860430"
  },
  {
    "text": "Stanford and then\nlearned, maybe that's important for\nrepresenting itself.",
    "start": "860430",
    "end": "865800"
  },
  {
    "text": "So you look across\nat the whole sentence and build up this soft\naccessing of information",
    "start": "865800",
    "end": "870960"
  },
  {
    "text": "across the sentence in order to\nrepresent learned in context. OK, so this is\njust a toy diagram.",
    "start": "870960",
    "end": "878670"
  },
  {
    "text": "So let's get into the math. So we're going to look\nat a sequence of words. That's W1 to N. Sequence\nof words in a vocabulary.",
    "start": "878670",
    "end": "886670"
  },
  {
    "text": "So this is like, Zuko\nmade his uncle tea. That's a good sequence. And for each word, we're going\nto embed it with this embedding",
    "start": "886670",
    "end": "893270"
  },
  {
    "text": "matrix just like we've been\ndoing in this class, right? So I have this embedding matrix\nthat goes from the vocabulary",
    "start": "893270",
    "end": "898820"
  },
  {
    "text": "size to the dimensionality\nD. So each word has a non-contextual\nonly dependent on itself",
    "start": "898820",
    "end": "906079"
  },
  {
    "text": "word embedding,\nand now I'm going to transform each word with\none of three different weight",
    "start": "906080",
    "end": "911780"
  },
  {
    "text": "matrices. So this is often called key\nquery value self-attention. So I have a matrix Q,\nwhich is in R D to D.",
    "start": "911780",
    "end": "919790"
  },
  {
    "text": "So this maps xi which is a\nvector of dimensionality D to another vector\nof dimensionality D,",
    "start": "919790",
    "end": "925430"
  },
  {
    "text": "and so that's going to\nbe a query vector, right? So it takes an xi and it sort of\nrotates it, shuffles it around,",
    "start": "925430",
    "end": "931910"
  },
  {
    "text": "stretches it, squishes\nit, makes it different, and now it's a query. And now for a different\nlearnable parameter",
    "start": "931910",
    "end": "937710"
  },
  {
    "text": "k, so that's another matrix, I'm\ngoing to come up with my keys. And with a different\nlearnable parameter V,",
    "start": "937710",
    "end": "945030"
  },
  {
    "text": "I'm going to come up\nwith my values, right? So I'm taking each of the\nnon-contextual word embeddings, each of these xi's and I'm\ntransforming each of them",
    "start": "945030",
    "end": "953220"
  },
  {
    "text": "to come up with my\nquery for that word, my key for that word, and\nmy value for that word.",
    "start": "953220",
    "end": "960000"
  },
  {
    "text": "OK, so every word is\ndoing each of these roles. Next, I'm going to compute\nall pairs of similarities",
    "start": "960000",
    "end": "966660"
  },
  {
    "text": "between the keys and queries. So in the toy example we saw,\nI was computing the similarity",
    "start": "966660",
    "end": "971819"
  },
  {
    "text": "between a single query for\nthe word learned and all of the keys for the\nentire sentence.",
    "start": "971820",
    "end": "977100"
  },
  {
    "text": "In this context, I'm computing\nall pairs of similarities between all keys and all\nvalues because I want",
    "start": "977100",
    "end": "983010"
  },
  {
    "text": "to represent all of these sums. So I've got this dot-- I'm just going to take the\ndot product between these two",
    "start": "983010",
    "end": "989820"
  },
  {
    "text": "vectors, right? So I've got qi. So this is saying the query\nfor word i dotted with the key",
    "start": "989820",
    "end": "994950"
  },
  {
    "text": "for word j, and I get this\nscore, which is a real value.",
    "start": "994950",
    "end": "1000605"
  },
  {
    "text": "It might be very large\nnegative, might be zero, might be very\nlarge and positive. And so that's like\nhow much should I",
    "start": "1000605",
    "end": "1006530"
  },
  {
    "text": "look at j in this lookup table. And then I do the\nsoftmax, right? So I softmax.",
    "start": "1006530",
    "end": "1012440"
  },
  {
    "text": "So I say that the\nactual weight that I'm going to look at j from i\nis softmax of this over all",
    "start": "1012440",
    "end": "1019160"
  },
  {
    "text": "of the possible indices. So it's like the\naffinity between i and j normalized by\nthe affinity between i",
    "start": "1019160",
    "end": "1025880"
  },
  {
    "text": "and all of the possible\nJ prime in the sequence. And then my output is just\nthe weighted sum of values.",
    "start": "1025880",
    "end": "1033990"
  },
  {
    "text": "So I've got this\noutput for word i. So maybe i is 1\nfor Zuko, and I'm representing it as the sum\nof these weights for all j.",
    "start": "1033990",
    "end": "1043039"
  },
  {
    "text": "So Zuko and made and\nhis and uncle and tea, and the value vector\nfor that word j.",
    "start": "1043040",
    "end": "1050000"
  },
  {
    "text": "I'm looking from i to\nj as much as alpha ij.",
    "start": "1050000",
    "end": "1055040"
  },
  {
    "text": "What's the dimension of Wi?  Oh, WI, you can either think\nof it as a symbol in vocab V.",
    "start": "1055040",
    "end": "1064840"
  },
  {
    "text": "So that's like-- you could think\nof it as a one hot vector in-- yeah, in this case, we are, I\nguess, thinking of it as this.",
    "start": "1064840",
    "end": "1071170"
  },
  {
    "text": "So one hot vector in\ndimensionality size of vocab. So in the matrix e,\nyou see that it's",
    "start": "1071170",
    "end": "1076780"
  },
  {
    "text": "RD by bars around v. That's\nsize of the vocabulary. So when I do e\nmultiplied by Wi that's",
    "start": "1076780",
    "end": "1085090"
  },
  {
    "text": "taking e which is D\nby v, multiplying it by W, which is v, and returning\na vector that's dimensionality",
    "start": "1085090",
    "end": "1092260"
  },
  {
    "text": "D. So W in that first\nline, like W1n, that's a matrix\nwhere it has maybe",
    "start": "1092260",
    "end": "1100690"
  },
  {
    "text": "like a column for every word in\nthat sentence and each column is a length of v? Yeah, usually, I guess we\nthink of it as having a--",
    "start": "1100690",
    "end": "1108039"
  },
  {
    "text": "I mean, if I'm putting the\nsequence length index first, you might think of it as\nhaving a row for each word.",
    "start": "1108040",
    "end": "1113889"
  },
  {
    "text": "But similarly, yeah, it's N,\nwhich is the sequence length, and then the second\ndimension would be V,",
    "start": "1113890",
    "end": "1119059"
  },
  {
    "text": "which is the vocabulary size. And then that gets mapped\nto this thing, which is sequence length by D.",
    "start": "1119060",
    "end": "1126380"
  },
  {
    "text": "Why do we learn two\ndifferent matrices, q and k, when q transpose-- qi\ntranspose kj is really",
    "start": "1126380",
    "end": "1133580"
  },
  {
    "text": "just one matrix [INAUDIBLE]? That's a great question. It ends up being\nbecause this will end up",
    "start": "1133580",
    "end": "1139580"
  },
  {
    "text": "being a low-rank\napproximation to that matrix. So it is for computational\nefficiency reasons,",
    "start": "1139580",
    "end": "1145400"
  },
  {
    "text": "although it also,\nI think, feels kind of nice in the presentation. But, yeah, what\nwe'll end up doing",
    "start": "1145400",
    "end": "1151309"
  },
  {
    "text": "is having a very low-rank\napproximation to QK transpose. And so you actually\ndo do it like this.",
    "start": "1151310",
    "end": "1157340"
  },
  {
    "text": "That's a good question. [INAUDIBLE] ii, so that\n[INAUDIBLE] specific?",
    "start": "1157340",
    "end": "1165830"
  },
  {
    "text": "Sorry, could you\nrepeat that for me? This eii, so the query of\nthe word dotted with the q",
    "start": "1165830",
    "end": "1172610"
  },
  {
    "text": "by itself, does it\nlook like an identity or does it look like any\nthings in particular? That's a good question.",
    "start": "1172610",
    "end": "1178250"
  },
  {
    "text": "OK, let me remember\nto repeat questions. So does eii for j equal to\ni, so looking at itself,",
    "start": "1178250",
    "end": "1184669"
  },
  {
    "text": "look like anything\nin particular? Does it look like the identity? Is that the question? OK, so it's unclear actually.",
    "start": "1184670",
    "end": "1192920"
  },
  {
    "text": "This question of should\nyou look at yourself for representing yourself. Well, it's going to be encoded\nby the matrices Q and K.",
    "start": "1192920",
    "end": "1200750"
  },
  {
    "text": "If I didn't have\nQ and K in there, if those were the identity\nmatrices, if Q is identity,",
    "start": "1200750",
    "end": "1205760"
  },
  {
    "text": "K is identity, then this would\nbe dot product with yourself, which is going to\nbe high on average.",
    "start": "1205760",
    "end": "1211567"
  },
  {
    "text": "You're pointing in the\nsame direction as yourself, but it could be that qxi\nand kxi might be arbitrarily",
    "start": "1211567",
    "end": "1219630"
  },
  {
    "text": "different from each other\nbecause Q could be the identity and K could map u to the\nnegative of yourself,",
    "start": "1219630",
    "end": "1227090"
  },
  {
    "text": "for example, so that you\ndon't look at yourself. So this is all learned in\npractice so you end up--",
    "start": "1227090",
    "end": "1232409"
  },
  {
    "text": "it can sort of\ndecide by learning whether you should be\nlooking at yourself",
    "start": "1232410",
    "end": "1237720"
  },
  {
    "text": "or not, and that's\nsome of the flexibility that parametrizing at s,\nq and k gives you that",
    "start": "1237720",
    "end": "1243150"
  },
  {
    "text": "wouldn't be there if I\njust used xi's everywhere in this equation.",
    "start": "1243150",
    "end": "1249700"
  },
  {
    "text": "I'm going to try to\nmove on, I'm afraid, because there's a lot\nto get on, but we'll keep talking about\nself-attention.",
    "start": "1249700",
    "end": "1255490"
  },
  {
    "text": "And so as more\nquestions come up, I can also potentially\nreturn back.",
    "start": "1255490",
    "end": "1261380"
  },
  {
    "text": "OK, so this is our\nbasic building block, but there are a bunch\nof barriers to using it",
    "start": "1261380",
    "end": "1268040"
  },
  {
    "text": "as a replacement for LSTMs. And so what we're going to do\nfor this portion of the lecture is talk about the\nminimal components",
    "start": "1268040",
    "end": "1274670"
  },
  {
    "text": "that we need in order to use\nself-attention as this very fundamental building block.",
    "start": "1274670",
    "end": "1281480"
  },
  {
    "text": "So we can't use it as it\nstands as I've presented it because there are\na couple of things",
    "start": "1281480",
    "end": "1286640"
  },
  {
    "text": "that we need to solve or fix. One of them is that there's\nno notion of sequence",
    "start": "1286640",
    "end": "1291680"
  },
  {
    "text": "order in self-attention. So what does this mean?",
    "start": "1291680",
    "end": "1297509"
  },
  {
    "text": "If I have a sentence,\nlike when I move over here to the whiteboard\nbriefly, and hopefully,",
    "start": "1297510",
    "end": "1302720"
  },
  {
    "text": "I'll write quite large. If I have a sentence\nlike Zuko made his uncle,",
    "start": "1302720",
    "end": "1315350"
  },
  {
    "text": "and let's say his\nuncle made Zuko.",
    "start": "1315350",
    "end": "1325730"
  },
  {
    "text": "If I were to embed\neach of these words, right, using its\nembedding matrix, the embedding matrix isn't\ndependent on the index",
    "start": "1325730",
    "end": "1334280"
  },
  {
    "text": "of the word. So this is the word at\nindex 1, 2, 3, 4 versus his",
    "start": "1334280",
    "end": "1339650"
  },
  {
    "text": "is over here and uncle, right? And so when I compute\nthe self-attention, and there's a lot more on\nthis in the lecture notes that",
    "start": "1339650",
    "end": "1346123"
  },
  {
    "text": "goes through a full example, the\nactual self-attention operation",
    "start": "1346123",
    "end": "1352100"
  },
  {
    "text": "will give you exactly\nthe same representations for this sequence, Zuko made\nhis uncle, as for this sequence,",
    "start": "1352100",
    "end": "1357919"
  },
  {
    "text": "his uncle made Zuko. And that's bad because\nthey're sentences that mean different things.",
    "start": "1357920",
    "end": "1363600"
  },
  {
    "text": "And so, right, it's this\nidea that self-attention is an operation on sets. You have a set of\nvectors that you're",
    "start": "1363600",
    "end": "1370700"
  },
  {
    "text": "going to perform\nself-attention on and nowhere does like the exact\nposition of the words come",
    "start": "1370700",
    "end": "1375860"
  },
  {
    "text": "into play directly. So we're going to encode\nthe position of words",
    "start": "1375860",
    "end": "1382350"
  },
  {
    "text": "through the keys, queries,\nand values that we have. So consider now representing\neach sequence index--",
    "start": "1382350",
    "end": "1389970"
  },
  {
    "text": "our sequences are going\nfrom 1 to n-- as a vector. So don't worry so far\nabout how it's being made,",
    "start": "1389970",
    "end": "1397050"
  },
  {
    "text": "but you can imagine\nrepresenting the number one, like the position\none, the position two, the position three as a\nvector in the dimensionality D",
    "start": "1397050",
    "end": "1405450"
  },
  {
    "text": "just like we're representing\nour keys, queries, and values. And so these are\nposition vectors.",
    "start": "1405450",
    "end": "1412980"
  },
  {
    "text": "If you were to\nwant to incorporate the information represented\nby these positions",
    "start": "1412980",
    "end": "1419640"
  },
  {
    "text": "into our self-attention,\nyou could just add these vectors, these pi\nvectors to the inputs, right?",
    "start": "1419640",
    "end": "1427980"
  },
  {
    "text": "So if I have this xi\nembedding of a word, which",
    "start": "1427980",
    "end": "1433290"
  },
  {
    "text": "is the word at position i but\nreally just represents-- oh, the word Zuko is here. Now I can say that,\noh, it's the word Zuko,",
    "start": "1433290",
    "end": "1439830"
  },
  {
    "text": "and it's at position 5\nbecause this vector represents position 5. ",
    "start": "1439830",
    "end": "1448670"
  },
  {
    "text": "OK, so how do we do this? And we might only\nhave to do this once. So we can do it once at the\nvery input to the network,",
    "start": "1448670",
    "end": "1456350"
  },
  {
    "text": "and then that is\nsort of sufficient. We don't have to do\nit at every layer because it sort of\nknows from the input.",
    "start": "1456350",
    "end": "1463580"
  },
  {
    "text": "So one way in which\npeople have done this is look at these sinusoidal\nposition representations.",
    "start": "1463580",
    "end": "1469440"
  },
  {
    "text": "So this looks a little bit\nlike this, where you have-- so this is a vector pi, which\nis in dimensionality d, right,",
    "start": "1469440",
    "end": "1475910"
  },
  {
    "text": "and each one of the dimensions,\nyou take the value i,' You modify it by some constant,\nand you pass it to the sine",
    "start": "1475910",
    "end": "1486190"
  },
  {
    "text": "or cosine function. And you get these\nsort of of values that vary according to the period--",
    "start": "1486190",
    "end": "1492070"
  },
  {
    "text": "differing periods depending\non the dimensionality. So I've got this\nrepresentation of a matrix",
    "start": "1492070",
    "end": "1497290"
  },
  {
    "text": "where d is the\nvertical dimension, and then n is the horizontal. And you can see that\nthere's sort of like, oh,",
    "start": "1497290",
    "end": "1504700"
  },
  {
    "text": "as I walk along, you see the\nperiod of the sine function going up and down and\neach of the dimensions d has a different period.",
    "start": "1504700",
    "end": "1511160"
  },
  {
    "text": "And so together,\nyou can represent a bunch of different\nsort of position indices, and it gives this\nintuition that, oh,",
    "start": "1511160",
    "end": "1519520"
  },
  {
    "text": "maybe the absolute position\nof a word isn't as important. You've got this sort of\nperiodicity of the sines and cosines, and maybe that\nallows you to extrapolate",
    "start": "1519520",
    "end": "1527800"
  },
  {
    "text": "to longer sequences. But in practice,\nthat doesn't work. But this is sort of\nlike an early notion",
    "start": "1527800",
    "end": "1534160"
  },
  {
    "text": "that this is still\nsometimes used for how to represent\nposition in transformers",
    "start": "1534160",
    "end": "1539360"
  },
  {
    "text": "and self-attention\nnetworks in general. So that's one idea.",
    "start": "1539360",
    "end": "1545059"
  },
  {
    "text": "You might think it's a\nlittle bit complicated. A little bit unintuitive.",
    "start": "1545060",
    "end": "1550100"
  },
  {
    "text": "Here's something that feels a\nlittle bit more deep learning. So we're just going to\nsay, oh, you know, I've",
    "start": "1550100",
    "end": "1557420"
  },
  {
    "text": "got a maximum\nsequence length of n, and I'm just going to learn a\nmatrix that's dimensionality",
    "start": "1557420",
    "end": "1562610"
  },
  {
    "text": "d by n, and that's going\nto represent my positions, and I'm going to learn\nit as a parameter, just like I learn\nevery other parameter.",
    "start": "1562610",
    "end": "1568889"
  },
  {
    "text": "And what do they mean? Oh, I have no idea, but\nit represents position. ",
    "start": "1568890",
    "end": "1575030"
  },
  {
    "text": "And so you just sort of\nadd this matrix to the xi, so your input embeddings,\nand it learns to fit to data.",
    "start": "1575030",
    "end": "1584130"
  },
  {
    "text": "So whatever\nrepresentation of position that's linear sort of index\nbased that you want you",
    "start": "1584130",
    "end": "1590390"
  },
  {
    "text": "can learn. And the cons are that,\nwell, you definitely now can't represent anything that's\nlonger than n words long.",
    "start": "1590390",
    "end": "1597950"
  },
  {
    "text": "No sequence longer than n. You can handle because-- well, you only learned a\nmatrix of this many positions.",
    "start": "1597950",
    "end": "1604320"
  },
  {
    "text": "And so in practice,\nyou'll get a model error. If you pass a self-attention\nmodel something longer than",
    "start": "1604320",
    "end": "1610980"
  },
  {
    "text": "length n, it will just sort\nof crash and say, I can't-- I can't do this.",
    "start": "1610980",
    "end": "1616309"
  },
  {
    "text": "And so this is what most\nsystems nowadays use. There are more flexible\nrepresentations of position,",
    "start": "1616310",
    "end": "1622250"
  },
  {
    "text": "including a couple\nin the lecture notes. You might want to look at\nthe relative linear position",
    "start": "1622250",
    "end": "1627940"
  },
  {
    "text": "or words before or\nafter each other but not their absolute position. There's also some\nrepresentations",
    "start": "1627940",
    "end": "1633190"
  },
  {
    "text": "that hearken back to\nour dependency syntax because like, oh,\nmaybe words that are close in the\ndependency parse tree",
    "start": "1633190",
    "end": "1639610"
  },
  {
    "text": "should be the\nthings that are sort of close in the\nself-attention operation.",
    "start": "1639610",
    "end": "1644919"
  },
  {
    "text": "OK, questions? In practice, do we\ntypically just make",
    "start": "1644920",
    "end": "1651430"
  },
  {
    "text": "n large enough that we don't\nrun into the issue of having something that could\nbe input longer than n?",
    "start": "1651430",
    "end": "1658905"
  },
  {
    "text": "So the question is, in\npractice, do we just make n long enough so that\nwe don't run into the problem where we're going to look\nat a text longer than n.",
    "start": "1658905",
    "end": "1666410"
  },
  {
    "text": "No, in practice,\nit's actually quite a problem, even today,\neven in the largest biggest language models,\nand can I fit this",
    "start": "1666410",
    "end": "1675560"
  },
  {
    "text": "prompt into ChatGPT or\nwhatever is the thing that you might see on Twitter? I mean, these\ncontinue to be issues.",
    "start": "1675560",
    "end": "1681800"
  },
  {
    "text": "And part of it is because the\nself-attention operation-- and we'll get into this\nlater in the lecture--",
    "start": "1681800",
    "end": "1687050"
  },
  {
    "text": "it's quadratic complexity\nin the sequence length. So you're going to spend n\nsquared sort of memory budget",
    "start": "1687050",
    "end": "1693862"
  },
  {
    "text": "in order to make\nsequence lengths longer. So in practice, this might be on\na large model, say 4,000 or so.",
    "start": "1693862",
    "end": "1701389"
  },
  {
    "text": "And it's 4,000, so you\ncan fit 4,000 words, which feels like a lot, but it's\nnot going to fit a novel. It's not going to\nfit a Wikipedia page.",
    "start": "1701390",
    "end": "1709700"
  },
  {
    "text": "And there are models that do\nlonger sequences for sure, and, again, we'll\ntalk a bit about it,",
    "start": "1709700",
    "end": "1715420"
  },
  {
    "text": "but no, this\nactually is an issue. ",
    "start": "1715420",
    "end": "1720490"
  },
  {
    "text": "How do you know that\nthe p you've learned is the position that is not\nany other without [INAUDIBLE]??",
    "start": "1720490",
    "end": "1728920"
  },
  {
    "text": "Yeah. So how do you know that\nthe p that you've learned, this matrix that you've learned,\nis representing position as opposed to anything else?",
    "start": "1728920",
    "end": "1735183"
  },
  {
    "text": "And the reason is the\nonly thing it correlates is position, right? So like when I\nsee these vectors,",
    "start": "1735183",
    "end": "1740760"
  },
  {
    "text": "when I'm adding this p\nmatrix to my x matrix, the word embeddings, I'm\nadding them together,",
    "start": "1740760",
    "end": "1746533"
  },
  {
    "text": "and the words that\nshow up at each index will vary depending on what\nword actually showed up there in the example, but the\np matrix never differs.",
    "start": "1746533",
    "end": "1753750"
  },
  {
    "text": "It's always exactly the\nsame at every index. And so it's the only\nthing in the data that it correlates with.",
    "start": "1753750",
    "end": "1759210"
  },
  {
    "text": "So you're sort of\nlearning it implicitly. Like this vector at\nindex 1 is always at index 1 for every example,\nfor every gradient update,",
    "start": "1759210",
    "end": "1766049"
  },
  {
    "text": "and nothing else\nco-occurs like that.",
    "start": "1766050",
    "end": "1771700"
  },
  {
    "text": "Yeah. So what you end up learning,\nI don't know, it's unclear, but it definitely\nallows you to know,",
    "start": "1771700",
    "end": "1776710"
  },
  {
    "text": "Oh, this word is with\nthis index at this. Yeah. OK.",
    "start": "1776710",
    "end": "1782020"
  },
  {
    "text": "Yeah. Just quickly, when you\nsay [INAUDIBLE] in space,",
    "start": "1782020",
    "end": "1787390"
  },
  {
    "text": "is this sequence right now\ndefined as a sequence-- so a sequence of words or--",
    "start": "1787390",
    "end": "1793270"
  },
  {
    "text": "I'm trying to figure out what\nthe unit is you're using. OK. So the question is, when this\nis quadratic in the sequence,",
    "start": "1793270",
    "end": "1799988"
  },
  {
    "text": "is that a sequence of words? Yeah, think of it as\na sequence of words. Sometimes there'll\nbe pieces that",
    "start": "1799988",
    "end": "1805060"
  },
  {
    "text": "are smaller than\nwords, which we'll go into in the next\nlecture, but yeah, think of this as a\nsequence of words but not necessarily\njust for a sentence,",
    "start": "1805060",
    "end": "1811990"
  },
  {
    "text": "maybe for an entire paragraph or\nan entire document or something like that.",
    "start": "1811990",
    "end": "1817030"
  },
  {
    "text": "OK, but the attention\npiece is word-based. Yeah, the attention is\nbased words to words.",
    "start": "1817030",
    "end": "1823560"
  },
  {
    "text": "OK. Cool. I'm going to move on. OK, right, so we\nhave another problem.",
    "start": "1823560",
    "end": "1830370"
  },
  {
    "text": "Another is that based on the\npresentation of self-attention that we've done, there's really\nno non-linearities for sort",
    "start": "1830370",
    "end": "1837720"
  },
  {
    "text": "of deep learning\nmagic; we're just sort of computing weighted\naverages of stuff. ",
    "start": "1837720",
    "end": "1844170"
  },
  {
    "text": "So if I apply self-attention\nand then apply self-attention again and again and\nagain and again, you",
    "start": "1844170",
    "end": "1850510"
  },
  {
    "text": "should get-- you should\nlook at the next lecture notes if you're\ninterested in this, that's actually quite cool. But what you end up doing\nis you're just re-averaging",
    "start": "1850510",
    "end": "1856920"
  },
  {
    "text": "value vectors together. So you're like computing\naverages of value vectors, and it ends up looking like\none big self-attention.",
    "start": "1856920",
    "end": "1863370"
  },
  {
    "text": "But there's an easy\nfix to this if you want sort of the traditional\ndeep learning magic. And you can just add a feed\nforward network to post-process",
    "start": "1863370",
    "end": "1870990"
  },
  {
    "text": "each output vector. So I've got a word\nhere, that's sort of the output of\nself-attention, and I'm going to pass it\nthrough, in this case,",
    "start": "1870990",
    "end": "1877840"
  },
  {
    "text": "I'm calling it a\nmulti-layer perceptron MLP. So this is a vector in\nrd that's going to be--",
    "start": "1877840",
    "end": "1883590"
  },
  {
    "text": "and it's taking in as\ninput a vector in rd, and you do the usual sort of\nmulti-layer perceptron thing,",
    "start": "1883590",
    "end": "1889720"
  },
  {
    "text": "right? Where you have the output, and\nyou multiply it by a matrix, pass it through a non-linearity,\nmultiply it by another matrix.",
    "start": "1889720",
    "end": "1895387"
  },
  {
    "text": "OK? And so what this looks\nlike in self-attention is that I've got this sort\nof sentence, the chef who dot",
    "start": "1895387",
    "end": "1900659"
  },
  {
    "text": "dot dot dot food, and I've\ngot my embeddings for it. I pass it through this whole\nbig self-attention block, right,",
    "start": "1900660",
    "end": "1906480"
  },
  {
    "text": "which looks at\nthe whole sequence and sort of incorporates\ncontext and all that, and then I pass each one\nindividually through a feed",
    "start": "1906480",
    "end": "1913140"
  },
  {
    "text": "forward layer, right? So this embedding,\nthat's sort of the output of the self-attention\nfor the word the,",
    "start": "1913140",
    "end": "1919470"
  },
  {
    "text": "is passed independently through\na multi-layer perceptron here, and that sort of--\nyou can think of it",
    "start": "1919470",
    "end": "1924690"
  },
  {
    "text": "as sort of combining\ntogether or processing the result of attention.",
    "start": "1924690",
    "end": "1931350"
  },
  {
    "text": "So there's a number of\nreasons why we do this. One of them also is\nthat you can actually stack a ton of computation into\nthese feed-forward networks",
    "start": "1931350",
    "end": "1940020"
  },
  {
    "text": "very, very efficiently, very\nparallelizable, very good for GPUs. But this is what's\ndone in practice.",
    "start": "1940020",
    "end": "1945520"
  },
  {
    "text": "So you do\nself-attention, and then you can pass it through this\nsort of position-wise feed",
    "start": "1945520",
    "end": "1950885"
  },
  {
    "text": "forward layer, right? Every word is\nprocessed independently by this feed forward network\nto process the result.",
    "start": "1950885",
    "end": "1959980"
  },
  {
    "text": "OK, so that's adding our sort\nof classical deep learning non-linearities\nfor self-attention.",
    "start": "1959980",
    "end": "1965650"
  },
  {
    "text": "And that's an easy\nfix for this sort of no non-linearities\nproblem in self-attention,",
    "start": "1965650",
    "end": "1970720"
  },
  {
    "text": "and then we have a\nlast issue before we have our final minimal\nself-attention building block",
    "start": "1970720",
    "end": "1976149"
  },
  {
    "text": "with which we can replace RNNS. And that's that-- when\nI've been writing out",
    "start": "1976150",
    "end": "1983200"
  },
  {
    "text": "all of these examples\nof self-attention, you can sort of look at\nthe entire sequence, right? And in practice, for some tasks\nsuch as machine translation",
    "start": "1983200",
    "end": "1992320"
  },
  {
    "text": "or language modeling,\nwhenever you want to define a probability\ndistribution over a sequence, you can't cheat and\nlook at the future.",
    "start": "1992320",
    "end": "2001200"
  },
  {
    "text": "So at every time\nstep, I could define the set of keys and\nqueries and values",
    "start": "2001200",
    "end": "2006900"
  },
  {
    "text": "to only include past words,\nbut this is inefficient-- bear with me. It's inefficient because you\ncan't parallelize it so well.",
    "start": "2006900",
    "end": "2014440"
  },
  {
    "text": "So instead, we compute\nthe entire n by n matrix just like I showed in the slide\ndiscussing self-attention,",
    "start": "2014440",
    "end": "2021039"
  },
  {
    "text": "and then I mask out\nwords in the future. So this score, eij, right,\nand I computed eij for all n",
    "start": "2021040",
    "end": "2027700"
  },
  {
    "text": "by n pairs of words,\nis equal to whatever it was before if the word that\nyou're looking at at index j",
    "start": "2027700",
    "end": "2037090"
  },
  {
    "text": "is an index that is less than\nor equal to where you are, index i, and it's equal\nto negative infinity-ish",
    "start": "2037090",
    "end": "2044140"
  },
  {
    "text": "otherwise if it's in the future. And when you softmax the\neij, negative infinity gets mapped to zero.",
    "start": "2044140",
    "end": "2050919"
  },
  {
    "text": "So now my attention\nis weighted zero, my weighted average\nis zero on the future,",
    "start": "2050920",
    "end": "2056359"
  },
  {
    "text": "so I can't look at it. What does this look like? So in order to encode\nthese words, the chef who,",
    "start": "2056360",
    "end": "2063940"
  },
  {
    "text": "and maybe the\nstart symbol there, I can look at\nthese words, right?",
    "start": "2063940",
    "end": "2069849"
  },
  {
    "text": "That's all pairs of words. And then I just gray out-- I sort of negative infinity\nout the words I can't look at.",
    "start": "2069850",
    "end": "2075760"
  },
  {
    "text": "So when encoding\nthe start symbol, I can just look at the start\nsymbol; when encoding the, I can look at the start symbol\nand the; when encoding chef,",
    "start": "2075760",
    "end": "2083888"
  },
  {
    "text": "I can look at start the chef,\nbut I can't look at who. And so with this\nrepresentation of chef",
    "start": "2083889",
    "end": "2092199"
  },
  {
    "text": "that is only looking\nat start the chef, I can define a\nprobability distribution",
    "start": "2092199",
    "end": "2097840"
  },
  {
    "text": "using this vector that\nallows me to predict who without having\ncheated by already looking ahead and seeing that,\nwell, who is the next word.",
    "start": "2097840",
    "end": "2105595"
  },
  {
    "text": " Questions.",
    "start": "2105595",
    "end": "2111840"
  },
  {
    "text": "So it says we're\nusing it in decoders. Do we do this for both the\nencoding layer and the decoding",
    "start": "2111840",
    "end": "2117660"
  },
  {
    "text": "layer, or for the\nencoding layer, are we allowing ourselves\nto look forward? The question is it\nsays here that we're",
    "start": "2117660",
    "end": "2123440"
  },
  {
    "text": "using this in a decoder. Do we also use it\nin the encoder? So this is the distinction\nbetween a bidirectional LSTM",
    "start": "2123440",
    "end": "2130860"
  },
  {
    "text": "and a unidirectional\nLSTM, right? So wherever you don't\nneed this constraint,",
    "start": "2130860",
    "end": "2137020"
  },
  {
    "text": "you probably don't use it. So if you're using an\nencoder on the source sentence of your machine\ntranslation problem,",
    "start": "2137020",
    "end": "2142269"
  },
  {
    "text": "you probably don't\ndo this masking because it's\nprobably good to let everything look at each other. And then whenever you\ndo need to use it,",
    "start": "2142270",
    "end": "2148720"
  },
  {
    "text": "because you have this\nautoregressive sort of probability of word\n1, probability of 2 given 1, 3 given 2 and one1\nthen you would use this.",
    "start": "2148720",
    "end": "2156280"
  },
  {
    "text": "So traditionally,\nyes, in decoders you will use it, in\nencoders you will not.",
    "start": "2156280",
    "end": "2162059"
  },
  {
    "text": "Yes. My question is a little\nbit philosophical.",
    "start": "2162060",
    "end": "2167410"
  },
  {
    "text": "How humans actually\ngenerate sentences by having some notion of the\nprobability of future words",
    "start": "2167410",
    "end": "2174940"
  },
  {
    "text": "before they say the\nwords that-- or before they choose the words that\nthey are currently speaking",
    "start": "2174940",
    "end": "2185100"
  },
  {
    "text": "or writing, generating? Good question. So the question is, isn't\nlooking ahead a little bit",
    "start": "2185100",
    "end": "2190410"
  },
  {
    "text": "and sort of predicting or\ngetting an idea of the words that you might say in the future\nhow humans generate language",
    "start": "2190410",
    "end": "2196290"
  },
  {
    "text": "instead of the strict constraint\nof not seeing into the future. Is that what you're-- OK.",
    "start": "2196290",
    "end": "2201480"
  },
  {
    "text": "So right, trying to plan ahead\nto see what I should do is",
    "start": "2201480",
    "end": "2206490"
  },
  {
    "text": "definitely an interesting\nidea, but when I am training the network, I can't--",
    "start": "2206490",
    "end": "2212310"
  },
  {
    "text": "if I'm teaching it to try\nto predict the next word, and if I give it\nthe answer, it's not going to learn anything useful.",
    "start": "2212310",
    "end": "2219670"
  },
  {
    "text": "So in practice, when\nI'm generating text, maybe it would be a good\nidea to make some guesses far into the future or have a\nhigh level plan or something,",
    "start": "2219670",
    "end": "2227619"
  },
  {
    "text": "but in training the network,\nI can't encode that intuition about how humans generate\nsequences of language",
    "start": "2227620",
    "end": "2235230"
  },
  {
    "text": "by just giving it the answer\nof the future directly at least because then\nit's just too easy. Like there's nothing to learn.",
    "start": "2235230",
    "end": "2241750"
  },
  {
    "text": "Yeah. But there might be interesting\nideas about maybe giving the network like a hint\nas to what kind of thing could come next for example.",
    "start": "2241750",
    "end": "2248325"
  },
  {
    "text": "But that's out of\nscope for this. Yeah. Yeah, question over here. So I understand\nlike why we would",
    "start": "2248325",
    "end": "2254398"
  },
  {
    "text": "want to mask the future for\nstuff like language models, but how does it apply\nto machine translation? Like why would we use it there?",
    "start": "2254398",
    "end": "2260380"
  },
  {
    "text": "Yeah, so in machine\ntranslation-- we're going to come\nover to this board",
    "start": "2260380",
    "end": "2265960"
  },
  {
    "text": "and hopefully get\na better marker. Yes. In machine translation, I have\na sentence like I like pizza,",
    "start": "2265960",
    "end": "2279280"
  },
  {
    "text": "and I want to be able to\ntranslate it [FRENCH]..",
    "start": "2279280",
    "end": "2285245"
  },
  {
    "text": " Nice. And so when I'm looking at\nthe I like pizza, right?",
    "start": "2285245",
    "end": "2295030"
  },
  {
    "text": "I get this as the input. And so I want self-attention\nwithout masking",
    "start": "2295030",
    "end": "2302619"
  },
  {
    "text": "because I want I to look at\nlike, and I to look at pizza, and like to look at\npizza, and I want it all.",
    "start": "2302620",
    "end": "2308950"
  },
  {
    "text": "And then when I'm\ngenerating this, if my tokens are like [FRENCH],,\nI want to-- in encoding this",
    "start": "2308950",
    "end": "2317050"
  },
  {
    "text": "word, I want to be able\nto look only at myself-- and we'll talk about\nencoder-decoder architectures",
    "start": "2317050",
    "end": "2323110"
  },
  {
    "text": "in this later in the lecture-- but I want to be able to look\nat myself none of the future",
    "start": "2323110",
    "end": "2328510"
  },
  {
    "text": "and all of this. And so what I'm talking about\nright now in this masking case is masking out with\nlike negative infinity,",
    "start": "2328510",
    "end": "2337720"
  },
  {
    "text": "all of these words. So that sort of attention score\nfrom [FRENCH] to everything else should be\nnegative infinity.",
    "start": "2337720",
    "end": "2345800"
  },
  {
    "text": "Yeah. Does that answer your question? Yes. Great. OK, let's move ahead.",
    "start": "2345800",
    "end": "2351400"
  },
  {
    "text": "OK. So that was our last big\nsort of building block issue",
    "start": "2351400",
    "end": "2356776"
  },
  {
    "text": "with self-attention. So this is what I\nwould call, and this is my personal opinion, a\nminimal self-attention building",
    "start": "2356777",
    "end": "2362420"
  },
  {
    "text": "block. You have self-attention,\nthe basis of the method, so that's sort of\nhere in the red,",
    "start": "2362420",
    "end": "2369130"
  },
  {
    "text": "and maybe we had the inputs\nto the sequence here, and then you embed it with\nthat embedding matrix e,",
    "start": "2369130",
    "end": "2374589"
  },
  {
    "text": "and then you add position\nembeddings, right? And then these three\narrows represent using the key, the\nvalue, and the query,",
    "start": "2374590",
    "end": "2382869"
  },
  {
    "text": "that sort of stylized there. This is often how you see\nthese diagrams, right? And so you pass it\nto self-attention",
    "start": "2382870",
    "end": "2390400"
  },
  {
    "text": "with the position\nrepresentation, right? So that specifies\nthe sequence order, because otherwise,\nyou'd have no idea what",
    "start": "2390400",
    "end": "2396880"
  },
  {
    "text": "order the words showed up in. You have the non-linearities in\nthe teal feed-forward network",
    "start": "2396880",
    "end": "2402400"
  },
  {
    "text": "there to sort of provide\nthat sort of squashing and sort of deep\nlearning expressivity,",
    "start": "2402400",
    "end": "2408819"
  },
  {
    "text": "and then you have\nmasking in order to have parallelizable\noperations that don't look at the future.",
    "start": "2408820",
    "end": "2415010"
  },
  {
    "text": "OK? So this is sort of our\nminimal architecture. And then up at the top above\nhere, so you have this thing;",
    "start": "2415010",
    "end": "2420970"
  },
  {
    "text": "maybe you repeat this\nsort of self-attention and feed forward many times. So self-attention, feed forward,\nself-attention, feed forward,",
    "start": "2420970",
    "end": "2427390"
  },
  {
    "text": "self-attention, feed\nforward, that's what I'm calling this block. And then maybe at the end of\nit, you predict something.",
    "start": "2427390",
    "end": "2433458"
  },
  {
    "text": "I don't know. We haven't really\ntalked about that. But you have these\nrepresentations, and then you predict\nthe next word",
    "start": "2433458",
    "end": "2438482"
  },
  {
    "text": "or you predict the sentiment\nor you predict whatever. So this is like a\nself-attention architecture.",
    "start": "2438482",
    "end": "2444615"
  },
  {
    "text": "OK, we're going to move on\nto the transformer next. So are there any questions? Yeah. [INAUDIBLE] just for encoders?",
    "start": "2444615",
    "end": "2452200"
  },
  {
    "text": "Other way around. We will use masking\nfor decoders, where I want to decode out\na sequence where I have",
    "start": "2452200",
    "end": "2460510"
  },
  {
    "text": "an informational constraint. Where to represent\nthis word properly, I cannot have the\ninformation of the future.",
    "start": "2460510",
    "end": "2466591"
  },
  {
    "text": "And masking when you\ndon't [INAUDIBLE],, right? Yeah. OK. ",
    "start": "2466592",
    "end": "2473690"
  },
  {
    "text": "OK. Great. So now let's talk\nabout the transformer. So what I've pitched\nto you is what",
    "start": "2473690",
    "end": "2480620"
  },
  {
    "text": "I call a minimal\nself-attention architecture. And I quite like\npitching it that way,",
    "start": "2480620",
    "end": "2488900"
  },
  {
    "text": "but really no one\nuses the architecture that was just up on the\nslide, the previous slide.",
    "start": "2488900",
    "end": "2494200"
  },
  {
    "text": "It doesn't work quite\nas well as it could, and there's a bunch\nof important details that we'll talk about now that\ngoes into the transformer.",
    "start": "2494200",
    "end": "2501630"
  },
  {
    "text": "But what I would hope though\nto sort of have you take away from that is that the\ntransformer architecture",
    "start": "2501630",
    "end": "2507920"
  },
  {
    "text": "as I'll present it now is\nnot necessarily the endpoint of our search for better and\nbetter ways of representing",
    "start": "2507920",
    "end": "2515270"
  },
  {
    "text": "language even though\nit's now ubiquitous and has been for\na couple of years. So think about\nthese sort of ideas",
    "start": "2515270",
    "end": "2521674"
  },
  {
    "text": "of the problems of\nusing self-attention and maybe ways of fixing some\nof the issues with transformers.",
    "start": "2521675",
    "end": "2528780"
  },
  {
    "text": "OK. So a transformer decoder\nis how we'll build systems like language models, right?",
    "start": "2528780",
    "end": "2534360"
  },
  {
    "text": "And so we've discussed this. It's like our decoder with\nour self-attention only sort of minimal architecture.",
    "start": "2534360",
    "end": "2540150"
  },
  {
    "text": "It's got a couple of extra\ncomponents, some of which have grayed out here, that\nwe'll go over one by one. The first that's\nactually different",
    "start": "2540150",
    "end": "2548550"
  },
  {
    "text": "is that we'll replace\nour self-attention with masking with masked\nmulti-head self-attention.",
    "start": "2548550",
    "end": "2555512"
  },
  {
    "text": "This ends up being\ncrucial; it's probably the most important distinction\nbetween the transformer",
    "start": "2555512",
    "end": "2560760"
  },
  {
    "text": "and this minimal architecture\nthat I've presented. So let's come back to our\ntoy example of attention,",
    "start": "2560760",
    "end": "2566550"
  },
  {
    "text": "where we've been trying to\nrepresent the word learned in the context of the sequence\nI went to Stanford CS 224n",
    "start": "2566550",
    "end": "2572550"
  },
  {
    "text": "and learned. And I was sort of\ngiving these teal bars to say, Oh, maybe intuitively\nyou look at various things",
    "start": "2572550",
    "end": "2580860"
  },
  {
    "text": "to build up your\nrepresentation of learned. But really there\nare varying ways",
    "start": "2580860",
    "end": "2586470"
  },
  {
    "text": "in which I want to look\nback at the sequence to see varying sort of\naspects of information",
    "start": "2586470",
    "end": "2593680"
  },
  {
    "text": "that I want to incorporate\ninto my representation. So maybe in this\nway, I sort of want",
    "start": "2593680",
    "end": "2599380"
  },
  {
    "text": "to look at Stanford\nCS 224n because I go, it's like entities, like\nyou learn different stuff",
    "start": "2599380",
    "end": "2606940"
  },
  {
    "text": "at Stanford CS 224n than\nyou do at other courses or other universities\nor whatever, right?",
    "start": "2606940",
    "end": "2611980"
  },
  {
    "text": "And so maybe I want to\nlook here for this reason. And maybe in another\nsense, I actually",
    "start": "2611980",
    "end": "2617710"
  },
  {
    "text": "want to look at\nthe word learned, and I want to look at I, i\nwent, and learned, right?",
    "start": "2617710",
    "end": "2623380"
  },
  {
    "text": "As you sort of like maybe\nsyntactically relevant words. Like there is very\ndifferent reasons for which I might want to\nlook at different things",
    "start": "2623380",
    "end": "2629753"
  },
  {
    "text": "in the sequence. And so trying to average it\nall out with a single operation of self-attention ends up being\nmaybe somewhat too difficult",
    "start": "2629753",
    "end": "2637930"
  },
  {
    "text": "in a way that will make\nprecise in assignment five. Nice, we'll get to do\na little bit more math.",
    "start": "2637930",
    "end": "2643960"
  },
  {
    "text": "OK. So any questions\nabout this intuition? ",
    "start": "2643960",
    "end": "2652030"
  },
  {
    "text": "[INAUDIBLE]  Yeah. So it should be an\napplication of attention,",
    "start": "2652030",
    "end": "2657060"
  },
  {
    "text": "just as I've presented it. So one independent, define\nthe keys, define the queries,",
    "start": "2657060",
    "end": "2662190"
  },
  {
    "text": "define the values. I'll define it more\nprecisely here. But think of it as\nI do attention once, and then I do it\nagain with different--",
    "start": "2662190",
    "end": "2670368"
  },
  {
    "text": "different parameters, being able\nto look at different things, et cetera. So if we have two\nseparate [INAUDIBLE],,",
    "start": "2670368",
    "end": "2676457"
  },
  {
    "text": "how do we ensure that they\nlearn different things? We do not-- OK, so\nthe question is, if we have two separate sets of\nweights trying to learn, say,",
    "start": "2676457",
    "end": "2682109"
  },
  {
    "text": "to do this and to do that,\nhow do we ensure that they learn different things? We do not ensure that\nthey hope-- that they",
    "start": "2682110",
    "end": "2687750"
  },
  {
    "text": "learn different things. And in practice they do,\nalthough not perfectly. So it ends up being the case\nthat you have some redundancy,",
    "start": "2687750",
    "end": "2695520"
  },
  {
    "text": "and you can sort of\ncut out some of these, but that's sort of\nout of scope for this. But we sort of\nhope-- just like we hope that different\nsort of dimensions",
    "start": "2695520",
    "end": "2702147"
  },
  {
    "text": "in our feed-forward layers will\nlearn different things because of lack of symmetry\nand whatever, that we hope that the heads\nwill start to specialize,",
    "start": "2702147",
    "end": "2709260"
  },
  {
    "text": "and that will mean they'll\nspecialize even more, and, yeah.",
    "start": "2709260",
    "end": "2714310"
  },
  {
    "text": "OK. All right. So in order to discuss\nmulti-head self-attention well,",
    "start": "2714310",
    "end": "2719620"
  },
  {
    "text": "we really need to talk\nabout the matrices; how we're going to implement\nthis in GPUs efficiently.",
    "start": "2719620",
    "end": "2725050"
  },
  {
    "text": "We're going to talk about\nthe sequence stacked form of attention. So we've been talking about\neach word sort of individually",
    "start": "2725050",
    "end": "2731589"
  },
  {
    "text": "as a vector in dimensionality\nD, But really we're going to be working on these as\nbig matrices that are stacked.",
    "start": "2731590",
    "end": "2738850"
  },
  {
    "text": "So I take all of my word\nembeddings x1 to xn, and I stack them\ntogether, and now I",
    "start": "2738850",
    "end": "2744100"
  },
  {
    "text": "have a big matrix that is\nin dimensionality R n by d. ",
    "start": "2744100",
    "end": "2749750"
  },
  {
    "text": "OK. And now with my\nmatrices k, q, and v,",
    "start": "2749750",
    "end": "2755080"
  },
  {
    "text": "I can just multiply them\nsort of on this side of x. So x is our n by d,\nk is our d by d, so n",
    "start": "2755080",
    "end": "2762789"
  },
  {
    "text": "by d times d by d\ngives you n by d again. So I can just compute\na big matrix multiply",
    "start": "2762790",
    "end": "2770410"
  },
  {
    "text": "on my whole sequence\nto multiply each one of the words with my key\nquery and value matrices",
    "start": "2770410",
    "end": "2775510"
  },
  {
    "text": "very efficiently. So this is sort of this\nvectorization idea, I don't want a for\nloop over the sequence,",
    "start": "2775510",
    "end": "2780740"
  },
  {
    "text": "I represent the sequence\nas a big matrix, and I just do one\nbig matrix multiply.",
    "start": "2780740",
    "end": "2787270"
  },
  {
    "text": "And then the output is\ndefined as this sort of inscrutable\nbit of math, which I'm going to go over visually.",
    "start": "2787270",
    "end": "2792565"
  },
  {
    "text": " So first, we're going to\ntake the key-query dot",
    "start": "2792565",
    "end": "2797770"
  },
  {
    "text": "products in one matrix. So we've got-- we've got\nXQ, which is our n by d,",
    "start": "2797770",
    "end": "2806740"
  },
  {
    "text": "and I've got XK transpose,\nwhich is our d by n. So n by d, d by n,\nthis is computing",
    "start": "2806740",
    "end": "2813790"
  },
  {
    "text": "all of the eij's, these scores\nfor self-attention, right? So this is all pairs of\nattention scores computed",
    "start": "2813790",
    "end": "2821050"
  },
  {
    "text": "in one big matrix multiply. OK?",
    "start": "2821050",
    "end": "2826340"
  },
  {
    "text": "So this is this big matrix here,\nnext I use the softmax, right?",
    "start": "2826340",
    "end": "2831710"
  },
  {
    "text": "So I softmax this over the\nsecond dimension, the second n dimension, and I get my\nsort of normalized scores,",
    "start": "2831710",
    "end": "2840829"
  },
  {
    "text": "and then I multiply with xv. So this is an n by n matrix\nmultiplied by an n by d matrix,",
    "start": "2840830",
    "end": "2847640"
  },
  {
    "text": "and what do I get? Well, this is just doing\nthe weighted average. So this is one big weighted\naverage contribution",
    "start": "2847640",
    "end": "2854180"
  },
  {
    "text": "on the whole matrix giving me\nmy whole self-attention output, an R n by d.",
    "start": "2854180",
    "end": "2859190"
  },
  {
    "text": "So I've just\nrestated identically the self-attention\noperations but computed in terms of matrices so\nthat you could do this",
    "start": "2859190",
    "end": "2866630"
  },
  {
    "text": "efficiently on a GPU.  OK.",
    "start": "2866630",
    "end": "2871790"
  },
  {
    "text": "So multi-headed attention. This is going to\ngive us-- and this is going to be\nimportant to compute",
    "start": "2871790",
    "end": "2877250"
  },
  {
    "text": "this in terms of the\nmatrices, which we'll see. This is going to\ngive us the ability to look in multiple places at\nonce for different reasons.",
    "start": "2877250",
    "end": "2884069"
  },
  {
    "text": "So sort of self attention looks\nwhere this dot product here is high, right?",
    "start": "2884070",
    "end": "2890510"
  },
  {
    "text": "This xi, the Q\nmatrix, the K matrix. But maybe we want to\nlook in different places",
    "start": "2890510",
    "end": "2897890"
  },
  {
    "text": "for different reasons,\nso we actually define multiple query,\nkey, and value matrices.",
    "start": "2897890",
    "end": "2904260"
  },
  {
    "text": "So I'm going to have\na bunch of heads. I'm going to have h\nself attention heads.",
    "start": "2904260",
    "end": "2910050"
  },
  {
    "text": "And for each head, I'm going\nto define an independent query, key, and value\nmatrix, and I'm going",
    "start": "2910050",
    "end": "2915110"
  },
  {
    "text": "to say that its shape is\ngoing to map from the model dimensionality to the model\ndimensionality over h.",
    "start": "2915110",
    "end": "2921273"
  },
  {
    "text": "So each one of these\nis doing projection down to a lower\ndimensional space. This can be for\ncomputational efficiency,",
    "start": "2921273",
    "end": "2927410"
  },
  {
    "text": "and I'll just apply self\nattention sort of independently for each output.",
    "start": "2927410",
    "end": "2933090"
  },
  {
    "text": "So this equation here\nis identical to the one we saw for single\nheaded self attention,",
    "start": "2933090",
    "end": "2938450"
  },
  {
    "text": "except I've got these sort\nof l indices everywhere. So I've got this lower\ndimensional thing,",
    "start": "2938450",
    "end": "2944430"
  },
  {
    "text": "I'm mapping to a lower\ndimensional space, and then I do have my lower\ndimensional value vector there,",
    "start": "2944430",
    "end": "2949600"
  },
  {
    "text": "so my output is an R d by h. But really you're doing exactly\nthe same kind of operation,",
    "start": "2949600",
    "end": "2954660"
  },
  {
    "text": "I'm just doing it\nh different times. And then you\ncombine the outputs. So I've done sort of\nlook in different places",
    "start": "2954660",
    "end": "2961950"
  },
  {
    "text": "with the different key,\nquery, and value matrices, and then I get each\nof their outputs,",
    "start": "2961950",
    "end": "2968039"
  },
  {
    "text": "and then I concatenate\nthem together. So each one is\ndimensionality d by h.",
    "start": "2968040",
    "end": "2973290"
  },
  {
    "text": "And I concatenate them together\nand then sort of mix them together with the final\nlinear transformation.",
    "start": "2973290",
    "end": "2979600"
  },
  {
    "text": "And so each head gets to\nlook at different things and construct their value\nvectors differently,",
    "start": "2979600",
    "end": "2985300"
  },
  {
    "text": "and then I combine the\nresult all together at once. OK, let's go through\nthis visually",
    "start": "2985300",
    "end": "2990920"
  },
  {
    "text": "because it's at\nleast helpful for me. So it's actually not more\ncostly to do this really",
    "start": "2990920",
    "end": "2998480"
  },
  {
    "text": "than it is to compute a\nsingle head of self-attention, and we'll see\nthrough the pictures. ",
    "start": "2998480",
    "end": "3005750"
  },
  {
    "text": "So in single headed\nself-attention, we computed XQ, and in multi-headed\nself-attention,",
    "start": "3005750",
    "end": "3010910"
  },
  {
    "text": "we'll also compute\nXQ the same way. So XQ is R n by d.",
    "start": "3010910",
    "end": "3016160"
  },
  {
    "text": " And then we can reshape it into\nR n, that's sequence length,",
    "start": "3016160",
    "end": "3024280"
  },
  {
    "text": "times the number of heads\ntimes the model dimensionality over the number of heads.",
    "start": "3024280",
    "end": "3030250"
  },
  {
    "text": "So I've just reshaped\nit to say now I've got a big three-axis\ntensor: the first axis",
    "start": "3030250",
    "end": "3036370"
  },
  {
    "text": "is the sequence\nlength, the second one is the number of heads, the\nthird is this reduced model dimensionality.",
    "start": "3036370",
    "end": "3041800"
  },
  {
    "text": "And that costs nothing, right? And do the same thing\nfor x and v. And then I transpose so that I've got\nthe head axis as the first axis.",
    "start": "3041800",
    "end": "3050540"
  },
  {
    "text": "And now I can compute\nall my other operations with the head axis\nkind of like a batch.",
    "start": "3050540",
    "end": "3057720"
  },
  {
    "text": "So what does this\nlook like in practice? Like instead of having\none big XQ matrix that's",
    "start": "3057720",
    "end": "3065069"
  },
  {
    "text": "model dimensionality D,\nI've got like, in this case, three XQ matrices of model\ndimensionality D by 3, D by 3,",
    "start": "3065070",
    "end": "3072720"
  },
  {
    "text": "D by 3, same thing with\nthe key matrix here. So everything looks\nalmost identical,",
    "start": "3072720",
    "end": "3078460"
  },
  {
    "text": "it's just a reshaping\nof the tensors. And now, at the\noutput of this, I've got three sets of\nattention scores",
    "start": "3078460",
    "end": "3086790"
  },
  {
    "text": "just by doing this reshape. And the cost is that, well,\neach of my attention heads",
    "start": "3086790",
    "end": "3093240"
  },
  {
    "text": "has only a d by h vector to work\nwith instead of a D dimensional vector to work with, right? So I get the output, I get these\nthree sets of pairs of scores.",
    "start": "3093240",
    "end": "3103079"
  },
  {
    "text": "I compute the\nsoftmax independently for each of the three, and then\nI have three value matrices",
    "start": "3103080",
    "end": "3109980"
  },
  {
    "text": "there as well, each of\nthem lower dimensional, and then finally, I get my\nthree different output vectors,",
    "start": "3109980",
    "end": "3116460"
  },
  {
    "text": "and I have a final\nlinear transformation to sort of mush them\ntogether, and I get an output.",
    "start": "3116460",
    "end": "3122380"
  },
  {
    "text": "And in summary, what\nthis allows you to do is exactly what I gave in\nthe toy example, which was I",
    "start": "3122380",
    "end": "3128529"
  },
  {
    "text": "can have each of\nthese heads look at different parts of a\nsequence for different reasons. ",
    "start": "3128530",
    "end": "3136390"
  },
  {
    "text": "Just a question-- so this\nis at a given block, right? Like all of these\nattention heads",
    "start": "3136390",
    "end": "3142150"
  },
  {
    "text": "are for a given\ntransformer block. Our next block would\nalso-- could also have three attention heads. The question is, are all\nof these for a given block?",
    "start": "3142150",
    "end": "3150232"
  },
  {
    "text": "And we'll talk\nabout a block again, but this block was\nthis sort of pair of self-attention and\nfeed-forward networks.",
    "start": "3150232",
    "end": "3156047"
  },
  {
    "text": "So you do like self-attention\nfeed-forward, that's one block. Another block is\nanother self-attention, another feed forward.",
    "start": "3156048",
    "end": "3161143"
  },
  {
    "text": "And the question is, are\nthe parameters shared between the blocks or not? Generally they are not shared. You'll have independent\nparameters at every block,",
    "start": "3161143",
    "end": "3168460"
  },
  {
    "text": "although there are\nsome exceptions. During loading on that,\nis it typically the case",
    "start": "3168460",
    "end": "3175309"
  },
  {
    "text": "that you have the same number\nof heads at each block, or do you vary the number\nof heads across blocks?",
    "start": "3175310",
    "end": "3181160"
  },
  {
    "text": "You have-- you\ndefinitely could vary it. People haven't found reason\nto vary-- so the question is,",
    "start": "3181160",
    "end": "3186210"
  },
  {
    "text": "do you have different\nnumbers of heads across the different\nblocks or do you have the same number\nof heads across all blocks?",
    "start": "3186210",
    "end": "3192905"
  },
  {
    "text": "The simplest thing\nis to just have it be the same everywhere,\nwhich is what people have done. I haven't yet found a\ngood reason to vary it,",
    "start": "3192905",
    "end": "3199040"
  },
  {
    "text": "but, well, it could\nbe interesting. It's definitely the case that\nafter training these networks,",
    "start": "3199040",
    "end": "3205340"
  },
  {
    "text": "you can actually just\ntotally zero out, remove some of the attention heads.",
    "start": "3205340",
    "end": "3210720"
  },
  {
    "text": "And I'd be curious to know\nif you could remove more or less depending on the layer\nindex, which might then say,",
    "start": "3210720",
    "end": "3219240"
  },
  {
    "text": "Oh, we should just have fewer. But again, it's not actually\nmore expensive to have a bunch. So people tend to instead\nset the number of heads",
    "start": "3219240",
    "end": "3226579"
  },
  {
    "text": "to be roughly so that you have a\nreasonable number of dimensions per head given the total model\ndimensionality D that you want.",
    "start": "3226580",
    "end": "3235420"
  },
  {
    "text": "So for example, I might\nwant at least 64 dimensions per head, which if D\nis 128, that tells me",
    "start": "3235420",
    "end": "3243839"
  },
  {
    "text": "how many heads I'm\ngoing to have roughly. So people tend to scale\nthe number of heads up with the model dimensionality.",
    "start": "3243840",
    "end": "3252329"
  },
  {
    "text": "Yeah. [INAUDIBLE] by slicing\nit in different columns, you're reducing the rank\nof the final matrix, right?",
    "start": "3252330",
    "end": "3259000"
  },
  {
    "text": "Yeah. But that doesn't really have\nany effect on the results? So the question is, by having\nthese sort of reduced XQ and XK",
    "start": "3259000",
    "end": "3269190"
  },
  {
    "text": "matrices, this is a very\nlow rank approximation. This little sliver\nand this little sliver",
    "start": "3269190",
    "end": "3275130"
  },
  {
    "text": "defining this whole big\nmatrix, this very low rank, is that not bad in practice?",
    "start": "3275130",
    "end": "3280200"
  },
  {
    "text": "No. Again, it's sort\nof the reason why we limit the number of\nheads depending on the model",
    "start": "3280200",
    "end": "3285780"
  },
  {
    "text": "dimensionality, because you\nwant intuitively at least some number of dimensions.",
    "start": "3285780",
    "end": "3290950"
  },
  {
    "text": "So 64 is sometimes done,\n128, something like that. But if you're not giving\neach head too much to do",
    "start": "3290950",
    "end": "3298260"
  },
  {
    "text": "and it's got sort of a simple\njob, you've got a lot of heads, it ends up sort of being OK.",
    "start": "3298260",
    "end": "3304140"
  },
  {
    "text": "All we really know is that\nempirically, it's way better to have more heads\nthan like one. ",
    "start": "3304140",
    "end": "3312280"
  },
  {
    "text": "Yes. I'm wondering, have\nthere been studies to see if information in one\nof the sets of the attention",
    "start": "3312280",
    "end": "3322120"
  },
  {
    "text": "scores-- like information\nthat one of them learns is consistent and\nrelated to each other,",
    "start": "3322120",
    "end": "3329530"
  },
  {
    "text": "or how are they related? So the question\nis, have there been studies to see if there is\nsort of consistent information",
    "start": "3329530",
    "end": "3336700"
  },
  {
    "text": "encoded by the attention heads? And yes, actually there's\nbeen quite a lot of study",
    "start": "3336700",
    "end": "3342520"
  },
  {
    "text": "in interpretability and\nanalysis of these models to try to figure out what roles,\nwhat sort of mechanistic roles",
    "start": "3342520",
    "end": "3348309"
  },
  {
    "text": "each of these heads takes on. And there's quite a bit\nof exciting results there around some attention\nheads learning",
    "start": "3348310",
    "end": "3355480"
  },
  {
    "text": "to pick out sort of the\nsyntactic dependencies or maybe doing a sort of a\nglobal averaging of context.",
    "start": "3355480",
    "end": "3363369"
  },
  {
    "text": "The question is\nquite nuanced though, because in a deep\nnetwork, it's unclear-- and we should talk about\nthis more offline--",
    "start": "3363370",
    "end": "3369460"
  },
  {
    "text": "but it's unclear if\nyou look at a word 10 layers deep in a network,\nwhat you're really looking at,",
    "start": "3369460",
    "end": "3374779"
  },
  {
    "text": "because it's already\nincorporated context from everyone else and\nit's a little bit unclear.",
    "start": "3374780",
    "end": "3380120"
  },
  {
    "text": "Active area of research. But I think I should\nmove on now to keep",
    "start": "3380120",
    "end": "3385130"
  },
  {
    "text": "discussing transformers. But yeah, if you want to talk\nmore about it, I'm happy to.",
    "start": "3385130",
    "end": "3390610"
  },
  {
    "text": "OK. So another sort of hack that\nI'm going to toss in here, and maybe they\nwouldn't call it hack,",
    "start": "3390610",
    "end": "3396220"
  },
  {
    "text": "but it's a nice little\nmethod to improve things, it's called scaled\ndot product attention.",
    "start": "3396220",
    "end": "3401960"
  },
  {
    "text": "So one of the issues with this\nkey-query value self attention is that when the model\ndimensionality becomes large,",
    "start": "3401960",
    "end": "3408820"
  },
  {
    "text": "the dot products between\nvectors, even random vectors, tend to become large.",
    "start": "3408820",
    "end": "3414859"
  },
  {
    "text": "And when that happens, the\ninputs to the softmax function can be very large, making\nthe gradients small.",
    "start": "3414860",
    "end": "3421240"
  },
  {
    "text": "So intuitively, if you have\ntwo random vectors and model dimensionality D, and you just\ndot product them together,",
    "start": "3421240",
    "end": "3427210"
  },
  {
    "text": "as D grows, their\ndot product grows, and expectation\ncould be very large. And so you sort of want to start\nout with everyone's attention",
    "start": "3427210",
    "end": "3435100"
  },
  {
    "text": "being very uniform, very\nflat, look everywhere. But if some dot products are\nvery large, then learning",
    "start": "3435100",
    "end": "3441910"
  },
  {
    "text": "will be inhibited. And so what you end\nup doing is you just-- for each of your heads, you just\nsort of divide all the scores",
    "start": "3441910",
    "end": "3449720"
  },
  {
    "text": "by this constant\nthat's determined by the model dimensionality. So as the vectors\ngrow very large,",
    "start": "3449720",
    "end": "3455600"
  },
  {
    "text": "their dot products don't, at\nleast at initialization time. So this is like a\nnice little important",
    "start": "3455600",
    "end": "3465350"
  },
  {
    "text": "but maybe not terribly-- like\nyeah, it's important to know.",
    "start": "3465350",
    "end": "3472110"
  },
  {
    "text": "And so that's called scaled\ndot product attention. From here on out, we'll\njust assume that we do this.",
    "start": "3472110",
    "end": "3478227"
  },
  {
    "text": "It's quite easy to\nimplement; you just do a little division in\nall of your computations.",
    "start": "3478227",
    "end": "3484900"
  },
  {
    "text": "OK. So now in the\ntransformer decoder. We've got a couple\nof other things that I have unfaded out here.",
    "start": "3484900",
    "end": "3492339"
  },
  {
    "text": "We have two big optimization\ntricks or optimization methods, I should say really, because\nthese are quite important.",
    "start": "3492340",
    "end": "3497770"
  },
  {
    "text": "They end up being\nvery important. We've got residual connections\nand layer normalization. And in transformer diagrams\nthat you see around the web,",
    "start": "3497770",
    "end": "3506829"
  },
  {
    "text": "they're often written together\nas this add and norm box.",
    "start": "3506830",
    "end": "3512170"
  },
  {
    "text": "And in practice, in the\ntransformer decoder, I'm going to apply masked\nmulti-head attention",
    "start": "3512170",
    "end": "3518410"
  },
  {
    "text": "and then do this sort of\noptimization; add a norm, then I'll do a feed\nforward application and then add a norm.",
    "start": "3518410",
    "end": "3524600"
  },
  {
    "text": "So this is quite important,\nso let's go over these two individual components.",
    "start": "3524600",
    "end": "3531599"
  },
  {
    "text": "The first is\nresidual connections. I mean, I think we've talked\nabout residual connections before, right? It's worth doing it again.",
    "start": "3531600",
    "end": "3537860"
  },
  {
    "text": "But it's really a good trick\nto help models train better. So just to recap,\nwe're going to take--",
    "start": "3537860",
    "end": "3544550"
  },
  {
    "text": "instead of having this sort\nof-- you have a layer i minus 1, and you pass it through a thing,\nmaybe it's self-attention,",
    "start": "3544550",
    "end": "3551450"
  },
  {
    "text": "maybe it's a\nfeed-forward network, now you've got layer i. I'm going to add the\nresult of layer i",
    "start": "3551450",
    "end": "3560300"
  },
  {
    "text": "to this sort of--\nto its input here. So now I'm saying I'm just\ngoing to compute the layer, and I'm going to add in\nthe input to the layer",
    "start": "3560300",
    "end": "3567590"
  },
  {
    "text": "so that I only have\nto learn the residual from the previous layer, right? So I've got this\nconnection here,",
    "start": "3567590",
    "end": "3573620"
  },
  {
    "text": "it's often written as this; this\nsort of like ooooh connection, OK, right?",
    "start": "3573620",
    "end": "3578890"
  },
  {
    "text": "Goes around. And you should think\nthat the gradient is just really great through the\nresidual connection, right? Like if ah, I've got vanishing\nor exploding gradient--",
    "start": "3578890",
    "end": "3587619"
  },
  {
    "text": "vanishing gradients\nthrough this layer, well, I can at least\nlearn everything behind it because I've got this\nresidual connection where",
    "start": "3587620",
    "end": "3594070"
  },
  {
    "text": "the gradient is 1,\nbecause it's the identity. So this is really nice.",
    "start": "3594070",
    "end": "3599650"
  },
  {
    "text": "And it also maybe is\nlike a bias-- at least at initialization,\neverything looks a little bit like the identity\nfunction now, right?",
    "start": "3599650",
    "end": "3606520"
  },
  {
    "text": "Because if the\ncontribution of the layer is somewhat small because all\nof your weights are small, and I have the addition\nfrom the input,",
    "start": "3606520",
    "end": "3613750"
  },
  {
    "text": "maybe the whole thing\nlooks a little bit like the identity,\nwhich might be a good sort of place to start.",
    "start": "3613750",
    "end": "3620292"
  },
  {
    "text": "And there are really\nnice visualizations; I just love this visualization. This is your loss\nlandscape, right?",
    "start": "3620292",
    "end": "3626380"
  },
  {
    "text": "So your gradient\ndescent, and you're trying to traverse the\nmountains of the loss landscape. This is like the\nparameter space,",
    "start": "3626380",
    "end": "3632380"
  },
  {
    "text": "and down is better in\nyour loss function. And it's really hard, so you\nget stuck in some local optima,",
    "start": "3632380",
    "end": "3638000"
  },
  {
    "text": "and you can't sort of\nfind your way to get out. And then this is your\nresidual connections.",
    "start": "3638000",
    "end": "3643160"
  },
  {
    "text": "I mean, come on, you\njust sort of walk down. It's not actually I guess really\nhow it works all the time,",
    "start": "3643160",
    "end": "3650420"
  },
  {
    "text": "but I really love this. It's great. ",
    "start": "3650420",
    "end": "3655525"
  },
  {
    "text": "OK.  So yeah, we've seen\nresidual connections, we should move on to\nlayer normalization.",
    "start": "3655525",
    "end": "3662520"
  },
  {
    "text": "So layer norm is another\nthing to help your model train faster.",
    "start": "3662520",
    "end": "3668010"
  },
  {
    "text": "And the intuitions around\nlayer normalization and sort",
    "start": "3668010",
    "end": "3675150"
  },
  {
    "text": "of the empiricism of it\nworking very well maybe aren't perfectly like,\nlet's say, connected.",
    "start": "3675150",
    "end": "3680910"
  },
  {
    "text": "But you should\nimagine, I suppose, that we want to, say, there's\nvariation within each layer.",
    "start": "3680910",
    "end": "3689730"
  },
  {
    "text": "Things can get very big,\nthings can get very small. That's not actually informative\nbecause of variations",
    "start": "3689730",
    "end": "3696630"
  },
  {
    "text": "between maybe the gradients or\nI've got weird things going on",
    "start": "3696630",
    "end": "3703140"
  },
  {
    "text": "in my layers that I\ncan't totally control, I haven't been able to make\neverything behave sort of nicely where everything stays\nroughly the same norm, maybe",
    "start": "3703140",
    "end": "3710640"
  },
  {
    "text": "some things explode,\nmaybe some things shrink. And I want to cut down on sort\nof uninformative variation",
    "start": "3710640",
    "end": "3719549"
  },
  {
    "text": "between layers. So I'm going to let x and\nRd be an individual word vector in the model.",
    "start": "3719550",
    "end": "3725050"
  },
  {
    "text": "So this is like at a\nsingle index one vector. And what I'm going to try\nto do is just normalize it.",
    "start": "3725050",
    "end": "3732670"
  },
  {
    "text": "Normalize it in the sense of\nit's got a bunch of variation, and I'm going to cut\nout on everything--",
    "start": "3732670",
    "end": "3737710"
  },
  {
    "text": "I'm going to\nnormalize it to unit mean and standard deviation. So I'm going to estimate\nthe mean here across--",
    "start": "3737710",
    "end": "3746529"
  },
  {
    "text": "so for all of the\ndimensions in the vector. So j equals 1 to the\nmodel dimensionality.",
    "start": "3746530",
    "end": "3752590"
  },
  {
    "text": "I'm going to sum up the value. So I've got this\none big word vector, and I sum up all the values. Division by d here, right?",
    "start": "3752590",
    "end": "3759369"
  },
  {
    "text": "That's the mean. I'm going to have my estimate\nof the standard deviation. Again, these should\nsay estimates.",
    "start": "3759370",
    "end": "3765040"
  },
  {
    "text": "This is my simple estimate\nof the standard deviation of the values within\nthis one vector.",
    "start": "3765040",
    "end": "3770250"
  },
  {
    "text": "And I'm just going to-- and then possibly I guess I\ncan have learned parameters",
    "start": "3770250",
    "end": "3778020"
  },
  {
    "text": "to try to scale back out in\nterms of multiplicatively and additively here,\nbut that's optional.",
    "start": "3778020",
    "end": "3785430"
  },
  {
    "text": "We're going to compute this\nstandardization, right? Where I'm going to\ntake my vector x, subtract out the mean, divide\nby the standard deviation,",
    "start": "3785430",
    "end": "3792750"
  },
  {
    "text": "plus this epsilon\nsort of constant. If there's not a\nlot of variation, I don't want things to explode.",
    "start": "3792750",
    "end": "3797830"
  },
  {
    "text": "So I'm going to have\nthis epsilon there. That's close to zero. So this part here, x\nminus mu over square root",
    "start": "3797830",
    "end": "3805109"
  },
  {
    "text": "sigma plus epsilon, is\nsaying take all the variation and sort of normalize it to unit\nmean and standard deviation.",
    "start": "3805110",
    "end": "3812250"
  },
  {
    "text": "And then maybe I want to sort of\nscale it, stretch it back out, and then maybe add an offset\nbeta that I've learned.",
    "start": "3812250",
    "end": "3820595"
  },
  {
    "text": "Although in practice\nactually, this part, and we'll discuss this in the\nlecture notes, in practice, this part maybe isn't\nactually that important.",
    "start": "3820595",
    "end": "3827670"
  },
  {
    "text": "But so layer normalization,\nyeah, you're-- you can think of this as when\nI get the output of layer",
    "start": "3827670",
    "end": "3834070"
  },
  {
    "text": "normalization,\nit's going to be-- it's going to sort\nof look nice and look similar to the next layer\nindependent of what's",
    "start": "3834070",
    "end": "3839830"
  },
  {
    "text": "gone on because it's\ngoing to be unit mean and standard deviation. So maybe that makes\nfor a better thing to learn off of\nfor the next layer.",
    "start": "3839830",
    "end": "3846280"
  },
  {
    "text": " OK. Any questions for\nresidual or layer norm?",
    "start": "3846280",
    "end": "3852290"
  },
  {
    "text": "Yes. [INAUDIBLE] to subtract the\n[INAUDIBLE] the vector x? Yeah, that's a good question.",
    "start": "3852290",
    "end": "3858690"
  },
  {
    "text": "When I subtract the scalar\nmu from the vector x, I broadcast mu to dimensionality\nd and remove mu from all d.",
    "start": "3858690",
    "end": "3867710"
  },
  {
    "text": "Yeah, good point. Thank you. That was unclear. ",
    "start": "3867710",
    "end": "3875190"
  },
  {
    "text": "In the fourth bullet, maybe\nI'm confused, is it divided? Should it be divided\nby d or [INAUDIBLE]??",
    "start": "3875190",
    "end": "3882495"
  },
  {
    "text": "Sorry, can you repeat that. In the fourth bullet point when\nyou're calculating the mean, is it divided by d or is it--",
    "start": "3882495",
    "end": "3889589"
  },
  {
    "text": "or maybe I'm just [INAUDIBLE]. I think it is divided by d. Yeah, cool. ",
    "start": "3889590",
    "end": "3895560"
  },
  {
    "text": "So this is the average\ndeviation from the mean of all of the-- yeah. Yes.",
    "start": "3895560",
    "end": "3901050"
  },
  {
    "text": "So if you have five words in\nthe sentence [INAUDIBLE] norm, do you normalize based on the\nstatistics of these five words",
    "start": "3901050",
    "end": "3909450"
  },
  {
    "text": "or just one word [INAUDIBLE]? So the question is, if I have\nfive words in the sequence, do I normalize by\nsort of aggregating",
    "start": "3909450",
    "end": "3917250"
  },
  {
    "text": "the statistics to estimate mu\nand sigma across all the five words, share their statistics or\ndo independently for each word?",
    "start": "3917250",
    "end": "3924000"
  },
  {
    "text": "This is a great question,\nwhich I think in all the papers that discuss transformers\nis under-specified.",
    "start": "3924000",
    "end": "3929970"
  },
  {
    "text": "You do not share across\nthe five words, which is somewhat confusing to me.",
    "start": "3929970",
    "end": "3935290"
  },
  {
    "text": "So each of the five words is\ndone completely independently. You could have shared\nacross the five words",
    "start": "3935290",
    "end": "3941289"
  },
  {
    "text": "and said that my estimate\nof the statistics are just based on all\nfive, but you do not.",
    "start": "3941290",
    "end": "3949590"
  },
  {
    "text": "I can't pretend I\nunderstand totally why. [INAUDIBLE] extension\nof that [INAUDIBLE] for example, per batch or per\noutput for the same position",
    "start": "3949590",
    "end": "3960140"
  },
  {
    "text": "[INAUDIBLE]? So a similar question. The question is, if you have\na batch of sequences, right?",
    "start": "3960140",
    "end": "3966710"
  },
  {
    "text": "So just like we're doing\nbatch-based training, do you for a single word, now\nwe don't share across a sequence",
    "start": "3966710",
    "end": "3973255"
  },
  {
    "text": "index for sharing\nthe statistics, but do you share\nacross the batch? And the answer is no. You also do not share\nacross the batch.",
    "start": "3973255",
    "end": "3979430"
  },
  {
    "text": "In fact, layer\nnormalization was sort of invented as a replacement\nfor batch normalization",
    "start": "3979430",
    "end": "3985130"
  },
  {
    "text": "which did just that. And the issue with\nbatch normalization is that now your forward\npass sort of depends in a way",
    "start": "3985130",
    "end": "3990920"
  },
  {
    "text": "that you don't like on examples\nthat should be not related to your example. And so yeah, you don't share\nstatistics across the batch.",
    "start": "3990920",
    "end": "3997565"
  },
  {
    "text": " OK. Cool.",
    "start": "3997565",
    "end": "4004140"
  },
  {
    "text": "OK, so now we have our\nfull transformer decoder, and we have our blocks.",
    "start": "4004140",
    "end": "4010540"
  },
  {
    "text": "So in this sort of slightly\ngrayed out thing here that says repeat for number of\nencoder or, sorry, decoder",
    "start": "4010540",
    "end": "4017250"
  },
  {
    "text": "blocks, each block consists of-- I pass it through\nself-attention. And then my add and norm, right?",
    "start": "4017250",
    "end": "4024630"
  },
  {
    "text": "So I've got this\nresidual connection here that goes around, and I've got\nthe layer normalization there",
    "start": "4024630",
    "end": "4030270"
  },
  {
    "text": "and then a feed-forward layer\nand then another add and norm. And so that set of\nfour operations,",
    "start": "4030270",
    "end": "4037950"
  },
  {
    "text": "I apply for some number of\ntimes, number of blocks, so that whole thing is\ncalled a single block.",
    "start": "4037950",
    "end": "4043710"
  },
  {
    "text": "And that's it, that's the\ntransformer decoder as it is.",
    "start": "4043710",
    "end": "4048930"
  },
  {
    "text": " Cool. So that's a whole\narchitecture right there. We've solved things like\nneeding to represent position,",
    "start": "4048930",
    "end": "4056570"
  },
  {
    "text": "we've solved things\nlike not being able to look into\nthe future, we've",
    "start": "4056570",
    "end": "4062000"
  },
  {
    "text": "solved a lot of different\noptimization problems. You've got a question. Yes. [INAUDIBLE] is the multi-headed\nattention [INAUDIBLE]??",
    "start": "4062000",
    "end": "4068000"
  },
  {
    "text": "Yes. Yes, masked\nmulti-head attention. Yeah. With the dot product\nscaling with the square root",
    "start": "4068000",
    "end": "4075710"
  },
  {
    "text": "d over h as well. Yeah. ",
    "start": "4075710",
    "end": "4083170"
  },
  {
    "text": "So the question is,\nhow do these models handle variable length inputs?",
    "start": "4083170",
    "end": "4088570"
  },
  {
    "text": "Yeah, so if you have--",
    "start": "4088570",
    "end": "4093910"
  },
  {
    "text": "so the input to the\nGPU forward pass is going to be a\nconstant length.",
    "start": "4093910",
    "end": "4100759"
  },
  {
    "text": "So you're going to maybe\npad to a constant length. And in order to not look at\nthe future, the stuff that's",
    "start": "4100760",
    "end": "4108309"
  },
  {
    "text": "sort happening in\nthe future, you can mask out the pad tokens. Just like the masking\nthat we showed",
    "start": "4108310",
    "end": "4114278"
  },
  {
    "text": "for not looking at\nthe future in general, you can just say set all\nof the attention weights to zero or the scores\nto negative infinity",
    "start": "4114279",
    "end": "4122229"
  },
  {
    "text": "for all of the pad tokens. ",
    "start": "4122229",
    "end": "4127420"
  },
  {
    "text": "Yeah, exactly. So you can set everything\nto this maximum length. Now, in practice--\nso the question was,",
    "start": "4127420",
    "end": "4133729"
  },
  {
    "text": "do you set this length\nthat you have everything be, be that maximum length? I mean, yes often, although\nyou can save computation",
    "start": "4133729",
    "end": "4140619"
  },
  {
    "text": "by setting it to something\nsmaller and everything-- the math all still works out.",
    "start": "4140620",
    "end": "4146080"
  },
  {
    "text": "You just have to code it\nproperly so it can handle-- you set everything- instead\nof to n, you set it all to 5",
    "start": "4146080",
    "end": "4151689"
  },
  {
    "text": "if everything is\nshorter than length 5, and you save a lot\nof computation. All of the self-attention\noperations just work.",
    "start": "4151689",
    "end": "4159160"
  },
  {
    "text": "So yeah. How many layers are in\nthe feed-forward normally?",
    "start": "4159160",
    "end": "4165439"
  },
  {
    "text": "There's one hidden layer in\nthe feed-forward usually. Yeah. OK, I should move on. I've got a couple more things\nand not very much time.",
    "start": "4165439",
    "end": "4172439"
  },
  {
    "text": "OK. But I'll be here after\nthe class as well. So in the encoder-- so the transformer encoder\nis almost identical.",
    "start": "4172439",
    "end": "4179460"
  },
  {
    "text": "But, again, we want\nbidirectional context, and so we just don't\ndo the masking, right? So I've got-- in my\nmulti-head attention here,",
    "start": "4179460",
    "end": "4186600"
  },
  {
    "text": "I've got no masking. And so it's that easy to\nmake the model bidirectional. OK?",
    "start": "4186600",
    "end": "4193139"
  },
  {
    "text": "So that's easy. So that's called the\ntransformer encoder. It's almost identical\nbut no masking. And then finally, we've\ngot the transformer encoder",
    "start": "4193140",
    "end": "4201000"
  },
  {
    "text": "decoder, which is actually how\nthe transformer was originally presented in this paper\nAttention Is All You Need.",
    "start": "4201000",
    "end": "4207730"
  },
  {
    "text": "And this is when we want to\nhave a bidirectional network. Here's the encoder. It takes in, say,\nmy source sentence",
    "start": "4207730",
    "end": "4213250"
  },
  {
    "text": "for machine translation;\nIt's multi-headed attention is not masked. And I have a decoder to\ndecode out my sentence.",
    "start": "4213250",
    "end": "4222000"
  },
  {
    "text": "Now, but you'll see that this\nis slightly more complicated. I have my masked\nmulti-head self-attention",
    "start": "4222000",
    "end": "4227250"
  },
  {
    "text": "just like I had\nbefore and my decoder, but now I have an\nextra operation which",
    "start": "4227250",
    "end": "4233050"
  },
  {
    "text": "is called cross\nattention, where I'm going to use my decoder\nvectors as my queries,",
    "start": "4233050",
    "end": "4241330"
  },
  {
    "text": "but then I'll take the\noutput of the encoder as my keys and values. So now for every\nword in the decoder,",
    "start": "4241330",
    "end": "4248020"
  },
  {
    "text": "I'm looking at all the possible\nwords in the output of all of the blocks of the encoder.",
    "start": "4248020",
    "end": "4253480"
  },
  {
    "text": "Yes. [INAUDIBLE] is no\nlonger [INAUDIBLE]",
    "start": "4253480",
    "end": "4259292"
  },
  {
    "text": "like the keys and the values. How do we get a key\nand value separated from the output, because\ndidn't we collapse those",
    "start": "4259292",
    "end": "4265570"
  },
  {
    "text": "into the single output? So we will-- sorry, how will\nwe get the keys and values out?",
    "start": "4265570",
    "end": "4272800"
  },
  {
    "text": "How do we-- because\nwhen we have the output, didn't we collapse like\nthe keys and values into a single output?",
    "start": "4272800",
    "end": "4279350"
  },
  {
    "text": "So the output-- [INAUDIBLE] calculate. Yeah, the question\nis, how do you get the keys and values and\nqueries out of this sort",
    "start": "4279350",
    "end": "4284590"
  },
  {
    "text": "of single collapsed output? Now, remember, the\noutput for each word is just this weighted\naverage of the value",
    "start": "4284590",
    "end": "4289719"
  },
  {
    "text": "vectors for the previous words. And then from that output,\nfor the next layer,",
    "start": "4289720",
    "end": "4295690"
  },
  {
    "text": "we apply a new key, query,\nand value transformation to each of them for the next\nlayer of self-attention.",
    "start": "4295690",
    "end": "4302230"
  },
  {
    "text": "So it's not actually\nthat you're-- --key and the value\nto the output. It's not the output\nitself, when you're",
    "start": "4302230",
    "end": "4307585"
  },
  {
    "text": "taking from the [INAUDIBLE]? Yeah, you apply the key matrix,\nthe query matrix to the output of whatever came before it.",
    "start": "4307585",
    "end": "4312640"
  },
  {
    "text": "Yeah. And so just in a little\nbit of math, right? We have these vectors h1 through\nhn, I'm going to call them,",
    "start": "4312640",
    "end": "4320829"
  },
  {
    "text": "that are the output\nof the encoder, right, and then I've got vectors that\nare the output of the decoder.",
    "start": "4320830",
    "end": "4327867"
  },
  {
    "text": "So I've got these z's\nI'm calling the output of the decoder, and then\nI simply define my keys",
    "start": "4327867",
    "end": "4333810"
  },
  {
    "text": "and my values from the\nencoder vectors, these h's.",
    "start": "4333810",
    "end": "4339360"
  },
  {
    "text": "So I take the h's, I apply a\nkey matrix and a value matrix, and then I define the\nqueries from my decoder.",
    "start": "4339360",
    "end": "4346440"
  },
  {
    "text": "So my query is here. So this is why two of the\narrows come from the encoder, and one of the arrows\ncomes from the decoder.",
    "start": "4346440",
    "end": "4352410"
  },
  {
    "text": "I've got my z's here and my\nqueries, my keys and values from the encoder.",
    "start": "4352410",
    "end": "4359860"
  },
  {
    "text": "OK. So that is it. I've got a couple\nof minutes, I want",
    "start": "4359860",
    "end": "4365760"
  },
  {
    "text": "to discuss some of the\nresults of transformers, and I'm happy to answer more\nquestions about transformers",
    "start": "4365760",
    "end": "4370860"
  },
  {
    "text": "after class. So really, the original\nresults of transformers,",
    "start": "4370860",
    "end": "4376170"
  },
  {
    "text": "they had this big\npitch for like, Oh, look, you can do way\nmore computation because of parallelization;\nthey got great results",
    "start": "4376170",
    "end": "4383250"
  },
  {
    "text": "in machine translation. So you had transformers\nsort of doing quite well,",
    "start": "4383250",
    "end": "4393000"
  },
  {
    "text": "although not like astoundingly\nbetter than existing machine translation systems, but\nthey were significantly more",
    "start": "4393000",
    "end": "4400987"
  },
  {
    "text": "efficient to train,\nright, Because you don't have this\nparallelization problem, you could compute on much\nmore data much faster,",
    "start": "4400987",
    "end": "4407520"
  },
  {
    "text": "and you could make use\nof faster GPUs much more. After that, there were things\nlike document generation,",
    "start": "4407520",
    "end": "4414900"
  },
  {
    "text": "where you had this sort of\nold standard of sequence to sequence models with LSTMs,\nand eventually, everything",
    "start": "4414900",
    "end": "4420659"
  },
  {
    "text": "became sort of transformers\nall the way down. Transformers also\nenabled this revolution",
    "start": "4420660",
    "end": "4427320"
  },
  {
    "text": "into pre-training, which we'll\ngo over in lecture next class. And this sort of the efficiency,\nthe parallelizability",
    "start": "4427320",
    "end": "4434610"
  },
  {
    "text": "allows you to compute on\ntons and tons of data. And so after a\ncertain point sort",
    "start": "4434610",
    "end": "4440220"
  },
  {
    "text": "of on standard large\nbenchmarks, everything became transformer-based. This ability to make use of lots\nand lots of data, lots and lots",
    "start": "4440220",
    "end": "4447989"
  },
  {
    "text": "of compute just put transformers\nhead and shoulders above LSTMs in, let's say, almost every\nsort of modern advancement",
    "start": "4447990",
    "end": "4456100"
  },
  {
    "text": "in natural language processing. There are many sort of drawbacks\nand variants to transformers.",
    "start": "4456100",
    "end": "4464415"
  },
  {
    "text": "The clearest one\nthat people have tried to work on quite a bit is\nthis quadratic compute problem. So this all pairs\nof interactions",
    "start": "4464415",
    "end": "4471639"
  },
  {
    "text": "means that our sort of total\ncomputation for each block grows quadratically with\nthe sequence length. And in a student's question,\nwe heard that, well,",
    "start": "4471640",
    "end": "4479739"
  },
  {
    "text": "as the sequence\nlength becomes long, if I want to process\na whole Wikipedia article, a whole novel, that\nbecomes quite unfeasible.",
    "start": "4479740",
    "end": "4488020"
  },
  {
    "text": "And actually, that's a step\nbackwards in some sense. Because for recurrent\nneural networks, it only grew linearly\nwith the sequence length.",
    "start": "4488020",
    "end": "4495642"
  },
  {
    "text": "Other things people\nhave tried to work on are sort of better\nposition representations because the absolute index of a\nword is not really the best way",
    "start": "4495642",
    "end": "4504430"
  },
  {
    "text": "maybe to represent its\nposition in a sequence. And just to give you an\nintuition of quadratic sequence",
    "start": "4504430",
    "end": "4510280"
  },
  {
    "text": "length, remember that we had\nthis big matrix multiply here that resulted in this\nmatrix of n by n,",
    "start": "4510280",
    "end": "4516670"
  },
  {
    "text": "and computing this is like a big\ncost, it costs a lot of memory.",
    "start": "4516670",
    "end": "4522170"
  },
  {
    "text": "And so there's been work-- yeah, and so if you think\nof the model dimensionality, as like 1,000, although\ntoday it gets much larger.",
    "start": "4522170",
    "end": "4528875"
  },
  {
    "text": "And then for a short\nsequence of n is roughly 30, maybe if you're computing\nn squared times d,",
    "start": "4528875",
    "end": "4536780"
  },
  {
    "text": "30 isn't so bad, but if you\nhad something like 50,000,",
    "start": "4536780",
    "end": "4541969"
  },
  {
    "text": "then n squared becomes huge\nand sort of totally infeasible. So people have tried\nto map things down",
    "start": "4541970",
    "end": "4549260"
  },
  {
    "text": "to a lower dimensional\nspace to get rid of the quadratic computation.",
    "start": "4549260",
    "end": "4554390"
  },
  {
    "text": "But in practice, as people\nhave gone to things like GPT 3, ChatGPT, most of the\ncomputation doesn't show up",
    "start": "4554390",
    "end": "4561590"
  },
  {
    "text": "in the self-attention,\nso people are wondering is it even necessary to get\nrid of the self-attention",
    "start": "4561590",
    "end": "4568429"
  },
  {
    "text": "operations quadratic constraint?\nit's an open form of research whether this is\nsort of necessary.",
    "start": "4568430",
    "end": "4574830"
  },
  {
    "text": "And then finally, there have\nbeen a ton of modifications to the transformer over the\nlast five, four-ish years.",
    "start": "4574830",
    "end": "4581810"
  },
  {
    "text": "And it turns out that\nthe original transformer plus maybe a couple\nof modifications",
    "start": "4581810",
    "end": "4587840"
  },
  {
    "text": "is pretty much the best\nthing there is still. There have been a\ncouple of things that end up being important.",
    "start": "4587840",
    "end": "4593960"
  },
  {
    "text": "Changing out the non-linearities\nin the feed-forward network ends up being important,\nbut it's sort of a--",
    "start": "4593960",
    "end": "4600626"
  },
  {
    "text": "it's had lasting power so far. But I think it's ripe for\npeople to come through and think",
    "start": "4600626",
    "end": "4606440"
  },
  {
    "text": "about how to improve\nit in various ways. So pre-training is on Tuesday.",
    "start": "4606440",
    "end": "4612102"
  },
  {
    "text": "Good luck on assignment four. And then yeah, we'll have the\nproject proposal documents out tonight for\nyou to talk about.",
    "start": "4612102",
    "end": "4619240"
  },
  {
    "start": "4619240",
    "end": "4623000"
  }
]