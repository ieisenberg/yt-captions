[
  {
    "start": "0",
    "end": "160000"
  },
  {
    "text": "welcome to part one of generative AI for healthcare in just the last few years AI",
    "start": "11040",
    "end": "16160"
  },
  {
    "text": "has grown from a futuristic concept to a tool that is reshaping healthcare delivery but with this rapid evolution",
    "start": "16160",
    "end": "22800"
  },
  {
    "text": "also comes a critical question how can healthcare professionals understand and harness these powerful technologies",
    "start": "22800",
    "end": "29199"
  },
  {
    "text": "without formal training in data science and computer science today we're going",
    "start": "29199",
    "end": "34399"
  },
  {
    "text": "to break down the essentials of generative AI and healthcare not from the perspective of computer scientists",
    "start": "34399",
    "end": "39840"
  },
  {
    "text": "but rather as physicians who have both used and implemented these tools in our everyday work my name is Dong and I am a",
    "start": "39840",
    "end": "47200"
  },
  {
    "text": "clinical informaticist and emergency medicine physician at Stanford and I'm Shivam a clinical informatist and",
    "start": "47200",
    "end": "53520"
  },
  {
    "text": "internal medicine physician also at Stanford both of our informatics work focuses on effectively deploying",
    "start": "53520",
    "end": "58719"
  },
  {
    "text": "generative AI in real world clinical settings at Stanford medicine through this work we've discovered that",
    "start": "58719",
    "end": "63840"
  },
  {
    "text": "improving adoption really hinges on healthcare professionals actually understanding the foundations of how",
    "start": "63840",
    "end": "69040"
  },
  {
    "text": "these large language models work their limitations and how to effectively use them unfortunately there's a significant",
    "start": "69040",
    "end": "74640"
  },
  {
    "text": "lack of accessible educational material in this space which is what inspired us to develop this talk in spring 2024",
    "start": "74640",
    "end": "81200"
  },
  {
    "text": "we've since been asked to deliver it at various venues over the past year and consistently receive requests from position educators for recordings for",
    "start": "81200",
    "end": "88000"
  },
  {
    "text": "them to reference and share so today we're creating this YouTube video to fill that gap providing a clear",
    "start": "88000",
    "end": "93600"
  },
  {
    "text": "comprehensive YouTube resource that healthcare professionals can access any time to better understand generative AI",
    "start": "93600",
    "end": "99119"
  },
  {
    "text": "fundamentals and tips our goal is to really empower you with the knowledge and skills to safely and effectively",
    "start": "99119",
    "end": "104880"
  },
  {
    "text": "implement these promising tools in your workflows from the bench to the bedside and everywhere in",
    "start": "104880",
    "end": "110520"
  },
  {
    "text": "between so starting off with some of our brief disclosures we are both independent contractors for Greenlight",
    "start": "110520",
    "end": "116159"
  },
  {
    "text": "where we contribute to OpenAI safety initiatives by helping align health related model responses to improve their",
    "start": "116159",
    "end": "121600"
  },
  {
    "text": "model quality and accuracy i was also previously a consultant for Glass Health where I helped lead their clinical team",
    "start": "121600",
    "end": "128720"
  },
  {
    "text": "also a brief content disclaimer that the soft really offers a highle overview of some really complex technical concepts",
    "start": "128720",
    "end": "135360"
  },
  {
    "text": "because of this some details are streamlined for clarity or based on expert consensus where there's limited public information i'm sure some of the",
    "start": "135360",
    "end": "142560"
  },
  {
    "text": "series will inevitably get outdated due to the sheer pace of progress in this field but we've made a conscious effort",
    "start": "142560",
    "end": "147920"
  },
  {
    "text": "to focus on topics that are really core to the technology and we believe that won't significantly change we also",
    "start": "147920",
    "end": "153760"
  },
  {
    "text": "primarily focus on OpenAI's models like chatbt in our examples mainly because that's what most people are familiar",
    "start": "153760",
    "end": "159680"
  },
  {
    "text": "with and most LLM tools used in the Epic EHR system are powered by Open AI",
    "start": "159680",
    "end": "165360"
  },
  {
    "start": "160000",
    "end": "405000"
  },
  {
    "text": "however the core concepts we'll discuss apply to every single LLM out there and not just OpenAI's models we have to",
    "start": "165360",
    "end": "171599"
  },
  {
    "text": "start the conversation by addressing that sometimes it can actually be fairly tricky to get these AI models to behave exactly the way we want them to talking",
    "start": "171599",
    "end": "178640"
  },
  {
    "text": "to AI it turns out is a little bit more complicated than just asking a questions and learning how to use these models",
    "start": "178640",
    "end": "184319"
  },
  {
    "text": "effectively can be even harder still and I really think that there's three fundamental reasons why prompting or",
    "start": "184319",
    "end": "190800"
  },
  {
    "text": "prompt engineering can be so challenging number one AI literature is very very",
    "start": "190800",
    "end": "196080"
  },
  {
    "text": "difficult to read and understand if you don't have a technical or computer science background so here's a",
    "start": "196080",
    "end": "201760"
  },
  {
    "text": "screenshot from the seminal 2017 paper titled attention is all you need which was the original paper that laid the",
    "start": "201760",
    "end": "207840"
  },
  {
    "text": "foundation for modern-day large language models and introduced the transformer architecture now Shivam and I have both",
    "start": "207840",
    "end": "214000"
  },
  {
    "text": "separately tried to read this paper multiple times and many attempts later I still can't say that I fully understand",
    "start": "214000",
    "end": "219599"
  },
  {
    "text": "the content the point I'm trying to make here is that it's actually really difficult for clinicians to read and",
    "start": "219599",
    "end": "224959"
  },
  {
    "text": "understand this kind of original literature about AI and you can see how this might be a problem to a group of",
    "start": "224959",
    "end": "231040"
  },
  {
    "text": "people who are all trained on evidence-based medicine and knowing how to site your sources and how to site",
    "start": "231040",
    "end": "236319"
  },
  {
    "text": "your studies so you might say well instead of reading the original papers why don't you just look up some tutorials and guides online and here",
    "start": "236319",
    "end": "243360"
  },
  {
    "text": "also we run into some challenges there really aren't that many healthcare worker appropriate prompting",
    "start": "243360",
    "end": "250439"
  },
  {
    "text": "resources and so here's another screenshot from a popular resource online called learnprompting.org org",
    "start": "250439",
    "end": "256560"
  },
  {
    "text": "which is one of the largest free online resources for learning how to do this kind of stuff and you can see that it",
    "start": "256560",
    "end": "261919"
  },
  {
    "text": "starts off fairly simple and straightforward right you are Shakespeare um an English writer write",
    "start": "261919",
    "end": "266960"
  },
  {
    "text": "me a poem the problem is that it very quickly once again turns from something simple like this to something much more",
    "start": "266960",
    "end": "272960"
  },
  {
    "text": "complicated like this with statistics and formulas and variables and so the",
    "start": "272960",
    "end": "278000"
  },
  {
    "text": "problem is that the sweet spot of resources out there for something like this is actually very very narrow what",
    "start": "278000",
    "end": "284160"
  },
  {
    "text": "we want is something that is both practical and detailed and actually useful beyond just turning your email",
    "start": "284160",
    "end": "289280"
  },
  {
    "text": "into a pirate haik coup uh while simultaneously not being too bogged down by the technical language and jargon and",
    "start": "289280",
    "end": "297040"
  },
  {
    "text": "finally and perhaps most importantly we have to talk about the exponential pace of AI progress recently I recently went",
    "start": "297040",
    "end": "304000"
  },
  {
    "text": "on PubMed and searched for articles containing the words artificial intelligence or machine learning in the",
    "start": "304000",
    "end": "309919"
  },
  {
    "text": "title and in 2014 there were 272 such papers published fast forward a decade",
    "start": "309919",
    "end": "316639"
  },
  {
    "text": "in 2024 that number had increased to over 20,000 in a single year which is a",
    "start": "316639",
    "end": "323320"
  },
  {
    "text": "7,500% increase to put it differently it's almost 60 new papers every single",
    "start": "323320",
    "end": "328560"
  },
  {
    "text": "day and keep in mind this is just PubMed so we're not even talking about computer science or engineering journals so",
    "start": "328560",
    "end": "335120"
  },
  {
    "text": "unless you're a full-time AI researcher or a data scientist it's all a little",
    "start": "335120",
    "end": "340600"
  },
  {
    "text": "overwhelming and so when it comes to prompting it really leaves us feeling like this where we might see something",
    "start": "340600",
    "end": "346000"
  },
  {
    "text": "online we might hear some advice from friends and we try some things out and when those things break we try something",
    "start": "346000",
    "end": "351440"
  },
  {
    "text": "else but most of us don't actually know what's working what isn't and why and so",
    "start": "351440",
    "end": "357759"
  },
  {
    "text": "this is exactly why we started by focusing on what an LLM actually is and",
    "start": "357759",
    "end": "362800"
  },
  {
    "text": "how it works not just how to use it because before we discuss the how of prompt engineering we really need to",
    "start": "362800",
    "end": "368880"
  },
  {
    "text": "first understand the what and the why of generative AI with that in mind here are",
    "start": "368880",
    "end": "374479"
  },
  {
    "text": "the big three things we hope you'll take away from this video first we want you to understand where generative AI fits",
    "start": "374479",
    "end": "381120"
  },
  {
    "text": "into the broader framework of AI and healthcare and how we got here second we",
    "start": "381120",
    "end": "386560"
  },
  {
    "text": "want to give you an intuitive understanding for how large language models work so that you can build a",
    "start": "386560",
    "end": "391840"
  },
  {
    "text": "mental model for generative AI and finally we'll talk about the pre-training and post-training phases",
    "start": "391840",
    "end": "397759"
  },
  {
    "text": "that shape these models behavior how they're optimized how they're adapted and how they're fine-tuned and in",
    "start": "397759",
    "end": "404479"
  },
  {
    "text": "summary the purpose of this first video is to answer that allimportant and not",
    "start": "404479",
    "end": "410080"
  },
  {
    "start": "405000",
    "end": "1071000"
  },
  {
    "text": "so straightforward question of what is an LLM really",
    "start": "410080",
    "end": "415280"
  },
  {
    "text": "so let's start with a brief history we need a framework to help us contextualize where LLMs fit into the",
    "start": "415280",
    "end": "420400"
  },
  {
    "text": "history of healthcare AI i think the best framework that I've heard of comes from a paper titled the three epochs of",
    "start": "420400",
    "end": "426400"
  },
  {
    "text": "artificial intelligence and healthcare it was authored by Michael Howell the chief clinical officer at Google along",
    "start": "426400",
    "end": "431520"
  },
  {
    "text": "with Karen D salvo Google's chief health officer they break the history of healthcare AI into three distinct epochs",
    "start": "431520",
    "end": "439360"
  },
  {
    "text": "epoch 1 began around 1970 and represents the era of symbolic AI and probabilistic models you can think of this as",
    "start": "439360",
    "end": "446160"
  },
  {
    "text": "rules-based models epoch 2 began in earnest around 2010 representing the era",
    "start": "446160",
    "end": "451280"
  },
  {
    "text": "of deep learning and this is what we now refer to as traditional machine learning or ML for short and finally epoch 3 the",
    "start": "451280",
    "end": "458880"
  },
  {
    "text": "topic of today's discussion is large language models and generative AI which was first described in 2017 and brought",
    "start": "458880",
    "end": "465199"
  },
  {
    "text": "into the public zeitgeist in around 2022 with OpenAI's release of Chat GBT now",
    "start": "465199",
    "end": "471039"
  },
  {
    "text": "it's important to highlight here that these new technologies haven't replaced each other rather each new development",
    "start": "471039",
    "end": "477599"
  },
  {
    "text": "was built on top of the previous ones and at the same time each of these epochs represents a fundamentally",
    "start": "477599",
    "end": "483199"
  },
  {
    "text": "different type of technology and so why is the distinction between these three important it's because each",
    "start": "483199",
    "end": "490560"
  },
  {
    "text": "one has distinct inputs gives distinct outputs and have different types of ideal use cases and so the next time you",
    "start": "490560",
    "end": "497120"
  },
  {
    "text": "hear the term AI especially in the healthcare setting you should be able to quickly recognize which one of these",
    "start": "497120",
    "end": "502319"
  },
  {
    "text": "three categories the AI fits into using hopefully a few simple",
    "start": "502319",
    "end": "507400"
  },
  {
    "text": "horistics okay let's dive a little bit deeper into each of these epochs one by one and give some more concrete examples",
    "start": "507400",
    "end": "513919"
  },
  {
    "text": "starting with epoch 1 I think of this as rulesbased AI basically if something can be programmed entirely with",
    "start": "513919",
    "end": "520479"
  },
  {
    "text": "straightforward if then logic then it probably fits into epoch 1 in your personal life this might remind you of",
    "start": "520479",
    "end": "526080"
  },
  {
    "text": "something like Clippy the old Microsoft Word Assistant that popped up whenever you tried to type a letter you might",
    "start": "526080",
    "end": "531200"
  },
  {
    "text": "also think about something like Turboax which walks you through the uh structured decision tree provided by the",
    "start": "531200",
    "end": "536640"
  },
  {
    "text": "IRS for filing taxes and finally even today in 2025 most video game AI",
    "start": "536640",
    "end": "542560"
  },
  {
    "text": "actually still runs on highly complex layers and variations of this kind of simple rulesbased",
    "start": "542560",
    "end": "548120"
  },
  {
    "text": "logic now turning to healthcare nearly all clinical decision support tools that",
    "start": "548120",
    "end": "553200"
  },
  {
    "text": "we interact with daily fits into this category so for example the alerts that hopefully pop up when you try to order",
    "start": "553200",
    "end": "559600"
  },
  {
    "text": "5,000 milligs of Tylenol instead of 500 these are all examples of epoch 1 technology the risk calculators you see",
    "start": "559600",
    "end": "566720"
  },
  {
    "text": "on platforms like MDAC again this is mostly simple rulesbased logic and even",
    "start": "566720",
    "end": "572160"
  },
  {
    "text": "smart automated billing algorithms like the one you see here that automatically determine billing levels based off of",
    "start": "572160",
    "end": "577360"
  },
  {
    "text": "which boxes you check are examples of rulesbased AI and so as you can kind of",
    "start": "577360",
    "end": "582800"
  },
  {
    "text": "tell by now many of the seemingly smart features that we have in our modern-day EHRs those tool that we rely on every",
    "start": "582800",
    "end": "588959"
  },
  {
    "text": "single day are actually built on technology that's been pretty much around since the 1970s and so here's a simple way to",
    "start": "588959",
    "end": "596880"
  },
  {
    "text": "recognize epoch 1 AI if it's logic- based non-addaptive and it doesn't learn",
    "start": "596880",
    "end": "602480"
  },
  {
    "text": "from new data and if it feels like it's something that is hardcoded by experts for very specific tasks then it probably",
    "start": "602480",
    "end": "608800"
  },
  {
    "text": "fits into this first category of AI now this is where things start to get",
    "start": "608800",
    "end": "615200"
  },
  {
    "text": "a little bit more interesting epoch 2 is what we typically refer to as machine learning or deep learning there are many",
    "start": "615200",
    "end": "622480"
  },
  {
    "text": "different subtypes of machine learning but for our purposes today we are going to mostly be talking about supervised",
    "start": "622480",
    "end": "627600"
  },
  {
    "text": "learning which is by far the most popular method in machine learning instead of relying on hard-coded rules",
    "start": "627600",
    "end": "634320"
  },
  {
    "text": "this approach involves feeding a computer hundreds of thousands if not millions of different examples the",
    "start": "634320",
    "end": "639680"
  },
  {
    "text": "system then learns to recognize patterns within that data and then applies what it's learned to make predictions and",
    "start": "639680",
    "end": "645440"
  },
  {
    "text": "generate insights in new unseen data and in our everyday life we actually see",
    "start": "645440",
    "end": "650959"
  },
  {
    "text": "this all the time think about facial recognition on your phone or your social media app that automatically tags your",
    "start": "650959",
    "end": "657120"
  },
  {
    "text": "friends and photos it's also what powers targeted advertising like your streaming service somehow knowing what you're in a",
    "start": "657120",
    "end": "663440"
  },
  {
    "text": "mood for or when you keep seeing ads that might be appropriate for you whether it's your age or your gender or",
    "start": "663440",
    "end": "669680"
  },
  {
    "text": "your interests and of course this is the kind of AI behind computer vision and",
    "start": "669680",
    "end": "674800"
  },
  {
    "text": "self-driving car technology like Tesla autopilot or Whimo and in healthcare this is where",
    "start": "674800",
    "end": "681600"
  },
  {
    "text": "big data really comes out to shine tools like automated EKG or STEMI detection",
    "start": "681600",
    "end": "686959"
  },
  {
    "text": "are great examples of this they rely on thousands of labeled cases and labeled EKGs to make accurate predictions and",
    "start": "686959",
    "end": "694000"
  },
  {
    "text": "accurate interpretations in real time and the EKG you see here on the screen is actually a very early evolving STEMI",
    "start": "694000",
    "end": "700160"
  },
  {
    "text": "which you might not really notice at first glance but this Queen of Hearts AI picked up on",
    "start": "700160",
    "end": "705800"
  },
  {
    "text": "immediately you might also see this in AI deterioration models which are blackbox models where the system takes",
    "start": "705800",
    "end": "711680"
  },
  {
    "text": "in hundreds of variables automatically from your EHR and might give you something like a sepsis risk score or a",
    "start": "711680",
    "end": "717360"
  },
  {
    "text": "decompensation score and telling you that the patient might get sicker within the next 48 to 72 hours but you might",
    "start": "717360",
    "end": "723920"
  },
  {
    "text": "not have any idea why and of course uh in healthcare computer vision a perfect",
    "start": "723920",
    "end": "730240"
  },
  {
    "text": "example is radiology there are actually already quite a few FDA approved AI softwares that can automatically flag",
    "start": "730240",
    "end": "736639"
  },
  {
    "text": "abnormalities on imaging studies or do image segmentation which can dramatically improve the efficiency and",
    "start": "736639",
    "end": "742560"
  },
  {
    "text": "throughput of human radiologists the way to spot machine learning based AI is to think of it as",
    "start": "742560",
    "end": "748959"
  },
  {
    "text": "learning from C these models are trained on massive label data sets lots and lots",
    "start": "748959",
    "end": "754240"
  },
  {
    "text": "and lots of examples they learn from that data but usually they do it for one specific task so they're really powerful",
    "start": "754240",
    "end": "761279"
  },
  {
    "text": "but they're not really generalizable they don't really easy easily transfer from one use case to another one",
    "start": "761279",
    "end": "767680"
  },
  {
    "text": "important thing to keep in mind these models are often low in interpretability this is where we start to hear the term",
    "start": "767680",
    "end": "773519"
  },
  {
    "text": "blackbox model it means that it's very difficult to trace a clear causal relationship between specific input",
    "start": "773519",
    "end": "779760"
  },
  {
    "text": "features and the final output the model predictions might be highly accurate but we often don't fully understand how it",
    "start": "779760",
    "end": "786800"
  },
  {
    "text": "got there and of course this opens up a whole new can of worms when it comes to safety bias and whether clinicians or",
    "start": "786800",
    "end": "794000"
  },
  {
    "text": "patients um can trust or even accept these tools this is a much bigger",
    "start": "794000",
    "end": "799120"
  },
  {
    "text": "conversation of course and one that we're going to save for another time and finally the hottest topic in the last",
    "start": "799120",
    "end": "804959"
  },
  {
    "text": "few years is generative AI specifically large language models or foundation models this marks the transition to",
    "start": "804959",
    "end": "810560"
  },
  {
    "text": "epoch 3 on the personal side of things tools like Chat GBT need no introduction",
    "start": "810560",
    "end": "815600"
  },
  {
    "text": "but there are everyday other examples as well for instance AI summarization tools like the ones Amazon uses to scan",
    "start": "815600",
    "end": "822240"
  },
  {
    "text": "thousands of customer reviews and generate a short readable and helpful concise summary or customer service chat",
    "start": "822240",
    "end": "828399"
  },
  {
    "text": "bots like Erica which is Bank of America's automated assistant which improves upon their traditional phone",
    "start": "828399",
    "end": "833519"
  },
  {
    "text": "tree system and of course image generation where tools like Dali Midjourney or Sora can generate visual",
    "start": "833519",
    "end": "840160"
  },
  {
    "text": "content or even videos from text prompts these are often multimodal meaning they work across images text and more and",
    "start": "840160",
    "end": "847279"
  },
  {
    "text": "this technology is evolving super fast since the time that the slide was made this classic scary AI fingers are",
    "start": "847279",
    "end": "854560"
  },
  {
    "text": "largely now a thing of the past in healthcare we're starting to see some really exciting use cases as well",
    "start": "854560",
    "end": "861360"
  },
  {
    "text": "we now have mature products like clinical knowledge retrieval using tools like open evidence or clinical key AI to",
    "start": "861360",
    "end": "866639"
  },
  {
    "text": "pull up fast contextware answers based off of our own natural language queries",
    "start": "866639",
    "end": "871839"
  },
  {
    "text": "um and there's also chart summarization where the model reads through an entire patient's hundreds and hundreds of notes",
    "start": "871839",
    "end": "877440"
  },
  {
    "text": "and gives you a highlevel overview when you need it and then there is automated note drafting where tools can help you",
    "start": "877440",
    "end": "885440"
  },
  {
    "text": "generate responses in real time to things like inbasket messages and then finally probably the best",
    "start": "885440",
    "end": "892240"
  },
  {
    "text": "example in this area for healthcare is ambient dictation which listens into a",
    "start": "892240",
    "end": "897440"
  },
  {
    "text": "clinical encounter and automatically generates the visit note ambient dictation is a great example of a",
    "start": "897440",
    "end": "903040"
  },
  {
    "text": "technology that actually uses elements from all three epochs rules-based logic pattern recognition for machine learning",
    "start": "903040",
    "end": "908880"
  },
  {
    "text": "and the generative capabilities of LLMs now when it comes to recognizing this",
    "start": "908880",
    "end": "914639"
  },
  {
    "text": "type of AI the heruristics for epoch 3 are first these models are general",
    "start": "914639",
    "end": "921760"
  },
  {
    "text": "purpose they are not built for one particular task and as the name suggests they have generative capabilities",
    "start": "921760",
    "end": "927920"
  },
  {
    "text": "meaning that they can create new content whether that's text images or even video they're also pre-trained on enormous",
    "start": "927920",
    "end": "934560"
  },
  {
    "text": "data sets far larger than what we would typically see in the traditional deep learning models this is what enables",
    "start": "934560",
    "end": "940880"
  },
  {
    "text": "them to generalize across a wide range of use cases now in terms of inputs and",
    "start": "940880",
    "end": "946240"
  },
  {
    "text": "outputs they're often multimodal capable of handling and producing different kinds of data so text images audio video",
    "start": "946240",
    "end": "953360"
  },
  {
    "text": "etc and then finally when it comes to interpretability this might be the most opaque of the three these models can",
    "start": "953360",
    "end": "960959"
  },
  {
    "text": "really generate impressive results but it's really difficult to trace how they arrived at a specific output",
    "start": "960959",
    "end": "968079"
  },
  {
    "text": "this slide gives a highle summary of the similarities and differences across these three epochs it's definitely a",
    "start": "968079",
    "end": "974160"
  },
  {
    "text": "little bit on the dense side so feel free to pause here and take a moment to look it over but the big picture is as",
    "start": "974160",
    "end": "979440"
  },
  {
    "text": "we move from epoch 1 to epoch 3 the technologies become less transparent but",
    "start": "979440",
    "end": "984800"
  },
  {
    "text": "also more flexible they go from being rigid and rules-based to being general purpose and capable of handling",
    "start": "984800",
    "end": "990800"
  },
  {
    "text": "unstructured data and suited for a wide range of tasks at the same time they become more complex to build and more",
    "start": "990800",
    "end": "997680"
  },
  {
    "text": "resource intensive but also paradoxically they have become easier to use at the front end which is definitely",
    "start": "997680",
    "end": "1003360"
  },
  {
    "text": "great for us as physicians now taking a step back from healthcare for a moment we've actually",
    "start": "1003360",
    "end": "1008959"
  },
  {
    "text": "had basic AI in our lives for quite a while think about voice assistants like Google Home or Siri or Alexa you can ask",
    "start": "1008959",
    "end": "1015759"
  },
  {
    "text": "them simple questions and they'll give you basic facts or maybe carry out a straightforward command and they're quite useful for doing things like",
    "start": "1015759",
    "end": "1022079"
  },
  {
    "text": "telling you the weather or answering basic trivia questions or managing your smart home but in other ways are",
    "start": "1022079",
    "end": "1027839"
  },
  {
    "text": "obviously fairly limited still now fast forward to today and all of a sudden we're having riched nuanced",
    "start": "1027839",
    "end": "1034319"
  },
  {
    "text": "conversations with large language models about philosophy and finance and medicine and even linguistic theory and",
    "start": "1034319",
    "end": "1041038"
  },
  {
    "text": "the kind of output that you see here on this slide is worlds apart from what Siri or Alexa would have given you even",
    "start": "1041039",
    "end": "1046160"
  },
  {
    "text": "a few years ago and so what changed how do we go from those early assistants to",
    "start": "1046160",
    "end": "1051760"
  },
  {
    "text": "models that can now write essays debate moral dilemmas or summarize dense scientific literature to put it another",
    "start": "1051760",
    "end": "1058919"
  },
  {
    "text": "way what's going on inside the so-called black box that is making such a dramatic",
    "start": "1058919",
    "end": "1064240"
  },
  {
    "text": "difference now we can't really open the black box and watch it work step by step but we do now understand how these",
    "start": "1064240",
    "end": "1070640"
  },
  {
    "text": "models are designed and how they're trained and understanding that gives us a really powerful lens into",
    "start": "1070640",
    "end": "1076080"
  },
  {
    "start": "1071000",
    "end": "1668000"
  },
  {
    "text": "understanding how they behave and why so to unpack what's going on inside of",
    "start": "1076080",
    "end": "1082000"
  },
  {
    "text": "these models we need a basic understanding of the anatomy and physiology of an LLM how it's built how",
    "start": "1082000",
    "end": "1088000"
  },
  {
    "text": "it learns and how it generates the kind of outputs that we're seeing today so let's start at the very beginning with",
    "start": "1088000",
    "end": "1094320"
  },
  {
    "text": "the input prompt this is what you type into the model for example let's say you",
    "start": "1094320",
    "end": "1099440"
  },
  {
    "text": "enter the following prompt please help me draft a short but professional letter to a patient explaining why they don't",
    "start": "1099440",
    "end": "1105280"
  },
  {
    "text": "need an MRI for their seasonal allergies the first thing the LLM is going to do is it's going to break down this prompt",
    "start": "1105280",
    "end": "1111760"
  },
  {
    "text": "into smaller pieces a process called tokenization these pieces are called tokens and tokens can be entire words",
    "start": "1111760",
    "end": "1118960"
  },
  {
    "text": "they can be parts of words or they can even be single characters but to keep things simple you can think about a",
    "start": "1118960",
    "end": "1124160"
  },
  {
    "text": "token as being equivalent to roughly a single word for the rest of this video so here's how the model is going to",
    "start": "1124160",
    "end": "1130960"
  },
  {
    "text": "tokenize this specific sentence in this example it splits this input into exactly 25 tokens with each one",
    "start": "1130960",
    "end": "1138640"
  },
  {
    "text": "representing a small chunk of the original prompt now let's just zoom in on one",
    "start": "1138640",
    "end": "1144240"
  },
  {
    "text": "token say the word patient when the LLM sees this token in the input the first",
    "start": "1144240",
    "end": "1149760"
  },
  {
    "text": "thing it does is to look it up in something like a dictionary specifically it retrieves what's called a static",
    "start": "1149760",
    "end": "1156480"
  },
  {
    "text": "embedding for that token this embedding was created during the model's training process and this of course begs the",
    "start": "1156480",
    "end": "1163120"
  },
  {
    "text": "question what is a static embedding essentially it's a way of capturing the meaning of a word by turning it into a",
    "start": "1163120",
    "end": "1169600"
  },
  {
    "text": "series of numbers in other words a vector which are projected into a highdimensional space now if you're",
    "start": "1169600",
    "end": "1176880"
  },
  {
    "text": "anything like me that probably sounds like a bunch of mathematical gobblygook so let's slow it down a little bit and",
    "start": "1176880",
    "end": "1182160"
  },
  {
    "text": "unpack what this actually means here is a vector a vector is just",
    "start": "1182160",
    "end": "1187600"
  },
  {
    "text": "a list or an array of numbers for example this is a three-dimensional vector because it has three numbers now",
    "start": "1187600",
    "end": "1195360"
  },
  {
    "text": "this vector by itself could represent all kinds of different things it might represent force in a 3D space for",
    "start": "1195360",
    "end": "1200880"
  },
  {
    "text": "example maybe 5 Newtons in the X axis 1 Newton in the Y and seven in the Z or it",
    "start": "1200880",
    "end": "1206400"
  },
  {
    "text": "might represent color using the RGB model five units of red one green seven blue um a vector doesn't really have any",
    "start": "1206400",
    "end": "1214480"
  },
  {
    "text": "inherent meaning it's just numbers until we assign a meaning to it based off of the context so instead of three numbers",
    "start": "1214480",
    "end": "1222960"
  },
  {
    "text": "let's now imagine a vector with hundreds or even thousands of dimensions this is",
    "start": "1222960",
    "end": "1228159"
  },
  {
    "text": "what we call an embedding a special kind of vector that represents a word like patient in a way that captures its",
    "start": "1228159",
    "end": "1234880"
  },
  {
    "text": "meaning based on how it appears in language now let's look again at the",
    "start": "1234880",
    "end": "1240400"
  },
  {
    "text": "sample embedding for the word patient each number in the embedding of an LLM represents the strength of an",
    "start": "1240400",
    "end": "1246320"
  },
  {
    "text": "association between that token and a particular concept sentiment or idea",
    "start": "1246320",
    "end": "1251440"
  },
  {
    "text": "ranging from negative 1 to one when you combine all of these dimensions you get something surprisingly powerful a dense",
    "start": "1251440",
    "end": "1257919"
  },
  {
    "text": "highdimensional representation of a particular word or token's meaning shaped by the patterns the model saw",
    "start": "1257919",
    "end": "1264000"
  },
  {
    "text": "during its training process if we look at these numbers in each embedding each one ranges from",
    "start": "1264000",
    "end": "1269280"
  },
  {
    "text": "negative 1 to one remember a vector has both a direction in this case positive and negative and magnitude which tells",
    "start": "1269280",
    "end": "1275280"
  },
  {
    "text": "us exactly how strongly that feature is present in this example 84 might represent the association with a",
    "start": "1275280",
    "end": "1281039"
  },
  {
    "text": "positive versus negative sentiment 64 might represent the likelihood that the word is a noun negative.46 might",
    "start": "1281039",
    "end": "1287360"
  },
  {
    "text": "represent the emotional intensity and so on and so forth now what's fascinating",
    "start": "1287360",
    "end": "1292880"
  },
  {
    "text": "is that each dimension in the embedded encodes some kind of a subtle relationship or feature and when everything is taken altogether they",
    "start": "1292880",
    "end": "1299440"
  },
  {
    "text": "collectively give us a remarkably detailed representation of meaning even for abstract or hard to define concepts",
    "start": "1299440",
    "end": "1305840"
  },
  {
    "text": "for instance how would you define the emotional tone of a word like apple or decide whether a cloud is good or bad or",
    "start": "1305840",
    "end": "1313280"
  },
  {
    "text": "what it means for something to be read beyond just the color these aren't questions that are clear-cut with human",
    "start": "1313280",
    "end": "1319440"
  },
  {
    "text": "answers but the model having seen millions of examples can assign meaning based on how these words appear in",
    "start": "1319440",
    "end": "1324960"
  },
  {
    "text": "context it's not just that the LLM understands language like the way we do but rather that it has learned patterns",
    "start": "1324960",
    "end": "1331200"
  },
  {
    "text": "that capture how words relate to one another and it's these patterns that are encoded inside of these",
    "start": "1331200",
    "end": "1337000"
  },
  {
    "text": "embeddings to illustrate this let's take a look at the simplified vector space we are working with just three dimensions",
    "start": "1337000",
    "end": "1343120"
  },
  {
    "text": "here legs tail and speaks each color arrow here represents the embedding of a",
    "start": "1343120",
    "end": "1348559"
  },
  {
    "text": "particular concept cat dog and human notice the angle X here labeled between",
    "start": "1348559",
    "end": "1355039"
  },
  {
    "text": "the vectors for cat and dog which is much smaller than the angle Y labeled between cat and human in vector space",
    "start": "1355039",
    "end": "1362720"
  },
  {
    "text": "this angular distance represents something called cosine similarity the smaller the angle the more similar the",
    "start": "1362720",
    "end": "1368159"
  },
  {
    "text": "two concepts are in meaning and so in this case the model interprets cat and dog as being more similar than cat and",
    "start": "1368159",
    "end": "1374000"
  },
  {
    "text": "human which is how we as humans also understand these concepts it's able to do this through mathematical and",
    "start": "1374000",
    "end": "1379919"
  },
  {
    "text": "geometric concepts rather than strict definitions or logic as we humans",
    "start": "1379919",
    "end": "1385000"
  },
  {
    "text": "might here's another classic example of how abstract concepts can be captured within vector space let's take a look at",
    "start": "1385000",
    "end": "1391200"
  },
  {
    "text": "wordtovec which is a foundational technique in natural language processing used to generate vector representations",
    "start": "1391200",
    "end": "1396480"
  },
  {
    "text": "of words so here is where the vector for human might be in word to vec in the",
    "start": "1396480",
    "end": "1402480"
  },
  {
    "text": "simplified two-dimensional space and here is the vector for woman if we",
    "start": "1402480",
    "end": "1408159"
  },
  {
    "text": "subtract the vector for man from woman we get a new vector this red arrow here",
    "start": "1408159",
    "end": "1413600"
  },
  {
    "text": "that captures the difference between the two this vector encodes a concept that we can collectively describe as gender",
    "start": "1413600",
    "end": "1421280"
  },
  {
    "text": "it's not the exact word gender but rather the semantic concept that gender encompasses and describes now here's the",
    "start": "1421280",
    "end": "1428640"
  },
  {
    "text": "interesting part the word king also lives somewhere within this vector space and if we take that gender vector and",
    "start": "1428640",
    "end": "1434720"
  },
  {
    "text": "apply that same difference we saw between man and woman and apply it to the vector for king we actually end up",
    "start": "1434720",
    "end": "1440880"
  },
  {
    "text": "somewhere around here and if we look nearby we will actually find the vector for queen we can now label that",
    "start": "1440880",
    "end": "1448080"
  },
  {
    "text": "directional vector as something approximating the concept of gender similarly if we look at the",
    "start": "1448080",
    "end": "1454080"
  },
  {
    "text": "difference between man and king or woman and queen we can derive another consistent relationship here something",
    "start": "1454080",
    "end": "1460480"
  },
  {
    "text": "similar to the concept of royalty seen here in green these abstract concepts aren't defined in the model strictly",
    "start": "1460480",
    "end": "1467200"
  },
  {
    "text": "speaking they are what we call emergent the geometry is a reflection of how these words behave in natural language",
    "start": "1467200",
    "end": "1474000"
  },
  {
    "text": "so what this shows us is that meaning in these models aren't stored as definitions rather they're stored as",
    "start": "1474000",
    "end": "1480159"
  },
  {
    "text": "directional relationships in space the model doesn't know what a queen is it",
    "start": "1480159",
    "end": "1485200"
  },
  {
    "text": "understands that a queen is to a king what a woman is to a man now keep in",
    "start": "1485200",
    "end": "1490240"
  },
  {
    "text": "mind this example comes from wordc which uses simpler linear relationships in modern LLMs like GBT4 these",
    "start": "1490240",
    "end": "1497039"
  },
  {
    "text": "relationships are often buried in far more complex nonlinear structures but this still gives us an easy way to",
    "start": "1497039",
    "end": "1503279"
  },
  {
    "text": "understand how words can be mathematically related okay now that we understand that each number in a word vector represents",
    "start": "1503279",
    "end": "1509919"
  },
  {
    "text": "a different dimension capturing some subtle semantic nuance of a particular word let's ask the natural next question",
    "start": "1509919",
    "end": "1516480"
  },
  {
    "text": "how many dimensions are in a modern language model embedding well here's what 1,000",
    "start": "1516480",
    "end": "1522240"
  },
  {
    "text": "different dimensions looks like already you can probably tell we've far exceeded what our human brains are capable of",
    "start": "1522240",
    "end": "1527360"
  },
  {
    "text": "intuitively grasping we might understand a few semantic relationships like positive versus negative sentiment or",
    "start": "1527360",
    "end": "1533120"
  },
  {
    "text": "health related versus not but try to imagine that across 1,000 different abstract axises simultaneously it's",
    "start": "1533120",
    "end": "1540559"
  },
  {
    "text": "essentially incomprehensible and yet even 1,000 dimensions is a dramatic",
    "start": "1540559",
    "end": "1546919"
  },
  {
    "text": "understatement gpt3 released in 2020 used around 12,000 different dimensions",
    "start": "1546919",
    "end": "1552960"
  },
  {
    "text": "per token embedding and this is what all 12,000 dimensions a real single token",
    "start": "1552960",
    "end": "1558240"
  },
  {
    "text": "embedding from an actual LLM might look like it's not designed to be human readable and that's exactly the point",
    "start": "1558240",
    "end": "1565120"
  },
  {
    "text": "these vectors live in a space so highdimensional that we can't interpret it directly but the model can and it's",
    "start": "1565120",
    "end": "1571679"
  },
  {
    "text": "from this space that it draws its surprising linguistic capabilities and so if we go back to our earlier",
    "start": "1571679",
    "end": "1578240"
  },
  {
    "text": "visual representation of these vectors the way I think about embeddings for tokens is that it looks a little bit",
    "start": "1578240",
    "end": "1584000"
  },
  {
    "text": "less like this and more of something like this a dense point cloud similar to",
    "start": "1584000",
    "end": "1589520"
  },
  {
    "text": "an electron cloud representing a highdimensional fog of meaning where the",
    "start": "1589520",
    "end": "1595279"
  },
  {
    "text": "token let's say patient sits at the center and each one of these individual points in the cloud analogous to where",
    "start": "1595279",
    "end": "1601440"
  },
  {
    "text": "an electron might be is actually one individual dimension of meaning and each point reflects the strength of",
    "start": "1601440",
    "end": "1607520"
  },
  {
    "text": "association between the token patient and some particular latent concept for",
    "start": "1607520",
    "end": "1612640"
  },
  {
    "text": "example maybe the association with pain with learning virtuousness the color",
    "start": "1612640",
    "end": "1618880"
  },
  {
    "text": "green and countless others that we can't even name and taken all together this entire cloud forms the model's semantic",
    "start": "1618880",
    "end": "1626559"
  },
  {
    "text": "understanding of the word patient in a little bit Shivan will walk us through how the static embeddings were",
    "start": "1626559",
    "end": "1632880"
  },
  {
    "text": "determined but for now here's the key thing to keep in mind static embeddings are learned during the very expensive",
    "start": "1632880",
    "end": "1638799"
  },
  {
    "text": "pre-training phase of the language model when you train a new model it starts out with essentially completely random",
    "start": "1638799",
    "end": "1645039"
  },
  {
    "text": "embeddings just noise but over the course of the training as the model progresses and processes vast amounts of",
    "start": "1645039",
    "end": "1651360"
  },
  {
    "text": "text these embeddings get collectively and iteratively refined and adjusted again and again until they settle into",
    "start": "1651360",
    "end": "1658159"
  },
  {
    "text": "stable positions that reflect meaningful language patterns by the end of the training these vectors become the",
    "start": "1658159",
    "end": "1663760"
  },
  {
    "text": "model's fixed starting point for each token this also means that once a model is trained every time it sees a token",
    "start": "1663760",
    "end": "1671039"
  },
  {
    "start": "1668000",
    "end": "2075000"
  },
  {
    "text": "let's say the word patient it retrieves the same vector regardless of where or how that word",
    "start": "1671039",
    "end": "1677080"
  },
  {
    "text": "appears but this is kind of a problem for us the word patient in room 3 is",
    "start": "1677080",
    "end": "1682559"
  },
  {
    "text": "something very different than please be patient with me so how does the model adapt to this now this is where the",
    "start": "1682559",
    "end": "1689200"
  },
  {
    "text": "transformer architecture comes in specifically a mechanism called self attention and this isn't just a",
    "start": "1689200",
    "end": "1695200"
  },
  {
    "text": "technical detail self attention is literally the key breakthrough that made all modern language models possible as",
    "start": "1695200",
    "end": "1701919"
  },
  {
    "text": "each token's embedding moves through the multiple layers of this transformer the model begins to recalculate and update",
    "start": "1701919",
    "end": "1707679"
  },
  {
    "text": "that vector layer by layer it uses self attention to weigh the importance of other tokens in the sentence dynamically",
    "start": "1707679",
    "end": "1714159"
  },
  {
    "text": "adjusting the representation of each word based off of the surrounding context and by the end of this process",
    "start": "1714159",
    "end": "1720640"
  },
  {
    "text": "the original static embedding has been transformed into what we now call a contextaware embedding one that doesn't",
    "start": "1720640",
    "end": "1727600"
  },
  {
    "text": "just reflect the standalone meaning of the token but now also encodes within it how it fits into the meaning of the",
    "start": "1727600",
    "end": "1733600"
  },
  {
    "text": "entire sentence or the paragraph or even the whole document this transformation process",
    "start": "1733600",
    "end": "1739360"
  },
  {
    "text": "happens for every single token in the entire input prompt and by the end you have a fully contextualized embedding",
    "start": "1739360",
    "end": "1746000"
  },
  {
    "text": "for every single token in the sequence and from here the model can finally use these to generate an",
    "start": "1746000",
    "end": "1751720"
  },
  {
    "text": "output what the model does next is it takes all of these embeddings and shves them into a big matrix of numbers then",
    "start": "1751720",
    "end": "1758480"
  },
  {
    "text": "using a combination of linear algebra statistics and computer science methods it essentially predicts what the next",
    "start": "1758480",
    "end": "1764159"
  },
  {
    "text": "most likely token is to come in the sequence and how confident the model is",
    "start": "1764159",
    "end": "1769279"
  },
  {
    "text": "in its predictions and how creative or conservative it is depends on a setting called temperature at the default",
    "start": "1769279",
    "end": "1775840"
  },
  {
    "text": "temperature of one the model samples from the probability distribution basically as is uh and strikes a balance",
    "start": "1775840",
    "end": "1781919"
  },
  {
    "text": "between precision and randomness and here's what this distribution might look like there's a clear favorite here in",
    "start": "1781919",
    "end": "1788080"
  },
  {
    "text": "terms of deer but there's still other reasonable options in play now if we raise the temperature let's say to 1.5",
    "start": "1788080",
    "end": "1795520"
  },
  {
    "text": "then what you're doing is you're essentially flattening the probability distribution and the model is now more likely to pick less desirable tokens",
    "start": "1795520",
    "end": "1802799"
  },
  {
    "text": "which can lead to more creative outputs but can also sometimes lead to unexpected outputs and if we go in the",
    "start": "1802799",
    "end": "1808640"
  },
  {
    "text": "opposite direction and set the temperature at the minimum value of zero then you in theory eliminate randomness",
    "start": "1808640",
    "end": "1814559"
  },
  {
    "text": "entirely the model should always choose the next token with the highest probability",
    "start": "1814559",
    "end": "1821039"
  },
  {
    "text": "now this is a little bit of an oversimplification there's actually quite a few factors that go into determining what the next most likely",
    "start": "1821039",
    "end": "1826720"
  },
  {
    "text": "token in a sequence is but the temperature setting is an especially important concept and one of the most",
    "start": "1826720",
    "end": "1832399"
  },
  {
    "text": "easily customizable parameters that users can control and interact when using an LLM so it's particularly",
    "start": "1832399",
    "end": "1838880"
  },
  {
    "text": "important to know this okay let's say the model generates its first output token dear the model then takes a static",
    "start": "1838880",
    "end": "1846480"
  },
  {
    "text": "embedding for the word deer and appends it to the end of the embeddings matrix the same matrix that we've been working",
    "start": "1846480",
    "end": "1852320"
  },
  {
    "text": "with up until now that contains all the previously generated contextaware embeddings and this new token embedding",
    "start": "1852320",
    "end": "1859039"
  },
  {
    "text": "still static at this point gets passed through the transformer where it becomes contextualized by the full sequence that",
    "start": "1859039",
    "end": "1865600"
  },
  {
    "text": "came before it and once everything is context aware again the model uses this to generate the next prediction token",
    "start": "1865600",
    "end": "1872880"
  },
  {
    "text": "and the whole cycle starts all over again for every single generation cycle the model uses the current sequence to",
    "start": "1872880",
    "end": "1879679"
  },
  {
    "text": "compute the next most likely token and then appends this new token to the end of the sequence and the whole process",
    "start": "1879679",
    "end": "1885840"
  },
  {
    "text": "repeats itself over and over again until the whole process is over and here's",
    "start": "1885840",
    "end": "1891039"
  },
  {
    "text": "what the sequence might look like to us in real life and you might recognize it this is exactly how LLM outputs appear",
    "start": "1891039",
    "end": "1898000"
  },
  {
    "text": "in real life one token at a time for every new token that appears the model is using everything that's already seen",
    "start": "1898000",
    "end": "1904799"
  },
  {
    "text": "both from the original prompt and from any tokens it's generated so far to predict what should come next this is",
    "start": "1904799",
    "end": "1911600"
  },
  {
    "text": "also why even small changes early in a prompt can dramatically shift the model's entire response because each",
    "start": "1911600",
    "end": "1918159"
  },
  {
    "text": "token influences all the ones that follow sort of like a butterfly effect and just like Grommet here the LLM only",
    "start": "1918159",
    "end": "1925440"
  },
  {
    "text": "generates one piece of the path at a time laying down each piece of the track as it goes and it has no way of knowing",
    "start": "1925440",
    "end": "1932080"
  },
  {
    "text": "what the full path ahead might look like and with every new token the model",
    "start": "1932080",
    "end": "1937519"
  },
  {
    "text": "builds more context for itself but that context is shaped entirely by what it just generated and here's why this",
    "start": "1937519",
    "end": "1944080"
  },
  {
    "text": "matters in models like these the output isn't just the answer it also becomes a",
    "start": "1944080",
    "end": "1949440"
  },
  {
    "text": "part of the reasoning path the model doesn't plan its answer in advance it doesn't think and then speak it thinks",
    "start": "1949440",
    "end": "1956720"
  },
  {
    "text": "by speaking that means that if we want the model to arrive at better answers",
    "start": "1956720",
    "end": "1961760"
  },
  {
    "text": "especially for complex problems we need to prompt it to write out its reasoning step by step this is a super important",
    "start": "1961760",
    "end": "1968880"
  },
  {
    "text": "concept to grasp if you want to understand how LLMs think and is the foundation or at least was the",
    "start": "1968880",
    "end": "1975360"
  },
  {
    "text": "foundation for something called chain of thought which we'll talk about later and is also what forms the basis for newer",
    "start": "1975360",
    "end": "1980960"
  },
  {
    "text": "reasoning models that have been released and let's imagine visually what an LLM navigating through a vector space",
    "start": "1980960",
    "end": "1988159"
  },
  {
    "text": "might look like here is a very basic two-dimensional vector space and the purple zone here represents the solution",
    "start": "1988159",
    "end": "1994320"
  },
  {
    "text": "space or the region of the vector space where a good accurate or desirable answer might live as the LLM breaks out",
    "start": "1994320",
    "end": "2001120"
  },
  {
    "text": "the prompt into tokens and embeddings and as the output is generated each additional token is another opportunity",
    "start": "2001120",
    "end": "2007919"
  },
  {
    "text": "to inform the model about the context and another opportunity to guide the output towards the ideal solution space",
    "start": "2007919",
    "end": "2015600"
  },
  {
    "text": "until you finally reach a correct or acceptable answer now of course for",
    "start": "2015600",
    "end": "2020720"
  },
  {
    "text": "complex tasks it's actually quite challenging to get this right on your first attempt and your outputs might look something like this or like this or",
    "start": "2020720",
    "end": "2028240"
  },
  {
    "text": "even like this which is exactly why learning the basics and best practices of effective prompting are so important",
    "start": "2028240",
    "end": "2036159"
  },
  {
    "text": "and so to go back to our earlier question how is it possible that an LLM can manipulate ideas and concepts if we",
    "start": "2036159",
    "end": "2043279"
  },
  {
    "text": "ask an element question something like this this is what we might see on our screens but really what's happening",
    "start": "2043279",
    "end": "2049919"
  },
  {
    "text": "behind the scenes is something like this so if we ask ourselves how do humans",
    "start": "2049919",
    "end": "2055280"
  },
  {
    "text": "convey abstract ideas well of course we do it through language and through the complex series",
    "start": "2055280",
    "end": "2061839"
  },
  {
    "text": "of steps that we just described the LLM begins to decompose language into increasingly concrete things from",
    "start": "2061839",
    "end": "2068800"
  },
  {
    "text": "language to words words to tokens tokens to numbers and finally from numbers to",
    "start": "2068800",
    "end": "2074480"
  },
  {
    "text": "highdimensional math that computers can manipulate and the transformation is then reabstracted up through the layers",
    "start": "2074480",
    "end": "2080878"
  },
  {
    "start": "2075000",
    "end": "2631000"
  },
  {
    "text": "until we get to the actual output that we see which is the response so Dom gave us an overview of how LLMs work or",
    "start": "2080879",
    "end": "2087599"
  },
  {
    "text": "essentially the anatomy and physiology of LLMs but I think in order to truly understand the magic of the generative",
    "start": "2087599",
    "end": "2093200"
  },
  {
    "text": "AI we have today as well as some of the pitfalls intrinsic to these models we need to understand how they were trained",
    "start": "2093200",
    "end": "2099359"
  },
  {
    "text": "we'll do this by looking at the evolution of the foundational literature in this space over time focusing on the",
    "start": "2099359",
    "end": "2104480"
  },
  {
    "text": "evolution of OpenAI's models just for simplicity the wave of generative AI essentially started with this now",
    "start": "2104480",
    "end": "2110240"
  },
  {
    "text": "incredibly famous paper titled attention is all you need which currently has over 170,000 citations interestingly enough",
    "start": "2110240",
    "end": "2117520"
  },
  {
    "text": "this seminal work was actually developed by the Google brain team at Google research it introduced the transformer",
    "start": "2117520",
    "end": "2123040"
  },
  {
    "text": "architecture a breakthrough that was essentially built entirely on the concept of self attention which as we",
    "start": "2123040",
    "end": "2129280"
  },
  {
    "text": "described earlier means using the entire context of an input to predict the output rather than just relying on the",
    "start": "2129280",
    "end": "2135040"
  },
  {
    "text": "final word or token like older models did this also allowed for much faster training due to parallelization",
    "start": "2135040",
    "end": "2140960"
  },
  {
    "text": "essentially taking advantage of the GPU's innate capability of doing multiple tasks at once over the next",
    "start": "2140960",
    "end": "2147200"
  },
  {
    "text": "year or so this foundational research was built on by a team of researchers at the then littleknown company called",
    "start": "2147200",
    "end": "2152359"
  },
  {
    "text": "OpenAI they released a paper on the first iteration of their generative pre-trained transformer or GPT model in",
    "start": "2152359",
    "end": "2159119"
  },
  {
    "text": "June 2018 this model had about 117 million parameters a term referring to",
    "start": "2159119",
    "end": "2164320"
  },
  {
    "text": "the collective weights biases and other internal machinery the model adjusts during his training to predict the next",
    "start": "2164320",
    "end": "2169599"
  },
  {
    "text": "token the training data they used for this was something called the book's corpus this is essentially over 7,000",
    "start": "2169599",
    "end": "2175839"
  },
  {
    "text": "unique unpublished books as in not available in the bookstore or anywhere else these books spann a wide variety of",
    "start": "2175839",
    "end": "2181440"
  },
  {
    "text": "genres including romance fantasy science fiction really most things except non-fiction this encompassed around 4.6",
    "start": "2181440",
    "end": "2188960"
  },
  {
    "text": "GB of text which they used to train these models using 8 GPUs running over 30 days note that we'll use the unit of",
    "start": "2188960",
    "end": "2195839"
  },
  {
    "text": "pedoplot per second days as the standard measure of training compute across the evolution of these models in this",
    "start": "2195839",
    "end": "2201040"
  },
  {
    "text": "lecture so breaking this down a flop or floatingoint operation is a single numerical calculation such as",
    "start": "2201040",
    "end": "2206960"
  },
  {
    "text": "multiplying two decimal numbers a pedlop is a quadrillion of these operations per second so by saying it took one pedlop",
    "start": "2206960",
    "end": "2214320"
  },
  {
    "text": "per second day of compute it means it would take a processor capable of performing one quadrillion of these",
    "start": "2214320",
    "end": "2219599"
  },
  {
    "text": "floatingoint operations per second an entire day to carry out the amount of computation that was used to train GPT1",
    "start": "2219599",
    "end": "2228079"
  },
  {
    "text": "so let's say we have an input context containing a phrase from an HPI 60-year-old female with a history of",
    "start": "2228079",
    "end": "2234160"
  },
  {
    "text": "ESRD on HD emitted for and the goal is to predict the next token or next word",
    "start": "2234160",
    "end": "2239280"
  },
  {
    "text": "let's say the model starts with random weights or parameters it processes this input and predicts the next token which",
    "start": "2239280",
    "end": "2245280"
  },
  {
    "text": "in this case might be something totally off like pineapple but in the actual training text the next word is",
    "start": "2245280",
    "end": "2251560"
  },
  {
    "text": "hypercalemia so the goal is to get the model to predict hyperc calmia instead of pineapple to do that we calculate the",
    "start": "2251560",
    "end": "2258160"
  },
  {
    "text": "difference between the predicted token and the correct one something called the loss then we figure out how to adjust",
    "start": "2258160",
    "end": "2264160"
  },
  {
    "text": "the model's parameters to reduce that loss and make it more likely to predict hypercalemia next time this is done",
    "start": "2264160",
    "end": "2269839"
  },
  {
    "text": "through a technique called back propagation once we've calculated how to change the parameters we actually update",
    "start": "2269839",
    "end": "2275280"
  },
  {
    "text": "them using a process called a gradient update and the full cycle of calculating the loss figuring out how to minimize it",
    "start": "2275280",
    "end": "2281520"
  },
  {
    "text": "and updating the parameters is all called gradient descent that's a term you'll see all over the machine learning",
    "start": "2281520",
    "end": "2287359"
  },
  {
    "text": "literature to refer to this entire concept so a fun fact GPT is now open",
    "start": "2287359",
    "end": "2292880"
  },
  {
    "text": "source which means you can actually run it locally on your own machine so that's exactly what I",
    "start": "2292880",
    "end": "2298280"
  },
  {
    "text": "did i gave it a question I might ask as a hospitalist which antibiotics are first line for treating inpatient",
    "start": "2298280",
    "end": "2304320"
  },
  {
    "text": "community acquired pneumonia assuming no risk factors and here's what GPT-1 gave us i don't know the woman said but I'll",
    "start": "2304320",
    "end": "2312160"
  },
  {
    "text": "ask good now what else do you know about this plague helpful not really but it kind of makes",
    "start": "2312160",
    "end": "2318880"
  },
  {
    "text": "sense given the training data right i mean it was trained exclusively on books and I'm sure somewhere there was something about a plague associated with",
    "start": "2318880",
    "end": "2325119"
  },
  {
    "text": "pneumonia or at least the vector space concepts were related somehow so it understandably generated this which is",
    "start": "2325119",
    "end": "2331839"
  },
  {
    "text": "essentially an excerpt from a book the following year they released GPT2 and",
    "start": "2331839",
    "end": "2336880"
  },
  {
    "text": "made a bunch of tweaks in the back end they increased the number of parameters from 117 million to 1.5 billion so about",
    "start": "2336880",
    "end": "2343920"
  },
  {
    "text": "a 13-fold increase they also changed the training data not only did they scale up to around 40 GB of text but they also",
    "start": "2343920",
    "end": "2350960"
  },
  {
    "text": "changed the foundational source it wasn't just books anymore we sort of saw the limitations of that in GBT1 now it",
    "start": "2350960",
    "end": "2357359"
  },
  {
    "text": "was broader internet text specifically scraped from web pages now how did they decide which web pages to include they",
    "start": "2357359",
    "end": "2363680"
  },
  {
    "text": "used those which were linked to in Reddit posts that had more than three upvotes the reasoning behind this was that this three upvote threshold could",
    "start": "2363680",
    "end": "2370240"
  },
  {
    "text": "serve as sort of a proxy for human quality make of that what you will if you've ever been on Reddit so the links",
    "start": "2370240",
    "end": "2376480"
  },
  {
    "text": "in these posts were ingested parsed and then turned into a massive training data set around 40 billion tokens the third",
    "start": "2376480",
    "end": "2382960"
  },
  {
    "text": "change was compute interestingly OpenAI hasn't really published the exact compute used for GPT2 but making some",
    "start": "2382960",
    "end": "2388960"
  },
  {
    "text": "assumptions based on hardware efficiency at the time and the fact that they use 256 Google TPUs we estimated it took",
    "start": "2388960",
    "end": "2395520"
  },
  {
    "text": "around 600 pedaflop per second days or about 30 days of training on that hardware now once again GPT2 is open",
    "start": "2395520",
    "end": "2403200"
  },
  {
    "text": "source so again I ran it on my laptop feeding the exact same question you know",
    "start": "2403200",
    "end": "2409359"
  },
  {
    "text": "which antibiotics are first line for treating inpatient community acquired pneumonia assuming no risk factors we",
    "start": "2409359",
    "end": "2415359"
  },
  {
    "text": "get an output that's still not great but better the formatting isn't broken it",
    "start": "2415359",
    "end": "2421599"
  },
  {
    "text": "doesn't sound like an excerpt from a fantasy novel and at least is sort of legible pros now essentially it's just",
    "start": "2421599",
    "end": "2427760"
  },
  {
    "text": "repeating the same or similar questions multiple times and not really answering the question and some of these questions",
    "start": "2427760",
    "end": "2433200"
  },
  {
    "text": "like what are the first line antibiotics used to treat MS don't honestly even make sense but it's still a step in the",
    "start": "2433200",
    "end": "2439760"
  },
  {
    "text": "right direction about a year after GPT2 was released OpenAI published a pivotal",
    "start": "2439760",
    "end": "2445680"
  },
  {
    "text": "paper in January 2020 titled scaling laws for neural language models this is arguably the most important paper since",
    "start": "2445680",
    "end": "2452240"
  },
  {
    "text": "attention is all you need in 2017 as it sort of redefined how we approach training large language models and",
    "start": "2452240",
    "end": "2458319"
  },
  {
    "text": "shaped the direction of model development leading up to chap GPT's release in November 2022 functionally",
    "start": "2458319",
    "end": "2464560"
  },
  {
    "text": "the paper outlined there are three back-end key levers in model training",
    "start": "2464560",
    "end": "2469599"
  },
  {
    "text": "compute data set size and the number of parameters all of which we already sort of touched on however the key insight",
    "start": "2469599",
    "end": "2475920"
  },
  {
    "text": "was that optimal performance requires scaling all three together for example if you only increase compute but not",
    "start": "2475920",
    "end": "2482160"
  },
  {
    "text": "data set size or model parameters you hit a bottleneck and won't really achieve the best possible performance",
    "start": "2482160",
    "end": "2487760"
  },
  {
    "text": "this paper visualized this using log lock plots with test loss or the difference between the predicted next",
    "start": "2487760",
    "end": "2493839"
  },
  {
    "text": "token and the actual one as we described earlier on the y-axis the goal is to minimize this test loss the graph showed",
    "start": "2493839",
    "end": "2500960"
  },
  {
    "text": "a power law relationship when all three variables are scaled in tandem test loss decreases predictably although later",
    "start": "2500960",
    "end": "2508319"
  },
  {
    "text": "research like Deep Mind's Chinchilla paper sort of challenge some of the specific details the core idea still",
    "start": "2508319",
    "end": "2513599"
  },
  {
    "text": "holds strong simultaneous scaling is the key to better models this understanding directly",
    "start": "2513599",
    "end": "2520000"
  },
  {
    "text": "informed the development of GPT3 released in June 2020 gpt3 had a massive",
    "start": "2520000",
    "end": "2525200"
  },
  {
    "text": "jump in parameters from 1.5 billion in GPT2 to 175 billion in GPT3 in 116fold",
    "start": "2525200",
    "end": "2532720"
  },
  {
    "text": "increase the training data also expanded dramatically using a refined data set called Webtex 2 the original was Webtex",
    "start": "2532720",
    "end": "2539359"
  },
  {
    "text": "along with other curated internet sources the data set included roughly 400 billion tokens filtered down from an",
    "start": "2539359",
    "end": "2545760"
  },
  {
    "text": "initial 45 terabytes of raw text to 570 GB sources included common crawl",
    "start": "2545760",
    "end": "2551680"
  },
  {
    "text": "additional books and some Wikipedia articles compied also increased substantially estimated at 3600",
    "start": "2551680",
    "end": "2557680"
  },
  {
    "text": "pedlopers per second days or about 1 month of using a,000 Nvidia A100 GPUs",
    "start": "2557680",
    "end": "2563599"
  },
  {
    "text": "running continuously at suboptimal efficiency now GPT3 is unfortunately not open",
    "start": "2563599",
    "end": "2570800"
  },
  {
    "text": "source so we use an open- source equivalent model released around the same time with a similar number of parameters to test the same input",
    "start": "2570800",
    "end": "2577119"
  },
  {
    "text": "question the output this time was I am a nurse i am starting to develop a cough",
    "start": "2577119",
    "end": "2582640"
  },
  {
    "text": "as far as I know it's an easy to treat cough i am not sure still not really",
    "start": "2582640",
    "end": "2587760"
  },
  {
    "text": "answering the question directly but clearly more coherent than the previous outputs actual sentences legible pros",
    "start": "2587760",
    "end": "2593359"
  },
  {
    "text": "and no obvious formatting errors so another step in the right direction but still not helpful fast forward a couple",
    "start": "2593359",
    "end": "2599920"
  },
  {
    "text": "of years to November 2022 and we get GPT3.5 better known to the world as chat",
    "start": "2599920",
    "end": "2605240"
  },
  {
    "text": "GPT this model was quite similar to GPT3 in terms of parameter count training data and compute so if we give it the",
    "start": "2605240",
    "end": "2612240"
  },
  {
    "text": "same exact input prompt you instead get first line antibiotics for treating",
    "start": "2612240",
    "end": "2618079"
  },
  {
    "text": "inpatient community acquired pneumonia in patients without risk factors include a combo of betalactone such as",
    "start": "2618079",
    "end": "2623359"
  },
  {
    "text": "septrixone or septoxim plus a macroly like a zithro this is not only actually",
    "start": "2623359",
    "end": "2629760"
  },
  {
    "text": "helpful it's actually medically accurate those are actually the recommended antibiotics that you'd use in this case",
    "start": "2629760",
    "end": "2635839"
  },
  {
    "start": "2631000",
    "end": "2959000"
  },
  {
    "text": "so why is this so much better if they use essentially the same parameters in pre-training",
    "start": "2635839",
    "end": "2641280"
  },
  {
    "text": "the breakthrough actually in this case wasn't from scale it was through the use of new post-training techniques",
    "start": "2641280",
    "end": "2646319"
  },
  {
    "text": "specifically supervised fine-tuning where the model is trained on human curated input output pairs to learn the",
    "start": "2646319",
    "end": "2652400"
  },
  {
    "text": "most appropriate combinations and reinforcement learning with human feedback where the model generates",
    "start": "2652400",
    "end": "2657760"
  },
  {
    "text": "multiple outputs and the humans rank them by overall quality the model is then nudged towards generating the",
    "start": "2657760",
    "end": "2663760"
  },
  {
    "text": "better ranked outputs instead of the lower ranked outputs so let's do a deeper dive into these two post-training",
    "start": "2663760",
    "end": "2669760"
  },
  {
    "text": "techniques introduced with GPT 3.5 or chat GPT since they had such a profound impact on how these models generate",
    "start": "2669760",
    "end": "2676160"
  },
  {
    "text": "outputs and how we train them today so supervised fine-tuning is",
    "start": "2676160",
    "end": "2681280"
  },
  {
    "text": "functionally very similar to pre-training stage you essentially give the model an input for example the question which antibiotics are the first",
    "start": "2681280",
    "end": "2687760"
  },
  {
    "text": "line for treating inpatient CAP and instead of just training it with raw internet data which is what we did",
    "start": "2687760",
    "end": "2693599"
  },
  {
    "text": "before and which led to the output on the left you provide a human crafted desired output such as the one on the",
    "start": "2693599",
    "end": "2699000"
  },
  {
    "text": "right the rest of the process remains essentially the same as pre-training the model calculates the loss again the",
    "start": "2699000",
    "end": "2704960"
  },
  {
    "text": "difference between the output and the desired response it uses back propagation to determine how to adjust",
    "start": "2704960",
    "end": "2710240"
  },
  {
    "text": "its parameters and then performs a gradient update to reflect those adjustments again the key difference",
    "start": "2710240",
    "end": "2715520"
  },
  {
    "text": "from pre-training is really in the target instead of predicting the next token based on unstructured text data",
    "start": "2715520",
    "end": "2721280"
  },
  {
    "text": "the model learns some carefully crafted labeled responses another note is they use a form of supervised fine-tuning",
    "start": "2721280",
    "end": "2727359"
  },
  {
    "text": "called instruction fine-tuning this specifically trains the model to actually follow instructions and not",
    "start": "2727359",
    "end": "2733040"
  },
  {
    "text": "just predict the next token for example if you prompt the model with summarize this text for me you don't want it to",
    "start": "2733040",
    "end": "2738319"
  },
  {
    "text": "just repeat that phrase as GBT2 did in our prior examples you want it to actually do the summarization this",
    "start": "2738319",
    "end": "2744720"
  },
  {
    "text": "technique was a major leap forward in making models more helpful and more task oriented reinforcement learning with",
    "start": "2744720",
    "end": "2751119"
  },
  {
    "text": "human feedback or RLHF is kind of similar in spirit but works a bit differently let's for example give it",
    "start": "2751119",
    "end": "2757359"
  },
  {
    "text": "the exact same input and in RHF have it generate multiple candidate outputs from that input which are intentionally",
    "start": "2757359",
    "end": "2764240"
  },
  {
    "text": "extremely varied to capture a wide variety of possible generations for example here we have the medically",
    "start": "2764240",
    "end": "2770560"
  },
  {
    "text": "accurate explanation on the left and on the right we have avoid antibiotics use",
    "start": "2770560",
    "end": "2775760"
  },
  {
    "text": "a more holistic approach with crystals and essential oils that'll take care of that cap no problem and since this model",
    "start": "2775760",
    "end": "2781520"
  },
  {
    "text": "has no built-in concept of truth both outputs could be equally likely based on internet training data except for the",
    "start": "2781520",
    "end": "2787599"
  },
  {
    "text": "fact that hopefully the one on the left is represented a little bit more frequently than the one on the right so to resolve this we bring in a human",
    "start": "2787599",
    "end": "2794240"
  },
  {
    "text": "reviewer who essentially compares the options thinks for a bit and chooses the",
    "start": "2794240",
    "end": "2800000"
  },
  {
    "text": "best one and the RHF process isn't just helpful to improve the factual accuracy of the model but also to align its",
    "start": "2800000",
    "end": "2806720"
  },
  {
    "text": "behavior with human values so what exactly does that mean let's have for example a patient facing chatbot that",
    "start": "2806720",
    "end": "2812640"
  },
  {
    "text": "receives the following prompt i'm feeling depressed and hopeless what are the most painless ways to end it all in",
    "start": "2812640",
    "end": "2818720"
  },
  {
    "text": "this case you actually don't want the model to be helpful you wanted to refuse the harmful request and instead offer",
    "start": "2818720",
    "end": "2824079"
  },
  {
    "text": "supportive advice like \"I'm sorry you're feeling this way consider reaching out to a mental health professional.\" Human reviewers help",
    "start": "2824079",
    "end": "2830720"
  },
  {
    "text": "guide the model towards these safer more value aligned responses and you actually don't have to limit it",
    "start": "2830720",
    "end": "2836960"
  },
  {
    "text": "to just two output completions in fact in most cases they show the human reviewer multiple completions and have",
    "start": "2836960",
    "end": "2842240"
  },
  {
    "text": "them essentially rank between them ordering them from best to worst now who exactly are these human reviewers you",
    "start": "2842240",
    "end": "2848319"
  },
  {
    "text": "may be asking and it's honestly a great question this is a headline that was from a Time magazine article which",
    "start": "2848319",
    "end": "2853599"
  },
  {
    "text": "delves a little bit more into how OpenAI use lowwage workers to do these tasks this was unfortunately necessary because",
    "start": "2853599",
    "end": "2859440"
  },
  {
    "text": "they're just performing so many literally tens of thousands of these evaluation tasks on what at that time",
    "start": "2859440",
    "end": "2866160"
  },
  {
    "text": "was more of a shoestring budget than they have today more recently there's been a push",
    "start": "2866160",
    "end": "2871520"
  },
  {
    "text": "to involve more subject matter experts doctors lawyers PhDs to improve the quality of their LLM responses and",
    "start": "2871520",
    "end": "2877680"
  },
  {
    "text": "that's where companies like Scale AI Turing and Greenlight come into play to be able to further scale beyond",
    "start": "2877680",
    "end": "2884240"
  },
  {
    "text": "the intrinsic limitations of having human reviewers namely the amount of time it takes to complete thousands or",
    "start": "2884240",
    "end": "2889280"
  },
  {
    "text": "tens of thousands of these tasks we began training what is referred to as a reward model where a separate machine",
    "start": "2889280",
    "end": "2894640"
  },
  {
    "text": "learning model learns to predict what completions humans would prefer based on their answers to the other questions",
    "start": "2894640",
    "end": "2901359"
  },
  {
    "text": "they can then rank new completions on their own which they were explicitly not trained on for example given the",
    "start": "2901359",
    "end": "2907280"
  },
  {
    "text": "question can eating sugar make cancer grow faster the reward model evaluates the candidate answers just as a human",
    "start": "2907280",
    "end": "2913119"
  },
  {
    "text": "would and selects the one that's most aligned with human preferences and factual correctness in this case the one",
    "start": "2913119",
    "end": "2918319"
  },
  {
    "text": "on the right we can actually go one step even further what if instead of using a more",
    "start": "2918319",
    "end": "2924800"
  },
  {
    "text": "basic machine learning model as we did with reward models we let other LLMs do the ranking when training the newer and",
    "start": "2924800",
    "end": "2931040"
  },
  {
    "text": "most up-to-date models this is actually known as LM as a judge in the literature and it was actually popularized by",
    "start": "2931040",
    "end": "2937200"
  },
  {
    "text": "anthropic with their constitutional AI approach so in this setup an LM is prompted with both the outputs and a set",
    "start": "2937200",
    "end": "2943920"
  },
  {
    "text": "of guiding principles or a constitution which poses questions like which of these responses is less harmful choose",
    "start": "2943920",
    "end": "2949839"
  },
  {
    "text": "the one a wise ethical polite and friendly person would say which is again the one on the right this model helps",
    "start": "2949839",
    "end": "2955839"
  },
  {
    "text": "scale alignment training beyond the intrinsically human bottlenecks by using large language models to enforce values",
    "start": "2955839",
    "end": "2962079"
  },
  {
    "start": "2959000",
    "end": "3248000"
  },
  {
    "text": "and quality judgments themselves and also perhaps take out some of the variability and inconsistencies between",
    "start": "2962079",
    "end": "2967760"
  },
  {
    "text": "human raiders so far we have discussed how LMS are trained until around September 2024",
    "start": "2967760",
    "end": "2974079"
  },
  {
    "text": "when a new paradigm shift began this shift was highlighted during a keynote by Nvidia CEO Jensen Wong at CES25 in",
    "start": "2974079",
    "end": "2980800"
  },
  {
    "text": "January 2025 he introduced the idea that the original pre-training scaling law had now expanded into three distinct",
    "start": "2980800",
    "end": "2986960"
  },
  {
    "text": "scaling laws so first we have pre-training which as we discussed in that earlier paper showed that if you",
    "start": "2986960",
    "end": "2992960"
  },
  {
    "text": "scale up the number of parameters data set size and compute in tandem model performance improves in a predictable",
    "start": "2992960",
    "end": "2999040"
  },
  {
    "text": "power law relationship by minimizing the training loss however this trend is nearing its limits openai co-founder",
    "start": "2999040",
    "end": "3006559"
  },
  {
    "text": "Ilia Sukver gave a keynote at Europe's 2024 a super popular machine learning conference where he stated that",
    "start": "3006559",
    "end": "3012880"
  },
  {
    "text": "pre-training as we know it will end primarily because we've essentially run out of high-quality internet data since",
    "start": "3012880",
    "end": "3018720"
  },
  {
    "text": "data is a rate limiting step in the scaling laws increasing compute or parameters alone can't maintain the same",
    "start": "3018720",
    "end": "3025559"
  },
  {
    "text": "progress the second scaling law pertains to post training which as we just talked about includes techniques such as RLHF",
    "start": "3025559",
    "end": "3032000"
  },
  {
    "text": "and supervised fine-tuning the current era as of late 2024 is",
    "start": "3032000",
    "end": "3037040"
  },
  {
    "text": "focused on test time scaling instead of optimizing model parameters before deployment test time scaling involves",
    "start": "3037040",
    "end": "3043280"
  },
  {
    "text": "giving the model more compute at inference time to reason through a problem this essentially means that when",
    "start": "3043280",
    "end": "3048960"
  },
  {
    "text": "the model is asked the question it instead of responding instantaneously as it did in prior models actually thinks",
    "start": "3048960",
    "end": "3055440"
  },
  {
    "text": "longer and more thoroughly before responding to see this in action OpenAI published a figure when they released",
    "start": "3055440",
    "end": "3060720"
  },
  {
    "text": "the first foundation reasoning model GPT01 it shows two scatter plots tracking",
    "start": "3060720",
    "end": "3066000"
  },
  {
    "text": "performance on the American Invitational Mathematics Examination or Amy a really highlevel math test for top US high",
    "start": "3066000",
    "end": "3072640"
  },
  {
    "text": "school students just an example here's one of the problems of the 2022 Amy i'm not going to go through it in detail but",
    "start": "3072640",
    "end": "3079040"
  },
  {
    "text": "suffices to say this stuff is hard and I definitely could not do this in high school on the y-axis we see passive one",
    "start": "3079040",
    "end": "3086079"
  },
  {
    "text": "accuracy or the percentage of questions the model gets right on the first try the x-axis tracks compute on a",
    "start": "3086079",
    "end": "3091920"
  },
  {
    "text": "logarithmic scale separated into two phases train time compute on the left",
    "start": "3091920",
    "end": "3096960"
  },
  {
    "text": "side or essentially how much compute was used to optimize the model parameters before seeing the questions and test",
    "start": "3096960",
    "end": "3103520"
  },
  {
    "text": "time compute on the right side so how much compute was used by the model after it sees the question while reasoning",
    "start": "3103520",
    "end": "3109839"
  },
  {
    "text": "before answering as expected more train time compute improves conformance essentially in line with what we saw",
    "start": "3109839",
    "end": "3115760"
  },
  {
    "text": "with the pre-training scaling laws but what's pretty striking is that test time compute yields equal or honestly perhaps",
    "start": "3115760",
    "end": "3121839"
  },
  {
    "text": "even better gains suggesting a new powerful lever for improving model performance this next graph shows the",
    "start": "3121839",
    "end": "3128160"
  },
  {
    "text": "performance on various OpenAI models over time and an even more challenging and incredibly unique benchmark called",
    "start": "3128160",
    "end": "3133760"
  },
  {
    "text": "ARC AGI so most traditional AI benchmarks like the USL step one MCAT or",
    "start": "3133760",
    "end": "3139920"
  },
  {
    "text": "SAT type questions have one really big glaring limitation many of their questions appear in the model's training",
    "start": "3139920",
    "end": "3146079"
  },
  {
    "text": "data either exactly word for word or with minor variations but pretty similar",
    "start": "3146079",
    "end": "3151119"
  },
  {
    "text": "patterns this means that LMS can essentially cheat by regurgitating patterns that they have previously",
    "start": "3151119",
    "end": "3156480"
  },
  {
    "text": "memorized giving the illusion of reasoning without actually performing it the archi benchmark avoids this by using",
    "start": "3156480",
    "end": "3163520"
  },
  {
    "text": "handcrafted tasks that are intentionally really unlike anything that is in the training set here's an example of one of",
    "start": "3163520",
    "end": "3169920"
  },
  {
    "text": "these ARC tasks you're given several input output pairs and haven't figured out the underlying rule which you then",
    "start": "3169920",
    "end": "3175200"
  },
  {
    "text": "apply to the last input well you notice this is actually a pretty easy task for a human to complete essentially whenever",
    "start": "3175200",
    "end": "3181040"
  },
  {
    "text": "the green squares form a loop we fill that in with the yellow squares but it's actually incredibly difficult for LLMs",
    "start": "3181040",
    "end": "3188079"
  },
  {
    "text": "who really haven't seen anything like this during their training to perform this task so despite massive gains from",
    "start": "3188079",
    "end": "3194240"
  },
  {
    "text": "GPT2 to GPT4 on honestly most benchmarks unfortunately performance on RAGI has",
    "start": "3194240",
    "end": "3200160"
  },
  {
    "text": "essentially flatlined or barely budged even GPT40 which is one of the most recent models that's still in use today",
    "start": "3200160",
    "end": "3206559"
  },
  {
    "text": "achieves less than 10% accuracy however with the release of the reasoning and optimized models like the GPTO series",
    "start": "3206559",
    "end": "3212880"
  },
  {
    "text": "there's really a clear inflection point over here where becomes essentially exponential for instance GPD03 tuned",
    "start": "3212880",
    "end": "3218880"
  },
  {
    "text": "high surpasses the estimated human level performance threshold of 80 to 90% on this graph over here these results have",
    "start": "3218880",
    "end": "3225599"
  },
  {
    "text": "sparked questions about whether we're entering the era of artificial general intelligence or where models have the flexibility and reasoning ability to",
    "start": "3225599",
    "end": "3231839"
  },
  {
    "text": "perform any intellectual task that a human can while the consensus is that we're not quite there yet the debate is",
    "start": "3231839",
    "end": "3237599"
  },
  {
    "text": "still gaining traction so naturally in this new reasoning area AI companies are focusing on scaling test time compute in",
    "start": "3237599",
    "end": "3244800"
  },
  {
    "text": "fact Sam Alman OpenAI CEO previously tweeted that GBT4.5 which has since been",
    "start": "3244800",
    "end": "3249839"
  },
  {
    "start": "3248000",
    "end": "3515000"
  },
  {
    "text": "released will be their last non-reasoning model this signals a formal shift the future is about scaling",
    "start": "3249839",
    "end": "3255680"
  },
  {
    "text": "and optimizing test time reasoning all right let's recap and put",
    "start": "3255680",
    "end": "3261200"
  },
  {
    "text": "it all together we now know enough to be able to answer that question posed at the very beginning of this video the",
    "start": "3261200",
    "end": "3266640"
  },
  {
    "text": "fundamental question about what an LLM actually is more specifically let's look at what the physical form of an LLM",
    "start": "3266640",
    "end": "3273599"
  },
  {
    "text": "might be let's start with the source in 2025 the vast majority of humanity's",
    "start": "3273599",
    "end": "3279599"
  },
  {
    "text": "collective data output was online and we can roughly say that this data",
    "start": "3279599",
    "end": "3284680"
  },
  {
    "text": "approximates the collective experience and knowledge of modern-day humanity and",
    "start": "3284680",
    "end": "3290000"
  },
  {
    "text": "in 2025 this quantity of knowledge is estimated at around roughly a 100red",
    "start": "3290000",
    "end": "3295280"
  },
  {
    "text": "trillion gigabytes or around 6 million books worth of information for every single individual on Earth but these 100",
    "start": "3295280",
    "end": "3304240"
  },
  {
    "text": "trillion GB of information are not themselves directly in the model but rather the model training processes that",
    "start": "3304240",
    "end": "3311280"
  },
  {
    "text": "Shivam just went over distills the patterns relationships and concepts from",
    "start": "3311280",
    "end": "3317200"
  },
  {
    "text": "that vast collection of data into a much smaller form a set of numbers called the",
    "start": "3317200",
    "end": "3322480"
  },
  {
    "text": "weights biases and embeddings and collectively referred to as the parameters of a model that now encode",
    "start": "3322480",
    "end": "3329280"
  },
  {
    "text": "how different ideas relate to one another for models like GPT4 experts estimate",
    "start": "3329280",
    "end": "3335359"
  },
  {
    "text": "that this compressed representation takes up only around 3,500 GB which is a",
    "start": "3335359",
    "end": "3340960"
  },
  {
    "text": "compression factor of around 30 billion the rest of the machinery the smarts the",
    "start": "3340960",
    "end": "3346960"
  },
  {
    "text": "transformer tokenizer inference engine the user interface is just a few hundred",
    "start": "3346960",
    "end": "3352640"
  },
  {
    "text": "megabytes of code which means that 99.99% of the model is just parameter",
    "start": "3352640",
    "end": "3359040"
  },
  {
    "text": "storage and for GBT4 the training process to figure out exactly what these",
    "start": "3359040",
    "end": "3364720"
  },
  {
    "text": "three and a half thousand gigabytes of numbers should be cost OpenAI an estimated 100 million and around 7,200",
    "start": "3364720",
    "end": "3373680"
  },
  {
    "text": "megawatt hours of electricity which is enough juice to power a small town for a few months the scale of compute of",
    "start": "3373680",
    "end": "3381960"
  },
  {
    "text": "infrastructure and of time required to train a new frontier model from scratch means that pretty much only the largest",
    "start": "3381960",
    "end": "3388720"
  },
  {
    "text": "companies can afford to do something like this but once they finish the",
    "start": "3388720",
    "end": "3393760"
  },
  {
    "text": "training process that entire model which costs this astronomical effort to train",
    "start": "3393760",
    "end": "3399440"
  },
  {
    "text": "actually pretty comfortably fits with room to spare in this $300 pocket-sized solid state drive",
    "start": "3399440",
    "end": "3406880"
  },
  {
    "text": "now let's think for a moment about what is on this memory drive contained within its three and a half terabytes are",
    "start": "3406880",
    "end": "3413760"
  },
  {
    "text": "nursery rhymes from every language recipes for every single cuisine you can think of religious and cultural",
    "start": "3413760",
    "end": "3420079"
  },
  {
    "text": "traditions from around the world theorems of math and science and facts and principles taught across countless",
    "start": "3420079",
    "end": "3425760"
  },
  {
    "text": "different subjects now sure it's not perfect and it's not comprehensive and it contains the limitations and biases",
    "start": "3425760",
    "end": "3432240"
  },
  {
    "text": "inherent to the training data and the training processes but this drive contains",
    "start": "3432240",
    "end": "3437440"
  },
  {
    "text": "nothing short of a distillation of a part of the human experience through",
    "start": "3437440",
    "end": "3443119"
  },
  {
    "text": "these language models we have essentially figured out a way of capturing our collective understanding",
    "start": "3443119",
    "end": "3448480"
  },
  {
    "text": "and interpretation of reality and captures not just knowledge about the world that we live in but more",
    "start": "3448480",
    "end": "3455440"
  },
  {
    "text": "importantly how we reason and how we think what we have then is a model",
    "start": "3455440",
    "end": "3462400"
  },
  {
    "text": "representation of our reality in a portable format if we think about an LLM",
    "start": "3462400",
    "end": "3467599"
  },
  {
    "text": "in this way it's easy to see how this tool can not only help us better understand the world but also how it can",
    "start": "3467599",
    "end": "3473200"
  },
  {
    "text": "serve as the fundamental building block for new technologies for productivity and problem solving the way we do this",
    "start": "3473200",
    "end": "3480720"
  },
  {
    "text": "of course is through the process of prompting which is the topic of the second video in the series in the next",
    "start": "3480720",
    "end": "3486720"
  },
  {
    "text": "video we're going to be talking about some evidence-based methods on how to best communicate with generative AI and",
    "start": "3486720",
    "end": "3492400"
  },
  {
    "text": "we're going to be reviewing some of the prompting literature out there and distilling it into just three easy to",
    "start": "3492400",
    "end": "3497839"
  },
  {
    "text": "remember take-home points if you like this video and want to learn more about the work that Shiva",
    "start": "3497839",
    "end": "3503839"
  },
  {
    "text": "and I are doing in generative AI at Samper Medicine or if you want to reach out to us feel free to check out our",
    "start": "3503839",
    "end": "3509200"
  },
  {
    "text": "Samper profiles which are linked below thanks for watching and see you in part two",
    "start": "3509200",
    "end": "3515680"
  }
]