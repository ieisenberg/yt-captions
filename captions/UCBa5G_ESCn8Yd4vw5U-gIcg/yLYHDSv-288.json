[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "Okay. So great to see everyone back for lecture four of the class.",
    "start": "4790",
    "end": "11940"
  },
  {
    "text": "Um, so, for lec, for today's lecture, um, what I want to do for most of the time is actually",
    "start": "11940",
    "end": "20040"
  },
  {
    "start": "20000",
    "end": "100000"
  },
  {
    "text": "get into the heart of these ideas of having the backpropagation algorithm for neural nets and how we can construct",
    "start": "20040",
    "end": "28289"
  },
  {
    "text": "computation graphs that allow sufficiently to do backpropagation,",
    "start": "28290",
    "end": "33690"
  },
  {
    "text": "neural nets to train the neural nets. So, overall, um, this is sort of what I plan to do it today.",
    "start": "33690",
    "end": "41330"
  },
  {
    "text": "So, at the end of last lecture, I slightly ran out of time and I started mumbling and waving my hands about the,",
    "start": "41330",
    "end": "47870"
  },
  {
    "text": "um, doing the derivatives with respect to the weight gradients. So, I kinda of wanted to do that but again.",
    "start": "47870",
    "end": "54265"
  },
  {
    "text": "So hopefully it actually communicates slightly better. So, we'll do that and talk a bit more about sort of just tips for doing matrix gradients,",
    "start": "54265",
    "end": "64070"
  },
  {
    "text": "um, and a particular issue that comes up with word vectors. And so then the main part of the class,",
    "start": "64070",
    "end": "70010"
  },
  {
    "text": "we'll be talking about the backpropagation algorithm and how it runs over computation graphs.",
    "start": "70010",
    "end": "75770"
  },
  {
    "text": "Um, and then for the last part of the class, um, is I'm not going to hide that, um,",
    "start": "75770",
    "end": "82380"
  },
  {
    "text": "this is sort of just a grab bag of miscellaneous stuff you should know about neural networks and training neural networks.",
    "start": "82380",
    "end": "92005"
  },
  {
    "text": "Um, like, I think, you know we dream of a future of artificial intelligence where our machines are",
    "start": "92005",
    "end": "98720"
  },
  {
    "text": "really intelligent and you can just say to them this is the data and this is my problem, go and train me a model and it might work.",
    "start": "98720",
    "end": "106850"
  },
  {
    "start": "100000",
    "end": "310000"
  },
  {
    "text": "Um, and in some future world, that may be  [NOISE] that comes along. It's something that's certainly being actively",
    "start": "106850",
    "end": "113150"
  },
  {
    "text": "researched at the moment under the topic of Auto ML. I guess the question is whether it turns out that Auto ML was a scalable solution or",
    "start": "113150",
    "end": "122240"
  },
  {
    "text": "the climate change consequences of Auto ML techniques are sufficiently bad that someone actually decides that these much lower power,",
    "start": "122240",
    "end": "131000"
  },
  {
    "text": "um, neural systems might actually be better still for doing some parts of the problem.",
    "start": "131000",
    "end": "137135"
  },
  {
    "text": "But anyway, either way we're not really there yet. And the fact of the matter is,",
    "start": "137135",
    "end": "142235"
  },
  {
    "text": "when you're training neural networks, there's just a whole bunch of stuff you have to know about",
    "start": "142235",
    "end": "147860"
  },
  {
    "text": "initialization and nonlinearities and learning rates and so on. And, you know, when I taught this class",
    "start": "147860",
    "end": "154440"
  },
  {
    "text": "last time I somehow thought that people would pick this up by osmosis.",
    "start": "154440",
    "end": "161410"
  },
  {
    "text": "That if we gave starter, cut code to people and now start",
    "start": "161410",
    "end": "167440"
  },
  {
    "text": "a code we initialized how matrices and we set our learning rates,",
    "start": "167440",
    "end": "172760"
  },
  {
    "text": "that by osmosis people would understand that's what you have to do and do it.",
    "start": "172760",
    "end": "178444"
  },
  {
    "text": "Um, it didn't really sort of teach in class the practical tips and tricks enough,",
    "start": "178445",
    "end": "183859"
  },
  {
    "text": "but it was perfectly obvious that when we got to final project time that at least for quite a few people, osmosis hadn't worked.",
    "start": "183859",
    "end": "192035"
  },
  {
    "text": "Um, so this time, [LAUGHTER] I'm at least wanting to spend a few minutes on that and at least point out some other things that are important.",
    "start": "192035",
    "end": "200510"
  },
  {
    "text": "And, I mean just in general, you know the reality of 2018, deep learning, no,",
    "start": "200510",
    "end": "207330"
  },
  {
    "text": "wait, it's 2019 now, 2019, um, deep learning, is deep learning is still kind of a craft.",
    "start": "207330",
    "end": "214430"
  },
  {
    "text": "There's quite a bit you have to know of techniques of doing things that lead",
    "start": "214430",
    "end": "219620"
  },
  {
    "text": "neural net training to work successfully as opposed to your models failing to work successfully.",
    "start": "219620",
    "end": "226050"
  },
  {
    "text": "Okay. One final announcement and I go in to it. Um, so, we've sort of been doing some further working on Office,",
    "start": "226050",
    "end": "236345"
  },
  {
    "text": "our placement and I guess there are sort of multiple issues which include the opportunities for local ICPD students without Stanford IDs.",
    "start": "236345",
    "end": "244459"
  },
  {
    "text": "We have to, um, get, um, to office hours. So for the Thursday night office hour,",
    "start": "244460",
    "end": "250670"
  },
  {
    "text": "um, that's after this class, if you'd like to go and talk about, um, the second homework, um,",
    "start": "250670",
    "end": "256704"
  },
  {
    "text": "the Thursday night office hour is going to be in Thorton- Thornton 110. Um, now I didn't know where Thornton was.",
    "start": "256705",
    "end": "264259"
  },
  {
    "text": "It made more sense to me when I translated that as that's the old terman annex,",
    "start": "264260",
    "end": "269390"
  },
  {
    "text": "but that's probably just showing my age since probably none of you remember when they used to be a building called Terman.",
    "start": "269390",
    "end": "275620"
  },
  {
    "text": "So that probably doesn't help you either. Um, but you know, if you're heading, right, I don't know which direction we're facing.",
    "start": "275620",
    "end": "282229"
  },
  {
    "text": "If you're heading that way I guess and if you know where the Papua New Guinea Sculpture Garden is, um, the,",
    "start": "282230",
    "end": "289520"
  },
  {
    "text": "the sort of open grassy area before you get to the Papua New Guinea Sculpture Garden,",
    "start": "289520",
    "end": "294650"
  },
  {
    "text": "that's where Terman used to be and the building that still stands in there is Thornton.",
    "start": "294650",
    "end": "299695"
  },
  {
    "text": "Um, Thornton 110 um tonight. I think it starts at 6:30, right? 6:30 to nine.",
    "start": "299695",
    "end": "306210"
  },
  {
    "text": "Okay. Right. So, let me just finish off where we were last time. So remember we had this window of five words and then we're",
    "start": "306210",
    "end": "315195"
  },
  {
    "start": "310000",
    "end": "375000"
  },
  {
    "text": "putting it through a neural net layer of C equals WX plus B, non-linearity of H equals F of X,",
    "start": "315195",
    "end": "322160"
  },
  {
    "text": "and then we're, um, going to just get a score as to whether this has in its center [NOISE]",
    "start": "322160",
    "end": "329150"
  },
  {
    "text": "named entity like Paris which is sort of taking this dot product of a vector times the hidden layer.",
    "start": "329150",
    "end": "335090"
  },
  {
    "text": "So this was our model, and then we are wanting to work out partial derivatives of S with",
    "start": "335090",
    "end": "341120"
  },
  {
    "text": "respect to all of our variables and we did various of the cases, but one we hadn't yet done is the weights,",
    "start": "341120",
    "end": "348110"
  },
  {
    "text": "and the weight through all of this neural net layer here. Okay. So, chain rule, um,",
    "start": "348110",
    "end": "353285"
  },
  {
    "text": "the partial of ds dw is DS times HD, um, dHDZ times DZ, DW.",
    "start": "353285",
    "end": "361695"
  },
  {
    "text": "And well, if you remember last time, we had sort of done some computation of what those first two,",
    "start": "361695",
    "end": "369945"
  },
  {
    "text": "um, partial derivatives were. And we could say that we could just call those delta which is our error signal coming from above.",
    "start": "369945",
    "end": "379145"
  },
  {
    "text": "And that concept of having an error signal coming from above is something I'll get back to in the main part of",
    "start": "379145",
    "end": "384919"
  },
  {
    "text": "the lecture and a sort of a central notion. But the bit we hadn't dealt with is this dz, dw and we started to look at that and I made the argument, um,",
    "start": "384920",
    "end": "395540"
  },
  {
    "text": "based on our shape convention that the shape of that should be the same shape as our W matrix.",
    "start": "395540",
    "end": "402319"
  },
  {
    "text": "So it should be, um, same in times M shape as this W matrix.",
    "start": "402320",
    "end": "407360"
  },
  {
    "text": "So we want to work out the partial of Z by W which is the same as this,",
    "start": "407360",
    "end": "415590"
  },
  {
    "text": "um, [NOISE] dwx plus b, dw. And so we want to work out what that derivative is.",
    "start": "415590",
    "end": "423065"
  },
  {
    "text": "Um, and if that's not obvious, one way to think about it is to go back to this elements of the matrix",
    "start": "423065",
    "end": "429740"
  },
  {
    "text": "and actually first off work it out element-wise and think out what it should be,",
    "start": "429740",
    "end": "434830"
  },
  {
    "text": "and then once you've thought out what it should be, um, to rewrite it back in matrix form to give the compact answer.",
    "start": "434830",
    "end": "442090"
  },
  {
    "text": "So what we have is we have the inputs here and a biased term and we're going to do the matrix multiply it this vector to produce these.",
    "start": "442090",
    "end": "451730"
  },
  {
    "text": "And if you think about what's happening there, so we've got this matrix of weights and for a particular weight,",
    "start": "451730",
    "end": "459199"
  },
  {
    "text": "a weight is first index is going to correspond to a position in",
    "start": "459200",
    "end": "464885"
  },
  {
    "text": "the hidden layer and its second index is going to correspond to a position in the input vector.",
    "start": "464885",
    "end": "473240"
  },
  {
    "text": "And one weight in the matrix ends up being part of what's used to compute one element of the hidden layer.",
    "start": "473240",
    "end": "481100"
  },
  {
    "text": "So, the one element of the hidden layer you're taking, um, a row of the matrix and you're multiplying it by",
    "start": "481100",
    "end": "488270"
  },
  {
    "text": "the components of this vector so they sum together when the bias is added on but one element of the matrix is sort of only being",
    "start": "488270",
    "end": "495050"
  },
  {
    "text": "used in the computation between one element of the, um, important one element of the hidden vector.",
    "start": "495050",
    "end": "502150"
  },
  {
    "text": "Okay. So, well, that means, um, if we're thinking about what's the partial derivative with respect to WIJ, well,",
    "start": "502150",
    "end": "511040"
  },
  {
    "text": "it's only contributing to ZI and it's only,",
    "start": "511040",
    "end": "518534"
  },
  {
    "text": "it's only doing anything with XJ. So, that we end up with,",
    "start": "518535",
    "end": "524850"
  },
  {
    "text": "we're getting the partial with respect to WIJ, we can work that out with respect to,",
    "start": "524850",
    "end": "530570"
  },
  {
    "text": "just to respect to ZI. And when we're going to look at this multiplication here,",
    "start": "530570",
    "end": "537080"
  },
  {
    "text": "what we're ending up is this sort of sum of terms WIK times Xk where there's sort of weights in that row",
    "start": "537080",
    "end": "544490"
  },
  {
    "text": "of the matrix going across the positions of the vector. So the only position in which WIJ is used is multiplying, um, by XJ.",
    "start": "544490",
    "end": "557105"
  },
  {
    "text": "And at that point, what we have in terms of sort of, in our basic one variable doing a differentiation,",
    "start": "557105",
    "end": "564440"
  },
  {
    "text": "this is just like we have 3x, um, and we say what's the derivative of 3x?",
    "start": "564440",
    "end": "570259"
  },
  {
    "text": "Actually X is confusing, so I shouldn't say that. Is like we have three W and what's the derivative of three W with respect to W?",
    "start": "570260",
    "end": "578450"
  },
  {
    "text": "It's three, right? So, that we've have a term here which is what would have been W,",
    "start": "578450",
    "end": "584660"
  },
  {
    "text": "will be WIJ times XJ, and its derivative with respect to WIJ is just XJ.",
    "start": "584660",
    "end": "591725"
  },
  {
    "text": "Does that makes sense? Everyone believe it? [NOISE] Fingers crossed.",
    "start": "591725",
    "end": "597270"
  },
  {
    "text": "Okay. Um, so, so for one element of this matrix, we're just getting out XJ.",
    "start": "597270",
    "end": "604290"
  },
  {
    "text": "And at that point, um, we say, um, well of course we want to know what the Jacobian is for the full matrix W. Well,",
    "start": "604290",
    "end": "614060"
  },
  {
    "text": "if you start thinking about it, this argument applies to every cell. So, that for every,",
    "start": "614060",
    "end": "620209"
  },
  {
    "text": "um, cell of, um, the Jacobian for W, um, it's going to be XJ.",
    "start": "620210",
    "end": "626900"
  },
  {
    "text": "So, that means, um, we're just going to be able to make use of that in calculating our Jacobian.",
    "start": "626900",
    "end": "634850"
  },
  {
    "text": "So, the derivative for a single WIJ is delta IXJ and that's true for all cells.",
    "start": "634850",
    "end": "642649"
  },
  {
    "text": "So we wanted to have a matrix for our Jacobian which has delta I,",
    "start": "642650",
    "end": "648430"
  },
  {
    "text": "um, XJ in every cell of it. And the way we can create that is by using an outer products.",
    "start": "648430",
    "end": "655850"
  },
  {
    "text": "So, if we have a row vector of the deltas, the error signals from above and a column,",
    "start": "655850",
    "end": "663325"
  },
  {
    "text": "right, I said that wrong, sorry. If we have a column of the delta error signals",
    "start": "663325",
    "end": "671420"
  },
  {
    "text": "from above and we have a row of X transfers vectors,",
    "start": "671420",
    "end": "677300"
  },
  {
    "text": "um, when we multiply those together we get the outer product and we get delta IXJ in each cell and that is our Jacobian answer,",
    "start": "677300",
    "end": "686839"
  },
  {
    "text": "um, for working out, um, the delta S delta W that we started off with at the beginning.",
    "start": "686840",
    "end": "694070"
  },
  {
    "text": "Okay. And this, um, and we get this form where it's a multiplication of",
    "start": "694070",
    "end": "700100"
  },
  {
    "text": "an error signal from above and our computed local gradient signal. And that's the pattern that we're going to see over and over",
    "start": "700100",
    "end": "707720"
  },
  {
    "text": "again and that will exploit and our computation graphs. Okay, all good?",
    "start": "707720",
    "end": "713315"
  },
  {
    "text": "Okay. Um, so, here's just, um, here's homework two.",
    "start": "713315",
    "end": "721070"
  },
  {
    "text": "You're meant to do some of this stuff. Um, here are just over a couple of collected tips,",
    "start": "721070",
    "end": "726845"
  },
  {
    "text": "um, which I hope will help. I mean keeping here track of your variables and",
    "start": "726845",
    "end": "733480"
  },
  {
    "text": "their dimensionality is really useful because we just can work out what the dimensionality of things should be.",
    "start": "733480",
    "end": "739255"
  },
  {
    "text": "You're often kind of halfway there. I mean basically what you're doing is sort of applying the chain rule over and over again.",
    "start": "739255",
    "end": "747100"
  },
  {
    "text": "It always looks like this. Um, but doing it in this sort of matrix calculus sense of the chain rule.",
    "start": "747100",
    "end": "753920"
  },
  {
    "text": "Um, in the homework you have to do a softmax, which we haven't done in class.",
    "start": "753920",
    "end": "759410"
  },
  {
    "text": "Um, something that I think you'll find useful, if you want to break apart the softmax is to consider two cases.",
    "start": "759410",
    "end": "766985"
  },
  {
    "text": "One, the case is to when you're working it out for the correct class. And then, the other case is for all the other incorrect classes.",
    "start": "766985",
    "end": "776795"
  },
  {
    "text": "Um, yeah. Um, in the the little derivation, I did before, I said well,",
    "start": "776795",
    "end": "783035"
  },
  {
    "text": "let's work out an element-wise partial derivative because that should give me some sense of what's going on,",
    "start": "783035",
    "end": "788660"
  },
  {
    "text": "what the answer is. I think that can be a really good thing to do if you're getting confused by matrix calculus.",
    "start": "788660",
    "end": "794779"
  },
  {
    "text": "And I sort of, um, slightly skipped past another slide.",
    "start": "794780",
    "end": "800660"
  },
  {
    "text": "Last time that was talking about the shape convention that I talked about it for a moment that for the homeworks you can work out your answer however you want,",
    "start": "800660",
    "end": "811550"
  },
  {
    "text": "you can work it out in terms of; you know numerator ordered Jacobians, if that seems best to you.",
    "start": "811550",
    "end": "817280"
  },
  {
    "text": "But we'd like you to give the final answer to your assignment questions following the shape convention.",
    "start": "817280",
    "end": "823730"
  },
  {
    "text": "So, that the derivative should be shaped in a vector matrix in the same way as the variable,",
    "start": "823730",
    "end": "830825"
  },
  {
    "text": "with respect to which you're working out your derivatives. Okay. Um, the last little bit for finishing up this example from last time,",
    "start": "830825",
    "end": "840320"
  },
  {
    "text": "I want to say a little bit about, is what happens with words. And one answer is nothing different.",
    "start": "840320",
    "end": "848300"
  },
  {
    "text": "But another answer is they are a little bit of a special case here because, you know, really we have a matrix of word vectors, right?",
    "start": "848300",
    "end": "856610"
  },
  {
    "text": "We have a vector for each word. And so then you can think of that as sort of this matrix of word vectors,",
    "start": "856610",
    "end": "863540"
  },
  {
    "text": "which row has a different word. But we're not actually kind of connecting up",
    "start": "863540",
    "end": "868760"
  },
  {
    "text": "that matrix directly to our classifier system. Instead of that, what we're connect connecting up to the classifier system is",
    "start": "868760",
    "end": "877190"
  },
  {
    "text": "this window and the window will have it in at five words. Most commonly they're different words.",
    "start": "877190",
    "end": "883490"
  },
  {
    "text": "But you know occasionally the same word might appear, um, in two positions in that window.",
    "start": "883490",
    "end": "889160"
  },
  {
    "text": "And so, we can nevertheless do exactly the same thing and continue our gradients down and say okay,",
    "start": "889160",
    "end": "897349"
  },
  {
    "text": "um, let's work out, um, the gradients of this word window vector.",
    "start": "897349",
    "end": "903335"
  },
  {
    "text": "And if, um, these are of dimension D we'll have this sort of 5-D, um, vector.",
    "start": "903335",
    "end": "909755"
  },
  {
    "text": "But, you know then what do we do about it, and the answer of what we do about it.",
    "start": "909755",
    "end": "915140"
  },
  {
    "text": "Is we can just sort of split this window vector into five pieces and say aha,",
    "start": "915140",
    "end": "921980"
  },
  {
    "text": "we have five updates to word vectors. We're just going to go off and apply them to the word Vector Matrix.",
    "start": "921980",
    "end": "930335"
  },
  {
    "text": "Um, and you know if we if the same word occurs twice, um, in that window we literally apply both of the updates.",
    "start": "930335",
    "end": "938990"
  },
  {
    "text": "So, it gets updated twice or maybe actually you want to sum them first and then do the update once but yeah,",
    "start": "938990",
    "end": "944810"
  },
  {
    "text": "that's a technical issue. Um, so what that actually means is that we're extremely sparsely",
    "start": "944810",
    "end": "953345"
  },
  {
    "text": "updating the word Vector Matrix because most of the word Vector Matrix will be unchanged and just a few rows of that,",
    "start": "953345",
    "end": "961654"
  },
  {
    "text": "um, will be being updated. And if- um, soon we're going to be here doing stuff with PyTorch",
    "start": "961655",
    "end": "967880"
  },
  {
    "text": "Um, and if you poke around Pytorch it even has some special stuff. Um, look for things like Sparse SGD for meaning",
    "start": "967880",
    "end": "975440"
  },
  {
    "text": "that you're sort of doing a very sparse updating like that. Um, but there's one other sort of interesting thing that you should know about.",
    "start": "975440",
    "end": "984589"
  },
  {
    "text": "For a lot of um, things that you do is just what actually happens if we push",
    "start": "984590",
    "end": "989600"
  },
  {
    "text": "down these gradients into our word vectors. Well, the idea is no,",
    "start": "989600",
    "end": "995645"
  },
  {
    "text": "if we do that would be just like all other neural net learning, that we will sort of in principle say move the word vectors around in such a way",
    "start": "995645",
    "end": "1006654"
  },
  {
    "text": "as they're more useful in helping determine named entity classification in this case because that was our motivating example.",
    "start": "1006655",
    "end": "1014665"
  },
  {
    "text": "Um, so you know it might for example learn that the word in is a very good indicator of a named entity fall or sorry the place name following.",
    "start": "1014665",
    "end": "1024970"
  },
  {
    "text": "So, after n you often get London, Paris et cetera. Right, so it's sort of got a special behavior that",
    "start": "1024970",
    "end": "1031029"
  },
  {
    "text": "other prepositions don't as being a good location indicator. And so, it could sort of um,",
    "start": "1031030",
    "end": "1036040"
  },
  {
    "text": "move it's location around and say here are words that are good location indicators and therefore help our classifier work even better.",
    "start": "1036040",
    "end": "1046134"
  },
  {
    "text": "So, in principle that's good and it's a good thing to do, to update word vectors to help you perform better on",
    "start": "1046135",
    "end": "1054100"
  },
  {
    "text": "a supervised task such as this Named Entity Recognition classification.",
    "start": "1054100",
    "end": "1059710"
  },
  {
    "text": "But, there's a catch which is that it doesn't always work actually.",
    "start": "1059710",
    "end": "1065049"
  },
  {
    "text": "And so, why doesn't it always work? Well, suppose that we're training a classifier.",
    "start": "1065050",
    "end": "1070750"
  },
  {
    "text": "Um, you know it could be the one I just did or a softmax or logistic regression.",
    "start": "1070750",
    "end": "1076360"
  },
  {
    "text": "And we wanting to classify um, movie reviews sentiment for positive or negative.",
    "start": "1076360",
    "end": "1082855"
  },
  {
    "text": "Well, you know if we have trained our word vectors, we've got some word vector space and maybe in the word vector space, um, TV,",
    "start": "1082855",
    "end": "1093309"
  },
  {
    "text": "telly and television are all very close together because they mean basically the same thing.",
    "start": "1093310",
    "end": "1099520"
  },
  {
    "text": "So, that's great, our word vectors are good. But, well suppose it was the case,",
    "start": "1099520",
    "end": "1105250"
  },
  {
    "text": "that in our training data for our classifier. So, this is our training data for movie sentiment review.",
    "start": "1105250",
    "end": "1112630"
  },
  {
    "text": "We had the word TV and telly but we didn't have the word television.",
    "start": "1112630",
    "end": "1118465"
  },
  {
    "text": "Well, then what's going to happen, is well while we try and train our sentiment classifier,",
    "start": "1118465",
    "end": "1125350"
  },
  {
    "text": "if we push gradient back down into the word vectors what's likely to happen",
    "start": "1125350",
    "end": "1131200"
  },
  {
    "text": "is that it will move around the word vectors of the words we saw in the training data.",
    "start": "1131200",
    "end": "1138039"
  },
  {
    "text": "But, necessarily television's not moving, right? Because we're only pushing gradient down to words that are in our training data.",
    "start": "1138040",
    "end": "1145090"
  },
  {
    "text": "So, this word goes nowhere, so it just stays where it was all along. So, if the result of our training is words get moved around.",
    "start": "1145090",
    "end": "1153760"
  },
  {
    "text": "So, here a good words for indicating negative sentiment, um, will actually if at test time,",
    "start": "1153760",
    "end": "1160839"
  },
  {
    "text": "when we're running our model, if we evaluate on a sentence with television in it, it's actually going to give the wrong answer.",
    "start": "1160839",
    "end": "1167620"
  },
  {
    "text": "Whereas if we haven't changed the word vectors at all and had just left them where our word embedding learning system put them.",
    "start": "1167620",
    "end": "1177610"
  },
  {
    "text": "Then it would have said television, that's a word that means about the same as TV or telly.",
    "start": "1177610",
    "end": "1182620"
  },
  {
    "text": "I should treat it the same and my sentiment classifier and it would actually do a better job.",
    "start": "1182620",
    "end": "1187840"
  },
  {
    "text": "So, it's sort of two-sided whether you gain by training word vectors.",
    "start": "1187840",
    "end": "1194740"
  },
  {
    "text": "And so, this is a summary um, that says; that it's two sided and practically what you should do.",
    "start": "1194740",
    "end": "1201715"
  },
  {
    "text": "So, the first choice is G is a good idea to use pre-trained word vectors like",
    "start": "1201715",
    "end": "1209080"
  },
  {
    "text": "the word2vec vectors that you used in assignment one or using the training methods that you're doing right now for homework two.",
    "start": "1209080",
    "end": "1217110"
  },
  {
    "text": "And the answer that is almost always yes. And the reason for that is this word vector training methods are",
    "start": "1217110",
    "end": "1224970"
  },
  {
    "text": "extremely easy to run on billions of words of texts. So, we you know train these models like [inaudible] on billions or tens of billions of words.",
    "start": "1224970",
    "end": "1235750"
  },
  {
    "text": "And it's easy to do that for two reasons. Firstly, because the training algorithms are very simple, right?",
    "start": "1235750",
    "end": "1241960"
  },
  {
    "text": "That um, the word2vec training algorithms skip grams very simple algorithm. Secondly; because we don't need any expensive resources,",
    "start": "1241960",
    "end": "1250900"
  },
  {
    "text": "all or we need as a big pile of text documents and we can run it on them. So, really easy to run it on,",
    "start": "1250900",
    "end": "1256870"
  },
  {
    "text": "you know five or 50 billion words. Whereas, you know, we can't do that for most of the classifiers that we",
    "start": "1256870",
    "end": "1263350"
  },
  {
    "text": "want to build because if it's something I sentiment classifier or a named entity recognizer, we need labeled training data to train",
    "start": "1263350",
    "end": "1270280"
  },
  {
    "text": "our classifier and then we ask someone how many words have labeled training data,",
    "start": "1270280",
    "end": "1275605"
  },
  {
    "text": "do you have for named entity recognition and they give this back a number like 300,000 words or one million words, right.",
    "start": "1275605",
    "end": "1282130"
  },
  {
    "text": "It's orders a magnitude smaller. Okay. Um. So, therefore,",
    "start": "1282130",
    "end": "1287554"
  },
  {
    "text": "we can gain using pre-trained word vectors, because they know about all the words that aren't",
    "start": "1287554",
    "end": "1292794"
  },
  {
    "text": "now supervised, classifies training data. And they also know much more about the words that actually",
    "start": "1292795",
    "end": "1298179"
  },
  {
    "text": "are in the training data, but only rarely. So, the exception to that is, if you have hundreds of millions of words of data,",
    "start": "1298180",
    "end": "1305490"
  },
  {
    "text": "then you can start off with random word vectors and go from there. And so, a case where this is actually commonly done,",
    "start": "1305490",
    "end": "1312775"
  },
  {
    "text": "is for machine translation, which we do later in the class. It's relatively easy for",
    "start": "1312775",
    "end": "1318415"
  },
  {
    "text": "large languages to get hundreds of millions of words of translated text. If you wanted to build something,",
    "start": "1318415",
    "end": "1324635"
  },
  {
    "text": "like a German- English or Chinese-English machine translation system. Not hard to get 150 million words of translated texts.",
    "start": "1324635",
    "end": "1334320"
  },
  {
    "text": "And so, that's sort of sufficiently much data, that people commonly just start with word vectors, um,",
    "start": "1334320",
    "end": "1341015"
  },
  {
    "text": "being randomly initialized and start training, um, their translation system.",
    "start": "1341015",
    "end": "1347000"
  },
  {
    "text": "Okay. So then the second question is, okay. I'm using pre-trained word vectors.",
    "start": "1347000",
    "end": "1352150"
  },
  {
    "text": "Um, when I train my supervised classifier, should I push gradients down into the word vectors and up, and update them?",
    "start": "1352150",
    "end": "1360285"
  },
  {
    "text": "Which is often referred to as fine tuning the word vectors, um, or should I not,",
    "start": "1360285",
    "end": "1365899"
  },
  {
    "text": "should I just sort of throw away those gradients and not push them down into the word vectors?",
    "start": "1365900",
    "end": "1371090"
  },
  {
    "text": "And you know, the answer to that is it depends, and it just depends on the size. So, if you only have a small training data set, um, typically,",
    "start": "1371090",
    "end": "1381345"
  },
  {
    "text": "it's best to just treat the pre-trained word vectors as fixed, um, and not do any updating of them at all.",
    "start": "1381345",
    "end": "1388804"
  },
  {
    "text": "If you have a large data set, then you can normally gain by doing fine tuning of the word vectors.",
    "start": "1388805",
    "end": "1396620"
  },
  {
    "text": "And of course, the answer here, is what counts as large. Um, you know, if certainly,",
    "start": "1396620",
    "end": "1401850"
  },
  {
    "text": "if you're down in the regime of 100 thousand words, a couple of hundred thousand words, you're small.",
    "start": "1401850",
    "end": "1407105"
  },
  {
    "text": "If you're starting to be over a million words, then maybe you're large. But you know, on practice, people do it both ways and see which number is higher,",
    "start": "1407105",
    "end": "1414265"
  },
  {
    "text": "and that's what they stick with. Um. Yes. Um, then, the sort of,",
    "start": "1414265",
    "end": "1419955"
  },
  {
    "text": "there's the sort of point here that is just worth underlying is \" Yes\", so on principle, we can back-propagate this gradient to every variable in our model.",
    "start": "1419955",
    "end": "1431980"
  },
  {
    "text": "Um, it's actually a theorem that we can arbitrarily decide to throw any subset of those gradients away,",
    "start": "1431980",
    "end": "1442360"
  },
  {
    "text": "and we are still improving the log-likelihood of our model, all right?",
    "start": "1442360",
    "end": "1447960"
  },
  {
    "text": "It kind of can't be inconsistent. You can just sort of pick some subset and say only train those 37 and throw away all the rest.",
    "start": "1447960",
    "end": "1454980"
  },
  {
    "text": "And the algorithm will still improve, um, the log-likelihood of the model. Perhaps not by as much as if you trained the rest of the variables,",
    "start": "1454980",
    "end": "1462355"
  },
  {
    "text": "as well, um, but yes, it can't actually do any harm not to train anything. Um, that's one of the reasons why often people don't notice bugs in their code, as well.",
    "start": "1462355",
    "end": "1472315"
  },
  {
    "text": "It is because if your code is kind of broken and only half of the variables are being updated,",
    "start": "1472315",
    "end": "1477550"
  },
  {
    "text": "it will still seem to be training something and improving. Um. It's just not doing as well as it could be doing,",
    "start": "1477550",
    "end": "1483415"
  },
  {
    "text": "if you've coded correctly. Okay. Um, so, at this point, um,",
    "start": "1483415",
    "end": "1489184"
  },
  {
    "start": "1485000",
    "end": "1687000"
  },
  {
    "text": "that's sort of, um, almost shown you back propagation, right? So, back-propagation is really taking derivatives with a generalized chain rule,",
    "start": "1489185",
    "end": "1499030"
  },
  {
    "text": "with the one further trick which we sort of represented with that delta, which is G. You want to be, um,",
    "start": "1499030",
    "end": "1506860"
  },
  {
    "text": "clever in doing this, so, you minimize computation by reusing shared stuff.",
    "start": "1506860",
    "end": "1512995"
  },
  {
    "text": "Um, but now what I want to move on is to sort of look at how we can do that much more systematically, which is this idea.",
    "start": "1512995",
    "end": "1519890"
  },
  {
    "text": "We have a computation graph and we're going to run a back-propagation algorithm through the computation graph.",
    "start": "1519890",
    "end": "1526380"
  },
  {
    "text": "So, this is kind of like an abstracts syntax tree,",
    "start": "1527170",
    "end": "1533730"
  },
  {
    "text": "expression tree that you might see in a compiler's class, or something like that, right? So, when we have an arithmetic expression of the kind that we're going to compute,",
    "start": "1533730",
    "end": "1544150"
  },
  {
    "text": "we can make this tipped over on its side tree representation. So, we've got the X and W variables,",
    "start": "1544150",
    "end": "1550990"
  },
  {
    "text": "we're going to multiply them. There's the B variable, we're going to add it to the previous partial result.",
    "start": "1550990",
    "end": "1556470"
  },
  {
    "text": "We're going to stick it through our non-linearity F and then we're going to multiply it by U. And that was the computation,",
    "start": "1556470",
    "end": "1563004"
  },
  {
    "text": "that we're doing in our neural network. So, um the source nodes or inputs,",
    "start": "1563005",
    "end": "1568789"
  },
  {
    "text": "the interior nodes of this tree are operations. And then we've got these edges that pass along the results of our computation.",
    "start": "1568790",
    "end": "1577255"
  },
  {
    "text": "And so, this is the computation graph for precisely the example I've been doing for the last lecture [NOISE].",
    "start": "1577255",
    "end": "1585330"
  },
  {
    "text": "Okay, so there are two things that we want to be able to do. The first one is, we want to be able to start with these variables and do this computation,",
    "start": "1585330",
    "end": "1594315"
  },
  {
    "text": "and calculate what S is. That's the part that's dead simple, that's referred to as forward propagation.",
    "start": "1594315",
    "end": "1601815"
  },
  {
    "text": "So, forward propagation is just expression evaluation, as you do in any any programming in language interpreter.",
    "start": "1601815",
    "end": "1608870"
  },
  {
    "text": "Um, that's not hard at all. Um, but the difference here is, \"Hey,",
    "start": "1608870",
    "end": "1614390"
  },
  {
    "text": "we want to do a learning algorithm\" so we're going to do the opposite of that, as well.",
    "start": "1614390",
    "end": "1619705"
  },
  {
    "text": "What we want to be able to do is also backward propagation, or back-propagation or just back-prop, it's commonly called,",
    "start": "1619705",
    "end": "1627804"
  },
  {
    "text": "which is we want to be able to go, um, from the final part. The final part here.",
    "start": "1627805",
    "end": "1634190"
  },
  {
    "text": "And then at each step, we want to be calculating these partial derivatives and passing them back through the graph.",
    "start": "1634190",
    "end": "1642679"
  },
  {
    "text": "And so, this was sort of the notion before that we had an error signal, right? So, we're starting from up here,",
    "start": "1642680",
    "end": "1648860"
  },
  {
    "text": "we've calculated a partial of S by Z, which is this with respect to that.",
    "start": "1648860",
    "end": "1654919"
  },
  {
    "text": "And so, that's sort of our calculated error signal, up to here, and then we want to pass that further back, to start, um,",
    "start": "1654920",
    "end": "1661940"
  },
  {
    "text": "computing, um, um, gradients further back. Right? And we started off, um, right here,",
    "start": "1661940",
    "end": "1669570"
  },
  {
    "text": "with the partial of S by S. What's the partial of S by S going to be? One. Okay, yes.",
    "start": "1669570",
    "end": "1677040"
  },
  {
    "text": "So, the rate at which S changes is the rate at which S changes. So, we just start off with one,",
    "start": "1677040",
    "end": "1682130"
  },
  {
    "text": "and then we want to work out how this gradient changes as we go along.",
    "start": "1682130",
    "end": "1687565"
  },
  {
    "start": "1687000",
    "end": "1883000"
  },
  {
    "text": "Um, so what we're doing here is when we're working out things for one node,",
    "start": "1687565",
    "end": "1694514"
  },
  {
    "text": "that a node is going to have passed in towards it upstream gradient, which is its error signal.",
    "start": "1694515",
    "end": "1700465"
  },
  {
    "text": "So, that's the partial of our final, f- final result,",
    "start": "1700465",
    "end": "1706044"
  },
  {
    "text": "which was our loss, um, by um, the va- variable was the output of these computation nodes.",
    "start": "1706045",
    "end": "1712970"
  },
  {
    "text": "So, that's the partial of S I H, here. And then, we did some operation here.",
    "start": "1712970",
    "end": "1719340"
  },
  {
    "text": "Here's the non-linearity, but it might be something else. And so what we want to then work out is a downstream gradient,",
    "start": "1719340",
    "end": "1727100"
  },
  {
    "text": "which is the partial of S by Z, which was the input to this function. And well then the question is,",
    "start": "1727100",
    "end": "1733320"
  },
  {
    "text": "how do we do that? And the answer to that is, we use the chain rule, of course, right?",
    "start": "1733320",
    "end": "1739045"
  },
  {
    "text": "So, at, we have a concept of a local gradients. So, here's H as the output,",
    "start": "1739045",
    "end": "1746425"
  },
  {
    "text": "um, Z is the input. So, this function here, this is our non-linearity, right?",
    "start": "1746425",
    "end": "1751980"
  },
  {
    "text": "So, this is whatever we're using as our non-linearity, like a logistic or T and H. We calculate H in terms of Z,",
    "start": "1751980",
    "end": "1759095"
  },
  {
    "text": "and we can work out the partial of H by Z. So, that's our local gradient. And so then, if we have both the upstream gradient and the local gradient.",
    "start": "1759095",
    "end": "1768370"
  },
  {
    "text": "We can then work out the downstream gradient because we know the partial of S by Z is going to be DSDH times, um, DHDZ.",
    "start": "1768370",
    "end": "1778880"
  },
  {
    "text": "And so, then we'll be able to pass down the downstream gradient to the next node.",
    "start": "1778880",
    "end": "1784995"
  },
  {
    "text": "Okay. So our basic rule, which is just the chain rule written in different terms",
    "start": "1784995",
    "end": "1792320"
  },
  {
    "text": "is downstream gradient equals upstream gradient times local gradient.",
    "start": "1792320",
    "end": "1798009"
  },
  {
    "text": "Um, easy as that,um, okay. So, this was um,",
    "start": "1798010",
    "end": "1803434"
  },
  {
    "text": "the very simplest case where we have a node with one input and one output.",
    "start": "1803435",
    "end": "1809510"
  },
  {
    "text": "So, that's a function um, like our logistic function. But, we also want to have things work out for general computation graphs.",
    "start": "1809510",
    "end": "1816780"
  },
  {
    "text": "So, how are we going to do that? Well, the next case is, um, what about if we have multiple inputs?",
    "start": "1816780",
    "end": "1824250"
  },
  {
    "text": "So, if we're calculating something like Z equals W times X.",
    "start": "1824250",
    "end": "1829760"
  },
  {
    "text": "Um, where actually yes Z and X are themselves vectors and W um,",
    "start": "1829760",
    "end": "1836965"
  },
  {
    "text": "is a matrix, but we're treating X as an input and W as an input,",
    "start": "1836965",
    "end": "1842304"
  },
  {
    "text": "and Z as our output, right? We kind of group vectors and matrices together.",
    "start": "1842305",
    "end": "1847405"
  },
  {
    "text": "Well, if you have multiple inputs, you then end up with multiple local gradients.",
    "start": "1847405",
    "end": "1854030"
  },
  {
    "text": "So, you can work out um, the partial of Z with respect to X, or the partial of Z with respect to W. And so,",
    "start": "1854030",
    "end": "1861645"
  },
  {
    "text": "you essentially you take the upstream gradient, you multiply it by each of the local gradients,",
    "start": "1861645",
    "end": "1869155"
  },
  {
    "text": "and you pass it down the respective path, and we calculate these different downstream gradients to pass along.",
    "start": "1869155",
    "end": "1877529"
  },
  {
    "text": "Is that making sense? Yeah. Okay. How chug.",
    "start": "1877530",
    "end": "1885930"
  },
  {
    "start": "1883000",
    "end": "2312000"
  },
  {
    "text": "Okay. So, let's sort of look in an example of this and then we'll see one other case.",
    "start": "1885930",
    "end": "1891930"
  },
  {
    "text": "So here's the little baby example. This isn't kind of really looking like a neural net,",
    "start": "1891930",
    "end": "1897150"
  },
  {
    "text": "but we've got three inputs x, y, and z. And x and y get added together and y and z you get maxed.",
    "start": "1897150",
    "end": "1905895"
  },
  {
    "text": "And then we take the results of those two operations and we multiply them together. So overall what we're calculating is x plus y times the max of y plus z.",
    "start": "1905895",
    "end": "1917340"
  },
  {
    "text": "But, you know, we have here a general technique and we can apply it in any cases.",
    "start": "1917340",
    "end": "1925350"
  },
  {
    "text": "Okay, so if we wanted to have this graph and we want to run it forward, well, we need to know the values of x, y, and z.",
    "start": "1925350",
    "end": "1933270"
  },
  {
    "text": "So, for my example x equals one y equals two z equals zero.",
    "start": "1933270",
    "end": "1939180"
  },
  {
    "text": "Um, so, we take the values of those variables and push them onto the calculations for the forward arrows.",
    "start": "1939180",
    "end": "1948600"
  },
  {
    "text": "And then well the first thing we do is add and the result of that is three. And so we can put that onto the arrow.",
    "start": "1948600",
    "end": "1954300"
  },
  {
    "text": "That's the output of add. Max it's two as the output of the value of add times is six.",
    "start": "1954300",
    "end": "1959730"
  },
  {
    "text": "And so the forward pass we have evaluated the expression. Its value is six. That wasn't hard. Okay. So then the next step is we",
    "start": "1959730",
    "end": "1968700"
  },
  {
    "text": "then want to run back-propagation to work out gradients.",
    "start": "1968700",
    "end": "1974100"
  },
  {
    "text": "Um, and so we sort of want to know how to sort of,",
    "start": "1974100",
    "end": "1980205"
  },
  {
    "text": "um work out these local gradients. So a is our right a is the result of sum.",
    "start": "1980205",
    "end": "1990570"
  },
  {
    "text": "So here's a as the result of sum. So a equals x plus y. So if you're taking da dx that's just one and d a d y is also one that makes sense.",
    "start": "1990570",
    "end": "2003560"
  },
  {
    "text": "Um, the max is slightly trickier because where there's some slopes and gradient for the max depends on which one's bigger.",
    "start": "2003560",
    "end": "2013610"
  },
  {
    "text": "So, if y is bigger than z d- delta, the partial of b by z,",
    "start": "2013610",
    "end": "2020299"
  },
  {
    "text": "plus partial b by y is one otherwise it's 0 and conversely for the partial of b by z.",
    "start": "2020299",
    "end": "2029029"
  },
  {
    "text": "So that one's a little bit dependent. And then we do the multiplication, um,",
    "start": "2029030",
    "end": "2036409"
  },
  {
    "text": "case at the end, um, and work out its partials with respect to a and b.",
    "start": "2036410",
    "end": "2044495"
  },
  {
    "text": "And, um, since that's a and b which has the values two and three.",
    "start": "2044495",
    "end": "2049520"
  },
  {
    "text": "If you're taking the partial of f by a it equals b which is two and vice versa.",
    "start": "2049520",
    "end": "2054725"
  },
  {
    "text": "Okay. So that means we can work out the local gradients at each node. And so then we want to use those to",
    "start": "2054725",
    "end": "2062690"
  },
  {
    "text": "calculate our gradients backwards and the back-propagation paths. So we start at the top.",
    "start": "2062690",
    "end": "2068165"
  },
  {
    "text": "The partial of f with respect to F is one. Because if you move if you know by a tenth then you've moved the f by a tenth.",
    "start": "2068165",
    "end": "2077375"
  },
  {
    "text": "So that's a cancels out as one. Okay. So then we want to pass backwards.",
    "start": "2077375",
    "end": "2082460"
  },
  {
    "text": "So, the first thing that we have is this sort of multiply node. And so we worked- we know its local gradients that partial of f by a is two,",
    "start": "2082460",
    "end": "2093095"
  },
  {
    "text": "and the partial of f by b is three. And so we get those values.",
    "start": "2093095",
    "end": "2099349"
  },
  {
    "text": "So formally we're taking the local gradients multiplying them by the upstream gradients and getting our three and two.",
    "start": "2099350",
    "end": "2107795"
  },
  {
    "text": "And notice the fact that so effectively what happens is the values on the two arcs swaps.",
    "start": "2107795",
    "end": "2113045"
  },
  {
    "text": "Um, and then we sort of continue back. Okay. There's a max node. So our upstream gradient is now three and then we want to multiply by the local gradient.",
    "start": "2113045",
    "end": "2123650"
  },
  {
    "text": "And since the max of these two as two has a slope of one on this side.",
    "start": "2123650",
    "end": "2129920"
  },
  {
    "text": "So you get three, there's no gradient on this side and we get zero. And then we do the similar calculation on",
    "start": "2129920",
    "end": "2137720"
  },
  {
    "text": "the other side where we have local gradients of one. And so both of them come out of two And then the one other thing to do is we notice,",
    "start": "2137720",
    "end": "2147744"
  },
  {
    "text": "well, wait a minute. There are two arcs that started from the y both of which we've backed complicated some gradient on.",
    "start": "2147744",
    "end": "2156290"
  },
  {
    "text": "And so what do we do about that. Um, what we do about that is we sum.",
    "start": "2156290",
    "end": "2162125"
  },
  {
    "text": "So, the partial of f by x is to the partial of f by z is 0 that the",
    "start": "2162125",
    "end": "2167740"
  },
  {
    "text": "partial of f by y is the sum of the two and five, right?",
    "start": "2167740",
    "end": "2173450"
  },
  {
    "text": "And so this isn't complete voodoo. This is something that should make sense in terms of what gradients are, right?",
    "start": "2173450",
    "end": "2181730"
  },
  {
    "text": "So, that what we're saying, is what we're calculating, is if you wiggle x a little bit",
    "start": "2181730",
    "end": "2187860"
  },
  {
    "text": "how big an effect does that have on the outcome of the whole thing? And so, you know, we should be able to work this out.",
    "start": "2187860",
    "end": "2194650"
  },
  {
    "text": "So, our x started offers one but let's suppose we wiggle it up a bit",
    "start": "2194650",
    "end": "2200184"
  },
  {
    "text": "and make it 1.1 well according to this output should change by about 0.2,",
    "start": "2200185",
    "end": "2207685"
  },
  {
    "text": "it should be magnified by two. And we should be able to work that out, right? So it's then 1.1 plus two,",
    "start": "2207685",
    "end": "2215510"
  },
  {
    "text": "so that's then 3.1. And then we've got the two here that multiplies by it and it's 6.2.",
    "start": "2215510",
    "end": "2223610"
  },
  {
    "text": "And lo and behold it went up by 0.2, right? So that seems correct. And if we try and do the same for,",
    "start": "2223610",
    "end": "2229940"
  },
  {
    "text": "well, let's do the z. It's easy. So if we wiggle the z which had a value of zero by 0.1.",
    "start": "2229940",
    "end": "2236510"
  },
  {
    "text": "This is 0.1. When we max if this is still two and so a calculated value doesn't change, it's still six.",
    "start": "2236510",
    "end": "2244655"
  },
  {
    "text": "So the gradient here is zero. Wiggling this does nothing. And then the final one is y.",
    "start": "2244655",
    "end": "2252230"
  },
  {
    "text": "So, it's starting off value as two. So, if we wiggle it a little and make it 2.1,",
    "start": "2252230",
    "end": "2258545"
  },
  {
    "text": "our claim is that the results are change by about 0.5.",
    "start": "2258545",
    "end": "2264349"
  },
  {
    "text": "It should be multiplied by five times. So, if we make this 2.1 we then have 2.1 plus one and b 3.1.",
    "start": "2264350",
    "end": "2274430"
  },
  {
    "text": "When we get the max here would also be 2.1. And so we'd have 2.1 times 3.1.",
    "start": "2274430",
    "end": "2283100"
  },
  {
    "text": "And that's too hard arithmetic for me to do in my head. But if we take 2.1 times 3.1 it comes out to 6.51.",
    "start": "2283100",
    "end": "2296360"
  },
  {
    "text": "So, basically it's gone up by half. We don't expect the answers to be exact of course, right?",
    "start": "2296360",
    "end": "2302390"
  },
  {
    "text": "Because you know that's not the way calculus works, right? [NOISE]. Where that it's showing that we're getting the gradients right.",
    "start": "2302390",
    "end": "2309545"
  },
  {
    "text": "Okay. So this actually works. So, what are the techniques that we need to know?",
    "start": "2309545",
    "end": "2315800"
  },
  {
    "start": "2312000",
    "end": "2601000"
  },
  {
    "text": "Um, so we've sort of already seen them all. So, you know, we discussed when there are multiple incoming arcs,",
    "start": "2315800",
    "end": "2324050"
  },
  {
    "text": "how he saw workout the different local derivatives. The main other case that we need to know is if, um,",
    "start": "2324050",
    "end": "2332000"
  },
  {
    "text": "in the function computation there's a branch outward the resultant something is used in multiple places.",
    "start": "2332000",
    "end": "2339050"
  },
  {
    "text": "And so this was like the case here. I mean, here this was an initial variable, but you know, it could have been computed by something further back.",
    "start": "2339050",
    "end": "2346400"
  },
  {
    "text": "So, if this thing is used in multiple places and you have the computation going out in different ways.",
    "start": "2346400",
    "end": "2354020"
  },
  {
    "text": "It's just this simple rule that when you do backpropagation backwards you sum the gradients that you get from the different output branches.",
    "start": "2354020",
    "end": "2363319"
  },
  {
    "text": "Okay. So, if a equals X plus Y and while that's the one we showed you before that were doing this some operation to work out the total partial of f by y.",
    "start": "2363320",
    "end": "2374555"
  },
  {
    "text": "Okay. And if you sort of think about it just a little bit more,",
    "start": "2374555",
    "end": "2380569"
  },
  {
    "text": "there's sort of these obvious patterns, um, which we saw in this very simple example.",
    "start": "2380570",
    "end": "2386855"
  },
  {
    "text": "So, if you've got a plus that really the upstream gradient is going to",
    "start": "2386855",
    "end": "2394310"
  },
  {
    "text": "be sort of heading down every one of these grant branches when you have multiple branches are things being summed.",
    "start": "2394310",
    "end": "2402079"
  },
  {
    "text": "Now, in this case, it just as copied unchanged but that's because our computation was x plus y.",
    "start": "2402080",
    "end": "2410079"
  },
  {
    "text": "You know, it could be more complicated, but we're passing it down down each of those branches. So plus distributes upstream gradient.",
    "start": "2410080",
    "end": "2419310"
  },
  {
    "text": "When you have a max that's kind of like a routing operation, because max is going to be sending the gradient to in the direction that's the max,",
    "start": "2419310",
    "end": "2429419"
  },
  {
    "text": "and other things are going to get no gradient being passed down to them. Um, and then when you have, um,",
    "start": "2429419",
    "end": "2436279"
  },
  {
    "text": "a multiplication this has this kind of fun effect that what you do is switch the gradient, right?",
    "start": "2436280",
    "end": "2442609"
  },
  {
    "text": "And so this reflects the fact that when you have u times v regardless of whether u and v are vectors or just,",
    "start": "2442610",
    "end": "2450865"
  },
  {
    "text": "um, scalars that the derivative of the result with respect to u is v and the derivative of those spot- result with respect to v is u.",
    "start": "2450865",
    "end": "2460050"
  },
  {
    "text": "And so, the, um, gradient signal is the flip, um, of the tw- two numbers on the different sides.",
    "start": "2460050",
    "end": "2467890"
  },
  {
    "text": "Okay. Um, so this is sort of most of how we have",
    "start": "2467890",
    "end": "2474069"
  },
  {
    "text": "these computation graphs and we can work out backpropagation backwards in them.",
    "start": "2474070",
    "end": "2479730"
  },
  {
    "text": "There's sort of one more part of this to do, um, which is to say g,",
    "start": "2479730",
    "end": "2485779"
  },
  {
    "text": "we want to do this eff- efficiently. So, there's a bad way to do this which is to say, \"Oh well,",
    "start": "2485780",
    "end": "2491830"
  },
  {
    "text": "we wanted to calculate the partial of this by b and so we can calculate that partial.\"",
    "start": "2491830",
    "end": "2497535"
  },
  {
    "text": "Which was essentially what I was doing on last time slides. We say, \"Um, partial of s by b equals the partial of s by h,",
    "start": "2497535",
    "end": "2508545"
  },
  {
    "text": "times the partial of h by z, times the partial of z by b,",
    "start": "2508545",
    "end": "2513590"
  },
  {
    "text": "and we have all of those partials. We work them all out and multiply them together and then someone says,",
    "start": "2513590",
    "end": "2519670"
  },
  {
    "text": "um, what's the partial of s by w? And we say, huh, that's the chain rule again, I'll do it all again.",
    "start": "2519670",
    "end": "2525105"
  },
  {
    "text": "It's the partial of s by, um, h times the partial of h by z,",
    "start": "2525105",
    "end": "2531530"
  },
  {
    "text": "times the partial of and z by x,",
    "start": "2531530",
    "end": "2537435"
  },
  {
    "text": "no, no, right, ah, lost it. But you do big long list of them and you calculate all again.",
    "start": "2537435",
    "end": "2543385"
  },
  {
    "text": "That's not what we want to do. Instead we want to say, \"Oh, look there's this shared stuff.",
    "start": "2543385",
    "end": "2549010"
  },
  {
    "text": "There's this error signal coming from above.\" And we can work out the error signal the upstream gradient for this node.",
    "start": "2549010",
    "end": "2557285"
  },
  {
    "text": "We can use it to calculate the upstream gradient for this node. We can use this to calculate the upstream gradient for this node and then,",
    "start": "2557285",
    "end": "2565860"
  },
  {
    "text": "using the local gradients of which there are two calculated this node we can then calculate this one and that one.",
    "start": "2565860",
    "end": "2573880"
  },
  {
    "text": "Um, and then, from here having knowing this upstream gradient,",
    "start": "2573880",
    "end": "2579884"
  },
  {
    "text": "we can use the local gradients at this node to compute this one and that one.",
    "start": "2579885",
    "end": "2585035"
  },
  {
    "text": "And so, we're sort of doing this efficient computer science like computation,",
    "start": "2585035",
    "end": "2590380"
  },
  {
    "text": "um, where we don't do any repeated work. That makes sense? Yeah. Okay. Um, and so if that is,",
    "start": "2590380",
    "end": "2598880"
  },
  {
    "text": "um, the whole of backprop. So, um, here's sort of a slightly sketchy um graph",
    "start": "2598880",
    "end": "2606220"
  },
  {
    "start": "2601000",
    "end": "2860000"
  },
  {
    "text": "which is sort of just re-capitulating this thing. So, if you have any computation that you want to perform, um, well,",
    "start": "2606220",
    "end": "2617175"
  },
  {
    "text": "the hope is that you can sort your nodes into",
    "start": "2617175",
    "end": "2622800"
  },
  {
    "text": "what's called a topological sort which means that things that are arguments,",
    "start": "2622800",
    "end": "2628215"
  },
  {
    "text": "variables that are arguments are sorted before variables that are results that depend on that argument.",
    "start": "2628215",
    "end": "2634484"
  },
  {
    "text": "You know, providing you have something there's an a cyclic graph, you'll be able to do that. If you have a cyclic graph, you're in trouble.",
    "start": "2634485",
    "end": "2642435"
  },
  {
    "text": "Um, well, I'd be there actually techniques people use to roll out those graphs but I'm not gonna go into that now.",
    "start": "2642435",
    "end": "2647890"
  },
  {
    "text": "So, we've sorted the nodes which is kind of loosely represented here from bottom to top in a topological sort area, sort.",
    "start": "2647890",
    "end": "2656615"
  },
  {
    "text": "Okay. So then, for the forward prop we sort of go through the nodes in",
    "start": "2656615",
    "end": "2661660"
  },
  {
    "text": "the topological sort order and we if it's a variable we just set its value to what it's favorite val- variable value is.",
    "start": "2661660",
    "end": "2670640"
  },
  {
    "text": "If it's computed from other variables their values must have been set already because there earlier in the topological sort, um,",
    "start": "2670640",
    "end": "2678330"
  },
  {
    "text": "and then we compute the value of those nodes according to their predecessors,",
    "start": "2678330",
    "end": "2683815"
  },
  {
    "text": "and we pass it up and work out the final output, the loss function of our neural network and that is our forward pass.",
    "start": "2683815",
    "end": "2691845"
  },
  {
    "text": "Okay. So then, after that we do our backward pass and so for the backward pass we initialize the output gradient with one.",
    "start": "2691845",
    "end": "2700305"
  },
  {
    "text": "The top thing is always one, the partial of z with respect to z. And then, we now sort of go through the nodes in reverse topological sort.",
    "start": "2700305",
    "end": "2709590"
  },
  {
    "text": "And so therefore, each of them will all ready- anything that's,",
    "start": "2709590",
    "end": "2714645"
  },
  {
    "text": "ah, anything that's, uh, language is complex. Anything that's above that. Anything that we calculated based on it in terms of, ah,",
    "start": "2714645",
    "end": "2722680"
  },
  {
    "text": "forward pass will already have had calculated it's, um,",
    "start": "2722680",
    "end": "2728085"
  },
  {
    "text": "it's gradient as a product of upstream gradient times local gradient and then we can use that,",
    "start": "2728085",
    "end": "2735855"
  },
  {
    "text": "um, to compute the next thing down. Um, and so basically the ov- the overall role",
    "start": "2735855",
    "end": "2743299"
  },
  {
    "text": "is for any node you work out its set of successors, the things that are above it that it,",
    "start": "2743299",
    "end": "2749770"
  },
  {
    "text": "that depend on it and then you say, \"Okay, the partial of z with respect to x is simply the sum over the set of",
    "start": "2749770",
    "end": "2759079"
  },
  {
    "text": "successors of the local gradient that you calculated the node times the upstream gradient of that node.\"",
    "start": "2759080",
    "end": "2768105"
  },
  {
    "text": "Um, and in the examples that I gave before there was never, never multiple upstream gradients.",
    "start": "2768105",
    "end": "2774950"
  },
  {
    "text": "But if you imagine a, a general big graph there could actually be so different upstream gradients that are being used in- for the various successors.",
    "start": "2774950",
    "end": "2783885"
  },
  {
    "text": "So, we apply that backwards and then we've worked out in backpropagation, um,",
    "start": "2783885",
    "end": "2791480"
  },
  {
    "text": "the gradient of every, the gradient of the final result z with respect to every node in our graph.",
    "start": "2791480",
    "end": "2799109"
  },
  {
    "text": "Um, and the thing to notice about this is, if you're doing it right and efficiently,",
    "start": "2799110",
    "end": "2805664"
  },
  {
    "text": "the bigger o order of complexity of doing backpropagation is exactly the same as doing forward propagation i.e expression evaluation.",
    "start": "2805665",
    "end": "2815390"
  },
  {
    "text": "So, it's not some super expensive complex procedure that you can imagine doing and scaling up.",
    "start": "2815390",
    "end": "2822505"
  },
  {
    "text": "Um, you're actually in exactly the same complexity order. Okay. Um, so as [inaudible] entered it here this procedure,",
    "start": "2822505",
    "end": "2831950"
  },
  {
    "text": "you could just think of something that you're running on an arbitrary graph and calculating this forward pass and the backwards pass.",
    "start": "2831950",
    "end": "2841875"
  },
  {
    "text": "I mean, almost without exception that the kind of neural nets that we actually use have a regular layer",
    "start": "2841875",
    "end": "2848660"
  },
  {
    "text": "like structure and that's then precisely why it makes to- sense to work out these gradients in terms of,",
    "start": "2848660",
    "end": "2855935"
  },
  {
    "text": "um, vectors matrices and Jacobian's as the kind we were before. Okay. Um, so since we have this sort of really nice algorithm now, um,",
    "start": "2855935",
    "end": "2867119"
  },
  {
    "start": "2860000",
    "end": "3066000"
  },
  {
    "text": "this sort of means that, um, we can do this just computationally and so we don't have to think or know how to do math.",
    "start": "2867120",
    "end": "2875204"
  },
  {
    "text": "Um, and we can just have our computers do all of this with this. Um, so that using this graph structure, um,",
    "start": "2875205",
    "end": "2883020"
  },
  {
    "text": "we can just automatically work out how to apply, um, backprop.",
    "start": "2883020",
    "end": "2889470"
  },
  {
    "text": "And there are sort of two cases of this, right? So, if what was calculated at each node,",
    "start": "2889470",
    "end": "2896455"
  },
  {
    "text": "um, is given as a symbolic expression, we could actually have our computer work out for",
    "start": "2896455",
    "end": "2904190"
  },
  {
    "text": "us what the derivative of that symbolic expression is. So, it could actually calculate, um,",
    "start": "2904190",
    "end": "2910760"
  },
  {
    "text": "the gradient of that node and that's referred to as often as automatic differentiation.",
    "start": "2910760",
    "end": "2916605"
  },
  {
    "text": "So, this is kind of like Mathematica Wolfram Alpha. You know how you can do your math homework on it?",
    "start": "2916605",
    "end": "2921950"
  },
  {
    "text": "You just type in your expression, say what's a derivative and it gives it back to you right? Um, it's working doing symbolic computation and working out the derivative for you.",
    "start": "2921950",
    "end": "2931625"
  },
  {
    "text": "Um, so that- so that method could be used to work out the local gradients and then we can use",
    "start": "2931625",
    "end": "2937970"
  },
  {
    "text": "the graph structure and now rule upstream gradient times local gradient gives downstream gradient,",
    "start": "2937970",
    "end": "2944844"
  },
  {
    "text": "i.e the chain rule, um, to then propagate it through the graph and do the whole backward pass completely automatically.",
    "start": "2944844",
    "end": "2953340"
  },
  {
    "text": "And so that sounds, um, great. Um, slight disappointment, um,",
    "start": "2953340",
    "end": "2960380"
  },
  {
    "text": "current deep learning frameworks don't quite give you that. Um, there was actually a famous framework that attempted to give you that.",
    "start": "2960380",
    "end": "2967070"
  },
  {
    "text": "So the Theano Framework that was developed at the University of Montreal, um, those they've now abandoned in the modern era",
    "start": "2967070",
    "end": "2974825"
  },
  {
    "text": "of large technology corporation, deep learning frameworks. Theano did precisely that.",
    "start": "2974825",
    "end": "2980060"
  },
  {
    "text": "It did the full thing of automatic differentiation, um, for reasons that we could either think of good or bad,",
    "start": "2980060",
    "end": "2987545"
  },
  {
    "text": "current deep learning frameworks like TensorFlow or PyTorch actually do a little bit less than that.",
    "start": "2987545",
    "end": "2993500"
  },
  {
    "text": "So what they do is, say, well for an indiv- for the computations at an individual node,",
    "start": "2993500",
    "end": "2999890"
  },
  {
    "text": "you have to do the calculus for yourself. Um, for this individual node,",
    "start": "2999890",
    "end": "3005155"
  },
  {
    "text": "you have to write the forward propagation, say, you know, return X plus Y and you have to write the backward propagation,",
    "start": "3005155",
    "end": "3013870"
  },
  {
    "text": "saying the local gradients, uh, one and one to the two inputs X and Y, um,",
    "start": "3013870",
    "end": "3020290"
  },
  {
    "text": "but providing you or someone else has written out the forward and backward local step at this node,",
    "start": "3020290",
    "end": "3028105"
  },
  {
    "text": "then TensorFlow or PyTorch does all the rest of it for you and runs the backpropagation algorithm.",
    "start": "3028105",
    "end": "3034030"
  },
  {
    "text": "[NOISE] Um, and then, you know, effectively, that sort of saves you having to have a big symbolic computation engine,",
    "start": "3034030",
    "end": "3042443"
  },
  {
    "text": "because somewhat, the person coding the node computations is writing",
    "start": "3042444",
    "end": "3049030"
  },
  {
    "text": "a bit of code as you might normally imagine doing it whether in, you know, C or Pascal,",
    "start": "3049030",
    "end": "3054355"
  },
  {
    "text": "of saying returning X plus Y, and, you know, local Gradient return one.",
    "start": "3054355",
    "end": "3060325"
  },
  {
    "text": "Right? And- and you don't actually have to have a whole symbolic computation engine.",
    "start": "3060325",
    "end": "3065680"
  },
  {
    "text": "Okay. So that means the overall picture looks like this. Right? So um, schematically,",
    "start": "3065680",
    "end": "3072040"
  },
  {
    "start": "3066000",
    "end": "3307000"
  },
  {
    "text": "we have a computation graph, um, and to calculate the forward computation, um,",
    "start": "3072040",
    "end": "3079480"
  },
  {
    "text": "we, um, so- sort of put inputs into our computation graph where there's sort of X and Y variables,",
    "start": "3079480",
    "end": "3086575"
  },
  {
    "text": "and then we run through the nodes in topologically sorted order,",
    "start": "3086575",
    "end": "3092005"
  },
  {
    "text": "and for each node we calculate its forward and necessarily the things that depends on and have already been",
    "start": "3092005",
    "end": "3100000"
  },
  {
    "text": "computed and we just do expression evaluation forward. And then we return, um,",
    "start": "3100000",
    "end": "3106345"
  },
  {
    "text": "the final gate in the graph, which is our loss function, or objective function. But then, also we have the backward pass,",
    "start": "3106345",
    "end": "3114430"
  },
  {
    "text": "and for the backward pass, we go in the nodes in reversed topological, um, resorted order,",
    "start": "3114430",
    "end": "3120520"
  },
  {
    "text": "and for each of those nodes, we've return their backward value, and for their top node,",
    "start": "3120520",
    "end": "3126580"
  },
  {
    "text": "we return backward value of one, and that will then give us our gradients. And so that means, um,",
    "start": "3126580",
    "end": "3134200"
  },
  {
    "text": "for any node, any piece of computation that we perform, we need to write a little bit of code that um",
    "start": "3134200",
    "end": "3143170"
  },
  {
    "text": "says what it's doing on the forward pass and what it's doing on the backward pass. So on the forward pass, um,",
    "start": "3143170",
    "end": "3150745"
  },
  {
    "text": "this is our multiplication, so we're just saying return X times Y.",
    "start": "3150745",
    "end": "3155935"
  },
  {
    "text": "So that's pretty easy. That's what you're used to doing. But while we also need to do the backward passes,",
    "start": "3155935",
    "end": "3162280"
  },
  {
    "text": "local gradients of return what is the partial of L with respect to Z and with respect to X.",
    "start": "3162280",
    "end": "3170395"
  },
  {
    "text": "And well, to do that, we have to do a little bit more work. So we have to do a little bit more work,",
    "start": "3170395",
    "end": "3176425"
  },
  {
    "text": "first of all, in the forward pass. So, in the forward pass, we have to remember to sort of stuff away in some variables",
    "start": "3176425",
    "end": "3184870"
  },
  {
    "text": "what values we computed in the for- what- what values were given to us in the forward pass,",
    "start": "3184870",
    "end": "3190420"
  },
  {
    "text": "or else we won't be able to calculate the backward pass. So we store away the values of X and Y,",
    "start": "3190420",
    "end": "3197619"
  },
  {
    "text": "um, and so then, when we're doing the backward pass, we are passed into us the upstream Gradient,",
    "start": "3197620",
    "end": "3204550"
  },
  {
    "text": "the error signal, and now we just do calculate, um,",
    "start": "3204550",
    "end": "3209845"
  },
  {
    "text": "upstream Gradient times local Gradient- upstream Gradient times local Gradient,",
    "start": "3209845",
    "end": "3215064"
  },
  {
    "text": "and we return backwards, um, those um downstream Gradients.",
    "start": "3215064",
    "end": "3220900"
  },
  {
    "text": "And so providing we do that for all the nodes of our graph, um, we then have something that, um,",
    "start": "3220900",
    "end": "3228365"
  },
  {
    "text": "the system can learn for us as a deep learning system. And so what that means in practice,",
    "start": "3228365",
    "end": "3234579"
  },
  {
    "text": "um, is that, you know, any of these deep learning frameworks come with a whole box of tools that says,",
    "start": "3234580",
    "end": "3241180"
  },
  {
    "text": "um, here is a fully connected forward layer, here is a sigmoid unit, here is other more complicated things we'll do later,",
    "start": "3241180",
    "end": "3248560"
  },
  {
    "text": "like convolutions and recurrent layers. And to the extent that you are using one of those,",
    "start": "3248560",
    "end": "3253569"
  },
  {
    "text": "somebody else has done this work for you. Right? That they've um defined, um,",
    "start": "3253570",
    "end": "3259795"
  },
  {
    "text": "nodes or a layer of nodes that have forward and backward already written for- for them.",
    "start": "3259795",
    "end": "3265855"
  },
  {
    "text": "And to the extent that that's true, um, that means that making neural nets is heaps of fun. It's just like lego.",
    "start": "3265855",
    "end": "3272850"
  },
  {
    "text": "Right? You just stick these layers together and say, \"God, I have to learn on some data and train it.\" You know, it's so easy that my high school student is building these things.",
    "start": "3272850",
    "end": "3280559"
  },
  {
    "text": "Right? Um, you don't have to understand much really, um, but, you know, to the extent that you actually want to do some original research and think,",
    "start": "3280560",
    "end": "3287714"
  },
  {
    "text": "\"I've got this really cool idea of how to do things differently. I'm going to define my own kind of different computation.\"",
    "start": "3287715",
    "end": "3293920"
  },
  {
    "text": "Well, then you have to do this and define your class, and as well as, sort of saying,",
    "start": "3293920",
    "end": "3299050"
  },
  {
    "text": "how to compute the forward value, you will have to pull out your copy of Wolfram Alpha and work out what the derivatives are,",
    "start": "3299050",
    "end": "3305665"
  },
  {
    "text": "um, and put that into the backward pass. Um, yeah. Okay. So here's just one little more note on that.",
    "start": "3305665",
    "end": "3313600"
  },
  {
    "start": "3307000",
    "end": "3521000"
  },
  {
    "text": "Um, you know, in the early days of deep learning, say prior to 2014,",
    "start": "3313600",
    "end": "3320994"
  },
  {
    "text": "what we always used to state to everybody very sternly is, \"You should check all your Gradients,",
    "start": "3320995",
    "end": "3326590"
  },
  {
    "text": "by doing numeric Gradient checks. It's really really important.\" Um, and so what that meant was, well, you know,",
    "start": "3326590",
    "end": "3334420"
  },
  {
    "text": "if you want to know whether you have coded your backward pass right, an easy way to check, um,",
    "start": "3334420",
    "end": "3340734"
  },
  {
    "text": "whether you've coded it right, is to do this numeric Gradient",
    "start": "3340735",
    "end": "3346450"
  },
  {
    "text": "where you're sort of estimating the slope by wiggling it a bit, and wiggling the input a bit,",
    "start": "3346450",
    "end": "3352914"
  },
  {
    "text": "and seeing what effect it has. So I'm working out the value of the function the F of X plus H,",
    "start": "3352915",
    "end": "3359079"
  },
  {
    "text": "for H very small like E to the minus four, and then F of X minus H, um,",
    "start": "3359080",
    "end": "3364150"
  },
  {
    "text": "and then dividing by 2H, and I'm saying well, what is the slope at this point, and I'm getting a numeric estimate of the Gradient with respect,",
    "start": "3364150",
    "end": "3371920"
  },
  {
    "text": "um, to my variable X here. Um, so this is what you will have seen in",
    "start": "3371920",
    "end": "3378309"
  },
  {
    "text": "high school when you did the sort of first um estimates of Gradients, where you sort of worked out F of X plus H divided by H",
    "start": "3378310",
    "end": "3386710"
  },
  {
    "text": "and you're doing rise over run and got a point estimate of the Gradient. Um, exactly the same thing,",
    "start": "3386710",
    "end": "3392770"
  },
  {
    "text": "except for the fact, in this case, rather than doing it one sided like that,",
    "start": "3392770",
    "end": "3398410"
  },
  {
    "text": "we are doing it two-sided. It turns out that if you actually wanna do this, two-sided is asymptotically hugely [NOISE] better,",
    "start": "3398410",
    "end": "3407035"
  },
  {
    "text": "and so you're always better off doing two-sided Gradient checks rather than one-sided Gradient checks.",
    "start": "3407035",
    "end": "3412900"
  },
  {
    "text": "Um, so since you saw that- since it's hard to implement this wrong, this is a good way to check that your Gradients are",
    "start": "3412900",
    "end": "3419470"
  },
  {
    "text": "correct if you've defined them yourselves. Um, as a technique to use it [NOISE] for anything,",
    "start": "3419470",
    "end": "3426625"
  },
  {
    "text": "it's completely, completely hopeless, because we're thinking of doing this over",
    "start": "3426625",
    "end": "3432040"
  },
  {
    "text": "our deep learning model for a fully connected layer. What this means [NOISE] is that,",
    "start": "3432040",
    "end": "3437080"
  },
  {
    "text": "if you've got this sort of like a W matrix of N by M and you want to, um,",
    "start": "3437080",
    "end": "3442555"
  },
  {
    "text": "calculate um your partial derivatives to check if they're correct, it means that you have to do this for every element of the matrix.",
    "start": "3442555",
    "end": "3451360"
  },
  {
    "text": "So you have to calculate the eventual loss, first jiggling W11, then jiggling W12,",
    "start": "3451360",
    "end": "3457390"
  },
  {
    "text": "then jiggling one- W13, 14 et cetera. So you have- in the complex network,",
    "start": "3457390",
    "end": "3462880"
  },
  {
    "text": "you'll end up literally doing millions of function evaluations to check the Gradients at one point in time.",
    "start": "3462880",
    "end": "3469565"
  },
  {
    "text": "So, you know, it's, it's not like what I advertised for backprop when I said it's just as efficient as calculating,",
    "start": "3469565",
    "end": "3477220"
  },
  {
    "text": "um, the forward value. Doing this is forward value computation time multiplied by number of parameters in our model,",
    "start": "3477220",
    "end": "3486190"
  },
  {
    "text": "which is often huge for deep learning networks. So this is something that you only want to have inside- if statements that you could turn off.",
    "start": "3486190",
    "end": "3494170"
  },
  {
    "text": "So you could just sort of run it to check that your code isn't bre- um, debuggy. Um, you know, in honesty,",
    "start": "3494170",
    "end": "3501640"
  },
  {
    "text": "this is just much less needed now because, you know, by and large you can plug together your components and layers and PyTorch,",
    "start": "3501640",
    "end": "3508119"
  },
  {
    "text": "um, and other people wrote the code right and it will work. Um, so you probably don't need to do this all the time.",
    "start": "3508120",
    "end": "3515515"
  },
  {
    "text": "But it is still a useful thing to look at and to know about if things um, are going wrong.",
    "start": "3515515",
    "end": "3522190"
  },
  {
    "start": "3521000",
    "end": "3600000"
  },
  {
    "text": "Yeah. Okay, so we- we've now mastered the core technology of neural nets. We saw now well, basically everything we need to know about neural nets,",
    "start": "3522190",
    "end": "3531070"
  },
  {
    "text": "and I sort of just, um, summarized it there. Um, just to sort of emphasize um once more.",
    "start": "3531070",
    "end": "3539140"
  },
  {
    "text": "Um, you know, I think some people think, why do we even lear- need to learn all this stuff about gradients?'",
    "start": "3539140",
    "end": "3547840"
  },
  {
    "text": "And there's a sense in which it's [inaudible] really, because these modern deep learning frameworks will compute all of the gradients for you.",
    "start": "3547840",
    "end": "3554770"
  },
  {
    "text": "You know, we make you suffer on homework two, but in homework three, you can have your gradients computed for you.",
    "start": "3554770",
    "end": "3561625"
  },
  {
    "text": "But, you know, I- so you know it's sort of just, like, well, why should you take a c- a class on compilers, right?",
    "start": "3561625",
    "end": "3567940"
  },
  {
    "text": "That there's actually something useful in understanding what goes on under the hood,",
    "start": "3567940",
    "end": "3573415"
  },
  {
    "text": "even though most of the time, we're just perfectly happy to let the C compiler do its thing,",
    "start": "3573415",
    "end": "3578815"
  },
  {
    "text": "without being experts on X86 assembler every day of the wa- week.",
    "start": "3578815",
    "end": "3584079"
  },
  {
    "text": "But, you know, there is more to it than that. Um, you know, because even though backpropagation is great,",
    "start": "3584080",
    "end": "3589839"
  },
  {
    "text": "once you're building complex models, backpropagation doesn't always work as you would expect it to.",
    "start": "3589840",
    "end": "3596320"
  },
  {
    "text": "Perfectly is maybe the wrong word, because you know mathematically it's perfect. Um, but it might not be achieving what you're wanting it to.",
    "start": "3596320",
    "end": "3603595"
  },
  {
    "text": "And well, if you want to sort of then debug an improved models, it's kind of crucial to understand what's going on.",
    "start": "3603595",
    "end": "3609775"
  },
  {
    "text": "So, there's a nice medium piece by Andre Karpathy, of yes you should understand backprop um that's on the syllabus page, um,",
    "start": "3609775",
    "end": "3617890"
  },
  {
    "text": "that talks about this and indeed um, um, week after next, Abby is actually going to lecture about recurrent neural networks,",
    "start": "3617890",
    "end": "3626410"
  },
  {
    "text": "and you know one of the places, um, where you can easily fail um, and doing backpropagation turns up there,",
    "start": "3626410",
    "end": "3633580"
  },
  {
    "text": "um, is a good example. Okay. So anyone have any questions about backpropagation and computation graphs?",
    "start": "3633580",
    "end": "3643250"
  },
  {
    "text": "Okay. If not the remainder of the time is, um,",
    "start": "3645660",
    "end": "3651204"
  },
  {
    "text": "the grab bag of things that you really should know about, if you're going to be doing deep learning.",
    "start": "3651205",
    "end": "3657310"
  },
  {
    "text": "And so, yeah, this is just itsy-bitsy and, but let me say them. Um, so up until now,",
    "start": "3657310",
    "end": "3664335"
  },
  {
    "text": "when we've had um loss functions, and we've been maximizing the likelihood of our data,",
    "start": "3664335",
    "end": "3670560"
  },
  {
    "text": "and stuff like that, we've sort of just had this part here which is the likelihood of our data,",
    "start": "3670560",
    "end": "3677235"
  },
  {
    "text": "and we've worked to maximize it. Um, however, um, in practice that works badly usually,",
    "start": "3677235",
    "end": "3687445"
  },
  {
    "text": "and we need to do something else which is regularize our models. And if you've done the Machine Learning class,",
    "start": "3687445",
    "end": "3694645"
  },
  {
    "text": "or something like that you will have seen regularization. And there are various techniques to do regularization, but, um,",
    "start": "3694645",
    "end": "3702444"
  },
  {
    "text": "compared to anything else, regularization is even more important, um, for deep learning models, right?",
    "start": "3702445",
    "end": "3708820"
  },
  {
    "text": "So, um, the general idea is if you have a lot of parameters in your model,",
    "start": "3708820",
    "end": "3714610"
  },
  {
    "text": "those parameters can just essentially memorize what's in the data that you trained at.",
    "start": "3714610",
    "end": "3720850"
  },
  {
    "text": "And so they're very good at predicting the answers. The model becomes very good at predicting the answers to the data you trained it on,",
    "start": "3720850",
    "end": "3729205"
  },
  {
    "text": "but the model may become poor at working in the real world, and different examples.",
    "start": "3729205",
    "end": "3735040"
  },
  {
    "text": "And somehow we want to stop that. And this problem is especially bad for deep learning models,",
    "start": "3735040",
    "end": "3742000"
  },
  {
    "text": "because typically deep learning models have vast, vast numbers of parameters. So in the good old days when statisticians ruled the show,",
    "start": "3742000",
    "end": "3749800"
  },
  {
    "text": "they told people that it was completely ridiculous to have a number of parameters that approached your number of training examples.",
    "start": "3749800",
    "end": "3758650"
  },
  {
    "text": "You know, you should never have more parameters in your model, than one-tenth of the number of your training examples.",
    "start": "3758650",
    "end": "3764710"
  },
  {
    "text": "So it's the kind of um rules of thumb you are told, so that you had lots of examples with which to estimate every parameter.",
    "start": "3764710",
    "end": "3771865"
  },
  {
    "text": "Um, that's just not true with deep learning models, is just really common that we trained",
    "start": "3771865",
    "end": "3777010"
  },
  {
    "text": "deep learning models that have 10 times as many parameters, as we have training examples.",
    "start": "3777010",
    "end": "3782980"
  },
  {
    "text": "Um, but miraculously it works. In fact it works brilliantly. Those highly over parameterized models,",
    "start": "3782980",
    "end": "3790120"
  },
  {
    "text": "and this one of the big secret sources of why deep learning has been so brilliant, but it only works if we regularize the model.",
    "start": "3790120",
    "end": "3798085"
  },
  {
    "text": "So, if you train a model without sufficient regularization, what you find is that you're training it and working out your loss on the training data,",
    "start": "3798085",
    "end": "3809049"
  },
  {
    "text": "and the model keeps on getting better, and better, and better, and better. Um, necessarily, alg- algorithm has to improve loss on the training data.",
    "start": "3809050",
    "end": "3818170"
  },
  {
    "text": "So the worst thing that could happen, is that the graph could become absolutely fa- flat.",
    "start": "3818170",
    "end": "3823375"
  },
  {
    "text": "What you'll find is with most models that we train, they have so many parameters that this will just keep on going down,",
    "start": "3823375",
    "end": "3831565"
  },
  {
    "text": "until the loss is sort of approaching the numerical precision of zero, if you leave it training for long enough.",
    "start": "3831565",
    "end": "3837940"
  },
  {
    "text": "It just learns the correct answer for every example, beca- because effectively can memorize the examples.",
    "start": "3837940",
    "end": "3844405"
  },
  {
    "text": "Okay, but if you then say, ''Let me test out this model on some different data.''",
    "start": "3844405",
    "end": "3849640"
  },
  {
    "text": "What you find is this red curve, that up until a certain point, um,",
    "start": "3849640",
    "end": "3855609"
  },
  {
    "text": "that you are also building a model that's better at predicting on different data, but after some point this curve starts to curve up again.",
    "start": "3855610",
    "end": "3863845"
  },
  {
    "text": "And ignore that bit where it seems to curve down again, that was a mistake in the drawing. Um, and so this is then referred to as over-fitting,",
    "start": "3863845",
    "end": "3871075"
  },
  {
    "text": "that the- from here on the training model is just learning to memorize whatever was in the training data,",
    "start": "3871075",
    "end": "3879535"
  },
  {
    "text": "but not in a way that later generalized to other examples.",
    "start": "3879535",
    "end": "3884589"
  },
  {
    "text": "And so this is not what we want. We want to try and avoid over-fitting as much as possible,",
    "start": "3884590",
    "end": "3890575"
  },
  {
    "text": "and there are various regularization techniques that we use for that. And simple starting one is this one here where we penalize the log-likelihood by saying,",
    "start": "3890575",
    "end": "3901059"
  },
  {
    "text": "''You're going to be penalized to the extent that you move parameters away from zero.''",
    "start": "3901060",
    "end": "3907389"
  },
  {
    "text": "So the default state of nature is all parameters are zeros, so they're ignored on computations.",
    "start": "3907389",
    "end": "3913675"
  },
  {
    "text": "You can have parameters that have big values, but you'll pee penalized a bit four, and this is referred to as L-2 regularization.",
    "start": "3913675",
    "end": "3921490"
  },
  {
    "text": "And, you know, that's sort of a starting point of something sensible you could do with regularization,",
    "start": "3921490",
    "end": "3926530"
  },
  {
    "text": "but there's more to say later. And we'll talk in this sort of lecture before we discuss",
    "start": "3926530",
    "end": "3932320"
  },
  {
    "text": "final projects of other clever regularization techniques at neural networks.",
    "start": "3932320",
    "end": "3937480"
  },
  {
    "text": "Okay. Um, grab bag number two, vectorization is the term that you have here,",
    "start": "3937480",
    "end": "3944290"
  },
  {
    "text": "um, but it's not only vectors. This is also matrixization, and higher dimensional matrices what are called tensors,",
    "start": "3944290",
    "end": "3952870"
  },
  {
    "text": "in this field tensorization. Um, getting deep learning systems to run fast and",
    "start": "3952870",
    "end": "3958690"
  },
  {
    "text": "efficiently is only possible if we vectorize things.",
    "start": "3958690",
    "end": "3965214"
  },
  {
    "text": "Um, and what does that mean? What that means is, you know, the straightforward way to write a lot of code um,",
    "start": "3965215",
    "end": "3973300"
  },
  {
    "text": "that you saw in your first CS class, is you say for I in range in um calculate random randi-1.",
    "start": "3973300",
    "end": "3982119"
  },
  {
    "text": "Um, but when we want to be clever, um, people, um, that are doing things fast,",
    "start": "3982120",
    "end": "3992305"
  },
  {
    "text": "um, we say rather than work out this W dot one word vector at a time,",
    "start": "3992305",
    "end": "3998619"
  },
  {
    "text": "and do it in a four loop, we could instead put all of our word vectors into one matrix,",
    "start": "3998620",
    "end": "4004950"
  },
  {
    "text": "and then do simply one matrix-matrix multiply of W by our word vector matrix.",
    "start": "4004950",
    "end": "4012675"
  },
  {
    "text": "And even if you run your code on your laptop on a CPU,",
    "start": "4012675",
    "end": "4018735"
  },
  {
    "text": "you will find out that if you do it the vectorized way, things will become hugely faster.",
    "start": "4018735",
    "end": "4024900"
  },
  {
    "text": "So in this example, it became over an order of magnitude faster, when doing it with a vector- vectorized rather than,",
    "start": "4024900",
    "end": "4031785"
  },
  {
    "text": "um, with a full loop. Um, and those gains are only compounded when we run code on a GPU,",
    "start": "4031785",
    "end": "4039000"
  },
  {
    "text": "that you'll get no gains and speed of tall on a GPU, unless your code is vectorized.",
    "start": "4039000",
    "end": "4044190"
  },
  {
    "text": "But if it is vectorized, then you can hope to have results, of oh, yeah, this runs 40 times faster,",
    "start": "4044190",
    "end": "4049650"
  },
  {
    "text": "than it did on the CPU. Okay, um, yeah, so always try to use vectors and matrices not for loops.",
    "start": "4049650",
    "end": "4059415"
  },
  {
    "text": "Um, of course it's useful when developing stuff to time your code, and find out what's slow. Um, okay.",
    "start": "4059415",
    "end": "4065954"
  },
  {
    "text": "Point three. Um, okay, so we discussed this idea, um, last time,",
    "start": "4065955",
    "end": "4073845"
  },
  {
    "text": "and the time before that after- after having the sort of affine layer,",
    "start": "4073845",
    "end": "4079575"
  },
  {
    "text": "where we took, you know, go from X to WX, plus B. That's referred to as an affine layer,",
    "start": "4079575",
    "end": "4085500"
  },
  {
    "text": "so we're doing this, um, multiplying a vector by a matrice- matrix, and adding um biases.",
    "start": "4085500",
    "end": "4092505"
  },
  {
    "text": "We necessarily to have power and a deep network, um, have to have some form of non-linearity.",
    "start": "4092505",
    "end": "4099915"
  },
  {
    "text": "And so, I just wanted to go through a bit of background on non-linearity is in what people use,",
    "start": "4099915",
    "end": "4105644"
  },
  {
    "text": "and what to use. So, if you're sort of starting from the idea of what we know is logistic regression, um,",
    "start": "4105644",
    "end": "4113339"
  },
  {
    "text": "what's commonly referred to as the sigmoid curve, or maybe more precisely is the logistic,",
    "start": "4113340",
    "end": "4119670"
  },
  {
    "text": "um, function is this picture here. So something that's squashes any real",
    "start": "4119670",
    "end": "4126060"
  },
  {
    "text": "number positive or negative into the range zero to one. It gives you a probability output.",
    "start": "4126060",
    "end": "4131730"
  },
  {
    "text": "Um, these- this use of this, um, logistic function was really really common in early neural nets.",
    "start": "4131730",
    "end": "4140400"
  },
  {
    "text": "If you go back to '80s, '90s neural nets, there were, um, sigmoid functions absolutely everywhere.",
    "start": "4140400",
    "end": "4147134"
  },
  {
    "text": "Um, in more recent times, 90 percent of the time nobody uses",
    "start": "4147135",
    "end": "4153150"
  },
  {
    "text": "this and they've been found to sort of actually work quite poorly. The only place these are used is when you",
    "start": "4153150",
    "end": "4159870"
  },
  {
    "text": "actually want a value between zero and one is your output. So we'll talk later about how you have gating in networks,",
    "start": "4159870",
    "end": "4168270"
  },
  {
    "text": "and so gating as a place where you want to have a probability between two things. And then you will use one of those,",
    "start": "4168270",
    "end": "4174795"
  },
  {
    "text": "but you use some absolutely nowhere else. Um, here is the tanh curve.",
    "start": "4174795",
    "end": "4180240"
  },
  {
    "text": "Um, so the formula for tanh, um, looks like a scary thing with thoughts of exponentials in it,",
    "start": "4180240",
    "end": "4186299"
  },
  {
    "text": "and it doesn't really look much like a logistic curve whatsoever. Um, but if you um dig up your math textbook you can convince yourself that",
    "start": "4186300",
    "end": "4196740"
  },
  {
    "text": "a tanh curve is actually exactly the same as the logistic curve apart from you multiply it by two,",
    "start": "4196740",
    "end": "4204090"
  },
  {
    "text": "so it has a range of two rather than one, and you shift it down line. So, this is sort of just a re-scaled logistic.",
    "start": "4204090",
    "end": "4210735"
  },
  {
    "text": "There's now symmetric between one and minus one, and the fact that some metric in the output actually helps",
    "start": "4210735",
    "end": "4216900"
  },
  {
    "text": "a lot for putting into neural networks. Um. So, tanh's, are still reasonably widely used",
    "start": "4216900",
    "end": "4225070"
  },
  {
    "text": "in quite a number of places um in um your networks. So, tanh should be a friend of yours and you should know about that.",
    "start": "4225070",
    "end": "4232755"
  },
  {
    "text": "But you know, one of the bad things about using um transcendental functions like the sigmoid or tanh is,",
    "start": "4232755",
    "end": "4241320"
  },
  {
    "text": "you know, they involve this expensive math operations um that slow you down.",
    "start": "4241320",
    "end": "4248300"
  },
  {
    "text": "Like, it's sort of a nuisance to be kind of computing exponentials and tanh's in your computer,",
    "start": "4248300",
    "end": "4253829"
  },
  {
    "text": "things are kind of slow. So people started um playing around with ways",
    "start": "4253830",
    "end": "4258869"
  },
  {
    "text": "to make things faster and so someone came up with this idea like, maybe we could come up with a hard tanh,",
    "start": "4258870",
    "end": "4265360"
  },
  {
    "text": "um where it's just sort of flat out here and then it has a linear slope and then it's flat at the top.",
    "start": "4265360",
    "end": "4272000"
  },
  {
    "text": "You know, it sort of looks like a tanh but we just squared it off. Um, and while this is really cheap to compute right, you say,",
    "start": "4272000",
    "end": "4279580"
  },
  {
    "text": "x less than minus one, return minus one, return plus one or just return the number.",
    "start": "4279580",
    "end": "4286630"
  },
  {
    "text": "No complex transcendentals. The funny thing is, it turns out that this actually works pretty well.",
    "start": "4286630",
    "end": "4293475"
  },
  {
    "text": "You might be scared and you might justifiably be scared because if you start thinking about gradients,",
    "start": "4293475",
    "end": "4300340"
  },
  {
    "text": "once you're over here, there's no gradient, right? It's completely flat at zero.",
    "start": "4300340",
    "end": "4306495"
  },
  {
    "text": "So, things go dead as soon as they're at one of the ends. So, it's sort of important to stay in this middle section at least for",
    "start": "4306495",
    "end": "4314350"
  },
  {
    "text": "a while and then its just got a slope of one, right? It's a constant slope of one.",
    "start": "4314350",
    "end": "4319800"
  },
  {
    "text": "But this is enough of a linearity that actually it works well in neural networks and you can train neural networks.",
    "start": "4319800",
    "end": "4328770"
  },
  {
    "text": "So, that's sent the whole field in the opposite direction and people thought,",
    "start": "4328770",
    "end": "4333995"
  },
  {
    "text": "oh, if that works, maybe we can make things even simpler. And that led to the now famous what's referred to [inaudible] as ReLU.",
    "start": "4333995",
    "end": "4344100"
  },
  {
    "text": "So there is a mistake in my editing there, delete off hard tanh. That was in slides by mistake.",
    "start": "4344100",
    "end": "4350830"
  },
  {
    "text": "[LAUGHTER] The ReLU unit, everyone calls it ReLU which stands for rectified linear unit.",
    "start": "4350830",
    "end": "4356795"
  },
  {
    "text": "So, the Re-, the ReLU is essentially the simplest non-linearity you can have.",
    "start": "4356795",
    "end": "4362250"
  },
  {
    "text": "So the ReLU is zero, slope zero as soon as you're in the negative regime and it's just a line slope one,",
    "start": "4362250",
    "end": "4372015"
  },
  {
    "text": "when you're in the positive regime. I mean, when I first saw this, I mean, it's sort of blew my mind it could possibly work.",
    "start": "4372015",
    "end": "4379835"
  },
  {
    "text": "Because it sort of, I guess, I was brought up on these sort of tanh's and sigmoids and the sorts of these arguments",
    "start": "4379835",
    "end": "4387220"
  },
  {
    "text": "about the slope and you get these gradients and you can move around with the gradient.",
    "start": "4387220",
    "end": "4393250"
  },
  {
    "text": "And how is it meant to work if half of this function just says output zero and no gradient and the other half is just this straight line.",
    "start": "4393250",
    "end": "4401720"
  },
  {
    "text": "And in particular, when you're in the positive regime, this is just an identity function.",
    "start": "4401720",
    "end": "4407905"
  },
  {
    "text": "And, you know, I sort of argued before that if you just compose linear transforms,",
    "start": "4407905",
    "end": "4415139"
  },
  {
    "text": "you don't get any power but provided when this is the right-hand part of the regime.",
    "start": "4415140",
    "end": "4420560"
  },
  {
    "text": "Since this is an identity function, that's exactly what we're doing. We're just composing linear transforms.",
    "start": "4420560",
    "end": "4425770"
  },
  {
    "text": "So you- you sort of believe it just can't possibly work but it turns out that this works brilliantly.",
    "start": "4425770",
    "end": "4431755"
  },
  {
    "text": "And this is now by far the default choice when people are building feed for deep networks.",
    "start": "4431755",
    "end": "4439639"
  },
  {
    "text": "That people use ReLU non-linearities and they are very fast,",
    "start": "4439640",
    "end": "4445190"
  },
  {
    "text": "they train very quickly and they perform very well. And so, effectively, you know,",
    "start": "4445190",
    "end": "4450844"
  },
  {
    "text": "it is, it is simply just each u-, depending on the inputs, each unit is just either dead or it's passing things on as an identity function.",
    "start": "4450845",
    "end": "4460495"
  },
  {
    "text": "But that's enough of lini-, non-linearity that you can do arbitrary function approximation still with a deep learning network.",
    "start": "4460495",
    "end": "4468400"
  },
  {
    "text": "And people now make precisely the opposite argument which is, because this unit just has a slope of one over it's non-zero range, that means,",
    "start": "4468400",
    "end": "4481775"
  },
  {
    "text": "the gradient is past spec very efficiently to the inputs and therefore the models train very efficiently whereas,",
    "start": "4481775",
    "end": "4490860"
  },
  {
    "text": "when you are with these kind of curves, when you're over here, there's very little slope so your models might train very slowly.",
    "start": "4490860",
    "end": "4498849"
  },
  {
    "text": "Okay. So, you know, for feed-forward network, try this before you try anything else.",
    "start": "4498850",
    "end": "4505324"
  },
  {
    "text": "But there's sort of then been a sub literature that says, well, maybe that's too simple and we could do a bit better.",
    "start": "4505325",
    "end": "4512620"
  },
  {
    "text": "And so that led to the leaky ReLU which said, \"Maybe we should put a tiny bit of slope over here so it's not completely dead.\"",
    "start": "4512620",
    "end": "4519775"
  },
  {
    "text": "So you can make it something like one, one 100th as the slope of this part.",
    "start": "4519775",
    "end": "4525405"
  },
  {
    "text": "And then people had, well, let's build off that, maybe we could actually put another parameter into",
    "start": "4525405",
    "end": "4531360"
  },
  {
    "text": "our neural network and we could have a parametric ReLU. So, there's some slope over here but we're also going to",
    "start": "4531360",
    "end": "4538980"
  },
  {
    "text": "backpropagate into our non-linearity which has this extra alpha parameter,",
    "start": "4538980",
    "end": "4545280"
  },
  {
    "text": "which is how ma- much slope it has. And so, variously people have used these,",
    "start": "4545280",
    "end": "4550835"
  },
  {
    "text": "you can sort of find 10 papers on archive where people say, you can get better results from using one or other of these.",
    "start": "4550835",
    "end": "4557949"
  },
  {
    "text": "You can also find papers where people said it made no difference for them versus just using a ReLU.",
    "start": "4557950",
    "end": "4563955"
  },
  {
    "text": "So, I think basically, you can start off with a ReLU and work from there. Yes. So, parameter initialization,",
    "start": "4563955",
    "end": "4573494"
  },
  {
    "text": "it's when, so, when we have these matrices and parameters in our model, it's vital, vital, vital,",
    "start": "4573495",
    "end": "4580910"
  },
  {
    "text": "that you have to initialize those parameter weights with small random values.",
    "start": "4580910",
    "end": "4587900"
  },
  {
    "text": "This was precisely the lesson that some people hadn't discovered when it came to final project time.",
    "start": "4587900",
    "end": "4593520"
  },
  {
    "text": "So I'll emphasize it is vital, vital. So, if you just start off with the weights being zero,",
    "start": "4593520",
    "end": "4600025"
  },
  {
    "text": "you kind of have these complete symmetries, right, that everything will be calculated the same,",
    "start": "4600025",
    "end": "4605320"
  },
  {
    "text": "everything will move the same and you're not actually training this complex network with a lot of units that are specializing to learn different things.",
    "start": "4605320",
    "end": "4614600"
  },
  {
    "text": "So, somehow, you have to break the symmetry and we do that by giving small random weights.",
    "start": "4614600",
    "end": "4620510"
  },
  {
    "text": "So, you know, there's sort of some fine points. When you have biases, you may as well just start them at Zero,",
    "start": "4620510",
    "end": "4626400"
  },
  {
    "text": "as neutral and see how the system learn the bias that you want et cetera.",
    "start": "4626400",
    "end": "4631640"
  },
  {
    "text": "But in general, the weights you want to initialize to small random values.",
    "start": "4631640",
    "end": "4638965"
  },
  {
    "text": "You'll find in PyTorch or other deep learning practi- packages,",
    "start": "4638965",
    "end": "4644905"
  },
  {
    "text": "a common initialization that's used and often recommended is this Xavier Initialization.",
    "start": "4644905",
    "end": "4650539"
  },
  {
    "text": "And so, the trick of this is that, for a lot of models and a lot of places,",
    "start": "4650540",
    "end": "4657350"
  },
  {
    "text": "think of some of these things like these ones and these, you'd like the values in the network to sort of stay small,",
    "start": "4657350",
    "end": "4666205"
  },
  {
    "text": "in this sort of middle range here. And well, if you kind of have a matrix with big values in it",
    "start": "4666205",
    "end": "4673145"
  },
  {
    "text": "and you multiply a vector by this matrix, you know, things might get bigger.",
    "start": "4673145",
    "end": "4678910"
  },
  {
    "text": "And then if you put in through another layer, it'll get bigger again and then sort of everything will be too big and you will have problems.",
    "start": "4678910",
    "end": "4685980"
  },
  {
    "text": "So, really, Xavier Initialization is seeking to avoid that by saying, how many inputs are there to this node?",
    "start": "4685980",
    "end": "4694590"
  },
  {
    "text": "How many outputs are there? We want to sort of temp it down the initialization based on the inputs",
    "start": "4694590",
    "end": "4700920"
  },
  {
    "text": "and the outputs because effectively we'll be using this number that many times.",
    "start": "4700920",
    "end": "4706395"
  },
  {
    "text": "It's a good thing to use, you can use that. Optimizers. Up till now,",
    "start": "4706395",
    "end": "4714955"
  },
  {
    "text": "we saw, just talked about plain SGD. You know, normally plain SGD actually works just fine.",
    "start": "4714955",
    "end": "4723785"
  },
  {
    "text": "But often if you want to use just plain SGD, you have to spend time tuning the learning rate,",
    "start": "4723785",
    "end": "4729860"
  },
  {
    "text": "that alpha that we multiplied the gradient by. For complex nets and situations or to avoid worry,",
    "start": "4729860",
    "end": "4738535"
  },
  {
    "text": "there's sort of now this big family and more sophisticated adaptive optimizers.",
    "start": "4738535",
    "end": "4743710"
  },
  {
    "text": "And so, effectively they're scaling the parameter adjustment by accumulated gradients,",
    "start": "4743710",
    "end": "4750055"
  },
  {
    "text": "which have the effect that they learn per parameter learning rates. So that they can see which parameters would be useful to move",
    "start": "4750055",
    "end": "4758120"
  },
  {
    "text": "more and which one is less depending on the sensitivity of those parameters. So, where things are flat,",
    "start": "4758120",
    "end": "4764040"
  },
  {
    "text": "you can be trying to move quickly. Where things are bouncing around a lot, you are going to be trying to move just a little so as not to overshoot.",
    "start": "4764040",
    "end": "4770550"
  },
  {
    "text": "And so, there's a whole family of these; Adagrad, RMSprop, Adam, there are actually other ones. There's Adam Max and whole lot of them.",
    "start": "4770550",
    "end": "4777675"
  },
  {
    "text": "I mean, Adam is one fairly reliable one that many people use and that's not bad.",
    "start": "4777675",
    "end": "4783395"
  },
  {
    "text": "And then one more slide and I'm done. Yes, so learning rates. So, normally you have to choose a learning rate.",
    "start": "4783395",
    "end": "4791420"
  },
  {
    "text": "So, one choice is just have a constant learning rate. You pick a number, may be 10 to the minus three and say that's my learning rate.",
    "start": "4791420",
    "end": "4799500"
  },
  {
    "text": "You want your learning rate to be order of magnitude, right. If your learning rate is too big,",
    "start": "4799500",
    "end": "4807670"
  },
  {
    "text": "your model might diverge or not converge because it just sort of leaps you around by",
    "start": "4807670",
    "end": "4812989"
  },
  {
    "text": "huge cram movements and you completely miss the good parts of your function space.",
    "start": "4812990",
    "end": "4819915"
  },
  {
    "text": "If your model, if your learning rate is too small, your model may not train by the assignment deadline and then you'll be unhappy.",
    "start": "4819915",
    "end": "4828599"
  },
  {
    "text": "So, you saw that, you know, commonly people sort of try powers of 10 and sees how it looks, right.",
    "start": "4828600",
    "end": "4835730"
  },
  {
    "text": "They might try, you know, 0.01, 0.001, 0.0001 and see, look at how the loss is declining and see what seems to work.",
    "start": "4835730",
    "end": "4845679"
  },
  {
    "text": "In general, you want to use the fastest learning rate that isn't making things become unstable.",
    "start": "4845680",
    "end": "4850960"
  },
  {
    "text": "Commonly, you could get better results by decreasing the learning rate as you train.",
    "start": "4850960",
    "end": "4858000"
  },
  {
    "text": "So, sometimes people just do that by hand. So, we use the term epoch for a full pass",
    "start": "4858000",
    "end": "4863190"
  },
  {
    "text": "through your training data and people might say, half the learning rate after every three epochs",
    "start": "4863190",
    "end": "4868520"
  },
  {
    "text": "as you train and that can work pretty well. You can use formulas to get per epoch tra- learning rates.",
    "start": "4868520",
    "end": "4876385"
  },
  {
    "text": "There are even fancier methods. You can look up cyclic learning rates online if you want,",
    "start": "4876385",
    "end": "4882074"
  },
  {
    "text": "which sort of actually makes the learning rates sometimes bigger and then sometimes smaller, and people have found that that can be useful for getting you out",
    "start": "4882075",
    "end": "4889460"
  },
  {
    "text": "of bad regions in interesting ways. The one other thing to know is,",
    "start": "4889460",
    "end": "4895820"
  },
  {
    "text": "if you're using one of the fancier optimizers, they still ask you for a learning rate but that learning rate is",
    "start": "4895820",
    "end": "4903159"
  },
  {
    "text": "the initial learning rate which typically the optimizer will shrink as you train.",
    "start": "4903160",
    "end": "4909790"
  },
  {
    "text": "So, commonly if you're using something like Adam, you might be starting off by saying the learning rate is 0.1,",
    "start": "4909790",
    "end": "4918739"
  },
  {
    "text": "so of a bigger number and it will be shrinking it later as the training goes along.",
    "start": "4918740",
    "end": "4923945"
  },
  {
    "text": "Okay, all done. See you next week.",
    "start": "4923945",
    "end": "4928480"
  }
]