[
  {
    "start": "0",
    "end": "102000"
  },
  {
    "start": "0",
    "end": "5360"
  },
  {
    "text": "OK. Hi, everyone. Welcome to the last lecture\nof CS330 this quarter.",
    "start": "5360",
    "end": "14750"
  },
  {
    "text": "OK, so let's get\ninto the content. So today, we're really\ngoing to talk about some of the frontiers of multi-task\nlearning and meta-learning",
    "start": "14750",
    "end": "22900"
  },
  {
    "text": "and talk about some of the\nthings that don't work very well or some of the\nthings that are really on the fringe of the\nresearch that's being done,",
    "start": "22900",
    "end": "31570"
  },
  {
    "text": "and some of the things\nthat we could potentially do to fix some of\nthese challenges. ",
    "start": "31570",
    "end": "38310"
  },
  {
    "text": "Great, there's also a\nquestion in the chat, just a logistical question about\nwhether we can use late days",
    "start": "38310",
    "end": "43910"
  },
  {
    "text": "on the final project paper. Yes, you can use late days\non your final project report. ",
    "start": "43910",
    "end": "52160"
  },
  {
    "text": "OK. So in particular, in\nterms of what we're going to talk about today. First, we're going to talk about\nmeta-learning for addressing",
    "start": "52160",
    "end": "59350"
  },
  {
    "text": "distribution shift. For being able to handle-- being able to quickly\nadapt to distribution",
    "start": "59350",
    "end": "65830"
  },
  {
    "text": "shift by capturing\nequivariances, and by adapting with unlabeled\ndata to distribution shift.",
    "start": "65830",
    "end": "73850"
  },
  {
    "text": "Then we're going to talk about\nmulti-task learning and meta-RL across very distinct\ntask distributions.",
    "start": "73850",
    "end": "81570"
  },
  {
    "text": "And what sorts of tasks\nwe might even train on. And some of the challenges\nthat arise when you do this.",
    "start": "81570",
    "end": "89020"
  },
  {
    "text": "And then we'll talk about\nsome of the open challenges and open directions\nthat we haven't yet",
    "start": "89020",
    "end": "95230"
  },
  {
    "text": "talked about in the course. ",
    "start": "95230",
    "end": "100640"
  },
  {
    "text": "OK, so first, let's talk about\naddressing distribution shift. And first, you might\nwonder, why should we",
    "start": "100640",
    "end": "107510"
  },
  {
    "text": "care about distribution\nshift in particular? And for me, the motivation\nto look at distribution shift",
    "start": "107510",
    "end": "114540"
  },
  {
    "text": "problems is that\nthere's this gap between our current paradigm in\nwhich we study machine learning",
    "start": "114540",
    "end": "120750"
  },
  {
    "text": "research, and our\ncurrent reality in which these algorithms\nare actually being deployed.",
    "start": "120750",
    "end": "125790"
  },
  {
    "text": "And for example, in at\nleast the realm of machine learning research, the way\nthat things typically work",
    "start": "125790",
    "end": "131280"
  },
  {
    "text": "is we assume that we\nhave some dataset. We can train some\nmodel on a data set,",
    "start": "131280",
    "end": "136560"
  },
  {
    "text": "and then we take that\nmodel and we evaluate it. And then we report some\nnumbers in our research paper,",
    "start": "136560",
    "end": "142080"
  },
  {
    "text": "and we're done.  However, in reality,\nthe world is constantly",
    "start": "142080",
    "end": "149050"
  },
  {
    "text": "changing from stock prices,\nto supply and demand over time, to robots exploring\ndifferent parts of the world.",
    "start": "149050",
    "end": "159300"
  },
  {
    "text": "The observations that machine\nlearning systems at any given point in time will change.",
    "start": "159300",
    "end": "164592"
  },
  {
    "text": "What they're seeing will\nchange and the way that they need to make predictions\nwill change as well.",
    "start": "164592",
    "end": "170920"
  },
  {
    "text": "Rather than just\ndeploying a static model, that we kind of-- we can\nassume a static model",
    "start": "170920",
    "end": "176500"
  },
  {
    "text": "will work as the\nworld is changing. So we need to be able to think\nabout how machine learning",
    "start": "176500",
    "end": "181870"
  },
  {
    "text": "algorithms can handle the fact\nthat the world is changing.",
    "start": "181870",
    "end": "186879"
  },
  {
    "start": "185000",
    "end": "256000"
  },
  {
    "text": "Now, you might wonder,\nwell, machine learning has been pretty successful\nin the industry. How does the industry cope\nwith this sort of change?",
    "start": "186880",
    "end": "195160"
  },
  {
    "text": "And one perspective on this\nthat I think is quite useful is from Chip Huyen\nwho actually works",
    "start": "195160",
    "end": "201850"
  },
  {
    "text": "on machine learning production. And she has said that machine\nlearning systems degrade really",
    "start": "201850",
    "end": "207790"
  },
  {
    "text": "quickly because\nof concept drift. And for online machine\nlearning systems,",
    "start": "207790",
    "end": "213220"
  },
  {
    "text": "you want to update them as\nfast as humanly possible.",
    "start": "213220",
    "end": "218530"
  },
  {
    "text": "So with this in mind,\nclearly, the way that most machine learning\ntechniques are being used",
    "start": "218530",
    "end": "224370"
  },
  {
    "text": "is not the way that many\nof these research papers intended for them to be used.",
    "start": "224370",
    "end": "230349"
  },
  {
    "text": "So what we'll talk about in the\nfirst part of this lecture is how we might use meta-learning\nto very quickly--",
    "start": "230350",
    "end": "236250"
  },
  {
    "text": "to allow models to very quickly\nadapt to distribution shifts. Rather than kind of the\ncurrent paradigm in machine",
    "start": "236250",
    "end": "243690"
  },
  {
    "text": "learning production, where you\nsimply just take your model and fine-tune it, for\nexample, on all the data",
    "start": "243690",
    "end": "249450"
  },
  {
    "text": "that you've collected so far,\nand try to update them as you get more and more data. ",
    "start": "249450",
    "end": "257440"
  },
  {
    "start": "256000",
    "end": "362000"
  },
  {
    "text": "OK. So now, with this in mind, we'll\ntalk about two different forms",
    "start": "257440",
    "end": "264450"
  },
  {
    "text": "of-- two different ways to\nhandle distribution shift nia meta-learning.",
    "start": "264450",
    "end": "269580"
  },
  {
    "text": "And the first solution\nthat we'll talk about is actually\nmotivated by the fact that if you build in\nstructure into your models",
    "start": "269580",
    "end": "275520"
  },
  {
    "text": "to solve-- if you build in\nstructure into your models they will be more robust\nto distribution shift.",
    "start": "275520",
    "end": "282990"
  },
  {
    "text": "For example, if you build in\nthe structure of convolutions into your model, then you're\neffectively telling your model",
    "start": "282990",
    "end": "290190"
  },
  {
    "text": "that it should be transitionally\nequivariant at least for that part of the function.",
    "start": "290190",
    "end": "297510"
  },
  {
    "text": "And therefore, if you then\nsee data that's translated, then you can expect\nthe predictions to be",
    "start": "297510",
    "end": "302820"
  },
  {
    "text": "also translated, for example. Or if you use things\nlike max pooling if you expect-- if you see\ninputs that are translated,",
    "start": "302820",
    "end": "308610"
  },
  {
    "text": "then the outputs won't\nchange because it'll be translation invariant.",
    "start": "308610",
    "end": "315780"
  },
  {
    "text": "And so, the solution\nto distribution shift is great when we know\nwhat the structure is and how to build it in,\nespecially if we know how",
    "start": "315780",
    "end": "322580"
  },
  {
    "text": "our distribution\nis going to change like translations\nthat we can build in things like convolutions.",
    "start": "322580",
    "end": "328985"
  },
  {
    "text": "But there are a\nlot of situations where we don't know\nhow the distribution is going to change. And we also don't know\nhow to build it in.",
    "start": "328985",
    "end": "337940"
  },
  {
    "text": "So one question that\nwe're going to think about",
    "start": "337940",
    "end": "343220"
  },
  {
    "text": "in this first part\nis can we discover equivariant and invariant\nstructure via meta-learning",
    "start": "343220",
    "end": "349190"
  },
  {
    "text": "in a way that allows us to\nbuild it into the model, and hence allow the model\nto generalize by kind",
    "start": "349190",
    "end": "356270"
  },
  {
    "text": "of leveraging the sort of\nequivariance and invariant structure?",
    "start": "356270",
    "end": "361630"
  },
  {
    "text": "OK.  Now one question you\nmight ask is, well, maybe",
    "start": "361630",
    "end": "366890"
  },
  {
    "start": "362000",
    "end": "847000"
  },
  {
    "text": "meta-learning algorithms\nalready do this? And so we could take the\nMAML algorithm, which",
    "start": "366890",
    "end": "372630"
  },
  {
    "text": "learns initialization runs a\nfew steps of gradient descent to get a new network, and\none thing you might note",
    "start": "372630",
    "end": "378980"
  },
  {
    "text": "is that MAML can learn\nequivariant initial features.",
    "start": "378980",
    "end": "384635"
  },
  {
    "text": "So this initial\nnetwork right here might exhibit features that\nhave a certain equivariant",
    "start": "384635",
    "end": "391980"
  },
  {
    "text": "structure like translation,\nlike rotational equivariance, and so forth or\nrotational invariance.",
    "start": "391980",
    "end": "398870"
  },
  {
    "text": "But the gradient update\nmay, may kind of get rid",
    "start": "398870",
    "end": "404490"
  },
  {
    "text": "of that equivariant structure. Equivariance may\nnot be preserved in that gradient update\nso even if it learned",
    "start": "404490",
    "end": "411180"
  },
  {
    "text": "initial features\nthat are equivariant, the resulting network\nmay not actually preserve that structure.",
    "start": "411180",
    "end": "416550"
  },
  {
    "text": "Do you have a question? Yes, I want to ask if there\nare any experimental add-ins",
    "start": "416550",
    "end": "423630"
  },
  {
    "text": "that MAML can learn\nequivariant initial features.",
    "start": "423630",
    "end": "429250"
  },
  {
    "text": " I don't know of any\nexperimental evidence.",
    "start": "429250",
    "end": "434690"
  },
  {
    "text": "I will show some\nexperiments in a few slides after I talk about one way\nthat we can fix this challenge,",
    "start": "434690",
    "end": "441970"
  },
  {
    "text": "and it will actually show\nthat MAML kind of struggles to capture equivariance at\nleast in the sense of this kind",
    "start": "441970",
    "end": "448660"
  },
  {
    "text": "of second network or kind of\nthe resulting network having that certain structure.",
    "start": "448660",
    "end": "454156"
  },
  {
    "text": "[INAUDIBLE] Was\nasking can you explain why the gradient update might\nnot preserve equivariance? So I guess the\nkind of what I mean",
    "start": "454157",
    "end": "463720"
  },
  {
    "text": "by it not preserving it is that\nlike the gradient update is--",
    "start": "463720",
    "end": "468880"
  },
  {
    "text": "like even if learns\na set of features. For example, this layer\nthat has a certain that",
    "start": "468880",
    "end": "475270"
  },
  {
    "text": "preserves some sort of\nlike equivariance property if you update the\nentire network that's going to update other\nparts of the features",
    "start": "475270",
    "end": "481780"
  },
  {
    "text": "and if because like\ngradients, especially gradients of small\namounts of data",
    "start": "481780",
    "end": "487840"
  },
  {
    "text": "have a lot of noise in them,\nit may be difficult for MAML to arrange its initial\nparameters in a way that allows",
    "start": "487840",
    "end": "494770"
  },
  {
    "text": "it allows this gradient update\nto essentially preserve those",
    "start": "494770",
    "end": "500889"
  },
  {
    "text": "features in such a way. I also think that the\nanswer to this question should be a little\nbit more clear",
    "start": "500890",
    "end": "506380"
  },
  {
    "text": "as we talk about a solution\nfor solving this problem. ",
    "start": "506380",
    "end": "513159"
  },
  {
    "text": "OK. So, in particular, if we\nwant to solve this problem. One thing that we\ncould try to do",
    "start": "513159",
    "end": "519428"
  },
  {
    "text": "is decompose the\nweights of this network into an equivariant\nstructure and",
    "start": "519429",
    "end": "524949"
  },
  {
    "text": "some corresponding\nparameters that are disentangled from this\nequivariant structure. And if we're able to decompose\nthe weights in this way,",
    "start": "524950",
    "end": "532960"
  },
  {
    "text": "then what we could do is\nonly update the parameters in the inner loop and\nretain the equivariance",
    "start": "532960",
    "end": "538990"
  },
  {
    "text": "and keep that fixed in this\nkind of update process. ",
    "start": "538990",
    "end": "545220"
  },
  {
    "text": "OK, so this may seem a\nbit high level and vague, so let's try to actually dig\ninto how we might do something",
    "start": "545220",
    "end": "550380"
  },
  {
    "text": "like this, and to\ndo this, we can think about how\nequivariances are represented",
    "start": "550380",
    "end": "555470"
  },
  {
    "text": "in neural networks, and\nin particular, let's just look at an example. So if you take the example\nof a 1D convolution",
    "start": "555470",
    "end": "563030"
  },
  {
    "text": "where you have some\nfilter here that is kind of slid across the\ninput image or the input x,",
    "start": "563030",
    "end": "571190"
  },
  {
    "text": "then you get an output\nthat looks like this. Now it turns out that you can\nactually represent something",
    "start": "571190",
    "end": "578820"
  },
  {
    "text": "like a 1D convolution layer\nusing a fully connected layer and what this looks like is\nsomething like this where",
    "start": "578820",
    "end": "587720"
  },
  {
    "text": "basically the\nweights of the filter are repeated in this weight\nmatrix along the diagonal",
    "start": "587720",
    "end": "594470"
  },
  {
    "text": "of the weight matrix and the\nremainder of the matrix is 0. And it's not hard to see\nthat basically this weight",
    "start": "594470",
    "end": "603530"
  },
  {
    "text": "matrix implements the same exact\nfunction as the convolution here by seeing\nthat basically when",
    "start": "603530",
    "end": "609710"
  },
  {
    "text": "you kind of do kind of take\nthe dot product of the rows--",
    "start": "609710",
    "end": "614930"
  },
  {
    "text": "each of the rows in the\nmatrix by the vector x, you get the resulting output.",
    "start": "614930",
    "end": "621589"
  },
  {
    "text": "So essentially, you can view\nthings like 1D convolution as a certain pattern in\nthis matrix that tells you",
    "start": "621590",
    "end": "628839"
  },
  {
    "text": "how you should share\nparameters within this fully connected matrix. ",
    "start": "628840",
    "end": "636310"
  },
  {
    "text": "OK, so with that in mind,\nhow might we try to decouple",
    "start": "636310",
    "end": "641650"
  },
  {
    "text": "or decompose the\nsharing pattern which covered which captures the\nequivariance and the underlying",
    "start": "641650",
    "end": "647890"
  },
  {
    "text": "parameters A, B and C? What we can do is\nwe can reparametrize this weight matrix\nW and in particular,",
    "start": "647890",
    "end": "656950"
  },
  {
    "text": "reparametrize it into first a\nvector of the underlying filter parameters A, B and C and a\nmatrix that basically just",
    "start": "656950",
    "end": "664750"
  },
  {
    "text": "tells you, for each of the\nelements in this week matrix which of these\nparameters, or what",
    "start": "664750",
    "end": "671250"
  },
  {
    "text": "combination of these parameters\ndoes that matrix value have? ",
    "start": "671250",
    "end": "678980"
  },
  {
    "text": "And so basically\nthis kind of-- if you take the matrix-vector\nmultiplication of this sharing matrix and the underlying\nfilter parameters,",
    "start": "678980",
    "end": "685780"
  },
  {
    "text": "you get a vector of\na bunch of weights, and then you can\nreshape that vector to have the matrix\nshown on the left here.",
    "start": "685780",
    "end": "692710"
  },
  {
    "text": " So we're going to call\nthis U and this V,",
    "start": "692710",
    "end": "700520"
  },
  {
    "text": "and U can capture symmetries\nby essentially capturing",
    "start": "700520",
    "end": "705740"
  },
  {
    "text": "how underlying parameters\nshould be repeated or copied in your weight matrix,\nand V can capture",
    "start": "705740",
    "end": "714340"
  },
  {
    "text": "the underlying\nshared parameters.  Now, this-- one thing\nthat you can note,",
    "start": "714340",
    "end": "722013"
  },
  {
    "text": "I guess a couple of\nthings that you can note. First this sharing matrix\ndoesn't need to be binary.",
    "start": "722013",
    "end": "728009"
  },
  {
    "text": "It can have real-valued entries,\nsuch that it's not just picking",
    "start": "728010",
    "end": "736260"
  },
  {
    "text": "one of these parameters. It can also take half of A\nand half of B, for example.",
    "start": "736260",
    "end": "742490"
  },
  {
    "text": "And then the other thing\nis that you can apply this to fully connected layers,\nbut you can also apply this",
    "start": "742490",
    "end": "748390"
  },
  {
    "text": "to things like\nconvolution layers, kind of the output of taking\nU times V is just",
    "start": "748390",
    "end": "755720"
  },
  {
    "text": "a vector of parameters,\nand vector parameters could correspond to\nsomething that you eventually reshape into a fully\nconnected weight matrix,",
    "start": "755720",
    "end": "761875"
  },
  {
    "text": "or it could be\nsomething that you reshape into like a set\nof convolution filters, for example. ",
    "start": "761875",
    "end": "769709"
  },
  {
    "text": "And interestingly,\nit turns out that for this particular\nreparametrization of the weight",
    "start": "769710",
    "end": "775470"
  },
  {
    "text": "matrix, you can show\nthat theoretically, this can directly represent a\ndecoupled equivariant sharing",
    "start": "775470",
    "end": "783269"
  },
  {
    "text": "pattern, and filter\nparameters and you can show this for all\ngroup convolutions",
    "start": "783270",
    "end": "790080"
  },
  {
    "text": "with finite groups. I don't want to get too much\ninto kind of the group theory",
    "start": "790080",
    "end": "795720"
  },
  {
    "text": "side of things, but the\nkind of gist of this is that it could represent\na number of kinds",
    "start": "795720",
    "end": "801090"
  },
  {
    "text": "of equivariances, not just\nthings like 2D convolutions and 1D convolutions. It could also represent\ncertain forms of rotational",
    "start": "801090",
    "end": "809030"
  },
  {
    "text": "equivariance, reflection\npermutation equivariance among",
    "start": "809030",
    "end": "817050"
  },
  {
    "text": "kind of a wide variety\nof equivariances, although not all equivariances,\nso there's also infinite groups",
    "start": "817050",
    "end": "824550"
  },
  {
    "text": "equivariances that\nthis cannot cover. ",
    "start": "824550",
    "end": "830259"
  },
  {
    "text": "OK. But beyond the theory,\nkind of the key idea is that we're going to\nreparametrize this matrix",
    "start": "830260",
    "end": "836140"
  },
  {
    "text": "W to have one component\nthat can capture symmetries, and one component\nthat can capture the underlying\nshared parameters.",
    "start": "836140",
    "end": "844130"
  },
  {
    "text": "Now once we have these\nreparametrized weight matrices what we can do is\nin the inner loop",
    "start": "844130",
    "end": "851530"
  },
  {
    "text": "we'll only update\nthe vector v, which are the underlying\nparameters, and keep the equivariance fixed.",
    "start": "851530",
    "end": "858420"
  },
  {
    "text": "This is such that when\nwe run gradient descent, we can kind of preserve\nthe equivariance structure",
    "start": "858420",
    "end": "863500"
  },
  {
    "text": "that weight sharing patterns\nthat are captured by U and only update the parameters.",
    "start": "863500",
    "end": "870003"
  },
  {
    "text": "And then, in the\nouter loop, we're going to learn both\nU and v, and this",
    "start": "870003",
    "end": "875440"
  },
  {
    "text": "allows us to essentially\ntry to extract or discover the equivariancies that are\npresent in a set of tasks.",
    "start": "875440",
    "end": "882640"
  },
  {
    "text": " So kind of visually\nwhat this looks like",
    "start": "882640",
    "end": "888400"
  },
  {
    "text": "is if this kind of red gradient\nis showing your inner loop",
    "start": "888400",
    "end": "893830"
  },
  {
    "text": "loss, the inner\nloop loss in MAML, then when you compute\nthis inner loop loss,",
    "start": "893830",
    "end": "899440"
  },
  {
    "text": "you're only going to apply\nit to v. This right here,",
    "start": "899440",
    "end": "906110"
  },
  {
    "text": "maybe I should have\npicked a different color. This is kind of the\npre-update where you are",
    "start": "906110",
    "end": "912400"
  },
  {
    "text": "computing the gradient update. After you've computed the\ngradient update right here,",
    "start": "912400",
    "end": "918070"
  },
  {
    "text": "you get the post-update\nmodel right here.",
    "start": "918070",
    "end": "924320"
  },
  {
    "text": "And this post-update model only\nhas the underlying parameters that have been updated, and\nthen you evaluate this model.",
    "start": "924320",
    "end": "933210"
  },
  {
    "text": "This is like-- this\nis your train set. This is your test\nset for these tasks.",
    "start": "933210",
    "end": "940084"
  },
  {
    "text": "Now one important\nassumption here is that there are\nsome symmetries that are shared by all the tasks,\nand if all the tasks share",
    "start": "940085",
    "end": "951750"
  },
  {
    "text": "some kind of fixed\nset of symmetries and the goal is\nfor this procedure to be able to extract those\nsymmetries in the matrix U. OK.",
    "start": "951750",
    "end": "963230"
  },
  {
    "text": "So we refer to this approach\nas meta-learning symmetries by reparametrization or MSR. Are there any questions\non the method before I",
    "start": "963230",
    "end": "970400"
  },
  {
    "text": "get to some of the experiments? ",
    "start": "970400",
    "end": "980980"
  },
  {
    "text": "OK. So-- yeah.",
    "start": "980980",
    "end": "985998"
  },
  {
    "text": "Why didn't we update U as well? ",
    "start": "985998",
    "end": "991352"
  },
  {
    "text": "Yeah, so we're not updating\nU in the inner loop because we want to\nbe able to preserve--",
    "start": "991352",
    "end": "996790"
  },
  {
    "text": "basically to\nextract equivariance from a set of tasks and preserve\nthat equivariance when we're",
    "start": "996790",
    "end": "1003750"
  },
  {
    "text": "trying to learn a new task\nand in particular, if we have that equivariance structure\nand that weight sharing pattern",
    "start": "1003750",
    "end": "1009060"
  },
  {
    "text": "fixed, so we're\nlearning a new task, then we should be\nable to generalize more readily from a\nsmall amount of data",
    "start": "1009060",
    "end": "1014850"
  },
  {
    "text": "because the network itself\nis kind of capturing things like rotational\nequivariance for example,",
    "start": "1014850",
    "end": "1020370"
  },
  {
    "text": "and then to be able to\ngeneralize to rotated images. OK, thank you.",
    "start": "1020370",
    "end": "1027300"
  },
  {
    "text": "Hi. So when learning the\nequivariance matrix, do we impose any\nconstraint on it,",
    "start": "1027300",
    "end": "1034380"
  },
  {
    "text": "or we just add it by itself? Because I see the example.",
    "start": "1034380",
    "end": "1039839"
  },
  {
    "text": "You sort of have some\nsort of structure that makes it preserve\nequivariance, right?",
    "start": "1039839",
    "end": "1047160"
  },
  {
    "text": "Yeah, I mean, in general,\nit makes a lot of sense, I think, to impose some sort\nof structure on the matrix,",
    "start": "1047160",
    "end": "1053640"
  },
  {
    "text": "either in a hard\nway or in a soft way through like a regularization. ",
    "start": "1053640",
    "end": "1062330"
  },
  {
    "text": "In this work, I guess\none thing that we did with U is because U\nhas quite a large matrix.",
    "start": "1062330",
    "end": "1068510"
  },
  {
    "text": "We experimented\nwith different ways to represent U, for example,\nusing a Kronecker factorization",
    "start": "1068510",
    "end": "1076580"
  },
  {
    "text": "or a low rank\nfactorization so that it has fewer overall parameters.",
    "start": "1076580",
    "end": "1081730"
  },
  {
    "text": "But it's likely that there's\nactually a lot more interesting things that you\ncould do there to try to impose certain kinds\nof regularizations on U",
    "start": "1081730",
    "end": "1088490"
  },
  {
    "text": "to encourage it to discover\nequivariances and not kind of capture other\nkinds of structure.",
    "start": "1088490",
    "end": "1094072"
  },
  {
    "text": "OK. Thank you. Yeah, and [INAUDIBLE]\nis asking, can you also talk a bit about what\nwe mean by equivariance?",
    "start": "1094072",
    "end": "1100630"
  },
  {
    "text": "And how do we measure It? So if you want to\nbe equivariant. So you have some function f of\nx that produces y then being--",
    "start": "1100630",
    "end": "1118320"
  },
  {
    "text": "maybe then you have this\nkind of other function that you want to\nbe equivariant to or that you want your\nfunction to be equivariant to,",
    "start": "1118320",
    "end": "1124800"
  },
  {
    "text": "maybe this is\nfunction g, maybe this is something that translates\nthe image, for example. ",
    "start": "1124800",
    "end": "1132640"
  },
  {
    "text": "What equivariance\nkind of means is that when you basically\napply f to something that's",
    "start": "1132640",
    "end": "1144840"
  },
  {
    "text": "been transformed by\ng, then kind of this",
    "start": "1144840",
    "end": "1150090"
  },
  {
    "text": "will give you an output\nthat corresponds to g of y.",
    "start": "1150090",
    "end": "1156120"
  },
  {
    "text": "So basically transformed\nin the same way. And then invariance\nmeans that when",
    "start": "1156120",
    "end": "1162750"
  },
  {
    "text": "you transform the\ninput in some way, the function still\noutputs the same thing. It's invariant to\nthose transformations.",
    "start": "1162750",
    "end": "1169872"
  },
  {
    "start": "1169872",
    "end": "1175283"
  },
  {
    "text": "Yeah. My question is\nregarding the structure of the use of that example\nthat is presented here.",
    "start": "1175284",
    "end": "1180840"
  },
  {
    "text": "It seems that you can be like\n[INAUDIBLE] us in nature, so can we limit that\nstructure to like maybe",
    "start": "1180840",
    "end": "1187341"
  },
  {
    "text": "minimize the parameters of U? As you said, like you\nhad like many parameters? Yeah, so encouraging\nsomething like sparsity",
    "start": "1187342",
    "end": "1193950"
  },
  {
    "text": "with like an L1 regularization\ncould make a lot of sense.",
    "start": "1193950",
    "end": "1199080"
  },
  {
    "text": "We haven't experimented\nwith that ourselves, but I think that it could make a\nlot of sense in future work",
    "start": "1199080",
    "end": "1204820"
  },
  {
    "text": "and this work is very, very\nrecent as you might have seen.",
    "start": "1204820",
    "end": "1211029"
  },
  {
    "text": "So it's kind of on the frontier\nand fringes of research.",
    "start": "1211030",
    "end": "1216897"
  },
  {
    "text": "Thank you.  OK, so on the\nexperiment side, we",
    "start": "1216897",
    "end": "1224960"
  },
  {
    "start": "1221000",
    "end": "1355000"
  },
  {
    "text": "looked at a few\ndifferent things. The first question is, can\nwe recover convolutions",
    "start": "1224960",
    "end": "1232549"
  },
  {
    "text": "from transitionally\nequivariant data? And to study this\nquestion, we set up",
    "start": "1232550",
    "end": "1238410"
  },
  {
    "text": "a fairly like a synthetic\nexperiment or experiment with synthetic data where we\njust took random convolution",
    "start": "1238410",
    "end": "1247820"
  },
  {
    "text": "functions, random 1D convolution\nfunctions and meta-learned across those sets of 1D\nconvolution functions,",
    "start": "1247820",
    "end": "1255200"
  },
  {
    "text": "where each function\ncorresponds to a different task And what we wanted to see\nis if we took this method",
    "start": "1255200",
    "end": "1262380"
  },
  {
    "text": "and used a fully connected\nlayer, a fully connected network instead of a\nconvolution network, if it could recover the\nsame performance as giving",
    "start": "1262380",
    "end": "1270090"
  },
  {
    "text": "the network the\nconvolutions itself. And so in particular, if we\nkind of take these convolution",
    "start": "1270090",
    "end": "1280848"
  },
  {
    "text": "functions and measure\nthe mean square on held out tasks, what\nwe see is that MAML",
    "start": "1280848",
    "end": "1286605"
  },
  {
    "text": "with fully connected-- MAML with a fully\nconnected network and MAML with a locally connected\nnetwork both struggle",
    "start": "1286605",
    "end": "1292500"
  },
  {
    "text": "to get good performance. And this is kind of\nwhat I was talking about before where it\nisn't able to capture",
    "start": "1292500",
    "end": "1298020"
  },
  {
    "text": "this sort of\nequivariant structure, it may be destroyed in\nthat gradient update.",
    "start": "1298020",
    "end": "1303240"
  },
  {
    "text": "Whereas MAML with a\nconvolution network can obviously do quite\nwell because the data is",
    "start": "1303240",
    "end": "1308520"
  },
  {
    "text": "translationally equivariant,\nand interestingly this approach building on top of a\nfully connected network",
    "start": "1308520",
    "end": "1315090"
  },
  {
    "text": "is also able to do\nabout as well as MAML with a convolutional\nnetwork, suggesting",
    "start": "1315090",
    "end": "1320610"
  },
  {
    "text": "that it is able to kind recover\nthe structure from the data and preserve that in\nthe gradient update.",
    "start": "1320610",
    "end": "1330420"
  },
  {
    "text": "And the other cool thing\nthat you can look at is can look at the matrix\nthat's recovered by MSR,",
    "start": "1330420",
    "end": "1337200"
  },
  {
    "text": "and you see a\nmatrix, matrix that looks like this, that\nlooks basically exactly like kind of the ABC repeated\nover the diagonal matrix,",
    "start": "1337200",
    "end": "1348360"
  },
  {
    "text": "showing that it did indeed\nrecover 1D convolutions. ",
    "start": "1348360",
    "end": "1355559"
  },
  {
    "start": "1355000",
    "end": "1515000"
  },
  {
    "text": "Now, next we can ask, can\nwe recover something better than convolutions in situations\nwhere our data isn't perfectly",
    "start": "1355560",
    "end": "1362520"
  },
  {
    "text": "translationally equivariant? So if we have data with\npartial translation symmetry,",
    "start": "1362520",
    "end": "1367809"
  },
  {
    "text": "for example, that's when--",
    "start": "1367810",
    "end": "1373000"
  },
  {
    "text": "that's shown in the results here\nwhen k equals 2 and k equals 5, and in particular what\nI mean by k here is,",
    "start": "1373000",
    "end": "1378654"
  },
  {
    "text": "k is the rank of a\nlocally connected layer. And if the rank\nis 1, then there's just one filter that's copied\nfor all of the kind of locally",
    "start": "1378655",
    "end": "1388507"
  },
  {
    "text": "connected parts of the network. If k is 2 that means there are\ntwo filters, and if k is 5, that means there\nare five filters.",
    "start": "1388508",
    "end": "1394970"
  },
  {
    "text": "And what we see here is\nthat MAML with convolutions isn't able to capture\nthis well, because it's",
    "start": "1394970",
    "end": "1402470"
  },
  {
    "text": "trying to encode too\nstrong of a symmetry. It's trying to encode\nfull translation symmetry, whereas MSR is able to\ndo well across the board,",
    "start": "1402470",
    "end": "1409520"
  },
  {
    "text": "because it's able to\nrecover the symmetries that are present in the data.",
    "start": "1409520",
    "end": "1415760"
  },
  {
    "text": "You could also look at data\nwith other forms of symmetries like rotation or reflection,\nand in this case,",
    "start": "1415760",
    "end": "1421230"
  },
  {
    "text": "we also see that MSR is\nable to effectively extract that same image symmetry to\ndo a lot better than MAML",
    "start": "1421230",
    "end": "1427130"
  },
  {
    "text": "just with convolutions\nby actually extracting the rotation and the\nreflection symmetry.",
    "start": "1427130",
    "end": "1433929"
  },
  {
    "text": "Do you have a question? Yes. So is these experiments done\nin the few-shot regiment or?",
    "start": "1433930",
    "end": "1442510"
  },
  {
    "text": "Yeah. These are all done in\nthe few-shot regime. And this is showing performance,\nmean squared error performance",
    "start": "1442510",
    "end": "1449610"
  },
  {
    "text": "on held out tasks. So if you had\nenough data, do you expect the MAML\nwhen fully connected",
    "start": "1449610",
    "end": "1457559"
  },
  {
    "text": "will be able to learn this task? Yeah, so if you have enough\ndata of the held-out test task,",
    "start": "1457560",
    "end": "1466049"
  },
  {
    "text": "then we would expect\nall of these methods to be able to do well, because\nthere is basically enough data",
    "start": "1466050",
    "end": "1475269"
  },
  {
    "text": "to learn that structure from\nscratch on the test task, you would probably need\nconsiderably more data",
    "start": "1475270",
    "end": "1481450"
  },
  {
    "text": "than the few-shot\nregime, because basically",
    "start": "1481450",
    "end": "1486820"
  },
  {
    "text": "a convolutional layer\nhas significantly fewer underlying parameters\nthan an entire fully connected",
    "start": "1486820",
    "end": "1492070"
  },
  {
    "text": "matrix. Thanks. ",
    "start": "1492070",
    "end": "1498582"
  },
  {
    "text": "Cool, that's a good question.  OK. And then I guess the one thing\nthat I didn't mention here",
    "start": "1498582",
    "end": "1505350"
  },
  {
    "text": "is in this last experiment. We actually applied MSR on top\nof a convolutional function",
    "start": "1505350",
    "end": "1510877"
  },
  {
    "text": "or a convolutional layer.  OK. And then lastly, also\nmention an experiment",
    "start": "1510878",
    "end": "1519270"
  },
  {
    "start": "1515000",
    "end": "1643000"
  },
  {
    "text": "where instead of trying\nto recover symmetries from the synthetic data, we're\nlooking at learning symmetries",
    "start": "1519270",
    "end": "1526860"
  },
  {
    "text": "from augmented data,\nand in particular, you can run a Meta-training\nprocess that looks something",
    "start": "1526860",
    "end": "1533160"
  },
  {
    "text": "like this, that's\nbasically the same as your standard\nmeta-training process.",
    "start": "1533160",
    "end": "1539940"
  },
  {
    "text": "But instead of just passing\nthe train and val set to each task as normal, you\naugment the validation set,",
    "start": "1539940",
    "end": "1548600"
  },
  {
    "text": "and by augmentation\nhere, I mean things like, random crops, random\nscaling, lighting variation,",
    "start": "1548600",
    "end": "1554150"
  },
  {
    "text": "and so forth. And then essentially\ntrain the model to be able to generalize to\nthis sort of augmented data",
    "start": "1554150",
    "end": "1560990"
  },
  {
    "text": "from an unaugmented\ntraining set. So you obviously change\nyour train test splits",
    "start": "1560990",
    "end": "1567809"
  },
  {
    "text": "to look to have a normal train\nsplit and an augmented val split, and then run your\nstandard meta-training",
    "start": "1567810",
    "end": "1575070"
  },
  {
    "text": "algorithm with this\nsplit, and essentially, what this is trying\nto accomplish",
    "start": "1575070",
    "end": "1580160"
  },
  {
    "text": "is baking data augmentation\ninto the architecture or update rule of your meta-learner.",
    "start": "1580160",
    "end": "1585424"
  },
  {
    "text": " Cool, and then what we\nfind is that if you first",
    "start": "1585425",
    "end": "1593700"
  },
  {
    "text": "compare MSR with\nMAML, you get kind of performance gains\nacross the board",
    "start": "1593700",
    "end": "1599669"
  },
  {
    "text": "by nature of being\nable to capture these equivariances\nfrom the augmentation into the training process.",
    "start": "1599670",
    "end": "1608129"
  },
  {
    "text": "It also outperforms\nAlmost No Inner Loop, which just corresponds to\nMAML just on adapting the last",
    "start": "1608130",
    "end": "1614519"
  },
  {
    "text": "layer, which is nice because if\nyou only adapt the last layer, then that actually allows MAML\nto capture equivariant features",
    "start": "1614520",
    "end": "1621900"
  },
  {
    "text": "but only update the features\n-- the weights on top of that.",
    "start": "1621900",
    "end": "1627680"
  },
  {
    "text": "And it's also able to\nperform slightly better or comparably to prototypical\nnetworks in the Omniglot",
    "start": "1627680",
    "end": "1633860"
  },
  {
    "text": "setting but significantly\nbetter on miniImageNet where things are a\nbit more challenging.",
    "start": "1633860",
    "end": "1638964"
  },
  {
    "text": " OK. So kind of one takeaway\nthat MSR provides",
    "start": "1638964",
    "end": "1646220"
  },
  {
    "text": "is that it really gives a\nframework for understanding the interplay of features and\nstructure and meta-learning,",
    "start": "1646220",
    "end": "1652309"
  },
  {
    "text": "and allows the network to\nfix the structure when you're",
    "start": "1652310",
    "end": "1657560"
  },
  {
    "text": "trying to learn, and\nonly learn features that are specific to the task.",
    "start": "1657560",
    "end": "1663890"
  },
  {
    "text": "And this idea of\nfixing some things and training other things\nmay provide kind of other--",
    "start": "1663890",
    "end": "1673340"
  },
  {
    "text": "may allow us to kind\nof learn and capture things other than\nequivariances for example. ",
    "start": "1673340",
    "end": "1680380"
  },
  {
    "text": "OK. So capturing\nequivariances is one way that may allow you\nto generalize better",
    "start": "1680380",
    "end": "1686090"
  },
  {
    "text": "from a small amount of data. Now I'm going to talk a bit\nabout adapting to distribution",
    "start": "1686090",
    "end": "1691850"
  },
  {
    "text": "shift in the setting where\nyou don't have any label data. ",
    "start": "1691850",
    "end": "1698390"
  },
  {
    "text": "And so, to start\noff this part, it's useful to think about\nwhat sort of distribution",
    "start": "1698390",
    "end": "1704890"
  },
  {
    "start": "1699000",
    "end": "1998000"
  },
  {
    "text": "shift we're going\nto be adapting to. So in the previous\nsection, we were adapting to distribution\nshift, where",
    "start": "1704890",
    "end": "1713140"
  },
  {
    "text": "the things that we're\nkind of changing were kind of translations,\nor rotations, or other sorts of transformations on the data.",
    "start": "1713140",
    "end": "1720460"
  },
  {
    "text": "In this work, or in this\nkind of part of the lecture we'll talk about a kind\nof distribution shift",
    "start": "1720460",
    "end": "1726169"
  },
  {
    "text": "that's a bit more broad than\nthose kinds of transformations.",
    "start": "1726170",
    "end": "1731255"
  },
  {
    "text": "And in particular,\nwe're going to focus on what's called group shift. We're going to assume that\nthere's a categorical group",
    "start": "1731255",
    "end": "1738410"
  },
  {
    "text": "variable z. This may correspond to different\nusers or different locations,",
    "start": "1738410",
    "end": "1744440"
  },
  {
    "text": "or different times of day. And this sort of\ninformation can typically",
    "start": "1744440",
    "end": "1749780"
  },
  {
    "text": "be derived from meta-data\nin your data set. And then, we're going to assume\nthat the training data comes",
    "start": "1749780",
    "end": "1757040"
  },
  {
    "text": "from this distribution\nwhere there's this distribution over\nthe underlying group, and then your data\ndistribution actually depends",
    "start": "1757040",
    "end": "1764780"
  },
  {
    "text": "on the underlying group, and\nthat your test data corresponds to this distribution where p\nof x, y given z is identical,",
    "start": "1764780",
    "end": "1773360"
  },
  {
    "text": "but the distribution over\nthis underlying group variable has changed.",
    "start": "1773360",
    "end": "1779210"
  },
  {
    "text": "And this can\ncapture label shift. It can also capture most forms\nof covariate shift as well. ",
    "start": "1779210",
    "end": "1787690"
  },
  {
    "text": "OK, and then our\ngoal will be to try to be able to adapt to this\nsort of group shift with only",
    "start": "1787690",
    "end": "1794460"
  },
  {
    "text": "unlabeled data with\nbasically only x, and without having any access\nto y from this part of the test",
    "start": "1794460",
    "end": "1802500"
  },
  {
    "text": "data.  Yeah, this sort of thing\nalso captures problems",
    "start": "1802500",
    "end": "1808799"
  },
  {
    "text": "like federated\nlearning where you want to adapt to different\nusers or different devices.",
    "start": "1808800",
    "end": "1814990"
  },
  {
    "text": "Now, it's worth\nunderstanding what sort of approaches have been\nused for a distribution shift in the past, and one very\npopular approach for handling",
    "start": "1814990",
    "end": "1823900"
  },
  {
    "text": "group shift is called\ngroup distributionally robust optimization. The way that this class\nof approaches works",
    "start": "1823900",
    "end": "1830680"
  },
  {
    "text": "is that you form an\nadversarial distribution q of z over your group variable.",
    "start": "1830680",
    "end": "1836850"
  },
  {
    "text": "And then basically, optimize\nfor the worst-case group during training.",
    "start": "1836850",
    "end": "1843870"
  },
  {
    "text": "And so you're essentially\ntrying to prepare for the worst kind of group that you're\ngoing to see at test time",
    "start": "1843870",
    "end": "1849419"
  },
  {
    "text": "such that when\nthings shift, you'll still be able to perform well.",
    "start": "1849420",
    "end": "1855630"
  },
  {
    "text": "And so basically you form\nthis adversarial distribution, and you're minimizing the\nloss over this worst-case z.",
    "start": "1855630",
    "end": "1862920"
  },
  {
    "text": " The benefit of group DRO is that\nit can enable robust solutions.",
    "start": "1862920",
    "end": "1870070"
  },
  {
    "text": "It's also less pessimistic than\nadversarial robustness, where you're trying to consider\nadversarial robustness",
    "start": "1870070",
    "end": "1876010"
  },
  {
    "text": "is considering really a\nmuch broader set of shifts where it's not just shifts in\nthis underlying group variable.",
    "start": "1876010",
    "end": "1884650"
  },
  {
    "text": "The downside of group\nDRO is that it often sacrifices average performance\nby in the process of trying",
    "start": "1884650",
    "end": "1892280"
  },
  {
    "text": "to be robust, it\ntends to be somewhat pessimistic about what's\ngoing to happen in the future.",
    "start": "1892280",
    "end": "1897419"
  },
  {
    "text": "And so what we're\ngoing to try to do here is to adapt a distribution\nshift or to be able to handle\ndistribution shift we're",
    "start": "1897420",
    "end": "1903680"
  },
  {
    "text": "going to try to use\nmeta-learning in a way that allows us to adapt\nto distribution shift rather than simply trying to be\nrobust to distribution shift.",
    "start": "1903680",
    "end": "1911270"
  },
  {
    "text": " OK. And so in particular, what's\ngoing to happen at test",
    "start": "1911270",
    "end": "1917580"
  },
  {
    "text": "time is you assume that you have\nunlabeled data from your test sub-distribution, for\nexample like a new user,",
    "start": "1917580",
    "end": "1923250"
  },
  {
    "text": "a different type of\nday, a new place. One example that you\ncan consider here",
    "start": "1923250",
    "end": "1929160"
  },
  {
    "text": "is a handwriting\nrecognition scenario. Where you're given data from a\nnew user of kind of handwriting",
    "start": "1929160",
    "end": "1935550"
  },
  {
    "text": "data of just the\ninputs, and you want to be able to use these\ninputs to better recognize",
    "start": "1935550",
    "end": "1941340"
  },
  {
    "text": "the handwriting of this user. So you take this unlabeled\ndata, you adapt your model",
    "start": "1941340",
    "end": "1946710"
  },
  {
    "text": "using this unlabeled\ndata, and also infer the labels for all of\nthe data points simultaneously.",
    "start": "1946710",
    "end": "1955520"
  },
  {
    "text": "And one of the assumptions\nis that this is introducing is that the test\ninputs from one group",
    "start": "1955520",
    "end": "1961070"
  },
  {
    "text": "are all available in\na batch at test time, or at least in a\nstreaming setting",
    "start": "1961070",
    "end": "1966080"
  },
  {
    "text": "where you're seeing\nthings like one at a time all from a single group, and\nyou can kind of store the data that you've seen so\nfar from that group",
    "start": "1966080",
    "end": "1972500"
  },
  {
    "text": "to allow you to adapt\nto that on the fly. ",
    "start": "1972500",
    "end": "1977820"
  },
  {
    "text": "OK. Now one way that this\nis different from things like fine-tuning\nand meta-learning is that this data is\ncompletely unlabeled.",
    "start": "1977820",
    "end": "1985260"
  },
  {
    "text": "You're only adapting\nto unlabeled data. You don't get any labels from\nthis part of-- from this task",
    "start": "1985260",
    "end": "1991110"
  },
  {
    "text": "or from this part\nof the distribution.  OK, so we'll refer to this as\nadaptive risk minimization,",
    "start": "1991110",
    "end": "1999210"
  },
  {
    "start": "1998000",
    "end": "2241000"
  },
  {
    "text": "and the way that it works\nis you basically just construct different\nsub-distributions of your training data.",
    "start": "1999210",
    "end": "2004430"
  },
  {
    "text": "This will correspond\nto different tasks, and then you train for fast\nadaptation with unlabeled data",
    "start": "2004430",
    "end": "2009980"
  },
  {
    "text": "to these different tasks\nor to these different sub-distributions, where\ndifferent sub-distributions correspond to different users,\ndifferent times of the day,",
    "start": "2009980",
    "end": "2017430"
  },
  {
    "text": "and so forth.  Now you might ask,\nwell, how do we actually",
    "start": "2017430",
    "end": "2023120"
  },
  {
    "text": "go about adopting\nwith unlabeled data? A lot of the\nalgorithms that we've talked about in the course\nmay not make that easy.",
    "start": "2023120",
    "end": "2030950"
  },
  {
    "text": "So there's two things\nthat we could do. One thing that we could\ndo is to use MAML.",
    "start": "2030950",
    "end": "2036170"
  },
  {
    "text": "But instead of\nusing kind of MAML with a standard loss function\nin the inner loop, which we can't do because we\ndon't have labels to compute",
    "start": "2036170",
    "end": "2043520"
  },
  {
    "text": "that loss function, we're\ngoing to actually learn a loss function such that when\nwe adapt with that loss",
    "start": "2043520",
    "end": "2049310"
  },
  {
    "text": "functional on this\nunlabeled data, our parameters can\neffectively solve the task",
    "start": "2049310",
    "end": "2055879"
  },
  {
    "text": "or solve the group. And so what this\nlooks like is we have some initial\nparameters, some loss",
    "start": "2055880",
    "end": "2061899"
  },
  {
    "text": "function network parameters,\nand our examples. We run gradient descent\non our learned loss",
    "start": "2061900",
    "end": "2068919"
  },
  {
    "text": "function, where this\nloss function has some parameters phi.",
    "start": "2068920",
    "end": "2073949"
  },
  {
    "text": "And then the result\nof gradient descent as we get a model with new\nparameters, theta prime,",
    "start": "2073949",
    "end": "2079230"
  },
  {
    "text": "and we run our examples through\nthose updated parameters.",
    "start": "2079230",
    "end": "2085158"
  },
  {
    "text": "And then another way we could\nadapt with unlabeled data is something more like\na black box approach where we take some\nneural network,",
    "start": "2085159",
    "end": "2092239"
  },
  {
    "text": "this neural network takes as\ninput the unlabeled examples, and this is fed into, this\ngives you some context,",
    "start": "2092239",
    "end": "2100430"
  },
  {
    "text": "and you could essentially\nview this as kind of this is D train for the test.",
    "start": "2100430",
    "end": "2108980"
  },
  {
    "text": "This is x test, and this is just\nbasically a neural network that takes as input the training\ndata and the test input",
    "start": "2108980",
    "end": "2115700"
  },
  {
    "text": "just like a black box approach. The main difference\nis that D train only consists of\nunlabeled examples",
    "start": "2115700",
    "end": "2123410"
  },
  {
    "text": "and doesn't have any labels.  Yeah.",
    "start": "2123410",
    "end": "2128630"
  },
  {
    "text": "So [INAUDIBLE] is asking can you\nexplain what is unlabeled here? And what is labeled? So these examples\nhere are unlabeled,",
    "start": "2128630",
    "end": "2136460"
  },
  {
    "text": "so like the examples that\nyou get from a new user are unlabeled, and then you're\nusing these unlabeled examples",
    "start": "2136460",
    "end": "2141650"
  },
  {
    "text": "to adapt to that users,\nthe way that they write. And then you're also--",
    "start": "2141650",
    "end": "2148218"
  },
  {
    "text": "you pick one of these\nexamples in your train set, this is also fed\nin here and you're predicting the\ncorresponding label,",
    "start": "2148218",
    "end": "2154509"
  },
  {
    "text": "you assume that you have labels\nfor training this meta-learner at kind of during\nmeta-training but you",
    "start": "2154510",
    "end": "2162470"
  },
  {
    "text": "don't have those labels to\nadapt at meta-test time. ",
    "start": "2162470",
    "end": "2168208"
  },
  {
    "text": "Does that answer your question? ",
    "start": "2168208",
    "end": "2177930"
  },
  {
    "text": "What if our parameters\n[INAUDIBLE] learning a loss function. Is it parameterized?",
    "start": "2177930",
    "end": "2183569"
  },
  {
    "text": "Yeah. So the loss function\nhas parameters itself, and this is\nessentially something",
    "start": "2183570",
    "end": "2188809"
  },
  {
    "text": "that takes as input the\nprediction from theta",
    "start": "2188810",
    "end": "2194650"
  },
  {
    "text": "and kind of outputs\na scalar value. And then you take the\ngradient of that scalar value and back propagate that into\nthe parameters of the network",
    "start": "2194650",
    "end": "2202220"
  },
  {
    "text": "theta. ",
    "start": "2202220",
    "end": "2209910"
  },
  {
    "text": "OK. Cool. And then in the\nsimplest setting, this context C in this network\ncan actually just correspond",
    "start": "2209910",
    "end": "2219710"
  },
  {
    "text": "batch norm statistics where\nyou're basically just computing batch norm statistics using--",
    "start": "2219710",
    "end": "2225430"
  },
  {
    "text": "when you pass in these unlabeled\nexamples into your network. And this kind of\nessentially provides a way",
    "start": "2225430",
    "end": "2231970"
  },
  {
    "text": "to adapt the network\nto this unlabeled data or to essentially condition\nthe network on that data.",
    "start": "2231970",
    "end": "2237520"
  },
  {
    "text": " OK. So now, how does this kind of\nmeta-learning with unlabeled",
    "start": "2237520",
    "end": "2243910"
  },
  {
    "start": "2241000",
    "end": "2710000"
  },
  {
    "text": "adaptation actually look like? Experimentally we compare it\nto empirical risk minimization",
    "start": "2243910",
    "end": "2250540"
  },
  {
    "text": "or just like standard training. We also compared to the group\ndistributional robustness",
    "start": "2250540",
    "end": "2255640"
  },
  {
    "text": "that I mentioned before,\nand then lastly, we can also compare to empirical\nrisk minimization",
    "start": "2255640",
    "end": "2261310"
  },
  {
    "text": "but upweight the groups to\nthe uniform distribution, and this allows you\nto essentially--",
    "start": "2261310",
    "end": "2268060"
  },
  {
    "text": "if some of the groups\nare underrepresented then upweighting them to the\nuniform distribution will allow",
    "start": "2268060",
    "end": "2273160"
  },
  {
    "text": "you to kind of do\nbetter on groups that are underrepresented.",
    "start": "2273160",
    "end": "2278860"
  },
  {
    "text": "Then we'll also evaluate\nARM, and we'll specifically look at three different\nversions of ARM.",
    "start": "2278860",
    "end": "2285370"
  },
  {
    "text": "One is adapting with\nthe context variable. This is somewhat similar to\nconditional neural processes,",
    "start": "2285370",
    "end": "2291280"
  },
  {
    "text": "which is just a certain\nblack-box architecture. One is adapting using the batch\nnorm statistics, which is also",
    "start": "2291280",
    "end": "2297790"
  },
  {
    "text": "kind of an example of a\nblack-box architecture but is the only kind\nof-- the way that",
    "start": "2297790",
    "end": "2303220"
  },
  {
    "text": "is conditioning on the data\nis through the batch norm statistics, and then this\ncorresponds to kind of MAML",
    "start": "2303220",
    "end": "2309601"
  },
  {
    "text": "with a learned loss function.  OK.",
    "start": "2309602",
    "end": "2315119"
  },
  {
    "text": "So we can look at-- one setting\nis the Federated Extended MNIST Dataset and what this\ncorresponds to is you want",
    "start": "2315120",
    "end": "2322260"
  },
  {
    "text": "to be able to predict the-- be able to recognize\nhandwriting of different users",
    "start": "2322260",
    "end": "2328800"
  },
  {
    "text": "and by extended MNIST\nit actually contains not just numbers but also\nlike the English alphabet.",
    "start": "2328800",
    "end": "2336089"
  },
  {
    "text": " OK.",
    "start": "2336090",
    "end": "2341135"
  },
  {
    "text": "And then we'll actually\nalso compare it to kind of a federated learning,\na recent federated learning method here as well.",
    "start": "2341135",
    "end": "2348020"
  },
  {
    "text": "And what we see is the-- first you kind of see a 5%\nimprovement in average accuracy",
    "start": "2348020",
    "end": "2357020"
  },
  {
    "text": "from being able to adapt\nwith unlabeled data, like kind of you take\nthe best one of these,",
    "start": "2357020",
    "end": "2362660"
  },
  {
    "text": "this is around 80%,\nthis is around 85%. And then what WC means\nis that we kind of see",
    "start": "2362660",
    "end": "2369010"
  },
  {
    "text": "a 10% improvement in\nworst-case accuracy. And in particular, what worst\ncase accuracy is looking at,",
    "start": "2369010",
    "end": "2375388"
  },
  {
    "text": "is it's looking at your accuracy\non the user in which you did the worst. And this is a useful metric\nbecause you want your users--",
    "start": "2375388",
    "end": "2384790"
  },
  {
    "text": "you generally want your users\nto have a similar experience. And if you have some\nusers that you're doing terribly on,\nthen they probably--",
    "start": "2384790",
    "end": "2391810"
  },
  {
    "text": "they might quit the\nplatform or whatever or they might not be\nvery happy, or it's",
    "start": "2391810",
    "end": "2396904"
  },
  {
    "text": "kind of unfair to that user\nbecause the model isn't doing very well, and this\nworst-case metric gives you a sense of kind of how fair\nthings are for the users,",
    "start": "2396905",
    "end": "2404770"
  },
  {
    "text": "and how well you're able to\ntreat your kind of minority users and so forth?",
    "start": "2404770",
    "end": "2411110"
  },
  {
    "text": "And so this 10% improvement\nis pretty substantial. Yeah.",
    "start": "2411110",
    "end": "2416529"
  },
  {
    "text": "It's asking is BN learned\nor just calculated from the unlabeled data?",
    "start": "2416530",
    "end": "2422650"
  },
  {
    "text": "So this is 6r just calculated\nfrom the unlabeled data. The critical thing, though,\nis that the thing that",
    "start": "2422650",
    "end": "2429012"
  },
  {
    "text": "makes this different from just\nlike standard neural network training with\nbatch normalization is that you're actually training\nit such that like when it gets",
    "start": "2429012",
    "end": "2438160"
  },
  {
    "text": "a batch of data from\na certain group, those batch statistics\nallow it to adapt",
    "start": "2438160",
    "end": "2446289"
  },
  {
    "text": "to that part of\nthe distribution. And so essentially,\nwhat we're doing here is when you sample\nbatches during training,",
    "start": "2446290",
    "end": "2452740"
  },
  {
    "text": "you're actually\nsampling the batches to be correlated from a certain\npart of the distribution. And then you're also\nassuming that at test time,",
    "start": "2452740",
    "end": "2459790"
  },
  {
    "text": "you're given a user,\nand you're given a batch your data from a single user\nrather than a batch of data",
    "start": "2459790",
    "end": "2465670"
  },
  {
    "text": "from like all users.",
    "start": "2465670",
    "end": "2473970"
  },
  {
    "text": "It seems like it\nperforms as well as learn CML or learn loss.",
    "start": "2473970",
    "end": "2479650"
  },
  {
    "text": "So you're kind of still\nmeta-learning it such that when these batch norms or statistics\nare computed across the--",
    "start": "2479650",
    "end": "2487060"
  },
  {
    "text": "computed across the\nexamples it does well, so you still kind of have\nthis meta-training process.",
    "start": "2487060",
    "end": "2492320"
  },
  {
    "text": "I guess the big thing that's\ndifferent is that, like in CML, it's actually learning what\nthis context should be,",
    "start": "2492320",
    "end": "2499390"
  },
  {
    "text": "whereas in with batch\nnorm adaptation, that context just corresponds\nto an average sort",
    "start": "2499390",
    "end": "2505150"
  },
  {
    "text": "of an average set of\nactivations and the variance of those activations.",
    "start": "2505150",
    "end": "2511390"
  },
  {
    "text": "By manipulating this network it\ncan be actually kind of choose what it wants its\nactivations to be such that those statistics are\ninformative of what part",
    "start": "2511390",
    "end": "2518413"
  },
  {
    "text": "of the distribution it's in.  Yeah. And is asking why do we\nneed CML and learn loss,",
    "start": "2518413",
    "end": "2526350"
  },
  {
    "text": "which is more complicated? I guess I think there may\nbe some settings where",
    "start": "2526350",
    "end": "2533100"
  },
  {
    "text": "CML performs better in--",
    "start": "2533100",
    "end": "2538650"
  },
  {
    "text": "I guess there were like at\nleast one example in the paper",
    "start": "2538650",
    "end": "2543829"
  },
  {
    "text": "where BN was not the\nbest performing method but in general, it does seem\nlike this kind of approach",
    "start": "2543830",
    "end": "2549800"
  },
  {
    "text": "is the simplest and\nalso works quite well. So it's probably what\nI would recommend. ",
    "start": "2549800",
    "end": "2558320"
  },
  {
    "text": "OK. And then, we can also look\nat a qualitative example. So if you were given\nthis image right here,",
    "start": "2558320",
    "end": "2565000"
  },
  {
    "text": "it's ambiguous whether\nthis is like a 2 or an A,",
    "start": "2565000",
    "end": "2572040"
  },
  {
    "text": "and also actually, in\nthis data set in general, numbers are kind\nof over-represented",
    "start": "2572040",
    "end": "2578040"
  },
  {
    "text": "compared to characters. And if you just trained\nERM on the entire data set,",
    "start": "2578040",
    "end": "2584370"
  },
  {
    "text": "it tells you that\nit thinks it's a 2. If you give the model just these\ntwo examples from that test",
    "start": "2584370",
    "end": "2592800"
  },
  {
    "text": "user, then you think\nthat it's kind of also a 2 because this doesn't\ntell you that much.",
    "start": "2592800",
    "end": "2598670"
  },
  {
    "text": "But if you give the meta-learner\nkind of all of these examples, then you could actually see\nexamples like this for example,",
    "start": "2598670",
    "end": "2606460"
  },
  {
    "text": "suggest that the user writes\n2s like this, or like this,",
    "start": "2606460",
    "end": "2611890"
  },
  {
    "text": "for example and not\nlike this and that allows you to actually\nto adapt your classifier",
    "start": "2611890",
    "end": "2617940"
  },
  {
    "text": "to this particular user, and by\nkind of adapting with these 50 examples you can successfully\nclassify this is an A.",
    "start": "2617940",
    "end": "2629080"
  },
  {
    "text": "OK.  And then kind of one\nmore experiment that's",
    "start": "2629080",
    "end": "2634210"
  },
  {
    "text": "a little bit more\nkind of larger-scale is you can look at CIFAR-C\nand TinylmageNet-C.",
    "start": "2634210",
    "end": "2639580"
  },
  {
    "text": "These correspond to basically\na variety of different kinds of corruptions to the CIFAR\ndata set and the TinyImageNet",
    "start": "2639580",
    "end": "2645940"
  },
  {
    "text": "dataset and the goal here is to\nadapt to new image corruptions after kind of training on\ndifferent other forms of image",
    "start": "2645940",
    "end": "2654250"
  },
  {
    "text": "corruptions. So we train on 56\ncorruptions and then test on 22 disjoint\ncorruptions that",
    "start": "2654250",
    "end": "2661510"
  },
  {
    "text": "weren't seen during\ntraining, and what we see is that we see a 3\nto 10% in average accuracy.",
    "start": "2661510",
    "end": "2668850"
  },
  {
    "text": "So about 3% improvement\non TinylmageNet-C and then about a 10% improvement\non CIFAR-10C",
    "start": "2668850",
    "end": "2676660"
  },
  {
    "text": "and then in\nworst-case accuracy we actually see a much larger\nimprovement, we see an 8 to 21%",
    "start": "2676660",
    "end": "2681850"
  },
  {
    "text": "improvement there, which\nis pretty substantial. ",
    "start": "2681850",
    "end": "2687630"
  },
  {
    "text": "Is asking what loss does\nCML learn in this case? So the black-box methods\naren't learning any loss.",
    "start": "2687630",
    "end": "2696180"
  },
  {
    "text": "They are kind of\nadapting by just feeding the data into the network. ",
    "start": "2696180",
    "end": "2707559"
  },
  {
    "text": "OK. So for the takeaways from\nthis part of the lecture are",
    "start": "2707560",
    "end": "2712990"
  },
  {
    "start": "2710000",
    "end": "2759000"
  },
  {
    "text": "the first kind of preliminary\nevidence that meta-learning can capture equivariancies by\nreparametrizing the weight",
    "start": "2712990",
    "end": "2718930"
  },
  {
    "text": "matrices and second\nby allowing-- we could allow adaptation\nand fine-tuning",
    "start": "2718930",
    "end": "2725110"
  },
  {
    "text": "without any label target\ndata by actually training for this sort of adaptive risk.",
    "start": "2725110",
    "end": "2732740"
  },
  {
    "text": "So these are the two\npromising approaches for addressing\ndistribution shift and being able to very quickly\nadapt when the distribution is",
    "start": "2732740",
    "end": "2739640"
  },
  {
    "text": "changing.  OK.",
    "start": "2739640",
    "end": "2746650"
  },
  {
    "text": "Cool, and then the second part\nof what I'd like to talk about is, what does it take to\nrun multi-task and meta-RL",
    "start": "2746650",
    "end": "2752890"
  },
  {
    "text": "across distinct tasks?  So first, when thinking about\nreally like distinct task",
    "start": "2752890",
    "end": "2761780"
  },
  {
    "start": "2759000",
    "end": "2917000"
  },
  {
    "text": "distributions, one thing that I\nthink is useful to think about is like have we accomplished\nour goal of making",
    "start": "2761780",
    "end": "2767960"
  },
  {
    "text": "policy adaptation, for example,\nvery fast and effective? Can we now, with\nthe meta-learning",
    "start": "2767960",
    "end": "2773119"
  },
  {
    "text": "techniques that we've\ntalked about in class can we now enable a robot, for\nexample, to learn a completely",
    "start": "2773120",
    "end": "2778250"
  },
  {
    "text": "new task really\nquickly after learning a wide variety of\nprevious tasks?",
    "start": "2778250",
    "end": "2783690"
  },
  {
    "text": "And I would argue that a lot\nof the examples that we've seen so far in class are allow\nthe policy to adapt quickly",
    "start": "2783690",
    "end": "2790920"
  },
  {
    "text": "to small variations on\nwhat it's seen before but not really kind\nof allowing policies",
    "start": "2790920",
    "end": "2797430"
  },
  {
    "text": "to adapt to completely new tasks\nthat we haven't seen before.",
    "start": "2797430",
    "end": "2802839"
  },
  {
    "text": "And if we wanted to adapt\nto entirely new tasks, one of the challenges is that\nwe need our meta-training task",
    "start": "2802840",
    "end": "2809849"
  },
  {
    "text": "distribution and our\nmeta-testing task distribution to be like roughly the same.",
    "start": "2809850",
    "end": "2815495"
  },
  {
    "text": "If we need our\ntask distributions to be roughly the same. And we want to adapt to entirely\ndistinctly qualitatively",
    "start": "2815495",
    "end": "2821160"
  },
  {
    "text": "new tasks that we're going to be\ngoing to need a very broad task distribution such that we kind\nof train on this broad task",
    "start": "2821160",
    "end": "2830370"
  },
  {
    "text": "distribution\nsomething, something looks like a distinctly new\ntask still kind of generally",
    "start": "2830370",
    "end": "2835560"
  },
  {
    "text": "falls under that broad\ntask distribution. So you need a broad distribution\nof tasks for media training.",
    "start": "2835560",
    "end": "2841840"
  },
  {
    "text": "We also need this distribution\nto not be completely sparse such that we can at least\nexpect generalization",
    "start": "2841840",
    "end": "2847410"
  },
  {
    "text": "within this broader\ndistribution. If you think about it in\nthe context of Meta-RL,",
    "start": "2847410",
    "end": "2856600"
  },
  {
    "text": "where might we get kind of a\nset of a broad distribution of tasks? There are a few options.",
    "start": "2856600",
    "end": "2862580"
  },
  {
    "text": "So there's things\nlike Open Al Gym, which have different\ncontinuous control problems.",
    "start": "2862580",
    "end": "2869640"
  },
  {
    "text": "There's also the Atari\nlearning environment, and there's this kind of surreal\nbenchmark of different robot",
    "start": "2869640",
    "end": "2875960"
  },
  {
    "text": "tasks. However, these different\nbenchmarks aren't particularly",
    "start": "2875960",
    "end": "2881450"
  },
  {
    "text": "well suited for meta-learning. So things like\nAtari, for example, don't have a lot of common\nshared structure between them.",
    "start": "2881450",
    "end": "2889700"
  },
  {
    "text": "They're quite distinct. I think I mentioned this\non Monday, the same thing with kind of Open Al Gym.",
    "start": "2889700",
    "end": "2895369"
  },
  {
    "text": "These are kind of very\nsparse and distinct tasks that wouldn't necessarily\nallow generalization",
    "start": "2895370",
    "end": "2900440"
  },
  {
    "text": "to a completely new agent. And for this last paper, there's\nonly like around six tasks,",
    "start": "2900440",
    "end": "2908180"
  },
  {
    "text": "I think, so we won't\nnecessarily expect the ability to generalize to a new\ntask or be able to quickly",
    "start": "2908180",
    "end": "2913430"
  },
  {
    "text": "adapt to a new task when only\nmeta-training on six tasks.",
    "start": "2913430",
    "end": "2918730"
  },
  {
    "start": "2917000",
    "end": "3038000"
  },
  {
    "text": "So with this in mind,\nwe wanted to create a benchmark that\nkind of captures",
    "start": "2918730",
    "end": "2924569"
  },
  {
    "text": "a large number of qualitatively\ndistinct tasks that have shaped reward functions and success\nmetrics and critically",
    "start": "2924570",
    "end": "2933060"
  },
  {
    "text": "have some sort of\nshared structure such that we can potentially\nexpect generalization",
    "start": "2933060",
    "end": "2940049"
  },
  {
    "text": "across these\nenvironments and tasks. And so the way that\nwe did this is we tried to unify a\nstate and action",
    "start": "2940050",
    "end": "2947220"
  },
  {
    "text": "space across all of\nthe tasks, and this was the result of this effort.",
    "start": "2947220",
    "end": "2954140"
  },
  {
    "text": "So this is the\nMeta-World benchmark, it has 50 different\ntasks, I mean basically it satisfies each of\nthese desiderata,",
    "start": "2954140",
    "end": "2962400"
  },
  {
    "text": "with the hope of\nenabling or allowing us to study whether meta-RL\nalgorithms can leverage",
    "start": "2962400",
    "end": "2967770"
  },
  {
    "text": "these very broad\nand distinct task distributions to generalize\nto completely new tasks that you want the robot to do.",
    "start": "2967770",
    "end": "2973770"
  },
  {
    "text": " And so now if you\nrun meta-learning",
    "start": "2973770",
    "end": "2980600"
  },
  {
    "text": "on these algorithms,\nmeta-learning algorithms seem to struggle. So if you evaluate them on\nheld-out meta-test tasks",
    "start": "2980600",
    "end": "2987650"
  },
  {
    "text": "their success rate is only\naround like 20% to 30%.",
    "start": "2987650",
    "end": "2992760"
  },
  {
    "text": "And it turns out that\neven like they're kind of struggling\non meta-test time, but they're also struggling\non the meta-training tasks.",
    "start": "2992760",
    "end": "3001005"
  },
  {
    "text": "So this suggests\nthat it's having trouble kind of even learning\nto learn the set of tasks that it's given\nduring meta-training",
    "start": "3001005",
    "end": "3009190"
  },
  {
    "text": "and so further this\nkind of suggested well, maybe we should just look\nat multi-task learning",
    "start": "3009190",
    "end": "3014440"
  },
  {
    "text": "if we can't learn the set\nof tasks themselves then meta-learning may also\nbe quite difficult.",
    "start": "3014440",
    "end": "3022980"
  },
  {
    "text": "So if you kind of evaluate\nmulti-task RL algorithms on these, you also see\nthat these algorithms are struggling to get like even\nabove like 35%, for example.",
    "start": "3022980",
    "end": "3033020"
  },
  {
    "text": " OK. So why the poor results?",
    "start": "3033020",
    "end": "3038945"
  },
  {
    "start": "3038000",
    "end": "3599000"
  },
  {
    "text": " First, maybe this is an\nexploration challenge, but we made it such that all\nthe tasks are individually",
    "start": "3038945",
    "end": "3045812"
  },
  {
    "text": "solvable. Maybe it's a data\nscarcity challenge. However, all the methods\nwere given a budget",
    "start": "3045812",
    "end": "3051530"
  },
  {
    "text": "with plenty of samples and\nmaybe kind of a limited model",
    "start": "3051530",
    "end": "3056780"
  },
  {
    "text": "capacity issue,\nbut in this case, all the methods were given\nplenty of model capacity,",
    "start": "3056780",
    "end": "3062150"
  },
  {
    "text": "and it turns out that\nactually just training models independently on all\nof the tasks actually",
    "start": "3062150",
    "end": "3069620"
  },
  {
    "text": "leads to a better\nsuccess rate than if you try to train with\nsoft-actor critic",
    "start": "3069620",
    "end": "3075740"
  },
  {
    "text": "on a single model and\npolicy on all the tasks. ",
    "start": "3075740",
    "end": "3083580"
  },
  {
    "text": "So this suggests that\nit seemed like there must be kind of an\noptimization challenge when you try to train\nacross all of these tasks",
    "start": "3083580",
    "end": "3090710"
  },
  {
    "text": "by nature of the fact that\nkind of independent training does better than if you\ntry to share weights. ",
    "start": "3090710",
    "end": "3098509"
  },
  {
    "text": "I have a question. I was just wondering if\nthere's a link to that project. How do you measure\nmodel capacity?",
    "start": "3098510",
    "end": "3105599"
  },
  {
    "text": "Because we are haven't studied\ncontextual fitting and meta learning, we claim that\nthese things overfit.",
    "start": "3105600",
    "end": "3111668"
  },
  {
    "text": "So how do you measure\nmodel capacity? And how do you measure\nplenty of samples? Yeah, I guess I think\nthat it's hard to measure",
    "start": "3111668",
    "end": "3121430"
  },
  {
    "text": "these sorts of things. In this case, for\nmodel capacity, we tried making the models\nbigger, and it didn't help,",
    "start": "3121430",
    "end": "3127910"
  },
  {
    "text": "so that suggests to us that\nthey had plenty of capacity. In terms of data scarcity. What we see here\nis that even kind",
    "start": "3127910",
    "end": "3134810"
  },
  {
    "text": "of a multi-task train, even\na multi-task policy levels off and even if you give it\nkind of more environment steps.",
    "start": "3134810",
    "end": "3141509"
  },
  {
    "text": "It doesn't continue to improve.  OK",
    "start": "3141510",
    "end": "3152310"
  },
  {
    "text": "I'm still confused\nabout why the task can be learned individually better\nthan learning with MAML.",
    "start": "3152310",
    "end": "3161349"
  },
  {
    "text": "So speaking of MAML,\nwe have to [INAUDIBLE],, which I think is almost\nexactly the same as learning",
    "start": "3161350",
    "end": "3169145"
  },
  {
    "text": "from scratch using such a\nsoft actor critic method. ",
    "start": "3169145",
    "end": "3176230"
  },
  {
    "text": "Yeah. So, in this case,\nsoft-actor critic is just-- this SAC corresponds to a\npolicy and a critic that",
    "start": "3176230",
    "end": "3185460"
  },
  {
    "text": "is conditioned on the\ntask ID and trained to perform all of the tasks.",
    "start": "3185460",
    "end": "3190887"
  },
  {
    "text": "So this doesn't correspond\nto MAML although. I showed MAML results\non the previous slide. In principle with\nMAML, you can--",
    "start": "3190887",
    "end": "3199829"
  },
  {
    "text": "if at test time you kind of\nfine-tuned with enough samples, then it would like to be able\nto do as well as independent.",
    "start": "3199830",
    "end": "3207342"
  },
  {
    "text": "When we looked at\nthat performance, that was with kind\nof a fixed budget of samples at meta-test time.",
    "start": "3207342",
    "end": "3214922"
  },
  {
    "text": "And here what I mean\nby plenty of samples is that's kind of the\nnumber of samples that you get during meta-training or\nduring multi-task training,",
    "start": "3214922",
    "end": "3223065"
  },
  {
    "text": "By speak of the samples,\nyou mean at the steps? Oh, by samples,\nyeah, I mean the kind",
    "start": "3223065",
    "end": "3228900"
  },
  {
    "text": "of the number of\nenvironment steps or the number of episodes. Oh, I see. Thank you",
    "start": "3228900",
    "end": "3237910"
  },
  {
    "text": "OK. So given this\noptimization challenge, I guess the first\ntakeaway here is",
    "start": "3237910",
    "end": "3243640"
  },
  {
    "text": "that it does seem like training\non diverse and distinct tasks shouldn't necessarily\nbe taken for granted.",
    "start": "3243640",
    "end": "3249700"
  },
  {
    "text": "It seems like when\nwe move from tasks that have a lot of\nshared structure to tasks that are much\nmore distinct then",
    "start": "3249700",
    "end": "3257020"
  },
  {
    "text": "additional challenges arise. And secondly, one\nof our hypotheses",
    "start": "3257020",
    "end": "3263290"
  },
  {
    "text": "about why we were seeing\nan optimization challenge was this hypothesis that maybe\ngradients from different tasks",
    "start": "3263290",
    "end": "3269529"
  },
  {
    "text": "are conflicting in some way. And what I mean by this is\nthat if they're conflicting,",
    "start": "3269530",
    "end": "3276190"
  },
  {
    "text": "then they're kind of pointed\nin different directions, meaning that kind\nof one gradient",
    "start": "3276190",
    "end": "3281692"
  },
  {
    "text": "wants the model to do one\nthing, and another gradient wants the model to do another\nthing for two different tasks.",
    "start": "3281692",
    "end": "3288960"
  },
  {
    "text": "And our second hypothesis\nis that in addition to conflicting, when the\ngradients do conflict,",
    "start": "3288960",
    "end": "3294720"
  },
  {
    "text": "they cause more\ndamage than expected. And in particular,\nwhat I mean by this?",
    "start": "3294720",
    "end": "3300590"
  },
  {
    "text": "Is like even if the\ngradients are conflicting, you should still be\nable to make progress on your multi-task objective\nby averaging the gradients.",
    "start": "3300590",
    "end": "3306942"
  },
  {
    "text": "However, there are\nsome situations where you won't\nnecessarily make progress, and in particular, if your\nobjective has high curvature,",
    "start": "3306943",
    "end": "3315780"
  },
  {
    "text": "then the amount of\nprogress that you'll make may be a bit different-- the amount of progress that\nyou think you're going to make",
    "start": "3315780",
    "end": "3321690"
  },
  {
    "text": "may be different than\nthe amount of progress that you actually make. So we can look at a really\nsimple example where you have--",
    "start": "3321690",
    "end": "3328890"
  },
  {
    "text": "kind of this is your parameter\nand this is your loss function. So this is like an example\nof a loss landscape that",
    "start": "3328890",
    "end": "3335579"
  },
  {
    "text": "has a lot of curvature in the\nsense that it's not linear. It's kind of very curved\nsay your current parameter",
    "start": "3335580",
    "end": "3343740"
  },
  {
    "text": "setting is right here and\nnow say that your gradient--",
    "start": "3343740",
    "end": "3352897"
  },
  {
    "text": "this is your gradient,\nthis loss function is showing the gradient, the\nloss function and the gradient for one of your tasks. ",
    "start": "3352897",
    "end": "3360720"
  },
  {
    "text": "Now, if your gradient is\ntelling you essentially to move this way so it's\nessentially telling you",
    "start": "3360720",
    "end": "3368170"
  },
  {
    "text": "that you should move kind\nof rightward to improve, and maybe there is-- maybe\nthis gradient is really strong,",
    "start": "3368170",
    "end": "3374970"
  },
  {
    "text": "and there's another\ntask that actually wants you to go in the\nopposite direction, then if you end up kind\nof ending up right here",
    "start": "3374970",
    "end": "3383292"
  },
  {
    "text": "when you have this kind of\nlarge gradient telling you that you should go right then.",
    "start": "3383292",
    "end": "3389170"
  },
  {
    "text": "I'm actually-- sorry I\nexplained that a bit wrong. ",
    "start": "3389170",
    "end": "3396270"
  },
  {
    "text": "Let me back up a bit. So here's your current\nparameter update.",
    "start": "3396270",
    "end": "3402090"
  },
  {
    "text": "Here's your gradient\nfor one task in general. This gradient is telling\nyou that you actually want to go left because your\noptimum is to the left",
    "start": "3402090",
    "end": "3410470"
  },
  {
    "text": "and say you had another\ntask that was like, cared very strongly\nabout going to the right.",
    "start": "3410470",
    "end": "3418590"
  },
  {
    "text": "Now right here, your gradient\nis kind of your gradient",
    "start": "3418590",
    "end": "3424010"
  },
  {
    "text": "for the first task for the\nblue task is somewhat small, and so you think that well\nI'd like to go to the left. But if I go to the right,\nit's kind of, not terrible,",
    "start": "3424010",
    "end": "3432020"
  },
  {
    "text": "but in reality, when you\nget this kind of second after you take the\ngradient update favoring the green task right here,\nthen you actually end up",
    "start": "3432020",
    "end": "3441320"
  },
  {
    "text": "in a spot that's kind of\nmuch worse than you expected. So essentially, this\ngreen conflicting gradient",
    "start": "3441320",
    "end": "3448550"
  },
  {
    "text": "caused more damage than what\nyour gradient was actually suggesting that it would cause,\nleading to a kind of worse,",
    "start": "3448550",
    "end": "3455840"
  },
  {
    "text": "much worse loss value than\nwhat your average gradient was expecting, and is asking,\nis the damage caused",
    "start": "3455840",
    "end": "3465290"
  },
  {
    "text": "by conflicts due to\ninsufficient model capacity? ",
    "start": "3465290",
    "end": "3470329"
  },
  {
    "text": "I think that the relationship\nbetween model capacity and the landscape\nis a bit tricky.",
    "start": "3470330",
    "end": "3477050"
  },
  {
    "text": "In general, I guess it doesn't\nseem like it's a model capacity issue, but model capacity\ncan certainly play a role",
    "start": "3477050",
    "end": "3484700"
  },
  {
    "text": "and is asking, can you\nexplain why it's worse? So the reason why it's\nworse is that your gradient",
    "start": "3484700",
    "end": "3491870"
  },
  {
    "text": "is kind of-- your\ngradient is just taking into account\nfirst-order information, and it's not taking\ninto account the fact",
    "start": "3491870",
    "end": "3497180"
  },
  {
    "text": "that your function is curved. And so your gradient\nbasically expects",
    "start": "3497180",
    "end": "3504230"
  },
  {
    "text": "that if you take a step that\nis this size, that you'll",
    "start": "3504230",
    "end": "3509660"
  },
  {
    "text": "end up right here. However, in reality,\nyour function is curved.",
    "start": "3509660",
    "end": "3515010"
  },
  {
    "text": "And so when you\nactually take this step, you end up in a situation\nthat's much worse than what",
    "start": "3515010",
    "end": "3520040"
  },
  {
    "text": "your gradient suggests. And this is something\nthat happens whenever you have high positive curvature.",
    "start": "3520040",
    "end": "3526772"
  },
  {
    "text": " Do you have another question?",
    "start": "3526772",
    "end": "3532230"
  },
  {
    "text": "Yes, so I just want\nto clarify my point, so I am wondering so if we\nhad a lot smaller capacity",
    "start": "3532230",
    "end": "3538440"
  },
  {
    "text": "I think in this high\ndimensional loss space we can find a direction\nwhere all the gradients--",
    "start": "3538440",
    "end": "3546460"
  },
  {
    "text": "or where to find the gradients\nmove in the direction where all the gradient steps are\ntaken to the right direction?",
    "start": "3546460",
    "end": "3558360"
  },
  {
    "text": "Yeah, so it's possible that\nthis is less of an issue when you have\nover-parameterized networks, although, in\npractice, we actually",
    "start": "3558360",
    "end": "3564900"
  },
  {
    "text": "do observe that some sort of\nconflict or some sort of issue",
    "start": "3564900",
    "end": "3569940"
  },
  {
    "text": "arises even with\ndeep neural networks. ",
    "start": "3569940",
    "end": "3575060"
  },
  {
    "text": "OK, thank you. And is asking, this is true even\nwithout conflicting gradients, so yeah, like high\ncurvature is kind of a thing",
    "start": "3575060",
    "end": "3582380"
  },
  {
    "text": "even if you don't have\nconflicting gradients. The issue here,\nthough, is that when you do have kind of two\nconflicting gradients,",
    "start": "3582380",
    "end": "3588338"
  },
  {
    "text": "for example, one\nthat tells you to go in the opposite\ndirection, then this is kind of more of an\nissue because you end up",
    "start": "3588338",
    "end": "3596760"
  },
  {
    "text": "needing to make decisions\nbetween different task gradients. And if those decisions are\nbased off of your gradients,",
    "start": "3596760",
    "end": "3603800"
  },
  {
    "text": "then you kind of may favor one\nof the conflicting gradients over another in a way that isn't\nkind of the ideal solution.",
    "start": "3603800",
    "end": "3610460"
  },
  {
    "start": "3610460",
    "end": "3616859"
  },
  {
    "text": "OK. And so kind of one solution\nthat we can think about",
    "start": "3616860",
    "end": "3622050"
  },
  {
    "text": "to addressing this is\ntry to avoid when you're kind of considering\nmultiple gradients try to avoid making\nother tasks worse when",
    "start": "3622050",
    "end": "3630119"
  },
  {
    "text": "you're taking a gradient step. And this may allow\nyou to kind of prevent",
    "start": "3630120",
    "end": "3635710"
  },
  {
    "text": "this sort of underestimation\nof how bad things will be when gradients\nare conflicting,",
    "start": "3635710",
    "end": "3641260"
  },
  {
    "text": "and in particular, the resulting\nalgorithm is really simple, so if two gradients conflict,\nif their cosine similarity,",
    "start": "3641260",
    "end": "3648070"
  },
  {
    "text": "is negative, then project the\ngradient onto the normal plane of the other gradient and do\nthis for both the gradients.",
    "start": "3648070",
    "end": "3656900"
  },
  {
    "text": "And this essentially\nprevents the great the kind of update from gi from\nhurting task J and vice versa.",
    "start": "3656900",
    "end": "3668920"
  },
  {
    "text": "And if the two gradients\ndon't conflict, then you can just\nleave them alone, and they'll have some\nsort of positive transfer.",
    "start": "3668920",
    "end": "3675255"
  },
  {
    "text": " We refer to this as projecting\nconflicting gradients or PCGrad",
    "start": "3675255",
    "end": "3683780"
  },
  {
    "text": "and what we can see is that\nwhile kind of performance of multi-task\ntraining is typically",
    "start": "3683780",
    "end": "3690860"
  },
  {
    "text": "worse than independent\ntraining, when we use this sort of\noptimization technique we actually see much\nbetter performance",
    "start": "3690860",
    "end": "3697790"
  },
  {
    "text": "than multi-task training\nand also better performance than independent training. ",
    "start": "3697790",
    "end": "3706800"
  },
  {
    "text": "OK, and then we also see\nperformance gains in multi-task supervised learning settings\nsuch as CIFAR and NYUv2",
    "start": "3706800",
    "end": "3713170"
  },
  {
    "text": "as well.  OK.",
    "start": "3713170",
    "end": "3719070"
  },
  {
    "text": "I was going to go a bit into\nkind of a little bit more of the theory of why it works. But we're running\na bit low on time,",
    "start": "3719070",
    "end": "3724660"
  },
  {
    "text": "so I'm going to skip this. The takeaways, though, are\nkind of still the same.",
    "start": "3724660",
    "end": "3730770"
  },
  {
    "text": "Scaling to broad\ntask distributions can be quite hard, and\nit can't necessarily be taken for granted.",
    "start": "3730770",
    "end": "3737430"
  },
  {
    "text": "Training on broadened\ntask distributions is kind of an important\nstep in this direction,",
    "start": "3737430",
    "end": "3743250"
  },
  {
    "text": "and it seems like\nthere's evidence that addressing some of\nthe optimization issues that arise from these\nbroad task distributions",
    "start": "3743250",
    "end": "3750480"
  },
  {
    "text": "can help a lot when\nyou are trying to learn across these broad tasks. ",
    "start": "3750480",
    "end": "3757640"
  },
  {
    "text": "OK. So now I want to talk a\nbit about open challenges, and in particular, we've talked\nalready about a lot of open",
    "start": "3757640",
    "end": "3765220"
  },
  {
    "text": "challenges in multi-task\nlearning and meta-learning through the course of this--",
    "start": "3765220",
    "end": "3770799"
  },
  {
    "text": "over the course of this class. So what we'll talk\na bit about today is just some of\nthe open challenges",
    "start": "3770800",
    "end": "3776020"
  },
  {
    "text": "that we haven't previously\ncovered in this realm. ",
    "start": "3776020",
    "end": "3782720"
  },
  {
    "text": "So one kind of\nclass of challenges is just addressing fundamental\nproblem assumptions",
    "start": "3782720",
    "end": "3790180"
  },
  {
    "text": "in multi-task and meta-learning. So one area that we've\ntalked about a bit,",
    "start": "3790180",
    "end": "3795220"
  },
  {
    "text": "especially earlier\ntoday, is handling out of distribution tasks and\nlong-tailed task distributions,",
    "start": "3795220",
    "end": "3801660"
  },
  {
    "text": "and the reason why\nthis is a challenge is that if you have a\nvery long-tailed task distribution where\ndifferent tasks correspond",
    "start": "3801660",
    "end": "3807990"
  },
  {
    "text": "to different objects, or\ndifferent driving scenarios and so forth. We know how to kind of generally\ndo few-shot learning now,",
    "start": "3807990",
    "end": "3815880"
  },
  {
    "text": "and this allows us\nto potentially adapt to this part of the\ndata distribution where we don't\nhave a lot of data",
    "start": "3815880",
    "end": "3821220"
  },
  {
    "text": "or don't have a lot\nof labeled examples. But these few-shot tasks in\nthis part of the distribution",
    "start": "3821220",
    "end": "3829370"
  },
  {
    "text": "may be out of distribution. They may be from a\ndifferent distribution than kind of the parts\nof the distribution",
    "start": "3829370",
    "end": "3836119"
  },
  {
    "text": "where we have a lot of examples. We've seen some kind of evidence\nthat meta-learning algorithms",
    "start": "3836120",
    "end": "3842980"
  },
  {
    "text": "can allow us to\ngeneralize to the tale, so one example that we\nsaw early in the class was the prototypical\nclustering networks",
    "start": "3842980",
    "end": "3849970"
  },
  {
    "text": "for dermatological\ndiseases, which allowed for kind of some success\non long-tail distributions.",
    "start": "3849970",
    "end": "3857559"
  },
  {
    "text": "Today we also saw adaptive\nrisk minimization, which allows us to\nadapt to different parts",
    "start": "3857560",
    "end": "3862780"
  },
  {
    "text": "of the distribution as well. But I think that there's\nstill kind of a lot of work to be done here to think\nabout how we might handle",
    "start": "3862780",
    "end": "3869720"
  },
  {
    "text": "out-of-distribution tasks. And I think that\nfurther hints might come from the domain\nadaptation literature and the robustness\nliterature to think",
    "start": "3869720",
    "end": "3876920"
  },
  {
    "text": "about how we might\nadapt to things that are out of distribution. ",
    "start": "3876920",
    "end": "3883920"
  },
  {
    "text": "Beyond generalization, another\nkind of fundamental challenge is multimodality.",
    "start": "3883920",
    "end": "3890550"
  },
  {
    "text": "So humans, for example,\nwhen we kind of leverage",
    "start": "3890550",
    "end": "3896460"
  },
  {
    "text": "previous experience,\nour previous experience isn't always in the form of a\nset of tasks that we're given.",
    "start": "3896460",
    "end": "3902790"
  },
  {
    "text": "We often have kind of rich\nsources of previous experiences that include visual imagery,\ntactile feedback, language",
    "start": "3902790",
    "end": "3909930"
  },
  {
    "text": "for reading, and\nalso social cues, and humans have this\nability to kind of merge",
    "start": "3909930",
    "end": "3915990"
  },
  {
    "text": "all of this rich previous\nexperience in a way that allows them to build knowledge\nand use that knowledge",
    "start": "3915990",
    "end": "3922950"
  },
  {
    "text": "to solve problems. And so, kind of an\ninteresting challenge",
    "start": "3922950",
    "end": "3928710"
  },
  {
    "text": "for a future algorithm\nis to try to learn priors across these multiple\ndata modalities.",
    "start": "3928710",
    "end": "3934000"
  },
  {
    "text": "This involves solving\nchallenges such as varying dimensionalities, varying units. They also carry different\nand complementary forms",
    "start": "3934000",
    "end": "3940650"
  },
  {
    "text": "of information. Some hints here might\ncome from the literature",
    "start": "3940650",
    "end": "3947070"
  },
  {
    "text": "on multi-modal\nlearning, but I think that there's still a lot kind\nof interesting challenges that come up here.",
    "start": "3947070",
    "end": "3953460"
  },
  {
    "text": "One kind of\ninteresting paper that is quite recent that\nstarts to study this is this paper here\nby Liang et al,",
    "start": "3953460",
    "end": "3961560"
  },
  {
    "text": "Learning in Low-resource\nModalities Via Cross-modal Generalization. ",
    "start": "3961560",
    "end": "3970500"
  },
  {
    "text": "And then finally\none open challenge also is understanding when\nmulti-task learning will help",
    "start": "3970500",
    "end": "3977460"
  },
  {
    "text": "and when independent training\nis just like the right solution. We saw one example today\nwhere independent training",
    "start": "3977460",
    "end": "3984599"
  },
  {
    "text": "is actually sometimes actually\nquite a good solution.",
    "start": "3984600",
    "end": "3990065"
  },
  {
    "text": "It is helpful to\nunderstand when that's a good solution versus when\ndifferent multi-task learning",
    "start": "3990065",
    "end": "3995440"
  },
  {
    "text": "techniques might be helpful\nfor actually allowing better transfer across tasks. ",
    "start": "3995440",
    "end": "4003850"
  },
  {
    "text": "Beyond addressing these\ndifferent problem assumptions, one open challenge\nin this kind of field",
    "start": "4003850",
    "end": "4011279"
  },
  {
    "text": "is handling different-- or developing and handling\ndifferent benchmarks. So ideally, we want\nbenchmarks that",
    "start": "4011280",
    "end": "4018260"
  },
  {
    "text": "show breadth that\nreally challenge current algorithms to\nfind the common structure among those tasks and leverage\nthat to learn more quickly,",
    "start": "4018260",
    "end": "4027350"
  },
  {
    "text": "and also, we want benchmarks\nthat are realistic, that reflects real-world\nproblems that we care about.",
    "start": "4027350",
    "end": "4034485"
  },
  {
    "text": "I think that there are steps\ntowards some good benchmarks such as Meta-Dataset, such\nas the Meta-World benchmark I",
    "start": "4034485",
    "end": "4039590"
  },
  {
    "text": "talked about today, the\nVisual Task Adaptation Benchmark, and the\nTaskonomy dataset,",
    "start": "4039590",
    "end": "4046829"
  },
  {
    "text": "and kind of a lot of these\ndo have some reflections of real-world problems.",
    "start": "4046830",
    "end": "4052000"
  },
  {
    "text": "But ultimately, we\nwant benchmarks that-- other benchmarks that\nstudy problems beyond like robotic manipulation\nand computer vision",
    "start": "4052000",
    "end": "4058680"
  },
  {
    "text": "that have an appropriate\nlevel of difficulty and are also easy to use.",
    "start": "4058680",
    "end": "4064240"
  },
  {
    "text": "We also saw kind of other\nbenchmarks in class, so we saw kind of the land\ncover classification task.",
    "start": "4064240",
    "end": "4069900"
  },
  {
    "text": "We saw the kind of drug\ndiscovery-like task where you're predicting the activity\nof different molecules,",
    "start": "4069900",
    "end": "4077309"
  },
  {
    "text": "and these could also\nmake promising benchmarks in the field, although so far,\nI think they were primarily introduced as an\ninteresting application",
    "start": "4077310",
    "end": "4083819"
  },
  {
    "text": "and not necessarily\nas a benchmark itself. ",
    "start": "4083820",
    "end": "4089980"
  },
  {
    "text": "And then beyond kind of\nfundamental problem assumptions and benchmarks,\nthere's also certainly",
    "start": "4089980",
    "end": "4094990"
  },
  {
    "text": "room for improvement\non improving the core algorithms themselves. This includes things like\ncomputation and memory,",
    "start": "4094990",
    "end": "4102160"
  },
  {
    "text": "so making like high-level\noptimization more practical,",
    "start": "4102160",
    "end": "4107413"
  },
  {
    "text": "developing more of a\ntheoretical understanding of the performance\nof these algorithms. There's actually been\na fair amount of work",
    "start": "4107413",
    "end": "4113109"
  },
  {
    "text": "very recently in\nthe past year, or so that has started to develop kind\nof some interesting theoretical",
    "start": "4113109",
    "end": "4119439"
  },
  {
    "text": "insights on these algorithms,\nbut there's still a lot more work to be done there.",
    "start": "4119439",
    "end": "4125500"
  },
  {
    "text": "And then also kind of another\nchallenge specifically in the reinforcement\nlearning setting",
    "start": "4125500",
    "end": "4130509"
  },
  {
    "text": "is not just performing\nlots of different tasks, but also performing\ntasks in sequence",
    "start": "4130510",
    "end": "4136810"
  },
  {
    "text": "where you want to kind of\ncompose different tasks that you've learned into\nlonger horizon behaviors.",
    "start": "4136810",
    "end": "4142380"
  },
  {
    "text": " And then also beyond\nthese, of course, there's also the challenges\nthat you discovered",
    "start": "4142380",
    "end": "4149399"
  },
  {
    "text": "in your homework and\nyour final projects, which I'm sure\nthere are multiple. ",
    "start": "4149399",
    "end": "4157189"
  },
  {
    "text": "OK. So those are some\nof the challenges that I wanted to highlight\nbefore we finish.",
    "start": "4157189",
    "end": "4163899"
  },
  {
    "text": "I wanted to end on kind\nof a bit of a broader note and touch back a bit\nto some of the things",
    "start": "4163899",
    "end": "4170350"
  },
  {
    "text": "that we talked about in\nthe very first lecture. So, in particular, if you think\nabout the biggest advances",
    "start": "4170350",
    "end": "4181399"
  },
  {
    "text": "in AI and machine learning. And kind of what the\nbiggest milestones have been in those areas,\nthey've been things",
    "start": "4181399",
    "end": "4189229"
  },
  {
    "text": "like TD Gammon or IBM Watson\nwith Jeopardy or machines",
    "start": "4189229",
    "end": "4195410"
  },
  {
    "text": "that can translate between\nlanguages or things that can play like Atari\ngames or play Go.",
    "start": "4195410",
    "end": "4201350"
  },
  {
    "text": "And I think that all of\nthese kind of big milestones are examples of machines\nthat are actually very specialized to solve\none task very, very well.",
    "start": "4201350",
    "end": "4211170"
  },
  {
    "text": "And as a result,\nthey kind of appear very impressive because\nthese are things that might be very difficult for humans.",
    "start": "4211170",
    "end": "4218429"
  },
  {
    "text": "So while impressive things may\nkind of appear potentially very intelligent, it seems\nlike there's still",
    "start": "4218430",
    "end": "4226430"
  },
  {
    "text": "things that are missing to get\nto the level of what humans are able to do.",
    "start": "4226430",
    "end": "4232590"
  },
  {
    "text": "Humans are kind of able to\ndo many, many more things than what our current machine\nlearning and AI techniques",
    "start": "4232590",
    "end": "4238219"
  },
  {
    "text": "are able to do. And I think that\nin general, we've covered a lot in\nCS330 to make steps",
    "start": "4238220",
    "end": "4245650"
  },
  {
    "text": "towards these kind of more\ngeneralist systems, systems that can learn\nmultiple tasks, they can leverage previous experience\nwhen learning new things,",
    "start": "4245650",
    "end": "4254020"
  },
  {
    "text": "learning general-purpose\nmodels of the world, preparing for tasks\nbefore you even",
    "start": "4254020",
    "end": "4259420"
  },
  {
    "text": "know what they are\nfor things like skill discovery and unsupervised\nmeta-learning,",
    "start": "4259420",
    "end": "4264600"
  },
  {
    "text": "performing tasks in\nsequence and also being able to kind\nof learn continuously",
    "start": "4264600",
    "end": "4269790"
  },
  {
    "text": "with lifelong learning. Now, what's missing?",
    "start": "4269790",
    "end": "4276060"
  },
  {
    "text": "It certainly seems like these\ningredients are things that are helpful for us taking steps\ntowards more general machine",
    "start": "4276060",
    "end": "4282750"
  },
  {
    "text": "learning systems, but it's\nlikely that they aren't. This isn't kind of\nthe full list of what",
    "start": "4282750",
    "end": "4288660"
  },
  {
    "text": "we need to actually make kind\nof really significant progress in this area. So hopefully, now that\nyou've kind of learned",
    "start": "4288660",
    "end": "4296278"
  },
  {
    "text": "a lot of material in this\ncourse and are equipped with kind of an overview\nof all these topics,",
    "start": "4296278",
    "end": "4302010"
  },
  {
    "text": "you'll have a better\nsense of figuring out exactly what's missing so that\nyou can also make progress",
    "start": "4302010",
    "end": "4307950"
  },
  {
    "text": "in this area of AI research.  Great, that's it.",
    "start": "4307950",
    "end": "4314280"
  },
  {
    "text": "A couple of reminders, the\nfinal projects are next week. The schedule's on Piazza,\nthe final project report",
    "start": "4314280",
    "end": "4321650"
  },
  {
    "text": "is due at the end of\nnext week, as well. And this is also\nthe last lecture.",
    "start": "4321650",
    "end": "4328070"
  },
  {
    "text": "That gives you kind of\nfive minutes, at the end, to fill out course evaluations.",
    "start": "4328070",
    "end": "4334280"
  },
  {
    "text": "Please do fill out\ncourse evaluations as this is still kind of\na relatively young course.",
    "start": "4334280",
    "end": "4339500"
  },
  {
    "text": "We really value your\nfeedback, your feedback during the mid-quarter survey\nwas already extremely helpful,",
    "start": "4339500",
    "end": "4345980"
  },
  {
    "text": "and we're planning\nto incorporate that into future iterations\nof the course.",
    "start": "4345980",
    "end": "4351610"
  },
  {
    "text": "I'd encourage you\nto take the time to do course evaluations\nnow but I'm also happy to answer other\nquestions as well beyond that.",
    "start": "4351610",
    "end": "4360160"
  },
  {
    "start": "4360160",
    "end": "4369510"
  },
  {
    "text": "Any lingering questions? ",
    "start": "4369510",
    "end": "4379720"
  },
  {
    "text": "So. just thank you so\nmuch for the course. So you just skip\nthe theoretical part",
    "start": "4379720",
    "end": "4387370"
  },
  {
    "text": "of the conflicts\nof the gradients, can you go over that. So I'm still a little\nconfused about that.",
    "start": "4387370",
    "end": "4395020"
  },
  {
    "text": "Cool, I'll spend a\nfew minutes to go through the theoretical\npart of PCGrad.",
    "start": "4395020",
    "end": "4400540"
  },
  {
    "text": "So the first part\nthat isn't theoretical",
    "start": "4400540",
    "end": "4407000"
  },
  {
    "text": "that's actually more empirical\nis that you can take PCGrad-- PCGrad is actually changing both\nthe direction and the magnitude",
    "start": "4407000",
    "end": "4412610"
  },
  {
    "text": "of the gradient. And you can just kind of\nablate these two components, and what you see is\nthat actually kind of it",
    "start": "4412610",
    "end": "4419270"
  },
  {
    "text": "seems like both of these\ntwo components are helpful, but actually changing\nthe direction",
    "start": "4419270",
    "end": "4424820"
  },
  {
    "text": "is more helpful than\nchanging the magnitude.  But getting back to kind of what\nwe were talking about before,",
    "start": "4424820",
    "end": "4433460"
  },
  {
    "text": "it seems like there's three\nproperties that are like--",
    "start": "4433460",
    "end": "4438540"
  },
  {
    "text": "that are kind of maybe arising\nin multi-task learning in which PCGrad might be helping with.",
    "start": "4438540",
    "end": "4443640"
  },
  {
    "text": "The first is\nconflicting gradients, the second is large\npositive curvature,",
    "start": "4443640",
    "end": "4449280"
  },
  {
    "text": "And the third is having\na large difference in the magnitude of gradients\nfor different tasks.",
    "start": "4449280",
    "end": "4456310"
  },
  {
    "text": "And if you have these\nthree different-- if these three properties\nkind of arise all together,",
    "start": "4456310",
    "end": "4462670"
  },
  {
    "text": "which we kind of refer\nto as the tragic triad, then you could\nactually show that,",
    "start": "4462670",
    "end": "4469050"
  },
  {
    "text": "that PCGrad actually kind\nof does better than just taking the average gradient.",
    "start": "4469050",
    "end": "4475920"
  },
  {
    "text": "One quick illustration of this\nis like here's one optimization landscape, and you\ncould show that Adam",
    "start": "4475920",
    "end": "4481200"
  },
  {
    "text": "gets stuck in this optimization\nlandscape right here, it doesn't end up\ngetting to the optimum whereas Adam with PCGrad is\nable to get to the optimum.",
    "start": "4481200",
    "end": "4490830"
  },
  {
    "text": "But kind of more theoretically,\nwe can show that--",
    "start": "4490830",
    "end": "4496410"
  },
  {
    "text": "we can show kind\nof theoretically that if you have a\nlarge enough conflict",
    "start": "4496410",
    "end": "4501480"
  },
  {
    "text": "curvature and a gradient\nmagnitude difference that for two tasks,\nthen you can show",
    "start": "4501480",
    "end": "4506940"
  },
  {
    "text": "that PCGrad is provably\nbetter than just taking the average gradient.",
    "start": "4506940",
    "end": "4512150"
  },
  {
    "text": "I don't want to go too\nmuch into the details, but basically, these\nthree conditions kind of translate into these\nthree conditions up here,",
    "start": "4512150",
    "end": "4520160"
  },
  {
    "text": "and then you can\nshow that basically, the result of the\nPCGrad update leads to a better loss than the result\nof the multi-task gradient.",
    "start": "4520160",
    "end": "4527480"
  },
  {
    "text": " And then we could also\nkind of empirically observe that in\npractice there seems",
    "start": "4527480",
    "end": "4533930"
  },
  {
    "text": "to be large positive\ncurvature, and there, the kind of conditions\nA and B seem",
    "start": "4533930",
    "end": "4540110"
  },
  {
    "text": "to also hold most of the time. ",
    "start": "4540110",
    "end": "4545210"
  },
  {
    "text": "So can I ask a quick question? Yeah. So from my understanding,\nthen the loss landscape",
    "start": "4545210",
    "end": "4552370"
  },
  {
    "text": "is the same whether with\nor without this PCGrad.",
    "start": "4552370",
    "end": "4557416"
  },
  {
    "text": " The loss landscape\ndoes seem to be similar",
    "start": "4557416",
    "end": "4564530"
  },
  {
    "text": "with and without PCGrad. PCGrad is really isn't changing\nthe loss landscape itself,",
    "start": "4564530",
    "end": "4569639"
  },
  {
    "text": "it's changing how\nyou apply gradients to the network from the\ninside the loss landscape.",
    "start": "4569640",
    "end": "4576300"
  },
  {
    "text": "OK, so PCGrad helps to prevent\nsome kind of local optimums",
    "start": "4576300",
    "end": "4581599"
  },
  {
    "text": "like kind of decreased\nconflicts gradients?",
    "start": "4581600",
    "end": "4591270"
  },
  {
    "text": "Yeah. I'm not sure if it's\nexactly necessarily like a local optimum per se,\nit seemed like it may actually",
    "start": "4591270",
    "end": "4597487"
  },
  {
    "text": "be something that's a bit\nmore like thrashing where it's kind of getting a large\ngradient update for one task and then a large gradient\nupdate for another task,",
    "start": "4597487",
    "end": "4605730"
  },
  {
    "text": "but yeah you can kind of think\nof it something like that. Yeah. Because from your\nlast slides, you're",
    "start": "4605730",
    "end": "4610860"
  },
  {
    "text": "arguing that the\nAdam optimizer can get stuck in local optimums?",
    "start": "4610860",
    "end": "4619889"
  },
  {
    "text": "So we observe that in this\nreally simple 2D example where Adam got stuck here, it's harder\nto visualize like the lost",
    "start": "4619890",
    "end": "4628230"
  },
  {
    "text": "landscapes of really\nhigh dimensional systems, so it's hard to\nunderstand exactly that's what's happening in\nthe deep neural network case,",
    "start": "4628230",
    "end": "4634800"
  },
  {
    "text": "but we did observe that\nin this 2D example. I see, thank you so much,\nthank you for the great class.",
    "start": "4634800",
    "end": "4641080"
  },
  {
    "text": " I have a quick question. I was wondering [INAUDIBLE]\norganization [INAUDIBLE]..",
    "start": "4641080",
    "end": "4651015"
  },
  {
    "text": "We call it [INAUDIBLE]\norganize [INAUDIBLE] are you able to do that in a\nback-prop in deep learning?",
    "start": "4651016",
    "end": "4660770"
  },
  {
    "start": "4660770",
    "end": "4666760"
  },
  {
    "text": "So you're saying, sorry,\nin convex optimization, there's an if loop, an if/else? No. I mean so you're saying\nhere if a dot, what is it,",
    "start": "4666760",
    "end": "4675156"
  },
  {
    "text": "gi.gj is negative, then you\ntake the knowledge range of--",
    "start": "4675156",
    "end": "4683010"
  },
  {
    "text": "I don't know what\nthe operation is. But you project gj onto the\nknowledge layer in gi and vice",
    "start": "4683010",
    "end": "4688612"
  },
  {
    "text": "versa and then you take that\nas your gradients gi and gj going forward. But so I feel like\nyou definitely",
    "start": "4688612",
    "end": "4696989"
  },
  {
    "text": "might break the graph\ninto hypotheticals",
    "start": "4696990",
    "end": "4703650"
  },
  {
    "text": "in terms of gradient flow. So I was wondering if\nthe if/else loop there",
    "start": "4703650",
    "end": "4709350"
  },
  {
    "text": "is differentiable or maybe\nI should go to the Github link or something,\nbut I was going",
    "start": "4709350",
    "end": "4716030"
  },
  {
    "text": "to see if you had any ideas on\nhow you can implement something like that? So it's quite straightforward\nto implement because you don't--",
    "start": "4716030",
    "end": "4724110"
  },
  {
    "text": "this isn't like a\nmeta-learning algorithm. So it's just like a\nmulti-task learning algorithm, and you don't need to\ndifferentiate through",
    "start": "4724110",
    "end": "4729735"
  },
  {
    "text": "the if and else.  And if you do need to\ndifferentiate through it,",
    "start": "4729735",
    "end": "4734790"
  },
  {
    "text": "that would be like it\nwould be a bit tricky. Although it might be yeah,\nit would be a bit tricky,",
    "start": "4734790",
    "end": "4742567"
  },
  {
    "text": "although it was kind\nof like differentiating through a value, which\nis sort of tricky but in reality seems to be OK.",
    "start": "4742567",
    "end": "4748949"
  },
  {
    "text": " Yeah, and the, yeah, I guess\nmaybe the other thing worth",
    "start": "4748950",
    "end": "4755411"
  },
  {
    "text": "mentioning is that\nyou can actually show that in the\nconvex setting, this does actually converge to the--",
    "start": "4755412",
    "end": "4760755"
  },
  {
    "text": "PCGrad does actually\nconverge to the right answer. So it doesn't break like\nthe convergence property",
    "start": "4760755",
    "end": "4766950"
  },
  {
    "text": "of gradient descent. OK, thank you, thank you\nfor a great course as well.",
    "start": "4766950",
    "end": "4773224"
  },
  {
    "text": " Great, thank you, everyone.",
    "start": "4773224",
    "end": "4778969"
  },
  {
    "start": "4778970",
    "end": "4784000"
  }
]