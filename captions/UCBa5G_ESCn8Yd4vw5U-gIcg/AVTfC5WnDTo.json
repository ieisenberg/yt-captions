[
  {
    "text": "OK, welcome back.",
    "start": "0",
    "end": "1520"
  },
  {
    "text": "Today we're going\nto run through some",
    "start": "1520",
    "end": "3980"
  },
  {
    "text": "of the lab for chapter\n8, tree-based methods,",
    "start": "3980",
    "end": "7160"
  },
  {
    "text": "focusing on random\nforest and boosting.",
    "start": "7160",
    "end": "11340"
  },
  {
    "text": "We're going to start off with\nsingle tree-based methods,",
    "start": "11340",
    "end": "14010"
  },
  {
    "text": "just to get a sense of how\nthat performs and compare it",
    "start": "14010",
    "end": "17312"
  },
  {
    "text": "to the ensemble methods.",
    "start": "17312",
    "end": "18312"
  },
  {
    "text": "OK.",
    "start": "18312",
    "end": "20450"
  },
  {
    "text": "As usual, we have some\nimports at the beginning.",
    "start": "20450",
    "end": "23870"
  },
  {
    "text": "And then we have some new\nimports for this model--",
    "start": "23870",
    "end": "27040"
  },
  {
    "text": "module, I should say.",
    "start": "27040",
    "end": "28470"
  },
  {
    "text": "So the main methods we're\ngoing to talk about today",
    "start": "28470",
    "end": "31670"
  },
  {
    "text": "are random forest and boosting.",
    "start": "31670",
    "end": "34970"
  },
  {
    "text": "And we're going to\nbe doing regression.",
    "start": "34970",
    "end": "36797"
  },
  {
    "text": "So that's why we're\nlooking at random forest",
    "start": "36797",
    "end": "38630"
  },
  {
    "text": "regressor and gradient\nboosting regressor",
    "start": "38630",
    "end": "40643"
  },
  {
    "text": "instead of classifier.",
    "start": "40643",
    "end": "41560"
  },
  {
    "text": "OK.",
    "start": "41560",
    "end": "43010"
  },
  {
    "text": "We'll also talk a little\nbit about decision trees.",
    "start": "43010",
    "end": "45109"
  },
  {
    "text": "We'll look at the\ndecision tree regressor.",
    "start": "45110",
    "end": "47270"
  },
  {
    "text": "And there's the corresponding\nclassifier, decision tree",
    "start": "47270",
    "end": "50000"
  },
  {
    "text": "classifier.",
    "start": "50000",
    "end": "52967"
  },
  {
    "text": "OK.",
    "start": "52967",
    "end": "53467"
  },
  {
    "text": "So fitting a classification\nor regression tree because,",
    "start": "53467",
    "end": "59070"
  },
  {
    "text": "well, they're, again,\nmethods from scikit-learn.",
    "start": "59070",
    "end": "62310"
  },
  {
    "text": "They will look almost\nidentical to the methods",
    "start": "62310",
    "end": "65820"
  },
  {
    "text": "we saw in chapter 4\nfor classification.",
    "start": "65820",
    "end": "68220"
  },
  {
    "text": "So once we form a set of\nfeatures and an outcome,",
    "start": "68220",
    "end": "71520"
  },
  {
    "text": "we'll just use the fit\nand predict method.",
    "start": "71520",
    "end": "73890"
  },
  {
    "text": "And, similarly, for\ncross validation,",
    "start": "73890",
    "end": "75930"
  },
  {
    "text": "like we saw in chapter 6,\nbecause it's an sklearn",
    "start": "75930",
    "end": "78690"
  },
  {
    "text": "estimator, we can use\nthe same kind of setup",
    "start": "78690",
    "end": "81480"
  },
  {
    "text": "to do classification.",
    "start": "81480",
    "end": "83170"
  },
  {
    "text": "So since we're going to focus\non regression for random forest",
    "start": "83170",
    "end": "86850"
  },
  {
    "text": "and boosting, we're going to\njump to the part of fitting",
    "start": "86850",
    "end": "90510"
  },
  {
    "text": "a regression decision tree\nrather than a classification",
    "start": "90510",
    "end": "93870"
  },
  {
    "text": "decision tree.",
    "start": "93870",
    "end": "96240"
  },
  {
    "text": "Again, they look similar.",
    "start": "96240",
    "end": "98229"
  },
  {
    "text": "The main difference being\none is a binary outcome",
    "start": "98230",
    "end": "100500"
  },
  {
    "text": "or categorical outcome.",
    "start": "100500",
    "end": "101482"
  },
  {
    "text": "And the other one\nis a continuous one.",
    "start": "101482",
    "end": "103065"
  },
  {
    "text": "OK.",
    "start": "103065",
    "end": "105300"
  },
  {
    "text": "The data set we're going to\nuse for regression today is",
    "start": "105300",
    "end": "108450"
  },
  {
    "text": "the Boston data set\nwe saw in chapter 3.",
    "start": "108450",
    "end": "111729"
  },
  {
    "text": "And the preprocessing\nis pretty simple.",
    "start": "111730",
    "end": "115220"
  },
  {
    "text": "We just have to find\na set of features.",
    "start": "115220",
    "end": "118150"
  },
  {
    "text": "And we'll do that by\nmaking a design matrix as",
    "start": "118150",
    "end": "120700"
  },
  {
    "text": "if we were doing\nlinear regression",
    "start": "120700",
    "end": "122140"
  },
  {
    "text": "and just taking\nthe design matrix.",
    "start": "122140",
    "end": "124240"
  },
  {
    "text": "Though we don't want to\ninclude an intercept.",
    "start": "124240",
    "end": "126907"
  },
  {
    "text": "So we're going to say\nintercept equals false",
    "start": "126907",
    "end": "128739"
  },
  {
    "text": "because the regression trees\nfit their own intercept.",
    "start": "128740",
    "end": "132100"
  },
  {
    "text": "And, well, it could\nbe problematic",
    "start": "132100",
    "end": "134680"
  },
  {
    "text": "having an intercept\ncolumn in there.",
    "start": "134680",
    "end": "138040"
  },
  {
    "text": "So regression\ntrees-- we're going",
    "start": "138040",
    "end": "140799"
  },
  {
    "text": "to convert the design matrix\nto an array rather than a data",
    "start": "140800",
    "end": "143490"
  },
  {
    "text": "frame.",
    "start": "143490",
    "end": "143990"
  },
  {
    "text": "So we lose some of\nthe feature names.",
    "start": "143990",
    "end": "147460"
  },
  {
    "text": "But this is the\npreferred data format",
    "start": "147460",
    "end": "150610"
  },
  {
    "text": "for fitting the regression tree.",
    "start": "150610",
    "end": "152305"
  },
  {
    "start": "152305",
    "end": "154709"
  },
  {
    "text": "OK.",
    "start": "154710",
    "end": "155210"
  },
  {
    "text": "So for validation, we're going\nto split our data into training",
    "start": "155210",
    "end": "161600"
  },
  {
    "text": "and test and fit on the training\nand evaluate on the test.",
    "start": "161600",
    "end": "165360"
  },
  {
    "text": "We've seen this before.",
    "start": "165360",
    "end": "167600"
  },
  {
    "text": "To construct a-- to\nfit a regression tree,",
    "start": "167600",
    "end": "170360"
  },
  {
    "text": "we first construct\nthe estimator.",
    "start": "170360",
    "end": "172130"
  },
  {
    "text": "And remember, for scikit-learn,\ninforming the estimator,",
    "start": "172130",
    "end": "175640"
  },
  {
    "text": "we give no data\nin this argument.",
    "start": "175640",
    "end": "177380"
  },
  {
    "text": "We just sort of specify the\nhyperparameters of the method.",
    "start": "177380",
    "end": "180140"
  },
  {
    "text": "Once we have constructed\nthe estimator,",
    "start": "180140",
    "end": "182780"
  },
  {
    "text": "we call the fit function.",
    "start": "182780",
    "end": "184920"
  },
  {
    "text": "And after, we can\ncall predict function",
    "start": "184920",
    "end": "187819"
  },
  {
    "text": "to predict on new outcomes.",
    "start": "187820",
    "end": "190650"
  },
  {
    "text": "So one thing slightly\ndifferent about a tree",
    "start": "190650",
    "end": "194720"
  },
  {
    "text": "is people sometimes want to\nvisualize what a regression",
    "start": "194720",
    "end": "197360"
  },
  {
    "text": "tree looks like.",
    "start": "197360",
    "end": "198060"
  },
  {
    "text": "So there's this\nfunction plot_tree",
    "start": "198060",
    "end": "200360"
  },
  {
    "text": "that gives us a visual\nrepresentation of the tree.",
    "start": "200360",
    "end": "203190"
  },
  {
    "text": "And you can see some measure\nof fit in each of the leaves.",
    "start": "203190",
    "end": "208310"
  },
  {
    "text": "The resolution of this\npicture is not great.",
    "start": "208310",
    "end": "211580"
  },
  {
    "text": "And also, you can\ntrace an observation",
    "start": "211580",
    "end": "215780"
  },
  {
    "text": "down the regression tree.",
    "start": "215780",
    "end": "218100"
  },
  {
    "text": "Whoops, let's see if\nI can zoom in on this.",
    "start": "218100",
    "end": "221230"
  },
  {
    "text": "So if the number of\nrooms is less than 6.8,",
    "start": "221230",
    "end": "224730"
  },
  {
    "text": "we'll go to the left.",
    "start": "224730",
    "end": "226230"
  },
  {
    "text": "And then the next variable\nchecked is lstat, et cetera.",
    "start": "226230",
    "end": "229690"
  },
  {
    "text": "And that's how the predictions\nare formed in a decision tree.",
    "start": "229690",
    "end": "233190"
  },
  {
    "start": "233190",
    "end": "236230"
  },
  {
    "text": "So let's look at the accuracy\nof fitting this regression tree.",
    "start": "236230",
    "end": "243430"
  },
  {
    "text": "So just for reference\nlater, we see--",
    "start": "243430",
    "end": "246599"
  },
  {
    "text": "using a regression\ntree, we get about",
    "start": "246600",
    "end": "248550"
  },
  {
    "text": "mean squared error of about 28.",
    "start": "248550",
    "end": "250470"
  },
  {
    "text": "This mean squared error is\nfor here, quote, \"the best",
    "start": "250470",
    "end": "254190"
  },
  {
    "text": "regression tree.\"",
    "start": "254190",
    "end": "254910"
  },
  {
    "text": "And what is best here?",
    "start": "254910",
    "end": "256209"
  },
  {
    "text": "Well, we didn't--",
    "start": "256209",
    "end": "258778"
  },
  {
    "text": "I haven't mentioned it yet.",
    "start": "258779",
    "end": "260049"
  },
  {
    "text": "But we show how to do the\ncost complexity pruning",
    "start": "260050",
    "end": "264659"
  },
  {
    "text": "path in this bit of code.",
    "start": "264660",
    "end": "266020"
  },
  {
    "text": "So we've optimized this\nparameter on the training data",
    "start": "266020",
    "end": "269520"
  },
  {
    "text": "to try and get a\nbetter estimator.",
    "start": "269520",
    "end": "272370"
  },
  {
    "text": "And this is, according\nto that metric,",
    "start": "272370",
    "end": "275490"
  },
  {
    "text": "the best estimator found\nwith the mean squared error",
    "start": "275490",
    "end": "277830"
  },
  {
    "text": "of about 28.",
    "start": "277830",
    "end": "280199"
  },
  {
    "text": "This grid parameter\nwe saw in chapter 6",
    "start": "280200",
    "end": "283740"
  },
  {
    "text": "when we have a tuning\nparameter, this",
    "start": "283740",
    "end": "285750"
  },
  {
    "text": "is a generic method\nin scikit-learn.",
    "start": "285750",
    "end": "288690"
  },
  {
    "text": "You can use to tune\na hyperparameter",
    "start": "288690",
    "end": "291600"
  },
  {
    "text": "for an estimator.",
    "start": "291600",
    "end": "293010"
  },
  {
    "text": "This is our\nestimator regression.",
    "start": "293010",
    "end": "294970"
  },
  {
    "text": "And we specify some arguments\nto the reg estimator.",
    "start": "294970",
    "end": "300880"
  },
  {
    "text": "And then we vary\nthat over the grid",
    "start": "300880",
    "end": "303130"
  },
  {
    "text": "to get this best parameter.",
    "start": "303130",
    "end": "304720"
  },
  {
    "text": "So just to summarize,\nthe general procedure",
    "start": "304720",
    "end": "308320"
  },
  {
    "text": "is to grow bigger\ntree and then use",
    "start": "308320",
    "end": "310330"
  },
  {
    "text": "cross-validation to determine\nhow much to prune it back.",
    "start": "310330",
    "end": "313389"
  },
  {
    "text": "In this case-- and this was all\ndone on the 70% of the data,",
    "start": "313390",
    "end": "318490"
  },
  {
    "text": "which was training data.",
    "start": "318490",
    "end": "319780"
  },
  {
    "text": "And then this final\nestimate of error",
    "start": "319780",
    "end": "322000"
  },
  {
    "text": "for the optimally pruned tree\nis on the 30% of the data, which",
    "start": "322000",
    "end": "327310"
  },
  {
    "text": "is test data.",
    "start": "327310",
    "end": "328090"
  },
  {
    "text": "And we see it's just over 28.",
    "start": "328090",
    "end": "333699"
  },
  {
    "text": "So that's using this-- tuning\nthis way is about as well as we",
    "start": "333700",
    "end": "337900"
  },
  {
    "text": "can do with the single tree.",
    "start": "337900",
    "end": "340120"
  },
  {
    "text": "And we're going to move\non to ensemble methods.",
    "start": "340120",
    "end": "344900"
  },
  {
    "text": "So the first ensemble\nmethod we'll talk about",
    "start": "344900",
    "end": "346900"
  },
  {
    "text": "is bagging and/or random forest.",
    "start": "346900",
    "end": "349389"
  },
  {
    "text": "Remember, the main difference\nbetween bagging and random",
    "start": "349390",
    "end": "352240"
  },
  {
    "text": "forest is random forest randomly\nhas some randomization injected",
    "start": "352240",
    "end": "358020"
  },
  {
    "text": "in which features are\nused for each tree.",
    "start": "358020",
    "end": "360090"
  },
  {
    "text": "But remember, the basic\nscheme of bagging is, well,",
    "start": "360090",
    "end": "364380"
  },
  {
    "text": "what it's--",
    "start": "364380",
    "end": "365460"
  },
  {
    "text": "bagging stands for\nbootstrap aggregation.",
    "start": "365460",
    "end": "367360"
  },
  {
    "text": "So a bootstrap sample\nis taken from the data.",
    "start": "367360",
    "end": "369930"
  },
  {
    "text": "That's a sample\nwith replacement.",
    "start": "369930",
    "end": "371610"
  },
  {
    "text": "A tree is fit to that\nbootstrap sample.",
    "start": "371610",
    "end": "373740"
  },
  {
    "text": "And then the final\nestimator is the average",
    "start": "373740",
    "end": "377069"
  },
  {
    "text": "of these trees fit on\neach bootstrap sample.",
    "start": "377070",
    "end": "380250"
  },
  {
    "text": "The random forest\nadds this wrinkle",
    "start": "380250",
    "end": "382050"
  },
  {
    "text": "of not using every\nfeature on every tree.",
    "start": "382050",
    "end": "385229"
  },
  {
    "text": "So along with taking a random\nsample of the feature--",
    "start": "385230",
    "end": "387870"
  },
  {
    "text": "of the rows of the\ndata set, the cases,",
    "start": "387870",
    "end": "390540"
  },
  {
    "text": "we also take a random\nsample of the features.",
    "start": "390540",
    "end": "392790"
  },
  {
    "text": "Every time a split's taken, we\ntake a sample of the features.",
    "start": "392790",
    "end": "396030"
  },
  {
    "text": "Oh, yes.",
    "start": "396030",
    "end": "397500"
  },
  {
    "text": "And I'm sure there are\nvariations of this.",
    "start": "397500",
    "end": "399775"
  },
  {
    "text": "But, yes.",
    "start": "399775",
    "end": "400275"
  },
  {
    "start": "400275",
    "end": "405900"
  },
  {
    "text": "OK, let's see.",
    "start": "405900",
    "end": "407340"
  },
  {
    "text": "So how do we specify it?",
    "start": "407340",
    "end": "408480"
  },
  {
    "text": "Well, the random forest is\na scikit-learn estimator.",
    "start": "408480",
    "end": "412330"
  },
  {
    "text": "So once we construct\nthe estimator using RF--",
    "start": "412330",
    "end": "418210"
  },
  {
    "text": "this is our shorthand for\nrandom forest regressor--",
    "start": "418210",
    "end": "422110"
  },
  {
    "text": "well, we just have to call\nthe fit method on the training",
    "start": "422110",
    "end": "424900"
  },
  {
    "text": "data.",
    "start": "424900",
    "end": "425949"
  },
  {
    "text": "That is X_train and y_train.",
    "start": "425950",
    "end": "427270"
  },
  {
    "text": "And then we can evaluate-- we\ncan predict on the test data",
    "start": "427270",
    "end": "429910"
  },
  {
    "text": "and evaluate the\naccuracy that way.",
    "start": "429910",
    "end": "432445"
  },
  {
    "text": "OK?",
    "start": "432445",
    "end": "434380"
  },
  {
    "text": "And note here, I've\nput an argument,",
    "start": "434380",
    "end": "436660"
  },
  {
    "text": "random_state equals 0.",
    "start": "436660",
    "end": "438160"
  },
  {
    "text": "Because this is a\nrandom procedure,",
    "start": "438160",
    "end": "440830"
  },
  {
    "text": "there is a random number\ngenerator underlying this.",
    "start": "440830",
    "end": "443349"
  },
  {
    "text": "And setting the state\nmeans that the results",
    "start": "443350",
    "end": "446320"
  },
  {
    "text": "will be reproducible.",
    "start": "446320",
    "end": "447400"
  },
  {
    "text": "So when we run this again,\nwe'll get the same answer.",
    "start": "447400",
    "end": "449680"
  },
  {
    "text": "If you don't specify\na random state,",
    "start": "449680",
    "end": "451375"
  },
  {
    "text": "then the results will\nchange from run to run.",
    "start": "451375",
    "end": "453250"
  },
  {
    "text": "OK.",
    "start": "453250",
    "end": "453750"
  },
  {
    "start": "453750",
    "end": "456730"
  },
  {
    "text": "So just with that\nsimple regressor,",
    "start": "456730",
    "end": "460480"
  },
  {
    "text": "we get a mean\nsquared error of 14.",
    "start": "460480",
    "end": "464710"
  },
  {
    "text": "That's about half that\nwe had on a single tree.",
    "start": "464710",
    "end": "467080"
  },
  {
    "text": "So we see that in terms\nof predictive performance.",
    "start": "467080",
    "end": "470229"
  },
  {
    "text": "Ensemble methods\nwithout very much effort",
    "start": "470230",
    "end": "473410"
  },
  {
    "text": "really outdo a single\ntree very easily.",
    "start": "473410",
    "end": "478030"
  },
  {
    "text": "So let's look-- this is--",
    "start": "478030",
    "end": "483440"
  },
  {
    "text": "I've called this the\nbag version of the tree.",
    "start": "483440",
    "end": "485810"
  },
  {
    "text": "And that's because\nI've put in this note",
    "start": "485810",
    "end": "488090"
  },
  {
    "text": "max_features is the number\nof columns of X_train.",
    "start": "488090",
    "end": "491300"
  },
  {
    "text": "And that essentially tells\nthe random forest regressor",
    "start": "491300",
    "end": "497000"
  },
  {
    "text": "to use every\nfeature when it does",
    "start": "497000",
    "end": "498620"
  },
  {
    "text": "a split so it doesn't do\nany sampling of features",
    "start": "498620",
    "end": "503660"
  },
  {
    "text": "when it splits.",
    "start": "503660",
    "end": "504680"
  },
  {
    "text": "If we made this smaller,\nthen the number of columns,",
    "start": "504680",
    "end": "507449"
  },
  {
    "text": "which is 12-- we've\nmade it smaller,",
    "start": "507450",
    "end": "509070"
  },
  {
    "text": "then it would be\na random forest.",
    "start": "509070",
    "end": "512270"
  },
  {
    "text": "And just to note,\nfor scikit-learn,",
    "start": "512270",
    "end": "516349"
  },
  {
    "text": "the default number of\nfeatures used for splits",
    "start": "516350",
    "end": "520399"
  },
  {
    "text": "varies between the\nregressor random forest",
    "start": "520400",
    "end": "522710"
  },
  {
    "text": "and the classification\nrandom forest.",
    "start": "522710",
    "end": "524540"
  },
  {
    "text": "I think it's p.",
    "start": "524540",
    "end": "525980"
  },
  {
    "text": "The default is to do bagging\nfor regression, I believe,",
    "start": "525980",
    "end": "530870"
  },
  {
    "text": "and square root of p or\nsquare root of the numbers",
    "start": "530870",
    "end": "533288"
  },
  {
    "text": "for classification.",
    "start": "533288",
    "end": "534080"
  },
  {
    "text": "OK.",
    "start": "534080",
    "end": "534590"
  },
  {
    "start": "534590",
    "end": "540480"
  },
  {
    "text": "So another parameter you can\nspecify in the random forest",
    "start": "540480",
    "end": "545190"
  },
  {
    "text": "is the number of trees grown.",
    "start": "545190",
    "end": "546480"
  },
  {
    "text": "The default is 100.",
    "start": "546480",
    "end": "548970"
  },
  {
    "text": "Let's try increasing the\nnumber of trees to 500",
    "start": "548970",
    "end": "551459"
  },
  {
    "text": "and see if we do any better.",
    "start": "551460",
    "end": "553078"
  },
  {
    "text": "We didn't really do any better.",
    "start": "553078",
    "end": "554370"
  },
  {
    "text": "It's about 14.6.",
    "start": "554370",
    "end": "557160"
  },
  {
    "text": "And, of course,\nwhat's happening here",
    "start": "557160",
    "end": "558899"
  },
  {
    "text": "is that we're averaging the\nindependently drawn trees",
    "start": "558900",
    "end": "564180"
  },
  {
    "text": "from the same distribution,\ngiven the data.",
    "start": "564180",
    "end": "566560"
  },
  {
    "text": "So at a certain point,\naveraging will kick in.",
    "start": "566560",
    "end": "569710"
  },
  {
    "text": "And we will no longer\nsee any improvement.",
    "start": "569710",
    "end": "571750"
  },
  {
    "text": "But if we take too few\ntrees, like a single tree,",
    "start": "571750",
    "end": "574740"
  },
  {
    "text": "we will not achieve as\ngood a mean squared error--",
    "start": "574740",
    "end": "579063"
  },
  {
    "text": "as low a mean squared error.",
    "start": "579063",
    "end": "580230"
  },
  {
    "start": "580230",
    "end": "584092"
  },
  {
    "text": "So if we want to have something\nmore like a random forest,",
    "start": "584093",
    "end": "586510"
  },
  {
    "text": "we can decrease the\nnumber of features.",
    "start": "586510",
    "end": "588370"
  },
  {
    "text": "Here we've set the number\nof features equal to 6.",
    "start": "588370",
    "end": "590950"
  },
  {
    "text": "That's bigger than\nthe square root of p.",
    "start": "590950",
    "end": "593110"
  },
  {
    "text": "But this will give\nus some selection",
    "start": "593110",
    "end": "597820"
  },
  {
    "text": "of features for every split.",
    "start": "597820",
    "end": "600010"
  },
  {
    "text": "And we see here that\nit actually degrades",
    "start": "600010",
    "end": "601870"
  },
  {
    "text": "the performance a little bit.",
    "start": "601870",
    "end": "603520"
  },
  {
    "text": "It's 20 instead of 14.6.",
    "start": "603520",
    "end": "608350"
  },
  {
    "text": "One of the nice things\nabout random forests",
    "start": "608350",
    "end": "611649"
  },
  {
    "text": "is that you can--",
    "start": "611650",
    "end": "613120"
  },
  {
    "text": "there's a measure of\nimportance for each feature",
    "start": "613120",
    "end": "615580"
  },
  {
    "text": "that you can use\nto sort of post-hoc",
    "start": "615580",
    "end": "618790"
  },
  {
    "text": "analyze which features seem\nto be important in producing",
    "start": "618790",
    "end": "622420"
  },
  {
    "text": "these predictions.",
    "start": "622420",
    "end": "623630"
  },
  {
    "start": "623630",
    "end": "626600"
  },
  {
    "text": "This measures these sort of\naverage improvement gain--",
    "start": "626600",
    "end": "629350"
  },
  {
    "text": "whenever we added\na split on lstat,",
    "start": "629350",
    "end": "631660"
  },
  {
    "text": "there are some improvement\nin mean squared error.",
    "start": "631660",
    "end": "634089"
  },
  {
    "text": "And this is, in some sense,\nthe average improvement",
    "start": "634090",
    "end": "636850"
  },
  {
    "text": "over whenever we had done a\nsplit in any of the trees.",
    "start": "636850",
    "end": "640250"
  },
  {
    "text": "So it's saying that lstat\nand a number of rooms",
    "start": "640250",
    "end": "643790"
  },
  {
    "text": "are by far the most\nimportant features.",
    "start": "643790",
    "end": "646100"
  },
  {
    "text": "And all the rest seem to be\nmuch smaller in comparison.",
    "start": "646100",
    "end": "648680"
  },
  {
    "text": "Yes.",
    "start": "648680",
    "end": "649250"
  },
  {
    "text": "Yeah, yeah.",
    "start": "649250",
    "end": "649940"
  },
  {
    "text": "And that's-- if we think back\nto that plot of our single tree,",
    "start": "649940",
    "end": "654590"
  },
  {
    "text": "lstat and room were also the\nfirst few features we split on.",
    "start": "654590",
    "end": "657240"
  },
  {
    "text": "So these seem to be important\nfor prediction here.",
    "start": "657240",
    "end": "659810"
  },
  {
    "start": "659810",
    "end": "666550"
  },
  {
    "text": "So the other ensemble\nmethod that we'll talk about",
    "start": "666550",
    "end": "670209"
  },
  {
    "text": "is boosting.",
    "start": "670210",
    "end": "671320"
  },
  {
    "text": "And so both boosting\nand random forest",
    "start": "671320",
    "end": "675220"
  },
  {
    "text": "use a collection of trees.",
    "start": "675220",
    "end": "679189"
  },
  {
    "text": "One of the big differences\nbetween random forest",
    "start": "679190",
    "end": "681190"
  },
  {
    "text": "and boosting is the type\nof trees fit at each stage.",
    "start": "681190",
    "end": "684370"
  },
  {
    "text": "In random forest, at\neach stage, the tree fit",
    "start": "684370",
    "end": "689830"
  },
  {
    "text": "is the same as the first stage\nbecause, once we fix the data,",
    "start": "689830",
    "end": "693565"
  },
  {
    "text": "a bootstrap sample at\nany given time is--",
    "start": "693565",
    "end": "696760"
  },
  {
    "text": "a bootstrap sample at stage 1\nis the same as the bootstrap",
    "start": "696760",
    "end": "699670"
  },
  {
    "text": "sample at the 500th stage.",
    "start": "699670",
    "end": "702430"
  },
  {
    "text": "In boosting, what happens--",
    "start": "702430",
    "end": "704290"
  },
  {
    "text": "there can be resampling.",
    "start": "704290",
    "end": "706570"
  },
  {
    "text": "It depends on the\nimplementation.",
    "start": "706570",
    "end": "708080"
  },
  {
    "text": "But what changes-- as\nyou go through the stages",
    "start": "708080",
    "end": "710740"
  },
  {
    "text": "is we're fitting trees to the\nresidual after having removed",
    "start": "710740",
    "end": "714490"
  },
  {
    "text": "some of the effect of the-- up\nto that point the fitted model.",
    "start": "714490",
    "end": "721300"
  },
  {
    "text": "So, Jonathan, unlike\nin random forest,",
    "start": "721300",
    "end": "723610"
  },
  {
    "text": "where once you fit\nenough trees, things",
    "start": "723610",
    "end": "726320"
  },
  {
    "text": "don't change-- with boosting,\nthe more trees you fit,",
    "start": "726320",
    "end": "728660"
  },
  {
    "text": "the more you're going to\nstart overfitting the data.",
    "start": "728660",
    "end": "731079"
  },
  {
    "text": "And so there really is going\nto be a bias variance trade-off",
    "start": "731080",
    "end": "733580"
  },
  {
    "text": "in the number of trees.",
    "start": "733580",
    "end": "735078"
  },
  {
    "text": "That's right.",
    "start": "735078",
    "end": "735620"
  },
  {
    "text": "And we'll see a plot\nof that coming up.",
    "start": "735620",
    "end": "738960"
  },
  {
    "text": "As the number of estimators,\nthat's the number of trees,",
    "start": "738960",
    "end": "741380"
  },
  {
    "text": "grows, we can plot both the test\nerror and the training error.",
    "start": "741380",
    "end": "744680"
  },
  {
    "text": "And we'll see what Trevor\njust mentioned coming up.",
    "start": "744680",
    "end": "749070"
  },
  {
    "text": "But in terms of\nfitting these models,",
    "start": "749070",
    "end": "751310"
  },
  {
    "text": "once we specify the\nhyperparameters,",
    "start": "751310",
    "end": "753080"
  },
  {
    "text": "again, since it's\nan sklearn method,",
    "start": "753080",
    "end": "756440"
  },
  {
    "text": "we use the fit method\nto fit on training data.",
    "start": "756440",
    "end": "762300"
  },
  {
    "text": "And we can predict on test data.",
    "start": "762300",
    "end": "765240"
  },
  {
    "text": "So let's make that plot of\nprediction of training error",
    "start": "765240",
    "end": "769910"
  },
  {
    "text": "and test error as\na function of tree.",
    "start": "769910",
    "end": "772129"
  },
  {
    "text": "And to do that, we can use\nthis for a boosting estimator.",
    "start": "772130",
    "end": "776750"
  },
  {
    "text": "There's this thing\ncalled staged_predict,",
    "start": "776750",
    "end": "779030"
  },
  {
    "text": "which, as its name suggests,\nit predicts in stages.",
    "start": "779030",
    "end": "783500"
  },
  {
    "text": "And the stages are--",
    "start": "783500",
    "end": "784730"
  },
  {
    "text": "as we go through the trees,\nfit in the boosting process.",
    "start": "784730",
    "end": "788430"
  },
  {
    "text": "So the residual\nchanges and get smaller",
    "start": "788430",
    "end": "792649"
  },
  {
    "text": "as the training\nresidual gets smaller",
    "start": "792650",
    "end": "796160"
  },
  {
    "text": "as we go along the stages.",
    "start": "796160",
    "end": "799620"
  },
  {
    "text": "So the training\nerror should go down.",
    "start": "799620",
    "end": "802190"
  },
  {
    "text": "And at some point, test\nerror will possibly go up.",
    "start": "802190",
    "end": "806040"
  },
  {
    "text": "So let's just look at the plot.",
    "start": "806040",
    "end": "809540"
  },
  {
    "text": "After 5,000 trees here,\nwe see training error",
    "start": "809540",
    "end": "813649"
  },
  {
    "text": "has sort of begun to saturate.",
    "start": "813650",
    "end": "816140"
  },
  {
    "text": "We didn't see here a real\nincrease in overfitting",
    "start": "816140",
    "end": "819620"
  },
  {
    "text": "on this data set.",
    "start": "819620",
    "end": "820589"
  },
  {
    "text": "But that doesn't mean that\nboosting will never overfit.",
    "start": "820590",
    "end": "823190"
  },
  {
    "text": "It's just on this particular\ndata set, it has not.",
    "start": "823190",
    "end": "826250"
  },
  {
    "text": "It's possible that we would\nsee some uptick in test error",
    "start": "826250",
    "end": "829160"
  },
  {
    "text": "as we grow the number of trees.",
    "start": "829160",
    "end": "831019"
  },
  {
    "text": "In this data set, we do not.",
    "start": "831020",
    "end": "832340"
  },
  {
    "start": "832340",
    "end": "836020"
  },
  {
    "text": "So let's take a look at our\nmean squared error on the test",
    "start": "836020",
    "end": "840220"
  },
  {
    "text": "data at the final stage.",
    "start": "840220",
    "end": "842300"
  },
  {
    "text": "And it's about 14.5, which\nis just slightly better",
    "start": "842300",
    "end": "846088"
  },
  {
    "text": "than random forest.",
    "start": "846088",
    "end": "846880"
  },
  {
    "text": "But they're very\ncomparable here.",
    "start": "846880",
    "end": "848080"
  },
  {
    "text": "Than the bagging version--",
    "start": "848080",
    "end": "849310"
  },
  {
    "text": "Oh, the bagging version\nof random forest.",
    "start": "849310",
    "end": "850630"
  },
  {
    "text": "That's right.",
    "start": "850630",
    "end": "851260"
  },
  {
    "text": "OK.",
    "start": "851260",
    "end": "851760"
  },
  {
    "start": "851760",
    "end": "855710"
  },
  {
    "text": "So the last thing\nwe can talk about",
    "start": "855710",
    "end": "860750"
  },
  {
    "text": "is there are other parameters.",
    "start": "860750",
    "end": "862560"
  },
  {
    "text": "And if you look at\nthe help for GBR,",
    "start": "862560",
    "end": "865430"
  },
  {
    "text": "you'll see many\nparameters you can set,",
    "start": "865430",
    "end": "868130"
  },
  {
    "text": "one of them being\nthe learning rate.",
    "start": "868130",
    "end": "870110"
  },
  {
    "text": "And here we just change\nthe learning rate just",
    "start": "870110",
    "end": "875360"
  },
  {
    "text": "to show you that you can\nchange several things",
    "start": "875360",
    "end": "878240"
  },
  {
    "text": "on the boosting estimator.",
    "start": "878240",
    "end": "879652"
  },
  {
    "text": "And, actually, in this case,\nit does not make a difference.",
    "start": "879652",
    "end": "882110"
  },
  {
    "text": "That doesn't mean it will\nnever make a difference.",
    "start": "882110",
    "end": "883790"
  },
  {
    "text": "But there are huge\nnumber of tuning effects,",
    "start": "883790",
    "end": "885649"
  },
  {
    "text": "tuning parameters.",
    "start": "885650",
    "end": "886710"
  },
  {
    "text": "And I think some\nimplementation of boosting also",
    "start": "886710",
    "end": "889910"
  },
  {
    "text": "do some resampling as\nwell as they go along.",
    "start": "889910",
    "end": "892699"
  },
  {
    "text": "So the general\nflavor of boosting",
    "start": "892700",
    "end": "894770"
  },
  {
    "text": "is fitting a tree\nto residual compared",
    "start": "894770",
    "end": "897410"
  },
  {
    "text": "to the general flavor\nof random forests, which",
    "start": "897410",
    "end": "899930"
  },
  {
    "text": "is fitting an identical tree\nto some randomized version",
    "start": "899930",
    "end": "903470"
  },
  {
    "text": "of the data at every stage.",
    "start": "903470",
    "end": "905750"
  },
  {
    "text": "Beyond that, there\nare different details",
    "start": "905750",
    "end": "907910"
  },
  {
    "text": "in their implementation.",
    "start": "907910",
    "end": "910940"
  },
  {
    "text": "We're not going to--",
    "start": "910940",
    "end": "912110"
  },
  {
    "text": "the final topic is Bayesian\nadditive regression trees.",
    "start": "912110",
    "end": "915149"
  },
  {
    "text": "We're not going to go\nin-- we're not going to--",
    "start": "915150",
    "end": "918000"
  },
  {
    "text": "We're not going to\ntalk about those.",
    "start": "918000",
    "end": "920190"
  },
  {
    "text": "And this is a good place to\nwrap up the lab for chapter 8.",
    "start": "920190",
    "end": "923790"
  },
  {
    "text": "Thank you.",
    "start": "923790",
    "end": "925820"
  },
  {
    "start": "925820",
    "end": "930000"
  }
]