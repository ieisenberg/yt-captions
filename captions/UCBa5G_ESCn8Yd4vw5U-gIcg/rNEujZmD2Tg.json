[
  {
    "start": "0",
    "end": "5250"
  },
  {
    "text": "Welcome to lecture 2. The plan for today is to\ntalk a little bit about what",
    "start": "5250",
    "end": "16960"
  },
  {
    "text": "a generative model is. And we're going to see that we\nencounter the first challenge",
    "start": "16960",
    "end": "25740"
  },
  {
    "text": "whenever we want to build a\ngenerative model of complex data sets like images,\ntexts, which is",
    "start": "25740",
    "end": "31140"
  },
  {
    "text": "the usual curse\nof dimensionality that you might have seen before\nin other machine learning classes.",
    "start": "31140",
    "end": "36690"
  },
  {
    "text": "And so the plan for\ntoday is to discuss a little bit various ways\nthat at a very high level",
    "start": "36690",
    "end": "45264"
  },
  {
    "text": "that people have\ncome up with to deal with the curse of\ndimensionality. And so we'll do a\nvery brief crash",
    "start": "45264",
    "end": "51720"
  },
  {
    "text": "course on graphical models. This is like my CS 228\nclass or a part of it",
    "start": "51720",
    "end": "57839"
  },
  {
    "text": "compressed in a single\nlecture or half of a lecture. And then we'll talk a little\nbit about the distinction",
    "start": "57840",
    "end": "65199"
  },
  {
    "text": "between a generative model\nand a discriminative model, which is something\nagain, you might have seen before in ML classes.",
    "start": "65200",
    "end": "71630"
  },
  {
    "text": "And finally, we'll\nget into the deep part of the deep generative\nmodels and we'll start to see how you can\nuse neural networks to deal",
    "start": "71630",
    "end": "78910"
  },
  {
    "text": "with the curse of\ndimensionality. So, all right. So this is going to be\na high-level picture,",
    "start": "78910",
    "end": "86440"
  },
  {
    "text": "a high-level overview\nthat roughly corresponds to a lot of the ideas\nthat we're going",
    "start": "86440",
    "end": "93130"
  },
  {
    "text": "to talk about in this course. And it deals with this problem\nof learning a generative model,",
    "start": "93130",
    "end": "99130"
  },
  {
    "text": "the kind of challenges\nthat you encounter and the kind of different\nways you can address them. And by changing different\npieces, in this picture,",
    "start": "99130",
    "end": "107320"
  },
  {
    "text": "you're going to get different\nclasses of generative models. You might get\nautoregressive models",
    "start": "107320",
    "end": "113079"
  },
  {
    "text": "like the ones that are\nusually used for language. You might get diffusion\nmodels, generative, adversarial networks,\nthese kinds of things",
    "start": "113080",
    "end": "120700"
  },
  {
    "text": "by changing ingredients into\nthis high-level picture. So this picture will\ncome up several times",
    "start": "120700",
    "end": "126880"
  },
  {
    "text": "throughout this quarter. And it deals with this\nbasic problem that you have whenever you want to\ntrain a generative model.",
    "start": "126880",
    "end": "133480"
  },
  {
    "text": "So the basic\nproblem is one where you're given a set of examples.",
    "start": "133480",
    "end": "139209"
  },
  {
    "text": "This might be images or\nit might be sentences that you've collected\non the internet,",
    "start": "139210",
    "end": "145300"
  },
  {
    "text": "or it could be DNA sequences. It could really be anything.",
    "start": "145300",
    "end": "151330"
  },
  {
    "text": "The assumption is\nthat these data points that you have\naccess to are sampled",
    "start": "151330",
    "end": "157420"
  },
  {
    "text": "from some unknown\nprobability distribution that we're often going\nto denote p data.",
    "start": "157420",
    "end": "164269"
  },
  {
    "text": "This is like\ndata-generating process. It's some kind of a complicated\nunknown process that",
    "start": "164270",
    "end": "172360"
  },
  {
    "text": "has generated the data for you. And so the assumption is\nthat all these different data points that you have access\nto are related to each other",
    "start": "172360",
    "end": "180130"
  },
  {
    "text": "because they come from some\ntrue underlying common data",
    "start": "180130",
    "end": "185560"
  },
  {
    "text": "generating process. And in the case\nof language, this might correspond to maybe if you\nhave a corpora of text collected",
    "start": "185560",
    "end": "193710"
  },
  {
    "text": "from the internet, this might\ncorrespond to the different ways people write text for\nwebsites or for whatever sites",
    "start": "193710",
    "end": "203160"
  },
  {
    "text": "you've scraped to\ncollect your data set, or it might correspond to the\ncomplicated physical processes",
    "start": "203160",
    "end": "209099"
  },
  {
    "text": "that give rise to a natural\ndistribution over images. The key point here is that\nthis data distribution",
    "start": "209100",
    "end": "215655"
  },
  {
    "text": "Pdata is unknown. You assume there\nis such an object, but the only thing\nyou have access to",
    "start": "215655",
    "end": "222210"
  },
  {
    "text": "are a bunch of examples\nor a bunch of samples from this distribution. And the whole\nproblem in this class",
    "start": "222210",
    "end": "230550"
  },
  {
    "text": "and in the space of generative\nmodels and generative AI is to basically come up\nwith a good approximation",
    "start": "230550",
    "end": "236310"
  },
  {
    "text": "of this data-generating\nprocess because the idea is that if you have access\nto a good approximation",
    "start": "236310",
    "end": "242790"
  },
  {
    "text": "to this data generating\nprocess, this data distribution Pdata, then you can sample\nfrom this approximation",
    "start": "242790",
    "end": "249310"
  },
  {
    "text": "that you have access to and\nyou can generate new text. Or if you have a\ndistribution over images,",
    "start": "249310",
    "end": "256450"
  },
  {
    "text": "then you can sample\nfrom the distribution and you can generate new\nimages that hopefully are close to the ones you've\nused for training or model",
    "start": "256450",
    "end": "265240"
  },
  {
    "text": "to the extent that\nyou're doing a good job. By coming up with a good\napproximation of this data distribution,\nhopefully, your samples",
    "start": "265240",
    "end": "271750"
  },
  {
    "text": "are also going to be good. And so in order to\ndo that, you need to define a model family, which\nis this set here in green.",
    "start": "271750",
    "end": "280330"
  },
  {
    "text": "And you can think of this as\na set of different probability distributions that are\nindexed or parameterized",
    "start": "280330",
    "end": "287139"
  },
  {
    "text": "with this variable theta. So think of it as all possible\nGaussian distributions",
    "start": "287140",
    "end": "292840"
  },
  {
    "text": "that you can get as you change\nthe mean and the covariance or all the kinds\nof distributions that you can get as you\nchange the parameters",
    "start": "292840",
    "end": "299410"
  },
  {
    "text": "of your neural network. And once you've\ndefined this set,",
    "start": "299410",
    "end": "305030"
  },
  {
    "text": "the goal becomes that\nof trying to find a good approximation of the data\ndistribution within the set.",
    "start": "305030",
    "end": "310460"
  },
  {
    "text": "And so in order to\ndo that, you need to define some\nnotion of distance. So you need to define\na loss function.",
    "start": "310460",
    "end": "317150"
  },
  {
    "text": "You need to specify\nwhat you care about and what you don't care about. Like these objects, this\nprobability distribution,",
    "start": "317150",
    "end": "323150"
  },
  {
    "text": "the data distribution, and\nyour model distributions are going to be pretty complex. They are defined over\nhigh-dimensional spaces.",
    "start": "323150",
    "end": "329669"
  },
  {
    "text": "So there's a lot of\ndifferent-- let's say, images you can assign probability to. And you somehow need to\nspecify what you care about",
    "start": "329670",
    "end": "337220"
  },
  {
    "text": "and what you don't,\nor equivalently, you need to specify some notion\nof distance or similarity",
    "start": "337220",
    "end": "343040"
  },
  {
    "text": "between the data\ndistribution and your model. And then you have an\noptimization problem then it becomes a\nquestion of, how",
    "start": "343040",
    "end": "350270"
  },
  {
    "text": "do you find the\ndistribution in your set, in your model family that is as\nclose as possible to the data",
    "start": "350270",
    "end": "355639"
  },
  {
    "text": "distribution? And so you try to\nfind this projection and try to find this point. And then hopefully, if you can\nsolve this potentially hard",
    "start": "355640",
    "end": "364160"
  },
  {
    "text": "optimization problem, you\ncome up with your model, you come up with a\ndistribution that is hopefully relatively close\nto your data distribution.",
    "start": "364160",
    "end": "372810"
  },
  {
    "text": "And again, then you can use it. Then you have your\nlanguage model or you have your\ndiffusion model. And you can use it\nto generate images,",
    "start": "372810",
    "end": "379169"
  },
  {
    "text": "you can use it to\ngenerate text, you can do many different things. And so we see that there\nare several components here.",
    "start": "379170",
    "end": "390660"
  },
  {
    "text": "You always need to\nstart with some data. Then you need to\ndefine a model family and then you need to define a\nloss function or a similarity",
    "start": "390660",
    "end": "399560"
  },
  {
    "text": "metric between distributions\nthat you should optimize over. And what we'll\nsee is that you're",
    "start": "399560",
    "end": "404690"
  },
  {
    "text": "going to get different\nclasses of generative models by changing these\nkinds of ingredients.",
    "start": "404690",
    "end": "409880"
  },
  {
    "text": "And the issue here\nis that it's not straightforward to\ncome up with-- it's not",
    "start": "409880",
    "end": "416540"
  },
  {
    "text": "an optimal solution here. And that's why there\nare many different kinds of generative models, which is\nnot clear what's the right model",
    "start": "416540",
    "end": "424670"
  },
  {
    "text": "family that we\nshould use, it's not clear what's the right\nnotion of similarity that we should use,\nfor example, if you",
    "start": "424670",
    "end": "430379"
  },
  {
    "text": "think about different\ndata modalities. So that's why we're going\nto see different families of the generative models\nthat will essentially",
    "start": "430380",
    "end": "437520"
  },
  {
    "text": "make different choices with\nrespect to the model family, with respect to the\nloss, and so forth.",
    "start": "437520",
    "end": "442920"
  },
  {
    "text": "But at the end of the day,\npretty much all of the models we'll see, we'll try to learn\nthis probability distribution.",
    "start": "442920",
    "end": "452047"
  },
  {
    "text": "And this is again useful because\nonce you have this probability",
    "start": "452047",
    "end": "457350"
  },
  {
    "text": "distribution, you\ncan sample from it, you can generate new data,\nyou can do density estimation.",
    "start": "457350",
    "end": "464320"
  },
  {
    "text": "So if you have access to a\nprobability distribution, then you can query your\nprobability distribution",
    "start": "464320",
    "end": "470940"
  },
  {
    "text": "for any input x and\nthe model can tell you how likely this object is.",
    "start": "470940",
    "end": "476380"
  },
  {
    "text": "So if you train a model over\na bunch of images of dogs, then you come up with this\np theta, this distribution",
    "start": "476380",
    "end": "481530"
  },
  {
    "text": "here that this is as close\nas possible to the data distribution. Then you can fit in a\nnew image and the model",
    "start": "481530",
    "end": "487289"
  },
  {
    "text": "will tell you how likely was it\nthat this image was generated",
    "start": "487290",
    "end": "492360"
  },
  {
    "text": "basically by this\nmodel distribution that you've come up with. And this is potentially useful\nbecause you can do, for example,",
    "start": "492360",
    "end": "499830"
  },
  {
    "text": "anomaly detection. You can do-- you can check how\nlikely that object is and you can start reasoning about\nyour own about the inputs",
    "start": "499830",
    "end": "507360"
  },
  {
    "text": "that your models are seeing. You can identify anomalies. You can do many\ninteresting things",
    "start": "507360",
    "end": "512559"
  },
  {
    "text": "once you have\naccess to a density. And finally, I know\nthis is also useful",
    "start": "512559",
    "end": "521140"
  },
  {
    "text": "because essentially,\nit's a clean way to think about\nunsupervised learning.",
    "start": "521140",
    "end": "527529"
  },
  {
    "text": "If you think about\nit, if you're trying to build a model that assigns\nhigh probability to images that",
    "start": "527530",
    "end": "534610"
  },
  {
    "text": "look like the ones you\nhave in your training set, that again in order\nto do well, you",
    "start": "534610",
    "end": "540490"
  },
  {
    "text": "need to understand what all\nthese images have in common. And so maybe in\nthis example, you might need to understand\nwhat does a dog look like,",
    "start": "540490",
    "end": "548810"
  },
  {
    "text": "what kind of parts you need\nto have, what kind of colors exist in the real world,\nwhich ones don't, and things",
    "start": "548810",
    "end": "556450"
  },
  {
    "text": "like that. And so implicitly by training\nthese models, perhaps on large quantities\nof unlabeled data,",
    "start": "556450",
    "end": "564010"
  },
  {
    "text": "you end up learning the\nstructure of the data. You end up learning what\nall these data points have in common.",
    "start": "564010",
    "end": "569090"
  },
  {
    "text": "You end up learning what\nare the axes of variation that this data set has.",
    "start": "569090",
    "end": "576140"
  },
  {
    "text": "And this is useful because it\nallows you to, for example, essentially discover features\nin an unsupervised way.",
    "start": "576140",
    "end": "582760"
  },
  {
    "text": "And so we'll see\nthat at least some of the generative\nmodels we'll talk about will actually allow you\nexplicitly to recover",
    "start": "582760",
    "end": "589480"
  },
  {
    "text": "features for the data points. And you can use them to\ndo controllable generation",
    "start": "589480",
    "end": "594670"
  },
  {
    "text": "or you can use them to do-- maybe semi-supervised\nlearning or few shot learning. Once you have good\nfeatures, it should",
    "start": "594670",
    "end": "601240"
  },
  {
    "text": "be relatively easy\nto, let's say, distinguish different breeds\nof dogs and things like that.",
    "start": "601240",
    "end": "607390"
  },
  {
    "text": "So that's the high-level story. And we'll see all these\ndifferent components",
    "start": "607390",
    "end": "612940"
  },
  {
    "text": "in much detail\nthroughout the course. The first big question is, how\ndo you represent a probability",
    "start": "612940",
    "end": "620380"
  },
  {
    "text": "distribution, and\nhow do we actually come up with a reasonable set\nover which we can optimize",
    "start": "620380",
    "end": "627760"
  },
  {
    "text": "if we want to recover a good\napproximation to the data distribution? And this is not\ngoing to be trivial",
    "start": "627760",
    "end": "633190"
  },
  {
    "text": "because we care about objects\nthat are pretty complicated in the sense that they have--",
    "start": "633190",
    "end": "639899"
  },
  {
    "text": "if you think about an image,\nit's going to have many pixels, or if you think about\ntext, we typically care about many tokens.",
    "start": "639900",
    "end": "646180"
  },
  {
    "text": "And so representing a\nprobability distribution over a high-dimensional\nspace is actually nontrivial.",
    "start": "646180",
    "end": "653920"
  },
  {
    "text": "And that's the first\nchallenge where you need to start making trade-offs. And if you're dealing\nwith low-dimensional data,",
    "start": "653920",
    "end": "663250"
  },
  {
    "text": "then the problem is not hard. And this is something you\nmight have seen before. If you have, let's say, a single\ndiscrete, random variable,",
    "start": "663250",
    "end": "673190"
  },
  {
    "text": "perhaps a binary\nrandom variable, then it's not hard to describe\nall the different things that can happen and assign\nprobabilities to these events.",
    "start": "673190",
    "end": "681570"
  },
  {
    "text": "So if you have a Bernoulli\ndistribution or a Bernoulli random variable, then you\nonly have two outcomes--",
    "start": "681570",
    "end": "687230"
  },
  {
    "text": "true/false, heads or\ntails, something like that. And in order to specify all the\npossible things that can happen,",
    "start": "687230",
    "end": "693920"
  },
  {
    "text": "you just need one parameter. You just need a single\nnumber, which tells you the probability of heads.",
    "start": "693920",
    "end": "699084"
  },
  {
    "text": "The probability\nof a tail is just going to be 1\nminus the number p. And learning these distributions\nfrom data is of course trivial.",
    "start": "699085",
    "end": "710449"
  },
  {
    "text": "And it's useful, but\nthis is not quite going to be enough to\ndeal with, let's say,",
    "start": "710450",
    "end": "717380"
  },
  {
    "text": "models over images\nor models over text. The other building block\nthat we're going to use",
    "start": "717380",
    "end": "725075"
  },
  {
    "text": "are categorical distributions. So if you have more\nthan two outcomes, you have, let's say, k\ndifferent outcomes, then you're",
    "start": "725075",
    "end": "733160"
  },
  {
    "text": "dealing with a categorical\nrandom variable, or you have m different\noutcomes here.",
    "start": "733160",
    "end": "739009"
  },
  {
    "text": "And again, this is a\nuseful building block. You can use it to model\nthings like rolling a die,",
    "start": "739010",
    "end": "745100"
  },
  {
    "text": "many other things. The challenge here\nor where you're starting to see where\nthe issues might arise",
    "start": "745100",
    "end": "754340"
  },
  {
    "text": "is that again, you\nbasically need, if you have m different\nthings that can happen,",
    "start": "754340",
    "end": "760460"
  },
  {
    "text": "you need to specify\na probability for each one of them. And so you basically\nneed to have m numbers,",
    "start": "760460",
    "end": "767370"
  },
  {
    "text": "and then these\nnumbers have to sum to 1 because it's a valid\nprobability distribution.",
    "start": "767370",
    "end": "773290"
  },
  {
    "text": "And if you sum all\nthe probabilities of all the different things that\ncan happen, you have to get 1.",
    "start": "773290",
    "end": "778980"
  },
  {
    "text": "And so these are the\ntwo building blocks. And then you can combine them to\nmodel more interesting objects.",
    "start": "778980",
    "end": "787090"
  },
  {
    "text": "So let's say you want to build\na generative model over images, then you're going to have to\nmodel many different pixels.",
    "start": "787090",
    "end": "794520"
  },
  {
    "text": "And to model the color\nof a single pixel, perhaps you're going\nto use some kind of RGB",
    "start": "794520",
    "end": "802560"
  },
  {
    "text": "encoding where you're going to\nhave to specify three numbers. You're going to have to\nspecify the intensity",
    "start": "802560",
    "end": "807930"
  },
  {
    "text": "of the red channel,\nwhich let's say is a number between 0 and 255.",
    "start": "807930",
    "end": "813069"
  },
  {
    "text": "You're going to have to specify\na green channel intensity and a blue channel intensity. So you can imagine that with\nthese three random variables,",
    "start": "813070",
    "end": "821190"
  },
  {
    "text": "you're going to capture the\nspace of possible colors that has been discretized\naccording to that granularity",
    "start": "821190",
    "end": "828705"
  },
  {
    "text": "that you've chosen. And now you are able to describe\nmany different colors that you",
    "start": "828705",
    "end": "835040"
  },
  {
    "text": "can get for that particular\npixel, each one corresponding to an entry in this cube.",
    "start": "835040",
    "end": "841399"
  },
  {
    "text": "And so now we have\na richer model. And if you somehow are able to\nmodel this distribution well,",
    "start": "841400",
    "end": "848570"
  },
  {
    "text": "so you are able to\nassign probabilities to all these entries, to\nall these different colors that this individual pixel can\ntake, then if you were to sample",
    "start": "848570",
    "end": "857180"
  },
  {
    "text": "from it, then you\nwould generate values,",
    "start": "857180",
    "end": "862279"
  },
  {
    "text": "colors for that pixel\nthat are reasonable. Hopefully, they match\nwhatever training data you had access to\nlearn this distribution.",
    "start": "862280",
    "end": "871100"
  },
  {
    "text": "And how many\nparameters do you need to specify this joint\nprobability distribution?",
    "start": "871100",
    "end": "877230"
  },
  {
    "start": "877230",
    "end": "882360"
  },
  {
    "text": "Three parameters. Other guesses? How many different\nthings can happen here?",
    "start": "882360",
    "end": "890810"
  },
  {
    "text": "There are basically 256 times\n256 times 256 different colors",
    "start": "890810",
    "end": "896240"
  },
  {
    "text": "that we're able to capture. And so if you want\nto be fully general, you have to specify\na probability",
    "start": "896240",
    "end": "902470"
  },
  {
    "text": "for each one of them. So there's basically 256\ncube entries in the cube,",
    "start": "902470",
    "end": "909130"
  },
  {
    "text": "and you have to be able to\nassign a nonnegative number to each one of them. And then, OK, you know that\nthey all have to sum to 1.",
    "start": "909130",
    "end": "917339"
  },
  {
    "text": "So you have slightly\nless parameters to fit, but it's still a\nreasonably high number.",
    "start": "917340",
    "end": "924185"
  },
  {
    "text": " So here you start\nto see the issue with having multiple\nrandom variables where",
    "start": "924185",
    "end": "932490"
  },
  {
    "text": "the space of possible outcomes,\nthe possible things that can happen it\ngrows exponentially",
    "start": "932490",
    "end": "937500"
  },
  {
    "text": "in however many random\nvariables you want to model. And so as another\nexample, now let's",
    "start": "937500",
    "end": "944010"
  },
  {
    "text": "say you want to model a\ndistribution over images, And for simplicity, let's\nsay the images are just black and white.",
    "start": "944010",
    "end": "950100"
  },
  {
    "text": "So you're going to model\nan image as a collection of random variables. There's going to be one random\nvariable for every pixel.",
    "start": "950100",
    "end": "957720"
  },
  {
    "text": "Maybe there is 28\ntimes 28 pixels. Each pixel by itself is a\nBernoulli random variable.",
    "start": "957720",
    "end": "964600"
  },
  {
    "text": "It can either be on or\noff, white or black. And let's say you have a\ntraining set, maybe MNIST.",
    "start": "964600",
    "end": "972310"
  },
  {
    "text": "You have a bunch of images\nof handwritten digits and that's your training set.",
    "start": "972310",
    "end": "977800"
  },
  {
    "text": "And then you would like to\nlearn a probability distribution over all these\nblack-and-white images.",
    "start": "977800",
    "end": "984640"
  },
  {
    "text": "So how do you represent it? Well, again, we have this\ncollection of random variables,",
    "start": "984640",
    "end": "989920"
  },
  {
    "text": "and they're all binary. And we have n of them, where\nn is the number of pixels",
    "start": "989920",
    "end": "994960"
  },
  {
    "text": "that you have in the image. So it depends on the resolution. And you can think about how\nmany different images are there,",
    "start": "994960",
    "end": "1003750"
  },
  {
    "text": "how many different black\nand white images are there with n pixels. 2 to the 28th squared.",
    "start": "1003750",
    "end": "1009620"
  },
  {
    "text": "Yeah, 2 to the-- whatever, 2 to the number\nof pixels that you have. So there's two possible\ncolors, two possible values",
    "start": "1009620",
    "end": "1017640"
  },
  {
    "text": "of the first pixel\ncan take times 2 in the second times\n2 times 2 times 2. You do it n times and you\nend up with 2 to the n.",
    "start": "1017640",
    "end": "1026250"
  },
  {
    "text": "So there is a huge number\nof different images. Even in this simple\nscenario where",
    "start": "1026250",
    "end": "1032140"
  },
  {
    "text": "they are just black and white-- very large state space,\nsometimes it's called.",
    "start": "1032140",
    "end": "1038380"
  },
  {
    "text": "And so if you want it to be-- somehow if you are able to\ncome up with this model,",
    "start": "1038380",
    "end": "1043900"
  },
  {
    "text": "somehow you are able to come up\nwith a probability distribution over this binary\nrandom variables, then you have this object\nthat given any input image,",
    "start": "1043900",
    "end": "1051520"
  },
  {
    "text": "it will tell you how likely\nit is according to the model. And if you can\nsample from it, you can assign values\nto all the pixels",
    "start": "1051520",
    "end": "1058100"
  },
  {
    "text": "then it will generate an image. And if you've done\na good job again at learning the distribution,\nit will generate,",
    "start": "1058100",
    "end": "1063940"
  },
  {
    "text": "let's say, images that\nlook like the ones that you had in\nthe training set. So they will look, let's\nsay like an MNIST digits.",
    "start": "1063940",
    "end": "1070335"
  },
  {
    "text": " But again, you see the issue\nis how many parameters do you",
    "start": "1070335",
    "end": "1076020"
  },
  {
    "text": "need to specify this\nobject in full generality?",
    "start": "1076020",
    "end": "1085270"
  },
  {
    "text": "Any guess? 2 to the n minus 1. Yeah, 2 to the n minus 1. That's the issue. There's 2 to the n possible\nthings that can happen.",
    "start": "1085270",
    "end": "1091610"
  },
  {
    "text": "You have to assign a\nprobability to each one of them. Then, well, you\nsave one parameter because you have to sum to\n1, but this number quickly",
    "start": "1091610",
    "end": "1098980"
  },
  {
    "text": "becomes huge. I guess for even a\nsmall number of n, this is more than the number\nof atoms in the universe.",
    "start": "1098980",
    "end": "1107320"
  },
  {
    "text": "And so the question\nis, how do you store these parameters in a computer,\nhow do you learn them from data?",
    "start": "1107320",
    "end": "1114350"
  },
  {
    "text": "You need some tricks, you\nneed some assumptions, you need somehow to deal\nwith this complexity.",
    "start": "1114350",
    "end": "1121173"
  },
  {
    "text": "That's a challenge\nthat you always encounter whenever\nyou want to build a generative model of\nanything interesting,",
    "start": "1121173",
    "end": "1127690"
  },
  {
    "text": "whether it's text, DNA\nsequences, images, videos, whatever, audio, you always\nhave this issue of representing",
    "start": "1127690",
    "end": "1135279"
  },
  {
    "text": "a distribution. Now, one way to make progress\nis to assume something",
    "start": "1135280",
    "end": "1144985"
  },
  {
    "text": "about how the random variables\nare related to each other. And that's always the assumption\nthat you have to make.",
    "start": "1144985",
    "end": "1150590"
  },
  {
    "text": "And one that is a strong\nassumption that you can make is to assume that all\nthese random variables are",
    "start": "1150590",
    "end": "1158030"
  },
  {
    "text": "independent of each other. And if you recall, if the random\nvariables are independent,",
    "start": "1158030",
    "end": "1164680"
  },
  {
    "text": "then it means that\nthe joint distribution can be factored as a product\nof marginal distributions.",
    "start": "1164680",
    "end": "1172550"
  },
  {
    "text": " Now if you're willing to\nmake this assumption, then",
    "start": "1172550",
    "end": "1180330"
  },
  {
    "text": "what happens? How many different\nimages are there here?",
    "start": "1180330",
    "end": "1185474"
  },
  {
    "text": "[INAUDIBLE] Cool. How many images are there? There is still 2 to the\nn possible images, right?",
    "start": "1185474",
    "end": "1193700"
  },
  {
    "text": " You still have a\nprobability distribution",
    "start": "1193700",
    "end": "1199140"
  },
  {
    "text": "over the same space. You're still able to\nassign a probability number to every\npossible assignment",
    "start": "1199140",
    "end": "1206280"
  },
  {
    "text": "of this n binary variables. So it's still a distribution\nover n binary variables.",
    "start": "1206280",
    "end": "1213430"
  },
  {
    "text": "It's still a\nhigh-dimensional space. However, what happens is\nthat you can drastically reduce the number of\nparameters that you",
    "start": "1213430",
    "end": "1219750"
  },
  {
    "text": "need to store this object. How many parameters\ndo you need to specify",
    "start": "1219750",
    "end": "1225150"
  },
  {
    "text": "this joint distribution? n. Now it starts to become\nn, because you just need to be able to store each\none of these entries, each one",
    "start": "1225150",
    "end": "1232710"
  },
  {
    "text": "of these marginals. And these are just\nBernoulli random variables, so you just need one\nparameter if they are binary.",
    "start": "1232710",
    "end": "1241200"
  },
  {
    "text": "And so if these are\nbinary variables, you need one\nparameter for each one",
    "start": "1241200",
    "end": "1246480"
  },
  {
    "text": "of those marginal distributions. You basically just need to\nmodel each pixel separately.",
    "start": "1246480",
    "end": "1252090"
  },
  {
    "text": "Modeling a single pixel is easy. And so if you're willing to\nmake this kind of assumption,",
    "start": "1252090",
    "end": "1259299"
  },
  {
    "text": "you are able to\nrepresent a complicated object, a probability\ndistribution over images",
    "start": "1259300",
    "end": "1265390"
  },
  {
    "text": "with a very small\nnumber of parameters, which means that\nthis is something",
    "start": "1265390",
    "end": "1271660"
  },
  {
    "text": "you can actually implement. You can afford to store\nthese things very easily. ",
    "start": "1271660",
    "end": "1278530"
  },
  {
    "text": "Of course, the challenge is that\nthis independence assumption is probably way too strong.",
    "start": "1278530",
    "end": "1283539"
  },
  {
    "text": "You are literally\nsaying that you can choose the values of\nthe pixels independently. And if you think about modeling,\nlet's say, images of digits,",
    "start": "1283540",
    "end": "1292049"
  },
  {
    "text": "it's probably not going to work. Because you imagine when you\nsample from this distribution,",
    "start": "1292050",
    "end": "1297150"
  },
  {
    "text": "you're not allowed to look\nat any other pixel value to choose a new pixel value.",
    "start": "1297150",
    "end": "1302470"
  },
  {
    "text": "And so you're literally picking\nvalues at random independently.",
    "start": "1302470",
    "end": "1308809"
  },
  {
    "text": "And so it's going to\nbe very hard to be able to capture the\nright structure if you make such a strong\nindependence assumption.",
    "start": "1308810",
    "end": "1317700"
  },
  {
    "text": "So this is not\nquite going to work. What you can do is you\ncan try to make progress",
    "start": "1317700",
    "end": "1325650"
  },
  {
    "text": "by basically making conditional\nindependence assumptions. And so one very important\ntool that is actually",
    "start": "1325650",
    "end": "1334990"
  },
  {
    "text": "the thing behind autoregressive\nmodels, language models, large language\nmodels, they're all",
    "start": "1334990",
    "end": "1341080"
  },
  {
    "text": "built on that first\ntool, which is the chain rule of probability, which\nhopefully you've seen before,",
    "start": "1341080",
    "end": "1348460"
  },
  {
    "text": "the basic idea is that\nyou can always write down the probability of a\nbunch of events happening",
    "start": "1348460",
    "end": "1355330"
  },
  {
    "text": "at the same time as a product\nof conditional probabilities. So you can always say that the\nprobability that S1 happens",
    "start": "1355330",
    "end": "1363429"
  },
  {
    "text": "and S2 happens and S3\nhappens and so forth, you can always write it\nas the probability",
    "start": "1363430",
    "end": "1369650"
  },
  {
    "text": "that S1 happens by itself\nand then the probability that S2 happens given that\nS1 happened, and so forth.",
    "start": "1369650",
    "end": "1377020"
  },
  {
    "text": "And this is always the case\nthat you can always factorize a distribution in that form.",
    "start": "1377020",
    "end": "1383170"
  },
  {
    "text": "And I guess a corollary of that\nis the famous Bayes' rule, which allows you to basically write\nthe conditional probability",
    "start": "1383170",
    "end": "1390580"
  },
  {
    "text": "of one event given\nanother one in terms of the prior probability\nand the likelihood of S2",
    "start": "1390580",
    "end": "1397480"
  },
  {
    "text": "happening given S1.  The important one\nfor now is going",
    "start": "1397480",
    "end": "1403120"
  },
  {
    "text": "to be the first one, chain\nrule, although we're also going to use Bayes' rule later.",
    "start": "1403120",
    "end": "1408880"
  },
  {
    "text": "But chain rule\nbasically gives you a way of writing down\na joint distribution",
    "start": "1408880",
    "end": "1413890"
  },
  {
    "text": "as a product of potentially\nsimpler objects, which",
    "start": "1413890",
    "end": "1418960"
  },
  {
    "text": "are these marginals or\nconditional probabilities.",
    "start": "1418960",
    "end": "1425620"
  },
  {
    "text": "And so this is how\nyou would use it.",
    "start": "1425620",
    "end": "1431230"
  },
  {
    "text": "You can always take a joint\ndistribution over n variables and write it down as a product\nin this way, as the probability",
    "start": "1431230",
    "end": "1440080"
  },
  {
    "text": "of x1 times the\nprobability of x2 given x1, the probability of x3 given\nx1 and x2, and so forth.",
    "start": "1440080",
    "end": "1448660"
  },
  {
    "text": "Using chain rule. This is something\nyou can always do. ",
    "start": "1448660",
    "end": "1455300"
  },
  {
    "text": "This is the kind\nof factorization that is used in\nautoregressive model, which",
    "start": "1455300",
    "end": "1461840"
  },
  {
    "text": "is the first class of models\nthat we're going to talk about, which is again, the same thing\nthat is used in, for example,",
    "start": "1461840",
    "end": "1467720"
  },
  {
    "text": "large language models. And here the idea is\nthat you can write down the probability of observing\na sequence of words,",
    "start": "1467720",
    "end": "1474530"
  },
  {
    "text": "let's say, in a sentence\nas the probability of observing the first\nword times the probability of observing the\nsecond word given",
    "start": "1474530",
    "end": "1480230"
  },
  {
    "text": "the first one, times the\nprobability of observing the third word given the\nfirst two, and so forth.",
    "start": "1480230",
    "end": "1486470"
  },
  {
    "text": "But this is fully general. You can apply it also to pixels. Any collection of\nrandom variables",
    "start": "1486470",
    "end": "1492110"
  },
  {
    "text": "can always be\nfactorized this way.  Now, how many parameters\ndo we need if you",
    "start": "1492110",
    "end": "1500870"
  },
  {
    "text": "use this kind of factorization? It seems like maybe\nwe've made progress",
    "start": "1500870",
    "end": "1506290"
  },
  {
    "text": "because this object here\nis very complicated. But now p of x1, for\nexample, is a simple object,",
    "start": "1506290",
    "end": "1513279"
  },
  {
    "text": "is a marginal distribution\nover a single pixel, so perhaps we've\nmade progress here.",
    "start": "1513280",
    "end": "1520420"
  },
  {
    "text": "So let's do the math. How many parameters do we need? It turns out that we still need\nan exponentially large number",
    "start": "1520420",
    "end": "1526780"
  },
  {
    "text": "of parameters, unfortunately. And the reason is that it's\nkind of like no free lunch.",
    "start": "1526780",
    "end": "1532990"
  },
  {
    "text": "We haven't made any assumptions\nto get this factorization, so we cannot expect\nto get any savings.",
    "start": "1532990",
    "end": "1539990"
  },
  {
    "text": "And you can kind of see it here,\nalthough the first distribution",
    "start": "1539990",
    "end": "1547100"
  },
  {
    "text": "here is indeed simple,\nyou can store it, represent it with\na single parameter.",
    "start": "1547100",
    "end": "1553250"
  },
  {
    "text": "Then how many parameters\ndo you need for the second? Well, if the\nvariables are binary,",
    "start": "1553250",
    "end": "1560180"
  },
  {
    "text": "then x1 can take two\ndifferent values, 0 and 1, and for each one\nof them, you have to specify a distribution\nover x, which",
    "start": "1560180",
    "end": "1567440"
  },
  {
    "text": "will take you one parameter. So p of x2 given x1 will\ntake two parameters.",
    "start": "1567440",
    "end": "1574260"
  },
  {
    "text": "One for the case where the\nfirst bit or the first variable is 0 and 1 for the\ncase where it's 1.",
    "start": "1574260",
    "end": "1581080"
  },
  {
    "text": "And then if you look at the\np of x3 given x1 and x2, there are four possible values\nthat x1 and x2 can take,",
    "start": "1581080",
    "end": "1590310"
  },
  {
    "text": "so you need four parameters. So that's where you get this\nkind of geometric series and that's where you get\nthe exponential blow up.",
    "start": "1590310",
    "end": "1598410"
  },
  {
    "text": "This last conditionals\nhere are very expensive. ",
    "start": "1598410",
    "end": "1604320"
  },
  {
    "text": "And so if you do the sum, you\nstill don't get anything here.",
    "start": "1604320",
    "end": "1610610"
  },
  {
    "text": "But it gives us a way to\nperhaps make progress. It's still a useful\nbuilding block.",
    "start": "1610610",
    "end": "1616460"
  },
  {
    "text": "And, for example,\none thing you can do is you can assume independence,\nconditional independence.",
    "start": "1616460",
    "end": "1624680"
  },
  {
    "text": "For example, you might\nbe willing to assume that the value of\nthe i plus 1 word",
    "start": "1624680",
    "end": "1632660"
  },
  {
    "text": "is conditionally independent\nfrom the previous--",
    "start": "1632660",
    "end": "1638620"
  },
  {
    "text": "given the i-th word, the\nvalue of the i-th plus 1 word is conditionally independent\nof all the previous words.",
    "start": "1638620",
    "end": "1646960"
  },
  {
    "text": "So this is a Markov assumption. So if these x's maybe\nrepresent the weather,",
    "start": "1646960",
    "end": "1652299"
  },
  {
    "text": "then you're saying\nthe weather tomorrow is conditionally independent\nfrom the past given the weather",
    "start": "1652300",
    "end": "1658120"
  },
  {
    "text": "today. And if you're willing\nto make this assumption and you get big savings.",
    "start": "1658120",
    "end": "1664690"
  },
  {
    "text": "What this means is\nthat if you think about the definition of\nconditional independence is that a lot of these\nconditional distributions",
    "start": "1664690",
    "end": "1671170"
  },
  {
    "text": "will simplify. And so, in particular,\nthis probability",
    "start": "1671170",
    "end": "1676810"
  },
  {
    "text": "of x3 given x1 and x2 becomes\nthe probability of x3 given x2.",
    "start": "1676810",
    "end": "1682990"
  },
  {
    "text": "So if you are predicting\nthe third word, this is saying you just need\nto know the second word,",
    "start": "1682990",
    "end": "1688190"
  },
  {
    "text": "you can ignore the first word. And if you are\npredicting the last word,",
    "start": "1688190",
    "end": "1693760"
  },
  {
    "text": "you don't need to remember\nthe entire sequence, the previous word is sufficient.",
    "start": "1693760",
    "end": "1699960"
  },
  {
    "text": "And if you do that then you\nget this nice expression where the conditionals are now simple.",
    "start": "1699960",
    "end": "1707220"
  },
  {
    "text": "You're always conditioning\non at most one variable, and so now we get big savings.",
    "start": "1707220",
    "end": "1715720"
  },
  {
    "text": "How many parameters\ndo we need here? [INAUDIBLE]",
    "start": "1715720",
    "end": "1721560"
  },
  {
    "text": "Yeah, something like that,\nthis linear in n, basically. Depending on if the variables\nare binary, this is the formula.",
    "start": "1721560",
    "end": "1731580"
  },
  {
    "text": "So big savings. And now we have a much\nmore reasonable model. This is much more reasonable\nthan the full independence",
    "start": "1731580",
    "end": "1739890"
  },
  {
    "text": "model. These Markovian models are\nquite useful in practice.",
    "start": "1739890",
    "end": "1745740"
  },
  {
    "text": "But again, if you\nthink about language or you think about\npixels in an image, it's probably not good enough.",
    "start": "1745740",
    "end": "1752468"
  },
  {
    "text": "You're probably not\ngoing to do a great job if you're trying to predict-- if you think about your\nautocomplete in your phone,",
    "start": "1752468",
    "end": "1758429"
  },
  {
    "text": "you're trying to predict\nthe next word just based on the previous one\nand ignore everything else,",
    "start": "1758430",
    "end": "1763650"
  },
  {
    "text": "you can do OK, but it's\nnot going to be great. You need more context to be\nable to make a good prediction",
    "start": "1763650",
    "end": "1770070"
  },
  {
    "text": "about the next word. And so although there is\nan exponential reduction,",
    "start": "1770070",
    "end": "1777510"
  },
  {
    "text": "maybe this assumption is\nstill a little bit too strong. And so one way to\ngeneralize this idea",
    "start": "1777510",
    "end": "1785490"
  },
  {
    "text": "is to use something called\na Bayesian network, which is essentially the same\nmachinery in slightly",
    "start": "1785490",
    "end": "1791100"
  },
  {
    "text": "more generality. The basic idea is, again,\nthat instead of-- we're",
    "start": "1791100",
    "end": "1798210"
  },
  {
    "text": "going to write down the joint\nas a product of conditionals. But instead of having these\nsimple conditionals where",
    "start": "1798210",
    "end": "1804320"
  },
  {
    "text": "it's always one variable\ngiven another variable,",
    "start": "1804320",
    "end": "1809830"
  },
  {
    "text": "we're going to use conditional\ndistributions where the i-th variable will depend on\nanother set of random variables,",
    "start": "1809830",
    "end": "1818230"
  },
  {
    "text": "which are the parents in\nthis Bayesian network. And so intuitively,\nthe idea is that we're",
    "start": "1818230",
    "end": "1827110"
  },
  {
    "text": "going to try to\nwrite down the joint as a product of conditionals,\nbut now the conditionals",
    "start": "1827110",
    "end": "1832750"
  },
  {
    "text": "are a little bit more complex. Now each variable is allowed to\ndepend on a subset of variables.",
    "start": "1832750",
    "end": "1839210"
  },
  {
    "text": "It could be 1, it\ncould be more so that buys you a little\nbit more flexibility.",
    "start": "1839210",
    "end": "1845110"
  },
  {
    "text": "And the idea is\nthat because we're using chain rule, as long\nas there is some ordering",
    "start": "1845110",
    "end": "1853090"
  },
  {
    "text": "that you've used to come up\nwith this joint distribution by simplifying the expression\nthat you would get from chain",
    "start": "1853090",
    "end": "1859690"
  },
  {
    "text": "rule, then this is guaranteed\nto correspond to a valid model.",
    "start": "1859690",
    "end": "1865879"
  },
  {
    "text": "So essentially, you can specify\nany conditional independence-- any conditional distribution\nyou want on the right-hand side.",
    "start": "1865880",
    "end": "1872790"
  },
  {
    "text": "Once you multiply\nthem together, you're going to get a valid\nprobability distribution on the left-hand side.",
    "start": "1872790",
    "end": "1879740"
  },
  {
    "text": "That's the key intuition\nbehind the Bayesian network. More formally, a\nBayesian network",
    "start": "1879740",
    "end": "1887840"
  },
  {
    "text": "is a data structure that you\ncan use to specify a probability distribution.",
    "start": "1887840",
    "end": "1893900"
  },
  {
    "text": "It's a graph-based\ndata structure where basically\nthere's going to be an underlying directed acyclic\ngraph, which basically gives you",
    "start": "1893900",
    "end": "1902140"
  },
  {
    "text": "the ordering in that\nchain rule factorization. So there's going to be\none node in the graph",
    "start": "1902140",
    "end": "1908200"
  },
  {
    "text": "for every random variable\nthat you're modeling. So if you're modeling images,\none node for every pixel,",
    "start": "1908200",
    "end": "1916659"
  },
  {
    "text": "if you're modeling text,\none word for every token or every word that you have. And then what you do is for\nevery node in the graph,",
    "start": "1916660",
    "end": "1924279"
  },
  {
    "text": "you specify its conditional\ndistribution given its parent",
    "start": "1924280",
    "end": "1929920"
  },
  {
    "text": "in this directed acyclic graph. And that's the graph\nis the structure.",
    "start": "1929920",
    "end": "1936350"
  },
  {
    "text": "And then by specifying different\nconditional distributions for each variable\ngiven the parents, you get different\nparameterizations",
    "start": "1936350",
    "end": "1943029"
  },
  {
    "text": "of these joints. And the claim is that basically\nthis is a valid probability",
    "start": "1943030",
    "end": "1950710"
  },
  {
    "text": "distribution. And the reason is that it's\nessentially the same trick we did for the Markov model.",
    "start": "1950710",
    "end": "1957750"
  },
  {
    "text": "You start with a\ndirected acyclic graph. You can always come\nup with an ordering.",
    "start": "1957750",
    "end": "1964070"
  },
  {
    "text": "You can just do topological\nsort on the graph. You get an ordering, you\ncan apply chain rule. You factorize with\nrespect to their ordering,",
    "start": "1964070",
    "end": "1971160"
  },
  {
    "text": "then you simplify\nthe conditionals based on this some conditional\nindependence assumption.",
    "start": "1971160",
    "end": "1978410"
  },
  {
    "text": "And that gives you a potentially\ncompact data structure.",
    "start": "1978410",
    "end": "1984020"
  },
  {
    "text": "It depends on how many parents,\nhow dense the graph is, but this can give you savings. Again, the challenge is that we\nhave this joint distribution.",
    "start": "1984020",
    "end": "1992130"
  },
  {
    "text": "It takes too many parameters\nto represent this object. But if these conditionals\nare relatively simple,",
    "start": "1992130",
    "end": "2000220"
  },
  {
    "text": "so you don't have too many\nparents for each variable, then these conditionals\nare simple enough",
    "start": "2000220",
    "end": "2006670"
  },
  {
    "text": "that you can store this object,\nyou can learn these parameters from data, and so forth.",
    "start": "2006670",
    "end": "2012220"
  },
  {
    "text": "So it's like exponential\nin the number of parents that you have for each variable. So if you make a\nvery dense graph,",
    "start": "2012220",
    "end": "2018970"
  },
  {
    "text": "you're going to get a very\nexpressive class of models and you're not going\nto get big savings.",
    "start": "2018970",
    "end": "2024290"
  },
  {
    "text": "If you use a chain\ngraph where there's only one parent per node, then\nyou get the Markov assumption",
    "start": "2024290",
    "end": "2030490"
  },
  {
    "text": "that we have before and\nthere are things in between. For example, this is just--",
    "start": "2030490",
    "end": "2037590"
  },
  {
    "text": "what does it mean? A directed cycle would\nbe something like this. So you need to make\nsure that there",
    "start": "2037590",
    "end": "2042750"
  },
  {
    "text": "is no directed cycle, which\nmeans that there is an ordering and it means you\ncan use chain rule.",
    "start": "2042750",
    "end": "2050550"
  },
  {
    "text": "This is an example of a very\nsimple Bayesian network. Here the idea is that you have\nthese five random variables",
    "start": "2050550",
    "end": "2058859"
  },
  {
    "text": "representing the difficulty\nof an exam, the intelligence of a student, the grade\nthat you get, and so forth.",
    "start": "2058860",
    "end": "2067770"
  },
  {
    "text": "And there is a\njoint distribution over these five random\nvariables, which",
    "start": "2067770",
    "end": "2073859"
  },
  {
    "text": "is obtained as a product of\nconditional distributions of each variable\ngiven the parent.",
    "start": "2073860",
    "end": "2080388"
  },
  {
    "text": "And so for this\nparticular graph, this node doesn't\nhave any parent,",
    "start": "2080389",
    "end": "2086360"
  },
  {
    "text": "so you just write the marginal\nprobability of that node. This node doesn't\nhave any parent.",
    "start": "2086360",
    "end": "2092100"
  },
  {
    "text": "So again, it's just\nthe probability of getting different\nintelligence values.",
    "start": "2092100",
    "end": "2097280"
  },
  {
    "text": "The grade has two arrows\nincoming from difficulty and intelligence. So what you're saying is\nthat the grades that you see",
    "start": "2097280",
    "end": "2104420"
  },
  {
    "text": "depend essentially on\nthe possible values of the difficulty of the\nexam and the intelligence of the student.",
    "start": "2104420",
    "end": "2110720"
  },
  {
    "text": "And so you can basically\nwrite down the joint as a product of conditionals\nthat would look like this.",
    "start": "2110720",
    "end": "2116630"
  },
  {
    "text": "And in this case,\nthis might be more economical than\nrepresenting the joint",
    "start": "2116630",
    "end": "2122060"
  },
  {
    "text": "because you basically\njust have to specify these tables, these conditional\nprobability distributions.",
    "start": "2122060",
    "end": "2127530"
  },
  {
    "text": "You only need to\nwork out basically how these random variables are\nrelated to each other locally",
    "start": "2127530",
    "end": "2133369"
  },
  {
    "text": "with respect to this graph. You only need to know how\nto assign grades given different values of\ndifficulty and intelligence,",
    "start": "2133370",
    "end": "2140860"
  },
  {
    "text": "but you're like breaking\ndown the complexity of the joint in terms of\nsmaller local interactions",
    "start": "2140860",
    "end": "2147580"
  },
  {
    "text": "between the random variables. And again, by making\nthis assumption",
    "start": "2147580",
    "end": "2155680"
  },
  {
    "text": "the global dependencies\ncan be broken down into simpler local dependencies. You get benefits because\nthese conditionals",
    "start": "2155680",
    "end": "2164140"
  },
  {
    "text": "are potentially much smaller,\nmuch simpler, and easier to represent.",
    "start": "2164140",
    "end": "2169710"
  },
  {
    "text": "And the idea is that\nassuming this factorization",
    "start": "2169710",
    "end": "2175170"
  },
  {
    "text": "is the same as assuming\nconditional independence. And you can see it here, we\nhave this kind of factorization",
    "start": "2175170",
    "end": "2184880"
  },
  {
    "text": "for the joint, which is\nimplied by this graph. In general, we know\nthat you can always",
    "start": "2184880",
    "end": "2190940"
  },
  {
    "text": "have a more complicated\nfactorization where every variable depends\non all the variables that",
    "start": "2190940",
    "end": "2196490"
  },
  {
    "text": "come before in some ordering. So in general, you\nwould have to specify the probability of having a\ncertain difficulty for the exam.",
    "start": "2196490",
    "end": "2204437"
  },
  {
    "text": "You would have to specify the\nprobability of some intelligence value given the difficulty, a\nprobability of g, given i and d,",
    "start": "2204437",
    "end": "2211010"
  },
  {
    "text": "the probability of the SAT\nscore given everything else, and so forth. Now, if you're willing to\nassume that the intelligence",
    "start": "2211010",
    "end": "2218480"
  },
  {
    "text": "of the student does not depend\non the difficulty of the exam, then you can start\nsimplifying these conditionals",
    "start": "2218480",
    "end": "2226280"
  },
  {
    "text": "and they become like\nthe ones you see above. So if you want to-- for example, the SAT score only\ndepends on the intelligence,",
    "start": "2226280",
    "end": "2235280"
  },
  {
    "text": "and you don't need to know\nthe difficulty of the exam, you don't need to know the\ngrade in the other exam to figure out the SAT score.",
    "start": "2235280",
    "end": "2242390"
  },
  {
    "text": "And so this\nfactorization basically corresponds to a bunch of\nconditional independencies.",
    "start": "2242390",
    "end": "2248430"
  },
  {
    "text": "We're saying the difficulty\nand the intelligence are independent of each other. The SAT score is conditionally\nindependent from the difficulty",
    "start": "2248430",
    "end": "2256230"
  },
  {
    "text": "and the grade given the\nintelligence and so forth. And so Bayesian\nnetworks are basically",
    "start": "2256230",
    "end": "2262860"
  },
  {
    "text": "a way to get to simplify\ncomplicated distributions based on conditional independence\nassumptions, which",
    "start": "2262860",
    "end": "2270510"
  },
  {
    "text": "are more reasonable than full\nindependence assumptions. Now, [CLEARS THROAT]\nso to summarize,",
    "start": "2270510",
    "end": "2282280"
  },
  {
    "text": "we can basically represent--\nuse Bayesian networks as a tool to factorize\ndistributions and write them",
    "start": "2282280",
    "end": "2288580"
  },
  {
    "text": "down as a product\nof conditionals. You get the joint by multiplying\ntogether the conditionals.",
    "start": "2288580",
    "end": "2294349"
  },
  {
    "text": "You can sample by basically\ngoing through the ordering. ",
    "start": "2294350",
    "end": "2299950"
  },
  {
    "text": "In this class, we're\nactually not going to be going this route. So that's the route\nthat you're going",
    "start": "2299950",
    "end": "2306070"
  },
  {
    "text": "to take if you want to build\nup a probabilistic graph like a graphical model, a PGM,\na probabilistic graphical model.",
    "start": "2306070",
    "end": "2312790"
  },
  {
    "text": "In this class, the\ngraphical model-- we'll still be using a little\nbit of graphical models notations, but the\ngraphical models",
    "start": "2312790",
    "end": "2317980"
  },
  {
    "text": "are going to be\nrelatively simple. They typically involve two\nor three random variables, random vectors.",
    "start": "2317980",
    "end": "2324670"
  },
  {
    "text": "And instead, we're going to be\nmaking other kinds of a softer notion of conditional\nindependence, which",
    "start": "2324670",
    "end": "2331360"
  },
  {
    "text": "is going to be\nessentially this idea of-- let's use neural networks\nto try to represent",
    "start": "2331360",
    "end": "2336760"
  },
  {
    "text": "how the different variables\nare related to each other. So it will still have somewhat\nthe flavor of a Bayesian",
    "start": "2336760",
    "end": "2342350"
  },
  {
    "text": "network, but it's going to\nbe a little bit of a software kind of constraint\nbetween the variables.",
    "start": "2342350",
    "end": "2351289"
  },
  {
    "text": "Now, obviously, this was\na bit of a crash course. But again, we're not going to\nbe leveraging these things too",
    "start": "2351290",
    "end": "2359319"
  },
  {
    "text": "much. We're going to use a little bit\nof graphical models notation",
    "start": "2359320",
    "end": "2364405"
  },
  {
    "text": "and a little bit of directed\nacyclic graph for some of the graphical models,\nbut nothing too heavy.",
    "start": "2364405",
    "end": "2370280"
  },
  {
    "text": "We're going to be using\ndifferent assumptions and different modeling ideas to\nbuild deep generative models.",
    "start": "2370280",
    "end": "2379270"
  },
  {
    "text": "And that's going to\nbe again inspired by the use of neural\nnetworks for, let's say,",
    "start": "2379270",
    "end": "2386109"
  },
  {
    "text": "classification or other\ndiscriminative tasks that you might have seen before.",
    "start": "2386110",
    "end": "2391190"
  },
  {
    "text": "And so now is a good time to\ntry to get a sense of what's the difference between\nbuilding a generative model",
    "start": "2391190",
    "end": "2398080"
  },
  {
    "text": "versus building-- as usual,\ndiscriminative model, and how do we get the\nideas from the things",
    "start": "2398080",
    "end": "2403660"
  },
  {
    "text": "that we know work\nwhen you're doing, let's say, image classification\nor these more standard machine",
    "start": "2403660",
    "end": "2409569"
  },
  {
    "text": "learning problems and\ntranslate them back into the generative\nmodeling world.",
    "start": "2409570",
    "end": "2415420"
  },
  {
    "text": "And in order to see\nthat, let's see-- again, one example\nwhere you could try to--",
    "start": "2415420",
    "end": "2424940"
  },
  {
    "text": "we're going to use a\nsimple generative model to solve a discriminative task.",
    "start": "2424940",
    "end": "2431480"
  },
  {
    "text": "And we'll see how that\nwill differ compared to a traditional approach based\non, let's say, a neural network.",
    "start": "2431480",
    "end": "2439280"
  },
  {
    "text": "So let's say that you want\nto solve a task where you're given a bunch of images,\na bunch of emails,",
    "start": "2439280",
    "end": "2445160"
  },
  {
    "text": "and the goal is to\npredict whether or not this email is spam. So there is a binary label Y\nthat you're trying to predict",
    "start": "2445160",
    "end": "2452329"
  },
  {
    "text": "and you're doing it using\na bunch of features Xi. And let's say the\nfeatures are just binary",
    "start": "2452330",
    "end": "2459470"
  },
  {
    "text": "and they are on or off\ndepending on whether or not different words in some\nvocabulary appear in the email.",
    "start": "2459470",
    "end": "2467670"
  },
  {
    "text": "And the usual\nassumption is that there is some underlying\ndata-generating process. And so there is\nsome relationship",
    "start": "2467670",
    "end": "2474060"
  },
  {
    "text": "between the different words that\nyou see in the email, the X's and the Y variable, which is the\nlabel you're trying to predict.",
    "start": "2474060",
    "end": "2483359"
  },
  {
    "text": "So one way to approach this is\nby building a Bayesian network.",
    "start": "2483360",
    "end": "2488580"
  },
  {
    "text": "This is a Bayesian classifier\ncalled the Naive Bayes classifier, which is\nbasically going to say,",
    "start": "2488580",
    "end": "2496080"
  },
  {
    "text": "we want to model this\njoint distribution. This joint distribution\nhas too many variables, we cannot afford to store it to\nlearn the parameters from data.",
    "start": "2496080",
    "end": "2505470"
  },
  {
    "text": "So we're going to make\nconditional independence assumptions and\nwe're going to assume that the joint can be described\nby this directed acyclic graph.",
    "start": "2505470",
    "end": "2513395"
  },
  {
    "text": " And if you are willing to make\nthis Bayes' net assumption, what",
    "start": "2513395",
    "end": "2521140"
  },
  {
    "text": "this means is that the features,\nthe words, the Xi's are basically conditionally\nindependent",
    "start": "2521140",
    "end": "2527680"
  },
  {
    "text": "given the label, given the\nY. If you're willing to make",
    "start": "2527680",
    "end": "2533910"
  },
  {
    "text": "this assumption, then\nyou're able to factorize the joint, which is\nusually complicated as a product of conditionals.",
    "start": "2533910",
    "end": "2540460"
  },
  {
    "text": "So you can write\nit as the p of y because y doesn't have\nany parent, and then the probability of one\nvariable given its parent,",
    "start": "2540460",
    "end": "2546750"
  },
  {
    "text": "probability of this variable\ngiven its parent, and so forth--",
    "start": "2546750",
    "end": "2551780"
  },
  {
    "text": "which means that you can\nbasically-- according to this very simplified\nmodel of the world, you can generate a data point\nby first choosing whether or not",
    "start": "2551780",
    "end": "2559580"
  },
  {
    "text": "it's spam, and then choosing\nwhether different words appear in the email based on whether\nthe email is spam or not.",
    "start": "2559580",
    "end": "2569390"
  },
  {
    "text": "And once you have the\nmodel, what you can do",
    "start": "2569390",
    "end": "2576819"
  },
  {
    "text": "is you can try to estimate\nthe parameters of this model from data. So you can try to estimate\nthese probabilities",
    "start": "2576820",
    "end": "2584590"
  },
  {
    "text": "by looking at how frequently\ndo you see different words in different types of emails. And then you can\ndo classification",
    "start": "2584590",
    "end": "2592089"
  },
  {
    "text": "because at the end of the\nday, what you're trying to do is you're trying to\nclassify whether or not a new email is spam or not.",
    "start": "2592090",
    "end": "2598290"
  },
  {
    "text": "And you can use Bayes'\nrule to write down the conditional\ndistribution of Y given x.",
    "start": "2598290",
    "end": "2604030"
  },
  {
    "text": "So given a new email, you\nobserve which words are there and which ones are not. And you can try to compute\nthe probability of Y",
    "start": "2604030",
    "end": "2611560"
  },
  {
    "text": "by basically using Bayes' rule\nprobability of x, y divided by the probability of x,\nessentially, which is what",
    "start": "2611560",
    "end": "2619360"
  },
  {
    "text": "you have at the denominator. And if you've done a good job\nat estimating these parameters,",
    "start": "2619360",
    "end": "2626380"
  },
  {
    "text": "this thing will-- and to the extent that\nthe assumption is true, this conditional independence\nassumption is true,",
    "start": "2626380",
    "end": "2633579"
  },
  {
    "text": "this model might\nperform reasonably well at predicting the label\nY given the features X.",
    "start": "2633580",
    "end": "2642660"
  },
  {
    "text": "The challenge of\ncourse is once again that perhaps these conditional\nindependence assumptions are not that great.",
    "start": "2642660",
    "end": "2647670"
  },
  {
    "text": "If you think about\nit, you're saying that different words appear\nin an email independently",
    "start": "2647670",
    "end": "2654180"
  },
  {
    "text": "of each other. So once you know why\nbasically knowing whether a word appears\nor not doesn't help you",
    "start": "2654180",
    "end": "2660540"
  },
  {
    "text": "predict whether some other word\nappears in the email or not, which is probably\nnot reasonable.",
    "start": "2660540",
    "end": "2667350"
  },
  {
    "text": "Nevertheless, this model\ntends to work OK in practice. So even though the\nassumption is not quite true,",
    "start": "2667350",
    "end": "2673620"
  },
  {
    "text": "it might give you reasonable\nresults in practice. Now, how does this fit into the\ndiscriminative versus generative",
    "start": "2673620",
    "end": "2684540"
  },
  {
    "text": "model of the problem? So at the end of the\nday, we're trying",
    "start": "2684540",
    "end": "2691060"
  },
  {
    "text": "to model this joint distribution\nbetween features and a label Y.",
    "start": "2691060",
    "end": "2696160"
  },
  {
    "text": "And using chain\nrule, we can write it like this as the\nprobability of the label",
    "start": "2696160",
    "end": "2702580"
  },
  {
    "text": "times the probability of the\nfeatures given the label. This is exactly what we've\ndone in the Naive Bayes model",
    "start": "2702580",
    "end": "2708190"
  },
  {
    "text": "that we just saw. Alternatively, you\ncan use chain rule",
    "start": "2708190",
    "end": "2713320"
  },
  {
    "text": "based on a different\nordering and you can say, I can write it as\nthe probability of observing",
    "start": "2713320",
    "end": "2718600"
  },
  {
    "text": "this feature vector\ntimes the probability that that particular\nfeature vector has label Y.",
    "start": "2718600",
    "end": "2727300"
  },
  {
    "text": "And so these are basically two\nBayesian networks that capture the same joint distribution,\none where we have y and then x,",
    "start": "2727300",
    "end": "2736150"
  },
  {
    "text": "and then one where\nwe have x and y. And the second one\nis basically the one",
    "start": "2736150",
    "end": "2741940"
  },
  {
    "text": "that you deal with when you\nthink about usual discriminative models.",
    "start": "2741940",
    "end": "2747099"
  },
  {
    "text": "If you think about\nit, at the end of the day, if all you\ncare about is predicting",
    "start": "2747100",
    "end": "2752950"
  },
  {
    "text": "whether a new data\npoint has label 0 or 1, all you care about\nis p of Y given X.",
    "start": "2752950",
    "end": "2760630"
  },
  {
    "text": "And so the second modeling\napproach where you're modeling p of Y given X directly\nmight be much more natural.",
    "start": "2760630",
    "end": "2771400"
  },
  {
    "text": "In the last model, we\nwere specifying p of Y, we were specifying\np of X given Y,",
    "start": "2771400",
    "end": "2777760"
  },
  {
    "text": "and then we would compute p\nof Y given X using Bayes rule. While in the second model you\nhave access to p of Y given X,",
    "start": "2777760",
    "end": "2786760"
  },
  {
    "text": "the probability of this variable\ngiven its parent directly.",
    "start": "2786760",
    "end": "2791950"
  },
  {
    "text": "And so the idea is\nthat if you know",
    "start": "2791950",
    "end": "2797890"
  },
  {
    "text": "that all you care about\nIs p of Y given X, then there is no point in\ntrying to learn or model or deal",
    "start": "2797890",
    "end": "2806560"
  },
  {
    "text": "with this marginal\ndistribution over the features. You know that you're always\never going to be given an email",
    "start": "2806560",
    "end": "2813549"
  },
  {
    "text": "and you just try to\npredict y, why do you bother trying to figure\nout what kind of feature",
    "start": "2813550",
    "end": "2818890"
  },
  {
    "text": "vectors x you're likely to see? p of X here will basically be a\ndistribution over the features",
    "start": "2818890",
    "end": "2827020"
  },
  {
    "text": "that your model is going to see. If you know you don't\ncare because you just",
    "start": "2827020",
    "end": "2832030"
  },
  {
    "text": "care about predicting\ny from x, then you don't even bother\nmodeling p of X.",
    "start": "2832030",
    "end": "2837140"
  },
  {
    "text": "And so that's more convenient. And that's why typically the\nkind of models that you're",
    "start": "2837140",
    "end": "2842210"
  },
  {
    "text": "building, that you use\nin machine learning, they don't bother about\nmodeling the distribution over the features.",
    "start": "2842210",
    "end": "2847920"
  },
  {
    "text": "They just bother about\nmodeling the relationship between a label\nand the features x.",
    "start": "2847920",
    "end": "2853190"
  },
  {
    "text": "While in a generative\nmodel, it's the opposite. You're basically\nmodeling the whole thing.",
    "start": "2853190",
    "end": "2858690"
  },
  {
    "text": "You're modeling the\nfull joint distribution. And so the discriminative\nmodel is basically",
    "start": "2858690",
    "end": "2867250"
  },
  {
    "text": "only useful for\ndiscriminating y given x, while a generative\nmodel is also able to reason about\nits inputs, it's",
    "start": "2867250",
    "end": "2873130"
  },
  {
    "text": "able to reason about the full\nrelationship between x and y. And so now there is still\nno free lunch in the sense",
    "start": "2873130",
    "end": "2882940"
  },
  {
    "text": "that if you think\nabout it, it's true that you can do these\ntwo factorizations.",
    "start": "2882940",
    "end": "2888310"
  },
  {
    "text": "You can use either factorized as\np of Y and then p of X given Y,",
    "start": "2888310",
    "end": "2894610"
  },
  {
    "text": "or you can do p of X and then p\nof Y given X. But in both cases,",
    "start": "2894610",
    "end": "2900520"
  },
  {
    "text": "you end up with some of\nthese conditionals which are pretty complicated. So in the generative model,\nyou have a Bayesian--",
    "start": "2900520",
    "end": "2907990"
  },
  {
    "text": "if you were to actually\nunpack the fact that X is a random vector, so you have\na bunch of individual features",
    "start": "2907990",
    "end": "2914920"
  },
  {
    "text": "that you have to deal with,\nthe two graphical models corresponding to the two\nchain rule factorizations",
    "start": "2914920",
    "end": "2920829"
  },
  {
    "text": "would look like this. In the generative view\nof the world, you have Y and then you have\nall the features.",
    "start": "2920830",
    "end": "2926980"
  },
  {
    "text": "In the discriminative\nview of the world, you have all the X's first\nand then you have Y given X.",
    "start": "2926980",
    "end": "2933130"
  },
  {
    "text": "And you still need to deal with\nthe fact that you have a lot of X's. You have potentially\na lot of features",
    "start": "2933130",
    "end": "2938410"
  },
  {
    "text": "that you have to take\ninto account when you're predicting Y. And so in the generative\nmodeling world, p of Y",
    "start": "2938410",
    "end": "2947560"
  },
  {
    "text": "is simple, but then you have a\nbunch of these variables here that have a lot of parents.",
    "start": "2947560",
    "end": "2952860"
  },
  {
    "text": "So there is a lot of\ncomplexity that you have to deal with when\nyou need to decide",
    "start": "2952860",
    "end": "2958240"
  },
  {
    "text": "what are the relationships\nbetween the features. In the discriminative\nmodeling world,",
    "start": "2958240",
    "end": "2965450"
  },
  {
    "text": "it's true that you're making\nsome progress because maybe you don't need to model all these\nrelationships between the x",
    "start": "2965450",
    "end": "2970550"
  },
  {
    "text": "variables, but you still need to\nbe able to model how Y depends on all the X's.",
    "start": "2970550",
    "end": "2976280"
  },
  {
    "text": "And Y has a lot of parents. So again, that\nconditional distribution is potentially very complicated.",
    "start": "2976280",
    "end": "2985100"
  },
  {
    "text": "And so one way to\nmake progress is to say, OK, let's make\nconditional independence assumptions.",
    "start": "2985100",
    "end": "2991070"
  },
  {
    "text": "So in general, you would have\nsomething-- a generative model would have to look like this. So you would have to\nbe able to capture",
    "start": "2991070",
    "end": "2998029"
  },
  {
    "text": "all sorts of dependencies\nbetween the X's and the Y. If you're willing to make\nsimplifying assumptions",
    "start": "2998030",
    "end": "3004300"
  },
  {
    "text": "and say, oh, things are\nconditionally independent, then you basically chop\nsome edges in the graph",
    "start": "3004300",
    "end": "3010780"
  },
  {
    "text": "and you end up with something\nthat is much simpler. Remember the last\nparents, the variables",
    "start": "3010780",
    "end": "3016090"
  },
  {
    "text": "have, the simpler\nthe relationships between the random variables\nare the simpler the model is.",
    "start": "3016090",
    "end": "3022430"
  },
  {
    "text": "And so you're saying\nonce I know Y, I can basically figure out\nthe values of the X variables",
    "start": "3022430",
    "end": "3028900"
  },
  {
    "text": "and there is no\nrelationship between them. That's one way to make progress.",
    "start": "3028900",
    "end": "3035220"
  },
  {
    "text": "Obviously, it's a\nstrong assumption. It might or might not\nwork in the real world.",
    "start": "3035220",
    "end": "3041430"
  },
  {
    "text": "In the discriminative model, you\nstill need to be able to model this conditional distribution\nof Y given all the X's.",
    "start": "3041430",
    "end": "3049950"
  },
  {
    "text": "And again, that's\nnot straightforward because if you think about\nall these features here,",
    "start": "3049950",
    "end": "3055559"
  },
  {
    "text": "let's say they are binary, there\nare 2 to the n possible feature vectors that you have to deal\nwith, and for each one of them,",
    "start": "3055560",
    "end": "3063060"
  },
  {
    "text": "you would have to specify-- like when you look at this\nlast conditional here, it's the same as before.",
    "start": "3063060",
    "end": "3069069"
  },
  {
    "text": "Your conditioning on\na lot of variables. There are 2 to the n possible\ncombinations of those X",
    "start": "3069070",
    "end": "3074840"
  },
  {
    "text": "variables and in\nfull generality, you would have to assign\na different number,",
    "start": "3074840",
    "end": "3079970"
  },
  {
    "text": "a different value for\nthe probability of Y for each one of them. And so again, like the\nconditional distribution of Y,",
    "start": "3079970",
    "end": "3087890"
  },
  {
    "text": "given all the parents,\nis not easy to deal with, even in a discriminative model.",
    "start": "3087890",
    "end": "3094310"
  },
  {
    "text": "So the way you make\nprogress, usually in a discriminative\nmodel, is to assume that the dependency\nis not fully general",
    "start": "3094310",
    "end": "3101510"
  },
  {
    "text": "and it somehow takes a\nparticular functional form. So it's true that\nthis x vector can take",
    "start": "3101510",
    "end": "3109490"
  },
  {
    "text": "many, many different values. And if you were to\nuse a big table, that table would have 2\nto the n possible rows.",
    "start": "3109490",
    "end": "3117700"
  },
  {
    "text": "So you would not be able to\nstore that, you would not be able to learn it from data,\nyou would not be able to use it.",
    "start": "3117700",
    "end": "3123260"
  },
  {
    "text": "But what you can assume is that\nthere is some simple function that you can use to take x and\nmap it to a probability value.",
    "start": "3123260",
    "end": "3133150"
  },
  {
    "text": "And so the assumption\nthat you have to make here to make progress\nis to assume that there",
    "start": "3133150",
    "end": "3138160"
  },
  {
    "text": "is some simple function\nf that you can apply to the different values that\nthe x variables can take",
    "start": "3138160",
    "end": "3144010"
  },
  {
    "text": "and that will map it to this\nnumber that you care about, which is the conditional\nprobability of Y given X.",
    "start": "3144010",
    "end": "3150279"
  },
  {
    "text": "And there is many\ndifferent ways to do it. One way is to do--",
    "start": "3150280",
    "end": "3158187"
  },
  {
    "text": "there are some constraints here. And one way to do\nit is to do what's done in logistic\nregression, for example.",
    "start": "3158187",
    "end": "3164970"
  },
  {
    "text": "So the idea is that-- and that's why it's called\nregression is that essentially it's not a table.",
    "start": "3164970",
    "end": "3171240"
  },
  {
    "text": "It's going to be some\nkind of function that will take different values\nof x and we'll regress them to probabilities for y.",
    "start": "3171240",
    "end": "3179100"
  },
  {
    "text": "And it's not an arbitrary\nregression problem because what we're doing is\nwe're trying to map these x's",
    "start": "3179100",
    "end": "3186960"
  },
  {
    "text": "to conditional probabilities. And we know that that\nconditional probability is a number that has\nto be between 1 and 0.",
    "start": "3186960",
    "end": "3195089"
  },
  {
    "text": "It doesn't make\nsense to say, oh, I fit in a certain\nfeature vector x in the spam classification.",
    "start": "3195090",
    "end": "3201870"
  },
  {
    "text": "It's a bunch of indicators of\nwhether different words appear in the email. If this function gives\nme a value of minus 1,",
    "start": "3201870",
    "end": "3210600"
  },
  {
    "text": "it doesn't make sense, because\nwe know that probabilities are numbers between 0 and 1.",
    "start": "3210600",
    "end": "3216150"
  },
  {
    "text": "So there are some constraints\non this regression problem.",
    "start": "3216150",
    "end": "3221369"
  },
  {
    "text": "And in particular, we want the\noutput to be between 0 and 1.",
    "start": "3221370",
    "end": "3227560"
  },
  {
    "text": "We want the dependency to\nbe simple but reasonable.",
    "start": "3227560",
    "end": "3232660"
  },
  {
    "text": "If it's too complicated\nand it's a table, a lookup, then you're back\nto the previous settings.",
    "start": "3232660",
    "end": "3238730"
  },
  {
    "text": "You don't gain anything. So somehow you want\na simple dependency, but it's sufficiently\nrich that it captures",
    "start": "3238730",
    "end": "3246130"
  },
  {
    "text": "real ways in which\nchanging x should change the probability of y.",
    "start": "3246130",
    "end": "3252180"
  },
  {
    "text": "And one way to do it\nis to assume that there is some vector of parameters.",
    "start": "3252180",
    "end": "3258370"
  },
  {
    "text": "I'll find this case. And then perhaps\nwhat you can do is",
    "start": "3258370",
    "end": "3263950"
  },
  {
    "text": "you can assume some linear\ndependence, where you basically",
    "start": "3263950",
    "end": "3268980"
  },
  {
    "text": "take a linear combination of\nthese axes, these features weighted by these\ncoefficients alpha,",
    "start": "3268980",
    "end": "3275850"
  },
  {
    "text": "and you try to do\nthis as a regression. It's like linear regression\nat the end of the day. You take different\nvalues of x and you",
    "start": "3275850",
    "end": "3283080"
  },
  {
    "text": "map them to different outputs. Now, by itself, this wouldn't\nwork because remember, we",
    "start": "3283080",
    "end": "3290070"
  },
  {
    "text": "have to assume that these\nnumbers are between 0 and 1. But that's something\neasy to fix.",
    "start": "3290070",
    "end": "3295780"
  },
  {
    "text": "You can just\ntransform that value with a function that\nrescales things and maps them",
    "start": "3295780",
    "end": "3301680"
  },
  {
    "text": "to be between 0 and 1. For example, you can use\nthe logistic function or the sigmoid.",
    "start": "3301680",
    "end": "3307890"
  },
  {
    "text": "And if you do that,\nthen you get what's known as logistic regression.",
    "start": "3307890",
    "end": "3313900"
  },
  {
    "text": "It's a way to model a\nconditional distribution of y given x, where\nyou're assuming that",
    "start": "3313900",
    "end": "3320970"
  },
  {
    "text": "that conditional\ndistribution takes a specific functional form. You're assuming that given\ndifferent values of x,",
    "start": "3320970",
    "end": "3328260"
  },
  {
    "text": "you can linearly\ncombine them based on some vector of\ncoefficients, alpha.",
    "start": "3328260",
    "end": "3333780"
  },
  {
    "text": "And then you pass them\nthrough this sigmoid function, this S-shaped function\nthat will take",
    "start": "3333780",
    "end": "3340769"
  },
  {
    "text": "z values between minus\ninfinity and plus infinity and will rescale them\nto be between 0 and 1.",
    "start": "3340770",
    "end": "3348140"
  },
  {
    "text": "So then they are\nvalid probabilities. And that's another\nway to make progress.",
    "start": "3348140",
    "end": "3355030"
  },
  {
    "text": "It's another way to deal with\nthe fact that, in general, you cannot represent this\ncomplicated dependency between y",
    "start": "3355030",
    "end": "3361900"
  },
  {
    "text": "and all the x\nvariables as a table. You have to either\nassume that there is conditional independencies\nor things don't even",
    "start": "3361900",
    "end": "3369099"
  },
  {
    "text": "depend on some of the inputs,\nor you assume that they take-- there is some kind of\nspecific functional form",
    "start": "3369100",
    "end": "3376150"
  },
  {
    "text": "that allows you to compute\nthese probabilities. And this one such assumption\nis the logistic regression",
    "start": "3376150",
    "end": "3381550"
  },
  {
    "text": "assumption. Question. Yeah. Yeah. When you make the linear\ndependence assumption,",
    "start": "3381550",
    "end": "3387610"
  },
  {
    "text": "is that basically saying\nthat the Bayesian network, that the x's are\nindependent and they're not",
    "start": "3387610",
    "end": "3395453"
  },
  {
    "text": "related to each other? And then is that also\nequivalent to just having the joint distribution\nbeing described",
    "start": "3395453",
    "end": "3400930"
  },
  {
    "text": "by the product of the marginals? So the question is\nwhether this implies",
    "start": "3400930",
    "end": "3406420"
  },
  {
    "text": "some conditional\nindependence assumptions. You can actually show\nthe other way around that basically if you assume the\nNaive Bayes factorization, then",
    "start": "3406420",
    "end": "3415005"
  },
  {
    "text": "the conditional\ndistribution of y given x will have this functional\nform, but not vice versa,",
    "start": "3415005",
    "end": "3420859"
  },
  {
    "text": "not necessarily vice versa. And so in some sense, you're\nmaking a weaker statement",
    "start": "3420860",
    "end": "3430040"
  },
  {
    "text": "about the relationships of\nthe random variables, which is why this model is\nstronger in practice.",
    "start": "3430040",
    "end": "3435740"
  },
  {
    "text": "You're assuming less about\nhow the random variables are related. So to the extent that\nyou have enough data",
    "start": "3435740",
    "end": "3441440"
  },
  {
    "text": "to really learn\nthe relationship, you're better off\nwith this model because you are assuming less.",
    "start": "3441440",
    "end": "3448829"
  },
  {
    "text": "If you have very\nlimited data, you might be better off with\nthe Naive Bayes model because you're making\na strong assumption,",
    "start": "3448830",
    "end": "3455283"
  },
  {
    "text": "but the prior helps\nyou more because you don't have enough data to\nfigure out how things are really related to each other.",
    "start": "3455283",
    "end": "3461130"
  },
  {
    "text": "But this is a\ndifferent assumption. You're really saying there\nis some functional form that",
    "start": "3461130",
    "end": "3466167"
  },
  {
    "text": "tells you how the\nrandom variables are related to each other. So that doesn't imply that your\njoint is a product distribution?",
    "start": "3466167",
    "end": "3475190"
  },
  {
    "text": "So the question is, does\nthis imply that the joint is a product distribution?",
    "start": "3475190",
    "end": "3481650"
  },
  {
    "text": "You're just working at the\nlevel of a single conditional. So what we'll see\nis that, in fact,",
    "start": "3481650",
    "end": "3487339"
  },
  {
    "text": "an autoregressive model, a\ndeep autoregressive model will essentially\nbe just be built",
    "start": "3487340",
    "end": "3492710"
  },
  {
    "text": "by assuming that there is\na chain rule factorization and then modeling the\nconditionals using",
    "start": "3492710",
    "end": "3499520"
  },
  {
    "text": "this functional relationship,\nmaybe a linear regression model or a deep neural network.",
    "start": "3499520",
    "end": "3505620"
  },
  {
    "text": "And that's how we will\nbuild the first type of useful, deep\ngenerative model.",
    "start": "3505620",
    "end": "3510650"
  },
  {
    "text": "But this by itself is just\nfor a single conditional. So it's not a statement\nabout the joint.",
    "start": "3510650",
    "end": "3517410"
  },
  {
    "text": "It's just saying I'm\nnot even going to care about modeling the p of X. I'm not going to reason about\nthe inputs that my logistic",
    "start": "3517410",
    "end": "3524570"
  },
  {
    "text": "regression model is going to\nsee, because at test time, I'm always ever going to be--\nsomebody is going to give me",
    "start": "3524570",
    "end": "3529670"
  },
  {
    "text": "the x's. So I don't need to\nbother about figuring out how the different words\nare related to each other.",
    "start": "3529670",
    "end": "3535080"
  },
  {
    "text": "I'm only going to\nbother about modeling how to predict y from x. That's already\nhard, but I'm going",
    "start": "3535080",
    "end": "3540799"
  },
  {
    "text": "to do it based on this\nsimplifying assumption. ",
    "start": "3540800",
    "end": "3548050"
  },
  {
    "text": "And by assuming that you're\nmaking this linear dependence, again, you're making some\nassumptions which might or might",
    "start": "3548050",
    "end": "3555550"
  },
  {
    "text": "not be true in the real world. So in particular, this is a\nrelatively simple dependency",
    "start": "3555550",
    "end": "3561280"
  },
  {
    "text": "that you're assuming\nbetween y and x. And so what you're doing\nis you're saying that--",
    "start": "3561280",
    "end": "3568630"
  },
  {
    "text": "let's say if you have\ntwo features x1 and x2, then you're basically saying\nthat equal probability",
    "start": "3568630",
    "end": "3576760"
  },
  {
    "text": "contours are straight lines. So there are some straight\nlines such that all the points",
    "start": "3576760",
    "end": "3583000"
  },
  {
    "text": "that lie on those\nstraight lines they have the same conditional\nprobability for y. Or it also means that\nthe decision boundary--",
    "start": "3583000",
    "end": "3591310"
  },
  {
    "text": "so if you are using\na threshold to decide whether a variable\nbelongs to class",
    "start": "3591310",
    "end": "3596580"
  },
  {
    "text": "0 or 1 is going to be\nagain, a straight line. So all the points on\nthis side of the line",
    "start": "3596580",
    "end": "3602337"
  },
  {
    "text": "are going to be positive,\nall the other ones are going to be negative. And specifically,\nbasically, it means",
    "start": "3602337",
    "end": "3609000"
  },
  {
    "text": "that if you think about\nhow the probability changes as you change x and y, it has a\nvery specific functional form.",
    "start": "3609000",
    "end": "3617080"
  },
  {
    "text": "It looks like this S kind\nof thing, where the way you change the\nprobability as you",
    "start": "3617080",
    "end": "3623160"
  },
  {
    "text": "change x, the\nprobability of y given x changes as you change x has a\nvery specific functional form.",
    "start": "3623160",
    "end": "3630790"
  },
  {
    "text": "If you think about the\nlookup version of this, it would be an\narbitrary function.",
    "start": "3630790",
    "end": "3637270"
  },
  {
    "text": "Here, you're saying, no, I'm\nwilling to assume that it takes a very specific, relatively\nsimple functional form,",
    "start": "3637270",
    "end": "3644540"
  },
  {
    "text": "which again, might or might\nnot be true in the real world. Maybe the probability\nof y given x",
    "start": "3644540",
    "end": "3649579"
  },
  {
    "text": "should have a very\ndifferent shape and then this model is\nnot going to work well.",
    "start": "3649580",
    "end": "3654710"
  },
  {
    "text": "It's like before,\nwe were assuming conditional independence\nmight or might not be true in the real world.",
    "start": "3654710",
    "end": "3660730"
  },
  {
    "text": "Here we are assuming a\nspecific functional form which might or might not be\ntrue in the real world,",
    "start": "3660730",
    "end": "3665893"
  },
  {
    "text": "and that determines\nwhether or not your model is going to work\nwell or not in practice. ",
    "start": "3665893",
    "end": "3673760"
  },
  {
    "text": "And so, again,\nbasically these are two are dealing with this\nissue of modeling distributions",
    "start": "3673760",
    "end": "3679400"
  },
  {
    "text": "over high dimensional spaces. You have to make assumptions. Naive Bayes is one\nway to make progress,",
    "start": "3679400",
    "end": "3686490"
  },
  {
    "text": "conditional\nindependence assumption. The logistic regression model\ndoes not make that assumption",
    "start": "3686490",
    "end": "3691550"
  },
  {
    "text": "explicitly. It does not assume that the\nfeatures are conditionally independent given the label. So it's a little\nbit more powerful.",
    "start": "3691550",
    "end": "3699680"
  },
  {
    "text": "If you think about the\nspam classification, there might be two\nwords in your vocabulary",
    "start": "3699680",
    "end": "3704960"
  },
  {
    "text": "like bank and account. Knowing whether one appears\nin the email, so knowing x1",
    "start": "3704960",
    "end": "3711830"
  },
  {
    "text": "tells you a lot about whether\nx2 appears in the email, But the Naive Bayes model\nassumes that it doesn't help.",
    "start": "3711830",
    "end": "3719870"
  },
  {
    "text": "So that assumption is clearly\nwrong in the real world. The discriminative model\ndoes not make that assumption",
    "start": "3719870",
    "end": "3727820"
  },
  {
    "text": "explicitly. And so let's say that\nin your data set, these two words always\nappear together.",
    "start": "3727820",
    "end": "3734369"
  },
  {
    "text": "So whenever there is bank,\nthere is also account. The Naive Bayes model is forced\nto assume by construction",
    "start": "3734370",
    "end": "3740859"
  },
  {
    "text": "that they are independent. So whenever you see-- that both of them\nappear, it's going",
    "start": "3740860",
    "end": "3746780"
  },
  {
    "text": "to double count the evidence. It's going to think both of them\nare telling me something about",
    "start": "3746780",
    "end": "3751819"
  },
  {
    "text": "whether this is spam or not. I know that they\nare independent. So when I see both of\nthem at the same time,",
    "start": "3751820",
    "end": "3757280"
  },
  {
    "text": "I'm doubly confident\nthat maybe this is spam. The logistic regression\nmodel can actually just",
    "start": "3757280",
    "end": "3764420"
  },
  {
    "text": "set one of the coefficients\nto 0 and it doesn't double count the evidence.",
    "start": "3764420",
    "end": "3769670"
  },
  {
    "text": "So you can see that you're\nmaking a weaker assumption. And it's actually powerful.",
    "start": "3769670",
    "end": "3775160"
  },
  {
    "text": "And that's why this logistic\nregression model tends to work better in practice. ",
    "start": "3775160",
    "end": "3781599"
  },
  {
    "text": "However, the issue is that\none thing you cannot do,",
    "start": "3781600",
    "end": "3787788"
  },
  {
    "text": "let's say if you have a logistic\nregression model is that you cannot reason about\nyour own inputs.",
    "start": "3787788",
    "end": "3792910"
  },
  {
    "text": "So the only thing you can\ndo is you can map x to y, but you cannot--",
    "start": "3792910",
    "end": "3799000"
  },
  {
    "text": "let's say somebody\ngives you maybe-- I mean, the same thing happens\nalso in image classification.",
    "start": "3799000",
    "end": "3806510"
  },
  {
    "text": "So let's say that\nyou have a model that is predicting a label of\nan image given the image x, that's the only thing you\ncan do-- predict y from x.",
    "start": "3806510",
    "end": "3815090"
  },
  {
    "text": "So if somebody gives you a new\nimage where some of the pixels are missing, there is no way for\nyou to impute the missing values",
    "start": "3815090",
    "end": "3824590"
  },
  {
    "text": "because you don't know\nwhat's the relationship between the x variables. You didn't model p of X at all.",
    "start": "3824590",
    "end": "3830750"
  },
  {
    "text": "You only model p of Y given\nX. And so that's one thing you cannot do with a discriminative\nmodel that you can do with",
    "start": "3830750",
    "end": "3839680"
  },
  {
    "text": "a generative model. A generative model\nis trying to model the full joint distribution\nbetween y and x.",
    "start": "3839680",
    "end": "3848119"
  },
  {
    "text": "And so at least in principle,\nas long as you can do inference, as long as you can compute\nthe right conditionals",
    "start": "3848120",
    "end": "3854480"
  },
  {
    "text": "like modulo\ncomputational issues, you have enough information to\npredict anything from anything.",
    "start": "3854480",
    "end": "3862089"
  },
  {
    "text": "So you can impute\nmissing values, you can do more\ninteresting things.",
    "start": "3862090",
    "end": "3867710"
  },
  {
    "text": "But it's a harder problem\nbecause you're not only modeling the\nrelationship between how",
    "start": "3867710",
    "end": "3873290"
  },
  {
    "text": "to predict y from x, you are\nalso modeling the full thing. You're modeling the relationship\nbetween the features,",
    "start": "3873290",
    "end": "3880010"
  },
  {
    "text": "between the inputs as well. ",
    "start": "3880010",
    "end": "3885410"
  },
  {
    "text": "And then, OK, now, how do\nneural networks come in here?",
    "start": "3885410",
    "end": "3891559"
  },
  {
    "text": "Well, as we said,\none of the issues",
    "start": "3891560",
    "end": "3896762"
  },
  {
    "text": "with a logistic\nregression model is that you're still making some\nkind of simplifying assumption on how y depends on x.",
    "start": "3896762",
    "end": "3903690"
  },
  {
    "text": "We're assuming that there\nis this linear dependence. You take the x, the features,\nyou combine them linearly,",
    "start": "3903690",
    "end": "3910540"
  },
  {
    "text": "you pass them\nthrough the sigmoid, and that's what gives you\ny, which again, might not",
    "start": "3910540",
    "end": "3915910"
  },
  {
    "text": "be true in the real world. And so one way to get\na more expressive--",
    "start": "3915910",
    "end": "3922420"
  },
  {
    "text": "make even weaker\nassumptions in some sense is to basically allow for\nsome nonlinear dependence.",
    "start": "3922420",
    "end": "3932410"
  },
  {
    "text": "You could say\ninstead of directly taking the x features and map\nthem by linearly combining them",
    "start": "3932410",
    "end": "3939380"
  },
  {
    "text": "to a probability value,\nI'm going to compute some features of the input x.",
    "start": "3939380",
    "end": "3945590"
  },
  {
    "text": "Perhaps I'll do it by taking\nsome linear combination of the features and then\napplying a nonlinear function",
    "start": "3945590",
    "end": "3954200"
  },
  {
    "text": "to each value that\nI get out of this, and then I'm going to\ndo linear regression",
    "start": "3954200",
    "end": "3961720"
  },
  {
    "text": "on top of these features. So instead of directly applying\nlinear regression to x, first,",
    "start": "3961720",
    "end": "3968520"
  },
  {
    "text": "I transform x by\nmultiplying it by a matrix A and then shifting by some\nvector of coefficients B,",
    "start": "3968520",
    "end": "3975330"
  },
  {
    "text": "and then I do a logistic\nregression on these features. That's essentially a very\nsimple one-layer neural network.",
    "start": "3975330",
    "end": "3983860"
  },
  {
    "text": "Instead of predicting\ndirectly based on x, I transform x to\nget these features h",
    "start": "3983860",
    "end": "3988960"
  },
  {
    "text": "and then I do linear\nregression based on that. And that's strictly\nmore powerful because now I'm allowed to do\nmore complicated computations.",
    "start": "3988960",
    "end": "3997430"
  },
  {
    "text": "And if you think\nabout that graph, that shape of that function\nof how y depends on x, now",
    "start": "3997430",
    "end": "4004410"
  },
  {
    "text": "I have two more parameters. I have this matrix\nA, this vector of coefficients of biases b.",
    "start": "4004410",
    "end": "4013560"
  },
  {
    "text": "And I can use this to change\nthe shape of the function. I can get more complicated\nrelationships between y and x.",
    "start": "4013560",
    "end": "4022160"
  },
  {
    "text": "And so there's a trade-off here. I'm using more\nparameters to represent",
    "start": "4022160",
    "end": "4027620"
  },
  {
    "text": "this conditional distribution. I no longer have just a\nvector of coefficients alpha, I also have a bunch of\nmatrices for the previous layer",
    "start": "4027620",
    "end": "4035690"
  },
  {
    "text": "in the neural network, but\nthat gives me more flexibility in predicting y from x.",
    "start": "4035690",
    "end": "4042110"
  },
  {
    "text": "And of course, you can imagine\nstacking this many, many times.",
    "start": "4042110",
    "end": "4047490"
  },
  {
    "text": "And then you can use\na deep neural network to predict y from x.",
    "start": "4047490",
    "end": "4052905"
  },
  {
    "text": "Was there a question? Yeah. Why do we still need alpha here? [COUGH]",
    "start": "4052905",
    "end": "4058474"
  },
  {
    "text": " I guess you still\nwant to have a--",
    "start": "4058474",
    "end": "4065714"
  },
  {
    "text": "you still want to make\nit deeper, I guess, and you want to map it\nto a scalar eventually.",
    "start": "4065715",
    "end": "4072140"
  },
  {
    "text": "So I guess ax plus\nb is a vector, and then I'm trying\nto map it to a scalar so I use this alpha vector.",
    "start": "4072140",
    "end": "4078670"
  },
  {
    "text": "But it's just making\nit explicit that it's a strict generalization\nof what I had before,",
    "start": "4078670",
    "end": "4087819"
  },
  {
    "text": "but you do want it\nto be eventually be mapped to a\nsingle scalar value.",
    "start": "4087820",
    "end": "4094540"
  },
  {
    "text": "That would be like the\nsoftmax at the end. Although this one\nis just binary, so it's not quite a softmax,\nbut it's essentially",
    "start": "4094540",
    "end": "4101693"
  },
  {
    "text": "the softmax at the end\nof a neural network that maps the output to a\nvalid probability value.",
    "start": "4101694",
    "end": "4107689"
  },
  {
    "text": "So if y were a categorical\nrandom variable, then that would exactly\nbe the softmax at the end.",
    "start": "4107689",
    "end": "4113950"
  },
  {
    "text": "Thank you.  OK.",
    "start": "4113950",
    "end": "4119866"
  },
  {
    "text": "And then, yeah. Essentially, what\nyou can do is you can repeat this\nmultiple times and you",
    "start": "4119866",
    "end": "4125318"
  },
  {
    "text": "can get a more expressive\nway of capturing the relationship between some y\nvariable and the input variable",
    "start": "4125319",
    "end": "4132939"
  },
  {
    "text": "x. And this is going\nto be the building block that we're going to use\nto build deep generative models.",
    "start": "4132939",
    "end": "4139990"
  },
  {
    "text": "So what we're going\nto do is we're going to take\nadvantage of this fact that neural networks seem\nto work very well at solving",
    "start": "4139990",
    "end": "4147430"
  },
  {
    "text": "this kind of prediction\ntask, and we're going to combine them to\nbuild generative models.",
    "start": "4147430",
    "end": "4153810"
  },
  {
    "text": "And the simplest way to\ndo it is to use chain rule and then use neural\nnetworks to represent",
    "start": "4153810",
    "end": "4160649"
  },
  {
    "text": "each one of those conditions. And that's essentially on\nneural autoregressive model",
    "start": "4160649",
    "end": "4166580"
  },
  {
    "text": "and essentially that's what\nlarge language models do. They use chain rule and then\nthey simplify the conditionals",
    "start": "4166580",
    "end": "4175210"
  },
  {
    "text": "by assuming that you can model\nthem using a neural network.",
    "start": "4175210",
    "end": "4181450"
  },
  {
    "text": "So you can predict the next\nword given the previous ones using a neural network. ",
    "start": "4181450",
    "end": "4189609"
  },
  {
    "text": "But there is going\nto be other ways. When we see other classes\nof generative models they are still going to\nuse this kind of ideas,",
    "start": "4189609",
    "end": "4195820"
  },
  {
    "text": "but maybe we're going to\ncombine them in different ways and we're going to get different\ntypes of generative models.",
    "start": "4195820",
    "end": "4202660"
  },
  {
    "text": "So that's the story. There is the chain\nrule factorization,",
    "start": "4202660",
    "end": "4207910"
  },
  {
    "text": "which is fully general. So given a joint, you\ncan always write it as a product of conditionals\nwith no assumptions.",
    "start": "4207910",
    "end": "4215260"
  },
  {
    "text": "In a Bayesian\nnetwork, you're going to try to simplify these\nconditionals somehow by assuming that the variables\nare conditionally independent.",
    "start": "4215260",
    "end": "4223570"
  },
  {
    "text": "So whenever you're\ntrying to predict x4, you don't really need x2\nand x3, you just need x1,",
    "start": "4223570",
    "end": "4229120"
  },
  {
    "text": "for example, which is\nusually too strong. And this doesn't work on\nhigh dimensional data sets,",
    "start": "4229120",
    "end": "4234820"
  },
  {
    "text": "on images, text, the kind\nof things we care about. The one class of deep generative\nmodels, a very successful one,",
    "start": "4234820",
    "end": "4244600"
  },
  {
    "text": "conceptually does this. It just replaces all\nthese conditionals that we don't know how to deal\nwith with neural networks.",
    "start": "4244600",
    "end": "4253690"
  },
  {
    "text": "And you can choose\ndifferent architectures, but fundamentally,\nthat's the whole idea.",
    "start": "4253690",
    "end": "4259540"
  },
  {
    "text": "We're going to use\na neural network to predict what's\nthe fourth word given",
    "start": "4259540",
    "end": "4264670"
  },
  {
    "text": "the first, the\nsecond, and the third. And again, there's no\nfree lunch in the sense that what we're\ngiving up is we're",
    "start": "4264670",
    "end": "4272380"
  },
  {
    "text": "assuming that there\nis some relationship that these conditional\ndistributions can basically be captured by a neural network,\nwhich might or might not",
    "start": "4272380",
    "end": "4282550"
  },
  {
    "text": "be the case in practice. But that's one way to get\ntractability to the extent",
    "start": "4282550",
    "end": "4289150"
  },
  {
    "text": "that these neural\nnetworks are not too big and somehow you're able\nto tie them together. You can see that you need\na different neural network",
    "start": "4289150",
    "end": "4296050"
  },
  {
    "text": "for every position in\nthe sequence, which would be very tricky. So somehow you\nneed to figure out",
    "start": "4296050",
    "end": "4301420"
  },
  {
    "text": "a way to tie together the\nweights of this neural network so this can be done in practice.",
    "start": "4301420",
    "end": "4307590"
  },
  {
    "text": "But ideally, this is the one way\nto get a deep generative model.",
    "start": "4307590",
    "end": "4314389"
  },
  {
    "text": "In the Bayes net,\nthe last factor should be x4 given x3, right?",
    "start": "4314390",
    "end": "4322219"
  },
  {
    "text": "Yeah, it could be\nanything, I guess. It depends what's the shape\nof the neural network. If you were to do a Markov\nmodel, it should be x3.",
    "start": "4322220",
    "end": "4331429"
  },
  {
    "text": "But you could say that maybe\nthe fourth world is completely specified by the first one.",
    "start": "4331430",
    "end": "4338357"
  },
  {
    "text": "And you don't need the\nsecond and the third, don't help you in predicting\nthe fourth and the first, which is probably a\nvery weird assumption.",
    "start": "4338357",
    "end": "4345150"
  },
  {
    "text": "It wouldn't work in practice,\nbut the underlying idea is that you're going to simplify\nthose conditionals by dropping",
    "start": "4345150",
    "end": "4352280"
  },
  {
    "text": "the dependence on\nsome variables, and that gives you\na Bayesian network. Depending on which\nvariables you drop,",
    "start": "4352280",
    "end": "4358220"
  },
  {
    "text": "you're going to get\ndifferent graphs. If you were to not drop\nany variable, you get this.",
    "start": "4358220",
    "end": "4364360"
  },
  {
    "text": "You get the fully general model\nand that makes no assumptions. So that's fully\ngeneral, but it's",
    "start": "4364360",
    "end": "4371730"
  },
  {
    "text": "too expensive because these\nconditionals are too--",
    "start": "4371730",
    "end": "4376770"
  },
  {
    "text": "whenever you're conditioning\non too many things, that conditional distribution\nis too complicated.",
    "start": "4376770",
    "end": "4382260"
  },
  {
    "text": "And you cannot store\nit, you cannot learn it, and so you cannot actually\nuse it in practice. ",
    "start": "4382260",
    "end": "4390750"
  },
  {
    "text": "Cool. The last thing I\nwanted to mention",
    "start": "4390750",
    "end": "4396150"
  },
  {
    "text": "is how to deal with\ncontinuous variables. So we often want to model\nnot just discrete data,",
    "start": "4396150",
    "end": "4403309"
  },
  {
    "text": "but actually data\nthat is more naturally thought of as continuous. So taking values over\nthe whole real axis.",
    "start": "4403310",
    "end": "4412219"
  },
  {
    "text": "And luckily the machinery\nis very similar. So here instead of working with\nprobability mass functions,",
    "start": "4412220",
    "end": "4418580"
  },
  {
    "text": "we work with probability\ndensity functions. And here you can\nstart to see how",
    "start": "4418580",
    "end": "4425360"
  },
  {
    "text": "the idea of working\nwith tables already doesn't work because there\nis an infinite number",
    "start": "4425360",
    "end": "4430730"
  },
  {
    "text": "of different values\nthat x can take. You cannot write down a table\nthat will assign a number",
    "start": "4430730",
    "end": "4435800"
  },
  {
    "text": "to each one of them. So you have to basically assume\nthat there is some functional",
    "start": "4435800",
    "end": "4442670"
  },
  {
    "text": "form. There's some\nfunctions that you can use to map different\nvalues of x to a scalar.",
    "start": "4442670",
    "end": "4451550"
  },
  {
    "text": "And for example, you\ncan assume that x is Gaussian, which\nmeans that there is a relatively simple function\nthat depends on two parameters,",
    "start": "4451550",
    "end": "4459600"
  },
  {
    "text": "mu and sigma. And then you can plug\nthem into this expression and you get back the\ndensity of the Gaussian",
    "start": "4459600",
    "end": "4467300"
  },
  {
    "text": "at any particular point x. Mu and sigma. Here are the mean and\nthe standard deviation",
    "start": "4467300",
    "end": "4472943"
  },
  {
    "text": "of the Gaussian. Or you could say, OK, maybe\na uniform random variable. Again, this is another\nrelatively simple function",
    "start": "4472943",
    "end": "4481610"
  },
  {
    "text": "that you can use to\nmap x to densities. Uniform distribution over\nthe interval between A and B",
    "start": "4481610",
    "end": "4489500"
  },
  {
    "text": "would have that kind of\nfunctional form, et cetera. And the good news is\nthat, again, we often care",
    "start": "4489500",
    "end": "4497750"
  },
  {
    "text": "about modeling many\nrandom variables, which could be continuous or maybe a\nmix of continuous and discrete.",
    "start": "4497750",
    "end": "4504139"
  },
  {
    "text": "In this case, we care about\nthe joint probability density",
    "start": "4504140",
    "end": "4509960"
  },
  {
    "text": "function. And the same kind of, for\nexample, a joint Gaussian",
    "start": "4509960",
    "end": "4516239"
  },
  {
    "text": "would have that functional form. So now x is a vector of numbers.",
    "start": "4516240",
    "end": "4522070"
  },
  {
    "text": "And the good news is that the\nwhole machinery of chain rule, based rule, they\nall still apply.",
    "start": "4522070",
    "end": "4527949"
  },
  {
    "text": "So, for example, we can write\ndown the joint over PDF, over probability density\nfunction over three",
    "start": "4527950",
    "end": "4533860"
  },
  {
    "text": "random variables as a marginal\nPDF over the first one, a conditional over the second\ngiven the first, and so forth.",
    "start": "4533860",
    "end": "4544270"
  },
  {
    "text": "And this is useful because\nwe can again mix and match.",
    "start": "4544270",
    "end": "4550240"
  },
  {
    "text": "We can use Bayesian\nnetworks or we can use neural networks\nplus Bayesian networks",
    "start": "4550240",
    "end": "4555610"
  },
  {
    "text": "in different ways to\nget different types of generative models.",
    "start": "4555610",
    "end": "4561070"
  },
  {
    "text": "So for example, you\ncan get a mixture of two Gaussians using a simple\nBayesian network with two",
    "start": "4561070",
    "end": "4568570"
  },
  {
    "text": "random variables, z and x. So the Bayesian network\nhas two random variables,",
    "start": "4568570",
    "end": "4574780"
  },
  {
    "text": "z and x. x has z as a parent. z doesn't have any parent.",
    "start": "4574780",
    "end": "4580449"
  },
  {
    "text": "And so what it means is\nthat the joint over x and z can be factorized as the\nprobability of z times",
    "start": "4580450",
    "end": "4587120"
  },
  {
    "text": "the probability of x given z. And for example, you could say\nz is a Bernoulli random variable",
    "start": "4587120",
    "end": "4595020"
  },
  {
    "text": "with parameter p. So z is binary. It's either 0 or 1,\nand you choose a value",
    "start": "4595020",
    "end": "4602640"
  },
  {
    "text": "with flipping a biased\ncoin with probability P. And then condition on z,\nyou choose a value for x by,",
    "start": "4602640",
    "end": "4611580"
  },
  {
    "text": "let's say, sampling\nfrom a Gaussian. And because z can take\ntwo different values, there's actually two Gaussians.",
    "start": "4611580",
    "end": "4618030"
  },
  {
    "text": "There is one\nGaussian when z is 0 and there is one\nGaussian when z is 1. And these two\nGaussians are allowed",
    "start": "4618030",
    "end": "4624360"
  },
  {
    "text": "to have different means\nand different variances. So this would be a\nkind of graphical model",
    "start": "4624360",
    "end": "4632160"
  },
  {
    "text": "that corresponds to a\nmixture of two Gaussians. And because you're mixing\ntogether two Gaussians, you have a slightly\nmore flexible model.",
    "start": "4632160",
    "end": "4640139"
  },
  {
    "text": "The parameters here are p, which\nis the probability of choosing 0 versus 1 for this\nlatency variables,",
    "start": "4640140",
    "end": "4646870"
  },
  {
    "text": "and then you have the means\nand the standard deviations.",
    "start": "4646870",
    "end": "4652270"
  },
  {
    "text": "Of course, you could\nchoose other things. For example, you could choose z\nto be a uniform random variable",
    "start": "4652270",
    "end": "4659290"
  },
  {
    "text": "between A and B. And then\ngiven z, x, let's say, is a Gaussian with\na mean, which is z,",
    "start": "4659290",
    "end": "4666130"
  },
  {
    "text": "and then maybe a fixed\nstandard deviation. ",
    "start": "4666130",
    "end": "4671140"
  },
  {
    "text": "Just another example. A more interesting one is\nthe variational autoencoder,",
    "start": "4671140",
    "end": "4678580"
  },
  {
    "text": "which we're going to cover\nin depth in future lectures. But at the end of the day,\na variational autoencoder",
    "start": "4678580",
    "end": "4684909"
  },
  {
    "text": "is this Bayesian network\nwith two nodes, z and x. And the assumption is that z\nis sampled from a Gaussian.",
    "start": "4684910",
    "end": "4692800"
  },
  {
    "text": "So p of z is just a simple\nGaussian random variable.",
    "start": "4692800",
    "end": "4698175"
  },
  {
    "text": "And here you see\nhow we are going to mix and match Bayesian\nnetworks and neural networks.",
    "start": "4698175",
    "end": "4703780"
  },
  {
    "text": "Given z, x is again a\nGaussian distribution.",
    "start": "4703780",
    "end": "4709599"
  },
  {
    "text": "But the mean and the\nvariance of this Gaussian are the outputs of\nsome neural network",
    "start": "4709600",
    "end": "4717390"
  },
  {
    "text": "or two neural networks\nmu theta and sigma phi, which depend on z.",
    "start": "4717390",
    "end": "4725840"
  },
  {
    "text": "So the sampling process is like\na generalization of the ones you see before where again, you\nfirst sample z, then you feed",
    "start": "4725840",
    "end": "4734389"
  },
  {
    "text": "z into a neural network\nthat will give you means and variances that\nyou're using another Gaussian",
    "start": "4734390",
    "end": "4740989"
  },
  {
    "text": "distribution to\nsample a value for x. And this kind of\nmachinery is essentially",
    "start": "4740990",
    "end": "4746750"
  },
  {
    "text": "a variational autoencoder. This corresponds to\nthe generative process that you use in a VAE or\na variational autoencoder.",
    "start": "4746750",
    "end": "4754400"
  },
  {
    "text": "And we're going to have to\ntalk about how you actually train these kinds of models\nand how to learn them,",
    "start": "4754400",
    "end": "4759900"
  },
  {
    "text": "but fundamentally, you\nsee how we take this idea",
    "start": "4759900",
    "end": "4765260"
  },
  {
    "text": "so I can mix and match them. There's a little bit\nof Bayesian network,",
    "start": "4765260",
    "end": "4770270"
  },
  {
    "text": "a little bit of chain rule, a\nlittle bit of neural networks to represent complicated\nconditionals,",
    "start": "4770270",
    "end": "4776449"
  },
  {
    "text": "but everything can\nbe stitched together. And that's how you get different\nkinds of generative models.",
    "start": "4776450",
    "end": "4782570"
  },
  {
    "text": "And yeah, just as a note,\neven though mu and sigma",
    "start": "4782570",
    "end": "4787949"
  },
  {
    "text": "could be very complicated,\nthe conditional distribution of x given z is still\nGaussian in this case.",
    "start": "4787950",
    "end": "4793090"
  },
  {
    "text": "So there are some\nkind of trade-offs that you have to deal with. And yeah, this is it for today.",
    "start": "4793090",
    "end": "4800350"
  },
  {
    "text": "And then next,\nwe're going to talk about autoregressive models. ",
    "start": "4800350",
    "end": "4809000"
  }
]