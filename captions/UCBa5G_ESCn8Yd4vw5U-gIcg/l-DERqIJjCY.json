[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "start": "0",
    "end": "5268"
  },
  {
    "text": "CHRIS POTTS: Welcome, everyone.",
    "start": "5268",
    "end": "6560"
  },
  {
    "text": "This is part three in our\nseries on methods and metrics.",
    "start": "6560",
    "end": "8770"
  },
  {
    "text": "We're going to be\ntalking about metrics",
    "start": "8770",
    "end": "10395"
  },
  {
    "text": "for assessing natural\nlanguage generation systems.",
    "start": "10395",
    "end": "13010"
  },
  {
    "text": "We previously talked\nabout classifier metrics,",
    "start": "13010",
    "end": "14967"
  },
  {
    "text": "and the issues seem\nrelatively straightforward.",
    "start": "14967",
    "end": "17980"
  },
  {
    "text": "As you'll see,\nassessment for NLG",
    "start": "17980",
    "end": "20020"
  },
  {
    "text": "systems is considerably\nmore difficult.",
    "start": "20020",
    "end": "22798"
  },
  {
    "text": "Let's actually begin with\nthose fundamental challenges.",
    "start": "22798",
    "end": "25090"
  },
  {
    "start": "23000",
    "end": "75000"
  },
  {
    "text": "Maybe the most\nfundamental of all",
    "start": "25090",
    "end": "26620"
  },
  {
    "text": "is that in natural\nlanguage, there",
    "start": "26620",
    "end": "28330"
  },
  {
    "text": "is more than one effective\nway to say most things.",
    "start": "28330",
    "end": "31240"
  },
  {
    "text": "The datasets we\nhave might have one",
    "start": "31240",
    "end": "33190"
  },
  {
    "text": "or a few good examples of\nhow something should be said.",
    "start": "33190",
    "end": "36460"
  },
  {
    "text": "But that's just a sample of\nthe many ways in which we",
    "start": "36460",
    "end": "38980"
  },
  {
    "text": "could communicate effectively.",
    "start": "38980",
    "end": "40690"
  },
  {
    "text": "And that leaves us with\nfundamental open questions",
    "start": "40690",
    "end": "42969"
  },
  {
    "text": "about what comparisons\nwe should make",
    "start": "42970",
    "end": "44920"
  },
  {
    "text": "and how we should assess\nso-called mistakes.",
    "start": "44920",
    "end": "48280"
  },
  {
    "text": "Relatedly, there's\njust an open question",
    "start": "48280",
    "end": "50199"
  },
  {
    "text": "of what we're actually\ntrying to measure.",
    "start": "50200",
    "end": "52000"
  },
  {
    "text": "Is it fluency?",
    "start": "52000",
    "end": "53530"
  },
  {
    "text": "Or truthfulness?",
    "start": "53530",
    "end": "54820"
  },
  {
    "text": "Or communicative effectiveness?",
    "start": "54820",
    "end": "56440"
  },
  {
    "text": "Or some blend of the three?",
    "start": "56440",
    "end": "58180"
  },
  {
    "text": "As we think about\ndifferent metrics,",
    "start": "58180",
    "end": "59920"
  },
  {
    "text": "we might find that\nthey capture one",
    "start": "59920",
    "end": "62109"
  },
  {
    "text": "or a few of these and\ncompletely neglect others",
    "start": "62110",
    "end": "64839"
  },
  {
    "text": "and that's sure to\nshape the trajectory",
    "start": "64840",
    "end": "66939"
  },
  {
    "text": "of our project and the\nactual goals that we achieve.",
    "start": "66940",
    "end": "69260"
  },
  {
    "text": "So we have to be really\nthoughtful about what",
    "start": "69260",
    "end": "71560"
  },
  {
    "text": "we're actually trying to\nmeasure in this space.",
    "start": "71560",
    "end": "75340"
  },
  {
    "start": "75000",
    "end": "243000"
  },
  {
    "text": "Let's begin with perplexity.",
    "start": "75340",
    "end": "76750"
  },
  {
    "text": "I would say what\nperplexity has going",
    "start": "76750",
    "end": "78460"
  },
  {
    "text": "for it is that it is at\nleast very tightly knit",
    "start": "78460",
    "end": "81400"
  },
  {
    "text": "to the structure of\nmany of the models",
    "start": "81400",
    "end": "83740"
  },
  {
    "text": "that we work with in NLG.",
    "start": "83740",
    "end": "85810"
  },
  {
    "text": "So the core calculation\nis that given",
    "start": "85810",
    "end": "88659"
  },
  {
    "text": "some sequence x of length n, and\na probability distribution p,",
    "start": "88660",
    "end": "93880"
  },
  {
    "text": "the perplexity of x relative\nto that distribution",
    "start": "93880",
    "end": "96790"
  },
  {
    "text": "is the product of the inverse of\nall the assigned probabilities.",
    "start": "96790",
    "end": "100120"
  },
  {
    "text": "And then we take\nan average here.",
    "start": "100120",
    "end": "101980"
  },
  {
    "text": "There are many ways to express\nthis calculation and many ways",
    "start": "101980",
    "end": "105100"
  },
  {
    "text": "to connect with information\ntheoretic measures.",
    "start": "105100",
    "end": "107770"
  },
  {
    "text": "Let me defer those\nissues for just a second,",
    "start": "107770",
    "end": "110020"
  },
  {
    "text": "and I'll try to build up an\nintuition just after getting",
    "start": "110020",
    "end": "113289"
  },
  {
    "text": "through the core calculation.",
    "start": "113290",
    "end": "115130"
  },
  {
    "text": "So that's perplexity.",
    "start": "115130",
    "end": "116049"
  },
  {
    "text": "And then when we do\ntoken-level perplexity, right,",
    "start": "116050",
    "end": "118780"
  },
  {
    "text": "we want to assign perplexity\nto individual examples,",
    "start": "118780",
    "end": "121570"
  },
  {
    "text": "we need to normalize by the\nlength of those examples.",
    "start": "121570",
    "end": "124330"
  },
  {
    "text": "And we do that in\nlog space in order",
    "start": "124330",
    "end": "126460"
  },
  {
    "text": "to capture the kind of\ngeometric mean, which",
    "start": "126460",
    "end": "129069"
  },
  {
    "text": "is arguably more appropriate for\ncomparing probability values.",
    "start": "129070",
    "end": "133360"
  },
  {
    "text": "And then if we\nwant the perplexity",
    "start": "133360",
    "end": "135640"
  },
  {
    "text": "for an entire corpus, we\nagain use a geometric mean",
    "start": "135640",
    "end": "139170"
  },
  {
    "text": "of all the token-level\nperplexity predictions.",
    "start": "139170",
    "end": "144010"
  },
  {
    "text": "And that gives us a single\nquantity over an entire batch",
    "start": "144010",
    "end": "146590"
  },
  {
    "text": "of examples.",
    "start": "146590",
    "end": "149077"
  },
  {
    "text": "What are the properties\nof perplexity?",
    "start": "149077",
    "end": "150659"
  },
  {
    "text": "Well its bounds are 1 to\ninfinity with 1 the best,",
    "start": "150660",
    "end": "153270"
  },
  {
    "text": "so we would like to\nminimize perplexity.",
    "start": "153270",
    "end": "155940"
  },
  {
    "text": "It is equivalent to\nthe exponentiation",
    "start": "155940",
    "end": "158130"
  },
  {
    "text": "of the cross-entropy loss.",
    "start": "158130",
    "end": "159580"
  },
  {
    "text": "That's the tight\nconnection with models",
    "start": "159580",
    "end": "161790"
  },
  {
    "text": "that I wanted to call out.",
    "start": "161790",
    "end": "163409"
  },
  {
    "text": "We often work with\nlanguage models",
    "start": "163410",
    "end": "165060"
  },
  {
    "text": "that use a cross-entropy loss.",
    "start": "165060",
    "end": "166657"
  },
  {
    "text": "And you can see that\nthey are directly",
    "start": "166657",
    "end": "168239"
  },
  {
    "text": "optimizing for a quantity that\nis proportional to perplexity.",
    "start": "168240",
    "end": "171698"
  },
  {
    "text": "Now that can be useful\nas a kind of getting",
    "start": "171698",
    "end": "173490"
  },
  {
    "text": "a direct insight into the nature\nof your model's predictions.",
    "start": "173490",
    "end": "177960"
  },
  {
    "text": "What value does it encode?",
    "start": "177960",
    "end": "179198"
  },
  {
    "text": "Well I think it's simple.",
    "start": "179198",
    "end": "180240"
  },
  {
    "text": "Does the model assign\nhigh probability",
    "start": "180240",
    "end": "182580"
  },
  {
    "text": "to the input sequences?",
    "start": "182580",
    "end": "183960"
  },
  {
    "text": "That is, does it\nassign low perplexity",
    "start": "183960",
    "end": "185970"
  },
  {
    "text": "to the input sequences?",
    "start": "185970",
    "end": "188300"
  },
  {
    "text": "The weaknesses, there\nare many actually.",
    "start": "188300",
    "end": "190700"
  },
  {
    "text": "First, it's heavily dependent\non the underlying vocabulary.",
    "start": "190700",
    "end": "193910"
  },
  {
    "text": "To see that,\nimagine an edge case",
    "start": "193910",
    "end": "195680"
  },
  {
    "text": "where we take every\nword in the vocabulary",
    "start": "195680",
    "end": "198049"
  },
  {
    "text": "and map it to a\nsingle UNK token.",
    "start": "198050",
    "end": "200240"
  },
  {
    "text": "In that case, we will\nabsolutely minimize perplexity,",
    "start": "200240",
    "end": "203240"
  },
  {
    "text": "but our system will be useless.",
    "start": "203240",
    "end": "205070"
  },
  {
    "text": "In that edge case, you can see\nthat I could reduce perplexity",
    "start": "205070",
    "end": "209060"
  },
  {
    "text": "simply by changing the\nsize of my vocabulary.",
    "start": "209060",
    "end": "212133"
  },
  {
    "text": "That's a way that you\ncould kind of game",
    "start": "212133",
    "end": "213800"
  },
  {
    "text": "this metric inadvertently.",
    "start": "213800",
    "end": "216110"
  },
  {
    "text": "As a result of that,\nwe can't really",
    "start": "216110",
    "end": "217790"
  },
  {
    "text": "make comparisons across\ndatasets because of course they",
    "start": "217790",
    "end": "220159"
  },
  {
    "text": "could have different\nvocabularies",
    "start": "220160",
    "end": "221960"
  },
  {
    "text": "and different intrinsic\nnotions of perplexity.",
    "start": "221960",
    "end": "225230"
  },
  {
    "text": "And it's also even tricky to\nmake comparisons across models.",
    "start": "225230",
    "end": "228409"
  },
  {
    "text": "You can see that in my\nfirst weakness there.",
    "start": "228410",
    "end": "231710"
  },
  {
    "text": "If we do compare models,\nwe need to fix the data set",
    "start": "231710",
    "end": "234860"
  },
  {
    "text": "and make sure that the\ndifferences between the models",
    "start": "234860",
    "end": "237560"
  },
  {
    "text": "are not inherently shaping\nthe range of perplexity values",
    "start": "237560",
    "end": "241130"
  },
  {
    "text": "that we're likely to see.",
    "start": "241130",
    "end": "243540"
  },
  {
    "start": "243000",
    "end": "362000"
  },
  {
    "text": "Let's move on now\ninto a family of what",
    "start": "243540",
    "end": "245209"
  },
  {
    "text": "you might think of as\nn-gram based methods",
    "start": "245210",
    "end": "247250"
  },
  {
    "text": "for assessing NLG\nsystems, beginning",
    "start": "247250",
    "end": "249470"
  },
  {
    "text": "with the word-error rate.",
    "start": "249470",
    "end": "251210"
  },
  {
    "text": "So the fundamental thing\nhere will be an edit distance",
    "start": "251210",
    "end": "254000"
  },
  {
    "text": "measure.",
    "start": "254000",
    "end": "255710"
  },
  {
    "text": "And therefore you can\nsee word-error rate",
    "start": "255710",
    "end": "257630"
  },
  {
    "text": "as a kind of family\nof measures depending",
    "start": "257630",
    "end": "259760"
  },
  {
    "text": "on the choice of the edit\ndistance function, which",
    "start": "259760",
    "end": "261980"
  },
  {
    "text": "we would just plug in.",
    "start": "261980",
    "end": "264230"
  },
  {
    "text": "The word error rate\nis the distance",
    "start": "264230",
    "end": "266660"
  },
  {
    "text": "between the actual sequence\nx and some predicted",
    "start": "266660",
    "end": "269510"
  },
  {
    "text": "sequence pred normalized by the\nlength of the actual sequence.",
    "start": "269510",
    "end": "273783"
  },
  {
    "text": "And if we would like\nthe word-error rate",
    "start": "273783",
    "end": "275449"
  },
  {
    "text": "for an entire corpus,\nit's easy to scale it up,",
    "start": "275450",
    "end": "278030"
  },
  {
    "text": "but there's one twist here.",
    "start": "278030",
    "end": "279630"
  },
  {
    "text": "The way that's\nstandardly calculated",
    "start": "279630",
    "end": "281300"
  },
  {
    "text": "is that the numerator\nis the sum of all",
    "start": "281300",
    "end": "283550"
  },
  {
    "text": "the distances between the\nactual and predicted sequences.",
    "start": "283550",
    "end": "287090"
  },
  {
    "text": "Not normalized as it was up\nhere for the word error rate.",
    "start": "287090",
    "end": "290360"
  },
  {
    "text": "The normalization that happens\nover the entire corpus,",
    "start": "290360",
    "end": "292789"
  },
  {
    "text": "it's the sum of all the\nlengths of the actual strings",
    "start": "292790",
    "end": "295580"
  },
  {
    "text": "in the corpus.",
    "start": "295580",
    "end": "296610"
  },
  {
    "text": "So we have one\naverage as opposed",
    "start": "296610",
    "end": "298069"
  },
  {
    "text": "to taking an\naverage of averages.",
    "start": "298070",
    "end": "301360"
  },
  {
    "text": "The properties of\nthe word error rate.",
    "start": "301360",
    "end": "302949"
  },
  {
    "text": "Its bounds are 0\nto infinity, and we",
    "start": "302950",
    "end": "304810"
  },
  {
    "text": "would like to minimize\nit, so 0 is the best.",
    "start": "304810",
    "end": "307690"
  },
  {
    "text": "The value encoded is\nsimilar to F scores.",
    "start": "307690",
    "end": "310120"
  },
  {
    "text": "We would like to\nanswer the question,",
    "start": "310120",
    "end": "311900"
  },
  {
    "text": "how aligned is the\npredicted sequence",
    "start": "311900",
    "end": "314350"
  },
  {
    "text": "with the actual sequence.",
    "start": "314350",
    "end": "315700"
  },
  {
    "text": "And I've invoked F scores here\nbecause if our edit distance",
    "start": "315700",
    "end": "318910"
  },
  {
    "text": "measure has notions of\ninsertion and deletion,",
    "start": "318910",
    "end": "321580"
  },
  {
    "text": "they play roles that are\nanalogous to precision",
    "start": "321580",
    "end": "323870"
  },
  {
    "text": "and recall.",
    "start": "323870",
    "end": "325900"
  },
  {
    "text": "The weaknesses.",
    "start": "325900",
    "end": "326750"
  },
  {
    "text": "Well first, we have just\none reference text here.",
    "start": "326750",
    "end": "329680"
  },
  {
    "text": "I called out before that\nthere are often many good ways",
    "start": "329680",
    "end": "333057"
  },
  {
    "text": "to say something,\nwhereas here we can",
    "start": "333058",
    "end": "334600"
  },
  {
    "text": "make only a single comparison.",
    "start": "334600",
    "end": "336760"
  },
  {
    "text": "And it's also, maybe\nthis is more fundamental,",
    "start": "336760",
    "end": "338860"
  },
  {
    "text": "word error rate is a\nvery syntactic notion.",
    "start": "338860",
    "end": "341150"
  },
  {
    "text": "Just consider comparing\ntext like, it was good,",
    "start": "341150",
    "end": "344139"
  },
  {
    "text": "it was not good,\nand it was great.",
    "start": "344140",
    "end": "346310"
  },
  {
    "text": "They're likely to have\nthe identical word error",
    "start": "346310",
    "end": "348550"
  },
  {
    "text": "rates, even though the first\ntwo differ dramatically",
    "start": "348550",
    "end": "351639"
  },
  {
    "text": "in their meanings and\nthe first and the third",
    "start": "351640",
    "end": "353590"
  },
  {
    "text": "are actually rather\nsimilar in their meanings.",
    "start": "353590",
    "end": "355750"
  },
  {
    "text": "That semantic\nnotion of similarity",
    "start": "355750",
    "end": "357700"
  },
  {
    "text": "is unlikely to be reflected\nin the word-error rate.",
    "start": "357700",
    "end": "362760"
  },
  {
    "start": "362000",
    "end": "469000"
  },
  {
    "text": "Let's move now to BLEU scores.",
    "start": "362760",
    "end": "364110"
  },
  {
    "text": "This is another\nn-gram based metric,",
    "start": "364110",
    "end": "366060"
  },
  {
    "text": "but it's going to try to\naddress the fact that we want",
    "start": "366060",
    "end": "368340"
  },
  {
    "text": "to make comparisons against\nmultiple human-created",
    "start": "368340",
    "end": "371490"
  },
  {
    "text": "reference texts.",
    "start": "371490",
    "end": "374180"
  },
  {
    "text": "It has a notion of\nprecision in it,",
    "start": "374180",
    "end": "375770"
  },
  {
    "text": "but it's called modified\nn-gram precision.",
    "start": "375770",
    "end": "377900"
  },
  {
    "text": "Let me walk you\nthrough an example",
    "start": "377900",
    "end": "379460"
  },
  {
    "text": "and hopefully that\nwill motivate.",
    "start": "379460",
    "end": "381185"
  },
  {
    "text": "Imagine we have\nthe candidate that",
    "start": "381185",
    "end": "382850"
  },
  {
    "text": "had just seven instances\nof the word the\" in it.",
    "start": "382850",
    "end": "385660"
  },
  {
    "text": "And we have two reference texts,\npresumably written by humans.",
    "start": "385660",
    "end": "388700"
  },
  {
    "text": "The cat is on the mat.",
    "start": "388700",
    "end": "390120"
  },
  {
    "text": "And there is a cat on the mat.",
    "start": "390120",
    "end": "392479"
  },
  {
    "text": "The modified precision\ntakes for the token \"the.\"",
    "start": "392480",
    "end": "395740"
  },
  {
    "text": "The maximum number\nof times that \"the\"",
    "start": "395740",
    "end": "397699"
  },
  {
    "text": "occurs in any reference text,\nand that's 2 with reference 1",
    "start": "397700",
    "end": "401150"
  },
  {
    "text": "here.",
    "start": "401150",
    "end": "402080"
  },
  {
    "text": "And it divides that\nby the number of times",
    "start": "402080",
    "end": "404060"
  },
  {
    "text": "that \"the\" appears in the\ncandidate, which is 7.",
    "start": "404060",
    "end": "406790"
  },
  {
    "text": "That would give us 2 over 7\nas the modified n-gram 1-gram",
    "start": "406790",
    "end": "410570"
  },
  {
    "text": "precision score\nfor this candidate.",
    "start": "410570",
    "end": "414070"
  },
  {
    "text": "There's also a\nbrevity penalty, which",
    "start": "414070",
    "end": "415870"
  },
  {
    "text": "will play the role of\nsomething like recall",
    "start": "415870",
    "end": "417970"
  },
  {
    "text": "in the BLEU scoring.",
    "start": "417970",
    "end": "419440"
  },
  {
    "text": "So we have a\nquantity r, which is",
    "start": "419440",
    "end": "420970"
  },
  {
    "text": "the sum of all the\nminimal absolute length",
    "start": "420970",
    "end": "423190"
  },
  {
    "text": "differences between\ncandidates and reference.",
    "start": "423190",
    "end": "425950"
  },
  {
    "text": "We have c, which is the total\nlength of all the candidates.",
    "start": "425950",
    "end": "429220"
  },
  {
    "text": "And then we said that\nthe brevity penalty is 1",
    "start": "429220",
    "end": "431410"
  },
  {
    "text": "if c is greater than r.",
    "start": "431410",
    "end": "432670"
  },
  {
    "text": "Otherwise, it's an\nexponential decay",
    "start": "432670",
    "end": "435100"
  },
  {
    "text": "off of the ratio of r and c.",
    "start": "435100",
    "end": "438160"
  },
  {
    "text": "And again, that will play\nkind of the notion of recall.",
    "start": "438160",
    "end": "440725"
  },
  {
    "text": "And then the BLEU\nscore is simply",
    "start": "440725",
    "end": "442100"
  },
  {
    "text": "the product of that\nbrevity penalty",
    "start": "442100",
    "end": "444980"
  },
  {
    "text": "with the sum of the weighted\nmodified n-gram precision",
    "start": "444980",
    "end": "448040"
  },
  {
    "text": "values for each n-gram\nvalue n considered.",
    "start": "448040",
    "end": "451530"
  },
  {
    "text": "So we probably go 1 through 4.",
    "start": "451530",
    "end": "453410"
  },
  {
    "text": "That's a standard set\nof n-grams to consider.",
    "start": "453410",
    "end": "456080"
  },
  {
    "text": "We would sum up all of\nthose notions of modified",
    "start": "456080",
    "end": "458720"
  },
  {
    "text": "n-gram precision for each n.",
    "start": "458720",
    "end": "460760"
  },
  {
    "text": "And possibly weight\nthem differently",
    "start": "460760",
    "end": "462470"
  },
  {
    "text": "depending on how we want\nto value 1-grams, 2-grams,",
    "start": "462470",
    "end": "465680"
  },
  {
    "text": "3-grams, and 4-grams.",
    "start": "465680",
    "end": "467985"
  },
  {
    "text": "So that's the BLEU scoring.",
    "start": "467985",
    "end": "469110"
  },
  {
    "start": "469000",
    "end": "571000"
  },
  {
    "text": "What are its properties?",
    "start": "469110",
    "end": "470409"
  },
  {
    "text": "Its bounds are 0 and 1.",
    "start": "470410",
    "end": "471690"
  },
  {
    "text": "And 1 is the best.",
    "start": "471690",
    "end": "472860"
  },
  {
    "text": "But we have really\nno expectation",
    "start": "472860",
    "end": "474509"
  },
  {
    "text": "that any system will\nactually achieve 1",
    "start": "474510",
    "end": "476670"
  },
  {
    "text": "because even comparisons\namong human translations",
    "start": "476670",
    "end": "479730"
  },
  {
    "text": "or human created text will\nnot have a BLEU score of 1.",
    "start": "479730",
    "end": "483980"
  },
  {
    "text": "The value encoded is\nan appropriate balance",
    "start": "483980",
    "end": "487160"
  },
  {
    "text": "of modified precision and\nrecall under the guise",
    "start": "487160",
    "end": "490670"
  },
  {
    "text": "of that brevity penalty.",
    "start": "490670",
    "end": "492770"
  },
  {
    "text": "It's very similar to the\nword-error rate in that sense,",
    "start": "492770",
    "end": "495560"
  },
  {
    "text": "but it seeks to accommodate the\nfact that there are typically",
    "start": "495560",
    "end": "498320"
  },
  {
    "text": "multiple suitable outputs\nfor a given input.",
    "start": "498320",
    "end": "500940"
  },
  {
    "text": "And that's a real\nstrength of BLEU score.",
    "start": "500940",
    "end": "504160"
  },
  {
    "text": "The weaknesses.",
    "start": "504160",
    "end": "505400"
  },
  {
    "text": "Well, this team has argued\nthat BLEU scores just",
    "start": "505400",
    "end": "507490"
  },
  {
    "text": "fail to correlate with human\nscores for translations.",
    "start": "507490",
    "end": "511580"
  },
  {
    "text": "And that's kind of worrying,\nbecause BLEU scores",
    "start": "511580",
    "end": "513579"
  },
  {
    "text": "were originally motivated in the\ncontext of machine translation.",
    "start": "513580",
    "end": "517539"
  },
  {
    "text": "And the issues\nthat they identify",
    "start": "517539",
    "end": "519250"
  },
  {
    "text": "are like it's very sensitive\nto n-gram order in a way",
    "start": "519250",
    "end": "522190"
  },
  {
    "text": "that human intuitions\nare not and it's",
    "start": "522190",
    "end": "524890"
  },
  {
    "text": "insensitive to the\ntype of the n-grams.",
    "start": "524890",
    "end": "527440"
  },
  {
    "text": "So again, just\nconsider comparisons",
    "start": "527440",
    "end": "529450"
  },
  {
    "text": "like that dog, the\ndog, and that toaster.",
    "start": "529450",
    "end": "532930"
  },
  {
    "text": "Those will likely have\nvery similar BLEU scores,",
    "start": "532930",
    "end": "536050"
  },
  {
    "text": "but that dog and the dog are\njust inherently much more",
    "start": "536050",
    "end": "538480"
  },
  {
    "text": "similar than that dog and that\ntoaster in virtue of the fact",
    "start": "538480",
    "end": "541779"
  },
  {
    "text": "that that and the,\nit's just a difference",
    "start": "541780",
    "end": "543700"
  },
  {
    "text": "at the level of\nfunctional vocabulary,",
    "start": "543700",
    "end": "545710"
  },
  {
    "text": "versus dog and toasters, a\nreally contentful change.",
    "start": "545710",
    "end": "549920"
  },
  {
    "text": "And then as we move into topics\nthat are more closely aligned",
    "start": "549920",
    "end": "553029"
  },
  {
    "text": "with NLU, could possibly have\nan even more worrying picture.",
    "start": "553030",
    "end": "556810"
  },
  {
    "text": "So this team argues that BLEU is\njust a fundamentally incorrect",
    "start": "556810",
    "end": "560950"
  },
  {
    "text": "measure for assessing\ndialogue systems.",
    "start": "560950",
    "end": "563530"
  },
  {
    "text": "And that could be an\nindicator that it's not",
    "start": "563530",
    "end": "565540"
  },
  {
    "text": "going to be appropriate for\nmany kinds of NLG tasks in NLU.",
    "start": "565540",
    "end": "571779"
  },
  {
    "start": "571000",
    "end": "634000"
  },
  {
    "text": "That's just a sample of\ntwo n-gram based metrics.",
    "start": "571780",
    "end": "574387"
  },
  {
    "text": "I thought I'd mention\na few more to give you",
    "start": "574387",
    "end": "576220"
  },
  {
    "text": "a framework for making\nsome comparisons.",
    "start": "576220",
    "end": "578300"
  },
  {
    "text": "So I mentioned the\nword-error rate.",
    "start": "578300",
    "end": "579940"
  },
  {
    "text": "That's fundamentally\nedit distance",
    "start": "579940",
    "end": "581650"
  },
  {
    "text": "from a single reference text.",
    "start": "581650",
    "end": "583870"
  },
  {
    "text": "BLEU, as we've seen,\nis modified precision",
    "start": "583870",
    "end": "586150"
  },
  {
    "text": "and a brevity penalty.",
    "start": "586150",
    "end": "587440"
  },
  {
    "text": "Kind of recall motion comparing\nagainst many reference texts.",
    "start": "587440",
    "end": "592000"
  },
  {
    "text": "ROUGE is a recall\nfocused variant of BLEU",
    "start": "592000",
    "end": "595390"
  },
  {
    "text": "that's focused on assessing\nsummarization systems.",
    "start": "595390",
    "end": "599380"
  },
  {
    "text": "METEOR is interestingly\ndifferent,",
    "start": "599380",
    "end": "601390"
  },
  {
    "text": "because it's trying to push\npast simple n-gram matching",
    "start": "601390",
    "end": "604150"
  },
  {
    "text": "and capture some\nsemantic notions.",
    "start": "604150",
    "end": "606250"
  },
  {
    "text": "It's a unit gram\nbased measure that",
    "start": "606250",
    "end": "609010"
  },
  {
    "text": "does an alignment measure\nbetween not only exact matches",
    "start": "609010",
    "end": "612070"
  },
  {
    "text": "of the unit grams, but also stem\nversions and synonyms really",
    "start": "612070",
    "end": "615640"
  },
  {
    "text": "trying to bring in\nsome semantic aspects.",
    "start": "615640",
    "end": "618850"
  },
  {
    "text": "And CIDEr is similar.",
    "start": "618850",
    "end": "619998"
  },
  {
    "text": "This is going to be even a more\nsemantic notion, because it's",
    "start": "619998",
    "end": "622540"
  },
  {
    "text": "going to do its comparisons\nin vector space.",
    "start": "622540",
    "end": "624970"
  },
  {
    "text": "It's kind of approximately\na weighted cosine",
    "start": "624970",
    "end": "627370"
  },
  {
    "text": "similarity between\nTF-IDF vectors created",
    "start": "627370",
    "end": "630753"
  },
  {
    "text": "from the corpus.",
    "start": "630753",
    "end": "631420"
  },
  {
    "start": "631420",
    "end": "634310"
  },
  {
    "start": "634000",
    "end": "695000"
  },
  {
    "text": "Finally, in closing,\nI just wanted",
    "start": "634310",
    "end": "635900"
  },
  {
    "text": "to exhort you all to think\nabout more communication based",
    "start": "635900",
    "end": "639230"
  },
  {
    "text": "metrics in the context of NLU.",
    "start": "639230",
    "end": "641510"
  },
  {
    "text": "For NLU, it's worth asking\nwhether you can evaluate",
    "start": "641510",
    "end": "644030"
  },
  {
    "text": "your system based on how\nwell it actually communicates",
    "start": "644030",
    "end": "646580"
  },
  {
    "text": "in the context of a real\nworld goal as opposed",
    "start": "646580",
    "end": "649190"
  },
  {
    "text": "to just comparing\ndifferent strings that",
    "start": "649190",
    "end": "651410"
  },
  {
    "text": "are inputs and reference texts.",
    "start": "651410",
    "end": "654019"
  },
  {
    "text": "And we've actually\nseen an example",
    "start": "654020",
    "end": "655490"
  },
  {
    "text": "of that in our assignment and\nbake off on color reference.",
    "start": "655490",
    "end": "658760"
  },
  {
    "text": "We didn't really assess\nhow well your system",
    "start": "658760",
    "end": "661100"
  },
  {
    "text": "could reproduce the utterances\nthat were in the corpus.",
    "start": "661100",
    "end": "664040"
  },
  {
    "text": "Rather, our fundamental\nnotion was listener accuracy,",
    "start": "664040",
    "end": "667500"
  },
  {
    "text": "which was keying into\na communication goal.",
    "start": "667500",
    "end": "669470"
  },
  {
    "text": "How well is your system actually\nable to take messages and use",
    "start": "669470",
    "end": "673610"
  },
  {
    "text": "them to figure out what the\nspeaker was referring to",
    "start": "673610",
    "end": "676339"
  },
  {
    "text": "in a simple color context?",
    "start": "676340",
    "end": "678290"
  },
  {
    "text": "And for much more on\nthat, and a perspective",
    "start": "678290",
    "end": "680168"
  },
  {
    "text": "on a lot of these\nissues, I encourage",
    "start": "680168",
    "end": "681710"
  },
  {
    "text": "you to check out this paper\nthat was led by Ben Newman.",
    "start": "681710",
    "end": "684500"
  },
  {
    "text": "It began as a course\nproject for this class",
    "start": "684500",
    "end": "687320"
  },
  {
    "text": "and grew into a really\nsuccessful paper.",
    "start": "687320",
    "end": "690700"
  },
  {
    "start": "690700",
    "end": "695000"
  }
]