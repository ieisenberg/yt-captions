[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "I'm a little bit sick today so I have a scratchy throat. So forgive me for that,",
    "start": "4700",
    "end": "10290"
  },
  {
    "text": "but I think we'll, we'll be able to manage without it. Uh, today, I'm gonna talk a little bit about writing up and presenting your work.",
    "start": "10290",
    "end": "18540"
  },
  {
    "text": "But before I jump into that, uh, I wanna jump over to our syllabus and orient us a little bit.",
    "start": "18540",
    "end": "25185"
  },
  {
    "text": "We're actually gonna cover three topics today. I'm gonna talk about writing up and presenting your work, but we're also gonna have a couple of mini-lectures.",
    "start": "25185",
    "end": "32625"
  },
  {
    "text": "Min and Jayadev are gonna talk about data augmentation, and Akhila is gonna talk about probing black-box models.",
    "start": "32625",
    "end": "40219"
  },
  {
    "text": "Uh, so we'll have those, uh, in the second half of, of today's session.",
    "start": "40220",
    "end": "45845"
  },
  {
    "start": "45000",
    "end": "192000"
  },
  {
    "text": "Um, also before we get into that, I wanna make an announcement. Um, Chris mentioned this on Piazza,",
    "start": "45845",
    "end": "53925"
  },
  {
    "text": "he posted this on Piazza. But just in case anyone missed it, I wanna make sure to draw attention to the- to this.",
    "start": "53925",
    "end": "60005"
  },
  {
    "text": "Um, we want you in your final paper, at the end of the paper,",
    "start": "60005",
    "end": "65110"
  },
  {
    "text": "to include a short section that describes, uh, the authorship of the paper,",
    "start": "65110",
    "end": "71659"
  },
  {
    "text": "so an authorship statement. And the idea is that this is a very concise statement that just says who contributed what to the final paper.",
    "start": "71660",
    "end": "80090"
  },
  {
    "text": "So this can be very brief. It could say, for example, uh, person A was mainly responsible for,",
    "start": "80090",
    "end": "88565"
  },
  {
    "text": "uh, gathering the dataset. Person B was mainly responsible for model optimization.",
    "start": "88565",
    "end": "94235"
  },
  {
    "text": "Person C was mainly responsible for writing the paper. Or it could say,",
    "start": "94235",
    "end": "100260"
  },
  {
    "text": "um, person A focused mainly on this approach. Person B focused on that approach, and they're both presented in the paper.",
    "start": "100260",
    "end": "107010"
  },
  {
    "text": "Or it could say all of the authors contributed equally to,",
    "start": "107010",
    "end": "112220"
  },
  {
    "text": "uh, collecting the data, conducting experiments and writing the paper. It's really up to you.",
    "start": "112220",
    "end": "117935"
  },
  {
    "text": "It should be a truthful reflection of the relative contributions of the respective authors.",
    "start": "117935",
    "end": "123350"
  },
  {
    "text": "And the motivation for this is that we think that this is just scientific good practice.",
    "start": "123350",
    "end": "128675"
  },
  {
    "text": "In fact, it's one that we'd like to see become more and more a part of the norms of the research community.",
    "start": "128675",
    "end": "134665"
  },
  {
    "text": "Uh, PNAS is already using this, and there's a link here to the guidance that's given to the- to,",
    "start": "134665",
    "end": "139930"
  },
  {
    "text": "to PNAS authors, that's the Proceedings of the National Academy of Science. And we're hopeful that this will become more and more",
    "start": "139930",
    "end": "146420"
  },
  {
    "text": "widespread in more and more, uh, venues. For example, in, in ACL which is the, uh,",
    "start": "146420",
    "end": "151745"
  },
  {
    "text": "dominant, uh, NLP conference. We do not intend or expect to use this information in assigning grades.",
    "start": "151745",
    "end": "162470"
  },
  {
    "text": "Um, our expectation is that the team members who are collaborating together will all receive the same grade.",
    "start": "162470",
    "end": "169955"
  },
  {
    "text": "We reserve the right to make distinctions and not to assign the same grade to,",
    "start": "169955",
    "end": "175310"
  },
  {
    "text": "to all members of a team, but we'll do that only in the most unusual cases, and we wouldn't do that without discussing it",
    "start": "175310",
    "end": "182555"
  },
  {
    "text": "first with the people involved in the project. Any questions about that? Yeah.",
    "start": "182555",
    "end": "189780"
  },
  {
    "text": "[inaudible]. Yeah. [LAUGHTER] For a solo project,",
    "start": "189780",
    "end": "195110"
  },
  {
    "text": "I think it can just be one line that says, \"I'm the sole author and I am responsible for everything.\"",
    "start": "195110",
    "end": "202129"
  },
  {
    "text": "I think for a solo project it's, it's a moot point. Um, I suppose it's to- just to meet the formal requirements,",
    "start": "202129",
    "end": "209450"
  },
  {
    "text": "a single sentence is, is still appropriate, but it doesn't need to say very much.",
    "start": "209450",
    "end": "214860"
  },
  {
    "text": "Any other questions about the final project before we go on? Note that, uh, today's class section- session is the last regular class session.",
    "start": "216370",
    "end": "228665"
  },
  {
    "text": "Next week, we encourage you to schedule time to meet with your mentors for the project,",
    "start": "228665",
    "end": "234129"
  },
  {
    "text": "to talk about their progress on the project. But this, this class would be the last regularly scheduled session. Yes.",
    "start": "234130",
    "end": "241635"
  },
  {
    "text": "You mentioned you think it was a good practice and you want it to spread. Um, I was kind of confused by this practice on a couple of NLP papers,",
    "start": "241635",
    "end": "249470"
  },
  {
    "text": "and I was wondering why you think it's a good practice. Um, my philosophy, and",
    "start": "249470",
    "end": "256285"
  },
  {
    "text": "this would be an interesting question to ask Chris, Chris Potts as well, but my philosophy on this is that it's about responsibility and accountability.",
    "start": "256285",
    "end": "264345"
  },
  {
    "text": "That it's about, um, making it clear who's accountable for which parts of the paper.",
    "start": "264345",
    "end": "271060"
  },
  {
    "text": "Um, I think it's unlikely in this class, but there are exceptional situations where,",
    "start": "271060",
    "end": "276260"
  },
  {
    "text": "um, accountability winds up really mattering. Uh, for example, if there are problems with the paper, if there- if, um,",
    "start": "276260",
    "end": "283910"
  },
  {
    "text": "concern arises that, like, some results were falsified or something like that,",
    "start": "283910",
    "end": "290800"
  },
  {
    "text": "um, you wanna know who's responsible for that. And so I think, uh, the authorship statement amounts to the authors making it clear,",
    "start": "290800",
    "end": "298490"
  },
  {
    "text": "I am responsible for this part of the work and I stand behind this part of the work. [NOISE] Okay.",
    "start": "298490",
    "end": "307530"
  },
  {
    "start": "305000",
    "end": "575000"
  },
  {
    "text": "So let me switch into the topic of writing up and presenting your work.",
    "start": "307530",
    "end": "313440"
  },
  {
    "text": "And I think first, I wanna say a little bit about why we're talking about this.",
    "start": "313820",
    "end": "319175"
  },
  {
    "text": "Um, to me this topic- this is a little bit unusual because this is not a technical topic.",
    "start": "319175",
    "end": "324815"
  },
  {
    "text": "This is about how to communicate the results of your work. And, um, it's about finding a way",
    "start": "324815",
    "end": "333950"
  },
  {
    "text": "to communicate the results of your work clearly and convincingly,",
    "start": "333950",
    "end": "339060"
  },
  {
    "text": "and in a way that's gonna have impact. Yes. [NOISE] Thank you.",
    "start": "339060",
    "end": "345569"
  },
  {
    "text": "I don't know why it does that. Maybe if I just do this, it won't do that.",
    "start": "345570",
    "end": "356740"
  },
  {
    "text": "Um, communicating your work both in written form and in spoken form,",
    "start": "358700",
    "end": "364430"
  },
  {
    "text": "so in an oral presentation. I think this, um, I think this topic is unfortunately one that sometimes,",
    "start": "364430",
    "end": "372050"
  },
  {
    "text": "um [NOISE] is undervalued by scientists and engineers.",
    "start": "372050",
    "end": "377884"
  },
  {
    "text": "I think there's, um, sometimes, uh, an unfortunate tendency to think that great work will speak for itself,",
    "start": "377885",
    "end": "386315"
  },
  {
    "text": "and that it doesn't matter that much how you present it. And I have to forcefully disagree",
    "start": "386315",
    "end": "393725"
  },
  {
    "text": "with the idea that great work will always speak for itself. Um, actually something happened this week that kind of,",
    "start": "393725",
    "end": "400664"
  },
  {
    "text": "um, sharpened this for me. Um, Murray Gell-Mann died on Friday. Murray Gell-Mann is a great physicist.",
    "start": "400665",
    "end": "406880"
  },
  {
    "text": "You've probably heard his name. Uh, he's responsible for inventing the quark, and introducing the theory that hadrons are made up out of quarks, in 1964.",
    "start": "406880",
    "end": "417020"
  },
  {
    "text": "Um, when I was reading bi- the, the biography or the, uh, what's the word?",
    "start": "417020",
    "end": "422395"
  },
  {
    "text": "Obituary, thank you, of Murray Gell-Mann, uh, one of the- the obituary was- I was reading pointed out that there's another guy,",
    "start": "422395",
    "end": "431240"
  },
  {
    "text": "George Zweig, who the same year independently arrived at the theory of quarks.",
    "start": "431240",
    "end": "437270"
  },
  {
    "text": "I had never heard of George Zweig before. I don't know if- do you guys know this name? I had never heard of his name.",
    "start": "437270",
    "end": "444020"
  },
  {
    "text": "I certainly had heard of Murray Gell-Mann. You're a physicist. [inaudible] known.",
    "start": "444020",
    "end": "457350"
  },
  {
    "text": "I don't know the reason why, though. Well, maybe part of the reason, and this is something that I learned from reading those obituaries.",
    "start": "457350",
    "end": "463925"
  },
  {
    "text": "Zweig named his concept aces, and Gellman named his on- the concept that he came up with quarks.",
    "start": "463925",
    "end": "472100"
  },
  {
    "text": "And quarks somehow was a much more memorable name. It was a sticky name.",
    "start": "472100",
    "end": "477995"
  },
  {
    "text": "It was good marketing. It was good marketing for his ideas.",
    "start": "477995",
    "end": "483349"
  },
  {
    "text": "And so everybody knows who Gellman is. A lot of people don't know who Zweig is.",
    "start": "483350",
    "end": "489095"
  },
  {
    "text": "I didn't know who he was. There may be lots of other reasons, right? They had- these guys had long careers.",
    "start": "489095",
    "end": "494180"
  },
  {
    "text": "They did lots of other stuff. Gellman is responsible for tons of other advances in physics. So I'm not saying this is the only reason why Gellman is well known,",
    "start": "494180",
    "end": "502525"
  },
  {
    "text": "but it doesn't hurt that he found a way to market his ideas in a very effective way.",
    "start": "502525",
    "end": "508070"
  },
  {
    "text": "I think a lot of times as, um, scientists and engineers, we tend to think of marketing as a bit icky,",
    "start": "508070",
    "end": "515195"
  },
  {
    "text": "like respectable people, you know, don't soil the- soil their hands with, with marketing and thinking about how to present their ideas,",
    "start": "515195",
    "end": "522260"
  },
  {
    "text": "but it really does make a difference. And it would be a terrible shame if you invested a huge amount of work in collecting data,",
    "start": "522260",
    "end": "529930"
  },
  {
    "text": "in staying up late at night running experiments, and being very innovative in your ideas and your thinking,",
    "start": "529930",
    "end": "536420"
  },
  {
    "text": "[NOISE] but then didn't follow through on that by finding a way to communicate your ideas in a way that will have a,",
    "start": "536420",
    "end": "545200"
  },
  {
    "text": "a lasting impact on the community. And it will let others benefit from your learning and your hard work.",
    "start": "545200",
    "end": "551980"
  },
  {
    "text": "So that's really what I want to focus on today. And our hope is that, by spending a little bit of time talking about this topic,",
    "start": "551980",
    "end": "558470"
  },
  {
    "text": "that this is one of the unique benefits of this class. That this class can be a very practical introduction,",
    "start": "558470",
    "end": "564680"
  },
  {
    "text": "uh, not only to natural language understanding, but in how to be an effective researcher. And part of being an effective researcher is",
    "start": "564680",
    "end": "571430"
  },
  {
    "text": "communicating your work in a way that's going to have impact. We also want to inspire you to think bigger than just this class.",
    "start": "571430",
    "end": "580070"
  },
  {
    "start": "575000",
    "end": "702000"
  },
  {
    "text": "Of course, your immediate goal is to finish your final project, make a video presentation, which I'll say a little bit more about at the end,",
    "start": "580070",
    "end": "586535"
  },
  {
    "text": "and write a final paper. Um, but you can aim higher than that as well.",
    "start": "586535",
    "end": "593220"
  },
  {
    "text": "Um, the work that you're doing right now could lead to insights and ideas that wind up having lasting influence.",
    "start": "593220",
    "end": "601220"
  },
  {
    "text": "I think Chris mentioned that there was a student in, I forget whether it was last year or two years ago, uh,",
    "start": "601220",
    "end": "608790"
  },
  {
    "text": "had this insight about the hypothesis only baseline for natural language inference,",
    "start": "608790",
    "end": "614810"
  },
  {
    "text": "specifically for the NL- SNLI dataset. And that was a pretty, um, impactful insight that showed the community something really meaningful about this task,",
    "start": "614810",
    "end": "625340"
  },
  {
    "text": "or at least about that dataset that so many people had been optimizing on,",
    "start": "625340",
    "end": "630680"
  },
  {
    "text": "on, on the, the SNLI leaderboard. Um, so it really is possible for, uh,",
    "start": "630680",
    "end": "637350"
  },
  {
    "text": "class projects to wind up having, uh, a larger impact on the research community.",
    "start": "637350",
    "end": "642560"
  },
  {
    "text": "And there have been many papers from this class over the last several years that have been turned into conference papers,",
    "start": "642560",
    "end": "648595"
  },
  {
    "text": "and thereby had- a had a broader reach. Another reason to think about the topics that we're looking at today is that,",
    "start": "648595",
    "end": "655210"
  },
  {
    "text": "um, oftentimes the next step in your career will involve presenting your research to others.",
    "start": "655210",
    "end": "662764"
  },
  {
    "text": "This is certainly true if you want to go on to an academic career, but it's even true for many positions in industry.",
    "start": "662765",
    "end": "668900"
  },
  {
    "text": "Certainly, uh, any research oriented position in industry, very commonly, as part of the interview process,",
    "start": "668900",
    "end": "675750"
  },
  {
    "text": "they'll invite you to give a research talk, uh, where you go and- at Google or at Apple or even at a startup,",
    "start": "675750",
    "end": "685040"
  },
  {
    "text": "uh, talk about some research project that you've, you've been involved in. And so finding a way to communicate that effectively not only helps to, um,",
    "start": "685040",
    "end": "693680"
  },
  {
    "text": "disseminate your research but also helps to, um, build your credibility and open doors for you.",
    "start": "693680",
    "end": "701070"
  },
  {
    "start": "702000",
    "end": "740000"
  },
  {
    "text": "So today, I'm gonna talk about four different topics. We'll talk about how to write a research paper,",
    "start": "702300",
    "end": "709420"
  },
  {
    "text": "specifically an NLP research paper. Uh, I'll talk a little bit about the conference submission process.",
    "start": "709420",
    "end": "716935"
  },
  {
    "text": "Just so that you have sort of, uh, an overview of how papers wind up getting accepted to conferences.",
    "start": "716935",
    "end": "723714"
  },
  {
    "text": "I'll give you some advice on oral presentations, and then I'll give you some very tactical tips on",
    "start": "723715",
    "end": "729399"
  },
  {
    "text": "the video presentations that we're looking for, for this class. Here is a New Yorker cartoon about writing papers,",
    "start": "729400",
    "end": "736914"
  },
  {
    "text": "sometimes it feels like this. Okay. So here's an outline of a typical NLP paper.",
    "start": "736914",
    "end": "745430"
  },
  {
    "start": "740000",
    "end": "896000"
  },
  {
    "text": "Um, there's, uh, the,",
    "start": "745800",
    "end": "750955"
  },
  {
    "text": "the field of NLP has evolved some conventions about how papers are typically structured.",
    "start": "750955",
    "end": "758200"
  },
  {
    "text": "And this kinda give you, it gives you a sense of what those conventions are.",
    "start": "758200",
    "end": "763285"
  },
  {
    "text": "Um, and so when you see papers that are presented at ACL, or EMNLP, or NAACL, or EACL,",
    "start": "763285",
    "end": "770785"
  },
  {
    "text": "they tend to follow these conventions. I'll say more in a- in a few minutes about what the conventions are,",
    "start": "770785",
    "end": "777850"
  },
  {
    "text": "but first I wanna talk about why these conventions. And there's really two reasons why it's a good idea to follow these conventions.",
    "start": "777850",
    "end": "787040"
  },
  {
    "text": "One is that the conventions have evolved for a reason. The community has- the research community has converged on these conventions",
    "start": "787320",
    "end": "796300"
  },
  {
    "text": "because they're effective- because they are- an effective way to communicate your ideas,",
    "start": "796300",
    "end": "801324"
  },
  {
    "text": "and I'll explain more why in a moment. The other reason to follow conventions is that,",
    "start": "801324",
    "end": "808375"
  },
  {
    "text": "because they're conventions, they make it easy for your audience to understand what they're reading.",
    "start": "808375",
    "end": "815305"
  },
  {
    "text": "They make it easy to grok. Typically, the content of your papers will be technically dense.",
    "start": "815305",
    "end": "823165"
  },
  {
    "text": "Your audience will already have a challenge understanding the technical content.",
    "start": "823165",
    "end": "828445"
  },
  {
    "text": "You don't wanna double that challenge by making them also struggle to understand the rhetorical structure that you're trying to follow.",
    "start": "828445",
    "end": "836110"
  },
  {
    "text": "If you follow established conventions, that won't be an issue. They'll know what to expect, why they're reading",
    "start": "836110",
    "end": "841779"
  },
  {
    "text": "what they're reading, and it will make it easier for them to understand your paper. So the, uh, the structure typically looks like this.",
    "start": "841780",
    "end": "851380"
  },
  {
    "text": "I'm not gonna dwell here because the next slide is somewhat redundant with this. But I'll just say, um,",
    "start": "851380",
    "end": "857920"
  },
  {
    "text": "this is kinda of a conventional ordering of sections of an NLP research paper.",
    "start": "857920",
    "end": "864579"
  },
  {
    "text": "You- you do have the, the freedom to play with this, to violate conventions,",
    "start": "864580",
    "end": "870550"
  },
  {
    "text": "but if you're gonna violate the conventions, you should do it very consciously and you should do it for a good reason,",
    "start": "870550",
    "end": "875995"
  },
  {
    "text": "not just sort of haphazardly. And the reason, this is shown with eight pages and that's because typically,",
    "start": "875995",
    "end": "883285"
  },
  {
    "text": "NLP conference papers are eight pages with two columns, single-spaced. Because part of the convention is a convention for laying out and formatting,",
    "start": "883285",
    "end": "893244"
  },
  {
    "text": "which you will follow in your final papers for this class. So this is somewhat redundant with the previous slide.",
    "start": "893244",
    "end": "899830"
  },
  {
    "start": "896000",
    "end": "1026000"
  },
  {
    "text": "It's just sort of a different way of presenting it. What should go into a paper?",
    "start": "899830",
    "end": "905995"
  },
  {
    "text": "Uh, this is actually a question of formatting- Yeah. Why the double columns? There's, do different journals have different views of this? Like,",
    "start": "905995",
    "end": "913120"
  },
  {
    "text": "NIPS is very for single, like, column. Uh, but it sounds like, NLP's against that.",
    "start": "913120",
    "end": "919750"
  },
  {
    "text": "I don't have a great answer for you on this. Um, that one might be somewhat arbitrary convention.",
    "start": "919750",
    "end": "927640"
  },
  {
    "text": "I think some of the conventions are very well-motivated. There's good reasons for them. Maybe some of them are arbitrary,",
    "start": "927640",
    "end": "933430"
  },
  {
    "text": "and that one might be an arbitrary one. But here's what's a lot less arbitrary.",
    "start": "933430",
    "end": "940060"
  },
  {
    "text": "I think a great paper should have a narrative arc, and the arc needs to include these elements.",
    "start": "940060",
    "end": "947545"
  },
  {
    "text": "What is the problem that you are trying to solve? Why is this problem important?",
    "start": "947545",
    "end": "954325"
  },
  {
    "text": "Why should we care about it? What have others done in this problem?",
    "start": "954325",
    "end": "961600"
  },
  {
    "text": "What approaches have others explored, and why are those approaches not fully satisfactory?",
    "start": "961600",
    "end": "967404"
  },
  {
    "text": "Why is there still room to do something more? That's- those three things are kind of the introduction to your paper.",
    "start": "967405",
    "end": "975640"
  },
  {
    "text": "That's kinda the preamble, it launches you into your story. Then you need to talk about, what's new?",
    "start": "975640",
    "end": "983995"
  },
  {
    "text": "What are you doing? And this is really the meat of your story. What is the, uh,",
    "start": "983995",
    "end": "989500"
  },
  {
    "text": "approach that you're gonna follow? What is the model that you're gonna develop? What distinguishes that from other things that have been done before?",
    "start": "989500",
    "end": "997330"
  },
  {
    "text": "And this is probably the longest part and the most technically dense part of your paper.",
    "start": "997330",
    "end": "1002700"
  },
  {
    "text": "Uh, and sometimes it will include a description of a dataset if, if your- if the dataset that you're working with is not completely standard,",
    "start": "1002700",
    "end": "1010950"
  },
  {
    "text": "something that lots of other people have worked on. If it's something new, if you collected data you need to explain how you did that.",
    "start": "1010950",
    "end": "1018190"
  },
  {
    "text": "Then you need to talk about how you're gonna evaluate the success of what you did.",
    "start": "1018230",
    "end": "1025799"
  },
  {
    "text": "Then you need to talk about the evaluation itself, the results of the evaluation.",
    "start": "1025800",
    "end": "1032890"
  },
  {
    "text": "Then some discussion of the significance of those results.",
    "start": "1033170",
    "end": "1039765"
  },
  {
    "text": "Um, so what does it mean? What can we learn? Um, what are the strengths?",
    "start": "1039765",
    "end": "1046290"
  },
  {
    "text": "What are the limitations of the approach that you took? Uh, and then perhaps,",
    "start": "1046290",
    "end": "1051885"
  },
  {
    "text": "finally, some pointers to future work. So, uh, almost assuredly the work that you've done is not complete,",
    "start": "1051885",
    "end": "1062745"
  },
  {
    "text": "it's not the last word. There's probably still limitations in whatever you did. And if you can highlight what those limitations are,",
    "start": "1062745",
    "end": "1070200"
  },
  {
    "text": "and point to opportunities for future work, you're doing the community a service by helping them",
    "start": "1070200",
    "end": "1076305"
  },
  {
    "text": "identify what the next steps are for, uh, further progress in this field.",
    "start": "1076305",
    "end": "1082680"
  },
  {
    "text": "So those are kinda of the key elements of the story that you wanna tell. I did that off the top of my head,",
    "start": "1082680",
    "end": "1088860"
  },
  {
    "text": "but hopefully it aligns really well. I think it does align really well with what's here,",
    "start": "1088860",
    "end": "1093960"
  },
  {
    "text": "because it's all- there's- it's all- there is a reason for all of these pieces.",
    "start": "1093960",
    "end": "1100649"
  },
  {
    "text": "You kinda need all of these pieces in order to have a complete story. What are you working on? Why does it matter?",
    "start": "1100650",
    "end": "1107475"
  },
  {
    "text": "What have others done? What are you doing? How are you going to evaluate it? What was the result of that- that evaluation?",
    "start": "1107475",
    "end": "1113804"
  },
  {
    "text": "And what does it all mean? That's a complete story of your work.",
    "start": "1113805",
    "end": "1120159"
  },
  {
    "text": "Um, later, I'm gonna talk about how to write a good abstract and also how to give a good talk.",
    "start": "1121880",
    "end": "1129270"
  },
  {
    "text": "And I'm gonna kinda repeat what I just said, because no matter what format you're telling your story in,",
    "start": "1129270",
    "end": "1135135"
  },
  {
    "text": "it might be written or spoken. It might be eight pages long or it might be a single paragraph,",
    "start": "1135135",
    "end": "1140684"
  },
  {
    "text": "but you kinda wanna have those key elements. Uh, Stuart Shieber is- yes.",
    "start": "1140685",
    "end": "1148679"
  },
  {
    "start": "1144000",
    "end": "1231000"
  },
  {
    "text": "Right. You know, in the metric section where you describe the model, I've seen both approaches.",
    "start": "1148680",
    "end": "1155460"
  },
  {
    "text": "One is like is if you're more or less various with [NOISE] components across to describe the details of each component and then how they all fit together,",
    "start": "1155460",
    "end": "1162375"
  },
  {
    "text": "and there are other approach being you first describe, you work on, like, the overview of the model and then going to depths.",
    "start": "1162375",
    "end": "1168360"
  },
  {
    "text": "So I mean, I- what- what is it that dictates how you would write that particular part down?",
    "start": "1168360",
    "end": "1174120"
  },
  {
    "text": "Like in some papers, you first describe, how you did feature engineering and then in the model was the classifiers that you used, or you could do just the other way around.",
    "start": "1174120",
    "end": "1181170"
  },
  {
    "text": "[NOISE]. [NOISE] I think there's room for variation here, um, and room for personal preference.",
    "start": "1181170",
    "end": "1188429"
  },
  {
    "text": "I think my inclination would be to, uh, sort of do a two-pass approach;",
    "start": "1188430",
    "end": "1194625"
  },
  {
    "text": "first a top-down and then a bottom-up. So first, start with a high level and say,",
    "start": "1194625",
    "end": "1200445"
  },
  {
    "text": "\"Here's the overall picture of the system,\" um, then dive into individual pieces and describe the individual pieces,",
    "start": "1200445",
    "end": "1207164"
  },
  {
    "text": "but then come back up to the top level and say, \"How all those pieces come together.\" [NOISE] But I think, um,",
    "start": "1207165",
    "end": "1215610"
  },
  {
    "text": "[NOISE] there- the- there's less of, uh, community convention or prescriptive answer on how to answer that question.",
    "start": "1215610",
    "end": "1225300"
  },
  {
    "text": "I think there's a lit- a little bit more room for variation. [NOISE] So Stuart Shieber is, uh,",
    "start": "1225300",
    "end": "1234360"
  },
  {
    "start": "1231000",
    "end": "1452000"
  },
  {
    "text": "an NLP professor and researcher at Harvard, and he has this, um,",
    "start": "1234360",
    "end": "1239820"
  },
  {
    "text": "document which we have a link to here, which is about writing great papers and he talks about the rational reconstruction format.",
    "start": "1239820",
    "end": "1247080"
  },
  {
    "text": "He describes- there's actually kind of, uh, uh, Hegelian, um, [NOISE] what's the word?",
    "start": "1247080",
    "end": "1253530"
  },
  {
    "text": "Di- dialectic. Thank you. [NOISE] There's a Hegelian dialectic here, uh, with a thesis and antithesis and synthesis, okay?",
    "start": "1253530",
    "end": "1261420"
  },
  {
    "text": "So we start with the continental style. He describes the continental style as one that's extremely concise,",
    "start": "1261420",
    "end": "1268815"
  },
  {
    "text": "and opaque, and just kind of lays the answer out but doesn't really tell you how it got there.",
    "start": "1268815",
    "end": "1276255"
  },
  {
    "text": "When I read this, I- I'm reminded of, um, John Nash. John Nash's PhD thesis was 26 pages long.",
    "start": "1276255",
    "end": "1284850"
  },
  {
    "text": "My PhD thesis was 120 pages long, that's because I'm not as smart as John Nash.",
    "start": "1284850",
    "end": "1290580"
  },
  {
    "text": "Um, by the way, John Nash had two citations in his PhD thesis.",
    "start": "1290580",
    "end": "1295685"
  },
  {
    "text": "If you're John Nash, you don't cite other people, other people cite you. [LAUGHTER]. Uh, my PhD thesis had,",
    "start": "1295685",
    "end": "1302640"
  },
  {
    "text": "like, 150 citations because I'm not John Nash. Um, if you're John Nash,",
    "start": "1302640",
    "end": "1308160"
  },
  {
    "text": "you can get away with this. If you're just about anybody else, it's not recommended.",
    "start": "1308160",
    "end": "1314520"
  },
  {
    "text": "This is not the best way to communicate, uh, your ideas and how you got to them and make them understandable to others.",
    "start": "1314520",
    "end": "1323100"
  },
  {
    "text": "It's too cryptic, it's too dense, um, and doesn't provide a way in. [NOISE] Uh, the antithesis, the historical style.",
    "start": "1323100",
    "end": "1331755"
  },
  {
    "text": "So this is where- this is kind of a narrative of your research process. You describe the whole research process in laborious detail,",
    "start": "1331755",
    "end": "1340890"
  },
  {
    "text": "and it's kinda like you start out, it was a dark and stormy night when I began to investigate the problem of natural language inference,",
    "start": "1340890",
    "end": "1348493"
  },
  {
    "text": "um, and you go on from there, right? And, uh, describe all the things that didn't work,",
    "start": "1348494",
    "end": "1354750"
  },
  {
    "text": "getting all the places where you got stuck. Um, this has the benefit of being more",
    "start": "1354750",
    "end": "1360495"
  },
  {
    "text": "understandable [NOISE] than the- the continental style, but it's gonna be long-winded and include",
    "start": "1360495",
    "end": "1367230"
  },
  {
    "text": "lots of information that doesn't actually help anybody, um, and it's also not really the recommended approach.",
    "start": "1367230",
    "end": "1373740"
  },
  {
    "text": "And really the ideal approach is what Stuart Shieber calls the rational reconstruction approach.",
    "start": "1373740",
    "end": "1379470"
  },
  {
    "text": "So in the rational reconstruction approach, you, um, [NOISE] uh,",
    "start": "1379470",
    "end": "1384790"
  },
  {
    "text": "lay out the problem in a way that makes the- the approach that you arrive at seem almost inevitable.",
    "start": "1385520",
    "end": "1395595"
  },
  {
    "text": "Makes it seem like lo- a logical and natural, uh, consequence of the constraints of the problem.",
    "start": "1395595",
    "end": "1403335"
  },
  {
    "text": "And makes the- the- the whole approach and, um, what you did and why you did it seemed very straightforward.",
    "start": "1403335",
    "end": "1411480"
  },
  {
    "text": "As- as Stuart Shieber says, um, \"Convince the reader that your solution is trivial.\" [NOISE] Um, unfortunately, the conventions of the community, these- these conventions,",
    "start": "1411480",
    "end": "1423059"
  },
  {
    "text": "this kind of conventional narrative arc that I described, part of the reason for these conventions is that",
    "start": "1423060",
    "end": "1430050"
  },
  {
    "text": "they naturally bring you toward this rational reconstruction style. This is not a narrative,",
    "start": "1430050",
    "end": "1436003"
  },
  {
    "text": "and it's also not an opaque John Nash style PhD thesis, it's an explanation of, uh,",
    "start": "1436004",
    "end": "1443640"
  },
  {
    "text": "a way of approaching a problem that makes sense, and it's driven by the motivations and the constraints of the problem.",
    "start": "1443640",
    "end": "1450690"
  },
  {
    "text": "Uh, some hints on mathematics. Oftentimes you'll have formulas or other mathematical notation in your paper,",
    "start": "1450690",
    "end": "1460260"
  },
  {
    "start": "1452000",
    "end": "1923000"
  },
  {
    "text": "Dave Goss has some good hints on how to do that effectively. [NOISE] Okay. Let me spend a couple of minutes talking about, um,",
    "start": "1460260",
    "end": "1466515"
  },
  {
    "text": "conferences, um, but first, an XKCD cartoon. This is one of my favorite.",
    "start": "1466515",
    "end": "1471660"
  },
  {
    "text": "This is an old XKCD cartoon, this one is number 541. If you're a regular XKCD reader,",
    "start": "1471660",
    "end": "1478020"
  },
  {
    "text": "you know that these are numbered sequentially and I think we're up to like 1,700 now or something like that. So this is an old one,",
    "start": "1478020",
    "end": "1484320"
  },
  {
    "text": "but this is one of my favorites because it's about how to put smileys inside parentheses,",
    "start": "1484320",
    "end": "1490335"
  },
  {
    "text": "and I struggle with this. Don't you struggle with this? We all struggle with this. And I think he's the first guy who,",
    "start": "1490335",
    "end": "1496304"
  },
  {
    "text": "you know, points out that we all struggle with this. I also like this because he starts out by saying, \"Hi, I'm Randall. Welcome to my TED talk.\"",
    "start": "1496305",
    "end": "1502305"
  },
  {
    "text": "And welcome to my TED talk is kind of a meme now. But as far as I know, he's the first person who like used it in this way to be funny.",
    "start": "1502305",
    "end": "1510195"
  },
  {
    "text": "So I love this, um, cartoon. [NOISE] Okay. So I wanna talk a little bit about, um,",
    "start": "1510195",
    "end": "1516945"
  },
  {
    "text": "what it's like to submit a paper to an NLP conference and what the review process is like and how",
    "start": "1516945",
    "end": "1523140"
  },
  {
    "text": "your paper actually gets accepted because this is the information that you'll need to know if you actually submit a paper to an NLP conference.",
    "start": "1523140",
    "end": "1529860"
  },
  {
    "text": "Um, I'm- by the way, I'm sure there are other people in the room, uh, from the teaching team and maybe others as well who have been through",
    "start": "1529860",
    "end": "1536700"
  },
  {
    "text": "this process and I hope you'll speak up and help fill in the gap, whatever gaps I leave or- or share your own insights on this process.",
    "start": "1536700",
    "end": "1543810"
  },
  {
    "text": "Um, [NOISE] so here's kind of an overview of the typical process for getting a paper accepted.",
    "start": "1543810",
    "end": "1550590"
  },
  {
    "text": "First, you write the paper. You guys are doing that now, um, so that's great. Then you submit the paper through, uh,",
    "start": "1550590",
    "end": "1557655"
  },
  {
    "text": "there's a- usually a web interface where you upload your PDF, and as part of the submission process,",
    "start": "1557655",
    "end": "1563820"
  },
  {
    "text": "they give you a bunch of keywords that you can choose from. And the keywords will be things like machine translation",
    "start": "1563820",
    "end": "1569100"
  },
  {
    "text": "or computational semantics or natural language inference. They sort of correspond to subfields of NLP.",
    "start": "1569100",
    "end": "1576195"
  },
  {
    "text": "They may correspond to specific techniques like reinforcement learning, um, [NOISE] they may, um,",
    "start": "1576195",
    "end": "1584730"
  },
  {
    "text": "be things like, um, you know, hu- human annotation or something like that, um,",
    "start": "1584730",
    "end": "1590625"
  },
  {
    "text": "techniques, strategies, uh, topics, subtopics of NLP that are particularly salient for your paper.",
    "start": "1590625",
    "end": "1598090"
  },
  {
    "text": "So hundreds of people, uh, upload lots and lots of titles. The reviewers, actually the organizers,",
    "start": "1599630",
    "end": "1607200"
  },
  {
    "text": "the conference chairs, um. Oh, yeah. Um, [NOISE] a conference typically has several different areas, um,",
    "start": "1607200",
    "end": "1620429"
  },
  {
    "text": "the areas each will have area chairs and the chairs are responsible for recruiting reviewers to help review papers in that area.",
    "start": "1620430",
    "end": "1629940"
  },
  {
    "text": "And they'll organize the papers into areas in large part based on the keywords, and then they'll send, um,",
    "start": "1629940",
    "end": "1636615"
  },
  {
    "text": "they'll put on a webpage lists of the submitted papers and invite all the reviewers to come look at the list of papers,",
    "start": "1636615",
    "end": "1644970"
  },
  {
    "text": "[NOISE] and submit bids on which ones they want to be responsible for reviewing.",
    "start": "1644970",
    "end": "1651960"
  },
  {
    "text": "So as a reviewer, you'll see this webpage with 100 papers on it. You'll see the titles.",
    "start": "1651960",
    "end": "1657929"
  },
  {
    "text": "You'll be able to click through to get the abstract of the paper. What you won't see is author names or affiliations or anything like that.",
    "start": "1657930",
    "end": "1667260"
  },
  {
    "text": "Um, you will see the keywords that are on the paper. And based on that, you need to choose which papers you wanna review.",
    "start": "1667260",
    "end": "1675059"
  },
  {
    "text": "And so naturally your, your decision on which papers to bid on will be largely",
    "start": "1675060",
    "end": "1680460"
  },
  {
    "text": "influenced by [NOISE] the titles that you're seeing. Which is, uh, a good reminder that choosing a good title for your paper really helps.",
    "start": "1680460",
    "end": "1692625"
  },
  {
    "text": "Um, good titles help to attract attention, um, and get people interested in your paper.",
    "start": "1692625",
    "end": "1698910"
  },
  {
    "text": "They're like an advertisement for your paper. The ideal title is one that",
    "start": "1698910",
    "end": "1705075"
  },
  {
    "text": "gives a very clear indication of what's going to be in the paper, but in a way that makes it interesting and enticing.",
    "start": "1705075",
    "end": "1714645"
  },
  {
    "text": "Um, here's a bad title that is from my- this is a title that I used from my own experience.",
    "start": "1714645",
    "end": "1720674"
  },
  {
    "text": "It was actually not a title of a paper, but it was a title of a talk that I gave one time. And the title was two aspects of the problem of natural language inference.",
    "start": "1720675",
    "end": "1730184"
  },
  {
    "text": "It's a terrible title. Ah, it tells you that it's about natural language inference, okay,",
    "start": "1730185",
    "end": "1735375"
  },
  {
    "text": "but two aspects of the problem- okay but which two aspects and what are they- why should I care, we have no idea?",
    "start": "1735375",
    "end": "1744270"
  },
  {
    "text": "There's nothing, there's nothing interesting. I mean actually now that I think about it it's kind of like those, um,",
    "start": "1744270",
    "end": "1749820"
  },
  {
    "text": "listicles on the web that are like seven things you should do to enhance your whatever.",
    "start": "1749820",
    "end": "1756240"
  },
  {
    "text": "Uh, not a great model to follow though. Um, a much better example of a title- one of my favorite titles of a recent,",
    "start": "1756240",
    "end": "1765030"
  },
  {
    "text": "uh, it's not NLP, but a machine-learning paper at NIPS in 2016. There was a -ah,",
    "start": "1765030",
    "end": "1770900"
  },
  {
    "text": "paper that was titled. Um, Learning to Learn by Gradient Descent, by Gradient Descent.",
    "start": "1770900",
    "end": "1777640"
  },
  {
    "text": "Maybe somebody- this was from DeepMind. Maybe some of you read this paper, this paper. And what I love about this title is that it's really descriptive.",
    "start": "1777640",
    "end": "1785549"
  },
  {
    "text": "You know what's going to be in that paper. It's going to be about learning to optimize the gradient descent process itself.",
    "start": "1785550",
    "end": "1793380"
  },
  {
    "text": "So you know what the- what the paper is about and you know what you're gonna learn from this paper, and you know whether you're going to be interested in this paper.",
    "start": "1793380",
    "end": "1800475"
  },
  {
    "text": "But then the title is also a little bit whimsical because it has this recursive structure and nerds love recursion and,",
    "start": "1800475",
    "end": "1806610"
  },
  {
    "text": "um, probably you can't get away with making every title into a recursive title like that.",
    "start": "1806610",
    "end": "1813285"
  },
  {
    "text": "But if you have a title that's both very descriptive of what it's about and also somehow intriguing or with a twist or something appealing about it,",
    "start": "1813285",
    "end": "1823050"
  },
  {
    "text": "that's a great way to attract attention. Okay. So, so all the reviewers submit all their bids on which papers they wanna",
    "start": "1823050",
    "end": "1831510"
  },
  {
    "text": "review, the program chairs then assign reviewers, um, based on their bids.",
    "start": "1831510",
    "end": "1838050"
  },
  {
    "text": "The reviewers actually read the papers, write comments, and supply ratings.",
    "start": "1838050",
    "end": "1843105"
  },
  {
    "text": "And I'll tell you a lot more about the ratings in just a moment. Then there's a response period.",
    "start": "1843105",
    "end": "1849000"
  },
  {
    "text": "So the, um, authors are allowed to respond to the reviews.",
    "start": "1849000",
    "end": "1855435"
  },
  {
    "text": "So the reviews- the comments from the reviews are sent back to the authors.",
    "start": "1855435",
    "end": "1861165"
  },
  {
    "text": "Not the ratings the- I'm actually not sure. The ratings too. Okay. So the authors will",
    "start": "1861165",
    "end": "1868590"
  },
  {
    "text": "see the ratings and they'll see the comments that the reviewers have supplied and they have the opportunity to",
    "start": "1868590",
    "end": "1874290"
  },
  {
    "text": "respond to those comments for some short period of time. Um, and this can stimulate some discussion amongst the reviewers.",
    "start": "1874290",
    "end": "1884880"
  },
  {
    "text": "Um, and this is the first time typically that the reviewers are no longer anonymous to each other.",
    "start": "1884880",
    "end": "1890490"
  },
  {
    "text": "So there may be three different reviewers who have read a particular paper up, until this point they're anonymous to each other,",
    "start": "1890490",
    "end": "1895740"
  },
  {
    "text": "but now their identities are revealed. The authors of the paper are still anonymous and we don't know who they are.",
    "start": "1895740",
    "end": "1902384"
  },
  {
    "text": "But the reviewers, uh, become known to each other and you can have some interesting discussion around the paper.",
    "start": "1902385",
    "end": "1908635"
  },
  {
    "text": "Um, and finally the program committee does some magic to figure out which papers get in to the conference and which ones don't.",
    "start": "1908635",
    "end": "1917840"
  },
  {
    "text": "Probably largely based on the scores that have been assigned. Um, [NOISE] so let's talk a little bit more now about the scores.",
    "start": "1917840",
    "end": "1926435"
  },
  {
    "start": "1923000",
    "end": "2232000"
  },
  {
    "text": "Um, this is, um, when reviewers are asked to assign scores for papers,",
    "start": "1926435",
    "end": "1936075"
  },
  {
    "text": "they're asked to evaluate on many different dimensions. This is a list that's typically used for ACL and I think",
    "start": "1936075",
    "end": "1942825"
  },
  {
    "text": "other NLP conferences have lists that are like this but with minor variation. So let's- let me talk a little bit about each of these.",
    "start": "1942825",
    "end": "1950460"
  },
  {
    "text": "Um, appropriateness basically means is this p- is this paper a good fit for this conference?",
    "start": "1950460",
    "end": "1955649"
  },
  {
    "text": "Does it sort of fit within the theme and the topics that are appropriate for this conference? It might be a great paper that doesn't belong in this conference,",
    "start": "1955650",
    "end": "1963210"
  },
  {
    "text": "and so I get a, a low score for appropriateness. Clarity I think is pretty self-explanatory.",
    "start": "1963210",
    "end": "1968625"
  },
  {
    "text": "Can you understand what the people actually did in this, um, in this investigation?",
    "start": "1968625",
    "end": "1975585"
  },
  {
    "text": "Replicatabil- replicability is a criterion that's growing in importance.",
    "start": "1975585",
    "end": "1980700"
  },
  {
    "text": "Um, I think five or ten years ago people didn't really think about this that much. More and more it's becoming part of",
    "start": "1980700",
    "end": "1986970"
  },
  {
    "text": "a community standard that results should be replicable. This is really healthy. And a big part of replic- replicability is making code and data",
    "start": "1986970",
    "end": "1996015"
  },
  {
    "text": "available online so that others can go and repeat the experiment and see if they get the same results.",
    "start": "1996015",
    "end": "2002090"
  },
  {
    "text": "Originality, I think it's pretty self-explanatory. Soundness and correctness were the decisions,",
    "start": "2002090",
    "end": "2009530"
  },
  {
    "text": "um, well- well-motivated decisions is- or the things that I try.",
    "start": "2009530",
    "end": "2015050"
  },
  {
    "text": "Were the things that I tried the right things to try? Do my, um, sort of logical steps that I followed in my investigation make sense?",
    "start": "2015050",
    "end": "2024890"
  },
  {
    "text": "Meaningful comparison. Did I, um, include comparisons to all of the work that I should have compared to,",
    "start": "2024890",
    "end": "2034070"
  },
  {
    "text": "or is their work out there that's very relevant but I seem to be unaware of it?",
    "start": "2034070",
    "end": "2039815"
  },
  {
    "text": "If I'm unaware of related work or if in my quantitative results, I don't make comparisons to closely related work,",
    "start": "2039815",
    "end": "2045845"
  },
  {
    "text": "um, that's a miss. Thoroughness. Did I, um, did I explain everything I need to explain?",
    "start": "2045845",
    "end": "2053135"
  },
  {
    "text": "Sometimes a lack of thoroughness can impair replicability. If I haven't actually specified all of the details of all of the steps,",
    "start": "2053135",
    "end": "2062165"
  },
  {
    "text": "it makes it hard to re- to replicate my results. Um, impact of ideas or results.",
    "start": "2062165",
    "end": "2068690"
  },
  {
    "text": "Ah, I think that's pretty self-explanatory. Impact of resources.",
    "start": "2068690",
    "end": "2074464"
  },
  {
    "text": "Um, this is particularly relevant if your work introduces a new dataset to the community that might be of use to others.",
    "start": "2074465",
    "end": "2084665"
  },
  {
    "text": "Not introducing a new dataset is not a problem. Um, it doesn't mean that there's anything wrong.",
    "start": "2084665",
    "end": "2090950"
  },
  {
    "text": "But if you have introduced a new dataset that's going to be valuable to the community, that can really help to imp- increase the impact in the appeal of your work.",
    "start": "2090950",
    "end": "2101300"
  },
  {
    "text": "And finally, the overall recommendation which is undoubtedly the most important score. It's sort- sort of, ah, intended to be a distillation of all the others.",
    "start": "2101300",
    "end": "2109130"
  },
  {
    "text": "So all of the other scores should inform the overall evaluation.",
    "start": "2109130",
    "end": "2114630"
  },
  {
    "text": "Those are the scores and then typically they ask for a couple of other bits of metadata. They ask.",
    "start": "2114670",
    "end": "2121115"
  },
  {
    "text": "Do you- as a reviewer do you recommend this work be presented as a poster in a poster session or as an oral presentation? So as a talk.",
    "start": "2121115",
    "end": "2131735"
  },
  {
    "text": "Um, it's definitely more prestigious to be selected for an oral presentation than a poster,",
    "start": "2131735",
    "end": "2139159"
  },
  {
    "text": "but on the other hand there's nothing wrong with presenting your work as a poster and it's a great way to get exposure for",
    "start": "2139159",
    "end": "2144859"
  },
  {
    "text": "your work and sometimes presenting your work as a poster is actually appealing because it facilitates discussion with your audience.",
    "start": "2144860",
    "end": "2152059"
  },
  {
    "text": "Ah, what sort of, one on one discussion makes it very easy for interested people to ask questions.",
    "start": "2152060",
    "end": "2158670"
  },
  {
    "text": "Um, [NOISE] sometimes by the way some conferences have both short and long oral presentation.",
    "start": "2160240",
    "end": "2167840"
  },
  {
    "text": "So long oral presentation is typically a 20-minute presentation. Sometimes they're also short oral presentations",
    "start": "2167840",
    "end": "2174050"
  },
  {
    "text": "which are like five minutes or seven minutes. Um, of course as an author you'd rather have a long form presentation than a short form presentation,",
    "start": "2174050",
    "end": "2181955"
  },
  {
    "text": "but they'll often ask that here. Um, and they may ask whether this paper should be considered as a candidate for Best Paper.",
    "start": "2181955",
    "end": "2190025"
  },
  {
    "text": "Um, typically a conference will have at least one Best Paper Award. Sometimes they'll have like, um,",
    "start": "2190025",
    "end": "2195365"
  },
  {
    "text": "Best Paper Overall and best, um, Best Student Paper or something like that.",
    "start": "2195365",
    "end": "2201039"
  },
  {
    "text": "[NOISE] Oh and some conferences have both long-form papers which are at the eight pages,",
    "start": "2201040",
    "end": "2207220"
  },
  {
    "text": "the eight page convention that we looked at before and also short-form papers which are four pages.",
    "start": "2207220",
    "end": "2212980"
  },
  {
    "text": "You may have submitted a paper as an eight-page paper but, a reviewer might have an opportunity to say",
    "start": "2212980",
    "end": "2219075"
  },
  {
    "text": "actually I think this one would be better as a short paper, if it were reworked a little bit, um,",
    "start": "2219075",
    "end": "2225545"
  },
  {
    "text": "and typically the bar is a little bit lower for short papers. [NOISE] Is the Best Paper possibility something that the reviewer is saying or the paper is saying, because like, poster talk as well,",
    "start": "2225545",
    "end": "2236930"
  },
  {
    "start": "2232000",
    "end": "2349000"
  },
  {
    "text": "like, is that something that we have to put on as the authors, is that part of the form? Um, no.",
    "start": "2236930",
    "end": "2242600"
  },
  {
    "text": "This is coming from the reviewers, and it will be the, the program chairs will make the final determination on the Best Paper Award,",
    "start": "2242600",
    "end": "2251765"
  },
  {
    "text": "but reviewers have the opportunity to effectively nominate a paper to- for consideration as a best paper.",
    "start": "2251765",
    "end": "2257450"
  },
  {
    "text": "[inaudible]",
    "start": "2257450",
    "end": "2271220"
  },
  {
    "text": "And the, and the Best Paper Award [inaudible] it's confidence value. That's something that even program chairs in filling it up to get- get together.",
    "start": "2271220",
    "end": "2278840"
  },
  {
    "text": "[NOISE] Yes.",
    "start": "2278840",
    "end": "2284420"
  },
  {
    "text": "[inaudible] work reproducible. You mentioned making the codebase like open-source, and making your data available within the codebase,",
    "start": "2284420",
    "end": "2291170"
  },
  {
    "text": "maybe you've set that random seeds, and so on and so forth. But beyond that, within the code structure in itself,",
    "start": "2291170",
    "end": "2296735"
  },
  {
    "text": "what are the things that you think make, like, make research reproducible?",
    "start": "2296735",
    "end": "2301859"
  },
  {
    "text": "Um, I think first and foremost just making the code available, um,",
    "start": "2302650",
    "end": "2307775"
  },
  {
    "text": "which 10 years ago unfortunately was not the norm or 20 years ago was not the norm. These days [NOISE] standards to put all the code on GitHub.",
    "start": "2307775",
    "end": "2316400"
  },
  {
    "text": "Um, and I think just putting the code on GitHub is, uh, a huge proportion of what you need to do to make the code reproducible.",
    "start": "2316400",
    "end": "2325460"
  },
  {
    "text": "Now of course, um, adopting good software engineering practices and making sure that the code is readable,",
    "start": "2325460",
    "end": "2330694"
  },
  {
    "text": "and documented and not a massive spaghetti that nobody could, you know, possibly, um, understand, uh, helps as well.",
    "start": "2330695",
    "end": "2339020"
  },
  {
    "text": "Um, but making it available is by far the most important thing. [NOISE].",
    "start": "2339020",
    "end": "2350390"
  },
  {
    "start": "2349000",
    "end": "2408000"
  },
  {
    "text": "Um, a slide with the names of some notable NLP conferences.",
    "start": "2350390",
    "end": "2357769"
  },
  {
    "text": "I won't talk about each of these but, um, these are kind of the,",
    "start": "2357770",
    "end": "2362885"
  },
  {
    "text": "the biggest and most reputable NLP conferences, um, some additional conferences.",
    "start": "2362885",
    "end": "2368855"
  },
  {
    "text": "These conferences tend to be a little bit more f- more focused on, uh, the web and information retrieval rather than NLP.",
    "start": "2368855",
    "end": "2377645"
  },
  {
    "text": "And these conferences are a little bit more focused on machine learning and include lots of other topics beyond NLP,",
    "start": "2377645",
    "end": "2386390"
  },
  {
    "text": "like vision for example, but NLP tends to be an important sort of",
    "start": "2386390",
    "end": "2391550"
  },
  {
    "text": "sub current or sub-theme in those conferences as well. So if you're thinking about,uh, trying to bring, uh,",
    "start": "2391550",
    "end": "2398810"
  },
  {
    "text": "a paper to a conference, these are kind of the, the likeliest suspects to look at.",
    "start": "2398810",
    "end": "2404105"
  },
  {
    "text": "I won't talk about this because I'm not knowledgeable about this. Um, the abstract of your paper is a little,",
    "start": "2404105",
    "end": "2414590"
  },
  {
    "start": "2408000",
    "end": "2505000"
  },
  {
    "text": "um, paragraph that summarizes your paper. And the best way to think about it is that it's an advertisement for your paper.",
    "start": "2414590",
    "end": "2424325"
  },
  {
    "text": "People will read the abstract to decide whether they want to make the investment in reading the eight-page paper.",
    "start": "2424325",
    "end": "2431375"
  },
  {
    "text": "Sometimes reviewers will read it to decide on whether to bid on- to bid to review your paper.",
    "start": "2431375",
    "end": "2437420"
  },
  {
    "text": "And so just like with the title, you want the abstract to be both informative and engaging.",
    "start": "2437420",
    "end": "2443194"
  },
  {
    "text": "And you kind of want the sa- most of the same key elements that I described before. You wanna say what problem you're working on,",
    "start": "2443195",
    "end": "2449555"
  },
  {
    "text": "you may not have enough room in the abstract to say why it's an important problem or what previous approaches have been employed,",
    "start": "2449555",
    "end": "2456799"
  },
  {
    "text": "but you definitely want to say what your contribution is. And you want to communicate the most important result of an evaluation in your abstract,",
    "start": "2456799",
    "end": "2468815"
  },
  {
    "text": "um, and why it's significant. Really, the abstract should summarize what are the key contributions of your paper,",
    "start": "2468815",
    "end": "2476510"
  },
  {
    "text": "and why should I as a reader be interested in learning more. And you have to find a way to do this in only about 100 words.",
    "start": "2476510",
    "end": "2485135"
  },
  {
    "text": "Abstracts are very concise and they should be polished like a, like a, like a gem.",
    "start": "2485135",
    "end": "2491900"
  },
  {
    "text": "Uh, it's really valuable to get feedback from other people on your abstracts, um,",
    "start": "2491900",
    "end": "2497750"
  },
  {
    "text": "on whether they are clear, and whether there's any way to pack more,",
    "start": "2497750",
    "end": "2503105"
  },
  {
    "text": "um, into a very tight space. Okay. A little bit of advice on giving talks.",
    "start": "2503105",
    "end": "2510515"
  },
  {
    "start": "2505000",
    "end": "2750000"
  },
  {
    "text": "Um, actually I have nothing new to say here,",
    "start": "2510515",
    "end": "2516559"
  },
  {
    "text": "this is kind of exactly what I said about writing a paper. Um, the format is different but the content is",
    "start": "2516560",
    "end": "2524000"
  },
  {
    "text": "gonna be largely similar to what I said for writing a paper. Um, Geoff Pullum has some great guidance for giving talks,",
    "start": "2524000",
    "end": "2533750"
  },
  {
    "text": "um, that I encourage you to go read. This is a very short document, uh, where he gives [NOISE] some great rules.",
    "start": "2533750",
    "end": "2541685"
  },
  {
    "text": "The one that I want to emphasize here is number 5. Remember that you are an advocate not the defendant,",
    "start": "2541685",
    "end": "2547715"
  },
  {
    "text": "um, so kind of a metaphor from law, right? Your- when you present your work orally,",
    "start": "2547715",
    "end": "2558319"
  },
  {
    "text": "it's not you that's on trial. It's some ideas that are on trial.",
    "start": "2558319",
    "end": "2564170"
  },
  {
    "text": "It's, uh, some, some work, an investigation, an approach, a model.",
    "start": "2564170",
    "end": "2569985"
  },
  {
    "text": "Don't take it personally, don't get wrapped up, don't get wrapped up in it.",
    "start": "2569985",
    "end": "2575065"
  },
  {
    "text": "And if people question it, don't take that as an attack on you. Don't let your ego be part of it.",
    "start": "2575065",
    "end": "2581530"
  },
  {
    "text": "It's okay if the work is incomplete. It's okay if the work has flaws.",
    "start": "2581530",
    "end": "2588425"
  },
  {
    "text": "Be forthright about that, um, be dispassionate in your presentation of it.",
    "start": "2588425",
    "end": "2594440"
  },
  {
    "text": "Do your best to present the work in its best light, but don't take anything about it personally.",
    "start": "2594440",
    "end": "2600710"
  },
  {
    "text": "And that really helps to make the presentation convincing and effective.",
    "start": "2600710",
    "end": "2606994"
  },
  {
    "text": "And it kind of echoes this insight from, from Patrick Blackburn. He says, \"Where do good talks come from?",
    "start": "2606995",
    "end": "2612620"
  },
  {
    "text": "From honesty.\" Be honest about the work that you've done. As long as you're honest,",
    "start": "2612620",
    "end": "2618425"
  },
  {
    "text": "you kind of can't go wrong, um, in presenting the work.",
    "start": "2618425",
    "end": "2623190"
  },
  {
    "text": "Um, a little bit about slides and different styles of creating slides.",
    "start": "2626320",
    "end": "2632360"
  },
  {
    "text": "Uh, there's kind of two very different approaches to creating slides. One is the minimalist approach,",
    "start": "2632360",
    "end": "2638780"
  },
  {
    "text": "the other is the maximalist approach. The minimalist approach is, um,",
    "start": "2638780",
    "end": "2644839"
  },
  {
    "text": "to have very little on each slide, maybe just a couple of words. The slides go very quickly.",
    "start": "2644840",
    "end": "2651994"
  },
  {
    "text": "You kinda blaze through them. You're like click click, click, click, click as you talk. Um, if you were to look at the slides in isolation,",
    "start": "2651995",
    "end": "2660530"
  },
  {
    "text": "if somebody just gave you the deck but you didn't see the oral presentation, they might be hard to understand because there's just a couple of",
    "start": "2660530",
    "end": "2666470"
  },
  {
    "text": "words or maybe a diagram or something on each slide. Um, but there- they tend to be more exciting and interesting as oral presentations.",
    "start": "2666470",
    "end": "2675230"
  },
  {
    "text": "The maximalist approach, by the way this slide is more like the maximalist approach.",
    "start": "2675230",
    "end": "2680390"
  },
  {
    "text": "The maximalist approach, has lots of words on the slide. They tend to go slower as you go through them,",
    "start": "2680390",
    "end": "2686315"
  },
  {
    "text": "um, but if you look at them in isolation, they're a lot easier to understand because all of the words or most of the words that you need are there on the slide.",
    "start": "2686315",
    "end": "2695450"
  },
  {
    "text": "A shortcoming of the maximalist approach is that the audience is torn between reading the slide and listening to you.",
    "start": "2695450",
    "end": "2703880"
  },
  {
    "text": "And if you have a lot of words on the slide, when the slide first comes up they- they're probably reading the slide, and they're probably not listening to you.",
    "start": "2703880",
    "end": "2710120"
  },
  {
    "text": "So there are pros and cons to these two different approaches. I'm, uh, preparing to speak at WWDC next week.",
    "start": "2710120",
    "end": "2717290"
  },
  {
    "text": "And if you know anything about Apple, you that Apple takes the minimalist approach.",
    "start": "2717290",
    "end": "2723065"
  },
  {
    "text": "We have slides that have like one word on them, and they go at a pace of one slide every five seconds or something like that.",
    "start": "2723065",
    "end": "2731809"
  },
  {
    "text": "Um, and then there's like graphics, and, you know it's a very different style. Um, there's no right answer here.",
    "start": "2731810",
    "end": "2738545"
  },
  {
    "text": "You need to find a style that works for you and works for the content that you're trying to present,",
    "start": "2738545",
    "end": "2743585"
  },
  {
    "text": "but I think it's valuable to think about the pros and cons of these different styles.",
    "start": "2743585",
    "end": "2749160"
  },
  {
    "start": "2750000",
    "end": "2916000"
  },
  {
    "text": "Um, Edward Tufte and Peter Norvig have thoughts about PowerPoint,",
    "start": "2750970",
    "end": "2756979"
  },
  {
    "text": "those thoughts are not flattering but if- but this is kind of amusing if you wanna go click on these links later.",
    "start": "2756979",
    "end": "2762680"
  },
  {
    "text": "Uh, more mundane things, um, I won't talk about all these but there's a bunch of really good advice here.",
    "start": "2762680",
    "end": "2771470"
  },
  {
    "text": "It's really good to test your setup in advance, typically when you're presenting at a conference,",
    "start": "2771470",
    "end": "2777065"
  },
  {
    "text": "there will be, um, like, five talks in a row. Before that there will be a break,",
    "start": "2777065",
    "end": "2782329"
  },
  {
    "text": "if you're presenting during that block test your setup in the break before that block.",
    "start": "2782330",
    "end": "2787595"
  },
  {
    "text": "I one time didn't do that, made that mistake and I spent- I had 20 minutes for my talk and I",
    "start": "2787595",
    "end": "2792980"
  },
  {
    "text": "spent the first five minutes of my talk fumbling with my setup, standing in front of a room of 300 people,",
    "start": "2792980",
    "end": "2799144"
  },
  {
    "text": "like, fumbling with my setup. And it was incredibly embarrassing, and then when I finally did get started with my talk I was all",
    "start": "2799145",
    "end": "2804500"
  },
  {
    "text": "flustered and my talk was a disaster. So learn from my mistake, test your setup before you launch into your talk.",
    "start": "2804500",
    "end": "2811190"
  },
  {
    "text": "Um, also make sure you don't have any windows in the background with embarrassing stuff on it.",
    "start": "2811190",
    "end": "2816395"
  },
  {
    "text": "I have not made that mistake, but I've seen other people who've made that mistake.",
    "start": "2816395",
    "end": "2821790"
  },
  {
    "text": "Um, and the question period. Typically when you give a talk at a conference, uh,",
    "start": "2821920",
    "end": "2829790"
  },
  {
    "text": "there'll be a- each talk is 20 minutes, and there's usually five minutes for questions.",
    "start": "2829790",
    "end": "2835145"
  },
  {
    "text": "The question period is really important because, um, this is when you get feedback from the audience about what they want to know more about.",
    "start": "2835145",
    "end": "2843665"
  },
  {
    "text": "Questions are great. If you get questions in your question period, especially thoughtful questions, that's a sign that you've really",
    "start": "2843665",
    "end": "2851900"
  },
  {
    "text": "connected with your audience and they're interested in your topic. If you don't get any questions,",
    "start": "2851900",
    "end": "2857930"
  },
  {
    "text": "you should be a little bit bummed out. That somehow you failed to get your audience interested in your topic.",
    "start": "2857930",
    "end": "2863984"
  },
  {
    "text": "But questions are also scary because you have no idea what's going to come at you.",
    "start": "2863985",
    "end": "2869660"
  },
  {
    "text": "The talk you can rehearse and rehearse and rehearse, and feel really well-prepared. The questions there is no way to prepare.",
    "start": "2869660",
    "end": "2876605"
  },
  {
    "text": "You just have to think on your feet and do your best. Um, you'll do better if you really know your topic well which you will.",
    "start": "2876605",
    "end": "2883550"
  },
  {
    "text": "But even so, you have no idea what's gonna come at you. Sometimes questions will come that you don't know how to respond to. That's okay.",
    "start": "2883550",
    "end": "2891945"
  },
  {
    "text": "It's okay to say, \"I don't know, but I would love to talk to you more about this afterward.\"",
    "start": "2891945",
    "end": "2898175"
  },
  {
    "text": "And actually that can be a great opportunity to make a new connection with somebody in a research community that can lead to anything,",
    "start": "2898175",
    "end": "2906890"
  },
  {
    "text": "it could lead to a future collaboration. So treat that as an opportunity and as a,",
    "start": "2906890",
    "end": "2912619"
  },
  {
    "text": "as a welcome thing. Okay. Just a couple of tactical things about your presentations.",
    "start": "2912620",
    "end": "2920599"
  },
  {
    "start": "2916000",
    "end": "2977000"
  },
  {
    "text": "For this class, you're gonna prepare a video presentation. It's due on June 5th. That's coming up soon.",
    "start": "2920600",
    "end": "2926255"
  },
  {
    "text": "It's going to be a four minute talk. You're gonna create a short deck in Google Slides or PowerPoint or Keynote.",
    "start": "2926255",
    "end": "2934505"
  },
  {
    "text": "You're gonna record yourself [NOISE] talking through this deck. You can do that, uh, on a Mac using QuickTime or if you're not on a Mac,",
    "start": "2934505",
    "end": "2942935"
  },
  {
    "text": "you'll have to figure it out [NOISE] but maybe, uh, other people in the class will have helpful suggestions for you.",
    "start": "2942935",
    "end": "2949070"
  },
  {
    "text": "Um, you'll upload the video to YouTube. You can make it private if you want to, um,",
    "start": "2949070",
    "end": "2954110"
  },
  {
    "text": "and then you'll send us the link for this four minute video. You don't have to be in the video.",
    "start": "2954110",
    "end": "2959510"
  },
  {
    "text": "Your, your face doesn't have to be in the video. The video will just show your slides and the, and the audio will be you talking through the deck.",
    "start": "2959510",
    "end": "2966934"
  },
  {
    "text": "Any questions about that before I hand off? [inaudible] you said about-",
    "start": "2966935",
    "end": "2973535"
  },
  {
    "text": "I think the answer is yes. Slightly unrelated, but I'm curious to know your advice on like reading papers,",
    "start": "2973535",
    "end": "2982578"
  },
  {
    "start": "2977000",
    "end": "3062000"
  },
  {
    "text": "so as you said, you must be reading like a lot of papers. Yeah. So how do you do that efficiently? Do you do it like in one go or you'd go to the",
    "start": "2982579",
    "end": "2989870"
  },
  {
    "text": "abstract and like introduction and conclusion and then do like a second pass later?",
    "start": "2989870",
    "end": "2995075"
  },
  {
    "text": "It's because, I mean it takes me like one and a half to two hours and it's pretty dense. I don't have any specific strategy to suggest,",
    "start": "2995075",
    "end": "3003130"
  },
  {
    "text": "but I share your experience like I can't absorb an eight-page dense paper in one sitting typically.",
    "start": "3003130",
    "end": "3011920"
  },
  {
    "text": "So if it's a paper that- I mean sometimes I'll- if it's the paper that I don't think it's that important for me to really get,",
    "start": "3011920",
    "end": "3018744"
  },
  {
    "text": "I'll sort of like read the abstract and introduction carefully and then kind of skim the rest,",
    "start": "3018745",
    "end": "3024295"
  },
  {
    "text": "but if it's a paper that I really wanna get, I typically need to read it in more than one sitting in order to fully absorb it.",
    "start": "3024295",
    "end": "3032529"
  },
  {
    "text": "Beyond that I don't have any specific strategy to recommend. Yeah. You can't actually record it like live as like with a video camera,",
    "start": "3032530",
    "end": "3043165"
  },
  {
    "text": "or does it need to be just like slides and voiceover? I think that's okay. It's not what people usually do, but I think it's okay.",
    "start": "3043165",
    "end": "3049480"
  },
  {
    "text": "Yeah. Okay, I'm gonna hand off at this point to Min and Jayadev.",
    "start": "3049480",
    "end": "3055670"
  },
  {
    "text": "Right. So I'm gonna be giving a short presentation on data augmentation for natural language processing.",
    "start": "3055830",
    "end": "3062030"
  },
  {
    "text": "So what is data augmentation? So data augmentation is a technique to uh,",
    "start": "3062310",
    "end": "3067480"
  },
  {
    "start": "3067000",
    "end": "3104000"
  },
  {
    "text": "to increase the amount of relevant data. In machine learning and especially in deep learning, more data usually means better accuracy.",
    "start": "3067480",
    "end": "3074275"
  },
  {
    "text": "So if you don't really have a lot of training examples, you can try data augmentation techniques to uh,",
    "start": "3074275",
    "end": "3079885"
  },
  {
    "text": "to create more synthetic training examples. Data augmentation is actually very popular in computer vision, uh,",
    "start": "3079885",
    "end": "3086455"
  },
  {
    "text": "people create new images by shifting the original images or uh, creating zoomed in or zoomed out versions,",
    "start": "3086455",
    "end": "3092859"
  },
  {
    "text": "rotating, flipping, and etc. So how do we do such transformation in natural language?",
    "start": "3092859",
    "end": "3098800"
  },
  {
    "text": "We can't flip a sentence or rotate a sentence. So we need to come up with different approaches.",
    "start": "3098800",
    "end": "3104575"
  },
  {
    "start": "3104000",
    "end": "3161000"
  },
  {
    "text": "So before we actually talk about the actual data augmentation techniques, we ran some simple experiments.",
    "start": "3104575",
    "end": "3110825"
  },
  {
    "text": "So we wanted to do um, sentiment classification of IMDB movie reviews using logistic regression,",
    "start": "3110825",
    "end": "3115980"
  },
  {
    "text": "and we wanted to see the correlation between number of one possible- number of training examples and the model test accuracy.",
    "start": "3115980",
    "end": "3122190"
  },
  {
    "text": "As you can see, if we only have 1,000 training examples, model- our model accuracy was only about 79%.",
    "start": "3122190",
    "end": "3128740"
  },
  {
    "text": "But with more training examples they actually see- increased significantly. So it's usually the case that",
    "start": "3128740",
    "end": "3135955"
  },
  {
    "text": "the small number of- with small number of training examples, you can't really have a really high-performance model.",
    "start": "3135955",
    "end": "3142135"
  },
  {
    "text": "So in those situations uh, data augmentation can really help boost the performance. So for this presentation,",
    "start": "3142135",
    "end": "3148255"
  },
  {
    "text": "I'm gonna assume that we only have 1,000 examples and we are going to apply different data augmentation techniques to see if we can boost the uh,",
    "start": "3148255",
    "end": "3155244"
  },
  {
    "text": "the accuracy of 79%. Feel free to uh, ask any questions.",
    "start": "3155245",
    "end": "3160944"
  },
  {
    "text": "So how do we actually augment natural language training data? So we're gonna be discussing two main approaches.",
    "start": "3160945",
    "end": "3168069"
  },
  {
    "text": "The first one is easy data augmentation uh, EDA, which uses uh very simple heuristics to uh,",
    "start": "3168070",
    "end": "3173815"
  },
  {
    "text": "to transport sentences to create um, new relevant examples. We're also gonna talk about back translation,",
    "start": "3173815",
    "end": "3179965"
  },
  {
    "text": "which uses neural machine translation to, um, to give noise to your sentences.",
    "start": "3179965",
    "end": "3186355"
  },
  {
    "text": "So as we mentioned before um, the baseline is going to be the IMDB 1,000 examples trained logistic regression model,",
    "start": "3186355",
    "end": "3193315"
  },
  {
    "text": "which has an accuracy of about 79% and we're gonna apply uh, EDA and back translation to um,",
    "start": "3193315",
    "end": "3199315"
  },
  {
    "text": "to augment the training samples. Okay, so we're gonna look at the sentence.",
    "start": "3199315",
    "end": "3206005"
  },
  {
    "text": "The final project is the main assignment of course, to explore different data augmentation models. We're going to transform this sentence using back translation and EDA to give you uh,",
    "start": "3206005",
    "end": "3214795"
  },
  {
    "text": "some sense of how sentences can be transformed. So let's first talk about easy data augmentation.",
    "start": "3214795",
    "end": "3222625"
  },
  {
    "start": "3219000",
    "end": "3261000"
  },
  {
    "text": "Uh, it's a paper written by Wei and Zou in 2019. So it's a relatively new research paper,",
    "start": "3222625",
    "end": "3227694"
  },
  {
    "text": "and it uses four very simple techniques. First, it uses synonym replacement.",
    "start": "3227695",
    "end": "3232900"
  },
  {
    "text": "So it randomly selects n words from a sentence and it replace- replaces those words with its synonyms- with their synonyms.",
    "start": "3232900",
    "end": "3240130"
  },
  {
    "text": "Random insertion is something very similar, uh, but instead of replacing it inserts uh, the synonyms.",
    "start": "3240130",
    "end": "3248015"
  },
  {
    "text": "A random swap and random deletion are pretty self-explanatory uh, random swap swaps two random words and random deletion uh, deletes a random word.",
    "start": "3248015",
    "end": "3258430"
  },
  {
    "text": "So um, let's look at our example sentence. The final project is the main assignment of the course.",
    "start": "3260610",
    "end": "3267160"
  },
  {
    "start": "3261000",
    "end": "3389000"
  },
  {
    "text": "So if we apply um, synonym replacement, we can replace main with principal because um,",
    "start": "3267160",
    "end": "3273369"
  },
  {
    "text": "principal is a synonym of main. So the resulting sentence can be um, the final project is the principal assignment of the course.",
    "start": "3273370",
    "end": "3280825"
  },
  {
    "text": "If we apply random insertion we can insert last, because last is a synonym of final and the resulting sentence now becomes,",
    "start": "3280825",
    "end": "3289360"
  },
  {
    "text": "the final project is the main assignment of last the course, which doesn't make a lot of sense as a sentence,",
    "start": "3289360",
    "end": "3294685"
  },
  {
    "text": "but it still roughly um, retains the original meaning. And if we perform random swap,",
    "start": "3294685",
    "end": "3300490"
  },
  {
    "text": "we can swap main with assignment, and we can also delete a random word by using random deletion and in this case,",
    "start": "3300490",
    "end": "3306640"
  },
  {
    "text": "um, the word project is deleted. So if you look at all these sentences you can see that um,",
    "start": "3306640",
    "end": "3313060"
  },
  {
    "text": "it doesn't necessarily preserve the original meanings, but they're roughly similar and in line with the original sentence.",
    "start": "3313060",
    "end": "3320930"
  },
  {
    "text": "So the authors of the EDA paper actually ran latent space visualization of the augmented sentences and the original sentences.",
    "start": "3321000",
    "end": "3330714"
  },
  {
    "text": "And you can see uh, the red dots roughly coincide with the red circles, and the blue dots also roughly coincide with the blue triangles.",
    "start": "3330715",
    "end": "3339460"
  },
  {
    "text": "Meaning that um, they roughly have similar meanings. So EDA can be very useful in",
    "start": "3339460",
    "end": "3344994"
  },
  {
    "text": "synthesizing new sentences that are relevant to our training samples. So we apply EDA to our 1,000 training sample dataset",
    "start": "3344995",
    "end": "3353530"
  },
  {
    "text": "and we use the GitHub repository that was published and released by the uh, EDA authors.",
    "start": "3353530",
    "end": "3359245"
  },
  {
    "text": "It's relatively simple to set up and it doesn't- it's relatively easy to um, to perform the EDA operation using the GitHub repository.",
    "start": "3359245",
    "end": "3366819"
  },
  {
    "text": "So we created 10,000 more EDA augmented examples, and then we used those examples in addition to the original examples into",
    "start": "3366820",
    "end": "3376299"
  },
  {
    "text": "our logistic regression model and we were able to see about 1.2% um, accuracy improvement.",
    "start": "3376300",
    "end": "3381730"
  },
  {
    "text": "Which we thought was pretty significant because um, EDA merely performs very simple heuristics.",
    "start": "3381730",
    "end": "3388100"
  },
  {
    "start": "3389000",
    "end": "3414000"
  },
  {
    "text": "Okay? So let's talk about back translation. So back translation uses neural machine translation to augment training data.",
    "start": "3389040",
    "end": "3396115"
  },
  {
    "text": "So more simply put, when you have an English sentence, you translate that into an intermediate language such as German,",
    "start": "3396115",
    "end": "3401950"
  },
  {
    "text": "French or anything you can think of and then you translate back- translate that back into English and the resulting sentence is going to",
    "start": "3401950",
    "end": "3408070"
  },
  {
    "text": "be kind of a paraphrased version of the original sentence. There's gonna be some noise into the sentence.",
    "start": "3408070",
    "end": "3414160"
  },
  {
    "text": "So let's take a look at an example. So this is an example of using um, Google Translator.",
    "start": "3414160",
    "end": "3420880"
  },
  {
    "text": "So our sentence um, the final project is the main assignment of the course.",
    "start": "3420880",
    "end": "3426010"
  },
  {
    "text": "If we translate that into German and translate it back into English, um, the resulting sentence now becomes,",
    "start": "3426010",
    "end": "3431710"
  },
  {
    "text": "the final product is the main task of the course. As you can see uh, the assignment is swapped with task.",
    "start": "3431710",
    "end": "3437680"
  },
  {
    "text": "So it's kind of- kind of like a paraphrased version of the original sentence. We can use different intermediate languages. I tried Korean.",
    "start": "3437680",
    "end": "3444550"
  },
  {
    "text": "Um, if we- if we translate to Korean and back into um, English, the sentence now becomes,",
    "start": "3444550",
    "end": "3450865"
  },
  {
    "text": "the final project is a major challenge for the course, which is a bit of a more paraphrased version.",
    "start": "3450865",
    "end": "3455935"
  },
  {
    "text": "So um, using Google's translator we discovered that it's possible to create paraphrased versions of sentence um, using back translation.",
    "start": "3455935",
    "end": "3464425"
  },
  {
    "text": "So we applied back translation to our 1,000 examples. So we used um, Google Translate API um,",
    "start": "3464425",
    "end": "3472494"
  },
  {
    "text": "to translate from English to German and German back to English. And we created 10- we created 1,000 um, back translated examples.",
    "start": "3472495",
    "end": "3481405"
  },
  {
    "text": "So when we combined those back translated examples to our original training data and used it to train our logistic regression model,",
    "start": "3481405",
    "end": "3488905"
  },
  {
    "text": "we were able to see a pretty impressive um, 1.8% accuracy improvement.",
    "start": "3488905",
    "end": "3494180"
  },
  {
    "start": "3496000",
    "end": "3600000"
  },
  {
    "text": "So lastly, uh, we also used back translation in addition to EDA to see,",
    "start": "3496200",
    "end": "3503454"
  },
  {
    "text": "uh, how things are gonna work out, and we trained our logistic regression model on the 1,000,",
    "start": "3503455",
    "end": "3509980"
  },
  {
    "text": "um, original training examples, and 10,000 augmented examples from EDA,",
    "start": "3509980",
    "end": "3515410"
  },
  {
    "text": "and also 1,000 augmented examples from back translation. Then when we combine all of them,",
    "start": "3515410",
    "end": "3520840"
  },
  {
    "text": "and then when we train a logistic regression model, we're able to get an accuracy of a- of around 81%,",
    "start": "3520840",
    "end": "3526615"
  },
  {
    "text": "which is about 2% increase from the baseline model. So these are some few, uh,",
    "start": "3526615",
    "end": "3533755"
  },
  {
    "text": "data augmentation techniques we can probably use. Um, they're relatively simple to implement. For the back translation,",
    "start": "3533755",
    "end": "3539635"
  },
  {
    "text": "you just have to set up a Google Translate API and just call it from- from your code.",
    "start": "3539635",
    "end": "3544580"
  },
  {
    "text": "And if you're working on your final project and, um, if you don't have a lot of data,",
    "start": "3544770",
    "end": "3550105"
  },
  {
    "text": "and you think that having more data is gonna help boost your model's performance, I recommend, um, looking into, um,",
    "start": "3550105",
    "end": "3556810"
  },
  {
    "text": "data augmentation techniques including EDA or back translation. Even if you do have a lot of data,",
    "start": "3556810",
    "end": "3561930"
  },
  {
    "text": "I think it's still worth, um, experimenting to see how things go, and you can always write down the results.",
    "start": "3561930",
    "end": "3567540"
  },
  {
    "text": "So, um, on your final paper or your final project, ev- even if it doesn't work out. Cool. That's all I have.",
    "start": "3567540",
    "end": "3573750"
  },
  {
    "text": "[inaudible] techniques like that",
    "start": "3573750",
    "end": "3580420"
  },
  {
    "text": "might give like behavior gains or is it always kind of like just a little bit of an increase?",
    "start": "3580420",
    "end": "3587680"
  },
  {
    "text": "So from the paper that I read, um, it seems that data augmentation is still relatively new in natural language processing,",
    "start": "3587680",
    "end": "3594414"
  },
  {
    "text": "so, um, I couldn't really find any more on [NOISE] comprehensive or complicated,",
    "start": "3594415",
    "end": "3600010"
  },
  {
    "text": "[NOISE] um, model for data augmentation. But these are some of the most widely used some augmentation techniques.",
    "start": "3600010",
    "end": "3606020"
  },
  {
    "text": "Any more questions? [NOISE] Cool.",
    "start": "3607020",
    "end": "3612175"
  },
  {
    "text": "Thank you. [APPLAUSE] Hi, everyone, I'm Akhila.",
    "start": "3612175",
    "end": "3617964"
  },
  {
    "text": "I'll be talking about, um, probing blackma- black box models today.",
    "start": "3617965",
    "end": "3623170"
  },
  {
    "text": "Um, so essentially this- the- the whole concept of probing is to be able to understand what models are actually doing.",
    "start": "3623170",
    "end": "3632619"
  },
  {
    "text": "It's some sort of introspection technique. Um, today we'll be talking specifically with regard to like,",
    "start": "3632620",
    "end": "3638210"
  },
  {
    "text": "um, contextualize world whe- word embeddings and sentence embeddings. But this is something which can be applied to a lot of other models as well.",
    "start": "3638210",
    "end": "3646150"
  },
  {
    "text": "Like, um, like I'm sure everyone's heard about the OpenAI GPT-2 lang- huge language model.",
    "start": "3646150",
    "end": "3652484"
  },
  {
    "text": "It's- it, it produces these contextualized word embeddings, which is trained on a simple objective of language modeling,",
    "start": "3652485",
    "end": "3659080"
  },
  {
    "text": "but they do encode a lot of other information, and it's really interesting to see how they encode this information,",
    "start": "3659080",
    "end": "3664900"
  },
  {
    "text": "and how it can be effectively applied to a lot more downstream tasks. [NOISE] Um, so, an",
    "start": "3664900",
    "end": "3671964"
  },
  {
    "text": "informal definition of what a probing task is that it's a classification problem which focuses",
    "start": "3671965",
    "end": "3678280"
  },
  {
    "text": "on some simple linguistic properties of these embeddings. Um, this is an informal definition from this paper which is by Facebook AI Research.",
    "start": "3678280",
    "end": "3687640"
  },
  {
    "text": "Uh, I would highly recommend reading this paper if you wanna go [NOISE] more into like how probing architectures work,",
    "start": "3687640",
    "end": "3695530"
  },
  {
    "text": "and that sort of stuff. [NOISE] Um, and so these probing methods are designed to evaluate to what extent,",
    "start": "3695530",
    "end": "3704949"
  },
  {
    "text": "um, like they, they encode this sort of linguistic properties. Um, so when you see probing tasks,",
    "start": "3704949",
    "end": "3713019"
  },
  {
    "text": "[NOISE] most of these probing tasks are gonna be very simple, so that you don't have like interfering [NOISE] results.",
    "start": "3713020",
    "end": "3718450"
  },
  {
    "text": "You're gonna be, be probing for something very specific, um, and these probing architectures are really",
    "start": "3718450",
    "end": "3724315"
  },
  {
    "text": "simple in the sense that there would be like a linear transformation, or like a two-layer NLP. Um, essentially the main idea is when you",
    "start": "3724315",
    "end": "3731710"
  },
  {
    "text": "do have these sort of embedding representations, you want to understand what sort of information they encode rather",
    "start": "3731710",
    "end": "3737500"
  },
  {
    "text": "than forcing some sort of classifier to learn this on its behalf. Um, and, uh, it's [NOISE] easier to control for biases in",
    "start": "3737500",
    "end": "3747130"
  },
  {
    "text": "these probing tasks since it's simple because I- you can't really control this in like the downstream tasks. And this is gonna be, uh,",
    "start": "3747130",
    "end": "3754480"
  },
  {
    "text": "an architecture which is agnostic of where do you get this embedding representation from, which makes it really, uh,",
    "start": "3754480",
    "end": "3761425"
  },
  {
    "text": "useful in my opinion because it's very applicable and one sort of probing task which is designed to,",
    "start": "3761425",
    "end": "3767485"
  },
  {
    "text": "you know, probe for certain properties can be applied to different models. [NOISE] Uh, so one example is, uh,",
    "start": "3767485",
    "end": "3775815"
  },
  {
    "text": "given that you have a pre-trained encoder which is trained on some objective, um,",
    "start": "3775815",
    "end": "3780855"
  },
  {
    "text": "you want to get some one vector representation from this encoder,",
    "start": "3780855",
    "end": "3786285"
  },
  {
    "text": "and then you would be passing that to- through your probing model, and as I already mentioned that this probing model is designed to be like really shallow,",
    "start": "3786285",
    "end": "3794319"
  },
  {
    "text": "like it's gonna be like a one-layer transformation, or a two-layer, and then you- uh, at the end of this,",
    "start": "3794320",
    "end": "3799765"
  },
  {
    "text": "it would be a classification task based on what sort of linguistic properties you are trying to, um, see is encoded in this sort of, um, word representation.",
    "start": "3799765",
    "end": "3810609"
  },
  {
    "text": "[NOISE] Um, so today, we'll be talking about two sorts of probes.",
    "start": "3810610",
    "end": "3815740"
  },
  {
    "text": "One is like sentence embedding probes, and the other one is word embedding probe. They both go hand in hand,",
    "start": "3815740",
    "end": "3821620"
  },
  {
    "text": "but I, I kind of want to, um, highlight some of the characteristics which you can",
    "start": "3821620",
    "end": "3827200"
  },
  {
    "text": "actually probe for when you're looking at a sentence level versus [NOISE] like a word level. So given a sentence level embedding,",
    "start": "3827200",
    "end": "3833530"
  },
  {
    "text": "you can- you- I just named two of them, but you can obtain like a sentence level embedding from a BiLSTM, or a Gated ConvNet,",
    "start": "3833530",
    "end": "3841450"
  },
  {
    "text": "or any other encoder which is trained on any objective, like it can be machine translation.",
    "start": "3841450",
    "end": "3848020"
  },
  {
    "text": "It can be SNLI. It can be SkipThought. SkipThought is a special kind where you're training",
    "start": "3848020",
    "end": "3853359"
  },
  {
    "text": "this encoder such that enabling sentences have- are most similar, um, or, uh, the- there's a lot of literature",
    "start": "3853360",
    "end": "3861790"
  },
  {
    "text": "on how encoders i- initialize with random weights. They can also encode,",
    "start": "3861790",
    "end": "3867175"
  },
  {
    "text": "um, properties of the sentence, or the word. So, uh, given that you do get an embedding, uh,",
    "start": "3867175",
    "end": "3873309"
  },
  {
    "text": "at the- like when you do pass a sentence through this encoder, and you get an embedding at the end of it,",
    "start": "3873310",
    "end": "3878785"
  },
  {
    "text": "there is a couple of properties you can probe for; like surface level information. When I say surface level information,",
    "start": "3878785",
    "end": "3885369"
  },
  {
    "text": "it can be like, um, does this embedding have some sort of information about what the actual length of the sentence was.",
    "start": "3885369",
    "end": "3893890"
  },
  {
    "text": "Um, word content, like can you actually get information about what sort of words are in the sentence?",
    "start": "3893890",
    "end": "3900760"
  },
  {
    "text": "Um, syntactic information is whether- uh, this is more on an encoder level,",
    "start": "3900760",
    "end": "3906414"
  },
  {
    "text": "where you're trying to understand whether this encoder is actually sensitive to some synta- like semantic information.",
    "start": "3906415",
    "end": "3912655"
  },
  {
    "text": "Like when I say bigram shift, given a sentence, if I swap two bigra- like if I take a bigram, and I swap that,",
    "start": "3912655",
    "end": "3919525"
  },
  {
    "text": "and I pass it through my encoder, is my encoder gonna be sensitive to the swap or not? Um, and then semantic is,",
    "start": "3919525",
    "end": "3927475"
  },
  {
    "text": "uh, for example, is tense. You can find the tense of the, the main clause verb.",
    "start": "3927475",
    "end": "3933040"
  },
  {
    "text": "Uh, I actually like the last task which is the Semantic Odd Man Out task, where basically what you do is,",
    "start": "3933040",
    "end": "3940705"
  },
  {
    "text": "um, you're gonna replace a, a noun or a verb randomly with another noun or a verb,",
    "start": "3940705",
    "end": "3946045"
  },
  {
    "text": "but the unigram frequency of this particular noun or verb which is swapped out is similar to the original word.",
    "start": "3946045",
    "end": "3954145"
  },
  {
    "text": "Um, and when you do pass it through- so you're trying to- you're trying to see if the sentence",
    "start": "3954145",
    "end": "3959454"
  },
  {
    "text": "is- the sentence embedding is just based on superficial like tone frequency properties,",
    "start": "3959455",
    "end": "3964810"
  },
  {
    "text": "or is the actually embedding some form of context. Does it make sense or? Yeah. [NOISE] Um.",
    "start": "3964810",
    "end": "3972320"
  },
  {
    "text": "So the next topic is- ah, rather the probe is like the Contextual Word Embeddings,",
    "start": "3972540",
    "end": "3980410"
  },
  {
    "text": "which seems to be, uh, hip- been happening. Place right now that like BERT and OpenAI.",
    "start": "3980410",
    "end": "3987025"
  },
  {
    "text": "So reiterating the same idea about a probe, it's just that- so when I say Contextual Word Embeddings,",
    "start": "3987025",
    "end": "3993340"
  },
  {
    "text": "they're very different from say GloVe embeddings, because in GloVe it would be like one vector for one word,",
    "start": "3993340",
    "end": "3999610"
  },
  {
    "text": "but Contextual Word Embeddings is- is when you do parse a sentence, you get like a word embedding for every token based on",
    "start": "3999610",
    "end": "4005910"
  },
  {
    "text": "the context in which it- in- of that sentence [NOISE]. So the idea is the same that, uh,",
    "start": "4005910",
    "end": "4012285"
  },
  {
    "text": "if you have a simple model which is trained to predict like some sort of linguistic information,",
    "start": "4012285",
    "end": "4017775"
  },
  {
    "text": "um, and this is a simple model. Then you can infer that this information is actually encoded in these Word Embeddings,",
    "start": "4017775",
    "end": "4024450"
  },
  {
    "text": "and not really a property of this probing architecture. Uh, so some of the tasks you can do for a word level,",
    "start": "4024450",
    "end": "4035430"
  },
  {
    "text": "um, embedding is whether it has syntactic information again. Like the part of speech,",
    "start": "4035430",
    "end": "4041355"
  },
  {
    "text": "the named entity recognition, uh, given two tokens whether you can actually get like the dependencies between these tokens.",
    "start": "4041355",
    "end": "4048445"
  },
  {
    "text": "Uh, the constituency labeling meaning that given a sentence you want to induce like a phrase structure on it,",
    "start": "4048445",
    "end": "4054720"
  },
  {
    "text": "so whether you can identify a spine of tokens as a verb phrase or a noun phrase. Um, semantic information is basically anything you can get from WordNet like, uh,",
    "start": "4054720",
    "end": "4064395"
  },
  {
    "text": "what about the Semantic Role, Entailment, Concreteness, the Coref, Sentiment, Relation, Extraction.",
    "start": "4064395",
    "end": "4073500"
  },
  {
    "text": "Um, then the next sort of thing you can probe for is like local and long-range dependencies.",
    "start": "4073500",
    "end": "4079470"
  },
  {
    "text": "Like if- if you were to take the task of dependency labeling, um, given two tokens which are,",
    "start": "4079470",
    "end": "4086160"
  },
  {
    "text": "um, like two words apart and given two tokens which are like ten words apart,",
    "start": "4086160",
    "end": "4091319"
  },
  {
    "text": "can you still identify the dependencies? Then, um, that would be like a fine-grained experiment to check",
    "start": "4091320",
    "end": "4097920"
  },
  {
    "text": "whether like long-range dependencies are actually encoded in these contextualized word embeddings.",
    "start": "4097920",
    "end": "4103455"
  },
  {
    "text": "Um, I guess- so most of my talk was more about like word embeddings,",
    "start": "4103455",
    "end": "4110670"
  },
  {
    "text": "probing them, sentence embeddings, and probing them, but this is something which needs to be applied or can be applied to any of the,",
    "start": "4110670",
    "end": "4118270"
  },
  {
    "text": "uh, any of the NLP tasks. Uh, like I, I guess, coming back to the word representations,",
    "start": "4118270",
    "end": "4124819"
  },
  {
    "text": "you are trying to see what sort of information is encoded in these embeddings, but another space is what sort of information is not.",
    "start": "4124820",
    "end": "4131464"
  },
  {
    "text": "Um, you're trying to analyze like what sort of information does it encode more? Is it syntactic or semantic?",
    "start": "4131465",
    "end": "4137984"
  },
  {
    "text": "Um, if it was language modeling, uh, you can have different objectives for language modeling,",
    "start": "4137985",
    "end": "4143819"
  },
  {
    "text": "but it's interesting to see how, um, if you were to encode a sentence using a language model,",
    "start": "4143820",
    "end": "4149909"
  },
  {
    "text": "how does that representation vary with the different objective? Um, then you're- you're- so given a language- okay,",
    "start": "4149910",
    "end": "4160634"
  },
  {
    "text": "if I were to take an LSTM as a language model, and then I had a bunch of layers in my LSTM,",
    "start": "4160635",
    "end": "4166005"
  },
  {
    "text": "would each of these layers encode the same sort of information, or different information? Um, how do I effectively, ah,",
    "start": "4166005",
    "end": "4173400"
  },
  {
    "text": "transfer this sort of information from my encoder to my downstream task? So like, there's this notion of that lower layers encoding more syntactic information,",
    "start": "4173400",
    "end": "4183960"
  },
  {
    "text": "and higher layers en- encoding more semantic information. There's- there's also this paper which talks about you want to do like, okay,",
    "start": "4183960",
    "end": "4192779"
  },
  {
    "text": "so ELMo embeddings has a bunch of layers, and the same sort of norm follows there that the lower levels are more syntactic,",
    "start": "4192780",
    "end": "4200340"
  },
  {
    "text": "and the higher levels are semantic. But effectively- if you wanna improve on downstream tasks, if you want to use them as out of the box,",
    "start": "4200340",
    "end": "4207375"
  },
  {
    "text": "then a weighted average of these embeddings would be a nicer way than just using one of the layer's embeddings.",
    "start": "4207375",
    "end": "4214770"
  },
  {
    "text": "So probing what layer encodes what sort of information, and, um, I think this is interesting.",
    "start": "4214770",
    "end": "4221460"
  },
  {
    "text": "So if you were doing a task of say, um, machine translation, then you can see how your Seq2Seq model objectives can be changed,",
    "start": "4221460",
    "end": "4228750"
  },
  {
    "text": "and how this normal encoder would, uh, differ in properties based on the objectives.",
    "start": "4228750",
    "end": "4235079"
  },
  {
    "text": "And I guess even just not like- even- not even linguistic properties,",
    "start": "4235080",
    "end": "4241620"
  },
  {
    "text": "but any if- if you're training like a deep neural architecture, um, sure, bigger seems to be better,",
    "start": "4241620",
    "end": "4249090"
  },
  {
    "text": "but you need to- you need to have interpretable models. You need to understand why models are doing the way they are,",
    "start": "4249090",
    "end": "4255270"
  },
  {
    "text": "and this is a good place to start off be- by seeing what sort of linguistic information do they capture?",
    "start": "4255270",
    "end": "4260489"
  },
  {
    "text": "Is it just based on huge amounts of data or is it some inherent property of the architecture?",
    "start": "4260490",
    "end": "4267375"
  },
  {
    "text": "Uh, so I guess I just wanna finish up by saying, there's a lot of work which can- is being done and",
    "start": "4267375",
    "end": "4274560"
  },
  {
    "text": "can be done in this whole probing style architecture, because it'll be a trade-off between what sort of task do you wanna probe?",
    "start": "4274560",
    "end": "4280800"
  },
  {
    "text": "What sort of architecture do you wanna- um, do you wanna use, and what sort of hypotheses do you wanna test on?",
    "start": "4280800",
    "end": "4287190"
  },
  {
    "text": "Um, [NOISE] Again, as I mentioned, [NOISE] there's a lot of interp- like there's",
    "start": "4287190",
    "end": "4292320"
  },
  {
    "text": "a lot of need for interpretability of models. So this will be a nice place to explore especially for your final projects to",
    "start": "4292320",
    "end": "4298500"
  },
  {
    "text": "see- given that you've trained your model to improve on some sort of evaluation metric, but does it- does it",
    "start": "4298500",
    "end": "4306150"
  },
  {
    "text": "do just the objective it's been trained on or is it doing something else as well? Um, then keep probing.",
    "start": "4306150",
    "end": "4313905"
  },
  {
    "text": "Uh, yeah. Thank you. Ah, do you have any questions before- Yes, sorry.",
    "start": "4313905",
    "end": "4322440"
  },
  {
    "text": "Where that, um, like the random initialization-",
    "start": "4322440",
    "end": "4327780"
  },
  {
    "text": "Yeah- yeah. -can it give you information? Like why would that be? Uh, so I haven't- I haven't,",
    "start": "4327780",
    "end": "4336059"
  },
  {
    "text": "uh, read much about it, but it does. So randomly ini- initialized neural networks",
    "start": "4336060",
    "end": "4343170"
  },
  {
    "text": "do see- do serve as a strong baseline at least, um, as opposed to like a randomly initialized vector.",
    "start": "4343170",
    "end": "4351015"
  },
  {
    "text": "Like if I wanted to see- and they- actually they do- for some of the probing task, um,",
    "start": "4351015",
    "end": "4356685"
  },
  {
    "text": "that regard to contextualized word embedding, uh, getting an embedding from this randomly initialized encoder,",
    "start": "4356685",
    "end": "4362400"
  },
  {
    "text": "does on par with like GloVe embeddings, which is very interesting. So it- it, ah,",
    "start": "4362400",
    "end": "4368040"
  },
  {
    "text": "as to why it is happening that way? I- [NOISE] I'm not too sure about it maybe, uh,",
    "start": "4368040",
    "end": "4374715"
  },
  {
    "text": "maybe we- maybe we can talk about more after the lecture, but, um, yeah.",
    "start": "4374715",
    "end": "4380980"
  },
  {
    "text": "Any other- yes. Is there, uh, also research on like probing",
    "start": "4381470",
    "end": "4388199"
  },
  {
    "text": "or interpretability of like non-contextualized embeddings?",
    "start": "4388200",
    "end": "4396030"
  },
  {
    "text": "Like just GloVe or look. So you can't probe",
    "start": "4396030",
    "end": "4402330"
  },
  {
    "text": "GloVe for what sort of information as a thing encoded because if you look at GloVe, I think there is some research on- that GloVe is",
    "start": "4402330",
    "end": "4409260"
  },
  {
    "text": "inherently encoding some form of frequency, like based on whatever corpus it is being trained on.",
    "start": "4409260",
    "end": "4415305"
  },
  {
    "text": "It does inherently encode like the unigram probabilities like the- so frequency is one of the properties which is being encoded,",
    "start": "4415305",
    "end": "4422219"
  },
  {
    "text": "but I guess you can- you could actually do a trial for different properties as well. Does this answer your question? Yeah. Did you have a question? Yeah.",
    "start": "4422220",
    "end": "4432960"
  },
  {
    "text": "I was gonna ask, you mentioned the infer for ELMo. Yeah. I just wondered if you can say anything more",
    "start": "4432960",
    "end": "4438540"
  },
  {
    "text": "like type of information is that's encoded by it. things like more recent models.",
    "start": "4438540",
    "end": "4445000"
  },
  {
    "text": "Uh, so if- from my understanding, uh, I think ELMo specifically does encode more syntactic information than semantic.",
    "start": "4446300",
    "end": "4455355"
  },
  {
    "text": "Like it does poorly on co-reference resolution and semantic role labeling, those sorts of tasks.",
    "start": "4455355",
    "end": "4461940"
  },
  {
    "text": "But syntactic information, it does do better. Um, but again, if you do look at syntactic task,",
    "start": "4461940",
    "end": "4469980"
  },
  {
    "text": "I think like POS tagging, that is a relatively easier task. So I think even GloVe does well on that task.",
    "start": "4469980",
    "end": "4478020"
  },
  {
    "text": "And so it's hard to say what specifically is being encoded, but this is definitely an active field of research at the moment from what I've read.",
    "start": "4478020",
    "end": "4486614"
  },
  {
    "text": "Syntactical information is something which is encoded a lot, and long-range dependencies is something which is encoded,",
    "start": "4486615",
    "end": "4492000"
  },
  {
    "text": "but semantic little bit. Any other questions? Okay, thank you.",
    "start": "4492000",
    "end": "4501820"
  },
  {
    "text": "Thank you. [APPLAUSE]",
    "start": "4503420",
    "end": "4513000"
  }
]