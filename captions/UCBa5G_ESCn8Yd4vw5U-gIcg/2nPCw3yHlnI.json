[
  {
    "text": "Welcome, uh, to the class.",
    "start": "4040",
    "end": "6765"
  },
  {
    "text": "Uh, today we are going to talk about scaling up,",
    "start": "6765",
    "end": "10155"
  },
  {
    "text": "uh, graph neural networks.",
    "start": "10155",
    "end": "11669"
  },
  {
    "text": "Uh, and in particular, we will be interested in how can we build, uh,",
    "start": "11669",
    "end": "15809"
  },
  {
    "text": "graph neural network models that can be applied, uh, to large, uh,",
    "start": "15810",
    "end": "19680"
  },
  {
    "text": "scale graphs, and what kind of, um,",
    "start": "19680",
    "end": "22320"
  },
  {
    "text": "techniques, uh, can we use,",
    "start": "22320",
    "end": "24045"
  },
  {
    "text": "uh, to be able to do that.",
    "start": "24045",
    "end": "25800"
  },
  {
    "text": "So, uh, let's first, uh,",
    "start": "25800",
    "end": "27990"
  },
  {
    "text": "kind of define and, uh, motivate the problem.",
    "start": "27990",
    "end": "31320"
  },
  {
    "text": "So, uh, there are large graphs in many modern applications.",
    "start": "31320",
    "end": "36620"
  },
  {
    "text": "Um, if you think about recommender systems, for example, uh,",
    "start": "36620",
    "end": "40579"
  },
  {
    "text": "systems that recommend you products at Amazon,",
    "start": "40580",
    "end": "43280"
  },
  {
    "text": "systems that recommend you mov- videos to watch on YouTube,",
    "start": "43280",
    "end": "46760"
  },
  {
    "text": "uh, pins to look at at Pinterest,",
    "start": "46760",
    "end": "49070"
  },
  {
    "text": "posts to examine at Instagram,",
    "start": "49070",
    "end": "51290"
  },
  {
    "text": "and so on, um,",
    "start": "51290",
    "end": "52455"
  },
  {
    "text": "you can think of these as a- as a- as a way where we wanna connect users, uh,",
    "start": "52455",
    "end": "57200"
  },
  {
    "text": "to the content, products, videos that,",
    "start": "57200",
    "end": "60110"
  },
  {
    "text": "uh, they might be interested in, right?",
    "start": "60110",
    "end": "62480"
  },
  {
    "text": "And, uh, you can think of this recommendation task,",
    "start": "62480",
    "end": "65360"
  },
  {
    "text": "um, as a way to, uh,",
    "start": "65360",
    "end": "66980"
  },
  {
    "text": "do link prediction, uh, in this, uh,",
    "start": "66980",
    "end": "69590"
  },
  {
    "text": "large bipartite graph where you have, you know,",
    "start": "69590",
    "end": "72710"
  },
  {
    "text": "hundreds of millions, billions of users on one end,",
    "start": "72710",
    "end": "76174"
  },
  {
    "text": "on the other side you have tens of millions,",
    "start": "76175",
    "end": "78950"
  },
  {
    "text": "hundreds of millions, tens of billions of items, uh,",
    "start": "78950",
    "end": "83430"
  },
  {
    "text": "on the other end, and you would like to predict what,",
    "start": "83430",
    "end": "86430"
  },
  {
    "text": "uh, what, uh, mo- what movie,",
    "start": "86430",
    "end": "88320"
  },
  {
    "text": "what video, or what product, uh,",
    "start": "88320",
    "end": "89865"
  },
  {
    "text": "is, uh, interesting, uh,",
    "start": "89865",
    "end": "91604"
  },
  {
    "text": "for what, uh, user,",
    "start": "91605",
    "end": "93165"
  },
  {
    "text": "and you can formulate this as a link prediction, uh, task.",
    "start": "93165",
    "end": "97515"
  },
  {
    "text": "Another example, uh, where,",
    "start": "97515",
    "end": "100140"
  },
  {
    "text": "uh, graph neural networks can be very, uh,",
    "start": "100140",
    "end": "102270"
  },
  {
    "text": "uh, well applied is in social networks,",
    "start": "102270",
    "end": "104820"
  },
  {
    "text": "for example, uh, Twitter, uh, Facebook, uh,",
    "start": "104820",
    "end": "107460"
  },
  {
    "text": "Instagram, where you have users and friend and follow relations,",
    "start": "107460",
    "end": "111800"
  },
  {
    "text": "and you want- wanna ask about how do I wanna make,",
    "start": "111800",
    "end": "114890"
  },
  {
    "text": "uh, recommend, uh, friends to each other at the link level?",
    "start": "114890",
    "end": "117979"
  },
  {
    "text": "Uh, do I wanna do some kind of user property prediction, for example,",
    "start": "117980",
    "end": "121910"
  },
  {
    "text": "uh, predict what ads the different users would be interested in or predict what, uh,",
    "start": "121910",
    "end": "127165"
  },
  {
    "text": "country they come from,",
    "start": "127165",
    "end": "128600"
  },
  {
    "text": "or perhaps I'm doing some kind of attribute imputation in a sense that maybe some-",
    "start": "128600",
    "end": "133330"
  },
  {
    "text": "some users tell me their gender but I don't know",
    "start": "133330",
    "end": "136340"
  },
  {
    "text": "the gender of other users and I wanna predict it.",
    "start": "136340",
    "end": "139385"
  },
  {
    "text": "Um, and again, in this case,",
    "start": "139385",
    "end": "141049"
  },
  {
    "text": "these networks have, uh,",
    "start": "141050",
    "end": "142685"
  },
  {
    "text": "billions of, uh, users and tens to hundreds of billions, uh, of edges.",
    "start": "142685",
    "end": "148150"
  },
  {
    "text": "And then, uh, another application I- you know that can be motivated",
    "start": "148150",
    "end": "152929"
  },
  {
    "text": "by what we are going to talk about today is- is a notion of heterogeneous graphs.",
    "start": "152929",
    "end": "157360"
  },
  {
    "text": "And, for example, you can- you can- one such, uh,",
    "start": "157360",
    "end": "159975"
  },
  {
    "text": "dataset is called Microsoft Academic Graph,",
    "start": "159975",
    "end": "163005"
  },
  {
    "text": "which is a set of 120 million papers,",
    "start": "163005",
    "end": "166129"
  },
  {
    "text": "120 million authors, as well as their affiliations,",
    "start": "166129",
    "end": "169925"
  },
  {
    "text": "institutions, um, and so on.",
    "start": "169925",
    "end": "172130"
  },
  {
    "text": "So, uh, this means now we have a giant heterogenous, uh,",
    "start": "172130",
    "end": "175830"
  },
  {
    "text": "knowledge graph on which you can try to do a lot of interesting machine learning tasks.",
    "start": "175830",
    "end": "179990"
  },
  {
    "text": "For example, a paper categorization which will be- predict what category,",
    "start": "179990",
    "end": "184340"
  },
  {
    "text": "what topic the paper is about, uh,",
    "start": "184340",
    "end": "186440"
  },
  {
    "text": "recommend collaborators to authors,",
    "start": "186440",
    "end": "188900"
  },
  {
    "text": "uh, predict, uh, what papers should cite each other.",
    "start": "188900",
    "end": "192140"
  },
  {
    "text": "You can imagine this to be very useful when you are writing,",
    "start": "192140",
    "end": "195319"
  },
  {
    "text": "uh, a new paper or doing a new piece of, uh, research.",
    "start": "195320",
    "end": "198350"
  },
  {
    "text": "And these are again, uh,",
    "start": "198350",
    "end": "199580"
  },
  {
    "text": "machine learning tasks over this giant heterogeneous graph.",
    "start": "199580",
    "end": "203485"
  },
  {
    "text": "And more broadly, right,",
    "start": "203485",
    "end": "205500"
  },
  {
    "text": "we can think about, uh, knowledge graphs,",
    "start": "205500",
    "end": "207615"
  },
  {
    "text": "the knowledge graphs coming out of Wikipedia,",
    "start": "207615",
    "end": "210155"
  },
  {
    "text": "coming out of, uh, Freebase,",
    "start": "210155",
    "end": "212060"
  },
  {
    "text": "where again we have, um,",
    "start": "212060",
    "end": "213635"
  },
  {
    "text": "100 million entities and we wanna do knowledge graph completion tasks or we wanna do,",
    "start": "213635",
    "end": "218629"
  },
  {
    "text": "uh, knowledge reasoning tasks, uh,",
    "start": "218630",
    "end": "221120"
  },
  {
    "text": "over these, uh, uh,",
    "start": "221120",
    "end": "222485"
  },
  {
    "text": "over these knowledge graphs.",
    "start": "222485",
    "end": "223910"
  },
  {
    "text": "And, you know, what all these, uh, um,",
    "start": "223910",
    "end": "226460"
  },
  {
    "text": "applications have in common is that they are large scale, right?",
    "start": "226460",
    "end": "229280"
  },
  {
    "text": "That number of nodes is in millions to billions,",
    "start": "229280",
    "end": "232290"
  },
  {
    "text": "and, you know, the number of soft edges is, I know,",
    "start": "232290",
    "end": "235040"
  },
  {
    "text": "tens- tens or hundreds of millions to,",
    "start": "235040",
    "end": "238370"
  },
  {
    "text": "uh, uh, or, uh, to tens- to,",
    "start": "238370",
    "end": "241129"
  },
  {
    "text": "uh, hundreds of billions.",
    "start": "241130",
    "end": "242570"
  },
  {
    "text": "And the question is, right,",
    "start": "242570",
    "end": "244070"
  },
  {
    "text": "like if we have these types of large-scale graph,",
    "start": "244070",
    "end": "246200"
  },
  {
    "text": "these large- this set of large-scale data,",
    "start": "246200",
    "end": "248750"
  },
  {
    "text": "and we would like to do, um,",
    "start": "248750",
    "end": "250745"
  },
  {
    "text": "tasks in- in all these cases that I have motivated,",
    "start": "250745",
    "end": "253730"
  },
  {
    "text": "we can do kind of node-level tasks in terms of user, item,",
    "start": "253730",
    "end": "257449"
  },
  {
    "text": "paper classification prediction as well as, uh,",
    "start": "257450",
    "end": "261375"
  },
  {
    "text": "pair-wise level tasks like, uh, link, uh,",
    "start": "261375",
    "end": "264240"
  },
  {
    "text": "recommendation, uh, uh, link prediction, and so on.",
    "start": "264240",
    "end": "268785"
  },
  {
    "text": "Um, and, er, the topic of today's class will be about how do",
    "start": "268785",
    "end": "272570"
  },
  {
    "text": "we scale up GNNs to- to graphs of this- of this size, right?",
    "start": "272570",
    "end": "276140"
  },
  {
    "text": "What kind of systems,",
    "start": "276140",
    "end": "277280"
  },
  {
    "text": "what kind of algorithms can be developed that will allow us to learn or over,",
    "start": "277280",
    "end": "281870"
  },
  {
    "text": "uh, these kind of, uh,",
    "start": "281870",
    "end": "283294"
  },
  {
    "text": "massive amounts, uh, of data?",
    "start": "283295",
    "end": "285875"
  },
  {
    "text": "So let me explain why is this hard or why is this non-trivial, um,",
    "start": "285875",
    "end": "291920"
  },
  {
    "text": "and it will become clearly- very quickly clear that we need,",
    "start": "291920",
    "end": "295160"
  },
  {
    "text": "uh, special methods and special approaches, right?",
    "start": "295160",
    "end": "298180"
  },
  {
    "text": "When we have a big, uh, dataset,",
    "start": "298180",
    "end": "300509"
  },
  {
    "text": "let just think in general- like in a general deep-learning framework,",
    "start": "300510",
    "end": "304280"
  },
  {
    "text": "when we have a large number of N data points,",
    "start": "304280",
    "end": "307280"
  },
  {
    "text": "uh, what do we wanna do, right?",
    "start": "307280",
    "end": "309010"
  },
  {
    "text": "In- in general, our objective is to minimize the- the- the loss,",
    "start": "309010",
    "end": "313415"
  },
  {
    "text": "the average loss, uh,",
    "start": "313415",
    "end": "314975"
  },
  {
    "text": "over the training data.",
    "start": "314975",
    "end": "316880"
  },
  {
    "text": "So if I have, uh,",
    "start": "316880",
    "end": "317950"
  },
  {
    "text": "N training data points, then, you know,",
    "start": "317950",
    "end": "319910"
  },
  {
    "text": "I go- I have the summation from 0 to n minus 1 where for every,",
    "start": "319910",
    "end": "324600"
  },
  {
    "text": "um, data point i, I'm asking what is the loss of it, right?",
    "start": "324600",
    "end": "328980"
  },
  {
    "text": "What is the discrepancy between the true label,",
    "start": "328980",
    "end": "331280"
  },
  {
    "text": "the true prediction, and the,",
    "start": "331280",
    "end": "333290"
  },
  {
    "text": "um, uh, uh, pre- uh, and the- and the,",
    "start": "333290",
    "end": "335510"
  },
  {
    "text": "um, and the prediction made by the model?",
    "start": "335510",
    "end": "338240"
  },
  {
    "text": "And- and I would like to compute this total loss, um,",
    "start": "338240",
    "end": "341750"
  },
  {
    "text": "under the current model parameters, uh,",
    "start": "341750",
    "end": "344135"
  },
  {
    "text": "Theta, so that then as I can compute the loss,",
    "start": "344135",
    "end": "347840"
  },
  {
    "text": "I can then compute the gradient with respect to the loss and update,",
    "start": "347840",
    "end": "352580"
  },
  {
    "text": "uh, model parameters in such a way that the total loss will be minimized,",
    "start": "352580",
    "end": "357275"
  },
  {
    "text": "that the- that the- so basically,",
    "start": "357275",
    "end": "359440"
  },
  {
    "text": "the model parameters will change,",
    "start": "359440",
    "end": "361010"
  },
  {
    "text": "which means model predictions should change such- such that the loss,",
    "start": "361010",
    "end": "365195"
  },
  {
    "text": "the discrepancy between the true labels and the predicted labels,",
    "start": "365195",
    "end": "368630"
  },
  {
    "text": "um, gets, uh, smaller.",
    "start": "368630",
    "end": "370580"
  },
  {
    "text": "And notice what is the point here,",
    "start": "370580",
    "end": "372830"
  },
  {
    "text": "the point here is that we have to sum up",
    "start": "372830",
    "end": "374750"
  },
  {
    "text": "these loss contributions over all the training data points.",
    "start": "374750",
    "end": "378985"
  },
  {
    "text": "Right? Um, and, uh, because, uh,",
    "start": "378985",
    "end": "382094"
  },
  {
    "text": "the number of da- training data points is too big,",
    "start": "382095",
    "end": "384830"
  },
  {
    "text": "what we then- what we then do is, um,",
    "start": "384830",
    "end": "387409"
  },
  {
    "text": "we would actually select small groups of M data points called the mini-batches and then",
    "start": "387410",
    "end": "393305"
  },
  {
    "text": "approximate this big summation with",
    "start": "393305",
    "end": "395810"
  },
  {
    "text": "smaller summations where i only goes over the points in the mini-batch,",
    "start": "395810",
    "end": "400385"
  },
  {
    "text": "right, uh, over, I know,",
    "start": "400385",
    "end": "401840"
  },
  {
    "text": "100, 1,000, uh, uh,",
    "start": "401840",
    "end": "403955"
  },
  {
    "text": "data points we have sampled and then",
    "start": "403955",
    "end": "406250"
  },
  {
    "text": "compute the loss over it and then make a gradient step.",
    "start": "406250",
    "end": "409250"
  },
  {
    "text": "This means that our, uh,",
    "start": "409250",
    "end": "410930"
  },
  {
    "text": "loss, our gradient will be- will be stochastic,",
    "start": "410930",
    "end": "414139"
  },
  {
    "text": "it will be imprecise, it will be a bit random because it will depend on what exact, uh,",
    "start": "414140",
    "end": "419360"
  },
  {
    "text": "subset, uh, of random points N have we compute- have we selected,",
    "start": "419360",
    "end": "425060"
  },
  {
    "text": "but it will be much faster to compute because M,",
    "start": "425060",
    "end": "428450"
  },
  {
    "text": "the size of the mini-batch,",
    "start": "428450",
    "end": "429815"
  },
  {
    "text": "will be much smaller than the total amount, uh, of data,",
    "start": "429815",
    "end": "432610"
  },
  {
    "text": "so our model will,",
    "start": "432610",
    "end": "434064"
  },
  {
    "text": "um, converge, uh, faster.",
    "start": "434065",
    "end": "436535"
  },
  {
    "text": "Now, what could you do in graph neural networks?",
    "start": "436535",
    "end": "439770"
  },
  {
    "text": "You would simply say, if let's say I'm doing a node-level prediction task,",
    "start": "439770",
    "end": "443444"
  },
  {
    "text": "let me subsample a set of nodes N,",
    "start": "443445",
    "end": "446865"
  },
  {
    "text": "uh, and put them in a mini-batch and let me learn, uh, over them.",
    "start": "446865",
    "end": "450735"
  },
  {
    "text": "Um, and, uh, here is the reason why this,",
    "start": "450735",
    "end": "453729"
  },
  {
    "text": "for example, won't work in graph neural networks.",
    "start": "453730",
    "end": "456890"
  },
  {
    "text": "Right, imagine that in a mini-batch I simply sample some set M of nodes, uh, independently.",
    "start": "456890",
    "end": "463675"
  },
  {
    "text": "And what this picture is trying to show,",
    "start": "463675",
    "end": "465780"
  },
  {
    "text": "is these are the nodes I'm going to sample.",
    "start": "465780",
    "end": "468210"
  },
  {
    "text": "And because, eh, the- the mini-batch is",
    "start": "468210",
    "end": "470925"
  },
  {
    "text": "much smaller than the total size- size of the graph,",
    "start": "470925",
    "end": "473970"
  },
  {
    "text": "what is going to happen, is that,",
    "start": "473970",
    "end": "475500"
  },
  {
    "text": "yes, I'm going to sample these nodes,",
    "start": "475500",
    "end": "477420"
  },
  {
    "text": "but the way graph neural networks operate is that",
    "start": "477420",
    "end": "479760"
  },
  {
    "text": "they aggregate information from the neighbors,",
    "start": "479760",
    "end": "482330"
  },
  {
    "text": "but most likely none or majority of this name neighbors won't be sampled in our,",
    "start": "482330",
    "end": "488560"
  },
  {
    "text": "uh, in our mini-batch.",
    "start": "488560",
    "end": "490330"
  },
  {
    "text": "So, um, what this means,",
    "start": "490330",
    "end": "492220"
  },
  {
    "text": "is that the nodes in a mini-batch will tend to be isolated nodes from each other.",
    "start": "492220",
    "end": "496210"
  },
  {
    "text": "And because the GNN generates node embeddings by aggregating, um,",
    "start": "496210",
    "end": "500854"
  },
  {
    "text": "information from the neighbors, uh,",
    "start": "500855",
    "end": "502740"
  },
  {
    "text": "in the graph, uh,",
    "start": "502740",
    "end": "504074"
  },
  {
    "text": "those neighbors won't be part of the- um,",
    "start": "504075",
    "end": "506535"
  },
  {
    "text": "of the mini-batch, so there will be nothing to, uh,",
    "start": "506535",
    "end": "509670"
  },
  {
    "text": "aggregate and whatever gradient we compute over",
    "start": "509670",
    "end": "512760"
  },
  {
    "text": "isolated nodes won't be representative of the- of the full gradient.",
    "start": "512760",
    "end": "517890"
  },
  {
    "text": "So this means that the stochastic gradient descent can- can,",
    "start": "517890",
    "end": "522010"
  },
  {
    "text": "uh, won't be able to train effectively on graph neural networks.",
    "start": "522010",
    "end": "525415"
  },
  {
    "text": "If we do this naive mini-batch implementation, where we simply,",
    "start": "525415",
    "end": "529509"
  },
  {
    "text": "uh, select a- a sub-sample of nodes,",
    "start": "529510",
    "end": "532660"
  },
  {
    "text": "as I illustrated here,",
    "start": "532660",
    "end": "534144"
  },
  {
    "text": "because nodes in the mini-batch won't be connected with each other,",
    "start": "534145",
    "end": "537745"
  },
  {
    "text": "so there won't be any message passing to be able,",
    "start": "537745",
    "end": "540790"
  },
  {
    "text": "uh- uh to be able to be done between the neighbors,",
    "start": "540790",
    "end": "543339"
  },
  {
    "text": "and the - the entire thing,",
    "start": "543340",
    "end": "544960"
  },
  {
    "text": "uh, is going to fail.",
    "start": "544960",
    "end": "546730"
  },
  {
    "text": "So, um, another way how we could, uh,",
    "start": "546730",
    "end": "549954"
  },
  {
    "text": "do this is to say,",
    "start": "549955",
    "end": "551185"
  },
  {
    "text": "let's forget mini batching,",
    "start": "551185",
    "end": "552985"
  },
  {
    "text": "let's not select a subset of the training data, uh,",
    "start": "552985",
    "end": "556450"
  },
  {
    "text": "to compute the gradient on,",
    "start": "556450",
    "end": "557950"
  },
  {
    "text": "let's do what is called full-batch implementation, full-batch training.",
    "start": "557950",
    "end": "561895"
  },
  {
    "text": "And the problem with full batch training is the following,",
    "start": "561895",
    "end": "564925"
  },
  {
    "text": "is that you can think of it that we generate",
    "start": "564925",
    "end": "567700"
  },
  {
    "text": "embeddings for all the nodes at the same time.",
    "start": "567700",
    "end": "571375"
  },
  {
    "text": "And the important thing here is- to notice that we",
    "start": "571375",
    "end": "574420"
  },
  {
    "text": "have embeddings of the nodes at several layers, right?",
    "start": "574420",
    "end": "577450"
  },
  {
    "text": "We start at layer 0,",
    "start": "577450",
    "end": "578965"
  },
  {
    "text": "which means the embedding of the node is just the node features,",
    "start": "578965",
    "end": "582040"
  },
  {
    "text": "then we propagate through the first layer",
    "start": "582040",
    "end": "584199"
  },
  {
    "text": "and we get the embeddings of the node at layer 1.",
    "start": "584200",
    "end": "586870"
  },
  {
    "text": "Now, to compute the embeddings of the node from layer 2,",
    "start": "586870",
    "end": "589945"
  },
  {
    "text": "we have to aggregate the first layer embeddings",
    "start": "589945",
    "end": "592240"
  },
  {
    "text": "to compute the second layer, uh, embeddings.",
    "start": "592240",
    "end": "594774"
  },
  {
    "text": "So what this means, is that you would first,",
    "start": "594775",
    "end": "597235"
  },
  {
    "text": "uh, load the graph in memory and all the features,",
    "start": "597235",
    "end": "600040"
  },
  {
    "text": "and then at each layer of the GNN,",
    "start": "600040",
    "end": "601990"
  },
  {
    "text": "you would compute the embeddings of all the nodes using",
    "start": "601990",
    "end": "604839"
  },
  {
    "text": "the embeddings of all the nodes from the previous layer, right?",
    "start": "604840",
    "end": "608580"
  },
  {
    "text": "So that now if I'm computing the layer K,",
    "start": "608580",
    "end": "610890"
  },
  {
    "text": "I need to have all the em- embeddings of all the nodes, um,",
    "start": "610890",
    "end": "614475"
  },
  {
    "text": "stored for layer K minus 1,",
    "start": "614475",
    "end": "616904"
  },
  {
    "text": "so that whenever I'm creating the next layer embedding of a given node,",
    "start": "616905",
    "end": "620475"
  },
  {
    "text": "I check who its neighbors are.",
    "start": "620475",
    "end": "622785"
  },
  {
    "text": "I take the previous layer embeddings, uh,",
    "start": "622785",
    "end": "625170"
  },
  {
    "text": "for those neighbors and aggregate them to create the next layer embedding.",
    "start": "625170",
    "end": "628935"
  },
  {
    "text": "And because of this recursive structure, I basically create,",
    "start": "628935",
    "end": "632820"
  },
  {
    "text": "um- I need to be able to keep in memory the",
    "start": "632820",
    "end": "636820"
  },
  {
    "text": "entire graph plus the embeddings of all the nodes from the previous layer,",
    "start": "636820",
    "end": "641080"
  },
  {
    "text": "as well as enough memory to create the embedding so for the nodes,",
    "start": "641080",
    "end": "645430"
  },
  {
    "text": "uh, for the next layer.",
    "start": "645430",
    "end": "646690"
  },
  {
    "text": "Uh, and then once I have that,",
    "start": "646690",
    "end": "648610"
  },
  {
    "text": "I can then compute the loss and I can compute the, perform the gradient descent.",
    "start": "648610",
    "end": "652240"
  },
  {
    "text": "So the way you can think of this is,",
    "start": "652240",
    "end": "653815"
  },
  {
    "text": "I start with the initial embeddings of the nodes,",
    "start": "653815",
    "end": "656845"
  },
  {
    "text": "I perform message-passing, create the embeddings of the nodes at layer K plus 1.",
    "start": "656845",
    "end": "662529"
  },
  {
    "text": "Now I put this, uh,",
    "start": "662530",
    "end": "663985"
  },
  {
    "text": "here and call this layer K, again,",
    "start": "663985",
    "end": "666370"
  },
  {
    "text": "do the message-passing to get to the next, uh, layer.",
    "start": "666370",
    "end": "670480"
  },
  {
    "text": "And this- this would be what is called a full-batch implementation of",
    "start": "670480",
    "end": "675625"
  },
  {
    "text": "a graph neural network.",
    "start": "675625",
    "end": "677215"
  },
  {
    "text": "Um, and why is full-batch implementation problematic?",
    "start": "677215",
    "end": "681100"
  },
  {
    "text": "Uh, it is problematic for two reasons.",
    "start": "681100",
    "end": "684459"
  },
  {
    "text": "One reason is that it takes, uh,",
    "start": "684460",
    "end": "686800"
  },
  {
    "text": "super long and it's kind of, uh, time-wise,",
    "start": "686800",
    "end": "689709"
  },
  {
    "text": "data-wise, inefficient to do full-batch training.",
    "start": "689710",
    "end": "692995"
  },
  {
    "text": "But imagine, you know,",
    "start": "692995",
    "end": "694120"
  },
  {
    "text": "you have all the time in the world,",
    "start": "694120",
    "end": "695620"
  },
  {
    "text": "so you say, I don't care if it takes a bit longer.",
    "start": "695620",
    "end": "698620"
  },
  {
    "text": "Um, even with that,",
    "start": "698620",
    "end": "700600"
  },
  {
    "text": "it is still impossible to implement a full batch training,",
    "start": "700600",
    "end": "705175"
  },
  {
    "text": "uh, for large, uh, for large graphs.",
    "start": "705175",
    "end": "707529"
  },
  {
    "text": "And the reason for this is because if we want the training to be,",
    "start": "707530",
    "end": "712120"
  },
  {
    "text": "uh- uh not- to be kind of, uh,",
    "start": "712120",
    "end": "714145"
  },
  {
    "text": "in the order of magnitude of what we are used today,",
    "start": "714145",
    "end": "717025"
  },
  {
    "text": "then we wanna use, um, GPUs.",
    "start": "717025",
    "end": "719110"
  },
  {
    "text": "So we wanna use um- uh,",
    "start": "719110",
    "end": "721795"
  },
  {
    "text": "graphic cards to do this,",
    "start": "721795",
    "end": "723610"
  },
  {
    "text": "right? For fast training.",
    "start": "723610",
    "end": "724959"
  },
  {
    "text": "But the GPU memory- GPUs have a very special kind of,",
    "start": "724960",
    "end": "728770"
  },
  {
    "text": "uh, super-fast, uh, memory,",
    "start": "728770",
    "end": "731050"
  },
  {
    "text": "but they have very little of it.",
    "start": "731050",
    "end": "732565"
  },
  {
    "text": "They have about 10-20 gigabytes, uh, of memory.",
    "start": "732565",
    "end": "736150"
  },
  {
    "text": "And I think the most you can buy today is around 30 gigabytes,",
    "start": "736150",
    "end": "739525"
  },
  {
    "text": "uh, bigger GPU cards.",
    "start": "739525",
    "end": "741130"
  },
  {
    "text": "And that's, you know, considered super state of the art.",
    "start": "741130",
    "end": "744115"
  },
  {
    "text": "Nobody has those graphics cards, right?",
    "start": "744115",
    "end": "746365"
  },
  {
    "text": "But the point is that the entire graphs and entire features,",
    "start": "746365",
    "end": "749410"
  },
  {
    "text": "if you have a billion nodes,",
    "start": "749410",
    "end": "750714"
  },
  {
    "text": "will be far, far bigger, right?",
    "start": "750715",
    "end": "752815"
  },
  {
    "text": "If you have a billion nodes,",
    "start": "752815",
    "end": "754525"
  },
  {
    "text": "um, and- and, uh, you just spend,",
    "start": "754525",
    "end": "756820"
  },
  {
    "text": "let's say one byte,",
    "start": "756820",
    "end": "758335"
  },
  {
    "text": "uh, um, uh, uh,",
    "start": "758335",
    "end": "760360"
  },
  {
    "text": "or do you have, I don't know, 100 bytes of, uh, features,",
    "start": "760360",
    "end": "762954"
  },
  {
    "text": "uh, per node, it will easily- you will need a terabyte of memory,",
    "start": "762954",
    "end": "766750"
  },
  {
    "text": "10 terabytes of, uh,",
    "start": "766750",
    "end": "767965"
  },
  {
    "text": "main memory, which is the amount of memory we",
    "start": "767965",
    "end": "770800"
  },
  {
    "text": "can get today for large servers using CPUs.",
    "start": "770800",
    "end": "774055"
  },
  {
    "text": "So the point is, CPUs have slow computation,",
    "start": "774055",
    "end": "777190"
  },
  {
    "text": "large amount of memory,",
    "start": "777190",
    "end": "778555"
  },
  {
    "text": "GPUs have very tiny amount of memory, but super-fast.",
    "start": "778555",
    "end": "782350"
  },
  {
    "text": "And the point is, if you wanna do full-batch,",
    "start": "782350",
    "end": "784735"
  },
  {
    "text": "you need to feed all the features, all the embeddings,",
    "start": "784735",
    "end": "788560"
  },
  {
    "text": "and the entire graph structure into the GPU memory,",
    "start": "788560",
    "end": "791935"
  },
  {
    "text": "and 10 gigabytes is- is- is- is- is nothing.",
    "start": "791935",
    "end": "795760"
  },
  {
    "text": "It's less than what your iPhone has memory.",
    "start": "795760",
    "end": "798130"
  },
  {
    "text": "So it is so small that we cannot scale,",
    "start": "798130",
    "end": "801145"
  },
  {
    "text": "uh, full-batch implementations beyond, uh,",
    "start": "801145",
    "end": "803650"
  },
  {
    "text": "a few thousand node, uh,",
    "start": "803650",
    "end": "805015"
  },
  {
    "text": "networks and definitely not to millions and not, uh, to billions.",
    "start": "805015",
    "end": "809470"
  },
  {
    "text": "So, um, this motivates the lecture for today, right?",
    "start": "809470",
    "end": "813220"
  },
  {
    "text": "So the lecture for today will be,",
    "start": "813220",
    "end": "814824"
  },
  {
    "text": "how can we change the way we think of graph neural networks?",
    "start": "814825",
    "end": "818620"
  },
  {
    "text": "How do we implement the training?",
    "start": "818620",
    "end": "820779"
  },
  {
    "text": "How do we, uh,",
    "start": "820780",
    "end": "822085"
  },
  {
    "text": "change the architecture so that we can scale them up,",
    "start": "822085",
    "end": "826045"
  },
  {
    "text": "uh, to, uh, large graphs of billions of nodes,",
    "start": "826045",
    "end": "829105"
  },
  {
    "text": "so that we can limit this issue with small, um,",
    "start": "829105",
    "end": "832990"
  },
  {
    "text": "GPU memory and that we can limit the issue that naive implementation of mini-batching,",
    "start": "832990",
    "end": "838360"
  },
  {
    "text": "uh, just doesn't work.",
    "start": "838360",
    "end": "840070"
  },
  {
    "text": "So I'm going to talk about two methods.",
    "start": "840070",
    "end": "842560"
  },
  {
    "text": "Uh, basically in this lecture,",
    "start": "842560",
    "end": "844180"
  },
  {
    "text": "I'm going to talk about three different methods,",
    "start": "844180",
    "end": "846399"
  },
  {
    "text": "two of them will be based on performing",
    "start": "846400",
    "end": "848740"
  },
  {
    "text": "message-passing over small subgraphs in each mini-batch.",
    "start": "848740",
    "end": "852399"
  },
  {
    "text": "So we are going to change how we design mini-batches.",
    "start": "852400",
    "end": "855505"
  },
  {
    "text": "Um, and this will lead to fast training time.",
    "start": "855505",
    "end": "858415"
  },
  {
    "text": "One- one is technique called neighborhood",
    "start": "858415",
    "end": "861160"
  },
  {
    "text": "sampling and the other one is technique called, uh, Cluster-GCN.",
    "start": "861160",
    "end": "865870"
  },
  {
    "text": "And then, uh, I'm also going to talk about the third method",
    "start": "865870",
    "end": "869755"
  },
  {
    "text": "which is called- which would be about simplifying the GCN, uh,",
    "start": "869755",
    "end": "873610"
  },
  {
    "text": "in such a way that we simplified the architecture so that, uh,",
    "start": "873610",
    "end": "877315"
  },
  {
    "text": "operations, uh, computation can be efficiently performed on a CPU.",
    "start": "877315",
    "end": "881770"
  },
  {
    "text": "And because CPU has a lot of memory,",
    "start": "881770",
    "end": "884005"
  },
  {
    "text": "we can scale, uh, to a large graph.",
    "start": "884005",
    "end": "886910"
  }
]