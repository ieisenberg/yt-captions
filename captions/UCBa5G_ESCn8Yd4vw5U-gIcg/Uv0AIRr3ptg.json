[
  {
    "start": "0",
    "end": "5410"
  },
  {
    "text": "SPEAKER: And so\ntoday I kind of just want to cover the\nfundamentals of PyTorch,",
    "start": "5410",
    "end": "10540"
  },
  {
    "text": "really just see what are the\nsimilarities between PyTorch and NumPy and Python, which you\nguys are used to at this point,",
    "start": "10540",
    "end": "18430"
  },
  {
    "text": "and see how we can build up\na lot of the building blocks that we'll need in order to\ndefine more complex models.",
    "start": "18430",
    "end": "25370"
  },
  {
    "text": "So specifically, we're going\nto talk today about tensors. What are tensor objects? How do we manipulate them?",
    "start": "25370",
    "end": "32439"
  },
  {
    "text": "What is autograd? How PyTorch helps us\ncompute different gradients? And finally, how we\nactually do optimization",
    "start": "32439",
    "end": "39610"
  },
  {
    "text": "and how we write the training\nloop for our neural networks. And if we have time\nat the end, then we'll",
    "start": "39610",
    "end": "45219"
  },
  {
    "text": "try and go through\na bit of a demo to put everything\ntogether and see how everything comes\ntogether when you want",
    "start": "45220",
    "end": "52480"
  },
  {
    "text": "to solve an actual NLP task. All right. So let's get started.",
    "start": "52480",
    "end": "58780"
  },
  {
    "text": "So if you go to the course\nwebsite, there is Notebook. And you can just make a\ncopy of this Colab notebook",
    "start": "58780",
    "end": "65610"
  },
  {
    "text": "and then just run\nthe cells as we go. And so to start, today\nwe're talking about PyTorch,",
    "start": "65610",
    "end": "72630"
  },
  {
    "text": "like I said. It's a deep learning\nframework that really does two main things. One, is it makes it\nvery easy to author",
    "start": "72630",
    "end": "79530"
  },
  {
    "text": "and manipulate tensors\nand make use of your GPU so that you can\nactually leverage a lot of that capability.",
    "start": "79530",
    "end": "86040"
  },
  {
    "text": "And two, is it makes\nthe process of authoring neural networks much simpler.",
    "start": "86040",
    "end": "91409"
  },
  {
    "text": "You can now use different\nbuilding blocks, like linear layers and\ndifferent loss functions",
    "start": "91410",
    "end": "96659"
  },
  {
    "text": "and compose them in different\nways in order to author the types of models that you\nneed for your specific use",
    "start": "96660",
    "end": "102570"
  },
  {
    "text": "cases. And so PyTorch is one of\nthe two main frameworks along with TensorFlow.",
    "start": "102570",
    "end": "108150"
  },
  {
    "text": "In this class, we'll\nfocus on PyTorch, but there are quite similar. And so we'll start\nby importing Torch",
    "start": "108150",
    "end": "114630"
  },
  {
    "text": "and we'll import\nthe neural network module, which is torch.mn.",
    "start": "114630",
    "end": "119990"
  },
  {
    "text": "And for this first\npart of the tutorial, I want to talk a\nbit about tensors. One thing that you guys\nare all familiar with now",
    "start": "119990",
    "end": "127700"
  },
  {
    "text": "is NumPy arrays. And so pretty much you can think\nabout tensors as the equivalent",
    "start": "127700",
    "end": "135080"
  },
  {
    "text": "in PyTorch to NumPy arrays. They're essentially\nmulti-dimensional arrays that you can manipulate\nin different ways.",
    "start": "135080",
    "end": "142140"
  },
  {
    "text": "And you'll essentially use\nthem to represent your data, to be able to actually\nmanipulate it, and perform",
    "start": "142140",
    "end": "149420"
  },
  {
    "text": "all the different\nmatrix operations that underlie your neural network. And so in this\ncase, for example,",
    "start": "149420",
    "end": "156900"
  },
  {
    "text": "if we're thinking of an image. One way you can think about\nit in terms of a tensor, is it's a 256 x\n256 tensor, where",
    "start": "156900",
    "end": "165890"
  },
  {
    "text": "it has a width of 256 pixels\nand a height of 256 pixels. And for instance, if we\nhave a batch of images,",
    "start": "165890",
    "end": "173180"
  },
  {
    "text": "and those images contain three\nchannels, like red, green, and blue, then we might have\na four-dimensional tensor,",
    "start": "173180",
    "end": "179890"
  },
  {
    "text": "which is the batch size\nby the number of channels by the width and the height.",
    "start": "179890",
    "end": "184900"
  },
  {
    "text": "And so everything we're\ngoing to see today is all going to be\nrepresented as tensors, which you can just think of\nas multi-dimensional arrays.",
    "start": "184900",
    "end": "193050"
  },
  {
    "text": "And so to kind of get\nsome intuition about this, we're going to spend a little\nbit of time going through,",
    "start": "193050",
    "end": "198450"
  },
  {
    "text": "essentially, lists of lists\nand how we can convert them into tensors and how\nwe can manipulate them",
    "start": "198450",
    "end": "205380"
  },
  {
    "text": "with different operations. So to start off with, we just\nhave a simple list of lists",
    "start": "205380",
    "end": "211320"
  },
  {
    "text": "that you're all familiar with. In this case, it's a 2x3 list. And now we want to\ncreate a tensor.",
    "start": "211320",
    "end": "219670"
  },
  {
    "text": "And so the way, we'll\ncreate this tensor is by doing\ntorch.tensor and then",
    "start": "219670",
    "end": "225599"
  },
  {
    "text": "essentially writing the same\nsyntax that we had before. Just write out the list\nof lists that represents",
    "start": "225600",
    "end": "232110"
  },
  {
    "text": "that particular tensor.  And so in this case, we\nget back a tensor object,",
    "start": "232110",
    "end": "239010"
  },
  {
    "text": "which is the same shape\nand contains the same data. And so now the second\nthing with the tensor",
    "start": "239010",
    "end": "245069"
  },
  {
    "text": "is that it contains a data type. So there's different data types. For instance, there are\ndifferent varying level",
    "start": "245070",
    "end": "251430"
  },
  {
    "text": "of precision floating point\nnumbers that you can use. You can have integers. You can have different\ndata types that",
    "start": "251430",
    "end": "257002"
  },
  {
    "text": "actually populate your tensor. And so by default, I believe\nthis will be float32,",
    "start": "257003",
    "end": "262140"
  },
  {
    "text": "but you can explicitly specify\nwhich data type your tensor is by passing in\nthe dtype argument.",
    "start": "262140",
    "end": "269220"
  },
  {
    "text": "And so we see here,\neven though we wrote in a bunch\nof integers, they have a decimal point, which\nindicates that they're",
    "start": "269220",
    "end": "276060"
  },
  {
    "text": "floating point numbers. And so same thing here. We can create another tensor,\nin this case with data type",
    "start": "276060",
    "end": "284479"
  },
  {
    "text": "float32. And in this third\nexample, you see",
    "start": "284480",
    "end": "289930"
  },
  {
    "text": "that we create another tensor. We don't actually\nspecify the data type, but PyTorch implicitly takes the\ndata type to be floating point",
    "start": "289930",
    "end": "299860"
  },
  {
    "text": "since we actually passed\nin a floating point number into this tensor. So pretty much at a\nhigh level, tensors",
    "start": "299860",
    "end": "308069"
  },
  {
    "text": "are like\nmulti-dimensional arrays. We can specify the\ndata type for them. We can populate them\njust like NumPy arrays.",
    "start": "308070",
    "end": "315150"
  },
  {
    "text": "So now we know how\nto create tensors. We know that ultimately,\neverything that we work with,",
    "start": "315150",
    "end": "320639"
  },
  {
    "text": "all the data we have is going\nto be expressed as tensors. Now the question is, what\nare the functions that we",
    "start": "320640",
    "end": "325890"
  },
  {
    "text": "have to manipulate them? And so we have some\nbasic utilities that can help us instantiate\ntensors easily, specifically",
    "start": "325890",
    "end": "334260"
  },
  {
    "text": "torch.0s and torch.1s. These are two ways to create\ntensors of a particular shape,",
    "start": "334260",
    "end": "342930"
  },
  {
    "text": "in this case, tensors of\nall 0s or tensors of all 1s. And you'll see that this\nwill be very helpful.",
    "start": "342930",
    "end": "349680"
  },
  {
    "text": "When you do your homeworks,\ntypically, you'll want to-- need to create a\nbunch of zero matrix.",
    "start": "349680",
    "end": "355530"
  },
  {
    "text": "And it'll be very easy to\njust specify the shape here without having to\nwrite everything out super explicitly.",
    "start": "355530",
    "end": "361230"
  },
  {
    "text": "And then you can update\nthat tensor as needed. Another thing you\ncan do is, just",
    "start": "361230",
    "end": "367810"
  },
  {
    "text": "like we have ranges\nin Python, if you want to loop over\na bunch of numbers, you can specify a range.",
    "start": "367810",
    "end": "374660"
  },
  {
    "text": "You can also use\ntorch.arange to be able to actually instantiate a\ntensor with a particular range.",
    "start": "374660",
    "end": "383470"
  },
  {
    "text": "In this case, we just looped\nover the numbers 1 through 10. You could reshape this\nand make it 1 through 5",
    "start": "383470",
    "end": "389530"
  },
  {
    "text": "and then 6 through 10. That's another way to be\nable to instantiate tensors.",
    "start": "389530",
    "end": "394630"
  },
  {
    "text": "And finally,\nsomething to note is that when we apply particular\noperations, such as just",
    "start": "394630",
    "end": "403180"
  },
  {
    "text": "simple Python operations like\naddition or multiplication, by default they're going\nto be element-wise.",
    "start": "403180",
    "end": "409970"
  },
  {
    "text": "So they'll apply to all of\nthe elements in our tensor. So in this case, we\ntook our tensor--",
    "start": "409970",
    "end": "416560"
  },
  {
    "text": "I think this one was\nprobably from earlier above-- and we added 2 everywhere.",
    "start": "416560",
    "end": "421630"
  },
  {
    "text": "Here we multiplied\neverything by 2. But the PyTorch semantics\nfor broadcasting",
    "start": "421630",
    "end": "427900"
  },
  {
    "text": "work pretty much the same\nas the NumPy semantics. So if you pretty much have\ndifferent matrix operations",
    "start": "427900",
    "end": "436990"
  },
  {
    "text": "where you need to batch\nacross a particular dimension, PyTorch will be smart about it. And it will actually\nmake sure that you",
    "start": "436990",
    "end": "443770"
  },
  {
    "text": "broadcast over the\nappropriate dimensions. Although, of course,\nyou have to make sure that the shapes are\ncompatible based",
    "start": "443770",
    "end": "449919"
  },
  {
    "text": "on the actual\nbroadcasting rules. So we'll get to that in a little\nbit when we look at reshaping",
    "start": "449920",
    "end": "455920"
  },
  {
    "text": "and how different operations\nhave those semantics. In this case, we\nhave to define the--",
    "start": "455920",
    "end": "463960"
  },
  {
    "text": "I guess, I'm not personally\naware of how you would define a jagged tensor that\nhas unequal dimensions,",
    "start": "463960",
    "end": "471940"
  },
  {
    "text": "but typically, we\ndon't want to do that because it makes our\ncomputation a lot more complex.",
    "start": "471940",
    "end": "477470"
  },
  {
    "text": "And so in cases where we have-- for instance, we have\ndifferent sentences that we turn into tokens, we\nmight have different length",
    "start": "477470",
    "end": "485620"
  },
  {
    "text": "sentences in our training set. We'll actually pad all of the\ndimensions to be the same. Because ultimately, we\nwant to do everything",
    "start": "485620",
    "end": "492610"
  },
  {
    "text": "with matrix operations. And so in order to\ndo that, we need to have a matrix\nof a fixed shape.",
    "start": "492610",
    "end": "497949"
  },
  {
    "text": "But that's a good point. I'm not sure if there\nis a way to do that, but typically, we just get\naround this by padding.",
    "start": "497950",
    "end": "506270"
  },
  {
    "text": "OK, so now we know\nhow to define tensors. We can do some interesting\nthings with them.",
    "start": "506270",
    "end": "511590"
  },
  {
    "text": "So here we've\ncreated two tensors. One of them is a 3x2 tensor.",
    "start": "511590",
    "end": "517640"
  },
  {
    "text": "The other one is a 2x4 tensor. And I think the answer\nis written up here,",
    "start": "517640",
    "end": "522799"
  },
  {
    "text": "but what do we expect\nis the shape when we multiply these two tensors?",
    "start": "522799",
    "end": "528120"
  },
  {
    "text": "So we have a 3x2 tensor\nand a 2x4 tensor--",
    "start": "528120",
    "end": "533279"
  },
  {
    "text": "yeah, 3 x 4. And so more generally, we\ncan use matmul in order",
    "start": "533280",
    "end": "541350"
  },
  {
    "text": "to do matrix multiplication. It also implements batched\nmatrix multiplication.",
    "start": "541350",
    "end": "546870"
  },
  {
    "text": "And so I won't go over the\nentire review of broadcasting semantics, but the main gist\nis that the dimensions of two",
    "start": "546870",
    "end": "555300"
  },
  {
    "text": "tensors are compatible\nif you can left pad the tensors with 1's so\nthat the dimensions that line up",
    "start": "555300",
    "end": "563040"
  },
  {
    "text": "either A, have the same\nnumber in that dimension, or B, one of them is\na dummy dimension. One of them has a 1.",
    "start": "563040",
    "end": "569400"
  },
  {
    "text": "And in that case, in\nthose dummy dimensions, PyTorch will actually make\nsure to copy over the tensor",
    "start": "569400",
    "end": "575220"
  },
  {
    "text": "as many times as needed so\nthat you can then actually perform the operation. And that's useful when\nyou want to do things",
    "start": "575220",
    "end": "581640"
  },
  {
    "text": "like batched dot products or\nbatched matrix multiplications.",
    "start": "581640",
    "end": "587040"
  },
  {
    "text": "And I guess the final point\nhere is there's also a shorthand notation that you can use.",
    "start": "587040",
    "end": "593060"
  },
  {
    "text": "So instead of having to\ntype out matmul every time, you can just use the @\noperator similar to NumPy.",
    "start": "593060",
    "end": "600130"
  },
  {
    "text": "Effectively, that's\nkind of where we get into how batching works. So for example, if you\nhad let's say two tensors",
    "start": "600130",
    "end": "609160"
  },
  {
    "text": "that have some batch dimension. And then one of them is m by\n1 and the other one is 1 by n.",
    "start": "609160",
    "end": "619780"
  },
  {
    "text": "And if you do a batched matrix\nmultiply to those two tensors, now what you effectively do\nis you preserve the batch",
    "start": "619780",
    "end": "626890"
  },
  {
    "text": "dimension and then you're\ndoing a matrix multiplication between an m by 1\ntensor and a 1 by n.",
    "start": "626890",
    "end": "632529"
  },
  {
    "text": "So you get something that's\nthe batch dimension by m by n. So effectively, there are more--",
    "start": "632530",
    "end": "639580"
  },
  {
    "text": "I think the full\nsemantics are written out on the PyTorch website\nfor how the matrix multiplication works.",
    "start": "639580",
    "end": "644770"
  },
  {
    "text": "But you're right. You don't just have\nthese cases where you have two\n2-dimensional tensors, you can have arbitrary\nnumber of dimensions.",
    "start": "644770",
    "end": "651430"
  },
  {
    "text": "And as long as the\ndimensions match up based on those\nsemantics I was saying, then you can multiply it.",
    "start": "651430",
    "end": "657380"
  },
  {
    "text": "Alternatively, you can do what\nI do, which is just multiply it anyways. And then if it throws an error,\nprint out the shapes and kind",
    "start": "657380",
    "end": "663680"
  },
  {
    "text": "of work from there. That tends to be faster, in\nmy opinion, in a lot of ways. But, yeah that's a good point.",
    "start": "663680",
    "end": "672700"
  },
  {
    "text": "All right. So let's keep going through\nsome of the other different functionalities here.",
    "start": "672700",
    "end": "678880"
  },
  {
    "text": "So we can define another tensor. And kind of one of the\nkey things that we always",
    "start": "678880",
    "end": "683980"
  },
  {
    "text": "want to look at is the shape. So in this case, we just\nhave a 1D tensor of length 3.",
    "start": "683980",
    "end": "690100"
  },
  {
    "text": "So the torch dot\nsize just gives us 3. In general, this is one\nof the key debugging steps",
    "start": "690100",
    "end": "697060"
  },
  {
    "text": "and something that\nI'll try and emphasize a lot throughout\nthis session, which is printing the shapes\nof all of your tensors",
    "start": "697060",
    "end": "703839"
  },
  {
    "text": "is probably your best resource\nwhen it comes to debugging. It's one of the hardest\nthings to intuit, exactly,",
    "start": "703840",
    "end": "710410"
  },
  {
    "text": "what's going on once\nyou start stacking a lot of different\noperations together. So printing out the shapes\nat each point and seeing",
    "start": "710410",
    "end": "718029"
  },
  {
    "text": "do they match what you expect\nis something important. And it's better to rely on that\nthan just on the error message",
    "start": "718030",
    "end": "725290"
  },
  {
    "text": "that PyTorch gives you\nbecause under the hood, PyTorch might implement\ncertain optimizations",
    "start": "725290",
    "end": "730720"
  },
  {
    "text": "and actually reshape the\nunderlying tensor you have. So you may not see the\nnumbers you expect.",
    "start": "730720",
    "end": "736310"
  },
  {
    "text": "So it's always great\nto print out the shape. ",
    "start": "736310",
    "end": "743200"
  },
  {
    "text": "So again, we can always\nprint out the shape and we can have a more complex--",
    "start": "743200",
    "end": "750390"
  },
  {
    "text": "in this case a 3-dimensional\ntensor, which is 3x2x4. And we can print out\nthe shape and we can",
    "start": "750390",
    "end": "757320"
  },
  {
    "text": "see all of the dimensions here. And so now you're\nlike, OK, great. We have tensors.",
    "start": "757320",
    "end": "763410"
  },
  {
    "text": "We can look at their shapes. But what do we\nactually do with them? And so now let's get\ninto kind of what",
    "start": "763410",
    "end": "768870"
  },
  {
    "text": "are the operations that we\ncan apply to these tensors.",
    "start": "768870",
    "end": "774029"
  },
  {
    "text": "And so one of them is, it's\nvery easy to reshape tensors.",
    "start": "774030",
    "end": "779270"
  },
  {
    "text": "So in this case, we're creating\nthis 15-dimensional tensor that's the numbers 1 to 15.",
    "start": "779270",
    "end": "786410"
  },
  {
    "text": "And now we're reshaping it. So now it's 5x3 tensor here.",
    "start": "786410",
    "end": "791610"
  },
  {
    "text": "And so you might wonder, well,\nwhat's the point of that? And it's because a\nlot of times when",
    "start": "791610",
    "end": "798889"
  },
  {
    "text": "we are doing machine\nlearning, we actually want to learn in batches. And so we might take our\ndata and we might reshape it.",
    "start": "798890",
    "end": "806100"
  },
  {
    "text": "So now that instead of being a\nlong flattened list of things, we actually have\na set of batches.",
    "start": "806100",
    "end": "811970"
  },
  {
    "text": "Or in some cases, we\nhave a set of batches, of a set of sentences\nor sequences",
    "start": "811970",
    "end": "818330"
  },
  {
    "text": "of a particular length\nand each of the elements in that sequence\nhas an embedding of a particular dimension.",
    "start": "818330",
    "end": "824630"
  },
  {
    "text": "And so based on the\ntypes of operations that you're trying to\ndo, you'll sometimes need to reshape those tensors.",
    "start": "824630",
    "end": "832040"
  },
  {
    "text": "And sometimes you'll want to-- particularly sometimes\ntranspose dimensions, if you want to, for instance,\nreorganize your data.",
    "start": "832040",
    "end": "840450"
  },
  {
    "text": "So that's another\noperation to keep in mind. I believe the\ndifference is View will",
    "start": "840450",
    "end": "850100"
  },
  {
    "text": "create a view of the\nunderlying tensor. And so I think the\nunderlying tensor will still have this same shape.",
    "start": "850100",
    "end": "855350"
  },
  {
    "text": "Reshape will actually\nmodify the tensor. ",
    "start": "855350",
    "end": "862959"
  },
  {
    "text": "All right. And then finally, like\nI said at the beginning, your intuition about\nPyTorch tensors",
    "start": "862960",
    "end": "868800"
  },
  {
    "text": "can simply be they're\nkind of a nice easy way to work with NumPy\narrays, but they have",
    "start": "868800",
    "end": "874920"
  },
  {
    "text": "all these great properties. Like, now we can essentially\nuse them with GPUs",
    "start": "874920",
    "end": "880350"
  },
  {
    "text": "and it's very optimized. And we can also compute\ngradients quickly. And to kind of just\nemphasize this point,",
    "start": "880350",
    "end": "887760"
  },
  {
    "text": "if you have some\nNumPy code and you have a bunch of\nNumPy arrays, you can directly convert\nthem into PyTorch sensors",
    "start": "887760",
    "end": "893940"
  },
  {
    "text": "by simply casting them. And you can also\ntake those tensors",
    "start": "893940",
    "end": "899010"
  },
  {
    "text": "and convert them\nback to NumPy arrays. ",
    "start": "899010",
    "end": "904149"
  },
  {
    "text": "All right. And so one of the things\nyou might be asking is, why do we care\nabout tensors?",
    "start": "904150",
    "end": "909670"
  },
  {
    "text": "What makes them good? And one of the great\nthings about them is that they support vectorized\noperations very easily.",
    "start": "909670",
    "end": "917560"
  },
  {
    "text": "Essentially we can parallelize\na lot of different computations and do them, for instance,\nacross a batch of data",
    "start": "917560",
    "end": "924070"
  },
  {
    "text": "all at once. And one of those operations you\nmight want to do, for instance, is a sum. So you can take, in this\ncase a tensor, which",
    "start": "924070",
    "end": "934480"
  },
  {
    "text": "is shape 5x7 and-- ",
    "start": "934480",
    "end": "940300"
  },
  {
    "text": "OK, it looks like\nthat's not working. You can take a tensor\nthat shaped 5x7. And now you can compute\ndifferent operations on it",
    "start": "940300",
    "end": "948370"
  },
  {
    "text": "that essentially collapse\nthe dimensionality. So the first one is sum. And so you can\ntake it and you can",
    "start": "948370",
    "end": "954610"
  },
  {
    "text": "sum across both the rows\nas well as the columns. And so one way I like to think\nabout this to kind of keep them",
    "start": "954610",
    "end": "961180"
  },
  {
    "text": "straight is that the dimension\nthat you specify in the sum is the dimension\nyou're collapsing.",
    "start": "961180",
    "end": "967600"
  },
  {
    "text": "So in this case, if you take the\ndata and sum over dimension 0 because you know the shape of\nthe underlying tensor is 5x7",
    "start": "967600",
    "end": "976720"
  },
  {
    "text": "you've collapsed\nthe 0-th dimension. So you should be left with\nsomething that's just shape 7.",
    "start": "976720",
    "end": "982959"
  },
  {
    "text": "And if you see the actual\ntensor, you got 75, 80, 85, 90, you get this tensor,\nwhich is shape 7.",
    "start": "982960",
    "end": "990698"
  },
  {
    "text": "Alternatively, you can\nthink about whether or not you're kind of summing\nacross the rows or summing across the columns.",
    "start": "990698",
    "end": "996980"
  },
  {
    "text": "But it's not just sum. It applies to other\noperations as well. You can compute\nstandard deviations.",
    "start": "996980",
    "end": "1002260"
  },
  {
    "text": "You can normalize your data. You can do other operations,\nwhich essentially batch across the entire set of data.",
    "start": "1002260",
    "end": "1009790"
  },
  {
    "text": "And not only do these\napply over 1 dimension, but here you can see\nthat if you don't specify",
    "start": "1009790",
    "end": "1015340"
  },
  {
    "text": "any dimensions, then by\ndefault, the operation actually applies to the entire tensor. So here we end up just taking\nthe sum of the entire thing.",
    "start": "1015340",
    "end": "1023810"
  },
  {
    "text": "So if you think about\nit, the 0-th dimension is the number of rows. There are five rows and\nthere are seven columns.",
    "start": "1023810",
    "end": "1029420"
  },
  {
    "text": "So if we sum out\nthe rows, then we're",
    "start": "1029420",
    "end": "1034750"
  },
  {
    "text": "actually summing\nacross the columns. And so now we only\nhave seven values.",
    "start": "1034750",
    "end": "1039939"
  },
  {
    "text": "But I like to think\nabout more just in terms of the dimensions\nto keep it straight, rather than rows or columns\nbecause it can get confusing.",
    "start": "1039940",
    "end": "1046060"
  },
  {
    "text": "If you're summing\nout dimension 0, then effectively\nyou've taken something which has some shape that's\ndimension 0 by dimension 1",
    "start": "1046060",
    "end": "1053590"
  },
  {
    "text": "to just whatever is the\ndimension one shape. And then from there\nyou can kind of figure out, OK, which\nway did I actually",
    "start": "1053590",
    "end": "1060220"
  },
  {
    "text": "sum to check if you were right. NumPy implements a lot\nof this vectorization.",
    "start": "1060220",
    "end": "1065890"
  },
  {
    "text": "And I believe in the homework\nthat you have right now, I think part of your\njob is to vectorize",
    "start": "1065890",
    "end": "1071980"
  },
  {
    "text": "a lot of these things. So the big advantage\nwith PyTorch is that essentially,\nit's optimized",
    "start": "1071980",
    "end": "1077830"
  },
  {
    "text": "to be able to take\nadvantage of your GPU. When we actually\nstart building out neural networks that are bigger,\nthat involve more computation,",
    "start": "1077830",
    "end": "1085410"
  },
  {
    "text": "we're going to be doing a lot\nof these matrix multiplication operations that is going to be\na lot better for our processor",
    "start": "1085410",
    "end": "1091660"
  },
  {
    "text": "if we can make use of the GPU. And so that's where\nPyTorch really comes in handy in\naddition to also defining",
    "start": "1091660",
    "end": "1099430"
  },
  {
    "text": "a lot of those neural\nnetwork modules, as we'll see later for you. So that now you don't\nneed to worry about,",
    "start": "1099430",
    "end": "1105850"
  },
  {
    "text": "for instance, implementing\na basic linear layer and backpropagation from\nscratch and also your optimizer.",
    "start": "1105850",
    "end": "1112929"
  },
  {
    "text": "All of those things\nwill be built in. And you can just call\nthe respective APIs to make use of them.",
    "start": "1112930",
    "end": "1118220"
  },
  {
    "text": "Whereas in Python\nand NumPy, you might have to do a lot of\nthat coding yourself.",
    "start": "1118220",
    "end": "1123950"
  },
  {
    "text": "Yeah.  All right. So we'll keep going.",
    "start": "1123950",
    "end": "1131680"
  },
  {
    "text": " So this is a quiz,\nexcept I think",
    "start": "1131680",
    "end": "1137750"
  },
  {
    "text": "it tells you the answer,\nso it's not much of a quiz. But what would you\ndo if now I told you",
    "start": "1137750",
    "end": "1145260"
  },
  {
    "text": "instead of summing\nover this tensor, I want you to\ncompute the average?",
    "start": "1145260",
    "end": "1150750"
  },
  {
    "text": "And so there's\ntwo different ways you could compute the average. You could compute the\naverage across the rows",
    "start": "1150750",
    "end": "1156690"
  },
  {
    "text": "or across the columns. And so essentially, now we get\nback to this question of, well,",
    "start": "1156690",
    "end": "1163940"
  },
  {
    "text": "which dimension am I actually\ngoing to reduce over? And so here if we want\nto preserve the rows,",
    "start": "1163940",
    "end": "1169549"
  },
  {
    "text": "then we need to actually sum\nover the second dimension--",
    "start": "1169550",
    "end": "1174790"
  },
  {
    "text": "really, the first-- 0-th and first. So the first\ndimension is what we have to sum over because we want\nto preserve the 0-th dimension.",
    "start": "1174790",
    "end": "1184030"
  },
  {
    "text": "And so that's why\nfor row average you see the dim equals 1. And for column\naverage, same reasoning",
    "start": "1184030",
    "end": "1190370"
  },
  {
    "text": "is why you see the dim equals 0. And so if we run\nthis code, we'll",
    "start": "1190370",
    "end": "1196430"
  },
  {
    "text": "see what are the\nshapes that we expect. If we're taking the\naverage Over Rows,",
    "start": "1196430",
    "end": "1202130"
  },
  {
    "text": "then an object that's\n2x3, should just become an object that's 2.",
    "start": "1202130",
    "end": "1207140"
  },
  {
    "text": "It's just a 1-dimensional\nalmost vector you can think of. And if we are averaging\nacross the columns,",
    "start": "1207140",
    "end": "1214670"
  },
  {
    "text": "there's three columns. So now our average\nshould have three values. And so now we're left with\na 1-dimensional tensor",
    "start": "1214670",
    "end": "1222140"
  },
  {
    "text": "of length 3.  So does that kind of\nmake sense, I guess,",
    "start": "1222140",
    "end": "1227400"
  },
  {
    "text": "is this general intuition\nabout how we deal with shapes, and how some of these\noperations manipulate shapes.",
    "start": "1227400",
    "end": "1233560"
  },
  {
    "text": "So now we'll get into indexing. This can get a\nlittle bit tricky, but I think you'll find\nthat the semantics are",
    "start": "1233560",
    "end": "1242290"
  },
  {
    "text": "very similar to NumPy. So one of the things\nthat you can do in NumPy",
    "start": "1242290",
    "end": "1248740"
  },
  {
    "text": "is that you can take\nthese NumPy arrays and you can slice across\nthem in many different ways. You can create copies of them.",
    "start": "1248740",
    "end": "1255340"
  },
  {
    "text": "And you can index across\nparticular dimensions to select out different\nelements, different rows",
    "start": "1255340",
    "end": "1261610"
  },
  {
    "text": "or different columns. And so in this case, let's\ntake this example tensor, which is 3x2x2.",
    "start": "1261610",
    "end": "1267910"
  },
  {
    "text": " And first thing you\nalways want to do",
    "start": "1267910",
    "end": "1273490"
  },
  {
    "text": "when you have a new tensor,\nprint out its shape. Understand what\nyou're working with. And so I may have shown\nthis already, but what",
    "start": "1273490",
    "end": "1285429"
  },
  {
    "text": "will x bracket 0 print out? What happens if we index\ninto just the first element?",
    "start": "1285430",
    "end": "1291463"
  },
  {
    "text": "What's the shape of this?  SPEAKER 2: 2X2. SPEAKER: Yeah.",
    "start": "1291463",
    "end": "1296670"
  },
  {
    "text": "2X2, right? Because if you think\nabout it, our tensor is really just a list of three\nthings, each of those things",
    "start": "1296670",
    "end": "1303300"
  },
  {
    "text": "happens to also be a 2X2 tensor. So we got a 2X2 object in this\ncase, the first thing, 1, 2, 3,",
    "start": "1303300",
    "end": "1310370"
  },
  {
    "text": "4. And so just like\nNumPy, if you provide a colon in a\nparticular dimension,",
    "start": "1310370",
    "end": "1317090"
  },
  {
    "text": "it means essentially\ncopy over that dimension. So if we do x bracket\n0, implicitly we're",
    "start": "1317090",
    "end": "1324610"
  },
  {
    "text": "putting a colon for all\nthe other dimensions. So it's saying grab the first\nthing along the 0-th dimension.",
    "start": "1324610",
    "end": "1332110"
  },
  {
    "text": "And then grab everything along\nthe other two dimensions. If we now take just\nthe 0-th element",
    "start": "1332110",
    "end": "1341710"
  },
  {
    "text": "along the first dimension,\nwhat are we going to get? Well, ultimately,\nwe're going to get.",
    "start": "1341710",
    "end": "1350040"
  },
  {
    "text": "Now if you look-- the kind of first\ndimension where these three things,\nthe second dimension",
    "start": "1350040",
    "end": "1356940"
  },
  {
    "text": "is now each of these two\nrows within those things. So like 1, 2, and 3, 4, 5, 6,\nand 7, 8, 9, 10, and 11, 12.",
    "start": "1356940",
    "end": "1365470"
  },
  {
    "text": "So if we index into\nthe second dimension-- or the first dimension--\nand get the 0-th element,",
    "start": "1365470",
    "end": "1372390"
  },
  {
    "text": "then we're going to end up\nwith 1, 2, 5, 6, and 9, 10.",
    "start": "1372390",
    "end": "1378340"
  },
  {
    "text": "And even if that's\na little bit tricky, you can kind of go\nback to the trick I mentioned before\nwhere we're slicing",
    "start": "1378340",
    "end": "1385930"
  },
  {
    "text": "across the first dimension. So if we look at the shape\nof our tensor, it's 3x2x2.",
    "start": "1385930",
    "end": "1392670"
  },
  {
    "text": "If we collapse the\nfirst dimension, that 2 in the middle, we're\nleft with something that's 3x2.",
    "start": "1392670",
    "end": "1398909"
  },
  {
    "text": "So it might seem a\nlittle bit trivial kind of going through this\nin a lot of detail.",
    "start": "1398910",
    "end": "1403980"
  },
  {
    "text": "But I think it's important\nbecause it can get tricky when your tensor shapes\nget more complicated, how",
    "start": "1403980",
    "end": "1409280"
  },
  {
    "text": "to actually reason about this. And so I won't go through\nevery example here, since a lot of them kind of\nreinforce the same thing,",
    "start": "1409280",
    "end": "1416910"
  },
  {
    "text": "but I'll just\nhighlight a few things. Just like NumPy, you can choose\nto get a range of elements.",
    "start": "1416910",
    "end": "1425330"
  },
  {
    "text": "In this case, we're\ntaking this new tensor, which is 1 through 15\nrearranged as a 5x3 tensor.",
    "start": "1425330",
    "end": "1435920"
  },
  {
    "text": "And if we take the 0-th\nthrough third row, exclusive,",
    "start": "1435920",
    "end": "1440990"
  },
  {
    "text": "we'll get the first three rows. And we can do the\nsame thing, but now with slicing across\nmultiple dimensions.",
    "start": "1440990",
    "end": "1450500"
  },
  {
    "text": "And I think the final point\nI want to talk about here is list indexing.",
    "start": "1450500",
    "end": "1456430"
  },
  {
    "text": "List indexing is also\npresent in NumPy. And it's a very clever\nshorthand for being",
    "start": "1456430",
    "end": "1461860"
  },
  {
    "text": "able to essentially select\nout multiple elements at once. So in this case,\nwhat you can do is",
    "start": "1461860",
    "end": "1469710"
  },
  {
    "text": "if you want to get the 0-th, the\nsecond, and the fourth element of our matrix, you\ncan just instead of",
    "start": "1469710",
    "end": "1477270"
  },
  {
    "text": "indexing with a particular\nnumber or set of numbers, index with a list of indices.",
    "start": "1477270",
    "end": "1483540"
  },
  {
    "text": "So in this case, if we\ngo up to our tensor, if we take out the 0-th,\nthe second, and the fourth,",
    "start": "1483540",
    "end": "1491970"
  },
  {
    "text": "we should see those three rows. And that's what\nwe end up getting. ",
    "start": "1491970",
    "end": "1500380"
  },
  {
    "text": "Yeah. Again, these are kind of a lot\nof examples to just reiterate the same point, which is that\nyou can slice across your data",
    "start": "1500380",
    "end": "1508260"
  },
  {
    "text": "in multiple ways. And at different points you're\ngoing to need to do that. So being familiar with the\nshapes that you understand",
    "start": "1508260",
    "end": "1515910"
  },
  {
    "text": "what's the underlying output\nthat you expect is important. In this case, for\ninstance, we're",
    "start": "1515910",
    "end": "1521910"
  },
  {
    "text": "slicing across the first\nand the second dimension and we're keeping the 0-th.",
    "start": "1521910",
    "end": "1528030"
  },
  {
    "text": "And so we're going to end up\ngetting essentially the kind of top left element of\neach of those three things",
    "start": "1528030",
    "end": "1535020"
  },
  {
    "text": "in our tensor if we scroll\nall the way up here. We'll get this 1,\nwe'll get this 5,",
    "start": "1535020",
    "end": "1541230"
  },
  {
    "text": "and we'll get this 9\nbecause we go across all of the 0-th dimension.",
    "start": "1541230",
    "end": "1546240"
  },
  {
    "text": "And then across the\nfirst and the second, we only take the 0-th element\nin both of those positions.",
    "start": "1546240",
    "end": "1554230"
  },
  {
    "text": "And so that's why\nwe get 1, 5, 9. ",
    "start": "1554230",
    "end": "1561539"
  },
  {
    "text": "Also, of course, you can\napply all of the colons to get back the original tensor. ",
    "start": "1561540",
    "end": "1572060"
  },
  {
    "text": "OK. And then I think the last\nthing when it comes to indexing is conversions.",
    "start": "1572060",
    "end": "1577790"
  },
  {
    "text": "So typically, when we're writing\ncode with neural networks, ultimately, we're\ngoing to process",
    "start": "1577790",
    "end": "1585390"
  },
  {
    "text": "some data through a network\nand we're going to get a loss. And that loss needs\nto be a scalar. And then we're going\nto compute gradients,",
    "start": "1585390",
    "end": "1591810"
  },
  {
    "text": "with respect to that loss. So one thing to keep in\nmind is that sometimes you",
    "start": "1591810",
    "end": "1597060"
  },
  {
    "text": "might have an operation and it\nfails because it was actually expecting a scalar value\nrather than a tensor.",
    "start": "1597060",
    "end": "1602620"
  },
  {
    "text": "And so you can extract out the\nscalar from this 1x1 tensor by just calling .item.",
    "start": "1602620",
    "end": "1608700"
  },
  {
    "text": " So in this case, if you\nhave a tensor, which is just literally 1,\nthen you can actually",
    "start": "1608700",
    "end": "1616500"
  },
  {
    "text": "get the Python scalar\nthat corresponds to it by calling dot item. So now we can get into the\nmore interesting stuff.",
    "start": "1616500",
    "end": "1623000"
  },
  {
    "text": "One of the really cool things\nwith PyTorch is autograd. And what autograd is, is\nPyTorch essentially provides",
    "start": "1623000",
    "end": "1632120"
  },
  {
    "text": "an automatic\ndifferentiation package where when you define\nyour neural network,",
    "start": "1632120",
    "end": "1638910"
  },
  {
    "text": "you're essentially\ndefining many nodes that compute some function.",
    "start": "1638910",
    "end": "1644240"
  },
  {
    "text": "And in the forward\npass, you're kind of running your data\nthrough those nodes. But what PyTorch is\ndoing on the back end,",
    "start": "1644240",
    "end": "1650480"
  },
  {
    "text": "is that at each of\nthose points, it's going to actually store the\ngradients and accumulate them,",
    "start": "1650480",
    "end": "1656269"
  },
  {
    "text": "so that every time you\ndo your backwards pass, you apply the chain rule\nto be able to calculate all",
    "start": "1656270",
    "end": "1662270"
  },
  {
    "text": "of these different gradients. And PyTorch caches\nthose gradients. And then you will have access\nto all of those gradients",
    "start": "1662270",
    "end": "1670009"
  },
  {
    "text": "to be able to actually then\nrun your favorite optimizer and optimize with\nSGD, or with Adam,",
    "start": "1670010",
    "end": "1677240"
  },
  {
    "text": "or whichever\noptimizer you choose. And so that's kind of one\nof the great features.",
    "start": "1677240",
    "end": "1682820"
  },
  {
    "text": "You don't have to worry about\nactually writing the code that computes all of these gradients\nand actually caches all of them",
    "start": "1682820",
    "end": "1689360"
  },
  {
    "text": "properly, applies the chain\nrule, does all these steps. You can abstract all of\nthat away with just one call",
    "start": "1689360",
    "end": "1695390"
  },
  {
    "text": "to .backward. So in this case, we'll\nrun through a little bit of an example where we'll see\nthe gradients getting computed",
    "start": "1695390",
    "end": "1703970"
  },
  {
    "text": "automatically. ",
    "start": "1703970",
    "end": "1709380"
  },
  {
    "text": "So in this case, we're going\nto initialize a tensor. And requiresgrad\nis true by default.",
    "start": "1709380",
    "end": "1716220"
  },
  {
    "text": "It just means that by\ndefault for a given tensor. PyTorch will store the\ngradient associated with it.",
    "start": "1716220",
    "end": "1724740"
  },
  {
    "text": "And you might wonder, well, why\ndo we have this when we always",
    "start": "1724740",
    "end": "1730920"
  },
  {
    "text": "want to store the gradient? And the answer\nis, at train time, you need the gradients in order\nto actually train your network.",
    "start": "1730920",
    "end": "1737610"
  },
  {
    "text": "But at inference\ntime, you'd actually want to disable your gradients. And you can actually\ndo that because it's",
    "start": "1737610",
    "end": "1743460"
  },
  {
    "text": "a lot of extra computation\nthat's not needed, since you're not making\nany updates to your network anymore.",
    "start": "1743460",
    "end": "1750890"
  },
  {
    "text": "Let's create this right now. We don't have any\ngradients being computed",
    "start": "1750890",
    "end": "1756830"
  },
  {
    "text": "because we haven't\nactually called backwards to actually compute some\nquantity with respect",
    "start": "1756830",
    "end": "1764990"
  },
  {
    "text": "to this particular tensor. We haven't actually computed\nthose gradients yet. So right now the .grad feature,\nwhich will actually store",
    "start": "1764990",
    "end": "1773720"
  },
  {
    "text": "the gradient associated\nwith that tensor, is none. And so now let's just define\na really simple function.",
    "start": "1773720",
    "end": "1779940"
  },
  {
    "text": "We have x. We're going to define the\nfunction y equals 3x squared.",
    "start": "1779940",
    "end": "1786753"
  },
  {
    "text": "And so now we're going\nto call y dot backward.  Now what happens is, when\nwe actually print out x.",
    "start": "1786753",
    "end": "1794539"
  },
  {
    "text": "grad, what we should expect\nto see is the number 12. And the reason is that our\nfunction y is 3x squared.",
    "start": "1794540",
    "end": "1803809"
  },
  {
    "text": "If we compute the\ngradient of that function, we're going to get 6x. And our actual value was 2.",
    "start": "1803810",
    "end": "1811810"
  },
  {
    "text": "So the actual gradient\nis going to be 12. And we see that when we print\nout x.grad that's what we get.",
    "start": "1811810",
    "end": "1821200"
  },
  {
    "text": "And now we'll just run it again. Let's set z equal to 3x squared. We call z.backwards.",
    "start": "1821200",
    "end": "1827580"
  },
  {
    "text": "And we print out x.grad again. And now we see that--",
    "start": "1827580",
    "end": "1833090"
  },
  {
    "text": "I may not have run this\nin the right order. OK. So here in the second one that\nI reran, we see that it says 24.",
    "start": "1833090",
    "end": "1843278"
  },
  {
    "text": "And so you might be\nwondering, well, I just did the same thing twice. Shouldn't I see 12, again?",
    "start": "1843278",
    "end": "1848590"
  },
  {
    "text": "And the answer is that\nby default, PyTorch will accumulate the gradients. So it won't actually rewrite\nthe gradient each time",
    "start": "1848590",
    "end": "1857130"
  },
  {
    "text": "you compute it. It will sum it. And the reason is\nbecause when you actually have backpropagation\nfor your network,",
    "start": "1857130",
    "end": "1863620"
  },
  {
    "text": "you want to accumulate\nthe gradients across all of your examples and then\nactually apply your update.",
    "start": "1863620",
    "end": "1869350"
  },
  {
    "text": "You don't want to\noverwrite the gradient. But this also means\nthat every time you have a training iteration\nfor your network,",
    "start": "1869350",
    "end": "1876510"
  },
  {
    "text": "you need to zero\nout the gradient because you don't want\nthe previous gradients from the last epoch where\nyou iterated through all",
    "start": "1876510",
    "end": "1883380"
  },
  {
    "text": "of your training data to\nmess with the current update that you're doing.",
    "start": "1883380",
    "end": "1888820"
  },
  {
    "text": "So that's kind of\none thing to note, which is that that's\nessentially why we will see when",
    "start": "1888820",
    "end": "1895200"
  },
  {
    "text": "we actually write the training\nloop you have to run zerograd in order to zero\nout the gradient.",
    "start": "1895200",
    "end": "1900780"
  },
  {
    "text": "Yes. So I accidentally ran the\ncells in the wrong order. Maybe to make it more clear,\nlet me put this one first.",
    "start": "1900780",
    "end": "1909030"
  },
  {
    "text": " So this is actually\nwhat it should",
    "start": "1909030",
    "end": "1914210"
  },
  {
    "text": "look like, which is\nthat we ran it once and I ran this cell first. And it has 12.",
    "start": "1914210",
    "end": "1919670"
  },
  {
    "text": "And then we ran it a\nsecond time, and we get 24. Yes. So if you have all of\nyour tensors defined, then",
    "start": "1919670",
    "end": "1927980"
  },
  {
    "text": "when you actually call\ndot backwards, if it's a function of\nmultiple variables, it's going to compute all\nof those partials, all",
    "start": "1927980",
    "end": "1934429"
  },
  {
    "text": "of those gradients. Yeah. So what's happening here is\nthat the way PyTorch works, is that it's storing the\naccumulated gradient at x.",
    "start": "1934430",
    "end": "1944900"
  },
  {
    "text": "And so we've essentially made\ntwo different backwards passes. We've called it once\non this function",
    "start": "1944900",
    "end": "1952309"
  },
  {
    "text": "y, which is a function of x. And we've called it once on z,\nwhich is also a function of x.",
    "start": "1952310",
    "end": "1957740"
  },
  {
    "text": "And so you're right. We can't actually disambiguate\nwhich came from what. We just see the\naccumulated gradient.",
    "start": "1957740",
    "end": "1963620"
  },
  {
    "text": "But typically, that's\nactually exactly what we want. Because what we want is to\nbe able to run our network",
    "start": "1963620",
    "end": "1970909"
  },
  {
    "text": "and accumulate the gradient\nacross all of the training examples that define our\nloss and then perform",
    "start": "1970910",
    "end": "1976830"
  },
  {
    "text": "our optimizer step. So, yeah, even with\nrespect to one thing, it doesn't matter because in\npractice, each of those things",
    "start": "1976830",
    "end": "1982980"
  },
  {
    "text": "is really a different example\nin our set of training examples. And so we're not interested in\nthe gradient from one example.",
    "start": "1982980",
    "end": "1990010"
  },
  {
    "text": "We're actually interested\nin the overall gradient. So going back to this example,\nwhat's happening here,",
    "start": "1990010",
    "end": "1996600"
  },
  {
    "text": "is that in the backwards pass,\nwhat it's doing is you can imagine there's the x tensor\nand then there's the .grad",
    "start": "1996600",
    "end": "2004490"
  },
  {
    "text": "attribute, which is\nanother separate tensor. It's going to be\nthe same shape as x. And what that is storing, is\nit's storing the accumulated",
    "start": "2004490",
    "end": "2012529"
  },
  {
    "text": "gradient from every single time\nthat you've called dot backward on a quantity that essentially\nhas some dependency on x, that",
    "start": "2012530",
    "end": "2022130"
  },
  {
    "text": "will have a non-zero gradient. And so the first\ntime we call it, the gradient will be 12\nbecause 6x, 6 times 2.",
    "start": "2022130",
    "end": "2029510"
  },
  {
    "text": "12. The second time we do it\nwith z, it's also still 12, but the point is that .grad\ndoesn't actually overwrite",
    "start": "2029510",
    "end": "2036980"
  },
  {
    "text": "the gradient each time\nyou call dot backwards. It simply adds them. It accumulates them. And the intuition there\nis that ultimately, you're",
    "start": "2036980",
    "end": "2045260"
  },
  {
    "text": "going to want to compute\nthe gradient with respect to the loss.",
    "start": "2045260",
    "end": "2050419"
  },
  {
    "text": "And that loss is\ngoing to be made up of many different examples. And so you need to accumulate\nthe gradient from all of those",
    "start": "2050420",
    "end": "2056359"
  },
  {
    "text": "in order to make\na single update. And then, of course,\nyou'll have to zero that out because every time\nyou make one pass through all",
    "start": "2056360",
    "end": "2062960"
  },
  {
    "text": "of your data, you don't\nwant that next batch of data to also be double counting\nthe previous batches update.",
    "start": "2062960",
    "end": "2068599"
  },
  {
    "text": "You want to keep those separate. And so we'll see\nthat in a second. OK. Yeah.",
    "start": "2068600",
    "end": "2073989"
  },
  {
    "text": " All right. So now we're going\nto move on to one",
    "start": "2073989",
    "end": "2082343"
  },
  {
    "text": "of the final pieces\nof the puzzle, which is neural networks. How do we actually\nuse them in PyTorch?",
    "start": "2082343",
    "end": "2087820"
  },
  {
    "text": "And once we have that and\nwe have our optimization, we'll finally be able to\nfigure out how do we actually",
    "start": "2087820",
    "end": "2094629"
  },
  {
    "text": "train a neural network,\nwhat does that look like, and why it's so clean and\nefficient when you do it in PyTorch.",
    "start": "2094630",
    "end": "2102550"
  },
  {
    "text": "So the first thing\nthat you want to do is we're going to be defining\nneural networks in terms",
    "start": "2102550",
    "end": "2107920"
  },
  {
    "text": "of existing building blocks, in\nterms of existing APIs, which will implement for\ninstance linear layers",
    "start": "2107920",
    "end": "2115150"
  },
  {
    "text": "or different activation\nfunctions that we need. So we're going to\nimport torch.nn",
    "start": "2115150",
    "end": "2120370"
  },
  {
    "text": "because that is the neural\nnetwork package that we're going to make use of. And so let's start\nwith the linear layer.",
    "start": "2120370",
    "end": "2127710"
  },
  {
    "text": "The way the linear\nlayer works in PyTorch, is it takes in two arguments. It takes in the input dimension\nand then the output dimension.",
    "start": "2127710",
    "end": "2136950"
  },
  {
    "text": "And so what it does, is it\ntakes in some input, which",
    "start": "2136950",
    "end": "2142920"
  },
  {
    "text": "has some arbitrary\namount of dimensions, and then finally,\nthe input dimension.",
    "start": "2142920",
    "end": "2148680"
  },
  {
    "text": "And it will\nessentially output it to that same set of dimensions,\nexcept the output dimension",
    "start": "2148680",
    "end": "2154559"
  },
  {
    "text": "and the very last place. And you can think\nof the linear layer",
    "start": "2154560",
    "end": "2160000"
  },
  {
    "text": "as essentially just\nperforming a simple ax plus b. By default, it's\ngoing to apply a bias,",
    "start": "2160000",
    "end": "2168790"
  },
  {
    "text": "but you can also disable that\nif you don't want a bias term. And so let's look\nat a small example.",
    "start": "2168790",
    "end": "2175020"
  },
  {
    "start": "2175020",
    "end": "2183660"
  },
  {
    "text": "So here we have our input. And we're going to create a\nlinear layer, in this case,",
    "start": "2183660",
    "end": "2191460"
  },
  {
    "text": "as an input size of 4\nand output size of 2. And all we're going to\ndo is, once we define it",
    "start": "2191460",
    "end": "2199960"
  },
  {
    "text": "by instantiating\nit with nn.linear. Whatever the name of our\nlayer is, in this case,",
    "start": "2199960",
    "end": "2205130"
  },
  {
    "text": "we call it linear. We just essentially\napply it with parentheses as if it were a function\nto whatever input.",
    "start": "2205130",
    "end": "2211539"
  },
  {
    "text": "And that actually does\nthe actual forward pass through this linear\nlayer to get our output.",
    "start": "2211540",
    "end": "2217405"
  },
  {
    "text": " And so you can see that the\noriginal shape was 2x3x4.",
    "start": "2217405",
    "end": "2226640"
  },
  {
    "text": "Then we pass it through\nthis linear layer, which has an output\ndimension of size 2. And so ultimately, our output\nis 2x3x2, which is good.",
    "start": "2226640",
    "end": "2235860"
  },
  {
    "text": "That's what we expect. That's not shape error. But something common\nthat you'll see",
    "start": "2235860",
    "end": "2241460"
  },
  {
    "text": "is maybe you decide to\nget a little confused",
    "start": "2241460",
    "end": "2248440"
  },
  {
    "text": "and maybe you do let's say 2x2. You match the wrong dimension.",
    "start": "2248440",
    "end": "2256010"
  },
  {
    "text": "Here we're going to\nget a shape error. And you see that the error\nmessage isn't as helpful",
    "start": "2256010",
    "end": "2261589"
  },
  {
    "text": "because it's actually\nchanged the shape of what we were working with. We said this was 2x3x4. Under the hood, PyTorch\nhas changed this to a 6x4.",
    "start": "2261590",
    "end": "2269240"
  },
  {
    "text": "But in this case, it's obvious\nbecause we instantiated it",
    "start": "2269240",
    "end": "2274670"
  },
  {
    "text": "with the shape. But if we didn't\nhave the shape, then one simple thing we\ncould do is actually just",
    "start": "2274670",
    "end": "2280160"
  },
  {
    "text": "print out the\nshape and we'd see, OK, this last\ndimension is size 4, so I actually need\nto change my input",
    "start": "2280160",
    "end": "2286370"
  },
  {
    "text": "dimension in my linear\nlayer to be size 4. ",
    "start": "2286370",
    "end": "2294750"
  },
  {
    "text": "And you also notice\non this output we have this grad function. And so that's because\nwe're actually",
    "start": "2294750",
    "end": "2301140"
  },
  {
    "text": "computing and storing the\ngradients here for our tensor. ",
    "start": "2301140",
    "end": "2312450"
  },
  {
    "text": "Yeah. So typically, we think\nof the first dimension as the batch dimension. So in this case,\nit said n-- this",
    "start": "2312450",
    "end": "2318588"
  },
  {
    "text": "you can think of as if\nyou had a batch of images, it would be the\nnumber of images. If you had a training\ncorpus of text,",
    "start": "2318588",
    "end": "2324500"
  },
  {
    "text": "it would be\nessentially the number of sentences or sequences.",
    "start": "2324500",
    "end": "2331039"
  },
  {
    "text": "That is usually considered\nthe batch dimension. The star indicates that\nthere can be an arbitrary number of dimensions.",
    "start": "2331040",
    "end": "2336720"
  },
  {
    "text": "So for instance,\nif we had images, this could be a\n4-dimensional tensor object.",
    "start": "2336720",
    "end": "2342330"
  },
  {
    "text": "It could be the batch size\nby the number of channels by the height, by the width.",
    "start": "2342330",
    "end": "2347599"
  },
  {
    "text": "But in general, there's no\nfixed number of dimensions. Your input tensor can be\nany number of dimensions.",
    "start": "2347600",
    "end": "2353510"
  },
  {
    "text": "The key is just\nthat last dimension needs to match up with the input\ndimension of your linear layer.",
    "start": "2353510",
    "end": "2360960"
  },
  {
    "text": "The 2 is the output size. So essentially, we're\nsaying that we're going to map this\nlast dimension, which",
    "start": "2360960",
    "end": "2368940"
  },
  {
    "text": "is 4-dimensional to\nnow 2-dimensional. So in general, you\ncan think of this as if we're stacking\na neural network,",
    "start": "2368940",
    "end": "2375930"
  },
  {
    "text": "this is the kind of\ninput dimension size. And this would be like\nthe hidden dimension size. ",
    "start": "2375930",
    "end": "2383882"
  },
  {
    "text": "And so one thing we can\ndo is, we can actually print out the parameters. And we can actually see what are\nthe values of our linear layer,",
    "start": "2383883",
    "end": "2390480"
  },
  {
    "text": "or in general, for\nany layer that we define in our neural network\nwhat are the actual parameters.",
    "start": "2390480",
    "end": "2396460"
  },
  {
    "text": "And in this case, we see that\nthere's two sets of parameters because we have a bias, as\nwell as the actual linear layer",
    "start": "2396460",
    "end": "2407550"
  },
  {
    "text": "itself. And so both of them\nstore the gradients. And in this case, these\nare what the current values",
    "start": "2407550",
    "end": "2418359"
  },
  {
    "text": "of these parameters are. And they'll change as\nwe trained the network. ",
    "start": "2418360",
    "end": "2424160"
  },
  {
    "text": "OK. So now let's go through some\nof the other module layers. ",
    "start": "2424160",
    "end": "2431480"
  },
  {
    "text": "So in general, nn.Linear\nis one of the layers you have access to. You have a couple of\nother different layers.",
    "start": "2431480",
    "end": "2437620"
  },
  {
    "text": "They are pretty common. You have 2D convolutions. You have transpose convolutions.",
    "start": "2437620",
    "end": "2442900"
  },
  {
    "text": "You have batch norm layers when\nyou need to do normalization in your network. You can do upsampling.",
    "start": "2442900",
    "end": "2448359"
  },
  {
    "text": "You can do max pooling. You can do lots of\ndifferent operators. But the main key here\nis that all of them",
    "start": "2448360",
    "end": "2453670"
  },
  {
    "text": "are built in building blocks\nthat you can just call, just like we did with nn.Linear. ",
    "start": "2453670",
    "end": "2461040"
  },
  {
    "text": "Let's just go-- I guess I'm running out of\ntime, but let's just try and go through these last few\nlayers and then I'll",
    "start": "2461040",
    "end": "2467250"
  },
  {
    "text": "wrap up by showing\nyou an example that puts it all together. So in this case, we can define\nan activation function, which",
    "start": "2467250",
    "end": "2475020"
  },
  {
    "text": "is typical with our networks. We need to introduce\nnon-linearities. In this case, we use\nthe sigmoid function.",
    "start": "2475020",
    "end": "2481030"
  },
  {
    "text": "And so now we can\ndefine our network as this very simple thing, which\nhad one linear layer and then",
    "start": "2481030",
    "end": "2486090"
  },
  {
    "text": "an activation. And in general, when we\ncompose these layers together,",
    "start": "2486090",
    "end": "2492759"
  },
  {
    "text": "we don't need to actually\nwrite every single line-by-line applying the next layer.",
    "start": "2492760",
    "end": "2498160"
  },
  {
    "text": "We can actually stack\nall of them together. In this case, we can\nuse nn.sequential",
    "start": "2498160",
    "end": "2503170"
  },
  {
    "text": "and list all of the layers. So here we have our linear\nlayer, followed by our sigmoid. And then now we're just\nessentially passing the input",
    "start": "2503170",
    "end": "2512410"
  },
  {
    "text": "through this whole set\nof layers all at once. So we take our input. We call block on the input\nand we get the output.",
    "start": "2512410",
    "end": "2518845"
  },
  {
    "text": " Let's just kind of see\nputting it all together",
    "start": "2518845",
    "end": "2524160"
  },
  {
    "text": "what does it look like\nto define a network and what does it look\nlike when we train one? So here we're going\nto actually define",
    "start": "2524160",
    "end": "2530480"
  },
  {
    "text": "a multi-layer perceptron. And the way it works, is\nto define a neural network, you extend the nn.module class.",
    "start": "2530480",
    "end": "2537920"
  },
  {
    "text": "The key here is there's\nreally two main things you have to define when you\ncreate your own network. One, is the initialization.",
    "start": "2537920",
    "end": "2544349"
  },
  {
    "text": "So in the init\nfunction, you actually initialize all the\nparameters you need. In this case, we initialize\nan input size, a hidden size,",
    "start": "2544350",
    "end": "2552230"
  },
  {
    "text": "and we actually define\nthe model itself. In this case, it's\na simple model, which consists of a\nlinear layer, followed",
    "start": "2552230",
    "end": "2560840"
  },
  {
    "text": "by an activation, followed\nby another linear layer, followed by a final activation. And the second function\nwe have to define",
    "start": "2560840",
    "end": "2567710"
  },
  {
    "text": "is the forward, which\nactually does the forward pass of the network. And so here our forward\nfunction takes in our input x.",
    "start": "2567710",
    "end": "2576630"
  },
  {
    "text": "In general, it could take in\nsome arbitrary amount of inputs into this function,\nbut essentially, it",
    "start": "2576630",
    "end": "2582800"
  },
  {
    "text": "needs to figure out how are you\nactually computing the output? And in this case,\nit's very simple. We just pass it into the\nnetwork that we just defined",
    "start": "2582800",
    "end": "2590200"
  },
  {
    "text": "and return the output. And again, you could\ndo this more explicitly",
    "start": "2590200",
    "end": "2596640"
  },
  {
    "text": "by doing what we did earlier\nwhere we could actually write out all of the\nlayers individually instead",
    "start": "2596640",
    "end": "2602099"
  },
  {
    "text": "of wrapping them into\none object and then doing a line-by-line operation for\neach one of these layers.",
    "start": "2602100",
    "end": "2612610"
  },
  {
    "text": "And so finally, if\nwe define our class, it's very simple to use it. We can now just\ninstantiate some input,",
    "start": "2612610",
    "end": "2619680"
  },
  {
    "text": "instantiate our model by\ncalling multi-layer perceptron with our parameters, and then\njust pass it through our model.",
    "start": "2619680",
    "end": "2625725"
  },
  {
    "text": " So that's great, but this is\nall just the forward pass.",
    "start": "2625725",
    "end": "2632328"
  },
  {
    "text": "How do we actually\ntrain the network? How do we actually\nmake it better? And so this is the\nfinal step, which",
    "start": "2632328",
    "end": "2637470"
  },
  {
    "text": "is we have optimization\nbuilt in to PyTorch. So we have this\nbackward function,",
    "start": "2637470",
    "end": "2642753"
  },
  {
    "text": "which goes and computes\nall of these gradients in the backward pass. And now the only step left is to\nactually update the parameters",
    "start": "2642753",
    "end": "2649230"
  },
  {
    "text": "using those gradients. And so here we'll import\nthe torch.optim package, which contains all of the\noptimizers that you need.",
    "start": "2649230",
    "end": "2656640"
  },
  {
    "text": " This part is just\ncreating some random data,",
    "start": "2656640",
    "end": "2662089"
  },
  {
    "text": "so that we can actually\ndecide how to fit our data. But this is really\nthe key here, which",
    "start": "2662090",
    "end": "2668360"
  },
  {
    "text": "is we'll instantiate our\nmodel that we defined. We'll define the Adam optimizer.",
    "start": "2668360",
    "end": "2675410"
  },
  {
    "text": "And we'll define it with a\nparticular learning rate. We'll define a loss\nfunction, which is again,",
    "start": "2675410",
    "end": "2680569"
  },
  {
    "text": "another built in module. In this case, we're using\nthe cross entropy loss. And finally, to calculate our\npredictions, all we do simply",
    "start": "2680570",
    "end": "2688430"
  },
  {
    "text": "is just call model\non our actual input. And to calculate\nour loss, we just call our loss function\non our predictions",
    "start": "2688430",
    "end": "2696020"
  },
  {
    "text": "and our true labels. And we extract the scalar here. And now when we put\nit all together,",
    "start": "2696020",
    "end": "2703020"
  },
  {
    "text": "this is what the\ntraining loop looks like. We have some number\nof epochs that we want to train our network.",
    "start": "2703020",
    "end": "2708890"
  },
  {
    "text": "For each of these epochs,\nthe first thing we do is we take our optimizer and\nwe zero out the gradient.",
    "start": "2708890",
    "end": "2713978"
  },
  {
    "text": "And the reason we do that,\nis because, like many of you noted, we actually are\naccumulating the gradient.",
    "start": "2713978",
    "end": "2719490"
  },
  {
    "text": "We're not resetting it every\ntime we call dot backward. So we zero out the gradient. We get our model predictions\nby doing a forward pass.",
    "start": "2719490",
    "end": "2728099"
  },
  {
    "text": "We then compute the loss\nbetween the predictions and the true values.",
    "start": "2728100",
    "end": "2733650"
  },
  {
    "text": "Finally, we call loss.backward. This is what\nactually computes all of the gradients in the\nbackward pass from our loss.",
    "start": "2733650",
    "end": "2741529"
  },
  {
    "text": "And the final step is we\ncall .step on our optimizer. In this case, we're using Adam.",
    "start": "2741530",
    "end": "2747589"
  },
  {
    "text": "And this will take a step\non our loss function. And so if we run\nthis code, we end up",
    "start": "2747590",
    "end": "2752780"
  },
  {
    "text": "seeing that we're able to start\nwith some training loss, which is relatively high. And in 10 epochs, we're\nable to essentially",
    "start": "2752780",
    "end": "2759740"
  },
  {
    "text": "completely fit our data. And if we print out\nour model parameters",
    "start": "2759740",
    "end": "2765210"
  },
  {
    "text": "and we printed them out\nfrom the start as well, we'd see that they've\nchanged as we've actually done this optimization.",
    "start": "2765210",
    "end": "2771990"
  },
  {
    "text": "And so I'll wrap\nit up here, but I think the key takeaway is that\na lot of the things that you're",
    "start": "2771990",
    "end": "2778200"
  },
  {
    "text": "doing at the beginning\nof this class are really about understanding\nthe basics of how neural networks work, how\nyou actually implement them,",
    "start": "2778200",
    "end": "2785609"
  },
  {
    "text": "how you implement\nthe backward pass. The great thing about\nPyTorch is that once you get to the very next\nassignment, you'll",
    "start": "2785610",
    "end": "2791160"
  },
  {
    "text": "see that now that you\nhave a good underlying understanding of\nthose things, you can abstract a lot of\nthe complexity of how",
    "start": "2791160",
    "end": "2797560"
  },
  {
    "text": "do you do backprop, how do you\nstore all of these gradients, how do you compute them, how do\nyou actually run the optimizer,",
    "start": "2797560",
    "end": "2803910"
  },
  {
    "text": "and let PyTorch handle\nall of that for you. And you can use all of\nthese building blocks, all of these different\nneural network layers",
    "start": "2803910",
    "end": "2810510"
  },
  {
    "text": "to now define your\nown networks that you can use to solve whatever\nproblems you need.",
    "start": "2810510",
    "end": "2816110"
  },
  {
    "start": "2816110",
    "end": "2820000"
  }
]