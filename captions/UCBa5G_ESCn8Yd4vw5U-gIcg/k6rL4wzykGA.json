[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "Thank you so much for, uh, coming and [NOISE] putting up with a few minutes of technical difficulties.",
    "start": "4000",
    "end": "11025"
  },
  {
    "text": "Okay. So, um, I'm here to give a lecture today on exploration and meta-reinforcement learning.",
    "start": "11025",
    "end": "18000"
  },
  {
    "start": "17000",
    "end": "67000"
  },
  {
    "text": "And so, this is just a little outline of the things I'm planning to talk about. So first we're gonna recap policy gradient and how we can use that to",
    "start": "18000",
    "end": "25890"
  },
  {
    "text": "build some meta-reinforcement learning algorithms that Chelsea touched on last week. And then we're going to look at the problem of exploration a little bit more deeply.",
    "start": "25890",
    "end": "35295"
  },
  {
    "text": "And finally before the break, see an approach to encourage better exploration. Then we'll have a break and afterwards,",
    "start": "35295",
    "end": "42969"
  },
  {
    "text": "we'll come back and take a bit of a different view of meta-reinforcement learning framing as a POMDP. And then we'll look at how this framing allows us to design a,",
    "start": "42970",
    "end": "51680"
  },
  {
    "text": "a different algorithm that crucially is off policy, so much more sample efficient and uses a bit of a different way to explore.",
    "start": "51680",
    "end": "58880"
  },
  {
    "text": "And at any point, um, please raise your hands, ask questions, uh, interactive is always better.",
    "start": "58880",
    "end": "66130"
  },
  {
    "start": "67000",
    "end": "235000"
  },
  {
    "text": "Okay. So just to kind of motivate the whole problem statement, um,",
    "start": "67010",
    "end": "72570"
  },
  {
    "text": "learning approaches have really excelled in producing agents that are specialists, right, they're good at one specific task.",
    "start": "72570",
    "end": "77870"
  },
  {
    "text": "But in the real world, we'd like our agents to be generalists, um, skilled at a variety of behaviors and able to exploit",
    "start": "77870",
    "end": "84470"
  },
  {
    "text": "the structure in the world in order to learn new things faster. Um, so here's a nice little toy example with some, some art, um,",
    "start": "84470",
    "end": "92450"
  },
  {
    "text": "surfing, skateboarding and sledding, all involve bouncing your body on the board while it moves, right, so we'd like the agent to be able to extract this kind of",
    "start": "92450",
    "end": "100490"
  },
  {
    "text": "shared knowledge between the tasks in order to learn a new similar task faster.",
    "start": "100490",
    "end": "105759"
  },
  {
    "text": "So let's briefly go back to how meta-learning works in the supervised learning case just to build up some common formalism.",
    "start": "105760",
    "end": "114220"
  },
  {
    "text": "So in the supervised learning case, say image classification, we'd like to be able to recognize new classes from just a few labeled examples.",
    "start": "114220",
    "end": "122165"
  },
  {
    "text": "And we'll do that by training across many such, uh, small classification tasks,",
    "start": "122165",
    "end": "128095"
  },
  {
    "text": "tasks in our training set. So just to get some notation, we'll say that we our dat- our training set is d-train and we're going to",
    "start": "128095",
    "end": "136849"
  },
  {
    "text": "use some function F-Theta which I'm not going to describe at all right now.",
    "start": "136850",
    "end": "142115"
  },
  {
    "text": "And just say that F-Theta extracts some task-specific information that I'll call Phi_i, and then, I want to use Phi_i to make predictions on the test set, right.",
    "start": "142115",
    "end": "150319"
  },
  {
    "text": "And so then I compute the loss and I sum over all of my training tasks.",
    "start": "150320",
    "end": "155490"
  },
  {
    "text": "So my goal is to maximize performance across all the training tasks. Okay, so now let's add the RL formulas in side-by-side.",
    "start": "155500",
    "end": "164334"
  },
  {
    "text": "So in RL, now, instead of, um, adapting to a new classification task,",
    "start": "164335",
    "end": "170105"
  },
  {
    "text": "we want to adapt to a new MDP. So suppose we have some, again, going back to",
    "start": "170105",
    "end": "175730"
  },
  {
    "text": "our set of MDPs that involves balancing yourself, right, so we want to be able to train across the set of",
    "start": "175730",
    "end": "181100"
  },
  {
    "text": "MDPs in order to generalize to a new one. And so the notation will be largely the same, again,",
    "start": "181100",
    "end": "186770"
  },
  {
    "text": "we have our F-Theta extracting task information Phi_i and we'll use Phi_i to try to maximize our expected return in the new MDP.",
    "start": "186770",
    "end": "196190"
  },
  {
    "text": "So just to get some terms down now to be clear,",
    "start": "196190",
    "end": "202340"
  },
  {
    "text": "I'm gonna call this F-Theta entity the adaptation or sometimes the inner loop,",
    "start": "202340",
    "end": "207920"
  },
  {
    "text": "and we'll call this outer optimization, the outer loop or the meta-training.",
    "start": "207920",
    "end": "213540"
  },
  {
    "text": "And so, as we look at these different algorithms, we'll keep coming back to what does meta-training look like and what does adaptation look like.",
    "start": "213540",
    "end": "221515"
  },
  {
    "text": "And so, meta-training will generally just simply be stochastic gradient descent on this objective,",
    "start": "221515",
    "end": "227065"
  },
  {
    "text": "but we'll see a lot of different choices for how adaptation can work throughout these algorithms.",
    "start": "227065",
    "end": "233629"
  },
  {
    "start": "235000",
    "end": "333000"
  },
  {
    "text": "Okay, so let's look a little deeper at the differences between these two frameworks that I've written down.",
    "start": "235160",
    "end": "242020"
  },
  {
    "text": "So what's the difference in RL? Well, in supervised learning, notice that that new task that we're given,",
    "start": "242020",
    "end": "249340"
  },
  {
    "text": "it's all the data is given to us, right, someone gave us the training examples that were labeled, um,",
    "start": "249340",
    "end": "255655"
  },
  {
    "text": "and so, all our job was, was to figure out how to adapt quickly given that data.",
    "start": "255655",
    "end": "261655"
  },
  {
    "text": "But now if we consider the RL setting, when I want to compute F-Theta of M_i where M is the MDP,",
    "start": "261655",
    "end": "268840"
  },
  {
    "text": "I have to figure out how, like I'm not usually given the, say, transition function or word function that explicitly define the MDP,",
    "start": "268840",
    "end": "275570"
  },
  {
    "text": "right, I'm given experience, and I have to choose what experience I'm going to use to adapt. So we introduced this problem of exploration in addition to adaptation.",
    "start": "275570",
    "end": "285905"
  },
  {
    "text": "So now your job is not only to adapt, but also to collect the data that will best allow you to adapt.",
    "start": "285905",
    "end": "292890"
  },
  {
    "text": "Okay, so let's go back for a second to policy gradients so that we can build-up our meta-RL algorithm.",
    "start": "295880",
    "end": "302820"
  },
  {
    "text": "So in policy gradient we're doing direct policy search, um, with a policy Pi that we're going to parameterize with Theta.",
    "start": "302820",
    "end": "310670"
  },
  {
    "text": "And so we'll simply run the REINFORCE algorithm, which I think, uh, Chelsea mentioned last week,",
    "start": "310670",
    "end": "316444"
  },
  {
    "text": "where we'll sample data from our, from our policy and then, we'll compute the policy gradient to update the parameters.",
    "start": "316445",
    "end": "322700"
  },
  {
    "text": "And this is really just kind of a formalization of trial and error, right, you're trying to make good trajectories more probable,",
    "start": "322700",
    "end": "328730"
  },
  {
    "text": "and bad trajectories less probable. Okay, so how can we turn policy gradient into a meta-learning algorithm?",
    "start": "328730",
    "end": "337030"
  },
  {
    "start": "333000",
    "end": "461000"
  },
  {
    "text": "Well, intuitively what we wanna do is collect and remember information as we go,",
    "start": "337030",
    "end": "342590"
  },
  {
    "text": "right, as we collect experience and we want that information to kind of inform the next action that we take.",
    "start": "342590",
    "end": "348190"
  },
  {
    "text": "So this kinda starts sounding like recurrent networks, so let's just try plugging that in for the policy. So the idea is our policy is just going to be a recurrent network,",
    "start": "348190",
    "end": "357290"
  },
  {
    "text": "we're gonna train with policy gradient as usual, and we're going to train across a bunch of different meta-training tasks.",
    "start": "357290",
    "end": "364365"
  },
  {
    "text": "Right, so I'll sample a task, in this case, sledding, I'll roll out a bunch of experience from it and through my RNN.",
    "start": "364365",
    "end": "370310"
  },
  {
    "text": "And through each time-step that I accumulate I'll be updating the hidden state of the RNN,",
    "start": "370310",
    "end": "376275"
  },
  {
    "text": "right, which will inform my next action. And so, as I train across all of these tasks,",
    "start": "376275",
    "end": "381284"
  },
  {
    "text": "I'll be optimizing Theta such that I get Theta-star. And what Theta-star will represent is like an op- like a learning algorithm for a new task,",
    "start": "381285",
    "end": "389479"
  },
  {
    "text": "right, where the, the learning happens through the recurrent update of the hidden state. So to kind of keep track of all this in the upper right hand corner,",
    "start": "389480",
    "end": "398090"
  },
  {
    "text": "I'm just going to keep that, that RL formulism that we wrote down at the beginning. And so now our choice of F-Theta here is just an RNN,",
    "start": "398090",
    "end": "405620"
  },
  {
    "text": "right, and we're still doing policy gradient from the outside loop. And so, one, one note about this, is that, um,",
    "start": "405620",
    "end": "414180"
  },
  {
    "text": "you may be wondering like how is this different from just using your current policy in general and one difference is that we want to persist the hidden state across episodes,",
    "start": "414180",
    "end": "424425"
  },
  {
    "text": "right, because we may want adaptation to extend across many episodes.",
    "start": "424425",
    "end": "429960"
  },
  {
    "text": "Okay, so what are the pros of this method? Um, it's pretty general and it's pretty expressive,",
    "start": "431180",
    "end": "437190"
  },
  {
    "text": "right, RNNs are very powerful models. One con is that it's not consistent.",
    "start": "437190",
    "end": "442820"
  },
  {
    "text": "And what I mean by consistent is that we don't really have a guarantee that the RNN learning algorithm will converge or what it will converge to,",
    "start": "442820",
    "end": "451474"
  },
  {
    "text": "right, we're just hoping that by optimizing at the outer loop, it'll do something reasonable.",
    "start": "451475",
    "end": "458190"
  },
  {
    "text": "Okay, let's think about a different approach. So what if we made the inner loop look just like the outer loop?",
    "start": "460210",
    "end": "468139"
  },
  {
    "start": "461000",
    "end": "597000"
  },
  {
    "text": "So remember the outer loop is policy gradient, what if we just do policy gradient on the inner loop too? And you guys have seen this already, this is MAML, right, model-agnostic meta-learning.",
    "start": "468140",
    "end": "476960"
  },
  {
    "text": "And so the idea is we're trying to find some parameters Theta, such that when we take a few gradient steps on that Theta we'll get to",
    "start": "476960",
    "end": "485060"
  },
  {
    "text": "a Theta-star that's optimal for a given MDP, right. So here we'd expect Theta-star 3 to be optimal for",
    "start": "485060",
    "end": "491120"
  },
  {
    "text": "your skateboarding and Theta-star 2 to be optimal for surfing. So again, in our little figure up on the right,",
    "start": "491120",
    "end": "497180"
  },
  {
    "text": "now the outer optimization is PG and the inner one is also.",
    "start": "497180",
    "end": "501870"
  },
  {
    "text": "So what's a pro of this? It's consistent because now the adaptation step is gradient descent, right,",
    "start": "504580",
    "end": "511930"
  },
  {
    "text": "and we know what gradient descent converges to it, converges in non-linear settings to local minima.",
    "start": "511930",
    "end": "518274"
  },
  {
    "text": "And it's clear that in the limit of enough data, it would just reduce to the reinforcement-learning.",
    "start": "518275",
    "end": "524014"
  },
  {
    "text": "So we know that we would eventually acquire their optimal behavior. So this is a nice property.",
    "start": "524015",
    "end": "529660"
  },
  {
    "text": "One con is that it's not as expressive as the recurrent model, um,",
    "start": "529660",
    "end": "536925"
  },
  {
    "text": "so question, can you think of an example, an example MDP, um,",
    "start": "536925",
    "end": "543240"
  },
  {
    "text": "in which the recurrent method would be more expressive?",
    "start": "543240",
    "end": "547029"
  },
  {
    "text": "So what happens if the rewards are zero? Or in general if the rewards are sparse?",
    "start": "553890",
    "end": "559945"
  },
  {
    "text": "So if the rewards are sparse then what does the policy gradient look- say, say you get one trajectory and you get- collect zero rewards,",
    "start": "559945",
    "end": "567055"
  },
  {
    "text": "what does the policy gradient update look like? It's zero. Like you don't update at all. Which is kind of silly because you actually should've learned",
    "start": "567055",
    "end": "574209"
  },
  {
    "text": "some information from getting no rewards, right? First of all, you learned about the dynamics of",
    "start": "574210",
    "end": "579310"
  },
  {
    "text": "the new environment, which may have changed. And second of all, you learned that that's a really bad place to go because you didn't get any rewards.",
    "start": "579310",
    "end": "585925"
  },
  {
    "text": "Um, but this formulation is not really able to capture that, whereas, um, the recurrent method we saw before would be able to.",
    "start": "585925",
    "end": "593750"
  },
  {
    "start": "597000",
    "end": "927000"
  },
  {
    "text": "Okay. So let's look about- look at how, um, MAML-based algorithms learn to explore.",
    "start": "597060",
    "end": "604375"
  },
  {
    "text": "So here's a computation graph of what's happening when you adapt with MAML.",
    "start": "604375",
    "end": "610330"
  },
  {
    "text": "So you start with your policy, Pi Theta, right? On the left. And then you collect some data, Tau,",
    "start": "610330",
    "end": "617440"
  },
  {
    "text": "some trajectories, and we use those trajectories to do an update u, which is a step of policy gradient,",
    "start": "617440",
    "end": "624685"
  },
  {
    "text": "and that gets us our adapted policy, Pi Theta prime, in the middle, right?",
    "start": "624685",
    "end": "630010"
  },
  {
    "text": "Now what are we going to do? We're going to again, collect data with our adapted policy, that'll be Tau prime,",
    "start": "630010",
    "end": "636055"
  },
  {
    "text": "and we'll evaluate, right? We'll, we'll look at the rewards that we got for those trajectories. So the goal is to assign credit- like give credit for",
    "start": "636055",
    "end": "646000"
  },
  {
    "text": "those rewards to both the adapted policy and the pre-adapted policy, right?",
    "start": "646000",
    "end": "651700"
  },
  {
    "text": "Because the pre-adapted policy should get credit for doing whatever exploratory behavior led to good results later on.",
    "start": "651700",
    "end": "658420"
  },
  {
    "text": "So that's exactly what happens. So if you write out, um, the gradient, what you get is,",
    "start": "658420",
    "end": "666325"
  },
  {
    "text": "um, is this kind of credit assignment where it takes the structure of the problem into account.",
    "start": "666325",
    "end": "672550"
  },
  {
    "text": "So it takes that causal relationship between, um, the Tau prime and the Theta parameters into account when,",
    "start": "672550",
    "end": "680274"
  },
  {
    "text": "uh, when we compute that gradient. And, um, RL-squared is similar, right?",
    "start": "680275",
    "end": "686470"
  },
  {
    "text": "Except for right now it's happening in the context of an RNN. So through backprop through time, the steps you took earlier in the trajectory are getting",
    "start": "686470",
    "end": "693750"
  },
  {
    "text": "credit for the rewards received at the end of the trajectory. And if you want, um, a few more details and,",
    "start": "693750",
    "end": "701805"
  },
  {
    "text": "and the derivation for how this works out, it's, it's kind of interesting and, uh, you can find it in this paper, uh,",
    "start": "701805",
    "end": "707560"
  },
  {
    "text": "Rothfuss et al, 2018 and I also have the reference at the end of the slides.",
    "start": "707560",
    "end": "712130"
  },
  {
    "text": "Okay. So we've just kind of hand waved our way through with the, um,",
    "start": "714540",
    "end": "719850"
  },
  {
    "text": "with the idea that these algorithms are capable of learning to explore, and if we could optimize them perfectly they would learn good exploration trajectories.",
    "start": "719850",
    "end": "728269"
  },
  {
    "text": "So let's look at how they actually do in practice. So this, on the left, is the recurrent approach that we saw first,",
    "start": "728270",
    "end": "735955"
  },
  {
    "text": "and the goal here is to adapt to new maze configurations. So it's been trained on a bunch of different-looking mazes and",
    "start": "735955",
    "end": "743080"
  },
  {
    "text": "now we're asking it to adapt to this maze layout here. And so the goal is you always start in",
    "start": "743080",
    "end": "748690"
  },
  {
    "text": "the blue square and you want to end up in the red square. And so what we're seeing on the left is first some exploration trajectories,",
    "start": "748690",
    "end": "755140"
  },
  {
    "text": "and then on the right the final trajectory that the recurrent policy produces. And so we see pretty nice coverage of the maze here, right?",
    "start": "755140",
    "end": "762580"
  },
  {
    "text": "It goes way out, kinda explores one direction, and then, and then goes back. So it's probably covering more than 50% of the maze.",
    "start": "762580",
    "end": "769910"
  },
  {
    "text": "Um, and then on the right, we have a different kind of example where, um,",
    "start": "770100",
    "end": "775165"
  },
  {
    "text": "we have gradient-based meta-learning in a kind of navigation setting. So what's happening here is the rewards are initially sparse around the initial position,",
    "start": "775165",
    "end": "785365"
  },
  {
    "text": "and then you start getting some reward signal as you move further away. So it requires you to learn",
    "start": "785365",
    "end": "790930"
  },
  {
    "text": "exploration trajectories that explore far enough away from the initial position. And, and in this case it's working quite well, right?",
    "start": "790930",
    "end": "797140"
  },
  {
    "text": "You have the blue trajectories as the exploration trajectories, and they're successfully going far enough away to get more signal",
    "start": "797140",
    "end": "804084"
  },
  {
    "text": "such that the yellow trajectories to solve the task. I have a question. Yeah.",
    "start": "804085",
    "end": "809334"
  },
  {
    "text": "I'm still not sure I'm understanding the recurrent method [inaudible]. If you're learning a hidden state and an RNN that then just generalizes well at a certain task. Yep.",
    "start": "809335",
    "end": "818259"
  },
  {
    "text": "Yep. Yeah so you just make the policy and RNN, uh, you keep rolling out across",
    "start": "819000",
    "end": "825519"
  },
  {
    "text": "episodes and you train it across a whole set of tasks. Yeah. [inaudible] and then, um,",
    "start": "825520",
    "end": "834790"
  },
  {
    "text": "for policy gradient, If we're reversing a policy gradient here, would it be possible to swap in a Q-learning for policy gradients?",
    "start": "834790",
    "end": "841390"
  },
  {
    "text": "Um, it does not work well or at all,",
    "start": "841390",
    "end": "846490"
  },
  {
    "text": "in pretty much, uh, all of the research so far. It's definitely an active area of research.",
    "start": "846490",
    "end": "853120"
  },
  {
    "text": "Um, so in the second half of the talk, I'm going to talk about one approach that does work for off policy RL,",
    "start": "853120",
    "end": "859495"
  },
  {
    "text": "but certainly combining these approaches with off policy RL is a very interesting and actively researched topic but we haven't figured it out yet.",
    "start": "859495",
    "end": "867590"
  },
  {
    "text": "Okay, um, but in other cases of course it doesn't work so well. So here's an example of- um,",
    "start": "871500",
    "end": "878575"
  },
  {
    "text": "here's gradient-based meta-learning trying to explore in this sparse reward environment, and so I'm just showing two different environments here.",
    "start": "878575",
    "end": "886495"
  },
  {
    "text": "In the first agent is the ant and the second it's this kind of cart thing, and on the right I'm plotting- um, so,",
    "start": "886495",
    "end": "893260"
  },
  {
    "text": "so the job of the agent is to navigate out to one of those little red circles, kind of on the unit circle.",
    "start": "893260",
    "end": "898645"
  },
  {
    "text": "And so to explore effectively, it needs to be able to reach that outer rim right so that I can",
    "start": "898645",
    "end": "904510"
  },
  {
    "text": "get any kind of rewards that it can figure out how to complete the task. And so then what's being plotted in these diagrams is",
    "start": "904510",
    "end": "911140"
  },
  {
    "text": "the actual explanation trajectories that you get when you run MAML. And as you can see, they're not able to explore coherently enough in order to solve these tasks,",
    "start": "911140",
    "end": "921040"
  },
  {
    "text": "so there's some more work needed here.",
    "start": "921040",
    "end": "924350"
  },
  {
    "text": "Okay. So let's think about what the problem is, and I want to start by just giving some intuition for the problem here.",
    "start": "926700",
    "end": "936745"
  },
  {
    "start": "927000",
    "end": "1365000"
  },
  {
    "text": "Let's see if these work or not. Oh, yes. Okay. So here's a video of a,",
    "start": "936745",
    "end": "943570"
  },
  {
    "text": "of a child exploring, or what I would call exploring, right? And as you can see,",
    "start": "943570",
    "end": "949389"
  },
  {
    "text": "her behavior is very kind of coherent. Like she picks a strategy and then she executes on",
    "start": "949390",
    "end": "955150"
  },
  {
    "text": "that strategy for some amount of time before switching, right? And as a result, she's able to kind of explore in much more meaningful states, right?",
    "start": "955150",
    "end": "963399"
  },
  {
    "text": "She's not waving her arm around out here away from the box. She's exploring in the states that are relevant",
    "start": "963400",
    "end": "969475"
  },
  {
    "text": "for inserting that shaped peg into the box, right? And then by contrast,",
    "start": "969475",
    "end": "975610"
  },
  {
    "text": "here's what exploration in RL usually looks like. So this is at hyper speed.",
    "start": "975610",
    "end": "981250"
  },
  {
    "text": "But as you can see, it's not doing- it's not exploring in the same kind of meaningful states, right?",
    "start": "981250",
    "end": "986860"
  },
  {
    "text": "It's spending most of its time not interacting with the object and just kind of like doing random exploration in the joints.",
    "start": "986860",
    "end": "994075"
  },
  {
    "text": "So what we'd really like is to develop a method that, that has this temporal coherency that we see on the left, right?",
    "start": "994075",
    "end": "1001780"
  },
  {
    "text": "Okay. So now we're getting a bit more technical. Let's think about this problem that more.",
    "start": "1003440",
    "end": "1009735"
  },
  {
    "text": "So we know that we want exploration to be stochastic, right? Um, because we need to collect a whole bunch of diverse data from different behaviors,",
    "start": "1009735",
    "end": "1018465"
  },
  {
    "text": "and whereas optimal behavior is deterministic. So optimal behavior and exploration policies are quite different, right?",
    "start": "1018465",
    "end": "1025035"
  },
  {
    "text": "And it can be hard to represent both of those in the same policy, especially when you have time-invariant action distributions.",
    "start": "1025035",
    "end": "1033105"
  },
  {
    "text": "And typical methods of adding noise to get that stochasticity are time-invariant, right?",
    "start": "1033105",
    "end": "1038730"
  },
  {
    "text": "So you might have a stochastic policy that's on the left, um, but then there is no correlation between time- between noise at each time step, right?",
    "start": "1038730",
    "end": "1046904"
  },
  {
    "text": "Or you could, um, noise the policy parameters, but again kinda same problem.",
    "start": "1046905",
    "end": "1052320"
  },
  {
    "text": "So we'd like a bit of a more, um, explicit way of getting these temporally coherent trajectories.",
    "start": "1052320",
    "end": "1059710"
  },
  {
    "text": "And it seems like we should be able to use our meta-training tasks to do this, right? We've got a whole set of tasks and we should be able to",
    "start": "1061670",
    "end": "1070169"
  },
  {
    "text": "learn some kind of strategy that would work well for other tasks of that nature. So let's build on top of the gradient-based meta-RL that we saw so far.",
    "start": "1070170",
    "end": "1079695"
  },
  {
    "text": "And the idea here is we're going to augment the policy with a latent variable Z that will inject the structured exploration into the policy.",
    "start": "1079695",
    "end": "1090000"
  },
  {
    "text": "So we're going to do this by sampling a z and holding it- so I'm going to refer to that latent variable as z.",
    "start": "1090000",
    "end": "1097140"
  },
  {
    "text": "So we're going to sample a z and hold it constant across the entire episode, right? Which will give us our kind of input correlation.",
    "start": "1097140",
    "end": "1104070"
  },
  {
    "text": "And then we'll enforce that, um, that, that z gives us something relevant to the task through meta-training.",
    "start": "1104070",
    "end": "1113980"
  },
  {
    "text": "And the way we're gonna do that is by adopting the z to give us optimal trajectories.",
    "start": "1113980",
    "end": "1119755"
  },
  {
    "text": "So the way this is gonna work, is we're start- we're going to start with a prior on z, right? We'll sample a z from that prior and use that to collect exploration trajectories,",
    "start": "1119755",
    "end": "1130015"
  },
  {
    "text": "then we'll adapt the z via gradient descent and we'll then enforce via meta-training that that gives us optimal trajectories for that task.",
    "start": "1130015",
    "end": "1140875"
  },
  {
    "text": "So in the RL diagram on the upper right again,",
    "start": "1140875",
    "end": "1145910"
  },
  {
    "text": "our outer loop is still policy gradient but our inner loop is now policy gradient on the z variables.",
    "start": "1145910",
    "end": "1152920"
  },
  {
    "text": "Any questions about this? Yeah. [inaudible]",
    "start": "1152920",
    "end": "1163000"
  },
  {
    "text": "Uh, no. Here, I'm using the gradient-based approach. So all the policies will just be feedforward,",
    "start": "1163000",
    "end": "1169304"
  },
  {
    "text": "um, and we'll do adaptation via gradient descent.",
    "start": "1169305",
    "end": "1173720"
  },
  {
    "text": "Okay. And of course, the result we're hoping to get is if your job is to pick up one of",
    "start": "1178610",
    "end": "1184000"
  },
  {
    "text": "these shapes here rather than doing kind of random incoherent exploration, we expect the agent to systematically explore by picking up each different block.",
    "start": "1184000",
    "end": "1192475"
  },
  {
    "text": "Okay. So this was a method that was published and called MAESN, and so here we're going to look at some results from that paper.",
    "start": "1192475",
    "end": "1199240"
  },
  {
    "text": "So here the task is to push one of these colored blocks across the table,",
    "start": "1199240",
    "end": "1205150"
  },
  {
    "text": "and the agent is this kind of disembodied gripper here. And so we're going to compare what happens when you do MAML and when you do MAESN,",
    "start": "1205150",
    "end": "1214805"
  },
  {
    "text": "which is that structured exploration we just talked about. So when you do MAML, we find that the explanation is kind of random and",
    "start": "1214805",
    "end": "1221289"
  },
  {
    "text": "incoherent and doesn't succeed at pushing any of the blocks across, and so it doesn't- it's not able to get a signal about which one is correct.",
    "start": "1221290",
    "end": "1227995"
  },
  {
    "text": "Whereas if we train with this latent variable model, the exploration policy systematically pushes blocks across",
    "start": "1227995",
    "end": "1236020"
  },
  {
    "text": "the table because it knows that the task distribution involves that, right? So it just needs to figure out which one.",
    "start": "1236020",
    "end": "1242540"
  },
  {
    "text": "Okay. Any more questions about, uh, this method? Yeah.",
    "start": "1244980",
    "end": "1250790"
  },
  {
    "text": "So this is like [inaudible] of, I guess, pre-training? Yeah. So this is the meta-trained policy. Yep. Yep.",
    "start": "1250830",
    "end": "1259140"
  },
  {
    "text": "[inaudible]",
    "start": "1259140",
    "end": "1271865"
  },
  {
    "text": "Can you elaborate? Um, I was just wondering if I should ask specific or if it was just, like, I- because you said you were- you were- you were a- adding in Z,",
    "start": "1271865",
    "end": "1279650"
  },
  {
    "text": "like, variable for each episode. And, like, wha- I was wondering if that changed or updated across episode and what [inaudible].",
    "start": "1279650",
    "end": "1284840"
  },
  {
    "text": "Ah, I see, I see. Um, right, so an episode only contains a single task.",
    "start": "1284840",
    "end": "1291143"
  },
  {
    "text": "Yeah. You can't switch tasks in the middle of an episode. Um, so if you were to think about",
    "start": "1291143",
    "end": "1296810"
  },
  {
    "text": "[NOISE] the more realistic setting of kind of continual learning, then we would basically break it up into,",
    "start": "1296810",
    "end": "1304220"
  },
  {
    "text": "uh, we could potentially make episode and task be the same boundary just whenever the reward changes or the dynamics change.",
    "start": "1304220",
    "end": "1311610"
  },
  {
    "text": "Um, right. So, you have kind of choices here about how long you want to stick with the current, like,",
    "start": "1314790",
    "end": "1323480"
  },
  {
    "text": "variable and that's kind of heuristic. Cool.",
    "start": "1323480",
    "end": "1334365"
  },
  {
    "text": "[inaudible]? The reward? [inaudible]. Um, I believe the reward is sparse until you get so far to the right.",
    "start": "1334365",
    "end": "1345080"
  },
  {
    "text": "So you need the- you need, um, yeah I'm not sure if- you might get a reward when you move the wrong block too.",
    "start": "1345080",
    "end": "1355280"
  },
  {
    "text": "I'm not sure. But I- I think it's only for the right one.",
    "start": "1355280",
    "end": "1358710"
  },
  {
    "text": "Okay. So, let's take a bit of stock of where we are.",
    "start": "1364630",
    "end": "1369830"
  },
  {
    "start": "1365000",
    "end": "1723000"
  },
  {
    "text": "So, what do we want from a Meta-RL algorithm? Um, and we're going to compare the three that we've looked at so far.",
    "start": "1369830",
    "end": "1376820"
  },
  {
    "text": "So we have the recurrent one, the gradient one or MAML, and the structured exploration one or MAESN.",
    "start": "1376820",
    "end": "1382925"
  },
  {
    "text": "So, we talked about consistency. We said that the recurrent one's not consistent,",
    "start": "1382925",
    "end": "1388010"
  },
  {
    "text": "but the gradient one and its derivative, of course, are. Talked about expressivity and why the gradient based ones are ma- not as expressive.",
    "start": "1388010",
    "end": "1397850"
  },
  {
    "text": "We talked about structured exploration and how we can, like, endow the policy with the capability for structured exploration.",
    "start": "1397850",
    "end": "1404929"
  },
  {
    "text": "[NOISE] But finally, what about efficient and off-policy algorithms?",
    "start": "1404930",
    "end": "1410765"
  },
  {
    "text": "So everything that we talked about so far has been based on policy gradient, and policy gradient is very sample inefficient, right?",
    "start": "1410765",
    "end": "1418700"
  },
  {
    "text": "Because- partly because it's on-policy. So it has to collect new data at each time it wants to do an update.",
    "start": "1418700",
    "end": "1424340"
  },
  {
    "text": "Um, so we haven't looked at this question at all yet, um, but this question is pretty important.",
    "start": "1424340",
    "end": "1430205"
  },
  {
    "text": "And we know from single task reinforcement learning that off-policy algorithms are one to two orders of magnitude more efficient in terms of samples, right?",
    "start": "1430205",
    "end": "1438050"
  },
  {
    "text": "And they can achieve just as high a performance. So this is something that we'd really like. And it also makes a huge difference for real-world applications.",
    "start": "1438050",
    "end": "1445774"
  },
  {
    "text": "So if we're talking, for example, about wanting to run Meta-RL on a robot,",
    "start": "1445775",
    "end": "1450875"
  },
  {
    "text": "the- all the algorithms we just looked at would be kind of hopelessly, uh, sample inefficient.",
    "start": "1450875",
    "end": "1456065"
  },
  {
    "text": "I calculated with kind of a, um, frequency- control frequency of 20 hertz and, say,",
    "start": "1456065",
    "end": "1461480"
  },
  {
    "text": "you ran the robot continuously [NOISE], and it would take about a month to train them all on the robot, uh, for a- for a reasonable task distribution.",
    "start": "1461480",
    "end": "1469745"
  },
  {
    "text": "And so if we- if we could bring these, um, off-policy methods to bear, we would cut that down to maybe 10 hours and we could, uh, we could run that.",
    "start": "1469745",
    "end": "1477635"
  },
  {
    "text": "So this is a really important question that we're going to look at next.",
    "start": "1477635",
    "end": "1482700"
  },
  {
    "text": "Okay. So why is off-policy Meta-RL difficult?",
    "start": "1484480",
    "end": "1490715"
  },
  {
    "text": "As a disclaimer, this is a very much unresolved question and there's active research into trying to figure this out.",
    "start": "1490715",
    "end": "1500165"
  },
  {
    "text": "And it's also very related to why off-policy RL is hard too, so that's it- there- a lot of the reasons overlap [NOISE].",
    "start": "1500165",
    "end": "1508070"
  },
  {
    "text": "Um, but one reason that I want to point out is that a key characteristic",
    "start": "1508070",
    "end": "1513110"
  },
  {
    "text": "of meta-learning is that the conditions at meta-training and meta-test time should match, right?",
    "start": "1513110",
    "end": "1518434"
  },
  {
    "text": "So what I mean by this is- let's go back to the image classification case. And let's say that your meta-training distribution consisted of classifying these dogs.",
    "start": "1518435",
    "end": "1527120"
  },
  {
    "text": "These Dalmatians, German Shepherds, and Pugs. [NOISE] Now, when you go to, um,",
    "start": "1527120",
    "end": "1533015"
  },
  {
    "text": "learn a new class you're gonna have a pro- much easier time learning to classify Corgis, which are kind of in distribution versus,",
    "start": "1533015",
    "end": "1540799"
  },
  {
    "text": "say, motorcycles, which are not, [NOISE] right? And so we can make the same statement about RL.",
    "start": "1540800",
    "end": "1549020"
  },
  {
    "text": "So when we have a meta-trained policy, and we run it- we roll it out in our new task that's going to be on-policy, right?",
    "start": "1549020",
    "end": "1556700"
  },
  {
    "text": "We're going to be collecting on-policy data that we want to adapt from. But we decided we wanted to make use of off-policy data during meta-training.",
    "start": "1556700",
    "end": "1565325"
  },
  {
    "text": "So now, we're presented with this kind of distribution shift where we know that we have to be able to use on-policy data,",
    "start": "1565325",
    "end": "1572495"
  },
  {
    "text": "but we don't want to have to use it exclusively during meta-training. So that's what we're, um,",
    "start": "1572495",
    "end": "1579650"
  },
  {
    "text": "let's take a short break now, and then after the break we'll talk about, um, how we're going to address this problem of off-policy RL.",
    "start": "1579650",
    "end": "1587850"
  },
  {
    "text": "Okay. So we're gonna switch gears just a bit and talk about kind of a different way to think about meta-reinforcement learning,",
    "start": "1588040",
    "end": "1595970"
  },
  {
    "text": "and then we'll circle back to how this impacts our design of an off-policy algorithm.",
    "start": "1595970",
    "end": "1601860"
  },
  {
    "text": "Okay. So POMDPs. A POMDP is just a partially observed MDP,",
    "start": "1601870",
    "end": "1609710"
  },
  {
    "text": "right? So what does that mean? That means that we have some state, which is unobserved to us,",
    "start": "1609710",
    "end": "1615410"
  },
  {
    "text": "but we have access to observations, which are here at narratives O, which give us kind of incomplete information about the state.",
    "start": "1615410",
    "end": "1623030"
  },
  {
    "text": "And so, an example of this, uh, might be that you have, uh, incomplete sensor data.",
    "start": "1623030",
    "end": "1629750"
  },
  {
    "text": "There might be occlusions. Um, perhaps you- to do the task you need an estimate of your velocity,",
    "start": "1629750",
    "end": "1635840"
  },
  {
    "text": "but you only have an estimate of your position. Things like this are kind of usual examples of a POMDP.",
    "start": "1635840",
    "end": "1642140"
  },
  {
    "text": "So it's a very, very general framework and in general very, very difficult to solve.",
    "start": "1642140",
    "end": "1648330"
  },
  {
    "text": "So let's see how we can put our meta-RL problem into this POMDP framework.",
    "start": "1648850",
    "end": "1655655"
  },
  {
    "text": "So, if we write down meta-RL as a kind of graphical model,",
    "start": "1655655",
    "end": "1660950"
  },
  {
    "text": "and then what we have is essentially an MDP, right? But where the rewards and",
    "start": "1660950",
    "end": "1666440"
  },
  {
    "text": "the dynamics functions are also dependent on this task variable. And in general, we don't observe the task variable, right?",
    "start": "1666440",
    "end": "1674120"
  },
  {
    "text": "That's our job during adaptation to kind of- we could think of adaptation as figuring out what that task is, right?",
    "start": "1674120",
    "end": "1681140"
  },
  {
    "text": "And so we can just redefine things a little bit and put this in the POMDP formalism.",
    "start": "1681140",
    "end": "1686990"
  },
  {
    "text": "So, let's just call that hidden state to be the concatenation between our true state and the task,",
    "start": "1686990",
    "end": "1692285"
  },
  {
    "text": "and now we don't observe it anymore because we don't know the task, and we'll just redefine our observations to be,",
    "start": "1692285",
    "end": "1698855"
  },
  {
    "text": "um, the state and the rewards. So you could extend this a little bit more into a partially observed meta-RL problem,",
    "start": "1698855",
    "end": "1707240"
  },
  {
    "text": "right, where you- there's also some other part of the state that you don't observe. Say, you are doing meta-RL from images or something.",
    "start": "1707240",
    "end": "1714049"
  },
  {
    "text": "Um, but for now, let's assume that we do observe the state and the only thing we don't observe is that task.",
    "start": "1714050",
    "end": "1720690"
  },
  {
    "start": "1723000",
    "end": "2029000"
  },
  {
    "text": "Okay. So, let's think about how we might go about trying to solve a POMDP.",
    "start": "1723520",
    "end": "1729695"
  },
  {
    "text": "So, a good strategy might be to maintain a belief distribution over what state we're in, right?",
    "start": "1729695",
    "end": "1737405"
  },
  {
    "text": "So, let's take this really toy grid world just for an example. So there's three states and they're called S0, S1,",
    "start": "1737405",
    "end": "1744620"
  },
  {
    "text": "and S2, and you're the, the circle agent, and you start in the far left. And in this MDP the- the goal is to get to state two, right?",
    "start": "1744620",
    "end": "1753424"
  },
  {
    "text": "And you get no rewards until you get to state two and then you get a reward of one. So, when you first start out the episode,",
    "start": "1753425",
    "end": "1759635"
  },
  {
    "text": "you don't know where you are, right? So you might assume a uniform prior over,",
    "start": "1759635",
    "end": "1765770"
  },
  {
    "text": "uh, what you think your state is assuming you have no other information. So that's what I'm illustrating there on the right with the blue boxes.",
    "start": "1765770",
    "end": "1773765"
  },
  {
    "text": "Um, okay. So now, let's say, we take an action left.",
    "start": "1773765",
    "end": "1779255"
  },
  {
    "text": "We hit the wall, we don't go anywhere, we're still in state zero, and we get a rewa- get a reward of zero.",
    "start": "1779255",
    "end": "1784655"
  },
  {
    "text": "So now, I know exactly where I am because I just hit the wall, right? So I can say with 100% certainty,",
    "start": "1784655",
    "end": "1790743"
  },
  {
    "text": "I'm in the leftmost state. Okay. So how does this work for when we're thinking about the meta-RL POMDP?",
    "start": "1790744",
    "end": "1800690"
  },
  {
    "text": "So now, we're considering a whole set of MDPs, right? So let's take the same grid world,",
    "start": "1800690",
    "end": "1808010"
  },
  {
    "text": "but let's define three MDPs on it. And I'm just going to change the reward function between MDPs.",
    "start": "1808010",
    "end": "1813275"
  },
  {
    "text": "So for MDP zero, you get a reward of one if you're in the leftmost, for MDP one, you get a reward if you're in the middle, and so forth.",
    "start": "1813275",
    "end": "1821390"
  },
  {
    "text": "So now, let's say you have observed where you are, but you don't know what the task is, right?",
    "start": "1821390",
    "end": "1827150"
  },
  {
    "text": "That's, that's your job at meta-RL. You wanna figure out what the task is. So let's say again that you start out with no prior knowledge,",
    "start": "1827150",
    "end": "1833560"
  },
  {
    "text": "you have a uniform belief that you could be in any of the tasks. And now, let's say,",
    "start": "1833560",
    "end": "1839740"
  },
  {
    "text": "you take the same action. You go left, you hit the wall, you're still in state zero, you get zero reward. Well, now you can update your belief,",
    "start": "1839740",
    "end": "1846560"
  },
  {
    "text": "right, because you know you're definitely not at MDP zero. Because if you were, you would have gotten a reward. So we could do this- and as we could take more actions,",
    "start": "1846560",
    "end": "1854780"
  },
  {
    "text": "we can update the belief more and more, so we have a better idea of what task we're in, right? And as we get a better idea,",
    "start": "1854780",
    "end": "1860840"
  },
  {
    "text": "we can act more and more optimally.",
    "start": "1860840",
    "end": "1863279"
  },
  {
    "text": "Okay. So now we have an idea for what our belief is and how we would go about updating it.",
    "start": "1866750",
    "end": "1873015"
  },
  {
    "text": "Um, how are you going to use it to pick actions? So basically what I mean is given this belief how does the policy use it to,",
    "start": "1873015",
    "end": "1881565"
  },
  {
    "text": "to do something to, to explore. So the strategy we're going to talk about is posterior sampling or Thompson sampling.",
    "start": "1881565",
    "end": "1889200"
  },
  {
    "text": "And the idea is that you take a sample from the belief and then act as if that sample was the truth.",
    "start": "1889200",
    "end": "1896520"
  },
  {
    "text": "So as an example of this suppose that we sampled, um, that we were in MDP 0, from our belief we picked the first bin.",
    "start": "1896520",
    "end": "1905924"
  },
  {
    "text": "So we should then take the action as if that was the MDP that we're in and the optimal action in that case",
    "start": "1905925",
    "end": "1913185"
  },
  {
    "text": "is to go left, right? Because then you hit the wall and you stay in the state and you would accrue rewards if that was the MDP that you were in.",
    "start": "1913185",
    "end": "1920549"
  },
  {
    "text": "Of course in this case, it's not the MDP that you're in, right? So then you can update your belief as we have there on",
    "start": "1920550",
    "end": "1926759"
  },
  {
    "text": "the lower right and you can repeat the procedure. You can sample again. Maybe this time you sample that you're in MDP 2, right?",
    "start": "1926760",
    "end": "1932760"
  },
  {
    "text": "And then you would move right because that would be the optimal strategy. So as you do this posterior sampling,",
    "start": "1932760",
    "end": "1939634"
  },
  {
    "text": "you're smoothly trading off between exploration and exploitation, right? Because as you collect more experience,",
    "start": "1939634",
    "end": "1946415"
  },
  {
    "text": "you're narrowing your belief distribution. So you've got a better and better idea of in this case what task you're in.",
    "start": "1946415",
    "end": "1953559"
  },
  {
    "text": "Okay. So we're going to use that intuition to build an algorithm.",
    "start": "1955880",
    "end": "1961005"
  },
  {
    "text": "So just as we have before what we want to do is maintain a posterior belief over the task and update it.",
    "start": "1961005",
    "end": "1968640"
  },
  {
    "text": "So we're going to call that P of Z given C. So C here is going to denote contexts, but contexts or adaptation data, same thing.",
    "start": "1968640",
    "end": "1976755"
  },
  {
    "text": "Okay. So and we're gonna make it a continuous distribution. So this is our continuous belief distribution P of Z given",
    "start": "1976755",
    "end": "1982380"
  },
  {
    "text": "C. And we're going to get it from our adaptation data, right?",
    "start": "1982380",
    "end": "1988540"
  },
  {
    "text": "Then we'll sample from that distribution. And our agent will use samples from that belief in order to explore.",
    "start": "1988550",
    "end": "1996630"
  },
  {
    "text": "Just like we talked about with posterior sampling. So now kinda going back again to our little RL framework on the upper right hand corner.",
    "start": "1996630",
    "end": "2005919"
  },
  {
    "text": "Now our function f which used to be for example an RNN used to be gradient descent,",
    "start": "2005920",
    "end": "2012640"
  },
  {
    "text": "now, it's a stochastic encoder, right? It's that phi thing that's going to take our data and produce our belief distribution.",
    "start": "2012640",
    "end": "2021450"
  },
  {
    "text": "So before we get into how we're going to make all of this work practically,",
    "start": "2023020",
    "end": "2028354"
  },
  {
    "text": "let's take a look at the results that we want to get. So this would be- this is",
    "start": "2028354",
    "end": "2034580"
  },
  {
    "start": "2029000",
    "end": "2107000"
  },
  {
    "text": "posterior sampling in action for a real continuous control domain. So you're the agent, you start at that gray circle,",
    "start": "2034580",
    "end": "2042005"
  },
  {
    "text": "and you know from meta training that your goal is somewhere on that unit circle kind of denoted as a- as a arc.",
    "start": "2042005",
    "end": "2049655"
  },
  {
    "text": "But you don't know where on the unit circle your goal is. And you only get a reward when you're within the current goal.",
    "start": "2049655",
    "end": "2058234"
  },
  {
    "text": "So you- in this example, you only get a reward when you're within that blue circle. So what we would hope is that if that belief represents our kind of task distribution,",
    "start": "2058235",
    "end": "2069875"
  },
  {
    "text": "right, which we know is all along that arc. Then when we sample from the belief essentially what we're",
    "start": "2069875",
    "end": "2075589"
  },
  {
    "text": "doing is we're sampling different goals along that arc. Right. And when we sample a goal what do we do?",
    "start": "2075590",
    "end": "2081619"
  },
  {
    "text": "We act optimally according to it. So we act as if for example the goal is way over there on the right-hand side.",
    "start": "2081620",
    "end": "2088310"
  },
  {
    "text": "Of course, we seem to find out that it's not because we get zero reward, but this gives us kind of an,",
    "start": "2088310",
    "end": "2093649"
  },
  {
    "text": "um, easy way to eliminate hypotheses and narrow our estimate for where the goal would be.",
    "start": "2093650",
    "end": "2100109"
  },
  {
    "text": "And this is actually data from the algorithm that we're going to build up.",
    "start": "2100240",
    "end": "2105810"
  },
  {
    "text": "Okay. So how do we make this work in practice? Well, inferring the true posterior P of Z given C as intractable.",
    "start": "2106510",
    "end": "2115040"
  },
  {
    "start": "2107000",
    "end": "2298000"
  },
  {
    "text": "So we're going to use variational inference. So we'll approximate the posterior with Q of Z given",
    "start": "2115040",
    "end": "2120440"
  },
  {
    "text": "C. And we'll parameterize that distribution by Phi. So it will, that distribution will be the output of some non-linear neural network.",
    "start": "2120440",
    "end": "2130805"
  },
  {
    "text": "Um, and similarly, we will give an approximation for the prior. And so we can write down the ELBO just like this.",
    "start": "2130805",
    "end": "2137800"
  },
  {
    "text": "And this is the same ELBO that you guys saw before when Chelsea drives variational inference just probably with a bit of different notation.",
    "start": "2137800",
    "end": "2144789"
  },
  {
    "text": "Um, so the first term, right, is the likelihood term and the second term we might call the regularization term or the information bottleneck.",
    "start": "2144790",
    "end": "2155600"
  },
  {
    "text": "So let's think about semantically what this equation means, right? So we're trying to maximize",
    "start": "2155600",
    "end": "2164299"
  },
  {
    "text": "this likelihood term subject to some- to the regularization constraint, right?",
    "start": "2164299",
    "end": "2169685"
  },
  {
    "text": "And the likelihood term is supposed to give us a belief over the task. So what's supervision can we use for a belief over that task?",
    "start": "2169685",
    "end": "2177740"
  },
  {
    "text": "Um, so we have choices. We could for example reconstruct the reward and transition functions.",
    "start": "2177740",
    "end": "2183965"
  },
  {
    "text": "Right. If we did that, then we know that our representation has the information of what",
    "start": "2183965",
    "end": "2189440"
  },
  {
    "text": "MDP it's it- it is because it's able to reconstruct the rewards and transitions. Um, another option we can use which is the one we're gonna choose",
    "start": "2189440",
    "end": "2197540"
  },
  {
    "text": "here is we could adopt the Bellman error. And so this will just be essentially training a Q function.",
    "start": "2197540",
    "end": "2203839"
  },
  {
    "text": "And of course Bellman error is not actually likelihood, but if you squint a little and you read the control as inference tutorial,",
    "start": "2203840",
    "end": "2210920"
  },
  {
    "text": "it might start to look like one. And so we're just going to wave our hands a bit and,",
    "start": "2210920",
    "end": "2216050"
  },
  {
    "text": "and say that that's okay. Um, maybe a more intuitive way to think about it though rather than is",
    "start": "2216050",
    "end": "2221720"
  },
  {
    "text": "the Bellman error likelihood is to think about it as information bottleneck. So in the information bottleneck point of view,",
    "start": "2221720",
    "end": "2228050"
  },
  {
    "text": "you want to maximize that first term, that R term, right, subject to the- subject to the KL constraint.",
    "start": "2228050",
    "end": "2233869"
  },
  {
    "text": "And what that means is you want to pass enough information through that Z such that you get good predictions for R, right? Such that you get good Bellman error.",
    "start": "2233870",
    "end": "2244369"
  },
  {
    "text": "But you don't want to pass any more information than that. And that's what the KL term does. The KL term tries to maximize the entropy so it says less information.",
    "start": "2244370",
    "end": "2253145"
  },
  {
    "text": "The denominator says more information. And so as you have these kind of two competing objectives which you",
    "start": "2253145",
    "end": "2258440"
  },
  {
    "text": "get is just the information you need for the Bellman error. Does that make sense? Any questions about this?",
    "start": "2258440",
    "end": "2271350"
  },
  {
    "text": "Cool. Okay. So one more thing before we're ready to put the algorithm together is,",
    "start": "2277600",
    "end": "2286925"
  },
  {
    "text": "um, we didn't really talk about what the structure of that phi is, right? I just kind of waved my hands and said it was a, ah,",
    "start": "2286925",
    "end": "2293270"
  },
  {
    "text": "neural network that takes in all of your experience and produces this distribution. Great. But let's look inside that a bit more and see how we want to design that element.",
    "start": "2293270",
    "end": "2301910"
  },
  {
    "start": "2298000",
    "end": "2625000"
  },
  {
    "text": "So remember our goal is to just infer belief over the task. And to do that because of the Markov property,",
    "start": "2301910",
    "end": "2309500"
  },
  {
    "text": "we don't actually have to retain the order of the transitions that we saw them in, right? So it doesn't matter, um,",
    "start": "2309500",
    "end": "2316205"
  },
  {
    "text": "for all the transitions you've seen so far, it doesn't matter that we encode them in the order we saw them.",
    "start": "2316205",
    "end": "2321800"
  },
  {
    "text": "Because of the Markov property all we need is S, A, S prime R, right? And that defines the reward function and the transitions.",
    "start": "2321800",
    "end": "2331700"
  },
  {
    "text": "So we can encode them in a totally permutation invariant way. Which is exactly what we're gonna do.",
    "start": "2331700",
    "end": "2336755"
  },
  {
    "text": "And so what we're gonna do is just take each tuple S, A, S prime R and encode it independently to produce",
    "start": "2336755",
    "end": "2343760"
  },
  {
    "text": "a Gaussian factor and then multiply those Gaussian factors together to get a Gaussian posterior. And we're just gonna use Gaussians because they make the reparameterization trick nice.",
    "start": "2343760",
    "end": "2353825"
  },
  {
    "text": "Um, so why would we want to do this as opposed to using an RNN?",
    "start": "2353825",
    "end": "2358984"
  },
  {
    "text": "Um, well mostly for implementation reasons. Um, because this is- this approach is going to be,",
    "start": "2358985",
    "end": "2365885"
  },
  {
    "text": "um, faster to train, more stable to train, and generally kind of simple architectures are gonna- are going to work better.",
    "start": "2365885",
    "end": "2373984"
  },
  {
    "text": "Ah, you can also replace this with an RNN and probably with a bit more tuning, ah, it will also- it will also work.",
    "start": "2373985",
    "end": "2382110"
  },
  {
    "text": "Okay. So we're almost ready to construct the algorithm, but now we need to talk about off policy RL so we can put that piece in.",
    "start": "2382450",
    "end": "2390930"
  },
  {
    "text": "Okay. So here, ah, we're going to build on the algorithm soft actor-critic.",
    "start": "2391210",
    "end": "2397865"
  },
  {
    "text": "Um, and I'm not gonna go too deeply into this, but I'll give some pointers for references if you're interested.",
    "start": "2397865",
    "end": "2404525"
  },
  {
    "text": "Um, so let's just kind of look at it from a high level. So the soft part, what does that mean?",
    "start": "2404525",
    "end": "2409870"
  },
  {
    "text": "That means maximize rewards, right? So in this equation at the top, reward, maximum expected returns that's same from policy gradient.",
    "start": "2409870",
    "end": "2417325"
  },
  {
    "text": "Um, but then also maximize the entropy of the policy. And they do this in soft actor-critic because",
    "start": "2417325",
    "end": "2424100"
  },
  {
    "text": "they want- it gives them some better exploration properties, ah, when they're- when they're learning.",
    "start": "2424100",
    "end": "2430640"
  },
  {
    "text": "Um. Oops. Okay. And then the actor-critic part. What does that mean?",
    "start": "2430640",
    "end": "2438335"
  },
  {
    "text": "Well, we just modeled both the actor and the critic as kind of separate agents, right?",
    "start": "2438335",
    "end": "2444109"
  },
  {
    "text": "And we're going to train the critic which is the Q function here with the Bellman error.",
    "start": "2444110",
    "end": "2449630"
  },
  {
    "text": "And then we're going to train the actor to take actions",
    "start": "2449630",
    "end": "2454924"
  },
  {
    "text": "that give us high Q values and also give us high entropy.",
    "start": "2454925",
    "end": "2461010"
  },
  {
    "text": "And for the derivation of how all of this kind of falls out, ah,",
    "start": "2462130",
    "end": "2467765"
  },
  {
    "text": "I would refer you to the control as inference tutorial by Sergey Levine as well as the original SAC paper.",
    "start": "2467765",
    "end": "2474905"
  },
  {
    "text": "Is this enough detail for going  forward? Yeah.",
    "start": "2474905",
    "end": "2480030"
  },
  {
    "text": "I have also [inaudible] is this [inaudible] when people say the policy is getting [inaudible]",
    "start": "2480030",
    "end": "2491255"
  },
  {
    "text": "Right. So it's- here it's not, um, yeah.",
    "start": "2491255",
    "end": "2499085"
  },
  {
    "text": "Ba- yeah, it's basically the same. Uh-huh. [inaudible]",
    "start": "2499085",
    "end": "2515430"
  },
  {
    "text": "Um, I think that",
    "start": "2515430",
    "end": "2524990"
  },
  {
    "text": "would take maybe more di- divergence, um, but basically, like they're- so",
    "start": "2524990",
    "end": "2535520"
  },
  {
    "text": "SAC is derived from a framework where you model the whole thing as,",
    "start": "2535520",
    "end": "2541850"
  },
  {
    "text": "um, a probabilistic graphical model. And then you can, um,",
    "start": "2541850",
    "end": "2547505"
  },
  {
    "text": "basically relate the probability of your trajectory being optimal to your- the exponent of your rewards.",
    "start": "2547505",
    "end": "2554570"
  },
  {
    "text": "And once you kind of make this assumption, then you can, um, then you can basically, like,",
    "start": "2554570",
    "end": "2561275"
  },
  {
    "text": "that assumption basically leads directly to this algorithm. And, uh, HUC like doesn't start from that kind of framework.",
    "start": "2561275",
    "end": "2569790"
  },
  {
    "text": "Okay. So, uh, just of- just some results from this algorithm,",
    "start": "2576100",
    "end": "2582845"
  },
  {
    "text": "um, it does pretty good on humanoid and if they ran it on this kinda cool claw robot at Berkeley and it learns to turn the valve.",
    "start": "2582845",
    "end": "2590839"
  },
  {
    "text": "Um, and so this- this algorithm right now is, is, uh, the kind of highest performing in more sample efficient algorithm that, that we're using.",
    "start": "2590840",
    "end": "2599630"
  },
  {
    "text": "And so we're really interested in being able to build on top of this algorithm for our meta-learning uses.",
    "start": "2599630",
    "end": "2606210"
  },
  {
    "text": "Okay. So here's kind of a simplified soft actor-critic in a diagram form.",
    "start": "2607120",
    "end": "2612710"
  },
  {
    "text": "Um, and so it's soft policy, we're gonna maintain a replay buffer as we collect data.",
    "start": "2612710",
    "end": "2618335"
  },
  {
    "text": "And we're going to optimize both the actor and the critic with their respective losses.",
    "start": "2618335",
    "end": "2623820"
  },
  {
    "text": "Okay. So now let's add our task belief distribution on top of this algorithm. So how's that gonna work?",
    "start": "2624700",
    "end": "2632270"
  },
  {
    "start": "2625000",
    "end": "2783000"
  },
  {
    "text": "So again, we're going to maintain this approximate belief posterior,",
    "start": "2632270",
    "end": "2638645"
  },
  {
    "text": "that Q of Z given C, right? And we're going to take samples from it and pass those to the actor and critics.",
    "start": "2638645",
    "end": "2644210"
  },
  {
    "text": "So now the actor and critic essentially are getting a state that's the concatenation of the original state of the MDP with the Z variable, right?",
    "start": "2644210",
    "end": "2652339"
  },
  {
    "text": "And where Z here includes, is representing a, uh, task ID in some sense, right?",
    "start": "2652340",
    "end": "2660260"
  },
  {
    "text": "And that KL term comes from the, uh, the regularization from the variational inference as we talked about before.",
    "start": "2660260",
    "end": "2668435"
  },
  {
    "text": "And you'll notice that we back-propagate the critic clause into the encoder. And that's the Bellman era likelihood term that we saw before.",
    "start": "2668435",
    "end": "2677040"
  },
  {
    "text": "Okay. So now let's just look at some results from this algorithm that we've put together.",
    "start": "2682600",
    "end": "2688430"
  },
  {
    "text": "Um, so I'm gonna show results on these four domains. Um, and for the agents,",
    "start": "2688430",
    "end": "2694310"
  },
  {
    "text": "we're gonna be looking under the cheetah, humanoid, art, and walker. And we'll look at meta training distribution is that",
    "start": "2694310",
    "end": "2700430"
  },
  {
    "text": "differ both in reward function and in dynamics. So for the first three agents, we're gonna look at variable reward function.",
    "start": "2700430",
    "end": "2706760"
  },
  {
    "text": "So that means, um, which direction you move in, your target velocity, or a goal position.",
    "start": "2706760",
    "end": "2713225"
  },
  {
    "text": "And then for the final agent, we'll look at varying dynamics. So that will be like the- the masses and physical parameters of the agent.",
    "start": "2713225",
    "end": "2722190"
  },
  {
    "text": "So here is the first three ste- the first algorithms that we looked at.",
    "start": "2724840",
    "end": "2730880"
  },
  {
    "text": "Um, excuse me, evaluated on these tasks. So in, uh, that light greenish color,",
    "start": "2730880",
    "end": "2738200"
  },
  {
    "text": "we have, um, RL squared which is an instantiation of the recurrent method that we talked about at the beginning.",
    "start": "2738200",
    "end": "2743660"
  },
  {
    "text": "And then in purple and in yellow, we have kind of two variants of the gradient based approach.",
    "start": "2743660",
    "end": "2749375"
  },
  {
    "text": "And so this is just kind of for baseline, how well things do. And we're on a log scale here which is why the curves kind",
    "start": "2749375",
    "end": "2755510"
  },
  {
    "text": "of trend up like outward that way. Okay. And then in blue here is the algorithm that we just put together.",
    "start": "2755510",
    "end": "2764089"
  },
  {
    "text": "So be- it's- as you can see it's about 20 to 100 times more sample efficient,",
    "start": "2764090",
    "end": "2769160"
  },
  {
    "text": "and is also outperforming the previous approaches. And the reason it's able to do this is because it's leveraging that off",
    "start": "2769160",
    "end": "2775160"
  },
  {
    "text": "policy algorithm SAC that we just talked about in the meta-learning context.",
    "start": "2775160",
    "end": "2780569"
  },
  {
    "start": "2783000",
    "end": "3136000"
  },
  {
    "text": "Okay. So why does this view that we took of meta RL as a POMDP and wanting to have a belief over the task?",
    "start": "2784330",
    "end": "2793010"
  },
  {
    "text": "Why does that make off policy RL easier? Like going back to that question where I was showing the distribution of the dogs and",
    "start": "2793010",
    "end": "2799450"
  },
  {
    "text": "like it's better if you have a test SAS that's in distribution, how does this help us with that?",
    "start": "2799450",
    "end": "2804535"
  },
  {
    "text": "So the interesting thing is that this approach allows us to separate the data we use to",
    "start": "2804535",
    "end": "2810200"
  },
  {
    "text": "infer the task right at the top with the Phi from the data that we use to train the RL agent.",
    "start": "2810200",
    "end": "2816170"
  },
  {
    "text": "And so what this means is that we can address the distribution shifts by making that adaptation data on policy,",
    "start": "2816170",
    "end": "2824045"
  },
  {
    "text": "but making the rest of the data used to train the actor critic off policy. So- and this can still get us",
    "start": "2824045",
    "end": "2830630"
  },
  {
    "text": "big sample efficiency speed ups because if you think about it, probably a lot of the data that you're using is to figure out how to,",
    "start": "2830630",
    "end": "2838595"
  },
  {
    "text": "um, to figure out policy behaviors that are like common across all of the tasks, right? So in the example of navigating to different positions,",
    "start": "2838595",
    "end": "2846185"
  },
  {
    "text": "the knowledge of just how to walk at all is not task-specific, right?",
    "start": "2846185",
    "end": "2851495"
  },
  {
    "text": "And we can learn in that off policy through kind of the bottom half of this figure whereas the top half which goes through S_c",
    "start": "2851495",
    "end": "2858079"
  },
  {
    "text": "can be on policy. And we have this kind of knob that we can turn to test,",
    "start": "2858080",
    "end": "2863225"
  },
  {
    "text": "you know, how off policy can we make that data or not. Does that make sense? Yeah.",
    "start": "2863225",
    "end": "2869240"
  },
  {
    "text": "[inaudible] [NOISE] after exploring updating the- the inference parameters [inaudible]",
    "start": "2869240",
    "end": "2881090"
  },
  {
    "text": "Right. Uh-huh. [inaudible] a new thing. The only parameters that are changing",
    "start": "2881090",
    "end": "2887150"
  },
  {
    "text": "as I accumulate data for the new task, is the- is the SP,  is what you think?",
    "start": "2887150",
    "end": "2894694"
  },
  {
    "text": "So actually, no parameters change at all. All the parameters of the model are fixed.",
    "start": "2894695",
    "end": "2900290"
  },
  {
    "text": "So it's like the RNN case in that- in that regard, right? Where after meta-training is done,",
    "start": "2900290",
    "end": "2905300"
  },
  {
    "text": "your parameters are fixed. So then what's- what's changing here is the Z the- the belief distribution,",
    "start": "2905300",
    "end": "2911494"
  },
  {
    "text": "the Q of Z given C. So as you accumulate data, right? You're updating your estimate of the belief with,",
    "start": "2911494",
    "end": "2922310"
  },
  {
    "text": "with the meta-trained Phi parameters.",
    "start": "2922310",
    "end": "2925770"
  },
  {
    "text": "Yeah. Have you seen the variance on that [inaudible] decreasing over the course of the task?",
    "start": "2929000",
    "end": "2937945"
  },
  {
    "text": "Um, yeah. Okay, does it narrow down to, stays constant throughout the rest of the task?",
    "start": "2937945",
    "end": "2946210"
  },
  {
    "text": "Um, yeah. I mean, kind of by construction it- because of this multiplication,",
    "start": "2946210",
    "end": "2952780"
  },
  {
    "text": "it tends to decrease, um, and then at some point it- it just kind of stabilizes. Yeah. Yeah.",
    "start": "2952780",
    "end": "2967010"
  },
  {
    "text": "[inaudible] Yeah. [inaudible] like RL squared seem pretty [inaudible] well.",
    "start": "2967010",
    "end": "2974710"
  },
  {
    "text": "So I'm curious why ProMP which I'm not familiar with is doing better on humanoid direction TB, which is probably one of the harder tasks.",
    "start": "2974710",
    "end": "2985248"
  },
  {
    "text": "Yeah, yeah. Whereas RL squared is doing much better on everything else. Um, so why RL s, uh, oh.",
    "start": "2985248",
    "end": "2995930"
  },
  {
    "text": "Should- I use to know why RL squared or ProMP was better on that one.",
    "start": "2996030",
    "end": "3001455"
  },
  {
    "text": "There's like two versions of humanoid. Let me get back to you on that. Um, but in general, the different algorithms.",
    "start": "3001455",
    "end": "3008820"
  },
  {
    "text": "Um, so ProMP is basically MAML but it's implemented on top of a different policy gradient algorithm.",
    "start": "3008820",
    "end": "3017070"
  },
  {
    "text": "So MAML is built on top of TRPO which is trust reasoning policy optimization and proMP's built on top of PPO.",
    "start": "3017070",
    "end": "3024435"
  },
  {
    "text": "And there's some, like little things you have to do to make that work. PPO in general it's kind of- it is about better performing algorithms.",
    "start": "3024435",
    "end": "3032670"
  },
  {
    "text": "So in general ProMP outperforms MAML. And then RL squared here also is implemented on top of PPO which actually",
    "start": "3032670",
    "end": "3041460"
  },
  {
    "text": "makes it improve a lot more over previous results which were usually plotted with TRPO.",
    "start": "3041460",
    "end": "3046589"
  },
  {
    "text": "So [NOISE] there's a lot of variables in RL and [LAUGHTER] um,",
    "start": "3046590",
    "end": "3053295"
  },
  {
    "text": "you kind of have to get all of them right to totally have a+ comparison. Um, the other thing,",
    "start": "3053295",
    "end": "3059955"
  },
  {
    "text": "while I'm thinking about the humanoid thing, um, the other thing I would say is that these curves are a bit misleading in terms of",
    "start": "3059955",
    "end": "3066645"
  },
  {
    "text": "if you actually care about what the agents are doing it's not really working in in in some of the cases.",
    "start": "3066645",
    "end": "3071715"
  },
  {
    "text": "So for example in that humanoid case, um, what you want is the human or to run forward or backward or in different directions.",
    "start": "3071715",
    "end": "3079530"
  },
  {
    "text": "But wh- none of these algorithms are actually achieving that reward. So, uh, this algorithm in blue is",
    "start": "3079530",
    "end": "3087480"
  },
  {
    "text": "essentially just standing up for longer than other algorithms. Uh, so I think that kind of points",
    "start": "3087480",
    "end": "3095550"
  },
  {
    "text": "to the difficulty of optimization in Meta-RL in general. Because we know we can train a single task policy,",
    "start": "3095550",
    "end": "3103965"
  },
  {
    "text": "uh, with SAC on humanoid, right? But when we go to the meta case, we're not able to achieve that same performance.",
    "start": "3103965",
    "end": "3113655"
  },
  {
    "text": "So that the curves can be a little misleading. In some cases. And that's the only one that it.",
    "start": "3113655",
    "end": "3122010"
  },
  {
    "text": "That's the only environment here where that's true. The other ones actually do the tasks to some degree.",
    "start": "3122010",
    "end": "3127780"
  },
  {
    "text": "Um, okay. We talked about that.",
    "start": "3129290",
    "end": "3135329"
  },
  {
    "text": "So I'll talk about some limitations of this method. Um, so posterior sampling is pretty good.",
    "start": "3135330",
    "end": "3142484"
  },
  {
    "start": "3136000",
    "end": "3306000"
  },
  {
    "text": "And, uh, there's a reference at the end for a paper that that shows you can show a regret bound for posterior sampling. That's pretty good.",
    "start": "3142485",
    "end": "3151500"
  },
  {
    "text": "Um, but it's not optimal obviously. And here like this",
    "start": "3151500",
    "end": "3156930"
  },
  {
    "text": "the same diagram that I showed it off with also shows off why it's not optimal, right?",
    "start": "3156930",
    "end": "3161954"
  },
  {
    "text": "Because if you're in this environment the best thing to do is to go out to to the top to the arc and simply walk along the arc until you find the goal,",
    "start": "3161955",
    "end": "3170235"
  },
  {
    "text": "and posterior sampling can't do this. And so the algorithm we built also can't do this.",
    "start": "3170235",
    "end": "3176040"
  },
  {
    "text": "Um, so it's, in some cases it's going to have you know, potentially pretty bad exploration,",
    "start": "3176040",
    "end": "3182265"
  },
  {
    "text": "uh, pretty far from optimal exploration. So one kind of interesting way to look at this I think is is",
    "start": "3182265",
    "end": "3193500"
  },
  {
    "text": "to look at the difference between that MAESN algorithm that we saw in the first half and what we just designed here.",
    "start": "3193500",
    "end": "3200505"
  },
  {
    "text": "So to get a bit into the details, um, the difference is in how you constrain that latent distribution?",
    "start": "3200505",
    "end": "3208859"
  },
  {
    "text": "And if you constrain it, before you do the adaptation or after? And so graphically the difference is that in the algorithm we saw before,",
    "start": "3208860",
    "end": "3218985"
  },
  {
    "text": "um, the- so let's let the yellow be the prior distribution, right?",
    "start": "3218985",
    "end": "3224365"
  },
  {
    "text": "You start off with samples from the prior and then when you adapt, your distributions move away from that prior.",
    "start": "3224365",
    "end": "3231945"
  },
  {
    "text": "And that's because you're constraining the- the pre-updated distribution to be close to the unit Gaussian, right?",
    "start": "3231945",
    "end": "3238680"
  },
  {
    "text": "Not the post- updated one. So essentially that kind of separates your exploration trajectories which are",
    "start": "3238680",
    "end": "3245700"
  },
  {
    "text": "in the yellow region from your task optimal trajectories which are in the different blue regions depending on the task.",
    "start": "3245700",
    "end": "3251865"
  },
  {
    "text": "Versus the method we just described what we just built up this, er, the off-policy method PEARL.",
    "start": "3251865",
    "end": "3258675"
  },
  {
    "text": "What it's doing is it's constraining the post- adapted, um, z-distribution to the prior,",
    "start": "3258675",
    "end": "3266445"
  },
  {
    "text": "right? Remember that KL-regularization term. So essentially what that's doing is it's trying to get all of the task",
    "start": "3266445",
    "end": "3273210"
  },
  {
    "text": "distributions to kind of overlap with each other and with the prior. And so essentially in the,",
    "start": "3273210",
    "end": "3282075"
  },
  {
    "text": "in the PEARL approach, you're not going to learn- um, you have no chance of learning optimal exploration trajectories, right?",
    "start": "3282075",
    "end": "3289920"
  },
  {
    "text": "The latent distribution you learn is is simply over the task distribution and so all the exploration trajectories [NOISE] that you",
    "start": "3289920",
    "end": "3296730"
  },
  {
    "text": "sample will just be optimal ones for some other task. [NOISE] Okay,",
    "start": "3296730",
    "end": "3306765"
  },
  {
    "start": "3306000",
    "end": "4047000"
  },
  {
    "text": "Um, so in summary, we first looked at how building on policy gradient RL.",
    "start": "3306765",
    "end": "3313500"
  },
  {
    "text": "We can implement different Meta-RL methods with recurrence or with optimization.",
    "start": "3313500",
    "end": "3319080"
  },
  {
    "text": "And we looked at how adaptation of RL is both an exploration, and adaptation problem.",
    "start": "3319080",
    "end": "3325860"
  },
  {
    "text": "And we looked at how we can improve exploration by conditioning the policy on latent variables.",
    "start": "3325860",
    "end": "3332325"
  },
  {
    "text": "And in the second half, we looked at how meta-RL is a particular kind of POMDP.",
    "start": "3332325",
    "end": "3338280"
  },
  {
    "text": "And how using that kind of intuition. Um, we can build up an algorithm that estimates a belief over the task and integrates well",
    "start": "3338280",
    "end": "3345750"
  },
  {
    "text": "with off-policy RL. Any other questions? Yeah.",
    "start": "3345750",
    "end": "3355170"
  },
  {
    "text": "Seems like this algorithm is a lot less stable in certain regions than the other algorithms, do you know why that would be? Um, here?",
    "start": "3355170",
    "end": "3363420"
  },
  {
    "text": "Yes. I'm not sure if that's actually the case or if it's actually a problem with how we record it.",
    "start": "3363420",
    "end": "3372255"
  },
  {
    "text": "I think it might be a problem with the frequency of how we recorded the, um, the data.",
    "start": "3372255",
    "end": "3377310"
  },
  {
    "text": "Okay, on a log scale type? Yeah. It could- it could be a bit more and see what I mean",
    "start": "3377310",
    "end": "3384150"
  },
  {
    "text": "in general off-policy algorithms tend to be more unstable, than on-policy ones, though soft-actor critic in a single task results, uh,",
    "start": "3384150",
    "end": "3393075"
  },
  {
    "text": "is pretty stable. Yeah.",
    "start": "3393075",
    "end": "3398000"
  },
  {
    "text": "[inaudible]",
    "start": "3402590",
    "end": "3413320"
  },
  {
    "text": "In where? No matter wallpaper can be release by, um- Oh, yes. Yeah, the meta-wallpaper.",
    "start": "3413320",
    "end": "3419290"
  },
  {
    "text": "Yeah. [NOISE] Yeah, so that paper is- um, just to give some context, that paper is interesting.",
    "start": "3419290",
    "end": "3425365"
  },
  {
    "text": "Um, they made- so this is people at Berkeley and Centroid. They made a data set of, uh,",
    "start": "3425365",
    "end": "3431350"
  },
  {
    "text": "robot manipulation tasks in MuJoCo, like they have like 50 different tasks, and they're quite different tasks.",
    "start": "3431350",
    "end": "3437050"
  },
  {
    "text": "So here we're looking at things like cheetah running at 10 meters per second versus 20 meters per second.",
    "start": "3437050",
    "end": "3442615"
  },
  {
    "text": "But in this meta-world task distribution, they're actually looking at things, you know, like, picking, [NOISE] pushing, moving,",
    "start": "3442615",
    "end": "3449515"
  },
  {
    "text": "um, things that you would actually consider to be different tasks. And they evaluate these algorithms that we've looked at on this,",
    "start": "3449515",
    "end": "3455530"
  },
  {
    "text": "ah, benchmark of tasks. And, um, yeah, I'm",
    "start": "3455530",
    "end": "3460870"
  },
  {
    "text": "not- I actually haven't seen the results myself, I've just heard about it. Um, so I would say that in general,",
    "start": "3460870",
    "end": "3469915"
  },
  {
    "text": "um- so off-policy RL to begin with is, is,",
    "start": "3469915",
    "end": "3477955"
  },
  {
    "text": "um, less stable than on policy, ah, partly for like the same distribution shift reason",
    "start": "3477955",
    "end": "3484240"
  },
  {
    "text": "that we talked about earlier, right? Like you're expected to learn from data that's not from your current distribution, right?",
    "start": "3484240",
    "end": "3492789"
  },
  {
    "text": "Um, so that's, that's one problem. And the second problem going against it is the meta-training optimization is itself hard,",
    "start": "3492790",
    "end": "3500995"
  },
  {
    "text": "and that's hard even just in a multi-task setting. So for example, forget about the adaptation stuff and just try to train a policy that",
    "start": "3500995",
    "end": "3510309"
  },
  {
    "text": "can do 50 different tasks given some indication of what task it's solving.",
    "start": "3510310",
    "end": "3515695"
  },
  {
    "text": "That in itself is a difficult optimization problem because what ends up happening is the gradients for the different tasks cannot, kind of,",
    "start": "3515695",
    "end": "3522640"
  },
  {
    "text": "interfere with each other and, like, kind of, lock and prevent the, the model from learning anything.",
    "start": "3522640",
    "end": "3529405"
  },
  {
    "text": "So I think those two problems combined are, are what's causing the issue and I,",
    "start": "3529405",
    "end": "3536005"
  },
  {
    "text": "I think looking at each of them in isolation will, kind of, yield progress that we can then put back together. Yeah.",
    "start": "3536005",
    "end": "3543490"
  },
  {
    "text": "So I kind of have two questions, the first is when you are like storing the, um,",
    "start": "3543490",
    "end": "3552930"
  },
  {
    "text": "experience, like the replay buffer, do you store as well like the optimal Z, like,",
    "start": "3552930",
    "end": "3559530"
  },
  {
    "text": "like variables that you would find after doing the inner loop updates step to the replay buffer?",
    "start": "3559530",
    "end": "3567780"
  },
  {
    "text": "[NOISE] Um- Or is that not part of the experience- [OVERLAPPING]?",
    "start": "3567780",
    "end": "3574079"
  },
  {
    "text": "No, but, um, sort of,",
    "start": "3574080",
    "end": "3579165"
  },
  {
    "text": "in the sense that, um- so, so sort of yes,",
    "start": "3579165",
    "end": "3584770"
  },
  {
    "text": "but through the data rather than through the Zs. So what I mean by that is, when we collect data, we collect,",
    "start": "3584770",
    "end": "3590545"
  },
  {
    "text": "um, some of it with a policy condition on a Z from the prior, and some of it with a Z that's conditioned on,",
    "start": "3590545",
    "end": "3600565"
  },
  {
    "text": "um- sorry, with a Z that's computed from posterior trajectories. So essentially, you get that effect,",
    "start": "3600565",
    "end": "3608049"
  },
  {
    "text": "but it's through the data in the buffer. So you just start the [inaudible] from the data? Uh-huh. Okay. And then the second question was,",
    "start": "3608050",
    "end": "3615700"
  },
  {
    "text": "if you were to start at Z, um, can you use that somehow as a way of, like,",
    "start": "3615700",
    "end": "3620920"
  },
  {
    "text": "prioritizing the experience that you're sampling from the replay buffer to not have this distributionship problem because you're only going to select, like,",
    "start": "3620920",
    "end": "3630819"
  },
  {
    "text": "experience that's similar to the task that you're currently doing, um, based on the latency variable that you are using in the posterior,",
    "start": "3630820",
    "end": "3640734"
  },
  {
    "text": "that I need to be able to stay current. So you can do, like, a sort of, like, ah, probabilistic sampling over the experience using Z.",
    "start": "3640735",
    "end": "3650290"
  },
  {
    "text": "Um, yeah so doing some kind of like priority- prioritization over the replay buffer. Um-",
    "start": "3650290",
    "end": "3659170"
  },
  {
    "text": "Like that, I don't know. You want it to- [inaudible]",
    "start": "3659170",
    "end": "3664390"
  },
  {
    "text": "So yeah, I'm not sure how addressing the distribution shift, shift explicitly,",
    "start": "3664390",
    "end": "3672339"
  },
  {
    "text": "I mean- I guess- So okay, we're kind of, um, we're kind of,",
    "start": "3672340",
    "end": "3679720"
  },
  {
    "text": "we're kind of doing that, right, when we sample the context from the replay buffers so that SC thing is,",
    "start": "3679720",
    "end": "3685119"
  },
  {
    "text": "is representing like a sampling operator, which, essentially, only samples the most recent things so that the data is,",
    "start": "3685120",
    "end": "3692005"
  },
  {
    "text": "is on policy as possible. Um, so that seems like,",
    "start": "3692005",
    "end": "3697945"
  },
  {
    "text": "kind of, what you're getting at in terms of addressing the distribution shifts. Um, yeah, in general,",
    "start": "3697945",
    "end": "3705220"
  },
  {
    "text": "there's- I mean, there's other techniques for, like, how you might better sample from the replay buffer that you could probably marry with this. Yeah.",
    "start": "3705220",
    "end": "3712900"
  },
  {
    "text": "[inaudible] so at test time you don't sample any other experience from the- from like training at all?",
    "start": "3712900",
    "end": "3718840"
  },
  {
    "text": "Or it's just basically just that same experience they are collecting on all [inaudible]. Right. Because, remember, at test time it's a brand new task.",
    "start": "3718840",
    "end": "3728360"
  },
  {
    "text": "Yeah. Yeah. So one- um, well, okay, that's another thing,",
    "start": "3728360",
    "end": "3734230"
  },
  {
    "text": "but, yes, yeah, sorry. [inaudible] you're training both your encoder and your SAC in the directory, right?",
    "start": "3734230",
    "end": "3741015"
  },
  {
    "text": "Uh-huh. So the actor-critic, when it comes to a complete variable is,",
    "start": "3741015",
    "end": "3746625"
  },
  {
    "text": "kind of, tracking the new new target, right? Because your embedding is changing during the meta-training. Yep.",
    "start": "3746625",
    "end": "3751695"
  },
  {
    "text": "Do you use, like, target networks or anything [inaudible] or does everything kind of blend well? Yeah. So SAC uses a target network, um,",
    "start": "3751695",
    "end": "3759415"
  },
  {
    "text": "already, and we just- yeah, we also use that. Um, yeah, it is a bit of a moving target.",
    "start": "3759415",
    "end": "3768025"
  },
  {
    "text": "Um, one thing you could've considered doing is, ah,",
    "start": "3768025",
    "end": "3773454"
  },
  {
    "text": "if you- like if you are not using the Bellman error to train the encoder, if you're using, like,",
    "start": "3773455",
    "end": "3778855"
  },
  {
    "text": "reconstructing the rewards and the transitions, then you could train that offline beforehand, right, and then do this.",
    "start": "3778855",
    "end": "3785365"
  },
  {
    "text": "Um, and I haven't experimented with that in particular, I've experimented with training it all in parallel,",
    "start": "3785365",
    "end": "3793045"
  },
  {
    "text": "but with a reconstruction- the reward reconstruction objective instead of development error.",
    "start": "3793045",
    "end": "3798445"
  },
  {
    "text": "And it, um, it does a bit worse. Yeah.",
    "start": "3798445",
    "end": "3806529"
  },
  {
    "text": "So also at meta and test time you're sampling your context framework at the beginning of the episode, and then preconditioning your context distribution on,",
    "start": "3806530",
    "end": "3815710"
  },
  {
    "text": "like, the trajectory that you set, could you imagine if you- at every step of your episode,",
    "start": "3815710",
    "end": "3823690"
  },
  {
    "text": "re-sample your context parameter? You lose your coherence of [inaudible] , I don't know if you tried that.",
    "start": "3823690",
    "end": "3832675"
  },
  {
    "text": "Yeah. I think I spec- so it, it- that does work, um, in past situations where exploration is not as important.",
    "start": "3832675",
    "end": "3839095"
  },
  {
    "text": "So for example, in the MuJoCo benchmarks, where you get, um, dense rewards, ah, at every time step that actually works fine.",
    "start": "3839095",
    "end": "3849040"
  },
  {
    "text": "Um, but it, it doesn't work, for example, in the sparse, uh- this guy, this case.",
    "start": "3849040",
    "end": "3856030"
  },
  {
    "text": "Um, but in general, I think getting an off-policy method that can learn",
    "start": "3856030",
    "end": "3862959"
  },
  {
    "text": "op- that has that property of being able to learn optimal exploration trajectories, is kind of an open question.",
    "start": "3862959",
    "end": "3869710"
  },
  {
    "text": "I haven't got anything towards that yet.",
    "start": "3869710",
    "end": "3872270"
  },
  {
    "text": "Actually, if you want to carry over some statement across all of the different tasks, like, [inaudible] squared?",
    "start": "3881970",
    "end": "3891865"
  },
  {
    "text": "So in RL-squared you don't carry the hidden state across the task. Okay. Only the episode, right, because",
    "start": "3891865",
    "end": "3897609"
  },
  {
    "text": "the hidden state in RL-squared is supposed to tell you what task you are in, essentially, or it's supposed to give you some task-specific information.",
    "start": "3897610",
    "end": "3905150"
  },
  {
    "text": "Um, so one interesting idea which we tried and didn't work, um,",
    "start": "3905460",
    "end": "3911589"
  },
  {
    "text": "is sort of getting at this idea of, like, can we use different data from different tasks for,",
    "start": "3911590",
    "end": "3918339"
  },
  {
    "text": "for- so sorry, can I take data from task A and, like, use it for task B? Like why can't I do that? That's annoying. Um, so if you wanted to do that,",
    "start": "3918340",
    "end": "3926365"
  },
  {
    "text": "what you'd have to do is, like, relabel the data, right? So say- because say the reward function changes between task A and task B.",
    "start": "3926365",
    "end": "3933535"
  },
  {
    "text": "If I could relabel task A data with a task B reward function, I could use it, and that'd be great, right?",
    "start": "3933535",
    "end": "3940510"
  },
  {
    "text": "Um, so what we tried to do is, essentially, put a model for the rewards that change from Z.",
    "start": "3940510",
    "end": "3947770"
  },
  {
    "text": "So basically have Z reconstruct the rewards, and so we would, essentially, have a generative model of rewards,",
    "start": "3947770",
    "end": "3955135"
  },
  {
    "text": "and we train this across all of the meta-training tasks. So now, it should be that given a Z, I get some reward function,",
    "start": "3955135",
    "end": "3961944"
  },
  {
    "text": "and then I could use that reward function to relabel some data. Um, it didn't work.",
    "start": "3961944",
    "end": "3967775"
  },
  {
    "text": "But I think in- I think figuring out how to reuse data from tasks for each other",
    "start": "3967775",
    "end": "3975075"
  },
  {
    "text": "would be really useful in terms of making this more sample efficient.",
    "start": "3975075",
    "end": "3979690"
  },
  {
    "text": "In theory, [inaudible] conditional relabeling of the rewards, right? [OVERLAPPING] Yeah. Yep. So in that case, like, why does that work?",
    "start": "3983340",
    "end": "3990520"
  },
  {
    "text": "Um, is it because they're not translate [NOISE] to a generative model [inaudible] in that case? Yeah, probably.",
    "start": "3990520",
    "end": "3997160"
  },
  {
    "text": "Um, yeah.",
    "start": "4000590",
    "end": "4007050"
  },
  {
    "text": "I mean, if you're in a sparser reward case than your, your, ah- like any kind of signal is going to help you a lot,",
    "start": "4007050",
    "end": "4017280"
  },
  {
    "text": "versus if you are, in this case, where you have some labeled data and you just wanna, kind of, generate more to help you.",
    "start": "4017280",
    "end": "4023355"
  },
  {
    "text": "Like, it's, it's less clear if it'll help you, I guess. [NOISE] Cool.",
    "start": "4023355",
    "end": "4031515"
  },
  {
    "text": "Um, I heard this ends at 2:50. So thanks all for coming. [APPLAUSE]",
    "start": "4031515",
    "end": "4040000"
  }
]