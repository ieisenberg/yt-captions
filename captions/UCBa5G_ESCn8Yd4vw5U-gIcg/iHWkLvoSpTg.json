[
  {
    "start": "0",
    "end": "117000"
  },
  {
    "start": "0",
    "end": "5270"
  },
  {
    "text": "OK, so hi, everyone. Welcome back to CS224N.",
    "start": "5270",
    "end": "10470"
  },
  {
    "text": "So today we get to have the\nsecond of our invited speakers for this quarter. And so given all\nof the excitement",
    "start": "10470",
    "end": "17150"
  },
  {
    "text": "that there's been about\ntransformer pre-trained language models and\nall the great things",
    "start": "17150",
    "end": "22520"
  },
  {
    "text": "have been done with them,\nwe're especially excited today to be able to welcome\nColin Raffel who's",
    "start": "22520",
    "end": "28130"
  },
  {
    "text": "been one of the\nkey people who've been pushing the exploration\nof large pre-trained language",
    "start": "28130",
    "end": "33560"
  },
  {
    "text": "models. In particular, he was very\ninterested in the development of the T5 language model which\nhe'll be telling you plenty",
    "start": "33560",
    "end": "41060"
  },
  {
    "text": "about today. But to tell you a\nfew more sentences. So Colin worked for a number\nof years at Google Brain,",
    "start": "41060",
    "end": "49700"
  },
  {
    "text": "including working with Geoff\nHinton on capsule networks. He then got interested in\nthe effectiveness of transfer",
    "start": "49700",
    "end": "57320"
  },
  {
    "text": "using large pre-trained\nlanguage models. And as part of the\nwork on that, he started together\nwith other people",
    "start": "57320",
    "end": "64670"
  },
  {
    "text": "on building even bigger large\npre-trained language models and doing a lot\nof investigations",
    "start": "64670",
    "end": "69770"
  },
  {
    "text": "with those, which\nled to the T5 papers that you'll be\nhearing about today. So welcome, Colin.",
    "start": "69770",
    "end": "76560"
  },
  {
    "text": "Right, yeah, thanks so\nmuch for the introduction and for having me. It's definitely an honor to\nspeak at the legendary CS224N",
    "start": "76560",
    "end": "83229"
  },
  {
    "text": "class. So yeah, I'm going to be talking\ntoday about large language models kind of in general\nbut focusing specifically",
    "start": "83230",
    "end": "91670"
  },
  {
    "text": "on this model T5 that\nwe released about a year and a half ago. I'll be presenting five or\nso papers that kind of should",
    "start": "91670",
    "end": "99800"
  },
  {
    "text": "represent the full spectrum\nof good things, bad things, and ugly things about\nlarge language models.",
    "start": "99800",
    "end": "105380"
  },
  {
    "text": "And actually I'll talk a\nbit about a paper that just appeared on archive last night. So hopefully everyone will\nlearn something new today,",
    "start": "105380",
    "end": "112909"
  },
  {
    "text": "even if you're already familiar\nwith some of these papers. So just to give\nyou an idea of what",
    "start": "112910",
    "end": "119690"
  },
  {
    "start": "117000",
    "end": "426000"
  },
  {
    "text": "I'll be covering in\nthis talk, I kind of will be answering each of\nthese questions in turn over the course of\nthe presentation.",
    "start": "119690",
    "end": "128880"
  },
  {
    "text": "And I should mention that this,\nsince some of these papers are new and some of\nthis material is new,",
    "start": "128880",
    "end": "134628"
  },
  {
    "text": "this is the first time I'll\nbe presenting these slides. So if anything is\nconfusing, I understand. There's a way for\nyou to ping questions",
    "start": "134628",
    "end": "141290"
  },
  {
    "text": "to me through the hosts and feel\nfree to ask about anything that might turn out to be confusing.",
    "start": "141290",
    "end": "147209"
  },
  {
    "text": "So yeah, the first question\nthat I'll try to answer is kind of the\nanswer we sort out",
    "start": "147210",
    "end": "152840"
  },
  {
    "text": "to focus on in the T5 paper,\nwhich is which of the transfer",
    "start": "152840",
    "end": "158269"
  },
  {
    "text": "learning methods people have\nproposed so far work best, and what happens when\nwe scale them up?",
    "start": "158270",
    "end": "163409"
  },
  {
    "text": "And then after the\nT5 paper, we decided to investigate non-English\npre-trained language models.",
    "start": "163410",
    "end": "170510"
  },
  {
    "text": "T5 is an English only model. There are lots of languages\nspoken in the world. So what happens when we\nmodify T5 so that it's",
    "start": "170510",
    "end": "177260"
  },
  {
    "text": "a massively multilingual model? And then I'll talk\nabout another paper where we try to investigate\nwhat kinds of knowledge",
    "start": "177260",
    "end": "183890"
  },
  {
    "text": "and how much knowledge\na model picks up over the course of pre-training. And then relatedly\nin a follow up",
    "start": "183890",
    "end": "189830"
  },
  {
    "text": "work we tried to figure\nout if the model actually memorized this training\ndata during pre-training.",
    "start": "189830",
    "end": "195740"
  },
  {
    "text": "If it actually, if we\ncan get it to spit out verbatim entries\nfrom the training data set after\nit's been trained.",
    "start": "195740",
    "end": "202520"
  },
  {
    "text": "And then finally, I'll talk\nabout a very recent work that's similar in spirit to the\nT5 paper, where the goal is",
    "start": "202520",
    "end": "208640"
  },
  {
    "text": "to answer not what transfer\nlearning methods work best, but what modifications to\nthe transformer architecture",
    "start": "208640",
    "end": "214700"
  },
  {
    "text": "that people have\nproposed to work best. So just to motivate\nthis I actually",
    "start": "214700",
    "end": "220052"
  },
  {
    "text": "looked through some\nof the lectures that you all have\nhad so far just to get a sense of what\nyou've learned already.",
    "start": "220052",
    "end": "225350"
  },
  {
    "text": "And I know that you\nare, all are already pretty familiar with this\ntransfer learning paradigm that has kind of taken the field\nof natural language processing",
    "start": "225350",
    "end": "233000"
  },
  {
    "text": "by storm. But just as a quick refresher,\nin this style of transfer",
    "start": "233000",
    "end": "238340"
  },
  {
    "text": "learning what we typically do is\ntake a bunch of unlabeled text data and we apply some\nunsupervised objective,",
    "start": "238340",
    "end": "245900"
  },
  {
    "text": "or you might say\nself-supervised objective, where you do something like\nyou mask words at random and then you train the model\nto predict the missing words.",
    "start": "245900",
    "end": "252740"
  },
  {
    "text": "So you can see these\nblanks in the block of text in green and\nthe missing words that the language\nmodel will be trained",
    "start": "252740",
    "end": "259370"
  },
  {
    "text": "to predict in the yellow\nbox at the bottom. And then after we do this\npre-training for a while, we fine tune the model on some\ndownstream supervised task.",
    "start": "259370",
    "end": "269330"
  },
  {
    "text": "This example, I'm showing\na sentiment analysis task for movie reviews. And the upshot of all\nof this is that doing",
    "start": "269330",
    "end": "275660"
  },
  {
    "text": "this first unsupervised\npre-training step is just ridiculously helpful,\nnot only does it usually",
    "start": "275660",
    "end": "280730"
  },
  {
    "text": "make the performance\nbetter it often gets you very good performance\nwith relatively little fine tuning data compared\nto training from scratch.",
    "start": "280730",
    "end": "288090"
  },
  {
    "text": "So this is really\na very common-- it's kind of the de\nfacto standard way to attack many natural language\nprocessing problems now.",
    "start": "288090",
    "end": "296070"
  },
  {
    "text": "And I think you've already\nreviewed some of these methods but because of how effective\nthe transfer learning recipe",
    "start": "296070",
    "end": "303990"
  },
  {
    "text": "was there really was a kind\nof an explosion of work on transfer learning starting\nin maybe 2018, or so.",
    "start": "303990",
    "end": "310500"
  },
  {
    "text": "There some prior work\non other approaches to doing transfer learning like\nword vectors, like word2vec.",
    "start": "310500",
    "end": "316680"
  },
  {
    "text": "Some sort of preliminary\nwork that proposed the recipe that I just described. While some has supervised\nsequence learning,",
    "start": "316680",
    "end": "323080"
  },
  {
    "text": "some work that suggested\nthat this kind of stuff might be really possible, like\nthe unsupervised sentiment neuron paper.",
    "start": "323080",
    "end": "328590"
  },
  {
    "text": "But I would say\naround in 2018, there was kind of a string of\npapers that kicked off the excitement in\nthe field, including",
    "start": "328590",
    "end": "335460"
  },
  {
    "text": "the universal language\nmodel fine tuning paper, the ELMO paper what\nwe now call GPT-1,",
    "start": "335460",
    "end": "341160"
  },
  {
    "text": "and of course the BERT\npaper in late 2018. And then starting in\n2019, there really",
    "start": "341160",
    "end": "346320"
  },
  {
    "text": "was just an incredible\nexplosion of different methods for doing transfer\nlearning, including new transfer learning, sorry,\nnew pre-training objectives,",
    "start": "346320",
    "end": "355199"
  },
  {
    "text": "new data sets, new ways of\ndoing fine tuning, and so on. And we started working on\nthe T5 project in late 2018.",
    "start": "355200",
    "end": "364830"
  },
  {
    "text": "And we noticed that as all\nthese methods were coming up, it was getting harder\nand harder to figure out what actually works best.",
    "start": "364830",
    "end": "372270"
  },
  {
    "text": "And part of the reason for that\nwas just because there was so many methods that were\nbeing proposed kind of",
    "start": "372270",
    "end": "377790"
  },
  {
    "text": "simultaneously. And when that happens, even when\neveryone is working in earnest with good faith, you might\nhave situations like this",
    "start": "377790",
    "end": "386070"
  },
  {
    "text": "where one paper\ncomes along, paper A that proposes a new unsupervised\npre-training like technique",
    "start": "386070",
    "end": "391770"
  },
  {
    "text": "called FancyLearn, another\npaper comes along maybe around the same time with\na pre-training technique",
    "start": "391770",
    "end": "396930"
  },
  {
    "text": "called FancierLearn. And paper A pre-trains on\nWikipedia for unlabeled data while paper B uses Wikipedia and\nthe Toronto Books Corpus which",
    "start": "396930",
    "end": "404490"
  },
  {
    "text": "is a collection of novel text. And then the question\nobviously is, is FancierLearn",
    "start": "404490",
    "end": "409530"
  },
  {
    "text": "better than FancyLearn? Well, it's hard to say because\nthey use different sources of pre-training data.",
    "start": "409530",
    "end": "414630"
  },
  {
    "text": "You might imagine that maybe\nthey use different model sizes, maybe they pre-train for a\ndifferent amount of time.",
    "start": "414630",
    "end": "420870"
  },
  {
    "text": "They use different optimizers. There are tons of\ndesign decisions that come into play here.",
    "start": "420870",
    "end": "426550"
  },
  {
    "start": "426000",
    "end": "671000"
  },
  {
    "text": "And so given that these\ndesign decisions can make it hard to determine what\nworked best our goal in the T5",
    "start": "426550",
    "end": "433620"
  },
  {
    "text": "paper was kind of to\nstep back, and just say given the current\nlandscape of transfer learning,",
    "start": "433620",
    "end": "439200"
  },
  {
    "text": "all of the methods that\npeople have proposed, what actually works best\nwhen we compare them in the same exact setting?",
    "start": "439200",
    "end": "446139"
  },
  {
    "text": "And once we know\nwhat works best, how far can we push these\ntools that we already have?",
    "start": "446140",
    "end": "451710"
  },
  {
    "text": "And how much can we\nexplore the limits and figure out how well\nthese things work at scale?",
    "start": "451710",
    "end": "457760"
  },
  {
    "text": "And so to attack this problem\nwe kind of the only thing that we introduced,\nsince again we're",
    "start": "457760",
    "end": "463540"
  },
  {
    "text": "kind of exploring\nexisting techniques, was this idea of\ntreating all text",
    "start": "463540",
    "end": "469450"
  },
  {
    "text": "problems in the same format. And this kind of approach, this\ndogma of treating every text",
    "start": "469450",
    "end": "477460"
  },
  {
    "text": "problem in the same format\ngives rise to our model, which we call the text-to-text\ntransfer transformer.",
    "start": "477460",
    "end": "482720"
  },
  {
    "text": "And so to explain\nthis format to you, the basic idea is that we cast\nevery text based NLP problem",
    "start": "482720",
    "end": "489910"
  },
  {
    "text": "as a text-to-text task. And by that I mean, the\nmodel takes text as input and it produces text as output.",
    "start": "489910",
    "end": "496639"
  },
  {
    "text": "So in things like English\nto German translation, this is pretty typical, we\nfeed in a English sentence",
    "start": "496640",
    "end": "503070"
  },
  {
    "text": "on the input and\nwe train the model to predict a German\nsentence on the output. And you'll notice in\nour case, we actually",
    "start": "503070",
    "end": "509250"
  },
  {
    "text": "also feed what we call\na task prefix, translate English to German, that\njust tells the model what",
    "start": "509250",
    "end": "514320"
  },
  {
    "text": "we want it to do with the input\nbecause especially if we're training a multi-task model if\nyou just feed the model \"that",
    "start": "514320",
    "end": "519664"
  },
  {
    "text": "is good\" the model doesn't\nknow what to do with it. It doesn't know if you're\ntrying to do sentiment analysis, or English to German\ntranslation, or what.",
    "start": "519664",
    "end": "526770"
  },
  {
    "text": "So this shouldn't be\nthat surprising so far. You probably learned about\nencoder-decoder model, sequence",
    "start": "526770",
    "end": "533462"
  },
  {
    "text": "to sequence models,\nand that's basically all that we're doing here. It maybe gets a\nlittle more unusual when we start to tackle things\nlike text classification tasks.",
    "start": "533463",
    "end": "542040"
  },
  {
    "text": "So this is an example\nfrom the cola benchmark, which is the Corpus of\nLinguistic Acceptability.",
    "start": "542040",
    "end": "547639"
  },
  {
    "text": "And the goal here is\nto take a sentence and determine if the sentence\nis quote-unquote \"acceptable\"",
    "start": "547640",
    "end": "552740"
  },
  {
    "text": "which kind of means whether\nit's grammatically correct, and also if it's\nnonsense or not.",
    "start": "552740",
    "end": "558029"
  },
  {
    "text": "And in this case,\nthe sentence is \"The course is jumping well\". And of course, courses can't\njump so the sentence is not",
    "start": "558030",
    "end": "564470"
  },
  {
    "text": "acceptable. But rather than training our\nmodel through a classifier layer that outputs a class\nlabel, or a probability",
    "start": "564470",
    "end": "571550"
  },
  {
    "text": "distribution over\na class indices, we actually train the model to\noutput the literal text \"not",
    "start": "571550",
    "end": "577040"
  },
  {
    "text": "acceptable.\" So it's outputting this\ntext string token by token.",
    "start": "577040",
    "end": "582470"
  },
  {
    "text": "And it can get even\na little weirder. We can attack things\nlike regression problems where effectively the model\nis supposed to be outputting",
    "start": "582470",
    "end": "589850"
  },
  {
    "text": "a floating point value. And we actually just do this\nby taking the floating point numbers, and converting\nthem to strings,",
    "start": "589850",
    "end": "596810"
  },
  {
    "text": "and training the model to\njust predict the string. And it turns out that at least\nfor this particular task, which",
    "start": "596810",
    "end": "602150"
  },
  {
    "text": "is the STSb benchmark,\nit works perfectly fine. And we ultimately\nactually got state",
    "start": "602150",
    "end": "607422"
  },
  {
    "text": "of the art on this benchmark. So doing this type\nof sort of float to string conversion with a\nlittle bit of quantization",
    "start": "607422",
    "end": "614030"
  },
  {
    "text": "turns out to work fine\nfor regression problems. And finally, the\nmain point of this",
    "start": "614030",
    "end": "619700"
  },
  {
    "text": "is that there are really\ntons and tons of problems that we can cast\ninto this format. So here's an example of\nabstract summarization,",
    "start": "619700",
    "end": "626180"
  },
  {
    "text": "we feed in a news\narticle on the left, and we predict the\nsummary on the right. And again, we can really\nattack all of these problems",
    "start": "626180",
    "end": "633440"
  },
  {
    "text": "in exactly the same way. So we're using exactly the\nsame objective during training.",
    "start": "633440",
    "end": "638690"
  },
  {
    "text": "And exactly the same\ndecoding procedure at test time to attack a huge variety\nof natural language processing",
    "start": "638690",
    "end": "645770"
  },
  {
    "text": "problems. And the nicest part\nabout doing this is that as long as a transfer\nlearning improvement is",
    "start": "645770",
    "end": "651740"
  },
  {
    "text": "applicable to our model and\nto this text to text format, we can try it on a huge\nsuite of downstream tasks",
    "start": "651740",
    "end": "659600"
  },
  {
    "text": "while using exactly\nthe same model, exactly the same learning\nrate, optimizer, training procedure, exactly\nthe same inference procedure.",
    "start": "659600",
    "end": "667080"
  },
  {
    "text": "So we can get rid of a\nton of the confounders that I mentioned earlier. Hey, Colin.",
    "start": "667080",
    "end": "672320"
  },
  {
    "start": "671000",
    "end": "964000"
  },
  {
    "text": "Yeah. Can you take a second as\nwe're, as you're discussing the formatting here, to\ntalk about how the 3.8",
    "start": "672320",
    "end": "679430"
  },
  {
    "text": "sort of regression\nexample works, and yeah, and just go that\none more time for students.",
    "start": "679430",
    "end": "684842"
  },
  {
    "text": "Yeah, absolutely. So in this particular\ntask STSb, the goal is to take in two sentences\nand predict a floating point",
    "start": "684842",
    "end": "692089"
  },
  {
    "text": "number, which denotes\nhow similar those two sentences are, and that\nfloating point number ranges between 1.0 and 5.0.",
    "start": "692090",
    "end": "700070"
  },
  {
    "text": "So we basically took the\nground truth annotated values that we're supposed to\nregress, we quantized them",
    "start": "700070",
    "end": "706129"
  },
  {
    "text": "to the nearest 0.2 and\nwe cast it to a string. And now we have a string\nlike the three period eight,",
    "start": "706130",
    "end": "713269"
  },
  {
    "text": "not three point eight. But you could think of it\nlike three period eight or four period zero.",
    "start": "713270",
    "end": "718370"
  },
  {
    "text": "And we just train the model\nto predict that string, which ultimately it's a string. It's not a number anymore.",
    "start": "718370",
    "end": "724250"
  },
  {
    "text": "We just predict\nit token by token. So in a sense you can\nkind of think of it as converting the regression\nproblem to a classification",
    "start": "724250",
    "end": "731839"
  },
  {
    "text": "problem because you're\ndoing this quantization. But you also more broadly\ncan just think of it as converting the\nregression problem",
    "start": "731840",
    "end": "738170"
  },
  {
    "text": "to a text-to-text problem,\nwhich is what we're doing. Thanks. Yeah, it's a little funky\nbut I promise it works.",
    "start": "738170",
    "end": "748370"
  },
  {
    "text": "And great. So the nice thing,\nagain, about using this sort of\nsequence-to-sequence text-to-text format is\nthat we can actually",
    "start": "748370",
    "end": "754550"
  },
  {
    "text": "just use the original vanilla\ntransformer as it was proposed. Because if you remember the\ntransformer was actually",
    "start": "754550",
    "end": "760880"
  },
  {
    "text": "proposed for English\nto, well it was proposed for machine\ntranslation primarily, which is a sequence-to-sequence\ntask taking",
    "start": "760880",
    "end": "767959"
  },
  {
    "text": "a language-- a sentence\nin one language as input and producing the corresponding\nsentence in another language.",
    "start": "767960",
    "end": "774076"
  },
  {
    "text": "And so really, I won't say a lot\nabout the model that we used. There are relatively\nfew changes that we made to the standard\ntransformer architecture",
    "start": "774077",
    "end": "781040"
  },
  {
    "text": "as originally\nproposed in Attention Is All You Need when\nconstructing our T5 model.",
    "start": "781040",
    "end": "787933"
  },
  {
    "text": "I will towards the\nend of the talk discuss lots of\narchitectural modifications that people have made\nsince, but for T5,",
    "start": "787933",
    "end": "794210"
  },
  {
    "text": "for the T5 paper we really\nbasically stuck to the basics. ",
    "start": "794210",
    "end": "799790"
  },
  {
    "text": "So the next big question when\nyou're attacking a transfer learning problem is, what should\nmy pre-training data set be?",
    "start": "799790",
    "end": "806720"
  },
  {
    "text": "And because of the\ninternet, there are lots of possible sources\nof unlabeled text data.",
    "start": "806720",
    "end": "812300"
  },
  {
    "text": "One common source is Wikipedia. I'm displaying a ton\nof Wikipedia articles on the screen here. And in undertaking\nthis project, one",
    "start": "812300",
    "end": "821040"
  },
  {
    "text": "of the factors that\nwe wanted to study was the effect of the\npre-training data set itself.",
    "start": "821040",
    "end": "826470"
  },
  {
    "text": "And so we actually constructed\na new pre-training data set that would allow us to vary\nthe size across many orders",
    "start": "826470",
    "end": "832319"
  },
  {
    "text": "of magnitude, also that\nhas a filtering pipeline that would allow us to control\nthe quality and type of data",
    "start": "832320",
    "end": "838320"
  },
  {
    "text": "that was pre-trained on. And I'll describe how we\nbuilt that data set now.",
    "start": "838320",
    "end": "843800"
  },
  {
    "text": "So the first thing\nwe did is we wanted to source our data from a\npublicly available source.",
    "start": "843800",
    "end": "849760"
  },
  {
    "text": "We didn't want to use some\nGoogle internal web scrape that we couldn't release. So we made use of\nthese web scrapes",
    "start": "849760",
    "end": "855670"
  },
  {
    "text": "by a non-profit organization\ncalled Common Crawl which is really just an organization\nthat sends web crawlers out",
    "start": "855670",
    "end": "863641"
  },
  {
    "text": "through the internet and\ndownloads as much text as they can. And every month\nthey dump out what",
    "start": "863642",
    "end": "868660"
  },
  {
    "text": "they call web\nextraced text which is, you can think of them as\nwebsites with all of the HTML",
    "start": "868660",
    "end": "874330"
  },
  {
    "text": "and JavaScript ideally removed. And this produces text that\nhas a pretty decent amount",
    "start": "874330",
    "end": "880570"
  },
  {
    "text": "of natural language in it, also\na lot of boilerplate and menu",
    "start": "880570",
    "end": "885820"
  },
  {
    "text": "text, and also a little\nbit of gibberish. But as a whole it's kind\nof a good starting point for constructing these\npre-training data sets.",
    "start": "885820",
    "end": "894490"
  },
  {
    "text": "And then we took a few\nsteps to kind of try to make it so this data\nset was a little cleaner.",
    "start": "894490",
    "end": "899750"
  },
  {
    "text": "So the first thing\nwe did is we removed any lines that didn't end in\na terminal punctuation mark.",
    "start": "899750",
    "end": "904930"
  },
  {
    "text": "We used a language classifier\nto only retain English text. We removed anything that\nlooked like placeholder text,",
    "start": "904930",
    "end": "912700"
  },
  {
    "text": "like lorem ipsum\ntext on the right. We removed anything\nthat looked like code. We de-duplicated things\non a sentence level.",
    "start": "912700",
    "end": "919690"
  },
  {
    "text": "So any time that any chunk of\ntext appeared on multiple pages we only retained it on one\nof the pages and so on.",
    "start": "919690",
    "end": "926110"
  },
  {
    "text": "And ultimately these heuristics\nwere relatively simple but produced\nreasonably clean text.",
    "start": "926110",
    "end": "932230"
  },
  {
    "text": "And I will discuss later\nthe effect of these choices in cleaning. That was one of the\nexperiments that we ran.",
    "start": "932230",
    "end": "939370"
  },
  {
    "text": "And so after doing\nthis we created this data set called C4, which\nis the colossal clean crawl",
    "start": "939370",
    "end": "944410"
  },
  {
    "text": "corpus. And it's available in\nTensorFlow Datasets. You actually, you need to\ndo the processing yourself,",
    "start": "944410",
    "end": "951170"
  },
  {
    "text": "which is somewhat\ncomputationally expensive but nevertheless it\nis entirely possible.",
    "start": "951170",
    "end": "956829"
  },
  {
    "text": "And it produces about 750\ngigabytes of reasonably clean natural text data.",
    "start": "956830",
    "end": "964830"
  },
  {
    "start": "964000",
    "end": "1053000"
  },
  {
    "text": "OK, so now we have our\nframework, our model, our pre-training\ndataset, we need our pre-training objective.",
    "start": "964830",
    "end": "971050"
  },
  {
    "text": "What are we going to\ndo to train the model on our unlabeled text. And so just to explain the\nobjective that we chose kind",
    "start": "971050",
    "end": "979529"
  },
  {
    "text": "of for our baseline experimental\nprocedure, which again we will experiment with different\npre-training objectives later.",
    "start": "979530",
    "end": "985410"
  },
  {
    "text": "Imagine that you have some\noriginal text sentence like \"Thank you for inviting\nme to your party last week.\"",
    "start": "985410",
    "end": "991139"
  },
  {
    "text": "And what we do is we basically\nchoose some words at random, and technically we're\nchoosing tokens at random",
    "start": "991140",
    "end": "997410"
  },
  {
    "text": "but for now let's just\nassume that tokens are words, and we're going to\ndrop those tokens out.",
    "start": "997410",
    "end": "1002660"
  },
  {
    "text": "And so we end up with\nsomething that looks like this. For every consecutive\nspan of tokens that have been dropped\nout, we replace it",
    "start": "1002660",
    "end": "1009110"
  },
  {
    "text": "with a sentinel token. And each sentinel token\ngets a unique index. So one of them we call it, let's\njust call it the sentinel X",
    "start": "1009110",
    "end": "1016100"
  },
  {
    "text": "and the other one will\nbe the sentinel Y. And you can see that because\nthe words \"for\" and \"inviting\"",
    "start": "1016100",
    "end": "1021200"
  },
  {
    "text": "are subsequent words that\nwe decided to mask out by randomly masking words.",
    "start": "1021200",
    "end": "1026230"
  },
  {
    "text": "We're going to replace\nboth of those words with a single sentinel,\nsingle unique sentinel token.",
    "start": "1026230",
    "end": "1031579"
  },
  {
    "text": "And then the model's goal will\njust be to fill in the blanks. And so if you're familiar\nwith the BERT pre-training",
    "start": "1031579",
    "end": "1036920"
  },
  {
    "text": "objective, this is\nsomewhat similar. The fact that we're collapsing\nsubsequent tokens into a span",
    "start": "1036920",
    "end": "1043220"
  },
  {
    "text": "that we're going to be\nreplacing words from is slightly different. And the fact that we're\nreconstructing just the missing",
    "start": "1043220",
    "end": "1048830"
  },
  {
    "text": "words, and not the\nentire sequence is maybe slightly different too. OK, so now I'll\nkind of talk through",
    "start": "1048830",
    "end": "1056300"
  },
  {
    "start": "1053000",
    "end": "1237000"
  },
  {
    "text": "our baseline\nexperimental procedure that we're going to use. We're going to kind of tweak\nover time along various axes",
    "start": "1056300",
    "end": "1062929"
  },
  {
    "text": "to explore the landscape\nof transfer learning, at least circa when\nthis paper came out.",
    "start": "1062930",
    "end": "1069200"
  },
  {
    "text": "So to pre-train the\nmodel we're going to take a model that has a BERT\nbase size encoder and decoder.",
    "start": "1069200",
    "end": "1075090"
  },
  {
    "text": "So technically has twice\nas many parameters as BERT base, because there's a\nBERT base size encoder and a BERT base size decoder.",
    "start": "1075090",
    "end": "1081649"
  },
  {
    "text": "We're going to use the\ndenoising objective, the sort of masked\nlanguage modeling objective that I just described.",
    "start": "1081650",
    "end": "1086990"
  },
  {
    "text": "And we're going to apply\nit on the C4 dataset that I mentioned earlier. We're going to pre-train\nfor about 34 billion tokens,",
    "start": "1086990",
    "end": "1094100"
  },
  {
    "text": "which is about a quarter as\nlong as BERT base was trained. So it's not a ton\nof pre-training time but because we're\ntraining on, we're",
    "start": "1094100",
    "end": "1100430"
  },
  {
    "text": "doing so many\nexperiments we need to cut it back a little bit. We're going to use an\ninverse square root learning",
    "start": "1100430",
    "end": "1106340"
  },
  {
    "text": "rate schedule that turned\nout to work reasonably well in our setting but\nis not a terribly important design decision.",
    "start": "1106340",
    "end": "1112730"
  },
  {
    "text": "And then we'll fine tune on a\nvariety of downstream tasks, the kind of tasks that people\ncared a lot about at the time.",
    "start": "1112730",
    "end": "1118730"
  },
  {
    "text": "There's the GLUE\nbenchmark, which is kind of a meta benchmark\nof many individual downstream tasks, like cola and STSb\nthat I already mentioned.",
    "start": "1118730",
    "end": "1126410"
  },
  {
    "text": "These are what some people\nmight call natural language understanding tasks. But for the most part\nyou can think of them",
    "start": "1126410",
    "end": "1132800"
  },
  {
    "text": "as sentence classification,\nsentence pair classification, or regression tasks.",
    "start": "1132800",
    "end": "1139080"
  },
  {
    "text": "We also consider the\nCNN Daily Mail abstract of summarization corpus. This is a\nsequence-to-sequence problem",
    "start": "1139080",
    "end": "1144110"
  },
  {
    "text": "where you are given\na news article and you have to\noutput the summary. The SQuAD question\nanswering benchmark,",
    "start": "1144110",
    "end": "1149610"
  },
  {
    "text": "which is a reading comprehension\nbenchmark where you're given a paragraph and\nyou have to answer a question about the paragraph.",
    "start": "1149610",
    "end": "1155309"
  },
  {
    "text": "You can either attack it in\nan extractive setting where you extract the answer\nfrom the paragraph, or an abstractive setting where\nyou just output the answer.",
    "start": "1155310",
    "end": "1163430"
  },
  {
    "text": "We use the abstractive\nform because it's a text-to-text problem. We also included the\nSuperGLUE benchmark,",
    "start": "1163430",
    "end": "1169700"
  },
  {
    "text": "which was a new benchmark\nat the time that was designed to essentially\nbe a more difficult version of the GLUE benchmark. It has a new set of tasks that\nwere hard for existing models.",
    "start": "1169700",
    "end": "1178850"
  },
  {
    "text": "And then finally we included\nthree translation datasets, English to German, English to\nFrench, and English to Romanian",
    "start": "1178850",
    "end": "1184070"
  },
  {
    "text": "translation. English to French being\nthe largest, which was an extremely large dataset.",
    "start": "1184070",
    "end": "1189679"
  },
  {
    "text": "And English to Romanian being\nmany orders of magnitude smaller. And we're going to fine\ntune on each of these tasks",
    "start": "1189680",
    "end": "1196550"
  },
  {
    "text": "individually and separately. So we take the pre-trained\nmodel and separately fine tune it on each of\nthese downstream tasks.",
    "start": "1196550",
    "end": "1202160"
  },
  {
    "text": "And we're going to fine tune\nfor up to 17 billion tokens. But we're going to save\ncheckpoints along the way,",
    "start": "1202160",
    "end": "1207980"
  },
  {
    "text": "evaluate each checkpoint\non the validation set, and report the performance\non the best checkpoint.",
    "start": "1207980",
    "end": "1213530"
  },
  {
    "text": "And note that this is not\nan experimentally valid way to report your\nperformance because you're",
    "start": "1213530",
    "end": "1219230"
  },
  {
    "text": "basically doing model\nselection on the dataset that you are, on the\ndata split that you are-- you're reporting performance\non the data split",
    "start": "1219230",
    "end": "1226340"
  },
  {
    "text": "that you're doing\nmodel selection on which is not a good way\nto compare different methods. But to compare\nwithin methods, we're",
    "start": "1226340",
    "end": "1234080"
  },
  {
    "text": "reasonably comfortable\ndoing this. So now I'm going to kind of\ngive you a very high level",
    "start": "1234080",
    "end": "1241077"
  },
  {
    "start": "1237000",
    "end": "1372000"
  },
  {
    "text": "overview of some of the\nexperimental results in this paper. This paper is pretty huge\nin terms of just the number",
    "start": "1241077",
    "end": "1247770"
  },
  {
    "text": "of experiments we ran. And so if I was to really\ndrill into this paper, it probably would\ntake me the whole time",
    "start": "1247770",
    "end": "1253500"
  },
  {
    "text": "but there's other fun stuff that\nI want to tell you about so. But anyways the point is I will\nbe showing you lots of tables",
    "start": "1253500",
    "end": "1259620"
  },
  {
    "text": "like this one. And so in these\ntables on the columns you have the performance on\nthe various downstream tasks",
    "start": "1259620",
    "end": "1264997"
  },
  {
    "text": "and on the rows you have\ndifferent experimental settings that we considered. So to give you an example,\nhere is kind of the scores",
    "start": "1264997",
    "end": "1271409"
  },
  {
    "text": "that we got from\nour baseline which is the exact experimental\nprocedure that I must describe,",
    "start": "1271410",
    "end": "1278429"
  },
  {
    "text": "that I just described. And we also ran that\nbaseline 10 times and reporting the standard\ndeviation on the second line.",
    "start": "1278430",
    "end": "1285420"
  },
  {
    "text": "And then in this\nlast line here we're reporting the performance\nof the same model",
    "start": "1285420",
    "end": "1290460"
  },
  {
    "text": "without any pre-training,\njust basically only trained separately with supervision on\nall of these downstream tasks.",
    "start": "1290460",
    "end": "1296700"
  },
  {
    "text": "And just to point out a couple\nof things on this table. The first obvious thing\nis that in most cases,",
    "start": "1296700",
    "end": "1303390"
  },
  {
    "text": "the no pre-training setting\nis dramatically worse. So indeed transfer learning\ndoes tend to be helpful.",
    "start": "1303390",
    "end": "1310340"
  },
  {
    "text": "The place where that's\nnot true is actually on this English to\nFrench translation task. And that's probably because\nit's such a big task",
    "start": "1310340",
    "end": "1317240"
  },
  {
    "text": "that you actually\ndon't really need pre-training to do well on it. We wanted to include this\nbecause if the performance",
    "start": "1317240",
    "end": "1323750"
  },
  {
    "text": "regresses on this\ntask then that's something that we\nshould be worried about. The next feature of\nthis table to notice",
    "start": "1323750",
    "end": "1330650"
  },
  {
    "text": "is this little star appeared. That star will appear\nany time there's a row in the table that's\nequivalent to our baseline.",
    "start": "1330650",
    "end": "1337220"
  },
  {
    "text": "And another little\nthing to note maybe if you're familiar with\nthe history, the score that we got on\nGLUE and SQuAD was",
    "start": "1337220",
    "end": "1343580"
  },
  {
    "text": "reasonably comparable to BERT. So it's a decent sanity check. We have a model that\nhas more parameters.",
    "start": "1343580",
    "end": "1350550"
  },
  {
    "text": "But it's only trained\nfor a quarter as long and it nevertheless got\ncomparable performance",
    "start": "1350550",
    "end": "1356900"
  },
  {
    "text": "which using a similar\nobjective so we shouldn't be too surprised about that. And then the last\nthing to mention is that we're going to use\nthis standard deviation over",
    "start": "1356900",
    "end": "1363962"
  },
  {
    "text": "and over again, so that we\ncan bold entries in the table when they're within one standard\ndeviation of the maximum value",
    "start": "1363962",
    "end": "1369410"
  },
  {
    "text": "for that dataset in the table. So now I'll just make\na big disclaimer, which is we're going to compare\nlots of different things,",
    "start": "1369410",
    "end": "1375799"
  },
  {
    "start": "1372000",
    "end": "1432000"
  },
  {
    "text": "we're going to run\nlots of experiments, but we're not going to tweak any\nhyperparameters because if we did, like the change\nthe learning rate,",
    "start": "1375800",
    "end": "1382950"
  },
  {
    "text": "or whatever, it would be\njust too computationally expensive to do this for\neach individual method.",
    "start": "1382950",
    "end": "1388700"
  },
  {
    "text": "Our hope is that\nthis is OK because we are treating all problems in\nexactly the same framework.",
    "start": "1388700",
    "end": "1395600"
  },
  {
    "text": "We're always doing text-to-text\nmaximum likelihood training. So hopefully we can keep\nhyperparameters fixed,",
    "start": "1395600",
    "end": "1401090"
  },
  {
    "text": "and arguably if you\npropose a new method that requires extensive\nhyperparameter tuning it's not a very useful\nmethod for practitioners.",
    "start": "1401090",
    "end": "1407742"
  },
  {
    "text": "And we'll get into that\na little bit more later when I talk about architectural\nmodifications too.",
    "start": "1407742",
    "end": "1412820"
  },
  {
    "text": "The other thing I'll\nsay is that, while we did run lots of\nexperiments there's no way we could be\ncomprehensive because there",
    "start": "1412820",
    "end": "1418039"
  },
  {
    "text": "were so many methods out there. And the inclusion or exclusion\nof one particular method",
    "start": "1418040",
    "end": "1423740"
  },
  {
    "text": "is not meant as a\njudgment on its quality. It's just, it's what\nwe were able to do",
    "start": "1423740",
    "end": "1429049"
  },
  {
    "text": "given the constraints that\nwe were working under. So the first set of\nexperiments that we ran",
    "start": "1429050",
    "end": "1436300"
  },
  {
    "start": "1432000",
    "end": "1586000"
  },
  {
    "text": "were to compare different\nmodel structures. So as I mentioned earlier\nthe main baseline T5 model",
    "start": "1436300",
    "end": "1443170"
  },
  {
    "text": "is an encoder-decoder model. And in this case, you have\na separate layer stack that encodes a sequence, and\na separate layer stack that",
    "start": "1443170",
    "end": "1450940"
  },
  {
    "text": "decodes the target sequence. Basically it generates\nthe target sequence one token by token,\nwhile attending back",
    "start": "1450940",
    "end": "1457060"
  },
  {
    "text": "to the encoder's\noutput to figure out what it should condition on. The next set up\nthat we considered",
    "start": "1457060",
    "end": "1463660"
  },
  {
    "text": "is an encoder-decoder\nmodel except that all of the relevant parameters in\nthe encoder-decoder are shared.",
    "start": "1463660",
    "end": "1469000"
  },
  {
    "text": "So there are basically\nhalf as many parameters. And then finally another\nvariant that we considered",
    "start": "1469000",
    "end": "1474414"
  },
  {
    "text": "as an encoder-decoder\nmodel where the encoder and decoder have half\nas many layers as they do in the baseline.",
    "start": "1474415",
    "end": "1480280"
  },
  {
    "text": "And that's because\nwe're also considering single stack models,\nthe language model and what we call a\nprefix language model.",
    "start": "1480280",
    "end": "1487480"
  },
  {
    "text": "The language model is\na model that models the sequence strictly\nfrom the left to right fashion,\nin a causal fashion.",
    "start": "1487480",
    "end": "1493660"
  },
  {
    "text": "It basically just ingests\ntokens one at a time and predicts the next token. And you can actually apply\nthese to text-to-text problems",
    "start": "1493660",
    "end": "1501190"
  },
  {
    "text": "by basically feeding\nthe input as a prefix before you start\npredicting anything. Now if you just use a language\nmodel in its strict format then",
    "start": "1501190",
    "end": "1509860"
  },
  {
    "text": "you still have to have what we\nwould call a causal mask, so a causal attention\npattern on the prefix.",
    "start": "1509860",
    "end": "1516250"
  },
  {
    "text": "And that's actually how\nthe GPT series of models treat all of their problems. But because we are\nexplicitly denoting",
    "start": "1516250",
    "end": "1524268"
  },
  {
    "text": "part of the sequence as an input\nand the rest of the sequence as a target, we actually\ncan allow the model to have full visibility, a\nnon-causal mask on the input",
    "start": "1524268",
    "end": "1532360"
  },
  {
    "text": "region of the sequence. And when we make\nthat change we call that the prefix language model.",
    "start": "1532360",
    "end": "1537730"
  },
  {
    "text": "And now the upshot\nof all of this really is that the encoder-decoder\nmodel for our framework turns out to work best.",
    "start": "1537730",
    "end": "1543700"
  },
  {
    "text": "You can see that when we\nshare the parameters it does hurt performance a little\nbit but maybe a little less than you might expect.",
    "start": "1543700",
    "end": "1549490"
  },
  {
    "text": "The prefix language\nmodel attains slightly worse performance\nbut significantly better performance than doing strictly\ncausal left-to-right language",
    "start": "1549490",
    "end": "1557230"
  },
  {
    "text": "modeling, which is what you\nsee in the fourth row here. And finally having the\nnumber of parameters",
    "start": "1557230",
    "end": "1562390"
  },
  {
    "text": "in the encoder and decoder\nharms performance significantly.",
    "start": "1562390",
    "end": "1567430"
  },
  {
    "text": "One thing to note is that\nin all of these cases, we're processing the same\ntotal sequence length.",
    "start": "1567430",
    "end": "1572889"
  },
  {
    "text": "It's the same input sequence\nand the same target sequence. So in most of these\ncases, the total number",
    "start": "1572890",
    "end": "1578140"
  },
  {
    "text": "of flops required to\nprocess the sequence is the same, even though\nthe number of parameters is twice as many in\nthe baseline model.",
    "start": "1578140",
    "end": "1586077"
  },
  {
    "start": "1586000",
    "end": "1682000"
  },
  {
    "text": "So the next thing we looked\ninto were different variants on our pre-training objective. So the first thing\nwe did was kind",
    "start": "1586077",
    "end": "1591860"
  },
  {
    "text": "of compare different\nhigh level approaches, maybe just training the model\nto predict the next token one token at a time, that's kind of\na language modeling objective.",
    "start": "1591860",
    "end": "1599850"
  },
  {
    "text": "Another would be a take\nthe input sequence, shuffle it up, and\ntrain the model to predict the\nunshuffled sequence,",
    "start": "1599850",
    "end": "1606080"
  },
  {
    "text": "or to consider a mass language\nmodel style, a BERT style objective like the one\nthat we mentioned earlier.",
    "start": "1606080",
    "end": "1612620"
  },
  {
    "text": "And now on the second step\nthat I'm showing here, the results for, we considered\na BERT style objective,",
    "start": "1612620",
    "end": "1619028"
  },
  {
    "text": "where the model is\ntrained to predict the entire original uncorrupted\ninput sequence, a MASS style",
    "start": "1619028",
    "end": "1625700"
  },
  {
    "text": "objective, which\nis quite similar. And then a replace\ncorrupted spans objective,",
    "start": "1625700",
    "end": "1630818"
  },
  {
    "text": "which is like the one that\nI described at the beginning that we're using in\nour baseline model. And finally a variant\nwhere rather than replacing",
    "start": "1630818",
    "end": "1637610"
  },
  {
    "text": "each token with a\nunique sentinel token we just drop the mass\ntokens completely",
    "start": "1637610",
    "end": "1643190"
  },
  {
    "text": "and train the model to\npredict the dropped tokens. And you can see that the latter\ntwo options work roughly as",
    "start": "1643190",
    "end": "1649850"
  },
  {
    "text": "well as one another. But another pertinent\ndifference between these sets",
    "start": "1649850",
    "end": "1654920"
  },
  {
    "text": "of objectives is that\nthe first two involve predicting the\nentire input sequence and the last two basically\njust involve predicting",
    "start": "1654920",
    "end": "1662120"
  },
  {
    "text": "the masked out tokens. And when you only predict\nthe masked out tokens you have a much shorter\ntarget sequence.",
    "start": "1662120",
    "end": "1667800"
  },
  {
    "text": "And so the overall cost\nis significantly lower for pre-training. So we decided that that\nwas the best approach",
    "start": "1667800",
    "end": "1674060"
  },
  {
    "text": "and then we considered\nother hyperparameters in our masking strategy, such\nas how many tokens to mask out.",
    "start": "1674060",
    "end": "1683570"
  },
  {
    "start": "1682000",
    "end": "1830000"
  },
  {
    "text": "So the next thing we considered\nwere different variants of a pre-training dataset. In our baseline we\nuse the C4 dataset",
    "start": "1683570",
    "end": "1690620"
  },
  {
    "text": "that I proposed at the\nbeginning of the talk. We also compared to pre-training\non only on unfiltered data",
    "start": "1690620",
    "end": "1697280"
  },
  {
    "text": "from C4. So rather than doing all these\nheuristic filtering steps, we just take the raw web\nextracted text from C4",
    "start": "1697280",
    "end": "1703430"
  },
  {
    "text": "and pre-trained on that. And you can see that,\nthat does uniformly worse. So it is, it does\nseem to be true",
    "start": "1703430",
    "end": "1708470"
  },
  {
    "text": "that these cleaning steps\nthat we're doing are actually useful. The next four datasets\nfor our attempt to pre-train on\nsimilar datasets that",
    "start": "1708470",
    "end": "1715820"
  },
  {
    "text": "had been used in past work. The real news dataset came\nfrom the GROVER paper. It's essentially pre-training\nonly on data from news sites.",
    "start": "1715820",
    "end": "1723679"
  },
  {
    "text": "Web text is this dataset that\nwas used in a GPT-2 paper where you only train on\nweb text that was linked to",
    "start": "1723680",
    "end": "1730280"
  },
  {
    "text": "and received a reasonably\nhigh score on Reddit. And then the last two variants\nare either Wikipedia alone,",
    "start": "1730280",
    "end": "1736550"
  },
  {
    "text": "or as was used in the\nBERT paper, Wikipedia with the Toronto Books Corpus.",
    "start": "1736550",
    "end": "1741980"
  },
  {
    "text": "And you might actually\nnotice that some of these more\nspecialized datasets",
    "start": "1741980",
    "end": "1746990"
  },
  {
    "text": "we get better performance. So for example, you can see that\non the Wikipedia and Toronto Books Corpus on the\nbottom row, we actually",
    "start": "1746990",
    "end": "1753830"
  },
  {
    "text": "do much better on SuperGLUE\nwith a score of a little over 73 compared to pre-training on C4.",
    "start": "1753830",
    "end": "1760255"
  },
  {
    "text": "And it turns out\nthis is because, or we conjecture that this\nis because SuperGLUE contains",
    "start": "1760255",
    "end": "1766550"
  },
  {
    "text": "a task called multi RC which\nis a reading comprehension task on news, sorry,\nencyclopedia articles",
    "start": "1766550",
    "end": "1774860"
  },
  {
    "text": "and on novels. So the basic takeaway\nhere is that when you pre-train on data that's\nsimilar to your downstream",
    "start": "1774860",
    "end": "1781700"
  },
  {
    "text": "task, that has a\nsimilar domain you often get a big boost\nin that downstream task. And that's indeed\nwhat happened here.",
    "start": "1781700",
    "end": "1788090"
  },
  {
    "text": "Interestingly, you can also\nsee the opposite effect. So if you look on Wikipedia\non the second to last row.",
    "start": "1788090",
    "end": "1794419"
  },
  {
    "text": "If you only pre-trained\non Wikipedia, you end up doing\nmuch worse on CoLA, which is the Corpus of\nLinguistic Acceptability tasks",
    "start": "1794420",
    "end": "1800809"
  },
  {
    "text": "that I mentioned early on. And we conjecture that this\nis because Wikipedia has very little unacceptable text.",
    "start": "1800810",
    "end": "1807380"
  },
  {
    "text": "You're basically only\npre-training on clean text, whereas C4 has some\nungrammatical texts, some nonsense in it,\nand so that actually",
    "start": "1807380",
    "end": "1814070"
  },
  {
    "text": "can boost your performance\na little bit on CoLA. The last thing to\nnote is that while you do see some gains sometimes on\nusing these smaller data sets,",
    "start": "1814070",
    "end": "1821292"
  },
  {
    "text": "these data sets are about\nan order of magnitude smaller than C4. So then the natural question\nis, does it actually",
    "start": "1821292",
    "end": "1826970"
  },
  {
    "text": "hurt you to pre-train\non a smaller dataset? So to answer that\nquestion, what we did is basically took C4 and\nartificially made it smaller,",
    "start": "1826970",
    "end": "1835580"
  },
  {
    "start": "1830000",
    "end": "1898000"
  },
  {
    "text": "so that it was repeated over\nthe course of pre-training. And you can see here that\nwhen you repeat the data",
    "start": "1835580",
    "end": "1841820"
  },
  {
    "text": "set 64 times, so it's 34\nbillion divided by 64 tokens,",
    "start": "1841820",
    "end": "1847250"
  },
  {
    "text": "because that's how much\npre-training we did. You actually don't\nsacrifice much performance. The performance is\nroughly the same.",
    "start": "1847250",
    "end": "1854659"
  },
  {
    "text": "But if you repeat the data set\n256 times, 1,024 times or more you actually start\nto see degradation.",
    "start": "1854660",
    "end": "1860633"
  },
  {
    "text": "And the reason that we\nthink this is happening is because you're\nbasically overfitting during pre-training. And you can get a sense\nfor whether that's true",
    "start": "1860633",
    "end": "1867980"
  },
  {
    "text": "or not by looking, just\nlooking at the training loss. You can see that\nthe model attains a much, much smaller training\nloss as you repeat the data set",
    "start": "1867980",
    "end": "1874549"
  },
  {
    "text": "more and more times. So the upshot of this\nis that your data set should be at least\nas big that you don't see significant\noverfitting",
    "start": "1874550",
    "end": "1881419"
  },
  {
    "text": "during pre-training. And later on when we\nscale up these models and pre-trained them\non much more data,",
    "start": "1881420",
    "end": "1888200"
  },
  {
    "text": "we would do enough repeats of\nthe smaller sort of more domain specific data sets\nthat we imagine",
    "start": "1888200",
    "end": "1894019"
  },
  {
    "text": "we would see harmful effects.  The next thing we\nexperimented with",
    "start": "1894020",
    "end": "1900170"
  },
  {
    "start": "1898000",
    "end": "2000000"
  },
  {
    "text": "were multi-task\nlearning strategies. So when you're doing\nmulti task learning you're essentially training the model\non multiple tasks at once.",
    "start": "1900170",
    "end": "1907880"
  },
  {
    "text": "And most of the, in\nall the experiments I'm showing on this\nslide here, we're",
    "start": "1907880",
    "end": "1913309"
  },
  {
    "text": "actually training\non every single task at once, so the\npre-training task, and all of the downstream\ntasks together.",
    "start": "1913310",
    "end": "1919700"
  },
  {
    "text": "And the most pertinent\nquestion when you're doing multi-task\ntraining like this is, how often should I\nsample data from each task?",
    "start": "1919700",
    "end": "1926460"
  },
  {
    "text": "So one approach is just to\nsample data at an equal rate across all of the tasks.",
    "start": "1926460",
    "end": "1932400"
  },
  {
    "text": "Another case is to\nbasically pretend like you've just concatenated\nall the datasets,",
    "start": "1932400",
    "end": "1937620"
  },
  {
    "text": "we call that\nexamples proportional mixing because it's\nequivalent to sampling from the dataset in accordance\nto how many examples there",
    "start": "1937620",
    "end": "1943640"
  },
  {
    "text": "are in the dataset. The difficult thing\nwith that though, is that our pre-training\ndataset is so big that its proportion\nwould be much, much, much",
    "start": "1943640",
    "end": "1951260"
  },
  {
    "text": "bigger than every\ndownstream task, and we basically\nwould never train on any of the downstream data. So we introduced\nthis hyperparameter K",
    "start": "1951260",
    "end": "1959030"
  },
  {
    "text": "which is a constant\nthat basically is how big should we pretend\nthat the pre-training data",
    "start": "1959030",
    "end": "1964370"
  },
  {
    "text": "set is. The last thing\nyou can do is take the number of examples\nin each data set",
    "start": "1964370",
    "end": "1970340"
  },
  {
    "text": "and scale it by a temperature. The larger the\ntemperature, the closer you get to equal mixing, to uniform\nsampling from each data set.",
    "start": "1970340",
    "end": "1979550"
  },
  {
    "text": "But at any rate, the main\ntakeaway from this table is that you can get pretty\nclose to the performance",
    "start": "1979550",
    "end": "1985910"
  },
  {
    "text": "of separate\npre-training and fine tuning like we do\non our baseline if you get the mixing\nstrategy right.",
    "start": "1985910",
    "end": "1991910"
  },
  {
    "text": "But ultimately we\nfound that you do tend to sacrifice\nsome performance when doing multi-task training on\nat least some of the tasks.",
    "start": "1991910",
    "end": "1999184"
  },
  {
    "text": "And Colin, there were lots of\nquestions back on the choice-- on the slide with the\ndifferent datasets,",
    "start": "1999185",
    "end": "2004660"
  },
  {
    "start": "2000000",
    "end": "2173000"
  },
  {
    "text": "one showing Realnews\nand C4 and so on. You want to take a couple?",
    "start": "2004660",
    "end": "2009760"
  },
  {
    "text": "Absolutely. Yeah, so firstly if\nyou just look at this, it still does kind\nof look like you",
    "start": "2009760",
    "end": "2018400"
  },
  {
    "text": "can get great results with\nmore than an order of magnitude less text than C4, and it seemed\nlike that's not the message",
    "start": "2018400",
    "end": "2027040"
  },
  {
    "text": "you wanted to be\nputting forward. Yeah, so there's a\nlittle nuance here, which is basically that in our\nbaseline, in these experiments",
    "start": "2027040",
    "end": "2033940"
  },
  {
    "text": "that I've been running\nso far, we're actually not pre-training for that long. So as I mentioned earlier, we're\nactually training for a quarter",
    "start": "2033940",
    "end": "2040330"
  },
  {
    "text": "as long as BERT and\nactually for as a, I believe 1/256th as long\nas X on that for example.",
    "start": "2040330",
    "end": "2051040"
  },
  {
    "text": "And when later in\nthe paper, we're going to pre-train for\nmuch, much, much longer. And in that case,\nwe would end up",
    "start": "2051040",
    "end": "2057250"
  },
  {
    "text": "repeating these datasets many,\nmany times over the course of pre-training and we'd start\nto see these negative effects",
    "start": "2057250",
    "end": "2062620"
  },
  {
    "text": "that I explained\non the next slide. So that's why you're\nthen doing the repeat. OK, someone then\nasked about that as to why you're training on the\nsame stuff over and over again.",
    "start": "2062620",
    "end": "2070353"
  },
  {
    "text": "That's the test. Yeah, exactly. And then on the datasets--",
    "start": "2070353",
    "end": "2076550"
  },
  {
    "text": "so to a first\napproximation, does C4 contain Wikipedia,\nor partially--",
    "start": "2076550",
    "end": "2082870"
  },
  {
    "text": "It contains many, many\npages of Wikipedia but not all of\nWikipedia, you know",
    "start": "2082870",
    "end": "2088919"
  },
  {
    "text": "the Common Crawl is\ndone by like-- it is a sort of web\ncrawl by following",
    "start": "2088920",
    "end": "2094839"
  },
  {
    "text": "links at some priority\nand ultimately it didn't cover all of Wikipedia. I don't actually know the exact\nproportion of Wikipedia that's",
    "start": "2094840",
    "end": "2101740"
  },
  {
    "text": "included in C4 but definitely\nwhen training on C4",
    "start": "2101740",
    "end": "2106810"
  },
  {
    "text": "you will see some\nWikipedia text. It will be at a relatively\nlow proportion compared to all of the other data that--",
    "start": "2106810",
    "end": "2114220"
  },
  {
    "text": "sources of data you'll see. Sure. And then someone wasn't quite\nconvinced with your argument",
    "start": "2114220",
    "end": "2119410"
  },
  {
    "text": "that the good\nquality of Wikipedia explained the worst performance\non CoLA because they thought,",
    "start": "2119410",
    "end": "2126190"
  },
  {
    "text": "well surely RealNews, that's\nbasically well-edited text as",
    "start": "2126190",
    "end": "2132099"
  },
  {
    "text": "well, and yet it\nseems to work fine. Yeah, it's a good point. I'm not sure.",
    "start": "2132100",
    "end": "2137859"
  },
  {
    "text": "It could be that because\nRealNews has quotes in it, or maybe RealNews ended\nup having some content",
    "start": "2137860",
    "end": "2143770"
  },
  {
    "text": "from the common\nsections of sites. I should say that when I-- the reason that they're\nRealNews-like and WebText-like",
    "start": "2143770",
    "end": "2149860"
  },
  {
    "text": "is that these are our\nown reproductions of them so they might not be exactly\nthe same as the originally proposed variants, because\nWebText for example",
    "start": "2149860",
    "end": "2157048"
  },
  {
    "text": "was never released.  Yeah, but it's an\ninteresting point and that's also why I would\nsay that it's a conjecture.",
    "start": "2157048",
    "end": "2164600"
  },
  {
    "text": "It's not something that I can\nmake a rigorous claim about. Yeah. OK. Maybe we should let\nyou go on for now.",
    "start": "2164600",
    "end": "2171120"
  },
  {
    "text": "Great. Thanks. Yeah, thanks for\nthose questions. So then the next\nthing after looking",
    "start": "2171120",
    "end": "2176750"
  },
  {
    "text": "at these different multi-task\ntraining strategies, is to see if there's\nany way for us",
    "start": "2176750",
    "end": "2181970"
  },
  {
    "text": "to close the gap between\nmulti-task training and this pre-training followed\nby separate fine tuning.",
    "start": "2181970",
    "end": "2188240"
  },
  {
    "text": "And we experimented with many\ndifferent strategies here, basically, strict\nmultitask training,",
    "start": "2188240",
    "end": "2193670"
  },
  {
    "text": "doing multi-task training\nfollowed by individual task fine tuning, doing\nmulti-task training",
    "start": "2193670",
    "end": "2199010"
  },
  {
    "text": "but without any\nunsupervised data. And really the main takeaway\nfrom all of these experiments was that if you do the\nmulti-task training first,",
    "start": "2199010",
    "end": "2207619"
  },
  {
    "text": "including the unsupervised\ntask, and then you fine tune the model on each\ntask separately, which",
    "start": "2207620",
    "end": "2214052"
  },
  {
    "text": "is the third row\nhere, you actually don't really sacrifice\nmuch performance at all. You are-- you don't end\nup with a multi-task model",
    "start": "2214052",
    "end": "2220970"
  },
  {
    "text": "because you're fine tuning\non each task individually. But the nice thing\nabout this approach",
    "start": "2220970",
    "end": "2226160"
  },
  {
    "text": "is that you can monitor\nthat performance on your downstream tasks while\nyou're doing pre-training.",
    "start": "2226160",
    "end": "2232519"
  },
  {
    "text": "And you don't sacrifice\nmuch performance. One setting that\nwe didn't consider",
    "start": "2232520",
    "end": "2237890"
  },
  {
    "text": "is the unsupervised\npre-training followed by a supervised\nmulti-task training. I wish that we had run that\nbut we just, we didn't.",
    "start": "2237890",
    "end": "2245000"
  },
  {
    "text": " So then sort of the last\nset of experiments we ran",
    "start": "2245000",
    "end": "2250550"
  },
  {
    "text": "try to answer the\nfollowing question. Let's say that someone comes\nalong and all of a sudden gives you four times\nas much compute,",
    "start": "2250550",
    "end": "2257030"
  },
  {
    "text": "what should you do with it? And so there are a number\nof things you could do. You could increase the\nnumber of training steps",
    "start": "2257030",
    "end": "2262268"
  },
  {
    "text": "by a factor of four. You could increase your batch\nsize by a factor of four. You can make your\nmodel twice as big,",
    "start": "2262268",
    "end": "2267680"
  },
  {
    "text": "and train for twice as long. You can make your model\nfour times as big. You could train four models\nseparately and ensemble them.",
    "start": "2267680",
    "end": "2274567"
  },
  {
    "text": "Or you could do this last\nthing, which doesn't actually use four times as much compute,\nwhere you pre-train one model",
    "start": "2274568",
    "end": "2280010"
  },
  {
    "text": "and you fine tune it\nfour times separately and then ensemble those. And the main takeaway here\nis that scaling helps.",
    "start": "2280010",
    "end": "2287460"
  },
  {
    "text": "This is very unsurprising\nespecially in 2021, but interestingly you\nget significant gains",
    "start": "2287460",
    "end": "2296570"
  },
  {
    "text": "whether you just increase\nthe training time or if you increase the size. So you can see that\nalong both of these axes",
    "start": "2296570",
    "end": "2304108"
  },
  {
    "text": "we get significant\nperformance improvements although the performance\nimprovements are more dramatic when we increase the size.",
    "start": "2304108",
    "end": "2309380"
  },
  {
    "text": "And in particular,\nyou can see that we've gone from a score of about\n71 on SuperGLUE to 78 just",
    "start": "2309380",
    "end": "2315349"
  },
  {
    "text": "by making the model\nfour times bigger. OK, so let me just kind of give\na quick recap of all of that.",
    "start": "2315350",
    "end": "2321790"
  },
  {
    "text": "And then use that\nrecap to explain the design decisions that\nwent into the final sort of T5 models.",
    "start": "2321790",
    "end": "2327860"
  },
  {
    "text": "The first thing is\nthat we're going to choose an encoder-decoder\narchitecture because that seemed to work best in\nour text- to-text format.",
    "start": "2327860",
    "end": "2334173"
  },
  {
    "text": "The next thing is\nthat we're going to use the span prediction\nobjective, which is ultimately quite similar to the\nbaseline objective",
    "start": "2334173",
    "end": "2339789"
  },
  {
    "text": "that I described earlier. We will use the C4\ndataset because it did attain reasonable\nperformance,",
    "start": "2339790",
    "end": "2345682"
  },
  {
    "text": "but was large enough\nthat we didn't have to worry about repeating\nthe data and seeing detrimental overfitting during\npre-training when we scale up",
    "start": "2345682",
    "end": "2353710"
  },
  {
    "text": "the number of\npre-training steps. We actually decided to do\nmulti-task pre-training because we will be scaling up\nthe amount of pre-training.",
    "start": "2353710",
    "end": "2360700"
  },
  {
    "text": "Our longest training\nruns took about a month and we wanted to be able\nto monitor performance over the course of pre-training\nwithout doing fine tuning.",
    "start": "2360700",
    "end": "2367467"
  },
  {
    "text": "So we're going to be doing\nthis multi-task pre-training followed by fine tuning. And then the last\nthing, of course,",
    "start": "2367468",
    "end": "2372560"
  },
  {
    "text": "is we're going to train\nbigger models for longer. And specifically the model\nsizes that we ended up",
    "start": "2372560",
    "end": "2378880"
  },
  {
    "start": "2375000",
    "end": "2492000"
  },
  {
    "text": "releasing we called small,\nbase, large, 3B, and 11B. The small model has\n60 million parameters.",
    "start": "2378880",
    "end": "2385599"
  },
  {
    "text": "It's about a quarter\nas big as our baseline which again was a BERT-base\nsize encoder, BERT-base size",
    "start": "2385600",
    "end": "2390730"
  },
  {
    "text": "decoder. We also trained a model that\nwas a BERT-Large size encoder, BERT-Large size decoder.",
    "start": "2390730",
    "end": "2396260"
  },
  {
    "text": "And then we created two\nlarger variants simply by scaling up the feed-forward\ndimension of the transformer",
    "start": "2396260",
    "end": "2402400"
  },
  {
    "text": "and the number of attention\nheads in the transformer. You can see our\nlargest model actually had a hidden dimensionality\nof 65,000 in the feed",
    "start": "2402400",
    "end": "2411070"
  },
  {
    "text": "forward layers. The reason that we did this\nkind of unusual way of scaling up the parameter count is just\nbecause the feed forward layers",
    "start": "2411070",
    "end": "2418180"
  },
  {
    "text": "are just gigantic\nmatrix multiplies and that's the best way to make\nuse of hardware accelerators.",
    "start": "2418180",
    "end": "2425087"
  },
  {
    "text": "Can I just stick in\none more question? Absolutely. Someone was asking about how\nyou did the multi-task training,",
    "start": "2425087",
    "end": "2434260"
  },
  {
    "text": "like so was that sticking\na simple softmax classifier",
    "start": "2434260",
    "end": "2439300"
  },
  {
    "text": "on top for each task or? So in our case, because we're\nusing this text-to-text format,",
    "start": "2439300",
    "end": "2444940"
  },
  {
    "text": "basically where-- you train on\nexactly the same-- it's exactly the same model, no\nnew classification",
    "start": "2444940",
    "end": "2450580"
  },
  {
    "text": "heads for every task. The only difference is that each\ntask gets its own task prefix. So if you remember all the\nway back at the beginning",
    "start": "2450580",
    "end": "2456550"
  },
  {
    "text": "we say you know translate\nEnglish to German colon English sentence, or summarize\ncolon English paragraph.",
    "start": "2456550",
    "end": "2463150"
  },
  {
    "text": "And that tells the\nmodel what it should do. And then you just\ntrain the model to predict the\ncorresponding target. ",
    "start": "2463150",
    "end": "2471660"
  },
  {
    "text": "Cool, so the last\npertinent detail again is that we did scale up\nthe amount of pre-training,",
    "start": "2471660",
    "end": "2478050"
  },
  {
    "text": "we ended up pre-training on\na trillion tokens of data rather than 34 billion tokens. So it's quite a lot\nmore pre-training,",
    "start": "2478050",
    "end": "2485180"
  },
  {
    "text": "although it's still\nless pre-training that was used in XLNet I\nthink by a factor of 2, If I'm remembering correctly.",
    "start": "2485180",
    "end": "2492760"
  },
  {
    "start": "2492000",
    "end": "2567000"
  },
  {
    "text": "So here are the results. And these were kind of the way\nthat things stood at the time that we released the paper.",
    "start": "2492760",
    "end": "2498900"
  },
  {
    "text": "We ended up getting\nstate of the art results on the GLUE meta\nbenchmark, CNN Daily Mail",
    "start": "2498900",
    "end": "2505050"
  },
  {
    "text": "abstractive summarization,\nSQuAD question answering. And we were actually\nquite excited to see how well we did on SuperGLUE.",
    "start": "2505050",
    "end": "2512070"
  },
  {
    "text": "We ultimately came pretty\nclose to the human score which was a score of 89.8, and we\nperformed significantly better",
    "start": "2512070",
    "end": "2518190"
  },
  {
    "text": "than RoBERTa. SuperGLUE, it turns\nout is a benchmark that benefits a lot from large\nmodels and so you can really",
    "start": "2518190",
    "end": "2524790"
  },
  {
    "text": "see a dramatic increase\nin the model's performance as we scale the model up.",
    "start": "2524790",
    "end": "2530370"
  },
  {
    "text": "On the other hand, we did not\nattain state of the art results on any of the\ntranslation datasets. And the reason that we\nthink that this is true",
    "start": "2530370",
    "end": "2537000"
  },
  {
    "text": "is because all of\nthe state of the art results at the time on\nthese translation datasets used back translation.",
    "start": "2537000",
    "end": "2542830"
  },
  {
    "text": "And if you remember we did\nEnglish only pre-training in our model. And we expect that in terms of\nmaking use of unlabeled data,",
    "start": "2542830",
    "end": "2552210"
  },
  {
    "text": "it's more effective to use\nback translation for machine translation problems than to use\nthis English only pre-training that we did.",
    "start": "2552210",
    "end": "2559073"
  },
  {
    "text": "I should mention of\ncourse these results are now quite a bit stale. And some of these\nscores have been beaten by subsequent models.",
    "start": "2559073",
    "end": "2568040"
  },
  {
    "start": "2567000",
    "end": "2610000"
  },
  {
    "text": "So now just quick, make a\nplug that all of our code is released, our pre-trained\nmodels have been released.",
    "start": "2568040",
    "end": "2573490"
  },
  {
    "text": "You can make use of\nthem in our code base. They're also, of course, in the\nHugging Face transformers code base.",
    "start": "2573490",
    "end": "2579370"
  },
  {
    "text": "We made a colab at the time\nthat shows a pretty basic demo",
    "start": "2579370",
    "end": "2585490"
  },
  {
    "text": "of how to take one of\nour pre-trained models and basically train it on a\nTSV file of inputs and targets.",
    "start": "2585490",
    "end": "2592330"
  },
  {
    "text": "So because all problems\nare text-to-text problems, you just need to give the\nmodel some input text, and some target text,\nand that's all you",
    "start": "2592330",
    "end": "2598869"
  },
  {
    "text": "need to fine tune the model. And you can make,\nyou can actually fine tune up to the 3 billion\nparameter model on a free colab",
    "start": "2598870",
    "end": "2605770"
  },
  {
    "text": "TPU using the link\nat the bottom here.",
    "start": "2605770",
    "end": "2610970"
  },
  {
    "text": "Great so, so far\nwe've been talking about an English only\npre-training model. I mean we did apply it\nto machine translation",
    "start": "2610970",
    "end": "2617180"
  },
  {
    "text": "and downstream tasks. So a kind of a natural\nquestion is, what about all the other languages?",
    "start": "2617180",
    "end": "2622780"
  },
  {
    "text": "Why not train a\nmultilingual model? And so that's something\nthat we did more recently. And actually let me\njust pause because I did",
    "start": "2622780",
    "end": "2628980"
  },
  {
    "text": "see a couple of\nquestions coming out. I want to make sure that I\nam not leaving anyone behind as I move to the next section.",
    "start": "2628980",
    "end": "2635585"
  },
  {
    "text": " Sure. So, one of the questions is\nabout the multi-task setting.",
    "start": "2635585",
    "end": "2644910"
  },
  {
    "text": "If you include an\nunknown task prefix, does anything\ninteresting happen? And if you don't include\na prefix what does it do?",
    "start": "2644910",
    "end": "2653760"
  },
  {
    "text": "So if you include an\nunknown task prefix, or if you don't include a prefix\nat all, what it will probably",
    "start": "2653760",
    "end": "2660569"
  },
  {
    "text": "do is apply the\nunsupervised objective because we actually\ndidn't use a task prefix",
    "start": "2660570",
    "end": "2666120"
  },
  {
    "text": "for the unsupervised objective. Well I guess I\nshould say, what--",
    "start": "2666120",
    "end": "2672600"
  },
  {
    "text": "it's not quite,\nthat's not quite true because there won't be any\nsentinel tokens in the input. So what we actually see\nis it typically does",
    "start": "2672600",
    "end": "2680063"
  },
  {
    "text": "is it outputs kind\nof some related words and some other\nsentinel tokens in gibberish. It's not very useful\nis I guess the upshot.",
    "start": "2680063",
    "end": "2689891"
  },
  {
    "text": "We have questions\nabout back translation, I don't think they heard\nabout back translation in the rest of the course.",
    "start": "2689892",
    "end": "2695238"
  },
  {
    "text": "So back translation is a\npretty straightforward method. The basic idea is that if I\nhave unlabeled text data in one",
    "start": "2695238",
    "end": "2700410"
  },
  {
    "text": "language, I use my current\nmodel to translate that data to some particular language.",
    "start": "2700410",
    "end": "2707250"
  },
  {
    "text": "And I use that as training data\nthen subsequently for my model. It's similar to self-training\nif you're familiar with it.",
    "start": "2707250",
    "end": "2714750"
  },
  {
    "text": "Basically you're making\npredictions on unlabeled data and then using those\npredictions to train the model.",
    "start": "2714750",
    "end": "2720050"
  },
  {
    "text": "Turns out to be helpful. Yep, and then one detail\nquestion, maximum input length",
    "start": "2720050",
    "end": "2727234"
  },
  {
    "text": "and maximum output length,\nhow did you choose them? Did you do a study\non that as well? Yeah, so most of the\ntasks we considered",
    "start": "2727235",
    "end": "2734150"
  },
  {
    "text": "did not have input length\nsignificantly longer than 512 tokens\nmost of the time, using the tokenization\nstrategy that we made use of.",
    "start": "2734150",
    "end": "2742970"
  },
  {
    "text": "And so we used a maximum\ninput length of 512 but we used that position\nencoding scheme that",
    "start": "2742970",
    "end": "2748940"
  },
  {
    "text": "allows arbitrary input lengths. And we actually have\nin subsequent work fine tuned T5 on\nsequences of length 2,048.",
    "start": "2748940",
    "end": "2758599"
  },
  {
    "text": "Beyond that you start to get\ninto memory issues because of attention's quadratic\nmemory complexity.",
    "start": "2758600",
    "end": "2763609"
  },
  {
    "text": "But in principle you can\napply it to long sequences. And then maybe one last\nthing if you have a second,",
    "start": "2763610",
    "end": "2770300"
  },
  {
    "text": "is talking again about\nhow translation again seems to be not the, not\nthe killer app for this.",
    "start": "2770300",
    "end": "2777780"
  },
  {
    "text": "And so what's your intuition\nas to why translation-- does it not benefit from\npre-training for what reason?",
    "start": "2777780",
    "end": "2786650"
  },
  {
    "text": "I'm not, I'm really not sure. ",
    "start": "2786650",
    "end": "2791810"
  },
  {
    "text": "Yeah, I can only conjecture. I think that pre-training\nhelps the model learn",
    "start": "2791810",
    "end": "2797240"
  },
  {
    "text": "the meaning of words. It helps the model learn\nsome world knowledge which I'll talk about a\nlittle bit later,",
    "start": "2797240",
    "end": "2802370"
  },
  {
    "text": "and that's a very loose concept. I think that for\ntranslation learning",
    "start": "2802370",
    "end": "2808130"
  },
  {
    "text": "world knowledge is\nnot very useful, because everything, all\nof the knowledge you need to translate a\nsentence for the most part",
    "start": "2808130",
    "end": "2815810"
  },
  {
    "text": "is in the sentence. And so basically all of the sort\nof contextual knowledge style",
    "start": "2815810",
    "end": "2822590"
  },
  {
    "text": "information that you need to\nproduce the German sentence in the input sentence. So gaining world knowledge\nduring pre-training",
    "start": "2822590",
    "end": "2828620"
  },
  {
    "text": "is not very useful. Of course it's useful\nto know what words mean, but to a certain extent that's\nkind of the easiest signal",
    "start": "2828620",
    "end": "2836030"
  },
  {
    "text": "to pick up on during training. And I imagine, that\nwould be my guess, I don't have any rigorous\nproof of any of this.",
    "start": "2836030",
    "end": "2844150"
  },
  {
    "text": "Thanks. Great, so like I was saying, we\ntrained this English only model",
    "start": "2844150",
    "end": "2851359"
  },
  {
    "text": "and we wanted to address the\nmajor shortcoming that really only can speak one language.",
    "start": "2851360",
    "end": "2857940"
  },
  {
    "start": "2857000",
    "end": "2885000"
  },
  {
    "text": "So we introduced a model\ncalled mT5, multilingual T5. And really for\nthe most part, the",
    "start": "2857940",
    "end": "2863150"
  },
  {
    "text": "if you remember\none thing about mT5 it's basically that it's\nexactly the same model but trained on a\nmultilingual corpus.",
    "start": "2863150",
    "end": "2869630"
  },
  {
    "text": "And the text-to-text\nformat is the same. We feed in task prefixes\nbut we can feed content",
    "start": "2869630",
    "end": "2876380"
  },
  {
    "text": "in different languages. And we can do\nclassification tasks. We can do question\nanswering tasks with mT5",
    "start": "2876380",
    "end": "2883040"
  },
  {
    "text": "in exactly the same way\nthat we can do with T5. So like I said the\npertinent thing about mT5",
    "start": "2883040",
    "end": "2889070"
  },
  {
    "start": "2885000",
    "end": "2920000"
  },
  {
    "text": "was creating a\nmultilingual variant of C4. Overall the process is\nvery similar to the process",
    "start": "2889070",
    "end": "2894410"
  },
  {
    "text": "we used for C4, except that\nit includes 101 languages that we detected using a open\nsource language detector.",
    "start": "2894410",
    "end": "2903350"
  },
  {
    "text": "We also extracted data from\nmore Common Crawl dumps, because especially for\nthe low resource languages",
    "start": "2903350",
    "end": "2908660"
  },
  {
    "text": "it was hard to get enough data\nfrom only one Common Crawl dump. And you can see a list of\nlanguages that we include here.",
    "start": "2908660",
    "end": "2915140"
  },
  {
    "text": "Ultimately the data\nset ended up being about 27 terabytes in size.",
    "start": "2915140",
    "end": "2920260"
  },
  {
    "start": "2920000",
    "end": "2943000"
  },
  {
    "text": "So here's a distribution\nof the number of pages in the mC4 training\ndataset for various languages.",
    "start": "2920260",
    "end": "2926220"
  },
  {
    "text": "You can see our most, our\nhighest resource language is English where we have\nabout 3 billion pages",
    "start": "2926220",
    "end": "2931290"
  },
  {
    "text": "with 3 trillion tokens total. The lowest resource\nlanguages Yoruba with only about 50,000 pages.",
    "start": "2931290",
    "end": "2937110"
  },
  {
    "text": "So you can see that\nthe amount of data that we got for each language\nvaries by many, many orders of magnitude.",
    "start": "2937110",
    "end": "2944290"
  },
  {
    "start": "2943000",
    "end": "2972000"
  },
  {
    "text": "Because of that\na common strategy is to use this sort\nof temperature scaling that I mentioned earlier\nwhere basically you",
    "start": "2944290",
    "end": "2949630"
  },
  {
    "text": "sample from data in\na particular language by using a-- by scaling\nthe number of examples",
    "start": "2949630",
    "end": "2956530"
  },
  {
    "text": "in that language\nby a temperature. And as the, and I\napologize, the temperature",
    "start": "2956530",
    "end": "2962200"
  },
  {
    "text": "here is 1 over the temperature\nthat I described previously. So in this case,\nas the temperature",
    "start": "2962200",
    "end": "2967930"
  },
  {
    "text": "gets smaller and smaller,\nyou get closer and closer to a uniform distribution. The net effect of this is that\nfor very small temperatures",
    "start": "2967930",
    "end": "2975430"
  },
  {
    "start": "2972000",
    "end": "2988000"
  },
  {
    "text": "you tend to do better\non downstream tasks on low resource\nlanguages like Urdu, but as you increase the\ntemperature so that you get,",
    "start": "2975430",
    "end": "2981820"
  },
  {
    "text": "so you basically are doing\nexamples proportional mixing of the different\nlanguages, you do better on high resource\nlanguages like Russian.",
    "start": "2981820",
    "end": "2988900"
  },
  {
    "start": "2988000",
    "end": "3041000"
  },
  {
    "text": "So we took, we took\nmT4, we pre-trained mT5,",
    "start": "2988900",
    "end": "2994539"
  },
  {
    "text": "again basically everything\nwas kept the same, we made the vocabulary a\nbit bigger to accommodate all the different languages.",
    "start": "2994540",
    "end": "3000720"
  },
  {
    "text": "But overall that\namount of pre-training the model sizes, et cetera\nare basically the same. And ultimately got state of\nthe art on some of the tasks",
    "start": "3000720",
    "end": "3009270"
  },
  {
    "text": "in the XTREME benchmark. You'll notice that we\ndon't report results for some of these tasks,\nthat's partially because XTREME",
    "start": "3009270",
    "end": "3015990"
  },
  {
    "text": "is designed for sentence\nencoders like BERT, T5 is a-- and mT5 are\nencoder-decoder models.",
    "start": "3015990",
    "end": "3023670"
  },
  {
    "text": "We did not experiment with\nusing the encoder on its own but in order to attack\nsome of these problems,",
    "start": "3023670",
    "end": "3028720"
  },
  {
    "text": "like the sentence\nretrieval problems, you need a model that can output\na single vector representation",
    "start": "3028720",
    "end": "3034620"
  },
  {
    "text": "of your sequence. And we don't have\nthat in T5 so we didn't apply it to those tasks.",
    "start": "3034620",
    "end": "3042100"
  },
  {
    "start": "3041000",
    "end": "3132000"
  },
  {
    "text": "One interesting finding\nfrom this paper, that I'll just\nquickly mention here, is that there are\nbasically multiple settings",
    "start": "3042100",
    "end": "3050950"
  },
  {
    "text": "that people consider\nmultilingual benchmarks. One is the case,\nthe zero-shot case.",
    "start": "3050950",
    "end": "3056450"
  },
  {
    "text": "And in that case, you\ndon't do any pre-training on a language of, sorry, you\ndon't have any fine tuning data",
    "start": "3056450",
    "end": "3062800"
  },
  {
    "text": "on each particular language. You only have pre-training\ndata on those languages. So you fine tune, let's\nsay only in English,",
    "start": "3062800",
    "end": "3068470"
  },
  {
    "text": "and then you feed the model\nsome text in another language and see if it produces\nthe right predictions.",
    "start": "3068470",
    "end": "3073779"
  },
  {
    "text": "The next setting is the\ntranslate-train setting. That's where you take a\nmachine translation model and translate the data in the\nEnglish fine tuning corpus",
    "start": "3073780",
    "end": "3080710"
  },
  {
    "text": "into different languages. And then the last setting is the\nin-language multi-task setting. That's the setting\nwhere you assume",
    "start": "3080710",
    "end": "3087070"
  },
  {
    "text": "that you have gold\nstandard ground truth data in every language that\nyou want the model to be able to process data in.",
    "start": "3087070",
    "end": "3093940"
  },
  {
    "text": "And the takeaway\nhere actually is that the difference\nin performance",
    "start": "3093940",
    "end": "3099400"
  },
  {
    "text": "between small models and\nour largest model as we go along the x-axis\nis much, much bigger",
    "start": "3099400",
    "end": "3106240"
  },
  {
    "text": "for the zero-shot and\ntranslate-train settings than it is for the\nin-language multitask setting.",
    "start": "3106240",
    "end": "3111410"
  },
  {
    "text": "So what this suggests\nto us is basically that the model learns a much\nwider distribution of languages",
    "start": "3111410",
    "end": "3118960"
  },
  {
    "text": "if it has a much larger\namount of parameters. And it is able to do this\nkind of like zero-shot task",
    "start": "3118960",
    "end": "3125650"
  },
  {
    "text": "learning, multilingual task\nlearning much better when it has more parameters.",
    "start": "3125650",
    "end": "3132569"
  },
  {
    "text": "So kind of along those lines,\nlarger models can maybe fit more knowledge\nabout more languages,",
    "start": "3132570",
    "end": "3138595"
  },
  {
    "text": "we had another paper\nwhere we basically tried to answer the\nquestion, you know, how much and what\nkind of knowledge",
    "start": "3138595",
    "end": "3143820"
  },
  {
    "text": "does a model pick up\nduring pre-training? And so to answer that\nquestion, we took a,",
    "start": "3143820",
    "end": "3149250"
  },
  {
    "text": "we basically introduced\na new variant of the question answering task. So the question answering\ntask kind of comes",
    "start": "3149250",
    "end": "3156030"
  },
  {
    "text": "in a couple of\ndifferent flavors. The simplest flavor, which\nI've mentioned already is reading comprehension.",
    "start": "3156030",
    "end": "3161170"
  },
  {
    "text": "And in that case, the\nmodel is basically given a paragraph or\nan article, and then it's asked a question about\nthe paragraph or article,",
    "start": "3161170",
    "end": "3167730"
  },
  {
    "text": "and it has to basically\nextract the answer. So you can see if it's being\nasked, \"what color is a lemon?\"",
    "start": "3167730",
    "end": "3173040"
  },
  {
    "text": "It has to look in the\nparagraph that it's seen, and see that it's, the lemon\nis a yellow fruit and output",
    "start": "3173040",
    "end": "3179970"
  },
  {
    "text": "the word \"yellow\". So this is kind of the simplest\nform of the question answering task. A more difficult form is\nwhat people call open domain",
    "start": "3179970",
    "end": "3186599"
  },
  {
    "start": "3184000",
    "end": "3212000"
  },
  {
    "text": "question answering. And in that case, you\nassume that the model is given a question\nand has access to a large external\ndatabase of knowledge,",
    "start": "3186600",
    "end": "3193950"
  },
  {
    "text": "maybe all of Wikipedia. So the model has\nto do two things. It first has to\nfind the article,",
    "start": "3193950",
    "end": "3200160"
  },
  {
    "text": "or the snippet of\ntext, that contains the answer in the database. And then it has to extract\nthe answer from the article.",
    "start": "3200160",
    "end": "3207819"
  },
  {
    "text": "And so there's this\nadditional retrieval step that makes the problem\nquite a bit harder. But we introduced a\nsort of third variant",
    "start": "3207820",
    "end": "3213828"
  },
  {
    "start": "3212000",
    "end": "3296000"
  },
  {
    "text": "of question answering that\nwe call closed book question answering. The name takes inspiration\nfrom closed book exams.",
    "start": "3213828",
    "end": "3219990"
  },
  {
    "text": "The goal here is that you just\nfeed the model the question. It does not have access to\nan external knowledge source.",
    "start": "3219990",
    "end": "3225570"
  },
  {
    "text": "It cannot look up\ninformation anywhere. It could only answer the\nquestion based on the knowledge that it picked up\nduring pre-training.",
    "start": "3225570",
    "end": "3232440"
  },
  {
    "text": "So if you feed the\nmodel the question, \"what color is a lemon?\" It has to output the\nmodel \"yellow\" correctly because it so, so to speak knows\nthat the lemons are yellow.",
    "start": "3232440",
    "end": "3242289"
  },
  {
    "text": "So this is a good way, we\nargue, of testing the knowledge, the amount of knowledge\nstored in the model",
    "start": "3242290",
    "end": "3248340"
  },
  {
    "text": "during pre-training. So why would we\nexpect that to work? Well, you can\nimagine here, we're",
    "start": "3248340",
    "end": "3254470"
  },
  {
    "text": "doing our normal\npre-training of T5. We're masking out words\nand training the model to predict the masked\nout spans of words.",
    "start": "3254470",
    "end": "3262150"
  },
  {
    "text": "And you might imagine that\nsomewhere during pre-training it sees a sentence that says,\n\"President Franklin blank",
    "start": "3262150",
    "end": "3268180"
  },
  {
    "text": "born blank January 1882.\" The goal here would be to output\n\"D. Roosevelt was blank in\".",
    "start": "3268180",
    "end": "3274840"
  },
  {
    "text": "And then during fine-tuning\nwe train the model to predict when the year 1882,\nwhen it was asked the question,",
    "start": "3274840",
    "end": "3282670"
  },
  {
    "text": "\"when was Franklin\nD. Roosevelt born?\" And you might hope that\nit kind of recalls back",
    "start": "3282670",
    "end": "3287860"
  },
  {
    "text": "to its pre-training task\nand recalls some knowledge that it picked up during\npre-training in order",
    "start": "3287860",
    "end": "3293590"
  },
  {
    "text": "to answer this\nquestion correctly. So we took some standard\nopen domain question",
    "start": "3293590",
    "end": "3300570"
  },
  {
    "start": "3296000",
    "end": "3380000"
  },
  {
    "text": "answering datasets, natural\nquestions, web questions, and trivia QA. And basically removed\nall of the context.",
    "start": "3300570",
    "end": "3307829"
  },
  {
    "text": "And trained our model to\npredict the correct answer when asked some particular\nquestion, and then",
    "start": "3307830",
    "end": "3314040"
  },
  {
    "text": "evaluated its\nperformance on the test set for each of these tasks. And in this table we're\ncomparing the state of the art",
    "start": "3314040",
    "end": "3320160"
  },
  {
    "text": "results for an\nopen domain system, these are systems that\nexplicitly retrieve knowledge from an external\nknowledge source,",
    "start": "3320160",
    "end": "3325829"
  },
  {
    "text": "compared to T5 when it's been\ntrained in this closed book setting.",
    "start": "3325830",
    "end": "3330960"
  },
  {
    "text": "You'll notice that we're\nactually using a slightly different version of\nT5 here, using T5.1.1.",
    "start": "3330960",
    "end": "3336180"
  },
  {
    "text": "The pertinent difference is just\nthat T5.1.1 was not multi-task pre-trained. It was only pre-trained using\nan unsupervised objective.",
    "start": "3336180",
    "end": "3344225"
  },
  {
    "text": "The reason we did\nthat, again, is because we want to measure\nthe amount of knowledge that the model picked\nup during pre-training.",
    "start": "3344225",
    "end": "3350530"
  },
  {
    "text": "And you can see we\nactually got reasonably, strong performance, maybe\nrespectable performance the--",
    "start": "3350530",
    "end": "3355702"
  },
  {
    "text": " I don't want to use the\nperformance, the word",
    "start": "3355702",
    "end": "3362260"
  },
  {
    "text": "performance again. But the accuracy basically\non each of these datasets increases as the\nmodel size increases,",
    "start": "3362260",
    "end": "3368110"
  },
  {
    "text": "which maybe in a\nloose way suggests that the larger models\nhave picked up more knowledge during pre-training.",
    "start": "3368110",
    "end": "3373980"
  },
  {
    "text": "But we ultimately\nlagged behind the state of the art results for\nopen domain systems that explicitly\nretrieve knowledge.",
    "start": "3373980",
    "end": "3380670"
  },
  {
    "start": "3380000",
    "end": "3440000"
  },
  {
    "text": "So to try to close this gap,\nwe made use of this objective called salient span\nmasking from a paper called",
    "start": "3380670",
    "end": "3388560"
  },
  {
    "text": "\"Retrieval Augmented\nLanguage Model Pre-training.\" And salient span masking\nis a very simple idea.",
    "start": "3388560",
    "end": "3394470"
  },
  {
    "text": "The idea is that rather\nthan masking out words at random in your\npre-training objective, you actually mask out\nentities explicitly.",
    "start": "3394470",
    "end": "3402630"
  },
  {
    "text": "So that's people's names,\nplaces, dates et cetera. And you basically just\nuse an off the shelf",
    "start": "3402630",
    "end": "3409710"
  },
  {
    "text": "named entity recognizer to\nfigure out what entities are in your pre-training dataset. And you train the model to\nfill in salient spans instead",
    "start": "3409710",
    "end": "3418290"
  },
  {
    "text": "of random spans. So what we did, is we took\nT5.1.1 after it was pre-trained",
    "start": "3418290",
    "end": "3423869"
  },
  {
    "text": "and did continued pre-training\non salient span masking. And then measured\nthe performance on our downstream tasks\nafter fine tuning.",
    "start": "3423870",
    "end": "3430710"
  },
  {
    "text": "And you can see that the more\nsalient span mask pre-training we did, the better, excuse\nme, the better and better",
    "start": "3430710",
    "end": "3436590"
  },
  {
    "text": "the performance got when we fine\ntuned on the downstream tasks. And we ultimately were able\nto close some of these gaps",
    "start": "3436590",
    "end": "3443220"
  },
  {
    "start": "3440000",
    "end": "3473000"
  },
  {
    "text": "significantly. And actually outperformed the\nbest open domain system on web",
    "start": "3443220",
    "end": "3448260"
  },
  {
    "text": "questions by adding salient\nspan masking to T5.1.1.",
    "start": "3448260",
    "end": "3454680"
  },
  {
    "text": "So this just is a\nmessage to tell you that the objective matters, and\ndoing this kind of now people,",
    "start": "3454680",
    "end": "3461815"
  },
  {
    "text": "at the time people\ndidn't call it this, but now people call this domain\nadaptive, or task adaptive, pre-training.",
    "start": "3461815",
    "end": "3467220"
  },
  {
    "text": "And this is a good way of\ngetting better performance on your downstream tasks.",
    "start": "3467220",
    "end": "3472640"
  },
  {
    "text": "So I'm going to go\nto the questions here, if now is a good time.",
    "start": "3472640",
    "end": "3478160"
  },
  {
    "start": "3473000",
    "end": "3599000"
  },
  {
    "text": "Yes, that'd be great. So for some context,\nthe students in their most recent\nassignment had",
    "start": "3478160",
    "end": "3484100"
  },
  {
    "text": "to make effectively a\nmini T5 thing, where the only questions that were\nasked were from a simple domain",
    "start": "3484100",
    "end": "3489860"
  },
  {
    "text": "so that they could\npre-train on a single GPU. And one of the\nquestions they have is, how can we be sure that the\nanswer produced by the model",
    "start": "3489860",
    "end": "3497930"
  },
  {
    "text": "is not made up? They were asked this on the\nassignments as well, I think. ",
    "start": "3497930",
    "end": "3506492"
  },
  {
    "text": "So if you don't have access\nto a ground truth answer, it's actually very hard to know. There's a nice\npaper that came out",
    "start": "3506492",
    "end": "3512210"
  },
  {
    "text": "after this paper called \"How\nCan We Know When Language Models Know\" and the goal of that paper\nis to make it so that T5 is",
    "start": "3512210",
    "end": "3520250"
  },
  {
    "text": "what we call well-calibrated. And when a model\nis well calibrated it means that when it\ndoesn't know the answer,",
    "start": "3520250",
    "end": "3525800"
  },
  {
    "text": "it doesn't output highly\nconfident predictions. And this paper\nexplored various ways",
    "start": "3525800",
    "end": "3530900"
  },
  {
    "text": "of calibrating T5 for closed\nbook question answering. And they ultimately found\nthat when the model doesn't",
    "start": "3530900",
    "end": "3535970"
  },
  {
    "text": "know the answer when\nit's outputting something made up that they\ncould effectively",
    "start": "3535970",
    "end": "3541250"
  },
  {
    "text": "make it be very unconfident\nin its predictions.",
    "start": "3541250",
    "end": "3546545"
  },
  {
    "text": "So that's one way to do it.  Oh, I think you actually\nare muted, John.",
    "start": "3546545",
    "end": "3554150"
  },
  {
    "text": "Great. Thanks. And then another\nquestion is the knowledge that's necessary for\ndoing this fine tuning",
    "start": "3554150",
    "end": "3560349"
  },
  {
    "text": "like the Q&A on these\nfine tuning datasets, is it knowledge that is all\npresent at pre-training time?",
    "start": "3560350",
    "end": "3569039"
  },
  {
    "text": "Yeah, so that's\nalso something that was explored by a\nsubsequent paper, where it was shown that actually there\nis a decent amount of train",
    "start": "3569040",
    "end": "3575450"
  },
  {
    "text": "and test overlap in terms of\nknowledge in these data sets. So it's definitely\npossible in these cases",
    "start": "3575450",
    "end": "3580970"
  },
  {
    "text": "that the model is\npicking up knowledge during fine tuning\nand not pre-training. However, just as kind of\na side experimental note,",
    "start": "3580970",
    "end": "3591860"
  },
  {
    "text": "we find that the\nperformance of T5 actually plateaus before\nit makes a single pass",
    "start": "3591860",
    "end": "3598970"
  },
  {
    "text": "over the fine tuning dataset. So basically T5 will\nvery, very quickly figure out what the heck\nyou're trying to get it to do.",
    "start": "3598970",
    "end": "3604940"
  },
  {
    "text": "It doesn't even need to\nsee the full training set before it gets basically its\nmaximum performance on the test",
    "start": "3604940",
    "end": "3610040"
  },
  {
    "text": "set. So we don't actually\nthink that, that's a major factor\nfor these results. Great, and then last question.",
    "start": "3610040",
    "end": "3616640"
  },
  {
    "text": "How did the, if you studied it,\nhow did the multi-task model do?",
    "start": "3616640",
    "end": "3622098"
  },
  {
    "text": "Yeah, those results\nare in the paper. The results are almost\nexactly the same. It's just, it's a little easier\nto explain the whole knowledge",
    "start": "3622098",
    "end": "3628009"
  },
  {
    "text": "pre-training thing when you\nare talking about T5.1.1. In the interest of time, I\nmight skip these next three",
    "start": "3628010",
    "end": "3636470"
  },
  {
    "text": "slides, which are, the short\nsummary of these slides is just that the evaluation\nprocedure that we use unfairly",
    "start": "3636470",
    "end": "3643730"
  },
  {
    "text": "penalizes closed book\nquestion answering systems. If you want to learn\na bit more about that you can poke into\nthe paper a bit.",
    "start": "3643730",
    "end": "3649310"
  },
  {
    "text": "It doesn't really\nsupport the main point that I'm trying to make\nin any meaningful way.",
    "start": "3649310",
    "end": "3656040"
  },
  {
    "text": "So, and I want to get to some\nof the more recent papers and I should be able to\nhave time to do that.",
    "start": "3656040",
    "end": "3662660"
  },
  {
    "text": "Cool, so we've kind of\nanswered this question, how much knowledge does a model\npick up during pre-training?",
    "start": "3662660",
    "end": "3670010"
  },
  {
    "text": "The answer is arguably a lot. So kind of a follow\nup question is, the model is kind of memorizing\nthis knowledge, right?",
    "start": "3670010",
    "end": "3676880"
  },
  {
    "text": "But does it also memorize,\ndo large language models also memorize stuff that we\ndon't want them to memorize?",
    "start": "3676880",
    "end": "3682400"
  },
  {
    "text": "Specifically like private\ndata, like you can imagine, let's say that somewhere in C4\nis someone's social security",
    "start": "3682400",
    "end": "3688730"
  },
  {
    "text": "number. We probably don't want our model\nto memorize that, and spit it out when we're decoding from it.",
    "start": "3688730",
    "end": "3695570"
  },
  {
    "text": "And we certainly don't\nwant it to happen for unconditional models\nlike GPT-2 or GPT-3.",
    "start": "3695570",
    "end": "3700980"
  },
  {
    "text": "So in this next work we try\nto answer this question here, do large language\nmodels memorize stuff",
    "start": "3700980",
    "end": "3706460"
  },
  {
    "text": "from their pre-training dataset? And we can first\nactually turn to experts",
    "start": "3706460",
    "end": "3712200"
  },
  {
    "text": "and see what experts think. And here are two statements\nmade by the EFF and OpenAI",
    "start": "3712200",
    "end": "3718620"
  },
  {
    "text": "that were sent to the US\nPatent and Trademark Office when there were a call\nfor comments on basically exactly this question.",
    "start": "3718620",
    "end": "3724920"
  },
  {
    "text": "And you can see\nthat in both cases, these organizations\nbasically say,",
    "start": "3724920",
    "end": "3730440"
  },
  {
    "text": "there's basically no reason to\nbelieve that a large language model would output, would copy\ndata from its training dataset.",
    "start": "3730440",
    "end": "3738270"
  },
  {
    "text": "And OpenAI kind of calls this\na well constructed AI system. And I think what\nthey actually mean",
    "start": "3738270",
    "end": "3744029"
  },
  {
    "text": "by that, If you read their\nstatement a little longer, they kind of say if\nyou construct an AI system appropriately, the\nAI system will not overfit",
    "start": "3744030",
    "end": "3751070"
  },
  {
    "text": "to the training dataset. And if it's not\noverfit, we don't expect them to actually output\nany of their training set",
    "start": "3751070",
    "end": "3757800"
  },
  {
    "text": "in any non-trivial way.  So these are kind of\nstatements that were hunches.",
    "start": "3757800",
    "end": "3765600"
  },
  {
    "text": "And in this work we tried to\ninvestigate more rigorously whether they were true.",
    "start": "3765600",
    "end": "3770750"
  },
  {
    "text": "And the way that we\ndid that was basically by taking the pre-trained GPT-2\nmodel and feeding it prefixes.",
    "start": "3770750",
    "end": "3776560"
  },
  {
    "text": "So you can imagine, you take\na causal language model, like GPT-2 that just predicts\ntokens auto-regressively, you",
    "start": "3776560",
    "end": "3784320"
  },
  {
    "text": "feed it a prefix and then\nask it to basically predict what comes next. And we're showing here\nthis sort of odd prefix,",
    "start": "3784320",
    "end": "3790650"
  },
  {
    "text": "\"East Stroudsburg\nStroudsburg\" that we found when we fed this particular\nprefix into GPT-2,",
    "start": "3790650",
    "end": "3795720"
  },
  {
    "text": "it actually output\nverbatim an address, name, email address, phone number,\nand fax number of a real person",
    "start": "3795720",
    "end": "3802560"
  },
  {
    "text": "that appears on the internet. This example actually\nonly appears six times on the entire public internet.",
    "start": "3802560",
    "end": "3808799"
  },
  {
    "text": "So it's unlikely that GPT-2 saw\nthis address very many times.",
    "start": "3808800",
    "end": "3814350"
  },
  {
    "text": "And the main point of\nthis work is that yes, it does seem like GPT-2\nat least has memorized",
    "start": "3814350",
    "end": "3820650"
  },
  {
    "text": "a significant amount of\nnon-trivial information from its pre-training dataset.",
    "start": "3820650",
    "end": "3826080"
  },
  {
    "text": "So how did we\nundertake this study? We used this procedure that we\nare, that's shown on the screen",
    "start": "3826080",
    "end": "3831920"
  },
  {
    "text": "here. We basically consider\nthree different ways of sampling data from GPT-2.",
    "start": "3831920",
    "end": "3838310"
  },
  {
    "text": "The first is just a sample\nauto-regressively from it. The next is to sample\nauto-regressively",
    "start": "3838310",
    "end": "3843349"
  },
  {
    "text": "but with a decay in temperature. This basically\nmeans that you want the model to become\nmore and more",
    "start": "3843350",
    "end": "3849230"
  },
  {
    "text": "confident in its predictions\nover the course of sampling. And the last option is to take\nrandom text from the internet",
    "start": "3849230",
    "end": "3856370"
  },
  {
    "text": "and use that as conditioning\nto GPT-2 before asking it to generate what comes next.",
    "start": "3856370",
    "end": "3861450"
  },
  {
    "text": "So now for each\nof these samples, for each of these generations,\neach of these 200,000",
    "start": "3861450",
    "end": "3866750"
  },
  {
    "text": "generations we did for each\nof these sampling methods, we want a way of kind of\ntrying to predict whether it",
    "start": "3866750",
    "end": "3872330"
  },
  {
    "text": "might be memorized or not. And so we came up\nwith six metrics to use to give us a notion of\nwhether a particular sample",
    "start": "3872330",
    "end": "3880340"
  },
  {
    "text": "from GPT-2 might be memorized. All of these different\nmetrics basically make use of GPT-2's\nperplexity for the sample.",
    "start": "3880340",
    "end": "3887780"
  },
  {
    "text": "The perplexity, as I think\nyou probably learned, is basically a measure of\nhow confident GPT-2 was",
    "start": "3887780",
    "end": "3895460"
  },
  {
    "text": "in generating this\nparticular sample. You can also think of it as\na measure of compression.",
    "start": "3895460",
    "end": "3902080"
  },
  {
    "text": "So these metrics all make\nuse of the perplexity, either we just measure GPT-2's\nperplexity for the thing",
    "start": "3902080",
    "end": "3909440"
  },
  {
    "text": "that it generated, or we compute\nthe ratio of GPT-2's perplexity",
    "start": "3909440",
    "end": "3915680"
  },
  {
    "text": "to the perplexity for\nanother variant of GPT, or to a text compression\nlibrary called zlib.",
    "start": "3915680",
    "end": "3922070"
  },
  {
    "text": "We also compared via\na ratio the perplexity of the original sample\nversus a lowercase version",
    "start": "3922070",
    "end": "3927290"
  },
  {
    "text": "of the sample. And also a windowed\nperplexity where we only compute the perplexity over\na small window of the sample",
    "start": "3927290",
    "end": "3933320"
  },
  {
    "text": "instead of the whole thing. We take the top, we do some\ndeduplication on the generation",
    "start": "3933320",
    "end": "3940910"
  },
  {
    "text": "and then choose the top\n100 generations according to each of these metrics. So that will\nultimately give us 600",
    "start": "3940910",
    "end": "3946610"
  },
  {
    "text": "possible memorized generations. And then we actually just\ndo a basic, excuse me, a basic Google\nsearch to see if we",
    "start": "3946610",
    "end": "3953150"
  },
  {
    "text": "can find that text\nthat GPT-2 generated on the internet somewhere.",
    "start": "3953150",
    "end": "3958490"
  },
  {
    "text": "And if we do find\nit on the internet somewhere we ask the\nGPT-2 authors was this in the training\nset or not, and they",
    "start": "3958490",
    "end": "3964755"
  },
  {
    "text": "checked for all of\nthe examples that we found and let us know if GPT-2\nactually spit out something from its training dataset.",
    "start": "3964755",
    "end": "3972380"
  },
  {
    "text": "So just to give you an idea\nof what these metrics are and why they might be helpful. This scatterplot is showing the\nperplexity assigned by GPT-2",
    "start": "3972380",
    "end": "3981950"
  },
  {
    "text": "and the perplexity\nassigned by zlib to 200,000 samples generated by GPT-2.",
    "start": "3981950",
    "end": "3988430"
  },
  {
    "text": "and you can see that most of\nthem kind of fall on this line, there's a big clump of them\non the right there in gray.",
    "start": "3988430",
    "end": "3995569"
  },
  {
    "text": "But highlighted\nhere in red and blue are samples that\nkind of are outliers,",
    "start": "3995570",
    "end": "4002440"
  },
  {
    "text": "GPT-2 is assigning a much\nlower perplexity to zlib, sorry, to those\nsamples than zlib is.",
    "start": "4002440",
    "end": "4009099"
  },
  {
    "text": "And what that means is\nthat GPT-2 is very, very good at predicting what comes\nnext in these samples and zlib",
    "start": "4009100",
    "end": "4014860"
  },
  {
    "text": "is not. Zlib is kind of an\nunbiased source, right.",
    "start": "4014860",
    "end": "4019990"
  },
  {
    "text": "It's not really pre-trained\non a bunch of data. It's kind of data agnostic. So it might be the\ncase that if GPT-2",
    "start": "4019990",
    "end": "4026560"
  },
  {
    "text": "is very good at predicting what\ncomes next in a given sequence, but zlib is not, that GPT-2\nhas memorized those samples.",
    "start": "4026560",
    "end": "4033342"
  },
  {
    "text": "So all of these things\nkind of in the top left there are possible\nmemorized samples. And actually we marked\nthe ones in blue",
    "start": "4033342",
    "end": "4039520"
  },
  {
    "text": "that it turned out, were\nactually in the training data set and that GPT-2\nhad memorized.",
    "start": "4039520",
    "end": "4046270"
  },
  {
    "text": "Overall, we found many examples\nof verbatim text memorized from the training dataset.",
    "start": "4046270",
    "end": "4052599"
  },
  {
    "text": "This included news, log files,\nlicenses, pages from Wikipedia,",
    "start": "4052600",
    "end": "4057910"
  },
  {
    "text": "URLs. And we highlight\ntwo types of data here that we found that\nGPT-2 had memorized",
    "start": "4057910",
    "end": "4065619"
  },
  {
    "text": "that in our opinion constitute\nprivate information, like named individuals\nfrom non-news samples,",
    "start": "4065620",
    "end": "4071410"
  },
  {
    "text": "or contact information like\nthe example I showed early on. And so you might ask, OK,\nmaybe it's not that surprising",
    "start": "4071410",
    "end": "4079500"
  },
  {
    "text": "that GPT-2 memorized\na news article if that news article\nappears hundreds of times on the internet.",
    "start": "4079500",
    "end": "4084569"
  },
  {
    "text": "We actually got lucky\nbecause it turned out there were a bunch of\nexamples of memorized data",
    "start": "4084570",
    "end": "4089760"
  },
  {
    "text": "that only appeared\non one document on the entire public internet. It was basically a\npaste, like a paste",
    "start": "4089760",
    "end": "4097740"
  },
  {
    "text": "of a bunch of URLs from Reddit,\nfrom a controversial subreddit called The Donald. And all of these URLs had\nexactly the same form.",
    "start": "4097740",
    "end": "4105359"
  },
  {
    "text": "It was\nhttp://reddit.com/r/thedonald/ a bunch of random\nnumbers and letters,",
    "start": "4105359",
    "end": "4112920"
  },
  {
    "text": "slash the name of the thread. Now this random numbers\nand letters part is nice because it's equally\nhard to predict in all cases.",
    "start": "4112920",
    "end": "4121259"
  },
  {
    "text": "It's basically a random hash. So we know that\npart of the sequence should be equally hard\nfor any model to memorize.",
    "start": "4121260",
    "end": "4128818"
  },
  {
    "text": "And what that means is\nthat we can memorize-- we can measure how many times\ndoes a particular URL need",
    "start": "4128819",
    "end": "4136439"
  },
  {
    "text": "to appear in this list of URLs,\nbecause there were repetitions in the list, in order for\none of the particular GPT-2",
    "start": "4136439",
    "end": "4143339"
  },
  {
    "text": "sized models to memorize it. And what we found was that the\nlargest variant GPT-2, GPT-2",
    "start": "4143340",
    "end": "4150660"
  },
  {
    "text": "XL, memorized a URL\nthat appeared 33 times",
    "start": "4150660",
    "end": "4157410"
  },
  {
    "text": "in this particular\ndocument but not URLs that appeared 17 times or fewer.",
    "start": "4157410",
    "end": "4162930"
  },
  {
    "text": "The medium sized model was only\nreally able to fully memorize a URL that appeared 56 times.",
    "start": "4162930",
    "end": "4168989"
  },
  {
    "text": "And the small model really\ndidn't memorize any. The half basically\nmeans that we could get it to spit out the URL\nif we gave it some additional",
    "start": "4168990",
    "end": "4175229"
  },
  {
    "text": "prompting. We basically hinted at\nsome of the numbers where. And what the takeaway of\nthis is that we actually",
    "start": "4175229",
    "end": "4181979"
  },
  {
    "text": "by coincidence, because there\nis this particular document with the structure, we were able\nto say reasonably confidently",
    "start": "4181979",
    "end": "4188310"
  },
  {
    "text": "that larger models tend\nto memorize more data. They need to see\nexamples, they need",
    "start": "4188310",
    "end": "4193318"
  },
  {
    "text": "to see particular examples\nfewer times in order to memorize them,\nwhich we thought was an interesting finding.",
    "start": "4193319",
    "end": "4199380"
  },
  {
    "text": " So, so far I have\nkind of mostly been",
    "start": "4199380",
    "end": "4205800"
  },
  {
    "text": "talking about the benefits\nof larger models, right. Because larger models\ndid better on SuperGLUE,",
    "start": "4205800",
    "end": "4210840"
  },
  {
    "text": "larger models did better on\nclosed book question answering. Of course, there is this\ncaveat that larger models also",
    "start": "4210840",
    "end": "4216360"
  },
  {
    "text": "seem to be better at memorizing\ntheir training data set. But larger models are\nalso inconvenient.",
    "start": "4216360",
    "end": "4222869"
  },
  {
    "text": "They are more computationally\nexpensive to run. They consume more energy.",
    "start": "4222870",
    "end": "4227880"
  },
  {
    "text": "And they don't fit\non, for example T5 11B doesn't fit\non a single GPU unless you use kind\nof clever methods.",
    "start": "4227880",
    "end": "4237030"
  },
  {
    "text": "So the last paper\nthat I'll discuss, which is super recent\nwork is, can we",
    "start": "4237030",
    "end": "4242790"
  },
  {
    "text": "basically close\nthe performance gap between large and small\nmodels through improvements to the transformer architecture?",
    "start": "4242790",
    "end": "4248590"
  },
  {
    "text": "So in this work we take\nbasically the same strategy that we took in\nthe T5 paper, where we took sort of the landscape\nof existing modifications",
    "start": "4248590",
    "end": "4256349"
  },
  {
    "text": "to the transformer\narchitecture and evaluated them in the same exact setting. And there have been\nlots of variants",
    "start": "4256350",
    "end": "4263070"
  },
  {
    "text": "proposed to the\ntransformer architecture. In T5 we use basically the\nstandard encoder-decoder architecture from the\n\"Attention Is All You",
    "start": "4263070",
    "end": "4269490"
  },
  {
    "text": "Need\" Paper, that's\nvisualized here. But there have been lots\nand lots of modifications that have been proposed since\nthe transformer was released",
    "start": "4269490",
    "end": "4276797"
  },
  {
    "text": "in 2017. For example, maybe\npeople suggested that you should factorize\nyour embedding matrix.",
    "start": "4276797",
    "end": "4283380"
  },
  {
    "text": "You should share the embedding\nmatrix and the softmax output layer. You should use different\nforms of softmax,",
    "start": "4283380",
    "end": "4288930"
  },
  {
    "text": "like a mixture of softmax's,\nor an adaptive softmax. Different ways of\nnormalizing, or initializing,",
    "start": "4288930",
    "end": "4294840"
  },
  {
    "text": "the model, maybe different\nattention mechanisms, alternatives to\nattention mechanisms",
    "start": "4294840",
    "end": "4301110"
  },
  {
    "text": "like lightweight and\ndynamical convolutions. Different nonlinearities\nin the feed-forward layers, different structures for\nthe feed-forward layers,",
    "start": "4301110",
    "end": "4308070"
  },
  {
    "text": "like a mixture of experts\nto the switch transformer. Completely different\narchitectures that were inspired\nby the transformer,",
    "start": "4308070",
    "end": "4314340"
  },
  {
    "text": "like the funnel\ntransformer, evolved transformer, the universal\ntransformer, and so on. Really, there have just been\ntons and tons and tons of them.",
    "start": "4314340",
    "end": "4321190"
  },
  {
    "text": "And again, the\ngoal in this paper was to take a bunch\nof these modifications and apply the same basic\nmethodology from the T5 paper",
    "start": "4321190",
    "end": "4328290"
  },
  {
    "text": "where we test them in the\nsame experimental setting. Specifically, we basically\ntested them in exactly the T5",
    "start": "4328290",
    "end": "4333840"
  },
  {
    "text": "setting that I described at\nthe beginning of the talk, where we pre-trained a\nT5 base sized model on C4",
    "start": "4333840",
    "end": "4342690"
  },
  {
    "text": "and then fine tune it on\na few downstream tasks. I won't discuss\nthat too much more",
    "start": "4342690",
    "end": "4347789"
  },
  {
    "text": "because I gave a pretty\nthorough introduction to it at the beginning of the talk. And so here is a sort of\na first set of results",
    "start": "4347790",
    "end": "4356170"
  },
  {
    "text": "that I'll show you. Along the x-axis are different\ntransformer modifications, I'm not labeling\nwhich one is which",
    "start": "4356170",
    "end": "4362020"
  },
  {
    "text": "because I don't want to call\nany particular modification out. This is the validation\nloss attained by the model",
    "start": "4362020",
    "end": "4370480"
  },
  {
    "text": "for the pre-training objective. So when we do\npre-training on C4, we hold out some\ndata from C4 and then",
    "start": "4370480",
    "end": "4375910"
  },
  {
    "text": "we basically measure\nthe validation loss on the held out data. So lower is better in this case.",
    "start": "4375910",
    "end": "4382340"
  },
  {
    "text": "And you can see this\ndotted black line is the performance\nof the baseline model without any transformer\nmodifications.",
    "start": "4382340",
    "end": "4388360"
  },
  {
    "text": "It's basically a\nvanilla transformer. And you can see that actually\nsome of the transformer modifications did attain better\nperformance, which was good.",
    "start": "4388360",
    "end": "4396639"
  },
  {
    "text": "But a lot of them\ndidn't and a lot of them actually got significantly\nworse performance. And but maybe even worse\nsome of these variants",
    "start": "4396640",
    "end": "4405280"
  },
  {
    "text": "of the transformer that\nattained better performance were pretty minor\nchanges, like for example just taking the\nReLU in the dense--",
    "start": "4405280",
    "end": "4411790"
  },
  {
    "text": "ReLU dense layer and swapping\nit with another non-linearity. So it's a pretty minor change.",
    "start": "4411790",
    "end": "4418190"
  },
  {
    "text": "And some of the other really\nhighly performant ones were actually ultimately\nmore expensive models.",
    "start": "4418190",
    "end": "4423340"
  },
  {
    "text": "We did use the same base model. It was the same\nT5 base size model but some of these methods\nlike the switch transformer,",
    "start": "4423340",
    "end": "4431559"
  },
  {
    "text": "for example, increases the\nparameter count dramatically. So it has, it's more\nexpensive in terms of memory.",
    "start": "4431560",
    "end": "4437800"
  },
  {
    "text": "Some of the other methods\nkind of by coincidence maybe if you make\nthe model deeper, it's not able to make use of\nit, make use of the accelerator",
    "start": "4437800",
    "end": "4446620"
  },
  {
    "text": "as efficiently. And so it makes the\ntraining time and inference time a little more expensive.",
    "start": "4446620",
    "end": "4452900"
  },
  {
    "text": "So once you factor out\nthe very simple changes to the transformer and the\nones that ultimately made",
    "start": "4452900",
    "end": "4458170"
  },
  {
    "text": "the model more expensive\nalong some axis there actually were very few, if\nany modifications that improved",
    "start": "4458170",
    "end": "4464020"
  },
  {
    "text": "performance meaningfully. And this is true on\nthe pre-training task.",
    "start": "4464020",
    "end": "4470510"
  },
  {
    "text": "It's also true on the\ndownstream tasks we considered. So this is the\nROUGE-2 score, it's just one of the metrics\npeople use on the XSum task.",
    "start": "4470510",
    "end": "4478990"
  },
  {
    "text": "XSum is, you can sort\nof think of it like, a harder version of CNN Daily\nMail via the summarization task.",
    "start": "4478990",
    "end": "4484130"
  },
  {
    "text": "And you can see that\nthe model variants that attained a better\nvalidation score",
    "start": "4484130",
    "end": "4489590"
  },
  {
    "text": "tended to also attain a\nbetter XSum ROUGE-2 score. But again almost all of\nthe variants we tried",
    "start": "4489590",
    "end": "4496730"
  },
  {
    "text": "decreased the performance. And just as an aside I kind of\nalluded to this a little bit,",
    "start": "4496730",
    "end": "4503480"
  },
  {
    "text": "there is a reasonably\ngood correlation between the validation loss\nand the SuperGLUE score.",
    "start": "4503480",
    "end": "4508992"
  },
  {
    "text": "Although I'll just point\nout a couple of interesting points here. One is this method called\ntransparent attention.",
    "start": "4508993",
    "end": "4514940"
  },
  {
    "text": "It attained a pretty\ngood validation loss but ultimately a very\nbad SuperGLUE score which was surprising to us.",
    "start": "4514940",
    "end": "4521840"
  },
  {
    "text": "The switch transformer,\nwhich I'll highlight here, attained the best\nvalidation loss but it did not get the\nbest SuperGLUE score.",
    "start": "4521840",
    "end": "4529040"
  },
  {
    "text": "On the closed book\nvariant of web questions, the switch transformer actually\nachieveed nearly the best",
    "start": "4529040",
    "end": "4535520"
  },
  {
    "text": "validation and accuracy. And we, this kind of supports a\nloose conjecture in the field,",
    "start": "4535520",
    "end": "4541520"
  },
  {
    "text": "that scaling up the\nnumber of parameters improves the amount of knowledge\nthat the model can internalize.",
    "start": "4541520",
    "end": "4548120"
  },
  {
    "text": "But it doesn't help\nthe model reason. So kind of very, very\nloosely speaking again",
    "start": "4548120",
    "end": "4553640"
  },
  {
    "text": "this is kind of a\nconjecture, SuperGLUE requires deep\nreasoning capabilities.",
    "start": "4553640",
    "end": "4559075"
  },
  {
    "text": "closed book web questions\nrequires knowledge intensive capabilities. And so a switch transformer,\nwhich only scales up",
    "start": "4559075",
    "end": "4565580"
  },
  {
    "text": "the parameter count without\nscaling up the processing, maybe does better on\nthe web questions task.",
    "start": "4565580",
    "end": "4573170"
  },
  {
    "text": "So this kind of raises\na number, should raise some red flags for you. Because this is a pretty bold\nclaim that most of these things",
    "start": "4573170",
    "end": "4580100"
  },
  {
    "text": "don't actually help that much. And there's kind of a\ncouple of possible reasons that this could be the case.",
    "start": "4580100",
    "end": "4586315"
  },
  {
    "text": "One is that our\ncode base is just very unusual and non-standard. We don't think this is the case\nbecause the code base that we",
    "start": "4586315",
    "end": "4592760"
  },
  {
    "text": "used was actually developed by\none of the people who invented the transformer, Noam Shazeer.",
    "start": "4592760",
    "end": "4598850"
  },
  {
    "text": "And it's been used a lot. It's been used in lots\nof various papers. It's basically the same as\nthe tensor2tensor code base.",
    "start": "4598850",
    "end": "4606949"
  },
  {
    "text": "And so we think that\narguably our code base and the implementation detail\nshould be reasonably standard.",
    "start": "4606950",
    "end": "4612770"
  },
  {
    "text": "Maybe the tasks we\nconsider are non-standard. We think this is\nprobably not true.",
    "start": "4612770",
    "end": "4618350"
  },
  {
    "text": "Pre-training followed by\nfine tuning is pretty common. Basically, all the\ntasks we tested out on",
    "start": "4618350",
    "end": "4624140"
  },
  {
    "text": "have state of the art\nresults from transformers. And actually we included\nseparately supervised",
    "start": "4624140",
    "end": "4629600"
  },
  {
    "text": "only training on\nWMT English German, which was the task that\ntransformer was actually",
    "start": "4629600",
    "end": "4635840"
  },
  {
    "text": "proposed on. Maybe we need more\nhyperparameter tuning, because we didn't,\nagain, we didn't",
    "start": "4635840",
    "end": "4641570"
  },
  {
    "text": "do significant\nhyperparameter tuning for each of these methods. To test how true\nthis was we actually took one of the methods that\nperformed significantly worse",
    "start": "4641570",
    "end": "4649730"
  },
  {
    "text": "than we expected. And we ran maybe a\ncouple hundred trials of hyperparameter\noptimization, one",
    "start": "4649730",
    "end": "4654867"
  },
  {
    "text": "of the researchers\non this paper spent a long time trying to\nget hyperparameters right to make it work. And it ultimately never worked\nas well as the baseline method.",
    "start": "4654867",
    "end": "4664010"
  },
  {
    "text": "A next possibility that we've\nimplemented these modifications incorrectly. To sanity check\nthis, we actually emailed the authors of all of\nthe different modifications",
    "start": "4664010",
    "end": "4671540"
  },
  {
    "text": "and asked them to check\nour implementation. All of the ones\nthat got back to us said that it looked\ncorrect to them.",
    "start": "4671540",
    "end": "4676922"
  },
  {
    "text": "And then finally\nthe last option is that maybe these modifications\nof the transformer don't really kind of transfer.",
    "start": "4676922",
    "end": "4682550"
  },
  {
    "text": "They don't transfer across code\nbases, and implementations, and applications. And to us at least, based on\nthe evidence that we have this",
    "start": "4682550",
    "end": "4689810"
  },
  {
    "text": "is a plausible possibility. In my opinion, the best\nway to control for this is if you're proposing\na new modification",
    "start": "4689810",
    "end": "4696679"
  },
  {
    "text": "to the transformer, try to\napply it to as many code bases and tasks as you can without\ntweaking hyperparameters.",
    "start": "4696680",
    "end": "4703700"
  },
  {
    "text": "And if it works in all of those\nsettings then you're golden. And your thing\nprobably will transfer. And we think that\nit's probably the case",
    "start": "4703700",
    "end": "4710120"
  },
  {
    "text": "that simpler modifications,\nlike changing the non-linearity, are not so dependent\non hyperparameters",
    "start": "4710120",
    "end": "4716330"
  },
  {
    "text": "and implementation details. And so they may be\nthe ones that are more likely to transfer so to speak.",
    "start": "4716330",
    "end": "4723680"
  },
  {
    "text": "So that's all\ndiscussed in this talk. I recognize that was\nkind of a whirlwind tour. So I've linked all of the papers\nthat I discussed on this slide",
    "start": "4723680",
    "end": "4730630"
  },
  {
    "text": "here. Of course, this was work done\nby a huge and truly amazing group of collaborators over\nthe course of these five papers",
    "start": "4730630",
    "end": "4738340"
  },
  {
    "text": "who I've listed on\nthe screen here. And yeah, I'm happy to answer\nany additional questions",
    "start": "4738340",
    "end": "4743410"
  },
  {
    "text": "that you all have. OK, so thank you so much,\nColin for that great talk. And yeah, it was a bit of\na fire hose of information.",
    "start": "4743410",
    "end": "4751480"
  },
  {
    "text": "I realized also there\nwas one thing I forgot to say in my introduction. So I guess I need\nto have an afterword",
    "start": "4751480",
    "end": "4756820"
  },
  {
    "text": "as well, which is\nthat Colin has now started as a professor at the\nUniversity of North Carolina.",
    "start": "4756820",
    "end": "4764500"
  },
  {
    "text": "So effectively the\nUniversity of North Carolina is playing a big\npart in this course because it was also the\nsource of the Cherokee data",
    "start": "4764500",
    "end": "4772060"
  },
  {
    "text": "that we used for assignment\n4 for the Cherokee English translation. So Go Tar Heels.",
    "start": "4772060",
    "end": "4779140"
  },
  {
    "text": "But yeah, so, yeah, so Colin's\nhappy to stay and answer",
    "start": "4779140",
    "end": "4784990"
  },
  {
    "text": "some questions. So if you'd like to have more\nquestions, use the raise hand",
    "start": "4784990",
    "end": "4790420"
  },
  {
    "text": "and we'll then\nsort of invite you into the where people can\nsee each other Zoom Room.",
    "start": "4790420",
    "end": "4797739"
  },
  {
    "text": "And if you're up\nto it would even be nice to turn on\nyour video so people",
    "start": "4797740",
    "end": "4802750"
  },
  {
    "text": "can see who they're talking to. And yeah, maybe in\nthe first instance",
    "start": "4802750",
    "end": "4809420"
  },
  {
    "text": "you should stop\nsharing the screen. And if there's something\nyou want to show again",
    "start": "4809420",
    "end": "4814460"
  },
  {
    "text": "you can turn it back on. Yeah, maybe I'll\njust say while there are people are still around\non the point of me being",
    "start": "4814460",
    "end": "4822590"
  },
  {
    "text": "a professor at UNC, in the event\nthat there are any master's or undergraduate students in\nthe audience who are applying",
    "start": "4822590",
    "end": "4827990"
  },
  {
    "text": "for PhD programs that the\napplication deadline for UNC has actually, has\nnot occurred yet.",
    "start": "4827990",
    "end": "4833429"
  },
  {
    "text": "So if you maybe want to\napply to another school, have another option, you're\nexcited about the work that I presented, you\ncan still apply to UNC.",
    "start": "4833430",
    "end": "4840980"
  },
  {
    "text": "We have a remarkably late\napplication deadline. So just a plug, in\ncase there's anyone who is looking for a PhD.",
    "start": "4840980",
    "end": "4849139"
  },
  {
    "text": "And UNC is the oldest public\nuniversity in the nation. And we did a full\nUNC advertisement.",
    "start": "4849140",
    "end": "4855409"
  },
  {
    "text": "And I think we have the\nsecond oldest CS department too, which yeah, it's\nbeen around for long.",
    "start": "4855410",
    "end": "4860840"
  },
  {
    "text": "It's pretty small. It's only about\n50 faculty or so.",
    "start": "4860840",
    "end": "4866210"
  },
  {
    "text": "So while we're waiting\nfor someone to join up-- We do have one question already\nactually from [AUDIO OUT]",
    "start": "4866210",
    "end": "4874370"
  },
  {
    "text": "Yeah, hi. Thanks for the lecture. I have a question about earlier\nwhen you discussed like the T5",
    "start": "4874370",
    "end": "4884450"
  },
  {
    "text": "overfitting, and how many\npasses through the dataset it took for it to over fit.",
    "start": "4884450",
    "end": "4890450"
  },
  {
    "text": "So I'm curious as\nto if you think some of the larger models like\nthe 3 billion, 11 billion,",
    "start": "4890450",
    "end": "4898040"
  },
  {
    "text": "ones with the scaled C4\nlayers are overfitting.",
    "start": "4898040",
    "end": "4903380"
  },
  {
    "text": "And kind of generally\nhow do you know when a model is overfitting\nespecially on this scale?",
    "start": "4903380",
    "end": "4910310"
  },
  {
    "text": "Yeah, so I mean if you\nmeasure overfitting sort of the standard\nway where you compare the loss on training data versus\nthe loss on validation data,",
    "start": "4910310",
    "end": "4919040"
  },
  {
    "text": "we see that even in the\nvery, very large models it's roughly the\nsame, which suggests",
    "start": "4919040",
    "end": "4924290"
  },
  {
    "text": "in the traditional sense\nthat there is no overfitting. One reason for that is that\nC4 is actually big enough",
    "start": "4924290",
    "end": "4929810"
  },
  {
    "text": "that we do just over\none pass over it when we train for\na trillion tokens.",
    "start": "4929810",
    "end": "4935960"
  },
  {
    "text": "And you know you might hope\nthat you see limited overfitting when you only see\neach piece of data basically once over\nthe course of training.",
    "start": "4935960",
    "end": "4942747"
  },
  {
    "text": "So it's sort of like\nevery time you're seeing data it's new data. So there's not a huge\ndifference between the data on the training set\nand the validation set.",
    "start": "4942747",
    "end": "4949400"
  },
  {
    "text": "Of course, there\nis also this notion of overfitting that's\nkind of like worst case overfitting, which ties into the\nmemorization work I mentioned.",
    "start": "4949400",
    "end": "4957480"
  },
  {
    "text": "It does seem that it's\npossible for language models to memorize data even when\nthey do relatively few passes",
    "start": "4957480",
    "end": "4964190"
  },
  {
    "text": "over the training\ndataset and you don't see kind of\naverage case overfitting by comparing the training\nloss and the validation loss.",
    "start": "4964190",
    "end": "4971680"
  },
  {
    "text": "I see. That answers that. Thank you. Yep. OK, then [AUDIO OUT]\nthere's a question or two.",
    "start": "4971680",
    "end": "4978020"
  },
  {
    "text": " Sure, sorry I was trying\nto unmute my video but I can't do that\nfor whatever reason.",
    "start": "4978020",
    "end": "4986965"
  },
  {
    "text": "First of all,\nColin, thank you so much for this fantastic lecture. I really enjoyed it. ",
    "start": "4986965",
    "end": "4993440"
  },
  {
    "text": "One thing that I\nparticularly enjoyed was the work that you guys\ndid on your training data",
    "start": "4993440",
    "end": "5000039"
  },
  {
    "text": "extraction attack, trying\nto identify, really test this hunch on OpenAI\nand EFF's point",
    "start": "5000040",
    "end": "5007079"
  },
  {
    "text": "that these models don't\nactually memorize trained data. I'm wondering, I actually\nhave two questions.",
    "start": "5007080",
    "end": "5013570"
  },
  {
    "text": "One, have OpenAI and EFF\nsince changed their tone? Since you actually\nor has your team",
    "start": "5013570",
    "end": "5020119"
  },
  {
    "text": "actually published\nthe results of that? And they since acknowledged\nthat well-constructed models",
    "start": "5020119",
    "end": "5026800"
  },
  {
    "text": "may actually do this? And two, would this\napproach actually work for detecting other say\nexternally encoded biases",
    "start": "5026800",
    "end": "5036250"
  },
  {
    "text": "towards, or like\nextreme biases which are prevalent in\nsome language models? Would you be able\nto create packages",
    "start": "5036250",
    "end": "5043150"
  },
  {
    "text": "that could simulate these sorts\nof attacks on these models and then determine with\nsome degree of accuracy",
    "start": "5043150",
    "end": "5049090"
  },
  {
    "text": "how much these biases\nactually are present? Yeah, so with regards\nto the first question,",
    "start": "5049090",
    "end": "5055940"
  },
  {
    "text": "I don't know of any\nofficial statements that have been made by\nanyone but I will say that actually on\nthe memorization paper",
    "start": "5055940",
    "end": "5062380"
  },
  {
    "text": "we had multiple\nco-authors from OpenAI. So it was very much a\ncooperation with them. I mean, we're all\nscientists and you",
    "start": "5062380",
    "end": "5070480"
  },
  {
    "text": "know we all kind of make\nhypotheses that sometimes turn out correctly and incorrectly. And so I think OpenAI\nis definitely aware",
    "start": "5070480",
    "end": "5079180"
  },
  {
    "text": "and on the side of\nthe fact that yeah, it is possible that these\nmodels might memorize data even",
    "start": "5079180",
    "end": "5085960"
  },
  {
    "text": "when they don't exhibit\nthe traditional signs of overfitting.",
    "start": "5085960",
    "end": "5092560"
  },
  {
    "text": "To the second point, the\nway that people have kind of measured this in\nan ad hoc way is",
    "start": "5092560",
    "end": "5099070"
  },
  {
    "text": "by feeding a prefix\ninto the model about a particular demographic\ngroup, or type of person,",
    "start": "5099070",
    "end": "5104949"
  },
  {
    "text": "and see what the model\nsays about that person. And I think in principle\nyou can kind of",
    "start": "5104950",
    "end": "5112180"
  },
  {
    "text": "think of our approach\nas related to that, except that we have this\nadditional step that kind of",
    "start": "5112180",
    "end": "5118450"
  },
  {
    "text": "measures whether the model is\ngenerating that text because it",
    "start": "5118450",
    "end": "5123730"
  },
  {
    "text": "saw it in its training\ndata, basically because the perplexity\nis excessively",
    "start": "5123730",
    "end": "5129160"
  },
  {
    "text": "low for some continuation\ncompared to a model that wasn't trained on the same data.",
    "start": "5129160",
    "end": "5134929"
  },
  {
    "text": "So it might be\ninteresting, for example, if you feed the model\na prefix that you're",
    "start": "5134930",
    "end": "5140050"
  },
  {
    "text": "asking it to fill in some\noffensive information about some demographic\ngroup, to check",
    "start": "5140050",
    "end": "5145090"
  },
  {
    "text": "whether the perplexity of the\nmodel for its continuation is dramatically lower\nthan zlib, for example.",
    "start": "5145090",
    "end": "5152835"
  },
  {
    "text": "And in that case,\nyou might think that the bias actually, maybe\nthis like bias that the model has picked up is because\nit saw some, actually",
    "start": "5152835",
    "end": "5160840"
  },
  {
    "text": "a sentence that looks just\nlike that in its training data. Or if it's a more\nkind of loose concept,",
    "start": "5160840",
    "end": "5167950"
  },
  {
    "text": "that the model has internalized\nover the course of training. Fantastic. Thank you, very, very\nmuch for sharing.",
    "start": "5167950",
    "end": "5175050"
  },
  {
    "text": "Yeah, thanks for the questions. OK, so next up I\nguess is [AUDIO OUT]",
    "start": "5175050",
    "end": "5180820"
  },
  {
    "text": "Great, thank you for the talk. That was super interesting.",
    "start": "5180820",
    "end": "5185870"
  },
  {
    "text": "So my question is sort\nof what are your thoughts on potentially doing\nlike multiple rounds",
    "start": "5185870",
    "end": "5194510"
  },
  {
    "text": "of pre-trainings, so to make\nit more concrete, you know,",
    "start": "5194510",
    "end": "5200119"
  },
  {
    "text": "like let's say you have a\ntask somewhere like something like response generation.",
    "start": "5200120",
    "end": "5205310"
  },
  {
    "text": "And you've got very bespoke\nto the particular response generation dataset that you\nuse, but potentially you",
    "start": "5205310",
    "end": "5212269"
  },
  {
    "text": "want to kind of spruce\nthat up by bringing in some general dialogue\ndataset that consists",
    "start": "5212270",
    "end": "5218650"
  },
  {
    "text": "of naturalistic human data. So I'm wondering if you kind of\nhave any thoughts or intuitions",
    "start": "5218650",
    "end": "5224030"
  },
  {
    "text": "on how effective it\nis to maybe start with the general internet\nand then fine tune",
    "start": "5224030",
    "end": "5231710"
  },
  {
    "text": "on this dialogue, unstructured\ndialogue dataset, and then fine tune on maybe a more kind\nof tightly scoped response",
    "start": "5231710",
    "end": "5240950"
  },
  {
    "text": "generation dataset. Yeah, so the technique\nyou're describing",
    "start": "5240950",
    "end": "5246380"
  },
  {
    "text": "sounds pretty similar to this\nreally excellent approach that people now call domain\nadaptive pre-training or task",
    "start": "5246380",
    "end": "5254060"
  },
  {
    "text": "adaptive pre-training. I was introduced in\na paper called \"Don't Stop Pre-training,\" and then\nthere's a less catchy subtitle.",
    "start": "5254060",
    "end": "5263390"
  },
  {
    "text": "And the idea is very similar\nto what you proposed. Basically, you take\na pre-trained model that was trained\non generic text,",
    "start": "5263390",
    "end": "5269970"
  },
  {
    "text": "you do what you might call like\nintermediate task training, or you do continued pre-training\non domain specific data.",
    "start": "5269970",
    "end": "5276800"
  },
  {
    "text": "And then finally,\nyou do fine tuning on your specific\nfine tuning dataset. In their case, they're\nconsidering things",
    "start": "5276800",
    "end": "5283100"
  },
  {
    "text": "like doing a scientific\ntext classification, or biomedical text analysis.",
    "start": "5283100",
    "end": "5289070"
  },
  {
    "text": "And when they do an intermediate\npre-training step in domain data, or even just doing\nthe pre-training objective",
    "start": "5289070",
    "end": "5296270"
  },
  {
    "text": "on the data from the task, it\ndefinitely helps significantly. And yeah, so that's a\nvery excellent intuition.",
    "start": "5296270",
    "end": "5304400"
  },
  {
    "text": "And I think that that's the most\nsimilar method to what you're describing.",
    "start": "5304400",
    "end": "5310010"
  },
  {
    "text": "It does kind of raise a clear\nquestion that I don't think has been addressed\nto my knowledge",
    "start": "5310010",
    "end": "5315079"
  },
  {
    "text": "in the literature,\nwhich is we usually think of transfer learning as\npre-train and then fine tune.",
    "start": "5315080",
    "end": "5320390"
  },
  {
    "text": "And now we're doing kind\nof like pre-training, then maybe some more pre-training,\nand then fine tuning.",
    "start": "5320390",
    "end": "5325909"
  },
  {
    "text": "And there are other methods\nthat kind of inject other steps along the way. And so there's this\nnatural question",
    "start": "5325910",
    "end": "5332000"
  },
  {
    "text": "of what should the\ncurriculum of tasks be? How many intermediate\nsteps should there be?",
    "start": "5332000",
    "end": "5339300"
  },
  {
    "text": "What should the\nintermediate steps be? What's the benefit of one\ndomain versus the other?",
    "start": "5339300",
    "end": "5344360"
  },
  {
    "text": "How much domain shift is there? And what are the corresponding\nbenefits and so on? And I think there\nwould be, there's",
    "start": "5344360",
    "end": "5350210"
  },
  {
    "text": "a fascinating line of work\nthat would be better basically better answering\nthose questions.",
    "start": "5350210",
    "end": "5356239"
  },
  {
    "text": "Great. And what was the acronym called?",
    "start": "5356240",
    "end": "5361320"
  },
  {
    "text": "Yeah, so it's called a DAPT\nor TAPT, Domain Adaptive Pre-training or Task\nAdaptive Pre-training.",
    "start": "5361320",
    "end": "5368340"
  },
  {
    "text": "The other paper's called\n\"Don't Stop Pre-training\" which is easy to remember if you like\nthe song \"Don't Stop Believing\"",
    "start": "5368340",
    "end": "5374489"
  },
  {
    "text": "which is how I,\ndon't know if it's an intended reference to that\nsong, I assume it must be.",
    "start": "5374490",
    "end": "5381570"
  },
  {
    "text": "I think they should\nhave done a Don't Stop Pre Trainin'\nwith an apostrophe if they really wanted\nto drive it home but maybe that would\nhave been too cheesy.",
    "start": "5381570",
    "end": "5388410"
  },
  {
    "text": "But anyways yeah,\nthe paper's called \"Don't stop Pre-training.\" All right, that's great. Thank you so much.",
    "start": "5388410",
    "end": "5394170"
  },
  {
    "text": "Yeah. Absolutely. OK, next question is\n[AUDIO OUT] I'm not sure quite",
    "start": "5394170",
    "end": "5399550"
  },
  {
    "text": "what that name corresponds to. Yes, hi. Thank you, Colin for the talk.",
    "start": "5399550",
    "end": "5405260"
  },
  {
    "text": "Found it really interesting. I've got kind of an\nopen ended question. I'm really looking\nfor some advice here.",
    "start": "5405260",
    "end": "5411610"
  },
  {
    "text": "So it feels like the recent\nheadline grabbing advancements in the NLP industry\nhave been achieved",
    "start": "5411610",
    "end": "5417135"
  },
  {
    "text": "by building these\nmassive models, like GPT-3 with billions of\nparameters that oftentimes cost",
    "start": "5417135",
    "end": "5422619"
  },
  {
    "text": "millions of dollars to train. And these advancements\nare funded by like larger organizations,\nlike Google, Facebook, OpenAI,",
    "start": "5422620",
    "end": "5428980"
  },
  {
    "text": "who more or less have infinite\nresources to train, right? So my question is as\na sole practitioner",
    "start": "5428980",
    "end": "5435700"
  },
  {
    "text": "with limited resources but an\ninfinite appetite for learning, what are some ways\nthat I can participate",
    "start": "5435700",
    "end": "5442600"
  },
  {
    "text": "in these advancements\nand kind of just partake in what's happening\nin the industry?",
    "start": "5442600",
    "end": "5448242"
  },
  {
    "text": "Yeah, absolutely. I mean, I actually totally\nsympathize with you and agree with you in\nthe sense that most",
    "start": "5448242",
    "end": "5454283"
  },
  {
    "text": "of the development\nof these models is taking place by small\ngroups behind closed doors at large corporations.",
    "start": "5454283",
    "end": "5461010"
  },
  {
    "text": "And that's not usually how I\nlike to see science developed. I like to see it as a\ncommunity endeavor that",
    "start": "5461010",
    "end": "5467070"
  },
  {
    "text": "involves all kinds of\nstakeholders with varying amounts of resources. And we're not really quite at\nthat stage with this work yet.",
    "start": "5467070",
    "end": "5474600"
  },
  {
    "text": "I do think that to the\nextent that people are still releasing pre-trained models,\nwhich is true for example,",
    "start": "5474600",
    "end": "5481679"
  },
  {
    "text": "for T5 but not for GPT-3. There is a lot of work to be\ndone on basically analysis.",
    "start": "5481680",
    "end": "5488850"
  },
  {
    "text": "Some of the stuff that we\nwere discussing earlier, you know even the memorization\nwork is basically I",
    "start": "5488850",
    "end": "5495150"
  },
  {
    "text": "would say it's\nlike analysis where some of the stuff\npertaining to bias involves and analyzing these models.",
    "start": "5495150",
    "end": "5501090"
  },
  {
    "text": "And I think there's so\nlittle that we actually know about how these models work.",
    "start": "5501090",
    "end": "5507060"
  },
  {
    "text": "And what makes them\nuseful at scale that there's plenty of room\nfor interesting analytical work",
    "start": "5507060",
    "end": "5515730"
  },
  {
    "text": "which requires\nsignificantly less compute.  I guess I would say a\ncouple other things.",
    "start": "5515730",
    "end": "5524390"
  },
  {
    "text": "One is that I do really\nhope that the field moves more towards community\ndevelop models and moves",
    "start": "5524390",
    "end": "5529620"
  },
  {
    "text": "towards frameworks that allow\npeople to collaboratively train a model, for example like\nin a distributed fashion.",
    "start": "5529620",
    "end": "5536615"
  },
  {
    "text": "I think that that's an\nincredibly exciting research direction. It's something that I'm\nworking with my students on in my lab at UNC now.",
    "start": "5536615",
    "end": "5544500"
  },
  {
    "text": "And the last thing I'll say,\nand I actually don't usually like saying this, but\nI'll say it anyways.",
    "start": "5544500",
    "end": "5551250"
  },
  {
    "text": "I do think that our\nfield often undergoes sort of a tick-tock pattern,\nwhere we show something",
    "start": "5551250",
    "end": "5558840"
  },
  {
    "text": "is possible at\nscale, and then we show that the scale is\nnot necessary to achieve the same thing. And to some extent,\nyou could argue",
    "start": "5558840",
    "end": "5564989"
  },
  {
    "text": "that this has happened\nalready for GPT-3 in the sense that we saw GPT-3\ncome along, get outstanding results\non for example",
    "start": "5564990",
    "end": "5571440"
  },
  {
    "text": "SuperGLUE with only\n32 examples per class. And then there\nwas the paper that proposed this method\ncalled IPET, which I think",
    "start": "5571440",
    "end": "5578730"
  },
  {
    "text": "is interactive-- an iterative\npattern exploited training, that is basically comparable\nperformance in a dramatically",
    "start": "5578730",
    "end": "5586380"
  },
  {
    "text": "smaller model with the\nsame amount of data. And you can point\nto other examples.",
    "start": "5586380",
    "end": "5594690"
  },
  {
    "text": "I personally like to attribute\nthe story of attention's invention to the fact that\nresearchers at the Montreal",
    "start": "5594690",
    "end": "5601020"
  },
  {
    "text": "Institute for\nLearning Algorithms couldn't afford an\neight GPU machine, so they couldn't run the\ngiant LSTM in the sequence",
    "start": "5601020",
    "end": "5606900"
  },
  {
    "text": "to sequence paper. So they needed to\ninvent something that worked better but didn't\nrequire such a big model so they invented attention.",
    "start": "5606900",
    "end": "5614070"
  },
  {
    "text": "Of course, it's not good\nadvice to give, to tell someone that they should just go\ninvent something smaller but I am at least hopeful that\nsome of these things that we've",
    "start": "5614070",
    "end": "5621360"
  },
  {
    "text": "shown are possible at scale\nare also possible at a much smaller scale.",
    "start": "5621360",
    "end": "5627820"
  },
  {
    "text": "So thank you. Yeah.  And I think there's\nno one else who",
    "start": "5627820",
    "end": "5635370"
  },
  {
    "text": "has a hand up at the moment. Maybe now's the moment for\nJohn to ask his question, but if any other\npeople have questions,",
    "start": "5635370",
    "end": "5642580"
  },
  {
    "text": "now's a good point to jump in. I was just going to ask\na question from the Q&A, [AUDIO OUT] came in\nand asked it live so.",
    "start": "5642580",
    "end": "5652470"
  },
  {
    "text": "Yeah, I will say one other thing\njust quickly, which is that T5,",
    "start": "5652470",
    "end": "5657962"
  },
  {
    "text": "I was very, like I said I was\nvery excited that we achieved near human performance\non SuperGLUE.",
    "start": "5657962",
    "end": "5663060"
  },
  {
    "text": "The model that came along and\nactually closed that 0.5% gap is a model that is about\n10 times smaller in terms",
    "start": "5663060",
    "end": "5670289"
  },
  {
    "text": "of parameter count. So that that's like another\nreasonable example of, I mean it's still\nquite big but at least",
    "start": "5670290",
    "end": "5676530"
  },
  {
    "text": "as you make algorithmic and\narchitectural improvements sometimes you can\nclose these gaps.",
    "start": "5676530",
    "end": "5684090"
  },
  {
    "text": "Well, thank you again, Colin. And let you\nwhatever, have a beer and go to bed or something.",
    "start": "5684090",
    "end": "5691020"
  },
  {
    "text": "Yeah, yeah, sounds great. Yeah, thanks again\nfor having me. It's such a pleasure\nso, and I should",
    "start": "5691020",
    "end": "5696630"
  },
  {
    "text": "say if anyone has any\nfollow up questions they think of later on I'm\nalways excited to get emails about this kind of stuff.",
    "start": "5696630",
    "end": "5701940"
  },
  {
    "text": "It's the stuff I\nlike working on, so. OK, thank you again for the\ngreat and informative talk.",
    "start": "5701940",
    "end": "5708770"
  },
  {
    "start": "5708770",
    "end": "5713152"
  }
]