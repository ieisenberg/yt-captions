[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "start": "0",
    "end": "4330"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome back, everyone.",
    "start": "4330",
    "end": "6080"
  },
  {
    "text": "This is part 3 in our\nseries on distributed word",
    "start": "6080",
    "end": "8080"
  },
  {
    "text": "representations.",
    "start": "8080",
    "end": "8960"
  },
  {
    "text": "We're going to be talking about\nvector comparison methods.",
    "start": "8960",
    "end": "12300"
  },
  {
    "start": "12000",
    "end": "102000"
  },
  {
    "text": "To try to make this\ndiscussion pretty intuitive,",
    "start": "12300",
    "end": "14430"
  },
  {
    "text": "I'm going to ground things\nin this running example.",
    "start": "14430",
    "end": "17250"
  },
  {
    "text": "On the left, I have a very\nsmall vector space model.",
    "start": "17250",
    "end": "20190"
  },
  {
    "text": "We have three\nwords, A, B, and C.",
    "start": "20190",
    "end": "22100"
  },
  {
    "text": "And you can imagine that we've\nmeasured two dimensions, dx",
    "start": "22100",
    "end": "24517"
  },
  {
    "text": "and dy.",
    "start": "24517",
    "end": "25590"
  },
  {
    "text": "You could think of them as\ndocuments if you wanted.",
    "start": "25590",
    "end": "28173"
  },
  {
    "text": "There are two perspectives\nthat you might take",
    "start": "28173",
    "end": "30090"
  },
  {
    "text": "on this vector space model.",
    "start": "30090",
    "end": "31990"
  },
  {
    "text": "The first is just at the\nlevel of raw frequency,",
    "start": "31990",
    "end": "35190"
  },
  {
    "text": "B and C seem to be united.",
    "start": "35190",
    "end": "36780"
  },
  {
    "text": "They are frequent in both\nthe x and the y dimension.",
    "start": "36780",
    "end": "40020"
  },
  {
    "text": "Whereas A is comparatively\ninfrequent along both",
    "start": "40020",
    "end": "43570"
  },
  {
    "text": "those dimensions.",
    "start": "43570",
    "end": "44970"
  },
  {
    "text": "That's the first perspective.",
    "start": "44970",
    "end": "46600"
  },
  {
    "text": "The second perspective,\nthough, is more subtle.",
    "start": "46600",
    "end": "48960"
  },
  {
    "text": "You might just observe\nthat if we kind of correct",
    "start": "48960",
    "end": "51570"
  },
  {
    "text": "for the overall frequency\nof the individual words,",
    "start": "51570",
    "end": "55748"
  },
  {
    "text": "then it's actually A\nand B that are united.",
    "start": "55748",
    "end": "57540"
  },
  {
    "text": "Because they both have a\nbias in some sense for the dy",
    "start": "57540",
    "end": "60510"
  },
  {
    "text": "dimension.",
    "start": "60510",
    "end": "61920"
  },
  {
    "text": "Whereas by comparison, C has\na bias for the x dimension,",
    "start": "61920",
    "end": "65549"
  },
  {
    "text": "again, thinking proportionally.",
    "start": "65550",
    "end": "68020"
  },
  {
    "text": "Both of those are perspectives\nthat we might want to capture",
    "start": "68020",
    "end": "70869"
  },
  {
    "text": "and different\nnotions of distance",
    "start": "70870",
    "end": "72820"
  },
  {
    "text": "will key into one or\nthe other of them.",
    "start": "72820",
    "end": "76600"
  },
  {
    "text": "One more preliminary, I\nthink it's very intuitive",
    "start": "76600",
    "end": "79360"
  },
  {
    "text": "to depict these vector spaces.",
    "start": "79360",
    "end": "81102"
  },
  {
    "text": "And in only two dimensions,\nthat's pretty easy.",
    "start": "81102",
    "end": "83060"
  },
  {
    "text": "You can imagine that this is the\ndx dimension along the x-axis.",
    "start": "83060",
    "end": "86680"
  },
  {
    "text": "And this is the dy\ndimension along the y-axis.",
    "start": "86680",
    "end": "89690"
  },
  {
    "text": "And then I have placed\nthese individual points",
    "start": "89690",
    "end": "91750"
  },
  {
    "text": "in that plane.",
    "start": "91750",
    "end": "92920"
  },
  {
    "text": "And then you can see graphically\nthat B and C are pretty close",
    "start": "92920",
    "end": "95650"
  },
  {
    "text": "together.",
    "start": "95650",
    "end": "96430"
  },
  {
    "text": "And A is kind of\nlonely down here",
    "start": "96430",
    "end": "98260"
  },
  {
    "text": "in the corner, the\ninfrequent one.",
    "start": "98260",
    "end": "100330"
  },
  {
    "start": "100330",
    "end": "102870"
  },
  {
    "start": "102000",
    "end": "156000"
  },
  {
    "text": "Let's start with\nEuclidean distance,",
    "start": "102870",
    "end": "104460"
  },
  {
    "text": "very common notion of\ndistance in these spaces",
    "start": "104460",
    "end": "106770"
  },
  {
    "text": "and quite intuitive.",
    "start": "106770",
    "end": "108490"
  },
  {
    "text": "We can measure the Euclidean\ndistance between vectors u",
    "start": "108490",
    "end": "111299"
  },
  {
    "text": "and v if they share the same\ndimension n by just calculating",
    "start": "111300",
    "end": "115230"
  },
  {
    "text": "the sum of the squared\nelement wide differences,",
    "start": "115230",
    "end": "118770"
  },
  {
    "text": "absolute differences, and then\ntaking the square root of that.",
    "start": "118770",
    "end": "122880"
  },
  {
    "text": "That's the math here.",
    "start": "122880",
    "end": "123939"
  },
  {
    "text": "Let's look at that in\nterms of this space.",
    "start": "123940",
    "end": "125690"
  },
  {
    "text": "So here we have our vector\nspace depicted graphically,",
    "start": "125690",
    "end": "128970"
  },
  {
    "text": "A, B, and C, and Euclidean\ndistances measuring",
    "start": "128970",
    "end": "131700"
  },
  {
    "text": "the length of these lines.",
    "start": "131700",
    "end": "133379"
  },
  {
    "text": "I've annotated with\nthe full calculations.",
    "start": "133380",
    "end": "135515"
  },
  {
    "text": "But the intuition\nis just that we",
    "start": "135515",
    "end": "136890"
  },
  {
    "text": "are measuring the length\nof these lines, the most",
    "start": "136890",
    "end": "139170"
  },
  {
    "text": "direct path between these points\nin our high dimensional space.",
    "start": "139170",
    "end": "143673"
  },
  {
    "text": "And you can see that Euclidean\ndistance is capturing",
    "start": "143673",
    "end": "145840"
  },
  {
    "text": "the first perspective that we\ntook on the vector space, which",
    "start": "145840",
    "end": "149650"
  },
  {
    "text": "unites the frequent items B and\nC as against the infrequent one",
    "start": "149650",
    "end": "153909"
  },
  {
    "text": "A. As a stepping stone\ntoward cosine distance, which",
    "start": "153910",
    "end": "159460"
  },
  {
    "start": "156000",
    "end": "234000"
  },
  {
    "text": "will behave quite\ndifferently, let's",
    "start": "159460",
    "end": "160960"
  },
  {
    "text": "talk about length normalization.",
    "start": "160960",
    "end": "163180"
  },
  {
    "text": "Given the vector u of\ndimension n, the L2 length of u",
    "start": "163180",
    "end": "167560"
  },
  {
    "text": "is the sum of the squared\nvalues in that matrix.",
    "start": "167560",
    "end": "171030"
  },
  {
    "text": "And then we take\nthe square root.",
    "start": "171030",
    "end": "172780"
  },
  {
    "text": "That's our normalization\nquantity there.",
    "start": "172780",
    "end": "175270"
  },
  {
    "text": "And then the actual\nnormalization",
    "start": "175270",
    "end": "177190"
  },
  {
    "text": "of that original\nvector u involves",
    "start": "177190",
    "end": "179470"
  },
  {
    "text": "taking each one of its\nelements and dividing it",
    "start": "179470",
    "end": "182500"
  },
  {
    "text": "by that fixed\nquantity, the L2 ranks.",
    "start": "182500",
    "end": "185880"
  },
  {
    "text": "Let's look at what happens\nto our little illustrative",
    "start": "185880",
    "end": "188280"
  },
  {
    "text": "example.",
    "start": "188280",
    "end": "188819"
  },
  {
    "text": "On the left here, I have\nthe original count matrix.",
    "start": "188820",
    "end": "191980"
  },
  {
    "text": "And in this column here,\nI've given the L2 length",
    "start": "191980",
    "end": "194400"
  },
  {
    "text": "as a quantity.",
    "start": "194400",
    "end": "195870"
  },
  {
    "text": "And then when we\ntake in that quantity",
    "start": "195870",
    "end": "197610"
  },
  {
    "text": "and divide each one of\nthe values in that vector",
    "start": "197610",
    "end": "199980"
  },
  {
    "text": "to get its L2 norm, you can\nsee that we've done something",
    "start": "199980",
    "end": "203220"
  },
  {
    "text": "significant to the space.",
    "start": "203220",
    "end": "204510"
  },
  {
    "text": "So they're all kind of united\non the same scale here.",
    "start": "204510",
    "end": "207780"
  },
  {
    "text": "And A and B are\nnow close together.",
    "start": "207780",
    "end": "210310"
  },
  {
    "text": "Whereas B and C are\ncomparatively far apart.",
    "start": "210310",
    "end": "212800"
  },
  {
    "text": "So that is capturing\nthe second perspective",
    "start": "212800",
    "end": "215100"
  },
  {
    "text": "that we took on the matrix,\nwhere A and B have something",
    "start": "215100",
    "end": "217560"
  },
  {
    "text": "in common as against C.\nAnd that has come entirely",
    "start": "217560",
    "end": "221099"
  },
  {
    "text": "from the normalization step.",
    "start": "221100",
    "end": "222810"
  },
  {
    "text": "And if we measured\nEuclidean distance",
    "start": "222810",
    "end": "224580"
  },
  {
    "text": "in this space, just\nthe length of the lines",
    "start": "224580",
    "end": "226890"
  },
  {
    "text": "between these points,\nwe would again",
    "start": "226890",
    "end": "228720"
  },
  {
    "text": "be capturing that\nA and B are alike",
    "start": "228720",
    "end": "230630"
  },
  {
    "text": "and B and C are\ncomparatively different.",
    "start": "230630",
    "end": "234630"
  },
  {
    "start": "234000",
    "end": "360000"
  },
  {
    "text": "Cosine kind of does\nthat all in one step.",
    "start": "234630",
    "end": "237370"
  },
  {
    "text": "So the cosine distance or\napproximately the distance,",
    "start": "237370",
    "end": "240480"
  },
  {
    "text": "as you'll see,\nbetween two vectors",
    "start": "240480",
    "end": "242040"
  },
  {
    "text": "u and v of shared dimension n.",
    "start": "242040",
    "end": "244799"
  },
  {
    "text": "This calculation has two parts.",
    "start": "244800",
    "end": "246450"
  },
  {
    "text": "This is the similarity\ncalculation, cosine similarity.",
    "start": "246450",
    "end": "249360"
  },
  {
    "text": "And it is the dot\nproduct of the two",
    "start": "249360",
    "end": "251370"
  },
  {
    "text": "vectors divided by the\nproduct of their L2 lengths.",
    "start": "251370",
    "end": "255480"
  },
  {
    "text": "And then to get something like\nthe distance, we just take 1",
    "start": "255480",
    "end": "258148"
  },
  {
    "text": "and subtract out\nthat similarity.",
    "start": "258149",
    "end": "261060"
  },
  {
    "text": "Again, let's ground\nthis in our example.",
    "start": "261060",
    "end": "262950"
  },
  {
    "text": "Here we have the original\ncount vector space model.",
    "start": "262950",
    "end": "266880"
  },
  {
    "text": "And what we do with\ncosine distance",
    "start": "266880",
    "end": "269040"
  },
  {
    "text": "is essentially measure the\nangles between these lines",
    "start": "269040",
    "end": "273240"
  },
  {
    "text": "that I've drawn from\nthis origin point.",
    "start": "273240",
    "end": "275889"
  },
  {
    "text": "And so you can see that cosine\ndistance is capturing the fact",
    "start": "275890",
    "end": "279180"
  },
  {
    "text": "that A and and B\nare close together",
    "start": "279180",
    "end": "281340"
  },
  {
    "text": "as measured by this angle.",
    "start": "281340",
    "end": "282840"
  },
  {
    "text": "Whereas B and C are\ncomparatively far apart.",
    "start": "282840",
    "end": "286090"
  },
  {
    "text": "So again with cosine,\nwe're abstracting away",
    "start": "286090",
    "end": "288600"
  },
  {
    "text": "from frequency\ninformation and keying",
    "start": "288600",
    "end": "291180"
  },
  {
    "text": "into that abstract\nnotion of similarity that",
    "start": "291180",
    "end": "293520"
  },
  {
    "text": "connects A and B as against C.",
    "start": "293520",
    "end": "298655"
  },
  {
    "text": "Another perspective\nthat you could take",
    "start": "298655",
    "end": "300280"
  },
  {
    "text": "is just observe that\nif we first normalize",
    "start": "300280",
    "end": "304840"
  },
  {
    "text": "the vectors via the\nL2 norm and then apply",
    "start": "304840",
    "end": "309110"
  },
  {
    "text": "the cosine calculation,\nwe changed the space",
    "start": "309110",
    "end": "311949"
  },
  {
    "text": "as I showed you before.",
    "start": "311950",
    "end": "313070"
  },
  {
    "text": "So they're all up here\nkind of on the units here.",
    "start": "313070",
    "end": "315890"
  },
  {
    "text": "And notice that the actual\nvalues that we get out",
    "start": "315890",
    "end": "318850"
  },
  {
    "text": "are the same whether or not\nwe did that L2 norming step.",
    "start": "318850",
    "end": "322630"
  },
  {
    "text": "And that is because\ncosine is building",
    "start": "322630",
    "end": "324970"
  },
  {
    "text": "the effects of L2\nnorming directly",
    "start": "324970",
    "end": "327400"
  },
  {
    "text": "into this normalization\nhere in the denominator.",
    "start": "327400",
    "end": "330525"
  },
  {
    "start": "330525",
    "end": "333418"
  },
  {
    "text": "There are a few other methods\nthat we could think about",
    "start": "333418",
    "end": "335710"
  },
  {
    "text": "or classes of methods.",
    "start": "335710",
    "end": "336729"
  },
  {
    "text": "I think we don't need to get\ndistracted by the details.",
    "start": "336730",
    "end": "339040"
  },
  {
    "text": "But I thought I\nwould mention them",
    "start": "339040",
    "end": "340457"
  },
  {
    "text": "in case they come up in\nyour reading or research.",
    "start": "340457",
    "end": "342550"
  },
  {
    "text": "The first class are what I\ncalled matching based methods.",
    "start": "342550",
    "end": "345580"
  },
  {
    "text": "They're all kind of based on\nthis matching coefficient.",
    "start": "345580",
    "end": "348639"
  },
  {
    "text": "And then Jaccard,\nDice, and Overlap",
    "start": "348640",
    "end": "350530"
  },
  {
    "text": "are terms that you might\nsee in the literature.",
    "start": "350530",
    "end": "352510"
  },
  {
    "text": "These are often defined\nonly for binary vectors.",
    "start": "352510",
    "end": "354970"
  },
  {
    "text": "Here, I've given\ntheir generalizations",
    "start": "354970",
    "end": "356890"
  },
  {
    "text": "to the real valued vectors\nthat we're talking about.",
    "start": "356890",
    "end": "360155"
  },
  {
    "start": "360000",
    "end": "401000"
  },
  {
    "text": "And the other class of methods\nthat you might see come up",
    "start": "360155",
    "end": "362530"
  },
  {
    "text": "are probabilistic\nmethods which tend",
    "start": "362530",
    "end": "364330"
  },
  {
    "text": "to be grounded in this\nnotion of KL divergence.",
    "start": "364330",
    "end": "367539"
  },
  {
    "text": "KL divergence is\nessentially a way",
    "start": "367540",
    "end": "369490"
  },
  {
    "text": "of measuring the distance\nbetween two probability",
    "start": "369490",
    "end": "372250"
  },
  {
    "text": "distributions.",
    "start": "372250",
    "end": "375160"
  },
  {
    "text": "To be more precise, from\na reference distribution p",
    "start": "375160",
    "end": "378640"
  },
  {
    "text": "to some other probability\ndistribution q.",
    "start": "378640",
    "end": "382660"
  },
  {
    "text": "And it has symmetric\nnotions, symmetric KL,",
    "start": "382660",
    "end": "385630"
  },
  {
    "text": "and Jensen-Shannon\ndistance, which",
    "start": "385630",
    "end": "387310"
  },
  {
    "text": "is another symmetric notion\nthat's based in KL divergence.",
    "start": "387310",
    "end": "390763"
  },
  {
    "text": "Again, these are probably\nappropriate measures",
    "start": "390763",
    "end": "392680"
  },
  {
    "text": "to choose if the quantities\nthat you're thinking of",
    "start": "392680",
    "end": "395770"
  },
  {
    "text": "are appropriately thought\nof as probability values.",
    "start": "395770",
    "end": "398965"
  },
  {
    "start": "398965",
    "end": "401860"
  },
  {
    "start": "401000",
    "end": "558000"
  },
  {
    "text": "Now I've alluded to the fact\nthat the cosine distance",
    "start": "401860",
    "end": "404439"
  },
  {
    "text": "measure that I gave you before\nis not quite what's called",
    "start": "404440",
    "end": "407300"
  },
  {
    "text": "the proper distance metric.",
    "start": "407300",
    "end": "408849"
  },
  {
    "text": "Let me expand on\nthat a little bit.",
    "start": "408850",
    "end": "410980"
  },
  {
    "text": "To qualify as a proper\ndistance metric,",
    "start": "410980",
    "end": "413650"
  },
  {
    "text": "a vector comparison method\nhas to have three properties.",
    "start": "413650",
    "end": "416800"
  },
  {
    "text": "It needs to be symmetric.",
    "start": "416800",
    "end": "418280"
  },
  {
    "text": "That is, it needs to give\nthe same value for xy",
    "start": "418280",
    "end": "420850"
  },
  {
    "text": "as it does to yx.",
    "start": "420850",
    "end": "422455"
  },
  {
    "text": "KL divergence actually\nfails that first rule.",
    "start": "422455",
    "end": "426639"
  },
  {
    "text": "It needs to assign 0\nto identical vectors.",
    "start": "426640",
    "end": "429790"
  },
  {
    "text": "And crucially, it\nneeds to satisfy",
    "start": "429790",
    "end": "431770"
  },
  {
    "text": "what's called the\ntriangle inequality, which",
    "start": "431770",
    "end": "434030"
  },
  {
    "text": "says that the distance\nbetween x and z is less than",
    "start": "434030",
    "end": "438070"
  },
  {
    "text": "or equal to the distance\nbetween x and y and then y to z.",
    "start": "438070",
    "end": "443610"
  },
  {
    "text": "Cosine distance, as I\nshowed it to you before,",
    "start": "443610",
    "end": "446189"
  },
  {
    "text": "fails to satisfy the\ntriangle inequality.",
    "start": "446190",
    "end": "448860"
  },
  {
    "text": "And this is just\na simple example",
    "start": "448860",
    "end": "450580"
  },
  {
    "text": "that makes it intuitive.",
    "start": "450580",
    "end": "451580"
  },
  {
    "text": "It just happens that\nthis distance here",
    "start": "451580",
    "end": "454050"
  },
  {
    "text": "is actually greater\nthan these two values",
    "start": "454050",
    "end": "456509"
  },
  {
    "text": "here, which is a failure of\nthe statement of the triangle",
    "start": "456510",
    "end": "459690"
  },
  {
    "text": "inequality.",
    "start": "459690",
    "end": "461110"
  },
  {
    "text": "Now this is relatively\neasily corrected.",
    "start": "461110",
    "end": "463560"
  },
  {
    "text": "But this is also kind\nof a useful framework.",
    "start": "463560",
    "end": "465570"
  },
  {
    "text": "Of all the different\nchoices that we could make,",
    "start": "465570",
    "end": "468510"
  },
  {
    "text": "of all the options\nfor vector comparison,",
    "start": "468510",
    "end": "470850"
  },
  {
    "text": "suppose we decided to\nfavor the ones that counted",
    "start": "470850",
    "end": "473250"
  },
  {
    "text": "as true distance metrics.",
    "start": "473250",
    "end": "475470"
  },
  {
    "text": "Then that would at least push\nus to favor Euclidean distance,",
    "start": "475470",
    "end": "479340"
  },
  {
    "text": "Jaccard for binary vectors only,\nand Jensen-Shannon distance",
    "start": "479340",
    "end": "483240"
  },
  {
    "text": "if we were talking about\nprobabilistic spaces.",
    "start": "483240",
    "end": "485819"
  },
  {
    "text": "And we would further amend the\ndefinition of cosine distance",
    "start": "485820",
    "end": "488820"
  },
  {
    "text": "to the more careful one\nthat I've given here,",
    "start": "488820",
    "end": "491530"
  },
  {
    "text": "which satisfies the\ntriangle inequality as well",
    "start": "491530",
    "end": "494490"
  },
  {
    "text": "as the other two criteria.",
    "start": "494490",
    "end": "496802"
  },
  {
    "text": "And by this kind of way\nof dividing the world,",
    "start": "496803",
    "end": "498720"
  },
  {
    "text": "we would also reject matching\nJaccard, Dice, Overlap, KL",
    "start": "498720",
    "end": "504460"
  },
  {
    "text": "divergence, and\nsymmetrical KL divergence",
    "start": "504460",
    "end": "506580"
  },
  {
    "text": "as ones that fail to be\nproper distance metrics.",
    "start": "506580",
    "end": "509513"
  },
  {
    "text": "And so that might be a\nuseful framework for thinking",
    "start": "509513",
    "end": "511680"
  },
  {
    "text": "about choices in this space.",
    "start": "511680",
    "end": "513870"
  },
  {
    "text": "One other point in\nrelation to this.",
    "start": "513870",
    "end": "515955"
  },
  {
    "text": "This is obviously a more\ninvolved calculation",
    "start": "515955",
    "end": "518309"
  },
  {
    "text": "than the one that\nI gave you before.",
    "start": "518309",
    "end": "520260"
  },
  {
    "text": "And in truth, it is probably\nnot worth the effort.",
    "start": "520260",
    "end": "522940"
  },
  {
    "text": "Here's an example of\njust a bunch of vectors",
    "start": "522940",
    "end": "525510"
  },
  {
    "text": "that I sampled from one of\nour vector space models.",
    "start": "525510",
    "end": "528120"
  },
  {
    "text": "And I've compared the\nimproper cosine distance",
    "start": "528120",
    "end": "530580"
  },
  {
    "text": "that I showed you\nbefore on the x-axis",
    "start": "530580",
    "end": "532980"
  },
  {
    "text": "with the proper cosine distance\nmetric that I just showed you.",
    "start": "532980",
    "end": "536820"
  },
  {
    "text": "And the correlation between\nthe two is almost perfect.",
    "start": "536820",
    "end": "539920"
  },
  {
    "text": "So there is essentially\nno difference",
    "start": "539920",
    "end": "541860"
  },
  {
    "text": "between these two different\nways of measuring cosine.",
    "start": "541860",
    "end": "545160"
  },
  {
    "text": "And I think that\nthey are probably",
    "start": "545160",
    "end": "546810"
  },
  {
    "text": "essentially identical up\nto ranking, which is often",
    "start": "546810",
    "end": "549540"
  },
  {
    "text": "the quantity that\nwe care about when",
    "start": "549540",
    "end": "551070"
  },
  {
    "text": "we're doing these comparisons.",
    "start": "551070",
    "end": "552820"
  },
  {
    "text": "So probably stick with the\nsimpler and less involved",
    "start": "552820",
    "end": "555000"
  },
  {
    "text": "calculation would be my advice.",
    "start": "555000",
    "end": "558930"
  },
  {
    "start": "558000",
    "end": "639000"
  },
  {
    "text": "Let's close with\nsome generalizations",
    "start": "558930",
    "end": "560520"
  },
  {
    "text": "and relationships first.",
    "start": "560520",
    "end": "561900"
  },
  {
    "text": "Euclidean, as well as Jaccard\nand Dice with raw count vectors",
    "start": "561900",
    "end": "565530"
  },
  {
    "text": "will tend to favor raw frequency\nover other distributional",
    "start": "565530",
    "end": "568590"
  },
  {
    "text": "patterns, like that more\nabstract one that I showed you",
    "start": "568590",
    "end": "571800"
  },
  {
    "text": "with our illustrative example.",
    "start": "571800",
    "end": "574790"
  },
  {
    "text": "Euclidean with L2\nnorm vectors is",
    "start": "574790",
    "end": "577040"
  },
  {
    "text": "equivalent to cosine when it\ncomes to ranking, which is just",
    "start": "577040",
    "end": "580519"
  },
  {
    "text": "to say that if you\nwant to use Euclidean",
    "start": "580520",
    "end": "582290"
  },
  {
    "text": "and you first L2\nnorm your vectors,",
    "start": "582290",
    "end": "584227"
  },
  {
    "text": "you're probably just doing\nsomething that might as well",
    "start": "584228",
    "end": "586520"
  },
  {
    "text": "just be the cosine calculation.",
    "start": "586520",
    "end": "589400"
  },
  {
    "text": "Jaccard and Dice are equivalent\nwith regard to ranking.",
    "start": "589400",
    "end": "591830"
  },
  {
    "text": "That's something\nto keep in mind.",
    "start": "591830",
    "end": "593832"
  },
  {
    "text": "And then this is maybe\na more fundamental point",
    "start": "593832",
    "end": "595790"
  },
  {
    "text": "that you'll see recurring\nthroughout this unit.",
    "start": "595790",
    "end": "598009"
  },
  {
    "text": "Both L2 norming and also\na related calculation",
    "start": "598010",
    "end": "600740"
  },
  {
    "text": "which will just create\nprobability distributions out",
    "start": "600740",
    "end": "602990"
  },
  {
    "text": "of the rows.",
    "start": "602990",
    "end": "604100"
  },
  {
    "text": "They can be useful\nsteps, as we've seen.",
    "start": "604100",
    "end": "606709"
  },
  {
    "text": "But they can obscure differences\nin the amount or strength",
    "start": "606710",
    "end": "609650"
  },
  {
    "text": "of evidence that you have,\nwhich can in turn have",
    "start": "609650",
    "end": "611780"
  },
  {
    "text": "an effect on the\nreliability of, for example,",
    "start": "611780",
    "end": "614360"
  },
  {
    "text": "cosine, non-Euclidean,\nor KL divergence.",
    "start": "614360",
    "end": "617377"
  },
  {
    "text": "Right, these shortcomings might\nbe addressed through weighting",
    "start": "617377",
    "end": "619960"
  },
  {
    "text": "schemes though.",
    "start": "619960",
    "end": "620900"
  },
  {
    "text": "But here's the bottom line.",
    "start": "620900",
    "end": "622100"
  },
  {
    "text": "There is valuable\ninformation in raw frequency.",
    "start": "622100",
    "end": "625339"
  },
  {
    "text": "If we abstract away from\nit, some other information",
    "start": "625340",
    "end": "627947"
  },
  {
    "text": "might come to the surface.",
    "start": "627947",
    "end": "629030"
  },
  {
    "text": "But we also might lose that\nimportant frequency information",
    "start": "629030",
    "end": "632120"
  },
  {
    "text": "in distorting the\nspace in that way.",
    "start": "632120",
    "end": "634070"
  },
  {
    "text": "And it can be difficult\nto balance these competing",
    "start": "634070",
    "end": "636470"
  },
  {
    "text": "pressures.",
    "start": "636470",
    "end": "639250"
  },
  {
    "start": "639000",
    "end": "704000"
  },
  {
    "text": "Finally, I'll just close\nwith some code snippets.",
    "start": "639250",
    "end": "641700"
  },
  {
    "text": "Our course repository has\nlots of hand utilities",
    "start": "641700",
    "end": "644590"
  },
  {
    "text": "for doing these distance\ncalculations and also",
    "start": "644590",
    "end": "647830"
  },
  {
    "text": "length norming your\nvectors and so forth.",
    "start": "647830",
    "end": "650140"
  },
  {
    "text": "And it also has\nthis function called",
    "start": "650140",
    "end": "652270"
  },
  {
    "text": "Neighbors in the VSM module.",
    "start": "652270",
    "end": "654610"
  },
  {
    "text": "It allows you to\npick a target word",
    "start": "654610",
    "end": "656680"
  },
  {
    "text": "and supply a vector space model.",
    "start": "656680",
    "end": "659140"
  },
  {
    "text": "And then it will give\nyou a full ranking",
    "start": "659140",
    "end": "661540"
  },
  {
    "text": "of the entire vocabulary in\nthat vector space with respect",
    "start": "661540",
    "end": "664480"
  },
  {
    "text": "to your target work, starting\nwith the ones that are closest.",
    "start": "664480",
    "end": "668139"
  },
  {
    "text": "So here the results for\n\"bad\" using cosine distance",
    "start": "668140",
    "end": "671560"
  },
  {
    "text": "in cell 12 and Jaccard\ndistance in cell 13.",
    "start": "671560",
    "end": "675390"
  },
  {
    "text": "And I would just like to say\nthat these neighbors don't look",
    "start": "675390",
    "end": "677890"
  },
  {
    "text": "especially intuitive to me.",
    "start": "677890",
    "end": "679570"
  },
  {
    "text": "It does not look\nlike this analysis",
    "start": "679570",
    "end": "681700"
  },
  {
    "text": "is revealing really interesting\nsemantic information.",
    "start": "681700",
    "end": "685420"
  },
  {
    "text": "But don't worry, we're\ngoing to correct this.",
    "start": "685420",
    "end": "687700"
  },
  {
    "text": "We're going to start\nto massage and stretch",
    "start": "687700",
    "end": "689890"
  },
  {
    "text": "and bend our vector\nspace models.",
    "start": "689890",
    "end": "691870"
  },
  {
    "text": "And we will see much better\nresults for these neighbor",
    "start": "691870",
    "end": "694720"
  },
  {
    "text": "functions and everything else\nas we go through that material.",
    "start": "694720",
    "end": "699360"
  },
  {
    "start": "699360",
    "end": "704000"
  }
]