[
  {
    "text": "today,\nwe're going to talk about self-supervised learning. This is a lecture that\ndoesn't have a lot of math.",
    "start": "5069",
    "end": "12429"
  },
  {
    "text": "But it's going to be all\nabout very recent works like probably in the last\nthree or four years the most.",
    "start": "12429",
    "end": "18410"
  },
  {
    "text": "And these are\npretty interesting, intriguing kind of concepts,\nbut nothing very complex.",
    "start": "18410",
    "end": "25830"
  },
  {
    "text": "Everything is kind of simple. Basically, I think, about\nprobably like 2013, 2014,",
    "start": "25830",
    "end": "32630"
  },
  {
    "text": "I think deep learning\nneural networks, this ideas start to take off\nand we have this revolution",
    "start": "32630",
    "end": "39879"
  },
  {
    "text": "of AI in some sense. We have a lot of amazing results\nin the last about 10 years",
    "start": "39880",
    "end": "45079"
  },
  {
    "text": "after deep learning took off. But in the last\ntwo or three years, I think we see an emergent new\nparadigm of AI, which is still",
    "start": "45079",
    "end": "55149"
  },
  {
    "text": "based on deep learning\nneural networks, but I think, in some sense,\nthat kind of the paradigm",
    "start": "55149",
    "end": "60570"
  },
  {
    "text": "shift a little bit towards\nthis kind of large scale unsupervised learning or\nself-supervised learning.",
    "start": "60570",
    "end": "67420"
  },
  {
    "text": "So that's kind of what we're\ngoing to talk about today. So I guess last\nof your lectures, I think Chris talked about\nunsupervised learning.",
    "start": "67420",
    "end": "74360"
  },
  {
    "text": "So those are mostly ideas,\nmore classical ideas that was developed like\nbefore deep learning took off.",
    "start": "74360",
    "end": "80640"
  },
  {
    "text": "And today, I think\nwe're going to talk about the kind of new type of\nunsupervised learning, which is not that different from the\nold ones with neural networks.",
    "start": "80640",
    "end": "90759"
  },
  {
    "text": "With some more\ntechnical differences and and also some differences\nin the conceptual way.",
    "start": "90759",
    "end": "99100"
  },
  {
    "text": "So I think in some sense,\nthis is called like-- actually, we also\nwrite a white paper about this, a bunch\nof Stanford people.",
    "start": "99100",
    "end": "106200"
  },
  {
    "text": "Actually, there are hundreds\npeople, more than 100 people in the paper, mostly Stanford\nresearchers, Stanford faculty,",
    "start": "106200",
    "end": "114810"
  },
  {
    "text": "and students. We wrote a white paper on this. We call it foundation model. So in some sense,\nit's just a name.",
    "start": "114810",
    "end": "121810"
  },
  {
    "text": "It's a name for kind of\nthis large set of ideas that involves pre-training\nor unsupervised data",
    "start": "121810",
    "end": "128289"
  },
  {
    "text": "and then use it for a\nbunch of downstream tasks. I'm not sure whether you\nknow any of these passwords,",
    "start": "128289",
    "end": "133640"
  },
  {
    "text": "but that's what I'm here for. I'm going to define\nsome of these words and tell you what's the\npipeline and the paradigm is.",
    "start": "133640",
    "end": "140989"
  },
  {
    "text": "So basically, kind of the\nmain thing for deep learning",
    "start": "140989",
    "end": "146150"
  },
  {
    "text": "is that we use neural\nnetworks to state larger data than we\nused to do before",
    "start": "146150",
    "end": "151739"
  },
  {
    "text": "like 10 years, 15 years ago. And typically, at the beginning\nwhen you use deep learning, you use it for supervised\nlearning, where",
    "start": "151739",
    "end": "158580"
  },
  {
    "text": "you have a lot of data, maybe\na million data ImageNet, and you have labels for them. And then you pick neural\nnetwork to predict the label.",
    "start": "158580",
    "end": "166270"
  },
  {
    "text": "And it turns out that even\nthough neural network is pretty big, your data is also somewhat\nbig, you can see good results,",
    "start": "166270",
    "end": "172930"
  },
  {
    "text": "and sometimes you can even\nmake it work on small data. But all of this requires some\namount of labelled data, right?",
    "start": "172930",
    "end": "181110"
  },
  {
    "text": "And then, recently,\npeople figured out maybe we should use unlabeled\ndata as well, right? So when you use unlabeled\ndata, then you have much more.",
    "start": "181110",
    "end": "189010"
  },
  {
    "text": "For example, for\ntext data, I think there is this data set which\nhas 40 trillion words in it.",
    "start": "189010",
    "end": "195330"
  },
  {
    "text": "But if you have\nlabeled it as such, then you probably have a million\nlabeled documents and also",
    "start": "195330",
    "end": "201319"
  },
  {
    "text": "a label has specific meanings. Maybe you need multiple\nlabels for documents many times in many cases.",
    "start": "201319",
    "end": "208519"
  },
  {
    "text": "So if you say I can train\nwith unlabeled data, then you suddenly\nhave availability of a lot more data.",
    "start": "208519",
    "end": "215010"
  },
  {
    "text": "So I think that's the\nmain driving force. Now, we are training on\nunsupervised like unlabeled",
    "start": "215010",
    "end": "223640"
  },
  {
    "text": "data. So how do we train with\nunsupervised, unlabeled data? I think often people\nhave different words",
    "start": "223640",
    "end": "231829"
  },
  {
    "text": "for the same kind of concept. I think one of the way\nto call it is called",
    "start": "231829",
    "end": "238129"
  },
  {
    "text": "self-supervised learning. As you will see, you're\ngoing to supervise",
    "start": "238130",
    "end": "246550"
  },
  {
    "text": "your model with yourself,\nthe input itself.",
    "start": "246550",
    "end": "252260"
  },
  {
    "text": "So you don't have\nto use any label. And sometimes, as I\nsaid, in a white paper,",
    "start": "252260",
    "end": "257310"
  },
  {
    "text": "we call this family of models\ncalled foundation model. Foundation. I'll explain this word\na little bit more.",
    "start": "257310",
    "end": "267509"
  },
  {
    "text": "And some other bears worth\nincluding pretraining, I'll explain them as\nwell, and adaptation.",
    "start": "267509",
    "end": "278170"
  },
  {
    "text": "So I guess I would just start\nby explaining pre-training. So in some sense, the way\nthat people do this right now",
    "start": "278170",
    "end": "285520"
  },
  {
    "text": "is the following. So you have this so-called\npre-training stage where you pre-train on large scale.",
    "start": "285520",
    "end": "299830"
  },
  {
    "text": "Here, large scale really\nmeans very, very large scale",
    "start": "299830",
    "end": "308629"
  },
  {
    "text": "unlabeled data set. It's not always the\ncase that you have",
    "start": "308630",
    "end": "314069"
  },
  {
    "text": "to use a unlabeled data set. We will see actually,\nin some cases, you can use the labeled\ndata set for pre-training, but you really have to\nmake it large scale.",
    "start": "314069",
    "end": "320090"
  },
  {
    "text": "An unlabeled set is more common\nthan the labeled data set.",
    "start": "320090",
    "end": "328600"
  },
  {
    "text": "And also you approach\na large model. A very large model. And then the second step is\nthat you somehow adapt this.",
    "start": "328600",
    "end": "341789"
  },
  {
    "text": "So adapt this pre-trained\nmodel ",
    "start": "341789",
    "end": "352860"
  },
  {
    "text": "to some downstream tasks. Actually, often, you can adapt\nto many downstream tasks,",
    "start": "352860",
    "end": "359819"
  },
  {
    "text": "not only one. But the adaptation to the task\nis the same as adapting to one.",
    "start": "359819",
    "end": "365590"
  },
  {
    "text": "You just do it one by one. So you adapt to some\ndownstream tasks. And these downstream tasks\nare often labeled data set.",
    "start": "365590",
    "end": "376060"
  },
  {
    "text": "For downstream tasks, you\nhave different settings, I would describe in more\ndetail, but generally you",
    "start": "376060",
    "end": "383069"
  },
  {
    "text": "have a few examples in\nthe downstream task. For example, one\nexample could be",
    "start": "383069",
    "end": "389770"
  },
  {
    "text": "that you pre-train\non unlabeled text, right, that you can\ndownload from internet.",
    "start": "389770",
    "end": "395789"
  },
  {
    "text": "So you could have a trillion\nwords like documents, like a lot of documents\nwith a trillion words",
    "start": "395790",
    "end": "402370"
  },
  {
    "text": "downloaded from\ndata from internet and you pretrain your model\non this unlabeled data set.",
    "start": "402370",
    "end": "408699"
  },
  {
    "text": "How do you approach it? I'm going to tell you. And then after you\nget this model, this model is pre-trained\nwith no labels,",
    "start": "408699",
    "end": "415150"
  },
  {
    "text": "so it doesn't really\nsolve any particular task. And then you say\nI'm going to have a downstream task about text.",
    "start": "415150",
    "end": "421690"
  },
  {
    "text": "Maybe I care about\nsentiment analysis. Meaning you care about whether\nthis sentiment or document",
    "start": "421690",
    "end": "427139"
  },
  {
    "text": "or sentence is\npositive or not, right? Maybe for Amazon review\nwhere you care about whether the reviews\nis positive or not.",
    "start": "427139",
    "end": "433479"
  },
  {
    "text": "And then you say I'm going to\nhave a small number of examples for sentiment analysis.",
    "start": "433479",
    "end": "439000"
  },
  {
    "text": "I have a small number of\nexamples with documents and label the positive or\nnegative label pair, right?",
    "start": "439000",
    "end": "444789"
  },
  {
    "text": "So maybe 100 or 1,000 pairs\nof documents unlabeled.",
    "start": "444789",
    "end": "450580"
  },
  {
    "text": "And then that's called\ndownstream task, and then you want to adapt\nyour pretrained model, which is very general, generic,\nto the specific task using",
    "start": "450580",
    "end": "458919"
  },
  {
    "text": "some additional tuning. So that's the general idea.",
    "start": "458919",
    "end": "464470"
  },
  {
    "text": "But sometimes, of\ncourse, the distinction between these 2-step is that\nthis time, you may still",
    "start": "464470",
    "end": "471050"
  },
  {
    "text": "involve some training, some\ntraining in this adaptation step because you're going\nto see the new examples",
    "start": "471050",
    "end": "476110"
  },
  {
    "text": "on the downstream task,\nthe sentiment analysis tasks, right? And then you have to do\nsomething with those examples.",
    "start": "476110",
    "end": "483210"
  },
  {
    "text": "This is a training optimization\nstep adaptation step as well.",
    "start": "483210",
    "end": "489590"
  },
  {
    "text": "But the difference here\nis that this step often involves very large scale\ndata and this generic.",
    "start": "489590",
    "end": "495110"
  },
  {
    "text": "It's not about task\nspecific learning. Here, it's really\nabout task itself.",
    "start": "495110",
    "end": "500370"
  },
  {
    "text": "And oftentimes, you have\nmuch fewer data points than the pre-training step.",
    "start": "500370",
    "end": "506139"
  },
  {
    "text": "Sometimes you have you have 10,000, but\ngenerally not a lot. Sometimes you can even work\nwith even zero examples",
    "start": "506139",
    "end": "513740"
  },
  {
    "text": "in adaptation step. You can sometimes still\nadapt to the task. So kind of the intuition is\nthat the pre-training step",
    "start": "513740",
    "end": "521390"
  },
  {
    "text": "is about learning\nthe generic structure or the intrinsic structure\nof, for example, text.",
    "start": "521390",
    "end": "528670"
  },
  {
    "text": "If you do it for image, you\nare learning the intrinsic kind of structure of the images,\nmaybe the intrinsic features",
    "start": "528670",
    "end": "536160"
  },
  {
    "text": "about images. And then the adaptation step\nis more about the task itself.",
    "start": "536160",
    "end": "541290"
  },
  {
    "text": "For images, you can care about\nthe many different tasks, maybe you care\nabout recognition, maybe you care about\nclassification, or maybe you care about\ndifferent kind of labels.",
    "start": "541290",
    "end": "551019"
  },
  {
    "text": "You can have labels of\ndifferent granularities. A bunch of different\ntasks, right? So this step is\nmore about tasks.",
    "start": "551019",
    "end": "558740"
  },
  {
    "text": "So that's kind of the\ngeneral intuition. You push a lot of\ndata so that give you",
    "start": "558740",
    "end": "566290"
  },
  {
    "text": "the intrinsic structure or the\nbest intrinsic representations for this kind of data, and\nthat representations are",
    "start": "566290",
    "end": "572740"
  },
  {
    "text": "useful for downstream tasks. So that you don't have\nto use a lot of examples",
    "start": "572740",
    "end": "579430"
  },
  {
    "text": "in downstream tasks,\nright, because you already learned some interesting\nrepresentations that helps you for the downstream task.",
    "start": "579430",
    "end": "586850"
  },
  {
    "text": "All right. So basically, kind of\nthe one implication",
    "start": "586850",
    "end": "592399"
  },
  {
    "text": "here is that you expect\nthat this pipeline is going to do better than just\ntraining your model only",
    "start": "592399",
    "end": "601529"
  },
  {
    "text": "on the downstream data set. So that's the basic goal. The baseline is that you just\ndirectly train your model",
    "start": "601529",
    "end": "610339"
  },
  {
    "text": "on the task, downstream task. But generally, you don't\nhave enough data for that. But if you do this\nkind of transferring",
    "start": "610339",
    "end": "618529"
  },
  {
    "text": "from the unlabeled\ndata set to this one, then you may do better.",
    "start": "618529",
    "end": "625380"
  },
  {
    "text": "And we call this kind\nof pretraining model also foundation model. So here, I call it\npretraining model.",
    "start": "625380",
    "end": "630640"
  },
  {
    "text": "You can also call\nit foundation model. And the reason for\nthe word foundation, I think the implication is\nthat this pretrained model is",
    "start": "630640",
    "end": "641490"
  },
  {
    "text": "a general purpose model that\ncan do a lot of different things and that's the foundation\nand this is the adaptation.",
    "start": "641490",
    "end": "652829"
  },
  {
    "text": "I haven't talked\nabout waste yet.",
    "start": "652829",
    "end": "658720"
  },
  {
    "text": "Yes. But that's a good question,\nbut I'll talk about those.",
    "start": "658720",
    "end": "667709"
  },
  {
    "text": "OK. Any questions so far? I'm going to tell you-- OK, so this is a general idea.",
    "start": "667709",
    "end": "672910"
  },
  {
    "text": "I'm going to tell you some-- I'm going to start with\nsome simple notations and tell you how do people\ngenerally do this in kind",
    "start": "672910",
    "end": "681269"
  },
  {
    "text": "of a more mathematical way. OK, so pretraining. So I guess let's say you\nhave some data, something",
    "start": "681269",
    "end": "695101"
  },
  {
    "text": "like x1 up to xn. So you have n data\npoints and n be very big.",
    "start": "695101",
    "end": "702580"
  },
  {
    "text": "And there is no\nlabel here so this is unsupervised pre-training. Of course, sometimes you\ncan still have labels in it,",
    "start": "702580",
    "end": "709329"
  },
  {
    "text": "but let's say we focus on a\ncase where you have no labels and you have a model.",
    "start": "709329",
    "end": "717320"
  },
  {
    "text": "So let's specify the input\nand output of the model. So typically, you can think of\nthis as the model, let's say,",
    "start": "717320",
    "end": "723800"
  },
  {
    "text": "is called phi of theta. And sometimes you view this as\na feature extractor, a feature map, but it's a\nlearned feature map.",
    "start": "723800",
    "end": "731519"
  },
  {
    "text": "So this is something that\nmaps x to phi of theta. Phi theta of x, and let's say\nphi theta of x is some vector.",
    "start": "731519",
    "end": "742610"
  },
  {
    "text": "So I guess you can-- as we have kind of seen, we\nhave this kind of feature map,",
    "start": "742610",
    "end": "750459"
  },
  {
    "text": "often you call this\nrepresentation, features, there are many names for it.",
    "start": "750459",
    "end": "758990"
  },
  {
    "text": "Representation slash features.",
    "start": "758990",
    "end": "765029"
  },
  {
    "text": "Sometimes we also call\nit embeddings, especially if you talk to more\nmathematical people because in math, this\nword embedding is",
    "start": "765029",
    "end": "772569"
  },
  {
    "text": "used in a similar context. So this is a representation\nof the raw data, right?",
    "start": "772570",
    "end": "779730"
  },
  {
    "text": "The raw data could be text,\nimage, and this is a vector. So this is something we\nare familiar with, right?",
    "start": "779730",
    "end": "785899"
  },
  {
    "text": "In both neural networks,\nwe have viewed the last by one layer as\nthe representation.",
    "start": "785899",
    "end": "792160"
  },
  {
    "text": "In the feature lecture,\nthis is the feature map. Although in a feature,\nin the kernel, sorry,",
    "start": "792160",
    "end": "798300"
  },
  {
    "text": "in a kernel lecture,\nthis is the feature map. But in the kernel\nlecture, this feature map is given and designed.",
    "start": "798300",
    "end": "805610"
  },
  {
    "text": "But later, when we do\nthe neural networks, this is the learned feature map. Here, we are still\nlearning a feature map.",
    "start": "805610",
    "end": "810870"
  },
  {
    "text": "And we are still learning our\nfeature map, feature extractor, whatever you call this. So and then-- but here, you\nlearn this without labels.",
    "start": "810870",
    "end": "819910"
  },
  {
    "text": "So you say you have some\npretraining loss, maybe some pretraining loss. There are many forms of\npretraining loss as well.",
    "start": "819910",
    "end": "825990"
  },
  {
    "text": "Here, I'm just only\ngiving one form. Something like\nprobably nothing, maybe",
    "start": "825990",
    "end": "831800"
  },
  {
    "text": "like a sum of loss\non single example.",
    "start": "831800",
    "end": "839199"
  },
  {
    "text": "I'm going to define the\nloss for different cases. Sometimes this loss can also\ndepend on multiple examples,",
    "start": "839199",
    "end": "846139"
  },
  {
    "text": "sometimes they can\ndepend on labels, but something like this. You have a pretraining loss, OK? And then what you do is you\njust optimize this loss.",
    "start": "846139",
    "end": "857610"
  },
  {
    "text": "This Lpre theta. Let's say obtain some theta hat.",
    "start": "857610",
    "end": "870089"
  },
  {
    "text": "I'm going to call this\nmy pre-training model. Oh, maybe we can call it\nfoundation model or something",
    "start": "870089",
    "end": "878680"
  },
  {
    "text": "like that. OK? So nothing fancy so far,\nyet I haven't told you",
    "start": "878680",
    "end": "884850"
  },
  {
    "text": "what the training loss is. There are many, many different\nways of doing pretraining, I'm going to tell\nyou a few ways.",
    "start": "884850",
    "end": "891060"
  },
  {
    "text": "But so far, you have\ndefined some loss function, but the loss only depends\non the unlabeled data, and then you minimize loss\nand you get some model.",
    "start": "891060",
    "end": "898300"
  },
  {
    "text": "OK, so, and then, I'm going\nto the adaptation step, which",
    "start": "898300",
    "end": "905170"
  },
  {
    "text": "I'm going to be a little bit\nmore concrete because it's a little bit easier.",
    "start": "905170",
    "end": "912259"
  },
  {
    "text": "So in the adaptation step, let's\nfirst clarify what kind of data",
    "start": "912259",
    "end": "918800"
  },
  {
    "text": "you have before talking\nabout how do you do it. So what data do you have?",
    "start": "918800",
    "end": "924500"
  },
  {
    "text": "Often, it's a label\nthat asset, it's a labeled prediction task,\nthe downstream tasks.",
    "start": "924500",
    "end": "931230"
  },
  {
    "text": "Even though I think if\nyou look at enough papers, there are many\nother papers talking about different type of tasks.",
    "start": "931230",
    "end": "936840"
  },
  {
    "text": "But, here, let's say we have\nsome labeled some prediction",
    "start": "936840",
    "end": "942269"
  },
  {
    "text": "task and we have\nsome label data sets. So let's say we\nhave a few examples. Let's call it x task one, y\ntask one, so on and so forth.",
    "start": "942269",
    "end": "959240"
  },
  {
    "text": "x task nt, y task nt.",
    "start": "959240",
    "end": "964299"
  },
  {
    "text": "And this is the number\nof downstream examples",
    "start": "964299",
    "end": "969880"
  },
  {
    "text": "which is presumably\nmuch smaller than-- I'm going to use nt as the\nnumber of downstream task",
    "start": "969880",
    "end": "986279"
  },
  {
    "text": "examples. So on this one,\nit's supposed to be",
    "start": "986279",
    "end": "996170"
  },
  {
    "text": "much smaller than the\nnumber of unlabeled examples in a pretraining step.",
    "start": "996170",
    "end": "1001610"
  },
  {
    "text": "And there are actually a\nfew different settings. So one setting is that nt is 0.",
    "start": "1001610",
    "end": "1008230"
  },
  {
    "text": "So this is called\nzero shot learning.",
    "start": "1008230",
    "end": "1014300"
  },
  {
    "text": "So nt zero, it just means\nyou don't have anything. You don't have a data set. So it's called\nzero shot learning. We'll see how you do it.",
    "start": "1014300",
    "end": "1021259"
  },
  {
    "text": "Sometimes you can still do\nzero, like you can still do the task without even\nknowing anything about the task.",
    "start": "1021259",
    "end": "1028428"
  },
  {
    "text": "And when nt is\nrelatively small, I",
    "start": "1028429",
    "end": "1035928"
  },
  {
    "text": "think this is called few shot. And what do I mean by small?",
    "start": "1035929",
    "end": "1041159"
  },
  {
    "text": "I think in the\nliterature, maybe 5 is typically considered small.",
    "start": "1041160",
    "end": "1048159"
  },
  {
    "text": "not more like 50. If more than 50 examples,\nI think, of course, there's nobody really defined\nwhat exactly it means.",
    "start": "1048160",
    "end": "1054360"
  },
  {
    "text": "But I think if it's more\nthan 50 examples per class in the downstream\ntask, I don't think",
    "start": "1054360",
    "end": "1059400"
  },
  {
    "text": "you would call it few shot. Most likely 10. But, of course,\nthere are cases where",
    "start": "1059400",
    "end": "1065269"
  },
  {
    "text": "you can have more examples. I think I've seen\ncases where you have 10,000 examples\nor maybe even",
    "start": "1065270",
    "end": "1075169"
  },
  {
    "text": "But the n there,\ntypically, it could be a billion or even\na trillion sometimes.",
    "start": "1075169",
    "end": "1080799"
  },
  {
    "text": "A hundred billion.",
    "start": "1080799",
    "end": "1086010"
  },
  {
    "text": "This is the setup. I'm giving this information,\nand then how do I do it?",
    "start": "1086010",
    "end": "1092730"
  },
  {
    "text": "The first thing\nI'm going to do is I'm going to first\ndefine how do I adapt. I'm giving this model theta hat.",
    "start": "1092730",
    "end": "1099700"
  },
  {
    "text": "What's the model\nto predict a label? I need to have a model\nthat predicts a label. So here the theta hat predicts\na feature vector, but not",
    "start": "1099700",
    "end": "1109770"
  },
  {
    "text": "the label. The first thing is that\nI'm going to take the call. I'm going to have\nmaybe this one way",
    "start": "1109770",
    "end": "1119210"
  },
  {
    "text": "to do this, which is\ncalled linear probe,",
    "start": "1119210",
    "end": "1124740"
  },
  {
    "text": "often people also\ncall it linear hat. Something like this.",
    "start": "1124740",
    "end": "1132480"
  },
  {
    "text": "So kind of like the\nidea is you probably should be familiar with this.",
    "start": "1132480",
    "end": "1137990"
  },
  {
    "text": "It's just like you take this\nfeature phi theta hat x. And then you apply a linear\nclassifier on top of it.",
    "start": "1137990",
    "end": "1146250"
  },
  {
    "text": "You say I'm going to take an\ninner product of this feature with some linear\nclassifier W, and W",
    "start": "1146250",
    "end": "1153480"
  },
  {
    "text": "transpose x is my prediction\nmodel for the downstream task. Let's say suppose you\nhave a regression model,",
    "start": "1153480",
    "end": "1159580"
  },
  {
    "text": "then you just want this\nnumber to be some real number. And if you have\nclassification, then this is supposed to predict\nthe probability",
    "start": "1159580",
    "end": "1166320"
  },
  {
    "text": "of the label above the label. So it's just almost the same as\nwhat we do in neural networks",
    "start": "1166320",
    "end": "1172620"
  },
  {
    "text": "where you just have some feature\nand you apply some linear hat on this feature.",
    "start": "1172620",
    "end": "1179640"
  },
  {
    "text": "And then we will\ndo linear probe, what we do is that, we have\ndefined a prediction model.",
    "start": "1179640",
    "end": "1187340"
  },
  {
    "text": "And then you can\ndefine how to-- you can say how do I learn this W? Here, the theta\nhat here is fixed.",
    "start": "1187340",
    "end": "1194929"
  },
  {
    "text": "So I'm only learning W when\nI'm doing a linear probe. How do you learn W?",
    "start": "1194929",
    "end": "1199970"
  },
  {
    "text": "I'm going to just tarin W\nwith some loss function,",
    "start": "1199970",
    "end": "1205539"
  },
  {
    "text": "with some loss function.",
    "start": "1205539",
    "end": "1212419"
  },
  {
    "text": "On the downstream examples. So basically I just do\nsomething like maybe 1 over Nt",
    "start": "1212419",
    "end": "1217700"
  },
  {
    "text": "and minimize over W and\nI have a loss function, which is the sum over the\nloss of the downstream tasks.",
    "start": "1217700",
    "end": "1226780"
  },
  {
    "text": "Assuming you have downstream\ntasks or assuming you have downstream examples. If you don't have\ndownstream examples,",
    "start": "1226780",
    "end": "1232640"
  },
  {
    "text": "this doesn't apply anymore. I will tell you how\ndo we do it if we don't have downstream examples.",
    "start": "1232640",
    "end": "1237720"
  },
  {
    "text": "But suppose you have\ndownstream examples, then you just minimize the loss\nsomething like the loss called",
    "start": "1237720",
    "end": "1243120"
  },
  {
    "text": "l task and this loss\nfunction is something",
    "start": "1243120",
    "end": "1248240"
  },
  {
    "text": "that compares the label. And your prediction\nof your model,",
    "start": "1248240",
    "end": "1254910"
  },
  {
    "text": "the prediction of your model\nis something like this, right?",
    "start": "1254910",
    "end": "1271450"
  },
  {
    "text": "This task, for example,\ncould be just our L2 loss, mean square loss. This loss could be just\nthe mean square loss",
    "start": "1271450",
    "end": "1278010"
  },
  {
    "text": "or could be some cross entropy\nloss or some other loss that you care about.",
    "start": "1278010",
    "end": "1284010"
  },
  {
    "text": "And so when you do\nso-called linear probe, it means that you\nonly optimize W. So you are only\noptimizing W, which",
    "start": "1284010",
    "end": "1290630"
  },
  {
    "text": "is the vector of dimension m.",
    "start": "1290630",
    "end": "1296920"
  },
  {
    "text": "W is an Rm. So you are just optimizing\nthis m dimensional vector to make your model fit\nto the downstream task.",
    "start": "1296920",
    "end": "1306039"
  },
  {
    "text": "And this is the so-called\nadaptation step. Basically, the W is\nthe thing that you want to use to adapt\nyour model to a new task.",
    "start": "1306039",
    "end": "1313960"
  },
  {
    "text": "Questions? [INAUDIBLE] labels are\nyou know downstream--",
    "start": "1313960",
    "end": "1325000"
  },
  {
    "text": "[INAUDIBLE] So where the label\ncomes from, you will see we have a\nlabel in the set. Somebody gave you, right.",
    "start": "1325000",
    "end": "1330550"
  },
  {
    "text": "So you collect the data set as\nthe same as what we did before. But the difference is\nthat you may not have",
    "start": "1330550",
    "end": "1336650"
  },
  {
    "text": "to collect as many as before. Like if you just turn on this\ndata set just from scratch,",
    "start": "1336650",
    "end": "1343760"
  },
  {
    "text": "then you may need more\nthan what you do here.",
    "start": "1343760",
    "end": "1351210"
  },
  {
    "text": "OK. [INAUDIBLE]",
    "start": "1351210",
    "end": "1361139"
  },
  {
    "text": "So I think-- so I guess maybe\nthe question is, what transfer",
    "start": "1361140",
    "end": "1366460"
  },
  {
    "text": "learning comes into play here? So I think you\ncan, in some sense, call this also\ntransfer learning. So transfer learning\nused to be--",
    "start": "1366460",
    "end": "1373429"
  },
  {
    "text": "the transfer in\nits word occurs-- like this term was used\nway before people have done",
    "start": "1373429",
    "end": "1382490"
  },
  {
    "text": "any of the pre-training. So like, now like I think like\ntransfer learning probably",
    "start": "1382490",
    "end": "1389020"
  },
  {
    "text": "like people started like\nin early 2000 already, even maybe before that.",
    "start": "1389020",
    "end": "1394529"
  },
  {
    "text": "And at that time, transfer\nlearning means what? It means that if I use\npretraining language,",
    "start": "1394529",
    "end": "1399970"
  },
  {
    "text": "it means that you pretrain\nwith the labeled dataset. And then, you do\nsome adaptation.",
    "start": "1399970",
    "end": "1406080"
  },
  {
    "text": "So basically transfer learning\nmeans this in the new language. But now, these days,\nwhen we pretrain,",
    "start": "1406080",
    "end": "1412169"
  },
  {
    "text": "we pretrain on\nunlabeled dataset. And another--\nmaybe another thing is that it used to be the case\nthat with transfer learning,",
    "start": "1412170",
    "end": "1421679"
  },
  {
    "text": "the first thing\nthat you train on is also classification\ntask, which",
    "start": "1421679",
    "end": "1427450"
  },
  {
    "text": "is kind of similar to the\nfinal task you care about. So but here-- it's\nsometimes the reason",
    "start": "1427450",
    "end": "1434929"
  },
  {
    "text": "why people introduce new\nterms for this is that when you pretrain this\ntask could be nothing",
    "start": "1434929",
    "end": "1440549"
  },
  {
    "text": "like the downstream tasks. I haven't told you what\nexactly the task is, but at least you can\nimagine there's not",
    "start": "1440549",
    "end": "1448350"
  },
  {
    "text": "a lot of similarity. Because here, we don't\neven have labels. It has to be\nsomething different.",
    "start": "1448350",
    "end": "1454110"
  },
  {
    "text": "Yeah. But I think you can still\nsay this is transferring. It's not like there's no\nprecise boundary between this.",
    "start": "1454110",
    "end": "1463360"
  },
  {
    "text": "OK. So let me introduce another\nway to do adaptation.",
    "start": "1463360",
    "end": "1469670"
  },
  {
    "text": "So another very common\nway to do adaptation is that so-called finetuning.",
    "start": "1469670",
    "end": "1475450"
  },
  {
    "text": "And it's also pretty\neasy to understand. So basically, your\nmodel is the same.",
    "start": "1475450",
    "end": "1483860"
  },
  {
    "text": "But you just-- your\nprediction is some W transpose",
    "start": "1483860",
    "end": "1490160"
  },
  {
    "text": "phi theta of x. So here, I write theta\nbut not with the hat to indicate that I'm allowed\nto change this theta.",
    "start": "1490160",
    "end": "1497880"
  },
  {
    "text": "So here, theta doesn't have to\nbe exactly the pretrained model anymore. It could be something\nthat you can change.",
    "start": "1497880",
    "end": "1504330"
  },
  {
    "text": "So then, what you\ndo is you say I'm going to optimize both W and\ntheta on the downstream tasks.",
    "start": "1504330",
    "end": "1523049"
  },
  {
    "text": "But if I just say\nthis, then this just sounds like the standard\nsupervised learning, right. There's nothing different\nfrom supervised learning.",
    "start": "1523049",
    "end": "1529408"
  },
  {
    "text": "You are just training\nsome neural network where W transpose phi of\nx on the downstream task.",
    "start": "1529409",
    "end": "1534789"
  },
  {
    "text": "The difference from that\nis that you optimize, but you also initialize\nwith initialization",
    "start": "1534789",
    "end": "1543950"
  },
  {
    "text": "that theta is initialized\nto be theta hat.",
    "start": "1543950",
    "end": "1549179"
  },
  {
    "text": "Recall that theta hat\nis the pretrained model. So basically, you\njust train as if you",
    "start": "1549179",
    "end": "1554770"
  },
  {
    "text": "are doing supervised\nlearning, but theta is initialized to be theta hat. And W there is nothing you\ncan do with that network,",
    "start": "1554770",
    "end": "1560669"
  },
  {
    "text": "because you didn't\nknow W before. So W says something new, so\njust initialize W to be random.",
    "start": "1560669",
    "end": "1569179"
  },
  {
    "text": "So that's so-called finetuning. And you can optimize the\nsame loss or whatever loss you care about.",
    "start": "1569179",
    "end": "1574600"
  },
  {
    "text": "What's the question? [INAUDIBLE] Theta is the parameter\nfor this function phi.",
    "start": "1574600",
    "end": "1581929"
  },
  {
    "text": "Phi is a function\nparameterized by theta. So that's what-- [INAUDIBLE]",
    "start": "1581930",
    "end": "1588100"
  },
  {
    "text": "No, I just mean\nthat-- this is just--",
    "start": "1588100",
    "end": "1598760"
  },
  {
    "text": "phi is a name. You can call it f, f sub\ntheta or h sub theta.",
    "start": "1598760",
    "end": "1607340"
  },
  {
    "text": "It's just a name for this model\nthat is parameterized by theta.",
    "start": "1607340",
    "end": "1612740"
  },
  {
    "text": "So single phi theta\nis the neural network with parameter theta.",
    "start": "1612740",
    "end": "1617898"
  },
  {
    "text": "I just need a notation to\nindicate this function. I cannot just write theta. Theta and phi theta, they\ncorresponds to same thing.",
    "start": "1617899",
    "end": "1625159"
  },
  {
    "text": "But mathematically I\nhave to write phi sub theta of something\nto indicate that I'm applying this model on x.",
    "start": "1625159",
    "end": "1631179"
  },
  {
    "text": "[INAUDIBLE] [INAUDIBLE] specified. It could be anything.",
    "start": "1631179",
    "end": "1636980"
  },
  {
    "text": "So I can give a name for\nit, let's say the name is p. But it's not--",
    "start": "1636980",
    "end": "1642220"
  },
  {
    "text": "I didn't use it explicitly, yes. But dimensional theta,\nit could be big.",
    "start": "1642220",
    "end": "1651039"
  },
  {
    "text": "Any other questions? Yeah, it was just\nrelating to the loss",
    "start": "1651040",
    "end": "1657518"
  },
  {
    "text": "function for the training data. I'm having trouble understanding\n[INAUDIBLE] clustering",
    "start": "1657519",
    "end": "1667279"
  },
  {
    "text": "and the-- So I haven't told\nyou yet at all.",
    "start": "1667279",
    "end": "1673139"
  },
  {
    "text": "So just because there are\nso many different variants, I have to somehow do it\nin a top down fashion.",
    "start": "1673140",
    "end": "1680289"
  },
  {
    "text": "So here, for the\ndownstream task, I can do it just because\nthese two things are very--",
    "start": "1680290",
    "end": "1685840"
  },
  {
    "text": "you can use this for almost,\nalmost every solution. But when you talk\nabout pretraining, I have to talk about, what\ndo you do in computer vision,",
    "start": "1685840",
    "end": "1692770"
  },
  {
    "text": "or you're doing\nlanguage, at least there are some differences\ndepending on the domain. Of course, there are\nalso recent works",
    "start": "1692770",
    "end": "1698720"
  },
  {
    "text": "that try to unify all of this. But at least I have to somewhat\ntalk about the domains.",
    "start": "1698720",
    "end": "1707240"
  },
  {
    "text": "And that's what I'm going\nto talk about next, yeah. OK, cool, I guess maybe\nI'll just do that. So how do we pretrain?",
    "start": "1707240",
    "end": "1713840"
  },
  {
    "text": "So I'm going to pretrain\na representation. How do we do that? So let's first talk about\nthe vision settings,",
    "start": "1713840",
    "end": "1724679"
  },
  {
    "text": "or more kind of the\nclassifications, like the standard\nclassification settings.",
    "start": "1724679",
    "end": "1735610"
  },
  {
    "text": "So let's just for a\nminute think of a vision. So suppose you have\nsome images, right?",
    "start": "1735610",
    "end": "1741390"
  },
  {
    "text": "So there are two\ntypes of pretraining. So one type is called\nsupervised pretraining.",
    "start": "1741390",
    "end": "1751750"
  },
  {
    "text": "You may wonder why-- I already emphasize so much\nthat the pretraining should be mostly on unlabeled data.",
    "start": "1751750",
    "end": "1757350"
  },
  {
    "text": "But, actually, for\nvision, because you have a lot of label\ndata already-- ImageNet is a\npretty big dataset--",
    "start": "1757350",
    "end": "1762830"
  },
  {
    "text": "so you can actually do\nsupervised pretraining as well. And this is just exactly\nwhat we have seen before.",
    "start": "1762830",
    "end": "1769010"
  },
  {
    "text": "So you can just-- what you do is you just\nlearn a neural network.",
    "start": "1769010",
    "end": "1779190"
  },
  {
    "text": "Let's call this\nneural network, maybe let's call it u transpose\nphi of x, on label",
    "start": "1779190",
    "end": "1787450"
  },
  {
    "text": "that dataset, say, ImageNet.",
    "start": "1787450",
    "end": "1795919"
  },
  {
    "text": "And here the notation is\nthat this is the last layer of the neural network.",
    "start": "1795919",
    "end": "1804169"
  },
  {
    "text": "The last layer was\nlinear, recall. And phi theta of x is\nall the other layers.",
    "start": "1804169",
    "end": "1810289"
  },
  {
    "text": "It's basically the last,\nbut one layer activation.",
    "start": "1810290",
    "end": "1823789"
  },
  {
    "text": "So phi theta of x just\ndenote what you do in the first r minus 1 layers.",
    "start": "1823789",
    "end": "1830660"
  },
  {
    "text": "And then u transpose means\nthe last layer of the network.",
    "start": "1830660",
    "end": "1836090"
  },
  {
    "text": "And so theta, of\ncourse-- so basically, if I have a new artwork-- I don't know whether this-- I think we discussed this\nin one of the lectures,",
    "start": "1836090",
    "end": "1844480"
  },
  {
    "text": "so in deep learning. So you have a neural network. You have a lot of connections,\nand eventually you",
    "start": "1844480",
    "end": "1849980"
  },
  {
    "text": "output some y. And you view all of this\nas your phi of theta.",
    "start": "1849980",
    "end": "1855580"
  },
  {
    "text": "And you view this\nas the so-called u. And sometimes we call\nthis linear hat, right? So you just do this.",
    "start": "1855580",
    "end": "1862010"
  },
  {
    "text": "This is just exactly as what\nwe do in the neural network",
    "start": "1862010",
    "end": "1868929"
  },
  {
    "text": "lecture. And then you just\ndiscard u and just take",
    "start": "1868929",
    "end": "1877638"
  },
  {
    "text": "the phi theta of x, the learned\nas the pre-trained model.",
    "start": "1877639",
    "end": "1885090"
  },
  {
    "text": "So you think of\nthis as the feature. And think of this as a\nkind of universal feature",
    "start": "1885090",
    "end": "1892350"
  },
  {
    "text": "in some sense. And the hat is\nspecial to the tasks you use, right, to the\nlabel they say they used.",
    "start": "1892350",
    "end": "1899160"
  },
  {
    "text": "Maybe you used ImageNet. The hat has 1,000 classes.",
    "start": "1899160",
    "end": "1904549"
  },
  {
    "text": "That's something special. But the feature is something\nkind of more universal. You just take the features\nand discard the hat.",
    "start": "1904549",
    "end": "1911148"
  },
  {
    "text": "And then once you have a new\ntask, maybe not ImageNet, maybe some other classification\ntask, maybe let's say now you have a new\ntask which is medical imaging,",
    "start": "1911149",
    "end": "1919390"
  },
  {
    "text": "right, like you're\ndetecting from a scan image whether some person has,\nlet's say, cancer, right?",
    "start": "1919390",
    "end": "1925840"
  },
  {
    "text": "So then you just take this\npart, this phi theta, and then you apply a new hat.",
    "start": "1925840",
    "end": "1931790"
  },
  {
    "text": "I'm using W as the new hat. You apply a new hat. And then you fine tune\nall, just a linear probe,",
    "start": "1931790",
    "end": "1941020"
  },
  {
    "text": "on your downstream task. So you remove this hat\nand apply something else. And then you do some training\non the downstream task.",
    "start": "1941020",
    "end": "1949200"
  },
  {
    "text": "So that's the\nsupervised pretraining. If you're-- the medical images\nare a different size than",
    "start": "1949200",
    "end": "1959649"
  },
  {
    "text": "the ones that it was pretrained\non, what kind of stuff we need",
    "start": "1959649",
    "end": "1965909"
  },
  {
    "text": "to do to make it work? Yeah, I think typically\nif the size are different, you just have to do\nthe upsampling of the-- I'm assuming maybe it might\nmake this lower resolution.",
    "start": "1965909",
    "end": "1971890"
  },
  {
    "text": "You just have to do\nsome upsampling of the, I guess what's the\nright word for it?",
    "start": "1971890",
    "end": "1978049"
  },
  {
    "text": "You just make the size bigger. I think either you can\npass some zeros outside, or you can just maybe replicate\nthe pixels in some ways.",
    "start": "1978049",
    "end": "1983908"
  },
  {
    "text": "There's nothing fancy. Also, you mentioned earlier\nthat if the task isn't similar,",
    "start": "1983909",
    "end": "1990050"
  },
  {
    "text": "it might not work as well. So is that a concern here?",
    "start": "1990050",
    "end": "1998470"
  },
  {
    "text": "So here, at least\n[INAUDIBLE],, you don't have to concern much\nbecause of that, because your,",
    "start": "1998470",
    "end": "2008090"
  },
  {
    "text": "let's say the pretrained images,\nthey have the ImageNet classes. They are all sometimes animals,\nall common objects, right?",
    "start": "2008090",
    "end": "2015789"
  },
  {
    "text": "But, anyway, you\ndiscard the hat. You remove the hat and\nthen you add a new hat.",
    "start": "2015790",
    "end": "2021399"
  },
  {
    "text": "Maybe now your new task, I just\nhave two labels, right, cancer or not, right? You just apply it to that.",
    "start": "2021399",
    "end": "2028130"
  },
  {
    "text": "But so in terms\nof the method, you can actually apply it when\nthe task labels are different.",
    "start": "2028130",
    "end": "2033169"
  },
  {
    "text": "But whether it\nwill work sometimes depends on how different your\nnew task is from the old task.",
    "start": "2033169",
    "end": "2040268"
  },
  {
    "text": "So typically if you use\nImageNet pretraining, you always learn something\nreasonable about the features.",
    "start": "2040269",
    "end": "2045360"
  },
  {
    "text": "You wouldn't be terrible. But if your task-- for\nexample, your downstream task is really not even\ncommon images,",
    "start": "2045360",
    "end": "2052560"
  },
  {
    "text": "that's just some kind\nof random images, then probably it\nwouldn't help that much.",
    "start": "2052560",
    "end": "2060358"
  },
  {
    "text": "OK, so here there's nothing\nreally that's fancy.",
    "start": "2060359",
    "end": "2065769"
  },
  {
    "text": "And people did this anyways\nlike in the beginning of the deep learning era.",
    "start": "2065770",
    "end": "2073608"
  },
  {
    "text": "And there are some\nother kind of like-- and the second one I'm\ngoing to talk about is the so-called\ncontrastive learning, which",
    "start": "2073609",
    "end": "2079579"
  },
  {
    "text": "is now unlabeled, like\nunsupervised learning",
    "start": "2079579",
    "end": "2086589"
  },
  {
    "text": "algorithm for pretraining, or\npeople call it self-supervized learning.",
    "start": "2086589",
    "end": "2091720"
  },
  {
    "text": "So contrastive learning-- so\nnow I don't have any labels.",
    "start": "2091720",
    "end": "2099000"
  },
  {
    "text": "I just have unlabeled dataset. I need to define loss\nfunction on unlabeled dataset.",
    "start": "2099000",
    "end": "2104940"
  },
  {
    "text": "So how do I do that? So I need to first introduce\na notion called the data",
    "start": "2104940",
    "end": "2113900"
  },
  {
    "text": "augmentation. So a data augmentation\nis something",
    "start": "2113900",
    "end": "2119240"
  },
  {
    "text": "that, as the name suggests,\nyou are augmenting one example into a artificial example\nthat still makes some sense.",
    "start": "2119240",
    "end": "2128270"
  },
  {
    "text": "So, and typical for images,\nwhat you do is you just say, I have an original image.",
    "start": "2128270",
    "end": "2138609"
  },
  {
    "text": "And then you augment by doing\nsome kind of so-called random-- you can do a few things,\nmaybe a random crop",
    "start": "2138609",
    "end": "2148990"
  },
  {
    "text": "to crop a patch of\nimage as the new image.",
    "start": "2148990",
    "end": "2154750"
  },
  {
    "text": "Or you can apply a\nrandom crop plus a flip. You can flip the image\njust with a mirror flip.",
    "start": "2154750",
    "end": "2161890"
  },
  {
    "text": "Or you can do some\ncolor transformation. Color transformation means\nthat maybe you make the image",
    "start": "2161890",
    "end": "2170450"
  },
  {
    "text": "darker, brighter. Or sometimes you can just\neven do weird, more weird color transformation.",
    "start": "2170450",
    "end": "2176049"
  },
  {
    "text": "You change all the brown\ncolor to the white color. You can do some kind of\nchanging of the color scheme.",
    "start": "2176050",
    "end": "2182340"
  },
  {
    "text": "And there are many others,\nsome of them are more advanced. And sometimes you can even\nlearn a transformation.",
    "start": "2182340",
    "end": "2187750"
  },
  {
    "text": "But these are the common ones. So, basically,\ngiven an image, you can do this random operations,\nkind of like you can try",
    "start": "2187750",
    "end": "2193910"
  },
  {
    "text": "choose to flip or not. That's a random decision. You can choose to crop\nwhich part of the image.",
    "start": "2193910",
    "end": "2199680"
  },
  {
    "text": "Like you can do some kind of\nrandom color transformation. So you have some\nrandomness here.",
    "start": "2199680",
    "end": "2204730"
  },
  {
    "text": "And then give an\nimage x, basically you can generate a random\naugmentation that's called x hat.",
    "start": "2204730",
    "end": "2211220"
  },
  {
    "text": "And if you do this again, you\ncan generate another one, which I'm going to call x tilde.",
    "start": "2211220",
    "end": "2217890"
  },
  {
    "text": "And if we do it\nmore and more times, you can train even more\nof these augmentations. So this augmentation\nwas used in supervised",
    "start": "2217890",
    "end": "2225180"
  },
  {
    "text": "learning actually as well. We didn't discuss\nthem just because they are low-level details.",
    "start": "2225180",
    "end": "2230220"
  },
  {
    "text": "For supervised learning,\nwhat you actually do is that you can, given\nan image and the label, you can generate\nthe augmentation",
    "start": "2230220",
    "end": "2237318"
  },
  {
    "text": "and then just assume the\naugmentation is your new image. So you just replace all the\nimage by the augmentation.",
    "start": "2237319",
    "end": "2244579"
  },
  {
    "text": "And every time you do this\nwith a new augmentation, like every time\nyou see this image,",
    "start": "2244579",
    "end": "2250849"
  },
  {
    "text": "you replace the image\nby a new augmentation, and tune with that\naugment images.",
    "start": "2250849",
    "end": "2256300"
  },
  {
    "text": "And that seems to\nimprove-- in some sense, you make the dataset bigger in\nsome sense because these are--",
    "start": "2256300",
    "end": "2261849"
  },
  {
    "text": "every time you\naugment, you're going to get a different\nimage, in some sense, even though they are\nsomewhat similar,",
    "start": "2261849",
    "end": "2267599"
  },
  {
    "text": "but still you make the\neffective size of the dataset a little bit bigger. So that's why people use\nit in supervised learning.",
    "start": "2267599",
    "end": "2278020"
  },
  {
    "text": "But now I'm going to use it\nin unsupervised learning. Actually, now people call\nit self-supervised learning.",
    "start": "2278020",
    "end": "2283119"
  },
  {
    "text": "So I'm going to use this to\ncreate some kind of supervision or create some kind\nof unsupervised loss.",
    "start": "2283120",
    "end": "2288569"
  },
  {
    "text": "What you do is you say, I'm\ngoing to call this x hat-- first of all, I'm going\nto define some notation.",
    "start": "2288570",
    "end": "2294700"
  },
  {
    "text": "This is called a positive pair.",
    "start": "2294700",
    "end": "2301109"
  },
  {
    "text": "So positive pairs\nare augmentations of the same image. So one, you can imagine one\nproperty of the positive pair",
    "start": "2301109",
    "end": "2310349"
  },
  {
    "text": "is that these two images,\ntwo augment the images, probably should be somewhat\nsimilar semantically.",
    "start": "2310349",
    "end": "2316568"
  },
  {
    "text": "Even though they may not\nhave the same color scheme, they may not have exactly\nthe same orientation, they are still somewhat\nsimilar semantically.",
    "start": "2316569",
    "end": "2326450"
  },
  {
    "text": "So what you do is you say,\nyou are going to try to make--",
    "start": "2326450",
    "end": "2333300"
  },
  {
    "text": "you are going to design loss\nfunction such that, one, you want to make a phi of\ntheta of x hat and phi of theta",
    "start": "2333300",
    "end": "2343970"
  },
  {
    "text": "x tilde closer.",
    "start": "2343970",
    "end": "2348990"
  },
  {
    "text": "So basically you say, I\nhave two augmentations. One is x hat the\nother is x tilde.",
    "start": "2348990",
    "end": "2354570"
  },
  {
    "text": "You want these two augmentations\nto have similar representations",
    "start": "2354570",
    "end": "2360090"
  },
  {
    "text": "in the Euclidean space. So that's one goal of the loss.",
    "start": "2360090",
    "end": "2365670"
  },
  {
    "text": "I'm going to tell you exactly\nwhat the loss looks like. But this is one\ngoal of the loss. But if you just\nhave this goal, you",
    "start": "2365670",
    "end": "2371319"
  },
  {
    "text": "can see that this\nloss function is a little bit questionable\nin some sense because maybe you\nshould just map every image to the same point.",
    "start": "2371319",
    "end": "2378960"
  },
  {
    "text": "Then everything has exactly\nthe same representation. Then you satisfy this goal. So, ideally, you\nshould have something",
    "start": "2378960",
    "end": "2385838"
  },
  {
    "text": "that counterbalance to avoid\nyou to just collapse everything into one thing. So how do you counterbalance?",
    "start": "2385839",
    "end": "2391859"
  },
  {
    "text": "The way you counterbalance\nis that you, just to say, you want random images to have\ndifferent representations.",
    "start": "2391859",
    "end": "2400710"
  },
  {
    "text": "So let's erase something here.",
    "start": "2400710",
    "end": "2406530"
  },
  {
    "text": "[INAUDIBLE] So, basically, to\ncounterbalance this,",
    "start": "2406530",
    "end": "2423480"
  },
  {
    "text": "the second goal\nis that you want-- suppose you say, I'm going to\nhave some two random example.",
    "start": "2423480",
    "end": "2433220"
  },
  {
    "text": "And so you have maybe x and\nz, so maybe a cat and dog.",
    "start": "2433220",
    "end": "2441810"
  },
  {
    "text": "And then you augment x into\nx hat as the same way here.",
    "start": "2441810",
    "end": "2447290"
  },
  {
    "text": "And then you augment z to z hat.",
    "start": "2447290",
    "end": "2453020"
  },
  {
    "text": "So because x and z are\ntwo different images, maybe one is cat and\nthe other is dog, so the augmentations probably\nlooks very different as well.",
    "start": "2453020",
    "end": "2460480"
  },
  {
    "text": "And the augmentations are\nsemantically very different. So then your second goal\nis to make phi theta",
    "start": "2460480",
    "end": "2470480"
  },
  {
    "text": "x hat and phi theta z hat,\nthe augmentations of two",
    "start": "2470480",
    "end": "2479609"
  },
  {
    "text": "different examples, far away.",
    "start": "2479610",
    "end": "2486859"
  },
  {
    "text": "So this is to counterbalance, to\navoid you to collect everything to the same point. And, actually, there's\na name for this pair.",
    "start": "2486859",
    "end": "2493290"
  },
  {
    "text": "It's x hat, z hat is often\ncalled either a random pair",
    "start": "2493290",
    "end": "2501099"
  },
  {
    "text": "or an active pair. At the very beginning,\nI think people",
    "start": "2501099",
    "end": "2509530"
  },
  {
    "text": "call it negative pair, which is\nnot exactly right in the sense that x and z are just\nrandom choice of two images.",
    "start": "2509530",
    "end": "2518720"
  },
  {
    "text": "There's no guarantee\nthat they are exactly-- they don't have the same\nclass or same meaning.",
    "start": "2518720",
    "end": "2526569"
  },
  {
    "text": "There is some small\nchance that both x and z are both from the same class.",
    "start": "2526569",
    "end": "2532200"
  },
  {
    "text": "You may have a thousand classes. There's still a\nlittle bit chance that x and z are\nfrom the same class. But in most of the cases, x and\nz are from different classes.",
    "start": "2532200",
    "end": "2539480"
  },
  {
    "text": "And they are\nsemantically different. So random pair might be a\nlittle bit more accurate, but negative pair is what\npeople call it at the beginning.",
    "start": "2539480",
    "end": "2546100"
  },
  {
    "text": "And I think now people just\nuse these interchangeably. So, basically, you\nwant the random pairs",
    "start": "2546100",
    "end": "2553630"
  },
  {
    "text": "to have different\nrepresentations. And you want positive pairs to\nhave similar representations.",
    "start": "2553630",
    "end": "2560859"
  },
  {
    "text": "So this is the design principle. How do you do this exactly? So there are much more papers.",
    "start": "2560859",
    "end": "2566160"
  },
  {
    "text": "There are at least\nfour or five papers that use this kind\nof like a principle.",
    "start": "2566160",
    "end": "2572380"
  },
  {
    "text": "And sometimes actually,\nsome papers actually even drop the number two. You just use one.",
    "start": "2572380",
    "end": "2577800"
  },
  {
    "text": "And somehow still sometimes\nit works just because there's some other kind\nof counterbalance",
    "start": "2577800",
    "end": "2583000"
  },
  {
    "text": "in a system that can achieve two\nwithout explicit encouragement. But let's not talk about those.",
    "start": "2583000",
    "end": "2588800"
  },
  {
    "text": "Let's talk about\none case where-- the simplest case. We have both one and\ntwo explicitly encoded in the loss function,\nwhich is called SIMCLR.",
    "start": "2588800",
    "end": "2595600"
  },
  {
    "text": "And this is, I think,\nbasically the first paper that makes this kind of idea work.",
    "start": "2595600",
    "end": "2604480"
  },
  {
    "text": "So how do you exactly encode\nthese two kind of principle? So in SIMCLR what people\ndo is the following.",
    "start": "2604480",
    "end": "2613710"
  },
  {
    "text": "So what you do is\nyou say you'll first take a batch of example,\na random batch of example,",
    "start": "2613710",
    "end": "2620740"
  },
  {
    "text": "like in SVG. Let's call this\nexample x1 up to xB.",
    "start": "2620740",
    "end": "2627420"
  },
  {
    "text": "So you have B example. And then you first do\nsome augmentations.",
    "start": "2627420",
    "end": "2633300"
  },
  {
    "text": "So you augment to x\nhat 1 up to x hat B,",
    "start": "2633300",
    "end": "2644980"
  },
  {
    "text": "and x tilde 1 up to x\ntilde B. As you can see,",
    "start": "2644980",
    "end": "2651800"
  },
  {
    "text": "these two are the augmentations\nof the first example. These two are the augmentations\nof the second example.",
    "start": "2651800",
    "end": "2656890"
  },
  {
    "text": "And these two are\nthe augmentations of the B-th example. And then here is\nthe loss function.",
    "start": "2656890",
    "end": "2663960"
  },
  {
    "text": "So let me write down\nthe loss function and then I can explain.",
    "start": "2663960",
    "end": "2674250"
  },
  {
    "text": "So the intuition is that\nyou want to design a loss function that basically maybe-- let's see, so-- oh, I don't\nhave a different color today.",
    "start": "2674250",
    "end": "2684260"
  },
  {
    "text": "So because these two\nare augmentations of the same example,\nyou want them to have",
    "start": "2684260",
    "end": "2689780"
  },
  {
    "text": "similar representations. And maybe the same thing\nhappens for these two. So any pairs you should have\nsimilar representations.",
    "start": "2689780",
    "end": "2697500"
  },
  {
    "text": "But if you look at\nthis one and this one, suppose you pick\nsomething like this, then you want them to have\ndifferent representations.",
    "start": "2697500",
    "end": "2706190"
  },
  {
    "text": "And what you do is that\nyou make the loss function. The loss function\nis equal to this.",
    "start": "2706190",
    "end": "2712549"
  },
  {
    "text": "This is a little bit\ncomplicated at the first sight,",
    "start": "2712549",
    "end": "2718801"
  },
  {
    "text": "but it's actually not that hard.",
    "start": "2718801",
    "end": "2779530"
  },
  {
    "text": "So I guess-- so here the indices\nis the most important thing. So this is i.",
    "start": "2779530",
    "end": "2785670"
  },
  {
    "text": "This is i. This is i. This is i. And here this is j.",
    "start": "2785670",
    "end": "2791810"
  },
  {
    "text": "Sorry, my handwriting\nis a little bit unclear. So, OK, so maybe the first thing\nto realize is the following.",
    "start": "2791810",
    "end": "2802020"
  },
  {
    "text": "So maybe let's\nfocus on this term.",
    "start": "2802020",
    "end": "2809050"
  },
  {
    "text": "Maybe lets call this\nterm A and this term B. Realize that this is also\nA, so just the same term.",
    "start": "2809050",
    "end": "2818420"
  },
  {
    "text": "So this is really something like\na minus log, a over a plus b,",
    "start": "2818420",
    "end": "2824240"
  },
  {
    "text": "something like this,\nabstractly speaking. And if you do some\nsimple math, you'll see that this one\nis decreasing in A",
    "start": "2824240",
    "end": "2842500"
  },
  {
    "text": "and increasing in B.\nThis is relatively easy.",
    "start": "2842500",
    "end": "2854599"
  },
  {
    "text": "I guess you can either\ntake a derivative or just-- at least the increase\nin B is pretty easy because this function is--",
    "start": "2854599",
    "end": "2861078"
  },
  {
    "text": "this function is\ndecreasing in B. And log is a monotone function. And you have a minus in front.",
    "start": "2861079",
    "end": "2866380"
  },
  {
    "text": "And for A it's the\nreverse direction. So that means that if you\nminimize this loss of function,",
    "start": "2866380",
    "end": "2878590"
  },
  {
    "text": "you are trying to encourage\nthe term A to be smaller because it's decreasing A--",
    "start": "2878590",
    "end": "2884680"
  },
  {
    "text": "sorry, you want\nthe term A bigger because the loss\nfunction is decreasing A. The bigger the A is, the\nsmaller the loss function is.",
    "start": "2884680",
    "end": "2892990"
  },
  {
    "text": "So you want this A term, which\nis this inner product between--",
    "start": "2892990",
    "end": "2900970"
  },
  {
    "text": "actually A term is exponential\nof the inner product. So you want A, which is\nequal to exponential,",
    "start": "2900970",
    "end": "2908359"
  },
  {
    "text": "phi theta x hat i transpose\nphi theta x tilde i to be--",
    "start": "2908359",
    "end": "2918119"
  },
  {
    "text": "you want this to be big. You want this to be big.",
    "start": "2918119",
    "end": "2928300"
  },
  {
    "text": "That means that you want this\nrepresentation of the x hat i and the representation\nof the x tilde i",
    "start": "2928300",
    "end": "2934500"
  },
  {
    "text": "to be as close to each\nother as possible. You want their inner\nproduct to be big. That fulfills our\nfirst goal where",
    "start": "2934500",
    "end": "2941470"
  },
  {
    "text": "we want the representation\nof two examples, the representation\nof the augmentation",
    "start": "2941470",
    "end": "2948330"
  },
  {
    "text": "of the same example, to\nbe as close as possible. That's the first goal.",
    "start": "2948330",
    "end": "2954710"
  },
  {
    "text": "And then you want this B\nterm to be small, right? Because the function\nis increasing B,",
    "start": "2954710",
    "end": "2961089"
  },
  {
    "text": "and you are minimizing\nthe function, the bigger the function\nB is, the smaller-- sorry, the function\nis increasing B.",
    "start": "2961090",
    "end": "2968790"
  },
  {
    "text": "And you want the\nfunction to be small. So you want the B\nto be also small. The smaller the B is, the\nsmaller the function is.",
    "start": "2968790",
    "end": "2975030"
  },
  {
    "text": "If you want these\ntwo to be small, and one of the terms\nin B, basically you have this kind of term.",
    "start": "2975030",
    "end": "2986960"
  },
  {
    "text": "This is the terms in B,\nwhere you have i and j here.",
    "start": "2986960",
    "end": "2992710"
  },
  {
    "text": "So basically you want to\nsay that for different-- this is augmentation\nof the i-th example.",
    "start": "2992710",
    "end": "2998210"
  },
  {
    "text": "This is augmentation\nof the j-th example. So their augmentations\nshould be small.",
    "start": "2998210",
    "end": "3005180"
  },
  {
    "text": "So their inner product\nshould be small. So that means that\nthe repetitions should",
    "start": "3005180",
    "end": "3010270"
  },
  {
    "text": "be far away from each other. Any questions?",
    "start": "3010270",
    "end": "3018200"
  },
  {
    "text": "Does [INAUDIBLE] also\nhave [INAUDIBLE]??",
    "start": "3018200",
    "end": "3027660"
  },
  {
    "text": "Exponentially is\nincreasing function. If you want the exponential\nof something to be small, then it means you want\nargument of the exponential to be also small.",
    "start": "3027660",
    "end": "3042480"
  },
  {
    "text": "[INAUDIBLE] The first one is i\nand the second is j.",
    "start": "3042480",
    "end": "3048240"
  },
  {
    "text": "So j only shows up once. All the others are i.",
    "start": "3048240",
    "end": "3055549"
  },
  {
    "text": "So there are other\ninterpretations of this loss function. I'm not sure whether they\nare easier to understand",
    "start": "3055549",
    "end": "3060660"
  },
  {
    "text": "or harder to\nunderstand than this. One interpretation\nof this loss function is that you can view these as\na multi class classification",
    "start": "3060660",
    "end": "3068910"
  },
  {
    "text": "kind of question. So you are trying\nto distinguish-- so basically, in\nsome sense, you can--",
    "start": "3068910",
    "end": "3074720"
  },
  {
    "text": "supposedly do a lot\nof math, you can see that this is the same laws\nas the following hypothetical",
    "start": "3074720",
    "end": "3081780"
  },
  {
    "text": "question. So the question is, given\nthis dataset, I want to, say, given for example x hat\ni, the first augmentation",
    "start": "3081780",
    "end": "3092460"
  },
  {
    "text": "of the i-th example,\nI want to distinguish which one is the positive pair\nand which one is negative pair.",
    "start": "3092460",
    "end": "3100260"
  },
  {
    "text": "So, basically,\nlet's say, suppose you are-- this is another\ninterpretation, which I personally think it's\nharder to understand.",
    "start": "3100260",
    "end": "3108260"
  },
  {
    "text": "But let me just anyway say it. So suppose you have\nsome example x hat i.",
    "start": "3108260",
    "end": "3113940"
  },
  {
    "text": "And you have the\ncorresponding x tilde i. So you can kind of view\nthis as, you want to--",
    "start": "3113940",
    "end": "3121740"
  },
  {
    "text": "given this x hat\ni, you want to test which one of these B examples\nare the most correlated",
    "start": "3121740",
    "end": "3130910"
  },
  {
    "text": "with i in some sense. And you want the\nx hat i, this one,",
    "start": "3130910",
    "end": "3136640"
  },
  {
    "text": "to stand out in\nsome sense compared to the other correlation. You want the correlation\nbetween these two",
    "start": "3136640",
    "end": "3142380"
  },
  {
    "text": "to be dominating\nall the others so that you can kind of say this\none is my buddy, in some sense.",
    "start": "3142381",
    "end": "3147960"
  },
  {
    "text": "It's the other\nexample in the pair. Yeah, so but anyway, the exact\nform of the loss function",
    "start": "3147960",
    "end": "3157200"
  },
  {
    "text": "is not that important. They are actually other\nways to implement this. It's not like it's\nreally necessary to have",
    "start": "3157200",
    "end": "3163069"
  },
  {
    "text": "to use this loss function. The principle is probably\nmore important than the loss function.",
    "start": "3163069",
    "end": "3168390"
  },
  {
    "text": "So, cool-- so I guess\njust to summarize,",
    "start": "3168390",
    "end": "3174650"
  },
  {
    "text": "this is a loss function\nthat only depends on x. So you didn't use\nanything about label.",
    "start": "3174650",
    "end": "3180000"
  },
  {
    "text": "So it's unsupervised or self\nsupervised loss function. So it's self\nsupervising because--",
    "start": "3180000",
    "end": "3186710"
  },
  {
    "text": "self supervise just means\nyou don't use any supervision except some part of your self.",
    "start": "3186710",
    "end": "3195130"
  },
  {
    "text": "OK, any other questions? [INAUDIBLE] you'll find\nin this case [INAUDIBLE],,",
    "start": "3195130",
    "end": "3205078"
  },
  {
    "text": "like classify which image it is? No, no, phi is\nstill the same phi.",
    "start": "3205079",
    "end": "3212349"
  },
  {
    "text": "The phi is just the feature,\nthe representations-- the function that computes\nthe representation",
    "start": "3212349",
    "end": "3218869"
  },
  {
    "text": "is the feature extractor,\nor something like that. Does that make sense?",
    "start": "3218869",
    "end": "3226040"
  },
  {
    "text": "So phi [INAUDIBLE] That's\nwhy I have the transpose.",
    "start": "3226040",
    "end": "3232750"
  },
  {
    "text": "OK, so next thing I'm\ngoing to talk about",
    "start": "3232750",
    "end": "3239790"
  },
  {
    "text": "is how do you do pretraining\nwhen you have language. And I'm going I'm going to tell\nyou one method, which is also",
    "start": "3239790",
    "end": "3250619"
  },
  {
    "text": "a self supervised or\nunsupervised pretraining, but the method is a\nlittle bit different.",
    "start": "3250619",
    "end": "3255849"
  },
  {
    "text": "[INAUDIBLE] So after training the\nmodel, we basically",
    "start": "3255849",
    "end": "3284839"
  },
  {
    "text": "discard the very\nlast layer and use all to remaining available to\nperform all the possible image",
    "start": "3284839",
    "end": "3294380"
  },
  {
    "text": "classifications. No, there is no loss\nlayer anymore here, right? This phi of theta is a\nneural network, rihgt?",
    "start": "3294380",
    "end": "3302490"
  },
  {
    "text": "This phi of theta is\nlike some neural network start from x, a lot of neurons. We draw a lot of edges,\nsomething like this.",
    "start": "3302490",
    "end": "3310329"
  },
  {
    "text": "And, eventually, you\noutput some embeddings. This is phi of theta.",
    "start": "3310329",
    "end": "3316349"
  },
  {
    "text": "And that's it. That's your representation. So you don't have\nto discard losses.",
    "start": "3316349",
    "end": "3321440"
  },
  {
    "text": "But you have to\nadd loss layer when you do the classification task. Does that make sense?",
    "start": "3321440",
    "end": "3331970"
  },
  {
    "text": "This drawing is a little bit\ntoo [INAUDIBLE],, OK, cool.",
    "start": "3331970",
    "end": "3346950"
  },
  {
    "text": "So large language models--",
    "start": "3346950",
    "end": "3352950"
  },
  {
    "text": "so the first thing I\nneed to address is that, how do I encode the data? So I have some texts.",
    "start": "3352950",
    "end": "3359099"
  },
  {
    "text": "They are discrete words. I need to first turn them\ninto numerical numbers. I guess if you remember,\nI think a few weeks back,",
    "start": "3359099",
    "end": "3367930"
  },
  {
    "text": "so we talk about this\nevent model, this model that you include data by, this\nvery simple binary vector,",
    "start": "3367930",
    "end": "3375400"
  },
  {
    "text": "every document is encoded as\na 0, 1 vector of some steps.",
    "start": "3375400",
    "end": "3382460"
  },
  {
    "text": "So those are very simple. So today we're going to\ndo the more realistic one.",
    "start": "3382460",
    "end": "3387530"
  },
  {
    "text": "So you just-- but the encoding\nbecomes easier actually, just because what you\ndo is you just directly",
    "start": "3387530",
    "end": "3393130"
  },
  {
    "text": "encode each vector, each word,\nas just the discrete token.",
    "start": "3393130",
    "end": "3399299"
  },
  {
    "text": "So let me just say this. So I'm saying that this\nis a way to encode, but the encoding\nis, to some extent,",
    "start": "3399300",
    "end": "3408410"
  },
  {
    "text": "conceptually much simpler. So, basically, the first\nthing is that, let's define what is an example.",
    "start": "3408410",
    "end": "3413599"
  },
  {
    "text": "We have a language. So, typically, I think for me,\nthe best way to think about",
    "start": "3413600",
    "end": "3419299"
  },
  {
    "text": "the example in language is that\nyou think of it as a document or something like that, a\nsequence of words because you",
    "start": "3419299",
    "end": "3426151"
  },
  {
    "text": "cannot view every word as\nexample, then you lose the-- examples are supposed\nto be somewhat",
    "start": "3426151",
    "end": "3431250"
  },
  {
    "text": "independent with each other. So if you use each\nword as an example, that's too small\nfor granularity.",
    "start": "3431250",
    "end": "3437280"
  },
  {
    "text": "So you view each\ndocument as example. So that's the kind of\nmental picture I think of.",
    "start": "3437280",
    "end": "3442750"
  },
  {
    "text": "So you have a sequence of\nwords, maybe x1 up to xT. And this is one example.",
    "start": "3442750",
    "end": "3448619"
  },
  {
    "text": "In reality, what people do\nis that people don't really just literally look at which--",
    "start": "3448619",
    "end": "3456109"
  },
  {
    "text": "the documents because\nwhat you are given is that you are given, for\nexample, all the Wikipedia text on Wikipedia.",
    "start": "3456109",
    "end": "3462099"
  },
  {
    "text": "And this is a gigantic\nfile which is really just a sequence of words. And everything is\nconcatenated together.",
    "start": "3462099",
    "end": "3468099"
  },
  {
    "text": "And then what people do\nis that you just truncate, you just take a\nconsecutive sequence",
    "start": "3468099",
    "end": "3473950"
  },
  {
    "text": "of words as one example. Maybe you take something\nlike maybe 500 words,",
    "start": "3473950",
    "end": "3479510"
  },
  {
    "text": "consecutive 500 words, or maybe But in any case, I\nthink if you don't",
    "start": "3479510",
    "end": "3486099"
  },
  {
    "text": "have all the details,\nimplementation details, it's just fine to think of\neach example as a document.",
    "start": "3486100",
    "end": "3491420"
  },
  {
    "text": "And important thing is\nthe sequence of words. And there's another small\ndetail in the implementation.",
    "start": "3491420",
    "end": "3497380"
  },
  {
    "text": "So when you really do\nthis, sometimes you don't really operate on\na granularity of words.",
    "start": "3497380",
    "end": "3503760"
  },
  {
    "text": "For example, one\npossible choices can operate on the\ngranularity of characters. You do each character as one xi.",
    "start": "3503760",
    "end": "3511369"
  },
  {
    "text": "People don't really do that. What people do is that\npeople are operating on the level called tokens.",
    "start": "3511370",
    "end": "3517088"
  },
  {
    "text": "And each token--\ntypically these days in the best model, each\ntoken is kind of like a word, but sometimes its\nsmaller than a word.",
    "start": "3517089",
    "end": "3523510"
  },
  {
    "text": "So basically you can think of\nmost of the common tokens-- or sorry, most of the common\nwords are a single token.",
    "start": "3523510",
    "end": "3529680"
  },
  {
    "text": "The top-- I think\nlast time I checked this it's like the\nfirst 20k frequent words",
    "start": "3529680",
    "end": "3538180"
  },
  {
    "text": "are all just a single\ntoken by themselves. But sometimes you have\nlonger words that just never",
    "start": "3538180",
    "end": "3543660"
  },
  {
    "text": "show up very often. And then you break them in\nsome way into two tokens. So a very, very long\nword might be two tokens.",
    "start": "3543660",
    "end": "3553119"
  },
  {
    "text": "This is just another\nsmall detail, just in case you are\nimplementing anything like this. But for conceptually, you\njust think of each word",
    "start": "3553119",
    "end": "3559220"
  },
  {
    "text": "as a single xi here. And you have a sequence\nof words, key words here. That's my example.",
    "start": "3559220",
    "end": "3565780"
  },
  {
    "text": "And then what you\ndo is that you say-- and let's say, suppose\nyou have a vocabulary. So each word is in\nsome set 1 to V.",
    "start": "3565780",
    "end": "3576079"
  },
  {
    "text": "So you have V possible words. And each token, each example\nis a sequence of words.",
    "start": "3576079",
    "end": "3583869"
  },
  {
    "text": "And when you say language\nmodel, basically people always refers to a\nprobabilistic model--",
    "start": "3583869",
    "end": "3594960"
  },
  {
    "text": "probabilistic-- for p of\nthe joint probability.",
    "start": "3594960",
    "end": "3603289"
  },
  {
    "text": "In some sense, this\nis the same kind of modeling methodology\nas what we do with mixture",
    "start": "3603289",
    "end": "3609819"
  },
  {
    "text": "of Gaussians, right? You are modeling the joint\nprobability of your data, of your x.",
    "start": "3609819",
    "end": "3616220"
  },
  {
    "text": "But if you just directly\nmodel the probability, this joint probability\nis very difficult because this one has support.",
    "start": "3616220",
    "end": "3623410"
  },
  {
    "text": "This one has support size,\nits distribution, right?",
    "start": "3623410",
    "end": "3629200"
  },
  {
    "text": "So like how many possible\nsentences you can have here? You can have-- support size\nis something like V to the t",
    "start": "3629200",
    "end": "3636770"
  },
  {
    "text": "because each word can have V\nchoices and you have t words. So this is really a\nexponentially large family",
    "start": "3636770",
    "end": "3643640"
  },
  {
    "text": "of possible sentences. And if you model\nthis distribution it's kind of challenging. So what people do is that\npeople do is use chain rule.",
    "start": "3643640",
    "end": "3654390"
  },
  {
    "text": "So you say this\njoint probability can be written as p of x1\ntimes p of x2 given x1 times",
    "start": "3654390",
    "end": "3665589"
  },
  {
    "text": "p of x3 times x1, x2, up to p of\nxt, given x1 up to xt minus 1.",
    "start": "3665589",
    "end": "3683680"
  },
  {
    "text": "And then you model--",
    "start": "3683680",
    "end": "3690180"
  },
  {
    "text": "each of these p xt, little\nt, given x1 up to xt minus 1.",
    "start": "3690180",
    "end": "3695710"
  },
  {
    "text": "You this conditional\nprobability, this conditional probability,\nusing some parametric form.",
    "start": "3695710",
    "end": "3703440"
  },
  {
    "text": "And then you learn\nthe parameters.",
    "start": "3703440",
    "end": "3708700"
  },
  {
    "text": "And the good thing about\nthis conditional probability is that now you only care about\nthe one, the probability of one",
    "start": "3708700",
    "end": "3715079"
  },
  {
    "text": "word. The probability of\nthat one word is of size v, the support\nof this probability.",
    "start": "3715079",
    "end": "3721569"
  },
  {
    "text": "So the number of\nchoices of words here is v, so instead of\nv to the power of t.",
    "start": "3721570",
    "end": "3727720"
  },
  {
    "text": "So the next question is,\nhow do you model this? How do you build\na parametric form for this conditional\nprobability?",
    "start": "3727720",
    "end": "3734339"
  },
  {
    "text": "So I'm not going to tell\nyou exactly how you do it.",
    "start": "3734340",
    "end": "3739700"
  },
  {
    "text": "But generally you just do\nit with a neural network. I'm going to tell\nyou, there's some kind of details I'm omitting here.",
    "start": "3739700",
    "end": "3746170"
  },
  {
    "text": "But roughly speaking, what\nyou do is the following. So, first, the first\nthing, you have",
    "start": "3746170",
    "end": "3753430"
  },
  {
    "text": "to do embedding for the words,\nmeaning for every word x,",
    "start": "3753430",
    "end": "3763430"
  },
  {
    "text": "you embed this\nfor every word i--",
    "start": "3763430",
    "end": "3768760"
  },
  {
    "text": "this is a word i-- you embed this word\ninto a vector ei. And this vector ei, let's\nsay, is in dimension d.",
    "start": "3768760",
    "end": "3778099"
  },
  {
    "text": "So every word will\ncorrespond to a vector. So you're going\nto have v vectors. Each vector\ncorresponds to a word.",
    "start": "3778099",
    "end": "3784680"
  },
  {
    "text": "And these vectors\nwill be learned. So these are parameters\nof our system.",
    "start": "3784680",
    "end": "3792330"
  },
  {
    "text": "So these are the parameters. And then after I have\nthese parameters, what I'm",
    "start": "3792330",
    "end": "3799569"
  },
  {
    "text": "going to do is I'm going to-- after I have these\nembeddings, I'm going to put these embeddings\ninto a gigantic neural network",
    "start": "3799570",
    "end": "3808430"
  },
  {
    "text": "and let the neural\nnetwork to output on the conditional\nprobability in some sense. So, basically,\nroughly speaking here,",
    "start": "3808430",
    "end": "3815579"
  },
  {
    "text": "you're going to have some model,\nwhich I view as a black box.",
    "start": "3815579",
    "end": "3821440"
  },
  {
    "text": "These days people call\nthis-- use something called transformer. I'm not going to tell you\nexactly what transformer does",
    "start": "3821440",
    "end": "3828359"
  },
  {
    "text": "because it's actually\npretty complicated and it's out of the\nscope of this course.",
    "start": "3828359",
    "end": "3833500"
  },
  {
    "text": "You can take a look\nat a paper or take some other advanced courses. But for the first-order\nconcept, this is a black box.",
    "start": "3833500",
    "end": "3842539"
  },
  {
    "text": "Actually, many, many\npeople don't have to-- many people who\nuse the transformer don't have to open\nup the black box.",
    "start": "3842539",
    "end": "3849299"
  },
  {
    "text": "So that's why I'm only\ntelling you what's the input and output of this black box.",
    "start": "3849299",
    "end": "3854640"
  },
  {
    "text": "But this is a neural network. This is a complex\nneural network. And the way to use this\nblack box is the following.",
    "start": "3854640",
    "end": "3861040"
  },
  {
    "text": "So you just say, I have some\nsequence of words, x1 up to xt.",
    "start": "3861040",
    "end": "3867359"
  },
  {
    "text": "I'm going to first encode\nthem by the word embedding. I'm going to have e of x1\nup to e of xt, so e sub xt.",
    "start": "3867359",
    "end": "3878900"
  },
  {
    "text": "So these are vectors. And this transformer takes\nin the sequence of vectors",
    "start": "3878900",
    "end": "3885990"
  },
  {
    "text": "and then output you a\nsequence of vectors. So the output,\nlet's call them c1,",
    "start": "3885990",
    "end": "3894329"
  },
  {
    "text": "I think I call it c2,\nc3, up to Ct plus 1.",
    "start": "3894329",
    "end": "3902480"
  },
  {
    "text": "And let's also just give a\nname for this transformer. Let's call this\nfunction phi of theta.",
    "start": "3902480",
    "end": "3909829"
  },
  {
    "text": "So this function phi\nof theta, after given all of the input embeddings, it\noutputs a sequence of vectors.",
    "start": "3909829",
    "end": "3917410"
  },
  {
    "text": "Each of these vector, let's\nsay, is still of dimension d, even though this dimension\nd, in reality this d might",
    "start": "3917410",
    "end": "3923850"
  },
  {
    "text": "be the different d from this. But they are vectors. OK?",
    "start": "3923850",
    "end": "3929020"
  },
  {
    "text": "So after you get these\nvectors, then you can use these vectors to compute\nthe conditional probability.",
    "start": "3929020",
    "end": "3935599"
  },
  {
    "text": "You use these vectors to\npredict conditional probability. So, basically, you\nsay that I have this--",
    "start": "3935599",
    "end": "3944140"
  },
  {
    "text": "I'll use this, I guess.",
    "start": "3944140",
    "end": "3954039"
  },
  {
    "text": "So now after you get\nthese vectors c1 up to ct, so I'm going to use\nct to predict p of xt",
    "start": "3954040",
    "end": "3970240"
  },
  {
    "text": "given x1 up to xt minus 1.",
    "start": "3970240",
    "end": "3977950"
  },
  {
    "text": "So what I do here\nis the following. So what I want to predict\nis this thing, right?",
    "start": "3977950",
    "end": "3983089"
  },
  {
    "text": "So this is actual, this\nprobability distribution is, sometimes it's a vector.",
    "start": "3983089",
    "end": "3988349"
  },
  {
    "text": "It's a vector of dimension\nv because to describe this probability distribution,\nyou have to describe p of xt",
    "start": "3988349",
    "end": "3994328"
  },
  {
    "text": "is equal to 1, the first word,\ngiven x1 up to xt minus 1,",
    "start": "3994329",
    "end": "3999859"
  },
  {
    "text": "up to p of xt is equal to V\ngiven an x1 up to xt minus 1.",
    "start": "3999859",
    "end": "4008220"
  },
  {
    "text": "So to model this\nprobability, you have to model,\npredict V numbers. And this V numbers is\nsupposed to sum up to 1.",
    "start": "4008220",
    "end": "4017630"
  },
  {
    "text": "And how do you do this? This is kind of like a\nmulti class classification.",
    "start": "4017630",
    "end": "4023890"
  },
  {
    "text": "So what you do is\nbasically you say, I'm going to have a softmax of\nsome Wt times the vector ct.",
    "start": "4023890",
    "end": "4037119"
  },
  {
    "text": "So let me specify here. So ct is a vector\nof dimension d. And Wt is additional parameter.",
    "start": "4037119",
    "end": "4045048"
  },
  {
    "text": "This is a parameter\nthat you will learn.",
    "start": "4045049",
    "end": "4050740"
  },
  {
    "text": "So this Wt is of\ndimension v by d.",
    "start": "4050740",
    "end": "4056099"
  },
  {
    "text": "So, basically, Wt time\nCt will be dimension v.",
    "start": "4056099",
    "end": "4063460"
  },
  {
    "text": "So you have multiple-- so\nfor every possible choice, we get a vector. And they apply a softmax\nto make them probability.",
    "start": "4063460",
    "end": "4071130"
  },
  {
    "text": "So this whole thing\nwill be in dimension v. So, in some sense, these Wt\ntimes ct is just the logits.",
    "start": "4071130",
    "end": "4078789"
  },
  {
    "text": "And then you apply a softmax\nto turn them into probability. And that's your prediction\nfor this problem.",
    "start": "4078789",
    "end": "4085828"
  },
  {
    "text": "So in some sense, you just\nview each word as a class. You have v classes. And how do you predict\nsomething with--",
    "start": "4085829",
    "end": "4092820"
  },
  {
    "text": "how do you do a\nclassification with v classes? What you do is use\nfirst use a linear.",
    "start": "4092820",
    "end": "4098120"
  },
  {
    "text": "This is a linear hat\non top of the ct. And then you do a softmax to\nturn them into probability.",
    "start": "4098120",
    "end": "4106109"
  },
  {
    "text": "So I think the\ndefinition of softmax, I think we probably define this\nin one of the early lectures.",
    "start": "4106109",
    "end": "4112480"
  },
  {
    "text": "So let me just\ndefine it again, so--",
    "start": "4112480",
    "end": "4119000"
  },
  {
    "text": "I should use this, probably.",
    "start": "4119000",
    "end": "4126520"
  },
  {
    "text": "So softmax is just like, if\nyou have softmax of vector u",
    "start": "4126520",
    "end": "4133150"
  },
  {
    "text": "is really something like\nthis, exponential u1 over sum",
    "start": "4133150",
    "end": "4139730"
  },
  {
    "text": "of exponential ui, and\nthen exponential u sub",
    "start": "4139730",
    "end": "4145870"
  },
  {
    "text": "v over sum of exponential ui-- something like this.",
    "start": "4145870",
    "end": "4152920"
  },
  {
    "text": "So you turn the logits u\ninto a probability vector which sum up ",
    "start": "4152920",
    "end": "4170690"
  },
  {
    "text": "to 1-- any questions.? [INAUDIBLE]",
    "start": "4170690",
    "end": "4175850"
  },
  {
    "text": "So I'm going to use\nc2 to predict x2. I'm going to c3.",
    "start": "4175850",
    "end": "4181730"
  },
  {
    "text": "You can index I\nmean, I choose this is to indicate that c2 is\nused to predict x2, given x1.",
    "start": "4181730",
    "end": "4190500"
  },
  {
    "text": "And actually there's one\nthing I didn't tell you which is actually important. So when you predict\nthis probability,",
    "start": "4190500",
    "end": "4199850"
  },
  {
    "text": "this probability--\nthis is my model to predict the probability. I need to insist that I\nhaven't seen other words.",
    "start": "4199850",
    "end": "4209170"
  },
  {
    "text": "I have to insist that I only\nhave seen x1 up to xt minus 1. If I have seen xt\nalready, I can just",
    "start": "4209170",
    "end": "4215900"
  },
  {
    "text": "output the true value\nof xt I've seen. So actually this\ntransformer, there",
    "start": "4215900",
    "end": "4222720"
  },
  {
    "text": "are multiple versions\nof transformer. The transformer here\nI'm talking about, the term is called ultra\nregressive transformer.",
    "start": "4222720",
    "end": "4233420"
  },
  {
    "text": "This is just a name. What I mean is that you design\nthis transformer in a way",
    "start": "4233420",
    "end": "4240230"
  },
  {
    "text": "so that ct only depends\non x1 up to xt minus 1.",
    "start": "4240230",
    "end": "4254140"
  },
  {
    "text": "So you designed this\narchitecture such that you have this property,\nsuch that the ct only",
    "start": "4254140",
    "end": "4260110"
  },
  {
    "text": "depends on x1 up to xt minus",
    "start": "4260110",
    "end": "4265340"
  },
  {
    "text": "you are not going to be able to\nsee any other words after xt. So that's why you have\na proper definition",
    "start": "4265340",
    "end": "4271920"
  },
  {
    "text": "of the probabilistic model.",
    "start": "4271920",
    "end": "4277730"
  },
  {
    "text": "But this is like-- just think of this as given. The transformer's\ninternal property",
    "start": "4277730",
    "end": "4282790"
  },
  {
    "text": "ensures you to have this. How do you connect all of\nthe neurons [INAUDIBLE]",
    "start": "4282790",
    "end": "4292880"
  },
  {
    "text": "this property. [INAUDIBLE] So in many of the dimensions-- I call it d right now.",
    "start": "4292880",
    "end": "4299790"
  },
  {
    "text": "But the dimension is, it's just\na parameter you can change. You can change it to anything. Of course, it has to\nbe somewhat large,",
    "start": "4299790",
    "end": "4306880"
  },
  {
    "text": "something like maybe",
    "start": "4306880",
    "end": "4312870"
  },
  {
    "text": "I'm trying to\nunderstand, how does this relate to what we learned\nabout the one-hot vector?",
    "start": "4312870",
    "end": "4319350"
  },
  {
    "text": "Do we have to create\nthat many [INAUDIBLE]??",
    "start": "4319350",
    "end": "4326960"
  },
  {
    "text": "That's a good question. So if you use one hot\nvector, probably you should use embedding\nof dimension v because you have v choices.",
    "start": "4326960",
    "end": "4332730"
  },
  {
    "text": "But now I think the embedding\ndimension is often smaller than the vocabulary size. I think the vocabulary\npeople typically use",
    "start": "4332730",
    "end": "4338750"
  },
  {
    "text": "is something about something on that level.",
    "start": "4338750",
    "end": "4347020"
  },
  {
    "text": "But the dimension, the dimension\nis probably orthogonal to it.",
    "start": "4347020",
    "end": "4352100"
  },
  {
    "text": "[INAUDIBLE] Can you say it again? [INAUDIBLE] something\nlike like a hash function?",
    "start": "4352100",
    "end": "4360620"
  },
  {
    "text": "You mean the mapping\nfrom the i to the ei? It's not-- it's learned.",
    "start": "4360620",
    "end": "4370380"
  },
  {
    "text": "So basically you\njust learn this. So you have e1 up to ev.",
    "start": "4370380",
    "end": "4376420"
  },
  {
    "text": "Each word has a vector. And you concatenate all\nof them as a matrix. And you view these as\na parameter of your--",
    "start": "4376420",
    "end": "4383000"
  },
  {
    "text": "this is a part of the\nparameters of your training.",
    "start": "4383000",
    "end": "4391110"
  },
  {
    "text": "So now the final step is\njust that after you already defined the prediction, you\nhave to define a loss function",
    "start": "4391110",
    "end": "4397420"
  },
  {
    "text": "to learn the parameters. The parameters are--\nthe question was great. The parameters include e1\nup to ev, also include wt.",
    "start": "4397420",
    "end": "4407790"
  },
  {
    "text": "And that's it-- Also the parameters\nin the transformer, which I didn't specify.",
    "start": "4407790",
    "end": "4412989"
  },
  {
    "text": "That's a neural network which\nis viewed as a black box. And now you learned, you take\na loss function which contains",
    "start": "4412989",
    "end": "4421670"
  },
  {
    "text": "all the W'a, but which is a\nfunction of W's, the theta and the ei's the e, the e.",
    "start": "4421670",
    "end": "4429400"
  },
  {
    "text": "And then and what\nthis loss function is just the cross entropy\nloss of all the positions--",
    "start": "4429400",
    "end": "4436540"
  },
  {
    "text": "so cross entropy loss of all\npositions, of position i.",
    "start": "4436540",
    "end": "4453750"
  },
  {
    "text": "And if you really think about\nwhat's the cross entropy loss for the softmax,\nfor these kind of things,",
    "start": "4453750",
    "end": "4459719"
  },
  {
    "text": "it's really just this. But you don't necessarily\nhave to really exactly see",
    "start": "4459719",
    "end": "4465040"
  },
  {
    "text": "why it's this. But it's really just the minus\nlog probability of pt and xt",
    "start": "4465040",
    "end": "4475730"
  },
  {
    "text": "so if you call this thing pt. Suppose we call this pt, then\nbasically the xt-th entry minus",
    "start": "4475730",
    "end": "4486330"
  },
  {
    "text": "log p xt entry is-- this is the xt-th entry\nof that vector pt.",
    "start": "4486330",
    "end": "4492770"
  },
  {
    "text": "And this is just the\ncross entropy loss. But don't worry if you\ndon't get this line.",
    "start": "4492770",
    "end": "4500380"
  },
  {
    "text": "It's really just--\nempirically you just have to-- when you\nimplement this, you just have to call the cross\nentropy loss and give it to it.",
    "start": "4500380",
    "end": "4507909"
  },
  {
    "text": "We're running a\nlittle bit late--",
    "start": "4507909",
    "end": "4517120"
  },
  {
    "text": "any questions? So actually there's another\nsmall thing I didn't-- I cheated a little bit.",
    "start": "4517120",
    "end": "4524750"
  },
  {
    "text": "So here I'm taking-- so this definition, I\ncan only define h for t",
    "start": "4524750",
    "end": "4530370"
  },
  {
    "text": "is 2 in some sense. I don't have a\nprediction model for x1. I only have the\nprediction for x2",
    "start": "4530370",
    "end": "4537120"
  },
  {
    "text": "given an x1, x3 given x1, x2. I didn't have a\nprobabilistic model for x1. In practice, people\njust forgot about it.",
    "start": "4537120",
    "end": "4543780"
  },
  {
    "text": "Just don't use it. It doesn't matter that much. You can try to fix it\nto make it principled, but I don't think it\nmatters that much anyway.",
    "start": "4543780",
    "end": "4550600"
  },
  {
    "text": "So just because you have so\nmany probabilities to model,",
    "start": "4550600",
    "end": "4556950"
  },
  {
    "text": "and if you ignore one of\nthem, it's not a big deal.",
    "start": "4556950",
    "end": "4562100"
  },
  {
    "text": "So now let me talk\nabout how to adapt,",
    "start": "4562100",
    "end": "4568880"
  },
  {
    "text": "how to adapt this language\nmodel to downstream tasks.",
    "start": "4568880",
    "end": "4574000"
  },
  {
    "text": "So I've erased the fine\ntuning and linear probe. You can still do those.",
    "start": "4574000",
    "end": "4579670"
  },
  {
    "text": "Those are very general. You can use those fine\ntuning and linear probe for almost anything. And for this kind\nof language model,",
    "start": "4579670",
    "end": "4586540"
  },
  {
    "text": "the way you do fine\ntuning your probe is just the only thing\nyou have to decide",
    "start": "4586540",
    "end": "4594590"
  },
  {
    "text": "is that which output you should\nuse as the representation of these documents.",
    "start": "4594590",
    "end": "4600050"
  },
  {
    "text": "So you have so\nmany outputs here. And which one you take\nas the representation of this sentence?",
    "start": "4600050",
    "end": "4605639"
  },
  {
    "text": "So you just take c-- one\noption is you take ct as the representation.",
    "start": "4605639",
    "end": "4613719"
  },
  {
    "text": "And then you add some hat\nw and you w transpose. This is your prediction\nmodel for downstream tasks.",
    "start": "4613719",
    "end": "4626440"
  },
  {
    "text": "And then you can choose\nto only fine tune w, or you can choose to find\nyou w and the parameters",
    "start": "4626440",
    "end": "4632340"
  },
  {
    "text": "that you use to compute ct. The parameters\nused to compute ct are those parameters in the\ntransformers, those ei's,",
    "start": "4632340",
    "end": "4641580"
  },
  {
    "text": "those embeddings. So that's easy just\nbecause it's generic.",
    "start": "4641580",
    "end": "4647630"
  },
  {
    "text": "And for language models,\nthe interesting thing is that you can also do this\nwith some other ways, where",
    "start": "4647630",
    "end": "4655500"
  },
  {
    "text": "you can do adaptation\nwith other ways. So one way to do this is\nso-called zero shot learning.",
    "start": "4655500",
    "end": "4666599"
  },
  {
    "text": "And here it's just very easy. So basically, whatever\ntask you have, you just turn it into some\nquestions or some closed test,",
    "start": "4666600",
    "end": "4673070"
  },
  {
    "text": "like you have a\nblank to fill in. And then you just give it to\nthe model, and then the model",
    "start": "4673070",
    "end": "4680090"
  },
  {
    "text": "to generate the next word. So, basically,\nwhat I'm saying is that you just say, for example,\nsuppose you just turn--",
    "start": "4680090",
    "end": "4687190"
  },
  {
    "text": "you can have a x,\nwhich is just-- this model can take\nthe sequence for it.",
    "start": "4687190",
    "end": "4693460"
  },
  {
    "text": "You just say, maybe,\nis the speed of light--",
    "start": "4693460",
    "end": "4698710"
  },
  {
    "text": "suppose you care about the speed\nof light is a constant or not. And you just turn\nit into a question.",
    "start": "4698710",
    "end": "4705469"
  },
  {
    "text": "You call this, this is\nx1, x2, x3 up to xt.",
    "start": "4705469",
    "end": "4714429"
  },
  {
    "text": "And then, because this model\ncan generate-- maybe let's just say this is xt minus 1. And then you use this\nmodel to generate xt",
    "start": "4714429",
    "end": "4722679"
  },
  {
    "text": "because this model can do\nconditional generation. Given x1 up to xt minus",
    "start": "4722679",
    "end": "4728719"
  },
  {
    "text": "And given x1 up to xt, you\ncan generate xt plus 1. You can just keep generating\nthe tokens afterwards.",
    "start": "4728719",
    "end": "4735470"
  },
  {
    "text": "And you just put this into\nthe model and let it generate. And if you generate\nthe next word",
    "start": "4735470",
    "end": "4740489"
  },
  {
    "text": "is no or yes, then\nyou get the answer. And maybe sometimes\nit generates something slightly different from just\nyes or no, then you have",
    "start": "4740489",
    "end": "4746900"
  },
  {
    "text": "to pass the answer in some way. But you just let the model\nto generate the answer. That's one way.",
    "start": "4746900",
    "end": "4753980"
  },
  {
    "text": "And so basically you are\nusing the generation power of this model. That's probably the\nimportant thing, right, because giving this\nmodel-- so basically here",
    "start": "4753980",
    "end": "4759980"
  },
  {
    "text": "the important thing is that,\nthe way you have this model makes it that given\nsome sequence of words,",
    "start": "4759980",
    "end": "4766699"
  },
  {
    "text": "you can generate the next token. And then you can feedback\nthis new token to the system to generate another one.",
    "start": "4766700",
    "end": "4772980"
  },
  {
    "text": "And you can keep generate. So that's why this\ngives us opportunities to have other adaptation method\nwhich based on generation.",
    "start": "4772980",
    "end": "4781400"
  },
  {
    "text": "And another even more\nintriguing way to adapt is the following, is\ncalled in-context learning.",
    "start": "4781400",
    "end": "4792770"
  },
  {
    "text": "So the in-context\nlearning, so here you are dealing with the\nso-called few shot cases.",
    "start": "4792770",
    "end": "4803449"
  },
  {
    "text": "So you have a few example. So suppose you-- and let me\njust give an example to--",
    "start": "4803449",
    "end": "4808719"
  },
  {
    "text": "there are so many flexibilities. I'll just show one example. So what you do is you\nconcatenate all your examples",
    "start": "4808719",
    "end": "4814699"
  },
  {
    "text": "into a so-called prompt-- concatenate examples into\na, in some sense into--",
    "start": "4814699",
    "end": "4825070"
  },
  {
    "text": "actually, in the\nlanguage of this lecture, you call it document. But if you look at the\npaper, it's called prompt.",
    "start": "4825070",
    "end": "4833730"
  },
  {
    "text": "So what I mean, it just means\nyou concatenate all of this into a sequence. For example, suppose\nyou care about learning",
    "start": "4833730",
    "end": "4840350"
  },
  {
    "text": "how to add up two numbers. Suppose you have a question\nQ, which is something--",
    "start": "4840350",
    "end": "4851270"
  },
  {
    "text": "And then you do have-- you have some examples, right? You know that this is 5.",
    "start": "4851270",
    "end": "4857639"
  },
  {
    "text": "So this is your x task 1. And this is your y task 1.",
    "start": "4857640",
    "end": "4865719"
  },
  {
    "text": "So you just concatenate them\ninto a sequence of tokens, so like this. And then you just\nkeep concatenating.",
    "start": "4865719",
    "end": "4872170"
  },
  {
    "text": "And you say Q 6 plus 7-- no, plus. I choose to not use plus because\nI want to make it difficult,",
    "start": "4872170",
    "end": "4881130"
  },
  {
    "text": "so something like this. You are trying to learn\nwhat this symbol means. And then this is x task 2 and y.",
    "start": "4881130",
    "end": "4889350"
  },
  {
    "text": "And then you concatenate\ny task 2 here. So you say answer is 13. And now suppose you\njust have two examples.",
    "start": "4889350",
    "end": "4896440"
  },
  {
    "text": "And you want to learn. Then now your question\nis 15, this symbol, 2,",
    "start": "4896440",
    "end": "4904640"
  },
  {
    "text": "is equals to what? So you concatenate\nall of this together. And you call this x1 up to xt.",
    "start": "4904640",
    "end": "4910550"
  },
  {
    "text": "And you give this x1 up to xt\nthe sequence of all the symbols to the model and\nlet it generate.",
    "start": "4910550",
    "end": "4916940"
  },
  {
    "text": "And then you ask this sequence\nto generate xt plus 1,",
    "start": "4916940",
    "end": "4929600"
  },
  {
    "text": "so use these to generate xt\nplus 1, or maybe even xt plus 2,",
    "start": "4929600",
    "end": "4935640"
  },
  {
    "text": "so and so forth. And it turns out that if you\ngive these things to the model, and they will generate\nsomething reasonable for you.",
    "start": "4935640",
    "end": "4942170"
  },
  {
    "text": "So we do generate something. For example, for this case,\nit will generate A, 17.",
    "start": "4942170",
    "end": "4949489"
  },
  {
    "text": "And then you got answer. The answer is 17. And you see that so\nthis in some sense",
    "start": "4949489",
    "end": "4955080"
  },
  {
    "text": "says that you learn\nthe downstream tasks, you'll learn that the symbol\nmeans addition from this.",
    "start": "4955080",
    "end": "4963040"
  },
  {
    "text": "I'm not sure whether this\nis a little bit abstract. But I think what I can\ndo is the following. So I think this is about time.",
    "start": "4963040",
    "end": "4969250"
  },
  {
    "text": "And we can stop-- we can stop the class. But I can show you some\nexamples, just live.",
    "start": "4969250",
    "end": "4976710"
  }
]