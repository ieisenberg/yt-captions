[
  {
    "text": "Thank you for the introduction. I hope that I'm loud enough.",
    "start": "11640",
    "end": "16960"
  },
  {
    "text": "I'm usually a loud speaker,\nso I think either way, it should work. So like I mentioned, I\nactually work on vision before.",
    "start": "16960",
    "end": "24929"
  },
  {
    "text": "And one of the things\nI focus a lot in my PhD is actually video understanding.",
    "start": "24930",
    "end": "30630"
  },
  {
    "text": "And it has become a little bit\nboring in the end of my PhD on video stuff because it's\nall video action recognition.",
    "start": "30630",
    "end": "39220"
  },
  {
    "text": "And then I got to\nthink more about how is video going to be useful? And that actually leads to--",
    "start": "39220",
    "end": "45930"
  },
  {
    "text": "motivate me to do\nrobotics because, I think, I see a lot of humans\ndoing things in videos.",
    "start": "45930",
    "end": "53100"
  },
  {
    "text": "I think, that would be a good\nsource of demonstrations. So learning from video is\nactually my motivations",
    "start": "53100",
    "end": "59910"
  },
  {
    "text": "to come into robotics. So aside of that, but a\nlittle bit-- this is my house.",
    "start": "59910",
    "end": "69730"
  },
  {
    "text": "So it has been better. This is actually around one year\nago when my daughter is two.",
    "start": "69730",
    "end": "76190"
  },
  {
    "text": "She's not picking up things. But now she's better. But essentially, this is\na source of motivation",
    "start": "76190",
    "end": "84730"
  },
  {
    "text": "for me to do research\nthat this seen motivates me to shoot a demo on this one.",
    "start": "84730",
    "end": "91570"
  },
  {
    "text": "Basically, we have this\nB1 robot plus a Z1 arm that we are doing some\npick and place in here.",
    "start": "91570",
    "end": "99219"
  },
  {
    "text": "So I call this toy\nexample because really, A is picking up some toys.",
    "start": "99220",
    "end": "104380"
  },
  {
    "text": "B is not really\nthat realistic yet. So the real thing is\nmuch, much more complex.",
    "start": "104380",
    "end": "111580"
  },
  {
    "text": "So my daughter is\nwatching Peppa Pig there. So at that age is still very\nattractive to television,",
    "start": "111580",
    "end": "118210"
  },
  {
    "text": "now is, but less. So when we are shooting\nthese demos, what",
    "start": "118210",
    "end": "123790"
  },
  {
    "text": "I feel like having\nthis robot at home, my feeling is actually the\nrobot is a little bit too large.",
    "start": "123790",
    "end": "131590"
  },
  {
    "text": "So even for house\nin California it's still too large to walk into.",
    "start": "131590",
    "end": "136840"
  },
  {
    "text": "So I also give a talk\nin New York City. And obviously, that's impossible\nto put that robot there.",
    "start": "136840",
    "end": "142540"
  },
  {
    "text": "So you want to-- what\ndo you want to do? So you can buy a larger\nhouse, which is-- actually,",
    "start": "142540",
    "end": "148660"
  },
  {
    "text": "it was a good investment. I think as the interest going\ndown road it's-- maybe sooner,",
    "start": "148660",
    "end": "154070"
  },
  {
    "text": "it's also good to\nbuy a new house. But it's not very scalable. So that's kind of motivated\nme a bit on this--",
    "start": "154070",
    "end": "163299"
  },
  {
    "text": "working on humanoid robots. So essentially, you can\ntry to screen your robot,",
    "start": "163300",
    "end": "169400"
  },
  {
    "text": "make it smaller. And then it can work\nin a narrow space. I use this figure\nwith the pictures",
    "start": "169400",
    "end": "175780"
  },
  {
    "text": "to just show you that\nthe humanoid can be slim. And then every time--",
    "start": "175780",
    "end": "181490"
  },
  {
    "text": "if you think about--\nthey talk about whether we should do humanoid. There's always discussions on\nwhether I should do wheelbase",
    "start": "181490",
    "end": "188530"
  },
  {
    "text": "or whether I should use legs\nas a mobile manipulator. So my thinking is basically--",
    "start": "188530",
    "end": "197040"
  },
  {
    "text": "so bipedal walking\nis actually a space that is almost towards working.",
    "start": "197040",
    "end": "202120"
  },
  {
    "text": "And from all the experiments\nwe have been doing, I do see that it's in\nthe good time to work on.",
    "start": "202120",
    "end": "211060"
  },
  {
    "text": "It's a good place\nto do research. It's going to be useful. And then I would\nforesee, after one year,",
    "start": "211060",
    "end": "216390"
  },
  {
    "text": "no one will be\nquestioning whether you should use wheels or legs. I think you will work.",
    "start": "216390",
    "end": "223319"
  },
  {
    "text": "On the second, for the\npractical perspective, we recently got the\nrobot from Harry,",
    "start": "223320",
    "end": "229590"
  },
  {
    "text": "one of the postdoc who\nwork with [? Zhaojun. ?] So he has a startup now\nmaking this wheel-based robot",
    "start": "229590",
    "end": "235440"
  },
  {
    "text": "with arms. It's called Galaxy AI. So we bought the\nwheel-based robot,",
    "start": "235440",
    "end": "241049"
  },
  {
    "text": "and it is surprisingly huge. The base is very, very large.",
    "start": "241050",
    "end": "246810"
  },
  {
    "text": "So it's not smaller than\nthe quarterback we have. So that made me feel\nlike, yeah, it's actually",
    "start": "246810",
    "end": "253330"
  },
  {
    "text": "if you want to do wheelbase and\nyou need to make it balanced, and then you actually\ndo a lot of things and then maintain balance,\nyou need to have a large base.",
    "start": "253330",
    "end": "261040"
  },
  {
    "text": "So on the other\nhand, you have a leg. So that can actually be more\nagile and adjust balance easier.",
    "start": "261040",
    "end": "267760"
  },
  {
    "text": "So all these things\nmake me feel-- convincing me more\nand more to work on--",
    "start": "267760",
    "end": "275350"
  },
  {
    "text": "So I'm also a purely believer\nin data driven approach.",
    "start": "275350",
    "end": "280490"
  },
  {
    "text": "So a lot of things I do is-- like on this humanoid\nspace, I'm only",
    "start": "280490",
    "end": "285610"
  },
  {
    "text": "relying on data-driven approach. So there is a lot\nof discussions. Basically, you should\nuse simulation Sim2Real.",
    "start": "285610",
    "end": "294160"
  },
  {
    "text": "Or you should collect\nreal-world data. Or you should study\nfrom human data and all these different ways to\nacquire data to do the training.",
    "start": "294160",
    "end": "302740"
  },
  {
    "text": "So here is my thinking process. I would like to go for it. And then that leads\nto what I believe",
    "start": "302740",
    "end": "308650"
  },
  {
    "text": "will be the right way\nhow to use this data. So actually, in the\npast three, four years,",
    "start": "308650",
    "end": "315470"
  },
  {
    "text": "I worked on a lot of\nsimulation and Sim2Real. So one of the projects that we\ndid is actually [INAUDIBLE].",
    "start": "315470",
    "end": "322480"
  },
  {
    "text": "So you train a [INAUDIBLE]\nreinforcement learning in sim. And then the [INAUDIBLE]\nrepresentation transfer",
    "start": "322480",
    "end": "327685"
  },
  {
    "text": "quite well into [? Real. ?] And then you do\nsome door opening. And then we have a work\non, last year also,",
    "start": "327685",
    "end": "333070"
  },
  {
    "text": "combining sim and real data that\nyou do teleop in sim and real and combine the\ndata to do things.",
    "start": "333070",
    "end": "338190"
  },
  {
    "text": "So the thing I observe\nfrom simulation is that you need to make the\ntask simple, the environment",
    "start": "338190",
    "end": "347050"
  },
  {
    "text": "simple, so you can\nalign sim and real so that sim can transfer to real. So I have been\nstruggling on this a lot",
    "start": "347050",
    "end": "354190"
  },
  {
    "text": "because all people are\ndoing very cool demo. When I do Sim2Real, I can only\ndo this kind of-- yeah, it's OK.",
    "start": "354190",
    "end": "362134"
  },
  {
    "text": "It's doing something. But nobody cares\nkind of manipulation. So I have been struggling a lot\nin simulation real transfer.",
    "start": "362135",
    "end": "370430"
  },
  {
    "text": "You always makes\nthe task simple. So now, when it comes\nto the real robots--",
    "start": "370430",
    "end": "375650"
  },
  {
    "text": "so we have very cool people\nhere doing very cool things. So when this demo out,\neveryone knows about it,",
    "start": "375650",
    "end": "381250"
  },
  {
    "text": "and then like, oh, is so\ncool and this and that. So that made me curious about\nthe teleoperation space as well.",
    "start": "381250",
    "end": "389200"
  },
  {
    "text": "So we did a few works\nin that direction. Why it is very cool--",
    "start": "389200",
    "end": "394690"
  },
  {
    "text": "on the process, we do realize\nthat it's very expensive to collect this real data.",
    "start": "394690",
    "end": "400240"
  },
  {
    "text": "Apparently, right? So is a multi-million\ndollar business now. So real data is\ngood but expensive.",
    "start": "400240",
    "end": "409190"
  },
  {
    "text": "And sim data is more expensive. But this is very hard\nto do complex tasks.",
    "start": "409190",
    "end": "415860"
  },
  {
    "text": "So that is the\nreality we are facing. So when we try to think about\nwhat will be the next step, so",
    "start": "415860",
    "end": "425600"
  },
  {
    "text": "some work we have done before is\ntrying to look into human video. You are supposed to have\nrich, very rich, context,",
    "start": "425600",
    "end": "432900"
  },
  {
    "text": "human doing many things\nin different places. You have rich contexts. And then you're\nsupposed to can get",
    "start": "432900",
    "end": "438979"
  },
  {
    "text": "a lot of complex manipulation\nand real data from-- if you can learn from human. But that is just also a\nvery ideal picture, right?",
    "start": "438980",
    "end": "446670"
  },
  {
    "text": "So when we try to apply vision\nto 3D pose estimation in videos,",
    "start": "446670",
    "end": "452010"
  },
  {
    "text": "it doesn't really\nwork very well. So we published\na paper about AI. It's-- OK, it's doing\nsome 3D-pose estimation,",
    "start": "452010",
    "end": "459310"
  },
  {
    "text": "is providing some demonstration,\nbut it's not really that scaling up because vision is\nnot quite working.",
    "start": "459310",
    "end": "466380"
  },
  {
    "text": "But what happens is recently\nthat we do see a lot of cool VR",
    "start": "466380",
    "end": "472260"
  },
  {
    "text": "glasses and AR devices that\nsomehow provides quite good estimations on 3D domain.",
    "start": "472260",
    "end": "479400"
  },
  {
    "text": "So this is an example of how 3D\ndata set from the ARIA glass. It is getting quite accurate 3D.",
    "start": "479400",
    "end": "486129"
  },
  {
    "text": "But at the same time, you\nsee the white dots in here is actually using\nmotion capture. So it's not quite there yet.",
    "start": "486130",
    "end": "492220"
  },
  {
    "text": "But it gets reasonable good\n3D annotations to give you the data.",
    "start": "492220",
    "end": "497350"
  },
  {
    "text": "So I think it is reasonable to\nexplore this idea, this domain. Now we have better\nvision, a better ground",
    "start": "497350",
    "end": "504190"
  },
  {
    "text": "truth on human data. But what strikes me-- what is the turning\npoint for me is really--",
    "start": "504190",
    "end": "511990"
  },
  {
    "text": "is before this summer,\nwe have this work called Spatial Region GPT.",
    "start": "511990",
    "end": "517150"
  },
  {
    "text": "That's what makes me feel that\nI should invest in this domain a little bit more,\nis that we can now",
    "start": "517150",
    "end": "524020"
  },
  {
    "text": "actually-- if you're interested,\nyou can check out the paper. But now, we basically enable\nthe vision language model",
    "start": "524020",
    "end": "529540"
  },
  {
    "text": "to do very detailed\nmeasurement reasoning. So you see, you asked the\nquestion that whether you can",
    "start": "529540",
    "end": "536140"
  },
  {
    "text": "ride a motorcycle with 36-inch\nwide going through these two cabs? And then the model is able\nto reason that, oh, there's",
    "start": "536140",
    "end": "543400"
  },
  {
    "text": "actually-- how much distance\nbetween them so my bicycle can go through it.",
    "start": "543400",
    "end": "548950"
  },
  {
    "text": "So this work actually\ngive me a feeling that because it's not\njust about measuring,",
    "start": "548950",
    "end": "554569"
  },
  {
    "text": "it's not just estimating. It's also about reasoning\nthe metrics you measure.",
    "start": "554570",
    "end": "559790"
  },
  {
    "text": "So because it's how wide it\nis, so you can go through this. So you can use the knowledge\nof spatial to do something--",
    "start": "559790",
    "end": "568510"
  },
  {
    "text": "spatial relation\nto do something. So that makes me feel that can\nwe actually use these models?",
    "start": "568510",
    "end": "574880"
  },
  {
    "text": "Because once you\nhave understanding of distance and reasoning\non the distance, so can you actually use\nit to predict action?",
    "start": "574880",
    "end": "581449"
  },
  {
    "text": "So that makes feel\nthat it's doable. I mean, there is already\na lot of [? VR ?] models",
    "start": "581450",
    "end": "586960"
  },
  {
    "text": "before this and use predict\nrobot doing this and that. But you are always training\non a specific scale",
    "start": "586960",
    "end": "593530"
  },
  {
    "text": "of human-robot data. You're not sure whether it's\nactually fitting to the data",
    "start": "593530",
    "end": "598750"
  },
  {
    "text": "or it's actually doing\nspatial reasoning. But from this work,\nI start thinking-- become more\nconvinced that we can",
    "start": "598750",
    "end": "604750"
  },
  {
    "text": "use VLM to do spatial\nreasoning and not just semantic level, but\nvery detailed distance",
    "start": "604750",
    "end": "610509"
  },
  {
    "text": "and measuring level. So that leads to [INAUDIBLE]. It's a pretty long\nthinking process.",
    "start": "610510",
    "end": "616820"
  },
  {
    "text": "But that leads to the framework\nI'm recently more interested in. It's, basically, can we train\na vision language model, action",
    "start": "616820",
    "end": "624760"
  },
  {
    "text": "model, on human data? So you can predict how to\nplan where the human goes",
    "start": "624760",
    "end": "632820"
  },
  {
    "text": "and then how to manipulate\nthings in human action space. So it can be mid-level action.",
    "start": "632820",
    "end": "639550"
  },
  {
    "text": "So you cannot be\njust very semantics. So it needs to be some detailed\ndescription on trajectory level.",
    "start": "639550",
    "end": "646750"
  },
  {
    "text": "And then on the\nlow-level side, you can use teleoperation\nor Sim2Real to basically figure out the\nexact context and navigation.",
    "start": "646750",
    "end": "657089"
  },
  {
    "text": "So all these things\nare-- actually, in some sense it's\nmodeling human. We try to modeling\nhow human plants train",
    "start": "657090",
    "end": "663690"
  },
  {
    "text": "in [? VR8. ?] And then\nfrom low-level perspective, we do teleoperation for\nmanipulations and also Sim2Real",
    "start": "663690",
    "end": "672360"
  },
  {
    "text": "for robotic control\nby imitating human. So there are different\nlevels of modeling human.",
    "start": "672360",
    "end": "677560"
  },
  {
    "text": "But, I think, I now believe\nmore in these two layers structure model.",
    "start": "677560",
    "end": "686470"
  },
  {
    "text": "So I will go through a\ndifferent aspect of this model that hopefully, in the\nend, I can convince you",
    "start": "686470",
    "end": "694139"
  },
  {
    "text": "this is a plausible way. So first going for-- I would first go\nfrom bottom to top.",
    "start": "694140",
    "end": "701310"
  },
  {
    "text": "So maybe getting you to\nknow more about low level is more convincing as this\nis-- in the end of the day,",
    "start": "701310",
    "end": "707410"
  },
  {
    "text": "this is a robotic similar. It's not just vision language. So we have this Open-TeleVision\nwork that basically-- it's",
    "start": "707410",
    "end": "716670"
  },
  {
    "text": "just a teleoperation system. But I think the\ncool thing in here is really about the vision part.",
    "start": "716670",
    "end": "722220"
  },
  {
    "text": "So we have a very cute\ndesigns on the head. So this is a [? H1 ?] robot.",
    "start": "722220",
    "end": "727920"
  },
  {
    "text": "But we design the new\nleg that you basically move as your head in\nthe VR glass move.",
    "start": "727920",
    "end": "734440"
  },
  {
    "text": "And also, it\nbasically streams what the robot see in this\n[INAUDIBLE] camera,",
    "start": "734440",
    "end": "740620"
  },
  {
    "text": "in this stereo camera, streaming\nthe vision in real time",
    "start": "740620",
    "end": "745650"
  },
  {
    "text": "to your VR glasses. This is what the VR\nglasses is seeing. So the student sees\nwhat the robot sees.",
    "start": "745650",
    "end": "754560"
  },
  {
    "text": "And then basically, you can do\nquite real time teleop control. So you are streaming\nvision to your VR class.",
    "start": "754560",
    "end": "762060"
  },
  {
    "text": "And then your VR class\nestimate hand pose. And hand pose to retargeting\nto control the robot",
    "start": "762060",
    "end": "767190"
  },
  {
    "text": "hand to do things with\nrelatively small latency. Quite real time to\ndo all these things.",
    "start": "767190",
    "end": "774000"
  },
  {
    "text": "And the streaming is very useful\nin the sense that one aspect.",
    "start": "774000",
    "end": "779650"
  },
  {
    "text": "So this is another example. We show that the tele\noperator can actually",
    "start": "779650",
    "end": "785310"
  },
  {
    "text": "operate the robot remotely. So this guy [? Ga, ?]\nwho is in job market.",
    "start": "785310",
    "end": "791100"
  },
  {
    "text": "So he is operating\nthe robot from MIT to control the\nrobot in San Diego.",
    "start": "791100",
    "end": "796350"
  },
  {
    "text": "So you stream your vision\nacross the country. And then you stream\nback your pose across the country\nto control the robot.",
    "start": "796350",
    "end": "803410"
  },
  {
    "text": "So you can do very,\nvery remote control by doing this\nvision based thing.",
    "start": "803410",
    "end": "809260"
  },
  {
    "text": "So these kinds of things-- so we basically\ncollect some data. And then train policy.",
    "start": "809260",
    "end": "816040"
  },
  {
    "text": "But this paper has\nnot focusing too much on the imitation learning\npart, is really overfitting.",
    "start": "816040",
    "end": "821390"
  },
  {
    "text": "We are collecting data for\nthis particular container, particular can.",
    "start": "821390",
    "end": "826600"
  },
  {
    "text": "It's not generalizing\njust a disclaimer. But the cool thing is that it's\nactually continuously doing",
    "start": "826600",
    "end": "832860"
  },
  {
    "text": "things. So it's robot's in\nthe particular case. And then you can keep\ndoing the things. It's not just one time.",
    "start": "832860",
    "end": "838690"
  },
  {
    "text": "But a little bit long horizon\nkeep doing the same thing. And I like this a lot from\nthe egocentric perspective",
    "start": "838690",
    "end": "847720"
  },
  {
    "text": "because A, is a active camera. So the policy actually predicts\nnot only the arms and hands.",
    "start": "847720",
    "end": "854780"
  },
  {
    "text": "But also predicts the\nhead, how the head moves. So it's a active vision.",
    "start": "854780",
    "end": "860710"
  },
  {
    "text": "And also this makes you\nthink a lot on relate",
    "start": "860710",
    "end": "866140"
  },
  {
    "text": "to all these egocentric\nvideo I used to work on. So this is pretty much how ego\n[? 4D ?] and how epic kitchen",
    "start": "866140",
    "end": "874070"
  },
  {
    "text": "video looks like. But instead of human\nhand it is a robot hands. So I like that aspect a lot.",
    "start": "874070",
    "end": "880250"
  },
  {
    "text": "And then I want to connect\nit to later of the talk. But this is the part I really\nlike the active moving camera.",
    "start": "880250",
    "end": "888350"
  },
  {
    "text": "And the manipulation\nwith the active camera. And we can folding because\na lot of people are folding.",
    "start": "888350",
    "end": "896430"
  },
  {
    "text": "And but just on a particular\ntower again, but keep folding.",
    "start": "896430",
    "end": "903440"
  },
  {
    "text": "And we can do folding\nand then we can-- so this is teleop results. So people ask how much the\ninspired hand can work?",
    "start": "903440",
    "end": "911270"
  },
  {
    "text": "So it's not really\nthat good the hands. But then it still can do\nreasonably dexterous task",
    "start": "911270",
    "end": "917060"
  },
  {
    "text": "with this egocentric vision. So we have developed in parallel\nto other teleoperation platform.",
    "start": "917060",
    "end": "924319"
  },
  {
    "text": "And they asked people there\nto try to do this task. They are actually not able\nto do it because when--",
    "start": "924320",
    "end": "933140"
  },
  {
    "text": "in the other platforms, when\nwe are doing teleoperation, we are actually seeing it\nin the third person view.",
    "start": "933140",
    "end": "938700"
  },
  {
    "text": "And then this task\nis only doable when you have this\nfirst person egocentric view to conduct this task.",
    "start": "938700",
    "end": "945950"
  },
  {
    "text": "So the egocentric\nvisual feedback is quite important to conduct\nmuch more dexterous task.",
    "start": "945950",
    "end": "951629"
  },
  {
    "text": "So is one is remote\ncontrol, the other is this more like dexterous\ntask that is conducting.",
    "start": "951630",
    "end": "960589"
  },
  {
    "text": "And then I show this\none because it's-- so when you do 60\npost estimation. Every one do a drill\n60 post estimation.",
    "start": "960590",
    "end": "969960"
  },
  {
    "text": "But no one actually use it. So we figure maybe we\nuse it to do something. So this is drilling the wood.",
    "start": "969960",
    "end": "979520"
  },
  {
    "text": "And then to just show you how\nmuch precision you can reach. So this is also a little\nbit more precise control.",
    "start": "979520",
    "end": "986610"
  },
  {
    "text": "It's not doing the\nexact right thing, because you are not\nsupposed to touch. But that's the extreme\nwe can do for this.",
    "start": "986610",
    "end": "996830"
  },
  {
    "text": "So this is a teleop thing. So short conclusion\nis basically,",
    "start": "996830",
    "end": "1001990"
  },
  {
    "text": "egocentric vision is\ngood for dexterous task, and then also remote control.",
    "start": "1001990",
    "end": "1007300"
  },
  {
    "text": "So we enable this. And now we basically\nthink, can we act the robot to the\nwhole body control on it.",
    "start": "1007300",
    "end": "1014290"
  },
  {
    "text": "So this leads to an extension\nof the current projects that we basically also add the\nseem to real walking skills",
    "start": "1014290",
    "end": "1021280"
  },
  {
    "text": "into it. So now the humanoid\nrobot can walk and then also manipulate objects.",
    "start": "1021280",
    "end": "1027069"
  },
  {
    "text": "So this work is a little bit of\nengineering work, I would say, because it's not exactly\nthe whole body control.",
    "start": "1027069",
    "end": "1034400"
  },
  {
    "text": "We do separate the upper\nbody and lower body control. The upper body is going to use\nIK and retargeting to do teleop.",
    "start": "1034400",
    "end": "1042230"
  },
  {
    "text": "The lower body is\nusing reinforcement learning to basically\ncontrol the walking policy.",
    "start": "1042230",
    "end": "1048730"
  },
  {
    "text": "So a new technique thing\nin here, which I'm not going to cover is\nbasically, how do you",
    "start": "1048730",
    "end": "1055510"
  },
  {
    "text": "do robot's walking as you\nare moving upper body a bit? So we are doing\nsome conditional--",
    "start": "1055510",
    "end": "1062080"
  },
  {
    "text": "have some conditional\nvariable for the RL. You train a VAE to\nencode upper body motion.",
    "start": "1062080",
    "end": "1067150"
  },
  {
    "text": "And then get an\nembedding out of it. And then use it as a condition\nfor your walking policy.",
    "start": "1067150",
    "end": "1073330"
  },
  {
    "text": "So that allows us to do\nrobot's walking while we are moving the upper body. So it's a temporal solution\nbecause the [? UHH1 ?] does not",
    "start": "1073330",
    "end": "1082210"
  },
  {
    "text": "really have a lot of\ndegree of freedom. It's really hard to do\nvery agile motion really.",
    "start": "1082210",
    "end": "1087890"
  },
  {
    "text": "So the temporal solution\nwe have is actually do some separations that we have\nrobot's walking and then also",
    "start": "1087890",
    "end": "1094240"
  },
  {
    "text": "upper body control. So here I just show you\nsome teleop results again.",
    "start": "1094240",
    "end": "1100120"
  },
  {
    "text": "So this is just a extension. But basically there's one guy\nis controlling the upper body",
    "start": "1100120",
    "end": "1107740"
  },
  {
    "text": "to do things. Then there's another\nguy using a joystick to control the walking part.",
    "start": "1107740",
    "end": "1112870"
  },
  {
    "text": "So this is pulling\npushing a wheelchair. So showing that it\nhas some capability.",
    "start": "1112870",
    "end": "1118160"
  },
  {
    "text": "And then this is basically,\nagain, doing some walking. Just pulling the cart.",
    "start": "1118160",
    "end": "1124570"
  },
  {
    "text": "So doing something. So it's the exact same thing. But now we have a\nwalking thing in here.",
    "start": "1124570",
    "end": "1131140"
  },
  {
    "text": "So I think we can see the\noperator later for this example. So this is in our cafeteria.",
    "start": "1131140",
    "end": "1136600"
  },
  {
    "text": "And then you basically\nwalk through there. So again, it's all teleop. It's not automate.",
    "start": "1136600",
    "end": "1142659"
  },
  {
    "text": "So you can put the things. Go there and then\nyou grasp the things.",
    "start": "1142660",
    "end": "1147760"
  },
  {
    "text": "So the camera is still\nan active vision, if you see that the\ncamera is actually moving.",
    "start": "1147760",
    "end": "1153309"
  },
  {
    "text": "So the guy sitting there is\nnot doing anything useful. But the guy hiding in\nhere is basically the guy",
    "start": "1153310",
    "end": "1160780"
  },
  {
    "text": "who is doing the teleop. So he's wearing the Vision Pro. And you can see his\nhand motion is aligned",
    "start": "1160780",
    "end": "1167500"
  },
  {
    "text": "with the robot's motions. And then there's\nanother guy that is not appearing\nin the video that",
    "start": "1167500",
    "end": "1174250"
  },
  {
    "text": "is using a joystick to control\nthe directions of where the robot moves. So that a sort of\nidea or extensions",
    "start": "1174250",
    "end": "1181690"
  },
  {
    "text": "of what we have at this point. And there's a lot\nof demonstrations.",
    "start": "1181690",
    "end": "1188070"
  },
  {
    "text": "Are there any questions? I saw-- Yes. Yes. I was wanting you to know,\njust by watching the robot, that it must be teleop.",
    "start": "1188070",
    "end": "1194390"
  },
  {
    "text": "Yeah. OK. I think there is\nsome hesitation.",
    "start": "1194390",
    "end": "1200590"
  },
  {
    "text": "Every now and then it goes. [LAUGHTER] Which if it\nwas a learned thing,",
    "start": "1200590",
    "end": "1206570"
  },
  {
    "text": "it wouldn't bother-- [INTERPOSING VOICES] Yeah. But you do feel I\nmean, it would be",
    "start": "1206570",
    "end": "1213340"
  },
  {
    "text": "nice to have this kind\nof behavior to learn. So this is actually showing\nwe can do standing as well.",
    "start": "1213340",
    "end": "1221080"
  },
  {
    "text": "So it takes us a long time\nto train the standing thing. But anyway, so this\nis doing some teleop.",
    "start": "1221080",
    "end": "1229090"
  },
  {
    "text": "So I think there's still-- because we have remote control. So we show remote control.",
    "start": "1229090",
    "end": "1235300"
  },
  {
    "text": "So a little bit acting here.",
    "start": "1235300",
    "end": "1237220"
  },
  {
    "text": "[LAUGHTER] A little bit acting. Not quite serious. We can go faster.",
    "start": "1241144",
    "end": "1246230"
  },
  {
    "text": "But we are worried about the\nhardware so we didn't go faster. We have faster videos.",
    "start": "1246230",
    "end": "1251540"
  },
  {
    "text": "But yeah. So it's robot's standing as\nwell, because this guy also",
    "start": "1251540",
    "end": "1258830"
  },
  {
    "text": "went to Michigan. At that point I\nwas saying, maybe we can do it from\nMichigan as well. So robot standing and\nthen some teleop remote.",
    "start": "1258830",
    "end": "1267600"
  },
  {
    "text": "That's the thing. So this control is still-- so we are still\nexploring different UIs.",
    "start": "1267600",
    "end": "1275540"
  },
  {
    "text": "So this is a lot of UI. So basically you\nhave a UMI, no IMU.",
    "start": "1275540",
    "end": "1281779"
  },
  {
    "text": "Sorry. [LAUGHTER] Overfitting\nto the IMU on the chair.",
    "start": "1281780",
    "end": "1292530"
  },
  {
    "text": "And then basically\nyou can rotate. And then your wrist\ncan move as well. So we are still exploring\ndifferent interfaces,",
    "start": "1292530",
    "end": "1299940"
  },
  {
    "text": "how to control the robot. Yeah, so the interface is\nsomething we are exploring.",
    "start": "1299940",
    "end": "1309650"
  },
  {
    "text": "Our earlier time we have\nthis work that is actually the same kind of controller.",
    "start": "1309650",
    "end": "1315510"
  },
  {
    "text": "But we have a older\nversion of the robot. The leg is more slim. And then we do a\nwhole body control",
    "start": "1315510",
    "end": "1321530"
  },
  {
    "text": "to imitate the human motion. So I'm just showing that we can\nactually walk around the city.",
    "start": "1321530",
    "end": "1328019"
  },
  {
    "text": "So I think the big\nplus of this work is we actually walk around\nthe city, is not in the lab.",
    "start": "1328020",
    "end": "1333320"
  },
  {
    "text": "And I think this is also\nbecause I'm giving a talk. So I show you the left part.",
    "start": "1333320",
    "end": "1338549"
  },
  {
    "text": "If you see it online,\nit's only this part. Yes. Can you explain\na little bit more",
    "start": "1338550",
    "end": "1344570"
  },
  {
    "text": "about the lookout\nfor the system? Is it purely vision based? Yeah, so the input\nis really just--",
    "start": "1344570",
    "end": "1352340"
  },
  {
    "text": "so in here there's no vision. So this is really just\ninput a reference motion.",
    "start": "1352340",
    "end": "1358220"
  },
  {
    "text": "There's no mocap in here. So our input only have\na reference motion and proprioception.",
    "start": "1358220",
    "end": "1364920"
  },
  {
    "text": "So there's no observation in\nhere in this particular case. Yes, teleop is the same\nof the open television.",
    "start": "1364920",
    "end": "1372049"
  },
  {
    "text": "You basically have the\nvisions with the z camera. Set mini cameras on top.",
    "start": "1372050",
    "end": "1377840"
  },
  {
    "text": "And then you basically\nsee what the robot sees. So that's the same. But this again, is only teleop.",
    "start": "1377840",
    "end": "1384009"
  },
  {
    "text": "So the low level locomotion\ndoes not have vision. So locomotion does\nnot have vision. Upper body teleop\nhas visual feedback.",
    "start": "1384010",
    "end": "1392786"
  },
  {
    "text": "Yes. What is your intuition for why\negocentric camera view really",
    "start": "1392786",
    "end": "1398860"
  },
  {
    "text": "helps with those more\nfine grained tasks? I think you need to\nsee it basically,",
    "start": "1398860",
    "end": "1406807"
  },
  {
    "text": "because when you are\nseeing the first person view, as the robot\nteleoperating I mean there's a lot of occlusion\nhappening, for example.",
    "start": "1406807",
    "end": "1414150"
  },
  {
    "text": "I mean, you don't see\nthe little earplug. And then you don't\nsee how the small box,",
    "start": "1414150",
    "end": "1420049"
  },
  {
    "text": "how to articulate the object,\nhow it can unfold and collapse. So that is actually very hard.",
    "start": "1420050",
    "end": "1425880"
  },
  {
    "text": "Maybe there's some occlusion. And also not very\nintuitive because I want",
    "start": "1425880",
    "end": "1431470"
  },
  {
    "text": "to operate things in this way. But I see things in this way. And then I need to\noperate in this way. So that's actually not very\nintuitive for the human operator",
    "start": "1431470",
    "end": "1440080"
  },
  {
    "text": "to do it, because you are not\noperating in your normal way",
    "start": "1440080",
    "end": "1446200"
  },
  {
    "text": "to operate. You are seeing other\nview to operate. And you internally, you need\nto do the transformation",
    "start": "1446200",
    "end": "1452020"
  },
  {
    "text": "in yourself to do it. Maybe you will train yourself\nenough, that can be done.",
    "start": "1452020",
    "end": "1457290"
  },
  {
    "text": "But [? EVO ?] essentially,\nwe will offer you the most intuitive. You feel like you are inside\nthe robot to do this thing.",
    "start": "1457290",
    "end": "1465390"
  },
  {
    "text": "Would you say that\nit's making up for the lack of haptic\nfeedback or the fact that your hand is [INAUDIBLE]\ndifferent than a robot?",
    "start": "1465390",
    "end": "1472622"
  },
  {
    "text": "Like sometimes-- Haptic feedback is\ndefinitely useful. So we did actually do some\nstudy on that as well.",
    "start": "1472623",
    "end": "1479180"
  },
  {
    "text": "For example, there's\na work I didn't show in here, where we try to\ndo this haptic feedback as well.",
    "start": "1479180",
    "end": "1487730"
  },
  {
    "text": "So you can have tactile\nsensor in the finger. And then you have haptic,\nsome motors on your fingers.",
    "start": "1487730",
    "end": "1494870"
  },
  {
    "text": "And then when you do teleop,\nyou can have haptic feedback. So that is helpful. Of course, it makes\nit much faster.",
    "start": "1494870",
    "end": "1502190"
  },
  {
    "text": "But again, even with that, we\nstill cannot do the picking up",
    "start": "1502190",
    "end": "1507250"
  },
  {
    "text": "the small things. Yes. I'm not so sure that [INAUDIBLE]",
    "start": "1507250",
    "end": "1512610"
  },
  {
    "text": "OK. So it said that it was able\nto do it decently well. And it was still touching\nthe side mission.",
    "start": "1512610",
    "end": "1519550"
  },
  {
    "text": "Do you think it's a\nlimitation of the hardware? Yeah, I think the fingers is\nvery limited thing you can do.",
    "start": "1519550",
    "end": "1527440"
  },
  {
    "text": "Is really one degree\nof freedom here. So it's very constrained. It's like a larger gripper\nor more robot's gripper",
    "start": "1527440",
    "end": "1534460"
  },
  {
    "text": "at this point. So I think yeah, there's a lot\nof constraints on the hardware here? Do you think it still\nworks scaling up this data,",
    "start": "1534460",
    "end": "1540410"
  },
  {
    "text": "even though it's not as good? You know it's going tobe useful\nonce you get to a better robot. Yes, but that will\nrelate to what approach",
    "start": "1540410",
    "end": "1548050"
  },
  {
    "text": "you are going to use to\nlearn the representation. I fundamentally think\nis possible to scale up",
    "start": "1548050",
    "end": "1555700"
  },
  {
    "text": "across embodiments. But in a reasonable\nrepresentation space, which I will introduce a bit later.",
    "start": "1555700",
    "end": "1562870"
  },
  {
    "text": "But let me finish\nthe first part. So I want to talk about--",
    "start": "1562870",
    "end": "1568120"
  },
  {
    "text": "I mean, I showed you\nthis video a little bit because I want to go to the\nlegs thing that we recently do.",
    "start": "1568120",
    "end": "1573850"
  },
  {
    "text": "That we don't separate upper\nbody and lower body control for body control. Now we have a smaller robot that\nbasically it's just easier--",
    "start": "1573850",
    "end": "1586360"
  },
  {
    "text": "makes our lives easier once\nyou have small robots that we actually can track whole body. So you input a reference motion.",
    "start": "1586360",
    "end": "1592120"
  },
  {
    "text": "And then you can do\nthe whole body tracking on this smaller robot. So we call it ExBody2, because\nthe previous is called ExBody.",
    "start": "1592120",
    "end": "1599950"
  },
  {
    "text": "And so you can do\nmotions like this. Is it playing?",
    "start": "1599950",
    "end": "1604990"
  },
  {
    "text": "OK. It makes sense to have some-- [MUSIC PLAYING]",
    "start": "1604990",
    "end": "1612470"
  },
  {
    "text": "So what happened is you have\nthe human emotion there. And then you try to input\nthat motion, a retargeting",
    "start": "1614880",
    "end": "1621240"
  },
  {
    "text": "motion of that motion. And then that is a\nsimulation policy. And that transfer the real. So it's actually quite a line\nthat the sim-to-real gap is not",
    "start": "1621240",
    "end": "1630640"
  },
  {
    "text": "that huge with\nthis smaller robot. So you have more\ndegree of freedom. And then just lighter\nand make it easier.",
    "start": "1630640",
    "end": "1638429"
  },
  {
    "text": "So you can see the sim-to-real\nmotion is quite a line here.",
    "start": "1638430",
    "end": "1643790"
  },
  {
    "text": "How high is this? 1.4 meters, this. Yeah.",
    "start": "1643790",
    "end": "1648865"
  },
  {
    "text": "Yeah, so we have studied\na lot of things in here. I think one thing is still--",
    "start": "1652880",
    "end": "1659780"
  },
  {
    "text": "I mean, even though\nit's better hardware, there's still limitations\nthat you cannot jump too high.",
    "start": "1659780",
    "end": "1665960"
  },
  {
    "text": "You cannot run too fast. You cannot do too crazy motions. So there's this limitations. So we talk a little bit\nabout the data set part.",
    "start": "1665960",
    "end": "1673640"
  },
  {
    "text": "But essentially the\nframework we are using is we actually\nspend a lot of time.",
    "start": "1673640",
    "end": "1680000"
  },
  {
    "text": "We want to do motion imitations. We spend a lot of\ntime to filter. To study what kind of\nmotions data set we can use.",
    "start": "1680000",
    "end": "1687140"
  },
  {
    "text": "So the answer is, it turn\nout you cannot be too dirty. Or you cannot be too small.",
    "start": "1687140",
    "end": "1692150"
  },
  {
    "text": "So we will talk about\nsome dataset perspective. How do we curate\nbetter data, and then",
    "start": "1692150",
    "end": "1698090"
  },
  {
    "text": "do human data set, and\nthen robot motion dataset. But the general framework falls\ninto teacher student policy.",
    "start": "1698090",
    "end": "1705299"
  },
  {
    "text": "Basically, you train a policy. Again, the input is\nyour kinematics motion.",
    "start": "1705300",
    "end": "1710870"
  },
  {
    "text": "And then your policy is\ntrying to track output. The joint action to try\nto track the input motion.",
    "start": "1710870",
    "end": "1717350"
  },
  {
    "text": "So you get robot can\ndo the same motion. So you have some\nprivileged information. The velocity, which is very\nimportant if you ever--",
    "start": "1717350",
    "end": "1727130"
  },
  {
    "text": "like there's a Disney paper\nthat do all very agile, whole body control as well.",
    "start": "1727130",
    "end": "1732330"
  },
  {
    "text": "In there, they actually\nhave a mocap system that capture the linear\nvelocity of each joint as inputs",
    "start": "1732330",
    "end": "1740330"
  },
  {
    "text": "for the whole body controller. So that was very important. But we try to make\nit more realistic.",
    "start": "1740330",
    "end": "1745680"
  },
  {
    "text": "So we throw it away. And then so in teachers\nstudent policy,",
    "start": "1745680",
    "end": "1751470"
  },
  {
    "text": "we just try to distill\nthe teacher and student. And they have partial\nobservation in the sense that we only observe\nthe proprioception.",
    "start": "1751470",
    "end": "1759830"
  },
  {
    "text": "And basically,\nreference motion and limited partial observations. And then we do the\nwhole body control.",
    "start": "1759830",
    "end": "1767940"
  },
  {
    "text": "So that leads to a\none important aspect, is that we spend a\nlot of time on what is",
    "start": "1767940",
    "end": "1773910"
  },
  {
    "text": "a good data set for this task. So again, we want the policy\nto track all the motions",
    "start": "1773910",
    "end": "1779190"
  },
  {
    "text": "that as input\ninstead of training like overfitting one\nparticular motion.",
    "start": "1779190",
    "end": "1784535"
  },
  {
    "text": "So first, obviously it's easy to\nhave the data set that basically",
    "start": "1788082",
    "end": "1793470"
  },
  {
    "text": "does not move that much. Just walking, normal\nwalking in different way. And then we create a\nlittle bit dataset.",
    "start": "1793470",
    "end": "1802300"
  },
  {
    "text": "So it's all from CMU\nmocap by the way. We just manually selecting\nthings out of it.",
    "start": "1802300",
    "end": "1808620"
  },
  {
    "text": "So we eventually find the\ndata set that is good for us. It's basically, you have a\nlot diverse upper body motion.",
    "start": "1808620",
    "end": "1817440"
  },
  {
    "text": "And then the walking is\nstill relatively stable. Not too crazy.",
    "start": "1817440",
    "end": "1822870"
  },
  {
    "text": "As so this is something we\ndidn't train on at this point. But maybe we should eventually.",
    "start": "1822870",
    "end": "1829120"
  },
  {
    "text": "But we only get the robot\nfor one month, by the way. So we didn't train on too much\ncrazy motions at this point.",
    "start": "1829120",
    "end": "1837600"
  },
  {
    "text": "So this is the\nright combinations of data we have been\nusing at this point.",
    "start": "1837600",
    "end": "1843640"
  },
  {
    "text": "And so I call different dataset\nD50 means a small simple motion.",
    "start": "1843640",
    "end": "1850840"
  },
  {
    "text": "D250 is have a little bit\nmore diverse upper body motion, but stable walking.",
    "start": "1850840",
    "end": "1857490"
  },
  {
    "text": "And then D500 is a\nlot different motions. And then the whole\nCMU mocap data set.",
    "start": "1857490",
    "end": "1863370"
  },
  {
    "text": "And then there is basically,\nsome comparison basically, saying that if you\ndo train on D50.",
    "start": "1863370",
    "end": "1869669"
  },
  {
    "text": "And then you evaluate on D50\nin distribution obviously, then you have the\nbest performance.",
    "start": "1869670",
    "end": "1876029"
  },
  {
    "text": "And then if you train\non D250, and then you evaluate on the\nwhole CMU dataset.",
    "start": "1876030",
    "end": "1882629"
  },
  {
    "text": "So actually larger dataset\nmake you generalize better to the whole CMU dataset. And then you try to evaluate\non a lot of datasets",
    "start": "1882630",
    "end": "1890820"
  },
  {
    "text": "that is not CMU mocap dataset. You find you still have D250\nthat middle sized data works",
    "start": "1890820",
    "end": "1897960"
  },
  {
    "text": "best. And then other more\nnot as clean dataset actually does not\ngeneralize as well",
    "start": "1897960",
    "end": "1904289"
  },
  {
    "text": "as the middle sized dataset. Yes. Yeah, So is it they have\ndifferent hardware robot?",
    "start": "1904290",
    "end": "1910750"
  },
  {
    "text": "So if I try a different\none [INAUDIBLE] I don't know. It's actually more manual. It's like student feels\nlike this is easy.",
    "start": "1910750",
    "end": "1920450"
  },
  {
    "text": "So it's not currently too much\ncode selection or something. It's like [INAUDIBLE].",
    "start": "1920450",
    "end": "1926110"
  },
  {
    "text": "So it's all very much\nmanual at this point. We are trying to have some\nautomate process like using",
    "start": "1926110",
    "end": "1935350"
  },
  {
    "text": "your policy to filter. So we try that, but\nwe initially want",
    "start": "1935350",
    "end": "1942370"
  },
  {
    "text": "to include that as part of\ntech in our paper as well. But some students\njust figure out a way",
    "start": "1942370",
    "end": "1949320"
  },
  {
    "text": "to filter a better dataset, so. Yes. [INAUDIBLE] pull\napart [INAUDIBLE]",
    "start": "1949320",
    "end": "1954460"
  },
  {
    "text": "in a larger data set? It includes infeasible\ntrajectories. So it's infeasible for the robot\nor it's infeasible for the human",
    "start": "1954460",
    "end": "1961450"
  },
  {
    "text": "as well? It's infeasible in this-- I'm sorry.",
    "start": "1961450",
    "end": "1967315"
  },
  {
    "text": "So it's more infeasible\nfor the robot. What I mean infeasible, it's\nlike this, more exaggerating,",
    "start": "1967315",
    "end": "1978100"
  },
  {
    "text": "jumping, and these\nkind of things. That is probably very hard to-- I mean, this is motion\nmore like a kinematics one.",
    "start": "1978100",
    "end": "1985659"
  },
  {
    "text": "So it's hard to actually\nmake it work in simulation.",
    "start": "1985660",
    "end": "1990860"
  },
  {
    "text": "So motions like this, so\nI call it not feasible.",
    "start": "1990860",
    "end": "1996309"
  },
  {
    "text": "All right. So some results--\nthere's another technique I want to cover.",
    "start": "1996310",
    "end": "2001990"
  },
  {
    "text": "So there's a difference in here. So there's paper\nlike previous paper",
    "start": "2001990",
    "end": "2008950"
  },
  {
    "text": "on whole body control actually\ndoing global keypoint tracking. Meaning that you have input\nmotion, the key points.",
    "start": "2008950",
    "end": "2016850"
  },
  {
    "text": "And then you want\nyour output keypoint, outputs humanoid joints exactly\nmatching the absolute positions",
    "start": "2016850",
    "end": "2024309"
  },
  {
    "text": "of your input human joints. And that if not matching,\ngives you a reward or something",
    "start": "2024310",
    "end": "2031960"
  },
  {
    "text": "to change it. So that results in\nactually, it drifts away",
    "start": "2031960",
    "end": "2037270"
  },
  {
    "text": "as the policy roll out. So what we do a\nlittle bit different",
    "start": "2037270",
    "end": "2042580"
  },
  {
    "text": "is we are basically separated. We are matching the velocity\nof the root velocity.",
    "start": "2042580",
    "end": "2049969"
  },
  {
    "text": "And then for the other joints,\nwe have basically a relative. So you all centralize your\nhuman pose and the robot",
    "start": "2049969",
    "end": "2057050"
  },
  {
    "text": "key points into the same\ncoordinate, which is 0, coordinate in the root space.",
    "start": "2057050",
    "end": "2062100"
  },
  {
    "text": "And then you do\nrelative positions between the human joints\nand the robot joints.",
    "start": "2062100",
    "end": "2068989"
  },
  {
    "text": "So a little bit design\ndifference for the loss.",
    "start": "2068989",
    "end": "2075050"
  },
  {
    "text": "So a lot of small\nengineering things, these are the two things. So we are able to do some,\nfor example, style walking.",
    "start": "2075050",
    "end": "2081860"
  },
  {
    "text": "So this is not just walking,\nbut have a little bit style. And then it can walk out door.",
    "start": "2081860",
    "end": "2088879"
  },
  {
    "text": "So this is just a\nlittle bit motions. We are trying more. So this is not\nvery large motion.",
    "start": "2088880",
    "end": "2094530"
  },
  {
    "text": "But you can see in the middle\nis the simulation results. On the right is the\nreal deployment.",
    "start": "2094530",
    "end": "2101990"
  },
  {
    "text": "So it's relatively\naligned in here. And then a little bit more--",
    "start": "2101990",
    "end": "2108950"
  },
  {
    "text": "so this is doing that. Yeah.",
    "start": "2108950",
    "end": "2110540"
  },
  {
    "text": "And then I think\nthis is this right.",
    "start": "2114130",
    "end": "2117500"
  },
  {
    "text": "This is punching. And then the little bit more.",
    "start": "2120930",
    "end": "2127109"
  },
  {
    "text": "So you actually use this\ndegree of freedom a little bit. So it's nice to use the here.",
    "start": "2127110",
    "end": "2135380"
  },
  {
    "text": "All right. What is this?",
    "start": "2135380",
    "end": "2137865"
  },
  {
    "text": "I know. So we started to do a lot\nof testing, stress testing,",
    "start": "2140700",
    "end": "2145930"
  },
  {
    "text": "actually. So this is a small experiment. We test yesterday that\nwe can have the g1 robot",
    "start": "2145930",
    "end": "2152190"
  },
  {
    "text": "stand for two hours straight\nwithout overheating. And then also with 66%\nbattery consumption.",
    "start": "2152190",
    "end": "2160230"
  },
  {
    "text": "That's important because\nwhen we are doing H1 robot, sometimes student\nforgot that is standing.",
    "start": "2160230",
    "end": "2165360"
  },
  {
    "text": "After 10 minutes is just fall\ndown because overheating. So the large robot seems\nto overheat quite fast.",
    "start": "2165360",
    "end": "2172540"
  },
  {
    "text": "And then the\nsmaller one is more. You can work for longer hours. So we're just stressing\ntest things on this one.",
    "start": "2172540",
    "end": "2181140"
  },
  {
    "text": "So that's basically the thing. So any questions?",
    "start": "2181140",
    "end": "2186750"
  },
  {
    "text": "I should go to the next part. All right. So all these things\nare low level.",
    "start": "2186750",
    "end": "2192480"
  },
  {
    "text": "And then I'll talk about the\nmiddle level we try to do.",
    "start": "2192480",
    "end": "2197970"
  },
  {
    "text": "So we first try to apply\nthis in navigation. There's a manipulation version. But this is my home again.",
    "start": "2197970",
    "end": "2204470"
  },
  {
    "text": "So my home is always the testbed\nfor all the robots in our lab. And then so this is trying to\nsay, I'll walk out of the rooms.",
    "start": "2204470",
    "end": "2212050"
  },
  {
    "text": "And then go to my office. So this is actually the\noffice I have at home.",
    "start": "2212050",
    "end": "2218140"
  },
  {
    "text": "So this is more like\nvision, language navigation. So some media people in here,\nbecause we are training, vision,",
    "start": "2218140",
    "end": "2225559"
  },
  {
    "text": "language action model. So the cool thing\nabout this work is that it defaults\ninto the two level",
    "start": "2225560",
    "end": "2232450"
  },
  {
    "text": "framework I was mentioning. So you're training a vision\nlanguage action model",
    "start": "2232450",
    "end": "2237640"
  },
  {
    "text": "without a model. But you are not directly\npredicting the low level control",
    "start": "2237640",
    "end": "2242770"
  },
  {
    "text": "outputs. You're actually predicting the\ncontrol in the form of language. You say basically, you move\nforward 75 centimeters.",
    "start": "2242770",
    "end": "2250330"
  },
  {
    "text": "So it's not that high level\nand not that low level. So it's not just move forward.",
    "start": "2250330",
    "end": "2256518"
  },
  {
    "text": "It's actually pointing out how\nmuch you need to move forward, and how much angles\nyou want to turn.",
    "start": "2256518",
    "end": "2261670"
  },
  {
    "text": "So we find this training\nis actually helps the model",
    "start": "2261670",
    "end": "2267119"
  },
  {
    "text": "not overfitting to the\nnavigation task itself,",
    "start": "2267120",
    "end": "2272670"
  },
  {
    "text": "because we actually do code\ntraining with other QA pairs when we're doing vision\nlanguage model training.",
    "start": "2272670",
    "end": "2278490"
  },
  {
    "text": "And then you have a low\nlevel policy that basically follow the instructions. So they separate the\ntraining, low level policy.",
    "start": "2278490",
    "end": "2285720"
  },
  {
    "text": "So the benefit of\nthis is, first, you can run these things\ninto frequency. So vla runs in 1 Hertz.",
    "start": "2285720",
    "end": "2292200"
  },
  {
    "text": "And then the low level can run\nin much higher frequencies. So you don't worry\nabout that VLA is low.",
    "start": "2292200",
    "end": "2298660"
  },
  {
    "text": "And then you can basically\nhave put in different platforms",
    "start": "2298660",
    "end": "2304619"
  },
  {
    "text": "with the single VLA. So we are doing\nthe [? g1 ?] now. And then I like this part a lot\nbecause the VR now is not just",
    "start": "2304620",
    "end": "2314160"
  },
  {
    "text": "training with the robot data. It can be trained\nwith human videos.",
    "start": "2314160",
    "end": "2320400"
  },
  {
    "text": "And then you can be trained with\njust general question answering tasks that have\ngeneral knowledge.",
    "start": "2320400",
    "end": "2326980"
  },
  {
    "text": "So it's actually we are\nusing the VIDA model, which is the NVIDIA VR model. And then we are co-training not\nonly the robot data, but also",
    "start": "2326980",
    "end": "2333880"
  },
  {
    "text": "the VR data at the same time. So it's not forgetting about all\nthis vision language model data.",
    "start": "2333880",
    "end": "2342530"
  },
  {
    "text": "So we are also co-training\nwith these videos as well. So the video is\nbasically, we construct--",
    "start": "2342530",
    "end": "2348590"
  },
  {
    "text": "this is the\nRealEstate 10K videos. If some of you have\nworked in vision,",
    "start": "2348590",
    "end": "2354000"
  },
  {
    "text": "you know this is the\nview synthesis dataset. And then you have a camera\nwalking around the room.",
    "start": "2354000",
    "end": "2359630"
  },
  {
    "text": "So basically we find some way\nto label the caption for it. Now we can basically have\nthese captions like this,",
    "start": "2359630",
    "end": "2365690"
  },
  {
    "text": "walk down the hallway\nto enter the bathroom. So this is walk up the\nstairs, and stop at the top.",
    "start": "2365690",
    "end": "2371069"
  },
  {
    "text": "So we use some VLM to label it. And then use it back to train\nour vision language model.",
    "start": "2371070",
    "end": "2377180"
  },
  {
    "text": "So it's turning how many\ndegrees, moving forward, how many centimeters\nand this sort of thing.",
    "start": "2377180",
    "end": "2383120"
  },
  {
    "text": "So this is enabling, have\na representation like this, enabling you use this general\nvideo to train as well.",
    "start": "2383120",
    "end": "2389849"
  },
  {
    "text": "It's not just the\nhabitat videos. So basically this is\nrolling out the policy.",
    "start": "2389850",
    "end": "2397440"
  },
  {
    "text": "It stopped a little\nbit because-- the way it works actually, you\nsend the image to the robot.",
    "start": "2397440",
    "end": "2404210"
  },
  {
    "text": "And then you're going\nthrough your phone. The phone send to the server. And then you compute\nthe text in here.",
    "start": "2404210",
    "end": "2413450"
  },
  {
    "text": "And then you send\nback to the robot. So it's a little bit\nnot on board entirely. But the low level is on board.",
    "start": "2413450",
    "end": "2419460"
  },
  {
    "text": "Yes. Professor, the phone here\nis used as a Wi-Fi hub? Yeah.",
    "start": "2419460",
    "end": "2425029"
  },
  {
    "text": "So we didn't manage to\nrun VLA on board ship. It's still very slow, even\nthough we are actually",
    "start": "2425030",
    "end": "2431210"
  },
  {
    "text": "working with some [? songs ?]\ngroup on all quantization. And all these things\nstill far away.",
    "start": "2431210",
    "end": "2438050"
  },
  {
    "text": "So this is again my house. So this is showing\nthat robot is useful",
    "start": "2438050",
    "end": "2444380"
  },
  {
    "text": "because there are questions\non robot is not so useful. So we are able to\nworking out my windows.",
    "start": "2444380",
    "end": "2451440"
  },
  {
    "text": "Yes. So the instruction is\nalways in metric space? Yeah. Move forward 25 centimeters.",
    "start": "2451440",
    "end": "2457740"
  },
  {
    "text": "Yes, Yes, Yes, Yes. How did you come to this\ndecision eventually? Well it's easier you have this--",
    "start": "2457740",
    "end": "2465960"
  },
  {
    "text": "there's a metric. But then there's also\nthis discrete type of action space\nright now in here.",
    "start": "2465960",
    "end": "2473170"
  },
  {
    "text": "Yeah, it's easier, basically. So yeah, this is\nfinding the football.",
    "start": "2473170",
    "end": "2478350"
  },
  {
    "text": "And then you can step on the\nstones and find the football. And basically, that's it.",
    "start": "2478350",
    "end": "2485670"
  },
  {
    "text": "So it works on a\nlot of environments. It's showing here,\nbut let's skip that.",
    "start": "2485670",
    "end": "2491880"
  },
  {
    "text": "And I like the part that--\nbecause it's actually the exact same model. You can apply it back to\ndo this question answering.",
    "start": "2491880",
    "end": "2499630"
  },
  {
    "text": "So you take in a video. You ask the question, where\ndoes the trash can set? It's to the right of the toilet.",
    "start": "2499630",
    "end": "2505770"
  },
  {
    "text": "And what is hanging on the\nwall between the toilet and the bathtub? So is the toilet paper. So it's the exact same model\nthat can do navigation,",
    "start": "2505770",
    "end": "2514740"
  },
  {
    "text": "can also do this\nquestion answering. The VLA is more close to\nVLM than VLA basically.",
    "start": "2514740",
    "end": "2522760"
  },
  {
    "text": "It's actually more\nlike a general model that you can have\nknowledge about things.",
    "start": "2522760",
    "end": "2528290"
  },
  {
    "text": "And then also you\ncan output action. So that's actually why I hope\nthe reality is going through.",
    "start": "2528290",
    "end": "2535130"
  },
  {
    "text": "So the reason why\nthis is important is because eventually\nwe also want",
    "start": "2535130",
    "end": "2540140"
  },
  {
    "text": "this kind of reasoning ability\nfor the vision language action model to have. It's not just doing\na fixed set of tasks.",
    "start": "2540140",
    "end": "2548310"
  },
  {
    "text": "You should have some\ngeneralization ability to reasoning. And then extend to\nmore complex scenarios.",
    "start": "2548310",
    "end": "2553880"
  },
  {
    "text": "So that's why we have\nthis framework in here.",
    "start": "2553880",
    "end": "2559220"
  },
  {
    "text": "So last part is we want to apply\nthe same thing to manipulation.",
    "start": "2559220",
    "end": "2566300"
  },
  {
    "text": "So the same framework\nwhere i say ideally, you should work in manipulation. So that you have\nhigh level policy",
    "start": "2566300",
    "end": "2573050"
  },
  {
    "text": "to do where the hands are going. And then you have\nlow level policy to handle the grasping things.",
    "start": "2573050",
    "end": "2578849"
  },
  {
    "text": "That was the hope. But then in terms of\ngiving a talk in here, I didn't make that\ndeadline that way.",
    "start": "2578850",
    "end": "2585490"
  },
  {
    "text": "So I make a lot of deadline. Basically, try to have\na lot of framework that",
    "start": "2585490",
    "end": "2590820"
  },
  {
    "text": "is close to what I\ndescribed, that we still try to train the vision language\naction model on human data.",
    "start": "2590820",
    "end": "2598710"
  },
  {
    "text": "But then have some way to\ntransform the outputs of human-- action output to robot outputs.",
    "start": "2598710",
    "end": "2605640"
  },
  {
    "text": "So this is a EgoVLA very paper. So what we did is we tried\nto change the vision language",
    "start": "2605640",
    "end": "2611940"
  },
  {
    "text": "action model only\non human data first. We train on egocentric videos.",
    "start": "2611940",
    "end": "2617410"
  },
  {
    "text": "This really, really very limited\nnumbers of videos you can use. I mean, when we try to do\nthis task, there's not a lot.",
    "start": "2617410",
    "end": "2623920"
  },
  {
    "text": "So these are some example videos\nthat have relatively reliable 3D",
    "start": "2623920",
    "end": "2629400"
  },
  {
    "text": "annotations on the\nhands and effector. So we use this video\nto do the training.",
    "start": "2629400",
    "end": "2636820"
  },
  {
    "text": "I bring this up again,\nbecause from this project what we learn is really the\ndifference between human action",
    "start": "2636820",
    "end": "2644970"
  },
  {
    "text": "and the robot action is really\njust a transformation away because you are\ntransformation like this",
    "start": "2644970",
    "end": "2651450"
  },
  {
    "text": "doing IK and retargeting. You can transform\nthe human action to become the robot actions. So that makes me think, can\nwe just train a human VLA",
    "start": "2651450",
    "end": "2660390"
  },
  {
    "text": "and then go through\nsome transformation? Then it will become\nthe robot VLA.",
    "start": "2660390",
    "end": "2665400"
  },
  {
    "text": "So that's why we start\nthis projects only training on human VR.",
    "start": "2665400",
    "end": "2670560"
  },
  {
    "text": "So this is a model\nthat basically takes human videos, a few\nframes as input and language",
    "start": "2670560",
    "end": "2676710"
  },
  {
    "text": "instruction. And then you want to predict\na model hand basically, is the parameterized\nhand for human hands.",
    "start": "2676710",
    "end": "2684839"
  },
  {
    "text": "So you predict the hand pose. And also where the\nwrist is going to reach.",
    "start": "2684840",
    "end": "2691590"
  },
  {
    "text": "And these models, which only\noutputs the human action,",
    "start": "2691590",
    "end": "2698310"
  },
  {
    "text": "can actually be converted\nto robot action. So to do this, we create\nanother teleop dataset,",
    "start": "2698310",
    "end": "2707010"
  },
  {
    "text": "because we want to iterate\nvery fast in manipulation. So keep using real data\nmakes it very slow.",
    "start": "2707010",
    "end": "2712290"
  },
  {
    "text": "So we have [? ISA ?]\nlab thing that we developed on very similar\nteleop tasks we see in real.",
    "start": "2712290",
    "end": "2718450"
  },
  {
    "text": "But now it's in sim. so\nwe can iterate faster. So what we try to do\nin here is basically",
    "start": "2718450",
    "end": "2725460"
  },
  {
    "text": "we find a mapping, a conversion\nbetween the human action and the robot action.",
    "start": "2725460",
    "end": "2733069"
  },
  {
    "text": "So from robot response\nto human response, it's really a very simple 3D\ntransformation because the model",
    "start": "2733070",
    "end": "2740329"
  },
  {
    "text": "response has its own\ncoordinate system. So we need to convert that to 3D\nlocations of the robot hand pose",
    "start": "2740330",
    "end": "2751250"
  },
  {
    "text": "and hand raised to\nthe [INAUDIBLE]. And then to convert robot\nfingers to human fingers, what",
    "start": "2751250",
    "end": "2760250"
  },
  {
    "text": "we are doing is actually going\nthrough optimization process. The process is basically copying\nthe fingertip of the robot hand,",
    "start": "2760250",
    "end": "2771530"
  },
  {
    "text": "is going to align-- so where the finger\ntouch of the robot hand",
    "start": "2771530",
    "end": "2777109"
  },
  {
    "text": "is exactly where the human\nhand finger is going to touch. So you're aligning\nthe fingertip.",
    "start": "2777110",
    "end": "2782480"
  },
  {
    "text": "And then you do an\noptimization approach to basically to get the\nhuman hand pose parameter.",
    "start": "2782480",
    "end": "2788420"
  },
  {
    "text": "So we basically\nfigure out a mapping to convert robot video,\nthese neural actions,",
    "start": "2788420",
    "end": "2798440"
  },
  {
    "text": "into human actions. So once you have\nthe human actions, you basically have the\nrobot demonstrations.",
    "start": "2798440",
    "end": "2806099"
  },
  {
    "text": "But you're actually getting\nhuman action out of it. And then you use\nthis human actions to fine tune this human VLA.",
    "start": "2806100",
    "end": "2813920"
  },
  {
    "text": "So I'm keep basically\ntraining just the human VLA.",
    "start": "2813920",
    "end": "2818970"
  },
  {
    "text": "There's no robot VLA here. So I'm even converting the\nrobot actions to human actions.",
    "start": "2818970",
    "end": "2824490"
  },
  {
    "text": "So we train a human VLA. The cool thing is that\nafter you train a human VLA,",
    "start": "2824490",
    "end": "2830710"
  },
  {
    "text": "that you can actually\noutput the wrist pose and also the hand pose.",
    "start": "2830710",
    "end": "2835990"
  },
  {
    "text": "So you can map it back\ninto robot actions. For example, you\ncan first map it to the wrist of the robot hand.",
    "start": "2835990",
    "end": "2843039"
  },
  {
    "text": "And then also do IK to control\nall the joints of the arms. And then you can\nalso do retargeting",
    "start": "2843040",
    "end": "2848589"
  },
  {
    "text": "to get the robot\nhand pose so that you can convert the human hand\naction back to the robot hand.",
    "start": "2848590",
    "end": "2857660"
  },
  {
    "text": "So I will show you some video. So for better\nillustration, so we have these tasks\nwhich we still keep",
    "start": "2857660",
    "end": "2863860"
  },
  {
    "text": "developing but different tasks. We also do demonstration\ncollection in this task suite.",
    "start": "2863860",
    "end": "2869710"
  },
  {
    "text": "But essentially, this is our\nhuman VLA output results. I'm also only\nvisualizing the wrist.",
    "start": "2869710",
    "end": "2877520"
  },
  {
    "text": "But we actually predict all\nthe joints of the finger. And basically the blue\nthing is the prediction.",
    "start": "2877520",
    "end": "2885670"
  },
  {
    "text": "The red thing is\nthe ground truth. So we are basically\npredicting 10 step forward.",
    "start": "2885670",
    "end": "2892460"
  },
  {
    "text": "So it's also action chunking. All the things are\nhappening here. But the model is able to given\na few images you predict where",
    "start": "2892460",
    "end": "2899720"
  },
  {
    "text": "future how the hand moves. And this basically\nconverts to the same models",
    "start": "2899720",
    "end": "2905630"
  },
  {
    "text": "after we fine\ntuned on the robot. Here it converts to conduct\nthe task in this similarly.",
    "start": "2905630",
    "end": "2914970"
  },
  {
    "text": "So this is flipping it and\nthen pulling the thing. Questions. Yes. What about the hand gestures?",
    "start": "2914970",
    "end": "2922340"
  },
  {
    "text": "These are just the\ntrajectory of the hand. How do you predict the fingers? So we are predicting\nthe model hand.",
    "start": "2922340",
    "end": "2929539"
  },
  {
    "text": "Which means it actually has\nthe parameterized hand pose",
    "start": "2929540",
    "end": "2934970"
  },
  {
    "text": "and also shape. So it's actually 15\nparameters, which",
    "start": "2934970",
    "end": "2941540"
  },
  {
    "text": "is the PCA space\nof the bottle hand. Once you predict\nthe 15 parameters, you can obtain the hand\npose and everything.",
    "start": "2941540",
    "end": "2948440"
  },
  {
    "text": "So it's actually quite detailed\nhuman hand gesture predictions. It's not just wrist.",
    "start": "2948440",
    "end": "2954030"
  },
  {
    "text": "Yeah. Professor. Yes. So you need to collect\npaired data in simulation",
    "start": "2954030",
    "end": "2960050"
  },
  {
    "text": "and also real world\nhuman demonstration? Not in this project.",
    "start": "2960050",
    "end": "2966230"
  },
  {
    "text": "So this is more like\na fine tuning thing. It's not called training. So it's OK.",
    "start": "2966230",
    "end": "2972590"
  },
  {
    "text": "So the human data is really\nquite in the wild data. So that have relatively\naccurate 3D pose.",
    "start": "2972590",
    "end": "2979940"
  },
  {
    "text": "The hope is that we still\ncan fine tune this robot data to fine tune it. So that works.",
    "start": "2979940",
    "end": "2986420"
  },
  {
    "text": "But I can show you\nthe next slides that we do try to collect\ndata in similar environments",
    "start": "2986420",
    "end": "2991580"
  },
  {
    "text": "for both humans and robots. But currently for this,\nvery initial attempts.",
    "start": "2991580",
    "end": "2999200"
  },
  {
    "text": "So we tried this.",
    "start": "2999200",
    "end": "3000730"
  },
  {
    "text": "So again, the\nsimulation is really for evaluation purpose of\nstarting the algorithm purposes.",
    "start": "3005820",
    "end": "3012250"
  },
  {
    "text": "We're not counting that this\nwill be useful for sim-to-real. It's really just for us\nto develop the methods.",
    "start": "3012250",
    "end": "3018250"
  },
  {
    "text": "And then fast evaluations\non the methods that try to do this cross\nhuman and robot embodiment",
    "start": "3018250",
    "end": "3024970"
  },
  {
    "text": "kinds of learning. Yeah so this is something\nI mentioned that earlier, that I just want to\nanswer your question.",
    "start": "3024970",
    "end": "3033140"
  },
  {
    "text": "So we also try to\nmake this work, but it doesn't make in\ntime to [INAUDIBLE].",
    "start": "3033140",
    "end": "3039910"
  },
  {
    "text": "The paper is six\npages long already. But we didn't eventually\nmake all the tasks work, so we didn't submit.",
    "start": "3039910",
    "end": "3046790"
  },
  {
    "text": "But we are also having\nsome efforts basically to collect real robot data. And on the side you can\ncollect real human data.",
    "start": "3046790",
    "end": "3053560"
  },
  {
    "text": "And this is using Vision Pro. So you have to have enterprise\nlicense from somewhere.",
    "start": "3053560",
    "end": "3059440"
  },
  {
    "text": "Then you can have\nmain camera access. So you can actually\nrecord the human videos. And then you can also have\nrelatively accurate hand pose",
    "start": "3059440",
    "end": "3066740"
  },
  {
    "text": "estimation for this\nvideo, we recorded. So you can collect human\ndata doing the same thing.",
    "start": "3066740",
    "end": "3072440"
  },
  {
    "text": "So we are exploring\nthe similar framework, but using real human data\nand real robot data to do it.",
    "start": "3072440",
    "end": "3079520"
  },
  {
    "text": "So that's the thing. But that is again,\nteleoperation is one way.",
    "start": "3079520",
    "end": "3084650"
  },
  {
    "text": "I'm going to end my talk. But basically we are also\nstill exploring sim-to-real.",
    "start": "3084650",
    "end": "3090840"
  },
  {
    "text": "So this is some\ngrasping thing we are doing that we are\nexploring dexterous",
    "start": "3090840",
    "end": "3096350"
  },
  {
    "text": "grasping, but in simulation\nand do sim-to-real. So I do feel like\nsim-to-real might still",
    "start": "3096350",
    "end": "3101540"
  },
  {
    "text": "work to basically explore. But on the purpose of\njust very low level",
    "start": "3101540",
    "end": "3106760"
  },
  {
    "text": "contact interaction instead\nof very complex scene using digital twin and things.",
    "start": "3106760",
    "end": "3111980"
  },
  {
    "text": "So I think for the final\nmilestone of contact and grasping,\nthere's possibility that use this sim-to-real\non here as well.",
    "start": "3111980",
    "end": "3121490"
  },
  {
    "text": "So that's all. So I think in\ngeneral, the thing I try to convince\nyou is basically,",
    "start": "3121490",
    "end": "3129440"
  },
  {
    "text": "you can have controller,\nyou can have teleop, you can have sim-to-real. But I don't believe it\nwill work in task level.",
    "start": "3129440",
    "end": "3137010"
  },
  {
    "text": "I think it would be very useful. And then we've limited\ndata that solve a lot of problems in low level\nin contact in that context.",
    "start": "3137010",
    "end": "3146630"
  },
  {
    "text": "But it's still very hard\nto get large amount of data to do more planning on\nmore complex things.",
    "start": "3146630",
    "end": "3156980"
  },
  {
    "text": "So I do believe in that we\nshould leverage a lot human data at the same time\nthat we train not",
    "start": "3156980",
    "end": "3162890"
  },
  {
    "text": "just a VLA for particular robot,\nbut maybe agent [INAUDIBLE] or people across embodiment.",
    "start": "3162890",
    "end": "3169790"
  },
  {
    "text": "But I want to emphasize\nmore on the human part that we can train, human VLA.",
    "start": "3169790",
    "end": "3176210"
  },
  {
    "text": "And then connect these two. So this is more like the\npath I believe in now.",
    "start": "3176210",
    "end": "3182210"
  },
  {
    "text": "So that's all. Thank you. Let me know-- [APPLAUSE]",
    "start": "3182210",
    "end": "3188863"
  },
  {
    "text": "We have five minutes\nfor questions.",
    "start": "3193090",
    "end": "3194590"
  },
  {
    "text": "Yes. So for active sensing\ntask, the hand testing task will the robot actively search\nfor the can if the can is not",
    "start": "3199160",
    "end": "3207380"
  },
  {
    "text": "[INAUDIBLE]? No. So the training data\nis too small now. So it's not doing that\nintellect actions.",
    "start": "3207380",
    "end": "3216600"
  },
  {
    "text": "So it's more fixed action. So it's always left and\nright, left and right.",
    "start": "3216600",
    "end": "3221790"
  },
  {
    "text": "Yeah. But I do believe in\nif you have given enough data that\nsee around, it can",
    "start": "3221790",
    "end": "3227610"
  },
  {
    "text": "emerge this kind of behavior. But yeah, we are only training\non 50 or 100 demonstrations. So it's not that intellectually\nsearching for things.",
    "start": "3227610",
    "end": "3236220"
  },
  {
    "text": "Yeah. For the robot television,\nthe camera [INAUDIBLE]",
    "start": "3236220",
    "end": "3242320"
  },
  {
    "text": "Yeah. How would [INAUDIBLE]\nfor policy? Yeah, we are fixing that.",
    "start": "3242320",
    "end": "3247950"
  },
  {
    "text": "So I mean, the body\nwill always shake. So some stabilization of\nvision might be needed.",
    "start": "3247950",
    "end": "3255720"
  },
  {
    "text": "But also in the length, we\nare improving the design of the length a little\nbit on the hardware",
    "start": "3255720",
    "end": "3262440"
  },
  {
    "text": "wise, so to stabilize it. Yeah. So that is something\nwe are working on. And also, for the open\n[INAUDIBLE] egocentric video.",
    "start": "3262440",
    "end": "3272740"
  },
  {
    "text": "Do you think the\ncamera pose could also be useful for as part\nof the action space?",
    "start": "3272740",
    "end": "3279040"
  },
  {
    "text": "Yeah, I think we do-- oh, the camera pose. I don't think we have\nexplored that yet.",
    "start": "3279040",
    "end": "3286450"
  },
  {
    "text": "But yeah, so the thing\nis just it's very noisy. It's just the--",
    "start": "3286450",
    "end": "3292420"
  },
  {
    "text": "The Ego human video\nwill always back up. Yeah. So in that case, we\nare actually putting",
    "start": "3292420",
    "end": "3300580"
  },
  {
    "text": "things in the camera frame now. So for the action\nmodel-- so it's actually in the camera coordinate space.",
    "start": "3300580",
    "end": "3306290"
  },
  {
    "text": "So we are now in the robot\nframe of the world coordinates, so the action is following\nthe camera more now.",
    "start": "3306290",
    "end": "3313810"
  },
  {
    "text": "Yeah, I think it's\njust from those video, you don't have a very\naccurate camera models.",
    "start": "3313810",
    "end": "3320380"
  },
  {
    "text": "Yeah, so I think that's also\nsomething we are missing. So when we are collecting\nnew egocentric videos,",
    "start": "3320380",
    "end": "3326500"
  },
  {
    "text": "we should also get\nthe accurate camera. [INAUDIBLE] is that kind\nof [INAUDIBLE] video",
    "start": "3326500",
    "end": "3332990"
  },
  {
    "text": "with robot calibration? Yes, you can get it. Yes.",
    "start": "3332990",
    "end": "3335990"
  },
  {
    "text": "OK. And any questions? [INAUDIBLE], I have\na quick question from the online audience.",
    "start": "3340160",
    "end": "3346460"
  },
  {
    "text": "One question is, how would\nyou compare Exbody2 with H2O? Exbody2 with H2O.",
    "start": "3346460",
    "end": "3353779"
  },
  {
    "text": "Yeah, So that's the big\nthing that actually H2O--",
    "start": "3353780",
    "end": "3360720"
  },
  {
    "text": "So this is a\ncomparisons in here. So I think the two methods\nhighlight to 1 and 2",
    "start": "3360720",
    "end": "3368580"
  },
  {
    "text": "actually makes the difference. So for example, H2O is\nactually using this one. So the global tracking is\ntracking the absolute positions",
    "start": "3368580",
    "end": "3377610"
  },
  {
    "text": "of the point, the key points. And we are doing a\nvelocity tracking plus local relative\npost tracking.",
    "start": "3377610",
    "end": "3384250"
  },
  {
    "text": "So that is one difference. Second difference is really\na lot of study on data sites. So I think H2O only\nuse the whole data set.",
    "start": "3384250",
    "end": "3391810"
  },
  {
    "text": "But we do a lot of study\non how data affect it. We were trying to tell\na story on curriculum",
    "start": "3391810",
    "end": "3399480"
  },
  {
    "text": "learning in the beginning. But again, one\nstudent figure out a way to create a very, very\ngood dataset, so stopped",
    "start": "3399480",
    "end": "3407130"
  },
  {
    "text": "us to tell the story. So I think there must be a\nway to filter and automate",
    "start": "3407130",
    "end": "3413340"
  },
  {
    "text": "this data curation process. Maybe if that\nstudent is not there, there will be a good story.",
    "start": "3413340",
    "end": "3419350"
  },
  {
    "text": "But yeah, so manually\nfiltering is still good.",
    "start": "3419350",
    "end": "3424540"
  },
  {
    "text": "But we will find some\nway to automate it. And then also enlarge\nthe whole data set.",
    "start": "3424540",
    "end": "3430910"
  },
  {
    "text": "Yeah. So these 1 and 2 are\nthe major differences. It's not that--",
    "start": "3430910",
    "end": "3437040"
  },
  {
    "text": "But it's important. It's very, very critical\nto make this work.",
    "start": "3437040",
    "end": "3441160"
  },
  {
    "text": "OK. All right? We not going to have\none more question? OK. [INAUDIBLE] Yes.",
    "start": "3443880",
    "end": "3448990"
  },
  {
    "text": "So in terms of the\nsize of VLA models, they are of useful and what\nhardware they can run on.",
    "start": "3448990",
    "end": "3454240"
  },
  {
    "text": "Yeah. So I think in your cases\nyou're running in the cloud. Was that the-- Yeah, it's running\non desktop actually.",
    "start": "3454240",
    "end": "3459890"
  },
  {
    "text": "You're sending the picture\nback to my desktop. And then run it there. And then send it back.",
    "start": "3459890",
    "end": "3465250"
  },
  {
    "text": "So I think we are using actually\na small like 1.5 B, Language model.",
    "start": "3465250",
    "end": "3471450"
  },
  {
    "text": "It's not very large. And then the frame\ninput is actually eight frames as a history.",
    "start": "3471450",
    "end": "3477240"
  },
  {
    "text": "So we try to use something\ncalled longvida before, which is the new vida versions\nthat have 128 frames.",
    "start": "3477240",
    "end": "3484480"
  },
  {
    "text": "So that is not working\nbecause one inference takes four seconds. So that's-- even on the\nGPUs on the desktop.",
    "start": "3484480",
    "end": "3491770"
  },
  {
    "text": "So that's impossible. But these models, and then\nafter some quantization,",
    "start": "3491770",
    "end": "3497500"
  },
  {
    "text": "we are able to run it\nmaybe half a second. That is the maximum speed. And then there's\nsome transferring",
    "start": "3497500",
    "end": "3505290"
  },
  {
    "text": "pictures, some latency there. So I think we hope to. I mean, as also\nfor NVIDIA itself.",
    "start": "3505290",
    "end": "3512010"
  },
  {
    "text": "Yeah. They'd like to give\nsupport this some how or something [INAUDIBLE]",
    "start": "3512010",
    "end": "3517770"
  },
  {
    "text": "Yeah. Well I'm directly working on the\nteam that do this quantization. And we are a efficiency.",
    "start": "3517770",
    "end": "3523599"
  },
  {
    "text": "But it's still not\na very easy problem to directly work on the-- Did you find any trade offs, You\ngo to smaller fine tuned models",
    "start": "3523600",
    "end": "3531359"
  },
  {
    "text": "that they're losing\nsome other capabilities? So I think the task\nwe are doing actually",
    "start": "3531360",
    "end": "3538890"
  },
  {
    "text": "is not that difficult than now. So it's not that a big\ndifference, especially when we actually try to show--",
    "start": "3538890",
    "end": "3545560"
  },
  {
    "text": "I mean, initially, I was the one\nto tell a story that Longvida-- a long context VLM in model\nwill be much more helpful, when",
    "start": "3545560",
    "end": "3552480"
  },
  {
    "text": "reality is not. So the vision language\napplication problem is not very well defined\nin the current setting.",
    "start": "3552480",
    "end": "3561119"
  },
  {
    "text": "Actually, a short\nhistory works well. So I think there will\nbe a problem that require long context.",
    "start": "3561120",
    "end": "3567130"
  },
  {
    "text": "But right now it's not that\nnecessary to have bigger model. A longer context model. Yeah.",
    "start": "3567130",
    "end": "3573840"
  },
  {
    "text": "Thank you. Thank you. Thank you. [APPLAUSE]",
    "start": "3573840",
    "end": "3579590"
  }
]