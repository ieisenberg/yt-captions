[
  {
    "text": " Good afternoon, folks.",
    "start": "0",
    "end": "6710"
  },
  {
    "text": "Welcome to lecture 18. Today, we'll be\ntalking about some of the latest and greatest\ndevelopments in neural NLP,",
    "start": "6710",
    "end": "13690"
  },
  {
    "text": "where we've come, and\nwhere we're headed. Chris, just to be sure,\nare my presentation notes",
    "start": "13690",
    "end": "20869"
  },
  {
    "text": "visible from this\nboard at this time? You're visible. OK, but not my\npresenter notes, right?",
    "start": "20870",
    "end": "28210"
  },
  {
    "text": "Correct. OK, good, thank you. So just as a reminder, note that\nyour guest lecture reactions",
    "start": "28210",
    "end": "35230"
  },
  {
    "text": "are due tomorrow at 11:59 PM. A great job with the\nproject milestone reports.",
    "start": "35230",
    "end": "40570"
  },
  {
    "text": "You should have\nreceived feedback now. If not, contact\nthe course staff. I think we had some\nlast-minute issues,",
    "start": "40570",
    "end": "47680"
  },
  {
    "text": "but if that's not resolved,\nplease contact us. Finally, the project\nreports are due very soon,",
    "start": "47680",
    "end": "54580"
  },
  {
    "text": "on March 16th,\nwhich is next week. There's one question on\nEd about the leaderboard,",
    "start": "54580",
    "end": "60190"
  },
  {
    "text": "and the last day to\nsubmit on the leaderboard is March 19th as well.",
    "start": "60190",
    "end": "68430"
  },
  {
    "text": "OK, so for today,\nwe'll start by talking about extremely large\nlanguage models and GPT-3",
    "start": "68430",
    "end": "74579"
  },
  {
    "text": "that have recently gained\na lot of popularity. We'll then take a closer\nlook at compositionality",
    "start": "74580",
    "end": "81299"
  },
  {
    "text": "and generalization of\nthese neural models, why",
    "start": "81300",
    "end": "86480"
  },
  {
    "text": "transformer models\nlike BERT and GPT-3 have really high performance\non all benchmarks? They just fail in\nreally surprising ways",
    "start": "86480",
    "end": "92990"
  },
  {
    "text": "when they provide. How can we strengthen\nour understanding of evaluating these models so\nthey more closely reflect task",
    "start": "92990",
    "end": "99590"
  },
  {
    "text": "performance in the real world? And then, we end by\ntalking about how",
    "start": "99590",
    "end": "105900"
  },
  {
    "text": "we can move beyond this really\nlimited paradigm of teaching models language\nonly through text",
    "start": "105900",
    "end": "111210"
  },
  {
    "text": "and look at language grounding. Finally, I'll give\nsome practical tips",
    "start": "111210",
    "end": "116360"
  },
  {
    "text": "on how to move forward in\nyour neural NLP research, and this will include\nsome practical tips",
    "start": "116360",
    "end": "122330"
  },
  {
    "text": "for the final project as well. OK, so this meme\nreally kind of captures",
    "start": "122330",
    "end": "132190"
  },
  {
    "text": "what's been going on\nin the field, really, and it's just that ability\nto harness unlabeled data",
    "start": "132190",
    "end": "137710"
  },
  {
    "text": "has vastly increased\nover the last few years. And this has been made possible\ndue to advances in not just",
    "start": "137710",
    "end": "143500"
  },
  {
    "text": "hardware, but also systems,\nand our understanding of self-supervised\ntraining, so we",
    "start": "143500",
    "end": "150100"
  },
  {
    "text": "can use lots and lots\nof unlabeled data. So based on this, here's\na general representation",
    "start": "150100",
    "end": "157452"
  },
  {
    "text": "learning recipe that just\nworks for, basically, most modalities.",
    "start": "157452",
    "end": "162879"
  },
  {
    "text": "So the recipe is\nbasically as follows. So convert your data.",
    "start": "162880",
    "end": "168580"
  },
  {
    "text": "If it's images, convert it. Or it's not-- it's\nreally modality-agnostic.",
    "start": "168580",
    "end": "176480"
  },
  {
    "text": "So you take your data out, if\nit's images, text, or videos, and you convert it into\na sequence of integers.",
    "start": "176480",
    "end": "182070"
  },
  {
    "text": "And in step two,\nyou define a loss function to maximize\ndata likelihood, or create a denoising\nauto-encoder loss.",
    "start": "182070",
    "end": "189360"
  },
  {
    "text": "Finally, in step three, train\non lots and lots of data.",
    "start": "189360",
    "end": "194410"
  },
  {
    "text": "Certain properties\nemerge whenever we scale up model size. And this is really the\nsurprising fact about scale.",
    "start": "194410",
    "end": "200569"
  },
  {
    "text": "So to give some examples of this\nrecipe in action, here's GPT-3.",
    "start": "200570",
    "end": "205600"
  },
  {
    "text": "Which can learn to do a really\nnontrivial classification problem with just\ntwo demonstrations.",
    "start": "205600",
    "end": "210909"
  },
  {
    "text": "And we'll talk more\nabout this soon. Another example, as\nwe saw in lecture 14,",
    "start": "210910",
    "end": "216520"
  },
  {
    "text": "is T5, which does\nreally effective closed book QA by storing\nknowledge in its parameters.",
    "start": "216520",
    "end": "224110"
  },
  {
    "text": "Finally, just so I\ncover another modality, here's a recent\ntext-to-image generation",
    "start": "224110",
    "end": "230775"
  },
  {
    "text": "model with really impressive\nzero shot generalization.",
    "start": "230775",
    "end": "236409"
  },
  {
    "text": "OK, so now let's\ntalk about GPT-3 So how big, really,\nare these models?",
    "start": "236410",
    "end": "241840"
  },
  {
    "text": "This table kind of\npresents some numbers to put things in perspective.",
    "start": "241840",
    "end": "247400"
  },
  {
    "text": "So we have a\ncollection of models, starting with\nmedium-size LSTMs, which was sort of a staple in\npre-2016 NLP, all the way",
    "start": "247400",
    "end": "255340"
  },
  {
    "text": "to humans, who have\n100 trillion synapses. And somewhere in\nthe middle, we have GPT-2 with over a billion\nparameters and GPT-3 with over",
    "start": "255340",
    "end": "263380"
  },
  {
    "text": "150 billion parameters. And this exceeds the number\nof synaptic connections",
    "start": "263380",
    "end": "269110"
  },
  {
    "text": "in a honeybee brain. So obviously, anyone\nwith a little knowledge of neuroscience\nknows that this is",
    "start": "269110",
    "end": "278290"
  },
  {
    "text": "an apples-to-oranges\ncomparison, but the point here is that the scale\nof these models is really starting to\nreach astronomical numbers.",
    "start": "278290",
    "end": "286520"
  },
  {
    "text": "So here are some\nfacts about GPT-3. For one, it's a large\ntransformer with 96 layers.",
    "start": "286520",
    "end": "293850"
  },
  {
    "text": "It has more or less the\nsame architecture as GPT-2, with the exception that to\nscale up attention computation,",
    "start": "293850",
    "end": "301400"
  },
  {
    "text": "it uses these locally banded\nsparse attention patterns. And I really encourage\nyou to look at the paper to understand the details.",
    "start": "301400",
    "end": "307272"
  },
  {
    "text": "The reason we\nmention this here is because it kind of\nhighlights that scaling up is simply not just\nchanging hyperparameters,",
    "start": "307273",
    "end": "313100"
  },
  {
    "text": "as many might believe,\nand it was really non-trivial engineering\nand algorithms to make computations efficient.",
    "start": "313100",
    "end": "320610"
  },
  {
    "text": "Finally, all of this is\ntrained on 500 billion tokens taken from The Common\nCrawl, Toronto Book",
    "start": "320610",
    "end": "326449"
  },
  {
    "text": "Scholars, Wikipedia. So what's new\nabout GPT-3, right?",
    "start": "326450",
    "end": "332050"
  },
  {
    "text": "So let's look at some of the\nresults on the paper first. So obviously, it does better\non language modeling and text",
    "start": "332050",
    "end": "337510"
  },
  {
    "text": "completion problems. As you can see\nfrom this table, it does better than GPT-2 at\nlanguage modeling in the Penn",
    "start": "337510",
    "end": "344260"
  },
  {
    "text": "Treebank, as well as\nbetter on story completion on the story completion\ndata set called LAMBADA.",
    "start": "344260",
    "end": "350259"
  },
  {
    "text": "To get a flavor\nof what's to come, let's take a closer look at this\nLAMBADA story completion data set.",
    "start": "350260",
    "end": "356020"
  },
  {
    "text": "So the task here is that\nwe're given a short story, and we are supposed to\nfill in the last word.",
    "start": "356020",
    "end": "361360"
  },
  {
    "text": "Satisfying the\nconstraints of the problem can be hard for a language\nmodel, which could generate",
    "start": "361360",
    "end": "367210"
  },
  {
    "text": "a multi-word completion. But with GPT-3, the\nreally new thing is that we can just give\na few examples as prompts",
    "start": "367210",
    "end": "373180"
  },
  {
    "text": "and sort of communicate a task\nspecification to the model. And now, GPT-3 knows\nhow the completion must be a single word.",
    "start": "373180",
    "end": "379060"
  },
  {
    "text": "This is a very, very\npowerful paradigm, and we get some more examples\nof this in-context learning",
    "start": "379060",
    "end": "384340"
  },
  {
    "text": "in a couple more slides. So apart from language\nmodeling, it's",
    "start": "384340",
    "end": "389450"
  },
  {
    "text": "really good at these\nknowledge-intensive tasks, like closed-book QA, as well\nas reading comprehension.",
    "start": "389450",
    "end": "396140"
  },
  {
    "text": "And here, we\nobserve that scaling up parameters results\nin a massive improvement in performance.",
    "start": "396140",
    "end": "402410"
  },
  {
    "text": "So now let's talk about\nin-context learning. GPT-3 demonstrates some\nlevel of task adaptation",
    "start": "402410",
    "end": "408400"
  },
  {
    "text": "to completely new tasks. This happens via what's\ncalled in-context learning.",
    "start": "408400",
    "end": "413670"
  },
  {
    "text": "As shown in the figure,\nthe model training can be characterized as\nhaving an outer loop that learns the set of\nparameters that",
    "start": "413670",
    "end": "420340"
  },
  {
    "text": "makes the learning of the inner\nloop as efficient as possible. And with this sort\nof framework in mind,",
    "start": "420340",
    "end": "426940"
  },
  {
    "text": "we can really see how a good\nlanguage model can also serve as a good few-shot learner. ",
    "start": "426940",
    "end": "433910"
  },
  {
    "text": "So in this segment, we will\nhave some fun with GPT-3 and look at some demonstrations\nof this in-context learning.",
    "start": "433910",
    "end": "442340"
  },
  {
    "text": "So to start off,\nhere is an example where someone's trying to\ncreate an application that converts language, a\nlanguage description,",
    "start": "442340",
    "end": "450740"
  },
  {
    "text": "to bash one-liners. The first three\nexamples are prompts, followed by generated\nexamples from GPT-3.",
    "start": "450740",
    "end": "457910"
  },
  {
    "text": "So it gets a list of\nrunning processes, right? This one is easy. It probably just involves\nlooking through a hash table.",
    "start": "457910",
    "end": "463880"
  },
  {
    "text": "Some of the more\nchallenging ones that involve copying\nover some spans",
    "start": "463880",
    "end": "470260"
  },
  {
    "text": "from the texts-- like the scp\nexample is kind of interesting, as well as a harder\none to parse, grep.",
    "start": "470260",
    "end": "476240"
  },
  {
    "text": "The scp example comes up\na lot during office hours, so GPT-3 knows how to do that.",
    "start": "476240",
    "end": "483800"
  },
  {
    "text": "Here's a somewhat\nmore challenging one, where the model is\ngiven a description of a database in\nnatural language,",
    "start": "483800",
    "end": "489040"
  },
  {
    "text": "and it starts to\nemulate that behavior. So the text in bold is sort of\nthe prompt given to the model.",
    "start": "489040",
    "end": "496270"
  },
  {
    "text": "The prompt includes\nsomewhat of a function, function specification\nof what a database is.",
    "start": "496270",
    "end": "502910"
  },
  {
    "text": "So it says that the database\nbegins knowing nothing, and the database knows\neverything that's added to it, and the database does\nnot know anything else.",
    "start": "502910",
    "end": "510010"
  },
  {
    "text": "And when you ask the\nquestion to the database, if the answer is\nthere in the database, the database must\nreturn the answer.",
    "start": "510010",
    "end": "515320"
  },
  {
    "text": "Otherwise, it should say it\ndoes not know the answer. So this is very new\nand very powerful.",
    "start": "515320",
    "end": "522280"
  },
  {
    "text": "And the prompt also\nincludes some example usages of when you ask 2 plus 2,\nthe database does not know.",
    "start": "522280",
    "end": "528160"
  },
  {
    "text": "When you ask the\ncapital of France, the database does not know. And then you add in\na fact that Tom is",
    "start": "528160",
    "end": "533380"
  },
  {
    "text": "20 years old to the database. And now you can start\nasking questions like, where does Tom live?",
    "start": "533380",
    "end": "538810"
  },
  {
    "text": "And as expected, it says that\nthe database does not know. But now, if you ask\nit, what's Tom's age,",
    "start": "538810",
    "end": "545870"
  },
  {
    "text": "the database says that\nTom is 20 years old. And if you ask what's\nmy age, the database says, basically, that\nit does not know,",
    "start": "545870",
    "end": "552260"
  },
  {
    "text": "because that's not been added. So this is really powerful. Here's another one.",
    "start": "552260",
    "end": "557860"
  },
  {
    "text": "Now, in this example,\nthe model is asked to blend concepts together. And so there's a\ndefinition of what",
    "start": "557860",
    "end": "564040"
  },
  {
    "text": "does it mean to blend concepts. So if you take\nairplane and car, you can blend that and\nget flying car.",
    "start": "564040",
    "end": "570959"
  },
  {
    "text": "That's essentially--\nthere's a Wikipedia definition of what\nconcept blending is, along",
    "start": "570960",
    "end": "576240"
  },
  {
    "text": "with some examples. Now, let's look at some\nproblems for what GPT-3 answers.",
    "start": "576240",
    "end": "583709"
  },
  {
    "text": "So the first one\nis straightforward. Two-dimensional space\nblended with 3D space gives",
    "start": "583710",
    "end": "588770"
  },
  {
    "text": "2.5-dimension space. The one that is somewhat\ninteresting is old and new",
    "start": "588770",
    "end": "594050"
  },
  {
    "text": "gets recycled. Then, triangle and\nsquare gives trapezoid.",
    "start": "594050",
    "end": "599810"
  },
  {
    "text": "That's also interesting. The one that's really nontrivial\nis a geology plus neurology",
    "start": "599810",
    "end": "606500"
  },
  {
    "text": "is sediment neurology, and\nI had no idea what this was. It's apparently correct.",
    "start": "606500",
    "end": "611660"
  },
  {
    "text": "So clearly, it's able to do\nthese very flexible things just from a prompt.",
    "start": "611660",
    "end": "618389"
  },
  {
    "text": "So here's another\nclass of examples that GPT-3 gets somewhat\nright, and these",
    "start": "618390",
    "end": "626130"
  },
  {
    "text": "are these copycat\nanalogy problems which have been\nreally well-studied in cognitive science.",
    "start": "626130",
    "end": "631980"
  },
  {
    "text": "And the way it works\nis that I'm going to give you some\nexamples, and then ask you to induce a\nfunction from these examples",
    "start": "631980",
    "end": "639060"
  },
  {
    "text": "and apply it to a new query. So if a b c changes to a b\nd, what does p q i change to?",
    "start": "639060",
    "end": "645150"
  },
  {
    "text": "Well, p q i must\nchange to p q s, because the function you've done\nis that the last letter must be incremented by 1.",
    "start": "645150",
    "end": "651870"
  },
  {
    "text": "And this function,\nhumans can now apply to examples\nof varying types.",
    "start": "651870",
    "end": "657130"
  },
  {
    "text": "So p repeated twice, q repeated\ntwice, r repeated twice must change to p repeated\ntwice, q repeated twice,",
    "start": "657130",
    "end": "663810"
  },
  {
    "text": "and s repeated twice. And it seems like GPT-3\nis able to get them right,",
    "start": "663810",
    "end": "669300"
  },
  {
    "text": "more or less, but the\nproblem is that if you ask it to generalize to\nexamples that have an increasing",
    "start": "669300",
    "end": "678660"
  },
  {
    "text": "number of repetitions,\nthen, that we're seeing in the prompt,\nit's not able to do that. So in this situation, you\nask it to make an analogy",
    "start": "678660",
    "end": "688140"
  },
  {
    "text": "where the letters are\nrepeated four times. And it's never seen that before. It doesn't know what to do.",
    "start": "688140",
    "end": "694600"
  },
  {
    "text": "And so it gets all\nof these wrong. So there's a point\nto be made here about just maybe\nthese prompts are not",
    "start": "694600",
    "end": "701700"
  },
  {
    "text": "enough to convey the function\nthe model should be learning, and maybe with more\nexamples, it can be done.",
    "start": "701700",
    "end": "707649"
  },
  {
    "text": "But the point is\nthat it probably does not have the same\nkind of generalization",
    "start": "707650",
    "end": "713880"
  },
  {
    "text": "that humans have. And that brings us to sort of\nlimitations of these models",
    "start": "713880",
    "end": "719190"
  },
  {
    "text": "and some open questions. So just looking at the paper\nand parsing through the results,",
    "start": "719190",
    "end": "724690"
  },
  {
    "text": "it seems like the model is bad\nat logical and mathematical reasoning, and anything\nthat involves doing",
    "start": "724690",
    "end": "731220"
  },
  {
    "text": "multiple steps of reasoning. And that explains why\nit's bad at arithmetic, why it's bad at word\nproblems, why it's not",
    "start": "731220",
    "end": "738630"
  },
  {
    "text": "great at analogy-making. And even like traditional\ntextual intermediate datasets that seem to require\nlogical reasoning, like RTE.",
    "start": "738630",
    "end": "747579"
  },
  {
    "text": "So the second, more\nsubtle point is that it's unclear\nhow we can make permanent updates to the model.",
    "start": "747580",
    "end": "753310"
  },
  {
    "text": "Maybe if I wanted you\nto model a new concept, that's possible\nto do it while I'm interacting with the system.",
    "start": "753310",
    "end": "759320"
  },
  {
    "text": "But once the interaction's\nover, it kind of restarts and does not have\na notion of knowledge. And it's not that this is\nsomething that the model cannot",
    "start": "759320",
    "end": "766600"
  },
  {
    "text": "do in principle, but just\nsomething that's not really been explored.",
    "start": "766600",
    "end": "771880"
  },
  {
    "text": "It doesn't seem to exhibit\nhuman-like generalizations, often called systematicity, and\nI talk a lot more about that.",
    "start": "771880",
    "end": "778670"
  },
  {
    "text": "And finally,\nlanguage is situated, and GPT-3 is just\nlearning from text, and there's no exposure\nto other modalities.",
    "start": "778670",
    "end": "784770"
  },
  {
    "text": "There's no interaction. So maybe the aspects of\nmeaning that it acquires are somewhat\nlimited, and maybe we",
    "start": "784770",
    "end": "790600"
  },
  {
    "text": "should explore how we can\nbring in other modalities. So we'll talk a lot more about\nthese last few limitations",
    "start": "790600",
    "end": "798560"
  },
  {
    "text": "in the rest of the\nlecture, but maybe I can pause for some questions\nnow, if there are any.",
    "start": "798560",
    "end": "803650"
  },
  {
    "start": "803650",
    "end": "814190"
  },
  {
    "text": "I don't think there's a\nbig, outstanding question, but I mean, I think some people\naren't really clear on few-shot",
    "start": "814190",
    "end": "823600"
  },
  {
    "text": "setting and prompting\nversus learning, and I think it might\nactually be good to explain that a bit more.",
    "start": "823600",
    "end": "829630"
  },
  {
    "text": "OK, yeah. So maybe let me take\na simple example.",
    "start": "829630",
    "end": "837105"
  },
  {
    "start": "837105",
    "end": "842430"
  },
  {
    "text": "Let me take this example here. So prompting just\nmeans that-- so GPT-3,",
    "start": "842430",
    "end": "848802"
  },
  {
    "text": "if you go back to\nfirst principles, right, GPT-3 is basically\njust a language model. And what that means\nis, given a context,",
    "start": "848802",
    "end": "855635"
  },
  {
    "text": "it will tell you what's the\nprobability of the next word, right? So if I can give it a\ncontext w1 through wk,",
    "start": "855635",
    "end": "864826"
  },
  {
    "text": "GPT-3 will tell me what's\nthe probability of WK+1",
    "start": "864826",
    "end": "870360"
  },
  {
    "text": "for [INAUDIBLE] the vocabulary. So that's what a\nlanguage model is.",
    "start": "870360",
    "end": "876000"
  },
  {
    "text": "A prompt is\nessentially a context that gets prepended before\nGPT-3 can start generating.",
    "start": "876000",
    "end": "882660"
  },
  {
    "text": "And what's happening\nwith in-context learning is that the context\nthat you append,",
    "start": "882660",
    "end": "888630"
  },
  {
    "text": "that you prepend to GPT-3\nare basically x, y examples.",
    "start": "888630",
    "end": "895050"
  },
  {
    "text": "So that's the prompt, and\nthe reason why it's also-- it's equivalent to\nfew-shot learning",
    "start": "895050",
    "end": "901500"
  },
  {
    "text": "is because you prepend a\nsmall number of x, y examples. So in this case, if I just\nprepend this one example that's",
    "start": "901500",
    "end": "909390"
  },
  {
    "text": "highlighted in\npurple, then that's essentially one-shot learning\nbecause I just give it a single example as context.",
    "start": "909390",
    "end": "916589"
  },
  {
    "text": "And now, given this query, which\nis also appended to the model,",
    "start": "916590",
    "end": "921860"
  },
  {
    "text": "it has to make a prediction. So the input/output\nformat is the same",
    "start": "921860",
    "end": "927870"
  },
  {
    "text": "as how a few-shot\nlearner would receive, but since it's a language\nmodel, the training dataset",
    "start": "927870",
    "end": "935430"
  },
  {
    "text": "is essentially presented\nas the context. ",
    "start": "935430",
    "end": "943620"
  },
  {
    "text": "So someone is still\nasking, can you be more specific about the\nin-context learning setups?",
    "start": "943620",
    "end": "948720"
  },
  {
    "text": "What is the task? Right, so let's see.",
    "start": "948720",
    "end": "954430"
  },
  {
    "text": "Maybe I can go to-- ",
    "start": "954430",
    "end": "961271"
  },
  {
    "text": "yeah, so maybe I can\ngo to this slide. So the task is just that\nit's a language model.",
    "start": "961271",
    "end": "968620"
  },
  {
    "text": "So it gets a context, which\nis just a sequence of tokens, and the task is just to--",
    "start": "968620",
    "end": "976589"
  },
  {
    "text": "so you have a\nsequence of tokens, and then the model\nhas to generate, given a sequence of tokens.",
    "start": "976590",
    "end": "983070"
  },
  {
    "text": "And the way you can convert that\ninto an actual machine learning classification problem is that--",
    "start": "983070",
    "end": "989170"
  },
  {
    "text": "so for this example, maybe you\ngive it 5 plus 8 equals 13, 7 plus 2 equals 9, and\nthen 1 plus 0 equals,",
    "start": "989170",
    "end": "997560"
  },
  {
    "text": "and now, GPT-3 can\nfill in a number there. And so that's how you convert it\ninto a classification problem.",
    "start": "997560",
    "end": "1004730"
  },
  {
    "text": "The context here would be these\ntwo examples of arithmetic, like 5 plus 8 equals 13\nand 7 plus 2 equals 9.",
    "start": "1004730",
    "end": "1012380"
  },
  {
    "text": "And then, the query\nis 1 plus 0 equals. And then, the model, since\nit's just a language model, it has to fill in 1 plus\n0 equals question mark.",
    "start": "1012380",
    "end": "1019878"
  },
  {
    "text": "So it fills in something there. It doesn't have to\nfill in numbers; it could fill in anything. But if it fills in a 1,\nit does the right job.",
    "start": "1019878",
    "end": "1029449"
  },
  {
    "text": "So that's how you can\ntake a language model into few-shot learning with it.",
    "start": "1029450",
    "end": "1034980"
  },
  {
    "text": "I'll keep on these questions. How is in-context learning\ndifferent from transfer learning? ",
    "start": "1034980",
    "end": "1044010"
  },
  {
    "text": "So I guess the--  in-context learning,\nI mean, you can",
    "start": "1044010",
    "end": "1049520"
  },
  {
    "text": "think of in-context learning\nas being a kind of transfer learning, but transfer\nlearning does not",
    "start": "1049520",
    "end": "1054890"
  },
  {
    "text": "specify the mechanism\nthrough which the transfer is going to happen. With in-context\nlearning, the mechanism",
    "start": "1054890",
    "end": "1061370"
  },
  {
    "text": "is that the training\nexamples are sort of appended to the model, which\nis a language model, just",
    "start": "1061370",
    "end": "1070460"
  },
  {
    "text": "in order. So let's say you have\nx, y, x1, y1, x2, y2,",
    "start": "1070460",
    "end": "1075740"
  },
  {
    "text": "and these are just appended\ndirectly to the model. And now it makes prediction on\nsome queries, some queries that",
    "start": "1075740",
    "end": "1083720"
  },
  {
    "text": "are drawn from this data set. So yes, it is a subcategory\nof transfer learning, but transfer learning\ndoes not specify",
    "start": "1083720",
    "end": "1092000"
  },
  {
    "text": "exactly how this transfer\nlearning is achieved. But in-context learning\nis very specific and says that for\nlanguage models,",
    "start": "1092000",
    "end": "1098419"
  },
  {
    "text": "you can essentially concatenate\nthe training data set and then present that\nto the language model.",
    "start": "1098420",
    "end": "1105016"
  },
  {
    "text": "If people still aren't\nsufficiently clear on what is and what isn't happening\nwith learning and prompting,",
    "start": "1105016",
    "end": "1113820"
  },
  {
    "text": "so another question is so\nin-context learning still needs fine tuning question mark?",
    "start": "1113820",
    "end": "1119280"
  },
  {
    "text": "We need to train GPT-3 to do\nin-context learning, question mark?",
    "start": "1119280",
    "end": "1124450"
  },
  {
    "text": "Right, so there are two parts\nto this question, right?",
    "start": "1124450",
    "end": "1131370"
  },
  {
    "text": "So the answer is, yes and no. So of course, the model\nis a language model.",
    "start": "1131370",
    "end": "1136460"
  },
  {
    "text": "So it needs to be trained. So you start with some\nrandom parameters, and you need to train them.",
    "start": "1136460",
    "end": "1141870"
  },
  {
    "text": "But the model is trained\nas a language model, right? And once the model\nis trained, you",
    "start": "1141870",
    "end": "1147780"
  },
  {
    "text": "can now use it to do\ntransfer learning. And the model parameters in\nin-context learning are fixed.",
    "start": "1147780",
    "end": "1154950"
  },
  {
    "text": "You do not update\nthe model parameters. All you do is that you give\nit these small training",
    "start": "1154950",
    "end": "1161429"
  },
  {
    "text": "sets to the model,\nwhich is just appended to the model as context,\nand now the model",
    "start": "1161430",
    "end": "1166680"
  },
  {
    "text": "can start generating\nfrom that point on. So in this example, if 5 plus 8\nequals 13 and 7 plus 2 equals 9",
    "start": "1166680",
    "end": "1174980"
  },
  {
    "text": "are two x, y examples, in\nvanilla transfer learning, what",
    "start": "1174980",
    "end": "1180380"
  },
  {
    "text": "you would do is that you would\ntake some gradient steps, update your model parameters,\nand then make a prediction on 1 plus 0 equals what, right?",
    "start": "1180380",
    "end": "1187450"
  },
  {
    "text": "But with in-context\nlearning, all you're doing is you just concatenate 5\nplus 8 equals 13 and 7 plus 2",
    "start": "1187450",
    "end": "1195020"
  },
  {
    "text": "equals 9 through the\nmodel's context window and then make it predict what\n1 plus 0 should be equal to.",
    "start": "1195020",
    "end": "1202549"
  },
  {
    "start": "1202550",
    "end": "1208790"
  },
  {
    "text": "Maybe we should end\nfor now with one other bigger-picture\nquestion, which is, do you know of\nany research combining",
    "start": "1208790",
    "end": "1215420"
  },
  {
    "text": "these models with reinforcement\nlearning for the more complicated reasoning tasks? So that is an\nexcellent question.",
    "start": "1215420",
    "end": "1222539"
  },
  {
    "text": "There is some\nrecent work on kind of trying to align language\nmodels with human preferences,",
    "start": "1222540",
    "end": "1230120"
  },
  {
    "text": "where yes, there is some\namount of fine-tuning",
    "start": "1230120",
    "end": "1235309"
  },
  {
    "text": "with reinforcement\nlearning based on these preferences\nfrom humans. So maybe you want to do a\nsummarization problem in GPT-3,",
    "start": "1235310",
    "end": "1242420"
  },
  {
    "text": "and the model produces\nmultiple summaries. And for each summary,\nmaybe you have a reward",
    "start": "1242420",
    "end": "1248163"
  },
  {
    "text": "that is essentially\na human preference-- like maybe I want to\ninclude some fact, and I don't want to include\nsome other non-important fact.",
    "start": "1248163",
    "end": "1255870"
  },
  {
    "text": "And so I can construct\na reward out of that, and I can fine-tune the\nparameters of my language model",
    "start": "1255870",
    "end": "1262070"
  },
  {
    "text": "basically using\nreinforcement learning based on this reward, which is\nessentially human preferences.",
    "start": "1262070",
    "end": "1268700"
  },
  {
    "text": "So there's some very recent\nwork that tries to do this. But I'm not sure-- yeah, I'm not aware\nof any work that",
    "start": "1268700",
    "end": "1273769"
  },
  {
    "text": "tries to use reinforcement\nlearning to teach reasoning to these models,\nbut I think it's an interesting future\ndirection to explore.",
    "start": "1273770",
    "end": "1281140"
  },
  {
    "start": "1281140",
    "end": "1287920"
  },
  {
    "text": "OK, maybe you should\ngo on at this point. OK. ",
    "start": "1287920",
    "end": "1297409"
  },
  {
    "text": "OK, so we'll talk a bit more\nabout these last two points-- so systematicity and\nlanguage grounding.",
    "start": "1297410",
    "end": "1307950"
  },
  {
    "text": "So just to start off, how\ndo you define systematicity? So really, the\ndefinition is that there",
    "start": "1307950",
    "end": "1314520"
  },
  {
    "text": "is a definite and predictable\npattern among the sentences that native speakers of\na language understand.",
    "start": "1314520",
    "end": "1320130"
  },
  {
    "text": "And so there's a systematic\npattern among the sentences that we understand. What that means is, let's say\nthere's a sentence like, \"John",
    "start": "1320130",
    "end": "1327299"
  },
  {
    "text": "loves Mary\" right? And if a native speaker\nunderstands the sentence, then they should also be able\nto understand the sentence,",
    "start": "1327300",
    "end": "1333280"
  },
  {
    "text": "\"Mary loves John.\"  And closely related to\nthis idea of systematicity",
    "start": "1333280",
    "end": "1339180"
  },
  {
    "text": "is the principle of\ncompositionality. And for now, I'm going to ignore\nthe definition by Montague",
    "start": "1339180",
    "end": "1345367"
  },
  {
    "text": "and just look at the\nrough definition, and then we can come\nback to this other, more concrete definition. But the rough definition\nis essentially",
    "start": "1345367",
    "end": "1351870"
  },
  {
    "text": "that the meaning\nof an expression is a function of the\nmeaning of its parts.",
    "start": "1351870",
    "end": "1357700"
  },
  {
    "text": "So that brings us\nto the question, are human languages\nreally compositional? And here are some examples that\nmake us think that maybe yes.",
    "start": "1357700",
    "end": "1367710"
  },
  {
    "text": "So if you look at,\nwhat is the meaning of the noun phrase\n\"brown cow,\" so it is composed of the\nmeaning of the adjective",
    "start": "1367710",
    "end": "1375389"
  },
  {
    "text": "brown and the noun cow. So all the things that are brown\nand all things that are cow,",
    "start": "1375390",
    "end": "1380970"
  },
  {
    "text": "take the intersection\nand get \"brown cow.\" Similarly, red rabbits, it's\nall things that are red, all the things that are rabbits,\ncombine them, get red rabbits.",
    "start": "1380970",
    "end": "1387630"
  },
  {
    "text": "And then, kicked the\nball, this verb phrase can be understood as you\nhave some agent that's performing a kicking\noperation on the ball.",
    "start": "1387630",
    "end": "1396060"
  },
  {
    "text": "But this is not always\nthe case, that you can get at the meaning\nof the whole thing",
    "start": "1396060",
    "end": "1401550"
  },
  {
    "text": "by combining meanings of parts. So here you have\nsome counter-examples that people often use.",
    "start": "1401550",
    "end": "1406710"
  },
  {
    "text": "So a red herring does not mean\nall the things that are red and all things that\nare herring, and \"kick the bucket\" definitely\ndoes not mean",
    "start": "1406710",
    "end": "1413500"
  },
  {
    "text": "that there's an agent\nthat's kicking the bucket. So while these examples are\nsupposed to be provocative--",
    "start": "1413500",
    "end": "1420360"
  },
  {
    "text": "we think that language is\nmostly compositional-- there's lots of exceptions. But for the vast majority of\nsentences that we've never",
    "start": "1420360",
    "end": "1427380"
  },
  {
    "text": "heard before, we're\nable to understand what they mean by piecing\ntogether the words that the sentence\nis composed of.",
    "start": "1427380",
    "end": "1433827"
  },
  {
    "text": "And so what that means is\nthat maybe compositionality of representations is a\nhelpful prior that could lead",
    "start": "1433827",
    "end": "1438960"
  },
  {
    "text": "to systematicity in behavior. And that brings us\nto the questions that we ask in the segment.",
    "start": "1438960",
    "end": "1445289"
  },
  {
    "text": "Are neural representations\ncompositional? And the second question\nis, if so, do they",
    "start": "1445290",
    "end": "1450600"
  },
  {
    "text": "generalize systematically? So how do you even\nmeasure if representations",
    "start": "1450600",
    "end": "1458610"
  },
  {
    "text": "that a neural network learns\nexhibit compositionality? So let's go back\nto this definition",
    "start": "1458610",
    "end": "1465870"
  },
  {
    "text": "from Montague, which says\nthat compositionality is about the existence of\na homomorphism from syntax",
    "start": "1465870",
    "end": "1472260"
  },
  {
    "text": "to semantics. And to look at that, we\nhave this example, which",
    "start": "1472260",
    "end": "1478049"
  },
  {
    "text": "is, Lisa does not skateboard. And we have a syntax tree\ncorresponding to this example.",
    "start": "1478050",
    "end": "1484260"
  },
  {
    "text": "And the meaning of\nthe sentence can be composed according\nto the structure that's",
    "start": "1484260",
    "end": "1491520"
  },
  {
    "text": "decided by the syntax. So meaning of \"Lisa\ndoes not skateboard\" is a function of the\nmeaning of \"Lisa\"",
    "start": "1491520",
    "end": "1497100"
  },
  {
    "text": "and \"does not skateboard.\" The meaning of \"does\nnot skateboard\" is a function of \"does\"\nand \"not skateboard.\" The meaning of \"not skateboard\"\nis a function of \"not\"",
    "start": "1497100",
    "end": "1504165"
  },
  {
    "text": "and \"skateboard.\" So that's good, and\nso this gives us one way of formalizing how we\ncan measure compositionality",
    "start": "1504165",
    "end": "1511410"
  },
  {
    "text": "in neural representations. And so compositionality of\nrepresentations could be",
    "start": "1511410",
    "end": "1517559"
  },
  {
    "text": "thought of as how well the\nrepresentation approximates an explicitly homomorphic\nfunction in a learnt",
    "start": "1517560",
    "end": "1524760"
  },
  {
    "text": "representation space. So what we are going to\ndo is essentially measure, if we were to construct a neural\nnetwork whose computations are",
    "start": "1524760",
    "end": "1533190"
  },
  {
    "text": "based exactly according\nto these parse trees, how far are the representations\nof a learnt model from this",
    "start": "1533190",
    "end": "1539850"
  },
  {
    "text": "explicitly compositional\nrepresentation? And that will give\nus some understanding",
    "start": "1539850",
    "end": "1545550"
  },
  {
    "text": "of how compositional\nthe neural networks' representations really are. So to unpack that a little\nbit, instead of having--",
    "start": "1545550",
    "end": "1555370"
  },
  {
    "text": "yeah, so instead of\nhaving denotations, we have representations\nin the node.",
    "start": "1555370",
    "end": "1563230"
  },
  {
    "text": "And to kind of be more\nconcrete about that, we first start by\nchoosing a distance",
    "start": "1563230",
    "end": "1568840"
  },
  {
    "text": "function that tells us how far\naway two representations are. And then, you also need\na way to compose together",
    "start": "1568840",
    "end": "1575230"
  },
  {
    "text": "two constituents to give us sort\nof the meaning of the whole.",
    "start": "1575230",
    "end": "1581530"
  },
  {
    "text": "But once we have that,\nwe can start by-- we can create an explicitly\ncompositional function, right?",
    "start": "1581530",
    "end": "1587799"
  },
  {
    "text": "So what we do is we have\nthese representations",
    "start": "1587800",
    "end": "1593770"
  },
  {
    "text": "at the leaves that are\ninitialized randomly, and the composition function\nthat's also initialized",
    "start": "1593770",
    "end": "1599670"
  },
  {
    "text": "randomly, and then a forward\npass according to this syntax, is used to compute the\nrepresentation of \"Lisa",
    "start": "1599670",
    "end": "1606550"
  },
  {
    "text": "does not skateboard.\" And once you have\nthis representation, you can create a loss function.",
    "start": "1606550",
    "end": "1611980"
  },
  {
    "text": "And this loss function\nmeasures, how far are the representations\nof my neural network",
    "start": "1611980",
    "end": "1617169"
  },
  {
    "text": "from this second sort\nof proxy neural network that I've created? And then I can basically\noptimize both the composition",
    "start": "1617170",
    "end": "1626500"
  },
  {
    "text": "function and the\nembeddings of the leaves. And then, once the\noptimization is finished,",
    "start": "1626500",
    "end": "1632380"
  },
  {
    "text": "I can measure, how far\nwas the representation of my neural net from this\nexplicitly compositional",
    "start": "1632380",
    "end": "1639820"
  },
  {
    "text": "network on a handout set? And that then tells me\nwhether the representations that my neural net learned were\nactually compositional or not.",
    "start": "1639820",
    "end": "1648600"
  },
  {
    "text": "So to see how well this\nworks, let's look at a plot. And this is relatively\ncomplex, but just",
    "start": "1648600",
    "end": "1657860"
  },
  {
    "text": "to unpack this a little bit,\nit plots the mutual information",
    "start": "1657860",
    "end": "1663290"
  },
  {
    "text": "between the input that the\nneural network receives versus",
    "start": "1663290",
    "end": "1669050"
  },
  {
    "text": "the representation against\nthis tree reconstruction error that we were talking about.",
    "start": "1669050",
    "end": "1675380"
  },
  {
    "text": "And to give some more\nbackground about what's to come, there is a theory which\nis called the information",
    "start": "1675380",
    "end": "1683000"
  },
  {
    "text": "bottleneck theory, which\nsays that as a neural network trains, it first\ntries to maximize",
    "start": "1683000",
    "end": "1691789"
  },
  {
    "text": "the mutual information\nbetween the representation and the input in an attempt to\nmemorize the entire data set.",
    "start": "1691790",
    "end": "1698490"
  },
  {
    "text": "And that is called-- that is a memorization phase. And then, once\nmemorization is done,",
    "start": "1698490",
    "end": "1704669"
  },
  {
    "text": "there is a learning or a\ncompression phase where this mutual information\nstarts to decrease",
    "start": "1704670",
    "end": "1710850"
  },
  {
    "text": "and the model is essentially\ntrying to compress the data or consolidate the knowledge in\nthe data into its parameters.",
    "start": "1710850",
    "end": "1717870"
  },
  {
    "text": "And what we're seeing here\nis that as the model learns-- which is characterized by\ndecreasing mutual information--",
    "start": "1717870",
    "end": "1724890"
  },
  {
    "text": "we see that the\nrepresentations themselves are becoming more and\nmore compositional.",
    "start": "1724890",
    "end": "1730230"
  },
  {
    "text": "And overall, we\nobserve that learning is correlated with increased\ncompositionality as measured",
    "start": "1730230",
    "end": "1735810"
  },
  {
    "text": "by this tree\nreconstruction error. So that's really encouraging.",
    "start": "1735810",
    "end": "1741790"
  },
  {
    "text": "So now that we have a method\nof measuring compositionality,",
    "start": "1741790",
    "end": "1746810"
  },
  {
    "text": "of representations\nin these neural nets, how do we start to\ncreate benchmarks?",
    "start": "1746810",
    "end": "1753627"
  },
  {
    "text": "Let's see if they are\ngeneralizing systematically or not. So to do that, here is a\nmethod for taking any data set",
    "start": "1753628",
    "end": "1762140"
  },
  {
    "text": "and splitting it into\na train/test split that explicitly tests for this\nkind of generalization.",
    "start": "1762140",
    "end": "1769007"
  },
  {
    "text": " So to do that, we use this\nprinciple called maximizing",
    "start": "1769007",
    "end": "1776059"
  },
  {
    "text": "the compound divergence. And to illustrate how\nthis principle works, we look at the store example.",
    "start": "1776060",
    "end": "1783660"
  },
  {
    "text": "So in the store example,\nwe have a training data set that consists\nof just two examples and a test data set\nof just two examples.",
    "start": "1783660",
    "end": "1791640"
  },
  {
    "text": "The atoms are defined as sort\nof the primitive elements-- so entity words,\npredicates, question types.",
    "start": "1791640",
    "end": "1798870"
  },
  {
    "text": "So in the store example,\nGoldfinger, Christopher Nolan, these are all sort of\nthe primitive elements,",
    "start": "1798870",
    "end": "1805280"
  },
  {
    "text": "and the compounds\nare the compositions of these primitive elements. So, \"Who directed entity?\"",
    "start": "1805280",
    "end": "1810350"
  },
  {
    "text": "would be the composition\nof the question type \"did x predicate y\" and\nthen the predicate direct.",
    "start": "1810350",
    "end": "1815705"
  },
  {
    "text": " So here's a basic machinery\nfor producing compositionally",
    "start": "1815705",
    "end": "1821670"
  },
  {
    "text": "challenging splits. So let's start by introducing\ntwo distributions.",
    "start": "1821670",
    "end": "1827310"
  },
  {
    "text": "So first distribution is\nthe normalized frequency distribution of the atoms.",
    "start": "1827310",
    "end": "1832830"
  },
  {
    "text": "So given any data set, if we\nknow what the notion of atoms are, we can basically\ncompute the frequency of all",
    "start": "1832830",
    "end": "1840300"
  },
  {
    "text": "of the atoms and then normalize\nthat by the total count, and that's going to give\nus one distribution.",
    "start": "1840300",
    "end": "1847049"
  },
  {
    "text": "And we can repeat the same\nthing for the compounds, and that will give us a\nsecond frequency distribution.",
    "start": "1847050",
    "end": "1853600"
  },
  {
    "text": "So note that these are just\ntwo probability distributions, and once we have these\ntwo distributions,",
    "start": "1853600",
    "end": "1860280"
  },
  {
    "text": "we can essentially define the\natom and compound divergence simply as this quantity here.",
    "start": "1860280",
    "end": "1868440"
  },
  {
    "text": "And where there is the Chernoff\ncoefficient between two categorical distributions,\nthe Chernoff coefficient",
    "start": "1868440",
    "end": "1876360"
  },
  {
    "text": "basically measures how far two\ncategorical distributions are. So just to get a bit more\nintuition about this,",
    "start": "1876360",
    "end": "1883559"
  },
  {
    "text": "if we set p to q, then\nthe Chernoff coefficient is 1, which means these\nrepresentations are maximally",
    "start": "1883560",
    "end": "1891210"
  },
  {
    "text": "similar. And then, if p is non-zero,\neverywhere q is 0--",
    "start": "1891210",
    "end": "1897990"
  },
  {
    "text": "or if p is 0 in all the\nplaces where q is 0-- then the Chernoff\ncoefficient is exactly 0,",
    "start": "1897990",
    "end": "1905190"
  },
  {
    "text": "which means that these two\ndistributions are maximally far away. And the overall goal by\ndescribing this objective",
    "start": "1905190",
    "end": "1914310"
  },
  {
    "text": "is that-- this loss objective\nis just that we are going to maximize\nthe compound divergence",
    "start": "1914310",
    "end": "1921360"
  },
  {
    "text": "and minimize the\natom divergence. And so what is the intuition\nbehind doing such a thing?",
    "start": "1921360",
    "end": "1926380"
  },
  {
    "text": "So what we want is to ensure\nthat the unigram distribution, in some sense, is constant\nbetween the train and test",
    "start": "1926380",
    "end": "1933090"
  },
  {
    "text": "split so that the model does\nnot encounter any new words.",
    "start": "1933090",
    "end": "1938610"
  },
  {
    "text": "But we want the compound\ndivergence to be very high, which means that these same\nwords that the model has seen",
    "start": "1938610",
    "end": "1945660"
  },
  {
    "text": "many times must appear in\nnew combinations, which means that we are testing\nfor systematicity.",
    "start": "1945660",
    "end": "1953140"
  },
  {
    "text": "And so if you follow\nthis procedure for a semantic parsing data\nset, let's say, what we see",
    "start": "1953140",
    "end": "1961149"
  },
  {
    "text": "is that as you\nincrease the scale, we see that this\nmodel just does better",
    "start": "1961150",
    "end": "1966400"
  },
  {
    "text": "and better at a\ncompositional generalization. But just pulling out a\nquote from this paper,",
    "start": "1966400",
    "end": "1973245"
  },
  {
    "text": "pre-training helps for\ncompositional generalization but doesn't fully solve it. And what that means\nis that maybe,",
    "start": "1973245",
    "end": "1978669"
  },
  {
    "text": "as you keep scaling\nup these models, you'll see better and\nbetter performance, or maybe it starts to\nsaturate at some point.",
    "start": "1978670",
    "end": "1986722"
  },
  {
    "text": "In any case, we\nshould probably be thinking more\nabout this problem, instead of just trying\nto brute-force it. ",
    "start": "1986723",
    "end": "1994060"
  },
  {
    "text": "So now, this segment\nkind of tells us that the way we\nsplit a data set,",
    "start": "1994060",
    "end": "1999070"
  },
  {
    "text": "we can measure for\ndifferent kinds of-- we can measure different\nbehaviors of the model.",
    "start": "1999070",
    "end": "2005227"
  },
  {
    "text": "And that tells us\nthat maybe we should be thinking more critically\nabout how we're evaluating models in NLP in general.",
    "start": "2005227",
    "end": "2011340"
  },
  {
    "text": "So there has been a\nrevolution, basically, over the last few\nyears in the field",
    "start": "2011340",
    "end": "2016710"
  },
  {
    "text": "where we're seeing all of\nthese large transform models beat all of our benchmarks. At the same time, there is\nstill not complete confidence",
    "start": "2016710",
    "end": "2024972"
  },
  {
    "text": "that once you deploy these\nsystems in the real world, they're going to be-- going to maintain\ntheir performance.",
    "start": "2024972",
    "end": "2031740"
  },
  {
    "text": "And so it's unclear\nif these gains are coming from spurious\ncorrelations or some real task understanding.",
    "start": "2031740",
    "end": "2037080"
  },
  {
    "text": "And so how do we\ndesign benchmarks that accurately tell us\nhow well this model is going to do in the real world?",
    "start": "2037080",
    "end": "2043809"
  },
  {
    "text": "And so I'm going\nto give one example of works that try to\ndo this, and that's the idea of dynamic benchmarks.",
    "start": "2043810",
    "end": "2052780"
  },
  {
    "text": "And the idea of\ndynamic benchmarks is basically saying that\ninstead of testing our models",
    "start": "2052780",
    "end": "2058949"
  },
  {
    "text": "on static test sets, we\nshould be evaluating them on an ever-changing\ndynamic benchmark.",
    "start": "2058949",
    "end": "2064138"
  },
  {
    "text": "And there's many recent\nexamples of this, and the idea dates back to\na 2017 workshop at EMNLP.",
    "start": "2064139",
    "end": "2073199"
  },
  {
    "text": "And so the overall schematic\nlooks something like this-- that we start with a training\ndata set and a test data set,",
    "start": "2073199",
    "end": "2079340"
  },
  {
    "text": "which is the static corpora. We train a model\non that, and then, once the model is trained,\nwe deploy that and then",
    "start": "2079340",
    "end": "2087770"
  },
  {
    "text": "have humans create new\nexamples that the model fails to classify. And crucially, we're looking\nfor examples the model does not",
    "start": "2087770",
    "end": "2095540"
  },
  {
    "text": "get right, but humans\nhave no issue figuring out the answer to. So by playing this game\nof whack-a-mole where",
    "start": "2095540",
    "end": "2102740"
  },
  {
    "text": "we humans figure\nout what are sort of the holes in the\nmodel's understanding,",
    "start": "2102740",
    "end": "2108319"
  },
  {
    "text": "and then add that back\ninto the training data, retrain the model, deploy\nit again, have humans create",
    "start": "2108320",
    "end": "2113520"
  },
  {
    "text": "new examples, we can\nessentially construct this never-ending data\nset, this never-ending test",
    "start": "2113520",
    "end": "2120500"
  },
  {
    "text": "set which can hopefully be\na better proxy of estimating",
    "start": "2120500",
    "end": "2126390"
  },
  {
    "text": "real-world performance.  So this is some very\ncutting-edge research,",
    "start": "2126390",
    "end": "2133460"
  },
  {
    "text": "and one of the main challenges\nof this class of works is that it's unclear how\nmuch this can scale up.",
    "start": "2133460",
    "end": "2139790"
  },
  {
    "text": "Because maybe after\nmultiple iterations",
    "start": "2139790",
    "end": "2145010"
  },
  {
    "text": "of this whack-a-mole, humans\nare just fundamentally limited by creativity. So figuring out how to deal with\nthat is really an open problem.",
    "start": "2145010",
    "end": "2154820"
  },
  {
    "text": "And current approaches just\nuse examples from other data sets to prompt humans to\nthink more creatively.",
    "start": "2154820",
    "end": "2161029"
  },
  {
    "text": "But maybe we can come up with\nbetter, more automated methods of doing this.",
    "start": "2161030",
    "end": "2168090"
  },
  {
    "text": "So this brings us to sort\nof the final segment. Or actually, let me stop\nfor questions at this point",
    "start": "2168090",
    "end": "2175310"
  },
  {
    "text": "and see if people\nhave questions. ",
    "start": "2175310",
    "end": "2188350"
  },
  {
    "text": "Here's a question. With dynamic\nbenchmark, doesn't this mean that the model\ncreated will also",
    "start": "2188350",
    "end": "2194230"
  },
  {
    "text": "need to continually\ntest/evaluate the models on the new\nbenchmarks, new data test,",
    "start": "2194230",
    "end": "2200829"
  },
  {
    "text": "new data sets? Wait a second. ",
    "start": "2200830",
    "end": "2209700"
  },
  {
    "text": "Sorry, yeah. So with dynamic\nbenchmarks, yes, it's absolutely true that you will\nhave to continuously keep",
    "start": "2209700",
    "end": "2217630"
  },
  {
    "text": "training your model. And that's just to ensure that\nthe reason your model is not",
    "start": "2217630",
    "end": "2224470"
  },
  {
    "text": "doing well on the test\nset doesn't have to do with this domain mismatch.",
    "start": "2224470",
    "end": "2229540"
  },
  {
    "text": "And what we are really trying\nto do is v measure how--",
    "start": "2229540",
    "end": "2234610"
  },
  {
    "text": "just come up with\na better estimate of the model's performance\non the overall task,",
    "start": "2234610",
    "end": "2239859"
  },
  {
    "text": "and just trying to get\nmore and more data. So yes, to answer\nyour question, yes, we",
    "start": "2239860",
    "end": "2245290"
  },
  {
    "text": "need to keep lecturing\nthe model again and again. But this can be automated. OK, so I'll move on\nto language grounding.",
    "start": "2245290",
    "end": "2255640"
  },
  {
    "text": "So in this final\nsegment, I'll talk about how we can move beyond\njust training models on text",
    "start": "2255640",
    "end": "2262840"
  },
  {
    "text": "alone. So many have\narticulated the need",
    "start": "2262840",
    "end": "2268690"
  },
  {
    "text": "to use modalities\nother than text if we some day want to get at\nreal language understanding.",
    "start": "2268690",
    "end": "2274900"
  },
  {
    "text": "And this has-- ever since we've\nhad these big language models,",
    "start": "2274900",
    "end": "2280809"
  },
  {
    "text": "there has been sort of a\nre-kindling of this debate. And recently, there was\nmultiple papers on this.",
    "start": "2280810",
    "end": "2286210"
  },
  {
    "text": "And so at ACL last year,\nthere was this paper that argues through\nmultiple thought experiments",
    "start": "2286210",
    "end": "2291910"
  },
  {
    "text": "that it's actually\nimpossible to acquire meaning from form alone,\nwhere meaning refers",
    "start": "2291910",
    "end": "2297490"
  },
  {
    "text": "to the communicative\nintent of a speaker and form refers to\ntext or speech signals.",
    "start": "2297490",
    "end": "2305500"
  },
  {
    "text": "A more moderate version\nof this was put forward by the second paper,\nwhere they say that training on\nonly web-scale data",
    "start": "2305500",
    "end": "2313270"
  },
  {
    "text": "limits the world scope\nof models and limits the aspects of meanings that\nthe model can actually acquire.",
    "start": "2313270",
    "end": "2321130"
  },
  {
    "text": "And so here's a\nlook at a diagram that I've borrowed\nfrom the paper. And what they say\nis the error where",
    "start": "2321130",
    "end": "2327550"
  },
  {
    "text": "we were training models on\nthe supervised data sets, models were limited\nin world scope 1.",
    "start": "2327550",
    "end": "2333789"
  },
  {
    "text": "And now that we've moved on\nto exploiting unlabeled data, we're now in world scope\n2, where models just",
    "start": "2333790",
    "end": "2340600"
  },
  {
    "text": "have strictly more signal to get\nmore aspects of meaning here. If you mix in additional\nmodalities to this--",
    "start": "2340600",
    "end": "2347780"
  },
  {
    "text": "so maybe you mix in videos,\nand maybe you mix in images-- then that expands out the whole\nscope of the model further,",
    "start": "2347780",
    "end": "2355690"
  },
  {
    "text": "and now, maybe it can apply\nmore aspects of meaning, such that it knows that the lexical\nitem \"red\" refers to red image",
    "start": "2355690",
    "end": "2365349"
  },
  {
    "text": "visibility. And then, if you\ngo beyond that, you can have a model\nthat is embodied,",
    "start": "2365350",
    "end": "2371560"
  },
  {
    "text": "and it's actually\nliving in an environment where it can interact with its\ndata, conduct interventions",
    "start": "2371560",
    "end": "2377960"
  },
  {
    "text": "and experiments. And then, if you go out,\ngo even beyond that, you can have models that\nlive in a social world",
    "start": "2377960",
    "end": "2384400"
  },
  {
    "text": "where they can interact\nwith other models. Because after all, the purpose\nof language is to communicate. And so you can have a social\nworld where models can",
    "start": "2384400",
    "end": "2394260"
  },
  {
    "text": "communicate with other models. That kind of expands out\naspects of the world.",
    "start": "2394260",
    "end": "2400070"
  },
  {
    "text": "And so GPT-3 is\nin world scope 2. So there are a lot of open\nquestions in this space.",
    "start": "2400070",
    "end": "2407540"
  },
  {
    "text": "So given that there are all of\nthese good arguments about how we need to move\nbeyond text, what",
    "start": "2407540",
    "end": "2412690"
  },
  {
    "text": "is the best way to\ndo this at scale? We know that babies cannot\nlearn language from watching TV",
    "start": "2412690",
    "end": "2420700"
  },
  {
    "text": "alone, for example. So there has to be\nsome intervention, and there has to be\ninteractions, then,",
    "start": "2420700",
    "end": "2426580"
  },
  {
    "text": "that need to happen. But at the same\ntime, the question is, how far can models go by\njust training on static data,",
    "start": "2426580",
    "end": "2435400"
  },
  {
    "text": "as long as we have additional\nmodalities, especially when we combine this with scale?",
    "start": "2435400",
    "end": "2440480"
  },
  {
    "text": "And if interactions\nwith the environment are really necessary, how do we\ncollect data and design systems",
    "start": "2440480",
    "end": "2446270"
  },
  {
    "text": "that interact minimally or\nin a cost-effective way? And then, finally, could\npre-training on text",
    "start": "2446270",
    "end": "2452190"
  },
  {
    "text": "still be useful if any of\nthese other research directions",
    "start": "2452190",
    "end": "2461109"
  },
  {
    "text": "become more sample-efficient? So if you're interested in\nlearning more about this topic,",
    "start": "2461110",
    "end": "2467170"
  },
  {
    "text": "I highly encourage\nyou to take CS224U, which is offered in the spring. They have multiple lectures\non just language grounding.",
    "start": "2467170",
    "end": "2473601"
  },
  {
    "start": "2473601",
    "end": "2479010"
  },
  {
    "text": "OK, so in this\nfinal segment, I'm going to talk a little\nbit more about how you can get involved with NLP\nand deep learning research",
    "start": "2479010",
    "end": "2487440"
  },
  {
    "text": "and how you can\nmake more progress. So here are some\ngeneral principles",
    "start": "2487440",
    "end": "2494840"
  },
  {
    "text": "for how to make progress\nin your own NLP research. So I think the most\nimportant thing is to just read\nbroadly, which means--",
    "start": "2494840",
    "end": "2501799"
  },
  {
    "text": "and not just read the latest\nand greatest papers on arXiv, but also read pre-2010\nstatistical NLP.",
    "start": "2501800",
    "end": "2510020"
  },
  {
    "text": "Learn about the mathematical\nfoundations of machine learning to understand\nhow generalization works.",
    "start": "2510020",
    "end": "2515810"
  },
  {
    "text": "So take CS229M. Learn more about language,\nwhich means taking classes",
    "start": "2515810",
    "end": "2521790"
  },
  {
    "text": "in the Linguistics Department. In particular I would\nrecommend Linguist 130a, and also take CS224u.",
    "start": "2521790",
    "end": "2529500"
  },
  {
    "text": "And finally, if you\nwant to take inspiration from how babies\nlearn, then definitely",
    "start": "2529500",
    "end": "2535250"
  },
  {
    "text": "read about child language\nacquisition literature. It's fascinating.",
    "start": "2535250",
    "end": "2540859"
  },
  {
    "text": "Finally, learn your\nsoftware tools, which involves scripting\ntools, version control, data",
    "start": "2540860",
    "end": "2549290"
  },
  {
    "text": "wrangling, learning how\nto visualize quickly with Jupyter Notebooks. And deep learning often involves\nrunning multiple experiments",
    "start": "2549290",
    "end": "2558188"
  },
  {
    "text": "with different types of\nparameters and different ideas, all in parallel. And sometimes, it can\nget really hard to keep",
    "start": "2558188",
    "end": "2563240"
  },
  {
    "text": "track of everything. So learn how to use\nexperiment management tools like weights and biases. ",
    "start": "2563240",
    "end": "2570570"
  },
  {
    "text": "And finally, I'll talk\nabout some, really quick, final project tips.",
    "start": "2570570",
    "end": "2577200"
  },
  {
    "text": "So first, let's\njust start by saying that if your approach\ndoesn't seem to be working, please do not panic.",
    "start": "2577200",
    "end": "2583200"
  },
  {
    "text": "Put assert statements\neverywhere, and check if the computations\nthat you're doing are correct.",
    "start": "2583200",
    "end": "2588480"
  },
  {
    "text": "Use breakpoints\nextensively-- and I'll talk a bit more about this. Check if the loss function that\nyou've implemented is correct.",
    "start": "2588480",
    "end": "2596410"
  },
  {
    "text": "And one way of debugging that is\nto see that the initial values are correct.",
    "start": "2596410",
    "end": "2601490"
  },
  {
    "text": "So if you're doing a theory\nclassification problem, then the initial loss should\nbe the natural log of k.",
    "start": "2601490",
    "end": "2607100"
  },
  {
    "text": "Always, always, always start\nby create a small training data set which has 5 to 10\nexamples and see if your model",
    "start": "2607100",
    "end": "2613250"
  },
  {
    "text": "can completely overfit to that. If not, there's a problem\nwith your training loop.",
    "start": "2613250",
    "end": "2618440"
  },
  {
    "text": "Check for saturating\nactivations and dead ReLUs. And often, this\ncan be fixed by--",
    "start": "2618440",
    "end": "2624020"
  },
  {
    "text": "maybe there's some\nproblems to gradients, or maybe there's some problems\nwith the initialization. Which brings me to\nthe next point--",
    "start": "2624020",
    "end": "2629900"
  },
  {
    "text": "check the gradient values. See if they're too\nsmall, which means that maybe you should be using\nresidual connections or LSTMs.",
    "start": "2629900",
    "end": "2636505"
  },
  {
    "text": "Or if they're too\nlarge, then you should use gradient clipping. In fact, always use\ngradient clipping.",
    "start": "2636505",
    "end": "2642170"
  },
  {
    "text": "Overall, be methodical. If your approach doesn't\nwork, come up with hypotheses",
    "start": "2642170",
    "end": "2647180"
  },
  {
    "text": "for why this might be the case. Design Oracle\nexperiments to debug it. Look at your data, and look at\nthe errors that it's making,",
    "start": "2647180",
    "end": "2653960"
  },
  {
    "text": "and just try to be\nsystematic about everything. So I'll just say a little\nbit more about breakpoints.",
    "start": "2653960",
    "end": "2663280"
  },
  {
    "text": "So there's this great\nlibrary called pdb. It's like gdb, but\nit's for Python. So that's pdb.",
    "start": "2663280",
    "end": "2669140"
  },
  {
    "text": "To create a breakpoint, just\nadd the line import pdb, pdb.set_trace before the\nline you want to inspect.",
    "start": "2669140",
    "end": "2677130"
  },
  {
    "text": "So earlier today, I was\ntrying to play around with the Transformers\nlibrary, and I was trying",
    "start": "2677130",
    "end": "2684029"
  },
  {
    "text": "to do question answering. So I have a really\nsmall training corpus, and the context\nis, one morning, I",
    "start": "2684030",
    "end": "2689730"
  },
  {
    "text": "shot an elephant in my pajamas. How he got into my\npajamas, I don't know.",
    "start": "2689730",
    "end": "2694859"
  },
  {
    "text": "And the question is,\nwhat did I shoot? And to solve this\nproblem, I basically imported a organizer\nand a BERT model,",
    "start": "2694860",
    "end": "2703950"
  },
  {
    "text": "and I initialized my tokenizer,\ninitialized my model, I tokenized my input. I set my model\ninto the eval mode,",
    "start": "2703950",
    "end": "2710670"
  },
  {
    "text": "and I tried to\nlook at the output. But I get this error,\nand at face value",
    "start": "2710670",
    "end": "2716940"
  },
  {
    "text": "it's not clear what's\ncausing this error. And so the best way to look\nat what's causing this error",
    "start": "2716940",
    "end": "2722130"
  },
  {
    "text": "is to actually put a breakpoint. So right after model.eval,\nI put a breakpoint,",
    "start": "2722130",
    "end": "2727650"
  },
  {
    "text": "because I know that that's\nwhere the problem is. So the problem is in line 21, so\nI put a breakpoint at line 21.",
    "start": "2727650",
    "end": "2735380"
  },
  {
    "text": "And now, once I put\nthis breakpoint, I can just run my\nscript again, and it",
    "start": "2735380",
    "end": "2740840"
  },
  {
    "text": "stops before executing line 21. And at this point, I can\nexamine all of my variables.",
    "start": "2740840",
    "end": "2746250"
  },
  {
    "text": "So I can look at\nthe tokenized input, because maybe that's\nwhere the problem is. And lo and behold, I see\nthat it's actually a list.",
    "start": "2746250",
    "end": "2754250"
  },
  {
    "text": "So it's a dictionary of lists,\nwhereas models typically expect a dodged answer. So now I know what\nthe problem is,",
    "start": "2754250",
    "end": "2761119"
  },
  {
    "text": "and that means I can\nquickly go ahead and fix it, and everything just looks. So this just shows\nthat you should just",
    "start": "2761120",
    "end": "2767540"
  },
  {
    "text": "breakpoints everywhere if\nyour code is not working, and it can just help you\ndebug really quickly.",
    "start": "2767540",
    "end": "2775349"
  },
  {
    "text": "OK, so finally, I\nwould say that if you want to get involved with\nan independent research,",
    "start": "2775350",
    "end": "2781410"
  },
  {
    "text": "and if you really like\nto find a project, we have the CLIPS\nprogram at Stanford.",
    "start": "2781410",
    "end": "2786520"
  },
  {
    "text": "And this is a way for\nundergrads, masters students, and PhDs who are interested\nin doing NLP research",
    "start": "2786520",
    "end": "2793010"
  },
  {
    "text": "and want to get involved\nwith the NLP group. So we highly encourage\nyou to apply to CLIPS.",
    "start": "2793010",
    "end": "2800380"
  },
  {
    "text": "And so yeah, so I'll\nconclude today's class by saying that we've made a lot\nof progress in the last decade,",
    "start": "2800380",
    "end": "2808920"
  },
  {
    "text": "and that's mostly due\nto clever understanding of neural networks, data,\nhardware, all of that",
    "start": "2808920",
    "end": "2814740"
  },
  {
    "text": "combined with scale. We have some really\namazing technologies that can do pretty\nexciting things, and we saw some\nexamples of that today.",
    "start": "2814740",
    "end": "2822990"
  },
  {
    "text": "In the short term,\nI expect that we'll see more scaling,\nbecause it just seems to help-- so perhaps\neven larger models.",
    "start": "2822990",
    "end": "2830940"
  },
  {
    "text": "But this is not trivial. So I said that before,\nand I'll say it again, scaling requires really\nnon-trivial engineering",
    "start": "2830940",
    "end": "2838050"
  },
  {
    "text": "efforts, and sometimes\neven clever algorithms. And so there's a lot\nof interesting systems",
    "start": "2838050",
    "end": "2843900"
  },
  {
    "text": "work to be done here. But in the long\nterm, we really need to be thinking more\nabout these bigger",
    "start": "2843900",
    "end": "2849660"
  },
  {
    "text": "problems of systematicity,\ngeneralization. How can we make our models\nlearn a new concept really",
    "start": "2849660",
    "end": "2856260"
  },
  {
    "text": "quickly so that it's\nfast and efficient? And then, we also need\nto create benchmarks",
    "start": "2856260",
    "end": "2861330"
  },
  {
    "text": "that we can actually just-- if my model that has some\n[INAUDIBLE] and some sentiment analysis data set and I\ndeploy it in the real world,",
    "start": "2861330",
    "end": "2868470"
  },
  {
    "text": "that should be\nreflected in the number that I get from the benchmark. So we need to make\nprogress in the way",
    "start": "2868470",
    "end": "2874320"
  },
  {
    "text": "that we evaluate models. And then, also,\nfiguring out a way to move beyond text in\na more tractable way,",
    "start": "2874320",
    "end": "2881339"
  },
  {
    "text": "this is also really essential. So yeah, that's it. Good luck with\nyour final project.",
    "start": "2881340",
    "end": "2888000"
  },
  {
    "text": "I can take more\nquestions at this point.  So I answered a question\nearlier that, actually, I",
    "start": "2888000",
    "end": "2896080"
  },
  {
    "text": "think you could also opine on. It was the question of\nwhether you have a large model",
    "start": "2896080",
    "end": "2902620"
  },
  {
    "text": "that's pre-trained on language,\nif it will actually help you in other domains-- like you apply it to\nvision stuff, yeah.",
    "start": "2902620",
    "end": "2912850"
  },
  {
    "text": "Yeah, so I guess the\nanswer is actually yes. There was a paper that came\nout relatively recently,",
    "start": "2912850",
    "end": "2919300"
  },
  {
    "text": "like just a few days\nago, that it just takes-- I think it was GPT-2? I'm not sure.",
    "start": "2919300",
    "end": "2924840"
  },
  {
    "text": "It's one large transformer model\nthat's pre-trained on text, and other modalities,\nand I think it definitely",
    "start": "2924840",
    "end": "2932119"
  },
  {
    "text": "applied to images,\nand I think it applied to math problems\nand some more modalities",
    "start": "2932120",
    "end": "2938600"
  },
  {
    "text": "and showed that it's actually\npretty effective at transfer. So if you pre-train\non text, and then you",
    "start": "2938600",
    "end": "2943670"
  },
  {
    "text": "move to a different\nmodality, that helps. I think part of\nthe reason for that is just that across\nmodalities, there",
    "start": "2943670",
    "end": "2949609"
  },
  {
    "text": "is a lot of autoregressive\nstructure that is shared. And I think one reason for\nthat is that language is really",
    "start": "2949610",
    "end": "2958040"
  },
  {
    "text": "referring to the\nworld around it, and so you might expect\nthat there is some--",
    "start": "2958040",
    "end": "2963920"
  },
  {
    "text": "there is some\ncorrespondence that's just beyond\nautoregresssive structure. So there's also works that\nshow that if you have just",
    "start": "2963920",
    "end": "2972320"
  },
  {
    "text": "text-only representations and\nimage-only representations, you can actually learn a simple\nlinear classifier-- like you",
    "start": "2972320",
    "end": "2977990"
  },
  {
    "text": "can learn to align with\nthese representations. And all of these works are just\nshowing that there's actually a lot more in common\nbetween modalities",
    "start": "2977990",
    "end": "2984500"
  },
  {
    "text": "than we thought\nin the beginning. So yeah, I think yeah, it's\npossible to pre-train on text",
    "start": "2984500",
    "end": "2991340"
  },
  {
    "text": "and then fine-tune on\nyour modality of interest. And it should probably\nbe effective--",
    "start": "2991340",
    "end": "2998090"
  },
  {
    "text": "of course, based on\nwhat the modality is. But for images and videos,\nit's certainly effective.",
    "start": "2998090",
    "end": "3004345"
  },
  {
    "start": "3004345",
    "end": "3013750"
  },
  {
    "text": "No more questions? ",
    "start": "3013750",
    "end": "3020099"
  },
  {
    "text": "Well, a couple of\nquestions have turned up. One is, what's the\ndifference between CS224u",
    "start": "3020100",
    "end": "3028140"
  },
  {
    "text": "and this class in terms of\nthe topics covered and focus? Do you want to answer\nthat one, Shikar,",
    "start": "3028140",
    "end": "3033870"
  },
  {
    "text": "or should I have a\ngo at answering it? Maybe you should\nanswer this one. OK, so next quarter, CS224u,\nNatural Language Understanding,",
    "start": "3033870",
    "end": "3044640"
  },
  {
    "text": "is co-taught by Chris\nPotts and Bill McCartney.",
    "start": "3044640",
    "end": "3050609"
  },
  {
    "text": "So in essence it's\nmeant to be different",
    "start": "3050610",
    "end": "3057000"
  },
  {
    "text": "that natural language\nunderstanding focuses on what its name is--",
    "start": "3057000",
    "end": "3063180"
  },
  {
    "text": "sort of how to build\ncomputer systems that understand the sentences\nof natural language.",
    "start": "3063180",
    "end": "3069270"
  },
  {
    "text": "Now, in truth, the boundary\nis kind of complex, because we do some natural\nlanguage understanding",
    "start": "3069270",
    "end": "3077640"
  },
  {
    "text": "in this class as well. And certainly,\nfor the people who are doing the default\nfinal project,",
    "start": "3077640",
    "end": "3083520"
  },
  {
    "text": "question-answering, well, that's\nabsolutely a natural language understanding task.",
    "start": "3083520",
    "end": "3089040"
  },
  {
    "text": "But the distinction is\nmeant to be that at least a lot of what we\ndo in this class--",
    "start": "3089040",
    "end": "3097290"
  },
  {
    "text": "things like the assignment\n3 dependency parser",
    "start": "3097290",
    "end": "3102600"
  },
  {
    "text": "or building the\nmachine translation system in assignment 4--",
    "start": "3102600",
    "end": "3108540"
  },
  {
    "text": "that they are in some sense\nnatural language processing tasks where processing\ncan mean anything,",
    "start": "3108540",
    "end": "3114810"
  },
  {
    "text": "but it commonly means you're\ndoing useful, intelligent stuff",
    "start": "3114810",
    "end": "3120480"
  },
  {
    "text": "with human language input, but\nyou're not necessarily deeply understanding it.",
    "start": "3120480",
    "end": "3126760"
  },
  {
    "text": "So there is some\noverlap in the classes. If you do CS224u, you'll\ncertainly see word vectors",
    "start": "3126760",
    "end": "3134310"
  },
  {
    "text": "and transformers again. But the emphasis is on doing a\nlot more with natural language",
    "start": "3134310",
    "end": "3141000"
  },
  {
    "text": "understanding tasks. And so that includes things\nlike building semantic parsers--",
    "start": "3141000",
    "end": "3147890"
  },
  {
    "text": "so they're the kind\nof devices that will respond to\nquestions and commands",
    "start": "3147890",
    "end": "3154350"
  },
  {
    "text": "such as an Alexa or\nGoogle Assistant will do--",
    "start": "3154350",
    "end": "3160020"
  },
  {
    "text": "building relation\nextraction systems which get out particular facts\nout of a piece of text, of, oh,",
    "start": "3160020",
    "end": "3167339"
  },
  {
    "text": "this person took on this\nposition at this company,",
    "start": "3167340",
    "end": "3172500"
  },
  {
    "text": "looking at grounded language\nlearning and grounded language understanding, where\nyou're not only",
    "start": "3172500",
    "end": "3178320"
  },
  {
    "text": "using the language\nbut the world context to get information,\nand other tasks that--",
    "start": "3178320",
    "end": "3186089"
  },
  {
    "text": "I mean, I guess you\ncan look at the website to get more details of it.",
    "start": "3186090",
    "end": "3191549"
  },
  {
    "text": "Relevant to this class,\nI mean, a lot of people also find it an opportunity\nto just get further",
    "start": "3191550",
    "end": "3199050"
  },
  {
    "text": "in doing projects in the area\nof natural language processing.",
    "start": "3199050",
    "end": "3204690"
  },
  {
    "text": "That's by the nature of\nthe structure of the class, since it more\nassumes that people",
    "start": "3204690",
    "end": "3211170"
  },
  {
    "text": "know how to build deep\nlearning, natural language systems at the beginning,\nrather than a large percentage",
    "start": "3211170",
    "end": "3218549"
  },
  {
    "text": "of the class going\ninto, OK, you have to do all of these assignments. Although there are little\nassignments earlier on",
    "start": "3218550",
    "end": "3225570"
  },
  {
    "text": "that there's more time to work\non a project for the quarter. ",
    "start": "3225570",
    "end": "3234100"
  },
  {
    "text": "OK, here's one more question\nthat maybe Shikar could do. Do you know of\nattempts to crowdsource",
    "start": "3234100",
    "end": "3239589"
  },
  {
    "text": "dynamic benchmarks-- e.g. users uploading\nadversarial examples",
    "start": "3239590",
    "end": "3245200"
  },
  {
    "text": "for evaluation in\nonline learning? Yeah, so actually,\nthe main idea there",
    "start": "3245200",
    "end": "3254769"
  },
  {
    "text": "is to use crowdsourcing, right? So in fact, there\nis this bench-- so there is this platform that\nwas created by [INAUDIBLE]..",
    "start": "3254770",
    "end": "3262840"
  },
  {
    "text": "It's called DynaBench, and\nthe objective is just that. To construct this\ndynamically-evolving benchmark,",
    "start": "3262840",
    "end": "3271690"
  },
  {
    "text": "we are just going to offload\nit to users of this platform. And you can-- it\nessentially gives you",
    "start": "3271690",
    "end": "3278170"
  },
  {
    "text": "utilities for\ndeploying your model and then having humans\ntry to fool the model.",
    "start": "3278170",
    "end": "3286420"
  },
  {
    "text": "Yeah, so this is-- it's basically how\nthe dynamic evaluator, the dynamic benchmark\ncollection actually works.",
    "start": "3286420",
    "end": "3295420"
  },
  {
    "text": "So you deploy a model\non some platform, and then you get humans\nto fool the system, yeah.",
    "start": "3295420",
    "end": "3303880"
  },
  {
    "start": "3303880",
    "end": "3318710"
  },
  {
    "text": "Here's a question. Can you address the problems\nof NLP models not able to remember really long\ncontexts and techniques to infer",
    "start": "3318710",
    "end": "3326610"
  },
  {
    "text": "on really large input length? Yeah, so I guess there have\nbeen a few works recently",
    "start": "3326610",
    "end": "3337420"
  },
  {
    "text": "that kind of try to scale\nup transformers to really large context lengths.",
    "start": "3337420",
    "end": "3342579"
  },
  {
    "text": "One of them is\nlike the reformer, and there's also\nthe Transformer XL. That was-- I think it\nwas one of the first ones",
    "start": "3342580",
    "end": "3349338"
  },
  {
    "text": "to try and do that. I think what is unclear is\nwhether you can combine that",
    "start": "3349338",
    "end": "3358089"
  },
  {
    "text": "with the scale of\nthese GPT-like models, and if you see qualitatively\ndifferent things",
    "start": "3358090",
    "end": "3364750"
  },
  {
    "text": "once you do that. And part of it is\njust that all of this is just so recent, right?",
    "start": "3364750",
    "end": "3371032"
  },
  {
    "text": "But yeah, I think\nthe open question there is that, can you take\nthese really long context",
    "start": "3371032",
    "end": "3376690"
  },
  {
    "text": "transformers that can\noperate over long contexts, combine that with\nscale of GPT-3,",
    "start": "3376690",
    "end": "3383200"
  },
  {
    "text": "and then get models\nthat can actually reason with these\nreally large contexts?",
    "start": "3383200",
    "end": "3389349"
  },
  {
    "text": "Because I guess a\nhypothesis of scale is that once you train\nlanguage models at scale, it can start to do these things.",
    "start": "3389350",
    "end": "3396200"
  },
  {
    "text": "And so to do that\nfor long context, we actually need to have long\ncontext transformers that",
    "start": "3396200",
    "end": "3402849"
  },
  {
    "text": "are trained at scale. And I don't think people\nhave done that yet. ",
    "start": "3402850",
    "end": "3415575"
  },
  {
    "text": "So I'm seeing this\nother question about language acquisition. Chris, do you have\nsome thoughts on this?",
    "start": "3415575",
    "end": "3421740"
  },
  {
    "text": "Or maybe I can just\npresent to everyone. ",
    "start": "3421740",
    "end": "3430730"
  },
  {
    "text": "So the question is,\nwhat do you think we can learn from baby\nlanguage acquisition?",
    "start": "3430730",
    "end": "3436780"
  },
  {
    "text": "Can we build a language model\nin a more interactive way, like reinforcement learning?",
    "start": "3436780",
    "end": "3442030"
  },
  {
    "text": "Do you know any\nof these attempts? That's a big, huge\nquestion, and I",
    "start": "3442030",
    "end": "3449320"
  },
  {
    "text": "think the short and non-helpful\nanswer is that there are kind of no answers at the moment.",
    "start": "3449320",
    "end": "3455740"
  },
  {
    "text": "People have certainly tried to\ndo things at various scales, but we just have\nno technology that",
    "start": "3455740",
    "end": "3464650"
  },
  {
    "text": "is the least bit\nconvincing for being able to replicate\nthe language learning",
    "start": "3464650",
    "end": "3470830"
  },
  {
    "text": "ability of a human child. But after that prologue,\nwhat I could say is, I mean,",
    "start": "3470830",
    "end": "3478090"
  },
  {
    "text": "yeah, there are definitely\nideas to have in your head. So there are clear\nresults, which",
    "start": "3478090",
    "end": "3485860"
  },
  {
    "text": "is that little kids don't\nlearn by watching videos.",
    "start": "3485860",
    "end": "3491210"
  },
  {
    "text": "So it seems like interaction\nis completely key.",
    "start": "3491210",
    "end": "3497380"
  },
  {
    "text": "Little kids don't learn\nfrom language alone. They're in a very rich\nenvironment where people",
    "start": "3497380",
    "end": "3505210"
  },
  {
    "text": "are sort of both learning\nstuff from the environment in general, and in\nparticular, they're",
    "start": "3505210",
    "end": "3511780"
  },
  {
    "text": "learning a lot from what\nlanguage acquisition researchers refer\nto as \"attention,\"",
    "start": "3511780",
    "end": "3518770"
  },
  {
    "text": "which is different to\nwhat we mean by attention. But it means that\nthe caregiver will",
    "start": "3518770",
    "end": "3524980"
  },
  {
    "text": "be looking at the object\nthat's the focus of interest, and commonly other\nthings as well,",
    "start": "3524980",
    "end": "3530740"
  },
  {
    "text": "like picking it up, and bringing\nit near the kid, and all those kinds of things.",
    "start": "3530740",
    "end": "3537320"
  },
  {
    "text": "And babies and young kids get\nto experiment a lot, right?",
    "start": "3537320",
    "end": "3543290"
  },
  {
    "text": "So regardless of\nwhether it's learning what happens when\nyou have some blocks",
    "start": "3543290",
    "end": "3549560"
  },
  {
    "text": "that you stack up\nand play with them, or you're learning language,\nyou sort of experiment",
    "start": "3549560",
    "end": "3556730"
  },
  {
    "text": "by trying some things and see\nwhat kind of response you get. And again, that's essentially\nbuilding on the interactivity",
    "start": "3556730",
    "end": "3565430"
  },
  {
    "text": "of it, that you're getting\nsome kind of response to any utterance you make.",
    "start": "3565430",
    "end": "3570589"
  },
  {
    "text": "And this is something\nthat's been hotly debated in the language\nacquisition literature.",
    "start": "3570590",
    "end": "3576600"
  },
  {
    "text": "So a traditional\nChomskyan position is that if human beings don't\nget effective feedback--",
    "start": "3576600",
    "end": "3589099"
  },
  {
    "text": "supervised labels--\nwhen they talk-- and in some very narrow sense,\nwell, that's true right.",
    "start": "3589100",
    "end": "3596150"
  },
  {
    "text": "It's just not the case\nthat after a baby tries to say something,\nthat they get feedback of, syntax error in English\non word 4, or they get given,",
    "start": "3596150",
    "end": "3607880"
  },
  {
    "text": "here's the semantic form I\ntook away from your utterance. But in a more indirect way, they\nclearly get enormous feedback.",
    "start": "3607880",
    "end": "3617000"
  },
  {
    "text": "They can see what\nkind of response they get from their\ncaregiver at every corner.",
    "start": "3617000",
    "end": "3624380"
  },
  {
    "text": "And so in your question,\nyou were suggesting that, well, somehow, we\nshould be making",
    "start": "3624380",
    "end": "3632810"
  },
  {
    "text": "use of reinforcement learning,\nbecause we have something like a reward signal there.",
    "start": "3632810",
    "end": "3637970"
  },
  {
    "text": "And in a big picture way,\nI'd say, huh, yeah, I agree. In terms of a much more specific\nway as to, well, how could",
    "start": "3637970",
    "end": "3646400"
  },
  {
    "text": "we possibly get that to\nwork, to learn something with the richness\nof human language,",
    "start": "3646400",
    "end": "3653570"
  },
  {
    "text": "I think we don't have much idea. But there has started\nto be some work.",
    "start": "3653570",
    "end": "3659660"
  },
  {
    "text": "So people have been\nsort of building virtual environments, which\nyou have your avatar in,",
    "start": "3659660",
    "end": "3670010"
  },
  {
    "text": "and that you can manipulate\nin the virtual environment, and there's linguistic input,\nand it can succeed in getting",
    "start": "3670010",
    "end": "3676040"
  },
  {
    "text": "rewards for sort\nof doing a command, where the command can\nbe something like, pick up the orange block,\nor something like that.",
    "start": "3676040",
    "end": "3684530"
  },
  {
    "text": "And to a small extent,\npeople have been",
    "start": "3684530",
    "end": "3689830"
  },
  {
    "text": "able to build things that work. I mean, as you might be picking\nup, I mean, I guess so far,",
    "start": "3689830",
    "end": "3698320"
  },
  {
    "text": "at least, I've just been\nkind of underwhelmed, because it seems like the\ncomplexity of what people have",
    "start": "3698320",
    "end": "3704050"
  },
  {
    "text": "achieved is sort of just\nso primitive compared to the full complexity\nof language, right?",
    "start": "3704050",
    "end": "3711820"
  },
  {
    "text": "The kind of\nlanguages that people have been able to\nget systems to learn are ones that can\ndo pick up commands,",
    "start": "3711820",
    "end": "3719380"
  },
  {
    "text": "where they can learn \"blue\ncube\" versus \"orange sphere,\"",
    "start": "3719380",
    "end": "3724390"
  },
  {
    "text": "and that's sort of about\nhow far people have gotten. And that's sort of such a teeny,\nsmall corner of what's involved",
    "start": "3724390",
    "end": "3732549"
  },
  {
    "text": "in learning a human language.  One thing I'll just\nadd to that is I",
    "start": "3732550",
    "end": "3740260"
  },
  {
    "text": "think there are some\nprinciples of how kids learn that you\ncould have tried",
    "start": "3740260",
    "end": "3746200"
  },
  {
    "text": "to apply to deep learning. And one example\nthat comes to mind is curriculum\nlearning, where there's",
    "start": "3746200",
    "end": "3752590"
  },
  {
    "text": "a lot of literature that\nshows that babies tend to pay attention to things\nthat is just slightly",
    "start": "3752590",
    "end": "3760286"
  },
  {
    "text": "challenging for them, and\nthey don't pay attention to things that are\nextremely challenging and also don't pay attention\nto things that they",
    "start": "3760287",
    "end": "3766060"
  },
  {
    "text": "know how to solve. And many researchers\nhave really tried to get curriculum\nlearning to work,",
    "start": "3766060",
    "end": "3773080"
  },
  {
    "text": "and the verdict on that is\nthat it seems to kind of work when you're in reinforcement\nlearning settings,",
    "start": "3773080",
    "end": "3779350"
  },
  {
    "text": "but it's unclear if\nit's going to work on like supervised\nlearning settings. But I still think that\nit's under-explored,",
    "start": "3779350",
    "end": "3785569"
  },
  {
    "text": "and maybe there should\nbe more attempts to kind of see if you can look\nat the curriculum learning",
    "start": "3785570",
    "end": "3793370"
  },
  {
    "text": "and if that improves anything.  Yeah, I agree. Curriculum learning\nis an important idea",
    "start": "3793370",
    "end": "3801070"
  },
  {
    "text": "which we haven't\nreally talked about, but it seems like it certainly\nis central to human learning.",
    "start": "3801070",
    "end": "3807609"
  },
  {
    "text": "And there's been some\nminor successes with it in the machine learning\nworld, but it sort of",
    "start": "3807610",
    "end": "3812800"
  },
  {
    "text": "seems like it's\nan idea you should be able to do a lot\nmore with in the future as you move from\nmodels that are just",
    "start": "3812800",
    "end": "3821020"
  },
  {
    "text": "doing one narrow task to trying\nto do a more general language acquisition process.",
    "start": "3821020",
    "end": "3829480"
  },
  {
    "text": "Shall I attempt the\nnext question as well? OK, the next question\nis, is the reason",
    "start": "3829480",
    "end": "3834609"
  },
  {
    "text": "humans learn languages\nbetter just because we are pre-trained over millions\nof years of physics simulation?",
    "start": "3834610",
    "end": "3841300"
  },
  {
    "text": "Maybe we should pre-train\na model the same way. So I mean, I presume what you're\nsaying is \"physics simulation,\"",
    "start": "3841300",
    "end": "3849910"
  },
  {
    "text": "you're evoking evolution when\nyou're talking about millions of years.",
    "start": "3849910",
    "end": "3854930"
  },
  {
    "text": "So this is a controversial,\ndebated, big question.",
    "start": "3854930",
    "end": "3863920"
  },
  {
    "text": "So again, if I invoke\nChomsky again-- so Noam Chomsky is sort of the most\nfamous linguist in the world.",
    "start": "3863920",
    "end": "3873565"
  },
  {
    "text": " And essentially, Noam Chomsky's\ncareer, starting in the 1950s,",
    "start": "3873565",
    "end": "3881560"
  },
  {
    "text": "is built around the idea\nthat little children get",
    "start": "3881560",
    "end": "3887380"
  },
  {
    "text": "such dubious linguistic input-- because they hear a\nrandom bunch of stuff,",
    "start": "3887380",
    "end": "3893800"
  },
  {
    "text": "they don't get much feedback\non what they say, et cetera-- that language could not\nbe learned empirically",
    "start": "3893800",
    "end": "3902109"
  },
  {
    "text": "just from the data observed,\nand the only possible assumption",
    "start": "3902110",
    "end": "3907660"
  },
  {
    "text": "to work from is significant\nparts of human language",
    "start": "3907660",
    "end": "3914740"
  },
  {
    "text": "are innate in the human genome. Babies are born with it, and\nthat explains the miracle",
    "start": "3914740",
    "end": "3921370"
  },
  {
    "text": "by which very little\nhumans learn amazingly fast how human languages work.",
    "start": "3921370",
    "end": "3929230"
  },
  {
    "text": "Now, to speak in\ncredit for that idea, for those of you who have not\nbeen around little children,",
    "start": "3929230",
    "end": "3937660"
  },
  {
    "text": "I mean, I think one does\njust have to acknowledge, human language acquisition\nby live little kids,",
    "start": "3937660",
    "end": "3946690"
  },
  {
    "text": "I mean, it does just seem\nto be miraculous, right, that you go through\nthis sort of slow phase",
    "start": "3946690",
    "end": "3952510"
  },
  {
    "text": "for a couple of years where the\nkid sort of goos and gaas some",
    "start": "3952510",
    "end": "3959320"
  },
  {
    "text": "syllables. And then, there's a fairly long\nperiod where they've picked up a few words, and they\ncan say, juice, juice",
    "start": "3959320",
    "end": "3967090"
  },
  {
    "text": "when they want to drink some\njuice, and nothing else. And then, it just seems like\nthere's this phase change where",
    "start": "3967090",
    "end": "3974200"
  },
  {
    "text": "the kids suddenly\nrealize, wait, this is a productive,\ngenerative sentence system!",
    "start": "3974200",
    "end": "3979510"
  },
  {
    "text": "I can say whole sentences! And then, in an\nincredibly short period, they seem to\ntransition from saying",
    "start": "3979510",
    "end": "3987190"
  },
  {
    "text": "one- and two-word utterances\nto suddenly they can say,",
    "start": "3987190",
    "end": "3993490"
  },
  {
    "text": "Daddy come home in garage,\nputting bike in garage.",
    "start": "3993490",
    "end": "3999070"
  },
  {
    "text": "And you go, wow, how did they\nsuddenly discover language? ",
    "start": "3999070",
    "end": "4005350"
  },
  {
    "text": "So it is kind of amazing. But personally,\nfor me, at least,",
    "start": "4005350",
    "end": "4012270"
  },
  {
    "text": "I've just never believed\nthe strong versions of the hypothesis\nthat human beings have",
    "start": "4012270",
    "end": "4020700"
  },
  {
    "text": "much in the way of\nlanguage-specific knowledge or structure in\ntheir brains that",
    "start": "4020700",
    "end": "4027270"
  },
  {
    "text": "comes from genetic inheritance. Clearly, humans do have\nthese very clever brains,",
    "start": "4027270",
    "end": "4034380"
  },
  {
    "text": "and if we're at the level of,\nsaying, being able to think, or being able to interpret\nthe visual world, that's",
    "start": "4034380",
    "end": "4043860"
  },
  {
    "text": "things that have developed\nover tens of millions of years.",
    "start": "4043860",
    "end": "4049140"
  },
  {
    "text": "And evolution can be a large\npart of the explanation,",
    "start": "4049140",
    "end": "4054539"
  },
  {
    "text": "and humans are\nclearly born with lots of vision-specific\nhardware in their brains,",
    "start": "4054540",
    "end": "4062370"
  },
  {
    "text": "as are a lot of other creatures. But when you come\nto language, no one",
    "start": "4062370",
    "end": "4070170"
  },
  {
    "text": "knows when language was-- in a sort of modern-like form-- first became available,\nbecause there",
    "start": "4070170",
    "end": "4077640"
  },
  {
    "text": "aren't any fossils of\npeople saying the word \"spear\" or something like that.",
    "start": "4077640",
    "end": "4084869"
  },
  {
    "text": "But to the extent that\nthere are estimates based on what you can see of\nthe spread of proto-humans",
    "start": "4084870",
    "end": "4094710"
  },
  {
    "text": "and their apparent\nsocial structures, from what you can\nfind in fossils,",
    "start": "4094710",
    "end": "4102120"
  },
  {
    "text": "most people guess that language\nis at most a million years old.",
    "start": "4102120",
    "end": "4107470"
  },
  {
    "text": "And that's just too short a\ntime for any significant--",
    "start": "4107470",
    "end": "4113729"
  },
  {
    "text": "for evolution have built\nany significant structure inside human brains that's\nspecific to language.",
    "start": "4113729",
    "end": "4120109"
  },
  {
    "text": "So I kind of think that\nthe working assumption has to be that there's\njust about nothing",
    "start": "4120109",
    "end": "4129089"
  },
  {
    "text": "specific to language\nin human brains, and the most\nplausible hypothesis--",
    "start": "4129090",
    "end": "4135420"
  },
  {
    "text": "not that I know very much about\nneuroscience when it comes down to it-- is that humans were\nbeing able to repurpose",
    "start": "4135420",
    "end": "4143549"
  },
  {
    "text": "hardware that was originally\nbuilt for other purposes, like visual scene\ninterpretation and memory,",
    "start": "4143550",
    "end": "4150720"
  },
  {
    "text": "and that that gave a basis of\nhaving all this clever hardware",
    "start": "4150720",
    "end": "4156330"
  },
  {
    "text": "that you could then\nuse for language. So it's kind of like GPUs were\ninvented for playing computer",
    "start": "4156330",
    "end": "4161880"
  },
  {
    "text": "games, and we were able\nto repurpose that hardware to do deep learning. ",
    "start": "4161880",
    "end": "4173910"
  },
  {
    "text": "OK, we've got a lot have\ncome out at the end. OK, so this one\nwas answered live.",
    "start": "4173910",
    "end": "4181829"
  },
  {
    "text": "Let's see. Yeah, if you could name-- I guess this is\nfor either of you-- one main bottleneck as to\nif we could provide feedback",
    "start": "4181830",
    "end": "4192359"
  },
  {
    "text": "efficiently to our systems\nlike babies are given feedback, what's the bottleneck\nthat remains",
    "start": "4192359",
    "end": "4198570"
  },
  {
    "text": "in trying to have more\nhuman-like language",
    "start": "4198570",
    "end": "4203940"
  },
  {
    "text": "acquisition? ",
    "start": "4203940",
    "end": "4221080"
  },
  {
    "text": "I mean, I can opine on this. Or were you saying\nsomething, Shikar? Yeah, I was just\ngoing to say that I",
    "start": "4221080",
    "end": "4227590"
  },
  {
    "text": "think it's a bit of everything. I think in terms of models, one\nthing I would say is that we",
    "start": "4227590",
    "end": "4235750"
  },
  {
    "text": "know that there's more feedback\nconnections and feed-forward connections in the brain, and\nwe haven't really figured out",
    "start": "4235750",
    "end": "4244030"
  },
  {
    "text": "a way of-- so of course, we had RNNs,\nwhich sort of implements",
    "start": "4244030",
    "end": "4251300"
  },
  {
    "text": "you can move to a algorithm that\nsort of implements a feedback loop. But we still haven't\nreally figured out",
    "start": "4251300",
    "end": "4257020"
  },
  {
    "text": "how to use that knowledge, that\nthe brain has a lot of feedback connections, and then apply\nthat to practical systems.",
    "start": "4257020",
    "end": "4265490"
  },
  {
    "text": "So I think on the modeling,\nand maybe that's one problem.",
    "start": "4265490",
    "end": "4270550"
  },
  {
    "text": "There is-- I think [INAUDIBLE]\nlearning is maybe one of them, but I think the\none that's probably",
    "start": "4270550",
    "end": "4276909"
  },
  {
    "text": "going to have the most bang\nfor the buck is really just figuring out how we\ncan move beyond text.",
    "start": "4276910",
    "end": "4282225"
  },
  {
    "text": "And I think there's just\nso much more information that's available that\nwe're just not using.",
    "start": "4282225",
    "end": "4288620"
  },
  {
    "text": "And so I think that's where\nmost of the progress might come, is from figuring out what's\nmost practical way of going",
    "start": "4288620",
    "end": "4294910"
  },
  {
    "text": "beyond text. This is what I think. ",
    "start": "4294910",
    "end": "4305090"
  },
  {
    "text": "OK, let's see.",
    "start": "4305090",
    "end": "4310369"
  },
  {
    "text": "What are some\nimportant NLP topics that we have not\ncovered in this class? ",
    "start": "4310370",
    "end": "4320110"
  },
  {
    "text": "I'll do that. Well, sort of one\nanswer is, a lot of the topics that are\ncovered in CS224u, because we",
    "start": "4320110",
    "end": "4328870"
  },
  {
    "text": "do make a bit of an effort\nto keep them disjoint. They're not fully-- right.",
    "start": "4328870",
    "end": "4334060"
  },
  {
    "text": "So there's sort\nof lots of topics in language understanding that\nwe haven't covered, right?",
    "start": "4334060",
    "end": "4341350"
  },
  {
    "text": "So if you want to make a\nvoice assistant like Alexa,",
    "start": "4341350",
    "end": "4348370"
  },
  {
    "text": "Siri, or Google\nAssistant, well, you need to sort of be\nable to interface",
    "start": "4348370",
    "end": "4353710"
  },
  {
    "text": "with systems APIs that can do\nthings like delete your mail or buy you concert tickets.",
    "start": "4353710",
    "end": "4360670"
  },
  {
    "text": "And so you need to be able\nto convert from language into an explicit semantic\nform that can interact",
    "start": "4360670",
    "end": "4366850"
  },
  {
    "text": "with the systems of the world. And we haven't talked\nabout that at all. So there's lots of language\nunderstanding stuff.",
    "start": "4366850",
    "end": "4374380"
  },
  {
    "text": "There's also lots of\nlanguage generation things.",
    "start": "4374380",
    "end": "4379400"
  },
  {
    "text": "So effectively, for language\ngeneration, all we have done is--",
    "start": "4379400",
    "end": "4385030"
  },
  {
    "text": "neural language\nmodels, they are great. Run them, and they\nwill generate language. And in one sense, that's true.",
    "start": "4385030",
    "end": "4393700"
  },
  {
    "text": "It's just awesome the\nkind of generation you can do with things\nlike GPT-2 or -3.",
    "start": "4393700",
    "end": "4402370"
  },
  {
    "text": "But where that's missing is\nthat's really only giving you",
    "start": "4402370",
    "end": "4408220"
  },
  {
    "text": "the ability to produce fluent\ntext, where it rabbits off",
    "start": "4408220",
    "end": "4414955"
  },
  {
    "text": "and produces fluent text,\nthat, if you actually wanted to have a good natural\nlanguage generation system,",
    "start": "4414955",
    "end": "4422470"
  },
  {
    "text": "you also have to have\nhigher-level planning of what you're going to talk\nabout and how you",
    "start": "4422470",
    "end": "4430750"
  },
  {
    "text": "are going to express it, right? So that in most situations in\nnatural language, you think,",
    "start": "4430750",
    "end": "4437770"
  },
  {
    "text": "OK, well, I want to\nexplain to people something about why it's important to\ndo math classes at college.",
    "start": "4437770",
    "end": "4445270"
  },
  {
    "text": "Let me think how\nto organize this. Maybe I should talk about some\nof the different applications",
    "start": "4445270",
    "end": "4450820"
  },
  {
    "text": "where math turns up and how\nit's a really good grounding. Whatever-- you plan out, here's\nhow I can present some ideas.",
    "start": "4450820",
    "end": "4459970"
  },
  {
    "text": "And that kind of natural\nlanguage generation we're not doing and we\nhaven't done any of.",
    "start": "4459970",
    "end": "4469450"
  },
  {
    "text": "Yeah, so that's\nwhat I was saying-- more understanding,\nmore generation, which",
    "start": "4469450",
    "end": "4475870"
  },
  {
    "text": "is most of NLP, you could say. I mean, obviously, there are,\nthen, sort of particular tasks",
    "start": "4475870",
    "end": "4481870"
  },
  {
    "text": "that we can talk about that\nwe either have or have not explicitly addressed.",
    "start": "4481870",
    "end": "4487420"
  },
  {
    "start": "4487420",
    "end": "4492910"
  },
  {
    "text": "OK, has there been any work\nin putting language models",
    "start": "4492910",
    "end": "4498580"
  },
  {
    "text": "into an environment in\nwhich they can communicate to achieve a task? And do you think this would\nhelp with unsupervised learning?",
    "start": "4498580",
    "end": "4506770"
  },
  {
    "start": "4506770",
    "end": "4512070"
  },
  {
    "text": "So I guess there's been a lot of\nwork on emergent communication,",
    "start": "4512070",
    "end": "4517550"
  },
  {
    "text": "and also self-play, where you\nhave these different models",
    "start": "4517550",
    "end": "4524179"
  },
  {
    "text": "which are initialized\nas language models that attempt to communicate with\neach other to solve some task.",
    "start": "4524180",
    "end": "4530750"
  },
  {
    "text": "And then, you have\na reward at the end whether they were able to\nfinish the task or not.",
    "start": "4530750",
    "end": "4535860"
  },
  {
    "text": "And then, based on\nthat reward, you attempt to learn the\ncommunication strategy. And this started out as emergent\ncommunication with self-play,",
    "start": "4535860",
    "end": "4544387"
  },
  {
    "text": "and then there was recent work-- I think it was like either\nlast year or the year before that-- where they\nshowed that if you initialized",
    "start": "4544387",
    "end": "4550670"
  },
  {
    "text": "these models with language\nmodel pre-training, you basically prevent\nthis problem of language",
    "start": "4550670",
    "end": "4559040"
  },
  {
    "text": "drift, where the language\nor the communication protocol that your\nmodels end up learning",
    "start": "4559040",
    "end": "4564710"
  },
  {
    "text": "has nothing to do\nwith actual language. And so yeah, I mean\nfrom that sense,",
    "start": "4564710",
    "end": "4570469"
  },
  {
    "text": "there has been some work. But it's very little of it. I think it's some groups\nthat tried to study",
    "start": "4570470",
    "end": "4575849"
  },
  {
    "text": "this, but nothing beyond that. ",
    "start": "4575850",
    "end": "4583230"
  },
  {
    "text": "OK, I mean, the last two\nquestions are about genes. ",
    "start": "4583230",
    "end": "4590642"
  },
  {
    "text": "Well, there's one question\nabout whether genes may have some correlations\nfrom social cues",
    "start": "4590642",
    "end": "4596436"
  },
  {
    "text": "or a reward-based system. I don't know if either of\nyou have opinions about this,",
    "start": "4596436",
    "end": "4601545"
  },
  {
    "text": "but if you do--  Yeah, I mean, I don't\nhave anything very deep",
    "start": "4601545",
    "end": "4608560"
  },
  {
    "text": "to say about this question. So it's on the\nimportance of social cues as opposed to pure\nreward-based systems.",
    "start": "4608560",
    "end": "4616300"
  },
  {
    "text": "Well, I mean, in some\nsense, a social cue you can also regard as\na reward, that people",
    "start": "4616300",
    "end": "4624699"
  },
  {
    "text": "like to have other people\nput a smile on their face when you say something.",
    "start": "4624700",
    "end": "4630650"
  },
  {
    "text": "But I do think generally,\nwhen people are saying,",
    "start": "4630650",
    "end": "4636550"
  },
  {
    "text": "what have we not covered,\nanother thing that we've barely covered is the social\nside of language.",
    "start": "4636550",
    "end": "4644110"
  },
  {
    "text": "So a huge, interesting\nthing about language",
    "start": "4644110",
    "end": "4649780"
  },
  {
    "text": "is it has this very\ndynamic, big dynamic range. So on the one hand, you can\ntalk about very precise things",
    "start": "4649780",
    "end": "4657250"
  },
  {
    "text": "in language. So you can sort of talk\nabout math formulas, and steps in a proof,\nand things like that,",
    "start": "4657250",
    "end": "4663380"
  },
  {
    "text": "so that there's a lot of\nprecision in language. But on the other\nhand, you can just",
    "start": "4663380",
    "end": "4669600"
  },
  {
    "text": "phatically mumble, mumble\nwhatever words at all, and you're not really sort\nof communicating anything",
    "start": "4669600",
    "end": "4676030"
  },
  {
    "text": "in the way of a\npropositional content. What you're really\ntrying to communicate",
    "start": "4676030",
    "end": "4681220"
  },
  {
    "text": "is, oh, I'm thinking\nabout you right now. Oh, I'm concerned with\nhow you're feeling,",
    "start": "4681220",
    "end": "4688210"
  },
  {
    "text": "or whatever it is in the\ncircumstances, right? So that a huge part\nof language use",
    "start": "4688210",
    "end": "4694030"
  },
  {
    "text": "is in forms of\nsocial communication between human beings.",
    "start": "4694030",
    "end": "4700480"
  },
  {
    "text": "And that's another big\npart of actually building",
    "start": "4700480",
    "end": "4706840"
  },
  {
    "text": "successful natural\nlanguage systems, right? So if you think\nnegatively about something",
    "start": "4706840",
    "end": "4714159"
  },
  {
    "text": "like the virtual assistants\nI've been falling back on a lot, is that they have virtually\nno ability as social language",
    "start": "4714160",
    "end": "4723910"
  },
  {
    "text": "users, right? So we're now training a\ngeneration of little kids",
    "start": "4723910",
    "end": "4728980"
  },
  {
    "text": "that what you should\ndo is sort of bark out commands as if\nyou were serving",
    "start": "4728980",
    "end": "4735850"
  },
  {
    "text": "in the German Army in\nWorld War II or something and that there's none of the\nkind of social part of how",
    "start": "4735850",
    "end": "4745600"
  },
  {
    "text": "to use language to\ncommunicate satisfactorily",
    "start": "4745600",
    "end": "4752290"
  },
  {
    "text": "with human beings and to\nmaintain a social system, and that that's a huge\npart of human language use",
    "start": "4752290",
    "end": "4758950"
  },
  {
    "text": "that kids have to learn and\nlearn to use successfully, right? A lot of being\nsuccessful in the world",
    "start": "4758950",
    "end": "4766120"
  },
  {
    "text": "is, when you want someone\nto do something for you,",
    "start": "4766120",
    "end": "4771190"
  },
  {
    "text": "you know that there are good\nways to ask them for it. Yes, some of it is choice of\nhow to present the arguments,",
    "start": "4771190",
    "end": "4779470"
  },
  {
    "text": "but some of it is by\nbuilding social rapport, and asking nicely\nand reasonably,",
    "start": "4779470",
    "end": "4786520"
  },
  {
    "text": "and making it seem like you're\na sweet person that other people should do something for.",
    "start": "4786520",
    "end": "4792040"
  },
  {
    "text": "And human beings are\nvery good at that, and being good at that is\na really important skill",
    "start": "4792040",
    "end": "4797770"
  },
  {
    "text": "for being able to\nnavigate the world well. ",
    "start": "4797770",
    "end": "4805000"
  }
]