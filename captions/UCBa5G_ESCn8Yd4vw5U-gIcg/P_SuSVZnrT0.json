[
  {
    "start": "0",
    "end": "5810"
  },
  {
    "text": "Today we're going to finish\noff with an important topic. And actually, it's one that\nyou need to a little bit",
    "start": "5810",
    "end": "13520"
  },
  {
    "text": "in order to actually\nsolve, I think, one of the problems on\nthis week's homework.",
    "start": "13520",
    "end": "18620"
  },
  {
    "text": "And that's our targeted social\ncontract is that for a homework",
    "start": "18620",
    "end": "24170"
  },
  {
    "text": "due Friday we cover all\nthe material by the Tuesday lecture. We don't always make\nit, but that's our goal,",
    "start": "24170",
    "end": "31760"
  },
  {
    "text": "that's our aspiration. OK, then we're going to start\none of our last theory topics.",
    "start": "31760",
    "end": "41587"
  },
  {
    "text": "It's actually super interesting. After we cover this,\nwe're going to jump into duality, which is fun. And we'll do that for this week.",
    "start": "41587",
    "end": "47870"
  },
  {
    "text": "That's super interesting. These are probably things\nthat most of you haven't seen.",
    "start": "47870",
    "end": "53900"
  },
  {
    "text": "Or if you've seen, it\nwasn't people describing it, didn't use the D word for that.",
    "start": "53900",
    "end": "60230"
  },
  {
    "text": "But you will have seen\narguments for it, maybe in a statistics class\nor in other classes,",
    "start": "60230",
    "end": "66450"
  },
  {
    "text": "certainly in economics. I don't know if they use\nthat word or not there. We'll find out. OK.",
    "start": "66450",
    "end": "71880"
  },
  {
    "text": "All right. Let's go over\nvector optimization. Now just to put\nthis in context, we",
    "start": "71880",
    "end": "78930"
  },
  {
    "text": "looked at vector inequalities. In a vector inequality,\nwhat happens is something like\nthat is a vector",
    "start": "78930",
    "end": "85830"
  },
  {
    "text": "and this inequality is now to be\ninterpreted in a vector sense. This could state that a matrix\nis positive semi-definite,",
    "start": "85830",
    "end": "92850"
  },
  {
    "text": "a matrix variable is\npositive semi-definite. That would be an example. Now when we were\ndoing that, we did not",
    "start": "92850",
    "end": "98490"
  },
  {
    "text": "change the semantics of\nan optimization problem. The semantics was identical.",
    "start": "98490",
    "end": "103680"
  },
  {
    "text": "And the semantics, I'll repeat\nit one more time, it's this. Semantics of an\noptimization problem",
    "start": "103680",
    "end": "109920"
  },
  {
    "text": "is if a point does not\nsatisfy all the constraints, if it is not feasible, it's\ncompletely unacceptable",
    "start": "109920",
    "end": "116579"
  },
  {
    "text": "and there's no more discussion. Among those that do satisfy\nall the constraints,",
    "start": "116580",
    "end": "122910"
  },
  {
    "text": "the one with the\nleast objective, which is a scalar in a\nstandard optimization problem,",
    "start": "122910",
    "end": "128669"
  },
  {
    "text": "is the best one. Everybody got that? That's the semantics. There's nothing else to it. Now we're going to be talking\nabout vector optimization.",
    "start": "128669",
    "end": "135900"
  },
  {
    "text": "And what we're going\nto do is we're actually going to now generalize,\nnot constraints,",
    "start": "135900",
    "end": "141360"
  },
  {
    "text": "but the objective\nto be a vector. Now there, we're\nactually going to have",
    "start": "141360",
    "end": "146640"
  },
  {
    "text": "to mess with the semantics,\nbecause what does it mean to say minimize a vector?",
    "start": "146640",
    "end": "154020"
  },
  {
    "text": "Like offhand? And it turns out it can\nmean lots of things. And, not surprisingly,\nthe main distinction",
    "start": "154020",
    "end": "160020"
  },
  {
    "text": "is going to be things like\nminimal versus minimum. Because vector vectors\nare generally not",
    "start": "160020",
    "end": "168269"
  },
  {
    "text": "the-- most orderings you have on\nthem are not linear orderings. OK so this is what\nwe're going to do.",
    "start": "168270",
    "end": "174720"
  },
  {
    "text": "It's going to look like this. And now this one, we\ncan state what it is. That's its syntax.",
    "start": "174720",
    "end": "180480"
  },
  {
    "text": "What we have not yet given\nyou is the semantics.",
    "start": "180480",
    "end": "185970"
  },
  {
    "text": "I could do things like say-- I mean, here would\nbe an example. I could say, hey, please\nfind an estimator. And you go, what's\nyour objective?",
    "start": "185970",
    "end": "192037"
  },
  {
    "text": "And you go, make my error\ncovariance matrix small. Everybody got that?",
    "start": "192037",
    "end": "198660"
  },
  {
    "text": "That is a problem\njust like that. Of course, immediately\nsomeone would say,",
    "start": "198660",
    "end": "204370"
  },
  {
    "text": "what do you mean by small? Right? Because then there could\nbe lots of ways to say a covariance matrix is small.",
    "start": "204370",
    "end": "211950"
  },
  {
    "text": "One way that would\nbe clear is if there was one choice for which a\ncovariance matrix was actually",
    "start": "211950",
    "end": "218819"
  },
  {
    "text": "smaller than all others,\notherwise known as a minimum. Then it'd be\ncompletely unambiguous",
    "start": "218820",
    "end": "226350"
  },
  {
    "text": "that that's what would have\nto be the solution of this. Everybody got that? But we'll get into this. This is kind of the idea.",
    "start": "226350",
    "end": "232352"
  },
  {
    "text": " The first thing we'll\ndo is, let's look",
    "start": "232352",
    "end": "238739"
  },
  {
    "text": "at the set of objective of\nachievable objective value. So what that means\nis we screen out",
    "start": "238740",
    "end": "244320"
  },
  {
    "text": "all choices of x,\nwhich are not feasible and we only look at the\nones that are feasible. They're the ones that don't get\nlike an immediate rejection.",
    "start": "244320",
    "end": "251730"
  },
  {
    "text": "So we look at the\nones that are feasible and then we look at\nthe set of objectives. Now, if this was\na scalar, this O",
    "start": "251730",
    "end": "258359"
  },
  {
    "text": "would be a subset\nof R. In fact, it would probably be continuous.",
    "start": "258360",
    "end": "264840"
  },
  {
    "text": "Therefore it would be an\ninterval of some kind. And then it's completely\nsimple to say what",
    "start": "264840",
    "end": "271740"
  },
  {
    "text": "the semantics is for a scalar. It's basically the one\nfarthest to the left, which is the smallest\npoint in that interval.",
    "start": "271740",
    "end": "280200"
  },
  {
    "text": "So that would be that. Here, this is like a weird set. For example, let's pretend\nthat this is with respect",
    "start": "280200",
    "end": "287880"
  },
  {
    "text": "to R-plus to the two. ",
    "start": "287880",
    "end": "293130"
  },
  {
    "text": "Let's say that here are the\nobjective-- what that means is, basically, down and\nto the left is good,",
    "start": "293130",
    "end": "301020"
  },
  {
    "text": "if you take that to\nbe the objective. Then it could be that\nO looks like this.",
    "start": "301020",
    "end": "308610"
  },
  {
    "text": "And here you see there's a\nminimum point, in which case you just simply say,\nit's optimal, period.",
    "start": "308610",
    "end": "314880"
  },
  {
    "text": "It's just optimal.  That's what it\nmeans to be optimal.",
    "start": "314880",
    "end": "320580"
  },
  {
    "text": "Basically, that\nhappens pretty rarely, but it does happen in some very\nfamous examples in, let's say, statistics and estimation.",
    "start": "320580",
    "end": "326759"
  },
  {
    "text": "But generally, it's not. OK It's Pareto optimal, is the name\nthat you would give to a point",
    "start": "326760",
    "end": "337410"
  },
  {
    "text": "if f0 of x-- that's\na vector, remember-- is a minimal value of this.",
    "start": "337410",
    "end": "342430"
  },
  {
    "text": "And so what that means is here\nare the achievable things, here's what you can do.",
    "start": "342430",
    "end": "347920"
  },
  {
    "text": "Remember, in this one, R\nplus squared is the vector, is the cone we use\nto compare vectors.",
    "start": "347920",
    "end": "357280"
  },
  {
    "text": "What that means is if you\nhave a point here, everything to the left and\ndown, these things",
    "start": "357280",
    "end": "362350"
  },
  {
    "text": "are better than that point. So when we talked\nabout minimal points,",
    "start": "362350",
    "end": "369580"
  },
  {
    "text": "we didn't assign something's\nbetter than something. Now we are, because\nwe're doing minimization",
    "start": "369580",
    "end": "376300"
  },
  {
    "text": "and we want that quote,\n\"small,\" but that means small in this vector sense. So these are the\nthings that are better.",
    "start": "376300",
    "end": "382630"
  },
  {
    "text": "So for example, if there's an\nx that achieves this value,",
    "start": "382630",
    "end": "388450"
  },
  {
    "text": "and I mean the\ntechnical name for that is a stupid choice of x,\nbecause basically it's silly.",
    "start": "388450",
    "end": "395110"
  },
  {
    "text": "You're like look, if\nyou take this thing, there's all sorts\nof points over here, which actually are unambiguously\nbetter than that choice.",
    "start": "395110",
    "end": "404920"
  },
  {
    "text": "And this is in the\nvector sense, right. The minimal points of this\nset O are these, over here.",
    "start": "404920",
    "end": "413110"
  },
  {
    "text": "It's this boundary here. And then that's open. And then this boundary here.",
    "start": "413110",
    "end": "418660"
  },
  {
    "text": "So for every single\none of those points, there's no point\nthat is strictly better than that point.",
    "start": "418660",
    "end": "426780"
  },
  {
    "text": "These are the minimal--\nthey're literally the minimal points of O, which\nis the achievable objectives.",
    "start": "426780",
    "end": "433080"
  },
  {
    "text": "And it kind of all really\nmakes perfect sense.",
    "start": "433080",
    "end": "439590"
  },
  {
    "text": "And then, actually,\nif someone says, can you compare this and this,\nthen the answer would be no.",
    "start": "439590",
    "end": "447240"
  },
  {
    "text": "Then no one could say,\nwithout further information, you couldn't say that\nthat's better than that.",
    "start": "447240",
    "end": "454630"
  },
  {
    "text": "You could just-- in fact,\na very good name for Pareto is a great name. I forget which field uses it.",
    "start": "454630",
    "end": "460030"
  },
  {
    "text": "It's called \"non-dominated.\" A non-dominated point.",
    "start": "460030",
    "end": "465849"
  },
  {
    "text": "Because something like this\npoint, here, is dominated. It's dominated. In other words,\nthere are choices",
    "start": "465850",
    "end": "472030"
  },
  {
    "text": "which are simply better. They're strictly\ndown and to the left. OK, so that's dominated.",
    "start": "472030",
    "end": "478000"
  },
  {
    "text": "And these the points that are\nnot dominated is this boundary, it's the set of minimal points.",
    "start": "478000",
    "end": "483490"
  },
  {
    "text": "It's also called-- these\nare called Pareto optimal, is another name for it. That's from economics, actually,\nI think from the 1890s.",
    "start": "483490",
    "end": "489940"
  },
  {
    "text": "Something like that. So this is not\nexactly new material.",
    "start": "489940",
    "end": "495460"
  },
  {
    "text": "So this is the new semantics of\na vector optimization problem.",
    "start": "495460",
    "end": "500500"
  },
  {
    "text": "OK. It's clearest in the case-- in what's called\nmulti-criterion optimization.",
    "start": "500500",
    "end": "507550"
  },
  {
    "text": "And this is a vector problem,\nwhere we just have the cone. And our objectives actually\nhave that we would literally",
    "start": "507550",
    "end": "514120"
  },
  {
    "text": "refer to these as our\nobjectives, F1 up to Fq. Those are our objectives. Altogether, they become\na vector objective,",
    "start": "514120",
    "end": "520599"
  },
  {
    "text": "but literally you'd call\nthese the objectives. OK, let's see what it means.",
    "start": "520600",
    "end": "528029"
  },
  {
    "text": "So here's what it would mean. If you have these q\ndifferent objectives,",
    "start": "528030",
    "end": "533540"
  },
  {
    "text": "and roughly speaking, you\nwant all of them small, that's the idea. And so if you have a point\nthat's optimal, what it means",
    "start": "533540",
    "end": "543860"
  },
  {
    "text": "is it's both feasible and\nit says basically what that says that there is a choice\nof x where it minimizes F1",
    "start": "543860",
    "end": "554060"
  },
  {
    "text": "and it minimizes F2\nand it minimizes F3 and it minimizes all of them.",
    "start": "554060",
    "end": "559100"
  },
  {
    "text": "It simultaneously\nminimizes all of them. This is an example. Like this, right.",
    "start": "559100",
    "end": "564410"
  },
  {
    "text": "And this says that there was\nno competition among them. I just-- I found a single point\nthat just minimized everything.",
    "start": "564410",
    "end": "570830"
  },
  {
    "text": "I should add, this should\nbe kind of obvious, but I'm not aware of any problem\nin a practical setting that's",
    "start": "570830",
    "end": "580009"
  },
  {
    "text": "not a multi-criterion problem. Because a lot doesn't matter.",
    "start": "580010",
    "end": "585780"
  },
  {
    "text": "You'll see very shortly\nhow to solve them. It's not a big deal. Some people actually\nmake this into a field,",
    "start": "585780",
    "end": "590870"
  },
  {
    "text": "which is I find\nridiculous, because if you know how to do these things\nit's extremely straightforward.",
    "start": "590870",
    "end": "596170"
  },
  {
    "text": "So any field there\nis, there are going to be trade offs among\ncompeting objectives.",
    "start": "596170",
    "end": "602040"
  },
  {
    "text": "If you're fitting a model\nin machine learning, you'd have a loss function\nand you'd have a regularizer and you'd want to\ntrade off among them.",
    "start": "602040",
    "end": "609420"
  },
  {
    "text": "In control, you want\nto hit your target and you could trade off fuel\nand the comfort of the ride.",
    "start": "609420",
    "end": "618270"
  },
  {
    "text": "Or something like that. Pretty much any problem I can\npossibly imagine, there'll be--",
    "start": "618270",
    "end": "625080"
  },
  {
    "text": "but then they're very easily,\nas we'll see shortly, handled by straight-up optimization.",
    "start": "625080",
    "end": "631140"
  },
  {
    "text": "Anyway, OK, back to this. By the way, this\nhappens very rarely.",
    "start": "631140",
    "end": "637829"
  },
  {
    "text": "If you would say, oh, I'm\ndoing this circuit design. I care about the area,\nI care about the power,",
    "start": "637830",
    "end": "643860"
  },
  {
    "text": "I care about the\ndynamic power, and it doesn't matter-- that's it.",
    "start": "643860",
    "end": "649170"
  },
  {
    "text": "And I care about the maximum\npower output of my amplifier. There you go.",
    "start": "649170",
    "end": "654480"
  },
  {
    "text": "OK, so I have four objectives. And then you come back and\nyou go, cool, here's a design. And you go, well\nwhat does it do?",
    "start": "654480",
    "end": "660000"
  },
  {
    "text": "You say, oh it minimizes all\nfour of them simultaneously. Everybody got that? I mean, that would be awesome. But this happens\nlike basically never.",
    "start": "660000",
    "end": "667830"
  },
  {
    "text": "Almost never. Pareto optimal is\nactually interesting.",
    "start": "667830",
    "end": "673360"
  },
  {
    "text": "To be Pareto optimal, what\nit means is the following. It means you're non-dominated.",
    "start": "673360",
    "end": "679510"
  },
  {
    "text": "It means you say,\nwell, I have an x and you get these q different\nscores or objective values.",
    "start": "679510",
    "end": "686260"
  },
  {
    "text": "Then what you know if\nyou are Pareto optimal is that nobody can embarrass you.",
    "start": "686260",
    "end": "692380"
  },
  {
    "text": "Embarrass you\nmeans, oh yeah, well I've got an x that\nmatches you on all and beats you on at least one.",
    "start": "692380",
    "end": "698290"
  },
  {
    "text": "Everybody, that was\ninformal, but it was actually a precise statement of what\nit meant to be Pareto optimal.",
    "start": "698290",
    "end": "704290"
  },
  {
    "text": "Everybody got that? So you'd say, well, I know\nhow to do my production.",
    "start": "704290",
    "end": "710509"
  },
  {
    "text": "So I will use I'll take\nyour plan for production and I'll say, yeah\nI got a plan-- mine of the 12 resources\nthat we both use, I match you",
    "start": "710510",
    "end": "719290"
  },
  {
    "text": "on all and I beat you on two. That means my plan\ndominates yours, which means",
    "start": "719290",
    "end": "725200"
  },
  {
    "text": "yours is not Pareto optimal. But of course, mine\nneed not be, either. But that's what it means.",
    "start": "725200",
    "end": "731190"
  },
  {
    "text": "So I think these are\nideas you've seen before. They're very simple\nand, I think, intuitive",
    "start": "731190",
    "end": "736520"
  },
  {
    "text": "but I think it's not a bad thing\nto actually have it called out with a name associated with it.",
    "start": "736520",
    "end": "743360"
  },
  {
    "text": "So the one thing\nthat people in a lot of areas, when you talk\nabout Pareto optimal, they talk about--",
    "start": "743360",
    "end": "750650"
  },
  {
    "text": "you talk about a\ntrade-off, which is, I think, an extremely\ngood way to say it. So if you look back\nover here, remember",
    "start": "750650",
    "end": "758360"
  },
  {
    "text": "this is an optimization\nproblem where we're basically optimizing two things.",
    "start": "758360",
    "end": "764480"
  },
  {
    "text": "We want we want the first\ncomponent small, so left is good.",
    "start": "764480",
    "end": "769790"
  },
  {
    "text": "And we want the second component\nsmall, which means down is good. And so what we would do is\nif someone pointed to so--",
    "start": "769790",
    "end": "777980"
  },
  {
    "text": "that's a stupid choice. That's an impossible choice. You know, a perfectly\nnice one, but impossible.",
    "start": "777980",
    "end": "784220"
  },
  {
    "text": "This is possible, but stupid. These are the not stupid ones\nor the Pareto ones or something",
    "start": "784220",
    "end": "790139"
  },
  {
    "text": "like that. And people would refer to this\nas an optimal trade-off curve.",
    "start": "790140",
    "end": "795870"
  },
  {
    "text": "And then if you were to\nchoose these two points and say let's discuss,\nlet's compare,",
    "start": "795870",
    "end": "802160"
  },
  {
    "text": "and you would say, well neither\nis better than the other, because they're not comparable. It is not the case\nthat one-- and then",
    "start": "802160",
    "end": "808818"
  },
  {
    "text": "you'd say, well, what happened? Then you'd say, well, if I was\ngoing to compare this point and this point, what I would\nsay is this point up here beats",
    "start": "808818",
    "end": "818030"
  },
  {
    "text": "this one in the first\nobjective but pays for it with the second,\nbecause it's higher.",
    "start": "818030",
    "end": "825670"
  },
  {
    "text": "OK and so that's and\nthat actually gets you kind of to some\neconomics, I mean",
    "start": "825670",
    "end": "830680"
  },
  {
    "text": "a simple economic\ninterpretation, where you are exchanging\nbetween the two objectives, sort",
    "start": "830680",
    "end": "837820"
  },
  {
    "text": "of paying for one\nwith the other. Everybody got-- these\nare all just vague ideas, but they're about to be\nnot vague, very soon.",
    "start": "837820",
    "end": "845029"
  },
  {
    "text": "OK. ",
    "start": "845030",
    "end": "850620"
  },
  {
    "text": "Here's an example. It's a bi-criterion problem\nwith two objectives.",
    "start": "850620",
    "end": "855840"
  },
  {
    "text": "And in fact, the objective-- I mean, this is basically\na least squares objective. So this could be\nyour least squares",
    "start": "855840",
    "end": "863940"
  },
  {
    "text": "training error or something. And then this would\nbe like if you're doing ridge regression,\nwhich we'll get to later.",
    "start": "863940",
    "end": "872123"
  },
  {
    "text": "But actually, this\nis interesting. What this does is it says-- the semantics of this is\nsomeone saying, choose an x.",
    "start": "872123",
    "end": "878730"
  },
  {
    "text": "And you say, how\nwill you be judged? And you say you will be\njudged by two numbers-- number one, this thing you\nwant to call that your tracking",
    "start": "878730",
    "end": "886020"
  },
  {
    "text": "error or training error\nwhatever you like, I don't care. You'll be judged\nby that one number. And you'd say, I want you\nto please try to satisfy,",
    "start": "886020",
    "end": "893970"
  },
  {
    "text": "approximately, Ax equals b. Oh, but at the same time, I\ndon't want x to be too big and that's the second\nnumber you are judged by.",
    "start": "893970",
    "end": "901890"
  },
  {
    "text": "And so this is an\nexample of that set O.",
    "start": "901890",
    "end": "907230"
  },
  {
    "text": "And so what it says is we,\nwell we didn't do it this way, because you can actually\ncalculate that set.",
    "start": "907230",
    "end": "912520"
  },
  {
    "text": "But it doesn't matter. But in principle, what we\ndid is we took all x in Rn.",
    "start": "912520",
    "end": "917800"
  },
  {
    "text": "We evaluated that\nnumber and that number. That's a two-vector. And I put a dark spot\nwherever that happened.",
    "start": "917800",
    "end": "923140"
  },
  {
    "text": "Everybody got it? And so this is\nwhat's achievable. Now everything up here\nis ridiculous and silly,",
    "start": "923140",
    "end": "929230"
  },
  {
    "text": "because the semantics is\nleft is good, down is good. So it turns out the only\nones you should possibly",
    "start": "929230",
    "end": "936190"
  },
  {
    "text": "ever consider are either\nthe minimal ones in O or the Pareto optimal ones\nor something like that",
    "start": "936190",
    "end": "943150"
  },
  {
    "text": "and that's this little\ncurve right here. Including the end points.",
    "start": "943150",
    "end": "948640"
  },
  {
    "text": "Everybody got that? If, in fact, you cared\nonly in your choice",
    "start": "948640",
    "end": "954310"
  },
  {
    "text": "of x about these\ntwo numbers, then it would be extremely\nsilly to choose any choice of x other than\nthe ones on that line.",
    "start": "954310",
    "end": "962268"
  },
  {
    "text": "And then the way we\nwould say this is, we'd say things like if\nyou wanted to compare this point and this\npoint, we would say,",
    "start": "962268",
    "end": "969070"
  },
  {
    "text": "oh you just traded\noff fitting error with the size of your parameters\nor however you want to say it.",
    "start": "969070",
    "end": "975383"
  },
  {
    "text": "That's how you would say it. OK, so again, this is\nall kind of obvious, but this is what it\nlooks like, right.",
    "start": "975383",
    "end": "983470"
  },
  {
    "text": "OK.  Here's a famous one.",
    "start": "983470",
    "end": "988570"
  },
  {
    "text": "So you're doing\nportfolio construction. So I'll say a\nlittle bit about it",
    "start": "988570",
    "end": "994990"
  },
  {
    "text": "just so everybody is\nkind of on the same page.",
    "start": "994990",
    "end": "1000000"
  },
  {
    "text": "So x-- that's an\ninvestment portfolio, and it's usually\nnormalized to one.",
    "start": "1000000",
    "end": "1006660"
  },
  {
    "text": "Actually, this is the case\nwhere they're all non-negative and they add up to one. So this is a set\nof mixture weights.",
    "start": "1006660",
    "end": "1012030"
  },
  {
    "text": "And basically, it tells you\nwhat fraction of your wealth you invest in each\nof these things. And this is used like\nabsolutely everywhere.",
    "start": "1012030",
    "end": "1018870"
  },
  {
    "text": "It's used on a kind\nof a macroscopic scale like Stanford\nendowment sits there.",
    "start": "1018870",
    "end": "1024569"
  },
  {
    "text": "But there, the\nindividual investments are like asset classes, like\nUS equities, European equities,",
    "start": "1024569",
    "end": "1031919"
  },
  {
    "text": "alternative investments. Venture capital is\nactually one of the assets",
    "start": "1031920",
    "end": "1037619"
  },
  {
    "text": "in Stanford's endowment, right. So that's this. And then this is asking how\nto make a mixture of it.",
    "start": "1037619",
    "end": "1043449"
  },
  {
    "text": "By the way, later\nwe'll talk about it, but you can violate this,\nand it has a meaning.",
    "start": "1043450",
    "end": "1049710"
  },
  {
    "text": "So you can actually have-- Oh and you can interpret\nthe x's as percentages.",
    "start": "1049710",
    "end": "1056160"
  },
  {
    "text": "If I had four asset classes-- I mean, this would be really\ndumb-- but like stocks, bonds,",
    "start": "1056160",
    "end": "1061780"
  },
  {
    "text": "and cash. And I could give you 60, 40,\nand 0 or something, right?",
    "start": "1061780",
    "end": "1067139"
  },
  {
    "text": "Or something like that. You've heard of\nthese things, right. But in fact, you can\nactually-- in stuff",
    "start": "1067140",
    "end": "1074120"
  },
  {
    "text": "that we'll see later-- you can\nactually let x get negative. And you might ask, that's weird. What does it mean to hold\n-5% of your portfolio",
    "start": "1074120",
    "end": "1082940"
  },
  {
    "text": "in, I don't know,\nit doesn't matter, medium-term corporate bonds?",
    "start": "1082940",
    "end": "1090260"
  },
  {
    "text": "Everybody see what I'm saying? I can explain again. By the way, all of\nthis, for most of you,",
    "start": "1090260",
    "end": "1096710"
  },
  {
    "text": "this is nothing but\ncultural enrichment. If you ever bump into someone\nwho is interested in this,",
    "start": "1096710",
    "end": "1103640"
  },
  {
    "text": "you know what they're\ntalking about. So if you do genetics, too bad. Sorry about that. But it's not bad to\nknow these things",
    "start": "1103640",
    "end": "1110030"
  },
  {
    "text": "right to know how optimization\nis used in other fields. So what does it mean to\nown -5% in, whatever,",
    "start": "1110030",
    "end": "1117470"
  },
  {
    "text": "medium-term corporate bonds? What it means is you borrow\nthat asset from someone.",
    "start": "1117470",
    "end": "1125870"
  },
  {
    "text": "You now have the obligation\nto pay them back. And so you borrow it\nand immediately sell it.",
    "start": "1125870",
    "end": "1132840"
  },
  {
    "text": "Then later, when they come\nask, when the investment period is over, they go,\nhey, give me my bond back.",
    "start": "1132840",
    "end": "1138810"
  },
  {
    "text": "You buy it on the open market. If the price went down, it's\ncheaper and you did well.",
    "start": "1138810",
    "end": "1144570"
  },
  {
    "text": "Everybody got that? Again, you don't need it. But what's really cool about it\nis all the math goes through. And so you could actually\nown negative you know.",
    "start": "1144570",
    "end": "1153630"
  },
  {
    "text": "Anyway, so that's the idea. Fine. This is just a\ndiscussion of this,",
    "start": "1153630",
    "end": "1158640"
  },
  {
    "text": "because this is the first\ntime we're seeing this. This is sort of a\nvery classic analysis.",
    "start": "1158640",
    "end": "1164550"
  },
  {
    "text": "It's kind of like, although it's\ngot parts of it that were not in the original thing this\ngoes back to 1953 about",
    "start": "1164550",
    "end": "1171809"
  },
  {
    "text": "with someone named\nHarry Markowitz. So here's what it is. We care about two things. So I'm going to make\nthis investment portfolio",
    "start": "1171810",
    "end": "1178620"
  },
  {
    "text": "and there's going\nto be a return here.",
    "start": "1178620",
    "end": "1184410"
  },
  {
    "text": "There's going to be a return\nand the return on your-- so the return might\nlook like this.",
    "start": "1184410",
    "end": "1192450"
  },
  {
    "text": "It might be plus\n0.02 -0.01, you know,",
    "start": "1192450",
    "end": "1198450"
  },
  {
    "text": "and plus 0.02, or something. And this means asset\none went up 2%.",
    "start": "1198450",
    "end": "1204030"
  },
  {
    "text": "So did asset three. Asset one went down 1%,\nright, that's what it means. So and if you form\nr transpose x,",
    "start": "1204030",
    "end": "1211260"
  },
  {
    "text": "that's going to tell you\nthe portfolio return. So if you had 80%\nhere, 10% and 10%,",
    "start": "1211260",
    "end": "1219060"
  },
  {
    "text": "you just work it out and\nfigure out how well you did. And you probably went up about-- you got dinged a\nlittle bit here--",
    "start": "1219060",
    "end": "1225450"
  },
  {
    "text": "you probably went up somewhere\nnear 2%, but not quite. Everybody got that?",
    "start": "1225450",
    "end": "1231390"
  },
  {
    "text": "The model of r is that\nr is a random vector",
    "start": "1231390",
    "end": "1236430"
  },
  {
    "text": "and we'll give its\nfirst two moments. And those first two moments are\ngoing to be the expected value",
    "start": "1236430",
    "end": "1244290"
  },
  {
    "text": "and p-bar transpose. That's going to be the relative\nprice changes, the returns.",
    "start": "1244290",
    "end": "1249820"
  },
  {
    "text": "That's this thing. I guess I'm using\nbad notation here. That's fine or inconsistent.",
    "start": "1249820",
    "end": "1257610"
  },
  {
    "text": "That's going to be the expected\nreturn of the portfolio. And then the variance is going\nto be x-transpose sigma x.",
    "start": "1257610",
    "end": "1264750"
  },
  {
    "text": "And so you should have\nsome estimates of the mean",
    "start": "1264750",
    "end": "1270630"
  },
  {
    "text": "and the covariance\nof these things. Everybody following this? You care about two\nthings, which is",
    "start": "1270630",
    "end": "1277799"
  },
  {
    "text": "would like a portfolio, which\non average kind of goes up, so you want high mean return.",
    "start": "1277800",
    "end": "1284327"
  },
  {
    "text": "A lot of people drop the mean\nand just say high return. ",
    "start": "1284328",
    "end": "1290440"
  },
  {
    "text": "Of course, that doesn't\ntell you that every day it's going to go up. It just tells you that, if you\nbelieve the statistical models,",
    "start": "1290440",
    "end": "1297659"
  },
  {
    "text": "which by the way,\nare not very good, and if you were to wind back\nand repeat that day 10,000 times",
    "start": "1297660",
    "end": "1303659"
  },
  {
    "text": "and take the average,\nit would have gone up. But since the\nlatter is impossible and the first is also false,\nthat that's the rough idea.",
    "start": "1303660",
    "end": "1313200"
  },
  {
    "text": "And this thing,\nthat's the variance. That measures-- actually,\nthat's called the risk of the portfolio, right.",
    "start": "1313200",
    "end": "1318788"
  },
  {
    "text": "And if you take the square root,\nit's the standard deviation. And it says, if I'm\na portfolio manager and I wake up in the\nmorning, have my coffee,",
    "start": "1318788",
    "end": "1326010"
  },
  {
    "text": "then try to find out\ndid Stanford's endowment go up or down today, then I\ntake the mean square value",
    "start": "1326010",
    "end": "1333240"
  },
  {
    "text": "of that that's going to\nbe that is going to be the risk of the portfolio. It's called the volatility\nor something like that.",
    "start": "1333240",
    "end": "1341460"
  },
  {
    "text": "Again, this is just\nso you get the idea. So this is really a\nbi-criterion problem.",
    "start": "1341460",
    "end": "1350350"
  },
  {
    "text": "So they've got two objectives,\nwhich is risk and return. Well the return, I want big,\nand the risk, I want small.",
    "start": "1350350",
    "end": "1358289"
  },
  {
    "text": "So I could have done this\nwith some sick cone that was kind of like R-plus cross\nminus R-plus or something",
    "start": "1358290",
    "end": "1364260"
  },
  {
    "text": "like that. The problem with that\nis then your head explodes I mean it's correct\nbut it's hard to keep track of.",
    "start": "1364260",
    "end": "1370942"
  },
  {
    "text": "So we'll just do it this way. We're going to minimize\nthe negative return because we really want that\nto be high and the variance.",
    "start": "1370942",
    "end": "1378410"
  },
  {
    "text": "So that's the idea. OK that's what it looks like. And then, what you could do here\nis you can actually just look",
    "start": "1378410",
    "end": "1388610"
  },
  {
    "text": "through, you can walk through. There's a Pareto\nsurface, right, here.",
    "start": "1388610",
    "end": "1395330"
  },
  {
    "text": "And those correspond to the\nnon-dominated portfolios. And they actually\nwill trade off a curve",
    "start": "1395330",
    "end": "1401708"
  },
  {
    "text": "that will look like this. This is a very standard curve\nit's a risk-return trade-off curve.",
    "start": "1401708",
    "end": "1407270"
  },
  {
    "text": "And now, I haven't drawn it. If I was going to do this,\nI would flip one sign of it and all that kind of stuff.",
    "start": "1407270",
    "end": "1412650"
  },
  {
    "text": "But this is the traditional\nway people show it, with return this way\nand risk this way.",
    "start": "1412650",
    "end": "1419420"
  },
  {
    "text": "So that's the idea. So in this, without the flips\nof sign here, left is good.",
    "start": "1419420",
    "end": "1428299"
  },
  {
    "text": "And up is good. And so you get something\nthat looks like that. Yeah, there's a question.",
    "start": "1428300",
    "end": "1434120"
  },
  {
    "text": "Is this considered convex,\nbecause the negative changes things a little bit?",
    "start": "1434120",
    "end": "1439649"
  },
  {
    "text": "It is absolutely. This is convex. Because here, the rule\nto, in multi-criterion,",
    "start": "1439650",
    "end": "1447530"
  },
  {
    "text": "if I'm minimizing,\neach objective has to be convex, right. So this is convex\nquadratic, the risk.",
    "start": "1447530",
    "end": "1454970"
  },
  {
    "text": "And the negative return\nis affine, so it's convex.",
    "start": "1454970",
    "end": "1461490"
  },
  {
    "text": "Cool? OK. But yeah, good check. I just write these things\ndown without saying that,",
    "start": "1461490",
    "end": "1466830"
  },
  {
    "text": "but that's what it is. Anyway, so this shows\nyou the-- and you",
    "start": "1466830",
    "end": "1472170"
  },
  {
    "text": "would describe this as the\nrisk-return trade-off curve. And there's even some\nname which I've forgotten",
    "start": "1472170",
    "end": "1478085"
  },
  {
    "text": "and I forget what\nthis is called. This is called the-- I don't even remember anymore. Does someone here know?",
    "start": "1478085",
    "end": "1484230"
  },
  {
    "text": "The name? Anyway, it doesn't matter. It's the risk-return\ntrade-off curve. Let's just leave\nit that way, OK.",
    "start": "1484230",
    "end": "1490320"
  },
  {
    "text": "Or the risk-return\noptimal trade-off curve. And so, this is\nwhat it looks like.",
    "start": "1490320",
    "end": "1496049"
  },
  {
    "text": "And what this shows\nis actually the-- so here, I have four\nassets, so they add to one.",
    "start": "1496050",
    "end": "1503430"
  },
  {
    "text": "This is a stack plot. So a stack plot\nsays, each vertical",
    "start": "1503430",
    "end": "1509160"
  },
  {
    "text": "here gives you a point on the\noptimal risk-return trade-off curve. And at this point, you've\ngot a little bit of x1,",
    "start": "1509160",
    "end": "1516480"
  },
  {
    "text": "a little bit of x2, ooh a\nbunch of x3, and a bunch of x4. So over here, when you insist\non 0% return, you get x4.",
    "start": "1516480",
    "end": "1525940"
  },
  {
    "text": "By the way, x4 is\nprobably cash, which would have no risk, right.",
    "start": "1525940",
    "end": "1531190"
  },
  {
    "text": "I don't know what x1 is, but\nit's probably the most risky of the assets. And so, as you take,\nyou would say--",
    "start": "1531190",
    "end": "1537843"
  },
  {
    "text": "I'll use the\nlanguage people would use-- as you take on more\nrisk, you get a portfolio",
    "start": "1537843",
    "end": "1543130"
  },
  {
    "text": "that, actually, at some point,\nyou stop holding any cash. And you only hold\nthis, maybe these are like bonds or something.",
    "start": "1543130",
    "end": "1548710"
  },
  {
    "text": "I don't know, right. Then you start taking on-- ",
    "start": "1548710",
    "end": "1554020"
  },
  {
    "text": "then this would be\nlarge cap stocks, and these would be like\nsmall cap or something. I'm just making this up,\nbut not entirely, right.",
    "start": "1554020",
    "end": "1560617"
  },
  {
    "text": "So it would look\nsomething like this. So this all this makes sense? So this is the picture.",
    "start": "1560618",
    "end": "1567000"
  },
  {
    "text": "You'll see variations\non this sort of thing all over the place.",
    "start": "1567000",
    "end": "1572980"
  },
  {
    "text": "By the way, a little bit later. I will show how you\nactually compute this and for that matter\nyou will be doing it on your homework\nthat's due Friday.",
    "start": "1572980",
    "end": "1580300"
  },
  {
    "text": "It's actually quite\nstraightforward to do this. ",
    "start": "1580300",
    "end": "1586330"
  },
  {
    "text": "So now we're talking about\nscalarization, right. So scalarization refers to\ntaking a vector objective",
    "start": "1586330",
    "end": "1593830"
  },
  {
    "text": "and then mapping it\ndown to a single score. That way, that will give\nyou a linear ordering.",
    "start": "1593830",
    "end": "1601419"
  },
  {
    "text": "Because when we scalarize-- you had a production\ndesign and I had one-- one of ours is\nbetter, because it comes up",
    "start": "1601420",
    "end": "1607840"
  },
  {
    "text": "to a certain number. Now, let's see how to do that.",
    "start": "1607840",
    "end": "1613720"
  },
  {
    "text": "Well, you'll remember from\na couple of weeks ago, we talked about what happens\nif you take a vector that's",
    "start": "1613720",
    "end": "1621460"
  },
  {
    "text": "positive in the dual cone\nright and you minimize lambda-transpose f of x.",
    "start": "1621460",
    "end": "1627580"
  },
  {
    "text": "That's guaranteed to\ngive you a Pareto op-- a minimal point, which\nwe now in this context referred to as a\nPareto optimal point.",
    "start": "1627580",
    "end": "1635830"
  },
  {
    "text": "It says this problem, here, is\nan ordinary convex optimization problem. And there's a whole\nbunch of subtleties",
    "start": "1635830",
    "end": "1641799"
  },
  {
    "text": "that I'm not mentioning. Here's one. If f0 of x is\nk-convex, that means",
    "start": "1641800",
    "end": "1647710"
  },
  {
    "text": "it's a vector function\nwhich is k-convex. It's convex with\nrespect to the cone k.",
    "start": "1647710",
    "end": "1654549"
  },
  {
    "text": "If I take lambda as\npositive in the dual cone, then that's a scalar function.",
    "start": "1654550",
    "end": "1661600"
  },
  {
    "text": "That scalar function is convex. So that's the first thing. So this is an ordinary\nold convex optimization",
    "start": "1661600",
    "end": "1667630"
  },
  {
    "text": "problem and the\npicture is pretty cool. It basically says, if\nhere's O, what I do is,",
    "start": "1667630",
    "end": "1675490"
  },
  {
    "text": "let me look at level curves\nof lambda-transpose f0 of x. And so those are\ngoing to be, basically",
    "start": "1675490",
    "end": "1682720"
  },
  {
    "text": "hyperplanes in this O space. And then what you\nwant to do is you want to go as far\nas you can this way.",
    "start": "1682720",
    "end": "1689500"
  },
  {
    "text": "And in this case, you\nwould find this point. And sure enough, it is\nindeed an optimal point.",
    "start": "1689500",
    "end": "1696040"
  },
  {
    "text": "And also, a lot of\nother ideas come up here that you would see. You could see that right here,\nthis is tangent to this thing,",
    "start": "1696040",
    "end": "1704800"
  },
  {
    "text": "and that roughly\nmeans and it's got a slope that's like minus\none over lambda or something. But it basically\ntells you, locally,",
    "start": "1704800",
    "end": "1712060"
  },
  {
    "text": "the trade-off between the two\nvariables-- between sorry, the two objectives. So there's going to be--\nit's like an exchange rate.",
    "start": "1712060",
    "end": "1719920"
  },
  {
    "text": "We'll get to that. And in fact, it is correct\nto think of the lambda here as a vector of prices.",
    "start": "1719920",
    "end": "1725980"
  },
  {
    "text": "We'll get to that. So in some ways, this is making\nsomething very intuitive kind",
    "start": "1725980",
    "end": "1733870"
  },
  {
    "text": "of fancy. But that's OK. This doesn't bother me. I mean a gazillion people see\nmulti-criterion optimization",
    "start": "1733870",
    "end": "1742720"
  },
  {
    "text": "all the time. Then all they do is\nthey just immediately do-- without even saying\nanything-- they just",
    "start": "1742720",
    "end": "1748000"
  },
  {
    "text": "say, how do we solve it? And we'll just throw\nin some weights.",
    "start": "1748000",
    "end": "1753010"
  },
  {
    "text": "Like you're doing regularization\nin machine learning. And so you throw in\nloss plus lambda.",
    "start": "1753010",
    "end": "1760480"
  },
  {
    "text": "Why is this dinging at me? There we go. You throw in loss plus lambda\ntimes your regularizer.",
    "start": "1760480",
    "end": "1767850"
  },
  {
    "text": "Everybody, you've seen this. And you've seen this\nin lots of-- and people they don't even say\nanything about it, but just because it's\nobvious and it's fine.",
    "start": "1767850",
    "end": "1773700"
  },
  {
    "text": "But this is what you were doing OK, now for\nmulti-criterion problems,",
    "start": "1773700",
    "end": "1778710"
  },
  {
    "text": "we can be a lot more\nspecific about what",
    "start": "1778710",
    "end": "1785669"
  },
  {
    "text": "this particular\nscalarization does. So what it does is it's\nactually kind of cool.",
    "start": "1785670",
    "end": "1791700"
  },
  {
    "text": "It basically says,\nhere are these things that you want small. Capital F1 all the\nway to capital Fq.",
    "start": "1791700",
    "end": "1797010"
  },
  {
    "text": "You want them all small, right. Then, these are positive\nnumbers and we can actually",
    "start": "1797010",
    "end": "1802530"
  },
  {
    "text": "literally interpret the\nlambdas as literally the price",
    "start": "1802530",
    "end": "1808410"
  },
  {
    "text": "of that objective. That's how much you\nwill pay if that's 12 and that's like\nwhatever 10 then this",
    "start": "1808410",
    "end": "1815070"
  },
  {
    "text": "is going to cost you $120 bucks,\nso the lambdas are literally prices here.",
    "start": "1815070",
    "end": "1820390"
  },
  {
    "text": "And this would be\nlike, for example, if we had two production\nproposals-- you say, well, I know how to--",
    "start": "1820390",
    "end": "1825460"
  },
  {
    "text": "or if I'm designing\na circuit and I say I want both the\narea to be small, the dynamic power to be\nsmall, these types of things,",
    "start": "1825460",
    "end": "1835240"
  },
  {
    "text": "we would just we would just set\nprices for those objectives. And then this is\nthe total price.",
    "start": "1835240",
    "end": "1841150"
  },
  {
    "text": "What's interesting there is\nyou can change the prices and you'll get a\ndifferent choice.",
    "start": "1841150",
    "end": "1846230"
  },
  {
    "text": "In fact, this is what will\nallow you to actually move along the trade-off curve\nif there's two.",
    "start": "1846230",
    "end": "1852100"
  },
  {
    "text": "If there's more, people call\nit the trade-off surface. Everybody got this? So it's all pretty\nstraightforward,",
    "start": "1852100",
    "end": "1857920"
  },
  {
    "text": "but it's kind of\ninteresting to look at. So here we go.",
    "start": "1857920",
    "end": "1863169"
  },
  {
    "text": "Let's do the regularized\nleast square-- here we go. Here's-- regularized least\nsquares is, I want this thing,",
    "start": "1863170",
    "end": "1870270"
  },
  {
    "text": "interpreted as a fitting error,\nto small and then I want also the x, which in this case, we\ninterpret as the parameters",
    "start": "1870270",
    "end": "1876900"
  },
  {
    "text": "and let's say a\nregression model, I want those to be small. We'll talk much more\nabout that later.",
    "start": "1876900",
    "end": "1882030"
  },
  {
    "text": "But how do I do that? Well, I will take something\nthat's a positive two-vector.",
    "start": "1882030",
    "end": "1890909"
  },
  {
    "text": "By the way, if I\nmultiply that vector by-- I can scale it by any positive\nnumber and I'm going to get, when I solve this problem,\nI'm going to get this.",
    "start": "1890910",
    "end": "1897720"
  },
  {
    "text": "It's just a multiple\nof this so I can without loss of generality\nmake the first one one. By the way people often\ncall, they often interpret",
    "start": "1897720",
    "end": "1906480"
  },
  {
    "text": "the first objective-- they call\nthat their primary objective. And then, in this\ncase, you'd say,",
    "start": "1906480",
    "end": "1912550"
  },
  {
    "text": "well our training loss\nis the primary objective and your secondary objective\nis that the parameter shouldn't",
    "start": "1912550",
    "end": "1920250"
  },
  {
    "text": "be big. And so basically gamma\nis a translation.",
    "start": "1920250",
    "end": "1926770"
  },
  {
    "text": "It's an exchange rate that tells\nyou how much large x irritates",
    "start": "1926770",
    "end": "1935440"
  },
  {
    "text": "you compared to large residual. That's what it tells you. If it's 15, it\nmeans I don't know.",
    "start": "1935440",
    "end": "1943059"
  },
  {
    "text": "It means something, right. A lot. It depends on the units\nof these things, right. By the way in many cases,\nit's actually very cool",
    "start": "1943060",
    "end": "1949570"
  },
  {
    "text": "to think in terms o-- to put\nthe physical units in there. If one objective is fuel use, it\nmight be in kilograms of fuel.",
    "start": "1949570",
    "end": "1958390"
  },
  {
    "text": "Another objective could be\nthe smoothness of the ride. It could literally be something\nlike the RMS value of the jerk.",
    "start": "1958390",
    "end": "1966009"
  },
  {
    "text": "People know what that is? Sorry. OK, that was quasi-slang,\nquasi-dialect.",
    "start": "1966010",
    "end": "1972760"
  },
  {
    "text": "In control and\nmechanical engineering, the first you have the\nposition, first derivative.",
    "start": "1972760",
    "end": "1978760"
  },
  {
    "text": "Second derivative\nis acceleration, and the third is jerk. And then after that, the\nfourth derivative is snap.",
    "start": "1978760",
    "end": "1989759"
  },
  {
    "text": "Good, snap. Right. So this is just universal. For example, when you get\non a fancy modern elevator,",
    "start": "1989760",
    "end": "1995820"
  },
  {
    "text": "not in this country. But if you had one that\nhad real technology, you got a fancy one, you\nwouldn't even feel moving.",
    "start": "1995820",
    "end": "2001280"
  },
  {
    "text": "The reason is they actually\nhave a trajectory that has minimized the snap.",
    "start": "2001280",
    "end": "2006590"
  },
  {
    "text": "Everybody got that? Also, I think drone trajectories\nare designed to minimize snap.",
    "start": "2006590",
    "end": "2013320"
  },
  {
    "text": "So if you minimize the snap,\nthat's the fourth derivative. Again, this is\ncultural enrichment. You don't need to know\nthis, but it's fun.",
    "start": "2013320",
    "end": "2019220"
  },
  {
    "text": "And anybody the-- they\nhave three more derivatives which are never used,\nbut there's a good joke.",
    "start": "2019220",
    "end": "2026059"
  },
  {
    "text": "Crackle, pop. There you go. So the crackle and pop. That's it. So you can really impress\nyour mechanical engineering",
    "start": "2026060",
    "end": "2032270"
  },
  {
    "text": "or control friends by\nsaying, if they're saying, what are you doing, I'm making\na drone fly around the Stanford",
    "start": "2032270",
    "end": "2038660"
  },
  {
    "text": "campus. And you go, cool. What are you minimizing? And they'll say,\nRMS value of snap.",
    "start": "2038660",
    "end": "2045740"
  },
  {
    "text": "And you go, how\nabout crackle or pop? And you'll get a\nrise out of them.",
    "start": "2045740",
    "end": "2050810"
  },
  {
    "text": "I mean, I assume. OK sorry. That was a big digression. OK we're back. OK, so this is fine.",
    "start": "2050810",
    "end": "2057579"
  },
  {
    "text": "Everybody, this is just\na least squares problem. And you can solve it and\neveryone here has probably seen this in some context. It's ridge regression, but\nwe'll get to that later, OK?",
    "start": "2057580",
    "end": "2065840"
  },
  {
    "text": "Yeah. On the previous slide\nthere was a constraint that lambda had to\nbe in k-star, right?",
    "start": "2065840",
    "end": "2070940"
  },
  {
    "text": "That's right. Explain the intuition\nbehind that. Oh. Well, OK.",
    "start": "2070940",
    "end": "2076520"
  },
  {
    "text": "First, there's the math, right. When I form this, this better\nbe a scalar convex optimization",
    "start": "2076520",
    "end": "2082580"
  },
  {
    "text": "problem. And when I have when I have\nan objective function, which is a vector and it's convex\nwith respect to a cone k,",
    "start": "2082580",
    "end": "2091429"
  },
  {
    "text": "then actually if you\nform lambda-transpose f0 of x when lambda is\npositive in the dual cone,",
    "start": "2091429",
    "end": "2099200"
  },
  {
    "text": "that's guaranteed to\nbe a convex function. If not, if that fails,\nthis need not be convex.",
    "start": "2099200",
    "end": "2105490"
  },
  {
    "text": "And so this is all very\nnice, well and good, but we can't solve it. I don't know if that\ncounts as intuition.",
    "start": "2105490",
    "end": "2111930"
  },
  {
    "text": "But anyway. And the dual of R plus to\nthe q is R plus 2 the q.",
    "start": "2111930",
    "end": "2118140"
  },
  {
    "text": "So basically it says-- I mean this is a way to make\nin the multi-criterion case. It's making it much more\nfancy than it needs to be.",
    "start": "2118140",
    "end": "2125339"
  },
  {
    "text": "It would basically say take\na bunch of positive weights, minimize the positive weighted\nsum of the objectives.",
    "start": "2125340",
    "end": "2131980"
  },
  {
    "text": "And the statement is you\nwill get a Pareto optimal point, right. You want another one?",
    "start": "2131980",
    "end": "2137560"
  },
  {
    "text": "Change the weights, right. And we'll talk about what\nthe weights mean, too.",
    "start": "2137560",
    "end": "2142720"
  },
  {
    "text": "Yeah. [INAUDIBLE] almost an\noperator blue point Sorry, I missed that.",
    "start": "2142720",
    "end": "2148905"
  },
  {
    "text": "Is that we can find\nby varying lambda? Oh we'll get to that.",
    "start": "2148905",
    "end": "2154900"
  },
  {
    "text": "So it turns out, yeah,\nas you vary lambda, you will get almost all\nPareto optimal points.",
    "start": "2154900",
    "end": "2161920"
  },
  {
    "text": "You won't get the limits, right. In this problem, you would\nnot get that dark point right there.",
    "start": "2161920",
    "end": "2168102"
  },
  {
    "text": "So sorry actually--\nthere's going to be-- this is a non-convex problem. So I shouldn't be using\nthat as an example, right.",
    "start": "2168102",
    "end": "2173619"
  },
  {
    "text": "So we'll get to that. OK.",
    "start": "2173620",
    "end": "2179320"
  },
  {
    "text": "All right. I guess we looked at this. And the risk-return\ntrade-off, this",
    "start": "2179320",
    "end": "2185410"
  },
  {
    "text": "shows you actually\nhow we compute that, although there's\nseveral other ways and I'll explain that. By the way, this has a name.",
    "start": "2185410",
    "end": "2191050"
  },
  {
    "text": "If you flip the\nsign on this it's p-bar transpose x plus gamma. This that's actually\ncalled the return.",
    "start": "2191050",
    "end": "2197200"
  },
  {
    "text": "If I flip the sign,\np-bar transpose x is the return, minus\ngamma x-transpose sigma x.",
    "start": "2197200",
    "end": "2203619"
  },
  {
    "text": "So that, when you do\nthat, that's the risk. Gamma times-- that's called\nthe risk-adjusted return.",
    "start": "2203620",
    "end": "2208930"
  },
  {
    "text": "That's just a completely\nstandard phrase. Everybody got that? And then gamma is called the\nrisk-aversion parameter, right,",
    "start": "2208930",
    "end": "2217890"
  },
  {
    "text": "because basically\nbecause of the sign. In that case, you want to\nmaximize risk-adjusted return,",
    "start": "2217890",
    "end": "2224100"
  },
  {
    "text": "and it's basically something\ngood, which is a return minus-- that means it\npenalizes you to make",
    "start": "2224100",
    "end": "2231029"
  },
  {
    "text": "a choice that has high risk. And gamma tells you how\nmuch risk irritates you and it puts you on the\nscale of the return.",
    "start": "2231030",
    "end": "2237910"
  },
  {
    "text": "So that's why it's called\nthe risk-aversion parameter. But this is not a\nbad thing to know. Everybody got this?",
    "start": "2237910",
    "end": "2244230"
  },
  {
    "text": "By the way, there's\nplenty of other ways to solve this problem. I'm going to go back to\nthat and just mention it,",
    "start": "2244230",
    "end": "2250800"
  },
  {
    "text": "because you'll be doing\nthis on your homework. You don't have to--you\ncould scalarize. It'll work.",
    "start": "2250800",
    "end": "2255930"
  },
  {
    "text": "I guarantee it'll work. You could do another thing that\nwould work perfectly well here.",
    "start": "2255930",
    "end": "2261060"
  },
  {
    "text": "I could sweep. Let's say I'm going to sweep\nacross standard deviation of error like this, right\nThat's a square root",
    "start": "2261060",
    "end": "2267453"
  },
  {
    "text": "of this or something, right. So what I do, is\nI would simply-- I would minimize minus\np-bar transpose x.",
    "start": "2267453",
    "end": "2273420"
  },
  {
    "text": "And then I'd put a constraint\non that and sweep the constraint value and you'd\nget the same thing. It basically being--",
    "start": "2273420",
    "end": "2278829"
  },
  {
    "text": "I would say, suppose your-- I would say your risk\nshould be less than 10%.",
    "start": "2278830",
    "end": "2283910"
  },
  {
    "text": "And then you simply\nmaximize the return. And you would get this point. And then you'd have, for\nrisk limit in risk limits--",
    "start": "2283910",
    "end": "2293710"
  },
  {
    "text": "I don't know if I'm kind\nof speaking, roughly, in Python or something. Right.",
    "start": "2293710",
    "end": "2299170"
  },
  {
    "text": "You'd solve the problem and then\nreport the return and the risk and then you'd plot it.",
    "start": "2299170",
    "end": "2305119"
  },
  {
    "text": "Everybody got that? So OK. You can do that. I think-- I forget.",
    "start": "2305120",
    "end": "2311230"
  },
  {
    "text": "It asks you to get a trade-off\nin some energy storage system. I think that's on\nthe current homework",
    "start": "2311230",
    "end": "2318359"
  },
  {
    "text": "or did I just remember\nthat correctly? I think it is, OK.",
    "start": "2318360",
    "end": "2323400"
  },
  {
    "text": "OK, so all right. That finishes that.",
    "start": "2323400",
    "end": "2328530"
  },
  {
    "text": "Actually, it's a perfect\nsegue into the next topic. So the next topic is duality.",
    "start": "2328530",
    "end": "2335250"
  },
  {
    "text": "It's actually really fun. It has to do with Lagrange\nmultipliers and stuff. And I don't mind saying that\nthe first two or three times",
    "start": "2335250",
    "end": "2342830"
  },
  {
    "text": "I saw Lagrange multipliers,\nI had absolutely no idea what it meant. It was taught as a behavior\nand it was basically",
    "start": "2342830",
    "end": "2349460"
  },
  {
    "text": "the behavior was this. Like take your constraints,\nadd multiple-- sorry.",
    "start": "2349460",
    "end": "2355277"
  },
  {
    "text": "Take your objective,\nadd to it some multiple of the constraints\nand you're like, cool. And you go, what those? Those are Lagrange multipliers.",
    "start": "2355277",
    "end": "2360319"
  },
  {
    "text": "And you go, now what do you do? Take partial derivative\nset them equal to zero. You go, but what does\nit mean and they're like",
    "start": "2360320",
    "end": "2365780"
  },
  {
    "text": "shut up and just do it. I mean that was kind of my\nexperience of it, right. And the second time it\nhappened, there it was again.",
    "start": "2365780",
    "end": "2371990"
  },
  {
    "text": "And I was like, but\nwhat are these things? They were like,\ndon't worry about it. So anyway, good news is I\nhope that by next Thursday",
    "start": "2371990",
    "end": "2380570"
  },
  {
    "text": "you will not be-- you will understand\nexactly what it means or at least this\nis the first time",
    "start": "2380570",
    "end": "2386630"
  },
  {
    "text": "I ever understood it was in the\ncontext of convex optimization. OK, so let's look at this. And it starts slowly and it\njust very innocently goes step",
    "start": "2386630",
    "end": "2399180"
  },
  {
    "text": "by step and you'll\nsee after a while we start saying things that\nare not remotely obvious, OK.",
    "start": "2399180",
    "end": "2405060"
  },
  {
    "text": "OK, here it is. So we'll start with\nthe Lagrangian. Oh, by the way this\nstrongly suggests",
    "start": "2405060",
    "end": "2410340"
  },
  {
    "text": "that this is not a new\ndevelopment, right, because Lagrange was doing\nthis around 1820 or 1810",
    "start": "2410340",
    "end": "2416292"
  },
  {
    "text": "or something like that. Or maybe even earlier. I can't remember. Anyway, so now the\ntruth is he probably",
    "start": "2416292",
    "end": "2422579"
  },
  {
    "text": "he would recognize like\nbaby versions of it. And so this is just an honorary\nthing, where it's attributed.",
    "start": "2422580",
    "end": "2430050"
  },
  {
    "text": "It's called the Lagrangian\nand I don't know. ",
    "start": "2430050",
    "end": "2437430"
  },
  {
    "text": "It didn't have inequality\nconstraints then, but whatever it's just\nan honorary thing, OK. So here's what it is.",
    "start": "2437430",
    "end": "2442960"
  },
  {
    "text": "So you have this\nstandard form problem. And by the way, a lot of the\ninteresting applications,",
    "start": "2442960",
    "end": "2448390"
  },
  {
    "text": "the problem is not\nconvex, OK, so just I have a standard form problem.",
    "start": "2448390",
    "end": "2453500"
  },
  {
    "text": "And I'm just going to\nform the Lagrangian and the Lagrangian\nis going to be this. It's actually super interesting.",
    "start": "2453500",
    "end": "2458850"
  },
  {
    "text": "It goes like this. It's a single function. It's going to be the\nobjective plus a weighted sum",
    "start": "2458850",
    "end": "2466970"
  },
  {
    "text": "of the constraints, plus\nthis inequality constraints,",
    "start": "2466970",
    "end": "2472330"
  },
  {
    "text": "plus a weighted sum of\nthe equality constraints. OK, that's the Lagrangian.",
    "start": "2472330",
    "end": "2479170"
  },
  {
    "text": "That's the idea. And it's kind of--",
    "start": "2479170",
    "end": "2484600"
  },
  {
    "text": "then people would\ngive ideas like this. Oh, so one way to say\nit is you would say,",
    "start": "2484600",
    "end": "2489700"
  },
  {
    "text": "this is a planned\neconomy or something. This says these are just\nrules and regulations.",
    "start": "2489700",
    "end": "2495339"
  },
  {
    "text": "If you violate this\nor this, then somebody in law enforcement or the SEC\nis going to come after you.",
    "start": "2495340",
    "end": "2502790"
  },
  {
    "text": "That's a plan,\nit's a requirement. Then you'd say, oh, well this\nis like the market version.",
    "start": "2502790",
    "end": "2509950"
  },
  {
    "text": "And by the way, all of this\nwill become a bit more precise later. But the market\nversion says, well you know what you want to\nviolate that inequality?",
    "start": "2509950",
    "end": "2518920"
  },
  {
    "text": "OK, that's fine. And if lambda is\npositive, it just says you're going to pay for it. That's what it says.",
    "start": "2518920",
    "end": "2524260"
  },
  {
    "text": "That's the idea. Also, let me just draw a\npicture to show you actually how appallingly naive this\nlooks, because it's very weird.",
    "start": "2524260",
    "end": "2535040"
  },
  {
    "text": " I'm just going to draw-- let's have a problem.",
    "start": "2535040",
    "end": "2541070"
  },
  {
    "text": "This is zero and I'm going to--\nthis is going to be f1 of x. This is just f1 of x.",
    "start": "2541070",
    "end": "2547750"
  },
  {
    "text": "And I'm going to have\nan objective, so I'm going to plot f0 of x here.",
    "start": "2547750",
    "end": "2553100"
  },
  {
    "text": "So here, if you propose to me an\nx, we will evaluate f0 and f1.",
    "start": "2553100",
    "end": "2559250"
  },
  {
    "text": "And the semantics of\nan optimization problem is, if you're over here,\nit's completely unacceptable",
    "start": "2559250",
    "end": "2566359"
  },
  {
    "text": "because you violate\nthe constraint. Because f1 of x is positive,\nso this is unacceptable.",
    "start": "2566360",
    "end": "2572420"
  },
  {
    "text": "if you're on the left\nside, everything is cool and you want to go\nas low as possible. In fact, that's the semantics\nof an optimization problem.",
    "start": "2572420",
    "end": "2579440"
  },
  {
    "text": "Everybody got this? Roughly speaking,\nyou're saying, here's how I can also make it\nan unconstrained problem",
    "start": "2579440",
    "end": "2587930"
  },
  {
    "text": "by just writing it this way,\nthe indicator function of f1 of x is less than 0.",
    "start": "2587930",
    "end": "2593690"
  },
  {
    "text": "So this is 0 if this\npredicate evaluates to true and this is infinity,\nif this is violated.",
    "start": "2593690",
    "end": "2601910"
  },
  {
    "text": "And so solving the\nconstraint problem is the same as\nminimizing this function.",
    "start": "2601910",
    "end": "2607060"
  },
  {
    "text": "Everybody see what I mean? It's kind of silly, but that's\nwhat-- everybody got it? OK, so I'm going to draw\nthe indicator function.",
    "start": "2607060",
    "end": "2613430"
  },
  {
    "text": "It looks like this. So someone says, what are\nyou doing and you say, well, I'm converting my constrained\nproblem to one that's",
    "start": "2613430",
    "end": "2620750"
  },
  {
    "text": "unconstrained by putting\nthe second term, which handles the objective. And it's a weird term.",
    "start": "2620750",
    "end": "2626750"
  },
  {
    "text": "It's like a weird, emotional--\nit's like a bipolar, it's either 0 or plus infinity.",
    "start": "2626750",
    "end": "2632660"
  },
  {
    "text": "So it's just like,\nOK that's fine. So basically that's\nthis picture here.",
    "start": "2632660",
    "end": "2638510"
  },
  {
    "text": "OK, now let's look\nat the Lagrangian. Well the Lagrangian is going to\nbe f0 of x plus lambda f1 of x.",
    "start": "2638510",
    "end": "2649430"
  },
  {
    "text": "That's the Lagrangian. I'm going to draw that. OK, here, it looks like this.",
    "start": "2649430",
    "end": "2654579"
  },
  {
    "text": "That's lambda. That slope is lambda.",
    "start": "2654580",
    "end": "2660960"
  },
  {
    "text": "And now you can see-- and by the way, if\nthe story starts, I want to develop methods\nthat handle constraints,",
    "start": "2660960",
    "end": "2668750"
  },
  {
    "text": "but using unconstrained\noptimization. And the first thing\nsomeone does is say, yeah,",
    "start": "2668750",
    "end": "2674630"
  },
  {
    "text": "problem you see this thing\nI'm going to approximate it by that, you would say that is\nnot a very good approximation.",
    "start": "2674630",
    "end": "2683460"
  },
  {
    "text": "Like everybody agrees,\nit's ridiculous. So I'm just saying that the\nwhole beginning to this story is appalling.",
    "start": "2683460",
    "end": "2688770"
  },
  {
    "text": "It just doesn't make any sense. That curve, that line is\nnot a good approximation",
    "start": "2688770",
    "end": "2696490"
  },
  {
    "text": "of the true one. Also, it shows you all\nsorts of weird differences. In the true\noptimization problem,",
    "start": "2696490",
    "end": "2702850"
  },
  {
    "text": "if you have a\nwarehouse usage limit, this is how much your f1\nof x is, how much you're",
    "start": "2702850",
    "end": "2708670"
  },
  {
    "text": "above your limit. And we're using warehouse space. Then basically it says\nyou go above the limit. Totally unacceptable.",
    "start": "2708670",
    "end": "2714880"
  },
  {
    "text": "If you're below, I don't care. You could be using\nall of your warehouse or I don't know you\ncan come in over here,",
    "start": "2714880",
    "end": "2721512"
  },
  {
    "text": "which is like you got a bunch\nof warehouses you're not even using. You see what I'm saying? And guess what? In the optimization\nproblem, there's",
    "start": "2721512",
    "end": "2729760"
  },
  {
    "text": "nothing different about them. This one has a super\ninteresting interpretation. It's a price, because presumably\nf0 is in like US dollars",
    "start": "2729760",
    "end": "2738280"
  },
  {
    "text": "or something like that. And what this says is, yeah, you\nwant to go over your maximum? No problem.",
    "start": "2738280",
    "end": "2743590"
  },
  {
    "text": "You want to operate over here? Go ahead. This says you're going to pay\nfor extra warehouse space.",
    "start": "2743590",
    "end": "2748690"
  },
  {
    "text": "Everybody got that? But then, there's also a flip\nside, which is over here. You will be subsidized\nfor having extra.",
    "start": "2748690",
    "end": "2757309"
  },
  {
    "text": "You can rent out warehouse\nspace you're not using and actually generate income.",
    "start": "2757310",
    "end": "2762710"
  },
  {
    "text": "So it's really quite different. Everybody see this? This is just to give you\nthe idea of how all this is",
    "start": "2762710",
    "end": "2769549"
  },
  {
    "text": "going to go together. But these are the ideas. They're very basic. Everybody got this?",
    "start": "2769550",
    "end": "2775010"
  },
  {
    "text": "OK, so that's the idea. And people would blabber about a\nmarket economy versus a planned economy.",
    "start": "2775010",
    "end": "2780570"
  },
  {
    "text": "And so on and so forth. But this is kind of the idea. Also, that strongly\nhints these weights,",
    "start": "2780570",
    "end": "2788090"
  },
  {
    "text": "these Lagrange\nmultipliers they're going to be-- they're\ngoing to, in many cases, turn out to be prices.",
    "start": "2788090",
    "end": "2794570"
  },
  {
    "text": "OK, so that's what\nthey're going to be. OK, so now, and this gets very\nweird, but it's like this.",
    "start": "2794570",
    "end": "2803700"
  },
  {
    "text": "You'd say, OK, let's take\nthis Lagrangian, which is sort of an attempt\nto basically handle",
    "start": "2803700",
    "end": "2810380"
  },
  {
    "text": "the constraints for a\nproblem in the objective. But you're doing it in a\nweird price-directed way.",
    "start": "2810380",
    "end": "2815900"
  },
  {
    "text": "You're just saying there's a\nprice for each of these things and you can violate\nthem, blah, blah, blah. So it looks like that.",
    "start": "2815900",
    "end": "2821480"
  },
  {
    "text": "OK, now, what we're\ngoing to do is we're going to define\nthe dual function.",
    "start": "2821480",
    "end": "2827759"
  },
  {
    "text": "And it's going to be a function\nof these Lagrange multipliers. They're also called\ndual variables.",
    "start": "2827760",
    "end": "2833392"
  },
  {
    "text": "By the way, they're\nalso called all sorts in many application areas. They have specific names\nin that application area.",
    "start": "2833392",
    "end": "2842060"
  },
  {
    "text": "In many, they're\ncalled shadow prices. Or, for example,\nin energy, they're called locational marginal.",
    "start": "2842060",
    "end": "2848300"
  },
  {
    "text": "Locational marginal prices,\nis what they're called. In a lot of fields, these\nthings just have names.",
    "start": "2848300",
    "end": "2854930"
  },
  {
    "text": " So this thing is the least cost\nhere, over all your choices",
    "start": "2854930",
    "end": "2864860"
  },
  {
    "text": "with these Lagrange\nmultipliers, which we'll later see interpreted as prices.",
    "start": "2864860",
    "end": "2871430"
  },
  {
    "text": "You minimize that and\nwhatever the minimum is, that's called this\ndual function. You saw something like that.",
    "start": "2871430",
    "end": "2877430"
  },
  {
    "text": "It was a conjugate\nfunction, if you remember that one interpretation\nof a conjugate function was one is you interpreted\ny as a vector of prices.",
    "start": "2877430",
    "end": "2885650"
  },
  {
    "text": "And then the\nconjugate function was the optimal cost,\nthe minimal cost,",
    "start": "2885650",
    "end": "2892640"
  },
  {
    "text": "as a function of the prices. I don't know if you remember\nthat interpretation. This is exactly that.",
    "start": "2892640",
    "end": "2897660"
  },
  {
    "text": "If you think of these as\nprices, which you will shortly, then basically the dual\nfunction is the optimal cost",
    "start": "2897660",
    "end": "2904400"
  },
  {
    "text": "as a function of the prices. OK, now we're going to make some\nextremely simple observations.",
    "start": "2904400",
    "end": "2914329"
  },
  {
    "text": "The first is this.  The first one is this\nfunction. g is concave.",
    "start": "2914330",
    "end": "2921370"
  },
  {
    "text": "The dual function is concave. Even when the original\nproblem is not convex,",
    "start": "2921370",
    "end": "2927730"
  },
  {
    "text": "it's just concave. Actually, somebody\nwant to explain why? Why Is g concave?",
    "start": "2927730",
    "end": "2933745"
  },
  {
    "text": " A minimum of convex functions.",
    "start": "2933745",
    "end": "2941390"
  },
  {
    "text": "Yeah, it's a minimal\nof affine functions, which promote to convex. Yeah, if you look\nat the Lagrangian,",
    "start": "2941390",
    "end": "2948770"
  },
  {
    "text": "it is an affine function\nof lambda and nu. If you consider lambda\nand nu the variables,",
    "start": "2948770",
    "end": "2954290"
  },
  {
    "text": "it's an affine function. Now it might be\nreally complicated, because I don't know\nthat's for any that's for any particular value of x.",
    "start": "2954290",
    "end": "2960350"
  },
  {
    "text": "It's an affine function. This says, just take\nthe infimum or minimum over a family of affine\nfunctions that's concave,",
    "start": "2960350",
    "end": "2966830"
  },
  {
    "text": "period. OK, so the first\none is this thing is concave, no matter what. Always.",
    "start": "2966830",
    "end": "2972290"
  },
  {
    "text": "Just period. OK, second one is weird. And it looks very\ndeep and you will soon",
    "start": "2972290",
    "end": "2979010"
  },
  {
    "text": "see stunningly non-obvious\nconsequences come out of it, but it just says this.",
    "start": "2979010",
    "end": "2985310"
  },
  {
    "text": "It says as long as the\nLagrange multipliers associated with the inequality\nconstraints are non-negative,",
    "start": "2985310",
    "end": "2992150"
  },
  {
    "text": "then it says that g of lambda\nand nu is less than p-star. OK, so why would that be?",
    "start": "2992150",
    "end": "3001369"
  },
  {
    "text": "Well, suppose\nx-tilde is feasible. If x-tilde is feasible,\nthen phi of x-tilde,",
    "start": "3001370",
    "end": "3007480"
  },
  {
    "text": "these things are\nall non-positive. These are all zero. So if x-tilde is feasible,\nthen this term just goes away.",
    "start": "3007480",
    "end": "3017920"
  },
  {
    "text": "These, the lambda-I-s I'm\ngoing to assume are the prices, are non-negative. The phi of x are less\nthan or equal to 0",
    "start": "3017920",
    "end": "3024910"
  },
  {
    "text": "because it's feasible. So this term is less than\nor equal to zero, everybody.",
    "start": "3024910",
    "end": "3030460"
  },
  {
    "text": "By the way, I'm using extremely\ndeep mathematics here. Basically, the fact\nthat the product of a non-negative number\nand a non-positive number",
    "start": "3030460",
    "end": "3037869"
  },
  {
    "text": "is non-positive. And that when you sum\nnon-positive numbers, you get something\nthat's non-positive.",
    "start": "3037870",
    "end": "3043540"
  },
  {
    "text": "Oh, and one more deep fact. If you take a number and you\nadd a non-positive number to it, it's less than the\noriginal number.",
    "start": "3043540",
    "end": "3050380"
  },
  {
    "text": "I'm just saying there's\nnothing deep yet. I will tell you when we\nget to the deep part,",
    "start": "3050380",
    "end": "3055839"
  },
  {
    "text": "but this wasn't it. And that says that if\nx-tilde is feasible,",
    "start": "3055840",
    "end": "3064220"
  },
  {
    "text": "then the Lagrangian is less\nthan or equal to f0 of x.",
    "start": "3064220",
    "end": "3070380"
  },
  {
    "text": "The objective, right? Therefore, if you\nminimize over that, you're going to get something\nthat's a lower bound.",
    "start": "3070380",
    "end": "3076860"
  },
  {
    "text": "So you get g of lambda\nand nu is actually a lower bound on the optimal\nvalue for your problem.",
    "start": "3076860",
    "end": "3082950"
  },
  {
    "text": "OK so everybody got that again? Yeah.",
    "start": "3082950",
    "end": "3088840"
  },
  {
    "text": "--the same property holds\nfor nu as well as lambda? I missed that. What was that? This property also holds for nu\nin addition to lambda, right.",
    "start": "3088840",
    "end": "3095769"
  },
  {
    "text": "There's nothing special\nabout lambda in this case. No. What's required is that the\nLagrange multipliers associated",
    "start": "3095770",
    "end": "3104050"
  },
  {
    "text": "with the inequality constraints\nhave to be non-negative. By the way, it makes\nperfect sense over here, because while this is\nnot a good approximation.",
    "start": "3104050",
    "end": "3114250"
  },
  {
    "text": "I mean it's not a\ngood approximation. But what if someone came up\nto you and they said, oh yeah,",
    "start": "3114250",
    "end": "3120010"
  },
  {
    "text": "I can handle that for you. Yeah, your warehouse space. Yeah, I'm going to approximate\nthis function by that.",
    "start": "3120010",
    "end": "3128060"
  },
  {
    "text": "Now that's weird,\nbecause what this says is this basically says,\nI'll pay you to in fact,",
    "start": "3128060",
    "end": "3134850"
  },
  {
    "text": "I'll pay you more. And more to violate your\nwarehouse space limit. That's what it says. Oh, and by the way,\nif you actually",
    "start": "3134850",
    "end": "3140888"
  },
  {
    "text": "satisfy the constraint, I'm\ngoing to start charging you. So the other one was\nmerely a bad approximation.",
    "start": "3140888",
    "end": "3147650"
  },
  {
    "text": "This one is appallingly stupid. OK, so I'm just saying if you\nwanted a quick interpretation",
    "start": "3147650",
    "end": "3152960"
  },
  {
    "text": "of why lambda has to be\nbigger than or equal to 0, here is that. This doesn't even\nmake sense, right.",
    "start": "3152960",
    "end": "3158557"
  },
  {
    "text": "Actually, if you\nthink about it, it's like it's a scalarization with\na negative number in there or something like that.",
    "start": "3158558",
    "end": "3164970"
  },
  {
    "text": "Sorry. I didn't mean to lambda. It can be less than 0, but\nit means that it's also the case that if nu is greater\nthan or equal to zero then",
    "start": "3164970",
    "end": "3172880"
  },
  {
    "text": "g of lambda nu is less\nthan or equal to-- That's true, but in fact,\nit's a stronger statement.",
    "start": "3172880",
    "end": "3178790"
  },
  {
    "text": "There are no conditions\non nu, whatsoever. It can be negative, can be\npositive, doesn't matter.",
    "start": "3178790",
    "end": "3185730"
  },
  {
    "text": "What you said is a strength. You said, yeah but if nu is not\nnegative and you go, yeah yeah, I agree. And we'll see a lot of\ntimes, we'll get to it.",
    "start": "3185730",
    "end": "3192810"
  },
  {
    "text": "It'll be interesting when nu--\nso the point is there's no constraint on nu--it could\nbe negative or positive.",
    "start": "3192810",
    "end": "3198510"
  },
  {
    "text": " If there is similar intuition\nbehind nu, in terms of--",
    "start": "3198510",
    "end": "3205785"
  },
  {
    "text": "Yeah, because I feel like\nthere is all the same. Yeah. Yeah, I will show\nyou what it is. OK, yeah.",
    "start": "3205785",
    "end": "3211320"
  },
  {
    "text": "What's the\ninterpretation for this? What are the names of their-- h?",
    "start": "3211320",
    "end": "3217725"
  },
  {
    "text": "Is that? Is h the name of--\nyeah, so let's do this. ",
    "start": "3217725",
    "end": "3224370"
  },
  {
    "text": "OK, so it looks like that. This is now going to be h1. And now I'm going to\ndraw the true object.",
    "start": "3224370",
    "end": "3235920"
  },
  {
    "text": "The true objective is this. If h1 of zero, it's cool. You're satisfying your\nequality constraint and you get a 0 outside.",
    "start": "3235920",
    "end": "3243000"
  },
  {
    "text": "That it's out here. This is plus infinity. Everybody agree? So it's a function that\nkind of looks like super big",
    "start": "3243000",
    "end": "3250590"
  },
  {
    "text": "goes down to zero\nif you're zero, and then goes back up again.",
    "start": "3250590",
    "end": "3255690"
  },
  {
    "text": "Now what this says is I'm\ngoing to approximate that with a linear function.",
    "start": "3255690",
    "end": "3260990"
  },
  {
    "text": "I mean, it's appallingly\nbad approximation, but it's saying\nsomething like this, so that's a bad approximation.",
    "start": "3260990",
    "end": "3267640"
  },
  {
    "text": "But so is that one, and they're\nall kind of a little bit similar. So anyway, this is\nthe picture for that.",
    "start": "3267640",
    "end": "3276220"
  },
  {
    "text": "Does that make sense? Yeah, but it's\nstill kind of-- it says that we can just like\nincrease it in some directions,",
    "start": "3276220",
    "end": "3282190"
  },
  {
    "text": "like make it\nsmaller and smaller. We'll get to that. Actually. My point about\nthis approximation",
    "start": "3282190",
    "end": "3287800"
  },
  {
    "text": "is that when I\nexplain it this way, it doesn't look like a good-- it doesn't look\nlike a story that",
    "start": "3287800",
    "end": "3293660"
  },
  {
    "text": "starts this way when someone\nwalks up to you and says, yeah, no problem I have this\nthing that looks like this or this, or even\nworse, this thing that",
    "start": "3293660",
    "end": "3299380"
  },
  {
    "text": "looks like this\njumps down to zero and I go I'm going to\napproximate it with a line. A normal person would\nsay, dude that's",
    "start": "3299380",
    "end": "3305930"
  },
  {
    "text": "a terrible approximation. That story is not\ngoing to end well. ",
    "start": "3305930",
    "end": "3312559"
  },
  {
    "text": "If someone told me that,\nit would be ridiculous. But obviously, the story\nis going to end well, or I wouldn't be\ntelling it to you.",
    "start": "3312560",
    "end": "3319490"
  },
  {
    "text": "But the point is it's shocking\nright when someone first says what it is. So that's I guess that's my--",
    "start": "3319490",
    "end": "3325400"
  },
  {
    "text": "so I haven't said-- it's\nobvious, in fact to me, right now the intuition\nshould be this is ridiculous.",
    "start": "3325400",
    "end": "3332760"
  },
  {
    "text": "It's completely ridiculous. How could anything good come\nof such a crappy approximation",
    "start": "3332760",
    "end": "3338355"
  },
  {
    "text": "--with lambda? We like we say it\nhas to be positive. So it's always like\nan additional cost",
    "start": "3338355",
    "end": "3343849"
  },
  {
    "text": "and that's what makes it like\nless intuitive for this one I think this is-- I'm saying this is\na bad-- this looks",
    "start": "3343850",
    "end": "3350903"
  },
  {
    "text": "when I describe it this way. It sounds like a\nreally bad idea. OK, I totally admit that.",
    "start": "3350903",
    "end": "3356480"
  },
  {
    "text": " Fine. Let's look at some examples\nto see how this works.",
    "start": "3356480",
    "end": "3362630"
  },
  {
    "text": "Yeah, where's the bar on\nthe previous, the x-bar? x-tilde?",
    "start": "3362630",
    "end": "3368010"
  },
  {
    "text": "Oh this just says,\nsuppose x-tilde is anything that's feasible\nfor the original problem.",
    "start": "3368010",
    "end": "3375690"
  },
  {
    "text": "Then what we're doing is showing\nthat the objective for x-tilde",
    "start": "3375690",
    "end": "3382500"
  },
  {
    "text": "is bigger than that. But x-tilde was anything. So if you pick, for\nexample, an optimal point",
    "start": "3382500",
    "end": "3388080"
  },
  {
    "text": "for the original\nproblem, this would be p-star and that would be g. And that would be\nthis constraint",
    "start": "3388080",
    "end": "3393360"
  },
  {
    "text": "here, sorry, inequality. ",
    "start": "3393360",
    "end": "3401000"
  },
  {
    "text": "OK, let's look at some examples. So here's one.",
    "start": "3401000",
    "end": "3406640"
  },
  {
    "text": "Minimize the sum of squares of\na vector subject to Ax equals b. It's like a least norm or\nsomething like that problem.",
    "start": "3406640",
    "end": "3412310"
  },
  {
    "text": "And there's an\nanalytical solution. So this is silly, but just\nto see how this works, right so here's what we do.",
    "start": "3412310",
    "end": "3419119"
  },
  {
    "text": "We form the Lagrangian. So that's the objective\nplus nu-transpose. That's an equality\nconstraint, nu-transpose.",
    "start": "3419120",
    "end": "3424850"
  },
  {
    "text": "Ax minus. That's the violation. This is a convex\nquadratic function.",
    "start": "3424850",
    "end": "3430609"
  },
  {
    "text": "We minimize that to get the\ndual function to minimize that.",
    "start": "3430610",
    "end": "3436233"
  },
  {
    "text": "You just set the\ngradient equal to zero. It's the set of linear\nequations and you get x is that.",
    "start": "3436233",
    "end": "3441980"
  },
  {
    "text": "Now I plug this x back in\nto L, that's the Lagrangian,",
    "start": "3441980",
    "end": "3448070"
  },
  {
    "text": "to find what g of nu is and\nwhen I do that, I get g of nu is this thing here.",
    "start": "3448070",
    "end": "3453980"
  },
  {
    "text": "And I'm not going over the\nalgebra, but it's not much. And sure enough, look\nat that is concave.",
    "start": "3453980",
    "end": "3462140"
  },
  {
    "text": "Why? Because this is convex. A transpose is convex-- here, sorry-- is\npositive semi-definite.",
    "start": "3462140",
    "end": "3468500"
  },
  {
    "text": "This is convex\nwith a minus sign. It's going to be concave, right. So sure enough.",
    "start": "3468500",
    "end": "3474410"
  },
  {
    "text": "So that's good. I mean we knew it\nhad to be, right. But now what's interesting\nabout that is it",
    "start": "3474410",
    "end": "3479960"
  },
  {
    "text": "says if you wanted to\nsolve this problem, then it says immediately if\nyou pick any vector nu at all",
    "start": "3479960",
    "end": "3489050"
  },
  {
    "text": "and you evaluate that\nfunction, that is a lower bound on this problem.",
    "start": "3489050",
    "end": "3496095"
  },
  {
    "text": " I don't know. Let's try some. Here's one. nu equals zero.",
    "start": "3496095",
    "end": "3504095"
  },
  {
    "text": "What's the lower\nbound on this problem?  But take nu equals zero.",
    "start": "3504095",
    "end": "3509480"
  },
  {
    "text": "What lower bound do I get? Zero. Then I say to you, oh wow.",
    "start": "3509480",
    "end": "3514910"
  },
  {
    "text": "Yeah, thanks for\nshowing me your problem. I can tell you this. You ready for this? By duality?",
    "start": "3514910",
    "end": "3521270"
  },
  {
    "text": "I can tell you, this optimal\nvalue could never be negative. And people would look\nat you like you're",
    "start": "3521270",
    "end": "3527280"
  },
  {
    "text": "an idiot because, of course,\nthe objective is non-negative. Everybody see what I'm saying? ",
    "start": "3527280",
    "end": "3536228"
  },
  {
    "text": "That's the lower bound, anyway. That's the idea. But this actually already\nhas weird applications, because you could have some\ngigantic problem like this.",
    "start": "3536228",
    "end": "3543480"
  },
  {
    "text": "We could solve it\nanalytically, but you have some quick heuristic to do it. You can have a billion\nvariables and you have",
    "start": "3543480",
    "end": "3548615"
  },
  {
    "text": "some quick heuristic to do it. And then you could\njust ask yourself, how far am I from optimal?",
    "start": "3548615",
    "end": "3555270"
  },
  {
    "text": "And you would just\ncalculate this bound. You'd have to find you'd have\nto find a good choice of nu",
    "start": "3555270",
    "end": "3560310"
  },
  {
    "text": "and you get a lower bound. And you go, I'm 5% optimal. And you're like, totally cool,\nbecause in our original problem",
    "start": "3560310",
    "end": "3565860"
  },
  {
    "text": "nothing there was accurate\nto more than 5%, anyway. So we'll get to that. There are going to be a lot\nof applications of this,",
    "start": "3565860",
    "end": "3571810"
  },
  {
    "text": "which we'll get to shortly. OK. Now it's going to\nbe not obvious. So let's do a standard\nform linear program.",
    "start": "3571810",
    "end": "3580650"
  },
  {
    "text": "So here, I want to minimize\nthis linear function, subject to equality constraints\nand a non-negativity on x.",
    "start": "3580650",
    "end": "3587359"
  },
  {
    "text": "So my Lagrangian is this. It's the objective plus equality\nconstraints' inner product",
    "start": "3587360",
    "end": "3593300"
  },
  {
    "text": "with this Lagrange\nmultiplier nu. And then I'm going to rewrite\nthis silently as minus x less",
    "start": "3593300",
    "end": "3600799"
  },
  {
    "text": "than or equal to 0, so\nit's in our standard form. And then you get minus\nlambda-transpose x.",
    "start": "3600800",
    "end": "3606290"
  },
  {
    "text": "And so this is the Lagrangian. What kind of function\nof x is this function?",
    "start": "3606290",
    "end": "3614960"
  },
  {
    "text": "What is it? It's affine, right? So to find g we have to\nminimize that function over x.",
    "start": "3614960",
    "end": "3621290"
  },
  {
    "text": "What's the minimum of\nan affine function?  What?",
    "start": "3621290",
    "end": "3626750"
  },
  {
    "text": "Yeah it's basically\nminus infinity. Usually, there is one exception.",
    "start": "3626750",
    "end": "3633540"
  },
  {
    "text": "How could the minimum of\nan affine function not be minus infinity?",
    "start": "3633540",
    "end": "3639405"
  },
  {
    "text": "How restricted on the domain? No, I'm not talking about\nrestricted on the domain.",
    "start": "3639405",
    "end": "3644510"
  },
  {
    "text": "I'm just having an\nordinary affine function. Here's my affine function.",
    "start": "3644510",
    "end": "3651819"
  },
  {
    "text": "Here, let's go,\np-transpose x plus q.",
    "start": "3651820",
    "end": "3657730"
  },
  {
    "text": "There's my affine function. I'd like to minimize\nthat over x and I'd like to know what the minimum is.",
    "start": "3657730",
    "end": "3663620"
  },
  {
    "text": "Well, it's usually\nminus infinity, right? But there's one\ncase where it's not.",
    "start": "3663620",
    "end": "3669830"
  },
  {
    "text": "If p is zero, which\nmeans that there's no if the linear part vanishes.",
    "start": "3669830",
    "end": "3675750"
  },
  {
    "text": "Then what is this thing? So if I write\nsomething like this. Inf over x of this,\nwhoops, right of this,",
    "start": "3675750",
    "end": "3684170"
  },
  {
    "text": "is going to be minus infinity.",
    "start": "3684170",
    "end": "3689420"
  },
  {
    "text": "But here it's going to be q. And that's if p equals zero. OK, I already got\nthat, so that's OK.",
    "start": "3689420",
    "end": "3696230"
  },
  {
    "text": "That was a pretty quick\ncalculation, there. And then that's what\nwe have over here.",
    "start": "3696230",
    "end": "3701670"
  },
  {
    "text": "So when I minimize this\nLagrangian over x to get g, that's the dual function.",
    "start": "3701670",
    "end": "3707900"
  },
  {
    "text": "I minimize this\nand basically what happens is if this is non-zero,\nthe minimum is minus infinity.",
    "start": "3707900",
    "end": "3714500"
  },
  {
    "text": "Otherwise, I assume this is zero\nand it's equal to that thing. So this is what I get.",
    "start": "3714500",
    "end": "3719780"
  },
  {
    "text": " So g is, in fact,\na linear function",
    "start": "3719780",
    "end": "3728640"
  },
  {
    "text": "restricted to a hyperplane. That's not a hyperplane,\nan affine set that",
    "start": "3728640",
    "end": "3736260"
  },
  {
    "text": "is in fact a concave function. It kind of has to be, right. And you know what?",
    "start": "3736260",
    "end": "3741930"
  },
  {
    "text": "This says I'm going\nto rewrite this as that A-transpose nu\nminus lambda plus c is zero. That means A-transpose\nnu plus c equals lambda.",
    "start": "3741930",
    "end": "3749190"
  },
  {
    "text": "Lambda is anything that\nis any parameter that's bigger than or equal to zero. And then, I get this\nlower bound property.",
    "start": "3749190",
    "end": "3754950"
  },
  {
    "text": "It says this. It says, you look at\nthis linear program and it says, yeah,\nyou know what.",
    "start": "3754950",
    "end": "3760800"
  },
  {
    "text": "The optimal value of\nthis thing is bigger than or equal to minus b-transpose\nnu if for any nu that",
    "start": "3760800",
    "end": "3767190"
  },
  {
    "text": "satisfies A-transpose\nnu plus c is bigger than or equal to zero. ",
    "start": "3767190",
    "end": "3774700"
  },
  {
    "text": "And that just came straight\nfrom the dual function. Now, it turns out\nfor a lot of these,",
    "start": "3774700",
    "end": "3779799"
  },
  {
    "text": "you can just show it\ndirectly and it's not hard. I mean, I could do let\nme do it for that one. And then I don't think\nI'll ever do it again,",
    "start": "3779800",
    "end": "3785795"
  },
  {
    "text": "but I'll do it for that one. Let's do that. So my original problem is\nminimize c-transpose x.",
    "start": "3785795",
    "end": "3795730"
  },
  {
    "text": "And then subject\nto, I think it's Ax equals b and x is\nbigger than zero, right.",
    "start": "3795730",
    "end": "3803140"
  },
  {
    "text": "And then the claim\nis if somebody walks up to me on the street and\nsays, I have a vector nu and--",
    "start": "3803140",
    "end": "3810490"
  },
  {
    "text": "what is it-- A-transpose nu\nplus c is bigger than zero,",
    "start": "3810490",
    "end": "3816340"
  },
  {
    "text": "they claim then this implies\nthat minus b-transpose--",
    "start": "3816340",
    "end": "3823973"
  },
  {
    "text": "sorry you have to\nlook at it over here-- nu is a lower bound on p-star.",
    "start": "3823973",
    "end": "3829299"
  },
  {
    "text": " So this is pretty cool, right.",
    "start": "3829300",
    "end": "3834412"
  },
  {
    "text": "So that's what it says.  If we just asked, so you came\nup to someone on the street",
    "start": "3834412",
    "end": "3840710"
  },
  {
    "text": "or now if we put\nthat on a homework, you couldn't you\ncouldn't do this. How would you show that?",
    "start": "3840710",
    "end": "3846380"
  },
  {
    "text": "It looks very deep,\nbut in fact, it used absolutely nothing but\nthe completely trivial steps",
    "start": "3846380",
    "end": "3851420"
  },
  {
    "text": "I had before. So let's just show this\ndirectly, just for fun.",
    "start": "3851420",
    "end": "3856460"
  },
  {
    "text": "So here's what\nwe're going to do. Suppose x is feasible. Any x is feasible that\nmeans Ax equals bx,",
    "start": "3856460",
    "end": "3864005"
  },
  {
    "text": "x is bigger than\nor equal to zero. If that's the case, then let\nme look at this, nu-transpose.",
    "start": "3864005",
    "end": "3874850"
  },
  {
    "text": "Then this must be true,\nright, because x is feasible. So Ax equals b, so this\nsurely holds, right.",
    "start": "3874850",
    "end": "3882641"
  },
  {
    "text": "Oh, and I'm assuming\nsomebody came up with a nu that satisfies this. Everybody agree with that? I mean you kind\nof have to, right?",
    "start": "3882642",
    "end": "3887990"
  },
  {
    "text": "It's like simple stuff. OK, but then I'm going\nto write this thing.",
    "start": "3887990",
    "end": "3894200"
  },
  {
    "text": "This says that A-transpose here. This is A-transpose nu plus c\nis bigger than or equal to zero.",
    "start": "3894200",
    "end": "3904460"
  },
  {
    "text": "We agree with that, right. Now x is also bigger\nthan zero, and I'm going to use an\nextremely deep fact",
    "start": "3904460",
    "end": "3910940"
  },
  {
    "text": "The inner product of\ntwo non-negative vectors is non-negative. ",
    "start": "3910940",
    "end": "3918049"
  },
  {
    "text": "It's just not hard. I'll put this on the other side,\nso I'll write nu-transpose A",
    "start": "3918050",
    "end": "3925115"
  },
  {
    "text": "and I'll multiply, let's\nsee, plus c-transpose x.",
    "start": "3925115",
    "end": "3931670"
  },
  {
    "text": "That's bigger than\nor equal to zero. Everyone agrees with that? And I'm just using the fact that\nthat's an inner product of two",
    "start": "3931670",
    "end": "3937610"
  },
  {
    "text": "non-negative vectors. So you have this. OK, expand this out,\nyou get nu-transpose Ax",
    "start": "3937610",
    "end": "3945260"
  },
  {
    "text": "plus c-transpose x\nis bigger than zero. ",
    "start": "3945260",
    "end": "3951160"
  },
  {
    "text": "But wait a minute. This thing is equal\nto nu-transpose b.",
    "start": "3951160",
    "end": "3956230"
  },
  {
    "text": "So this is nu-transpose b. And then I put this over here\nand I have minus nu-transpose.",
    "start": "3956230",
    "end": "3965140"
  },
  {
    "text": "And I get this. So here's what I\njust showed you.",
    "start": "3965140",
    "end": "3970240"
  },
  {
    "text": "I said take any\nx that's feasible for this linear program. And what I showed you is I don't\nknow what c-transpose x is.",
    "start": "3970240",
    "end": "3978118"
  },
  {
    "text": "But I'll tell you\none thing about it. It cannot be smaller than\nminus nu-transpose b. So everyone see that\nI just completely",
    "start": "3978118",
    "end": "3984640"
  },
  {
    "text": "derived it from scratch with\nnone of this Lagrangian, blah, blah, blah. And dual function.",
    "start": "3984640",
    "end": "3989890"
  },
  {
    "text": "Everybody see it? So that's how that goes. Now this is already\nkind of interesting.",
    "start": "3989890",
    "end": "3997970"
  },
  {
    "text": "As a matter of\nfact, later, here's what you're going to find. It's pretty awesome.",
    "start": "3997970",
    "end": "4003280"
  },
  {
    "text": "When we do a numerical\nsolution of an LP, here's what's going to happen. When you solve that LP,\nyou will get x back,",
    "start": "4003280",
    "end": "4012620"
  },
  {
    "text": "whether you like it or not,\nyou're also going to get a nu.",
    "start": "4012620",
    "end": "4017750"
  },
  {
    "text": "And this is going to be\ntrue for most all the convex problems you solve,\nwhich is amazing,",
    "start": "4017750",
    "end": "4024402"
  },
  {
    "text": "because you're going to\nget a nu and the bound is going to be tight. So what's going to happen. So your model of a solver\nis going to look like this.",
    "start": "4024403",
    "end": "4031250"
  },
  {
    "text": "Here's my problem. And they go, no problem. They say, here's x. You check it's feasible and its\nobjective value is like 3.7.",
    "start": "4031250",
    "end": "4038830"
  },
  {
    "text": "Everybody following this? You will also be given what's\ncalled a dual certificate.",
    "start": "4038830",
    "end": "4046029"
  },
  {
    "text": "A dual certificate is, it'll\ngive you dual variables, which if you evaluate the dual\nobjective, which is g,",
    "start": "4046030",
    "end": "4053019"
  },
  {
    "text": "you would get a lower\nbound on the optimal value. Everybody following this? When you evaluate\nit, it's actually",
    "start": "4053020",
    "end": "4059140"
  },
  {
    "text": "equal to the prime\nvalue you were given, the objective value. Everybody see this?",
    "start": "4059140",
    "end": "4064780"
  },
  {
    "text": "That's called a\ndual certificate. So it means you don't even\nhave to trust the solver.",
    "start": "4064780",
    "end": "4071650"
  },
  {
    "text": "It basically,\nabstractly, I think it's beautiful it\nactually returns two things, an optimal point\nand a very, short self-contained",
    "start": "4071650",
    "end": "4080710"
  },
  {
    "text": "proof that no one\ncould do better. Everybody got it? That's what a dual\ncertificate is.",
    "start": "4080710",
    "end": "4087190"
  },
  {
    "text": "We'll talk much more\nabout this later. But it's super\ninteresting, right.",
    "start": "4087190",
    "end": "4092319"
  },
  {
    "text": "It's making sense, vaguely? ",
    "start": "4092320",
    "end": "4098500"
  },
  {
    "text": "Here's one. We can do a quality-constrained\nminimization. So here I just have a\ngeneral norm, right.",
    "start": "4098500",
    "end": "4104560"
  },
  {
    "text": "So I'm going to minimize a\nnorm subject to Ax equals b. This is the Lagrangian.",
    "start": "4104560",
    "end": "4110679"
  },
  {
    "text": "I take the norm\nminus and then I have nu-transpose times Ax\nminus b, or, in this case,",
    "start": "4110680",
    "end": "4117229"
  },
  {
    "text": "I wrote it as b minus\nAx, and I get this thing. And then we have\nto minimize this.",
    "start": "4117229",
    "end": "4123130"
  },
  {
    "text": "Now, by the way, if\nyou look at this, you'll already see\nsomething clear. b-transpose nu has\nnothing to do with x.",
    "start": "4123130",
    "end": "4129910"
  },
  {
    "text": "So that comes out\nof the infimum here, but here this is a norm\nminus a linear function.",
    "start": "4129910",
    "end": "4136000"
  },
  {
    "text": "That's actually the\nconjugate function. So what happens?",
    "start": "4136000",
    "end": "4141771"
  },
  {
    "text": "And you can also\nwork out what that is, the norm minus\na linear function.",
    "start": "4141771",
    "end": "4147568"
  },
  {
    "text": "When I minimize\nthat, I either get 0 or I get minus infinity,\nright, and so here you",
    "start": "4147569",
    "end": "4152630"
  },
  {
    "text": "would get this thing. And again I'm not\ngoing over the details. I would let you do that. But that you'd get\nsomething like that.",
    "start": "4152630",
    "end": "4158960"
  },
  {
    "text": "So the dual function is this\nlinear function b-transpose nu, provided A-transpose nu in the\ndual norm is less than one.",
    "start": "4158960",
    "end": "4167630"
  },
  {
    "text": "So that's what you get. Then, it says this.",
    "start": "4167630",
    "end": "4173649"
  },
  {
    "text": "You want to solve this problem. Then here's a lower bound. It says a lower bound is\nb-transpose nu for any nu that",
    "start": "4173649",
    "end": "4180850"
  },
  {
    "text": "satisfies this, right. In the dual norm right so let's\nsuppose this is a minimum fuel",
    "start": "4180850",
    "end": "4188278"
  },
  {
    "text": "trajectory generation\nproblem and you want to take a\nsatellite from where it is to where it\nis supposed to be",
    "start": "4188279",
    "end": "4193679"
  },
  {
    "text": "and you want to\nuse minimum fuel, this would probably be\nsomething like an l1 norm. OK, that's your job.",
    "start": "4193680",
    "end": "4199860"
  },
  {
    "text": "You work this out. And what's cool is this says,\nif I take any vector at all",
    "start": "4199860",
    "end": "4209520"
  },
  {
    "text": "and it satisfies this, then if I\nevaluate b-transpose nu, that's",
    "start": "4209520",
    "end": "4215670"
  },
  {
    "text": "a lower bound on the\nminimum amount of fuel it's going to take you\nto get from where you are",
    "start": "4215670",
    "end": "4220710"
  },
  {
    "text": "to where you're supposed to be. Everybody got this? So it's already getting\nkind of interesting. And there should be\nat least very strong",
    "start": "4220710",
    "end": "4227580"
  },
  {
    "text": "hints that a lot of this\nis unbelievably useful. So everybody got this?",
    "start": "4227580",
    "end": "4232810"
  },
  {
    "text": "So this is not obvious, right. These things are not obvious. I could give just a\nsimple proof directly",
    "start": "4232810",
    "end": "4237870"
  },
  {
    "text": "that this is a lower bound. And so on. Now it's about to get even\nmore interesting, because we're",
    "start": "4237870",
    "end": "4244060"
  },
  {
    "text": "going to look at a problem. Well, it'll come up again. So it's two-way partitioning.",
    "start": "4244060",
    "end": "4250450"
  },
  {
    "text": "So this is the two way\npartitioning problem. ",
    "start": "4250450",
    "end": "4256540"
  },
  {
    "text": "I have a set of n objects\nand I want to partition them",
    "start": "4256540",
    "end": "4263950"
  },
  {
    "text": "into to two groups. And the way I'll do that is I'll\nencode it with a vector x that",
    "start": "4263950",
    "end": "4271090"
  },
  {
    "text": "has plus one if\nxi equals plus 1, which means item i is\nin the first group.",
    "start": "4271090",
    "end": "4277780"
  },
  {
    "text": "xi equals minus one if\nyou're in the second group. Everybody got it? So that says that each x is\neither plus or minus one.",
    "start": "4277780",
    "end": "4287340"
  },
  {
    "text": "And then my objective\nis this thing. And to get a rough\nidea, we should actually say what x transpose\nWx is it's because it's",
    "start": "4287340",
    "end": "4294239"
  },
  {
    "text": "an interesting problem\nand maybe you've seen it in some other class\nor something like that. But it's worth\nsaying what it is.",
    "start": "4294240",
    "end": "4302820"
  },
  {
    "text": "It's x transpose Wx? Yeah. OK so it's x-transpose Wx,\nbut that's equal to this.",
    "start": "4302820",
    "end": "4310349"
  },
  {
    "text": "It's the sum over i\nand j of W i j xi xj.",
    "start": "4310350",
    "end": "4317910"
  },
  {
    "text": "I mean so we all\nagree with that. That's a quadratic form. Now xi and xj are each\nplus or minus one.",
    "start": "4317910",
    "end": "4324300"
  },
  {
    "text": "So this product is plus one. If items i and j are\nin the same group",
    "start": "4324300",
    "end": "4330719"
  },
  {
    "text": "and it's minus one if\nthey're in different groups. And so this is equal to\nthe sum of the W's when",
    "start": "4330720",
    "end": "4339270"
  },
  {
    "text": "they're in the same group\nminus the sum of W's when they're in different groups.",
    "start": "4339270",
    "end": "4344323"
  },
  {
    "text": "That's what this says. And so you should interpret--\nwhat are we doing? Are we maximizing or minimizing? We are minimizing.",
    "start": "4344323",
    "end": "4349800"
  },
  {
    "text": "So if we're minimizing\nthis, it basically says W i is the cost\nof having, let's",
    "start": "4349800",
    "end": "4356340"
  },
  {
    "text": "say you're doing we're going\nto split a group of people into two groups right\nso this says if Wij",
    "start": "4356340",
    "end": "4363750"
  },
  {
    "text": "is the cost of having person i\nand person j in the same team. If we're making two teams.",
    "start": "4363750",
    "end": "4369360"
  },
  {
    "text": "Everybody got this? So if Wij is positive, it means\nthey don't like each other if it's negative.",
    "start": "4369360",
    "end": "4375730"
  },
  {
    "text": "That's totally cool. It means there's\na strong affinity and we would like them\nto be in the same group. Everybody got it?",
    "start": "4375730",
    "end": "4381150"
  },
  {
    "text": "And the same is true for-- basically, you have an\nequal and opposite charge.",
    "start": "4381150",
    "end": "4386280"
  },
  {
    "text": "So if you say Wij\nis plus three, it means you will pay\nthree if you assign i",
    "start": "4386280",
    "end": "4394620"
  },
  {
    "text": "and j to the same group. And you will have a\nrevenue or income of three,",
    "start": "4394620",
    "end": "4400230"
  },
  {
    "text": "if it'll become minus three. And if it's minus three,\nit's the other way around,",
    "start": "4400230",
    "end": "4405280"
  },
  {
    "text": "so the W's specify how\nwe like how much it irritates us to have\nitems in the same group",
    "start": "4405280",
    "end": "4412096"
  },
  {
    "text": "or different groups.  So this is the problem.",
    "start": "4412097",
    "end": "4419120"
  },
  {
    "text": "OK, so let's look at\nthe dual function. It's obviously non-convex,\nfor two reasons.",
    "start": "4419120",
    "end": "4425000"
  },
  {
    "text": "W need not be positive\nsemi-definite. We didn't say that it was.",
    "start": "4425000",
    "end": "4430490"
  },
  {
    "text": "Reason two is we\ngot a big old fail right there right because we\nhave squares equal to one.",
    "start": "4430490",
    "end": "4438770"
  },
  {
    "text": "By the way, if I take xi squared\nless than or equal to one, that's convex.",
    "start": "4438770",
    "end": "4443900"
  },
  {
    "text": "If W is positive, semi-definite. We take the dual function.",
    "start": "4443900",
    "end": "4450620"
  },
  {
    "text": "We take the objective plus. And then I just\nsimply add up nu-i times these xi\nsquared minus one.",
    "start": "4450620",
    "end": "4457219"
  },
  {
    "text": "And what I end up with\nis a quadratic function. A quadratic form in\nx plus a constant.",
    "start": "4457220",
    "end": "4465079"
  },
  {
    "text": "OK, and now we're going\nto have our quadratic form minimization. Earlier today we had our how do\nyou minimize an affine function",
    "start": "4465080",
    "end": "4471860"
  },
  {
    "text": "conversation. Now we're going to have\nquadratic form minimization conversation. So if you minimize a quadratic\nform, what do you get?",
    "start": "4471860",
    "end": "4480085"
  },
  {
    "text": " Someone walks up to\nthe street and says,",
    "start": "4480085",
    "end": "4485200"
  },
  {
    "text": "here's my x-transpose,\nnot W, But x-transpose.",
    "start": "4485200",
    "end": "4490900"
  },
  {
    "text": "Ax, A is symmetric what's\nthe minimum of that? ",
    "start": "4490900",
    "end": "4499570"
  },
  {
    "text": "Well, if that matrix\nhas a negative, it's got real eigenvalues,\nsymmetric, right? If that matrix has a\nnegative eigenvalue,",
    "start": "4499570",
    "end": "4505300"
  },
  {
    "text": "that minimum is, for\nsure, minus infinity, because you sail out along that,\nfind a negative eigenvalue,",
    "start": "4505300",
    "end": "4512555"
  },
  {
    "text": "for that matter just\nfind a point where the quadratic form is negative. If you just go out\nfarther and farther,",
    "start": "4512555",
    "end": "4518935"
  },
  {
    "text": "it'll get more\nand more negative. In fact, quadratically so, then\nthe minimum is minus infinity.",
    "start": "4518935",
    "end": "4524200"
  },
  {
    "text": "Everybody got it? But if that matrix is\npositive, semi-definite. And I ask, what's the minimum?",
    "start": "4524200",
    "end": "4529570"
  },
  {
    "text": "The answer is real simple. It's zero, because\nyou could take zero as your x equals\nzero, but because it's",
    "start": "4529570",
    "end": "4537480"
  },
  {
    "text": "positive semi-definite,\nyou can't get it. It can't be anything\nless than that. So that's it. Everybody follow that.",
    "start": "4537480",
    "end": "4542980"
  },
  {
    "text": "So g is this function. It's minus one transpose nu\nprovided W plus diag of nu",
    "start": "4542980",
    "end": "4548970"
  },
  {
    "text": "is bigger than or equal to zero. That's in a symmetric, in\na symmetric matrix sense.",
    "start": "4548970",
    "end": "4555179"
  },
  {
    "text": "OK, and the lower bound property\nsays you see this problem here,",
    "start": "4555180",
    "end": "4560670"
  },
  {
    "text": "you know what. Here's a bound on its objective.",
    "start": "4560670",
    "end": "4565680"
  },
  {
    "text": "It is. That's a lower bound on\nits minus one transpose nu, provided for any nu you\ncan find that satisfies this.",
    "start": "4565680",
    "end": "4572790"
  },
  {
    "text": "OK? So that's what that says. And it turns out you can get\nall sorts of famous bounds out",
    "start": "4572790",
    "end": "4580260"
  },
  {
    "text": "of this by just plugging\nnumbers in, but that's the idea. And this is related to\nspectral partitioning.",
    "start": "4580260",
    "end": "4589350"
  },
  {
    "text": "How many people have seen that? One hand. Just a hand. Oh, I'm sorry,\neverybody needs to know",
    "start": "4589350",
    "end": "4596890"
  },
  {
    "text": "about spectral partitioning. You're not going to be able to\ndo any serious street fighting applied math if you\ndon't know about that,",
    "start": "4596890",
    "end": "4604090"
  },
  {
    "text": "because partitioning\nproblems come up all the time and you need to know about it.",
    "start": "4604090",
    "end": "4609170"
  },
  {
    "text": "I'll just show you quickly,\nwhat it is here, just for fun. I won't go into the details, but\nit's actually weirdly another--",
    "start": "4609170",
    "end": "4616900"
  },
  {
    "text": "Let me just explain\nit very quickly.  I won't go into that--",
    "start": "4616900",
    "end": "4622450"
  },
  {
    "text": "I'll give a very\nsimple version of it.",
    "start": "4622450",
    "end": "4627820"
  },
  {
    "text": "So you have this\nthing and then we're",
    "start": "4627820",
    "end": "4633250"
  },
  {
    "text": "going to minimize this\nthing, subject to xi squared",
    "start": "4633250",
    "end": "4638530"
  },
  {
    "text": "equals one. That's not convex, right? But if this is true,\nif xi squared is one,",
    "start": "4638530",
    "end": "4646240"
  },
  {
    "text": "then everyone here would\nhave to, I would hope, agree that that's the case.",
    "start": "4646240",
    "end": "4651910"
  },
  {
    "text": "Now, by the way,\nthis is a relaxation. Because if the former constraint\nholds, then this one has to.",
    "start": "4651910",
    "end": "4658310"
  },
  {
    "text": "OK W is symmetric. Anybody take a look at that\nhow to solve that problem. First of all, is it convex?",
    "start": "4658310",
    "end": "4664325"
  },
  {
    "text": " That convex. ",
    "start": "4664325",
    "end": "4670480"
  },
  {
    "text": "No. Because of this, right. If that was less than or\nequal to, it'd be convex.",
    "start": "4670480",
    "end": "4676270"
  },
  {
    "text": "But this is not convex. This is one of the-- there's\nabout seven problems we",
    "start": "4676270",
    "end": "4682540"
  },
  {
    "text": "can solve that are not convex. This is one of them. It's called it's just\nan eigenvalue problem. What's the solution of this,\nin terms of the eigenvectors?",
    "start": "4682540",
    "end": "4698210"
  },
  {
    "text": "What's that? The smallest one. Yeah, we know how to do this. You find the smallest\neigenvalue of W.",
    "start": "4698210",
    "end": "4704690"
  },
  {
    "text": "You find its\nassociated eigenvector and you like scale it by n. And that's literally\nthe solution.",
    "start": "4704690",
    "end": "4710480"
  },
  {
    "text": "That's not a heuristic\nor anything like that. Oh, by the way,\nwhen you do that--",
    "start": "4710480",
    "end": "4716990"
  },
  {
    "text": "what have I done here my God-- OK, when you do that, you get\nsomething really interesting.",
    "start": "4716990",
    "end": "4725450"
  },
  {
    "text": "First of all, you get a number\nbecause this is a relaxation. That's a lower bound on\nthe original problem.",
    "start": "4725450",
    "end": "4731150"
  },
  {
    "text": "But then you also get this. I get an x's which are all\nthey're not they satisfy this.",
    "start": "4731150",
    "end": "4737400"
  },
  {
    "text": "But they're definitely\nnot all plus, minus one. Somebody want to\nmake a suggestion? To make a heuristic\nassignment at that point?",
    "start": "4737400",
    "end": "4745489"
  },
  {
    "text": "You have a vector. They're all floats. Their sum of their squares is n.",
    "start": "4745490",
    "end": "4750600"
  },
  {
    "text": "But the original problem\nyou wanted all the xi's to be plus or minus one. Someone please\nmake a suggestion.",
    "start": "4750600",
    "end": "4756400"
  },
  {
    "text": "Round them. Round them, done. Take the sign. Done. Congratulations.",
    "start": "4756400",
    "end": "4761659"
  },
  {
    "text": "That is spectral partitioning. Except for one thing I\ndidn't tell you about, which is we can adjust\nthe diagonal of W",
    "start": "4761660",
    "end": "4769420"
  },
  {
    "text": "in a way that will make\nthat slightly better. That's spectral partitioning. This works for\ngigantic problems.",
    "start": "4769420",
    "end": "4777400"
  },
  {
    "text": "It works super well,\nso there you go. Yeah. So in this problem, W has\nto be symmetric or positive",
    "start": "4777400",
    "end": "4783820"
  },
  {
    "text": "semi-definite. No, no, no, no, no, no, no. In this problem, well, OK. Non-symmetric doesn't\neven make sense,",
    "start": "4783820",
    "end": "4789370"
  },
  {
    "text": "although just because you're\njust replacing W with its quadratic part-- I mean sorry, its\nsymmetric part-- if I have a quadratic\nform and I replace",
    "start": "4789370",
    "end": "4795970"
  },
  {
    "text": "the matrix non-symmetric,\nI just replace that matrix with that\nmatrix plus its transpose divided by two.",
    "start": "4795970",
    "end": "4801701"
  },
  {
    "text": "Number one, then does it have\nto be positive semi-definite? Absolutely not. We know this problem\nwe can solve completely",
    "start": "4801702",
    "end": "4811210"
  },
  {
    "text": "as long as W is symmetric. It can have negative\neigenvalues, positive eigenvalues,\ncouldn't care less. The solution to this is\nx is a scaled version",
    "start": "4811210",
    "end": "4819740"
  },
  {
    "text": "of the minimizing\neigenvector, right. Is that OK?",
    "start": "4819740",
    "end": "4825640"
  },
  {
    "text": "I think we'll quit here. It's already getting\nkind of interesting. And we'll keep\ngoing on Thursday.",
    "start": "4825640",
    "end": "4833340"
  },
  {
    "start": "4833340",
    "end": "4838000"
  }
]